{"title": "Combining Online and Offline Knowledge in UCT", "authors": "Sylvain Gelly; David Silver", "pub_date": "", "abstract": "The UCT algorithm learns a value function online using sample-based search. The T D(\u03bb) algorithm can learn a value function offline for the on-policy distribution. We consider three approaches for combining offline and online value functions in the UCT algorithm. First, the offline value function is used as a default policy during Monte-Carlo simulation. Second, the UCT value function is combined with a rapid online estimate of action values. Third, the offline value function is used as prior knowledge in the UCT search tree. We evaluate these algorithms in 9 \u00d7 9 Go against GnuGo 3.7.10. The first algorithm performs better than UCT with a random simulation policy, but surprisingly, worse than UCT with a weaker, handcrafted simulation policy. The second algorithm outperforms UCT altogether. The third algorithm outperforms UCT with handcrafted prior knowledge. We combine these algorithms in MoGo, the world's strongest 9 \u00d7 9 Go program. Each technique significantly improves MoGo's playing strength.", "sections": [{"heading": "Introduction", "text": "Value-based reinforcement learning algorithms have achieved many notable successes. For example, variants of the T D(\u03bb) algorithm have learned to achieve a master level of play in the games of Chess (Baxter et al., 1998), Checkers (Schaeffer et al., 2001) and Othello (Buro, 1999). In each case, the value function is approximated by a linear combination of binary features. The weights are adapted to find the relative contribution of each feature to the expected value, and the policy is improved with respect to the new value function. Using this approach, the agent learns knowledge that applies across the on-policy distribution of states.\nUCT is a sample-based search algorithm (Kocsis & Szepesvari, 2006) that has achieved remarkable success in the more challenging game of Go (Gelly et al., 2006). At every time-step the UCT algorithm builds a search tree, estimating the value function at each state by Monte-Carlo simulation. After each simulated episode, the values in the tree are updated online and the simulation policy is improved with respect to the new values. These values represent local knowledge that is highly specialised to the current state.\nIn this paper, we seek to combine the advantages of both approaches. We introduce two new algorithms that combine the general knowledge accumulated by an offline reinforcement learning algorithm, with the local knowledge found online by sample-based search. We also introduce a third algorithm that combines two sources of knowledge found online by sample-based search: one of which is unbiased, and the other biased but fast to learn.\nTo test our algorithms, we use the domain of 9 \u00d7 9 Go (Figure 1). The program MoGo, based on the UCT algorithm, has won the KGS Computer Go tournament at 9 \u00d7 9, 13 \u00d7 13 and 19 \u00d7 19 Go, and is the highest rated program on the Computer Go Online Server (Gelly et al., 2006;Wang & Gelly, 2007). The program RLGO, based on a linear combination of binary features and using the T D(0) algorithm, has learned the strongest known value function for 9 \u00d7 9 Go that doesn't incorporate significant prior domain knowledge (Silver et al., 2007). We investigate here whether combining the online knowledge of MoGo with the offline knowledge of RLGO can achieve better overall performance.", "publication_ref": ["b2", "b10", "b5", "b9", "b8", "b8", "b17", "b12"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Value-Based Reinforcement Learning", "text": "Value-based reinforcement learning methods use a value function as an intermediate step for computing a policy. In episodic tasks the return R t is the total reward accumulated in that episode from time t until it terminates at time T . The action value function Q \u03c0 (s, a) is the expected return after action a \u2208 A is taken in state s \u2208 S, using policy \u03c0 to select all subsequent actions,\nR t = T k=t+1 r k Q \u03c0 (s, a) = E \u03c0 [R t |s t = s, a t = a]\nThe value function can be estimated by a variety of different algorithms, for example using the T D(\u03bb) algorithm (Sutton, 1988). The policy can then be improved with respect to the new value function, for example using an \u01eb-greedy policy to balance exploitation (selecting the maximum value action) with exploration (selecting a random action). A cyclic process of evaluation and policy improvement, known as policy iteration, forms the basis of value-based reinforcement learning algorithms (Sutton & Barto, 1998).\nIf a model of the environment is available or can be learned, then value-based reinforcement learning algorithms can be used to perform sample-based search. Rather than learning from real experience, simulated episodes can be sampled from the model. The value function is then updated using the simulated experience. The Dyna architecture provides one example of sample-based search (Sutton, 1990).", "publication_ref": ["b13", "b16", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "UCT", "text": "The UCT algorithm (Kocsis & Szepesvari, 2006) is a value-based reinforcement learning algorithm that focusses exclusively on the start state and the tree of subsequent states.\nThe action value function Q U CT (s, a) is approximated by a partial tabular representation T \u2286 S \u00d7 A, containing a subset of all (state, action) pairs. This can be thought of as a search tree of visited states, with the start state at the root. A distinct value is estimated for each state and action in the tree by Monte-Carlo simulation.\nThe policy used by UCT is designed to balance exploration with exploitation, based on the multi-armed bandit algorithm UCB (Auer et al., 2002).\nIf all actions from the current state s are represented in the tree, \u2200a \u2208 A(s), (s, a) \u2208 T , then UCT selects the action that maximises an upper confidence bound on the action value,\nQ \u2295 U CT (s, a) = Q U CT (s, a) + c log n(s) n(s, a) \u03c0 U CT (s) = argmax a Q \u2295 U CT (s, a)\nwhere n(s, a) counts the number of times that action a has been selected from state s, and n(s) counts the total number of visits to a state, n(s) = a n(s, a).\nIf any action from the current state s is not represented in the tree, \u2203a \u2208 A(s), (s, a) \u2208 T , then the uniform random policy \u03c0 random is used to select an action from all unrepresented actions,\u00c3(s) = {a|(s, a) / \u2208 T }.\nAt the end of an episode s 1 , a 1 , s 2 , a 2 , ..., s T , each state action pair in the search tree, (s t , a t ) \u2208 T , is updated using the return from that episode,\nn(s t , a t ) \u2190 n(s t , a t ) + 1 (1) Q U CT (s t , a t ) \u2190 Q U CT (s t , a t ) (2) + 1 n(s t , a t ) [R t \u2212 Q U CT (s t , a t )]\nNew states and actions from that episode, (s t , a t ) \u2208 T , are then added to the tree, with initial value Q(s t , a t ) = R t and n(s t , a t ) = 1. In some cases, it is more memory efficient to only add the first visited state and action such that (s t , a t ) / \u2208 T (Coulom, 2006;Gelly et al., 2006). This procedure builds up a search tree containing n nodes after n episodes of experience.\nThe UCT policy can be thought of in two stages. In the beginning of each episode it selects actions according to knowledge contained within the search tree. But once it leaves the scope of its search tree it has no knowledge and behaves randomly. Thus each state in the tree estimates its value by Monte-Carlo simulation. As more information propagates up the tree, the policy improves, and the Monte-Carlo estimates are based on more accurate returns.\nIf a model is available, then UCT can be used as a sample-based search algorithm. Episodes are sampled from the model, starting from the actual state\u015d. A new representation T (\u015d) is constructed at every actual time-step, using simulated experience. Typically, thousands of episodes can be simulated at each step, so that the value function contains a detailed search tree for the current state\u015d.\nIn a two-player game, the opponent can be modelled using the agent's own policy, and episodes simulated by self-play. UCT is used to maximise the upper confidence bound on the agent's value and to minimise the lower confidence bound on the opponent's. Under certain assumptions about non-stationarity, UCT converges on the minimax value (Kocsis & Szepesvari, 2006). However, unlike other minimax search algorithms such as alpha-beta search, UCT requires no prior domain knowledge to evaluate states or order moves. Furthermore, the UCT search tree is nonuniform and favours the most promising lines. These properties make UCT ideally suited to the game of Go, which has a large state space and branching factor, and for which no strong evaluation functions are known.", "publication_ref": ["b9", "b0", "b6", "b8", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Linear Value Function Approximation", "text": "UCT is a tabular method and does not generalise between states. Other value-based reinforcement learning methods offer a rich variety of function approximation methods for state abstraction in complex domains (Schraudolph et al., 1994;Enzenberger, 2003;Sutton, 1996). We consider here a simple approach to function approximation that requires minimal prior domain knowledge (Silver et al., 2007), and which has proven successful in many other domains (Baxter et al., 1998;Schaeffer et al., 2001;Buro, 1999).\nWe wish to estimate a simple reward function: r = 1 if the agent wins the game and r = 0 otherwise. The value function is approximated by a linear combination of binary features \u03c6 with weights \u03b8,\nQ RLGO (s, a) = \u03c3 \u03c6(s, a) T \u03b8\nwhere the sigmoid squashing function \u03c3 maps the value function to an estimated probability of winning the game. After each time-step, weights are updated using the T D(0) algorithm (Sutton, 1988). Because the value function is a probability, we modify the loss function so as to minimise the cross entropy between the current value and the subsequent value,\n\u03b4 = r t+1 + Q RLGO (s t+1 , a t+1 ) \u2212 Q RLGO (s t , a t ) \u2206\u03b8 i = \u03b1 |\u03c6(s t , a t )| \u03b4\u03c6 i\nwhere \u03b4 is the TD-error and \u03b1 is a step-size parameter.\nIn the game of Go, the notion of shape has strategic importance. For this reason we use binary features \u03c6(s, a) that recognise local patterns of stones (Silver et al., 2007). Each local shape feature matches a specific configuration of stones and empty intersections within a particular rectangle on the board (Figure 1). Local shape features are created for all configurations, at all positions on the board, from 1 \u00d7 1 up to 3 \u00d7 3. Two sets of weights are used: in the first set, weights are shared between all local shape features that are rotationally or reflectionally symmetric. In the second set, weights are also shared between all local shape features that are translations of the same pattern.\nDuring training, two versions of the same agent play against each other, both using an \u01eb-greedy policy. Each game is started from the empty board and played through to completion, so that the loss is minimised for the on-policy distribution of states. Thus, the value function approximation learns the relative contribution of each local shape feature to winning, across the full distribution of positions encountered during selfplay.", "publication_ref": ["b11", "b7", "b15", "b12", "b2", "b10", "b5", "b13", "b12"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "UCT with a Default Policy", "text": "The UCT algorithm has no knowledge beyond the scope of its search tree. If it encounters a state that is not represented in the tree, it behaves randomly. Gelly et al. (Gelly et al., 2006) combined UCT with a default policy that is used to complete episodes once UCT leaves the search tree. We denote the original UCT algorithm by U CT (\u03c0 random ) and this extended algorithm by U CT (\u03c0) for a default policy \u03c0.\nThe world's strongest 9 \u00d7 9 Computer Go program MoGo uses UCT with a hand-crafted default policy \u03c0 M oGo (Gelly et al., 2006). However, in many domains it is difficult to construct a good default policy. Even when expert knowledge is available, it may be hard to interpret and encode. Furthermore, the default policy must be fast, so that many episodes can be simulated, and a large search tree built. We would like a method for learning a high performance default policy with minimal domain knowledge.\nA linear combination of binary features provides one such method. Binary features are fast to evaluate and can be updated incrementally. The representation learned by this approach is known to perform well in many domains (Baxter et al., 1998;Schaeffer et al., 2001). Minimal domain knowledge is necessary to specify the features (Silver et al., 2007;Buro, 1999).\nWe learn a value function Q RLGO for the domain of 9 \u00d7 9 Go using a linear combination of binary features (see Section 4). It is learned offline from games of self-play. We use this value function to generate a default policy for UCT. As Monte-Carlo simulation works best with a stochastic default policy, we consider three different approaches for generating a stochastic policy from the value function Q RLGO .\nFirst, we consider an \u01eb-greedy policy,\n\u03c0 \u01eb (s, a) = 1 \u2212 \u01eb + \u01eb |A(s)| if a = argmax a \u2032 QRLGO(s, a \u2032 ) \u01eb |A(s)| otherwise\nSecond, we consider a greedy policy over a noisy value function, corrupted by Gaussian noise \u03b7(s, a) \u223c N (0, \u03c3 2 ),\n\u03c0 \u03c3 (s, a) = 1 if a = argmax a \u2032 QRLGO(s, a \u2032 ) + \u03b7(s, a \u2032 ) 0 otherwise\nThird, we select moves using a softmax distribution with temperature parameter \u03c4 ,\n\u03c0 \u03c4 (s, a) = e QRLGO(s,a)/\u03c4 a \u2032 e QRLGO(s,a \u2032 )/\u03c4\nWe compared the performance of each class of default policy \u03c0 random , \u03c0 M oGo , \u03c0 \u01eb , \u03c0 \u03c3 , and \u03c0 \u03c4 . Figure 2 assesses the relative strength of each default policy (as a Go player), in a round-robin tournament of 6000 games between each pair of policies. With little or no randomisation, the policies based on Q RLGO outperform both the random policy \u03c0 random and MoGo's handcrafted policy \u03c0 M oGo by a margin of over 90%.\nAs the level of randomisation increases, the policies degenerate towards the random policy \u03c0 random .\nNext, we compare the accuracy of each default policy \u03c0 for Monte-Carlo simulation, on a test suite of 200 hand-labelled positions. 1000 episodes of self-play were played from each test position using each policy \u03c0. We measured the MSE between the average return (i.e. the Monte-Carlo estimate) and the hand-labelled value (see Figure 3). In general, a good policy for U CT (\u03c0) must be able to evaluate accurately in Monte-Carlo simulation. In our experiments with MoGo, the MSE appears to have a close relationship with playing strength.\nThe MSE improves from uniform random simulations when a stronger and appropriately randomised default policy is used. If the default policy is too deterministic, then Monte-Carlo simulation fails to provide any benefits and the performance of \u03c0 drops dramatically.\nIf the default policy is too random, then it becomes equivalent to the random policy \u03c0 random . Intuitively, one might expect that a stronger, appropriately randomised policy would outperform a weaker policy during Monte-Carlo simulation. However, the accuracy of \u03c0 \u01eb , \u03c0 \u03c3 and \u03c0 \u03c4 never come close to the accuracy of the handcrafted policy \u03c0 M oGo , despite the large improvement in playing strengths for these default policies. To verify that the default policies based on Q RLGO are indeed stronger in our particular suite of test positions, we reran the round-robin tournament, starting from each of these positions in turn, and found that the relative strengths of the default policies remain similar. We also compared the performance of the complete UCT algorithm, using the best default policy based on Q RLGO and the parameter minimising MSE (see Table 1). This experiment confirms that the MSE results apply in actual play.\nIt is surprising that an objectively stronger default policy does not lead to better performance in UCT. Furthermore, because this result only requires Monte-Carlo simulation, it has implications for other samplebased search algorithms. It appears that the nature of the simulation policy may be as or more important than its objective performance. Each policy has its own bias, leading it to a particular distribution of episodes during Monte-Carlo simulation. If the distribution is skewed towards an objectively unlikely outcome, then the predictive accuracy of the search algorithm may be impaired.", "publication_ref": ["b8", "b8", "b2", "b10", "b12", "b5"], "figure_ref": ["fig_1", "fig_2"], "table_ref": []}, {"heading": "Rapid Action Value Estimation", "text": "The UCT algorithm must sample every action from a state s \u2208 T before it has a basis on which to compare Algorithm Wins .v. GnuGo U CT (\u03c0 random ) 8.88 \u00b1 0.42 % U CT (\u03c0 \u03c3 ) 9.38 \u00b11.9% U CT (\u03c0 M oGo ) 48.62 \u00b11.1%\nTable 1. Winning rate of the UCT algorithm against GnuGo 3.7.10 (level 0), given 5000 simulations per move, using different default policies. The numbers after the \u00b1 correspond to the standard error from several thousand complete games. \u03c0\u03c3 is used with \u03c3 = 0.15.\nvalues. Furthermore, to produce a low-variance estimate of the value, each action in state s must be sampled multiple times. When the action space is large, this can cause slow learning. To solve this problem, we introduce a new algorithm U CT RAV E , which forms a rapid action value estimate for action a in state s, and combines this online knowledge into UCT.\nNormally, Monte-Carlo methods estimate the value by averaging the return of all episodes in which a is selected immediately. Instead, we average the returns of all episodes in which a is selected at any subsequent time. In the domain of Computer Go, this idea is known as the all-moves-as-first heuristic (Bruegmann, 1993). However, the same idea can be applied in any domain where action sequences can be transposed.\nLet Q RAV E (s, a) be the rapid value estimate for action a in state s. After each episode s 1 , a 1 , s 2 , a 2 , ..., s T , the action values are updated for every state s t1 \u2208 T and every subsequent action a t2 such that a t2 \u2208 A(s t1 ), t 1 \u2264 t 2 and \u2200t < t 2 , a t = a t2 ,\nm(s t1 , a t2 ) \u2190 m(s t1 , a t2 ) + 1 Q RAV E (s t1 , a t2 ) \u2190 Q RAV E (s t1 , a t2 ) +1/m(s t1 , a t2 )[R t1 \u2212 Q RAV E (s t1 , a t2 )]\nwhere m(s, a) counts the number of times that action a has been selected at any time following state s.\nThe rapid action value estimate can quickly learn a low-variance value for each action. However, it may introduce some bias, as the value of an action usually depends on the exact state in which it is selected. Hence we would like to use the rapid estimate initially, but use the original UCT estimate in the limit. To achieve this, we use a linear combination of the two estimates, with a decaying weight \u03b2, \nQ \u2295 RAV E (s, a) = Q RAV E (s, a) + c log m(s) m(s, a) \u03b2(s, a) = k 3n(s) + k Q \u2295 U R (s, a) = \u03b2(s, a)Q \u2295 RAV E (s, a) + (1 \u2212 \u03b2(s, a))Q \u2295 U CT (s, a) \u03c0 U R (s) = argmax a Q \u2295 U R (s, a)\nwhere m(s) = a m(s, a). The equivalence parameter k controls the number of episodes of experience when both estimates are given equal weight.\nWe tested the new algorithm U CT RAV E (\u03c0 M oGo ), using the default policy \u03c0 M oGo , for different settings of the equivalence parameter k. For each setting, we played a 2300 game match against GnuGo 3.7.10 (level 8). The results are shown in Figure 4, and compared to the U CT (\u03c0 M oGo ) algorithm with 3000 simulations per move. The winning rate using U CT RAV E varies between 50% and 60%, compared to 24% without rapid estimation. Maximum performance is achieved with an equivalence parameter of 1000 or more. This indicates that the rapid action value estimate is worth about the same as several thousand episodes of UCT simulation.", "publication_ref": ["b3"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "UCT with Prior Knowledge", "text": "The UCT algorithm estimates the value of each state by Monte-Carlo simulation. However, in many cases we have prior knowledge about the likely value of a state. We introduce a simple method to utilise offline knowledge, which increases the learning rate of UCT without biasing its asymptotic value estimates.\nWe modify UCT to incorporate an existing value function Q prior (s, a). When a new state and action (s, a) is added to the UCT representation T , we initialise its value according to our prior knowledge, n(s, a) \u2190 n prior (s, a)\nQ U CT (s, a) \u2190 Q prior (s, a)\nThe number n prior estimates the equivalent experience contained in the prior value function. This indicates the number of episodes that UCT would require to achieve an estimate of similar accuracy. After initialisation, the value function is updated using the normal UCT update (see equations 1 and 2). We denote the new UCT algorithm using default policy \u03c0 by U CT (\u03c0, Q prior ).\nA similar modification can be made to the U CT RAV E algorithm, by initialising the rapid estimates according to prior knowledge,\nm(s, a) \u2190 m prior (s, a) Q RAV E (s, a) \u2190 Q prior (s, a)\nWe compare several methods for generating prior knowledge in 9 \u00d7 9 Go. First, we use an even-game heuristic, Q even (s, a) = 0.5, to indicate that most positions encountered on-policy are likely to be close. Second, we use a grandfather heuristic, Q grand (s t , a) = Q U CT (s t\u22122 , a), to indicate that the value with player P to play is usually similar to the value of the last state with P to play. Third, we use a handcrafted heuristic Q M oGo (s, a). This heuristic was designed such that greedy action selection would produce the best known default policy \u03c0 M oGo (s, a). Finally, we use the linear combination of binary features, Q RLGO (s, a), learned offline by T D(\u03bb) (see section 4).\nFor each source of prior knowledge, we assign an equivalent experience m prior (s, a) = M eq , for various constant values of M eq . We played 2300 games between U CT RAV E (\u03c0 M oGo , Q prior ) and GnuGo 3.7.10 (level 8), alternating colours between each game. The UCT algorithms sampled 3000 episodes of experience at each move (see Figure 5), rather than a fixed time per move. In fact the algorithms have comparable execution time (Table 4).  2. Winning rate of the different UCT algorithms against GnuGo 3.7.10 (level 8), given 3000 simulations per move. The numbers after the \u00b1 correspond to the standard error from several thousand complete games.\nMaximum performance is achieved using an equivalent experience of M eq = 50, which indicates that Q RLGO is worth about as much as 50 episodes of U CT RAV E simulation. It seems likely that these results could be further improved by varying the equivalent experience according to the variance of the prior value estimate.", "publication_ref": [], "figure_ref": ["fig_5"], "table_ref": ["tab_3"]}, {"heading": "Conclusion", "text": "The UCT algorithm can be significantly improved by three different techniques for combining online and offline learning. First, a default policy can be used to complete episodes beyond the UCT search tree. Second, a rapid action value estimate can be used to boost initial learning. Third, prior knowledge can be used to initialise the value function within the UCT tree.\nWe applied these three ideas to the Computer Go program MoGo, and benchmarked its performance against GnuGo 3.7.10 (level 8), one of the strongest 9\u00d79 programs that isn't based on the UCT algorithm. Each new technique increases the winning rate significantly from the previous algorithms, from just 2% for the basic UCT algorithm up to 69% using all three techniques. Table 2 summarises these improvements, given the best parameter settings for each algorithm, and 3000 simulations per move. Table 4 indicates the CPU requirements of each algorithm.\nThese results are based on executing just 3000 simulations per move. When the number of simulations increases, the overall performance of MoGo improves correspondingly. For example, using the combined algorithm U CT RAV E (\u03c0 M oGo , Q M oGo ), the winning rate increases to 92% with more simulations per move. This version of MoGo has achieved an Elo rating of 2320 on the Computer Go Online Server, around 500 points higher than any program not based on UCT, and over 200 points higher than other UCT-based programs 1 (see    The domain knowledge required by this version of MoGo is contained in the default policy \u03c0 M oGo and in the prior value function Q M oGo . By learning a value function offline using T D(\u03bb), we have demonstrated how this requirement for domain knowledge can be removed altogether. The learned value function outperforms the heuristic value function when used as prior knowledge within the tree. Surprisingly, despite its objective superiority, the learned value function performs worse than the handcrafted heuristic when used as a default policy. Understanding why certain policies perform better than others in Monte-Carlo simulations may be a key component of any future advances.\nWe have focussed on the domain of 9\u00d79 Go. It is likely that larger domains, such as 13\u00d713 and 19\u00d719 Go, will increase the importance of the three algorithms. With larger branching factors it becomes increasingly important to simulate accurately, accelerate initial learning, and incorporate prior knowledge. When these ideas are incorporated, the UCT algorithm may prove to be successful in many other challenging domains. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3"]}], "references": [{"ref_id": "b0", "title": "", "journal": "", "year": "2002", "authors": "P Auer; N Cesa-Bianchi; P Fischer"}, {"ref_id": "b1", "title": "Finite-time analysis of the multi-armed bandit problem", "journal": "", "year": "", "authors": ""}, {"ref_id": "b2", "title": "Experiments in parameter learning using temporal differences", "journal": "International Computer Chess Association Journal", "year": "1998", "authors": "J Baxter; A Tridgell; L Weaver"}, {"ref_id": "b3", "title": "", "journal": "", "year": "1993", "authors": "B Bruegmann"}, {"ref_id": "b4", "title": "", "journal": "", "year": "", "authors": "Monte-Carlo Go"}, {"ref_id": "b5", "title": "From simple features to sophisticated evaluation functions", "journal": "", "year": "1999", "authors": "M Buro"}, {"ref_id": "b6", "title": "Efficient selectivity and backup operators in Monte-Carlo tree search", "journal": "", "year": "2006-05-29", "authors": "R Coulom"}, {"ref_id": "b7", "title": "Evaluation in Go by a neural network using soft segmentation", "journal": "", "year": "2003", "authors": "M Enzenberger"}, {"ref_id": "b8", "title": "Modification of UCT with patterns in Monte-Carlo Go", "journal": "", "year": "2006", "authors": "S Gelly; Y Wang; R Munos; O Teytaud"}, {"ref_id": "b9", "title": "Bandit based Monte-Carlo planning", "journal": "", "year": "2006", "authors": "L Kocsis; C Szepesvari"}, {"ref_id": "b10", "title": "Temporal difference learning applied to a high-performance game-playing program", "journal": "", "year": "2001", "authors": "J Schaeffer; M Hlynka; V Jussila"}, {"ref_id": "b11", "title": "Temporal difference learning of position evaluation in the game of Go", "journal": "Advances in Neural Information Processing Systems", "year": "1994", "authors": "N Schraudolph; P Dayan; T Sejnowski"}, {"ref_id": "b12", "title": "Reinforcement learning of local shape in the game of Go", "journal": "", "year": "2007", "authors": "D Silver; R Sutton; M M\u00fcller"}, {"ref_id": "b13", "title": "Learning to predict by the method of temporal differences", "journal": "", "year": "1988", "authors": "R Sutton"}, {"ref_id": "b14", "title": "Integrated architectures for learning, planning, and reacting based on approximating dynamic programming", "journal": "", "year": "1990", "authors": "R Sutton"}, {"ref_id": "b15", "title": "Generalization in reinforcement learning: Successful examples using sparse coarse coding", "journal": "Advances in Neural Information Processing Systems", "year": "1996", "authors": "R Sutton"}, {"ref_id": "b16", "title": "Reinforcement learning: An introduction", "journal": "MIT Press", "year": "1998", "authors": "R Sutton; A Barto"}, {"ref_id": "b17", "title": "Modifications of UCT and sequence-like simulations for Monte-Carlo Go", "journal": "", "year": "2007", "authors": "Y Wang; S Gelly"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. (Left) An example position from a game of 9 \u00d7 9 Go. Black and White take turns to place down stones. Stones can never move, but may be captured if completely surrounded. The player to surround most territory wins the game. (Right) Shapes are an important part of Go strategy. The figure shows (clockwise from top-left) 1 \u00d7 1, 2 \u00d7 1, 2 \u00d7 2 and 3 \u00d7 3 local shape features which occur in the example position.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 .2Figure2. The relative strengths of each class of default policy, against the random policy \u03c0 random (left) and against a handcrafted policy \u03c0MoGo (right). The x axis represents the degree of randomness in each policy.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 .3Figure 3. The MSE of each policy \u03c0 when Monte Carlo simulation is used to evaluate a test suite of 200 hand-labelled positions. The x axis indicates the degree of randomness in the policy.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 .4Figure 4. Winning rate of U CTRAV E (\u03c0MoGo) with 3000 simulations per move against GnuGo 3.7.10 (level 8), for different settings of the equivalence parameter k. The bars indicate the standard error. Each point of the plot is an average over 2300 complete games.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "U CT (\u03c0 M oGo ) 4300 g/s U CT (\u03c0 \u01eb ), U CT (\u03c0 \u03c3 ), U CT (\u03c0 \u03c4 ) 150 g/s U CT RAV E (\u03c0 M oGo ) 4300 g/s U CT RAV E (\u03c0 M oGo , Q M oGo ) 4200 g/s U CT RAV E (\u03c0 M oGo , Q RLGO ) 3600 g/s", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 .5Figure 5. Winning rate of U CTRAV E (\u03c0MoGo) with 3000 simulations per move against GnuGo 3.7.10 (level 8), using different prior knowledge as initialisation. The bars indicate the standard error. Each point of the plot is an average over 2300 complete games.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "The value function learned offline, Q RLGO , outperforms all the other heuristics and increases the winning rate of the U CT RAV E algorithm from 60% to 69%.Algorithm Wins .v. GnuGo U CT (\u03c0 random ) 1.84 \u00b1 0.22 % U CT (\u03c0 M oGo ) 23.88 \u00b10.85% U CT RAV E (\u03c0 M oGo ) 60.47 \u00b1 0.79 % U CT RAV E (\u03c0 M oGo , Q RLGO ) 69 \u00b1 0.91 %", "figure_data": "Table"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ")."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Winning rate of U CTRAV E (\u03c0MoGo, QMoGo) against GnuGo 3.7.10 (level 8) when the number of simulations per move is increased. The asterisked version used on CGOS modifies the simulations/move according to the available time, from 300000 games in the opening to 20000.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Number of simulations per second for each algorithm on a P4 3.4Ghz, at the start of the game. U CT (\u03c0 random ) is faster but much weaker, even with the same time per move. Apart from U CT (\u03c0RLGO), all the other algorithms have a comparable execution speed", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "R t = T k=t+1 r k Q \u03c0 (s, a) = E \u03c0 [R t |s t = s, a t = a]", "formula_coordinates": [2.0, 95.07, 205.63, 154.75, 46.19]}, {"formula_id": "formula_1", "formula_text": "Q \u2295 U CT (s, a) = Q U CT (s, a) + c log n(s) n(s, a) \u03c0 U CT (s) = argmax a Q \u2295 U CT (s, a)", "formula_coordinates": [2.0, 331.07, 132.94, 185.53, 48.0]}, {"formula_id": "formula_2", "formula_text": "n(s t , a t ) \u2190 n(s t , a t ) + 1 (1) Q U CT (s t , a t ) \u2190 Q U CT (s t , a t ) (2) + 1 n(s t , a t ) [R t \u2212 Q U CT (s t , a t )]", "formula_coordinates": [2.0, 320.7, 353.74, 220.74, 52.13]}, {"formula_id": "formula_3", "formula_text": "Q RLGO (s, a) = \u03c3 \u03c6(s, a) T \u03b8", "formula_coordinates": [3.0, 110.5, 500.61, 119.03, 12.58]}, {"formula_id": "formula_4", "formula_text": "\u03b4 = r t+1 + Q RLGO (s t+1 , a t+1 ) \u2212 Q RLGO (s t , a t ) \u2206\u03b8 i = \u03b1 |\u03c6(s t , a t )| \u03b4\u03c6 i", "formula_coordinates": [3.0, 57.66, 632.71, 229.55, 35.05]}, {"formula_id": "formula_5", "formula_text": "\u03c0 \u01eb (s, a) = 1 \u2212 \u01eb + \u01eb |A(s)| if a = argmax a \u2032 QRLGO(s, a \u2032 ) \u01eb |A(s)| otherwise", "formula_coordinates": [4.0, 55.44, 599.46, 239.9, 27.88]}, {"formula_id": "formula_6", "formula_text": "\u03c0 \u03c3 (s, a) = 1 if a = argmax a \u2032 QRLGO(s, a \u2032 ) + \u03b7(s, a \u2032 ) 0 otherwise", "formula_coordinates": [4.0, 55.44, 697.03, 230.55, 21.91]}, {"formula_id": "formula_7", "formula_text": "\u03c0 \u03c4 (s, a) = e QRLGO(s,a)/\u03c4 a \u2032 e QRLGO(s,a \u2032 )/\u03c4", "formula_coordinates": [4.0, 360.15, 329.7, 126.07, 27.32]}, {"formula_id": "formula_8", "formula_text": "m(s t1 , a t2 ) \u2190 m(s t1 , a t2 ) + 1 Q RAV E (s t1 , a t2 ) \u2190 Q RAV E (s t1 , a t2 ) +1/m(s t1 , a t2 )[R t1 \u2212 Q RAV E (s t1 , a t2 )]", "formula_coordinates": [5.0, 329.57, 524.27, 174.79, 37.71]}, {"formula_id": "formula_9", "formula_text": "Q \u2295 RAV E (s, a) = Q RAV E (s, a) + c log m(s) m(s, a) \u03b2(s, a) = k 3n(s) + k Q \u2295 U R (s, a) = \u03b2(s, a)Q \u2295 RAV E (s, a) + (1 \u2212 \u03b2(s, a))Q \u2295 U CT (s, a) \u03c0 U R (s) = argmax a Q \u2295 U R (s, a)", "formula_coordinates": [6.0, 71.16, 329.32, 201.35, 112.17]}, {"formula_id": "formula_10", "formula_text": "Q U CT (s, a) \u2190 Q prior (s, a)", "formula_coordinates": [6.0, 360.58, 200.67, 127.7, 10.71]}, {"formula_id": "formula_11", "formula_text": "m(s, a) \u2190 m prior (s, a) Q RAV E (s, a) \u2190 Q prior (s, a)", "formula_coordinates": [6.0, 356.89, 367.26, 135.1, 25.65]}], "doi": ""}