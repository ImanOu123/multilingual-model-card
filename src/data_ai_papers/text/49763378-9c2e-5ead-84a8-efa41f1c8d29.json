{"title": "Fast and Accurate k-llleans For Large Datasets", "authors": "Michael Shindler; Alex Wong; Adam Meyerson", "pub_date": "", "abstract": "Clustering is a popular problem with many applications. We consider the k-means problem in the situation where the data is too large to be stored in main memory and must be accessed sequentially, such as from a disk, and where we must use as little memory as possible. Our algorithm is based on recent theoretical results, with significant improvements to make it practical. Our approach greatly simplifies a recently developed algorithm, both in design and in analysis, and eliminates large constant factors in the approximation guarantee, the memory requirements, and the running time. We then incorporate approximate nearest neighbor search to compute k-means in o(nk) (where n is the number of data points; note that computing the cost, given a solution, takes 8(nk) time). We show that our algorithm compares favorably to existing algorithms -both theoretically and experimentally, thus providing state-of-the-art performance in both theory and practice.", "sections": [{"heading": "Introduction", "text": "We design improved algorithms for Euclidean k-means in the streaming model. In the k-means problem, we are given a set of n points in space. Our goal is to select k points in this space to designate asfacilities (sometimes called centers or means); the overall cost of the solution is the sum of the squared distances from each point to its nearest facility. The goal is to minimize this cost; unfortunately the problem is NP-Hard to optimize, although both heuristic [21] and approximation algorithm techniques [20,25,7] exist. In the streaming model, we require that the point set be read sequentially, and that our algorithm stores very few points at any given time. Many problems which are easy to solve in the standard batch-processing model require more complex techniques in the streaming model (a survey of streaming results is available [3]); nonetheless there are a number of existing streaming approximations for Euclidean k-means. We present a new algorithm for the problem based on [9] with several significant improvements; we are able to prove a faster worst-case running time and a better approximation factor. In addition, we compare our algorithm empirically with the previous state-of-the-art results of [2] and [4] on publicly available large data sets. Our algorithm outperforms them both.\nThe notion of clustering has widespread applicability, such as in data mining, pattern recognition, compression, and machine learning. The k-means objective is one of the most popular formalisms, and in particular Lloyd's algorithm [21] has significant usage [5,7,19,22,23,25,27,28]. Many of the applications for k-means have experienced a large growth in data that has overtaken the amount of memory typically available to a computer. This is expressed in the streaming model, where an algorithm must make one (or very few) passes through the data, reflecting cases where random access to the data is unavailable, such as a very large file on\u2022 a hard disk. Note that the data size, despite being large, is still finite.\nOur algorithm is based on the recent work of [9]. They \"guess\" the cost of the optimum, then run the online facility location algorithm of [24] until either the total cost of the solution exceeds a constant times the guess or the total number of facilities exceeds some computed value 1\\,. They then declare the end of a phase, increase the guess, consolidate the facilities via matching, and continue with the next point. When the stream has been exhausted, the algorithm has some I\\, facilities, which are then consolidated down to k. They then run a ball k-means step (similar to [25]) by maintaining samples of the points assigned to each facility and moving the facilities to the centers of mass of these samples. The algorithm uses O(k logn) memory, runs in O(nk logon) time, and obtains an 0(1) worst-case approximation. Provided that the original data set was o--separable (see section 1.2 for the definition), they use ball k-means to improve the approximation factor to 1 + 0(0-2 ).\nFrom a practical standpoint, the main issue with [9] is that the constants hidden in the asymptotic notation are quite large. The approximation factor is in the hundreds, and the 0 (k log n) memory requirement has sufficiently high constants that there are actually more than n facilities for many of the data sets analyzed in previous papers. Further, these constants are encoded into the algorithm itself, making it difficult to argue that the performance should improve for non-worst-case inputs.", "publication_ref": ["b19", "b18", "b23", "b5", "b1", "b7", "b0", "b2", "b19", "b3", "b5", "b17", "b20", "b21", "b23", "b25", "b26", "b7", "b22", "b23", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Our Contributions", "text": "We substantially simplify the algorithm of [9]. We improve the manner by which the algorithm determines better facility cost as the stream is processed, removing unnecessary checks and allowing the user to parametrize what remains. We show that our changes result in a better approximation guarantee than the previous work. We also develop a variant that computes a solution in o(nk) and show experimentally that both algorithms outperform previous techniques.\nWe remove the end-of-phase condition based on the total cost, ending phases only when the number of facilities exceeds 1\\,. While we require I\\, E n(k log n), we do not require any particular constants in the expression (in fact we will use I\\, == k log n in our experiments). We also simplify the transition between phases, observing that it's quite simple to bound the number of phases by log 0 PT (where OPT is the optimum k-means cost), and that in practice this number of phases is usually quite a bit less than n.\nWe show that despite our modifications, the worst case approximation factor is still constant. Our proof is based on a much tighter bound on the cost incurred per phase, along with a more flexible definition of the \"critical phase\" by which the algorithm should terminate. Our proofs establish that the algorithm converges for any I\\, > k; of course, there are inherent tradeoffs between I\\, and the approximation bound. For appropriately chosen constants our approximation factor will be roughly 17, substantially less than the factor claimed in [9] prior to the ball k-means step.\nIn addition, we apply approximate nearest-neighbor algorithms to compute the facility assignment of each point. The running time of our algorithm is dominated by repeated nearest-neighbor calculations, and an appropriate technique can change our running time from 8 (nk log n) to 8 (n(log k + loglogn)), an improvement for most values of k. Of course, this hurts our accuracy somewhat, but we are able to show that we take only a constant-factor loss in approximation. Note that our final running time is actually faster than the 8 (nk) time needed to compute the k-means cost of a given set of facilities! In addition to our theoretical improvements, we perform a number of empirical tests using realistic data. This allows us to compare our algorithm to previous [4,2] streaming k-means results.", "publication_ref": ["b7", "b7", "b2", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Previous Work", "text": "A simple local search heuristic for the k-means problem was proposed in 1957 by Lloyd [21]. The algorithm begins with k arbitrarily chosen points as facilities. At each stage, it allocates the points into clusters (each point assigned to closest facility) and then computes the center of mass for each cluster. These become the new facilities for the next phase, and the process repeats until it is stable. Unfortunately, Lloyd's algorithm has no provable approximation bound, and arbitrarily bad examples exist. Furthermore, the worst-case running time is exponential [29]. Despite these drawbacks, Lloyd's algorithm (frequently known simply as k-means) remains common in practice.\nThe best polynomial-time approximation for k-means is by Kanungo, Mount, Netanyahu, Piatko, Silverman, and Wu [20]. Their algorithm uses local search (similar to the k-median algorithm of [8]), and is a 9 +c approximation. However, Lloyd's observed runtime is superior, and this is a high priority for real applications.\nOstrovsky, Rabani, Schulman and Swamy [25] observed that the value of k is typically selected such that the data is \"well-clusterable\" rather than being arbitrary. They defined the notion of (J\"separability, where the input to k-means is said to be (J\"-separable if reducing the number of facilities from k to k -1 would incr~ase the cost of the optimum solution by a factor ;2' They designed an algorithm with approximation ratio 1 + O((J\"2). Subsequently, Arthur and Vassilvitskii [7] showed that the same procedure produces an O(log k) approximation for arbitrary instances of k-means.\nThere are two basic approaches to the streaming version of the k-means problem. Our approach is based on solving k-means as we go (thus at each point in the algorithm, our memory contains a current set of facilities). This type of approach was pioneered in 2000 by Guha, Mishra, Motwani, and O'Callaghan [17]. Their algorithm reads the data in blocks, clustering each using some nonstreaming approximation, and then gradually merges these blocks when enough of them arrive. An improved result for k-median was given by Charikar, O'Callaghan, and Panigrahy in 2003 [11], producing an 0 (1) approximation using 0 (k log2 n) space. Their work was based on guessing a lower bound on the optimum k-median cost and running O(log n) parallel versions of the online facility location algorithm of Meyerson [24] with facility cost based on the guessed lower bound. When these parallel calls exceeded the approximation bounds, they would be terminated and the guessed lower bound on the optimum k-median cost would increase. The recent paper of Braverman, Meyerson, Ostrovsky, Roytman, Shindler, and Tagiku [9] extended the result of [11] to k-means and improved the space bound to 0 (k log n) by proving high-probability bounds on the performance of online facility location. This result also added a ball k-means step (as in [25]) to substantially improve the approximation factor under the assumption that the original data was (J\"-separable.\nAnother recent result for streaming k-means, due to Ailon, Jaiswal, and Monteleoni [4], is based on a divide and conquer approach, similar to the k-median algorithm of Guha, Meyerson Mishra, Motwani, and O'Callaghan [16]. It uses the result of Arthur and Vassilvitskii [7] as a subroutine, finding 3k log k centers for each block. Their experiment showed that this algorithm is an improvement over an online variant of Lloyd's algorithm and was comparable to the batch version of Lloyd's.\nThe other approach to streaming k-means is based on coresets: selecting a weighted subset of the original input points such that any k-means solution on the subset has roughly the same cost as on the original point set. At any point in the algorithm, the memory should contain a weighted representative sample of the points. This approach was first used in a non-streaming setting for a variety of clustering problems by Badoiu, Har-Peled, and Indyk [10], and in the streaming setting by Har-Peled and Mazumdar [18]; the time and memory bounds were subsequently improved through a series of papers [14,13] with the current best theoretical bounds by Chen [12]. A practical implementation of the coreset paradigm is due to Ackermann, Lammersen, Martens, Raupach, Sohler, and Swierkot [2]. Their approach was shown empirically to be fast and accurate on a variety of benchmarks.", "publication_ref": ["b19", "b27", "b18", "b6", "b23", "b5", "b15", "b9", "b22", "b7", "b9", "b23", "b2", "b14", "b5", "b8", "b16", "b12", "b11", "b10", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm and Theory", "text": "Both our algorithm and that of [9] are based on the online facility location algorithm of [24]. For the facility location problem, the number of clusters is not part of the input (as it is for k-means), but rather a facility cost is given; an algorithm to solve this problem may have as many clusters as it desires in its output, simply by denoting some point as a facility. The solution cost is then the sum of the resulting k-means cost (\"service cost\") and the total paid for facilities.\nOur algorithm runs the online facility location algorithm of [24] with a small facility cost until we have more than~E e(k log n) facilities. It then increases the facility cost, re-evaluates the current facilities, and continues with the stream. This repeats until the entire stream is read. The details of the algorithm are given as Algorithm 1.\nThe major differences between our algorithm and that of [9] are as follows. We ignore the overall service cost in determining when to end a phase and raise our facility cost f. Further, the number of facilities which must open to end a phase can be any~E e(k log n), the constants do not depend directly on the competitive ratio of online facility location (as they did in [9]). Finally, we omit the somewhat complicated end-of-phase analysis of [9], which used matching to guarantee that the' number of facilities decreased substantially with each phase and allowed bounding the number of phases by kl~gn' We observe that our number of phases will be bounded by logj3 OPT; while this is not technically bounded in terms of n, in practice this term should be smaller than the linear number of phases implied in previous work. if probability fJ j f event occurs then Perform ball k-means (as per [9]) on the resulting set of clusters We will give a theoretical analysis of our modified algorithm to obtain a constant approximation bound. Our constant is substantially smaller than those implicit in [9], with most of the loss occurring in the final non-streaming k-means algorithm to consolidate~means down to k. The analysis will follow from the theorems stated below; proofs of these theorems are deferred to the appendix.\nTheorem 1. Suppose that our algorithm completes the data stream when the facility cost is f. Then the overall solution prior to the final re-clustering has expected service cost at most~~, and the probability of being within 1 + E of the expected service cost is at least 1 -pol~(n) . Theorem 2. With probability at least 1 -pol~(n)' the algorithm will either halt with 1 :::; e(~*){3, where C* is the optimum k-means cost, or it will halt within one phase of exceeding this value. Furthermore, for large values of~and {3, the hidden constant in 8 (C*) approaches 4.\nNote that while the worst-case bound of roughly 4 proven here may not seem particularly strong, 'unlike the previous work of [9], the worst-case performance is not directly encoded into the algorithm. In practice, we would expect the performance of online facility location to be substantially better than worst-case (in fact, if the ordering of points in the stream is non-adversarial there is a proof to this effect in [24]); in addition the assumption was made that distances add (i.e. triangle inequality is tight) which will not be true in practice (especially of points in low-dimensional space). We also assumed that using more than k facilities does not substantially help the optimum service cost (also unlikely to be true for real data). Combining these, it would be unsurprising if our service cost was actually better than optimum at the end of the data stream (of course, we used many more facilities than optimum, so it is not precisely a fair comparison). The following theorem summarizes the worst-case performance of the algorithm; its proof is direct from Theorems 1 and 2. Theorem 3. The cost of our algorithm's final~-mean solution is at most O(C*), where C* is the cost of the optimum k-means solution, with probability 1 -pol~(n)' If~is a large constant times k log nand j3 > 2 is fairly large, then the cost of our algorithm's solution will approach C* :~21; the extra j3 factor is due to \"overshooting\" the bestfacility cost f.\nWe note that if we run the streaming part of the algorithm NI times in parallel, we can take the solution with the smallest final facility cost. This improves the approximation factor to roughly\n4;31+(1/ M) hi h h 4' h l' . Of . . b ' 11 . h ,B-1\n,w c approac es In t e lIDlt. course, IncreasIng~can su stantla y Increase t e memory requirement and increasing NI can increase both memory and running time requirements.\nWhen the algorithm terminates, we have a set of~weighted means which we must reduce to k means. A theoretically sound approach involves mapping these means back to randomly selected points from the original set (these can be maintained in a streaming manner) and then approximating k-means on~points using a non-streaming algorithm. The overall approximation ratio will be twice the ratio established by our algorithm (we lose a factor of two by mapping back to the original points) plus the approximation ratio for the non-streaming algorithm. If we use the algorithm of [20] along with a large~, we will get an approximation factor of twice 4 plus 9+c for roughly 17. Ball k-means can then reduce the approximation factor to 1 + 0 ((J2) if the inputs were (J -separable (as in [25] and [9]; the hidden constant will be reduced by our more accurate algorithm).", "publication_ref": ["b7", "b22", "b22", "b7", "b7", "b7", "b7", "b7", "b7", "b22", "b18", "b23", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Approximate Nearest Neighbor", "text": "The most time-consuming step in our algorithm is measuring 6 in lines 5 and 17. This requires as many as~distance computations; there are a number of results enabling fast computation of approximate nearest neighbors and applying these results will improve our running time. If we can assume that errors in nearest neighbor computation are independent from one point to the next (and that the expected result is good), our analysis from the previous section applies. Unfortunately, many of the algorithms construct a random data structure to store the facilities, then use this structure to resolve all queries; this type of approach implies that errors are not independent from one query to the next. Nonetheless we can obtain a constant approximation for sufficiently large choices of (3.\nFor our empirical result, we will use a very simple approximate nearest-neighbor algorithm based on random projection. This has reasonable performance in expectation, but is not independent from one step to the next. While the theoretical results from this particular approach are not very strong, it works very well in our experiments. For this implementation, a vector w is created, with each of the d dimensions space being chosen independently and uniformly at random from [0,1). We store our facilities sorted by their inner product with w. When a new point x arrives, instead of taking O(~) to determine its (exact) nearest neighbor, we instead use O(log~) to find the two facilities that x . w is between. We determine the (exact) closer of these two facilities; this determines the value of 6 in lines 5 and 17 and the \"closest\" facility in lines 9 and 21.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Theorem 4. If our approximate nearest neighbor computation finds a facility with distance at most v times the distance to the closest facility in expectation, then the approximation ratio increase by a constantfactor.", "text": "We defer explanation of how we form the stronger theoretical result to the appendix.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Empirical Evaluation", "text": "A comparison of algorithms on real data sets gives a great deal of insight as to their relative performance. Real data is not worst-case, implying that neither the asymptotic performance or runningtime bounds claimed in theoretical results are necessarily tight. Of course, empirical evaluation depends heavily on the data sets selected for the experiments.\nWe selected data sets which have been used previously to demonstrate streaming algorithms. A number of the data sets analyzed in previous work were not particularly large, probably so that batch-processing algorithms would terminate quickly on those inputs. The main motivation for streaming is very large data sets, so we are more interested in sets that might be difficult to fit in a main memory and focused on the largest examples. We looked to [2], and used the two biggest data sets they considered. These were the BigCross dataset 1 and the Census1990 dataset 2. All the other data sets in [2,4] were either subsets of these or were well under a half million points.\nA necessary input for each of these algorithms is the desired number of clusters. Previous work chose k seemingly arbitrarily; typical values were of the form {5, 10, 15,20, 25}. While this input provides a well-defined geometry problem, it fails to capture any information about how k-means is IThe BigCross dataset is 11,620,300 points in 57-dimensional space; it is available from [1] 2The Census1990 dataset is 2,458,285 points in 68 dimensions; it is available from [15] used in practice and need not lead to separable data. Instead, we want to select k such that the best k-means solution is much cheaper than the best (k -I)-means solution. Since k-means is NP-Hard, we cannot solve large instances to optimality. For the Census dataset we ran several iterations of the algorithm of [25] for each of many values of k. We took the best observed cost for each value of k, and found the four values of k minimizing the ratio of k-means cost to (k -I)-means cost.\nThis was not possible for the larger BigCross dataset. Instead, we ran a modified version of our algorithm; at the end of a phase, it adjusts the facility cost and restarts the stream. This avoids the problem of compounding the approximation factor at the end of a phase. As with Census, we ran this for consecutive values of k and chose the best ratios of observed values; we chose two, rather than four, so that we could finish our experiments in a reasonable amount of time. Our approach to selecting k is closer to what's done in practice, and is more likely to yield meaningful results.\nWe do not compare to the algorithm of [9]. First, the memory is not configurable, making it not fit into the common baselme that we will define shortly. Second, the memory requirements and runtime, while asymptotically nice, have large leading constants that cause it to be impracticaL In fact, it was an attempt to implement this algorithm that initially motivated the work on this paper.", "publication_ref": ["b0", "b0", "b2", "b13", "b23", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Implementation Discussion", "text": "The divide and conquer (\"D&C\") algorithm [4] can use its available memory in two possible ways. First, it can use the entire amount to read from the stream, writing the results of computing their 3k log k means to disk; when the stream is exhausted, this file is treated as a stream, until an iteration produces a file that fits entirely into main memory. Alternately, the available memory could be partitioned into layers; the first layer would be filled by reading from the stream, and the weighted facilities produced would be stored in the second. When any layer is full, it can be clustered and the result placed in a higher layer, replacing the use of files and disk. Upon completion of the stream, any remaining points are gathered and clustered to produce k final means. When larger amounts of memory are available, the latter method is preferred. With smaller amounts, however, this isn't always possible, and when it is possible, it can produce worse actual running times than a disk-based approach. As our goal is to judge streaming algorithms under low memory conditions, we used the first approach, which is more fitting to such a constraint. Each algorithm 3 was programmed in C/C++, compiled with g++, and run under Ubuntu Linux (10.04 LTS) on HP Pavilion p6520fDesktop PC, with an AMD Athlon II X4 635 Processor running at 2.9 GhZ and with 6 GB main memory (although nowhere near the entirety of this was used by any algorithm). For StreamKM++, the authors' implementation [2], also in C, was used instead. With all algorithms, the reported cost is determined by taking the resulting k facilities and computing the k-means cost across the entire dataset. The time to compute this cost is not included in the reported running times of the algorithms. Each test case was run 10 times and the average costs and running times were reported.", "publication_ref": ["b2", "b1", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Design", "text": "Our goal is to compare the algorithms at a common basepoint. Instead of just comparing for the same dataset and cluster count, we further constrained each to use the same amount of memory (in terms of number of points stored in random access). The memory constraints were chosen to reflect the usage of small amounts of memory that are close to the algorithms' designers' specifications, where possible. Ailon et al [4] suggest -v:n:k memory for the batch process; this memory availability is marked in the charts by an asterisk. The suggestion from [2] for a coreset of size 200k was not run for all algorithms, as the amount of memory necessary for computing a coreset of this size is much larger than the other cases, and our goal is to compare the algorithms at a small memory limit. This does produce a drop in solution quality compared to running the algorithm at their suggested parameters, although their approach remains competitive. Finally, our algorithm suggests memory of~= k log n or a small constant times the same.\nIn each case, the memory constraint dictates the parameters; for the divide and conquer algorithm, this is simply the batch size. The coreset size is also dictated by the available memory. Our algorithm is a little more parametrizable; when M memory is available, we allowed~= M /5 and each facility to have four samples.  We see that our algorithms are much faster than the D&C algorithm, while having a comparable (and often better) solution quality. We find that we compare well to StreamKM++ in average results, with a closer standard deviation and a better sketch of the data produced. Furthermore, our algorithm stands to gain the most by improved solutions to batch k-means, due to the better representative sample present after the stream is processed.\nThe prohibitively high running time of the divide-and-conquer algorithm [4] is due to the many repeated instances of running their k-means# algorithm on each batch of the given size. For sufficiently large memory, this is not problematic, as very few batches will need this treatment. Unfortunately, with very small locally available memory, there will be an immense amount of repeated calls, and the overall running time will suffer greatly. In particular, the observed running time was much worse than the other approaches. For the Census dataset, k = 12 case, for example, the slowest run of our algorithm (20 minutes) and the fastest run of the D&C algorithm (125 minutes) occurred at the same case. It is because of this discrepancy that we present the chart of algorithm running times as a log-plot. Furthermore, due to the prohibitively high running time on the smaller data set, we omitted the divide-and-conquer algorithm for the experiment with the larger set.\nThe decline in accuracy for StreamKM++ at very low memory can be partially explained by the 8(k 2 1og 8 n) points' worth of memory needed for a strong guarantee in previous theory work [12].\nHowever, the fact that the algorithm is able to achieve a good approximation in practice while using far less than that amount of memory suggests that improved provable bounds for coreset algorithms may be on the horizon. We should note that the performance of the algorithm declines sharply as the memory difference with the authors' specification grows, but gains accuracy as the memory grows.\nAll three algorithms can be described as computing a weighted sketch of the data, and then solving k-means on that sketch. The final approximation ratios can be described as a(l + E) where a is the loss from the final batch algorithm. The coreset E is a direct function of the memory allowed to the algorithm, and can be made arbitrarily small. However, the memory needed to provably reduce E to a small constant is quite substantial, and while StreamKM++ does produce a good resulting clustering, it is not immediately clear the the discovery of better batch k-means algorithms would improve their solution quality. Our algorithm's E represents the ratio of the cost of our f1:-mean solution to the cost of the optimum k-means solution. The provable value is a large constant, but since~is much larger than k, we would expect better performance in practice, and we observe this effect in our experiments.\nFor our algorithm, the observed value of 1+E has been typically between 1 and 3, whereas the D&C approach did not yield one better than 24, and was high (low thousands) for the very low memory conditions. The coreset algorithm was the worst, with even the best values in the mid ten figures (tens to hundreds of billions). The low ratio for our algorithm also suggests that our~facilities are a good sketch of the overall data, and thus our observed accuracy can be expected to improve as more accurate batch k-means algorithms are discovered.", "publication_ref": ["b2", "b0", "b2", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We are grateful to Christian Sohler's research group for providing their code for the StreamKM++ algorithm. We also thank Jennifer Wortman Vaughan, Thomas G. Dietterich, Daniel Sheldon, Andrea Vattani, and Christian Sohler for helpful feedback on drafts of this paper. This work was done while all the authors were at UCLA; at that time, Adam Meyerson and Michael Shindler were partially supported by NSF CIF Grant CCF-1016540.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "StreamKM++: A clustering algorithms for data streams", "journal": "", "year": "2010", "authors": "Marcel R Ackermann; Christian Lammersen; Marcus Martens; Christoph Raupach; Christian Sohler; Kamil Swierkot"}, {"ref_id": "b1", "title": "Data Streams: Models and Algorithms", "journal": "Springer", "year": "2007", "authors": ""}, {"ref_id": "b2", "title": "Streaming k-means approximation", "journal": "", "year": "2009", "authors": "Nir Ailon; Ragesh Jaiswal; Claire Monteleoni"}, {"ref_id": "b3", "title": "An efficient k-means clustering algorithm", "journal": "", "year": "1998", "authors": "Khaled Alsabti; Sanjay Ranka; Vineet Singh"}, {"ref_id": "b4", "title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "journal": "Communications of the ACM", "year": "2008-01", "authors": "Alexandr Andoni; Piotr Indyk"}, {"ref_id": "b5", "title": "k-means++: The Advantages of Careful Seeding", "journal": "", "year": "2007", "authors": "David Arthur; Sergei Vassilvitskii"}, {"ref_id": "b6", "title": "Local search heuristic for k-median and facility location problems", "journal": "", "year": "2001", "authors": "Vijay Arya; Naveen Garg; Rohit Khandekar; Adam Meyerson; Kamesh Munagala; Vinayaka Pandit"}, {"ref_id": "b7", "title": "Streaming k-means on Well-Clusterable Data", "journal": "", "year": "", "authors": "Vladimir Braverman; Adam Meyerson; Rafail Ostrovsky; Alan Roytman; Michael Shindler; Brian Tagiku"}, {"ref_id": "b8", "title": "Approximate clustering via core-sets", "journal": "", "year": "2002", "authors": "Mihai Badoiu"}, {"ref_id": "b9", "title": "Better streaming algorithms for clustering problems", "journal": "", "year": "2003", "authors": "Moses Charikar; O' Liadan; Rina Callaghan;  Panigrahy"}, {"ref_id": "b10", "title": "On coresets for k-median and k-means clustering in metric and euclidean spaces and their applications", "journal": "SIAM J. Comput", "year": "2009", "authors": "Ke Chen"}, {"ref_id": "b11", "title": "A PTAS for k-means clustering based on weak coresets", "journal": "", "year": "2007", "authors": "Dan Feldman; Morteza Monemizadeh; Christian Sohler"}, {"ref_id": "b12", "title": "Coresets in dynamic geometric data streams", "journal": "", "year": "2005", "authors": "Gereon Frahling; Christian Sohler"}, {"ref_id": "b13", "title": "DCI machine learning repository", "journal": "", "year": "2010", "authors": "A Frank; A Asuncion"}, {"ref_id": "b14", "title": "Clustering data streams: Theory and practice", "journal": "", "year": "2003", "authors": "Sudipto Guha; Adam Meyerson; Nina Mishra; Rajeev Motwani; Liadan O' Callaghan"}, {"ref_id": "b15", "title": "Rajeev Motwani, and Liadan 0' Callaghan. Clustering data streams", "journal": "", "year": "2000", "authors": "Sudipto Guha; Nina Mishra"}, {"ref_id": "b16", "title": "On coresets for k-means and k-median clustering", "journal": "", "year": "2004", "authors": "Sariel Har; -Peled ; Soham Mazumdar"}, {"ref_id": "b17", "title": "Data clustering: a review", "journal": "ACM Computing Surveys", "year": "1999-09", "authors": "Anil Kumar Jain; Patrick Joseph Narasimha Murty;  Flynn"}, {"ref_id": "b18", "title": "A local search approximation algorithm for k-means clustering", "journal": "", "year": "2002", "authors": "Tapas Kanungo; David Mount; Nathan Netanyahu; Christine Piatko; Ruth Silverman; Angela Wu"}, {"ref_id": "b19", "title": "Least Squares Quantization in PCM", "journal": "", "year": "1982", "authors": "Stuart Lloyd"}, {"ref_id": "b20", "title": "Some methods for classification and analysis of multivariate observations", "journal": "", "year": "1967", "authors": "James Macqueen"}, {"ref_id": "b21", "title": "Quantizing for minimum distortion", "journal": "IEEE Transactions on Information Theory", "year": "1960", "authors": "Joel Max"}, {"ref_id": "b22", "title": "Online facility location", "journal": "", "year": "", "authors": "Adam Meyerson"}, {"ref_id": "b23", "title": "The Effectiveness of Lloyd-Type Methods for the k-Means Problem", "journal": "", "year": "2006", "authors": "Rafail Ostrovsky; Yuval Rabani; Leonard Schulman; Chaitanya Swamy"}, {"ref_id": "b24", "title": "Entropy based nearest neighbor search in high dimensions", "journal": "", "year": "2006", "authors": "Rina Panigrahy"}, {"ref_id": "b25", "title": "Accelerating exact k-means algorithms with geometric reasoning", "journal": "", "year": "1999", "authors": "Dan Pelleg; Andrew Moore"}, {"ref_id": "b26", "title": "Acceleration of k-means and related clustering problems", "journal": "", "year": "2002", "authors": "Steven 1 Phillips"}, {"ref_id": "b27", "title": "Andrea Vattani. k-means requires exponentially many iterations even in the plane", "journal": "Discrete Computational Geometry", "year": "2011-06", "authors": ""}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Algorithm 11Fast streaming k-means (data stream, k,~, (3) 1: Initialize f = Ij(k(l + logn)) and an empty set K 2: while some portion of the stream remains unread do 3: while IKI :::;~= 8(k log n) and some portion of the stream is unread do 4: Read the next point x from the stream 5: Measure 6 = minYEK d(x, y)2 6:", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "each x E K to the center-of-mass of its points 14: Let W x be the number of points assigned to x E K 15: Initialize K containing the first facility from K 16: for each x E K do 17: Measure fJ = min yEK d(x, y)2 18:if probability w x 6j f event occurs then batch k-means algorithm on weighted points K 25:", "figure_data": ""}, {"figure_label": "2520", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "2520 MemoryAvailableFigure2520Figure 1: Census Data, k=8, cost", "figure_data": ""}, {"figure_label": "35678", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 Figure 5 :Figure 6 Figure 7 :Figure 8 :35678Figure 3: Census Data, k= 12, cost", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "4;31+(1/ M) hi h h 4' h l' . Of . . b ' 11 . h ,B-1", "formula_coordinates": [5.0, 128.25, 138.61, 345.23, 10.27]}], "doi": ""}