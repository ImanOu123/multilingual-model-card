{"title": "Is a Knowledge-based Response Engaging?: An Analysis on Knowledge-Grounded Dialogue with Information Source Annotation", "authors": "Takashi Kodama; Hirokazu Kiyomaru; Yin Jou Huang; Taro Okahisa; Sadao Kurohashi", "pub_date": "", "abstract": "Currently, most knowledge-grounded dialogue response generation models focus on reflecting given external knowledge. However, even when conveying external knowledge, humans integrate their own knowledge, experiences, and opinions with external knowledge to make their utterances engaging. In this study, we analyze such human behavior by annotating the utterances in an existing knowledge-grounded dialogue corpus. Each entity in the corpus is annotated with its information source, either derived from external knowledge (database-derived) or the speaker's own knowledge, experiences, and opinions (speaker-derived). Our analysis shows that the presence of speaker-derived information in the utterance improves dialogue engagingness. We also confirm that responses generated by an existing model, which is trained to reflect the given knowledge, cannot include speakerderived information in responses as often as humans do.", "sections": [{"heading": "Introduction", "text": "More and more dialogue research has utilized external knowledge to enable dialogue systems to generate rich and informative responses (Ghazvininejad et al., 2018;Zhou et al., 2018;Moghe et al., 2018;Dinan et al., 2019;Zhao et al., 2020). The major focus of such research is in how to select appropriate external knowledge and reflect it accurately in the response (Kim et al., 2020;Zhan et al., 2021;Rashkin et al., 2021;Li et al., 2022).\nHowever, as shown in Figure 1 1 , a good speaker not only informs the dialogue partner of external knowledge but also incorporates his or her own knowledge, experiences, and opinions effectively, which makes the dialogue more engaging. The extent to which models specializing in reflecting 1 Examples of dialogues presented in this paper are originally in Japanese and were translated by the authors. ", "publication_ref": ["b8", "b0", "b2", "b12", "b10", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Released Year", "text": "The first of the worldwide hit movies about the pirates' struggle Review Figure 1: An example of Japanese Movie Recommendation Dialogue (Kodama et al., 2022). The table above the recommender's utterance indicates the external knowledge used in that utterance. The recommender incorporates not only database-derived information but also speaker-derived information.\ngiven external knowledge can achieve such an engaging behavior has not yet been explored quantitatively.\nIn this study, we first analyze how humans incorporate speaker-derived information by annotating the utterances in an existing knowledge-grounded dialogue corpus. Each entity in the utterances is annotated with its information source, either derived from external knowledge (database-derived) or the speaker's own knowledge, experiences, and opinions (speaker-derived). The analysis of the annotated dataset showed that engaging utterances contained more speaker-derived information.\nIn addition, we train a BART-based response generation model in a standard way, i.e., by minimizing perplexity, and investigate the extent to which it incorporates speaker-derived information. The result showed that the response generation model did not incorporate speaker-derived information into their utterances as often as humans do. This result implies that minimizing perplexity is insufficient to increase engagingness in knowledgegrounded response generation and suggests room for improvement in the training framework.", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}, {"heading": "Information Source Annotation", "text": "This section describes the annotation scheme for information sources and the annotation results.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Scheme", "text": "We annotate Japanese Movie Recommendation Dialogue (JMRD) (Kodama et al., 2022) with information sources 2 . JMRD is a human-to-human knowledge-grounded dialogue corpus in Japanese. A recommender recommends a movie to a seeker. Each utterance of the recommender is associated with movie information as external knowledge. Each piece of knowledge consists of a knowledge type (e.g., title) and the corresponding knowledge contents (e.g., \"Marvel's The Avengers\").\nIn this study, we extract entities from the recommender's utterances and annotate them with their information source. Entities are nouns, verbs, and adjectives and are extracted together with their modifiers to make it easier to grasp their meanings. Entities are extracted using Juman++ (Tolmachev et al., 2020), a widely-used Japanese morphological analyzer. Annotators classify the extracted entities into the following information source types: Database-derived: The entity is based on the external knowledge used in that utterance. Speaker-derived: The entity is based on the knowledge, experiences, and opinions that the recommender originally has about the recommended movie.\nOther: The entity does not fall under the above two types (e.g., greetings).\nAn annotation example is shown below.\n(1) Utterance: The action scenes (database) are spectacular (speaker) ! Used knowledge: Genre, Action\nWe recruited professional annotators, who are native Japanese speakers, to annotate these information source types. One annotator was assigned to each dialogue. After the annotation, another annotator double-checked the contents.", "publication_ref": ["b3", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Result", "text": "Table 1 shows the annotation statistics. While JMRD is a knowledge-grounded dialogue corpus and thus inherently contains many database-derived entities, it also contains about 60,000 speakerderived entities. This result verifies that humans   incorporate their own knowledge, experiences, and opinions into their utterances, even in dialogues to convey external knowledge.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Analysis of Human Utterances", "text": "We analyze human utterances at the dialogue level and utterance level.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dialogue-level Analysis", "text": "4,328 dialogues in JMRD have post-task questionnaires on 5-point Likert scale (5 is the best.) We regard the rating of the question to the seekers (i.e., Did you enjoy the dialogue?) as dialogue engagingness and analyze the relationship between this and the ratio of each information source label.\nFigure 2 shows that dialogues with high engagingness scores tend to have more speaker-derived entities (or less database-derived) than those with low engagingness scores. When constructing JMRD, recommenders were given a certain amount of external knowledge and asked to use that knowledge to respond. However, recommenders highly rated by their dialogue partners incorporated not only the given external knowledge but also speakerderived information to some extent in their dialogues.  ", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Utterance-level Analysis", "text": "We conduct the utterance-level evaluation via crowdsourcing. We randomly extract 500 responses along with their contexts (= 4 previous utterances) from the test set. For each utterance, workers rate utterance engagingness (i.e., Would you like to talk to the person who made this response?) on a 5-point Likert scale, with 5 being the best. Three workers evaluate each utterance, and the scores are averaged.\nThe average score for utterances with speakerderived entities was 3.31, while those without speaker-derived entities was 3.07. Student's t-test with p = 0.05 revealed a statistically significant difference between these scores.\nFurthermore, Figure 3 shows the relationship between utterance engagingness and the ratio of each information source label. This figure shows that utterances with high scores tend to have more speaker-derived entities. This trend is consistent with that of the dialogue engagingness.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Does subjective knowledge contribute to engagingness?", "text": "The knowledge type used in JMRD can be divided into subjective knowledge (review) and objective knowledge (title, etc.). Reviews are the opinions of individuals who have watched movies and have similar characteristics to speaker-derived information. We then examine whether there is a difference in engagingness between utterances using subjective and objective knowledge. The average engagingness scores were 3.32 and 3.16 3 , respectively, and Student's t-test with p = 0.05 revealed no statistically significant difference. The above analysis demonstrates that information obtained from the speaker's own experience is an important factor in utterance engagingness.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Analysis of System Utterances", "text": "We investigate the distribution of information source labels in the responses of the model trained on the knowledge-grounded dialogue dataset. First, we train a Response Generator ( \u00a74.1) with the dialogue contexts and external knowledge as input and responses as output. Next, an Information Source Classifier ( \u00a74.2) is trained with responses and external knowledge as input and information source labels as output. Then, the Information Source Classifier infers the information source labels for the system responses generated by the Response Generator. Finally, we analyze the distribution of inferred information source labels.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Response Generator", "text": "We use a BART large (Lewis et al., 2020) model as a backbone. 4 The input to the model is formed as follows:\n[CLS]u t\u22124 [SEP ]u t\u22123 [SEP ]u t\u22122 [SEP ] u t\u22121 [SEP ][CLS K ]kt 1 [SEP ]kc 1 [SEP ]... [CLS K ]kt M [SEP ]kc M [SEP ], (1)\nwhere t is the dialogue turn, u t is the t-th response, and kt i and kc i (1 <= i <= M ) are the knowledge type and knowledge content associated with the target response, respectively (M is the maximum number of knowledge associated with u t .) [CLS K ] is a special token. We feed the gold knowledge into the model to focus on how knowledge is reflected in the responses. The model learns to minimize perplexity in generating u t .\nWe evaluated the quality of response generation with the SacreBLEU (Post, 2018). BLEU-1/2/3/4 scored high, 81.1/73.5/71.0/69.9. This result is reasonable because the gold knowledge was given.", "publication_ref": ["b4", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Information Source Classifier", "text": "We fine-tune a RoBERTa large (Liu et al., 2019) model. 5 The Information Source Classifier performs a sequence labeling task to estimate BIO 6 engagingness ...", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "Context", "text": "Recommender: This movie is an animation movie released in 2015.\nSeeker: I see.    labels of the information source. The input to the model is formed as follows:\n[CLS]u t [SEP ][CLS K ]kt 1 [SEP ]kc 1 [SEP ]... [CLS K ]kt M [SEP ]kc M [SEP ] (2)\nTable 3 shows precision, recall, and F1 scores for each label and micro average scores across all labels. The micro average F1 score was 90.50, which is accurate enough for the further analysis.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Analysis for Inferred Labels", "text": "The information source labels for system responses are inferred using the classifier trained in Section 4.2. Table 4 shows distributions of information source labels for human and system responses. For a fair comparison, the human responses are also given labels inferred by the classifier (denoted as Human (pred)), although they have gold labels (denoted as Human (gold)). Human (gold) and Human (pred) have similar distributions, indicating that the accuracy of the classifier is sufficiently high. For System (pred), the percentage of database-derived labels increased significantly (66.75%\u219285.48%) and that  of speaker-derived information decreased significantly (27.49%\u219210.66%). This result shows that the response generation model, trained in a standard way, was not able to use speaker-derived information as often as humans do.\nTable 2 shows an example of human and system responses along with the engagingness scores. The system was able to reflect given knowledge in the response appropriately but did not incorporate additional speaker-derived information, such as the information two voice actors also work as singers.\nFor further analysis, we investigated the average ratios of speaker-derived information by knowledge type used. Table 5 shows the result. Significant drops were observed for reviews (31.42%\u21926.32%) and plots (13.68%\u21922.32%). This is probably because reviews and plots are relatively long and informative external knowledge, so the system judged there was no need to incorporate additional speaker-derived information.\nCombined with our observation that speakerderived information improves engagingness, the current model is likely to have lower engagingness due to its inability to effectively incorporate speaker-derived information. Such an ability is hardly learned by simply optimizing a model to reduce the perplexity of response generation, suggesting the need for a novel learning framework.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_6", "tab_4", "tab_8"]}, {"heading": "Conclusion", "text": "We analyzed the distribution of speaker-derived information in human and system responses in the knowledge-grounded dialogue. The analysis showed that the use of speaker-derived information, as well as external knowledge, made responses more engaging. We also confirmed that the response generation model trained in a standard way generated less speaker-derived information than humans.\nIt is difficult to make good use of speaker-derived information by simply minimizing the perplexity of the model because a wide variety of speakerderived information appears in each dialogue. We hope our published annotated corpus becomes a good launch pad for tackling this issue.   ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Implementation Details", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2.1 Response Generator", "text": "Dialogue contexts, knowledge (knowledge types and contents), and target responses are truncated to the maximum input length of 256, 256, and 128, respectively. The model is trained for up to 50 epochs with a batch size of 512 and 0.5 gradient clipping. We apply early stopping if no improvement of the loss for the development set is observed for three consecutive epochs. We use AdamW optimizer (Loshchilov and Hutter, 2019) with \u03b2 1 = 0.9, \u03b2 2 = 0.999, = 1e \u2212 8 and an initial learning rate = 1e \u2212 5. We use an inverse square root learning rate scheduler with the first 1,000 steps allocated for warmup. During decoding, we use the beam search with a beam size of 3.", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "A.2.2 Information Source Classifier", "text": "Target responses and knowledge (knowledge types and contents) are truncated to the maximum input length of 128 and 384, respectively. The model is trained for up to 20 epochs with a batch size of 64 and 0.5 gradient clipping. We apply early stopping if no improvement of the f1 score for the development set is observed for three consecutive epochs. We use AdamW optimizer (Loshchilov and Hutter, 2019) with \u03b2 1 = 0.9, \u03b2 2 = 0.999, = 1e\u22128 and an initial learning rate = 1e\u22125. We use an inverse square root learning rate scheduler with the first 1,000 steps allocated for warmup.", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We would like to thank anonymous reviewers for their insightful comments. This work was supported by NII CRIS collaborative research program operated by NII CRIS and LINE Corporation. This work was also supported by JST, CREST Grant Number JPMJCR20D2, Japan and JSPS KAK-ENHI Grant Number JP22J15317.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Wizard of wikipedia: Knowledge-powered conversational agents", "journal": "", "year": "2019", "authors": "Emily Dinan; Stephen Roller; Kurt Shuster; Angela Fan; Michael Auli; Jason Weston"}, {"ref_id": "b1", "title": "Wen tau Yih, and Michel Galley. 2018. A knowledgegrounded neural conversation model", "journal": "", "year": "", "authors": "Marjan Ghazvininejad; Chris Brockett; Ming-Wei Chang; William B Dolan; Jianfeng Gao"}, {"ref_id": "b2", "title": "Sequential latent knowledge selection for knowledge-grounded dialogue", "journal": "", "year": "2020", "authors": "Byeongchang Kim; Jaewoo Ahn; Gunhee Kim"}, {"ref_id": "b3", "title": "Construction of hierarchical structured knowledge-based recommendation dialogue dataset and dialogue system", "journal": "", "year": "2022", "authors": "Takashi Kodama; Ribeka Tanaka; Sadao Kurohashi"}, {"ref_id": "b4", "title": "BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal ; Abdelrahman Mohamed; Omer Levy; Veselin Stoyanov; Luke Zettlemoyer"}, {"ref_id": "b5", "title": "Enhancing knowledge selection for grounded dialogues via document semantic graphs", "journal": "", "year": "2022", "authors": "Sha Li; Mahdi Namazifar; Di Jin; Mohit Bansal; Heng Ji; Yang Liu; Dilek Hakkani-Tur"}, {"ref_id": "b6", "title": "", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b7", "title": "Decoupled weight decay regularization", "journal": "", "year": "2019", "authors": "Ilya Loshchilov; Frank Hutter"}, {"ref_id": "b8", "title": "Towards exploiting background knowledge for building conversation systems", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Nikita Moghe; Siddhartha Arora; Suman Banerjee; Mitesh M Khapra"}, {"ref_id": "b9", "title": "A call for clarity in reporting BLEU scores", "journal": "", "year": "2018", "authors": "Matt Post"}, {"ref_id": "b10", "title": "Increasing faithfulness in knowledge-grounded dialogue with controllable features", "journal": "Long Papers", "year": "2021", "authors": "Hannah Rashkin; David Reitter; Gaurav Singh Tomar; Dipanjan Das"}, {"ref_id": "b11", "title": "Design and structure of the Ju-man++ morphological analyzer toolkit", "journal": "Journal of Natural Language Processing", "year": "2020", "authors": "Arseny Tolmachev; Daisuke Kawahara; Sadao Kurohashi"}, {"ref_id": "b12", "title": "Augmenting knowledge-grounded conversations with sequential knowledge transition", "journal": "", "year": "2021", "authors": "Hainan Haolan Zhan; Hongshen Zhang; Zhuoye Chen; Yongjun Ding; Yanyan Bao;  Lan"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "What is the highlight of this movie? Highlight? This movie stands out because even though it was released in 2003, it doesn't feel outdated when you watch it now. The movie centers around a pirate war, and what I really find enjoyable is how the pirates bond and strengthen their relationships while facing betrayal. Seeker Recommender 2003", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Relationship between dialogue engagingness and ratio of each information source label.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Relationship between utterance engagingness and ratio of each information source label.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Statistics of the information source annotation. R indicates recommender.", "figure_data": "8.626.987.87.978.1411.9513.6815.1920.1922.4379.4479.3477.0171.8369.43"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Knowledge {director, Takahiko Kyogoku}, {cast, Emi Nitta}, {cast, Yoshino Nanjo}    ", "figure_data": "ResponseHuman: The director is Takahiko Kyogoku, and the voice actors are Emi Nitta and Yoshino Nanjo. These two are also singers. System: The director is Takahiko Kyogoku. The voice actors are Emi Nitta and Yoshino Nanjo.4.00 2.33"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "An example of the human and system response. The blue and red parts refer to database-derived and speaker-derived information, respectively.", "figure_data": "Prec.Rec.F1database-derived 94.92 95.61 95.27 speaker-derived 80.88 84.39 82.60 other 82.93 64.15 72.34micro avg.90.52 90.48 90.50"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Results of the sequence labeling by Information Source Classifier.", "figure_data": "Dist. (%)Human (gold) Human (pred) System (pred)database-derived speaker-derived other66.22 26.33 7.4566.75 27.49 5.7785.48 10.66 3.86"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Distributions of information source labels for human and system responses.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Average ratios of speaker-derived labels per knowledge type used.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Xueliang Zhao, Wei Wu, Can Xu, Chongyang Tao, Dongyan Zhao, and Rui Yan. 2020. Knowledgegrounded dialogue generation with pre-trained language models.In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3377-3390, Online. Association for Computational Linguistics.", "figure_data": "Kangyan Zhou, Shrimai Prabhumoye, and Alan W Black. 2018. A dataset for document grounded conversations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 708-713, Brussels, Bel-gium. Association for Computational Linguistics."}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "and 7 show examples of the dialogue and knowledge in JMRD.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "[CLS]u t\u22124 [SEP ]u t\u22123 [SEP ]u t\u22122 [SEP ] u t\u22121 [SEP ][CLS K ]kt 1 [SEP ]kc 1 [SEP ]... [CLS K ]kt M [SEP ]kc M [SEP ], (1)", "formula_coordinates": [3.0, 316.11, 394.68, 208.32, 47.56]}, {"formula_id": "formula_1", "formula_text": "[CLS]u t [SEP ][CLS K ]kt 1 [SEP ]kc 1 [SEP ]... [CLS K ]kt M [SEP ]kc M [SEP ] (2)", "formula_coordinates": [4.0, 80.82, 477.21, 208.31, 32.16]}], "doi": ""}