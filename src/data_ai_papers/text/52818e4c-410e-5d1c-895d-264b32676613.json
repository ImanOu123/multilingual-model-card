{"title": "Optimal Complexity in Decentralized Training", "authors": "Yucheng Lu; Christopher De Sa", "pub_date": "2022-01-28", "abstract": "Decentralization is a promising method of scaling up parallel machine learning systems. In this paper, we provide a tight lower bound on the iteration complexity for such methods in a stochastic non-convex setting. Our lower bound reveals a theoretical gap in known convergence rates of many existing decentralized training algorithms, such as D-PSGD. We prove by construction this lower bound is tight and achievable. Motivated by our insights, we further propose DeTAG, a practical gossip-style decentralized algorithm that achieves the lower bound with only a logarithm gap. Empirically, we compare DeTAG with other decentralized algorithms on image classification tasks, and we show DeTAG enjoys faster convergence compared to baselines, especially on unshuffled data and in sparse networks.", "sections": [{"heading": "Introduction", "text": "Parallelism is a ubiquitous method to accelerate model training [1,2,3,4]. A parallel learning system usually consists of three layers (Table 1): an application to solve, a communication protocol deciding how parallel workers coordinate, and a network topology determining how workers are connected. Traditional design for these layers usually follows a centralized setup: in the application layer, training data is required to be shuffled and shared among parallel workers; while in the protocol and network layers, workers either communicate via a fault-tolerant single central node (e.g. Parameter Server) [5,6,7] or a fully-connected topology (e.g. AllReduce) [8,9]. This centralized design limits the scalability of learning systems in two aspects. First, in many scenarios, such as Federated Learning [10,11] and Internet of Things (IOT) [12], a shuffled dataset or a complete (bipartite) communication graph is not possible or affordable to obtain. Second, a centralized communication protocol can significantly slow down the training, especially with a low-bandwidth or high-latency network [13,14,15]. Table 1: Design choice of centralization and decentralization in different layers of a parallel machine learning system. The protocol specifies how workers communicate. The topology refers to the overlay network that logically connects all the workers.  \u25cb: A fully decentralized system in all three layers where the workers communicate via Gossip. Our framework and theory are applicable to all kinds of decentralized learning systems.\nThe rise of decentralization. To mitigate these limitations, decentralization comes to the rescue. Decentralizing the application and network allows workers to learn with unshuffled local datasets [16] and arbitrary topologies [17,18]. Furthermore, the decentralized protocol, i.e. Gossip, helps to balance load, and has been shown to outperform centralized protocols in many cases [19,20,21,22].\nUnderstanding decentralization with layers. Many decentralized training designs have been proposed, which can lead to confusion as the term \"decentralization\" is used inconsistently in the literature. Some works use \"decentralized\" to refer to approaches that can tolerate non-iid or unshuffled datasets [16], while others use it to mean gossip communication [19], and still others use it to mean a sparse topology graph [23]. To eliminate this ambiguity, we formulate Table 1, which summarizes the different \"ways\" a system can be decentralized. Note that the choices to decentralize different layers are independent, e.g., the centralized protocol AllReduce can still be implemented on a decentralized topology like the Ring graph [23].\nThe theoretical limits of decentralization. Despite the empirical success, the best convergence rates achievable by decentralized training-and how they interact with different notions of decentralizationremains an open question. Previous works often show complexity of a given decentralized algorithm with respect to the number of iterations T or the number of workers n, ignoring other factors including network topologies, function parameters or data distribution. Although a series of decentralized algorithms have been proposed showing theoretical improvements-such as using variance reduction [24], acceleration [17], or matching [25]-we do not know how close they are to an \"optimal\" rate or whether further improvement is possible.\nIn light of this, a natural question is: What is the optimal complexity in decentralized training? Has it been achieved by any algorithm yet? Previous works have made initial attempts on this question, by analyzing this theoretical limit in a non-stochastic or (strongly) convex setting [17,26,27,28,29,30]. These results provide great heuristics but still leave the central question open, since stochastic methods are usually used in practice and many real-world problems of interest are non-convex (e.g. deep learning). In this paper we give the first full answer to this question: our contributions are as follows.\n\u2022 In Section 4, we prove the first (to our knowledge) tight lower bound for decentralized training in a stochastic non-convex setting. Our results reveal an asymptotic gap between our lower bound and known convergence rates of existing algorithms. \u2022 In Section 5, we prove our lower bound is tight by exhibiting an algorithm called DeFacto that achieves it-albeit while only being decentralized in the sense of the application and network layers. \u2022 In Section 6, we propose DeTAG, a practical algorithm that achieves the lower bound with only a logarithm gap and that is decentralized in all three layers. \u2022 In Section 7, we experimentally evaluate DeTAG on the CIFAR benchmark and show it converges faster compared to decentralized learning baselines.\nTable 2: Complexity comparison among different algorithms in the stochastic non-convex setting on arbitrary graphs. The blue text are the results from this paper. Definitions to all the parameters can be found in Section 3. Other algorithms like EXTRA [69] or MSDA [70] are not comparable since they are designed for (strongly) convex problems. Additionally, Liu and Zhang [71] provides alternative complexity bound for algorithms like D-PSGD which improves upon the spectral gap. However, the new bound would compromise the dependency on , which does not conflict with our comparison here.", "publication_ref": ["b0", "b1", "b2", "b3", "b4", "b5", "b6", "b7", "b8", "b9", "b10", "b11", "b12", "b13", "b14", "b15", "b16", "b17", "b18", "b19", "b20", "b21", "b15", "b18", "b22", "b22", "b23", "b16", "b24", "b16", "b25", "b26", "b27", "b28", "b29", "b68", "b69", "b70"], "figure_ref": [], "table_ref": []}, {"heading": "Source Protocol", "text": "Sample Complexity Comm. Complexity Gap to Lower Bound\nLower Bound Theorem 1 Central \u2126 \u2206L\u03c3 2 nB 4 \u2126 \u2206LD 2 / Corollary 1 Decentral \u2126 \u2206L\u03c3 2 nB 4 \u2126 \u2206L 2 \u221a 1\u2212\u03bb / Upper Bound DeFacto (Theorem 2) Central O \u2206L\u03c3 2 nB 4 O \u2206LD 2 O(1) DeTAG (Theorem 3) Decentral O \u2206L\u03c3 2 nB 4 O \u2206L log \u03c2 0 n \u221a \u2206L 2 \u221a 1\u2212\u03bb O log \u03c2 0 n \u221a \u2206L D-PSGD [19] Decentral O \u2206L\u03c3 2 nB 4 O \u2206Ln\u03c2 2 (1\u2212\u03bb) 2 O n\u03c2 (1\u2212\u03bb) 3 2 SGP [72] Decentral O \u2206L\u03c3 2 nB 4 O \u2206Ln\u03c2 2 (1\u2212\u03bb) 2 O n\u03c2 (1\u2212\u03bb) 3 2 D 2 [24] Decentral O \u2206L\u03c3 2 nB 4 O \u03bb 2 \u2206Ln\u03c2 0 2 (1\u2212\u03bb) 3 O \u03bb 2 n\u03c2 0 (1\u2212\u03bb) 5 2 DSGT [42] Decentral O \u2206L\u03c3 2 nB 4 O \u03bb 2 \u2206Ln\u03c2 0 2 (1\u2212\u03bb) 3 O \u03bb 2 n\u03c2 0 (1\u2212\u03bb) 5 2 GT-DSGD [44] Decentral O \u2206L\u03c3 2 nB 4 O \u03bb 2 \u2206Ln\u03c2 0 2 (1\u2212\u03bb) 3 O \u03bb 2 n\u03c2 0 (1\u2212\u03bb) 5 2", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Decentralized Training. In the application layer, decentralized training usually denotes federated learning [31]. Research on decentralization in this sense investigates convergence where each worker samples only from a local dataset which is not independent and identically distributed to other workers' datasets [32,33,34,35].\nAnother line of research on decentralization focuses on the protocol layer-with average gossip [36,37], workers communicate by averaging their parameters with neighbors on a graph. D-PSGD [19] is one of the most basic algorithms that scales SGD with this protocol, achieving a linear parallel speed up. Additional works extend D-PSGD to asynchronous and variance-reduced cases [13,24,38,39,40,41]. After those, Zhang and You [42], Xin et al. [43,44] propose adding gradient trackers to D-PSGD. Other works discuss the application of decentralization on specific tasks such as linear models or deep learning [45,46]. Zhang and You [47] treats the case where only directed communication can be performed. Wang et al. [25] proposes using matching algorithms to optimize the gossip protocol. Multiple works discuss using compression to decrease communication costs in decentralized training [10,22,48,49,50], and other papers connect decentralized training to other parallel methods and present a unified theory [4,27,51]. In some even earlier works like [52,53], full local gradients on a convex setting is investigated. Lower Bounds in Stochastic Optimization. Lower bounds are a well studied topic in non-stochastic optimization, especially in convex optimization [54,55,56,57,58]. In the stochastic setting, Allen-Zhu [59] and Foster et al. [60] discuss the complexity lower bound to find stationary points on convex problems. Other works study the lower bound in a convex, data-parallel setting [61,62,63], and Colin et al. [64] extends the result to a model-parallel setting. In the domain of non-convex optimization, Carmon et al. [65,66] propose a zero-chain model that obtains tight bound for a first order method to obtain stationary points. Zhou and Gu [67] extends this lower bound to a finite sum setting, and Arjevani et al. [68] proposes a probabilistic zero-chain model that obtains tight lower bounds for first-order methods on stochastic and non-convex problems.", "publication_ref": ["b30", "b31", "b32", "b33", "b34", "b35", "b36", "b18", "b12", "b23", "b37", "b38", "b39", "b40", "b41", "b42", "b43", "b44", "b45", "b46", "b24", "b9", "b21", "b47", "b48", "b49", "b3", "b26", "b50", "b51", "b52", "b53", "b54", "b55", "b56", "b57", "b58", "b59", "b60", "b61", "b62", "b63", "b64", "b65", "b66", "b67"], "figure_ref": [], "table_ref": []}, {"heading": "Setting", "text": "In this section, we introduce the notation and assumptions we will use. Throughout the paper, we consider the standard data-parallel training setup with n parallel workers. Each worker i stores a copy of the model x \u2208 R d and a local dataset D i . The model copy and local dataset define a local loss function (or empirical risk) f i . The ultimate goal of the parallel workers is to output a target modelx that minimizes the average over all the local loss functions, that is,\nx = arg min x\u2208R d \uf8ee \uf8ef \uf8f0f (x) = 1 n n i=1 E \u03bei\u223cDi f i (x; \u03be i ) fi(x) \uf8f9 \uf8fa \uf8fb . (1)\nHere, \u03be i is a data sample from D i and is used to compute a stochastic gradient via some oracle, e.g. backpropagation on a mini-batch of samples. The loss functions can (potentially) be non-convex so finding a global minimum is NP-Hard; instead, we expect the workers to output a pointx at which f (x) has a small gradient magnitude in expectation: E \u2207f (x) \u2264 , for some small . 1 The assumptions our theoretical analysis requires can be categorized by the layers from Table 1: in each layer, \"being decentralized\" corresponds to certain assumptions (or lack of assumptions). We now describe these assumptions for each layer separately.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "Application Layer", "text": "Application-layer assumptions comprise constraints on the losses f i from (1) and the gradient oracle via which they are accessed by the learning algorithm, as these are constraints on the learning task itself.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Function class (\u2206 and L).", "text": "As is usual in this space, we assume the local loss functions\nf i : R d \u2192 R are L-smooth, \u2207f i (x) \u2212 \u2207f i (y) \u2264 L x \u2212 y , \u2200x, y \u2208 R d ,(2)\nfor some constant L > 0, and that the total loss f is range-bounded by \u2206 in the sense that f (0\n)\u2212inf x f (x) \u2264 \u2206.\nWe let the function class F \u2206,L denote the set of all functions that satisfy these conditions (for any dimension d \u2208 N + ).\nOracle class (\u03c3 2 ). We assume each worker interacts with its local function f i only via a stochastic gradient oracleg i , and that when we query this oracle with model x, it returns an independent unbiased estimator to \u2207f i (x) based on some random variable z with distribution Z (e.g. the index of a mini-batch randomly chosen for backprop). Formally,\nE z\u223cZ [g i (x, z)] = \u2207f i (x), \u2200x \u2208 R d .(3)\nAs per the usual setup, we additionally assume the local estimator has bounded variance: for some constant \u03c3 > 0,\nE z\u223cZ g i (x, z) \u2212 \u2207f i (x) 2 \u2264 \u03c3 2 , \u2200x \u2208 R d .(4)\nWe let O denote a set of these oracles {g i } i\u2208 [n] , and let the oracle class O \u03c3 2 denote the class of all such oracle sets that satisfy these two assumptions. Data shuffling (\u03c2 2 and \u03c2 2 0 ). At this point, an analysis with a centralized application layer would make the additional assumption that all the f i are equal and theg i are identically distributed: this roughly corresponds to the assumption that the data all comes independently from a single centralized source. We do not make this assumption, and lacking such an assumption is what makes an analysis decentralized in the application layer. Still, some assumption that bounds the f i relative to each other somehow is needed: we now discuss two such assumptions used in the literature, from which we use the weaker (and more decentralized) one.\nOne commonly made assumption [10,19,22,48,50] in decentralized training is\n1 n n i=1 \u2207f i (x) \u2212 \u2207f (x) 2 \u2264 \u03c2 2 , \u2200x \u2208 R d ,(5)\nfor some constant \u03c2, which is said to bound the \"outer variance\" among workers. This is often unreasonable, as it suggests the local datasets on workers must have close distribution: in practice, ensuring this often requires some sort of shuffling or common centralized data source. We do not assume (5) but instead adopt the much weaker assumption 1 n\nn i=1 \u2207f i (0) \u2212 \u2207f (0) 2 \u2264 \u03c2 2 0 ,(6)\nfor constant \u03c2 0 > 0. 2 This assumption only requires a bound at point 0, which is, to the best of our knowledge, the weakest assumption of this type used in the literature [24,42]. Requiring such a weak assumption allows workers to (potentially) sample from different distributions or vary largely in their loss functions (e.g. in a federated learning environment).", "publication_ref": ["b9", "b18", "b21", "b47", "b49", "b1", "b23", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "Protocol Layer", "text": "Protocol-layer assumptions comprise constraints on the parallel learning algorithm itself, and especially on the way that the several workers communicate to approach consensus. Algorithm class (B). We consider algorithms A that divide training into multiple iterations, and between two adjacent iterations, there must be a synchronization process among workers (e.g. a barrier) such that they start each iteration simultaneously. 3 Each worker running A has a local copy of the model, and we let x t,i \u2208 R d denote this model on worker i at iteration t. We assume without loss of generality that A initializes each local model at zero: x 0,i = 0 for all i. At each iteration, each worker makes at most B queries to its gradient oracleg i , for some constant B \u2208 N + , and then uses the resulting gradients to update its model. We do not make any explicit rules for output and allow the output of the algorithmx t at the end of iteration t (the model that A would output if it were stopped at iteration t) to be any linear combination of all the local models, i.e.x\nt \u2208 span({x t,j } j\u2208[n] ) = { n j=1 c j x t,j | c j \u2208 R}.(7)\nBeyond these basic properties, we further require A to satisfy the following \"zero-respecting\" property from Carmon et al. [65]. Specifically, if z is any vector worker i queries its gradient oracle with at iteration t, then for any k \u2208 [d], if e k z = 0, then there exists a s \u2264 t and a j \u2208 [n] such that either j = i or j is a neighbor of i in the network connectivity graph G (i.e. (i, j) \u2208 {(i, i)} \u222a G) and (e k x s,j ) = 0. More informally, the worker will not query its gradient oracle with a nonzero value for some weight unless that weight was already nonzero in the model state of the worker or one of its neighbors at some point in the past. Similarly, for any k \u2208 [d], if (e k x t+1,i ) = 0, then either there exists an s \u2264 t and j such that (i, j) \u2208 {(i, i)} \u222a G and (e k x s,j ) = 0, or one of the gradient oracle's outputs v on worker i at iteration t has e k v = 0. Informally, a worker's model will not have a nonzero weight unless either (1) that weight was nonzero on that worker or one of its neighbors at a previous iteration, or (2) the corresponding entry in one of the gradients the worker sampled at that iteration was nonzero. Intuitively, we are requiring that algorithm A will not modify those coordinates that remain zero in all previous oracle outputs and neighboring models. 4 This lets A use a wide space of accessible information in communication and allows our class to cover first-order methods including SGD [73], Momentum SGD [74], Adam [75], RMSProp [76], Adagrad [77], and AdaDelta [78]. We let algorithm class A B denote the set of all algorithms A that satisfy these assumptions.\nSo far our assumptions in this layer cover both centralized and decentralized protocols. Decentralized protocols, however, must satisfy the additional assumption that they communicate via gossip (see Section 2) [36,37]. A single step of gossip protocol can be expressed as\nz t,i \u2190 j\u2208Ni y t,j W ji , \u2200i \u2208 [n](8)\nfor some constant doubly stochastic matrix W \u2208 R n\u00d7n called the communication matrix and y and z are the input and output of the gossip communication step, respectively. The essence of a single Gossip step is to take weighted average over the neighborhood specified by a fixed matrix. To simplify later discussion, we further define the gossip matrix class W n as the set of all matrices W \u2208 R n\u00d7n , where W is doubly stochastic and W ij = 0 only if (i, j) \u2208 G. We call every W \u2208 W n a gossip matrix and we use \u03bb = max{|\u03bb 2 |, |\u03bb n |} \u2208 [0, 1) to denote its general second-largest eigenvalue, where \u03bb i denotes the i-th largest eigenvalue of W . We let gossip algorithm class A B,W denote the set of all algorithms A \u2208 A B that only communicate via gossip using a single matrix W \u2208 W n . It trivially holds that A B,W \u2282 A B .", "publication_ref": ["b2", "b64", "b1", "b3", "b72", "b73", "b74", "b75", "b76", "b77", "b35", "b36"], "figure_ref": [], "table_ref": []}, {"heading": "Topology Layer", "text": "Topology-layer assumptions comprise constraints on how workers are connected topologically. We let the graph class G n,D denote the class of graphs G connecting n workers (vertices) with diameter D, where diameter of a graph measures the maximum distance between two arbitrary vertices (so 1 \u2264 D \u2264 n \u2212 1). A centralized analysis here typically will also require that G be either complete or complete-bipartite (with parameter servers and workers as the two parts): lacking this requirement and allowing arbitrary graphs is what makes an analysis decentralized in the topology layer.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Complexity Measures", "text": "Now that we have defined the classes we are interested in, we can use them to define the complexity measures we will bound in our theoretical results. Given a loss function\nf \u2208 F \u2206,L , a set of underlying oracles O \u2208 O \u03c3 2 , a graph G \u2208 G n,D\n, and an algorithm A \u2208 A B , letx A,f,O,G t denote the output of algorithm A at the end of iteration t under this setting. Then the iteration complexity of A solving f under O and G is defined as\nT (A, f, O, G) = min t \u2208 N E \u2207f (x A,f,O,G t ) \u2264 ,\nthat is, the least number of iterations required by A to find a -stationary-in-expectation point of f .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Lower Bound", "text": "Given the setup in Section 3, we can now present and discuss our lower bound on the iteration complexity.\nNote that in the formulation of protocol layer, the algorithm class A B only specifies the information available for each worker, and thus A B covers both centralization and decentralization in the protocol layer. Here, we show our lower bound in two parts: first a general bound where an arbitrary protocol that follows A B is allowed, and then a corollary bound for the case where only decentralized protocol is allowed.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Lower Bound for Arbitrary Protocol", "text": "We start from the general bound. We expect this lower bound to show given arbitrary setting (functions, oracles and graph), the smallest iteration complexity we could obtain from A B , i.e.\ninf A\u2208A B sup f \u2208F \u2206,L sup O\u2208O \u03c3 2 sup G\u2208G n,D T (A, f, O, G),(9)\nit suffices to construct a hard instance containing a loss functionf \u2208 F \u2206,L , a graph\u011c \u2208 G n,D and a set of oracles\u00d4 \u2208 O \u03c3 2 and obtain a valid lower bound on inf A\u2208A B T (A,f ,\u00d4,\u011c) since Equation ( 9) is always lower bounded by inf A\u2208A B T (A,f ,\u00d4,\u011c).\nFor the construction, we follow the idea of probabilistic zero-chain model [65,66,67,68], which is a special loss function where adjacent coordinates are closely dependent on each other like a \"chain.\" Our main idea is to use this function as f and split this chain onto different workers. Then the workers must conduct a sufficient number of optimization steps and rounds of communication to make progress. 5 From this, we obtain the following lower bound. Dependency on the parameters. The bound in Theorem 1 consists of a sample complexity term, which is the dominant one for small , and a communication complexity term. We can see the increase of query budget B will only reduce the sample complexity. On the other hand, as the diameter D of a graph will generally increase as the number of vertices n increases, we can observe a trade-off between two terms when the system scales up: when more workers join the system, the communication complexity will gradually become the dominant term.\nConsistency with the literature. Theorem 1 is tightly aligned with the state-of-the-art bounds in many settings. With n = B = D = 1, we recover the tight bound for sequential stochastic non-convex optimization \u0398(\u2206L\u03c3 2 \u22124 ) as shown in Arjevani et al. [68]. With \u03c3 = 0, D = 1, we recover the tight bound for sequential non-stochastic non-convex optimization \u0398(\u2206L \u22122 ) as shown in Carmon et al. [66]. With B = 1, D = 1, we recover the tight bound for centralized training \u0398(\u2206L\u03c3 2 (n 4 ) \u22121 ) given in Li et al. [6].\nImprovement upon previous results. Previous works like Seaman et al. [17], Scaman et al. [26] provide similar lower bounds in a convex setting which relates to the diameter. However, these results treat D as a fixed value, i.e., D = n \u2212 1, and thus makes the bound to be only tight on linear graph. By comparison, Theorem 1 allows D to be chosen independently to n.", "publication_ref": ["b64", "b65", "b66", "b67", "b4", "b67", "b65", "b5", "b16", "b25"], "figure_ref": [], "table_ref": []}, {"heading": "Lower Bound for Decentralized Protocol", "text": "The bound in Theorem 1 holds for both centralized and decentralized protocols. A natural question is: How would the lower bound adapt if the protocol is restricted to be decentralized? i.e., the quantity of\ninf A\u2208A B,W sup f \u2208F \u2206,L sup O\u2208O \u03c3 2 sup G\u2208G n,D T (A, f, O, G),\nwe can extend the lower bound to Gossip in the following corollary.\nCorollary 1 For every \u2206 > 0, L > 0, n \u2208 {2, 3, 4, \u2022 \u2022 \u2022 }, \u03c3 > 0, and B \u2208 N + , there exists a loss function f \u2208 F \u2206,L , a set of underlying oracles O \u2208 O \u03c3 2 , a gossip matrix W \u2208 W n with second largest eigenvalue being \u03bb = cos(\u03c0/n), and a graph G \u2208 G n,D , such that no matter what A \u2208 A B,W is used, T (A, f, O, G) will always be lower bounded by\n\u2126 \u2206L\u03c3 2 nB 4 + \u2206L 2 \u221a 1 \u2212 \u03bb .(11)\nGap in the existing algorithms. Comparing this lower bound with many state-of-the-art decentralized algorithms (Table 2), we can see they match on the sample complexity but leave a gap on the communication complexity. In many cases, the spectral gap significantly depends on the number of workers n and thus can be arbitrarily large. For example, when the graph G is a cycle graph or a linear graph, the gap of those baselines can increase by up to O(n 6 ) [79, 80]!", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "DeFacto: Optimal Complexity in Theory", "text": "In the previous section we show the existing algorithms have a gap compared to the lower bound. This gap could indicate the algorithms are suboptimal, but it could also be explained by our lower bound being loose.\nIn this section we address this issue by proposing DeFacto, an example algorithm showing the lower bound is achievable, which verifies the tightness of our lower bound-showing that (10) would hold with equality and \u0398(\u2022), not just \u2126(\u2022).\nWe start with the following insight on the theoretical gap: the goal of communication is to let all the workers obtain information from neighbors. Ideally, the workers would, at each iteration, perform (8) with W * = 1 n 1 n /n, where 1 n is the n-dimensional all-one vector. We call this matrix the Average Consensus Algorithm 1 Decentralized Stochastic Gradient Descent with Factorized Consensus Matrices (DeFacto) on worker i Input: initialized model x0,i, a copy of modelx0,i \u2190 x0,i, gradient buffer g = 0, step size \u03b1, a sequence of communication matrices {W r } 1\u2264r\u2264R of size R, number of iterations T , neighbor list Ni\n1: for t = 0, 1, \u2022 \u2022 \u2022 , T \u2212 1 do 2: k \u2190 t/2R . 3: r \u2190 t mod 2R. 4: if 0 \u2264 r < R then 5:\nSpend all B oracle budgets to compute stochastic gradientg at point x k,i and accumulate it to gradient buffer: g \u2190 g +g. Update model copy with the r-th matrix in {W r } 1\u2264r\u2264R :\nxt+1,i \u2190 j\u2208N i \u222a{i}x t,j [W r ]ji(12) 8:\nend if 9:\nif r = 2R \u2212 1 then 10:\nUpdate Model: xt+1,i \u2190xt+1,i \u2212 \u03b1 g R .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "11:", "text": "Reinitialize gradient buffer: g \u2190 0.\n12:\nCopy the current model:xt+1,i \u2190 xt+1,i.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "13:", "text": "end if 14: end for\n15: returnx = 1 n n i=1\nxT,i matrix. The Average Consensus is statistically equivalent to centralized communication (All-Reduce operation). However, due to the graph constraints, we can not use this W * unless workers are fully connected; instead, a general method is to repeatedly apply a sequence communication matrices in consecutive iterations and let workers achieve or approach the Average Consensus. Previous work uses Gossip matrix W and expect R r=1 W \u2248 1 n 1 n /n for some R. This R is known to be proportional to the mixing time of the Markov Chain W defines [4,22], which is related to the inverse of its spectral gap [81]. This limits convergence depending on the spectrum of the W chosen. The natural question to ask here is: can we do better? What are the limits of how fast we can reach average consensus on a connectivity graph G? This question is answered by the following lemma.\nLemma 1 For any G \u2208 G n,D , let W G denote the set of n \u00d7 n matrices such that for all W \u2208 W G , W ij = 0 if edge (i, j) does not appear in G. There exists a sequence of R matrices {W r } r\u2208[R] that belongs to W G such that R \u2208 {D, D + 1, \u2022 \u2022 \u2022 , 2D} and W R\u22121 W R\u22122 \u2022 \u2022 \u2022 W 0 = 1 n 1 n n = W * .\nLemma 1 is a classic result in the literature of graph theory. The formal proof and detailed methods to identify these matrices can be found in many previous works [82,83,84]. Here we treat this as a black box procedure. 6 Lemma 1 shows that we can achieve the exact average consensus by factorizing the matrix 1 n 1 n /n, and we can obtain the factors from a preprocessing step. From here, the path to obtain an optimal rate becomes clear: starting from t = 0, workers first spend R iterations only computing stochastic gradients and then another R iterations to reach consensus communicating via factors from Lemma 1; they then repeat this process until a stationary point is found. We call this algorithm DeFacto (Algorithm 1).\nDeFacto is statistically equivalent to centralized SGD operating T /2R iterations with a mini-batch size of BR. It can be easily verified that DeFacto holds membership in A B . A straightforward analysis gives the convergence rate of DeFacto shown in the following Theorem.\nTheorem 2 Let A 1 denote Algorithm 1. For F \u2206,L , O \u03c3 2 and G n,D defined with any \u2206 > 0, L > 0, n \u2208 N + , D \u2208 {1, 2, . . . , n \u2212 1}, \u03c3 > 0, and B \u2208 N + , the convergence rate of A 1 running on any loss function f \u2208 F \u2206,L , Algorithm 2 Decentralized Stochastic Gradient Tracking with By-Phase Accelerated Gossip (DeTAG) on worker i Input: initialized model x0,i, a copy of modelx0,i \u2190 x0,i, gradient tracker y 0,i , gradient buffer g (0) = g (\u22121) = 0, step size \u03b1, a gossip matrix W , number of iterations T , neighbor list Ni\n1: for t = 0, 1, \u2022 \u2022 \u2022 , T \u2212 1 do 2: k \u2190 t/R . 3:\nr \u2190 t mod R.", "publication_ref": ["b3", "b21", "b80", "b81", "b82", "b83", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "4:", "text": "Perform the r-th step in Accelerate Gossip:\nxt+1\n,i \u2190 AG(xt,i, W , Ni, i)(13)\ny t+1,i \u2190 AG(y t,i , W , Ni, i)(14)\n5:\nSpend all B oracle budgets to compute stochastic gradientg at point x k,i and accumulate it to gradient buffer:\ng (k) \u2190 g (k) +g. 6: if r = R \u2212 1 then 7:\nUpdate gradient tracker and model:\nxt+1,i \u2190xt+1,i \u2212 \u03b1y i (15\n)\ny t+1,i \u2190 y t+1,i + g (k) \u2212 g (k\u22121)(16) 8:\nReinitialize gradient buffer:\ng (k\u22121) \u2190 g (k)\nand then g (k) \u2190 0.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "9:", "text": "Copy the current model:xt+1,i \u2190 xt+1,i.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "10:", "text": "end if 11: end for\n12: returnx = 1 n n i=1 xT,i Algorithm 3 Accelerated Gossip (AG) with R steps Input: z 0,i , W , N i , i 1: z \u22121,i \u2190 z 0,i 2: \u03b7 \u2190 1\u2212 \u221a 1\u2212\u03bb 2 1+ \u221a 1\u2212\u03bb 2 3: for r = 0, 1, 2, \u2022 \u2022 \u2022 , R \u2212 1 do 4: z r+1,i \u2190 (1 + \u03b7) j\u2208Ni\u222a{i} z r,j W ji \u2212 \u03b7z r\u22121,i 5: end for 6: return z R,i any graph G \u2208 G n,D , and any oracles O \u2208 O \u03c3 2 is bounded by T (A 1 , f, O, G) \u2264 O \u2206L\u03c3 2 nB 4 + \u2206LD 2 .(17)\nComparing Theorem 1 and Theorem 2, DeFacto achieves the optimal rate asymptotically. This shows that our lower bound in Theorem 1 is tight. Despite its optimality, the design of DeFacto is unsatisfying in three aspects: (1) It compromises the throughput 7 by a factor of two because in each iteration, a worker either communicates with neighbors or computes gradients but not both. This fails to overlap communication and computation and creates extra idle time for the workers. (2) It needs to iterate over all the factor matrices before it can query the gradient oracle at subsequent parameters. When diameter D increases, the total time to finish such round will increase proportionally. (3) DeFacto works with decentralized data and arbitrary graph, achieving decentralization in both application and topology layers. However, the matrices used in Lemma 1 are not Gossip matrices as defined in W n , and thus it fails to be decentralized in the protocol-layer sense.", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "DeTAG: Optimal Complexity in Practice", "text": "To address the limitations of DeFacto, a natural idea is to replace all the factor matrices in Lemma 1 with a gossip matrix W . The new algorithm after this mild modification is statistically equivalent to a D-PSGD variant: every R iterations, it updates the model the same as one iteration in D-PSGD with a mini-batch size of BR and communicate with a matrix W whose second largest eigenvalue \u03bb = \u03bb R , with T /R iterations in total. However, even with arbitrarily large R, the communication complexity in this \"updated D-PSGD\" is still O(\u2206Ln\u03c2 \u22122 ) (Table 2), leaving an O(n\u03c2) gap compared to our lower bound.\nTo close this gap, we adopt two additional techniques: 8 one is a gradient tracker y that is used as reference capturing gradient difference in the neighborhood; the other is using acceleration in gossip as specified in Algorithm 3. Modifying DeFacto results in Algorithm 2, which we call DeTAG. DeTAG works as follows: it divides the total number of iterations T into several phases where each phase contains R iterations. In each iteration, the communication process calls Accelerated Gossip to update a model replicax and the gradient tracker (line 4) while the computation process constantly computes gradients at the same point (line 5). At the end of each phase, model x, its replicax and gradient tracker y are updated in line 7-10 and then DeTAG steps into the next phase. Aside from the two additional techniques, the main difference between DeTAG and DeFacto is that the communication matrix in DeTAG is a fixed gossip matrix W , which allows DeTAG to benefit from decentralization in the protocol layer as well as to adopt arbitrary R \u2265 1 in practice (allowing R to be tuned independently of G).\nImprovement on design compared to baselines. Comparing with other baselines in Table 2, the design of DeTAG improves in the sense that (1) It removes the dependency on the outer variance \u03c2. (2) It drops the requirement 9 on the gossip matrix assumed in Tang et al. [24]. (3) The baseline DSGT [42] and GT-DSGD [44] can be seen as special cases of taking R = 1 and \u03b7 = 0 in DeTAG. That implies in practice, a well tuned DeTAG can never perform worse than the baseline DSGT or GT-DSGD.\nThe convergence rate of DeTAG is given in the following theorem.\nTheorem 3 Let A 2 denote Algorithm 2. For F \u2206,L , O \u03c3 2 and G n,D defined with any \u2206 > 0, L > 0, n \u2208 N + , \u03bb \u2208 [0, 1)\n, \u03c3 > 0, and B \u2208 N + , under the assumption of Equation ( 6), if we set the phase length R to be\nR = max 1 2 log(n), 1 2 log \u03c2 2 0 T \u2206L \u221a 1 \u2212 \u03bb ,\nthe convergence rate of A 2 running on any loss function f \u2208 F \u2206,L , any graph G \u2208 G n,D , and any oracles O \u2208 O \u03c3 2 is bounded by\nT (A 2 , f, O, G) \u2264 O \uf8eb \uf8ed \u2206L\u03c3 2 nB 4 + \u2206L log n + \u03c20n \u221a \u2206L 2 \u221a 1 \u2212 \u03bb \uf8f6 \uf8f8 .\nComparing Theorem 1 and Theorem 3, DeTAG achieves the optimal complexity with only a logarithm gap. Improvement on complexity. Revisiting Table 2, we can see the main improvement of DeTAG's complexity is in the two terms on communication complexity: (1) DeTAG only depends on the outer variance term \u03c2 0 inside a log, and (2) It reduces the dependency on the spectral gap 1 \u2212 \u03bb to the lower bound of square root, as shown in Corollary 1.\nUnderstanding the phase length R. In DeTAG, the phase length R is a tunable parameter. Theorem 3 provides a suggested value for R. Intuitively, the value of R captures the level of consensus of workers should reach before they step into the next phase. Theoretically, we observe R is closely correlated to the mixing time of W : if we do not use acceleration in Gossip, then R will become\u00d5 1 1\u2212\u03bb , which is exactly the upper bound on the mixing time of the Markov Chain W defines [81].", "publication_ref": ["b23", "b41", "b43", "b0", "b80"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In this section we empirically compare the performance among different algorithms. All the models and training scripts in this section are implemented in PyTorch and run on an Ubuntu 16.04 LTS cluster using a SLURM workload manager running CUDA 9.2, configured with 8 NVIDIA GTX 2080Ti GPUs. We launch  one process from the host as one worker and let them use gloo as the communication backend. In each experiment, we compare the following algorithms 10 : D-PSGD [19], D 2 [24], DSGT [42] and DeTAG. Note that GT-DSGD [44] and DSGT [42] are essentially the same algorithm so we omit the comparison to GT-DSGD. Also note that SGP [72] reduces to D-PSGD for symmetric mixing matrices in undirected graphs. Throughout the experiment we use Ring graph. Hyperparameters can be found in the supplementary material.\nConvergence over different outer variance. In the first experiments, we investigate the correlation between convergence speed and the outer variance \u03c2(\u03c2 0 ). We train LeNet on CIFAR10 using 8 workers, which is a standard benchmark experiment in the decentralized data environment [24,42]. To create the decentralized data, we first sort all the data points based on its labels, shuffle the first X% data points and then evenly split to different workers. The X controls the degree of decentralization, we test X = 0, 25, 50, 100 and plot the results in Figure 2.\nWe can see in Figure 2(a) when the dataset is fully shuffled, all the algorithms converge at similar speed while D-PSGD converges a little slower than other variance reduced algorithms. From Figure 2(b) to Figure 2(d) we can see when we shuffle less portion of the dataset, i.e., the dataset becomes more decentralized, D-PSGD fails to converge even with fine-tuned hyperparameter. Meanwhile, among D 2 , DSGT and DeTAG, we can see DeTAG converges the fastest. When dataset becomes more decentralized, DSGT seems to receive more stable performance than D 2 .\nConvergence over different spectral gaps. In the second experiments, we proceed to explore the relation between convergence speed and spectral gap 1 \u2212 \u03bb of the gossip matrix W . We use 16 workers connected  with a Ring graph to train Resnet20 on CIFAR100, and we generate a W 0 on such graph using Metropolis method. Then we adopt the slack matrix method to modify the spectral gap [4]:\nW \u03ba = \u03baW 0 + (1 \u2212 \u03ba)I,\nwhere \u03ba is a control parameter. We test \u03ba = 1, 0.1, 0.05, 0.01 and plot the results in Figure 3. We can see with different \u03ba, DeTAG is able to achieve faster convergence compared to baselines. When the network becomes sparse, i.e., \u03ba decreases, DeTAG enjoys more robust convergence.", "publication_ref": ["b18", "b23", "b41", "b43", "b41", "b71", "b23", "b41", "b3"], "figure_ref": ["fig_5", "fig_5", "fig_5", "fig_5", "fig_7"], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper, we investigate the tight lower bound on the iteration complexity of decentralized training. We propose two algorithms, DeFacto and DeTAG, that achieve the lower bound in terms of different decentralization in a learning system. DeTAG uses Gossip protocol, and is shown to be empirically competitive to many baseline algorithms, such as D-PSGD. In the future, we plan to investigate the variants of the complexity bound with respect to communication that are compressed, asynchronous, etc.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Supplementary Material A Experimental Details", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1 Hyperparameter Tuning", "text": "In the experiment of training LeNet on CIFAR10, we tune the step size using grid search inside the following range: {5e-3, 1e-3, 5e-4, 2.5e-4, 1e-4, 5e-5}. Note that this range is in general smaller than the one chosen in [42], since here we are working with unshuffled data, and we found original range in baselines causes algorithms to diverge easily. Following [24], we let each run warm up for 10 epochs with step size 1e-5. For DeTAG, we further tune the accelerated gossip parameter \u03b7 within {0, 0.1, 0.2, 0.4} and phase length R within {1, 2, 3}. We fix the momentum term to be 0.9 and weight decay to be 1e-4.\nIn the experiment of training Resnet20 on CIFAR100, we tune the step size using grid search inside the following range: {0.5, 0.1, 0.05, 0.01, 0.005}. For DeTAG, we further tune the accelerated gossip parameter \u03b7 within {0, 0.1, 0.2, 0.4} and phase length R within {1, 2, 3}. We fix the momentum term to be 0.9 and weight decay to be 5e-4.\nThe hyperparameters adopted for each runs are shown in Table 3 and Table 4.", "publication_ref": ["b41", "b23"], "figure_ref": [], "table_ref": ["tab_0", "tab_1"]}, {"heading": "A.2 Techniques of Running DeTAG", "text": "We can see in the main loop of DeTAG, several gradient queries are made at the same point. This essentially is equivalent to a large mini-batch size. In practice, however, we can modify this to use local-steps and get better empirical results [85]. Another technique is to use warm-up epochs when data is decentralized. We observe it ensures a smooth convergence in practice. Last but not least, since at first the noise in the algorithms is generally large, we can use a dynamic phase length to obtain better results. That is, we start from phase length 1 for the first few epochs, and let DeTAG follow the special case of DSGT. Then we can gradually increase the phase length following given policies. The intuition is that as algorithm converges, we would need less noise from communication, and thus a longer phase length can benefit.   as the i-th coordinate of vector z \u2208 R d . For each setting, our constructions contain three main steps.\n(1) The first step is to follow the construction of a zero chain function model [65,66]. Following [68] and define prog(z) = max{i \u2265 0|z (i) = 0}, \u2200z \u2208 R d .\nA zero chain function f has the following property:\nprog(\u2207f (x)) \u2264 prog(x) + 1,(19)\nthat means, for a model start from x = 0, a single gradient evaluation can only make at most one more coordinate to be non-zero. The name of \"chain\" comes from the fact that the adjacent coordinates are linked like a chain and only if the previous coordinate becomes non-zero that the current coordinate can become non-zero via a gradient update. Consider a model with d dimension, if we show that \u2207f (x) \u2265 for any x \u2208 R d with x (d) = 0, we will obtain d as a lower bound on the gradient calls to obtain the -stationary point. We refer such sequential lower bound as T 0 .\n(\n)2\nStep two is to construct a graph G \u2208 G n,D and a set of oracle O \u2208 O \u03c3 2 . To do this, our basic idea is to follow [68] and introduce randomness on the prog(x), and thus the whole chain only make progress with probability p. As will be shown later, this requires \u2126(T 0 /p) iterations in total.\n(3) The third and last step is to rescale the function and distribution so as to make it belong to the function and oracle classes we consider. In other words, this step is to guarantee the result is shown in terms of \u2206, L, \u03c3, n and D.\nWe start from a smooth and (potentially) non-convex zero chain functionf [66] as defined below:\nf (x) = \u2212\u03a8(1)\u03a6(x (1) ) + T \u22121 i=1 [\u03a8(\u2212x (i) )\u03a6(\u2212x (i+1) ) \u2212 \u03a8(x (i) \u03a6(x (i+1) )],(20)\nwhere for \u2200z \u2208 R\n\u03a8(z) = 0 z \u2264 1/2 exp 1 \u2212 1 (2z\u22121) 2 z > 1/2 , \u03a6(z) = \u221a e z \u2212\u221e e 1 2 t 2 dt. (21\n)\nThis function, as shown in previous works [66,68], is a zero-chain function and thus is generally \"hard\" to optimize: it costs at least T gradient evaluations to find a stationary point. We summarize some properties of Equation (20) as the following (Proof can be found in Lemma 2 in [68]):\n1.f (x) \u2212 inf xf (x) \u2264 \u2206 0 T , \u2200x \u2208 R d , where \u2206 0 = 12.\n2.f is l 1 -smooth, where l 1 = 152.\n3. \u2200x \u2208 R T , \u2207f (x) \u221e \u2264 G \u221e , where G \u221e = 23.\n4. \u2200x \u2208 R T , if prog(x) < T , then f (x) \u221e \u2265 1.\n(Setting 1) Next we discuss the first setting with lower bound \u2126 \u2206L\u03c3 2 nB 4 . (Setting 1, Step 1) The loss functions are defined asf\ni (x) =f (x),(22)\nnote that 1/n n i=1f i =f . It can be seen from Property 2 that all thef i are l 1 -smooth. (Setting 1, Step 2) For this setting we consider complete graph. We construct the oracle on worker i as the following:\n[\u011d i (x)] j = \u2207 jfi (x) \u2022 1 + 1{j > prog(x)} z p \u2212 1 ,(23)\nwhere z \u223c Bernoulli(p). It can be seen that\nE[\u011d i (x)] = \u2207f i (x),(24)\nand from Property 3 we know\nE \u011d i (x) \u2212 \u2207f i (x) 2 = |\u2207 prog(x)+1f (x)| 2 E z p \u2212 1 2 \u2264 \u2207f i (x) 2 \u221e (1 \u2212 p) p \u2264 \u2207f (x) 2 \u221e (1 \u2212 p) p \u2264 G 2 \u221e (1 \u2212 p) p .\n(Setting 1,\nStep 3) Finally we rescale each function as f i = L\u03bb 2 /l 1fi (x/\u03bb) where \u03bb is a parameter subject to change. For L: note that all f i are L l1 \u2022 l 1 = L-smooth. For the \u2206,\nf \u2212 f * = L\u03bb 2 l 1 (f \u2212f * ) = L\u03bb 2 \u2206 0 T l 1 \u2264 \u2206.(25)\nFor the oracle, to be consistent with f i , we rescale it as g i (x) = L\u03bb/l 1\u011di (x/\u03bb), and we have\nE g i (x) \u2212 \u2207f i (x) 2 \u2264 L 2 \u03bb 2 l 2 1 E g i x \u03bb \u2212 \u2207f i x \u03bb 2 \u2264 L 2 \u03bb 2 G 2 \u221e (1 \u2212 p) l 2 1 p \u2264 \u03c3 2 . (26\n)\nWe assign \u03bb = 2l 1 /L, then Equation ( 25) and ( 26) are fulfilled with\nT = \u2206 \u2206 0 l 1 (2 ) 2 , p = min{(2G \u221e ) 2 /\u03c3 2 , 1}.\nTake \u03b4 = 1/2 in Lemma 2, we have for probability at least 1/2, \u2207f (x (t) ) \u2265 for all t \u2264 T +log(\u03b4) min{nBp,1}(e\u22121) . Use Property 4, for any x \u2208 R T such that prog(x) < T it holds that \u2207f (x) \u2265 2 , therefore,\nE \u2207f (x T ) > . (27\n)\nThen with small it follows that and that completes the proof for setting 1.\nT (A, f, O, G) \u2265 T \u2212 1 nBp(e \u2212 1) \u2265 \u2126 \u2206L\u03c3 2 nB 4 ,(28)\n(Setting 2) We proceed to the prove second bound\n\u2126 \u2206LD 2 . (Setting 2 Step 1 & Step 2)\nWe assign all the workers with index from 1 to n, we first define two indices set\nI 0 = {1, \u2022 \u2022 \u2022 , |I 0 |} , I 1 = {n, n \u2212 1, \u2022 \u2022 \u2022 , n \u2212 |I 1 | + 1} . (29\n)\nwhere | \u2022 | denotes a cardinality of a set. Consider the construction of G in Figure 4:\nIf D \u2265 n \u2212 2 n/3 + 2,\nthen it implies the number of nodes between A and B is larger than n/3 . In this case, denote A' and B' as a sub linear graph where its number of nodes is exactly n/3 . Let all the nodes on the left of A' be in I 0 and all the nodes on the right of B' be I 1 We define all the local functions on such graph as following:\nf i (x) = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 \u2212 2n n\u2212 n/3 \u03a8(1)\u03a6(x (1) ) + i=2k,k\u2208{1,2,\u2022\u2022\u2022 },i<T 2n n\u2212 n/3 [\u03a8(\u2212x (i) )\u03a6(\u2212x (i+1) ) \u2212 \u03a8(x (i) )\u03a6(x (i+1) )] i \u2208 I 0 , i=2k\u22121,k\u2208{1,2,\u2022\u2022\u2022 },i<T 2n n\u2212 n/3 [\u03a8(\u2212x (i) )\u03a6(\u2212x (i+1) ) \u2212 \u03a8(x (i) )\u03a6(x (i+1) )] i \u2208 I 1 , 0 i \u2208 I 0 , I 1 . (30)\nIf D < n \u2212 2 n/3 + 2, the distance between node A and node B is D \u2212 2 and the sub linear graph whose end points are A and B contains D \u2212 1 nodes. We let the number of nodes on the left of A be n\u2212D+1 2 , we denote the set of indices of all such nodes as I 0 ; and then we let the number of nodes on the right of B be n\u2212D+1 2 , we denote the set of indices of all such nodes as I I . Since D < n \u2212 2 n/3 + 2, this implies |I 0 |, |I 1 | > n/3. We define all the local functions on such graph as following:\nf i (x) = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 \u2212 n |I0| \u03a8(1)\u03a6(x (1) ) + i=2k,k\u2208{1,2,\u2022\u2022\u2022 },i<T n |I0| [\u03a8(\u2212x (i) )\u03a6(\u2212x (i+1) ) \u2212 \u03a8(x (i) )\u03a6(x (i+1) )] i \u2208 I 0 , i=2k\u22121,k\u2208{1,2,\u2022\u2022\u2022 },i<T n |I1| [\u03a8(\u2212x (i) )\u03a6(\u2212x (i+1) ) \u2212 \u03a8(x (i) )\u03a6(x (i+1) )] i \u2208 I 1 , 0 i \u2208 I 0 , I 1 .(31)\nIn both cases discussed based on D, we can see thatf (x) = 1 n n i=1f i (x), and we are splitting hard zero-chain function into two main different part: the even components of the chain and the odd components of the chain. It is easy to see that for the zero chain function to make progress, it takes at least n/3 , i.e., \u2126(D) number of iterations in the first case (since here D =\u03b3n for some\u03b3 > 1/3) and D number of iterations in the seconds case. Then the total number of iterations is lower bounded by \u2126(T D).\nFor the oracle, we let oracle on worker i as\n[\u011d i (x)] j = \u2207 jfi (x).(32)\n(Setting 2, Step 3) The last step is to rescale the parameters. Compared to setting 1, we know here all thef i are 3l 1 -smooth, as before we let\nf i (x) = L\u03bb 2 3l 1f i x \u03bb , \u03bb = 6l 1 L .(33)\nFor the \u2206 bound we have\nL\u03bb 2 \u2206 0 T /3l 1 \u2264 \u2206(34)\nto fulfill this it suffices to set\nT = \u2206L \u2206 0 l 1 (12 ) 2 .(35)\nIt also can be seen that f is L-smooth. So in this setting,\nT (A, f, O, G) \u2265 \u2126(T D) = \u2126 \u2206LD 2 .(36)\nCombining Setting 1 and 2 we complete the proof.", "publication_ref": ["b84", "b64", "b65", "b67", "b67", "b65", "b65", "b67", "b19", "b67"], "figure_ref": ["fig_9"], "table_ref": []}, {"heading": "Lemma 2", "text": "In setting 1 in the proof of Theorem 1, with probability at least 1 \u2212 \u03b4, \u2207f (x t ) \u2265 for all t \u2264\nT +log(\u03b4) min{nBp,1}(e\u22121) .\nProof Define a filtration at iteration t as the sigma field of all the previous events happened before iteration t. Let i\n(t) j = prog(x t,j ), \u2200j \u2208 [n] and i (t) = max j i (t)\nj . And we denote E (t,m,j) as the event of the i (t) m + 1-th coordinate of output of j-th query on worker m at iteration t is non-zero. Based on the independent sampling, these events are independent. Thus we know:\nP[i (t+1) \u2212 i (t) = 1|U (t) ] = P \uf8ee \uf8ef \uf8ef \uf8f0 i\u2208[n] j\u2264B E (t,i,j) |U (t) \uf8f9 \uf8fa \uf8fa \uf8fb \u2264 i\u2208[n],j\u2264B P E (t,i,j) |U (t) \u2264 min{nBp, 1}.(37)\nLet\nq (t) = i (t+1) \u2212 i (t)\n, with Chernoff bound, we obtain\nP[i (t) \u2265 T ] = P[e t\u22121 j=0 q (j) \u2265 e T ] \u2264 e \u2212T E[e t\u22121 j=0 q (j)\n].\nFor the expectation term we know that\nE[e t\u22121 j=0 q (j) ] = E \uf8ee \uf8f0 t\u22121 j=0 E e q (j) |U (j) \uf8f9 \uf8fb \u2264 (1 \u2212 min{nBp, 1} + min{nBp, 1}e) t \u2264 e min{nBp,1}t(e\u22121) .(39)\nThus we know\nP[i (t) \u2265 T ] \u2264 e (e\u22121) min{nBp,1}t\u2212T \u2264 \u03b4,(40)\nfor every t \u2264 T +log(\u03b4) min{nBp,1}(e\u22121) .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.2 Proof to Corollary 1", "text": "Proof Different from Theorem 3, in this corollary we do not choose D and n separately, so that our construction can just use the linear graph as follows: (Linear graph, Step 1) We first let |I 0 | = |I 1 | = n/3 in the proof of Theorem 1, meaning I 0 denotes the first n/3 workers and I 1 denotes the last n/3 workers. We define all the local functionsf i (x) as following:\n\uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 \u2212 n n/3 \u03a8(1)\u03a6(x (1) ) + i=2k,k\u2208{1,2,\u2022\u2022\u2022 },i<T n n/3 [\u03a8(\u2212x (i) )\u03a6(\u2212x (i+1) ) \u2212 \u03a8(x (i) )\u03a6(x (i+1) )] i \u2208 I 0 , i=2k\u22121,k\u2208{1,2,\u2022\u2022\u2022 },i<T n n/3 [\u03a8(\u2212x (i) )\u03a6(\u2212x (i+1) ) \u2212 \u03a8(x (i) )\u03a6(x (i+1) )] i \u2208 I 1 , 0 i \u2208 I 0 , I 1 . (41\n)\nwe can see thatf (x) = 1 n n i=1f i (x). (Linear graph,\nStep 2) We consider linear graph in this setting and from one end to the other, the worker's index is 1 to n, without the loss of generality. It is easy to see that for the zero chain function to make progress, it takes at least n \u2212 2 n/3 + 1 number of iterations. Note that in linear graph n \u2212 1 = D, the total number of iterations is at least \u2126 (T D) .\nFor the oracle, we let oracle on worker i as\n[\u011d i (x)] j = \u2207 jfi (x)(43)\n(Linear graph, Step 3) The last step is to rescale the parameters. Compared to setting 1, we know here all th\u00ea f i are 3l 1 -smooth, as before we let\nf i (x) = L\u03bb 2 3l 1f i x \u03bb , \u03bb = 6l 1 L .(44)\nFor the \u2206 bound we have\nL\u03bb 2 \u2206 0 T /3l 1 \u2264 \u2206,(45)\nto fulfill this it suffices to set\nT = \u2206L \u2206 0 l 1 (12 ) 2 .(46)\nIt also can be seen that f is L-smooth. So in this setting,\nT (A, f, O, G) \u2265 \u2126(T D) \u2265 \u2126 \u2206LD 2 .(47)\nGiven the bound, we use two additional results on linear graph as [86]: the random walk matrix W rw on linear graph with \u03bb fulfilling\n1 \u221a 1 \u2212 \u03bb = O(D).(48)\nThen we can rewrite the lower bound in the form of \u03bb as shown in Corollary 1.\nFinally, using the conclusion of \u03bb = cos(\u03c0/n) for n \u2208 {2, 3, \u2022 \u2022 \u2022 , } on linear graph we complete the proof.", "publication_ref": ["b85"], "figure_ref": [], "table_ref": []}, {"heading": "B.3 Proof to Theorem 2", "text": "Proof As (partially) discussed in the paper, DeFacto is statistically equivalent to centralized SGD. Specifically, it conduct K = T /2R gradient steps where each step contains a mini-batch of R at the point of\nx k,i , \u2200i \u2208 [n].\nTake the well-known convergence rate for centralized SGD:\n1 T T \u22121 t=0 \u2207f (x) 2 \u2264 O \u2206L\u03c3 \u221a nBT + \u2206L T .(49)\nThe convergence rate of DeFacto can be expressed as:\n1 T T \u22121 t=0 \u2207f (x) 2 \u2264 O \u2206L\u03c3/ \u221a R \u221a nBK + \u2206L K = O \u2206L\u03c3 \u221a nBT + \u2206LR T = O \u2206L\u03c3 \u221a nBT + \u2206LD T ,(50)\nthen we obtain for DeFacto, when\nT = O(\u2206L\u03c3 2 (nB 4 ) \u22121 + \u2206LD \u22122 ), min t=0,1,\u2022\u2022\u2022 ,T \u22121 E \u2207f (x) \u2264 min t=0,1,\u2022\u2022\u2022 ,T \u22121 E \u2207f (x) 2 \u2264 O \u2206L\u03c3 \u221a nBT + \u2206LD T \u2264 ,(51)\nthat completes the proof.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.4 Proof to Theorem 3", "text": "Proof In this proof, we adopt an updated version of notation: we denote at the beginning of phase k, the three quantities of interests are X k , Y k andG k , and the update rule becomes:\nY k+1 = M(Y k +G k \u2212G k\u22121 ),(52)\nX k+1 = M(X k \u2212 \u03b1Y k ),(53)\nwithG k+1 = \u2207f 1 (x k,1 ), \u2022 \u2022 \u2022 , \u2207f n (x k,n ) \u2208 R d\u00d7n ,(54)\nG k+1 = [\u2207f 1 (x k,1 ), \u2022 \u2022 \u2022 , \u2207f n (x k,n )] \u2208 R d\u00d7n ,(55)\nX k = [x k,1 , \u2022 \u2022 \u2022 , x k,n ] \u2208 R d\u00d7n ,(56)\nY k = y k,1 , \u2022 \u2022 \u2022 , y k,n \u2208 R d\u00d7n ,(57)\nwhere \u2207f i denotes the stochastic gradient oracle on worker i, and \u2207f i denotes the full gradient oracle on worker i. We use X denote X 1 n for any matrix X with appropriate shape. We use \u03bb i (W ) to denote the i-th general largest eigenvalue of matrix W . Under such notation, \u03bb in the main paper is equavilent to \u03bb 2 (W ). We use M(\u2022) to denote the R-step accelerated gossip which has the following property [87]:\nM(X) \u2212 X \u2264 \u03c1 X \u2212 X ; M(X) 1 n = X 1 n ,(58)\nwhere\n\u03c1 = 1 \u2212 1 \u2212 \u03bb 2 (W ) R .\nThe proof to the statement of Equation ( 58) can be found in [88]. For the stochastic oracle, based on the oracle class assumption, we have\nE \u2207f i (x) \u2212 \u2207f i (x) 2 \u2264 \u03c3 2 ,(59)\nand we denote\u03c3 2 = \u03c3 2 BR as the variance of mini-batch of R. First, from the update rule of DeTAG,\nY k = M(Y k\u22121 +G k\u22121 \u2212G k\u22122 ) 1 n = Y k\u22121 +G k\u22121 \u2212G k\u22122 = Y \u22121 + k\u22121 j=\u22121 (G j \u2212G j\u22121 ) =G k\u22121(60)\nand\nX k+1 = M(X k \u2212 \u03b1Y k ) 1 n = X k \u2212 \u03b1Y k .(61)\nBy Taylor Theorem, we obtain\nEf X k+1 =Ef X k \u2212 \u03b1Y k (62) \u2264Ef X k \u2212 \u03b1E \u2207f X k , Y k + \u03b1 2 L 2 E Y k 2 (63\n) (60) = Ef X k \u2212 \u03b1E \u2207f X k , G k\u22121 + \u03b1 2 L 2 E G k\u22121 2 . (64\n)\nFor the last term, we have\nE G k\u22121 2 =E G k\u22121 2 + E G k\u22121 \u2212G k\u22121 2 + 2E G k\u22121 , G k\u22121 \u2212G k\u22121 (65\n)\n=E G k\u22121 2 + E G k\u22121 \u2212G k\u22121 2 (66\n)\n=E G k\u22121 2 + 1 n 2 n i=1 E G k\u22121 e i \u2212G k\u22121 e i 2 (67\n)\n\u2264E G k\u22121 2 +\u03c3 2 n ,(68)\nwhere in the second step, we use the fact that the sampling noise is independent of the gradient itself. Putting it back we obtain\nEf X k+1 \u2264Ef X k \u2212 \u03b1E \u2207f X k , G k\u22121 + \u03b1 2 L 2 E G k\u22121 2 + \u03b1 2\u03c32 L 2n (69) =Ef X k \u2212 \u03b1 2 E \u2207f X k 2 \u2212 \u03b1 \u2212 \u03b1 2 L 2 G k\u22121 2 + \u03b1 2\u03c32 L 2n + \u03b1 2 E G k\u22121 \u2212 \u2207f X k 2 ,(70)\nwhere the last step we use 2 a, b = a 2 + b 2 \u2212 a \u2212 b 2 . Expand the last term, we obtain\nE G k\u22121 \u2212 \u2207f X k 2 (71) \u22642E G k\u22121 \u2212 G k+1 2 + 2E G k+1 \u2212 \u2207f X k 2 (72) =2E 1 n n i=1 \u2207f i (x k,i ) \u2212 1 n n i=1 \u2207f i (x k\u22122,i ) 2 + 2E 1 n n i=1 \u2207f i (x k,i ) \u2212 1 n n i=1 \u2207f i (X k ) 2 (73) \u2264 2 n n i=1 E \u2207f i (x k,i ) \u2212 \u2207f i (x k\u22122,i ) 2 + 2 n n i=1 E \u2207f i (x k,i ) \u2212 \u2207f i (X k ) 2 (74) \u2264 2L 2 n E X k \u2212 X k\u22122 2 F + 2L 2 n E X k \u2212 X k 1 n 2 F .(75)\nDenote f (0) \u2212 f * \u2264 \u2206, we obtain\nK\u22121 k=0 \u03b1(1 \u2212 \u03b1L) G k 2 + K\u22121 k=0 \u03b1E \u2207f X k 2 (76\n)\n\u22642\u2206 + \u03b1 2\u03c32 LK n + 2\u03b1L 2 n K\u22121 k=0 E X k \u2212 X k 1 n 2 F + 2\u03b1L 2 n K\u22121 k=0 E X k \u2212 X k\u22122 2 F (77\n)\n\u22642\u2206 + \u03b1 2\u03c32 LK n + 16\u03b1L 2 n K\u22121 k=0 E X k \u2212 X k 1 n 2 F + 6\u03b1L 2 n K\u22121 k=0 E X k 1 n \u2212 X k\u22122 1 n 2 F ,(78)\nwhere in the last step we use\n2\u03b1L 2 n K\u22121 k=0 E X k \u2212 X k\u22122 2 F (79) \u2264 6\u03b1L 2 n K\u22121 k=0 E X k \u2212 X k 1 n 2 F + 6\u03b1L 2 n K\u22121 k=0 E X k\u22122 \u2212 X k\u22122 1 n 2 F + 6\u03b1L 2 n K\u22121 k=0 E X k 1 n \u2212 X k\u22122 1 n 2 F .(80)\nIn addition, for the last term we have\n6\u03b1L 2 n K\u22121 k=0 E X k 1 n \u2212 X k\u22122 1 n 2 F = 6\u03b1L 2 n n K\u22121 k=0 E X k \u2212 X k\u22122 2 (81\n)\n(61) = 24\u03b1 3 L 2 n n K\u22121 k=0 E G k 2 (82)(65)\n\u2264 24\u03b1 3 L 2 K\u22121 k=0 E G k 2 + 24\u03b1 3\u03c32 L 2 K n .(83)\nPush it back we have\nK\u22121 k=0 \u03b1(1 \u2212 \u03b1L \u2212 24\u03b1 2 L 2 ) G k 2 + K\u22121 k=0 \u03b1E \u2207f X k 2 (84\n)\n\u22642\u2206 + \u03b1 2\u03c32 LK n + 16\u03b1L 2 n K\u22121 k=0 E X k \u2212 X k 1 n 2 F + 24\u03b1 3\u03c32 L 2 K n . (85\n)\nThe rest of the proof is to bound\n16\u03b1L 2 n K\u22121 k=0 E X k \u2212 X k 1 n 2 F . We start from X k+1 \u2212 X k+1 1 n 2 F (86) (61) = M(X k \u2212 \u03b1Y k ) \u2212 (X k \u2212 \u03b1Y k )1 n 2 F (87) = M(X k ) \u2212 X k 1 n 2 F \u2212 2\u03b1 M(X k ) \u2212 X k 1 n , M(Y k ) \u2212 Y k 1 n + \u03b1 2 M(Y k ) \u2212 Y k 1 n 2 F (88) (58) \u2264 \u03c1 2 X k \u2212 X k 1 n 2 F + \u03c1 2 (1 \u2212 \u03c1 2 ) 1 + \u03c1 2 X k \u2212 X k 1 n 2 F + \u03c1 2 (1 + \u03c1 2 )\u03b1 2 1 \u2212 \u03c1 2 Y k \u2212 Y k 1 n 2 F (89\n)\n+ \u03b1 2 \u03c1 2 Y k \u2212 Y k 1 n 2 F (90) = 2\u03c1 2 (1 + \u03c1 2 ) X k \u2212 X k 1 n 2 F + 2\u03c1 2 \u03b1 2 1 \u2212 \u03c1 2 Y k \u2212 Y k 1 n 2 F ,(91)\nwhere in the third step we use\n\u22122 a, b \u2264 1 \u2212 \u03c1 2 1 + \u03c1 2 a 2 + 1 + \u03c1 2 1 \u2212 \u03c1 2 b 2 . (92\n)\nSimilarly, for Y k+1 , we obtain\nE Y k+1 \u2212 Y k+1 1 n 2 F (93\n)\n(60) = E M(Y k +G k \u2212G k\u22121 ) \u2212 (Y k +G k \u2212G k\u22121 )1 n 2 F (94\n)\n=E M(Y k ) \u2212 Y k 1 n 2 F + E M(G k \u2212G k\u22121 ) \u2212 (G k \u2212G k\u22121 )1 n 2 F (95\n)\n+ 2E M(Y k ) \u2212 Y k 1 n , M(G k \u2212G k\u22121 ) \u2212 (G k \u2212G k\u22121 )1 n (96\n)\n(92)(58) \u2264 \u03c1 2 E Y k \u2212 Y k 1 n 2 F + \u03c1 2 E G k \u2212 G k\u22121 \u2212 (G k \u2212 G k\u22121 )1 n 2 F (97) + (1 \u2212 \u03c1 2 )\u03c1 2 1 + \u03c1 2 E Y k \u2212 Y k 1 n 2 F + (1 + \u03c1 2 )\u03c1 2 1 \u2212 \u03c1 2 E G k \u2212 G k\u22121 \u2212 (G k \u2212 G k\u22121 )1 n 2 F (98\n)\n+ 2\u03c1 2 E G k \u2212G k 2 F + 2\u03c1 2 E G k\u22121 \u2212G k\u22121 2 F + 2\u03c1 2 E G k 1 n \u2212G k 1 n 2 F + 2\u03c1 2 E G k\u22121 1 n \u2212G k\u22121 1 n 2 F (99\n) \u2264 2\u03c1 2 1 + \u03c1 2 E Y k \u2212 Y k 1 n 2 F + 4\u03c1 2 1 \u2212 \u03c1 2 E G k+2 \u2212 G k+1 2 F (100) + 4\u03c1 2 1 \u2212 \u03c1 2 E G k+2 \u2212 G k+1 \u2212 G k + G k\u22121 2 F + 8n\u03c1 2\u03c32 ,(101)\nwhere in the last step we use I \u2212 11 n \u2264 1 and AB F \u2264 A F B . For the second term, we have\nE G k+2 \u2212 G k+1 2 F (102) = n i=1 E \u2207f (x k+1,i ) \u2212 \u2207f (x k,i ) 2 (103) \u2264L 2 n i=1 E x k+1,i \u2212 x k,i 2 (104) =L 2 E X k+1 \u2212 X k 2 F (105) (61) = L 2 E M(X k ) \u2212 X k \u2212 \u03b1M(Y k ) 2 F (106) =L 2 E M(X k \u2212 X k 1 n ) \u2212 (X k \u2212 X k 1 n ) \u2212 \u03b1M(Y k ) 2 F (107) \u22644L 2 E M(X k ) \u2212 X k 1 n 2 F + 4L 2 E X k \u2212 X k 1 n 2 F + 4\u03b1 2 L 2 E M(Y k ) \u2212 Y k 1 n 2 F (108) + 4\u03b1 2 nL 2 E Y k 2 (109) \u22644(1 + \u03c1 2 )L 2 E X k \u2212 X k 1 n 2 F + 4\u03b1 2 \u03c1 2 L 2 E Y k \u2212 Y k 1 n 2 F + 4\u03b1 2 nL 2 E Y k 2 . (110\n)\nPutting it back we obtain\nE Y k+1 \u2212 Y k+1 1 n 2 F (111\n) \u2264 2\u03c1 2 1 + \u03c1 2 + 16\u03b1 2 \u03c1 4 L 2 1 \u2212 \u03c1 2 E Y k \u2212 Y k 1 n 2 F + 16\u03c1 2 (1 + \u03c1 2 )L 2 1 \u2212 \u03c1 2 E X k \u2212 X k 1 n 2 F + 16\u03b1 2 \u03c1 2 nL 2 1 \u2212 \u03c1 2 E Y k 2 (112) 4\u03c1 2 1 \u2212 \u03c1 2 E G k+2 \u2212 G k+1 \u2212 G k + G k\u22121 2 F + 8n\u03c1 2\u03c32 . (113\n)\nCombining Equation (91) and Equation (100), we have\nE X k+1 \u2212 X k+1 1 n 2 F E Y k+1 \u2212 Y k+1 1 n 2 F P 11 P 12 P 21 P 22 E X k \u2212 X k 1 n 2 F E Y k \u2212 Y k 1 n 2 F (114) + 0 4\u03c1 2 1\u2212\u03c1 2 E U k 2 F + 16\u03b1 2 \u03c1 2 nL 2 1\u2212\u03c1 2 E Y k 2 + 8n\u03c1 2\u03c32 ,(115)\nwhere\nP 11 = 2\u03c1 2 (1 + \u03c1 2 ) (116\n)\nP 12 = 2\u03c1 2 \u03b1 2 1 \u2212 \u03c1 2(117)\nP 21 = 16\u03c1 2 (1 + \u03c1 2 )L 2 1 \u2212 \u03c1 2(118)\nP 22 = 2\u03c1 2 1 + \u03c1 2 + 16\u03b1 2 \u03c1 4 L 2 1 \u2212 \u03c1 2 (119) U k =G k+2 \u2212 G k+1 \u2212 G k + G k\u22121 . (120\n)\nFor simplicity, define \nz k = E X k+1 \u2212 X k+1 1 n 2 F E Y k+1 \u2212 Y k+1 1 n 2 F(\nu k = 0 4\u03c1 2 1\u2212\u03c1 2 E U k 2 F + 16\u03b1 2 \u03c1 2 nL 2 1\u2212\u03c1 2 E Y k 2 + 8n\u03c1 2\u03c32 ,(123)\nthen we can write this linear system as\nz k P z k\u22121 + u k\u22121 P k z 0 + k\u22121 t=0 P k\u2212t u t ,(124)\nfor simplicity.\nLet \u03bb 1 (P ), \u03bb 2 (P ) denote the two eigenvalues of P (without the loss of generality, we denote \u03bb 1 (P ) < \u03bb 2 (P )), define \u03a8 = (P 11 \u2212 P 22 ) 2 + 4P 12 P 21 , (\nthen with eigendecomposition, we obtain \u03bb 1 (P ) = P 11 + P 22 \u2212 \u03a8 2 (126) \u03bb 2 (P ) =\nP 11 + P 22 + \u03a8 2 = 2\u03c1 2 1 + \u03c1 2 + 8\u03b1 2 \u03c1 4 L 2 1 \u2212 \u03c1 2 + 16\u03b1\u03c1 2 L \u03b1 2 \u03c1 4 L 2 + (1 + \u03c1 2 ) 1 \u2212 \u03c1 2 (127) P k \u03bb k 1 (P )+\u03bb k 2 (P ) 2 + (P 11 \u2212P 22)(\u03bb k 2 (P )\u2212\u03bb k 1 (P )) 2\u03a8 P 12 \u03a8 (\u03bb k 2 (P ) \u2212 \u03bb k 1 (P )) P 21 \u03a8 (\u03bb k 2 (P ) \u2212 \u03bb k 1 (P )) \u03bb k 1 (P )+\u03bb k 2 (P ) 2 + (P 11\u2212P 22)(\u03bb k 1 (P )\u2212\u03bb k 2 (P )) 2\u03a8 ,(128)\nwhen the step size is small enough such that\n\u03b1L < (1 \u2212 \u03c1) 2 32 ,(129)\nit can be verified that \u03bb 2 (P ) \u2264 \u221a \u03c1+\u03c1 1+\u03c1 , and then we can compute the E X k \u2212 X k 1 n 2 and E Y k \u2212 Y k 1 n 2 . We use X[1 :] to denote the first row of matrix X. First for X k , we obtain:\nP k z 0 [1 :] \u2264 P 12 k\u03bb k\u22121 2 (P )E Y 0 \u2212 Y 0 1 n 2 F = 2\u03c1 2 \u03b1 2 k 1 \u2212 \u03c1 2 \u03bb k\u22121 2 (P )E Y 0 \u2212 Y 0 1 n 2 F (130)\nwhere we use the property that \u03bb k 2 (P ) \u2212 \u03bb k 1 (P ) = (\u03bb 2 (P ) \u2212 \u03bb 1 (P ))\nk\u22121 l=0 \u03bb 2 (P ) l \u03bb 1 (P ) k\u22121\u2212l = \u03a8k\u03bb k\u22121 2\n(P ) and, similarly\nP k\u2212t u t [1 :] (131) \u2264 2\u03c1 2 \u03b1 2 (k \u2212 t) 1 \u2212 \u03c1 2 \u03bb k\u2212t\u22121 2 (P ) 4\u03c1 2 1 \u2212 \u03c1 2 E U t 2 F + 16\u03b1 2 \u03c1 2 nL 2 1 \u2212 \u03c1 2 E Y t 2 + 8n\u03c1 2\u03c32 (132) = 2\u03c1 2 \u03b1 2 (k \u2212 t) 1 \u2212 \u03c1 2 \u03bb k\u2212t\u22121 2 (P ) 4\u03c1 2 1 \u2212 \u03c1 2 E U t 2 F + 16\u03b1 2 \u03c1 2 nL 2 1 \u2212 \u03c1 2 E G t\u22121 2 + 8n\u03c1 2\u03c32 (133)(65)\n\u2264 2\u03c1 2 \u03b1 2 (k \u2212 t) 1 \u2212 \u03c1 2 \u03bb k\u2212t\u22121 2 (P ) 4\u03c1 2 1 \u2212 \u03c1 2 E U t 2 F + 16\u03b1 2 \u03c1 2 nL 2 1 \u2212 \u03c1 2 E G t\u22121 2 + 16\u03b1 2 \u03c1 2\u03c32 L 2 1 \u2212 \u03c1 2 + 8n\u03c1 2\u03c32 , (134)\nthen we obtain\nE X k \u2212 X k 1 n 2 F (135) \u2264 2\u03c1 2 \u03b1 2 k 1 \u2212 \u03c1 2 \u03bb k\u22121 2 (P )E Y 0 \u2212 Y 0 1 n 2 F (136) + k\u22121 t=0 2\u03c1 2 \u03b1 2 (k \u2212 t) 1 \u2212 \u03c1 2 \u03bb k\u2212t\u22121 2 (P ) 4\u03c1 2 1 \u2212 \u03c1 2 E U t 2 F + 16\u03b1 2 \u03c1 2 nL 2 1 \u2212 \u03c1 2 E G t\u22121 2 + 16\u03b1 2 \u03c1 2\u03c32 L 2 1 \u2212 \u03c1 2 + 8n\u03c1 2\u03c32 . (137\n)\nSumming over k = 0 to K \u2212 1 we obtain K\u22121 k=0 E X k \u2212 X k 1 n 2 F (138) \u2264 2\u03c1 2 \u03b1 2 (1 \u2212 \u03c1 2 )(1 \u2212 \u03bb 2 (P )) 2 K\u22121 k=0 E Y 0 \u2212 Y 0 1 n 2 F (139) + 2\u03c1 2 \u03b1 2 (1 \u2212 \u03c1 2 )(1 \u2212 \u03bb 2 (P )) 2 K\u22121 k=0 4\u03c1 2 1 \u2212 \u03c1 2 E U k 2 F + 16\u03b1 2 \u03c1 2 nL 2 1 \u2212 \u03c1 2 E G k 2 + 16\u03b1 2 \u03c1 2\u03c32 L 2 1 \u2212 \u03c1 2 + 8n\u03c1 2\u03c32 (140) \u2264 2\u03c1 2 \u03b1 2 (1 + \u03c1)nK\u03c2 2 0 (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 + 32\u03c1 4 \u03b1 4 nL 2 (1 \u2212 \u03c1) 2 (1 \u2212 \u221a \u03c1) 2 K\u22121 k=0 E G k 2 + 8\u03c1 4 \u03b1 2 (1 \u2212 \u03c1) 2 (1 \u2212 \u221a \u03c1) 2 K\u22121 k=0 E U k 2 F (141) + 32\u03c1 4 \u03b1 4\u03c32 L 2 K (1 \u2212 \u03c1) 2 (1 \u2212 \u221a \u03c1) 2 + 8\u03c1 4 \u03b1 2 n\u03c3 2 (1 + \u03c1)K (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 ] (142) \u2264 2\u03c1 2 \u03b1 2 (1 + \u03c1)nK\u03c2 2 0 (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 + 32\u03c1 4 \u03b1 4 nL 2 (1 \u2212 \u03c1) 2 (1 \u2212 \u221a \u03c1) 2 K\u22121 k=0 E G k 2 + 8\u03c1 4 \u03b1 2 (1 \u2212 \u03c1) 2 (1 \u2212 \u221a \u03c1) 2 K\u22121 k=0 E U k 2 F (143) + 16\u03c1 4 \u03b1 2 n\u03c3 2 (1 + \u03c1)K (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 ,(144)\nwhere in the second step we used\n1 1\u2212\u03bb2(P ) < 1+\u03c1 1\u2212 \u221a \u03c1 since \u03bb 2 (P ) \u2264 \u221a \u03c1+\u03c1\n1+\u03c1 . And the third step holds due to Equation (129). We proceed to analyze the case in Y k : we first have\n[P k ] 22 = \u03bb k 1 (P ) + \u03bb k 2 (P ) 2 + (P 11 \u2212 P 22 )(\u03bb k 1 (P ) \u2212 \u03bb k 2 (P )) 2\u03a8 (145\n)\n\u2264\u03bb k 2 (P ) + 8\u03b1 2 \u03c1 4 L 2 k\u03bb k\u22121 2 (P ) 1 \u2212 \u03c1 2 ,(146)\nthen we can have\nP k z 0 [2 :] \u2264 \u03bb k 2 (P ) + 8\u03b1 2 \u03c1 4 L 2 k\u03bb k\u22121 2 (P ) 1 \u2212 \u03c1 2 E Y 0 \u2212 Y 0 1 n 2 F ,(147)\nand\nP k\u2212t u t [2 :] (148) \u2264 \u03bb k\u2212t 2 (P ) + 8\u03b1 2 \u03c1 4 L 2 (k \u2212 t)\u03bb k\u2212t\u22121 2 (P ) 1 \u2212 \u03c1 2 \u2022 4\u03c1 2 1 \u2212 \u03c1 2 E U t 2 F + 16\u03b1 2 \u03c1 2 nL 2 1 \u2212 \u03c1 2 E Y t 2 + 8n\u03c1 2\u03c32 (149) \u2264 \u03bb k\u2212t 2 (P ) + 8\u03b1 2 \u03c1 4 L 2 (k \u2212 t)\u03bb k\u2212t\u22121 2 (P ) 1 \u2212 \u03c1 2 \u2022 4\u03c1 2 1 \u2212 \u03c1 2 E U t 2 F + 16\u03b1 2 \u03c1 2 nL 2 1 \u2212 \u03c1 2 E G t\u22121 2 + 16\u03b1 2 \u03c1 2\u03c32 L 2 1 \u2212 \u03c1 2 + 8n\u03c1 2\u03c32 .(150)\nSumming over k = 0 to K \u2212 1, we obtain\nK\u22121 k=0 E Y k \u2212 Y k 1 n 2 (151) \u2264 (1 + \u03c1)nK\u03c2 2 0 1 \u2212 \u221a \u03c1 + 8\u03b1 2 \u03c1 4 (1 + \u03c1)L 2 nK\u03c2 2 0 (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 (152) + 1 + \u03c1 1 \u2212 \u221a \u03c1 + 8\u03b1 2 \u03c1 4 (1 + \u03c1)L 2 (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 K\u22121 k=0 4\u03c1 2 1 \u2212 \u03c1 2 E U k 2 F + 16\u03b1 2 \u03c1 2 nL 2 1 \u2212 \u03c1 2 E G k 2 + 16\u03b1 2 \u03c1 2\u03c32 L 2 1 \u2212 \u03c1 2 + 8n\u03c1 2\u03c32 .(153)\nWe next solve U k F , from the definition of U k we obtain that\nK\u22121 k=0 E U k 2 F = K\u22121 k=0 E G k+2 \u2212 G k+1 \u2212 G k + G k\u22121 2 F (154) \u22642 K\u22121 k=0 E G k+2 \u2212 G k+1 2 F + 2 K\u22121 k=0 E G k \u2212 G k\u22121 2 F (155) \u22644 K\u22121 k=0 E G k+2 \u2212 G k+1 2 F (156) =4 K\u22121 k=0 n i=1 E \u2207f (x k+1,i ) \u2212 \u2207f (x k,i ) 2 (157) \u22644L 2 K\u22121 k=0 n i=1 E x k+1,i \u2212 x k,i 2 (158) =4L 2 K\u22121 k=0 E X k+1 \u2212 X k 2 F .(159)\nFit in the derivation from Equation ( 110) we obtain\nK\u22121 k=0 E U k 2 F (160) \u22644L 2 K\u22121 k=0 E X k+1 \u2212 X k 2 F (161) \u226416(1 + \u03c1 2 )L 2 K\u22121 k=0 E X k \u2212 X k 1 n 2 F + 16\u03b1 2 \u03c1 2 L 2 K\u22121 k=0 E Y k \u2212 Y k 1 n 2 F + 16\u03b1 2 nL 2 K\u22121 k=0 E Y k 2 (162) \u2264 32\u03c1 2 \u03b1 2 (1 + \u03c1) 2 nK\u03c2 2 0 L 2 (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 + 512\u03c1 4 (1 + \u03c1)\u03b1 4 nL 4 (1 \u2212 \u03c1) 2 (1 \u2212 \u221a \u03c1) 2 K\u22121 k=0 E G k 2 + 256\u03c1 4 (1 + \u03c1)\u03b1 2 L 2 (1 \u2212 \u03c1) 2 (1 \u2212 \u221a \u03c1) 2 K\u22121 k=0 E U k 2 F (163) + 256\u03c1 4 \u03b1 2 n\u03c3 2 (1 + \u03c1) 2 KL 2 (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 (164) + 16\u03b1 2 \u03c1 2 (1 + \u03c1)nK\u03c2 2 0 L 2 1 \u2212 \u221a \u03c1 + 128\u03b1 4 \u03c1 6 (1 + \u03c1)L 4 nK\u03c2 2 0 (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 (165) + 16\u03b1 2 \u03c1 2 (1 + \u03c1)L 2 1 \u2212 \u221a \u03c1 + 128\u03b1 4 \u03c1 6 (1 + \u03c1)L 4 (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 K\u22121 k=0 4\u03c1 2 1 \u2212 \u03c1 2 E U k 2 F (166) + 16\u03b1 2 \u03c1 2 (1 + \u03c1)L 2 1 \u2212 \u221a \u03c1 + 128\u03b1 4 \u03c1 6 (1 + \u03c1)L 4 (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 K\u22121 k=0 16\u03b1 2 \u03c1 2 nL 2 1 \u2212 \u03c1 2 E G k 2 + 16\u03b1 2 \u03c1 2\u03c32 L 2 1 \u2212 \u03c1 2 + 8n\u03c1 2\u03c32 (167) + 16\u03b1 2 nL 2 K\u22121 k=0 E Y k 2 (168) \u2264 64\u03c1 2 \u03b1 2 (1 + \u03c1) 2 nK\u03c2 2 0 L 2 (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 + 512\u03c1 4 (1 + \u03c1)\u03b1 2 L 2 (1 \u2212 \u03c1) 2 (1 \u2212 \u221a \u03c1) 2 K\u22121 k=0 E U k 2 F + 512\u03c1 4 \u03b1 2 n\u03c3 2 (1 + \u03c1) 2 KL 2 (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 (169) + 32\u03b1 2 nL 2 K\u22121 k=0 E G k 2 ,(170)\nwhere in the third step we use the derivation from Equation (138) and (151), in the fourth step we repeatedly use Equation (129) and Equation ( 65), solve it we obtain\nK\u22121 k=0 E U k 2 F \u2264 128\u03c1 2 \u03b1 2 (1 + \u03c1) 2 nK\u03c2 2 0 L 2 (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 + 1024\u03c1 4 \u03b1 2 n\u03c3 2 (1 + \u03c1) 2 KL 2 (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 + 64\u03b1 2 nL 2 K\u22121 k=0 E G k 2 , (171\n)\nwhere again we use Equation ( 129), combine it with Equation ( 138) we obtain\nK\u22121 k=0 E X k \u2212 X k 1 n 2 F (172) \u2264 4\u03c1 2 \u03b1 2 (1 + \u03c1)nK\u03c2 2 0 (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 + 544\u03c1 4 \u03b1 4 nL 2 (1 \u2212 \u03c1) 2 (1 \u2212 \u221a \u03c1) 2 K\u22121 k=0 E G k 2 + 32\u03c1 4 \u03b1 2 n\u03c3 2 (1 + \u03c1)K (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 ,(173)\nwhere we use Equation (129).\nRecall from Equation ( 84) that\nK\u22121 k=0 \u03b1(1 \u2212 \u03b1L \u2212 24\u03b1 2 L 2 ) G k 2 + K\u22121 k=0 \u03b1E \u2207f X k 2 (174\n)\n\u22642\u2206 + \u03b1 2\u03c32 LK n + 16\u03b1L 2 n K\u22121 k=0 E X k \u2212 X k 1 n 2 F + 24\u03b1 3\u03c32 L 2 K n .(175)\nCombine Equation ( 129) and (173), we obtain\n1 K K\u22121 k=0 E \u2207f X k 2 \u2264 O \u2206 \u03b1K + \u03b1\u03c3 2 L n + \u03c1 2 \u03b1 2 L 2 \u03c2 2 0 (1 \u2212 \u03c1) 3 + \u03c1 4 \u03b1 2\u03c32 L 2 (1 \u2212 \u03c1) 3 + \u03b1 2\u03c32 L 2 n ,(176)\nwhere we omit the numerical constants. Set\n\u03b1 = 1 \u03c3 KL/n\u2206 + \u03c1 2 3 L 2 3 \u03c2 2 3 0 K 1 3 \u2206 1 3 (1\u2212\u03c1) + 32L (1\u2212\u03c1) 2 ,(177)\nwe obtain\n1 K K\u22121 k=0 E \u2207f X k 2 \u2264 O \u221a \u2206L\u03c3 \u221a nK + (\u03c1\u2206L\u03c2 0 ) 2 3\n(1 \u2212 \u03c1)K\n2 3 + \u03c1 2 n\u2206L (1 \u2212 \u03c1) 3 K + \u2206L (1 \u2212 \u03c1) 2 K . (178\n)\nFit in T = KR and\u03c3 2 = \u03c3 2 /BR, we obtain\n1 K K\u22121 k=0 E \u2207f X k 2 \u2264 O \u221a \u2206L\u03c3 \u221a nBT + (\u03c1\u2206L\u03c2 0 R) 2 3\n(1 \u2212 \u03c1)T\n2 3 + \u03c1 2 nR\u2206L (1 \u2212 \u03c1) 3 T + R\u2206L (1 \u2212 \u03c1) 2 T ,(179)\nset R = 1 1 \u2212 \u03bb 2 (W ) max 1 2 log(n), 1 2 log \u03c2 2 0 T \u2206L , we first have \u03c1 \u2264 1/ \u221a 2 since R \u2265 log(n) 2 1 \u2212 \u03bb 2 (W ) \u2265 \u2212 log(n) 2 log(1 \u2212 1 \u2212 \u03bb 2 (W )) \u21d2 1 \u2212 1 \u2212 \u03bb 2 (W ) R \u2264 1 \u221a n \u2264 1 \u221a 2 ,(180)\nthis implies \n1 K K\u22121 k=0 E \u2207f X k 2 \u2264 O \u221a \u2206L\u03c3 \u221a nBT + (\u03c1\u2206L\u03c2 0 R)\nwith the assignment of R, \u03c1 2 n < 1 and \u03c1\u03c2 0 \u221a T / \u221a \u2206L < 1, so since it also holds that R \u2265 1 (and so R 2/3 \u2264 R), \nwhen\nT \u2264 O \u2206L\u03c3 2 nB 4 ,(187)\nwe have \u221a \u2206L\u03c3 \u221a nBT \u2264 O( 2 ).(188)\nOn the other hand, when\nT \u2264 O max log(n)\u2206L 2 1 \u2212 \u03bb 2 (W ) , \u2206L 2 1 \u2212 \u03bb 2 (W ) log \u03c2 2 0 2 \u2206L ,(189)\nwe have\n\u2206L T 1 \u2212 \u03bb 2 (W )\n\u2022 max log(n), log\n\u03c2 2 0 T \u2206L \u2264 O( 2 ),(190)\nto see this, note that\n\u2206L T 1 \u2212 \u03bb 2 (W ) log \u03c2 2 0 T \u2206L = 2 log \u03c2 2 0 2 \u2206L log \u03c2 2 0 2 \u2206L log \u03c2 2 0 2 \u2206L \u2264 O( 2 ).(191)\nFinally, we can obtain the upper bound\nT \u2264O \u2206L\u03c3 2 nB 4 + max log(n)\u2206L 2 1 \u2212 \u03bb 2 (W ) , \u2206L 2 1 \u2212 \u03bb 2 (W ) log \u03c2 2 0 2 \u2206L (192\n)\n=O \u2206L\u03c3 2 nB 4 + \u2206L 2 1 \u2212 \u03bb 2 (W ) log n + \u03c2 0 n \u221a \u2206L ,(193)\nas desired.", "publication_ref": ["b86", "b87"], "figure_ref": [], "table_ref": []}, {"heading": "C Details to footnotes", "text": "C.1 Asynchronous Algorithm (Footnote 2)\nIn the full paper, we focus on the synchronous algorithms, i.e., we assume the existence of a synchronization process among workers between two adjacent iterations. We now extend our formulation to asynchronous algorithms. Since workers now update and communicate asynchronously, we define any gradient update that took place on a randomly chosen worker as one iteration. This randomness depends on system implementation, stochastic events, etc. This is a commonly adopted definition in the analysis of (decentralized) asynchronous algorithms [13]. To obtain a lower bound in such case, consider the two settings as shown in the proof of Theorem 1. In setting 1, it can be easily verified that the lower bound for sample complexity is\n\u2126 \u2206L\u03c3 2 B 4 . (194\n)\nThis holds because in the extreme case, only one worker is making contributions to the optimization. And since we have not made any assumption on how workers are sampled to conduct the next iteration, this is a valid bound for arbitrary distribution. On the other hand, considering setting 2, the lower bound is still \u2126(T 0 D) where T 0 = \u2126(\u2206L \u22122 ) is the lower bound in the sequential case, since the systems need at least \u2126(D) iterations for the workers in I 0 and I 2 to contact. The lower bound for communication complexity is then\n\u2126 \u2206LD 2 . (195\n)\nCombining them together, we can get the final lower bound as:\n\u2126 \u2206L\u03c3 2 B 4 + \u2206LD 2 . (196\n)\nNote that this bound holds with probability 1. It is possible to propose finer-grained assumption on how workers are chosen (e.g. uniformly random) and use concentration inequalities (e.g. Hoeffding's inequality) to get tighter bounds, we leave this as future work.", "publication_ref": ["b12"], "figure_ref": [], "table_ref": []}, {"heading": "C.2 Relax zero-respecting assumption (Footnote 3)", "text": "To relax the zero-respecting assumption, we can use the technique proposed by [66] (See their proofs to Proposition 1 and 2). The basic idea is that to adversarially construct the loss function and rotate the non-zero coordinates in t-th iterations, such that when the algorithm operates on the rotated function, the first t iterations match with that of the old function. However, the new rotated function is still zero-respecting to the algorithm after t-th iteration so is generally hard to optimize. The details can be found in [66].\nC.3 Specific algorithm for Average Consensus (Footnote 8)\nMany algorithms have been proposed on solving the Average Consensus problem, readers can find details in many previous works on graph theory such as [82,83,84]. A straightforward algorithm is the Minimum Spanning Tree, that is, we first generate a spanning tree of the graph, and then the workers send and receive message using propagation on the tree. Specifically, starting from the leaves, all the children nodes of the tree send its accumulated value to the parents and the root compute the averaged value after gathering the information from the graph. And then reversely, the parent nodes send the value back to the child nodes and eventually all the nodes will get the averaged value. This algorithm is also known as the GATHER-PROPAGATE algorithm as discussed in [83], section 3. We include the detailed pseudo-code 11 in Algorithm 4.", "publication_ref": ["b65", "b65", "b81", "b82", "b83", "b82"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgement", "text": "This work is supported by NSF IIS-2046760. The authors would like to thank A. Feder Cooper, Jerry Chee, Zheng Li, Ran Xin, Jiaqi Zhang and anonymous reviewers from ICML 2021 for providing valuable feedbacks on earlier versions of this paper.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 4 GATHER-PROPAGATE (Spanning Tree) for a single coordinate", "text": "Input: communication graph G, a single coordinate on workers (all the coordinates follow the same instructions) to be communicated X \u2208 R n . 1: d \u2190 vector of 1's indexed by V (G) (vertices set of graph G).\n2: I \u2190 a spnning tree of G with root r arbitrarily picked.  v gives all its value onto its parents u:\ndu \u2190 du + dv re-distribute the results: ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Tensorflow: A system for large-scale machine learning", "journal": "", "year": "2016", "authors": "Mart\u00edn Abadi; Paul Barham; Jianmin Chen; Zhifeng Chen; Andy Davis; Jeffrey Dean; Matthieu Devin; Sanjay Ghemawat; Geoffrey Irving; Michael Isard"}, {"ref_id": "b1", "title": "A brief tutorial on distributed and concurrent machine learning", "journal": "", "year": "2018", "authors": "Dan Alistarh"}, {"ref_id": "b2", "title": "Elastic consistency: A general consistency model for distributed stochastic gradient descent", "journal": "", "year": "2020", "authors": "Dan Alistarh; Bapi Chatterjee; Vyacheslav Kungurtsev"}, {"ref_id": "b3", "title": "Mixml: A unified analysis of weakly consistent parallel learning", "journal": "", "year": "2020", "authors": "Yucheng Lu; Jack Nash; Christopher De Sa"}, {"ref_id": "b4", "title": "Scaling distributed machine learning with the parameter server", "journal": "", "year": "2014", "authors": "Mu Li; Jun Woo David G Andersen; Alexander J Park; Amr Smola; Vanja Ahmed; James Josifovski; Eugene J Long; Bor-Yiing Shekita;  Su"}, {"ref_id": "b5", "title": "Communication efficient distributed machine learning with the parameter server", "journal": "", "year": "2014", "authors": "Mu Li; G David; Alexander J Andersen; Kai Smola;  Yu"}, {"ref_id": "b6", "title": "More effective distributed ml via a stale synchronous parallel parameter server", "journal": "", "year": "2013", "authors": "Qirong Ho; James Cipar; Henggang Cui; Seunghak Lee; Jin Kyu Kim; Phillip B Gibbons; Garth A Gibson; Greg Ganger; Eric P Xing"}, {"ref_id": "b7", "title": "Using MPI-2: Advanced features of the message passing interface", "journal": "MIT press", "year": "1999", "authors": "William Gropp; Rajeev Thakur; Ewing Lusk"}, {"ref_id": "b8", "title": "Bandwidth optimal all-reduce algorithms for clusters of workstations", "journal": "Journal of Parallel and Distributed Computing", "year": "2009", "authors": "Pitch Patarasuk; Xin Yuan"}, {"ref_id": "b9", "title": "Decentralized deep learning with arbitrary communication compression", "journal": "", "year": "2019", "authors": "Anastasia Koloskova; Tao Lin; U Sebastian; Martin Stich;  Jaggi"}, {"ref_id": "b10", "title": "Communication-efficient learning of deep networks from decentralized data", "journal": "", "year": "2016", "authors": "Eider H Brendan Mcmahan; Daniel Moore; Seth Ramage;  Hampson"}, {"ref_id": "b11", "title": "Machine learning for predictive maintenance of industrial machines using iot sensor data", "journal": "IEEE", "year": "2017", "authors": "Ameeth Kanawaday; Aditya Sane"}, {"ref_id": "b12", "title": "Asynchronous decentralized parallel stochastic gradient descent", "journal": "", "year": "2017", "authors": "Xiangru Lian; Wei Zhang; Ce Zhang; Ji Liu"}, {"ref_id": "b13", "title": "Doublesqueeze: Parallel stochastic gradient descent with double-pass error-compensated compression", "journal": "", "year": "2019", "authors": "Hanlin Tang; Xiangru Lian; Chen Yu; Tong Zhang; Ji Liu"}, {"ref_id": "b14", "title": "Distributed learning over unreliable networks", "journal": "", "year": "2018", "authors": "Chen Yu; Hanlin Tang; Cedric Renggli; Simon Kassing; Ankit Singla; Dan Alistarh; Ce Zhang; Ji Liu"}, {"ref_id": "b15", "title": "On the convergence of fedavg on non-iid data", "journal": "", "year": "2019", "authors": "Xiang Li; Kaixuan Huang; Wenhao Yang; Shusen Wang; Zhihua Zhang"}, {"ref_id": "b16", "title": "Optimal algorithms for smooth and strongly convex distributed optimization in networks", "journal": "", "year": "2017", "authors": "Kevin Seaman; Francis Bach; S\u00e9bastien Bubeck; Yin Tat Lee; Laurent Massouli\u00e9"}, {"ref_id": "b17", "title": "A brief survey of machine learning methods and their sensor and iot applications", "journal": "IEEE", "year": "2017", "authors": "Uday Shankar Shanthamallu; Andreas Spanias; Cihan Tepedelenlioglu; Mike Stanley"}, {"ref_id": "b18", "title": "Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent", "journal": "", "year": "2017", "authors": "Xiangru Lian; Ce Zhang; Huan Zhang; Cho-Jui Hsieh; Wei Zhang; Ji Liu"}, {"ref_id": "b19", "title": "On the linear speedup analysis of communication efficient momentum sgd for distributed non-convex optimization", "journal": "", "year": "2019", "authors": "Hao Yu; Rong Jin; Sen Yang"}, {"ref_id": "b20", "title": "Dadam: A consensus-based distributed adaptive gradient method for online optimization", "journal": "", "year": "2019", "authors": "Parvin Nazari; Davoud Ataee Tarzanagh; George Michailidis"}, {"ref_id": "b21", "title": "Moniqua: Modulo quantized communication in decentralized sgd", "journal": "", "year": "2020", "authors": "Yucheng Lu; Christopher De Sa"}, {"ref_id": "b22", "title": "Rat-resilient allreduce tree for distributed machine learning", "journal": "", "year": "2020", "authors": "Xinchen Wan; Hong Zhang; Hao Wang; Shuihai Hu; Junxue Zhang; Kai Chen"}, {"ref_id": "b23", "title": "D2: Decentralized training over decentralized data", "journal": "", "year": "2018", "authors": "Hanlin Tang; Xiangru Lian; Ming Yan; Ce Zhang; Ji Liu"}, {"ref_id": "b24", "title": "Matcha: Speeding up decentralized sgd via matching decomposition sampling", "journal": "", "year": "2019", "authors": "Jianyu Wang; Anit Kumar Sahu; Zhouyi Yang; Gauri Joshi; Soummya Kar"}, {"ref_id": "b25", "title": "Optimal algorithms for non-smooth distributed optimization in networks", "journal": "", "year": "2018", "authors": "Kevin Scaman; Francis Bach; S\u00e9bastien Bubeck; Laurent Massouli\u00e9; Yin Tat Lee"}, {"ref_id": "b26", "title": "A unified theory of decentralized sgd with changing topology and local updates", "journal": "", "year": "2020", "authors": "Anastasia Koloskova; Nicolas Loizou; Sadra Boreiri; Martin Jaggi; Sebastian U Stich"}, {"ref_id": "b27", "title": "Graph oracle models, lower bounds, and gaps for parallel stochastic optimization", "journal": "", "year": "2018", "authors": "E Blake; Jialei Woodworth; Adam Wang; Brendan Smith; Nati Mcmahan;  Srebro"}, {"ref_id": "b28", "title": "Decentralized and parallelized primal and dual accelerated methods for stochastic convex programming problems", "journal": "", "year": "2019", "authors": "Darina Dvinskikh; Alexander Gasnikov"}, {"ref_id": "b29", "title": "Distributed non-convex first-order optimization and information processing: Lower complexity bounds and rate optimal algorithms", "journal": "IEEE Transactions on Signal processing", "year": "2019", "authors": "Haoran Sun; Mingyi Hong"}, {"ref_id": "b30", "title": "Federated learning with non-iid data", "journal": "", "year": "2018", "authors": "Yue Zhao; Meng Li; Liangzhen Lai; Naveen Suda; Damon Civin; Vikas Chandra"}, {"ref_id": "b31", "title": "Towards federated learning at scale: System design", "journal": "", "year": "2019", "authors": "Keith Bonawitz; Hubert Eichner; Wolfgang Grieskamp; Dzmitry Huba; Alex Ingerman; Vladimir Ivanov; Chloe Kiddon; Jakub Kone\u010dn\u1ef3; Stefano Mazzocchi;  H Brendan Mcmahan"}, {"ref_id": "b32", "title": "Federated learning over wireless networks: Optimization model design and analysis", "journal": "IEEE", "year": "2019", "authors": "H Nguyen; Wei Tran; Albert Bao;  Zomaya; N H Minh; Choong Seon Nguyen;  Hong"}, {"ref_id": "b33", "title": "Federated learning", "journal": "Synthesis Lectures on Artificial Intelligence and Machine Learning", "year": "2019", "authors": "Qiang Yang; Yang Liu; Yong Cheng; Yan Kang; Tianjian Chen; Han Yu"}, {"ref_id": "b34", "title": "Federated learning: Strategies for improving communication efficiency", "journal": "", "year": "2016", "authors": "Jakub Kone\u010dn\u1ef3; Brendan Mcmahan; X Felix; Peter Yu; Ananda Theertha Richt\u00e1rik; Dave Suresh;  Bacon"}, {"ref_id": "b35", "title": "Gossip algorithms: Design, analysis and applications", "journal": "IEEE", "year": "2005", "authors": "Stephen Boyd; Arpita Ghosh; Balaji Prabhakar; Devavrat Shah"}, {"ref_id": "b36", "title": "Randomized gossip algorithms", "journal": "IEEE transactions on information theory", "year": "2006", "authors": "Stephen Boyd; Arpita Ghosh; Balaji Prabhakar; Devavrat Shah"}, {"ref_id": "b37", "title": "Achieving linear convergence in distributed asynchronous multiagent optimization", "journal": "IEEE Transactions on Automatic Control", "year": "2020", "authors": "Ye Tian; Ying Sun; Gesualdo Scutari"}, {"ref_id": "b38", "title": "Asyspa: An exact asynchronous algorithm for convex optimization over digraphs", "journal": "IEEE Transactions on Automatic Control", "year": "2019", "authors": "Jiaqi Zhang; Keyou You"}, {"ref_id": "b39", "title": "Asynchronous accelerated proximal stochastic gradient for strongly convex distributed finite sums", "journal": "", "year": "2019", "authors": "Hadrien Hendrikx; Francis Bach; Laurent Massouli\u00e9"}, {"ref_id": "b40", "title": "A hybrid variance-reduced method for decentralized stochastic non-convex optimization", "journal": "", "year": "2021", "authors": "Ran Xin; A Usman; Soummya Khan;  Kar"}, {"ref_id": "b41", "title": "Decentralized stochastic gradient tracking for empirical risk minimization", "journal": "", "year": "2019", "authors": "Jiaqi Zhang; Keyou You"}, {"ref_id": "b42", "title": "Variance-reduced decentralized stochastic optimization with gradient tracking", "journal": "", "year": "2019", "authors": "Ran Xin; A Usman; Soummya Khan;  Kar"}, {"ref_id": "b43", "title": "An improved convergence analysis for decentralized online stochastic non-convex optimization", "journal": "IEEE Transactions on Signal Processing", "year": "2021", "authors": "Ran Xin; A Usman; Soummya Khan;  Kar"}, {"ref_id": "b44", "title": "Cola: Decentralized linear learning", "journal": "", "year": "2018", "authors": "Lie He; An Bian; Martin Jaggi"}, {"ref_id": "b45", "title": "Stochastic gradient push for distributed deep learning", "journal": "", "year": "2018", "authors": "Mahmoud Assran; Nicolas Loizou; Nicolas Ballas; Michael Rabbat"}, {"ref_id": "b46", "title": "Asynchronous decentralized optimization in directed networks", "journal": "", "year": "2019", "authors": "Jiaqi Zhang; Keyou You"}, {"ref_id": "b47", "title": "Decentralized stochastic optimization and gossip algorithms with compressed communication", "journal": "", "year": "2019", "authors": "Anastasia Koloskova; U Sebastian; Martin Stich;  Jaggi"}, {"ref_id": "b48", "title": "Deepsqueeze: Parallel stochastic gradient descent with double-pass error-compensated compression", "journal": "", "year": "2019", "authors": "Hanlin Tang; Xiangru Lian; Shuang Qiu; Lei Yuan; Ce Zhang; Tong Zhang; Ji Liu"}, {"ref_id": "b49", "title": "Communication compression for decentralized training", "journal": "", "year": "2018", "authors": "Hanlin Tang; Shaoduo Gan; Ce Zhang; Tong Zhang; Ji Liu"}, {"ref_id": "b50", "title": "Cooperative sgd: A unified framework for the design and analysis of communication-efficient sgd algorithms", "journal": "", "year": "2018", "authors": "Jianyu Wang; Gauri Joshi"}, {"ref_id": "b51", "title": "Distributed subgradient methods for multi-agent optimization", "journal": "IEEE Transactions on Automatic Control", "year": "2009", "authors": "Angelia Nedic; Asuman Ozdaglar"}, {"ref_id": "b52", "title": "Distributed dual averaging in networks", "journal": "Citeseer", "year": "2010", "authors": "Alekh John C Duchi; Martin J Agarwal;  Wainwright"}, {"ref_id": "b53", "title": "A lower bound for the optimization of finite sums", "journal": "", "year": "2014", "authors": "Alekh Agarwal; Leon Bottou"}, {"ref_id": "b54", "title": "Communication complexity of distributed convex learning and optimization", "journal": "", "year": "2015", "authors": "Yossi Arjevani; Ohad Shamir"}, {"ref_id": "b55", "title": "An optimal randomized incremental gradient method. Mathematical programming", "journal": "", "year": "2018", "authors": "Guanghui Lan; Yi Zhou"}, {"ref_id": "b56", "title": "Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator", "journal": "", "year": "2018", "authors": "Cong Fang; Chris Junchi Li; Zhouchen Lin; Tong Zhang"}, {"ref_id": "b57", "title": "Oracle complexity of second-order methods for finite-sum problems", "journal": "", "year": "2017", "authors": "Yossi Arjevani; Ohad Shamir"}, {"ref_id": "b58", "title": "How to make the gradients small stochastically: Even faster convex and nonconvex sgd", "journal": "", "year": "2018", "authors": "Zeyuan Allen-Zhu"}, {"ref_id": "b59", "title": "The complexity of making the gradient small in stochastic convex optimization", "journal": "", "year": "2019", "authors": "Dylan Foster; Ayush Sekhari; Ohad Shamir; Nathan Srebro; Karthik Sridharan; Blake Woodworth"}, {"ref_id": "b60", "title": "Lower bounds for parallel and randomized convex optimization", "journal": "", "year": "2018", "authors": "Jelena Diakonikolas; Crist\u00f3bal Guzm\u00e1n"}, {"ref_id": "b61", "title": "Parallelization does not accelerate convex optimization: Adaptivity lower bounds for non-smooth convex minimization", "journal": "", "year": "2018", "authors": "Eric Balkanski; Yaron Singer"}, {"ref_id": "b62", "title": "An adaptive primal-dual framework for nonsmooth convex minimization", "journal": "Mathematical Programming Computation", "year": "2019", "authors": "Quoc Tran-Dinh; Ahmet Alacaoglu; Olivier Fercoq; Volkan Cevher"}, {"ref_id": "b63", "title": "Theoretical limits of pipeline parallel optimization and application to distributed deep learning", "journal": "", "year": "2019", "authors": "Igor Colin; Dos Ludovic; Kevin Santos;  Scaman"}, {"ref_id": "b64", "title": "Lower bounds for finding stationary points ii: First-order methods", "journal": "", "year": "2017", "authors": "Yair Carmon; C John; Oliver Duchi; Aaron Hinder;  Sidford"}, {"ref_id": "b65", "title": "Lower bounds for finding stationary points i", "journal": "", "year": "2019", "authors": "Yair Carmon; C John; Oliver Duchi; Aaron Hinder;  Sidford"}, {"ref_id": "b66", "title": "Lower bounds for smooth nonconvex finite-sum optimization", "journal": "", "year": "2019", "authors": "Dongruo Zhou; Quanquan Gu"}, {"ref_id": "b67", "title": "Lower bounds for non-convex stochastic optimization", "journal": "", "year": "2019", "authors": "Yossi Arjevani; Yair Carmon; C John; Dylan J Duchi; Nathan Foster; Blake Srebro;  Woodworth"}, {"ref_id": "b68", "title": "Extra: An exact first-order algorithm for decentralized consensus optimization", "journal": "SIAM Journal on Optimization", "year": "2015", "authors": "Wei Shi; Qing Ling; Gang Wu; Wotao Yin"}, {"ref_id": "b69", "title": "Optimal algorithms for smooth and strongly convex distributed optimization in networks", "journal": "PMLR", "year": "2017", "authors": "Kevin Scaman; Francis Bach; S\u00e9bastien Bubeck; Yin Tat Lee; Laurent Massouli\u00e9"}, {"ref_id": "b70", "title": "Distributed learning systems with first-order methods", "journal": "", "year": "2021", "authors": "Ji Liu; Ce Zhang"}, {"ref_id": "b71", "title": "Stochastic gradient push for distributed deep learning", "journal": "PMLR", "year": "2019", "authors": "Mahmoud Assran; Nicolas Loizou; Nicolas Ballas; Mike Rabbat"}, {"ref_id": "b72", "title": "Stochastic first-and zeroth-order methods for nonconvex stochastic programming", "journal": "SIAM Journal on Optimization", "year": "2013", "authors": "Saeed Ghadimi; Guanghui Lan"}, {"ref_id": "b73", "title": "A method for unconstrained convex minimization problem with the rate of convergence o (1/k\u02c62). In Doklady an ussr", "journal": "", "year": "1983", "authors": "Yurii Nesterov"}, {"ref_id": "b74", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b75", "title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning", "journal": "", "year": "2012", "authors": "Tijmen Tieleman; Geoffrey Hinton"}, {"ref_id": "b76", "title": "Adagrad stepsizes: Sharp convergence over nonconvex landscapes, from any initialization", "journal": "", "year": "2018", "authors": "Rachel Ward; Xiaoxia Wu; Leon Bottou"}, {"ref_id": "b77", "title": "Adadelta: an adaptive learning rate method", "journal": "", "year": "2012", "authors": "D Matthew;  Zeiler"}, {"ref_id": "b78", "title": "Handbook of markov chain monte carlo", "journal": "CRC press", "year": "2011", "authors": "Steve Brooks; Andrew Gelman; Galin Jones; Xiao-Li Meng"}, {"ref_id": "b79", "title": "Markov chain mixing time on cycles", "journal": "", "year": "2011", "authors": "Bal\u00e1zs Gerencs\u00e9r"}, {"ref_id": "b80", "title": "Markov chains and mixing times", "journal": "American Mathematical Soc", "year": "2017", "authors": "A David; Yuval Levin;  Peres"}, {"ref_id": "b81", "title": "Definitive consensus for distributed data inference", "journal": "EPFL", "year": "2011", "authors": "Leonidas Georgopoulos"}, {"ref_id": "b82", "title": "On matrix factorization and scheduling for finite-time average-consensus", "journal": "", "year": "2010", "authors": "Chih-Kai Ko"}, {"ref_id": "b83", "title": "Graph diameter, eigenvalues, and minimum-time consensus", "journal": "Automatica", "year": "2014", "authors": "M Julien;  Hendrickx; M Rapha\u00ebl; Alexander Jungers; Guillaume Olshevsky;  Vankeerberghen"}, {"ref_id": "b84", "title": "Don't use large mini-batches, use local sgd", "journal": "", "year": "2018", "authors": "Tao Lin; U Sebastian; Kumar Kshitij Stich; Martin Patel;  Jaggi"}, {"ref_id": "b85", "title": "Accelerated gossip in networks of given dimension using jacobi polynomial iterations", "journal": "SIAM Journal on Mathematics of Data Science", "year": "2020", "authors": "Rapha\u00ebl Berthier; Francis Bach; Pierre Gaillard"}, {"ref_id": "b86", "title": "Accelerated linear iterations for distributed averaging", "journal": "Annual Reviews in Control", "year": "2011", "authors": "Ji Liu;  Stephen Morse"}, {"ref_id": "b87", "title": "Multi-consensus decentralized accelerated gradient descent", "journal": "", "year": "2020", "authors": "Haishan Ye; Luo Luo; Ziang Zhou; Tong Zhang"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Figure illustrating how decentralization in different layers lead to different learning systems. From left to right: : A fully centralized system where workers sample from shared and shuffled data; 2 \u25cb: Based on 1 \u25cb, workers maintain their own data sources, making it decentralized in the application layer; 3 \u25cb: Based on 2 \u25cb, workers are decentralized in the topology layer; 4\u25cb: A fully decentralized system in all three layers where the workers communicate via Gossip. Our framework and theory are applicable to all kinds of decentralized learning systems.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Theorem 11For function class F \u2206,L , oracle class O \u03c3 2 and graph class G n,D defined with any \u2206 > 0, L > 0, n \u2208 N + , D \u2208 {1, 2, . . . , n \u2212 1}, \u03c3 > 0, and B \u2208 N + , there exists f \u2208 F \u2206,L , O \u2208 O \u03c3 2 , and G \u2208 G n,D , such that no matter what A \u2208 A B is used, T (A, f, O, G) will always be lower bounded by", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 2 :2Figure 2: Fine tuned results of training LeNet on CIFAR10 with different shuffling strategies.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "\u03ba = 0.01 (1 \u2212 \u03bb \u2248 4e-4)", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 3 :3Figure 3: Fine tuned results of training Resnet20 on CIFAR100 with different spectral gaps.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 4 :4Figure 4: Illustration graph for setting 2 to in the proof of Theorem 1.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "", "figure_data": "ExperimentSettingAlgorithmD-PSGDD 2DSGT DeTAG100% Shuffled5e-35e-35e-35e-3LeNet/CIFAR1050% Shuffled 25% Shuffled5e-5 5e-52.5e-4 2.5e-4 1e-4 2.5e-45e-4 5e-40% Shuffled5e-51e-42.5e-45e-4"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "DeTAG-specific hyperparameters used for each experiments.", "figure_data": "ExperimentSettingAccelerate Factor \u03b7 Phase Length R100% Shuffled01LeNet/CIFAR1050% Shuffled 25% Shuffled0.2 0.22 20% Shuffled0.22\u03ba = 101Resnet20/CIFAR100\u03ba = 0.1 \u03ba = 0.050.2 0.22 2\u03ba = 0.010.42B Technical Proof"}], "formulas": [{"formula_id": "formula_0", "formula_text": "Lower Bound Theorem 1 Central \u2126 \u2206L\u03c3 2 nB 4 \u2126 \u2206LD 2 / Corollary 1 Decentral \u2126 \u2206L\u03c3 2 nB 4 \u2126 \u2206L 2 \u221a 1\u2212\u03bb / Upper Bound DeFacto (Theorem 2) Central O \u2206L\u03c3 2 nB 4 O \u2206LD 2 O(1) DeTAG (Theorem 3) Decentral O \u2206L\u03c3 2 nB 4 O \u2206L log \u03c2 0 n \u221a \u2206L 2 \u221a 1\u2212\u03bb O log \u03c2 0 n \u221a \u2206L D-PSGD [19] Decentral O \u2206L\u03c3 2 nB 4 O \u2206Ln\u03c2 2 (1\u2212\u03bb) 2 O n\u03c2 (1\u2212\u03bb) 3 2 SGP [72] Decentral O \u2206L\u03c3 2 nB 4 O \u2206Ln\u03c2 2 (1\u2212\u03bb) 2 O n\u03c2 (1\u2212\u03bb) 3 2 D 2 [24] Decentral O \u2206L\u03c3 2 nB 4 O \u03bb 2 \u2206Ln\u03c2 0 2 (1\u2212\u03bb) 3 O \u03bb 2 n\u03c2 0 (1\u2212\u03bb) 5 2 DSGT [42] Decentral O \u2206L\u03c3 2 nB 4 O \u03bb 2 \u2206Ln\u03c2 0 2 (1\u2212\u03bb) 3 O \u03bb 2 n\u03c2 0 (1\u2212\u03bb) 5 2 GT-DSGD [44] Decentral O \u2206L\u03c3 2 nB 4 O \u03bb 2 \u2206Ln\u03c2 0 2 (1\u2212\u03bb) 3 O \u03bb 2 n\u03c2 0 (1\u2212\u03bb) 5 2", "formula_coordinates": [3.0, 77.98, 190.81, 452.78, 182.08]}, {"formula_id": "formula_1", "formula_text": "x = arg min x\u2208R d \uf8ee \uf8ef \uf8f0f (x) = 1 n n i=1 E \u03bei\u223cDi f i (x; \u03be i ) fi(x) \uf8f9 \uf8fa \uf8fb . (1)", "formula_coordinates": [4.0, 206.82, 161.75, 333.18, 40.85]}, {"formula_id": "formula_2", "formula_text": "f i : R d \u2192 R are L-smooth, \u2207f i (x) \u2212 \u2207f i (y) \u2264 L x \u2212 y , \u2200x, y \u2208 R d ,(2)", "formula_coordinates": [4.0, 72.0, 340.22, 468.0, 35.14]}, {"formula_id": "formula_3", "formula_text": ")\u2212inf x f (x) \u2264 \u2206.", "formula_coordinates": [4.0, 465.88, 381.98, 76.06, 9.65]}, {"formula_id": "formula_4", "formula_text": "E z\u223cZ [g i (x, z)] = \u2207f i (x), \u2200x \u2208 R d .(3)", "formula_coordinates": [4.0, 230.52, 470.81, 309.48, 11.72]}, {"formula_id": "formula_5", "formula_text": "E z\u223cZ g i (x, z) \u2212 \u2207f i (x) 2 \u2264 \u03c3 2 , \u2200x \u2208 R d .(4)", "formula_coordinates": [4.0, 214.72, 513.88, 325.28, 11.72]}, {"formula_id": "formula_6", "formula_text": "1 n n i=1 \u2207f i (x) \u2212 \u2207f (x) 2 \u2264 \u03c2 2 , \u2200x \u2208 R d ,(5)", "formula_coordinates": [4.0, 217.04, 656.57, 322.96, 30.32]}, {"formula_id": "formula_7", "formula_text": "n i=1 \u2207f i (0) \u2212 \u2207f (0) 2 \u2264 \u03c2 2 0 ,(6)", "formula_coordinates": [5.0, 249.11, 120.0, 290.89, 30.32]}, {"formula_id": "formula_8", "formula_text": "t \u2208 span({x t,j } j\u2208[n] ) = { n j=1 c j x t,j | c j \u2208 R}.(7)", "formula_coordinates": [5.0, 208.37, 368.94, 331.63, 14.11]}, {"formula_id": "formula_9", "formula_text": "z t,i \u2190 j\u2208Ni y t,j W ji , \u2200i \u2208 [n](8)", "formula_coordinates": [5.0, 237.63, 626.29, 302.37, 11.81]}, {"formula_id": "formula_10", "formula_text": "f \u2208 F \u2206,L , a set of underlying oracles O \u2208 O \u03c3 2 , a graph G \u2208 G n,D", "formula_coordinates": [6.0, 72.0, 322.27, 469.38, 23.8]}, {"formula_id": "formula_11", "formula_text": "T (A, f, O, G) = min t \u2208 N E \u2207f (x A,f,O,G t ) \u2264 ,", "formula_coordinates": [6.0, 189.16, 369.45, 233.69, 13.36]}, {"formula_id": "formula_12", "formula_text": "inf A\u2208A B sup f \u2208F \u2206,L sup O\u2208O \u03c3 2 sup G\u2208G n,D T (A, f, O, G),(9)", "formula_coordinates": [6.0, 217.55, 577.37, 322.45, 19.08]}, {"formula_id": "formula_13", "formula_text": "inf A\u2208A B,W sup f \u2208F \u2206,L sup O\u2208O \u03c3 2 sup G\u2208G n,D T (A, f, O, G),", "formula_coordinates": [7.0, 212.35, 399.3, 187.3, 18.41]}, {"formula_id": "formula_14", "formula_text": "\u2126 \u2206L\u03c3 2 nB 4 + \u2206L 2 \u221a 1 \u2212 \u03bb .(11)", "formula_coordinates": [7.0, 250.85, 492.38, 289.15, 24.83]}, {"formula_id": "formula_15", "formula_text": "1: for t = 0, 1, \u2022 \u2022 \u2022 , T \u2212 1 do 2: k \u2190 t/2R . 3: r \u2190 t mod 2R. 4: if 0 \u2264 r < R then 5:", "formula_coordinates": [8.0, 76.47, 120.52, 114.92, 51.57]}, {"formula_id": "formula_16", "formula_text": "xt+1,i \u2190 j\u2208N i \u222a{i}x t,j [W r ]ji(12) 8:", "formula_coordinates": [8.0, 76.47, 211.96, 463.53, 30.3]}, {"formula_id": "formula_17", "formula_text": "Update Model: xt+1,i \u2190xt+1,i \u2212 \u03b1 g R .", "formula_coordinates": [8.0, 115.3, 254.56, 153.7, 12.27]}, {"formula_id": "formula_18", "formula_text": "15: returnx = 1 n n i=1", "formula_coordinates": [8.0, 72.24, 309.08, 96.56, 12.55]}, {"formula_id": "formula_19", "formula_text": "Lemma 1 For any G \u2208 G n,D , let W G denote the set of n \u00d7 n matrices such that for all W \u2208 W G , W ij = 0 if edge (i, j) does not appear in G. There exists a sequence of R matrices {W r } r\u2208[R] that belongs to W G such that R \u2208 {D, D + 1, \u2022 \u2022 \u2022 , 2D} and W R\u22121 W R\u22122 \u2022 \u2022 \u2022 W 0 = 1 n 1 n n = W * .", "formula_coordinates": [8.0, 71.24, 463.28, 469.01, 68.54]}, {"formula_id": "formula_20", "formula_text": "1: for t = 0, 1, \u2022 \u2022 \u2022 , T \u2212 1 do 2: k \u2190 t/R . 3:", "formula_coordinates": [9.0, 76.47, 120.52, 114.92, 29.65]}, {"formula_id": "formula_21", "formula_text": ",i \u2190 AG(xt,i, W , Ni, i)(13)", "formula_coordinates": [9.0, 273.59, 166.35, 266.41, 7.92]}, {"formula_id": "formula_22", "formula_text": "y t+1,i \u2190 AG(y t,i , W , Ni, i)(14)", "formula_coordinates": [9.0, 258.18, 180.3, 281.82, 9.58]}, {"formula_id": "formula_23", "formula_text": "g (k) \u2190 g (k) +g. 6: if r = R \u2212 1 then 7:", "formula_coordinates": [9.0, 76.47, 204.24, 98.74, 29.61]}, {"formula_id": "formula_24", "formula_text": "xt+1,i \u2190xt+1,i \u2212 \u03b1y i (15", "formula_coordinates": [9.0, 251.71, 239.08, 284.2, 9.58]}, {"formula_id": "formula_25", "formula_text": ")", "formula_coordinates": [9.0, 535.91, 239.08, 4.09, 7.92]}, {"formula_id": "formula_26", "formula_text": "y t+1,i \u2190 y t+1,i + g (k) \u2212 g (k\u22121)(16) 8:", "formula_coordinates": [9.0, 76.47, 253.03, 463.53, 20.68]}, {"formula_id": "formula_27", "formula_text": "g (k\u22121) \u2190 g (k)", "formula_coordinates": [9.0, 229.63, 266.01, 54.59, 9.55]}, {"formula_id": "formula_28", "formula_text": "12: returnx = 1 n n i=1 xT,i Algorithm 3 Accelerated Gossip (AG) with R steps Input: z 0,i , W , N i , i 1: z \u22121,i \u2190 z 0,i 2: \u03b7 \u2190 1\u2212 \u221a 1\u2212\u03bb 2 1+ \u221a 1\u2212\u03bb 2 3: for r = 0, 1, 2, \u2022 \u2022 \u2022 , R \u2212 1 do 4: z r+1,i \u2190 (1 + \u03b7) j\u2208Ni\u222a{i} z r,j W ji \u2212 \u03b7z r\u22121,i 5: end for 6: return z R,i any graph G \u2208 G n,D , and any oracles O \u2208 O \u03c3 2 is bounded by T (A 1 , f, O, G) \u2264 O \u2206L\u03c3 2 nB 4 + \u2206LD 2 .(17)", "formula_coordinates": [9.0, 71.24, 307.65, 468.76, 201.49]}, {"formula_id": "formula_29", "formula_text": "Theorem 3 Let A 2 denote Algorithm 2. For F \u2206,L , O \u03c3 2 and G n,D defined with any \u2206 > 0, L > 0, n \u2208 N + , \u03bb \u2208 [0, 1)", "formula_coordinates": [10.0, 72.0, 332.03, 469.37, 22.27]}, {"formula_id": "formula_30", "formula_text": "R = max 1 2 log(n), 1 2 log \u03c2 2 0 T \u2206L \u221a 1 \u2212 \u03bb ,", "formula_coordinates": [10.0, 232.33, 365.06, 147.34, 31.55]}, {"formula_id": "formula_31", "formula_text": "T (A 2 , f, O, G) \u2264 O \uf8eb \uf8ed \u2206L\u03c3 2 nB 4 + \u2206L log n + \u03c20n \u221a \u2206L 2 \u221a 1 \u2212 \u03bb \uf8f6 \uf8f8 .", "formula_coordinates": [10.0, 187.72, 438.81, 236.56, 30.13]}, {"formula_id": "formula_32", "formula_text": "W \u03ba = \u03baW 0 + (1 \u2212 \u03ba)I,", "formula_coordinates": [12.0, 435.25, 456.37, 106.13, 9.68]}, {"formula_id": "formula_34", "formula_text": "prog(\u2207f (x)) \u2264 prog(x) + 1,(19)", "formula_coordinates": [19.0, 246.14, 440.82, 293.86, 9.96]}, {"formula_id": "formula_35", "formula_text": ")2", "formula_coordinates": [19.0, 91.34, 535.08, 8.8, 8.8]}, {"formula_id": "formula_36", "formula_text": "f (x) = \u2212\u03a8(1)\u03a6(x (1) ) + T \u22121 i=1 [\u03a8(\u2212x (i) )\u03a6(\u2212x (i+1) ) \u2212 \u03a8(x (i) \u03a6(x (i+1) )],(20)", "formula_coordinates": [19.0, 156.14, 628.12, 383.87, 30.32]}, {"formula_id": "formula_37", "formula_text": "\u03a8(z) = 0 z \u2264 1/2 exp 1 \u2212 1 (2z\u22121) 2 z > 1/2 , \u03a6(z) = \u221a e z \u2212\u221e e 1 2 t 2 dt. (21", "formula_coordinates": [19.0, 141.9, 690.42, 393.67, 26.64]}, {"formula_id": "formula_38", "formula_text": ")", "formula_coordinates": [19.0, 535.57, 698.82, 4.43, 9.96]}, {"formula_id": "formula_39", "formula_text": "1.f (x) \u2212 inf xf (x) \u2264 \u2206 0 T , \u2200x \u2208 R d , where \u2206 0 = 12.", "formula_coordinates": [20.0, 83.78, 118.5, 234.44, 11.23]}, {"formula_id": "formula_40", "formula_text": "3. \u2200x \u2208 R T , \u2207f (x) \u221e \u2264 G \u221e , where G \u221e = 23.", "formula_coordinates": [20.0, 83.78, 159.43, 211.34, 11.23]}, {"formula_id": "formula_41", "formula_text": "i (x) =f (x),(22)", "formula_coordinates": [20.0, 281.47, 230.54, 258.53, 10.32]}, {"formula_id": "formula_42", "formula_text": "[\u011d i (x)] j = \u2207 jfi (x) \u2022 1 + 1{j > prog(x)} z p \u2212 1 ,(23)", "formula_coordinates": [20.0, 190.51, 282.78, 349.49, 22.31]}, {"formula_id": "formula_43", "formula_text": "E[\u011d i (x)] = \u2207f i (x),(24)", "formula_coordinates": [20.0, 265.29, 339.74, 274.71, 10.32]}, {"formula_id": "formula_44", "formula_text": "E \u011d i (x) \u2212 \u2207f i (x) 2 = |\u2207 prog(x)+1f (x)| 2 E z p \u2212 1 2 \u2264 \u2207f i (x) 2 \u221e (1 \u2212 p) p \u2264 \u2207f (x) 2 \u221e (1 \u2212 p) p \u2264 G 2 \u221e (1 \u2212 p) p .", "formula_coordinates": [20.0, 72.0, 381.76, 479.19, 25.51]}, {"formula_id": "formula_45", "formula_text": "f \u2212 f * = L\u03bb 2 l 1 (f \u2212f * ) = L\u03bb 2 \u2206 0 T l 1 \u2264 \u2206.(25)", "formula_coordinates": [20.0, 218.57, 455.55, 321.43, 24.8]}, {"formula_id": "formula_46", "formula_text": "E g i (x) \u2212 \u2207f i (x) 2 \u2264 L 2 \u03bb 2 l 2 1 E g i x \u03bb \u2212 \u2207f i x \u03bb 2 \u2264 L 2 \u03bb 2 G 2 \u221e (1 \u2212 p) l 2 1 p \u2264 \u03c3 2 . (26", "formula_coordinates": [20.0, 136.38, 510.61, 399.19, 25.96]}, {"formula_id": "formula_47", "formula_text": ")", "formula_coordinates": [20.0, 535.57, 518.26, 4.43, 9.96]}, {"formula_id": "formula_48", "formula_text": "T = \u2206 \u2206 0 l 1 (2 ) 2 , p = min{(2G \u221e ) 2 /\u03c3 2 , 1}.", "formula_coordinates": [20.0, 250.43, 568.0, 111.14, 38.45]}, {"formula_id": "formula_49", "formula_text": "E \u2207f (x T ) > . (27", "formula_coordinates": [20.0, 270.23, 657.22, 265.34, 10.32]}, {"formula_id": "formula_50", "formula_text": ")", "formula_coordinates": [20.0, 535.57, 657.22, 4.43, 9.96]}, {"formula_id": "formula_51", "formula_text": "T (A, f, O, G) \u2265 T \u2212 1 nBp(e \u2212 1) \u2265 \u2126 \u2206L\u03c3 2 nB 4 ,(28)", "formula_coordinates": [20.0, 209.63, 699.23, 330.37, 23.89]}, {"formula_id": "formula_52", "formula_text": "\u2126 \u2206LD 2 . (Setting 2 Step 1 & Step 2)", "formula_coordinates": [21.0, 86.94, 216.14, 266.94, 22.57]}, {"formula_id": "formula_53", "formula_text": "I 0 = {1, \u2022 \u2022 \u2022 , |I 0 |} , I 1 = {n, n \u2212 1, \u2022 \u2022 \u2022 , n \u2212 |I 1 | + 1} . (29", "formula_coordinates": [21.0, 234.77, 251.29, 300.8, 24.6]}, {"formula_id": "formula_54", "formula_text": ")", "formula_coordinates": [21.0, 535.57, 258.2, 4.43, 9.96]}, {"formula_id": "formula_55", "formula_text": "If D \u2265 n \u2212 2 n/3 + 2,", "formula_coordinates": [21.0, 86.94, 295.0, 99.04, 8.8]}, {"formula_id": "formula_56", "formula_text": "f i (x) = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 \u2212 2n n\u2212 n/3 \u03a8(1)\u03a6(x (1) ) + i=2k,k\u2208{1,2,\u2022\u2022\u2022 },i<T 2n n\u2212 n/3 [\u03a8(\u2212x (i) )\u03a6(\u2212x (i+1) ) \u2212 \u03a8(x (i) )\u03a6(x (i+1) )] i \u2208 I 0 , i=2k\u22121,k\u2208{1,2,\u2022\u2022\u2022 },i<T 2n n\u2212 n/3 [\u03a8(\u2212x (i) )\u03a6(\u2212x (i+1) ) \u2212 \u03a8(x (i) )\u03a6(x (i+1) )] i \u2208 I 1 , 0 i \u2208 I 0 , I 1 . (30)", "formula_coordinates": [21.0, 72.0, 348.83, 517.85, 74.71]}, {"formula_id": "formula_57", "formula_text": "f i (x) = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 \u2212 n |I0| \u03a8(1)\u03a6(x (1) ) + i=2k,k\u2208{1,2,\u2022\u2022\u2022 },i<T n |I0| [\u03a8(\u2212x (i) )\u03a6(\u2212x (i+1) ) \u2212 \u03a8(x (i) )\u03a6(x (i+1) )] i \u2208 I 0 , i=2k\u22121,k\u2208{1,2,\u2022\u2022\u2022 },i<T n |I1| [\u03a8(\u2212x (i) )\u03a6(\u2212x (i+1) ) \u2212 \u03a8(x (i) )\u03a6(x (i+1) )] i \u2208 I 1 , 0 i \u2208 I 0 , I 1 .(31)", "formula_coordinates": [21.0, 72.0, 492.49, 479.28, 74.71]}, {"formula_id": "formula_58", "formula_text": "[\u011d i (x)] j = \u2207 jfi (x).(32)", "formula_coordinates": [21.0, 264.41, 650.02, 275.59, 10.32]}, {"formula_id": "formula_59", "formula_text": "f i (x) = L\u03bb 2 3l 1f i x \u03bb , \u03bb = 6l 1 L .(33)", "formula_coordinates": [21.0, 229.44, 701.53, 310.56, 24.8]}, {"formula_id": "formula_60", "formula_text": "L\u03bb 2 \u2206 0 T /3l 1 \u2264 \u2206(34)", "formula_coordinates": [22.0, 268.12, 85.04, 271.88, 11.72]}, {"formula_id": "formula_61", "formula_text": "T = \u2206L \u2206 0 l 1 (12 ) 2 .(35)", "formula_coordinates": [22.0, 263.32, 114.98, 276.69, 23.23]}, {"formula_id": "formula_62", "formula_text": "T (A, f, O, G) \u2265 \u2126(T D) = \u2126 \u2206LD 2 .(36)", "formula_coordinates": [22.0, 220.78, 163.79, 319.22, 18.85]}, {"formula_id": "formula_63", "formula_text": "(t) j = prog(x t,j ), \u2200j \u2208 [n] and i (t) = max j i (t)", "formula_coordinates": [22.0, 133.69, 262.95, 201.06, 14.07]}, {"formula_id": "formula_64", "formula_text": "P[i (t+1) \u2212 i (t) = 1|U (t) ] = P \uf8ee \uf8ef \uf8ef \uf8f0 i\u2208[n] j\u2264B E (t,i,j) |U (t) \uf8f9 \uf8fa \uf8fa \uf8fb \u2264 i\u2208[n],j\u2264B P E (t,i,j) |U (t) \u2264 min{nBp, 1}.(37)", "formula_coordinates": [22.0, 113.51, 312.09, 426.49, 47.66]}, {"formula_id": "formula_65", "formula_text": "q (t) = i (t+1) \u2212 i (t)", "formula_coordinates": [22.0, 88.65, 371.0, 75.92, 10.31]}, {"formula_id": "formula_66", "formula_text": "P[i (t) \u2265 T ] = P[e t\u22121 j=0 q (j) \u2265 e T ] \u2264 e \u2212T E[e t\u22121 j=0 q (j)", "formula_coordinates": [22.0, 196.0, 391.2, 213.46, 13.44]}, {"formula_id": "formula_68", "formula_text": "E[e t\u22121 j=0 q (j) ] = E \uf8ee \uf8f0 t\u22121 j=0 E e q (j) |U (j) \uf8f9 \uf8fb \u2264 (1 \u2212 min{nBp, 1} + min{nBp, 1}e) t \u2264 e min{nBp,1}t(e\u22121) .(39)", "formula_coordinates": [22.0, 89.67, 434.6, 450.33, 33.53]}, {"formula_id": "formula_69", "formula_text": "P[i (t) \u2265 T ] \u2264 e (e\u22121) min{nBp,1}t\u2212T \u2264 \u03b4,(40)", "formula_coordinates": [22.0, 224.16, 490.19, 315.84, 11.37]}, {"formula_id": "formula_70", "formula_text": "\uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 \u2212 n n/3 \u03a8(1)\u03a6(x (1) ) + i=2k,k\u2208{1,2,\u2022\u2022\u2022 },i<T n n/3 [\u03a8(\u2212x (i) )\u03a6(\u2212x (i+1) ) \u2212 \u03a8(x (i) )\u03a6(x (i+1) )] i \u2208 I 0 , i=2k\u22121,k\u2208{1,2,\u2022\u2022\u2022 },i<T n n/3 [\u03a8(\u2212x (i) )\u03a6(\u2212x (i+1) ) \u2212 \u03a8(x (i) )\u03a6(x (i+1) )] i \u2208 I 1 , 0 i \u2208 I 0 , I 1 . (41", "formula_coordinates": [22.0, 73.03, 622.79, 462.54, 74.71]}, {"formula_id": "formula_71", "formula_text": ")", "formula_coordinates": [22.0, 535.57, 687.54, 4.43, 9.96]}, {"formula_id": "formula_72", "formula_text": "we can see thatf (x) = 1 n n i=1f i (x). (Linear graph,", "formula_coordinates": [22.0, 71.35, 698.27, 234.2, 14.56]}, {"formula_id": "formula_74", "formula_text": "[\u011d i (x)] j = \u2207 jfi (x)(43)", "formula_coordinates": [23.0, 265.8, 152.2, 274.2, 10.32]}, {"formula_id": "formula_75", "formula_text": "f i (x) = L\u03bb 2 3l 1f i x \u03bb , \u03bb = 6l 1 L .(44)", "formula_coordinates": [23.0, 229.44, 207.25, 310.56, 24.8]}, {"formula_id": "formula_76", "formula_text": "L\u03bb 2 \u2206 0 T /3l 1 \u2264 \u2206,(45)", "formula_coordinates": [23.0, 266.74, 251.6, 273.26, 11.72]}, {"formula_id": "formula_77", "formula_text": "T = \u2206L \u2206 0 l 1 (12 ) 2 .(46)", "formula_coordinates": [23.0, 263.32, 282.24, 276.69, 23.23]}, {"formula_id": "formula_78", "formula_text": "T (A, f, O, G) \u2265 \u2126(T D) \u2265 \u2126 \u2206LD 2 .(47)", "formula_coordinates": [23.0, 220.78, 332.94, 319.22, 18.85]}, {"formula_id": "formula_79", "formula_text": "1 \u221a 1 \u2212 \u03bb = O(D).(48)", "formula_coordinates": [23.0, 270.26, 398.84, 269.74, 23.25]}, {"formula_id": "formula_80", "formula_text": "x k,i , \u2200i \u2208 [n].", "formula_coordinates": [23.0, 484.81, 502.88, 56.72, 9.68]}, {"formula_id": "formula_81", "formula_text": "1 T T \u22121 t=0 \u2207f (x) 2 \u2264 O \u2206L\u03c3 \u221a nBT + \u2206L T .(49)", "formula_coordinates": [23.0, 219.5, 535.56, 320.5, 30.2]}, {"formula_id": "formula_82", "formula_text": "1 T T \u22121 t=0 \u2207f (x) 2 \u2264 O \u2206L\u03c3/ \u221a R \u221a nBK + \u2206L K = O \u2206L\u03c3 \u221a nBT + \u2206LR T = O \u2206L\u03c3 \u221a nBT + \u2206LD T ,(50)", "formula_coordinates": [23.0, 96.68, 592.36, 443.32, 34.97]}, {"formula_id": "formula_83", "formula_text": "T = O(\u2206L\u03c3 2 (nB 4 ) \u22121 + \u2206LD \u22122 ), min t=0,1,\u2022\u2022\u2022 ,T \u22121 E \u2207f (x) \u2264 min t=0,1,\u2022\u2022\u2022 ,T \u22121 E \u2207f (x) 2 \u2264 O \u2206L\u03c3 \u221a nBT + \u2206LD T \u2264 ,(51)", "formula_coordinates": [23.0, 124.56, 637.66, 415.44, 50.67]}, {"formula_id": "formula_84", "formula_text": "Y k+1 = M(Y k +G k \u2212G k\u22121 ),(52)", "formula_coordinates": [24.0, 237.9, 126.75, 302.11, 10.32]}, {"formula_id": "formula_85", "formula_text": "X k+1 = M(X k \u2212 \u03b1Y k ),(53)", "formula_coordinates": [24.0, 237.9, 141.7, 302.11, 10.32]}, {"formula_id": "formula_86", "formula_text": "withG k+1 = \u2207f 1 (x k,1 ), \u2022 \u2022 \u2022 , \u2207f n (x k,n ) \u2208 R d\u00d7n ,(54)", "formula_coordinates": [24.0, 71.35, 164.22, 468.65, 31.63]}, {"formula_id": "formula_87", "formula_text": "G k+1 = [\u2207f 1 (x k,1 ), \u2022 \u2022 \u2022 , \u2207f n (x k,n )] \u2208 R d\u00d7n ,(55)", "formula_coordinates": [24.0, 208.27, 203.54, 331.73, 11.72]}, {"formula_id": "formula_88", "formula_text": "X k = [x k,1 , \u2022 \u2022 \u2022 , x k,n ] \u2208 R d\u00d7n ,(56)", "formula_coordinates": [24.0, 216.98, 220.07, 323.02, 11.72]}, {"formula_id": "formula_89", "formula_text": "Y k = y k,1 , \u2022 \u2022 \u2022 , y k,n \u2208 R d\u00d7n ,(57)", "formula_coordinates": [24.0, 217.93, 236.6, 322.07, 12.67]}, {"formula_id": "formula_90", "formula_text": "M(X) \u2212 X \u2264 \u03c1 X \u2212 X ; M(X) 1 n = X 1 n ,(58)", "formula_coordinates": [24.0, 205.58, 318.41, 334.42, 22.34]}, {"formula_id": "formula_91", "formula_text": "\u03c1 = 1 \u2212 1 \u2212 \u03bb 2 (W ) R .", "formula_coordinates": [24.0, 99.48, 349.78, 114.71, 16.6]}, {"formula_id": "formula_92", "formula_text": "E \u2207f i (x) \u2212 \u2207f i (x) 2 \u2264 \u03c3 2 ,(59)", "formula_coordinates": [24.0, 245.28, 390.91, 294.72, 11.72]}, {"formula_id": "formula_93", "formula_text": "Y k = M(Y k\u22121 +G k\u22121 \u2212G k\u22122 ) 1 n = Y k\u22121 +G k\u22121 \u2212G k\u22122 = Y \u22121 + k\u22121 j=\u22121 (G j \u2212G j\u22121 ) =G k\u22121(60)", "formula_coordinates": [24.0, 86.73, 450.4, 453.27, 30.32]}, {"formula_id": "formula_94", "formula_text": "X k+1 = M(X k \u2212 \u03b1Y k ) 1 n = X k \u2212 \u03b1Y k .(61)", "formula_coordinates": [24.0, 216.82, 500.13, 323.18, 22.34]}, {"formula_id": "formula_95", "formula_text": "Ef X k+1 =Ef X k \u2212 \u03b1Y k (62) \u2264Ef X k \u2212 \u03b1E \u2207f X k , Y k + \u03b1 2 L 2 E Y k 2 (63", "formula_coordinates": [24.0, 161.75, 548.82, 378.25, 38.66]}, {"formula_id": "formula_96", "formula_text": ") (60) = Ef X k \u2212 \u03b1E \u2207f X k , G k\u22121 + \u03b1 2 L 2 E G k\u22121 2 . (64", "formula_coordinates": [24.0, 206.68, 571.24, 333.32, 41.91]}, {"formula_id": "formula_97", "formula_text": ")", "formula_coordinates": [24.0, 535.57, 596.91, 4.43, 9.96]}, {"formula_id": "formula_98", "formula_text": "E G k\u22121 2 =E G k\u22121 2 + E G k\u22121 \u2212G k\u22121 2 + 2E G k\u22121 , G k\u22121 \u2212G k\u22121 (65", "formula_coordinates": [24.0, 141.92, 641.66, 393.66, 16.6]}, {"formula_id": "formula_99", "formula_text": ")", "formula_coordinates": [24.0, 535.57, 647.94, 4.43, 9.96]}, {"formula_id": "formula_100", "formula_text": "=E G k\u22121 2 + E G k\u22121 \u2212G k\u22121 2 (66", "formula_coordinates": [24.0, 192.46, 665.61, 343.12, 16.61]}, {"formula_id": "formula_101", "formula_text": ")", "formula_coordinates": [24.0, 535.57, 671.89, 4.43, 9.96]}, {"formula_id": "formula_102", "formula_text": "=E G k\u22121 2 + 1 n 2 n i=1 E G k\u22121 e i \u2212G k\u22121 e i 2 (67", "formula_coordinates": [24.0, 192.46, 689.06, 343.12, 30.32]}, {"formula_id": "formula_103", "formula_text": ")", "formula_coordinates": [24.0, 535.57, 698.81, 4.43, 9.96]}, {"formula_id": "formula_104", "formula_text": "\u2264E G k\u22121 2 +\u03c3 2 n ,(68)", "formula_coordinates": [25.0, 192.46, 71.73, 347.54, 23.88]}, {"formula_id": "formula_105", "formula_text": "Ef X k+1 \u2264Ef X k \u2212 \u03b1E \u2207f X k , G k\u22121 + \u03b1 2 L 2 E G k\u22121 2 + \u03b1 2\u03c32 L 2n (69) =Ef X k \u2212 \u03b1 2 E \u2207f X k 2 \u2212 \u03b1 \u2212 \u03b1 2 L 2 G k\u22121 2 + \u03b1 2\u03c32 L 2n + \u03b1 2 E G k\u22121 \u2212 \u2207f X k 2 ,(70)", "formula_coordinates": [25.0, 84.61, 132.96, 455.39, 59.48]}, {"formula_id": "formula_106", "formula_text": "E G k\u22121 \u2212 \u2207f X k 2 (71) \u22642E G k\u22121 \u2212 G k+1 2 + 2E G k+1 \u2212 \u2207f X k 2 (72) =2E 1 n n i=1 \u2207f i (x k,i ) \u2212 1 n n i=1 \u2207f i (x k\u22122,i ) 2 + 2E 1 n n i=1 \u2207f i (x k,i ) \u2212 1 n n i=1 \u2207f i (X k ) 2 (73) \u2264 2 n n i=1 E \u2207f i (x k,i ) \u2212 \u2207f i (x k\u22122,i ) 2 + 2 n n i=1 E \u2207f i (x k,i ) \u2212 \u2207f i (X k ) 2 (74) \u2264 2L 2 n E X k \u2212 X k\u22122 2 F + 2L 2 n E X k \u2212 X k 1 n 2 F .(75)", "formula_coordinates": [25.0, 116.75, 224.6, 423.25, 130.14]}, {"formula_id": "formula_107", "formula_text": "K\u22121 k=0 \u03b1(1 \u2212 \u03b1L) G k 2 + K\u22121 k=0 \u03b1E \u2207f X k 2 (76", "formula_coordinates": [25.0, 124.14, 384.49, 411.43, 30.55]}, {"formula_id": "formula_108", "formula_text": ")", "formula_coordinates": [25.0, 535.57, 394.24, 4.43, 9.96]}, {"formula_id": "formula_109", "formula_text": "\u22642\u2206 + \u03b1 2\u03c32 LK n + 2\u03b1L 2 n K\u22121 k=0 E X k \u2212 X k 1 n 2 F + 2\u03b1L 2 n K\u22121 k=0 E X k \u2212 X k\u22122 2 F (77", "formula_coordinates": [25.0, 114.73, 419.67, 420.84, 30.55]}, {"formula_id": "formula_110", "formula_text": ")", "formula_coordinates": [25.0, 535.57, 429.42, 4.43, 9.96]}, {"formula_id": "formula_111", "formula_text": "\u22642\u2206 + \u03b1 2\u03c32 LK n + 16\u03b1L 2 n K\u22121 k=0 E X k \u2212 X k 1 n 2 F + 6\u03b1L 2 n K\u22121 k=0 E X k 1 n \u2212 X k\u22122 1 n 2 F ,(78)", "formula_coordinates": [25.0, 114.73, 454.85, 425.27, 30.55]}, {"formula_id": "formula_112", "formula_text": "2\u03b1L 2 n K\u22121 k=0 E X k \u2212 X k\u22122 2 F (79) \u2264 6\u03b1L 2 n K\u22121 k=0 E X k \u2212 X k 1 n 2 F + 6\u03b1L 2 n K\u22121 k=0 E X k\u22122 \u2212 X k\u22122 1 n 2 F + 6\u03b1L 2 n K\u22121 k=0 E X k 1 n \u2212 X k\u22122 1 n 2 F .(80)", "formula_coordinates": [25.0, 76.33, 515.57, 463.67, 77.23]}, {"formula_id": "formula_113", "formula_text": "6\u03b1L 2 n K\u22121 k=0 E X k 1 n \u2212 X k\u22122 1 n 2 F = 6\u03b1L 2 n n K\u22121 k=0 E X k \u2212 X k\u22122 2 (81", "formula_coordinates": [25.0, 147.57, 624.43, 388.0, 30.55]}, {"formula_id": "formula_114", "formula_text": ")", "formula_coordinates": [25.0, 535.57, 634.17, 4.43, 9.96]}, {"formula_id": "formula_115", "formula_text": "(61) = 24\u03b1 3 L 2 n n K\u22121 k=0 E G k 2 (82)(65)", "formula_coordinates": [25.0, 291.96, 659.61, 248.04, 44.56]}, {"formula_id": "formula_116", "formula_text": "\u2264 24\u03b1 3 L 2 K\u22121 k=0 E G k 2 + 24\u03b1 3\u03c32 L 2 K n .(83)", "formula_coordinates": [25.0, 295.17, 694.79, 244.83, 30.55]}, {"formula_id": "formula_117", "formula_text": "K\u22121 k=0 \u03b1(1 \u2212 \u03b1L \u2212 24\u03b1 2 L 2 ) G k 2 + K\u22121 k=0 \u03b1E \u2207f X k 2 (84", "formula_coordinates": [26.0, 172.5, 92.69, 363.07, 30.55]}, {"formula_id": "formula_118", "formula_text": ")", "formula_coordinates": [26.0, 535.57, 102.44, 4.43, 9.96]}, {"formula_id": "formula_119", "formula_text": "\u22642\u2206 + \u03b1 2\u03c32 LK n + 16\u03b1L 2 n K\u22121 k=0 E X k \u2212 X k 1 n 2 F + 24\u03b1 3\u03c32 L 2 K n . (85", "formula_coordinates": [26.0, 163.09, 127.87, 372.48, 30.55]}, {"formula_id": "formula_120", "formula_text": ")", "formula_coordinates": [26.0, 535.57, 137.62, 4.43, 9.96]}, {"formula_id": "formula_121", "formula_text": "16\u03b1L 2 n K\u22121 k=0 E X k \u2212 X k 1 n 2 F . We start from X k+1 \u2212 X k+1 1 n 2 F (86) (61) = M(X k \u2212 \u03b1Y k ) \u2212 (X k \u2212 \u03b1Y k )1 n 2 F (87) = M(X k ) \u2212 X k 1 n 2 F \u2212 2\u03b1 M(X k ) \u2212 X k 1 n , M(Y k ) \u2212 Y k 1 n + \u03b1 2 M(Y k ) \u2212 Y k 1 n 2 F (88) (58) \u2264 \u03c1 2 X k \u2212 X k 1 n 2 F + \u03c1 2 (1 \u2212 \u03c1 2 ) 1 + \u03c1 2 X k \u2212 X k 1 n 2 F + \u03c1 2 (1 + \u03c1 2 )\u03b1 2 1 \u2212 \u03c1 2 Y k \u2212 Y k 1 n 2 F (89", "formula_coordinates": [26.0, 86.94, 167.53, 453.06, 117.55]}, {"formula_id": "formula_122", "formula_text": ")", "formula_coordinates": [26.0, 535.57, 268.84, 4.43, 9.96]}, {"formula_id": "formula_123", "formula_text": "+ \u03b1 2 \u03c1 2 Y k \u2212 Y k 1 n 2 F (90) = 2\u03c1 2 (1 + \u03c1 2 ) X k \u2212 X k 1 n 2 F + 2\u03c1 2 \u03b1 2 1 \u2212 \u03c1 2 Y k \u2212 Y k 1 n 2 F ,(91)", "formula_coordinates": [26.0, 98.82, 288.8, 441.18, 42.84]}, {"formula_id": "formula_124", "formula_text": "\u22122 a, b \u2264 1 \u2212 \u03c1 2 1 + \u03c1 2 a 2 + 1 + \u03c1 2 1 \u2212 \u03c1 2 b 2 . (92", "formula_coordinates": [26.0, 224.34, 360.2, 311.24, 23.89]}, {"formula_id": "formula_125", "formula_text": ")", "formula_coordinates": [26.0, 535.57, 367.84, 4.43, 9.96]}, {"formula_id": "formula_126", "formula_text": "E Y k+1 \u2212 Y k+1 1 n 2 F (93", "formula_coordinates": [26.0, 100.34, 412.47, 435.24, 16.59]}, {"formula_id": "formula_127", "formula_text": ")", "formula_coordinates": [26.0, 535.57, 416.25, 4.43, 9.96]}, {"formula_id": "formula_128", "formula_text": "(60) = E M(Y k +G k \u2212G k\u22121 ) \u2212 (Y k +G k \u2212G k\u22121 )1 n 2 F (94", "formula_coordinates": [26.0, 86.17, 431.42, 449.4, 22.08]}, {"formula_id": "formula_129", "formula_text": ")", "formula_coordinates": [26.0, 535.57, 437.71, 4.43, 9.96]}, {"formula_id": "formula_130", "formula_text": "=E M(Y k ) \u2212 Y k 1 n 2 F + E M(G k \u2212G k\u22121 ) \u2212 (G k \u2212G k\u22121 )1 n 2 F (95", "formula_coordinates": [26.0, 92.59, 455.87, 442.98, 22.08]}, {"formula_id": "formula_131", "formula_text": ")", "formula_coordinates": [26.0, 535.57, 462.16, 4.43, 9.96]}, {"formula_id": "formula_132", "formula_text": "+ 2E M(Y k ) \u2212 Y k 1 n , M(G k \u2212G k\u22121 ) \u2212 (G k \u2212G k\u22121 )1 n (96", "formula_coordinates": [26.0, 102.55, 484.57, 433.02, 11.29]}, {"formula_id": "formula_133", "formula_text": ")", "formula_coordinates": [26.0, 535.57, 484.57, 4.43, 9.96]}, {"formula_id": "formula_134", "formula_text": "(92)(58) \u2264 \u03c1 2 E Y k \u2212 Y k 1 n 2 F + \u03c1 2 E G k \u2212 G k\u22121 \u2212 (G k \u2212 G k\u22121 )1 n 2 F (97) + (1 \u2212 \u03c1 2 )\u03c1 2 1 + \u03c1 2 E Y k \u2212 Y k 1 n 2 F + (1 + \u03c1 2 )\u03c1 2 1 \u2212 \u03c1 2 E G k \u2212 G k\u22121 \u2212 (G k \u2212 G k\u22121 )1 n 2 F (98", "formula_coordinates": [26.0, 72.0, 503.97, 468.0, 45.54]}, {"formula_id": "formula_135", "formula_text": ")", "formula_coordinates": [26.0, 535.57, 533.27, 4.43, 9.96]}, {"formula_id": "formula_136", "formula_text": "+ 2\u03c1 2 E G k \u2212G k 2 F + 2\u03c1 2 E G k\u22121 \u2212G k\u22121 2 F + 2\u03c1 2 E G k 1 n \u2212G k 1 n 2 F + 2\u03c1 2 E G k\u22121 1 n \u2212G k\u22121 1 n 2 F (99", "formula_coordinates": [26.0, 102.55, 555.81, 445.44, 23.32]}, {"formula_id": "formula_137", "formula_text": ") \u2264 2\u03c1 2 1 + \u03c1 2 E Y k \u2212 Y k 1 n 2 F + 4\u03c1 2 1 \u2212 \u03c1 2 E G k+2 \u2212 G k+1 2 F (100) + 4\u03c1 2 1 \u2212 \u03c1 2 E G k+2 \u2212 G k+1 \u2212 G k + G k\u22121 2 F + 8n\u03c1 2\u03c32 ,(101)", "formula_coordinates": [26.0, 92.59, 569.17, 447.41, 66.27]}, {"formula_id": "formula_138", "formula_text": "E G k+2 \u2212 G k+1 2 F (102) = n i=1 E \u2207f (x k+1,i ) \u2212 \u2207f (x k,i ) 2 (103) \u2264L 2 n i=1 E x k+1,i \u2212 x k,i 2 (104) =L 2 E X k+1 \u2212 X k 2 F (105) (61) = L 2 E M(X k ) \u2212 X k \u2212 \u03b1M(Y k ) 2 F (106) =L 2 E M(X k \u2212 X k 1 n ) \u2212 (X k \u2212 X k 1 n ) \u2212 \u03b1M(Y k ) 2 F (107) \u22644L 2 E M(X k ) \u2212 X k 1 n 2 F + 4L 2 E X k \u2212 X k 1 n 2 F + 4\u03b1 2 L 2 E M(Y k ) \u2212 Y k 1 n 2 F (108) + 4\u03b1 2 nL 2 E Y k 2 (109) \u22644(1 + \u03c1 2 )L 2 E X k \u2212 X k 1 n 2 F + 4\u03b1 2 \u03c1 2 L 2 E Y k \u2212 Y k 1 n 2 F + 4\u03b1 2 nL 2 E Y k 2 . (110", "formula_coordinates": [26.0, 111.99, 678.43, 428.02, 46.9]}, {"formula_id": "formula_139", "formula_text": ")", "formula_coordinates": [27.0, 535.46, 202.43, 4.54, 9.96]}, {"formula_id": "formula_140", "formula_text": "E Y k+1 \u2212 Y k+1 1 n 2 F (111", "formula_coordinates": [27.0, 88.15, 237.36, 447.31, 16.59]}, {"formula_id": "formula_141", "formula_text": ") \u2264 2\u03c1 2 1 + \u03c1 2 + 16\u03b1 2 \u03c1 4 L 2 1 \u2212 \u03c1 2 E Y k \u2212 Y k 1 n 2 F + 16\u03c1 2 (1 + \u03c1 2 )L 2 1 \u2212 \u03c1 2 E X k \u2212 X k 1 n 2 F + 16\u03b1 2 \u03c1 2 nL 2 1 \u2212 \u03c1 2 E Y k 2 (112) 4\u03c1 2 1 \u2212 \u03c1 2 E G k+2 \u2212 G k+1 \u2212 G k + G k\u22121 2 F + 8n\u03c1 2\u03c32 . (113", "formula_coordinates": [27.0, 80.4, 241.14, 459.6, 80.31]}, {"formula_id": "formula_142", "formula_text": ")", "formula_coordinates": [27.0, 535.46, 305.21, 4.54, 9.96]}, {"formula_id": "formula_143", "formula_text": "E X k+1 \u2212 X k+1 1 n 2 F E Y k+1 \u2212 Y k+1 1 n 2 F P 11 P 12 P 21 P 22 E X k \u2212 X k 1 n 2 F E Y k \u2212 Y k 1 n 2 F (114) + 0 4\u03c1 2 1\u2212\u03c1 2 E U k 2 F + 16\u03b1 2 \u03c1 2 nL 2 1\u2212\u03c1 2 E Y k 2 + 8n\u03c1 2\u03c32 ,(115)", "formula_coordinates": [27.0, 138.78, 346.08, 401.22, 62.79]}, {"formula_id": "formula_144", "formula_text": "P 11 = 2\u03c1 2 (1 + \u03c1 2 ) (116", "formula_coordinates": [27.0, 229.82, 431.43, 305.65, 23.89]}, {"formula_id": "formula_145", "formula_text": ")", "formula_coordinates": [27.0, 535.46, 439.08, 4.54, 9.96]}, {"formula_id": "formula_146", "formula_text": "P 12 = 2\u03c1 2 \u03b1 2 1 \u2212 \u03c1 2(117)", "formula_coordinates": [27.0, 229.82, 459.59, 310.18, 23.89]}, {"formula_id": "formula_147", "formula_text": "P 21 = 16\u03c1 2 (1 + \u03c1 2 )L 2 1 \u2212 \u03c1 2(118)", "formula_coordinates": [27.0, 229.82, 487.2, 310.18, 23.89]}, {"formula_id": "formula_148", "formula_text": "P 22 = 2\u03c1 2 1 + \u03c1 2 + 16\u03b1 2 \u03c1 4 L 2 1 \u2212 \u03c1 2 (119) U k =G k+2 \u2212 G k+1 \u2212 G k + G k\u22121 . (120", "formula_coordinates": [27.0, 229.82, 514.8, 310.18, 39.09]}, {"formula_id": "formula_149", "formula_text": ")", "formula_coordinates": [27.0, 535.46, 543.57, 4.54, 9.96]}, {"formula_id": "formula_150", "formula_text": "z k = E X k+1 \u2212 X k+1 1 n 2 F E Y k+1 \u2212 Y k+1 1 n 2 F(", "formula_coordinates": [27.0, 190.37, 578.6, 331.48, 31.31]}, {"formula_id": "formula_151", "formula_text": "u k = 0 4\u03c1 2 1\u2212\u03c1 2 E U k 2 F + 16\u03b1 2 \u03c1 2 nL 2 1\u2212\u03c1 2 E Y k 2 + 8n\u03c1 2\u03c32 ,(123)", "formula_coordinates": [27.0, 189.54, 643.25, 350.47, 26.04]}, {"formula_id": "formula_152", "formula_text": "z k P z k\u22121 + u k\u22121 P k z 0 + k\u22121 t=0 P k\u2212t u t ,(124)", "formula_coordinates": [27.0, 211.06, 695.13, 328.94, 30.2]}, {"formula_id": "formula_154", "formula_text": "P 11 + P 22 + \u03a8 2 = 2\u03c1 2 1 + \u03c1 2 + 8\u03b1 2 \u03c1 4 L 2 1 \u2212 \u03c1 2 + 16\u03b1\u03c1 2 L \u03b1 2 \u03c1 4 L 2 + (1 + \u03c1 2 ) 1 \u2212 \u03c1 2 (127) P k \u03bb k 1 (P )+\u03bb k 2 (P ) 2 + (P 11 \u2212P 22)(\u03bb k 2 (P )\u2212\u03bb k 1 (P )) 2\u03a8 P 12 \u03a8 (\u03bb k 2 (P ) \u2212 \u03bb k 1 (P )) P 21 \u03a8 (\u03bb k 2 (P ) \u2212 \u03bb k 1 (P )) \u03bb k 1 (P )+\u03bb k 2 (P ) 2 + (P 11\u2212P 22)(\u03bb k 1 (P )\u2212\u03bb k 2 (P )) 2\u03a8 ,(128)", "formula_coordinates": [28.0, 110.52, 187.71, 429.48, 59.53]}, {"formula_id": "formula_155", "formula_text": "\u03b1L < (1 \u2212 \u03c1) 2 32 ,(129)", "formula_coordinates": [28.0, 272.92, 275.44, 267.08, 23.89]}, {"formula_id": "formula_156", "formula_text": "P k z 0 [1 :] \u2264 P 12 k\u03bb k\u22121 2 (P )E Y 0 \u2212 Y 0 1 n 2 F = 2\u03c1 2 \u03b1 2 k 1 \u2212 \u03c1 2 \u03bb k\u22121 2 (P )E Y 0 \u2212 Y 0 1 n 2 F (130)", "formula_coordinates": [28.0, 131.8, 344.15, 408.21, 23.89]}, {"formula_id": "formula_157", "formula_text": "k\u22121 l=0 \u03bb 2 (P ) l \u03bb 1 (P ) k\u22121\u2212l = \u03a8k\u03bb k\u22121 2", "formula_coordinates": [28.0, 378.51, 378.56, 145.46, 14.11]}, {"formula_id": "formula_158", "formula_text": "P k\u2212t u t [1 :] (131) \u2264 2\u03c1 2 \u03b1 2 (k \u2212 t) 1 \u2212 \u03c1 2 \u03bb k\u2212t\u22121 2 (P ) 4\u03c1 2 1 \u2212 \u03c1 2 E U t 2 F + 16\u03b1 2 \u03c1 2 nL 2 1 \u2212 \u03c1 2 E Y t 2 + 8n\u03c1 2\u03c32 (132) = 2\u03c1 2 \u03b1 2 (k \u2212 t) 1 \u2212 \u03c1 2 \u03bb k\u2212t\u22121 2 (P ) 4\u03c1 2 1 \u2212 \u03c1 2 E U t 2 F + 16\u03b1 2 \u03c1 2 nL 2 1 \u2212 \u03c1 2 E G t\u22121 2 + 8n\u03c1 2\u03c32 (133)(65)", "formula_coordinates": [28.0, 82.71, 412.55, 457.3, 80.88]}, {"formula_id": "formula_159", "formula_text": "\u2264 2\u03c1 2 \u03b1 2 (k \u2212 t) 1 \u2212 \u03c1 2 \u03bb k\u2212t\u22121 2 (P ) 4\u03c1 2 1 \u2212 \u03c1 2 E U t 2 F + 16\u03b1 2 \u03c1 2 nL 2 1 \u2212 \u03c1 2 E G t\u22121 2 + 16\u03b1 2 \u03c1 2\u03c32 L 2 1 \u2212 \u03c1 2 + 8n\u03c1 2\u03c32 , (134)", "formula_coordinates": [28.0, 85.92, 486.15, 454.09, 23.89]}, {"formula_id": "formula_160", "formula_text": "E X k \u2212 X k 1 n 2 F (135) \u2264 2\u03c1 2 \u03b1 2 k 1 \u2212 \u03c1 2 \u03bb k\u22121 2 (P )E Y 0 \u2212 Y 0 1 n 2 F (136) + k\u22121 t=0 2\u03c1 2 \u03b1 2 (k \u2212 t) 1 \u2212 \u03c1 2 \u03bb k\u2212t\u22121 2 (P ) 4\u03c1 2 1 \u2212 \u03c1 2 E U t 2 F + 16\u03b1 2 \u03c1 2 nL 2 1 \u2212 \u03c1 2 E G t\u22121 2 + 16\u03b1 2 \u03c1 2\u03c32 L 2 1 \u2212 \u03c1 2 + 8n\u03c1 2\u03c32 . (137", "formula_coordinates": [28.0, 83.04, 538.17, 456.96, 89.61]}, {"formula_id": "formula_161", "formula_text": ")", "formula_coordinates": [28.0, 535.46, 617.82, 4.54, 9.96]}, {"formula_id": "formula_162", "formula_text": "Summing over k = 0 to K \u2212 1 we obtain K\u22121 k=0 E X k \u2212 X k 1 n 2 F (138) \u2264 2\u03c1 2 \u03b1 2 (1 \u2212 \u03c1 2 )(1 \u2212 \u03bb 2 (P )) 2 K\u22121 k=0 E Y 0 \u2212 Y 0 1 n 2 F (139) + 2\u03c1 2 \u03b1 2 (1 \u2212 \u03c1 2 )(1 \u2212 \u03bb 2 (P )) 2 K\u22121 k=0 4\u03c1 2 1 \u2212 \u03c1 2 E U k 2 F + 16\u03b1 2 \u03c1 2 nL 2 1 \u2212 \u03c1 2 E G k 2 + 16\u03b1 2 \u03c1 2\u03c32 L 2 1 \u2212 \u03c1 2 + 8n\u03c1 2\u03c32 (140) \u2264 2\u03c1 2 \u03b1 2 (1 + \u03c1)nK\u03c2 2 0 (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 + 32\u03c1 4 \u03b1 4 nL 2 (1 \u2212 \u03c1) 2 (1 \u2212 \u221a \u03c1) 2 K\u22121 k=0 E G k 2 + 8\u03c1 4 \u03b1 2 (1 \u2212 \u03c1) 2 (1 \u2212 \u221a \u03c1) 2 K\u22121 k=0 E U k 2 F (141) + 32\u03c1 4 \u03b1 4\u03c32 L 2 K (1 \u2212 \u03c1) 2 (1 \u2212 \u221a \u03c1) 2 + 8\u03c1 4 \u03b1 2 n\u03c3 2 (1 + \u03c1)K (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 ] (142) \u2264 2\u03c1 2 \u03b1 2 (1 + \u03c1)nK\u03c2 2 0 (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 + 32\u03c1 4 \u03b1 4 nL 2 (1 \u2212 \u03c1) 2 (1 \u2212 \u221a \u03c1) 2 K\u22121 k=0 E G k 2 + 8\u03c1 4 \u03b1 2 (1 \u2212 \u03c1) 2 (1 \u2212 \u221a \u03c1) 2 K\u22121 k=0 E U k 2 F (143) + 16\u03c1 4 \u03b1 2 n\u03c3 2 (1 + \u03c1)K (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 ,(144)", "formula_coordinates": [28.0, 71.49, 639.37, 468.51, 85.97]}, {"formula_id": "formula_163", "formula_text": "1 1\u2212\u03bb2(P ) < 1+\u03c1 1\u2212 \u221a \u03c1 since \u03bb 2 (P ) \u2264 \u221a \u03c1+\u03c1", "formula_coordinates": [29.0, 224.34, 238.48, 165.38, 19.27]}, {"formula_id": "formula_164", "formula_text": "[P k ] 22 = \u03bb k 1 (P ) + \u03bb k 2 (P ) 2 + (P 11 \u2212 P 22 )(\u03bb k 1 (P ) \u2212 \u03bb k 2 (P )) 2\u03a8 (145", "formula_coordinates": [29.0, 180.66, 280.68, 354.8, 23.89]}, {"formula_id": "formula_165", "formula_text": ")", "formula_coordinates": [29.0, 535.46, 288.32, 4.54, 9.96]}, {"formula_id": "formula_166", "formula_text": "\u2264\u03bb k 2 (P ) + 8\u03b1 2 \u03c1 4 L 2 k\u03bb k\u22121 2 (P ) 1 \u2212 \u03c1 2 ,(146)", "formula_coordinates": [29.0, 211.1, 306.69, 328.9, 24.53]}, {"formula_id": "formula_167", "formula_text": "P k z 0 [2 :] \u2264 \u03bb k 2 (P ) + 8\u03b1 2 \u03c1 4 L 2 k\u03bb k\u22121 2 (P ) 1 \u2212 \u03c1 2 E Y 0 \u2212 Y 0 1 n 2 F ,(147)", "formula_coordinates": [29.0, 170.75, 361.68, 369.25, 24.53]}, {"formula_id": "formula_168", "formula_text": "P k\u2212t u t [2 :] (148) \u2264 \u03bb k\u2212t 2 (P ) + 8\u03b1 2 \u03c1 4 L 2 (k \u2212 t)\u03bb k\u2212t\u22121 2 (P ) 1 \u2212 \u03c1 2 \u2022 4\u03c1 2 1 \u2212 \u03c1 2 E U t 2 F + 16\u03b1 2 \u03c1 2 nL 2 1 \u2212 \u03c1 2 E Y t 2 + 8n\u03c1 2\u03c32 (149) \u2264 \u03bb k\u2212t 2 (P ) + 8\u03b1 2 \u03c1 4 L 2 (k \u2212 t)\u03bb k\u2212t\u22121 2 (P ) 1 \u2212 \u03c1 2 \u2022 4\u03c1 2 1 \u2212 \u03c1 2 E U t 2 F + 16\u03b1 2 \u03c1 2 nL 2 1 \u2212 \u03c1 2 E G t\u22121 2 + 16\u03b1 2 \u03c1 2\u03c32 L 2 1 \u2212 \u03c1 2 + 8n\u03c1 2\u03c32 .(150)", "formula_coordinates": [29.0, 72.0, 420.5, 500.37, 92.33]}, {"formula_id": "formula_169", "formula_text": "K\u22121 k=0 E Y k \u2212 Y k 1 n 2 (151) \u2264 (1 + \u03c1)nK\u03c2 2 0 1 \u2212 \u221a \u03c1 + 8\u03b1 2 \u03c1 4 (1 + \u03c1)L 2 nK\u03c2 2 0 (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 (152) + 1 + \u03c1 1 \u2212 \u221a \u03c1 + 8\u03b1 2 \u03c1 4 (1 + \u03c1)L 2 (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 K\u22121 k=0 4\u03c1 2 1 \u2212 \u03c1 2 E U k 2 F + 16\u03b1 2 \u03c1 2 nL 2 1 \u2212 \u03c1 2 E G k 2 + 16\u03b1 2 \u03c1 2\u03c32 L 2 1 \u2212 \u03c1 2 + 8n\u03c1 2\u03c32 .(153)", "formula_coordinates": [29.0, 72.0, 545.69, 472.83, 106.26]}, {"formula_id": "formula_170", "formula_text": "K\u22121 k=0 E U k 2 F = K\u22121 k=0 E G k+2 \u2212 G k+1 \u2212 G k + G k\u22121 2 F (154) \u22642 K\u22121 k=0 E G k+2 \u2212 G k+1 2 F + 2 K\u22121 k=0 E G k \u2212 G k\u22121 2 F (155) \u22644 K\u22121 k=0 E G k+2 \u2212 G k+1 2 F (156) =4 K\u22121 k=0 n i=1 E \u2207f (x k+1,i ) \u2212 \u2207f (x k,i ) 2 (157) \u22644L 2 K\u22121 k=0 n i=1 E x k+1,i \u2212 x k,i 2 (158) =4L 2 K\u22121 k=0 E X k+1 \u2212 X k 2 F .(159)", "formula_coordinates": [29.0, 160.28, 685.92, 379.72, 30.55]}, {"formula_id": "formula_171", "formula_text": "K\u22121 k=0 E U k 2 F (160) \u22644L 2 K\u22121 k=0 E X k+1 \u2212 X k 2 F (161) \u226416(1 + \u03c1 2 )L 2 K\u22121 k=0 E X k \u2212 X k 1 n 2 F + 16\u03b1 2 \u03c1 2 L 2 K\u22121 k=0 E Y k \u2212 Y k 1 n 2 F + 16\u03b1 2 nL 2 K\u22121 k=0 E Y k 2 (162) \u2264 32\u03c1 2 \u03b1 2 (1 + \u03c1) 2 nK\u03c2 2 0 L 2 (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 + 512\u03c1 4 (1 + \u03c1)\u03b1 4 nL 4 (1 \u2212 \u03c1) 2 (1 \u2212 \u221a \u03c1) 2 K\u22121 k=0 E G k 2 + 256\u03c1 4 (1 + \u03c1)\u03b1 2 L 2 (1 \u2212 \u03c1) 2 (1 \u2212 \u221a \u03c1) 2 K\u22121 k=0 E U k 2 F (163) + 256\u03c1 4 \u03b1 2 n\u03c3 2 (1 + \u03c1) 2 KL 2 (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 (164) + 16\u03b1 2 \u03c1 2 (1 + \u03c1)nK\u03c2 2 0 L 2 1 \u2212 \u221a \u03c1 + 128\u03b1 4 \u03c1 6 (1 + \u03c1)L 4 nK\u03c2 2 0 (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 (165) + 16\u03b1 2 \u03c1 2 (1 + \u03c1)L 2 1 \u2212 \u221a \u03c1 + 128\u03b1 4 \u03c1 6 (1 + \u03c1)L 4 (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 K\u22121 k=0 4\u03c1 2 1 \u2212 \u03c1 2 E U k 2 F (166) + 16\u03b1 2 \u03c1 2 (1 + \u03c1)L 2 1 \u2212 \u221a \u03c1 + 128\u03b1 4 \u03c1 6 (1 + \u03c1)L 4 (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 K\u22121 k=0 16\u03b1 2 \u03c1 2 nL 2 1 \u2212 \u03c1 2 E G k 2 + 16\u03b1 2 \u03c1 2\u03c32 L 2 1 \u2212 \u03c1 2 + 8n\u03c1 2\u03c32 (167) + 16\u03b1 2 nL 2 K\u22121 k=0 E Y k 2 (168) \u2264 64\u03c1 2 \u03b1 2 (1 + \u03c1) 2 nK\u03c2 2 0 L 2 (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 + 512\u03c1 4 (1 + \u03c1)\u03b1 2 L 2 (1 \u2212 \u03c1) 2 (1 \u2212 \u221a \u03c1) 2 K\u22121 k=0 E U k 2 F + 512\u03c1 4 \u03b1 2 n\u03c3 2 (1 + \u03c1) 2 KL 2 (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 (169) + 32\u03b1 2 nL 2 K\u22121 k=0 E G k 2 ,(170)", "formula_coordinates": [30.0, 80.88, 276.04, 459.12, 370.04]}, {"formula_id": "formula_172", "formula_text": "K\u22121 k=0 E U k 2 F \u2264 128\u03c1 2 \u03b1 2 (1 + \u03c1) 2 nK\u03c2 2 0 L 2 (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 + 1024\u03c1 4 \u03b1 2 n\u03c3 2 (1 + \u03c1) 2 KL 2 (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 + 64\u03b1 2 nL 2 K\u22121 k=0 E G k 2 , (171", "formula_coordinates": [30.0, 85.52, 689.8, 449.94, 30.55]}, {"formula_id": "formula_173", "formula_text": ")", "formula_coordinates": [30.0, 535.46, 699.55, 4.54, 9.96]}, {"formula_id": "formula_174", "formula_text": "K\u22121 k=0 E X k \u2212 X k 1 n 2 F (172) \u2264 4\u03c1 2 \u03b1 2 (1 + \u03c1)nK\u03c2 2 0 (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 + 544\u03c1 4 \u03b1 4 nL 2 (1 \u2212 \u03c1) 2 (1 \u2212 \u221a \u03c1) 2 K\u22121 k=0 E G k 2 + 32\u03c1 4 \u03b1 2 n\u03c3 2 (1 + \u03c1)K (1 \u2212 \u03c1)(1 \u2212 \u221a \u03c1) 2 ,(173)", "formula_coordinates": [31.0, 134.28, 96.41, 405.72, 65.73]}, {"formula_id": "formula_175", "formula_text": "K\u22121 k=0 \u03b1(1 \u2212 \u03b1L \u2212 24\u03b1 2 L 2 ) G k 2 + K\u22121 k=0 \u03b1E \u2207f X k 2 (174", "formula_coordinates": [31.0, 172.5, 206.62, 362.96, 30.55]}, {"formula_id": "formula_176", "formula_text": ")", "formula_coordinates": [31.0, 535.46, 216.36, 4.54, 9.96]}, {"formula_id": "formula_177", "formula_text": "\u22642\u2206 + \u03b1 2\u03c32 LK n + 16\u03b1L 2 n K\u22121 k=0 E X k \u2212 X k 1 n 2 F + 24\u03b1 3\u03c32 L 2 K n .(175)", "formula_coordinates": [31.0, 163.09, 241.8, 376.91, 30.55]}, {"formula_id": "formula_178", "formula_text": "1 K K\u22121 k=0 E \u2207f X k 2 \u2264 O \u2206 \u03b1K + \u03b1\u03c3 2 L n + \u03c1 2 \u03b1 2 L 2 \u03c2 2 0 (1 \u2212 \u03c1) 3 + \u03c1 4 \u03b1 2\u03c32 L 2 (1 \u2212 \u03c1) 3 + \u03b1 2\u03c32 L 2 n ,(176)", "formula_coordinates": [31.0, 130.36, 304.87, 409.65, 30.55]}, {"formula_id": "formula_179", "formula_text": "\u03b1 = 1 \u03c3 KL/n\u2206 + \u03c1 2 3 L 2 3 \u03c2 2 3 0 K 1 3 \u2206 1 3 (1\u2212\u03c1) + 32L (1\u2212\u03c1) 2 ,(177)", "formula_coordinates": [31.0, 218.42, 363.46, 321.58, 35.89]}, {"formula_id": "formula_180", "formula_text": "1 K K\u22121 k=0 E \u2207f X k 2 \u2264 O \u221a \u2206L\u03c3 \u221a nK + (\u03c1\u2206L\u03c2 0 ) 2 3", "formula_coordinates": [31.0, 133.33, 424.76, 213.21, 35.32]}, {"formula_id": "formula_181", "formula_text": "2 3 + \u03c1 2 n\u2206L (1 \u2212 \u03c1) 3 K + \u2206L (1 \u2212 \u03c1) 2 K . (178", "formula_coordinates": [31.0, 344.73, 431.62, 190.73, 23.89]}, {"formula_id": "formula_182", "formula_text": ")", "formula_coordinates": [31.0, 535.46, 439.26, 4.54, 9.96]}, {"formula_id": "formula_183", "formula_text": "1 K K\u22121 k=0 E \u2207f X k 2 \u2264 O \u221a \u2206L\u03c3 \u221a nBT + (\u03c1\u2206L\u03c2 0 R) 2 3", "formula_coordinates": [31.0, 133.01, 488.47, 219.38, 35.31]}, {"formula_id": "formula_184", "formula_text": "2 3 + \u03c1 2 nR\u2206L (1 \u2212 \u03c1) 3 T + R\u2206L (1 \u2212 \u03c1) 2 T ,(179)", "formula_coordinates": [31.0, 345.77, 495.33, 194.23, 23.89]}, {"formula_id": "formula_185", "formula_text": "set R = 1 1 \u2212 \u03bb 2 (W ) max 1 2 log(n), 1 2 log \u03c2 2 0 T \u2206L , we first have \u03c1 \u2264 1/ \u221a 2 since R \u2265 log(n) 2 1 \u2212 \u03bb 2 (W ) \u2265 \u2212 log(n) 2 log(1 \u2212 1 \u2212 \u03bb 2 (W )) \u21d2 1 \u2212 1 \u2212 \u03bb 2 (W ) R \u2264 1 \u221a n \u2264 1 \u221a 2 ,(180)", "formula_coordinates": [31.0, 71.35, 533.65, 468.65, 88.71]}, {"formula_id": "formula_186", "formula_text": "1 K K\u22121 k=0 E \u2207f X k 2 \u2264 O \u221a \u2206L\u03c3 \u221a nBT + (\u03c1\u2206L\u03c2 0 R)", "formula_coordinates": [31.0, 122.28, 649.5, 214.79, 35.31]}, {"formula_id": "formula_189", "formula_text": "T \u2264 O \u2206L\u03c3 2 nB 4 ,(187)", "formula_coordinates": [32.0, 267.44, 254.29, 272.56, 23.89]}, {"formula_id": "formula_190", "formula_text": "we have \u221a \u2206L\u03c3 \u221a nBT \u2264 O( 2 ).(188)", "formula_coordinates": [32.0, 71.35, 289.02, 468.65, 43.05]}, {"formula_id": "formula_191", "formula_text": "T \u2264 O max log(n)\u2206L 2 1 \u2212 \u03bb 2 (W ) , \u2206L 2 1 \u2212 \u03bb 2 (W ) log \u03c2 2 0 2 \u2206L ,(189)", "formula_coordinates": [32.0, 163.73, 362.66, 376.27, 26.19]}, {"formula_id": "formula_192", "formula_text": "\u2206L T 1 \u2212 \u03bb 2 (W )", "formula_coordinates": [32.0, 193.11, 418.69, 64.84, 24.61]}, {"formula_id": "formula_193", "formula_text": "\u03c2 2 0 T \u2206L \u2264 O( 2 ),(190)", "formula_coordinates": [32.0, 347.95, 417.12, 192.05, 23.89]}, {"formula_id": "formula_194", "formula_text": "\u2206L T 1 \u2212 \u03bb 2 (W ) log \u03c2 2 0 T \u2206L = 2 log \u03c2 2 0 2 \u2206L log \u03c2 2 0 2 \u2206L log \u03c2 2 0 2 \u2206L \u2264 O( 2 ).(191)", "formula_coordinates": [32.0, 169.24, 472.56, 370.76, 36.98]}, {"formula_id": "formula_195", "formula_text": "T \u2264O \u2206L\u03c3 2 nB 4 + max log(n)\u2206L 2 1 \u2212 \u03bb 2 (W ) , \u2206L 2 1 \u2212 \u03bb 2 (W ) log \u03c2 2 0 2 \u2206L (192", "formula_coordinates": [32.0, 147.24, 542.64, 388.22, 26.19]}, {"formula_id": "formula_196", "formula_text": ")", "formula_coordinates": [32.0, 535.46, 550.28, 4.54, 9.96]}, {"formula_id": "formula_197", "formula_text": "=O \u2206L\u03c3 2 nB 4 + \u2206L 2 1 \u2212 \u03bb 2 (W ) log n + \u03c2 0 n \u221a \u2206L ,(193)", "formula_coordinates": [32.0, 157.21, 576.51, 382.79, 26.18]}, {"formula_id": "formula_198", "formula_text": "\u2126 \u2206L\u03c3 2 B 4 . (194", "formula_coordinates": [33.0, 278.03, 130.45, 257.44, 23.89]}, {"formula_id": "formula_199", "formula_text": ")", "formula_coordinates": [33.0, 535.46, 138.1, 4.54, 9.96]}, {"formula_id": "formula_200", "formula_text": "\u2126 \u2206LD 2 . (195", "formula_coordinates": [33.0, 279.03, 234.48, 256.44, 18.85]}, {"formula_id": "formula_201", "formula_text": ")", "formula_coordinates": [33.0, 535.46, 240.55, 4.54, 9.96]}, {"formula_id": "formula_202", "formula_text": "\u2126 \u2206L\u03c3 2 B 4 + \u2206LD 2 . (196", "formula_coordinates": [33.0, 258.94, 287.93, 276.52, 23.89]}, {"formula_id": "formula_203", "formula_text": ")", "formula_coordinates": [33.0, 535.46, 295.58, 4.54, 9.96]}], "doi": ""}