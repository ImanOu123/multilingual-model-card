{"title": "TA-DA: Topic-Aware Domain Adaptation for Scientific Keyphrase Identification and Classification (Student Abstract)", "authors": "R\u0203zvan-Alexandru Sm\u0203du; George-Eduard Zaharia; Andrei-Marius Avram; Dumitru-Clementin Cercel; Mihai Dascalu; Florin Pop", "pub_date": "", "abstract": "Keyphrase identification and classification is a Natural Language Processing and Information Retrieval task that involves extracting relevant groups of words from a given text related to the main topic. In this work, we focus on extracting keyphrases from scientific documents. We introduce TA-DA, a Topic-Aware Domain Adaptation framework for keyphrase extraction that integrates Multi-Task Learning with Adversarial Training and Domain Adaptation. Our approach improves performance over baseline models by up to 5% in the exact match of the F1-score.", "sections": [{"heading": "Introduction", "text": "Scientific Keyphrase Identification and Classification (SKIC) refers to the labeling of relevant words from a given input scientific document, which the SemEval-2017 workshop ) introduced in Task 10. This task is related to Named Entity Recognition (NER), where the aim is to extract and classify the named entities from a given set of documents. SKIC can be more challenging than NER problems due to the lack of available annotated scientific publications .\nThis work focuses on using domain adaptation through adversarial training and adversarial examples to address SKIC. We introduce a Topic-Aware Domain Adaptive (TA-DA) deep neural network framework that incorporates multi-task learning and adversarial learning. Figure 1 briefly presents the training procedure. Experiments showed that each component improves the performance of our model on all considered datasets. We summarize our main contributions as follows:\n\u2022 We propose a novel neural network architecture integrating multi-task learning, adversarial training, domain adaptation, and topic modeling for jointly addressing SKIC tasks;\n\u2022 Different techniques for latent representations required for topic modeling (i.e., LSA, NMF, K-Means, LDA) are considered, and LDA provided the most promising overall results;\nCopyright \u00a9 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. \u2022 We show that domain adaptation improves performance compared to baselines, while adding adversarial examples further increases the F1-scores.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Methodology", "text": "Data Representation. We cast the problem of SKIC to a tagging task. Given a document d = {w 1 , ..., w n } with n words, our goal is to identify and classify keyphrases by outputting a sequence {y 1 , ..., y n }, where each y i is a class from the BIO schema (Lample et al. 2016 (Ganin and Lempitsky 2015) to the feature extractor. The output is computed using a softmax layer that models a probability distribution across domains, i.e. the topics extracted using LDA.\nAdversarial Learning. We perform domain adaptation through adversarial training (Ganin and Lempitsky 2015). In this configuration, adversarial training seeks the saddle point that minimizes the expected likelihood of the loss associated with the label predictor while maximizing the likelihood for the discriminator. Additionally, we employ adversarial examples via Fast Gradient Sign Method (Goodfellow, Shlens, and Szegedy 2015) that adds a small value to the input (in our case, the latent space generated by SciBERT) proportional to the gradient in order to create more robust models.\nThe new input is defined as follows:\nx adv = x + sign(\u2207 x L(\u03b8, x, y))\n(1) where is the perturbation factor.\nOptimization. The optimization process involves minimizing the negative log-likelihood of the CRF output and the categorical cross-entropy of the softmax outputs. The total loss is described by:\nL total = L tag + L class + \u03bbL da (2)\nwhere L tag is the KI loss, L class is the KC loss, \u03bb controls the degree of enforced domain adaptation, and L da is the domain adaptation loss. In the case of using adversarial examples, the loss L adv total is similar to (2), and both losses for each pass are added during the training step.", "publication_ref": ["b6", "b4", "b4", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Datasets. We evaluate our approach on three datasets: SemEval-2017 Task 10 Dataset , SciERC (SciIE) Dataset (Luan et al. 2018), and ACL RD-TEC 2.0 Dataset (QasemiZadeh and Schumann 2016).\nExperimental Results. The results are shown in Table 1. Our approach (i.e., DA-Adv) obtains the best or second-best scores concerning the baselines. Although domain adaptation may hinder performance by 1 to 2%, we observe that adversarial examples enhance the model's performance in every scenario. Additionally, including LSA instead of LDA for topic modeling (i.e., DA-LSA), we observe similar performances, albeit on average, LDA performs better. Conversely, NMF (i.e., DA-NMF) and K-Means (i.e., DA-KM) hinder performances when employed as topic models. Compared with LSTM-CRF (Lample et al. 2016), our approach is comparable or performs better. Also, our model has considerably fewer trainable parameters (2.4M) than SciBERT (Beltagy, Lo, and ", "publication_ref": ["b7", "b6"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Conclusions", "text": "We proposed TA-DA, a topic-aware domain adaptation approach for tackling SKIC. We conducted multiple experiments and analyzed the performance impact of adversarial learning and topic modeling. We observed that the adversarially trained models outperform other models on one dataset, while considerable improvements are achieved in contrast to LSTM-CRF-based models. Moreover, we showed that domain adaptation and adversarial examples improved results over baselines. In future work, we aim to study a hierarchical representation of the input to increase robustness further.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "The research has been funded by the University Politehnica of Bucharest through the PubArt program.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Se-mEval", "journal": "ScienceIE -Extracting Keyphrases and Relations from Scientific Publications", "year": "2017", "authors": "I Augenstein; M Das; S Riedel; L Vikraman; A Mccallum"}, {"ref_id": "b1", "title": "Multi-Task Learning of Keyphrase Boundary Classification", "journal": "", "year": "2017", "authors": "I Augenstein; A S\u00f8gaard"}, {"ref_id": "b2", "title": "SciBERT: A Pretrained Language Model for Scientific Text", "journal": "", "year": "2019", "authors": "I Beltagy; K Lo; A Cohan"}, {"ref_id": "b3", "title": "", "journal": "Latent dirichlet allocation. JMLR", "year": "2003", "authors": "D M Blei; A Y Ng; M I Jordan"}, {"ref_id": "b4", "title": "Unsupervised domain adaptation by backpropagation", "journal": "", "year": "2015", "authors": "Y Ganin; V Lempitsky"}, {"ref_id": "b5", "title": "Explaining and Harnessing Adversarial Examples", "journal": "", "year": "2015", "authors": "I J Goodfellow; J Shlens; C Szegedy"}, {"ref_id": "b6", "title": "Neural Architectures for Named Entity Recognition", "journal": "", "year": "2016", "authors": "G Lample; M Ballesteros; S Subramanian; K Kawakami; C Dyer"}, {"ref_id": "b7", "title": "Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction", "journal": "", "year": "2018", "authors": "Y Luan; L He; M Ostendorf; H Hajishirzi"}, {"ref_id": "b8", "title": "The ACL RD-TEC 2.0: A language resource for evaluating term extraction and entity recognition methods", "journal": "", "year": "2016", "authors": "B Qasemizadeh; A.-K Schumann"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: The training procedure for TA-DA employing domain adaptation and adversarial training.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "It associates each document with a mixture of topics as probability distributions of words that are thematically related based on tight co-occurrence. To further enhance the latent space, this representation is fed into a stack of BiLSTM layers, whose outputs are concatenated along with the SciBERT's output. We use multi-task learning to incorporate branches for each task that share the feature representation. After the feature extractor, arXiv:2301.06902v1 [cs.CL] 30 Dec 2022 fully connected layers are used for keyphrase identification (KI) and classification (KC) tasks. For KI, the output is a linear-chain CRF, which models the probability of having the output label given the latent representation. For KC, a softmax activation function is considered to model a probability distribution across classes. We incorporate adversarial training by including a domain discriminator linked via a gradient reversal layer", "figure_data": "Neural Network Architecture. We propose a neural net-work architecture starting with a pre-trained SciBERT (Belt-agy, Lo, and Cohan 2019) that generates contextualized em-beddings."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Cohan 2019) (109M) and improved results when compared with similar approaches (i.e., MTL-LSTM (Augenstein and S\u00f8gaard 2017)).", "figure_data": "ModelSemEval KI KIC KISciIE KIC KI ACL RD-TEC KICDA-Adv53.6 42.6 78.4 65.9 82.570.0DA-LSA53.2 41.5 78.9 66.8 82.769.5DA-NMF54.0 41.1 76.6 63.6 81.669.3DA-KM49.7 38.0 77.7 65.3 81.569.0LSTM-58.6 33.7--74.435.5CRFMTL-67.7 38.0 72.3 58.1 81.958.5LSTMSciBERT66.7 48.2 81.0 67.4 79.865.1"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "F1-scores on the test sets (the highest scores are bolded, and the second-highest scores are underlined).", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "x adv = x + sign(\u2207 x L(\u03b8, x, y))", "formula_coordinates": [2.0, 106.44, 314.32, 133.63, 9.65]}, {"formula_id": "formula_1", "formula_text": "L total = L tag + L class + \u03bbL da (2)", "formula_coordinates": [2.0, 109.6, 389.21, 182.9, 9.65]}], "doi": ""}