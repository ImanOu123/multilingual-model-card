{"title": "Learning How to Ask: Querying LMs with Mixtures of Soft Prompts", "authors": "Guanghui Qin; Jason Eisner", "pub_date": "", "abstract": "Natural-language prompts have recently been used to coax pretrained language models into performing other AI tasks, using a fill-in-theblank paradigm (Petroni et al., 2019) or a few-shot extrapolation paradigm (Brown et al.,  2020). For example, language models retain factual knowledge from their training corpora that can be extracted by asking them to \"fill in the blank\" in a sentential prompt. However, where does this prompt come from? We explore the idea of learning prompts by gradient descent-either fine-tuning prompts taken from previous work, or starting from random initialization. Our prompts consist of \"soft words,\" i.e., continuous vectors that are not necessarily word type embeddings from the language model. Furthermore, for each task, we optimize a mixture of prompts, learning which prompts are most effective and how to ensemble them. Across multiple English LMs and tasks, our approach hugely outperforms previous methods, showing that the implicit factual knowledge in language models was previously underestimated. Moreover, this knowledge is cheap to elicit: random initialization is nearly as good as informed initialization.", "sections": [{"heading": "Introduction", "text": "Pretrained language models, such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), and BART (Lewis et al., 2020a), have proved to provide useful representations for other NLP tasks. Recently, Petroni et al. (2019) and Jiang et al. (2020) demonstrated that language models (LMs) also contain factual and commonsense knowledge that can be elicited with a prompt. For example, to query the date-of-birth of Mozart, we can use the Finding out what young children know is difficult because they can be very sensitive to the form of the question (Donaldson, 1978). Opinion polling is also sensitive to question design (Broughton, 1995). We observe that when we are querying an LM rather than a human, we have the opportunity to tune prompts using gradient descent-the workhorse of modern NLP-so that they better elicit the desired type of knowledge.\nA neural LM sees the prompt as a sequence of continuous word vectors (Baroni et al., 2014). We tune in this continuous space, relaxing the constraint that the vectors be the embeddings of actual English words. Allowing \"soft prompts\" consisting of \"soft words\" is not only convenient for optimization, but is also more expressive. Soft prompts can emphasize particular words (by lengthening their vectors) or particular dimensions of those words. They can also adjust words that are misleading, ambiguous, or overly specific. Consider the following prompt for the relation date-of-death:\nx performed until his death in y .\nThis prompt may work for the male singer Cab Calloway, but if we want it to also work for the female painter Mary Cassatt, it might help to soften \"performed\" and \"his\" so that they do not insist on the wrong occupation and gender, and perhaps to soften \"until\" into a weaker connective (as Cassatt was in fact too blind to paint in her final years). Another way to bridge between these cases is to have one prompt using \"performed\" and another using \"painted.\" In general, there may be many varied lexical patterns that signal a particular relation, and having more patterns will get better coverage (Hearst, 1992;Riloff and Jones, 1999). We therefore propose to learn a mixture of soft prompts.\nWe test the idea on several cloze language models, training prompts to complete factual and com-mon sense relations from 3 datasets. Comparing on held-out examples, our method dramatically outperforms previous work, even when initialized randomly. So when regarded as approximate knowledge bases, language models know more than we realized. We just had to find the right ways to ask.", "publication_ref": ["b19", "b5", "b13", "b20", "b10", "b6", "b3", "b0", "b9", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Factual knowledge is traditionally extracted from large corpora using a pipeline of NLP tools (Surdeanu and Ji, 2014), including entity extraction (Lample et al., 2016), entity linking (Rao et al., 2013) and relation extraction (Sorokin and Gurevych, 2017).\nHowever, recent work has shown that simply training a system to complete sentences-language modeling-causes it to implicitly acquire nonlinguistic abilities from its training corpora (Rogers et al., 2020), including factual knowledge (Petroni et al., 2019;Jiang et al., 2020), common sense (Bisk et al., 2019), reasoning (Talmor et al., 2020;Brown et al., 2020), summarization (Radford et al., 2019), and even arithmetic (Bouraoui et al., 2020).\nMost of the previous work manually creates prompts to extract answers from the trained language model. We use LAMA (Petroni et al., 2019) as a baseline. Building on LAMA, the LM Prompt And Query Archive (LPAQA) method (Jiang et al., 2020) searches for new prompts by either mining a corpus or paraphrasing existing prompts. AutoPrompt (Shin et al., 2020) searches for improved prompts using a gradient signal, although its prompts are limited to sequences of actual (\"hard\") English words, unlike our method. We compare our novel soft prompts against all of these systems.\nAfter we submitted the present paper in November 2020, two still unpublished manuscripts appeared on arXiv that also investigated soft prompts. Li and Liang (2021) considered the setting of generating text from a pretrained language model (GPT-2 or BART) conditioned on a textual prompt. To improve the results, they prepended a few taskspecific \"soft tokens\" to the prompt and tuned the embeddings of only these tokens (at all embedding layers). Liu et al. (2021) adopted a strategy similar to ours by tuning fill-in-the-blank prompts in a continuous space, testing on GPT-2 and BERT models, although they did not use the enhancements we proposed in \u00a7 \u00a73.2-3.4 below. Like our work, both these papers achieved strong gains.\nIn other work, Bouraoui et al. (2020) mine prompts from a corpus, then fine-tune the whole language model so that it more accurately completes the prompts. Schick and Sch\u00fctze (2020a,b) are similar but fine-tune the language model differently for each prompt. Our method complements these by tuning the prompts themselves. \"Probing\" systems that ask what language models know about particular sentences (e.g., Eichler et al., 2019) usually use feedforward networks rather than further natural-language prompts. Yet Shin et al. (2020) show how to use naturallanguage prompts to ask about particular sentences. Our method could potentially be applied to those prompts, or to \"few-shot learning\" prompts that include input-output examples (Brown et al., 2020).", "publication_ref": ["b32", "b12", "b22", "b30", "b24", "b20", "b10", "b1", "b21", "b2", "b20", "b10", "b28", "b2", "b7", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "Method", "text": "Our experiments will specifically aim at extracting relational knowledge from language models. We are given a fixed pretrained LM, a specific binary relation r such as date-of-death, and a training dataset E r consisting of known (x, y) pairs in r, such as (Mary Cassatt, 1926). We will then train a system to predict y from x, and evaluate it on held-out (x, y) pairs of the same relation.\nA prompt t is a sentence or phrase that includes two blanks, as illustrated in \u00a71. To pose the query, we fill the\nx blank with x: We can ask the LM for its probability distribution p LM (y | t, x) over single words that can now fill y . The correct answer would be 1926.\nMary", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Soft Prompts", "text": "Suppose the LM identifies the word types with vectors in R d . We also allow t to be a soft prompt, in which the tokens can be arbitrary vectors in R d :\nx v 1 v 2 v 3 v 4 v 5 y v 6\nWe can initialize these vectors to match those of a given hard prompt. (Each token of a hard prompt may be a word, subword, or punctuation mark, according to the tokenization procedure used by the LM.) However, we can then tune the vectors continuously. We do not change the number of vectors or their positions. For the prompt shown above, we have a 6d-dimensional search space.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Deeply Perturbed Prompts", "text": "For each token i of a prompt, the vector v i enters into the LM's computations that complete the prompt. For example, a Transformer architecture computes successively deeper contextual embeddings of the token, v\n( ) i : 0 \u2264 \u2264 L. Here v (0) i = v i and the embedding v ( ) i at layer > 0 is computed from all tokens' embeddings v ( \u22121) j\nat the previous layer, using the LM's parameters.\nWe can tune the prompt by additively perturbing each v ( ) i by a small vector \u2206 ( ) i before it is used in further computations. The \u2206 vectors for a given hard prompt are initialized to 0 and then tuned.\nPerturbing only layer 0 is equivalent to tuning v i directly as in \u00a73.1. However, if we are more aggressive and perturb all layers, we now have 6d \u2022 (L + 1) parameters to tune a 6-token prompt. The perturbations (\u2206 vectors) can be kept small through early stopping or some other form of regularization. Our intuition is that small perturbations will yield more \"familiar\" activation patterns that are similar to those that the LM was originally trained on. (Li and Liang (2021) tried a rather different approach to preventing overfitting when tuning all layers.)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Mixture Modeling", "text": "Given a set T r of soft prompts for relation r, we can define the ensemble predictive distribution\np(y | x, r) = t\u2208Tr p(t | r) \u2022 p LM (y | t, x) (1)\nwhere the learned mixture weights p(t | r) form a distribution over the soft prompts t \u2208 T r . Ensembling techniques other than mixture-of-experts could also be used, including product-of-experts (Jiang et al., 2020).", "publication_ref": ["b10"], "figure_ref": [], "table_ref": []}, {"heading": "Data-Dependent Mixture Modeling", "text": "As an extension, we can replace the mixture weights p(t | r) with p(t | r, x), to allow the model to select prompts that are appropriate for the given x. For example, a plural noun x might prefer prompts t that use a plural verb.\nWhile we could directly build a neural softmax model for p(t | r, x), it seems useful to capture the intuition that t may work better if x is plausible in its\nx . Thus, we instead use Bayes' Theorem to write p(t | r, x) as proportional to p(t | r) \u2022 p(x | t, r) 1/T , where we have included T to modulate the strength of the above intuition. 1 Here p(t | r) is still a learned distribution over prompts, and we use the fixed language model to estimate the second factor as y p LM (x, y | t) (dropping the dependence on r just as we did for the second factor of (1)). log T is tuned along with all other parameters.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Training Objective", "text": "Given an initial set of prompts T r , we jointly optimize the soft prompts t \u2208 T and their mixture weights p(t | r) (and log T in \u00a73.4) to minimize the log-loss of the predictive distribution (1):\n(x,y)\u2208Er \u2212 log t\u2208Tr p(y | t, x) (2)\nThis is a continuous and differentiable objective whose gradient can be computed by backpropagation. It can be locally minimized by gradient descent (using a softmax parameterization of the mixture weights). Equivalently, it can be locally minimized by the EM algorithm: the E step finds a posterior distribution over latent prompts for each (x, y) example, and the M step performs gradient descent to optimize the prompts in that mixture.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Relational Datasets", "text": "The relations we learn to predict are T-REx original (Elsahar et al., 2018), T-REx extended (Shin et al., 2020), Google-RE (Orr, 2013), and ConceptNet (Speer et al., 2017)-or rather, the subsets that were used by the LAMA and AutoPrompt papers. See Appendix A for some statistics.", "publication_ref": ["b8", "b28", "b18", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "Language Models", "text": "Following Petroni et al. (2019), we interrogate BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). These are masked (cloze) language models. For variety, we also interrogate BART (Lewis et al., 2020a), which conditions on the prompt with empty y and generates a copy where y has been filled in (by a single token). We constrain BART's decoding to ensure that its answer does take this form. Unlike BERT and RoBERTa, BART could be used to fill y with an arbitrarily long phrase, but we do not allow this because y in our datasets is always a single token. 2", "publication_ref": ["b20", "b5", "b17", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Dataset Splits", "text": "For the two T-REx datasets, we inherit the trainingvalidation-test split from Shin et al. (2020). For the other datasets, we split randomly in the ratio 80-10-10. 3 Since all pairs (x, y) are distinct, there are no common triples among these three sets. Common x values are also rare because each dataset has at least 174 distinct x values. However, the number of distinct y values can be as small as 6. Thus, in another set of experiments (Appendix E), we used a more challenging split that ensures that there are no common y values among these three sets. This tests whether our model generalizes to unseen values.", "publication_ref": ["b28"], "figure_ref": [], "table_ref": []}, {"heading": "Prompts", "text": "For the T-REx and Google-RE datasets, we have four sources of initial prompts:\n\u2022 (sin.) LAMA provides a single manually created hard prompt for each relation type r.\n\u2022 (par.) LPAQA (Jiang et al., 2020) provides a set of 13-30 hard prompts for each r, which are paraphrases of the LAMA prompt. 4\n\u2022 (min.) LPAQA also provides a set of 6-29 hard prompts for each r, based on text mining.\n\u2022 (ran.) For each (min.) prompt, we replace each word with a random vector, drawn from a Gaussian distribution fit to all of the LM's word embeddings. The number of words and the position of the blanks are preserved.\nFor the ConceptNet dataset, LAMA uses the gold Open Mind Common Sense (OMCS) dataset (Singh et al., 2002). In this dataset, each example (x i , y i ) is equipped with its own prompt t i . (Each example is really a sentence with two substrings marked as x and y, which are removed to obtain t i .) These prompts are often overly specific: often y i can be predicted from (t i , x i ), or just from t i alone, but y j cannot be predicted from (t i , x j ). Thus, for each relation r, we use only the prompts that appear more than 10 times, resulting in 1-38 prompts.\nStatistics about the prompts are in Appendix B. We used only a single copy of each prompt, but a generalization would be to allow multiple slightly perturbed copies of each prompt, which could diverge and specialize during training (Rose, 1998).", "publication_ref": ["b10", "b29", "b25"], "figure_ref": [], "table_ref": []}, {"heading": "Training", "text": "We optimize equation ( 2) with the method introduced in \u00a73.5. We use the Adam optimizer (Kingma and Ba, 2015) with its default configuration. For gradient training, we set the batch size as 64, early-stop patience as 4, and test with the model that performs best on the dev set among 16 training epochs.\nTraining is fast. Even for our largest model (BERT-large-cased) and largest dataset (T-REx extended), tuning a single prompt completes within a few minutes. With a mixture of prompts, training scales roughly linearly with the number of prompts. It is still presumably much cheaper in time and memory than fine-tuning the entire BERT model, which must back-propagate a much larger set of gradients.", "publication_ref": ["b11"], "figure_ref": [], "table_ref": []}, {"heading": "Metrics and Baselines", "text": "Our method outputs the most probable y given (r, x). Here and in the supplementary material, we report its average performance on all test examples, with precision-at-1 (P@1), precision-at-10 (P@10) and mean reciprocal rank (MRR) as metrics. We measure the improvement from tuning LAMA, LPAQA, and random prompts. We also compare with AutoPrompt. Baseline numbers come from prior papers or our reimplementations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Table 1 shows results on T-REx datasets obtained by querying three BERT-style models, with P@1 as the metric. Additional metrics and language models are shown in Tables 2 and 3 as well as Tables 5 and 6 in the supplementary material.\nWe consistently get large improvements by tuning the initial prompts. Remarkably, our method beats all prior methods even when throwing away the words of their informed prompts in favor of random initial vectors. It simply finds a prompt that works well on the (x, y) training examples.\nWe conduct an ablation study where we adjust only the mixture weights (which are initially uni- 39.4 \u2020 37.8 \u2020 Soft (sin., BEl) 51.1 (+22.2) 51.4 (+27.4) Soft (min., BEl) 51.6 (+12.2) 52.5 (+14.7) Soft (par., BEl) 51.1 (+11.7) 51.7 (+13.9) Soft (ran., BEl) 51.9 (+47.1) 51.9 (+50.5) AutoPrompt 40.0 -Soft (min., Rob) 40.6 ? (+39.4) -Table 1: Results on T-REx datasets with P@1 as the metric. The \"Soft\" lines (our method) parenthetically show the improvement over the initial parameters (boldfaced if significant). In each subcolumn of comparable results, we boldface the best result along with all that are not significantly worse (sign test, p < 0.02).\n(We marked a boldface number with \"?\" if we lacked access to per-example output for one of the systems; differences from such systems were simply assumed to be significant.) \u2020 marks baseline results obtained from our reimplementations. In the Model column, BEb is BERT-base, BEl is BERT-large, Rob is RoBERTa-base. form) or only the word vectors in the prompts t. As Table 4 shows, each helps, but the major benefit comes from tuning the word vectors to get soft prompts. Appendix C visualizes a set of soft prompts, and Appendix D analyzes the mixture weights. We also experiment on a challenging setting where the y labels are distinct for training and test (Appendix E in the supplementary materials), and find that soft prompts still yield some benefits.\nThe above results are for our basic method that tunes only the words of the prompt (i.e., layer 0). When we tune all layers-the \"deeply perturbed prompts\" of \u00a73.2-we typically obtain small additional gains, across various models and initializations, although tuning all layers does substantially hurt RoBERTa. These results are shown in Tables 5  and 6 in the supplementary material.\nThe tables show that the winning systemfor each combination of language model, T-REx dataset, and evaluation metric-always uses a mixture of soft prompts initialized to mined prompts. It always tunes all layers, except with RoBERTa.\nFinally, we also tried using data-dependent mix-  33.5 (+ 6.5) 18.9 (+3.3) Soft (min.) 12.9 (+2.3) 34.7 (+11.0) 20.3 (+5.0) Soft (par.) 11.5 (+0.9) 31.4 (+ 7.7) 18.3 (+3.0) Table 2: Results on Google-RE dataset obtained by querying the BERT-large-cased model.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3", "tab_4"]}, {"heading": "Model", "text": "P@1 P@10 MRR LAMA (BEb) 0.1 \u2020 2.6 \u2020 1.5 \u2020 LAMA (BEl) 0.1 \u2020 5.0 \u2020 1.9 \u2020 Soft (min.,BEb) 11.3(+11.2) 36.4(+33.8) 19.3(+17.8) Soft (ran.,BEb) 11.8(+11.8) 34.8(+31.9) 19.8(+19.6) Soft (min.,BEl) 12.8(+12.7) 37.0(+32.0) 20.9(+19.0) Soft (ran.,BEl) 14.5(+14.5) 38.6(+34.2) 22.1(+21.9)  ture weights as in \u00a73.4. This had little effect, because training learned to discard the x information by setting the temperature parameter T high.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "Well-crafted natural language prompts are a powerful way to extract information from pretrained language models. In the case of cloze prompts used to query BERT and BART models for single-word answers, we have demonstrated startlingly large and consistent improvements from rapidly learning prompts that work-even though the resulting \"soft prompts\" are no longer natural language. Our code and data are available at https:// github.com/hiaoxui/soft-prompts.\nHow about few-shot prediction with pretrained generative LMs? Here, Lewis et al. (2020b) show how to assemble a natural language prompt for input x from relevant input-output pairs (x i , y i ) selected by a trained retrieval model. Allowing fine-tuned soft string pairs is an intriguing future possibility for improving such methods without needing to fine-tune the entire language model.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Statistics of Relational Databases", "text": "The statistics of the various relational databases are shown in Table 8.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_8"]}, {"heading": "B Statistics of the Initial Prompts", "text": "Table 7 shows some statistics of the prompts we use to initialize the SoftPrompt model.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "C Visualization of Soft Prompts", "text": "Figure 1 shows what a mixture of soft prompts looks like when we tune only layer 0. The soft prompts are not too interpretable. The words closest to the tuned tokens (shown in blue) seem to be largely on the music topic. However, the soft templates do not seem to form meaningful phrases, nor is it obvious why they would prime for y to be an instrument when x is a musician.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "D Entropy of the Mixture Model", "text": "For any given relation r, the entropy of the mixture weights is\nH = t\u2208Tr p(t | r) \u2022 log 2 p(t | r)(3)\nWe then take 2 H \u2208 [1, |T r |] as a measure of the effective number of prompts that were retained. Table 10 shows some statistics of the effective number of prompts. In some cases, tuning the mixture weights essentially selected a single prompt, but on average, it settled on a mixture of several variant prompts (as illustrated by Figure 1).", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": ["tab_10"]}, {"heading": "E Challenging dataset with distinct y's", "text": "As described in \u00a74.3, we conducted an additional experiment to determine whether the prompts could generalize to novel y values. We conduct another experiment and ensure that there are no common y values among the train / dev / test sets. We use T-REx as the base relational database and split the datasets to make the ratio close to 80-10-10. The experiment results are shown in Table 9. We can observe that our method again improves the results, just as in Tables 5 and 6, which shows the generalizability of our method.  We show the effect of tuning the layer-0 token embeddings (but not higher layers) on BERT-large-cased. The prompts are sorted in decreasing order by mixture weight. Each prompt's weight is shown at left; note that after the first 12 prompts, the remaining ones have negligible contribution. We show each soft prompt in blue, followed by the original (mined) prompt in red. To visualize the tuned vector v, we display the blue word w that maximizes p(w | v). The brightness of the blue word w and the original red word w 0 are respectively proportional to p(w | v) and p(w 0 | v). The red word has size 1, and the blue word has size ||v||/||v 0 ||, where v 0 is the original untuned vector (the embedding of w 0 ). In this example, the blue probabilities p(w | v) range from 6.5e-5 to 9.7e-5 (mean 8.6e-5 \u00b1 8.1e-6), the red probabilities p(w 0 | v) range from 7.7e-5 to 1.1e-4 (mean 9.5e-5 \u00b1 7.8e-6), and the relative magnitudes ||v||/||v 0 || vary from 1.00 to 1.49 (mean 1.12 \u00b1 0.13).     5.4 3.9 1.2 18.4 ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_9"]}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for helpful comments. This work was supported by DARPA KAIROS and by the National Science Foundation under Grant No. 1718846. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes. The views and conclusions contained in this publication are those of the authors, and should not be interpreted as representing official policies nor endorsement by the funding agencies or by Microsoft (where Dr. Eisner is also a paid employee, in an arrangement that has been reviewed and approved by the Johns Hopkins University in accordance with its conflict of interest policies).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "In the results block, \"init\" uses the initial untuned prompts; \"soft\" starts at \"init\" and tunes the prompts (layer 0) and mixture weights; and \"deep\" starts at \"init\" and tunes all the layers. Numbers above the arrows are the relative change in the performance. Within each block, we boldface the best system and all those that are not significantly worse (paired permutation test, p < 0.02). We also boldface the relative changes that are significantly different from 0. Other symbols are as in Table 1.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "LM Method", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors", "journal": "", "year": "2014", "authors": "Marco Baroni; Georgiana Dinu; Germ\u00e1n Kruszewski"}, {"ref_id": "b1", "title": "PIQA: Reasoning about physical commonsense in natural language", "journal": "", "year": "2019", "authors": "Yonatan Bisk; Rowan Zellers; Jianfeng Ronan Le Bras; Yejin Gao;  Choi"}, {"ref_id": "b2", "title": "Inducing relational knowledge from BERT", "journal": "", "year": "2020", "authors": "Zied Bouraoui; Jose Camacho-Collados; Steven Schockaert"}, {"ref_id": "b3", "title": "The assumptions and theory of public opinion polling", "journal": "Springer", "year": "1995", "authors": "David Broughton"}, {"ref_id": "b4", "title": "", "journal": "", "year": "", "authors": "Tom B Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell; Sandhini Agarwal; Ariel Herbert-Voss; Gretchen Krueger; Tom Henighan; Rewon Child; Aditya Ramesh; Daniel M Ziegler; Jeffrey Wu; Clemens Winter; Christopher Hesse; Mark Chen; Eric Sigler; Mateusz Litwin; Scott Gray"}, {"ref_id": "b5", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019", "authors": "J Devlin; M Chang; K Lee; K Toutanova"}, {"ref_id": "b6", "title": "Children's Minds. W. W. Norton", "journal": "", "year": "1978", "authors": "M C Donaldson"}, {"ref_id": "b7", "title": "LINSPECTOR WEB: A multilingual probing suite for word representations", "journal": "", "year": "2019", "authors": "Max Eichler; G\u00f6zde G\u00fcl\u015fahin; Iryna Gurevych"}, {"ref_id": "b8", "title": "T-REx: A large scale alignment of natural language with knowledge base triples", "journal": "", "year": "2018", "authors": "Hady Elsahar; Pavlos Vougiouklis; Arslen Remaci; Christophe Gravier; Jonathon Hare; Elena Simperl; Frederique Laforest"}, {"ref_id": "b9", "title": "Automatic acquisition of hyponyms from large text corpora", "journal": "", "year": "1992", "authors": "M A Hearst"}, {"ref_id": "b10", "title": "How can we know what language models know?", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020-06", "authors": "Zhengbao Jiang; Frank F Xu"}, {"ref_id": "b11", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2015", "authors": "D P Kingma; J L Ba"}, {"ref_id": "b12", "title": "Neural architectures for named entity recognition", "journal": "", "year": "2016", "authors": "Guillaume Lample; Miguel Ballesteros; Sandeep Subramanian; Kazuya Kawakami; Chris Dyer"}, {"ref_id": "b13", "title": "BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension", "journal": "", "year": "2020", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal ; Abdelrahman Mohamed; Omer Levy; Ves Stoyanov; Luke Zettlemoyer"}, {"ref_id": "b14", "title": "Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe Kiela. 2020b. Retrieval-augmented generation for knowledge-intensive NLP tasks", "journal": "", "year": "", "authors": "Patrick Lewis; Ethan Perez; Aleksandara Piktus; Fabio Petroni; Vladimir Karpukhin; Naman Goyal; Heinrich K\u00fcttler; Mike Lewis"}, {"ref_id": "b15", "title": "Prefixtuning: Optimizing continuous prompts for generation", "journal": "", "year": "2021", "authors": "Lisa Xiang; Percy Li;  Liang"}, {"ref_id": "b16", "title": "Zhilin Yang, and Jie Tang. 2021. GPT understands, too", "journal": "", "year": "", "authors": "Xiao Liu; Yanan Zheng; Zhengxiao Du; Ming Ding; Yujie Qian"}, {"ref_id": "b17", "title": "", "journal": "", "year": "2019", "authors": "Y Liu; M Ott; N Goyal; J Du; M Joshi; D Chen; O Levy; M Lewis; L S Zettlemoyer; V Stoyanov"}, {"ref_id": "b18", "title": "50,000 lessons on how to read: A relation extraction corpus", "journal": "", "year": "2013", "authors": "Dave Orr"}, {"ref_id": "b19", "title": "Deep contextualized word representations", "journal": "", "year": "2018", "authors": "M E Peters; M Neumann; M Iyyer; M Gardner; C Clark; K Lee; L S Zettlemoyer"}, {"ref_id": "b20", "title": "Language models as knowledge bases?", "journal": "EMNLP", "year": "2019", "authors": "F Petroni; T Rockt\u00e4schel; P Lewis; A Bakhtin; Y Wu; A H Miller; S Riedel"}, {"ref_id": "b21", "title": "Language models are unsupervised multitask learners", "journal": "", "year": "2019", "authors": "Alec Radford; Jeffrey Wu; Rewon Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"ref_id": "b22", "title": "Entity linking: Finding extracted entities in a knowledge base", "journal": "Springer", "year": "2013", "authors": "Delip Rao; Paul Mcnamee; Mark Dredze"}, {"ref_id": "b23", "title": "Learning dictionaries for information extraction by multi-level bootstrapping", "journal": "", "year": "1999", "authors": "E Riloff; R Jones"}, {"ref_id": "b24", "title": "A primer in BERTology: What we know about how BERT works", "journal": "", "year": "2020", "authors": "Anna Rogers; Olga Kovaleva; Anna Rumshisky"}, {"ref_id": "b25", "title": "Deterministic annealing for clustering, compression, classification, regression, and related optimization problems", "journal": "Proceedings of the IEEE", "year": "1998", "authors": "Kenneth Rose"}, {"ref_id": "b26", "title": "Exploiting cloze questions for few-shot text classification and natural language inference", "journal": "", "year": "2020", "authors": "Timo Schick; Hinrich Sch\u00fctze"}, {"ref_id": "b27", "title": "It's not just size that matters: Small language models are also few-shot learners", "journal": "", "year": "2020", "authors": "Timo Schick; Hinrich Sch\u00fctze"}, {"ref_id": "b28", "title": "AutoPrompt: Eliciting knowledge from language models with automatically generated prompts", "journal": "EMNLP", "year": "2020", "authors": "Taylor Shin; Yasaman Razeghi; Robert L Logan; I V ; Eric Wallace; Sameer Singh"}, {"ref_id": "b29", "title": "Open Mind Common Sense: Knowledge acquisition from the general public", "journal": "Springer", "year": "2002", "authors": "Push Singh; Thomas Lin; Erik T Mueller; Grace Lim; Travell Perkins; Wan Li Zhu"}, {"ref_id": "b30", "title": "Contextaware representations for knowledge base relation extraction", "journal": "", "year": "2017", "authors": "Daniil Sorokin; Iryna Gurevych"}, {"ref_id": "b31", "title": "ConceptNet 5.5: An open multilingual graph of general knowledge", "journal": "", "year": "2017", "authors": "R Speer; J Chin; C Havasi"}, {"ref_id": "b32", "title": "Overview of the English slot filling track at the TAC2014 knowledge base population evaluation", "journal": "", "year": "2014", "authors": "Mihai Surdeanu; Heng Ji"}, {"ref_id": "b33", "title": "2020. oLMpics -On what language model pre-training captures", "journal": "Transactions of the Association for Computational Linguistics", "year": "", "authors": "Alon Talmor; Yanal Elazar; Yoav Goldberg; Jonathan Berant"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "prompt \"Mozart Mozart Mozart Mozart Mozart Mozart Mozart Mozart Mozart Mozart Mozart Mozart Mozart Mozart Mozart Mozart Mozart was born in ,\" where we have filled the first blank with \"Mozart,\" and ask a cloze language model to fill in the second blank. The prompts used by Petroni et al. (2019) are manually created, while Jiang et al. (2020) use mining and paraphrasing based methods to automatically augment the prompt sets.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure1: Visualization of the LPAQA mining prompts for relation P1303 Instrument (i.e., x plays instrument y) from T-REx extended. We show the effect of tuning the layer-0 token embeddings (but not higher layers) on BERT-large-cased. The prompts are sorted in decreasing order by mixture weight. Each prompt's weight is shown at left; note that after the first 12 prompts, the remaining ones have negligible contribution. We show each soft prompt in blue, followed by the original (mined) prompt in red. To visualize the tuned vector v, we display the blue word w that maximizes p(w | v). The brightness of the blue word w and the original red word w 0 are respectively proportional to p(w | v) and p(w 0 | v). The red word has size 1, and the blue word has size ||v||/||v 0 ||, where v 0 is the original untuned vector (the embedding of w 0 ). In this example, the blue probabilities p(w | v) range from 6.5e-5 to 9.7e-5 (mean 8.6e-5 \u00b1 8.1e-6), the red probabilities p(w 0 | v) range from 7.7e-5 to 1.1e-4 (mean 9.5e-5 \u00b1 7.8e-6), and the relative magnitudes ||v||/||v 0 || vary from 1.00 to 1.49 (mean 1.12 \u00b1 0.13).", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Soft (sin., BEb)  47.7 (+16.6 ? ) 49.6 (+23.2 ? ) Soft (min., BEb) 50.7 ? (+16.6 ? ) 50.5 ? (+19.3 ? ) Soft (par., BEb) 48.4 (+12.8 ? ) 49.7 (+18.5", "figure_data": "ModelT-REx orig. T-REx ext.LAMA (BEb)31.126.4LPAQA(BEb)34.131.2AutoPrompt43.345.6? )Soft (ran., BEb) 48.1 (+47.4) 50.6 (+49.8)LAMA (BEl)28.9  \u202024.0  \u2020LPAQA(BEl)"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Soft (sin.) 11.2 (+1.5)  ", "figure_data": "ModelP@1P@10MRRLAMA9.7  \u202027.0  \u202015.6  \u2020LPAQA10.6  \u202023.7  \u202015.3  \u2020"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Results on ConceptNet (winner: random init).", "figure_data": "ModelP@1 P@10 MRRbaseline39.467.449.1adjust mixture weights 40.069.153.3adjust token vectors50.780.761.1adjust both51.081.461.6"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Ablation experiments, conducted with the BERT-large model on the T-REx original dataset.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "promptsT-REx-min. T-REx-par. Goog-sin. Goog-min. Goog-par.", "figure_data": "ConceptNet#relations414133316avg. prompts28.426.2132.728.09.3min #prompts613129241max #prompts29301403038avg. #tokens5.14.54.75.34.27.1"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Statistics of prompts. The \"Goog\" stands for \"Google-RE.\" We do not list the statistics of randomized prompts, as they should match the statistics of the mined prompts (\"min.\") from which they are derived.", "figure_data": "databaseT-REx original T-REx extended Google-RE ConceptNet#relations4141316avg. #unique x15808341837511avg. #unique y217151372507min #(x, y)544310766510max #(x, y)1982100029374000mean #(x, y)171588518431861"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Statistics of the relational databases.", "figure_data": "ModelP@1P@10MRRLPAQA (BEb) 18.940.426.6Soft (BEb)23.0 (+4.1) 45.2 (+4.8) 30.5 (+3.9)LPAQA (BEl) 23.847.732.2Soft (BEl)27.0 (+3.2) 51.7 (+4.0) 35.4 (+3.2)"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Results with distinct y's. We use the BERTbase-cased and BERT-large-cased LMs and the LPAQA mining based prompts as initial prompts. The experiments are conducted on the T-REx original dataset.", "figure_data": ""}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Statistics of effective number of prompts.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "Mary", "formula_coordinates": [2.0, 327.96, 477.2, 23.63, 9.46]}, {"formula_id": "formula_1", "formula_text": "x v 1 v 2 v 3 v 4 v 5 y v 6", "formula_coordinates": [2.0, 348.51, 642.56, 130.52, 10.63]}, {"formula_id": "formula_2", "formula_text": "( ) i : 0 \u2264 \u2264 L. Here v (0) i = v i and the embedding v ( ) i at layer > 0 is computed from all tokens' embeddings v ( \u22121) j", "formula_coordinates": [3.0, 70.87, 145.79, 218.27, 47.96]}, {"formula_id": "formula_3", "formula_text": "p(y | x, r) = t\u2208Tr p(t | r) \u2022 p LM (y | t, x) (1)", "formula_coordinates": [3.0, 84.09, 489.23, 205.05, 22.29]}, {"formula_id": "formula_4", "formula_text": "(x,y)\u2208Er \u2212 log t\u2208Tr p(y | t, x) (2)", "formula_coordinates": [3.0, 352.61, 266.28, 171.8, 22.83]}, {"formula_id": "formula_5", "formula_text": "H = t\u2208Tr p(t | r) \u2022 log 2 p(t | r)(3)", "formula_coordinates": [8.0, 110.28, 390.59, 178.86, 22.26]}], "doi": "10.18653/v1/D19-3022"}