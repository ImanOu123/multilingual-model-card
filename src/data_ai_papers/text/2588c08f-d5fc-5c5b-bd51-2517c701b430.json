{"title": "Understanding Self-Supervised Learning Dynamics without Contrastive Pairs", "authors": "Yuandong Tian; Xinlei Chen; Surya Ganguli", "pub_date": "", "abstract": "While contrastive approaches of self-supervised learning (SSL) learn representations by minimizing the distance between two augmented views of the same data point (positive pairs) and maximizing views from different data points (negative pairs), recent non-contrastive SSL (e.g., BYOL and SimSiam) show remarkable performance without negative pairs, with an extra learnable predictor and a stop-gradient operation. A fundamental question arises: why do these methods not collapse into trivial representations? We answer this question via a simple theoretical study and propose a novel approach, DirectPred, that directly sets the linear predictor based on the statistics of its inputs, without gradient training. On ImageNet, it performs comparably with more complex two-layer non-linear predictors that employ BatchNorm and outperforms a linear predictor by 2.5% in 300-epoch training (and 5% in 60-epoch). DirectPred is motivated by our theoretical study of the nonlinear learning dynamics of non-contrastive SSL in simple linear networks. Our study yields conceptual insights into how non-contrastive SSL methods learn, how they avoid representational collapse, and how multiple factors, like predictor networks, stop-gradients, exponential moving averages, and weight decay all come into play. Our simple theory recapitulates the results of real-world ablation studies in both STL-10 and ImageNet. Code is released 1 .", "sections": [{"heading": "Introduction", "text": "Self-supervised learning (SSL) has emerged as a powerful method for learning useful representations without re-1 Facebook AI Research 2 Stanford University. Correspondence to: Yuandong Tian <yuandong@fb.com>.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proceedings of the 38 th International Conference on Machine", "text": "Learning, PMLR 139, 2021. Copyright 2021 by the author(s).\n1 https://github.com/facebookresearch/ luckmatters/tree/master/ssl quiring expensive target labels (Devlin et al., 2018). Many state-of-the-art SSL methods in computer vision employ the principle of contrastive learning (Oord et al., 2018;Tian et al., 2019;He et al., 2020;Chen et al., 2020a;Bachman et al., 2019) whereby the hidden representations of two augmented views of the same object (positive pairs) are brought closer together, while those of different objects (negative pairs) are encouraged to be further apart. Minimizing differences between positive pairs encourages modeling invariances, while contrasting negative pairs is thought to be required to prevent representational collapse (i.e., mapping all data to the same representation).\nHowever, some recent SSL work, notably BYOL (Grill et al., 2020) and SimSiam (Chen & He, 2020), have shown the remarkable capacity to learn powerful representations using only positive pairs, without ever contrasting negative pairs. These methods employ a dual pair of Siamese networks (Bromley et al., 1994) (Fig. 1): the representation of two views are trained to match, one obtained by the composition of an online and predictor network, and the other by a target network. The target network is not trained via gradient descent; and either employs a direct copy of the online network (e.g., SimSiam (Chen & He, 2020)), or a momentum encoder that slowly follows the online network in a delayed fashion through an exponential moving average (EMA) (e.g., MoCo (He et al., 2020;Chen et al., 2020b) and BYOL (Grill et al., 2020)). Compared to contrastive learning, these non-contrastive SSL methods do not require large batch size (e.g., 4096 in SimCLR (Chen et al., 2020a)) or memory queue (e.g., MoCo (He et al., 2020;Chen et al., 2020b)) to provide negative pairs. Therefore, they are generally more efficient and conceptually simple while maintaining state-of-the-art performance.\nSince the entire procedure in non-contrastive SSL encourages the online+predictor network and the target network to become similar to each other, this overall scheme raises several fundamental unsolved theoretical questions. Why/how does it avoid collapsed representations? What is the nature of the learned representations? How do multiple design choices and hyperparameters interact nonlinearly in the learning dynamics? While there are interesting theoretical studies of contrastive SSL (Arora et al., 2019;Lee et al., 2020;Tosh et al., 2020), any theoretical understanding of the nonlinear learning dynamics of non-contrastive arXiv:2102.06810v4 [cs.LG] 8 Oct 2021 SSL remains open.\nIn this paper, we make a first attempt to analyze the behavior of non-contrastive SSL training and the empirical effects of multiple hyperparameters, including (1) Exponential Moving Average (EMA) or momentum encoder,\n(2) Higher relative learning rate (\u03b1 p ) of the predictor, and\n(3) Weight decay \u03b7. We explain all these empirical findings with an exceedingly simple theory based on analyzing the nonlinear learning dynamics of simple linear networks. Note that deep linear networks have provided a useful tractable theoretical model of nonconvex loss landscapes (Kawaguchi, 2016;Du & Hu, 2019;Laurent & Brecht, 2018) and nonlinear learning dynamics (Saxe et al., 2013;Lampinen & Ganguli, 2018;Arora et al., 2018) in these landscapes, yielding insights like dynamical isometry (Saxe et al., 2013;Pennington et al., 2017;) that lead to improved training of nonlinear deep networks. Despite the simplicity of our theory, it can still predict how various hyperparameter choices affect performance in an extensive set of real-world ablation studies. Moreover, the simplicity also enables us to provide conceptual and analytic insights into why performance patterns vary the way they do. Specifically, our theory accounts for the following diverse empirical findings:\nEssential part of non-contrastive SSL. The existence of the predictor and stop-gradient is absolutely essential. Removing either of them leads to representational collapse in BYOL and SimSiam.\nEMA. While the original BYOL needs EMA to work, they later confirmed that EMA is not necessary (i.e., the online and target networks can be identical) if a higher \u03b1 p is used. This is also confirmed with SimSiam, as long as the predictor is updated more often or has larger learning rate (or larger \u03b1 p ). However, the performance is slightly lower.\nPredictor Optimality and Relative learning rate \u03b1 p . Both BYOL and SimSiam suggest that the predictor should always be optimal, in the sense of always achieving min-  1. Simply plugging in the \"optimal solution\" to the linear predictor shows poor performance after 100 BYOL epochs (Top-1 accuracy in STL-10 (Coates et al., 2011) downstream classification task). The optimal solution is obtained by solving (with regularization)\nWpE [f f ] = 1 2 (E [faf ] + E [f f a ]\n), in which the two expectations is estimated with exponential moving average. In comparison, with gradient descent, BYOL with a single linear layer predictor can reach 74%-75% Top-1 in STL-10 after 100 epochs. Unless explicitly stated, in all our experiments, we use ResNet-18 (He et al., 2016) as the backbone network for CIFAR-10/STL-10 experiments and SGD as the optimizer with learning rate \u03b1 = 0.03, momentum 0.9, weight decay\u03b7 = 0.0004 and EMA parameter \u03b3a = 0.996. Each setting is repeated 5 times. imal 2 error in predicting the target network's outputs from the online network's outputs. This optimality conjecture was motivated by observed superior performance when the predictor had large learning rates and/or was allowed more frequent updates than the rest of the network. However (Chen & He, 2020) also showed that if the predictor is updated too often, then performance drops, which questions the importance of an always optimal predictor as a key requirement for learning good representations.\nWeight Decay. Table 15 in BYOL (Grill et al., 2020) indicates that no weight decay may lead to unstable results. A recent blogpost (Fetterman & Albrecht, 2020) also mentions using weight decay leads to stable learning in BYOL.\nFinally, motivated by our theoretical analysis, we propose a new method DirectPred that directly sets the predictor weights based on principal components analysis of the predictor's input, thereby avoiding complicated predictor dynamics and initialization issues. We show that this simple DirectPred method nevertheless yields comparable performance in CIFAR-10 and outperforms gradient training of the linear predictor by +5% Top-1 accuracy in linear evaluation protocol on both STL-10 and ImageNet (60 epochs). On the standard ImageNet benchmark (300 epochs), DirectPred achieves 72.4%/91.0% Top-1/Top-5, 2.5% higher than BYOL with linear predictor (69.9%/89.6%) and comparable with default BYOL setting with 2-layer predictor (72.5%/90.8%).", "publication_ref": ["b11", "b24", "b18", "b6", "b2", "b16", "b7", "b4", "b7", "b18", "b8", "b16", "b6", "b18", "b8", "b1", "b23", "b19", "b12", "b22", "b21", "b0", "b25", "b17", "b7", "b16"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Two-layer linear model", "text": "To obtain analytic and conceptual insights into noncontrastive SSL we analyze a simple, bias-free linear BYOL model where the online, target and predictor networks are specified by the weight matrices W \u2208 R n2\u00d7n1 , W p \u2208 R n2\u00d7n2 and W a \u2208 R n2\u00d7n1 respectively (Fig. 1).\nLet x \u2208 R n1 be a data point drawn from the data distribution p(x) and let x 1 and x 2 be two augmented views of x: x 1 , x 2 \u223c p aug (\u2022|x) where p aug (\u2022|x) is the augmentation distribution. In practice such data augmentations correspond to random crops, blurs or color distortions of images (Chen et al., 2020a). Let f 1 = W x 1 \u2208 R n2 be the online representation of view 1, and f 2a = W a x 2 \u2208 R n2 be the target representation of view 2. In BYOL, the learning dynamics of W and W p are obtained by minimizing\nJ(W, W p ) := 1 2 E x1,x2 W p f 1 \u2212 StopGrad(f 2a ) 2 2 ,\n(1) while the dynamics of W a is obtained differently, via an exponential moving average (EMA) of W . We will analyze this combined dynamics for W , W p and W a , in the presence of additional weight decay, in the limit of large batch sizes and small discrete time learning rates. This limit can be well approximated by the gradient flow (see Supplementary Material (SM) for all derivations): Lemma 1. BYOL learning dynamics following Eqn. 1:\nW p = \u03b1 p (\u2212W p W (X + X ) + W a X) W \u2212 \u03b7W p (2) W = W p (\u2212W p W (X + X ) + W a X) \u2212 \u03b7W (3) W a = \u03b2(\u2212W a + W ) (4) Here, X := E [xx ] wherex(x) := E x \u223cpaug(\u2022|x) [x ]\nis the average augmented view of a data point x and X :=\nE x V x |x [x ] is the covariance matrix V x |x [x ]\nof augmented views x conditioned on x, subsequently averaged over the data x. Note that \u03b1 p and \u03b2 reflect multiplicative learning rate ratios between the predictor and target networks relative to the online network. Finally, the terms involving \u03b7 reflect weight decay.\nAs a gradient flow formulation, the learning rate \u03b1 does not appear in Lemma 1. In the actual finite time update, the learning rate for W p is \u03b1\u03b1 p , the EMA rate is \u03b1\u03b2 = 1 \u2212 \u03b3 a , where \u03b3 a is the usual EMA parameter (e.g,. BYOL uses 0.996), and the weight decay for actual training is\u03b7 := \u03b1\u03b7.\nWe note that since SimSiam is an ablation of BYOL that removes the EMA computation, the underlying dynamics of SimSiam can also be obtained from Lemma 1 simply by setting W a = W , inserting this relation into Eqn. 2 and Eqn. 3, and ignoring Eqn. 4. Importantly, the stop-gradient on the target branch is still there.\nOverall Eqns. 2-4 constitute our starting point for analyzing the combined roles of relative learning rates \u03b1 p and \u03b2, weight decay rate \u03b7 and various ablations in determining the performance of both BYOL and SimSiam.\nWe first derive two very general results (see SM). Theorem 1 (Weight decay promotes balancing of the predictor and online networks.  2. Top-1 accuracy of BYOL on STL-10 under linear evaluation protocol, trained for 100 epochs with no weight decay (\u03b7 = 0) and \u03b1p = 1. It is worse than the baseline (74.51\u00b10.47 without predictor bias) when the weight decay is set to be \u03b7 = 0.0004. \"No-bias\" means the linear predictor does not have a bias term.\nthe particular dynamics of W a in Eqn. 4, the update rules (Eqn. 2 and Eqn. 3) possess the invariance\nW (t)W (t) = \u03b1 \u22121 p W p (t)W p (t) + e \u22122\u03b7t C, (5\n)\nwhere C is a symmetric matrix that depends only on the initialization of W and W p .\nThis theorem implies that for both BYOL and SimSiam, there exists a \"balancing\" that ensures that any matching between the online and target representations will not be attributable solely to the predictor weights, rendering the online weights useless. Instead what the predictor learns, the online network will also learn, which is important as the online network's representations are what is used for downstream tasks. We note that similar weight balancing dynamics has been discovered in multi-layer linear networks and matrix factorization (Arora et al., 2018;Du et al., 2018). Our results generalize this to SSL dynamics. Second, a nonzero weight decay could help remove the extra constant C due to initialization, further balancing the predictor and online network weights and possibly leading to better performance on downstream tasks (Tbl. 2).\nTheorem 2 (The stop-gradient signal is essential for success.). With W a = W (SimSiam case), removing the stop-gradient signal yields a gradient update for W given by positive semi-definite (PSD) matrix H(t) := X \u2297 (W p W p + I n2 ) + X \u2297W pWp + \u03b7I n1n2 (hereW p := W p \u2212 I n2 and \u2297 is the Kronecker product):\nd dt vec(W ) = \u2212H(t)vec(W ). (6\n)\nIf the minimal eigenvalue \u03bb min (H(t)) over time is bounded below, inf t\u22650 \u03bb min (H(t)) \u2265 \u03bb 0 > 0, then W (t) \u2192 0.\nThus we have proven analytically in this simple setting that removing the stop-gradient leads to representational collapse, as observed in more complex settings in Sim-Siam (Chen & He, 2020). Similarly, with W a = W and no predictor (W p = I n2 ), then the dynamics Eqn. 3 also reduces to a similar form and W (t) \u2192 0 (see SM).", "publication_ref": ["b6", "b0", "b13", "b7"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "How multiple factors affect learning dynamics", "text": "The learning dynamics in Eqns. 2-4 constitute a set of high dimensional coupled nonlinear differential equations that (1) Evolvement of eigenvalues for F . Since F is PSD and its eigenvalue sj varies across scales, we plot log(si). We could see some eigenvalues are growing while others are shrinking to zero over training. (2) Similar \"step-function\" behaviors for the predictor Wp. Its negative eigenvalues shrinks towards zero and leading eigenvalues becomes larger.\n(3) The eigenspace of F and Wp gradually align with each other (Theorem 3). For each eigenvector uj of F , we compute cosine angle (normalized correlation) between uj and Wpuj to measure alignment. (4) Wp gradually becomes symmetric and PSD during training. can be difficult to solve analytically in general. Therefore, to obtain analytic insights into the functional roles of the relative learning rates \u03b1 p and \u03b2 and weight decay \u03b7, we make a series of simplifying assumptions. Intriguingly, under these simplifying assumptions we obtain a rich set of analytic predictions, which we then test experimentally in more realistic scenarios. We find, nicely, that these predictions still qualitatively hold even when our simplifying assumptions required for obtaining analytic results do not.\nAssumption 1 (Proportional EMA). We first reduce the dimensionality of the dynamics in Eqns. 2-4 by enforcing that the target network W a undergoes EMA but is forced to always be proportional to the online network via the relation W a (t) = \u03c4 (t)W (t). Inserting this relation into the EMA dynamics in Eqn. 4 yields\u03c4 W + \u03c4\u1e86 = \u03b2(1 \u2212 \u03c4 )W .\nThus we obtain a reduced dynamics for W , W p and \u03c4 . By not enforcing the stronger SimSiam constraint that W a = W , we can still model EMA dynamics. Intuitively, \u03c4 = \u03c4 (t) is a dynamic parameter that depends on how quickly W = W (t) grows over time. If W is constant, then\u1e86 = 0 and \u03c4 stabilizes to 1. On the other hand, if W grows rapidly, then \u03c4 becomes small. While Assumption 1 is a simplification, as we shall see, it still reveals interesting verifiable predictions about the functional role of EMA.\nAssumption 2 (Isotropic data and augmentation). We assume the data distribution p(x) has zero mean and identity covariance, while the augmentation distribution p aug (\u2022|x) has mean x and covariance \u03c3 2 I. This simplifies the dynamics in Eqns. 2-4 by reducing the augmentation averaged data covariance to X = I and the data averaged augmentation covariance to X = \u03c3 2 I.\nMany previous studies of deep learning dynamics made simplifying isotropic assumptions about data (Tian, 2017;Brutzkus & Globerson, 2017;Bartlett et al., 2018;Safran & Shamir, 2018). Since our fundamental goal is to obtain the first analytic understanding of the dynamics of non-contrastive SSL methods, it is useful to first achieve this in the simplest possible isotropic setting. Interestingly, we will find that our final conclusions generalize to nonisotropic real world settings.\nAssumption 3 (Symmetric predictor). We enforce symmetry in W p by initializing it to be a symmetric matrix, and then symmetrizing the flow for W p in Eqn. 2 (see SM).\nThis symmetry assumption was motivated by both fixed point analysis and empirical findings. First, the fixed point of Eqn. 2 under Assumption 1 and 2 and \u03b7 > 0 is always a symmetric matrix and in numerical simulation the asymmetric part W p \u2212 W p eventually vanishes (See Appendix for the proof and numerical simulations). Moreover, during BYOL training without a symmetry constraint on the predictor, W p gradually moves towards symmetry (Fig. 2).\nSecond, a set of experiments reveal that whether the predictor is symmetric or not has a dramatic effect in terms of both performance and interaction with EMA. In our STL-10 experiment, enforcing symmetric W p in the presence of EMA improves performance on downstream tasks (Tbl. 3). In contrast, in the absence of EMA, a symmetric W p fails while an asymmetric W p works reasonably well. Similar behavior holds on ImageNet: a symmetric one layer linear predictor W p in SimSiam (i.e. without EMA) achieves performance no better than random guessing (Top-1/5: 0.1%/0.5%), while an asymmetric W p achieves a Top-1/5 accuracy of 68.1%/88.2%. Our theory will explain this as well as show how to obtain good performance with a symmetric predictor without EMA by increasing its relative learning rate \u03b1 p .", "publication_ref": ["b5", "b3", "b27"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Dynamical alignment of eigenspaces between the predictor and its input correlation matrix", "text": "Under the three assumptions stated above, we analyze the coupled dynamics of F := W XW and W p . Note that F is the correlation matrix of the outputs of the online network which also serve as inputs to the predictor. By Assumption 2, E [x] = 0 and F is also the covariance matrix. We find F and W p obey the following dynamics (see SM):\nW p = \u2212 \u03b1 p 2 (1 + \u03c3 2 ){W p , F } + \u03b1 p \u03c4 F \u2212 \u03b7W p (7) F = \u2212(1 + \u03c3 2 ){W 2 p , F } + \u03c4 {W p , F } \u2212 2\u03b7F\nThis dynamics reveals that the eigenspace of W p will gradually align with that of F under certain conditions (see SM for derivation):\nTheorem 3 (Eigenspace alignment). Under Eqn. 7, the commutator [F, W p ] := F W p \u2212 W p F satisfies:\nd dt [F, W p ] = \u2212[F, W p ]K \u2212 K[F, W p ](8)\nwhere\nK(t) = (1+\u03c3 2 ) \u03b1 p 2 F (t) + W 2 p (t) \u2212 \u03c4 1 + \u03c3 2 W p (t) + 3 2 \u03b7I (9) If inf t\u22650 \u03bb min [K(t)] = \u03bb 0 > 0, then the commutator [F (t), W p (t)] F \u2264 e \u22122\u03bb0t [F (0), W p (0)] F \u2192 0 (10)\nFor symmetric W p , when W p and F commute they can be simultaneously diagonalized. Thus this shows that the eigenspace of W p gradually aligns with that of F .\nTo test this prediction, we performed extensive experiments showing that training BYOL using ResNet-18 on STL-10 yields eigenspace alignment, as demonstrated in Fig. 2. Now if the eigenspaces of W p and F do align, we can obtain fully decoupled dynamics. Let the columns of the matrix U be the common eigenvectors, so that W\np = U \u039b Wp U where \u039b Wp = diag[p 1 , p 2 , . . . , p d ], F = U \u039b F U where \u039b F = diag[s 1 , s 2 , . . . , s d ].\nFor each mode j, we have (see SM for derivation):\np j = \u03b1 p s j \u03c4 \u2212 (1 + \u03c3 2 )p j \u2212 \u03b7p j (11\n)\ns j = 2p j s j \u03c4 \u2212 (1 + \u03c3 2 )p j \u2212 2\u03b7s j (12) s j\u03c4 = \u03b2(1 \u2212 \u03c4 )s j \u2212 \u03c4\u1e61 j /2. (13\n)\nThis decoupled dynamics constitutes a dramatically simplified set of 3 dimensional nonlinear dynamical systems for BYOL learning, and two dimensional nonlinear systems (obtained by constraining \u03c4 = 1) for SimSiam. As expected, each mode's dynamics is equivalent to the 3 dimensional dynamics obtained by setting n 1 = n 2 = 1 in Eqns. 2-4 and making the replacements W 2 = s j , W p = p j , and W a /W = \u03c4 (see SM). Thus the decoupled dynamics in Eqns 11-13 reduce to the scalar case of BYOL dynamics in Eqns. 2-4 after a change of variables and the condition in Thm. 3 reveals when this decoupled regime is reachable.\nNon-symmetric W p . When Assumption 3 is absent, the analysis is much more convoluted. One possible way is to decompose W p = A + B where A = A is symmetric and B = \u2212B is skew-symmetric. We leave it for future work.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Analysis of decoupled dynamics", "text": "The simplified three (two) dimensional dynamics of BYOL (SimSiam) yields significant insights. First, there is clearly a collapsed fixed point at p j (t) = s j (t) = 0 and \u03c4 taking any value. We wish to understand conditions under which p j and s j can avoid this collapsed fixed point and grow from small random initial conditions. Since s j is an eigenvalue of W W , we are particularly interested in conditions under which s j achieves large final values, corresponding to a non-collapsed online network, that are moreover sensitive to the statistics of the data, governed by \u03c3 2 .\nExact integral. First, an important observation, similar to Theorem 1, is that the dynamics possesses an exact integral of motion, obtained by multiplying Eqn. 11 by 2\u03b1 \u22121 p p j , subtracting, Eqn. 12 and integrating over time yielding\ns j (t) = \u03b1 \u22121 p p 2 j (t) + e \u22122\u03b7t c j (14\n)\nwhere c j = \u03b1 \u22121 p p 2 j (0) \u2212 s j (0) is fixed by initial conditions. In absence of weight decay (\u03b7 = 0), this integral reveals that the initial condition encoded in c j is never forgotten and the dynamics of p j and s j are confined to parabolas of the form s j (t) = p 2 j (t) + c j , as can be seen by the blue flow lines in Fig. 3(left). With weight decay (\u03b7 > 0) over time the initial condition is forgotten and the dynamics approaches the invariant parabola s j = \u03b1 \u22121 p p 2 j as can been seen by the approach of the blue flow lines to the black dashed parabola in Fig. 3 right and middle. We discuss these two cases in turn. First we note that in both cases, since the EMA computation is often very slow (Grill et al., 2020), corresponding to small \u03b2, the dynamics of \u03c4 in Eqn. 13 is slow relative to that of p j and s j . Therefore to understand the combined dynamics, we can search for the fixed points that p j and s j will rapidly approach at fixed \u03c4 . Over time \u03c4 will then either slowly approach 1 (BYOL) or be always equal to 1 (SimSiam), and s j and p j will follow their \u03c4 -dependent fixed points.\nNo weight decay. When \u03b7 = 0, Eqns. 11 and 12 at a fixed value of \u03c4 yield a branch of collapsed fixed points given by s j = 0 and p j taking any value, and a branch of non-collapsed fixed points, with p j = \u03c4 /(1 + \u03c3 2 ) and s j taking any value (horizontal and vertical red/green lines in Fig. 3,left). A sufficient criterion on initial conditions to avoid the collapsed branch is s j (0) > p 2 j (0)/\u03b1 p corresponding to lying above the dashed black parabola in Fig. 3,left. This restricted initial condition reveals why a fast predictor (large \u03b1 p ) is advantageous (Obs#1): larger \u03b1 p leads to a smaller basin of attraction of the collapsed branch by flattening the dashed parabola. Indeed both BYOL and SimSiam have noted that a fast predictor can help avoid collapse. On the other hand, \u03b1 p cannot be infinitely large (Obs#2): since s j (+\u221e) = s j (0)+\u03b1 \u22121 p (p 2 j (+\u221e)\u2212p 2 j (0)), very large \u03b1 p implies that s j , the final value of the online network characterizing the learned representation, does not grow even if p j does. This is consistent with results which show that optimizing the predictor too often doesn't work in SimSiam (Chen & He, 2020), and directly setting an \"optimal\" predictor fails as well (Tbl. 1). The online network needs to grow along with the predictor and that cannot happen if the predictor is too fast.\nAdvantage of weight decay. In the non-collapsed branch of fixed points without weight decay (vertical red line in Fig. 3,left), the predictor p j takes the exact value \u03c4 /(1 +  4. Summarization of positive/negative effects of various hyperparameter choices (EMA \u03b2, relative predictor learning rate \u03b1p and weight decay \u03b7). \"#1\" means (Obs#1) in the text. Eqn. 16). Stable fixed points are in red, unstable in green and saddle in black. When the weight decay \u03b7 = 0, the trivial solution pj = 0 is a saddle. When \u03b7 > 0, the trivial solution becomes stable near to the origin and initial pj needs to be large enough to converge to the stable non-collapsed solution p * j+ .\n!\" * !$ * (b) (c) < % 4(1 + % ) > % 4(1 + % ) !$ * (a) = 0 O O O Saddle Point Figure 4. Fixed point of\u1e57j = \u2212pj(pj \u2212 p * j\u2212 )(pj \u2212 p * j+ ) (see\n\u03c3 2 ), which models the invariance to augmentation correctly: a large data augmentation variance \u03c3 2 should lead to a small magnitude of the learned representation. Ideally, we want s j to have the same property. With weight decay \u03b7 > 0 in Eqn. 14, memory of the initial condition c j fades away, yielding convergence to some point on the invariant parabola s j = \u03b1 \u22121 p p 2 j . (Obs#3): Therefore, by tying the online network to the predictor, weight decay allows s j to also model invariance to augmentations correctly if the predictor does, regardless of the random initial condition c j .\nDynamics on the invariant parabola. Because weight decay forces convergence to the invariant parabola s j = \u03b1 \u22121 p p 2 j , we next focus on dynamics along this parabola (i.e. c j = 0 in Eqn. 14). In this case, Eqn. 13 has a solution:\n\u03c4 (t) = p \u22121 j (t)\u03b2e \u2212\u03b2t t 0 p j (t )e \u03b2t dt,(15)\nwith initial condition \u03c4 (0) = 0. Inserting the invariant s j = \u03b1 \u22121 p p 2 j into Eqn. 11, the dynamics of p j is given by:\np j = p 2 j \u03c4 (t) \u2212 (1 + \u03c3 2 )p j \u2212 \u03b7p j . (16\n)\nWe first analyze the fixed points where\u1e57 j = 0 at fixed \u03c4 . When the weight decay 0 < \u03b7 \u2264 \u03c4 2 4(1+\u03c3 2 ) , p j has has three fixed points (Fig. 4(b)):\np * j\u00b1 = \u03c4 \u00b1 \u03c4 2 \u2212 4\u03b7(1 + \u03c3 2 ) 2(1 + \u03c3 2 ) > 0, p * j0 = 0\nwhere both p * j0 and p * j+ are stable and p * j\u2212 is unstable, as shown in Fig. 4(b). The basin of attraction of the collapsed fixed point p * j0 = 0 is p j < p * j\u2212 while the basin of attraction of the useful non-collapsed fixed point p * j+ is p j > p * j\u2212 , yielding an important constraint on initial conditions to avoid collapse. Note that p * j\u2212 is a decreasing function of \u03c4 and increasing function of \u03b7 (see SM). This means that with larger \u03b7, p * j\u2212 moves right and the basin of collapse expands (Obs#4). When \u03b7 > \u03c4 2 4(1+\u03c3 2 ) there is only one stable fixed point p * j0 = 0 (Fig. 4(c)). Under such strong weight decay collapse is unavoidable (Obs#5).\nWe now discuss the dynamics. First we define the quantity \u2206 j := p j [\u03c4 \u2212(1+\u03c3 2 )p j ]\u2212\u03b7, which must satisfy two criteria. Note that Eqn. 16 can be written as\u1e57 j = p j \u2206 j , so \u2206 j must at some point be positive to drive p j (t) to any positive non-collapsed fixed point p * j+ . Second, for eigenspace alignment in Theorem 3 to remain stable (even if the alignment has already happened), K(t) must be positive definite (PD) in Eqn. 9. Using the eigen-space alignment conditions and the invariance s j = \u03b1 \u22121 p p 2 j , the positive definite condition on K(t) can be written as\n\u2206 j < 1 2 \u03b1 p (1 + \u03c3 2 )s j + \u03b7 . (17\n)\nThis criterion and the criterion \u2206 j > 0 yield interesting insights into the roles of various hyperparameters choices.\nFirst (Obs#6), larger predictor learning rate \u03b1 p can play an advantageous role by loosening the upper bound in Eqn. 17, making it easier to satisfy. Second (Obs#7), increasing \u03b7 also has the same effect.\nRole of EMA. Without EMA, \u03c4 \u2261 1 and (Eqn. 17) may not hold initially when p j is small. The reason is \u2206 j is to leading order linear in p j when \u03c4 = 1 while the right hand side is to leading order s j \u223c p 2 j , so the left hand side has a larger contribution from p j than the right.\nEMA resolves this as follows. When the training begins, s j is often quite small, and \u03c4 remains small since W changes rapidly. When p j grows to the fixed point p * j+ \u223c \u03c4 /(1 + \u03c3 2 ), the growth of s j stops, making \u03c4 larger. This in turns sets a higher fixed point goal for p j . This process continues until the feature is stabilized and \u03c4 = 1 (Fig. 5 for details).\nTherefore, EMA can serve as an automatic curriculum (Obs#8): it sets an initial small goal of \u03c4 1+\u03c3 2 for p j so \u2206 j need only be small and positive to both drive p j larger and satisfy Eqn. 17. Then EMA gradually sets a higher goal for p j by increasing \u03c4 , so that p j and s j can grow, while keeping the eigenspaces of W p and F aligned.\nAs a trade-off, a very slow EMA schedule (\u03b2 small) yields a slow training procedure (Obs#9) (See Fig. 5). Also small \u03c4 leads to larger p * j\u2212 and more eigen modes can be trapped in the collapsed basin (Obs#10).", "publication_ref": ["b16", "b7"], "figure_ref": ["fig_2", "fig_2", "fig_2", "fig_2", "fig_2", "fig_3", "fig_3"], "table_ref": []}, {"heading": "Summarizing the effects of hyperparameters", "text": "We summarize the positive and negative effects of multiple hyperparameters in Tbl. 4. We next provide additional ablations and experiments to further justify our reasoning.\nDifferent weight decay \u03b7 p and \u03b7 s . If we set a higher weight decay for the predictor (\u03b7 p ) than the online net (\u03b7 s ),  5. Symmetric weight works without EMA, if we set weight decay for the predictor (\u03b7p = 0.0004) but not the trunk (\u03b7s = 0) in BYOL experiment on STL-10. Report Top-1 accuracy after 100 epochs. If there is no weight decay for all layers, then again symmetric weight doesn't work without EMA. then p j grows slower than s j and it is possible that the condition of Theorem 3 can still be satisfied without using EMA. Indeed Tbl. 5 shows this is the case.\nLarger learning rate of the predictor \u03b1 p > 1. Our analysis predicts that one way to make symmetric W p work with no EMA is to use \u03b1 p > 1 (i.e. Theorem 3 is more easily satisfied). Fig. 6 verifies this prediction. Moreover Table 22 in Appendix of BYOL (Grill et al., 2020) also shows that \u03b1 p > 1 is required to get BYOL working without EMA.\nAs a reference, Table 22 in Appendix I.2 of BYOL (Grill et al., 2020) also shows a similar trend: the learning rate of the (2-layer) predictor needs to be higher than that of the projector for strong performance in ImageNet, when EMA is absent.", "publication_ref": ["b16", "b16"], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Optimization-free Predictor W p", "text": "A direct consequence of our theory is a new method for choosing the predictor that avoids gradient descent altogether. Instead, we estimate the correlation matrix F of predictor inputs and directly set W p to be a function of this, thereby avoiding both the need to align the eigenspaces of F and W p through optimization, and the need to initialize W p outside the basin of collapse. As we shall see, this exceedingly simple, theory motivated method also yields better performance in practice compared to gradient-based optimization of a linear predictor. We call our method DirectPred which simply estimates F , computes its eigen-decompositionF =\u00db\u039b F\u00db , wher\u00ea\n\u039b F = diag[s 1 , s 2 , . . . , s d ], and sets W p via p j = \u221a s j + max j s j , W p =\u00db diag[p j ]\u00db . (18\n)\nThis choice is theoretically motivated by eigenspacealignment between W p and F (Theorem. 3) and convergence to the invariant parabola s j \u221d p 2 j in Eqn. 14 with weight decay (\u03b7 > 0). Here the estimate correlation matrix F can be obtained by a moving average:\nF = \u03c1F + (1 \u2212 \u03c1)E B [f f ] (19\n)\nwhere  . The learning rate \u03b1 = 0.01. Both terms boost the eigenvalue of K(t) to above 0 so that eigen space alignment could happen (Theorem 3), but also come with different trade-offs. Here \u03b2 = 0.4 so that \u03b1\u03b2 = 0.004 = 1 \u2212 \u03b3a where \u03b3a = 0.996 as in BYOL. Top row (Weight Decay \u03b7): A large \u03b7 boost the eigenvalue of K(t) up, but substantially decreases the final converging eigenvalues pj and sj (i.e., the final features are not salient), or even drags them to zero (no training happens). Bottom row (EMA \u03b2). A small EMA \u03b2 also boost the eigenvalue of K(t), but the training converges much slower. Here \u03b7 = 0.04 so that \u03b7\u03b1 equals to the weight decay (\u03b7 = 0.0004) in our STL-10 experiments. where f is not zero-mean, we keepF a correlation matrix (rather than a covariance) without zero-centering f , otherwise the performance deteriorates. We also added a regularization factor proportional to a small to boost the small eigenvalues s j so they can learn faster. In all our experiments on real-world datasets, we use 2 -normalization so the absolute magnitude of s j doesn't matter.\nHyper-parameter freq. Besides, we also evaluate a hybrid approach by introducing freq, which is how frequently eigen-decomposition is conducted for matrixF to set W p . For example, freq = 5 means that eigen decomposition is run every 5 minibatches. When W p is not set by eigen decomposition, it is updated by regular gradient updates. freq = 1 means the eigen-decomposition is performed at every minibatch.\nTbl. 6 shows that directly computing W p through Direct-Pred works better (76.77%) than training via gradient descent (74.51% in Tbl. 3, regular W p with EMA). Additional regularization through yields even better perfor-  mance (77.38%). Different ways to estimate F (moving average or simple average) yield only small differences.\nThe performance of DirectPred also remains good over many more training epochs (Tbl. 8). Moreover, if we allow some gradient steps in between directly setting W p (i.e., freq > 1), performance becomes even better (80.28%). This might occur because the estimatedF may not be accurate enough and SGD can help correct it. This also mitigates the computational cost of eigen-decomposition.\nThe constant c j . What happens if p j = max(s j \u2212 c j , 0) with c j = 0? If c j is small negative, performance is still fine but a positive c j leads to very poor performance (Tbl. 7), likely due to many small eigen-values s j becoming zero and therefore trapped in the collapsed basin. using two layer predictors is that W p can depend on the input features. We explored this idea by using a few random partitions of the input space, and within each random partition we estimated a different correlation matrixF . The finalF is the sum of all the correlation matrices. With 6 random partitions, DirectPred achieves 78.20\u00b10.16 Top-1 accuracy after 100 epochs, closing performance gap to twolayer predictors (78.85% in Tbl. 3). We leave a thorough analysis of the two layer setting to future work.\nImageNet experiments. We conducted additional experiments on ImageNet (Deng et al., 2009), with our own BYOL (Grill et al., 2020) implementation. We used ResNet-50 (He et al., 2016) as the backbone to produce features for a linear probe, followed by a projector and a predictor. The architecture design (e.g., feature dimensions), augmentation strategies (e.g., color jittering, blur (Chen et al., 2020a), solarization, etc.) and linear classification protocol strictly follow BYOL (Grill et al., 2020).\nWe experimented with two different training settings to study the generalization ability of DirectPred. In the first setting, we employ an asymmetric loss (given two views, only one view is used as the prediction target). The loss is optimized using standard SGD for 60 epochs with a batch size of 256.   (Grill et al., 2020). Without gradient-based training, Di-rectPred is able to match the performance of the default 2-layer predictor introduced by BYOL, and significantly outperform the linear predictor by 5% (60 epoch) and 2.5% (300 epoch).\nrun BYOL with a learned linear predictor. We find the performance drops to 69.9%, and 89.6% respectively (2.5% gap to our method). The gap is even bigger in 60-epoch settings, up to 5.0% in top-1 (59.4% vs. 64.4%). These experiments demonstrate the success of DirectPred on STL-10 and CIFAR can also generalize and scale to ImageNet.", "publication_ref": ["b10", "b16", "b17", "b6", "b16", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Discussion", "text": "Summary. Therefore, remarkably, our theoretical analysis of non-contrastive SSL, primarily centered around a 3 dimensional nonlinear dynamical system, not only yields conceptual insights into the functional roles of complex ingredients like EMA, stop-gradients, predictors, predictor symmetry, diverse learning rates, weight decay and all their interactions, but also predicts the performance patterns of many ablation studies as well as suggests an exceedingly simple DirectPred method that rivals the performance of more complex predictor dynamics in real-world settings.\nTwo-layer non-linear predictor. With only a linear predictor, our results on ImageNet (Tbl. 9) have already shown strong performance, on par with a default BYOL setting with a 2-layer predictor on ImageNet. One interesting question is how the dynamics changes if the predictor has 2 layers. While we don't provide a formal analysis and the math can be quite complicated, the intuition here is that the \"fat\" 2-layer predictor used in practice (e.g., more (4096) hidden dimension than input/output dimensions (256), and a ReLU in between) essentially provides a large pool of initial weight directions to start with, and some of them could be \"lucky draws\", that make eigen-space alignment faster. On the other hand, a 1-layer predictor with gradient updates may get stuck in local minima. Therefore, with the same number of epochs, a 2-layer predictor outperforms 1layer, and is comparable with DirectPred which does not suffer from local minima issues. Supplementary Materials for \"Understanding Self-Supervised Learning Dynamics without Contrastive Pairs\"\nA. Section 2\nLemma 1 (Dynamics of BYOL/SimSiam). For objective (f 1 = W x 1 and f 2a = W a x 2 where W a is EMA weight):\nJ(W, W p ) := 1 2 E x\u223cp(\u2022), x1,x2\u223cpaug(\u2022|x) W p f 1 \u2212 StopGrad(f 2a ) 2 2 (20\n)\nLet X = E [xx ] wherex(x) := E x \u223cpaug(\u2022|x) [x ]\nis the average augmented view of a data point x and X =\nE x V x |x [x ] is the covariance matrix V x |x [x ]\nof augmented views x conditioned on x, subsequently averaged over the data x. The dynamics is the following:\nW p = \u2212 \u2202J \u2202W p = \u2212W p W (X + X )W + W a XW (21) W = \u2212 \u2202J \u2202W = \u2212W p W p W (X + X ) + W p W a X (22) Proof. Note that (W p f 1 \u2212 f 2a ) (W p f 1 \u2212 f 2a ) (23) = f 1 W p W p f 1 \u2212 f 2a W p f 1 \u2212 f 1 W p f 2a + f 2a f 2a (24) = tr(W p W p f 1 f 1 ) \u2212 tr(W p f 1 f 2a ) \u2212 tr(W p f 2a f 1 ) + tr(f 2a f 2a ) (25) Let F 1 = E [f 1 f 1 ] = W (X + X )W where X = E x [xx ] and X = E x V x |x [x ] , F 1,2a = E [f 1 f 2a ], F 2a,1 = E [f 2a f 1 ] = F 1,2a and F 2a = E [f 2a f 2a ]\n. This leads to:\nJ(W, W p ) = 1 2 tr(W p W p F 1 ) \u2212 tr(W p F 1,2a ) \u2212 tr(F 1,2a W p ) + tr(F 2a )(26)\nTaking partial derivative with respect to W p and we get the gradient update rule:\nW p = \u2212 \u2202J \u2202W p = \u2212W p F 1 + F 1,2a(27)\nNow we take the derivative with respect to W . Note that we have stop-gradient in f 2a , so we would like to be careful when taking derivatives. We first compute \u2202J/\u2202F 1 and \u2202J/\u2202F 1,2a . Note that both F 1 and F 1,2a contains W , due to the fact that we have stop gradient, F 1 is a quadratic form of W but F 1,2a is a linear form of W . This is critical.\n\u2202J \u2202F 1 = 1 2 W p W p (28\n)\n\u2202J \u2202F 1,2a = \u2212W p (29\n)\nLet W = [w ij ] and X = E [xx ] (X tot and X are defined similarly). We have F 1 = W (X + X )W and F 1,2a = W XW a . So we have:\n\u2202J \u2202w ij = kl \u2202J \u2202F 1 kl \u2202[F 1 ] kl \u2202w ij + kl \u2202J \u2202F 1,2a kl \u2202[F 1,2a ] kl \u2202w ij (30\n)\nLet C = X + X , here we have:\nkl \u2202J \u2202F 1 kl \u2202[F 1 ] kl \u2202w ij = kl \u2202J \u2202F 1 kl mn \u2202w km c mn w ln \u2202w ij (31) = kl \u2202J \u2202F 1 kl \u03b4(i = k) n c jn w ln + \u03b4(i = l) m w km c mj (32) = l \u2202J \u2202F 1 il n c jn w ln + k \u2202J \u2202F 1 ki m w km c mj (33) = \u2202J \u2202F 1 W C + \u2202J \u2202F 1 W C ij (34\n)\nSimilarly (note that we don't take derivative with respect to W a ):\nkl \u2202J \u2202F 1,2a kl \u2202[F 1,2a ] kl \u2202w ij = \u2202J \u2202F 1,2a W a X ij (35\n)\nSo we have:\u1e86 = \u2212 \u2202J \u2202W = \u2212W p W p W (X + X ) + W p W a X (36\n)\nAfter some manipulation, we finally arrive at the following gradient update rule:\nW p = [\u2212W p W (X + X ) + W a X]W (37) W = W p [\u2212W p W (X + X ) + W a X](38)\nRemarks. For symmetric loss:\nJ(W, W p ) := 1 4 E x\u223cp(\u2022), x1,x2\u223cpaug(\u2022|x) W p f 1 \u2212 StopGrad(f 2a ) 2 2 + W p f 2 \u2212 StopGrad(f 1a ) 2 2 (39)\nThe update rule is done by swapping subscript 1 and 2 in the update rule of W p (here\nF 2 = E [f 2 f 2 ]): W p = \u2212 \u2202J \u2202W p = \u2212 1 2 W p (F 1 + F 2 ) + 1 2 (F 2a,1 + F 1a,2 )(40)\nUnder the large batch limit, it is the same as Eqn. 41.\nNote that the Lemma doesn't include weight decay. With weight decay \u03b7, it is not hard to see that we will arrive at the following slightly altered gradient flow:\nW p = [\u2212W p W (X + X ) + W a X]W \u2212 \u03b7W p (41) W = W p [\u2212W p W (X + X ) + W a X] \u2212 \u03b7W (42)\nTheorem 1 (Invariance of the Gradient Update). The gradient update rules (Eqn. 2 and Eqn. 3) has the following invariance (where the symmetric matrix C depends on initialization):\nW (t)W (t) = W p (t)W p (t) + e \u22122\u03b7t C(43)\nProof. From Eqn. 42 and Eqn. 41, we know that\n\u03b1 \u22121 p W p\u1e86p + \u03b1 \u22121 p \u03b7W p W p =\u1e86 W + \u03b7W W (44)\nTaking transpose and we have:\n\u03b1 \u22121 p\u1e86 p W p + \u03b1 \u22121 p \u03b7W p W p = W\u1e86 + \u03b7W W (45)\nAdding them together and multiply both side with e 2\u03b7t :\n\u03b1 \u22121 p d dt (e 2\u03b7t W p W p ) = d dt (e 2\u03b7t W W ) (46)\nThis leads to\ne 2\u03b7t W W = \u03b1 \u22121 p e 2\u03b7t W p W p + C, or W W = \u03b1 \u22121 p W p W p + e \u22122\u03b7t C\n. Lemma 2 (Dynamics of a negative definite system). Let H(t) be d-by-d time-varying positive definite (PD) matrices whose minimal eigenvalues are bounded away from 0: inf t\u22650 \u03bb min (H(t)) \u2265 \u03bb 0 > 0, then the following dynamics:\ndw(t) dt = \u2212H(t)w(t)(47)\nsatisfies w(t) 2 \u2264 e \u2212\u03bb0t w(0) 2 , which means that w(t) \u2192 0.\nProof. Construct the following Lyapunov function V (w) := 1 2 w 2 2 . For V (w(t)) we have:\ndV dt = dV dw dw dt = \u2212w (t)H(t)w(t)(48)\nNote that H(t) has eigen-decomposition:\nH(t) = j \u03bb j (t)u j (t)u j (t) with all \u03bb j (t) \u2265 \u03bb 0 and [u 1 (t), u 2 (t), . . . , u d (t)]\nforming an orthonormal bases. Therefore:\nw Hw = j \u03bb j w u j u j w \u2265 \u03bb 0 w \uf8ee \uf8f0 j u j u j \uf8f9 \uf8fb w = \u03bb 0 w 2 2 (49)\nTherefore, we have:\ndV dt \u2264 \u2212\u03bb 0 w(t) 2 2 = \u22122\u03bb 0 V (50)\nwhich leads to V (t) \u2264 e \u22122\u03bb0t V (0). That is w(t) 2 \u2264 e \u2212\u03bb0t w(0) 2 .\nTheorem 2 (No-stop gradient will not work). With W a = W (SimSiam case), removing the stop-gradient signal yields a gradient update for W given by positive semi-definite (PSD) matrix H(t) := X \u2297 (W p W p + I) + X \u2297W pWp (her\u1ebd W p := W p \u2212 I and \u2297 is the Kronecker product):\nd dt vec(W ) = \u2212H(t)vec(W ). (51\n)\nIf inf t\u22650 \u03bb min (H(t)) \u2265 \u03bb 0 > 0, then W (t) \u2192 0.\nProof. Note that if we don't have stop gradient and W a = W , then we have additional terms (and we also need to compute \u2202J/\u2202F 2 ). LetW p = W p \u2212 I n2 and we have:\nW = \u2212 \u2202J \u2202W = \u2212W p W p W (X + X ) + (W p + W p )W X \u2212 W (X + X ) \u2212 \u03b7W (52) = \u2212(W p W p + I)W X \u2212 (W p W p \u2212 W p \u2212 W p + I)W X \u2212 \u03b7W (53) = \u2212(W p W p + I)W X \u2212 (W p \u2212 I) (W p \u2212 I)W X \u2212 \u03b7W (54) = \u2212(W p W p + I)W X \u2212W pWp W X \u2212 \u03b7W (55)\nWith vec(AXB) = (B \u2297 A)vec(X) and we see:\nd dt vec(W ) = \u2212 X \u2297 (W p W p + I) + X \u2297W pWp + \u03b7I n1n2 vec(W )(56)\nIf inf t\u22650 \u03bb min (H(t)) \u2265 \u03bb 0 > 0, then applying Lemma 2 and we have vec(W (t)) 2 \u2264 e \u2212\u03bb0t vec(W (0)) 2 \u2192 0, and there is no chance for W to learn any meaningful features. Figure 7. Check the validity of EMA assumption (Assumption 1) with different EMA coefficients \u03b3a for BYOL dynamics with X = I and X = \u03c3 2 I (Assumption 2). \u03c3 = 0.03. All experiments are run 10 times to get mean and standard derivation (shaded area). We could see the EMA assumption is largely correct. Even at the region with \u03b3a close to 1 (e.g., 0.996) and large \u03b7, the normalized correlation between Wa and W are still high (\u223c 0.9). Note that throughout our analysis, the initial value of Wa(0) = 0. Left: weight decay \u03b7 = 0, Middle: \u03b7 = 0.01, Right: \u03b7 = 0.1.\nRemark. Note that if W a = W and we choose not to use the predictor (W p = I), then no matter whether we choose to use stop-gradient or not, W (t) always goes to 0. The theorem above already proved that without stop gradient, it is the case. When there is stop gradient, from Eqn. 3, we have:\nW = \u2212(X + \u03b7I)W(57)\nNote that X + \u03b7I is a PD matrix and with similar arguments, W (t) \u2192 0.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B. Section 3", "text": "Isometric assumptions. Now we use the assumption that X = I and X = \u03c3 2 I, which leads t\u022f\nF =\u1e86 XW + W X\u1e86 = \u2212(1 + \u03c3 2 )(W p W p F + F W p W p ) + W p W a W + W W a W p(58)\nhere F = W XW = W W . If we also have weight decay \u2212\u03b7W for W , then we have:\nF = \u2212(1 + \u03c3 2 )(W p W p F + F W p W p ) + W p W a W + W W a W p \u2212 2\u03b7F(59)\nor using anticommutator {A, B} := AB + BA:\nF = \u2212(1 + \u03c3 2 ){F, W p W p } + W p W a W + W W a W p \u2212 2\u03b7F(60)\nSimilarly, for W p we have:\n\u1e86 p = \u2212\u03b1 p (1 + \u03c3 2 )W p F + \u03b1 p \u03c4 F \u2212 \u03b7W p(61)\nEMA assumption (Assumption 1). Now we further study the effect of EMA. To model it, we just let W a = \u03c4 W where \u03c4 < 1 is a coefficient that measure how much EMA attenuates W . If \u03c4 = 1 then W a = W and there is no EMA. Note that \u03c4 is not the same as the EMA parameter 1 \u2212 \u03b3 a , which is often set to be a fixed 0.004 (or 1 \u2212 0.996). Instead, \u03c4 = \u03c4 (t) is a changing parameter depends on how quickly W = W (t) grows over time. If W remains stable, then \u03c4 \u2248 1; if W grows rapidly, then \u03c4 becomes small. Fig. 7 shows that this assumption is largely correct.\nUnder this condition, using F = W XW = W W , the dynamics becomes (Now we also put weight decay for W p ):\nW p = \u2212\u03b1 p (1 + \u03c3 2 )W p F + \u03b1 p \u03c4 F \u2212 \u03b7W p(62)\nF = \u2212(1 + \u03c3 2 )(W p W p F + F W p W p ) + \u03c4 (W p F + F W p ) \u2212 2\u03b7F(63)\nDerivation of Fixed point of Eqn. 2. Given the dynamics Eqn. 62 we now want to check its fixed point:\n\u2212\u03b1 p (1 + \u03c3 2 )W p F + \u03b1 p \u03c4 F \u2212 \u03b7W p = 0(64)\nfor some PSD matrix F . For convenience, let \u03b7 = \u03b7/\u03b1 p . Since F is always PSD, we have eigendecomposition F = U \u039bU . Left-multiplying U and right-multiplying U , we have:\n(1 + \u03c3 2 )W p \u039b + \u03b7 W p = \u03c4 \u039b(65)\nwhereW p := U W p U . Let \u039b = (1 + \u03c3 2 )\u039b + \u03b7 I is a diagonal matrix with all positive diagonal element since \u03b7 > 0. Therefore, we have:\nW p \u039b = \u03c4 \u039b(66)\nand thusW p = \u03c4 \u039b(\u039b ) \u22121 is a symmetric matrix and so does W p = UW p U . When \u03b7 = 0 and F has zero eigenvalues, W p can have infinite solutions (or fixed points), and some of them might not be symmetric.\nSymmetrization of W p . Now we need to assume W p is symmetric and also symmetrize its dynamics, which yields (here {A, B} :\n= AB + BA):\u1e86 p = \u2212 \u03b1 p 2 (1 + \u03c3 2 ){W p , F } + \u03b1 p \u03c4 F \u2212 \u03b7W p (67\n)\nF = \u2212(1 + \u03c3 2 ){W 2 p , F } + \u03c4 {W p , F } \u2212 2\u03b7F\nNote that the asymmetric dynamic might be interesting and we will leave it later.\nB.1. Section 3.1\nTheorem 3 (Alignment of Eigenspace). Under the dynamics of Eqn. 67, the commutator [F, W p ] := F W p \u2212W p F satisfies:\nd dt [F, W p ] = \u2212[F, W p ]K \u2212 K[F, W p ](68)\nwhere\nK = K(t) = (1 + \u03c3 2 ) \u03b1 p 2 F (t) + W 2 p (t) \u2212 \u03c4 1 + \u03c3 2 W p (t) + 3 2 \u03b7I (69)\nIf max t\u22650 \u03bb min [K(t)] = \u03bb 0 > 0, then the commutator [F (t), W p (t)] F \u2264 e \u22122\u03bb0t [F (0), W p (0)] F \u2192 0, i.e., the eigenspace of W p gradually aligns with F .\nProof. Let's compute the commutator L := [F, W p ] := F W p \u2212 W p F and its time derivative. First we have:\nF\u1e86 p \u2212\u1e86 p F = \u2212 \u03b1 p 2 (1 + \u03c3 2 )(F L + LF ) \u2212 \u03b7L(70)\nThen we have\u1e1e\nW p \u2212 W p\u1e1e = \u2212(1 + \u03c3 2 )(W 2 p L + LW 2 p ) + \u03c4 (W p L + LW p ) \u2212 2\u03b7L(71)\nSo we haveL\n= F\u1e86 p +\u1e1e W p \u2212 (W p\u1e1e +\u1e86 p F ) = \u2212KL \u2212 LK(72)\nwhere\nK = K(t) = (1 + \u03c3 2 ) \u03b1 p 2 F + W 2 p \u2212 \u03c4 1 + \u03c3 2 W p + 3 2 \u03b7I (73\n)\nis a symmetric matrix. We can write the dynamics of L(t):\ndvec(L(t)) dt = \u2212 [K(t) \u2295 K(t)] vec(L(t))(74)\nwhere K(t) \u2295 K(t) := I \u2297 K(t) + K(t) \u2297 I is the Kronecker sum and is a PSD matrix if K is PSD.\nIf inf t\u22650 \u03bb min (K(t)) \u2265 \u03bb 0 > 0 for all t, then inf t\u22650 \u03bb min [K(t) \u2295 K(t)] \u2265 2\u03bb 0 . Applying Lemma 2 and we have:\nvec(L) 2 \u2264 e \u22122\u03bb0t vec(L(0)) 2 \u2192 0(75)\nThis means that W p and F can commute, and the eigen space of W p and F will gradually align. When \u03b7 is large and/or \u03c4 is small, A F can also be dragged to zero, which is consistent with analysis in Sec. 3.2 (Obs#4 and Obs#5). On the other hand, B F always seems to vanish over time. In this numerical simulation, we set F = W p Wp following invariant in Theorem 1 with C = 0.\nRemark. Fig. 9 shows numerical simulation of the symmetrized dynamics (Eqn. 67). If K(t) has negative eigenvalues, then even if W p and F have already approximately aligned, the dynamics is also unstable and might diverge due to noise and/or numerical instability. Fig. 8 shows a numerical simulation of Eqn. 62 (dynamics with Assumption 1 and Assumption 2 but without the symmetric dynamics). We can clearly see that the asymmetric component converges to zero.\nWhen eigenspace aligns exactly. Let U be the common eigenvectors.\nW p = U \u039b Wp U where \u039b Wp = diag[p 1 , p 2 , . . . , p d ], F = U \u039b F U where \u039b F = diag[s 1 , s 2 , . . . , s d ].\nIn this case, the time derivatives\u1e86 p and\u1e1e can all be written as decoupled form:\u1e86 p = U G 1 U and\u1e1e = U G 2 U where G 1 and G 2 are diagonal matrices. In other words, they are both decoupled into each eigen mode, and so does the future value of W p and F . Then U won't change over time.\nTo see why, we consider the general case where we have a symmetric matrix M (t) with eigen decomposition\nM (t) = U (t)D(t)U (t). M follows\u1e40 = U (t)G(t)U (t)\nwhere G(t) is an arbitrary diagonal matrix.\nTo see whyU = 0, at each time step we have:\nM =U DU + U\u1e0aU + U DU = U GU (76\n)\nsince U is unitary, we have:\nU U D + DU U = G \u2212\u1e0a (77)\nSince U (t)U (t) = I, we haveU U + U U = 0 so Q := U U is a skew-symmetric matrix and we have\nQD \u2212 DQ = G \u2212\u1e0a (78\n)\nSince the right hand side is a diagonal matrix, checking each entry and we have q ij d j \u2212 q ij d i = 0 for i = j. If M has distinctive eigenvalues, then we know q ij = 0 for i = j. Q is skew-symmetric so q ii = 0. So Q = U U = 0  and thusU = 0. If M has duplicated eigenvalues, then we can show q ij = 0 for any d i = d j . Within high-dimensional eigenspace for duplicated eigenvalues, its eigen-decomposition is not unique and we can always pick the eigenspace within each duplicated eigenspace so thatU = 0.\nTherefore, we just multiply U and U to Eqn. 67 and the system becomes decoupled. Then after some algebraic manipulation, we arrive at the following:\u1e57\nj = \u03b1 p (1 + \u03c3 2 )s j \u03c4 1 + \u03c3 2 \u2212 p j \u2212 \u03b7p j (79\n)\ns j = 2(1 + \u03c3 2 )p j s j \u03c4 1 + \u03c3 2 \u2212 p j \u2212 2\u03b7s j (80)\nMultiply Eqn. 79 with 2\u03b1 \u22121 p p j and subtract with Eqn. 80, we get:\n2\u03b1 \u22121 p p j\u1e57j \u2212\u1e61 j = \u22122\u03b7\u03b1 \u22121 p p 2 j + 2\u03b7s j (81) which gives \u03b1 \u22121 p dp 2 j dt + 2\u03b7p 2 j =\u1e61 j + 2\u03b7s j (82) \u03b1 \u22121 p d dt (e 2\u03b7t p 2 j ) = d dt (e 2\u03b7t s j ) (83) \u03b1 \u22121 p e 2\u03b7t p 2 j = e 2\u03b7t s j \u2212 c j (84) \u03b1 \u22121 p p 2 j (t) = s j (t) \u2212 e \u22122\u03b7t c j (85)\nTherefore, we have integral s j (t) = \u03b1 \u22121 p p 2 j (t) + c j e \u22122\u03b7t . For finite weight decay (\u03b7 > 0), we could simply expect s j (t) \u2248 \u03b1 \u22121 p p 2 j (t). On the other hand, the dynamics of \u03c4 is:\n\u1e86 a = \u03b2(W \u2212 W a )(86)\nApplying our assumption about EMA (Assumption 1) W a (t) = \u03c4 (t)W (t), then we have:\n\u03c4 W + \u03c4\u1e86 = \u03b2(1 \u2212 \u03c4 )W (87) \u03c4 W W + \u03c4\u1e86 W = \u03b2(1 \u2212 \u03c4 )W W (88) 2\u03c4 F + \u03c4\u1e1e = 2\u03b2(1 \u2212 \u03c4 )F (89)\nWhen F and W p aligns, we have\u1e1e all in the same eigen space.\nF = \u2212(1 + \u03c3 2 ){W 2 p , F } + \u03c4 {W p , F } \u2212 2\u03b7F(90)\nSo the eigenvectors U won't change and thus we have:\n2\u03c4 s j + \u03c4\u1e61 j = 2\u03b2(1 \u2212 \u03c4 )s j (91) or\u03c4 = \u03b2(1 \u2212 \u03c4 ) \u2212 \u03c4\u1e61 j 2s j (92\n)\nwhich has a close form solution when c j = 0. Note that in the case, we have s j = \u03b1 \u22121 p p 2 j and thus\u1e61 j = 2\u03b1 \u22121 p p j\u1e57j and we have:\u03c4\n= \u03b2(1 \u2212 \u03c4 ) \u2212 \u03c4\u1e57 j p j (93\n) or\u03c4 + \u03c4 \u1e57 j p j + \u03b2 = \u03b2 (94) or d dt (e f (t) \u03c4 ) = \u03b2e f (t)(95)\nwhere f (t) = (\u1e57 j /p j + \u03b2)dt = ln p j + \u03b2t and thus e f (t) = e \u03b2t p j . Take integral on both side and we have (here \u03c4 (0) = 0 is the initial condition):\ne \u03b2t p j \u03c4 = \u03b2 t 0 e \u03b2t p j (t )dt (96\n)\nwhich is: \n\u03c4 j (t) = p \u22121 j (t)\u03b2e \u2212\u03b2t\np * j\u2212 = \u03c4 \u2212 \u03c4 2 \u2212 4\u03b7(1 + \u03c3 2 ) 2(1 + \u03c3 2 ) (98\n)\nis the (right) boundary of trivial basin p < p * j\u2212 and determines the size of trivial attractive region towards p * j0 = 0. It is dependent on \u03b7 and \u03c4 . It is clear that p * j\u2212 is a increasing function of \u03b7. This means that if the weight decay \u03b7 is large, so does trivial region (and more eigenvalues will be trapped to trivial solution).\nOn the other hand, we can compute the derivative of g(x) = x \u2212 \u221a x 2 \u2212 c for c > 0 and x 2 > c:\ndg dx = 1 \u2212 1 1 \u2212 c/x 2 < 0 (99) So g(x)\nis a decreasing function with respect to x. Or p * j\u2212 is a decreasing function with respect to \u03c4 .", "publication_ref": [], "figure_ref": ["fig_8", "fig_6"], "table_ref": []}, {"heading": "C. Section 4", "text": "Experiment setup. Unless explicitly stated, in all our experiments, we use ResNet-18 as the backbone network, two-layer MLP (with BN and ReLU) as the projector, and a linear predictor. For STL-10 and CIFAR-10, we use SGD as the optimizer with learning rate \u03b1 = 0.03, momentum 0.9, weight decay\u03b7 = 0.0004 and EMA parameter \u03b3 a = 0.996. The batchsize is 128. Each setting is repeated 5 times to compute mean and standard derivation. We report final number as \"mean\u00b1std\".", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D. Analysis of BYOL and SimSiam learning dynamics without isotropic assumptions on data", "text": "In the main paper we focused on isotropic data assumptions to obtain analytic insights into when and why BYOL and SimSiam learning dynamics avoid representational collapse. Here we provide an alternate perspective using a different assumption, involving decoupled initial conditions, that enables us to address the case of learning with non-isotropic data. First, we recall the data generation and augmentation process. Let x be a data point drawn from the data distribution p(x) and let x 1 and x 2 be two augmented views of x: x 1 , x 2 \u223c p aug (\u2022|x) where p aug (\u2022|x) is the augmentation distribution. Let \u03a3 s = E [x 1 x 1 ] be the correlation matrix of a single augmented view x 1 of the data x, and let \u03a3 d = E [x 1 x 2 ] be the correlation matrix between two augmented views x 1 and x 2 of the same data point x. In the notation of the main paper, \u03a3 s and \u03a3 d can be decomposed as \u03a3 s = X + X and \u03a3 d = X, where\nX = E [xx ] andx(x) := E x \u223cpaug(\u2022|x) [x ]\nis the average augmented view of a data point x. In turn\nX = E x V x |x [x ] is the covariance matrix V x |x [x ]\nof augmented views x conditioned on x, subsequently averaged over the data x. Intuitively, X is the correlation matrix of augmentation averaged data, while X is the augmentation covariance matrix averaged over data.\nAlso recall that the BYOL learning dynamics, without weight decay, is given b\u1e8f\nW = W p \u2212W p W \u03a3 s + W a \u03a3 d (100) W p = \u03b1 p \u2212W p W \u03a3 s + W a \u03a3 d W (101) W a = \u03b2(\u2212W a + W )(102)\nSimSiam learning dynamics is a special case in which W a = W and the final equation is ignored.\nWe first derive exact fixed point solutions to both BYOL and SimSiam learning dynamics in this setting. We then discuss specific models for data distributions and augmentation procedures, and show how the fixed point solutions depend on both data and augmentation distributions. We then discuss how our theory reveals a fundamental role for the predictor in avoiding collapse in BYOL solutions. Finally, we derive a highly reduced three dimensional description of BYOL and SimSiam learning dynamics, assuming decopuled initial conditions, that provides considerable insights into dynamical mechanisms enabling both to avoid collapsed solutions without negative pairs to force apart representations of different objects.\nD.1. The fixed point structure of BYOL and Simsiam learning dynamics.\nExamining equation 100-equation 102, we find sufficient conditions for a fixed point given by W p W \u03a3 s = W a \u03a3 d and W = W a . Note these are sufficient conditions for fixed points of both BYOL and SimSiam. Inserting the second equation into the first and right multiplying both sides by [\u03a3 s ] \u22121 (assuming \u03a3 s is invertible), yields a manifold of fixed point solutions in W 1 and W 2 satisfying the nonlinear equation\nW p W = W \u03a3 d [\u03a3 s ] \u22121 .(103)\nThis constitutes a set of n 1 \u00d7 n 2 nonlinear equations in (n 1 \u00d7 n 2 ) + (n 2 \u00d7 n 2 ) unknowns, yielding generically a nonlinear manifold of solutions in W 1 and W 2 of dimensionality n 2 \u00d7 n 2 corresponding to the number of predictor parameters.\nFor concreteness, we will assume that n 2 \u2264 n 1 , so that the online and target networks perform dimensionality reduction.\nThen a special class of solutions to equation 103 can be obtained by assuming the n 2 rows of W correspond to n 2 lefteigenvectors of \u03a3 d [\u03a3 s ] \u22121 and W p is a diagonal matrix with the corresponding eigenvalues. This special class of solutions can then be generalized by a transformation W p \u2192 SW p S \u22121 and W \u2192 SW where S is any invertible n 2 by n 2 matrix. Indeed this transformation is a symmetry of equation 103, which defines the solution manifold. In addition to these families of solutions, the collapsed solution W = W p = W a = 0 also exists.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.2. Illustrative models for data and data augmentation", "text": "The above section suggests that the top eigenmodes of Multiplicative scrambling. Consider for example a multiplicative subspace scrambling model. In this model, data augmentation scrambles a subspace by multiplying by a random Gaussian matrix, while identically preserving the orthogonal complement of the subspace. In applications, the scrambled subspace could correspond to a space of nuisance features, while the preserved subspace could correspond to semantically important features. Indeed many augmentation procedures, including random color distortions and blurs, largely preserve important semantic information, like object identity in images.\nMore precisely, we consider a random scrambling operator A which only scrambles data vectors x within a fixed k dimensional subspace spanned by the orthonormal columns of the n 0 \u00d7 k matrix U . Within this subspace, data vectors are scrambled by a random Gaussian k \u00d7 k matrix B. Thus A takes the form A = P c + U BU T where P c = I \u2212 U U T is a projection operator onto the n 0 \u2212 k dimensional conserved, semantically important, subspace orthogonal to the span of the columns of U , and the elements of B are i.i.d. zero mean unit variance Gaussian random variables so that E [B ij B kl ] = \u03b4 ik \u03b4 jl . Under this simple model, the augmentation averagex(x) := E x \u223cpaug(\u2022|x) [x ] becomesx(x) = P c x. Thus, intuitively, under multiplicative subspace scrambling, the only aspect of a data vector that survives averaging over augmentations is the projection of this data vector onto the preserved subspace. Then the correlation matrix of two different augmented views is \u03a3 d = P c \u03a3 x P c while the correlation matrix of two identical views is \u03a3 s = \u03a3 x where \u03a3 x \u2261 E x\u223cp(\u2022) xx T is the correlation matrix of the data distribution. Thus non-collapsed solutions of both BYOL and SimSiam can correspond to principal eigenmodes of\n\u03a3 d [\u03a3 s ] \u22121 = P c \u03a3 x P c [\u03a3 x ] \u22121 .\nIn the special case in which P c commutes with \u03a3 x , we have the simple result that \u03a3 d [\u03a3 s ] \u22121 = P c , which is completely independent of the data correlation matrix \u03a3 x . Thus in this simple setting BYOL and SimSiam can learn the subspace of features that are identically conserved under data augmentation, independent of how much data variance there is in the different dimensions of this conserved subspace.\nAdditive scrambling. We also consider, as an illustrative example, data augmentation procedures which simply add Gaussian noise with a prescribed noise covariance matrix \u03a3 n . Under this model, we have\n\u03a3 s = \u03a3 x + \u03a3 n while \u03a3 d = \u03a3 x .\nThus in this setting, BYOL learns principal eigenmodes of \u03a3\nd [\u03a3 s ] \u22121 = \u03a3 x [\u03a3 x + \u03a3 n ] \u22121 .\nThus intuitively, dimensions with larger noise variance are attenuated in learned BYOL representations. On the otherhand, correlations in the data that are not attenuated by noise are preferentially learned, but the degree to which they are learned is not strongly influenced by the magnitude of the data correlation (i.e. consider dimensions that lie along small eigenvalues of \u03a3 n ). Note that in the main paper we focused on the case where \u03a3 x = I and \u03a3 n = \u03c3 2 I. D.3. The importance of the predictor in BYOL and SimSiam.\nHere we note that our theory explains why the predictor plays a crucial role in BYOL and SimSiam learning in this simple setting, as is observed empirically in more complex settings. To see this, we can model the removal of the predictor by simply setting W p = I in all the above equations. The fixed point solutions then obey W = W \u03a3 d [\u03a3 s ] \u22121 . This will only have nontrivial, non-collapsed solutions if \u03a3 d [\u03a3 s ] \u22121 has eigenvectors with eigenvalue 1. Rows of W consisting of linear combinations of these eigenvectors will then constitute non-collapsed solutions.\nThis constraint of eigenvalue 1 yields a much more restrictive condition on data distributions and augmentation procedures for BYOL and Simsiam to have non-collapsed solutions. It can however be satisfied in multiplicative scrambling if an eigenvector of the data matrix \u03a3 x lies in the column space of the projection operator P c (in which case it is an eigenvector of eigenvalue 1 of \u03a3 d [\u03a3 s ] \u22121 = P c \u03a3 x P c [\u03a3 x ] \u22121 . This condition cannot however be generically satisfied for additive scrambling case, in which generically all the eigenvalues of \u03a3 d [\u03a3 s ] \u22121 = \u03a3 x [\u03a3 x + \u03a3 n ] \u22121 are less than 1. In this case, without a predictor, it can be checked that the collapsed solution W = W a = 0 is stable.\nThus overall, in this simple setting, our theory provides conceptual insight into how the introduction of a predictor is crucial for creating new non-collapsed solutions for both BYOL and SimSiam, even though the predictor confers no new expressive capacity in allowing the online network to match the target network.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.4. Reduction of BYOL learning dynamics to low dimensions", "text": "The full learning dynamics in equation 100 to equation 102 constitutes a set of high dimensional nonlinear ODEs which are difficult to solve from arbitrary initial conditions. However, there is a special class of decoupled initial conditions which permits additional insight. Consider the special case in which \u03a3 s and \u03a3 d commute, and so are simultaneously diagonalizable and share a common set of eigenvectors, which we denote by u \u03b1 \u2208 R n0 . Consider also a special set of initial conditions where each row of W and the corresponding row of W a are both proportional to one of the eigenmodes u \u03b1 , with scalar proportionality constants w \u03b1 and w \u03b1 a respectively, and W p is diagonal, with the corresponding diagonal element given by w \u03b1 p . Then it is straightforward to see that under the dynamics in equation 100 to equation 102, that the shows the hyperoblic manifold of stable fixed points wpw = wa\u03bb d \u03bb \u22121 s , while the red point at the origin is an unstable fixed point. For a fixed target network, the online and predictor weights will cooperatively amplify each other to escape the collapsed solution at the origin. Middle: A visualization of the full low dimensional BYOL dynamics in Eqns 104-106 when the online and predictor weights are tied so that w = wp. The green curve shows the nullcline wa = w corresponding to dwa dt = 0 and the blue curve shows part of the nullcline dw dt = 0 corresponding to w 2 = wa\u03bb d \u03bb \u22121 s . The intersection of these two nullclines yields two fixed points (red dots): an unstable collapsed solution at the origin w = wa = 0, and a stable non-collapsed solution with wa = w and w = \u03bb d \u03bb \u22121 s . Right: A visualization of dynamics in Eqns 104-106 when the the predictor is removed, so that w2 is fixed to 1. The resulting two dimensional flow field on w and wa is shown (black arrows). The green curve shows the nullcline w = wa corresponding to dwa dt = 0, while the blue curve shows the nullcline w = wa\u03bb d \u03bb \u22121 s . The slope of this nullcline is \u03bbs\u03bb \u22121 d > 1. The resulting nullcline structure yields a single fixed point at the origin which is stable. Thus there only exists a collapsed solution. In the special case where \u03bbs\u03bb \u22121 d = 1, the two nullclines coincide, yielding a one dimensional manifold of solutions. \nAlternatively, this low dimensional dynamics can be obtained from equation 100 to equation 102 not only by considering a special class of decoupled initial conditions, but also by considering the special case where every matrix is simply a 1 by 1 matrix, making the scalar replacements W \u2192 w, W p \u2192 w p , W a \u2192 w a , \u03a3 s \u2192 \u03bb s , and \u03a3 d \u2192 \u03bb d . Note furthermore that this 3 dimensional dynamical system is equivalent to that studied in the main paper under the change of variables s = w 2 and \u03c4 = w a /w and the special case of \u03bb s = 1 + \u03c3 2 and \u03bb d = 1.\nThe fixed point conditions of this dynamics are given by w a = w and w p w = w a \u03bb d \u03bb \u22121 s . Thus the collapsed point w = w p = w a = 0 is a solution. Additionally w p = \u03bb d \u03bb \u22121 s and w = w a taking any value is also a family of non-collapsed solutions. We can understand the three dimensional dynamics intuitively as follows when \u03b2 is much less than both 1 and \u03b1 p , so that the dynamics of w and w p are very fast relative to the dynamics of w a . In this case, the target network evolves very slowly compared to the online network, as is done in practice. For simplicity we use the same learning rate for the predictor as we do for the online network (i.e. \u03b1 p = 1). In this situation, we can treat w a as approximately constant on the fast time scale over which the online and predictor weights w and w p evolve. Then the joint dynamics in equation 104 and", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We thank Lantao Yu for helpful discussions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Iso-contours of constant error are hyperbolas in the w by w p plane, and for fixed w a , the origin w = w p = 0 is a saddle point, yielding an unstable fixed point (see Fig. 10 (left)). From generic initial conditions, w and w p will then cooperatively amplify each other to rapidly escape the collapsed solution at the origin, and approach the zero error hyperbolic contour w p w = w a \u03bb d \u03bb \u22121 s where w a is close to its initial value. Then the slower target network w a will adjust, slowly moving this contour until w a = w. The more rapid dynamics of w and w p will hug the moving contour w p w = w a \u03bb d \u03bb \u22121 s as w a slowly adjusts. In this fashion, the joint fast dynamics of w and w p , combined with the slow dynamics of w a , leads to a nonzero fixed point for all 3 values, despite the existence of a collapsed fixed point at the origin. Moreover, the larger the ratio \u03bb d \u03bb \u22121 s , which is determined by the data and augmentation, the larger the final values of both w and w p will tend to be. We can obtain further insight by noting that the submanifold w = w p , in which the online and predictor weights are tied, constitutes an invariant submanifold of the dynamics in Eqns. 104 to 106; if w = w p at any instant of time, then this condition holds for all future time. Therefore we can both analyze and visualize the dynamics on this two dimensional invariant submanifold, with coordinates w = w p and w a (Fig. 10 (middle)). This analysis clearly shows an unstable collapsed solution at the origin, with w = w a = 0, and a stable non-collapsed solution at w = w a = \u03bb d \u03bb \u22121 s . We note again, that the generic existence of these non-collapsed solutions in Fig. 10 depends critically on the presence of a predictor with adjustable weights w p . Removing the predictor corresponds to forcing w p = 1, and non-collapsed solutions cannot exist unless \u03bb d = \u03bb s , as demonstrated in Fig. 10 (right). Thus, remarkably, in BYOL in this simple setting, the introduction of a predictor network plays a crucial role, even though it neither adds to the expressive capacity of the online network, nor improves its ability to match the target network. Instead, it plays a crucial role by dramatically modifying the learning dynamics (compare e.g. Fig 10 middle and right panels), thereby enabling convergence to noncollapsed solutions through a dynamical mechanism whereby the online and predictor network cooperatively amplify each others' weights to escape collapsed solutions ( Fig. 10 (left)).\nOverall, this analysis of BYOL learning dynamics provides considerable insight into the dynamical mechanisms enabling BYOL to avoid collapsed solutions, without negative pairs to force apart representations, in what is likely to be the simplest nontrivial setting. Further analysis on this model, in direct analogy to the analysis performed on the equivalent 3 dynamical system (derived under different assumptions) studied in the main paper, can yield similar insights into the dynamics of BYOL and SimSiam under various conditions on learning rates.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "On the optimization of deep networks: Implicit acceleration by overparameterization", "journal": "", "year": "2018", "authors": "S Arora; N Cohen; E Hazan"}, {"ref_id": "b1", "title": "A theoretical analysis of contrastive unsupervised representation learning", "journal": "", "year": "2019", "authors": "S Arora; H Khandeparkar; M Khodak; O Plevrakis; N Saunshi"}, {"ref_id": "b2", "title": "Learning representations by maximizing mutual information across views", "journal": "", "year": "2019", "authors": "P Bachman; R D Hjelm; W Buchwalter"}, {"ref_id": "b3", "title": "Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks", "journal": "", "year": "2018", "authors": "P Bartlett; D Helmbold; P Long"}, {"ref_id": "b4", "title": "Signature verification using a\" siamese\" time delay neural network", "journal": "NeurIPS", "year": "1994", "authors": "J Bromley; I Guyon; Y Lecun; E S\u00e4ckinger; R Shah"}, {"ref_id": "b5", "title": "Globally optimal gradient descent for a convnet with gaussian inputs", "journal": "", "year": "2017", "authors": "A Brutzkus; A Globerson"}, {"ref_id": "b6", "title": "A simple framework for contrastive learning of visual representations", "journal": "", "year": "2020", "authors": "T Chen; S Kornblith; M Norouzi; G Hinton"}, {"ref_id": "b7", "title": "Exploring simple siamese representation learning", "journal": "", "year": "2020", "authors": "X Chen; K He"}, {"ref_id": "b8", "title": "Improved baselines with momentum contrastive learning", "journal": "", "year": "2020", "authors": "X Chen; H Fan; R Girshick; K He"}, {"ref_id": "b9", "title": "An analysis of single-layer networks in unsupervised feature learning", "journal": "", "year": "2011", "authors": "A Coates; A Ng; H Lee"}, {"ref_id": "b10", "title": "ImageNet: A Large-Scale Hierarchical Image Database", "journal": "", "year": "2009", "authors": "J Deng; W Dong; R Socher; L.-J Li; K Li; L Fei-Fei"}, {"ref_id": "b11", "title": "Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2018", "authors": "J Devlin; M.-W Chang; K Lee; K Toutanova;  Bert"}, {"ref_id": "b12", "title": "Width provably matters in optimization for deep linear neural networks", "journal": "", "year": "2019", "authors": "S Du; W Hu"}, {"ref_id": "b13", "title": "Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced", "journal": "", "year": "2018", "authors": "S S Du; W Hu; J D Lee"}, {"ref_id": "b14", "title": "Gradient descent finds global minima of deep neural networks", "journal": "ICML", "year": "2019", "authors": "S S Du; J D Lee; H Li; L Wang; X Zhai"}, {"ref_id": "b15", "title": "Understanding selfsupervised and contrastive learning with \"bootstrap your own latent", "journal": "", "year": "", "authors": "A Fetterman; J Albrecht"}, {"ref_id": "b16", "title": "Bootstrap your own latent: A new approach to self-supervised learning", "journal": "", "year": "2020", "authors": "J.-B Grill; F Strub; F Altch\u00e9; C Tallec; P H Richemond; E Buchatskaya; C Doersch; B A Pires; Z D Guo; M G Azar"}, {"ref_id": "b17", "title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "K He; X Zhang; S Ren; J Sun"}, {"ref_id": "b18", "title": "Momentum contrast for unsupervised visual representation learning", "journal": "", "year": "2020", "authors": "K He; H Fan; Y Wu; S Xie; R Girshick"}, {"ref_id": "b19", "title": "Deep learning without poor local minima", "journal": "NeurIPS", "year": "2016", "authors": "K Kawaguchi"}, {"ref_id": "b20", "title": "Learning multiple layers of features from tiny images", "journal": "", "year": "2009", "authors": "A Krizhevsky; G Hinton"}, {"ref_id": "b21", "title": "An analytic theory of generalization dynamics and transfer learning in deep linear networks", "journal": "", "year": "2018", "authors": "A K Lampinen; S Ganguli"}, {"ref_id": "b22", "title": "Deep linear networks with arbitrary loss: All local minima are global", "journal": "PMLR", "year": "2018", "authors": "T Laurent; J Brecht"}, {"ref_id": "b23", "title": "Predicting what you already know helps: Provable self-supervised learning", "journal": "", "year": "2020", "authors": "J D Lee; Q Lei; N Saunshi; J Zhuo"}, {"ref_id": "b24", "title": "Representation learning with contrastive predictive coding", "journal": "", "year": "2018", "authors": "A Oord; Y Li; O Vinyals"}, {"ref_id": "b25", "title": "Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice", "journal": "", "year": "2017", "authors": "J Pennington; S Schoenholz; S Ganguli"}, {"ref_id": "b26", "title": "The emergence of spectral universality in deep networks", "journal": "", "year": "2018", "authors": "J Pennington; S S Schoenholz; S Ganguli"}, {"ref_id": "b27", "title": "Spurious local minima are common in two-layer relu neural networks", "journal": "", "year": "2018", "authors": "I Safran; O Shamir"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure1. Two-layer setting with a linear, bias-free predictor.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 .2Figure2. Training BYOL in STL-10 for 100 epochs with EMA. Top row: No symmetric regularization imposed on Wp, Bottom row: symmetric regularization on Wp. From left to right: (1) Evolvement of eigenvalues for F . Since F is PSD and its eigenvalue sj varies across scales, we plot log(si). We could see some eigenvalues are growing while others are shrinking to zero over training. (2) Similar \"step-function\" behaviors for the predictor Wp. Its negative eigenvalues shrinks towards zero and leading eigenvalues becomes larger.(3) The eigenspace of F and Wp gradually align with each other (Theorem 3). For each eigenvector uj of F , we compute cosine angle (normalized correlation) between uj and Wpuj to measure alignment. (4) Wp gradually becomes symmetric and PSD during training.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 .3Figure 3. State space dynamics in Eqns. 11 and 12 for no (\u03b7 = 0) weak (\u03b7 = 0.01) and strong (\u03b7 = 1) weight decay at fixed \u03c4 = 1 and \u03b1p = 1. Red (green) points indicate stable (unstable) fixed points, blue curves indicate flow lines, and the dashed black curve indicates the parabola sj = p 2 j /\u03b1p.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 .5Figure5. The role played by weight decay \u03b7 and EMA \u03b2 when applying symmetric regularization on Wp on synthetic experiments simulating decoupled dynamics. The learning rate \u03b1 = 0.01. Both terms boost the eigenvalue of K(t) to above 0 so that eigen space alignment could happen (Theorem 3), but also come with different trade-offs. Here \u03b2 = 0.4 so that \u03b1\u03b2 = 0.004 = 1 \u2212 \u03b3a where \u03b3a = 0.996 as in BYOL. Top row (Weight Decay \u03b7): A large \u03b7 boost the eigenvalue of K(t) up, but substantially decreases the final converging eigenvalues pj and sj (i.e., the final features are not salient), or even drags them to zero (no training happens). Bottom row (EMA \u03b2). A small EMA \u03b2 also boost the eigenvalue of K(t), but the training converges much slower. Here \u03b7 = 0.04 so that \u03b7\u03b1 equals to the weight decay (\u03b7 = 0.0004) in our STL-10 experiments.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 6 .6Figure 6. The effects of relative learning rate \u03b1p without EMA. If \u03b1p > 1, symmetric Wp with no EMA can also work. Experiments on STL-10 and CIFAR-10 (Krizhevsky et al., 2009) (100 epochs with 5 random seeds).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": ".77\u00b10.24 77.11\u00b10.35 77.86\u00b10.16 75.06\u00b11.10 \u03c1 = 0.5 76.65\u00b10.20 76.76\u00b10.33 77.56\u00b10.25 75.22\u00b10.81", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 8 .8Figure 8. Dynamics of the symmetric A := (Wp + W p )/2 and asymmetric part B := (Wp \u2212 W p )/2 of Wp under different timeindependent \u03c4 of Eqn. 62. Each row is a different weight decay \u03b7 (i.e., \u03b7 = 0.001, 0.01 and 0.05).When \u03b7 is large and/or \u03c4 is small, A F can also be dragged to zero, which is consistent with analysis in Sec. 3.2 (Obs#4 and Obs#5). On the other hand, B F always seems to vanish over time. In this numerical simulation, we set F = W p Wp following invariant in Theorem 1 with C = 0.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 9 .9Figure 9. The norm of the communicator [F, Wp] over time under different hyper-parameters (different time-independent \u03c4 and different weight decay \u03b7) in symmetrized dynamics Eqn. 67. When weight decay is small or zero, and/or \u03c4 is large, the norm of the communicator [F, Wp] F can shoot up (no eigenspace alignment).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "p * j\u2212 with respect to \u03b7 and \u03c4 . Note that", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "\u03a3 d [\u03a3 s ] \u22121 control the non-collapsed solutions. Here we make this result more concrete by giving illustrative examples of data distributions and data augmentation procedures, and the resulting properties of \u03a3 d [\u03a3 s ] \u22121 .", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 10 .10Figure 10. A visualization of BYOL dynamics in low dimensions. Left: Black arrows denote the vector field of the flow in the w and wp plane of online and predictor weights in Eqns. 104 and 105 when the target network weight wa is fixed to 1. For all 3 panels, \u03bbs = 1, \u03bb d = 1/2, and \u03b1p = \u03b2 = 1. All flow field vectors are normalized to unit length to indicate direction of flow alone. The red curve shows the hyperoblic manifold of stable fixed points wpw = wa\u03bb d \u03bb \u22121s , while the red point at the origin is an unstable fixed point. For a fixed target network, the online and predictor weights will cooperatively amplify each other to escape the collapsed solution at the origin. Middle: A visualization of the full low dimensional BYOL dynamics in Eqns 104-106 when the online and predictor weights are tied so that w = wp. The green curve shows the nullcline wa = w corresponding to dwa dt = 0 and the blue curve shows part of the nullcline dw dt = 0 corresponding to w 2 = wa\u03bb d \u03bb \u22121 s . The intersection of these two nullclines yields two fixed points (red dots): an unstable collapsed solution at the origin w = wa = 0, and a stable non-collapsed solution with wa = w and w = \u03bb d \u03bb \u22121 s . Right: A visualization of dynamics in Eqns 104-106 when the the predictor is removed, so that w2 is fixed to 1. The resulting two dimensional flow field on w and wa is shown (black arrows). The green curve shows the nullcline w = wa corresponding to dwa dt = 0, while the blue curve shows the nullcline w = wa\u03bb d \u03bb \u22121 s . The slope of this nullcline is \u03bbs\u03bb \u22121", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "structure of this initial condition will remain the same, with only the scalars w \u03b1 , w \u03b1 a and w \u03b1 p changing over time. Moreover, the scalars decouple across the different indices \u03b1, and the dynamics are driven by the eigenvalues \u03bb \u03b1 s and \u03bb \u03b1 d of \u03a3 s and \u03a3 d respectively. Inserting this special class of initial conditions into the dynamics in equation 100 to equation 102, and dropping the \u03b1 index, we find the dynamics of the triplet of scalars is given by dw p dt = \u03b1 p [w a \u03bb d \u2212 w p w\u03bb s ] w (104) dw dt = w p [w a \u03bb d \u2212 w p w\u03bb s ](105)dw a dt = \u03b2(\u2212w a + w).", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "EMA 40.67\u00b10.50 35.29\u00b12.49 34.60\u00b10.98 35.63\u00b12.66 no EMA 39.45\u00b11.26 34.01\u00b11.54 34.58\u00b12.93 32.22\u00b12.94", "figure_data": "Plug-in frequency (every N minibatches)1235Table"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "). Completely independent of EMA + no-bias EMA + bias no EMA + no-bias no EMA + bias 70.62\u00b11.05 70.99\u00b11.01 71.36\u00b10.44 71.37\u00b10.77 Table", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "EMA 75.09\u00b10.48 74.51\u00b10.47 74.52\u00b10.29 74.16\u00b10.33 no EMA 36.62\u00b11.85 72.85\u00b10.16 36.04\u00b12.74 72.13\u00b10.53 Two-layer predictor with BatchNorm and ReLU EMA 71.58\u00b16.46 78.85\u00b10.25 77.64\u00b10.41 78.53\u00b10.34 no EMA 35.59\u00b12.10 65.98\u00b10.71 41.92\u00b14.25 65.59\u00b10.66 The effect of symmetrization of Wp on downstream classification task (BYOL Top-1 on STL-10). Symmetric Wp leads to slightly better performance compared to regular Wp in the presence of EMA. On the other hand, without EMA, symmetric Wp crashes. Same effects happen in two-layer predictor with Batch-Norm and ReLU as well. Weight decay\u03b7 = 0.0004 and \u03b1p = 1.", "figure_data": "No predictor biasWith predictor biassym Wpregular Wpsym Wpregular WpOne-layer linear predictor"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Wp regular Wp sym Wp regular Wp Weight decay only for predictor (\u03b7p = 0.0004 and\u03b7s = 0) EMA 71.91\u00b10.70 70.54\u00b10.93 73.67\u00b10.47 70.89\u00b10.98 no EMA 71.12\u00b10.71 71.34\u00b10.63 73.01\u00b10.37 71.70\u00b10.83 No weight decay for all (\u03b7p =\u03b7s = 0) EMA 71.76\u00b10.28 70.62\u00b11.05 71.86\u00b10.39 70.99\u00b11.01 no EMA 43.04\u00b12.32 71.36\u00b10.44 41.36\u00b13.33 71.37\u00b10.77", "figure_data": "No predictor biasWith predictor biassym Table"}, {"figure_label": "67", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "STL-10 Top-1 after BYOL training for 100 epochs, if we use DirectPred (Eqn. 18). It outperforms training Wp using gradient descent (74.51% in Tbl. 3, regular Wp with EMA). EMA is used in all experiments. No predictor bias. \u03c1 defined in Eqn. 19. 57\u00b118.43 65.31\u00b118.22 77.11\u00b10.66 76.46\u00b10.55 freq=2 75.01\u00b10.48 75.10\u00b10.35 76.83\u00b10.52 76.31\u00b10.27 STL-10 Top-1 Accuracy after BYOL training for 100 epochs. With different cj. \u03c1 = 0.3 and = 0. EMA is used in all experiments. No predictor bias.", "figure_data": "Initial constant cj0.10.05\u22120.05\u22120.1freq=1 46."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Feature-dependent W p . Note one of the advantages of DirectPred 77.86\u00b10.16 78.77\u00b10.97 78.86\u00b11.15 DirectPred (freq=5) 77.54\u00b10.11 79.90\u00b10.66 80.28\u00b10.62 SGD baseline 75.06\u00b10.52 75.25\u00b10.74 75.25\u00b10.74 CIFAR-10 DirectPred 85.21\u00b10.23 88.88\u00b10.15 89.52\u00b10.04 DirectPred (freq=5) 84.93\u00b10.29 88.83\u00b10.10 89.56\u00b10.13 SGD baseline 84.49\u00b10.20 88.57\u00b10.15 89.33\u00b10.27", "figure_data": "Number of epochs100300500STL-10Table 8. STL-10/CIFAR-10 Top-1 accuracy of DirectPred, aftertraining for longer epochs. \u03c1 = 0.3, = 0.1 with EMA."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "2-layer predictor is BYOL default setting.", "figure_data": "BYOL variantsAccuracy (60 ep) Top-1 Top-5Accuracy (300 ep) Top-1 Top-52-layer predictor * 64.785.872.590.8linear predictor59.482.369.989.6DirectPred64.485.872.491.0The second setting follows BYOL moreclosely, where we use a symmetrized loss, 4096 batch sizeand LARS optimizer (You et al., 2017), and train for 300epochs.The results are summarized in Tbl. 9. Both settings ex-hibit similar behaviors in comparison, and we take the 300-epoch results as our highlights in the following. As a base-line, the default 2-layer predictor from BYOL (with Batch-Norm and ReLU, 4096 hidden dimension, 256 input/outputdimension) achieves 72.5% top-1 accuracy, and 90.8% top-5 accuracy with 300-epoch pre-training. This reproducesthe accuracy reported in BYOL (Grill et al., 2020). We"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "ImageNet experiments comparing DirectPred with BYOL", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Saxe, A. M., McClelland, J. L., and Ganguli, S. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013. Saxe, A. M., McClelland, J. L., and Ganguli, S. A mathematical theory of semantic development in deep neural networks. Proc. Natl. Acad. Sci. U. S. A., 2019.", "figure_data": "Tian, Y. An analytical formula of population gradient fortwo-layered relu network and its applications in conver-gence and critical point analysis. In ICML, 2017.Tian, Y., Krishnan, D., and Isola, P. Contrastive multiviewcoding. arXiv preprint arXiv:1906.05849, 2019.Tosh, C., Krishnamurthy, A., and Hsu, D. Contrastivelearning, multi-view redundancy, and linear models.arXiv preprint arXiv:2008.10150, 2020.You, Y., Gitman, I., and Ginsburg, B. Large batchtraining of convolutional networks. arXiv preprintarXiv:1708.03888, 2017."}], "formulas": [{"formula_id": "formula_0", "formula_text": "WpE [f f ] = 1 2 (E [faf ] + E [f f a ]", "formula_coordinates": [2.0, 351.03, 160.46, 138.22, 11.96]}, {"formula_id": "formula_1", "formula_text": "J(W, W p ) := 1 2 E x1,x2 W p f 1 \u2212 StopGrad(f 2a ) 2 2 ,", "formula_coordinates": [3.0, 61.47, 190.59, 221.95, 22.31]}, {"formula_id": "formula_2", "formula_text": "W p = \u03b1 p (\u2212W p W (X + X ) + W a X) W \u2212 \u03b7W p (2) W = W p (\u2212W p W (X + X ) + W a X) \u2212 \u03b7W (3) W a = \u03b2(\u2212W a + W ) (4) Here, X := E [xx ] wherex(x) := E x \u223cpaug(\u2022|x) [x ]", "formula_coordinates": [3.0, 55.44, 327.05, 423.94, 66.23]}, {"formula_id": "formula_3", "formula_text": "E x V x |x [x ] is the covariance matrix V x |x [x ]", "formula_coordinates": [3.0, 55.44, 407.23, 201.63, 9.96]}, {"formula_id": "formula_4", "formula_text": "W (t)W (t) = \u03b1 \u22121 p W p (t)W p (t) + e \u22122\u03b7t C, (5", "formula_coordinates": [3.0, 335.89, 194.67, 201.68, 12.69]}, {"formula_id": "formula_5", "formula_text": ")", "formula_coordinates": [3.0, 537.57, 197.06, 3.87, 8.64]}, {"formula_id": "formula_6", "formula_text": "d dt vec(W ) = \u2212H(t)vec(W ). (6", "formula_coordinates": [3.0, 365.38, 511.29, 172.19, 22.31]}, {"formula_id": "formula_7", "formula_text": ")", "formula_coordinates": [3.0, 537.57, 518.35, 3.87, 8.64]}, {"formula_id": "formula_8", "formula_text": "If the minimal eigenvalue \u03bb min (H(t)) over time is bounded below, inf t\u22650 \u03bb min (H(t)) \u2265 \u03bb 0 > 0, then W (t) \u2192 0.", "formula_coordinates": [3.0, 307.44, 541.49, 234.0, 21.61]}, {"formula_id": "formula_9", "formula_text": "W p = \u2212 \u03b1 p 2 (1 + \u03c3 2 ){W p , F } + \u03b1 p \u03c4 F \u2212 \u03b7W p (7) F = \u2212(1 + \u03c3 2 ){W 2 p , F } + \u03c4 {W p , F } \u2212 2\u03b7F", "formula_coordinates": [5.0, 69.23, 417.84, 404.84, 37.35]}, {"formula_id": "formula_10", "formula_text": "d dt [F, W p ] = \u2212[F, W p ]K \u2212 K[F, W p ](8)", "formula_coordinates": [5.0, 96.11, 548.6, 193.33, 22.31]}, {"formula_id": "formula_11", "formula_text": "K(t) = (1+\u03c3 2 ) \u03b1 p 2 F (t) + W 2 p (t) \u2212 \u03c4 1 + \u03c3 2 W p (t) + 3 2 \u03b7I (9) If inf t\u22650 \u03bb min [K(t)] = \u03bb 0 > 0, then the commutator [F (t), W p (t)] F \u2264 e \u22122\u03bb0t [F (0), W p (0)] F \u2192 0 (10)", "formula_coordinates": [5.0, 55.44, 601.68, 240.68, 68.96]}, {"formula_id": "formula_12", "formula_text": "p = U \u039b Wp U where \u039b Wp = diag[p 1 , p 2 , . . . , p d ], F = U \u039b F U where \u039b F = diag[s 1 , s 2 , . . . , s d ].", "formula_coordinates": [5.0, 307.44, 135.97, 234.0, 33.56]}, {"formula_id": "formula_13", "formula_text": "p j = \u03b1 p s j \u03c4 \u2212 (1 + \u03c3 2 )p j \u2212 \u03b7p j (11", "formula_coordinates": [5.0, 346.91, 192.52, 307.93, 11.72]}, {"formula_id": "formula_14", "formula_text": ")", "formula_coordinates": [5.0, 654.83, 194.91, 43.33, 8.64]}, {"formula_id": "formula_15", "formula_text": "s j = 2p j s j \u03c4 \u2212 (1 + \u03c3 2 )p j \u2212 2\u03b7s j (12) s j\u03c4 = \u03b2(1 \u2212 \u03c4 )s j \u2212 \u03c4\u1e61 j /2. (13", "formula_coordinates": [5.0, 341.76, 208.59, 199.68, 26.67]}, {"formula_id": "formula_16", "formula_text": ")", "formula_coordinates": [5.0, 537.29, 225.93, 4.15, 8.64]}, {"formula_id": "formula_17", "formula_text": "s j (t) = \u03b1 \u22121 p p 2 j (t) + e \u22122\u03b7t c j (14", "formula_coordinates": [5.0, 367.03, 659.52, 170.26, 12.69]}, {"formula_id": "formula_18", "formula_text": ")", "formula_coordinates": [5.0, 537.29, 661.92, 4.15, 8.64]}, {"formula_id": "formula_19", "formula_text": "!\" * !$ * (b) (c) < % 4(1 + % ) > % 4(1 + % ) !$ * (a) = 0 O O O Saddle Point Figure 4. Fixed point of\u1e57j = \u2212pj(pj \u2212 p * j\u2212 )(pj \u2212 p * j+ ) (see", "formula_coordinates": [6.0, 307.44, 149.69, 234.0, 64.77]}, {"formula_id": "formula_20", "formula_text": "\u03c4 (t) = p \u22121 j (t)\u03b2e \u2212\u03b2t t 0 p j (t )e \u03b2t dt,(15)", "formula_coordinates": [6.0, 349.27, 464.19, 192.17, 26.29]}, {"formula_id": "formula_21", "formula_text": "p j = p 2 j \u03c4 (t) \u2212 (1 + \u03c3 2 )p j \u2212 \u03b7p j . (16", "formula_coordinates": [6.0, 351.92, 532.13, 185.37, 12.69]}, {"formula_id": "formula_22", "formula_text": ")", "formula_coordinates": [6.0, 537.29, 534.52, 4.15, 8.64]}, {"formula_id": "formula_23", "formula_text": "p * j\u00b1 = \u03c4 \u00b1 \u03c4 2 \u2212 4\u03b7(1 + \u03c3 2 ) 2(1 + \u03c3 2 ) > 0, p * j0 = 0", "formula_coordinates": [6.0, 331.32, 602.64, 186.25, 23.33]}, {"formula_id": "formula_24", "formula_text": "\u2206 j < 1 2 \u03b1 p (1 + \u03c3 2 )s j + \u03b7 . (17", "formula_coordinates": [7.0, 112.29, 246.6, 173.0, 22.31]}, {"formula_id": "formula_25", "formula_text": ")", "formula_coordinates": [7.0, 285.29, 253.66, 4.15, 8.64]}, {"formula_id": "formula_26", "formula_text": "\u039b F = diag[s 1 , s 2 , . . . , s d ], and sets W p via p j = \u221a s j + max j s j , W p =\u00db diag[p j ]\u00db . (18", "formula_coordinates": [7.0, 307.44, 578.69, 229.85, 34.75]}, {"formula_id": "formula_27", "formula_text": ")", "formula_coordinates": [7.0, 537.29, 599.33, 4.15, 8.64]}, {"formula_id": "formula_28", "formula_text": "F = \u03c1F + (1 \u2212 \u03c1)E B [f f ] (19", "formula_coordinates": [7.0, 366.71, 689.24, 170.59, 9.68]}, {"formula_id": "formula_29", "formula_text": ")", "formula_coordinates": [7.0, 537.29, 689.59, 4.15, 8.64]}, {"formula_id": "formula_30", "formula_text": "J(W, W p ) := 1 2 E x\u223cp(\u2022), x1,x2\u223cpaug(\u2022|x) W p f 1 \u2212 StopGrad(f 2a ) 2 2 (20", "formula_coordinates": [12.0, 155.2, 216.68, 382.09, 22.31]}, {"formula_id": "formula_31", "formula_text": ")", "formula_coordinates": [12.0, 537.29, 223.74, 4.15, 8.64]}, {"formula_id": "formula_32", "formula_text": "Let X = E [xx ] wherex(x) := E x \u223cpaug(\u2022|x) [x ]", "formula_coordinates": [12.0, 55.44, 254.86, 223.55, 9.96]}, {"formula_id": "formula_33", "formula_text": "E x V x |x [x ] is the covariance matrix V x |x [x ]", "formula_coordinates": [12.0, 55.44, 268.09, 201.81, 9.96]}, {"formula_id": "formula_34", "formula_text": "W p = \u2212 \u2202J \u2202W p = \u2212W p W (X + X )W + W a XW (21) W = \u2212 \u2202J \u2202W = \u2212W p W p W (X + X ) + W p W a X (22) Proof. Note that (W p f 1 \u2212 f 2a ) (W p f 1 \u2212 f 2a ) (23) = f 1 W p W p f 1 \u2212 f 2a W p f 1 \u2212 f 1 W p f 2a + f 2a f 2a (24) = tr(W p W p f 1 f 1 ) \u2212 tr(W p f 1 f 2a ) \u2212 tr(W p f 2a f 1 ) + tr(f 2a f 2a ) (25) Let F 1 = E [f 1 f 1 ] = W (X + X )W where X = E x [xx ] and X = E x V x |x [x ] , F 1,2a = E [f 1 f 2a ], F 2a,1 = E [f 2a f 1 ] = F 1,2a and F 2a = E [f 2a f 2a ]", "formula_coordinates": [12.0, 55.44, 300.16, 798.3, 160.45]}, {"formula_id": "formula_35", "formula_text": "J(W, W p ) = 1 2 tr(W p W p F 1 ) \u2212 tr(W p F 1,2a ) \u2212 tr(F 1,2a W p ) + tr(F 2a )(26)", "formula_coordinates": [12.0, 141.56, 471.38, 399.88, 22.31]}, {"formula_id": "formula_36", "formula_text": "W p = \u2212 \u2202J \u2202W p = \u2212W p F 1 + F 1,2a(27)", "formula_coordinates": [12.0, 230.15, 529.79, 311.29, 23.23]}, {"formula_id": "formula_37", "formula_text": "\u2202J \u2202F 1 = 1 2 W p W p (28", "formula_coordinates": [12.0, 262.8, 609.09, 274.49, 23.23]}, {"formula_id": "formula_38", "formula_text": ")", "formula_coordinates": [12.0, 537.29, 616.15, 4.15, 8.64]}, {"formula_id": "formula_39", "formula_text": "\u2202J \u2202F 1,2a = \u2212W p (29", "formula_coordinates": [12.0, 252.5, 635.07, 284.8, 23.22]}, {"formula_id": "formula_40", "formula_text": ")", "formula_coordinates": [12.0, 537.29, 642.13, 4.15, 8.64]}, {"formula_id": "formula_41", "formula_text": "\u2202J \u2202w ij = kl \u2202J \u2202F 1 kl \u2202[F 1 ] kl \u2202w ij + kl \u2202J \u2202F 1,2a kl \u2202[F 1,2a ] kl \u2202w ij (30", "formula_coordinates": [12.0, 178.5, 693.52, 358.79, 26.88]}, {"formula_id": "formula_42", "formula_text": ")", "formula_coordinates": [12.0, 537.29, 700.58, 4.15, 8.64]}, {"formula_id": "formula_43", "formula_text": "kl \u2202J \u2202F 1 kl \u2202[F 1 ] kl \u2202w ij = kl \u2202J \u2202F 1 kl mn \u2202w km c mn w ln \u2202w ij (31) = kl \u2202J \u2202F 1 kl \u03b4(i = k) n c jn w ln + \u03b4(i = l) m w km c mj (32) = l \u2202J \u2202F 1 il n c jn w ln + k \u2202J \u2202F 1 ki m w km c mj (33) = \u2202J \u2202F 1 W C + \u2202J \u2202F 1 W C ij (34", "formula_coordinates": [13.0, 169.43, 89.24, 372.01, 122.1]}, {"formula_id": "formula_44", "formula_text": ")", "formula_coordinates": [13.0, 537.29, 193.52, 4.15, 8.64]}, {"formula_id": "formula_45", "formula_text": "kl \u2202J \u2202F 1,2a kl \u2202[F 1,2a ] kl \u2202w ij = \u2202J \u2202F 1,2a W a X ij (35", "formula_coordinates": [13.0, 194.35, 241.04, 342.94, 26.88]}, {"formula_id": "formula_46", "formula_text": ")", "formula_coordinates": [13.0, 537.29, 248.1, 4.15, 8.64]}, {"formula_id": "formula_47", "formula_text": "So we have:\u1e86 = \u2212 \u2202J \u2202W = \u2212W p W p W (X + X ) + W p W a X (36", "formula_coordinates": [13.0, 55.44, 283.76, 481.85, 30.05]}, {"formula_id": "formula_48", "formula_text": ")", "formula_coordinates": [13.0, 537.29, 298.56, 4.15, 8.64]}, {"formula_id": "formula_49", "formula_text": "W p = [\u2212W p W (X + X ) + W a X]W (37) W = W p [\u2212W p W (X + X ) + W a X](38)", "formula_coordinates": [13.0, 212.71, 345.06, 614.26, 26.63]}, {"formula_id": "formula_50", "formula_text": "J(W, W p ) := 1 4 E x\u223cp(\u2022), x1,x2\u223cpaug(\u2022|x) W p f 1 \u2212 StopGrad(f 2a ) 2 2 + W p f 2 \u2212 StopGrad(f 1a ) 2 2 (39)", "formula_coordinates": [13.0, 91.74, 425.23, 449.71, 22.31]}, {"formula_id": "formula_51", "formula_text": "F 2 = E [f 2 f 2 ]): W p = \u2212 \u2202J \u2202W p = \u2212 1 2 W p (F 1 + F 2 ) + 1 2 (F 2a,1 + F 1a,2 )(40)", "formula_coordinates": [13.0, 186.82, 456.45, 527.73, 42.94]}, {"formula_id": "formula_52", "formula_text": "W p = [\u2212W p W (X + X ) + W a X]W \u2212 \u03b7W p (41) W = W p [\u2212W p W (X + X ) + W a X] \u2212 \u03b7W (42)", "formula_coordinates": [13.0, 196.96, 560.05, 645.75, 26.63]}, {"formula_id": "formula_53", "formula_text": "W (t)W (t) = W p (t)W p (t) + e \u22122\u03b7t C(43)", "formula_coordinates": [13.0, 219.55, 627.68, 321.89, 12.69]}, {"formula_id": "formula_54", "formula_text": "\u03b1 \u22121 p W p\u1e86p + \u03b1 \u22121 p \u03b7W p W p =\u1e86 W + \u03b7W W (44)", "formula_coordinates": [13.0, 200.8, 673.36, 340.64, 12.69]}, {"formula_id": "formula_55", "formula_text": "\u03b1 \u22121 p\u1e86 p W p + \u03b1 \u22121 p \u03b7W p W p = W\u1e86 + \u03b7W W (45)", "formula_coordinates": [13.0, 200.8, 706.18, 340.64, 12.69]}, {"formula_id": "formula_56", "formula_text": "\u03b1 \u22121 p d dt (e 2\u03b7t W p W p ) = d dt (e 2\u03b7t W W ) (46)", "formula_coordinates": [14.0, 219.3, 90.18, 322.15, 22.31]}, {"formula_id": "formula_57", "formula_text": "e 2\u03b7t W W = \u03b1 \u22121 p e 2\u03b7t W p W p + C, or W W = \u03b1 \u22121 p W p W p + e \u22122\u03b7t C", "formula_coordinates": [14.0, 108.85, 121.17, 282.36, 12.19]}, {"formula_id": "formula_58", "formula_text": "dw(t) dt = \u2212H(t)w(t)(47)", "formula_coordinates": [14.0, 255.03, 174.76, 286.41, 22.31]}, {"formula_id": "formula_59", "formula_text": "dV dt = dV dw dw dt = \u2212w (t)H(t)w(t)(48)", "formula_coordinates": [14.0, 225.39, 253.82, 316.05, 22.31]}, {"formula_id": "formula_60", "formula_text": "H(t) = j \u03bb j (t)u j (t)u j (t) with all \u03bb j (t) \u2265 \u03bb 0 and [u 1 (t), u 2 (t), . . . , u d (t)]", "formula_coordinates": [14.0, 223.56, 286.0, 317.89, 11.18]}, {"formula_id": "formula_61", "formula_text": "w Hw = j \u03bb j w u j u j w \u2265 \u03bb 0 w \uf8ee \uf8f0 j u j u j \uf8f9 \uf8fb w = \u03bb 0 w 2 2 (49)", "formula_coordinates": [14.0, 166.86, 317.83, 374.58, 33.53]}, {"formula_id": "formula_62", "formula_text": "dV dt \u2264 \u2212\u03bb 0 w(t) 2 2 = \u22122\u03bb 0 V (50)", "formula_coordinates": [14.0, 236.67, 373.89, 304.77, 22.31]}, {"formula_id": "formula_63", "formula_text": "d dt vec(W ) = \u2212H(t)vec(W ). (51", "formula_coordinates": [14.0, 239.38, 468.37, 297.91, 22.31]}, {"formula_id": "formula_64", "formula_text": ")", "formula_coordinates": [14.0, 537.29, 475.43, 4.15, 8.64]}, {"formula_id": "formula_65", "formula_text": "If inf t\u22650 \u03bb min (H(t)) \u2265 \u03bb 0 > 0, then W (t) \u2192 0.", "formula_coordinates": [14.0, 55.44, 500.3, 195.33, 9.65]}, {"formula_id": "formula_66", "formula_text": "W = \u2212 \u2202J \u2202W = \u2212W p W p W (X + X ) + (W p + W p )W X \u2212 W (X + X ) \u2212 \u03b7W (52) = \u2212(W p W p + I)W X \u2212 (W p W p \u2212 W p \u2212 W p + I)W X \u2212 \u03b7W (53) = \u2212(W p W p + I)W X \u2212 (W p \u2212 I) (W p \u2212 I)W X \u2212 \u03b7W (54) = \u2212(W p W p + I)W X \u2212W pWp W X \u2212 \u03b7W (55)", "formula_coordinates": [14.0, 127.71, 557.81, 413.73, 68.09]}, {"formula_id": "formula_67", "formula_text": "d dt vec(W ) = \u2212 X \u2297 (W p W p + I) + X \u2297W pWp + \u03b7I n1n2 vec(W )(56)", "formula_coordinates": [14.0, 153.57, 657.41, 387.88, 22.31]}, {"formula_id": "formula_68", "formula_text": "W = \u2212(X + \u03b7I)W(57)", "formula_coordinates": [15.0, 256.02, 311.47, 285.42, 8.96]}, {"formula_id": "formula_69", "formula_text": "F =\u1e86 XW + W X\u1e86 = \u2212(1 + \u03c3 2 )(W p W p F + F W p W p ) + W p W a W + W W a W p(58)", "formula_coordinates": [15.0, 115.17, 398.53, 426.28, 12.69]}, {"formula_id": "formula_70", "formula_text": "F = \u2212(1 + \u03c3 2 )(W p W p F + F W p W p ) + W p W a W + W W a W p \u2212 2\u03b7F(59)", "formula_coordinates": [15.0, 148.4, 439.73, 393.04, 12.69]}, {"formula_id": "formula_71", "formula_text": "F = \u2212(1 + \u03c3 2 ){F, W p W p } + W p W a W + W W a W p \u2212 2\u03b7F(60)", "formula_coordinates": [15.0, 170.52, 480.92, 370.92, 12.69]}, {"formula_id": "formula_72", "formula_text": "\u1e86 p = \u2212\u03b1 p (1 + \u03c3 2 )W p F + \u03b1 p \u03c4 F \u2212 \u03b7W p(61)", "formula_coordinates": [15.0, 208.81, 503.91, 332.63, 22.35]}, {"formula_id": "formula_73", "formula_text": "W p = \u2212\u03b1 p (1 + \u03c3 2 )W p F + \u03b1 p \u03c4 F \u2212 \u03b7W p(62)", "formula_coordinates": [15.0, 151.82, 643.0, 733.7, 11.72]}, {"formula_id": "formula_74", "formula_text": "F = \u2212(1 + \u03c3 2 )(W p W p F + F W p W p ) + \u03c4 (W p F + F W p ) \u2212 2\u03b7F(63)", "formula_coordinates": [15.0, 158.05, 659.01, 383.39, 12.69]}, {"formula_id": "formula_75", "formula_text": "\u2212\u03b1 p (1 + \u03c3 2 )W p F + \u03b1 p \u03c4 F \u2212 \u03b7W p = 0(64)", "formula_coordinates": [15.0, 217.37, 706.18, 324.07, 11.72]}, {"formula_id": "formula_76", "formula_text": "(1 + \u03c3 2 )W p \u039b + \u03b7 W p = \u03c4 \u039b(65)", "formula_coordinates": [16.0, 240.27, 101.17, 301.17, 12.17]}, {"formula_id": "formula_77", "formula_text": "W p \u039b = \u03c4 \u039b(66)", "formula_coordinates": [16.0, 254.92, 137.47, 286.52, 21.29]}, {"formula_id": "formula_78", "formula_text": "= AB + BA):\u1e86 p = \u2212 \u03b1 p 2 (1 + \u03c3 2 ){W p , F } + \u03b1 p \u03c4 F \u2212 \u03b7W p (67", "formula_coordinates": [16.0, 93.39, 208.63, 669.41, 39.64]}, {"formula_id": "formula_79", "formula_text": ")", "formula_coordinates": [16.0, 762.8, 233.02, 79.32, 8.64]}, {"formula_id": "formula_80", "formula_text": "F = \u2212(1 + \u03c3 2 ){W 2 p , F } + \u03c4 {W p , F } \u2212 2\u03b7F", "formula_coordinates": [16.0, 201.46, 250.62, 198.81, 12.69]}, {"formula_id": "formula_81", "formula_text": "d dt [F, W p ] = \u2212[F, W p ]K \u2212 K[F, W p ](68)", "formula_coordinates": [16.0, 222.11, 343.84, 319.33, 22.31]}, {"formula_id": "formula_82", "formula_text": "K = K(t) = (1 + \u03c3 2 ) \u03b1 p 2 F (t) + W 2 p (t) \u2212 \u03c4 1 + \u03c3 2 W p (t) + 3 2 \u03b7I (69)", "formula_coordinates": [16.0, 162.05, 379.84, 379.39, 22.31]}, {"formula_id": "formula_83", "formula_text": "F\u1e86 p \u2212\u1e86 p F = \u2212 \u03b1 p 2 (1 + \u03c3 2 )(F L + LF ) \u2212 \u03b7L(70)", "formula_coordinates": [16.0, 198.97, 465.74, 342.47, 22.31]}, {"formula_id": "formula_84", "formula_text": "W p \u2212 W p\u1e1e = \u2212(1 + \u03c3 2 )(W 2 p L + LW 2 p ) + \u03c4 (W p L + LW p ) \u2212 2\u03b7L(71)", "formula_coordinates": [16.0, 163.9, 506.55, 377.54, 12.69]}, {"formula_id": "formula_85", "formula_text": "= F\u1e86 p +\u1e1e W p \u2212 (W p\u1e1e +\u1e86 p F ) = \u2212KL \u2212 LK(72)", "formula_coordinates": [16.0, 199.65, 538.26, 341.79, 9.65]}, {"formula_id": "formula_86", "formula_text": "K = K(t) = (1 + \u03c3 2 ) \u03b1 p 2 F + W 2 p \u2212 \u03c4 1 + \u03c3 2 W p + 3 2 \u03b7I (73", "formula_coordinates": [16.0, 179.07, 564.79, 358.22, 22.31]}, {"formula_id": "formula_87", "formula_text": ")", "formula_coordinates": [16.0, 537.29, 571.85, 4.15, 8.64]}, {"formula_id": "formula_88", "formula_text": "dvec(L(t)) dt = \u2212 [K(t) \u2295 K(t)] vec(L(t))(74)", "formula_coordinates": [16.0, 214.41, 615.71, 327.03, 22.31]}, {"formula_id": "formula_89", "formula_text": "vec(L) 2 \u2264 e \u22122\u03bb0t vec(L(0)) 2 \u2192 0(75)", "formula_coordinates": [16.0, 223.9, 684.67, 317.54, 11.72]}, {"formula_id": "formula_90", "formula_text": "W p = U \u039b Wp U where \u039b Wp = diag[p 1 , p 2 , . . . , p d ], F = U \u039b F U where \u039b F = diag[s 1 , s 2 , . . . , s d ].", "formula_coordinates": [17.0, 55.44, 482.76, 486.0, 21.61]}, {"formula_id": "formula_91", "formula_text": "M (t) = U (t)D(t)U (t). M follows\u1e40 = U (t)G(t)U (t)", "formula_coordinates": [17.0, 55.44, 554.49, 486.0, 20.91]}, {"formula_id": "formula_92", "formula_text": "M =U DU + U\u1e0aU + U DU = U GU (76", "formula_coordinates": [17.0, 209.33, 604.84, 327.96, 8.96]}, {"formula_id": "formula_93", "formula_text": ")", "formula_coordinates": [17.0, 537.29, 605.16, 4.15, 8.64]}, {"formula_id": "formula_94", "formula_text": "U U D + DU U = G \u2212\u1e0a (77)", "formula_coordinates": [17.0, 242.13, 634.74, 299.31, 11.47]}, {"formula_id": "formula_95", "formula_text": "QD \u2212 DQ = G \u2212\u1e0a (78", "formula_coordinates": [17.0, 255.04, 674.78, 282.25, 8.96]}, {"formula_id": "formula_96", "formula_text": ")", "formula_coordinates": [17.0, 537.29, 675.09, 4.15, 8.64]}, {"formula_id": "formula_97", "formula_text": "j = \u03b1 p (1 + \u03c3 2 )s j \u03c4 1 + \u03c3 2 \u2212 p j \u2212 \u03b7p j (79", "formula_coordinates": [18.0, 208.29, 442.67, 554.27, 22.31]}, {"formula_id": "formula_98", "formula_text": ")", "formula_coordinates": [18.0, 762.56, 449.73, 79.24, 8.64]}, {"formula_id": "formula_99", "formula_text": "s j = 2(1 + \u03c3 2 )p j s j \u03c4 1 + \u03c3 2 \u2212 p j \u2212 2\u03b7s j (80)", "formula_coordinates": [18.0, 203.62, 470.56, 337.82, 22.31]}, {"formula_id": "formula_100", "formula_text": "2\u03b1 \u22121 p p j\u1e57j \u2212\u1e61 j = \u22122\u03b7\u03b1 \u22121 p p 2 j + 2\u03b7s j (81) which gives \u03b1 \u22121 p dp 2 j dt + 2\u03b7p 2 j =\u1e61 j + 2\u03b7s j (82) \u03b1 \u22121 p d dt (e 2\u03b7t p 2 j ) = d dt (e 2\u03b7t s j ) (83) \u03b1 \u22121 p e 2\u03b7t p 2 j = e 2\u03b7t s j \u2212 c j (84) \u03b1 \u22121 p p 2 j (t) = s j (t) \u2212 e \u22122\u03b7t c j (85)", "formula_coordinates": [18.0, 55.44, 525.46, 486.0, 129.4]}, {"formula_id": "formula_101", "formula_text": "\u1e86 a = \u03b2(W \u2212 W a )(86)", "formula_coordinates": [18.0, 253.21, 696.62, 288.23, 21.29]}, {"formula_id": "formula_102", "formula_text": "\u03c4 W + \u03c4\u1e86 = \u03b2(1 \u2212 \u03c4 )W (87) \u03c4 W W + \u03c4\u1e86 W = \u03b2(1 \u2212 \u03c4 )W W (88) 2\u03c4 F + \u03c4\u1e1e = 2\u03b2(1 \u2212 \u03c4 )F (89)", "formula_coordinates": [19.0, 214.87, 88.71, 615.55, 38.84]}, {"formula_id": "formula_103", "formula_text": "F = \u2212(1 + \u03c3 2 ){W 2 p , F } + \u03c4 {W p , F } \u2212 2\u03b7F(90)", "formula_coordinates": [19.0, 205.54, 154.58, 335.9, 12.69]}, {"formula_id": "formula_104", "formula_text": "2\u03c4 s j + \u03c4\u1e61 j = 2\u03b2(1 \u2212 \u03c4 )s j (91) or\u03c4 = \u03b2(1 \u2212 \u03c4 ) \u2212 \u03c4\u1e61 j 2s j (92", "formula_coordinates": [19.0, 55.44, 193.64, 486.0, 49.48]}, {"formula_id": "formula_105", "formula_text": ")", "formula_coordinates": [19.0, 537.29, 226.95, 4.15, 8.64]}, {"formula_id": "formula_106", "formula_text": "= \u03b2(1 \u2212 \u03c4 ) \u2212 \u03c4\u1e57 j p j (93", "formula_coordinates": [19.0, 264.4, 272.63, 272.89, 19.69]}, {"formula_id": "formula_107", "formula_text": ") or\u03c4 + \u03c4 \u1e57 j p j + \u03b2 = \u03b2 (94) or d dt (e f (t) \u03c4 ) = \u03b2e f (t)(95)", "formula_coordinates": [19.0, 55.44, 276.15, 486.0, 83.85]}, {"formula_id": "formula_108", "formula_text": "e \u03b2t p j \u03c4 = \u03b2 t 0 e \u03b2t p j (t )dt (96", "formula_coordinates": [19.0, 242.36, 386.08, 294.93, 26.29]}, {"formula_id": "formula_109", "formula_text": ")", "formula_coordinates": [19.0, 537.29, 395.46, 4.15, 8.64]}, {"formula_id": "formula_110", "formula_text": "\u03c4 j (t) = p \u22121 j (t)\u03b2e \u2212\u03b2t", "formula_coordinates": [19.0, 223.12, 430.23, 85.15, 13.15]}, {"formula_id": "formula_111", "formula_text": "p * j\u2212 = \u03c4 \u2212 \u03c4 2 \u2212 4\u03b7(1 + \u03c3 2 ) 2(1 + \u03c3 2 ) (98", "formula_coordinates": [19.0, 235.15, 501.18, 302.14, 23.33]}, {"formula_id": "formula_112", "formula_text": ")", "formula_coordinates": [19.0, 537.29, 509.26, 4.15, 8.64]}, {"formula_id": "formula_113", "formula_text": "dg dx = 1 \u2212 1 1 \u2212 c/x 2 < 0 (99) So g(x)", "formula_coordinates": [19.0, 55.44, 591.45, 486.0, 41.96]}, {"formula_id": "formula_114", "formula_text": "X = E [xx ] andx(x) := E x \u223cpaug(\u2022|x) [x ]", "formula_coordinates": [20.0, 336.9, 173.28, 180.55, 9.96]}, {"formula_id": "formula_115", "formula_text": "X = E x V x |x [x ] is the covariance matrix V x |x [x ]", "formula_coordinates": [20.0, 259.5, 186.51, 224.63, 9.96]}, {"formula_id": "formula_116", "formula_text": "W = W p \u2212W p W \u03a3 s + W a \u03a3 d (100) W p = \u03b1 p \u2212W p W \u03a3 s + W a \u03a3 d W (101) W a = \u03b2(\u2212W a + W )(102)", "formula_coordinates": [20.0, 223.67, 247.08, 585.6, 45.34]}, {"formula_id": "formula_117", "formula_text": "W p W = W \u03a3 d [\u03a3 s ] \u22121 .(103)", "formula_coordinates": [20.0, 252.85, 491.61, 288.59, 11.72]}, {"formula_id": "formula_118", "formula_text": "\u03a3 d [\u03a3 s ] \u22121 = P c \u03a3 x P c [\u03a3 x ] \u22121 .", "formula_coordinates": [21.0, 164.62, 241.99, 118.39, 11.23]}, {"formula_id": "formula_119", "formula_text": "\u03a3 s = \u03a3 x + \u03a3 n while \u03a3 d = \u03a3 x .", "formula_coordinates": [21.0, 413.17, 320.23, 128.27, 10.53]}, {"formula_id": "formula_120", "formula_text": "d [\u03a3 s ] \u22121 = \u03a3 x [\u03a3 x + \u03a3 n ] \u22121 .", "formula_coordinates": [21.0, 304.68, 332.19, 117.24, 10.53]}], "doi": ""}