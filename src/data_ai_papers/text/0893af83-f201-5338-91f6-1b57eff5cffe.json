{"title": "Variational Stereovision and 3D Scene Flow Estimation with Statistical Similarity Measures", "authors": "J.-P Pons; R Keriven; O Faugeras; G Hermosillo", "pub_date": "", "abstract": "We present a common variational framework for dense depth recovery and dense three-dimensional motion field estimation from multiple video sequences, which is robust to camera spectral sensitivity differences and illumination changes. For this purpose, we first show that both problems reduce to a generic image matching problem after backprojecting the input images onto suitable surfaces. We then solve this matching problem in the case of statistical similarity criteria that can handle frequently occurring nonaffine image intensities dependencies. Our method leads to an efficient and elegant implementation based on fast recursive filters. We obtain good results on real images.", "sections": [{"heading": "Introduction", "text": "The correspondence problem is the core problem of both structure and motion estimation. To solve this highly ambiguous problem, most methods compare image intensities by their difference, relying on very strong assumptions, such as the lambertian assumption for the stereo problem or the brightness constancy assumption for optical flow.\nCorrelation techniques can cope with affine changes of image intensities. They have been successfully used both for the stereo problem and in optical flow block matching algorithms. However, these techniques often use a fixed neighborhood, whereas a surface patch of the scene may have different shapes in different cameras and over time. In the stereo problem, the underlying hypothesis is that the camera retinal planes are identical and that the scene is made of fronto parallel planes. In some works, this limitation is alleviated by taking into account the tangent plane to the object [5] or by using adaptative windows [9,17].\nIn this paper, in order to avoid projective distorsion, we map depth recovery and three-dimensional motion estimation from multiple calibrated video sequences to a generic image matching problem by backprojecting the input images onto suitable surfaces. This way, no shape approximation such as the tangent plane approximation is needed. Our matching window is not a hard window in the input images as in standard correlation techniques, but a smooth Gaussian window that operates along the objects' surfaces inside the backprojected volume images.\nMoreover, in order to cope with non-affine intensity dependencies we use statistical similarity criteria which have already proven successful in multimodal image registration [22,10,16,7].\nWe have designed a theoretical and computational framework for both problems: we minimize an energy functional. Furthermore, we have proved the well-posedness of the minimization process in both cases.\nThree-dimensional structure and motion estimation from multiple video sequences has long been limited to rigid or piecewise-rigid scenes [23,4,19] or parametric models [11,24]. The problem of computing a dense non-rigid 3D motion field, namely scene flow [20], from multiple video sequences has been addressed only recently.\nSome techniques [25,3,12] use the spatio-temporal derivatives of the input images. As pointed out in [20], estimating scene flow from these derivatives without regularization is an ill-posed problem. Indeed, the associated normal flow equations only constrain the scene flow vector to lie on a line parallel to the iso-brightness contour on the object. This is nothing but a 3D version of the aperture problem for optical flow [2]. In [3,12], several samples of the spatio-temporal derivatives are combined in order to overconstrain scene flow, whereas in [25], the aperture problem is solved by combining the normal flow constraint with a Tikhonov smoothness term. However, due to the underlying brightness constancy assumption, and to the local relevance of spatio-temporal derivatives, differential methods apply mainly to slowly-moving lambertian scenes under constant illumination.\nIn [21], shape and scene flow are estimated simultane-ously using a plane-sweep carving algorithm in a 6D space. However, this approach still relies on a brightness constancy assumption, has a very high computational and memory cost, and is unable to enforce the smoothness of the recovered shape and motion. Some other techniques [18,20,25,6] rely on previous optical flow computations. However, the latter may be noisy and/or physically inconsistent through cameras. The heuristic spatial smoothness constraints used in most optical flow methods may also alter the recovered scene flow.\nOur method for scene flow estimation neither needs previous optical flow computations nor makes use of ambiguous spatio-temporal image derivatives. It proceeds by directly evolving a 3D vector field so as to fit to the intensity changes in all cameras. It is robust to illumination changes through the use of statistical similarity criteria. Moreover, it can recover large displacements thanks to a multi-resolution coarse-to-fine strategy.\nIn the sequel, we focus on the case of two cameras not to overload notations. Our framework extends easily to the AE-camera case simply by summing the statistical criteria over all pairs of cameras. The rest of this paper is organized as follows. Section 2 defines the statistical similiarity criteria to be used in subsequent sections. In Section 3, we present our stereovision method. Section 4 describes our novel method for scene flow estimation.", "publication_ref": ["b4", "b8", "b16", "b21", "b9", "b15", "b6", "b22", "b3", "b18", "b10", "b23", "b19", "b24", "b2", "b11", "b19", "b1", "b2", "b11", "b24", "b20", "b17", "b19", "b24", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Statistical intensity similarity criteria", "text": "We consider two similarity criteria which assume different relations between the image intensities. The cross correlation (CC) is a measure of the affine dependency. The mutual information (MI) [22,10] measures how the intensity distributions of two images fail to be independent. Our two criteria can also take two different forms. A global form computed for the entire image, and a local form computed on corresponding regions. The latter can cope with nonstationary joint probability distributions of the intensities.\nThe ", "publication_ref": ["b21", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Variational stereovision with statistical measures", "text": "Our approach proceeds by deforming a surface so as to match the backprojected images of the different cameras. The matching criterion is one of the statistical measures defined in Section 2. The surface deformation is driven by the minimization of an energy functional through a gradient descent method.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Notations", "text": "We model the objects of the scene as the graph, in a cartesian setting, of an unknown function defined over a bounded domain \u00aa of \u00ca \u00be with smooth boundary \u00aa, and belonging to a dense linear subspace \u00bd of the Hilbert space \u00c0 \u00bd \u00c4 \u00be\u00b4\u00aa \u00ca\u00b5.\nWe note \u00c1 the image captured by camera , and \u00c1 the backprojection of \u00c1 onto the entire 3D space. That is, \u00c1 \u00b4\u00dc \u00dd \u00de\u00b5 is the intensity of the pixel obtained by projecting the 3D point\u00b4\u00dc \u00dd \u00de\u00b5 onto image . Thus, the camera geometry is encapsulated in function \u00c1 . The gradient of \u00c1 can be readily obtained from the gradient of image \u00c1 and the coefficients of the perspective projection matrix of camera . We note \u00c1 the pair\u00b4\u00c1 \u00bd \u00c1 \u00be \u00b5. We denote by \u00cb the parameterized surface\u00b4\u00dc \u00dd\u00b5 \u00b4\u00dc \u00dd \u00b4\u00dc \u00dd\u00b5\u00b5, so that the backprojection of image onto the surface is given by \u00c1 AE \u00cb. We note \u00c5 the 3D point\u00b4\u00dc \u00dd \u00b4\u00dc \u00dd\u00b5\u00b5.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Variational formulation", "text": "We define the stereo problem as the minimization of a cost functional \u00bd\u00b4 \u00b5 \u00c5 \u00bd\u00b4 \u00b5 \u2022 \u00ca \u00bd\u00b4 \u00b5 where \u00c5 \u00bd\u00b4 \u00b5 measures the statistical dissimilarity of the backprojected images \u00c1 \u00bd AE \u00cb and \u00c1 \u00be AE \u00cb , while \u00ca \u00bd\u00b4 \u00b5 defines regularizing constraints on .\nNote that, thanks to the backprojecting step, our method matches against intensities along the objects' surfaces in constrast with standard correlation techniques which rely on an approximation of the objects' shapes. Moreover, our local similarity measures operate on smooth Gaussian windows in the backprojected volume images, in contrast with the popular rigid rectangular windows in the input images.\nA \n\u00b4\u00bc\u00b5 \u00bc \u00be \u00c0 \u00bd \u00d6 \u00c0\u00bd \u00bd\u00b4 \u00b4 \u00b5\u00b5 \u00bc (1)\nWe call a global classical solution of equation ( 1) a function\n\u00be \u00bc\u00b4 \u00bc \u2022\u00bd \u00c0 \u00bd \u00b5 \u00bd\u00b4 \u00bc \u2022\u00bd \u00c0 \u00bd \u00b5 \u00bc\u00b4 \u00bc \u2022\u00bd \u00bd \u00b5 which satisfies equation (1).\nThe explicit computation of the gradient of the statistical dissimilarity term \u00c5 \u00bd was carried out using the same pattern as in [8,7]. The gradient in \u00c0 \u00bd of \u00c5 \u00bd for the two global criteria is given by\n\u00d6\u00c5 \u00bd\u00b4 \u00b5\u00b4\u00dc\u00b5 \u00bd \u00aa \u00bd \u00be\u00b4 \u00ac \u03bc \u00c1\u00b4\u00c5\u00b5\u00b5 \u00c1 \u00de\u00b4\u00c5 \u00b5 (2)\nwhere indicates a convolution with respect to the two intensities and depends on the criterion:\n\u00b4 \u00b5 \u00bd \u00da \u00bd \u00da \u00be \u00be\u00da \u00bd \u00be\u00b4 \u00bd \u00be \u00bd \u00be \u00be \u00bd \u00b5 \u00b4 \u00be \u00da \u00bd\u00b4 \u00be \u00be \u00be \u00b5 \u2022 \u00bd \u00da \u00be\u00b4 \u00bd \u00be \u00bd \u00b5\u00b5 \u00c5\u00c1 \u00b4 \u00b5 \u00bd \u2022 \u00d0 \u00d3 \u00c8 \u00b4 \u00b5 \u00c8 \u00b4 \u00bd \u00b5 \u00c8 \u00b4 \u00be \u00b5 (3)\nwhere \u00c8 is the global joint probability distribution, \u00da \u00bd \u00be are the averages and the variances and \u00da \u00bd \u00be is the covariance of the backprojected images \u00c1 \u00bd AE \u00cb and \u00c1 \u00be AE \u00cb .\nThe gradient in \u00c0 \u00bd of \u00c5 \u00bd for the local criteria is given by\n\u00d6\u00c5 \u00bd\u00b4 \u00b5\u00b4\u00dc\u00b5 \u00bd \u00be \u00ac \u00bd \u00b4\u00c1\u00b4\u00c5\u00b5 \u00dc\u00b5 \u00c1 \u00de\u00b4\u00c5 \u00b5 (4)\nwhere the first convolution acts on the two spatial variables while the second is still on the intensity variables.\n\u00d0 and \u00c5\u00c1 \u00d0 are space-dependent versions of equation (3).\nThe gradient in \u00c0 \u00bd of \u00ca \u00bd is given by\n\u00d6\u00ca \u00bd\u00b4 \u00b5 \u00ab div\u00b4\u00d6 AE \u00d6 \u00b5 (5)\nSelf occlusions can be taken into account by restricting the integration domain of the similarity criteria to the portion of the surface visible from both cameras. If we carry out the derivation of these modified criteria under the widely accepted technical assumption that the visibility remains constant, we get the same expressions as equations ( 2) and ( 4), except that the gradients are now supported by the visible domain. This approach is not included in our current implementation because it is of limited practical interest in the case of depth maps.\nThe following theorem, detailed in [14], addresses the well-posedness of the minimization process: Consider the Tikhonov case, and suppose image itensities are bounded. Let us enforce \u00c1 \u00b4\u00dc \u00dd \u00de\u00b5 \u00bc \u00de , which simply states that we do not consider arbitrarily high depth values. Let us substitute to \u00c1 its convolution with a 3D Gaussian kernel of variance \u00bc: \u00c1 \u00c1 . Then Theorem 1 applies. Moreover, the Gaussian smoothing stage is compatible with a multi-resolution strategy. Indeed, the energy functional \u00bd may be nonconvex due to its data term, so that the gradient descent may be trapped in a local minimizer. As a consequence, its asymptotic state depends on the initial guess \u00bc .\nIn order to avoid convergence to physically irrelevant local minima, we adopt a multi-resolution coarse-to-fine strategy as in [1]. The flow equation ( 1) is applied to a set of smoothed and subsampled volume images \u00c1 . In our experiments, the initial guess for the coarsest resolution is a plane with a suitable constant depth \u00de \u00bc .", "publication_ref": ["b7", "b6", "b2", "b13", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Numerical experiments", "text": "The gradients of the statistical criteria described in Section 2 can be implemented efficiently thanks to fast recursive filtering . The computation time is of a few seconds per frame for medium resolution reconstructions.\nFigure 1   global cross correlation suggest that no global affine dependency exists for the considered stereo pair. Local cross correlation and global mutual information yield good results, whereas local mutual information performs worse. Indeed, the amount of information used for matching images is smaller in this case. Hence, local mutual information should be reserved to extreme imaging conditions in which all other criteria fail. Figure 2 shows a high resolution reconstruction obtained with local cross correlation and some views with backprojected texture. Figure 3 illustrates the robustness of our method to camera spectral sensitivity differences. We have transformed the intensities of the initial stereo pair to simulate different sensors. While local cross correlation fails in this case, local cross correlation global mutual information global mutual information yields good results. Figure 3 also represents the evolution of the joint probability distribution of backprojected images in this case: we can see that the mutual information criterion tends to cluster the joint probability distribution, and that the non-affine dependency we had imposed could be recovered correctly.", "publication_ref": [], "figure_ref": ["fig_0", "fig_3", "fig_4", "fig_4"], "table_ref": []}, {"heading": "Variational scene flow estimation with statistical measures", "text": "We evolve a 3D vector field defined on an estimation of the object surface so as to match the backprojected image at time instant \u00d8 and the backprojected image onto the predicted surface at time instant \u00d8\u2022 \u00bd in all cameras. This way, we enforce the agreement between the estimated scene flow and the 2D displacements in all cameras, without an explicit use of previous optical flow computations or of ambiguous spatio-temporal image derivatives. As in Section 3, the vector field evolution is driven by the minimization of an energy functional through a gradient descent method.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Notations", "text": "We denote by \u00cb \u00d8 the estimated surface at time instant \u00d8, modelled as the graph of a function \u00d8 \u00be \u00bd . We model the 3D scene flow between \u00d8 and \u00d8 \u2022 \u00bd as an unknown function \u00da \u00d8 belonging to a dense linear subspace \u00be of the Hilbert space \u00c0 \u00be \u00c4 \u00be \u00aa \u00ca \u00bf \u00a1 . The predicted surface at time intant \u00d8 \u2022 \u00bd is given by \u00cb \u00d8 \u2022 \u00da \u00d8 . We do not constrain the deformed surface \u00cb \u00d8 \u2022 \u00da \u00d8 to agree with \u00cb \u00d8\u2022\u00bd in order to make scene flow estimation robust to surface estimation errors.\nIn this section, we note \u00c1 \u00d8 the image captured by camera at time instant \u00d8, and we define \u00c1 \u00d8 as in paragraph 3.1. We note \u00c5 the 3D point\u00b4\u00dc \u00dd \u00d8\u00b4\u00dc \u00dd\u00b5\u00b5 and \u00ce the vector \u00da \u00d8\u00b4\u00dc \u00dd\u00b5.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Variational formulation", "text": "We consider the minimization of a cost functional  (7) Under the same assumptions as those of Section 3, we have proved (see [14]) the well-posedness of the minimization process.", "publication_ref": ["b6", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Numerical experiments", "text": "Figure 4 shows some frames of the input stereo sequence and the computed scene flow between the first two frames. We clearly see that the overall movement of the head and the closing of the mouth are recovered but it is somewhat difficult to evaluate the details of the flow in this figure. In order to show the precision of the computed scene flow, we have synthetized a motion-compensated 3D sequence from the initial surface and texture, and from the successive scene flows. Note that this is a challenging experiment since potential errors are accumulated over many frames. Figure 5 shows some previews of this sequence. The movement of the jaw is successfully recovered. ", "publication_ref": [], "figure_ref": ["fig_5", "fig_6"], "table_ref": []}, {"heading": "Conclusion and future work", "text": "We have described a common variational framework for depth recovery and scene flow estimation from multiple calibrated video sequences. Our method avoids projective distorsion by backprojecting the input images onto suitable surfaces and uses statistical similarity criteria to handle camera spectral sensitivity differences and illumination changes.\nOur future work includes extending our framework to implicit surfaces defined by a level set function, and integrating shape and motion estimations in order to exploit their coherence and to improve their robustness and/or precision, as pointed out in [20]. We believe that this present work, by unifying stereo and scene flow estimation in the same coherent theoretical and computational framework, is a promising step towards this integration.", "publication_ref": ["b19"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Reliable estimation of dense optical flow fields with large displacements", "journal": "The International Journal of Computer Vision", "year": "2000-08", "authors": "L Alvarez; J Weickert; J S\u00e0nchez"}, {"ref_id": "b1", "title": "Performance of optical flow techniques", "journal": "The International Journal of Computer Vision", "year": "1994", "authors": "J Barron; D Fleet; S Beauchemin"}, {"ref_id": "b2", "title": "Multi-view scene capture by surfel sampling: From video streams to non-rigid 3d motion, shape & reflectance", "journal": "IEEE Computer Society Press", "year": "2001", "authors": "R Carceroni; K Kutulakos"}, {"ref_id": "b3", "title": "Stereo correspondence from motion correspondence", "journal": "IEEE Computer Society", "year": "1999-06", "authors": "F Dornaika; R Chung"}, {"ref_id": "b4", "title": "Variational principles, surface evolution, PDE's, level set methods and the stereo problem", "journal": "IEEE Transactions on Image Processing", "year": "1998-03", "authors": "O Faugeras; R Keriven"}, {"ref_id": "b5", "title": "A combined temporal tracking and stereo-correlation technique for accurate measurement of 3d displacements: Application to sheet metal forming", "journal": "Journal of Materials Processing Technology", "year": "2002-09", "authors": "D Garcia; J Orteu; L Penazzi"}, {"ref_id": "b6", "title": "Variational Methods for Multimodal Image Matching. PhD thesis, INRIA, The document is accessible at ftp://ftp-sop", "journal": "", "year": "2002", "authors": "G Hermosillo"}, {"ref_id": "b7", "title": "Dense image matching with global and local statistical criteria: a variational approach", "journal": "", "year": "2001", "authors": "G Hermosillo; O Faugeras"}, {"ref_id": "b8", "title": "A stereo matching algorithm with an adaptive window: Theory and experiment", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "1994-09", "authors": "T Kanade; M Okutomi"}, {"ref_id": "b9", "title": "Multimodality image registration by maximization of mutual information", "journal": "IEEE transactions on Medical Imaging", "year": "1997-04", "authors": "F Maes; A Collignon; D Vandermeulen; G Marchal; P Suetens"}, {"ref_id": "b10", "title": "Model based joint motion and structure estimation from stereo images. Computer Vision and Image Understanding", "journal": "", "year": "1997", "authors": "M Malassiotis;  Strintzis"}, {"ref_id": "b11", "title": "Spatio-temporal stereo using multi-resolution subdivision surfaces", "journal": "The International Journal of Computer Vision", "year": "2002", "authors": "J Neumann; Y Aloimonos"}, {"ref_id": "b12", "title": "On the estimation of probability density function", "journal": "Ann. Math. Statist", "year": "1962", "authors": "E Parzen"}, {"ref_id": "b13", "title": "Variational stereovision and 3D motion estimation with statistical measures", "journal": "Research report", "year": "2002", "authors": "J.-P Pons; R Keriven; O Faugeras; G Hermosillo"}, {"ref_id": "b14", "title": "Dense depth map reconstruction: A minimization and regularization approach which preserves discontinuities", "journal": "", "year": "1996-04", "authors": "L Robert; R Deriche"}, {"ref_id": "b15", "title": "The correlation ratio as new similarity metric for multimodal image registration", "journal": "Springer", "year": "1998-10", "authors": "A Roche; G Malandain; X Pennec; N Ayache"}, {"ref_id": "b16", "title": "Stereo matching with nonliear diffusion", "journal": "International Journal of Computer Vision", "year": "1998-06", "authors": "D Scharstein; R Szeliski"}, {"ref_id": "b17", "title": "Unified optical flow field approach to motion analysis from a sequence of stereo images", "journal": "Pattern Recognition", "year": "1994", "authors": "Y Shi; C Shu; P J "}, {"ref_id": "b18", "title": "Motion -stereo integration for depth estimation", "journal": "Springer-Verlag", "year": "2002-05", "authors": "C Strecha; L Van Gool"}, {"ref_id": "b19", "title": "Three-dimensional scene flow", "journal": "IEEE Computer Society Press", "year": "1999", "authors": "S Vedula; S Baker; P Rander; R Collins; T Kanade"}, {"ref_id": "b20", "title": "Shape and motion carving in 6d", "journal": "IEEE Computer Society", "year": "2000-06", "authors": "S Vedula; S Baker; S Seitz; T Kanade"}, {"ref_id": "b21", "title": "Alignement by maximization of mutual information", "journal": "The International Journal of Computer Vision", "year": "1997", "authors": "P Viola; W M Wells"}, {"ref_id": "b22", "title": "Recovering the three-dimensional motion and structure of multiple moving objects from binocular image flows. Computer Vision and Image Understanding", "journal": "", "year": "1996", "authors": "W Wang; J Duncan"}, {"ref_id": "b23", "title": "Integrated 3D scene flow and structure recovery from multiview image sequences", "journal": "IEEE Computer Society", "year": "2000-06", "authors": "Y Zhang; C Kambhamettu"}, {"ref_id": "b24", "title": "On 3d scene flow and structure estimation", "journal": "", "year": "2001", "authors": "Y Zhang; C Kambhamettu"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Theorem 11If the following assumptions are satisfied: is a positive definite quadratic form, , \u00c1 and \u00d6\u00c1 are bounded and Lipschitz continuous, then equation (1) has a unique global classical solution.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "shows the stereo pair used in our first series of experiments and compares the reconstructed surfaces obtained for different statistical criteria. The poor results of global cross correlation local cross correlation global mutual information local mutual information", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 1 .1Figure 1. Stereo pair (top) and reconstructed surface for different statistical criteria.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 2 .2Figure 2. Some detailed views of the reconstructed surface.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 3 .3Figure 3. Modified stereo pair (top). Reconstructed surface with local cross correlation (middle left) and with global mutual information (middle right). Joint probability distribution of backprojected images onto the initial (bottom left) and the final (bottom right) surface with global mutual information.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 4 .4Figure 4. Some frames of the left input sequence (top) and some views of the estimated scene flow (bottom).", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 5 .5Figure 5. Preview of the motion-compensated 3D sequence.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "classical choice for the regularization term \u00ca \u00bd is Several other functions have been designed in order to preserve depth discontinuities[15]. In our implementation, we have considered the Perona-Malik, the Rudin and the Aubert functions.We seek a minimum \u00d6 \u00d1 \u00d2 \u00be \u00bd \u00bd\u00b4 \u00b5. One can show that a necessary condition of optimality is the socalled Euler equation \u00d6 \u00c0\u00bd \u00bd\u00b4 \u00b5 \u00bc. Rather than trying to solve directly the Euler equation, which is impossible in most cases, we follow a gradient descent strategy starting from a guess \u00bc . That is, we solve the following evolution problem for , which becomes a function \u00bc \u2022\u00bd \u00c0", "figure_data": "\u00bd \u00be\u00be ."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "AE \u00cb \u00d8 at time instant \u00d8 and the backprojected images \u00c1 \u00d8\u2022\u00bd AE\u00b4\u00cb \u00d8 \u2022 \u00da \u00d8 \u00b5 onto the predicted surface at time instant \u00d8\u2022 \u00bd , while \u00ca \u00be defines regularizing constraints and smoothness assumptions on \u00da \u00d8 . In our experiments, we consider a Tikhonov regularization, but any other physicsbased or application-specific smoothness term \u00ca \u00be could be considered.The gradient in \u00c0 \u00be of \u00c5 \u00be for global criteria is given by The gradient in \u00c0 \u00be of \u00c5 \u00be for local criteria is given by", "figure_data": "\u00be\u00b4\u00da\u00d8 \u00b5 \u00c5 \u00be\u00b4\u00da\u00d8 \u00b5 \u2022 \u00ca \u00be\u00b4\u00da\u00d8 \u00b5 where \u00c5 \u00be\u00b4\u00da\u00d8 \u00b5 measures theglobal or local statistical dissimilarity between the backpro-jected images \u00c1 \u00d8 \u00d6\u00c5 \u00be\u00b4\u00da \u00d8 \u00b5\u00b4\u00dc\u00b5\u00bd \u00aa\u00ac\u00be \u00da\u00a1\u00b4\u00c1 \u00d8 \u00b4\u00c5\u00b5 \u00c1 \u00d8\u2022\u00bd \u00b4\u00c5\u2022\u00ce\u00b5\u00b5\u00d6\u00c1 \u00d8\u2022\u00bd \u00b4\u00c5\u2022\u00ce\u00b5(6)where \u00da is defined from images \u00c1 \u00d8 AE \u00cb \u00d8 and \u00c1 \u00d8\u2022\u00bd \u00da \u00d8 \u00b5 as in equation (3).AE\u00b4\u00cb \u00d8 \u2022\u00d6\u00c5 \u00be\u00b4\u00da\u00d8 \u00b5\u00b4\u00dc\u00b5\u00ac\u00bd\u00be \u00da\u00b4\u00c1 \u00d8 \u00b4\u00c5\u00b5 \u00c1 \u00d8\u2022\u00bd \u00b4\u00c5 \u2022 \u00ce\u00b5 \u00dc\u00b5\u00d6\u00c1\u00d8\u2022\u00bd \u00b4\u00c5 \u2022 \u00ce\u00b5"}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u00b4\u00bc\u00b5 \u00bc \u00be \u00c0 \u00bd \u00d6 \u00c0\u00bd \u00bd\u00b4 \u00b4 \u00b5\u00b5 \u00bc (1)", "formula_coordinates": [3.0, 100.56, 209.16, 185.96, 28.32]}, {"formula_id": "formula_1", "formula_text": "\u00be \u00bc\u00b4 \u00bc \u2022\u00bd \u00c0 \u00bd \u00b5 \u00bd\u00b4 \u00bc \u2022\u00bd \u00c0 \u00bd \u00b5 \u00bc\u00b4 \u00bc \u2022\u00bd \u00bd \u00b5 which satisfies equation (1).", "formula_coordinates": [3.0, 57.96, 255.36, 217.44, 27.36]}, {"formula_id": "formula_2", "formula_text": "\u00d6\u00c5 \u00bd\u00b4 \u00b5\u00b4\u00dc\u00b5 \u00bd \u00aa \u00bd \u00be\u00b4 \u00ac \u03bc \u00c1\u00b4\u00c5\u00b5\u00b5 \u00c1 \u00de\u00b4\u00c5 \u00b5 (2)", "formula_coordinates": [3.0, 60.12, 344.46, 226.4, 42.78]}, {"formula_id": "formula_3", "formula_text": "\u00b4 \u00b5 \u00bd \u00da \u00bd \u00da \u00be \u00be\u00da \u00bd \u00be\u00b4 \u00bd \u00be \u00bd \u00be \u00be \u00bd \u00b5 \u00b4 \u00be \u00da \u00bd\u00b4 \u00be \u00be \u00be \u00b5 \u2022 \u00bd \u00da \u00be\u00b4 \u00bd \u00be \u00bd \u00b5\u00b5 \u00c5\u00c1 \u00b4 \u00b5 \u00bd \u2022 \u00d0 \u00d3 \u00c8 \u00b4 \u00b5 \u00c8 \u00b4 \u00bd \u00b5 \u00c8 \u00b4 \u00be \u00b5 (3)", "formula_coordinates": [3.0, 65.52, 429.9, 221.0, 78.12]}, {"formula_id": "formula_4", "formula_text": "\u00d6\u00c5 \u00bd\u00b4 \u00b5\u00b4\u00dc\u00b5 \u00bd \u00be \u00ac \u00bd \u00b4\u00c1\u00b4\u00c5\u00b5 \u00dc\u00b5 \u00c1 \u00de\u00b4\u00c5 \u00b5 (4)", "formula_coordinates": [3.0, 60.12, 604.26, 226.28, 55.44]}, {"formula_id": "formula_5", "formula_text": "\u00d6\u00ca \u00bd\u00b4 \u00b5 \u00ab div\u00b4\u00d6 AE \u00d6 \u00b5 (5)", "formula_coordinates": [3.0, 359.28, 80.16, 185.96, 16.08]}], "doi": ""}