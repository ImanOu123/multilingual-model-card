{"title": "Distribution-Independent PAC Learning of Halfspaces with Massart Noise", "authors": "Ilias Diakonikolas; Themis Gouleakis; Christos Tzamos", "pub_date": "2019-12-11", "abstract": "We study the problem of distribution-independent PAC learning of halfspaces in the presence of Massart noise. Specifically, we are given a set of labeled examples (x, y) drawn from a distribution D on R d+1 such that the marginal distribution on the unlabeled points x is arbitrary and the labels y are generated by an unknown halfspace corrupted with Massart noise at noise rate \u03b7 < 1/2. The goal is to find a hypothesis h that minimizes the misclassification error We give a poly (d, 1/ ) time algorithm for this problem with misclassification error \u03b7 + . We also provide evidence that improving on the error guarantee of our algorithm might be computationally hard. Prior to our work, no efficient weak (distribution-independent) learner was known in this model, even for the class of disjunctions. The existence of such an algorithm for halfspaces (or even disjunctions) has been posed as an open question in various works, starting with Sloan (1988), Cohen (1997, and was most recently highlighted in Avrim Blum 's  FOCS 2003 tutorial.     ", "sections": [{"heading": "Introduction", "text": "Halfspaces, or Linear Threshold Functions (henceforth LTFs), are Boolean functions f : R d \u2192 {\u00b11} of the form f (x) = sign( w, x \u2212 \u03b8), where w \u2208 R d is the weight vector and \u03b8 \u2208 R is the threshold. (The function sign : R \u2192 {\u00b11} is defined as sign(u) = 1 if u \u2265 0 and sign(u) = \u22121 otherwise.) The problem of learning an unknown halfspace is as old as the field of machine learning -starting with Rosenblatt's Perceptron algorithm [Ros58] -and has arguably been the most influential problem in the development of the field. In the realizable setting, LTFs are known to be efficiently learnable in Valiant's distribution-independent PAC model [Val84] via Linear Programming [MT94]. In the presence of corrupted data, the situation is more subtle and crucially depends on the underlying noise model. In the agnostic model [Hau92,KSS94] -where an adversary is allowed to arbitrarily corrupt an arbitrary \u03b7 < 1/2 fraction of the labels -even weak learning is known to be computationally intractable [GR06,FGKP06,Dan16]. On the other hand, in the presence of Random Classification Noise (RCN) [AL88] -where each label is flipped independently with probability exactly \u03b7 < 1/2 -a polynomial time algorithm is known [BFKV96,BFKV97]. In this work, we focus on learning halfspaces with Massart noise [MN06]: Definition 1.1 (Massart Noise Model). Let C be a class of Boolean functions over X = R d , D x be an arbitrary distribution over X, and 0 \u2264 \u03b7 < 1/2. Let f be an unknown target function in C. A noisy example oracle, EX Mas (f, D x , \u03b7), works as follows: Each time EX Mas (f, D x , \u03b7) is invoked, it returns a labeled example (x, y), where x \u223c D x , y = f (x) with probability 1 \u2212 \u03b7(x) and y = \u2212f (x) with probability \u03b7(x), for an unknown parameter \u03b7(x) \u2264 \u03b7. Let D denote the joint distribution on (x, y) generated by the above oracle. A learning algorithm is given i.i.d. samples from D and its goal is to output a hypothesis h such that with high probability the error Pr (x,y)\u223cD [h(x) = y] is small.\nAn equivalent formulation of the Massart model [Slo88,Slo92] is the following: With probability 1\u2212\u03b7, we have that y = f (x), and with probability \u03b7 the label y is controlled by an adversary. Hence, the Massart model lies in between the RCN and the agnostic models. (Note that the RCN model corresponds to the special case that \u03b7(x) = \u03b7 for all x \u2208 X.) It is well-known (see, e.g., [MN06]) that poly(d, 1/ ) samples information-theoretically suffice to compute a hypothesis with misclassification error OPT + , where OPT is the misclassification error of the optimal halfspace. Also note that OPT \u2264 \u03b7 by definition. The question is whether a polynomial time algorithm exists.\nThe existence of an efficient distribution-independent learning algorithm for halfspaces (or even disjunctions) in the Massart model has been posed as an open question in a number of works. In the first COLT conference [Slo88] (see also [Slo92]), Sloan defined the malicious misclassification noise model (an equivalent formulation of Massart noise, described above) and asked whether there exists an efficient learning algorithm for disjunctions in this model. About a decade later, Cohen [Coh97] asked the same question for the more general class of all LTFs. The question remained open -even for weak learning of disjunctions! -and was highlighted in Avrim Blum's FOCS 2003 tutorial [Blu03]. Specifically, prior to this work, even the following very basic special case remained open:\nGiven labeled examples from an unknown disjunction, corrupted with 1% Massart noise, can we efficiently find a hypothesis that achieves misclassification error 49%?\nThe reader is referred to slides 39-40 of Avrim Blum's FOCS'03 tutorial [Blu03], where it is suggested that the above problem might be easier than agnostically learning disjunctions. As a corollary of our main result (Theorem 1.2), we answer this question in the affirmative. In particular, we obtain an efficient algorithm that achieves misclassification error arbitrarily close to \u03b7 for all LTFs.", "publication_ref": ["b33", "b38", "b32", "b23", "b28", "b22", "b21", "b10", "b3", "b5", "b6", "b35", "b36", "b31", "b35", "b36", "b9", "b7", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Our Results", "text": "The main result of this paper is the following:\nTheorem 1.2 (Main Result).\nThere is an algorithm that for all 0 < \u03b7 < 1/2, on input a set of i.i.d. examples from a distribution D = EX Mas (f, D x , \u03b7) on R d+1 , where f is an unknown halfspace on R d , it runs in poly(d, b, 1/ ) time, where b is an upper bound on the bit complexity of the examples, and outputs a hypothesis h that with high probability satisfies Pr (x,y)\u223cD [h(x) = y] \u2264 \u03b7 + .\nSee Theorem 2.9 for a more detailed formal statement. For large-margin halfspaces, we obtain a slightly better error guarantee; see Theorem 2.2 and Remark 2.6.\nDiscussion. We note that our algorithm is non-proper, i.e., the hypothesis h itself is not a halfspace. The polynomial dependence on b in the runtime cannot be removed, even in the noiseless case, unless one obtains strongly-polynomial algorithms for linear programming. Finally, we note that the misclassification error of \u03b7 + translates to error 2\u03b7 + with respect to the target LTF.\nOur algorithm gives error \u03b7 + , instead of the information-theoretic optimum of OPT + . To complement our positive result, we provide some evidence that improving on our (\u03b7 + ) error guarantee may be challenging. Roughly speaking, we show (see Theorems 3.1 and 3.2) that natural approaches -involving convex surrogates and refinements thereof -inherently fail, even under margin assumptions. (See Section 1.2 for a discussion.)\nBroader Context. This work is part of the broader agenda of designing robust estimators in the distribution-independent setting with respect to natural noise models. has given efficient robust estimators for a range of learning tasks (both supervised and unsupervised) in the presence of a small constant fraction of adversarial corruptions. A limitation of these results is the assumption that the good data comes from a \"tame\" distribution, e.g., Gaussian or isotropic log-concave distribution. On the other hand, if no assumption is made on the good data and the noise remains fully adversarial, these problems become computationally intractable [Ber06,GR06,Dan16]. This suggests the following general question: Are there realistic noise models that allow for efficient algorithms without imposing (strong) assumptions on the good data? Conceptually, the algorithmic results of this paper could be viewed as an affirmative answer to this question for the problem of learning halfspaces.", "publication_ref": ["b4", "b22", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Technical Overview", "text": "In this section, we provide an outline of our approach and a comparison to previous techniques. Since the distribution on the unlabeled data is arbitrary, we can assume w.l.o.g. that the threshold \u03b8 = 0.\nMassart Noise versus RCN. Random Classification Noise (RCN) [AL88] is the special case of Massart noise where each label is flipped with probability exactly \u03b7 < 1/2. At first glance, it might seem that Massart noise is easier to deal with computationally than RCN. After all, in the Massart model we add at most as much noise as in the RCN model with noise rate \u03b7. It turns out that this intuition is fundamentally flawed. Roughly speaking, the ability of the Massart adversary to choose whether to perturb a given label and, if so, with what probability (which is unknown to the learner), makes the design of efficient algorithms in this model challenging. In particular, the well-known connection between learning with RCN and the Statistical Query (SQ) model [Kea93,Kea98] no longer holds, i.e., the property of being an SQ algorithm does not automatically suffice for noisetolerant learning with Massart noise. We note that this connection with the SQ model is leveraged in [BFKV96,BFKV97] to obtain their polynomial time algorithm for learning halfspaces with RCN.\nLarge Margin Halfspaces. To illustrate our approach, we start by describing our learning algorithm for \u03b3-margin halfspaces on the unit ball. That is, we assume | w * , x | \u2265 \u03b3 for every x in the support, where w * \u2208 R d with w * 2 = 1 defines the target halfspace h w * (x) = sign( w * , x ). Our goal is to design a poly(d, 1/ , 1/\u03b3) time learning algorithm in the presence of Massart noise.\nIn the RCN model, the large margin case is easy because the learning problem is essentially convex. That is, there is a convex surrogate that allows us to formulate the problem as a convex program. We can use SGD to find a near-optimal solution to this convex program, which automatically gives a strong proper learner. This simple fact does not appear explicitly in the literature, but follows easily from standard tools. [Byl94] showed that a variant of the Perceptron algorithm (which can be viewed as gradient descent on a particular convex objective) learns \u03b3-margin halfspaces in poly(d, 1/ , 1/\u03b3) time. The algorithm in [Byl94] requires an additional anti-concentration condition about the distribution, which is easy to remove. In Appendix A, we show that a \"smoothed\" version of Bylander's objective suffices as a convex surrogate under only the margin assumption.\nRoughly speaking, the reason that a convex surrogate works for RCN is that the expected effect of the noise on each label is known a priori. Unfortunately, this is not the case for Massart noise. We show (Theorem 3.1 in Appendix 3) that no convex surrogate can lead to a weak learner, even under a margin assumption. That is, if w is the minimizer of G(w) = E (x,y)\u223cD [\u03c6(y w, x )], where \u03c6 can be any convex function, then the hypothesis sign( w, x ) is not even a weak learner. So, in sharp contrast with the RCN case, the problem is non-convex in this sense.\nOur Massart learning algorithm for large margin halfspaces still uses a convex surrogate, but in a qualitatively different way. Instead of attempting to solve the problem in one-shot, our algorithm adaptively applies a sequence of convex optimization problems to obtain an accurate solution in disjoint subsets of the space. Our iterative approach is motivated by a new structural lemma (Lemma 2.5) establishing the following: Even though minimizing a convex proxy does not lead to small misclassification error over the entire space, there exists a region with non-trivial probability mass where it does. Moreover, this region is efficiently identifiable by a simple thresholding rule. Specifically, we show that there exists a threshold T > 0 (which can be found algorithmically) such that the hypothesis sign( w, x ) has error bounded by \u03b7 + in the region R T = {x : | w, x | \u2265 T }.\nHere w is any near-optimal solution to an appropriate convex optimization problem, defined via a convex surrogate objective similar to the one used in [Byl94]. We note that Lemma 2.5 is the main technical novelty of this paper and motivates our algorithm. Given Lemma 2.5, in any iteration i we can find the best threshold T (i) using samples, and obtain a learner with misclassification error \u03b7 + in the corresponding region. Since each region has non-trivial mass, iterating this scheme a small number of times allows us to find a non-proper hypothesis (a decision-list of halfspaces) with misclassification error at most \u03b7 + in the entire space.\nThe idea of iteratively optimizing a convex surrogate was used in [BFKV96] to learn halfspaces with RCN without a margin. Despite this similarity, we note that the algorithm of [BFKV96] fails to even obtain a weak learner in the Massart model. We point out two crucial technical differences: First, the iterative approach in [BFKV96] was needed to achieve polynomial running time. As mentioned already, a convex proxy is guaranteed to converge to the true solution with RCN, but the convergence may be too slow (when the margin is tiny). In contrast, with Massart noise (even under a margin condition) convex surrogates cannot even give weak learning in the entire domain. Second, the algorithm of [BFKV96] used a fixed threshold in each iteration, equal to the margin parameter obtained after an appropriate pre-processing of the data (that is needed in order to ensure a weak margin property). In contrast, in our setting, we need to find an appropriate threshold T (i) in each iteration i, according to the criterion specified by our Lemma 2.5. General Case. Our algorithm for the general case (in the absence of a margin) is qualitatively similar to our algorithm for the large margin case, but the details are more elaborate. We borrow an idea from [BFKV96] that in some sense allows us to \"reduce\" the general case to the large margin case. Specifically, [BFKV96] (see also [DV04a]) developed a pre-processing routine that slightly modifies the distribution on the unlabeled points and guarantees the following weak margin property: After pre-processing, there exists an explicit margin parameter \u03c3 = \u2126(1/poly(d, b)), such that any hyperplane through the origin has at least a non-trivial mass of the distribution at distance at least \u03c3 from it. Using this pre-processing step, we are able to adapt our algorithm from the previous subsection to work without margin assumptions in poly(d, b, 1/ ) time. While our analysis is similar in spirit to the case of large margin, we note that the margin property obtained via the [BFKV96,DV04a] preprocessing step is (necessarily) weaker, hence additional careful analysis is required.\nLower Bounds Against Natural Approaches. We have already explained our Theorem 3.1, which shows that using a convex surrogate over the entire space cannot not give a weak learner. Our algorithm, however, can achieve error \u03b7 + by iteratively optimizing a specific convex surrogate in disjoint subsets of the domain. A natural question is whether one can obtain qualitatively better accuracy, e.g., f (OPT) + , by using a different convex objective function in our iterative thresholding approach. We show (Theorem 3.2) that such an improvement is not possible: Using a different convex proxy cannot lead to error better than (1 \u2212 o(1)) \u2022 \u03b7. It is a plausible conjecture that improving on the error guarantee of our algorithm is computationally hard. We leave this as an intriguing open problem for future work.", "publication_ref": ["b3", "b24", "b25", "b5", "b6", "b8", "b8", "b8", "b5", "b5", "b5", "b5", "b5", "b19", "b5", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Prior and Related Work", "text": "Bylander [Byl94] gave a polynomial time algorithm to learn large margin halfspaces with RCN (under an additional anti-concentration assumption). The work of Blum et al. [BFKV96,BFKV97] gave the first polynomial time algorithm for distribution-independent learning of halfspaces with RCN without any margin assumptions. Soon thereafter, [Coh97] gave a polynomial-time proper learning algorithm for the problem. Subsequently, Dunagan and Vempala [DV04b] gave a rescaled perceptron algorithm for solving linear programs, which translates to a significantly simpler and faster proper learning algorithm.\nThe term \"Massart noise\" was coined after [MN06]. An equivalent version of the model was previously studied by Rivest and Sloan [Slo88, Slo92, RS94, Slo96], and a very similar asymmetric random noise model goes back to Vapnik [Vap82]. Prior to this work, essentially no efficient algorithms with non-trivial error guarantees were known in the distribution-free Massart noise model. It should be noted that polynomial time algorithms with error OPT+ are known [ABHU15, ZLC17, YZ17] when the marginal distribution on the unlabeled data is uniform on the unit sphere. For the case that the unlabeled data comes from an isotropic log-concave distribution, [ABHZ16] give a d 2 poly(1/(1\u22122\u03b7)) /poly( ) sample and time algorithm.", "publication_ref": ["b8", "b5", "b6", "b9", "b20", "b31", "b39", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "For n \u2208 Z + , we denote [n] def = {1, . . . , n}. We will use small boldface characters for vectors and we let e i denote the i-th vector of an orthonormal basis.\nFor x \u2208 R d , and i \u2208 [d], x i denotes the i-th coordinate of x, and x 2 def = ( d i=1 x 2 i ) 1/2 denotes the 2 -norm of x. We will use x, y for the inner product between x, y \u2208 R d . We will use E[X] for the expectation of random variable X and Pr[E] for the probability of event E.\nAn origin-centered halfspace is a Boolean-valued function h w : R d \u2192 {\u00b11} of the form h w (x) = sign ( w, x ), where w \u2208 R d . (Note that we may assume w.l.o.g. that w 2 = 1.) We denote by H d the class of all origin-centered halfspaces on R d .\nWe consider a classification problem where labeled examples (x, y) are drawn i.i.d. from a distribution D. We denote by D x the marginal of D on x, and for any x denote D y (x) the distribution of y conditional on x. Our goal is to find a hypothesis classifier h with low misclassification error. We will denote the misclassification error of a hypothesis h with respect to D by err\nD 0\u22121 (h) = Pr (x,y)\u223cD [h(x) = y]. Let OPT = min h\u2208H d err D 0\u22121 (h)\ndenote the optimal misclassification error of any halfspace, and w * be the normal vector to a halfspace h w * that achieves this.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm for Learning Halfspaces with Massart Noise", "text": "In this section, we present the main result of this paper, which is an efficient algorithm that achieves \u03b7 + misclassification error for distribution-independent learning of halfspaces with Massart noise, where \u03b7 is an upper bound on the noise rate.\nOur algorithm uses (stochastic) gradient descent on a convex proxy function L(w) for the misclassification error to identify a region with small misclassification error. The loss function penalizes the points which are misclassified by the threshold function h w , proportionally to the distance from the corresponding hyperplane, while it rewards the correctly classified points at a smaller rate. Directly optimizing this convex objective does not lead to a separator with low error, but guarantees that for a non-negligible fraction of the mass away from the separating hyperplane the misclassification error will be at most \u03b7 + . By classifying points in this region according to the hyperplane and recursively working on the remaining points, we obtain an improper learning algorithm that achieves \u03b7 + error overall.\nWe now develop some necessary notation before proceeding with the description and analysis of our algorithm.\nOur algorithm considers the following convex proxy for the misclassification error as a function of the weight vector w:\nL(w) = E (x,y)\u223cD [LeakyRelu \u03bb (\u2212y w, x )] , under the constraint w 2 \u2264 1, where LeakyRelu \u03bb (z) = (1 \u2212 \u03bb)z if z \u2265 0 \u03bbz if z < 0\nand \u03bb is the leakage parameter, which we will set to be \u03bb \u2248 \u03b7.\nWe define the per-point misclassification error and the error of the proxy function as err(w, x) = Pr y\u223cDy(x) [w(x) = y] and (w,\nx) = E y\u223cDy(x) [LeakyRelu \u03bb (\u2212y w, x )] respectively. Notice that err D 0\u22121 (h w ) = E x\u223cDx [err(w, x)] and L(w) = E x\u223cDx [ (w, x)]. Moreover, OPT = E x\u223cDx [err(w * , x)] = E x\u223cDx [\u03b7(x)].\nRelationship between proxy loss and misclassification error We first relate the proxy loss and the misclassification error: Proof. We consider two cases:\n\u2022 Case sign( w, x ) = sign( w * , x ): In this case, we have that err(w, x) = \u03b7(x), while (w,\nx) = \u03b7(x)(1 \u2212 \u03bb)| w, x | \u2212 (1 \u2212 \u03b7(x))\u03bb| w, x | = (\u03b7(x) \u2212 \u03bb)| w, x |.\n\u2022 Case sign( w, x ) = sign( w * , x ): In this case, we have that err(w,\nx) = 1 \u2212 \u03b7(x), while (w, x) = (1 \u2212 \u03b7(x))(1 \u2212 \u03bb)| w, x | \u2212 \u03b7(x)\u03bb| w, x | = (1 \u2212 \u03b7(x) \u2212 \u03bb)| w, x |.\nThis completes the proof of Claim 2.1.\nClaim 2.1 shows that minimizing E x\u223cDx (w,x)\n| w,x | is equivalent to minimizing the misclassification error. Unfortunately, this objective is hard to minimize as it is non-convex, but one would hope that minimizing L(w) instead may have a similar effect. As we show in Section 3, this is not true because | w, x | might vary significantly across points, and in fact it is not possible to use a convex proxy that achieves bounded misclassification error directly.\nOur algorithm circumvents this difficulty by approaching the problem indirectly to find a nonproper classifier. Specifically, our algorithm works in multiple rounds, where within each round only points with high value of | w, x | are considered. The intuition is based on the fact that the approximation of the convex proxy to the misclassification error is more accurate for those points that have comparable distance to the halfspace. In Section 2.1, we handle the large margin case and in Section 2.2 we handle the general case.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Warm-up: Learning Large Margin Halfspaces", "text": "We consider the case that there is no probability mass within distance \u03b3 from the separating hyperplane w * , x = 0, w * 2 = 1. Formally, assume that for every x \u223c D x , x 2 \u2264 1 and that | w * , x | \u2265 \u03b3.\nThe pseudo-code of our algorithm is given in Algorithm 1. Our algorithm returns a decision list [(w (1) , T (1) ), (w (2) , T (2) ), \u2022 \u2022 \u2022 ] as output. To classify a point x given the decision list, the first i is identified such that | w (i) , x | \u2265 T (i) and sign( w (i) , x ) is returned. If no such i exists, an arbitrary prediction is returned.\nAlgorithm 1 Main Algorithm (with margin)\n1: Set S (1) = R d , \u03bb = \u03b7 + , m =\u00d5( 1 \u03b3 2 4 ). 2: Set i \u2190 1. 3: Draw O (1/ 2 ) log(1/( \u03b3)) samples from D x to form an empirical distributionD x . 4: while Pr x\u223cDx x \u2208 S (i) \u2265 do 5: Set D (i) = D| S (i)\n, the distribution conditional on the unclassified points.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "6:", "text": "Let\nL (i) (w) = E (x,y)\u223cD (i) [LeakyRelu \u03bb (\u2212y w, x )] 7:\nRun SGD on L (i) (w) for\u00d5(1/(\u03b3 2 2 )) iterations to get w (i) with w (i)\n2 = 1 such that L (i) (w (i) ) \u2264 min w: w 2 \u22641 L (i) (w) + \u03b3 /2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "8:", "text": "Draw m samples from D (i) to form an empirical distribution D\n(i) m . 9: Find a threshold T (i) such that Pr (x,y)\u223cD (i) m [| w (i) , x | \u2265 T (i) ] \u2265 \u03b3 and the empirical mis- classification error, Pr (x,y)\u223cD (i) m [h w (i) (x) = y | w (i) , x | \u2265 T (i) ]\n, is minimized.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "10:", "text": "Update the unclassified region S (i+1) \u2190 S (i) \\ {x : | w (i) , x | \u2265 T (i) } and set i \u2190 i + 1.\n11: Return the classifier [(w (1) , T (1) ), (w (2) , T (2) ), \u2022 \u2022 \u2022 ]\nThe main result of this section is the following: Theorem 2.2. Let D be a distribution on B d \u00d7 {\u00b11} such that D x satisfies the \u03b3-margin property with respect to w * and y is generated by sign( w * , x ) corrupted with Massart noise at rate \u03b7 < 1/2. Algorithm 1 uses\u00d5(1/(\u03b3 3 5 )) samples from D, runs in poly(d, 1/ , 1/\u03b3) time, and returns, with probability 2/3, a classifier h with misclassification error err D 0\u22121 (h) \u2264 \u03b7 + .\nOur analysis focuses on a single iteration of Algorithm 1. We will show that a large fraction of the points is classified at every iteration within error \u03b7 + . To achieve this, we analyze the convex objective L. We start by showing that the optimal classifier w * obtains a sufficiently small negative objective value.\nLemma 2.3. If \u03bb \u2265 \u03b7, then L(w * ) \u2264 \u2212\u03b3(\u03bb \u2212 OPT).\nProof. For any fixed x, using Claim 2.1, we have that\n(w * , x) = (err(w * , x) \u2212 \u03bb)| w * , x | = (\u03b7(x) \u2212 \u03bb)| w * , x | \u2264 \u2212\u03b3(\u03bb \u2212 \u03b7(x)) ,\nsince | w * , x | \u2265 \u03b3 and \u03b7(x) \u2212 \u03bb \u2264 0. Taking expectation over x \u223c D x , the statement follows.\nLemma 2.3 is the only place where the Massart noise assumption is used in our approach and establishes that points with sufficiently small negative value exist. As we will show, any weight vector w with this property can be found with few samples and must accurately classify some region of non-negligible mass away from it (Lemma 2.5).\nWe now argue that we can use stochastic gradient descent (SGD) to efficiently identify a point w that achieves comparably small objective value to the guarantee of Lemma 2.3. We use the following standard property of SGD:\nLemma 2.4 (see, e.g., Theorem 3.4.11 in [Duc16]). Let L be any convex function. Consider the (projected) SGD iteration that is initialized at w (0) = 0 and for every step computes w (t+ 1 2 ) = w (t) \u2212 \u03c1v (t) and w (t+1) = arg min\nw: w 2 \u22641 w \u2212 w (t+ 1 2 ) 2\n, where v (t) is a stochastic gradient such that for all steps E[v (t) |w (t) ] \u2208 \u2202L(w (t) ) and v (t) 2 \u2264 1. Assume that SGD is run for T iterations with step size \u03c1 = 1 \u221a T and letw = 1 T T t=1 w (t) . Then, for any , \u03b4 > 0, after T = \u2126(log(1/\u03b4)/ 2 ) iterations with probability with probability at least 1 \u2212 \u03b4 we have that L(w) \u2264 min w: w 2 \u22641 L(w) + .\nBy Lemma 2.3, we know that min w: w 2 \u22641 L(w) \u2264 \u2212\u03b3(\u03bb \u2212 OPT). By Lemma 2.4, it follows that by running SGD on L(w) with projection to the unit 2 -ball for O log(1/\u03b4)/(\u03b3 2 (\u03bb \u2212 OPT) 2 ) steps, we find a w such that L(w) \u2264 \u2212\u03b3(\u03bb \u2212 OPT)/2 with probability at least 1 \u2212 \u03b4.\nNote that we can assume without loss of generality that w 2 = 1, as increasing the magnitude of w only decreases the objective value.\nWe now consider the misclassification error of the halfspace h w conditional on the points that are further than some distance T from the separating hyperplane. We claim that there exists a threshold T > 0 where the restriction has non-trivial mass and the conditional misclassification error is small: Lemma 2.5. Consider a vector w with L(w) < 0. There exists a threshold T \u2265 0 such that (i)\nPr (x,y)\u223cD [| w, x | \u2265 T ] \u2265 |L(w)|\n2\u03bb , and (ii)\nPr (x,y)\u223cD [h w (x) = y | w, x | \u2265 T ] \u2264 \u03bb \u2212 |L(w)| 2 .\nProof. We will show there is a T \u2265 0 such that Pr\n(x,y)\u223cD [h w (x) = y | w, x | \u2265 T ] \u2264 \u03bb \u2212 \u03b6, where \u03b6 def = |L(w)|/2, or equivalently, E x\u223cDx [(err(w, x) \u2212 \u03bb + \u03b6)1 | w,x\n|\u2265T ] \u2264 0. For a T drawn uniformly at random in [0, 1], we have that:\n1 0 E x\u223cDx [(err(w, x) \u2212 \u03bb + \u03b6)1 | w,x |\u2265T ]dT = E x\u223cDx [(err(w, x) \u2212 \u03bb)| w, x |] + \u03b6E x\u223cDx [| w, x |] \u2264 E x\u223cDx [ (w, x)] + \u03b6 = L(w) + \u03b6 = L(w)/2 < 0 ,\nwhere the first inequality uses Claim 2.1. Thus, there exists aT such that\nE x\u223cDx [(err(w, x) \u2212 \u03bb + \u03b6)1 | w,x |\u2265T ] \u2264 0 .\nConsider the minimum suchT . Then we have\n1 T E x\u223cDx [(err(w, x) \u2212 \u03bb + \u03b6)1 | w,x |\u2265T ]dT \u2265 \u2212\u03bb \u2022 Pr (x,y)\u223cD [| w, x | \u2265T ] .\nBy definition ofT , it must be the case that\nT 0 E x\u223cDx [(err(w, x) \u2212 \u03bb + \u03b6)1 | w,x |\u2265T ]dT \u2265 0 . Therefore, L(w) 2 \u2265 1 T E x\u223cDx [(err(w, x) \u2212 \u03bb + \u03b6)1 | w,x |\u2265T ]dT \u2265 \u2212\u03bb \u2022 Pr (x,y)\u223cD [| w, x | \u2265T ] , which implies that Pr (x,y)\u223cD [| w, x | \u2265T ] \u2265 |L(w)| 2\u03bb .\nThis completes the proof of Lemma 2.5.\nEven though minimizing the convex proxy L does not lead to low misclassification error overall, Lemma 2.5 shows that there exists a region of non-trivial mass where it does. This region is identifiable by a simple threshold rule. We are now ready to prove Theorem 2.2.\nProof of Theorem 2.2. We consider the steps of Algorithm 1 in each iteration of the while loop. At iteration i, we consider a distribution D (i) consisting only of points not handled in previous iterations.\nWe start by noting that with high probability the total number of iterations is\u00d5(1/(\u03b3 )). This can be seen as follows: The empirical probability mass under D\n(i) m of the region {x : | w (i) , x | \u2265 T (i) } removed from S (i) to obtain S (i+1) is at least \u03b3 (Step 9). Since m =\u00d5(1/(\u03b3 2 4\n)), the DKW inequality [DKW56] implies that the true probability mass of this region is at least \u03b3 /2 with high probability. By a union bound over i \u2264 K = \u0398(log(1/ )/( \u03b3)), it follows that with high probability we have that Pr Dx [S (i+1) ] \u2264 (1 \u2212 \u03b3 /2) i for all i \u2208 [K]. After K iterations, we will have that Pr Dx [S (i+1) ] \u2264 /3. Step 3 guarantees that the mass of S (i) underD x is within an additive /3 of its mass under D x , for i \u2208 [K]. This implies that the loop terminates after at most K iterations with high probability.\nBy Lemma 2.3 and the fact that every D (i) has margin \u03b3, it follows that the minimizer of the loss L (i) has value less than \u2212\u03b3(\u03bb \u2212 OPT (i) ) \u2264 \u2212\u03b3 , as OPT (i) \u2264 \u03b7 and \u03bb = \u03b7 + . By the guarantees of Lemma 2.4, running SGD in line 7 on L (i) (\u2022) with projection to the unit 2 -ball for O log(1/\u03b4)/(\u03b3 2 2 ) steps, we obtain a w (i) such that, with probability at least 1 \u2212 \u03b4, it holds L (i) (w (i) ) \u2264 \u2212\u03b3 /2 and w (i) 2 = 1. Here \u03b4 > 0 is a parameter that is selected so that the following claim holds: With probability at least 9/10, for all iterations i of the while loop we have that L (i) (w (i) ) \u2264 \u2212\u03b3 /2. Since the total number of iterations is\u00d5(1/(\u03b3 )), setting \u03b4 to\u03a9( \u03b3) and applying a union bound over all iterations gives the previous claim. Therefore, the total number of SGD steps per iteration is\u00d5(1/(\u03b3 2 2 )). For a given iteration of the while loop, running SGD requires\u00d5(1/(\u03b3 2 2 )) samples from D (i) which translate to at most\u00d5 1/(\u03b3 2 3 ) samples from D, as Pr x\u223cDx x \u2208 S (i) \u2265 2 /3.\nLemma 2.5 implies that there exists T \u2265 0 such that:\n(a) Pr (x,y)\u223cD (i) [| w, x | \u2265 T ] \u2265 \u03b3 , and (b) Pr (x,y)\u223cD (i) [h w (x) = y | w, x | \u2265 T ] \u2264 \u03b7 + .\nLine 9 of Algorithm 1 estimates the threshold using samples. By the DKW inequality [DKW56], we know that with m =\u00d5(1/(\u03b3 2 4 )) samples we can estimate the CDF within error \u03b3 2 with probability 1 \u2212 poly( , \u03b3). This suffices to estimate the probability mass of the region within additive \u03b3 2 and the misclassification error within /3. This is satisfied for all iterations with constant probability. In summary, with high constant success probability, Algorithm 1 runs for\u00d5(1/(\u03b3 )) iterations and draws\u00d5(1/(\u03b3 2 4 )) samples per round for a total of\u00d5(1/(\u03b3 3 5 )) samples. As each iteration runs in polynomial time, the total running time follows.\nWhen the while loop terminates, we have that Pr x\u223cDx [x \u2208 S (i) ] \u2264 4 /3, i.e., we will have accounted for at least a (1 \u2212 4 /3)-fraction of the total probability mass. Since our algorithm achieves misclassification error at most \u03b7 + 4 /3 in all the regions we accounted for, its total misclassification error is at most \u03b7 + 8 /3. Rescaling by a constant factor gives Theorem 2.2.\nRemark 2.6. If the value of OPT is smaller than \u03b7 \u2212 \u03be for some value \u03be > 0, Algorithm 1 gets misclassification error less than \u03b7 \u2212 \u2126(\u03b3 2 \u03be 2 ) when run for = O(\u03b3 2 \u03be 2 ). This is because, in the first iteration, L (1) (w (1) ) \u2264 \u2212\u03b3(\u03bb \u2212 OPT)/2 \u2264 \u2212\u03b3\u03be/2, which implies, by Lemma 2.5, that the obtained error in S (1) is at most \u03bb \u2212 \u03b3\u03be/4. The misclassification error in the remaining regions is at most \u03bb + , and region S (1) has probability mass at least \u03b3\u03be/4. Thus, the total misclassification error is at most \u03bb + \u2212 \u03b3 2 \u03be 2 /16 = \u03b7 \u2212 \u2126(\u03b3 2 \u03be 2 ), when run for = O(\u03b3 2 \u03be 2 ).", "publication_ref": ["b18", "b17", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "The General Case", "text": "In the general case, we assume that D x is an arbitrary distribution supported on b-bit integers. While such a distribution might have exponentially small margin in the dimension d (or even 0), we will preprocess the distribution to ensure a margin condition by removing outliers.\nWe will require the following notion of an outlier: DV04a]). We call a point x in the support of a distribution D x a \u03b2-outlier, if there exists a vector w \u2208 R d such that w,\nDefinition 2.7 ([\nx 2 \u2265 \u03b2 E x\u223cDx [ w, x 2 ].\nWe will use Theorem 3 of [DV04a], which shows that any distribution supported on b-bit integers can be efficiently preprocessed using samples so that no large outliers exist. Given this lemma, we can adapt Algorithm 1 for the large margin case to work in general. The pseudo-code is given in Algorithm 2. It similarly returns a decision list [(w (1) , T (1) , E (1) ), (w (2) , T (2) , E (2) ), \u2022 \u2022 \u2022 ] as output.\nAlgorithm 2 Main Algorithm (general case)\n1: Set S (1) = R d , \u03bb = \u03b7 + , \u0393 \u22121 =\u00d5(db), m =\u00d5( 1 \u0393 2 4 ). 2: Set i \u2190 1. 3: Draw O (1/ 2 ) log(1/( \u0393)\n) samples from D x to form an empirical distributionD x . 4: while Pr x\u223cDx x \u2208 S (i) \u2265 do", "publication_ref": ["b19", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "5:", "text": "Run the algorithm of Lemma 2.8 to remove \u0393 \u22121 -outliers from the distribution D S (i) by filtering points outside the ellipsoid E (i) .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "6:", "text": "Let \u03a3\n(i) = E (x,y)\u223cD (i) | S (i) [xx T ] and set D (i) = \u0393\u03a3 (i)\u22121/2 \u2022 D| S (i) \u2229E (i) be the distribution D| S (i) \u2229E (i)\nbrought in isotropic position and rescaled by \u0393 so that all vectors have 2 -norm at most 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "7:", "text": "Let\nL (i) (w) = E (x,y)\u223cD (i) [LeakyRelu \u03bb (\u2212y w, x )] 8:\nRun SGD on L (i) (w) for\u00d5(1/(\u0393 2 2 )) iterations, to get w (i) with w (i)\n2 = 1 such that L (i) (w (i) ) \u2264 min w: w 2 \u22641 L (i) (w) + \u0393 /2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "9:", "text": "Draw m samples from D (i) to form an empirical distribution D\n(i) m .\n10:\nFind a threshold T (i) such that Pr (x,y)\u223cD (i) m [| w (i) , x | \u2265 T (i) ] \u2265 \u0393 and the empirical misclassification error, Pr (x,y)\u223cD (i) m [h w (x) = y | w (i) , x | \u2265 T (i) ]\n, is minimized.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "11:", "text": "Revert the linear transformation by setting w (i) \u2190 \u0393\u03a3 (i)\u22121/2 w (i) .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "12:", "text": "Update the unclassified region\nS (i+1) \u2190 S (i) \\ {x : x \u2208 E (i) \u2227 | w (i) , x | \u2265 T (i) } and set i \u2190 i + 1. 13: Return the classifier [(w (1) , T (1) , E (1) ), (w (2) , T (2) , E (2) ), \u2022 \u2022 \u2022 ]\nOur main result is the following theorem:\nTheorem 2.9. Let D be a distribution over (d+1)-dimensional labeled examples with bit-complexity b, generated by an unknown halfspace corrupted by Massart noise at rate \u03b7 < 1/2. Algorithm 2 uses\u00d5(d 3 b 3 / 5 ) samples, runs in poly(d, 1/ , b) time, and returns, with probability 2/3, a classifier h with misclassification error err D 0\u22121 (h) \u2264 \u03b7 + .\nWe now analyze Algorithm 2 and establish Theorem 2.9. To do this, we need to adapt Lemma 2.3 to the case without margin. We replace the margin condition by requiring that the minimum eigenvalue of the covariance matrix is at least \u0393.\nLemma 2.10. Let D x be any distribution over points with 2 -norm bounded by 1, with covariance having minimum eigenvalue at least \u0393. If \u03bb \u2265 \u03b7, then min w: w 2 \u22641 L(w) \u2264 \u2212\u0393(\u03bb \u2212 \u03b7).\nProof. We will show the statement for the optimal unit vector w * . For any fixed x, we have that\n(w * , x) = (err(w * , x) \u2212 \u03bb)| w * , x | = (\u03b7(x) \u2212 \u03bb)| w * , x | \u2264 \u2212(\u03bb \u2212 \u03b7)| w * , x |.\nTaking expectation over x drawn from D x , we get the statement as\nE[| w * , x |] \u2265 E[| w * , x | 2 ] \u2265 \u0393,\nwhere we used the fact that for all points x, | w * ,\nx | \u2264 x 2 2 \u2264 1.\nWith Lemma 2.10 in hand, we are ready to prove Theorem 2.9. We will use Lemma 2.4 and Lemma 2.5 whose statements do not require that the distribution of points has large margin.\nProof of Theorem 2.9. We again consider the steps of Algorithm 2 in every iteration i. At every iteration, we consider a distribution D (i) consisting only of points not handled in previous iterations.\nSimilar to the proof of Theorem 2.2, we start by noting that with high probability the total number of iterations is\u00d5(1/(\u0393 )). This is because at every iteration, the empirical probability mass under D (i) m of the region {x : | w (i) , x | \u2265 T (i) } removed from S (i) to obtain S (i+1) is at least \u0393 and thus by the DKW inequality [DKW56] implies the true probability mass of this region is at least \u0393 /2 with high probability. After K = \u0398(log(1/ )/( \u0393)) iterations, we will have that Pr Dx [S (i+1) ] \u2264 /3. Step 3 guarantees that the mass of S (i) underD x is within an additive /3 of its mass under D x , for i \u2208 [K]. This implies that the loop terminates after at most K iterations with high probability.\nAt every iteration, the distribution D (i) is rescaled so that the norm of all points is bounded by 1 and the covariance matrix has minimum eigenvalue \u0393 as guaranteed by Lemma 2.8. By Lemma 2.10, it follows that the minimizer of the loss L (i) has value less than \u2212\u0393(\u03bb \u2212 \u03b7) \u2264 \u2212\u0393 . By the guarantees of Lemma 2.4, running SGD in line 8 on L (i) (\u2022) with projection to the unit 2 -ball for O log(1/\u03b4)/(\u0393 2 2 ) steps, we obtain a w (i) such that, with probability at least 1 \u2212 \u03b4, it holds L (i) (w (i) ) \u2264 \u2212\u0393 /2 and w (i) 2 = 1. Here \u03b4 > 0 is a parameter that is selected so that the following claim holds: With probability at least 9/10, for all iterations i of the while loop we have that L (i) (w (i) ) \u2264 \u2212\u0393 /2. Since the total number of iterations is\u00d5(1/(\u0393 )), setting \u03b4 to\u03a9( \u0393) and applying a union bound over all iterations gives the previous claim. Therefore, the total number of SGD steps per iteration is\u00d5(1/(\u0393 2 2 )). For a given iteration of the while loop, running SGD requires\u00d5(1/(\u0393 2 2 )) samples from D (i) which translate to at most\u00d5 1/(\u0393 2 3 ) samples from D, as Pr x\u223cDx x \u2208 S (i) \u2265 2 /3.\nThen, similar to the proof of Theorem 2.2, Lemma 2.5 implies that there exists a threshold T \u2265 0, such that:\n(a) Pr (x,y)\u223cD (i) [| w, x | \u2265 T ] \u2265 \u0393 , and (b) Pr (x,y)\u223cD (i) [h w (x) = y | w, x | \u2265 T ] \u2264 \u03b7 + .\nLine 10 of Algorithm 2 estimates the threshold using samples. By the DKW inequality [DKW56], we know that with m =\u00d5( 1 \u0393 2 4 ) samples we can estimate the CDF within error \u0393 2 with probability 1 \u2212 poly( , \u0393). This suffices to estimate the probability mass of the region within additive \u0393 2 and the misclassification error within /3. This is satisfied for all iterations with constant probability.\nIn summary, with high constant success probability, Algorithm 2 runs for\u00d5(1/(\u0393 )) iterations and draws\u00d5(1/(\u0393 2 4 )) samples per round for a total of\u00d5(1/(\u0393 3 5 )) samples. As each iteration runs in polynomial time, the total running time follows.\nWhen the while loop terminates, we have that Pr x\u223cDx [x \u2208 S (i) ] \u2264 4 /3, i.e., we will have accounted for at least a (1 \u2212 4 /3)-fraction of the total probability mass. Since our algorithm achieves misclassification error at most \u03b7 + 4 /3 in all the regions we accounted for, its total misclassification error is at most \u03b7 + 8 /3. Rescaling by a constant factor gives Theorem 2.9.", "publication_ref": ["b17", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Lower Bounds Against Natural Approaches", "text": "In this section, we show that certain natural approaches for learning halfspaces with Massart noise inherently fail, even in the large margin case.\nWe begin in Section 3.1 by showing that the common approach of using a convex surrogate function for the 0-1 loss cannot lead to non-trivial misclassification error. (We remark that this comes in sharp contrast with the problem of learning large margin halfpaces with RCN, where a convex surrogate works, see, e.g., Theorem A.1 in Section A).\nIn Section 3.2, we provide evidence that improving the misclassification guarantee of \u03b7 + achieved by our algorithm requires a genuinely different approach. In particular, we show that the approach of iteratively using any convex proxy followed by thresholding gets stuck at error \u2126(\u03b7)+ , even in the large margin case.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Lower Bounds Against Minimizing a Convex Surrogate Function", "text": "One of the most common approaches in machine learning is to replace the 0-1 loss in the ERM by an appropriate convex surrogate and solve the corresponding convex optimization problem. In this section, we show that this approach inherently fails to even give a weak learner in the presence of Massart noise -even under a margin assumption.\nIn more detail, we construct distributions over a finite sets of points in the two-dimensional unit ball for which the method of minimizing a convex surrogate will always have misclassification error min{1/2, \u0398(\u03b7/\u03b3)}, where \u03b3 is the maximum margin with respect to any hyperplane. Our proof is inspired by an analogous construction in [LS10], which shows that one cannot achieve non-trivial misclassification error for learning halfspaces in the presence of RCN, using certain convex boosting techniques. Our argument is more involved in the sense that we need to distinguish two cases and consider different distributions for each one. Furthermore, by leveraging the additional strength of the Massart noise model, we are able to show that the misclassification error has to be larger than the noise level \u03b7 by a factor of 1/\u03b3.\nIn particular, our first case corresponds to the situation where the convex surrogate function is such that misclassified points are penalized by a fair amount and therefore the effect of noise of correctly classified points on the gradient is significant. This allows a significant amount of probability mass to be in the region where the true separating hyperplane and the one defined by the minimum of the convex surrogate function disagree. The second case, which is the complement of the first one, uses the fact that the contribution of a correctly classified point on the gradient is not much smaller than that of a misclassified point, again allowing a significant amount of probability mass to be given to the aforementioned disagreement region. Formally, we prove the following: Theorem 3.1. Consider the family of algorithms that produce a classifier sign( w * , x ), where w * is the minimum of the function G\n(w) = E (x,y)\u223cD [\u03c6(y w, x )]. For any decreasing convex 1 function \u03c6 : R \u2192 R, there exists a distribution D over B 2 \u00d7 {\u00b11}with margin \u03b3 \u2264 \u221a 3\u22121 4\nsuch that the classifier sign( w * , x ), misclassifies a min{ \u03b7 8\u03b3 , 1 2 } fraction of the points.\nProof. We consider algorithms that perform ERM with a convex surrogate, i.e., minimize a loss of the form G(w) = E (x,y)\u223cD [\u03c6(y w, x )], for some convex function \u03c6 : R \u2192 R for w 2 \u2264 1. We can assume without loss of generality that \u03c6 is differentiable and its derivative is non-decreasing. Even if there is a countable number of points in which it is not, there is a subderivative that we can pick for each of those points such that the derivative is increasing overall, since we have assumed that \u03c6 is convex. Therefore, our argument still goes through even without assuming differentiability. We start by calculating the gradient of G as a function of the derivative of \u03c6 at the minimum of G. Suppose that v \u2208 R d is the minimizer of G subject to w 2 \u2264 1. This requires that either \u2207G(v) is parallel to v, in case the unconstrained minimum lies outside the region w 2 \u2264 1, or \u2207G(v) = 0. Therefore, we have that for every i > 1, the following holds:\n\u2202G \u2202w i (v) = E (x,y)\u223cD [\u03c6 (y v, x )(yx i )] = 0 .\nOur lower bound construction produces a distribution D over (x, y) whose x marginal, D x , is supported on the 2-dimensional unit ball. We need to consider two complementary cases for the convex function \u03c6. For each case, we will define judiciously chosen distributions, D 1 , D 2 for which the result holds. ). We need to pick the parameter p so that v = e 1 is the minimum of G(w).\nNote that the misclassification error is err\nD 1 0\u22121 (sign( v, x )) = p + (1 \u2212 p) \u2022 \u03b7.\nThe condition that v = e 1 is a minimizer of G(w) is equivalent to E (x,y)\u223cD 1 [\u03c6 (y v, x )(yx 2 )] = 0. Substituting for our choice of D 1 with noise level \u03b7 on (z, \u2212\u03b3) and 0 on (z, \u221a 1 \u2212 z 2 ), we get:\np \u2022 \u03c6 (\u2212z) \u2022 \u03b3 + (1 \u2212 p) \u2022 (1 \u2212 \u03b7)\u03c6 (z) \u2022 1 \u2212 z 2 + (1 \u2212 p) \u2022 \u03b7 \u2022 \u03c6 (\u2212z) \u2022 (\u2212 1 \u2212 z 2 ) = 0 .\nEquivalently, we have:\n(1 \u2212 p) \u2022 \u03b7 \u2022 |\u03c6 (\u2212z)| \u2022 1 \u2212 z 2 = p \u2022 \u03b3 \u2022 |\u03c6 (\u2212z)| + (1 \u2212 p) \u2022 (1 \u2212 \u03b7)|\u03c6 (z)| 1 \u2212 z 2 .\nNow, suppose that |\u03c6 (z)| = (1 \u2212 \u03b1) \u03b7 1\u2212\u03b7 |\u03c6 (\u2212z)|, for some \u03b1 > 1 2 . By substituting and simplifying, we get:\np \u2022 \u03b3 = \u03b1(1 \u2212 p)\u03b7 1 \u2212 z 2 = (1 \u2212 p)\u03b7\u2206 ,\nwhere \u2206 = \u03b1 \u221a 1 \u2212 z 2 , which in turns gives that p = \u03b7\u2206 \u03b3 + \u03b7\u2206 .\nThus, the misclassification error is\nerr D 1 0\u22121 (sign( v, x )) = p + (1 \u2212 p)\u03b7 = \u03b7 + (1 \u2212 \u03b7)p = \u03b7 + (1 \u2212 \u03b7)\u03b7\u2206 \u03b3 + \u03b7\u2206 = \u03b7(\u03b3 + \u2206) \u03b3 + \u03b7\u2206 \u2265 1 1 + \u03b3 \u03b7\u2206 .\nNote that for margin \u03b3 \u2264 \u03b7 \u2022 \u2206, we have that err D 1 0\u22121 (sign( v, x )) \u2265 1 2 , and we can achieve error exactly 1 2 by setting the point Q 1 at distance exactly \u03b7 \u2022 \u2206. On the other hand, when the margin is \u03b3 \u2264 \u03b7 \u2022 \u2206, we have:\nerr D 1 0\u22121 (sign( v, x )) \u2265 \u03b7\u2206 2\u03b3 \u2265 \u03b7 8\u03b3 .\nThe last inequality comes from the fact that \u2206 = \u03b1 \u221a 1 \u2212 z 2 \u2265 1/4, since \u03b1 \u2265 1/2 and z \u2264 \u221a 3/2.\nCase II: For all z \u2208 [0, \u221a 3/2] we have that |\u03c6 (z\n)| \u2265 1 2 \u03b7 1\u2212\u03b7 |\u03c6 (\u2212z)|.\nIn this case, we consider the distribution shown in Figure 1 (right), where the only points that have non-zero mass are: (0, \u22122\u03b3), which has probability mass p, and (1/2, \u2212r), with mass 1 \u2212 p. We need to appropriately select the parameters p and r, so that v is actually the minimizer of the function G(w), and the misclassification error (which is equal to p in this case) is maximized. Note that v satisfies E (x,y)\u223cD 2 [\u03c6 (y v, x )(y \u2022 x 2 )] = 0. Substituting for this particular distribution D 2 with noise level 0 on both points, we get:\np \u2022 \u03c6 (0) \u2022 (2\u03b3) + (1 \u2212 p)\u03c6 (1/2) \u2022 (\u2212r) = 0 .\nSince \u03c6 is monotone, we get:\np|\u03c6 (0)| \u2022 (2\u03b3) = (1 \u2212 p)|\u03c6 (1/2)| \u2022 r .\nBy rearranging, we get:\np = |\u03c6 (1/2)| \u2022 r |\u03c6 (1/2)| \u2022 r + 2\u03b3|\u03c6 (0)| .\nBy the definition of Case II and the fact that \u03c6 is decreasing and convex, we have that:\n|\u03c6 (1/2)| \u2265 (\u03b7/2)|\u03c6 (\u22121/2)| \u2265 (\u03b7/2)|\u03c6 (0)| .\nTherefore, we can get misclassification error:\nerr D 2 0\u22121 (sign( v, x )) = p \u2265 |\u03c6 (1/2)| \u2022 r |\u03c6 (1/2)| \u2022 r + 4\u03b3 \u03b7 |\u03c6 (1/2)| = 1 1 + 4\u03b3 \u03b7r .\nWe note that r must be chosen within the interval 0, \u221a 3/2 \u2212 2\u03b3 , so that the \u03b3-margin requirement is satisfied.\nFor margin \u221a 3\u22121 4 \u03b3 \u2264 \u03b7r 4 , we get err D 2 0\u22121 (sign( v, x )) > 1/2, and we can achieve error exactly 1/2 by moving the probability mass p from Q 1 (0, \u22122\u03b3) to Q 3 (0, \u2212 \u03b7r 2 ). If \u03b3 \u2265 \u03b7r 4 , then err D 2 0\u22121 (sign( v, x )) \u2265 \u03b7r 4\u03b3 \u2265 \u03b7r 8\u03b3 . The last inequality comes from the fact that we can pick r = 1/2 \u2264 \u221a 3/2 \u2212 2\u03b3. This completes the proof of Theorem 3.1.", "publication_ref": ["b30"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Lower Bound Against Convex Surrogate Minimization Plus Thresholding", "text": "The lower bound established in the previous subsection does not preclude the possibility that our algorithmic approach in Section 2 giving misclassification error \u2248 \u03b7 can be improved by replacing the LeakyRelu function by a different convex surrogate. In this section, we prove that using a different convex surrogate in our thresholding approach indeed does not help.\nThat is, we show that any approach which attempts to obtain an accurate classifier by considering a thresholded region cannot get misclassification error better than \u2126(\u03b7) within that region, i.e., the bound of our algorithm cannot be improved with this approach. Formally, we prove: Proof. Our proof proceeds along the same lines as the proof of Theorem 3.1, but with some crucial modifications. In particular, we argue that Case I above remains unchanged but Case II requires a different construction.\nFirstly, we note that the points Q 1 , Q 2 in Case I are the only points that are assigned non-zero mass by the distribution and they are at equal distance z from the output classifier's hyperplane. Therefore, any set of the form 1 v,x >T , where v is the unit vector perpendicular to the hyperplane, will either contain the entire probability mass or 0 mass. Thus, for all the meaningful choices of the threshold T , we get the same misclassification error as with T = 0. This means that the example distribution and the analysis for Case I remain unchanged.\nHowever, Case II in the proof of Theorem 3.1 requires modification as the points Q 1 , Q 2 are at different distances from the classifier's hyperplane.\nHere we will restrict our attention to the case where the distances of the two points from the classifier's hyperplane are actually equal and get a lower bound nearly matching the upper bound in Section 2. This lower bound applies, due to reasons explained above, to all approaches that use a combination of minimizing a convex surrogate function and thresholding.\nModified Case II: We recall that in this case the following assumption on the function \u03c6 holds: For all z \u2208 [0,\n\u221a 3/2] it holds |\u03c6 (z)| \u2265 1 2 \u03b7 1\u2212\u03b7 |\u03c6 (\u2212z)|.\nThe new distribution D 2 is going to be as shown in Figure 2. That is, we assign mass p on the point Q 1 (1/4, \u221a 3/4 + 2\u03b3) and mass 1 \u2212 p on the point Q 2 (1/4, \u221a 3/4 \u2212 2\u03b3). Similarly to the previous section, we use the equation:\nE (x,y)\u223cD [\u03c6 (y v, x )(y \u2022 x 2 )] = 0, that holds for v being the minimum of G(w) = E (x,y)\u223cD [\u03c6(y w, x )], to get: p \u2022 \u03c6 (\u22121/4) \u2022 \u221a 3/4 + 2\u03b3 + (1 \u2212 p) \u2022 \u03c6 (1/4) \u2022 \u2212 \u2265 \u221a 3/4 \u2212 2\u03b3 \u221a 3/4 + 2\u03b3 \u2022 1 1 + 2(1\u2212\u03b7) \u03b7 \u2265 1 \u2212 8\u03b3 \u221a 3/3 \u03b7 4(1 \u2212 \u03b7)\n. ", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Local min Local min True True", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusions", "text": "The main contribution of this paper is the first non-trivial learning algorithm for the class of halfspaces (or even disjunctions) in the distribution-free PAC model with Massart noise. Our algorithm achieves misclassification error \u03b7 + in time poly(d, 1/ ), where \u03b7 < 1/2 is an upper bound on the Massart noise rate.\nThe most obvious open problem is whether this error guarantee can be improved to f (OPT) + (for some function f : R \u2192 R such that lim x\u21920 f (x) = 0) or, ideally, to OPT + . It follows from our lower bound constructions that such an improvement would require new algorithmic ideas. It is a plausible conjecture that obtaining better error guarantees is computationally intractable. This is left as an interesting open problem for future work. Another open question is whether there is an efficient proper learner matching the error guarantees of our algorithm. We believe that this is possible, building on the ideas in [DV04b], but we did not pursue this direction.\nMore broadly, what other concept classes admit non-trivial algorithms in the Massart noise model? Can one establish non-trivial reductions between the Massart noise model and the agnostic model? And are there other natural semi-random input models that allow for efficient PAC learning algorithms in the distribution-free setting?", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Learning Large-Margin Halfspaces with RCN", "text": "In this section, we show that the problem of learning \u03b3-margin halfspaces in the presence of RCN can be formulated as a convex optimization problem that can be efficiently solved with any firstorder method. Prior work by Bylander [Byl94] used a variant of the Perceptron algorithm to learn \u03b3-margin halfspaces with RCN. To the best of our knowledge, the result of this section is not explicit in prior work.\nIn order to avoid problems that would arise if the distribution D is degenerate (i.e., it assigns non-zero mass on a lower dimensional subspace), we introduce Gaussian noise to the points of the distribution. That is, we sample points x + r, where r \u223c N (0, c 2 I) and c \n.\nIn particular, we will show that solving the following convex optimization problem: \nminimize\nfor \u03bb \u03b7 + c \u221a 2\u03c0 \u2248 \u03b7 suffices to solve this learning problem. Intuitively, the idea here is that by adding the right amount of noise r, we make sure that: (a) the probability that the true halfspace misclassifies the noisy version of a point x is negligible, and (b) if a point is misclassified by the current halfspace, then it has, on average, a significant contribution to the objective function. Therefore, any solution with sufficiently small value yields a halfspace misclassifying a small fraction of points.\nAs in Section 2.1, we choose the parameter \u03bb for the LeakyRelu function such that G \u03bb (w) has a slightly negative minimum. This is done in order to avoid w = 0 being the minimizer of the function G \u03bb (w). The minimizer for the convex region w 2 \u2264 1 will instead lie in the (non-convex) set w 2 = 1.\nWe can solve Problem (1) with a standard first-order method through samples using SGD. Formally, we show the following: Theorem A.1. Let D be a distribution over (d + 1)-dimensional labeled examples obtained by an unknown \u03b3-margin halfspace corrupted with RCN at rate \u03b7 < 1/2. An application of SGD on G \u03bb (w) using\u00d5(1/( 2 \u03b3 4 )) samples returns, with probability 2/3, a halfspace with misclassification error at most \u03b7 + .\nThe rest of this section is devoted to the proof of Theorem A.1.\nWe consider the contribution to the objective G \u03bb of a single point x, denoted by G \u03bb (w, x). That is, we define G \u03bb (w, The proof of the claim follows similarly to the proof of Claim 2.1 and is omitted. Given this decomposition, we move on to show that G \u03bb (w * , x) is sufficiently negative for any x and provide a lower bound on G \u03bb (w, x) for any unit vector w. Proof. To bound the second term in Claim A.2, we note that for any x, w, we have that\nE r\u223cN (0,c 2 I) | w, x + r | \u2264 1 + c \u2264 2 .\nTo bound the first term, note that for any x such that h w (x) = h w * (x), it holds From (2) we obtain that the above is at most \u03b7 + . This completes the proof of Theorem A.1.\nE", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Efficient learning of linear separators under bounded noise", "journal": "", "year": "2015", "authors": "P Awasthi; M F Balcan; N Haghtalab; R Urner"}, {"ref_id": "b1", "title": "Learning and 1-bit compressed sensing under asymmetric noise", "journal": "", "year": "2016", "authors": "P Awasthi; M F Balcan; N Haghtalab; H Zhang"}, {"ref_id": "b2", "title": "The power of localization for efficiently learning linear separators with noise", "journal": "J. ACM", "year": "2017", "authors": "P Awasthi; M F Balcan; P M Long"}, {"ref_id": "b3", "title": "Learning from noisy examples", "journal": "Mach. Learn", "year": "1988", "authors": "D Angluin; P Laird"}, {"ref_id": "b4", "title": "Robust estimators are hard to compute", "journal": "", "year": "2006", "authors": "T Bernholt"}, {"ref_id": "b5", "title": "A polynomial-time algorithm for learning noisy linear threshold functions", "journal": "", "year": "1996", "authors": "A Blum; A M Frieze; R Kannan; S Vempala"}, {"ref_id": "b6", "title": "A polynomial time algorithm for learning noisy linear threshold functions", "journal": "Algorithmica", "year": "1997", "authors": "A Blum; A Frieze; R Kannan; S Vempala"}, {"ref_id": "b7", "title": "Machine learning: My favorite results, directions, and open problems", "journal": "", "year": "2003", "authors": "A Blum"}, {"ref_id": "b8", "title": "Learning linear threshold functions in the presence of classification noise", "journal": "", "year": "1994", "authors": "T Bylander"}, {"ref_id": "b9", "title": "Learning noisy perceptrons by a perceptron in polynomial time", "journal": "", "year": "1997", "authors": "E Cohen"}, {"ref_id": "b10", "title": "Complexity theoretic limitations on learning halfspaces", "journal": "", "year": "2016", "authors": "A Daniely"}, {"ref_id": "b11", "title": "Robust estimators in high dimensions without the computational intractability", "journal": "", "year": "2016", "authors": "I Diakonikolas; G Kamath; D M Kane; J Li; A Moitra; A Stewart"}, {"ref_id": "b12", "title": "Being robust (in high dimensions) can be practical", "journal": "", "year": "2017", "authors": "I Diakonikolas; G Kamath; D M Kane; J Li; A Moitra; A Stewart"}, {"ref_id": "b13", "title": "Robustly learning a gaussian: Getting optimal error, efficiently", "journal": "", "year": "2018", "authors": "I Diakonikolas; G Kamath; D M Kane; J Li; A Moitra; A Stewart"}, {"ref_id": "b14", "title": "Sever: A robust meta-algorithm for stochastic optimization", "journal": "", "year": "2019", "authors": "I Diakonikolas; G Kamath; D Kane; J Li; J Steinhardt; Alistair Stewart"}, {"ref_id": "b15", "title": "Learning geometric concepts with nasty noise", "journal": "", "year": "2018", "authors": "I Diakonikolas; D M Kane; A Stewart"}, {"ref_id": "b16", "title": "Efficient algorithms and lower bounds for robust linear regression", "journal": "", "year": "2019", "authors": "I Diakonikolas; W Kong; A Stewart"}, {"ref_id": "b17", "title": "Asymptotic minimax character of the sample distribution function and of the classical multinomial estimator", "journal": "Ann. Mathematical Statistics", "year": "1956", "authors": "A Dvoretzky; J Kiefer; J Wolfowitz"}, {"ref_id": "b18", "title": "Introductory lectures on stochastic convex optimization. Park City Mathematics Institute", "journal": "", "year": "2016", "authors": "J C Duchi"}, {"ref_id": "b19", "title": "Optimal outlier removal in high-dimensional spaces", "journal": "J. Computer & System Sciences", "year": "2004", "authors": "J Dunagan; S Vempala"}, {"ref_id": "b20", "title": "A simple polynomial-time rescaling algorithm for solving linear programs", "journal": "", "year": "2004", "authors": "J Dunagan; S Vempala"}, {"ref_id": "b21", "title": "New results for learning noisy parities and halfspaces", "journal": "", "year": "2006", "authors": "V Feldman; P Gopalan; S Khot; A Ponnuswami"}, {"ref_id": "b22", "title": "Hardness of learning halfspaces with noise", "journal": "IEEE Computer Society", "year": "2006", "authors": "V Guruswami; P Raghavendra"}, {"ref_id": "b23", "title": "Decision theoretic generalizations of the PAC model for neural net and other learning applications", "journal": "Information and Computation", "year": "1992", "authors": "D Haussler"}, {"ref_id": "b24", "title": "Efficient noise-tolerant learning from statistical queries", "journal": "", "year": "1993", "authors": "M J Kearns"}, {"ref_id": "b25", "title": "Efficient noise-tolerant learning from statistical queries", "journal": "Journal of the ACM", "year": "1998", "authors": "M J Kearns"}, {"ref_id": "b26", "title": "Efficient algorithms for outlier-robust regression", "journal": "", "year": "2018", "authors": "A R Klivans; P K Kothari; R Meka"}, {"ref_id": "b27", "title": "Learning halfspaces with malicious noise", "journal": "", "year": "2009", "authors": "A Klivans; P Long; R Servedio"}, {"ref_id": "b28", "title": "Toward Efficient Agnostic Learning", "journal": "", "year": "1994", "authors": "M Kearns; R Schapire; L Sellie"}, {"ref_id": "b29", "title": "Agnostic estimation of mean and covariance", "journal": "", "year": "2016", "authors": "K A Lai; A B Rao; S Vempala"}, {"ref_id": "b30", "title": "Random classification noise defeats all convex potential boosters", "journal": "", "year": "2010", "authors": "P M Long; R A Servedio"}, {"ref_id": "b31", "title": "Risk bounds for statistical learning", "journal": "Ann. Statist", "year": "2006", "authors": "P Massart; E Nedelec"}, {"ref_id": "b32", "title": "How fast can a threshold gate learn", "journal": "MIT Press", "year": "1994", "authors": "W Maass; G Turan"}, {"ref_id": "b33", "title": "The Perceptron: a probabilistic model for information storage and organization in the brain", "journal": "Psychological Review", "year": "1958", "authors": "F Rosenblatt"}, {"ref_id": "b34", "title": "A formal model of hierarchical concept learning", "journal": "Information and Computation", "year": "1994", "authors": "R Rivest; R Sloan"}, {"ref_id": "b35", "title": "Types of noise in data for concept learning", "journal": "Morgan Kaufmann Publishers Inc", "year": "1988", "authors": "R H Sloan"}, {"ref_id": "b36", "title": "Corrigendum to types of noise in data for concept learning", "journal": "", "year": "1992", "authors": "R H Sloan"}, {"ref_id": "b37", "title": "Pac Learning, Noise, and Geometry", "journal": "Birkh\u00e4user Boston", "year": "1996", "authors": "R H Sloan"}, {"ref_id": "b38", "title": "A theory of the learnable", "journal": "ACM Press", "year": "1984", "authors": "L G Valiant"}, {"ref_id": "b39", "title": "Estimation of Dependences Based on Empirical Data: Springer Series in Statistics", "journal": "Springer-Verlag", "year": "1982", "authors": "V Vapnik"}, {"ref_id": "b40", "title": "Revisiting perceptron: Efficient and label-optimal learning of halfspaces", "journal": "", "year": "2017", "authors": "S Yan; C Zhang"}, {"ref_id": "b41", "title": "A hitting time analysis of stochastic gradient langevin dynamics", "journal": "", "year": "1980", "authors": "Y Zhang; P Liang; M Charikar"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Claim 2.1. For any w, x, we have that (w, x) = (err(w, x) \u2212 \u03bb)| w, x |.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Lemma 2. 8 (8Rephrasing of Theorem 3 of [DV04a]). Using m =\u00d5(d 2 b) samples from D x , one can identify with high probability an ellipsoid E such that Pr x\u223cDx [x \u2208 E] \u2265 1 2 and D x | E has no \u0393 \u22121 =\u00d5(db)-outliers.", "figure_data": ""}, {"figure_label": ":", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Case I ::There exists z \u2208 [0, \u221a 3/2] such that: |\u03c6 (z)| < 1 2 \u03b7 1\u2212\u03b7 |\u03c6 (\u2212z)|. In this case, we consider the distribution shown in Figure 1 (left), where the point (z, \u2212\u03b3) has probability mass p and the remaining 1 \u2212 p mass in on the point (z, \u221a 1 \u2212 z 2", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 1 :1Figure 1: Probability distribution for Case I is on the left and for the complementary Case II is on the right.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Theorem 3. 2 .2Consider the family of algorithms that produce a classifier sign( w * , x ), where w * is the minimizer of the function G(w) = E (x,y)\u223cD [\u03c6(y w, x )]. For any decreasing convex function \u03c6 : R \u2192 R, there exists a distribution D over B 2 \u00d7 {\u00b11}with margin \u03b3 \u2264 \u221a 3/8 such that the classifier sign( w * , x ) misclassifies a (1 \u2212 O(\u03b3)) \u2022 \u2126(\u03b7) fraction of the points x that lie in the region {x : w, x > T } for any threshold T .", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 2 :2Figure 2: Probability Distribution for Modified Case II.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "w 2 \u22641 G \u03bb (w) = E (x,y)\u223cD E r\u223cN (0,c 2 I) [LeakyRelu \u03bb (\u2212y w, x + r )] ,", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "x) = E y\u223cDy(x) [E r\u223cN (0,c 2 I) [LeakyRelu \u03bb (\u2212y w, x+r )]] and write G \u03bb (w) = E x\u223cDx [G \u03bb (w, x)].We start with the following claim:Claim A.2. G \u03bb (w, x) can be rewritten as:(1 \u2212 2\u03b7) \u2022 E r\u223cN (0,c 2 I) | w, x + r |1 hw(x+r) =h w * (x) \u2212 (\u03bb \u2212 \u03b7) \u2022 E r\u223cN (0,c 2 I) | w, x + r | .", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Lemma A. 3 .3For any x such that | w * , x | \u2265 \u03b3, it holdsG \u03bb (w * , x) \u2264 \u2212(\u03bb \u2212 \u03b7)\u03b3/2 = \u2212\u03a9(\u03b3 2 ) .Proof. For any x such that | w * , x | \u2265 \u03b3, we have thatE r\u223cN (0,c 2 I) | w * , x + r | \u2265 | w * , x + E r\u223cN (0,c 2 I) [r] | \u2265 \u03b3.Thus, it suffices to show that:E r\u223cN (0,c 2 I) | w * , x + r |1 h w * (x+r) =h w * (x) \u2264 (\u03bb \u2212 \u03b7)\u03b3/2 .We have thatE r\u223cN (0,c 2 I) | w * , x + r |1 h w * (x+r) =h w * (x) \u2264 E r\u223cN (0,c 2 ) r1 r\u2265\u03b3 = c \u221a 2\u03c0 exp(\u2212(\u03b3/c) 2 /2) .The choice of c, implies that c\u221a 2\u03c0 exp(\u2212(\u03b3/c) 2 /2) = c \u221a 2\u03c0 \u03b3/2 = (\u03bb \u2212 \u03b7)\u03b3/2. Lemma A.4. For any unit vectors w, x, it holds G \u03bb (w, x) \u2265 2c \u221a 2\u03c0 (1 \u2212 2\u03b7)1 hw(x) =h w * (x) \u2212 .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "r\u223cN (0,c 2 I) | w, x + r |1 hw(x+r) =h w * (x) \u2265 E r\u223cN (0,c 2 ) r1 r\u22650 \u2265 2c \u221a 2\u03c0 .Combining the above gives Lemma A.4.Proof of Theorem A.1. Taking expectation in Lemma A.3, we get that G \u03bb (w * ) \u2264 \u2212\u03a9(\u03b3 2 ). From the guarantees of SGD (Lemma 2.4), running SGD with\u00d5(1/( 2 \u03b3 4 )) iterations and samples gives a point w where G \u03bb (w) \u2264 G \u03bb (w * ) + O( \u03b3 2 ) \u2264 0. Furthermore, taking expectation in Lemma A.4, we obtain that(1 \u2212 2\u03b7)Pr x\u223cDx [h w (x) = h w * (x)] \u2264 .(2)Overall, the misclassification error of h w is equal to(1 \u2212 \u03b7)Pr x\u223cDx [h w (x) = h w * (x)] + \u03b7(1 \u2212 Pr x\u223cDx [h w (x) = h w * (x)]) = \u03b7 + (1 \u2212 2\u03b7)Pr x\u223cDx [h w (x) = h w * (x)] .", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "Theorem 1.2 (Main Result).", "formula_coordinates": [3.0, 64.8, 119.45, 141.3, 9.6]}, {"formula_id": "formula_1", "formula_text": "D 0\u22121 (h) = Pr (x,y)\u223cD [h(x) = y]. Let OPT = min h\u2208H d err D 0\u22121 (h)", "formula_coordinates": [6.0, 94.05, 259.5, 300.67, 13.87]}, {"formula_id": "formula_2", "formula_text": "L(w) = E (x,y)\u223cD [LeakyRelu \u03bb (\u2212y w, x )] , under the constraint w 2 \u2264 1, where LeakyRelu \u03bb (z) = (1 \u2212 \u03bb)z if z \u2265 0 \u03bbz if z < 0", "formula_coordinates": [6.0, 64.8, 563.55, 365.22, 49.47]}, {"formula_id": "formula_3", "formula_text": "x) = E y\u223cDy(x) [LeakyRelu \u03bb (\u2212y w, x )] respectively. Notice that err D 0\u22121 (h w ) = E x\u223cDx [err(w, x)] and L(w) = E x\u223cDx [ (w, x)]. Moreover, OPT = E x\u223cDx [err(w * , x)] = E x\u223cDx [\u03b7(x)].", "formula_coordinates": [6.0, 64.8, 643.16, 468.0, 39.05]}, {"formula_id": "formula_4", "formula_text": "x) = \u03b7(x)(1 \u2212 \u03bb)| w, x | \u2212 (1 \u2212 \u03b7(x))\u03bb| w, x | = (\u03b7(x) \u2212 \u03bb)| w, x |.", "formula_coordinates": [7.0, 75.71, 117.98, 457.09, 23.15]}, {"formula_id": "formula_5", "formula_text": "x) = 1 \u2212 \u03b7(x), while (w, x) = (1 \u2212 \u03b7(x))(1 \u2212 \u03bb)| w, x | \u2212 \u03b7(x)\u03bb| w, x | = (1 \u2212 \u03b7(x) \u2212 \u03bb)| w, x |.", "formula_coordinates": [7.0, 80.25, 153.37, 452.55, 23.15]}, {"formula_id": "formula_6", "formula_text": "1: Set S (1) = R d , \u03bb = \u03b7 + , m =\u00d5( 1 \u03b3 2 4 ). 2: Set i \u2190 1. 3: Draw O (1/ 2 ) log(1/( \u03b3)) samples from D x to form an empirical distributionD x . 4: while Pr x\u223cDx x \u2208 S (i) \u2265 do 5: Set D (i) = D| S (i)", "formula_coordinates": [7.0, 70.72, 520.69, 411.37, 71.74]}, {"formula_id": "formula_7", "formula_text": "L (i) (w) = E (x,y)\u223cD (i) [LeakyRelu \u03bb (\u2212y w, x )] 7:", "formula_coordinates": [7.0, 70.72, 591.96, 261.79, 27.25]}, {"formula_id": "formula_8", "formula_text": "(i) m . 9: Find a threshold T (i) such that Pr (x,y)\u223cD (i) m [| w (i) , x | \u2265 T (i) ] \u2265 \u03b3 and the empirical mis- classification error, Pr (x,y)\u223cD (i) m [h w (i) (x) = y | w (i) , x | \u2265 T (i) ]", "formula_coordinates": [7.0, 70.72, 636.76, 462.08, 50.02]}, {"formula_id": "formula_9", "formula_text": "11: Return the classifier [(w (1) , T (1) ), (w (2) , T (2) ), \u2022 \u2022 \u2022 ]", "formula_coordinates": [7.0, 66.12, 704.16, 253.45, 11.52]}, {"formula_id": "formula_10", "formula_text": "Lemma 2.3. If \u03bb \u2265 \u03b7, then L(w * ) \u2264 \u2212\u03b3(\u03bb \u2212 OPT).", "formula_coordinates": [8.0, 64.8, 222.33, 253.13, 11.52]}, {"formula_id": "formula_11", "formula_text": "(w * , x) = (err(w * , x) \u2212 \u03bb)| w * , x | = (\u03b7(x) \u2212 \u03bb)| w * , x | \u2264 \u2212\u03b3(\u03bb \u2212 \u03b7(x)) ,", "formula_coordinates": [8.0, 125.07, 268.12, 352.01, 12.06]}, {"formula_id": "formula_12", "formula_text": "w: w 2 \u22641 w \u2212 w (t+ 1 2 ) 2", "formula_coordinates": [8.0, 343.72, 456.71, 112.92, 23.29]}, {"formula_id": "formula_13", "formula_text": "Pr (x,y)\u223cD [| w, x | \u2265 T ] \u2265 |L(w)|", "formula_coordinates": [8.0, 64.8, 710.04, 149.9, 14.58]}, {"formula_id": "formula_14", "formula_text": "Pr (x,y)\u223cD [h w (x) = y | w, x | \u2265 T ] \u2264 \u03bb \u2212 |L(w)| 2 .", "formula_coordinates": [8.0, 263.53, 710.04, 230.46, 16.13]}, {"formula_id": "formula_15", "formula_text": "(x,y)\u223cD [h w (x) = y | w, x | \u2265 T ] \u2264 \u03bb \u2212 \u03b6, where \u03b6 def = |L(w)|/2, or equivalently, E x\u223cDx [(err(w, x) \u2212 \u03bb + \u03b6)1 | w,x", "formula_coordinates": [9.0, 64.8, 76.34, 467.99, 29.27]}, {"formula_id": "formula_16", "formula_text": "1 0 E x\u223cDx [(err(w, x) \u2212 \u03bb + \u03b6)1 | w,x |\u2265T ]dT = E x\u223cDx [(err(w, x) \u2212 \u03bb)| w, x |] + \u03b6E x\u223cDx [| w, x |] \u2264 E x\u223cDx [ (w, x)] + \u03b6 = L(w) + \u03b6 = L(w)/2 < 0 ,", "formula_coordinates": [9.0, 82.27, 129.72, 439.13, 43.73]}, {"formula_id": "formula_17", "formula_text": "E x\u223cDx [(err(w, x) \u2212 \u03bb + \u03b6)1 | w,x |\u2265T ] \u2264 0 .", "formula_coordinates": [9.0, 199.03, 211.67, 199.54, 11.86]}, {"formula_id": "formula_18", "formula_text": "1 T E x\u223cDx [(err(w, x) \u2212 \u03bb + \u03b6)1 | w,x |\u2265T ]dT \u2265 \u2212\u03bb \u2022 Pr (x,y)\u223cD [| w, x | \u2265T ] .", "formula_coordinates": [9.0, 125.98, 256.45, 351.7, 28.58]}, {"formula_id": "formula_19", "formula_text": "T 0 E x\u223cDx [(err(w, x) \u2212 \u03bb + \u03b6)1 | w,x |\u2265T ]dT \u2265 0 . Therefore, L(w) 2 \u2265 1 T E x\u223cDx [(err(w, x) \u2212 \u03bb + \u03b6)1 | w,x |\u2265T ]dT \u2265 \u2212\u03bb \u2022 Pr (x,y)\u223cD [| w, x | \u2265T ] , which implies that Pr (x,y)\u223cD [| w, x | \u2265T ] \u2265 |L(w)| 2\u03bb .", "formula_coordinates": [9.0, 64.8, 317.5, 433.92, 117.86]}, {"formula_id": "formula_20", "formula_text": "(i) m of the region {x : | w (i) , x | \u2265 T (i) } removed from S (i) to obtain S (i+1) is at least \u03b3 (Step 9). Since m =\u00d5(1/(\u03b3 2 4", "formula_coordinates": [9.0, 64.8, 560.63, 468.0, 26.86]}, {"formula_id": "formula_21", "formula_text": "(a) Pr (x,y)\u223cD (i) [| w, x | \u2265 T ] \u2265 \u03b3 , and (b) Pr (x,y)\u223cD (i) [h w (x) = y | w, x | \u2265 T ] \u2264 \u03b7 + .", "formula_coordinates": [10.0, 72.07, 194.9, 236.62, 36.41]}, {"formula_id": "formula_22", "formula_text": "Definition 2.7 ([", "formula_coordinates": [10.0, 64.8, 580.44, 90.66, 9.6]}, {"formula_id": "formula_23", "formula_text": "x 2 \u2265 \u03b2 E x\u223cDx [ w, x 2 ].", "formula_coordinates": [10.0, 270.19, 592.07, 113.62, 12.64]}, {"formula_id": "formula_24", "formula_text": "1: Set S (1) = R d , \u03bb = \u03b7 + , \u0393 \u22121 =\u00d5(db), m =\u00d5( 1 \u0393 2 4 ). 2: Set i \u2190 1. 3: Draw O (1/ 2 ) log(1/( \u0393)", "formula_coordinates": [11.0, 70.72, 144.97, 265.63, 38.95]}, {"formula_id": "formula_25", "formula_text": "(i) = E (x,y)\u223cD (i) | S (i) [xx T ] and set D (i) = \u0393\u03a3 (i)\u22121/2 \u2022 D| S (i) \u2229E (i) be the distribution D| S (i) \u2229E (i)", "formula_coordinates": [11.0, 83.35, 228.94, 449.46, 30.16]}, {"formula_id": "formula_26", "formula_text": "L (i) (w) = E (x,y)\u223cD (i) [LeakyRelu \u03bb (\u2212y w, x )] 8:", "formula_coordinates": [11.0, 70.72, 272.17, 261.79, 27.25]}, {"formula_id": "formula_27", "formula_text": "(i) m .", "formula_coordinates": [11.0, 400.51, 316.98, 13.3, 14.16]}, {"formula_id": "formula_28", "formula_text": "Find a threshold T (i) such that Pr (x,y)\u223cD (i) m [| w (i) , x | \u2265 T (i) ] \u2265 \u0393 and the empirical misclassification error, Pr (x,y)\u223cD (i) m [h w (x) = y | w (i) , x | \u2265 T (i) ]", "formula_coordinates": [11.0, 83.35, 332.32, 449.45, 34.68]}, {"formula_id": "formula_29", "formula_text": "S (i+1) \u2190 S (i) \\ {x : x \u2208 E (i) \u2227 | w (i) , x | \u2265 T (i) } and set i \u2190 i + 1. 13: Return the classifier [(w (1) , T (1) , E (1) ), (w (2) , T (2) , E (2) ), \u2022 \u2022 \u2022 ]", "formula_coordinates": [11.0, 66.12, 381.65, 466.68, 39.53]}, {"formula_id": "formula_30", "formula_text": "(w * , x) = (err(w * , x) \u2212 \u03bb)| w * , x | = (\u03b7(x) \u2212 \u03bb)| w * , x | \u2264 \u2212(\u03bb \u2212 \u03b7)| w * , x |.", "formula_coordinates": [11.0, 117.27, 637.54, 367.6, 12.07]}, {"formula_id": "formula_31", "formula_text": "E[| w * , x |] \u2265 E[| w * , x | 2 ] \u2265 \u0393,", "formula_coordinates": [11.0, 222.68, 686.46, 152.24, 12.06]}, {"formula_id": "formula_32", "formula_text": "x | \u2264 x 2 2 \u2264 1.", "formula_coordinates": [11.0, 305.27, 710.39, 73.73, 15.24]}, {"formula_id": "formula_33", "formula_text": "(a) Pr (x,y)\u223cD (i) [| w, x | \u2265 T ] \u2265 \u0393 , and (b) Pr (x,y)\u223cD (i) [h w (x) = y | w, x | \u2265 T ] \u2264 \u03b7 + .", "formula_coordinates": [12.0, 72.07, 450.85, 236.62, 36.41]}, {"formula_id": "formula_34", "formula_text": "(w) = E (x,y)\u223cD [\u03c6(y w, x )]. For any decreasing convex 1 function \u03c6 : R \u2192 R, there exists a distribution D over B 2 \u00d7 {\u00b11}with margin \u03b3 \u2264 \u221a 3\u22121 4", "formula_coordinates": [13.0, 64.8, 526.12, 467.5, 31.63]}, {"formula_id": "formula_35", "formula_text": "\u2202G \u2202w i (v) = E (x,y)\u223cD [\u03c6 (y v, x )(yx i )] = 0 .", "formula_coordinates": [14.0, 200.82, 112.18, 197.15, 25.5]}, {"formula_id": "formula_36", "formula_text": "D 1 0\u22121 (sign( v, x )) = p + (1 \u2212 p) \u2022 \u03b7.", "formula_coordinates": [14.0, 281.01, 272.6, 157.34, 14.98]}, {"formula_id": "formula_37", "formula_text": "p \u2022 \u03c6 (\u2212z) \u2022 \u03b3 + (1 \u2212 p) \u2022 (1 \u2212 \u03b7)\u03c6 (z) \u2022 1 \u2212 z 2 + (1 \u2212 p) \u2022 \u03b7 \u2022 \u03c6 (\u2212z) \u2022 (\u2212 1 \u2212 z 2 ) = 0 .", "formula_coordinates": [14.0, 94.83, 329.52, 407.94, 10.71]}, {"formula_id": "formula_38", "formula_text": "(1 \u2212 p) \u2022 \u03b7 \u2022 |\u03c6 (\u2212z)| \u2022 1 \u2212 z 2 = p \u2022 \u03b3 \u2022 |\u03c6 (\u2212z)| + (1 \u2212 p) \u2022 (1 \u2212 \u03b7)|\u03c6 (z)| 1 \u2212 z 2 .", "formula_coordinates": [14.0, 107.56, 379.8, 382.48, 10.71]}, {"formula_id": "formula_39", "formula_text": "p \u2022 \u03b3 = \u03b1(1 \u2212 p)\u03b7 1 \u2212 z 2 = (1 \u2212 p)\u03b7\u2206 ,", "formula_coordinates": [14.0, 204.8, 432.67, 188.0, 10.71]}, {"formula_id": "formula_40", "formula_text": "err D 1 0\u22121 (sign( v, x )) = p + (1 \u2212 p)\u03b7 = \u03b7 + (1 \u2212 \u03b7)p = \u03b7 + (1 \u2212 \u03b7)\u03b7\u2206 \u03b3 + \u03b7\u2206 = \u03b7(\u03b3 + \u2206) \u03b3 + \u03b7\u2206 \u2265 1 1 + \u03b3 \u03b7\u2206 .", "formula_coordinates": [14.0, 81.08, 534.15, 435.44, 27.62]}, {"formula_id": "formula_41", "formula_text": "err D 1 0\u22121 (sign( v, x )) \u2265 \u03b7\u2206 2\u03b3 \u2265 \u03b7 8\u03b3 .", "formula_coordinates": [14.0, 162.73, 602.35, 150.75, 15.68]}, {"formula_id": "formula_42", "formula_text": ")| \u2265 1 2 \u03b7 1\u2212\u03b7 |\u03c6 (\u2212z)|.", "formula_coordinates": [14.0, 303.87, 647.54, 88.36, 15.68]}, {"formula_id": "formula_43", "formula_text": "p \u2022 \u03c6 (0) \u2022 (2\u03b3) + (1 \u2212 p)\u03c6 (1/2) \u2022 (\u2212r) = 0 .", "formula_coordinates": [15.0, 198.28, 391.0, 201.05, 9.57]}, {"formula_id": "formula_44", "formula_text": "p|\u03c6 (0)| \u2022 (2\u03b3) = (1 \u2212 p)|\u03c6 (1/2)| \u2022 r .", "formula_coordinates": [15.0, 214.03, 440.02, 169.53, 9.57]}, {"formula_id": "formula_45", "formula_text": "p = |\u03c6 (1/2)| \u2022 r |\u03c6 (1/2)| \u2022 r + 2\u03b3|\u03c6 (0)| .", "formula_coordinates": [15.0, 230.73, 476.03, 136.13, 24.43]}, {"formula_id": "formula_46", "formula_text": "|\u03c6 (1/2)| \u2265 (\u03b7/2)|\u03c6 (\u22121/2)| \u2265 (\u03b7/2)|\u03c6 (0)| .", "formula_coordinates": [15.0, 196.93, 533.18, 203.74, 9.57]}, {"formula_id": "formula_47", "formula_text": "err D 2 0\u22121 (sign( v, x )) = p \u2265 |\u03c6 (1/2)| \u2022 r |\u03c6 (1/2)| \u2022 r + 4\u03b3 \u03b7 |\u03c6 (1/2)| = 1 1 + 4\u03b3 \u03b7r .", "formula_coordinates": [15.0, 148.78, 580.15, 300.03, 29.01]}, {"formula_id": "formula_48", "formula_text": "\u221a 3/2] it holds |\u03c6 (z)| \u2265 1 2 \u03b7 1\u2212\u03b7 |\u03c6 (\u2212z)|.", "formula_coordinates": [16.0, 131.75, 493.66, 180.8, 21.78]}, {"formula_id": "formula_49", "formula_text": "E (x,y)\u223cD [\u03c6 (y v, x )(y \u2022 x 2 )] = 0, that holds for v being the minimum of G(w) = E (x,y)\u223cD [\u03c6(y w, x )], to get: p \u2022 \u03c6 (\u22121/4) \u2022 \u221a 3/4 + 2\u03b3 + (1 \u2212 p) \u2022 \u03c6 (1/4) \u2022 \u2212 \u2265 \u221a 3/4 \u2212 2\u03b3 \u221a 3/4 + 2\u03b3 \u2022 1 1 + 2(1\u2212\u03b7) \u03b7 \u2265 1 \u2212 8\u03b3 \u221a 3/3 \u03b7 4(1 \u2212 \u03b7)", "formula_coordinates": [16.0, 64.8, 546.66, 468.0, 173.59]}, {"formula_id": "formula_51", "formula_text": "E r\u223cN (0,c 2 I) | w, x + r | \u2264 1 + c \u2264 2 .", "formula_coordinates": [22.0, 209.21, 336.54, 179.17, 11.39]}, {"formula_id": "formula_52", "formula_text": "E", "formula_coordinates": [22.0, 132.26, 390.14, 8.24, 9.6]}], "doi": ""}