{"title": "Online certification of preference-based fairness for personalized recommender systems", "authors": "Virginie Do; Sam Corbett-Davies; Jamal Atif; Nicolas Usunier", "pub_date": "2023-03-06", "abstract": "Recommender systems are facing scrutiny because of their growing impact on the opportunities we have access to. Current audits for fairness are limited to coarse-grained parity assessments at the level of sensitive groups. We propose to audit for envy-freeness, a more granular criterion aligned with individual preferences: every user should prefer their recommendations to those of other users. Since auditing for envy requires to estimate the preferences of users beyond their existing recommendations, we cast the audit as a new pure exploration problem in multi-armed bandits. We propose a sample-efficient algorithm with theoretical guarantees that it does not deteriorate user experience. We also study the tradeoffs achieved on real-world recommendation datasets.", "sections": [{"heading": "Introduction", "text": "Recommender systems shape the information and opportunities available to us, as they help us prioritize content from news outlets and social networks, sort job postings, or find new people to connect with. To prevent the risk of unfair delivery of opportunities across users, substantial work has been done to audit recommender systems (Sweeney 2013;Asplund et al. 2020;Imana et al. 2021). For instance, Datta et al. (2015) found that women received fewer online ads for highpaying jobs than equally qualified men, while Imana et al. (2021) observed different delivery rates of ads depending on gender for different companies proposing similar jobs.\nThe audits above aim at controlling for the possible acceptable justifications of the disparities, such as education level in job recommendation audits. Yet, the observed disparities in recommendation do not necessarily imply that a group has a less favorable treatment: they might as well reflect that individuals of different groups tend to prefer different items. To strengthen the conclusions of the audits, it is necessary to develop methods that account for user preferences. Audits for equal satisfaction between user groups follow this direction (Mehrotra et al. 2017), but they also have limitations. For example, they require interpersonal comparisons of measures of satisfaction, a notoriously difficult task (Sen 1999).\nWe propose an alternative approach to incorporating user preferences in audits which focuses on envy-free recommendations: the recommender system is deemed fair if each user Copyright \u00a9 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nprefers their recommendation to those of all other users. Envyfreeness allows a system to be fair even in the presence of disparities between groups as long as these are justified by user preferences. On the other hand, if user B systematically receives better opportunities than user A from A's perspective, the system is unfair. The criterion does not require interpersonal comparisons of satisfaction, since it relies on comparisons of different recommendations from the perspective of the same user. Similar fairness concepts have been studied in classification tasks under the umbrella of preference-based fairness (Zafar et al. 2017;Kim et al. 2019;Ustun et al. 2019). Envy-free recommendation is the extension of these approaches to personalized recommender systems.\nCompared to auditing for recommendation parity or equal satisfaction, auditing for envy-freeness poses new challenges. First, envy-freeness requires answering counterfactual questions such as \"would user A get higher utility from the recommendations of user B than their own?\", while searching for the users who most likely have the best recommendations from A's perspective. This type of question can be answered reliably only through active exploration, hence we cast it in the framework of pure exploration bandits (Bubeck et al. 2009). To make such an exploration possible, we consider a scenario where the auditor is allowed to replace a user's recommendations with those that another user would have received in the same context. Envy, or the absence thereof, is estimated by suitably choosing whose recommendations should be shown to whom. While this scenario is more intrusive than some black-box audits of parity, auditing for envy-freeness provides a more compelling guarantee on the wellbeing of users subject to the recommendations.\nThe second challenge is that active exploration requires randomizing the recommendations, which in turn might alter the user experience. In order to control this cost of the audit (in terms of user utility), we follow the framework of conservative exploration (Wu et al. 2016;Garcelon et al. 2020), which guarantees a performance close to the audited system. We provide a theoretical analysis of the trade-offs that arise, in terms of the cost and duration of the audit (measured in the number of timesteps required to output a certificate).\nOur technical contributions are twofold. (1) We provide a novel formal analysis of envy-free recommender systems, including a comparison with existing item-side fairness criteria and a probabilistic relaxation of the criterion. (2) We cast the problem of auditing for envy-freeness as a new pure exploration problem in bandits with conservative exploration constraints, and propose a sample-efficient auditing algorithm which provably maintains, throughout the course of the audit, a performance close to the audited system.\nWe discuss the related work in Sec. 2. Envy-free recommender systems are studied in Sec. 3. In Sec. 4, we present the bandit-based auditing algorithm. In Sec. 5, we investigate the trade-offs achieved on real-world datasets.", "publication_ref": ["b45", "b0", "b24", "b13", "b24", "b38", "b42", "b49", "b31", "b46", "b7", "b48", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Related work", "text": "Fair recommendation The domain of fair machine learning is organized along two orthogonal axes. The first axis is whether fairness is oriented towards groups defined by protected attributes (Barocas and Selbst 2016), or rather oriented towards individuals (Dwork et al. 2012). The second axis is whether fairness is a question of parity (predictions [or prediction errors] should be invariant by group or individual) (Corbett-Davies and Goel 2018;Kusner et al. 2017), or preference-based (predictions are allowed to be different if they faithfully reflect the preferences of all parties) (Zafar et al. 2017;Kim et al. 2019;Ustun et al. 2019). Our work takes the perspective of envy-freeness, which follows the preference-based approach and is aimed towards individuals.\nThe literature on fair recommender systems covers two problems: auditing existing systems, and designing fair recommendation algorithms. Most of the auditing literature focused on group parity in recommendations (Hannak et al. 2014;Lambrecht and Tucker 2019), and equal user utility (Mehrotra et al. 2017;Ekstrand et al. 2018), while our audit for envy-freeness focuses on whether personalized results are aligned with (unknown) user preferences. On the designing side, Patro et al. (2020); Ilvento, Jagadeesan, and Chawla (2020) cast fair recommendation as an allocation problem, with criteria akin to envy-freeness. They do not address the partial observability of preferences, so they cannot guarantee user-side fairness without an additional certificate that the estimated preferences effectively represent the true user preferences. Our work is thus complementary to theirs.\nWhile we study fairness for users, recommender systems are multi-sided (Burke 2017;Patro et al. 2020), thus fairness can also be oriented towards recommended items (Celis, Straszak, and Vishnoi 2017;Biega et al. 2018;Geyik, Ambler, and Kenthapadi 2019).\nMulti-armed bandits In pure exploration bandits (Bubeck et al. 2009;Audibert and Bubeck 2010), an agent has to identify a specific set of arms after exploring as quickly as possible, without performance constraints. Our setting is close to threshold bandits (Locatelli, Gutzeit, and Carpentier 2016;Kano et al. 2019) where the goal is to find arms with better performance than a given baseline. Outside pure exploration, in the regret minimization setting, conservative exploration (Wu et al. 2016) enforces the anytime average performance to be not too far worse than that of a baseline arm.\nIn our work, the baseline is unknown -it is the current recommender system -and the other \"arms\" are other users' policies. The goal is to make the decision as to whether an arm is better than the baseline, while not deteriorating performance compared to the baseline. We thus combine pure exploration and conservative constraints.\nExisting work on fairness in exploration/exploitation (Joseph et al. 2016;Jabbari et al. 2017;Liu et al. 2017) is different from ours because unrelated to personalization.\nFair allocation Envy-freeness was first studied in fair allocation (Foley 1967) in social choice. Our setting is different because: a) the same item can be given to an unrestricted number of users, and b) true user preferences are unknown.\n3 Envy-free recommendations", "publication_ref": ["b4", "b14", "b12", "b33", "b49", "b31", "b46", "b20", "b34", "b38", "b15", "b39", "b8", "b39", "b10", "b6", "b19", "b7", "b1", "b37", "b30", "b48", "b29", "b25", "b36", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Framework", "text": "There are M users, and we identify the set of users with [M ] = {1, . . . , M }. A personalized recommender system has one stochastic recommendation policy \u03c0 m per user m. We denote by \u03c0 m (a|x) the probability of recommending item a \u2208 A for user m \u2208 [M ] in context x \u2208 X . We assume that X and A are finite to simplify notation, but this has no impact on the results. We consider a synchronous setting where at each time step t, the recommender system observes a context x m t \u223c q m for each user, selects an item a m t \u223c \u03c0 m (.|x m t ) and observes reward r m\nt \u223c \u03bd m (a m t |x m t ) \u2208 [0, 1].\nWe denote by \u03c1 m (a|x) the expected reward for user m and item a in context x, and, for any recommendation policy \u03c0, u m (\u03c0) is the utility of m for \u03c0:\nu m (\u03c0) = E x\u223cq m E a\u223c\u03c0(.|x) E r\u223c\u03bd m (a|x) [r] = x\u2208X a\u2208A q m (x)\u03c0(a|x)\u03c1 m (a|x) (1)\nWe assume that the environment is stationary: the context and reward distributions q m and \u03bd m , as well as the policies \u03c0 m are fixed. Even though in practice policies evolve as they learn from user interactions and user needs change over time, we leave the study of non-stationarities for future work.\nThe stationary assumption approximately holds when these changes are slow compared to the time horizon of the audit, which is reasonable when significant changes in user needs or recommendation policies take e.g., weeks. Our approach applies when items a are single products as well as when items are structured objects such as rankings. Examples of (context x, item a) pairs include: x is a query to a search engine and a is a document or a ranking of documents, or x is a song chosen by the user and a a song to play next or an entire playlist. Remember, our goal is not to learn the user policies \u03c0 m , but rather to audit existing \u03c0 m s for fairness.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "-envy-free recommendations", "text": "Existing audits for user-side fairness in recommender systems are based on two main criteria: 1. recommendation parity: the distribution of recommended items should be equal across (groups of) users, 2. equal user utility: all (groups of) users should receive the same utility, i.e. \u2200m, n, u m (\u03c0 m ) = u n (\u03c0 n ). There are two ways in which these criteria conflict with the goal of personalized recommender systems to best accomodate user preferences. First, recommendation parity does not control for disparities that are aligned with user preferences. Second, equal user utility drives utility down as soon as users have different best achievable utilities. To address these shortfalls, we propose envy-freeness as a complementary diagnosis for the fairness assessment of personalized recommender systems. In this context, envy-freeness requires that users prefer their recommendations to those of any other user:\nDefinition 3.1. Let \u2265 0. A recommender system is -envy- free if: \u2200m, n \u2208 [M ] : u m (\u03c0 n ) \u2264 + u m (\u03c0 m ).\nEnvy-freeness, originally studied in fair allocation (Foley 1967) and more recently fair classification (Balcan et al. 2018;Ustun et al. 2019;Kim et al. 2019), stipulates that it is fair to apply different policies to different individuals or groups as long as it benefits everyone. Following this principle, we consider the personalization of recommendations as fair only if it better accommodates individuals' preferences. In contrast, we consider unfair the failure to give users a better recommendation when one such is available to others.\nUnlike parity or equal utility, envy-freeness is in line with giving users their most preferred recommendations (see Sec. 3.3). Another improvement from equal user utility is that it does not involve interpersonal utility comparisons.\nEnvy can arise from a variety of sources, for which we provide concrete examples in our experiments (Sec. 5.1). Remark. We discuss an immediate extension of envy-freeness from individuals to groups of users in App. B, in the special case where groups have homogeneous preferences and policies. Defining group envy-free recommendations in the general case is nontrivial and left for future work.", "publication_ref": ["b16", "b2", "b46", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "Compatibility of envy-freeness", "text": "Optimal recommendations are envy-free 1 Let \u03c0 m, * \u2208 argmax \u03c0 u m (\u03c0) denote an optimal recommendation policy for m. Then the optimal recommender system (\u03c0 m, * ) m\u2208M is envy-free since: u m (\u03c0 m, * ) = max \u03c0 u m (\u03c0) \u2265 u m (\u03c0 n, * ). In contrast, achieving equal user utility in general can only be achieved by decreasing the utility of best-served users for the benefit of no one. It is also well-known that achieving parity in general requires to deviate from optimal predictions (Barocas, Hardt, and Narayanan 2018).\nEnvy-freeness vs. item-side fairness Envy-freeness is a user-centric notion. Towards multisided fairness (Burke 2017), we analyze the compatibility of envy-freeness with item-side fairness criteria for rankings from Singh and Joachims (2018), based on sensitive categories of items (denoted A 1 , ..., A S ). Parity of exposure prescribes that for each user, the exposure of an item category should be proportional to the number of items in that category. In Equity of exposure 2 , the exposure of item categories should be proportional to their average relevance to the user.\nThe optimal policies under parity and equity of exposure constraints, denoted respectively by (\u03c0 m,par ) M m=1 and\n(\u03c0 m,eq ) M m=1 , are defined given user m and context x as: (parity) \u03c0 m,par (.|x) = argmax . We show their relation to envy-freeness: Proposition 1. With the above notation:\n\u2022 the policies (\u03c0 m,par ) M m=1 are envy-free, while \u2022 the policies (\u03c0 m,eq ) M m=1 are not envy-free in general. Optimal recommendations under parity of exposure are envy-free because the parity constraint (2) is the same for all users. Given two users m and n, \u03c0 m,par is optimal for m under (2) and \u03c0 n,par satisfies the same constraint, so we have u m (\u03c0 m,par ) \u2265 u m (\u03c0 n,par ).\nIn contrast, the optimal recommendations under equity of exposure are, in general, not envy-free. A first reason is that less relevant item categories reduce the exposure of more relevant categories: a user who prefers item a but who also likes item b from another category envies a user who only liked item is a. Note that amortized versions of the criterion and other variants considering constraint averages over user/contexts (Biega et al. 2018;Patro et al. 2020) have similar pitfalls unless envy-freeness is explictly enforced, as in Patro et al. (2020) who developed an envy-free algorithm assuming the true preferences are known. For completeness, we describe in App.A a second reason why equity of exposure constraints create envy, and an edge case where they do not.", "publication_ref": ["b3", "b8", "b43", "b6", "b39", "b39"], "figure_ref": [], "table_ref": []}, {"heading": "Probabilistic relaxation of envy-freeness", "text": "Envy-freeness, as defined in Sec. 3.2, (a) compares the recommendations of a target user to those of all other users, and (b) these comparisons must be made for all users. In practice, as we show, this means that the sample complexity of the audit increases with the number of users, and that all users must be part of the audit.\nIn practice, it is likely sufficient to relax both conditions on all users to give a guarantee for most recommendation policies and most users. Given two small probabilities \u03bb and \u03b3, the relaxed criterion we propose requires that for at least 1 \u2212 \u03bb fraction of users, the utility of users for their own policy is in the top-\u03b3% of their utilities for anyone else's policy. The formal definition is given below. The fundamental observation, which we prove in Th. 2 in Sec. 4.5, is that the sample complexity of the audit and the number of users impacted by the audit are now independent on the total number of users. We believe that these relaxed criteria are thus likely to encourage the deployment of envy-free audits in practice. \nP n\u223cU M u m (\u03c0 m ) + < u m (\u03c0 n ) > \u03b3.\nA recommender system is ( , \u03b3, \u03bb)-envy-free if at least a (1 \u2212 \u03bb) fraction of its users are not ( , \u03b3)-envious.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Certifying envy-freeness 4.1 Auditing scenario", "text": "The envy-freeness auditor must answer the counterfactual question: \"had user m been given the recommendations of user n, would m get higher utility?\". The main challenge is that the answer requires to access to user preferences, which are only partially observed since users only interact with recommended items. There is thus a need for an active exploration process that recommends items which would not have been recommended otherwise.\nTo make such an exploration possible, we consider the following auditing scenario: at each time step t, the auditor chooses to either (a) give the user a \"normal\" recommendation, or (b) explore user preferences by giving the user a recommendation from another user (see Fig. 1) . This scenario has the advantage of lightweight infrastructure requirements, since the auditor only needs to query another user's policy, rather than implementing a full recommender system within the operational constraints of the platform. Moreover, this interface is sufficient to estimate envy because envy is defined based on the performance of other user's policies. This type of internal audit (Raji et al. 2020) requires more access than usual external audits that focus on recommendation parity, but this is necessary to explore user preferences.\nWe note that the auditor must make sure that this approach follows the relevant ethical standard for randomized experiments in the context of the audited system. The auditor must also check that using other users' recommendation policies does not pose privacy problems. From now on, we assume these issues have been resolved.", "publication_ref": ["b40"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "The equivalent bandit problem", "text": "We now cast the audit for envy-freeness as a new variant of pure exploration bandit problems. We first focus on auditing envy for a single target user and define the corresponding objectives, then we present our auditing algorithm. Finally we specify how to use it for the certification of either the exact or probabilistic envy-freeness criteria.\nFor a target user m, the auditor must estimate whether u m (\u03c0 m ) + \u2265 u m (\u03c0 n ), for n in a subset {n 1 , ..., n K } of K users from [M ] (where K is specified later, depending Algorithm 1: OCEF algorithm. \u03be t (line 4) evaluates the conservative exploration constraint and is defined in (4). Values for \u03b2 k (t) and confidence bounds \u00b5 k and \u00b5 k are given in Lemma 4.\ninput :Confidence parameter \u03b4, conservative exploration parameter \u03b1, envy parameter output :envy or \u2212no-envy 1 S 0 \u2190 [K] // all arms except 0 2 for t=1, . . . do 3 Choose t from S t\u22121 // e.g., unif.sample\n4 if \u03b2 0 (t\u22121) > min k\u2208St\u22121 \u03b2 k (t\u22121) or \u03be t < 0 then k t \u2190 0 5 else k t \u2190 t 6\nObserve context x t \u223c q, show a t \u223c \u03c0 kt (.|x t ) and observe r t \u223c \u03bd(a t |x t ) // i.e., pull arm kt and update conf.intervals with Lem.4\n7 S t \u2190 k \u2208 S t\u22121 : \u00b5 k (t) > \u00b5 0 (t) + 8 if \u2203k \u2208 S t , \u00b5 k (t) > \u00b5 0 (t) then return envy 9\nif S t = \u2205 then return -no-envy 10 end on the criterion). As we first focus on auditing envy for one target user m, we drop all superscripts m to simplify notation. We identify {n 1 , ..., n K } with [K] and rename u m (\u03c0 n1 ), ..., u m (\u03c0 n K ) as (\u00b5 1 , ..., \u00b5 K ). To estimate \u00b5 k , we obtain samples by making recommendations using the policy \u03c0 k and observing the reward. The remaining challenge is to choose which user k to sample at each time step while not deteriorating the experience of the target user too much. Index 0 represents the target user: we use \u00b5 0 for the utility of the user for their policy (i.e., u m (\u03c0 m )). Because the audit is a special form of bandit problem, following the bandit literature, an index of a user is called an arm, and arm 0 is the baseline.\nObjectives and evaluation metrics We present our algorithm OCEF (Online Certification of Envy-Freeness) in the next subsection. Given > 0 and \u03b1 \u2265 0, OCEF returns either envy or -no-envy and has two objectives: 1. Correctness: if OCEF returns envy, then \u2203k, \u00b5 k > \u00b5 0 . If OCEF returns -no-envy then max\nk\u2208[K] \u00b5 k \u2264 \u00b5 0 + .\n2. Recommendation performance: during the audit, OCEF must maintain a fraction 1\u2212\u03b1 of the baseline performance.\nDenoting by k s \u2208 {0, . . . , K} the arm (group index) chosen at round s, this requirement is formalized as a conservative exploration constraint (Wu et al. 2016):\n\u2200t, 1 t t s=1 \u00b5 ks \u2265 (1 \u2212 \u03b1)\u00b5 0 .(3)\nWe focus on the fixed confidence setting, where given a confidence parameter \u03b4 \u2208 (0, 1) the algorithm provably satisfies both objectives with probability 1 \u2212 \u03b4. In addition, there are two criteria to assess an online auditing algorithm: 1. Duration of the audit: the number of time-steps before the algorithm stops.\n2. Cost of the audit: the cumulative loss of rewards incurred. Denoting the duration by \u03c4 , the cost is \u03c4 \u00b5 0 \u2212 \u03c4 s=1 \u00b5 ks . It is possible that the cost is negative when there is envy. In that case, the audit increased recommendation performance by finding better recommendations for the group.\nWe note the asymmetry in the return statements of the algorithm: envy does not depend on . This asymmetry is necessary to obtain finite worst-case bounds on the duration and the cost of audit, as we see in Theorem 1.\nOur setting had not yet been addressed by the pure exploration bandit literature, which mainly studies the identification of ( -)optimal arms (Audibert and Bubeck 2010). Auditing for envy-freeness requires proper strategies in order to efficiently estimate the arm performances compared to the unknown baseline. Additionally, by making the cost of the audit a primary evaluation criterion, we also bring the principle of conservative exploration to the pure exploration setting, while it had only been studied in regret minimization (Wu et al. 2016). In our setting, conservative constraints involve nontrivial trade-offs between the duration and cost of the audit. We now present the algorithm, and then the theoretical guarantees for the objectives and evaluation measures.", "publication_ref": ["b48", "b1", "b48"], "figure_ref": [], "table_ref": []}, {"heading": "The OCEF algorithm", "text": "OCEF is described in Alg. 1. It maintains confidence intervals on arm performances (\u00b5 k ) K k=0 . Given the confidence parameter \u03b4, the lower and upper bounds on \u00b5 k at time step t, denoted by \u00b5 k (t) and \u00b5 k (t), are chosen so that with probability at least 1 \u2212 \u03b4, we have \u2200k, OCEF maintains an active set S t of all arms in [K] (i.e., excluding the baseline) whose performance are not confidently less than \u00b5 0 + . It is initialized to S 0 = [K] (line 1). At each round t, the algorithm selects an arm t \u2208 S t (line 3). Then, depending on the state of the conservative exploration constraint (described later), the algorithm pulls k t , which is either t or the baseline (lines 4-6). After observing the reward r t , the confidence interval of \u00b5 t is updated, and all active arms that are confidently worse than the baseline plus are de-activated (line 7). The algorithm returns envy if an arm k is confidently better than the baseline (line 8), returns -no-envy if there are no more active arms, (line 9) or continues if neither of these conditions are met.\nt, \u00b5 k \u2208 [\u00b5 k (t), \u00b5 k (t)]. In the algorithm, \u03b2 k (t) = (\u00b5 k (t) \u2212 \u00b5 k (t))/2.\nConservative exploration To deal with the conservative exploration constraint (3), we follow (Garcelon et al. 2020). Denoting A t = {s \u2264 t : k s = 0} the time steps at which the baseline was not pulled, we maintain a confidence interval such that with probability \u2265 1 \u2212 \u03b4, we have \u2200t > 0, s\u2208At (\u00b5 ks \u2212 r s ) \u2264 \u03a6(t). The formula for \u03a6 is given in Lem. 6 in App. E. This confidence interval is used to estimate whether the conservative constraint (3) is met at round t as follows. First, let us denote by N k (t) the number of times arm k has been pulled until t, and notice that (3) is equivalent to s\u2208At \u00b5 ks \u2212 ((1 \u2212 \u03b1)t \u2212 N 0 (t))\u00b5 0 \u2265 0. After choosing t (line 3), we use the lower bound on s\u2208At \u00b5 ks and the upper bound for \u00b5 0 to obtain a conservative estimate of (3). Using \u03c4 = t \u2212 1, this leads to:\n\u03be t = s\u2208A\u03c4 r s \u2212 \u03a6(t) + \u00b5 t (\u03c4 ) + (N 0 (\u03c4 ) \u2212 (1 \u2212 \u03b1)t)\u00b5 0 (\u03c4 ) . (4)\nThen, as long as the confidence intervals hold, pulling t does not break the constraint (3) if \u03be t \u2265 0. The algorithm thus pulls the baseline arm when \u03be t < 0. To simplify the theoretical analysis, OCEF also pulls the baseline if it does not have the tightest confidence interval (lines 4-6).", "publication_ref": ["b18"], "figure_ref": [], "table_ref": []}, {"heading": "Analysis", "text": "The main theoretical result of the paper is the following:\nTheorem 1. Let \u2208 (0, 1], \u03b1 \u2208 (0, 1], \u03b4 \u2208 (0, 1 2 ) and \u03b7 k = max(\u00b5 k \u2212\u00b5 0 , \u00b5 0 + \u2212\u00b5 k ) and h k = max(1, 1 \u03b7 k ).\nUsing \u00b5, \u00b5 and \u03a6 given in Lemmas 4 and 6 (App. E), OCEF achieves the following guarantees with probability \u2265 1 \u2212 \u03b4:\n\u2022 OCEF is correct and satisfies the conservative constraint on the recommendation performance (3).\n\u2022 The duration is in\nO K k=1 h k log K log( Kh k/\u03b4\u03b7 k ) \u03b4 min(\u03b1\u00b5 0 , \u03b7 k ) . \u2022 The cost is in O k:\u00b5 k <\u00b50 (\u00b50\u2212\u00b5 k )h k \u03b7 k log K log( Kh k/\u03b4\u03b7 k )) \u03b4 .\nThe important problem-dependent quantity \u03b7 k is the gap between the baseline and other arms k. It is asymmetric depending on whether the arm is better than the baseline (\u00b5 k \u2212 \u00b5 0 ) or the converse (\u00b5 0 \u2212 \u00b5 k + ) because the stopping condition for envy does not depend on . This leads to a worst case that only depends on , since \u03b7 k = max(\u00b5 k \u2212 \u00b5 0 , \u00b5 0 \u2212 \u00b5 k + ) \u2265 2 , while if the condition was symmetric, we would have possibly unbounded duration when \u00b5 k = \u00b5 0 + for some k = 0. Overall, ignoring log terms, we conclude that when \u03b1\u00b5 0 is large, the duration is of order k 1 \u03b7 2 k and the cost is of order k 1 \u03b7 k . This becomes k 1 \u03b1\u00b50\u03b7 k and k 1 \u03b7 k when \u03b1\u00b5 0 is small compared to \u03b7 k . This means that the conservative constraint has an impact mostly when it is strict. It also means that when either \u03b1\u00b5 0 \u03b7 k or \u03b7 2 k \u03b7 k the cost can be small even when the duration is fairly high.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Full audit", "text": "Exact criterion To audit for envy-freeness on the full system, we apply OCEF to all M users simultaneously and with K = M , meaning that the set of arms corresponds to all the users' policies. By the union bound, using \u03b4 = \u03b4 M instead of \u03b4 in OCEF's confidence intervals, the guarantees of Theorem 1 hold simultaneously for all users.\nFor recommender systems with large user databases, the duration of OCEF thus becomes less manageable as M increases. We show how to use OCEF to certify the probabilistic criterion with guarantees that do not depend on M .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Probabilistic criterion", "text": "The AUDIT algorithm for auditing the full recommender system is described in Alg. 2. AUDIT samples a subset of users and a subset of arms for each sampled user. Then it applies OCEF to each user simultaneously Algorithm 2: AUDIT algorithm. The algorithm either outputs a probabilistic certificate of ( , \u03b3, \u03bb)-envyfreeness, or evidence of envy.\ninput :Confidence parameter \u03b4, conservative exploration parameter \u03b1, envy parameters ( , \u03b3, \u03bb) output :( , \u03b3, \u03bb)-envy-free or not-envy-free\n1 Draw a sampleS ofM = log(3/\u03b4) \u03bb users from [M ] 2 for each user m \u2208S in parallel do 3 Sample K = log(3M /\u03b4) log(1/(1\u2212\u03b3)) arms from [M ] \\ {m} 4\nRun OCEF \u03b4 3M , \u03b1, for user m with the K arms 5 if OCEF outputs envy then return not-envy-free 6 end 7 return ( , \u03b3, \u03bb)-envy-free with their sampled arms. It stops either upon finding an envious user, or when all sampled users are certified with -no envy. Again there is a necessary asymmetry in the return statements of AUDIT to obtain finite worst-case bounds whether or not the system is envy-free.\nThe number of target usersM and arms K in Alg. 2 are chosen so that -envy-freeness w.r.t. the sampled users and arms translates into ( , \u03b3, \u03bb)-envy-freeness. Combining these random approximation guarantees with Th. 1, we get:\nTheorem 2. LetM = log(3/\u03b4) \u03bb and K = log(3M /\u03b4) log(1/(1\u2212\u03b3)) .\nWith probability 1 \u2212 \u03b4, AUDIT is correct, it satisfies the conservative constraint (3) for allM target users, and the bounds on duration and cost from Th. 1 (using \u03b4 3M instead of \u03b4) are simultaneously valid.\nImportantly, in contrast to naively using OCEF to compare all users against all, the audit for the probabilistic relaxation of envy-freeness only requires to query a constant number of users and policies that does not depend on the total number of users M . Therefore, the bounds on duration and cost are also independent of M , which is a drastic improvement.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We present experiments describing sources of envy (Sec. 5.1) and evaluating the auditing algorithm OCEF on two recommendation tasks (Sec. 5.2).\nWe create a music recommendation task based on the Last.fm dataset from Cantador et al. (2011), which contains the music listening histories of 1.9k users. We select the 2500 items most listened to, and simulate ground truth user preferences by filling in missing entries with a popular matrix completion algorithm for implicit feedback data 4 . We also address movie recommendation with the MovieLens-1M dataset (Harper and Konstan 2015), which contains ratings of movies by real users, and from which we extract the top 2000 users and 2500 items with the most ratings. We binarize Figure 2: Envy from model mispecification on MovieLens and Lastfm: envy is high when the latent factor model is mispecified, but it decreases as the number of factors increases.\nratings by setting those < 3 to zero, and as for Last.fm we complete the matrix to generate ground truth preferences.\nFor both recommendation tasks, the simulated recommender system estimates relevance scores using low-rank matrix completion (Bell and Sejnowski 1995) on a training sample of 70% of the ground truth preferences, where the rated / played items are sampled uniformly at random. Recommendations are given by a fixed-temperature softmax policy over the predicted scores. We generate binary rewards using a Bernoulli distribution with expectation given by our ground truth preferences.", "publication_ref": ["b9", "b21", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Sources of envy", "text": "We consider two measures of the degree of envy. Denoting \u2206 m = max( max\nn\u2208[M ] u m (\u03c0 n ) \u2212 u m (\u03c0 m ), 0\n), these are:\n\u2022 the average envy experienced by users:\n1 M m\u2208[M ] \u2206 m ,\n\u2022 the proportion of -envious users:\n1 M m\u2208[M ] 1 {\u2206 m > } .\nEnvy from model mispecification We demonstrate that envy arises from a standard recommendation model when the modeling assumptions are too strong. We vary the number of latent factors of the matrix completion model and evaluate a softmax policy with inverse temperature set to 5. In Fig. 2, with one latent factor we observe no envy. This is because all users receive the same recommendations since matrix completion is then equivalent to a popularity-based recommender system. With enough latent factors, preferences are properly captured by the model and the degree of envy decreases. For intermediate number of latent factors, envy is visible.\nEnvy from equal user utility We show that in contrast to envy-freeness, enforcing equal user utility (EUU) degrades user satisfaction and creates envy between users. We compute optimal EUU policies and unconstrained optimal policies (OPT) on the ground truth preferences of Last.fm and Movie-Lens. Our results in Table 1 confirm the pitfalls of EUU, while illustrating that OPT policies are always envy-free.\nWe discuss more sources of envy and provide the details of these computations in App. C.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Evaluation of the auditing algorithm", "text": "Our goal is now to answer for OCEF: in practice, what is the interplay between the required sample size per user, the cost of exploration and the conservative exploration parameter?   Bandit experiments We first study the trade-off between duration and cost of the audit on 4 bandit problems with Bernoulli rewards and 10 arms. In Problem 1, the baseline is the best arm and all other arms are equally bad. In Prob. 2, arm 1 is best and all other arms are as bad as the baseline. In Prob.3 the baseline is best and the means of arms from best to worst decrease rapidly. Prob. 4 uses the same means as Prob. 3, but the means of the baseline and arm 1 are swapped, making the baseline second-to-best. We set \u03b4 = = 0.05 and report results averaged over 100 trials. The details of the bandit configurations are given in Appendix D.1. Figure 3 plots the duration and the cost of exploration (8) as a function of the conservative constraint parameter \u03b1 (smaller \u03b1 means more conservative). The curves show that for Problems 2, 3, and 4, duration is minimal for a non-trivial \u03b1. This is because when \u03b1 is large, all arms are pulled as much as the baseline, so their confidence intervals are similar. When \u03b1 decreases, the baseline is pulled more, which reduces the length of the relevant confidence intervals \u03b2 0 (t) + \u03b2 k (t) for all arms k. This, in turn, shortens the audit because nonbaseline arms are more rapidly discarded or declared better. When \u03b1 becomes too small, however, the additional pulls of the baseline have no effect on \u03b2 0 (t) + \u03b2 k (t) because it is dominated by \u03b2 k (t), so the duration only increases. This subtle phenomenon is not captured by our analysis (Th. 1), because the ratios \u03b2 0 (t)/\u03b2 k (t) are difficult to track formally.\nThe sign of the cost of exploration depends on whether there is envy. In Prob. 2 where the baseline has the worst performance, exploration is beneficial to the user and so the cost is negative. On all other instances however, the cost is positive. The cost of exploration is closest to 0 when \u03b1 becomes small because then \u03b2 0 (t) + \u03b2 k (t) is the smallest possible for a given number of pulls of k. For instance, in Prob. 4, the cost is close to 0 when \u03b1 is very small and increases with \u03b1. It is the case where the baseline is not the best arm but is close to it, and there are many bad arms. When the algorithm is very conservative, bad arms are discarded rapidly thanks to the good estimation of the baseline performance. In this \"low-cost\" regime however, the audit is significantly longer.\nAppendix D.1 contains additional results when varying the number of arms and the confidence parameter \u03b4.", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "MovieLens and Last.fm experiments", "text": "We now evaluate the certification of the (absence of) envy of recommendation policies on MovieLens (ML) and Last.fm. We consider two recommendation policies which are softmax functions over predicted relevance scores with inverse temperature set to either 5 or 10. These scores were obtained by matrix completion with 48 latent factors. On both datasets, with inverse temperature equal to 5, the softmax recommender system is envy-free, whereas there is envy when it is set to 10. We use AUDIT with OCEF to certify the probabilistic criterion. The envy parameters are set to = \u03b4 = 0.05 and \u03bb = \u03b3 = 0.1, therefore we haveM = 41 target users and K = 75 arms, independently on the number of users in each dataset.\nThe results of applying OCEF on each dataset (ML or Last.fm) with each policy (envy-free or with envy) are shown in Fig. 4. For the ( , \u03b3, \u03bb)-envy-free policies, results are averaged over 20 trials and over all the non-( , \u03b3)-envious users, whereas when there is envy, results are averaged over the target users who are -envious. We observe clear tendencies similar to those of the previous section, although the exact sweet spots in terms of \u03b1 depends on the specific configuration. In particular, on envy-free configurations, the cost of the audit is positive and grows when relaxing the conservative constraint, while it is negative and decreasing with \u03b1 when there is envy. More details are provided in App. D.2.", "publication_ref": [], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "Conclusion", "text": "We proposed the audit of recommender systems for user-side fairness with the criterion of envy-freeness. The auditing problem requires an explicit exploration of user preferences, which leads to a formulation as a bandit problem with conservative constraints. We presented an algorithm for this problem and analyzed its performance experimentally. (rewards) \u03c1 1 1 0 0.8 0.7 \u03c1 2 0.8 0.7 1 0 (policies) \u03c0 1,eq 0.4 0 0.6 0 0.88 0.92 \u03c0 2,eq 0.6 0 0.4 0 0.92 0.88\nTable 2: Example where the optimal recommendations under item-side equity of exposure constraints are not user-side fair because both users envy each other. There are 4 items, 2 item categories and 2 users. User 1 envies user 2 since u 1 (\u03c0 2,eq ) > u 1 (\u03c0 1,eq ). Also, u 2 (\u03c0 1,eq ) > u 2 (\u03c0 2,eq ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A (In-)Compatibility of envy-freeness", "text": "A.1 Envy-freeness vs. optimality certificates\nWe showed in Section 3.3 that envy-freeness is compatible with optimal predictions. To understand the differences between a certificate of envy-freeness and a certificate of optimality, let us denote by \u03a0 * = {\u03c0 : \u2203u satisfying (1) , \u03c0 \u2208 argmax \u03c0 u(\u03c0 )} the set of potentially optimal policies. If the set of users policies approximately covers the set of potentially optimal policies \u03a0 * , then an envy-free system is also optimal. Formally, let D(\u03c0, \u03c0 ) such that |u(\u03c0) \u2212 u(\u03c0\n)| \u2264 D(\u03c0, \u03c0 ). It is easy to see that if max \u03c0\u2208\u03a0 * min m\u2208M D(\u03c0, \u03c0 m ) \u2264\u02dc ,\nthen -envy-freeness implies +\u02dc -optimality.\nIn practice, the space of optimal policies is much larger than the number of users (for instance, there are |A| |X | optimal policies in our setting), so that auditing for envy is tractable in cases where auditing for optimality is not.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Envy-freeness vs. equity of exposure", "text": "We remind the definition of optimal policies with equity of exposure constraints from Section 3.3:\n(equity) \u03c0 m,eq (.|x) = argmax Following Proposition 1 from Section 3.3, we describe here a second source of envy when using optimal policies with equity of exposure constraints. By the linearity of the optimization problem for \u03c0 m,eq , the policy assigns to the best item in a category the exposure of the entire category. It implies that categories with high average relevance have more exposure than categories with few but highly relevant items. Table 2 gives an example with two users and two categories of items where both users envy each other with the optimal recommendations under equity of exposure constraints.\nIn some degenerate cases though, equity of exposure policies are envy-free.\nLemma 3. If for all contexts x \u2208 X , each user m \u2208 [M ] only likes a single item category A sm , i.e. \u2200a \u2208 A \\ A sm , \u03c1 m (a|x) = 0, then the policies (\u03c0 m,eq ) M m=1 are envyfree.\nProof. We set contexts x aside to simplify notation, but the generalization is straightforward.\nWe actually prove a stronger result than the lemma: if each user m only likes a single item, then (\u03c0 m,eq ) M m=1 = (\u03c0 m, * ) M m=1 , where \u03c0 m, * is the optimal unconstrained policy for m.\nLet a m s = argmax a\u2208As \u03c1 m (a) be the favorite item in category A s for user m, then the optimal equity of exposure constrained policies has the following analytical expression:\n\u2200s \u2208 S, \u2200a \u2208 A s , \u03c0 m,eq (a) = 1 {a=a m s } a\u2208As \u03c1 m (a ) a \u2208A \u03c1 m (a )\n, and we thus have: Then u m (\u03c0 m,eq ) = \u03c1 m (a m sm ) = max a\u2208A \u03c1 m (a). Then \u03c0 m,eq is the optimal unconstrained policy for user m, meaning the whole system is envy-free (cf. Sec 3.2).\nu m (\u03c0 m,eq ) = s\u2208[S]\nFrom Eq. 5, we actually note that (\u03c0 m,eq ) M m=1 = (\u03c0 m, * ) M m=1 if and only if each user m equally values their favorite items in each category they like, i.e. \u2200m, \u2203\u03ba > 0, \u2200s \u2208 S, \u03c1 m (a m s ) > 0 \u21d2 \u03c1 m (a m s ) = \u03ba.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Extension to group envy-freeness", "text": "We briefly discuss an extension of envy-free recommendation to groups, since most of the literature on fair machine learning focuses on systematic differences between groups. Certifying envy-freeness at the level of groups rather than individuals also relaxes the criterion because it requires less exploration.\nLet us assume we are given a partition G of the users into disjoint groups. For g, g \u2208 G, we define the group utility of g with respect to g as:\nU (g, g ) = 1 |g| m\u2208g u m 1 |g | n\u2208g \u03c0 n . (6\n) Definition B.1. Given \u2265 0, the recommender system is - group-envy-free if: \u2200g, g \u2208 G, U (g, g ) \u2264 U (g, g) + .\nGroup envy-freeness is equivalent to envy-freeness when each group is a singleton. When we have prior knowledge that user preferences and policies are homogeneous within each group, -envy-freeness translates to -group envy-freeness, with \u2248 , and the reciprocal is also true: Proposition 2. Let ,\u02dc > 0, and assume that for all groups and all pairs of users m, n in the same group g, we have sup The result is natural since when all groups have users with homogeneous preferences and policies, groups and users are a similar entity as regards the assessment of envy-freeness. The proof is straightforward and omitted. When groups have heterogeneous policies, the \"average policy\" 1 |g| n\u2208g \u03c0 n is uninformative because it does not represent any user's policy. Defining a notion of group utility in the general case is thus nontrivial and left for future work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C Sources of envy", "text": "In this section, we first list a few possible sources of envy in recommender systems. Then we provide the details of experiments 5 which showcase one of these sources, namely model mispecification (App. C.2).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1 Examples of sources of envy", "text": "Model mispecification Recommender systems often rely on strong modeling assumptions and multi-task learning, with methods such as low-rank matrix factorization (Koren, Bell, and Volinsky 2009). The limited capacity of the models (e.g., a rank that is too low) or incorrect assumptions might leave aside users with less common preference patterns. Appendix C.2 gives a more detailed example on two simulated recommendation tasks.\nMisaligned incentives A recommender system might have incentives to recommend some items to specific users, e.g., sponsored content. Envy appears when there is a mismatch between users who like these items and users to whom they are recommended.\nMeasurement bias Many hybrid recommender systems rely on user interactions together with user-side data (Burke 2002). This includes side-information such as browsing history on third-party, partner websites. Envy arises in these settings if there is measurement bias (Suresh and Guttag 2019), e.g., if the side information is unevenly collected for all users (e.g., browsing patterns are different across users and partners are aligned with the patterns of a user groups only).\nOperational constraints Regardless of incentives, recommendations might need to obey additional constraints. As described in Proposition 1, the item-side fairness constraint of equity of exposure is an example of possible source of (user-side) envy. The user-side fairness constraint of equal utility also creates envy, as we showed in Sec. 5.1.\nIn the following, we provide the details of our experiments from Sec. 5.1 where we showcase examples of environments with envy based on movie and music recommendation tasks.\nIn these experiments, we measure envy based on the quantity:\n\u2206 m = max max n\u2208[M ] u m (\u03c0 n ) \u2212 u m (\u03c0 m ), 0\nIn line with (Chevaleyre, Endriss, and Maudet 2017), we consider two ways of measuring the degree of envy:\n\u2022 the average envy experienced by users:\n1 M m\u2208[M ] \u2206 m ,\n\u2022 the proportion of -envious users:\n1 M m\u2208[M ] 1 {\u2206 m > } .", "publication_ref": ["b32", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "C.2 Setup of the experiments on envy from model mispecification", "text": "We describe in this section the details of the experiments on envy from mispecification presented in Section 5.1. We used Lastfm-2k (Cantador et al. 2011), a dataset from the online music service Last.fm 6 which contains real play counts of 2k users for 19k artists, and was used by Patro et al. (2020) who also study envy-freeness as a user-side fairness criterion. We filter the top 2, 500 items most listened to. Following (Johnson 2014), we pre-process the raw counts with logtransformation. We split the dataset into train/validation/test sets, each including 70%/10%/20% of the user-item listening counts. We create three different splits using three random seeds. We estimate relevance scores for the whole user-item matrix using the standard matrix factorization algorithm 7 of Hu, Koren, and Volinsky (2008) trained on the train set, with hyperparameters selected on the validation set by grid search with DCG@40 as metric. The number of latent factors is chosen in [16,32,64,128], the regularization in [0.01, 0.1, 1., 10.], and the confidence weighting parameter in [0.1, 1., 10., 100.]. The resulted matrix of estimated relevance scores serves as the ground truth preferences. We also address movie recommendation using the MovieLens-1M dataset (Harper and Konstan 2015), which contains 1 million ratings on a 5-star scale from approximately 6000 users and 4000 movies. We extract a 2000\u00d72500 user \u00d7 items matrix, keeping users and items with the most rating. We transform MovieLens ratings into an implicit feedback dataset similar to Last.fm. Since setting ratings < 3 are usually considered as negative (Wang et al. 2018), we set ratings < 3 to zero, resulting in a dataset with preference values among {0, 3, 3.5, 4, 4.5, 5}. We then use the same algorithm as for Last.fm to obtain relevance scores that we use to simulate ground truth preferences.\nWe then simulate a recommender system's estimation of preferences using low-rank matrix completion 8 (Bell and Sejnowski 1995) on a training sample of 70% of the whole \"ground truth\" preferences, with hyperparameter selection on a 10% validation sample. Here, the regularization is chosen in [0.001, 0.01, 0.1, 1.], and the confidence weighting parameter in [0.1, 1., 10., 100.]. The estimated preference scores are given as input to the recommendation policies.\nThe recommendation policies we consider are softmax distributions over the predicted scores with fixed inverse temperature. These policies recommend a single item, drawn from the softmax distribution.\nWe generate binary rewards using a Bernoulli distribution with expectation given by our ground truth. We consider no context in these experiments, so that the policies and rewards only depend on the user and the item.\nFigure 2 in Sec. 5.1 was generated by varying the number of latent factors in the recommender system's preference estimation model. For each number of latent factors in the range [1,2,4,8,16,32,64,128,256], a new model was trained on the train set with hyperparameter selection on the validation set. The degrees of envy are measured on the whole ground truth preference matrix.", "publication_ref": ["b9", "b39", "b28", "b23", "b21", "b47", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "C.3 Envy from equal user utility constraints", "text": "We provide the full details of the experiments on envy from equal user utility presented in Sec. 5.1 from the main paper. The goal of these experiments is to show that in contrast to envy-freeness, enforcing equal user utility (EUU) degrades user satisfaction and creates envy between users. We remind from Sec. 3.2 that the fairness constraint of EUU is defined as:\n\u2200m, n \u2208 [M ], u m (\u03c0 m ) = u n (\u03c0 n ), or equivalently: \u2200m \u2208 [M ], u m (\u03c0 m ) = 1 M n\u2208[M ] u n (\u03c0 n ).\nEqual user utility is enforced by adding a penalty to the maximization of user utilities. Optimal EUU policies are found by maximizing the following concave objective function, where the parameter b > 0 controls the strength of the penalty:\n(EUU) \u03c0 euu b = argmax p:A\u2192[0,1] M \u2200m, a p m (a)=1 m\u2208[M ] u m (p m ) \u2212 b D(p) with D(p) = m\u2208[M ] u m (p m ) \u2212 1 M n\u2208[M ] u n (p n ) 2 .(7)\nWe infer EUU policies using the Frank-Wolfe algorithm (Frank and Wolfe 1956) with the ground truth preferences given as input. The parameter of the penalty is set to b = 50. We also generate the unconstrained optimal policies (OPT) based on the ground truth (recall that these are u m (\u03c0 m, * ) = max \u03c0 u m (\u03c0) \u2265 u m (\u03c0 n, * )).\nA comparison of EUU and OPT is provided in Table 1 in Sec. 5.1, with the following evaluation measures : total utility (higher is better), average envy and proportion of 0.05envious users (lower is better). The results on both dataset confirm the claim that enforcing EUU penalties deteriorates total utility and creates envy between users, while illustrating the known property that OPT policies are compatible with envy-freeness.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "D OCEF experiments D.1 Bandit experiments", "text": "We performed experiments on toy bandit environments to assess the performance of our algorithm OCEF on various configurations, which were also considered in . The four bandits instances have 10 arms. They are Bernoulli variables with means equal to\n1) \u00b5 0 = 0.6 and \u00b5 k = 0.3 for k \u2208 [9], 2) \u00b5 0 = 0.3, \u00b5 1 = 0.6 and \u00b5 k = 0.3 for k = 2..9,\n3) \u00b5 k = 0.7 \u2212 0.7 * k 10 0.6 , k = 0, ..., 9, and the baseline is \u00b5 0 , 4) same as 3), but permuting \u00b5 0 and \u00b5 1 . Fig. 5 shows the result of applying OCEF on the various configurations, where we set \u03b4 = = 0.05, \u03c9 = 0.99 9 and report results averaged over 100 trials. We observe clear tendencies similar to those presented in Section 5.2, although the exact sweet spots in terms of \u03b1 depends on the specific configuration.\nThe cost of exploration follows similar patterns as in in Section 5.2. In Prob. 2, the baseline has the worst performance, so exploration is beneficial to the user and the cost is negative. On the other hand, for instance in Prob. 4, the cost is close to 0 when \u03b1 is very small and increases with \u03b1. It is the case where the baseline is not the best arm but is close to it, and there are many bad arms. When the algorithm is very conservative, bad arms are discarded rapidly thanks to the good estimation of the baseline performance. In this \"low-cost\" regime however, the audit is significantly longer.\nWe show additional results when varying \u03b4 in Figure 6. Results are averaged over 100 simulations and the conservative exploration parameter is set to \u03b1 = 0.05. The duration decreases as \u03b4 increases, i.e. a lower confidence certificate requires fewer samples per user. The duration for Problem 1 is longer than for the other instances. This is because with \u03b1 set to 0.05 and the baseline mean being much higher than  non-baseline arms, the conservative constraint 3 enforces many pulls of the baseline, since each exploration round is very costly. As a consequence, too little data is collected on the non-baseline arms to conclude that they are below \u00b5 0 + . Since all non-baseline arms have equal means, the size of the active set remains the same for a long time, while in Problem 3, where the baseline is also the best arm, arms are eliminated one at a time.\nWe show how OCEF scales with the number of arms in Figure 7, for fixed values \u03b1 = \u03b4 = = 0.05. We set K max = 100 and define 4 instances as in the list above, except that K = K max instead of K = 9. We run OCEF on the instances \u00b5 0:K and vary the value of K \u2264 K max . The duration increases for all problems, and the slope depends on the gaps between \u00b5 0 and the \u00b5 k .", "publication_ref": [], "figure_ref": ["fig_10", "fig_11", "fig_12"], "table_ref": []}, {"heading": "D.2 Setup of the MovieLens and Last.fm experiments", "text": "We now provide additional details on the experimental evaluation of OCEF on MovieLens and Last.fm presented in Sec. 5.2. The protocole to generate the recommendation task is the same as the one described in App. C for the experiments on sources of envy. The policies are softmax distributions over scores predicted by the matrix factorization model with a number of factors equal to 48.\nIn these experiments, the auditor interacts with the audited users. Rewards are drawn from Bernoulli distributions with expectation equal to the ground truth preferences.\nTwo recommendation policies are audited. The first one is a softmax with inverse temperature equal to 5. Since the inverse temperature is small, the softmax distribution is closer to random, which means users get more similar recommendations: the recommender system is thus envy-free. The second one is a softmax with inverse temperature equal to 15. With higher inverse temperature, the distribution is more peaked, which exacerbates differences between policies. Since the model with 48 factors is mispecified (see Sec.5.1), envy is visible.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E Proofs E.1 Theoretical results", "text": "Useful lemmas Recall that OCEF considers a single audited group m, therefore we do not use superscripts m in the following (e.g., \u00b5 k , r t ...).\nThe algorithm relies on valid confidence intervals. As in , we use anytime bounds inspired by the law of the iterated algorithm (LIL), and a union bound.\nWe say that a random variable is \u03c3-subgaussian if it is subgaussian with variance proxy \u03c3 2 . Since we assume the rewards for each user are bounded, more precisely r t \u2208 [0, 1], they are 1 2 -subgaussian. Throughout the paper, we assume that rewards for each user are independent conditionally to the arm played. Lemma 4. Let \u03b4 \u2208 (0, 1). Assume the rewards are \u03c3subgaussian.\nLet \u03c9 \u2208 (0, 1), \u03b8 = log(1 + \u03c9) \u03c9\u03b4 2(2+\u03c9) 1 1+\u03c9 . Let N k (t) = t s=1 1 {ks=k} \u00b5 k (t) = t s=1 r s 1 {ks=k} N k (t) \u03b2 k (t) = 2\u03c3 2 (1 + \u221a \u03c9) 2 (1 + \u03c9) N k (t) \u00d7 log 2(K + 1) \u03b8 log((1 + \u03c9)N k (t)) \u00b5 k (t) = \u00b5 k (t) \u2212 \u03b2 k (t) \u00b5 k (t) = \u00b5 k (t) + \u03b2 k (t)\nThen,\nP \u2200t > 0, \u2200k \u2208 [K], \u00b5 k \u2208 [\u00b5 k (t); \u00b5 k (t)] \u2265 1 \u2212 \u03b4 2 .\nNotice that the choice of \u03b8 makes sure that \u03b2 k is well defined as long as N k (t) > 0. We use the convention that when N k (t) = 0, \u03b2 k (t) is strictly larger than when N k (t) = 1 to ensure \u03b2 k is strictly decreasing with N k . Also, when N k (t) = 0, we set \u00b5 k (t) = 0.\nFollowing (Garcelon et al. 2020), our lower bound on the conservative constraint relies on Freedman's martingale inequality (Freedman 1975). Lemma 5. Assume all rewards are \u03c3-subgaussian. Let A t = {s \u2264 t : k s = 0} be the number of times a nonbaseline arm k = 0 has been pulled up to time t. Let\n\u03c6(t) = \u03c3 2|A t\u22121 | log 6|At\u22121| 2 \u03b4 + 2 3 log 6|At\u22121| 2 \u03b4 .\nThen, \u2200\u03b4 > 0,\nP \uf8ee \uf8f0 \u2200t > 0, s\u2208At\u22121 (\u00b5 ks \u2212 r s ) \u2264 \u03c6(t) \uf8f9 \uf8fb \u2265 1 \u2212 \u03b4 2 .\nAs in Lemma 4, we use the convention \u03c6(t) = 0 when |A t\u22121 | = 0. Lemma 6. Let \u03b4 \u2208 (0, 1).\nLet \u03a6(t) = min\nK k=1 \u03b2 k (t \u2212 1)N k (t \u2212 1), \u03c6(t)\n, with \u03c6(t) defined in Lemma 5. Let E be the event under which all confidence intervals are valid, i.e.:\nE = E 1 \u2229 E 2 with E 1 = \u2200k \u2208 {0, . . . , K}, \u2200t > 0, \u00b5 k (t) \u2208 [\u00b5 k (t); \u00b5 k (t) E 2 = \u2200t > 0, s\u2208At\u22121 (\u00b5 ks \u2212 r s ) \u2264 \u03a6(t) . Then P [E] \u2265 1 \u2212 \u03b4. Proof. By Lemma 4, P [E 1 ] \u2265 1 \u2212 \u03b4 2\n. By the lemma above, with probability 1 \u2212 \u03b4 2 , we have for all t > 0, s\u2208At\u22121 (\u00b5 ks \u2212 r s ) \u2264 \u03c6(t). Then, notice that\ns\u2208At\u22121 (\u00b5 ks \u2212 r s ) = K k=1 N k (t \u2212 1)(\u00b5 k \u2212 \u00b5 k (t \u2212 1)) .\nHence under E 1 we also have:\ns\u2208At\u22121 (\u00b5 ks \u2212 r s ) \u2264 K k=1 N k (t \u2212 1)\u03b2 k (t \u2212 1).\nTherefore,\nE = E 1 \u2229 E 2 = E 1 \u2229 s\u2208At\u22121 (\u00b5 ks \u2212 r s ) \u2264 \u03c6(t) ,\nand thus, by a union bound, we have:\nP [E] \u2265 1 \u2212 \u03b4.\nTheorems We now provide our complete theoretical guarantees for correctness (Theorem 7), duration (Theorem 8) and cost (Theorem 9), which we then prove in App. E.2 and E.3. From these results, we derive Theorem 1 in the main paper, which we prove in App. E.4. Theorem 7 (Correctness). With probability at least 1 \u2212 \u03b4:\n1. OCEF satisfies the safety constraint (3) at every time step, 2. if OCEF outputs -no-envy then the user m is notenvious, and if it outputs envy, then m is envious.\nWe denote log + (.) = max(1, log(.)). Theorem 8 (Duration). Let \u03b7 k = max(\u00b5 k \u2212\u00b5 0 , \u00b5 0 + \u2212\u00b5 k ), \u03b4 \u2208 (0, 1), \u03b8 = log(2) \u03b4 6 , and\n\u2200k = 0, H k = 1 + 64 \u03b7 2 k log 2(K + 1) log + 128(K+1) \u03b8\u03b7 2 k \u03b8 , H 0 = max max k\u2208[K] H k , 6K + 2 \u03b1\u00b5 0 + K k=1 256 log 2(K+1) log(2H k ) \u03b8 \u03b1\u00b5 0 \u03b7 k .\nWith probability at least 1 \u2212 \u03b4, OCEF stops in at most \u03c4 steps, with\n\u03c4 \u2264 K k=0 H k .\nFinally, we define the cost of exploration as the potential reward lost because of exploration actions, in our case the cumulative reward lost, on average over users in the group:\nC t = t\u00b5 0 \u2212 t s=1 \u00b5 ks .(8)\nIn the worst case, the following bound holds: Theorem 9 (Cost of exploration). Under the assumptions and notation of Theorem 8, let \u03c4 be the time step where OCEF stops. With probability 1 \u2212 \u03b4, we have:\nC \u03c4 \u2264 k:\u00b5 k <\u00b50 (\u00b5 0 \u2212 \u00b5 k )H k (9)\nCertification of the exact criterion for all users The audit of the full system for the exact envy-freeness criterion consists in running OCEF for every user. Since we are making multiple tests, we need to use a tighter confidence parameter for each user so that the confidence intervals simultaneously hold for all users. Corollary 9.1 (Online certification). With probability at least 1 \u2212 \u03b4, running OCEF simultaneously for all M users, each with confidence parameter \u03b4 = \u03b4 M , we have: 1. for all m \u2208 [M ] OCEF satisfies the constraints (3), 2. all users for which OCEF returns -NO ENVY are notenvious of any other users, and all users for which OCEF returns ENVY are envious of another user. 3. For every user, the bounds on the duration of the experiment and the cost of exploration given by Theorems 8 and 9 (using \u03b4/M instead of \u03b4) are simultaneously valid.\nFor the certification of the probabilistic envy-freeness criterion, we refer to Theorem 2 in the main paper, which we prove in App. E.5.", "publication_ref": ["b18", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "E.2 Proof of Theorem 7", "text": "Proof. We assume that event E holds true. Then all confidence intervals are valid, i.e., for all k = 0, ..., K, \u00b5 k (t) \u2264 \u00b5 k \u2264 \u00b5 k (t), and s\u2208At\u22121 \u00b5 ks \u2265 s\u2208At\u22121 r s \u2212 \u03a6(t).\nLet Z t be the safety budget, defined as Z t = t s=1 \u00b5 ks \u2212 (1\u2212\u03b1)\u00b5 0 t, so that the conservative constraint (3) is equivalent to \u2200t, Z t \u2265 0. We have Z t = s\u2208At\u22121 \u00b5 ks + \u00b5 kt + (N 0 (t \u2212 1) \u2212 (1 \u2212 \u03b1)t)\u00b5 0 . Therefore, \u03be t (eq. ( 4)) is a lower bound on the safety budget Z t if t is played. By construction of the algorithm, the safety constraint (3) is immediately satisfied since a pull that could violate it is not permitted.\nBy the validity of confidence intervals under E, if OCEF stops because of the first condition, then \u2203k, \u00b5 k > \u00b5 0 . Therefore 0 is not -envious of k and OCEF is correct.\nIf OCEF stops because of the second condition, i.e., \u2200k, \u00b5 k (t) \u2264 \u00b5 0 (t) + , then \u2200k, \u00b5 k \u2264 \u00b5 0 + . Therefore 0 is not envious and OCEF is correct.\nSince P [E] \u2265 1 \u2212 \u03b4, OCEF satisfies the safety constraint and is correct with probability \u2265 1 \u2212 \u03b4.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.3 Proofs of Theorem 8 and Theorem 9", "text": "Notation For conciseness, we useK = K + 1, and\n\u03c8 k (t) = 2\u03c3 2 (1 + \u221a \u03c9) 2 (1 + \u03c9) log 2K \u03b8 log((1 + \u03c9)N k (t)) , so that \u03b2 k (t) = \u03c8 k (t) N k (t) .\nWe shall also use \u0393 \u03c9 = 2\u03c3 2 (1 + \u221a \u03c9) 2 (1 + \u03c9). We use the convention \u03c8 k (t) = 0 when N k (t) = 0, and set \u03b2 k (t) to some value strictly larger than when N k (t) = 1.\nWe remind that \u03c9 \u2208 (0, 1), \u03b8 = log(1+\u03c9) \u03c9\u03b4\n2(2+\u03c9) 1 1+\u03c9 and \u03b7 k = max(\u00b5 k \u2212 \u00b5 0 , \u00b5 0 + \u2212 \u00b5 k ). We denote by \u03b7 min = min k\u2208[K] \u03b7 k .\nFinally, we notice that under event E (as defined in Sec. E.1), we have for all k \u2208 {0, . . . , K} and all t:\n\u00b5 k + 2\u03b2 k (t) \u2265 \u00b5 k (t) \u2265 \u00b5 k \u2265 \u00b5 k (t) \u2265 \u00b5 k \u2212 2\u03b2 k (t). (10) Lemma 10. Under event E, for every k \u2208 [K], if k is pulled at round t, then 4\u03b2 k (t) \u2265 \u03b7 k .\nProof of Lemma 10. Since k is pulled at t, the two following inequalities hold:\n\u00b5 k (t \u2212 1) > \u00b5 0 (t \u2212 1) + (11) \u00b5 k (t \u2212 1) \u2264 \u00b5 0 (t \u2212 1)(12)\nWe prove them by contradiction. If (11) does not hold, then k should be discarded from the active set at time t \u2212 1, and therefore cannot be pulled at t. Likewise, if (12) does not hold, then the algorithm stops at t \u2212 1, so k cannot be pulled at t. Using ( 11) and (10), we have:\n\u00b5 k +2\u03b2 k (t\u22121) \u2265 \u00b5 k (t\u22121) > \u00b5 0 (t\u22121)+ \u2265 \u00b5 0 \u22122\u03b2 0 (t\u22121)+ .\nSince 0 was not pulled at time t, we also have\n\u03b2 0 (t \u2212 1) \u2264 \u03b2 k (t \u2212 1), hence 4\u03b2 k (t \u2212 1) \u2265 \u00b5 0 + \u2212 \u00b5 k .\nUsing ( 12) and ( 10)\nwe have \u00b5 k \u2212 2\u03b2 k (t) \u2264 \u00b5 0 + 2\u03b2 0 (t) and since \u03b2 0 (t) \u2264 \u03b2 k (t), we obtain 4\u03b2 k (t \u2212 1) \u2265 \u00b5 k \u2212 \u00b5 0 .\nIn the following lemma, we recall that we denote log + (.) = max(1, log(.)).\nLemma 11. Under event E, \u2200\u03c4 > 0, \u2200k \u2208 [K], we have\nN k (\u03c4 ) \u2264 H k with H k = 1 + 32\u03c3 2 (1 + \u221a \u03c9) 2 (1 + \u03c9) \u03b7 2 k \u00d7 log 2(K + 1) log + 64(K+1)\u03c3 2 (1+ \u221a \u03c9) 2 (1+\u03c9) 2 \u03b8\u03b7 2 k \u03b8 Proof. Let \u03c4 > 0, k \u2208 [K]\n, and let t \u2264 \u03c4 be last time step before \u03c4 at which k was pulled. If such a t does not exist, then N k (\u03c4 ) = 0 and the result holds. In all cases, we have N k (t) = N k (\u03c4 ).\nWe consider t > 0 from now on. By Lemma 10, we have 4\u03b2 k (t \u2212 1) \u2265 \u03b7 k , and thus , we obtain\nN k (t \u2212 1) \u2264 16\u03c8 k (t\u22121) \u03b7 2 k , which writes, if N k (t) > 0: N k (t \u2212 1) \u2264 16\u03c8 k (t \u2212 1) \u03b7 2 k \u2264 16\u0393 \u03c9 \u03b7 2 k log 2K \u03b8 log ((1 + \u03c9)N k (t \u2212 1)) .(13\nN k (t \u2212 1) \u2264 16\u0393 \u03c9 \u03b7 2 k log 2K \u03b8 log (1 + \u03c9)32K\u0393 \u03c9 \u03b8\u03b7 2 k (14\n)\nSince N k (t) = N k (t \u2212 1) + 1, using log + instead of log inside to deal with the case N k (t \u2212 1) = 0 gives the desired result.\nLemma 12. Under event E, at every time step \u03c4 , we have\nN 0 (\u03c4 ) \u2264 max max k\u2208[K] H k , 6K + 2 \u03b1\u00b5 0 + K k=1 64\u03c3 2 (1 + \u221a \u03c9) 2 (1 + \u03c9) log 2(K+1) log((1+\u03c9)H k ) \u03b8 \u03b1\u00b5 0 \u03b7 k Proof.\nLet \u03c4 > 0 and t \u2264 \u03c4 the last time 0 was pulled before \u03c4 . We assume t > 0.\nCase 1: 0 was pulled because\n\u03b2 0 (t\u22121) > min k\u2208[K] \u03b2 k (t\u2212 1). Then N 0 (\u03c4 ) = N 0 (t \u2212 1) + 1 \u2264 1 + max k =0 N k (t \u2212 1).\nBy lemma 10, we thus have N 0 (\u03c4 ) \u2264 max k\u2208[K] H k .\nCase 2: 0 was pulled because \u03be t < 0. Here the proof follows similar steps as that of Theorem 5 in (Wu et al. 2016).\ns\u2208At\u22121 r s \u2212 \u03a6(t)+\u00b5 t (t\u22121) + (N 0 (t\u22121) \u2212 (1 \u2212 \u03b1)t)\u00b5 0 (t\u22121) < 0\nWe drop \u00b5 t (t\u22121), replace t by K k=0 N k (t \u2212 1) + 1 and rearrange terms to obtain:\n\u03b1N (t \u2212 1)\u00b5 0 (t \u2212 1) \u2264 (1 \u2212 \u03b1)\u00b5 0 (t \u2212 1) + (1 \u2212 \u03b1) K k=1 N k (t \u2212 1)\u00b5 0 (t \u2212 1) \u2212 s\u2208At\u22121 r s + \u03a6(t) (15)\nSince we have \u03b2 0 (t \u2212 1) \u2264 \u03b2 k (t \u2212 1) (otherwise we would be in case 1), and A t\u22121 = K k=1 N k (t \u2212 1), we bound the the sum over arms in (15):\nK k=1 N k (t \u2212 1)\u00b5 0 (t \u2212 1) \u2264 K k=1 N k (t \u2212 1)(\u00b5 0 + 2\u03b2 0 (t \u2212 1)) \u2264 K k=1 N k (t \u2212 1)(\u00b5 0 + 2\u03b2 k (t \u2212 1)) = s\u2208At\u22121 \u00b5 0 + K k=1 2\u03b2 k (t \u2212 1)N k (t \u2212 1). Using Lemma 6, we also bound \u2212 s\u2208At\u22121 r s \u2265 s\u2208At\u22121 \u00b5 s + \u03a6(t) (under E).\nPlugging this into (15) gives:\n\u03b1N 0 (t \u2212 1)\u00b5 0 (t \u2212 1) \u2264 (1 \u2212 \u03b1)\u00b5 0 (t \u2212 1) + 2(1 \u2212 \u03b1) K k=1 N k (t \u2212 1)\u03b2 k (t \u2212 1) + s\u2208At\u22121 ((1 \u2212 \u03b1)\u00b5 0 \u2212 \u00b5 ks ) + 2\u03a6(t).\nRecall that \u03a6(t) = min( K k=1 N k (t \u2212 1)\u03b2 k (t \u2212 1), \u03c6(t)), and therefore \u03a6(t) \u2264 K k=1 N k (t \u2212 1)\u03b2 k (t \u2212 1). Using \u00b5 0 \u2212 \u00b5 ks \u2264 \u03b7 ks and s\u2208At\u22121 \u03b7 ks = K k=1 N k (t \u2212 1)\u03b7 k , we obtain:\n\u03b1N 0 (t \u2212 1)\u00b5 0 (t \u2212 1) \u2264 (1 \u2212 \u03b1)\u00b5 0 (t \u2212 1) + K k=1 (\u03b7 k \u2212 \u03b1\u00b5 0 )N k (t \u2212 1) + 4 \u03a8 k (t \u2212 1)N k (t \u2212 1) . We bound f k := (\u03b7 k \u2212 \u03b1\u00b5 0 )N k (t \u2212 1) + 4 \u03a8 k (t \u2212 1)N k (t \u2212 1). Since (13) N k (t\u22121) \u2264 16\u03c8 k (t\u22121) \u03b7 2 k +1\n, and \u03b7 k \u2212\u03b1\u00b5 0 \u2264 \u03b7 k , we have\nf k \u2264 16\u03c8 k (t \u2212 1) \u03b7 k + \u03b7 k + 4 16\u03c8 k (t \u2212 1) 2 \u03b7 2 k + \u03c8 k (t \u2212 1)\nUsing ( x z ) 2 + x \u2264 x z + z 2 for x \u2265 0, z > 0, with x = 4\u03c8 k (t \u2212 1) and z = \u03b7 k , we obtain:\nf k \u2264 16\u03c8 k (t \u2212 1) \u03b7 k + 16\u03c8 k (t \u2212 1) \u03b7 k + 3\u03b7 k \u2264 32\u03c8 k (t \u2212 1) \u03b7 k + 3\u03b7 k . (16\n)\nUsing \u03c8 k (t \u2212 1) = \u0393 \u03c9 log 2K \u03b8 log((1 + \u03c9)N k (t \u2212 1)) if N k (t) > 0 and N k (t \u2212 1) \u2264 H k by Lemma 11, we obtain\nf k \u2264 32\u0393 \u03c9 \u03b7 k log 2K \u03b8 log ((1 + \u03c9)H k ) + 3\u03b7 k .\nThis bound is also valid when N k (t) > 0.\nGoing back to (15), and since \u00b5 0 \u2264 \u00b5 0 (t \u2212 1) under E, we have (notice \u03b7 k \u2264 2 since \u00b5 k \u2208 [0, 1] and \u2208 [0, 1]):\n\u03b1N 0 (t \u2212 1)\u00b5 0 \u2264(1 \u2212 \u03b1)\u00b5 0 (t \u2212 1) + 6K + K k=1 32\u0393 \u03c9 \u03b7 k log 2K \u03b8 log ((1 + \u03c9)H k ) .\n(17) To bound the first term of the right-hand side, let us first notice that the final result holds if N 0 (t\u22121) \u2264 max k\u2208[K] H k . So we can assume N 0 (t \u2212 1) > max k\u2208[K] H k from now on. By the definition of the H k s (see above ( 13)), this implies N 0 (t \u2212 1) > 16\u03c80(t\u22121) \u03b7 2 min , which in turn implies 4\u03b2 0 (t \u2212 1) \u2264 \u03b7 min .\nWe thus use \u00b5 0 (t\u22121) \u2264 \u00b5 0 +2\u03b2 0 (t\u22121) \u2264 \u00b5 0 + \u03b7min 2 \u2264 2, which gives the final result.\nThe result directly follows from (17).\nThe proof of Theorem 8 follows from \u03c4 = K k=1 N k (\u03c4 ) + N 0 (\u03c4 ), by setting \u03c9 = 1 for ease of reading, and \u03c3 = 1 2 since Bernoulli variables are 1 2 -subgaussian (using Hoeffding's inequality (Hoeffding 1963)).\nWe prove Corollary 9.1 from Theorem 7 and Theorem 8. We now prove Theorem 9:\nProof. Since playing the baseline is neutral in the cost of exploration, it can be re-written as:\nC \u03c4 = K k=1 (\u00b5 0 \u2212 \u00b5 k )N k (\u03c4 ) \u2264 k:\u00b5 k <\u00b50 (\u00b5 0 \u2212 \u00b5 k )N k (\u03c4 ),\nwhere \u03c4 is the time the algorithm stops. Using Lemma 11 to upper bound N k (\u03c4 ), we obtain the result.\nCorollary 9.1 simply follows from the fact that by applying each algorithm with confidence \u03b4/M , the confidence intervals are then simultaneously valid for all users with probability 1 \u2212 \u03b4, so all the correctness/duration/cost proofs holds for all groups simultaneously with probability 1 \u2212 \u03b4. For the statistical guarantees on certifying the probabilistic envyfreeness criterion, we provide the proof of Theorem 2 in App. E.5.", "publication_ref": ["b48", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "E.4 Proof of Theorem 1", "text": "Theorems 7, 8, and 9 are summarized in Theorem 1 in the main paper. We restate Theorem 1 and prove it below: Theorem. Let \u2208 (0, 1], \u03b1 \u2208 (0, 1], \u03b4 \u2208 (0, 1 2 ) and \u03b7 k = max(\u00b5 k \u2212 \u00b5 0 , \u00b5 0 + \u2212 \u00b5 k ) and h k = max(1, 1 \u03b7 k ).\nUsing \u00b5, \u00b5 and \u03a6 given in Lemmas 4 and 6, OCEF achieves the following guarantees with probability at least 1 \u2212 \u03b4:\n\u2022 OCEF is correct and satisfies the conservative constraint on the recommendation performance (3).\n\u2022 The duration is in O\nK k=1 h k log K log( Kh k \u03b4\u03b7 k\n) \u03b4 min(\u03b1\u00b5 0 , \u03b7 k ) .\n\u2022 The cost is in O\nk:\u00b5 k <\u00b50 (\u00b50\u2212\u00b5 k )h k \u03b7 k log K log( Kh k \u03b4\u03b7 k ) \u03b4 .\nProof. With \u03b4 \u2208 (0, 1 2 ), let \u03b8 = log(2) \u03b4 6 . Then Theorems 8 and 9 hold for (\u03b4, \u03b8).\nDuration We first show that:\nH k = O h k \u03b7 k log Kh k \u03b4\u03b7 k ,(18)\nlog(H k ) = O log Kh k \u03b4\u03b7 k .(19)\nRecall from Th. 8 that H k is defined as: because Kh k \u03b4 \u2265 3 as soon as K \u2265 2. We thus have\nH k = 1 + O 1 \u03b7 2 k log K \u03b4 log Kh k \u03b4\u03b7 k =B ,(20)\nUsing log(x) \u2264 x \u21d2 x log(x) \u2264 x 2 for x \u2265 0, and the fact that log Kh k \u03b4\u03b7 k \u2265 0, we have: 128 log 2(K+1) log(2H k ) \u03b8 \u03b1\u00b5 0 \u03b7 k , so that H 0 = max(max k\u2208[K] H k , \u0393).\nB \u2264 log Kh k \u03b4\u03b7 k log Kh k \u03b4\u03b7 k \u2264 2 log Kh k \u03b4\u03b7 k . Since 1 + 1 \u03b7 2 k \u2264 2 h k \u03b7 k ,\nWe have:\n\u0393 = O K \u03b1\u00b5 0 + K k=1 h k \u03b1\u00b5 0 log K log(H k ) \u03b4 = O K k=1 h k \u03b1\u00b5 0 log K log(H k ) \u03b4 = O K k=1 h k \u03b1\u00b5 0 log K log( Kh k \u03b4\u03b7 k ) \u03b4 ,\nwhere the second equality is because K = K k=1 1 \u2264 K k=1 h k , and the last equality uses eq. ( 19). Combining this with eq. ( 18) we have:\nH 0 = O K k=1 h k min(\u03b1\u00b5 0 , \u03b7 k ) log K log( Kh k \u03b4\u03b7 k ) \u03b4 .\nUsing eq. ( 18) again to bound \u03c4 = H 0 + K k=1 H k , , we get the desired bound for duration.\nCost For the cost, we remind the bound given in Th. 9:\nC \u03c4 \u2264 k:\u00b5 k <\u00b50 (\u00b5 0 \u2212 \u00b5 k )H k = O k:\u00b5 k <\u00b50 (\u00b5 0 \u2212 \u00b5 k )h k \u03b7 k log K \u03b4 log Kh k \u03b4\u03b7 k(24\n) using ( 20) and 1 + 1\n\u03b7 2 k = O( h k \u03b7 k ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.5 Proof of Theorem 2", "text": "We restate Theorem 2 which summarizes the guarantees for the audit of the probabilistic envy-freeness criterion with AUDIT, and we prove it below:\nTheorem. Let , \u03b3, \u03bb \u2208 (0, 1], \u03b4 \u2208 (0, 1 2 ). LetM = log(3/\u03b4) \u03bb and K = log(3M /\u03b4) log(1/(1\u2212\u03b3)) . With probability at least 1 \u2212 \u03b4,\n\u2022 AUDIT satisfies the conservative constraint (3) for allM audited users, \u2022 the bounds on duration and cost from Th. 1 (using \u03b4 3M instead of \u03b4) are simultaneously valid, \u2022 if AUDIT outputs ( , \u03b3, \u03bb)-envy-free, then the recommender system is ( , \u03b3, \u03bb)-envy-free, and if it outputs not-envy-free, then \u2203(m, n), u m (\u03c0 m ) < u m (\u03c0 n ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We would like to thank J\u00e9r\u00f4me Lang, Levent Sagun and the anonymous reviewers for their constructive comments on earlier versions of this paper.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Proof. The first point is a consequence of Theorem 7 and the second point is a consequence of Theorems 8 and 9. Since we apply OCEF to each target user with confidence \u03b4 3M , by the union bound the confidence intervals are simultaneously valid for allM target users with probability 1 \u2212 \u03b4 3 . Therefore, with probability at least 1 \u2212 \u03b4 3 , the conservative constraint is satisfied for allM users and the bounds on cost and duration hold simultaneously for allM users.\nWe now prove the third bullet point in two steps.\nStep 1 We show that the value of K = log(3M /\u03b4) log(1/(1\u2212\u03b3)) is chosen to guarantee the following result: with probability 1 \u2212 \u03b4 3M , if for a user we have\n\u00b5 k , then the user is not ( , \u03b3)-envious.\nFirst, we apply the theorem on random subset selection from (Sch\u00f6lkopf and Smola (2002), Theorem 6.33), which guarantees that with probability 1 \u2212 (1 \u2212 \u03b3) K , the arm with maximal reward among the K arms is in the (1 \u2212 \u03b3)-quantile range of all possible M arms. Solving for (1 \u2212 \u03b3) K = \u03b4 3M , we get that when K = log(3M /\u03b4) log(1/(1\u2212\u03b3)) , the arm with maximal reward among the K is in the (1 \u2212 \u03b3) quantile range with probability 1 \u2212 \u03b4 3M . This means that if for a target user m, we have u m (\u03c0\n3M , we also have:\nmeaning the user is not ( , \u03b3)-envious. By a union bound over theM target users, the property holds simultaneously for allM target users with probability 1 \u2212 \u03b4 3 .\nStep 2 We now show that the number of users to audit M = log(3/\u03b4) \u03bb is chosen to guarantee that if none of th\u1ebd M sampled users are ( , \u03b3)-envious, then this holds true for an (1 \u2212 \u03bb) fraction of the whole population with probability 1 \u2212 \u03b4 3 . Let \u03b4 = \u03b4 3 . Denoting q the probability that a user is not ( , \u03b3)-envious, we want to guarantee that q \u2265 1 \u2212 \u03bb with probability at least 1 \u2212 \u03b4 , usingM Bernoulli trials where p := 1 \u2212 q is the probability of success.\nLetB(M , k, \u03b4 ) denote the largest p such that the probability of observing k or more successes is at least 1 \u2212 \u03b4 (i.e.,B(M , k, \u03b4 ) is the binomial tail inversion). By definition, we have p \u2264B(M , 0, \u03b4 ). Using the property that B(M , 0, \u03b4 ) \u2264 log(1/\u03b4 ) M (see e.g., (Langford 2005)), we can guarantee that p \u2264 \u03bb as soon as\nWe combining Step 1 and 2 by a union bound: if forM users and K arms, we have \u00b5 0 + \u2265 max k\u2208 [K] \u00b5 k , then with probability 1 \u2212 2\u03b4 3 , an (1 \u2212 \u03bb) fraction of the whole population is not ( , \u03b3)-envious -or equivalently, the recommender system is ( , \u03b3, \u03bb)-envy-free. Since OCEF is correct with probability 1 \u2212 \u03b4 3 when outputting that \u00b5 0 + \u2265 max k\u2208 [K] \u00b5 k (i.e., -no-envy), the union bound guarantees with probability 1 \u2212 \u03b4 that AUDIT is correct when outputting ( , \u03b3, \u03bb)envy-free. Since OCEF is correct with probability \u2265 1\u2212\u03b4 when outputting envy, then so is AUDIT when outputting not-envy-free, which achieves the proof of the third bullet point.", "publication_ref": ["b41", "b35"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Auditing race and gender discrimination in online housing markets", "journal": "", "year": "2020", "authors": "J Asplund; M Eslami; H Sundaram; C Sandvig; K Karahalios"}, {"ref_id": "b1", "title": "Best arm identification in multi-armed bandits", "journal": "", "year": "2010", "authors": "J.-Y Audibert; S Bubeck"}, {"ref_id": "b2", "title": "Envy-free classification", "journal": "", "year": "2018", "authors": "M.-F Balcan; T Dick; R Noothigattu; A D Procaccia"}, {"ref_id": "b3", "title": "Fairness and Machine Learning", "journal": "", "year": "2018", "authors": "S Barocas; M Hardt; A Narayanan"}, {"ref_id": "b4", "title": "Big Data's Disparate Impact", "journal": "Calif. L. Rev", "year": "2016", "authors": "S Barocas; A D Selbst"}, {"ref_id": "b5", "title": "An informationmaximization approach to blind separation and blind deconvolution", "journal": "Neural computation", "year": "1995", "authors": "A J Bell; T J Sejnowski"}, {"ref_id": "b6", "title": "Equity of attention: Amortizing individual fairness in rankings", "journal": "", "year": "2018", "authors": "A J Biega; K P Gummadi; G Weikum"}, {"ref_id": "b7", "title": "Hybrid recommender systems: Survey and experiments. User modeling and user-adapted interaction", "journal": "Springer", "year": "2002", "authors": "S Bubeck; R Munos; G Stoltz"}, {"ref_id": "b8", "title": "", "journal": "", "year": "2017", "authors": "R Burke"}, {"ref_id": "b9", "title": "2nd Workshop on Information Heterogeneity and Fusion in Recommender Systems", "journal": "ACM", "year": "2011", "authors": "I Cantador; P Brusilovsky; T Kuflik"}, {"ref_id": "b10", "title": "", "journal": "", "year": "2017", "authors": "L E Celis; D Straszak; N K Vishnoi"}, {"ref_id": "b11", "title": "Distributed fair allocation of indivisible goods", "journal": "Artificial Intelligence", "year": "2017", "authors": "Y Chevaleyre; U Endriss; N Maudet"}, {"ref_id": "b12", "title": "The measure and mismeasure of fairness: A critical review of fair machine learning", "journal": "", "year": "2018", "authors": "S Corbett-Davies; S Goel"}, {"ref_id": "b13", "title": "Automated experiments on ad privacy settings", "journal": "", "year": "2015", "authors": "A Datta; M C Tschantz; A Datta"}, {"ref_id": "b14", "title": "Fairness through awareness", "journal": "ACM", "year": "2012", "authors": "C Dwork; M Hardt; T Pitassi; O Reingold; R Zemel"}, {"ref_id": "b15", "title": "All the cool kids, how do they fit in?: Popularity and demographic biases in recommender evaluation and effectiveness", "journal": "PMLR", "year": "2018", "authors": "M D Ekstrand; M Tian; I M Azpiazu; J D Ekstrand; O Anuyah; D Mcneill; M S Pera"}, {"ref_id": "b16", "title": "Resource allocation and the public sector", "journal": "Naval research logistics quarterly", "year": "1956", "authors": "D K Foley; M Frank; P Wolfe"}, {"ref_id": "b17", "title": "On tail probabilities for martingales. the Annals of Probability", "journal": "", "year": "1975", "authors": "D A Freedman"}, {"ref_id": "b18", "title": "Improved algorithms for conservative exploration in bandits", "journal": "", "year": "2020", "authors": "E Garcelon; M Ghavamzadeh; A Lazaric; M Pirotta"}, {"ref_id": "b19", "title": "Fairnessaware ranking in search & recommendation systems with application to linkedin talent search", "journal": "", "year": "2019", "authors": "S C Geyik; S Ambler; K Kenthapadi"}, {"ref_id": "b20", "title": "Measuring price discrimination and steering on ecommerce web sites", "journal": "", "year": "2014", "authors": "A Hannak; G Soeller; D Lazer; A Mislove; C Wilson"}, {"ref_id": "b21", "title": "The movielens datasets: History and context", "journal": "", "year": "2015", "authors": "F M Harper; J A Konstan"}, {"ref_id": "b22", "title": "Probability Inequalities for Sums of Bounded Random Variables", "journal": "Journal of the American Statistical Association", "year": "1963", "authors": "W Hoeffding"}, {"ref_id": "b23", "title": "Collaborative filtering for implicit feedback datasets", "journal": "", "year": "2008", "authors": "Y Hu; Y Koren; C ; C Volinsky; M Jagadeesan; S Chawla"}, {"ref_id": "b24", "title": "Auditing for Discrimination in Algorithms Delivering Job Ads", "journal": "", "year": "2021", "authors": "B Imana; A Korolova; J Heidemann"}, {"ref_id": "b25", "title": "Fairness in reinforcement learning", "journal": "", "year": "2017", "authors": "S Jabbari; M Joseph; M Kearns; J Morgenstern; A Roth"}, {"ref_id": "b26", "title": "lil'ucb: An optimal exploration algorithm for multi-armed bandits", "journal": "", "year": "2014", "authors": "K Jamieson; M Malloy; R Nowak; S Bubeck"}, {"ref_id": "b27", "title": "Best-arm identification algorithms for multi-armed bandits in the fixed confidence setting", "journal": "IEEE", "year": "2014", "authors": "K Jamieson; R Nowak"}, {"ref_id": "b28", "title": "Logistic matrix factorization for implicit feedback data", "journal": "", "year": "2014", "authors": "C C Johnson"}, {"ref_id": "b29", "title": "Fairness in learning: Classic and contextual bandits", "journal": "", "year": "2016", "authors": "M Joseph; M Kearns; J H Morgenstern; A Roth"}, {"ref_id": "b30", "title": "Good arm identification via bandit feedback", "journal": "", "year": "2019", "authors": "H Kano; J Honda; K Sakamaki; K Matsuura; A Nakamura; M Sugiyama"}, {"ref_id": "b31", "title": "", "journal": "", "year": "2019", "authors": "M P Kim; A Korolova; G N Rothblum; G Yona"}, {"ref_id": "b32", "title": "Matrix factorization techniques for recommender systems", "journal": "Computer", "year": "2009", "authors": "Y Koren; R Bell; C Volinsky"}, {"ref_id": "b33", "title": "Counterfactual Fairness", "journal": "Curran Associates, Inc", "year": "2017", "authors": "M J Kusner; J Loftus; C Russell; R Silva; I Guyon; U V Luxburg; S Bengio; H Wallach; R Fergus; S Vishwanathan; R Garnett"}, {"ref_id": "b34", "title": "Algorithmic bias? An empirical study of apparent gender-based discrimination in the display of STEM career ads", "journal": "Management Science", "year": "2019", "authors": "A Lambrecht; C Tucker"}, {"ref_id": "b35", "title": "Tutorial on Practical Prediction Theory for Classification", "journal": "Journal of machine learning research", "year": "2005", "authors": "J Langford"}, {"ref_id": "b36", "title": "", "journal": "", "year": "2017", "authors": "Y Liu; G Radanovic; C Dimitrakakis; D Mandal; D C Parkes"}, {"ref_id": "b37", "title": "An optimal algorithm for the thresholding bandit problem", "journal": "", "year": "2016", "authors": "A Locatelli; M Gutzeit; A Carpentier"}, {"ref_id": "b38", "title": "Auditing search engines for differential satisfaction across demographics", "journal": "", "year": "2017", "authors": "R Mehrotra; A Anderson; F Diaz; A Sharma; H Wallach; E Yilmaz"}, {"ref_id": "b39", "title": "FairRec: Two-Sided Fairness for Personalized Recommendations in Two-Sided Platforms", "journal": "", "year": "2020", "authors": "G K Patro; A Biswas; N Ganguly; K P Gummadi; A Chakraborty"}, {"ref_id": "b40", "title": "Closing the AI accountability gap: defining an end-toend framework for internal algorithmic auditing", "journal": "", "year": "2020", "authors": "I D Raji; A Smart; R N White; M Mitchell; T Gebru; B Hutchinson; J Smith-Loud; D Theron; P Barnes"}, {"ref_id": "b41", "title": "Learning with kernels: support vector machines, regularization, optimization, and beyond", "journal": "MIT press", "year": "2002", "authors": "B Sch\u00f6lkopf; A J Smola"}, {"ref_id": "b42", "title": "The possibility of social choice", "journal": "American economic review", "year": "1999", "authors": "A Sen"}, {"ref_id": "b43", "title": "Fairness of exposure in rankings", "journal": "ACM", "year": "2018", "authors": "A Singh; T Joachims"}, {"ref_id": "b44", "title": "A framework for understanding unintended consequences of machine learning", "journal": "", "year": "2019", "authors": "H Suresh; J V Guttag"}, {"ref_id": "b45", "title": "Discrimination in online ad delivery", "journal": "Queue", "year": "2013", "authors": "L Sweeney"}, {"ref_id": "b46", "title": "Fairness without harm: Decoupled classifiers with preference guarantees", "journal": "", "year": "2019", "authors": "B Ustun; Y Liu; D Parkes"}, {"ref_id": "b47", "title": "Modeling dynamic missingness of implicit feedback for recommendation", "journal": "", "year": "2018", "authors": "M Wang; M Gong; X Zheng; K Zhang"}, {"ref_id": "b48", "title": "Conservative bandits. In International Conference on Machine Learning", "journal": "", "year": "2016", "authors": "Y Wu; R Shariff; T Lattimore; C Szepesv\u00e1ri"}, {"ref_id": "b49", "title": "From parity to preference-based notions of fairness in classification", "journal": "", "year": "2017", "authors": "M B Zafar; I Valera; M Rodriguez; K Gummadi; A Weller"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Auditing scenario: the auditor either shows the user their recommendation in the current rec. system, or explores by showing the recommendation given to another user.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": ", we use anytime bounds inspired by the law of the iterated logarithm. These are given in Lem. 4 in App. E.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 3 :3Figure 3: Effect of the conservative exploration parameter \u03b1 on the duration and cost of auditing on Bandit experiments.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 4 :4Figure 4: Scaling w.r.t. \u03b1 on MovieLens (ML) and Last.fm, for recommender systems that are either envy-free (EF) or with envy. There are 41 target users and 75 arms.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "m \u2208 [M ] only likes a single item category s m \u2208 [S], i.e. \u2200a \u2208 A \\ A sm , \u03c1 m (a) = 0, then a\u2208As \u03c1 m (a) a\u2208A \u03c1 m (a) = 1 {s=sm} .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "x\u2208X \u03c0 m (.|x) \u2212 \u03c0 n (.|x) 1 \u2264\u02dc and sup x\u2208X \u03c1 m (.|x) \u2212 \u03c1 n (.|x) 1 \u2264\u02dc . Then, -group envy-freeness implies ( + 4\u02dc )-envy-freeness.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 5 :5Figure 5: Effect of the conservative exploration parameter \u03b1 on the duration and cost of auditing on Bandit experiments.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 6 :6Figure 6: Effect of the confidence parameter \u03b4 on the duration and cost on 4 different bandit instances.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Figure 7 :7Figure 7: Effect of the number of arms on the duration on 4 different bandit instances.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "\u03b8We replace the log + term from Th. 8 by log Kh k \u03b4\u03b7 k > 0,", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Optimal policies with equal user utility penalty (EUU) vs. Unconstrained optimal policies (OPT), computed on ground truth preferences: EUU deteriorates total utility and creates envy between users.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "t \u223c \u03bd m (a m t |x m t ) \u2208 [0, 1].", "formula_coordinates": [2.0, 407.94, 299.61, 105.46, 12.19]}, {"formula_id": "formula_1", "formula_text": "u m (\u03c0) = E x\u223cq m E a\u223c\u03c0(.|x) E r\u223c\u03bd m (a|x) [r] = x\u2208X a\u2208A q m (x)\u03c0(a|x)\u03c1 m (a|x) (1)", "formula_coordinates": [2.0, 356.47, 349.72, 201.53, 40.12]}, {"formula_id": "formula_2", "formula_text": "Definition 3.1. Let \u2265 0. A recommender system is -envy- free if: \u2200m, n \u2208 [M ] : u m (\u03c0 n ) \u2264 + u m (\u03c0 m ).", "formula_coordinates": [3.0, 54.0, 137.89, 240.16, 19.99]}, {"formula_id": "formula_3", "formula_text": "P n\u223cU M u m (\u03c0 m ) + < u m (\u03c0 n ) > \u03b3.", "formula_coordinates": [4.0, 94.61, 192.7, 157.27, 12.34]}, {"formula_id": "formula_4", "formula_text": "4 if \u03b2 0 (t\u22121) > min k\u2208St\u22121 \u03b2 k (t\u22121) or \u03be t < 0 then k t \u2190 0 5 else k t \u2190 t 6", "formula_coordinates": [4.0, 321.39, 173.57, 225.18, 39.07]}, {"formula_id": "formula_5", "formula_text": "7 S t \u2190 k \u2208 S t\u22121 : \u00b5 k (t) > \u00b5 0 (t) + 8 if \u2203k \u2208 S t , \u00b5 k (t) > \u00b5 0 (t) then return envy 9", "formula_coordinates": [4.0, 321.39, 238.75, 204.14, 34.45]}, {"formula_id": "formula_6", "formula_text": "k\u2208[K] \u00b5 k \u2264 \u00b5 0 + .", "formula_coordinates": [4.0, 461.41, 521.1, 76.5, 15.05]}, {"formula_id": "formula_7", "formula_text": "\u2200t, 1 t t s=1 \u00b5 ks \u2265 (1 \u2212 \u03b1)\u00b5 0 .(3)", "formula_coordinates": [4.0, 388.62, 599.76, 169.38, 30.2]}, {"formula_id": "formula_8", "formula_text": "t, \u00b5 k \u2208 [\u00b5 k (t), \u00b5 k (t)]. In the algorithm, \u03b2 k (t) = (\u00b5 k (t) \u2212 \u00b5 k (t))/2.", "formula_coordinates": [5.0, 54.0, 376.62, 238.49, 25.48]}, {"formula_id": "formula_9", "formula_text": "\u03be t = s\u2208A\u03c4 r s \u2212 \u03a6(t) + \u00b5 t (\u03c4 ) + (N 0 (\u03c4 ) \u2212 (1 \u2212 \u03b1)t)\u00b5 0 (\u03c4 ) . (4)", "formula_coordinates": [5.0, 325.62, 87.84, 232.39, 18.53]}, {"formula_id": "formula_10", "formula_text": "Theorem 1. Let \u2208 (0, 1], \u03b1 \u2208 (0, 1], \u03b4 \u2208 (0, 1 2 ) and \u03b7 k = max(\u00b5 k \u2212\u00b5 0 , \u00b5 0 + \u2212\u00b5 k ) and h k = max(1, 1 \u03b7 k ).", "formula_coordinates": [5.0, 319.17, 206.4, 238.83, 26.99]}, {"formula_id": "formula_11", "formula_text": "O K k=1 h k log K log( Kh k/\u03b4\u03b7 k ) \u03b4 min(\u03b1\u00b5 0 , \u03b7 k ) . \u2022 The cost is in O k:\u00b5 k <\u00b50 (\u00b50\u2212\u00b5 k )h k \u03b7 k log K log( Kh k/\u03b4\u03b7 k )) \u03b4 .", "formula_coordinates": [5.0, 319.5, 281.84, 240.24, 61.88]}, {"formula_id": "formula_12", "formula_text": "1 Draw a sampleS ofM = log(3/\u03b4) \u03bb users from [M ] 2 for each user m \u2208S in parallel do 3 Sample K = log(3M /\u03b4) log(1/(1\u2212\u03b3)) arms from [M ] \\ {m} 4", "formula_coordinates": [6.0, 55.49, 129.68, 227.79, 58.68]}, {"formula_id": "formula_13", "formula_text": "Theorem 2. LetM = log(3/\u03b4) \u03bb and K = log(3M /\u03b4) log(1/(1\u2212\u03b3)) .", "formula_coordinates": [6.0, 53.67, 372.17, 240.57, 14.38]}, {"formula_id": "formula_14", "formula_text": "n\u2208[M ] u m (\u03c0 n ) \u2212 u m (\u03c0 m ), 0", "formula_coordinates": [6.0, 370.24, 350.86, 112.3, 16.62]}, {"formula_id": "formula_15", "formula_text": "1 M m\u2208[M ] \u2206 m ,", "formula_coordinates": [6.0, 487.06, 373.22, 56.52, 19.42]}, {"formula_id": "formula_16", "formula_text": "1 M m\u2208[M ] 1 {\u2206 m > } .", "formula_coordinates": [6.0, 465.35, 397.07, 76.74, 19.42]}, {"formula_id": "formula_17", "formula_text": ")| \u2264 D(\u03c0, \u03c0 ). It is easy to see that if max \u03c0\u2208\u03a0 * min m\u2208M D(\u03c0, \u03c0 m ) \u2264\u02dc ,", "formula_coordinates": [10.0, 54.0, 334.51, 239.75, 25.54]}, {"formula_id": "formula_18", "formula_text": "\u2200s \u2208 S, \u2200a \u2208 A s , \u03c0 m,eq (a) = 1 {a=a m s } a\u2208As \u03c1 m (a ) a \u2208A \u03c1 m (a )", "formula_coordinates": [10.0, 327.47, 214.25, 218.59, 40.28]}, {"formula_id": "formula_19", "formula_text": "u m (\u03c0 m,eq ) = s\u2208[S]", "formula_coordinates": [10.0, 356.57, 291.78, 76.91, 22.6]}, {"formula_id": "formula_20", "formula_text": "U (g, g ) = 1 |g| m\u2208g u m 1 |g | n\u2208g \u03c0 n . (6", "formula_coordinates": [10.0, 359.05, 588.6, 195.08, 26.8]}, {"formula_id": "formula_21", "formula_text": ") Definition B.1. Given \u2265 0, the recommender system is - group-envy-free if: \u2200g, g \u2208 G, U (g, g ) \u2264 U (g, g) + .", "formula_coordinates": [10.0, 319.5, 595.66, 240.15, 49.3]}, {"formula_id": "formula_22", "formula_text": "\u2206 m = max max n\u2208[M ] u m (\u03c0 n ) \u2212 u m (\u03c0 m ), 0", "formula_coordinates": [11.0, 351.93, 117.18, 169.08, 17.12]}, {"formula_id": "formula_23", "formula_text": "1 M m\u2208[M ] \u2206 m ,", "formula_coordinates": [11.0, 487.06, 168.09, 56.52, 19.42]}, {"formula_id": "formula_24", "formula_text": "1 M m\u2208[M ] 1 {\u2206 m > } .", "formula_coordinates": [11.0, 465.35, 192.33, 76.74, 19.42]}, {"formula_id": "formula_25", "formula_text": "\u2200m, n \u2208 [M ], u m (\u03c0 m ) = u n (\u03c0 n ), or equivalently: \u2200m \u2208 [M ], u m (\u03c0 m ) = 1 M n\u2208[M ] u n (\u03c0 n ).", "formula_coordinates": [12.0, 54.0, 370.09, 203.15, 62.64]}, {"formula_id": "formula_26", "formula_text": "(EUU) \u03c0 euu b = argmax p:A\u2192[0,1] M \u2200m, a p m (a)=1 m\u2208[M ] u m (p m ) \u2212 b D(p) with D(p) = m\u2208[M ] u m (p m ) \u2212 1 M n\u2208[M ] u n (p n ) 2 .(7)", "formula_coordinates": [12.0, 54.0, 500.6, 238.54, 76.5]}, {"formula_id": "formula_27", "formula_text": "1) \u00b5 0 = 0.6 and \u00b5 k = 0.3 for k \u2208 [9], 2) \u00b5 0 = 0.3, \u00b5 1 = 0.6 and \u00b5 k = 0.3 for k = 2..9,", "formula_coordinates": [12.0, 319.5, 362.21, 200.89, 23.69]}, {"formula_id": "formula_28", "formula_text": "Let \u03c9 \u2208 (0, 1), \u03b8 = log(1 + \u03c9) \u03c9\u03b4 2(2+\u03c9) 1 1+\u03c9 . Let N k (t) = t s=1 1 {ks=k} \u00b5 k (t) = t s=1 r s 1 {ks=k} N k (t) \u03b2 k (t) = 2\u03c3 2 (1 + \u221a \u03c9) 2 (1 + \u03c9) N k (t) \u00d7 log 2(K + 1) \u03b8 log((1 + \u03c9)N k (t)) \u00b5 k (t) = \u00b5 k (t) \u2212 \u03b2 k (t) \u00b5 k (t) = \u00b5 k (t) + \u03b2 k (t)", "formula_coordinates": [13.0, 327.05, 362.34, 221.71, 156.42]}, {"formula_id": "formula_29", "formula_text": "P \u2200t > 0, \u2200k \u2208 [K], \u00b5 k \u2208 [\u00b5 k (t); \u00b5 k (t)] \u2265 1 \u2212 \u03b4 2 .", "formula_coordinates": [13.0, 330.45, 538.74, 216.6, 22.31]}, {"formula_id": "formula_30", "formula_text": "\u03c6(t) = \u03c3 2|A t\u22121 | log 6|At\u22121| 2 \u03b4 + 2 3 log 6|At\u22121| 2 \u03b4 .", "formula_coordinates": [13.0, 319.5, 690.48, 216.36, 16.1]}, {"formula_id": "formula_31", "formula_text": "P \uf8ee \uf8f0 \u2200t > 0, s\u2208At\u22121 (\u00b5 ks \u2212 r s ) \u2264 \u03c6(t) \uf8f9 \uf8fb \u2265 1 \u2212 \u03b4 2 .", "formula_coordinates": [14.0, 71.62, 73.93, 203.27, 33.68]}, {"formula_id": "formula_32", "formula_text": "K k=1 \u03b2 k (t \u2212 1)N k (t \u2212 1), \u03c6(t)", "formula_coordinates": [14.0, 146.44, 154.81, 118.92, 14.11]}, {"formula_id": "formula_33", "formula_text": "E = E 1 \u2229 E 2 with E 1 = \u2200k \u2208 {0, . . . , K}, \u2200t > 0, \u00b5 k (t) \u2208 [\u00b5 k (t); \u00b5 k (t) E 2 = \u2200t > 0, s\u2208At\u22121 (\u00b5 ks \u2212 r s ) \u2264 \u03a6(t) . Then P [E] \u2265 1 \u2212 \u03b4. Proof. By Lemma 4, P [E 1 ] \u2265 1 \u2212 \u03b4 2", "formula_coordinates": [14.0, 53.44, 198.46, 227.24, 99.91]}, {"formula_id": "formula_34", "formula_text": "s\u2208At\u22121 (\u00b5 ks \u2212 r s ) = K k=1 N k (t \u2212 1)(\u00b5 k \u2212 \u00b5 k (t \u2212 1)) .", "formula_coordinates": [14.0, 62.77, 342.74, 225.94, 30.55]}, {"formula_id": "formula_35", "formula_text": "s\u2208At\u22121 (\u00b5 ks \u2212 r s ) \u2264 K k=1 N k (t \u2212 1)\u03b2 k (t \u2212 1).", "formula_coordinates": [14.0, 82.52, 400.08, 186.44, 30.55]}, {"formula_id": "formula_36", "formula_text": "E = E 1 \u2229 E 2 = E 1 \u2229 s\u2208At\u22121 (\u00b5 ks \u2212 r s ) \u2264 \u03c6(t) ,", "formula_coordinates": [14.0, 67.71, 463.96, 211.08, 20.06]}, {"formula_id": "formula_37", "formula_text": "P [E] \u2265 1 \u2212 \u03b4.", "formula_coordinates": [14.0, 204.73, 494.19, 57.67, 9.3]}, {"formula_id": "formula_38", "formula_text": "\u2200k = 0, H k = 1 + 64 \u03b7 2 k log 2(K + 1) log + 128(K+1) \u03b8\u03b7 2 k \u03b8 , H 0 = max max k\u2208[K] H k , 6K + 2 \u03b1\u00b5 0 + K k=1 256 log 2(K+1) log(2H k ) \u03b8 \u03b1\u00b5 0 \u03b7 k .", "formula_coordinates": [14.0, 54.0, 72.9, 496.33, 633.74]}, {"formula_id": "formula_39", "formula_text": "\u03c4 \u2264 K k=0 H k .", "formula_coordinates": [14.0, 412.49, 164.96, 52.53, 30.55]}, {"formula_id": "formula_40", "formula_text": "C t = t\u00b5 0 \u2212 t s=1 \u00b5 ks .(8)", "formula_coordinates": [14.0, 396.15, 239.73, 161.86, 30.2]}, {"formula_id": "formula_41", "formula_text": "C \u03c4 \u2264 k:\u00b5 k <\u00b50 (\u00b5 0 \u2212 \u00b5 k )H k (9)", "formula_coordinates": [14.0, 383.41, 335.09, 174.59, 20.81]}, {"formula_id": "formula_42", "formula_text": "\u03c8 k (t) = 2\u03c3 2 (1 + \u221a \u03c9) 2 (1 + \u03c9) log 2K \u03b8 log((1 + \u03c9)N k (t)) , so that \u03b2 k (t) = \u03c8 k (t) N k (t) .", "formula_coordinates": [15.0, 54.0, 224.72, 258.16, 59.31]}, {"formula_id": "formula_43", "formula_text": "2(2+\u03c9) 1 1+\u03c9 and \u03b7 k = max(\u00b5 k \u2212 \u00b5 0 , \u00b5 0 + \u2212 \u00b5 k ). We denote by \u03b7 min = min k\u2208[K] \u03b7 k .", "formula_coordinates": [15.0, 54.0, 328.1, 238.5, 39.83]}, {"formula_id": "formula_44", "formula_text": "\u00b5 k + 2\u03b2 k (t) \u2265 \u00b5 k (t) \u2265 \u00b5 k \u2265 \u00b5 k (t) \u2265 \u00b5 k \u2212 2\u03b2 k (t). (10) Lemma 10. Under event E, for every k \u2208 [K], if k is pulled at round t, then 4\u03b2 k (t) \u2265 \u03b7 k .", "formula_coordinates": [15.0, 54.0, 398.49, 238.5, 41.14]}, {"formula_id": "formula_45", "formula_text": "\u00b5 k (t \u2212 1) > \u00b5 0 (t \u2212 1) + (11) \u00b5 k (t \u2212 1) \u2264 \u00b5 0 (t \u2212 1)(12)", "formula_coordinates": [15.0, 119.31, 479.12, 173.19, 40.51]}, {"formula_id": "formula_46", "formula_text": "\u00b5 k +2\u03b2 k (t\u22121) \u2265 \u00b5 k (t\u22121) > \u00b5 0 (t\u22121)+ \u2265 \u00b5 0 \u22122\u03b2 0 (t\u22121)+ .", "formula_coordinates": [15.0, 54.0, 600.06, 251.36, 12.59]}, {"formula_id": "formula_47", "formula_text": "\u03b2 0 (t \u2212 1) \u2264 \u03b2 k (t \u2212 1), hence 4\u03b2 k (t \u2212 1) \u2265 \u00b5 0 + \u2212 \u00b5 k .", "formula_coordinates": [15.0, 54.0, 620.37, 238.5, 20.61]}, {"formula_id": "formula_48", "formula_text": "we have \u00b5 k \u2212 2\u03b2 k (t) \u2264 \u00b5 0 + 2\u03b2 0 (t) and since \u03b2 0 (t) \u2264 \u03b2 k (t), we obtain 4\u03b2 k (t \u2212 1) \u2265 \u00b5 k \u2212 \u00b5 0 .", "formula_coordinates": [15.0, 54.0, 642.51, 239.67, 20.61]}, {"formula_id": "formula_49", "formula_text": "N k (\u03c4 ) \u2264 H k with H k = 1 + 32\u03c3 2 (1 + \u221a \u03c9) 2 (1 + \u03c9) \u03b7 2 k \u00d7 log 2(K + 1) log + 64(K+1)\u03c3 2 (1+ \u221a \u03c9) 2 (1+\u03c9) 2 \u03b8\u03b7 2 k \u03b8 Proof. Let \u03c4 > 0, k \u2208 [K]", "formula_coordinates": [15.0, 319.5, 75.61, 206.62, 91.65]}, {"formula_id": "formula_50", "formula_text": "N k (t \u2212 1) \u2264 16\u03c8 k (t\u22121) \u03b7 2 k , which writes, if N k (t) > 0: N k (t \u2212 1) \u2264 16\u03c8 k (t \u2212 1) \u03b7 2 k \u2264 16\u0393 \u03c9 \u03b7 2 k log 2K \u03b8 log ((1 + \u03c9)N k (t \u2212 1)) .(13", "formula_coordinates": [15.0, 319.0, 213.62, 239.0, 102.84]}, {"formula_id": "formula_51", "formula_text": "N k (t \u2212 1) \u2264 16\u0393 \u03c9 \u03b7 2 k log 2K \u03b8 log (1 + \u03c9)32K\u0393 \u03c9 \u03b8\u03b7 2 k (14", "formula_coordinates": [15.0, 324.84, 381.65, 229.01, 24.74]}, {"formula_id": "formula_52", "formula_text": ")", "formula_coordinates": [15.0, 553.85, 388.71, 4.15, 8.64]}, {"formula_id": "formula_53", "formula_text": "N 0 (\u03c4 ) \u2264 max max k\u2208[K] H k , 6K + 2 \u03b1\u00b5 0 + K k=1 64\u03c3 2 (1 + \u221a \u03c9) 2 (1 + \u03c9) log 2(K+1) log((1+\u03c9)H k ) \u03b8 \u03b1\u00b5 0 \u03b7 k Proof.", "formula_coordinates": [15.0, 319.5, 475.54, 239.34, 79.45]}, {"formula_id": "formula_54", "formula_text": "\u03b2 0 (t\u22121) > min k\u2208[K] \u03b2 k (t\u2212 1). Then N 0 (\u03c4 ) = N 0 (t \u2212 1) + 1 \u2264 1 + max k =0 N k (t \u2212 1).", "formula_coordinates": [15.0, 319.0, 574.93, 239.0, 37.89]}, {"formula_id": "formula_55", "formula_text": "s\u2208At\u22121 r s \u2212 \u03a6(t)+\u00b5 t (t\u22121) + (N 0 (t\u22121) \u2212 (1 \u2212 \u03b1)t)\u00b5 0 (t\u22121) < 0", "formula_coordinates": [15.0, 322.51, 665.52, 232.47, 36.98]}, {"formula_id": "formula_56", "formula_text": "\u03b1N (t \u2212 1)\u00b5 0 (t \u2212 1) \u2264 (1 \u2212 \u03b1)\u00b5 0 (t \u2212 1) + (1 \u2212 \u03b1) K k=1 N k (t \u2212 1)\u00b5 0 (t \u2212 1) \u2212 s\u2208At\u22121 r s + \u03a6(t) (15)", "formula_coordinates": [16.0, 54.0, 87.36, 241.1, 57.84]}, {"formula_id": "formula_57", "formula_text": "K k=1 N k (t \u2212 1)\u00b5 0 (t \u2212 1) \u2264 K k=1 N k (t \u2212 1)(\u00b5 0 + 2\u03b2 0 (t \u2212 1)) \u2264 K k=1 N k (t \u2212 1)(\u00b5 0 + 2\u03b2 k (t \u2212 1)) = s\u2208At\u22121 \u00b5 0 + K k=1 2\u03b2 k (t \u2212 1)N k (t \u2212 1). Using Lemma 6, we also bound \u2212 s\u2208At\u22121 r s \u2265 s\u2208At\u22121 \u00b5 s + \u03a6(t) (under E).", "formula_coordinates": [16.0, 59.89, 194.84, 232.61, 169.6]}, {"formula_id": "formula_58", "formula_text": "\u03b1N 0 (t \u2212 1)\u00b5 0 (t \u2212 1) \u2264 (1 \u2212 \u03b1)\u00b5 0 (t \u2212 1) + 2(1 \u2212 \u03b1) K k=1 N k (t \u2212 1)\u03b2 k (t \u2212 1) + s\u2208At\u22121 ((1 \u2212 \u03b1)\u00b5 0 \u2212 \u00b5 ks ) + 2\u03a6(t).", "formula_coordinates": [16.0, 72.48, 382.88, 201.53, 72.97]}, {"formula_id": "formula_59", "formula_text": "\u03b1N 0 (t \u2212 1)\u00b5 0 (t \u2212 1) \u2264 (1 \u2212 \u03b1)\u00b5 0 (t \u2212 1) + K k=1 (\u03b7 k \u2212 \u03b1\u00b5 0 )N k (t \u2212 1) + 4 \u03a8 k (t \u2212 1)N k (t \u2212 1) . We bound f k := (\u03b7 k \u2212 \u03b1\u00b5 0 )N k (t \u2212 1) + 4 \u03a8 k (t \u2212 1)N k (t \u2212 1). Since (13) N k (t\u22121) \u2264 16\u03c8 k (t\u22121) \u03b7 2 k +1", "formula_coordinates": [16.0, 53.65, 536.07, 240.79, 121.66]}, {"formula_id": "formula_60", "formula_text": "f k \u2264 16\u03c8 k (t \u2212 1) \u03b7 k + \u03b7 k + 4 16\u03c8 k (t \u2212 1) 2 \u03b7 2 k + \u03c8 k (t \u2212 1)", "formula_coordinates": [16.0, 56.63, 675.8, 233.25, 25.57]}, {"formula_id": "formula_61", "formula_text": "f k \u2264 16\u03c8 k (t \u2212 1) \u03b7 k + 16\u03c8 k (t \u2212 1) \u03b7 k + 3\u03b7 k \u2264 32\u03c8 k (t \u2212 1) \u03b7 k + 3\u03b7 k . (16", "formula_coordinates": [16.0, 355.38, 87.86, 198.47, 50.19]}, {"formula_id": "formula_62", "formula_text": ")", "formula_coordinates": [16.0, 553.85, 121.89, 4.15, 8.64]}, {"formula_id": "formula_63", "formula_text": "f k \u2264 32\u0393 \u03c9 \u03b7 k log 2K \u03b8 log ((1 + \u03c9)H k ) + 3\u03b7 k .", "formula_coordinates": [16.0, 340.78, 187.46, 195.94, 23.22]}, {"formula_id": "formula_64", "formula_text": "\u03b1N 0 (t \u2212 1)\u00b5 0 \u2264(1 \u2212 \u03b1)\u00b5 0 (t \u2212 1) + 6K + K k=1 32\u0393 \u03c9 \u03b7 k log 2K \u03b8 log ((1 + \u03c9)H k ) .", "formula_coordinates": [16.0, 319.5, 264.13, 242.58, 45.63]}, {"formula_id": "formula_65", "formula_text": "C \u03c4 = K k=1 (\u00b5 0 \u2212 \u00b5 k )N k (\u03c4 ) \u2264 k:\u00b5 k <\u00b50 (\u00b5 0 \u2212 \u00b5 k )N k (\u03c4 ),", "formula_coordinates": [16.0, 327.94, 546.31, 221.62, 31.22]}, {"formula_id": "formula_66", "formula_text": "K k=1 h k log K log( Kh k \u03b4\u03b7 k", "formula_coordinates": [17.0, 152.81, 191.9, 87.01, 35.34]}, {"formula_id": "formula_67", "formula_text": "k:\u00b5 k <\u00b50 (\u00b50\u2212\u00b5 k )h k \u03b7 k log K log( Kh k \u03b4\u03b7 k ) \u03b4 .", "formula_coordinates": [17.0, 133.03, 228.42, 148.79, 30.4]}, {"formula_id": "formula_68", "formula_text": "H k = O h k \u03b7 k log Kh k \u03b4\u03b7 k ,(18)", "formula_coordinates": [17.0, 117.08, 319.85, 175.42, 23.22]}, {"formula_id": "formula_69", "formula_text": "log(H k ) = O log Kh k \u03b4\u03b7 k .(19)", "formula_coordinates": [17.0, 113.29, 358.38, 179.21, 23.23]}, {"formula_id": "formula_70", "formula_text": "H k = 1 + O 1 \u03b7 2 k log K \u03b4 log Kh k \u03b4\u03b7 k =B ,(20)", "formula_coordinates": [17.0, 78.56, 478.93, 213.94, 37.13]}, {"formula_id": "formula_71", "formula_text": "B \u2264 log Kh k \u03b4\u03b7 k log Kh k \u03b4\u03b7 k \u2264 2 log Kh k \u03b4\u03b7 k . Since 1 + 1 \u03b7 2 k \u2264 2 h k \u03b7 k ,", "formula_coordinates": [17.0, 54.0, 557.6, 213.67, 46.9]}, {"formula_id": "formula_72", "formula_text": "\u0393 = O K \u03b1\u00b5 0 + K k=1 h k \u03b1\u00b5 0 log K log(H k ) \u03b4 = O K k=1 h k \u03b1\u00b5 0 log K log(H k ) \u03b4 = O K k=1 h k \u03b1\u00b5 0 log K log( Kh k \u03b4\u03b7 k ) \u03b4 ,", "formula_coordinates": [17.0, 348.11, 172.05, 168.18, 100.91]}, {"formula_id": "formula_73", "formula_text": "H 0 = O K k=1 h k min(\u03b1\u00b5 0 , \u03b7 k ) log K log( Kh k \u03b4\u03b7 k ) \u03b4 .", "formula_coordinates": [17.0, 336.34, 327.74, 204.82, 30.92]}, {"formula_id": "formula_74", "formula_text": "C \u03c4 \u2264 k:\u00b5 k <\u00b50 (\u00b5 0 \u2212 \u00b5 k )H k = O k:\u00b5 k <\u00b50 (\u00b5 0 \u2212 \u00b5 k )h k \u03b7 k log K \u03b4 log Kh k \u03b4\u03b7 k(24", "formula_coordinates": [17.0, 326.96, 421.1, 226.89, 63.85]}, {"formula_id": "formula_75", "formula_text": "\u03b7 2 k = O( h k \u03b7 k ).", "formula_coordinates": [17.0, 397.9, 484.84, 52.46, 16.01]}], "doi": ""}