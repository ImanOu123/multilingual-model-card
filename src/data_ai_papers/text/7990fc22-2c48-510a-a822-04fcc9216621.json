{"title": "DeepCap: Monocular Human Performance Capture Using Weak Supervision", "authors": "Marc Habermann; Weipeng Xu; Michael Zollhoefer; Gerard Pons-Moll; Christian Theobalt", "pub_date": "2020-03-18", "abstract": "Human performance capture is a highly important computer vision problem with many applications in movie production and virtual/augmented reality. Many previous performance capture approaches either required expensive multi-view setups or did not recover dense space-time coherent geometry with frame-to-frame correspondences. We propose a novel deep learning approach for monocular dense human performance capture. Our method is trained in a weakly supervised manner based on multi-view supervision completely removing the need for training data with 3D ground truth annotations. The network architecture is based on two separate networks that disentangle the task into a pose estimation and a non-rigid surface deformation step. Extensive qualitative and quantitative evaluations show that our approach outperforms the state of the art in terms of quality and robustness.", "sections": [{"heading": "Introduction", "text": "Human performance capture, i.e, the space-time coherent 4D capture of full pose and non-rigid surface deformation of people in general clothing, revolutionized the film and gaming industry in recent years. Apart from visual effects, it has many use cases in generating personalized dynamic virtual avatars for telepresence, virtual try-on, mixed reality, and many other areas. In particular for the latter applications, being able to performance capture humans from monocular video would be a game changer. The majority of established monocular methods only captures articulated motion (including hands or sparse facial expression at most). However, the monocular tracking of dense full-body deformations of skin and clothing, in addition to articulated pose, which play an important role in producing realistic virtual characters, is still at its infancy.\nIn literature, multi-view marker-less methods [13,14,15,17,24,29,50,55,81,82,86,64,65] have shown compelling results. However, these approaches rely on well-controlled multi-camera studios (typically with green screen), which prohibits them from being used for location shootings of films and telepresence in living spaces.\nRecent monocular human modeling approaches have shown compelling reconstructions of humans, including clothing, hair and facial details [70,99,2,3,9,60,52]. Figure 1. We present the first learning-based approach for dense monocular human performance capture using weak multi-view supervision that not only predicts the pose but also the space-time coherent non-rigid deformations of the model surface. Some directly regress voxels [28,99] or the continuous occupancy of the surface [70]. Since predictions are pixel aligned, reconstructions have nice detail, but limbs are often missing, especially for difficult poses. Moreover, the recovered motion is not factorized into articulation and non-rigid deformation, which prevents the computer-graphics style control over the reconstructions that is required in many of the aforementioned applications. Importantly, surface vertices are not tracked over time, so no space-time coherent model is captured. Another line of work predicts deformations or displacements to an articulated template, which prevents missing limbs and allows more control [2,9,5,67]. However, these works do not capture motion and the surface deformations.\nThe state-of-the-art monocular human performance capture methods [89,32] densely track the deformation of the surface. They leverage deep learning-based sparse keypoint detections and perform an expensive template fitting afterwards. In consequence, they can only non-rigidly fit to the input view and suffer from instability. By contrast, we present the first learning-based method that jointly infers the articulated and non-rigid 3D deformation parameters in a single feed-forward pass at much higher performance, accuracy and robustness. The core of our method is a CNN model which integrates a fully differentiable mesh template parameterized with pose and an embedded deformation graph. From a single image, our network predicts the skeletal pose, and the rotation and translation parameters for each node in the deformation graph. In stark contrast to implicit representations [70,99,22], our mesh-based method tracks the surface vertices over time, which is crucial for adding semantics, and for texturing and rendering in graphics. Further, by virtue of our parameterization, our model always produces a human surface without missing limbs, even during occlusions and out-of-plane motions.\nWhile previous methods [70,99,2,9] rely on 3D ground truth for training, our method is weakly supervised from multi-view images. To this end, we propose a fully differentiable architecture which is trained in an analysis-bysynthesis fashion, without explicitly using any 3D ground truth annotation. Specifically, during training, our method only requires a personalized template mesh of the actor and a multi-view video sequence of the actor performing various motions. Then, our network learns to predict 3D pose and dense non-rigidly deformed surface shape by comparing its single image feed-forward predictions in a differentiable manner against the multi-view 2D observations. At test time, our method only requires a single-view image as input and produces a deformed template matching the actor's non-rigid motion in the image. In summary, the main technical contributions of our work are:\n\u2022 A learning-based 3D human performance capture approach that jointly tracks the skeletal pose and the nonrigid surface deformations from monocular images.\n\u2022 A new differentiable representation of deforming human surfaces which enables training from multi-view video footage directly.\nOur new model achieves high quality dense human performance capture results on our new challenging dataset, demonstrating, qualitatively and quantitatively, the advantages of our approach over previous work. We experimentally show that our method produces reconstructions of higher accuracy and 3D stability, in particular in depth, than related work, also under difficult poses.", "publication_ref": ["b12", "b13", "b14", "b16", "b23", "b28", "b49", "b54", "b80", "b81", "b85", "b63", "b64", "b69", "b98", "b1", "b2", "b8", "b59", "b51", "b27", "b98", "b69", "b1", "b8", "b4", "b66", "b88", "b31", "b69", "b98", "b21", "b69", "b98", "b1", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "In the following, we focus on related work in the field of dense 3D human performance capture and do not review work on sparse 2D pose estimation. Capture using Parametric Models. Monocular human performance capture is an ill-posed problem due to its high dimensionality and ambiguity. Low-dimensional parametric models can be employed as shape and deformation prior. First, model-based approaches leverage a set of simple geometric primitives [63,74,71,54]. Recent methods employ detailed statistical models learned from thousands of highquality 3D scans [6,33,59,65,51,41,45,85,35,97,10]. Deep learning is widely used to obtain 2D and/or 3D joint detections or 3D vertex positions that can be used to inform model fitting [37,48,53,11,46]. An alternative is to regress model parameters directly [42,62,43]. Beyond body shape and pose, recent models also include facial expressions and hand motion [61,88,40,69] leading to very expressive reconstruction results. Since parametric body models do not represent garments, variation in clothing cannot be reconstructed, and therefore many methods recover the naked body shape under clothing [8,7,95,90]. The full geometry of the actor can be reconstructed by non-rigidly deforming the base parametric model to better fit the observations [68,3,4]. But they can only model tight clothes such as T-shirts and pants, but not loose apparel which has a different topology than the body model, such as skirts. To overcome this problem, ClothCap [64] captures the body and clothing separately, but requires active multi-view setups. Physics based simulations have recently been leveraged to constrain tracking (SimulCap [78]), or to learn a model of clothing on top of SMPL (TailorNet [60]). Instead, our method is based on person-specific templates including clothes and employs deep learning to predict clothing deformation based on monocular video directly.\nDepth-based Template-free Capture. Most approaches based on parametric models ignore clothing. The other side of the spectrum are prior-free approaches based on one or multiple depth sensors. Capturing general nonrigidly deforming scenes [73,31], even at real-time frame rates [57,39,31], is feasible, but only works reliably for small, controlled, and slow motions. Higher robustness can be achieved by using higher frame rate sensors [30,47] or multi-view setups [91,27,58,26,96]. Techniques that are specifically tailored to humans increase robustness [93,94,92] by integrating a skeletal motion prior [93] or a parametric model [94,84]. HybridFusion [98] additionally incorporates a sparse set of inertial measurement units. These fusion-style volumetric capture techniques [36,1,49,23,66] achieve impressive results, but do not establish a set of dense correspondences between all frames. In addition, such depth-based methods do not directly generalize to our monocular setting, have a high power consumption, and typically do not work well under sunlight.\nMonocular Template-free Capture. Quite recently, fueled by the progress in deep learning, many template-free monocular reconstruction approaches have been proposed. Due to their regular structure, many implicit reconstruction techniques [80,99] make use of uniform voxel grids. Dee-pHuman [99] combines a coarse scale volumetric reconstruction with a refinement network to add high-frequency details. Multi-view CNNs can map 2D images to 3D volumetric fields enabling reconstruction of a clothed human body at arbitrary resolution [38]. SiCloPe [56] reconstructs a complete textured 3D model, including cloth, from a single image. PIFu [70] regresses an implicit surface representation that locally aligns pixels with the global context of the corresponding 3D object. Unlike voxel-based representations, this implicit per-pixel representation is more memory efficient. These approaches have not been demonstrated to generalize well to strong articulation. Furthermore, implicit approaches do not recover frame-to-frame correspondences which are of paramount importance for downstream applications, e.g., in augmented reality and video editing. In contrast, our method is based on a mesh representation and can explicitly obtain the per-vertex correspondences over time while being slightly less general. Template-based Capture. An interesting trade-off between being template-free and relying on parametric models are approaches that only employ a template mesh as prior. Historically, template-based human performance capture techniques exploit multi-view geometry to track the motion of a person [76]. Some systems also jointly reconstruct and obtain a foreground segmentation [13,15,50,87]. Given a sufficient number of multi-view images as input, some approaches [21,17,24] align a personalized template model to the observations using non-rigid registration. All the aforementioned methods require expensive multi-view setups and are not practical for consumer use. Depth-based techniques enable template tracking from less cameras [100,91] and reduced motion models [86,29,81,50] increase tracking robustness. Recently, capturing 3D dense human body deformation just with a single RGB camera has been enabled [89] and real-time performance has been achieved [32]. However, their methods rely on expensive optimization leading either to very long per-frame computation times [89] or the need for two graphics cards [32]. Similar to them, our approach also employs a person-specific template mesh. But differently, our method directly learns to predict the skeletal pose and the non-rigid surface deformations. As shown by our experimental results, benefiting from our multi-view based self-supervision, our reconstruction accuracy significantly outperforms the existing methods.", "publication_ref": ["b62", "b73", "b70", "b53", "b5", "b32", "b58", "b64", "b50", "b40", "b44", "b84", "b34", "b96", "b9", "b36", "b47", "b52", "b10", "b45", "b41", "b61", "b42", "b60", "b87", "b39", "b68", "b7", "b6", "b94", "b89", "b67", "b2", "b3", "b63", "b77", "b59", "b72", "b30", "b56", "b38", "b30", "b29", "b46", "b90", "b26", "b57", "b25", "b95", "b92", "b93", "b91", "b92", "b93", "b83", "b97", "b35", "b0", "b48", "b22", "b65", "b79", "b98", "b98", "b37", "b55", "b69", "b75", "b12", "b14", "b49", "b86", "b20", "b16", "b23", "b99", "b90", "b85", "b28", "b80", "b49", "b88", "b31", "b88", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "Method", "text": "Given a single RGB video of a moving human in general clothing, our goal is to capture the dense deforming surface of the full body. This is achieved by training a neural network consisting of two components: As illustrated in Fig. 2, our pose network, PoseNet, estimates the skeletal pose of the actor in the form of joint angles from a monocular image (Sec. 3.2). Next, our deformation network, DefNet, regresses the non-rigid deformation of the dense surface, which cannot be modeled by the skeletal motion, in the embedded deformation graph representation (Sec. 3.3). To avoid generating dense 3D ground truth annotation, our network is trained in a weakly supervised manner. To this end, we propose a fully differentiable human deformation and rendering model, which allows us to compare the rendering of the human body model to the 2D image evidence and back-propagate the losses. For training, we first capture a video sequence in a calibrated multi-camera green screen studio (Sec. 3.1). Note that our multi-view video is only used during training. At test time we only require a single RGB video to perform dense non-rigid tracking.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Template and Data Acquisition", "text": "Character Model. Our method relies on a person-specific 3D template model. We first scan the actor with a 3D scanner [79] to obtain the textured mesh. Then, it is automatically rigged to a kinematic skeleton, which is parameterized with joint angles \u03b8 \u2208 R 27 , the camera relative rotation \u03b1 \u2208 R 3 and translation t \u2208 R 3 . To model the nonrigid surface deformation, we automatically build an embedded deformation graph G with K nodes following [77]. The nodes are parameterized with Euler angles A \u2208 R K\u00d73 and translations T \u2208 R K\u00d73 . Similar to [32], we segment the mesh into different non-rigidity classes resulting in pervertex rigidity weights s i . This allows us to model varying deformation behaviors of different surface materials, e.g. skin deforms less than clothing (see Eq. 13). Training Data. To acquire the training data, we record a multi-view video of the actor doing various actions in a calibrated multi-camera studio with green screen. To provide weak supervision for the training, we first perform 2D pose detection on the sequences using OpenPose [19,18,72,83] and apply temporal filtering. Then, we generate the foreground mask using color keying and compute the corresponding distance transform image D f,c [12], where f \u2208 [0, F ] and c \u2208 [0, C] denote the frame index and camera index, respectively. During training, we randomly sample one camera view c and frame f for which we crop the recorded image with a bounding box, based on the 2D joint detections. The final training input image I f ,c \u2208 R 256\u00d7256\u00d73 is obtained by removing the background and augmenting the foreground with random brightness, hue, contrast and saturation changes. For simplicity, we describe the operation on frame f and omit the subscript f in following equations.", "publication_ref": ["b78", "b76", "b31", "b18", "b17", "b71", "b82", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Pose Network", "text": "In our PoseNet, we use ResNet50 [34] pretrained on Ima-geNet [25] as backbone and modify the last fully connected layer to output a vector containing the joint angles \u03b8 and the camera relative root rotation \u03b1, given the input image I c . Since generating the ground truth for \u03b8 and \u03b1 is a nontrivial task, we propose weakly supervised training based on fitting the skeleton to multi-view 2D joint detections. Kinematics Layer. To this end, we introduce a kinematics layer as the differentiable function that takes the joint angles \u03b8 and the camera relative rotation \u03b1 and computes the positions P c \u2208 R M \u00d73 of the M 3D landmarks attached to the skeleton (17 body joints and 4 face landmarks). Note that P c lives in a camera-root-relative coordinate system. In order to project the landmarks to other camera views, we Figure 2. Overview of our approach. Our method takes a single segmented image as input. First, our pose network, PoseNet, is trained to predict the joint angles and the camera relative rotation using sparse multi-view 2D joint detections as weak supervision. Second, the deformation network, DefNet, is trained to regress embedded graph rotation and translation parameters to account for non-rigid deformations. To train DefNet, multi-view 2D joint detections and silhouettes are used for supervision.\nneed to transform P c to the world coordinate system:\nP m = R T c P c ,m + t,(1)\nwhere R c is the rotation matrix of the input camera c and t is the global translation of the skeleton.\nGlobal Alignment Layer. To obtain the global translation t, we propose a global alignment layer that is attached to the kinematics layer. It localizes our skeleton model in the world space, such that the globally rotated landmarks R T c P c ,m project onto the corresponding detections in all camera views. This is done by minimizing the distance between the rotated landmarks R T c P c ,m and the corresponding rays cast from the camera origin o c to the 2D joint detections:\nc m \u03c3 c,m (R T c P c ,m + t \u2212 o c ) \u00d7 d c,m 2 , (2\n)\nwhere d c,m is the direction of a ray from camera c to the 2D joint detection p c,m corresponding to landmark m:\nd c,m = (E \u22121 cpc,m ) xyz \u2212 o c (E \u22121 cpc,m ) xyz \u2212 o c .(3)\nHere, E c \u2208 R 4\u00d74 is the projection matrix of camera c andp c,m = (p c,m , 1, 1) T . Each point-to-line distance is weighted by the joint detection confidence \u03c3 c,m , which is set to zero if below 0.4. The minimization problem of Eq. 2 can be solved in closed form:\nt = W \u22121 c,m D c,m (R T c P c ,m \u2212o c )+o c \u2212R T c P c ,m ,(4)\nwhere\nW = c m I \u2212 D c,m .(5)\nHere, I is the 3 \u00d7 3 identity matrix and D c,m = d c,m d T c,m . Note that the operation in Eq. 4 is differentiable with respect to the landmark position P c . Sparse Keypoint Loss. Our 2D sparse keypoint loss for the PoseNet can be expressed as\nL kp (P) = c m \u03bb m \u03c3 c,m \u03c0 c (P m ) \u2212 p c,m 2 ,(6)\nwhich ensures that each landmark projects onto the corresponding 2D joint detections p c,m in all camera views.\nHere, \u03c0 c is the projection function of camera c and \u03c3 c,m is the same as in Eq. 2. \u03bb m is a kinematic chain-based hierarchical weight which varies during training for better convergence (see the supplementary material for details).\nPose Prior Loss. To avoid unnatural poses, we impose a pose prior loss on the joint angles\nL limit (\u03b8) = 27 i=1 \u03a8(\u03b8 i )(7)\n\u03a8(x) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 (x \u2212 \u03b8 max,i ) 2 , if x > \u03b8 max,i (\u03b8 min,i \u2212 x) 2 , if x < \u03b8 min,i 0 , otherwise ,(8)\nthat encourages that each joint angle \u03b8 i stays in a range [\u03b8 min,i , \u03b8 max,i ] depending on the anatomic constraints.", "publication_ref": ["b33", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Deformation Network", "text": "With the skeletal pose from PoseNet alone, the non-rigid deformation of the skin and clothes cannot be fully explained. Therefore, we disentangle the non-rigid deformation and the articulated skeletal motion. DefNet takes the input image I c and regresses the non-rigid deformation parameterized with rotation angles A and translation vectors T of the nodes of the embedded deformation graph. DefNet uses the same backbone architecture as PoseNet, while the last fully connected layer outputs a 6K-dimensional vector reshaped to match the dimensions of A and T. The weights of PoseNet are fixed while training DefNet. Again, we do not use direct supervision on A and T. Instead, we propose a deformation layer with differentiable rendering and use multi-view silhouette-based weak supervision. Deformation Layer. The deformation layer takes A and T from DefNet as input to non-rigidly deform the surface\nY i = k\u2208Nvn(i) w i,k (R(A k )(V i \u2212 G k ) + G k + T k ). (9)\nHere, Y,V \u2208 R N \u00d73 are the vertex positions of the deformed and undeformed template mesh, respectively. w i,k are vertex-to-node weights, but in contrast to [77] we compute them based on geodesic distances. G \u2208 R K\u00d73 are the node positions of the undeformed graph, N vn (i) is the set of nodes that influence vertex i, and R(\u2022) is a function that converts the Euler angles to rotation matrices. We further apply the skeletal pose on the deformed mesh vertices to obtain the vertex positions in the input camera space (10) where the node rotation R sk,k and translation t sk,k are derived from the pose parameters using dual quaternion skinning [44]. Eq. 9 and Eq. 10 are differentiable with respect to pose and graph parameters. Thus, our layer can be integrated in the learning framework and gradients can be propagated to DefNet. So far, V c ,i is still rotated relative to the camera c and located around the origin. To bring them to global space, we apply the inverse camera rotation and the global translation, defined in Eq. 4, V i = R T c V c ,i + t. Non-rigid Silhouette Loss. This loss encourages that the non-rigidly deformed mesh matches the multi-view silhouettes in all camera views. It can be formulated using the distance transform representation [12] \nV c ,i = k\u2208Nvn(i) w i,k (R sk,k (\u03b8, \u03b1)Y i +t sk,k (\u03b8, \u03b1)),\nL sil (V) = c i\u2208Bc \u03c1 c,i D c (\u03c0 c (V i )) 2 . (11\n)\nHere, B c is the set of vertices that lie on the boundary when the deformed 3D mesh is projected onto the distance transform image D c of camera c, and \u03c1 c,i is a directional weighting [32] that guides the gradient in D c . The silhouette loss ensures that the boundary vertices project onto the zero-set of the distance transform, i.e., the foreground silhouette. Sparse Keypoint Graph Loss. Only using the silhouette loss can lead to wrong mesh-to-image assignments, especially for highly articulated motions. To this end, we use a sparse keypoint loss to constrain the mesh deformation, which is similar to the keypoint loss for PoseNet in Eq. 6\nL kpg (M) = c m \u03c3 c,m \u03c0 c (M m ) \u2212 p c,m 2 . (12)\nDifferently from Eq. 6, the deformed and posed landmarks M are derived from the embedded deformation graph. To this end, we can deform and pose the canonical landmark positions by attaching them to its closest graph node g in canonical pose with weight w m,g = 1.0. Landmarks can then be deformed according to Eq. 9, 10, resulting in M c which is brought to global space via M m = R T c M c ,m + t. As-rigid-as-possible Loss. To enforce local smoothness of the surface, we impose an as-rigid-as-possible loss [75] \nL arap (A, T) = k l\u2208Nn(k) u k,l d k,l (A, T) 1 ,(13)\nwhere\nd k,l (A, T)=R(A k )(G l \u2212 G k ) + T k + G k \u2212 (G l + T l ). N n (k)\nis the set of indices of the neighbors of node k. In contrast to [75], we propose weighting factors u k,l that influence the rigidity of respective parts of the graph. We derive u k,l by averaging all per-vertex rigidity weights s i [32] for all vertices (see Sec. 3.1), which are connected to node k or l. Thus, the mesh can deform either less or more depending on the surface material. For example, graph nodes that are mostly connected to vertices on a skirt can deform more freely than nodes that are mainly connected to vertices on the skin.", "publication_ref": ["b76", "b9", "b43", "b11", "b31", "b74", "b74", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "In-the-wild Domain Adaptation", "text": "Since our training set is captured in a green screen studio and our test set is captured in the wild, there is a significant domain gap between them, due to different lighting conditions and camera response functions. To improve the performance of our method on in-the-wild images, we finetune our networks on the monocular test images for a small number of iterations using the same 2D keypoint and silhouette losses as before, but only on a single view. This drastically improves the performance at test time as shown in the supplemental material.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "All our experiments were performed on a machine with an NVIDIA Tesla V100 GPU. A forward pass of our method takes around 50ms, which breaks down to 25ms for PoseNet and 25ms for DefNet. During testing, we use the off-the-shelf video segmentation method of [16] to remove the background in the input image. Our method requires OpenPose's 2D joint detections [19,18,72,83] as Figure 3. Qualitative results. Each row shows results for a different person with varying types of apparel. We visualize input frames and our reconstruction overlayed to the corresponding frame. Note that our results precisely overlay to the input. Further, we show our reconstructions from a virtual 3D viewpoint. Note that they also look plausible in 3D. input during testing to crop the frames and to obtain the 3D global translation with our global alignment layer. Finally, we temporally smooth the output mesh vertices with a Gaussian kernel of size 5 frames.\nDataset. We evaluate our approach on 4 subjects (S1 to S4) with varying types of apparel. For qualitative evaluation, we recorded 13 in-the-wild sequences in different indoor and outdoor environments shown in Fig. 3. For quantitative evaluation, we captured 4 sequences in a calibrated multicamera green screen studio (see Fig. 4), for which we computed the ground truth 3D joint locations using the multiview motion capture software, The Captury [20], and we use a color keying algorithm for ground truth foreground segmentation. All sequences contain a large variety of motions, ranging from simple ones like walking up to more difficult ones like fast dancing or baseball pitching. We will release the dataset for future research.\nQualitative Comparisons. Fig. 3 shows our qualitative Figure 4. Results on our evaluation sequences where input views (IV) and reference views (RV) are available. Note that our reconstruction also precisely overlays on RV even though they are not used for tracking. results on in-the-wild test sequences with various clothing styles, poses and environments. Our reconstructions not only precisely overlay with the input images, but also look plausible from arbitrary 3D view points. In Fig. 5, we qualitatively compare our approach to the related human capture and reconstruction methods [42,32,70,99]. In terms of the shape representation, our method is most Input HMR [42] LiveCap [32] PIFu [70] DeepHuman [99] Ours  [42,32,70,99]. Note that our results overlay more accurately to the input view and also look more plausible from a reference view that was not used for tracking. Ground truth global translation is used to match the reference view for the results of [42,32]. Since PIFu [70] and DeepHuman [99] output meshes with varying topology in a canonical volume without an attached root, it is not possible to apply the ground truth translation and therefore we show the reference view without overlay.\nclosely related to LiveCap [32] that also uses a personspecific template. Since they non-rigidly fit the template only to the monocular input view, their results do not faithfully depict the deformation in other view points. Further, their pose estimation severely suffers from the monocular ambiguities, whereas our pose results are more robust and accurate (see supplemental video). Comparing to the other three methods [42,70,99] that are trained for general subjects, our approach has the following advantages: First, our method recovers the non-rigid deformations of humans in general clothes whereas the parametric modelbased approaches [42,43] only recover naked body shape. Second, our method directly provides surface correspondences over time which is important for AR/VR applications (see supplemental video). In contrast, the results of implicit representation-based methods, PIFu [70] and Dee-pHuman [99], lack temporal surface correspondences and do not preserve the skeletal structure of the human body, i.e., they often exhibit missing arms and disconnected geometry. Furthermore, DeepHuman [99] only recovers a coarse shape in combination with a normal image of the input view, while our method can recover medium-level detailed geometry that looks plausible from all views. Last but not least, all these existing methods have problems when overlaying their reconstructions on the reference view, even though some of the methods show a very good overlay on the input view. In contrast, our approach reconstructs accurate 3D geometry, and therefore, our results can precisely overlay on the reference views (also see Fig. 4). Skeletal Pose Accuracy. We quantitatively compare our pose results (output of PoseNet) to existing pose estimation methods on S1 and S4. To account for different types of apparel, we choose S1 wearing trousers and a T-shirt and S4 wearing a short dress. We rescale the bone length for all methods to the ground truth and evaluate the following metrics on the 14 commonly used joints [53] for every 10th frame: 1) We evaluate the root joint position error or global localization error (GLE) to measure how good the skeleton is placed in global 3D space. Note that GLE can only be evaluated for LiveCap [32] and ours, since other methods only produce up-to-scale depth. 2) To evaluate the accuracy of the pose estimation, we report the 3D percentage of correct keypoints (3DPCK) with a threshold of 150mm of the root aligned poses and the area under the 3DPCK curve (AUC). 3) To factor out the errors in the global rotation, we also report the mean per joint position error (MPJPE) after Procrustes alignment. We compare our approach against the state-of-the-art pose estimation approaches including VNect [53], HMR [42], HMMR [43], and LiveCap [32]. We also compare to a multi-view baseline approach (MVBL), where we use our differentiable skeleton model in an optimization framework to solve for the pose per frame using the proposed multi-view losses. We can see from Tab. 3 that our approach outperforms the related monocular methods in all metrics by a large margin and is even close to MVBL although our method only takes a single image as input. We further compare to VNect [53] fine-tuned on our training images for S1. To this end, we compute the 3D joint position using The Captury [20] to provide ground truth supervision for VNect. On the evaluation sequence for S1, the fine-tuned VNect achieved 95.66% 3DPCK, 52.13% AUC and 47.16mm MPJPE. This shows our weakly supervised approach yields comparable or better results than supervised methods in the person-specific setting. However, our approach does not require 3D ground truth annotation that is difficult to obtain, even for only sparse keypoints, let alone the dense surfaces. Surface Reconstruction Accuracy. To evaluate the accuracy of the regressed non-rigid deformations, we compute the intersection over union (IoU) between the ground truth foreground masks and the 2D projection of the estimated shape on S1 and S4 for every 100th frame. We evaluate the IoU on all views, on all views expect the input view, and on the input view which we refer to as AMVIoU, RVIoU and SVIoU, respectively. To factor out the errors in global localization, we apply the ground truth translation to the reconstructed geometries. For DeepHuman [99] and PIFu [70], we cannot report the AMVIoU and RVIoU, since we cannot overlay their results on reference views as discussed before. Further, PIFu [70] by design achieves perfect overlay on the input view, since they regress the depth for each foreground pixel. However, their reconstruction does not reflect the true 3D geometry (see Fig. 5). Therefore, it is meaningless to report their SVIoU. Similarly, DeepHuman [99] achieves high SVIoU, due to their volumetric representation. But their results are often wrong, when looking from side views. In contrast, our method consistently outperforms all other approaches in terms of AMVIoU and RVIoU, which shows the high accuracy of our method in recovering the 3D geometry. Further, we are again close to the multi-view baseline.\nAblation Study. To evaluate the importance of the number of cameras, the number of training images, and our DefNet, we performed an ablation study on S4 in Tab. 3. 1) In the first group of Tab. 3, we train our networks with supervision using 1 to 7 views. We can see that adding more views consistently improves the quality of the estimated poses and deformations. The most significant improvement is from one to two cameras. This is not surprising, since the single camera settings is inherently ambiguous.   the difference is visually even more significant as shown in Fig. 6. Especially, the skirt is correctly deformed according to the input image whereas the PoseNet-only result cannot fit the input due to the limitation of skinning.", "publication_ref": ["b15", "b18", "b17", "b71", "b82", "b19", "b41", "b31", "b69", "b98", "b41", "b31", "b69", "b98", "b41", "b31", "b69", "b98", "b41", "b31", "b69", "b98", "b31", "b41", "b69", "b98", "b41", "b42", "b69", "b98", "b98", "b52", "b31", "b52", "b41", "b42", "b31", "b52", "b19", "b98", "b69", "b69", "b98"], "figure_ref": ["fig_0", "fig_0", "fig_1"], "table_ref": []}, {"heading": "Conclusion", "text": "We have presented a learning-based approach for monocular dense human performance capture using only weak multi-view supervision. In contrast to existing methods, our approach directly regresses poses and surface deformations from neural networks, produces temporal surface correspondences, preserves the skeletal structure of the human body, and can handle loose clothes. Our qualitative and quantitative results in different scenarios show that our method produces more accurate 3D reconstruction of pose and non-rigid deformation than existing methods. In the future, we plan to incorporate hands and the face to our mesh representation to enable joint tracking of body, facial expressions and hand gestures. We are also interested in physically more correct multi-layered representations to model the garments even more realistically.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "An Efficient Volumetric Framework for Shape Tracking", "journal": "", "year": "2015-06", "authors": "B Allain; J.-S Franco; E Boyer"}, {"ref_id": "b1", "title": "Learning to reconstruct people in clothing from a single RGB camera", "journal": "", "year": "2002", "authors": "T Alldieck; M Magnor; B L Bhatnagar; C Theobalt; G Pons-Moll"}, {"ref_id": "b2", "title": "Video based reconstruction of 3d people models", "journal": "", "year": "", "authors": "T Alldieck; M Magnor; W Xu; C Theobalt; G Pons-Moll"}, {"ref_id": "b3", "title": "Detailed human avatars from monocular video", "journal": "", "year": "2002", "authors": "T Alldieck; M Magnor; W Xu; C Theobalt; G Pons-Moll"}, {"ref_id": "b4", "title": "Tex2shape: Detailed full human body geometry from a single image", "journal": "IEEE", "year": "2001", "authors": "T Alldieck; G Pons-Moll; C Theobalt; M Magnor"}, {"ref_id": "b5", "title": "SCAPE: Shape Completion and Animation of People", "journal": "ACM Transactions on Graphics", "year": "2005", "authors": "D Anguelov; P Srinivasan; D Koller; S Thrun; J Rodgers; J Davis"}, {"ref_id": "b6", "title": "The naked truth: Estimating body shape under clothing", "journal": "Springer", "year": "2008", "authors": "A O B\u0203lan; M J Black"}, {"ref_id": "b7", "title": "Detailed human shape and pose from images", "journal": "", "year": "2007", "authors": "A O Balan; L Sigal; M J Black; J E Davis; H W Haussecker"}, {"ref_id": "b8", "title": "Multi-garment net: Learning to dress 3d people from images", "journal": "IEEE", "year": "2002", "authors": "B L Bhatnagar; G Tiwari; C Theobalt; G Pons-Moll"}, {"ref_id": "b9", "title": "Detailed full-body reconstructions of moving people from monocular RGB-D sequences", "journal": "", "year": "2002", "authors": "F Bogo; M J Black; M Loper; J Romero"}, {"ref_id": "b10", "title": "Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image", "journal": "", "year": "2016", "authors": "F Bogo; A Kanazawa; C Lassner; P Gehler; J Romero; M J Black"}, {"ref_id": "b11", "title": "Distance transformations in digital images. Computer Vision, Graphics, and Image Processing", "journal": "", "year": "1986", "authors": "G Borgefors"}, {"ref_id": "b12", "title": "Posecut: Simultaneous segmentation and 3d pose estimation of humans using dynamic graph-cuts", "journal": "Springer", "year": "2006", "authors": "M Bray; P Kohli; P H Torr"}, {"ref_id": "b13", "title": "High accuracy optical flow serves 3-d pose tracking: exploiting contour and flow based constraints", "journal": "Springer", "year": "2006", "authors": "T Brox; B Rosenhahn; D Cremers; H.-P Seidel"}, {"ref_id": "b14", "title": "Combined region and motion-based 3d tracking of rigid and articulated objects", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2010", "authors": "T Brox; B Rosenhahn; J Gall; D Cremers"}, {"ref_id": "b15", "title": "One-shot video object segmentation", "journal": "", "year": "2017", "authors": "S Caelles; K.-K Maninis; J Pont-Tuset; L Leal-Taix\u00e9; D Cremers; L Van Gool"}, {"ref_id": "b16", "title": "Free-form mesh tracking: a patch-based approach", "journal": "IEEE", "year": "2010", "authors": "C Cagniart; E Boyer; S Ilic"}, {"ref_id": "b17", "title": "OpenPose: realtime multi-person 2D pose estimation using Part Affinity Fields", "journal": "", "year": "2018", "authors": "Z Cao; G Hidalgo; T Simon; S.-E Wei; Y Sheikh"}, {"ref_id": "b18", "title": "Realtime multi-person 2d pose estimation using part affinity fields", "journal": "", "year": "2017", "authors": "Z Cao; T Simon; S.-E Wei; Y Sheikh"}, {"ref_id": "b19", "title": "The Captury", "journal": "", "year": "", "authors": ""}, {"ref_id": "b20", "title": "Free-viewpoint video of human actors", "journal": "ACM Trans. Graph", "year": "2003-07", "authors": "J Carranza; C Theobalt; M A Magnor; H.-P Seidel"}, {"ref_id": "b21", "title": "Implicit functions in feature space for 3d shape reconstruction and completion", "journal": "IEEE", "year": "2001", "authors": "J Chibane; T Alldieck; G Pons-Moll"}, {"ref_id": "b22", "title": "Highquality streamable free-viewpoint video", "journal": "ACM Transactions on Graphics (TOG)", "year": "2015", "authors": "A Collet; M Chuang; P Sweeney; D Gillett; D Evseev; D Calabrese; H Hoppe; A Kirk; S Sullivan"}, {"ref_id": "b23", "title": "Performance capture from sparse multiview video", "journal": "In ACM Transactions on Graphics (TOG)", "year": "2008", "authors": "E De Aguiar; C Stoll; C Theobalt; N Ahmed; H.-P Seidel; S Thrun"}, {"ref_id": "b24", "title": "ImageNet: A Large-Scale Hierarchical Image Database", "journal": "", "year": "2009", "authors": "J Deng; W Dong; R Socher; L.-J Li; K Li; L Fei-Fei"}, {"ref_id": "b25", "title": "Motion2fusion: Real-time volumetric performance capture", "journal": "ACM Trans. Graph", "year": "2002", "authors": "M Dou; P Davidson; S R Fanello; S Khamis; A Kowdle; C Rhemann; V Tankovich; S Izadi"}, {"ref_id": "b26", "title": "Fusion4d: Real-time performance capture of challenging scenes", "journal": "ACM Transactions on Graphics (TOG)", "year": "2016", "authors": "M Dou; S Khamis; Y Degtyarev; P Davidson; S R Fanello; A Kowdle; S O Escolano; C Rhemann; D Kim; J Taylor"}, {"ref_id": "b27", "title": "Moulding humans: Non-parametric 3d human shape estimation from single images", "journal": "", "year": "2019", "authors": "V Gabeur; J.-S Franco; X Martin; C Schmid; G Rogez"}, {"ref_id": "b28", "title": "Motion capture using joint skeleton tracking and surface estimation", "journal": "IEEE", "year": "2009", "authors": "J Gall; C Stoll; E De Aguiar; C Theobalt; B Rosenhahn; H.-P Seidel"}, {"ref_id": "b29", "title": "Twinfusion: High framerate non-rigid fusion through fast correspondence tracking", "journal": "", "year": "2002", "authors": "K Guo; J Taylor; S Fanello; A Tagliasacchi; M Dou; P Davidson; A Kowdle; S Izadi"}, {"ref_id": "b30", "title": "Real-time geometry, albedo, and motion reconstruction using a single rgb-d camera", "journal": "ACM Transactions on Graphics (TOG)", "year": "2017", "authors": "K Guo; F Xu; T Yu; X Liu; Q Dai; Y Liu"}, {"ref_id": "b31", "title": "Livecap: Real-time human performance capture from monocular video", "journal": "ACM Trans. Graph", "year": "2008", "authors": "M Habermann; W Xu; M Zollhoefer; G Pons-Moll; C Theobalt"}, {"ref_id": "b32", "title": "Multilinear pose and body shape estimation of dressed subjects from image sets", "journal": "IEEE", "year": "2010", "authors": "N Hasler; H Ackermann; B Rosenhahn; T Thorm\u00e4hlen; H.-P Seidel"}, {"ref_id": "b33", "title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "K He; X Zhang; S Ren; J Sun"}, {"ref_id": "b34", "title": "Realtime body tracking with one depth camera and inertial sensors", "journal": "", "year": "2013-12", "authors": "T Helten; M Muller; H.-P Seidel; C Theobalt"}, {"ref_id": "b35", "title": "Volumetric 3d tracking by detection", "journal": "", "year": "2016", "authors": "C.-H Huang; B Allain; J.-S Franco; N Navab; S Ilic; E Boyer"}, {"ref_id": "b36", "title": "Towards accurate marker-less human shape and pose estimation over time", "journal": "", "year": "2017", "authors": "Y Huang; F Bogo; C Lassner; A Kanazawa; P V Gehler; J Romero; I Akhter; M J Black"}, {"ref_id": "b37", "title": "Deep Volumetric Video From Very Sparse Multi-View Performance Capture", "journal": "", "year": "2002", "authors": "Z Huang; T Li; W Chen; Y Zhao; J Xing; C Legendre; L Luo; C Ma; H Li"}, {"ref_id": "b38", "title": "VolumeDeform: Real-time Volumetric Non-rigid Reconstruction", "journal": "", "year": "2002", "authors": "M Innmann; M Zollh\u00f6fer; M Nie\u00dfner; C Theobalt; M Stamminger"}, {"ref_id": "b39", "title": "Total capture: A 3d deformation model for tracking faces, hands, and bodies", "journal": "CoRR", "year": "2018", "authors": "H Joo; T Simon; Y Sheikh"}, {"ref_id": "b40", "title": "Reconstructing personalized anatomical models for physics-based body animation", "journal": "ACM Trans. Graph", "year": "2016", "authors": "P Kadlecek; A.-E Ichim; T Liu; J Krivanek; L Kavan"}, {"ref_id": "b41", "title": "End-to-end recovery of human shape and pose. CoRR, abs", "journal": "", "year": "1712", "authors": "A Kanazawa; M J Black; D W Jacobs; J Malik"}, {"ref_id": "b42", "title": "Learning 3d human dynamics from video", "journal": "", "year": "2008", "authors": "A Kanazawa; J Y Zhang; P Felsen; J Malik"}, {"ref_id": "b43", "title": "Skinning with dual quaternions", "journal": "", "year": "2007", "authors": "L Kavan; S Collins; J \u017d\u00e1ra; C O'sullivan"}, {"ref_id": "b44", "title": "Data-driven physics for human soft tissue animation", "journal": "", "year": "2017", "authors": "M Kim; G Pons-Moll; S Pujades; S Bang; J Kim; M Black; S.-H Lee"}, {"ref_id": "b45", "title": "Convolutional mesh regression for single-image human shape reconstruction", "journal": "", "year": "2019", "authors": "N Kolotouros; G Pavlakos; K Daniilidis"}, {"ref_id": "b46", "title": "The need 4 speed in real-time dense visual tracking", "journal": "ACM", "year": "2018", "authors": "A Kowdle; C Rhemann; S Fanello; A Tagliasacchi; J Taylor; P Davidson; M Dou; K Guo; C Keskin; S Khamis; D Kim; D Tang; V Tankovich; J Valentin; S Izadi"}, {"ref_id": "b47", "title": "Unite the people: Closing the loop between 3d and 2d human representations", "journal": "", "year": "2017", "authors": "C Lassner; J Romero; M Kiefel; F Bogo; M J Black; P V Gehler"}, {"ref_id": "b48", "title": "Multi-View Dynamic Shape Refinement Using Local Temporal Integration", "journal": "", "year": "2002", "authors": "V Leroy; J.-S Franco; E Boyer"}, {"ref_id": "b49", "title": "Markerless motion capture of interacting characters using multi-view image segmentation", "journal": "IEEE", "year": "2011", "authors": "Y Liu; C Stoll; J Gall; H.-P Seidel; C Theobalt"}, {"ref_id": "b50", "title": "SMPL: A skinned multi-person linear model", "journal": "ACM Trans. Graphics (Proc. SIGGRAPH Asia)", "year": "2002", "authors": "M Loper; N Mahmood; J Romero; G Pons-Moll; M J Black"}, {"ref_id": "b51", "title": "Learning to dress 3d people in generative clothing", "journal": "IEEE", "year": "2001", "authors": "Q Ma; J Yang; A Ranjan; S Pujades; G Pons-Moll; S Tang; M Black"}, {"ref_id": "b52", "title": "Vnect: Real-time 3d human pose estimation with a single rgb camera", "journal": "", "year": "2007", "authors": "D Mehta; S Sridhar; O Sotnychenko; H Rhodin; M Shafiei; H.-P Seidel; W Xu; D Casas; C Theobalt"}, {"ref_id": "b53", "title": "Shape and nonrigid motion estimation through physics-based synthesis", "journal": "IEEE Trans. PAMI", "year": "1993", "authors": "D Metaxas; D Terzopoulos"}, {"ref_id": "b54", "title": "General dynamic scene reconstruction from multiple view video", "journal": "", "year": "2015", "authors": "A Mustafa; H Kim; J.-Y Guillemaut; A Hilton"}, {"ref_id": "b55", "title": "Siclope: Silhouette-based clothed people", "journal": "", "year": "2018", "authors": "R Natsume; S Saito; Z Huang; W Chen; C Ma; H Li; S Morishima"}, {"ref_id": "b56", "title": "Dynamicfusion: Reconstruction and tracking of non-rigid scenes in real-time", "journal": "", "year": "2002", "authors": "R A Newcombe; D Fox; S M Seitz"}, {"ref_id": "b57", "title": "Holoportation: Virtual 3d teleportation in real-time", "journal": "ACM", "year": "2016", "authors": "S Orts-Escolano; C Rhemann; S Fanello; W Chang; A Kowdle; Y Degtyarev; D Kim; P L Davidson; S Khamis; M Dou"}, {"ref_id": "b58", "title": "Data-driven modeling of skin and muscle deformation", "journal": "In ACM Transactions on Graphics", "year": "2008", "authors": "S I Park; J K Hodgins"}, {"ref_id": "b59", "title": "Tailornet: Predicting clothing in 3d as a function of human pose, shape and garment style", "journal": "IEEE", "year": "2002", "authors": "C Patel; Z Liao; G Pons-Moll"}, {"ref_id": "b60", "title": "Expressive body capture: 3d hands, face, and body from a single image", "journal": "", "year": "2002", "authors": "G Pavlakos; V Choutas; N Ghorbani; T Bolkart; A A A Osman; D Tzionas; M J Black"}, {"ref_id": "b61", "title": "Learning to estimate 3D human pose and shape from a single color image", "journal": "", "year": "2018", "authors": "G Pavlakos; L Zhu; X Zhou; K Daniilidis"}, {"ref_id": "b62", "title": "Tracking and modeling people in video sequences. Computer Vision and Image Understanding", "journal": "", "year": "2001", "authors": "R Pl\u00e4nkers; P Fua"}, {"ref_id": "b63", "title": "ClothCap: Seamless 4D clothing capture and retargeting", "journal": "", "year": "2002", "authors": "G Pons-Moll; S Pujades; S Hu; M Black"}, {"ref_id": "b64", "title": "Dyna: a model of dynamic human shape in motion", "journal": "ACM Transactions on Graphics (TOG)", "year": "2015", "authors": "G Pons-Moll; J Romero; N Mahmood; M J Black"}, {"ref_id": "b65", "title": "Spatiotemporal atlas parameterization for evolving meshes", "journal": "ACM Transactions on Graphics (TOG)", "year": "2017", "authors": "F Prada; M Kazhdan; M Chuang; A Collet; H Hoppe"}, {"ref_id": "b66", "title": "Modeling the geometry of dressed humans", "journal": "", "year": "2001", "authors": "A Pumarola; J Sanchez-Riera; G P T Choi; A Sanfeliu; F Moreno-Noguer"}, {"ref_id": "b67", "title": "General automatic human shape and motion capture using volumetric contour cues", "journal": "Springer International Publishing", "year": "2016", "authors": "H Rhodin; N Robertini; D Casas; C Richardt; H.-P Seidel; C Theobalt"}, {"ref_id": "b68", "title": "Embodied hands: Modeling and capturing hands and bodies together", "journal": "", "year": "2002", "authors": "J Romero; D Tzionas; M J Black"}, {"ref_id": "b69", "title": "Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization. CoRR, abs", "journal": "", "year": "1905", "authors": "S Saito; Z Huang; R Natsume; S Morishima; A Kanazawa; H Li"}, {"ref_id": "b70", "title": "Tracking loose-limbed people", "journal": "IEEE", "year": "2004", "authors": "L Sigal; S Bhatia; S Roth; M J Black; M Isard"}, {"ref_id": "b71", "title": "Hand keypoint detection in single images using multiview bootstrapping", "journal": "", "year": "2017", "authors": "T Simon; H Joo; I Matthews; Y Sheikh"}, {"ref_id": "b72", "title": "Killingfusion: Non-rigid 3d reconstruction without correspondences", "journal": "", "year": "", "authors": "M Slavcheva; M Baust; D Cremers; S Ilic"}, {"ref_id": "b73", "title": "Estimating articulated human motion with covariance scaled sampling. The International", "journal": "Journal of Robotics Research", "year": "2003", "authors": "C Sminchisescu; B Triggs"}, {"ref_id": "b74", "title": "As-rigid-as-possible surface modeling", "journal": "", "year": "2007", "authors": "O Sorkine; M Alexa"}, {"ref_id": "b75", "title": "Surface capture for performancebased animation", "journal": "IEEE Computer Graphics and Applications", "year": "2007", "authors": "J Starck; A Hilton"}, {"ref_id": "b76", "title": "Embedded deformation for shape manipulation", "journal": "ACM Trans. Graph", "year": "2007-07", "authors": "R W Sumner; J Schmid; M Pauly"}, {"ref_id": "b77", "title": "Simulcap : Single-view human performance capture with cloth simulation", "journal": "", "year": "2002", "authors": "Y Tao; Z Zheng; Y Zhong; J Zhao; D Quionhai; G Pons-Moll; Y Liu"}, {"ref_id": "b78", "title": "", "journal": "", "year": "", "authors": " Treedys"}, {"ref_id": "b79", "title": "BodyNet: Volumetric inference of 3D human body shapes", "journal": "", "year": "2018", "authors": "G Varol; D Ceylan; B Russell; J Yang; E Yumer; I Laptev; C Schmid"}, {"ref_id": "b80", "title": "Articulated mesh animation from multi-view silhouettes", "journal": "In ACM Transactions on Graphics", "year": "2008", "authors": "D Vlasic; I Baran; W Matusik; J Popovi\u0107"}, {"ref_id": "b81", "title": "Dynamic shape capture using multi-view photometric stereo", "journal": "ACM Transactions on Graphics (TOG)", "year": "2009", "authors": "D Vlasic; P Peers; I Baran; P Debevec; J Popovi\u0107; S Rusinkiewicz; W Matusik"}, {"ref_id": "b82", "title": "Convolutional Pose Machines", "journal": "", "year": "2016", "authors": "S.-E Wei; V Ramakrishna; T Kanade; Y Sheikh"}, {"ref_id": "b83", "title": "Accurate realtime full-body motion capture using a single depth camera", "journal": "", "year": "2012", "authors": "X Wei; P Zhang; J Chai"}, {"ref_id": "b84", "title": "Home 3d body scans from noisy image and range data", "journal": "", "year": "2011", "authors": "A Weiss; D Hirshberg; M J Black"}, {"ref_id": "b85", "title": "On-set Performance Capture of Multiple Actors With A Stereo Camera", "journal": "", "year": "2013-11", "authors": "C Wu; C Stoll; L Valgaerts; C Theobalt"}, {"ref_id": "b86", "title": "Full body performance capture under uncontrolled and varying illumination: A shading-based approach", "journal": "", "year": "2012", "authors": "C Wu; K Varanasi; C Theobalt"}, {"ref_id": "b87", "title": "Monocular total capture: Posing face, body, and hands in the wild", "journal": "CoRR", "year": "2018", "authors": "D Xiang; H Joo; Y Sheikh"}, {"ref_id": "b88", "title": "Monoperfcap: Human performance capture from monocular video", "journal": "ACM Trans. Graph", "year": "2018-05", "authors": "W Xu; A Chatterjee; M Zollh\u00f6fer; H Rhodin; D Mehta; H.-P Seidel; C Theobalt"}, {"ref_id": "b89", "title": "Estimation of Human Body Shape in Motion with Wide Clothing", "journal": "", "year": "2002", "authors": "J Yang; J.-S Franco; F H\u00e9troy-Wheeler; S Wuhrer"}, {"ref_id": "b90", "title": "Performance capture of interacting characters with handheld kinects", "journal": "", "year": "2012", "authors": "G Ye; Y Liu; N Hasler; X Ji; Q Dai; C Theobalt"}, {"ref_id": "b91", "title": "Real-time simultaneous pose and shape estimation for articulated objects using a single depth camera", "journal": "", "year": "2014", "authors": "M Ye; R Yang"}, {"ref_id": "b92", "title": "Bodyfusion: Real-time capture of human motion and surface geometry using a single depth camera", "journal": "ACM", "year": "2002", "authors": "T Yu; K Guo; F Xu; Y Dong; Z Su; J Zhao; J Li; Q Dai; Y Liu"}, {"ref_id": "b93", "title": "Doublefusion: Real-time capture of human performances with inner body shapes from a single depth sensor", "journal": "IEEE", "year": "2002", "authors": "T Yu; Z Zheng; K Guo; J Zhao; Q Dai; H Li; G Pons-Moll; Y Liu"}, {"ref_id": "b94", "title": "Detailed, accurate, human shape estimation from clothed 3D scan sequences", "journal": "", "year": "", "authors": "C Zhang; S Pujades; M Black; G Pons-Moll"}, {"ref_id": "b95", "title": "Leveraging depth cameras and wearable pressure sensors for fullbody kinematics and dynamics capture", "journal": "ACM Transactions on Graphics (TOG)", "year": "2014", "authors": "P Zhang; K Siu; J Zhang; C K Liu; J Chai"}, {"ref_id": "b96", "title": "Quality dynamic human body modeling using a single low-cost depth camera", "journal": "", "year": "2014", "authors": "Q Zhang; B Fu; M Ye; R Yang"}, {"ref_id": "b97", "title": "HybridFusion: Real-Time Performance Capture Using a Single Depth Sensor and Sparse IMUs", "journal": "", "year": "2018-09", "authors": "Z Zheng; T Yu; H Li; K Guo; Q Dai; L Fang; Y Liu"}, {"ref_id": "b98", "title": "Deephuman: 3d human reconstruction from a single image. CoRR, abs", "journal": "", "year": "1903", "authors": "Z Zheng; T Yu; Y Wei; Q Dai; Y Liu"}, {"ref_id": "b99", "title": "Real-time non-rigid reconstruction using an rgb-d camera", "journal": "ACM Transactions on Graphics (TOG)", "year": "2014", "authors": "M Zollh\u00f6fer; M Nie\u00dfner; S Izadi; C Rhemann; C Zach; M Fisher; C Wu; A Fitzgibbon; C Loop; C Theobalt; M Stamminger"}], "figures": [{"figure_label": "5", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 5 .5Figure 5. Qualitative comparison to other methods[42,32,70,99]. Note that our results overlay more accurately to the input view and also look more plausible from a reference view that was not used for tracking. Ground truth global translation is used to match the reference view for the results of[42,32]. Since PIFu[70] and DeepHuman[99] output meshes with varying topology in a canonical volume without an attached root, it is not possible to apply the ground truth translation and therefore we show the reference view without overlay.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 6 .6Figure 6. PoseNet + DefNet vs. PoseNet-only. DefNet can deform the template to accurately match the input, especially for loose clothing. In addition, DefNet also corrects slight errors in the pose and typical skinning artifacts.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "MPJPE/GLE (in mm) and 3DPCK/AUC (in %) on S1", "figure_data": "MethodGLE\u21933DPCK\u2191AUC\u2191MPJPE\u2193VNect [53]-66.0628.0277.19HMR [42]-82.3943.6172.61HMMR [43]-87.4845.3372.40LiveCap [32]317.0171.1337.9092.84Ours91.0898.4358.7149.11MVBL76.0399.1757.7945.44MPJPE/GLE (in mm) and 3DPCK/AUC (in %) on S4MethodGLE\u21933DPCK\u2191AUC\u2191MPJPE\u2193VNect [53]-82.0642.7372.62HMR [42]-86.8843.9173.63HMMR [43]-82.8041.1877.41LiveCap [32]248.6775.1137.3583.48Ours96.5696.7459.2545.40MVBL75.8296.2057.2745.12Table 1. Skeletal pose accuracy. Note that we are consistentlybetter than other monocular approaches. Moreover, we are evenclose to the multi-view baseline."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "AMVIoU, RVIoU, and SVIoU (in %) on S1 sequence Surface deformation accuracy. We outperform all other monocular methods and are even close to the multi-view baseline.", "figure_data": "MethodAMVIoU\u2191RVIoU\u2191SVIoU\u2191HMR [42]62.2561.768.85HMMR [43]65.9865.5870.77LiveCap [32]56.0254.2177.75DeepHuman [99]--91.57Ours87.287.0389.26MVBL91.7491.7292.02AMVIoU, RVIoU, and SVIoU (in %) on S4 sequenceMethodAMVIoU\u2191RVIoU\u2191SVIoU\u2191HMR [42]65.164.6670.84HMMR [43]63.7963.2970.23LiveCap [32]59.9659.0272.16DeepHuman [99]--84.15Ours82.5382.2286.66MVBL88.1488.0389.66"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "2) In the second group of Tab. 3, we reduce the training data to 1/2 and 1/4. We can see that the more frames with different poses and deformations are seen during training, the better the reconstruction quality is. This is expected since a larger number of frames may better sample the possible space of poses and deformations. 3) In the third group of Tab. 3, we evaluate the AMVIoU on the template mesh animated with the results of PoseNet, which we refer to as PoseNet-only. One can see that on average, the AMVIoU is improved by around 4%. Since most non-rigid deformations rather happen locally,", "figure_data": "3DPCK and AMVIoU (in %) on S4 sequenceMethod3DPCK\u2191AMVIoU\u21911 camera view62.1165.112 camera views93.5278.443 camera views94.7079.757 camera views95.9581.736500 frames85.1973.4113000 frames92.2578.97PoseNet-only96.7478.51Ours(14 views, 26000 frames)96.7482.53"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Ablation study. We evaluate the number of cameras and the number of frames used during training in terms of the 3DPCK and AMVIoU metrics. Adding more cameras and frames consistently improves the quality of reconstruction. Further, DefNet improves the AMVIoU compared to pure pose estimation.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "P m = R T c P c ,m + t,(1)", "formula_coordinates": [4.0, 124.51, 330.31, 161.85, 12.69]}, {"formula_id": "formula_1", "formula_text": "c m \u03c3 c,m (R T c P c ,m + t \u2212 o c ) \u00d7 d c,m 2 , (2", "formula_coordinates": [4.0, 81.37, 490.03, 201.13, 21.69]}, {"formula_id": "formula_2", "formula_text": ")", "formula_coordinates": [4.0, 282.49, 492.42, 3.87, 8.64]}, {"formula_id": "formula_3", "formula_text": "d c,m = (E \u22121 cpc,m ) xyz \u2212 o c (E \u22121 cpc,m ) xyz \u2212 o c .(3)", "formula_coordinates": [4.0, 103.51, 551.07, 182.85, 25.63]}, {"formula_id": "formula_4", "formula_text": "t = W \u22121 c,m D c,m (R T c P c ,m \u2212o c )+o c \u2212R T c P c ,m ,(4)", "formula_coordinates": [4.0, 55.09, 652.97, 231.27, 21.69]}, {"formula_id": "formula_5", "formula_text": "W = c m I \u2212 D c,m .(5)", "formula_coordinates": [4.0, 118.76, 696.69, 167.6, 19.64]}, {"formula_id": "formula_6", "formula_text": "L kp (P) = c m \u03bb m \u03c3 c,m \u03c0 c (P m ) \u2212 p c,m 2 ,(6)", "formula_coordinates": [4.0, 324.54, 383.01, 220.57, 21.69]}, {"formula_id": "formula_7", "formula_text": "L limit (\u03b8) = 27 i=1 \u03a8(\u03b8 i )(7)", "formula_coordinates": [4.0, 381.23, 521.93, 163.89, 30.32]}, {"formula_id": "formula_8", "formula_text": "\u03a8(x) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 (x \u2212 \u03b8 max,i ) 2 , if x > \u03b8 max,i (\u03b8 min,i \u2212 x) 2 , if x < \u03b8 min,i 0 , otherwise ,(8)", "formula_coordinates": [4.0, 338.95, 565.36, 206.16, 40.69]}, {"formula_id": "formula_9", "formula_text": "Y i = k\u2208Nvn(i) w i,k (R(A k )(V i \u2212 G k ) + G k + T k ). (9)", "formula_coordinates": [5.0, 55.58, 228.88, 230.78, 20.56]}, {"formula_id": "formula_10", "formula_text": "V c ,i = k\u2208Nvn(i) w i,k (R sk,k (\u03b8, \u03b1)Y i +t sk,k (\u03b8, \u03b1)),", "formula_coordinates": [5.0, 55.09, 380.95, 209.69, 20.56]}, {"formula_id": "formula_11", "formula_text": "L sil (V) = c i\u2208Bc \u03c1 c,i D c (\u03c0 c (V i )) 2 . (11", "formula_coordinates": [5.0, 85.3, 576.15, 196.91, 22.13]}, {"formula_id": "formula_12", "formula_text": ")", "formula_coordinates": [5.0, 282.21, 578.55, 4.15, 8.64]}, {"formula_id": "formula_13", "formula_text": "L kpg (M) = c m \u03c3 c,m \u03c0 c (M m ) \u2212 p c,m 2 . (12)", "formula_coordinates": [5.0, 318.83, 111.13, 226.29, 21.69]}, {"formula_id": "formula_14", "formula_text": "L arap (A, T) = k l\u2208Nn(k) u k,l d k,l (A, T) 1 ,(13)", "formula_coordinates": [5.0, 324.44, 261.65, 220.67, 20.56]}, {"formula_id": "formula_15", "formula_text": "d k,l (A, T)=R(A k )(G l \u2212 G k ) + T k + G k \u2212 (G l + T l ). N n (k)", "formula_coordinates": [5.0, 308.86, 315.48, 235.33, 31.12]}], "doi": ""}