{"title": "Question-Answering in a Low-resourced Language: Benchmark Dataset and Models for Tigrinya", "authors": "Fitsum Gaim; Wonsuk Yang; Hancheol Park; Jong C Park", "pub_date": "", "abstract": "Question-Answering (QA) has seen significant advances recently, achieving near human-level performance over some benchmarks. However, these advances focus on high-resourced languages such as English, while the task remains unexplored for most other languages, mainly due to the lack of annotated datasets. This work presents a native QA dataset for an East African language, Tigrinya. The dataset contains 10.6K question-answer pairs spanning 572 paragraphs extracted from 290 news articles on various topics. The dataset construction method is discussed, which is applicable to constructing similar resources for related languages. We present comprehensive experiments and analyses of several resource-efficient approaches to QA, including monolingual, cross-lingual, and multilingual setups, along with comparisons against machine-translated silver data. Our strong baseline models reach 76% in the F1 score, while the estimated human performance is 92%, indicating that the benchmark presents a good challenge for future work. We make the dataset, models, and leaderboard publicly available. 1 * Coressponding author 1 TiQuAD: https://github.com/fgaim/tiquad Article: \u25cayM \u00a7M\" [The Red Sea] Paragraph: \u25cayM \u00a7M\" K\u00d4 \u00bfb\u00b5m b\u0192r \u00a7'e\u00b0Mb\" z\u00d5w'U \u00a7Mr\u00b3t 'A \u2020m\u00b9 \u2248 \u2020t b\u00bb \u00a7M\", \u00d0\u02c6m \u00a7M\", \u25cayM \u00a7M\" \u00bcm\u222b'wn \u00d3'e\u00d7 \u00a7M\" \u2026\u00cf~OE\u00a4\u00a3l s\u2126 d\u2248 \u00bfb\u00b5m \u0192b g\u00cc k m\u00b2 bbzJ z\u2022bI bf\u2030y d\u2248 t\u2022y\u00c1dz\u221am \u201a\"t\"n z\u00b0 \u00a7h \u2021 OEr\u00cb\u2248t z\u221a\u2022\u00b0\u0192 \u00c5s\u00d1n bzhb\u00c7 \u25cayM Mb\" mff\u2022 yn\u00da \u2021[ The Red Sea is one of the four seas in the world that are named after common colors: the Yellow Sea, the Black Sea, the Red Sea and the White Sea. The origin of its name is attributed to the red color given by the poisonous bacteria, especially the Trichodesmium Erythraeum, which breed in large numbers during the Summer season.]", "sections": [{"heading": "Introduction", "text": "Question Answering (QA) and Machine Reading Comprehension (MRC) have seen significant advances in recent years, achieving human-level performance on large-scale benchmarks (Rajpurkar et al., 2016(Rajpurkar et al., , 2018. The main factors driving the progress are the adaption of large pre-trained large language models and the proliferation of QA datasets (Rogers et al., 2022). However, most studies focus on high-resourced languages, while the task remains unexplored for most of the World's diverse languages. The primary challenge for non-English QA is the lack of native annotated datasets. In particular, there is little to no study done on scarcely resourced languages such as Tigrinya that are markedly different from English in terms of linguistic properties including syntax, morphology, and typography.\nThis work presents TiQuAD, the first publicly available Question-Answering Dataset for Tigrinya; see Figure 1 for an example entry. We collaborate with native Tigrinya speakers to collect documents and annotate the dataset, yielding a total of 10.6K question-answer pairs with 6.5K unique questions over 572 paragraphs gathered from 290 news articles.\nWe assess the quality of annotations and explore strong baselines by fine-tuning TiRoBERTa and TiELECTRA (Gaim et al., 2021) as monolingual models of Tigrinya and XLM-R (Conneau et al., 2020) and AfriBERTa (Ogueji et al., 2021) as representative multilingual models. In addition to the monolingual QA setup, we perform three scenarios of cross-lingual and multilingual experiments. First, we translate SQuAD1.1 to Tigrinya and evaluate the performance in conjunction with the native TiQuAD. Second, we assess a zero-shot crosslingual transfer learning approach (Artetxe et al., 2020;Lewis et al., 2020) by evaluating on the new dataset. Third, we explore the performance of a multilingual setup by jointly fine-tuning the models on English and Tigrinya datasets. The experimental settings are depicted in Figure 2. The bestperforming baseline model achieves up to 76% in F1 score in the multilingual setup, while the estimated human performance is 92%. Considering the challenges of constructing annotated datasets for under-represented languages, we believe this work could serve as a reference case for similar languages. In particular, the TiQuAD benchmark is an important milestone in the advancement of question-answering for the Tigrinya language.\nThe contributions of this work are summarized as follows: (1) We build the first questionanswering dataset for Tigrinya and make it publicly available. (2) We present an in-depth analysis of the challenges of question answering in Tigrinya based on the dataset. (3) We apply transformer-based language models to the question-answering task in Tigrinya and compare it with datasets of other languages. (4) We investigate various resourceefficient cross-lingual and multilingual approaches to QA and assess the utility of the native dataset.", "publication_ref": ["b30", "b29", "b31", "b10", "b4", "b26", "b0", "b20"], "figure_ref": ["fig_0", "fig_2"], "table_ref": []}, {"heading": "Related Work", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Tigrinya Language", "text": "Tigrinya (ISOv3: tir) is a Semitic language, part of the Afro-Asiatic family with over 10 million native speakers in the East African regions of Eritrea and Northern Ethiopia. Tigrinya is closely related to Amharic and Tigre languages that are also spoken in similar regions and share the same ancestor, the now extinct Ge'ez language. In recent years, there is a growing research body and interest in Tigrinya. Gasser (2011) developed HornMorph, a morphological analysis and generation framework for Tigrinya, Amharic, and Oromo by employing Finite State Transducers (FSTs). Later, Tedla and Yamamoto (2018) employed a manually constructed dataset to train a Long Short-Term Memory (LSTM) model for morphological segmentation in Tigrinya. Osman and Mikami (2012) proposed a rule-based stemmer for a Lucene based Tigrinya information retrieval.  presented a part-of-speech (POS) corpus for Tigrinya with over 72K annotated tokens across 4.6K sentences. A few studies explored statistical and neural machine translation, between English and Tigrinya, by exploiting morphological segmentation Gaim, 2017;Tedla and Yamamoto, 2018) and data augmentation via back-translation (\u00d6ktem et al., 2020;Kidane et al., 2021). More recent studies applied pretrained language models to various downstream tasks such as part-of-speech tagging, sentiment analysis, and named entity recognition (Tela et al., 2020;Gaim et al., 2021;Yohannes and Amagasa, 2022). Moreover, Gaim et al. (2022) presented a dataset and method for the automatic identification of five typologically related East African languages that include Tigrinya. However, despite the recent progress, Tigrinya still lacks basic computational resources for most downstream tasks with very limited availability of annotated datasets.", "publication_ref": ["b12", "b35", "b28", "b9", "b35", "b27", "b18", "b37", "b10", "b41", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Question-Answering beyond English", "text": "Native reading comprehension datasets beyond the English language are relatively rare. Efforts have been made to build MRC datasets in Chinese, French, German, and Korean, among others, all of which are designed following the formulation of SQuAD. The SberQuAD dataset (Efimov et al., 2020) is a Russian native reading comprehension dataset made up of 50K samples. The CMRC 2018 (Cui et al., 2019) dataset is a Chinese reading comprehension dataset that gathers 20K question and answer pairs. The KorQuAD dataset (Lim et al., 2019) is a Korean native reading comprehension dataset containing 70K samples. On the end of low-resourced languages, Mozannar et al. (2019) developed ARCD for Arabic with 1.3K samples. Keren and Levy (2021) presented ParaShoot, a reading comprehension dataset for Hebrew with a size of 3.8K question-answer pairs. More recently, Kazemi et al. (2022) built PersianQuAD, a native MRC dataset for Persian with over 20K samples.\nCross-lingual Question Answering Languagespecific datasets are costly and challenging to build, and one alternative is to develop cross-lingual models that can transfer to a target without requiring training data in that language (Lewis et al., 2020). It has been shown that unsupervised multilingual models generalize well in a zero-shot cross-lingual setting (Artetxe et al., 2020). For this reason, crosslingual question answering has recently gained traction with the availability of a few benchmarks. Artetxe et al. (2020) built XQuAD by translating 1190 question-answer pairs from the SQuAD1.1 development set by professional translators into ten other languages.\nMultilingual Question Answering The MLQA dataset (Lewis et al., 2020) consists of over 12K question and answer samples in English and 5000 samples in six other languages such as Arabic, German and Spanish. More recently, Clark et al. (2020) presented TyDiQA, a dataset particularly designed to address information-seeking and natural questions covering 11 typologically diverse languages with a total of 204K samples. Longpre et al. (2021) presented an open domain dataset comprising 10K question-answer pairs aligned across 26 typologically diverse languages, yielding a total of 260K samples. Hu et al. (2020) presented XTREME, a multi-task benchmark for nine prominent NLP tasks including question-answering across 40 languages. Ruder et al. (2021) further extended the benchmark to XTREME-R, covering ten tasks across 50 typologically diverse languages. Xue et al. (2021) proposed a large multilingual pretrained model that handles 101 languages. Note that none of the aforementioned datasets and models include the Tigrinya language.\nTranslated QA datasets Another relatively inexpensive alternative to building a native annotated QA dataset is translating an existing English dataset to the target language. Carrino et al. (2020) explored this by proposing the Translate-Align-Retrieve (TAR) method to translate the English SQuAD1.1 dataset to Spanish. Then the resulting dataset was used to fine-tune a multilingual model achieving a performance of 68.1/48.3% F1/EM on MLQA (Lewis et al., 2020) and 77.6/61.8% F1/EM on XQuAD (Artetxe et al., 2020). Similar approaches were also adapted for the Japanese and French languages (Asai et al., 2018;Siblini et al., 2019), where a multilingual version of BERT    is trained on the English SQuAD1.1 and evaluated on the small translated corpus, reaching promising scores of 76.7% in F1 and 61.8% in EM.", "publication_ref": ["b8", "b5", "b21", "b25", "b17", "b16", "b20", "b0", "b0", "b20", "b3", "b22", "b14", "b32", "b2", "b20", "b0", "b1", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "Dataset Annotation", "text": "TiQuAD is designed following the task formulation of SQuAD (Rajpurkar et al., 2016), where each entry in the dataset is a triple consisting of a paragraph, a question, and the corresponding answer. The answer is a contiguous span of text in the paragraph, a typical setup of extractive questionanswering.\nThe dataset was constructed in four stages: First, a diverse set of articles are collected from which we extract paragraphs that will serve as contexts. Second, the initial question and answer pairs are annotated for all the extracted paragraphs. Third, additional answers are annotated for all the questions in the development and test sets. Fourth, we post-process the annotations for quality control and remove noisy examples. The final dataset contains over 10.6K question-answer pairs across 572 paragraphs. While the size is on the smaller end compared to the English datasets, it reflects a realistic amount of data that researchers of low-resourced languages can acquire with a limited annotation budget. 2 The dataset characteristics are presented in Table 1. In the following sections, we present the data collection and annotation processes.", "publication_ref": ["b30"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Collecting Articles", "text": "In the absence of sufficient Tigrinya content on Wikipedia 3 , the Haddas Ertra 4 newspaper provides a large body of professionally edited Tigrinya text, covering diverse domains and has been used as the main source in previous research    Gaim et al., 2021). We collected 550 Tigrinya articles from Haddas Ertra covering a wide range of topics, including science, health, business, history, culture, and sports, published in a period of seven years, 2015-2021. The articles that contain at least 500 characters of plain text are kept after filtering out images and tabular content. We split the dataset randomly into training, development, and test sets of 205, 43, and 42 articles, respectively.", "publication_ref": ["b10"], "figure_ref": [], "table_ref": []}, {"heading": "Annotating Question-Answer Pairs", "text": "In the first round of annotation, we recruited eight native speakers of Tigrinya [4 female, 4 male] with ages ranging from 20 to 46. Each annotator is presented with a random paragraph from the collection and tasked to write questions that can be explicitly answered by a contiguous segment of text in the provided context. The annotators were encouraged to phrase questions in their own words instead of copying words from the context and to highlight the minimal span of characters that answer the respective question. The annotators were asked to spend on average one minute for each question and answer pair. The end result of this stage is the set of 6,674 unique questions across all documents.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Collecting Additional Answers", "text": "In the second round of annotation, we asked four of the original annotators to provide a second answer to questions in the development and test parts of the dataset. Our annotation tool ensures that annotators cannot give a second answer to the questions they contributed already in the second stage. Finally, we recruited two new annotators to provide a third reference answer to all the questions in the evaluation sets. These annotators were not involved in the first round of the annotation; with no prior exposure to the task, they are expected to show less bias towards the question formulation. We ensure that all entries in the test and development sets have at least three answers from different annotators, resulting in 6,205 answers for 2,056 questions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Post-processing Annotations", "text": "Throughout the annotation campaign, we collected over 6,674 unique questions and 10,600 answers, i.e., 2,056 of the questions had at least three groundtruth answers by different annotators. From these annotations, we discarded 166 entries (2.5%) that either contained apparent errors, were incomplete, unanswerable by the context, or had a wrong question formulation such as verification (yes/no) and cloze type. For instance, the question \"]\u2248n\u00dbs \u00b3M\u00b3y \u00bfb \u0192sOE\u2022 \u0192s\u00b3t 28 \u00be\u2039 \u00b0r nX \u00b6\u2126 ___ rJc tr\u00bcb~[Lower Shmangus is located about 28 km from Asmara on the ___ side.]\" is in cloze format, hence deleted. We also removed outlier entries that had answers with more than 250 characters.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dataset Analysis", "text": "To assess the quality and diversity of the dataset, we perform various analyses of the annotations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Question-type Analysis", "text": "We clustered all questions in the development set into nine types using a manually curated list of question words. As presented in  for \u224872% of all the questions. These types of questions also make up the largest proportions in other datasets (Keren and Levy, 2021;d'Hoffschmidt et al., 2020 ", "publication_ref": ["b17", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Question-Context Lexical Overlap", "text": "The degree of lexical overlap between questions and paragraphs might affect the difficulty of a dataset. To assess this behavior in TiQuAD, we analyzed 100 random samples from the development set and assigned them to four categories of question-context-answer linguistic relationships proposed by Rajpurkar et al. (2016): (1) Synonymy implies that key terms in the question are synonyms of words in the context; (2) World knowledge implies that the question requires world knowledge to find the corresponding answer in the context; (3) Syntactic/Morphological variation implies a difference in the structure between the question and the answer in the context; (4) Multi-sentence reasoning implies that answering a question requires combining knowledge from multiple sentences in the context. We observe that syntax and morphology variations are the most common type in TiQuAD.\nThe results of our findings are presented in Table 3.", "publication_ref": ["b30"], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Answer correctness and length", "text": "We randomly selected 100 question-answer pairs from the validation set to assess the accuracy and length of the answers manually. We specifically check whether each annotated answer is correct and has a minimal length in answering the corresponding question. We observe that 74% of the answers are accurate and with a minimum span length, while a significant minority, 23%, contain extra information and are longer by a factor of 1.5 on average than the desired span. Only 3% were shorter than the optimal span length, such as partial annotation of the answer.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sequence Lengths", "text": "The ]. Over 57% of the answers have three or fewer words, but there are cases with up to 32 words that typically constitute a list of items.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Estimating Human Performance", "text": "We assess the human performance on TiQuAD's development and test sets, where each question has at least three answers. In SQuAD, Rajpurkar et al. (2016) use the second answer as the prediction and the rest as ground truths; while in FQuAD, d'Hoffschmidt et al. ( 2020) compute the average by successively taking each of the three answers as the prediction. For TiQuAD, the third answer is regarded as a prediction, and it is annotated by a control group who had no prior exposure to the task, as elaborated in Section 3.3. We obtain scores of 84.80% EM and 92.80% F1 in the development set, and 82.80% EM and 92.24% F1 in the test set, which are comparable to those of the SQuAD and FQuAD benchmarks.\nWe analyzed the cases where the human annotators failed to agree and observed that they are mainly due to extra tokens in the answer spans rather than fundamental differences. For instance, the question \u00a1\u00aen\u00b1s n\u02dc\" \u0192 bkn\u00d4y\u00b0OErN \u0192\u2030? [With how many ", "publication_ref": ["b30"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Model Training", "text": "Given a question Q and a context paragraph P from an entry in a QA dataset, the training objective is to predict the start and end positions of the answer span within the paragraph. Following , we set the input to the transformer model as a concatenation of Q and P , separated by a special delimiter token, SEP. Two linear layers, S and E, are introduced to learn the starting and ending positions of answer spans, respectively. Then the probability distributions of token i being the start or the end of an answer span with respect to all tokens in the context can be computed as follows:  where T is the model's output of the context sequence, and T i is the hidden state of the i-th token.\nP start (i) = exp (S \u2022 T i ) j=1 exp (S \u2022 T j ) ,(1)\nP end (i) = exp (E \u2022 T i ) j=1 exp (E \u2022 T j ) ,(2)\nThe score for a candidate span (i, j) is defined as the product of the start and end position probabilities, and then the highest-scoring span where j \u2265 i is used as the final prediction.\nScore(i, j) = P start (i) \u2022 P end (j).\n(\n)3\nThe loss function L is the sum of the negative log-likelihoods of the ground truth start and end positions, denoted as i * and j * , respectively.\nL = \u2212 log P start (i * ) \u2212 log P end (j * ) (4)\nDuring training, a gradient-based optimizer minimizes the loss and gradually enables the model to accurately predict the answer spans in the context.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation Metrics", "text": "We use the standard Exact Match (EM) and F1 metrics for evaluation. EM is the percentage of predictions that exactly match the ground truth. F1 score is the average overlap between the predicted tokens and the ground truth, hence rewards partial matches. For both metrics, when there are multiple ground truth answers for a given question in the test set, the final score represents the highest overlap between the prediction and all the reference answers. To improve the robustness of the evaluation, SQuAD (Rajpurkar et al., 2016) removes the English punctuation and articles before computing the scores. Other non-English datasets have also adapted the metrics (d'Hoffschmidt et al., 2020;M\u00f6ller et al., 2021). In the case of TiQuAD, we remove Tigrinya's articles, common functional tokens, and the punctuation set of its writing system, the Ge'ez Script (Gaim et al., 2022). ", "publication_ref": ["b30", "b7", "b24", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental setup", "text": "We designed six experimental configurations and evaluated each on four models of varying sizes, ranging from 14 to 278 million parameters. Details of the models are presented in In all experiments, we use AdamW (Loshchilov and Hutter, 2019) as the optimizer with the weight decay parameter set to 0.01 and a learning rate of 3e\u22125. We set the mini-batch size to 16 and finetune for 3 epochs, except in the Native settings, where only the small native dataset is used, the batch size and number of epochs are set to 8 and 5, respectively. In the settings where only the small native dataset is used for training, we use a minibatch size of 8 and fine-tune for 5 epochs; in all other settings, the batch size and the number of epochs are set to 16 and 3, respectively. The experiments were implemented using the HuggingFace Transformers library (Wolf et al., 2020) and ran on a single NVIDIA V100 GPU.\nTranslation of English dataset For the experiments, we machine translated the training part of SQuAD v1.1 to Tigrinya. The positional information of the answer spans needs to be computed as it is generally lost during translation, making it difficult to retain the original data size. As a remedy,  we applied two machine translation services 5 and aggregated the aligned entries, while discarding the spurious ones. This resulted in 46.7K questionanswer pairs that we use for model training in our experiments.", "publication_ref": ["b23"], "figure_ref": [], "table_ref": []}, {"heading": "Results and Discussions", "text": "In this section, we present and discuss the results of the proposed experimental setups.\n6.1 End-to-end Tigrinya QA\nIn this setup, we train all models on the native and translated Tigrinya datasets then evaluate on the TiQuAD development and test sets. The experimental results are presented in Table 5. Native vs. Translated QA Datasets For models TiRoBERTa BASE and XLM-R BASE , we observe significant gains when training on the native dataset over the translated one, despite the latter being 10 times larger. The performance of TiRoBERTa BASE increases by 5 and 3 points in EM and F1 scores on the test set, respectively. However, we observe that the smaller models TiELECTRA SMALL and AfriBERTa BASE perform better when trained on the translated data than on the native one. More consistent performance benefits are observed in all models when the two datasets are combined. For instance, TiRoBERTa BASE gains up to 10 points in EM and F1 than when it is trained on the datasets separately. Overall, our experiments show: (1) A small native dataset can make a positive impact when augmented with larger low-quality data; (2) Machine translated datasets are useful augmentation but can be suboptimal when used alone depending on the quality; and (3) A native dataset could be a vital resource in the evaluation process.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_12"]}, {"heading": "Monolingual vs. Multilingual QA Models", "text": "When comparing models of comparable sizes, we observe that the monolingual models achieve better performance than their multilingual counterparts. As shown in Table 5, TiRoBERTa BASE is consistently better than AfriRoBERTa BASE , with gains of 6-15 points in F1 score. Conversely, the larger multilingual model, XLM-R BASE , outperformed all models despite not being exposed to Tigrinya during its pre-training. While TiELECTRA SMALL trailed in performance in all settings, confirming the impact of model size on the QA task.  Training Sample Efficiency To assess the impact of data size, we fine-tuned the TiRoBERTa BASE and XLM-R BASE models on subsets of the TiQuAD train set gradually increased by 10% of randomly selected samples and evaluated every step on the test set. We observe a promising trajectory where the models do not show signs of saturation and can potentially benefit from a larger dataset. The progress in F1 score performance is depicted in Figure 3, and a similar trend was observed for the EM score.", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": ["tab_12"]}, {"heading": "Zero-shot Cross-lingual QA", "text": "We investigate the transferability of QA models in a zero-shot setting by training on the high-resource language English and evaluate them on Tigrinya. The multilingual models, AfriBERTa BASE and XLM-R BASE , trained on the English SQuAD1.1 achieve 32-34% in F1 score on the TiQuAD test set and outperform their monolingual counterparts. While the models show promising results in transferring the task between two linguistically distant languages, those trained on the small native dataset remain vastly superior. Table 6 presents the results of the cross-lingual and multilingual experiments.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_14"]}, {"heading": "Multilingual QA", "text": "In this setup, we train the models on combined English and Tigrinya training datasets, exposing the models to both languages, then evaluate on the native TiQuAD. We observe a consistent improvement in performance across all models in contrast to the previous setups. For instance, XLM-R BASE in the multilingual setup obtains an increase of over three points in F1 score, setting the state-of-the-art on the TiQuAD test set at 68.06% EM and 76.58% F1 score. Our experiments show that the transfer of models from high to low resourced languages is a viable approach to mitigate the scarcity of annotated datasets. In our case, the benefit emerges when the native and translated Tigrinya datasets are combined with their English counterpart.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "In this work, we presented the Tigrinya Question Answering Dataset (TiQuAD ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "There are two known limitations of the SQuADlike annotation approach we used in this work: (1) It can result in higher lexical-overlap between the context and question pairs. (2) It leads to proportionally fewer truly information-seeking questions (Gururangan et al., 2018;Kaushik and Lipton, 2018). The main reason is that the annotators create questions after reading a paragraph, which can induce bias towards recycling words and phrases observed in the context. Our annotation guidelines advise against this, but it is difficult to avoid entirely. Several approaches have been proposed to mitigate this issue, such as Natural Questions (Kwiatkowski et al., 2019) and TyDiQA (Clark et al., 2020). However, they tend to be expensive, and comparatively, the SQuAD-like method is resource efficient and a more suitable starting point for low-resourced languages such as Tigrinya. Finally, the current dataset does not include adversarial examples to measure the capability of models to abstain from providing an answer when it does not exist in the context; this extension is left for future work.", "publication_ref": ["b13", "b15", "b19", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "Ethics Statement", "text": "This research adheres to the academic and professional ethics guidelines of our university. Our annotation task was approved by the Institutional Review Board (IRB) 6 . All the data collection and annotation procedures were conducted with respect and the informed consent of the participants, and best effort was made to ensure their privacy and autonomy. All participants of annotation tasks indicated their understanding of the procedure for the annotation and acknowledged their agreement to participate. The data sources are published News articles, and for our dataset, we have made an effort to ensure that (1) no personally identifying sensitive information is included, and (2) there exists a fair representation of various genres of news. Furthermore, we ensure that the dataset is available for public use. There may exist inaccuracies or inconsistencies in the questions or answers that could be misleading or ambiguous, potentially due to mistakes and subjective decisions made by the annotators. Furthermore, a bias in the dataset could lead to wrong answers or answers that are only applicable to specific groups of people. We have made the best effort to avoid such issues, but these types of limitations are difficult to detect and remove entirely and potentially present in all similar datasets. The dataset and models released in this work are for research purposes only and may not be suitable for production services without further scrutiny.", "publication_ref": ["b44"], "figure_ref": [], "table_ref": []}, {"heading": "ACL 2023 Responsible NLP Checklist", "text": "A For every submission:\nA1. Did you describe the limitations of your work?\nSection 7, after the conclusion section.\nA2. Did you discuss any potential risks of your work?\nSection 7, after the conclusion section. 3 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? 3 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? 3 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? 3 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. 4\nC Did you run computational experiments?", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "5", "text": "C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? 5.2", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We would like to thank all the annotators who participated in this work and the anonymous reviewers for their time and constructive feedback. This work was supported by Institute for Information and communications Technology Promotion (IITP) grant funded by the Korea government (MSIT) (No. 2018-0-00582, Prediction and augmentation of the credibility distribution via linguistic analysis and automated evidence document collection).", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "On the cross-lingual transferability of monolingual representations", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Mikel Artetxe; Sebastian Ruder; Dani Yogatama"}, {"ref_id": "b1", "title": "Multilingual extractive reading comprehension by runtime machine translation", "journal": "CoRR", "year": "2018", "authors": "Akari Asai; Akiko Eriguchi; Kazuma Hashimoto; Yoshimasa Tsuruoka"}, {"ref_id": "b2", "title": "Automatic spanish translation of squad dataset for multi-lingual question answering", "journal": "", "year": "2020", "authors": "Marta Ruiz Casimiro Pio Carrino; Jos\u00e9 A R Costa-Juss\u00e0;  Fonollosa"}, {"ref_id": "b3", "title": "TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020", "authors": "Jonathan H Clark; Eunsol Choi; Michael Collins; Dan Garrette; Tom Kwiatkowski; Vitaly Nikolaev; Jennimaria Palomaki"}, {"ref_id": "b4", "title": "Unsupervised cross-lingual representation learning at scale", "journal": "", "year": "2020", "authors": "Alexis Conneau; Kartikay Khandelwal; Naman Goyal; Vishrav Chaudhary; Guillaume Wenzek; Francisco Guzm\u00e1n; Edouard Grave; Myle Ott; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b5", "title": "A span-extraction dataset for Chinese machine reading comprehension", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Yiming Cui; Ting Liu; Wanxiang Che; Li Xiao; Zhipeng Chen; Wentao Ma; Shijin Wang; Guoping Hu"}, {"ref_id": "b6", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b7", "title": "FQuAD: French question answering dataset", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Wacim Martin D'hoffschmidt; Quentin Belblidia; Tom Heinrich; Maxime Brendl\u00e9;  Vidal"}, {"ref_id": "b8", "title": "SberQuAD-Russian reading comprehension dataset: Description and analysis", "journal": "Springer", "year": "2020", "authors": "Pavel Efimov; Leonid Boytsov; Pavel Braslavski"}, {"ref_id": "b9", "title": "Applying morphological segmentation to machine translation of low-resourced and morphologically complex languages: The case of tigrinya", "journal": "", "year": "2017-07", "authors": "Fitsum Gaim"}, {"ref_id": "b10", "title": "Monolingual pre-trained language models for tigrinya", "journal": "EMNLP", "year": "2021", "authors": "Fitsum Gaim; Wonsuk Yang; Jong C Park"}, {"ref_id": "b11", "title": "Geezswitch: Language identification in typologically related low-resourced east african languages", "journal": "", "year": "2022", "authors": "Fitsum Gaim; Wonsuk Yang; Jong C Park"}, {"ref_id": "b12", "title": "Hornmorpho: a system for morphological processing of amharic, oromo, and tigrinya", "journal": "", "year": "2011", "authors": "Michael Gasser"}, {"ref_id": "b13", "title": "Annotation artifacts in natural language inference data", "journal": "", "year": "2018", "authors": "Swabha Suchin Gururangan; Omer Swayamdipta; Roy Levy; Samuel Schwartz; Noah A Bowman;  Smith"}, {"ref_id": "b14", "title": "XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation", "journal": "PMLR", "year": "2020", "authors": "Junjie Hu; Sebastian Ruder; Aditya Siddhant; Graham Neubig; Orhan Firat; Melvin Johnson"}, {"ref_id": "b15", "title": "How much reading does reading comprehension require? a critical investigation of popular benchmarks", "journal": "", "year": "2018", "authors": "Divyansh Kaushik; Zachary C Lipton"}, {"ref_id": "b16", "title": "Persianquad: The native question answering dataset for the persian language", "journal": "IEEE Access", "year": "2022", "authors": "Arefeh Kazemi; Jamshid Mozafari; Mohammad Ali Nematbakhsh"}, {"ref_id": "b17", "title": "ParaShoot: A Hebrew question answering dataset", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Omri Keren; Omer Levy"}, {"ref_id": "b18", "title": "An exploration of data augmentation techniques for improving english to tigrinya translation", "journal": "ArXiv", "year": "2021", "authors": "Lidia Kidane; Sachin Kumar; Yulia Tsvetkov"}, {"ref_id": "b19", "title": "Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics", "journal": "", "year": "2019", "authors": "Tom Kwiatkowski; Jennimaria Palomaki; Olivia Redfield; Michael Collins; Ankur Parikh; Chris Alberti; Danielle Epstein; Illia Polosukhin; Jacob Devlin; Kenton Lee; Kristina Toutanova; Llion Jones; Matthew Kelcey; Ming-Wei Chang; Andrew M Dai; Jakob Uszkoreit; Quoc Le; Slav Petrov"}, {"ref_id": "b20", "title": "MLQA: Evaluating cross-lingual extractive question answering", "journal": "", "year": "2020", "authors": "Patrick Lewis; Barlas Oguz; Ruty Rinott; Sebastian Riedel; Holger Schwenk"}, {"ref_id": "b21", "title": "Korquad1.0: Korean qa dataset for machine reading comprehension. arXiv", "journal": "", "year": "2019", "authors": "Seungyoung Lim; Myungji Kim; Jooyoul Lee"}, {"ref_id": "b22", "title": "MKQA: A linguistically diverse benchmark for multilingual open domain question answering", "journal": "", "year": "2021", "authors": "Shayne Longpre; Yi Lu; Joachim Daiber"}, {"ref_id": "b23", "title": "Decoupled weight decay regularization", "journal": "", "year": "2019", "authors": "Ilya Loshchilov; Frank Hutter"}, {"ref_id": "b24", "title": "GermanQuAD and GermanDPR: Improving non-English question answering and passage retrieval", "journal": "", "year": "2021", "authors": "Timo M\u00f6ller; Julian Risch; Malte Pietsch"}, {"ref_id": "b25", "title": "Neural Arabic question answering", "journal": "", "year": "2019", "authors": "Hussein Mozannar; Elie Maamary; Karl El Hajal; Hazem Hajj"}, {"ref_id": "b26", "title": "Small data? no problem! exploring the viability of pretrained multilingual language models for lowresourced languages", "journal": "", "year": "2021", "authors": "Kelechi Ogueji; Yuxin Zhu; Jimmy Lin"}, {"ref_id": "b27", "title": "Tigrinya neural machine translation with transfer learning for humanitarian response", "journal": "", "year": "2020", "authors": "Alp \u00d6ktem; Mirko Plitt; Grace Tang"}, {"ref_id": "b28", "title": "Stemming tigrinya words for information retrieval", "journal": "", "year": "2012", "authors": "Omer Osman; Yoshiki Mikami"}, {"ref_id": "b29", "title": "Know what you don't know: Unanswerable questions for SQuAD", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Pranav Rajpurkar; Robin Jia; Percy Liang"}, {"ref_id": "b30", "title": "SQuAD: 100,000+ questions for machine comprehension of text", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Pranav Rajpurkar; Jian Zhang; Konstantin Lopyrev; Percy Liang"}, {"ref_id": "b31", "title": "Qa dataset explosion: A taxonomy of nlp resources for question answering and reading comprehension", "journal": "ACM Comput. Surv. Just Accepted", "year": "2022", "authors": "Anna Rogers; Matt Gardner; Isabelle Augenstein"}, {"ref_id": "b32", "title": "XTREME-R: Towards more challenging and nuanced multilingual evaluation", "journal": "", "year": "2021", "authors": "Sebastian Ruder; Noah Constant; Jan Botha; Aditya Siddhant; Orhan Firat; Jinlan Fu; Pengfei Liu; Junjie Hu; Dan Garrette; Graham Neubig; Melvin Johnson"}, {"ref_id": "b33", "title": "Multilingual question answering from formatted text applied to conversational agents", "journal": "ArXiv", "year": "2019", "authors": "Wissam Siblini; Charlotte Pasqual"}, {"ref_id": "b34", "title": "The effect of shallow segmentation on english-tigrinya statistical machine translation", "journal": "IEEE", "year": "2016", "authors": "Yemane Tedla; Kazuhide Yamamoto"}, {"ref_id": "b35", "title": "Morphological segmentation with lstm neural networks for tigrinya", "journal": "", "year": "2018", "authors": "Yemane Tedla; Kazuhide Yamamoto"}, {"ref_id": "b36", "title": "Tigrinya part-of-speech tagging with morphological patterns and the new nagaoka tigrinya corpus", "journal": "International Journal of Computer Applications", "year": "2016", "authors": "Yemane Tedla; Kazuhide Yamamoto; A Marasinghe"}, {"ref_id": "b37", "title": "Transferring monolingual model to lowresource language: The case of tigrinya", "journal": "", "year": "2020", "authors": "Abrhalei Tela; Abraham Woubie; Ville Hautamaki"}, {"ref_id": "b38", "title": "", "journal": "", "year": "", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; Remi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao;  Gugger"}, {"ref_id": "b39", "title": "Transformers: State-of-the-art natural language processing", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Quentin Lhoest; Alexander Rush"}, {"ref_id": "b40", "title": "Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer", "journal": "Association for Computational Linguistics", "year": "", "authors": "Linting Xue; Noah Constant; Adam Roberts; Mihir Kale; Rami Al-Rfou; Aditya Siddhant"}, {"ref_id": "b41", "title": "Named-entity recognition for a low-resource language using pre-trained language model", "journal": "Association for Computing Machinery", "year": "2022", "authors": "Yohannes Hailemariam Mehari; Toshiyuki Amagasa"}, {"ref_id": "b42", "title": "Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values", "journal": "", "year": "", "authors": ""}, {"ref_id": "b43", "title": "error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc", "journal": "", "year": "", "authors": ""}, {"ref_id": "b44", "title": "If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used", "journal": "", "year": "", "authors": " Nltk;  Spacy;  Rouge"}, {"ref_id": "b45", "title": "Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators", "journal": "", "year": "", "authors": " D1"}, {"ref_id": "b46", "title": "crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic", "journal": "", "year": "", "authors": ""}, {"ref_id": "b47", "title": "Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?", "journal": "", "year": "", "authors": " D3"}, {"ref_id": "b48", "title": "Was the data collection protocol approved (or determined exempt) by an ethics review", "journal": "", "year": "", "authors": " D4"}, {"ref_id": "b49", "title": "Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data", "journal": "", "year": "", "authors": " D5"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Example entry from TiQuAD: A paragraph as context and the corresponding annotated questionanswer pairs. Some context was redacted for brevity.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "[points] is Juventus leading the Serie A?] has three different annotations: (1) 10 \u00b6\u00b5\u00a6 [10 points]; (2) 10 \u00b6\u00b5\u00a6 flly [10 points difference]; (3) b\u00b9y 10 \u00b6\u00b5\u00a6 flly [with a 10 points difference], resulting in zero EM agreement.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: Experimental Setups: Native, Translated, Cross-lingual, and Multilingual Question-Answering settings. T: the native TiQuAD; T' SQuAD : SQuAD1.1 translated to Tigrinya; and E: the English SQuAD1.1.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: Performance of models with respect to the training data size in the monolingual setup. Evaluated on TiQuAD test set. The x-axis indicates the portion of the training dataset used with an increment of 10%.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "A3.Do the abstract and introduction summarize the paper's main claims? 1 A4. Have you used AI writing assistants when working on this paper? Left blank. B Did you use or create scientific artifacts? Section 3 B1. Did you cite the creators of artifacts you used? 3 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ": Data Statistics: Articles, Paragraphs, Questions, and Answers. The dataset is partitioned by articles."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "37% \u0192b OE\u00c2\u00d7\u2026\u00b3 \u00c2rJ 1987, \u00b9b \u0192\u00ce \u00b6y\u00b2 \u00ddn\u00b3\u00b0\u02dc\u00dd\u00dc\"? [In the last month of 1987, which team did Moje moved to?] How many/much kn\u00d4y 26.23% \u0192b\u00b2 \u00bc \u00a7\u00a6\u00d1\u00c4 b\u00bc\u2030 bms\u00b3\u00e0n z\u00b0'\u00d5'A \u00bd \u00a7n\u00d1\u00b3t kn\u00d4y \u2026\u00cen? [How many companies have been fined for participating in the environmental pollution?] What \u20acn\u00b3y 14.78% m\u2126tl\u00b0 \u2020n\u00b0Kmd \u2026d\"s \u2026n\u00b3y K\u2030f \u00b6t \u0192 \u2020\u00c7? [What are the responsibilities of Deputy Lieutenant Hamid Idris?] Who OEn 11.99% \u00d3h\u00d1y \u00a7M\" qd\u221a 1866 'A.m ms OEn ym\u00d4\u00a5 \u00ba-m?", "figure_data": "Question TypeProportion ExampleWhich When Where Why How Other\u0192\u00ce \u00b6y\u00b2 OEAs \u0192\u00a4y s \u2020mn\u00b3y b\u02c6OEy \u00b5's/'\u00d931.[What were sea weeds classified into before 1866?] 7.60% \u00c5n\u00bb \u0192\u00da\u2022t \u0192f\"\u00a1 2015 OE'As\u00b0\u00cb\u00ca\u2126? [When did the 2015 Africa Cup of Nations end?] 4.82% \u0192m \u00a7\u203a\u00d4r 'A\u02c6n \u221a\u00b8s\u00b0r KOEdn \u0192\u00a4y\u00b0\u2022\u0160\u00a9m? [When did Ambassador Ali and Minister Ahmed meet?] 1.28% g\u2022\u00e3yt s \u2020mn\u00b3y \u2026\u00cf\u00b0 \uf6be\u2248]n OE\u00bd\u2030fin\u00b0 \u00a7y \u00c8 \u2020\u00c7? [Why does graphite have a malleable and slippery character?] 1.18% \u2026\u00b5m \u00b9y \u00da\u00a4n \u00db\u00a5 \u2020\u00b3t \u00bcOEy \u00de-m  \u00b6\u00d7\u00d6 k\u02dcr\u0178 k\u201a\u2039m? [How did the crime groups manage to steal the oil?] 0.75%\u02dc \u00a7t z\u00d0l\u2026\u00c7m gn \u00dan\u00c8b \u00d4l\u00d3m \u00bfb z\u00dabr\u00c7m  \u00b6\u00da\u2022t \u0192b \u00b6t \u00b5's? [Give examples of things that people hate but do for money?];"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Question-type distributions in the development set of TiQuAD, grouped by question words.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": ", the top three types are which [\u0192\u00ce \u00b6y\u00b2/\u0192\u00ce\u00b9y], how many/much [kn\u00d4y], and what [\u2026n\u00b3y], accounting \u12a3\u1265 \u122d\u1233\u1235 \u12d8\u120e \u1295\u1218\u133d\u1213\u134a \u12dd\u1320\u1245\u121d \u121b\u12d5\u12f5\u1295 \u12a5\u1295\u1273\u12ed \u12ed\u1260\u1203\u120d? [ What is the mineral in pencils useful for writing? ] Context: \u2026\u12a3\u1265 \u12cd\u123d\u1322 \u122d\u1233\u1235 \u12dd\u122d\u12a8\u1265 \u1295\u133d\u1215\u1348\u1275 \u12dd\u1215\u130d\u12dd \u12a5\u121d\u1292 \u12c8\u12ed \u121b\u12d5\u12f5\u1295 \u130d\u122b\u134b\u12ed\u1275 \u1270\u1263\u1202\u1209 \u12ed\u133d\u12cb\u12d5\u1362\u2026 [ The stone or mineral inside a pencil that is used for writing is called graphite. ] 35.1% World knowledge Question: \u130d\u122b\u134b\u12ed\u1275 \u12a3\u1260\u12e8\u1296\u1275 \u1203\u1308\u122b\u1275 \u1265\u12dd\u12eb\u12f3 \u12ed\u122d\u12a8\u1265? [ In which countries is Graphite found? ] Context: \u2026\u130d\u122b\u134b\u12ed\u1275 \u12a3\u1265 \u1218\u120b\u12a5 \u12d3\u1208\u121d \u12f3\u122d\u130b \u1265\u121d\u12d5\u1229\u12ed \u12dd\u122d\u130b\u1210'\u12e9 \u12dd\u122d\u12a8\u1265\u1362 \u1265\u12dd\u12eb\u12f3 \u130d\u1295 \u12a3\u1265 \u127b\u12ed\u1293\u1361 \u1205\u1295\u12f2\u1361 \u1230\u121c\u1295 \u12ae\u122d\u12eb\u1361 \u121c\u12ad\u1232\u12ae\u1361 \u1265\u122b\u12da\u120d\u1361 \u127c\u12ad \u122a\u1353\u1265\u120a\u12ad\u1295 \u1271\u122d\u12ad\u1295 \u12ed\u12dd\u12cd\u1270\u122d\u1362\u2026 [ Graphite is almost evenly distributed worldwide. But it is most common in China, India, North Korea, Mexico, Brazil, Czech Republic and Turkey. ] \u12a3\u1265 \u122d\u1233\u1235 \u12d8\u120e \u1295\u1218\u133d\u1213\u134a \u12dd\u1320\u1245\u121d \u121b\u12d5\u12f5\u1295 \u12a5\u1295\u1273\u12ed \u12ed\u1260\u1203\u120d? [ What is the mineral in pencils useful for writing? ] Context: \u2026\u12a3\u1265 \u12cd\u123d\u1322 \u122d\u1233\u1235 \u12dd\u122d\u12a8\u1265 \u1295\u133d\u1215\u1348\u1275 \u12dd\u1215\u130d\u12dd \u12a5\u121d\u1292 \u12c8\u12ed \u121b\u12d5\u12f5\u1295 \u130d\u122b\u134b\u12ed\u1275 \u1270\u1263\u1202\u1209 \u12ed\u133d\u12cb\u12d5\u1362\u2026 [ The stone or mineral inside a pencil that is used for writing is called graphite. ]", "figure_data": "Reasoning Type Example"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "[In the men's soccer match between Halhale College and Adikeih College of Arts and Social Sciences, how many goals did Adikeih College of Arts and Social Sciences score in the match?]. The shortest questions have three words, for example, \u00e5-\u0192\u00a4y tr\u00bcb? [Where is Foro located?", "figure_data": "lengths of paragraphs in TiQuAD rangebetween 39-278 words or 198-1264 characters.Around 60% of the questions have 5-10 words, but we observe some verbose examples such as \u0192b g\u00b5\u2248t ff'e\u2206 \u2026g\" \u00d4 \u00b0 \u00a7'et\u00d3, \u0192b\u00b2 \u0192bOEn\u00df \u00c1 \u2020\u00a5 KlK \u2020n \u00c1 \u2020\u00a5 \u00b5\u00a4 \u00a7tn \u2248M\u00a4\u2022\u00c4g\u00b5m \u00c1 \u2020\u00a5 \u00b5\u00a4 \u00a7tn \u2248M\u00a4\u2022\u00c4 s \u00b6fl\u00b0tn 'A\u00d6'yM kn\u00d4y s \u00b6fl\u00b0tn 'A\u00d6'yMn z \u00b6\u00a4 X\u00b5\u00b3t \u0192OEz\u00dc \u00a7?"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Models: The monolingual and multilingual pretrained models used in our experiments. #L, #AH, and Param denote the number of layers, self-attention heads, and parameters, respectively. #Langs is the number of languages in the pre-training data. Except for XLM-R, the other models have seen Tigrinya during pre-training.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "", "figure_data": ". The experi-ments can be grouped into three setups, based onthe language of the training data: (1) Monolingualsetting: We train and evaluate models using the na-tive and machine translated datasets, separately andin combination; (2) Zero-shot cross-lingual setting:We investigate transfer learning by training mod-els on an English dataset and evaluating them onTigrinya -treating QA as a language-independenttask; and (3) Multilingual setting: We investigatemodels trained on combined Tigrinya and EnglishQA datasets and evaluated in a native setup. Fig-ure 2 illustrates the experimental settings."}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Performance of models in the Monolingual setups, evaluated on the development and test sets of TiQuAD. The models were trained on TiQuAD (Native) and machine translated SQuAD (Translated).", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "", "figure_data": ": Performance of the Zero-shot Cross-lingual transfer and Multilingual setups on the development and test sets of TiQuAD. The models were trained on the English SQuAD, TiQuAD (Native), machine translated SQuAD (Translated), and their combination."}], "formulas": [{"formula_id": "formula_0", "formula_text": "P start (i) = exp (S \u2022 T i ) j=1 exp (S \u2022 T j ) ,(1)", "formula_coordinates": [6.0, 112.96, 713.24, 176.18, 33.79]}, {"formula_id": "formula_1", "formula_text": "P end (i) = exp (E \u2022 T i ) j=1 exp (E \u2022 T j ) ,(2)", "formula_coordinates": [6.0, 113.38, 749.92, 175.75, 33.79]}, {"formula_id": "formula_2", "formula_text": ")3", "formula_coordinates": [6.0, 515.94, 337.16, 8.48, 13.15]}, {"formula_id": "formula_3", "formula_text": "L = \u2212 log P start (i * ) \u2212 log P end (j * ) (4)", "formula_coordinates": [6.0, 337.23, 429.66, 187.19, 21.19]}], "doi": "10.18653/v1/2020.acl-main.421"}