{"title": "A Unified Maximum Likelihood Approach for Estimating Symmetric Properties of Discrete Distributions", "authors": "Jayadev Acharya; Hirakendu Das; Alon Orlitsky; Ananda Theertha Suresh", "pub_date": "", "abstract": "Symmetric distribution properties such as support size, support coverage, entropy, and proximity to uniformity, arise in many applications. Recently, researchers applied different estimators and analysis tools to derive asymptotically sample-optimal approximations for each of these properties. We show that a single, simple, plug-in estimator-profile maximum likelihood (PML)is sample competitive for all symmetric properties, and in particular is asymptotically sampleoptimal for all the above properties.", "sections": [{"heading": "Introduction", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Symmetric distribution properties", "text": "Let \u2206 def = {(p 1 , . . . ,p k ) : p i \u2265 0, k i=1 p i = 1, 1 \u2264 k \u2264 \u221e} denote the collection of all discrete distributions over finite or infinite support. A distribution property is a mapping f : \u2206 \u2192 R. It is symmetric if it remains unchanged under relabeling of domain symbols, namely if it is determined by just the probability multiset {p 1 , p 2 , . . . ,p k }. Many important properties are symmetric. For example: Support size S(p) = |{x : p(x) > 0}|, plays an important role in population and vocabulary estimation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Support coverage S m (p) =", "text": "x (1\u2212(1\u2212p(x)) m ), the expected number of elements observed in m samples, arises in ecological and biological studies, e.g., (Colwell et al., 2012).", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}, {"heading": "Shannon entropy H(p) =", "text": "x p(x) log 1 p(x) , central to information theory (Cover & Thomas, 2006), has numerous Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).", "publication_ref": ["b15"], "figure_ref": [], "table_ref": []}, {"heading": "applications.", "text": "Distance to uniform p\u2212u 1 = x |p(x)\u22121/|X ||, where u is the uniform distribution over the domain X of p. This distance measure appears in the error of hypothesis testing, and the uniform distribution is arguably one of the commonest discrete distributions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Distribution estimation", "text": "Considerable research, over many years, has focused on estimating distribution properties. In the common setting, an unknown underlying distribution p \u2208 \u2206 generates n independent samples X n def = X 1 , , . . . ,X n , and the objective is to estimate a given property f (p) as accurately as possible.\nSpecifically, an estimator for a distribution p over X is a functionf : X n \u2192 R mapping observed samples to a property estimate. The sample complexity off is the smallest number of samples it requires to estimate a property f with accuracy \u03b5 and confidence probability \u03b4, for all distributions in a collection P \u2286 \u2206, Cf (f, P, \u03b4, \u03b5)\ndef = min n : p(|f (p) \u2212f (X n )| \u2265 \u03b5) \u2264 \u03b4 \u2200p \u2208 P .\nThe sample complexity of estimating f is the lowest sample complexity of any estimator, C * (f, P, \u03b4, \u03b5) = min f Cf (f, P, \u03b4, \u03b5).\nBy taking the median of about log 1 \u03b4 independent estimators, the error rate can be driven down from a constant to \u03b4. Therefore, the sample complexity depends on \u03b4 only through a factor of at most log 1 \u03b4 . For simplicity, we therefore abbreviate Cf (f, P, 1/3, \u03b5) by Cf (f, P, \u03b5).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Result summary", "text": "Recent research has shown that while simple estimators for the aforementioned properties require sample size n proportional to the support size k, more sophisticated techniques need only a sub-linear sample size n = \u0398(k/ log k).\nHowever, each of the problems was approximated via different estimators and analysis techniques, that for some properties were rather complex.\nMotivated by the principle of maximum likelihood, we show that a single, simple, plug-in estimator-profile maximum likelihood (PML) (Orlitsky et al., 2004b)-is competitive for estimating any symmetric property. Its sample complexity is at most quadratically worse than that of any estimator.\nSpecifically, we show that if a symmetric property can be estimated using n samples with confidence \u03b4, then the PML plug-in estimator can estimate it using as many samples with confidence \u03b4\u2022e \u221a n . While this increase may seem high, note that it is sub-exponential. We show that if a property has an estimator that has a small bounded difference constant (how much the estimator changes when we change one sample), then the error probability reduces exponentially with n (Please see Section 7.1). Combined, these two facts imply that for properties with locally-smooth estimators, the PML plug-in estimator is optimal up to a constant: C PML = \u0398(C * ). We then show that all the above properties have locally-smooth estimators, hence they can be estimated by the PML plug-in estimator with up to a constant factor more than the optimal number of samples.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Outline", "text": "The rest of the paper is organized as follows. In Section 2 we describe existing results and those shown in this paper. In Section 3 we formally define the quantities involved and state the results. In Section 4 we define profiles and PML. In Section 5, we outline the new approach. In Section 6, we demonstrate auxiliary results for maximum likelihood estimators. In Section 7, we outline how we apply maximum likelihood to support, support coverage, entropy, and uniformity. In Section 8, we provide the details for support, and support coverage and in the appendix we outline results for distance to uniformity and entropy.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Previous and New Results", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Previous Results", "text": "Plug-in estimation is a general approach for estimating distribution properties. It uses the samples X n to find an approximationp of p, and lets f (p) estimate f (p).\nOne of the most common distribution estimators, dating back to Fisher is maximum likelihood, that for clarity we call sequence maximum likelihood (SML) (Aldrich, 1997). To any sample x n it assigns the distribution p that maximizes p(x n ). The SML estimate is exceedingly simple to derive. The multiplicity N x def = N x (x n ) of symbol x is the number of times it appears in the sequence x n . The empiri-cal frequency estimator assigns to each symbol x, the fractionp(x) def = N x /n of times it appears in the sample x n . For example, if x 7 =bananas, empirical frequency would assignp(a) = 3/7,p(n) = 2/7, andp(b) =p(s) = 1/7. It can be readily shown that SML is exactly the empirical frequency estimator.\nWhile the SML plug-in estimator performs well in the limit of many samples, sophisticated techniques have recently yielded more accurate estimators for several important symmetric properties.\nSupport size. With finitely many samples, S(p) cannot be estimated to any accuracy as many symbols with arbitrarily small probability may not be observed. Motivated by databases, where each entry appears at least once, (Raskhodnikova et al., 2009) considered distributions whose non-zero probabilities are at least 1 k ,\n\u2206 \u2265 1 k def = {p \u2208 \u2206 : p(x) \u2208 {0} \u222a [1/k, 1]} ,\nand estimated the normalized supportS(p)\ndef = S(p)/k. It can be shown that C SML (S(p), \u2206 \u2265 1 k , \u03b5) = \u0398(k log 1 \u03b5\n). Yet (Valiant & Valiant, 2011a;Wu & Yang, 2015) \nshowed that C * (S(p), \u2206 \u2265 1 k , \u03b5) = \u0398 k log k \u2022 log 2 1 \u03b5 .\nSupport coverage. Here too we consider the normalized coverageS m (p) (Good & Toulmin, 1956) proposed the Good Toulmin (GT) estimator that achieves C GT (S m (p), \u2206, \u03b5) = m/2. Recently, (Orlitsky et al., 2016)  Shannon entropy. Since elements with arbitrarily small probability can contribute to an arbitrarily high entropy, H(p) cannot be estimated over aribtrary support with finitely many samples. Therefore researchers are mostly interested in estimating entropy of distributions with support size at most k. ski, 2003). Moreover, (Paninski, 2003) showed that C * (H(p), \u2206 k , \u03b5) is sublinear in k, (Valiant & Valiant, 2011a) showed that the optimal dependence on k is k/ log k and (Wu & Yang, 2016;Jiao et al., 2015) obtained the optimal dependence on both k, and \u03b5, and showed that\ndef = S m (p)/m.\n\u2206 k def = {p \u2208 \u2206 : S(p) \u2264 k}. It can be shown that C SML (H(p), \u2206 k , \u03b5) = \u0398( k \u03b5 ) (Panin\nC * (H(p), \u2206 k , \u03b5) = \u0398 k log k \u2022 1 \u03b5 .\nDistance to uniform. (Valiant & Valiant, 2011b) (Jiao et al., 2016) showed that this bound is tight.\nshowed that C * ( p \u2212 u 1 , \u2206 k , \u03b5) = O k log k \u2022 1 \u03b5 2 , and\nThese results are summarized in Table 1.\nOther properties were considered as well. (Bar-Yossef et al., 2001;Acharya et al., 2015;Caferov et al., 2015;Obremski & Skorski, 2017) estimated R\u00e9nyi entropy and (Bu et al., 2016) estimated KL divergence. (Canonne, 2015) surveyed testing whether distributions have certain properties, and (Jiao et al., 2014) studied the performance of SML estimators for several properties. Closest to this work in terms of approach and techniques are (Acharya et al., 2011;2013a;b;Valiant & Valiant, 2013;Orlitsky & Suresh, 2015) that design algorithms whose sample complexity is provably close to the best possible regardless of the domain size.", "publication_ref": ["b5", "b27", "b29", "b36", "b17", "b26", "b29", "b37", "b20", "b30", "b21", "b7", "b4", "b11", "b10", "b13", "b19", "b1", "b3", "b31", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Profile Maximum Likelihood", "text": "Symmetric distribution properties do not depend on the symbol labels. They are determined by a simple sufficient statistic: the number of elements appearing any given number of times. The profile of a sequence X n , denoted \u03d5(X n ) is the multiset of the multiplicities of all the symbols appearing in X n . For example, \u03d5(a b r a c a d a b r a) = {1, 1, 2, 2, 5}, as two symbols appearing once, two appearing twice, and one symbol appearing five times, removing the association of the individual symbols with the multiplicities. Profiles are also referred to as histograms of histograms (Batu et al., 2000), histogram order statistics (Paninski, 2003), and fingerprints (Valiant & Valiant, 2011a).\nMotivated by the principle of maximum likelihood, (Orlitsky et al., 2004b;2017b) discarded the symbol labels, and considered the profile maximum likelihood (PML) distribution that maximizes the probability of the observed profile.\nA number of PML properties were established. (Orlitsky et al., 2004b;2005) proved PML's existence, consistency, and some of its properties. (Orlitsky et al., 2004d;2005;Orlitsky & Pan, 2009;Pan et al., 2009) described additional properties and derived the PML distributions of several short and simple profiles. (Orlitsky et al., 2017b;c) provide a unified review of several of these results. (Anevski et al., 2013) contains a combination of previously-known and new results. A related distribution-estimation approach is described in (Orlitsky et al., 2004c;2003). Several approaches were taken to computing the PML distribution.\nAlgebraic computation was considered in (Acharya et al., 2010). A combination of the EM and MCMC algorithms have shown excellent results for calculating the PML distribution and applying it to support-size estimation (Orlitsky et al., 2004a;2006;Pan, 2012) and a summary of some of the results appears in (Orlitsky et al., 2017a). (Vontobel, 2012;2014) derived the Bethe approximation of these algorithms.\nFollowing the first draft of this work, (Vatedka & Vontobel, 2016) showed that both theoretically and empirically plug-in estimators obtained from the PML estimate yield good estimates for symmetric functionals of Markov distributions.", "publication_ref": ["b8", "b26", "b29", "b25", "b25", "b6", "b26", "b0", "b15", "b24", "b34", "b35", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "New Results", "text": "We show that replacing the SML plug-in estimator by PML yields a unified estimator that, like the best results shown via specialized techniques developed, is optimal.\nTheorem 1. There is a unified approach based on PML distribution that achieves the optimal sample complexity for the problems of estimating the entropy, support, support coverage, and distance to uniformity.\nWe prove in Corollary 1 that the PML approach is competitive with respect to any symmetric property. For symmetric properties, these results are perhaps a justification of Fisher's thoughts on Maximum Likelihood: \"Of course nobody has been able to prove that maximum likelihood estimates are best under all circumstances. Maximum likelihood estimates computed with all the information available may turn out to be inconsistent. Throwing away a substantial part of the information may render them consistent.\" R. A. Fisher's thoughts on Maximum Likelihood (Le Cam, 1979).\nTo prove these PML guarantees, we establish two results that are of interest on their own right.\n\u2022 With n samples, PML estimates any symmetric property of p with essentially the same accuracy, and at most e 3 \u221a n times the error, of any other estimator. This follows by combining Theorem 3 with Lemma 1.\n\u2022 For a large class of symmetric properties, including all those mentioned above, if there is an estimator that uses n samples, and has an error probability 1/3, we design an estimator using O(n) samples, whose error probability is nearly exponential in n. We remark that this decay is much faster than applying the median trick. This result follows by combining McDiarmid's inequality with Lemma 2.\nCombined, these results prove that PML plug-in estimators are sample-optimal.\nWe also introduce the notion of \u03b2-approximate ML distributions, described in Definition 1. These distributions are more relaxed version of PML, hence may be more easily computed, yet they provide essentially the same performance guarantees.\nProperty name\nf (p) P C SML C * PML Entropy H(p) \u2206 k k \u03b5 k log k 1 \u03b5\nTheorem 5 and Section 8.1 Support sizeS(p) \n\u2206 \u2265 1 k k log 1 \u03b5 k log k log\np \u2212 u 1 \u2206 X k \u03b5 2 k log k 1 \u03b5 2\nTheorem 5 and Section A Table 1. Estimation complexity for various properties up to a constant factor. For all properties shown, PML achieves the best known results up to a constant factor. The details of where the optimal sample complexity was derived for each problem is discussed in Section 2.1.", "publication_ref": ["b22"], "figure_ref": [], "table_ref": []}, {"heading": "Formal Definitions and Results", "text": "In the past, different sophisticated estimators were used for every property in Table 1. We show that the simple plug-in estimator that uses any PML approximationp, has optimal performance guarantees for all these properties.\nIn the next theorem, assume n is at least the optimal sample complexity of estimating entropy, support, support coverage, and distance to uniformity (given in Table 1\n) respec- tively. Theorem 2. For all \u03b5 > c/n 0.2 , any plug-in exp (\u2212 \u221a n)- approximate PMLp satisfies, Entropy Cp(H(p), \u2206 k , \u03b5) C * (H(p), \u2206 k , \u03b5), Support size Cp(S(p)/k, \u2206 \u2265 1 k , \u03b5) C * (S(p)/k, \u2206 \u2265 1 k , \u03b5), Support coverage Cp(S m (p)/m, \u2206, \u03b5) C * (S m (p)/m, \u2206, \u03b5),", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Distance to uniformity", "text": "Cp( p \u2212 u 1 , \u2206 X , \u03b5) C * ( p \u2212 u 1 , \u2206 k , \u03b5).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "PML: Profile Maximum Likelihood", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "For a sequence X n , recall that the multilplicity N x is the number of times x appears in X n . Discarding, the labels, profile of a sequence (Orlitsky et al., 2004b) is defined below. Let \u03a6 n be all profiles of length-n sequences. Then,\n\u03a6 4 = {{1, 1, 1, 1}, {1, 1, 2}, {1, 3}, {2, 2}, {4}}.\nIn particular, a profile of a length-n sequence is an unordered partition of n. Therefore, |\u03a6 n |, the number of profiles of length-n sequences is equal to the partition number of n. Then, by the Hardy-Ramanujan bounds on the partition number, Lemma 1 ( (Hardy & Ramanujan, 1918)).\n|\u03a6 n | \u2264 exp(3 \u221a n).\nFor a distribution p, the probability of a profile \u03d5 is defined as p(\u03d5)\ndef = X n :\u03d5(X n )=\u03d5 p(X n ),\nthe probability of observing a sequence with profile \u03d5. Under i.i.d. sampling, p(\u03d5) =\nX n :\u03d5(X n )=\u03d5 n i=1 p(X i ). For example, the probability of observing a sequence with profile \u03d5 = {1, 2} is the probability of observing a sequence with one symbol appearing once, and one symbol appearing twice. A sequence with a symbol x appearing twice and y appearing once (e.g., x y x) has probability p(x) 2 p(y). Appropriately normalized, for any p, the probability of the profile {1, 2} is\np({1, 2}) = \u03d5(X n )={1,2} n i=1 p(X i ) = 3 1 a =b\u2208X p(a) 2 p(b),(1)\nwhere the normalization factor is independent of p. The summation is a monomial symmetric polynomial in the probability values. See (Pan, 2012) for more examples.", "publication_ref": ["b18", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "PML Estimation Scheme", "text": "Recall that p X n is the distribution maximizing the probability of X n . Similarly, define (Orlitsky et al., 2004b):\np \u03d5 def = max p\u2208P p(\u03d5)\nas the distribution in P that maximizes the probability of observing a sequence with profile \u03d5.\nFor example, for \u03d5 = {1, 2}. For P = \u2206 k , from (1),\np \u03d5 = arg max p\u2208\u2206 k a =b p(a) 2 p(b).\nNote that in contrast, SML only maximizes one term of this expression.\nWe give two examples from the table in (Orlitsky et al., 2004b) to distinguish between SML and PML distributions, and also show an instance where PML outputs distributions with a larger support than those appearing in the sample. Example 1. Let X = {a, b, . . . , z}. Suppose X n = x y x, then the SML distribution is (2/3, 1/3). However, the distribution in \u2206 that maximizes the probability of the profile \u03d5(x y x) = {1, 2} is (1/2, 1/2). Another example, illustrating the power of PML to predict new symbols is X n = a b a c, with profile \u03d5(a b a c) = {1, 1, 2}. The SML distribution is (1/2, 1/4, 1/4), but the PML is a uniform distribution over 5 elements, namely (1/5, 1/5, 1/5, 1/5, 1/5).\nSuppose we want to estimate a symmetric property f (p) of an unknown distribution p \u2208 P given n independent samples. Our high level approach using PML is described below.\nInput: Class of distributions P, symmetric function f (\u2022), sample X n 1. Compute p \u03d5 : arg max p\u2208P p(\u03d5(X n )).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Output f (p \u03d5 ).", "text": "There are a few advantages of this approach (as is true with any plug-in approach): (i) the computation of PML is agnostic to the function f at hand, (ii) there are no parameters to be tuned, (iii) techniques such as Poisson sampling or median tricks are not necessary, (iv) well motivated by the maximum-likelihood principle.\nComparison to the linear-programming plug-in estimator (Valiant & Valiant, 2011a). Our approach is perhaps closest in flavor to the plug-in estimator of (Valiant & Valiant, 2011a). Indeed, as mentioned in (Valiant, 2012), their linear-programming estimator is motivated by the question of estimating the PML. Their result was the first estimator to provide sample complexity bounds in terms of the alphabet size, and accuracy the problems of entropy and support estimation. Before we explain the differences of the two approaches, we briefly explain their approach.\nDefine, \u03d5 \u00b5 (X n ) to be the number of elements that appear \u00b5 times. For example, when X n = a b r a c a d a b r a, \u03d5 1 = 2, \u03d5 2 = 2, and \u03d5 5 = 1. (Valiant & Valiant, 2011a) design a linear program that uses SML for high values of \u00b5, and formulate a linear program to find a distribution for which E[\u03d5 \u00b5 ]'s are close to the observed \u03d5 \u00b5 's. They then plug-in this estimate to estimate the property. On the other hand, our approach, by the nature of ML principle, tries to find the distribution that best explains the entire profile of the observed data, not just some partial characteristics. It therefore has the potential to estimate any symmetric property and estimate the distribution closely in any distance measures, competitive with the best possible. For example, the guarantees of the linear program approach are suboptimal in terms of the desired accuracy \u03b5. For entropy estimation the optimal dependence is 1 \u03b5 , whereas (Valiant & Valiant, 2011a) yields 1 \u03b5 2 . This is more prominent for support size and support coverage, which have optimal dependence of polylog( 1\u03b5 ), whereas (Valiant & Valiant, 2011a) gives a 1 \u03b5 2 dependence. Besides, we analyze the first method proposed for estimating symmetric properties, designed from the first principles, and show that in fact it is competitive with the optimal estimators for various problems.", "publication_ref": ["b29", "b29", "b32", "b29", "b29", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Proof Outline", "text": "Our arguments have two components. In Section 6 we prove a general result for the performance of plug-in estimation via maximum likelihood approaches.\nLet P be a class of distributions over Z, and f : P \u2192 R be a function. For z \u2208 Z, let\np z def = arg max p\u2208P p(z)\nbe the maximum-likelihood estimator of z in P. Upon observing z, f (p z ) is the ML estimator of f . In Theorem 4, we show that if there is an estimator that achieves error probability \u03b4, then the ML estimator has an error probability at most \u03b4|Z|. We note that variations of this result in the asymptotic statistics were studied before (see (Lehmann & Casella, 1998)). Our contribution is to use these results in the context of symmetric properties and show sample complexity bounds in the non-asymptotic regime.\nWe emphasize that, throughout this paper Z will be the set of profiles of length n, and P will be distributions induced over profiles by length-n i.i.d. samples. Therefore, we have |Z| = |\u03a6 n |. By Lemma 1, if there is a profile based estimator with error probability \u03b4, then the PML approach will have error probability at most \u03b4 exp(3 \u221a n). Such arguments were used in hypothesis testing to show the existence of competitive testing algorithms for fundamental statistical problems (Acharya et al., 2011;.\nAt its face value this seems like a weak result. Our second key step is to prove that for the properties we are interested, it is possible to obtain very sharp guarantees. For example, we show that if we can estimate the entropy to an accuracy \u00b1\u03b5 with error probability 1/3 using n samples, then we can estimate the entropy to accuracy \u00b12\u03b5 with error probability exp(\u2212n 0.9 ) using only 2n samples. Using this sharp concentration, the new error probability term dominates |\u03a6 n |, and we obtain our results. The arguments for sharp concentration are based on modifications to existing estimators and a new analysis. Most of these results are technical and are in the appendix.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "Maximum Likelihood Property Estimation", "text": "We establish performance guarantees of ML property estimation in a general set-up. Recall that P is a collection of distributions over Z, and f : P \u2192 R. Given a sample Z from an unknown p \u2208 P, we want to estimate f (p). The maximum likelihood approach is the following twostep procedure.\n1. Find p Z = arg max p\u2208P p(Z).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Output f (p Z ).", "text": "We bound the performance of this approach in the following theorem. Theorem 3. Suppose there is an estimatorf : Z \u2192 R, such that for any p, and\nZ \u223c p, Pr f (p) \u2212f (Z) > \u03b5 < \u03b4,(2)\nthen\nPr (|f (p) \u2212 f (p Z )| > 2\u03b5) \u2264 \u03b4 \u2022 |Z| .(3)\nProof. Consider symbols with p(z) \u2265 \u03b4 and p(z) < \u03b4 separately. A distribution p with p(z) \u2265 \u03b4 outputs z with probability at least \u03b4. For (2) to hold, we must have, f (p) \u2212f (z) < \u03b5. By the definition of ML, p z (z) \u2265 p(z) \u2265 \u03b4, and for (2) to hold for p z , f (p z ) \u2212f (z) < \u03b5. By the triangle inequality, for all such z,\n|f (p) \u2212 f (p z )| \u2264 f (p) \u2212f (z) + f (p z ) \u2212f (z) \u2264 2\u03b5.\nThus if p(z) \u2265 \u03b4, then PML satisfies the required guarantee with zero probability of error, and any error occurs only when p(z) < \u03b4. We bound this probability as follows. When Z \u223c p,\nPr (p(Z) < \u03b4) \u2264 z\u2208Z:p(z)<\u03b4 p(z) < \u03b4 \u2022 |Z| .\nFor some problems, it might be easier to just approximate the ML, instead of finding it exactly. We define an approximation ML as follows:\nDefinition 1 (\u03b2-approximate ML). Let \u03b2 \u2264 1. For Z \u2208 Z, p Z \u2208 P is a \u03b2-approximate ML distribution ifp z (z) \u2265 \u03b2 \u2022 p z (z). When Z is profiles of length-n, a \u03b2-approximate PML is a distributionp \u03d5 such thatp \u03d5 (\u03d5) \u2265 \u03b2 \u2022 p \u03d5 (\u03d5).\nThe next result proves guarantees for any \u03b2-approximate ML estimator. Theorem 4. Suppose there exists an estimator satisfying (2). For any p \u2208 P and Z \u223c p, any \u03b2-approximate MLp Z satisfies:\nPr (|f (p) \u2212 f (p Z )| > 2\u03b5) \u2264 \u03b4 \u2022 |Z|/\u03b2.\nThe proof is very similar to the previous theorem and is presented in the Appendix B.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Competitiveness of ML via Median Trick", "text": "Suppose for a property f (p), there is an estimator with sample complexity n that achieves an accuracy \u00b1\u03b5 with probability of error at most 1/3. The standard method to boost the error probability is the median trick: (i) Obtain O(log(1/\u03b4)) independent estimates using O(n log(1/\u03b4)) independent samples. (ii) Output the median of these estimates. This is an \u03b5-accurate estimator of f (p) with error probability at most \u03b4. By definition, estimators are a mapping from the samples to R. However, in many applications the estimators map from a much smaller (some sufficient statistic) of the samples. Denote by Z n the space consisting of all sufficient statistics that the estimator uses. For example, estimators for symmetric properties, such as entropy typically use the profile of the sequence, and hence Z n = \u03a6 n . Using the median-trick, we get the following result.\nCorollary 1. Letf : Z n \u2192 R be an estimator of f (p) with accuracy \u03b5 and error-probability 1/3. The ML estimator achieves accuracy 2\u03b5 with probability at least 2/3 using\nmin n : n 20 log(3Z n ) > n samples.\nProof. Since n is the number of samples to get error probability 1/3, by the Chernoff bound, the error after n samples is at most exp(\u2212(n /(20n))). Therefore, the error probability of the ML estimator for accuracy 2\u03b5 is at most exp(\u2212(n /(20n)))Z n , which we desire to be at most 1/3.\nFor estimators that use the profile of sequences,\n|\u03a6 n | < exp(3 \u221a n).\nPlugging this in the previous result shows that the PML based approach has a sample complexity of at most O(n 2 ). This result holds for all symmetric properties, independent of \u03b5, and the alphabet size k. For the problems mentioned earlier, something much better in possible, namely the PML approach is optimal up to constant factors.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sample optimality of PML", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sharp Concentration for Some Properties", "text": "To obtain sample-optimality guarantees for PML, we need to drive the error probability down much faster than the median trick. We achieve this by using McDiarmid's inequality stated below. Letf : X * \u2192 R. Supposef gets n independent samples X n from an unknown distribution. Moreover, changing one of the X j to any X j changedf by at most c * . Then McDiarmid's inequality (bounded difference inequality, (Boucheron et al., 2013)) states that,\nPr f (X n ) \u2212 E[f (X n )] > t \u2264 2 exp \u2212 2t 2 nc 2 * . (4)\nThis inequality can be used to show strong error probability bounds for many problems. We mention a simple application for estimating discrete distributions.\nExample 2. It is well known (Devroye & Lugosi, 2001) that SML requires \u0398(k/\u03b5 2 ) samples to estimate p in 1 distance with probability at least 2/3. In this case,f (X n ) =\nx Nx n \u2212 p(x) , and therefore c * is at most 2/n. Using McDiarmid's inequality, it follows that SML has an error probability of \u03b4 = 2 exp(\u2212k/2), while still using \u0398(k/\u03b5 2 ) samples.\nLet B n be the bias of an estimatorf (X n ) of f (p), namely B n def = f (p) \u2212 E[f (X n )] . By the triangle inequality, f (p) \u2212f (X n ) \u2264 f (p) \u2212 E[f (X n )] + f (X n ) \u2212 E[f (X n )] = B n + f (X n ) \u2212 E[f (X n )] . Plugging this in (4), Pr f (p) \u2212f (X n )] > t + B n \u2264 2 exp \u2212 2t 2 nc 2 * . (5)\nWith this in hand, we need to show that c * can be bounded for estimators for the properties we consider. In particular, we will show that Lemma 2. Let \u03b1 > 0 be a fixed constant. For entropy, support, support coverage, and distance to uniformity there exist profile based estimators that use the optimal number of samples (given in Table 1), have bias \u03b5 and if we change any of the samples, changes by at most c \u2022 n \u03b1 n , where c is a positive constant.\nWe prove this lemma by proposing several modifications to the existing sample-optimal estimators. The modified estimators will preserve the sample complexity up to constant factors and also have a small c * . The proof details are given in the appendix.\nUsing (5) with Lemma 2, Theorem 5. Let n be the optimal sample complexity of estimating entropy, support, support coverage and distance to uniformity (given in table 1) and c be a large positive constant. Let \u03b5 \u2265 c/n 0.2 , then any for any \u03b2 > exp (\u2212 \u221a n), the \u03b2-PML estimator estimates entropy, support, support coverage, and distance to uniformity to an accuracy of 4\u03b5 with probability at least 1 \u2212 exp(\u2212 \u221a n).\nProof. Let \u03b1 = 0.05. By Lemma 2, for each property of interest, there are estimators based on the profiles of the samples such that using near-optimal number of samples, they have bias \u03b5 and maximum change if we change any of the samples is at most c n \u03b1 /n for some constant c . Hence, by McDiarmid's inequality, an accuracy of 2\u03b5 is achieved with probability at least 1\u22122 exp \u22122\u03b5 2 n 1\u22122\u03b1 /c 2 . Now supposep is any \u03b2-approximate PML distribution. Then by Theorem 4\nPr (|f (p)\u2212f (p)| > 4\u03b5) < \u03b4 \u2022 |\u03a6 n | \u03b2 \u2264 2 exp(\u22122\u03b5 2 n 1\u22122\u03b1 /c 2 + 3 \u221a n) \u03b2 \u2264 exp(\u2212 \u221a n),\nwhere in the last step we used \u03b5 2 n 1\u22122\u03b1 c \u221a n, and \u03b2 > exp(\u2212 \u221a n).", "publication_ref": ["b9", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Support and Support Coverage", "text": "We analyze both support coverage and the support estimation via a single approach. We first start with support coverage. Recall that the goal is to estimate S m (p), the expected number of distinct symbols that we see after observing m samples from p. By the linearity of expectation,\nS m (p) = x\u2208X E[I Nx(X m )>0 ] = x\u2208X (1 \u2212 (1 \u2212 p(x)) m ) .\nThe problem is closely related to the support coverage problem (Orlitsky et al., 2016), where the goal is to estimate U t (X n ), the number of new distinct symbols that we observe in n \u2022 t additional samples. Hence\nS m (p) = E n i=1 \u03d5 i + E[U t ],\nwhere t = (m \u2212 n)/n. We show that the modification of an estimator in (Orlitsky et al., 2016) is also near-optimal and satisfies conditions in Lemma 2. We propose to use the following estimator\nS m (p) = n i=1 \u03d5 i + U SGT t (X n ),\nwhere\nU SGT t (X n ) = n i=1 \u03d5 i (\u2212t) i Pr(Z \u2265 i)\nand Z is a Poisson random variable with mean r and t = (m \u2212 n)/n.\nThe above theorem also works for any \u03b5 1/n 0.25\u2212\u03b7 for any \u03b7 > 0\nWe remark that the proof also holds for Binomial smoothed random variables as discussed in (Orlitsky et al., 2016).\nWe need to bound the maximum coefficient and the bias to apply Lemma 2. We first bound the maximum coefficient of this estimator.\nLemma 3. For all n \u2264 m/2, the maximum coefficient of S m (p) is at most 1 + e r(t\u22121) . Proof. For any i, the coefficient of \u03d5 i is 1 + (\u2212t) i Pr(Z \u2265 i). It can be upper bounded as 1 + t i=0 e \u2212r (rt\n) i i! = 1 + e r(t\u22121) .\nThe next lemma bounds the bias of the estimator.\nLemma 4. For all n \u2264 m/2, the bias of the estimator is bounded by\n|E[\u015c m (p)] \u2212 S m (p)| \u2264 2 + 2e r(t\u22121) + min(m, S(p))e \u2212r . Proof. As before let t = (m \u2212 n)/n. E[\u015c m (p)] \u2212 S m (p) = n i=1 E[\u03d5 i ] + E[U SGT t (X n )] \u2212 x\u2208X (1 \u2212 (1 \u2212 p(x)) m ) = E[U SGT t (X n )] \u2212 x\u2208X ((1 \u2212 p(x)) n \u2212 (1 \u2212 p(x)) m ) .\nHence by Lemma 8 and Corollary 2, in (Orlitsky et al., 2016), we get\n|E[\u015c m (p)] \u2212 S m (p)| \u2264 2+2e r(t\u22121) + min(m, S(p))e \u2212r .\nUsing the above two lemmas we prove results for both the observed support coverage and support estimator.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Support Coverage Estimator", "text": "Recall that the quantity of interest in support coverage estimation is S m (p)/m, which we wish to estimate to an accuracy of \u03b5.\nProof of Lemma 2 for observed. If we choose r = log 3 \u03b5 , then by Lemma 3, the maximum coefficient of\u015c\nm (p)/m is at most 2 m e m n log 3 \u03b5 , which for m \u2264 \u03b1 n log(n/2 1/\u03b1 ) log(3/\u03b5) is at most n \u03b1 /m < n \u03b1 /n. Similarly, by Lemma 4, 1 m |E[\u015c m (p)] \u2212 S m (p)| \u2264 1 m (2 + 2e r(t\u22121) + me \u2212r ) \u2264 \u03b5,\nfor all \u03b5 > 6n \u03b1 /n.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Support Estimator", "text": "Recall that the quantity of interest in support estimation is S(p), which we wish to estimate to an accuracy of \u03b5.\nProof of Lemma 2 for support. Note that we are interested in distributions with all the non zero probabilities are at least 1/k. We propose to estimateS(p) using\u015c m (p)/k, for m = k log 3 \u03b5 . If we choose r = log 3 \u03b5 , then by Lemma 3, the maximum coefficient of\u015c m (p)/k is at most\n2 k e m n log 3 \u03b5 , which for n \u2265 k \u03b1 log(k/2 1/\u03b1 ) log 2 3 \u03b5 is at most k \u03b1 /k < n \u03b1 /n.\nTo bound the bias, note that for this choice of m\n0 \u2264 S(p) \u2212 S m (p) = x (1 \u2212 p(x)) m \u2264 x e \u2212mp(x) \u2264 ke \u2212 log 3 \u03b5 = k\u03b5 3 .\nSimilarly, by Lemma 4,\n1 k |E[\u015c m (p)] \u2212 S(p)| \u2264 1 k |E[\u015c m (p)] \u2212 S m (p)| + 1 k |S(p) \u2212 S m (p)| \u2264 1 k (2 + 2e r(t\u22121) + ke \u2212r ) + \u03b5 3 \u2264 \u03b5,\nfor all \u03b5 > 12n \u03b1 /n.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Discussion and Future Directions", "text": "We studied estimation of symmetric properties of discrete distributions using the principle of maximum likelihood, and proved optimality of this approach for a number of problems. A number of directions are of interest. We believe that the lower bound requirement on \u03b5 is perhaps an artifact of our proof technique, and that the PML based approach is indeed optimal for all ranges of \u03b5. Approximation algorithms for estimating the PML distributions would be a fruitful direction to pursue. Given our results, approximations stronger than exp(\u2212\u03b5 2 n) would be very interesting. In the particular case when the desired accuracy is a constant, even an exponential approximation would be sufficient for many properties. We plan to apply the heuristics proposed by (Vontobel, 2012) for various problems we consider, and compare with the state of the art provable methods. ", "publication_ref": ["b34"], "figure_ref": [], "table_ref": []}, {"heading": "A. Entropy and Distance to Uniformity", "text": "The known optimal estimators for entropy and distance to uniformity both depend on the best polynomial approximation of the corresponding functions and the splitting trick (Wu & Yang, 2016;Jiao et al., 2015). Building on their techniques, we show that a slight modification of their estimators satisfy conditions in Lemma 2. Both these functions can be written as functionals of the form:\nf (p) = x g(p(x)),\nwhere g(y) = \u2212y log y for entropy and g(y) = y \u2212 1 k for uniformity.\nBoth (Wu & Yang, 2016;Jiao et al., 2015) first approximate g(y) with P L,g (y) polynomial of some degree L. Clearly a larger degree implies a smaller bias/approximation error, but estimating a higher degree polynomial also implies a larger statistical estimation error. Therefore, the approach is the following:\n\u2022 For small values of p(x), we estimate the polynomial\nP L,g (p(x)) = L i=1 b i \u2022 (p(x)) i .\n\u2022 For large values of p(x) we simply use the empirical estimator for g(p(x)).\nHowever, it is not a priori known which symbols have high probability and which have low probability. Hence, they both assume that they receive 2n samples from p. They then divide them into two set of samples, X 1 , . . . , X n , and X 1 , . . . , X n . Let N x , and N x be the number of appearances of symbol x in the first and second half respectively. They propose to use the estimator of the following form:\ng(X 2n 1 ) = max min x g x , f max , 0 .\nwhere f max is the maximum value of the property f and\ng x = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 G L,g (N x ), for N x < c 2 log n, and N x < c 1 log n, g Nx n , for N x < c 2 log n, and N x \u2265 c 1 log n, g Nx n + g n , for N x \u2265 c 2 log n, where g n is the first order bias correction term for g, G L,g (N x ) = L i=1 b i N i\nx /n i is the unbiased estimator for P L,g , and c 1 and c 2 are two constants which we decide later. We remark that unlike previous works, we set g x to 0 for some values of N x and N x to ensure that c * is bounded. The following lemma bounds c * for any such estimator\u011d. Lemma 5. For any estimator\u011d defined as above, changing any one of the values changes the estimator by at most\n8 max e L 2 /n max |b i |, L g n , g c 1 log(n) n , g n ,\nwhere L g = n max i\u2208N |g(i/n) \u2212 g((i \u2212 1)/n)|.", "publication_ref": ["b37", "b20", "b37", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "A.1. Entropy", "text": "The following lemma is adapted from Proposition 4 in (Wu & Yang, 2016) where we make the constants explicit.\nLemma 6. Let g n = 1/(2n) and \u03b1 > 0. Suppose c 1 = 2c 2 , and c 2 > 35, Further suppose that n 3 16c1\n\u03b1 2 + 1 c2 > log k \u2022 log n.\nThere exists a polynomial approximation of \u2212y log y with degree L = 0.25\u03b1, over [0, c 1 log n n ] such that max i |b i | \u2264 n \u03b1 /n and the bias of the entropy estimator is\nat most O c1 \u03b1 2 + 1 c2 + 1 n 3.9 k n log n .\nProof. Our estimator is similar to that of (Wu & Yang, 2016;Jiao et al., 2016) except for the case when N x < c 2 log n, and N x > c 1 log n. For any p(x), and N x and N x both distributed Bin(np(x)). By the Chernoff bounds for binomial distributions, the probability of this event can be bounded by,\nmax p(x) Pr N x < c 2 log n, N x > 2c 2 log n \u2264 1 n 0.1 \u221a 2c2 \u2264 1 n 4.9 .\nTherefore, the additional bias the modification introduces is at most k log k/n 4.9 which is smaller than the bias term of (Wu & Yang, 2016;Jiao et al., 2016).\nThe largest coefficient can be bounded by using that the best polynomial approximation of degree L of x log x in the interval [0, 1] has all coefficients at most 2 3L . Therefore, the largest change we have (after appropriately normalizing) is the largest value of b i which is\n2 3L e L 2 /n n .\nFor L = 0.25\u03b1 log n, this is at most n a n .\nThe proof of Lemma 2 for entropy follows from the above lemma and Lemma 5 and by substituting n = O k log k 1 \u03b5 .", "publication_ref": ["b37", "b37", "b21", "b37", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "A.2. Distance to Uniformity", "text": "We state the following result stated in (Jiao et al., 2016).\nLemma 7. Let c 1 > 2c 2 , c 2 = 35. There is an estimator for distance to uniformity that changes by at most n \u03b1 /n when a sample is changed, and the bias of the estimator is\nat most O( 1 \u03b1 c1 log n k\u2022n ).\nProof. Estimating the distance to uniformity has two regions based on N x and N x .\nCase 1: 1 k < c 2 log n/n. In this case, we use the estimator defined in the last section for g(x) = |x \u2212 1/k|. Case 2: 1 k > c 2 log n/n. In this case, we have a slight change to the conditions under which we use various estimators.\n\u2022 For N x \u2212 1 k < c2 log n kn , & N x \u2212 1 k < c1 log n kn : g x = G L,g (N x ), \u2022 For N x \u2212 1 k < c2 log n kn , & N x \u2212 1 k < c1 log n kn : g x = 0, \u2022 For N x \u2212 1 k \u2265 c2 log n kn : g x = Nx n .\nThe estimator proposed in (Jiao et al., 2016) is slightly different, assigning G L,g (N x ) for the first two cases. We design the second case to bound the maximum deviation. The bias of their estimator was shown to be at most O 1 L log n k\u2022n log n , which can be shown by using Equation Equation 7.2.2 of (Timan, 1963)\nE |x\u2212\u03c4 |,L,[0,1] \u2264 O \u03c4 (1 \u2212 \u03c4 ) L .(6)\nBy our choice of c 1 , c 2 , our modification changes the bias by at most 1/n 4 < \u03b5 2 .\nTo bound the largest deviation, we use the fact from Lemma 2 in (Cai et al., 2011) that the largest coefficient of the best degree-L polynomial approximation of |x| in [\u22121, 1] has all coefficients at most 2 3L . Similar argument as with entropy yields that after appropriate normalization, the largest difference in estimation will be at most n \u03b1 /n. ", "publication_ref": ["b21", "b21", "b28", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "The authors thank the reviewers for valuable feedback and the NSF for support through grants CIF-1564355, CIF-1619448, CRII-CIF-1657471, and a Cornell University start-up grant. Jayadev Acharya thanks Clement Canonne, Jiantao Jiao, and Pascal Vontobel for interesting discussions.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Exact calculation of pattern probabilities", "journal": "", "year": "2010", "authors": "Jayadev Acharya;  Das;  Hirakendu;  Mohimani;  Hosein; Alon Orlitsky;  Pan;  Shengjun"}, {"ref_id": "b1", "title": "", "journal": "Competitive closeness testing. COLT", "year": "2011", "authors": "Jayadev Acharya;  Das;  Hirakendu;  Jafarpour;  Ashkan; Alon Orlitsky;  Pan;  Shengjun"}, {"ref_id": "b2", "title": "Competitive classification and closeness testing", "journal": "", "year": "2012", "authors": "Jayadev Acharya;  Das;  Hirakendu;  Jafarpour;  Ashkan;  Orlitsky;  Alon;  Pan;  Shengjun; Ananda Suresh;  Theertha"}, {"ref_id": "b3", "title": "Optimal probability estimation with applications to prediction and classification", "journal": "", "year": "2013", "authors": "Jayadev Acharya;  Jafarpour;  Ashkan; Alon Orlitsky; Ananda Suresh;  Theertha"}, {"ref_id": "b4", "title": "A competitive test for uniformity of monotone distributions", "journal": "", "year": "2013", "authors": "Jayadev Acharya;  Jafarpour;  Ashkan; Alon Orlitsky; Ananda Suresh; ; Theertha;  Acharya;  Jayadev;  Orlitsky;  Alon; Ananda Suresh;  Theertha;  Tyagi;  Himanshu"}, {"ref_id": "b5", "title": "fisher and the making of maximum likelihood 1912-1922", "journal": "Statistical Science", "year": "1997-09", "authors": "John R Aldrich"}, {"ref_id": "b6", "title": "Estimating a probability mass function with unknown labels", "journal": "", "year": "2013", "authors": "Dragi Anevski;  Gill; D Richard; Stefan Zohren"}, {"ref_id": "b7", "title": "Sampling algorithms: lower bounds and applications", "journal": "ACM", "year": "2001", "authors": " Bar-Yossef;  Ziv; Ravi Kumar; D Sivakumar"}, {"ref_id": "b8", "title": "Testing that distributions are close", "journal": "", "year": "2000", "authors": "Tugkan Batu;  Fortnow;  Lance;  Rubinfeld;  Ronitt; Warren D Smith; Patrick White"}, {"ref_id": "b9", "title": "Concentration Inequalities: A Nonasymptotic Theory of Independence", "journal": "OUP", "year": "2013", "authors": "S Boucheron; G Lugosi; P Massart"}, {"ref_id": "b10", "title": "Estimation of KL divergence between large-alphabet distributions", "journal": "", "year": "2016", "authors": "Yuheng Bu;  Zou;  Shaofeng; Yingbin Liang;  Veeravalli; V Venugopal"}, {"ref_id": "b11", "title": "Optimal bounds for estimating entropy with pmf queries", "journal": "Springer", "year": "2015", "authors": "Cafer Caferov;  Kaya;  Bar\u0131\u015f; Ryan Odonnell;  Say;  Cem"}, {"ref_id": "b12", "title": "Testing composite hypotheses, hermite polynomials and optimal estimation of a nonsmooth functional", "journal": "The Annals of Statistics", "year": "2011", "authors": "T Cai;  Tony;  Low; G Mark"}, {"ref_id": "b13", "title": "A survey on distribution testing: Your data is big. but is it blue?", "journal": "Electronic Colloquium on Computational Complexity (ECCC)", "year": "2015", "authors": "Cl\u00e9ment L Canonne"}, {"ref_id": "b14", "title": "Models and estimators linking individual-based and sample-based rarefaction, extrapolation and comparison of assemblages", "journal": "Journal of plant ecology", "year": "2012", "authors": "Robert K Colwell; Anne Chao; Nicholas J Gotelli;  Lin; Mao Shang-Yi; Chang Xuan;  Chazdon; L Robin; Longino ; John T "}, {"ref_id": "b15", "title": "Elements of information theory", "journal": "Wiley", "year": "2006", "authors": "Thomas M Cover; Joy A Thomas"}, {"ref_id": "b16", "title": "Combinatorial methods in density estimation", "journal": "Springer", "year": "2001", "authors": "Luc Devroye; G\u00e1bor Lugosi"}, {"ref_id": "b17", "title": "The number of new species, and the increase in population coverage, when a sample is increased", "journal": "Biometrika", "year": "1956", "authors": " Good; G H Toulmin"}, {"ref_id": "b18", "title": "Asymptotic formulaae in combinatory analysis", "journal": "Proceedings of the London Mathematical Society", "year": "1918", "authors": "Godfrey H Hardy; Srinivasa Ramanujan"}, {"ref_id": "b19", "title": "Maximum likelihood estimation of functionals of discrete distributions", "journal": "", "year": "2014", "authors": "Jiantao Jiao;  Venkat;  Kartik; Yanjun Han; Tsachy Weissman"}, {"ref_id": "b20", "title": "Minimax estimation of functionals of discrete distributions", "journal": "IEEE Transactions on Information Theory", "year": "2015", "authors": "Jiantao Jiao;  Venkat;  Kartik; Yanjun Han; Tsachy Weissman"}, {"ref_id": "b21", "title": "Minimax estimation of the L1 distance", "journal": "", "year": "2016", "authors": "Jiantao Jiao; Yanjun Han; Tsachy Weissman"}, {"ref_id": "b22", "title": "Maximum likelihood: an introduction", "journal": "JSTOR", "year": "1979", "authors": "Le Cam; Lucien Marie"}, {"ref_id": "b23", "title": "On estimating the probability multiset part ii: Properties of the pattern maximum likelihood estimator", "journal": "Arxiv", "year": "2017", "authors": "Alon Orlitsky;  Santhanam;  Narayana; Krishnamurthy Viswanathan; Junan Zhang"}, {"ref_id": "b24", "title": "On the theory and application of pattern maximum likelihood", "journal": "", "year": "2012", "authors": "Shengjun Pan"}, {"ref_id": "b25", "title": "The maximum likelihood probability of unique-singleton, ternary, and length-7 patterns", "journal": "", "year": "2009", "authors": " Pan;  Shengjun; Jayadev Acyarya; Alon Orlitsky"}, {"ref_id": "b26", "title": "Estimation of entropy and mutual information", "journal": "Neural computation", "year": "2003", "authors": "Liam Paninski"}, {"ref_id": "b27", "title": "Strong lower bounds for approximating distribution support size and the distinct elements problem", "journal": "SIAM Journal on Computing", "year": "2009", "authors": " Raskhodnikova; Ron Sofya;  Dana; Amir Shpilka; Adam Smith"}, {"ref_id": "b28", "title": "Theory of Approximation of Functions of a Real Variable", "journal": "Pergamon Press", "year": "1963", "authors": "A F Timan"}, {"ref_id": "b29", "title": "Estimating the unseen: an n/log(n)-sample estimator for entropy and support size, shown optimal via new clts", "journal": "", "year": "2011", "authors": "Gregory Valiant; Paul Valiant"}, {"ref_id": "b30", "title": "The power of linear estimators", "journal": "IEEE", "year": "2011", "authors": "Gregory Valiant; Paul Valiant"}, {"ref_id": "b31", "title": "Instance-by-instance optimal identity testing", "journal": "Electronic Colloquium on Computational Complexity (ECCC)", "year": "2013", "authors": "Gregory Valiant; Paul Valiant"}, {"ref_id": "b32", "title": "Algorithmic approaches to statistical questions", "journal": "", "year": "2012", "authors": "Gregory Valiant;  John"}, {"ref_id": "b33", "title": "Pattern maximum likelihood estimation of finite-state discrete-time markov chains", "journal": "", "year": "2016", "authors": "Shashank Vatedka; Pascal O Vontobel"}, {"ref_id": "b34", "title": "The bethe approximation of the pattern maximum likelihood distribution", "journal": "", "year": "2012", "authors": "Pascal O Vontobel"}, {"ref_id": "b35", "title": "The bethe and sinkhorn approximations of the pattern maximum likelihood estimate and their connections to the valiant-valiant estimate", "journal": "", "year": "2014", "authors": "Pascal O Vontobel"}, {"ref_id": "b36", "title": "Chebyshev polynomials, moment matching, and optimal estimation of the unseen", "journal": "CoRR", "year": "2015", "authors": "Yihong Wu; Pengkun Yang"}, {"ref_id": "b37", "title": "Minimax rates of entropy estimation on large alphabets via best polynomial approximation", "journal": "IEEE Trans. Information Theory", "year": "2016", "authors": "Yihong Wu; Pengkun Yang"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "derived a simple estimator showing that C * (S m (p), \u2206, \u03b5) = \u0398( m log m \u2022 log 1 \u03b5 ). (Zou et al., 2016) derived a more complex estimator with similar dependence on m but worse dependence on \u03b5.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "For a, b > 0, denote a b or b a if for some universal constant c, a/b \u2264 c. Denote a b if both a b and a b.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "The proof of Lemma 2 for entropy follows from the above lemma and Lemma 5 and by substituting n = O k of Approximate ML Performance Proof. We consider symbols such that p(z) \u2265 \u03b4/\u03b2 and p(z) < \u03b4/\u03b2 separately. For an z with p(z) \u2265 \u03b4/\u03b2, by the definition of f (p Z ),p z (z) \u2265 p z (z)\u03b2 \u2265 p(z)\u03b2 \u2265 \u03b4. Applying (2) top z , we have for Z \u223cp z , \u03b4 > Pr f (p z ) \u2212f (Z) > \u03b5 \u2265p z (z) \u2022 I f (p z ) \u2212f (z) > \u03b5 \u2265 \u03b4 \u2022 I f (p z ) \u2212f (z) > \u03b5 ,where I is the indicator function, and therefore,I f (p z ) \u2212f (z) > \u03b5 = 0.This implies that f (p z ) \u2212f (z) < \u03b5. By an identical reasoning, since p(z) > \u03b4/\u03b2, we have f (p) \u2212f (z) < \u03b5. By the triangle inequality,|f (p) \u2212 f (p z )| \u2264 f (p) \u2212f (z) + f (p z ) \u2212f (z) < 2\u03b5.Thus if p(z) \u2265 \u03b4/\u03b2, then PML satisfies the required guarantee with zero probability of error, and any error occurs only when p(z) < \u03b4/\u03b2. We bound this probability as follows. When Z \u223c p, Pr (p(Z) \u2264 \u03b4/\u03b2) \u2264 z\u2208Z:p(z)<\u03b4/\u03b2 p(z) \u2264 \u03b4 \u2022 |Z| /\u03b2.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Lehmann, Erich Leo and Casella, George. Theory of point estimation, volume 31. Springer Science & Business Media, 1998. Obremski, Maciej and Skorski, Maciej. Renyi entropy estimation revisited. In APPROX, 2017. James, Valiant, Gregory, Valiant, Paul, Karczewski, Konrad, Chan, Siu On, Samocha, Kaitlin, Lek, Monkol, Sunyaev, Shamil, Daly, Mark, and MacArthur, Daniel G. Quantifying unobserved protein-coding variants in human populations provides a roadmap for large-scale sequencing projects. Nature Communications, 7:13293 EP, 10, 2016.", "figure_data": "Zou, whiteOrlitsky, A., Pan, S., Sajama, Santhanam, P., Viswanathan,K., and Zhang, J. Pattern maximum likelihood: Compu-tation and experiments. Arxiv, 2017a.Orlitsky, Alon and Pan, Shengjun. The maximum likeli-hood probability of skewed patterns. In ISIT, 2009.Orlitsky, Alon and Suresh, Ananda Theertha. Competitivedistribution estimation: Why is good-turing good. InNIPS, pp. 2143-2151, 2015.Orlitsky, Alon, Santhanam, Narayana P., and Zhang, Junan.Always good turing: Asymptotically optimal probabilityestimation. In FOCS, 2003.Orlitsky, Alon, Sajama, S, Santhanam, NP, Viswanathan,K, and Zhang, Junan. Algorithms for modeling distribu-tions over large alphabets. In ISIT, 2004a.Orlitsky, Alon, Santhanam, Narayana P., Viswanathan, Kr-ishnamurthy, and Zhang, Junan. On modeling profilesinstead of values. In UAI, 2004b.Orlitsky, Alon, Santhanam, Narayana P, and Zhang, Ju-nan. Universal compression of memoryless sources overunknown alphabets. IEEE Transactions on InformationTheory, 50(7):1469-1481, 2004c.Orlitsky,Alon,Santhanam,Narayana Prasad,Viswanathan, Krishna, and Zhang, Junan.Low(size) and order in distribution modeling. 2004d.Orlitsky, Alon, Santhanam, Narayana, Viswanathan, Kr-ishnamurthy, and Zhang, Junan. Convergence of pro-file based estimators. In Proceedings of the 2005 IEEEInternational Symposium on Information Theory (ISIT),pp. 1843 -1847, 2005.Orlitsky,Alon,Santhanam,Narayana Prasad,Viswanathan, Krishna, and Zhang, Junan.Theo-retical and experimental results on modeling lowprobabilities. In Information Theory Workshop, 2006.Orlitsky, Alon, Suresh, Ananda Theertha, and Wu, Yihong.Optimal prediction of the number of unseen species.Proceedings of the National Academy of Sciences, 2016.doi: 10.1073/pnas.1607774113.Orlitsky, Alon, Santhanam, Narayana, Viswanathan, Krish-namurthy, and Zhang, Junan. On estimating the proba-bility multiset part i: The pattern maximum likelihoodapproach. Arxiv, 2017b."}], "formulas": [{"formula_id": "formula_0", "formula_text": "def = min n : p(|f (p) \u2212f (X n )| \u2265 \u03b5) \u2264 \u03b4 \u2200p \u2208 P .", "formula_coordinates": [1.0, 328.64, 455.64, 193.26, 31.15]}, {"formula_id": "formula_1", "formula_text": "\u2206 \u2265 1 k def = {p \u2208 \u2206 : p(x) \u2208 {0} \u222a [1/k, 1]} ,", "formula_coordinates": [2.0, 338.97, 290.46, 170.95, 16.01]}, {"formula_id": "formula_2", "formula_text": "def = S(p)/k. It can be shown that C SML (S(p), \u2206 \u2265 1 k , \u03b5) = \u0398(k log 1 \u03b5", "formula_coordinates": [2.0, 307.44, 315.03, 234.0, 27.96]}, {"formula_id": "formula_3", "formula_text": "showed that C * (S(p), \u2206 \u2265 1 k , \u03b5) = \u0398 k log k \u2022 log 2 1 \u03b5 .", "formula_coordinates": [2.0, 307.44, 342.64, 234.0, 26.92]}, {"formula_id": "formula_4", "formula_text": "def = S m (p)/m.", "formula_coordinates": [2.0, 385.09, 389.22, 61.01, 13.03]}, {"formula_id": "formula_5", "formula_text": "\u2206 k def = {p \u2208 \u2206 : S(p) \u2264 k}. It can be shown that C SML (H(p), \u2206 k , \u03b5) = \u0398( k \u03b5 ) (Panin", "formula_coordinates": [2.0, 307.44, 561.47, 229.83, 34.25]}, {"formula_id": "formula_6", "formula_text": "C * (H(p), \u2206 k , \u03b5) = \u0398 k log k \u2022 1 \u03b5 .", "formula_coordinates": [2.0, 307.44, 656.64, 137.82, 13.47]}, {"formula_id": "formula_7", "formula_text": "showed that C * ( p \u2212 u 1 , \u2206 k , \u03b5) = O k log k \u2022 1 \u03b5 2 , and", "formula_coordinates": [2.0, 307.44, 680.09, 234.0, 25.5]}, {"formula_id": "formula_8", "formula_text": "f (p) P C SML C * PML Entropy H(p) \u2206 k k \u03b5 k log k 1 \u03b5", "formula_coordinates": [4.0, 132.84, 67.45, 303.74, 25.92]}, {"formula_id": "formula_9", "formula_text": "\u2206 \u2265 1 k k log 1 \u03b5 k log k log", "formula_coordinates": [4.0, 243.26, 94.26, 104.43, 14.51]}, {"formula_id": "formula_10", "formula_text": "p \u2212 u 1 \u2206 X k \u03b5 2 k log k 1 \u03b5 2", "formula_coordinates": [4.0, 200.62, 122.64, 149.51, 13.47]}, {"formula_id": "formula_11", "formula_text": ") respec- tively. Theorem 2. For all \u03b5 > c/n 0.2 , any plug-in exp (\u2212 \u221a n)- approximate PMLp satisfies, Entropy Cp(H(p), \u2206 k , \u03b5) C * (H(p), \u2206 k , \u03b5), Support size Cp(S(p)/k, \u2206 \u2265 1 k , \u03b5) C * (S(p)/k, \u2206 \u2265 1 k , \u03b5), Support coverage Cp(S m (p)/m, \u2206, \u03b5) C * (S m (p)/m, \u2206, \u03b5),", "formula_coordinates": [4.0, 55.44, 301.1, 234.0, 174.04]}, {"formula_id": "formula_12", "formula_text": "Cp( p \u2212 u 1 , \u2206 X , \u03b5) C * ( p \u2212 u 1 , \u2206 k , \u03b5).", "formula_coordinates": [4.0, 89.01, 505.04, 186.79, 11.72]}, {"formula_id": "formula_13", "formula_text": "\u03a6 4 = {{1, 1, 1, 1}, {1, 1, 2}, {1, 3}, {2, 2}, {4}}.", "formula_coordinates": [4.0, 55.44, 619.95, 199.82, 10.31]}, {"formula_id": "formula_14", "formula_text": "|\u03a6 n | \u2264 exp(3 \u221a n).", "formula_coordinates": [4.0, 307.44, 202.12, 234.0, 22.27]}, {"formula_id": "formula_15", "formula_text": "def = X n :\u03d5(X n )=\u03d5 p(X n ),", "formula_coordinates": [4.0, 389.3, 257.42, 92.32, 23.91]}, {"formula_id": "formula_16", "formula_text": "p({1, 2}) = \u03d5(X n )={1,2} n i=1 p(X i ) = 3 1 a =b\u2208X p(a) 2 p(b),(1)", "formula_coordinates": [4.0, 308.47, 402.09, 232.97, 43.84]}, {"formula_id": "formula_17", "formula_text": "p \u03d5 def = max p\u2208P p(\u03d5)", "formula_coordinates": [4.0, 392.19, 555.7, 64.5, 17.96]}, {"formula_id": "formula_18", "formula_text": "p \u03d5 = arg max p\u2208\u2206 k a =b p(a) 2 p(b).", "formula_coordinates": [4.0, 364.4, 633.95, 120.09, 22.21]}, {"formula_id": "formula_19", "formula_text": "p z def = arg max p\u2208P p(z)", "formula_coordinates": [5.0, 385.56, 303.83, 77.76, 17.97]}, {"formula_id": "formula_20", "formula_text": "Z \u223c p, Pr f (p) \u2212f (Z) > \u03b5 < \u03b4,(2)", "formula_coordinates": [6.0, 111.83, 254.12, 177.61, 30.58]}, {"formula_id": "formula_21", "formula_text": "Pr (|f (p) \u2212 f (p Z )| > 2\u03b5) \u2264 \u03b4 \u2022 |Z| .(3)", "formula_coordinates": [6.0, 98.73, 316.22, 190.71, 10.27]}, {"formula_id": "formula_22", "formula_text": "|f (p) \u2212 f (p z )| \u2264 f (p) \u2212f (z) + f (p z ) \u2212f (z) \u2264 2\u03b5.", "formula_coordinates": [6.0, 58.39, 434.71, 228.09, 10.26]}, {"formula_id": "formula_23", "formula_text": "Pr (p(Z) < \u03b4) \u2264 z\u2208Z:p(z)<\u03b4 p(z) < \u03b4 \u2022 |Z| .", "formula_coordinates": [6.0, 75.07, 513.15, 177.03, 20.53]}, {"formula_id": "formula_24", "formula_text": "Definition 1 (\u03b2-approximate ML). Let \u03b2 \u2264 1. For Z \u2208 Z, p Z \u2208 P is a \u03b2-approximate ML distribution ifp z (z) \u2265 \u03b2 \u2022 p z (z). When Z is profiles of length-n, a \u03b2-approximate PML is a distributionp \u03d5 such thatp \u03d5 (\u03d5) \u2265 \u03b2 \u2022 p \u03d5 (\u03d5).", "formula_coordinates": [6.0, 55.44, 581.8, 441.17, 45.59]}, {"formula_id": "formula_25", "formula_text": "Pr (|f (p) \u2212 f (p Z )| > 2\u03b5) \u2264 \u03b4 \u2022 |Z|/\u03b2.", "formula_coordinates": [6.0, 93.8, 708.26, 157.29, 9.65]}, {"formula_id": "formula_26", "formula_text": "min n : n 20 log(3Z n ) > n samples.", "formula_coordinates": [6.0, 343.86, 367.86, 161.16, 23.23]}, {"formula_id": "formula_27", "formula_text": "|\u03a6 n | < exp(3 \u221a n).", "formula_coordinates": [6.0, 307.44, 496.63, 234.0, 22.27]}, {"formula_id": "formula_28", "formula_text": "Pr f (X n ) \u2212 E[f (X n )] > t \u2264 2 exp \u2212 2t 2 nc 2 * . (4)", "formula_coordinates": [7.0, 61.96, 103.26, 227.48, 25.77]}, {"formula_id": "formula_29", "formula_text": "Let B n be the bias of an estimatorf (X n ) of f (p), namely B n def = f (p) \u2212 E[f (X n )] . By the triangle inequality, f (p) \u2212f (X n ) \u2264 f (p) \u2212 E[f (X n )] + f (X n ) \u2212 E[f (X n )] = B n + f (X n ) \u2212 E[f (X n )] . Plugging this in (4), Pr f (p) \u2212f (X n )] > t + B n \u2264 2 exp \u2212 2t 2 nc 2 * . (5)", "formula_coordinates": [7.0, 55.44, 274.67, 234.0, 163.2]}, {"formula_id": "formula_30", "formula_text": "Pr (|f (p)\u2212f (p)| > 4\u03b5) < \u03b4 \u2022 |\u03a6 n | \u03b2 \u2264 2 exp(\u22122\u03b5 2 n 1\u22122\u03b1 /c 2 + 3 \u221a n) \u03b2 \u2264 exp(\u2212 \u221a n),", "formula_coordinates": [7.0, 307.94, 226.44, 231.81, 67.28]}, {"formula_id": "formula_31", "formula_text": "S m (p) = x\u2208X E[I Nx(X m )>0 ] = x\u2208X (1 \u2212 (1 \u2212 p(x)) m ) .", "formula_coordinates": [7.0, 310.82, 436.9, 227.24, 22.13]}, {"formula_id": "formula_32", "formula_text": "S m (p) = E n i=1 \u03d5 i + E[U t ],", "formula_coordinates": [7.0, 362.96, 526.94, 122.96, 30.32]}, {"formula_id": "formula_33", "formula_text": "S m (p) = n i=1 \u03d5 i + U SGT t (X n ),", "formula_coordinates": [7.0, 362.28, 624.34, 124.32, 30.32]}, {"formula_id": "formula_34", "formula_text": "U SGT t (X n ) = n i=1 \u03d5 i (\u2212t) i Pr(Z \u2265 i)", "formula_coordinates": [7.0, 334.68, 664.25, 162.21, 14.11]}, {"formula_id": "formula_35", "formula_text": ") i i! = 1 + e r(t\u22121) .", "formula_coordinates": [8.0, 55.44, 194.03, 234.0, 26.45]}, {"formula_id": "formula_36", "formula_text": "|E[\u015c m (p)] \u2212 S m (p)| \u2264 2 + 2e r(t\u22121) + min(m, S(p))e \u2212r . Proof. As before let t = (m \u2212 n)/n. E[\u015c m (p)] \u2212 S m (p) = n i=1 E[\u03d5 i ] + E[U SGT t (X n )] \u2212 x\u2208X (1 \u2212 (1 \u2212 p(x)) m ) = E[U SGT t (X n )] \u2212 x\u2208X ((1 \u2212 p(x)) n \u2212 (1 \u2212 p(x)) m ) .", "formula_coordinates": [8.0, 55.44, 293.25, 233.93, 127.58]}, {"formula_id": "formula_37", "formula_text": "|E[\u015c m (p)] \u2212 S m (p)| \u2264 2+2e r(t\u22121) + min(m, S(p))e \u2212r .", "formula_coordinates": [8.0, 58.83, 467.49, 227.22, 11.72]}, {"formula_id": "formula_38", "formula_text": "m (p)/m is at most 2 m e m n log 3 \u03b5 , which for m \u2264 \u03b1 n log(n/2 1/\u03b1 ) log(3/\u03b5) is at most n \u03b1 /m < n \u03b1 /n. Similarly, by Lemma 4, 1 m |E[\u015c m (p)] \u2212 S m (p)| \u2264 1 m (2 + 2e r(t\u22121) + me \u2212r ) \u2264 \u03b5,", "formula_coordinates": [8.0, 55.44, 623.94, 234.01, 72.76]}, {"formula_id": "formula_39", "formula_text": "2 k e m n log 3 \u03b5 , which for n \u2265 k \u03b1 log(k/2 1/\u03b1 ) log 2 3 \u03b5 is at most k \u03b1 /k < n \u03b1 /n.", "formula_coordinates": [8.0, 307.44, 185.57, 234.0, 25.68]}, {"formula_id": "formula_40", "formula_text": "0 \u2264 S(p) \u2212 S m (p) = x (1 \u2212 p(x)) m \u2264 x e \u2212mp(x) \u2264 ke \u2212 log 3 \u03b5 = k\u03b5 3 .", "formula_coordinates": [8.0, 316.75, 240.29, 215.38, 51.78]}, {"formula_id": "formula_41", "formula_text": "1 k |E[\u015c m (p)] \u2212 S(p)| \u2264 1 k |E[\u015c m (p)] \u2212 S m (p)| + 1 k |S(p) \u2212 S m (p)| \u2264 1 k (2 + 2e r(t\u22121) + ke \u2212r ) + \u03b5 3 \u2264 \u03b5,", "formula_coordinates": [8.0, 332.16, 322.2, 184.56, 70.27]}, {"formula_id": "formula_42", "formula_text": "f (p) = x g(p(x)),", "formula_coordinates": [12.0, 131.38, 179.1, 82.13, 19.61]}, {"formula_id": "formula_43", "formula_text": "P L,g (p(x)) = L i=1 b i \u2022 (p(x)) i .", "formula_coordinates": [12.0, 75.37, 329.4, 130.73, 14.11]}, {"formula_id": "formula_44", "formula_text": "g(X 2n 1 ) = max min x g x , f max , 0 .", "formula_coordinates": [12.0, 81.96, 481.26, 180.96, 21.69]}, {"formula_id": "formula_45", "formula_text": "g x = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 G L,g (N x ), for N x < c 2 log n, and N x < c 1 log n, g Nx n , for N x < c 2 log n, and N x \u2265 c 1 log n, g Nx n + g n , for N x \u2265 c 2 log n, where g n is the first order bias correction term for g, G L,g (N x ) = L i=1 b i N i", "formula_coordinates": [12.0, 55.44, 525.51, 235.07, 72.75]}, {"formula_id": "formula_46", "formula_text": "8 max e L 2 /n max |b i |, L g n , g c 1 log(n) n , g n ,", "formula_coordinates": [12.0, 70.33, 677.76, 204.22, 22.31]}, {"formula_id": "formula_47", "formula_text": "\u03b1 2 + 1 c2 > log k \u2022 log n.", "formula_coordinates": [12.0, 307.44, 130.46, 234.0, 25.01]}, {"formula_id": "formula_48", "formula_text": "at most O c1 \u03b1 2 + 1 c2 + 1 n 3.9 k n log n .", "formula_coordinates": [12.0, 307.44, 183.55, 157.17, 13.64]}, {"formula_id": "formula_49", "formula_text": "max p(x) Pr N x < c 2 log n, N x > 2c 2 log n \u2264 1 n 0.1 \u221a 2c2 \u2264 1 n 4.9 .", "formula_coordinates": [12.0, 307.44, 297.54, 245.71, 23.71]}, {"formula_id": "formula_50", "formula_text": "at most O( 1 \u03b1 c1 log n k\u2022n ).", "formula_coordinates": [12.0, 307.44, 625.37, 94.99, 14.0]}, {"formula_id": "formula_51", "formula_text": "\u2022 For N x \u2212 1 k < c2 log n kn , & N x \u2212 1 k < c1 log n kn : g x = G L,g (N x ), \u2022 For N x \u2212 1 k < c2 log n kn , & N x \u2212 1 k < c1 log n kn : g x = 0, \u2022 For N x \u2212 1 k \u2265 c2 log n kn : g x = Nx n .", "formula_coordinates": [13.0, 65.4, 121.16, 222.65, 109.39]}, {"formula_id": "formula_52", "formula_text": "E |x\u2212\u03c4 |,L,[0,1] \u2264 O \u03c4 (1 \u2212 \u03c4 ) L .(6)", "formula_coordinates": [13.0, 100.4, 333.96, 189.04, 22.5]}], "doi": ""}