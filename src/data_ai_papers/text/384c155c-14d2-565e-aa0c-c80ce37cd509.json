{"title": "Inducing Positive Perspectives with Text Reframing", "authors": "Caleb Ziems; Minzhi Li; Anthony Zhang; Diyi Yang", "pub_date": "", "abstract": "Sentiment transfer is one popular example of a text style transfer task, where the goal is to reverse the sentiment polarity of a text. With a sentiment reversal comes also a reversal in meaning. We introduce a different but related task called positive reframing in which we neutralize a negative point of view and generate a more positive perspective for the author without contradicting the original meaning. Our insistence on meaning preservation makes positive reframing a challenging and semantically rich task. To facilitate rapid progress, we introduce a large-scale benchmark, POSITIVE PSY-CHOLOGY FRAMES, with 8,349 sentence pairs and 12,755 structured annotations to explain positive reframing in terms of six theoreticallymotivated reframing strategies. Then we evaluate a set of state-of-the-art text style transfer models, and conclude by discussing key challenges and directions for future work. To download the data, see https://github.", "sections": [{"heading": "Introduction", "text": "Gratitude is not only the greatest of virtues, but the parent of all the others.\n-Marcus Tullius Cicero\nText style transfer (TST) has received much attention from the language technologies community (Hovy, 1987;Jin et al., 2020), where the goal is to change some attribute, like the sentiment of the text, without changing any attribute-independent content (Mir et al., 2019;Fu et al., 2018;Logeswaran et al., 2018). Some TST applications such as de-biasing (Pryzant et al., 2020;Ma et al., 2020) and paraphrasing (den Bercken et al., 2019;Xu et al., 2012) require meaning-preserving transformations, while political leaning (Prabhumoye et al., 2018), sentiment (Shen et al., 2017;Hu et al., 2017), and topical transfer (Huang et al., 2020)   in the underlying meaning. For instance, for a negative review, \"this was a bland dish,\" we can use a sentiment TST model to create a more positive \"this was a tasty dish,\" by swapping the word bland with tasty. Although the input's structure and attribute-independent content are preserved, the truth-conditional meaning is clearly altered.\nIn this work, we introduce a closely related taskpositive reframing-that differs from sentiment TST in important ways. We effectively reframe negative text by inducing a complementary positive viewpoint (e.g. glass-half-full), which nevertheless supports the underlying content of the original sentence. The reframe should implicate rather than contradict the source (see Figure 1), and the transformation should be motivated by theoretically justified strategies from from positive psychology (Harris et al. 2007; see Section 3).\nTo use the example from before, we could reframe \"this was a bland dish\" with the self-affirmation strategy and say \"I've made dishes that are much tastier than this one.\" This reframed one still communicates the author's original intention by conversationally implicating that the dish was unsatisfying (Grice, 1975), but it shifts the focus away from the negative judgment and onto a positive and self-affirming perspective. Numerous studies have shown the positive effects of this and other reframing strategies on well-being and cognitive performance (Martens et al., 2006;Cohen et al., 2006;Good et al., 2003), which motivate this work.\nOur main contribution is the design and implementation of a new positive reframing task. To facilitate research in this space, we introduce a parallel corpus of 8,349 reframed sentence pairs and 12,755 structured annotations for six theoreticallymotivated re-write strategies. This is a significant contribution, especially since rich parallel corpora are scarce in TST tasks. Some related datasets exist for politeness (Madaan et al., 2020) and sentiment transfer (Shen et al., 2017;He and McAuley, 2016), but they lack this parallel structure. With only unaligned corpora, researchers are limited to unsupervised training paradigms, which notoriously fail to disentangle style from content, and thus also fail to preserve meaning (Lample et al., 2019). Using our parallel corpus, we examine how current state-ofthe-art neural models work for positive reframing. We find that, supervised transformer-based neural models appear capable of rewriting a negative text without contradicting the original premise of that text. However, these models still struggle to generate reasonable positive perspectives, suggesting that our dataset will serve as a useful benchmark for understanding psychologically well-motivated strategies for augmenting text with positive perspectives.", "publication_ref": ["b42", "b46", "b70", "b31", "b61", "b78", "b64", "b17", "b104", "b77", "b89", "b43", "b44", "b38", "b34", "b67", "b12", "b33", "b65", "b89", "b40"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Related Work", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Style-Transfer", "text": "There is a longstanding interest in style transfer, starting with the early days schema-based systems (McDonald and Pustejovsky, 1985;Hovy, 1987), and then syntax-based (Zhu et al., 2010;Xu et al., 2016) and phrase-based machine translation (Xu et al., 2012;Wubben et al., 2012), into the age of end-to-end neural models. Recent works include supervised seq2seq tasks on parallel data (Rao and Tetreault, 2018;Fu et al., 2018) or pseudo-parallel data Zhang et al., 2020b), as well as unsupervised generative modeling on nonparallel data (Hu et al., 2017;Shen et al., 2017), and semi-supervised techniques (Shang et al., 2019). Other ideas include domain adaptation  or multi-task learning (Niu et al., 2018), zeroshot translation (Korotkova et al., 2019), unsupervised \"delete and generate\" approaches (Li et al., 2018;Sudhakar et al., 2019;Malmi et al., 2020;Madaan et al., 2020), and reinforcement learning (Zhang and Lapata, 2017;Wang et al., 2016).\nMany existing datasets lack parallel structure, so the unsupervised setting is common in TST. Unfortunately, many of these methods still fail to disentangle style from content and adequately preserve the meaning of the original text (Lample et al., 2019). Autoencoders are particularly vulnerable to this shortcoming (Hu et al., 2017;, but some unsupervised machine translation techniques appear less vulnerable (Artetxe et al., 2018;Lample et al., 2018). In contrast, our positive reframing task requires source meaningpreservation and the introduction of new content and new perspectives, posing a unique challenge to unsupervised methods. We also provide a parallel corpus to train supervised models for this task.", "publication_ref": ["b69", "b42", "b112", "b103", "b104", "b102", "b81", "b31", "b110", "b43", "b89", "b86", "b74", "b52", "b57", "b95", "b66", "b65", "b109", "b100", "b43", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Language and Positive Psychology", "text": "Positivity is contagious and can spread quickly across social networks (Coviello et al., 2014;Hatfield et al., 1993). Positive contagion in teams can reduce group conflict and improve group cooperation and even task performance (Barsade, 2002). Effective leaders also harness the power of positive reframing to promote company growth (Sy and Choi, 2013;Sy et al., 2005;Johnson, 2009;Masters, 1992) and beneficially shape negotiations (Filipowicz et al., 2011), customer relations (Dietz et al., 2004), decision making (G\u00e4chter et al., 2009;Druckman, 2001) and policy outcomes (Erisen et al., 2014). At an individual level, people who express optimism and gratitude are less likely to have depressive symptoms (Lambert et al., 2012) and more likely to experience emotional and psychological well-being (Carver et al., 1999;Watkins et al., 2008;Scheier et al., 2001).\nOn the other hand, fake expressions of positivity are correlated with negative brain activity (Ekman et al., 1990) and may actually be more harmful than helpful (Fredrickson, 2000;Fredrickson and Losada, 2005;Gross, 2013;Logel et al., 2009). That is why in our task it is essential that any positively reframed rephrased text remain true to the original premise of the source. In this way, our task is most similar to meaning-preserving transformations via parallel corpora from domains such as political argumentation (Chakrabarty et al., 2021), de-biasing (Pryzant et al., 2020;Ma et al., 2020), politeness (Madaan et al., 2020), and paraphrasing (den Bercken et al., 2019;Xu et al., 2012).", "publication_ref": ["b14", "b39", "b3", "b97", "b98", "b50", "b68", "b28", "b19", "b32", "b20", "b27", "b54", "b9", "b101", "b83", "b23", "b29", "b30", "b36", "b60", "b11", "b78", "b64", "b65", "b17", "b104"], "figure_ref": [], "table_ref": []}, {"heading": "Positive Reframing Framework", "text": "In this section, we present our psychologicallymotivated taxonomy of positive reframing strategies. Instead of merely swapping antonyms for negative words or inserting unfounded positive language into a sentence, these strategies work to more fundamentally reconstruct the author's fixed, global, and ultimately harmful selfnarratives, which are known in the literature as cognitive distortions (Burns, 1981;Abramson et al., 2002;Walton and Brady, 2020). Cognitive distortions include many exaggerated or irrational self-focused thoughts (Nalabandian and Ireland, 2019), such as dichotomous \"all-or-nothing\" thinking (Oshio, 2012), over-generalization (Muran and Motta, 1993), and catastrophizing (Sullivan et al., 2001. We can reconstruct these ideas using strategies from positive psychology (Harris et al., 2007). Each strategy is designed to promote a beneficial shift in perspective without distorting the underlying context of the author's situation.\nGrowth Mindset or, alternatively, the incremental theory of personality (Yeager et al., 2014;Burnette and Finkel, 2012), is the belief that one's skills and abilities are not immutable but can instead be changed and improved over time (Dweck, 2016); that one's willpower is an abundant rather than limited or exhaustible resource (Job et al., 2010(Job et al., , 2015; and that apparent setbacks like stress can be enhancing rather than debilitating (Crum et al., 2013). Instead of saying \"I'm such a lazy procrastinator,\" a growth-mindset would say \"I'm determined to learn better time management.\" This mindset has demonstrable benefits like improved performance on school tests (Good et al., 2003;Blackwell et al., 2007;Dweck and Yeager, 2019;Yeager et al., 2014).\nImpermanence means understanding that negative experiences are finite and temporary, and that others have also experienced or even overcome similar forms of adversity. Someone might say \"since I failed this test, I must be too stupid for school.\" An impermanence reframe could be \"This wasn't the test score I hoped for, but everyone slips up now and then.\" This category is also related to those proposed by Walton and Brady (2020):\n(1) focus on the \"possibility of improvement,\" (2) recognize \"specific, normal causes,\" and (3) under-stand \"you're not the only one.\"\nNeutralizing involves removing or rewriting negative phrases and terms so they are more neutral (Pryzant et al., 2020). Someone might complain that \"Wendy's customer service is terrible.\" A neutralized reframe could be \"Wendy's customer service could use some improvement.\"\nOptimism does not mean to negate or deny the negative aspects of a situation, but instead to shift the emphasis to the more positive aspects of the situation, including expectations for a bright future (Carver et al., 2010). For example, if there is a negative emphasis, like in the sentence, \"I've completely worked myself to the bone this week, burning the candle at both ends... TGIF,\" we can use optimism to shift the emphasis towards the positive as follows: \"It's been a long week, but now I can kick back, relax, and enjoy my favorite shows because it's the weekend.\"\nSelf-affirmation means to assert a more holistic or expansive version of oneself by listing one's values, skills, and positive characteristics (Cohen and Sherman, 2014;Silverman et al., 2013). Positive psychology gives many examples like love, courage, hope, gratitude, patience, forgiveness, creativity, and humor (Harris et al., 2007). Reflecting on these values can bolster one's sense of integrity (see Self-Affirmation Theory; Steele 1988), can reduce depressive affect (Enright and Fitzgibbons, 2000), and can translate to increased performance on measurable tasks like exams (Martens et al., 2006;Cohen et al., 2006;Sherman et al., 2009). Thankfulness can also be described more broadly as an \"attitude of gratitude\" (Emmons and Shelton, 2002). Adding more positive words that convey thankfulness or gratitude (e.g. appreciate, glad that, thankful for). For example, we can reframe the rhetorical question ,\"Is it sad that I don't wanna be at home and wish that work could call me in early?\" by expressing gratitude for career: \"I am thankful that I have a job that makes me want to get out of bed everyday.\"", "publication_ref": ["b8", "b0", "b99", "b72", "b75", "b38", "b106", "b7", "b21", "b49", "b48", "b15", "b33", "b5", "b22", "b106", "b78", "b10", "b13", "b92", "b38", "b26", "b67", "b12", "b90", "b25"], "figure_ref": [], "table_ref": []}, {"heading": "Data Collection", "text": "We sourced all of our data from the Twitter API, filtering tweets according to the hashtag #stressed due to a few reasons. Note that at the time of data collection and annotation, there were no publicly available datasets with annotated  cognitive distortions, and the literature on distortion classification was still relatively unexplored (Simms et al., 2017;Shickel et al., 2020). We instead chose the simple keyword #stressed to signal the anxiety, negative affect, and hopelessness that has been shown to accompany cognitive distortions by prior work (Sears and Kraus, 2009). 1 Our decision to use Twitter was also motivated by the 280 character limit, which ensured that samples were short, focused expressions of relatively atomic ideas, as opposed to longer narrative-style texts from discussion platforms like Reddit's r/rant.\nOur filtered collection of negative texts comes from a collection of over 1 million #stressed tweets written between 2012 and 2021, and it excludes any replies and retweets, any insubstantial tweets less than 30 characters, and any text containing a URL, which is often associated with spam (Zhang et al., 2012;Grier et al., 2010). After we removed other hashtags or Twitter handles from the text, we used TextBlob (Loria, 2018) to exclude any overtly positive texts with a non-negative sentiment score. Finally, to reduce any confounds between cognitive distortions and hate speech, and to make the human annotation task more agreeable for crowd-workers, we excluded examples that were flagged as offensive with over 80% confidence according to HateSonar (Davidson et al., 2017). 1 We also considered pet peeve, fml, and other keywords but manual inspection revealed that these tweets were unlikely to contain cognitive distortions. In contrast, stressed hashtag provides a high precision data collection. We acknowledge this as a limitation and urge readers to keep this mind when interpreting our findings.", "publication_ref": ["b93", "b91", "b84", "b108", "b35", "b62", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Annotation", "text": "We recruited crowdworkers to reframe 8,687 randomly-sampled texts with two workers assigned to each task, so we had two unique reframe annotations for every tweet. The annotators were encouraged to decide independently which reframing strategy to use, and they could combine multiple strategies in the same reframe. We simply asked annotators to record the strategies they selected. Additionally, they gave us, on a scale from 1-5, a score indicating how positive the original text was, and separately, how positive the text had become after they reframed it. Finally, we asked workers to mark advertisements, spam, or any text they felt they could not understand or effectively reframe. These examples were later removed from the corpus (see Appendix A for details).\nIn total, 204 workers participated in this task. Before they worked on the task, workers were asked to be familiar with our task by reading our provided reframing examples for each of the six strategies (Section 3), along with detailed annotation instructions. Then they had to pass a qualification test to show they can recognize different strategies in different reframing examples, with at least 5 out of 6 multiple-choice questions answered correctly.\nWe paid all annotators a fair wage above the federal minimum and both manually and programmatically inspected their work for quality (see Appendix A). After removing any poor-quality data, we were left with 8,349 reframed sentences. The strategy label distribution is given on the left side of Table 1, where a single reframe can have more than one strategy label.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Data Quality", "text": "To determine the reliability of the reframing strategy constructs, we randomly sampled 100 annotations from Section 4.1 and asked three annotators to consider both the original text and the reframed text, and then the annotators marked which of the six strategies were used in the given reframe. This allowed us to compute inter-annotator agreement scores for the strategy labels in Table 1. We observe the Intra-class Correlation for one-way random effects between the three raters and find moderate inter-rater agreement across these attribute categories (min 0.32; max 68). We also asked this second round of annotators to evaluate the genuineness of the reframes on a scale from 1-5. Our instructions explain that, with a more genuine reframe, it is more likely that someone in the original situation would say something similar. We find that, across all strategy labels, the average genuineness score is \u223c 4 out of 5, so we know the data conforms reasonably well to our task instructions.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Positive Reframing", "text": "With POSITIVE PSYCHOLOGY FRAMES, we then examine how generative models work to automatically suggest a negatively-oriented self-narrative with a more positive shift in perspective without distorting any of the underlying meaning of that text. To do so will make use of encoder-decoder or conditional language models, as well as the six positive psychology strategies outlined in Section 3.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Task Formulation", "text": "Let (s, t, \u03c8 t ) be a single annotation tuple in POSI-TIVE PSYCHOLOGY FRAMES for original source text s and positive reframe target t, which uses positive psychology strategies given by the multihot encoded vector \u03c8 t . In the Positive Reframing task, our goal is to encode s and, at decoding time, produce t which makes use of \u03c8 t strategies and preserves the underlying meaning of s. Therefore, we formulate the problem as conditional generation and, during training, we maximize the standard language modeling objective\n1 N N i=0 log p(g i |g 0:i\u22121 ) over the string g = {s, \u03c8 t , t} = {<BOS>, s 1 , s 2 , ..., s n , <STRG>, \u03c8 grow , \u03c8 imp , ..., \u03c8 thank , <REFR>, t 1 , t 2 , ..., t m , <EOS>}\nwhere g i is the ith token in the string of length N , which contains the start token <BOS>, the tokenized source s 1:n , the tokenized reframe target t 1:m , and the binary tokens \u03c8 grow , \u03c8 imp , ... indicating whether a particular strategy (e.g. growth mindset) was used in reframe t.\nAt decoding time, we consider three settings: Unconstrained generation p(t|s), Controlled generation p(t|s, \u03c8 t ), and a strategy Prediction form of generation p(t, \u03c8 t |s). Unlike in the Unconstrained setting, the Controlled generation is conditioned on the desired strategies \u03c8 t . In the Prediction setting, the model will concurrently predict the strategies it used to generate its own reframe.\nNote that, we introduce three different model settings here to capture how positive reframing assistance might be used by people in the real world. Specifically, the Unconstrained setting models reframing text directly without being aware of any specific strategy to use. The Prediction setting extends the unconstrained mode, i.e., produce the reframed text and also output the reframing strategies used in the reframing process spontaneously. The Controlled setting simulates the scenario of producing a reframed text with the help of concrete positive reframing strategies.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup", "text": "For ground truth training, development, and testing, we randomly partition the annotations using an 8:1:1 ratio, with 6,679 train, 835 development and 835 test data. We fine-tune the GPT and GPT-2 language models (Radford et al., 2019) as well as two Seq2Seq neural machine translation models -LSTM (Hochreiter and Schmidhuber, 1997) and CopyNMT (See et al., 2017) -and finally, two encoder-decoder models, BART (Lewis et al., 2020) and T5 (Raffel et al., 2020). For all models, we use greedy decoding. As an ablation in the Unconstrained setting, we also test a No-pretrain condition for GPT-2 in which we randomly initialize the model parameters before fine-tuning.\nRetrieval: We test two simple retrieval systems: Random retrieval of a reframed sentence from the training set, and SBERT (Reimers and Gurevych,  \n-1 (R-1), ROUGE-1 (R-2), ROUGE-L (R-L),\nBLEU, BERTScore (BScore), Positivity via \u2206 TextBlob (\u2206 TB) and Fluency. State-of-the-art models can generate meaningpreserving reframes in the unconstrained setting p(t|s) and strategy-predictive setting p(t, \u03c8 t |s) as well as when we condition the generation to use the reframing strategy from the ground truth p(t|s, \u03c8 t ). The best in-category performance is bolded; best overall performance is highlighted .\n2019) retrieval, which finds the most similar t in train by cosine similarity and retrieves one of the corresponding ground-truth r from the training set. Few-shot Learning: Brown et al. (2020) shows the few-shot capabilities of language models and especially larger models like GPT-3. We evaluate few-shot abilities of both GPT-3 and its opensource implementation, GPT-Neo (Black et al., 2021) using k = 5 exemplars (See Appendix C).", "publication_ref": ["b79", "b41", "b85", "b55", "b80", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation", "text": "Following other style transfer work with a parallel corpus (Jhamtani et al., 2017;Xu et al., 2012), we evaluate our models for semantic similarity with the ground truth using the BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), and BERTScore (Zhang et al., 2020a). Since there are two ground truth annotations per tweet, we take the maximum of the two scores and report the average across these maxima. We also report \u2206T extBlob or the average change in sentiment score according to TextBlob (Loria, 2018). Finally, we conduct human evaluation in which 50 items are distributed to 3 raters who score the reframed sentences for three criteria, each on a scale from 1 to 5. The criteria include Meaning Preservation (Shang et al., 2019), our task-specific objective, as well as the Positivity and Fluency of the generated text, following the sentiment style transfer literature (Luo et al., 2019) ", "publication_ref": ["b45", "b104", "b76", "b58", "b107", "b62", "b86", "b63"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Automatic Evaluation Across these metrics (Table 2, left) in the unconstrained generation setting, the BART model provided the highest quality of positive reframes, while GPT provided the worst quality with results similar to the No-pretrain version of GPT-2. The pre-trained version of GPT-2 was trained on English web text, while GPT was trained on works of fiction, so it appears that pretraining decisions can affect performance.\nWe tested the two best-performing models, T-5 and BART, on the controlled generation and strategy-prediction settings as well and found that the both models performed reasonably. Overall, controlled generation boosts performance, since the model can target the gold standard's strategies, but these improvements are only slight (see the Controlled part in Table 2). This warrants further investigation: in Section 5.6, we explore models' ability to identify the underlying strategies given an existing reframe to understand whether models can make sense of these underlying constructs. Unsurprisingly, all supervised models outperformed our simple retrieval baselines. Most interestingly, few-shot GPT-3 and GPT-Neo also could not match the supervised models in terms of overlap with the ground truth (ROUGE, BLEU, BERTScore), but they still achieved a comparable positive shift in sentiment (\u2206 TextBlob).\nHuman Evaluation Human judgments both support and elaborate on the automatic evaluation findings. For our best performing BART and T-5 models, the average scores are very high, even surpassing the quality of the Human gold standard in all of the unconstrained, predictive, and controlled settings. These systems most effectively induce a natural-sounding positive reframe while also preserving the meaning of the original text. This is critical: controlled BART model scored 4.07 in Positivity and 4.27 in Fluency while also achieving the winning Meaning preservation score.\nIn contrast with BART, the few-shot systems fail to preserve the meaning of the original sentence, despite their ability to articulately induce a more positive sentiment (Positivity scores up to 4.17; Fluency scores up to 4.27). Meaning preservation is absolutely critical for this task. From these results, we can conclude that, at the present time, supervised learning may be the most viable option for achieving reliable positive reframing results. POSITIVE PSYCHOLOGY FRAMES will facilitate ongoing efforts in this direction.\nQualitative Investigation Table 3 shows example reframes generated by our best controlled BART model, with one example for each strategy (for a similar comparison between models, see Table 5 in Appendix D). We see that, even without explicit lexical overlap between the generation and ground truth, the model reframes can still shift the cognitive distortions and negative outlook to a more positive perspective. In each of these examples, the model does so without losing the underlying meaning of the original text. Transformer-based models appear to be capable of solving our task with reasonable success. However, success can be highly variable (as evidenced by Table 5), so there is still room for significant improvement.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Error Analysis", "text": "We manually go through 100 randomly sampled model generations by our best controlled BART model, and summarize the main error classes here. We manually investigated 100 randomly sampled model generations by our best controlled BART model, and summarize the four largest error classes here. First, 26% of generations contained (1) insubstantial changes. These were especially prominent in the neutralizing strategy where the model would swap only a few negative words, like changing the phrase \"I hate it\" to \"I don't like it.\" On the other hand, some reframed generations were so drastically modified they contained (2) contradictions to the premise (9% of instances). For example, \"Feel like crying, this math class is impossible to pass\" was transformed into \"This math class is hard, but I know I can pass it\" -a failure of meaning preservation. More concerningly, the system can generate (3) self-contradictions (6%) like the phrase, \"I don't like opening up to people, but I'm glad I have the courage to do it.\" Finally, like many other NLG systems, our system can produce (4) hallucinations (2%) with unmotivated perspectives, like mentioning a good night sleep when the original post was about nosebleeds in the bath.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Frame Strategy Classification", "text": "In Section 5.4, we observed only slight performance gains when conditioning the generation based on the ground-truth reframing strategy (Control section in Table 2). For this reason, we take a closer look at whether models can reliably understand and classify the reframe strategies underlying a given source-reframe text pair. We formulate this problem as a multi-label multi-class classification task over sentence pairs (s, t). Given both the source text and positive reframe target in the annotation tuple (s, t) from POSITIVE PSYCHOL-OGY FRAMES, we predict the multi-hot encoded strategy vector \u03c8 t = [s grow ; s imp ; ...; s thank ] using transformer models. We experiment with a set of state-of-the-art classifiers, including BERT (Devlin et al., 2019), RoBERTA (Liu et al., 2019), and XLNet .\nAs shown in Table 4, all of the classification models can learn to recognize the thankfulness, optimism, and growth mindset strategies with moderate reliability (F 1 > 0.60). Although XLNet model cannot identify the neutralizing strategy very well, BERT and RoBERTa models can achieve an", "publication_ref": ["b18", "b59"], "figure_ref": [], "table_ref": ["tab_3", "tab_5"]}, {"heading": "Original Text", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Strategy", "text": "Positive Reframe (a) Could someone just give me like $1000? It would change my life, stressing about rent, bills and food money is just the worst.\ngrowth I need to learn how to manage my money better so that I don't have to stress about rent, bills and food money. Time to focus on making a budget, so I don't have to stress about rent, bills, and food money. Well, just think by the end of next week we'll be all done with the midterms! (c) talacre beach really need to sort there checking in routeen caravans are never ready 30mins later and still sat in the car waiting ..", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "neutralizing", "text": "The caravans at Talacre beach need to sort out their check-in process, so the kids don't get stuck in the car. Talacre beach's check in process could do with some improvement to reduce the wait time for customers.\n(d) So glad that tomorrow is Friday. This has seriously been the longest week of my life! tired optimism I'm glad that tomorrow is Friday. This week has been long, but I'm looking forward to the weekend. I'm glad the weekend is coming up, so I can rest.\n(e) Sometimes I get these impulses to just throw a tantrum. Like throw/break things, cry and scream.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "selfaffirmation", "text": "Sometimes I get these impulses to just throw a tantrum. Like throw/break things, cry and scream. But I'm strong, and I know I can handle it.\nIt's normal for to feel overwhelmed sometimes but I know I am strong to handle and go through it.\n(f) The fact that my mom's been calling me every night to calm me down from school says so much. I've needed it these past few days thankfulness I'm really lucky to have such a caring mum who is willing to call me every night when I'm having a hard time.\nMy mom has been calling me every night to calm me down from school. I've needed it these past few days. I'm thankful for her.\nTable 3: Example reframes. We examine one original text for each of the strategies in our dataset along with its BARTgenerated positive reframe (first row of each block) and its ground truth reframe (italics in second row of each block). Even when the generations differ from the ground truth, the model's reframes are largely successful at shifting the perspective while still maintaining the underlying meaning of the original text.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Strategy", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Discussion and Conclusion", "text": "This work introduces a new and challenging NLG task called positive reframing. The objective is to construct a more positive outlook as a way of rephrasing a negative source text such that the meaning of that source is preserved. Our parallel dataset, POSITIVE PSYCHOLOGY FRAMES, will serve as a benchmark that will enable sustained work on this task. We experiment with many of the leading style-transfer models and show that these models can learn to shift from a negative to a more positive perspective using a combination of strategies from positive psychology. Importantly, the best models are fluent and effective reframing systems that can learn to largely preserve the meaning of the original text, even under a perspective shift. However, these models still struggle to generate reasonable positive perspectives, and even the best models are still prone to errors. We discuss four key error classes: insubstantial changes, contradictions to the premise, self-contradictions, and hallucinations, as shown in Error Analyses in Section 5.5. Overall, this suggests that our dataset can serve as a useful benchmark for understanding well-motivated positive reframing strategies and equipping natural language generation systems with positive perspectives.\nFuture work can dive deeper into these issues by enforcing a stronger level of semantic equivalence between the generation and the source text (Nie et al., 2019). Even with semantic equivalence constraints, it would be necessary to also allow for the injection of new positive perspectives. Methods ranging from guided sequence generation (Krause et al., 2020) or semantic attention-guided decoding (Nie et al., 2019) to pragmatic reconstruction (Shen et al., 2019) and persona consistency (Kim et al., 2020) may all be applicable in follow-up studies.", "publication_ref": ["b73", "b53", "b73", "b88"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "The authors would like to thank reviewers for their helpful insights and feedback. CZ is supported by the NSF Graduate Research Fellowship under Grant No. DGE-2039655 and DY is supported by the Microsoft Research Faculty Fellowship. This work is funded in part by a grant from Amazon.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ethics", "text": "Annotation. We followed the guidelines for ethical annotation practices and crowdsourcing that are outlined in (Sheehan, 2018), including paying workers a fair wage above the federal minimum. If workers contacted us with any questions or concerns, we responded promptly to them within 24 hours. In the task interface, in the header, we warned annotators that the content might be upsetting, and we gave the following recommendation: \"if any point you do not feel comfortable, please feel free to skip the HIT or take a break.\".\nDeployment. Although this data is designed for pro-social outcomes (i.e. increasing positivity in text), there may be unexpected use-cases for this data, such as obfuscating impolite or even hateful data to avoid detection (ElSherief et al., 2021). The parallel structure of the data means it is also possible to invert the direction of the seq2seq task to introduce more negative or pessimistic perspectives into a positive source. This is not a particularly new risk, since sentiment style transfer can accomplish a similar outcome in this direction. Still, we will require interested parties to sign a data-use agreement that encourages only ethical uses of POSITIVE PSYCHOLOGY FRAMES. ", "publication_ref": ["b87", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "A Data Quality-Control Methods", "text": "We used programmatic methods to ensure highquality reframing annotations at submission time. Workers could not submit their task if the reframe:\n(1) contained fewer than 3 word types; (2) had a length less than 25% of the original text; (3) had more than 3 repetitions of a single bigram; or (4) was too similar to the original text, with a token Jaccard Similarity greater than 90%. Furthermore, we used the LanguageTool API 2 to prompt workers to fix any grammatical mistakes in their writing. Cumulatively, these heuristics greatly improved the annotation quality. Later, in the post-processing stage, we employed additional programmatic measures as well as manual quality-checks to filter out the unsatisfactory examples. This process was iterated after each batch, with a batch size of 100. First, one of the authors manually checked any sentences where annotators had scored the original text with a postivity score greater than 3 (out of 5). If that author found that the text was not negative enough or did not contain the requisite cognitive distortions to warrant a substantial reframing, the sentence was removed from the corpus. Next, we considered all neutralizing reframes with a score less than 4 (out of 5). If the text was not effectively neutralized, we removed the sentence from the corpus. Then we considered all annotations containing the first person pronoun you. If the text abandoned the author's first-person voice and shifted into a 3rd-person critique or commentary (e.g. \"I feel hopeless\" \u2192 \"you should find hope\"), then we removed this from the corpus. Finally, we grouped the annotations by Worker ID and, for each worker, scanned the top 10 annotations. If the annotator produced poor quality work, we removed the examples and blocked the worker from future tasks. After a last pass through the data to manually correct noticeable punctuation and grammar errors, we were left with our cleaned corpus of 8,349 reframed sentences.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Task Interface", "text": "Figure 2 shows the Instructions we gave to the Amazon Mechanical Turk (MTurk) workers. Figure 3 shows the examples we displayed for each reframe strategy. Figure 4 shows the MTurk HIT interface that we used for the Section 4.1 task to collect positive reframes with their associated strategies as well as the positivity scores for both the original TEXT 2 api.languagetoolplus.com/v2/check  and the REFRAME. Figure 5 shows the interface for the Section 4.2 task where we collected new strategy labels for prior annotations to compute inter-annotator agreement scores.", "publication_ref": [], "figure_ref": ["fig_3", "fig_4", "fig_6"], "table_ref": []}, {"heading": "C Few-shot Learning Setting", "text": "Following (Han et al., 2018;Baldini Soares et al., 2019) and others, we consider 5-shot learning. We ", "publication_ref": ["b37", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "D Example Reframes", "text": "In Table 5, we compare examples of modelgenerated reframes from different models. The examples are structurally and semantically diverse, which may suggest that different architectures could serve as complementary systems in a broader effort to introduce a range of positive perspectives in text. However, the generations are not perfect.\nIn this particular example, CopyNMT and GPT-2 fail to integrate the key concept of the \"heavy workload\" into their reframe generations. There is still significant room to improve upon these models in future work. We were also interested in the success of the Controlled generations p(t, \u03c8 t |s). Do models appropriately follow a given strategy when the generation is conditioned on that strategy? Table 6 shows different reconstructions of the same text using different controlling strategies. Qualitatively, we can see that controlled generation is a reasonable method for influencing the most prominent strategy. The 'thankfulness', 'growth' and 'self-affirmation' strategies are particularly distinct. Figure 5: Amazon Mechanical Turk interface used to find inter-annotator agreement for the taxonomy (in Section 4.2).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Original Text", "text": "Always stressing and thinking about loads of things at once need I take it one at a time overload stressed need to rant.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ground Truth", "text": "Loads of things on my mind, I need to make a list, prioritise and work through it all calmly and I will feel much better.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Model Generation", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Random", "text": "Annoyed because the USPS lost 1 of the 8 flat rate boxes of books I shipped from California to Kentucky. Hope it shows up :-/", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "SBERT", "text": "Really stressed at the moment but I know that this will pass and I'll get everything sorted out.\nSeq2seq-LSTM i have a lot of things to do it is a good night's sleep and i can get through it.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "CopyNMT", "text": "Always stressing and thinking easy of things of things at things at all, but I know that I need it.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "GPT", "text": "i ' m so relieved that i ' m going to get to organize my thoughts in such a short time ! i hope i am done with the list of things i have to do today , so that i can go back to bed . GPT-2 No-pretrain I'm going to try to seek a new job to get some newspect me through today.\nGPT-2 I should be more energetic in my thoughts, and not worry too much.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "T5", "text": "I am stressed and thinking about loads of things at once, I need to take it one at a time stressed need to rant. I'm sure I'll be able to focus on it.\nBART I am stressed and thinking about loads of things at once, but I am going to take it one at a time. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "GPT-2 Generations", "text": "Growth Mindset I'm doing really well in some class and bad in other classes I hate this. But I'm doing better than I was at the age of 11.\nImpermanence I'm having a hard time in some classes and I hope to get through the rest of the semester.\nNeutralizing I'm doing really well in some class and bad in other classes I hate this. But I'm going to go clear it out.\nOptimism I need to take a good hard look at myself and realize that I am quite capable of doing better in other classes.\nSelf-affirmation I'm really happy about my class, I'm doing really well in some class, bad or not, I can do better in another.\nThankfulness I'm so lucky that I'm able to get my classes to a good start and get off the couch.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "T5 Generations", "text": "Growth Mindset I'm doing really well in some class and bad in other classes I hate this, but I'll be able to do it again.\nImpermanence I'm doing really well in some class and bad in other classes I hate this. But I'll be able to do my best in the future.\nNeutralizing I'm doing really well in some class and bad in other classes I hate this. But I can't wait to see if I can do it.\nOptimism I'm doing really well in some class and bad in other classes I hate this. I hope I'll be able to do better in the future.\nSelf-affirmation I'm doing really well in some class and bad in other classes I hate this. It's a good thing to do, but I'll be able to do it.\nThankfulness I'm doing really well in some class and bad in other classes I hate this. But I can't wait to see if I can do it.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "BART Generations", "text": "Growth Mindset I'm doing really well in some class and bad in other classes. I'm going to try to improve my grades.\nImpermanence I'm doing really well in some class and bad in other classes, but I'm sure it will all work out in the end.\nNeutralizing I'm doing really well in some class and bad in other classes I don't like this.\nOptimism I'm doing really well in some class and bad in other classes, but I'm sure it will all work out.\nSelf-affirmation I'm doing really well in some class and bad in other classes, but I know I can do better.\nThankfulness I'm doing really well in some class and bad in other classes, but I'm thankful that I have the opportunity to study.\nTable 6: A model comparison for reframing the same text using different controlling strategy. Here, we observe models can learn some information from the input strategy label and make distinctive generations, especially for the 'thankfulness', 'growth' and 'self-affirmation' strategies.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Cognitive vulnerability-stress models of depression in a self-regulatory and psychobiological context", "journal": "", "year": "2002", "authors": "Lauren B Lyn Y Abramson;  Alloy; L Benjamin; Gerald J Hankin; Donal G Haeffel; Brandon E Maccoon;  Gibb"}, {"ref_id": "b1", "title": "Unsupervised statistical machine translation", "journal": "", "year": "2018", "authors": "Mikel Artetxe; Gorka Labaka; Eneko Agirre"}, {"ref_id": "b2", "title": "Matching the blanks: Distributional similarity for relation learning", "journal": "Association for Computational Linguistics", "year": "2019", "authors": " Livio Baldini; Nicholas Soares; Jeffrey Fitzgerald; Tom Ling;  Kwiatkowski"}, {"ref_id": "b3", "title": "The ripple effect: Emotional contagion and its influence on group behavior. Administrative science quarterly", "journal": "", "year": "2002", "authors": "G Sigal;  Barsade"}, {"ref_id": "b4", "title": "GPT-Neo: Large scale autoregressive language modeling with meshtensorflow", "journal": "", "year": "2021", "authors": "Sid Black; Leo Gao; Phil Wang; Connor Leahy; Stella Biderman"}, {"ref_id": "b5", "title": "Implicit theories of intelligence predict achievement across an adolescent transition: A longitudinal study and an intervention", "journal": "Child development", "year": "2007", "authors": "S Lisa; Kali H Blackwell; Carol Sorich Trzesniewski;  Dweck"}, {"ref_id": "b6", "title": "Mc-Candlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners", "journal": "", "year": "", "authors": "Tom B Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell; Sandhini Agarwal; Ariel Herbert-Voss; Gretchen Krueger; Tom Henighan; Rewon Child; Aditya Ramesh; Daniel M Ziegler; Jeffrey Wu; Clemens Winter; Christopher Hesse; Mark Chen; Eric Sigler; Mateusz Litwin"}, {"ref_id": "b7", "title": "Buffering against weight gain following dieting setbacks: An implicit theory intervention", "journal": "Journal of Experimental Social Psychology", "year": "2012", "authors": "L Jeni; Eli J Burnette;  Finkel"}, {"ref_id": "b8", "title": "Feeling good", "journal": "", "year": "1981", "authors": "D David;  Burns"}, {"ref_id": "b9", "title": "How coping mediates the effect of optimism on distress: a study of women with early stage breast cancer", "journal": "", "year": "1999", "authors": "S Charles; Christina Carver;  Pozo; D Suzanne; Victoria Harris;  Noriega; F Michael;  Scheier; S David; Alfred S Robinson; Frederick L Moffat Ketcham; Kimberley C Jr;  Clark"}, {"ref_id": "b10", "title": "", "journal": "Optimism. Clinical psychology review", "year": "2010", "authors": "S Charles;  Carver; F Michael; Suzanne C Scheier;  Segerstrom"}, {"ref_id": "b11", "title": "ENTRUST: Argument reframing with language models and entailment", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Tuhin Chakrabarty; Christopher Hidey; Smaranda Muresan"}, {"ref_id": "b12", "title": "Reducing the racial achievement gap: A social-psychological intervention", "journal": "science", "year": "2006", "authors": "Julio Geoffrey L Cohen; Nancy Garcia; Allison Apfel;  Master"}, {"ref_id": "b13", "title": "The psychology of change: Self-affirmation and social psychological intervention. Annual review of psychology", "journal": "", "year": "2014", "authors": "L Geoffrey; David K Cohen;  Sherman"}, {"ref_id": "b14", "title": "Detecting emotional contagion in massive social networks", "journal": "PloS one", "year": "2014", "authors": "Lorenzo Coviello; Yunkyu Sohn; D I Adam; Cameron Kramer; Massimo Marlow;  Franceschetti; A Nicholas; James H Christakis;  Fowler"}, {"ref_id": "b15", "title": "Rethinking stress: the role of mindsets in determining the stress response", "journal": "Journal of personality and social psychology", "year": "2013", "authors": "Alia J Crum; Peter Salovey; Shawn Achor"}, {"ref_id": "b16", "title": "Automated hate speech detection and the problem of offensive language", "journal": "", "year": "2017", "authors": "Thomas Davidson; Dana Warmsley; Michael Macy; Ingmar Weber"}, {"ref_id": "b17", "title": "Evaluating neural text simplification in the medical domain", "journal": "ACM", "year": "2019-05-13", "authors": "Laurens Van Den Bercken; Robert-Jan Sips; Christoph Lofi"}, {"ref_id": "b18", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Long and Short Papers", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b19", "title": "Service climate effects on customer attitudes: An examination of boundary conditions. Academy of management journal", "journal": "", "year": "2004", "authors": "Joerg Dietz; Douglas Pugh; Jack W Wiley"}, {"ref_id": "b20", "title": "Using credible advice to overcome framing effects", "journal": "Journal of Law, Economics, and Organization", "year": "2001", "authors": " James N Druckman"}, {"ref_id": "b21", "title": "What having a \"growth mindset\" actually means", "journal": "Harvard Business Review", "year": "2016", "authors": "Carol Dweck"}, {"ref_id": "b22", "title": "Mindsets: A view from two eras", "journal": "Perspectives on Psychological science", "year": "2019", "authors": "S Carol; David S Dweck;  Yeager"}, {"ref_id": "b23", "title": "The duchenne smile: emotional expression and brain physiology: Ii", "journal": "", "year": "1990", "authors": "Paul Ekman; J Richard; Wallace V Davidson;  Friesen"}, {"ref_id": "b24", "title": "Latent hatred: A benchmark for understanding implicit hate speech", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Mai Elsherief; Caleb Ziems; David Muchlinski; Vaishnavi Anupindi; Jordyn Seybolt; Diyi Munmun De Choudhury;  Yang"}, {"ref_id": "b25", "title": "Gratitude and the science of positive psychology. Handbook of positive psychology", "journal": "", "year": "2002", "authors": "A Robert; Charles M Emmons;  Shelton"}, {"ref_id": "b26", "title": "Helping clients forgive: An empirical guide for resolving anger and restoring hope", "journal": "American Psychological Association", "year": "2000", "authors": "D Robert; Richard P Enright;  Fitzgibbons"}, {"ref_id": "b27", "title": "Affective contagion in effortful political thinking", "journal": "Political Psychology", "year": "2014", "authors": "Cengiz Erisen; Milton Lodge; Charles S Taber"}, {"ref_id": "b28", "title": "Understanding emotional transitions: the interpersonal consequences of changing emotions in negotiations", "journal": "Journal of personality and social psychology", "year": "2011", "authors": "Allan Filipowicz; Sigal Barsade; Shimul Melwani"}, {"ref_id": "b29", "title": "Extracting meaning from past affective experiences: The importance of peaks, ends, and specific emotions", "journal": "Cognition & Emotion", "year": "2000", "authors": "L Barbara;  Fredrickson"}, {"ref_id": "b30", "title": "Positive affect and the complex dynamics of human flourishing", "journal": "American psychologist", "year": "2005", "authors": "L Barbara;  Fredrickson;  Losada"}, {"ref_id": "b31", "title": "Style transfer in text: Exploration and evaluation", "journal": "AAAI Press", "year": "2018-02-02", "authors": "Zhenxin Fu; Xiaoye Tan; Nanyun Peng; Dongyan Zhao; Rui Yan"}, {"ref_id": "b32", "title": "Are experimental economists prone to framing effects? a natural field experiment", "journal": "Journal of Economic Behavior & Organization", "year": "2009", "authors": "Simon G\u00e4chter; Henrik Orzen; Elke Renner; Chris Starmer"}, {"ref_id": "b33", "title": "Improving adolescents' standardized test performance: An intervention to reduce the effects of stereotype threat", "journal": "Journal of Applied Developmental Psychology", "year": "2003", "authors": "Catherine Good; Joshua Aronson; Michael Inzlicht"}, {"ref_id": "b34", "title": "Logic and conversation", "journal": "Brill", "year": "1975", "authors": "P Herbert;  Grice"}, {"ref_id": "b35", "title": "@ spam: the underground on 140 characters or less", "journal": "", "year": "2010", "authors": "Chris Grier; Kurt Thomas; Vern Paxson; Michael Zhang"}, {"ref_id": "b36", "title": "Handbook of emotion regulation", "journal": "Guilford publications", "year": "2013", "authors": "J James;  Gross"}, {"ref_id": "b37", "title": "FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation", "journal": "", "year": "2018", "authors": "Xu Han; Hao Zhu; Pengfei Yu; Ziyun Wang; Yuan Yao; Zhiyuan Liu; Maosong Sun"}, {"ref_id": "b38", "title": "Integrating positive psychology into counseling: Why and (when appropriate) how", "journal": "Journal of Counseling & Development", "year": "2007", "authors": "Alex Hs Harris; Carl E Thoresen; Shane J Lopez"}, {"ref_id": "b39", "title": "Emotional contagion. Current directions in psychological science", "journal": "", "year": "1993", "authors": "Elaine Hatfield; T John; Richard L Cacioppo;  Rapson"}, {"ref_id": "b40", "title": "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering", "journal": "ACM", "year": "2016-04-11", "authors": "Ruining He; Julian J Mcauley"}, {"ref_id": "b41", "title": "Long short-term memory", "journal": "Neural computation", "year": "1997", "authors": "Sepp Hochreiter; J\u00fcrgen Schmidhuber"}, {"ref_id": "b42", "title": "Generating natural language under pragmatic constraints", "journal": "Journal of Pragmatics", "year": "1987", "authors": "Eduard Hovy"}, {"ref_id": "b43", "title": "Toward controlled generation of text", "journal": "PMLR", "year": "2017-08-11", "authors": "Zhiting Hu; Zichao Yang; Xiaodan Liang; Ruslan Salakhutdinov; Eric P Xing"}, {"ref_id": "b44", "title": "Cycleconsistent adversarial autoencoders for unsupervised text style transfer", "journal": "", "year": "2020", "authors": "Yufang Huang; Wentao Zhu; Deyi Xiong; Yiye Zhang; Changjian Hu; Feiyu Xu"}, {"ref_id": "b45", "title": "Shakespearizing modern language using copy-enriched sequence to sequence models", "journal": "", "year": "2017", "authors": "Harsh Jhamtani; Varun Gangal; Eduard Hovy; Eric Nyberg"}, {"ref_id": "b46", "title": "Deep learning for text style transfer: A survey", "journal": "", "year": "2020", "authors": "Di Jin; Zhijing Jin; Zhiting Hu; Olga Vechtomova; Rada Mihalcea"}, {"ref_id": "b47", "title": "IMaT: Unsupervised text attribute transfer via iterative matching and translation", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Zhijing Jin; Di Jin; Jonas Mueller; Nicholas Matthews; Enrico Santus"}, {"ref_id": "b48", "title": "Implicit theories about willpower predict selfregulation and grades in everyday life", "journal": "Journal of personality and social psychology", "year": "2015", "authors": "V Job; G Walton; K Bernecker; C Dweck"}, {"ref_id": "b49", "title": "Ego depletionis it all in your head? implicit theories about willpower affect self-regulation", "journal": "Psychological science", "year": "2010", "authors": "Veronika Job; Gregory M Dweck;  Walton"}, {"ref_id": "b50", "title": "Do you feel what i feel? mood contagion and leadership outcomes", "journal": "The Leadership Quarterly", "year": "2009", "authors": "K Stefanie;  Johnson"}, {"ref_id": "b51", "title": "2020. Will I sound like me? improving persona consistency in dialogues through pragmatic selfconsciousness", "journal": "", "year": "", "authors": "Hyunwoo Kim; Byeongchang Kim; Gunhee Kim"}, {"ref_id": "b52", "title": "Grammatical error correction and style transfer via zero-shot monolingual translation", "journal": "", "year": "2019", "authors": "Elizaveta Korotkova; Agnes Luhtaru; Maksym Del; Krista Liin; Daiga Deksne; Mark Fishel"}, {"ref_id": "b53", "title": "Gedi: Generative discriminator guided sequence generation", "journal": "", "year": "2009", "authors": "Ben Krause; Akhilesh Deepak Gotmare; Bryan Mc-Cann; Nitish Shirish Keskar; Shafiq Joty; Richard Socher; Nazneen Fatema Rajani"}, {"ref_id": "b54", "title": "Gratitude and depressive symptoms: The role of positive reframing and positive emotion", "journal": "Cognition & emotion", "year": "2012", "authors": "M Nathaniel;  Lambert; D Frank; Tyler F Fincham;  Stillman"}, {"ref_id": "b55", "title": "BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension", "journal": "", "year": "2020", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal ; Abdelrahman Mohamed; Omer Levy; Veselin Stoyanov; Luke Zettlemoyer"}, {"ref_id": "b56", "title": "Domain adaptive text style transfer", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Dianqi Li; Yizhe Zhang; Zhe Gan; Yu Cheng; Chris Brockett; Bill Dolan; Ming-Ting Sun"}, {"ref_id": "b57", "title": "Delete, retrieve, generate: a simple approach to sentiment and style transfer", "journal": "Long Papers", "year": "2018", "authors": "Juncen Li; Robin Jia; He He; Percy Liang"}, {"ref_id": "b58", "title": "ROUGE: A package for automatic evaluation of summaries", "journal": "Association for Computational Linguistics", "year": "2004", "authors": "Chin-Yew Lin"}, {"ref_id": "b59", "title": "Roberta: A robustly optimized bert pretraining approach", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b60", "title": "The perils of double consciousness: The role of thought suppression in stereotype threat", "journal": "Journal of Experimental Social Psychology", "year": "2009", "authors": "Christine Logel; C Emma;  Iserman; G Paul; Diane M Davies; Steven J Quinn;  Spencer"}, {"ref_id": "b61", "title": "Content preserving text generation with attribute controls", "journal": "", "year": "2018-12-03", "authors": "Lajanugen Logeswaran; Honglak Lee; Samy Bengio"}, {"ref_id": "b62", "title": "", "journal": "", "year": "2018", "authors": "Steven Loria"}, {"ref_id": "b63", "title": "Towards fine-grained text sentiment transfer", "journal": "", "year": "2019", "authors": "Fuli Luo; Peng Li; Pengcheng Yang; Jie Zhou; Yutong Tan; Baobao Chang; Zhifang Sui; Xu Sun"}, {"ref_id": "b64", "title": "PowerTransformer: Unsupervised controllable revision for biased language correction", "journal": "", "year": "2020", "authors": "Xinyao Ma; Maarten Sap; Hannah Rashkin; Yejin Choi"}, {"ref_id": "b65", "title": "Politeness transfer: A tag and generate approach", "journal": "", "year": "2020", "authors": "Aman Madaan; Amrith Setlur; Tanmay Parekh; Barnabas Poczos; Graham Neubig; Yiming Yang; Ruslan Salakhutdinov; Alan W Black; Shrimai Prabhumoye"}, {"ref_id": "b66", "title": "Unsupervised text style transfer with padded masked language models", "journal": "", "year": "2020", "authors": "Eric Malmi; Aliaksei Severyn; Sascha Rothe"}, {"ref_id": "b67", "title": "Combating stereotype threat: The effect of self-affirmation on women's intellectual performance", "journal": "Journal of Experimental Social Psychology", "year": "2006", "authors": "Andy Martens; Michael Johns; Jeff Greenberg; Jeff Schimel"}, {"ref_id": "b68", "title": "The use of positive reframing in the context of supervision", "journal": "Journal of Counseling & Development", "year": "1992", "authors": "A Mark;  Masters"}, {"ref_id": "b69", "title": "A computational theory of prose style for natural language generation", "journal": "Association for Computational Linguistics", "year": "1985", "authors": "D David; James D Mcdonald;  Pustejovsky"}, {"ref_id": "b70", "title": "Evaluating style transfer for text", "journal": "Long and Short Papers", "year": "2019", "authors": "Remi Mir; Bjarke Felbo; Nick Obradovich; Iyad Rahwan"}, {"ref_id": "b71", "title": "Cognitive distortions and irrational beliefs in posttraumatic stress, anxiety, and depressive disorders", "journal": "Journal of Clinical Psychology", "year": "1993", "authors": "M Elizabeth; Robert W Muran;  Motta"}, {"ref_id": "b72", "title": "Depressed individuals use negative self-focused language when recalling recent interactions with close romantic partners but not family or Friends", "journal": "", "year": "2019", "authors": "Taleen Nalabandian; Molly Ireland"}, {"ref_id": "b73", "title": "A simple recipe towards reducing hallucination in neural surface realisation", "journal": "", "year": "2019", "authors": "Feng Nie; Jin-Ge Yao; Jinpeng Wang; Rong Pan; Chin-Yew Lin"}, {"ref_id": "b74", "title": "Multi-task neural models for translating between styles within and across languages", "journal": "", "year": "2018", "authors": "Xing Niu; Sudha Rao; Marine Carpuat"}, {"ref_id": "b75", "title": "An all-or-nothing thinking turns into darkness: Relations between dichotomous thinking and personality disorders 1", "journal": "Japanese Psychological Research", "year": "2012", "authors": "Atsushi Oshio"}, {"ref_id": "b76", "title": "Bleu: a method for automatic evaluation of machine translation", "journal": "Association for Computational Linguistics", "year": "2002", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"ref_id": "b77", "title": "Style transfer through back-translation", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Yulia Shrimai Prabhumoye; Ruslan Tsvetkov; Alan W Salakhutdinov;  Black"}, {"ref_id": "b78", "title": "Automatically neutralizing subjective bias in text", "journal": "", "year": "2020", "authors": "Reid Pryzant; Richard Diehl Martinez; Nathan Dass; Sadao Kurohashi; Dan Jurafsky; Diyi Yang"}, {"ref_id": "b79", "title": "Language models are unsupervised multitask learners", "journal": "OpenAI blog", "year": "2019", "authors": "Alec Radford; Jeffrey Wu; Rewon Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"ref_id": "b80", "title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "Journal of Machine Learning Research", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b81", "title": "Dear sir or madam, may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer", "journal": "Long Papers", "year": "2018", "authors": "Sudha Rao; Joel Tetreault"}, {"ref_id": "b82", "title": "Sentence-BERT: Sentence embeddings using Siamese BERTnetworks", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Nils Reimers; Iryna Gurevych"}, {"ref_id": "b83", "title": "Optimism, pessimism, and psychological well-being", "journal": "", "year": "2001", "authors": "F Michael; Charles S Scheier; Michael W Carver;  Bridges"}, {"ref_id": "b84", "title": "I think therefore i om: Cognitive distortions and coping style as mediators for the effects of mindfulness meditation on anxiety, positive and negative affect, and hope", "journal": "Journal of clinical psychology", "year": "2009", "authors": "Sharon Sears; Sue Kraus"}, {"ref_id": "b85", "title": "Get to the point: Summarization with pointergenerator networks", "journal": "Long Papers", "year": "2017", "authors": "Abigail See; J Peter; Christopher D Liu;  Manning"}, {"ref_id": "b86", "title": "Semi-supervised text style transfer: Cross projection in latent space", "journal": "", "year": "2019", "authors": "Mingyue Shang; Piji Li; Zhenxin Fu; Lidong Bing; Dongyan Zhao; Shuming Shi; Rui Yan"}, {"ref_id": "b87", "title": "Crowdsourcing research: data collection with amazon's mechanical turk", "journal": "Communication Monographs", "year": "2018", "authors": "Bartel Kim;  Sheehan"}, {"ref_id": "b88", "title": "Pragmatically informative text generation", "journal": "Long and Short Papers", "year": "2019", "authors": "Sheng Shen; Daniel Fried; Jacob Andreas; Dan Klein"}, {"ref_id": "b89", "title": "Style transfer from non-parallel text by cross-alignment", "journal": "", "year": "2017", "authors": "Tianxiao Shen; Tao Lei; Regina Barzilay; Tommi S Jaakkola"}, {"ref_id": "b90", "title": "Affirmed yet unaware: exploring the role of awareness in the process of self-affirmation", "journal": "", "year": "2009", "authors": "Geoffrey L David K Sherman; Leif D Cohen;  Nelson; Debra P David Nussbaum; Julio Bunyan;  Garcia"}, {"ref_id": "b91", "title": "Automatic detection and classification of cognitive distortions in mental health text", "journal": "IEEE", "year": "2020", "authors": "Benjamin Shickel; Scott Siegel; Martin Heesacker; Sherry Benton; Parisa Rashidi"}, {"ref_id": "b92", "title": "Self-affirmation as a deliberate coping strategy: The moderating role of choice", "journal": "Journal of Experimental Social Psychology", "year": "2013", "authors": "Arielle Silverman; Christine Logel; Geoffrey L Cohen"}, {"ref_id": "b93", "title": "Detecting cognitive distortions through machine learning text analytics", "journal": "IEEE", "year": "2017", "authors": "Taetem Simms; Clayton Ramstedt; Megan Rich; Michael Richards; T Martinez; C Giraud-Carrier"}, {"ref_id": "b94", "title": "The psychology of selfaffirmation: Sustaining the integrity of the self", "journal": "Elsevier", "year": "1988", "authors": "M Claude;  Steele"}, {"ref_id": "b95", "title": "transforming\" delete, retrieve, generate approach for controlled text style transfer", "journal": "", "year": "2019", "authors": "Akhilesh Sudhakar; Bhargav Upadhyay; Arjun Maheswaran"}, {"ref_id": "b96", "title": "Catastrophizing, depression and expectancies for pain and emotional distress", "journal": "Pain", "year": "2001", "authors": "J L Michael; Wendy M Sullivan; Irving Rodgers;  Kirsch"}, {"ref_id": "b97", "title": "Contagious leaders and followers: Exploring multi-stage mood contagion in a leader activation and member propagation (lamp) model", "journal": "Organizational Behavior and Human Decision Processes", "year": "2013", "authors": "Thomas Sy; Jin Nam Choi"}, {"ref_id": "b98", "title": "The contagious leader: impact of the leader's mood on the mood of group members, group affective tone, and group processes", "journal": "Journal of applied psychology", "year": "2005", "authors": "Thomas Sy; St\u00e9phane C\u00f4t\u00e9; Richard Saavedra"}, {"ref_id": "b99", "title": "bad\" things reconsidered", "journal": "", "year": "2020", "authors": "M Gregory; Shannon T Walton;  Brady"}, {"ref_id": "b100", "title": "Text simplification using neural machine translation", "journal": "AAAI Press", "year": "2016-02-12", "authors": "Tong Wang; Ping Chen; John Rochford; Jipeng Qiang"}, {"ref_id": "b101", "title": "Taking care of business? grateful processing of unpleasant memories", "journal": "The Journal of Positive Psychology", "year": "2008", "authors": "C Philip; Lilia Watkins;  Cruz"}, {"ref_id": "b102", "title": "Sentence simplification by monolingual machine translation", "journal": "Long Papers", "year": "2012", "authors": " Sander Wubben;  Van Den; Emiel Bosch;  Krahmer"}, {"ref_id": "b103", "title": "Optimizing statistical machine translation for text simplification", "journal": "Transactions of the Association for Computational Linguistics", "year": "2016", "authors": "Wei Xu; Courtney Napoles; Ellie Pavlick; Quanze Chen; Chris Callison-Burch"}, {"ref_id": "b104", "title": "Paraphrasing for style", "journal": "", "year": "2012", "authors": "Wei Xu; Alan Ritter; Bill Dolan; Ralph Grishman; Colin Cherry"}, {"ref_id": "b105", "title": "Xlnet: Generalized autoregressive pretraining for language understanding", "journal": "", "year": "2019-12-08", "authors": "Zhilin Yang; Zihang Dai; Yiming Yang; Jaime G Carbonell; Ruslan Salakhutdinov; V Quoc;  Le"}, {"ref_id": "b106", "title": "The far-reaching effects of believing people can change: implicit theories of personality shape stress, health, and achievement during adolescence", "journal": "Journal of personality and social psychology", "year": "2014", "authors": "David Scott Yeager; Rebecca Johnson; Brian James Spitzer; Kali H Trzesniewski; Joseph Powers; Carol S Dweck"}, {"ref_id": "b107", "title": "Bertscore: Evaluating text generation with BERT", "journal": "", "year": "2020-04-26", "authors": "Tianyi Zhang; Varsha Kishore; Felix Wu; Kilian Q Weinberger; Yoav Artzi"}, {"ref_id": "b108", "title": "Detecting spam and promoting campaigns in the twitter social network", "journal": "IEEE", "year": "2012", "authors": "Xianchao Zhang; Shaoping Zhu; Wenxin Liang"}, {"ref_id": "b109", "title": "Sentence simplification with deep reinforcement learning", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Xingxing Zhang; Mirella Lapata"}, {"ref_id": "b110", "title": "Parallel data augmentation for formality style transfer", "journal": "", "year": "2020", "authors": "Yi Zhang; Tao Ge; Xu Sun"}, {"ref_id": "b111", "title": "Adversarially regularized autoencoders", "journal": "PMLR", "year": "2018-07-10", "authors": "Jake Junbo; Yoon Zhao; Kelly Kim; Alexander M Zhang; Yann Rush;  Lecun"}, {"ref_id": "b112", "title": "A monolingual tree-based translation model for sentence simplification", "journal": "", "year": "2010", "authors": "Zhemin Zhu; Delphine Bernhard; Iryna Gurevych"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "allow for a change Equal contribution.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Positive reframing vs. negative-to-positive sentiment style transfer.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "(b) I just went back to school today And I'm already stressed cause we have MIDTERMS NEXT WEEK !!! AND THIS WEEKEND -JAM PACKED :-( impermanence I just went back to school today and I'm already stressed because we have midterms next week and this weekend -JAM PACKED", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 2 :2Figure 2: Instructions for the Positive Reframing HIT.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 3 :3Figure 3: Example reframes.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "pull 5 representative exemplars from the training set to indicate a range of strategies: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . NEGATIVE: \"I have a huge project due tomorrow morning. But where do I have to be, a stupid basketball game dumb\" POSITIVE: \"I should plan ahead next time so that my basketball game does not conflict too closely with my projects.\" . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . NEGATIVE: \"This has been like the worst week ever im so done with everything. sick tired\" POSITIVE: \"I made it to the end of the most challenging week ever!\" . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . NEGATIVE: \"Ugh my mac is starting to slow up and I need to figure out how to defragment the hard drive...\" POSITIVE: \"I need to defragment the hard drive to speed up my mac. Good thing I'm smart, and I know I can do this.\" . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . NEGATIVE: \"I am SO stressed with all my exams and my lit review hanging over my head this week.\" POSITIVE: \"Only one more week until my exams and lit review are all done!\" . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . NEGATIVE: \"I am the only person I know who writes a healthy grocery list and plans meals when I am stressed:( CantSleep\" POSITIVE: \"I'm so thankful that I am still able to eat healthy even when I'm stressed.\" . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 4 :4Figure 4: Amazon Mechanical Turk interface used to collect positive reframes (in Section 4.1).", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": ": Strategy classification F1 scoresF1 score of around 0.6. The impermanence andself-affirmation strategies appear more challengingfor all three models to identify. Overall, the resultshere show that this task is tractable: reframe strate-gies are learnable by various classification models.This further supports the reliability of our Posi-tive Psychology framework, confirming what wefound with human reliability metrics in Section 4.2.Although we mainly treat this frame strategy clas-sification as a robustness check and deep dive intothe role of framing strategies, this task can also bea novel NLP or computational social science ap-plication on its own, i.e., determining the positivereframing relation between a pair of sentences."}], "formulas": [{"formula_id": "formula_0", "formula_text": "1 N N i=0 log p(g i |g 0:i\u22121 ) over the string g = {s, \u03c8 t , t} = {<BOS>, s 1 , s 2 , ..., s n , <STRG>, \u03c8 grow , \u03c8 imp , ..., \u03c8 thank , <REFR>, t 1 , t 2 , ..., t m , <EOS>}", "formula_coordinates": [5.0, 131.57, 74.7, 369.96, 702.27]}, {"formula_id": "formula_1", "formula_text": "-1 (R-1), ROUGE-1 (R-2), ROUGE-L (R-L),", "formula_coordinates": [6.0, 356.18, 371.53, 169.37, 7.77]}], "doi": "10.18653/v1/D18-1399"}