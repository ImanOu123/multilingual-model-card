{"title": "Partial Optimality by Pruning for MAP-Inference with General Graphical Models", "authors": "Paul Swoboda; Alexander Shekhovtsov; Hendrik J\u00f6rg; Christoph Kappes; Bogdan Schn\u00f6rr;  Savchynskyy", "pub_date": "2015-08-18", "abstract": "We consider the energy minimization problem for undirected graphical models, also known as MAP-inference problem for Markov random fields which is NP-hard in general. We propose a novel polynomial time algorithm to obtain a part of its optimal non-relaxed integral solution. Our algorithm is initialized with variables taking integral values in the solution of a convex relaxation of the MAP-inference problem and iteratively prunes those, which do not satisfy our criterion for partial optimality. We show that our pruning strategy is in a certain sense theoretically optimal. Also empirically our method outperforms previous approaches in terms of the number of persistently labelled variables. The method is very general, as it is applicable to models with arbitrary factors of an arbitrary order and can employ any solver for the considered relaxed problem. Our method's runtime is determined by the runtime of the convex relaxation solver for the MAP-inference problem.", "sections": [{"heading": "Introduction", "text": "Finding the most likely configuration of a Markov random field (MRF), also called MAP-inference or energy minimization problem for graphical models, is of big importance in computer vision, bioinformatics, communication theory, statistical physics, combinatorial optimization, signal processing, information retrieval and statistical machine learning, see [1,14,43] for an overview of applications. This key problem however is NP-hard. Therefore approximate methods have been developed to tackle big instances commonly arising in image processing, see [14,41] for an overview of such methods. These approximate methods often cannot find an optimal configuration, but deliver close solutions. If one could prove, that some variables of the solution given by such approximate algorithms belong to an optimal configuration, the value of such approximate methods would be greatly enhanced. In particular, the problem for the remaining variables could be solved by stronger, but computationally more expensive methods to obtain a global optimum as done e.g. in [16].\nIn this paper we propose a way to gain such a partially optimal solution for the MAP-inference problem with general discrete MRFs from possibly also non-exact solutions of the commonly used local polytope relaxation (see [44]). Solving over the local polytope amounts to solving a linear problem for which any linear programming (LP) solver can be used and for which dedicated and efficient algorithms exist.", "publication_ref": ["b0", "b13", "b43", "b13", "b41", "b15", "b44"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "We distinguish two classes of approaches to partial optimality.\n(i) Roof duality based approaches. The earliest paper dealing with persistency is [24], which states a persistency criterion for the stable set problem and verifies it for every solution of a certain relaxation. This relaxation is the same, as used by the roof duality method in [2] and which is also the basis for the well known QPBOalgorithm [2,25]. The MQPBO method [18] extends roof duality to the multi-label case. The authors transform multi-label problems into quadratic binary ones and solve them via QPBO [2]. However, their transformation is dependent upon choosing a label order and their results are so as well, see the experiments in [39], where the label order is sampled randomly. It is not known how to choose an optimal label order to obtain the maximum number of persistent variables.\nThe roof duality method has been extended to higher order binary problems in [4,11,13,20]. The generalized roof duality method for binary higher order problems [13,20] computes partially optimal variables directly for higher order potentials, while Ishikawa's and Fix et al's approaches [4,11] transform the higher order problem to one with unary and pairwise terms only. Fix et al's method [4] is an improvement upon Ishikawa's [11].\nWindheuser et al [45] proposed a multi-label higherorder roof duality method, which is a generalization of both MQPBO [18] to higher order and Kahl and Strandmark's work [13] to the multi-label case. However Windheuser et al neither describe an implementation nor pro-vide experimental validation for the higher order multilabel case.\n(ii) Labeling testing approaches. A different approach, specialized for Potts models, is pursued by Kovtun [22], where possible labelings are tested for persistency by auxiliary submodular problems. The parametric max-flow method for the Potts model by Gridchin and Kolmogorov [6] reduces the number of max-flow computations to compute the persistencies of Kovtun's method [22] to log(K), where K is the number of labels. The dead-end elimination procedure [3] tests, if certain labels of nodes cannot belong to an optimal solution. It is a local heuristic and does not perform any optimization.\nSince for non-binary multi-labeling problems the submodular approximations constructed by approaches of class (i) are provably less tight than the standard local polytope relaxation [34,Prop. 1], we consider class (ii) in this paper. Specifically, based on ideas in [39] to handle the Potts model, we develop a theoretically substantiated approach to recognizing partial optimality for general graphical models, together with a competitive comparison to the 5 approaches [4, 11,13,18,22] discussed above, that define the state-of-the-art. Unified study. In addition we point to the recent paper [32], which provides a unified study of most mentioned methods and a systematic way of their analysis. While their persistency criterion is provably not weaker than ours, due to the general structure of the resulting LP it cannot be applied to large-scale problems in a straightforward manner. Moreover, our approach is directly applicable to higher order models and tighter then the local polytope relaxations, whereas [32] requires generalization to these cases, though such a generalization is presumably possible. We show that our algorithm solves a special case of the maximal presistency problem formulated in [32]. Shrinking technique. The recent work [27] proposes a method for efficient shrinking of the combinatorial search area with the local polytope relaxation. Though the algorithmic idea is similar to the presented one, the method [27] does not provide partially optimal solutions. We refer to Section 4 for further discussion.\nFurthermore, preliminary shorter version of the our study was published at a conference as [40].", "publication_ref": ["b1", "b1", "b25", "b18", "b1", "b39", "b10", "b12", "b20", "b12", "b20", "b10", "b10", "b45", "b18", "b12", "b5", "b2", "b34", "b39", "b10", "b12", "b18", "b27", "b27", "b40"], "figure_ref": [], "table_ref": []}, {"heading": "Contribution and Organization", "text": "Adopting ideas from [39], we propose a novel method for computing partial optimality, which is applicable to general graphical models with arbitrary higher order potentials. Similarly to [39] our algorithm is initialized with variables taking integral values in the solution of a convex relaxation of the MAP-inference problem and itera-tively prunes those, which do not satisfy our persistency criterion. We show that our pruning strategy is in a certain sense theoretically optimal. Though the used relaxation can be chosen arbitrarily, for brevity we restrict our exposition and experiments to the local polytope relaxation. Tighter relaxations provably yield better results. However even by using the local polytope relaxation we can often achieve a substantially higher number of persistent variables, than competing approaches, which we confirm experimentally. We also show how our approach can be made invariant against reparametrizations. This improves our partial optimality criterion and we can show equivalence with the all-to-one improving mapping class of partial optimality methods proposed in [32]. Our approach is very general, as it can use any, also approximate, solver for the considered convex relaxation. Moreover, the computational complexity of our method is determined mainly by the runtime of the used solver.\nThe comparison to existing persistency methods is summarized in Table 1.\nOur code together with the experimental setup is available at http://paulswoboda.net. Organization. In Section 2 we review the energy minimization problem and the local polytope relaxation, in Section 3 our persistency criterion is presented. The corresponding algorithm and its theoretical analysis are presented in Sections 4, 5 and 6 respectively. Extensions to the higher order case and tighter relaxations are discussed in Section 8. Section 9 provides experimental validation of our approach and a comparison to the existing methods [4,11,13,18,22].", "publication_ref": ["b39", "b39", "b10", "b12", "b18"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "MAP-Inference Problem", "text": "The MAP-inference problem for a graphical model over an undirected graph G = (V, E), reads min\nx\u2208X V E V (x) := v\u2208V \u03b8 v (x v ) + uv\u2208E \u03b8 uv (x u , x v ) , (2.1)\nwhere x u belongs to a finite label set X u for each node u \u2208 V, \u03b8 u : X u \u2192 R and \u03b8 uv : X u \u00d7X v \u2192 R are the unary and pairwise potentials associated with the nodes and edges of G. The label space for A \u2282 V is X A = u\u2208A X u , where stands for the Cartesian product. For notational convenience we write X uv = X u \u00d7 X v and x uv = (x u , x v ) for uv \u2208 E. Notations like x \u2208 X A implicitly indicate that the vector x only has components x u indexed by u \u2208 A. With x |A \u2208 X A we denote restriction of the labeling x \u2208 X V to the set A \u2282 V.\nMore general graphical models with terms depending on three or more variables can be considered as well. For  \ns.t. \u00b5w(xw) \u2208 {0, 1} for w \u2208 V \u222a E, xw \u2208 Xw ,(2.2)\nwhere the local polytope \u039b V [43] is the set of \u00b5 fulfilling\nxv \u2208V \u00b5v(xv) = 1, v \u2208 V, xv \u2208V \u00b5uv(xu, xv) = \u00b5u(xu), xu \u2208 Xu, uv \u2208 E, xu\u2208V \u00b5uv(xu, xv) = \u00b5v(xv), xv \u2208 Xv, uv \u2208 E, \u00b5w(xw) \u2265 0, w \u2208 V \u222a E, xw \u2208 Xw . (2.3)\nWe define \u039b A for A \u2282 V similarly. Slightly abusing notation we will denote the objective function in (2.2) as E V (\u00b5). The formulation (2.2) utilizes the overcomplete representation [43] of labelings in terms of indicator vectors \u00b5, which are often called marginals. The problem of finding \u00b5 * \u2208 argmin \u00b5\u2208\u039b V E V (\u00b5) (i.e. solving (2.2) without integrality constraints) is called the local polytope relaxation of (2.1).\nWhile solving the local polytope relaxation can be done in polynomial time, the corresponding optimal marginal \u00b5 * may not be integral anymore, hence infeasible and not optimal for (2.2). For a wide spectrum of problems however most of the entries of optimal marginals \u00b5 * for the local polytope relaxation will be integral. Unfortunately, there is no guarantee that any of these integral variables will be part of a globally optimal solution to (2.2), except in the case of binary variables, that is X u = {0, 1} \u2200u \u2208 V, and unary and pairwise potentials [7]. Natural questions are: (i) Is there a subset A \u2282 V and a minimizer \u00b5 0 of the original NP-hard problem (2.2) such that\n\u00b5 0 v = \u00b5 * v \u2200v \u2208 A?\nIn other words, is \u00b5 * partially optimal or persistent on some set A? (ii) Given a relaxed solution \u00b5 * \u2208 \u039b V , how can we determine such a set A? We provide a novel approach to tackle these problems in what follows.", "publication_ref": ["b43", "b43", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Persistency", "text": "Assume we have marginals \u00b5 \u2208 \u039b V . We say that the marginal\n\u00b5 u , u \u2208 V, is integral if \u00b5 u (x u ) \u2208 {0, 1} \u2200x u \u2208 X u .\nIn this case the marginal corresponds uniquely to a label x u with \u00b5 u (x u ) = 1. If this integrality condition holds for all u \u2208 V the corresponding vector \u00b5 will be denoted as \u03b4(x). The convex hull of marginals corresponding to all labelings known as marginal polytope will be denoted as M V := conv(\u03b4(X V )). The non-relaxed energy minimization (2.1) can be equivalently written as min\n\u00b5\u2208M V E V (\u00b5).\nLet the boundary nodes and edges of a subset of nodes A \u2282 V be defined as follows: Definition 1 (Boundary and Interior). For the set A \u2282 V the set \u2202V A := {u \u2208 A : \u2203v \u2208 V\\A s.t. uv \u2208 E} is called its boundary. The respective set of boundary edges is defined as \u2202E A = {uv \u2208 E : u \u2208 A and v \u2208 V\\A}. The set A\\\u2202V A is called the interior of A.\nAn example graph illustrating the concept of interior and boundary nodes can be seen in Figure 1.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Definition 2 (Persistency).", "text": "A labeling x 0 \u2208 X A on a subset A \u2282 V is partially optimal or persistent if x 0 coincides with an optimal solution to (2.1) on A.\nIn the remainder of this section, we state our novel persistency criterion in Theorem 1. Taking additionally into account a convex relaxation yields a computationally tractable approach in Corollary 1.\nAs a starting point, consider the following sufficient criterion for persistency of x 0 \u2208 X A . Introducing a concatenation of labelings x 0 \u2208 X A andx \u2208 X V\\A as\n(x 0 ,x) := x 0 v , v \u2208 A, x v , v \u2208 V\\A\n, the criterion reads:\nProposition 1. A partial labeling x 0 \u2208 X A is persistent if there holds \u2200x \u2208 X V\\A : x 0 \u2208 argmin x\u2208X A E V ((x,x)) . (3.1)\nProof. Consider the equation min\nx\u2208X V E V (x) = mi\u00f1 x\u2208X V\\A min x\u2208X A E V ((x,x)) . (3.2)\nLetx \u2208 X V\\A be such that it leads to a minimal value on the right hand side of (3.2). Thenx is part of an optimal solution. By the assumption (3.1), x 0 is an optimal solution to the inner minimization problem of (3.2), hence (x 0 ,x) is optimal for (2.1).\nThis means that the concatenated labeling (x 0 ,x) has to be optimal for min x E(x) s.t. x v =x v \u2200v \u2208 V\\A. Informally this means that the solution x 0 is independent of what happens on V\\A. This criterion however is hard to check directly, as it entails solving NP-hard minimization problems over an exponential number of labelings x \u2208 X V\\A .\nWe relax the above criterion (3.1) so that we have to check the solution of only one energy minimization problem by modifying the unaries \u03b8 v on boundary nodes so that they bound the influence of all labelings on V\\A uniformly.\nDefinition 3 (Boundary potentials and energies). For a set A \u2282 V and a test labeling y \u2208 X \u2202V A , we define for each boundary edge uv \u2208 \u2202E A , u \u2208 \u2202V A the \"boundary\" potential\u03b8 uv,yu :\nX u \u2192 R a\u015d \u03b8 uv,yu (x u ) := max xv\u2208Xv \u03b8 uv (x u , x v ), y u = x u min xv\u2208Xv \u03b8 uv (x u , x v ), y u = x u .\n(3.3) Define the energy\u00ca A,y : X A \u2192 R with test labeling y a\u015d\nE A,y (x) := E A (x) + uv\u2208\u2202E A : u\u2208\u2202V A\u03b8 uv,yu (x u ) , (3.4) min max x u x 0 u x v u v \u03b8 uv,x 0 u (x u )\nFigure 2: Illustration of a boundary potential\u03b8 x 0 constructed in (3.3). The second label comes from the test labeling x 0 , therefore entries are maximized for the first row and minimized otherwise.\nwhere\nE A (x) = u\u2208A \u03b8 u (x u )+ uv\u2208E:u,v\u2208A \u03b8 uv (x uv )\nis the energy with potentials with support in A.\nGiven a test labeling y \u2208 X A , energy (3.4) assigns a higher value than the original energy (2.1) for all labelings conforming to y and makes it more favourable for all labelings to not conform to y. An illustration of a boundary potential is depicted by Figure ??.\nAs a consequence, if the test labeling y from Definition 1 minimizes the energy (3.4), the proof of the following theorem asserts that changing an arbitrary labeling x \u2208 X V as follows:\nx v = y v , v \u2208 A x v , v / \u2208 A\nwill always result in a labeling with not bigger energy (2.1), hence y in particular fulfills the conditions (3.1) of Proposition 1 and thus is persistent.\nTheorem 1 (Partial optimality criterion). A labeling\nx 0 \u2208 X A on a subset A \u2282 V is persistent if x 0 \u2208 argmin x\u2208X A\u00ca A,x 0 (x) ,(3.5)\nwhere\u00ca A,x 0 is the augmented energy functional (3.4).\nTo prove the theorem we need the following technical lemma.\nLemma 1. Let A \u2282 V be given together with y \u2208 X \u2202V A . Let x 0 and x be two labelings on V such that x\n0 | A = y. Then it holds for uv \u2208 \u2202E A , u \u2208 \u2202V A that \u03b8 uv (x 0 u , x v ) +\u03b8 uv,y (x u ) \u2212\u03b8 uv,y (x 0 u ) \u2264 \u03b8 uv (x u , x v ) . (3.6) Proof. The case x u = x 0 u is trivial. Otherwise, by Defini- tion 3, inequality (3.6) is equivalent to \u03b8 uv (x 0 u , x v ) + min xv\u2208Xv \u03b8 uv (x u , x v ) \u2212 max xv\u2208Xv \u03b8 uv (x 0 u , x v ) \u2212 \u03b8 uv (x u , x v ) \u2264 0 . (3.7)\nChoose x v for x v in the minimization and maximization in (3.7) to obtain the result.\nProof of Theorem 1. Let\nx \u2208 arg min x\u2208X V x| A =x 0 | A E V (x) .\n(3.8)\nand let x \u2208 X V be an arbitrary labeling. Then\nE V (x) = E A (x 0 ) + E V\\A (x) + uv\u2208\u2202E A \u03b8 uv (x 0 u ,x v ) (3.9) =E A (x 0 ) + uv\u2208\u2202E A\u03b8 uv,y (x 0 u ) + E V\\A (x) + uv\u2208\u2202E A \u03b8 uv (x 0 u ,x v ) \u2212\u03b8 uv,y (x 0 u ) =\u00ca A,x 0 (x 0 ) + E V\\A (x) + uv\u2208\u2202E A \u03b8 uv (x 0 ,x v ) \u2212\u03b8 uv,x 0 (x 0 u ) \u2264\u00ca A,x 0 (x ) + E V\\A (x ) + uv\u2208\u2202E A \u03b8 uv (x 0 , x v ) \u2212\u03b8 uv,x 0 (x 0 u ) (3.10) =E A (x ) + uv\u2208\u2202E A\u03b8 uv,x 0 (x u ) + E V\\A (x ) + uv\u2208\u2202E A \u03b8 uv (x 0 u , x v ) \u2212\u03b8 uv,x 0 (x 0 u ) \u2264E A (x )+E V\\A (x )+ uv\u2208\u2202E A \u03b8 uv (x u , x v ) = E V (x ). (3.11)\nThe equality (3.9) is due to definition ofx in (3.7). The first inequality (3.10) is due to x 0 \u2208 argmin x\u00caA,x 0 (x), as assumed, and ofx for (3.8). The second inequality (3.11) is due to Lemma 1. Hence x 0 is part of a globally optimal solution, as x was arbitrary.\nChecking the criterion in Theorem 1 is NP-hard, because (3.5) is a MAP-inference problem of the same class as (2.1). By relaxing the minimization problem (3.5) one obtains the polynomially verifiable persistency criterion in Corollary 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Corollary 1 (Tractable partial optimality criterion", "text": "). La- beling x 0 \u2208 X A on A \u2282 V fulfilling the condition \u03b4(x 0 ) \u2208 argmin \u00b5\u2208\u039b A\u00ca A,x 0 (\u00b5) (3.12)\nis also a solution to (3.5), hence persistent on A.\nProof. Expression (3.12) implies\n\u03b4(x 0 ) \u2208 argmin \u00b5\u2208\u039b A ,\u00b5\u2208{0,1} dim \u039b A\u00caA,x 0 (\u00b5) (3.13) because \u03b4(x 0\n) is integral by definition. As (2.1) and (2.2) are equivalent and the corresponding labeling x 0 satisfies the conditions of Theorem 1, x 0 is partially optimal on A.\nAlgorithm 1: Finding persistent variables.\nData: G = (V, E), \u03b8 u : X u \u2192 R, \u03b8 uv : X uv \u2192 R Result: A * \u2282 V, x * \u2208 X A * Initialize: Choose \u00b5 0 \u2208 argmin \u00b5\u2208\u039b V E V (\u00b5) A 0 = {u \u2208 V : \u00b5 0 u \u2208 {0, 1} |Xu| } t = 0 repeat Set x t u such that \u00b5 t u (x t u ) = 1, u \u2208 A t Choose \u00b5 t+1 \u2208 argmin \u00b5\u2208\u039b A t\u00ca A t ,x t (\u00b5) t = t + 1 W t = {u \u2208 \u2202V A t\u22121 : \u00b5 t u (x t\u22121 u ) = 1} A t = {u \u2208 A t\u22121 : \u00b5 t u \u2208 {0, 1} |Xu| }\\W t until A t = A t\u22121 ; A * = A t Set x * \u2208 X A * such that \u00b5 t u (x * u ) = 1", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Persistency Algorithm", "text": "Now we concentrate on finding a set A and labeling x \u2208 X A such that the solution of min \u00b5\u2208\u039b A\u00ca A,x (\u00b5) fulfills the conditions of Corollary 1. Our approach is summarized in Algorithm 1.\nIn the initialization step of Algorithm 1 we solve the relaxed problem over V without boundary labeling and initialize the set A 0 with nodes having an integer label. Then in each iteration t we minimize over the local polytope the energy\u00ca A t ,x t defined in (3.4), corresponding to the set A t and boundary labeling coming from the solution of the last iteration. We remove from A t all variables which are not integral or do not conform to the boundary labeling. In each iteration t of Algorithm 1 we shrink the set A t by removing variables taking non-integral values or not conforming to the current boundary condition. Convergence. Since V is finite and |A t | is monotonically decreasing, the algorithm converges in at most |V| steps. Solving each subproblem in Algorithm 1 can be done in polynomial time. As the number of iterations of Algorithm 1 is at most |V|, Algorithm 1 itself is polynomial as well. In practice only few iterations are needed.\nAfter termination of Algorithm 1, we have  method become inapplicable. It is important that one can also employ approximate solvers, as soon as they provide (i) a proposal for potentially persistent nodes and (ii) sufficient conditions for optimality of the found integral solutions such as e.g. zero duality gap. These properties have the following precise formulation.\n\u03b4(x * ) \u2208 argmin \u00b5\u2208\u039b A * \u00caA * ,x * (\u00b5) . (4\nDefinition 4 (Integrally Correct Algorithm). Assume an algorithm that takes as the input an energy minimization prboblem and outputs a labeling x * \u2208 v\u2208V (X v \u222a {#}). We call such an algorithm integrally correct if\nx * v \u2208 X v \u2200v \u2208 V implies x * \u2208 argmin x\u2208X V E V (x).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Integrally correct algorithms include", "text": "\u2022 Dual decomposition based algorithms [15,19,21,26,28] deliver strong tree agreement [42] and algorithms considering the Lagrangian dual [5,8,30] return strong arc consistency [44] for some nodes. If one of these properties holds for a node v, we set c v as the corresponding label. Otherwise we set c v = #.\n\u2022 Naturally, any algorithm solving min \u00b5\u2208\u039b V E(\u00b5) exactly is integrally correct with\nc v = x v , \u00b5 v (x v ) = 1 #, \u00b5 v / \u2208 {0, 1} |Xv| .\nProposition 2. Let operations \u00b5 \u2208 argmin(...) in Algorithm 1 be exchanged with\n\u2200v \u2208 V, x v \u2208 X v , \u00b5 v (x v ) := \uf8f1 \uf8f2 \uf8f3 1, c v = x v 0, c v / \u2208 {x v , #}, 1/|X v |, c v = #\nwhere c are consistent labelings returned by an integrally correct algorithm applied to the corresponding minimization problems. Then the output labeling x * is persistent.\nProof. At termination of Algorithm 1 we have obtained a subset of nodes A * , a test labeling y * \u2208 X \u2202V A , a labeling x * equal to y * on \u2202V A and a consistency mapping c u = x * u for u \u2208 A * . Hence, by Definition 4, x * \u2208 argmin x\u2208X A\u00ca A * ,y * and x * fulfills the conditions of Theorem 1.\nRemark 1. Note that a bad or early stopped solver, i.e. one which rarely (or even never) returns an optimality certificate or solves a weak relaxation, will also work with Algorithm 1. However it will find smaller (or even empty) partial optimal solutions.\nComparison to the Shrinking Technique (Com-biLP) [27]. The recently published approach [27], similar to Algorithm 1, describes how to shrink the combinatorial search area with the local polytope relaxation. However (i) Algorithm 1 solves a series of auxiliary problems on the subsets A t of integer labels, whereas the method [27] considers nodes, which got fractional labels in the relaxed solution; (ii) Algorithm 1 is polynomial and provides only persistent labels, whereas the method [27] has exponential complexity and either finds an optimal solution or gives no information about persistence.\nFrom the practical point of view, both algorithms have different application scenarios: CombiLP [27] will only work on sparse graphs, as otherwise the combinatorial part, which one has to solve with exact methods, becomes too big, as the boundary \u2202V A for A V grows very quickly then. Also, even for sparse graphs, the combinatorial part may not grow too big during the application of the algorithm, as otherwise the combinatorial solver will again not be able to cope with it. Our algorithm does not possess these two disadvantages. From the perspective of running time it does not matter how big the set V\\A t becomes during the iterations of Algo-  rithm 1. On the other hand, the subsets of variables to which the method [27] applies a combinatorial solver to achieve global optimality are often smaller than V\\A t in Algorithm 1, because potentials in CombiLP [27] remain unchanged in contrast to the perturbation (3.4). Another advantage of the method [27] is that it needs to solve the (typically) big LP relaxation of the original problem only once, whereas our method does this iteratively, which makes it often slower then CombiLP. One other possible application scenario which is possible with our method but not with CombiLP [27] is the following: Assume we want to solve an extremely big inference problem, one that does not fit even into memory. To do this, choose a subset A V of nodes of the graphical model, solve the inference problem on the induced subgraph G(A) with some boundary conditions, and find a partially optimal labeling on it. This is akin to the windowing technique of [33]. By doing so for an overlapping set of subgraphs, one may try to find a labeling for the overall problem on G.\nThe major differences between CombiLP [27] and our method are summarised in Table 2.", "publication_ref": ["b14", "b19", "b21", "b26", "b28", "b42", "b4", "b7", "b30", "b44", "b27", "b27", "b27", "b27", "b27", "b27", "b27", "b27", "b27", "b33", "b27"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Largest Persistent Labeling", "text": "Let A 0 \u2282 V and \u00b5 0 \u2208 \u039b A 0 be defined as in Algorithm 1. Subsets A \u2282 A 0 which fulfill the conditions of Corollary 1 taken with labelings \u00b5 0 | A can be partially ordered with respect to inclusion \u2282. In this section we will show that the following holds:\n\u2022 There is a largest set among those, for which there exists a unique persistent labeling fufilling the conditions of Corollary 1.\n\u2022 Algorithm 1 finds this largest set.\nThis will imply that Algorithm 1 cannot be improved upon with regard to the criterion in Corollary 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definition 5 (Strong Persistency).", "text": "A labeling x * \u2208 X A is called strongly persistent on A, if from\nx 0 \u2208 argmin x\u2208X A\u00ca A,x 0 (x) , (5.1)\nwith\u00ca A,x * as in (3.4) follows x * = x 0 , i.e.\nx * is the unique labeling on A such that x * \u2208 argmin x\u2208X A\u00ca A,x * (x).\nLemma 2. Let x * \u2208 X A be strongly persistent. Then for any optimal solution x of (2.1) we have\nx * = x |A .\nProof. This follows from Inequality (3.10) being strict in this case.\nTheorem 2 (Largest persistent labeling). Let x 0 \u2208 X A * strong and A * strong \u2282 V be such that\n\u03b4(x 0 ) \u2208 argmin \u00b5\u2208\u039b A * strong\u00ca A * strong ,x 0 (\u00b5) (5.2)\nand x 0 is the unique such labeling on A * strong . Then Algorithm 1 finds a persistent labeling on A * such that A * strong \u2282 A * \u2282 V, i.e. A * is a superset of all sets on which strongly persistent labelings identifiable by the criterion of Corollary 1 exist.\nTo prove the theorem we need the following technical lemma.\nLemma 3. Let A \u2282 B \u2282 V be two subsets of V and \u00b5 A \u2208 \u039b A marginals on A and x A \u2208 X A a labeling fulfilling the conditions of Corollary 1 uniquely (i.e. x A is strongly persistent). Let y B \u2208 X B be a test labeling such that y\nB | A = x A .\nThen for all marginals \u00b5 * \u2208 argmin \u00b5\u2208\u039b B\u00ca B,y\nB (\u00b5) on B it holds that \u00b5 * v (x A v ) = 1 \u2200v \u2208 A.\nProof. Similar to the proof of Theorem 1. Replace V by B.\nProof of Theorem 2. We will use the notation from Algorithm 1. It will be enough to show that for every A \u2282 V such that there exists a strongly persistent labeling x \u2208 X A we have A \u2282 A t in each iteration of Algorithm 1 and furthermore x v = x t v for all v \u2208 V A . Hence the union of sets A strong , for which a strongly persistent labeling exists which fulfills the conditions of Corollary 1, is a subset of A t \u2200t. Also by Lemma 2 the associated strongly persistent labelings agree where they overlap, hence we are done.\nFor t = 0 apply Lemma 3 with A := A and B := A 0 (= V). Condition x = y B | A in Lemma 3 is assured by Corollary 1. Hence, Lemma 3 ensures that for all \u00b5 0 \u2208 argmin \u00b5\u2208\u039b V E(\u00b5) it holds that \u00b5 0 v (x v ) = 1 for all v \u2208 A. Now assume the claim to hold for iteration t \u2212 1. We need to show that it also holds for t. For this invoke Lemma 3 with A := A, B := A t\u22121 and y B := x t\u22121 . The conditions of Lemma 3 hold by assumption on t \u2212 1. Lemma 3 now ensures that for all \u00b5 t \u2208 argmin \u00b5\u2208\u039b A t\u22121\u00ca A t\u22121 ,x t\u22121 (\u00b5) there holds \u00b5 t (x A v ) = 1 \u2200v \u2208 A.\nFrom the proof of Theorem 2 we can directly conclude the existence of the largest set A \u2282 V such that there is a strongly persistent labeling on A identifiably by the criterion in Corollary 1.\nCorollary 2. There exists a unique largest set A * strong with a strongly persistent labeling x 0 \u2208 A * strong identifiable by the criterion in Corollary 1, i.e. such that\n\u03b4(x 0 ) \u2208 argmin \u00b5\u2208\u039b A * strong\u00ca A * strong ,x 0 (\u00b5) , (5.3)\nand x 0 is the unique such labeling.\nAlso exactly the largest strongly persistent labeling identifiable by Corollary 1 can be found under a mild uniqueness assumption. Corollary 3. If there is a unique solution of min \u00b5\u2208\u039b A t\u00caA t ,x t (\u00b5) for all t = 0, . . . obtained during the iterations of Algorithm 1, then Algorithm 1 finds the largest subset of persistent variables identifiable by the sufficient partial optimality criterion in Corollary 1.\nRemark 2. Above we showed that Algorithm 1 will find a persistent labeling which contains the largest strongly persistent one identifiably by Corollary 1. The two may differ when the optimization problems solved in the course of Algorithm 1 have multiple optima. The simplest example of such a situation occurs if the relaxation min \u00b5\u2208\u039b V E V (\u00b5) is tight, but has several integer solutions. Any convex combination of these solutions will form a non-integral solution, hence the strongly persistent labeling is defined on a smaller set than any integral solution of min \u00b5\u2208\u039b V E V (\u00b5), which is non strongly persistent. Note however that a labeling obtained by Algorithm 1, also when it is not strongly persistent, comes from one globally optimal labeling, i.e. it can be completed to a globally optimal labeling by solving for the remaining variables.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Optimal Reparametrization", "text": "It is well-known [29] (see also [44]) that representation (2.1) of the energy function is not unique. There are other potentials, which keep the energy of all labelings unchanged. Any such potentials \u03b8 \u03d5 can be represented as\n\u03b8 \u03d5 v (x v ) := \u03b8 v (x v ) \u2212 u\u2208nb(v) \u03d5 v,u (x v ) , (6.1) \u03b8 \u03d5 uv (x u , x v ) := \u03b8 uv (x u , x v ) + \u03d5 v,u (x v ) + \u03d5 u,v (x u ) (6.\n2) with some numbers \u03d5 u,v (x u ), uv \u2208 E, x u \u2208 X u , where nb(v) := {u \u2208 V : uv \u2208 E} denotes the set of nodes adjacent to v \u2208 V. The vector \u03d5 with coordinates \u03d5 u,v (x u ) is called reparametrization.\nThe boundary potentials (3.3) and hence the persistency approach described above are dependent on reparametrization. The natural question is existence of an optimal reparametrization, that is, the one providing the largest persistent set.\nThe only coordinates of the reparametrization vector \u03d5, which can potentially influence the solution of the test problem (3.5) are \u03d5 v,u (x v ), u \u2208 \u2202V A , uv \u2208 \u2202E A . Reparametrization \u03d5 v,u (x v ), v \u2208 A \"inside\" A does not influence the solution, because it does not change the augmented energy\u00ca A,y of any labeling y. Similarly, the reparametrization \u03d5 u,v (x u ), u, v / \u2208 A \"outside\" A does not influence it, because the optimization is performed over A only.\nConsidering the reparametrized potentials \u03b8 \u03d5 and subtracting max xv\u2208Xv \u03b8 uv (y u , x v ) in (3.3) the boundary potentials\u03b8 \u03d5 uv,yu (x u ) can be equivalently exchanged with\n0, y u = x u min xv\u2208Xv \u03b8 \u03d5 uv (x u , x v ) \u2212 max xv\u2208Xv \u03b8 \u03d5 uv (y u , x v ), y u = x u . (6.\n3) It means that the labelings x not coinciding with y on \u2202V A will be \"encouraged\" with (typically negative) value \u2206 \u03d5 uv (x u ) := min\nxv\u2208Xv \u03b8 \u03d5 uv (x u , x v ) \u2212 max xv\u2208Xv \u03b8 \u03d5 uv (y u , x v ).\nIntuitively clear that the bigger \u2206 \u03d5 uv (x u ) is, the better the proposal labeling y| A comparing to x| A = y| A is and hence the greater the found persistent set A * returned by Algorithm 1 would be. We will prove correctness of this intuition formally, but first let us find the maximal possible value of \u2206 \u03d5 uv (x u ) w.r.t. the reparametrization \u03d5, where we consider as non-zero only coordinates\n\u03d5 v,u (x v ), u \u2208 \u2202V A , uv \u2208 \u2202E A , x v \u2208 X v . Clearly \u2206 \u03d5 uv (x u ) \u2264 min xv\u2208Xv (\u03b8 \u03d5 uv (x u , x v ) \u2212 \u03b8 \u03d5 uv (y u , x v )) = min xv\u2208Xv (\u03b8 uv (x u , x v ) + \u03d5 v,u (x v ) \u2212 \u03b8 uv (y u , x v ) \u2212 \u03d5 v,u (x v )) = min xv\u2208Xv (\u03b8 uv (x u , x v ) \u2212 \u03b8 uv (y u , x v\n)) , (6.4) hence, the right-hand-side of this inequality does not depend on the reparametrization, whereas the left-hand-side does. There is indeed such a reparametrization that turns the inequality (6.4) into equality and in this way guarantees the largest possible values of \u2206 \u03d5 uv (x u ) for all x u . This reparametrization (as we show below it is an optimal one) is defined as\n\u03d5 u,v (x v ) = \u2212\u03b8 uv (y u , x v ) , (6.5)\nwhich can be seen when plugging (6.5) into (6.3). Moreover, since as we mentioned above the reparametrization \"outside\" an \"inside\" A t does not influence the criterion (3.3), we can construct a single, equal for all iterations of Algortihm 1 optimal reparametrization \u03c8 according to the rule (6.5) as\n\u03c8 u,v (x v ) = \u2212\u03b8 uv (y u , x v ), u \u2208 V, uv \u2208 E , (6.6)\nwhere y is arbitrarily extended from A 0 to V. Now we are ready to formulate our main result related to the reparametrization.\nLet us denote by\u00ca \u03d5 A,y the energy with boundary labeling defined as in Definition 3 w.r.t. the potentials \u03b8 \u03d5 . Then for the reparametrization \u03c8 defined as in (6.6) there holds Proof. From (6.4) and (6.7) it follows that for all uv \u2208 E A , x u \u2208 X u there holds\u03b8 \u03c8 uv,x 0\nx (x u ) \u2212\u03b8 \u03c8 uv,x 0 u (x 0 u ) \u2265 \u03b8 uv,x 0 u (x u ) \u2212\u03b8 uv,x 0 u (x 0 u ) and henc\u00ea E \u03c8 A,x 0 (\u00b5) \u2212\u00ca \u03c8 A,x 0 (x 0 ) (6.4) \u2265\u00ca A,x 0 (\u00b5) \u2212\u00ca A,x 0 (x 0 ) \u2265 0 (6.8)\nfor all \u00b5 \u2208 \u039b A . Thus\u00ca \u03c8 A,x 0 (x 0 ) \u2264\u00ca \u03c8 A,x 0 (\u00b5), which proves the statement of the lemma. Let now A \u03d5, *\nx 0 be the largest set containing all strongly persistent variables satisfying Corollary 1 w.r.t. the reparametrized potentials \u03b8 \u03d5 and test labeling y \u2208 X V . Let also A *\nx 0 correspond to the trivial reparametrization \u03d5 \u2261 0.\nApplying Lemma 4 to the set A * x 0 leads to the following Theorem 3. For any test labeling x 0 \u2208 X V there holds\nA * x 0 \u2282 A \u03c8, * x 0 .\nProof. Same proof as in Lemma 4 applied to A * x 0 .\nRemark 4. For Potts models, where\n\u03b8 uv (x u , x v ) = 0, x u = x v \u03b1, x u = x v\n, the inequality (6.4) holds as equality also for the trivial reparametrization\n\u03d5 v,u (x v ) = 0 \u2200u, v \u2208 V, uv \u2208 E, x v \u2208 X v .\nFor such models Algorithm 1 with the trivial reparametrization delivers the same persistent set as with the optimal one (6.6).", "publication_ref": ["b29", "b44"], "figure_ref": [], "table_ref": []}, {"heading": "Optimality of the Method", "text": "Theorem 2 proves optimality of Algorithm 1 w.r.t. the formulated persistency criterion provided by Theorem 1. However it does not prove optimality of the method with respect to other possible criteria and hence does not guarantee its superiority over other partial optimality techniques. There is however a recent study [31,32], which provides such an optimal relaxed persistency criterion covering all existing methods. In what follows we will introduce key notions from [32] and show that our persistency criterion coincides with the optimal one provided in [32] for a certain class of persistency methods, those providing only node-persistency, i.e. either eliminating all labels except one in a given node or not eliminating any. Definition 6. A mapping p :\nX V \u2192 X V is called (strictly) improving if for all x \u2208 X V such that p(x) = x there holds E V (p(x)) \u2264 E V (x) (resp. E V (p(x)) < E V (x)).\nIn what follows we will restrict ourselfs only to idempotent mappings p, i.e. satisfying p(p(x)) = p(x).\nFollowing [32] we consider only node-wise maps of the form p(\nx) v = p v (x v ), where p v : X v \u2192 X v are idempo- tent, i.e. p v (p v (x v )) = p v (x v ) for all x v \u2208 X v .\nThis class is already general enough to include nearly all existing techniques.\nImproving mappings define persistency due to the following proposition: Proposition 3 (Stat. 1 [32]). Let p be an improving mapping. Then there exists an optimal solution x of (2.1) such that for all v \u2208 V from p v (i) = i follows x v = i. In case p is strictly improving this holds for any optimal solution.\nFor an idempotent mapping p a linear mapping P : R I \u2192 R I satisfying \u03b4(p(x)) = P \u03b4(x) for all x \u2208 X V is called its linear extension. A particular linear extension denoted as [p] is defined as follows. For each p v we define the matrix\nP v \u2208 R Xv\u00d7Xv by P v,ii = 1, p v (i ) = i 0, p v (i ) = i .\nThe linear extension P = [p] is given by\n(P \u00b5) v = i \u2208Xv P v,ii \u00b5 v (i ) = P v \u00b5 v ; (7.1) (P \u00b5) uv = P u \u00b5 uv P v .\nIn what follows we will employ the commonly used representation of energy E V (\u00b5) in a form of an inner product \u03b8, \u00b5 , where vectors of potentials \u03b8 and marginals \u00b5 belong to the vector space R I with the suitably selected\ndimension I = v\u2208V |X v | + uv\u2208E |X uv |.\nDenote by I the identity matrix. From Definition 6 follows that p is improving iff the value of\nmin x\u2208X V (E V (x) \u2212 E V (p(x))) = min x\u2208X V \u03b8, (I \u2212 [p])\u03b4(x) = min x\u2208X V (I \u2212 [p]) \u03b8, \u03b4(x) = min \u00b5\u2208M V (I \u2212 [p]) \u03b8, \u00b5(7.2)\nis zero. If additionally p(x) = x for all minimizers of (7.2) then the mapping p is strictly improving. Problem (7.2) is of the same form as energy minimization (2.1) and is therefore as hard as Problem (7.2). Its relaxation is obtained by letting \u00b5 to vary in the local polytope \u039b V \u2282 R I , an outer approximation to M V . Definition 7. An idempotent mapping p :\nX V \u2192 X V is \u039b V -improving for potentials \u03b8 \u2208 R I if min \u00b5\u2208\u039b V (I \u2212 [p]) \u03b8, \u00b5 = 0 . (7.3)\nIf additionally [p]\u00b5 = \u00b5 for all minimizers \u00b5 of (7.3) then p is strictly \u039b V -improving.\nCompared to (7.2), only the polytope was changed to \u039b V \u2283 M V . This implies the following simple fact:\nProposition 4. If mapping p is (strictly) \u039b V -improving then it is (strictly) improving.\nThe method presented in this work can be interpreted as considering all-to-one node-wise idempotent mappings p having the form\np v (i) = y v , if v \u2208 A i, if v / \u2208 A (7.4)\nfor a fixed test labeling y. All labels in the nodes v \u2208 A \u2282 V are mapped to y v . Among all all-to-one (strictly) \u039b Vimproving mappings the one with the largest set A will be called maximal. Corollary 1 determines \u039b V -improving mappings, as stated by Lemma 5. The relaxed persistency criterion provided by Corollary 1 with the reparametrization given by (6.6) is equivalent to Definition 7 with the improving mapping p defined as in (7.4) for a given test labeling y.\nProof. For future references we write down potentials \u03b8 \u03c8 with \u03c8 defined by (6.6) explicitly:\n\u03b8 \u03c8 u (x u ) = \u03b8 u (x u ) + v\u2208nb(u) \u03b8 uv (x u , y v ) ,(7.5)\n\u03b8 \u03c8 uv (x u , x v ) = \u03b8 uv (x u , x v ) \u2212 \u03b8 uv (x u , y v ) \u2212 \u03b8 uv (y u , x v ) .\nIn what follows we will show that the criteria (3.12) and ( 7.3) coincide. Both of them represent the local polytope relaxation of specially constructed energy minimization problems. To prove that the relaxations coinside it is sufficient to prove that the non-relaxed energies are equal. Energy of Criterion 1. First we write down the nonrelaxed test problem (3.5) with potentials \u03b8 \u03c8 as arg min\nx\u2208X V v\u2208V \u03b2 v (x v )+ uv\u2208E \u03b2 uv (x u , x v )+ uv\u2208\u2202E A : u\u2208\u2202V \u00c2 \u03b8 \u03c8\nuv,yu (x u ) (7.6) with potentials \u03b2 equal to \u03b8 \u03c8 on A and vanishing outside it, i.e.\n\u03b2 u (x u ) = \u03b8 u (x u ) + v\u2208nb(u) \u03b8 uv (x u , y v ), u \u2208 A 0, u \u2208 V\\A (7.7) \u03b2 uv (x u , x v ) = \u03b8 uv (x u , x v ) \u2212 \u03b8 uv (x u , y v ) \u2212 \u03b8 uv (y u , x v ), u, v \u2208 A 0, otherwise . (7.8)\nBorder potentials\u03b8 \u03c8 for uv \u2208 E, u \u2208 V A , v \u2208 V\\A and x u = y u read: Note that (7.9) turns into (7.10) when x u = y u , hence it is sufficient to use only expression (7.9). Energy of Definition 7. The non-relaxed version of condition (7.3) defining \u039b V -improving all-to-one mapping, with the labeling proposal y can be formulated as checking whether y \u2208 arg min\n\u03b8 \u03c8 uv,yu (x u ) = min xv\u2208Xv \u03b8 \u03c8 uv (x u , x v ) = = min xv\u2208Xv (\u03b8 uv (x u , x v ) \u2212 \u03b8 uv (x u , y v ) \u2212 \u03b8 uv (y u , x v )) = \u2212\u03b8 uv (x u , y v ) + min\nx\u2208X V v\u2208V \u03b3 v (x v )+ uv\u2208E \u03b3 uv (x u , x v )+ u\u2208\u2202E \u00c2\n\u03b3 uv,yu (x u ) (7.11) with potentials \u03b3 defined as: Equivalency of Energies. Comparing (7.12), (7.13) and (7.14) to (7.7), (7.8) and (7.9) respectively it can be seen that they can be transformed to each other by several operations, which equally change energies of all labelings and thus do not influence the criterions provided by Theorem 1 and [32, eq.( 14)]. These operations are:\n\u03b3 u (x u ) = \u03b8 u (x u ) \u2212 \u03b8 u (y u ), u \u2208 A 0, u \u2208 V\\A (7.12) \u03b3 uv (x u , x v ) = \u03b8 uv (x u , x v ) \u2212 \u03b8 uv (y u , y v ), u, v \u2208 A 0, otherwise . (7.\n1. Subtract \u03b8 u (y u ) from \u03b2 u (x u ) for all u \u2208 V A , x u \u2208 X u .\n2. Subtract \u03b8 uv (y u , y v ) from \u03b2 uv (x u , x v ) for all uv \u2208 E A , (x u , x v ) \u2208 X u \u00d7 X v .\n3. Reparametrize \u03b2 with the reparametrization vector \u03c6 defined as\n\u03c6 u,v (x u ) = \u2212\u03b8 uv (x u , y v ), u \u2208 A 0, u \u2208 V\\A . (7.15)\nThe following theorem states that our method provably delivers the best results among the methods providing node-persistency: Theorem 4. Under conditions of Corollary 3, Algorithm 1 with the reparametrizations given by (6.6) finds the maximal strict \u039b V -improving all-to-one mapping for a given proposal labeling x 0 . Proof. Under condition of Corollary 3 (i.e. when on each iteration there is a unique solution \u00b5 t ) Lemma 5 guarantees equivalence of our criterion (Corollary 1 with reparametrization \u03c8) to Definition 7 for the strict \u039b Vimproving all-to-one mapping. Theorem 2 states that Algorithm 1 delivers the largest set A * satisfying this criterion, which in turn proves the theorem.", "publication_ref": ["b31"], "figure_ref": [], "table_ref": []}, {"heading": "Extensions", "text": "Higher Order Models. Assume now we are not in the pairwise case anymore but have an energy minimization problem over a hyper graph G = (V, E) with E \u2282 P(V) a set of subsets of V: (8.4) Now Theorem 1, Corollary 1 and Algorithm 1 can be directly translated to the higher order case. Tighter Relaxations. Essentially, Algorithm 1 can be applied also to tighter relaxations than \u039b A , e.g. when one includes cycle inequalities [35]. One merely has to replace the local polytope \u039b A for A \u2282 V by the tighter feasible convex set: Proposition 5. Let the polytopes\u039b A \u2287 M A satisf\u1ef9 \u039b A \u2282 \u039b A \u2200A \u2282 V. Use\u039b A t in place of \u039b A t in Algorithm 1 and let\u00c3 * be the corresponding persistent set returned by the modified algorithm. Let A * strong \u2282 A * be the largest subset of strongly persistent variables identifiable by Corollary 1 subject to the relaxations\u039b A and \u039b A . Then A * strong \u2282\u00c3 * strong . Remark 5. For approximate dual solvers for tighter relaxations like [36,37] there are analogues of strict arcconsistency, hence these are also integrally correct algorithms as in Definition 4 and we can also use these algorithms in Algorithm 1 with the obvious modifications.\nOptimal reparametrization for tighter relaxations and higher order models is beyond the scope of this paper.", "publication_ref": ["b35", "b36", "b37"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We tested our approach with initial and optimal reparametrizations (described in Section 6) on several datasets from different computer vision and machine learning benchmarks, 47 problem instances overall, see  [11] algorithms. For the first two methods we used our own implementation, and for the other the freely available code of Strandmark [38]. We were unable to compare to the method of Windheuser et al. [45], because the authors do not give a description for implementing their method in the higher order case and only provide experimental evaluation for problems with pairwise potentials, where their method coincides with MQPBO [18]. Implementation details. We employed TRWS as an approximate solver for Algorithm 1 and strong tree agreement as a consistency mapping (see Proposition 2) for most of the pairwise problems. We stop TRWS once it has either arrived at (i) tree-agreement; (ii) a small duality gap of 10 \u22125 ; (iii) when number of nodes with tree agreement did not increase over the last 100 iterations or (iv) overall 1500 iterations. For the higher-order models protein-interaction, cell-tracking and geo-surf we employed CPLEX [10] as an exact linear programming solver. We have run Algorithm 1 with boundary potentials computed as in (3.3) for all problems and with boundary potentials computed with the optimal reparametrization as in (6.3) for the pairwise problems. Datasets and Evaluation. We give a brief characterization of all datasets and report the obtained total percentage of persistent variables of our and competing methods in Table 3. The percentage of partial optimality is computed as follows: Suppose we have found a persistent labeling on set A \u2282 V. Then the percentage is 1 \u2212 u \u2208A log |Xu| u\u2208V log |Xu| . Note that by this formulation we take into account the size of the label space for each node. For an uniform label space the above formula equals |A| |V| . The latter measure was used in [40]. Remark 6. Note that in comparison to our conference paper [40], persistency results for some datasets with higher order potentials, which were solved with CPLEX are lower now. This is due to two reasons: First, we weight the size of the label space instead of simply counting the number of variables which are partially optimal. In models with nonuniform label space our method tends to find partial optimality for nodes with small label space, hence the new formula gives a smaller percentage. Second, our original research implementation contained subtle bugs which resulted in a higher number of wrongly assigned partially optimal nodes for these models. We apologize for reporting incorrect results in the experimental section of [40].\nThe problem instances teddy, venus, family, pano, Potts and geo-surf were made available by [14], while the datasets side-chain and protein-interaction were made available by [1].\nThe problem instances teddy and venus come from the disparity estimation for stereo vision [41]. None of the competing approaches was able to find even a single persistent variable for these datasets, presumably because of the large number of labels, whereas we labeled over one third of them as persistent in teddy, though none in venus.\nInstances named pano and family come from the photomontage dataset [41]. These problems have more complicated pairwise potentials than the disparity estimation problems, but less labels. For both datasets we found significantly more persistent variables than MQPBO, in particular, we were able to label more than a third of the variables in pano.\nWe also chose 12 relatively big energy minimization problems with grid structure and Potts interaction terms. The underlying application is a color segmentation problem previously considered in [39]. Our general approach reproduces results of [39] for the specific Potts model.\nWe considered also side-chain prediction problems in protein folding [46]. The datasets consist of pairwise graphical models with 32 \u2212 1971 variables and 2 \u2212 483 labels. The problems with fewer variables are densely connected and have very big label spaces, while the larger ones are less densely connected and have label space up to 81 variables.\nThe protein interaction models [12] aim to find the subset of proteins, which interact with each other. Roofduality based methods, i.e. Fix et at, GRD,HOCR [4,11,13] gave around a quarter of persistent labels. This is the only dataset where our methods gives worse results. Note that for higher-order models we do not provide an optimal reparametrization and hence our method is not provably better then the competitors. We consider this as a direction for future work.\nThe cell tracking problem consists of a binary higher order graphical model [17]. Given a sequence of microscopy images of a growing organism, the aim is to find the lineage tree of all cells. For implementation reasons we were not able to solve cell-tracking dataset with Ishikawa's [11] method. However Fix [4] reports that his method outperforms Ishikawa's method [11]. Other methods are not applicable even theoretically.\nLast, we took the higher order multi-label geometric surface labeling problems (denoted as geo-surf in Table 3) from [9]. The only instance having an integrality  [11] and our methods with boundary potentials computed as in (3.4) (Ours original) and as in (6.3) (Ours optimal). Notation \u2020 means inapplicability of the method. The columns #I,#L,#V,O denote the number of instances, labels, variables and the highest order of potentials respectively.  [11] methods and the generalized roof duality method by Strandmark and Kahl [13] cannot handle more than 2 labels. Hence we report our results without comparison.\nRuntime. The runtime of our algorithm mainly depends on the speed of the underlying solver for the local polytope relaxation. Currently there seems to be no general rule regarding the runtime of our algorithm, neither in the number of Algorithm 1-iterations nor in the number of TRWS [19]-iterations. We show three iteration counts for instances of the Potts dataset in Figure 4. Exemplary pictures comparing the pixels optimally labelled between Kovtuns's method [22] and our method for some Potts-models can be seen in Figure 9.", "publication_ref": ["b10", "b38", "b45", "b18", "b9", "b40", "b40", "b40", "b13", "b0", "b41", "b41", "b39", "b39", "b46", "b11", "b10", "b12", "b16", "b10", "b10", "b8", "b10", "b10", "b12", "b19"], "figure_ref": ["fig_3"], "table_ref": ["tab_4", "tab_4"]}, {"heading": "Conclusion and Outlook", "text": "We have presented a novel method for finding persistent variables for undirected graphical models. Empirically it outperforms all tested approaches with respect to the number of persistent variables found on every single dataset. Our method is general: it can be applied to graphical models of arbitrary order and type of potentials. Moreover, there is no fixed choice of convex relaxation for the energy minimization problem and also approximate solvers for these relaxations can be employed in our approach.\nIn the future we plan to significantly speed-up the implementation of our method and consider finer persistency criteria, as done in [32], where the subset-to-one class of persistency conditions was introduced, but no efficient algorithm for finding persistency in this class was proposed.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgments. This work has been supported by the German Research Foundation (DFG) within the program \"Spatio-/Temporal Graphical Models and Applications in Image Analysis\", grant GRK 1653.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "The probabilistic inference challenge (PIC2011)", "journal": "", "year": "", "authors": ""}, {"ref_id": "b1", "title": "Pseudo-Boolean optimization", "journal": "Discrete Applied Mathematics", "year": "2002", "authors": "E Boros; P L Hammer"}, {"ref_id": "b2", "title": "The dead-end elimination theorem and its use in protein sidechain positioning", "journal": "Nature", "year": "1992-04", "authors": "J Desmet; M D Maeyer; B Hazes; I Lasters"}, {"ref_id": "b3", "title": "A graph cut algorithm for higher-order Markov random fields", "journal": "", "year": "2011", "authors": "A Fix; A Gruber; E Boros; R Zabih"}, {"ref_id": "b4", "title": "Fixing max-product: Convergent message passing algorithms for MAP LPrelaxations", "journal": "Curran Associates, Inc", "year": "2007", "authors": "A Globerson; T Jaakkola"}, {"ref_id": "b5", "title": "Potts model, parametric maxflow and k-submodular functions", "journal": "", "year": "2013", "authors": "I Gridchyn; V Kolmogorov"}, {"ref_id": "b6", "title": "Roof duality, complementation and persistency in quadratic 0-1 optimization", "journal": "Math. Programming", "year": "1984", "authors": "P L Hammer; P Hansen; B Simeone"}, {"ref_id": "b7", "title": "Norm-product belief propagation: Primal-dual message-passing for approximate inference", "journal": "IEEE Transactions on Information Theory", "year": "2010", "authors": "T Hazan; A Shashua"}, {"ref_id": "b8", "title": "Recovering surface layout from an image", "journal": "Int. J. Comput. Vision", "year": "2007-10", "authors": "D Hoiem; A A Efros; M Hebert"}, {"ref_id": "b9", "title": "High-performance software for mathematical programming and optimization", "journal": "", "year": "", "authors": "Inc Ilog;  Ilog;  Cplex"}, {"ref_id": "b10", "title": "Transformation of general binary MRF minimization to the first-order case", "journal": "IEEE Trans. Pattern Anal. Mach. Intell", "year": "2011-06-01", "authors": "H Ishikawa"}, {"ref_id": "b11", "title": "Towards an integrated protein-protein interaction network: A relational markov network approach", "journal": "Journal of Computational Biology", "year": "2006", "authors": "A Jaimovich; G Elidan; H Margalit; N Friedman"}, {"ref_id": "b12", "title": "Generalized roof duality. Discrete Applied Mathematics", "journal": "", "year": "2012", "authors": "F Kahl; P Strandmark"}, {"ref_id": "b13", "title": "A comparative study of modern inference techniques for discrete energy minimization problem", "journal": "", "year": "2013", "authors": "J H Kappes; B Andres; F A Hamprecht; C Schn\u00f6rr; S Nowozin; D Batra; S Kim; B X Kausler; J Lellmann; N Komodakis; C Rother"}, {"ref_id": "b14", "title": "A bundle approach to efficient MAP-inference by Lagrangian relaxation", "journal": "", "year": "2012", "authors": "J H Kappes; B Savchynskyy; C Schn\u00f6rr"}, {"ref_id": "b15", "title": "Towards efficient and exact MAP-inference for large scale discrete computer vision problems via combinatorial optimization", "journal": "", "year": "2013", "authors": "J H Kappes; M Speth; G Reinelt; C Schn\u00f6rr"}, {"ref_id": "b16", "title": "A discrete chain graph model for 3d+t", "journal": "", "year": "", "authors": "B X Kausler; M Schiegg; B Andres; M S Lindner; U K\u00f6the; H Leitte; J Wittbrodt; L Hufnagel; F A Hamprecht"}, {"ref_id": "b17", "title": "The red area denotes pixels which could not be labelled persistently. Contrary to ours the Kovtun's method allows to eliminate separate labels, which is denoted by different intensity of the red color: the more intensive is red, the less labels were eliminated. cell tracking with high misdetection robustness", "journal": "Springer", "year": "2012", "authors": ""}, {"ref_id": "b18", "title": "On partial optimality in multi-label MRFs", "journal": "", "year": "2008", "authors": "P Kohli; A Shekhovtsov; C Rother; V Kolmogorov; P Torr"}, {"ref_id": "b19", "title": "Convergent tree-reweighted message passing for energy minimization", "journal": "IEEE Trans. Pattern Anal. Mach. Intell", "year": "2006-10", "authors": "V Kolmogorov"}, {"ref_id": "b20", "title": "Generalized roof duality and bisubmodular functions", "journal": "Discrete Applied Mathematics", "year": "2012", "authors": "V Kolmogorov"}, {"ref_id": "b21", "title": "MRF energy minimization and beyond via dual decomposition", "journal": "IEEE Trans. Pattern Anal. Mach. Intell", "year": "2011", "authors": "N Komodakis; N Paragios; G Tziritas"}, {"ref_id": "b22", "title": "Partial optimal labeling search for a NPhard subclass of (max,+) problems", "journal": "", "year": "2003", "authors": "I Kovtun"}, {"ref_id": "b23", "title": "Sufficient condition for partial optimality for (max, +) labeling problems and its usage", "journal": "Control Systems and Computers", "year": "2011", "authors": "I Kovtun"}, {"ref_id": "b24", "title": "Vertex packings: Structural properties and algorithms", "journal": "", "year": "", "authors": "G L Nemhauser; L E Trotter"}, {"ref_id": "b25", "title": "Optimizing binary MRFs via extended roof duality", "journal": "", "year": "2007", "authors": "C Rother; V Kolmogorov; V S Lempitsky; M Szummer"}, {"ref_id": "b26", "title": "A study of Nesterov's scheme for Lagrangian decomposition and MAP labeling", "journal": "IEEE", "year": "2011", "authors": "B Savchynskyy; J H Kappes; S Schmidt; C Schn\u00f6rr"}, {"ref_id": "b27", "title": "Global MAP-optimality by shrinking the combinatorial search area with convex relaxation", "journal": "", "year": "2013", "authors": "B Savchynskyy; J H Kappes; P Swoboda; C Schn\u00f6rr"}, {"ref_id": "b28", "title": "Efficient MRF energy minimization via adaptive diminishing smoothing", "journal": "", "year": "2012", "authors": "B Savchynskyy; S Schmidt; J H Kappes; C Schn\u00f6rr"}, {"ref_id": "b29", "title": "Syntactic analysis of two-dimensional visual signals in the presence of noise", "journal": "Kibernetika", "year": "1976", "authors": "M Schlesinger"}, {"ref_id": "b30", "title": "Evaluation of a first-order primal-dual algorithm for MRF energy minimization", "journal": "Springer", "year": "2011", "authors": "S Schmidt; B Savchynskyy; J H Kappes; C Schn\u00f6rr"}, {"ref_id": "b31", "title": "Exact and Partial Energy Minimization in Computer Vision", "journal": "", "year": "2013", "authors": "A Shekhovtsov"}, {"ref_id": "b32", "title": "Maximum persistency in energy minimization", "journal": "", "year": "2009", "authors": "A Shekhovtsov"}, {"ref_id": "b33", "title": "Maximum persistency in energy minimization", "journal": "", "year": "2014", "authors": "A Shekhovtsov"}, {"ref_id": "b34", "title": "LP-relaxation of binarized energy minimization", "journal": "", "year": "2008", "authors": "A Shekhovtsov; V Kolmogorov; P Kohli; V Hlavac; C Rother; P Torr"}, {"ref_id": "b35", "title": "Approximate Inference in Graphical Models using LP Relaxations", "journal": "", "year": "2010", "authors": "D Sontag"}, {"ref_id": "b36", "title": "Efficiently searching for frustrated cycles in MAP inference", "journal": "AUAI Press", "year": "2012", "authors": "D Sontag; D K Choe; Y Li"}, {"ref_id": "b37", "title": "Tightening lp relaxations for map using message passing", "journal": "AUAI Press", "year": "2008", "authors": "D Sontag; T Meltzer; A Globerson; T Jaakkola; Y Weiss"}, {"ref_id": "b38", "title": "Generalized roof duality", "journal": "", "year": "", "authors": "P Strandmark"}, {"ref_id": "b39", "title": "Partial optimality via iterative pruning for the Potts model", "journal": "", "year": "2003", "authors": "P Swoboda; B Savchynskyy; J H Kappes; J Schn\u00f6rr"}, {"ref_id": "b40", "title": "Partial optimality by pruning for MAPinference with general graphical models", "journal": "", "year": "2014", "authors": "P Swoboda; B Savchynskyy; J H Kappes; C Schn\u00f6rr"}, {"ref_id": "b41", "title": "A comparative study of energy minimization methods for Markov random fields with smoothness-based priors", "journal": "IEEE Trans. Pattern Anal. Mach. Intell", "year": "2008", "authors": "R Szeliski; D Zabih; O Scharstein; V Veksler; A Kolmogorov; M F Agarwala; C Tappen;  Rother"}, {"ref_id": "b42", "title": "MAP estimation via agreement on trees: message-passing and linear programming", "journal": "IEEE Trans. Inf. Theor", "year": "2005-11", "authors": "M J Wainwright; T S Jaakkola; A S Willsky"}, {"ref_id": "b43", "title": "Graphical models, exponential families, and variational inference", "journal": "Found. Trends Mach. Learn", "year": "2008-01", "authors": "M J Wainwright; M I Jordan"}, {"ref_id": "b44", "title": "A linear programming approach to max-sum problem: A review", "journal": "IEEE Trans. Pattern Anal. Mach. Intell", "year": "2007-07-01", "authors": "T Werner"}, {"ref_id": "b45", "title": "Generalized roof duality for multi-label optimization: Optimal lower bounds and persistency", "journal": "", "year": "2001", "authors": "T Windheuser; H Ishikawa; D Cremers"}, {"ref_id": "b46", "title": "Minimizing and learning energy functions for side-chain prediction", "journal": "Journ. of Comput. Biol", "year": "2008", "authors": "C Yanover; O Schueler-Furman; Y Weiss"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. An exemple graph containing inside nodes (yellow with crosshatch pattern) and boundary nodes (green with diagonal pattern). The blue dashed line encloses the set A. Boundary edges are those crossed by the dashed line.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Illustration of one iteration of Algorithm 1.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Lemma 4 .4From \u03b4(x 0 ) \u2208 arg min \u00b5\u2208\u039b A\u00ca A,x 0 (\u00b5) (6.7) follows \u03b4(x 0 ) \u2208 arg min \u00b5\u2208\u039b A\u00ca \u03c8 A,x 0 (\u00b5), which means: if x 0 satisfies the persistency criterion of Corollary 1 w.r.t. potentials \u03b8 then it satisfies it w.r.t. the reparametrized potentials \u03b8 \u03c8 .", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Remark 3 .3Lemma 4 holds for any polytope containing all integer solutions, i.e. \u039b A \u2287 M A and hence it holds also when \u039b A = M A . In this case it corresponds to the non-relaxed persistency criterion provided by Theorem 1.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "xv\u2208Xv (\u03b8 uv (x u , x v ) \u2212 \u03b8 uv (y u , x v )) ; (7.9) for x u = y u : \u03b8 \u03c8 uv,yu (y u ) = max xv\u2208Xv \u03b8 \u03c8 uv (y u , x v ) = = max xv\u2208Xv (\u03b8 uv (y u , x v ) \u2212 \u03b8 uv (y u , y v ) \u2212 \u03b8 uv (y u , x v ))= \u2212\u03b8 uv (y u , y v ) . (7.10)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "13) and the border term\u03b3 uv,yu (x u ) = min xv\u2208Xv (\u03b8 uv (x u , x v ) \u2212 \u03b8 uv (y u , x v )) .(7.14)    ", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "1 )1All definitions, our persistency criterion and Algorithm 1 admit a straightforward generalization. Analoguously to Definition 1 define for a subset of nodes A \u2282 V the boundary nodes as\u2202V A := {u \u2208 A : \u2203v \u2208 V\\A, \u2203e \u2208 E s.t. u, v \u2208 e} (8.2)and the boundary edges as\u2202E A := {e \u2208 E : \u2203u \u2208 A, \u2203v \u2208 V\\A s.t. u, v \u2208 e} . (8.3) The equivalent of boundary potential in Definition 3 for e \u2208 \u2202E A i\u015d \u03b8 e,y (x) := \uf8f1 \uf8f2 \uf8f3 max x\u2208Xe :x |A\u2229e =x |A\u2229e \u03b8 e (x), x |A\u2229e = y |A\u2229e mi\u00f1 x\u2208Xe :x |A\u2229e =x |A\u2229e \u03b8 e (x), x |A\u2229e = y |A\u2229e .", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Comparison between partial optimality methods. Detailed descriptions are presented in Section 1.1.", "figure_data": "brevity we restrict ourselves here to the pairwise case. Anextension to the higher order case is discussed in Section 8.Problem (2.1) is equivalent to the integer linear problemmin \u00b5\u2208\u039b V v\u2208V xv \u2208Xv\u03b8v(xv)\u00b5v(xv) +uv\u2208E xuv \u2208Xuv \u03b8uv(xuv)\u00b5uv(xuv)"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Hence x * and A * fulfill the conditions of Corollary 1, which proves persistency. Choice of Solver. All our results are independent of the specific algorithm one uses to solve the relaxed problems min \u00b5\u2208\u039b A\u00ca A,y , provided it returns an exact solution. However this can be an issue for large-scale datasets, where classical exact LP solvers like e.g. the simplex nodes A t , nodes V\\A t .", "figure_data": "Having solved the infer-Boundary costs are as-signed to boundary nodes on \u2202V A t .ence problem: nodes are fractional, nodes disagree with pre-vious labeling,Variables are pruned. A new set A t+1 is con-structed.nodes agree..1)"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Comparison between our method and Com-biLP[27].", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "We describe each dataset and the corresponding experiments in detail below. Competing methods. We compared our method to MQPBO [18, 34], Kovtun's method [22], Generalized Roof Duality (GRD) by Kahl and Strandmark [13], Fix et al's [4] and Ishikawa's Higer Order Clique Reduction (HOCR)", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Percentage of persistent variables obtained by methods [18], [22], [13], [4],", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Iterations needed by TRWS [19] in Algorithm 1 for three instances from the Potts dataset. gap has 968 variables with 7 labels each and has ternary terms. Note that MQPBO cannot handle ternary terms, Fix et al's [4] Ishikawa's", "figure_data": "1000TRWS iterations100pfau clownfish crops051015202530354045Algorithm 1 iterationsFigure 4:"}], "formulas": [{"formula_id": "formula_0", "formula_text": "x\u2208X V E V (x) := v\u2208V \u03b8 v (x v ) + uv\u2208E \u03b8 uv (x u , x v ) , (2.1)", "formula_coordinates": [2.0, 334.1, 550.96, 231.42, 20.06]}, {"formula_id": "formula_1", "formula_text": "s.t. \u00b5w(xw) \u2208 {0, 1} for w \u2208 V \u222a E, xw \u2208 Xw ,(2.2)", "formula_coordinates": [3.0, 49.19, 383.58, 245.33, 7.86]}, {"formula_id": "formula_2", "formula_text": "xv \u2208V \u00b5v(xv) = 1, v \u2208 V, xv \u2208V \u00b5uv(xu, xv) = \u00b5u(xu), xu \u2208 Xu, uv \u2208 E, xu\u2208V \u00b5uv(xu, xv) = \u00b5v(xv), xv \u2208 Xv, uv \u2208 E, \u00b5w(xw) \u2265 0, w \u2208 V \u222a E, xw \u2208 Xw . (2.3)", "formula_coordinates": [3.0, 64.17, 422.42, 230.35, 42.08]}, {"formula_id": "formula_3", "formula_text": "\u00b5 0 v = \u00b5 * v \u2200v \u2208 A?", "formula_coordinates": [3.0, 46.49, 713.06, 75.43, 12.19]}, {"formula_id": "formula_4", "formula_text": "\u00b5 u , u \u2208 V, is integral if \u00b5 u (x u ) \u2208 {0, 1} \u2200x u \u2208 X u .", "formula_coordinates": [3.0, 317.48, 492.59, 248.03, 21.61]}, {"formula_id": "formula_5", "formula_text": "\u00b5\u2208M V E V (\u00b5).", "formula_coordinates": [3.0, 334.09, 588.23, 57.11, 10.27]}, {"formula_id": "formula_6", "formula_text": "(x 0 ,x) := x 0 v , v \u2208 A, x v , v \u2208 V\\A", "formula_coordinates": [4.0, 46.49, 224.82, 131.97, 23.18]}, {"formula_id": "formula_7", "formula_text": "Proposition 1. A partial labeling x 0 \u2208 X A is persistent if there holds \u2200x \u2208 X V\\A : x 0 \u2208 argmin x\u2208X A E V ((x,x)) . (3.1)", "formula_coordinates": [4.0, 46.49, 257.15, 248.03, 52.23]}, {"formula_id": "formula_8", "formula_text": "x\u2208X V E V (x) = mi\u00f1 x\u2208X V\\A min x\u2208X A E V ((x,x)) . (3.2)", "formula_coordinates": [4.0, 76.64, 341.7, 217.88, 15.53]}, {"formula_id": "formula_9", "formula_text": "X u \u2192 R a\u015d \u03b8 uv,yu (x u ) := max xv\u2208Xv \u03b8 uv (x u , x v ), y u = x u min xv\u2208Xv \u03b8 uv (x u , x v ), y u = x u .", "formula_coordinates": [4.0, 58.85, 623.92, 222.11, 43.43]}, {"formula_id": "formula_10", "formula_text": "E A,y (x) := E A (x) + uv\u2208\u2202E A : u\u2208\u2202V A\u03b8 uv,yu (x u ) , (3.4) min max x u x 0 u x v u v \u03b8 uv,x 0 u (x u )", "formula_coordinates": [4.0, 65.39, 101.1, 514.98, 625.29]}, {"formula_id": "formula_11", "formula_text": "E A (x) = u\u2208A \u03b8 u (x u )+ uv\u2208E:u,v\u2208A \u03b8 uv (x uv )", "formula_coordinates": [4.0, 344.7, 295.02, 164.88, 17.07]}, {"formula_id": "formula_12", "formula_text": "x v = y v , v \u2208 A x v , v / \u2208 A", "formula_coordinates": [4.0, 369.08, 426.59, 87.24, 21.61]}, {"formula_id": "formula_13", "formula_text": "x 0 \u2208 X A on a subset A \u2282 V is persistent if x 0 \u2208 argmin x\u2208X A\u00ca A,x 0 (x) ,(3.5)", "formula_coordinates": [4.0, 317.48, 502.3, 248.03, 31.54]}, {"formula_id": "formula_14", "formula_text": "0 | A = y. Then it holds for uv \u2208 \u2202E A , u \u2208 \u2202V A that \u03b8 uv (x 0 u , x v ) +\u03b8 uv,y (x u ) \u2212\u03b8 uv,y (x 0 u ) \u2264 \u03b8 uv (x u , x v ) . (3.6) Proof. The case x u = x 0 u is trivial. Otherwise, by Defini- tion 3, inequality (3.6) is equivalent to \u03b8 uv (x 0 u , x v ) + min xv\u2208Xv \u03b8 uv (x u , x v ) \u2212 max xv\u2208Xv \u03b8 uv (x 0 u , x v ) \u2212 \u03b8 uv (x u , x v ) \u2264 0 . (3.7)", "formula_coordinates": [4.0, 317.48, 600.45, 248.03, 120.35]}, {"formula_id": "formula_15", "formula_text": "x \u2208 arg min x\u2208X V x| A =x 0 | A E V (x) .", "formula_coordinates": [5.0, 119.66, 147.81, 101.69, 21.9]}, {"formula_id": "formula_16", "formula_text": "E V (x) = E A (x 0 ) + E V\\A (x) + uv\u2208\u2202E A \u03b8 uv (x 0 u ,x v ) (3.9) =E A (x 0 ) + uv\u2208\u2202E A\u03b8 uv,y (x 0 u ) + E V\\A (x) + uv\u2208\u2202E A \u03b8 uv (x 0 u ,x v ) \u2212\u03b8 uv,y (x 0 u ) =\u00ca A,x 0 (x 0 ) + E V\\A (x) + uv\u2208\u2202E A \u03b8 uv (x 0 ,x v ) \u2212\u03b8 uv,x 0 (x 0 u ) \u2264\u00ca A,x 0 (x ) + E V\\A (x ) + uv\u2208\u2202E A \u03b8 uv (x 0 , x v ) \u2212\u03b8 uv,x 0 (x 0 u ) (3.10) =E A (x ) + uv\u2208\u2202E A\u03b8 uv,x 0 (x u ) + E V\\A (x ) + uv\u2208\u2202E A \u03b8 uv (x 0 u , x v ) \u2212\u03b8 uv,x 0 (x 0 u ) \u2264E A (x )+E V\\A (x )+ uv\u2208\u2202E A \u03b8 uv (x u , x v ) = E V (x ). (3.11)", "formula_coordinates": [5.0, 55.51, 195.69, 240.67, 238.75]}, {"formula_id": "formula_17", "formula_text": "). La- beling x 0 \u2208 X A on A \u2282 V fulfilling the condition \u03b4(x 0 ) \u2208 argmin \u00b5\u2208\u039b A\u00ca A,x 0 (\u00b5) (3.12)", "formula_coordinates": [5.0, 46.49, 574.61, 248.03, 41.59]}, {"formula_id": "formula_18", "formula_text": "\u03b4(x 0 ) \u2208 argmin \u00b5\u2208\u039b A ,\u00b5\u2208{0,1} dim \u039b A\u00caA,x 0 (\u00b5) (3.13) because \u03b4(x 0", "formula_coordinates": [5.0, 46.49, 656.91, 248.03, 30.6]}, {"formula_id": "formula_19", "formula_text": "Data: G = (V, E), \u03b8 u : X u \u2192 R, \u03b8 uv : X uv \u2192 R Result: A * \u2282 V, x * \u2208 X A * Initialize: Choose \u00b5 0 \u2208 argmin \u00b5\u2208\u039b V E V (\u00b5) A 0 = {u \u2208 V : \u00b5 0 u \u2208 {0, 1} |Xu| } t = 0 repeat Set x t u such that \u00b5 t u (x t u ) = 1, u \u2208 A t Choose \u00b5 t+1 \u2208 argmin \u00b5\u2208\u039b A t\u00ca A t ,x t (\u00b5) t = t + 1 W t = {u \u2208 \u2202V A t\u22121 : \u00b5 t u (x t\u22121 u ) = 1} A t = {u \u2208 A t\u22121 : \u00b5 t u \u2208 {0, 1} |Xu| }\\W t until A t = A t\u22121 ; A * = A t Set x * \u2208 X A * such that \u00b5 t u (x * u ) = 1", "formula_coordinates": [5.0, 327.44, 115.09, 207.27, 179.14]}, {"formula_id": "formula_20", "formula_text": "\u03b4(x * ) \u2208 argmin \u00b5\u2208\u039b A * \u00caA * ,x * (\u00b5) . (4", "formula_coordinates": [5.0, 371.32, 617.8, 181.91, 13.66]}, {"formula_id": "formula_21", "formula_text": "x * v \u2208 X v \u2200v \u2208 V implies x * \u2208 argmin x\u2208X V E V (x).", "formula_coordinates": [6.0, 46.49, 415.44, 248.03, 24.74]}, {"formula_id": "formula_22", "formula_text": "c v = x v , \u00b5 v (x v ) = 1 #, \u00b5 v / \u2208 {0, 1} |Xv| .", "formula_coordinates": [6.0, 66.41, 575.28, 125.67, 22.08]}, {"formula_id": "formula_23", "formula_text": "\u2200v \u2208 V, x v \u2208 X v , \u00b5 v (x v ) := \uf8f1 \uf8f2 \uf8f3 1, c v = x v 0, c v / \u2208 {x v , #}, 1/|X v |, c v = #", "formula_coordinates": [6.0, 49.94, 640.75, 234.96, 35.52]}, {"formula_id": "formula_24", "formula_text": "x * = x |A .", "formula_coordinates": [7.0, 494.96, 242.03, 41.16, 11.53]}, {"formula_id": "formula_25", "formula_text": "\u03b4(x 0 ) \u2208 argmin \u00b5\u2208\u039b A * strong\u00ca A * strong ,x 0 (\u00b5) (5.2)", "formula_coordinates": [7.0, 345.93, 330.2, 219.58, 16.23]}, {"formula_id": "formula_26", "formula_text": "B | A = x A .", "formula_coordinates": [7.0, 322.72, 504.59, 44.64, 11.23]}, {"formula_id": "formula_27", "formula_text": "B (\u00b5) on B it holds that \u00b5 * v (x A v ) = 1 \u2200v \u2208 A.", "formula_coordinates": [7.0, 317.48, 518.12, 248.03, 23.79]}, {"formula_id": "formula_28", "formula_text": "\u03b4(x 0 ) \u2208 argmin \u00b5\u2208\u039b A * strong\u00ca A * strong ,x 0 (\u00b5) , (5.3)", "formula_coordinates": [8.0, 72.73, 323.57, 221.79, 16.23]}, {"formula_id": "formula_29", "formula_text": "\u03b8 \u03d5 v (x v ) := \u03b8 v (x v ) \u2212 u\u2208nb(v) \u03d5 v,u (x v ) , (6.1) \u03b8 \u03d5 uv (x u , x v ) := \u03b8 uv (x u , x v ) + \u03d5 v,u (x v ) + \u03d5 u,v (x u ) (6.", "formula_coordinates": [8.0, 326.69, 128.7, 238.83, 40.16]}, {"formula_id": "formula_30", "formula_text": "0, y u = x u min xv\u2208Xv \u03b8 \u03d5 uv (x u , x v ) \u2212 max xv\u2208Xv \u03b8 \u03d5 uv (y u , x v ), y u = x u . (6.", "formula_coordinates": [8.0, 340.08, 437.61, 217.24, 38.15]}, {"formula_id": "formula_31", "formula_text": "xv\u2208Xv \u03b8 \u03d5 uv (x u , x v ) \u2212 max xv\u2208Xv \u03b8 \u03d5 uv (y u , x v ).", "formula_coordinates": [8.0, 372.94, 501.31, 163.85, 16.16]}, {"formula_id": "formula_32", "formula_text": "\u03d5 v,u (x v ), u \u2208 \u2202V A , uv \u2208 \u2202E A , x v \u2208 X v . Clearly \u2206 \u03d5 uv (x u ) \u2264 min xv\u2208Xv (\u03b8 \u03d5 uv (x u , x v ) \u2212 \u03b8 \u03d5 uv (y u , x v )) = min xv\u2208Xv (\u03b8 uv (x u , x v ) + \u03d5 v,u (x v ) \u2212 \u03b8 uv (y u , x v ) \u2212 \u03d5 v,u (x v )) = min xv\u2208Xv (\u03b8 uv (x u , x v ) \u2212 \u03b8 uv (y u , x v", "formula_coordinates": [8.0, 317.48, 591.5, 248.03, 102.2]}, {"formula_id": "formula_33", "formula_text": "\u03d5 u,v (x v ) = \u2212\u03b8 uv (y u , x v ) , (6.5)", "formula_coordinates": [9.0, 116.11, 165.65, 178.41, 9.65]}, {"formula_id": "formula_34", "formula_text": "\u03c8 u,v (x v ) = \u2212\u03b8 uv (y u , x v ), u \u2208 V, uv \u2208 E , (6.6)", "formula_coordinates": [9.0, 71.13, 264.69, 223.39, 9.65]}, {"formula_id": "formula_35", "formula_text": "x (x u ) \u2212\u03b8 \u03c8 uv,x 0 u (x 0 u ) \u2265 \u03b8 uv,x 0 u (x u ) \u2212\u03b8 uv,x 0 u (x 0 u ) and henc\u00ea E \u03c8 A,x 0 (\u00b5) \u2212\u00ca \u03c8 A,x 0 (x 0 ) (6.4) \u2265\u00ca A,x 0 (\u00b5) \u2212\u00ca A,x 0 (x 0 ) \u2265 0 (6.8)", "formula_coordinates": [9.0, 46.49, 487.08, 474.76, 72.31]}, {"formula_id": "formula_36", "formula_text": "A * x 0 \u2282 A \u03c8, * x 0 .", "formula_coordinates": [9.0, 317.48, 107.41, 52.66, 14.27]}, {"formula_id": "formula_37", "formula_text": "\u03b8 uv (x u , x v ) = 0, x u = x v \u03b1, x u = x v", "formula_coordinates": [9.0, 329.93, 154.02, 235.58, 33.46]}, {"formula_id": "formula_38", "formula_text": "\u03d5 v,u (x v ) = 0 \u2200u, v \u2208 V, uv \u2208 E, x v \u2208 X v .", "formula_coordinates": [9.0, 317.48, 189.88, 248.03, 21.61]}, {"formula_id": "formula_39", "formula_text": "X V \u2192 X V is called (strictly) improving if for all x \u2208 X V such that p(x) = x there holds E V (p(x)) \u2264 E V (x) (resp. E V (p(x)) < E V (x)).", "formula_coordinates": [9.0, 317.48, 447.61, 248.03, 33.56]}, {"formula_id": "formula_40", "formula_text": "x) v = p v (x v ), where p v : X v \u2192 X v are idempo- tent, i.e. p v (p v (x v )) = p v (x v ) for all x v \u2208 X v .", "formula_coordinates": [9.0, 317.48, 528.77, 248.03, 21.61]}, {"formula_id": "formula_41", "formula_text": "P v \u2208 R Xv\u00d7Xv by P v,ii = 1, p v (i ) = i 0, p v (i ) = i .", "formula_coordinates": [10.0, 97.52, 96.61, 197.0, 21.61]}, {"formula_id": "formula_42", "formula_text": "(P \u00b5) v = i \u2208Xv P v,ii \u00b5 v (i ) = P v \u00b5 v ; (7.1) (P \u00b5) uv = P u \u00b5 uv P v .", "formula_coordinates": [10.0, 89.38, 143.54, 205.14, 30.54]}, {"formula_id": "formula_43", "formula_text": "dimension I = v\u2208V |X v | + uv\u2208E |X uv |.", "formula_coordinates": [10.0, 46.49, 232.2, 166.95, 17.07]}, {"formula_id": "formula_44", "formula_text": "min x\u2208X V (E V (x) \u2212 E V (p(x))) = min x\u2208X V \u03b8, (I \u2212 [p])\u03b4(x) = min x\u2208X V (I \u2212 [p]) \u03b8, \u03b4(x) = min \u00b5\u2208M V (I \u2212 [p]) \u03b8, \u00b5(7.2)", "formula_coordinates": [10.0, 58.11, 286.43, 236.41, 48.25]}, {"formula_id": "formula_45", "formula_text": "X V \u2192 X V is \u039b V -improving for potentials \u03b8 \u2208 R I if min \u00b5\u2208\u039b V (I \u2212 [p]) \u03b8, \u00b5 = 0 . (7.3)", "formula_coordinates": [10.0, 46.49, 425.87, 248.03, 48.08]}, {"formula_id": "formula_46", "formula_text": "Proposition 4. If mapping p is (strictly) \u039b V -improving then it is (strictly) improving.", "formula_coordinates": [10.0, 46.49, 547.12, 248.03, 20.72]}, {"formula_id": "formula_47", "formula_text": "p v (i) = y v , if v \u2208 A i, if v / \u2208 A (7.4)", "formula_coordinates": [10.0, 115.93, 622.43, 178.59, 20.69]}, {"formula_id": "formula_48", "formula_text": "\u03b8 \u03c8 u (x u ) = \u03b8 u (x u ) + v\u2208nb(u) \u03b8 uv (x u , y v ) ,(7.5)", "formula_coordinates": [10.0, 344.02, 184.02, 221.49, 22.6]}, {"formula_id": "formula_49", "formula_text": "\u03b8 \u03c8 uv (x u , x v ) = \u03b8 uv (x u , x v ) \u2212 \u03b8 uv (x u , y v ) \u2212 \u03b8 uv (y u , x v ) .", "formula_coordinates": [10.0, 326.02, 212.07, 230.95, 12.69]}, {"formula_id": "formula_50", "formula_text": "x\u2208X V v\u2208V \u03b2 v (x v )+ uv\u2208E \u03b2 uv (x u , x v )+ uv\u2208\u2202E A : u\u2208\u2202V \u00c2 \u03b8 \u03c8", "formula_coordinates": [10.0, 334.8, 325.44, 197.04, 22.83]}, {"formula_id": "formula_51", "formula_text": "\u03b2 u (x u ) = \u03b8 u (x u ) + v\u2208nb(u) \u03b8 uv (x u , y v ), u \u2208 A 0, u \u2208 V\\A (7.7) \u03b2 uv (x u , x v ) = \u03b8 uv (x u , x v ) \u2212 \u03b8 uv (x u , y v ) \u2212 \u03b8 uv (y u , x v ), u, v \u2208 A 0, otherwise . (7.8)", "formula_coordinates": [10.0, 320.36, 403.06, 245.16, 116.9]}, {"formula_id": "formula_52", "formula_text": "\u03b8 \u03c8 uv,yu (x u ) = min xv\u2208Xv \u03b8 \u03c8 uv (x u , x v ) = = min xv\u2208Xv (\u03b8 uv (x u , x v ) \u2212 \u03b8 uv (x u , y v ) \u2212 \u03b8 uv (y u , x v )) = \u2212\u03b8 uv (x u , y v ) + min", "formula_coordinates": [10.0, 327.44, 565.92, 219.86, 53.27]}, {"formula_id": "formula_53", "formula_text": "x\u2208X V v\u2208V \u03b3 v (x v )+ uv\u2208E \u03b3 uv (x u , x v )+ u\u2208\u2202E \u00c2", "formula_coordinates": [11.0, 78.08, 174.97, 177.42, 20.76]}, {"formula_id": "formula_54", "formula_text": "\u03b3 u (x u ) = \u03b8 u (x u ) \u2212 \u03b8 u (y u ), u \u2208 A 0, u \u2208 V\\A (7.12) \u03b3 uv (x u , x v ) = \u03b8 uv (x u , x v ) \u2212 \u03b8 uv (y u , y v ), u, v \u2208 A 0, otherwise . (7.", "formula_coordinates": [11.0, 56.45, 225.72, 238.07, 74.61]}, {"formula_id": "formula_55", "formula_text": "\u03c6 u,v (x u ) = \u2212\u03b8 uv (x u , y v ), u \u2208 A 0, u \u2208 V\\A . (7.15)", "formula_coordinates": [11.0, 79.09, 498.97, 215.43, 20.69]}], "doi": "10.1007/978-3-540-45243-052.2"}