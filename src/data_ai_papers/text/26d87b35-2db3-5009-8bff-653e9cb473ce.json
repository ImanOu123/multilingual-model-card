{"title": "Piecewise Rigid Scene Flow", "authors": "Christoph Vogel; Konrad Schindler; Stefan Roth", "pub_date": "", "abstract": "Estimating dense 3D scene flow from stereo sequences remains a challenging task, despite much progress in both classical disparity and 2D optical flow estimation. To overcome the limitations of existing techniques, we introduce a novel model that represents the dynamic 3D scene by a collection of planar, rigidly moving, local segments. Scene flow estimation then amounts to jointly estimating the pixelto-segment assignment, and the 3D position, normal vector, and rigid motion parameters of a plane for each segment. The proposed energy combines an occlusion-sensitive data term with appropriate shape, motion, and segmentation regularizers. Optimization proceeds in two stages: Starting from an initial superpixelization, we estimate the shape and motion parameters of all segments by assigning a proposal from a set of moving planes. Then the pixel-to-segment assignment is updated, while holding the shape and motion parameters of the moving planes fixed. We demonstrate the benefits of our model on different real-world image sets, including the challenging KITTI benchmark. We achieve leading performance levels, exceeding competing 3D scene flow methods, and even yielding better 2D motion estimates than all tested dedicated optical flow techniques.", "sections": [{"heading": "Introduction", "text": "Scene flow estimation is the task of estimating dense 3D surface shape as well as a dense 3D motion field from two (or more) views of a scene taken at two (or more) time steps [20]. Applications include motion analysis and motion capture, driver assistance and autonomous navigation, and virtual or augmented reality. The 3D scene flow generalizes two classical problems of computer vision, dense stereo matching and dense optical flow estimation. Yet, despite significant progress in both stereo [4,9,26] and 2D optical flow estimation [5,16,17], existing 3D scene flow techniques [e.g., 3,10,24] have remained quite limited in comparison. Perhaps surprisingly, the additional information available in stereo motion sequences has not been leveraged to the extent that 3D scene flow outperforms dedicated stereo or 2D optical flow techniques at their respective task.\nMuch like stereo or 2D motion estimation, scene flow estimation is ill-posed due to the 3D equivalent of the aperture problem, and thus requires prior assumptions on geometry and motion. Shortcomings of general-purpose regularization have prompted the development of stronger priors, e.g., encouraging locally rigid motion [22] as is common to many scenes. This mirrors a general trend toward more expressive priors in stereo [e.g., 4] and optical flow [12,16].\nWe posit that existing 3D scene flow techniques have been limited by the underlying representation, and propose to model the scene as a collection of planar regions, each undergoing a rigid motion. Following prior work in stereo [4], we argue that most scenes of interest consist of regions with a consistent motion pattern, into which they can be segmented -at least implicitly -during scene flow estimation. Since a larger support is required to fit a plane and its rigid motion (9 unknowns) reliably, we base the initial estimation not on individual pixels, but on a superpixel segmentation of the reference image. From these segments we generate a large number of candidate planes in 3D object space, each with an associated rigid motion. Scene flow estimation is then cast as a labeling problem, which assigns each pixel to a segment and each segment to a rigidly moving 3D plane.\nAlthough the superpixels significantly simplify and stabilize the inference, they lead to inaccuracies at flow boundaries, since the initial segmentation does not take into account depth or motion discontinuities. We address this by going back to the pixel level and updating the assignment of pixels to segments, thus removing artifacts due to the superpixel discretization. We also show how to explicitly include occlusion reasoning both at the segment and pixel level.\nWe make the following contributions: (i) We propose a novel 3D scene flow approach based on piecewise planar, rigidly moving regions, including regularization between these regions as well as explicit occlusion reasoning; (ii) we formulate an appropriate (discrete, non-submodular) energy toward inference in this model; and (iii) report scene flow estimates of hitherto unmatched accuracy. In experiments on challenging, realistic data the proposed approach substantially outperforms three state-of-the-art 3D scene flow methods. To the best of our knowledge, our method is moreover the first to realize the theoretical advantage afforded by the additional information from stereo sequences, and outperforms recent dedicated stereo and optical flow algorithms in challenging settings on their respective task.", "publication_ref": ["b19", "b3", "b8", "b25", "b4", "b15", "b16", "b2", "b9", "b23", "b21", "b11", "b15", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "The term \"scene flow\" was coined by Vedula et al. [20], who were among the first, if not the first, to estimate both dense 3D geometry and a dense 3D motion field from multiview image data. Estimation proceeds in two independent steps: First, 2D optical flow fields are estimated for all views (without requiring that they must be projections of the same 3D flow). Then a 3D flow field is fitted to them. Wedel et al. [24] proceed the other way around. Stereo disparity is precomputed for each time step; then the optical flow for a reference view and the disparity differences for the other view are estimated. Rabe et al. [13] integrate a Kalman filter into this approach to yield smooth flow fields over multiple frames. One of the limitations of these approaches is that a 2D regularizer is used, which encourages smooth projections, and not smooth 3D scene flow.\nHuguet and Devernay [10] were possibly the first to estimate geometry and flow in an integrated manner with a variational formulation. Basha et al. [3] parameterize the scene flow by depth and a 3D motion vector w.r.t. a reference view, and estimate all parameters jointly with a 3D extension of the widely used optical flow method of Brox et al. [5]. This approach was modified by Vogel et al. [22], who argue that the total variation prior on the 3D motion field is biased for realistic baselines, and instead encourage locally rigid motion. The local rigidity assumption, which for sparse motion estimation dates back to at least Adiv [1], has also been used in 3D motion capture with explicit surface models [e.g., 6]. Also related is the optical flow approach of Nir et al. [12], in which the flow field is (over-) parameterized by explicitly searching for rigid motion parameters, and then encouraging their smoothness.\nValgaerts et al. [18] generalize the problem by assuming that only the camera intrinsics, but not the relative pose are known. In the presence of a dominant rigid motion (\"background motion\") they alternatingly estimate both the relative camera pose and the scene flow.\nCommon to these previous approaches to 3D scene flow is that they penalize deviations from spatial smoothness, typically in a robust way. In the context of stereo disparity and optical flow, explicit modeling of discontinuities by means of segmentation or layer-based formulations has a long history [23] and has recently gained renewed attention: Bleyer et al. [4] estimate disparity by assuming the scene to be segmented into planar superpixels and parameterizing their geometry. Segment-based stereo is also advocated by Yamaguchi et al. [26], who additionally penalize deviations from the (not segment-based) initialization. This method was further extended to epipolar flow, i.e. optical flow that enforces epipolar motion as hard constraint [27]. Sun et al. [16] estimate general 2D motion by decomposition into several layers, which enables occlusion reasoning. Unger et al. [17] compute optical flow by parameterizing the motion per segment with 2D affine transformations, and also perform occlusion handling. A key difference, aside from estimating 2D and not 3D motion, is that they do not consider any inter-patch regularization, such that the motion fields assigned to different segments are independent of each other. Discrete optimization based on fusion of proposals has been applied before to 2D optical flow estimation by Lempitsky et al. [11]. Here, such an optimization scheme is employed for 3D scene flow.", "publication_ref": ["b19", "b23", "b12", "b9", "b2", "b4", "b21", "b0", "b11", "b17", "b22", "b3", "b25", "b26", "b15", "b16", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Piecewise Rigid Model for 3D Scene Flow", "text": "In contrast to typical approaches to 3D scene flow, our novel model parameterizes the scene as a collection of piecewise planar regions, each of which moves rigidly over time. As we show below, each region can be described using nine parameters, which are estimated by means of energy minimization. To that end we define an energy function that assigns each pixel to a segment and each segment to the 3D geometry and motion of a plane. This allows us to estimate the 3D scene flow and depth for every pixel of a reference view. The segmentation of the scene is only part of the internal representation and is not returned as an output.\nWe formulate our model for the classical case of two consecutive image pairs acquired with a calibrated stereo rig: Irrespective of their true configuration the two views will be referred to as \"left\" and \"right\", denoted with subscripts l, r, while the two time steps will be denoted with superscripts 0, 1. The scene is parameterized w.r.t. a reference frame I 0 l (the \"left\" camera at time 0). For convenience of notation we assume w.o.l.g. that all (perspective) cameras have identical intrinsics K. The reference camera hence has the projection matrix (K|0); the projection matrix of the \"right\" image at time 0 is written as (M|m).\nEach moving 3D plane \u03c0 = \u03c0(R, t, n) in the scene is described by nine parameters: A scaled normal vector n, a rotation matrix R, and a translation vector t. Note that since any plane visible in the reference image cannot contain the origin (camera center), we can denote a plane n t x = d with normal n and distance d to the origin by the scaled normal n := n/d. Also note that we do not (need to) assume that the pixels belonging to each moving plane form a connected component. Parameterizing the scene with moving planes also conveniently allows the pixel locations assigned to each plane to be transformed easily between images or mapped into 3D space using the corresponding homographies.\nThe energy we define below is minimized in two steps: Starting from a set of superpixels, each segment is first labeled as belonging to one out of a large set of rigidly moving proposal planes, while keeping the pixel-to-segment assignment fixed; then, pixels are re-assigned to the best-fitting segment, while keeping the planar geometry and motion of each segment fixed. Note that while the model is based on segments, the aim here is not segmentation into semantic objects. Rather, the segments support accurate scene flow estimation. Over-segmentation is deliberately accepted, both to ensure correct depth and flow estimation for nonplanar and articulated objects, and to maximize boundary recall, even at the cost of spurious segment boundaries that do not correspond to depth or motion discontinuities.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Model overview", "text": "Our aim is to estimate depth and a 3D scene flow vector for each pixel of the reference frame I 0 l . For now we assume that we have a finite set of possible rigidly moving proposal planes \u03a0 = {\u03c0 j } in 3D. We then search for two mappings: A mapping S : I 0 l \u2192 S, which assigns each pixel p \u2208 I 0 l to a segment s \u2208 S; and a mapping P : S \u2192 \u03a0 to assign each segment to a rigidly moving 3D plane \u03c0 \u2208 \u03a0.\nWe thus formulate scene flow estimation as minimizing\nE(P, S) = E D (P, S) + \u03bbE R (P, S) + \u03bcE S (S). (1)\nAs is common in correspondence problems, a data term E D ensures consistency of the corresponding appearance between the four views. The regularization term E R encourages piecewise smooth geometry and motion, and a boundary term E S additionally assesses the quality of the segmentation. We now define each of the three terms in detail.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data term", "text": "The data term models the assumption that corresponding points across the four images should be similar in appearance. This amounts to a total of 4 constraints per pixel (two stereo constraints at time steps 0 and 1, and two optical flow constraints for the two left, respectively right images; see Fig. 2, left). Our representation with rigidly moving 3D planes induces homographies, which map pixels from the reference view I 0 l to the remaining views:\nH 0 r (\u03c0) = (M \u2212 mn t )K \u22121 (2a) H 1 l (\u03c0) = K(R \u2212 tn t )K \u22121 (2b) H 1 r (\u03c0) = (MR \u2212 Mt + m)n t K \u22121 (2c)\nFor convenience, we define H 0 l (\u03c0) to be the identity that maps the reference view onto itself, and denote the moving 3D plane of a pixel p as \u03c0 p = P (S(p)). We can thus define optical flow-induced appearance constraints across time as\nD f i = p\u2208I 0 l \u03c1 H 0 i (\u03c0 p )p, H 1 i (\u03c0 p )p , i \u2208 {l, r} ,(3)\nand stereo constraints between the simultaneous views as\nD s t = p\u2208I 0 l \u03c1 H t l (\u03c0 p )p, H t r (\u03c0 p )p , t \u2208 {0, 1} . (4)\nThe function \u03c1(\u2022, \u2022) may simply encourage brightness constancy by penalizing brightness changes, e.g. through a robust, truncated penalty. Alternative choices include the more robust census transform [28]. The complete data term is given as the sum of the four terms in Eqs. ( 3) and ( 4):\nE D (P, S) = D s 0 + D s 1 + D f l + D f r .\n(5)", "publication_ref": ["b27"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Shape and motion regularization", "text": "The regularization terms shall encourage piecewise smooth 3D shape, as well as a piecewise smooth 3D motion field. Since each segment is assigned to one rigidly moving plane, smoothness within a segment is always satisfied; we thus only need to consider the segment boundaries. Assume that p and q are two adjacent pixels that are assigned to different moving planes \u03c0 p = P(S(p)) and \u03c0 q = P(S(q)).\nLet us begin with the shape. To assess the regularity of the boundary between two pixels, we consider them to be tiny square patches of equal area. We further assume that the boundary shared by the two pixels in image space has the (2D) endpoints c 1 and c 2 . If we now project each endpoint onto each of the two 3D planes, we obtain the 3D endpoints c 1 p , c 1 q , c 2 p and c 2 q (see Fig. 2, right). Since we have chosen p and q to lie on different planes, the pixel boundaries will in general not coincide in 3D space, and our goal is to penalize the boundary distances. To that end we first denote the vectors between the 3D endpoints as\nd 1 = c 1 p \u2212c 1 q and d 2 = c 2 p \u2212c 2 q .\nSince we are using planes as primitives, the 3D distance along the boundary is a convex combination of the endpoint distances \u03b1||d 1 || + (1 \u2212 \u03b1)||d 2 ||. In order to take into account surface curvature as well, we not only evaluate the distance of the endpoints themselves, but also the distance after shifting the endpoints along the respective normals n p , n q . Denoting the difference of the normals as d n = n p \u2212 n q , we define a distance function 2). The weight \u03b3 balances plain boundary distance vs. curvature. The shape regularizer is then defined as the integral of (f \u03b3 ) 2 along the boundary (w.r.t. \u03b1) and along the normal direction (w.r.t. \u03b2), which yields a closed form:\nf \u03b3 (\u03b1, \u03b2) = ||\u03b1(d 1 + \u03b3\u03b2d n ) + (1 \u2212 \u03b1)(d 2 + \u03b3\u03b2d n )|| (see Fig.\nE 1 R (P, S) = (p,q)\u2208N w p,q \u03c8 3 1 0 1 \u22121 f \u03b3 (\u03b1, \u03b2) 2 d\u03b2d\u03b1 (6) = (p,q)\u2208N w p,q \u03c8 ||d 1 || 2 +||d 2 || 2 + d 1 , d 2 +\u03b3 2 ||d n || 2 .\nHere, N are all neighboring pixels of the reference image (8-neighborhood), the length of the edge between the pixels is given by w p,q , and \u03c8(\u2022) denotes a penalty function.\nNote that we can sum over all neighboring pixels, since for adjacent pixels on the same segment f \u03b3 \u2261 0. The arbitrary scaling factor 3 is used for mathematical convenience. The motion field is regularized in a similar manner, by integrating over the distances between adjacent motion vectors using\nd m i = R p c i p + t p \u2212 c i p \u2212 (R q c i q + t q \u2212 c i q )\n, as well as the differences between the (rotated) normals\nd m n = (R p n p \u2212 n p ) \u2212 (R q n q \u2212 n q ), which leads to E 2 R (P, S) = (7) (p,q)\u2208N w p,q \u03c8 ||d m 1 || 2 +||d m 2 || 2 + d m 1 , d m 2 +\u03b3 2 ||d m n || 2 .\nThe regularizer E R (P, S) is given as the sum of the shape and motion regularizers. To yield the necessary robustness against occasional discontinuities, we use truncated penalties \u03c8(y) = min( \u221a y, \u03b7) (with thresholds \u03b7 1 , \u03b7 2 ).\nNote that the proposed regularizer is not restricted to 3D, since one is free to replace the distances. If 2D regularization in the image plane is desired instead, one can mimic the regularizer of [18] by replacing the 3D distances with disparity differences, differences between 2D optical flow vectors, and changes of the disparity difference over time.", "publication_ref": ["b17"], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "Segmentation regularization", "text": "As the data term and the previous regularization term consider not only the motion and geometry of the moving plane assigned to each segment, but also which segment each pixel is being assigned to, it is necessary to regularize the segmentation further to encourage spatially coherent (though not necessarily compact) segments. We employ a segment regularization term similar in spirit to the approach of [21], which encourages smooth segments whose boundaries coincide with image edges. The energy is defined as\nE S (S) = (p,q)\u2208N , S(p) =S(q) exp \u2212a|I 0 l (p)\u2212I 0 l (q)| \u03c3 I (p,q)+ (8) + p\u2208I 0 l 0, \u2203 e \u2208 E(s i ) : ||e \u2212 p|| \u221e < N S \u221e, else.\nThe first term is a contrast-sensitive pairwise Potts model, which penalizes segment transitions such that transitions that coincide with large image gradients are penalized less. The standard deviation \u03c3 I (p, q) is estimated from a 50\u00d750 window around p, q; we set a = 10, = 0.01. As above, N denotes the 8-neighborhood of each pixel. The second term ensures that each pixel can only be assigned to those segments s i , for which a seed point e \u2208 E(s i ) is less than N S pixels away (w.r.t. the \u221e distance); we set N S = 25. The motivation is twofold: First, this prevents segments from getting too large, such that the scene is not overly simplified by the assumed piecewise planarity and piecewise rigid motion. Second, since only a subset of possible segment assignments needs to be considered at each pixel during optimization, a significant speedup is achieved. The set of seed points E(s i ) for a segment contains the center pixel of the segment s i in the initial superpixelization, as well as all pixels from a regularly-spaced grid (with spacing N S ) that fell into the respective initial superpixel.", "publication_ref": ["b20"], "figure_ref": [], "table_ref": []}, {"heading": "Approximate inference", "text": "We perform inference in our model by approximately minimizing the energy from Eq. (1) w.r.t. the segmentation S and the rigid motion of each segment, represented as the mapping P. To bootstrap this process, we obtain an initial segmentation S through a superpixelization of the reference image. To that end we first minimize the segmentation energy E S from Eq. (8) alone, which amounts to the superpixelization approach of [21]. Seed points E on a regular grid ensure a sufficiently fine tiling. Strongly non-convex segments are split into (near-)convex pieces.\nNext, we update depth and motion by approximately minimizing the energy w.r.t. the segment-to-plane map P, assuming for now that the segmentation S is fixed to the superpixelization. Since the segmentation regularization E S does not depend on P, we only need to consider the data term E D and the shape and motion regularization E 1,2 R . Recall that these regularizers can only incur penalties at segment boundaries, since neighboring pixels within a segment will be assigned to the same moving plane, thus incur no regularization penalty. For efficiency, we simplify the energy by computing the penalty \u03c8(\u2022) in Eqs. ( 6) and ( 7) from the endpoints of a segment boundary. Since the length of the actual boundary can be precomputed, this is much more efficient than computing the penalty for each boundary pixel. This approximation is reasonable, since the superpixel segments are near-convex. The optimization is performed over a set of proposal planes with their associated motion (see Sec. 3.6) using fusion moves [11] and QPBO [15].\nFinally, we update the segmentation S assuming a fixed P. Note that the preceding simplification cannot be applied in this case, since the segmentation itself is updated. We thus minimize Eq. (1) directly. This is still reasonably efficient, because the region constraint from Eq. (8) ensures that pixels can only be assigned to a certain segment if they lie within N S pixels from one of its seed points. Consequently, optimization involves \u03b1-expansion (with QPBO) in a local graph of at most (2N S \u2212 1) 2 nodes. Although the energy is not submodular, unlabeled nodes rarely occur.", "publication_ref": ["b20", "b10", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Proposal generation", "text": "To perform inference over the depth and motion of each segment, we require a comprehensive proposal set of 3D planes along with their rigid motions. We can generate these from the output of other scene flow algorithms or by combining the results of stereo and optical flow algorithms (see Sec. 4.2). To that end we fit a 3D plane to each superpixel segment, and estimate its rigid motion from the flow field(s). In either case, fitting must be robust to a potentially large amount of outliers, caused both by inaccurate depth and motion estimates and by superpixels not being aligned with surface or motion boundaries. We address this by robustly minimizing the transfer error: We first generate an initial solution by minimizing the quadratic transfer error using efficient algebraic methods, and then refine the rigidly moving proposal planes by gradient descent on the robust transfer error (Lorentzian penalty). Each locally fitted proposal is considered valid for the closest \u2248 100 segments in its spatial vicinity. Thus, fusion moves can be made efficient using a partial (local) instantiation of the graph.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Occlusion handling", "text": "The data term as defined in Eq. (5) does not contain any form of occlusion handling; every pixel is always assumed visible. Since our scene representation is defined in 3D, it allows for explicit occlusion reasoning. This is particularly interesting in case of scene flow, since we have four views of the scene (2 cameras at 2 time steps). Hence, even if a pixel is occluded in a subset of the views, there may still be a view pair where no occlusion takes place.\nWe apply occlusion handling to all view pairs for which we formulate a data term (c.f . Eq. ( 5)); for the remainder we assume that we consider one of the view pairs with its data term, D. We apply the well-known principle of using a fixed occlusion penalty \u03b8, if a certain pixel p (in the reference view) is occluded in at least one view of a pair. Since in case of an occlusion, the corresponding pixel locations are not related in their appearance, we do not apply the usual data penalty from Eqs. (3) or (4). Note that we only have to consider occlusions between pixels in different segments, since a visible 3D plane cannot occlude itself.\nTo ease understanding, we describe occlusion reasoning directly during a fusion/expansion move of the inference procedure (Sec. 3.5), i.e. we solve a binary optimization problem. Assume for now that the segment-to-plane mapping P is fixed, and we update the per-pixel segmentation S. We address updating the segment-to-plane mapping later. Let x p = 0 denote that a pixel p remains in its current segment, and x p = 1 indicate that p will be switched to the candidate segment. We can thus rewrite the data term without occlusion reasoning (Sec. 3.2) as the Boolean function\nD(x) = p\u2208I 0 l u 0 p (1 \u2212 x p ) + u 1 p x p , (9\n)\nwhere x is the binary vector of all pixel assignments, u 0 p is the data penalty for p being in its current segment, and u 1 p the data penalty for switching p to the candidate segment.\nWe now consider whether a pixel p is occluded or not, which depends both on its binary segment assignment x p , and on whether there is any other pixel q (or possibly multiple pixels) that occludes p. Determining whether q leads to an occlusion in turn depends on its segment assignment x q . We call O i p the set of all pixel-assignment pairs (q, j) for which pixel q occludes pixel p if x p = i and x q = j. Then we replace Eq. (9) with the occlusion-sensitive data term\nD O (x) = p\u2208I 0 l \u03b8 + 1 i=0\u00fb i p [x p = i] (q,j)\u2208O i p [x q = j] (10)\nHere,\u00fb i p = u i p \u2212 \u03b8 is the difference of the (unoccluded) data penalty and the occlusion cost \u03b8, and [\u2022] denotes the Iverson bracket. To understand this, let us consider a single pixel p. The summand becomes\u00fb 0 p , if x p = 0 and the product equals 1, which is the case if there is no pixel that could possibly occlude p, or if all possibly occluding pixels q are assigned a segment x q in which they do not lead to an occlusion. The data cost for a pixel p thus equals \u03b8 in case of an occlusion, and otherwise the standard data penalty u i p . We detect potential occlusions using z-buffering. The per-pixel penalty may be a higher-order pseudo-Boolean function (|O i p | > 1), depending on the number of possibly occluding pixels. To facilitate applying QPBO, we reduce these higher-order terms to quadratic ones by introducing auxiliary variables [2].\nTo update the segment-to-plane mapping P given a fixed segmentation S, we perform inference at the segment level for efficiency (see Sec. 3.5), i.e. by computing penalties for entire segments. The Boolean functions for this case are the same as in Eq. ( 10), but with the variables x p , x q representing segments rather than pixels. A segment is considered occluded if its center is occluded.", "publication_ref": ["b3", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "To give an impression of what the proposed model is capable of on realistic data, we first report qualitative results on a street scene from [19], which is recorded from a vehicle as it approaches a roundabout with multiple moving traffic participants. Both the independent object motion, but also complex occlusion patterns pose difficult challenges. Fig. 1 shows the estimated 3D scene flow (left), and the results after various processing stages (right). The segment-based scene reconstruction (Segment) without occlusion handling already gives fairly plausible results in unoccluded areas, but assigns incorrect depth and motion to regions not visible in the reference image (best seen immediately left of the pedestrian). By adding the segment-based occlusion model (Segment & Occlusion), the occlusion regions are properly detected and their motion is extrapolated in a more realistic manner. Finally, the per-pixel refinement (Per-Pixel & Occlusion) visibly improves object and occlusion boundaries. To ensure the necessary robustness, we use aggressive truncation values \u03b7 1 = \u03b7 2 = 1 for the robust penalty, assuming that depth and motion changes exceeding 1 world unit are due to discontinuities. We use \u03c1(a, b) = min(|a\u2212b|, \u03b6), i.e. brightness constancy truncated at \u03b6 = 10% of the intensity range, and set \u03bb = 0.1, \u03bc= 0.1, \u03b8 = 0.03, \u03b3 = 1.", "publication_ref": ["b18"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Comparison with 2D scene flow", "text": "For comparison with other scene flow methods from the literature [10,18,24] we consider the synthetic scene of [10], which consists of two independently rotating hemispheres in front of a plane. Our method performs slightly better than competing ones (Table 1), even though the dataset does not conform to our assumptions: The spheres are not well approximated well by planar segments, and the texture gradients do not coincide with depth/motion boundaries, so the initial over-segmentation is rather arbitrary.", "publication_ref": ["b9", "b17", "b23", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "KITTI dataset", "text": "For quantitative evaluation we test our method on twoframe stereo pairs from the KITTI dataset [8]. The dataset provides images (1240 \u00d7 376 pixels) recorded with a calibrated stereo rig on a car, for benchmarking of optical flow and stereo algorithms in the context of automotive applications. Semi-dense ground truth data was acquired by a laser [18] [ scanner attached to the car. Having stereo pairs from consecutive video frames, the dataset also fulfills the requirements for scene flow estimation (see Fig. 3 for examples). We use the training portion of the dataset for detailed quantitative analysis, and the test portion (non-public ground truth) to compare to the state of the art.\nThe KITTI dataset provides a very challenging testbed for today's stereo, optical flow and scene flow algorithms: First, pixel displacements in the data set are large in general, exceeding 150 pixels for stereo and 250 pixels for optical flow. Second, the images exhibit strongly varying lighting conditions and many non-Lambertian surfaces, especially translucent windows and specular glass and metal surfaces. Third, the high speed of the forward motion creates large regions on the image boundaries that move out of the field of view between frames, such that no correspondence can be established. To identify these areas one might use the vehicles' ego-motion, which is not available, however. Appearance modeling. We address the challenging lighting conditions using the census transform [28] over a 7\u00d77 neighborhood to measure the data fidelity \u03c1, which has been shown to cope well with complex outdoor lighting [14]. In detail, we scale the Hamming distances by 1/24 for the census data term, see [28], and set \u03bb = 10\u03bc, \u03b3 = 1, \u03ba = 1.05. The parameters are fixed for all image sets. Visibility. To cope with areas that are out of bounds, i.e. not visible in all four images, we let the stereo and 2D flow algorithms from the proposal generator predict which pixels are out-of-bounds and encourage the scene flow estimate to stay near that prediction. Let V 1 l , V 0 r and V 1 r be the predicted binary visibility masks for all but the reference image (out-of-bounds: 0, pixel visible: 1), and further let \u0393 j i [\u2022] be a binary function that determines whether its argument lies within the boundaries of image I j i . To penalize deviations from the predicted visibility, we add an energy term\nE V (P, S) = \u03ba p\u2208I 0 l V 0 r (p) \u2212 \u0393 0 r H 0 r (\u03c0 p )p + (11) V 1 l (p) \u2212 \u0393 1 l H 1 l (\u03c0 p )p + V 1 r (p) \u2212 \u0393 1 r H 1 r (\u03c0 p )p .\nEvaluation. In Table 2 we compare the average errors on the KITTI training set. Because of the uncertainty in the LiDAR data, the ground truth is not directly suited to assess sub-pixel accuracy. Therefore, the recommended error metric is to threshold the deviations from ground truth with a range of inlier/outlier thresholds, Z, and count the fraction of outliers for each Z. Note that the benchmark is biased towards methods that focus on the dominant background, be- cause independently moving objects usually have no ground truth points. Nonetheless, we believe that this dataset is better suited for scene flow evaluation than the very limited, synthetic datasets used before [e.g., 10,22]. Following the KITTI protocol, we distinguish between an evaluation on all pixels (All) and only on unoccluded pixels (Noc), and report results for error thresholds of 2, 3, 4 and 5 pixels.\nAs baselines we use our implementations of three methods: L 1 -regularized 3D scene flow (LSF, [3]); locally rigid 3D scene flow (Rig, [22]); and independently derived 2D stereo (semi-global matching [9]) and optical flow (census data term, total generalized variation [25] regularization), indicated by (S+F). At time of writing the results of the 2D baseline rank 13 th in both stereo and optical flow among published methods in the official KITTI ranking. Our baseline implementations are on par with the published benchmark results, suggesting that they match the state of the art.\nWe quantitatively evaluate the individual steps of our approach, as well as different variants: First, we report results only based on superpixels (PRSSeg) and after per-pixel refinement (PRSPix). Second, we compare regularization in the image (-2D, with \u03b7 1 = \u03b7 2 = 20 and \u03bc = 1/30) and 3D regularization (-3D, with \u03b7 1 = \u03b7 2 = 2.5 and \u03bc = 0.2). We use less aggressive truncation thresholds here, since the lighting conditions are more challenging than in Fig. 1. Third, we include results with occlusion modeling (-O) and without. Finally, we distinguish the use of various proposal sets: All experiments default to proposals from the stereo and flowderived 2D baseline technique (S+F). The suffix (+R) denotes that a proposal set composed from locally rigid scene flow (Rig, [22]) is additionally used. Finally, we optionally make use of egomotion proposals (+E), by estimating the dominant 3D motion using our proposal fitting technique on the 2D baseline output. In contrast to [27] we do not make any hard assumptions about the scene, or prefer epipolar motion in the energy. Rather, epipolar motion is one of several proposal solutions, which are used to minimize an energy that can cope with general, non-epipolar motion.\nFrom the results in Table 2 we first observe that the perpixel refinement (PRSPix) improves results significantly in all measures, on average by 9% to 15%. 2D and 3D regularization lead to rather similar results, possibly explained by the fact that the evaluation does not have ground truth for 3D scene flow, but only for disparity and 2D optical flow. We thus mostly rely on the 2D regularizer, and note that better 3D benchmarks are needed for quantitative evaluation of 3D scene flow methods. Additional occlusion reasoning (-O) improves the results, especially for motion estimates in occluded areas, but performance in the stereo case slightly decreases. While occlusion reasoning has a positive overall effect on the accuracy, the effect is somewhat limited here (unlike Fig. 1) due to the limited amount of independent motion in KITTI. Like any proposal-based optimization technique [e.g., 4,11], the quality of the proposals is of some importance. Still, already the 2D proposal set from S+F alone is sufficient to surpass all our baselines by a large margin, including two recent 3D scene flow techniques [3,22], on average by 33%. Incorporating additional proposals, the average improvement becomes 38% (PRSPix-2D+R) and 44% (PRSPix-2D+R+E).\nWe have evaluated our model on the official KITTI benchmark. On a dual-core Intel i7 machine and for one KITTI scene, our current implementation takes \u2248 12s for fitting 2 proposal sets and 2000 segments, \u2248 32s for persegment optimization and \u2248 18s for per-pixel optimization. The data term is the bottleneck. At time of writing our method (PRSPix-2D+R+E) ranks 1 st out of 28 published approaches for optical flow in all measures, and 3 rd out of 25 published methods in stereo, while (PRSPix-2D) ranked 3 th and 5 th respectively. Similar performance is only achieved by [27], which can only handle epipolar motion. Our approach, in contrast, can cope with independent object motion (see Fig. 1). Moreover, it strongly outperforms another 3D scene flow technique [7], even on its semi-dense output. Finally, even the best general 2D optical flow method is surpassed. To the best of our knowledge, this is the first scene flow method to outperform optical flow algorithms w.r.t. reprojection error on a benchmark set, and thus realize the advantage stemming from the additional stereo information.", "publication_ref": ["b7", "b17", "b27", "b13", "b27", "b9", "b21", "b2", "b21", "b8", "b24", "b21", "b26", "b3", "b10", "b2", "b21", "b26", "b6"], "figure_ref": ["fig_2", "fig_0", "fig_0", "fig_0"], "table_ref": ["tab_1", "tab_1"]}, {"heading": "Conclusion", "text": "We have shown that modeling a dynamic scene with local regions corresponding to rigidly moving planes can lead to compelling results for the task of joint geometry and 3D  motion estimation. The proposed model achieves accurate geometry and motion boundaries by refining an initial oversegmentation of the scene, and allows for occlusion reasoning. We show that our method substantially outperforms previous dense scene flow approaches on a challenging data set, and even surpasses dedicated state-of-the-art stereo and optical flow techniques at their respective task. Its main limitation are scenes with strongly non-rigid motion or extreme curvature, where the piecewise planar and rigid approximation does not hold. In practice such scenes are quite rare.\nIn future work we plan to extend our method to sequences of more than two frames, which we believe our formulation is well-suited for. Another interesting avenue would be to embed object-level semantic image understanding into the segmentation scheme.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Determining three-dimensional motion and structure from optical flow generated by several moving objects", "journal": "PAMI", "year": "1985", "authors": "G Adiv"}, {"ref_id": "b1", "title": "Optimizing binary MRFs with higher order cliques", "journal": "", "year": "2008", "authors": "A M Ali; A A Farag; G L Gimel'farb"}, {"ref_id": "b2", "title": "Multi-view scene flow estimation: A view centered variational approach", "journal": "", "year": "", "authors": "T Basha; Y Moses; N Kiryati"}, {"ref_id": "b3", "title": "Object stereo -Joint stereo matching and object segmentation", "journal": "", "year": "2011", "authors": "M Bleyer; C Rother; P Kohli; D Scharstein; S N Sinha"}, {"ref_id": "b4", "title": "High accuracy optical flow estimation based on a theory for warping", "journal": "", "year": "2004", "authors": "T Brox; A Bruhn; N Papenberg; J Weickert"}, {"ref_id": "b5", "title": "Multi-view scene capture by surfel sampling: From video streams to non-rigid 3D motion, shape and reflectance", "journal": "IJCV", "year": "2002", "authors": "R L Carceroni; K N Kutulakos"}, {"ref_id": "b6", "title": "Scene flow estimation by growing correspondence seeds", "journal": "", "year": "2011", "authors": "J Cech; J Sanchez-Riera; R P Horaud"}, {"ref_id": "b7", "title": "Are we ready for autonomous driving", "journal": "", "year": "2012", "authors": "A Geiger; P Lenz; R Urtasun"}, {"ref_id": "b8", "title": "Stereo processing by semiglobal matching and mutual information", "journal": "PAMI", "year": "2008", "authors": "H Hirschm\u00fcller"}, {"ref_id": "b9", "title": "A variational method for scene flow estimation from stereo sequences", "journal": "", "year": "2007", "authors": "F Huguet; F Devernay"}, {"ref_id": "b10", "title": "FusionFlow: Discretecontinuous optimization for optical flow estimation", "journal": "", "year": "2008", "authors": "V Lempitsky; S Roth; C Rother"}, {"ref_id": "b11", "title": "Over-parameterized variational optical flow", "journal": "IJCV", "year": "2008", "authors": "T Nir; A Bruckstein; R Kimmel"}, {"ref_id": "b12", "title": "Dense, robust, and accurate motion field estimation from stereo image sequences in real-time", "journal": "", "year": "2010", "authors": "C Rabe; T M\u00fcller; A Wedel; U Franke"}, {"ref_id": "b13", "title": "Pushing the limits of stereo using variational stereo estimation", "journal": "", "year": "2012", "authors": "R Ranftl; S Gehrig; T Pock; H Bischof"}, {"ref_id": "b14", "title": "Optimizing binary MRFs via extended roof duality", "journal": "", "year": "", "authors": "C Rother; V Kolmogorov; V Lempitsky; M Szummer"}, {"ref_id": "b15", "title": "Layered image motion with explicit occlusions, temporal consistency, and depth ordering", "journal": "NIPS", "year": "2010", "authors": "D Sun; E B Sudderth; M J Black"}, {"ref_id": "b16", "title": "Joint motion estimation and segmentation of complex scenes with label costs and occlusion modeling", "journal": "", "year": "2012", "authors": "M Unger; M Werlberger; T Pock; H Bischof"}, {"ref_id": "b17", "title": "Joint estimation of motion, structure and geometry from stereo sequences", "journal": "", "year": "2010", "authors": "L Valgaerts; A Bruhn; H Zimmer; J Weickert; C Stoll; C Theobalt"}, {"ref_id": "b18", "title": "Differences between stereo and motion behaviour on synthetic and realworld stereo sequences", "journal": "", "year": "2008", "authors": "T Vaudrey; C Rabe; R Klette; J Milburn"}, {"ref_id": "b19", "title": "Three-dimensional scene flow", "journal": "", "year": "1999", "authors": "S Vedula; S Baker; R Collins; T Kanade; P Rander"}, {"ref_id": "b20", "title": "Superpixels and supervoxels in an energy optimization framework", "journal": "", "year": "2010", "authors": "O Veksler; Y Boykov; P Mehrani"}, {"ref_id": "b21", "title": "3D scene flow estimation with a rigid motion prior", "journal": "", "year": "2011", "authors": "C Vogel; K Schindler; S Roth"}, {"ref_id": "b22", "title": "Representing moving images with layers", "journal": "IEEE TIP", "year": "1994", "authors": "J Wang; E Adelson"}, {"ref_id": "b23", "title": "Efficient dense scene flow from sparse or dense stereo data", "journal": "", "year": "2008", "authors": "A Wedel; C Rabe; T Vaudrey; T Brox; U Franke; D Cremers"}, {"ref_id": "b24", "title": "Convex Approaches for High Performance Video Processing", "journal": "", "year": "2012", "authors": "M Werlberger"}, {"ref_id": "b25", "title": "Continuous Markov random fields for robust stereo estimation", "journal": "", "year": "2012", "authors": "K Yamaguchi; T Hazan; D Mcallester; R Urtasun"}, {"ref_id": "b26", "title": "Robust monocular epipolar flow estimation", "journal": "", "year": "2013", "authors": "K Yamaguchi; D Mcallester; R Urtasun"}, {"ref_id": "b27", "title": "Non-parametric local transforms for computing visual correspondence", "journal": "", "year": "1994", "authors": "R Zabih; J Woodfill"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. Example scene from [19]: (left) Jointly estimated 3D geometry, 3D motion vectors, and superpixel boundaries, rendered from a slightly different viewpoint. (right) Processing steps and final result of piecewise rigid scene flow estimation. Estimated depth, the lateral 3D motion component, and the re-projected 2D flow are shown. Occlusion areas are highlighted in white.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 .2Figure 2. (left) Data terms in the two-view case; green: homographies. (right) Illustration of the regularization scheme.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 .3Figure 3. Example results from the KITTI benchmark. (left) input images; (middle) disparity w.r.t. reference frame; (right) flow reprojected to the reference image. We use the color scheme of the benchmark. See text for details.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "FLOW", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Table1. 2D errors for the \"sphere\" sequence[10].", "figure_data": "10][24]OursRMSE 2D Flow0.630.690.770.63RMSE Disparity3.83.810.92.84RMSE Scene Flow 1.762.512.551.73"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Average error rates for the KITTI training set. Error given as the percentage of erroneous pixels (deviation to ground truth above threshold of Z pixels) in non-occluded areas (Noc) and over the full image (All).", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "E(P, S) = E D (P, S) + \u03bbE R (P, S) + \u03bcE S (S). (1)", "formula_coordinates": [3.0, 328.82, 93.64, 224.34, 10.82]}, {"formula_id": "formula_1", "formula_text": "H 0 r (\u03c0) = (M \u2212 mn t )K \u22121 (2a) H 1 l (\u03c0) = K(R \u2212 tn t )K \u22121 (2b) H 1 r (\u03c0) = (MR \u2212 Mt + m)n t K \u22121 (2c)", "formula_coordinates": [3.0, 355.26, 314.16, 197.91, 46.37]}, {"formula_id": "formula_2", "formula_text": "D f i = p\u2208I 0 l \u03c1 H 0 i (\u03c0 p )p, H 1 i (\u03c0 p )p , i \u2208 {l, r} ,(3)", "formula_coordinates": [3.0, 329.79, 424.35, 223.37, 26.01]}, {"formula_id": "formula_3", "formula_text": "D s t = p\u2208I 0 l \u03c1 H t l (\u03c0 p )p, H t r (\u03c0 p )p , t \u2208 {0, 1} . (4)", "formula_coordinates": [3.0, 329.1, 472.55, 224.07, 26.01]}, {"formula_id": "formula_4", "formula_text": "E D (P, S) = D s 0 + D s 1 + D f l + D f r .", "formula_coordinates": [3.0, 361.48, 570.02, 144.74, 13.4]}, {"formula_id": "formula_5", "formula_text": "d 1 = c 1 p \u2212c 1 q and d 2 = c 2 p \u2212c 2 q .", "formula_coordinates": [4.0, 57.39, 167.76, 238.1, 25.26]}, {"formula_id": "formula_6", "formula_text": "f \u03b3 (\u03b1, \u03b2) = ||\u03b1(d 1 + \u03b3\u03b2d n ) + (1 \u2212 \u03b1)(d 2 + \u03b3\u03b2d n )|| (see Fig.", "formula_coordinates": [4.0, 57.39, 277.82, 238.61, 21.96]}, {"formula_id": "formula_7", "formula_text": "E 1 R (P, S) = (p,q)\u2208N w p,q \u03c8 3 1 0 1 \u22121 f \u03b3 (\u03b1, \u03b2) 2 d\u03b2d\u03b1 (6) = (p,q)\u2208N w p,q \u03c8 ||d 1 || 2 +||d 2 || 2 + d 1 , d 2 +\u03b3 2 ||d n || 2 .", "formula_coordinates": [4.0, 63.1, 342.68, 234.42, 60.6]}, {"formula_id": "formula_8", "formula_text": "d m i = R p c i p + t p \u2212 c i p \u2212 (R q c i q + t q \u2212 c i q )", "formula_coordinates": [4.0, 100.38, 506.41, 192.41, 13.31]}, {"formula_id": "formula_9", "formula_text": "d m n = (R p n p \u2212 n p ) \u2212 (R q n q \u2212 n q ), which leads to E 2 R (P, S) = (7) (p,q)\u2208N w p,q \u03c8 ||d m 1 || 2 +||d m 2 || 2 + d m 1 , d m 2 +\u03b3 2 ||d m n || 2 .", "formula_coordinates": [4.0, 57.35, 530.56, 239.04, 59.49]}, {"formula_id": "formula_10", "formula_text": "E S (S) = (p,q)\u2208N , S(p) =S(q) exp \u2212a|I 0 l (p)\u2212I 0 l (q)| \u03c3 I (p,q)+ (8) + p\u2208I 0 l 0, \u2203 e \u2208 E(s i ) : ||e \u2212 p|| \u221e < N S \u221e, else.", "formula_coordinates": [4.0, 320.68, 206.13, 232.44, 70.46]}, {"formula_id": "formula_11", "formula_text": "D(x) = p\u2208I 0 l u 0 p (1 \u2212 x p ) + u 1 p x p , (9", "formula_coordinates": [5.0, 348.88, 347.88, 199.77, 19.17]}, {"formula_id": "formula_12", "formula_text": ")", "formula_coordinates": [5.0, 548.65, 350.17, 3.91, 9.48]}, {"formula_id": "formula_13", "formula_text": "D O (x) = p\u2208I 0 l \u03b8 + 1 i=0\u00fb i p [x p = i] (q,j)\u2208O i p [x q = j] (10)", "formula_coordinates": [5.0, 319.31, 517.42, 233.26, 34.43]}, {"formula_id": "formula_14", "formula_text": "E V (P, S) = \u03ba p\u2208I 0 l V 0 r (p) \u2212 \u0393 0 r H 0 r (\u03c0 p )p + (11) V 1 l (p) \u2212 \u0393 1 l H 1 l (\u03c0 p )p + V 1 r (p) \u2212 \u0393 1 r H 1 r (\u03c0 p )p .", "formula_coordinates": [6.0, 317.59, 574.82, 235.98, 42.73]}], "doi": "10.1109/ICCV.2013.174"}