{"title": "Should I Follow the Crowd? A Probabilistic Analysis of the Effectiveness of Popularity in Recommender Systems", "authors": "Roc\u00edo Ca\u00f1amares; Pablo Castells", "pub_date": "", "abstract": "The use of IR methodology in the evaluation of recommender systems has become common practice in recent years. IR metrics have been found however to be strongly biased towards rewarding algorithms that recommend popular items -the same bias that state of the art recommendation algorithms display. Recent research has confirmed and measured such biases, and proposed methods to avoid them. The fundamental question remains open though whether popularity is really a bias we should avoid or not; whether it could be a useful and reliable signal in recommendation, or it may be unfairly rewarded by the experimental biases. We address this question at a formal level by identifying and modeling the conditions that can determine the answer, in terms of dependencies between key random variables, involving item rating, discovery and relevance. We find conditions that guarantee popularity to be effective or quite the opposite, and for the measured metric values to reflect a true effectiveness, or qualitatively deviate from it. We exemplify and confirm the theoretical findings with empirical results. We build a crowdsourced dataset devoid of the usual biases displayed by common publicly available data, in which we illustrate contradictions between the accuracy that would be measured in a common biased offline experimental setting, and the actual accuracy that can be measured with unbiased observations.", "sections": [{"heading": "INTRODUCTION", "text": "The use of IR methodologies and metrics for the evaluation of recommender systems has spread in recent years and is becoming common practice in the area, under the understanding of recommendation as a ranking task [14]. Yet IR metrics have been found to be strongly biased towards rewarding algorithms that recommend popular items, that is, items that many people know, like, rate or interact with [4,21,35]. At the same time, state of the art recommendation algorithms have similarly been found to display a marked bias towards recommending items most people like [21].\nThis may naturally cast doubt on the reliability of common experiments and the outcome on which the best algorithms really are.\nThis problem has been of no particular concern to IR methodology, as popularity biases do not occur, or not in such a dramatic way, in traditional search and IR tasks. The popularity bias is so strong in common datasets for recommender system evaluation that even a pure and simple popularity ranking appears to achieve suboptimal but non-negligible recommendation accuracy compared to the best state of the art personalized algorithms [14]. And it is in fact not necessarily trivial to outperform, for instance, in high rating sparsity conditions. Research has therefore been recently undertaken addressing the issue, so far mainly focusing on confirming and measuring the popularity biases, and removing them [4,21,34,35]. But a basic question remains yet unanswered: is the popularity bias actually something we should get rid of at all? If recommending popular items happened to be the right thing to do, then should not both the evaluation metrics and the recommendation algorithms rightfully favor them?\nThe majority opinion is indeed useful information for people -it is a simple yet fair and useful default criterion we keep in sight most of the time through our human decisions, even when we do not follow it. And we in fact often do adopt it, for instance, in the absence of enough evidence to form one's own personal choice, or as guidance to reduce the cost of building a decision from scratch, or as a social learning mechanism [3]. From an application point of view, a recommendation based on the choices of many can be acceptable in many circumstances [16] -and requires minimum development skills and maintenance costs. It is actually a widespread approach that many applications display in the form of top charts, best-selling lists, average people's ratings, etc. Even in the presence of a full-fledged personalized recommender system, majority listings are still a good resort for new or cold users.\nThe effectiveness of majority taste makes indeed statistical sense: the items that many people like (according to the records of observed user activity) are liked by many people (in test data for evaluation) [19]. Yet from an experimental perspective, if the observations are somehow biased, and the bias is consistent across training to test data, the majority bias in recommendation might be accurately guessing where the observations have been placed by the experimenter, rather than where true user tastes are being actually most satisfied. Moreover, the majority signal might be contaminated by trends that deviate from actual user appreciation [5,29]. Recent studies show that majority formation involves a degree of chance, by which different outcomes are possible as to what choices make it to the top of popularity [31]. Crowd dynamics are moreover known to be exposed to external and internal influence and bias factors [26,27,29], such as mass media [7], marketing, opinion management [6], algorithmic bias [28], or social conformity [13].\nThe issue is therefore open whether or not popularity is a truly effective ingredient to achieve accurate recommendations, to what extent and in what cases, and whether we are measuring it properly. We address the question by considering, analyzing and comparing two views on IR metrics: biased and unbiased. The former represents what is measured in common offline experiments in the literature, where relevance information is missing not at random (MNAR) [23,24,25,34,35], and the latter represents the true metric value that would be obtained if the missing information became available.\nWe do this at both a theoretical and an empirical level. At the analytical level, we formulate a probabilistic expression of the problem. Starting by a revised probability ranking principle [30] for recommender systems, we analyze popularity-based recommendation by comparison to the optimal ranking. We find that the effectiveness or ineffectiveness of popularity depends on the interplay of three main variables: item relevance, item discovery by users, and the decision by users to interact with discovered items. We identify the key probabilistic dependencies among these factors that determine the outcome for popularity, and we characterize a set of trends defined by different independence assumptions, each resulting in a particular pattern of behavior for popularity. We back our theoretical findings with empirical observations with a dataset we build on a crowdsourcing platform, in which we remove several of the common biases of publicly available datasets.\nAmong other findings, we prove and illustrate qualitative contradictions between the accuracy that is measured in a common offline experimental setting, and the actual accuracy that can be estimated with unbiased observations. We identify conditions that guarantee popularity to be a safe element in recommendation, and we characterize and exemplify situations where, on the contrary, popularity can be a totally misleading direction to follow, to the point of leading to worse effectiveness than random recommendation. We furthermore find that the average rating can be more effective than the number of ratings as a trend to follow in recommendation in many cases, contrarily to what the biased metric values suggest -which represent what the literature commonly reports [14,21]. Finally, we look at the signification our findings can have in personalized collaborative filtering algorithms.", "publication_ref": ["b14", "b3", "b21", "b35", "b21", "b14", "b3", "b21", "b34", "b35", "b2", "b16", "b19", "b5", "b29", "b31", "b26", "b27", "b29", "b7", "b6", "b28", "b13", "b23", "b24", "b25", "b34", "b35", "b30", "b14", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "POPULARITY IN RECOMMENDATION", "text": "Several authors have recently paid attention to the role and effects of popularity in recommender systems. Fleder and Hosanagar [15] observed the concentration effect of the recommendation feedback loop. Cremonesi et al. [14] were among the first to point at and analyze the fair results of popularity in the top-k recommendation task. Fig. 1 illustrates a typical observation in line with those findings, on two popular public datasets [17,20] (algorithm configuration and details are the same as in [9]), where popularity performs about half as well as the personalized algorithms (kNN [14] and matrix factorization [20]). Cremonesi et al. furthermore observed that even though recommendation by average rating value achieves worse accuracy than ranking items by their number of ratings (as is the case in Fig. 1), the comparison gets reversed when the top few most popular items are removed from the data. An explanation for such different outcomes was yet to be given, and will hopefully be found in the present paper.\nSteck [34,35] raised awareness on the fact that ratings are missing not at random [23,24] and subject to biases affecting both the input for algorithms and the data for evaluation. He proposed metrics and algorithmic corrections to better cope with popularity. Bellog\u00edn et al. [4] studied the strong popularity biases that surface in IR evaluation methodologies when applied to recommendation, and proposed further experimental methods to neutralize the biases. Jannach et al. [21] verified and measured the correlation with popularity observed in common state of the art algorithms, and proposed approaches to counter it. In our own prior work [9] we formalized the popularity bias as an intrinsic trend in memory-based collaborative filtering. Earlier on we studied the effect of social mouth-to-mouth on the raise of popularity distributions and the positive or negative effect on the accuracy of popularity-based recommendation [8].\nHowever, whether popularity is actually a good or bad feature to have -and whether its measured accuracy is reliable or not-is an implicit question that has not been directly explained yet. One obvious, negative answer has been given considering that popularity is the antithesis of novelty, a key ingredient in most cases for recommendations to be useful [1,11,12]. But from a broad perspective, this answer is partial, and does not refute the usefulness of some degree of popularity. While lack of novelty is an obvious drawback of popularity, the effect of popularity on pure accuracy should be properly understood. Even avoiding the head of the popularity distribution, even anywhere in the long tail, some items are more popular than others, and we need to understand the difference when we settle for recommendation at one precise point or the other on the popularity curve. Furthermore, since top-performing recommendation algorithms are strongly biased towards popular items, the question concerns state of the art methods as well, and any findings on the issue would help better understand and properly compare the effectiveness of personalized algorithms.", "publication_ref": ["b15", "b14", "b17", "b20", "b9", "b14", "b20", "b34", "b35", "b23", "b24", "b3", "b21", "b9", "b8", "b0", "b11", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "THEORETICAL FORMULATION", "text": "We start our study by setting out a mathematical formalization of the relevant involved elements. We first settle some definitions, a general framing for the recommendation task, and some formal notation to be used in the rest of the paper.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "The recommendation task [3] considers a set of users , a set of items \u2110, and a set of observed rating values for a subset of \u00d7 \u2110. We need not make any specific assumption about what ratings exactly consist of: explicit scores, implicit interaction records, etc.; it is sufficient for our purposes to consider they reflect some evidence of a positive or non-positive preference by the user for the rated item. The ratings are supplied as input (training data) to recommendation algorithms, which return a ranking of items for each user. In offline experiments, a subset of the available ratings is held out as test data for evaluation, and the rest of ratings are fed as input to the algorithms under evaluation [32]. In online evaluation, all available ratings are potentially used as input, and user feedback in response to recommendations in a live system is taken as test data. In both settings, test ratings are used as relevance judgments to compute the evaluation metrics of interest.   [21]. In the recommender systems literature, popularity is commonly defined as the number of users who have rated (who have been observed interacting with) an item, regardless of whether the interaction reflected positive or negative preference [4,14,21]. We find it more meaningful to consider an alternative refinement that only counts the interactions evidencing a positive preference (as in [9,34]). In usual datasets this makes no noticeable difference, but it can be proved that the total number of votes is never better than the number of positive votes as a signal for recommendation, whereby we shall focus on the latter definition.\nAnother sensible and common notion in the scope of popularity is the average rating value [14,21]. We shall use a simplifiedand empirically equivalent in all our experiments-binarized definition of average rating, as the ratio of users who liked an item, which is better suited to a probabilistic analysis. The average rating tends to display lower empirical effectiveness than popularity in common datasets, though it has been show to outperform popularity when, for instance, the few top most popular items are removed from the data, as reported by Cremonesi et al. [14].\nPopularity notions can be used for recommendation by just delivering the popularity ranking to all users. Formally, we shall denote by ( ) and ( ) the ranking functions of popularity and average rating, respectively, for \u2208 \u2110. Given a data split, we define 3.1.2 Observed vs. True Accuracy. Ranking-based recommendation accuracy metrics such as precision, recall, nDCG, MAP, MRR, etc., measure how well a recommendation ranks the relevant items above non-relevant ones. In the recommendation context, an item is considered relevant for a user if a positive rating by the user for the item is available in the test data. Such relevance knowledge is however generally incomplete -this is particularly true in recommender system experiments, where most of the user preferences are unknown, by definition of the recommendation task [2]. The difference between the metric value we can measure in common experiments, and the true metric value we would compute if we had full relevance knowledge, is a key distinction in our study.", "publication_ref": ["b2", "b32", "b21", "b3", "b14", "b21", "b9", "b34", "b14", "b21", "b14", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Data Split", "text": "Protocol. Our analysis shall assume an offline experimental design based on a random rating data split with a given ratio \u2208 (0,1) of training data. We consider a common data partition procedure which consists of iterating over each of the available ratings in the dataset at hand, assigning it to the training or test subset with probability and 1 \u2212 respectively. As in the most usual settings, we consider a recommendation task definition where the system should not recommend items to users who have already rated them (in the input training data) [2]. The data split is not necessary when we consider true metric values, which assume full (or at least unbiased) relevance knowledge: all ratings can be supplied as input to the algorithm (as if = 1), and the metrics use extra (unrated) relevance information obtained somehow. In our theoretical analysis we will abstract ourselves from the problem of obtaining this relevance knowledge, and we will later describe how we manage to get it in our experiments.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "3.1.4", "text": "User-Item Random Variables. We shall formalize key elements involved in the problem as random variables, in order to reason in terms of probabilities and expected values. First, we define the random variable : \u00d7 \u2110 \u2192 {0,1} over the set of useritem pairs as = 1 if a rating by the user for the item is available in the dataset and 0 otherwise. Given a rating split, we similarly define the variables and on user-item pairs as taking value 1 if = 1 and the rating was assigned to the training or test partition respectively, and 0 otherwise. Similarly, we define : \u00d7 \u2110 \u2192 {0,1} as = 1 if the user likes the item (regardless of the presence or absence of a rating), and 0 otherwise. Throughout the paper we will use the abbreviation ( ) , ( ), etc., for ( = 1), ( = 1), and so forth. Now considering a random variable : \u00d7 \u2110 \u2192 \u2110 defined as the item in a user-item pair, we can handle conditional probabilities such as ( | ), ( | ), and so on, for \u2208 \u2110 -where shall stand as an abbreviation of = . The probability ( | ), for instance, represents the ratio of users who like item .\nUsing these random variables and the definitions of section 3.1.1 above, we have  , ) = 0 and, in the random split procedure described earlier in section 3.1.3, the probability for a rating to be sampled for training or test is independent from both the item and the rating value, and is equal to the split ratio , we have:\n( ) \u221d ( , | ) ( ) \u223c ( | , )\nWe shall use the popularity and average rating ranking functions in this form in the rest of the paper.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Expected Precision", "text": "We choose precision for an accuracy metric, as a representative yet tractable option for theoretical analysis. Moreover and for the same reason we shall take @1. In the experiments of section 6 we will see that our analysis and results generalize well empirically to other accuracy metrics and common deeper cutoffs. Given a recommendation = \u2329 1 , 2 , \u2026 , \u232a, @1 is a binary value that is equal to 1 if the target user likes the top ranked item 1 , and 0 if she does not. This makes it easier to reason about the expected value of this metric. As a binary function, the expectation of @1 for a given recommendation is [ @1| ] = ( @1 = 1| ) = ( | 1 ) . As we have stressed in section 3.1.2, we shall distinguish between observed precision, which we shall denote by the symbol \u0302, and true precision, which we denote as . We have @1 = 1 iff the target user likes the top-ranked item, whereas \u0302@ 1 = 1 iff the user likes the top item and a rating by the user for the item is present in the test set. Therefore, [\u0302@1| ] = ( , | 1 ) for observed precision. Now we need to be more precise with the computation of the metrics: in fact @1 = 1 iff the first ranked recommendable item in is relevant (and analogously for \u0302) . Let this item be , ranked at the -th position of . As mentioned in section 3.1.3, recommendable means that does not have a training rating by the target user, and being the first means that all the items 1 , 2 ,\u2026, \u22121 above are not recommendable because they do have a training rating. If we marginalize ( @1 = 1| ) and (\u0302@1 = 1| ) by the possibility that the -th item is the first recommendable, and we make the mild assumption that whether two items are rated or not by some user are mutually independent events (whereby so is the event, since ratings are independently sampled in the random split as described in section 3.1.3), we have:\n[ @1| ] \u223c \u2211 ( , \u00ac | ) \u220f ( | ) \u22121 =1 =1 (1) [\u0302@1| ] \u223c \u2211 ( , | ) \u220f ( | ) \u22121 =1 =1 (2)\nwhere in equation 2 we can remove the condition \u00ac in\n( , ,\u00ac\n| ) as it is redundant: if a rating is present in the test set it cannot be present in the training set.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Optimal Recommendation", "text": "We can now set forth a first result on the optimal rankings for expected observed and true precisions.\nLemma 1 -Assuming pairwise item rating independence, the optimal recommendation that maximizes the (true) @1 expectation under a random rating split ranks items \u2208 \u2110 by non-increasing value of:\n( ) = ( |\u00ac , ) = ( | ) 1 \u2212 ( | , ) 1 \u2212 ( | )(3)\nUnder the same assumptions, the optimal recommendation that maximizes the expected (observed) \u0302@ 1 ranks items by non-increasing value of:\n\u0302( ) = ( , | ) (\u00ac | ) \u221d ( | ) ( | , ) 1 \u2212 ( | )(4)\nProof In order to show that the above rankings maximize the corresponding precision, it suffices to show that a consecutive swap against or \u0302 in a ranking produces a smaller value for [ @1| ] or [\u0302@1| ] respectively [10]. Given that any ranking can be generated by a sequence of pairwise counter-order swaps on any other ranking (as per e.g. the proof of correction of bubble sort), we would have proven our point. For true precision, let be some ranking so that ( ) \u2265 ( +1 ) for some , and let us consider a ranking \u2032 consisting of swapping and +1 in . Using equation 2 it is easy to see that, by trivial algebraic cancellation and rearrangement of terms, we have:\n[ @1| ] \u2265 [ @1| \u2032 ] \u21d4 ( , \u00ac | ) 1 \u2212 ( | ) \u2265 ( , \u00ac | +1 ) 1 \u2212 ( | +1 ) \u21d4 ( ) \u2265 ( +1 )\nWhich is true by description of . That is, swapping and ", "publication_ref": ["b10"], "figure_ref": [], "table_ref": []}, {"heading": "RELEVANCE BIAS IN RATING DISTRIBUTION", "text": "We now analyze how the relation between relevance and rating can determine the effectiveness of popularity. We do so by examining how the popularity and average rating rankings relate to the optimal ranking, and random recommendation. We start by considering two extreme cases in the relation between rating and relevance: a) the probability that a user rates an item depends only on relevance; and b) the probability that a user rates an item is independent from relevance. These two conditions can be expressed as conditional independence assumptions between rating, relevance and items: a) | , respectively. We analyze the consequences of each of these conditions in the next subsections. The analytic findings we will reach therein are summarized in Table 1. \u2044 is a monotonically increasing function of as long as 1 > 0, whatever the value of 2 .", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Conditional Item Independence", "text": "For observed precision, we similarly get:\n\u0302( ) \u221d ( | ) 1 \u2212 + ( \u2212 ) ( | ) \u221d ( | )\nwhere and are defined as before. We thus find, in particular, that if the rating probability depends only on relevance, then observed and true precision are consistent as to what the optimal recommendation is.\nOn the other hand, with the independence assumption at hand, the ranking functions for popularity and average rating become:\n( ) \u223c ( | ) \u221d ( ) \u221d\u0302( ) ( ) \u223c ( | ) + ( \u2212 ) ( | ) \u221d ( | ) \u221d ( ) \u221d\u0302( )\nWe thus come to:\nConclusion 1 -If the probability of rating depends just on whether the item is liked, then 1) the expected observed and true precision agree, and 2) both popularity and average rating produce the optimal non-personalized recommendation.\nNote that the scope of this finding, and all the ones that shall follow, is non-personalized: popularity, for instance, ranks items ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Rating independence assumptions/cases Corresponding assumptions on rating decision + discovery Subcases # conclusion", "text": "Popularity Average rating Fig.\n[\n\u0302@1] [ @1] [\u0302@1] [ @1]\nItem-independent , but not specifically by ( | , , ) for each user -and analogously for the average rating. Hence optimality is in those precise terms: without having the ranking depend on the user, thus applying lemma 1 in a non-personalized version. Note also that by optimal we shall always be meaning in expectation (of precision) with respect to the random data split and the detailed placement of ratings in the user-item matrix.\n\u22a5 | \u22a5 | , \u22a5 | - 1 5d Optimal Relevance-independent \u22a5 | \u22a5 | , \u22a5|", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conditional Relevance Independence", "text": "The relevance independence assumption means ( | , ) \u223c ( | ). Under this assumption, the optimal rankings obtained in equations 3 and 4 become:\n( ) \u223c ( | ) 1 \u2212 ( | ) 1 \u2212 ( | ) = ( | ) \u0302( ) \u223c ( | ) ( | ) 1 \u2212 ( | ) \u221d ( | ) ( | )\nwhere the final rank equivalence for \u0302 holds because (1 \u2212 ) \u2044 is monotonically increasing in and almost equal to the identity function for small values of . Observed and true precision are thus not necessarily consistent when the rating probability depends not just on relevance.\nThe popularity rankings, on their side, become:\n( ) \u221d ( , | ) = ( | ) ( | ) \u221d\u0302( ) ( ) \u221d ( | , ) = ( | ) \u221d ( )\nwhereby popularity and average rating would match the optimal ranking for observed and true precision, respectively. We therefore find:\nConclusion 2 -If the probability of rating does not depend on relevance, then the average rating is optimal in true precision, whereas popularity is optimal in observed precision.\nWe can draw further conclusions depending on which distribution, relevance or rating, is steeper. If the relevance distribution is steeper enough than the rating distribution, then ( | ) would dominate over ( ) would be totally unrelated to ( ) -hence equivalent to random recommendation-, and same for ( ) with respect to \u0302( ). We thus conclude: Conclusion 3 -If the probability of rating does not depend on relevance, we have: a) if relevance is steeper enough than rating, true and observed precision come close to agree, whereby popularity and the average rating approximate the optimal in both metrics; b) if rating dominates over relevance, popularity and the average rating tend to become equivalent to random recommendation in true and observed precision, respectively; and c) we can further conclude that in the average case where neither rating nor relevance are clearly steeper than the other, popularity and the average rating can be expected to be not optimal but still better than random recommendation in true and observed precision, respectively.\nWe hence find a contradiction between observed and true precision when rating and relevance are independent, unless the relevance distribution is very much steeper than the rating distribution. If the latter is very skewed, the contradiction can become extreme: observed and true precision report opposite outcomes.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "General Case", "text": "The simplifying assumptions considered above are not meant to reflect situations that would strictly occur in real scenarios: they just serve the purpose of identifying and understanding fundamental factors that make part, as mixed trends, of real situations. Moreover, we will show later that it is actually possible to enforce them in a controlled experiment.\nHowever, in the general case, with no particular independence assumptions, any outcome is actually possible. It is easy to build simple toy examples where popularity and the average rating are better or worse than each other and/or random recommendation, either in terms of true or observed precision. We may nonetheless expect, based on the findings of the previous subsections, that to the extent that rating dependence on either relevance or items (while coexisting) dominate one over the other, the results will be closer to the corresponding trends characterized so far.\nWe may also realize that since\n\u0302( ) = ( ) (1 \u2212 \u2044 (|\n)\n), popularity will tend to get a favorable assessment in observed precision -unless the items with the highest number of relevant ratings have a low total number of ratings, which is quite unlikely-whereas the average rating does not have such a direct relation to observed precision. We may hence expect to see the average rating lagging behind popularity in evaluations such as the one shown in Fig. 1, which need not necessarily reflect the actual situation if true precision could be measured.\nWe seek further insights in the next section, by analyzing where the probabilistic dependences between rating, relevance, and items may arise from.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "THE INTERPLAY OF RATING, DISCOVERY AND RELEVANCE", "text": "In order to better understand how rating may come to depend on relevance and individual items, we can consider the basic question: how does a rating come to existence? For a rating to be generated, the user must first of all discover the item at hand somehow. Then, she needs to examine, consume, buy, use (whatever applies in the application domain) the item, in order to form an opinion about it; and finally, she needs to decide to enter a rating. For simplicity, we shall collapse consumption and rating as a single event (as is in fact the case for systems working with implicit user preference feedback), which is sufficient for our analysis.\nThe characterization of popularity distributions can be thus decomposed into (and explained by) the discovery and rating decision distributions that give rise to the rating distribution. To reflect this view, let : \u00d7 \u2110 \u2192 {0,1} be a binary random variable that takes value 1 if the user knows the item exists, and 0 otherwise. We can marginalize the probability that an item has been rated by the event that it has been discovered or not. Given that( |\u00ac , ) = 0 (a user cannot rate items she has not yet discovered), and further marginalizing over relevance, we have:\n( | ) = ( | , ) ( | ) = ( | , , ) ( | , ) ( | ) + ( | , \u00ac , ) ( |\u00ac , )(1 \u2212 ( | ))\nIf we rewrite all the equations in section 4 using this decomposition, we realize that the ideal rankings -and hence the precision of popularity-depend on, and can be fully expressed in terms of, the following factors appearing above:\n\u2022 The bias in the user decision towards rating discovered items depending on whether they like them or not, reflected in \u2022 The potential bias in item discovery, towards finding liked or non-liked items more often, represented in ( | , ), and ( |\u00ac , ).\n( |,\n\u2022 The relevance distribution over items ( | ), reflecting that some items may be liked by more people than others.\nThe conditional dependence between rating, relevance, and items can be therefore reformulated as conditional dependencies of user behavior and item discovery on relevance and items, as we summarize in Fig. 2: if (and only if) rating decision and item discovery only depend on relevance, then rating only depends on relevance:\n( | , ) = ( | , , ) ( | , ) \u223c ( | , ) ( | ) = (\n| ), and conclusion 1 holds. Analogously, if the former are conditionally independent from relevance, so is the latter, and conclusions 2 and 3 hold. And when discovery and user behavior are not conditionally independent from the same thing (relevance or the item), rating depends on both relevance and the item. We briefly reflect on the meaning of dependencies at this level the next subsections.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "User Rating Behavior", "text": "Rating decision, in face of a discovered item, is essentially determined by human behavior [18]. The user bias towards rating relevant items more often than non-relevant ones has been mentioned as a likely possibility (a MNAR case) in prior work [25], and some studies have confirmed this trend in particular environments [24]. Considering that relevance is the main intrinsic property of consumed items that may bias the user's rating decisions (conditional independence from the item given its relevance) may be a reasonable simplification for many purposes. It is moreover possible to make the decision independent from both the item and user tastes e.g. in a controlled experiment, where users are prompted for explicit feedback on items they did not freely choose, as we shall describe in section 6.", "publication_ref": ["b18", "b25", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Item Discovery", "text": "Item discovery results from a more complex combination of actions by the user (e.g. searching and browsing) and external agents (advertisement, mouth-to-mouth [8], recommender systems [14,21], random chance, etc.). Discovery thus results from the interplay of a variety of processes, some of which are typically not the same for all items, and thus certainly do depend on the specific item. For instance, the item producer and/or marketer is one active, item-specific agent in the dissemination of the item towards potential consumers. At the same time, discovery may depend on relevance, as is generally the case when items are found by users through a search engine, a recommender system, or a suggestion by a friend. If such discovery means are more accurate than random, discovery will be biased towards items that users will like.\nDiscovery independence from the item given its relevance represents a fair situation in which all items have equal opportunity to be discovered, except for favoring positive matches in the interest of users. On the other end, conditional discovery independence from relevance represents a scenario where each item has its own, somewhat arbitrary degree of promotion, which fails to properly consider what users may or may not like -users are at the mercy of marketing or fashion [5,7], and dispense with search or recommendation aids, or these are ineffective.", "publication_ref": ["b8", "b14", "b21", "b5", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Multiple Dependence", "text": "We noted at the end of section 4 that in the general case with no assumption any outcome can occur for the effectiveness of popularity, agreements or disagreements between observed and true precision. We may however seek further insights beyond that, by estimating the expected situation considering all the possible values that the five fundamental distributions may take:\n( | , , ), ( | , \u00ac , ), (| ,\n), ( |\u00ac , ), and ( | ). This means assessing the expected precisions for popularity, average rating, and the optimal rankings over the space of such values. Formally, we should compute:\n[ @1| ] = \u222b [ @1| , ]\n\u03a9 and similarly for [\u0302@1| ], with denoting the different rankers, and \u03a9 representing the set of all possible valid values of the five conditional probabilities for all \u2208 \u2110 -namely \u03a9 = [0,1] 5 and = |\u2110|. This expectation can be estimated in a Monte Carlo approach, by sampling points \u2208 \u03a9 uniformly at random, computing the ranking that each recommender returns given (which is straightforward since , , , and \u0302 are direct functions of the five probabilities in \u03a9), and computing [ @1| ] and [\u0302@1| ], which again are functions of the same probabilities. Fig. 3 shows the result for |\u2110| = 3,700 (using the MovieLens 1M size [17] as an example) -it is easy to check that the results do not qualitatively depend on |\u2110|. We see that we may expect a substantial and qualitative contradiction between the observed and true precision: Conclusion 4 -In the absence of any independence assumption, while according to observed precision (as we would measure in a standard experiment) popularity would appear to be optimal and the average rating would seem barely better than random, popularity can be expected to be in truth just better than random, and the average rating to be better than popularity.\nNote the difference in extent of this finding compared to conclusions 1-3 in section 4. Whereas the results here are in expectation P@1 over all possible values of the conditional discovery, conditional rating decision and relevance probabilities (which means that the results may differ for specific probability values), the conclusions in section 4 hold strictly for any value of such probabilities, as long as the stated assumptions hold. Moreover, in the Monte Carlo estimation we have assumed a uniform \"meta-distribution\" of the probability values, while in practice some configurations can be expected to be more likely than others. Be that as it may, this neutral expectation provides an additional reference point in our analysis.", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}, {"heading": "EMPIRICAL OBSERVATIONS", "text": "We now run some experiments to see if the analytical results match empirical observations and to what extent. We build for this purpose a new dataset simultaneously supporting measurements under MAR (data missing at random [23,24]) and MNAR conditions.", "publication_ref": ["b23", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "A Crowdsourced MAR Dataset", "text": "Common publicly available datasets [12,17] usually provide user ratings for items in some domain, which have been collected through some natural process where users freely interact with items, under the influence of a myriad of discovery sources, some internal to the system where ratings are collected, and others exogenous. When using such collections there is generally no way to know precisely the discovery and behavior distributions from which the rating distribution resulted. We can only compute observed metric values, aware that they are measured upon MNAR data [25,34], and we have no means to contrast this to true unbiased values. Fig. 1 showed earlier an example of this usual situation on two well-known datasets. We see that popularity performs fairly well, far above random recommendation, and the average rating stands behind popularity by a clear difference. In our experiments we use a smoothed rating average, to avoid an extremely poor accuracy due to the high variance in the items with lowest number of ratings. We use Dirichlet smoothing with = avg \u2208\u2110 | | -the average sample size-as a fair default setting [36]. We may suspect the presence of biases and some effect on the observed results, but we cannot explain or verify much further with the available data.\nSeeking further insights, we build a new dataset where data is (essentially) MAR, by a crowdsourcing approach as follows. 1 We sample around 1,000 music tracks from the Deezer database 2 uniformly at random using the public API. Then, we work with around 1,000 users (after discarding unreliable input through several checks and filters) in the CrowdFlower 3 service. We randomly assign tracks to users in such a way that each track is assigned to about 100 users, and each user gets about 100 tracks assigned, adding to a total of about 100,000 assignments. For each assignment, we ask the user to play the music and tell whether or not a) she likes it, and b) she had heard it before this survey.\nThe novelty of the resulting dataset with respect to others is that we completely remove the discovery bias by sampling and assigning items to users uniformly at random. Moreover, we completely remove the rating decision bias by requiring users to rate everything they are presented with, that is ( | ) = 1 in the resulting dataset. Furthermore, the declared user music knowledge information enables recreating MNAR data condi-tions, as we shall see. Fig. 4 shows the rating and relevant rating distributions ( | ) and ( , | ) in our crowdsourced dataset and in MovieLens. For the former we show, additionally, the discovery distribution ( | , ). We can see that the rating distribution in our dataset corresponds to a uniform probability (with a natural binomial sampling variance), whereas discovery and relevance are heavy-headed.", "publication_ref": ["b12", "b17", "b25", "b34", "b36", "b0"], "figure_ref": ["fig_11"], "table_ref": []}, {"heading": "Evaluation under Different Scenarios", "text": "The unbiased data thus collected enables reproducing different scenarios for experimentation, resulting in different outcomes for popularity, which are shown in Fig. 5. The dataset as is enables two different scenarios, and two additional ones are recreated by resampling the ratings in different ways. We describe each scenario and the corresponding experimental results in turn in the following paragraphs labeled a-d, matching the labels in Fig. 5. In all scenarios, we split the ratings into training and test with = 0.8 (5-fold validation) and we interpret the absence of rating as nonrelevance (alike to negative ratings). We average the results over 10 executions to reduce the variance in all experiments with the crowdsourced data. Along with the evaluated methods we show the metric values for the optimal non-personalized rankings defined by and \u0302 in equation 5, as skyline oracles that are given access to relevance information that is hidden from the other recommenders. We also show results on MovieLens for quick reference. a) Rating fully independent from relevance and items. Since rating decision is, by our data collection design, independent from items and relevance, the dataset as is fits in the case described in section 4.2, where conclusions 2 and 3 apply. Because the rating distribution is uniform over items, the relevance distribution is much steeper in comparison, and we have specifically conclusion 3a. The results can be seen in Fig. 5a: popularity and the average rating perform significantly better than random, and not far from the optimal non-personalized ranking, which confirms the analytical expectation. We can only measure observed precision here, since we do not have further relevance knowledge beyond the collected ratings.\nWe can also see the advantage of popularity over random is smaller than in MovieLens: because of the flat rating distribution, the relevant rating distribution is much less steep in our dataset (see Fig. 4), and popularity therefore gets a considerably lesser advantage. Moreover, we find that the accuracy of popularity and the average rating become statistically equivalent. This provides an explanation for the results reported by Cremonesi et al. [14], where removing the head of the rating distribution reversed the comparison between popularity and the average rating, as the rating distribution was made flatter -moreover, popularity was defined in [14] as the total number of ratings (rather than just positive ones), which naturally converges to random recommendation as the rating distribution tends towards uniformity. b) Mixed discovery dependencies. Based on the user feedback on what music they had heard before, we can reproduce a natural MNAR discovery bias by providing the recommender systems as input only the ratings for music that users declared to know. We likewise compute observed precision by counting relevant items only when they were known to the user. But at the same time, we can estimate true precision by using the full available MAR relevance information. This knowledge only covers about 10% of items for each user but, as a uniform sample, it enables an unbiased estimate of true precision. We still have\n( | ) = 1 (\nwhere \" \" now means the user declared to know the item before the experiment) and rating decision of a discovered item is independent from items and relevance, therefore the resulting setting corresponds to the mixed situation described in sections 4.3 and 5.3. We have no particular reason to assume discovery (by whatever processes lead users to discover music before our experiment) could be independent from neither items, nor relevance. In fact, the results shown in Fig. 5b suggest, by their difference to cases c and d (to be described next), that both dependences must be present in the data.\nWhile observed precision depicts a comparable outcome to the results on MovieLens, true precision tells quite a different story, revealing a quite inadequate performance, even slightly (but statistically significantly in @1) below random. On the other hand, the average rating performs somewhat poorly but better than popularity in true accuracy, most particularly in terms of nDCG@10. Overall, the results seem not far from conclusion 4. c) Relevance-independent discovery. We can reproduce separate discovery biases by simple resampling procedures. By randomly shuffling the discovery distribution over items, i.e. reassigning each ( | ) to a random item , we can decouple discovery from relevance, that is we remove any dependence there might be between and given an item. Discovery therefore only depends (by random arbitrary assignment) on the item. Discovery (to which the rating distribution is proportional in this case) seems to have a slightly steeper distribution than relevance in Fig. 4, but this does not seem to be enough and the setting tends to fit in conclusion 3c. The contradiction between observed and true accuracy is most striking in this scenario: popularity stands out in observed precision, where the average rating is just above random, while almost the opposite is the case in true precision -though popularity is still better than random, as expected.\nd) Item-independent discovery. We can reproduce a relevance-only dependent discovery by randomly reassigning (fictional) discovery to user-item pairs with probability\n( | )\nif the user likes the item, and with probability ( |\u00ac ) otherwise, using the values of ( | ) and ( |\u00ac ) estimated from our initial data. By doing so, the resulting discovery distribution will just depend on the relevance of items, and we remove the potential direct dependence on the item. As in cases b and c above, we only use the ratings (as system input and for observed precision computation) on items that users have \" \". Once again, we see the results in Fig. 5d match the analytical prediction (conclusion 1). Popularity seems to take better advantage of the relevance dependence than the average rating. The advantage is nonetheless quantitatively small.\nThe empirical results are thus largely coherent with the analytically characterized behaviors. The non-personalized rankings approach but do not fully reach the oracle theoretical optima when the theory suggests it should. This can be attributed to the sampling variance involved in rating assignment and the data split. The comparisons between popularity, average rating and random rankings are however similarly affected by the variance, and do seem to match more closely what theory expects. Though we focused on  We confirm several inconsistencies between observed and true accuracy, and find below-random recommendation performance for popularity in scenario d. We also show the accuracy of the (oracle) optimal non-personalized rankings. Nonstatistically significant differences (2-tailed Student's t-test at < . ) are indicated in the graphs with a red double arrow. ", "publication_ref": ["b14", "b14"], "figure_ref": ["fig_13", "fig_13", "fig_13", "fig_11", "fig_13", "fig_11", "fig_13"], "table_ref": []}, {"heading": "Observed", "text": "P@1 as a tractable metric, it seems to consistently generalize empirically to other metrics (we just show nDCG@10 for the sake of space, but other metrics follow similar patterns). In fact, deeper cutoffs align slightly more tightly with the analytical findings, possibly because they are more robust to the potential peculiarities of a single top 1 item. We have also found some sensitiveness to the split ratio at specific points, which we envisage analyzing along with the sampling protocol in further depth in future work.\nThe crowdsourced dataset provides information that allows supplying MAR (scenario a) or MNAR (scenarios b, c, d) input data for the algorithms; and MAR or MNAR relevance information for computing the observed and true value, respectively, of evaluation metrics in any scenario. In other words, we evaluate with ratings that are usually missing. Related to this, Marlin et al. [24,25] also used semi-randomly polled ratings with a different focus: learning and correcting for the MNAR biases in recommendation algorithms. They did not explain how the biases result in metric disagreements, but they empirically observe them. Nor do they analyze popularity or consider the role of item discovery, but the overall idea of randomly sampling ratings for unbiased metric estimates is a direct precedent of our experimental approach.", "publication_ref": ["b24", "b25"], "figure_ref": [], "table_ref": []}, {"heading": "Popularity-Biased Personalized Algorithms", "text": "As we discussed in section 2, popularity is known to be a major trend in most state of the art collaborative filtering methods [9,14,21]. We may therefore wonder whether similar patterns may manifest in some way in the algorithms that are biased to this feature. Focusing on the most general, mixed dependence scenario (case c), we test, as just a sample and representative method, two variants of the user-based nearest neighbors (kNN) algorithm: normalized and not normalized, as defined in e.g. [14]. The latter is known to be popularity-biased, whereas the former is biased towards the average rating [9]. We tune on MovieLens 1M for @1 by grid search by multiples of 10, then 100 then 1,000, using a validation subset of the training data. On the crowdsourced dataset we just take all users as neighbors to avoid the burden of tuning for true precision -we nevertheless checked that doing so only makes the resulting differences larger and clearer. Fig. 6 shows the result: the popularity-biased algorithm appears to be clearly better than the one biased to the average rating in terms of observed metric values (in line with prior reported results [9,14]), while the true values reveal rather the opposite is the case. One may wonder if we would see a similar result in MovieLens had we had an unbiased glimpse of the unseen relevance for this dataset.", "publication_ref": ["b9", "b14", "b21", "b14", "b9", "b9", "b14"], "figure_ref": ["fig_15"], "table_ref": []}, {"heading": "DISCUSSION", "text": "Our study confirms the general popularity effectiveness trend [14,21], formally proving and explaining where this comes from. We also find that the apparent accuracy can be misleading (i.e. does not match the true accuracy) in many cases: this mainly happens, in essence, when item discovery is broadly detached from user taste.\nWe show that common experiments (i.e. observed accuracy metric values) may be rather unfair to the average rating, and its personalized derivatives. Contrarily to what has been observed in the literature so far [14,21], the average rating may be in fact a better, safer, more robust signal than the number of positive ratings in terms of true achieved accuracy in most general situations -a quick glimpse at Table 1 or Fig. 3 and 5 evidences that while the observed accuracy of popularity would appear better than the average rating in many cases, the latter actually outperforms -or at worst is not far from-the former in true accuracy. In exchange, the rating average needs smoothing and hence parameter tuning -we see that a simple default configuration works quite well nonetheless. Furthermore, if unbiased item judgments are available for training, the average rating can definitely and systematically outperform popularity (we omit such experiments here for the sake of space).\nWe further find out that among the factors producing MNAR conditions in rating data [24,25,34], taste biases in users' decision to rate items may not have exactly the role that has been suggested in the literature. In particular, it does not matter whether liked items are rated more often or less, when it comes to the effectiveness of popularity or the average rating, and its measurement. The situation with regards to our analytical findings is the same regardless, for all purposes, since we did not need to assume \u00ac ) or the opposite anywhere in our analysis. What matters is just whether rating depends on relevance or not, and whether the dependence is full or partial; but not in what direction.\n( | , ) > ( |,\nThe ideal conditions for popularity and the average rating to be truly accurate are cases when discovery mainly depends on relevance, or barely depends on it. The average rating seems more robust than popularity to relevance-independence -so it would be preferable over popularity in highly biased (e.g. marketing-driven) scenarios. Popularity might take a slightly better advantage of relevance dependence, though further experiments would be needed to check this point, as the difference is small in our observations, and the theoretical conclusions would suggest a tie. Problems can arise in the mixed case where, in our particular experiment, we strikingly discover not only a contradiction between observed and true accuracy, but a below random performance for popularity.\nFinally, we observe that the findings for popularity, worthy of research in themselves, can have further signification on top-performing recommendation algorithms as far as they are popularitybiased. For instance, Cremonesi et al. [14] had found that a normalized kNN algorithm outperformed the non-normalized version when the top head items were removed. We now find an explanation in our analysis, by the trends described in scenario a (section 6.2) for a flattened rating distribution, along with the respective bias of the kNN variants to popularity and the average rating.\nOur findings may call for a second look at the algorithmic state of the art in light of new approximations to true accuracy. In this perspective, preference data on randomly sampled users and items may be costly to obtain, but can be a highly clarifying complement of common experiments with biased user interaction data.", "publication_ref": ["b14", "b21", "b14", "b21", "b24", "b25", "b34", "b14"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "ML 1M", "text": "Crowd 100k ML 1M Crowd 100k P@1 nDCG@10 ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSIONS", "text": "We have developed a formal analysis of the effectiveness of popularity-based recommendation, upon the identification and formalization of key factors on a probabilistic basis. Our findings provide some principled explanation of the general trend observed in experiments reported in the recent literature in the field [14,21]. At the same time, insights from a deeper analysis suggest we may want to scratch beneath the surface of common experiments as we may discover unperceived and potentially different outcomes. To the best of our knowledge, these represent the first specific results to be reached on the question whether popularity is an effective or misleading signal in recommendation -and the first to suggest the average rating might be preferable to the number of favorable preferences as a non-personalized signal.\nThe presented findings can be useful in different ways. In a working application, we may wish to know if popularity is truly effective or not, in order to leave it or not as a trend in our algorithms. In an evaluation experiment, we may want to neutralize the interference of the popularity bias by an experimental design where popularity amounts to random recommendation [4,21,35]. Or we might want to rethink recommendation algorithms in light of what formal analysis or new experiments on true precision can reveal. Our reported experiments show that getting such estimates on unbiased samples is feasible.\nOur research can be extended in many directions. To begin with, our findings may have implications on state of the art recommendation algorithms, inasmuch as they are strongly biased towards popularity. Re-examining their effectiveness in view of our findings may deserve further study. We also envision the construction of further and larger datasets as a worthy endeavor, perhaps by more coordinated efforts in the community. These should allow to further confirm, revise, or extend our findings. Different scenarios defined by different -or fewer-assumptions could be explored as well. For instance, even though random data splitting is very common practice in the recommender systems literature, we find it worthwhile exploring beyond this and consider, for instance, temporal data splits, which better represent a real setting.", "publication_ref": ["b14", "b21", "b3", "b21", "b35"], "figure_ref": [], "table_ref": []}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by the national Spanish Government (grant nr. TIN2016-80630-P).", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "On unexpectedness in recommender systems: or how to better expect the unexpected", "journal": "ACM Transactions on Intelligent Systems and Technology", "year": "2014-01", "authors": "P Adamopoulos; A Tuzhilin"}, {"ref_id": "b1", "title": "Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions", "journal": "IEEE TKDE", "year": "2005-06", "authors": "G Adomavicius; A Tuzhilin"}, {"ref_id": "b2", "title": "Social Learning Theory", "journal": "General Learning Press", "year": "1971", "authors": "A Bandura"}, {"ref_id": "b3", "title": "Statistical Biases in Information Retrieval Metrics for Recommender Systems", "journal": "Information Retrieval", "year": "2017-07", "authors": "A Bellog\u00edn; P Castells; I Cantador"}, {"ref_id": "b4", "title": "", "journal": "", "year": "", "authors": " Springer"}, {"ref_id": "b5", "title": "A Theory of Fads, Custom, and Cultural Change as Informational Cascades", "journal": "The Journal of Political Economy", "year": "1992-10", "authors": "S Bikhchandani; D Hirshleifer; I Welch"}, {"ref_id": "b6", "title": "Manipulating Opinion Diffusion in Social Networks", "journal": "Morgan Kaufmann Publishers", "year": "2017", "authors": "R Bredereck; E Elkind"}, {"ref_id": "b7", "title": "Media Effects: Advances in Theory and Research", "journal": "", "year": "2008", "authors": ""}, {"ref_id": "b8", "title": "Exploring social network effects on popularity biases in recommender systems", "journal": "", "year": "2014-10", "authors": "R Ca\u00f1amares; P Castells"}, {"ref_id": "b9", "title": "A Probabilistic Reformulation of Memory-Based Collaborative Filtering -Implications on Popularity Biases", "journal": "ACM", "year": "2017", "authors": "R Ca\u00f1amares; P Castells"}, {"ref_id": "b10", "title": "On the Optimal Non-Personalized Recommendation: From the PRP to the Discovery False Negative Principle", "journal": "", "year": "2017-08", "authors": "R Ca\u00f1amares; P Castells"}, {"ref_id": "b11", "title": "Novelty and Diversity in Recommender Systems", "journal": "Springer", "year": "2015", "authors": "P Castells; N J Hurley; S Vargas"}, {"ref_id": "b12", "title": "A new approach to evaluating novel recommendations", "journal": "ACM", "year": "2008", "authors": "O Celma; P Herrera"}, {"ref_id": "b13", "title": "Social Influence: Compliance and Conformity", "journal": "Annual Review of Psychology", "year": "2004-02", "authors": "R B Cialdini; N J Goldstein"}, {"ref_id": "b14", "title": "Performance of recommender algorithms on top-n recommendation tasks", "journal": "ACM", "year": "2010", "authors": "P Cremonesi; Y Koren; R Turrin"}, {"ref_id": "b15", "title": "Blockbuster culture's next rise or fall: The impact of recommender systems on sales diversity", "journal": "Management Science", "year": "2009-05", "authors": "D Fleder; K Hosanagar"}, {"ref_id": "b16", "title": "Anatomy of the long tail: ordinary people with extraordinary tastes", "journal": "ACM", "year": "2010", "authors": "S Goel; A Broder; E Gabrilovich; B Pang"}, {"ref_id": "b17", "title": "The MovieLens Datasets: History and Context", "journal": "ACM TOIS", "year": "2016-01", "authors": "F M Harper; J A Konstan"}, {"ref_id": "b18", "title": "An Economic Model of User Rating in an Online Recommender System", "journal": "Springer", "year": "2005", "authors": "F M Harper; X Li; Y Chen; J A Konstan"}, {"ref_id": "b19", "title": "Learning from Imbalanced Data", "journal": "IEEE TKDE", "year": "2009-09", "authors": "H He; E A Garcia"}, {"ref_id": "b20", "title": "Collaborative Filtering for Implicit Feedback Datasets", "journal": "IEEE Computer Society", "year": "2008", "authors": "Y Hu; Y Koren; C Volinsky"}, {"ref_id": "b21", "title": "What recommenders recommend: an analysis of recommendation biases and possible countermeasures", "journal": "User Modeling and User-Adapted Interaction", "year": "2015-12", "authors": "D Jannach; L Lerche; I Kamehkhosh; M Jugovac"}, {"ref_id": "b22", "title": "Amazon.com Recommendations: Itemto-Item Collaborative Filtering", "journal": "IEEE Internet Computing", "year": "2003-01", "authors": "G Linden; B Smith; J York"}, {"ref_id": "b23", "title": "Statistical analysis with missing data", "journal": "John Wiley & Sons", "year": "1987", "authors": "R J A Little; D B Rubin"}, {"ref_id": "b24", "title": "Collaborative Filtering and the Missing at Random Assumption", "journal": "AUAI Press", "year": "2007", "authors": "B M Marlin; R S Zemel; S T Roweis; M Slaney"}, {"ref_id": "b25", "title": "Collaborative prediction and ranking with nonrandom missing data", "journal": "ACM", "year": "2009", "authors": "B Marlin; R Zemel"}, {"ref_id": "b26", "title": "Social Influence and the Collective Dynamics of Opinion Formation", "journal": "", "year": "2013-11", "authors": "M Moussa\u00efd; J E K\u00e4mmer; P P Analytis; H Neth"}, {"ref_id": "b27", "title": "Information diffusion and external influence in networks", "journal": "ACM", "year": "2012", "authors": "S A Myers; C Zhu; J Leskovec"}, {"ref_id": "b28", "title": "The Filter Bubble: How the New Personalized Web Is Changing What We Read and How We Think", "journal": "Penguin Books", "year": "2012", "authors": "E Pariser"}, {"ref_id": "b29", "title": "Characterizing and Modeling the Dynamics of Online Popularity", "journal": "Physical Review Letters", "year": "2010-10", "authors": "J Ratkiewicz; S Fortunato; A Flammini; F Menczer; A Vespignani"}, {"ref_id": "b30", "title": "The Probability Ranking in IR", "journal": "Journal of Documentation", "year": "1977-01", "authors": "S E Robertson"}, {"ref_id": "b31", "title": "Experimental Study of Inequality and Unpredictability in an Artificial Cultural Market", "journal": "Science", "year": "2006-02", "authors": "M J Salganik; P S Dodds; D J Watts"}, {"ref_id": "b32", "title": "Evaluating Recommendation Systems", "journal": "Springer", "year": "2015", "authors": "G Shani; A Gunawardana"}, {"ref_id": "b33", "title": "Deconvolving Feedback Loops in Recommender Systems", "journal": "", "year": "2016-12", "authors": "A Sinha; D F Gleich; K Ramani"}, {"ref_id": "b34", "title": "Training and testing of recommender systems on data missing not at random", "journal": "ACM", "year": "2010", "authors": "H Steck"}, {"ref_id": "b35", "title": "Item popularity and recommendation accuracy", "journal": "ACM", "year": "2011", "authors": "H Steck"}, {"ref_id": "b36", "title": "A study of smoothing methods for language models applied to information retrieval", "journal": "ACM Transactions on Information Systems", "year": "2004-04", "authors": "C Zhai; J D Lafferty"}], "figures": [{"figure_label": "11", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 : 111Figure 1: Typical offline experimental results for non-personalized popularity-based recommendation compared to personalized algorithms on two public datasets. 0 0.1 0.2 0.3", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "( ) = | + | as the number of positive training ratings the item has, and ( ) = | + | | | \u2044 as the ratio of positive training ratings.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "+1 decreases [ @1| ]. And an analogous reasoning proves the corresponding statement for observed precision. The right-side form of and \u0302 in equations 3 and 4 is trivially obtained by applying ( | ) = ( | ) and ( | ) = (1 \u2212 ) ( | ) as explained in section 3.1.4. \uf0a8", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Applying this to equation 3, we get that the optimal ranking for true precision is given by: The rank equivalence holds because ( 1 + 2 )", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "a.( | )  steeper enough than(    ", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "( | ) when multiplying them, and we would have approximately \u0302( ) \u221d ( | ) ( | ) \u221d ( | ) \u221d ( ). If on the contrary the rating distribution is steeper enough than relevance, we would have \u0302( ) \u221d ( | ) (", "figure_data": ""}, {"figure_label": "32", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 3 :Figure 2 :32Figure 3: Expected precision with no assumption. The expectation is estimated by Monte Carlo random sampling over the set of all possible valid values of ( | ,) , (", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 4 :4Figure 4: Data distribution in MovieLens 1M (left) and our crowdsourced dataset (right). Note that each curve has axis (items) sorted by decreasing order of the corresponding distribution so as to better show its shape -the values of the curves therefore do not match with each other. 0 25 50 75", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Figure 5 :5Figure 5: Empirical confirmation of the analytical results. Column a) reflects and confirms conclusions 2 and 3a; c) corresponds to conclusions 2 and 3c, d) matches conclusion 1, and b) exemplifies the general scenario addressed in conclusion 4.We confirm several inconsistencies between observed and true accuracy, and find below-random recommendation performance for popularity in scenario d. We also show the accuracy of the (oracle) optimal non-personalized rankings. Nonstatistically significant differences (2-tailed Student's t-test at < . ) are indicated in the graphs with a red double arrow.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "Figure 6 :6Figure 6: User-based kNN in MovieLens 1M and the crowdsourced 100k dataset, mixed dependency scenario. All pairwise differences are statistically significant (2tailed Student's t test at < .).", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Summary of cases. \"Optimal\" is meant in the scope of non-personalized recommendations. We indicate the conclusion numbering corresponding to each case, and the figure(s) where it is shown or tested.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "( ) \u221d ( , | ) ( ) \u223c ( | , )", "formula_coordinates": [3.0, 354.79, 331.34, 180.81, 9.0]}, {"formula_id": "formula_1", "formula_text": "[ @1| ] \u223c \u2211 ( , \u00ac | ) \u220f ( | ) \u22121 =1 =1 (1) [\u0302@1| ] \u223c \u2211 ( , | ) \u220f ( | ) \u22121 =1 =1 (2)", "formula_coordinates": [4.0, 81.98, 192.15, 211.99, 62.19]}, {"formula_id": "formula_2", "formula_text": "( , ,\u00ac", "formula_coordinates": [4.0, 58.8, 269.66, 42.51, 9.0]}, {"formula_id": "formula_3", "formula_text": "( ) = ( |\u00ac , ) = ( | ) 1 \u2212 ( | , ) 1 \u2212 ( | )(3)", "formula_coordinates": [4.0, 67.94, 381.62, 226.03, 21.12]}, {"formula_id": "formula_4", "formula_text": "\u0302( ) = ( , | ) (\u00ac | ) \u221d ( | ) ( | , ) 1 \u2212 ( | )(4)", "formula_coordinates": [4.0, 78.86, 442.84, 215.11, 21.12]}, {"formula_id": "formula_5", "formula_text": "[ @1| ] \u2265 [ @1| \u2032 ] \u21d4 ( , \u00ac | ) 1 \u2212 ( | ) \u2265 ( , \u00ac | +1 ) 1 \u2212 ( | +1 ) \u21d4 ( ) \u2265 ( +1 )", "formula_coordinates": [4.0, 57.96, 598.43, 234.59, 36.0]}, {"formula_id": "formula_6", "formula_text": "\u0302( ) \u221d ( | ) 1 \u2212 + ( \u2212 ) ( | ) \u221d ( | )", "formula_coordinates": [4.0, 357.43, 500.2, 163.65, 21.12]}, {"formula_id": "formula_7", "formula_text": "( ) \u223c ( | ) \u221d ( ) \u221d\u0302( ) ( ) \u223c ( | ) + ( \u2212 ) ( | ) \u221d ( | ) \u221d ( ) \u221d\u0302( )", "formula_coordinates": [4.0, 352.87, 594.91, 184.53, 33.12]}, {"formula_id": "formula_8", "formula_text": "\u0302@1] [ @1] [\u0302@1] [ @1]", "formula_coordinates": [4.0, 448.45, 108.37, 107.04, 7.92]}, {"formula_id": "formula_9", "formula_text": "\u22a5 | \u22a5 | , \u22a5 | - 1 5d Optimal Relevance-independent \u22a5 | \u22a5 | , \u22a5|", "formula_coordinates": [4.0, 55.92, 122.02, 454.24, 35.3]}, {"formula_id": "formula_10", "formula_text": "( ) \u223c ( | ) 1 \u2212 ( | ) 1 \u2212 ( | ) = ( | ) \u0302( ) \u223c ( | ) ( | ) 1 \u2212 ( | ) \u221d ( | ) ( | )", "formula_coordinates": [5.0, 77.78, 209.27, 194.84, 45.27]}, {"formula_id": "formula_11", "formula_text": "( ) \u221d ( , | ) = ( | ) ( | ) \u221d\u0302( ) ( ) \u221d ( | , ) = ( | ) \u221d ( )", "formula_coordinates": [5.0, 90.74, 328.7, 180.68, 22.56]}, {"formula_id": "formula_12", "formula_text": "\u0302( ) = ( ) (1 \u2212 \u2044 (|", "formula_coordinates": [5.0, 448.27, 254.42, 109.88, 10.44]}, {"formula_id": "formula_13", "formula_text": ")", "formula_coordinates": [5.0, 321.07, 267.38, 3.1, 8.52]}, {"formula_id": "formula_14", "formula_text": "( | ) = ( | , ) ( | ) = ( | , , ) ( | , ) ( | ) + ( | , \u00ac , ) ( |\u00ac , )(1 \u2212 ( | ))", "formula_coordinates": [5.0, 322.87, 629.95, 235.22, 32.76]}, {"formula_id": "formula_15", "formula_text": "( |,", "formula_coordinates": [6.0, 71.06, 185.99, 46.98, 9.0]}, {"formula_id": "formula_16", "formula_text": "( | , ) = ( | , , ) ( | , ) \u223c ( | , ) ( | ) = (", "formula_coordinates": [6.0, 58.8, 307.58, 235.32, 19.8]}, {"formula_id": "formula_17", "formula_text": "( | , , ), ( | , \u00ac , ), (| ,", "formula_coordinates": [6.0, 344.11, 393.14, 204.22, 9.0]}, {"formula_id": "formula_18", "formula_text": "[ @1| ] = \u222b [ @1| , ]", "formula_coordinates": [6.0, 384.79, 447.04, 100.69, 9.0]}, {"formula_id": "formula_19", "formula_text": "( | ) = 1 (", "formula_coordinates": [8.0, 58.8, 528.06, 71.01, 10.26]}, {"formula_id": "formula_20", "formula_text": "( | )", "formula_coordinates": [8.0, 519.82, 496.48, 38.34, 8.64]}, {"formula_id": "formula_21", "formula_text": "( | , ) > ( |,", "formula_coordinates": [9.0, 322.87, 390.74, 125.95, 9.0]}], "doi": "10.1145/3209978.3210014"}