{"title": "IRLbot: Scaling to 6 Billion Pages and Beyond", "authors": "Hsin-Tsang Lee; Derek Leonard; Xiaoming Wang; Dmitri Loguinov", "pub_date": "", "abstract": "This paper shares our experience in designing a web crawler that can download billions of pages using a single-server implementation and models its performance. We show that with the quadratically increasing complexity of verifying URL uniqueness, BFS crawl order, and fixed per-host ratelimiting, current crawling algorithms cannot effectively cope with the sheer volume of URLs generated in large crawls, highly-branching spam, legitimate multi-million-page blog sites, and infinite loops created by server-side scripts. We offer a set of techniques for dealing with these issues and test their performance in an implementation we call IRLbot. In our recent experiment that lasted 41 days, IRLbot running on a single server successfully crawled 6.3 billion valid HTML pages (7.6 billion connection requests) and sustained an average download rate of 319 mb/s (1, 789 pages/s). Unlike our prior experiments with algorithms proposed in related work, this version of IRLbot did not experience any bottlenecks and successfully handled content from over 117 million hosts, parsed out 394 billion links, and discovered a subset of the web graph with 41 billion unique nodes.", "sections": [{"heading": "INTRODUCTION", "text": "Over the last decade, the World Wide Web (WWW) has evolved from a handful of pages to billions of diverse objects. In order to harvest this enormous data repository, search engines download parts of the existing web and offer Internet users access to this database through keyword search. Search engines consist of two fundamental components -web crawlers, which find, download, and parse con-", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Scalability", "text": "With the constant growth of the web, discovery of usercreated content by web crawlers faces an inherent tradeoff between scalability, performance, and resource usage. The first term refers to the number of pages N a crawler can handle without becoming \"bogged down\" by the various algorithms and data structures needed to support the crawl. The second term refers to the speed S at which the crawler discovers the web as a function of the number of pages already crawled. The final term refers to the CPU and RAM resources \u03a3 that are required to sustain the download of N pages at an average speed S. In most crawlers, larger N implies higher complexity of checking URL uniqueness, verifying robots.txt, and scanning the DNS cache, which ultimately results in lower S and higher \u03a3. At the same time, higher speed S requires smaller data structures, which often can be satisfied only by either lowering N or increasing \u03a3.\nCurrent research literature [2], [4], [6], [8], [13], [15], [19], [22], [23], [25], [26], [27] generally provides techniques that can solve a subset of the problem and achieve a combination of any two objectives (i.e., large slow crawls, small fast crawls, or large fast crawls with unbounded resources). They also do not analyze how the proposed algorithms scale for very large N given fixed S and \u03a3. Even assuming sufficient Internet bandwidth and enough disk space, the problem of designing a web crawler that can support large N (hundreds of billions of pages), sustain reasonably high speed S (thousands of pages/s), and operate with fixed resources \u03a3 remains open.", "publication_ref": ["b1", "b3", "b5", "b7", "b12", "b14", "b18", "b22", "b23", "b25", "b26", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "Reputation and Spam", "text": "The web has changed significantly since the days of early crawlers [4], [23], [25], mostly in the area of dynamically generated pages and web spam. With server-side scripts that can create infinite loops, high-density link farms, and unlimited number of hostnames, the task of web crawling has changed from simply doing a BFS scan of the WWW [24] to deciding in real time which sites contain useful information and giving them higher priority as the crawl progresses.\nOur experience shows that BFS eventually becomes trapped in useless content, which manifests itself in multiple ways: a) the queue of pending URLs contains a non-negligible fraction of links from spam sites that threaten to overtake legitimate URLs due to their high branching factor; b) the DNS resolver succumbs to the rate at which new hostnames are dynamically created within a single domain; and c) the crawler becomes vulnerable to the delay attack from sites that purposely introduce HTTP and DNS delays in all requests originating from the crawler's IP address.\nNo prior research crawler has attempted to avoid spam or document its impact on the collected data. Thus, designing low-overhead and robust algorithms for computing site reputation during the crawl is the second open problem that we aim to address in this work.", "publication_ref": ["b3", "b23", "b25", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Politeness", "text": "Even today, webmasters become easily annoyed when web crawlers slow down their servers, consume too much Internet bandwidth, or simply visit pages with \"too much\" frequency. This leads to undesirable consequences including blocking of the crawler from accessing the site in question, various complaints to the ISP hosting the crawler, and even threats of legal action. Incorporating per-website and per-IP hit limits into a crawler is easy; however, preventing the crawler from \"choking\" when its entire RAM gets filled up with URLs pending for a small set of hosts is much more challenging. When N grows into the billions, the crawler ultimately becomes bottlenecked by its own politeness and is then faced with a decision to suffer significant slowdown, ignore politeness considerations for certain URLs (at the risk of crashing target servers or wasting valuable bandwidth on huge spam farms), or discard a large fraction of backlogged URLs, none of which is particularly appealing.\nWhile related work [2], [6], [13], [23], [27] has proposed several algorithms for rate-limiting host access, none of these studies have addressed the possibility that a crawler may stall due to its politeness restrictions or discussed management of rate-limited URLs that do not fit into RAM. This is the third open problem that we aim to solve in this paper.", "publication_ref": ["b1", "b5", "b12", "b23", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "Our Contributions", "text": "The first part of the paper presents a set of web-crawler algorithms that address the issues raised above and the second part briefly examines their performance in an actual web crawl. 2 Our design stems from three years of web crawling experience at Texas A&M University using an implementation we call IRLbot [16] and the various challenges posed in simultaneously: 1) sustaining a fixed crawling rate of several thousand pages/s; 2) downloading billions of pages; and 3) operating with the resources of a single server.\nThe first performance bottleneck we faced was caused by the complexity of verifying uniqueness of URLs and their compliance with robots.txt. As N scales into many billions, even the disk algorithms of [23], [27] no longer keep up with the rate at which new URLs are produced by our crawler (i.e., up to 184K per second). To understand this problem, we analyze the URL-check methods proposed in the literature and show that all of them exhibit severe performance limitations when N becomes sufficiently large. We then introduce a new technique called Disk Repository with Update Management (DRUM) that can store large volumes of arbitrary hashed data on disk and implement very fast check, update, and check+update operations using bucket sort. We model the various approaches and show that DRUM's overhead remains close to the best theoretically possible as N reaches into the trillions of pages and that for common disk and RAM size, DRUM can be thousands of times faster than prior disk-based methods.\nThe second bottleneck we faced was created by multimillion-page sites (both spam and legitimate), which became backlogged in politeness rate-limiting to the point of overflowing the RAM. This problem was impossible to overcome unless politeness was tightly coupled with site reputation. In order to determine the legitimacy of a given domain, we use a very simple algorithm based on the number of incoming links from assets that spammers cannot grow to infinity. Our algorithm, which we call Spam Tracking and Avoidance through Reputation (STAR), dynamically allocates the budget of allowable pages for each domain and all of its subdomains in proportion to the number of in-degree links from other domains. This computation can be done in real time with little overhead using DRUM even for millions of domains in the Internet. Once the budgets are known, the rates at which pages can be downloaded from each domain are scaled proportionally to the corresponding budget.\nThe final issue we faced in later stages of the crawl was how to prevent live-locks in processing URLs that exceed their budget. Periodically re-scanning the queue of overbudget URLs produces only a handful of good links at the cost of huge overhead. As N becomes large, the crawler ends up spending all of its time cycling through failed URLs and makes very little progress. The solution to this problem, which we call Budget Enforcement with Anti-Spam Tactics (BEAST), involves a dynamically increasing number of disk queues among which the crawler spreads the URLs based on whether they fit within the budget or not. As a result, almost all pages from sites that significantly exceed their budgets are pushed into the last queue and are examined with lower frequency as N increases. This keeps the overhead of reading spam at some fixed level and effectively prevents it from \"snowballing.\"\nThe above algorithms were deployed in IRLbot [16] and tested on the Internet in June-August 2007 using a single server attached to a 1 gb/s backbone of Texas A&M. Over a period of 41 days, IRLbot issued 7, 606, 109, 371 connection requests, received 7, 437, 281, 300 HTTP responses from 117, 576, 295 hosts, and successfully downloaded N = 6, 380, 051, 942 unique HTML pages at an average rate of 319 mb/s (1, 789 pages/s). After handicapping quickly branching spam and over 30 million low-ranked domains, IRLbot parsed out 394, 619, 023, 142 links and found 41, 502, 195, 631 unique pages residing on 641, 982, 061 hosts, which explains our interest in crawlers that scale to tens and hundreds of billions of pages as we believe a good fraction of 35B URLs not crawled in this experiment contains useful content.", "publication_ref": ["b1", "b15", "b23", "b27", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "RELATED WORK", "text": "There is only a limited number of papers describing detailed web-crawler algorithms and offering their experimen-  tal performance. First-generation designs [8], [22], [25], [26] were developed to crawl the infant web and commonly reported collecting less than 100, 000 pages. Second-generation crawlers [2], [6], [14], [13], [23], [27] often pulled several hundred million pages and involved multiple agents in the crawling process. We discuss their design and scalability issues in the next section.\nAnother direction was undertaken by the Internet Archive [5], [15], which maintains a history of the Internet by downloading the same set of pages over and over. In the last 10 years, this database has collected over 85 billion pages, but only a small fraction of them are unique. Additional crawlers are [4], [7], [12], [19], [28], [29]; however, their focus usually does not include the large scale assumed in this paper and their fundamental crawling algorithms are not presented in sufficient detail to be analyzed here.\nThe largest prior crawl using a fully-disclosed implementation appeared in [23], where Mercator obtained N = 473 million HTML pages in 17 days (we exclude non-HTML content since it has no effect on scalability). The fastest reported crawler was [12] with 816 pages/s, but the scope of their experiment was only N = 25 million. Finally, to our knowledge, the largest webgraph used in any paper was AltaVista's 2003 crawl with 1.4B pages and 6.6B links [10].", "publication_ref": ["b7", "b22", "b25", "b26", "b1", "b5", "b13", "b12", "b23", "b27", "b4", "b14", "b3", "b6", "b11", "b18", "b28", "b29", "b23", "b11", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "OBJECTIVES AND CLASSIFICATION", "text": "This section formalizes the purpose of web crawling and classifies algorithms in related work, some of which we study later in the paper. Due limited space, all proofs in this paper have been relegated to the technical report [20].", "publication_ref": ["b19"], "figure_ref": [], "table_ref": []}, {"heading": "Crawler Objectives", "text": "We assume that the ideal task of a crawler is to start from a set of seed URLs \u2126 0 and eventually crawl the set of all pages \u2126 \u221e that can be discovered from \u2126 0 using HTML links. The crawler is allowed to dynamically change the order in which URLs are downloaded in order to achieve a reasonably good coverage of \"useful\" pages \u2126 U \u2286 \u2126 \u221e in some finite amount of time. Due to the existence of legitimate sites with hundreds of millions of pages (e.g., ebay.com, yahoo.com, blogspot.com), the crawler cannot make any restricting assumptions on the maximum number of pages per host, the number of hosts per domain, the number of domains in the Internet, or the number of pages in the crawl. We thus classify algorithms as non-scalable if they impose hard limits on any of these metrics or are unable to maintain crawling speed when these parameters become very large.\nWe should also explain why this paper focuses on the performance of a single server rather than some distributed ar-chitecture. If one server can scale to N pages and maintain speed S, then with sufficient bandwidth it follows that m servers can maintain speed mS and scale to mN pages by simply partitioning the subset of all URLs and data structures between themselves (we assume that the bandwidth needed to shuffle the URLs between the servers is also well provisioned). Therefore, the aggregate performance of a server farm is ultimately governed by the characteristics of individual servers and their local limitations. We explore these limits in detail throughout the paper.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Crawler Operation", "text": "The functionality of a basic web crawler can be broken down into several phases: 1) removal of the next URL u from the queue Q of pending pages; 2) download of u and extraction of new URLs u 1 , . . . , u k from u's HTML tags; 3) for each u i , verification of uniqueness against some structure URLseen and checking compliance with robots.txt using some other structure RobotsCache; 4) addition of passing URLs to Q and URLseen; 5) update of RobotsCache if necessary. The crawler may also maintain its own DNScache structure in cases when the local DNS server is not able to efficiently cope with the load (e.g., its RAM cache does not scale to the number of hosts seen by the crawler or it becomes very slow after caching hundreds of millions of records).\nA summary of prior crawls and their methods in managing URLseen, RobotsCache, DNScache, and queue Q is shown in Table 1. The table demonstrates that two approaches to storing visited URLs have emerged in the literature: RAMonly and hybrid RAM-disk. In the former case [2], [5], [6], crawlers keep a small subset of hosts in memory and visit them repeatedly until a certain depth or some target number of pages have been downloaded from each site. URLs that do not fit in memory are discarded and sites are assumed to never have more than some fixed volume of pages. This approach performs truncated web crawls that require different techniques from those studied here and will not be considered in our comparison.\nIn the latter approach [13], [23], [25], [27], URLs are first checked against a buffer of popular links and those not found are examined using a disk file. The RAM buffer may be an LRU cache [13], [23], an array of recently added URLs [13], [23], a general-purpose database with RAM caching [25], and a balanced tree of URLs pending a disk check [27]. To fully understand whether caching provides improved performance, one must consider a complex interplay between the available CPU capacity, spare RAM size, disk speed, performance of the caching algorithm, and crawling rate. Due to insufficient space, we do not study caching here and direct the reader to the technical report [20].\nMost prior approaches keep RobotsCache in RAM and either crawl each host to exhaustion [2], [5], [6] or use an LRU cache in memory [13], [23]. The only hybrid approach is used in [27], which employs a general-purpose database for storing downloaded robots.txt and relevant DNS records. Finally, with the exception of [27], prior crawlers do not perform DNS caching and rely on the local DNS server to store these records for them.", "publication_ref": ["b1", "b4", "b5", "b12", "b23", "b25", "b27", "b12", "b23", "b12", "b23", "b25", "b27", "b19", "b1", "b4", "b5", "b12", "b23", "b27", "b27"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "SCALABILITY OF DISK METHODS", "text": "We next describe disk-check algorithms proposed in prior literature, analyze their performance, and then introduce our approach.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithms", "text": "Mercator-B [23] and Polybot [27] use a so-called batch disk check -they accumulate a buffer of URLs in memory and then merge it with a sorted URLseen file in one pass. Mercator-B stores only hashes of new URLs in RAM and places their text on disk. In order to retain the mapping from hashes to the text, a special pointer is attached to each hash. After the memory buffer is full, it is sorted in place and then compared with blocks of URLseen as they are read from disk. Non-duplicate URLs are merged with those already on disk and written into the new version of URLseen. Pointers are then used to recover the text of unique URLs and append it to the disk queue.\nPolybot keeps the entire URLs (i.e., actual strings) in memory and organizes them into a binary search tree. Once the tree size exceeds some threshold, it is merged with the disk file URLseen, which contains compressed URLs already seen by the crawler. Besides being enormously CPU intensive (i.e., compression of URLs and search in binary string trees are rather slow in our experience), this method has to perform more frequent scans of URLseen than Mercator-B due to the less-efficient usage of RAM.", "publication_ref": ["b23", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "Modeling Prior Methods", "text": "Assume the crawler is in some steady state where the probability of uniqueness p among new URLs remains constant (we verify that this holds in practice later in the paper). Further assume that the current size of URLseen is U entries, the size of RAM allocated to URL checks is R, the average number of links per downloaded page is l, the average URL length is b, the URL compression ratio is q, and the crawler expects to visit N pages. It then follows that n = lN links must pass through URL check, np of them are unique, and bq is the average number of bytes in a compressed URL. Finally, denote by H the size of URL hashes used by the crawler and P the size of a memory pointer. Then we have the following result.\nTheorem 1. The overhead of URLseen batch disk check is \u03c9(n, R) = \u03b1(n, R)bn bytes, where for Mercator-B:\n\u03b1(n, R) = 2(2U H + pHn)(H + P ) bR + 2 + p (1)\nand for Polybot:\n\u03b1(n, R) = 2(2U bq + pbqn)(b + 4P ) bR + p. (2\n)\nThis result shows that \u03c9(n, R) is a product of two elements: the number of bytes bn in all parsed URLs and how many times \u03b1(n, R) they are written to/read from disk. If \u03b1(n, R) grows with n, the crawler's overhead will scale superlinearly and may eventually become overwhelming to the point of stalling the crawler. As n \u2192 \u221e, the quadratic term in \u03c9(n, R) dominates the other terms, which places Mercator-B's asymptotic performance at\n\u03c9(n, R) = 2(H + P )pH R n 2 (3)\nand that of Polybot at\n\u03c9(n, R) = 2(b + 4P )pbq R n 2 . (4\n)\nThe ratio of these two terms is (H + P )H/[bq(b + 4P )], which for the IRLbot case with H = 8 bytes/hash, P = 4 bytes/pointer, b = 110 bytes/URL, and using very optimistic bq = 5 bytes/URL shows that Mercator-B is roughly 7.2 times faster than Polybot as n \u2192 \u221e.\nThe best performance of any method that stores the text of URLs on disk before checking them against URLseen (e.g., Mercator-B) is \u03b1 min = 2 + p, which is the overhead needed to write all bn bytes to disk, read them back for processing, and then append bpn bytes to the queue. Methods with memory-kept URLs (e.g., Polybot) have an absolute lower bound of \u03b1 min = p, which is the overhead needed to write the unique URLs to disk. Neither bound is achievable in practice, however.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "DRUM", "text": "We now describe the URL-check algorithm used in IRLbot, which belongs to a more general framework we call Disk Repository with Update Management (DRUM). The purpose of DRUM is to allow for efficient storage of large collections of <key,value> pairs, where key is a unique identifier (hash) of some data and value is arbitrary information attached to the key. There are three supported operations on these pairs -check, update, and check+update. In the first case, the incoming set of data contains keys that must be checked against those stored in the disk cache and classified as being duplicate or unique. For duplicate keys, the value associated with each key can be optionally retrieved from disk and used for some processing. In the second case, the incoming list contains <key,value> pairs that need to be merged into the existing disk cache. If a given key exists, its value is updated (e.g., overridden or incremented); if it does not, a new entry is created in the disk file. Finally, the third operation performs both check and update in one pass through the disk cache. Also note that DRUM may be supplied with a mixed list where some entries require just a check, while others need an update.\nA high-level overview of DRUM is shown in Figure 1. In the figure, a continuous stream of tuples <key,value,aux> arrives into DRUM, where aux is some auxiliary data associated with each key. DRUM spreads pairs <key,value> between k disk buckets Q H 1 , . . . , Q H k based on their key (i.e., all keys in the same bucket have the same bit-prefix). This is accomplished by feeding pairs <key,value> into k memory arrays of size M each and then continuously writing them to disk as the buffers fill up. The aux portion of each key (which usually contains the text of URLs) from the i-th bucket is kept in a separate file Q T i in the same FIFO order as pairs <key,value> in Q H i . Note that to maintain fast sequential writing/reading, all buckets are pre-allocated on disk before they are used. Once the largest bucket reaches a certain size r < R, the following process is repeated for i = 1, . . . , k: 1) bucket Q H i is read into the bucket buffer shown in Figure 1 and sorted; 2) the disk cache Z is sequentially read in chunks of \u2206 bytes and compared with the keys in bucket Q H i to determine their uniqueness; 3) those <key,value> pairs in Q H i that require an update are merged with the contents of the disk cache and written to the updated version of Z; 4) after all unique keys in Q H i are found, their original order is restored, Q T i is sequentially read into memory in blocks of size \u2206, and the corresponding aux portion of each unique key is sent for further processing (see below). An important aspect of this algorithm is that all buckets are checked in one pass through disk cache Z. 3 We now explain how DRUM is used for storing crawler data. The most important DRUM object is URLseen, which implements only one operation -check+update. Incoming tuples are <URLhash,-,URLtext>, where the key is an 8-byte hash of each URL, the value is empty, and the auxiliary data is the URL string. After all unique URLs are found, their text strings (aux data) are sent to the next queue for possible crawling. For caching robots.txt, we have another DRUM structure called RobotsCache, which supports asynchronous check and update operations. For checks, it receives tuples <HostHash,-,URLtext> and for updates <HostHash, HostData,->, where HostData contains the robots.txt file, IP address of the host, and optionally other host-related information. The last DRUM object of this section is called RobotsRequested and is used for storing the hashes of sites for which a robots.txt has been requested. Similar to URLseen, it only supports simultaneous check+update and its incoming tuples are <HostHash,-,HostText>.\nFigure 2 shows the flow of new URLs produced by the crawling threads. They are first sent directly to URLseen using check+update. Duplicate URLs are discarded and unique ones are sent for verification of their compliance with the budget (both STAR and BEAST are discussed later in the paper). URLs that pass the budget are queued to be checked against robots.txt using RobotsCache. URLs that have a matching robots.txt file are classified immediately as passing or failing. Passing URLs are queued in Q and later downloaded by the crawling threads. Failing URLs are discarded.\nURLs that do not have a matching robots.txt are sent to the back of queue Q R and their hostnames are passed It should be noted that URLs are kept in memory only when they are needed for immediate action and all queues in Figure 2 are stored on disk. We should also note that DRUM data structures can support as many hostnames, URLs, and robots.txt exception rules as disk space allows.", "publication_ref": ["b2"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "DRUM Model", "text": "Assume that the crawler maintains a buffer of size M = 256 KB for each open file and that the hash bucket size r must be at least \u2206 = 32 MB to support efficient reading during the check-merge phase. Further assume that the crawler can use up to D bytes of disk space for this process. Then we have the following result.\nTheorem 2. Assuming that R \u2265 2\u2206(1+P/H), DRUM's URLseen overhead is \u03c9(n, R) = \u03b1(n, R)bn bytes, where:\n\u03b1(n, R) = 8M (H+P )(2U H+pHn) bR 2 + 2 + p + 2H b R 2 < \u039b (H+b)(2U H+pHn) bD + 2 + p + 2H b R 2 \u2265 \u039b (5) and \u039b = 8M D(H + P )/(H + b).\nThe two cases in (5) can be explained as follows. The first condition R 2 < \u039b means that R is not enough to fill up the entire disk space D since 2M k memory buffers do not leave enough space for the bucket buffer with size r \u2265 \u2206. In this case, the overhead depends only on R since it is the bottleneck of the system. The second case R 2 \u2265 \u039b means that memory size allows the crawler to use more disk space than D, which results in the disk now becoming the bottleneck. In order to match D to a given RAM size R  and avoid unnecessary allocation of disk space, one should operate at the optimal point given by R 2 = \u039b:\nDopt = R 2 (H + b) 8M (H + P ) . (6\n)\nFor example, R = 1 GB produces D opt = 4.39 TB and R = 2 GB produces D opt = 17 TB. For D = D opt , the corresponding number of buckets is kopt = R/(4M ), the size of the bucket buffer is r opt = RH/[2(H + P )] \u2248 0.33R, and the leading quadratic term of \u03c9(n, R) in ( 5) is now R/(4M ) times smaller than in Mercator-B. This ratio is 1, 000 for R = 1 GB and 8, 000 for R = 8 GB. The asymptotic speedup in either case is significant.\nFinally, observe that the best possible performance of any method that stores both hashes and URLs on disk is \u03b1 min = 2 + p + 2H/b.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Comparison", "text": "We next compare disk performance of the studied methods when non-quadratic terms in \u03c9(n, R) are non-negligible. Table 2 shows \u03b1(n, R) of the three studied methods for fixed RAM size R and disk D as N increases from 800 million to 8 trillion (p = 1/9, U = 100M pages, b = 110 bytes, l = 59 links/page). As N reaches into the trillions, both Mercator-B and Polybot exhibit overhead that is thousands of times larger than the optimal and invariably become \"bogged down\" in re-writing URLseen. On the other hand, DRUM stays within a factor of 50 from the best theoretically possible value (i.e., \u03b1 min = 2.256) and does not sacrifice nearly as much performance as the other two methods.\nSince disk size D is likely to be scaled with N in order to support the newly downloaded pages, we assume for the next example that D(n) is the maximum of 1 TB and the size of unique hashes appended to URLseen during the crawl of N pages, i.e., D(n) = max(pHn, 10 12 ). Table 3 shows how dynamically scaling disk size allows DRUM to keep the overhead virtually constant as N increases.\nTo compute the average crawling rate that the above methods support, assume that W is the average disk I/O speed and consider the next result.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4", "tab_6"]}, {"heading": "Theorem 3. Maximum download rate (in pages/s) supported by the disk portion of URL uniqueness checks is:", "text": "S disk = W \u03b1(n, R)bl . (7\n)\nWe use IRLbot's parameters to illustrate the applicability of this theorem. Neglecting the process of appending new URLs to the queue, the crawler's read and write overhead is symmetric. Then, assuming IRLbot's 1-GB/s read speed and 350-MB/s write speed (24-disk RAID-5), we obtain that its average disk read-write speed is equal to 675 MB/s. Al-  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "SPAM AND REPUTATION", "text": "This section explains the necessity for detecting spam during crawls and proposes a simple technique for computing domain reputation in real-time.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Problems with BFS", "text": "Prior crawlers [6], [13], [23], [27] have no documented spam-avoidance algorithms and are typically assumed to perform BFS traversals of the web graph. Several studies [1], [3] have examined in simulations the effect of changing crawl order by applying bias towards more popular pages. The conclusions are mixed and show that PageRank order [4] can be sometimes marginally better than BFS [1] and sometimes marginally worse [3], where the metric by which they are compared is the rate at which the crawler discovers popular pages.\nWhile BFS works well in simulations, its performance on infinite graphs and/or in the presence of spam farms remains unknown. Our early experiments show that crawlers eventually encounter a quickly branching site that will start to dominate the queue after 3 \u2212 4 levels in the BFS tree. Some of these sites are spam-related with the aim of inflating the page rank of target hosts, while others are created by regular users sometimes for legitimate purposes (e.g., calendars, testing of asp/php engines), sometimes for questionable purposes (e.g., intentional trapping of unwanted robots), and sometimes for no apparent reason at all. What makes these pages similar is the seemingly infinite number of dynamically generated pages and/or hosts within a given domain. Crawling these massive webs or performing DNS lookups on millions of hosts from a given domain not only places a significant burden on the crawler, but also wastes bandwidth on downloading largely useless content.\nSimply restricting the branching factor or the maximum number of pages/hosts per domain is not a viable solu-tion since there is a number of legitimate sites that contain over a hundred million pages and over a dozen million virtual hosts (i.e., various blog sites, hosting services, directories, and forums). For example, Yahoo currently reports indexing 1.2 billion objects just within its own domain and blogspot claims over 50 million users, each with a unique hostname. Therefore, differentiating between legitimate and illegitimate web \"monsters\" becomes a fundamental task of any crawler.\nNote that this task does not entail assigning popularity to each potential page as would be the case when returning query results to a user; instead, the crawler needs to decide whether a given domain or host should be allowed to massively branch or not. Indeed, spam-sites and various auto-generated webs with a handful of pages are not a problem as they can be downloaded with very little effort and later classified by data-miners using PageRank or some other appropriate algorithm. The problem only occurs when the crawler assigns to domain x download bandwidth that is disproportionate to the value of x's content.\nAnother aspect of spam classification is that it must be performed with very little CPU/RAM/disk effort and run in real-time at speed SL links per second, where L is the number of unique URLs per page.", "publication_ref": ["b5", "b12", "b23", "b27", "b0", "b2", "b3", "b0", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Controlling Massive Sites", "text": "Before we introduce our algorithm, several definitions are in order. Both host and site refer to Fully Qualified Domain Names (FQDNs) on which valid pages reside (e.g., motors.ebay.com). A server is a physical host that accepts TCP connections and communicates content to the crawler. Note that multiple hosts may be co-located on the same server. A top-level domain (TLD) or a country-code TLD (cc-TLD) is a domain one level below the root in the DNS tree (e.g., .com, .net, .uk). A pay-level domain (PLD) is any domain that requires payment at a TLD or cc-TLD registrar. PLDs are usually one level below the corresponding TLD (e.g., amazon.com), with certain exceptions for cc-TLDs (e.g., ebay.co.uk, det.wa.edu.au). We use a comprehensive list of custom rules for identifying PLDs, which have been compiled as part of our ongoing DNS project.\nWhile computing PageRank [18], BlockRank [17], or Sit-eRank [9], [30] is a potential solution to the spam problem, these methods become extremely disk intensive in large-scale applications (e.g., 41 billion pages and 641 million hosts found in our crawl) and arguably with enough effort can be manipulated [11] by huge link farms (i.e., millions of pages and sites pointing to a target spam page). In fact, strict page-level rank is not absolutely necessary for controlling massively branching spam. Instead, we found that spam could be \"deterred\" by budgeting the number of allowed pages per PLD based on domain reputation, which we determine by domain in-degree from resources that spammers must pay for. There are two options for these resources -PLDs and IP addresses. We chose the former since classification based on IPs (first suggested in Lycos [21]) has proven less effective since large subnets inside link farms could be given unnecessarily high priority and multiple independent sites co-hosted on the same IP were improperly discounted.\nWhile it is possible to classify each site and even each subdirectory based on their PLD in-degree, our current implementation uses a coarse-granular approach of only limiting spam at the PLD level. Each PLD x starts with  a default budget B 0 , which is dynamically adjusted using some function F (d x ) as x's in-degree d x changes. Budget Bx represents the number of pages that are allowed to pass from x (including all hosts and subdomains in x) to crawling threads every T time units. Figure 3 shows how our system, which we call Spam Tracking and Avoidance through Reputation (STAR), is organized. In the figure, crawling threads aggregate PLD-PLD link information and send it to a DRUM structure PLDindegree, which uses a batch update to store for each PLD x its hash h x , in-degree d x , current budget B x , and hashes of all indegree neighbors in the PLD graph. Unique URLs arriving from URLseen perform a batch check against PLDindegree, and are given B x on their way to BEAST, which we discuss in the next section.\nNote that by varying the budget function F (d x ), one can implement a number of policies -crawling of only popular pages (i.e., zero budget for low-ranked domains and maximum budget for high-ranked domains), equal distribution between all domains (i.e., budget B x = B 0 for all x), and crawling with a bias toward popular/unpopular pages (i.e., budget directly/inversely proportional to the PLD indegree).", "publication_ref": ["b17", "b16", "b8", "b30", "b10", "b21"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "POLITENESS AND BUDGETS", "text": "This section discusses how to enable polite crawler operation and scalably enforce budgets.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Rate Limiting", "text": "One of the main goals of IRLbot from the beginning was to adhere to strict rate-limiting policies in accessing poorly provisioned (in terms of bandwidth or server load) sites. Even though larger sites are much more difficult to crash, unleashing a crawler that can download at 500 mb/s and allowing it unrestricted access to individual machines would generally be regarded as a denial-of-service attack.\nPrior work has only enforced a certain per-host access delay \u03c4 h (which varied from 10 times the download delay of a page [23] to 30 seconds [27]), but we discovered that this presented a major problem for hosting services that co-located thousands of virtual hosts on the same physical server and did not provision it to support simultaneous access to all sites (which in our experience is rather common in the current Internet). Thus, without an additional per-server limit \u03c4 s , such hosts could be easily crashed or overloaded.\nWe keep \u03c4 h = 40 seconds for accessing all low-ranked PLDs, but then for high-ranked PLDs scale it down proportional to B x , up to some minimum value \u03c4 0 h . The reason for doing so is to prevent the crawler from becoming \"bogged down\" in a few massive sites with millions of pages in RAM. Without this rule, the crawler would make very slow progress through individual sites in addition to eventually running out of RAM as it becomes clogged with URLs from a few \"monster\" networks. For similar reasons, we keep per-server crawl delay \u03c4 s at the default 1 second for lowranked domains and scale it down with the average budget of PLDs hosted on the server, up to some minimum \u03c4 0 s . By properly controlling the coupling between budgets and crawl delays, one can ensure that the rate at which pages are admitted into RAM is no less than their crawl rate, which results in no memory backlog.", "publication_ref": ["b23", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "Budget Checks", "text": "We now discuss how IRLbot's budget enforcement works in a method we call Budget Enforcement with Anti-Spam Tactics (BEAST). The goal of this method is not to discard URLs, but rather to delay their download until more is known about their legitimacy. Most sites have a low rank because they are not well linked to, but this does not necessarily mean that their content is useless or they belong to a spam farm. All other things equal, low-ranked domains should be crawled in some approximately round-robin fashion with careful control of their branching. In addition, as the crawl progresses, domains change their reputation and URLs that have earlier failed the budget check need to be rebudgeted and possibly crawled at a different rate. Ideally, the crawler should shuffle URLs without losing any of them and eventually download the entire web if given infinite time.\nA naive implementation of budget enforcement in prior versions of IRLbot maintained two queues Q and QF , where Q contained URLs that had passed the budget and Q F those that had failed. After Q was emptied, Q F was read in its entirety and again split into two queues -Q and Q F . This process was then repeated indefinitely.\nWe next offer a simple overhead model for this algorithm. As before, assume that S is the number of pages crawled per second and b is the average URL size. Further define E[B x ] < \u221e to be the expected budget of a domain in the Internet, V to be the total number of PLDs seen by the crawler in one pass through QF , and L to be the number of unique URLs per page (recall that l in our earlier notation allowed duplicate links). The next result shows that the naive version of BEAST must increase disk I/O performance with crawl size N . Theorem 4. Lowest disk I/O speed (in bytes/s) that allows the naive budget-enforcement approach to download N pages at fixed rate S is:\n\u03bb = 2Sb(L \u2212 1)\u03b1 N , (8\n)\nwhere\n\u03b1N = max 1, N E[Bx]V . (9\n)\nThis theorem shows that \u03bb \u223c \u03b1N = \u0398(N ) and that rechecking failed URLs will eventually overwhelm any crawler regardless of its disk performance. For IRLbot (i.e., V = 33M, E[B x ] = 11, L = 6.5, S = 3, 100 pages/s, and b = 110), we get \u03bb = 3.8 MB/s for N = 100 million, \u03bb = 83 MB/s for N = 8 billion, and \u03bb = 826 MB/s for N = 80 billion. Given other disk-intensive tasks, IRLbot's bandwidth for BEAST was capped at about 100 MB/s, which explains why this design eventually became a bottleneck in actual crawls.  The correct implementation of BEAST rechecks Q F at exponentially increasing intervals. As shown in Figure 4, suppose the crawler starts with j \u2265 1 queues Q1, . . . , Qj, where Q 1 is the current queue and Q j is the last queue. URLs are read from the current queue Q 1 and written into queues Q 2 , . . . , Q j based on their budgets. Specifically, for a given domain x with budget Bx, the first Bx URLs are sent into Q2, the next Bx into Q3 and so on. BEAST can always figure out where to place URLs using a combination of B x (attached by STAR to each URL) and a local array that keeps for each queue Qj the left-over budget of each domain. URLs that do not fit in Qj are all placed in QF as in the previous design.\nAfter Q 1 is emptied, the crawler moves to reading the next queue Q 2 and spreads newly arriving pages between Q3, . . . , Qj, Q1 (note the wrap-around). After it finally empties Qj, the crawler re-scans QF and splits it into j additional queues Q j+1 , . . . , Q 2j . URLs that do not have enough budget for Q 2j are placed into the new version of Q F . The process then repeats starting from Q 1 until j reaches some maximum OS-imposed limit or the crawl terminates.\nThere are two benefits to this approach. First, URLs from sites that exceed their budget by a factor of j or more are pushed further back as j increases. This leads to a higher probability that good URLs with enough budget will be queued and crawled ahead of URLs in QF . The second benefit, shown in the next theorem, is that the speed at which the disk must be read does not skyrocket to infinity. Theorem 5. Lowest disk I/O speed (in bytes/s) that allows BEAST to download N pages at fixed rate S is:\n\u03bb = 2Sb 2\u03b1 N 1 + \u03b1N (L \u2212 1) + 1 \u2264 2Sb(2L \u2212 1). (10\n)\nFor N \u2192 \u221e and fixed V , disk speed \u03bb \u2192 2Sb(2L \u2212 1), which is roughly four times the speed needed to write all unique URLs to disk as they are discovered during the crawl.\nFor the examples used earlier in this section, this implementation needs \u03bb \u2264 8.2 MB/s regardless of crawl size N . From the proof of Theorem 5 in [20], it also follows that the last stage of an N -page crawl will contain:\nj = 2 log 2 (\u03b1 N +1) \u22121 (11)\nqueues. This value for N = 8B is 16 and for N = 80B only 128, neither of which is too imposing for a modern server.", "publication_ref": ["b19"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "EXPERIMENTS", "text": "This section briefly examines the important parameters of the crawl and highlights our observations. We next plot average 10-minute download rates for the active duration of the crawl in Figure 5, in which fluctuations correspond to day/night bandwidth limits imposed by the university. 5 The average download rate during this crawl was 319 mb/s (1, 789 pages/s) with the peak 10-minute average rate of 470 mb/s (3, 134 pages/s). The crawler received 143 TB of data, out of which 254 GB were robots.txt files, and transmitted 1.8 TB of HTTP requests. The parser processed 161 TB of HTML code (i.e., 25.2 KB per uncompressed page) and the gzip library handled 6.6 TB of HTML data containing 1, 050, 955, 245 pages, or 16% of the total. The average compression ratio was 1:5, which resulted in the peak parsing demand being close to 800 mb/s (i.e., 1.64 times faster than the maximum download rate).\nIRLbot parsed out 394, 619, 023, 142 links from downloaded pages. After discarding invalid URLs and known non-HTML extensions, the crawler was left with K = 374, 707, 295, 503 potentially \"crawlable\" links that went through URL uniqueness checks. We use this number to obtain K/N = l \u2248 59 links/page used throughout the paper. The average URL size was 70.6 bytes (after removing \"http://\"), but with crawler overhead (e.g., depth in the crawl tree, IP address and port, timestamp, and parent link) attached to each URL, their average size in the queue was b \u2248 110 bytes. The number of pages recorded in URLseen was 41, 502, 195, 631 (332 GB on disk), which yielded L = 6.5 unique URLs per page. These pages were hosted by 641, 982, 061 unique sites.\nAs promised earlier, we now show in Figure 6(a) that the probability of uniqueness p stabilizes around 0.11 once the first billion pages have been downloaded. Since p is bounded away from 0 even at N = 6.3 billion, this suggests that our crawl has discovered only a small fraction of the web. While we certainly know there are at least 41 billion pages in the 5 The day limit was 250 mb/s for days 5 \u2212 32 and 200 mb/s for the rest of the crawl. The night limit was 500 mb/s. Internet, the fraction of them with useful content and the number of additional pages not seen by the crawler remain a mystery at this stage.", "publication_ref": ["b4", "b4"], "figure_ref": ["fig_3", "fig_4"], "table_ref": []}, {"heading": "Domain Reputation", "text": "The crawler received responses from 117, 576, 295 sites, which belonged to 33, 755, 361 pay-level domains (PLDs) and were hosted on 4, 260, 532 unique IPs. The total number of nodes in the PLD graph was 89, 652, 630 with the number of PLD-PLD edges equal to 1, 832, 325, 052. During the crawl, IRLbot performed 260, 113, 628 DNS lookups, which resolved to 5, 517, 743 unique IPs.\nWithout knowing how our algorithms would perform, we chose a conservative budget function F (d x ) where the crawler would give only moderate preference to highly-ranked domains and try to branch out to discover a wide variety of lowranked PLDs. Specifically, top 10K ranked domains were given budget B x linearly interpolated between 10 and 10K pages. All other PLDs received the default budget B0 = 10. Figure 6(b) shows the average number of downloaded pages per PLD x based on its in-degree d x . IRLbot crawled on average 1.2 pages per PLD with d x = 1 incoming link, 68 pages per PLD with d x = 2, and 43K pages per domain with dx \u2265 512K. The largest number of pages pulled from any PLD was 347, 613 (blogspot.com), while 90% of visited domains contributed to the crawl fewer than 586 pages each and 99% fewer than 3, 044 each. As seen in the figure, IRLbot succeeded at achieving a strong correlation between domain popularity (i.e., in-degree) and the amount of bandwidth allocated to that domain during the crawl.\nOur manual analysis of top-1000 domains shows that most of them are highly-ranked legitimate sites, which attests to the effectiveness of our ranking algorithm. Several of them are listed in Table 4 together with Google's PageRank of the main page of each PLD and the number of pages downloaded by IRLbot. The exact coverage of each site depended on its link structure, as well as the number of hosts and physical servers (which determined how polite the crawler needed to be). By changing the budget function F (dx), much more aggressive crawls of large sites could be achieved, which may be required in practical search-engine applications.\nWe believe that PLD-level domain ranking by itself is not sufficient for preventing all types of spam from infiltrating the crawl and that additional fine-granular ranking algorithms may be needed for classifying individual hosts within a domain and possibly their subdirectory structure. Future  ", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": ["tab_10"]}, {"heading": "CONCLUSION", "text": "This paper tackled the issue of scaling web crawlers to billions and even trillions of pages using a single server with constant CPU, disk, and memory speed. We identified several impediments to building an efficient large-scale crawler and showed that they could be overcome by simply changing the BFS crawling order and designing low-overhead diskbased data structures. We experimentally tested our algorithms in the Internet and found them to scale much better than the methods proposed in prior literature.\nFuture work involves refining reputation algorithms, assessing their performance, and mining the collected data.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "ACKNOWLEDGMENT", "text": "We are grateful to Texas A&M University and its network administrators for providing the enormous amount of bandwidth needed for this project and patiently handling webmaster complaints.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Searching the Web", "journal": "ACM Transactions on Internet Technology", "year": "2001-08", "authors": "A Arasu; J Cho; H Garcia-Molina; A Paepcke; S Raghavan"}, {"ref_id": "b1", "title": "UbiCrawler: A Scalable Fully Distributed Web Crawler", "journal": "Software: Practice & Experience", "year": "2004-07", "authors": "P Boldi; B Codenotti; M Santini; S Vigna"}, {"ref_id": "b2", "title": "Do Your Worst to Make the Best: Paradoxical Effects in PageRank Incremental Computations", "journal": "LNCS: Algorithms and Models for the Web-Graph", "year": "2004-10", "authors": "P Boldi; M Santini; S Vigna"}, {"ref_id": "b3", "title": "The Anatomy of a Large-Scale Hypertextual Web Search Engine", "journal": "", "year": "1998-04", "authors": "S Brin; L Page"}, {"ref_id": "b4", "title": "Crawling Towards Eternity: Building an Archive of the World Wide Web", "journal": "Web Techniques Magazine", "year": "1997-05", "authors": "M Burner"}, {"ref_id": "b5", "title": "Stanford WebBase Components and Applications", "journal": "ACM Transactions on Internet Technology", "year": "2006-05", "authors": "J Cho; H Garcia-Molina; T Haveliwala; W Lam; A Paepcke; S R G Wesley"}, {"ref_id": "b6", "title": "An Adaptive Model for Optimizing Performance of an Incremental Web Crawler", "journal": "", "year": "2001-05", "authors": "J Edwards; K Mccurley; J Tomlin"}, {"ref_id": "b7", "title": "The RBSE Spider -Balancing Effective Search Against Web Load", "journal": "", "year": "1994-05", "authors": "D Eichmann"}, {"ref_id": "b8", "title": "AggregateRank: Bringing Order to Web Sites", "journal": "", "year": "2006-08", "authors": "G Feng; T.-Y Liu; Y Wang; Y Bao; Z Ma; X.-D Zhang; W.-Y Ma"}, {"ref_id": "b9", "title": "Scalable Computing for Power Law Graphs: Experience with Parallel PageRank", "journal": "", "year": "2005-11", "authors": "D Gleich; L Zhukov"}, {"ref_id": "b10", "title": "Link Spam Alliances", "journal": "", "year": "2005-08", "authors": "Z Gyongyi; H Garcia-Molina"}, {"ref_id": "b11", "title": "High Performance Crawling System", "journal": "", "year": "2004-10", "authors": "Y Hafri; C Djeraba"}, {"ref_id": "b12", "title": "Mercator: A Scalable, Extensible Web Crawler", "journal": "World Wide Web", "year": "1999-12", "authors": "A Heydon; M Najork"}, {"ref_id": "b13", "title": "WebBase: A Repository of Web Pages", "journal": "", "year": "2000-05", "authors": "J Hirai; S Raghavan; H Garcia-Molina; A Paepcke"}, {"ref_id": "b14", "title": "", "journal": "", "year": "", "authors": "Internet Archive"}, {"ref_id": "b15", "title": "IRLbot Project at Texas A&M", "journal": "", "year": "", "authors": ""}, {"ref_id": "b16", "title": "Exploiting the Block Structure of the Web for Computing PageRank", "journal": "", "year": "2003-03", "authors": "S D Kamvar; T H Haveliwala; C D Manning; G H Golub"}, {"ref_id": "b17", "title": "Extrapolation methods for accelerating PageRank computations", "journal": "", "year": "2003-05", "authors": "S D Kamvar; T H Haveliwala; C D Manning; G H Golub"}, {"ref_id": "b18", "title": "High Performance Large Scale Web Spider Architecture", "journal": "", "year": "2002-10", "authors": "K Koht-Arsa; S Sanguanpong"}, {"ref_id": "b19", "title": "IRLbot: Scaling to 6 Billion Pages and Beyond", "journal": "Texas A&M University", "year": "2008-02-02", "authors": "H.-T Lee; D Leonard; X Wang; D Loguinov"}, {"ref_id": "b20", "title": "Available", "journal": "", "year": "", "authors": ""}, {"ref_id": "b21", "title": "Lycos: Design Choices in an Internet Search Service", "journal": "IEEE Expert Magazine", "year": "1997-02", "authors": "M Mauldin"}, {"ref_id": "b22", "title": "GENVL and WWWW: Tools for Taming the Web", "journal": "", "year": "1994-05", "authors": "O A Mcbryan"}, {"ref_id": "b23", "title": "High-Performance Web Crawling", "journal": "Compaq Systems Research Center", "year": "2001-09", "authors": "M Najork; A Heydon"}, {"ref_id": "b24", "title": "Breadth-First Search Crawling Yields High-Quality Pages", "journal": "", "year": "2001-05", "authors": "M Najork; J L Wiener"}, {"ref_id": "b25", "title": "Finding What People Want: Experiences with the Web Crawler", "journal": "", "year": "1994-10", "authors": "B Pinkerton"}, {"ref_id": "b26", "title": "WebCrawler: Finding What People Want", "journal": "", "year": "2000", "authors": "B Pinkerton"}, {"ref_id": "b27", "title": "Design and Implementation of a High-Performance Distributed Web Crawler", "journal": "", "year": "2002-03", "authors": "V Shkapenyuk; T Suel"}, {"ref_id": "b28", "title": "Apoidea: A Decentralized Peer-to-Peer Architecture for Crawling the World Wide Web", "journal": "", "year": "2003-08", "authors": "A Singh; M Srivatsa; L Liu; T Miller"}, {"ref_id": "b29", "title": "ODISSEA: A Peer-to-Peer Architecture for Scalable Web Search and Information Retrieval", "journal": "", "year": "2003-06", "authors": "T Suel; C Mathur; J Wu; J Zhang; A Delis; M Kharrazi; X Long; K Shanmugasundaram"}, {"ref_id": "b30", "title": "Using SiteRank for Decentralized Computation of Web Document Ranking", "journal": "", "year": "2004-08", "authors": "J Wu; K Aberer"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Operation of DRUM.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Operation of STAR.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Operation of BEAST.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure 5: Download rates during the experiment.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 6 :6Figure 6: Evolution of p throughout the crawl and effectiveness of budget-control in limiting lowranked PLDs.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Comparison of prior crawlers and their data structures.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Overhead \u03b1(n, R) for R = 1 GB and D = 4.39 TB.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Overhead \u03b1(n, R) for D = D(n).", "figure_data": "locating 15% of this rate for checking URL uniqueness 4 , theeffective disk bandwidth of the server can be estimated atW = 101.25 MB/s. Given the conditions of Table 3 forR = 8 GB and assuming N = 8 trillion pages, DRUM yieldsa sustained download rate of S disk = 4, 192 pages/s (i.e.,711 mb/s using IRLbot's average HTML page size of 21.2KB). With 10 DRUM servers and a 10-gb/s Internet link,one could create a search engine with a download capacityof 100 billion pages per month. In crawls of the same scale,Mercator-B would be 3, 075 times slower and would admitan average rate of only 1.4 pages/s. Since with these pa-rameters Polybot is 7.2 times slower than Mercator-B, itsaverage crawling speed would be 0.2 pages/s."}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Top ranked PLDs, their PLD in-degree, Google PageRank, and total pages crawled.work will address this issue, but our first experiment with spam-control algorithms demonstrates that these methods are not only necessary, but also very effective in helping crawlers scale to billions of pages.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u03b1(n, R) = 2(2U H + pHn)(H + P ) bR + 2 + p (1)", "formula_coordinates": [4.0, 87.41, 630.85, 205.5, 21.29]}, {"formula_id": "formula_1", "formula_text": "\u03b1(n, R) = 2(2U bq + pbqn)(b + 4P ) bR + p. (2", "formula_coordinates": [4.0, 93.91, 672.09, 195.08, 21.29]}, {"formula_id": "formula_2", "formula_text": ")", "formula_coordinates": [4.0, 288.99, 677.9, 3.92, 9.41]}, {"formula_id": "formula_3", "formula_text": "\u03c9(n, R) = 2(H + P )pH R n 2 (3)", "formula_coordinates": [4.0, 383.47, 123.22, 172.46, 21.29]}, {"formula_id": "formula_4", "formula_text": "\u03c9(n, R) = 2(b + 4P )pbq R n 2 . (4", "formula_coordinates": [4.0, 382.05, 162.4, 169.96, 21.3]}, {"formula_id": "formula_5", "formula_text": ")", "formula_coordinates": [4.0, 552.0, 168.21, 3.92, 9.41]}, {"formula_id": "formula_6", "formula_text": "\u03b1(n, R) = 8M (H+P )(2U H+pHn) bR 2 + 2 + p + 2H b R 2 < \u039b (H+b)(2U H+pHn) bD + 2 + p + 2H b R 2 \u2265 \u039b (5) and \u039b = 8M D(H + P )/(H + b).", "formula_coordinates": [5.0, 316.81, 572.34, 239.12, 46.5]}, {"formula_id": "formula_7", "formula_text": "Dopt = R 2 (H + b) 8M (H + P ) . (6", "formula_coordinates": [6.0, 131.31, 197.2, 157.68, 22.58]}, {"formula_id": "formula_8", "formula_text": ")", "formula_coordinates": [6.0, 288.99, 204.29, 3.92, 9.41]}, {"formula_id": "formula_9", "formula_text": "S disk = W \u03b1(n, R)bl . (7", "formula_coordinates": [6.0, 136.32, 628.19, 152.67, 21.28]}, {"formula_id": "formula_10", "formula_text": ")", "formula_coordinates": [6.0, 288.99, 633.98, 3.92, 9.41]}, {"formula_id": "formula_11", "formula_text": "\u03bb = 2Sb(L \u2212 1)\u03b1 N , (8", "formula_coordinates": [8.0, 134.82, 569.71, 154.17, 9.81]}, {"formula_id": "formula_12", "formula_text": ")", "formula_coordinates": [8.0, 288.99, 569.71, 3.92, 9.41]}, {"formula_id": "formula_13", "formula_text": "\u03b1N = max 1, N E[Bx]V . (9", "formula_coordinates": [8.0, 124.14, 598.12, 164.85, 21.29]}, {"formula_id": "formula_14", "formula_text": ")", "formula_coordinates": [8.0, 288.99, 603.92, 3.92, 9.41]}, {"formula_id": "formula_15", "formula_text": "\u03bb = 2Sb 2\u03b1 N 1 + \u03b1N (L \u2212 1) + 1 \u2264 2Sb(2L \u2212 1). (10", "formula_coordinates": [8.0, 337.79, 524.72, 214.03, 21.29]}, {"formula_id": "formula_16", "formula_text": ")", "formula_coordinates": [8.0, 551.83, 530.52, 4.09, 9.41]}, {"formula_id": "formula_17", "formula_text": "j = 2 log 2 (\u03b1 N +1) \u22121 (11)", "formula_coordinates": [8.0, 397.59, 632.07, 158.33, 11.18]}], "doi": ""}