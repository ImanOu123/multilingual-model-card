{"title": "Influence of graph construction on graph-based clustering measures", "authors": "Markus Maier; Ulrike Von Luxburg; Matthias Hein", "pub_date": "", "abstract": "Graph clustering methods such as spectral clustering are defined for general weighted graphs. In machine learning, however, data often is not given in form of a graph, but in terms of similarity (or distance) values between points. In this case, first a neighborhood graph is constructed using the similarities between the points and then a graph clustering algorithm is applied to this graph. In this paper we investigate the influence of the construction of the similarity graph on the clustering results. We first study the convergence of graph clustering criteria such as the normalized cut (Ncut) as the sample size tends to infinity. We find that the limit expressions are different for different types of graph, for example the r-neighborhood graph or the k-nearest neighbor graph. In plain words: Ncut on a kNN graph does something systematically different than Ncut on an r-neighborhood graph! This finding shows that graph clustering criteria cannot be studied independently of the kind of graph they are applied to. We also provide examples which show that these differences can be observed for toy and real data already for rather small sample sizes.", "sections": [{"heading": "Introduction", "text": "In many areas of machine learning such as clustering, dimensionality reduction, or semi-supervised learning, neighborhood graphs are used to model local relationships between data points and to build global structure from local information. The easiest and most popular neighborhood graphs are the r-neighborhood graph, in which every point is connected to all other points within a distance of r, and the k-nearest neighbor (kNN) graph, in which every point is connected to the k closest neighboring points. When applying graph based machine learning methods to given sets of data points, there are several choices to be made: the type of the graph to construct (e.g., r-neighborhood graph or kNN graph), and the connectivity parameter (r or k, respectively). However, the question how these choices should be made has received only little attention in the literature. This is not so severe in the domain of supervised learning, where parameters can be set using cross-validation. However, it poses a serious problem in unsupervised learning. While different researchers use different heuristics and their \"gut feeling\" to set these parameters, neither systematic empirical studies have been conducted (for example: how sensitive are the results to the graph parameters?), nor do theoretical results exist which lead to well-justified heuristics. Our goal in this paper is to address the theoretical side of this question in the context of graph based clustering.\nIn this work, we consider clustering in a statistical setting: we assume that a finite set of data points has been sampled from some underlying distribution. Ultimately, what we want to find is a good clustering of the underlying data space. We assume that the quality of a clustering is defined by some clustering objective function. In this paper we focus on the case of the normalized cut objective function Ncut (Shi and Malik, 2000) and on the question if and how the results of graph based clustering algorithms are affected by the graph type and the parameters that are chosen for the construction of the neighborhood graph.\nTo this end, we first want to study the convergence of the clustering criterion (Ncut) on different kinds of graphs (kNN graph and r-neighborhood graph), as the sample size tends to infinity. To our own surprise, when studying this convergence it turned out that, depending on the type of graph, the normalized cut converges to different limit values! That is, the (suitably normalized) values of Ncut tend to a different limit functional, depending on whether we use the r-neighborhood graph or the kNN graph on the finite sample. Intuitively, what happens is as follows: On any given graph, the normalized cut is one unique, well-defined mathematical expression. But of course, given a fixed partition of a sample of points, this Ncut value is different for different graphs constructed on the sample (different graph constructions put different numbers of edges between points, which leads to different Ncut values). It can now be shown that even after appropriate rescaling, such differences remain visible in the limit for the sample size tending to infinity. For example, we will see that depending on the type of graph, the limit criterion integrates over different powers of the density. This can lead to the effect that the minimizer of Ncut on the kNN graph is different from the minimizer of Ncut on the r-graph.\nThis means that ultimately, the question about the \"best Ncut\" clustering, given infinite amount of data, has different answers, depending on which underlying graph we use! This observation opens Pandora's box on clustering criteria: the \"meaning\" of a clustering criterion does not only depend on the exact definition of the criterion itself, but also on how the graph on the finite sample is constructed. In the case of Ncut this means that Ncut is not just \"one well-defined criterion\", but it corresponds to a whole bunch of criteria, which differ depending on the underlying graph. More sloppy: Ncut on a kNN graph does something different than Ncut on an r-neighborhood graph!\nThe first part of our paper is devoted to the mathematical derivation of our results. We investigate how and under which conditions the Ncut criterion converges on the different graphs, and what the corresponding limit expressions are. The second part of our paper shows that these findings are not only of theoretical interest, but that they also influence concrete algorithms such as spectral clustering in practice. We give examples of well-clustered distributions (mixtures of Gaussians), where the optimal limit cut on the kNN graph is different from the one on the r-neighborhood graph. Moreover, these results can be reproduced with finite samples. That is, given a finite sample from some well-clustered distribution, normalized spectral clustering on the kNN graph produces systematically different results from spectral clustering on the r-neighborhood graph.", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}, {"heading": "Definitions and assumptions", "text": "Given a graph G = (V, E) with weights w : E \u2192 R and a partition of the nodes V into (C, V \\ C) we define cut(C, V \\ C) = u\u2208C,v\u2208V \\C w(u, v), vol(C) = u\u2208C,v\u2208V w(u, v), and\nNcut(C, V \\ C) = cut(C, V \\ C) 1 vol(C) + 1 vol(V \\ C) .\nGiven a finite set of points x 1 , . . . , x n we consider two main types of neighborhood graphs:\n\u2022 the r-neighborhood graph G n,r : there is an edge from point x i to point x j if dist(x i , x j ) \u2264 r for all 1 \u2264 i, j \u2264 n, i = j.\n\u2022 the directed k-nearest neighbor graph G n,k : there is a directed edge from x i to x j if x j is one of the k nearest neighbors of x i for 1 \u2264 i, j \u2264 n, i = j.\nIn the following we work on the space R d with Euclidean metric dist. We denote by \u03b7 d the volume of the d-dimensional unit ball in R d and by B(x, r) the ball with radius r centered at x. On the space R d we will study partitions which are induced by some hypersurface S. Given a surface S which separates the data points in two non-empty parts C + and C \u2212 , we denote by cut n,r (S) the number of edges in G n,r that go from a sample point on one side of the surface to a sample point on the other side of the surface. The corresponding quantity for the directed k-nearest neighbor graph is denoted by cut n,k (S). For a set A \u2286 R d the volume of {x 1 , . . . , x n } \u2229 A in the graph G n,r is denoted by vol n,r (A), and correspondingly vol n,k (A) in the graph G n,k .\nGeneral assumptions in the whole paper: The data points x 1 , ..., x n are drawn independently from some density p on R d . This density is bounded from below and above, that is 0 < p min \u2264 p(x) \u2264 p max . In particular, it has compact support C. We assume that the boundary \u2202C of C is well-behaved, that means it is a set of Lebesgue measure 0 and we can find a constant \u03b3 > 0 such that for r sufficiently small, vol(B(x, r) \u2229 C) \u2265 \u03b3 vol(B(x, r)) for all x \u2208 C. Furthermore we assume that p is twice differentiable in the interior of C and that the derivatives are bounded. The measure on R d induced by p will be denoted by \u00b5, that means, for a measurable set A we set \u00b5(A) = A p(x)dx. For the cut surface S, we assume that the volume of S \u2229 \u2202C with respect to the (d \u2212 1)-dimensional measure on S is a set of measure 0. Moreover, S splits the space R d into two sets C + and C \u2212 with positive probability mass.\nWhile the setting introduced above is very general, we make some substantial simplifications in this paper. First, we consider all graphs as unweighted graphs (the proofs are already technical enough in this setting). We have not yet had time to prove the corresponding theorems for weighted graphs, but would expect that this might lead yet to other limit expressions. This will be a point for future work. Moreover, in the case of the kNN-graph we consider the directed graph for simplicity. Some statements can be carried over by simple arguments from the directed graph to the symmetric graph, but not all of them. In general, we study the setting where one wants to find two clusters which are induced by some hypersurface in R d . In this paper we only consider the case where S is a hyperplane. Our results can be generalized to more general (smooth) surfaces, provided one makes a few assumptions on the regularity of the surface S. The proofs are more technical, though.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limits of quality measures", "text": "In this section we study the asymptotic behavior of the quantities introduced above for both the unweighted directed kNN graph and the unweighted r-graph. Due to the lack of space we only provide proof sketches; detailed proofs can be found in the supplement Maier et al. (2008).\nLet (k n ) n\u2208N be an increasing sequence. Given a finite sample x 1 , ..., x n from the underlying distribution, we will construct the graph G n,kn and study the convergence of Ncut n,kn (S), that is the Ncut value induced by S, evaluated on the graph G n,kn . Similarly, given a sequence (r n ) n\u2208N of radii, we consider the convergence of Ncut n,rn induced by S on the graph G n,rn . In the following \n(k n ) n\u2208N \u2282 N and (r n ) n\u2208N \u2282 R. For the kNN graph, assume that k n /n \u2192 0. In case d = 1, assume that k n / \u221a n \u2192 \u221e, in case d \u2265 2 assume k n / log n \u2192 \u221e. Then we have for n \u2192 \u221e d r n kn Ncut n,kn (S) a.s. \u2212\u2192 2\u03b7 d\u22121 (d + 1)\u03b7 1+1/d d Z S p 1\u22121/d (s)ds \" \" Z C + p(x)dx \" \u22121 + \" Z C \u2212 p(x)dx \" \u22121 \u00ab .\nFor the r-neighborhood graph, assume r n > 0, r n \u2192 0 and nr\nd+1 n \u2192 \u221e for n \u2192 \u221e. Then 1 rn Ncutn,r n (S) a.s. \u2212\u2192 2\u03b7 d\u22121 (d + 1)\u03b7 d Z S p 2 (s)ds \" \" Z C + p 2 (x)dx \" \u22121 + \" Z C \u2212 p 2 (x)dx \" \u22121 \u00ab .\nProof (Sketch for the case of the kNN graph, the case of the r graph is similar. Details see Maier et al., 2008.). Define the scaling constants\nc cut (n, k n ) = n \u22121+1/d k \u22121\u22121/d and c vol (n, k n ) = (nk n ) \u22121 .\nThen, (n/k n ) 1/d Ncut(S) can be decomposed in cut and volume term:\nc cut (n, k n ) cut n,kn (S) \u2022 (c vol (n, k n ) vol n,kn (C + )) \u22121 + (c vol (n, k n ) vol n,kn (C \u2212 )) \u22121 .\nIn Proposition 3 below we will see that the volume term satisfies\nc vol (n, k n ) vol n,kn (C + ) a.s. \u2212\u2192 C + p(x)dx,\nand the corresponding expression holds for C \u2212 . For the cut term we will prove below that\nc cut (n, k n ) cut n,kn (S) a.s. \u2212\u2192 2\u03b7 d\u22121 (d + 1)\u03b7 1+1/d d S p 1\u22121/d (s)ds.(1)\nThis will be done using a standard decomposition into variance and bias term, which will be treated in Propositions 1 and 2, respectively.\nProposition 1 (Limit values of E cut n,kn and E cut n,rn ) Let the general assumptions hold, and S be an arbitrary, but fixed hyperplane. For the kNN graph, if k n /n \u2192 0 and\nk n / log n \u2192 \u221e for n \u2192 \u221e, then E 1 nk n d n k n cut n,kn (S) \u2192 2\u03b7 d\u22121 d + 1 \u03b7 \u22121\u22121/d d S p 1\u22121/d (s)ds. For the r-neighborhood graph, if r n \u2192 0, r n > 0 for n \u2192 \u221e, then E cut n,rn (S) n 2 r d+1 n \u2192 2\u03b7 d\u22121 d + 1 S p 2 (s)ds.\nProof (Sketch, see Maier et al., 2008) . We start with the case of the r-neighborhood graph. By N i (i = 1, ..., n) denote the number of edges in the graph that start in point x i and end in some point on the other side of the cut surface S. As all points are sampled i.i.d, we have\nE cut n,rn (S) = n i=1 EN i = nEN 1 .\nSuppose the position of the first point is x. The idea to compute the expected number of edges originating in x is as follows. We consider a ball B(x, r n ) of radius r n around x (where r n is the current parameter of the r-neighborhood graph). The expected number of edges originating in x equals the expected number of points which lie in the intersection of this ball with the other side of the hyperplane. That is, setting\ng(x, r n ) = \u00b5 B(x, r n ) \u2229 C + if x \u2208 C \u2212 \u00b5 B(x, r n ) \u2229 C \u2212 if x \u2208 C + we have E(N 1 |X 1 = x) = (n \u2212 1)g(x, r n )\n, since the number of points in the intersection of B(x, r n ) with the other side of the hyperplane is binomially distributed with parameters n \u2212 1 and g(x, r n ). Integrating this conditional expectation over all positions of the point\nx in R d gives E cut n,rn (S) = n(n \u2212 1) R d g(x, r n )p(x)dx.\nThe second important idea is that instead of integrating over R d , we first integrate over the hyperplane S and then, at each point s \u2208 S, along the normal line through s, that is the line s + t n, t \u2208 R, where n denotes the normal vector of the hyperplane pointing towards C + . This leads to\nn(n \u2212 1) R d g(x, r n )p(x)dx = n(n \u2212 1) S \u221e \u2212\u221e g(s + t n, r n )p(s + t n) dt ds.\nThis has two advantages. First, if x is far enough from S (that is, dist(x, s) > r n for all s \u2208 S), then g(x, r n ) = 0 and the corresponding terms in the integral vanish. Second, if x is close to s \u2208 S and the radius r n is small, then the density on the ball B(x, r n ) can be considered approximately homogeneous, that is p(y) \u2248 p(s) for all y \u2208 B(x, r n ). Thus,\n\u221e \u2212\u221e g(s + t n, r n )p(s + t n) dt = rn \u2212rn g(s + t n, r n )p(s + t n) dt \u2248 2 rn 0 p(s) vol B(s + t n, r n ) \u2229 C \u2212 p(s) dt.\nIt is not hard to see that vol B(s + t n, r n ) \u2229 C \u2212 = r d n A(t/r n ), where A(t/r n ) denotes the volume of the cap of the unit ball capped at distance t/r n . Solving the integrals leads to\nrn 0 vol B(s + t n, r n ) \u2229 C \u2212 dt = r n 1 0 A(t)dt = \u03b7 d\u22121 d + 1 .\nCombining the steps above we obtain the result for the r-neighborhood graph.\nIn the case of the kNN graph, the proof follows a similar principle. We have to replace the radius r n by the k-nearest neighbor radius, that is, the distance of a data point to its kth nearest neighbor. This leads to additional difficulties, as this radius is a random variable as well. By a technical lemma one can show that for large n, under the condition k n / log n \u2192 \u221e we can replace the integration over the possible values of the kNN radius by its expectation. Then we observe that as k n /n \u2192 0, the expected kNN radius converges to 0, that is for large n we only have to integrate over balls of homogeneous density. In a region of homogeneous densityp, the expected kNN radius is given as (k/((n\u22121)\u03b7 dp )) 1/d . Now similar arguments as above lead to the desired result.\nProposition 1 already shows one of the most important differences between the limits of the expected cut for the different graphs: For the r-graph we integrate over p 2 , while we integrate over p 1\u22121/d for the kNN graph. This difference comes from the fact that the kNN-radius is a random quantity, which is not the case for the deterministically chosen radius r n in the r-graph.\nProposition 2 (Deviation of cut n,kn and cut n,rn from their means) Let the general assumptions hold. For the kNN graph, if the dimension\nd = 1 then assume k n / \u221a n \u2192 \u221e, for d \u2265 2 assume k n / log n \u2192 \u221e. In both cases let k n /n \u2192 0. Then 1 nk n d n k n cut n,kn (S) \u2212 E 1 nk n d n k n cut n,kn (S) a.s. \u2212\u2192 0. For the r-neighborhood graph, let r n > 0, r n \u2192 0 such that nr d+1 n \u2192 \u221e for n \u2192 \u221e. Then 1 n 2 r d+1 n cut n,rn (S) \u2212 E 1 n 2 r d+1 n cut n,rn (S) a.s. \u2212\u2192 0.\nProof (Sketch, details see Maier et al., 2008). Using McDiarmid's inequality (with a kissing number argument to obtain the bounded differences condition) or a U-statistics argument leads to exponential decay rates for the deviation probabilities (and thus to convergence in probability). The almost sure convergence can then be obtained using the Borel-Cantelli lemma.\nProposition 3 (Limits of vol n,kn and vol n,rn ) Let the general assumptions hold, and H \u2286 R d an arbitrary measurable subset. Then, as n \u2192 \u221e, for the kNN graph we have 1 nk n vol n,kn (H) a.s.", "publication_ref": ["b1", "b1", "b1", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "\u2212\u2192 \u00b5(H).", "text": "For the r-neighborhood graph, if nr d \u2192 \u221e we have\n1 n 2 r d n vol n,rn (H) a.s. \u2212\u2192 \u03b7 d H p 2 (x)dx.\nProof. In the graph G n,kn there are exactly k outgoing edges from each node. Thus the expected number of edges originating in H depends on the number of sample points in H only, which is binomially distributed with parameters n and \u00b5(H). For the graph G n,rn we decompose the volume into the contributions of all the points, and for a single point we condition on its location. The number of outgoing edges, provided the point is at position x, is the number of other points in B(x, r n ), which is binomially distributed with parameters (n \u2212 1) and \u00b5(B(x, r n )). If r n is sufficiently small we can approximate \u00b5(B(x, r n )) by \u03b7 d r d n p(x) under our conditions on the density. Almost sure convergence is proved using McDiarmid's inequality or a U-statistics argument.\nOther convergence results. In the literature, we only know of one other limit result for graph cuts, proved by Narayanan et al. (2007). Here the authors study the case of a fully connected graph with Gaussian weights w t (x i , x j ) = 1/(4\u03c0t) d/2 exp(\u2212dist(x i \u2212 x j ) 2 /4t). Denoting the corresponding cut value by cut n,t , the authors show that if t n \u2192 0 such that t n > 1/n 1/(2d+2) , then\n\u221a \u03c0 n \u221a t n cut n,tn \u2192 S p(s) ds a.s.\nComparing this result to ours, we can see that it corroborates our finding: yet another graph leads to yet another limit result (for cut, as the authors did not study the Ncut criterion).", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}, {"heading": "Examples where different limits of Ncut lead to different optimal cuts", "text": "In Theorem 1 we have proved that the kNN graph leads to a different limit functional for Ncut(S) than the r-neighborhood graph. Now we want to show that this difference is not only a mathematical subtlety without practical relevance. We will see that if we select an optimal cut based on the limit criterion for the kNN graph we can obtain a different result than if we use the limit criterion based on the r-neighborhood graph. Moreover, this finding does not only apply to the limit cuts, but also to cuts constructed on finite samples. This shows that on finite data sets, different constructions of the graph can lead to systematic differences in the clustering results.\nConsider Gaussian mixture distributions in one and two dimensions of the form\n3\ni=1 \u03b1 i N ([\u00b5 i , 0, . . . , 0], \u03c3 i I)\nwhich are set to 0 where they are below a threshold \u03b8 (and properly rescaled), with specific parameters For density plots, see Figure 1. We first investigate the theoretic limit Ncut values, for hyperplanes which cut perpendicular to the first dimension (which is the \"informative\" dimension of the data).\ndim \u00b5 1 \u00b5 2 \u00b5 3 \u03c3 1 \u03c3 1 \u03c3 1 \u03b1 1 \u03b1 2 \u03b1\nFor the chosen densities, the limit Ncut expressions from Theorem 1 can be computed analytically. The plots in Figure 2 show the theoretic limits. In particular, the minimal Ncut value in the kNN case is obtained at a different position than the minimal value in the r-neighborhood case.\nThis effect can also be observed in a finite sample setting. We sampled n = 2000 points from the given distributions and constructed the (unweighted) kNN graph (we tried a range of parameters of k and r, our results are stable with respect to this choice). Then we evaluated the empirical Ncut values for all hyperplanes which cut perpendicular to the informative dimension, similar as in the last paragraph. This experiment was repeated 100 times. Figure 2 shows the means of the Ncut values of these hyperplanes, evaluated on the sample graphs. We can see that the empirical plots are very similar to the limit plots produced above. In particular, we can reproduce the effect that the kNN and r-graphs lead to different positions of the optimal hyperplane.\nMoreover, we applied normalized spectral clustering (cf. von Luxburg, 2007) to the mixture data sets. Instead of the directed kNN graph we used the undirected one, as standard spectral clustering is not defined for directed graphs. We compare different clusterings by the minimal matching distance:\nd M M (Clust 1 , Clust 2 ) = min \u03c0 n i=1 1 Clust1(xi) =\u03c0(Clust2(xi)) /(2n)\nwhere the minimum is taken over all permutations \u03c0 of the labels. In the case of two clusters, this distance corresponds to the 0-1-loss as used in classification: a minimal matching distance of 0.38, say, means that 38% of the data points lie in different clusters. In our spectral clustering experiment, we could observe that the clusterings obtained by spectral clustering are usually very close to the theoretically optimal hyperplane splits predicted by theory (the minimal matching distances to the optimal hyperplane splits were always in the order of 0.03 or smaller). As predicted by theory, both kinds of graph give different cuts in the data. An illustration of this phenomenon for the case of dimension 2 can be found in Figure 3 We can see that for the same graph, the clustering results are very stable (differences in the order of 10 \u22123 ) whereas the differences between the kNN graph and the r-neighborhood graph are substantial (0.32 and 0.48, respectively). This difference is exactly the one induced by assigning the middle mode of the density to different clusters, which is the effect predicted by theory.\nIt is tempting to conjecture that these effects might be due to the fact that the number of Gaussians and the number of clusters we are looking for do not conincide. But this is not the case: for a sum of In the two-dimensional case, we plot the informative dimension (marginal over the other dimensions) only. The dashed blue vertical line depicts the optimal limit cut of the r-graph, the solid red vertical line the optimal limit cut of the kNN graph. The optimal cut is indicated by the dotted line. The top row shows the results for the kNN graph, the bottom row for the r-graph. In the left column the result for one dimension, in the right column for two dimensions.\ntwo Gaussians in one dimension with means 0.2 and 0.4, variances 0.05 and 0.03, weights 0.8 and 0.2, and a threshold of 0.1 the same effects can be observed.\nFinally, we conducted an experiment similar to the last one on two real data sets (breast cancer and heart from the Data Repository by G. R\u00e4tsch). Here we chose the parameters k = 20 and r = 3.2 for breast cancer and r = 4.3 for heart (among the parameters we tried, these were the parameters where the results were most stable, that is where d kNN and d r were minimal). Then we ran spectral clustering on different subsamples of the data sets (n = 200 for breast cancer, n = 170 for heart). To evaluate whether our clusterings were any useful at all, we computed the minimal matching distance between the clusterings and the true class labels and obtained distances of 0.27 for the r-graph and 0.44 for the kNN graph on breast cancer and 0.17 and 0.19 for heart. These results are reasonable (standard classifiers lead to classification errors of 0.27 and 0.17 on these data sets). Moreover, to exclude other artifacts such as different cluster sizes obtained with the kNN or r-graph, we also computed the expected random distances between clusterings, based on the actual cluster sizes we obtained in the experiments. We obtained the following table: We can see that in the example of breast cancer, the distances d kNN and d r are much smaller than the distance d kNN \u2212r . This shows that the clustering results differ considerably between the two kinds of graph (and compared to the expected random effects, this difference does not look random at all).\nFor heart, on the other side, we do not observe significant differences between the two graphs. This experiment shows that for some data sets a systematic difference between the clusterings based on different graph types exists. But of course, such differences can occur for many reasons. The different limit results might just be one potential reason, and other reasons might exist. But whatever the reason is, it is interesting to observe these systematic differences between graph types in real data.", "publication_ref": ["b4"], "figure_ref": ["fig_3", "fig_4", "fig_4", "fig_6"], "table_ref": []}, {"heading": "Discussion", "text": "In this paper we have investigated the influence of the graph construction on graph-based clustering measures such as the normalized cut. We have seen that depending on the type of graph, the Ncut criterion converges to different limit results. In our paper, we computed the exact limit expressions for the r-neighborhood graph and the kNN graph. Moerover, yet a different limit result for a complete graph using Gaussian weights exists in the literature (Narayanan et al., 2007). The fact that all these different graphs lead to different clustering criteria shows that these criteria cannot be studied isolated from the graph they will be applied to.\nFrom a theoretical side, there are several directions in which our work can be improved. Some technical improvements concern using the symmetric instead of the directed kNN graph, and adding weights to the edges. In the supplement (Maier et al., 2008) we also prove rates of convergence for our results. It would be interesting to use these to determine an optimal choice of the connectivity parameter k or r of the graphs (we have already proved such results in a completely different graph clustering setting, cf. Maier et al., 2007). Another extension which does not look too difficult is obtaining uniform convergence results. Here one just has to take care that one uses a suitably restricted class of candidate surfaces S (note that uniform convergence results over the set of all partitions of R d are impossible, cf. von .\nFor practice, it will be important to study how the different limit results influence clustering results. So far, we do not have much intuition about when the different limit expressions lead to different optimal solutions, and when these solutions will show up in practice. The examples we provided above already show that different graphs indeed can lead to systematically different clusterings in practice. Gaining more understanding of this effect will be an important direction of research if one wants to understand the nature of different graph clustering criteria.", "publication_ref": ["b2", "b1", "b0"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Cluster identification in nearest-neighbor graphs", "journal": "Springer", "year": "2007", "authors": "M Maier; M Hein; U Von Luxburg"}, {"ref_id": "b1", "title": "Influence of graph construction on graph-based quality measures -technical supplement", "journal": "", "year": "2008", "authors": "Markus Maier; Ulrike Von Luxburg; Matthias Hein"}, {"ref_id": "b2", "title": "On the relation between low density separation, spectral clustering and graph cuts", "journal": "", "year": "2007", "authors": "Hariharan Narayanan; Mikhail Belkin; Partha Niyogi"}, {"ref_id": "b3", "title": "Normalized cuts and image segmentation", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2000", "authors": "J Shi; J Malik"}, {"ref_id": "b4", "title": "A tutorial on spectral clustering", "journal": "Statistics and Computing", "year": "2007", "authors": "U Luxburg"}, {"ref_id": "b5", "title": "Consistent minimization of clustering objective functions", "journal": "", "year": "2008", "authors": "U Luxburg; S Bubeck; S Jegelka; M Kaufmann"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Sds denotes the (d \u2212 1)-dimensional surface integral along S. Here is our main result: Theorem 1 (Limit values of Ncut on different graphs) Assume the general assumptions hold for the density p on R d and a fixed hyperplane S in R d . Consider the sequences", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": ". To give a quantitative evaluation of this phenomenon, we computed the mean minimal matching distances between clusterings obtained by the same type of graph over the different samples (denoted d kNN and d r ), and the mean difference d kNN \u2212r between the clusterings obtained by different graph types:", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 1 :1Figure 1: Densities in the examples.In the two-dimensional case, we plot the informative dimension (marginal over the other dimensions) only. The dashed blue vertical line depicts the optimal limit cut of the r-graph, the solid red vertical line the optimal limit cut of the kNN graph.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 2 :2Figure2: Ncut values for hyperplanes: theoretical predictions (dashed) and empirical means (solid). The optimal cut is indicated by the dotted line. The top row shows the results for the kNN graph, the bottom row for the r-graph. In the left column the result for one dimension, in the right column for two dimensions.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "d kNN \u2212r rand. d kNN \u2212r breast canc. 0.13 \u00b1 0.15 0.48 \u00b1 0.01 0.40 \u00b1 0.10 0.22 \u00b1 0.01 0.40 \u00b1 0.10 0.44 \u00b1 0.01 heart 0.06 \u00b1 0.02 0.47 \u00b1 0.02 0.06 \u00b1 0.02 0.44 \u00b1 0.02 0.07 \u00b1 0.03 0.47 \u00b1 0.02", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 3 :3Figure 3: Results of spectral clustering in two dimensions, for r-graph (left) and kNN graph (right)", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "Ncut(C, V \\ C) = cut(C, V \\ C) 1 vol(C) + 1 vol(V \\ C) .", "formula_coordinates": [2.0, 182.84, 536.03, 246.33, 22.31]}, {"formula_id": "formula_1", "formula_text": "(k n ) n\u2208N \u2282 N and (r n ) n\u2208N \u2282 R. For the kNN graph, assume that k n /n \u2192 0. In case d = 1, assume that k n / \u221a n \u2192 \u221e, in case d \u2265 2 assume k n / log n \u2192 \u221e. Then we have for n \u2192 \u221e d r n kn Ncut n,kn (S) a.s. \u2212\u2192 2\u03b7 d\u22121 (d + 1)\u03b7 1+1/d d Z S p 1\u22121/d (s)ds \" \" Z C + p(x)dx \" \u22121 + \" Z C \u2212 p(x)dx \" \u22121 \u00ab .", "formula_coordinates": [3.0, 108.0, 468.75, 396.0, 61.98]}, {"formula_id": "formula_2", "formula_text": "d+1 n \u2192 \u221e for n \u2192 \u221e. Then 1 rn Ncutn,r n (S) a.s. \u2212\u2192 2\u03b7 d\u22121 (d + 1)\u03b7 d Z S p 2 (s)ds \" \" Z C + p 2 (x)dx \" \u22121 + \" Z C \u2212 p 2 (x)dx \" \u22121 \u00ab .", "formula_coordinates": [3.0, 132.59, 535.73, 348.98, 39.31]}, {"formula_id": "formula_3", "formula_text": "c cut (n, k n ) = n \u22121+1/d k \u22121\u22121/d and c vol (n, k n ) = (nk n ) \u22121 .", "formula_coordinates": [3.0, 108.0, 595.74, 396.0, 23.56]}, {"formula_id": "formula_4", "formula_text": "c cut (n, k n ) cut n,kn (S) \u2022 (c vol (n, k n ) vol n,kn (C + )) \u22121 + (c vol (n, k n ) vol n,kn (C \u2212 )) \u22121 .", "formula_coordinates": [3.0, 127.48, 627.6, 363.0, 11.72]}, {"formula_id": "formula_5", "formula_text": "c vol (n, k n ) vol n,kn (C + ) a.s. \u2212\u2192 C + p(x)dx,", "formula_coordinates": [3.0, 215.06, 667.22, 181.89, 20.84]}, {"formula_id": "formula_6", "formula_text": "c cut (n, k n ) cut n,kn (S) a.s. \u2212\u2192 2\u03b7 d\u22121 (d + 1)\u03b7 1+1/d d S p 1\u22121/d (s)ds.(1)", "formula_coordinates": [3.0, 179.69, 708.98, 324.31, 27.21]}, {"formula_id": "formula_7", "formula_text": "k n / log n \u2192 \u221e for n \u2192 \u221e, then E 1 nk n d n k n cut n,kn (S) \u2192 2\u03b7 d\u22121 d + 1 \u03b7 \u22121\u22121/d d S p 1\u22121/d (s)ds. For the r-neighborhood graph, if r n \u2192 0, r n > 0 for n \u2192 \u221e, then E cut n,rn (S) n 2 r d+1 n \u2192 2\u03b7 d\u22121 d + 1 S p 2 (s)ds.", "formula_coordinates": [4.0, 108.0, 144.37, 396.0, 100.27]}, {"formula_id": "formula_8", "formula_text": "E cut n,rn (S) = n i=1 EN i = nEN 1 .", "formula_coordinates": [4.0, 220.44, 293.83, 171.12, 14.11]}, {"formula_id": "formula_9", "formula_text": "g(x, r n ) = \u00b5 B(x, r n ) \u2229 C + if x \u2208 C \u2212 \u00b5 B(x, r n ) \u2229 C \u2212 if x \u2208 C + we have E(N 1 |X 1 = x) = (n \u2212 1)g(x, r n )", "formula_coordinates": [4.0, 108.0, 375.72, 287.52, 43.47]}, {"formula_id": "formula_10", "formula_text": "x in R d gives E cut n,rn (S) = n(n \u2212 1) R d g(x, r n )p(x)dx.", "formula_coordinates": [4.0, 202.39, 429.88, 275.26, 42.92]}, {"formula_id": "formula_11", "formula_text": "n(n \u2212 1) R d g(x, r n )p(x)dx = n(n \u2212 1) S \u221e \u2212\u221e g(s + t n, r n )p(s + t n) dt ds.", "formula_coordinates": [4.0, 138.19, 517.18, 335.63, 26.68]}, {"formula_id": "formula_12", "formula_text": "\u221e \u2212\u221e g(s + t n, r n )p(s + t n) dt = rn \u2212rn g(s + t n, r n )p(s + t n) dt \u2248 2 rn 0 p(s) vol B(s + t n, r n ) \u2229 C \u2212 p(s) dt.", "formula_coordinates": [4.0, 149.75, 599.37, 318.03, 54.45]}, {"formula_id": "formula_13", "formula_text": "rn 0 vol B(s + t n, r n ) \u2229 C \u2212 dt = r n 1 0 A(t)dt = \u03b7 d\u22121 d + 1 .", "formula_coordinates": [4.0, 183.37, 690.46, 250.79, 26.29]}, {"formula_id": "formula_14", "formula_text": "d = 1 then assume k n / \u221a n \u2192 \u221e, for d \u2265 2 assume k n / log n \u2192 \u221e. In both cases let k n /n \u2192 0. Then 1 nk n d n k n cut n,kn (S) \u2212 E 1 nk n d n k n cut n,kn (S) a.s. \u2212\u2192 0. For the r-neighborhood graph, let r n > 0, r n \u2192 0 such that nr d+1 n \u2192 \u221e for n \u2192 \u221e. Then 1 n 2 r d+1 n cut n,rn (S) \u2212 E 1 n 2 r d+1 n cut n,rn (S) a.s. \u2212\u2192 0.", "formula_coordinates": [5.0, 108.0, 253.88, 396.0, 97.79]}, {"formula_id": "formula_15", "formula_text": "1 n 2 r d n vol n,rn (H) a.s. \u2212\u2192 \u03b7 d H p 2 (x)dx.", "formula_coordinates": [5.0, 224.23, 492.17, 164.75, 24.19]}, {"formula_id": "formula_16", "formula_text": "\u221a \u03c0 n \u221a t n cut n,tn \u2192 S p(s) ds a.s.", "formula_coordinates": [5.0, 231.93, 678.31, 149.34, 31.15]}, {"formula_id": "formula_17", "formula_text": "3", "formula_coordinates": [6.0, 118.52, 198.96, 3.97, 6.12]}, {"formula_id": "formula_18", "formula_text": "i=1 \u03b1 i N ([\u00b5 i , 0, . . . , 0], \u03c3 i I)", "formula_coordinates": [6.0, 118.52, 201.93, 111.58, 11.15]}, {"formula_id": "formula_19", "formula_text": "dim \u00b5 1 \u00b5 2 \u00b5 3 \u03c3 1 \u03c3 1 \u03c3 1 \u03b1 1 \u03b1 2 \u03b1", "formula_coordinates": [6.0, 158.11, 233.56, 253.34, 9.65]}, {"formula_id": "formula_20", "formula_text": "d M M (Clust 1 , Clust 2 ) = min \u03c0 n i=1 1 Clust1(xi) =\u03c0(Clust2(xi)) /(2n)", "formula_coordinates": [6.0, 173.15, 466.29, 265.7, 17.1]}], "doi": ""}