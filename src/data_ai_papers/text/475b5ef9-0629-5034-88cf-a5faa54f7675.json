{"title": "NLPositionality: Characterizing Design Biases of Datasets and Models", "authors": "Sebastin Santy; Jenny T Liang; Ronan Le Bras; Katharina Reinecke; Maarten Sap; Carl Jones; Aditya Sharma", "pub_date": "", "abstract": "Design biases in NLP systems, such as performance differences for different populations, often stem from their creator's positionality, i.e., views and lived experiences shaped by identity and background. Despite the prevalence and risks of design biases, they are hard to quantify because researcher, system, and dataset positionality is often unobserved. We introduce NLPositionality, a framework for characterizing design biases and quantifying the positionality of NLP datasets and models. Our framework continuously collects annotations from a diverse pool of volunteer participants on LabintheWild, and statistically quantifies alignment with dataset labels and model predictions. We apply NLPositionality to existing datasets and models for two tasks-social acceptability and hate speech detection. To date, we have collected 16, 299 annotations in over a year for 600 instances from 1, 096 annotators across 87 countries. We find that datasets and models align predominantly with Western, White, college-educated, and younger populations. Additionally, certain groups, such as nonbinary people and non-native English speakers, are further marginalized by datasets and models as they rank least in alignment across all tasks. Finally, we draw from prior literature to discuss how researchers can examine their own positionality and that of their datasets and models, opening the door for more inclusive NLP systems.", "sections": [{"heading": "Introduction", "text": "\"Treating different things the same can generate as much inequality as treating the same things differently.\" -Kimberl\u00e9 Crenshaw When creating NLP datasets and models, researchers' design choices are partly influenced * Equal contribution; work done while at the Allen Institute for AI \u2022 Quality annotations only from master turkers, better if educated.\n\u2022 Maybe only English phrases?\n\u2022 Are micro-aggressions toxic? And misogynistic comments?\n\u2022 Should I include gender stereotypes?\nFigure 1: Example Scenario. Carl from the U.S. and Aditya from India both want to use Perspective API, but it works better for Carl than it does for Aditya. This is because toxicity researchers' positionalities lead them to make design choices that make toxicity datasets, and thus Perspective API, to have positionalities that are Western-centric.\nby their positionality, i.e., their views shaped by their lived experience, identity, culture, and background (Savin-Baden and Howell-Major, 2013). While researcher positionality is commonly discussed outside of NLP, it is highly applicable to NLP research but remains largely overlooked. For example, a U.S.-born English-speaking researcher building a toxicity detection system will likely start with U.S.-centric English statements to annotate for toxicity. This can cause the tool to work poorly for other populations (e.g., not detect offensive terms like \"presstitute\" in Indian contexts; see Figure 1). Such design biases in the creation of datasets and models, i.e., disparities in how well datasets and models work for different populations, stem from factors including latent design choices and the researcher's positionality. However, they can perpetuate systemic inequalities by imposing one group's standards onto the rest of the world (Ghosh et al., 2021;Gururangan et al., 2022;Blasi et al., 2022). The challenge is that design biases arise from the myriad of design choices made; in the context of creating datasets and models, only some of these choices may be documented (e.g., through model cards and data sheets; Bender and Friedman, 2018;Mitchell et al., 2019;. Further, many popular deployed models are hidden behind APIs, and thus design biases can only be characterized indirectly (e.g., by observing model behavior).\nWe introduce NLPositionality, a framework for characterizing design biases and positionality of NLP datasets and models. For a given dataset and task, we obtain a wide set of new annotations for a data sample, from a diverse pool of volunteers from various countries and of different backgrounds (recruited through LabintheWild; Reinecke and Gajos, 2015). We then quantify design biases by comparing which identities and backgrounds have higher agreement with the original dataset labels or model predictions. NLPositionality offers three advantages over other approaches (e.g., paid crowdsourcing or laboratory studies). First, the demographic diversity of participants on LabintheWild is better than on other crowdsourcing platforms (Reinecke and Gajos, 2015) and in traditional laboratory studies. Second, the compensation and incentives in our approach rely on a participant's motivation to learn about themselves instead of monetary compensation. This has been shown to result in higher data quality compared to using paid crowdsourcing platforms (August and Reinecke, 2019), as well as in opportunities for participant learning (Oliveira et al., 2017). This allows our framework to continuously collect new annotations and reflect more up-to-date measurements of design biases for free over long periods of time, compared to one-time paid studies such as in previous works Davani et al., 2022). 1 Finally, our approach is dataset-and model-agnostic and can be applied post-hoc to any dataset or model using only instances and their labels or predictions.\nWe apply NLPositionality to two case studies of NLP tasks-social acceptability and hate speech detection-which are known to exhibit design biases (Talat et al., 2022;Ghosh et al., 2021). We examine datasets and supervised models related to these tasks as well as generalpurpose large language models (i.e., . As of May 25 2023, a total of 16, 299 annotations were collected from 1, 096 annotators from 87 countries, with an average of 38 annotations per day. We discover that the datasets and models we investigate are most aligned with White and educated young people from English-speaking countries, which are a subset of \"WEIRD\" (Western, Educated, Industrialized, Rich, Democratic;Henrich et al., 2010) populations. We also see that datasets exhibit close alignment with their original annotators, emphasizing the importance of gathering data and annotations from diverse groups.\nOur paper highlights the importance of considering design biases in NLP. Our findings showcase the usefulness of our framework in quantifying dataset and model positionality. In a discussion of the implications of our results, we consider how positionality may manifest in other NLP tasks.", "publication_ref": ["b75", "b35", "b38", "b13", "b10", "b60", "b69", "b69", "b5", "b62", "b20", "b79", "b35", "b44"], "figure_ref": [], "table_ref": []}, {"heading": "Dataset & Model Positionality: Definitions and Background", "text": "A person's positionality is the perspectives they hold as a result of their demographics, identity, and life experiences (Holmes, 2020;Savin-Baden and Howell-Major, 2013). For researchers, positionality \"reflects the position that [they have] chosen to adopt within a given research study\" (Savin-Baden and Howell-Major, 2013). It influences the research process and its outcomes and results (Rowe, 2014). Some aspects of positionality, such as gender, race, skin color, and nationality, are culturally ascribed and part of one's identity. Others, such as political views and life history, are more subjective (Holmes, 2020;Foote and Gau Bartell, 2011).  similar way as models. This results in perspectives embedded within language technologies, making them less inclusive towards certain populations.\nB la c k W h it e A s ia n M a n W o m a n N o n -b in a r y P h D P re -H ig h U K In d ia A u s tr a li a U S A P o la n d G e\nDesign Biases In NLP, design biases occur when a researcher or practitioner makes design choicesoften based on their positionality-that cause models and datasets to systematically work better for some populations over others. Curating datasets involves design choices such as what source to use, what language to use, what perspectives to include or exclude, or who to get annotations from. For example, a researcher's native language may influence them to create datasets in that language due to their familiarity with the domain (as in the example in Figure 1). When training models, these choices include the type of training data, data pre-processing techniques, or the objective function (Hall et al., 2022). For example, a researcher's institutional affiliation may influence the training datasets they select (e.g., choosing a dataset made by a coworker). Since the latent choices that result in design biases are fundamental to research itself, some researchers have argued that it is impossible to completely de-bias datasets and models . Current discussions around bias in NLP often focus on ones that originate from social biases embedded within the data. In comparison, design biases originate from the developer who makes assumptions. Based on Friedman and Nissenbaum (1996)'s framework on bias, social biases are preexisting biases in society, whereas design biases are emergent biases that originate from the computing system itself. 'Gender bias' in computing systems means that the system does not perform well for some genders; \"man is to doctor as woman is to nurse\" (Bolukbasi et al., 2016) is a social bias, while captioning systems that fail to understand women's voices (Tatman, 2017) is a design bias.\nOne prominent example of design bias in NLP is the overt emphasis on English (Joshi et al., 2020;Blasi et al., 2022). Others include the use of block lists in dataset creation or toxicity classifiers as a filter, which can marginalize minority voices (Dodge et al., 2021;Xu et al., 2021). In this work, we extend the discussion of design biases from prior work into NLP, discuss it in relation to researcher positionality, and show its effects on datasets and models.\n3 NLPositionality: Quantifying Dataset and Model Positionality\nOur NLPositionality framework follows a twostep process for characterizing the design biases and positionality of datasets and models. First, a subset of data for a task is re-annotated by annotators from around the world to obtain globally representative data in order to quantify positionality ( \u00a73.1). We specifically rely on re-annotation to capture self-reported demographic data of annotators with each label. Then, the positionality of the dataset or model is computed by comparing the responses of the dataset or model with different demographic groups for identical instances ( \u00a73.2). While relying on demographics as a proxy for positionality is limited (see discussion in \u00a77), we use demographic information for an initial exploration in uncovering design biases in datasets and models.", "publication_ref": ["b46", "b75", "b75", "b72", "b46", "b29", "b40", "b32", "b15", "b80", "b53", "b13", "b26", "b87"], "figure_ref": [], "table_ref": []}, {"heading": "Collecting Diverse Annotations", "text": "Cost-effectively collecting annotations from a diverse crowd at scale is challenging. Popular crowdsourcing platforms like Amazon Mechanical Turk (MTurk) are not culturally diverse, as a majority of workers are from the United States and India (Difallah et al., 2018;Ipeirotis, 2010). Further, MTurk does not easily support continuous and longitudinal data collection. To address these challenges, we use LabintheWild (Reinecke and Gajos, 2015), which hosts web-based online experiments. Compared to traditional laboratory settings, it has more diverse participants and collects equally high-quality data for free (August and Reinecke, 2019;Oliveira et al., We compare the labels collected from LabintheWild with the dataset's original labels and models' predictions. Analysis (step 6): We compute the Pearson's r correlation between the LabintheWild annotations by demographic for the dataset's original labels and the models' predictions. We apply the Bonferroni correction to account for multiple hypothesis testing. 2017); instead of monetary compensation, participants typically partake in LabintheWild experiments because they learn something about themselves. Thus, we motivate people to participate in our IRB-approved study ( \u00a78) by enabling them to learn how their responses on a given task (e.g., judging hate speech) compare to a judgment by AI systems as well as by others who are demographically similar to them (see Appendix B.1).\nFor a given task, we choose a dataset to be annotated. To select instances for re-annotation, we filter the dataset based on relevant information that could indicate subjectivity (such as controversiality label for the social acceptability dataset), and then sample 300 diverse instances by stratified sampling across different dataset metadata, (such as the targeted groups of toxic speech label for the hate speech dataset) (see Appendix A.1). These instances are then hosted as an experiment on LabintheWild to be annotated by a diverse crowd, where participants report their demographics. To ensure consistency in the re-annotated data, the instructions and annotation setups are similar to the original tasks'. Figure 2 is an example from the Social Chemistry dataset and its annotations.", "publication_ref": ["b25", "b48", "b69", "b5", "b62"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Quantifying Positionality", "text": "We use correlation as a quantitative construct for positionality. First, we group the annotations by specific demographics. When datasets contain multiple annotations from the same demographic for the same instance, we take the mean of the labels from annotators of that demographic to obtain an aggregated score (see Table 1). Next, for each demographic, we compute Pearson's r using the demographic's aggregated score for each instance and correlated it to the dataset label or model prediction 2 . We then apply the Bonferroni correction to account for multiple hypotheses testing (Wickens and Keppel, 2004). We rank the correlations to reveal which demographic groups best align with the positionality of datasets and models. Finally, we report the total number of annotators and interannotator agreements for each demographic using Krippendorff's \u03b1 (Krippendorff, 2006).  ", "publication_ref": ["b85", "b56"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Social Acceptability", "text": "Social acceptability is the task of evaluating how acceptable certain actions are in society. Related to making moral judgments, it requires models to have commonsense rules about social norms and acceptable behavior (Forbes et al., 2020). Social acceptability is likely to vary by background, as morality (Atari et al., 2022), values (Haerpfer andKizilova, 2012), and moral decision-making (Awad et al., 2018(Awad et al., , 2020 vary by culture. In NLP, social acceptability models' behavior have been shown to systematically display certain values (Arora et al., 2023) and biases (H\u00e4mmerl et al., 2022), some of which are Western-centric (Jiang et al., 2021;Talat et al., 2022).\nSetup 612 participants annotated the social acceptability of 300 examples from the Social Chemistry dataset (Forbes et al., 2020). We collected 11, 294 annotations for this task and compared participants' responses to the original Social Chemistry dataset, the Delphi model (Jiang et al., 2021), and GPT-4 zero-shot (OpenAI, 2023). In order to evaluate GPT-4, we combine Delphi's GPT-3 zeroshot evaluation setup from Jiang et al. (2021) with the template used by Rae et al. ( 2021):\nText: <Social Chemistry action> Question: How socially acceptable is the above situation: good, okay, or bad? Answer:\nResults We find that the instances we selected for the social acceptability task can vary by the annotator's demographics (see Table 1). For example, men and non-binary people are more likely than women to say it's okay to tell a woman what to do.\nWe also report the Pearson's r results in Table 2, with corresponding p-values after applying the Bonferroni correction in the Appendix C.1. We use * to represent statistically significant Pearson's r coefficients (p < 2.04e \u2212 05). Social Chemistry is most aligned with people who grow up (r = 0.76 * ) and live in (r = 0.76 * ) English-speaking countries, who have a college education (r = 0.74 * ), are White (r = 0.73 * ), and are 20-30 years old (r = 0.74 * ), indicating a preference to younger WEIRD populations.\nDelphi also exhibits a similar pattern, but to a lesser degree. While it strongly aligns with people who grow up (r = 0.61 * ) and live in (r = 0.65 * ) English-speaking countries, who have a college education (r = 0.66 * ), are White (r = 0.61 * ), and are 20-30 years old (r = 0.66 8 ); it also correlates more with other populations, such as people who grow up (r = 0.72 * ) in Baltic countries compared to English-speaking countries.\nWe also observe a similar pattern with GPT-4. It has the highest Pearson's r value for people who grow up (r = 0.74 * ) and live in (r = 0.73 * ) English-speaking countries, are college-educated (r = 0.69 * ), are White (r = 0.70 * ) and are between 20-30 years old (r = 0.70 * ). However, it  does have strong correlations with other populations, such as people who grow up in Confucian countries (r = 0.74 * ) and have a graduate degree (r = 0.69 * ). Native English speakers strongly correlate with both datasets and models. Non-binary people align less to both Social Chemistry, Delphi, and GPT-4 compared to men and women. People with professional school education and Black, Latinx, and Native American populations consistently rank least in correlation for education level and ethnicity.", "publication_ref": ["b39", "b6", "b7", "b2", "b41", "b79"], "figure_ref": [], "table_ref": ["tab_2", "tab_4"]}, {"heading": "Hate Speech Detection", "text": "Hate speech detection is the task of determining whether an instance is \"abusive speech targeting specific group characteristics, such as ethnic origin, religion, gender, or sexual orientation\" (Warner and Hirschberg, 2012). Rating whether text is hate speech is likely to vary by background. Also, annotators rate the toxicity of tweets with African American English (AAE) differently by background, such as political affiliation . Hate speech classifiers also produce errors for instances outside Western contexts (Ghosh et al., 2021 Results We find that the instances we selected for this task can vary by the annotator's demographics (see Table 1). For instance, annotators who were racial minorities rated the instance \"Justice for the yellow man lol\" as hate speech, while White annotators voted being not sure.\nWe also report the results in Table 2. We report the p-values after applying the Bonferroni correction in the Appendix C.1. Dynahate is highly correlated with people who grow up in English-speaking countries (r = 0.70 * ), who have a college education (r = 0.66 * ), are White (r = 0.69 * ), and are 20-30 years old (r = 0.65 * ). However, it also has high alignment with other populations, such as people who live in West South Asia (r = 0.77).\nPerspective API also tends to align with WEIRD populations, though to a lesser degree than Dynahate. Perspective API exhibits some alignment with people who grow up and live in English-speaking (r = 0.33 * , r = 0.34 * respectively), have a college education (r = 0.34 * ), are White (r = 0.29 * ), and are 20-30 years old (r = 0.34 * ). It also exhibits higher alignment with other populations, such as people who live in Confucian countries (r = 0.36) compared to English-speaking countries. Unexpectedly, White people rank lowest in Pearson's r score within the ethnicity category.\nRewire API similarly shows this bias. It has a moderate correlation with people who grow up and live in English-speaking countries (r = 0.58 * , r = 0.60 * respectively), have a college education (r = 0.56 * ), are White (r = 0.56 * ), and are 20-30 years old (r = 0.56 * ).\nA Western bias is also shown in ToxiGen RoBERTa. ToxiGen RoBERTa shows some alignment with people who grow up (r = 0.37 * ) and live in (r = 0.38 * ) English-speaking countries, have a college education (r = 0.38 * ), are White (r = 0.32 * ), and are between 20-30 years of age (r = 0.38 * ).\nWe also observe similar behavior with GPT-4. The demographics with some of the higher Pearson's r values in its category are people who grow up (r = 0.41 * ) and live in (r = 0.42 * ) English-speaking countries, are college-educated (r = 0.39 * ), are White (r = 0.38 * ), and are 20-30 years old (r = 0.42 * ). It shows stronger alignment to Asian-Americans (r = 0.39 * ) compared to White people, as well as people who live in Baltic countries (r = 0.75) and people who grow up in Confucian countries (r = 0.52 * ) compared to people from English-speaking countries.\nAs in the previous task, labels from native English speakers are strongly correlated with datasets and models. Non-binary people align less with Dynahate, Perspective API, Rewire, ToxiGen RoBERTa, and GPT-4 compared to other genders. Also, people who are professional school-educated or are Black, Latinx, and Native American rank least in alignment for education and ethnicity respectively.\nIn this paper, we characterized design biases and the positionality of datasets and models in NLP. We introduced the NLPositionality framework for identifying design biases in NLP datasets and models. NLPositionality consists of a two-step process of collecting annotations from diverse annotators for a specific task and then computing the alignment of the annotations to dataset labels and model predictions using Pearson's r. We applied NLPositionality to two tasks: social acceptability and hate speech detection, with two datasets and five models in total. In this section, we discuss key takeaways from our experiments and offer recommendations to account for design biases in datasets and models.\nThere Is Positionality in NLP Models and datasets have positionality, as they align better with some populations than others. This corroborates work from Cambo and Gergle (2022) on model positionality, which quantifies positionality by inspecting the content of annotated documents, as well as work from Rogers (2021), who argues that collecting a corpus of speech inherently encodes a particular world view (e.g., via linguistic structures, topic of conversations, and the speaker's social context). We extend these works by showing design biases and quantifying dataset and model positionality by computing correlations between LabintheWild annotations, dataset labels, and model predictions.\nOur case studies show examples of positionality in NLP. However, most socially-aligned tasks may encode design biases due to differences in language use between demographic groups, for example, commonsense reasoning (Shwartz, 2022), question answering (Gor et al., 2021), and sentiment analysis (Mohamed et al., 2022). Even tasks that are considered purely linguistic have seen design biases: in parsing and tagging, performance differences exist between texts written by people of different genders (Garimella et al., 2019), ages , and races (Johannsen et al., 2015;J\u00f8rgensen et al., 2015). This shows how common design biases are in NLP, as language is a social construct (Burr, 2015) and technologies are imbued with their creator's values (Friedman, 1996). This raises the question of whether there are any valueneutral language technologies (Birhane et al., 2022;Winner, 2017).\nDatasets and Models Skew Western Across all tasks, models, and datasets, we find statistically significant moderate correlations with Western, educated, White, and young populations, indicating that language technologies are WEIRD to an extent, though each to varying degrees. Prior work identifies Western-centric biases in NLP research (Hershcovich et al., 2022), as a majority of research is conducted in the West (ACL, 2017;Caines, 2021). Joshi et al. (2020); Blasi et al. (2022) find disproportionate amounts of resources dedicated to English in NLP research, while Ghosh et al. (2021) identify cross-geographic errors made by toxicity models in non-Western contexts. This could lead to serious downstream implications such as language extinction (Kornai, 2013). Not addressing these biases risks imposing Western standards on non-Western populations, potentially resulting in a new kind of colonialism in the digital age (Irani et al., 2010). Some Populations Are Left Behind Certain demographics consistently rank lowest in their alignment with datasets and models across both tasks compared to other demographics of the same type. Prior work has also reported various biases against these populations in datasets and models: people who are non-binary (e.g., Dev et al., 2021), Black (e.g., Sap et al., 2019;Davidson et al., 2019), Latinx (e.g., Dodge et al., 2021), Native American (e.g., Mager et al., 2018); and people who are not native English speakers (e.g., Joshi et al., 2020). These communities are historically marginalized by technological systems (Bender et al., 2021).", "publication_ref": ["b83", "b35", "b71", "b76", "b36", "b33", "b51", "b52", "b16", "b31", "b12", "b86", "b0", "b17", "b53", "b13", "b35", "b55", "b49", "b23", "b73", "b21", "b26", "b59", "b53", "b11"], "figure_ref": [], "table_ref": ["tab_2", "tab_4"]}, {"heading": "Datasets Tend to Align with Their Annotators", "text": "We observe that the positionality we compute is similar to the reported annotator demographics of the datasets, indicating that annotator background contributes to dataset positionality. Social Chemistry reports their annotators largely being women, White, between 30-39 years old, having a college education, and from the U.S. (Forbes et al., 2020), all of which have high correlation to the dataset. Similarly, Dynahate exhibits high correlation with their annotator populations, which are mostly women, White, 18-29 years old, native English speakers, and British (Vidgen et al., 2021). This could be because annotators' positionalities cause them to make implicit assumptions about the context of subjective annotation tasks, which affects its labels (Wan et al., 2023;Birhane et al., 2022). In toxicity modeling, men and women value speaking freely versus feeling safe online differently (Duggan et al., 2014).\nRecommendations Based on these findings, we discuss some recommendations. Following prior work on documenting the choices made in building datasets  and models (Bender and Friedman, 2018;Bender et al., 2021), researchers should keep a record of all design choices made while building them. This can improve reproducibility (NAACL, 2021;AAAI, 2023) and aid others in understanding the rationale behind the decisions, revealing some of the researcher's positionality. Similar to the \"Bender Rule\" (Bender, 2019), which suggests stating the language used, researchers should report their positionality and the assumptions they make (potentially after paper acceptance to preserve anonymity).\nWe echo prior work in recommending methods to center the perspectives of communities who are harmed by design biases (Blodgett et al., 2020;Bender et al., 2021). This can be done using approaches such as participatory design (Spinuzzi, 2005), including interactive storyboarding (Madsen and Aiken, 1993), as well as value-sensitive design (Friedman, 1996), including panels of experiential experts (Madsen and Aiken, 1993). Building datasets and models with large global teams such as BigBench (Srivastava et al., 2022) and NL-Augmenter (Dhole et al., 2021) could also reduce design biases by having diverse teams (Li, 2020).\nTo account for annotator subjectivity (Aroyo and Welty, 2015), researchers should make concerted efforts to recruit annotators from diverse backgrounds. Websites like LabintheWild can be platforms where these annotators are recruited. Since new design biases could be introduced in this process, we recommend following the practice of documenting the demographics of annotators as in prior works (e.g., Forbes et al., 2020;Vidgen et al., 2021) to record a dataset's positionality.\nWe urge considering research through the lens of perspectivism (Basile et al., 2021), i.e. being mindful of different perspectives by sharing datasets with disaggregated annotations and finding modeling techniques that can handle inherent disagreements or distributions (Plank, 2022), instead of forcing a single answer in the data (e.g., by majority vote; Davani et al., 2022) or model (e.g., by classification to one label; Costanza-Chock, 2018). Researchers also should carefully consider how they aggregate labels from diverse annotators during modeling so their perspectives are represented, such as not averaging annotations to avoid the \"tyranny of the mean\" (Talat et al., 2022).\nFinally, we argue that the notion of \"inclusive NLP\" does not mean that all language technologies have to work for everyone. Specialized datasets and models are immensely valuable when the data collection process and other design choices are intentional and made to uplift minority voices or historically underrepresented cultures and languages, such as Masakhane-NER (Adelani et al., 2021) and AfroLM (Dossou et al., 2022). There have also been efforts to localize the design of technologies, including applications that adapt their design and functionality to the needs of different cultures (e.g., Oyibo, 2016;Bernstein, 2011, 2013). Similarly, language models could be made in more culturally adaptive ways, because one size does not fit all (Groenwold et al., 2020;Rettberg, 2022). Therefore, we urge the NLP community to value the adaptation of language technologies from one language or culture to another (Joshi et al., 2020).", "publication_ref": ["b81", "b82", "b12", "b28", "b10", "b11", "b9", "b14", "b11", "b77", "b31", "b58", "b57", "b3", "b81", "b8", "b20", "b19", "b79", "b1", "b64", "b37", "b70", "b53"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We introduce NLPositionality, a framework to quantify design biases and positionality of datasets and models. In this work, we present how researcher positionality leads to design biases and subsequently gives positionality to datasets and models, potentially resulting in these artifacts not working equally for all populations. Our framework involves recruiting a demographically diverse pool of crowdworkers from around the world on LabintheWild, who then re-annotate a sample of a dataset for an NLP task. We apply NLPositionality to two tasks, social acceptability and hate speech detection, to show that models and datasets have a positionality and design biases by aligning better with Western, White, college-educated, and younger populations. Our results indicate the need for more inclusive models and datasets, paving the way for NLP research that benefits all people.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "Our study has several limitations. First, demographics may not be the best construct for positionality, as there may be variability of beliefs within demographic groups. Assuming that there is homogeneity within demographic groups is reductionist and limited. Rather, capturing an individual's attitudes or beliefs may be a more reliable way to capture one's positionality that future work can investigate.\nStudy annotators could also purposefully answer untruthfully, producing low-quality annotations. We address this risk by using LabintheWild. LabintheWild has been shown to produce highquality data because participants are intrinsically motivated to participate by learning something about themselves (Reinecke and Gajos, 2015). However, as is the case for all online recruiting methods, our sample of participants is not representative of the world's population due to the necessity of having access to the Internet. In addition, there is likely a selection bias in who decides to participate in a LabintheWild study.\nPearson's r may not fully capture alignment as it does not consider interaction effects between different demographics (i.e., intersectionality). Thus, there may be additional mediating or moderating variables that may explain the results that our analysis does not consider. We also took the average of the annotations per group, which could mask individual variations (Talat et al., 2022). Also, having a low number of participants from specific demographic groups may limit how well the results generalize to the entire group; further, it may risk tokenizing already marginalized communities.\nAs part of our study, we apply NLPositionality to only two tasks which have relatively straightforward annotation schemes. It may be difficult to generalize to other NLP tasks which have harder annotation schemes, especially ones that require a lot of explanation to the annotators, for example, natural language inference (NLI) tasks.\nOur approach is evaluated and works the best for classification tasks and classifiers. Generation tasks would need more careful annotator training which is difficult to achieve on a voluntary platform without adequate incentives. Having annotators use one Likert scale to rate the social acceptability and toxicity of a situation or text may not be a sufficient measure to represent these complex social phenomena. To reduce this threat, we provide detailed instructions that describe how to provide annotations and followed the original annotation setup as closely as possible.", "publication_ref": ["b69", "b79"], "figure_ref": [], "table_ref": []}, {"heading": "Ethics Statement", "text": "Towards Inclusive NLP Systems Building inclusive NLP systems is important so that everyone can benefit from their usage. Currently, these sys-tems exhibit many design biases that negatively impact minoritized or underserved communities in NLP (Joshi et al., 2020;Blodgett et al., 2020;Bender et al., 2021). Our work is a step towards reducing these disparities by understanding that models and datasets have positionalities and by identifying design biases. The authors take inspiration from fields outside of NLP by studying positionality (Rowe, 2014) and acknowledge crossdisciplinary research as crucial to building inclusive AI systems.", "publication_ref": ["b53", "b14", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Ethical Considerations", "text": "We recognize that the demographics we collected only represent a small portion of a person's positionality. There are many aspects of positionality that we did not collect, such as sexual orientation, socioeconomic status, ability, and size. Further, we acknowledge the limitation of assigning labels to people as being inherently reductionist. As mentioned in \u00a77, using a single Likert scale for social acceptability and toxicity is not sufficient in capturing the complexities in these phenomena, such as situational context.\nWe note that quantifying positionality of existing systems is not an endorsement of the system. In addition to making sure that language technologies work for all populations, researchers should also continue to examine whether these systems should exist in the first place (Denton and Gebru, 2020;Keyes et al., 2019). Further, we note that understanding a dataset or model's positionality does not preclude researchers from the responsibilities of adjusting it further.\nThis study was undertaken following approval from the IRB at the University of Washington (STUDY00014813). LabintheWild annotators were not compensated financially. They were lay people from a wide range of ages (including minors) and diverse backgrounds. Participants were asked for informed consent to the study procedures as well as the associated risks, such as being exposed to toxic or mature content, prior to beginning the study.\nResearch Team Positionality We discuss aspects of our positionality below that we believe are most relevant to this research. The research team is comprised of computer scientists who study human-computer interaction and NLP and have a bent for using quantitative methods. Thus, we approach the topic from a perspective that assumes that positionality can be characterized, fixed, and quantified.\nThe entire research team currently resides in the United States. In alphabetical order, the team members originate from Belgium and Switzerland, France, Germany, India, and the United States; and identify as East Asian, South Asian, and White. These nationalities and ethnicities are overrepresented in the development of NLP technologies. Thus, we acknowledge that our knowledge of how design biases in NLP datasets and models impact people is largely through research, rather than personal experience.", "publication_ref": ["b22", "b54"], "figure_ref": [], "table_ref": []}, {"heading": "A Data", "text": "In this section, we describe all the decisions that went into sampling data points from the different datasets and its post-processing.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1 Sampling", "text": "For Social Chemistry, we sample instances whose label for anticipated agreement by the general public was \"Controversial (\u223c 50%)\". We ensure the samples are equally represented by the moral foundation label, which we compute based on majority vote across annotators. In the study, annotators respond whether they found a presented action socially acceptable.\nFor Dynahate, we randomly sample instances from rounds 3 and 4. In these rounds, annotators generated examples of implicit hate, which is subtler and harder to detect and could yield differences in annotations. We ensure that there are equal amounts of hateful and not hateful instances and that the types of targets of the hateful instances are equally represented. During the studsy, annotators respond whether they found a presented instance toxic.\nFor both social acceptability and hate speech detection, annotators responded whether they found the situation moral and whether they found the instance to be hate speech respectively.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Post-Processing", "text": "Because Social Chemistry has multiple annotations for each instance, we compute an aggregate score by taking the average score across annotators. This score is then used to correlate to the annotators' aggregated scores.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Study Design", "text": "In this section, we discuss the design of the LabintheWild experiments. The social acceptability task was released to the public in April 2022. The hate speech detection task was released August 2022. To reduce confounding factors on the data collection process, we conduct multiple user studies of the LabintheWild experiments prior to the public release. Additionally, all the annotations collected through the experiments are anonymous and are stored securely.\nThe social acceptability task is marketed as \"Could you live with an AI and its morals?\" Participants for this study provide annotations for 25 situations. The hate speech detection task is marketed as \"Do you and AI agree on what is hate speech? Let's find out!\" Participants provid annotations for 15 instances.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.1 LabintheWild Study Flow", "text": "We describe the format of the LabintheWild experiment. The phases of the experiment are: obtaining consent, collecting demographics, explaining instructions, collecting annotations, collecting study feedback, and displaying results.\nObtaining Consent Prior to beginning the study, participants review a consent form. The consent form includes information on the purpose of the research, what the participant will do, risks and benefits of the research, privacy and data collection methods, and contact information of the researchers. At the end of the form, participants give explicit consent to participate in the study.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Collecting Demographics", "text": "We then collect the demographics of study participants. LabintheWild participants enter in whether they had taken this test before, the country they lived in the longest, the country of residence, age, native language, religion, education, and ethnicity. No demographics are required except for the country the participant lived in the longest and whether they have taken the test before. Additionally, we only display ethnicity for people within the United States.\nExplaining Instructions For each task, we provide instructions to participants on how to perform the annotation task. For social acceptability, we explain social acceptability as rating \"what you think about the situation in general from an ethical perspective\" (see Figure 5). For hate speech detection, use the definition of hate speech from Dynahate and we provide three examples of hate speech (see Figure 6). We also present examples of negative sentiment, profanity, or discussing groups that could be confused as hate speech, but are not hate speech.\nCollecting Annotations After being presented with instructions, participants begin data collection from the 300 instances selected from Section A.1. For each task, we keep the annotation setup identical to the original one. For social acceptability, we collect Likert-scale ratings of situations ranging from \"It's very bad\", \"It's bad\", \"It's okay\", \"It's good\", and \"It's very good\". Participants can provide rationale for their decision by using an open text box. The data collection interface is presented in Figure 4. For hate speech detection, we collect ratings of instances ranging from \"Hate speech\", \"Not sure\", \"Not hate speech\". We also provide an optional open-text box for participants to explain their rationale. The data collection interface is presented in Figure 7. After submitting the annotation, the participant is able to see a visualization on how the AI responded as well as how other participants from the same country responded to the instance.\nWe also specifically sample which instances to present to participants for annotation. We sample a third of the instances that did not have any annotations from the demographic and a third that are already sampled by participants of the demographic. The rest are equally split across the different of types of instances (i.e., moral foundation for Social Chemistry, hate type for Dynahate).\nProviding Study Feedback Following typical LabintheWild experiment procedures, we collect feedback from participants about the study. Participants can enter open-text feedback on anything. They also submit whether they encountered technical difficulties during the study or whether they cheated. Participants can elaborate on their answers from the prior questions in an open-text box.\nDisplaying Overall Results Finally, participants see their overall results for the experiment task. First, participants are presented with the percentage of time they agreed with the AI as well as with participants as the same demographic as them (see Figure 8). Each of these agreement scores are further broken down by the type of the instance (i.e., moral foundation for Social Chemistry and hate type for Dynahate).", "publication_ref": [], "figure_ref": ["fig_4", "fig_5", "fig_3", "fig_6"], "table_ref": []}, {"heading": "C Additional Results", "text": "In this section, we report additional results from our analyses of the LabintheWild data.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1 p-values", "text": "We report the p-values from our analyses from Table 3.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D Cultural Spheres", "text": "Division of countries can be done through continents. However, continents are often not representative of the countries within it and clustering based on them can lead to inaccurate findings. For example, Asia includes both Japan and Saudi Arabia, which are different culturally. We instead adopt cultural spheres as used in World Values Survey (Haerpfer and Kizilova, 2012), which clusters the countries in terms of the values they uphold and norms they follow. Table 4 shows the countries and the spheres.    Figure 8: Results interface for the social acceptability task. Participants can view how well they aligned with the AI, as well as how other demographics they reported aligned with the AI. The AI alignment is further broken down by the type of moral foundation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We thank Yejin Choi and Liwei Jiang for their invaluable inputs in the early stages of the project, especially their ideas in shaping the direction of this work, as well as the ReViz team at the Allen Institute for AI for their technical support for building the LabintheWild experiments. We also thank the members of the University of Washington NLP, HCI, and ML/AI groups for their feedback throughout the project. We give a special thanks to Mei , an outstanding canine researcher, for providing support and motivation throughout the study. Jenny T. Liang was supported by the National Science Foundation under grants DGE1745016 and DGE2140739. This research was partially supported by the National Science Foundation under grant 2230466.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "", "journal": "ACL Diversity Statistics", "year": "2017", "authors": "Acl "}, {"ref_id": "b1", "title": "Masakhaner: Named entity recognition for african languages", "journal": "Transactions of the Association for Computational Linguistics", "year": "2021", "authors": "Jade David Ifeoluwa Adelani; Graham Abbott;  Neubig; D Daniel; Julia 'souza; Constantine Kreutzer; Chester Lignos; Happy Palen-Michel; Shruti Buzaaba; Sebastian Rijhwani;  Ruder"}, {"ref_id": "b2", "title": "Probing pre-trained language models for cross-cultural differences in values", "journal": "", "year": "2023", "authors": "Arnav Arora; Lucie-Aim\u00e9e Kaffee; Isabelle Augenstein"}, {"ref_id": "b3", "title": "Truth is a lie: Crowd truth and the seven myths of human annotation", "journal": "AI Magazine", "year": "2015", "authors": "Lora Aroyo; Chris Welty"}, {"ref_id": "b4", "title": "Morality beyond the WEIRD: How the nomological network of morality varies across cultures", "journal": "", "year": "2022", "authors": "Mohammad Atari; Jonathan Haidt; Jesse Graham; Sena Koleva; T Sean; Morteza Stevens;  Dehghani"}, {"ref_id": "b5", "title": "Pay attention, please: Formal language improves attention in volunteer and paid online experiments", "journal": "", "year": "2019", "authors": "Tal August; Katharina Reinecke"}, {"ref_id": "b6", "title": "Jean-Fran\u00e7ois Bonnefon, and Iyad Rahwan", "journal": "Nature", "year": "2018", "authors": "Edmond Awad; Sohan Dsouza; Richard Kim; Jonathan Schulz; Joseph Henrich; Azim Shariff"}, {"ref_id": "b7", "title": "Universals and variations in moral decisions made in 42 countries by 70,000 participants", "journal": "National Academy of Sciences", "year": "2020", "authors": "Edmond Awad; Sohan Dsouza; Azim Shariff"}, {"ref_id": "b8", "title": "Toward a perspectivist turn in ground truthing for predictive computing", "journal": "", "year": "2021", "authors": "Valerio Basile; Federico Cabitza; Andrea Campagner; Michael Fell"}, {"ref_id": "b9", "title": "The# benderrule: On naming the languages we study and why it matters. The Gradient", "journal": "", "year": "2019", "authors": "Emily Bender"}, {"ref_id": "b10", "title": "Data statements for natural language processing: Toward mitigating system bias and enabling better science", "journal": "Transactions of the Association for Computational Linguistics", "year": "2018", "authors": "Emily M Bender; Batya Friedman"}, {"ref_id": "b11", "title": "On the dangers of stochastic parrots: Can language models be too big", "journal": "", "year": "2021", "authors": "Emily M Bender; Timnit Gebru; Angelina Mcmillan-Major; Shmargaret Shmitchell"}, {"ref_id": "b12", "title": "The values encoded in machine learning research", "journal": "", "year": "2022", "authors": "Abeba Birhane; Pratyusha Kalluri; Dallas Card; William Agnew; Ravit Dotan; Michelle Bao"}, {"ref_id": "b13", "title": "Systematic inequalities in language technology performance across the world's languages", "journal": "Long Papers", "year": "2022", "authors": "Damian Blasi; Antonios Anastasopoulos; Graham Neubig"}, {"ref_id": "b14", "title": "Language (technology) is power: A critical survey of \"bias\" in NLP", "journal": "", "year": "2020", "authors": " Su Lin; Solon Blodgett; Hal Barocas; Iii Daum\u00e9; Hanna Wallach"}, {"ref_id": "b15", "title": "Man is to computer programmer as woman is to homemaker? Debiasing word embeddings", "journal": "", "year": "2016", "authors": "Tolga Bolukbasi; Kai-Wei Chang; Y James; Venkatesh Zou; Adam T Saligrama;  Kalai"}, {"ref_id": "b16", "title": "", "journal": "", "year": "2015", "authors": "Vivien Burr"}, {"ref_id": "b17", "title": "The geographic diversity of NLP conferences", "journal": "", "year": "2021", "authors": "Andrew Caines"}, {"ref_id": "b18", "title": "Model positionality and computational reflexivity: Promoting reflexivity in data science", "journal": "", "year": "2022", "authors": "Darren Scott Allen Cambo;  Gergle"}, {"ref_id": "b19", "title": "Design justice, AI, and escape from the matrix of domination", "journal": "Journal of Design and Science", "year": "2018", "authors": "Sasha Costanza-Chock"}, {"ref_id": "b20", "title": "Dealing with disagreements: Looking beyond the majority vote in subjective annotations", "journal": "Transactions of the Association for Computational Linguistics", "year": "2022", "authors": "Aida Mostafazadeh Davani; Mark D\u00edaz; Vinodku "}, {"ref_id": "b21", "title": "Racial bias in hate speech and abusive language detection datasets", "journal": "", "year": "2019", "authors": "Thomas Davidson; Debasmita Bhattacharya; Ingmar Weber"}, {"ref_id": "b22", "title": "Tutorial on fairness, accountability, transparency, and ethics in computer vision at CVPR 2020", "journal": "", "year": "2020", "authors": "Emily Denton; Timnit Gebru"}, {"ref_id": "b23", "title": "Harms of gender exclusivity and challenges in non-binary representation in language technologies", "journal": "", "year": "2021", "authors": "Sunipa Dev; Masoud Monajatipoor; Anaelia Ovalle; Arjun Subramonian; Jeff Phillips; Kai-Wei Chang"}, {"ref_id": "b24", "title": "NL-augmenter: A framework for task-sensitive natural language augmentation", "journal": "", "year": "2021", "authors": "Varun Kaustubh D Dhole; Sebastian Gangal; Aadesh Gehrmann; Zhenhao Gupta; Saad Li; Abinaya Mahamood; Simon Mahendiran; Ashish Mille; Samson Srivastava;  Tan"}, {"ref_id": "b25", "title": "Demographics and dynamics of mechanical turk workers", "journal": "", "year": "2018", "authors": "Djellel Difallah; Elena Filatova; Panos Ipeirotis"}, {"ref_id": "b26", "title": "Documenting large webtext corpora: A case study on the colossal clean crawled corpus", "journal": "", "year": "2021", "authors": "Jesse Dodge; Maarten Sap; Ana Marasovi\u0107; William Agnew; Gabriel Ilharco; Dirk Groeneveld; Margaret Mitchell; Matt Gardner"}, {"ref_id": "b27", "title": "Afrolm: A selfactive learning-based multilingual pretrained language model for 23 African languages", "journal": "", "year": "2022", "authors": "F P Bonaventure; Atnafu Dossou; Oreen Lambebo Tonja; Salomey Yousuf; Abigail Osei; Iyanuoluwa Oppong;  Shode"}, {"ref_id": "b28", "title": "Online harassment", "journal": "Pew Research Center", "year": "2014", "authors": "Maeve Duggan;  Rainie;  Smith;  Funk; M Lenhart;  Madden"}, {"ref_id": "b29", "title": "Pathways to equity in mathematics education: How life experiences impact researcher positionality", "journal": "Educational Studies in Mathematics", "year": "2011", "authors": "Q Mary; Tonya Gau Foote;  Bartell"}, {"ref_id": "b30", "title": "Learning to reason about social and moral norms", "journal": "", "year": "", "authors": "Maxwell Forbes; Jena D Hwang; Vered Shwartz; Maarten Sap; Yejin Choi"}, {"ref_id": "b31", "title": "Value-sensitive design", "journal": "", "year": "1996", "authors": "Batya Friedman"}, {"ref_id": "b32", "title": "Bias in computer systems", "journal": "ACM Transactions on Information Systems", "year": "1996", "authors": "Batya Friedman; Helen Nissenbaum"}, {"ref_id": "b33", "title": "Women's syntactic resilience and men's grammatical luck: Gender-bias in part-ofspeech tagging and dependency parsing", "journal": "", "year": "2019", "authors": "Aparna Garimella; Carmen Banea; Dirk Hovy; Rada Mihalcea"}, {"ref_id": "b34", "title": "Datasheets for datasets", "journal": "Communications of the ACM", "year": "2021", "authors": "Timnit Gebru; Jamie Morgenstern; Briana Vecchione; Jennifer Wortman Vaughan; Hanna Wallach; Hal Daum\u00e9; Iii ; Kate Crawford"}, {"ref_id": "b35", "title": "Detecting crossgeographic biases in toxicity modeling on social media", "journal": "", "year": "2021", "authors": "Sayan Ghosh; Dylan Baker; David Jurgens; Vinodkumar Prabhakaran"}, {"ref_id": "b36", "title": "Toward deconfounding the effect of entity demographics for question answering accuracy", "journal": "", "year": "2021", "authors": "Maharshi Gor; Kellie Webster; Jordan Boyd-Graber"}, {"ref_id": "b37", "title": "Investigating African-American Vernacular English in transformer-based text generation", "journal": "", "year": "2020", "authors": "Sophie Groenwold; Lily Ou; Aesha Parekh; Samhita Honnavalli; Sharon Levy; Diba Mirza; William Yang Wang"}, {"ref_id": "b38", "title": "Whose language counts as high quality? measuring language ideologies in text data selection", "journal": "", "year": "2022", "authors": "Suchin Gururangan; Dallas Card; K Sarah; Emily K Drier;  Gade; Z Leroy; Zeyu Wang; Luke Wang; Noah A Zettlemoyer;  Smith"}, {"ref_id": "b39", "title": "The world values survey. The Wiley-Blackwell Encyclopedia of Globalization", "journal": "", "year": "2012", "authors": "W Christian; Kseniya Haerpfer;  Kizilova"}, {"ref_id": "b40", "title": "A systematic study of bias amplification", "journal": "", "year": "2022", "authors": "Melissa Hall; Laurens Van Der Maaten; Laura Gustafson; Aaron Adcock"}, {"ref_id": "b41", "title": "Speaking multiple languages affects the moral bias of language models", "journal": "", "year": "2022", "authors": "Katharina H\u00e4mmerl; Bj\u00f6rn Deiseroth; Patrick Schramowski; Jind\u0159ich Libovick\u1ef3; A Constantin; Alexander Rothkopf; Kristian Fraser;  Kersting"}, {"ref_id": "b42", "title": "Towards a critical race methodology in algorithmic fairness", "journal": "", "year": "2020", "authors": "Alex Hanna; Emily Denton; Andrew Smart; Jamila Smith-Loud"}, {"ref_id": "b43", "title": "Toxigen: Controlling language models to generate implied and adversarial toxicity", "journal": "Long Papers", "year": "2022", "authors": "Thomas Hartvigsen; Saadia Gabriel; Hamid Palangi; Maarten Sap; Dipankar Ray; Ece Kamar"}, {"ref_id": "b44", "title": "The weirdest people in the world?", "journal": "Behavioral and Brain Sciences", "year": "2010", "authors": "Joseph Henrich; J Steven; Ara Heine;  Norenzayan"}, {"ref_id": "b45", "title": "Laura Cabello Piqueras, Ilias Chalkidis, Ruixiang Cui, Constanza Fierro, Katerina Margatina, Phillip Rust, and Anders S\u00f8gaard. 2022. Challenges and strategies in cross-cultural NLP", "journal": "Long Papers", "year": "", "authors": "Daniel Hershcovich; Stella Frank; Heather Lent; Mostafa Miryam De Lhoneux; Stephanie Abdou; Emanuele Brandl;  Bugliarello"}, {"ref_id": "b46", "title": "Researcher positionality-A consideration of its influence and place in qualitative research-A new researcher guide", "journal": "Shanlax International Journal of Education", "year": "2020", "authors": "Andrew Gary; Darwin Holmes"}, {"ref_id": "b47", "title": "Tagging performance correlates with author age", "journal": "Short Papers", "year": "2015", "authors": "Dirk Hovy; Anders S\u00f8gaard"}, {"ref_id": "b48", "title": "Demographics of Mechanical Turk", "journal": "", "year": "2010", "authors": "G Panagiotis;  Ipeirotis"}, {"ref_id": "b49", "title": "Postcolonial computing: A lens on design and development", "journal": "", "year": "2010", "authors": "Lilly Irani; Janet Vertesi; Paul Dourish; Kavita Philip; Rebecca E Grinter"}, {"ref_id": "b50", "title": "", "journal": "", "year": "", "authors": "Liwei Jiang; Jena D Hwang; Chandra Bhagavatula; Jenny Ronan Le Bras; Jesse Liang; Keisuke Dodge; Maxwell Sakaguchi; Jon Forbes; Saadia Borchardt;  Gabriel"}, {"ref_id": "b51", "title": "Cross-lingual syntactic variation over age and gender", "journal": "", "year": "2015", "authors": "Anders Johannsen; Dirk Hovy; Anders S\u00f8gaard"}, {"ref_id": "b52", "title": "Challenges of studying and processing dialects in social media", "journal": "", "year": "2015", "authors": "Anna J\u00f8rgensen; Dirk Hovy; Anders S\u00f8gaard"}, {"ref_id": "b53", "title": "The state and fate of linguistic diversity and inclusion in the NLP world", "journal": "", "year": "2020", "authors": "Pratik Joshi; Sebastin Santy; Amar Budhiraja; Kalika Bali; Monojit Choudhury"}, {"ref_id": "b54", "title": "A mulching proposal: Analysing and improving an algorithmic system for turning the elderly into highnutrient slurry", "journal": "", "year": "2019", "authors": "Os Keyes; Jevan Hutson; Meredith Durbin"}, {"ref_id": "b55", "title": "Digital language death", "journal": "PLOS ONE", "year": "2013", "authors": "Andr\u00e1s Kornai"}, {"ref_id": "b56", "title": "Reliability in content analysis: Some common misconceptions and recommendations", "journal": "Human Communication Research", "year": "2006", "authors": "Klaus Krippendorff"}, {"ref_id": "b57", "title": "To build less-biased AI, hire a more diverse team", "journal": "Harvard Business Review", "year": "2020", "authors": "Michael Li"}, {"ref_id": "b58", "title": "Experiences using cooperative interactive storyboard prototyping", "journal": "Communications of the ACM", "year": "1993", "authors": "Halskov Kim; Peter H Madsen;  Aiken"}, {"ref_id": "b59", "title": "Challenges of language technologies for the indigenous languages of the Americas", "journal": "", "year": "2018", "authors": "Manuel Mager; Ximena Gutierrez-Vasques; Gerardo Sierra; Ivan Meza-Ruiz"}, {"ref_id": "b60", "title": "Model cards for model reporting", "journal": "", "year": "2019", "authors": "Margaret Mitchell; Simone Wu; Andrew Zaldivar; Parker Barnes; Lucy Vasserman; Ben Hutchinson; Elena Spitzer; Deborah Inioluwa; Timnit Raji;  Gebru"}, {"ref_id": "b61", "title": "Artelingo: A million emotion annotations of WikiArt with emphasis on diversity over language and culture", "journal": "", "year": "", "authors": ""}, {"ref_id": "b62", "title": "Citizen science opportunities in volunteerbased online experiments", "journal": "", "year": "2017", "authors": "Nigini Oliveira; Eunice Jun; Katharina Reinecke"}, {"ref_id": "b63", "title": "OpenAI. 2023. Gpt-4 technical report. arXiv", "journal": "", "year": "", "authors": ""}, {"ref_id": "b64", "title": "Designing culture-based persuasive technology to promote physical activity among university students", "journal": "", "year": "2016", "authors": "Kiemute Oyibo"}, {"ref_id": "b65", "title": "2022. The 'problem' of human label variation: On ground truth in data, modeling and evaluation", "journal": "", "year": "", "authors": "Barbara Plank"}, {"ref_id": "b66", "title": "Scaling language models: Methods, analysis & insights from training Gopher", "journal": "", "year": "2021", "authors": "Sebastian Jack W Rae; Trevor Borgeaud; Katie Cai; Jordan Millican; Francis Hoffmann; John Song; Sarah Aslanides; Roman Henderson; Susannah Ring;  Young"}, {"ref_id": "b67", "title": "Improving performance, perceived usability, and aesthetics with culturally adaptive user interfaces", "journal": "ACM Transactions on Computer-Human Interaction", "year": "2011", "authors": "Katharina Reinecke; Abraham Bernstein"}, {"ref_id": "b68", "title": "Knowing what a user likes: A design science approach to interfaces that automatically adapt to culture", "journal": "Mis Quarterly", "year": "2013", "authors": "Katharina Reinecke; Abraham Bernstein"}, {"ref_id": "b69", "title": "LabInTheWild: Conducting large-scale online experiments with uncompensated samples", "journal": "", "year": "2015", "authors": "Katharina Reinecke; Krzysztof Z Gajos"}, {"ref_id": "b70", "title": "ChatGPT is multilingual but monocultural, and it's learning your values", "journal": "", "year": "2022", "authors": "Jill Walker Rettberg"}, {"ref_id": "b71", "title": "Changing the world by changing the data", "journal": "Long Papers", "year": "2021", "authors": "Anna Rogers"}, {"ref_id": "b72", "title": "Positionality. The SAGE encyclopedia of action research", "journal": "", "year": "2014", "authors": "E Wendy;  Rowe"}, {"ref_id": "b73", "title": "The risk of racial bias in hate speech detection", "journal": "", "year": "2019", "authors": "Maarten Sap; Dallas Card; Saadia Gabriel; Yejin Choi; Noah A Smith"}, {"ref_id": "b74", "title": "Annotators with attitudes: How annotator beliefs and identities bias toxic language detection", "journal": "", "year": "2022", "authors": "Maarten Sap; Swabha Swayamdipta; Laura Vianna; Xuhui Zhou; Yejin Choi; Noah A Smith"}, {"ref_id": "b75", "title": "Qualititative research: The essential guide to theory and practice. Qualitative Research: The Essential Guide to Theory and Practice", "journal": "", "year": "2013", "authors": "Maggi Savin-Baden; Claire Howell-Major"}, {"ref_id": "b76", "title": "Good night at 4 pm?! time expressions in different cultures", "journal": "", "year": "2022", "authors": "Vered Shwartz"}, {"ref_id": "b77", "title": "The methodology of participatory design", "journal": "Technical Communication", "year": "2005", "authors": "Clay Spinuzzi"}, {"ref_id": "b78", "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models", "journal": "", "year": "2022", "authors": "Aarohi Srivastava; Abhinav Rastogi; Abhishek Rao; Abu Awal Md Shoeb; Abubakar Abid; Adam Fisch; R Adam; Adam Brown; Aditya Santoro; Adri\u00e0 Gupta;  Garriga-Alonso"}, {"ref_id": "b79", "title": "On the machine learning of ethical judgments from natural language", "journal": "", "year": "2022", "authors": "Zeerak Talat; Hagen Blix; Josef Valvoda; Maya Indira Ganesh; Ryan Cotterell; Adina Williams"}, {"ref_id": "b80", "title": "Gender and dialect bias in YouTube's automatic captions", "journal": "", "year": "2017", "authors": "Rachael Tatman"}, {"ref_id": "b81", "title": "Learning from the worst: Dynamically generated datasets to improve online hate detection", "journal": "Long Papers", "year": "2021", "authors": "Bertie Vidgen; Tristan Thrush; Zeerak Waseem; Douwe Kiela"}, {"ref_id": "b82", "title": "Everyone's voice matters: Quantifying annotation disagreement using demographic information", "journal": "", "year": "2023", "authors": "Ruyuan Wan; Jaehyung Kim; Dongyeop Kang"}, {"ref_id": "b83", "title": "Detecting hate speech on the world wide web", "journal": "", "year": "2012", "authors": "William Warner; Julia Hirschberg"}, {"ref_id": "b84", "title": "Disembodied machine learning: On the illusion of objectivity in NLP", "journal": "", "year": "2021", "authors": "Zeerak Waseem; Smarika Lulz; Joachim Bingel; Isabelle Augenstein"}, {"ref_id": "b85", "title": "Design and Analysis: A Researcher's Handbook", "journal": "Prentice-Hall", "year": "2004", "authors": "D Thomas; Geoffrey Wickens;  Keppel"}, {"ref_id": "b86", "title": "Do artifacts have politics?", "journal": "", "year": "2017", "authors": "Langdon Winner"}, {"ref_id": "b87", "title": "Detoxifying language models risks marginalizing minority voices", "journal": "", "year": "2021", "authors": "Albert Xu; Eshaan Pathak; Eric Wallace; Suchin Gururangan; Maarten Sap; Dan Klein"}, {"ref_id": "b88", "title": "", "journal": "", "year": "", "authors": " Country"}, {"ref_id": "b89", "title": "", "journal": "African Islamic", "year": "", "authors": ""}, {"ref_id": "b90", "title": "College 1", "journal": "", "year": "", "authors": ""}, {"ref_id": "b91", "title": "", "journal": "", "year": "", "authors": ". . Ethnicity"}, {"ref_id": "b92", "title": "", "journal": "Asian American", "year": "", "authors": " Asian"}, {"ref_id": "b93", "title": "", "journal": "Native American, Alaskan Native", "year": "", "authors": ""}, {"ref_id": "b94", "title": "", "journal": "", "year": "", "authors": ". . Gender"}, {"ref_id": "b95", "title": "", "journal": "Man", "year": "", "authors": ""}, {"ref_id": "b96", "title": "", "journal": "", "year": "", "authors": " English"}, {"ref_id": "b97", "title": "", "journal": "", "year": "", "authors": " Country"}, {"ref_id": "b98", "title": "", "journal": "African Islamic", "year": "", "authors": ""}, {"ref_id": "b99", "title": "Buddhist 7", "journal": "", "year": "", "authors": ""}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure2: Example Annotation. An example instance from the Social Chemistry dataset that was sent to LabintheWild along with the mean of the received annotation scores across various demographics.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure3: Overview of the NLPositionality Framework. Collection (steps 1-4): A subset of datasets' instances are re-annotated via diverse volunteers recruited on LabintheWild. Processing (step 5): We compare the labels collected from LabintheWild with the dataset's original labels and models' predictions. Analysis (step 6): We compute the Pearson's r correlation between the LabintheWild annotations by demographic for the dataset's original labels and the models' predictions. We apply the Bonferroni correction to account for multiple hypothesis testing.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure4: Data collection interface for the social acceptability task. Participants were given a sentence (an action from the Social Chemsitry dataset) and asked to rate how ethical the action was. Participants are shown how other people from their country responded after each attempt.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure5: Instructions for the social acceptability task. Participants were asked to describe their thoughts about a situation from an ethical perspective.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 6 :6Figure 6: Instructions for the toxicity task. Participants were provided with examples of hate speech examples and not hate examples.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 7 :7Figure7: Data collection interface for the hate speech task. Participants were given a sentence (an instance from the Dynahate dataset) and asked to rate whether the instance was toxic or not. Participants are shown how other people from their country responded after each attempt.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Country (Lived Longest) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Education Level . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Ethnicity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Gender . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Native Language . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Age . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Residence) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Religion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "figure_data": "DATASETS:SocialChemistryDynaHateMODELS:GPT-4DelphiPerspectiveAPIRewireAPIToxiGen RoBERTaDemographicPearson's rSocial AcceptabilityToxicity & Hate Speech#\u03b1#\u03b1African Islamic3160.200.54*0.490.472340.220.390.290.390.270.25Baltic1400.410.73*0.72*0.71*540.500.38-0.080.200.050.46Catholic Europe4520.280.64*0.59*0.68*1830.410.320.120.320.210.21Confucian5280.420.75*0.58*0.74*1540.240.470.280.51*0.120.52*English-Speaking8289 0.510.76*0.61*0.74*4025 0.400.70*0.33*0.58*0.37*0.41*Latin American2810.330.450.410.47650.200.390.100.280.090.17Orthodox Europe4260.390.56*0.58*0.67*1390.320.360.180.470.150.13Protestant Europe7060.480.65*0.57*0.67*0.370.40*0.320.230.290.34West South Asia4130.400.63*0.60*0.59*1160.210.340.200.330.300.21College4489 0.480.74*0.66*0.69*2383 0.390.66*0.34*0.56*0.38*0.39*Graduate School1116 0.530.72*0.54*0.69*6040.360.59*0.28*0.51*0.250.38*High School2183 0.490.67*0.54*0.64*9080.410.60*0.250.49*0.30*0.37*PhD7090.460.65*0.55*0.61*3590.450.48*0.190.43*0.260.31Pre-High School4060.400.56*0.46*0.59*1160.260.370.240.45*0.250.38Professional School4600.400.53*0.46*0.49*1950.090.61*0.100.350.090.19Asian, Asian American1160 0.550.66*0.55*0.63*6440.450.57*0.35*0.47*0.33*0.39*Black, African American4650.520.61*0.50*0.57*2870.340.56*0.320.36*0.310.37*Latino / Latina, Hispanic3140.570.62*0.52*0.54*2390.360.43*0.39*0.46*0.310.31Native American, Alaskan Native1030.640.59*0.52*0.64*65-0.230.310.310.320.33Pacific Islander, Native Australian3800.65*0.630.6227-0.360.650.540.640.57White3102 0.550.73*0.61*0.70*1831 0.440.69*0.29*0.56*0.32*0.38*Man4082 0.450.73*0.63*0.69*1798 0.370.65*0.34*0.56*0.34*0.36*Non-Binary8580.410.60*0.51*0.55*3290.480.57*0.210.37*0.270.31*Woman4368 0.550.74*0.60*0.73*2357 0.390.63*0.34*0.53*0.38*0.37*English7338 0.510.76*0.64*0.71*3622 0.400.70*0.33*0.60*0.39*0.42*Not English2157 0.400.62*0.54*0.64*1020 0.270.46*0.32*0.39*0.32*0.36*10-20 yrs old3360 0.500.70*0.61*0.69*1615 0.390.61*0.32*0.55*0.36*0.36*20-30 yrs old4066 0.470.74*0.66*0.70*2114 0.390.65*0.34*0.56*0.38*0.42*30-40 yrs old8700.510.66*0.52*0.61*4190.280.48*0.140.41*0.240.2940-50 yrs old6550.440.62*0.55*0.63*2560.280.63*0.290.57*0.310.37*50-60 yrs old3080.490.69*0.53*0.60*1990.390.57*0.260.41*0.200.2560-70 yrs old2040.480.64*0.49*0.60*19-0.570.420.460.05-0.1870-80 yrs old68-0.56*0.52*0.56*24-0.500.350.360.240.85*80+ yrs old24-0.520.480.4812-0.630.010.45-0.090.43Country (African Islamic1640.270.490.480.461160.210.350.230.290.150.16Baltic530.020.650.650.33140.000.420.140.520.350.75Catholic Europe4060.330.53*0.41*0.64*1720.370.320.110.380.150.22Confucian2680.420.68*0.55*0.77*830.170.410.360.450.330.48English-Speaking7315 0.500.76*0.65*0.73*3819 0.400.72*0.34*0.60*0.38*0.42*Latin American1660.430.54*0.56*0.59*530.150.300.120.26-0.040.17Orthodox Europe2640.380.470.57*0.60*900.310.250.280.370.290.17Protestant Europe7360.460.63*0.57*0.61*3870.360.45*0.310.230.310.31West South Asia1660.440.61*0.57*0.53*21-0.770.220.570.070.16Buddhist1890.330.64*0.58*0.55*690.400.480.100.250.190.50Christian1969 0.500.73*0.55*0.73*1080 0.290.56*0.34*0.49*0.36*0.34*Hindu2010.750.65*0.60*0.58*1090.460.63*0.340.410.300.38Jewish2040.500.66*0.60*0.60*1440.450.64*0.290.43*0.290.33Muslim3190.360.63*0.59*0.72*890.330.420.160.290.140.31Spritual880.480.61*0.60*0.72*13-0.35-0.160.150.20-"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Positionality of NLP datasets and models quantified using Pearson's r correlation coefficients. # denotes the number of annotations associated with a demographic group. \u03b1 denotes Krippendorff's alpha of a demographic group for a task.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": ").", "figure_data": "Setup For the hate speech detection task, 484LabintheWild participants annotated 300 examplesfrom the Dynahate dataset (Vidgen et al., 2021)and rated whether an instance was hate speech ornot. We collected 5, 005 annotations and comparedparticipants' responses to the original Dynahatedataset as well as Perspective API 3 , Rewire API 4 ,ToxiGen RoBERTa (Hartvigsen et al., 2022), andGPT-4 zero-shot. To evaluate GPT-4, we use theprompt used by Rae et al. (2021):Text:<Dynahate instance>Question: Does the above text contain rude,hateful, aggressive, disrespectful,or unreasonable language?Answer:"}], "formulas": [{"formula_id": "formula_0", "formula_text": "B la c k W h it e A s ia n M a n W o m a n N o n -b in a r y P h D P re -H ig h U K In d ia A u s tr a li a U S A P o la n d G e", "formula_coordinates": [3.0, 116.42, 132.86, 136.72, 95.48]}], "doi": "10.1145/3290605.3300478"}