{"title": "Neural Machine Translation Methods for Translating Text to Sign Language Glosses", "authors": "Dele Zhu; Vera Czehmann; Eleftherios Avramidis", "pub_date": "", "abstract": "State-of-the-art techniques common to low resource Machine Translation (MT) are applied to improve MT of spoken language text to Sign Language (SL) glosses. In our experiments, we improve the performance of the transformer-based models via (1) data augmentation, (2) semi-supervised Neural Machine Translation (NMT), (3) transfer learning and (4) multilingual NMT. The proposed methods are implemented progressively on two German SL corpora containing gloss annotations. Multilingual NMT combined with data augmentation appear to be the most successful setting, yielding statistically significant improvements as measured by three automatic metrics (up to over 6 points BLEU), and confirmed via human evaluation. Our best setting outperforms all previous work that report on the same test-set and is also confirmed on a corpus of the American Sign Language (ASL).", "sections": [{"heading": "Introduction", "text": "Sign Language Translation (SLT) aims to break the language barrier between the deaf or hard-ofhearing communities and the hearing communities. One challenging aspect of SLT is the fact that Sign Languages (SLs) are multi-channeled and non-written languages (Langer et al., 2014). Therefore, Machine Translation (MT) for SLs cannot directly take advantage of the recent developments in text-based MT. For this purpose, previous work has used written representations of the SLs. One of these representations are glosses, where signs are labeled by words of the corresponding spoken language, often including affixes and markers.\nIt is known that glosses have strong limitations as a linguistic representation (Pizzuto et al., 2006). However, given the current status of SLT, we have indications that research on SL gloss translation can still be useful. For instance, translation from spoken language text to SL glosses can be useful for interpreters and educational uses (Collins Figure 1: Text-to-video SLT using glosses as an intermediate step (Source of images: M\u00fcller et al., 2020). et al., 2012). Secondly, SL glosses are the only SL representation having several parallel corpora big enough to train MT, and the results may provide indications for the future treatment of other more appropriate representations. Previous research on SLT has used glosses as an intermediate step to build MT systems for translating from SLs to spoken language text (Camgoz et al., 2017(Camgoz et al., , 2018Chen et al., 2023) or from spoken language text to SLs (Stoll et al., 2020;Saunders et al., 2020aSaunders et al., ,b, 2022. In the latter case, glosses allow building the system in two steps, i.e., text-to-gloss translation and glossto-video production (Figure 1). The glosses can be given to a system for the generation of SL (avatar animations, autoencoders, GANs). Our work focuses on the first part of this pipeline, text-to-gloss translation, whose results are responsible for the generated sign animations. We find that prior research, despite its improvements, has still not made a big breakthrough in this direction (Rastgoo et al., 2021).\nSLs are Low-Resource Languages (LRLs) with regards to MT, since there is little parallel data (Coster et al., 2022). Despite the recent progress of MT for LRLs (Sennrich et al., 2016a;Zoph et al., 2016;Sennrich and Zhang, 2019;Ranathunga et al., 2021), few of these methods have been used for MT of SLs, such as data augmentation (Moryossef et al., 2021;Zhang and Duh, 2021;Angelova et al., 2022) and transfer learning . Other efficient techniques, e.g. semi-supervised NMT (Cheng et al., 2016) and multilingual NMT (Johnson et al., 2017) have not been explored. We are therefore inspired to extensively explore the effects of the relevant methods on text-to-gloss translation. To the best of our knowledge, this paper is the first work on text-to-gloss:\n\u2022 to achieve significant improvements, as compared to the baseline methods, on the two known natural SL datasets annotated with glosses (namely for the German SL: Deutsche Geb\u00e4rdensprache, further abbreviated as DGS),\n\u2022 to perform extensive experimentation with most known LRL-related MT methods and their combinations and in particular:\n\u2022 to apply semi-supervised NMT by copying the monolingual data to both the source and target side, for lack of monolingual corpora with glosses,\n\u2022 to use transfer learning via the warm-start strategy, and\n\u2022 to use a multilingual NMT setting with the focus on improving the text-to-gloss direction.\nAll code of this work has been open sourced. 1", "publication_ref": ["b31", "b43", "b36", "b4", "b5", "b8", "b57", "b49", "b47", "b53", "b55", "b46", "b35", "b1", "b9", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Related work", "text": "The early-stage of text-to-gloss translation systems were built using Statistical Machine Translation (SMT; San-Segundo et al., 2012;L\u00f3pez-Lude\u00f1a et al., 2014), in an attempt to translate spoken language into a signing 3D avatar using SL glosses as intermediate. Although the system evaluations reported good results based on limited data and automatic metrics, deaf users assessed the system conversely. Recently, with the advance of NMT, more promising systems have emerged, based on RNNs (Stoll et al., 2020) or as parts of end-to-end transformer systems (Saunders et al., 2020b(Saunders et al., , 2022, which contrary to our work do not try particular LRL-related methods. More related to our work, in terms of text-togloss translation using LRL-related techniques, Li et al. (2021) implement a transformer architecture equipped with an editing agent that learns to synthesize and execute editing actions on the source 1 https://github.com/DFKI-SignLanguage/ text-to-gloss-sign-language-translation  2022) achieve remarkable results with a transfer learning strategy that uses various ways of aggregating linguistic features and takes advantage of a pre-trained mBART model by filtering the original embedding and slicing model weights. In our work, we improve over these transfer learning methods using the warm-start strategy.\nData augmentation has been seen in gloss-totext translation (Moryossef et al., 2021;Zhang and Duh, 2021;Angelova et al., 2022;. Empirical comparison of our efforts with all state-of-the-art systems is presented in Section 5 (Table 4).", "publication_ref": ["b48", "b34", "b57", "b50", "b51", "b32", "b35", "b1"], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Methods", "text": "Our experiments (Figure 2) start from data preprocessing and setting the baseline. We then explore data augmentation, semi-supervised NMT, transfer learning and multilingual NMT as measured by automatic metrics. To confirm the consistency of system improvements between the best performing model and baseline, we conduct human evaluation.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Data augmentation", "text": "Data augmentation is a common technique used to face low resource conditions by adding synthetically generated data from various sources (Li et al., 2019). Here, we focus on the following methods:\nCombining preprocessing methods is based on applying different preprocessing techniques on the source sentences and pairing them with copied target glosses. The differently pre-processed versions are concatenated into a new training dataset. This technique may be beneficial in that no changes are made to the target glosses and meanwhile the datasets are enlarged, being more robust to variable appearances of the spoken language sentences. Back-translation is to obtain additional sourceside data by translating a target-language monolingual dataset with target-to-source model (Sennrich et al., 2016a). The generated source sentences are then paired with their target side into a synthetic parallel dataset. However, we lack a monolingual glosses dataset, so we use a gloss-to-text system to only translate the target-side glosses of the parallel corpus into spoken language text. This results in a synthetic version of the corpus, with the side of spoken language text modified. The synthetic corpus is then concatenated with the original one.\nForward translation or self-learning (Zhang and Zong, 2016) provides synthetic parallel pairs, in which the synthetic target data are obtained by translating an additional source-language monolingual dataset with the baseline system.\nTagging aims at informing the NMT model which sentences are original and which are synthetic, as the augmented data may be of lesser quality (Caswell et al., 2019). For this purpose, a special token is added in the beginning of each synthetic source sentence in the training data.", "publication_ref": ["b33", "b53", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Semi-supervised NMT", "text": "To a certain extent, text-to-gloss translation can be regarded as a monolingual rephrasing task, as there is a large overlap in vocabulary of both sides. Thus, it triggers the assumption, that instead of generating synthetic data by models, we simply copy the monolingual data to both source and target side (Currey et al., 2017). This can be regarded as semi-supervised NMT, in which the model takes advantage of the concatenation of unlabeled monolingual data and labeled parallel data (Cheng et al., 2016). In this work, we do not delve into other potential effective factors of this method, e.g. size and domain of the monolingual data.", "publication_ref": ["b14", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Transfer learning", "text": "Transfer learning uses learned knowledge to improve related tasks (Pan and Yang, 2010), i.e., a parent model is pre-trained on a large corpus, used to initialize the parameters of the child model on a relatively small corpus. Zoph et al. (2016) first introduced the feasibility of transfer learning for NMT. We follow two approaches which differ in whether the child language pair (SL) is included during the parent model pre-training:\nModel fine-tuning refers to fine-tuning a pretrained model to train a child model. Although the pre-trained model usually contains a large vocabulary, it does not guarantee a full coverage of the child language pair. To alleviate this situation, the core operation of this approach is to modify the given vocabulary file manually. We tokenize the parallel SL dataset (i.e., the child language pair) with the source-side tokenizer of the pre-trained model. Then, we append the vocabulary of the SL dataset into the pre-trained vocabulary. Since the vocabulary of the fine-tuned model has to be the same size as the original one, we replace the most frequent vocabulary occurrences of the pre-trained vocabulary with entries from the SL vocabulary.\nOur method of fine-tuning by modifying the vocabulary is a simplification of the replacing algorithm used for Vocabulary Transformation (Kocmi and Bojar, 2020).\nWarm-start training addresses the problem of vocabulary mismatch between parent and child models by introducing a joint vocabulary (Nguyen and Chiang, 2017). In this case, a parent model is pre-trained, but the training data of the child language pair is included during the pre-training of the parent model (Neubig and Hu, 2018). When the pre-training converges, this model is fine-tuned by training only on the child language pair. In order to select which language pair should be chosen as a parent one, Neubig and Hu (2018) suggest that using resources from related languages helps in improving the effectiveness of transfer learning, as it benefits from a high probability of words or characters overlapping within the related languages. For this reason we will be using a parallel dataset for paraphrasing of the spoken language.", "publication_ref": ["b24", "b39", "b38", "b38"], "figure_ref": [], "table_ref": []}, {"heading": "Multilingual NMT", "text": "Multilingual NMT handles the simultaneous translation between more than one language through a single model. We suggest multilingual NMT, considering that the amount of parallel data for our intended language direction is small but there is a larger parallel corpus for another related language direction (Johnson et al., 2017). Here we follow the case of one-to-many translation, i.e., one source language to multiple target languages. Parallel corpora from the two language pairs are concatenated and a target-language-indicator token is added at the beginning of each source language sentence. A joint vocabulary across all the training data is built.\nIn our case, the first target language refers to the SL glosses and the second target language is another spoken language. Contrary to other multilingual NMT experiments, we only focus on the performance of the text-to-gloss direction. An example of the combined parallel set follows:\n\u2022 German-to-English: <2en> Wie hei\u00dft du? \u2192 What is your name?\n\u2022 German-to-DGSglosses: <2gloss> im s\u00fcden freundliches wetter \u2192 sued region besser", "publication_ref": ["b20"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation", "text": "Following most of the MT tasks, we use three automatic evaluation metrics including BLEU-4 (Papineni et al., 2002), ChrF (Popovi\u0107, 2015) and TER (Snover et al., 2006), with disabled internal tokenization, as suggested by Muller et al. (2022). Paired bootstrap resampling (Koehn, 2004) was performed to indicate the systems that are significantly better than the baseline, and the ones that are tied with the best-scoring system. In order to confirm our conclusions and because the reliability of these metrics has not been confirmed for SL glosses, we conduct human evaluation. Since performing human evaluation for all system requires a lot of effort, we only collect human evaluation for the translation outputs from the best-scoring model and the baseline of every corpus, testing the hypothesis that the best-scoring system is significantly better than the baseline. Significance testing between pairs of systems is based on a one-tailed t-test, with a confidence threshold of \u03b1 = 0.05. As a means of quantitative human evaluation, we use Direct Assessment (Graham et al., 2013). Alternative translations of the same source by different systems are displayed shuffled at the same screen. A signer scores each output of shuffled systems from 0 to 6 (similar to Kocmi et al., 2022). Outputs marked with 0 fail to translate any of the contents of the original sentence, whereas outputs marked with 6 show no significant mistakes in the translation.", "publication_ref": ["b44", "b56", "b25", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Datasets", "text": "We conduct our experiments on two parallel German SL (DGS) corpora containing gloss annotations.\nRWTH-PHOENIX-Weather 2014T (Camgoz et al., 2018), abbreviated as PHOENIX, is a parallel corpus of SL containing weather forecasts. The original language was German, translated into DGS by professional interpreters and then annotated with DGS glosses. We use the provided split of parallel train-, dev-and test-set with respective sizes of 7,096, 519 and 642 sentences.\nThe Public DGS Corpus  2 , further abbreviated as DGS corpus, contains conversations and narrations on topics culturally relevant to the deaf/Deaf community. The original language was DGS, which was then annotated with DGS glosses and German translation. We use the parallel corpus in plain text as extracted by Angelova et al. (2022) 3 , including the alignment of the DGS glosses to the German text by using the corresponding timestamps and prepending the gloss of the dominant hand to the non-dominant one, in case they co-occurred. We also follow the same data split into 54,325 training, 4,470 development and 5,113 test sentence pairs. Due to the big size of the test-set, for the human evaluation, we sample randomly 10% of the test sentences.\nThe DGS corpus gloss annotation (Konrad et al., 2022) includes suffixes to indicate different word variants, types, or groups. Muller et al. (2022) note that some annotation conventions may not be relevant to SLT and may make the problem unnecessarily harder. We confirmed this via our preliminary experiments (Appendix D), which yielded very low scores (\u223c1 BLEU) when generating suffixes and we decided to strip all suffixes, for the following reasons. In order to be able to see the improvements of our methods we needed more generous references. Secondly, a criterion was to preserve basic lexical and syntactic information. A signer, part of our group, reviewed several gloss examples and noticed that while the suffixes might indicate lexical and phonological variants, so do the corresponding words in the German text, and with having written language as source there was in theory no way to determine which variant was necessary (except training a system to learn from context which seemed excessive at this point). Finally, PHOENIX glosses had no suffixes whatsoever, so by stripping the suffixes, the automatic metrics between the two corpora are comparable. Further work should focus on the importance of the suffixes, the right granularity for every purpose and how to optimize their generation. An example of suffix stripping follows:\n\u2022 original: $INDEX1* SCH\u00d6N1A ALLE2B ICH1 NICHT1* $GEST-OFF\u2022 stripped: $INDEX SCH\u00d6N ALLE ICH NICHT $GEST- OFF\nNCSLGR is a very small American Sign Language (ASL) parallel corpus (Vogler and Neidle, 2012), which we split ourselves to 1500, 177 and 178 sentences for train, development and test set.\nOther corpora The German monolingual weather domain sentences (Angelova et al., 2022) and Europarl-v10 (Koehn, 2005) are used in data augmentation and semi-supervised NMT section (Sections 3.1 and 3.2) respectively. For training the parent model in transfer learning (Section 3.3), we use the parallel German paraphrasing corpus Tatoeba-Challenge (Tiedemann, 2020) for the main experiments in DGS and the synthetic text-to-gloss corpus ASLG-PC12 (Othman and Jemni, 2012) for the supplementary experiment in ASL. The German-English bilingual corpora News-commentary-v16 (Barrault et al., 2019) and Europarl-v10 are used in the section of Multilingual NMT (Section 3.4).\nWe report the statistics of vocabulary level and sentence lexical overlap of corpora with the custom split in Appendix F.", "publication_ref": ["b5", "b1", "b63", "b1", "b26", "b59", "b40", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "Data Preprocessing", "text": "For the data preprocessing, at source side, we perform lemmatization on both corpora and alphabet normalization specifically on the PHOENIX (the letters \u00fc, \u00f6, \u00e4, and \u00df in the glosses are prenormalized by dataset creators). We then apply Byte Pair Encoding (BPE; Sennrich et al., 2016b) to decompose the words and build vocabulary. In the end, we set the lemmatized+normalized sentences with lowercased glosses of PHOENIX and lemmatized sentences with generalized glosses of the DGS corpus to train the models. We present the relevant statistics in Appendixes A and B.", "publication_ref": ["b54"], "figure_ref": [], "table_ref": []}, {"heading": "Software", "text": "All software used is open source. MT models are trained with MarianNMT 1.11.0 (Junczys-Dowmunt et al., 2018). We also used Sentencepiece 0.1.97 (Kudo and Richardson, 2018), Mosesscripts (Koehn et al., 2007), Subword_nmt 0.3.8 (Sennrich et al., 2016b), Hanover Tagger Lemmatization library 1.0 (Wartena, 2019), Scipy library 1.9.3 for t-test (Virtanen et al., 2020), SacreBLEU 2.2 (Post, 2018) for the automatic metrics and Streamlit 1.17 for the evaluation interface. To avoid model overfitting, we use several techniques such as early stopping (Zhang and Yu, 2005) during the model training.", "publication_ref": ["b30", "b27", "b54", "b45"], "figure_ref": [], "table_ref": []}, {"heading": "Baselines", "text": "For the training hyperparameters, we start from the settings for a transformer (Vaswani et al., 2017) by the MarianNMT tutorial 4 . Specifically by baseline training, we take the advice of some paper that indicate in LRL MT scenarios with small data size, the model performance increases when the number of encoders/decoders are reduced compared to the original transformer architecture, e.g. one encoder and two decoders (Gu et al., 2018) and five encoders and five decoders (Chen et al., 2019;Araabi and Monz, 2020). After running extensive experiments with different combinations, which indicates we should reduce the encoder depth from 6 to 1 and the decoder depth from 6 to 2 to have the neural network fit better the small datasets. We present the baselines in Table 1. Our baseline models achieved a BLEU score of 22.78 on the PHOENIX dev set and 4.04 on the DGS dev set.", "publication_ref": ["b61", "b18", "b7", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Effect of monolingual dataset", "text": "We first investigate the effect of using the additional monolingual dataset.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data augmentation", "text": "Combining preprocessed data We collect different types of source text applied with different preprocessing methods of Section 4.2. For PHOENIX, we combine the original, the normalized, the lemmatized, and the lemmatized+normalized text with the copied target glosses. For DGS, we mix the original and lemmatized text with the corresponding target glosses into a new training dataset.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Back-translation", "text": "We first train simple gloss-totext translation models for both corpora and then they generate sets of new source sentences from the target-side glosses. The synthetic texts are paired appropriately and then mixed with the original dataset.\nForward translation For PHOENIX, we use a German weather-domain monolingual dataset with the size of 1,203 to get a set of new glosses. Towards DGS, as it is a multiple-domain corpus, we obtain the new glosses by translating its source sentences with the baseline system.\nWe summarize the detailed statistics of the augmented datasets in Appendix C.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Semi-supervised NMT", "text": "Here, we use the German monolingual dataset Europarl-v10 with a size of 2,107,971 sentences as auxiliary data. The monolingual data are copied to both the source and target side. To fit the neural network better with a larger training dataset, the encoder-depth is increased from 1 to 6, the decoderdepth from 2 to 6, the validation frequency from 500 to 5,000 and the max batch size from 64 to 1,000, as compared to the baseline. We build up a joint vocabulary of 32,000 entries after the corresponding BPE merge operations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Effect of bilingual dataset", "text": "Then we start investigating the impact of the additional bilingual dataset on the model performance.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Transfer learning", "text": "Model fine-tuning We take the German to English pre-trained model 5 from Opus (Tiedemann and Thottingal, 2020), whose vocabulary size is 65k. By applying the pre-trained tokenizer to both corpora, we get new vocabulary with size of 2,155 and 7,435 for PHOENIX and DGS corpus, respectively. We then crop the pre-trained vocabulary accordingly and merge the newly built vocabulary into it.", "publication_ref": ["b60"], "figure_ref": [], "table_ref": []}, {"heading": "Warm-start training", "text": "We select the Tatoeba challenge German paraphrasing dataset with a size of 4,574,760 as the parent language pair. In the first round of training, the training data contain German paraphrasing pairs and SL pairs. The parent model is trained using the Tatoeba challenge validation set. When it converges, we use this as pre-trained model to train with only the SL dataset. The child model is further trained using the SL development set for validation, until it converges too. We again build the joint vocabulary as in Section 4.5.2. During the two training phases, we reduce the validation frequency from 1,000 to 100 for a better observation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Multilingual NMT", "text": "We set up the identical source language in this part, i.e., German. Only one additional language is selected to train the multilingual NMT, i.e., English. We assume that a larger auxiliary dataset could be more helpful. Therefore, we set up two groups of sub-experiments with different sizes of auxiliary datasets in this section, i.e., a relatively small dataset New-commentary-v16 with the size of 398,981 (\"Multi\") and a larger one Europarl-v10 with the size of 1,828,521 (\"Multi-big\"). Vocabulary and hyperparameters follow those of Section 4.5.2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Effect of combining methods", "text": "We run the experiments independently and separately in Section 4.5 and Section 4.6. However, we cannot refuse the assumption that additional gain could be achieved by combining some or all of the best performing methods from above sections. Explicitly, we continue our experiments as following:\n1. Combine all the data augmentation techniques of Section 4.5.1 2. Tag the monolingual data in the semisupervised NMT setting of Section 4.5.2.\n3. Combine multilingual NMT setting of Section 4.6.2 with combined preprocessed data and back-translation, respectively.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "In this part, we will present the performance of the various methods on both SL datasets and offer some further analysis. Table 1: Automatic metric scores of extensive experimentation search of the two DGS corpora. We boldface all the values that are not statistically significantly different from the best value of each evaluation metric and underline the results that are statistically significantly higher than baseline at the 95% confidence level.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Automatic evaluation", "text": "The performance of the various experiments, as measured with automatic metrics can be seen in Table 1. Looking at the scores on the test sets, we can observe that:\n(1) Overall, the results on PHOENIX are better than on DGS corpus in all aspects. One of the reasons may be that DGS corpus is of broader domain and has a much bigger vocabulary. To support our assumption, we calculate the type-to-token ratio (Templin, 1957) for both corpora (PHOENIX: 2.2% and DGS corpus: 3.2%).\n(2) For PHOENIX, data augmentation has shown a significant improvement in comparison with the baseline, as measured by BLEU (+1.74) and TER (-1.59), although ChrF fails to measure a significant improvement. On the contrary, the performance on DGS corpus declines as compared to the baseline.\n(3) Incorporating the large-scale monolingual dataset, (semi-supervised NMT), could further improve the scores of translation systems for both SL datasets. Tagging here seems to be of big importance for PHOENIX (+1.5 BLEU).\n(4) Transfer learning incurs further improvement, with scores equal or better to the ones achieved with semi-supervised NMT. Here, each metric favors a different setting. ChrF indicates a significant improvement with fine tuning, TER prefers warm  start, whereas BLEU indicates only a very small difference between the two.\n(5) Multilingual NMT increases the automatic scores even further. The best scoring methods, favored by two automatic metrics each, and taking into consideration the significance tests, are (a) for PHOENIX the Multi with back-translation, Multibig and Multi-big with back-translation, and (b) for DGS corpus the Multi-big, the Multi-big with backtranslation, and the Multi-big with combined preprocessing.\nIn order to confirm the generalizability of our findings, we repeated the experiments with a very small corpus of the ASL, and the results are shown in Table 2. We observe that our best-scored method for the German SL (DGS) also gives the best performance for the ASL corpus, which is confirmed with two out of the automatic metrics.\nIn Table 4, we compare our best model to the approaches of recent work which have run experiments on PHOENIX text-to-gloss translation task. One can see that our best-scoring system performs 3.13 points BLEU higher than the closest result.", "publication_ref": ["b58"], "figure_ref": [], "table_ref": ["tab_2", "tab_5"]}, {"heading": "Quantitative Human Evaluation", "text": "As part of the human evaluation, an effort of approximately 40 hours for the PHOENIX test set and 20 hours for the DGS corpus was made. The results of the human evaluation (Table 3) confirm the basic hypothesis: that the best performing method of multi-NMT is statistically significantly better than the baseline. The density of the human evaluation scores of the two best scoring systems can be seen in Figure 3. One can see that more than half of the test-sentences of the best PHOENIX system are scored with a 5 or 6, whereas the corresponding percentage for the DGS corpus is only around 20%. Despite the extremely low automatic scores of the best model on the DGS corpus, it is promising that the human evaluator assigned the best score to 10% of the test sentences.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Conclusion", "text": "In this paper, we applied several techniques, commonly used in low resource MT scenarios, for MT from spoken language text to sign language glosses. We presented an extensive experimentation including data augmentation (combination of different pre-processing methods, back-and forward-translation), semi-supervised NMT, transfer learning with two different methods and multilingual NMT with different data sizes. The experiments were based on the two known natural datasets including gloss annotation, the RWTH-PHOENIX-Weather 2014T dataset and the Public DGS Corpus. Automatic metrics indicate significant improvement on the evaluation scores for both datasets when using most of the above methods, whereas the best results are achieved via a Multilingual NMT model (6.18 and 2.65 BLEU against the baseline respectively). Our best system outperforms all other state-of-the-art systems from previous work that report on the same test-set. Additionally, the best setting is confirmed with an experiment run on a corpus of the ASL. The conclusions are supported by human evaluation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "\u2022 These methods have been performed on three SL datasets (Section 4.1) as these were the only publicly available natural SL corpora found to contain gloss annotations. Therefore, the generalization of these conclusions to other SLs is limited and should be confirmed upon availability of suitable data.\n\u2022 SL glosses are not an accurate representation of SLs and critical information can be missing, causing further limitations to the usability of the results (e.g. for SL video production) and the reliability of the automatic evaluation. However, as explained in the Introduction (Section 1), we think that given the current resource limitation, investigation of MT on glosses may be a research step to provide indications for other SL representations.\n\u2022 As explained in Section 4.1, stripping the gloss suffixes from the DGS corpus was done in order to allow more clear comparisons with the automatic evaluation metrics, given the low scores incurred when the suffixes were there. It is clear that suffix stripping limits the   representational capacity of the glosses. As stated, further work should focus on the importance of the DGS gloss suffixes, the right granularity for every purpose and how to optimize their generation from MT.\n\u2022 The original language direction of the DGS corpus was opposite to the one that we run our training and evaluation on. This is known to create translationese artifacts. Similar concerns have been expressed regarding the cleanliness of the PHOENIX corpus (Muller et al., 2022). Finally, whereas in MT of spoken language text, test-sets have been manually curated by professional translators for this purpose, in our experiments we use data splits, whose test set quality may not have been confirmed.\n\u2022 The human evaluation part (Section 3.5) was performed with one signer, but evaluation by more people and coverage of the Deaf community would be ideal. Additionally, due to the high effort required, we could only validate the hypothesis that the best system is significantly better than the baseline. Given more evaluation capacity one could verify whether there is a significantly perceived quality difference between methods that were scored closely by the automatic metrics (e.g. transfer learning and multilingual MT).\n\u2022 The automatic metrics used have been designed for evaluating the textual output for MT of spoken languages. Whether they are applicable and reliable with regards to SLs and particularly to SL glosses has not been sufficiently analyzed and should be considered for further work. Any interpretation of the scores should consider this limitation.\n\u2022 Despite the big progress regarding the model trained on the DGS-corpus (Section 5.1), the BLEU scores achieved indicate very low performance, if judged from the experience on the automatic scores for text translation for spoken languages. Whereas we tried to get some information about this by looking at the distribution of scores, further investigation on whether such a system is usable with regards to particular use cases (interpretation, text-tovideo) is needed.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ethical considerations", "text": "In our work, we present experiments on the German Sign Language (DGS) that are part of a broader research aiming to provide equal access to language technology for sign language users. Nevertheless, the fact that the majority of the researchers in NLP are hearing people entails the risk of developments that are not in accordance with the will of the respective communities, and therefore it is required that every research step takes them in constant consideration. In our broader research we have included members of the Deaf/deaf and hard of hearing communities as part of the research team, consultants and participants in user studies and workshops and we have been in co-operation with related unions and communication centers.\nThe fact that we are performing experiments on glosses, known to be inferior to the full linguistic capacity of the sign languages, should be seen as a methodological tool to aid further research.\nThe Public DGS corpus is provided under a limited license for linguistic research (Schulder and Hanke, 2022), prohibiting any further commercial usage. Any further usage of relevant artifacts from our work should respect the license of the original corpus. Removal of information that names or uniquely identifies individual people or offensive content was not deemed necessary. In the Public DGS corpus, participants provided consensus, whereas the content was carefully curated. The PHOENIX corpus does not pose any relevant risk because the content (weather forecasts) does not include any personal information. All other datasets used have been published with open or public domain licenses. Since our work does not use videos of SLs, there should be no ethical concerns regarding processing of human faces.        For sentence lexical overlap within the DGS corpus there are no sentences with 100% lexical overlap, 6.45% of the test sentences had approximately 90% overlap with the train set and 1.51% of the test sentences had approximately 80% overlap with the train set, whereas the sentence-level overlap between the dev set and the train set is similar. For the NCSLGR test set, the overlaps are 0, 12.23% and 4,26% and for dev set 0, 14.44% and 5.88% respectively. The sentence-level lexical overlaps of our custom splits are lower or comparable to the ones of the official PHOENIX corpus. These are 0, 11.06% and 16.04% in the test set and 0, 7.32% and 15.42% in the dev set respectively.", "publication_ref": ["b52"], "figure_ref": [], "table_ref": []}, {"heading": "G Statistics on computational experiments", "text": "Experiments were run in a GPU computational cluster on an Nvidia RTXA-6000, using 1 GPU, 2 CPUs and 50 GB of RAM and summing approximately 100 hours of computational time.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "H Human Evaluation", "text": "The human evaluator and consultant is a user of the German Sign Language (DGS) and an employed member of our research team, having consented on the use of their evaluation effort for this research. The interface used for the human evaluation can be seen in Figure 4.\nThe evaluation rating was that outputs marked with 0 failed to translate any of the contents of the original sentence, whereas outputs marked with 6 show no significant mistakes in the translation. Insignificant mistakes or minor issues dropped the rating from 6 to a 5 or 4, some correctly translated words or phrases pushed it up from 0 to 1 or 2 or, if some information was conveyed but it was missing significant interrelations, to 3. ", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Acknowledgements", "text": "The research reported in this paper was supported by BMBF (German Federal Ministry of Education and Research) via the project SocialWear (grant no. 01IW20002). We would like to thank Mathias M\u00fcller and Amit Moryossef for their advice with regards to DGS glosses.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Harry Walsh, Ben Saunders, and Richard Bowden. 2022.\nChanging the representation: Examining language representation for neural sign language production. ", "publication_ref": ["b51"], "figure_ref": [], "table_ref": []}, {"heading": "Appendix A Statistics of sign language datasets", "text": "We present the statistical analysis of all sign language corpora in Table 5 and Table 6. Out-of-Vocabulary (OOV) are the words that only appear in development or test set and singletons are the least frequent words appearing only once.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Effect of source-side preprocessing", "text": "Previous work on gloss-to-text translation (Moryossef et al., 2021) suggested the use of lemmatization of the spoken language words as part of their data augmentation pipelines. Lemmatization of spoken language words in the text-to-gloss is justified by the fact that they contain inflection (e.g. for nouns, or verb conjugation), something that does not exist in SL glosses (Moryossef et al., 2021). Therefore we ran preliminary experiments on PHOENIX with lemmatization in order to determine the baseline settings. The results of these experiments on PHOENIX appear in Table 7. One can see that lemmatization incurs considerable automatic metric improvements, with an improvement of around 0.9 BLEU score on test set.\nIn Table 8, we demonstrate the statistics of the source side after the data preprocessing. We can see that the vocabulary size has dropped by around 23% and 25% after data preprocessing, respectively.", "publication_ref": ["b35", "b35"], "figure_ref": [], "table_ref": []}, {"heading": "C Statistics of augmented datasets", "text": "As an appendix for Section 4.5.1, we present here the statistics of the datasets through our data augmentation methods in Table 9.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D Effect of DGS-Corpus gloss suffixes to the automatic evaluation", "text": "We train these two multilingual NMT systems on the DGS corpus under the same configurations but they are evaluated against two different types of reference translations: the original DGS glosses and the glosses with stripped suffixes. In Table 10 we can observe the results, indicating that generating glosses with correct suffices is a much harder problem and that current automatic metrics are not optimized to measure that.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E Statistics of additional datasets", "text": "The statistics of additional datasets used for data augmentation (Section 3.1), semi-supervised NMT (Section 3.2), warm-start of transfer learning (Section 3.3) and multilingual NMT (Section 3.4) are shown in Table 11. A1. Did you describe the limitations of your work?\nLimitations have been described as separate section after the Conclusions, as required by the ACL instructions.\nA2. Did you discuss any potential risks of your work?\nRisks with regards to technological developments and their acceptance by communities using the Sign Languages are described in the Section of the \"Ethical Considerations\".\nA3. Do the abstract and introduction summarize the paper's main claims? and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)? Appendix D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\nWe didn't create any data. Evaluator consented on the use of their evaluation effort.\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board? Not applicable. There was no data collection D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\nWe only had one human evaluator. Further demographic and geographic characteristics are not relevant to the experiment, given the current state of research, and would unnecessarily reveal personal information of the evaluator.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Sign language gloss translation using deep learning models", "journal": "International Journal of Advanced Computer Science and Applications", "year": "2021", "authors": "Mohamed Amin; Hesahm Hefny; Ammar Mohammed"}, {"ref_id": "b1", "title": "Using neural machine translation methods for sign language translation", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Galina Angelova; Eleftherios Avramidis; Sebastian M\u00f6ller"}, {"ref_id": "b2", "title": "Optimizing transformer for low-resource neural machine translation", "journal": "", "year": "2020", "authors": "Ali Araabi; Christof Monz"}, {"ref_id": "b3", "title": "Findings of the 2019 conference on machine translation (WMT19)", "journal": "", "year": "2019", "authors": "Lo\u00efc Barrault; Ond\u0159ej Bojar; Marta R Costa-Juss\u00e0; Christian Federmann; Mark Fishel; Yvette Graham; Barry Haddow; Matthias Huck; Philipp Koehn; Shervin Malmasi; Christof Monz; Mathias M\u00fcller"}, {"ref_id": "b4", "title": "Subunets: End-to-end hand shape and continuous sign language recognition", "journal": "", "year": "2017", "authors": "Simon Necati Cihan Camgoz; Oscar Hadfield; Richard Koller;  Bowden"}, {"ref_id": "b5", "title": "Neural sign language translation", "journal": "", "year": "2018", "authors": "Simon Necati Cihan Camgoz; Oscar Hadfield; Hermann Koller; Richard Ney;  Bowden"}, {"ref_id": "b6", "title": "Tagged back-translation", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Isaac Caswell; Ciprian Chelba; David Grangier"}, {"ref_id": "b7", "title": "Facebook AI's WAT19 Myanmar-English translation task submission", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Peng-Jen Chen; Jiajun Shen; Matthew Le; Vishrav Chaudhary; Ahmed El-Kishky; Guillaume Wenzek; Myle Ott; Marc'aurelio Ranzato"}, {"ref_id": "b8", "title": "Two-stream network for sign language recognition and translation", "journal": "", "year": "2023", "authors": "Yutong Chen; Ronglai Zuo; Fangyun Wei; Yu Wu; Shujie Liu; Brian Mak"}, {"ref_id": "b9", "title": "Semi-supervised", "journal": "", "year": "2016", "authors": "Yong Cheng; Wei Xu; Zhongjun He; Wei He; Hua Wu; Maosong Sun; Yang Liu"}, {"ref_id": "b10", "title": "Annual Meeting of the Association for Computational Linguistics", "journal": "Association for Computational Linguistics", "year": "", "authors": ""}, {"ref_id": "b11", "title": "Translating Spanish into Spanish Sign Language: Combining rules and data-driven approaches", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Luis Chiruzzo; Euan Mcgill; Santiago Egea-G\u00f3mez; Horacio Saggion"}, {"ref_id": "b12", "title": "A translation studies approach to glossing using ELAN", "journal": "International Journal of Interpreter Education", "year": "2012", "authors": "Judith Collins; Granville Tate; Paul Hann"}, {"ref_id": "b13", "title": "Mieke van Herreweghe, and Joni Dambre. 2022. Machine translation from signed to spoken languages: State of the art and challenges", "journal": "ArXiv", "year": "", "authors": "Dimitar Mathieu De Coster;  Shterionov"}, {"ref_id": "b14", "title": "Copied monolingual data improves low-resource neural machine translation", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Anna Currey; Antonio Valerio Miceli; Kenneth Barone;  Heafield"}, {"ref_id": "b15", "title": "Linguistically enhanced text to sign gloss machine translation", "journal": "Springer-Verlag", "year": "2022-06-15", "authors": "Luis Santiago Egea G\u00f3mez; Euan Chiruzzo; Horacio Mcgill;  Saggion"}, {"ref_id": "b16", "title": "Syntax-aware transformers for neural machine translation: The case of text to sign gloss translation", "journal": "", "year": "2021", "authors": "Euan Santiago Egea G\u00f3mez;  Mcgill"}, {"ref_id": "b17", "title": "Continuous measurement scales in human evaluation of machine translation", "journal": "", "year": "2013", "authors": "Yvette Graham; Timothy Baldwin; Alistair Moffat; Justin Zobel"}, {"ref_id": "b18", "title": "Universal neural machine translation for extremely low resource languages", "journal": "", "year": "2018", "authors": "Jiatao Gu; Hany Hassan; Jacob Devlin; O K Victor;  Li"}, {"ref_id": "b19", "title": "Extending the Public DGS Corpus in size and depth", "journal": "", "year": "2020", "authors": "Thomas Hanke; Marc Schulder; Reiner Konrad; Elena Jahn"}, {"ref_id": "b20", "title": "Google's multilingual neural machine translation system: Enabling zero-shot translation", "journal": "Transactions of the Association for Computational Linguistics", "year": "2017", "authors": "Melvin Johnson; Mike Schuster; Quoc V Le; Maxim Krikun; Yonghui Wu; Zhifeng Chen; Nikhil Thorat; Fernanda Vi\u00e9gas; Martin Wattenberg; Greg Corrado; Macduff Hughes; Jeffrey Dean"}, {"ref_id": "b21", "title": "", "journal": "", "year": "", "authors": "Marcin Junczys-Dowmunt; Roman Grundkiewicz; Tomasz Dwojak; Hieu Hoang; Kenneth Heafield"}, {"ref_id": "b22", "title": "Marian: Fast neural machine translation in C++", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Alexandra Martins;  Birch"}, {"ref_id": "b23", "title": "Toshiaki Nakazawa, Michal Nov\u00e1k, Martin Popel, and Maja Popovi\u0107. 2022. Findings of the 2022 conference on machine translation (WMT22)", "journal": "", "year": "", "authors": "Tom Kocmi; Rachel Bawden; Ond\u0159ej Bojar; Anton Dvorkovich; Christian Federmann; Mark Fishel; Thamme Gowda; Yvette Graham; Roman Grundkiewicz; Barry Haddow; Rebecca Knowles; Philipp Koehn; Christof Monz; Makoto Morishita; Masaaki Nagata"}, {"ref_id": "b24", "title": "Efficiently reusing old models across languages via transfer learning", "journal": "European Association for Machine Translation", "year": "2020", "authors": "Tom Kocmi; Ond\u0159ej Bojar"}, {"ref_id": "b25", "title": "Statistical significance tests for machine translation evaluation", "journal": "Association for Computational Linguistics", "year": "2004", "authors": "Philipp Koehn"}, {"ref_id": "b26", "title": "Europarl: A parallel corpus for statistical machine translation", "journal": "", "year": "2005", "authors": "Philipp Koehn"}, {"ref_id": "b27", "title": "Moses: Open source toolkit for statistical machine translation", "journal": "", "year": "2007", "authors": "Philipp Koehn; Hieu Hoang; Alexandra Birch; Chris Callison-Burch; Marcello Federico; Nicola Bertoldi; Brooke Cowan; Wade Shen"}, {"ref_id": "b28", "title": "MEINE DGS -annotiert. \u00d6ffentliches Korpus der Deutschen Geb\u00e4rdensprache, 3. Release / MY DGSannotated", "journal": "", "year": "2020", "authors": "Reiner Konrad; Thomas Hanke; Gabriele Langer; Dolly Blanck; Julian Bleicken; Ilona Hofmann; Olga Jeziorski; Lutz K\u00f6nig; Susanne K\u00f6nig; Rie Nishio; Anja Regen; Uta Salden; Sven Wagner; Satu Worseck; Oliver B\u00f6se; Elena Jahn; Marc Schulder"}, {"ref_id": "b29", "title": "Rie Nishio, and Anja Regen. 2022. \u00d6ffentliches DGS-Korpus: Annotationskonventionen / Public DGS Corpus: Annotation conventions", "journal": "", "year": "2018-01", "authors": "Reiner Konrad; Thomas Hanke; Gabriele Langer; Susanne K\u00f6nig; Lutz K\u00f6nig"}, {"ref_id": "b30", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Taku Kudo; John Richardson"}, {"ref_id": "b31", "title": "Compiling a Basic Vocabulary for German Sign Language (DGS) -lexicographic issues with a focus on word senses", "journal": "", "year": "2014", "authors": "Gabriele Langer; Susanne K\u00f6nig; Silke Matthes"}, {"ref_id": "b32", "title": "Transcribing natural languages for the deaf via neural editing programs", "journal": "", "year": "2021", "authors": "Dongxu Li; Chenchen Xu; Liu Liu; Yiran Zhong; Rongzhao Wang; Lars Petersson; Hongdong Li"}, {"ref_id": "b33", "title": "Understanding data augmentation in neural machine translation: Two perspectives towards generalization", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Guanlin Li; Lemao Liu; Guoping Huang; Conghui Zhu; Tiejun Zhao"}, {"ref_id": "b34", "title": "Methodology for developing an advanced communications system for the deaf in a new domain. Knowledge-Based Systems", "journal": "", "year": "2014", "authors": "V L\u00f3pez-Lude\u00f1a; C Gonz\u00e1lez-Morcillo; J C L\u00f3pez; E Ferreiro; J Ferreiros; R San-Segundo"}, {"ref_id": "b35", "title": "Data augmentation for sign language gloss translation", "journal": "", "year": "2021", "authors": "Amit Moryossef; Kayo Yin; Graham Neubig; Yoav Goldberg"}, {"ref_id": "b36", "title": "From dictionary to corpus and back again -linking heterogeneous language resources for DGS", "journal": "", "year": "2020", "authors": "Anke M\u00fcller; Thomas Hanke; Reiner Konrad; Gabriele Langer; Sabrina W\u00e4hl"}, {"ref_id": "b37", "title": "Amit Moryossef, Annette Rios Gonzales, and Sarah Ebling. 2022. Considerations for meaningful sign language machine translation based on glosses", "journal": "ArXiv", "year": "", "authors": "Mathias Muller; Zifan Jiang"}, {"ref_id": "b38", "title": "Rapid adaptation of neural machine translation to new languages", "journal": "", "year": "2018", "authors": "Graham Neubig; Junjie Hu"}, {"ref_id": "b39", "title": "Transfer learning across low-resource, related languages for neural machine translation", "journal": "Short Papers", "year": "2017", "authors": "Q Toan; David Nguyen;  Chiang"}, {"ref_id": "b40", "title": "Englishasl gloss parallel corpus 2012: Aslg-pc12", "journal": "", "year": "2012", "authors": "Achraf Othman; Mohamed Jemni"}, {"ref_id": "b41", "title": "A survey on transfer learning", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": "2010", "authors": "Qiang Sinno Jialin Pan;  Yang"}, {"ref_id": "b42", "title": "Bleu: a method for automatic evaluation of machine translation", "journal": "Association for Computational Linguistics", "year": "2002", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"ref_id": "b43", "title": "Representing signed languages in written form: questions that need to be posed", "journal": "ELRA", "year": "2006", "authors": "Elena Pizzuto; Paolo Rossini; Tommaso Russo"}, {"ref_id": "b44", "title": "chrF: character n-gram F-score for automatic MT evaluation", "journal": "Association for Computational Linguistics", "year": "2015", "authors": "Maja Popovi\u0107"}, {"ref_id": "b45", "title": "A call for clarity in reporting BLEU scores", "journal": "", "year": "2018", "authors": "Matt Post"}, {"ref_id": "b46", "title": "Neural machine translation for low-resource languages: A survey", "journal": "ACM Computing Surveys", "year": "2021", "authors": "Surangika Ranathunga; Annie En-Shiun; Marjana Prifti Lee; Ravi Skenduli; Mehreen Shekhar; Rishemjit Alam;  Kaur"}, {"ref_id": "b47", "title": "Sign language production: A review", "journal": "", "year": "2021", "authors": "Razieh Rastgoo; Kourosh Kiani; Sergio Escalera; M Sabokrou"}, {"ref_id": "b48", "title": "Design, development and field evaluation of a spanish into sign language translation system", "journal": "", "year": "2012", "authors": "Rub\u00e9n San-Segundo; Juan Montero; Ricardo Cordoba; V Sama; Fernando Fern\u00e1ndez-Mart\u00ednez; D' Luis; Ver\u00f3nica Haro; D L\u00f3pez-Lude\u00f1a; A S\u00e1nchez;  Garc\u00eda"}, {"ref_id": "b49", "title": "Adversarial training for multi-channel sign language production. ArXiv, abs", "journal": "", "year": "2008", "authors": "Ben Saunders; R Necati Cihan Camg\u00f6z;  Bowden"}, {"ref_id": "b50", "title": "Progressive transformers for end-to-end sign language production. ArXiv, abs", "journal": "", "year": "2004", "authors": "Ben Saunders; R Necati Cihan Camg\u00f6z;  Bowden"}, {"ref_id": "b51", "title": "Signing at scale: Learning to co-articulate signs for large-scale photo-realistic sign language production", "journal": "", "year": "2022", "authors": "Ben Saunders; Richard Necati Cihan Camgoz;  Bowden"}, {"ref_id": "b52", "title": "How to be FAIR when you CARE: The DGS Corpus as a case study of open science resources for minority languages", "journal": "", "year": "2022", "authors": "Marc Schulder; Thomas Hanke"}, {"ref_id": "b53", "title": "Improving neural machine translation models with monolingual data", "journal": "Long Papers", "year": "2016", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"ref_id": "b54", "title": "Neural machine translation of rare words with subword units", "journal": "Long Papers", "year": "2016", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"ref_id": "b55", "title": "Revisiting lowresource neural machine translation: A case study", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Rico Sennrich; Biao Zhang"}, {"ref_id": "b56", "title": "A study of translation edit rate with targeted human annotation", "journal": "", "year": "2006", "authors": "Matthew Snover; Bonnie Dorr; Rich Schwartz; Linnea Micciulla; John Makhoul"}, {"ref_id": "b57", "title": "Text2sign: Towards sign language production using neural machine translation and generative adversarial networks", "journal": "International Journal of Computer Vision", "year": "2020", "authors": "Stephanie Stoll; Necati Camgoz; Simon Hadfield; Richard Bowden"}, {"ref_id": "b58", "title": "Certain Language Skills in Children: Their Development and Interrelationships", "journal": "University of Minnesota Press", "year": "1957", "authors": "Mildred C Templin"}, {"ref_id": "b59", "title": "The tatoeba translation challenge -realistic data sets for low resource and multilingual MT", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "J\u00f6rg Tiedemann"}, {"ref_id": "b60", "title": "OPUS-MT -building open translation services for the world", "journal": "", "year": "2020", "authors": "J\u00f6rg Tiedemann; Santhosh Thottingal"}, {"ref_id": "b61", "title": "", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam M Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Lukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b62", "title": "", "journal": "Nature Methods", "year": "", "authors": "Pauli Virtanen; Ralf Gommers; Travis E Oliphant; Matt Haberland; Tyler Reddy; David Cournapeau; Evgeni Burovski; Pearu Peterson; Warren Weckesser; Jonathan Bright; J St\u00e9fan; Matthew Van Der Walt; Joshua Brett; K Jarrod Wilson; Nikolay Millman;  Mayorov; R J Andrew; Eric Nelson; Robert Jones; Eric Kern; C J Larson; \u0130lhan Carey; Yu Polat; Eric W Feng; Jake Moore; Denis Vanderplas; Josef Laxalde; Robert Perktold;  Cimrman"}, {"ref_id": "b63", "title": "A new web interface to facilitate access to corpora: development of the ASLLRP data access interface", "journal": "", "year": "2012", "authors": "Christian Vogler; Carol Neidle"}, {"ref_id": "b64", "title": "error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc", "journal": "", "year": "", "authors": ""}, {"ref_id": "b65", "title": "If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used", "journal": "", "year": "", "authors": " Nltk;  Spacy;  Rouge"}, {"ref_id": "b66", "title": "crowdworkers) or research with human participants? Sections 3.5", "journal": "", "year": "", "authors": ""}, {"ref_id": "b67", "title": "Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators", "journal": "", "year": "", "authors": " D1"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Scheme of experiments.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 4 :4Figure 4: The interface used for human evaluation.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Automatic metric scores for NCSLGR corpus.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "System comparison based on the human evaluation. The bold-faced systems are significantly better than the respective baselines. Density of human evaluation scores for the two best-scoring systems.", "figure_data": "(a) PHOENIX Multi+back(b) DGS Multi-bigFigure 3: ApproachDev BLEU\u2191 BLEU\u2191 TestAmin et al. (2021) Egea G\u00f3mez et al. (2021) \u2020 Stoll et al. (2020) Zhang and Duh (2021) Li et al. (2021) Saunders et al. (2020b) Saunders et al. (2022) Egea G\u00f3mez et al. (2022) Walsh et al. (2022) Our PHOENIX Multi+back--16.34 --20.23 21.93 -25.09 28.4110.42 13.13 15.26 16.43 18.89 19.10 20.08 20.57 23.19 26.32"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Results comparison with recent work. ( \u2020) We compute the BLEU by ourselves, as the authors of paper only present the BLEU score in character level.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177-180, Prague, Czech Republic. Association for Computational Linguistics.Tot. words 99, 081 6, 820 7, 816 55, 247 3, 748 4, 264 472, 609 36, 629 44, 452 301, 772 21, 715 28, 405   ", "figure_data": "TrainText DevPHOENIX Test TrainGlosses DevTestTrainText DevGeneralized DGS Test TrainGlosses DevTestSentences Vocabulary7, 096 2, 887519 951642 1, 0017, 096 1, 085519 393642 41154, 325 20, 8684, 470 4, 6175, 113 4, 99254, 325 19, 5214, 470 4, 8945, 113 5, 688Tot. OOVs Singletons-1, 07757 -60 --35514 -19 --9, 946971 -1, 080 --6, 286614 -752 -"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Statistics of both corpora.", "figure_data": "TrainText DevNCSLGR Test TrainGlosses DevTest1, 500 Vocabulary 2, 796 Sentences Tot. words 13, 904 1, 860 1, 832 11, 064 1, 471 1, 449 177 178 1, 500 177 178 745 754 2, 287 662 639 -219 230 -210 192 Tot. OOVs Singletons 1, 665 --1, 209 --"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Statistics of NCSLGR.", "figure_data": "PreprocessingDev BLEU ChrF TER BLEU ChrF TER TestNo lemmatization With lemmatization27.90 28.4157.50 49.92 57.54 49.3925.44 26.3256.30 51.76 56.70 51.15"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Effect of lemmatization on preliminary experiments of the PHOENIX corpus.", "figure_data": "TrainText DevPHOENIX Preprocessed text Test Train Dev TestTrainText DevGeneralized DGS Preprocessed text Test Train Dev Test7, 096 Vocabulary 2, 887 Sentences Tot. words 99, 081 6, 820 7, 816 99, 081 6, 820 7, 816 472, 609 36, 629 44, 452 472, 609 36, 629 44, 452 519 642 7, 096 519 642 54, 325 4, 470 5, 113 54, 325 4, 470 5, 113 951 1, 001 2, 216 793 836 20, 868 4, 617 4, 992 15, 170 3, 497 3, 791 -57 60 -39 38 -971 1, 080 -691 773 Tot. OOVs Singletons 1, 077 --765 --9, 946 --6, 929 --"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Statistics of preprocessed corpora.", "figure_data": "PHOENIX Text Authentic Synthetic Authentic Synthetic Authentic Synthetic Authentic Synthetic PHOENIX Glosses DGS Text DGS GlossesOriginal Combining Back-translation Forward-translation7, 096 7, 096 7, 096 7, 096\u2212 3  *  7096 7, 096 1, 0237096 4  *  7096 2  *  7, 096 7, 096\u2212 \u2212 \u2212 1, 02354, 325 54, 325 54, 325 2  *  54, 325\u2212 54, 325 54, 325 \u221254, 325 2  *  54, 325 2  *  54, 325 54, 325\u2212 \u2212 \u2212 54, 325"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Statistics of augmented datasets", "figure_data": "DGS gloss referenceDev BLEU-4 ChrF TER BLEU-4 ChrF TER TestOriginal_DGS Generalized_DGS1.21 6.0632.67 92.24 35.18 74.510.81 5.3231.34 91.78 33.55 74.71"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Results comparison with different DGS gloss references. 12537", "figure_data": "DatasetLanguage (pair) #MonolingualGerman weather domain sentences Europarl-v10de de1, 203 2, 107, 971BilingualTatoeba-Challenge News-commentary-v16 Europarl-v10 ASLG-PC12de-de de-en de-en en-ASL4, 574, 760 398, 981 1, 828, 521 87, 710"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Auxiliary language datasets overview.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u2022 original: $INDEX1* SCH\u00d6N1A ALLE2B ICH1 NICHT1* $GEST-OFF\u2022 stripped: $INDEX SCH\u00d6N ALLE ICH NICHT $GEST- OFF", "formula_coordinates": [5.0, 71.7, 285.0, 218.92, 46.05]}], "doi": "10.14569/IJACSA.2021.0121178"}