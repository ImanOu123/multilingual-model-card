{"title": "Specializing Word Embeddings (for Parsing) by Information Bottleneck", "authors": "Lisa Xiang;  Li; Jason Eisner", "pub_date": "", "abstract": "Pre-trained word embeddings like ELMo and BERT contain rich syntactic and semantic information, resulting in state-of-the-art performance on various tasks. We propose a very fast variational information bottleneck (VIB) method to nonlinearly compress these embeddings, keeping only the information that helps a discriminative parser. We compress each word embedding to either a discrete tag or a continuous vector. In the discrete version, our automatically compressed tags form an alternative tag set: we show experimentally that our tags capture most of the information in traditional POS tag annotations, but our tag sequences can be parsed more accurately at the same level of tag granularity. In the continuous version, we show experimentally that moderately compressing the word embeddings by our method yields a more accurate parser in 8 of 9 languages, unlike simple dimensionality reduction.", "sections": [{"heading": "Introduction", "text": "Word embedding systems like BERT and ELMo use spelling and context to obtain contextual embeddings of word tokens. These systems are trained on large corpora in a task-independent way. The resulting embeddings have proved to then be useful for both syntactic and semantic tasks, with different layers of ELMo or BERT being somewhat specialized to different kinds of tasks (Peters et al., 2018b;Goldberg, 2019). State-of-the-art performance on many NLP tasks can be obtained by fine-tuning, i.e., back-propagating task loss all the way back into the embedding function (Peters et al., 2018a;Devlin et al., 2018).\nIn this paper, we explore what task-specific information appears in the embeddings before finetuning takes place. We focus on the task of dependency parsing, but our method can be easily extended to other syntactic or semantic tasks. Our method compresses the embeddings by extracting just their syntactic properties-specifically, the information needed to reconstruct parse trees (because that is our task). Our nonlinear, stochastic compression function is explicitly trained by variational information bottleneck (VIB) to forget task-irrelevant information. This is reminiscent of canonical correspondence analysis (Anderson, 2003), a method for reducing the dimensionality of an input vector so that it remains predictive of an output vector, although we are predicting an output tree instead. However, VIB goes beyond mere dimensionality reduction to a fixed lower dimensionality, since it also avoids unnecessary use of the dimensions that are available in the compressed representation, blurring unneeded capacity via randomness. The effective number of dimensions may therefore vary from token to token. For example, a parser may be content to know about an adjective token only that it is adjectival, whereas to find the dependents of a verb token, it may need to know the verb's number and transitivity, and to attach a preposition token, it may need to know the identity of the preposition.\nWe try compressing to both discrete and continuous task-specific representations. Discrete representations yield an interpretable clustering of words. We also extend information bottleneck to allow us to control the contextual specificity of the token embeddings, making them more like type embeddings.\nThis specialization method is complementary to the previous fine-tuning approach. Fine-tuning introduces new information into word embeddings by backpropagating the loss, whereas the VIB method learns to exploit the existing information found by the ELMo or BERT language model. VIB also has less capacity and less danger of overfitting, since it fits fewer parameters than fine-tuning (which in the case of BERT has the freedom to adjust the embeddings of all words and word pieces, even those that are rare in the supervised fine-tuning data). VIB is also very fast to train on a single GPU.\nWe discover that our syntactically specialized embeddings are predictive of the gold POS tags in the setting of few-shot-learning, validating the intuition that a POS tag summarizes a word token's syntactic properties. However, our representations are tuned explicitly for discriminative parsing, so they prove to be even more useful for this task than POS tags, even at the same level of granularity. They are also more useful than the uncompressed ELMo representations, when it comes to generalizing to test data. (The first comparison uses discrete tags, and the second uses continuous tags.)", "publication_ref": ["b22", "b9", "b21", "b4", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Background: Information Bottleneck", "text": "The information bottleneck (IB) method originated in information theory and has been adopted by the machine learning community as a training objective (Tishby et al., 2000) and a theoretical framework for analyzing deep neural networks (Tishby and Zaslavsky, 2015).\nLet X represent an \"input\" random variable such as a sentence, and Y represent a correlated \"output\" random variable such as a parse. Suppose we know the joint distribution p(X, Y ). (In practice, we will use the empirical distribution over a sample of (x, y) pairs.) Our goal is to learn a stochastic map p \u2713 (t | x) from X to some compressed representation T, which in our setting will be something like a tag sequence. IB seeks to minimize\nL I B = I(Y ; T) + \u2022 I(X; T)(1)\nwhere I(\u2022; \u2022) is the mutual information. 1 A low loss means that T does not retain very much information about X (the second term), while still retaining enough information to predict Y . 2 The balance between the two MI terms is controlled by a Lagrange multiplier . By increasing , we increase the pressure to keep I(X; T) small, which \"narrows the bottleneck\" by favoring compression over predictive accuracy I(Y ; T). Regarding as a Lagrange multiplier, we see that the goal of IB is to maximize the predictive power of T subject to some constraint on the amount of information about X that T carries. If the map from X to T were deterministic, then it could lose information only by being non-injective: the traditional example is dimensionality reduction, as in the encoder of an encoder-decoder neural net. But IB works even if T can take values throughout a high-dimensional space, because the randomness in p \u2713 (t | x) means that T is noisy in a way that wipes out information about X. Using a high-dimensional space is desirable because it permits the amount of effective dimensionality reduction to vary, with T perhaps retaining much more information about some x values than others, as long as the average retained information I(X; T) is small.", "publication_ref": ["b28", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Formal Model", "text": "In this paper, we extend the original IB objective (1) and add terms I(T i ; X |X i ) to control the contextsensitivity of the extracted tags. Here T i is the tag associated with the ith word, X i is the ELMo token embedding of the ith word, andX i is the same word's ELMo type embedding (before context is incorporated).\nL I B = I(Y ; T)+ I(X; T)+ n X i=1 I(T i ; X |X i ) (2)\nIn this section, we will explain the motivation for the additional term and how to efficiently estimate variational bounds on all terms (lower bound for I(Y ; T) and upper bound for the rest). 3\n1 In our IB notation, larger means more compression. Note that there is another version of IB that puts as the coefficient in front of I(Y ; T): L I B =\n\u2022 I(Y ; T) + I(X; T) The two versions are equivalent.\n2 Since T is a stochastic function of X with no access to Y , it obviously cannot convey more information about Y than the uncompressed input X does. As a result, Y is independent of T given X, as in the graphical model T ! X ! Y .\n3 Traditional Shannon entropy H(\u2022) is defined on discrete variables. In the case of continuous variables, we interpret H We instantiate the variational IB (VIB) estimation method (Alemi et al., 2016) on our dependency parsing task, as illustrated in Figure 1. We compress a sentence's word embeddings X i into continuous vector-valued tags or discrete tags T i (\"encoding\") such that the tag sequence T retains maximum ability to predict the dependency parse Y (\"decoding\"). Our chosen architecture compresses each X i independently using the same stochastic, information-losing transformation.\nThe IB method introduces the new random variable T, the tag sequence that compresses X, by defining the conditional distribution p \u2713 (t | x). In our setting, p \u2713 is a stochastic tagger, for which we will adopt a parametric form ( \u00a73.1 below). Its parameters \u2713 are chosen to minimize the IB objective (2). By IB's independence assumption, 2 the joint probability can be factored as\np \u2713 (x, y, t) = p(x) \u2022 p(y | x) \u2022 p \u2713 (t | x). 3.1 I(X; T) -the Token Encoder p \u2713 (t | x) Under this distribution, I(X; T) def = Ex,t [log p \u2713 (t |x) p \u2713 (t) ] = Ex [E t\u21e0p \u2713 (t |x) [log p \u2713 (t |x) p \u2713 (t) ]\n]. Making this term small yields a representation T that, on average, retains little information about X. The outer expectation is over the true distribution of sentences x; we use an empirical estimate, averaging over the unparsed sentences in a dependency treebank. To estimate the inner expectation, we could sample, drawing taggings t from p \u2713 (t | x).\nWe must also compute the quantities within the inner brackets. The p \u2713 (t | x) term is defined by our parametric form. The troublesome term is p \u2713 (t) = Ex 0 [p \u2713 (t | x 0 )], since even estimating it from a treebank requires an inner loop over treebank sentences x 0 . To avoid this, variational IB replaces p \u2713 (t) with some variational distribution r (t). This can only increase our objective function, since the difference between the variational and original versions of this term is a KL divergence and hence non-negative:\nupper bound z }| { E x [ E t\u21e0p \u2713 (t |x) [log p \u2713 (t|x) r (t) ]] I(X;T ) z }| { E x [ E t\u21e0p \u2713 (t |x) [log p \u2713 (t | x) p \u2713 (t) ]] = E x [KL(p \u2713 (t) || r (t))] 0\nto instead denote differential entropy (which would be 1 for discrete variables). Scaling a continuous random variable affects its differential entropy-but not its mutual information with another random variable, which is what we use here.\nThus, the variational version (the first term above) is indeed an upper bound for I(X; T) (the second term above). We will minimize this upper bound by adjusting not only \u2713 but also , thus making the bound as tight as possible given \u2713. Also we will no longer need to sample t for the inner expectation of the upper bound, E t\u21e0p \u2713 (t |x) [log p \u2713 (t |x) r (t) ], because this expectation equals KL[p \u2713 (t | x) || r (t)], and we will define the parametric p \u2713 and r so that this KL divergence can be computed exactly: see \u00a74.", "publication_ref": ["b0"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Two Token Encoder Architectures", "text": "We choose to define\np \u2713 (t | x) = Q n i=1 p \u2713 (t i | x i ).\nThat is, our stochastic encoder will compress each word x i individually (although x i is itself a representation that depends on context): see Figure 1. We make this choice not for computational reasons-our method would remain tractable even without this-but because our goal in this paper is to find the syntactic information in each individual ELMo token embedding (a goal we will further pursue in \u00a73.3 below).\nTo obtain continuous tags, define p \u2713 (t i | x i ) such that t i 2 R d is Gaussian-distributed with mean vector and diagonal covariance matrix computed from the ELMo word vector x i via a feedforward neural network with 2d outputs and no transfer function at the output layer. To ensure positive semidefiniteness of the diagonal covariance matrix, we squared the latter d outputs to obtain the diagonal entries. 4 Alternatively, to obtain discrete tags, define\np \u2713 (t i | x i ) such that t i 2 {1, .\n. . , k} follows a softmax distribution, where the k softmax parameters are similarly computed by a feedforward network with k outputs and no transfer function at the output layer.\nWe similarly define r (t) = Q n i=1 r (t i ), where directly specifies the 2d or k values corresponding to the output layer above (since there is no input x i to condition on).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "I(T", "text": "i ; X |X i ) -the Type Encoder s \u21e0 (t i |x i )\nWhile the IB objective (1) asks each tag t i to be informative about the parse Y , we were concerned that it might not be interpretable as a tag of word i specifically. Given ELMo or any other black-box conversion of a length-n sentence to a sequence of contextual vectors x 1 , . . . , x n , it is possible that x i contains not only information about word i but also information describing word i + 1, say, or the syntactic constructions in the vicinity of word i. Thus, while p \u2713 (t i | x i ) might extract some information from x i that is very useful for parsing, there is no guarantee that this information came from word i and not its neighbors. Although we do want tag t i to consider context-e.g., to distinguish between noun and verb uses of word i-we want \"most\" of t i 's information to come from word i itself. Specifically, it should come from ELMo's level-0 embedding of word i, denoted byx i -a word type embedding that does not depend on context.\nTo penalize T i for capturing \"too much\" contextual information, our modified objective (2) adds a penalty term \u2022 I(T i ; X |X i ), which measures the amount of information about T i given by the sentence X as a whole, beyond what is given byX i :\nI(T i ; X |X i ) def = Ex [E t i \u21e0p \u2713 (t i |x) [log p \u2713 (t i |x) p \u2713 (t i |x i ) ]\n]. Setting > 0 will reduce this contextual information.\nIn practice, we found that I(T i ; X |X i ) was small even when = 0, on the order of 3.5 nats whereas I(T i ; X) was 50 nats. In other words, the tags extracted by the classical method were already fairly local, so increasing above 0 had little qualitative effect. Still, might be important when applying our method to ELMo's competitors such as BERT.\nWe can derive an upper bound on I(T i ; X |X i ) by approximating the conditional distribution p \u2713 (t i |\nx i ) with a variational distribution s \u21e0 (t i |x i ), similar to \u00a73.1.\nupper bound z }| { E x [ E t i \u21e0p \u2713 (t i |x) [log p \u2713 (t i |x) s \u21e0 (t i |x i ) ]] I(T i ;X |X i ) z }| { E x [ E t i \u21e0p \u2713 (t i |x) [log p \u2713 (t i |x) p \u2713 (t i |x i ) ]] = E x [KL(p \u2713 (t i |x i ) || s \u21e0 (t i |x i ))] 0\nWe replace it in (2) with this upper bound, which is equal to Ex [\nP n i=1 KL[p \u2713 (t i |x) || s \u21e0 (t i |x i )]\n]. The formal presentation above does not assume the specific factored model that we adopted in \u00a73.2. When we adopt that model, p \u2713 (t i | x) above reduces to p \u2713 (t i | x i )-but our method in this section still has an effect, because x i still reflects the context of the full sentence whereasx i does not.\nType Encoder Architectures Notice that s \u21e0 (t i | x i ) may be regarded as a type encoder, with parameters \u21e0 that are distinct from the parameters \u2713 of our token encoder p \u2713 (t i | x i ). Given a choice of neural architecture for p \u2713 (t i | x i ) (see \u00a73.2), we always use the same architecture for s \u21e0 (t i |x i ), except that p \u2713 takes a token vector as input whereas s \u21e0 takes a context-independent type vector. s \u21e0 is not used at test time, but only as part of our training objective.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "I(Y ;", "text": "T) -the Decoder q (y | t) Finally, I(Y ; T) def = Ey,t\u21e0p \u2713 [log p \u2713 (y |t) p(y) ].\nThe p(y) can be omitted during optimization as it does not depend on \u2713. Thus, making I(Y ; T) large tries to obtain a high log-probability p \u2713 (y | t) for the true parse y when reconstructing it from t alone.\nBut how do we compute p \u2713 (y | t)? This quantity effectively marginalizes over possible sentences x that could have explained t. Recall that p \u2713 is a joint distribution over x, y, t: see just above \u00a73.1. So\np \u2713 (y | t) def = P x p \u2713 (x,y,t) P\nx, y 0 p \u2713 (x,y 0 ,t) . To estimate these sums accurately, we would have to identify the sentences x that are most consistent with the tagging t (that is, p(x) \u2022 p \u2713 (t|x) is large): these contribute the largest summands, but might not appear in any corpus.\nTo avoid this, we replace p \u2713 (y | t) with a variational approximation q (y | t) in our formula for I(Y ; T). Here q (\u2022 | \u2022) is a tractable conditional distribution, and may be regarded as a stochastic parser that runs on a compressed tag sequence t instead of a word embedding sequence x. This modified version of I(Y ; T) forms a lower bound on I(Y ; T), for any value of the variational parameters , since the difference between them is a KL divergence and hence positive:\nI(Y;T ) z }| { E y,t\u21e0p \u2713 [log p \u2713 (y |t) p(y) ] lower bound z }| { E y,t\u21e0p \u2713 [log q (y |t) p(y) ] = E t\u21e0p \u2713 [KL(p \u2713 (y | t) || q (y | t))] 0\nWe will maximize this lower bound of I(Y ; T) with respect to both \u2713 and . For any given \u2713, the optimal minimizes the expected KL divergence, meaning that q approximates p \u2713 well. More precisely, we again drop p(y) as constant and then maximize a sampling-based estimate of Ey,t\u21e0p \u2713 [log q (y|t)]. To sample y, t from the joint p \u2713 (x, y, t) we must first sample x, so we rewrite as Ex,y [E t\u21e0p \u2713 (t |x) [log q (y|t)]]. The outer expectation Ex,y is estimated as usual over a training treebank. The expectation E t\u21e0p \u2713 (t |x) recognizes that t is stochastic, and again we estimate it by sampling. In short, when t is a stochastic compression of a treebank sentence x, we would like our variational parser on average to assign high log-probability q (y | t) to its treebank parse y.\nDecoder Architecture We use the deep biaffine dependency parser (Dozat and Manning, 2016) as our variational distribution q (y | t), which functions as the decoder. This parser uses a Bi-LSTM to extract features from compressed tags or vectors and assign scores to each tree edge, setting q (y | t) proportional to the exp of the total score of all edges in y. During IB training, the code 5 computes only an approximation to q (y|t) for the gold tree y (although in principle, it could have computed the exact normalizing constant in polytime with Tutte's matrix-tree theorem (Smith and Smith, 2007;Koo et al., 2007;McDonald and Satta, 2007)). When we test the parser, the code does exactly find argmax y q (y | t) via the directed spanning tree algorithm of Edmonds (1966).", "publication_ref": ["b5", "b26", "b15", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Training and Inference", "text": "With the approximations in \u00a73, our final minimization objective is this upper bound on (2):\nE x,y h E t\u21e0p \u2713 (t |x) [ log q (y|t)] + KL(p \u2713 (t|x)||r (t)) + n X i=1 KL(p \u2713 (t i | x) || s \u21e0 (t i |x i )) i (3)\nWe apply stochastic gradient descent to optimize this objective. To get a stochastic estimate of the objective, we first sample some (x, y) from the treebank. We then have many expectations over t \u21e0 p \u2713 (t | x), including the KL terms. We could estimate these by sampling t from the token encoder p \u2713 (t | x) and then evaluating all q , p \u2713 , r , and s \u21e0 probabilities. However, in fact we use the sampled t only to estimate the first expectation (by computing the decoder probability q (y | t) of the gold tree y); we can compute the KL terms exactly by exploiting the structure of our distributions. The structure of p \u2713 and r means that the first KL term decomposes into\nP n i=1 KL(p \u2713 (t i |x i )||r (t i ))\n. All KL terms are now between either two Gaussian distributions over a continuous tagset 6 or two categorical distributions over a small discrete tagset. 7 To compute the stochastic gradient, we run backpropagation on this computation. We must apply the reparametrization trick to backpropagate 5 We use the implementation from AllenNLP library (Gardner et al., 2017).  1: Statistics of the datasets used in this paper. \"Treebank\" is the treebank identifier in UD, \"#Token\" is the number of tokens in the treebank, \"H(A)\" is the entropy of a gold POS tag (in nats), and \"H(A |X)\" is the conditional entropy of a gold POS tag conditioned on a word type (in nats).\n6 KL(N 0 || N 1 ) = 1 2 (tr(\u2303 1 1 \u2303 0 )+(\u00b5 1 \u00b5 0 ) T \u2303 1 1 (\u00b5 1 \u00b5 0 ) d + log( det(\u2303 1 ) det(\u2303 0 ) ) 7 KL(p \u2713 (t i |x i )||r (t i )) = P k t i =1 p \u2713 (t i | x i ) log p \u2713 (t i |x i ) r (t i )\nthrough the step that sampled t. This finds the gradient of parameters that derive t from a random variate z, while holding z itself fixed. For continuous t, we use the reparametrization trick for multivariate Gaussians (Rezende et al., 2014). For discrete t, we use the Gumbel-softmax variant (Jang et al., 2016;Maddison et al., 2017).\nTo evaluate our trained model's ability to parse a sentence x from compressed tags, we obtain a parse as argmax y q (y | t), where t \u21e0 p \u2713 (\u2022 | x) is a single sample. A better parser would instead estimate argmax y Et [q (y | t)] where Et averages over many samples t, but this is computationally hard.", "publication_ref": ["b8", "b11", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup", "text": "Data Throughout \u00a7 \u00a76-7, we will examine our compressed tags on a subset of Universal Dependencies (Nivre et al., 2018), or UD, a collection of dependency treebanks across 76 languages using the same POS tags and dependency labels. We experiment on Arabic, Hindi, English, French, Spanish, Portuguese, Russian, Italian, and Chinese (Table 1)-languages with different syntactic properties like word order. We use only the sentences with length \uf8ff 30. For each sentence, x is obtained by running the standard pre-trained ELMo on the UD token sequence (although UD's tokenization may not perfectly match that of ELMo's training data), and y is the labeled UD dependency parse without any part-of-speech (POS) tags. Thus, our tags t are tuned to predict only the dependency relations in UD, and not the gold POS tags a also in UD.\nPretrained Word Embeddings For English, we used the pre-trained English ELMo model from the AllenNLP library (Gardner et al., 2017). For the other 8 languages, we used the pre-trained models from Che et al. (2018). Recall that ELMo has two layers of bidirectional LSTM (layer 1 and 2) built upon a context-independent character CNN (layer 0). We use either layer 1 or 2 as the input (x i ) to our token encoder p \u2713 . Layer 0 is the input (x i ) to our type encoder s \u21e0 . Each encoder network ( \u00a7 \u00a73.2-3.3) has a single hidden layer with a tanh transfer function, which has 2d hidden units (typically 128 or 512) for continuous encodings and 512 hidden units for discrete encodings.\nOptimization We optimize with Adam (Kingma and Ba, 2014), a variant of stochastic gradient descent. We alternate between improving the model p \u2713 (t|x) on even epochs and the variational distributions q (y|t), r (t), s \u21e0 (t i |x i ) on odd epochs.\nWe train for 50 epochs with minibatches of size 20 and L 2 regularization. The learning rate and the regularization coefficients are tuned on dev data for each language separately. For each training sentence, we average over 5 i.i.d. samples of T to reduce the variance of the stochastic gradient. The initial parameters \u2713, , , \u21e0 are all drawn from N(0, I). We experiment with different dimensionalities d 2 {5, 32, 256, 512} for the continuous tags, and different cardinalities k 2 {32, 64, 128} for the discrete tag set. We also tried different values , 2 {10 6 , 10 5 , \u2022 \u2022 \u2022 , 10 1 } of the compression tradeoff parameter. We use temperature annealing when sampling from the Gumbel-softmax distribution ( \u00a74). At training epoch i, we use temperature \u2327 i , where \u2327 1 = 5 and \u2327 i+1 = max(0.5, e \u2327 i ). We set the annealing rate = 0.1. During testing, we use \u2327 = 0, which gives exact softmax sampling.", "publication_ref": ["b19", "b8", "b3", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Scientific Evaluation", "text": "In this section, we study what information about words is retained by our automatically constructed tagging schemes. First, we show the relationship between I(Y ; T) and I(X; T) on English as we reduce to capture more information in our tags. 8 Second, across 9 languages, we study how our automatic tags correlate with gold part-of-speech tags (and in English, with other syntactic properties), while suppressing information about semantic properties. We also show how decreasing gradually refines the automatic discrete tag set, giving intuitive fine-grained clusters of English words. The \"dim\" in the legends means the dimensionality of the continuous tag vector or the cardinality of the discrete tag set. On the left, we plot predictiveness I(Y ; T) versus I(X; T) as we lower multiplicatively from 10 1 to 10 6 on a log-scale. On the right, we alter the y-axis to show the labeled attachment score (LAS) of 1-best dependency parsing. All mutual information and entropy values in this paper are reported in nats per token. Furthermore, the mutual information values that we report are actually our variational upper bounds, as described in \u00a73. The reason that I(X; T) is so large for continuous tags is that it is differential mutual information (see footnote 3). Additional tradeoff curves w.r.t. I(T i ; X |X i ) are in Appendix B.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Tradeoff Curves", "text": "As we lower to retain more information about X, both I(X; T) and I(Y ; T) rise, as shown in Figure 2. There are diminishing returns: after some point, the additional information retained in T does not contribute much to predicting Y . Also noteworthy is that at each level of I(X, T), very low-dimensional tags (d = 5) perform on par with high-dimensional ones (d = 256). (Note that the high-dimensional stochastic tags will be noisier to keep the same I(X, T).) The low-dimensional tags allow far faster CPU parsing. This indicates that VIB can achieve strong practical task-specific compression.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Learned Tags vs. Gold POS Tags", "text": "We investigate how our automatic tag T i correlates with the gold POS tag A i provided by UD. Continuous Version We use t-SNE (van der Maaten and Hinton, 2008) to visualize our compressed continuous tags on held-out test data, coloring each token in Figure 3 according to its gold POS tag. (Similar plots for the discrete tags are in Figure 6 in the appendix.) In Figure 3, the first figure shows the original uncompressed level-1 ELMo embeddings of the tokens in test data. In the two-dimensional visualization, the POS tags are vaguely clustered but the boundaries merge together and some tags are diffuse. The second figure is when = 10 3 (moderate compression): our compressed embeddings show clear clusters that correspond well to gold POS tags. Note that the gold POS tags were not used in training either ELMo or our method. The third figure is when = 1 (too much compression), when POS information is largely lost. An interesting observation is that the purple NOUN and blue PROPN distributions overlap in the middle distribution, meaning that it was unnecessary to distinguish common nouns from proper nouns for purposes of our parsing task. 9\nDiscrete Version We also quantify how well our specialized discrete tags capture the traditional POS categories, by investigating I(A; T). This can be written as H(A) H(A | T). Similarly to \u00a73.4, our probability distribution has the form\np \u2713 (x, a, t) = p(x, a) \u2022 p \u2713 (t | x), leading us to write H(A | T) \uf8ff Ex,a [E t\u21e0p \u2713 (t |x) [ log q(a | t)]] where q(a | t) = Q i q(a i | t i\n) is a variational distribution that we train to minimize this upper bound. This is equivalent to training q(a | t) by maximum conditional likelihood. In effect, we are doing transfer learning, fixing our trained IB encoder (p \u2713 ) and now using it to predict A instead of Y , but otherwise 9 Both can serve as arguments of verbs and prepositions. Both can be modified by determiners and adjectives, giving rise to proper NPs like \"The Daily Tribune.\" following \u00a73.4. We similarly upper-bound H(A) by assuming a model q 0 (a) = Q i q 0 (a i ) and estimating q 0 as the empirical distribution over training tags. Having trained q and q 0 on training data, we estimate H(A | T) and H(A) using the same upperbound formulas on our test data.\nWe experiment on all 9 languages, taking T i at the moderate compression level = 0.001, k = 64. As Figure 4 shows, averaging over the 9 languages, the reconstruction retains 71% of POS information (and as high as 80% on Spanish and French). We can conclude that the information encoded in the specialized tags correlates with the gold POS tags, but does not perfectly predict the POS.\nThe graph in Figure 4 shows a \"U-shaped\" curve, with the best overall error rate at = 0.01. That is, moderate compression of ELMo embeddings helps for predicting POS tags. Too much compression squeezes out POS-related information, while too little compression allows the tagger to overfit the training data, harming generalization to test data. We will see the same pattern for parsing in \u00a77.\nSyntactic Features As a quick check, we determine that our tags also make syntactic distinctions beyond those that are recognized by the UD POS tag set, such as tense, number, and transitivity. See Appendix D for graphs. For example, even with moderate compression, we achieve 0.87 classification accuracy in distinguishing between transitive and intransitive English verbs, given only tag t i .\nStem When we compress ELMo embeddings to k discrete tags, the semantic information must be squeezed out because k is small. But what about the continuous case? In order to verify that semantic information is excluded, we train a classifier that predicts the stem of word token i from its mean tag vector E [T i ]. We expect \"player\" and \"buyer\" to have similar compressed vectors, because they share syntactic roles, but we should fail  to predict that they have different stems \"play\" and \"buy.\" The classifier is a feedforward neural network with tanh activation function, and the last layer is a softmax over the stem vocabulary. In the English treebank, we take the word lemma in UD treebank and use the NLTK library (Bird et al., 2009) to stem each lemma token. Our result (Appendix E in the appendix) suggests that more compression destroys stem information, as hoped. With light compression, the error rate on stem prediction can be below 15%. With moderate compression = 0.01, the error rate is 89% for ELMo layer 2 and 66% for ELMo layer 1. Other languages show the same pattern, as shown in Appendix E in the appendix. Thus, moderate and heavy compression indeed squeeze out semantic information.", "publication_ref": ["b30", "b2"], "figure_ref": ["fig_3", "fig_3"], "table_ref": []}, {"heading": "Annealing of Discrete Tags", "text": "Deterministic annealing (Rose, 1998;Friedman et al., 2001) is a method that gradually decreases during training of IB. Each token i has a stochastic distribution over the possible tags {1, . . . , k}. This can be regarded as a soft clustering where each token is fractionally associated with each of the k clusters. With high , the optimal solution turns out to assign to all tokens an identical distribution over clusters, for a mutual information of 0. Since all clusters then have the same membership, this is equivalent to having a single cluster. As we gradually reduce , the cluster eventually splits. Further reduction of leads to recursive splitting, yielding a hierarchical clustering of tokens (Appendix A).\nWe apply deterministic annealing to the English dataset, and the resulting hierarchical structure reflects properties of English syntax. At the top of the hierarchy, the model places nouns, adjectives, adverbs, and verbs in different clusters. At lower levels, the anaphors (\"yourself,\" \"herself\" . . . ), possessive pronouns (\"his,\" \"my,\" \"their\" . . . ), accusativecase pronouns (\"them,\" \"me,\" \"him,\" \"myself\" . . . ), and nominative-case pronouns (\"I,\" \"they,\" \"we\" . . . ) each form a cluster, as do the wh-words (\"why,\" \"how,\" \"which,\" \"who,\" \"what,\" . . . ).", "publication_ref": ["b25", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Engineering Evaluation", "text": "As we noted in \u00a71, learning how to compress ELMo's tags for a given task is a fast alternative to fine-tuning all the ELMo parameters. We find that indeed, training a compression method to keep only the relevant information does improve our generalization performance on the parsing task.\nWe compare 6 different token representations according to the test accuracy of a dependency parser trained to use them. The same training data is used to jointly train the parser and the token encoder that produces the parser's input representations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Continuous tags:", "text": "Iden is an baseline model that leaves the ELMo embeddings uncompressed, so d = 1024.\nPCA is a baseline that simply uses Principal Components Analysis to reduce the dimensionality to d = 256. Again, this is not task-specific. MLP is another deterministic baseline that uses a multi-layer perceptron (as in Dozat and Manning (2016)) to reduce the dimensionality to d = 256 in a task-specific and nonlinear way. This is identical to our continuous VIB method except that the variance of the output Gaussians is fixed to 0, so that the d dimensions are fully informative.\nVIBc uses our stochastic encoder, still with d = 256. The average amount of stochastic noise is controlled by , which is tuned per-language on dev data.", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}, {"heading": "Discrete tags:", "text": "POS is a baseline that uses the k \uf8ff 17 gold POS tags from the UD dataset.\nVIBd is our stochastic method with k = 64 tags. To compare fairly with POS, we pick a value for each language such that H(T\ni | X i ) \u21e1 H(A i | X i ).\nRuntime. Our VIB approach is quite fast. With minibatching on a single GPU, it is able to train on 10,000 sentences in 100 seconds, per epoch.  Analysis. Table 2 shows the test accuracies of these parsers, using the standard training/development/test split for each UD language.\nIn the continuous case, the VIB representation outperforms all three baselines in 8 of 9 languages, and is not significantly worse in the 9th language (Hindi). In short, our VIB joint training generalizes better to test data. This is because the training objective (2) includes terms that focus on the parsing task and also regularize the representations.\nIn the discrete case, the VIB representation outperforms gold POS tags (at the same level of granularity) in 6 of 9 languages, and of the other 3, it is not significantly worse in 2. This suggests that our learned discrete tag set could be an improved alternative to gold POS tags (cf. Klein and Manning, 2003) when a discrete tag set is needed for speed.", "publication_ref": ["b14"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Related Work", "text": "Much recent NLP literature examines syntactic information encoded by deep models (Linzen et al., 2016) and more specifically, by powerful unsupervised word embeddings. Hewitt and Manning (2019) learn a linear projection from the embedding space to predict the distance between two words in a parse tree. Peters et al. (2018b) and Goldberg (2019) assess the ability of BERT and ELMo directly on syntactic NLP tasks. Tenney et al. (2019) extract information from the contextual embeddings by self-attention pooling within a span of word embeddings.\nThe IB framework was first used in NLP to cluster distributionally similar words (Pereira et al., 1993). In cognitive science, it has been used to argue that color-naming systems across languages are nearly optimal (Zaslavsky et al., 2018). In machine learning, IB provides an information-theoretic perspective to explain the performance of deep neural networks (Tishby and Zaslavsky, 2015).\nThe VIB method makes use of variational upper and lower bounds on mutual information. An alternative lower bound was proposed by Poole et al. (2019), who found it to work better empirically.", "publication_ref": ["b16", "b10", "b22", "b9", "b20", "b31", "b29", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion and Future Work", "text": "In this paper, we have proposed two ways to syntactically compress ELMo word token embeddings, using variational information bottleneck. We automatically induce stochastic discrete tags that correlate with gold POS tags but are as good or better for parsing. We also induce stochastic continuous token embeddings (each is a Gaussian distribution over R d ) that forget non-syntactic information captured by ELMo. These stochastic vectors yield improved parsing results, in a way that simpler dimensionality reduction methods do not. They also transfer to the problem of predicting gold POS tags, which were not used in training.\nOne could apply the same training method to compress the ELMo or BERT token sequence x for other tasks. All that is required is a modelspecific decoder q (y | t). For example, in the case of sentiment analysis, the approach should preserve only sentiment information, discarding most of the syntax. One possibility that does not require supervised data is to create artificial tasks, such as reproducing the input sentence or predicting missing parts of the input (such as affixes and function words). In this case, the latent representations would be essentially generative, as in the variational autoencoder (Kingma and Welling, 2013).", "publication_ref": ["b13"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "This work was supported by the National Science Foundation under Grant No. 1718846 and by a Provost's Undergraduate Research Award to the first author. The Maryland Advanced Research Computing Center provided computing facilities. We thank the anonymous reviewers and Hongyuan Mei for helpful comments.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Deep variational information bottleneck", "journal": "", "year": "2016", "authors": "Alexander A Alemi; Ian Fischer; Joshua V Dillon; Kevin Murphy"}, {"ref_id": "b1", "title": "An Introduction to Multivariate Statistical Analysis", "journal": "Wiley", "year": "2003", "authors": "T W Anderson"}, {"ref_id": "b2", "title": "Natural Language Processing with Python", "journal": "Reilly Media, Inc", "year": "2009", "authors": "Steven Bird; Ewan Klein; Edward Loper"}, {"ref_id": "b3", "title": "Towards better UD parsing: Deep contextualized word embeddings, ensemble, and treebank concatenation", "journal": "", "year": "2018", "authors": "Wanxiang Che; Yijia Liu; Yuxuan Wang; Bo Zheng; Ting Liu"}, {"ref_id": "b4", "title": "BERT: pre-training of deep bidirectional transformers for language understanding", "journal": "CoRR", "year": "2018", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b5", "title": "Deep biaffine attention for neural dependency parsing", "journal": "CoRR", "year": "2016", "authors": "Timothy Dozat; Christopher D Manning"}, {"ref_id": "b6", "title": "Optimum Branchings", "journal": "", "year": "1966", "authors": ""}, {"ref_id": "b7", "title": "Multivariate information bottleneck", "journal": "Morgan Kaufmann Publishers Inc", "year": "2001", "authors": "Nir Friedman; Ori Mosenzon; Noam Slonim; Naftali Tishby"}, {"ref_id": "b8", "title": "AllenNLP: A Deep Semantic Natural Language Processing Platform", "journal": "", "year": "2017", "authors": "Matt Gardner; Joel Grus; Mark Neumann; Oyvind Tafjord; Pradeep Dasigi; Nelson F Liu; Matthew Peters; Michael Schmitz; Luke S Zettlemoyer"}, {"ref_id": "b9", "title": "Assessing BERT's syntactic abilities. CoRR, abs", "journal": "", "year": "1901", "authors": "Yoav Goldberg"}, {"ref_id": "b10", "title": "A structural probe for finding syntax in word representations", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "John Hewitt; Christopher D Manning"}, {"ref_id": "b11", "title": "Categorical reparameterization with Gumbel-softmax", "journal": "", "year": "2016", "authors": "Eric Jang; Shixiang Gu; Ben Poole"}, {"ref_id": "b12", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "Diederik Kingma; Jimmy Ba"}, {"ref_id": "b13", "title": "Autoencoding variational Bayes", "journal": "", "year": "2013", "authors": "P Diederik; Max Kingma;  Welling"}, {"ref_id": "b14", "title": "Accurate unlexicalized parsing", "journal": "", "year": "2003", "authors": "D Klein; C D Manning"}, {"ref_id": "b15", "title": "Structured prediction models via the matrix-tree theorem", "journal": "", "year": "2007", "authors": "Terry Koo; Amir Globerson; Xavier Carreras P\u00e9rez; Michael Collins"}, {"ref_id": "b16", "title": "Assessing the ability of LSTMs to learn syntax-sensitive dependencies", "journal": "Transactions of the Association for Computational Linguistics", "year": "2016", "authors": "Tal Linzen; Emmanuel Dupoux; Yoav Goldberg"}, {"ref_id": "b17", "title": "The concrete distribution: A continuous relaxation of discrete random variables", "journal": "", "year": "2017", "authors": "Chris J Maddison; Andriy Mnih; Yee Whye Teh"}, {"ref_id": "b18", "title": "On the complexity of non-projective data-driven dependency parsing", "journal": "Association for Computational Linguistics", "year": "2007", "authors": "Ryan Mcdonald; Giorgio Satta"}, {"ref_id": "b19", "title": "Universal dependencies 2.3. LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics (\u00daFAL", "journal": "", "year": "2018", "authors": "Joakim Nivre"}, {"ref_id": "b20", "title": "Distributional clustering of English words", "journal": "", "year": "1993", "authors": "Fernando Pereira; Naftali Tishby; Lillian Lee"}, {"ref_id": "b21", "title": "Deep contextualized word representations", "journal": "Long Papers", "year": "2018", "authors": "Matthew Peters; Mark Neumann; Mohit Iyyer; Matt Gardner; Christopher Clark; Kenton Lee; Luke Zettlemoyer"}, {"ref_id": "b22", "title": "Dissecting contextual word embeddings: Architecture and representation", "journal": "", "year": "2018", "authors": "Matthew E Peters; Mark Neumann; Luke Zettlemoyer; Wen-Tau Yih"}, {"ref_id": "b23", "title": "On variational bounds of mutual information", "journal": "CoRR", "year": "2019", "authors": "Ben Poole; Sherjil Ozair ; Alexander; A Alemi; George Tucker"}, {"ref_id": "b24", "title": "Stochastic backpropagation and approximate inference in deep generative models", "journal": "", "year": "2014", "authors": "Danilo Jimenez Rezende; Shakir Mohamed; Daan Wierstra"}, {"ref_id": "b25", "title": "Deterministic annealing for clustering, compression, classification, regression, and related optimization problems", "journal": "Proceedings of the IEEE", "year": "1998", "authors": "Kenneth Rose"}, {"ref_id": "b26", "title": "Probabilistic models of nonprojective dependency trees", "journal": "", "year": "2007", "authors": "A David; Noah A Smith;  Smith"}, {"ref_id": "b27", "title": "", "journal": "", "year": "", "authors": "Ian Tenney; Patrick Xia; Berlin Chen; Alex Wang; Adam Poliak; R Thomas Mccoy; Najoung Kim; Benjamin Van Durme; Samuel R Bowman; Dipanjan Das"}, {"ref_id": "b28", "title": "The information bottleneck method", "journal": "", "year": "2000", "authors": "Naftali Tishby; Fernando C Pereira; William Bialek"}, {"ref_id": "b29", "title": "Deep learning and the information bottleneck principle", "journal": "", "year": "2015", "authors": "Naftali Tishby; Noga Zaslavsky"}, {"ref_id": "b30", "title": "Visualizing high-dimensional data using t-SNE", "journal": "Journal of Machine Learning Research", "year": "2008", "authors": "L J P Van Der Maaten; G E Hinton"}, {"ref_id": "b31", "title": "Efficient human-like semantic representations via the information bottleneck principle", "journal": "CoRR", "year": "2018", "authors": "Noga Zaslavsky; Charles Kemp; Terry Regier; Naftali Tishby"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure1: Our instantiation of the information bottleneck, with bottleneck variable T. A jagged arrow indicates a stochastic mapping, i.e. the jagged arrow points from the parameters of a distribution to a sample drawn from that distribution.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Compression-prediction tradeoff curves of VIB in our dependency parsing setting. The upper figures use discrete tags, while the lower figures use continuous tags. The dashed lines are for test data, and the solid lines for training data.The \"dim\" in the legends means the dimensionality of the continuous tag vector or the cardinality of the discrete tag set. On the left, we plot predictiveness I(Y ; T) versus I(X; T) as we lower multiplicatively from 10 1 to 10 6 on a log-scale. On the right, we alter the y-axis to show the labeled attachment score (LAS) of 1-best dependency parsing. All mutual information and entropy values in this paper are reported in nats per token. Furthermore, the mutual information values that we report are actually our variational upper bounds, as described in \u00a73. The reason that", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3: t-SNE visualization of VIB model (d = 256) on the projected space of the continuous tags. Each marker in the figure represents a word token, colored by its gold POS tag. This series of figures (from left to right) shows a progression from no compression to moderate compression and to too-much compression.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure4: Graph at left: I(A; T) vs. I(X; T) in English (in units of nats per token). Table at right: how well the discrete specialized tags predict gold POS tags for 9 languages. The H(A) row is the entropy (in nats per token) of the gold POS tags in the test data corpus, which is an upper bound for I(A; T). The remaining rows report the percentage I(A; T)/H(A).", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Language Treebank #Tokens H(A |X) H(A)", "figure_data": "ArabicPADT282k 0.059 2.059ChineseGSD123k 0.162 2.201EnglishEWT254k 0.216 2.494FrenchGSD400k 0.106 2.335HindiHDTB351k 0.146 2.261Portuguese Bosque319k 0.179 2.305RussianGSD98k 0.049 2.132SpanishAnCora549k 0.108 2.347ItalianISDT298K 0.120 2.304Table"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "embeddings Arabic English Spanish French Hindi Italian Portuguese Russian Chinese", "figure_data": "H(A)2.0162.4862.3452.2062.247 2.291 2.3062.1312.195ELMo067.2% 74.2%75.7%79.6% 70.1% 77.9% 76.5%73.2%57.3%ELMo167.2% 76.1%71.7%78.0% 70.5% 78.1% 72.3%73.8%59.8%ELMo263.8% 71.0%79.7%78.7% 67.2% 74.5% 75.3%72.2%59.4%"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Models Arabic Hindi English French Spanish Portuguese Russian Chinese Italian", "figure_data": "Iden0.7510.870 0.8240.7840.8080.8130.7830.7090.863PCA0.7430.866 0.8230.7490.8020.8080.7770.6970.857MLP0.7590.871 0.8390.8160.8350.8210.8000.7340.867VIBc0.7790.866 0.8510.8280.8370.8360.8140.7540.867POS0.6520.713 0.7120.7180.7390.7430.6620.5100.779VIBd0.6720.736 0.7420.7230.7250.7100.6510.5910.781"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Parsing accuracy of 9 languages (LAS). Black rows use continuous tags; gray rows use discrete tags (which does worse). In each column, the best score for each color is boldfaced, along with all results of that color that are not significantly worse (paired permutation test, p < 0.05). These results use only ELMo layer 1; results from all layers are shown in Table3in the appendix, for both LAS and UAS metrics.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "L I B = I(Y ; T) + \u2022 I(X; T)(1)", "formula_coordinates": [2.0, 353.18, 65.88, 172.36, 18.19]}, {"formula_id": "formula_1", "formula_text": "L I B = I(Y ; T)+ I(X; T)+ n X i=1 I(T i ; X |X i ) (2)", "formula_coordinates": [2.0, 312.95, 559.78, 212.6, 36.18]}, {"formula_id": "formula_2", "formula_text": "p \u2713 (x, y, t) = p(x) \u2022 p(y | x) \u2022 p \u2713 (t | x). 3.1 I(X; T) -the Token Encoder p \u2713 (t | x) Under this distribution, I(X; T) def = Ex,t [log p \u2713 (t |x) p \u2713 (t) ] = Ex [E t\u21e0p \u2713 (t |x) [log p \u2713 (t |x) p \u2713 (t) ]", "formula_coordinates": [3.0, 72.0, 296.53, 218.27, 89.17]}, {"formula_id": "formula_3", "formula_text": "upper bound z }| { E x [ E t\u21e0p \u2713 (t |x) [log p \u2713 (t|x) r (t) ]] I(X;T ) z }| { E x [ E t\u21e0p \u2713 (t |x) [log p \u2713 (t | x) p \u2713 (t) ]] = E x [KL(p \u2713 (t) || r (t))] 0", "formula_coordinates": [3.0, 79.45, 648.3, 205.18, 69.69]}, {"formula_id": "formula_4", "formula_text": "p \u2713 (t | x) = Q n i=1 p \u2713 (t i | x i ).", "formula_coordinates": [3.0, 401.68, 217.02, 125.77, 31.35]}, {"formula_id": "formula_5", "formula_text": "p \u2713 (t i | x i ) such that t i 2 {1, .", "formula_coordinates": [3.0, 307.75, 487.95, 125.51, 18.19]}, {"formula_id": "formula_6", "formula_text": "i ; X |X i ) -the Type Encoder s \u21e0 (t i |x i )", "formula_coordinates": [3.0, 344.3, 619.45, 170.87, 18.19]}, {"formula_id": "formula_7", "formula_text": "I(T i ; X |X i ) def = Ex [E t i \u21e0p \u2713 (t i |x) [log p \u2713 (t i |x) p \u2713 (t i |x i ) ]", "formula_coordinates": [4.0, 72.0, 308.21, 188.75, 22.51]}, {"formula_id": "formula_8", "formula_text": "upper bound z }| { E x [ E t i \u21e0p \u2713 (t i |x) [log p \u2713 (t i |x) s \u21e0 (t i |x i ) ]] I(T i ;X |X i ) z }| { E x [ E t i \u21e0p \u2713 (t i |x) [log p \u2713 (t i |x) p \u2713 (t i |x i ) ]] = E x [KL(p \u2713 (t i |x i ) || s \u21e0 (t i |x i ))] 0", "formula_coordinates": [4.0, 74.79, 489.81, 214.51, 69.69]}, {"formula_id": "formula_9", "formula_text": "P n i=1 KL[p \u2713 (t i |x) || s \u21e0 (t i |x i )]", "formula_coordinates": [4.0, 136.95, 570.58, 132.05, 31.35]}, {"formula_id": "formula_10", "formula_text": "T) -the Decoder q (y | t) Finally, I(Y ; T) def = Ey,t\u21e0p \u2713 [log p \u2713 (y |t) p(y) ].", "formula_coordinates": [4.0, 307.28, 118.83, 169.92, 39.93]}, {"formula_id": "formula_11", "formula_text": "p \u2713 (y | t) def = P x p \u2713 (x,y,t) P", "formula_coordinates": [4.0, 307.75, 252.03, 101.9, 31.04]}, {"formula_id": "formula_12", "formula_text": "I(Y;T ) z }| { E y,t\u21e0p \u2713 [log p \u2713 (y |t) p(y) ] lower bound z }| { E y,t\u21e0p \u2713 [log q (y |t) p(y) ] = E t\u21e0p \u2713 [KL(p \u2713 (y | t) || q (y | t))] 0", "formula_coordinates": [4.0, 334.65, 472.75, 165.35, 65.33]}, {"formula_id": "formula_13", "formula_text": "E x,y h E t\u21e0p \u2713 (t |x) [ log q (y|t)] + KL(p \u2713 (t|x)||r (t)) + n X i=1 KL(p \u2713 (t i | x) || s \u21e0 (t i |x i )) i (3)", "formula_coordinates": [5.0, 79.91, 341.27, 210.36, 70.12]}, {"formula_id": "formula_14", "formula_text": "P n i=1 KL(p \u2713 (t i |x i )||r (t i ))", "formula_coordinates": [5.0, 116.92, 587.08, 110.62, 31.35]}, {"formula_id": "formula_15", "formula_text": "6 KL(N 0 || N 1 ) = 1 2 (tr(\u2303 1 1 \u2303 0 )+(\u00b5 1 \u00b5 0 ) T \u2303 1 1 (\u00b5 1 \u00b5 0 ) d + log( det(\u2303 1 ) det(\u2303 0 ) ) 7 KL(p \u2713 (t i |x i )||r (t i )) = P k t i =1 p \u2713 (t i | x i ) log p \u2713 (t i |x i ) r (t i )", "formula_coordinates": [5.0, 72.22, 725.09, 211.73, 48.01]}, {"formula_id": "formula_16", "formula_text": "p \u2713 (x, a, t) = p(x, a) \u2022 p \u2713 (t | x), leading us to write H(A | T) \uf8ff Ex,a [E t\u21e0p \u2713 (t |x) [ log q(a | t)]] where q(a | t) = Q i q(a i | t i", "formula_coordinates": [7.0, 72.0, 617.44, 218.27, 45.28]}, {"formula_id": "formula_17", "formula_text": "i | X i ) \u21e1 H(A i | X i ).", "formula_coordinates": [8.0, 430.63, 707.9, 88.48, 18.19]}], "doi": "10.3115/981574.981598"}