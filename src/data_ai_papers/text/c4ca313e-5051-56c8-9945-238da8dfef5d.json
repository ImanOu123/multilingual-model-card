{"title": "Nonparametric Word Segmentation for Machine Translation", "authors": "Thuylinh Nguyen; Stephan Vogel; Noah A Smith", "pub_date": "", "abstract": "We present an unsupervised word segmentation model for machine translation. The model uses existing monolingual segmentation techniques and models the joint distribution over source sentence segmentations and alignments to the target sentence. During inference, the monolingual segmentation model and the bilingual word alignment model are coupled so that the alignments to the target sentence guide the segmentation of the source sentence. The experiments show improvements on Arabic-English and Chinese-English translation tasks.", "sections": [{"heading": "Introduction", "text": "In statistical machine translation, the smallest unit is usually the word, defined as a token delimited by spaces. Given a parallel corpus of source and target text, the training procedure first builds a word alignment, then extracts phrase pairs from this word alignment. However, in some languages (e.g., Chinese) there are no spaces between words.\nThe same problem arises when translating between two very different languages, such as from a language with rich morphology like Hungarian or Arabic to a language with poor morphology like English or Chinese. A single word in a morphologically rich language is often the composition of several morphemes, which correspond to separate words in English. 1 Often some preprocessing is applied involving word segmentation or morphological analysis of the source and/or target text. Such preprocessing tokenizes the text into morphemes or words, which linguists consider the smallest meaningbearing units of the language. Take as an example the Arabic word \"fktbwha\" and its English translation \"so they wrote it\". The preferred segmentation of \"fktbwha\" would be \"f-ktb-w-ha (so-wrote-they-it),\" which would allow for a oneto-one mapping between tokens in the two languages. However, the translation of the phrase in Hebrew is \"wktbw ath\". Now the best segmentation of the Arabic words would be \"fktbw-ha,\" corresponding to the two Hebrew words. This example shows that there may not be one correct segmentation that can be established in a preprocessing step. Rather, tokenization depends on the language we want to translate into and needs to be tied in with the alignment process. In short, we want to find the tokenization yielding the best alignment, and thereby the best translation system.\nWe propose an unsupervised tokenization method for machine translation by formulating a generative Bayesian model to \"explain\" the bilingual training data. Generation of a sentence pair is described as follows: first a monolingual tokenization model generates the source sentence, then the alignment model generates the target sentence through the alignments with the source sentence. Breaking this generation process into two steps provides flexibility to incorporate existing monolingual morphological segmentation models such as those of Mochihashi et al. (2009) or Creutz and Lagus (2007). Using nonparametric models and the Bayesian framework makes it possible to incorporate linguistic knowledge as prior distributions and obtain the posterior distribution through inference techniques such as MCMC or variational inference.\nAs new test source sentences do not have translations which can help to infer the best segmentation, we decode the source string according to the posterior distribution from the inference step.\nIn summary, our segmentation technique consists of the following steps:\n\u2022 A joint model of segmented source text and its target translation.\n\u2022 Inference of the posterior distribution of the model given the training data.\n\u2022 A decoding algorithm for segmenting source text.\n\u2022 Experiments in translation using the preprocessed source text.\nOur experiments show that the proposed segmentation method leads to improvements on Arabic-English and Chinese-English translation tasks.\nIn the next section we will discuss related work. Section 3 will describe our model in detail. The inference will be covered in Section 4, and decoding in Section 5. Experiments and results will be presented in Section 6.", "publication_ref": ["b14", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "The problem of segmentation for machine translation has been studied extensively in recent literature. Most of the work used some linguistic knowledge about the source and the target languages (Nie\u00dfen and Ney, 2004;Goldwater and McClosky, 2005). Sadat and Habash (2006) experimented with a wide range of tokenization schemes for Arabic-English translation. These experiments further show that even for a single language pair, different tokenizations are needed depending on the training corpus size. The experiments are very expensive to conduct and do not generalize to other language pairs. Recently, Dyer (2009) created manually crafted lattices for a subset of source words as references for segmentation when translating into English, and then learned the segmentation of the source words to optimize the translation with respect to these references. He showed that the parameters of the model can be applied to similar languages when translating into English. However, manually creating these lattices is time-consuming and requires a bilingual person with some knowledge of the underlying statistical machine translation system.\nThere have been some attempts to apply unsupervised methods for tokenization in machine translation (Chung and Gildea, 2009;Xu et al., 2008). The alignment model of Chung and Gildea (2009) forces every source word to align with a target word. Xu et al. (2008) modeled the source-to-null alignment as in the source word to target word model. Their models are special cases of our proposed model when the source model 2 is a unigram model. Like Xu et al. (2008), we use Gibbs sampling for inference. Chung and Gildea (2009) applied efficient dynamic programming-based variational inference algorithms.\nWe benefit from existing unsupervised monolingual segmentation. The source model uses the nested Pitman-Yor model as described by Mochihashi et al. (2009). When sampling each potential word boundary, our inference technique is a bilingual extension of what is described by Goldwater et al. (2006) for monolingual segmentation.\nNonparametric models have received attention in machine translation recently. For example, DeNero et al. (2008) proposed a hierarchical Dirichlet process model to learn the weights of phrase pairs to address the degeneration in phrase extraction. Teh (2006) used a hierarchical Pitman-Yor process as a smoothing method for language models.\nRecent work on multilingual language learning successfully used nonparametric models for language induction tasks such as grammar induction (Snyder et al., 2009;Cohen et al., 2010), morphological segmentation (Goldwater et al., 2006;, and part-of-speech tagging (Goldwater and Griffiths, 2007;.", "publication_ref": ["b15", "b8", "b17", "b6", "b2", "b23", "b2", "b23", "b23", "b2", "b14", "b9", "b5", "b21", "b20", "b3", "b9", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Models", "text": "We start with the generative process for a source sentence and its alignment with a target sentence. Then we describe individual models employed by this generation scheme.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Generative Story", "text": "A source sentence is a sequence of word tokens, and each word is either aligned or not aligned. We focus only on the segmentation problem and not reordering source words; therefore, the model will not generate the order of the target word tokens. A sentence pair and its alignment are captured by four components:\n\u2022 a sequence of words in the source sentence,\n\u2022 a set of null-aligned source tokens,\n\u2022 a set of null-aligned target tokens, and \u2022 a set of (source word to target word) alignment pairs.\nWe will start with a high-level story of how the segmentation of the source sentence and the alignment are generated.\n1. A source language monolingual segmentation model generates the source sentence.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Generate alignments:", "text": "(a) Given the sequence of words of the source sentence already generated in step 1, the alignment model marks each source word as either aligned or unaligned. If a source word is aligned, the model also generates the target word. (b) Unaligned target words are generated.\nThe model defines the joint probability of a segmented source language sentence and its alignment. During inference, the two parts are coupled, so that the alignment will influence which segmentation is selected. However, there are several advantages in breaking the generation process into two steps.\nFirst of all, in principle the model can incorporate any existing probabilistic monolingual segmentation to generate the source sentence. For example, the source model can be the nested Pitman-Yor process as described by Mochihashi et al. (2009), the minimum description length model presented by Creutz and Lagus (2007), or something else. Also the source model can incorporate linguistic knowledge from a rule-based or statistical morphological disambiguator.\nThe model generates the alignment after the source sentence with word boundaries already generated. Therefore, the alignment model can be any existing word alignment model (Brown et al., 1993;Vogel et al., 1996). Even though the choices of source model or alignment model can lead to different inference methods, the model we propose here is highly extensible. Note that we assume that the alignment consists of at most one-to-one mappings between source and target words, with null alignments possible on both sides.\nAnother advantage of a separate source model lies in the segmentation of an unseen test set. In section 5 we will show how to apply the source model distribution learned from training data to find the best segmentation of an unseen test set.", "publication_ref": ["b14", "b4", "b1", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Notation and Parameters", "text": "We will use bold font for a sequence or bags of words and regular font for an individual word. A source sentence s is a sequence of |s| words s i : s 1 , . . . , s |s| ; the translation of sentence s is the target sentence t of |t| words t 1 , . . . , t |t| . In sentence s the list of unaligned words is s nal and the list of aligned source words is s al . In the target sentence t the list of unaligned words is t nal and the list of target words having oneto-one alignment with source words s al is t al .\nThe alignment a of s and t is represented by\n{ s i , null | s i \u2208 s nal } \u222a { s i , t a i | s i \u2208 s al ; t a i \u2208 t al } \u222a { null, t j | t j \u2208 t nal }\nwhere a i denotes the index in t of the word aligned to s i .\nThe probability of a sequence or a set is denoted by P (.), probability at the word level is p (.). For example, the probability of sentence s is P (s), the probability of a word s is p (s), the probability that the target word t aligns to an aligned source word s is p (t | s).\nA sentence pair and its alignment are generated from the following models:\n\u2022 The source model generates sentence s with probability P (s).\n\u2022 The source-to-null alignment model decides independently for each word s whether it is unaligned with probability p (null | s i ) or aligned with probability:\n1 \u2212 p (null | s i ).\nThe probability of this step, for all source words, is:\nP (s nal , s al | s) = s i \u2208s nal p (null | s i ) \u00d7 s i \u2208s al (1 \u2212 p (null | s i )) .\nWe will also refer to the source-to-null model as the deletion model, since words in s nal are effectively deleted for the purposes of alignment.\n\u2022 The source-to-target alignment model generates a bag of target words t al aligned to the source words s al with probability:\nP (t al | s al ) = s i \u2208s al ;ta i \u2208t al p (t a i | s i ).\nNote that we do not need to be concerned with generating a explicitly, since we do not model word order on the target side.\n\u2022 The null-to-target alignment model generates the list of unaligned target words t nal given aligned target words t al with P (t nal | t al ) as follows:\n- The resulting null-to-target probability is therefore:\nP (t nal | t al ) = P (|t nal | | |t al |) t\u2208t nal p (t | null) .\nWe also call the null-to-target model the insertion model.\nThe above generation process defines the joint probability of source sentence s and its alignment a as follows:\nP (s, a) = P (s) source model \u00d7 P (a | s) alignment model (1) P (a | s) = P (t al | s al ) \u00d7 P (t nal | t al )(2)\n\u00d7 s i \u2208s nal p (null | s i ) \u00d7 s i \u2208s al (1 \u2212 p (null | s i ))", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Source Model", "text": "Our generative process provides the flexibility of incorporating different monolingual models into the probability distribution of a sentence pair.\nIn particular we use the existing state-of-the-art nested Pitman-Yor n-gram language model as described by Mochihashi et al. (2009). The probability of s is given by\nP (s) = P (|s|) |s| i=1 p (s i | s i\u2212n , . . . , s i\u22121 ) (3)\nwhere the n-gram probability is a hierarchical Pitman-Yor language model using (n \u2212 1)-gram as the base distribution. At the unigram level, the model uses the base distribution p (s) as the infinite-gram characterlevel Pitman-Yor language model.", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}, {"heading": "Modeling Null-Aligned Source Words", "text": "The probability that a source word aligns to null p (null | s) is defined by a binomial distribution with Beta prior Beta (\u03b1p, \u03b1 (1 \u2212 p)), where \u03b1 and p are model parameters. When p \u2192 0 and \u03b1 \u2192 \u221e the probability p (null | s) converges to 0 forcing each source words align to a target word. We fixed p = 0.1 and \u03b1 = 20 in our experiment. Xu et al. (2008) view the null word as another target word, hence in their model the probability that a source word aligns to null can only depend on itself.\nBy modeling the source-to-null alignment separately, our model lets the distribution depend on the word's n-gram context as in the source model. p (null | s i\u2212n , . . . , s i ) stands for the probability that the word s i is not aligned given its context (s i\u2212n , . . . , s i\u22121 ).\nThe n-gram source-to-null distribution p (null | s i\u2212n , . . . , s i ) is defined similarly to p (null | s i ) definition above in which the base distribution p now becomes the (n \u2212 1)-gram: p (null | s i\u2212n+1 , . . . , s i ). 3", "publication_ref": ["b23"], "figure_ref": [], "table_ref": []}, {"heading": "Source-Target Alignment Model", "text": "The probability p (t | s) that a target word t aligns to a source word s is a Pitman-Yor process:\nt | s \u223c PY (d, \u03b1, p 0 (t | s))\nhere d and \u03b1 are the input parameters, and p 0 (t | s) is the base distribution.\nLet |s, \u2022| denote the number of times s is aligned to any t in the corpus and let |s, t| denote the number of times s is aligned to t anywhere in the corpus. And let ty(s) denote the number of different target words t the word s is aligned to anywhere in the corpus. In the Chinese Restaurant Process metaphor, there is one restaurant for each source word s, the s restaurant has ty(s) tables and total |s, \u2022| customers; table t has |s, t| customers.\nThen, at a given time in the generative process for the corpus, we can write the probability that t is generated by the word s as:\n\u2022 if |s, t| > 0: p (t | s) = |s, t| \u2212 d + [\u03b1 + dty(s)]p 0 (t | s) |s, \u2022| + \u03b1 \u2022 if |s, t| = 0: p (t | s) = [\u03b1 + dty(s)]p 0 (t | s) |s, \u2022| + \u03b1\nFor language pairs with similar character sets such as English and French, words with similar surface form are often translations of each other. The base distribution can be defined based on the edit distance between two words .\nWe are working with diverse language pairs (Arabic-English and Chinese-English), so we use the base distribution as the flat distribution p 0 (t | s) = 1 T ; T is the number of distinct target words in the training set. In our experiment, the model parameters are \u03b1 = 20 and d = .5.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Modeling Null-Aligned Target Words", "text": "The null-aligned target words are modeled conditioned on previously generated target words as:\nP (t nal | t al ) = P (|t nal | | |t al |) t\u2208t nal p (t | null)\nThis model uses two probability distributions:\n\u2022 the number of unaligned target words: P (|t nal | | |t al |), and\n\u2022 the probability that each word in t nal is generated by null: p (t | null).\nWe model the number of unaligned target words similarly to the distribution in the IBM3 word alignment model (Brown et al., 1993). IBM3 assumes that each aligned target words generates a null-aligned target word with probability p 0 and fails to generate a target word with probability 1 \u2212 p 0 . So the parameter p 0 can be used to control the number of unaligned target words. In our experiments, we fix p 0 = .05. Following this assumption, the probability of |t nal | unaligned target words generated from |t al | words is:\nP (|t nal | | |t al |) = |t al | |t nal | p |t nal | 0 (1 \u2212 p 0 ) |t al |\u2212|t nal | .\nThe probability that a target word t aligns to null, p (t | null), also has a Pitman-Yor process prior. The base distribution of the model is similar to the source-to-target model's base distribution which is the flat distribution over target words.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "Inference", "text": "We have defined a probabilistic generative model to describe how a corpus of alignments and segmentations can be generated jointly. In this section we discuss how to obtain the posterior distributions of the missing alignments and segmentations given the training corpus, using Gibbs sampling.\nSuppose we are provided a morphological disambiguator for the source language such as MADA morphology tokenization toolkit (Sadat and Habash, 2006) for Arabic. 4 The morphological disambiguator segments a source word to morphemes of smallest meaning-bearing units of the source language. Therefore, a target word is equivalent to one or several morphemes. Given a morphological disambiguation toolkit, we use its output to bias our inference by not considering word boundaries after every character but only considering potential word boundaries as a subset of the morpheme boundaries set. In this way, the inference uses the morphological disambiguation toolkit to limit its search space.\nThe inference starts with an initial segmentation of the source corpus and also its alignment to the target corpus. The Gibbs sampler considers one potential word boundary at a time. There are two hypotheses at any given boundary position of a sentence pair (s, t): the merge hypothesis stands for no word boundary and the resulting source sentence s merge has a word s spanning over the sample point; the split hypothesis indicates the resulting source sentence s split has a word boundary at the sample point separating two words s 1 s 2 . Similar to Goldwater et al. (2006) for monolingual segmentation, the sampler randomly chooses the boundary according to the relative probabilities of the merge hypothesis and the split hypothesis.\nThe model consists of source and alignment model variables; given the training corpora size of a machine translation system, the number of variables is large. So if the Gibbs sampler samples both source variables and alignment variables, the inference requires many iterations until the sampler mixes. Xu et al. (2008) fixed this by repeatedly applying GIZA++ word alignment after each sampling iteration through the training corpora.\nOur inference technique is not precisely Gibbs sampling. Rather than sampling the alignment or attempting to collapse it out (by summing over all possible alignments when calculating the relative probabilities of the merge and split hypotheses), we seek the best alignment for each hypothesis. In other words, for each hypothesis, we perform a local search for a high-probability alignment of the merged word or split words, given the rest of alignment for the sentence. Up to one word may be displaced and realigned. This \"localbest\" alignment is used to score the hypothesis, and after sampling merge or split, we keep that best alignment. This inference technique is motivated by runtime demands, but we do not yet know of a theoretical justification for combining random steps with maximization over some variables. A more complete analysis is left to future work.", "publication_ref": ["b17", "b9", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Decoding for Unseen Test Sentences", "text": "Section 4 described how to get the model's posterior distribution and the segmentation and alignment of the training data under the model. We are left with the problem of decoding or finding the segmentation of test sentences where the translations are not available. This is needed when we want to translate new sentences. Here, tokenization is performed as a preprocessing step, decoupled from the subsequent translation steps.\nThe decoding step uses the model's posterior distribution for the training data to segment unseen source sentences. Because of the clear separation of the source model and the alignment model, the source model distribution learned from the Gibbs sampling directly represents the distribution over the source language and can therefore also handle the segmentation of unknown words in new test sentences. Only the source model is used in preprocessing.\nThe best segmentation s * of a string of characters c = c 1 , . . . , c |c| according to the n-gram source model is:\ns * = argmax s from c p (|s|) i=|s| i=1 p (s i | s i\u2212n , . . . , s i\u22121 )\nWe use a stochastic finite-state machine for decoding. This is possible by composition of the following two finite state machines:\n\u2022 Acceptor A c . The string of characters c is represented as an finite state acceptor machine where any path through the machine represents an unweighted segmentation of c.\n\u2022 Source model weighted finite state transducer L c . Knight and Al-Onaizan (1998) show how to build an n-gram language model by a weighted finite state machine. The states of the transducer are (n \u2212 1)gram history, the edges are words from the language. The arc s i coming from state (s i\u2212n , . . . , s i\u22121 ) to state (s i\u2212n+1 , . . . , s i ) has weight p (s i | s i\u2212n , . . . , s i\u22121 ).\nThe best segmentation s * is given as s * = BestPath(A c \u2022 L c ).", "publication_ref": ["b11"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "This section presents experimental results on Arabic-English and Chinese-English translation tasks using the proposed segmentation technique.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Arabic-English", "text": "As a training set we use the BTEC corpus distributed by the International Workshop on Spoken Language Translation (IWSLT) (Matthias and Chiori, 2005). The corpus is a collection of conversation transcripts from the travel domain.\nThe \"Supplied Data\" track consists of nearly 20K Arabic-English sentence pairs. The development set consists of 506 sentences from the IWSLT04 evaluation test set and the unseen set consists of 500 sentences from the IWSLT05 evaluation test set. Both development set and test set have 16 references per Arabic sentence.", "publication_ref": ["b13"], "figure_ref": [], "table_ref": []}, {"heading": "Chinese-English", "text": "The training set for Chinese-English translation task is also distributed by the IWSLT evaluation campaign. It consists of 67K Chinese-English sentence pairs. The development set and the test set each have 489 Chinese sentences and each sentence has 7 English references.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "We will report the translation results where the preprocessing of the source text are our unigram, bigram, and trigram source models and source-tonull model. The MCMC inference algorithm starts with an initial segmentation of the source text into full word forms. For Chinese, we use the original word segmentation as distributed by IWSLT. To get an initial alignment, we generate the IBM4 Viterbi alignments in both directions using the GIZA++ toolkit (Och and Ney, 2003) and combine them using the \"grow-diag-final-and\" heuristic. The output of combining GIZA++ alignment for a sentence pair is a sequence of s i -t j entries where i is an index of the source sentence and j is an index of the target sentence. As our model allows only one-to-one mappings between the words in the source and target sentences, we remove s i -t j from the sequence if either the source word s i or target word t j is already in a previous entry of the combined alignment sequence. The resulting alignment is our initial alignment for the inference.\nWe also apply the MADA morphology segmentation toolkit (Habash and Rambow, 2005) to preprocess the Arabic corpus. We use the D3 scheme (each Arabic word is segmented into morphemes in sequence [CONJ+ [PART+ [Al+ BASE +PRON]]]), mark the morpheme boundaries, and then combine the morphemes again to have words in their original full word form. During inference, we only sample over these morpheme boundaries as potential word boundaries. In this way, we limit the search space, allowing only segmentations consistent with MADA-D3.\nThe inference samples 150 iterations through the whole training set and uses the posterior probability distribution from the last iteration for decoding. The decoding process is then applied to the entire training set as well as to the development and test sets to generate a consistent tokenization across all three data sets. We used the OpenFST toolkit (Allauzen et al., 2007) for finite-state machine implementation and operations. The output of the decoding is the preprocessed data for translation. We use the open source Moses phrase-based MT system (Koehn et al., 2007) to test the impact of the preprocessing technique on translation quality. 5", "publication_ref": ["b16", "b10", "b0", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Arabic-English Translation Results", "text": "We consider the Arabic-English setting. We use two baselines: original full word form and MADA-D3 tokenization scheme for Arabic-English translation. Table 1 compares the translation results of our segmentation methods with these baselines. Our segmentation method shows improvement over the two baselines on both the development and test sets. According to Sadat and Habash (2006)  forms best for their Arabic-English translation especially for small and moderate data sizes. In our experiments, we see an improvement when using the MADA-D3 preprocessing over using the original Arabic corpus on the unseen test set, but not on the development set.\nThe Gibbs sampler only samples on the morphology boundary points of MADA-D3, so the improvement resulting from our segmentation technique does not come from removing unknown words. It is due to a better matching between the source and target sentences by integrating segmentation and alignment. We therefore expect the same impact on a larger training data set in future experiments.  We next consider the Chinese-English setting. The translation performance using our word segmentation technique is shown in Table 2. There are two baselines for Chinese-English translation: (a) the source text in the full word form distributed by the IWSLT evaluation and (b) no segmentation of the source text, which is equivalent to interpreting each Chinese character as a single word.", "publication_ref": ["b17"], "figure_ref": [], "table_ref": ["tab_2", "tab_4"]}, {"heading": "Chinese-English Translation Results", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dev", "text": "Taking development and test sets into account, the best Chinese-English translation system results from our unigram model. It is significantly better than other systems on the development set and performs almost equally well with the IWSLT segmentation on the test set. Note that the segmentation distributed by IWSLT is a manual segmentation for the translation task. Chung and Gildea (2009) and Xu et al. ( 2008) also showed improvement over a simple monolingual segmentation for Chinese-English translation. Our character-based translation result is comparable to their monolingual segmentations. Both trigram and unigram translation results outperform the character-based translation.\nWe also observe that there are no additional gains for Chinese-English translation when using a higher n-gram model. Our Gibbs sampler has the advantage that the samples are guaranteed to converge eventually to the model's posterior distributions, but in each step the modification to the current hypothesis is small and local. In iterations 100-150, the average number of boundary changes for the unigram model is 14K boundaries versus only 1.5K boundary changes for the trigram model. With 150 iterations, the inference output of trigram model might not yet represent its posterior distribution. We leave a more detailed investigation of convergence behavior to future work.", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion and Future Work", "text": "We presented an unsupervised segmentation method for machine translation and presented experiments for Arabic-English and Chinese-English translation tasks. The model can incorporate existing monolingual segmentation models and seeks to learn a segmenter appropriate for a particular translation task (target language and dataset).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We thank Kevin Gimpel for interesting discussions and technical advice. We also thank the anonymous reviewers for useful feedback. This work was supported by DARPA Gale project, NSF grants 0844507 and 0915187.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "OpenFst: A General and Efficient Weighted Finite-State Transducer Library", "journal": "Springer", "year": "2007", "authors": "C Allauzen; M Riley; J Schalkwyk; W Skut; M Mohri"}, {"ref_id": "b1", "title": "The Mathematics of Statistical Machine Translation: Parameter Estimation", "journal": "", "year": "1993", "authors": "Peter F Brown; J Della Vincent; Stephen A Pietra; Robert L Della Pietra;  Mercer"}, {"ref_id": "b2", "title": "Unsupervised Tokenization for Machine Translation", "journal": "Association for Computational Linguistics", "year": "2009", "authors": "T Chung; D Gildea"}, {"ref_id": "b3", "title": "Variational Inference for Adaptor Grammars", "journal": "", "year": "2010-06", "authors": "S B Cohen; D M Blei; N A Smith"}, {"ref_id": "b4", "title": "Unsupervised Models for Morpheme Segmentation and Morphology Learning", "journal": "ACM Trans. Speech Lang. Process", "year": "2007", "authors": "Mathias Creutz; Krista Lagus"}, {"ref_id": "b5", "title": "Sampling Alignment Structure under a Bayesian Translation Model", "journal": "Association for Computational Linguistics", "year": "2008-10", "authors": "J Denero; A Bouchard-C\u00f4t\u00e9; D Klein"}, {"ref_id": "b6", "title": "Using a Maximum Entropy model to build segmentation lattices for MT", "journal": "", "year": "2009-06", "authors": "C Dyer"}, {"ref_id": "b7", "title": "A Fully Bayesian Approach to Unsupervised Part-of-Speech Tagging", "journal": "", "year": "2007", "authors": "S Goldwater; T L Griffiths"}, {"ref_id": "b8", "title": "Improving Statistical Machine Translation Through Morphological Analysis", "journal": "", "year": "2005", "authors": "S Goldwater; D Mcclosky"}, {"ref_id": "b9", "title": "Contextual Dependencies in Unsupervised Word Segmentation", "journal": "", "year": "2006", "authors": "S Goldwater; T L Griffiths; M Johnson"}, {"ref_id": "b10", "title": "Arabic Tokenization, Part-of-Speech Tagging, and Morphological Disambiguation in One Fell Swoop", "journal": "", "year": "2005", "authors": "N Habash; O Rambow"}, {"ref_id": "b11", "title": "Translation with Finite-State Devices", "journal": "", "year": "1998", "authors": "K Knight; Y Al-Onaizan"}, {"ref_id": "b12", "title": "Moses: Open Source Toolkit for Statistical Machine Translation", "journal": "", "year": "2007", "authors": "P Koehn; H Hoang; A Birch; C Callison-Burch; M Federico; N Bertoldi; B Cowan; W Shen; C Moran; R Zens; C Dyer; O Bojar; A Constantin; E Herbst"}, {"ref_id": "b13", "title": "Overview of the IWSLT 2005 Evaluation Campaign", "journal": "", "year": "2005", "authors": "E Matthias; H Chiori"}, {"ref_id": "b14", "title": "Bayesian Unsupervised Word Segmentation with Nested Pitman-Yor Language Modeling", "journal": "", "year": "2009-08", "authors": "D Mochihashi; T Yamada; N Ueda"}, {"ref_id": "b15", "title": "Statistical Machine Translation with Scarce Resources Using Morpho-Syntactic Information", "journal": "Computational Linguistics", "year": "2004-06", "authors": "S Nie\u00dfen; H Ney"}, {"ref_id": "b16", "title": "A Systematic Comparison of Various Statistical Alignment Models", "journal": "Computational Linguistics", "year": "2003", "authors": "F Och; H Ney"}, {"ref_id": "b17", "title": "Combination of Arabic Preprocessing Schemes for Statistical Machine Translation", "journal": "", "year": "2006", "authors": "F Sadat; N Habash"}, {"ref_id": "b18", "title": "Unsupervised Multilingual Learning for Morphological Segmentation", "journal": "", "year": "2008-06", "authors": "B Snyder; R Barzilay"}, {"ref_id": "b19", "title": "Unsupervised Multilingual Learning for POS Tagging", "journal": "", "year": "2008", "authors": "B Snyder; T Naseem; J Eisenstein; R Barzilay"}, {"ref_id": "b20", "title": "Unsupervised Multilingual Grammar Induction", "journal": "", "year": "2009-08", "authors": "B Snyder; T Naseem; R Barzilay"}, {"ref_id": "b21", "title": "A Hierarchical Bayesian Language Model Based On Pitman-Yor Processes", "journal": "", "year": "2006-07", "authors": "Y W Teh"}, {"ref_id": "b22", "title": "HMM-Based Word Alignment in Statistical Translation", "journal": "", "year": "1996", "authors": "S Vogel; H Ney; C Tillmann"}, {"ref_id": "b23", "title": "Bayesian Semi-Supervised Chinese Word Segmentation for Statistical Machine Translation", "journal": "", "year": "2008-08", "authors": "J Xu; J Gao; K Toutanova; H Ney"}], "figures": [{"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Generate the number of unaligned target words |t nal | given the number of aligned target words |t al | with probability P (|t nal | | |t al |). -Generate |t nal | unaligned words t \u2208 t nal independently, each with probability p (t | null).", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ": Arabic-English translation results(BLEU)."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Chinese-English translation result in BLEU score metric.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "{ s i , null | s i \u2208 s nal } \u222a { s i , t a i | s i \u2208 s al ; t a i \u2208 t al } \u222a { null, t j | t j \u2208 t nal }", "formula_coordinates": [3.0, 302.99, 619.02, 216.5, 32.42]}, {"formula_id": "formula_1", "formula_text": "1 \u2212 p (null | s i ).", "formula_coordinates": [4.0, 121.31, 219.28, 78.13, 18.9]}, {"formula_id": "formula_2", "formula_text": "P (s nal , s al | s) = s i \u2208s nal p (null | s i ) \u00d7 s i \u2208s al (1 \u2212 p (null | s i )) .", "formula_coordinates": [4.0, 94.13, 244.76, 194.71, 34.37]}, {"formula_id": "formula_3", "formula_text": "P (t al | s al ) = s i \u2208s al ;ta i \u2208t al p (t a i | s i ).", "formula_coordinates": [4.0, 94.13, 382.42, 169.7, 20.84]}, {"formula_id": "formula_4", "formula_text": "P (t nal | t al ) = P (|t nal | | |t al |) t\u2208t nal p (t | null) .", "formula_coordinates": [4.0, 94.13, 628.31, 194.72, 34.0]}, {"formula_id": "formula_5", "formula_text": "P (s, a) = P (s) source model \u00d7 P (a | s) alignment model (1) P (a | s) = P (t al | s al ) \u00d7 P (t nal | t al )(2)", "formula_coordinates": [4.0, 302.99, 96.38, 223.11, 69.22]}, {"formula_id": "formula_6", "formula_text": "\u00d7 s i \u2208s nal p (null | s i ) \u00d7 s i \u2208s al (1 \u2212 p (null | s i ))", "formula_coordinates": [4.0, 322.88, 164.84, 203.23, 28.56]}, {"formula_id": "formula_7", "formula_text": "P (s) = P (|s|) |s| i=1 p (s i | s i\u2212n , . . . , s i\u22121 ) (3)", "formula_coordinates": [4.0, 318.16, 319.17, 201.32, 35.89]}, {"formula_id": "formula_8", "formula_text": "t | s \u223c PY (d, \u03b1, p 0 (t | s))", "formula_coordinates": [5.0, 121.5, 176.16, 118.18, 18.9]}, {"formula_id": "formula_9", "formula_text": "\u2022 if |s, t| > 0: p (t | s) = |s, t| \u2212 d + [\u03b1 + dty(s)]p 0 (t | s) |s, \u2022| + \u03b1 \u2022 if |s, t| = 0: p (t | s) = [\u03b1 + dty(s)]p 0 (t | s) |s, \u2022| + \u03b1", "formula_coordinates": [5.0, 83.71, 392.01, 180.65, 131.46]}, {"formula_id": "formula_10", "formula_text": "P (t nal | t al ) = P (|t nal | | |t al |) t\u2208t nal p (t | null)", "formula_coordinates": [5.0, 310.44, 126.87, 201.58, 25.77]}, {"formula_id": "formula_11", "formula_text": "P (|t nal | | |t al |) = |t al | |t nal | p |t nal | 0 (1 \u2212 p 0 ) |t al |\u2212|t nal | .", "formula_coordinates": [5.0, 302.99, 395.99, 216.5, 32.58]}, {"formula_id": "formula_12", "formula_text": "s * = argmax s from c p (|s|) i=|s| i=1 p (s i | s i\u2212n , . . . , s i\u22121 )", "formula_coordinates": [6.0, 310.48, 477.6, 201.51, 35.66]}], "doi": ""}