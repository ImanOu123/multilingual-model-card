{"title": "Generalized Nested Rollout Policy Adaptation", "authors": "Tristan Cazenave", "pub_date": "", "abstract": "Nested Rollout Policy Adaptation (NRPA) is a Monte Carlo search algorithm for single player games. In this paper we propose to generalize NRPA with a temperature and a bias and to analyze theoretically the algorithms. The generalized algorithm is named GNRPA. Experiments show it improves on NRPA for different application domains: SameGame and the Traveling Salesman Problem with Time Windows.", "sections": [{"heading": "Introduction", "text": "Monte Carlo Tree Search (MCTS) has been successfully applied to many games and problems [4].\nNested Monte Carlo Search (NMCS) [5] is an algorithm that works well for puzzles and optimization problems. It biases its playouts using lower level playouts. At level zero NMCS adopts a uniform random playout policy. Online learning of playout strategies combined with NMCS has given good results on optimization problems [22]. Other applications of NMCS include Single Player General Game Playing [16], Cooperative Pathfinding [2], Software testing [20], heuristic Model-Checking [21], the Pancake problem [3], Games [8] and the RNA inverse folding problem [18].\nOnline learning of a playout policy in the context of nested searches has been further developed for puzzles and optimization with Nested Rollout Policy Adaptation (NRPA) [23]. NRPA has found new world records in Morpion Solitaire and crosswords puzzles. NRPA has been applied to multiple problems: the Traveling Salesman with Time Windows (TSPTW) problem [9,11], 3D Packing with Object Orientation [13], the physical traveling salesman problem [14], the Multiple Sequence Alignment problem [15] or Logistics [12]. The principle of NRPA is to adapt the playout policy so as to learn the best sequence of moves found so far at each level.\nThe use of Gibbs sampling in Monte Carlo Tree Search dates back to the general game player Cadia Player and its MAST playout policy [1].\nWe now give the outline of the paper. The second section describes NRPA. The third section gives a theoretical analysis of NRPA. The fourth section describes the generalization of NRPA. The fifth section details optimizations of GNRPA. The sixth section gives experimental results for SameGame and TSPTW.", "publication_ref": ["b3", "b4", "b21", "b15", "b1", "b19", "b20", "b2", "b7", "b17", "b22", "b8", "b10", "b12", "b13", "b14", "b11", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "NRPA", "text": "NRPA learns a rollout policy by adapting weights on each action. Vanilla NRPA starts with all weights set to zero. During the playout phase, action is sampled with a probability proportional to the exponential of the associated weight. The playout algorithm is given in Algorithm 1. The algorithm starts with initializing the sequence of moves that it will play (line 2). Then it performs a loop until it reaches a terminal states (lines 3-6). At each step of the playout it calculates the sum of all the exponentials of the weights of the possible moves (lines 7-10) and chooses a move proportional to its probability given by the softmax function (line 11). Then it plays the chosen move and adds it to the sequence of moves (lines 12-13). Each move is associated to a code which is usually independent of the state.\nThen, the policy is adapted on the best current sequence found, by increasing the weight of the best actions and decreasing the weights of all the moves proportionally to their probabilities of being played. The Adapt algorithm is given in Algorithm 2. For all the states of the sequence passed as a parameter it adds \u03b1 to the weight of the move of the sequence (lines 3-5). Then it reduces all the moves proportionally to \u03b1\u00d7 the probability of playing the move so as to keep the sum of logits unchanged (lines 6-12).\nIn NRPA, each nested level takes as input a policy, and returns a sequence. At each step, the algorithm makes a recursive call to the lower level and gets a sequence as a result. It adapts the policy to the best sequence of the level at each step. At level zero it makes a playout.\nThe NRPA algorithm is given in Algorithm 3. At level zero it simply performs a playout (lines 2-3). At greater levels it performs N iterations and for each iteration it calls itself recursively to get a score and a sequence (lines 4-7). If it finds a new best sequence for the level it keeps it as the best sequence (lines 8-11). Then it adapts the policy using the best sequence found so far at the current level (line 12).\nNRPA balances exploitation by adapting the probabilities of playing moves toward the best sequence of the level, and exploration by using Gibbs sampling at the lowest level. It is a general algorithm that has proven to work well for many optimization problems. In NRPA each move is associated to a weight. The goal of the algorithm is to learn these weights so as to produce a playout policy that generates good sequences of moves. At each level of the algorithm the best sequence found so far is memorized. Let s 1 , ..., s m be the sequence of states of the best sequence. Let n i be the number of possible moves in a state s i . Let m i1 , ..., m ini be the possible moves in state s i and m ib be the move of the best sequence in state s i . The goal is to learn to play the move m ib in state s i . The playouts use Gibbs sampling. Each move m ik is associated to a weight w ik . The probability p ik of choosing the move m ik in a playout is the softmax function:\np ik = e w ik \u03a3 j e w ij\nThe cross-entropy loss for learning to play move m ib is C i = \u2212log(p ib ). In order to apply the gradient we calculate the partial derivative of the loss:\n\u03b4Ci \u03b4p ib = \u2212 1 p ib .\nWe then calculate the partial derivative of the softmax with respect to the weights:\n\u03b4p ib \u03b4w ij = p ib (\u03b4 bj \u2212 p ij )\nWhere \u03b4 bj = 1 if b = j and 0 otherwise. Thus the gradient is:\n\u2207w ij = \u03b4C i \u03b4p ib \u03b4p ib \u03b4w ij = \u2212 1 p ib p ib (\u03b4 bj \u2212 p ij ) = p ij \u2212 \u03b4 bj\nIf we use \u03b1 as a learning rate we update the weights with:\nw ij = w ij \u2212 \u03b1(p ij \u2212 \u03b4 bj )\nThis is the formula used in the NRPA algorithm to adapt weights.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Generalization of NRPA", "text": "We propose to generalize the NRPA algorithm by generalizing the way the probability is calculated using a temperature \u03c4 and a bias \u03b2 ij :\np ik = e w ik \u03c4 +\u03b2 ik \u03a3 j e w ij \u03c4 +\u03b2 ij", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Theoretical Analysis", "text": "The formula for the derivative of f (x) = g(x) h(x) is:\nf (x) = g (x)h(x) \u2212 h (x)g(x) h 2 (x)\nSo the derivative of p ib relative to w ib is: \n\u03b4p ib \u03b4w ib =\n\u03b4p ib \u03b4w ib = 1 \u03c4 p ib (1 \u2212 p ib )\nThe derivative of p ib relative to w ij with j = b is:\n\u03b4p ib \u03b4w ij = \u2212 1 \u03c4 e w ij \u03c4 +\u03b2ij e w ib \u03c4 +\u03b2 ib (\u03a3 j e w ij \u03c4 +\u03b2ij ) 2 \u03b4p ib \u03b4w ij = \u2212 1 \u03c4 p ij p ib\nWe then derive the cross-entropy loss and the softmax to calculate the gradient:\n\u2207w ij = \u03b4C i \u03b4p ib \u03b4p ib \u03b4w ij = \u2212 1 \u03c4 1 p ib p ib (\u03b4 bj \u2212 p ij ) = p ij \u2212 \u03b4 bj \u03c4\nIf we use \u03b1 as a learning rate we update the weights with:\nw ij = w ij \u2212 \u03b1 p ij \u2212 \u03b4 bj \u03c4\nThis is a generalization of NRPA since when we set \u03c4 = 1 and \u03b2 ij = 0 we get NRPA.\nThe corresponding algorithms are given in Algorithms 4 and 5.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Equivalence of Algorithms", "text": "Let the weights and probabilities of playing moves be indexed by the iteration of the GNRPA level. Let w nij be the weight w ij at iteration n, p nij be the probability of playing move j at step i at iteration n, \u03b4 nbj the \u03b4 bj at iteration n.\nWe have:\np 0ij = e 1 \u03c4 w0ij +\u03b2ij \u03a3 k e 1 \u03c4 w 0ik +\u03b2 ik w 1ij = w 0ij \u2212 \u03b1 \u03c4 (p 0ij \u2212 \u03b4 0bj ) p 1ij = e 1 \u03c4 w1ij +\u03b2ij \u03a3 k e 1 \u03c4 w 1ik +\u03b2 ik = e 1 \u03c4 w0ij \u2212 \u03b1 \u03c4 2 (p0ij \u2212\u03b4 0bj )+\u03b2ij \u03a3 k e 1 \u03c4 w 1ik +\u03b2 ik w 2ij = w 1ij \u2212 \u03b1 \u03c4 (p 1ij \u2212 \u03b4 1bj ) = w 0ij \u2212 \u03b1 \u03c4 (p 0ij \u2212 \u03b4 0bj + p 1ij \u2212 \u03b4 1bj )\nBy recurrence we get:\np nij = e 1 \u03c4 wnij +\u03b2ij \u03a3 k e 1 \u03c4 w nik +\u03b2 ik = e w 0ij \u03c4 \u2212 \u03b1 \u03c4 2 (\u03a3 k p kij \u2212\u03b4 kbj )+\u03b2ij \u03a3 k e 1 \u03c4 w nik +\u03b2 ik\nFrom this equation we can deduce the equivalence between different algorithms. For example GNRPA 1 with \u03b1 1 = ( \u03c41 \u03c42 ) 2 \u03b1 2 and \u03c4 1 is equivalent to GNRPA 2 with \u03b1 2 and \u03c4 2 provided we set w 0ij in GNRPA 1 to \u03c41 \u03c42 w 0ij . It means we can always use \u03c4 = 1 provided we correspondingly set \u03b1 and w 0ij .\nAnother deduction we can make is we can set \u03b2 ij = 0 provided we set w 0ij = w 0ij + \u03c4 \u00d7 \u03b2 ij . We can also set w 0ij = 0 and use only \u03b2 ij which is easier.\nThe equivalences mean that GNRPA is equivalent to NRPA with the appropriate \u03b1 and w 0ij . However, it can be more convenient to use \u03b2 ij than to initialize the weights w 0ij as we will see for SameGame. In problems such as SameGame the computation of the possible moves is costly. It is important in this case to avoid to compute again the possible moves for the best playout in the Adapt function. The possible moves have already been calculated during the playout that found the best sequence. The optimized playout algorithm memorizes in a matrix code the codes of the possible moves during a playout. The cell code[i][m] contains the code of the possible move of index m at the state number i of the best sequence. The state number 0 is the initial state of the problem. The index array memorizes the index of the code of the best move for each state number, len(index) is the length of the best sequence and index[i] is the index of the best move for state number i.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Avoid the Copy of the Policy", "text": "Tha Adapt algorithm of NRPA and GNRPA considers the states of the sequence to learn as a batch. The sum of the gradients is calculated for the entire sequence and then applied. The way it is done in NRPA is by copying the policy to a temporary policy, modifying the temporary policy computing the gradient with the unmodified policy, and then copying the modified temporary policy to the policy. When the number of possible codes is large copying the policy can be costly. We propose to change the Adapt algorithm to avoid to copy twice the policy at each Adapt call. We also use the memorized codes and index so as to avoid calculating again the possible moves of the best sequence.\nThe way to avoid copying the policy is to make a first loop to compute the probabilities of each move of the best sequence, lines 2-8 of Algorithm 6. The matrix o[i][m] contains the probability for move index m in state number i, the array z[i] contains the sum of the probabilities of state number i. The second step is to apply the gradient directly to the policy for each state number i and each code, see lines 9-14.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 6", "text": "The optimized generalized adapt algorithm 1: Adapt (policy, code, index) 2:\nfor i \u2208 [0, len(index)[ do 3:\nz[i] \u2190 0 4: for m \u2208 [0, len(code[i])[ do 5: o[i][m] \u2190 e policy[code[i][m]] \u03c4 +\u03b2(m) 6: z[i] \u2190 z[i] + o[i][m] 7:\nend for 8: end for 9:\nfor\ni \u2208 [0, len(index)[ do 10: b \u2190 index[i] 11: for m \u2208 [0, len(code[i])[ do 12: policy[code[i][m]] \u2190 policy[code[i][m]] \u2212 \u03b1 \u03c4 ( o[i][m] z|i] \u2212 \u03b4 bm ) 13:\nend for 14:\nend for", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Results", "text": "We now give experimental results for SameGame and TSPTW.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "SameGame", "text": "The first algorithm we test is the standard NRPA algorithm with codes of the moves using a Zobrist hashing [24] of the cells of the moves [17,10,6]. The selective policy used is to avoid the moves of the dominant color except for moves of size two after move number ten. The codes of the possible moves of the best playout are recorded so as to avoid computing again the possible moves in the Adapt function. It is called NRPA. Using Zobrist hashing of the moves and biasing the policy with \u03b2 is better than initializing the weights at SameGame since there are too many possible moves and weights. We tried to reduce the possible codes for the moves but it gave worse results. The second algorithm we test is to use Zobrist hashing and the selective policy associated to the bias. It is GNRPA with \u03c4 = 1 and \u03b2 ij = min(n \u2212 2 \u2212 tabu, 8), with tabu = 1 if the move is of size 2 and of the tabu color and tabu = 0 otherwise. The variable n being the number of cells of the move. The algorithm is called GNRPA.beta.\nThe third algorithm we test is to use Zobrist hashing, the selective policy, \u03b2 and the optimized Adapt function. The algorithm is called GNRPA.beta.opt.\nAll algorithms are run 200 times for 655.36 seconds and average scores are recorded each time the search time is doubled.\nThe evolution of the average score of the algorithms is given in figure 1. We can see that GNRPA.beta is better than NRPA but that for scores close to the current record of the problem the difference is small. GNRPA.beta.opt is the best algorithm as it searches more than GNRPA.beta for the same time. Table 1 gives the average scores for the three algorithms associated to the 95% confidence interval in parenthesis (2 \u00d7 \u03c3 \u221a n ). ", "publication_ref": ["b23", "b16", "b9", "b5"], "figure_ref": ["fig_1"], "table_ref": ["tab_2"]}, {"heading": "TSPTW", "text": "The Traveling Salesman with Time Windows problem (TSPTW) is a practical problem that has everyday applications. NRPA can be used to efficiently solve practical logistics problems faced by large companies such as EDF [7].\nIn NRPA paths with violated constraints can be generated. As presented in [22] , a new score T cost(p) of a path p can be defined as follow:\nT cost(p) = cost(p) + 10 6 * \u2126(p), with, cost(p) the sum of the distances of the path p and \u2126(p) the number of violated constraints. 10 6 is a constant chosen high enough so that the algorithm first optimizes the constraints.\nThe problem we use to experiment with the TSPTW problem is the most difficult problem from the set of [19].\nIn order to initialize \u03b2 ij we normalize the distances and multiply the result by ten. So \u03b2 ij = 10 \u00d7 dij \u2212min max\u2212min , where min is the smallest possible distance and max the greatest possible one.\nAll algorithms are run 200 times for 655.36 seconds and average scores are recorded each time the search time is doubled.\nFigure 2 gives the curves for the three GNRPA algorithms we haves tested with a logarithmic time scale for the x axis. We could not represent the curve for NRPA in figure 2 since the average values are too low. They are given in table 2. It is possible to improve much on standard NRPA by initializing the weights with the distances between cities [11,7]. However this solution is not practical for all problems as we have seen with SameGame and using a bias \u03b2 is more convenient and general. We also tried initializing the weights with \u03b2 instead of using \u03b2 and we got similar results to the use of \u03b2.\nWe can see in figure 2 that using a temperature of 1.4 improves on a temperature of 1.0. Using the optimized Adapt function does not improve GNRPA for TSPTW since in the TSPTW problem the policy array and the number of possible moves is very small and copying the policy is fast.\nThe curve of the best algorithm is asymptotic toward the best value found by all algorithms. It reaches better scores faster.\nTable 2 gives the average values for NRPA and the three GNRPA algorithms we have tested. As there is a penalty of 1 million for each constraint violation, NRPA has very low scores compared to GNRPA. This is why NRPA is not depicted in figure 2. For a search time of 655.36 seconds and not taking into account the constraints, NRPA usually reaches tour scores between -900 and -930. Much worse than GNRPA. We can observe that using a temparature is beneficial until we use 655.36 seconds and approach the asymptotic score when both algorithms have similar scores. The numbers in parenthesis in the table are the 95% confidence interval (2 \u00d7 \u03c3 \u221a n ). ", "publication_ref": ["b6", "b21", "b18", "b10", "b6"], "figure_ref": ["fig_2", "fig_2", "fig_2", "fig_2"], "table_ref": ["tab_3", "tab_3"]}, {"heading": "Conclusion", "text": "We presented a theoretical analysis and a generalization of NRPA named GNRPA. It uses a temperature \u03c4 and a bias \u03b2.\nWe have theoretically shown that using a bias is equivalent to initializing the weights. For SameGame initializing the weights can be difficult if we initialize all the weights at the start of the program since there are too many possible weights, whereas using a bias \u03b2 is easier and improves search at SameGame. A lazy initialization of the weights would also be possible in this case and would solve the weight initialization problem for SameGame. For some other problems the bias could be more specific than the code of the move, i.e. a move could be associated to different bias depending on the state. In this case different bias could be used in different states for the same move which would not be possible with weight initialization.\nWe have also theoretically shown that the learning rate and the temperature can replace each other. Tuning the temperature and using a bias has been very beneficial for the TSPTW.\nThe remaining work is to apply the algorithm to other domains and to improve the way to design formulas for the bias \u03b2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgment", "text": "This work was supported in part by the French government under management of Agence Nationale de la Recherche as part of the \"Investissements d'avenir\" program, reference ANR19-P3IA-0001 (PRAIRIE 3IA Institute).", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Cadiaplayer: A simulation-based general game player", "journal": "IEEE Transactions on Computational Intelligence and AI in Games", "year": "2009", "authors": "Y Bjornsson; H Finnsson"}, {"ref_id": "b1", "title": "Monte-carlo fork search for cooperative path-finding", "journal": "", "year": "2013-08-03", "authors": "B Bouzy"}, {"ref_id": "b2", "title": "Burnt pancake problem: New lower bounds on the diameter and new experimental optimality ratios", "journal": "", "year": "2016", "authors": "B Bouzy"}, {"ref_id": "b3", "title": "A survey of Monte Carlo tree search methods", "journal": "IEEE Transactions on Computational Intelligence and AI in Games", "year": "2012-03", "authors": "C Browne; E Powley; D Whitehouse; S Lucas; P Cowling; P Rohlfshagen; S Tavener; D Perez; S Samothrakis; S Colton"}, {"ref_id": "b4", "title": "Nested Monte-Carlo Search", "journal": "", "year": "2009", "authors": "T Cazenave"}, {"ref_id": "b5", "title": "Nested rollout policy adaptation with selective policies", "journal": "Springer", "year": "2016", "authors": "T Cazenave"}, {"ref_id": "b6", "title": "Monte carlo vehicle routing", "journal": "", "year": "2020", "authors": "T Cazenave; J Y Lucas; H Kim; T Triboulet"}, {"ref_id": "b7", "title": "Nested monte carlo search for two-player games", "journal": "", "year": "2016", "authors": "T Cazenave; A Saffidine; M J Schofield; M Thielscher"}, {"ref_id": "b8", "title": "Application of the nested rollout policy adaptation algorithm to the traveling salesman problem with time windows", "journal": "", "year": "2012", "authors": "T Cazenave; F Teytaud"}, {"ref_id": "b9", "title": "Improved diversity in nested rollout policy adaptation", "journal": "Springer", "year": "2016", "authors": "S Edelkamp; T Cazenave"}, {"ref_id": "b10", "title": "Algorithm and knowledge engineering for the tsptw problem", "journal": "IEEE", "year": "2013", "authors": "S Edelkamp; M Gath; T Cazenave; F Teytaud"}, {"ref_id": "b11", "title": "Monte-carlo tree search for logistics", "journal": "Springer International Publishing", "year": "2016", "authors": "S Edelkamp; M Gath; C Greulich; M Humann; O Herzog; M Lawo"}, {"ref_id": "b12", "title": "Monte-carlo tree search for 3d packing with object orientation", "journal": "Springer International Publishing", "year": "2014", "authors": "S Edelkamp; M Gath; M Rohde"}, {"ref_id": "b13", "title": "Solving physical traveling salesman problems with policy adaptation", "journal": "IEEE", "year": "2014", "authors": "S Edelkamp; C Greulich"}, {"ref_id": "b14", "title": "Monte-carlo tree search for the multiple sequence alignment problem", "journal": "Eighth Annual Symposium on Combinatorial Search", "year": "2015", "authors": "S Edelkamp; Z Tang"}, {"ref_id": "b15", "title": "Combining UCT and Nested Monte Carlo Search for single-player general game playing", "journal": "IEEE Transactions on Computational Intelligence and AI in Games", "year": "2010", "authors": "J M\u00e9hat; T Cazenave"}, {"ref_id": "b16", "title": "Distributed nested rollout policy for samegame", "journal": "Springer", "year": "2017", "authors": "B Negrevergne; T Cazenave"}, {"ref_id": "b17", "title": "An unexpectedly effective monte carlo technique for the rna inverse folding problem", "journal": "", "year": "2018", "authors": "F Portela"}, {"ref_id": "b18", "title": "The vehicle routing problem with time windows part ii: genetic search", "journal": "INFORMS journal on Computing", "year": "1996", "authors": "J Y Potvin; S Bengio"}, {"ref_id": "b19", "title": "Generating structured test data with specific properties using nested monte-carlo search", "journal": "", "year": "2014", "authors": "S M Poulding; R Feldt"}, {"ref_id": "b20", "title": "Heuristic model checking using a monte-carlo tree search algorithm", "journal": "", "year": "2015-07-11", "authors": "S M Poulding; R Feldt"}, {"ref_id": "b21", "title": "Optimization of the Nested Monte-Carlo algorithm on the traveling salesman problem with time windows", "journal": "Springer", "year": "2011-04-27", "authors": "A Rimmel; F Teytaud; T Cazenave;  Evocomnet;  Evofin;  Evohot; Evostim Evomusart; Evotranslog "}, {"ref_id": "b22", "title": "Nested rollout policy adaptation for Monte Carlo Tree Search", "journal": "IJCAI", "year": "2011", "authors": "C D Rosin"}, {"ref_id": "b23", "title": "A new hashing method with application for game playing", "journal": "ICCA journal", "year": "1970", "authors": "A L Zobrist"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Fig. 1 :1Fig. 1: Evolution of the average scores of the three algorithms at SameGame.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Fig. 2 :2Fig. 2: Evolution of the average scores of the three algorithms for TSPTW.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Results for the first SameGame problem of the standard test suite.", "figure_data": "TimeNRPAGNRPA.betaGNRPA.beta.opt40.962435.12 (49.26)2513.35 (53.57)2591.46 (52.50)81.922676.39 (47.16)2749.33 (47.82)2777.83 (48.05)163.84 2838.99 (41.82)2887.78 (39.50)2907.23 (38.45)327.68 2997.74 (21.39)3024.68 (18.27)3057.78 (13.52)655.36 3081.25 (10.66)3091.44 (10.96)3116.54 ( 7.42)"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Results for the TSPTW rc204.1 problem Time NRPA GNRPA.beta GNRPA.beta.t.1.4 GNRPA.beta.t.1.4.opt", "figure_data": "40.96 -3745986.46 (245766.53 ) -897.60 (1.32 )-892.89 (0.96 )-892.17 (1.04 )81.92 -1750959.11 (243210.68 ) -891.04 (1.05 )-886.97 (0.87 )-886.52 (0.83 )163.84 -1030946.86 (212092.35 ) -888.44 (0.98 )-883.87 (0.71 )-884.07 (0.70 )327.68 -285933.63 (108975.99 ) -883.61 (0.63 )-880.76 (0.40 )-880.83 (0.32 )655.36-45918.97 (38203.97 ) -880.42 (0.30 )-879.35 (0.16 )-879.45 (0.17 )"}], "formulas": [{"formula_id": "formula_0", "formula_text": "p ik = e w ik \u03a3 j e w ij", "formula_coordinates": [4.0, 275.62, 253.73, 61.58, 29.41]}, {"formula_id": "formula_1", "formula_text": "\u03b4Ci \u03b4p ib = \u2212 1 p ib .", "formula_coordinates": [4.0, 410.34, 307.56, 54.11, 14.31]}, {"formula_id": "formula_2", "formula_text": "\u03b4p ib \u03b4w ij = p ib (\u03b4 bj \u2212 p ij )", "formula_coordinates": [4.0, 257.71, 346.41, 101.13, 27.79]}, {"formula_id": "formula_3", "formula_text": "\u2207w ij = \u03b4C i \u03b4p ib \u03b4p ib \u03b4w ij = \u2212 1 p ib p ib (\u03b4 bj \u2212 p ij ) = p ij \u2212 \u03b4 bj", "formula_coordinates": [4.0, 184.76, 413.55, 244.87, 27.79]}, {"formula_id": "formula_4", "formula_text": "w ij = w ij \u2212 \u03b1(p ij \u2212 \u03b4 bj )", "formula_coordinates": [4.0, 248.08, 485.35, 119.19, 11.5]}, {"formula_id": "formula_5", "formula_text": "p ik = e w ik \u03c4 +\u03b2 ik \u03a3 j e w ij \u03c4 +\u03b2 ij", "formula_coordinates": [4.0, 265.84, 617.91, 81.14, 35.81]}, {"formula_id": "formula_6", "formula_text": "f (x) = g (x)h(x) \u2212 h (x)g(x) h 2 (x)", "formula_coordinates": [5.0, 242.13, 163.01, 129.9, 22.31]}, {"formula_id": "formula_7", "formula_text": "\u03b4p ib \u03b4w ib =", "formula_coordinates": [5.0, 199.83, 219.2, 30.47, 23.23]}, {"formula_id": "formula_8", "formula_text": "\u03b4p ib \u03b4w ib = 1 \u03c4 p ib (1 \u2212 p ib )", "formula_coordinates": [5.0, 263.43, 294.43, 89.68, 23.23]}, {"formula_id": "formula_9", "formula_text": "\u03b4p ib \u03b4w ij = \u2212 1 \u03c4 e w ij \u03c4 +\u03b2ij e w ib \u03c4 +\u03b2 ib (\u03a3 j e w ij \u03c4 +\u03b2ij ) 2 \u03b4p ib \u03b4w ij = \u2212 1 \u03c4 p ij p ib", "formula_coordinates": [5.0, 246.67, 344.12, 121.01, 64.04]}, {"formula_id": "formula_10", "formula_text": "\u2207w ij = \u03b4C i \u03b4p ib \u03b4p ib \u03b4w ij = \u2212 1 \u03c4 1 p ib p ib (\u03b4 bj \u2212 p ij ) = p ij \u2212 \u03b4 bj \u03c4", "formula_coordinates": [5.0, 179.16, 439.26, 254.88, 27.79]}, {"formula_id": "formula_11", "formula_text": "w ij = w ij \u2212 \u03b1 p ij \u2212 \u03b4 bj \u03c4", "formula_coordinates": [5.0, 251.44, 502.72, 110.33, 26.77]}, {"formula_id": "formula_12", "formula_text": "p 0ij = e 1 \u03c4 w0ij +\u03b2ij \u03a3 k e 1 \u03c4 w 0ik +\u03b2 ik w 1ij = w 0ij \u2212 \u03b1 \u03c4 (p 0ij \u2212 \u03b4 0bj ) p 1ij = e 1 \u03c4 w1ij +\u03b2ij \u03a3 k e 1 \u03c4 w 1ik +\u03b2 ik = e 1 \u03c4 w0ij \u2212 \u03b1 \u03c4 2 (p0ij \u2212\u03b4 0bj )+\u03b2ij \u03a3 k e 1 \u03c4 w 1ik +\u03b2 ik w 2ij = w 1ij \u2212 \u03b1 \u03c4 (p 1ij \u2212 \u03b4 1bj ) = w 0ij \u2212 \u03b1 \u03c4 (p 0ij \u2212 \u03b4 0bj + p 1ij \u2212 \u03b4 1bj )", "formula_coordinates": [6.0, 165.98, 127.25, 283.39, 135.21]}, {"formula_id": "formula_13", "formula_text": "p nij = e 1 \u03c4 wnij +\u03b2ij \u03a3 k e 1 \u03c4 w nik +\u03b2 ik = e w 0ij \u03c4 \u2212 \u03b1 \u03c4 2 (\u03a3 k p kij \u2212\u03b4 kbj )+\u03b2ij \u03a3 k e 1 \u03c4 w nik +\u03b2 ik", "formula_coordinates": [6.0, 200.28, 291.32, 212.31, 30.59]}, {"formula_id": "formula_14", "formula_text": "z[i] \u2190 0 4: for m \u2208 [0, len(code[i])[ do 5: o[i][m] \u2190 e policy[code[i][m]] \u03c4 +\u03b2(m) 6: z[i] \u2190 z[i] + o[i][m] 7:", "formula_coordinates": [8.0, 138.55, 201.55, 164.59, 54.89]}, {"formula_id": "formula_15", "formula_text": "i \u2208 [0, len(index)[ do 10: b \u2190 index[i] 11: for m \u2208 [0, len(code[i])[ do 12: policy[code[i][m]] \u2190 policy[code[i][m]] \u2212 \u03b1 \u03c4 ( o[i][m] z|i] \u2212 \u03b4 bm ) 13:", "formula_coordinates": [8.0, 134.77, 270.24, 271.38, 53.02]}], "doi": "10.1109/TCIAIG.2012.2186810"}