{"title": "We Need to Talk About Random Splits", "authors": "Anders S\u00f8gaard; Sebastian Ebert; Jasmijn Bastings; Katja Filippova; Google Research", "pub_date": "", "abstract": "Gorman and Bedrick (2019) argued for using random splits rather than standard splits in NLP experiments. We argue that random splits, like standard splits, lead to overly optimistic performance estimates. We can also split data in biased or adversarial ways, e.g., training on short sentences and evaluating on long ones. Biased sampling has been used in domain adaptation to simulate real-world drift; this is known as the covariate shift assumption. In NLP, however, even worst-case splits, maximizing bias, often under-estimate the error observed on new samples of in-domain data, i.e., the data that models should minimally generalize to at test time. This invalidates the covariate shift assumption. Instead of using multiple random splits, future benchmarks should ideally include multiple, independent test sets instead; if infeasible, we argue that multiple biased splits leads to more realistic performance estimates than multiple random splits.", "sections": [{"heading": "Introduction", "text": "It is common practice in NLP to collect and annotate a text corpus -and split it into training, development and test data. These splits are often based on the order in which texts were published or sampled, and are referred to as 'standard splits'. Gorman and Bedrick (2019) recently showed that system ranking results based on standard splits differ from results based on random splits and used this to argue in favor of using random splits. While perhaps less common, random splits are already used in probing (Elazar and Goldberg, 2018), interpretability (P\u00f6rner et al., 2018), as well as core NLP tasks (Yu et al., 2019;Geva et al., 2019). 1 Gorman and Bedrick (2019) focus on whether there is a significant performance difference \u03b4 between systems S 1 and S 2 ; M(G test , S 1 ) \u2212 M(G test , S 2 ), in their notation. They argue Mc-Nemar's test (Gillick and Cox, 1989) or bootstrap (Efron, 1981) can establish that \u03b4 = 0, using random splits to sample from G test . This, of course, relies on the assumption that data is representative, i.e., was sampled i.i.d. (Wolpert, 1996).\nIn reality, what Gorman and Bedrick (2019) call the true difference in system performance, i.e., \u03b4 = M(G test , S 1 ) \u2212 M(G test , S 2 ), is the system difference on data that users would expect the systems to work well on (see \u00a72 for practical examples) -and not just on the corpus that we have annotations for. Our corpus-based estimates of \u03b4 can in fact be very misleading, i.e., very different from performance on new samples of data. In this paper, we investigate how misleading our estimates can be: We show that random splits consistently over-estimate performance at test time. This favors systems that overfit. We investigate alternatives across a heterogeneous set of NLP tasks. Based on our experiments, our answer to community-wide overfitting to standard splits is not to use random splits but to collect more diverse data with different biases -or if that is not feasible, split your data in adversarial, not random, ways. In general, we observe that estimates of test time error are worst for random splits, slightly better for standard splits  (if those exist), better for heuristic and adversarial splits, but error still tends to be higher on new (in-domain) samples; see Figure 1.\nOur results not only refute the hypothesis that \u03b4 can be estimated using random splits (Gorman and Bedrick, 2019), 2 but also the covariate shift hypothesis (Shimodaira, 2000;Shah et al., 2020) that \u03b4 can be estimated using reweightings of the data. While biased splits are useful in the absence of multiple held-out samples, and have been proposed before (Karimi et al., 2015), 3 they often overestimate performance in the wild. Our code is made publicly available at https://github.com/ google-research/google-research/tree/ master/talk_about_random_splits.", "publication_ref": ["b12", "b8", "b26", "b42", "b9", "b10", "b7", "b41", "b12", "b12", "b35", "b33", "b15"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Experiments", "text": "We consider 7 different NLP tasks: POS tagging (like Gorman and Bedrick (2019)), two sentence representation probing tasks, headline generation, translation quality estimation, emoji prediction, and news classification. We experiment with these tasks, because they a) are diverse, b) have not been subject to decades of community-wide overfitting (with the exception of POS tagging), and c) three of them enabled temporal splits (see Appendix \u00a7A.5).", "publication_ref": ["b12"], "figure_ref": [], "table_ref": []}, {"heading": "Data splits", "text": "The datasets which we will use in our experiments are presented in Table 1. For all seven tasks, we will present results for standard splits when possible (POS, PROBING,QE, HEAD-LINES), random splits, heuristic and adversarial splits, as well as on new samples. In the case of EMOJIS, HEADLINES and NEWS, which are all time-stamped datasets, we leave out historically 2 Or cross-validation, as more recently proposed in Szyma\u0144ski and Gorman (2020). In this very interesting followup paper, about Bayesian inference of \u03b4, the authors write that their \"estimates are valid insofar as the data sets used to estimate the Bayesian models comprise a representative sample of a coherent population of data sets.\" Our results show how off this assumption is.\n3 Karimi et al. (2015) discuss temporal splits and splits based on neighbor-based heuristics that are similar in spirit to our worst-case splits. more recent data as our new samples. All new samples are in-domain samples of data where models are supposed to generalize, i.e, samples from similar text sources. 4 This is a key point: These are samples that any end user would expect decent NLP models to fair well on. Examples include a sample of newspaper articles from newspaper A for a POS tagger trained on articles from newspaper B; tweets sampled the day after the training data was sampled; or news headlines sampled from the same sources, but a year later.\nWe resample random splits multiple times (3-10 per task) and report average results. The heuristic splits are obtained by finding a sentence length threshold and putting the long sentences in the test split. We choose a threshold so that approximately 10% of the data ends up in this split. The idea of checking whether models generalize to longer sentences is not new; on the contrary, this goes back, at least, to early formal studies of recurrent neural networks, e.g., Siegelmann and Sontag (1992). In the \u00a7A.3, we present a few experiments with alternative heuristic splits, but in our main experiments we limit ourselves to splits based on sentence length.\nFinally, the adversarial splits are computed by approximately maximizing the Wasserstein distance between the splits. The Wasserstein distance is often used to measure divergence between distributions (Arjovsky et al., 2017;Tolstikhin et al., 2018;Shen et al., 2018;Shah et al., 2018), and while alternatives exist (Ben- David et al., 2006;Borgwardt et al., 2006), it is easy to compute and parameter-free. Since selecting the worst-case split is an NP-hard problem (e.g., by reduction of the knapsack problem), we have to rely on an approximation. We first compute a ball tree encoding the Wasserstein distances between the data points in our sample. We then randomly select a centroid for our test split and find its k nearest neighbors. Those k nearest neighbors constitute our test split; the rest is used to train and validate our model. We repeat these steps to estimate performance on worst-case splits of our sample. See \u00a7A.4 for an algorithm sketch. Random, heuristic, and adversarial results are averaged across five runs.", "publication_ref": ["b38", "b15", "b36", "b0", "b39", "b34", "b32", "b1", "b3"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "POS tagging", "text": "We first consider the task in Gorman and Bedrick (2019), experiment with heuristic and adversarial splits of the original Penn Treebank (Marcus et al., 1993), and add the Xinhua section of OntoNotes 5.0 5 as our New Sample. Our tagger is NCRF ++ with default parameters. 6 Probing We also include two SentEval probing tasks (Conneau et al., 2018) with data from the Toronto Book Corpus: PROBING-WC (word classification) and PROBING-BSHIFT (whether a bigram was swapped) (Conneau et al., 2018). Unlike the other probing tasks, these two tasks do not rely on external syntactic parsers, which would otherwise introduce a new type of bias that we would have to take into account in our analysis. We use the official SentEval framework 7 and BERT (Devlin et al., 2019) as our sentence encoder. The probing model is a logistic regression classifier with L 2 regularization, tuned on the development set. As our New Samples, we use five random samples of the 2018 Gutenberg Corpus 8 for each task, preprocessed in the same way as Conneau et al. (2018).\nQuality estimation We use the WMT 2014 shared task datasets for QUALITY ESTIMATION. Specifically, we use the Spanish-English data from Task 1.1: scoring for perceived post-editing effort. The dataset comes with a training and test set, and a second, unofficial test set, which we use as our New Sample. In the \u00a7A.2, we also present results training on Spanish-English and evaluating on German-English. We present a simple model that only considers the target sentence, but performs better than the best shared task systems: we train an MLP over a LASER sentence embedding (Schwenk et al., 2019) with the following hyperparameters: two hidden layers with 100 parameters each and ReLU activation functions, trained using the Adam stochastic gradient-based optimizer (Kingma and Ba, 2015), a batch size of 200, and L 2 penalty of strength \u03b1 = 0.01.", "publication_ref": ["b21", "b4", "b4", "b6", "b4", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "Headline generation", "text": "We use the standard dataset for headline generation, derived from the Gigaword corpus (Napoles et al., 2012), as published by Rush et al. (2015). The task is to generate a headline from the first sentence of a news article. Our architecture is a sequence-to-sequence model with stacked bi-directional LSTMs with dropout, attention (Luong et al., 2015) and beam decoding; the number of hidden units is 128; we do not pre-train. Different from Rush et al. (2015), we use subword units (Sennrich et al., 2016) to overcome the OOV problem and speed up training. The ROUGE scores we obtain on the standard splits are higher than those reported by Rush et al. (2015) and comparable to those of Nallapati et al. (2016) Emoji prediction Go et al. (2009) introduce an emoji prediction dataset, collected from Twitter and is time-stamped. We use the 67,980 tweets from June 16 as our New Sample, and tweets from all previous days for the remaining experiments. For this task, we again train an MLP over a LASER embedding (Schwenk et al., 2019) with hyper-parameters: two hidden layers with 50 parameters each and ReLU activation functions, trained using the Adam stochastic gradient-based optimizer (Kingma and Ba, 2015), a batch size of 200, and L 2 penalty of strength \u03b1 = 0.01. See \u00a75 for a discussion of temporal drift in this data.", "publication_ref": ["b23", "b28", "b19", "b28", "b31", "b28", "b22", "b11", "b30", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "News classification We use a UCI Machine", "text": "Learning Repository text classification problem. 9 Our datapoints are headlines associated with five different news genres. We use the last year of this corpus as our New Sample. We sample 100,000 headlines from the rest and train an MLP over a LASER embedding (Schwenk et al., 2019) with the following hyper-parameters: two hidden layers with 100 parameters and ReLU activation functions, trained using the Adam stochastic gradientbased optimizer (Kingma and Ba, 2015), dynamic batch sizes, and L 2 penalty of strength \u03b1 = 0.01.  We bold face the lowest error reduction, i.e., where results differ the most from the random baseline. We see that standard and random splits consistently over-estimate real performance on new samples, which is sometimes even lower than performance on adversarial splits. We also report the mean squared error (MSE) with respect to New Samples, which shows Adversarial estimates empirical error best. Note: While annotator bias could explain POS tagging results, there is no annotator bias in the other tasks. * : For HEADLINES we use an identity baseline. Scores are ROUGE-2; see \u00a7A.1 for more. \u2020 : For QUALITY ESTIMATION, we report RMSE. The WMT QE 2014 best system obtained RMSE of 0.64; our system is significantly better with 0.50 on the standard split.", "publication_ref": ["b30"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Our results are presented in Table 2. Since the results are computed on different subsamples of data, we report error reductions over multinomial random (or, for HEADLINES, identity) baselines, following previous work comparing system rankings across different samples (S\u00f8gaard, 2013). More formally, we present error reduction as r = ps\u2212p b 1\u2212p b , where p s and p b are the performances of the system at hand and the multinomial random baseline.\nOur main observations are the following: (a) Random splits (and standard splits) consistently under-estimate error on new samples. The absolute differences between error reductions over random baselines for random splits and on new samples are often higher than 20%, and in the case of PROBING-BSHIFT, for example, the BERT model reduces 80% of the error of a random baseline when data is randomly split, but only 45% averaging over five samples of new data from the same domain. (b) Heuristic splits sometimes under-estimate error on new samples. Our heuristic splits in the above experiments are quite aggressive. We only evaluate our models on sentences that are longer than any of the sentences observed during training. Nevertheless for 5/7 tasks, this leads to more optimistic performance estimates than evaluating on new samples! (c) The same story holds for adversarial splits based on approximate maximization of Wasserstein distances between training and test data. While adversarial splits are very challenging, results on adversarial splits are more optimistic than on new samples in 4/7 cases. Note the fact that random splits over-estimate real-life performance also leads to misleading system rankings. If, for example, we remove the CRF inference layer from our POS tagger, performance on our Random splits drops to 0.952; on the New Sample, however, performance is 0.930, which is significantly better than with a CRF layer.", "publication_ref": ["b37"], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Discussion", "text": "In the spirit of earlier work (Sakaguchi et al., 2017;Madnani and Cahill, 2018;Gorman and Bedrick, 2019), we provide recommendations for future evaluation protocols: (i) In the absence of multiple held-out samples, using biased splits better approximates real-world performance and can help determine what data characteristics affect performance. (ii) Evaluating on new samples is superior and also enables significance testing across datasets (Demsar, 2006), providing confidence estimates. Several benchmarks already provide multiple, diverse test sets (e.g. Hovy et al., 2006;Petrov and McDonald, 2012;Williams et al., 2018); we hope more will follow. What explains the high variance across samples in NLP? One reason is the dimensionality of language (Bengio et al., 2003), but in \u00a7A.5 we also show significant impact of temporal drift.", "publication_ref": ["b29", "b20", "b12", "b5", "b14", "b25", "b40", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusions", "text": "We have shown that out-of-sample error can be hard to estimate from random splits, which tend to underestimate error by some margin, but even biased and adversarial splits sometimes underestimate error on new samples. We show this phenomenon across seven very different NLP tasks and provide practical recommendations on how to best bridge the gap between experimental practices and what is needed to produce truly robust NLP models that perform well in the wild.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Appendices", "text": "We present supplementary details about two of our tasks in \u00a7A.1 and \u00a7A.2 and discuss variations over heuristic splits in \u00a7A.3. In \u00a7A.4, we present the pseudo-algorithm for how we compute adversarial splits, and finally, in \u00a7A.5, we present our results documenting temporal drift.\nA.1 Headlines  2 gives more details on an interesting drift phenomenon, which contributed to the superior performance of the model trained on the most recent five years (1999)(2000)(2001)(2002)(2003). Apparently, the dotless spelling of U.S./US ('United States') became more common over time. Consequently, the model trained on the 1999-2003 part generated US more frequently than the model trained on 1994-1998.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Quality Estimation", "text": "In the results above, we train and test our quality estimation regressor on Spanish-English from WMT QE 2014. We also ran a similar experiment where we used the German-English test data as our New Sample. Here, we see a similar pattern to the one above: The RMSE on the Standard split was 0.630, which is slightly higher than for Spanish-English; with our Heuristic split, RMSE is 0.652; for Adversarial, it is 0.626 (which is slightly better than with standard splits), and on our New Sample, RMSE is 0.813.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.3 Alternative Heuristic Splits", "text": "For both SentEval tasks we experimented with the following alternatives for heuristic splits. Rare Words Another alternative for heuristic splits is to use word frequency information. Here we assign those sentences containing at least one of the rarest words of the dataset to the test set. This way we end up again with approximately 10% of the data in the test set. Note that this way we create only 1 dataset, because it's not a random process.\nResults Table 4 lists the results. While bootstrap resampling leads to slightly lower error reduction than cross-validation we decided to report the latter in the main part of this paper, because it is a more wide-spread way to randomly split datasets. Random Length results are comparable to standard splits results. The split based on word frequency (Rare Words) leads to considerable drop in both tasks. However, it is not as strong as the drop of the heuristic split (length threshold) in the main part of the paper.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_11"]}, {"heading": "A.4 Computing adversarial splits", "text": "We present the pseudo-algorithm of our implementation of approximate Wasserstein splitting in Algorithm 1. We also make the corresponding code available as part of our code repository for this paper.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We would like to thank our reviewers for their comments and for engaging in an interesting discussion. The paper also benefited greatly from discussions with several of our colleagues at Google Research, including Slav Petrov and Sascha Rothe.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "A.5 The significance of drift Some of our splits in the main experiments were based on slicing data into different time periods (HEADLINES, EMOJIS). Since temporal drift is a potential explanation for sampling bias, we analyze this in more detail here. We show that temporal drift is pervasive and leads to surprising drops in performance. We note, however, that temporal drift is not the only cause of sampling bias, of course. Since we have time stamps for two of our datasets we study these in greater detail. For similar studies of temporal drift, see Lukes and S\u00f8gaard (2018); Rijhwani and Preotiuc-Pietro (2020). As Table 5 indicates, shifting the training data by five years to the past results in a big performance drop. Sampling training data randomly or taking the most recent period produces models with similar ROUGE scores, both much better than the identity baseline. However, about half of the gap to the identity baseline disappears when older training data is taken. In the \u00a7A.1, we give an example of temporal drift in the HEADLINES data: US largely replaces U.S. in the newer training set and the test set.\nEmoji prediction For emoji prediction, Go et al. (2009) provide data for a temporal span of 62 days. We split the data into single days and keep the splits with more than 25,000 datapoints in which both classes are represented. We use the last of these, June 16, as our test sample and vary the training data from the first day to the day before June 16. Figure 3 (left) visualizes the results.", "publication_ref": ["b18", "b27", "b11"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Wasserstein generative adversarial networks", "journal": "PMLR", "year": "2017-08-11", "authors": "Mart\u00edn Arjovsky; Soumith Chintala; L\u00e9on Bottou"}, {"ref_id": "b1", "title": "Analysis of representations for domain adaptation", "journal": "MIT Press", "year": "2006-12-04", "authors": "Shai Ben-David; John Blitzer; Koby Crammer; Fernando Pereira"}, {"ref_id": "b2", "title": "A neural probabilistic language model", "journal": "J. Mach. Learn. Res", "year": "2003", "authors": "Yoshua Bengio; R\u00e9jean Ducharme; Pascal Vincent; Christian Janvin"}, {"ref_id": "b3", "title": "Integrating structured biological data by kernel maximum mean discrepancy", "journal": "", "year": "2006-08-06", "authors": "Karsten M Borgwardt; Arthur Gretton; Malte J Rasch; Hans-Peter Kriegel; Bernhard Sch\u00f6lkopf; Alexander J Smola"}, {"ref_id": "b4", "title": "What you can cram into a single \\$&!#* vector: Probing sentence embeddings for linguistic properties", "journal": "Long Papers", "year": "2018-07-15", "authors": "Alexis Conneau; Germ\u00e1n Kruszewski; Guillaume Lample; Lo\u00efc Barrault; Marco Baroni"}, {"ref_id": "b5", "title": "Statistical comparisons of classifiers over multiple data sets", "journal": "J. Mach. Learn. Res", "year": "2006", "authors": "Janez Demsar"}, {"ref_id": "b6", "title": "BERT: pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019-06-02", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b7", "title": "Nonparametric estimates of standard error: the jackknife, the bootstrap and other methods", "journal": "Biometrika", "year": "1981", "authors": "Bradley Efron"}, {"ref_id": "b8", "title": "Adversarial removal of demographic attributes from text data", "journal": "Association for Computational Linguistics", "year": "2018-10-31", "authors": "Yanai Elazar; Yoav Goldberg"}, {"ref_id": "b9", "title": "Discofuse: A large-scale dataset for discourse-based sentence fusion", "journal": "", "year": "2019-06-02", "authors": "Mor Geva; Eric Malmi; Idan Szpektor; Jonathan Berant"}, {"ref_id": "b10", "title": "Some statistical issues in the comparison of speech recognition algorithms", "journal": "IEEE", "year": "1989-05-23", "authors": "L Gillick; Stephen J Cox"}, {"ref_id": "b11", "title": "Twitter sentiment classification using distant supervision", "journal": "", "year": "2009", "authors": "Alec Go; Richa Bhayani; Lei Huang"}, {"ref_id": "b12", "title": "We need to talk about standard splits", "journal": "Long Papers", "year": "2019-07-28", "authors": "Kyle Gorman; Steven Bedrick"}, {"ref_id": "b13", "title": "Domainspecific disambiguation for typing with ambiguous keyboards", "journal": "Association for Computational Linguistics", "year": "2003", "authors": "Karin Harbusch; Sa\u0161a Hasan; Hajo Hoffmann; Michael K\u00fchn; Bernhard Sch\u00fcler"}, {"ref_id": "b14", "title": "OntoNotes: The 90% solution", "journal": "", "year": "2006", "authors": "Eduard Hovy; Mitchell Marcus; Martha Palmer; Lance Ramshaw; Ralph Weischedel"}, {"ref_id": "b15", "title": "Evaluation methods for statistically dependent text", "journal": "Comput. Linguistics", "year": "2015", "authors": "Sarvnaz Karimi; Jie Yin; Jiri Baum"}, {"ref_id": "b16", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2015-05-07", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b17", "title": "Six challenges for neural machine translation", "journal": "", "year": "2017-08-04", "authors": "Philipp Koehn; Rebecca Knowles"}, {"ref_id": "b18", "title": "Sentiment analysis under temporal shift", "journal": "", "year": "2018", "authors": "Jan Lukes; Anders S\u00f8gaard"}, {"ref_id": "b19", "title": "Effective approaches to attention-based neural machine translation", "journal": "", "year": "2015", "authors": "Thang Luong; Hieu Pham; Christopher D Manning"}, {"ref_id": "b20", "title": "Automated scoring: Beyond natural language processing", "journal": "", "year": "2018-08-20", "authors": "Nitin Madnani; Aoife Cahill"}, {"ref_id": "b21", "title": "Building a large annotated corpus of english: The penn treebank", "journal": "Comput. Linguistics", "year": "1993", "authors": "Mitchell P Marcus; Beatrice Santorini; Mary Ann Marcinkiewicz"}, {"ref_id": "b22", "title": "Abstractive text summarization using sequence-to-sequence RNNs and beyond", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Ramesh Nallapati; Bowen Zhou;  Cicero Dos Santos; Bing Aglar Gu\u00ec \u2021l\u00e7ehre;  Xiang"}, {"ref_id": "b23", "title": "Annotated Gigaword", "journal": "", "year": "2012", "authors": "Courtney Napoles; Matthew Gormley; Benjamin Van Durme"}, {"ref_id": "b24", "title": "", "journal": "Association for Computational Linguistics", "year": "", "authors": "Canada Montr\u00e9al"}, {"ref_id": "b25", "title": "Overview of the 2012 shared task on parsing the web", "journal": "", "year": "2012", "authors": "Slav Petrov; Ryan Mcdonald"}, {"ref_id": "b26", "title": "Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement", "journal": "Association for Computational Linguistics", "year": "2018-07-15", "authors": "Nina P\u00f6rner; Hinrich Sch\u00fctze; Benjamin Roth"}, {"ref_id": "b27", "title": "Temporally-informed analysis of named entity recognition", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Shruti Rijhwani; Daniel Preotiuc-Pietro"}, {"ref_id": "b28", "title": "A neural attention model for abstractive sentence summarization", "journal": "Association for Computational Linguistics", "year": "2015", "authors": "Alexander M Rush; Sumit Chopra; Jason Weston"}, {"ref_id": "b29", "title": "GEC into the future: Where are we going and how do we get there?", "journal": "", "year": "2017-09-08", "authors": "Keisuke Sakaguchi; Courtney Napoles; Joel R Tetreault"}, {"ref_id": "b30", "title": "Ccmatrix: Mining billions of high-quality parallel sentences on the", "journal": "", "year": "2019", "authors": "Holger Schwenk; Guillaume Wenzek; Sergey Edunov; Edouard Grave; Armand Joulin"}, {"ref_id": "b31", "title": "Neural machine translation of rare words with subword units", "journal": "Long Papers", "year": "2016", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"ref_id": "b32", "title": "Adversarial domain adaptation for duplicate question detection", "journal": "", "year": "2018-10-31", "authors": "J Darsh; Tao Shah; Alessandro Lei; Salvatore Moschitti; Preslav Romeo;  Nakov"}, {"ref_id": "b33", "title": "Predictive biases in natural language processing models: A conceptual framework and overview", "journal": "", "year": "2020-07-05", "authors": "Deven Shah; H Andrew Schwartz; Dirk Hovy"}, {"ref_id": "b34", "title": "Wasserstein distance guided representation learning for domain adaptation", "journal": "AAAI Press", "year": "2018-02-02", "authors": "Jian Shen; Yanru Qu; Weinan Zhang; Yong Yu"}, {"ref_id": "b35", "title": "Improving predictive inference under covariate shift by weighting the loglikelihood function", "journal": "Journal of Statistical Planning and Inference", "year": "2000", "authors": "Hidetoshi Shimodaira"}, {"ref_id": "b36", "title": "On the computational power of neural nets", "journal": "ACM", "year": "1992-07-27", "authors": "T Hava; Eduardo D Siegelmann;  Sontag"}, {"ref_id": "b37", "title": "Estimating effect size across datasets", "journal": "", "year": "2013-06-09", "authors": "Anders S\u00f8gaard"}, {"ref_id": "b38", "title": "Is the best better? Bayesian statistical model comparison for natural language processing", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Piotr Szyma\u0144ski; Kyle Gorman"}, {"ref_id": "b39", "title": "Wasserstein autoencoders", "journal": "", "year": "2018-04-30", "authors": "O Ilya; Olivier Tolstikhin; Sylvain Bousquet; Bernhard Gelly;  Sch\u00f6lkopf"}, {"ref_id": "b40", "title": "A broad-coverage challenge corpus for sentence understanding through inference", "journal": "Long Papers", "year": "2018", "authors": "Adina Williams; Nikita Nangia; Samuel Bowman"}, {"ref_id": "b41", "title": "The lack of A priori distinctions between learning algorithms", "journal": "Neural Comput", "year": "1996", "authors": "H David;  Wolpert"}, {"ref_id": "b42", "title": "What you see is what you get: Visual pronoun coreference resolution in dialogues", "journal": "", "year": "2019-11-03", "authors": "Xintong Yu; Hongming Zhang; Yangqiu Song; Yan Song; Changshui Zhang"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Data splitting strategies. Each ball corresponds to a sentence represented in (two-dimensional) feature space. Blue (dark)/orange (bright) balls represent examples for training/test. Numbers represent sentence length. Heuristic splits can, e.g., be based on sentence length; adversarial splits maximize divergence.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Data used in our experiments. *: We time slice the original data to create different samples.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Error reductions over random baselines on Standard (original) splits, if available, Random splits (obtained using cross-validation), Heuristic splits resulting from a sentence length-based threshold, Adversarial splits based on (five) approximate maximizations of Wasserstein differences between splits, and on New Samples.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Proportions of US vs. U.S. spellings in the headlines for two training sets and the test set (2004) as well as in the two models' predictions on the test set.", "figure_data": ": Error reduction as compared with an identitybaseline (output as input) for three ROUGE metrics.Random is a five-fold cross-validation result.Figure 2:"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "", "figure_data": "reports the error reduction in ROUGE-1,ROUGE-2 and ROUGE-L over the identity base-line (see  \u00a72) for the different data splits. The re-sults are consistent with Table 2. Figure"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "to be part of the test set. We repeat this procedure until approximately 10% of the data ends up in the test set. With this procedure we create 5 different test sets. We included this heuristic in order to see how fragile the probing setup is.", "figure_data": "Bootstrap Resampling Instead of cross-validation, a random split can be generated bybootstrap resampling. For this we randomly select10% of the data as test set and then randomlysample (with replacement) a new training and devset from the remaining examples.Random Length As alternative to the lengththreshold heuristic in earlier experiments we ran-domly sample a length and select all examples hav-ing this length"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Error reductions over random baselines on Standard (original) splits, if available, Bootstrap splits, Random Length splits resulting from a sentence length-based separation, Rare Words splits based on word frequency.", "figure_data": "0.700MLP0.6950.690Accuracy0.680 0.6850.6750.6700.66501020304050DaysFigure 3: Temporal drift in emoji prediction. The cor-relation between temporal gap and performance is sig-nificant (p < 0.05)."}], "formulas": [], "doi": "10.1093/bioinformatics/btl242"}