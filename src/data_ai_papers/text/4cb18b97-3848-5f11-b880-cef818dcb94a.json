{"title": "We need to talk about standard splits", "authors": "Kyle Gorman; Steven Bedrick", "pub_date": "", "abstract": "It is standard practice in speech & language technology to rank systems according to performance on a test set held out for evaluation. However, few researchers apply statistical tests to determine whether differences in performance are likely to arise by chance, and few examine the stability of system ranking across multiple training-testing splits. We conduct replication and reproduction experiments with nine part-of-speech taggers published between 2000 and 2018, each of which reports state-of-the-art performance on a widely-used \"standard split\". We fail to reliably reproduce some rankings using randomly generated splits. We suggest that randomly generated splits should be used in system comparison.", "sections": [{"heading": "Introduction", "text": "Evaluation with a held-out test set is one of the few methodological practices shared across nearly all areas of speech and language processing. In this study we argue that one common instantiation of this procedure-evaluation with a standard splitis insufficient for system comparison, and propose an alternative based on multiple random splits.\nStandard split evaluation can be formalized as follows. Let G be a set of ground truth data, partitioned into a training set G train , a development set G dev and a test (evaluation) set G test . Let S be a system with arbitrary parameters and hyperparameters, and let M be an evaluation metric. Without loss of generality, we assume that M is a function with domain G \u00d7 S and that higher values of M indicate better performance. Furthermore, we assume a supervised training scenario in which the free parameters of S are set so as to maximize M(G train , S), optionally tuning hyperparameters so as to maximize M(G dev , S). Then, if S 1 and S 2 are competing systems so trained, we prefer S 1 to S 2 if and only if M(G test , S 1 ) > M(G test , S 2 ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Hypothesis testing for system comparison", "text": "One major concern with this procedure is that it treats M(G test , S 1 ) and M(G test , S 2 ) as exact quantities when they are better seen as estimates of random variables corresponding to true system performance. In fact many widely used evaluation metrics, including accuracy and F-score, have known statistical distributions, allowing hypothesis testing to be used for system comparison.\nFor instance, consider the comparison of two systems S 1 and S 2 trained and tuned to maximize accuracy. The difference in test accuracy\n,\u03b4 = M(G test , S 1 ) \u2212 M(G test , S 2 )\n, can be thought of as estimate of some latent variable \u03b4 representing the true difference in system performance. While the distribution of\u03b4 is not obvious, the probability that there is no population-level difference in system performance (i.e., \u03b4 = 0) can be computed indirectly using McNemar's test (Gillick and Cox, 1989). Let n 1>2 be the number of samples in G test which S 1 correctly classifies but S 2 misclassifies, and n 2>1 be the number of samples which S 1 misclassifies but S 2 correctly classifies. When \u03b4 = 0, roughly half of the disagreements should favor S 1 and the other half should favor S 2 . Thus, under the null hypothesis, n 1>2 \u223c Bin(n, .5) where n = n 1>2 + n 2>1 . And, the (one-sided) probability of the null hypothesis is the probability of sampling n 1>2 from this distribution. Similar methods can be used for other evaluation metrics, or a reference distribution can be estimated with bootstrap resampling (Efron, 1981).\nDespite this, few recent studies make use of statistical system comparison. Dror et al. (2018) survey statistical practices in all long papers presented at the 2017 meeting of the Association for Computational Linguistics (ACL), and all articles published in the 2017 volume of the Transactions of the ACL. They find that the majority of these works do not use appropriate statistical tests for system comparison, and many others do not report which test(s) were used. We hypothesize that the lack of hypothesis testing for system comparison may lead to type I error, the error of rejecting a true null hypothesis. As it is rarely possible to perform the necessary hypothesis tests from published results, we evaluate this risk using a replication experiment.", "publication_ref": ["b12", "b8", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Standard vs. random splits", "text": "Furthermore, we hypothesize that standard split methodology may be insufficient for system evaluation. While evaluations based on standard splits are an entrenched practice in many areas of natural language processing, the static nature of standard splits may lead researchers to unconsciously \"overfit\" to the vagaries of the training and test sets, producing poor generalization. This tendency may also be amplified by publication bias in the sense of Scargle (2000). The field has chosen to define \"state of the art\" performance as \"the best performance on a standard split\", and few experiments which do not report improvements on a standard split are ultimately published. This effect is likely to be particularly pronounced on highly-saturated tasks for which system performance is near ceiling, as this increases the prior probability of the null hypothesis (i.e., of no difference). We evaluate this risk using a series of reproductions.", "publication_ref": ["b20"], "figure_ref": [], "table_ref": []}, {"heading": "Replication and reproduction", "text": "In this study we perform a replication and a series of reproductions. These techniques were until recently quite rare in this field, despite the inherently repeatable nature of most natural language processing experiments. Researchers attempting replications or reproductions have reported problems with availability of data (Mieskes, 2017;Wieling et al., 2018) and software (Pedersen, 2008), and various details of implementation (Fokkens et al., 2013;Reimers and Gurevych, 2017;Schluter and Varab, 2018). While we cannot completely avoid these pitfalls, we select a task-English part-ofspeech tagging-for which both data and software are abundantly available. This task has two other important affordances for our purposes. First, it is face-valid, both in the sense that the equivalence classes defined by POS tags reflect genuine linguistic insights and that standard evaluation metrics such as token and sentence accuracy directly measure the underlying construct. Secondly, POS tagging is useful both in zero-shot settings (e.g., Elkahky et al., 2018;Trask et al., 2015) and as a source of features for many downstream tasks, and in both settings, tagging errors are likely to propagate. We release the underlying software under a permissive license. 1", "publication_ref": ["b16", "b28", "b17", "b11", "b19", "b21", "b9", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Materials & Methods", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data", "text": "The Wall St. Journal (WSJ) portion of Penn Treebank-3 (LDC99T42; Marcus et al., 1993) is commonly used to evaluate English part-of-speech taggers. In experiment 1, we also use a portion of OntoNotes 5 (LDC2013T19; Weischedel et al., 2011), a substantial subset of the Penn Treebank WSJ data re-annotated for quality assurance.", "publication_ref": ["b15", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "Models", "text": "We attempted to choose a set of taggers claiming state-of-the-art performance at time of publication. We first identified candidate taggers using the \"State of the Art\" page for part-of-speech tagging on the ACL Wiki. 2 We then selected nine taggers for which all needed software and external data was available at time of writing. These taggers are described in more detail below.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Metrics", "text": "Our primarily evaluation metric is token accuracy, the percentage of tokens which are correctly tagged with respect to the gold data. We compute 95% Wilson (1927) score confidence intervals for accuracies, and use the two-sided mid-p variant (Fagerland et al., 2013) of McNemar's test for system comparison. We also report out-of-vocabulary (OOV) accuracy-that is, token accuracy limited to tokens not present in the training data-and sentence accuracy, the percentage of sentences for which there are no tagging errors.", "publication_ref": ["b29", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Table 1 reports statistics for the standard split. The OntoNotes sample is slightly smaller as it omits sentences on financial news, most of which is highly redundant and idiosyncratic. However, the entire OntoNotes sample was tagged by a single experienced annotator, eliminating any annotatorspecific biases in the Penn Treebank (e.g., Ratnaparkhi, 1997, 137f.).  ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Models", "text": "Three models-SVMTool (Gim\u00e9nez and M\u00e0rquez, 2004), MElt (Denis and Sagot, 2009), and Mor\u010de/COMPOST (Spoustov\u00e1 et al., 2009)produced substantial compilation or runtime errors. However, we were able to perform replication with the remaining six models:\n\u2022 TnT (Brants, 2000): a second-order (i.  3 We use an implementation by Yarmohammadi (2014).\nembedding features, a cross-entropy objective optimized with stochastic gradient descent, decoded globally", "publication_ref": ["b13", "b5", "b22", "b2", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "Experiment 1: Replication", "text": "In experiment 1, we adopt the standard split established by Collins (2002): sections 00-18 are used for training, sections 19-21 for development, and sections 22-24 for testing, roughly a 80%-10%-10% split. We train and evaluate the six remaining taggers using this standard split. For each tagger, we train on the training set and evaluate on the test set. For taggers which support it, we also perform automated hyperparameter tuning on the development set. Results are shown in Table 2. We obtain exact replications for TnT and LAPOS, and for the remaining four taggers, our results are quite close to previously reported numbers. Token accuracy, OOV accuracy, and sentence accuracy give the same ranking, one consistent with published results. For Penn Treebank, McNemar's test on token accuracy is significant for all pairwise comparisons at \u03b1 = .05; for OntoNotes, one comparison is non-significant: LAPOS vs. Stanford (p = .1366).", "publication_ref": ["b4"], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Experiment 2: Reproduction", "text": "We now repeat these analyses across twenty randomly generated 80%-10%-10% splits. After Dror et al. (2017), we use the Bonferroni procedure to control familywise error rate, the probability of falsely rejecting at least one true null hypothesis. This is appropriate insofar as each individual trial (i.e, evaluation on a random split) has a non-trivial statistical dependence on other trials. Table 3 reports the number of random splits, out of twenty, where the McNemar test p-value is significant after the correction for familywise error rate. This provides a coarse estimate of how often the second system would be likely to significantly outperform the first system given a random partition of similar size. Most of these pairwise comparisons are stable across random trials. However, for example, Stanford tagger is not a significant improvement over LAPOS for nearly all random trials, and in some random trials-two for Penn Treebank, fourteen for OntoNotes-it is in fact worse. Recall also that the Stanford tagger was also not significantly better than LAPOS for OntoNotes in experiment 1.\nFigure 1 shows token accuracies across the two experiments. The last row of the figure gives results for an oracle ensemble which correctly pre-", "publication_ref": ["b6"], "figure_ref": ["fig_0"], "table_ref": ["tab_5"]}, {"heading": "Penn Treebank OntoNotes", "text": "Token OOV Sentence Token   dicts the tag just in case any of the six taggers predicts the correct tag.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Error analysis", "text": "From experiment 1, we estimate that the last two decades of POS tagging research has produced a 1.28% absolute reduction in token errors. At the same time, the best tagger is 1.16% below the oracle ensemble. Thus we were interested in disagreements between taggers. We investigate this by treating each of the six taggers as separate coders in a collaborative annotation task. We compute persentence inter-annotator agreement using Krippendorff's \u03b1 (Artstein and Poesio, 2008), then manually inspect sentences with the lowest \u03b1 values, i.e., with the highest rate of disagreement. By far the most common source of disagreement are \"headline\"-like sentences such as Foreign Bonds.\nWhile these sentences are usually quite short, high disagreement is also found for some longer headlines, as in the example sentence in table 4; the effect seems to be due more to capitalization than sentence length. Several taggers lean heavily on capitalization cues to identify proper nouns, and thus capitalized tokens in headline sentences are frequently misclassified as proper nouns and vice versa, as are sentence-initial capitalized nouns in general. Most other sentences with low \u03b1 have local syntactic ambiguities. For example, the word lining, acting as a common noun (NN) in the context \u2026a silver for the\u2026, is mislabeled as a gerund (VBG) by two of six taggers.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "Discussion", "text": "We draw attention to two distinctions between the replication and reproduction experiments. First, we find that a system judged to be significantly better than another on the basis of performance on the  standard split, does not in outperform that system on re-annotated data or randomly generated splits, suggesting that it is \"overfit to the standard split\" and does not represent a genuine improvement in performance. Secondly, as can be seen in figure 1, overall performance is slightly higher on the random splits. We posit this to be an effect of randomization at the sentence-level. For example, in the standard split the word asbestos occurs fifteen times in a single training set document, but just once in the test set. Such discrepancies are far less likely to arise in random splits. Diversity of languages, data, and tasks are all highly desirable goals for natural language processing. However, nothing about this demonstration depends on any particularities of the English language, the WSJ data, or the POS tagging task. English is a somewhat challenging language for POS tagging because of its relatively impoverished inflectional morphology and pervasive noun-verb ambiguity (Elkahky et al., 2018). It would not do to use these six taggers for other languages as they are designed for English text and in some cases depend on English-only external resources for feature generation. However, random split experiments could, for instance, be performed for the subtasks of the CoNLL-2018 shared task on multilingual parsing (Zeman et al., 2018).\nWe finally note that repeatedly training the Flair tagger in experiment 2 required substantial grid computing resources and may not be feasible for many researchers at the present time.", "publication_ref": ["b9", "b31"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Conclusions", "text": "We demonstrate that standard practices in system comparison, and in particular, the use of a single standard split, may result in avoidable Type I error. We suggest that practitioners who wish to firmly establish that a new system is truly state-of-the-art augment their evaluations with Bonferronicorrected random split hypothesis testing.\nIt is said that statistical praxis is of greatest import in those areas of science least informed by theory. While linguistic theory and statistical learning theory both have much to contribute to part-ofspeech tagging, we still lack a theory of the tagging task rich enough to guide hypothesis formation. In the meantime, we must depend on system comparison, backed by statistical best practices and error analysis, to make forward progress on this task.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We thank Mitch Marcus for valuable discussion of the Wall St. Journal data.\nSteven Bedrick was supported by the National Institute on Deafness and Other Communication Disorders of the National Institutes of Health under award number R01DC015999. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Contextual string embedding for sequence labeling", "journal": "", "year": "2018", "authors": "Alan Akbik; Duncan Blythe; Roland Vollgraf"}, {"ref_id": "b1", "title": "Inter-coder agreement for computational linguistics", "journal": "Computational Linguistics", "year": "2008", "authors": "Ron Artstein; Massimo Poesio"}, {"ref_id": "b2", "title": "TnT: a statistical part-of-speech tagger", "journal": "", "year": "2000", "authors": "Thorsten Brants"}, {"ref_id": "b3", "title": "Dynamic feature induction: The last gist to the state-of-the-art", "journal": "", "year": "2016", "authors": "D Jinho;  Choi"}, {"ref_id": "b4", "title": "Discriminative training methods for hidden Markov models: theory and experiments with perceptron algorithms", "journal": "", "year": "2002", "authors": "Michael Collins"}, {"ref_id": "b5", "title": "Coupling an annotated corpus and a morphosyntactic lexicon for state-of-the-art POS tagging with less human effort", "journal": "", "year": "2009", "authors": "Pascal Denis; Beno\u00eet Sagot"}, {"ref_id": "b6", "title": "Replicability analysis for natural language processing: testing significance with multiple datasets", "journal": "Transactions of the Association for Computational Linguistics", "year": "2017", "authors": "Rotem Dror; Gili Baumer; Marina Bogomolov; Roi Reichart"}, {"ref_id": "b7", "title": "The hitchhiker's guide to testing statistical significance in natural language processing", "journal": "", "year": "2018", "authors": "Rotem Dror; Gili Baumer"}, {"ref_id": "b8", "title": "Nonparametric estimates of standard error: the jackknife, the bootstrap and other methods", "journal": "Biometrika", "year": "1981", "authors": "Bradley Efron"}, {"ref_id": "b9", "title": "A challenge set and methods for nounverb ambiguity", "journal": "", "year": "2018", "authors": "Ali Elkahky; Kellie Webster; Daniel Andor; Emily Pitler"}, {"ref_id": "b10", "title": "The McNemar test for binary matched-pairs data: mid-p and asymptotic are better than exact conditional", "journal": "BMC Medical Research Methodology", "year": "2013", "authors": "Morten W Fagerland; Stian Lydersen; Petter Laake"}, {"ref_id": "b11", "title": "Offspring from reproduction problems: What replication failure teaches us", "journal": "", "year": "2013", "authors": "Antske Fokkens; Marten Marieke Van Erp; Ted Postma; Piek Pedersen; Nuno Vossen;  Freire"}, {"ref_id": "b12", "title": "Some statistical issues in the comparison of speech recognition algorithms", "journal": "", "year": "1989", "authors": "Larry Gillick; Stephen J Cox"}, {"ref_id": "b13", "title": "SVMTool: A general POS tagger generator based on support vector machines", "journal": "", "year": "2004", "authors": "Jes\u00fas Gim\u00e9nez; Llu\u00eds M\u00e0rquez"}, {"ref_id": "b14", "title": "Part-of-speech tagging from 97% to 100%: is it time for some linguistics?", "journal": "", "year": "2011", "authors": "Christopher D Manning"}, {"ref_id": "b15", "title": "Building a large annotated corpus of English: the Penn Treebank", "journal": "Computational Linguistics", "year": "1993", "authors": "Mitchell P Marcus; Mary Ann Marcinkiewicz; Beatrice Santorini"}, {"ref_id": "b16", "title": "A quantitative study of data in the NLP community", "journal": "", "year": "2017", "authors": "Margot Mieskes"}, {"ref_id": "b17", "title": "Empiricism is not a matter of faith", "journal": "Computational Linguistics", "year": "2008", "authors": "Ted Pedersen"}, {"ref_id": "b18", "title": "A maximum entropy model for part-of-speech tagging", "journal": "", "year": "1997", "authors": "Adwait Ratnaparkhi"}, {"ref_id": "b19", "title": "Reporting score distributions makes a difference: performance study of LSTM-networks for sequence tagging", "journal": "", "year": "2017", "authors": "Nils Reimers; Iryna Gurevych"}, {"ref_id": "b20", "title": "Publication bias: the \"filedrawer problem\" in scientific inference", "journal": "Journal of Scientific Exploration", "year": "2000", "authors": "D Jeffrey;  Scargle"}, {"ref_id": "b21", "title": "When data permutations are pathological: the case of neural natural language inference", "journal": "", "year": "2018", "authors": "Natalie Schluter; Daniel Varab"}, {"ref_id": "b22", "title": "Semi-supervised training for the averaged perceptron POS tagger", "journal": "", "year": "2009", "authors": "Drahom\u00edra Spoustov\u00e1; Jan Haji\u010d; Jan Raab; Miroslav Spousta"}, {"ref_id": "b23", "title": "Feature-rich part-ofspeech tagging with a cyclic dependency network", "journal": "", "year": "2003", "authors": "Kristina Toutanova; Dan Klein; Christopher D Manning; Yoram Singer"}, {"ref_id": "b24", "title": "sense2vec: A fast and accurate method for word sense disambiguation in neural word embeddings", "journal": "", "year": "2015", "authors": "Andrew Trask; Phil Michalak; John Liu"}, {"ref_id": "b25", "title": "Learning with lookahead: can history-based models rival globally optimized models?", "journal": "", "year": "2011", "authors": "Yoshimasa Tsuruoka; Yusuke Miyao; Jun'ichi Kazama"}, {"ref_id": "b26", "title": "Stochastic gradient descent training for L1-regularized log-linear models with cumulative penalty", "journal": "", "year": "2009", "authors": "Yoshimasa Tsuruoka; Sophia Tsujii;  Ananiadou"}, {"ref_id": "b27", "title": "OntoNotes: a large training corpus for enhanced processing", "journal": "Springer", "year": "2011", "authors": "Ralph Weischedel; Eduard Hovy; Mitchell P Marcus; Martha Palmer; Robert Belvin; Sameer Pradhan; \u2026 ; Nianwen Xue"}, {"ref_id": "b28", "title": "Reproducibility in computational linguistics: are we willing to share?", "journal": "Computational Linguistics", "year": "2018", "authors": "Martijn Wieling; Josine Rawee; Gertjan Van Noord"}, {"ref_id": "b29", "title": "Probable inference, the law of succession, and statistical inference", "journal": "Journal of the American Statistical Association", "year": "1927", "authors": "Edwin B Wilson"}, {"ref_id": "b30", "title": "Discriminative training with perceptron algorithm for POS tagging task", "journal": "", "year": "2014", "authors": "Mahsa Yarmohammadi"}, {"ref_id": "b31", "title": "CoNLL 2018 shared task: multilingual parsing from raw text to Universal Dependencies", "journal": "", "year": "2018", "authors": "Daniel Zeman; Martin Popel; Milan Straka; Jan Haji\u010d; Joakim Nivre; Filip Ginter; \u2026 ; Josie Li"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: A visualization of Penn Treebank token accuracies in the two experiments. The whiskers shows accuracy and 95% confidence intervals in experiment 1, and shaded region represents the range of accuracies in experiment 2.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Summary statistics for the standard split.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Previously reported, and replicated, accuracies for the standard split of the WSJ portion of Penn Treebank; we also provide token accuracies for a reproduction with the WSJ portion of OntoNotes.", "figure_data": "PTB ONTnTvs. Collins2020Collins vs. LAPOS207LAPOS vs. Stanford10Stanford vs. NLP4J1920NLP4J vs. Flair2020"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "The number of random trials (out of twenty) for which the second system has significantly higher token accuracy than the first after Bonferroni correction. PTB, Penn Treebank; ON, OntoNotes.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Example error analysis for a Penn Treebank sentence; \u03b1 = .521.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": ",\u03b4 = M(G test , S 1 ) \u2212 M(G test , S 2 )", "formula_coordinates": [1.0, 307.28, 386.89, 218.27, 24.47]}], "doi": ""}