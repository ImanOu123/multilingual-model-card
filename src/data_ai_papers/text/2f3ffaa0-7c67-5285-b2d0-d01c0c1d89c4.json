{"title": "G-Mixup: Graph Data Augmentation for Graph Classification", "authors": "Xiaotian Han; Zhimeng Jiang; Ninghao Liu; Xia Hu", "pub_date": "", "abstract": "This work develops mixup for graph data. Mixup has shown superiority in improving the generalization and robustness of neural networks by interpolating features and labels between two random samples. Traditionally, Mixup can work on regular, grid-like, and Euclidean data such as image or tabular data. However, it is challenging to directly adopt Mixup to augment graph data because different graphs typically: 1) have different numbers of nodes; 2) are not readily aligned; and 3) have unique typologies in non-Euclidean space. To this end, we propose G-Mixup to augment graphs for graph classification by interpolating the generator (i.e., graphon) of different classes of graphs. Specifically, we first use graphs within the same class to estimate a graphon. Then, instead of directly manipulating graphs, we interpolate graphons of different classes in the Euclidean space to get mixed graphons, where the synthetic graphs are generated through sampling based on the mixed graphons. Extensive experiments show that G-Mixup substantially improves the generalization and robustness of GNNs.", "sections": [{"heading": "Introduction", "text": "Recently deep learning has been widely adopted to graph analysis. Graph Neural Networks (GNNs) (Wu et al., 2020;Zhou et al., 2020a;Zhang et al., 2020;Xu et al., 2018) have shown promising performance on graph classification. Meanwhile, data augmentation (e.g., DropEdge (Rong et al., 2020), Subgraph (You et al., 2020;Wang et al., 2020a) ) has also been adopted to graph analysis by generating synthetic graphs to create more training data for improving the generalization of graph classification models. However, existing graph data augmentation strategies typically aim to augment graphs at a within-graph level by either modifying Preprint edges or nodes in an individual graph, which does not enable information exchange between different instances. The between-graph augmentation methods (i.e., data augmentation between graphs) are still under-explored.\nIn parallel with the development of graph neural networks, Mixup (Zhang et al., 2017) and its variants (e.g., Manifold Mixup (Verma et al., 2019a)), as data augmentation methods, have been theoretically and empirically shown to improve the generalization and robustness of deep neural networks in image recognition (Zhang et al., 2017;Verma et al., 2019a;Zhang et al., 2021) and natural language processing (Guo et al., 2019a;Guo, 2020). The basic idea of Mixup is to linearly interpolate continuous values of random sample pairs to generate more synthetic training data. The formal mathematical expression of Mixup is x new = \u03bbx i + (1 \u2212 \u03bb)x j , y new = \u03bby i +(1\u2212\u03bb)y j , where (x i , y i ) and (x j , y j ) are two samples drawn at random from training data and the target y are one-hot labels. With graph neural networks and mixup in mind, the following question naturally arises:\nCan we mix up graph data to improve the generalization and robustness of GNNs?\nIt remains an open and challenging problem to mix up graph data due to the characteristics of graphs and the requirements of applying Mixup. Typically, Mixup requires that original data instances are regular and well-aligned in Euclidean space, such as image data and table data. However, graph data is distinctly different from them due to the following reasons: (i) graph data is irregular, since the number of nodes in different graphs are typically different from each other; (ii) graph data is not well-aligned, where nodes in graphs are not naturally ordered and it is hard to match up nodes between different graphs; (iii) graph topology between classes are divergent, where the topologies of a pair of graphs from different classes are usually different while the topologies of those from the same class are usually similar. Thus, it is nontrivial to directly adopt the Mixup strategy to graph data.\nTo tackle the aforementioned problems, we propose G-Mixup, a class-level graph data augmentation method, to mix up graph data based on graphons. The graphs within one class are produced under the same generator (i.e., graphon). We mix up the graphons of different classes and then generate synthetic graphs. Informally, a graphon can be thought arXiv:2202.07179v2 [cs.LG] 16 Feb 2022 of as a probability matrix (e.g., the matrix W G and W H in Figure 1), where W (i, j) represents the probability of edge between node i and j. The real-world graphs can be regraded as generated from graphons. Since the graphons of different graphs is regular, well-aligned, and is defined in Euclidean space, it is easy and natural to mix up the graphons and then generate the synthetic graphs therefrom.\nOn this basis, we can achieve graphs mixup by mixing their generators. We also provide theoretical analysis of graphons mixup, which guarantees that the generated graphs will preserve the key characteristics of both original classes. Our proposed method is illustrated in Figure 1 with an example. Given two graph training sets G = {G 1 , G 2 , \u2022 \u2022 \u2022 , G m } and H = {H 1 , H 2 , \u2022 \u2022 \u2022 , H m } with different labels and distinct topologies (i.e., G has two communities while H has eight communities), we estimate graphons W G and W H respectively from G and H. We then mix up the two graphons and obtain a mixed graphon W I . After that, we sample synthetic graphs from W I as additional training graphs. The generated synthetic graphs have two major communities and each of them have four sub-communities, which is a mixture of the two graph sets. It thus shows that G-Mixup is capable of mixing up graphs.\nOur main contributions are highlighted as follows: Firstly, we propose G-Mixup to augment the training graphs for graph classification. Since directly mixing up graphs is intractable, G-Mixup mixes the graphons of different classes of graphs to generate synthetic graphs. Secondly, we theoretically prove that the synthetic graph will be the mixture of the original graphs, where the key topology (i.e., discriminative motif) of source graphs will be mixed up. Thirdly, we demonstrate the effectiveness of the proposed G-Mixup on various graph neural networks and datasets. Extensive experimental results show that G-Mixup substantially improves the performance of graph neural networks in terms of enhancing their generalization and robustness.", "publication_ref": ["b38", "b50", "b47", "b40", "b26", "b42", "b35", "b44", "b32", "b44", "b32", "b45", "b12", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "In this section, we first go over the notations used in this paper, and then introduce graph related concepts including graph homomorphism and graphons, which will be used for theoretical analysis in this work. Finally, we briefly review the graph neural networks for graph classification.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Notations", "text": "Given a graph G, we use V (G) and E(G) to denote its nodes and edges, respectively. The number of nodes is v(G) = |V (G)|, and the number of edges is e(G) = |E(G)|. We use m, l to denote the number of graphs and N, K to denote the number of nodes. We use G, H, I/G, H, I to denote graphs/graph set. y G \u2208 R C denotes the label of graph set G, where C is number of classes of graphs. A graph could contain some frequent subgraphs which are called motifs. The motifs in graph G is denoted as F G . The set of motifs in graph set G is denoted as F G . W G denotes the graphon of graph set G. W denotes the step function. G(K, W ) denotes the random graph with K nodes based on graphon W .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Graph Homomorphism and Graphons", "text": "Graph Homomorphism. A graph homomorphism is an adjacency-preserving mapping between two graphs, i.e., mapping adjacent vertices in one graph to adjacent vertices in the other. Formally, a graph homomorphism \u03c6 :\nF \u2192 G is a map from V (F ) to V (G), where if {u, v} \u2208 E(F ), then {\u03c6(u), \u03c6(v)} \u2208 E(G).\nFor two graphs H and G, there could be multiple graph homomorphisms between them. Let hom(H, G) denotes the total number of graph homomorphisms from graph H to graph G. For example, hom( G) is six times the number of triangles in G. There are in total |V (G)| |V (H)| mappings from H to G, but only some of them are homomorphisms. Thus, we define homomorphism density to measure the relative frequency that the graph\n, G) = |V (G)| if graph H is , hom( , G) = 2|E(G)| if graph H is , and hom( ,\nH ap- pears in graph G as t(H, G) = hom(H,G) |V (G)| |V (H)| . For example, t( , G) = |V (G)|/N 1 = 1, t( , G) = 2|E(G)|/N 2 .\nGraphon. A graphon (Airoldi et al., 2013) is a continuous, bounded and symmetric function W : [0, 1] 2 \u2192 [0, 1] which may be thought of as the weight matrix of a graph with infinite number of nodes. Then, given two points u i , u j \u2208 [0, 1], W (i, j) represents the probability that nodes i and j be related with an edge. Various quantities of a graph can be calculated as a function of the graphon. For example, the degree of nodes in graphs can be easily extended to a degree distribution function in graphons, which is characterized by its graphon marginal d W (x) = 1 0 W (x, y)dy. Similarly, the concept of homomorphism density can be naturally extended from graphs to graphons. Given an arbitrary graph motif F , its homomorphism density with respect to graphon W is defined by t(F, W\n) = [0,1] V (F ) i,j\u2208E(F ) W (x i , x j ) i\u2208V (F ) dx i . For example, the edge density of graphon W is t( , W ) = [0,1] 2 W (x, y) dxdy, and the triangle density of graphon W is t( , W ) = [0,1] 3 W (x, y)W (x, z)W (y, z) dxdydz.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "Graph Classification with Graph Neural Networks", "text": "Given a set of graphs, graph classification aims to assign a class label for each graph G. Recently, graph neural networks have become the state-of-the-art approach for graph classification. Without loss of generalization, we present the formal expression of a graph convolution network (GCN) (Kipf & Welling, 2016). The forward propaga-\nW I W H W G I = {I 1 , I 2 , \u2022 \u2022 \u2022 , I k } with label (0.5, 0.5) H = {H 1 , H 2 , \u2022 \u2022 \u2022 , H k } with label (0, 1) G = {G 1 , G 2 , \u2022 \u2022 \u2022 , G k } with label (1, 0) \u2026 \u2026 \u2026 1) graphon estimation 3) graph sampling W I = 0.5 \u21e4 W G + 0.5 \u21e4 W H 1) graphon estimation 2) graphon mixup Figure 1. An overview of G-Mixup.\nThe task is binary graph classification. We have two classes of graphs G and H with different topologies (G has two communities while H has eight communities). G and H have different graphons. We mix up the graphons WG and WH to obtain a mixed graphon WI, and then sample new graphs from the mixed graphon. Intuitively, the synthetic graphs have two major communities and each of which has four sub-communities, demonstrating that the generated graphs preserve the structure of original graphs from both classes.\ntion at k-th layer is described as the following:\na (k) i = AGG (k) h (k\u22121) j : j \u2208 N (i) , h (k) i = COMBINE (k) h (k\u22121) i , a (k) i ,(1)\nwhere h (k) i \u2208 R n\u00d7d k is the intermediate representation of node i at the k-th layer, N (i) denotes the neighbors of node i. AGG(\u2022) is an aggregation function to collect embedding representations from neighbors, and COMBINE(\u2022) combines neighbors' representation and its representation at (k \u2212 1)-th layer. For graph classification, a graph-level representation is obtained by summarizing all node-level representations in the graph by a readout function:\nh G = READOUT h (k) i : i \u2208 E(G) , y = softmax(h G ),(2)\nwhere READOUT(\u2022) is the readout function, which can be a simple function such as average or sophisticated pooling function (Gao & Ji, 2019;Ying et al., 2018), h G is the representation of graph G, and\u0177 \u2208 R C is the predicted probability that G belongs to each of the C classes.", "publication_ref": ["b21", "b10", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "Methodology", "text": "In this section, we formally introduce the proposed G-Mixup and elaborate its implementation details.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G -Mixup", "text": "Different from the interpolation of image data in Euclidean space, adopting Mixup to graph data is nontrivial since graphs are irregular, unaligned and non-Euclidean data. In this work, we show that the challenges could be tackled with graphon theory. Intuitively, a graphon can be regarded as a graph generator. Graphs of the same class can be seen as being generated from the same graphon. With this in mind, we propose G-Mixup, a class-level data augmentation method via graphon interpolation. Specifically, G-Mixup interpolates different graph generators to obtain a new mixed one. Then, synthetic graphs are sampled based on the mixed graphon for data augmentation. The graphs sampled from this generator partially possess properties of the original graphs. Formally, G-Mixup is formulated as:\nGraphon Estimation: G \u2192 W G , H \u2192 W H (3)\nGraphon Mixup:\nW I = \u03bbW G + (1 \u2212 \u03bb)W H (4) Graph Generation: {I 1 , I 2 , \u2022 \u2022 \u2022 , I m } i.i.d \u223c G(K, W I ) (5)\nLabel Mixup:\ny I = \u03bby G + (1 \u2212 \u03bb)y H (6)\nwhere W G , W H are graphons of the graph set G and H. The mixed graphon is denoted by W I , and \u03bb \u2208 [0, 1] is the trade-off hyperparameter to control the contributions from different source sets. The set of synthetic graphs generated from\nW I is I = {I 1 , I 2 , \u2022 \u2022 \u2022 , I m }.\nThe y G \u2208 R C and y H \u2208 R C are vectors containing ground-truth labels for graph G and H, respectively, where C is the number of classes. The label vector of synthetic graphs in graph set I is denoted as y I \u2208 R C .\nAs illustrated in Figure 1 ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Implementation", "text": "In this section, we introduce the implementation details of graphon estimation and synthetic graphs generation. We provide the pseudo-code of G-Mixup in Appendix E.\nGraphon Estimation and Mixup. Estimating graphons from observed graphs is a prerequisite for G-Mixup. However, it is intractable because a graphon is an unknown function without a closed-form expression for real-world graphs. Therefore, we use the step function (Lov\u00e1sz, 2012;Xu et al., 2021) to approximate graphons 1 . In general, the step function can be seen as\na matrix W = [w kk ] \u2208 [0, 1] K\u00d7K ,\nwhere W ij is the probability that an edge exists between node i and node j. In practice, we use the matrix-form step function as graphon to mix up and generate synthetic graphs. The step function estimation methods are wellstudied, which first align the nodes in a set of graphs based on node measurements (e.g., degree) and then estimate the step function from all the aligned adjacency matrices. The typical step function estimation methods includes sortingand-smoothing (SAS) method (Chan & Airoldi, 2014), stochastic block approximation (SBA) (Airoldi et al., 2013), \"largest gap\" (LG) (Channarond et al., 2012), matrix completion (MC) (Keshavan et al., 2010), universal singular value thresholding (USVT) (Chatterjee et al., 2015). 2 Formally, a step function\nW P : [0, 1] 2 \u2192 [0, 1] is defined as W P (x, y) = K k,k =1\nw kk 1 P k \u00d7P k (x, y), where P = (P 1 , .., P K ) denotes the partition of [0, 1] into K adjacent intervals of length 1/K, w kk \u2208 [0, 1], and indicator function 1 P k \u00d7P k (x, y) equals to 1 if (x, y) \u2208 P k \u00d7 P k and otherwise it is 0. For binary classification, we have\nG = {G 1 , G 2 , \u2022 \u2022 \u2022 , G m } and H = {H 1 , H 2 , \u2022 \u2022 \u2022 , H m }\nwith different labels, we estimate their step functions W G \u2208 R K\u00d7K and W H \u2208 R K\u00d7K , where we let K be the average number of nodes in all graphs. For multi-class classification, we first estimate the step function for each class of graphs and then randomly select two to perform mix-up. The resultant step function is\nW I = \u03bbW G + (1 \u2212 \u03bb)W H \u2208 R K\u00d7K , which\nserves as the generator of synthetic graphs.", "publication_ref": ["b22", "b39", "b5", "b0", "b6", "b19", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Synthetic Graphs Generation.", "text": "A graphon W provides a distribution to generate arbitrarily sized graphs. Specifically, a K-node random graph G(K, W I ) can be generated following the process:\nu 1 , . . . , u K iid \u223c Unif [0,1] , G(K, W ) ij iid \u223c Bern(W (u i , u j )), \u2200i, j \u2208 [K].\nSince we use the step function W to approximate the 1 Because weak regularity lemma of graphon (Frieze & Kannan, 1999) indicates that an arbitrary graphon can be approximated well by step function. Detailed discussion is in Appendix A.4.\n2 The details about these step function estimation methods are presented in Appendix B. graphon W , we set W (u i , u j ) = W[ 1/u i , 1/u j ], and \u2022 is the floor function. The first step samples K nodes independently from a uniform distribution\nUnif [0,1] on [0, 1].\nThe second step generates an adjacency matrix A = [a ij ] \u2208 {0, 1} K\u00d7K , whose element values follow the Bernoulli distributions Bern(\u2022) determined by the step function. A graph is thus obtained as G where V (G) = {1, ..., K} and E(G) = {(i, j) | a ij = 1}. A set of synthetic graphs can be generated by conducting the above process multiple times. The generation of node features of synthetic graphs includes two steps: 1) build the graphon node features based on the original node features, 2) generate node features of synthetic graphs based on the graphon node features. Specifically, at the graphon estimation phase, we align original node features while aligning the adjacency matrices, so we have a set of aligned original node features for each graphon, then we conduct pooling (average pooling in our experiments) on the aligned original node features to obtain the graphon node features. The node features of generated graphs are the same as graphon features.\nTable 1. Computational complexity of graphon estimation.\nMethod Complexity MC O(N 3 ) USVT O(N 3 ) LG O(mN 2 ) SBA O(mKN log N ) SAS O(mN log N + K 2 log K 2 )\nComputational Complexity Analysis. We hereby discuss computational complexity of G-Mixup. The major computation costs come from graphon estimation and synthetic graph generation. For graphon estimation, suppose we have m graphs and each of them has N nodes, and estimate step function with K partitions to approximate a graphon, the complexity of used graphon estimation methods (Xu et al., 2021) is in Table 1. For graph generation, suppose we need to generate l graphs with K nodes, the computational complexity is O(lK) for node generation and O(lK 2 ) for edge generation, so the overall complexity of graph generation is O(lK 2 ).", "publication_ref": ["b9", "b39"], "figure_ref": [], "table_ref": []}, {"heading": "Theoretical Justification", "text": "In the following, we theoretically prove that: the synthetic graphs generated by G-Mixup will be a mixture of original graphs. We first define the discriminative motif, and then we justify the graphon mixup operation (Equation ( 4)) and graph generation operation (Equation ( 5)) by analysing the homomorphism density of discriminative motifs of the original graphs and the synthetic graphs. Definition 4.1 (Discriminative Motif). A discriminative motif F G of graph G is the subgraph, with the minimal number of nodes and edges, that can decide the class the graph G. Furthermore, F G is the set of discriminative motifs for graphs in the set G.\nIntuitively, the discriminative motif is the key topology of a graph. We assume that (i) every graph G has a discrimina-tive motif F G , and (ii) a set of graphs G has a finite set of discriminative motifs F G . The goal of graph classification is to filter out structural noise in graphs (Fox & Rajamanickam, 2019) and recognize the key typologies (discriminative motifs) to predict the class label. For example, benzene (a chemical compound) is distinguished by the motif (benzene ring). In the following, we analyze G-Mixup from the perspective of discriminative motifs.\n4.1. Will discriminative motifs F G and F H exist in\n\u03bbW G + (1 \u2212 \u03bb)W H ?\nWe answer this question by exploring the difference in homomorphism density of discriminative motifs between the original and mixed graphon, as the following theorems, Theorem 4.2. Given two sets of graphs G and H, the corresponding graphons are W G and W H , and the corresponding discriminative motif set F G and F H . For every discriminative motif F G \u2208 F G and F H \u2208 F H , the difference between the homomorphism density of F G /F H in the mixed graphon Ideally, the generated graphs should inherit the homomorphism density of discriminative motifs from the graphon.\nW I = \u03bbW G + (1 \u2212 \u03bb)W H and that of the graphon W H /W G is upper bounded by |t(F G , W I ) \u2212 t(F G , W G )| \u2264 (1 \u2212 \u03bb)e(F G )||W H \u2212 W G || , |t(F H , W I ) \u2212 t(F H , W H )| \u2264 \u03bbe(F H )||W H \u2212 W G || where e(F )\nTo verify this, we propose the following theorem. Theorem 4.3. Let W I be the mixed graphon, n \u2265 1, 0 < \u03b5 < 1, and let F I be the mixed discriminative motif, then the W I -random graph G = G(n, W I ) satisfies\nP (|t(F I , G) \u2212 t(F I , W I )| > \u03b5) \u2264 2exp \u2212 \u03b5 2 n 8v(F I ) 2 .\nTheorem 4.3 states that for any specified nonzero margin \u03b5, with a sufficient number of graphs sampled from the mixed graphon, the homomorphism density of discriminative motif in synthetic graphs will approximately equal to that in graphon t(F I , G) \u2248 t(F I , W I ) with high probability. In other words, the synthetic graphs will preserve the discriminative motif of the mixed graphon with a very high probability if the sample number n is large enough. The detailed proof is in Appendix A.3. Therefore, G-Mixup can preserve the discriminative motifs of the two different graphs into one mixed graph.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "Discussion", "text": "We discuss the differences and relations between G-Mixup and other augmentation strategies.\nRelation to Edge Perturbation Methods. The commonly used edge perturbation methods are spacial cases of G-Mixup. Edge perturbation methods randomly perturb the edges to improve the GNNs, inlcuding DropEdge (Rong et al., 2020), and Graphon-based edge perturbation (Hu et al., 2021). DropEdge removes graph edges independently with a specified probability, aiming to prevent oversmoothing and over-fitting issues in GNNs. Graphon-based edge perturbation (Rong et al., 2020) improves the Dropedge by dropping edge based on an estimated probability. One limitation of such methods is that the edge permutation is based on one individual graph, so the graphs will not mix up. DropEdge and Graphon-based edge perturbation (Hu et al., 2021) are special cases of G-Mixup while setting different hyperparameter \u03bb. i) G-Mixup will degenerate into Graphon-based edge perturbation, if \u03bb = 0 in Equation ( 4), where the mathematical expression is\nW I = W H , {I 1 , I 2 , \u2022 \u2022 \u2022 , I m } i.i.d \u223c G(k, W I ), y I = y H . ii)\nG-Mixup will degenerate into DropEdge, if \u03bb = 0 and using the element-wise product of graphons W and adjacency matrix A in Equation ( 4) as edge probability. The expression is W\nI = A W H , {I 1 , I 2 , \u2022 \u2022 \u2022 , I m } i.i.d \u223c G(k, W I ), y I = y H ,\nwhere is element-wise multiplication.\nRelation to Manifold Mixup. As a model-agnostic augmentation method, G-Mixup has broader applications, e.g., creating graphs for graph contrastive learning, than Manifold Mixup. Manifold Mixup (Wang et al., 2021) is proposed to mix up graphs in the embedding space, which interpolates hidden representations of graphs. Interpolating hidden representation could limit its applications because: 1) algorithms must have hidden representation of graphs, and ", "publication_ref": ["b26", "b16", "b26", "b16", "b37"], "figure_ref": [], "table_ref": []}, {"heading": "IMDB-BINARY", "text": "* W0 + 0 * W1) / (0 * W0 + 1 * W1\n), which have one/two high-degree node/nodes (marked with in (c) and (d)) because the mixed graphon only contains W0/W1. The synthetic graphs generated from (0.5 * W0 + 0.5 * W1) is the mixture of graphs of class 0 and 1, which appears as a high-degree node and a dense subgraph (marked with and in (e), respectively). The results show that synthetic graphs are the mixture of the original graphs.\n2) models must be modified to adapt it. In contrast, G-Mixup generates synthetic graphs without modifying models.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We evaluate the performance of G-Mixup in this section. First, we visualize graphons and graph generation results in Sections 5.1 and 5.2 to investigate what G-Mixup actually do on real-world datasets. Then, we evaluate the effectiveness of G-Mixup in graph classification with various datasets and GNN backbones in Section 5.3, as well as how it improves the robustness of GNNs against label corruption and adversarial attacks in Section 5.4. The experiment setting and more experiments are in Appendices F and G. The observations are highlighted with # boldface.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Do different classes of real-world graphs have different graphons?", "text": "We visualize the estimated graphons in Figure 2. It shows that, 1 the graphons of different class of graphs in one dataset are distinctly different. The graphons of IMDB-BINAERY in Figure 2 shows that the graphon of class 1 has larger dense area, which indicates that the graphs in this class have a more large communities than the graphs of class 0. The graphons of REDDIT-BINARY in Figure 2 shows that graphs of class 0 have one high-degree nodes while the graphs of class 1 have two. This observation validates that real-world graphs of different classes have distinctly different graphons, which lays a solid foundation for generating the mixture of graphs by mixing up graphons.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "What is G-Mixup doing? A case study", "text": "To investigate the outcome of G-Mixup in real-world scenarios, we visualize the generated synthetic graphs in REDDIT-BINARY dataset in Figure 3. We observed that 2 The synthetic graphs are indeed the mixture of the original graphs. Original graphs and the generated synthetic graphs are visualized in Figure 3(a)(b) and Figure 3(c)(d)(e), respectively. Figure 3 demonstrates that mixed graphon 0.5 * W G + 0.5 * W H is able to generate graphs with a highdegree node and a dense subgraph, which can be regarded as the mixture of graphs with one high-degree node and two high-degree nodes. It validates that G-Mixup prefer to preserve the discriminative motifs from the original graphs.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Can G-Mixup improve the performance and generalization of GNNs?", "text": "To validate the effectiveness of G-Mixup, we compare the performance of GNNs with various backbones on differ- ent datasets, and summarize results in Tables 2 and 3 as well as the training curves in Figure 4. We make the following observations: 3 G-Mixup can improve the performance of graph neural networks on various datasets. From Table 2, G-Mixup gain 12 best performances among 15 reported accuracies, which substantially improve the performance of GNNs. Overall, G-Mixup performs 2.84% better than the vanilla model. Note that G-Mixup and baseline models adopt the same architecture of GNNs (e.g., layers, activation functions) and the same training hyperparameters (e.g., optimizer, learning rate). From Table 3, G-Mixup gains 7 best performances among 8 cases, which substantially improve the performance of DiffPool and Min-cutPool. Meanwhile, 4 G-Mixup can improve the generalization of graph neural networks. From the loss curve on test data (green line) in Figure 4, the loss of test data of G-Mixup (dashed green lines) are consistently lower than the vanilla model (solid green lines). Considering both the better performance and the better test loss curves, G-Mixup is able to substantially improve the generalization of GNNs. Also, 5 G-Mixup could stabilize the model training. As shown in Table 2, G-Mixup achieves 11 lower standard deviation among total 15 reported numbers than the vanilla model. Additionally, the train/validation/test curves of G-Mixup (dashed line) in Figure 4 are more stable than vanilla model (solid line), indicating that G-Mixup sta-   (Hu et al., 2020) and more pooling method (GMT) are in Appendices G.2 and G.3.", "publication_ref": ["b15"], "figure_ref": [], "table_ref": ["tab_3", "tab_3"]}, {"heading": "IMDB-BINARY IMDB-MULTI REDDIT-BINARY REDDIT-MULTI-5K", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Can G -Mixup improve the robustness of GNNs?", "text": "We investigate the two kinds of robustness of G-Mixup, including Label Corruption Robustness and Topology Corruption Robustness, and report the results in Table 4 and  Table 5, respectively. More experimental settings are presented in Appendix F.4. 6 G-Mixup improves the robustness of graph neural networks. Table 4 shows G-Mixup gains better performance in general, indicating it is more robust to noisy labels than the vanilla baseline. Table 5 shows that G-Mixup is more robust when graph topology is corrupted since the accuracy is consistently better than baselines. This can be an advantage of G-Mixup when graph label or topology are noisy.  5.5. Further Analysis", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4", "tab_4", "tab_5"]}, {"heading": "THE NODES NUMBER OF GENERATED GRAPHS", "text": "We investigate the impact of the nodes number in generated synthetic graphs by G-Mixup and present the results in Figure 5. Specifically, G-Mixup generates synthetic graphs with different numbers (hyperparameters K) of nodes and use them to train graph neural networks. We observed form Figure 5 that 7 using the average node number of all the original graphs is a better choice for hyperparameter K in G-Mixup, which is in line with the intuition.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "IMPACT ON DEEPER MODELS", "text": "We investigate the performance of G-Mixup when GCN goes deeper. We experiment with different numbers (2 \u2212 9) of layers and report the results in Figure 6. 8 G-Mixup improves the performance of graph neural networks with varying layers. In Figure 6, the left figure shows G -Mixup gains better performance while the depth of GCNs is 2 \u2212 6.\nThe performance with deeper GCNs (7 \u2212 9) are comparable to baselines, however, the accuracy is much lower than shallow ones. The right figure shows G-Mixup gains better performance by a significant margin while the depth of GCNs is 2 \u2212 9. This validates the effectiveness of G-Mixup when graph neural network goes deeper.", "publication_ref": [], "figure_ref": ["fig_2", "fig_2"], "table_ref": []}, {"heading": "Related Works", "text": "Graph Data Augmentation. Graph neural networks (GNNs) achieve the state-of-the-art performance on graph classification tasks (Kipf & Welling, 2016;Veli\u010dkovi\u0107 et al., 2017;Hamilton et al., 2017;Xu et al., 2018;  2018). In parallel, graph data augmentation methods improve the performance of GNNs. There are three categories of graph data augmentation, including node perturbation (You et al., 2020;Huang et al., 2018), edge perturbation (Rong et al., 2020;You et al., 2020), and subgraph sampling (You et al., 2020;Wang et al., 2020a). However, the major limitation of the existing graph augmentation methods is that they are based on one single graph while G-Mixup leverages multiple input graphs. Besides, there are a line of works focusing on graph data augmentation methods for node classification (Zhao et al., 2021;Wang et al., 2020b;Tang et al., 2021;Park et al., 2021;Verma et al., 2019b). The more discussion are in Appendix D.\nGraphon Estimation. Graphons and convergent graph sequences have been broadly studied in mathematics (Lov\u00e1sz, 2012;Lov\u00e1sz & Szegedy, 2006;Borgs et al., 2008) and have been applied to network science (Avella-Medina et al., 2018;Vizuete et al., 2021) and graph neural networks (Ruiz et al., 2020a;. There are tow lines of works to estimate step functions, one is based on stochastic block models, such as stochastic block approximation (SBA) (Airoldi et al., 2013), \"largest gap\" (LG) (Channarond et al., 2012) and sortingand-smoothing (SAS) (Chan & Airoldi, 2014); another one is based on low-rank matrix decomposition, such as matrix completion (MC) (Keshavan et al., 2010), universal singular value thresholding (USVT) (Chatterjee et al., 2015). More discussion about graphon estimation are in Appendix B.", "publication_ref": ["b21", "b31", "b14", "b40", "b42", "b17", "b26", "b42", "b42", "b35", "b48", "b36", "b30", "b33", "b22", "b23", "b4", "b1", "b34", "b27", "b0", "b6", "b5", "b19", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "This work develops a novel graph augmentation method called G-Mixup. Unlike image data, graph data is irregular, unaligned and in non-Euclidean space, making it hard to be mixed up. However, the graphs within one class have the same generator (i.e., graphon), which is regular, wellaligned and in Euclidean space. Thus we turn to mix up the graphons of different classes to generate synthetic graphs. G-Mixup is mix up and interpolate the topology of different classes of graphs. Comprehensive experiments show that GNNs trained with G-Mixup achieve better performance and generalization, and improve the model robustness to noisy labels and corrupted topology.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Proof of Theorem", "text": "In the appendix, we first present the preliminaries in Appendix A.1. And then we present complete proof for Theorems 4.2 and 4.3 in Appendices A.2 and A.3, respectively.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1. Preliminaries", "text": "Cut norm (Lov\u00e1sz, 2012;Zhao, 2019) is used to measure structural similarity of two graphons. The definition of cut norm is as follow:\nDefinition A.1. The cut norm of grapon W is defined as\nW = sup S,T\u2282[0,1] S\u00d7T W (x, y)dxdy ,(7)\nwhere the supremum is taken over all measurable subsets S and T.\nThe following lemma follows the derivation of counting lemma for graphons, are known in the paper (Lov\u00e1sz, 2012). It will be used to prove the Theorem 4.2.\nLemma A.2. Let F be a simple graph and let W, W \u2208 W. Then\n|t(F, W ) \u2212 t(F, W )| \u2264 e(F )||W \u2212 W ||(8)\nProof of Lemma A.2: The proof follows Zhao (2019). For an arbitrary simple graph F , by the triangle inequality we have\n|t(F, W ) \u2212 t(F, W )| = uivi\u2208E W (u i , v i ) \u2212 uivi\u2208E W (u i , v i ) v\u2208V dv \u2264 |E| i=1 \uf8eb \uf8ed i\u22121 j=1 W (u j , v j ) (W (u i , v i ) \u2212 W (u i , v i )) |E| k=i+1 W (u k , v k ) \uf8f6 \uf8f8 v\u2208V dv (9)\nHere, each absolute value term in the sum is bounded by the cut norm W \u2212 W if we fix all other irrelavant variables (everything except u i and v i for the i-th term), altogether implying that\n| t(F, W ) \u2212 t(F, W )| \u2264 e(F )||W \u2212 W || (10)\nLemma A.3 (Corollary 10.4 in (Lov\u00e1sz & Szegedy, 2006)). Let W be a graphon, n \u2265 1, 0 < \u03b5 < 1, and let F be a simple graph, then the W -random graph G = G(n, W ) satisfies\nP (|t(F, G) \u2212 t(F, W )| > \u03b5) \u2264 2exp \u2212 \u03b5 2 n 8v(F ) 2\n(11)", "publication_ref": ["b22", "b49", "b22", "b49", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "A.2. Proof of Theorem 1", "text": "We have the mixed graphon\nW I = \u03bbW G + (1 \u2212 \u03bb)W H . Let W = W I , W = W G , and F = F G in Lemma A.2, we have, |t(F G , W I ) \u2212 t(F G , W G )| \u2264 e(F G )||W I \u2212 W G || |t(F G , \u03bbW G + (1 \u2212 \u03bb)W H ) \u2212 t(F, W G )| \u2264 e(F G )||\u03bbW G + (1 \u2212 \u03bb)W H \u2212 W G || \u2264 e(F G )||(1 \u2212 \u03bb)(W H \u2212 W G )|| (12\n)\nRecall that the cut norm W = sup S,T \u2286[0,1] S\u00d7T W .\nobviously, suppose \u03b1 \u2208 R, we have\n\u03b1W = sup S,T \u2286[0,1] S\u00d7T \u03b1W = sup S,T \u2286[0,1] \u03b1 S\u00d7T W = \u03b1 W (13)\nBased on Equation ( 12) and Equation ( 13), we have\n|t(F G , \u03bbW G + (1 \u2212 \u03bb)W H ) \u2212 t(F G , W G )| \u2264 e(F G )||(1 \u2212 \u03bb)(W H \u2212 W G )|| \u2264 (1 \u2212 \u03bb)e(F G )||W H \u2212 W G || (14)\nSimilarly, let W = W I , W = W H and F = F H in Lemma A.2, We can also easily obtain\n|t(F H , \u03bbW G + (1 \u2212 \u03bb)W H ) \u2212 t(F H , W H )| \u2264 \u03bbe(F H )||W H \u2212 W G || (15)\nEquation ( 14) and Equation ( 15) produce the upper bound in Equation (7).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.3. Proof of Theorem 2", "text": "Let F and W be the discriminative motif F I and the mixed graphon W I in Lemma A.3, we will have\nP (|t(F I , G) \u2212 t(F I , W I )| > \u03b5) \u2264 2exp \u2212 \u03b5 2 n 8v(F I ) 2 (16)\nwhich produces the result in Equation (7).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.4. Graphons Estimation by Step Function", "text": "The proof follows Xu et al. (2021). A graphon can always be approximated by a step function in the cut norm (Frieze & Kannan, 1999).\nLet P = (P 1 , .., P K ) be a partition of \u2126 into K measurable sets. We define a step function W P : \u2126 2 \u2192 [0, 1] as\nW P (x, y) = K k,k =1 w kk 1 P k \u00d7P k (x, y),(17)\nwhere each w kk \u2208 [0, 1] and the indicator function 1 P k \u00d7P k (x, y) is 1 if (x, y) \u2208 P k \u00d7 P k , otherwise it is 0. The weak regularity lemma (Lov\u00e1sz, 2012) shown below guarantees that every graphon can be approximated well in the cut norm by step functions.\nTheorem A.4 (Weak Regularity Lemma (Lemma 9.9 in (Lov\u00e1sz, 2012)) ). For every graphon W and K \u2265 1, there always exists a step function W with |P| = K steps such that\nW \u2212 W \u2264 2 \u221a log K W L2 .(18)", "publication_ref": ["b39", "b9", "b22", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "B. Graphons Estimation Methods", "text": "The adopted graphon estimated methods (e.g., LG, USVT, SBA) are well-studied methods. Typically they have rigorous mathematical proof to upper bound the graphon estimation error. For example, Theorem 2.10 in (Chatterjee et al., 2015) shows the graphon estimation error of USVT is strictly upper bounded. And we also copy the results of graphon estimation methods on synthetic graphon from (Xu et al., 2021) in Table 6. The results show the graphon estimation methods in our work can precisely estimate graphon. The details of them are listed as the following:\n\u2022 SBA (Airoldi et al., 2013) The Stochastic Block Approximation learns stochastic block models to approximate graphons. This method can consistently estimate the graphon with extremely small error and the estimation error vanishes provably as the node number of the graph goes infinity.  (Xu et al., 2021). The graphon estimation is based on 10 graphs, the error is Mean Square Error, and the resolution of graphon is 1000 \u00d7 1000. 5.0\u00b19.5 23.1\u00b13.2 64.6\u00b10.5 37.3\u00b10.6 73.3\u00b10.7\n\u2022 LG (Channarond et al., 2012) The \"largest gap\" algorithm improve the SBA method, which can be used for both large-scale and small graphs.\n\u2022 SAS (Chan & Airoldi, 2014) The smoothing-and-sorting (SAS) is a improved variant of SBA, which first sorts the graphs based on the node degree, then smooths the sorted graph using total variation minimization.\n\u2022 MC and USVT (Keshavan et al., 2010;Chatterjee et al., 2015) Matrix Completion and Universal Singular Value Thresholding are matrix decomposition based methods, which learn low-rank matrices to approximate graphons.", "publication_ref": ["b7", "b39", "b0", "b39", "b6", "b5", "b19", "b7"], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "C. Discussion about Manifold Intrusion in G-Mixup", "text": "In this appendix, we discuss that manifold intrusion in G-Mixup and argue that G-Mixup does not suffer from manifold intrusion issue. The manifold intrusion may be harmful for mixup method. Manifold intrusion in mixup is a form of under-fitting resulting from conflicts between the labels of the synthetic examples and the labels of original training data (Guo et al., 2019b). The manifold intrusion in graph learning represents that the generated graphs have identical topology but different labels. In our method, the adjacency matrix A \u2208 R K\u00d7K of generated graphs are generated from the matrix-from graphon W \u2208 R K\u00d7K , thus we have\nA ij iid \u223c Bern(W ij ), \u2200i, j \u2208 [K].\nIn the graph generation phase, G-Mixup may cause manifold intrusion in two cases: 1) two generated two graphs are identical, 2) a generated graph is identical to an original graph. We hereby show that graph manifold intrusion issue will not happen with a very high probability in G-Mixup as follows:\n\u2022 Two generated two graphs are identical. The probability of generating two identical graphs from the same graphon\nW is \u03a0 K i=1 \u03a0 K j=1 (W 2 ij + (1 \u2212 W ij ) 2\n), which is extremely small since 0 < W 2 ij + (1 \u2212 W ij ) 2 < 1 and K is large enough in the real-world graphs. The probability that two generated two graphs are identical are extremely small.\n\u2022 A generated graph is identical to an original graph. The probability of generating a new graph that is identical to an original graph (the adjacency matrix is\u00c3) is\n\u03a0 K i=1 \u03a0 K j=1 (W\u00c3 ij ij (1 \u2212 W ij ) 1\u2212\u00c3ij ), which is extremely small since 0 < W\u00c3 ij ij (1 \u2212 W ij ) 1\u2212\u00c3ij < 1\nand K is large enough in the real-world graphs. The probability that a generated graph is identical to an original graph are identical are extremely small too.", "publication_ref": ["b13"], "figure_ref": [], "table_ref": []}, {"heading": "D. More Discussion about Related Works", "text": "In this appendix, we discuss two categories of related works. The first one is graph data augmentation for node classification, and the second is model-dependent graph data augmentation for graph classification. Both of them are different to our proposed G-Mixup.\nGraph Data Augmentation for Node Classification. There is another line of works targeting graph data augmentation for node classification (Zhao et al., 2021;Wang et al., 2020b;Tang et al., 2021;Park et al., 2021;Verma et al., 2019b). Zhou et al. (2020b) leverage information inherent in the graph to predict edge probability to augment a new graph for node classification task. Verma et al. (2019b) proposed GraphMix to augment the vanilla GNN with a Fully-Connected Network  Verma et al. (2019b) proposed to generate augmented graphs from an explicit target distribution for semi-supervised learning, which has flexible control of the strength and diversity of augmentation. Many graph augmentation methods are proposed to solve node classificaiton task. However, the node classification task is a different task in graph learning from graph classification task. The node classification task usually has one input graph, thus the graph augmentation methods for node classification is limited to one graph while the graph augmentation for graph classification can manipulate multiple graphs. Thus graph data augmentation for node-level task is not applicable to our scenario.\nModel-Dependent Graph Data Augmentation for Graph Classification. There are some model-dependent graph augmentation methods (Suresh et al., 2021;You et al., 2022;Zhou et al., 2020b) for graph classification task. Suresh et al. (2021) proposed to enable GNNs to avoid capturing redundant information during the training by optimizing adversarial graph augmentation strategies used in graph contrastive learning during the training phase. You et al. (2022) proposed to learn a continuous prior parameterized by a neural network from data during contrastive training, which is used to augment graph. The difference between our proposal and these methods is that G-Mixup an general model-agnostic graph data augmentation methods for graph classification.", "publication_ref": ["b48", "b36", "b30", "b33", "b51", "b33", "b33", "b29", "b43", "b51", "b29", "b43"], "figure_ref": [], "table_ref": []}, {"heading": "E. Implementation Details", "text": "In this appendix, we present the pseudo code for G-Mixup. We first present the pseudo code for graphon estimation in Algorithm 1, which depicts how to generate the graphon and the node features. Since our proposed method is a modelagnostic method, which can be conducted before the model training. Then we present the pseudo code G-Mixup. The graphon estimation is based on the one class of graphs, thus we can estimate on graphon using all the graphs in the same class or a random batch of graphs in the same class. On this basis, we have two version of concrete implementations: 1) estimating graphon on graphon using all the graphs in the same class (Algorithm 2), 2) estimating graphon on graphon using a random batch of graphs in the same class (Algorithm 3). The first implementation provide more accurate estimated graphons while the second encourages more diversity of the synthetic graphs. Note that all these two versions can be done as a pre-processing before model training. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G.4. Sensitivity Analysis to Mixup Ratio \u03bb", "text": "To further investigate the performance of G-Mixup, we provide experimental results of G-Mixup to analyse the sensitivity to hyperparameter mixup ratio \u03bb. Specifically, we use the different mixing ratio \u03bb in W I = \u03bbW G +(1\u2212\u03bb)W H , \u03bby G +(1\u2212\u03bb)y H on molecular property prediction task (i.e., ogbg-molbbbp, ogbg-molbace). The p-value 17 is computed with the best performance compared to the Vanilla GCN (last column in Table 9 and Table 9). We can observed that G-Mixup significantly improves graph neural networks' performance while we tune the hyperparameter of G-Mixup.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 3 G-Mixup (batch)", "text": "Input: train graph set S with B batches, graphon estimator g, mixup ratio \u03bb, augmented ratio \u03b1 0 < \u03bb, \u03b1 < 1 Init: Synthetic graph set I = {} for batch in S do Obtain two graph sets G and H with different labels y G and y H Estimate (W G ,X G ) and (W H ,X H ) from G and H using Algorithm 1 Mix up step function W I = \u03bbW G + (1 \u2212 \u03bb)W H Mix up graphon node featuresX I = \u03bbX G + (1 \u2212 \u03bb)X H Sample \u03b1 \u2022 |S|/B synthetic graphs based on W I andX I and add them to I |I| = \u03b1 \u2022 |S| after for loop ends end for Return: synthetic graph set I", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F. Experiments Details", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F.1. Experimental Setting", "text": "To ensure a fair comparison, we use the same hyperparater for modeling training and the same architecture for vanilla model and other baselines. For model training, we use the Adam optimizer (Kingma & Ba, 2015). The initial learning rate is 0.01 and will drop the learning rate by half every 100 epochs. The batch size is set to 128. We split the dataset into train/val/test data by 7 : 1 : 2. Note that best test epoch is selected on a validation set, and we report the test accuracy on ten runs. For hyperparemeter in G-Mixup, we generate 20% more graph for training graph. The graphons are estimated based on the training graphs. We use different \u03bb \u2208 [0.1, 0.2] to mix up the graphon and generate synthetic with different strength of mixing up.", "publication_ref": ["b20"], "figure_ref": [], "table_ref": []}, {"heading": "F.2. Architectures of Graph Neural Networks", "text": "We adopted two categories of graph neural networks as our baselines, The first category is Graph Convolutional Network (GCN) and Graph Isomorphism Network (GIN). The second category is graph polling methods, including TopK Pooling (TopKPool), Differentiable Pooling (DiffPool), MinCut Pooling (MincutPool) and Graph Multiset Pooling (GMT). The details of the GNNs are listed as follows:\n\u2022 GCN 4 (Kipf & Welling, 2016). Four GNN layers and global mean pooling are applied. All the hidden units is set to 64.\nThe activation is ReLU (Nair & Hinton, 2010).\n\u2022 GIN 5 (Xu et al., 2018). We apply five GNN layers and all MLPs have two layers. Batch normalization (Ioffe & Szegedy, 2015) is applied on every hidden layer. All hidden units are set to 64. The activation is ReLU (Nair & Hinton, 2010).\n\u2022 TopKPool 6 (Gao & Ji, 2019). Three GNN layers and three TopK pooling are applied. A there-layer percetron are adopted to predict the labels. All the hidden units is set to 64. The activation is ReLU (Nair & Hinton, 2010).\n\u2022 DiffPool 7 (Ying et al., 2018) is a differentiable graph pooling methods that can be adapted to various GNN architectures, which maps nodes to clusters based on their learned embeddings.\n\u2022 MincutPool 8 (Bianchi et al., 2020) is a differentiable pooling baselines. It learns a clustering function that can be quickly evaluated on out-of-sample graphs.\n\u2022 GMT 9 (Baek et al., 2020) is a multi-head attention based global pooling layer to generate graph representation, which captures the interaction between nodes according to their structure.\nLG ", "publication_ref": ["b21", "b24", "b40", "b18", "b24", "b10", "b24", "b41", "b3", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "F.3. Baseline Methods", "text": "We adopted three mainstream graph data augmentation methods as our baselines, including DropEdge, DropNode, Subgraph and Manifold-Mixup. The details of the baselines are listed as follows,\n\u2022 DropEdge 10 ( Rong et al., 2020). DropEdge randomly removes a certain ratios of edges from the input graph at each training epoch, which can prevent over-fitting and alleviate over-smoothing.\n\u2022 DropNode 11 (You et al., 2020). DropNode randomly remove certain portion of nodes as well as their connections, which under a underlying assumption that missing part of nodes will note affect the semantic meaning of original graph.\n\u2022 Subgraph 12 (You et al., 2020;Wang et al., 2020a). Subgraph method samples a subgraph from the original graph using random walk The generated graph will keep part of the the semantic meaning of original graphs.\n\u2022 M-Manifold 13 (Wang et al., 2021) Manifold-Mixup conducts Mixup operation for graph classification in the embedding space, which interpolates graph-level embedding after the READOUT function.", "publication_ref": ["b26", "b42", "b42", "b35", "b37"], "figure_ref": [], "table_ref": []}, {"heading": "F.4. Experimental Setting of Robustness", "text": "The graph neural network adopted in this experiment is GCN, the architecture of which is as above. For label corruption, we randomly corrupt the graph labels with different corruption ratio 10%, 20%, 30%, 40%. For topology corruption, we we randomly remove/add edges with different corruption ratio 10%, 20%, 30%, 40%. The dataset for topology corruption is REDDIT-BINARY.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G. Additional Experiments", "text": "In this appendix, we conduct additional experiments to further investigate the proposed method.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G.1. Visualization of Graphons on More Real-world Dataset", "text": "G-Mixup explores five graphon estimation methods, including sorting-and-smoothing (SAS) method (Chan & Airoldi, 2014), stochastic block approximation (SBA) (Airoldi et al., 2013), \"largest gap\" (LG) (Channarond et al., 2012), matrix completion (MC) (Keshavan et al., 2010) and the universal singular value thresholding (USVT) (Chatterjee et al., 2015). We present the estimated graphon by LG in Figure 2. Here we present more visualization of graphons on IMDB-BINARY, REDDIT-BINARY and IMDB-MULTI dataset. An obvious observation is that graphons of different classes of graphs are different. This observation further validates the divergence of graphon between different classes of graphs.\n10 https://github.com/DropEdge/DropEdge 11 https://github.com/Shen-Lab/GraphCL 12 https://github.com/Shen-Lab/GraphCL 13 https://github.com/vanoracai/MixupForGraph ", "publication_ref": ["b5", "b0", "b6", "b19", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "G.2. Experiment on More Graph Neural Networks Pooling Method (GMT)", "text": "To further validate the effectiveness of G-Mixup on more graph neural networks, we experiment with GMT (Baek et al., 2020), a modern pooling method. To reproduce GMT results, we the released code and the recommended hyperparameters for their used datasets (D&D, MUTAG, PROTEINS, IMDB-B, IMDB-M) in their paper. The results are presented in Table 7. 9 G-Mixup can significantly improve the performance of GMT. Table 7 shows that G-Mixup outperform all the baselines on all datasets. Overall, G-Mixup outperform vanilla, Dropedge, ManifoldMixup by 1.44%, 1.28%, 2.01%, respectively. This indicates the superiority of G-Mixup for graph classification task.", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}, {"heading": "G.3. Experiment on Molecular Property Prediction", "text": "We experiment on molecular property prediction task (Hu et al., 2020), including ogbg-molhiv, ogbg-molbace, ogbgmolbbbp. In these dataset, each graph represents a molecule, where nodes are atoms, and edges are chemical bonds. We adopte official reference graph neural network backbones (gcn, gcn-vitual, gin, gin-vitual) 14 as our backbones, and we generate the edge attributes randomly for synthetic graphs. The results are presented in Table 8. 10 G-Mixup can improve the performance of GNNs on molecular property prediction task with the experimental setting for a fair comparison. Table 8 shows that G-Mixup gains 9 best performances among 12 reported AUCs.", "publication_ref": ["b15"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Stochastic blockmodel approximation of a graphon: Theory and consistent estimation", "journal": "", "year": "2013", "authors": "E M Airoldi; T B Costa; S H Chan"}, {"ref_id": "b1", "title": "Centrality measures for graphons: Accounting for uncertainty in networks", "journal": "IEEE Transactions on Network Science and Engineering", "year": "2018", "authors": "M Avella-Medina; F Parise; M T Schaub; S Segarra"}, {"ref_id": "b2", "title": "Accurate learning of graph representations with graph multiset pooling", "journal": "", "year": "2020", "authors": "J Baek; M Kang; S J Hwang"}, {"ref_id": "b3", "title": "Spectral clustering with graph neural networks for graph pooling", "journal": "PMLR", "year": "2020", "authors": "F M Bianchi; D Grattarola; Alippi ; C "}, {"ref_id": "b4", "title": "Convergent sequences of dense graphs i: Subgraph frequencies, metric properties and testing", "journal": "Advances in Mathematics", "year": "2008", "authors": "C Borgs; J T Chayes; L Lov\u00e1sz; V T S\u00f3s; K Vesztergombi"}, {"ref_id": "b5", "title": "A consistent histogram estimator for exchangeable graph models", "journal": "", "year": "2014", "authors": "S Chan; E Airoldi"}, {"ref_id": "b6", "title": "Classification and estimation in the stochastic blockmodel based on the empirical degrees", "journal": "Electronic Journal of Statistics", "year": "2012", "authors": "A Channarond; J.-J Daudin; Robin ; S "}, {"ref_id": "b7", "title": "Matrix estimation by universal singular value thresholding", "journal": "The Annals of Statistics", "year": "2015", "authors": "S Chatterjee"}, {"ref_id": "b8", "title": "How robust are graph neural networks to structural noise", "journal": "", "year": "2019", "authors": "J Fox; S Rajamanickam"}, {"ref_id": "b9", "title": "Quick approximation to matrices and applications", "journal": "Combinatorica", "year": "1999", "authors": "A Frieze; R Kannan"}, {"ref_id": "b10", "title": "", "journal": "", "year": "2019", "authors": "H Gao; S Ji"}, {"ref_id": "b11", "title": "Out-of-manifold data augmentation for text classification", "journal": "", "year": "2020", "authors": "H Guo;  Nonlinear;  Mixup"}, {"ref_id": "b12", "title": "Augmenting data with mixup for sentence classification: An empirical study", "journal": "", "year": "2019", "authors": "H Guo; Y Mao; R Zhang"}, {"ref_id": "b13", "title": "Mixup as locally linear out-of-manifold regularization", "journal": "", "year": "2019", "authors": "H Guo; Y Mao; R Zhang"}, {"ref_id": "b14", "title": "Inductive representation learning on large graphs", "journal": "", "year": "2017", "authors": "W Hamilton; Z Ying; J Leskovec"}, {"ref_id": "b15", "title": "Open graph benchmark: Datasets for machine learning on graphs", "journal": "", "year": "2020", "authors": "W Hu; M Fey; M Zitnik; Y Dong; H Ren; B Liu; M Catasta; J Leskovec"}, {"ref_id": "b16", "title": "Training graph neural networks by graphon estimation", "journal": "", "year": "2021", "authors": "Z Hu; Y Fang; Lin ; L "}, {"ref_id": "b17", "title": "Adaptive sampling towards fast graph representation learning", "journal": "", "year": "2018", "authors": "W Huang; T Zhang; Y Rong; J Huang"}, {"ref_id": "b18", "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "journal": "PMLR", "year": "2015", "authors": "S Ioffe; C Szegedy"}, {"ref_id": "b19", "title": "Matrix completion from a few entries", "journal": "IEEE transactions on information theory", "year": "2010", "authors": "R H Keshavan; A Montanari; S Oh"}, {"ref_id": "b20", "title": "A method for stochastic optimization", "journal": "", "year": "2015", "authors": "D P Kingma; J Ba;  Adam"}, {"ref_id": "b21", "title": "Semi-supervised classification with graph convolutional networks", "journal": "", "year": "2016", "authors": "T N Kipf; M Welling"}, {"ref_id": "b22", "title": "Large networks and graph limits", "journal": "American Mathematical Soc", "year": "2012", "authors": "L Lov\u00e1sz"}, {"ref_id": "b23", "title": "Limits of dense graph sequences", "journal": "Journal of Combinatorial Theory, Series B", "year": "2006", "authors": "L Lov\u00e1sz; B Szegedy"}, {"ref_id": "b24", "title": "Rectified linear units improve restricted boltzmann machines", "journal": "", "year": "2010", "authors": "V Nair; G E Hinton"}, {"ref_id": "b25", "title": "Metropolis-hastings data augmentation for graph neural networks", "journal": "Advances in Neural Information Processing Systems", "year": "", "authors": "H Park; S Lee; S Kim; J Park; J Jeong; K.-M Kim; J.-W Ha; H J Kim"}, {"ref_id": "b26", "title": "Towards deep graph convolutional networks on node classification", "journal": "", "year": "2020", "authors": "Y Rong; W Huang; T Xu; J Huang;  Dropedge"}, {"ref_id": "b27", "title": "Graphon neural networks and the transferability of graph neural networks", "journal": "Advances in Neural Information Processing Systems", "year": "2020", "authors": "L Ruiz; L Chamon; A Ribeiro"}, {"ref_id": "b28", "title": "Graph and graphon neural network stability", "journal": "", "year": "2020", "authors": "L Ruiz; Z Wang; A Ribeiro"}, {"ref_id": "b29", "title": "Adversarial graph augmentation to improve graph contrastive learning", "journal": "", "year": "2021", "authors": "S Suresh; P Li; C Hao; Neville ; J "}, {"ref_id": "b30", "title": "Data augmentation for graph convolutional network on semi-supervised classification", "journal": "", "year": "2021", "authors": "Z Tang; Z Qiao; X Hong; Y Wang; F A Dharejo; Y Zhou; Y Du"}, {"ref_id": "b31", "title": "Graph attention networks", "journal": "", "year": "2017", "authors": "P Veli\u010dkovi\u0107; G Cucurull; A Casanova; A Romero; P Lio; Y Bengio"}, {"ref_id": "b32", "title": "Manifold mixup: Better representations by interpolating hidden states", "journal": "PMLR", "year": "2019", "authors": "V Verma; A Lamb; C Beckham; A Najafi; I Mitliagkas; D Lopez-Paz; Y Bengio"}, {"ref_id": "b33", "title": "Improved training of gnns for semi-supervised learning", "journal": "", "year": "2019", "authors": "V Verma; M Qu; K Kawaguchi; A Lamb; Y Bengio; J Kannala; J Tang;  Graphmix"}, {"ref_id": "b34", "title": "The laplacian spectrum of large graphs sampled from graphons", "journal": "IEEE Transactions on Network Science and Engineering", "year": "2021", "authors": "R Vizuete; F Garin; P Frasca"}, {"ref_id": "b35", "title": "Subgraph cropping for graph classification", "journal": "", "year": "2020", "authors": "Y Wang; W Wang; Y Liang; Y Cai; B Hooi;  Graphcrop"}, {"ref_id": "b36", "title": "Nodeaug: Semi-supervised node classification with data augmentation", "journal": "", "year": "2020", "authors": "Y Wang; W Wang; Y Liang; Y Cai; J Liu; B Hooi"}, {"ref_id": "b37", "title": "Mixup for node and graph classification", "journal": "", "year": "2021", "authors": "Y Wang; W Wang; Y Liang; Y Cai; B Hooi"}, {"ref_id": "b38", "title": "A comprehensive survey on graph neural networks", "journal": "", "year": "2020", "authors": "Z Wu; S Pan; F Chen; G Long; C Zhang; S Y Philip"}, {"ref_id": "b39", "title": "Learning graphons via structured gromov-wasserstein barycenters", "journal": "", "year": "2021", "authors": "H Xu; D Luo; L Carin; H Zha"}, {"ref_id": "b40", "title": "How powerful are graph neural networks?", "journal": "", "year": "2018", "authors": "K Xu; W Hu; J Leskovec; S Jegelka"}, {"ref_id": "b41", "title": "Hierarchical graph representation learning with differentiable pooling", "journal": "", "year": "2018", "authors": "R Ying; J You; C Morris; X Ren; W L Hamilton; J Leskovec"}, {"ref_id": "b42", "title": "Graph contrastive learning with augmentations", "journal": "Advances in Neural Information Processing Systems", "year": "2020", "authors": "Y You; T Chen; Y Sui; T Chen; Z Wang; Y Shen"}, {"ref_id": "b43", "title": "Bringing your own view: Graph contrastive learning without prefabricated data augmentations", "journal": "", "year": "2022", "authors": "Y You; T Chen; Z Wang; Y Shen"}, {"ref_id": "b44", "title": "Beyond empirical risk minimization", "journal": "", "year": "2017", "authors": "H Zhang; M Cisse; Y N Dauphin; D Lopez-Paz;  Mixup"}, {"ref_id": "b45", "title": "How does mixup help with robustness and generalization", "journal": "", "year": "2021", "authors": "L Zhang; Z Deng; K Kawaguchi; A Ghorbani; J Zou"}, {"ref_id": "b46", "title": "An end-toend deep learning architecture for graph classification", "journal": "", "year": "2018", "authors": "M Zhang; Z Cui; M Neumann; Chen ; Y "}, {"ref_id": "b47", "title": "Deep learning on graphs: A survey", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": "2020", "authors": "Z Zhang; P Cui; W Zhu"}, {"ref_id": "b48", "title": "Data augmentation for graph neural networks", "journal": "", "year": "2021", "authors": "T Zhao; Y Liu; L Neves; O Woodford; M Jiang; N Shah"}, {"ref_id": "b49", "title": "Graph theory and additive combinatorics", "journal": "", "year": "2019", "authors": "Y Zhao"}, {"ref_id": "b50", "title": "Graph neural networks: A review of methods and applications", "journal": "AI Open", "year": "2020", "authors": "J Zhou; G Cui; S Hu; Z Zhang; C Yang; Z Liu; L Wang; C Li; M Sun"}, {"ref_id": "b51", "title": "Data augmentation for graph classification", "journal": "", "year": "2020", "authors": "J Zhou; J Shen; Xuan ; Q "}], "figures": [{"figure_label": "23", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 .Figure 3 .23Figure 2. Estimated graphons on IMDB-BINARY, REDDIT-BINARY, and IMDB-MULTI. Obviously, graphons of different graph classes are quiet different. This observation validates the divergence of graphons between different classes of graphs, which is the basis of the G-Mixup. The graphons are estimated by LG. More estimated graphons via various methods are in Appendix G.1.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 5 .5Figure 5. The impact of the node numbers of generated synthetic graphs. The red vertical line indicates the average number of all the original training graphs. The blue line represents that classification accuracy with different number of nodes of generated graphs.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 6 .6Figure 6. The performance of G-Mixup using GCNs with different layers on IMDB-BINARY and REDDIT-BINARY.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "{G 1 , G 2 , \u2022 \u2022 \u2022 , G m } with label y G , and H = {H 1 , H 2 , \u2022 \u2022 \u2022 , H m } with label y H .Graphons W G and W H are estimated from graph sets G and H, respectively. Then, we mix them up by linearly interpolating the two graphons and their labels, and obtain W I and y I . Finally, a set of synthetic graphs I is sampled based on W I , which will be used as additional training graph data.", "figure_data": "and the above equations, the pro-posed G-Mixup includes three key steps: i) estimate a graphon for each class of graphs, ii) mix up the graphonsof different graph classes, and iii) generate syntheticgraphs based on the mixed graphons. Specifically, supposewe have two graph sets G ="}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "is the number of nodes in graph F , and ||W H \u2212 W G || denotes the cut norm 3 . Proof Sketch. The proof follows the derivation of Counting Lemma for Graphons (Lemma 10.23 in Lov\u00e1sz (2012)), which associates the homomorphism density with the cut norm ||W H \u2212 W G || of graphons. Specifically, we take the two graphons in this Lemma to deduce the bound of the difference of homomorphism densities of W I and W G /W H . Detailed proof are in Appendix A.2.Theorem 4.2 suggests that the difference in the homomorphism densities of the mixed graphon and original graphons is upper bounded. Note that difference depends on the hyperparameter \u03bb, the edge number e(F G )/e(F H ) and the cut norm ||W H \u2212 W G || . Since the e(F G )/e(F H ) and the cut norm ||W H \u2212 W G || are decided by the dataset (can be seen as a constant), the difference in homomorphism densities will be decided by \u03bb. On this basis, the label of the mixed graphon is set to \u03bby G + (1 \u2212 \u03bb)y H . Therefore, G-Mixup can preserve the different discriminative motifs of the two different graphons into one mixed graphon. 4.2. Will the generated graphs from graphon W I preserve the mixture of discriminative motifs?", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Dropedge 72.50\u00b10.31 49.08\u00b11.89 81.25\u00b18.15 51.35\u00b11.54 47.08\u00b10.55 w/ DropNode 72.00\u00b14.09 48.58\u00b12.85 79.25\u00b10.35 49.35\u00b11.80 47.93\u00b10.64 w/ Subgraph 68.50\u00b14.76 49.58\u00b12.61 74.33\u00b12.88 48.70\u00b11.63 47.49\u00b10.93 w/ M-Mixup 72.83\u00b11.75 49.50\u00b11.97 75.75\u00b14.53 49.82\u00b10.85 46.92\u00b11.05 w/ G-Mixup 72.87\u00b13.85 51.30\u00b12.14 89.81\u00b10.74 51.51\u00b11.70 48.06\u00b10.53 GIN vanilla 71.55\u00b13.53 48.83\u00b12.75 92.59\u00b10.86 55.19\u00b11.02 50.23\u00b10.83 w/ Dropedge 72.20\u00b11.82 48.83\u00b13.02 92.00\u00b11.13 55.10\u00b10.44 49.77\u00b10.76 w/ DropNode 72.16\u00b10.28 48.33\u00b10.98 90.25\u00b10.98 53.26\u00b14.99 49.95\u00b11.70 w/ Subgraph 68.50\u00b10.86 47.25\u00b13.78 90.33\u00b10.87 54.60\u00b13.15 49.67\u00b10.90 w/ M-Mixup 70.83\u00b11.04 49.88\u00b11.34 90.75\u00b11.78 54.95\u00b10.86 49.81\u00b10.80 w/ G-Mixup 71.94\u00b13.00 50.46\u00b11.49 92.90\u00b10.87 55.49\u00b10.53 50.50\u00b10.41", "figure_data": "Cross-entropy LossEpochEpochEpochEpochFigure 4. The training/validation/test curves on IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY and REDDIT-MULTI-5K with GCNas backbone. The curves are depicted on ten runs.Table 2. Performance comparisons of G-Mixup with differentGNNs on different datasets. The metric is the classification accu-racy. Experimental settings are in Appendix F.DatasetIMDB-BIMDB-M REDD-B REDD-M5 REDD-M12#graphs100015002000499911929#classes232511#avg.nodes19.7713.00429.63508.52391.41#avg.edges96.5365.94497.75594.87456.89GCNvanilla w/72.18\u00b11.55 48.79\u00b12.72 78.82\u00b11.33 45.07\u00b11.70 46.90\u00b10.73"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Performance comparisons of G-Mixup with different Pooling methods. The metric is classification accuracy.", "figure_data": "MethodIMDB-BIMDB-MREDD-BREDD-M5kTopKPoolvanilla w/ Dropedge w/ DropNode 69.16\u00b11.04 48.50\u00b12.50 81.33\u00b14.48 72.37\u00b15.01 50.57\u00b11.62 90.30\u00b11.47 71.75\u00b12.18 48.75\u00b12.94 88.96\u00b11.90 w/ Subgraph 67.83\u00b14.01 50.83\u00b12.38 86.08\u00b12.1245.07\u00b11.70 47.43\u00b11.82 46.15\u00b12.28 45.75\u00b12.47w/ M-Mixup71.83\u00b13.03 51.22\u00b11.17 87.58\u00b13.1645.60\u00b12.35w/ G-Mixup72.80\u00b13.33 51.30\u00b12.14 90.40\u00b10.8946.48\u00b11.70DiffPoolvanilla w/ Dropedge w/ DropNode 70.25\u00b13.01 46.83\u00b11.34 76.68\u00b12.57 71.68\u00b13.40 47.75\u00b12.34 78.40\u00b14.38 69.16\u00b12.51 49.44\u00b12.50 76.00\u00b15.5031.61\u00b15.95 34.46\u00b16.80 33.10\u00b15.53w/ Subgraph69.50\u00b12.16 46.00\u00b14.43 76.06\u00b12.8131.65\u00b14.43w/ M-Mixup66.50\u00b14.04 45.16\u00b14.63 78.37\u00b12.2934.46\u00b16.80w/ G-Mixup73.25\u00b13.89 50.70\u00b12.79 78.87\u00b12.2738.42\u00b16.51MincutPoolvanilla w/ Dropedge w/ DropNode 73.50\u00b13.89 49.91\u00b12.83 85.68\u00b12.04 73.25\u00b13.27 49.04\u00b13.57 84.95\u00b13.25 69.16\u00b12.51 49.66\u00b11.73 81.37\u00b11.59 w/ Subgraph 70.25\u00b11.84 48.18\u00b11.10 84.91\u00b12.50 w/ M-Mixup 70.62\u00b12.09 49.96\u00b11.86 85.12\u00b12.2949.32\u00b12.67 47.20\u00b11.10 46.82\u00b14.60 49.22\u00b12.49 47.20\u00b11.10w/ G-Mixup73.93\u00b12.84 50.29\u00b12.30 85.87\u00b11.3750.12\u00b12.47"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Robustness to label corruption with different ratios. 30\u00b13.67 69.43\u00b14.80 63.65\u00b18.87 55.21\u00b18.75 w/ Dropedge 72.00\u00b12.44 69.52\u00b13.25 64.12\u00b13.44 48.50\u00b10.00 w/ M-Mixup 71.87\u00b13.56 69.03\u00b14.85 65.62\u00b19.89 48.50\u00b10.00 w/ G-Mixup 72.56\u00b13.08 69.87\u00b15.41 65.50\u00b18.90 52.56\u00b16.97 REDD-B vanilla 73.90\u00b11.43 75.68\u00b12.75 68.12\u00b10.81 46.50\u00b10.00 w/ Dropedge 73.75\u00b11.28 72.06\u00b11.42 46.50\u00b10.00 46.50\u00b10.00 w/ M-Mixup 71.96\u00b11.97 76.00\u00b12.24 54.43\u00b11.09 46.50\u00b10.00 w/ G-Mixup 71.94\u00b13.00 76.34\u00b11.49 74.21\u00b11.85 53.50\u00b10.00 bilize the training of graph neural networks. Experiments on OGB", "figure_data": "ModelsMethods10%20%30%40%IMDB-B vanilla72."}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Robustness to topology corruption with different ratios. 96\u00b13.71 67.59\u00b15.73 64.96\u00b18.87 65.71\u00b18.31 edges w/ Dropedge 74.40\u00b12.26 65.12\u00b13.51 65.93\u00b12.32 57.87\u00b14.14 w/ M-Mixup 75.62\u00b11.59 65.81\u00b13.84 59.81\u00b19.45 57.31\u00b13.15 w/ G-Mixup 81.46\u00b13.08 71.12\u00b17.47 67.46\u00b18.90 66.25\u00b17.78 Mixup 73.41\u00b12.40 71.87\u00b11.28 71.50\u00b12.03 71.21\u00b12.00 w/ G-Mixup 84.31\u00b13.21 82.21\u00b14.31 77.00\u00b12.25 75.56\u00b13.05", "figure_data": "ModelsMethods10%20%30%40%Removing vanilla 77.Adding vanilla 76.12\u00b15.73 74.37\u00b16.48 72.31\u00b12.69 72.00\u00b12.92edgesw/ Dropedge 70.53\u00b11.47 70.18\u00b11.29 71.18\u00b11.53 70.90\u00b11.53w/ M-IMDB-BINARYREDDIT-BINARYAvg. #Nodes ofAvg. #Nodes ofOriginal GraphsOriginal Graphs"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "The MSE error of graphon estimation methods on synthetic graphs", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Algorithm 1 Graphon EstimationInput: graph set G, graphon estimator g each graph G has adjacency matrix A and node features matrix X Init: sorted adjacency matrix set\u0100 = {} for each graph G in G do Calculate the degree of each nodes in G Calculate sorted adjacency matrix\u0100 by sorting A based on the degree Calculate sorted node features matrixX by sorting X based on the degree Add the sorted adjacency matrix\u0100 to\u0100 end for Estimate step function W G with\u0100 using g.we use LG as g in experiments Obtain graphon node featureX G by average pooling X we can use other pooling method (e.g., maxpooling) Return: W G ,X G Synthetic graph set I = {}. Obtain two graph sets G and H with different labels y G and y H Estimate (W G ,X G ) and (W H ,X H ) from G and H using Algorithm 1 Mix up step functionW I = \u03bbW G + (1 \u2212 \u03bb)W H Mix up graphon node featuresX I = \u03bbX G + (1 \u2212 \u03bb)X H Sample \u03b1 \u2022|S| synthetic graphs based on W I andX I and add them to I |I| = \u03b1 \u2022 |S| after augmentation Return: synthetic graph set I (FCN) and the FCN loss is computed using Manifold Mixup.", "figure_data": "Algorithm 2 G-Mixup Input: train graph set S, graphon estimator g, mixup ratio \u03bb, augmented ratio \u03b1 Init:0 < \u03bb, \u03b1 < 1"}, {"figure_label": "910", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "The sensitivity of G-Mixup to Mixup Ratio \u03bb on ogbg-molbbbp dataset. The p-value is 0.00515, 0.0994, 0.0109, 0.0471 , indicating the 3 improvements are statistically significant (p < 0.05). 23\u00b10.75 68.23\u00b11.81 68.45\u00b10.84 67.54\u00b13.09 69.51\u00b11.20 67.79\u00b10.82 67.60\u00b11.31 69.48\u00b12.62 67.86\u00b11.02 68.78\u00b12.61 68.05\u00b11.52 GCN-virtual 68.57\u00b12.61 68.81\u00b11.57 67.20\u00b11.30 68.64\u00b12.09 70.05\u00b11.78 68.77\u00b12.31 69.11\u00b11.12 68.82\u00b10.98 69.07\u00b11.48 68.37\u00b10.95 65.13\u00b11.11 GIN 68.20\u00b11.04 69.37\u00b11.38 69.28\u00b11.24 68.89\u00b12.70 70.17\u00b11.03 66.95\u00b10.92 69.86\u00b11.05 70.01\u00b11.14 68.65\u00b11.03 69.73\u00b11.32 68.42\u00b12.31 GIN-virtual 70.58\u00b11.55 69.44\u00b11.88 70.02\u00b11.68 69.77\u00b10.88 69.18\u00b10.87 68.17\u00b11.67 68.62\u00b11.15 69.16\u00b11.87 70.15\u00b11.32 68.66\u00b10.68 67.10\u00b12.10 The sensitivity of G-Mixup to Mixup Ratio \u03bb on ogbg-molbace dataset. The p-value is 0.0227, 0.0375, 0.0401, 0.0427, indicating the 4 improvements are statistically significant (p < 0.05). 41\u00b12.24 77.33\u00b12.10 80.73\u00b12.06 78.42\u00b12.25 77.98\u00b12.03 79.25\u00b11.64 75.80\u00b14.31 78.40\u00b11.88 79.54\u00b11.25 77.90\u00b12.67 80.36\u00b11.56 GCN-virtual 75.64\u00b14.03 76.80\u00b11.74 73.55\u00b14.79 76.46\u00b11.05 73.97\u00b14.11 76.55\u00b12.28 75.91\u00b12.73 77.99\u00b12.59 78.34\u00b11.10 72.84\u00b15.52 74.49\u00b13.04 GIN 76.44\u00b12.19 75.55\u00b14.05 77.79\u00b13.34 75.20\u00b12.91 74.79\u00b12.64 76.27\u00b14.61 73.02\u00b13.68 76.29\u00b13.55 75.77\u00b12.30 74.12\u00b14.12 75.91\u00b11.01 GIN-virtual 74.51\u00b14.91 74.07\u00b12.76 73.53\u00b13.98 78.85\u00b11.98 77.15\u00b12.44 76.85\u00b13.42 79.69\u00b11.37 75.13\u00b15.46 77.04\u00b11.37 78.63\u00b12.04 74.19\u00b14.99", "figure_data": "\u03bb0.050.100.150.200.250.300.350.400.450.50VanillaGCN 68.\u03bb 0.050.100.150.200.250.300.350.400.450.50VanillaGCN77."}], "formulas": [{"formula_id": "formula_0", "formula_text": "F \u2192 G is a map from V (F ) to V (G), where if {u, v} \u2208 E(F ), then {\u03c6(u), \u03c6(v)} \u2208 E(G).", "formula_coordinates": [2.0, 307.44, 208.73, 235.25, 41.2]}, {"formula_id": "formula_1", "formula_text": ", G) = |V (G)| if graph H is , hom( , G) = 2|E(G)| if graph H is , and hom( ,", "formula_coordinates": [2.0, 307.19, 280.46, 234.25, 22.17]}, {"formula_id": "formula_2", "formula_text": "H ap- pears in graph G as t(H, G) = hom(H,G) |V (G)| |V (H)| . For example, t( , G) = |V (G)|/N 1 = 1, t( , G) = 2|E(G)|/N 2 .", "formula_coordinates": [2.0, 307.44, 340.49, 235.65, 44.94]}, {"formula_id": "formula_3", "formula_text": ") = [0,1] V (F ) i,j\u2208E(F ) W (x i , x j ) i\u2208V (F ) dx i . For example, the edge density of graphon W is t( , W ) = [0,1] 2 W (x, y) dxdy, and the triangle density of graphon W is t( , W ) = [0,1] 3 W (x, y)W (x, z)W (y, z) dxdydz.", "formula_coordinates": [2.0, 307.44, 554.95, 235.74, 52.79]}, {"formula_id": "formula_4", "formula_text": "W I W H W G I = {I 1 , I 2 , \u2022 \u2022 \u2022 , I k } with label (0.5, 0.5) H = {H 1 , H 2 , \u2022 \u2022 \u2022 , H k } with label (0, 1) G = {G 1 , G 2 , \u2022 \u2022 \u2022 , G k } with label (1, 0) \u2026 \u2026 \u2026 1) graphon estimation 3) graph sampling W I = 0.5 \u21e4 W G + 0.5 \u21e4 W H 1) graphon estimation 2) graphon mixup Figure 1. An overview of G-Mixup.", "formula_coordinates": [3.0, 54.94, 71.15, 485.06, 141.7]}, {"formula_id": "formula_5", "formula_text": "a (k) i = AGG (k) h (k\u22121) j : j \u2208 N (i) , h (k) i = COMBINE (k) h (k\u22121) i , a (k) i ,(1)", "formula_coordinates": [3.0, 88.83, 295.45, 200.61, 36.84]}, {"formula_id": "formula_6", "formula_text": "h G = READOUT h (k) i : i \u2208 E(G) , y = softmax(h G ),(2)", "formula_coordinates": [3.0, 85.37, 452.84, 337.16, 32.47]}, {"formula_id": "formula_7", "formula_text": "Graphon Estimation: G \u2192 W G , H \u2192 W H (3)", "formula_coordinates": [3.0, 313.17, 385.48, 228.27, 17.29]}, {"formula_id": "formula_8", "formula_text": "W I = \u03bbW G + (1 \u2212 \u03bb)W H (4) Graph Generation: {I 1 , I 2 , \u2022 \u2022 \u2022 , I m } i.i.d \u223c G(K, W I ) (5)", "formula_coordinates": [3.0, 313.17, 400.42, 228.27, 36.26]}, {"formula_id": "formula_9", "formula_text": "y I = \u03bby G + (1 \u2212 \u03bb)y H (6)", "formula_coordinates": [3.0, 425.49, 434.34, 115.95, 17.29]}, {"formula_id": "formula_10", "formula_text": "W I is I = {I 1 , I 2 , \u2022 \u2022 \u2022 , I m }.", "formula_coordinates": [3.0, 330.92, 510.08, 123.46, 17.29]}, {"formula_id": "formula_11", "formula_text": "a matrix W = [w kk ] \u2208 [0, 1] K\u00d7K ,", "formula_coordinates": [4.0, 139.15, 200.4, 151.53, 18.41]}, {"formula_id": "formula_12", "formula_text": "W P : [0, 1] 2 \u2192 [0, 1] is defined as W P (x, y) = K k,k =1", "formula_coordinates": [4.0, 54.85, 367.78, 234.59, 32.82]}, {"formula_id": "formula_13", "formula_text": "G = {G 1 , G 2 , \u2022 \u2022 \u2022 , G m } and H = {H 1 , H 2 , \u2022 \u2022 \u2022 , H m }", "formula_coordinates": [4.0, 55.44, 436.82, 234.0, 29.24]}, {"formula_id": "formula_14", "formula_text": "W I = \u03bbW G + (1 \u2212 \u03bb)W H \u2208 R K\u00d7K , which", "formula_coordinates": [4.0, 101.37, 519.38, 188.08, 18.41]}, {"formula_id": "formula_15", "formula_text": "u 1 , . . . , u K iid \u223c Unif [0,1] , G(K, W ) ij iid \u223c Bern(W (u i , u j )), \u2200i, j \u2208 [K].", "formula_coordinates": [4.0, 56.38, 608.27, 232.11, 34.7]}, {"formula_id": "formula_16", "formula_text": "Unif [0,1] on [0, 1].", "formula_coordinates": [4.0, 470.85, 93.46, 72.33, 11.01]}, {"formula_id": "formula_17", "formula_text": "Method Complexity MC O(N 3 ) USVT O(N 3 ) LG O(mN 2 ) SBA O(mKN log N ) SAS O(mN log N + K 2 log K 2 )", "formula_coordinates": [4.0, 438.56, 353.72, 99.21, 49.17]}, {"formula_id": "formula_18", "formula_text": "\u03bbW G + (1 \u2212 \u03bb)W H ?", "formula_coordinates": [5.0, 72.88, 190.08, 88.01, 17.29]}, {"formula_id": "formula_19", "formula_text": "W I = \u03bbW G + (1 \u2212 \u03bb)W H and that of the graphon W H /W G is upper bounded by |t(F G , W I ) \u2212 t(F G , W G )| \u2264 (1 \u2212 \u03bb)e(F G )||W H \u2212 W G || , |t(F H , W I ) \u2212 t(F H , W H )| \u2264 \u03bbe(F H )||W H \u2212 W G || where e(F )", "formula_coordinates": [5.0, 55.44, 307.49, 241.17, 73.17]}, {"formula_id": "formula_20", "formula_text": "P (|t(F I , G) \u2212 t(F I , W I )| > \u03b5) \u2264 2exp \u2212 \u03b5 2 n 8v(F I ) 2 .", "formula_coordinates": [5.0, 310.67, 118.44, 227.54, 25.66]}, {"formula_id": "formula_21", "formula_text": "W I = W H , {I 1 , I 2 , \u2022 \u2022 \u2022 , I m } i.i.d \u223c G(k, W I ), y I = y H . ii)", "formula_coordinates": [5.0, 307.44, 540.88, 234.67, 19.75]}, {"formula_id": "formula_22", "formula_text": "I = A W H , {I 1 , I 2 , \u2022 \u2022 \u2022 , I m } i.i.d \u223c G(k, W I ), y I = y H ,", "formula_coordinates": [5.0, 307.14, 591.3, 234.3, 24.99]}, {"formula_id": "formula_23", "formula_text": "* W0 + 0 * W1) / (0 * W0 + 1 * W1", "formula_coordinates": [6.0, 329.82, 351.77, 129.59, 8.06]}, {"formula_id": "formula_24", "formula_text": "W = sup S,T\u2282[0,1] S\u00d7T W (x, y)dxdy ,(7)", "formula_coordinates": [11.0, 203.43, 199.17, 338.01, 18.29]}, {"formula_id": "formula_25", "formula_text": "|t(F, W ) \u2212 t(F, W )| \u2264 e(F )||W \u2212 W ||(8)", "formula_coordinates": [11.0, 211.14, 300.09, 330.3, 17.29]}, {"formula_id": "formula_26", "formula_text": "|t(F, W ) \u2212 t(F, W )| = uivi\u2208E W (u i , v i ) \u2212 uivi\u2208E W (u i , v i ) v\u2208V dv \u2264 |E| i=1 \uf8eb \uf8ed i\u22121 j=1 W (u j , v j ) (W (u i , v i ) \u2212 W (u i , v i )) |E| k=i+1 W (u k , v k ) \uf8f6 \uf8f8 v\u2208V dv (9)", "formula_coordinates": [11.0, 130.32, 349.47, 411.12, 84.76]}, {"formula_id": "formula_27", "formula_text": "| t(F, W ) \u2212 t(F, W )| \u2264 e(F )||W \u2212 W || (10)", "formula_coordinates": [11.0, 209.75, 486.43, 331.69, 17.29]}, {"formula_id": "formula_28", "formula_text": "P (|t(F, G) \u2212 t(F, W )| > \u03b5) \u2264 2exp \u2212 \u03b5 2 n 8v(F ) 2", "formula_coordinates": [11.0, 195.7, 562.53, 196.46, 25.15]}, {"formula_id": "formula_29", "formula_text": "W I = \u03bbW G + (1 \u2212 \u03bb)W H . Let W = W I , W = W G , and F = F G in Lemma A.2, we have, |t(F G , W I ) \u2212 t(F G , W G )| \u2264 e(F G )||W I \u2212 W G || |t(F G , \u03bbW G + (1 \u2212 \u03bb)W H ) \u2212 t(F, W G )| \u2264 e(F G )||\u03bbW G + (1 \u2212 \u03bb)W H \u2212 W G || \u2264 e(F G )||(1 \u2212 \u03bb)(W H \u2212 W G )|| (12", "formula_coordinates": [11.0, 135.01, 622.26, 403.92, 68.94]}, {"formula_id": "formula_30", "formula_text": ")", "formula_coordinates": [11.0, 537.29, 660.31, 4.15, 8.64]}, {"formula_id": "formula_31", "formula_text": "\u03b1W = sup S,T \u2286[0,1] S\u00d7T \u03b1W = sup S,T \u2286[0,1] \u03b1 S\u00d7T W = \u03b1 W (13)", "formula_coordinates": [12.0, 164.27, 97.41, 377.18, 18.29]}, {"formula_id": "formula_32", "formula_text": "|t(F G , \u03bbW G + (1 \u2212 \u03bb)W H ) \u2212 t(F G , W G )| \u2264 e(F G )||(1 \u2212 \u03bb)(W H \u2212 W G )|| \u2264 (1 \u2212 \u03bb)e(F G )||W H \u2212 W G || (14)", "formula_coordinates": [12.0, 144.97, 160.49, 396.47, 32.23]}, {"formula_id": "formula_33", "formula_text": "|t(F H , \u03bbW G + (1 \u2212 \u03bb)W H ) \u2212 t(F H , W H )| \u2264 \u03bbe(F H )||W H \u2212 W G || (15)", "formula_coordinates": [12.0, 153.06, 230.12, 388.38, 17.29]}, {"formula_id": "formula_34", "formula_text": "P (|t(F I , G) \u2212 t(F I , W I )| > \u03b5) \u2264 2exp \u2212 \u03b5 2 n 8v(F I ) 2 (16)", "formula_coordinates": [12.0, 186.44, 312.78, 355.0, 25.27]}, {"formula_id": "formula_35", "formula_text": "W P (x, y) = K k,k =1 w kk 1 P k \u00d7P k (x, y),(17)", "formula_coordinates": [12.0, 201.43, 443.82, 340.01, 20.95]}, {"formula_id": "formula_36", "formula_text": "W \u2212 W \u2264 2 \u221a log K W L2 .(18)", "formula_coordinates": [12.0, 227.1, 547.75, 314.34, 23.92]}, {"formula_id": "formula_37", "formula_text": "A ij iid \u223c Bern(W ij ), \u2200i, j \u2208 [K].", "formula_coordinates": [13.0, 204.02, 427.9, 127.02, 19.75]}, {"formula_id": "formula_38", "formula_text": "W is \u03a0 K i=1 \u03a0 K j=1 (W 2 ij + (1 \u2212 W ij ) 2", "formula_coordinates": [13.0, 74.77, 503.54, 149.87, 18.41]}, {"formula_id": "formula_39", "formula_text": "\u03a0 K i=1 \u03a0 K j=1 (W\u00c3 ij ij (1 \u2212 W ij ) 1\u2212\u00c3ij ), which is extremely small since 0 < W\u00c3 ij ij (1 \u2212 W ij ) 1\u2212\u00c3ij < 1", "formula_coordinates": [13.0, 75.37, 551.61, 466.08, 35.41]}], "doi": ""}