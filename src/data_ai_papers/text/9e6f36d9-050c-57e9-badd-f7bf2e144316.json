{"title": "Open Set Domain Adaptation", "authors": "Pau Panareda Busto; Juergen Gall; Computer Vision Group", "pub_date": "", "abstract": "When the training and the test data belong to different domains, the accuracy of an object classifier is significantly reduced. Therefore, several algorithms have been proposed in the last years to diminish the so called domain shift between datasets. However, all available evaluation protocols for domain adaptation describe a closed set recognition task, where both domains, namely source and target, contain exactly the same object classes. In this work, we also explore the field of domain adaptation in open sets, which is a more realistic scenario where only a few categories of interest are shared between source and target data. Therefore, we propose a method that fits in both closed and open set scenarios. The approach learns a mapping from the source to the target domain by jointly solving an assignment problem that labels those target instances that potentially belong to the categories of interest present in the source dataset. A thorough evaluation shows that our approach outperforms the state-of-the-art.", "sections": [{"heading": "Introduction", "text": "For many applications, training data is scarce due to the high cost of acquiring annotated training data. Although there are large annotated image datasets publicly available, the images collected from the Internet often differ from the type of images which are relevant for a specific application. Depending on the application, the type of sensor or the perspective of the sensor, the entire captured scene might greatly differ from pictures on the Internet. The two types of images are therefore in two different domains, namely the source and target domain. In order to classify the images in the target domain using the annotated images in the source domain, the source and target domains can be aligned. In our case, we will map the feature space of the source domain to the feature space of the target domain. Any classifier can then be learned on the transformed data of the source domain to classify the images in the target domain. This process is termed domain adaptation and is further di- In this setting, both source and target domain contain images that do not belong to the classes of interest. Furthermore, the target domain contains images that are not related to any image in the source domain and vice versa.\nSource\nvided in unsupervised and semi-supervised approaches depending on whether the target images are unlabelled or partially labelled.\nBesides of the progress we have seen for domain adaptation over the last years [34,19,18,9,21,13,31,15], the methods have been so far evaluated using a setting where the images of the source and target domain are from the same set of categories. This setting can be termed closed set domain adaptation as illustrated in Fig. 1(a). An example of such a closed set protocol is the popular Office dataset [34].\nThe assumption that the target domain contains only images of the categories of the source domain is, however, unrealistic. For most applications, the dataset in the target domain contains many images and only a small portion of it might belong to the classes of interest. We therefore introduce the concept of open sets [28,37,36] to the domain adaptation problem and propose open set domain adaptation, which avoids the unrealistic assumptions of closed set domain adaptation. The differences between closed and open set domain adaptation are illustrated in Fig. 1.\nAs a second contribution, we propose a domain adaptation method that suits both closed and open sets. To this end, we map the feature space of the source domain to the target domain. The mapping is estimated by assigning images in the target domain to some categories of the source domain. The assignment problem is defined by a binary linear program that also includes an implicit outlier handling, which discards images that are not related to any image in the source domain. An overview of the approach is given in Fig. 2. The approach can be applied to the unsupervised or semi-supervised setting, where a few images in the target domain are annotated by a known category.\nWe provide a thorough evaluation and comparison with state-of-the-art methods on 24 combinations of source and target domains including the Office dataset [34] and the Cross-Dataset Analysis [44]. We revisit these evaluation datasets and propose a new open set protocol for domain adaptation, both unsupervised and semi-supervised, where our approach achieves state-of-the-art results in all settings.", "publication_ref": ["b33", "b18", "b17", "b8", "b20", "b12", "b30", "b14", "b33", "b27", "b36", "b35", "b33", "b43"], "figure_ref": ["fig_0", "fig_0", "fig_1"], "table_ref": []}, {"heading": "Related Work", "text": "The interest in studying domain adaptation techniques for computer vision problems increased with the release of a benchmark by Saenko et al. [34] for domain adaptation in the context of object classification. The first relevant works on unsupervised domain adaptation for object categorisation were presented by Golapan et al. [19] and Gong et al. [18], who proposed an alignment in a common subspace of source and target samples using the properties of Grassmanian manifolds. Jointly transforming source and target domains into a common low dimensional space was also done together with a conjugate gradient minimisation of a transformation matrix with orthogonality constraints [3] and with dictionary learning to find subspace interpolations [32,38,47]. Sun et al. [40,39] presented a very efficient solution based on second-order statistics to align a source domain with a target domain. Similarly, Csurka et al. [10] jointly denoise source and target samples to reconstruct data without partial random corruption. Sharing certain similarities with associations between domains, Gong et al. [17] minimise the Maximum Mean Discrepancy (MMD) [20] of two datasets. They assign instances to latent domains and solve it by a relaxed binary optimisation. Hsu et al. [31] use a similar idea allowing instances to be linked to all other samples.\nSemi-supervised domain adaptation approaches take advantage of knowing the class labels of a few target sam-ples. Aytar et al. [2] proposed a transfer learning formulation to regularise the training of target classifiers. Exploiting pairwise constraints across domains, Saenko et al. [34] and Kulis et al. [27] learn a transformation to minimise the effect of the domain shift while also training target classifiers. Following the same idea, Hoffman et al. [22] considered an iterative process to alternatively minimise the classification weights and the transformation matrix. In a different context, [7] proposed a weakly supervised approach to refine coarse viewpoint annotations of real images by synthetic images. In contrast to semi-supervised approaches, the task of viewpoint refinement assumes that all images in the target domain are labelled but not with the desired granularity.\nThe idea of selecting the most relevant information of each domain has been studied in early domain adaptation methods in the context of natural language processing [5]. Pivot features that behave the same way for discriminative learning in both domains were selected to model their correlations. Gong et al. [16] presented an algorithm that selects a subset of source samples that are distributed most similarly to the target domain. Another technique that deals with instance selection has been proposed by Sangineto et al. [35]. They train weak classifiers on random partitions of the target domain and evaluate them in the source domain. The best performing classifiers are then selected. Other works have also exploited greedy algorithms that iteratively add target samples to the training process, while the least relevant source samples are removed [6,42].\nSince CNN features show some robustness to domain changes [11], several domain adaptation approaches based on CNNs have been proposed [39,31,45,48]. Chopra et al. [9] extended the joint training of CNNs with source and target images by learning intermediate feature encoders and combine them to train a deep regressor. The MMD distance has been also proposed as regulariser to learn features for source and target samples jointly [14,46,29,30]. Ganin et al. [13] added a domain classifier network after the CNN to minimise the domain loss together with the classification loss. More recently, Ghifary et al. [15] combined two CNN models for labelled source data classification and for unsupervised target data reconstruction.\nStandard object classification tasks ignore the impact of impostors that are not represented by any of the object categories. These open sets started getting attention in face recognition tasks, where some test exemplars did not appear in the training database and had to be rejected [28]. Current techniques to detect unrelated samples in multi-class recognition with open sets have lately been revisited by Scheirer et al. [37]. [23] and [36] detect unknown instances by learning SVMs that assign probabilistic decision scores instead of class labels. Similarly, [49] and [4] add a regulariser to detect outliers and penalise a misclassification. The source domain contains some labelled images, indicated by the colours red, blue and green, and some images belonging to unknown classes (grey). For the target domain, we do not have any labels but the shapes indicate if they belong to one of the three categories or an unknown category (circle). (b) In the first step, we assign class labels to some target samples, leaving outliers unlabelled. (c) By minimising the distance between the samples of the source and the target domain that are labelled by the same category, we learn a mapping from the source to the target domain. The image shows the samples in the source domain after the transformation. This process iterates between (b) and (c) until it converges to a local minimum. (d) In order to label all samples in the target domain either by one of the three classes (red, green, blue) or as unknown (grey), we learn a classifier on the source samples that have been mapped to the target domain (c) and apply it to the samples of the target domain (a). In this image, two samples with unknown classes are wrongly classified as red or green.", "publication_ref": ["b33", "b18", "b17", "b2", "b31", "b37", "b46", "b39", "b38", "b9", "b16", "b19", "b30", "b1", "b33", "b26", "b21", "b6", "b4", "b15", "b34", "b5", "b41", "b10", "b38", "b30", "b44", "b47", "b8", "b13", "b45", "b28", "b29", "b12", "b14", "b27", "b36", "b22", "b35", "b48", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "Open Set Domain Adaptation", "text": "We present in this paper an approach that iterates between solving the labelling problem of target samples, i.e., associating a subset of the target samples to the known categories of the source domain, and computing a mapping from the source to the target domain by minimising the distances of the assignments. The transformed source samples are then used in the next iteration to re-estimate the assignments and update the transformation. This iterative process is repeated until convergence and is illustrated in Fig. 2.\nIn Section 3.1, we describe the unsupervised assignment of target samples to categories of the source domain. The semi-supervised case is described in Section 3.2. Section 3.3 finally describes how the mapping from the source domain to the target domain is estimated from the previous assignments. This part is the same for the unsupervised and semi-supervised setting.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Unsupervised Domain Adaptation", "text": "We first address the problem of unsupervised domain adaptation, i.e., none of the target samples are annotated, in an open set protocol. Given a set of classes C in the source domain, including |C \u2212 1| known classes and an additional unknown class that gathers all instances from other irrelevant categories, we aim to label the target samples T = {T 1 , . . . , T |T | } by a class c \u2208 C. We define the cost of assigning a target sample T t to a class c by\nd ct = S c \u2212 T t 2 2\nwhere T t \u2208 R D is the feature representation of the target sample t and S c \u2208 R D is the mean of all samples in the source domain labelled by class c. To increase the robustness of the assignment, we do not enforce that all target samples are assigned to a class as shown in Fig. 2(b). The cost of declaring a target sample as outlier is defined by a parameter \u03bb, which is discussed in Section 4.1.\nHaving defined the individual assignment costs, we can formulate the entire assignment problem by:\nminimise xct,ot t c d ct x ct + \u03bbo t subject to c x ct + o t = 1 \u2200t , t x ct \u2265 1 \u2200c , x ct , o t \u2208 {0, 1} \u2200c, t .(1)\nBy minimising the constrained objective function, we obtain the binary variables x ct and o t as solution of the assignment problem. The first type of constraints ensures that a target sample is either assigned to one class, i.e., x ct = 1, or declared as outlier, i.e., o t = 1. The second type of constraints ensures that at least one target sample is assigned to each class c \u2208 C. We use the constraint integer program package SCIP [1] to solve all proposed formulations.\nAs it is shown in Fig. 2(b), we label the targets also by the unknown class. Note that the unknown class combines all objects that are not of interest. Even if the unknowns in the source and target domain belong to different semantic classes, a target sample might be closer to the mean of all negatives than to any other positive class. In this case, we can confidentially label a target sample as unknown. In our experiments, we show that it makes not much difference if the unknown class is included in the unsupervised setting since the outlier handling discards target samples that are not close to the mean of negatives.", "publication_ref": ["b0"], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "Semi-supervised Domain Adaptation", "text": "The unsupervised assignment problem naturally extends to a semi-supervised setting when a few target samples are annotated. In this case, we only have to extend the formulation (1) by additional constraints that enforce that the annotated target samples do not change the label, i.e.,\nx\u0109 t t = 1 \u2200(t,\u0109 t ) \u2208 L, (2)\nwhere L denotes the set of labelled target samples and\u0109 t the class label provided for target sample t. In order to exploit the labelled target samples better, one can use the neighbourhood structure in the source and target domain. While the constraints remain the same, the objective function ( 1) can be changed to\nt c x ct d ct + t \u2032 \u2208Nt c \u2032 d cc \u2032 x c \u2032 t \u2032 + \u03bbo t ,(3)\nwhere\nd cc \u2032 = S c \u2212 S c \u2032 2\n2 . While in (1) the cost of labelling a target sample t by the class c is given only by d ct , a second term is added in (3). It is computed over all neighbours N t of t and adds the distance between the classes in the source domain as additional cost if a neighbour is assigned to another class than the target sample t.\nThe objective function (3), however, becomes quadratic and therefore NP-hard to solve. Thus, we transform the quadratic assignment problem into a mixed 0-1 linear program using the Kaufman and Broeckx linearisation [25]. By substituting\nw ct = x ct t \u2032 \u2208Nt c \u2032 x c \u2032 t \u2032 d cc \u2032 ,(4)\nwe derive to the linearised problem minimise\nxct,wct,ot t c\nd ct x ct + c w ct + \u03bbo t subject to c x ct + o t = 1 \u2200t , t x ct \u2265 1 \u2200c , a ct x ct + t \u2032 \u2208Nt c \u2032 d cc \u2032 x c \u2032 t \u2032 \u2212 w ct \u2264 a ct \u2200s, t , x ct , o t \u2208 {0, 1} \u2200c, t , w ct \u2265 0 \u2200c, t ,(5)\nwhere a ct = t \u2032 \u2208Nt c \u2032 d cc \u2032 .", "publication_ref": ["b24"], "figure_ref": [], "table_ref": []}, {"heading": "Mapping", "text": "As illustrated in Fig. 2, we iterate between solving the assignment problem, as described in Section 3.1 or 3.2, and estimating the mapping from the source domain to the target domain. We consider a linear transformation, which is represented by a matrix W \u2208 R D\u00d7D . We estimate W by minimising the following loss function:\nf (W ) = 1 2 t c x ct W S c \u2212 T t 2 2 ,(6)\nwhich we can rewrite in matrix form:\nf (W ) = 1 2 ||W P S \u2212 P T || 2 F . (7\n)\nThe matrices P S and P T \u2208 R DxL with L = t c x ct represent all assignments, where the columns denote the actual associations. The quadratic nature of the convex objective function may be seen as a linear least squares problem, which can be easily solved by any available QP solver. State-of-the-art features based on convolutional neural networks, however, are high dimensional and the number of target instances is usually very large. We use therefore nonlinear optimisation [41,24] to optimise f (W ). The derivatives of ( 6) are given by\n\u2202f (W ) \u2202W = W (P S P T S ) \u2212 P T P T S .(8)\nIf L < D, i.e., the number of samples, which have been assigned to a known class, is smaller than the dimensionality of the features, the optimisation also deals with an underdetermined linear least squares formulation. In this case, the solver converges to the matrix W with the smallest norm, which is still a valid solution.\nAfter the transformation W is estimated, we map the source samples to the target domain. We therefore iterate the process of solving the assignment problem and estimating the mapping from the source domain to the target domain until it converges. After the approach has converged, we train linear SVMs in a one-vs-one setting on the transformed source samples. For the semi-supervised setting, we also include the annotated target samples L (2) to the training set. The linear SVMs are then used to obtain the final labelling of the target samples as illustrated in Fig. 2(d).", "publication_ref": ["b40", "b23"], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "Experiments", "text": "We evaluate our method in the context of domain adaptation for object categorisation. In this setting, the images of the source domain are annotated by class labels and the goal is to classify the images in the target domain. We report the accuracies for both unsupervised and semi-supervised scenarios, where target samples are unlabelled or partially labelled, respectively. For consistency, we use libsvm [8] since it has also been used in other works, e.g., [12] and [39]. We set the misclassification parameter C = 0.001 in all experiments, which allows for a soft margin optimisation that works best in such classification tasks [12,39].", "publication_ref": ["b7", "b11", "b38", "b11", "b38"], "figure_ref": [], "table_ref": []}, {"heading": "Parameter configuration", "text": "Our algorithm contains a few parameters that need to be defined. For the outlier rejection, we use\n\u03bb = 0.5 max t,c d ct + min t,c d ct ,(9)\ni.e., \u03bb is adapted automatically based on the distances d ct , since higher values closer to the largest distance barely discard any outlier and lower values almost reject all assignments. We iterate the approach until the maximum number of 10 iterations is reached or if the distance\nc t x ct |S c,k \u2212 T t | 2 (10\n)\nis below \u01eb = 0.01 where S c,k corresponds to the transformed class mean at iteration k. In practice, the process converges after 3-5 iterations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Office dataset", "text": "We evaluate and compare our approach on the Office dataset [34], which is the standard benchmark for domain adaptation with CNN features. It provides three different domains, namely Amazon (A), DSLR (D) and Webcam (W). While the Amazon dataset contains centred objects on white background, the other two comprise pictures taken in an office environment but with different quality levels. In total, there are 31 common classes for 6 source-target combinations. This means that there are 4 combinations with a considerable domain shift (A \u2192 D, A \u2192 W, D \u2192 A, W \u2192 A) and 2 with a minor domain shift (D \u2192 W, W \u2192 D).\nWe introduce an open set protocol for this dataset by taking the 10 classes that are also common in the Caltech dataset [18] as shared classes. In alphabetical order, the classes 11-20 are used as unknowns in the source domain and 21-31 as unknowns in the target domain, i.e., the unknown classes in the source and target domain are not shared. For evaluation, each sample in the target domain needs to be correctly classified either by one of the 10 shared classes or as unknown. In order to compare with a closed setting (CS), we report the accuracy when source and target domain contain only samples of the 10 shared classes. Since OS is evaluated on all target samples, we also report the numbers when the accuracy is only measured on the same target samples as CS, i.e., only for the shared 10 classes. The latter protocol is denoted by OS * (10) and provides a direct comparison to CS (10). Additional results for the closed setting with all classes are reported in the supplementary material. Unsupervised domain adaptation We firstly compare the accuracy of our method in the unsupervised set-up with state-of-the-art domain adaptation techniques embedded in the training of CNN models. DAN [29]  model by freezing the first 3 convolutional layers, finetuning the last 2 and learning the weights from each fully connected layer by also minimising the discrepancy between both domains. RTN [30] extends DAN by adding a residual transfer module that bridges the source and target classifiers. BP [13] trains a CNN for domain adaptation by a gradient reversal layer and minimises the domain loss jointly with the classification loss. For training, we use all samples per class as proposed in [17], which is the standard protocol for CNNs on this dataset. As proposed in [13], we use for all methods linear SVMs for classification instead of the soft-max layer for a fair comparison.\nTo analyse the formulations that are discussed in Section 3, we compare several variants: ATI (Assign-and-Transform-Iteratively) denotes our formulation in (1) assigning a source class to all target samples, i.e., \u03bb = \u221e. Then, ATI-\u03bb includes the outlier rejection and ATI-\u03bb-N 1 is the unsupervised version of the locality constrained formulation corresponding to (3) with 1 nearest neighbour. In addition, we denote LSVM as the linear SVMs trained on the source domain without any domain adaptation.\nThe results of these techniques using the described open set protocol are shown in Table 1. Our approach ATI improves over the baseline without domain adaptation (LSVM) by +6.8% for CS and +14.3% for OS. The improvement is larger for the combinations that have larger domain shifts, i.e. with Amazon. We also observe that ATI  2. Open set domain adaptation on the unsupervised Office dataset with 10 shared classes (OS). We report the average and the standard deviation using a subset of samples per class in 5 random splits [34]. For comparison, results for closed set domain adaptation (CS) and modified open set (OS * ) are reported. outperforms all CNN-based domain adaptation methods for the closed (+2.2%) and open setting (+5.2%). It can also be observed that the accuracy for the open set is lower than for the closed set for all methods, but that our method handles the open set protocol best. While ATI-\u03bb does not obtain any considerable improvement compared to ATI in CS, the outlier rejection allows for an improvement in OS. The locality constrained formulation, ATI-\u03bb-N 1 , which we propose only for the semi-supervised setting, decreases the accuracy in the unsupervised setting.\nAdditionally, we report accuracies of popular domain adaptation methods that are not related to deep learning. We report the results of methods that transform the data to a common low dimensionality subspace, including Transfer Component Analysis (TCA) [33], Geodesic Flow Kernel (GFK) [18] and Subspace alignment (SA) [12]. In addition, we also include CORAL [39], which whitens and recolours the source towards the target data. Following the standard protocol of [34], we take 20 samples per object class when Amazon is used as source domain, and 8 for DSLR or Webcam. We extract feature vectors from the fully connected layer-7 (fc7) of the AlexNet model [26]. Each evaluation is executed 5 times with random samples from the source domain. The average accuracy and standard deviation of the five runs are reported in Table 2. The results are similar to the protocol reported in Table 1. Our approach ATI outperforms the other methods both for CS and OS and the additional outlier handling (ATI-\u03bb) does not improve the accuracy for the closed set but for the open set.", "publication_ref": ["b33", "b17", "b9", "b28", "b29", "b12", "b16", "b12", "b33", "b32", "b17", "b11", "b38", "b33", "b25"], "figure_ref": [], "table_ref": ["tab_0", "tab_0"]}, {"heading": "Impact of unknown class", "text": "The linear SVM that we employ in the open set protocol uses the unknown classes of the transformed source domain for the training. Since unknown object samples from the source domain are from different classes than the ones from the target domain, using an SVM that does not require any negative samples might be a better choice. Therefore, we compare the performance of a standard SVM classifier with a specific open set SVM (OS-SVM) [36], where only the 10 known classes are used for training. OS-SVM introduces an inclusion probability and labels target instances as unknown if this inclusion is not satisfied for any class. Table 3 compares the classification accuracies of both classifiers in the 6 domain shifts of the Office dataset. While the performance is comparable when no domain adaptation is applied, ATI-\u03bb obtains significantly better accuracies when the learning includes negative instances.\nAs discussed in Section 3.1, the unknown class is also part of the labelling set C for the target samples. The labelled target samples are then used to estimate the mapping W (6). To evaluate the impact of including the unknown class, Table 4 compares the accuracy when the unknown class is not included in C. Adding the unknown class improves the accuracy slightly since it enforces that the negative mean of the source is mapped to a negative sample in the target. The impact, however, is very small.\nAdditionally, we also analyse the impact of increasing the amount of unknown samples in both source and target domain on the configuration Amazon \u2192 DSLR+Webcam. Since the domain shift between DSLR and Webcam is close to zero (same scenario, but different cameras), they can be merged to get more unknown samples. Following the described protocol, we take 20 samples per known category, also in this case for the target domain, and we randomly increase the number of unknown samples from 20 to 400 in both domains at the same time. As shown in Table 5, that reports the mean accuracies of 5 random splits, adding more unknown samples decreases the accuracy if domain adaptation is not used (LSVM), but also for the domain adaption method CORAL [39]. This is expected since the unknowns are from different classes and the impact of the unknowns compared to the samples from the shared classes increases. Our method handles such an increase and the accuracies remain stable between 80.3% and 82.5%.", "publication_ref": ["b35", "b38"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Semi-supervised domain adaptation", "text": "We also evaluate our approach for open set domain adaptation on the Office dataset in its semi-supervised setting. Applying again the standard protocol of [34] with the subset of source samples, we also take 3 labelled target samples per class and leave the rest unlabelled. We compare our method with the  Table 4. Impact of including the unknown class to the set of classes C. The evaluation is performed on the unsupervised Office dataset with 10 shared classes using all samples per class [17].   6. Open set domain adaptation on the semi-supervised Office dataset with 10 shared classes (OS). We report the average and the standard deviation using a subset of samples per class in 5 random splits [34].\ndeep learning method MMD [46]. As baselines, we report the accuracy for the linear SVMs without domain adapta-tion (LSVM) when they are trained only on the source samples (s), only on the annotated target samples (t) or on both (st). As expected, the baseline trained on both performs best as shown in Table 6. Our approach ATI outperforms the baseline and the CNN approach [46]. As in the unsupervised case, the improvement compared to the CNN approach is larger for the open set (+4.8%) than for the closed set (+2.2%). While the locality constrained formulation, ATI-\u03bb-N , decreased the accuracy for the unsupervised setting, it improves the accuracy for the semi-supervised case since the formulation enforces that neighbours of the target samples are assigned to the same class. The results with one (ATI-\u03bb-N 1) or two neighbours (ATI-\u03bb-N 2) are similar.", "publication_ref": ["b33", "b16", "b33", "b45", "b45"], "figure_ref": [], "table_ref": []}, {"heading": "Dense Cross-Dataset Analysis", "text": "In order to measure the performance of our method and the open set protocol across popular datasets with more intra-class variation, we also conduct experiments on the dense set-up of the Testbed for Cross-Dataset Analysis [44]. This protocol provides 40 classes from 4 wellknown datasets, Bing (B), Caltech256 (C), ImageNet (I) and Sun (S). While the samples from the first 3 datasets are mostly centred and without occlusions, Sun becomes more challenging due to its collection of object class instances from cluttered scenes. As for the Office dataset, we take the first 10 classes as shared classes, the classes 11-25 are used as unknowns in the source domain and 26-40 as unknowns in the target domain. We use the provided DeCAF features (DeCAF7). Following the unsupervised protocol described in [43], we take 50 source samples per class for training and we test on 30 target images per class for all datasets, except Sun, where we take 20 samples per class.\nThe results reported in Table 7 are consistent with the Office dataset. ATI outperforms the baseline and the other methods by +4.4% for the closed set and by +5.3% for the open set. ATI-\u03bb obtains the best accuracies for the open set.", "publication_ref": ["b43", "b42"], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "Sparse Cross-Dataset Analysis", "text": "We also introduce an open set evaluation using the sparse set-up from [44] 10 10) CS ( 10) OS ( 10) CS ( 10) OS ( 10) CS ( 10) OS ( 10) CS ( 10) OS ( 10) CS ( 10) OS ( 10) CS ( 10   For each domain shift, we take all samples of the shared classes and consider all other samples as unknowns. Table 8 summarises the amount of shared classes for each shift and the percentage of unknown target samples, which varies from 30% to 90%.\nUnsupervised domain adaptation For the unsupervised experiment, we conduct a single run for each domain shift using all source and unlabelled target samples. The results are reported in Table 8. ATI outperforms the baseline and the other methods by +5.3% for this highly unbalanced open set protocol. ATI-\u03bb improves the accuracy of ATI slightly.\nSemi-supervised domain adaptation In order to evaluate the semi-supervised setting, we take all source samples and 3 annotated target samples per shared class as it is done in the semi-supervised setting for the Office dataset [34]. The  average and standard deviation over 5 random splits are reported in Table 9. While ATI improves over the baseline trained on the source and target samples together (st) by +4.5%, ATI-\u03bb and the locality constraints with one neighbour boost the performance further. ATI-\u03bb-N 1 improves the accuracy of the baseline by +5.1%.", "publication_ref": ["b43", "b33"], "figure_ref": [], "table_ref": ["tab_7", "tab_7", "tab_9"]}, {"heading": "Conclusions", "text": "In this paper we have introduced the concept of open set domain adaptation. In contrast to closed set domain adaptation, the source and target domain share only a subset of object classes whereas most samples of the target domain belong to classes not present in the source domain. We proposed new open set protocols for existing datasets and evaluated both CNN methods as well as standard unsupervised domain adaptation approaches. In addition, we have proposed an approach for unsupervised open set domain adaptation. The approach can also be applied to closed set domain adaptation and semi-supervised domain adaptation. In all settings, our approach achieves state-of-the-art results.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "SCIP: Solving constraint integer programs", "journal": "Mathematical Programming Computation", "year": "2009", "authors": "T Achterberg"}, {"ref_id": "b1", "title": "Tabula rasa: model transfer for object category detection", "journal": "", "year": "2011", "authors": "Y Aytar; A Zisserman"}, {"ref_id": "b2", "title": "Unsupervised domain adaptation by domain invariant projection", "journal": "", "year": "2013", "authors": "M Baktashmotlagh; M T Harandi; B C Lovell; M Salzmann"}, {"ref_id": "b3", "title": "Classification with a reject option using a hinge loss", "journal": "Journal of Machine Learning Research", "year": "2008", "authors": "P L Bartlett; M H Wegkamp"}, {"ref_id": "b4", "title": "Domain adaptation with structural correspondence learning", "journal": "", "year": "2006", "authors": "J Blitzer; R Mcdonald; F Pereira"}, {"ref_id": "b5", "title": "Domain adaptation problems: a DASVM classification technique and a circular validation strategy", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2010", "authors": "L Bruzzone; M Marconcini"}, {"ref_id": "b6", "title": "Adaptation of synthetic data for coarse-to-fine viewpoint refinement", "journal": "", "year": "2015", "authors": "P Busto; J Liebelt; J Gall"}, {"ref_id": "b7", "title": "LIBSVM: A library for support vector machines", "journal": "ACM Transactions on Intelligent Systems and Technology", "year": "2011", "authors": "C.-C Chang; C.-J Lin"}, {"ref_id": "b8", "title": "DLID: Deep learning for domain adaptation by interpolating between domains", "journal": "", "year": "2013", "authors": "S Chopra; S Balakrishnan; R Gopalan"}, {"ref_id": "b9", "title": "Unsupervised domain adaptation with regularized domain instance denoising", "journal": "", "year": "2016", "authors": "G Csurka; B Chidlowskii; S Clinchant; S Michel"}, {"ref_id": "b10", "title": "Decaf: A deep convolutional activation feature for generic visual recognition", "journal": "", "year": "2014", "authors": "J Donahue; Y Jia; O Vinyals; J Hoffman; N Zhang; E Tzeng; T Darrell"}, {"ref_id": "b11", "title": "Unsupervised visual domain adaptation using subspace alignment", "journal": "", "year": "2013", "authors": "B Fernando; A Habrard; M Sebban; T Tuytelaars"}, {"ref_id": "b12", "title": "Unsupervised domain adaptation by backpropagation", "journal": "", "year": "2015", "authors": "Y Ganin; V Lempitsky"}, {"ref_id": "b13", "title": "Domain adaptive neural networks for object recognition", "journal": "", "year": "2014", "authors": "M Ghifary; W B Kleijn; M Zhang"}, {"ref_id": "b14", "title": "Deep reconstruction-classification networks for unsupervised domain adaptation", "journal": "", "year": "2016", "authors": "M Ghifary; W B Kleijn; M Zhang; D Balduzzi; W Li"}, {"ref_id": "b15", "title": "Connecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation", "journal": "", "year": "2013", "authors": "B Gong; K Grauman; F Sha"}, {"ref_id": "b16", "title": "Reshaping visual datasets for domain adaptation", "journal": "", "year": "2013", "authors": "B Gong; K Grauman; F Sha"}, {"ref_id": "b17", "title": "Geodesic flow kernel for unsupervised domain adaptation", "journal": "", "year": "2012", "authors": "B Gong; Y Shi; F Sha; K Grauman"}, {"ref_id": "b18", "title": "Domain adaptation for object recognition: An unsupervised approach", "journal": "", "year": "2011", "authors": "R Gopalan; R Li; R Chellappa"}, {"ref_id": "b19", "title": "A kernel method for the two-sampleproblem", "journal": "", "year": "2006", "authors": "A Gretton; K M Borgwardt; M Rasch; B Sch\u00f6lkopf; A J Smola"}, {"ref_id": "b20", "title": "Asymmetric and category invariant feature transformations for domain adaptation", "journal": "International Journal of Computer Vision", "year": "2014", "authors": "J Hoffman; E Rodner; J Donahue; B Kulis; K Saenko"}, {"ref_id": "b21", "title": "Efficient learning of domain-invariant image representations", "journal": "", "year": "2013", "authors": "J Hoffman; E Rodner; J Donahue; K Saenko; T Darrell"}, {"ref_id": "b22", "title": "Multi-class open set recognition using probability of inclusion", "journal": "", "year": "2014", "authors": "L P Jain; W J Scheirer; T E Boult"}, {"ref_id": "b23", "title": "The NLopt nonlinear-optimization package", "journal": "", "year": "2007", "authors": "S G Johnson"}, {"ref_id": "b24", "title": "An algorithm for the quadratic assignment problem using bender's decomposition", "journal": "European Journal of Operational Research", "year": "1978", "authors": "L Kaufman; F Broeckx"}, {"ref_id": "b25", "title": "Imagenet classification with deep convolutional neural networks", "journal": "", "year": "2012", "authors": "A Krizhevsky; I Sutskever; G E Hinton"}, {"ref_id": "b26", "title": "What you saw is not what you get: domain adaptation using asymmetric kernel transforms", "journal": "", "year": "2011", "authors": "B Kulis; K Saenko; T Darrell"}, {"ref_id": "b27", "title": "Open set face recognition using transduction", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2005", "authors": "F Li; H Wechsler"}, {"ref_id": "b28", "title": "Learning transferable features with deep adaptation networks", "journal": "", "year": "2015", "authors": "M Long; Y Cao; J Wang; M Jordan"}, {"ref_id": "b29", "title": "Unsupervised domain adaptation with residual transfer networks", "journal": "", "year": "2016", "authors": "M Long; H Zhu; J Wang; M I Jordan"}, {"ref_id": "b30", "title": "Unsupervised domain adaptation with imbalanced cross-domain data", "journal": "", "year": "2015", "authors": "T. Ming Harry Hsu; W Yu Chen; C.-A Hou; Y.-H. Hubert Tsai; Y.-R Yeh; Y.-C. Frank Wang"}, {"ref_id": "b31", "title": "Subspace interpolation via dictionary learning for unsupervised domain adaptation", "journal": "", "year": "2013", "authors": "J Ni; Q Qiu; R Chellappa"}, {"ref_id": "b32", "title": "Domain adaptation via transfer component analysis", "journal": "", "year": "2009", "authors": "S J Pan; I W Tsang; J T Kwok; Q Yang"}, {"ref_id": "b33", "title": "Adapting visual category models to new domains", "journal": "", "year": "2010", "authors": "K Saenko; B Kulis; M Fritz; T Darrell"}, {"ref_id": "b34", "title": "Statistical and spatial consensus collection for detector adaptation", "journal": "", "year": "2014", "authors": "E Sangineto"}, {"ref_id": "b35", "title": "Probability models for open set recognition", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2014", "authors": "W J Scheirer; L P Jain; T E Boult"}, {"ref_id": "b36", "title": "Towards open set recognition", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2013", "authors": "W J Scheirer; A Rocha; A Sapkota; T E Boult"}, {"ref_id": "b37", "title": "Generalized domain-adaptive dictionaries", "journal": "", "year": "2013", "authors": "S Shekhar; V M Patel; H V Nguyen; R Chellappa"}, {"ref_id": "b38", "title": "Return of frustratingly easy domain adaptation", "journal": "", "year": "2015", "authors": "B Sun; J Feng; K Saenko"}, {"ref_id": "b39", "title": "From virtual to reality: Fast adaptation of virtual object detectors to real domains", "journal": "", "year": "2014", "authors": "B Sun; K Saenko"}, {"ref_id": "b40", "title": "A class of globally convergent optimization methods based on conservative convex separable approximations", "journal": "SIAM Journal on Optimization", "year": "2002", "authors": "K Svanberg"}, {"ref_id": "b41", "title": "Frustratingly easy NBNN domain adaptation", "journal": "", "year": "2013", "authors": "T Tommasi; B Caputo"}, {"ref_id": "b42", "title": "A deeper look at dataset bias", "journal": "CoRR", "year": "2015", "authors": "T Tommasi; N Patricia; B Caputo; T Tuytelaars"}, {"ref_id": "b43", "title": "A testbed for cross-dataset analysis", "journal": "", "year": "2014", "authors": "T Tommasi; T Tuytelaars"}, {"ref_id": "b44", "title": "Simultaneous deep transfer across domains and tasks", "journal": "", "year": "2015", "authors": "E Tzeng; J Hoffman; T Darrell; K Saenko"}, {"ref_id": "b45", "title": "Deep domain confusion: Maximizing for domain invariance", "journal": "", "year": "2014", "authors": "E Tzeng; J Hoffman; N Zhang; K Saenko; T Darrell"}, {"ref_id": "b46", "title": "bridging the domain shift by domain adaptive dictionary learning", "journal": "", "year": "2015", "authors": "H Xu; J Zheng; R Chellappa"}, {"ref_id": "b47", "title": "Semisupervised domain adaptation with subspace learning for visual recognition", "journal": "", "year": "2015", "authors": "T Yao; Y Pan; C.-W Ngo; H Li; T Mei"}, {"ref_id": "b48", "title": "Ro-svm: Support vector machine with reject option for image categorization", "journal": "", "year": "2006", "authors": "R Zhang; D N Metaxas"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. (a) Standard domain adaptation benchmarks assume that source and target domains contain images only of the same set of object classes. This is denoted as closed set domain adaptation since it does not include images of unknown classes or classes which are not present in the other domain. (b) We propose open set domain adaptation.In this setting, both source and target domain contain images that do not belong to the classes of interest. Furthermore, the target domain contains images that are not related to any image in the source domain and vice versa.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 .2Figure 2. Overview of the proposed approach for unsupervised open set domain adaptation. (a)The source domain contains some labelled images, indicated by the colours red, blue and green, and some images belonging to unknown classes (grey). For the target domain, we do not have any labels but the shapes indicate if they belong to one of the three categories or an unknown category (circle). (b) In the first step, we assign class labels to some target samples, leaving outliers unlabelled. (c) By minimising the distance between the samples of the source and the target domain that are labelled by the same category, we learn a mapping from the source to the target domain. The image shows the samples in the source domain after the transformation. This process iterates between (b) and (c) until it converges to a local minimum. (d) In order to label all samples in the target domain either by one of the three classes (red, green, blue) or as unknown (grey), we learn a classifier on the source samples that have been mapped to the target domain (c) and apply it to the samples of the target domain (a). In this image, two samples with unknown classes are wrongly classified as red or green.", "figure_data": ""}, {"figure_label": "95", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "9 Table 5 .9582.4 81.2 81.7 82.5 80.9 80.7 81.Impact of increasing the amount of unknown samples in the domain shift Amazon \u2192 DSLR+Webcam on the unsupervised Office dataset with 10 shared classes using 20 random samples per known class in both domains. A\u2192D A\u2192W CS (10) OS * (10) OS (10) CS (10) OS * (10) OS (10) LSVM (s) 85.8\u00b13.2 62.1\u00b17.9 65.9\u00b16.2 76.4\u00b12.1 45.7\u00b15.0 50.4\u00b14.5 LSVM (t) 92.3\u00b13.9 68.2\u00b15.2 71.1\u00b14.7 91.5\u00b14.9 59.6\u00b13.7 63.2\u00b13.4 LSVM (st) 95.7\u00b11.3 82.5\u00b13.0 84.0\u00b12.6 92.4\u00b11.8 72.5\u00b13.7 74.8\u00b13.4 MMD [46] 94.1\u00b12.3 86.1\u00b12.3 86.8\u00b12.2 92.4\u00b12.8 76.4\u00b11.5 78.3\u00b11.3 ATI 95.4\u00b11.3 89.0\u00b11.4 89.7\u00b11.3 95.9\u00b11.3 84.0\u00b11.7 85.1\u00b11.5 ATI-\u03bb 97.1\u00b11.1 89.5\u00b11.4 90.2\u00b11.3 96.1\u00b12.0 84.1\u00b11.8 85.2\u00b11.5 ATI-\u03bb-N1 97.6\u00b11.0 89.5\u00b11.3 90.3\u00b11.2 96.4\u00b11.7 84.4\u00b13.6 85.5\u00b11.5 ATI-\u03bb-N2 97.9\u00b11.4 89.4\u00b11.2 90.1\u00b11.0 92.8\u00b11.6 84.3\u00b12.4 85.4\u00b11.5 D\u2192A D\u2192W CS (10) OS * (10) OS (10) CS (10) OS * (10) OS (10) LSVM (s) 85.2\u00b11.7 40.3\u00b14.3 45.2\u00b13.8 97.2\u00b10.7 81.4\u00b12.4 83.0\u00b12.2 LSVM (t) 88.7\u00b12.2 52.8\u00b16.0 57.0\u00b15.5 91.5\u00b14.9 59.6\u00b13.7 63.2\u00b13.4 LSVM (st) 91.9\u00b10.7 68.7\u00b12.5 71.2\u00b12.3 98.7\u00b10.9 87.3\u00b12.3 88.5\u00b12.1 MMD [46] 90.2\u00b11.8 69.0\u00b13.4 71.3\u00b13.0 98.5\u00b11.0 85.5\u00b11.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "with the datasets Caltech101 (C), Pas-) OS (10) CS (10) OS (10) CS (10) OS (10) CS (10) OS (10) CS (10) OS (10) CS (", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "cal07 (P) and Office (O). These datasets are quite unbalanced and offer distinctive characteristics: Office contains centred class instances with barely any background (17 classes, 2300 samples in total, 68-283 samples per class), Caltech101 allows for more class variety (35 classes, 5545 samples in total, 35-870 samples per class) and Pascal07 gathers more realistic scenes with partially occluded objects in various image locations (16 classes, 12219 samples in total, 193-4015 samples per class).", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "retrains the AlexNet A\u2192D A\u2192W CS (10) OS * (10) OS (10) CS (10) OS * (10) OS (10)", "figure_data": "LSVM87.170.772.677.553.957.5DAN [29] 88.176.577.690.570.272.5RTN [30] 93.074.776.687.070.873.0BP [13]91.977.378.389.273.875.9ATI92.478.278.885.177.778.4ATI-\u03bb93.079.279.884.076.577.6ATI-\u03bb-N1 91.978.378.984.674.275.6D\u2192AD\u2192WCS (10) OS  *  (10) OS (10) CS (10) OS  *  (10) OS (10)LSVM79.440.045.197.987.588.5DAN [29] 83.453.557.096.187.588.4RTN [30] 82.853.857.297.988.189.0BP [13]84.354.157.697.588.989.8ATI93.470.071.198.592.292.6ATI-\u03bb93.870.071.398.593.293.5ATI-\u03bb-N1 93.365.667.897.994.094.4W\u2192AW\u2192DAVG.CS (10) OS  *  (10) OS (10) CS (10) OS  *  (10) OS (10) CS OS  *  OSLSVM80.044.949.210096.596.6 87.0 65.6 68.3DAN [29] 84.958.560.810097.598.3 90.5 74.0 75.8RTN [30] 85.160.262.410098.398.8 91.0 74.3 76.2BP [13]86.261.864.010098.098.7 91.6 75.7 77.4ATI93.476.476.610099.198.3 93.8 82.1 82.6ATI-\u03bb93.776.576.710099.298.3 93.7 82.4 82.9ATI-\u03bb-N1 93.471.672.410099.698.8 93.5 80.6 81.3"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "A\u2192D A\u2192W CS (10) OS * (10) OS (10) CS (10) OS * (10) OS (10)LSVM 84.4\u00b15.9 63.7\u00b16.7 66.6\u00b15.9 76.5\u00b12.9 48.2\u00b14.8 52.5\u00b14.2 TCA [33] 85.9\u00b16.3 75.5\u00b16.6 75.7\u00b15.9 80.4\u00b16.9 67.0\u00b15.9 67.9\u00b15.5 gfk [18] 84.8\u00b15.1 68.6\u00b16.7 70.4\u00b16.0 76.7\u00b13.1 54.1\u00b14.8 57.4\u00b14.2 SA [12] 84.0\u00b13.4 71.5\u00b15.9 72.6\u00b15.3 76.6\u00b12.8 57.4\u00b14.2 60.1\u00b13.7 CORAL [39] 85.8\u00b17.2 79.9\u00b15.7 79.6\u00b15.0 81.9\u00b12.8 68.1\u00b13.6 69.3\u00b13.1 ATI 91.4\u00b11.3 80.5\u00b12.0 81.1\u00b12.8 86.1\u00b11.1 73.4\u00b12.0 75.3\u00b11.7 (10) OS (10) CS (10) OS * (10) OS (10) CS OS * OS LSVM 72.5\u00b12.7 34.3\u00b14.9 39.9\u00b14.4 99.1\u00b10.5 89.8\u00b11.5 90.5\u00b11.3 84.1 58.9 62.5 TCA 85.5\u00b13.3 68.1\u00b15.1 68.6\u00b14.6 98.8\u00b10.9 94.1\u00b12.9 93.6\u00b12.6 89.5 78.1 78.2 gfk 75.0\u00b12.9 43.2\u00b15.1 47.6\u00b14.6 99.0\u00b10.5 92.0\u00b11.5 92.2\u00b11.4 85.2 64.7 67.3 SA 76.5\u00b13.2 49.7\u00b15.1 53.0\u00b14.6 98.8\u00b10.7 92.4\u00b12.9 92.4\u00b12.8 85.7 68.4 70.3 CORAL 86.9\u00b11.9 63.9\u00b14.9 65.6\u00b14.3 99.2\u00b10.7 96.0\u00b12.1 95.0\u00b12.0 90.1 77.6 78.2 ATI 92.2\u00b11.1 75.1\u00b11.7 76.0\u00b12.0 98.9\u00b11.3 95.5\u00b12.3 95.4\u00b12.1 93.2 80.7 81.5 ATI-\u03bb 92.4\u00b11.1 75.4\u00b11.8 76.4\u00b11.8 98.9\u00b11.3 96.5\u00b12.1 95.8\u00b11.8 93.2 81.5 82.3", "figure_data": "ATI-\u03bb91.1\u00b12.1 81.1\u00b10.4 82.2\u00b12.0 85.5\u00b12.1 73.7\u00b12.6 75.3\u00b11.4D\u2192AD\u2192WCS (10) OS  *  (10) OS (10) CS (10) OS  *  (10) OS (10)LSVM75.5\u00b12.1 36.1\u00b13.7 42.2\u00b13.3 96.2\u00b11.0 81.5\u00b11.5 83.1\u00b11.3TCA [33]88.2\u00b11.5 71.8\u00b12.5 71.8\u00b12.0 97.8\u00b10.5 92.0\u00b10.9 91.5\u00b11.0gfk [18]79.7\u00b11.0 45.3\u00b13.7 49.7\u00b13.4 96.3\u00b10.9 85.1\u00b12.7 86.2\u00b12.4SA [12]81.7\u00b10.7 52.5\u00b13.0 55.8\u00b12.7 96.3\u00b10.8 86.8\u00b12.5 87.7\u00b12.3CORAL [39] 89.6\u00b11.0 66.6\u00b12.8 68.2\u00b12.5 97.2\u00b10.7 91.1\u00b11.7 91.4\u00b11.5ATI93.5\u00b10.3 69.8\u00b11.4 70.8\u00b12.1 97.3\u00b10.5 89.6\u00b12.1 90.3\u00b11.8ATI-\u03bb93.9\u00b10.4 71.1\u00b10.9 72.0\u00b10.5 97.5\u00b11.1 92.1\u00b11.3 92.5\u00b10.7W\u2192AW\u2192DAVG.CS (10) OS  Table"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "SVM LSVM OS-SVM LSVM OS-SVM LSVM OS-SVM LSVM OS-SVM LSVM OS-SVM LSVM OS-SVM LSVM", "figure_data": "A\u2192DA\u2192WD\u2192AD\u2192WW\u2192AW\u2192DAVG.OS-No Adap. 67.572.658.457.554.845.180.088.555.349.294.096.668.368.3ATI-\u03bb72.079.865.377.666.471.382.293.571.676.792.798.375.082.9"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Comparison of a standard linear SVM (LSVM) with a specific open set SVM (OS-SVM)[37] on the unsupervised Office dataset with 10 shared classes using all samples per class[17].", "figure_data": "A\u2192D A\u2192W D\u2192A D\u2192W W\u2192A W\u2192D AVG.OS(10)ATI-\u03bb (C w/o unknown) 79.077.170.593.475.898.282.3ATI-\u03bb (C with unknown) 79.877.671.393.576.798.382.9"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "(10) OS (10) CS (10) OS * (10) OS (10) CS OS * OS LSVM (s) 78.8\u00b12.9 32.4\u00b13.8 38.2\u00b13.4 99.5\u00b10.3 88.7\u00b12.2 89.6\u00b11.9 87.1 58.4 62.0 LSVM (t) 88.7\u00b12.2 52.8\u00b16.0 57.0\u00b15.5 92.3\u00b13.9 68.2\u00b15.2 71.1\u00b14.7 90.9 60.2 63.8 LSVM (st) 90.8\u00b11.3 66.2\u00b14.4 69.0\u00b14.1 99.4\u00b10.7 93.5\u00b12.7 94.0\u00b12.5 94.8 78.4 80.3 MMD [46] 89.1\u00b13.2 65.1\u00b13.8 67.8\u00b13.4 98.2\u00b11.4 93.9\u00b12.9 94.4\u00b12.7 93.8 79.3 80.9 ATI 93.0\u00b10.5 71.3\u00b14.6 74.3\u00b14.3 99.3\u00b10.6 96.3\u00b11.8 96.6\u00b11.7 96.0 84.4 85.7 ATI-\u03bb 93.0\u00b10.5 71.5\u00b14.8 73.6\u00b14.4 99.5\u00b10.6 96.3\u00b11.8 96.6\u00b11.7 96.3 84.6 85.7 ATI-\u03bb-N1 93.0\u00b10.6 72.2\u00b14.5 74.2\u00b14.1 99.3\u00b10.6 96.7\u00b12.1 97.0\u00b11.9 96.4 84.9 86.0 ATI-\u03bb-N2 93.0\u00b10.6 72.8\u00b14.2 74.8\u00b13.9 99.3\u00b10.6 95.5\u00b12.2 95.9\u00b12.0 96.6 84.8 86.0", "figure_data": "6 86.7\u00b11.4ATI93.5\u00b10.2 74.4\u00b12.7 76.1\u00b12.5 98.7\u00b10.7 91.6\u00b11.7 92.4\u00b11.5ATI-\u03bb93.5\u00b10.2 74.4\u00b12.5 76.2\u00b12.3 98.7\u00b10.8 91.6\u00b11.7 92.4\u00b11.5ATI-\u03bb-N1 93.4\u00b10.2 74.6\u00b12.5 76.4\u00b12.3 98.9\u00b10.5 92.0\u00b11.6 92.7\u00b11.5ATI-\u03bb-N2 93.5\u00b10.1 74.9\u00b12.3 76.7\u00b12.1 99.3\u00b10.5 92.2\u00b11.9 92.9\u00b11.7W\u2192AW\u2192DAVG.CS (10) OS  Table"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": ") OS (10) LSVM 82.4\u00b12.4 66.6\u00b14.0 75.1\u00b10.4 59.0\u00b12.7 43.0\u00b12.0 24.2\u00b13.0 53.5\u00b12.1 40.1\u00b11.9 76.9\u00b14.3 62.5\u00b11.2 46.3\u00b12.7 28.2\u00b11.4 TCA [33] 74.9\u00b13.0 62.8\u00b13.8 68.4\u00b14.0 56.6\u00b14.5 38.3\u00b11.7 29.6\u00b14.2 49.2\u00b11.1 38.9\u00b11.9 73.1\u00b13.6 60.2\u00b11.4 45.9\u00b13.6 29.7\u00b11.6 gfk [18] 82.0\u00b12.2 66.2\u00b14.0 74.3\u00b11.0 58.3\u00b13.1 42.2\u00b11.4 23.8\u00b12.0 53.2\u00b12.6 40.2\u00b11.8 77.1\u00b13.3 62.2\u00b11.5 46.2\u00b13.0 28.5\u00b11.0 SA [12] 81.1\u00b11.8 66.0\u00b13.4 73.9\u00b10.9 57.8\u00b13.2 41.9\u00b12.4 24.3\u00b12.6 53.4\u00b12.5 40.3\u00b11.7 77.3\u00b14.2 62.5\u00b1.8 46.1\u00b13.3 29.0\u00b11.5 CORAL [39] 80.1\u00b13.5 68.8\u00b13.3 73.7\u00b12.0 60.9\u00b12.6 42.2\u00b12.4 27.2\u00b13.9 53.6\u00b12.9 40.7\u00b11.5 78.2\u00b15.1 64.0\u00b12.6 48.2\u00b13.9 31.4\u00b10.8 ATI 86.3\u00b11.6 71.4\u00b11.8 80.1\u00b10.7 68.0\u00b11.9 49.2\u00b13.2 36.8\u00b11.2 53.2\u00b13.4 45.4\u00b13.4 81.7\u00b13.7 66.7\u00b14.2 52.0\u00b13.4 35.8\u00b11.8 ATI-\u03bb 86.7\u00b11.3 71.4\u00b12.3 80.6\u00b12.4 69.0\u00b12.8 48.6\u00b12.5 37.4\u00b12.6 54.2\u00b11.9 45.7\u00b13.0 82.2\u00b13.7 67.9\u00b14.2 53.1\u00b12.8 37.5\u00b12.7", "figure_data": "I\u2192BI\u2192CI\u2192SS\u2192BS\u2192CS\u2192IAVG.CS (10) OS ("}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_6", "figure_caption": ") OS (10) LSVM 59.1\u00b12.0 42.7\u00b12.0 86.2\u00b12.6 73.3\u00b13.9 50.1\u00b14.0 32.1\u00b13.2 33.1\u00b11.7 16.4\u00b11.1 53.1\u00b12.6 27.9\u00b12.9 52.3\u00b11.8 25.2\u00b10.5 59.3 41.5 TCA [33] 56.1\u00b13.8 40.9\u00b12.9 83.4\u00b13.2 68.6\u00b11.8 49.3\u00b12.6 34.5\u00b13.8 30.6\u00b11.3 19.4\u00b12.1 47.5\u00b13.5 32.0\u00b13.9 45.2\u00b11.9 31.1\u00b14.7\u00b11.8 43.1\u00b11.6 85.9\u00b12.9 72.8\u00b13.1 50.0\u00b13.6 32.2\u00b13.7 34.2\u00b11.1 17.5\u00b11.6 52.5\u00b13.2 29.2\u00b14.2 52.6\u00b12.4 27.1\u00b11.3 59.0 41.1 CORAL [39] 58.5\u00b12.7 44.6\u00b12.5 85.8\u00b11.5 74.5\u00b13.4 49.5\u00b14.8 35.4\u00b14.4 32.9\u00b11.6 18.7\u00b11.2 52.1\u00b12.8 33.6\u00b15.3 52.9\u00b11.8 31.3\u00b11.3 59.0 44.2 ATI 57.9\u00b11.9 48.8\u00b12.3 89.3\u00b12.2 77.1\u00b12.6 55.0\u00b15.0 42.2\u00b14.0 34.9\u00b12.6 22.8\u00b13.1 59.8\u00b11.3 46.9\u00b12.5 60.8\u00b13.4 32.9\u00b12.2 63.4 49.5 ATI-\u03bb 58.6\u00b11.4 48.7\u00b11.8 89.7\u00b12.3 77.5\u00b12.2 55.3\u00b14.3 43.4\u00b14.8 34.1\u00b12.4 23.2\u00b13.2 60.2\u00b12.7 47.3\u00b12.9 60.3\u00b12.4 33.0\u00b11.1 63.6 50.2 Unsupervised open set domain adaptation on the Testbed dataset (dense setting) with 10 shared classes (OS). For comparison, results for closed set domain adaptation (CS) are reported. 36.2 61.0 29.7 79.1 72.6 54.2 SA [12] 46.4 36.8 61.1 30.2 79.8 71.1 54.2 CORAL [39] 48.0 35.9 60.2 29.1 78.9 68.8 53.5 ATI 51.6 52.1 63.1 38.8 80.6 70.9 59.5 ATI-\u03bb 51.5 52.0 63.4 39.1 81.1 71.1 59.7", "figure_data": "6 55.242.0"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Unsupervised open set domain adaptation on the sparse set-up from[44].", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_8", "figure_caption": ") 46.5\u00b10.1 36.2\u00b10.1 60.8\u00b10.3 29.7\u00b10.0 79.5\u00b10.3 73.5\u00b10.7 54.4 LSVM (t) 53.1\u00b13.7 44.6\u00b12.1 73.7\u00b11.5 40.5\u00b13.0 81.1\u00b12.5 70.5\u00b14.3 60.6 LSVM (st) 56.0\u00b11.3 44.5\u00b11.2 68.9\u00b11.1 40.9\u00b12.2 80.9\u00b10.6 76.7\u00b10.3 61.3 ATI 59.6\u00b11.2 55.2\u00b11.3 75.8\u00b11.2 45.2\u00b11.4 81.6\u00b10.2 77.1\u00b10.8 65.8 ATI-\u03bb 60.3\u00b11.2 56.0\u00b11.2 75.8\u00b11.1 45.8\u00b11.2 81.8\u00b10.2 76.9\u00b11.3 66.1 ATI-\u03bb-N1 60.7\u00b11.2 56.3\u00b11.2 76.7\u00b11.6 45.8\u00b11.4 82.0\u00b10.4 76.7\u00b11.1 66.4", "figure_data": "C\u2192OC\u2192PO\u2192CO\u2192PP\u2192CP\u2192O AVG.LSVM (s"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Semi-supervised open set domain adaptation on the sparse set-up from[44] with 3 labelled target samples per shared class.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "d ct = S c \u2212 T t 2 2", "formula_coordinates": [3.0, 50.11, 641.58, 76.43, 14.37]}, {"formula_id": "formula_1", "formula_text": "minimise xct,ot t c d ct x ct + \u03bbo t subject to c x ct + o t = 1 \u2200t , t x ct \u2265 1 \u2200c , x ct , o t \u2208 {0, 1} \u2200c, t .(1)", "formula_coordinates": [3.0, 336.48, 379.09, 208.63, 88.92]}, {"formula_id": "formula_2", "formula_text": "x\u0109 t t = 1 \u2200(t,\u0109 t ) \u2208 L, (2)", "formula_coordinates": [4.0, 117.44, 160.31, 168.93, 10.31]}, {"formula_id": "formula_3", "formula_text": "t c x ct d ct + t \u2032 \u2208Nt c \u2032 d cc \u2032 x c \u2032 t \u2032 + \u03bbo t ,(3)", "formula_coordinates": [4.0, 79.44, 268.84, 206.92, 31.33]}, {"formula_id": "formula_4", "formula_text": "d cc \u2032 = S c \u2212 S c \u2032 2", "formula_coordinates": [4.0, 78.38, 300.32, 83.25, 12.48]}, {"formula_id": "formula_5", "formula_text": "w ct = x ct t \u2032 \u2208Nt c \u2032 x c \u2032 t \u2032 d cc \u2032 ,(4)", "formula_coordinates": [4.0, 101.29, 450.74, 185.07, 21.12]}, {"formula_id": "formula_6", "formula_text": "d ct x ct + c w ct + \u03bbo t subject to c x ct + o t = 1 \u2200t , t x ct \u2265 1 \u2200c , a ct x ct + t \u2032 \u2208Nt c \u2032 d cc \u2032 x c \u2032 t \u2032 \u2212 w ct \u2264 a ct \u2200s, t , x ct , o t \u2208 {0, 1} \u2200c, t , w ct \u2265 0 \u2200c, t ,(5)", "formula_coordinates": [4.0, 51.77, 509.23, 242.92, 142.91]}, {"formula_id": "formula_7", "formula_text": "f (W ) = 1 2 t c x ct W S c \u2212 T t 2 2 ,(6)", "formula_coordinates": [4.0, 350.21, 127.55, 194.9, 27.02]}, {"formula_id": "formula_8", "formula_text": "f (W ) = 1 2 ||W P S \u2212 P T || 2 F . (7", "formula_coordinates": [4.0, 369.17, 177.33, 172.07, 23.54]}, {"formula_id": "formula_9", "formula_text": ")", "formula_coordinates": [4.0, 541.24, 184.81, 3.87, 8.91]}, {"formula_id": "formula_10", "formula_text": "\u2202f (W ) \u2202W = W (P S P T S ) \u2212 P T P T S .(8)", "formula_coordinates": [4.0, 360.85, 333.75, 184.26, 23.09]}, {"formula_id": "formula_11", "formula_text": "\u03bb = 0.5 max t,c d ct + min t,c d ct ,(9)", "formula_coordinates": [5.0, 107.97, 125.59, 178.39, 14.8]}, {"formula_id": "formula_12", "formula_text": "c t x ct |S c,k \u2212 T t | 2 (10", "formula_coordinates": [5.0, 128.66, 222.41, 153.56, 22.44]}, {"formula_id": "formula_13", "formula_text": ")", "formula_coordinates": [5.0, 282.21, 225.32, 4.15, 8.91]}], "doi": ""}