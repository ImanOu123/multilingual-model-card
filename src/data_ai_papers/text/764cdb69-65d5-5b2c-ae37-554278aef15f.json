{"title": "Fast and Memory-E icient Tucker decomposition for Answering Diverse Time Range eries", "authors": "Jun-Gi Jang; U Kang", "pub_date": "", "abstract": "Given a temporal dense tensor and an arbitrary time range, how can we e ciently obtain latent factors in the range? Tucker decomposition is a fundamental tool for analyzing dense tensors to discover hidden factors, and has been exploited in many data mining applications. However, existing decomposition methods do not provide the functionality to analyze a speci c range of a temporal tensor. The existing methods are one-o , with the main focus on performing Tucker decomposition once for a whole input tensor. Although a few existing methods with a preprocessing phase can deal with a time range query, they are still time-consuming and su er from low accuracy. In this paper, we propose Z T , a fast and memory-e cient Tucker decomposition method for nding hidden factors of temporal tensor data in an arbitrary time range. Z T allows us to e ciently deal with various time range queries. Z T fully exploits block structure to compress a given tensor, supporting an e cient query and capturing local information. Z T answers diverse time range queries quickly and memory-e ciently, by elaborately decoupling the preprocessed results included in the range and carefully determining the order of computations. We demonstrate that Z T is up to 171.9\u00d7 faster and requires up to 230\u00d7 less space than existing methods while providing comparable accuracy.", "sections": [{"heading": "Time dimension", "text": "Temporal tensor From Jan. 1,2008 To May 6, 2020\n\u2460 \u2461 \u2462\nTucker results for the time range query", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "COVID-19", "text": "Start time -Jan. 1,2020 End time -Apr. 30, 2020", "publication_ref": ["b0", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Given Given Goal", "text": "A time range query Figure 1: Given a temporal tensor and a user-provided time range (start time and end time) query, the goal of the timeranged Tucker decomposition is to nd the patterns of the temporal tensor at the range using Tucker decomposition.\nTensor decomposition has played an important role in various applications including data clustering [4,10], concept discovery [1,13,14], dimensionality reduction [16,36], anomaly detection [18], and link prediction [19,24]. Tucker decomposition, one of the tensor decomposition methods, has been recognized as a crucial tool for discovering latent factors and detecting relations between latent factors.\nIn practice, we analyze a given temporal tensor from various perspectives. Assume a user is interested in investigating patterns of various time ranges using Tucker decomposition. Given a temporal tensor and a user-provided time range (start time and end time) query, our goal is to nd the patterns of the temporal tensor at the range using Tucker decomposition. For example, given a temporal tensor including matrices collected between Jan. 1,2008 to May 6, 2020, a user may be interested in Tucker decomposition of a subrange between Jan. 1, 2020 to April 30, 2020 (see Figure 1). Since Tucker decomposition generates factor matrices and a core tensor to accurately approximate an input tensor, answering time range queries, (i.e., performing Tucker decomposition of di erent sub-tensors) yields di erent Tucker results. However, conventional Tucker decomposition methods [5,20,25] based on Alternating Least Square (ALS) is not appropriate for answering diverse time range queries since they target performing Tucker decomposition once for a given tensor; the methods require a high computational cost and large storage space since they need to perform Tucker decomposition of the sub-tensor included in a time range query from scratch, every time the query is given. Due to this limitation, the existing methods are not e cient in exploring diverse time ranges for a given temporal tensor.\nA few methods [12,35] with a preprocessing phase can be adapted to the time range query problem; before the query phase, they preprocess a given tensor, and perform Tucker decomposition with the preprocessed tensor for each time range query. However, they su er from an accuracy issue for narrow time ranges since preprocessed results are tailored for performing Tucker decomposition of the whole given temporal tensor. The results fail to capture local patterns that appear only in a speci c range.   In this paper, we propose Z T (Zoomable Tucker decomposition), a fast and memory-e cient Tucker decomposition method to analyze a temporal tensor for diverse time ranges. Z T enables us to discover local patterns in a narrow time range (zoom-in), or global patterns in a wider time range (zoomout). Z T consists of two phases: the preprocessing phase and the query phase. The preprocessing phase of Z T exploits block structure to lay the groundwork in achieving an e cient query phase and capturing local information. In the query phase, Z T addresses the high computational cost and space cost by elaborately decoupling block results and carefully determining the order of computation. Thanks to these ideas, Z T answers an arbitrary time range query with higher e ciency than existing methods. Through extensive experiments, we demonstrate the e ectiveness and e ciency of our method compared to other methods. The main contributions of this paper are as follows:\n\u2022 Algorithm. We propose Z T , a fast and memorye cient Tucker decomposition method for answering diverse time range queries.\n\u2022 Analysis. We provide both time and space complexities for the preprocessing and query phases of Z T .\n\u2022 Experiment. Experimental results show that Z T answers time range queries up to 171.9\u00d7 faster and requires up to 230\u00d7 less space than other methods while providing comparable accuracy, as shown in Figures 2 and 6.\n\u2022 Discovery. Thanks to Z T , we discover anomalous ranges and trend changes in Stock dataset (Figures 8  and 9). The code of our method and datasets are available at https: //datalab.snu.ac.kr/zoomtucker.", "publication_ref": ["b3", "b9", "b0", "b12", "b13", "b15", "b35", "b17", "b18", "b23", "b0", "b4", "b19", "b24", "b11", "b34"], "figure_ref": ["fig_6", "fig_6", "fig_1", "fig_12"], "table_ref": []}, {"heading": "PRELIMINARIES", "text": "We describe preliminaries on tensor operations and Tucker decomposition, and then de ne the problem addressed in this paper (Section 2.3). The symbols we use in this paper are described in Table 1.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Tensor and Its Operation", "text": "A tensor is a multi-dimensional array. Each dimension of a tensor is called . The length of each mode is called dimensionality and denoted by 1 , \u2022 \u2022 \u2022 , . In this paper, a vector, a matrix, and an -mode tensor are denoted by the boldface lower case (e.g. a), boldface capitals (e.g. A), and boldface Euler script capital (e.g. X \u2208 R 1 \u00d7 2 \u00d7\u2022\u2022\u2022\u00d7 ), respectively. Key operations for tensor include Frobenius norm, Kronecker product, mode-matricization, and -mode product. We refer the reader to [17] for their de nitions.", "publication_ref": ["b16"], "figure_ref": [], "table_ref": []}, {"heading": "Tucker Decomposition", "text": "Tucker decomposition transforms an -order tensor X \u2208 R 1 \u00d7...\u00d7 into a core tensor G \u2208 R 1 \u00d7...\u00d7 and factor matrices A ( ) \u2208 R \u00d7 for = 1, ..., . Factor matrices A ( ) are column-orthogonal, and a core tensor G is small and dense. Each factor matrix A ( ) represents the latent features of the -th mode of X, and each element of a core tensor G is the weight of the relation composed of columns of factor matrices. Given a tensor X, the goal of Tucker decomposition is to obtain factor matrices A ( ) and the core tensor G by minimizing X \u2212 G \u00d7 1 A (1) \u2022 \u2022 \u2022 \u00d7 A ( ) 2 as shown in the following equations:\nX \u2248 G \u00d7 1 A (1) \u2022 \u2022 \u2022 \u00d7 A ( ) \u21d4 X ( ) \u2248 A ( ) G ( ) (\u2297 \u2260 A ( ) )(1)\nNote that X ( ) indicates the mode-matricized version of X, G ( ) indicates the mode-matricized version of G, and (\u2297 \u2260 A ( ) )\nindicates performing the entire Kronecker product of A ( ) in descending order for = , ..., + 1, \u2212 1, ..., 1. ALS (Alternating Least Square) is a common approach for Tucker decomposition as described in Appendix A. ALS approach iteratively updates a factor matrix of a mode while xing all factor matrices of other modes. For updating each factor matrix A ( ) , a dominant operation is to compute -mode products between an input tensor X (\u2208 1 \u00d7 ... \u00d7 ) and factor matrices A ( ) (\u2208 \u00d7 ) for = , ..., + 1, \u2212 1, ..., 1 (line 4 of Algorithm 3). Computing X ( ) (\u2297 \u2260 A ( ) ), the mode-matricized version of the dominant operation, takes O( =1 ) time where X ( ) is the mode-matricized version of X.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "Problem De nition", "text": "We describe the formal de nition of the time range query problem as follows: P 1 (T R T T ). Given: a temporal dense tensor X \u2208 R 1 \u00d7 2 \u2022\u2022\u2022\u00d7 and a time range [ , ] where is the length of the time dimension, and is the dimensionality of mode-for = 1, ..., \u2212 1, Find: the Tucker results of the sub-tensorX of X in the time range [ , ] e ciently. The Tucker result includes factor matrices\u00c3 (1) , ..., A ( ) , and core tensorG.\nTo address the time range query problem, a method should eciently handle various time range queries. Given an arbitrary time range query, existing methods [5,20,25] performing Tucker decomposition from scratch requires a high computational cost and large space cost. Compared to the aforementioned methods, Tucker decomposition methods [12,35] with a preprocessing phase save time and space costs in that they allow us to compress a whole tensor before a query phase, and then perform Tucker decomposition of a sub-tensor corresponding to a given time range query by exploiting the compressed tensor instead of the input tensor. However, they are still unsatisfactory in terms of time, space, and accuracy for the time range query problem since they are tailored for performing Tucker decomposition of only the whole tensor once. ", "publication_ref": ["b0", "b4", "b19", "b24", "b11", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "PROPOSED METHOD", "text": "X \u2208 R 1 \u00d7 2 \u00d7\u2022\u2022\u2022\u00d7 \u22121 \u00d7\nOutput: result sets C for = 1, ..., + 1 Parameters: block size 1: compute the number = of blocks 2: split X into block tensors X < > \u2208 R 1 \u00d7...\u00d7 for = 1, ..., 3: for \u2190 1 to do 4:\nperform Tucker decomposition of X \u2248 G < > \u00d7 1 (A < > ) (1) \u2022 \u2022 \u2022 \u00d7 (A < > ) ( ) 5:\nstore each factor matrices (A < > ) ( ) in the results set C , for = 1, ..., 6:\nstore core tensor G < > in the result set C +1 7: end for We address the challenges with the following main ideas:\nI1 Exploiting block structure enables a query phase to decrease the number of operations and memory requirements, while capturing local information. I2 Elaborately decoupling block results decreases the computational cost of Tucker decomposition for a tensor obtained in a given time range. I3 Carefully determining the order of computation minimizes intermediate data generation while avoiding redundant computation. Z T e ciently computes Tucker decomposition for various time range queries. Z T consists of two phases: the preprocessing phase and the query phase. The preprocessing phase is computed once for a given temporal tensor, while the query phase is computed using the results of the preprocessing phase for each time range query. Z T compresses a given tensor block by block along the time dimension in the preprocessing phase. Z T performs Tucker decomposition for each block. In the query phase, Z T performs Tucker decomposition for each time range query by 1) adjusting the rst and the last blocks included in the time range to t the range and 2) carefully stitching the block results in the time range.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "Preprocessing Phase", "text": "The objective of the preprocessing phase is to manipulate a given temporal tensor for an e cient query phase. In the query phase, performing Tucker decomposition from scratch requires high computational cost and large space cost as the number of queries increases. To avoid it, compressing a given tensor is inevitable to provide fast processing in the query phase. Additionally, we consider that compressed results need to contain local patterns that appear only in speci c ranges. The preprocessing phase of existing Tucker decomposition methods [12,25,35] fails to support high e ciency of the query phase while maintaining local patterns. Then, how can we compress a given tensor to deal with various time range queries? Our main idea is to exploit a block structure: 1) carefully designating the form of a block, and 2) selecting a compression approach for each block. In this paper, we 1) split a given temporal tensor into sub-tensors along the time dimension, and 2) leverage Tucker decomposition for each sub-tensor. The idea allows Z T to support an e cient query phase and capture local patterns. Additionally, the preprocessing phase is extensible for new incoming tensors by performing Tucker decomposition of them. To capture local information, we split a given tensor along the time dimension. Let the reconstruction error at each timestep be measured by performing Tucker decomposition. The reconstruction error is de ned as\nX( )\u2212X( ) 2 F X( ) 2 F\nwhere X( ) is an input sub-tensor obtained at each timestep andX( ) is the sub-tensor at timestep reconstructed from Tucker results. Figure 3 shows the reconstruction errors of Stock dataset at each time point. Given a sub-tensor in a range that has relatively high errors, performing Tucker decomposition of the sub-tensor (orange line in Figure 3) provides lower errors than the preceding result computed from a whole temporal tensor (blue line in Figure 3). This observation implies that decomposing a sub-tensor allows us to capture local information, leading to low errors. Based on the observation, we construct sub-tensors by splitting a temporal tensor along the time dimension and perform Tucker decomposition of each sub-tensor. It provides lower error than performing Tucker decomposition of a whole tensor on all the timesteps, by capturing local information.\nTo support an e cient query phase, we store the Tucker decomposition results of sub-tensors. There are the two bene ts to leveraging Tucker decomposition in the preprocessing phase: 1) saving the space cost due to the small preprocessed results compared to the given tensor, and 2) enabling the query phase to exploit the mixedproduct property applicable to mixing matrix multiplication and Kronecker product, i.e., (\nA \u2297 B )(C \u2297 D) = (A C \u2297 B D). Computing (A C \u2297 B D) requires less costs than computing (A \u2297 B )(C \u2297 D)\nwhen the size of the four matrices is \u00d7 and >> . The reason is that the size of A C and B D is only \u00d7 while the size of (A \u2297 B ) and (C \u2297 D) is 2 \u00d7 2 and 2 \u00d7 2 , respectively. We further present the exploitation of this property to achieve high e ciency of the query phase in Sections 3.2.3 and 3.2.4.\nFigure 4 presents an overview of the preprocessing phase. Without loss of generality, we assume that the temporal mode is the last mode ( th mode). We express a given tensor X as temporal block tensors\nX < > \u2208 R 1 \u00d7 2 \u00d7\u2022\u2022\u2022\u00d7 \u22121 \u00d7 for = 1, ...,\n(line 2 in Algorithm 1) where is a block size and is the dimensionality of the time dimension. Then, we perform Tucker decomposition for each temporal block tensor X < > (line 4 in Algorithm 1), and store each factor matrix (A < > ) ( ) in a set C and the core tensor G < > in a set C +1 (lines 5 and 6 in Algorithm 1). Since the preprocessing phase is computed once and a ects errors of the query phase, this phase prefers an accurate but slow Tucker decomposition method rather . than a fast but approximate Tucker decomposition one. Speci cally, we use Tucker-ALS, which is stable and accurate, in this phase.", "publication_ref": ["b11", "b24", "b34"], "figure_ref": ["fig_2", "fig_2", "fig_2", "fig_3"], "table_ref": []}, {"heading": "Query Phase", "text": "The objective of the query phase is to e ciently compute Tucker decomposition for a given time range [ , ]. The query phase of Z T operates as follows: Step 1. Given a time range [ , ], we load Tucker results (i.e., (A < > ) ( ) , G < > ) of temporal block tensors X < > for = , ..., where = and = are the indices of the rst and the last temporal block tensors including and , respectively.\nStep 2. We adjust the Tucker results of X < > and X < > to t the range since a part of them may not be within the given range.\nStep 3. Given the Tucker results of X < > for = , .., included in the range, Z T updates factor matrices by eciently stitching the Tucker results.\nStep 4. After that, Z T updates the core tensor using factor matrices updated at Step 3 and the Tucker results.\nStep 5. Z T repeatedly performs Steps 3 and 4 until convergence.\nThe most important challenge of the e cient query phase is how to minimize the computational cost for updating factor matrices (Step 3) and the core tensor (Step 4) of the time range while minimizing the intermediate data. To tackle the challenge, our main ideas are to 1) elaborately decoupleX ( ) \u2297 \u2260\u00c3 ( ) based on preprocessed results, and 2) carefully determine the order of computation. We rst give an objective function and an update rule for the query phase (Section 3.2.1). Then, we describe how to achieve high e ciency of Z T in detail (Sections 3.2.2 to 3.2.4).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Objective function and update rule.", "text": "In the query phase, our goal is to obtain factor matrices\u00c3 (1) , ...,\u00c3 ( ) , and core tensorG for a given time range query [ , ]. The query phase of Z T alternately updates factor matrices, and core tensor as in ALS. We minimize the following objective function as mode-matricized form for a time range [ , ]: Output: factor matrices\u00c3 ( ) for = 1, .., , and core tensorG Parameters: tolerance , and block size 1: \u2190 and \u2190 2: load (A < > ) ( ) and G < > for = , ..., from C for = 1, ..., + 1 3: obtain (\u0100 < > ) ( ) and (\u0100 < > ) ( ) by eliminating the rows of (A < > ) ( ) and (A < > ) ( ) excluded in the range 4: (\u0100 < > ) ( ) \u2192 Q < > R < > , (\u0100 < > ) ( ) \u2192 Q < > R < > 5: (A < > ) ( ) \u2190 Q < > , G < > \u2190 G < > \u00d7 R < > , (A < > ) ( ) \u2190 Q < > , and G < > \u2190 G < > \u00d7 R < > 6: repeat 7:\n( ) = X ( ) \u2212\u00c3 ( )G ( ) (\u2297 \u2260\u00c3 ( ) ) 2 (2)\nfor = 1... \u2212 1 do 8:\nupdate\u00c3 ( ) by computing Equation ( 5) and orthogonalizing it with QR decomposition 9:\nend for 10:\nupdate\u00c3 ( ) by computing Equation ( 7) and orthogonalizing it with QR decomposition 11:\nupdate core tensorG by computing Equation ( 8) 12: until the variation of an error is less than or the number of iterations is larger than the maximum number of iterations 13: return\u00c3 ( ) for = 1, ..., andG where C ( ) \u2208 R \u00d7 of the -th mode is given by\nC ( ) =G ( ) \u2297 \u2260\u00c3 ( )\u00c3( ) G ( )\nIn contrast to naively computing Equation ( 3) withX ( ) , Z T e ciently computes Equation ( 3) by exploiting preprocessed results obtained in the preprocessing phase.\nBefore describing an e cient update procedure, we introduce a useful lemma (see the proof in Appendix B.2).", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "L", "text": "2. Let S \u2208 R \u00d7...\u00d7 and S \u2208 R \u00d7...\u00d7 be -order tensors, and U ( ) and V ( ) for = 1, ..., \u2212 1, + 1, ..., be matrices of size \u00d7 . Assume our goal is to compute the following equation: For all = 1, ..., , C ( ) is computed based on Lemma 2, by replacing S ( ) , U ( ) , V ( ) , and S ( ) withG ( ) ,\u00c3 ( ) ,\u00c3 ( ) , andG ( ) , respectively.\nS ( ) \u2297 \u2260 U ( ) V ( ) S ( )(4", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Adjusting edge blocks of time range query (", "text": "Step 2). Before updates, we adjust the Tucker results of X < > and X < > , the temporal block tensors corresponding to and of the given time range [ , ], respectively. The temporal factor matrices (A < > ) ( ) of X < > and (A < > ) ( ) of X < > may contain the rows that are not included in the range (see Figure 5(a)). To t to the given time range, we need to remove the non-included rows of (A < > ) ( ) and (A < > ) ( ) , and adjust the Tucker results of X < > and X < > . Let be or . For the temporal factor matrix (A < > ) ( ) of X < > in the range, Z T obtains the manipulated temporal factor matrix (\u0100 < > ) ( ) by removing the rows of (A < > ) ( ) Next, we perform QR decomposition to make (\u0100 < > ) ( ) maintain column-orthogonality (line 4 in Algorithm 2); we use (Q < > ) ( ) as the temporal factor matrix of X < > and update the core tensor G < > \u2190 G < > \u00d7 (R < > ) ( ) where (Q < > ) ( ) and (R < > ) ( ) are the results of QR decomposition (line 5 in Algorithm 2).\n!\"# (%) \" + 1 \u22ee # \u2212 1 \" \u2212 1 \u22ee # + 1", "publication_ref": [], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "Out of range", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Out of range", "text": "\u22ee !\"# (%) !'# (%)(", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E icient update of factor matrices (", "text": "Step 3). We present how to e ciently update the factor matrix of the non-temporal modes and the temporal mode.\nUpdating factor matrix of non-temporal modes. Consider updating the -th factor matrix, which corresponds to a non-temporal mode. A naive approach is to reconstructX ( ) from the Tucker results of the preprocessing phase and compute Equation (3). However, it requires large time and space costs since the reconstructed tensor is much larger than the preprocessed results. Our main ideas are to 1) elaborately decoupleX ( ) \u2297 \u2260\u00c3 ( ) block by block using the preprocessed results, and 2) carefully determine the order of computations, which signi cantly reduces time and space costs compared to the naive approach. We derive Equation (5) in Lemma 3 to update\u00c3 ( ) (see the proof in Appendix B.3). L 3 (U ).\nAssume thatX ( ) is replaced with the preprocessed results (i.e., (A < > ) ( ) and G < > ). Then, the following equation is equal to Equation (3) in Lemma 1 for -th mode:\nA ( ) \u2190 = (A < > ) ( ) (B < > ) ( ) C ( ) \u22121(5)\nwhere the -th block matrix (B < > ) ( ) of the -th mode is\n(B < > ) ( ) = G < > ( ) (A < > ) ( )\u00c3( ) [ ] \u2297 \u2297 \u22121 \u2260 (A < > ) ( )\u00c3( ) G ( ) ,(6)\nand C (n) is de ned in Lemma 1. (A < > ) ( ) is the -th factor matrix of the temporal block tensor X < > , and G < > ( ) is the mode-matricized version of the core tensor of X < > .\u00c3 ( ) [ ] is a sub-matrix of the temporal factor matrix\u00c3 ( ) such that,\n\uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0\u00c3 ( ) [ ] . . . A ( ) [ ] \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb =\u00c3 ( )\nTo compute (A < > ) ( )\u00c3( ) [ ], we split\u00c3 ( ) into sub-matrices A ( ) [ ] ( = , ..., ) along the time dimension (see Figure 5(b)); the size of\u00c3 ( ) [ ] for = + 1, ..., \u2212 1 is \u00d7 , and that of\u00c3 ( ) [ ] and\u00c3 ( ) [ ] is ( \u2212 ) \u00d7 and ( \u2212 ) \u00d7 , respectively, where and are the number of the rows removed with respect to and , respectively.\nZ T e ciently updates\u00c3 ( ) with Equation (5). Z T minimizes the intermediate data and reduces the high computational cost by independently computing C ( ) and (B < > ) ( ) for = , ..., . Note that (B < > ) ( ) for = , ..., is computed based on Lemma 2, by replacing S ( ) , U ( ) , V ( ) , and S ( ) with G < > ( ) , (A < > ) ( ) ,\u00c3 ( ) (or\u00c3 ( ) [ ]), andG ( ) , respectively. After that, we obtain\u00c3 ( ) by summing up the results of (A < > ) ( ) (B < > ) ( ) C ( ) \u22121 for = , ..., . For orthogonalization, we then updat\u1ebd A ( ) \u2190Q ( ) after QR decomposition\u00c3 ( ) \u2192Q ( )R( ) (line 8 in Algorithm 2).\nUpdating factor matrix of temporal mode. The goal is to update the factor matrix\u00c3 ( ) of the temporal mode by using the preprocessed results instead ofX ( ) . ReconstructingX ( ) requires high space and time costs in Equation (3). Based on our ideas used for the non-temporal modes, we e ciently update\u00c3 ( ) by computing Equation (7) in Lemma 4 (see the proof in Appendix B.4). L 4 (U ). Assume thatX ( ) is replaced with the preprocessed results (i.e., (A < > ) ( ) and G < > ). Then, the following equation is equal to Equation (3) in Lemma 1 for the temporal mode:\nA ( ) \u2190 \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 (A < > ) ( ) (B < > ) ( ) . . . (A < > ) ( ) (B < > ) ( ) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb C ( ) \u22121 (7)\nwhere the -th matrix (B < > ) ( ) \u2208 R \u00d7 for = , ..., is\n(B < > ) ( ) = G < > ( ) \u2297 \u22121 =1 (A < > ) ( )\u00c3( ) G ( ) (A < > ) ( )\nis the -th factor matrix of X < > , G < > ( ) is the modematricized version of the core tensor of X < > , and\nC ( ) is equal t\u00f5 G ( ) \u2297 \u22121 =1\u00c3 ( )\u00c3( ) G ( ) .\nWe obtain\u00c3 ( ) by using (C ( ) ) \u22121 , (A < > ) ( ) , and (B < > ) ( ) for = , ..., . Z T e ciently updates\u00c3 ( ) by independently computing C ( ) and (B < > ) ( ) for = , ..., . (B < > ) ( ) is e ciently computed based on Lemma 2, by replacing S ( ) , U ( ) , V ( ) , and S ( ) with G < > ( ) , (A < > ) ( ) ,\u00c3 ( ) , andG ( ) , respectively. For orthogonalization, we update\u00c3 ( ) \u2190Q ( ) after QR decom-position\u00c3 ( ) \u2192Q ( )R( ) (line 10 in Algorithm 2).", "publication_ref": ["b4", "b2", "b6"], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "E icient update of core tensor (Step 4). At the end of each iteration, Z", "text": "T updates the core tensor using the factor matrices:G ( ) \u2190\u00c3 ( )X ( ) \u2297 \u22121", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "=1\u00c3", "text": "( ) (mode-matricization of line 8 in Algorithm 3). We e ciently compute the core tensor by avoiding reconstruction ofX ( ) and carefully determining the order of computation. We replaceX ( ) with the preprocessed results and re ne the equation with block decoupling and the mixed-product property (see Equation ( 10) in Appendix B.4). \nG ( ) \u2190 = (\u00c3 ( ) [ ]) (A < > ) ( ) G < > ( ) \u2297 \u22121 =1 (A < > ) ( )\u00c3( )(8)\nIMN 2 J 2 /b) O(l [ts,te ] NIJ/b) D-Tucker [12] O( [ , ] \u22122\n2 ) O( [ , ] \u22122 ) Tucker-ALS O( [ , ]\u22121\n) O( [ , ] \u22121 ) MACH [35] O( [ , ]\u22121\n) O( [ , ] \u22121 ) RTD [5] O( [ , ]\u22121\n) O( [ , ]\u22121\n) Tucker-ts [25] O\n( [ , ] \u22121 + ) O( [ , ] \u22121 + ) Tucker-ttmts [25] O( [ , ] \u22121 + 2 \u22122 ) O( [ , ] \u22121 + )\nWith Equation ( 8), Z T e ciently updatesG, reducing the intermediate data and the computational cost. For each , Z T computes (\u00c3 ( ) [ ])(A < > ) ( ) G < > ( ) \u2297 \u22121 =1 (A < > ) ( )\u00c3( ) after transforming it into -mode products as in Equation ( 1). After that, Z T obtainsG ( ) by summing up the results and reshape it to the core tensorG (line 11 in Algorithm 2).", "publication_ref": ["b24"], "figure_ref": [], "table_ref": []}, {"heading": "Analysis", "text": "We analyze the time and space complexities of Z T in the preprocessing phase and the query phase. We assume that = 1 = ... = \u22121 , and = 1 = ... = . is the number of iterations, [ , ] = \u2212 + 1 is the length of a time range query, is the order of a given tensor, is the dimensionality, is the block size, is the number of blocks, and is the rank. All proofs are summarized in Appendices B.5 to B.8.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Time complexity.", "text": "We analyze computational cost of Z T in the preprocessing phase and the query phase.  Space complexity. We provide analysis for the space cost of Z T in the preprocessing phase and the query phase.   . We also note that the block size reduces the complexities of Z T . We compare the time and space complexities of Z T with those of the second-best method, D-Tucker. For both time and space complexities, the result of dividing the complexity of Z T by that of D-Tucker is \u22123 . Z T has better time and space complexities than D-Tucker since \u22123 is larger than in real-world datasets; for example, in the experiments, we use 50 as the default block size while the order of the real-world datasets is 3 or 4. As increases, the space complexity of the preprocessing and the query phases, and the time complexity of the query phase decrease; however, a large block size can provoke a high reconstruction error for a narrow time range query ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Settings", "text": "Machine. We run experiments on a workstation with a single CPU (Intel Xeon E5-2630 v4 @ 2.2GHz), and 512GB memory.\nDataset. We use six real-world dense tensors in Table 3. Boats 1 [37] and Walking Video 2 [25] datasets contain grayscale videos in the form of (height, width, time; value). Stock dataset 3 [30] contains tra c volume information in the form of (sensor, frequency, time; measurement). FMA dataset 5 [8] contains music information: (song, frequency, time; value). We convert a time series into an image of a log-power spectrogram for each song. Absorb dataset 6 is about absorption of aerosol in the form of (longitudes, latitudes, altitude, time; measurement).\nCompetitors. We compare Z T with 6 Tucker decomposition methods based on ALS approach. Z T and other methods are implemented in MATLAB (R2019b). We use the open sourced codes for 4 competitors: D-Tucker 7 , Tucker-ALS [3], Tucker-ts 8 , and Tucker-ttmts 8 . For MACH, we run Tucker-ALS in Tensor Toolbox [3] for a sampled tensor after sampling elements of a tensor; we use our implementation for a sampling scheme. We use the source code of RTD [5] provided by the authors.\nParameters. The parameter settings used for experiments are described in Appendix C. Space Cost (MB)\n! \u00d7 $% \u00d7 &'( \u00d7 )* \u00d7 ') \u00d7\n). ,\u00d7 Figure 6: Space cost for storing preprocessed results. Input Tensor corresponds to the space cost of Tucker-ALS, Tuckerts, Tucker-ttmts, and RTD. Z T requires up to 230\u00d7 less space than competitors. Implementation details. In the time range query problem, Z T , D-Tucker, and MACH preprocess a given tensor, and then perform Tucker decomposition for a time range query using preprocessed results included in the range. In contrast, Tucker-ALS and RTD perform Tucker decomposition using a sub-tensor included in a time range query. Although Tucker-ts and Tucker-ttmts have a preprocessing phase, they also perform Tucker decomposition from scratch for a time range query since there is an inseparable preprocessed result along the time dimension.\nReconstruction error. Given an input tensor X and the recon-structionX from the output of Tucker decomposition, reconstruction error is de ned as\nX\u2212X 2 F X 2 F\n. Reconstruction error describes how well the reconstructionX of Tucker decomposition represents an input tensor X.", "publication_ref": ["b36", "b24", "b2", "b29", "b4", "b2", "b7", "b7", "b2", "b4"], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Trade-o between Query Time and Reconstruction Error (Q1)", "text": "We compare the running time and reconstruction error of Z T with those of competitors for various time ranges. For each dataset, we use the narrowest and the widest time ranges among the ranges described in Table 3. Figure 2 shows that Z T is the closest method to the best point with the smallest error and running time. Z T is up to 171.9\u00d7 and 111.9\u00d7 faster than the second-fastest method, in narrow and wide time ranges, respectively, with similar errors.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": ["tab_5"]}, {"heading": "Space Cost (Q2)", "text": "We compare the storage cost of Z T with those of competitors for storing preprocessed results. Note that memory requirements for a time range query are proportional to the storage cost since preprocessed results or an input tensor is the dominant term in the space cost. Figure 6 shows that Z T requires the lowest space; Z T requires up to 230\u00d7 less space than the second-best method D-Tucker. Z T has more compression rate on the 4-order tensor, Absorb dataset.  Figure 8: Anomalous two-month ranges and their related events, found by Z T . error is not sensitive to . A large prevents the preprocessing phase from capturing local information so that it is challenging to serve narrow time range queries. For wide time range queries, local information has little e ect on reconstruction errors since capturing widespread patterns is more bene cial in reducing errors. Therefore, we select 50, which is the largest value providing small errors for narrow time range queries, for the default block size to preprocess all datasets in other experimental sections.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E ects of Block Size (Q3)", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Discovery (Q4)", "text": "On Stock dataset, we discover interesting results by answering various time range queries with Z T .\nFinding anomalous ranges. The goal is to nd narrow time ranges that are anomalous, compared to the entire time range. For the goal, we select every consecutive two-month interval from Jan. 1, 2008 to Apr. 30, 2020, perform Tucker decomposition for each of the intervals using Z T , and nd anomalous ranges that deviate the most from the entire ranges. Given a two-month range , and its corresponding sub-tensorX, we compute the anomaly score for using the di erence ratio\nX \u2212\u0176 2 F X \u2212\u1e90 2 F\nwhere\u0176 and\u1e90 are the sub-tensors for reconstructed from the Tucker results of 1) the entire range query, and 2) the two-month range query, respectively. The leftmost plot of Figure 8 shows the di erence ratios and the top three anomalous ranges where the threshold indicates 2 standard deviations from the mean. The right three plots of Figure 8 show that the three anomalies follow the similar plunging pattern of prices from issues a ecting the stock market.\nAnalyzing trend change. We analyze the change of yearly trend of Samsung Electronics in the years 2013 and 2018. For each of the range (year 2013 or 2018), we perform Z T and get the feature matrix\u00c3 (1) each of whose rows contain the latent features of a stock. We also manually pick 33 smartphone-related Figure 9 shows the result. Note that there is a clear change of the distances between year 2013 and 2018: Samsung Electronics is more close to smartphone-related stocks in 2013, but to semiconductorrelated stocks in 2018. This result exactly re ects the sales trend of Samsung Electronics; the annual sales of its smartphone division are 3.7\u00d7 larger than those of its semiconductor division in 2013, while in 2018 the annual sales of its semiconductor division are 30% larger than those of its smartphone division. Z T enables us to quickly and accurately capture this trend change.", "publication_ref": ["b0"], "figure_ref": ["fig_12"], "table_ref": []}, {"heading": "RELATED WORK", "text": "We review related works for e cient tensor decomposition, blockbased tensor decomposition, and time range query for tensors.\nE cient tensor decomposition. Many works have been devoted to computing e cient tensor decomposition in various settings. Previous works [2,14,15,28,38] develop e cient tensor decomposition methods on distributed systems. Several tensor decomposition methods [21,26,27,31,32,34] have been proposed for sparse tensors; however, they target performing tensor decomposition only once for the whole data. There are several works [7,9,18,22,33,39] that perform tensor decomposition in streaming settings. Z T is di erent from the above methods since it handles arbitrary time range queries in a single machine.\nTensor decomposition with block-wise computation. Many tensor decomposition methods have exploited block-wise computation for parallel computation. [6,23,29] proposed parallel CP decomposition methods which perform CP decomposition block by block and then concatenate the results of the blocks. Austin et al. [2] proposed a distributed algorithm that computes -mode product, gram matrix, and eigenvectors with the small blocks of a given tensor. Unlike the above methods which do not consider time ranges, the goal of our Z T is to quickly provide Tucker decomposition results for a given time range query. Time range query for tensors. Zoom-SVD [11] deals with the time range query problem, but it is suitable only for multiple time series data represented as a matrix. Although there is no existing method that precisely addresses the time range query problem for tensors, there are several methods [12,25,35] that can be adapted to solve the problem. They perform a preprocessing phase by exploiting a sampling technique [35] or randomized SVD [12] before the query phase, and then obtain Tucker results using the preprocessed results in the query phase. However, they do not satisfy the desired properties for the solution: fast running time, low space cost, and accuracy. On the other hand, Z T e ciently and accurately provides answers to time range queries by exploiting the preprocessed results.", "publication_ref": ["b1", "b13", "b14", "b27", "b37", "b20", "b25", "b26", "b30", "b31", "b33", "b6", "b8", "b17", "b21", "b32", "b38", "b5", "b22", "b28", "b1", "b10", "b11", "b24", "b34", "b34", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSIONS", "text": "In this work, we propose Z T , an e cient Tucker decomposition method to discover latent factors in a given time range from a temporal tensor. Z T e ciently answers diverse time range queries with the preprocessing phase and the query phase. In the preprocessing phase, Z T lays the groundwork for an e cient time range query by compressing sub-tensors along time dimension block by block. Given a time range query in the query phase, Z T elaborately stitches compressed results reducing computational cost and space cost. Experiments show that Z T is up to 171.9\u00d7 faster and requires up to 230\u00d7 less space than existing methods, with comparable accuracy to competitors. With Z T , we discover interesting patterns including anomalous ranges and trend changes in a real-world stock dataset. Future research includes extending the method for sparse tensors.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "APPENDIX", "text": "A TUCKER-ALS Algorithm 3: Tucker-ALS (HOOI) [17,20] Input: tensor X \u2208 R 1 \u00d7...\u00d7 and dimensionalities 1 , ..., of core tensor Output: core tensor G \u2208 R 1 ,..., and factor matrices A ( ) \u2208 R \u00d7 ( = 1, ..., ) 1: initialize: factor matrices A ( ) ( = 1, ..., ) 2: repeat 3:\nfor = 1, ..., do 4:\nY \u2190 X \u00d7 1 A (1) \u2022 \u2022 \u2022 \u00d7 \u22121 A ( \u22121) \u00d7 +1 A ( +1) \u2022 \u2022 \u2022 \u00d7 A ( )", "publication_ref": ["b16", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "5:", "text": "A ( ) \u2190 leading left singular vectors of Y ( ) 6: end for 7: until convergence criterion is met; 8:\nG \u2190 X \u00d7 1 A (1) \u00d7 2 A (2) \u2022 \u2022 \u2022 \u00d7 A ( ) B PROOFS B.1 Proof of Lemma 1 P .\nAfter xing all factor matrices except for the -th factor matrix, the partial derivative of the Equation (2) with respect to the factor matrix\u00c3 ( ) is as follows:\n( ) A ( ) = \u22122X ( ) (\u2297 \u2260\u00c3 ( ) )G ( ) +2\u00c3 ( )G ( ) \u2297 \u2260\u00c3 ( )\u00c3( ) G ( )\nWe set ( ) A ( ) to zero, and solve the equation with respect to the factor matrix\u00c3 ( ) : ), respectively. We compute Equation (4) using -mode product instead of Kronecker product. Let Z ( ) = S ( ) \u2297 \u2260 U ( ) V ( ) be equal to ( ) where I ( ) \u2208 R \u00d7 is an identity matrix. Then, we transform Z into Equation (9) using Equation (1).\nA ( ) G ( ) \u2297 \u2260\u00c3 ( )\u00c3( ) G ( ) =X ( ) \u2297 \u2260\u00c3 ( ) G ( )\u21d4\u00c3\nI ( ) S ( ) \u2297 \u2260 U ( ) V\nZ = S \u00d7 1 (U (1) V (1) ) \u2022 \u2022 \u2022 \u00d7 \u22121 (U ( \u22121) V ( \u22121) ) \u00d7 I ( ) \u00d7 +1 (U ( +1) V ( +1) ) \u2022 \u2022 \u2022 \u00d7 (U ( ) V ( ) )(9)\nBased on Equation (9), we compute Equation (4) in the following order: 1) U ( ) V ( ) for = 1, ..., \u2212 1, + 1, ..., , 2) Z ( ) , and 3) Z ( ) S ( ) . Therefore, the computational cost is O( 2 + +1 ).\nIn addition, the size of intermediate data is always no larger than so that the space complexity is O( + ).\nB. block by block so that we represent the term as a summation of block matrices:\nA ( ) = X < > ( ) \u2022 \u2022 \u2022 X < > ( ) \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0\u00c3 ( ) [ ] . . . A ( ) [ ] \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb \u2297 \u2297 \u22121 \u2260\u00c3 ( ) G ( ) C ( ) \u22121 = = X < > ( ) \u00c3 ( ) [ ] \u2297 \u2297 \u22121 \u2260\u00c3 ( ) G ( ) C ( ) \u22121\nNext, we express -th block matrix X < > ( ) as the result (A < > ) ( ) G < > ( ) (A < > ) ( ) \u2297 \u2297 \u22121 \u2260 (A < > ) ( ) obtained in the preprocessing step. ", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "=1\u00c3", "text": "( ) using temporal block tensors X < > for = , .., as follows:\nX ( ) \u2297 \u22121 =1\u00c3 ( ) = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 X < > ( ) \u2297 \u22121 =1\u00c3 ( ) . . . X < > ( ) \u2297 \u22121 =1\u00c3 ( ) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb\nThen, we replace X < > ( ) with the tucker results obtained at the preprocessing phase.\nX ( ) \u2297 \u22121 =1\u00c3 ( ) \u2248 \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 (A < > ) ( ) G < > ( ) \u2297 \u22121 =1 (A < > ) ( )\u00c3( ) . . . (A < > ) ( ) G < > ( ) \u2297 \u22121 =1 (A < > ) ( )\u00c3( ) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb (10)\nNext, we obtain the following equation by inserting the right term of the above equation into Equation (3): . We split a tensor X into temporal block tensors X < > , and then perform Tucker decomposition of X < > for = 1, ..., . Since we use Tucker-ALS in the preprocessing phase, the time complexity for each temporal block tensor X < > is O( \u22121 ). Therefore, the preprocessing phase takes O( B.8 Proof of Theorem 4 P . Given a time range [ , ], summing up the size of the factor matrices of the time dimension is equal to [ , ] \u00d7 ; the size of the factor matrix of a non-temporal mode is \u00d7 , and the number of block is equal to [ , ] or ( [ , ] ) + 1. The size of the block results used in the query phase is O( ( [ , ] ) +\nA ( ) = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 (A < > ) ( ) G < > ( ) \u2297 \u22121 =1 (A < > ) ( )\u00c3( ) G ( ) . . . (A < > ) ( ) G < > ( ) \u2297 \u22121 =1 (A < > ) ( )\u00c3( ) G ( ) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb C ( ) \u22121 = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 (A\n[ , ] ). By carefully stitching the block results, intermediate data are always smaller than the block results. Therefore, the space cost of Z T is O ( [ , ] ) + [ , ] for a given time range [ , ].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C PARAMETERS SETTINGS", "text": "We use the following parameters.\n(1) Number of threads: we use a single thread.\n(2) Max number of iterations: the maximum number of iterations is set to 100. (3) Rank: we set the dimensionality of each mode of core tensor to 10. (4) Choosing a time range query: we randomly choose a start time of a time range, and compute = + [ , ] \u2212 1 where [ , ] is the length of the time range; we choose\n[ , ] among the sets described in Table 3. (5) Block size : we set to 50 except in Section 4.4. (6) Tolerance: the iteration stops when the variation of the error 17] is less than = 10 \u22124 .\nX 2 F \u2212 G 2 F X F [\nOther parameters for competitors are set to the values proposed in each paper. To compare the running time, we run each method 5 times, and report the average.", "publication_ref": ["b16"], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by the National Research Foundation of Korea(NRF) funded by MSIT(2019R1A2C2004990). U Kang is the corresponding author.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Gtensor: Fast and Accurate Tensor Analysis System using GPUs", "journal": "", "year": "2020", "authors": "Dawon Ahn; Sangjun Son; U Kang"}, {"ref_id": "b1", "title": "Parallel Tensor Compression for Large-Scale Scienti c Data", "journal": "", "year": "2016", "authors": "Woody Austin; Grey Ballard; Tamara G Kolda"}, {"ref_id": "b2", "title": "MATLAB Tensor Toolbox Version 3.0-dev", "journal": "", "year": "2017", "authors": "W Brett; Tamara G Bader;  Kolda"}, {"ref_id": "b3", "title": "Robust Face Clustering Via Tensor Decomposition", "journal": "IEEE Trans. Cybernetics", "year": "2015", "authors": "Xiaochun Cao; Xingxing Wei; Yahong Han; Dongdai Lin"}, {"ref_id": "b4", "title": "Randomized algorithms for the approximations of Tucker and the tensor train decompositions", "journal": "Adv. Comput. Math", "year": "2019", "authors": "Maolin Che; Yimin Wei"}, {"ref_id": "b5", "title": "H-PARAFAC: Hierarchical Parallel Factor Analysis of Multidimensional Big Data", "journal": "TPDS", "year": "2017", "authors": "Dan Chen; Yangyang Hu; Lizhe Wang; Albert Y Zomaya; Xiaoli Li"}, {"ref_id": "b6", "title": "S3CMTF: Fast, accurate, and scalable method for incomplete coupled matrix-tensor factorization", "journal": "PLOS ONE", "year": "2019-06", "authors": "Dongjin Choi; Jun-Gi Jang; U Kang"}, {"ref_id": "b7", "title": "FMA: A Dataset for Music Analysis", "journal": "", "year": "2017", "authors": "Micha\u00ebl De Errard; Kirell Benzi; Pierre Vandergheynst; Xavier Bresson"}, {"ref_id": "b8", "title": "SamBaTen: Sampling-based Batch Incremental Tensor Decomposition", "journal": "", "year": "2018", "authors": "Ekta Gujral; Ravdeep Pasricha; Evangelos E Papalexakis"}, {"ref_id": "b9", "title": "Simultaneous tensor subspace selection and clustering: the equivalence of high order svd and k-means clustering", "journal": "", "year": "2008", "authors": "Heng Huang; H Q Chris; Dijun Ding; Tao Luo;  Li"}, {"ref_id": "b10", "title": "Zoom-SVD: Fast and Memory E cient Method for Extracting Key Patterns in an Arbitrary Time Range", "journal": "", "year": "2018", "authors": "Jun-Gi Jang; Dongjin Choi; Jinhong Jung; U Kang"}, {"ref_id": "b11", "title": "D-Tucker: Fast and Memory-E cient Tucker Decomposition for Dense Tensors", "journal": "", "year": "2020", "authors": "Jun-Gi Jang; U Kang"}, {"ref_id": "b12", "title": "SCouT: Scalable coupled matrix-tensor factorization -algorithm and discoveries", "journal": "", "year": "2016", "authors": "Inah Byungsoo Jeon; Lee Jeon; U Sael;  Kang"}, {"ref_id": "b13", "title": "HaTen2: Billion-scale tensor decompositions", "journal": "", "year": "2015", "authors": "Inah Jeon; Evangelos E Papalexakis; U Kang; Christos Faloutsos"}, {"ref_id": "b14", "title": "Scalable sparse tensor decompositions in distributed memory systems", "journal": "", "year": "2015", "authors": "Oguz Kaya; Bora U\u00e7ar"}, {"ref_id": "b15", "title": "Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications", "journal": "", "year": "2015", "authors": "Yong-Deok Kim; Eunhyeok Park; Sungjoo Yoo; Taelim Choi; Lu Yang; Dongjun Shin"}, {"ref_id": "b16", "title": "Tensor Decompositions and Applications", "journal": "SIAM Rev", "year": "2009", "authors": "Tamara G Kolda; Brett W Bader"}, {"ref_id": "b17", "title": "SliceNStitch: Continuous CP Decomposition of Sparse Tensor Streams", "journal": "", "year": "2021", "authors": "Taehyung Kwon; Inkyu Park; Dongjin Lee; Kijung Shin"}, {"ref_id": "b18", "title": "Tensor Decompositions for Temporal Knowledge Base Completion", "journal": "", "year": "2020", "authors": "Timoth\u00e9e Lacroix; Guillaume Obozinski; Nicolas Usunier"}, {"ref_id": "b19", "title": "On the Best Rank-1 and Rank-(R 1 , R 2", "journal": "SIAM J. Matrix Analysis Applications", "year": "2000", "authors": "Bart De Lieven De Lathauwer; Joos Moor; ; . Vandewalle"}, {"ref_id": "b20", "title": "Fast Tucker Factorization for Large-Scale Tensor Completion", "journal": "", "year": "2018", "authors": "Dongha Lee; Jaehyung Lee; Hwanjo Yu"}, {"ref_id": "b21", "title": "Robust Factorization of Real-world Tensor Streams with Patterns, Missing Values, and Outliers", "journal": "", "year": "2021", "authors": "Dongjin Lee; Kijung Shin"}, {"ref_id": "b22", "title": "2PCP: Two-phase CP decomposition for billion-scale dense tensors", "journal": "", "year": "2016", "authors": "Xinsheng Li; Shengyu Huang; K Sel\u00e7uk Candan; Maria Luisa Sapino"}, {"ref_id": "b23", "title": "Generalizing Tensor Decomposition for N-ary Relational Knowledge Bases", "journal": "", "year": "2020", "authors": "Yu Liu; Quanming Yao; Yong Li"}, {"ref_id": "b24", "title": "Low-Rank Tucker Decomposition of Large Tensors Using TensorSketch", "journal": "", "year": "2018", "authors": "Asif Osman; Stephen Malik;  Becker"}, {"ref_id": "b25", "title": "High-Performance Tucker Factorization on Heterogeneous Platforms", "journal": "IEEE Trans. Parallel Distributed Syst", "year": "2019", "authors": "Sejoon Oh; Namyong Park; Jun-Gi Jang; Lee Sael; U Kang"}, {"ref_id": "b26", "title": "Scalable Tucker Factorization for Sparse Tensors -Algorithms and Discoveries", "journal": "", "year": "2018", "authors": "Sejoon Oh; Namyong Park; Lee Sael; U Kang"}, {"ref_id": "b27", "title": "BIGtensor: Mining Billion-Scale Tensor Made Easy", "journal": "", "year": "2016", "authors": "Namyong Park; Byungsoo Jeon; Jungwoo Lee; U Kang"}, {"ref_id": "b28", "title": "PARAFAC algorithms for large-scale problems", "journal": "Neurocomputing", "year": "2011", "authors": "Anh Huy Phan; Andrzej Cichocki"}, {"ref_id": "b29", "title": "Tra c forecasting in complex urban networks: Leveraging big data and machine learning", "journal": "IEEE", "year": "2015", "authors": "Florin Schimbinschi; Xuan Vinh Nguyen; James Bailey; Chris Leckie; Hai Vu; Rao Kotagiri"}, {"ref_id": "b30", "title": "Distributed Methods for High-Dimensional and Large-Scale Tensor Factorization", "journal": "", "year": "2014", "authors": "Kijung Shin;  Kang"}, {"ref_id": "b31", "title": "Fully Scalable Methods for Distributed Tensor Factorization", "journal": "IEEE Trans. Knowl. Data Eng", "year": "2017", "authors": "Kijung Shin; Lee Sael; U Kang"}, {"ref_id": "b32", "title": "Streaming Tensor Factorization for In nite Data Sources", "journal": "", "year": "2018", "authors": "Shaden Smith; Kejun Huang; Nicholas D Sidiropoulos; George Karypis"}, {"ref_id": "b33", "title": "SPLATT: E cient and Parallel Sparse Tensor-Matrix Multiplication", "journal": "", "year": "2015", "authors": "Shaden Smith; Niranjay Ravindran; Nicholas D Sidiropoulos; George Karypis"}, {"ref_id": "b34", "title": "MACH: Fast Randomized Tensor Decompositions", "journal": "", "year": "2010", "authors": "Charalampos E Tsourakakis"}, {"ref_id": "b35", "title": "A Tensor Approximation Approach to Dimensionality Reduction", "journal": "Int. J. Comput. Vis", "year": "2008", "authors": "Hongcheng Wang; Narendra Ahuja"}, {"ref_id": "b36", "title": "CDnet 2014: An Expanded Change Detection Benchmark Dataset", "journal": "", "year": "2014", "authors": "Yi Wang; Pierre-Marc Jodoin; Fatih Murat Porikli; Janusz Konrad; Yannick Benezeth; Prakash Ishwar"}, {"ref_id": "b37", "title": "LFTF: A Framework for E cient Tensor Analytics at Scale", "journal": "", "year": "2017", "authors": "Fan Yang; Fanhua Shang; Yuzhen Huang; James Cheng; Jinfeng Li; Yunjian Zhao; Ruihao Zhao"}, {"ref_id": "b38", "title": "Accelerating Online CP Decompositions for Higher Order Tensors", "journal": "", "year": "2016", "authors": "Shuo Zhou; Xuan Vinh Nguyen; James Bailey; Yunzhe Jia; Ian Davidson"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Trade-o between query time and reconstruction error of Z T and competitors, for narrow (a-f) and wide (g-l) time range queries. o.o.t.: out of time (takes more than 20,000 seconds). Numbers after the data name represent the length of time ranges; e.g., (128) means the length of a time range is 128 timesteps. Z T is closest to the best point with the fastest query speed and the lowest reconstruction error.Table 1: Symbol description.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Reconstruction errors at each time point on Stock dataset. The blue line presents reconstruction errors computed from a whole temporal tensor, while the orange line describes reconstruction errors computed from a sub-tensor in a range. Performing Tucker decomposition from a subtensor provides relatively low reconstruction errors.To capture local information, we split a given tensor along the time dimension. Let the reconstruction error at each timestep be measured by performing Tucker decomposition. The reconstruction", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Preprocessing phase of ZT . than a fast but approximate Tucker decomposition one. Speci cally, we use Tucker-ALS, which is stable and accurate, in this phase.", "figure_data": ""}, {"figure_label": "32", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "3 ) 2 :32whereX ( ) is the mode-matricized version of a tensor obtained in the time range [ , ], andG ( ) is the mode-matricized version of G. From the objective function (2), we derive the following update rule for -th factor matrix (see the proof in Appendix B.1): L 1 (U ). When xing all but the -th factor matrix, the following update rule for the -th factor matrix minimizes the objective function (2). A ( ) \u2190X ( ) \u2297 \u2260\u00c3 ( ) G ( ) C ( ) \u22121 (Algorithm Query phase of Z T Input: a time range [ , ], and Tucker result sets C for n = 1, ..., + 1", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 :5Figure 5: Examples of adjustment (Section 3.2.2) and division (Section 3.2.3). that are not included in the time range (line 3 in Algorithm 2).Next, we perform QR decomposition to make (\u0100 < > ) ( ) maintain column-orthogonality (line 4 in Algorithm 2); we use (Q < > ) ( ) as the temporal factor matrix of X < > and update the core tensor G < > \u2190 G < > \u00d7 (R < > )( )  where (Q < > ) ( ) and (R < > ) ( ) are the results of QR decomposition (line 5 in Algorithm 2).", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "T 1 .1The preprocessing phase takes O(", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "T 2 .2Given a time range query [ , ], the query phase of", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Tucker results in the preprocessing phase.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "T 4 .4Given a time range query [ , ], Z T requires O ( [ , ] ) + [ , ] space in the query phase.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "We investigate the e ects of block size on running time and reconstruction error of Z T . We use block sizes 10, 25, 50, 100, and 200 on Stock, Tra c, and Absorb datasets. As shown in Figures 7(a) to 7(c), there are trade-o relationships between running time and reconstruction error for narrow time range queries. In Figures 7(d) to 7(f), the running time of Z T is inversely proportional to for a wide range query while the reconstruction", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 7 :\u24627Figure 7: Sensitivity with respect to block size on Stock, Tra c, and Absorb datasets. Numbers after the data name represent the length of time ranges; e.g., (128) means the length of time range is \u2212 + 1 = 128 timesteps. (a,b,c) There are tradeo relationships between running time and reconstruction error for narrow time range queries. (d,e,f) For wide time range queries, the running times decrease while the errors do not change much, as block size increases.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Figure 9 :9Figure 9: Cosine distance between feature vectors of Samsung Electronics and other stocks related to smartphone or semiconductors in 2013 and 2018. Z T helps capture the clear change of the trend, where Samsung Electronics is more close to smartphone-related stocks in 2013, but to semiconductor-related stocks in 2018. stocks and 46 semiconductor-related stocks, and compare the cosine distance between the latent feature vectors of each stock and Samsung Electronics.Figure9shows the result. Note that there is a clear change of the distances between year 2013 and 2018: Samsung Electronics is more close to smartphone-related stocks in 2013, but to semiconductorrelated stocks in 2018. This result exactly re ects the sales trend of Samsung Electronics; the annual sales of its smartphone division are 3.7\u00d7 larger than those of its semiconductor division in 2013, while in 2018 the annual sales of its semiconductor division are 30% larger than those of its smartphone division. Z T enables us to quickly and accurately capture this trend change.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "> ) ( ) G < > ( ) (A < > ) ( )\u00c3( ) [ ] \u2297 \u2297 \u22121 \u2260 (A < > ) ( )\u00c3( ) \u00d7G ( ) C ( ) \u22121 = = (A < > ) ( ) (B < > ) ( ) C ( ) \u22121Note that\u00c3( )  [ ] is described in Lemma 4.B.4 Proof of Lemma 4P . From Equation (3), we decoupleX ( ) for updating -th factor matrix. We rst re-expressX ( ) \u2297\u22121    ", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "1 (1< > ) ( ) (B < > ) ( ) . . . (A < > ) ( ) (B < > ) ( ) A < > ) ( ) and (A < > ) ( ) are adjusted to t to a range [ , ]. B.5 Proof of Theorem 1 P", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "complexity of the query phase depends on updating factor matrices and core tensor. Updating a factor matrix or core tensor takes O 2[ , time complexity of updating factor matrices and core tensor, the number of iterations, and the number of factor matrices.B.7 Proof of Theorem 3P. For the mode-, summing up the size of the factor matrices of the time dimension is equal to . For each mode \u2260 , there are factor matrices, for the -th mode, of size O( ) where = is the number of blocks. Then, the space complexity is", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Symbol description.", "figure_data": "SymbolDescriptionindex of temporal block tensor corresponding toindex of temporal block tensor corresponding to\u2297Kronecker product\u00d7-mode product"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Time and space complexities of Z T and other methods for a time range [ , ]. The optimal complexities are in bold. , , , , and [ , ] are described in Section 3.3. is a sampling rate for MACH.", "figure_data": "AlgorithmTimeSpaceZTO(l [ts,te ]"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "shows the time and space complexities of Z T and competitors for a given time range query [ , ].", "figure_data": "The time"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Description of real-world tensor datasets.", "figure_data": "DatasetDimensionality Length [ , ] of Time RangeSummaryBoats 1 [37]320 \u00d7 240 \u00d7 7000(128, 2048)VideoWalking Video [25]1080 \u00d7 1980 \u00d7 2400(128, 2048)VideoStock 33028 \u00d7 54 \u00d7 3050(128, 2048)Time seriesTra c 4 [30]1084 \u00d7 96 \u00d7 2000(64, 1024) Tra c volumeFMA 5 [8]7994 \u00d7 1025 \u00d7 700(32, 512)MusicAbsorb 6192 \u00d7 288 \u00d7 30 \u00d7 1200(64, 1024)Climatesince the preprocessing phase with the large cannot capture localinformation. In Section 4.4, we experimentally nd a block size thatenables the preprocessing phase to capture local information withlow reconstruction errors for narrow time range queries.4 EXPERIMENTWe present experimental results to answer the following questions.Q1 Performance Trade-o (Section 4.2). Does ZTprovide the best trade-o between query time and recon-struction error?Q2 Space Cost (Section 4.3). What is the space cost of ZTand competitors for preprocessed results?Q3 E ects of the block size (Section 4.4). How does a blocksize a ect query time and reconstruction error of ZT?Q4 Discovery (Section 4.5). What pattern does ZTdiscover in di erent time ranges?"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "contains 5 basic features (open price, high price, low price, close price, trade volume) and 49 technical indicators features of Korea Stocks. Stock dataset has the form of (stock, features, date; value). The basic features are collected daily from Jan. 2, 2008 to May 6, 2020. Tra c dataset 4", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Kronecker product \u2297 \u2260 U ( ) V ( ) of the size \u22121 \u00d7 \u22121 . We compute matrix multiplication between the preceding result S ( ) and S ( ) . Therefore, the time and space complexities are O( 2 + 2 + +1 ) and O( 2 +", "figure_data": "( ) =X ( ) \u2297 \u2260\u00c3( ) G( ) C ( ) \u22121B.2 Proof of Lemma 2P. A naive approach computing Equation (4) is to explicitlycompute the entire"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "3 Proof of Lemma 3 P . From Equation (3), we carefully decoupleX ( ) \u2297 \u2260\u00c3 ( )", "figure_data": ""}], "formulas": [{"formula_id": "formula_1", "formula_text": "\u2460 \u2461 \u2462", "formula_coordinates": [1.0, 373.8, 222.71, 124.4, 4.67]}, {"formula_id": "formula_2", "formula_text": "X \u2248 G \u00d7 1 A (1) \u2022 \u2022 \u2022 \u00d7 A ( ) \u21d4 X ( ) \u2248 A ( ) G ( ) (\u2297 \u2260 A ( ) )(1)", "formula_coordinates": [3.0, 60.24, 84.8, 233.81, 22.26]}, {"formula_id": "formula_3", "formula_text": "X \u2208 R 1 \u00d7 2 \u00d7\u2022\u2022\u2022\u00d7 \u22121 \u00d7", "formula_coordinates": [3.0, 408.3, 103.19, 66.69, 9.25]}, {"formula_id": "formula_4", "formula_text": "X( )\u2212X( ) 2 F X( ) 2 F", "formula_coordinates": [4.0, 124.85, 262.62, 43.9, 20.72]}, {"formula_id": "formula_5", "formula_text": "A \u2297 B )(C \u2297 D) = (A C \u2297 B D). Computing (A C \u2297 B D) requires less costs than computing (A \u2297 B )(C \u2297 D)", "formula_coordinates": [4.0, 54.25, 493.35, 239.8, 20.11]}, {"formula_id": "formula_6", "formula_text": "X < > \u2208 R 1 \u00d7 2 \u00d7\u2022\u2022\u2022\u00d7 \u22121 \u00d7 for = 1, ...,", "formula_coordinates": [4.0, 82.46, 602.69, 135.98, 10.64]}, {"formula_id": "formula_7", "formula_text": "( ) = X ( ) \u2212\u00c3 ( )G ( ) (\u2297 \u2260\u00c3 ( ) ) 2 (2)", "formula_coordinates": [4.0, 368.18, 569.33, 190.02, 13.92]}, {"formula_id": "formula_8", "formula_text": "C ( ) =G ( ) \u2297 \u2260\u00c3 ( )\u00c3( ) G ( )", "formula_coordinates": [5.0, 107.99, 338.0, 130.78, 13.6]}, {"formula_id": "formula_9", "formula_text": "S ( ) \u2297 \u2260 U ( ) V ( ) S ( )(4", "formula_coordinates": [5.0, 122.71, 455.42, 168.17, 13.6]}, {"formula_id": "formula_10", "formula_text": "!\"# (%) \" + 1 \u22ee # \u2212 1 \" \u2212 1 \u22ee # + 1", "formula_coordinates": [5.0, 334.78, 95.59, 36.42, 75.25]}, {"formula_id": "formula_11", "formula_text": "\u22ee !\"# (%) !'# (%)(", "formula_coordinates": [5.0, 334.45, 128.47, 30.79, 57.97]}, {"formula_id": "formula_12", "formula_text": "A ( ) \u2190 = (A < > ) ( ) (B < > ) ( ) C ( ) \u22121(5)", "formula_coordinates": [5.0, 361.23, 496.6, 196.97, 24.87]}, {"formula_id": "formula_13", "formula_text": "(B < > ) ( ) = G < > ( ) (A < > ) ( )\u00c3( ) [ ] \u2297 \u2297 \u22121 \u2260 (A < > ) ( )\u00c3( ) G ( ) ,(6)", "formula_coordinates": [5.0, 318.59, 542.91, 249.92, 21.43]}, {"formula_id": "formula_14", "formula_text": "\uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0\u00c3 ( ) [ ] . . . A ( ) [ ] \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb =\u00c3 ( )", "formula_coordinates": [5.0, 404.0, 617.88, 67.88, 45.05]}, {"formula_id": "formula_15", "formula_text": "A ( ) \u2190 \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 (A < > ) ( ) (B < > ) ( ) . . . (A < > ) ( ) (B < > ) ( ) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb C ( ) \u22121 (7)", "formula_coordinates": [6.0, 92.57, 355.55, 201.47, 45.05]}, {"formula_id": "formula_16", "formula_text": "(B < > ) ( ) = G < > ( ) \u2297 \u22121 =1 (A < > ) ( )\u00c3( ) G ( ) (A < > ) ( )", "formula_coordinates": [6.0, 54.25, 418.94, 209.32, 29.32]}, {"formula_id": "formula_17", "formula_text": "C ( ) is equal t\u00f5 G ( ) \u2297 \u22121 =1\u00c3 ( )\u00c3( ) G ( ) .", "formula_coordinates": [6.0, 53.8, 453.27, 462.19, 27.14]}, {"formula_id": "formula_18", "formula_text": "G ( ) \u2190 = (\u00c3 ( ) [ ]) (A < > ) ( ) G < > ( ) \u2297 \u22121 =1 (A < > ) ( )\u00c3( )(8)", "formula_coordinates": [6.0, 60.58, 682.79, 233.46, 26.55]}, {"formula_id": "formula_19", "formula_text": "IMN 2 J 2 /b) O(l [ts,te ] NIJ/b) D-Tucker [12] O( [ , ] \u22122", "formula_coordinates": [6.0, 334.07, 134.24, 201.26, 17.26]}, {"formula_id": "formula_20", "formula_text": "2 ) O( [ , ] \u22122 ) Tucker-ALS O( [ , ]\u22121", "formula_coordinates": [6.0, 337.07, 142.47, 199.07, 17.8]}, {"formula_id": "formula_21", "formula_text": ") O( [ , ] \u22121 ) MACH [35] O( [ , ]\u22121", "formula_coordinates": [6.0, 337.37, 151.47, 197.07, 17.57]}, {"formula_id": "formula_22", "formula_text": ") O( [ , ] \u22121 ) RTD [5] O( [ , ]\u22121", "formula_coordinates": [6.0, 342.38, 160.24, 193.81, 17.56]}, {"formula_id": "formula_23", "formula_text": ") O( [ , ]\u22121", "formula_coordinates": [6.0, 454.04, 169.0, 77.74, 8.8]}, {"formula_id": "formula_24", "formula_text": "( [ , ] \u22121 + ) O( [ , ] \u22121 + ) Tucker-ttmts [25] O( [ , ] \u22121 + 2 \u22122 ) O( [ , ] \u22121 + )", "formula_coordinates": [6.0, 329.0, 177.77, 217.29, 17.57]}, {"formula_id": "formula_25", "formula_text": "! \u00d7 $% \u00d7 &'( \u00d7 )* \u00d7 ') \u00d7", "formula_coordinates": [7.0, 357.73, 114.43, 163.25, 25.02]}, {"formula_id": "formula_26", "formula_text": "X\u2212X 2 F X 2 F", "formula_coordinates": [7.0, 411.69, 341.58, 25.08, 20.72]}, {"formula_id": "formula_27", "formula_text": "X \u2212\u0176 2 F X \u2212\u1e90 2 F", "formula_coordinates": [8.0, 197.66, 558.46, 22.26, 21.71]}, {"formula_id": "formula_28", "formula_text": "Y \u2190 X \u00d7 1 A (1) \u2022 \u2022 \u2022 \u00d7 \u22121 A ( \u22121) \u00d7 +1 A ( +1) \u2022 \u2022 \u2022 \u00d7 A ( )", "formula_coordinates": [11.0, 96.94, 192.23, 201.42, 10.0]}, {"formula_id": "formula_29", "formula_text": "G \u2190 X \u00d7 1 A (1) \u00d7 2 A (2) \u2022 \u2022 \u2022 \u00d7 A ( ) B PROOFS B.1 Proof of Lemma 1 P .", "formula_coordinates": [11.0, 53.8, 232.26, 155.7, 65.5]}, {"formula_id": "formula_30", "formula_text": "( ) A ( ) = \u22122X ( ) (\u2297 \u2260\u00c3 ( ) )G ( ) +2\u00c3 ( )G ( ) \u2297 \u2260\u00c3 ( )\u00c3( ) G ( )", "formula_coordinates": [11.0, 59.26, 331.25, 247.23, 20.19]}, {"formula_id": "formula_31", "formula_text": "A ( ) G ( ) \u2297 \u2260\u00c3 ( )\u00c3( ) G ( ) =X ( ) \u2297 \u2260\u00c3 ( ) G ( )\u21d4\u00c3", "formula_coordinates": [11.0, 61.74, 396.73, 223.27, 31.82]}, {"formula_id": "formula_32", "formula_text": "I ( ) S ( ) \u2297 \u2260 U ( ) V", "formula_coordinates": [11.0, 53.8, 579.9, 81.42, 13.6]}, {"formula_id": "formula_33", "formula_text": "Z = S \u00d7 1 (U (1) V (1) ) \u2022 \u2022 \u2022 \u00d7 \u22121 (U ( \u22121) V ( \u22121) ) \u00d7 I ( ) \u00d7 +1 (U ( +1) V ( +1) ) \u2022 \u2022 \u2022 \u00d7 (U ( ) V ( ) )(9)", "formula_coordinates": [11.0, 74.19, 613.97, 219.86, 27.67]}, {"formula_id": "formula_34", "formula_text": "A ( ) = X < > ( ) \u2022 \u2022 \u2022 X < > ( ) \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0\u00c3 ( ) [ ] . . . A ( ) [ ] \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb \u2297 \u2297 \u22121 \u2260\u00c3 ( ) G ( ) C ( ) \u22121 = = X < > ( ) \u00c3 ( ) [ ] \u2297 \u2297 \u22121 \u2260\u00c3 ( ) G ( ) C ( ) \u22121", "formula_coordinates": [11.0, 317.96, 137.41, 252.11, 74.26]}, {"formula_id": "formula_35", "formula_text": "X ( ) \u2297 \u22121 =1\u00c3 ( ) = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 X < > ( ) \u2297 \u22121 =1\u00c3 ( ) . . . X < > ( ) \u2297 \u22121 =1\u00c3 ( ) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb", "formula_coordinates": [11.0, 360.26, 425.79, 154.83, 52.94]}, {"formula_id": "formula_36", "formula_text": "X ( ) \u2297 \u22121 =1\u00c3 ( ) \u2248 \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 (A < > ) ( ) G < > ( ) \u2297 \u22121 =1 (A < > ) ( )\u00c3( ) . . . (A < > ) ( ) G < > ( ) \u2297 \u22121 =1 (A < > ) ( )\u00c3( ) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb (10)", "formula_coordinates": [11.0, 317.96, 508.95, 242.16, 58.06]}, {"formula_id": "formula_37", "formula_text": "A ( ) = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 (A < > ) ( ) G < > ( ) \u2297 \u22121 =1 (A < > ) ( )\u00c3( ) G ( ) . . . (A < > ) ( ) G < > ( ) \u2297 \u22121 =1 (A < > ) ( )\u00c3( ) G ( ) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb C ( ) \u22121 = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 (A", "formula_coordinates": [11.0, 317.96, 600.1, 252.6, 97.29]}, {"formula_id": "formula_38", "formula_text": "X 2 F \u2212 G 2 F X F [", "formula_coordinates": [12.0, 374.33, 334.1, 43.16, 20.16]}], "doi": "10.1145/3447548.3467290"}