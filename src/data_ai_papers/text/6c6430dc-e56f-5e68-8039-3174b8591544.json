{"title": "Efficient Globally Optimal Consensus Maximisation with Tree Search", "authors": "Tat-Jun Chin; Pulak Purkait; Anders Eriksson; David Suter", "pub_date": "", "abstract": "Maximum consensus is one of the most popular criteria for robust estimation in computer vision. Despite its widespread use, optimising the criterion is still customarily done by randomised sample-and-test techniques, which do not guarantee optimality of the result. Several globally optimal algorithms exist, but they are too slow to challenge the dominance of randomised methods. We aim to change this state of affairs by proposing a very efficient algorithm for global maximisation of consensus. Under the framework of LP-type methods, we show how consensus maximisation for a wide variety of vision tasks can be posed as a tree search problem. This insight leads to a novel algorithm based on A* search. We propose efficient heuristic and support set updating routines that enable A* search to rapidly find globally optimal results. On common estimation problems, our algorithm is several orders of magnitude faster than previous exact methods. Our work identifies a promising solution for globally optimal consensus maximisation 1 .", "sections": [{"heading": "Introduction", "text": "Maximum consensus is one of the most popular robust criteria for geometric estimation problems in computer vision. Given a set of measurements X = {x i } N i=1 , the criterion aims to find the estimate \u03b8 that agrees with as many of the data as possible (i.e., the inliers) up to a threshold max \u03b8, I\u2286X |I| subject to r i (\u03b8) \u2264 \u2200x i \u2208 I,\nwhere r i (\u03b8) is the residual of x i . For example, in triangulation we wish to estimate the 3D point \u03b8 seen in N views, where X contains the 2D observations of the point. The residual r i (\u03b8) is the reprojection error in the i-th view.\nRandom sample consensus (RANSAC) [7] has been the dominant approach. The method randomly draws p-tuples from X , where p is the minimum number of data to instantiate \u03b8. The consensus score |I| of each sample \u03b8 is obtained, and the \u03b8 with the highest score and its consensus set I are returned. A probabilistic bound on the number of samples required can be derived as a stopping criterion.\nA major shortcoming of randomised methods such as RANSAC is the lack of absolute certainty that the obtained solution is optimal, or indeed whether it represents a satisfactory approximation at all. A less recognised fact is that even if all N p subsets are examined, we may not find the globally optimal solution \u03b8 * to (1), since \u03b8 * does not generally correspond to an estimate from a p-tuple (see below).\nSolving (1) exactly is computationally challenging. Several authors proposed methods based on branch-and-bound (BnB) [13,21]. However, BnB is typically slow, especially if \u03b8 is high-dimensional. In fact, RANSAC is suggested to suboptimally preprocess the data (which may cause genuine inliers to be discarded), before BnB is invoked to refine the result. More fundamentally, the BnB methods are specialised for linear residuals. For many vision problems, this entails linearising r i (\u03b8) and adopting algebraic residuals which are not geometrically meaningful.\nIt has been proven [15,5] that for various estimation tasks, \u03b8 * can be found as the solution on a subset of X of size d, where d \u2265 p and d N (the actual value of d depends on the particular problem). Both works proposed to find \u03b8 * by exhaustively searching over all N d subsets of X . Although the number of subsets is polynomial w.r.t. N , in realistic problems the number is impracticably large. Olsson et al. [15] also proposed using RANSAC with an optimality verification step. However, the fact remains that an enormous number of subsets may need to be sampled.\nDue to the much greater computational expense, currently available global methods are not competitive against RANSAC and its variants. In this paper, we make significant progress towards solving (1) exactly and efficiently. Leveraging on the framework of LP-type methods [18,14], we show how maximising consensus can be casted as a tree search problem. We then propose an algorithm based on A* search [8] to traverse the tree. Similar in spirit to [15,5], we aim to find the optimal data subset. However, instead of sampling or enumerating the subsets, our algorithm deterministically locates the best subset. The A* technique ensures that only a tiny fraction of available subsets need to be explored. Despite its combinatorial nature, our algorithm is fast -on several common estimation problems, our algorithm is orders of magnitude faster than previous exact methods for maximum consensus. Further, our method does not require linearising the residual.", "publication_ref": ["b6", "b12", "b20", "b14", "b4", "b14", "b17", "b13", "b7", "b14", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Related work", "text": "L \u221e minimisation (minmax problem) is well established in the context of geometric estimation [10,11]. The task is to find the estimate \u03b8 that minimises the largest residual\nmin \u03b8 max i r i (\u03b8).(2)\nWhilst ( 2) is inherently non-robust since it effectively fits \u03b8 on the outliers, it contains a single (global) minimum, provided r i (\u03b8) is strictly quasiconvex and the data is nondegenerate. Algorithms based on bisection and other schemes [6] have been proposed to solve (2) exactly.\nSince the largest residuals are contributed by the outliers, one can construct an outlier removal scheme where (2) is first solved and the data corresponding to the largest residuals are removed [19,16,20]. The fitting and removal steps can be conducted iteratively until the maximum error is below a threshold . There is no guarantee, however, that the remaining data is the largest possible consensus set, since genuine inliers may also be removed during the iterations.\nIf, instead of the largest, we minimise the j-th largest residual (j \u2264 N ), the outliers can be ignored and a robust estimate can be obtained. However, the problem is now much harder since multiple local minima exist. Ke and Kanade [11] extended their algorithm to approximately minimise the j-th largest residual. Matou\u0161ek proposed a method [14] to examine all the minima of the problem to find the global minimiser. Li [12] pioneered the usage of Matou\u0161ek's method to conduct robust triangulation. It is provable that the time complexity of Matou\u0161ek's procedure is a p-th order polynomial on the number of outliers. However, on typical-sized problems the search is painfully slow.\nIn general, knowing in advance the correct value of j (i.e., the number of inliers) can be non-trivial. On the other hand setting the inlier threshold in (1) is arguably easier since we usually have an idea of how far the inliers can deviate (assuming geometrically meaningful residual functions are employed). For example, the reprojection error of inliers in triangulation is typically within a few pixels.", "publication_ref": ["b9", "b10", "b5", "b18", "b15", "b19", "b10", "b13", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Background", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Solvable problems", "text": "We first define the type of problems solvable by our maximum consensus algorithm. We require that the residual function r i (\u03b8) be pseudoconvex. This is known to include many common applications [17]. Examples are as follows.", "publication_ref": ["b16"], "figure_ref": [], "table_ref": []}, {"heading": "Linear regression", "text": "The linear relation is defined by vector \u03b8 \u2208 R M and the residual function is defined as\nr i (\u03b8) = |b i \u2212 \u03b8 T a i |,(3)\nwhere\nx i = [a T i b i ] T .\nHere r i (\u03b8) is convex, which is a stricter condition than pseudoconvexity.\nTriangulation We wish to estimate the 3D point \u03b8 seen in N views. The i-th image point and camera matrix are respectively x i and P i \u2208 R 3\u00d74 . The reprojection error is\nr i (\u03b8) = (P i,1:2 \u2212 x i P i,3 )\u03b8 P i,3\u03b8 ,(4)\nwhere\u03b8 = [\u03b8 T 1] T , P i,1:2 is the first-two rows of P i , and P i,3 is the third row of P i . It is known that (4) is pseudoconvex. The additional constraint P i,3\u03b8 > 0 must be imposed such that the 3D point lies in front of the cameras.\nHomography fitting Given a set of point matches X = {(u i , u i )} N i=1 across two views, we wish to estimate the homography \u03b8 \u2208 R 3\u00d73 that aligns the points. The residual is\nr i (\u03b8) = (\u03b8 1:2 \u2212 u i \u03b8 3 )\u0169 i \u03b8 3\u0169i ,(5)\nwhere\u0169 i = [u T i 1] T , \u03b8 1:2 is the first-two rows of \u03b8, and \u03b8 3 is the third row of \u03b8. It has been established that (5) is pseudoconvex. Note the resemblance of (5) to (4).\nMany other estimation problems in computer vision naturally involve pseudoconvex residuals, e.g., camera resectioning, SfM with known rotations. In fact, Olsson et al. showed that all the problems in [10] are pseudoconvex.", "publication_ref": ["b2", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "LP-type problems", "text": "LP-type problems can be regarded as generalisations of linear programming (LP) problems [18]. They are intimately connected to minmax problems such as (2). In particular, it has been established that the minmax of a set of pseudoconvex functions is an LP-type problem [2,6]. This implies the satisfaction of several key properties [1].\nFirst, we define f (X ) as the solution (the minimal objective value) of the minmax problem (2) on data X , and \u03b8(X ) as the corresponding globally optimal estimate. Property 1 (Monotonicity) For every two sets P \u2286 Q \u2286 X , we can establish the inequality f (P) \u2264 f (Q) \u2264 f (X ).\nProperty 2 (Locality) For every two sets P \u2286 Q \u2286 X and every x \u2208 X , if f (P) = f (Q) = f (P \u222a {x}), then we can establish the equality f (P) = f (Q \u222a {x}).\nThe concept of basis is integral to LP-type problems.\nDefinition 1 (Basis) A basis B is a subset of X whereby every proper subset of B has a strictly smaller value of f than B itself, i.e., if A \u2282 B then f (A) < f (B).", "publication_ref": ["b17", "b0", "b0", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Definition 2 (Combinatorial dimension)", "text": "The combinatorial dimension is the largest possible size for a basis. For pseudoconvex problems where \u03b8 lies in a p-dimensional domain, the combinatorial dimension is p + 1 [6]. Definition 3 (Violation set, coverage and level) The violation set V(B) of a basis B \u2286 X contains the data from X whose residuals are greater than f (B) when evaluated at \u03b8(B). The coverage C(B) of B is the complement of V(B). The level l(B) of B is the size of V(B).", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}, {"heading": "Definition 4 (Support set)", "text": "The level-0 basis for a dataset X is called the support set of X .\nSolving (2) amounts to finding the support set of X ; Fig. 1 illustrates. Assuming X is non-degenerate 2 , it has only one support set (cf. the minmax problem has a single minimum).\nConsider modifying ( 2) such that the j-th largest residual is minimised\nmin \u03b8 r (j) (\u03b8),(6)\nwhere r (1) (\u03b8), . . . , r (N ) (\u03b8) are the sorted residuals given \u03b8. Defining k := N \u2212 j, we can rewrite (6) as\nmin B f (B), s.t. l(B) = k(7)\ni.e., we seek the level-k basis B whose estimate \u03b8(B) gives the smallest f (B) value. By definition all level-k bases will have j residuals less than or equal to f (B) at \u03b8(B). To solve (7) for k > 0, we describe Matou\u0161ek's method [14].\nDefinition 5 (Basis adjacency) Two bases B and B of respectively levels k and k + 1 are adjacent if V(B ) = V(B) \u222a {x} for some x \u2208 B.\nProperty 3 (Existence of basis path) There exists a path from the level-0 basis to every level-k basis (k > 0) by following adjacent bases. See [14] for the proof.\nThe above property implies that the set of bases can be arranged in a tree structure, where two bases are linked by an edge if they are adjacent. Matou\u0161ek proposed to generate the tree up to the k-th level (by iteratively removing data from bases and refitting to obtain adjacent bases in the next level), such that all level-k bases can be examined; Fig. 2(a) illustrates. Further, he also proved that the number of levelk bases is bounded above by O((k + 1) p ). In practice, however, the number of bases can be very high, and the method is suitable only for a small number of outliers (one or two) 3 . 2 Else, data perturbation methods can be applied [14]. Henceforth, for brevity we assume that all input data X is non-degenerate 3 As the title of [14] suggests, only a few outliers can be handled. ", "publication_ref": ["b0", "b13", "b13", "b1", "b0", "b13", "b1", "b13"], "figure_ref": ["fig_0", "fig_2"], "table_ref": []}, {"heading": "Consensus maximisation as tree search", "text": "We are now ready to describe our novel algorithm for solving the maximum consensus problem (1).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definition 6 (Feasibility)", "text": "A basis B is feasible if f (B) \u2264 . Similarly, a set Q \u2286 X is feasible if f (Q) \u2264 .\nWe can rewrite (1) as the following related problem\nmin B l(B), s.t. f (B) \u2264 .(8)\nIn words, we seek the lowest level basis (i.e., exclude as few data as possible) that is feasible. Given an optimal B * for (8), the maximum consensus set I * is simply C(B * ).\nTo find B * , we can traverse Matou\u0161ek's tree structure using breadth-first search (BFS), i.e., test for feasibility and expand (generate adjacent bases of) all the bases at a particular level, before going to the next level. Such a search regime guarantees that the first feasible basis found is the shallowest (has the lowest level). Fig. 2(b) depicts the idea.\nAssume that B * occurs at level k. In contrast to solving (7) with the same k, the BFS algorithm does not attempt to generate all level-k bases. However, it must still generate all the bases at the previous levels, thus BFS is not efficient in practice. Nonetheless, considerable improvements to this basic scheme can be made, as we describe in the following. For quick reference, Alg. 1 summarises our method.", "publication_ref": ["b6"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Avoiding repeated bases", "text": "Starting from the root node (level-0 basis), there are multiple paths for arriving at the same level-k basis. This corresponds to the fact that as long as the same set of data are removed, the order of the removal is not important. Significant redundancy thus exists in the tree. This characteristic was not exploited in Matou\u0161ek's original method. Note that for the tree in Fig. 2, the redundancy has been removed.\nIn our algorithm, a basis B (and the branch leading from it) are ignored if B has been seen previously (Line 10 in   Retrieve from q the B with the lowest e(B).", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "6:", "text": "if f (B) \u2264 then 7:\nExit with C(B) as the maximum consensus set. for each x \u2208 B do 10:\nif indices of V(B) \u222a {x} do not exist in T then 11:\nHash indices of V(B) \u222a {x} into T .\n12:\nB \u2190 Support set of [C(B) \\ {x}].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "13:", "text": "Insert B with priority e(B ) into q.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "14:", "text": "end if 15:\nend for 16: end while 17: Return error (no feasible solution).\nAlg. 1). Since it is possible for two equal bases to have different coverages, two bases B 1 and B 2 are declared the same\nonly if C(B 1 ) = C(B 2 ), or equivalently, if V(B 1 ) = V(B 2 ).\nWe propose to hash the violation set of seen bases into a table. Let Q \u2286 X be an arbitrary subset. The indices of the data in Q are sorted to yield an integer sequence. We convert the integers to ASCII characters and concatenate them into a string. The string is then hashed into a table via standard techniques to facilitate constant time repetition detections. For example, the set Q = {x 71 , x 10 , x 5 , x 4 , x 28 } is converted to the string 0405102871 and hashed. This simple idea brings significant computational savings.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Informed search with A* algorithm", "text": "Our approach is based on the A* algorithm, which is an informed search method used extensively in pathfinding [8]. Whilst BFS chooses the shallowest basis in the queue to test and expand, A* chooses the basis with the lowest evaluation value e (Step 5 in Alg. 1), which is defined as\ne(B) = l(B) + h(B),(9)\nwhere h is a heuristic. Intuitively, h(B) estimates the number of data to removed from C(B) to make it feasible. Bases with low e are shallow and close to feasibility. Expanding according to e thus accelerates the search; see Fig. 2(c).\nNote that if h(B) = 0 for all B, A* reduces to BFS. An immediate question is whether A* is optimal, i.e., does it always find the shallowest feasible basis?\nDefinition 7 (Admissibility) A heuristic h is admissible if h(B) \u2265 0 and h(B) \u2264 h * (B),(10)\nwhere h * (B) is the minimum number of data that must be removed from C(B) to make the data feasible. Implied by the above is that h(B) = 0 if B is feasible.\nNote that calculating h * (B) amounts to solving exactly the original maximum consensus problem on C(B).\nTheorem 1 A* is optimal if h is admissible.\nProof Following [8], we need to show that when a suboptimal feasible basis B \u2020 exists in the queue, it will not be chosen and tested before an optimal basis B * exists in the queue. Let B be another basis in the queue on the path leading to B * (such a basis B always exists in a tree structure, e.g., the root basis is one). The following can be established\ne(B \u2020 ) = l(B \u2020 ) (since B \u2020 is feasible) > l(B * ) (since B \u2020 is suboptimal) = l(B) + h * (B) (since B leads to B * ) \u2265 l(B) + h(B). (since h is admissible) (11)\nThe above result shows that B which lies on the path to B * will always have a higher priority than B \u2020 to be tested and expanded. Therefore A* will always find B * before B \u2020 .\nIf the current lowest e value in the queue is given by more than one basis, we choose the B with the lowest f (B) first. This tie breaking does not affect the optimality of A*.", "publication_ref": ["b7", "b7"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Heuristic for consensus maximisation", "text": "Our heuristic is inspired by previous outlier removal and reinsertion techniques [19,16,20] but with significant modifications. We call our heuristic h ins and prove that it is applicable under A*. To calculate h ins (B), let\nO = {B 1 , B 2 , . . . , B M }, f (B m ) > \u2200m (12\n)\ncontain the sequence of support sets that must be recursively removed from C(B) to make C(B) feasible, where\nB m = Support set of [C(B) \\ {B 1 , . . . , B m\u22121 }] for m > 1 and B 1 = B.\nAs an example, Fig. 1 shows the first-2 support sets of such a sequence. Let F be the remaining data, i.e.,\nF = C(B) \\ O, where f (F) \u2264 .(13)\nWe then attempt to insert the individual data from O oneby-one into F. If an insertion makes F infeasible, the support set of the enlarged F is removed and the heuristic h ins (B) is incremented by one; else if the enlarged F remains feasible, the insertion is made permanent with no change to the heuristic. Alg. 2 gives a formal description. \nf (F * \u222a B ) \u2265 f (B ) > ,(14)\ni.e., B must contain at least one true outlier. The removal of B from F \u222a {x} thus removes at least one true outlier from F \u222a {x} (note that the remaining set F \u2190 F \u222a {x} \\ B is always feasible). There are thus at most |O * | infeasible support set removals. Since each removal increments h ins (B) by one, its final value cannot be greater than h * (B). Note also that in general O * is not a subset of O and the proof above does not assume that it is.", "publication_ref": ["b18", "b15", "b19"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Early termination", "text": "Define g(B) as an upper bound on the number of data that must be removed from C(B) to make C(B) feasible. Coupled with the heuristic function, we can thus establish\nh(B) \u2264 h * (B) \u2264 g(B).(15)\nGiven g(B), it is possible to terminate Alg. 1 early. Specifically, if h(B) = g(B) for the B currently under test in Line 6, we can exit regardless of whether B is feasible. The maximum consensus set I * is simply F as defined in (13).\nAlgorithm 2 Calculation of h ins (B).  F \u2190 F \u222a {x}. Theorem 3 A* is optimal with early termination.\nProof The value given by\ne * (B) = l(B) + h * (B)(16)\nis the minimum number of data that must be removed from X to achieve feasibility, given that we have removed V(B).\nComparing with (9), it is clear that e(B) \u2264 e * (B).\nIf the current B has h(B) = g(B), then from ( 15), e(B) = e * (B). Since we search the tree according to the A* method, e(B) \u2264 e(B ) where B is any other basis in the queue. Thus e * (B) \u2264 e(B ) \u2264 e * (B ), i.e., the path to the shallowest feasible basis must pass through B.\nWe propose the following upper bound\ng rem (B) = |C(B) \\ F | = |O|,(17)\nwhich can be obtained as a by-product of Alg. 2. ", "publication_ref": ["b12"], "figure_ref": [], "table_ref": []}, {"heading": "Efficient support set updating", "text": "It is clear that the overall efficiency of our algorithm hinges on the efficient calculation of support sets. This amounts to solving the minmax problem (2) for the relevant data subsets. Fortunately, except the initialisation in Step 1 of Alg. 1 where the problem needs to be solved from scratch on all X , all other support set computations merely requires updating a known support set after a small number of data are removed (Step 12 in Alg. 1 and Step 5 in Alg. 2) or inserted (Step 10 in Alg. 2). Thus, our method can be made very fast if the update routine is implemented carefully.\nVarious schemes can be used to calculate support sets for pseudoconvex residuals, e.g., generalised LP [3], bisection [10,11] or pseudoconvex programming [6]. In fact, nonlinear (iterative) optimisation can be used to directly to seek the globally optimal solution [17]. To this end, rewrite and solve (2) as the constrained nonlinear problem\nmin \u03b8,\u03b3 \u03b3, s.t. r i (\u03b8) \u2264 \u03b3,(18)\nwhere the inner max operator is removed. Note that for certain applications, additional constraints must imposed on \u03b8, e.g., cheirality for triangulation (4). Given the solution \u03b8 * and \u03b3 * , the support set can be obtained as the data whose residual r i (\u03b8 * ) equals \u03b3 * . In our work, we use Matlab's SQP algorithm in fmincon to solve (18).\nWe build an efficient support set update routine based on SQP. Let P and Q be two subsets of X , where \u03b8(P) is known and we wish to update it to obtain \u03b8(Q). Also, let Q be obtained from P by removing from or adding to P a small number of data; we thus expect \u03b8(Q) to be close to \u03b8(P). Our update procedure is to simply \"hot start\" SQP with \u03b8(P) when solving (18) on Q. Given the solution \u03b8(Q), the support set of Q can be \"read off\" from the residuals. In practice, this simple update scheme is very efficient.\nMuch greater efficiency can be obtained by specialising the update routine for certain types of residuals. For linear regression (3), we employ the well-known vertex-to-vertex descend algorithm (see [4,Chapter 2]) for Chebyshev regression (l \u221e minimisation), which is extremely fast.", "publication_ref": ["b1", "b9", "b10", "b5", "b16", "b2", "b17", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "We performed a number of experiments on several common estimation problems. We focussed on comparing our algorithm (A*) against globally optimal methods for robust estimation. For the quantile regressor (7), we tested Matou\u0161ek's method [14]. For the maximum consensus estimator (1), we tested BFS (Sec. 3), BnB with MaxFS formulation 4 [21], and BnB with bilinear formulation (BILIN) 5 [13]. RANSAC was also run as a baseline.\nWe divided our results into two categories: linear regression (3) and problems with pseudoconvex residuals. Note that MaxFS and BILIN only works for linearised residuals.\nA* was implemented fully in Matlab: support set updating was conducted using fmincon for the pseudo-  convex case, whilst the vertex-to-vertex algorithm was programmed by ourselves. These routines were also used for Matou\u0161ek and BFS. MaxFS requires solving Mixed Integer Linear Programming (MILP) problems, for which we used the state-of-the-art Gurobi solver. For the LP subroutines in BILIN, we made use of the highly efficient MOSEK solver.\nFor RANSAC, the standard stopping criterion [7] was used with confidence \u03c1 = 0.99. However, if the criterion had been satisfied before 100 iterations, we forced RANSAC to continue until 100 iterations. On the other hand, if the number of unique p-subsets for the problem was less than 100, all the p-subsets were enumerated and tested.\nAll the experiments were run on a standard system with 2.70 GHz processor and 8 GB of RAM.", "publication_ref": ["b13", "b2", "b20", "b12", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Linear regression", "text": "Line fitting We generated N = 100 points around a straight line under Gaussian noise with \u03c3 in = 0.1. A number of the points were then corrupted by a larger Gaussian noise (\u03c3 out = 1) to simulate outliers. The inlier threshold in (1) was chosen as = 0.3, and k for (7) was set as the true number of outliers (known after solving (1) exactly).\nFig. 3(a) shows a typical data instance and result. Inliers found by RANSAC are circled in red, while those found by A* are circled in blue. Points circled in green are inliers found by both RANSAC and A*. As expected, RANSAC found a suboptimal solution with 76 inliers, while A* found the globally optimal solution with 87 inliers.   We varied the number of points that were corrupted as outliers, and invoke all the compared methods. Each method was run 100 times, and the average runtime was recorded. Note that, apart from RANSAC, the methods deterministically gave the same regression result. Fig. 3(b) shows the average consumed time of the different methods against number of outliers (again, this number is known after (1) is solved). As predicted, Matou\u0161ek and BFS scaled extremely badly with the number of outliers, since progressively larger tree depth need to be explored. MaxFS also scaled badly with the number of outliers. We do not plot BILIN since it required more than 120 seconds even for 1 outlier. A* was only slower than RANSAC but vastly outperformed the other methods. The scaling property of A* was also much better. We observed that RANSAC produced suboptimal solutions in 97% of the runs.", "publication_ref": [], "figure_ref": ["fig_9", "fig_9"], "table_ref": []}, {"heading": "Linearised homography", "text": "We tested on homography estimation using linear regression, where the dimensionality of \u03b8 is 8 (see [9,Chapter 4] on linearising the homography constraint). In Sec. 5.2, we will present results on homography estimation using geometric (pseudoconvex) residuals. Note that each keypoint match produces two residual functions; thus the number of data N and outliers k are doubled in the actual optimisation (see [9,Chapter 4] for details).\nWe tested on three image pairs, the first two pairs used extensively in previous works on robust geometric estimation: Road Sign (image index 1 and 2) [16], Keble College (image index 2 and 3) 6 , and Cambridge (downloaded from Flickr); see Fig. 4. SIFT keypoints were detected and matched using the VLFeat toolbox 7 , and Lowe's second nearest neighbour test was invoked to prune matches. The keypoints were then normalised before generating the linearised homography constraints. The inlier threshold was chosen as = 0.1, and k for quantile regression was set as the true outlier count. Table 1 presents the results of all methods (results of RANSAC are averages over 100 runs).\nIn addition to reporting actual runtime, we also show the  number of subproblems (NSub) invoked/solved. This is defined as the following for the various methods:\n\u2022 RANSAC: Number of trials before termination.   A runtime limit of 1 hour was imposed on all methods. For methods that did not terminate within the limit, the NSub values reported were as obtained at the 1 hour mark. It is clear that A* was much faster than the other global optimisers. Although A* still took two orders of magnitude more time than RANSAC, the fact that A* terminated in seconds makes it a very practical global optimiser. BFS and Matou\u0161ek did not finish within 1 hour, thus pointing to the vast improvement in tree search efficiency given by our ideas in Sec. 3. MaxFS was efficient for Road Sign and Cambridge; however, in Keble College its run time increased drastically, owing to the much larger data size N . BILIN did not finish within 1 hour on all datasets; the method also suffers from scalability issues as N increases.", "publication_ref": ["b8", "b8", "b15", "b5", "b6"], "figure_ref": ["fig_7"], "table_ref": ["tab_3"]}, {"heading": "Linearised fundamental matrix", "text": "We repeated the previous experiment for linearised fundamental matrix estimation, where the parameter of interest \u03b8 is 8 dimensional and inlier threshold = 0.1 (see [9,Chapter 11] for the constraint linearisation step). To cogently test the optimisation performance of all methods, we did not enforce the rank-2 constraint on the resulting fundamental matrices. For this experiment, we used Valbonne Church (image index 3 and 7) and Merton College III (image index 2 and 3) images from Oxford VGG. Results are shown in Table 1. Similar performances were observed for all methods.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Pseudoconvex residuals", "text": "Homography estimation We used the same datasets and settings as the linearised homography estimation experiment. The inlier threshold was set to 0.1 pixels, but due to using the pseudoconvex residual function (5), the maximum consensus sets obtained were different from the linearised case. The results are shown in Table 2. It is apparent that BFS and Matou\u0161ek were faster here than in the linearised case, even though support set updating is slower for pseudoconvex residuals. The reason behind this is that there is no doubling of the effective data size and outlier numbers due to linearising the homography constraint (observe that NSub values were much higher in Table 1). Although A* was slower in this experiment, it was still much faster than BFS and Matou\u0161ek, especially on the larger Cambridge and Keble College datasets. Note that A* managed to return the globally optimal estimate within tens of seconds.\nTriangulation We used the Cathedral and House image sequences [16], which contain a large number of feature tracks. Tracks which are shorter than 7 frames were removed, thus 191 and 547 tracks remained for Cathedral and House respectively. For each feature track, the different methods were executed with an inlier threshold of 0.01 pixels. Table 2 summarises the results, which were averaged over the number of tracks in each dataset. Again, A* was much more efficient than Matou\u0161ek and BFS.\nFrom a practical standpoint, owing to the small p and N for this problem (the longest track was less than N = 20 for both datasets), the globally optimal result can be more efficiently found by exhaustively testing all N p+1 subsets of X [15,5]. Nonetheless, our conclusion that A* is more efficient than Matou\u0161ek and BFS still holds. Note also that although RANSAC enumerated all N p minimal subsets in these problems, it was unable to find the global optima.", "publication_ref": ["b15", "b14", "b4"], "figure_ref": [], "table_ref": ["tab_5", "tab_3", "tab_5"]}, {"heading": "Conclusions", "text": "We presented an efficient globally optimal algorithm for maximum consensus, based on the A* tree search algorithm. A heuristic function was proposed and proven to be admissible for use with A*. Experiments showed that the runtime of our method is several orders of magnitude smaller than previous exact methods. Our work identifies a promising direction to solve maximum consensus exactly. The good performance of A* relies on the tightness of the heuristic h as a lower bound of h * . Our method thus has great potential to be speeded up further by constructing specialised heuristic functions for the intended applications.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgements This work was supported by ARC grants DP130102524 and DE130101775.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Optimal point placement for mesh smoothing", "journal": "", "year": "1997", "authors": "N Amenta; M Bern; D Eppstein"}, {"ref_id": "b1", "title": "On linear-time deterministic algorithms for optimization problems in fixed dimensions", "journal": "", "year": "1993", "authors": "B Chazelle; J Matou\u0161ek"}, {"ref_id": "b2", "title": "Introduction to approximation theory", "journal": "", "year": "", "authors": "E W Cheney"}, {"ref_id": "b3", "title": "", "journal": "", "year": "1966", "authors": " Mcgraw-Hill"}, {"ref_id": "b4", "title": "Robust fitting for multiple view geometry", "journal": "", "year": "2008", "authors": "O Enqvist; E Ask; F Kahl; K \u00c5str\u00f6m"}, {"ref_id": "b5", "title": "Quasiconvex programming", "journal": "Combinatorial and Computational Geometry", "year": "2005", "authors": "D Eppstein"}, {"ref_id": "b6", "title": "Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography", "journal": "Comm. of the ACM", "year": "1981", "authors": "M A Fischler; R C Bolles"}, {"ref_id": "b7", "title": "A formal basis for the heuristic determination of minimum cost paths", "journal": "IEEE Trans. on Systems Science and Cybernetics", "year": "1968", "authors": "P E Hart; N J Nilsson; B Raphael"}, {"ref_id": "b8", "title": "Multiple view geometry in computer vision", "journal": "Cambridge University Press", "year": "2004", "authors": "R Hartley; A Zisserman"}, {"ref_id": "b9", "title": "Multiple view geometry and the l\u221e-norm", "journal": "", "year": "2005", "authors": "F "}, {"ref_id": "b10", "title": "Quasiconvex optimization for robust geometric reconstruction", "journal": "", "year": "2005", "authors": "Q Ke; T Kanade"}, {"ref_id": "b11", "title": "A practical algorithm for l\u221e triangulation with outliers", "journal": "", "year": "2007", "authors": "H Li"}, {"ref_id": "b12", "title": "Consensus set maximization with guaranteed global optimality for robust geometry estimation", "journal": "", "year": "2009", "authors": "H Li"}, {"ref_id": "b13", "title": "On geometric optimization with few violated constraints", "journal": "Discrete and computational geometry", "year": "1995", "authors": "J Matou\u0161ek"}, {"ref_id": "b14", "title": "A polynomial-time bound for matching and registration with outliers", "journal": "", "year": "2008", "authors": "C Olsson; O Enqvist; F Kahl"}, {"ref_id": "b15", "title": "Outlier removal using duality", "journal": "", "year": "2008", "authors": "C Olsson; A Eriksson; R Hartley"}, {"ref_id": "b16", "title": "Efficient optimization for l\u221e-problems using pseudoconvexity", "journal": "", "year": "2007", "authors": "C Olsson; A Eriksson; F Kahl"}, {"ref_id": "b17", "title": "A combinatorial bound for linear programming and related problems", "journal": "", "year": "1992", "authors": "M Sharir; E Welzl"}, {"ref_id": "b18", "title": "Removing outliers using the l\u221e norm", "journal": "", "year": "2006", "authors": "K Sim; R Hartley"}, {"ref_id": "b19", "title": "An adversarial optimization approach to efficient outlier removal", "journal": "", "year": "2005", "authors": "J Yu; A Eriksson; T.-J Chin; D Suter"}, {"ref_id": "b20", "title": "Deterministically maximizing feasible subsystems for robust model fitting with unit norm constraints", "journal": "", "year": "2006", "authors": "Y Zheng; S Sugimoto; M Okutomi"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure1. Illustrating the concept of basis on linear regression with \u03b8 \u2208 R 2 (p = 2). Finding the level-0 basis and its corresponding estimate (red solid line) solves (2) on X . A level-3 basis is also shown; observe that 3 points violate the corresponding estimate. For the linear regression problem (3), all bases are of size p + 1.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Matou\u0161ek's method for solving (6) with k = 4. A* search for solving(8).", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 .2Figure 2. Tree traversal for solving (6) (panel a) and (8) (panels b and c) on a problem with p = 1. The ordinate is f (B) while the abscissa is \u03b8(B). Nodes in red are bases expanded in the respective algorithms. In Matou\u0161ek's method, all level-k bases (here, k = 4) must be tested, since there are multiple local minima. In panels b and c, the two different globally optimal solutions found are equally good w.r.t.(8).", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Theorem 2 h2ins is admissible. Proof That h ins is nonnegative is obvious. Let C(B) = F * \u222a O * , where F * is the largest feasible subset of C(B). O * is thus the smallest subset that must be removed from C(B) to achieve feasibility, i.e., h * (B) = |O * |. For intuition, we refer to F * and O * as \"true\" inliers and outliers. Let x be a datum from O, and B the support set of [F \u222a {x}]. If B is infeasible, then by Property 1 (monotonicity)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "1:If f (B) \u2264 , return 0. 2: O \u2190 \u2205. 3: while f (B) > do Support set of [C(B) \\ B]. 6: end while 7: h ins \u2190 0, F \u2190 C(B). 8: for each B \u2208 O do 9: for each x \u2208 B do 10: B \u2190 Support set of [F \u222a {x}].", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "11 :11if f (B ) \u2264 then 12:", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Theorem 4 g4rem (B) \u2265 h * (B). Proof Since F is feasible, then |F| \u2264 |F * | where F * is the largest feasible subset of C(B). This implies that |g rem (B)| \u2265 |O * | = h * (B), where O * = C(B) \\ F * .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Typical data instance and lines fitted by RANSAC and A*. In this data instance, RANSAC found 76 inliers while A* found 87 inliers. Average runtime of different methods against number of outliers.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 3 .3Figure 3. Line fitting results.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "p = 8, = 0.1 N = 55, p = 8, = 0.1 N = 299, p = 8, = 0.1 N = 58, p = 8, = 0.1 N = 187, p = 8, = 0.1 |I R | = 29.67, |I * | = 30 |I R | = 44.80, |I * | = 47 |I R | = 282.74, |I * | = 284 |I R | = 43.66, |I * | = 52 |I R | = 176.69, |I * | = 181 N: data size, p: number of parameters, : inlier threshold |I R |: average consensus size of RANSAC, |I * |: optimal consensus size, NSub: number of subproblems solved", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Figure 4 .4Figure 4. Homography estimation result by A* on the three image pairs. Lines in green and red respectively indicate detected inlier and outlier SIFT keypoint matches.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "p = 8, = 0.1 N = 55, p = 8, = 0.1 N = 299, p = 8, = 0.1 N = 7, p = 3, = 0.01 N = 8, p = 3, = 0.01 |I R | = 29.85, |I * | = 30 |I R | = 44.70, |I * | = 47 |I R | = 282.47, |I * | = 284 |I R | = 1, |I * | = 2 |I R | = 3, |I * | = 4 N: data size, p: number of parameters, : inlier threshold |I R |: average consensus size of RANSAC, |I * |: optimal consensus size, NSub: number of subproblems solved", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Results for linearised homography estimation and linearised fundamental matrix estimation.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Results for homography estimation and triangulation with pseudoconvex residuals.", "figure_data": "\u2022 BILIN: Number of LP instances solved as reported inthe MOSEK output.\u2022 Matou\u0161ek, BFS and A*: Number of support set updateinstances (including in heuristic computation for A*)."}], "formulas": [{"formula_id": "formula_1", "formula_text": "min \u03b8 max i r i (\u03b8).(2)", "formula_coordinates": [2.0, 134.6, 208.5, 151.77, 14.66]}, {"formula_id": "formula_2", "formula_text": "r i (\u03b8) = |b i \u2212 \u03b8 T a i |,(3)", "formula_coordinates": [2.0, 386.0, 106.96, 159.12, 11.72]}, {"formula_id": "formula_3", "formula_text": "x i = [a T i b i ] T .", "formula_coordinates": [2.0, 336.94, 130.39, 66.09, 12.32]}, {"formula_id": "formula_4", "formula_text": "r i (\u03b8) = (P i,1:2 \u2212 x i P i,3 )\u03b8 P i,3\u03b8 ,(4)", "formula_coordinates": [2.0, 364.81, 216.74, 180.31, 24.61]}, {"formula_id": "formula_5", "formula_text": "r i (\u03b8) = (\u03b8 1:2 \u2212 u i \u03b8 3 )\u0169 i \u03b8 3\u0169i ,(5)", "formula_coordinates": [2.0, 370.62, 362.86, 174.49, 23.25]}, {"formula_id": "formula_6", "formula_text": "min \u03b8 r (j) (\u03b8),(6)", "formula_coordinates": [3.0, 142.53, 339.92, 143.84, 14.66]}, {"formula_id": "formula_7", "formula_text": "min B f (B), s.t. l(B) = k(7)", "formula_coordinates": [3.0, 112.85, 394.84, 173.52, 14.58]}, {"formula_id": "formula_8", "formula_text": "A basis B is feasible if f (B) \u2264 . Similarly, a set Q \u2286 X is feasible if f (Q) \u2264 .", "formula_coordinates": [3.0, 308.86, 305.85, 236.25, 20.69]}, {"formula_id": "formula_9", "formula_text": "min B l(B), s.t. f (B) \u2264 .(8)", "formula_coordinates": [3.0, 370.94, 363.46, 174.17, 14.58]}, {"formula_id": "formula_10", "formula_text": "only if C(B 1 ) = C(B 2 ), or equivalently, if V(B 1 ) = V(B 2 ).", "formula_coordinates": [4.0, 50.11, 517.56, 236.25, 9.65]}, {"formula_id": "formula_11", "formula_text": "e(B) = l(B) + h(B),(9)", "formula_coordinates": [4.0, 384.21, 268.2, 160.91, 8.96]}, {"formula_id": "formula_12", "formula_text": "Definition 7 (Admissibility) A heuristic h is admissible if h(B) \u2265 0 and h(B) \u2264 h * (B),(10)", "formula_coordinates": [4.0, 308.86, 374.74, 236.25, 26.46]}, {"formula_id": "formula_13", "formula_text": "e(B \u2020 ) = l(B \u2020 ) (since B \u2020 is feasible) > l(B * ) (since B \u2020 is suboptimal) = l(B) + h * (B) (since B leads to B * ) \u2265 l(B) + h(B). (since h is admissible) (11)", "formula_coordinates": [4.0, 321.71, 573.01, 223.41, 57.44]}, {"formula_id": "formula_14", "formula_text": "O = {B 1 , B 2 , . . . , B M }, f (B m ) > \u2200m (12", "formula_coordinates": [5.0, 73.38, 148.84, 208.83, 9.65]}, {"formula_id": "formula_15", "formula_text": ")", "formula_coordinates": [5.0, 282.21, 149.15, 4.15, 8.64]}, {"formula_id": "formula_16", "formula_text": "B m = Support set of [C(B) \\ {B 1 , . . . , B m\u22121 }] for m > 1 and B 1 = B.", "formula_coordinates": [5.0, 50.11, 200.55, 233.53, 29.53]}, {"formula_id": "formula_17", "formula_text": "F = C(B) \\ O, where f (F) \u2264 .(13)", "formula_coordinates": [5.0, 97.61, 252.26, 188.75, 8.96]}, {"formula_id": "formula_18", "formula_text": "f (F * \u222a B ) \u2265 f (B ) > ,(14)", "formula_coordinates": [5.0, 116.31, 461.21, 170.05, 11.03]}, {"formula_id": "formula_19", "formula_text": "h(B) \u2264 h * (B) \u2264 g(B).(15)", "formula_coordinates": [5.0, 121.09, 646.38, 165.27, 11.03]}, {"formula_id": "formula_20", "formula_text": "e * (B) = l(B) + h * (B)(16)", "formula_coordinates": [5.0, 381.01, 383.39, 164.1, 11.03]}, {"formula_id": "formula_21", "formula_text": "g rem (B) = |C(B) \\ F | = |O|,(17)", "formula_coordinates": [5.0, 366.39, 535.87, 178.73, 9.65]}, {"formula_id": "formula_22", "formula_text": "min \u03b8,\u03b3 \u03b3, s.t. r i (\u03b8) \u2264 \u03b3,(18)", "formula_coordinates": [6.0, 116.01, 239.06, 170.35, 14.66]}], "doi": ""}