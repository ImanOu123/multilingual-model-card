{"title": "Don't Stop Pretraining: Adapt Language Models to Domains and Tasks", "authors": "Suchin Gururangan; Ana Marasovi\u0107; Swabha Swayamdipta; Kyle Lo; Iz Beltagy; Doug Downey; Noah A Smith", "pub_date": "", "abstract": "Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining indomain (domain-adaptive pretraining) leads to performance gains, under both high-and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multiphase adaptive pretraining offers large gains in task performance.", "sections": [{"heading": "Introduction", "text": "Today's pretrained language models are trained on massive, heterogeneous corpora (Raffel et al., 2019;Yang et al., 2019). For instance, ROBERTA  was trained on over 160GB of uncompressed text, with sources ranging from Englishlanguage encyclopedic and news articles, to literary works and web content. Representations learned by such models achieve strong performance across many tasks with datasets of varying sizes drawn from a variety of sources (e.g., Wang et al., 2018. This leads us to ask whether a task's textual domain-a term typically used to denote a distribution over language characterizing a given topic or genre (such as \"science\" or \"mystery novels\")-is still relevant. Do the latest large pretrained models work universally or is it still helpful to build Figure 1: An illustration of data distributions. Task data is comprised of an observable task distribution, usually non-randomly sampled from a wider distribution (light grey ellipsis) within an even larger target domain, which is not necessarily one of the domains included in the original LM pretraining domain -though overlap is possible. We explore the benefits of continued pretraining on data from the task distribution and the domain distribution.\nseparate pretrained models for specific domains?\nWhile some studies have shown the benefit of continued pretraining on domain-specific unlabeled data (e.g., , these studies only consider a single domain at a time and use a language model that is pretrained on a smaller and less diverse corpus than the most recent language models. Moreover, it is not known how the benefit of continued pretraining may vary with factors like the amount of available labeled task data, or the proximity of the target domain to the original pretraining corpus (see Figure 1).\nWe address this question for one such highperforming model, ROBERTA ) ( \u00a72). We consider four domains (biomedical and computer science publications, news, and reviews; \u00a73) and eight classification tasks (two in each domain). For targets that are not already in-domain for ROBERTA, our experiments show that contin-ued pretraining on the domain (which we refer to as domain-adaptive pretraining or DAPT) consistently improves performance on tasks from the target domain, in both high-and low-resource settings.\nAbove, we consider domains defined around genres and forums, but it is also possible to induce a domain from a given corpus used for a task, such as the one used in supervised training of a model. This raises the question of whether pretraining on a corpus more directly tied to the task can further improve performance. We study how domainadaptive pretraining compares to task-adaptive pretraining, or TAPT, on a smaller but directly taskrelevant corpus: the unlabeled task dataset ( \u00a74), drawn from the task distribution. Task-adaptive pretraining has been shown effective (Howard and Ruder, 2018), but is not typically used with the most recent models. We find that TAPT provides a large performance boost for ROBERTA, with or without domain-adaptive pretraining.\nFinally, we show that the benefits from taskadaptive pretraining increase when we have additional unlabeled data from the task distribution that has been manually curated by task designers or annotators. Inspired by this success, we propose ways to automatically select additional task-relevant unlabeled text, and show how this improves performance in certain low-resource cases ( \u00a75). On all tasks, our results using adaptive pretraining techniques are competitive with the state of the art.\nIn summary, our contributions include:\n\u2022 a thorough analysis of domain-and taskadaptive pretraining across four domains and eight tasks, spanning low-and high-resource settings; \u2022 an investigation into the transferability of adapted LMs across domains and tasks; and \u2022 a study highlighting the importance of pretraining on human-curated datasets, and a simple data selection strategy to automatically approach this performance. Our code as well as pretrained models for multiple domains and tasks are publicly available. 1 2 Background: Pretraining Learning for most NLP research systems since 2018 consists of training in two stages. First, a neural language model (LM), often with millions of parameters, is trained on large unlabeled cor-pora. The word (or wordpiece; Wu et al. 2016) representations learned in the pretrained model are then reused in supervised training for a downstream task, with optional updates (fine-tuning) of the representations and network from the first stage.\nOne such pretrained LM is ROBERTA , which uses the same transformerbased architecture (Vaswani et al., 2017) as its predecessor, BERT . It is trained with a masked language modeling objective (i.e., cross-entropy loss on predicting randomly masked tokens). The unlabeled pretraining corpus for ROBERTA contains over 160 GB of uncompressed raw text from different English-language corpora (see Appendix \u00a7A.1). ROBERTA attains better performance on an assortment of tasks than its predecessors, making it our baseline of choice.\nAlthough ROBERTA's pretraining corpus is derived from multiple sources, it has not yet been established if these sources are diverse enough to generalize to most of the variation in the English language. In other words, we would like to understand what is out of ROBERTA's domain. Towards this end, we explore further adaptation by continued pretraining of this large LM into two categories of unlabeled data: (i) large corpora of domain-specific text ( \u00a73), and (ii) available unlabeled data associated with a given task ( \u00a74).", "publication_ref": ["b47", "b65", "b58", "b21", "b62", "b56"], "figure_ref": [], "table_ref": []}, {"heading": "Domain-Adaptive Pretraining", "text": "Our approach to domain-adaptive pretraining (DAPT) is straightforward-we continue pretraining ROBERTA on a large corpus of unlabeled domain-specific text. The four domains we focus on are biomedical (BIOMED) papers, computer science (CS) papers, newstext from REALNEWS, and AMAZON reviews. We choose these domains because they have been popular in previous work, and datasets for text classification are available in each. Table 1 lists the specifics of the unlabeled datasets in all four domains, as well as ROBERTA's training corpus. 1", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Analyzing Domain Similarity", "text": "Before performing DAPT, we attempt to quantify the similarity of the target domain to ROBERTA's pretraining domain. We consider domain vocabularies containing the top 10K most frequent unigrams (excluding stopwords) in comparably sized  random samples of held-out documents in each domain's corpus. We use 50K held-out documents for each domain other than REVIEWS, and 150K held-out documents in REVIEWS, since they are much shorter. We also sample 50K documents from sources similar to ROBERTA's pretraining corpus (i.e., BOOKCORPUS, STORIES, WIKIPEDIA, and REALNEWS) to construct the pretraining domain vocabulary, since the original pretraining corpus is not released. Figure 2 shows the vocabulary overlap across these samples. We observe that ROBERTA's pretraining domain has strong vocabulary overlap with NEWS and REVIEWS, while CS and BIOMED are far more dissimilar to the other domains. This simple analysis suggests the degree of benefit to be expected by adaptation of ROBERTA to different domains-the more dissimilar the domain, the higher the potential for DAPT.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "Our  Beltagy et al. (2020). For RCT, we represent all sentences in one long sequence for simultaneous prediction.\nBaseline As our baseline, we use an off-the-shelf ROBERTA-base model and perform supervised fine-tuning of its parameters for each classification task. On average, ROBERTA is not drastically behind the state of the art (details in Appendix \u00a7A.2), and serves as a good baseline since it provides a single LM to adapt to different domains.\nClassification Architecture Following standard practice  we pass the final layer [CLS] token representation to a task-specific feedforward layer for prediction (see Table 14  Table 2: Specifications of the various target task datasets. \u2020 indicates high-resource settings. Sources: CHEMPROT (Kringelum et al., 2016), RCT (Dernoncourt and Lee, 2017), ACL-ARC (Jurgens et al., 2018), SCIERC (Luan et al., 2018), HYPERPARTISAN (Kiesel et al., 2019), AGNEWS (Zhang et al., 2015), HELPFULNESS (McAuley et al., 2015), IMDB (Maas et al., 2011 ", "publication_ref": ["b4", "b30", "b11", "b24", "b36", "b26", "b68", "b38", "b37"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Domain Relevance for DAPT", "text": "Additionally, we compare DAPT against a setting where for each task, we adapt the LM to a domain outside the domain of interest. This controls for the case in which the improvements over ROBERTA might be attributed simply to exposure to more data, regardless of the domain. In this setting, for NEWS, we use a CS LM; for REVIEWS, a BIOMED LM; for CS, a NEWS LM; for BIOMED, a REVIEWS LM. We use the vocabulary overlap statistics in Figure 2 to guide these choices.\nOur results are shown in Table 3, where the last column (\u00acDAPT) corresponds to this setting. For each task, DAPT significantly outperforms adapting to an irrelevant domain, suggesting the importance of pretraining on domain-relevant data. Furthermore, we generally observe that \u00acDAPT results in worse performance than even ROBERTA on end-tasks. Taken together, these results indicate that in most settings, exposure to more data without considering domain relevance is detrimental to end-task performance. However, there are two tasks (SCIERC and ACL-ARC) in which \u00acDAPT marginally improves performance over ROBERTA. This may suggest that in some cases, continued pretraining on any additional data is useful, as noted in Baevski et al. (2019).", "publication_ref": ["b2"], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Domain Overlap", "text": "Our analysis of DAPT is based on prior intuitions about how task data is assigned to specific domains. For instance, to perform DAPT for HELPFULNESS, we only adapt to AMAZON reviews, but not to any REALNEWS articles. However, the gradations in Figure 2 suggest that the boundaries between domains are in some sense fuzzy; for example, 40% of unigrams are shared between REVIEWS and NEWS. As further indication of this overlap, we also qualitatively identify documents that overlap cross-domain: in Table 4, we showcase reviews and REALNEWS articles that are similar to these reviews (other examples can be found in Appendix \u00a7D). In fact, we find that adapting ROBERTA to IMDB review REALNEWS article \"The Shop Around the Corner\" is one of the great films from director Ernst Lubitsch . In addition to the talents of James Stewart and Margaret Sullavan , it's filled with a terrific cast of top character actors such as Frank Morgan and Felix Bressart. [...] The makers of \"You've Got Mail\" claim their film to be a remake , but that's just nothing but a lot of inflated self praise. Anyway, if you have an affection for romantic comedies of the 1940 's, you'll find \"The Shop Around the Corner\" to be nothing short of wonderful. Just as good with repeat viewings.  NEWS not as harmful to its performance on RE-VIEWS tasks (DAPT on NEWS achieves 65.5 2.3 on HELPFULNESS and 95.0 0.1 on IMDB). Although this analysis is by no means comprehensive, it indicates that the factors that give rise to observable domain differences are likely not mutually exclusive. It is possible that pretraining beyond conventional domain boundaries could result in more effective DAPT; we leave this investigation to future work. In general, the provenance of data, including the processes by which corpora are curated, must be kept in mind when designing pretraining procedures and creating new benchmarks that test out-of-domain generalization abilities.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Task-Adaptive Pretraining", "text": "Datasets curated to capture specific tasks of interest tend to cover only a subset of the text available within the broader domain. For example, the CHEMPROT dataset for extracting relations between chemicals and proteins focuses on abstracts of recently-published, high-impact articles from hand-selected PubMed categories (Krallinger et al., 2017(Krallinger et al., , 2015. We hypothesize that such cases where the task data is a narrowly-defined subset of the broader domain, pretraining on the task dataset itself or data relevant to the task may be helpful.\nTask-adaptive pretraining (TAPT) refers to pretraining on the unlabeled training set for a given task; prior work has shown its effectiveness (e.g. Howard and Ruder, 2018). Compared to domainadaptive pretraining (DAPT; \u00a73), the task-adaptive approach strikes a different trade-off: it uses a far smaller pretraining corpus, but one that is much more task-relevant (under the assumption that the training set represents aspects of the task well). This makes TAPT much less expensive to run than DAPT, and as we show in our experiments, the performance of TAPT is often competitive with that of DAPT.", "publication_ref": ["b28", "b29", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "Similar to DAPT, task-adaptive pretraining consists of a second phase of pretraining ROBERTA, but only on the available task-specific training data. In contrast to DAPT, which we train for 12.5K steps, we perform TAPT for 100 epochs. We artificially augment each dataset by randomly masking different words (using the masking probability of 0.15) across epochs. As in our DAPT experiments, we pass the final layer [CLS] token representation to a task-specific feedforward layer for classification (see Table 14 in Appendix for more hyperparameter details).\nOur results are shown in the TAPT column of Table 5. TAPT consistently improves the ROBERTA baseline for all tasks across domains. Even on the news domain, which was part of ROBERTA pretraining corpus, TAPT improves over ROBERTA, showcasing the advantage of task adaptation. Particularly remarkable are the relative differences between TAPT and DAPT. DAPT is more resource intensive (see Table 9 in \u00a75.3), but TAPT manages to match its performance in some of the tasks, such as SCIERC. In RCT, HYPERPARTISAN, AGNEWS, HELPFULNESS, and IMDB, the results even exceed those of DAPT, highlighting the efficacy of this cheaper adaptation technique.    5), it is harmful when applied across tasks. These findings illustrate differences in task distributions within a domain.\nCombined DAPT and TAPT We investigate the effect of using both adaptation techniques together. We begin with ROBERTA and apply DAPT then TAPT under this setting. The three phases of pretraining add up to make this the most computationally expensive of all our settings (see Table 9). As expected, combined domain-and task-adaptive pretraining achieves the best performance on all tasks (Table 5). 2\nOverall, our results show that DAPT followed by TAPT achieves the best of both worlds of domain and task awareness, yielding the best performance. While we speculate that TAPT followed by DAPT would be susceptible to catastrophic forgetting of the task-relevant corpus (Yogatama et al., 2019), alternate methods of combining the procedures may result in better downstream performance. Future work may explore pretraining with a more sophisticated curriculum of domain and task distributions.", "publication_ref": ["b66"], "figure_ref": [], "table_ref": ["tab_1", "tab_16", "tab_9", "tab_16", "tab_9"]}, {"heading": "Cross-Task Transfer", "text": "We complete the comparison between DAPT and TAPT by exploring whether adapting to one task transfers to other tasks in the same domain. For instance, we further pretrain the LM using the RCT unlabeled data, fine-tune it with the CHEMPROT labeled data, and observe the effect. We refer to this setting as Transfer-TAPT. Our results for tasks in all four domains are shown in Table 6. We see that TAPT optimizes for single task performance, to the detriment of cross-task transfer. These results demonstrate that data distributions of tasks within a given domain might differ. Further, this could also explain why adapting only to a broad domain is not sufficient, and why TAPT after DAPT is effective.  typically curated by humans. We explore two scenarios. First, for three tasks (RCT, HYPERPARTISAN, and IMDB) we use this larger pool of unlabeled data from an available human-curated corpus ( \u00a75.1). Next, we explore retrieving related unlabeled data for TAPT, from a large unlabeled in-domain corpus, for tasks where extra human-curated data is unavailable ( \u00a75.2).", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_10"]}, {"heading": "Human Curated-TAPT", "text": "Dataset creation often involves collection of a large unlabeled corpus from known sources. This corpus is then downsampled to collect annotations, based on the annotation budget. The larger unlabeled corpus is thus expected to have a similar distribution to the task's training data. Moreover, it is usually available. We explore the role of such corpora in task-adaptive pretraining.\nData We simulate a low-resource setting RCT-500, by downsampling the training data of the RCT dataset to 500 examples (out of 180K available), and treat the rest of the training data as unlabeled. The HYPERPARTISAN shared task (Kiesel et al., 2019) has two tracks: low-and high-resource. We use 5K documents from the high-resource setting as Curated-TAPT unlabeled data and the original lowresource training documents for task fine-tuning. For IMDB, we use the extra unlabeled data manually curated by task annotators, drawn from the same distribution as the labeled data (Maas et al., 2011).", "publication_ref": ["b26", "b37"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "We compare Curated-TAPT to TAPT and DAPT + TAPT in Table 7. Curated-TAPT further improves our prior results from \u00a74 across all three datasets. Applying Curated-TAPT after adapting to the domain results in the largest boost in performance on all tasks; in HYPERPARTISAN, DAPT + Curated-TAPT is within standard deviation of Curated-TAPT. Moreover, curated-TAPT achieves Figure 3: An illustration of automated data selection ( \u00a75.2). We map unlabeled CHEMPROT and 1M BIOMED sentences to a shared vector space using the VAMPIRE model trained on these sentences. Then, for each CHEMPROT sentence, we identify k nearest neighbors, from the BIOMED domain.  95% of the performance of DAPT + TAPT with the fully labeled RCT corpus (Table 5) with only 0.3% of the labeled data. These results suggest that curating large amounts of data from the task distribution is extremely beneficial to end-task performance.\nWe recommend that task designers release a large pool of unlabeled task data for their tasks to aid model adaptation through pretraining.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_12", "tab_9"]}, {"heading": "Automated Data Selection for TAPT", "text": "Consider a low-resource scenario without access to large amounts of unlabeled data to adequately benefit from TAPT, as well as absence of computational resources necessary for DAPT (see Table 9 for details of computational requirements for different pretraining phases). We propose simple unsuper-vised methods to retrieve unlabeled text that aligns with the task distribution, from a large in-domain corpus. Our approach finds task-relevant data from the domain by embedding text from both the task and domain in a shared space, then selects candidates from the domain based on queries using the task data. Importantly, the embedding method must be lightweight enough to embed possibly millions of sentences in a reasonable time. Given these constraints, we employ VAMPIRE  Figure 3), a lightweight bag-of-words language model. We pretrain VAM-PIRE on a large deduplicated 3 sample of the domain (1M sentences) to obtain embeddings of the text from both the task and domain sample. We then select k candidates of each task sentence from the domain sample, in embeddings space. Candidates are selected (i) via nearest neighbors selection (kNN-TAPT) 4 , or (ii) randomly (RAND-TAPT). We continue pretraining ROBERTA on this augmented corpus with both the task data (as in TAPT) as well as the selected candidate pool.\nResults Results in Table 8 show that kNN-TAPT outperforms TAPT for all cases. RAND-TAPT is generally worse than kNN-TAPT, but within a standard deviation arising from 5 seeds for RCT and ACL-ARC. As we increase k, kNN-TAPT performance steadily increases, and approaches that of DAPT. Appendix F shows examples of nearest neighbors of task data. Future work might consider a closer study of kNN-TAPT, more sophisticated data selection methods, and the tradeoff between the diversity and task relevance of selected examples.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_16", "tab_14"]}, {"heading": "Computational Requirements", "text": "The computational requirements for all our adaptation techniques on RCT-500 in the BIOMED domain in Table 9. TAPT is nearly 60 times faster to train than DAPT on a single v3-8 TPU and storage requirements for DAPT on this task are 5.8M times that of TAPT. Our best setting of DAPT + TAPT amounts to three phases of pretraining, and at first glance appears to be very expensive. However, once the LM has been adapted to a broad domain, it can be reused for multiple tasks within that domain, with only a single additional TAPT phase per task. While Curated-TAPT tends to achieve the best cost-  benefit ratio in this comparison, one must also take into account the cost of curating large in-domain data. Automatic methods such as kNN-TAPT are much cheaper than DAPT.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_16"]}, {"heading": "Related Work", "text": "Transfer learning for domain adaptation Prior work has shown the benefit of continued pretraining in domain (Alsentzer et al., 2019;Chakrabarty et al., 2019;. 5 We have contributed further investigation of the effects of a shift between a large, diverse pretraining corpus and target domain on task performance.\nOther studies (e.g.,  have trained language models (LMs) in their domain of interest, from scratch. In contrast, our work explores multiple domains, and is arguably more cost effective, since we continue pretraining an already powerful LM.\nTask-adaptive pretraining Continued pretraining of a LM on the unlabeled data of a given task (TAPT) has been show to be beneficial for endtask performance (e.g. in Howard and Ruder, 2018;Phang et al., 2018;Sun et al., 2019). In the presence of domain shift between train and test data distributions of the same task, domain-adaptive pretraining (DAPT) is sometimes used to describe what we term TAPT (Logeswaran et al., 2019;Han and Eisenstein, 2019). Related approaches include language modeling as an auxiliary objective to task classifier fine-tuning (Chronopoulou et al., 2019;Radford et al., 2018)   data (Swayamdipta et al., 2019). We compare DAPT and TAPT as well as their interplay with respect to dataset size for continued pretraining (hence, expense of more rounds of pretraining), relevance to a data sample of a given task, and transferability to other tasks and datasets. See Table 11 in Appendix \u00a7A for a summary of multi-phase pretraining strategies from related work.\nData selection for transfer learning Selecting data for transfer learning has been explored in NLP (Moore and Lewis, 2010;Ruder and Plank, 2017;Zhang et al., 2019, among others).  focus on identifying the most suitable corpus to pretrain a LM from scratch, for a single task: NER, whereas we select relevant examples for various tasks in \u00a75.2. Concurrent to our work, Aharoni and Goldberg (2020) propose data selection methods for NMT based on cosine similarity in embedding space, using DISTILBERT (Sanh et al., 2019) for efficiency. In contrast, we use VAMPIRE, and focus on augmenting TAPT data for text classification tasks. Khandelwal et al. (2020) introduced kNN-LMs that allows easy domain adaptation of pretrained LMs by simply adding a datastore per domain and no further training; an alternative to integrate domain information in an LM. Our study of human-curated data \u00a75.1 is related to focused crawling (Chakrabarti et al., 1999) for collection of suitable data, especially with LM reliance (Remus and Biemann, 2016).\nWhat is a domain? Despite the popularity of domain adaptation techniques, most research and practice seems to use an intuitive understanding of domains. A small body of work has attempted to address this question (Lee, 2001;Eisenstein et al., 2014;van der Wees et al., 2015;Plank, 2016;Ruder et al., 2016, among others). For instance, Aharoni and Goldberg (2020) define domains by implicit clusters of sentence representations in pretrained LMs. Our results show that DAPT and TAPT complement each other, which suggests a spectra of domains defined around tasks at various levels of granularity (e.g., Amazon reviews for a specific product, all Amazon reviews, all reviews on the web, the web).", "publication_ref": ["b1", "b6", "b21", "b44", "b52", "b35", "b18", "b8", "b46", "b53", "b40", "b50", "b0", "b51", "b25", "b5", "b48", "b31", "b14", "b59", "b45", "b0"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Conclusion", "text": "We investigate several variations for adapting pretrained LMs to domains and tasks within those domains, summarized in Table 10. Our experiments reveal that even a model of hundreds of millions of parameters struggles to encode the complexity of a single textual domain, let alone all of language. We show that pretraining the model towards a specific task or small corpus can provide significant benefits. Our findings suggest it may be valuable to complement work on ever-larger LMs with parallel efforts to identify and use domain-and taskrelevant corpora to specialize models. While our results demonstrate how these approaches can improve ROBERTA, a powerful LM, the approaches we studied are general enough to be applied to any pretrained LM. Our work points to numerous future directions, such as better data selection for TAPT, efficient adaptation large pretrained language models to distant domains, and building reusable language models after adaptation.\nAppendix F. Illustration of our data selection method and examples of nearest neighbours.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "A Related Work", "text": "Table 11 shows which of the strategies for continued pretraining have already been explored in the prior work from the Related Work ( \u00a76). As evident from the table, our work compares various strategies as well as their interplay using a pretrained language model trained on a much more heterogeneous pretraining corpus.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "A.1 ROBERTA's Pretraining Corpus", "text": "ROBERTA was trained on data from BOOKCOR-PUS (Zhu et al., 2015), 6 WIKIPEDIA, 7 a portion of the CCNEWS dataset (Nagel, 2016), 8 OPENWEB-TEXT corpus of Web content extracted from URLs shared on Reddit (Gokaslan and Cohen, 2019), 9 and a subset of CommonCrawl that it is said to resemble the \"story-like\" style of WINOGRAD schemas (STORIES; Trinh and Le, 2018). 10", "publication_ref": ["b70", "b41", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "A.2 State of the Art", "text": "In this section, we specify the models achieving state of the art on our tasks. See the caption of 6 https://github.com/soskek/bookcorpus 7 https://github.com/google-research/ bert 8 https://github.com/fhamborg/ news-please 9 https://github.com/jcpeterson/ openwebtext 10 https://github.com/tensorflow/models/ tree/master/research/lm_commonsense Table 5 for the reported performance of these models. For ACL-ARC, that is SCIBERT , a BERT-base model for trained from scratch on scientific text. For CHEMPROT and SCI-ERC, that is S2ORC-BERT (Lo et al., 2020), a similar model to SCIBERT. For AGNEWS and IMDB, XLNet-large, a much larger model. For RCT, . For HYPERPARTISAN, LONGFORMER, a modified Transformer language model for long documents (Beltagy et al., 2020). Thongtan and Phienthrakul (2019) report a higher number (97.42) on IMDB, but they train their word vectors on the test set. Our baseline establishes the first benchmark for the HELPFULNESS dataset.", "publication_ref": ["b34", "b4", "b54"], "figure_ref": [], "table_ref": ["tab_9"]}, {"heading": "B Experimental Setup", "text": "Preprocessing for DAPT The unlabeled corpus in each domain was pre-processed prior to language model training. Abstracts and body paragraphs from biomedical and computer science articles were used after sentence splitting using scispaCy (Neumann et al., 2019). We used summaries and full text of each news article, and the entire body of review from Amazon reviews. For both news and reviews, we perform sentence splitting using spaCy (Honnibal and Montani, 2017).\nTraining details for DAPT We train ROBERTA on each domain for 12.5K steps. We focused on matching all the domain dataset sizes (see Table 1) such that each domain is exposed to the same amount of data as for 12.5K steps it is trained for. AMAZON reviews contain more documents, but each is shorter. We used an effective batch size of 2048 through gradient accumulation, as recommended in . See Table 13 for more hyperparameter details.\nTraining details for TAPT We use the same pretraining hyperparameters as DAPT, but we artificially augmented each dataset for TAPT by randomly masking different tokens across epochs, using the masking probability of 0.15. Each dataset was trained for 100 epochs. For tasks with less than 5K examples, we used a batch size of 256 through gradient accumulation. See Table 13 for more hyperparameter details.\nOptimization We used the Adam optimizer (Kingma and Ba, 2015), a linear learning rate scheduler with 6% warm-up, a maximum learning rate of 0.0005. When we used a batch size of 256, we   Chelba et al., 2014); GPT on BOOKCORPUS; BERT on English Wikipedia and BOOKCORPUS. In comparison to these pretraining corpora, ROBERTA's pretraining corpus is substantially more diverse (see Appendix \u00a7A.1).\nused a maximum learning rate of 0.0001, as recommended in . We observe a high variance in performance between random seeds when fine-tuning ROBERTA to HYPERPARTISAN, because the dataset is extremely small. To produce final results on this task, we discard and resample degenerate seeds. We display the full hyperparameter settings in Table 13.\nImplementation Our LM implementation uses the HuggingFace transformers library (Wolf et al., 2019) 11 and PyTorch XLA for TPU compatibility. 12 Each adaptive pretraining exper-11 https://github.com/huggingface/ transformers 12 https://github.com/pytorch/xla iment was performed on a single v3-8 TPU from Google Cloud. 13 For the text classification tasks, we used AllenNLP (Gardner et al., 2018). Following standard practice  we pass the final layer [CLS] token representation to a task-specific feedforward layer for prediction.", "publication_ref": ["b42", "b20", "b27", "b7", "b15"], "figure_ref": [], "table_ref": ["tab_1", "tab_1", "tab_1", "tab_1"]}, {"heading": "C Development Set Results", "text": "Adhering to the standards suggested by Dodge et al. (2019) for replication, we report our development set results in Tables 15, 17, and 18.", "publication_ref": ["b13"], "figure_ref": [], "table_ref": []}, {"heading": "D Analysis of Domain Overlap", "text": "In ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E Analysis of Cross-Domain Masked LM Loss", "text": "In Section \u00a73.2, we provide ROBERTA's masked LM loss before and after DAPT. We display crossdomain masked-LM loss in Table 12, where we evaluate masked LM loss on text samples in other domains after performing DAPT.\nWe observe that the cross-domain masked-LM loss mostly follows our intuition and insights from the paper, i.e. ROBERTA's pretraining corpus and NEWS are closer, and BIOMED to CS (relative to other domains). However, our analysis in \u00a73.1 illustrates that REVIEWS and NEWS also have some similarities. This is supported with the loss of ROBERTA that is adapted to NEWS, calculated on a sample of REVIEWS. However, ROBERTA that is adapted to REVIEWS results in the highest loss for a NEWS sample. This is the case for all domains. One of the properties that distinguishes REVIEWS from all other domains is that its documents are significantly shorter. In general, we find that cross-DAPT masked-LM loss can in some cases be a noisy predictor of domain similarity.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "F k-Nearest Neighbors Data Selection", "text": "In Table 21, we display nearest neighbor documents in the BIOMED domain identified by our selection method, on the RCT dataset.       Table 19: Mean development set macro-F 1 (for HYP. and IMDB) and micro-F 1 (for RCT), across five random seeds, with standard deviations as subscripts, comparing RAND-TAPT (with 50 candidates) and kNN-TAPT selection. Neighbors of the task data are selected from the domain data.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1", "tab_1"]}, {"heading": "IMDB review REALNEWS article", "text": "Spooks is enjoyable trash, featuring some well directed sequences, ridiculous plots and dialogue, and some third rate acting. Many have described this is a UK version of \"24\", and one can see the similarities. The American version shares the weak silly plots, but the execution is so much slicker, sexier and I suspect, expensive. Some people describe weak comedy as \"gentle comedy\". This is gentle spy story hour, the exact opposite of anything created by John Le Carre. Give me Smiley any day.\n[...] Remember poor Helen Flynn from Spooks? In 2002, the headlong BBC spy caper was in such a hurry to establish the high-wire stakes of its morally compromised world that Lisa Faulkner's keen-as-mustard MI5 rookie turned out to be a lot more expendable than her prominent billing suggested. [...] Functioning as both a shocking twist and rather callous statement that No-One Is Safe, it gave the slick drama an instant patina of edginess while generating a record-breaking number of complaints.\n[...]\nThe Sopranos is perhaps the most mind-opening series you could possibly ever want to watch. It's smart, it's quirky, it's funny -and it carries the mafia genre so well that most people can't resist watching.\nThe best aspect of this show is the overwhelming realism of the characters, set in the subterranean world of the New York crime families. For most of the time, you really don't know whether the wise guys will stab someone in the back, or buy them lunch. Further adding to the realistic approach of the characters in this show is the depth of their personalities -These are dangerous men, most of them murderers, but by God if you don't love them too. I've laughed at their wisecracks, been torn when they've made err in judgement, and felt scared at the sheer ruthlessness of a serious criminal. [...] The drumbeat regarding the \"Breaking Bad\" finale has led to the inevitable speculation on whether the final chapter in this serialized gem will live up to the hype or disappoint (thank you, \"Dexter,\" for setting that bar pretty low), with debate, second-guessing and graduate-thesis-length analysis sure to follow. The Most Memorable TV Series Finales of All-Time [...] No ending in recent years has been more divisive than \"The Sopranos\" -for some, a brilliant flash (literally, in a way) of genius; for others (including yours truly), a too-cute copout, cryptically leaving its characters in perpetual limbo. The precedent to that would be \"St.\nElsewhere,\" which irked many with its provocative, surreal notion that the whole series was, in fact, conjured in the mind of an autistic child.\n[...]\nThe Wicker Man, starring Nicolas Cage, is by no means a good movie, but I can't really say it's one I regret watching. I could go on and on about the negative aspects of the movie, like the terrible acting and the lengthy scenes where Cage is looking for the girl, has a hallucination, followed by another hallucination, followed by a dream sequence-with a hallucination, etc., but it's just not worth dwelling on when it comes to a movie like this. Instead, here's five reasons why you SHOULD watch The Wicker Man, even though it's bad: 5. It's hard to deny that it has some genuinely creepy ideas to it, the only problem is in its cheesy, unintentionally funny execution. If nothing else, this is a movie that may inspire you to see the original 1973 film, or even read the short story on which it is based. 4. For a cheesy horror/thriller, it is really aesthetically pleasing. [...] NOTE: The Unrated version of the movie is the best to watch, and it's better to watch the Theatrical version just for its little added on epilogue, which features a cameo from James Franco.\n[...] What did you ultimately feel about \"The Wicker Man\" movie when all was said and done? [...] I'm a fan of the original and I'm glad that I made the movie because they don't make movies like that anymore and probably the result of what \"Wicker Man\" did is the reason why they don't make movies like that anymore. Again, it's kind of that '70's sensibility, but I'm trying to do things that are outside the box. Sometimes that means it'll work and other times it won't. Again though I'm going to try and learn from anything that I do. I think that it was a great cast, and Neil La Bute is one of the easiest directors that I've ever worked with. He really loves actors and he really gives you a relaxed feeling on the set, that you can achieve whatever it is that you're trying to put together, but at the end of the day the frustration that I had with 'The Wicker Man,' which I think has been remedied on the DVD because I believe the DVD has the directors original cut, is that they cut the horror out of the horror film to try and get a PG-13 rating. I mean, I don't know how to stop something like that. So I'm not happy with the way that the picture ended, but I'm happy with the spirit with which it was made. [...]\nDr. Seuss would sure be mad right now if he was alive. Cat in the Hat proves to show how movie productions can take a classic story and turn it into a mindless pile of goop. We have Mike Myers as the infamous Cat in the Hat, big mistake! Myers proves he can't act in this film. He acts like a prissy show girl with a thousand tricks up his sleeve. The kids in this movie are all right, somewhere in between the lines of dull and annoying. The story is just like the original with a couple of tweaks and like most movies based on other stories, never tweak with the original story! Bringing in the evil neighbor Quin was a bad idea. He is a stupid villain that would never get anywhere in life. [...] The Cat in the Hat, [...] Based on the book by Dr. Seuss [...] From the moment his tall, red-and-white-striped hat appears at their door, Sally and her brother know that the Cat in the Hat is the most mischievous cat they will ever meet. Suddenly the rainy afternoon is transformed by the Cat and his antics. Will their house ever be the same? Can the kids clean up before mom comes home? With some tricks (and a fish) and Thing Two and Thing One, with the Cat in The Hat, the fun's never done!Dr. Seuss is known worldwide as the imaginative master of children's literature. His books include a wonderful blend of invented and actual words, and his rhymes have helped many children and adults learn and better their understanding of the English language. [...] ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "The authors thank Dallas Card, Mark Neumann, Nelson Liu, Eric Wallace, members of the Al-lenNLP team, and anonymous reviewers for helpful feedback, and Arman Cohan for providing data. This research was supported in part by the Office of Naval Research under the MURI grant N00014-18-1-2670.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix Overview", "text": "In this supplementary material, we provide: (i) additional information for producing the results in the paper, and (ii) results that we could not fit into the main body of the paper. Appendix A. A tabular overview of related work described in Section \u00a76, a description of the corpus used to train ROBERTA in , and references to the state of the art on our tasks.  Neighbor 0 Of this group, 26% died after discharge from hospital, and the median time to death was 11 days (interquartile range, 4.0-15.0 days) after discharge. Neighbor 1", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix", "text": "The median hospital stay was 17 days (range 8-26 days), and all the patients were discharged within 1 month. Neighbor 2\nThe median hospital stay was 17 days (range 8-26 days). Neighbor 3\nThe median time between discharge and death was 25 days (mean, 59.1 days) and no patient was alive after 193 days. Neighbor 4\nThe length of hospital stay after colostomy formation ranged from 3 days to 14 days with a median duration of 6 days (+IQR of 4 to 8 days).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Source", "text": "Randomized , controlled , parallel clinical trial . Source Forty primary molar teeth in 40 healthy children aged 5-9 years were treated by direct pulp capping .\nNeighbor 0 In our study, we specifically determined the usefulness of the Er:YAG laser in caries removal and cavity preparation of primary and young permanent teeth in children ages 4 to 1 8 years. Neighbor 1\nMales watched more TV than females, although it was only in primary school-aged children and on weekdays. Neighbor 2\nAssent was obtained from children and adolescents aged 7-17 years. Neighbor 3 Cardiopulmonary resuscitation was not applied to children aged \u00a15 years (Table 2). Neighbor 4\nIt measures HRQoL in children and adolescents aged 2 to 25 years. ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Unsupervised domain clusters in pretrained language models", "journal": "", "year": "2020", "authors": "Roee Aharoni; Yoav Goldberg"}, {"ref_id": "b1", "title": "Publicly available clinical BERT embeddings", "journal": "", "year": "2019", "authors": "Emily Alsentzer; John Murphy; William Boag; Wei-Hung Weng; Di Jindi; Tristan Naumann; Matthew Mcdermott"}, {"ref_id": "b2", "title": "Cloze-driven pretraining of self-attention networks", "journal": "", "year": "2019", "authors": "Alexei Baevski; Sergey Edunov; Yinhan Liu; Luke Zettlemoyer; Michael Auli"}, {"ref_id": "b3", "title": "SciB-ERT: A pretrained language model for scientific text", "journal": "", "year": "2019", "authors": "Iz Beltagy; Kyle Lo; Arman Cohan"}, {"ref_id": "b4", "title": "Longformer: The long-document transformer", "journal": "", "year": "2020", "authors": "Iz Beltagy; Matthew E Peters; Arman Cohan"}, {"ref_id": "b5", "title": "Focused Crawling: A New Approach to Topic-Specific Web Resource Discovery", "journal": "Comput. Networks", "year": "1999", "authors": "Soumen Chakrabarti; Martin Van Den; Byron Berg;  Dom"}, {"ref_id": "b6", "title": "IMHO fine-tuning improves claim detection", "journal": "", "year": "2019", "authors": "Tuhin Chakrabarty; Christopher Hidey; Kathy Mckeown"}, {"ref_id": "b7", "title": "One billion word benchmark for measuring progress in statistical language modeling", "journal": "", "year": "2014", "authors": "Ciprian Chelba; Tomas Mikolov; Michael Schuster; Qi Ge; Thorsten Brants; Phillipp Koehn; Tony Robinson"}, {"ref_id": "b8", "title": "An embarrassingly simple approach for transfer learning from pretrained language models", "journal": "", "year": "2019", "authors": "Alexandra Chronopoulou; Christos Baziotis; Alexandros Potamianos"}, {"ref_id": "b9", "title": "Pretrained language models for sequential sentence classification", "journal": "", "year": "2019", "authors": "Arman Cohan; Iz Beltagy; Daniel King; Bhavana Dalvi; Dan Weld"}, {"ref_id": "b10", "title": "Using similarity measures to select pretraining data for NER", "journal": "", "year": "2019", "authors": "Xiang Dai; Sarvnaz Karimi; Ben Hachey; Cecile Paris"}, {"ref_id": "b11", "title": "Pubmed 200k RCT: a dataset for sequential sentence classification in medical abstracts", "journal": "", "year": "2017", "authors": "Franck Dernoncourt; Ji Young Lee"}, {"ref_id": "b12", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b13", "title": "Show your work: Improved reporting of experimental results", "journal": "", "year": "2019", "authors": "Jesse Dodge; Suchin Gururangan; Dallas Card; Roy Schwartz; Noah A Smith"}, {"ref_id": "b14", "title": "Diffusion of lexical change in social media", "journal": "PloS ONE", "year": "2014", "authors": "Jacob Eisenstein; O' Brendan; Noah A Connor; Eric P Smith;  Xing"}, {"ref_id": "b15", "title": "AllenNLP: A deep semantic natural language processing platform", "journal": "", "year": "2018", "authors": "Matt Gardner; Joel Grus; Mark Neumann; Oyvind Tafjord; Pradeep Dasigi; Nelson F Liu; Matthew Peters; Michael Schmitz; Luke Zettlemoyer"}, {"ref_id": "b16", "title": "", "journal": "", "year": "2019", "authors": "Aaron Gokaslan; Vanya Cohen"}, {"ref_id": "b17", "title": "Variational pretraining for semi-supervised text classification", "journal": "", "year": "2019", "authors": "Suchin Gururangan; Tam Dang; Dallas Card; Noah A Smith"}, {"ref_id": "b18", "title": "Unsupervised domain adaptation of contextualized embeddings for sequence labeling", "journal": "", "year": "2019", "authors": "Xiaochuang Han; Jacob Eisenstein"}, {"ref_id": "b19", "title": "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering", "journal": "", "year": "2016", "authors": "Ruining He; Julian Mcauley"}, {"ref_id": "b20", "title": "spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing", "journal": "", "year": "2017", "authors": "Matthew Honnibal; Ines Montani"}, {"ref_id": "b21", "title": "Universal language model fine-tuning for text classification", "journal": "", "year": "2018", "authors": "Jeremy Howard; Sebastian Ruder"}, {"ref_id": "b22", "title": "ClinicalBERT: Modeling clinical notes and predicting hospital readmission", "journal": "", "year": "2019", "authors": "Kexin Huang; Jaan Altosaar; Rajesh Ranganath"}, {"ref_id": "b23", "title": "Billion-scale similarity search with gpus", "journal": "IEEE Transactions on Big Data", "year": "2019", "authors": "Jeff Johnson; Matthijs Douze; Herv\u00e9 J\u00e9gou"}, {"ref_id": "b24", "title": "Measuring the evolution of a scientific field through citation frames", "journal": "TACL", "year": "2018", "authors": "David Jurgens; Srijan Kumar; Raine Hoover; Daniel A Mcfarland; Dan Jurafsky"}, {"ref_id": "b25", "title": "Generalization through memorization: Nearest neighbor language models", "journal": "", "year": "2020", "authors": "Urvashi Khandelwal; Omer Levy; Dan Jurafsky; Luke Zettlemoyer; Mike Lewis"}, {"ref_id": "b26", "title": "SemEval-2019 Task 4: Hyperpartisan news detection", "journal": "", "year": "2019", "authors": "Johannes Kiesel; Maria Mestre; Rishabh Shukla; Emmanuel Vincent; Payam Adineh; David Corney; Benno Stein; Martin Potthast"}, {"ref_id": "b27", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2015", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b28", "title": "Overview of the biocreative vi chemical-protein interaction track", "journal": "", "year": "2017", "authors": "Martin Krallinger; Obdulia Rabal; Ahmad Saber; Mart\u00edn Akhondi; J\u00e9s\u00fas L\u00f3pez P\u00e9rez P\u00e9rez; Gael P\u00e9rez Santamar\u00eda; Georgios Rodr\u00edguez; Ander Tsatsaronis; Jos\u00e9 Intxaurrondo;  Antonio Baso; Umesh L\u00f3pez; E M Nandal; A Poorna Van Buel; Marleen Chandrasekhar; Astrid Rodenburg; Marius A Laegreid; Julen Doornenbal; An\u00e1lia Oyarz\u00e1bal; Alfonso Louren\u00e7o;  Valencia"}, {"ref_id": "b29", "title": "The chemdner corpus of chemicals and drugs and its annotation principles", "journal": "Journal of cheminformatics", "year": "2015", "authors": "Martin Krallinger; Obdulia Rabal; Florian Leitner; Miguel Vazquez; David Salgado; Zhiyong Lu; Robert Leaman; Yanan Lu; Donghong Ji; M Daniel;  Lowe"}, {"ref_id": "b30", "title": "ChemProt-3.0: a global chemical biology diseases mapping", "journal": "", "year": "2016", "authors": "Jens Kringelum; Sonny Kim Kjaerulff; S\u00f8ren Brunak; Ole Lund; Tudor I Oprea; Olivier Taboureau"}, {"ref_id": "b31", "title": "Genres, registers, text types, domains and styles: Clarifying the concepts and navigating a path through the BNC jungle", "journal": "Language Learning & Technology", "year": "2001", "authors": "Y W David;  Lee"}, {"ref_id": "b32", "title": "BioBERT: A pre-trained biomedical language representation model for biomedical text mining", "journal": "Bioinformatics", "year": "2019", "authors": "Jinhyuk Lee; Wonjin Yoon; Sungdong Kim; Donghyeon Kim; Sunkyu Kim; Chan Ho So; Jaewoo Kang"}, {"ref_id": "b33", "title": "RoBERTa: A robustly optimized BERT pretraining approach", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b34", "title": "S2ORC: The Semantic Scholar Open Research Corpus", "journal": "", "year": "2020", "authors": "Kyle Lo; Lucy Lu Wang; Mark Neumann; Rodney Kinney; Daniel S Weld"}, {"ref_id": "b35", "title": "Zero-shot entity linking by reading entity descriptions", "journal": "", "year": "2019", "authors": "Lajanugen Logeswaran; Ming-Wei Chang; Kenton Lee; Kristina Toutanova; Jacob Devlin; Honglak Lee"}, {"ref_id": "b36", "title": "Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction", "journal": "", "year": "2018", "authors": "Yi Luan; Luheng He; Mari Ostendorf; Hannaneh Hajishirzi"}, {"ref_id": "b37", "title": "Learning word vectors for sentiment analysis", "journal": "", "year": "2011", "authors": "Andrew L Maas; Raymond E Daly; Peter T Pham; Dan Huang; Andrew Y Ng; Christopher Potts"}, {"ref_id": "b38", "title": "Image-based recommendations on styles and substitutes", "journal": "", "year": "2015", "authors": "Julian Mcauley; Christopher Targett; Qinfeng Shi; Anton Van Den;  Hengel"}, {"ref_id": "b39", "title": "Exploring ways to incorporate additional knowledge to improve natural language commonsense question answering", "journal": "", "year": "2020", "authors": "Arindam Mitra; Pratyay Banerjee; Kuntal Kumar Pal; Chitta Swaroop Ranjan Mishra;  Baral"}, {"ref_id": "b40", "title": "Intelligent selection of language model training data", "journal": "", "year": "2010", "authors": "C Robert; William Moore;  Lewis"}, {"ref_id": "b41", "title": "", "journal": "CC-NEWS", "year": "2016", "authors": "Sebastian Nagel"}, {"ref_id": "b42", "title": "Scispacy: Fast and robust models for biomedical natural language processing", "journal": "", "year": "2019", "authors": "Mark Neumann; Daniel King; Iz Beltagy; Waleed Ammar"}, {"ref_id": "b43", "title": "To tune or not to tune? Adapting pretrained representations to diverse tasks", "journal": "", "year": "2019", "authors": "Matthew E Peters; Sebastian Ruder; Noah A Smith"}, {"ref_id": "b44", "title": "Bowman", "journal": "", "year": "2018", "authors": "Jason Phang; Thibault F\u00e9vry; Samuel R "}, {"ref_id": "b45", "title": "What to do about non-standard (or non-canonical) language in NLP", "journal": "", "year": "2016", "authors": "Barbara Plank"}, {"ref_id": "b46", "title": "Improving language understanding by generative pre-training", "journal": "", "year": "2018", "authors": "Alec Radford; Karthik Narasimhan"}, {"ref_id": "b47", "title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "", "year": "2019", "authors": "Colin Raffel; Noam Shazeer; Adam Kaleo Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b48", "title": "Domain-Specific Corpus Expansion with Focused Webcrawling", "journal": "", "year": "2016", "authors": "Steffen Remus; Chris Biemann"}, {"ref_id": "b49", "title": "Towards a continuous modeling of natural language domains", "journal": "", "year": "2016", "authors": "Sebastian Ruder; Parsa Ghaffari; John G Breslin"}, {"ref_id": "b50", "title": "Learning to select data for transfer learning with Bayesian optimization", "journal": "", "year": "2017", "authors": "Sebastian Ruder; Barbara Plank"}, {"ref_id": "b51", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "journal": "", "year": "2019", "authors": "Victor Sanh; Lysandre Debut; Julien Chaumond; Thomas Wolf"}, {"ref_id": "b52", "title": "How to fine-tune BERT for text classification", "journal": "", "year": "2019", "authors": "Chi Sun; Xipeng Qiu; Yige Xu; Xuanjing Huang"}, {"ref_id": "b53", "title": "Shallow syntax in deep water", "journal": "", "year": "2019", "authors": "Swabha Swayamdipta; Matthew Peters; Brendan Roof; Chris Dyer; Noah A Smith"}, {"ref_id": "b54", "title": "Sentiment classification using document embeddings trained with cosine similarity", "journal": "", "year": "2019", "authors": "Tan Thongtan; Tanasanee Phienthrakul"}, {"ref_id": "b55", "title": "A simple method for commonsense reasoning", "journal": "", "year": "2018", "authors": "H Trieu; Quoc V Trinh;  Le"}, {"ref_id": "b56", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b57", "title": "SuperGLUE: A stickier benchmark for general-purpose language understanding systems", "journal": "", "year": "2019", "authors": "Alex Wang; Yada Pruksachatkun; Nikita Nangia; Amanpreet Singh; Julian Michael; Felix Hill; Omer Levy; Samuel R Bowman"}, {"ref_id": "b58", "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding", "journal": "", "year": "2018", "authors": "Alex Wang; Amanpreet Singh; Julian Michael; Felix Hill; Omer Levy; Samuel Bowman"}, {"ref_id": "b59", "title": "What's in a domain? Analyzing genre and topic differences in statistical machine translation", "journal": "", "year": "2015", "authors": "Arianna Marlies Van Der Wees; Wouter Bisazza; Christof Weerkamp;  Monz"}, {"ref_id": "b60", "title": "A broad-coverage challenge corpus for sentence understanding through inference", "journal": "", "year": "2018", "authors": "Adina Williams; Nikita Nangia; Samuel Bowman"}, {"ref_id": "b61", "title": "Morgan Funtowicz, and Jamie Brew. 2019. HuggingFace's Transformers: State-of-the-art natural language processing", "journal": "", "year": "", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; R\u00e9mi Louf"}, {"ref_id": "b62", "title": "Google's neural machine translation system: Bridging the gap between human and machine translation", "journal": "", "year": "2016", "authors": "Yonghui Wu; Mike Schuster; Zhifeng Chen; V Quoc; Mohammad Le; Wolfgang Norouzi; Maxim Macherey; Yuan Krikun; Qin Cao; Klaus Gao;  Macherey"}, {"ref_id": "b63", "title": "BERT post-training for review reading comprehension and aspect-based sentiment analysis", "journal": "", "year": "2019", "authors": "Hu Xu; Bing Liu; Lei Shu; Philip Yu"}, {"ref_id": "b64", "title": "Review conversational reading comprehension", "journal": "", "year": "2019", "authors": "Hu Xu; Bing Liu; Lei Shu; Philip S Yu"}, {"ref_id": "b65", "title": "XLNet: Generalized autoregressive pretraining for language understanding", "journal": "", "year": "2019", "authors": "Zhilin Yang; Zihang Dai; Yiming Yang; Jaime G Carbonell; Ruslan Salakhutdinov; V Quoc;  Le"}, {"ref_id": "b66", "title": "Learning and evaluating general linguistic intelligence", "journal": "", "year": "2019", "authors": "Dani Yogatama; Cyprien De Masson D'autume; Jerome Connor; Tom\u00e1s Kocisk\u00fd; Mike Chrzanowski; Lingpeng Kong; Angeliki Lazaridou; Wang Ling; Lei Yu; Chris Dyer; Phil Blunsom"}, {"ref_id": "b67", "title": "Defending against neural fake news", "journal": "", "year": "2019", "authors": "Rowan Zellers; Ari Holtzman; Hannah Rashkin; Yonatan Bisk; Ali Farhadi; Franziska Roesner; Yejin Choi"}, {"ref_id": "b68", "title": "Character-level convolutional networks for text classification", "journal": "", "year": "2015", "authors": "Xiang Zhang; Junbo Jake Zhao; Yann Lecun"}, {"ref_id": "b69", "title": "Curriculum learning for domain adaptation in neural machine translation", "journal": "", "year": "2019", "authors": "Xuan Zhang; Pamela Shapiro; Gaurav Kumar; Paul Mc-Namee; Marine Carpuat; Kevin Duh"}, {"ref_id": "b70", "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "journal": "", "year": "2015", "authors": "Yukun Zhu; Ryan Kiros; Richard S Zemel; Ruslan Salakhutdinov; Raquel Urtasun; Antonio Torralba; Sanja Fidler"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "or consider simple syntactic structure of the input while adapting to task-specific", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "List of the domain-specific unlabeled datasets. In columns 5 and 6, we report ROBERTA's masked LM loss on 50K randomly sampled held-out documents from each domain before (L ROB. ) and after (L DAPT ) DAPT (lower implies a better fit on the sample). \u2021 indicates that the masked LM loss is estimated on data sampled from sources similar to ROBERTA's pretraining corpus.", "figure_data": "PT100.0 54.134.527.319.2News54.1 100.0 40.024.917.3Reviews34.540.0 100.0 18.312.7BioMed27.324.918.3 100.0 21.4CS19.217.312.721.4 100.0PTNews Reviews BioMedCSFigure 2: Vocabulary overlap (%) between do-mains. PT denotes a sample from sources similar toROBERTA's pretraining corpus. Vocabularies for eachdomain are created by considering the top 10K mostfrequent words (excluding stopwords) in documentssampled from each domain."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": ": Comparison of ROBERTA (ROBA.) andDAPT to adaptation to an irrelevant domain (\u00acDAPT). Reported results are test macro-F 1 , except forCHEMPROT and RCT, for which we report micro-F 1 ,following Beltagy et al. (2019). We report averagesacross five random seeds, with standard deviations assubscripts.  \u2020 indicates high-resource settings. Best taskperformance is boldfaced. See  \u00a73.3 for our choice ofirrelevant domains.ments over ROBERTA, demonstrating the benefitof DAPT when the target domain is more distantfrom ROBERTA's source domain. The pattern isconsistent across high-and low-resource settings.Although DAPT does not increase performance onAGNEWS, the benefit we observe in HYPERPAR-TISAN suggests that DAPT may be useful even fortasks that align more closely with ROBERTA'ssource domain."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "[...]  Three great festive films... The Shop Around the Corner (1940) Delightful Comedy by Ernst Lubitsch stars James Stewart and Margaret Sullavan falling in love at Christmas. Remade as You've Got Mail.[...]    Simply the Best! I've owned countless Droids and iPhones, but this one destroys them all. Samsung really nailed it with this one, extremely fast , very pocketable, gorgeous display , exceptional battery life , good audio quality, perfect GPS & WiFi performance, transparent status bar, battery percentage, ability to turn off soft key lights, superb camera for a smartphone and more![...]    We're living in a world with a new Samsung. [...] more on battery life later[...]  Exposure is usually spot on and focusing is very fast. [...] The design, display, camera and performance are all best in class, and the phone feels smaller than it looks.[...]    ", "figure_data": "HELPFULNESS reviewREALNEWS article"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Examples that illustrate how some domains might have overlaps with others, leading to unexpected positive transfer. We highlight expressions in the reviews that are also found in the REALNEWS articles.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Results on different phases of adaptive pretraining compared to the baseline ROBERTA (col. 1). Our approaches are DAPT (col. 2, \u00a73), TAPT (col. 3, \u00a74), and a combination of both (col. 4). Reported results follow the same format as Table3. State-of-the-art results we can compare to: CHEMPROT (84.", "figure_data": "6), RCT (92.9), ACL-ARC"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "TAPT 79.8 1.4 90.4 5.2 95.5 0.1 DAPT + TAPT 83.0 0.3 90.0 6.6 95.6 0.1 Curated-TAPT 83.4 0.3 89.9 9.5 95.7 0.1 DAPT + Curated-TAPT 83.8 0.5 92.1 3.6 95.8 0.1", "figure_data": "PretrainingBIOMED NEWS REVIEWS RCT-500 HYP. IMDB  \u20205 Augmenting Training Data forTask-Adaptive Pretraining"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Mean test set macro-F 1 (for HYP. and IMDB) and micro-F 1 (for RCT-500), with Curated-TAPT across five random seeds, with standard deviations as subscripts. \u2020 indicates high-resource settings.", "figure_data": ""}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "", "figure_data": ""}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "Computational requirements for adapting to the RCT-500 task, comparing DAPT ( \u00a73) and the various TAPT modifications described in \u00a74 and \u00a75.", "figure_data": ""}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "Summary of strategies for multi-phase pretraining explored in this paper.", "figure_data": ""}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "Overview of prior work across strategies for continued pre-training summarized in Table10. ULMFIT is pretrained on English Wikipedia; ULMFIT \u2020 on English tweets; ELMO on the 1BWORDBENCHMARK (newswire;", "figure_data": ""}, {"figure_label": "20", "figure_type": "table", "figure_id": "tab_19", "figure_caption": "we display additional examples that highlight the overlap between IMDB reviews and REALNEWS articles, relevant for analysis in \u00a73.1.", "figure_data": ""}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_21", "figure_caption": "The lowest masked LM for each domain sample is boldfaced.", "figure_data": ": ROBERTA's (row 1) and domain-adapted ROBERTA's (rows 2-5) masked LM loss on randomly sam-pled held-out documents from each domain (lower implies a better fit). PT denotes a sample from sources similarto ROBERTA's pretraining corpus. Computing InfrastructureGoogle Cloud v3-8 TPUModel implementationshttps://github.com/allenai/tpu_pretrainHyperparameterAssignmentnumber of steps100 epochs (TAPT) or 12.5K steps (DAPT)batch size256 or 2058maximum learning rate0.0001 or 0.0005learning rate optimizerAdamAdam epsilon1e-6Adam beta weights0.9, 0.98learning rate schedulerNone or warmup linearWeight decay0.01Warmup proportion0.06learning rate decaylinear"}, {"figure_label": "13", "figure_type": "table", "figure_id": "tab_22", "figure_caption": "Hyperparameters for domain-and task-adaptive pretraining.", "figure_data": "Computing InfrastructureQuadro RTX 8000 GPUModel implementationhttps://github.com/allenai/dont-stop-pretrainingHyperparameterAssignmentnumber of epochs3 or 10patience3batch size16learning rate2e-5dropout0.1feedforward layer1feedforward nonlinearitytanhclassification layer1"}, {"figure_label": "14", "figure_type": "table", "figure_id": "tab_23", "figure_caption": "Hyperparameters for ROBERTA text classifier.", "figure_data": ""}, {"figure_label": "16", "figure_type": "table", "figure_id": "tab_24", "figure_caption": "Development comparison of ROBERTA (ROBA.) and DAPT to adaptation to an irrelevant domain (\u00ac DAPT). See \u00a73.3 for our choice of irrelevant domains. Reported results follow the same format as Table5.", "figure_data": "BIOMEDRCTCHEMPROTCSACL-ARCSCIERCTAPT88.3 0.183.0 0.6TAPT73.2 3.685.9 0.8Transfer-TAPT 88.0 0.1 (\u2193 0.3)81.1 0.5 (\u2193 1.9)Transfer-TAPT74.0 4.5 (\u2191 1.2)85.5 1.1 (\u2193 0.4)NEWSHYPERPARTISAN AGNEWSAMAZON reviews HELPFULNESS IMDBTAPT82.7 3.394.7 0.1TAPT69.2 2.495.4 0.1Transfer-TAPT 77.6 3.6 (\u2193 5.1)94.4 0.1 (\u2193 0.4)Transfer-TAPT65.4 2.7 (\u2193 3.8)94.9 0.1 (\u2193 0.5)"}, {"figure_label": "17", "figure_type": "table", "figure_id": "tab_25", "figure_caption": "Development results for TAPT transferability.", "figure_data": "PretrainingBIOMEDNEWSREVIEWSRCT-500 HYPERPARTISAN\u2020 IMDBTAPT80.51.382.73.395.40.1DAPT + TAPT83.90.380.82.395.70.2Curated-TAPT84.40.384.91.995.80.1DAPT + Curated-TAPT84.50.383.13.796.00.1"}, {"figure_label": "18", "figure_type": "table", "figure_id": "tab_26", "figure_caption": "Mean development set macro-F 1 (for HYPERPARTISAN and IMDB) and micro-F 1 (for RCT-500), with Curated-TAPT across five random seeds, with standard deviations as subscripts. \u2020 indicates high-resource settings.", "figure_data": "PretrainingBIOMEDCSCHEMPROT RCT-500 ACL-ARCROBERTA83.21.480.30.571.32.8TAPT83.00.680.51.373.23.6RAND-TAPT83.30.581.60.678.74.050NN-TAPT83.30.881.70.570.13.5150NN-TAPT83.30.981.90.878.52.2500NN-TAPT84.50.482.60.477.42.3DAPT84.10.583.50.873.21.5"}, {"figure_label": "20", "figure_type": "table", "figure_id": "tab_27", "figure_caption": "Additional examples that highlight the overlap between IMDB reviews and REALNEWS articles.", "figure_data": ""}], "formulas": [], "doi": "10.1186/1758-2946-7-S1-S2"}