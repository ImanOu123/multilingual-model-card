{"title": "Teaching Language Models to Self-Improve through Interactive Demonstrations", "authors": "Xiao Yu; Baolin Peng; Michel Galley; Jianfeng Gao; Zhou Yu; Rohan Taori; Ishaan Gulrajani; Tianyi Zhang; Yann Dubois; Xuechen Li; Carlos Guestrin; Percy Liang; Tatsunori B Hashimoto;  Stan; Hugo Touvron; Thibaut Lavril; Gautier Izacard; Xavier Martinet; Marie-Anne Lachaux; Timoth\u00e9e Lacroix; Baptiste Rozi\u00e8re; Naman Goyal; Eric Hambro; Faisal Azhar; Aurelien Rodriguez; Armand Joulin; Louis Martin; Kevin Stone; Peter Al- Bert; Amjad Almahairi; Yasmine Babaei; Nikolay Bashlykov; Soumya Batra; Prajjwal Bhargava; Shruti Bhosale; Dan Bikel; Lukas Blecher; Cristian Canton Ferrer; Moya Chen; Guillem Cucurull; David Esiobu; Jude Fernandes; Jeremy Fu; Wenyin Fu; Brian Fuller; Cynthia Gao; Vedanuj Goswami; An- Thony Hartshorn; Saghar Hosseini; Rui Hou; Hakan Inan; Marcin Kardas; Viktor Kerkez; Madian Khabsa; Isabel Kloumann; Artem Korenev; Singh Koura; Jenya Lee; Di- Ana Liskovich; Yinghai Lu; Yuning Mao; Xavier Mar- Tinet; Todor Mihaylov; Pushkar Mishra; Igor Moly- Bog; Yixin Nie; Andrew Poulton; Jeremy Reizen- Stein; Rashi Rungta; Kalyan Saladi; Alan Schelten; Ruan Silva; Eric Michael Smith; Ranjan Subrama- Nian; Ellen Tan; Binh Tang; Ross Tay- Lor; Adina Williams; Jian Xiang Kuan; Puxin Xu; Zheng Yan; Iliyan Zarov; Yuchen Zhang; Angela Fan; Melanie Kambadur; Sharan Narang; Aurelien Ro- Driguez; Robert Stojnic; Sergey Edunov; Qizhou Wang; Feng Liu; Bo Han; Tongliang Liu; Chen Gong; Gang Niu; Mingyuan Zhou; Masashi 2022 Sugiyama;  Probabilistic; Yisen Wang; Difan Zou; Jinfeng Yi; James Bailey; Xingjun Ma; Quanquan 2020 Gu; Jason Wei; Xuezhi Wang; Dale Schuurmans; Maarten Bosma; Brian Ichter; Fei Xia; Ed Chi; Quoc Le; Denny 2023 Zhou", "pub_date": "", "abstract": "The self-improving ability of large language models (LLMs), enabled by prompting them to analyze and revise their own outputs, has garnered significant interest in recent research. However, this ability has been shown to be absent and difficult to learn for smaller models, thus widening the performance gap between state-of-the-art LLMs and more costeffective and faster ones. To reduce this gap, we introduce TRIPOST, a training algorithm that endows smaller models with such selfimprovement ability, and show that our approach can improve LLaMA-7B's performance on math and reasoning tasks by up to 7.13%. In contrast to prior work, we achieve this by using the smaller model to interact with LLMs to collect feedback and improvements on its own generations. We then replay this experience to train the small model. Our experiments on four math and reasoning datasets show that the interactive experience of learning from and correcting its own mistakes is crucial for small models to improve their performance. big-bench tasks and whether chain-of-thought can solve them.", "sections": [{"heading": "Introduction", "text": "Large language models (OpenAI, 2023;Ouyang et al., 2022) together with techniques such as fewshot prompting (Brown et al., 2020) and Chain-of-Thought (CoT) prompting (Wei et al., 2023;Kojima et al., 2023) have been shown to be effective in achieving strong performance on various downstream language tasks. More recently, a new way to adapt LLMs to downstream tasks has captured the attention of many researchers, namely to further enhance the LLM's downstream task performance by asking the LLM to provide feedback on its own generations and then use the feedback to revise its outputs (Bai et al., 2022;Huang et al., 2023;Peng et al., 2023a;Shinn et al., 2023). This process is often called \"self-improvement\", and has proven to * Now at Tencent AI (baolinpeng@global.tencent.com).  be an effective technique to make the LLM's generations more diverse, more precise, or more faithful to a given piece of knowledge (Schick et al., 2022;Madaan et al., 2023;Peng et al., 2023a). However, Saunders et al. (2022); Huang et al. (2023) found that the ability to generate critical feedback or to self-improve is hardly evident in smaller models 1 . Similarly, Ye et al. (2023) found that fine-tuning smaller models (e.g. 7-13B) with self-improvement demonstrations from LLMs can still fail on tasks such as math, reasoning, and factuality. Following these previous works, we performed a similar study on two math and reasoning tasks in Figure 1. We compared the accuracy of the final answer generated by prompting Method Codex (175B) LLaMA+ft (7B) Input Q: ((4 -0 * -1 * 8 + 6) + (0 + 4 + 9 * 5 * -2)) = Attempt:\n(1) This equation can be written as \"(A + B)\" ...\n(1) This equation can be written as \"(A + B)\" ... (2) Let's calculate A=(4 -0 * -1 * 8 + 6)\n(2) Let's calculate A=(4 -0 * -1 * 8 + 6) =(4 -(0 * -1 * 8) + 6) =(4 -(0 * -1 * 8) + 6) =(4 -(0 + 6)) =(4 -(0 + 8) + 6) =(4 -6) =(4 -8 + 6) =-2 =2 (3) Let's calculate B=(0 + 4 + 9 * 5 * -2)\n(3) Let's calculate B=(0 + 4 + 9 * 5 * -2) =(0 + 4 + (9 * 5 * -2)) =(0 + (4 + (9 * 5))) =(0 + 4 + (-90)) =(0 + ( 13)) =-86 =13 (4) Then, the final eq is (A + B) = (-2 + -86) = -88. (4) Then, the final eq is (A + B) = (2 + 13) = 15. (Final response) So the answer is -88.\n(Final response) So the answer is 15.\nTable 1: Training smaller models using self-improvement demonstrations from LLMs can be ineffective, as models of different sizes make different types and amount of mistakes (highlighted in red). Small models can make simple copying errors, while LLMs can make other arithmetic errors, such as not switching plus or minus signs when adding parentheses. See Appendix B for a more quantitative analysis.\na 175B Codex (Chen et al., 2021) to self-improve, with prompting or training a LLaMA-7B model to self-improve using demonstrations from Codex (Ye et al., 2023). In Figure 1, we surprisingly find that smaller models performed worse using prior selfimprovement-related methods than simply training on ground-truth step-by-step rationales (+ft). By comparing the generated solutions from Codex-175B and LLaMA-7B, we find that smaller models, such as LLaMA-7B, not only make more mistakes, but also different types of mistakes compared to an LLM (Table 1 and Appendix B). Due to the smaller model's weaker math and reasoning ability, we believe training on LLM self-improvement demonstrations is less effective, as it forces the smaller model to learn from mistakes not of its own.\nMotivated by this finding, we propose TRIPOST, a training algorithm that can more effectively train a small model to learn from its mistakes, generate feedback, and improve its performance on math and reasoning tasks. TRIPOST is an iterative algorithm consisting of three stages: Interactive Trajectory Editing, Data Post-processing, and Model Training. Similar to the exploration stage in reinforcement learning, TRIPOST first creates improvement demonstrations using the small model to interact with the expert LLMs or relevant Python scripts. Then, TRIPOST postprocesses the collected data by filtering out failed improvement attempts, and then re-balances the dataset to disincentivize the model from trying to self-\"improve\" when it is not needed. Finally, TRIPOST replays the post-process dataset (Andrychowicz et al., 2018;Schaul et al., 2016), and trains the smaller model using weighted supervised learning. TRIPOST repeats entire the process several times. We evaluate our approach on four maths and reasoning datasets from the BIG-Bench Hard (Suzgun et al., 2022) collection, and find that TRIPOST-trained models can use its learned selfimprovement ability to improve their task performance. We also find that TRIPOST-trained models achieve better in-domain and out-of-domain performance than models trained using just the ground truth step-by-step rationales and trained using direct LLM demonstrations (Saunders et al., 2022;Ye et al., 2023). This paper makes the following contributions:\n\u2022 We illustrate how prior work (Saunders et al., 2022;Ye et al., 2023) can be ineffective in training smaller models to self-improve their performance on math and reasoning tasks.\n\u2022 We propose TRIPOST, an iterative training algorithm that trains a smaller language model to learn to self-improve.\n\u2022 We show that TRIPOST-trained models achieve better performance than models trained using ground-truth rationales or using LLM demonstrations on four math and reasoning datasets from BIG-Bench Hard.", "publication_ref": ["b36", "b21", "b17", "b33", "b40", "b25", "b33", "b36", "b17", "b48", "b7", "b48", "b0", "b38", "b36", "b48", "b36", "b48"], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "Approach", "text": "TRIPOST is an algorithm that trains a small language model to self-improve by learning from its own mistakes. Each iteration of TRIPOST consists of three stages. On a high level, we first collect  a set of improving trajectories by using a smaller model M \u03b8 to interact with LLMs. We use M \u03b8 to generate initial attempts and then use a feedback module FBK and an improvement module IMP to edit parts of the M \u03b8 generated attempts. This creates a trajectory that includes attempts generated by the small model, with feedbacks and improvements tailored to the small model's capability (Figure 2). Next, we post-process the collected trajectories by 1) using scripts and other heuristics to filter out failed \"improvement\" attempts; and 2) re-balancing the dataset using both directly correct attempts and the improving trajectories. Finally, we use weighted supervised learning to train a smaller model M \u03b8 using the post-processed data.\nWe provide an overview of our algorithm in Figure 2, and detail each of the three stages in Section 2.2, Section 2.3, and Section 2.4, respectively.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Notation", "text": "We denote the entire attempt from a language model to solve a given question as a trajectory x:\nx = (x att 0 , x fb 1 , x att 1 , x fb 2 , x att 2 , ..., x fb m ),\nwhere x att 0 denotes the initial attempt, and x fb i , x att i denotes the i-th feedback and updated attempt, respectively. Such a trajectory ends when the last feedback x fb m contains the phrase \"the final response is correct\". Therefore, directly correct trajectories take the form of x \u2713 = (x att 0 , x fb 1 ), and self-improving trajectories take the form of\nx SI = (x att 0 , x fb 1 , x att 1 , ..., x fb m )\nwhere m > 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Interactive Trajectory Editing", "text": "In our prior study in Figure 1 and Table 1, we find that it is difficult to elicit a 7B model to perform self-improvement due to its significantly weaker math and reasoning capability compared to LLMs.\nTo address this issue, we use the smaller model M \u03b8 to first generate an initial attempt 2 , and then apply a feedback module FBK and an improvement module IMP to rewrite parts of the M \u03b8 trajectories. Specifically, we first use FBK (prompting text-davinci-003 or using a Python script) to generate a feedback x fb * i based on the first error step it identified for each incorrect attempt. After that, we edit the trajectory by replacing the first feedback that M \u03b8 and FBK disagree on with the FBKgenerated feedback, creating an edited trajectory:\n(x att 0 , ..., x att i\u22121 , x fb * i ).\nFinally, we use our improvement module IMP (prompting Codex) to generate an improved attempt x att * i conditioned on the previous x att i\u22121 and feedback x fb * i , and append it to the trajectory:\nx edited = (x att 0 , ..., x att i\u22121 , x fb * i , x att * i ).\nAs an example, if feedback x fb * i identifies that the first mistake in x att i\u22121 appears in step 3, then step 1-2 in x att i\u22121 is kept untouched, and IMP is used to generate an improved solution by only changing steps \u2265 3. This design is to prevent IMP from re-writing the whole attempt from scratch (e.g., generating the gold solution), which would violate our motivation to create trajectories with feedback and improvements that are incremental and tailored to the small model's capability.\nWe repeat this process, up to a maximum number of iterations, until the last attempt in x edited is correct. Otherwise, we discard x edited that failed to reach the correct answer.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Data Post-processing", "text": "After the interactive trajectory editing step, we have three types of data: 1) gold step-by-step demonstrations x gold for the task, 2) directly correct trajectories x \u2713 generated by M \u03b8 , and 3) edited trajectories x edited created using M \u03b8 , FBK, and IMP.\nTo make training easier, we first split all data into triplets of single-step improvement x imp = (x att i , x fb i , x att i+1 ) if an attempt x att i was incorrect, or into x T = (x att , x fb ) where the attempt is correct and the trajectory ends with x fb containing the phrase \"the final response is correct\". To learn from expert's correction, x att j and x fb j may be the edited x att * j and x fb * j , respectively (see Section 2.2). Next, we filter out some x imp triplets that contain incorrect feedbacks or improvement steps using some rules (see more in Appendix I). Then, we combine x T and filtered x imp into a single dataset, and balance them using a hyperparameter p specifying the proportion of x imp . We find that this parameter is important for the model to learn to improve its attempt only when necessary. This is because we found that training with too many x imp can cause the model to attempt self-improvement even when the last attempt is already correct, thus damaging its performance (see Section 4.2 for more details).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Model Training", "text": "Finally, we use supervised learning (SL) to train a smaller model M \u03b8 on the combined dataset. To promote the model to focus on learning the feedback and improvement steps in x imp , we use a weighted cross-entropy loss. We weight the loss for all the tokens in x T with w = 1.0, but with w > 1.0 for the tokens that belong to x fb i or x att i+1 in single-step improvement triplets x imp . We note that we also experimented with masking x att i (Zheng et al., 2023), but found it to be less effective than weighted SL in our case. See Appendix E for more empirical analysis and discussions on related techniques.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "TRIPOST", "text": "In Figure 2 and Algorithm 1 we summarize our TRIPOST algorithm. For each of the t iterations, we first utilize M \u03b8 to generate its own attempts X, and then use FBK and IMP to generate and create a set of edited trajectories as described in Section 2.2. Next, we process the newly collected trajectories and the gold task demonstrations X gold by first splitting them into a unified format of x imp triplet or x T , and then filtering out erroneous x imp data (Section 2.3). Finally, we create a training dataset D by balancing the number of x imp and x T using a hyperparameter p, and finetune M \u03b8 on D using weighted SL. Unless otherwise specified, we repeat this procedure for t = 3 iterations, and refer to the model trained using TRIPOST with t iterations as TRIPOST(t).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 1 TRIPOST Training Algorithm", "text": "Require: Generative language model M \u03b8 Require: FBK and IMP modules Require: Gold task demonstrations X gold Require: Data buffer B 1: for t iterations do 2:\n// interactive trajectory editing 3:\nGen. trajectories X = {X \u2713 , X \u2717 } with M \u03b8 4:\nAdd correct trajectories X \u2713 to B", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "5:", "text": "for each incorrect trajectory x \u2717 \u2208 X \u2717 do 6:\nUse FBK to generate feedbacks x fb * i 7:\nReplace feedback from x \u2717 with x fb * Train M \u03b8 on D using weighted SL 18: end for", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In this section, we test if our TRIPOST can 1) help distill self-improvement ability into a smaller model M \u03b8 , and 2) help M \u03b8 improve performance on math and reasoning tasks. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dataset and Preprocessing", "text": "We utilize the BIG-Bench (Srivastava et al., 2023) benchmark to evaluate our approach. BIG-Bench is a collection of more than 200 text-based tasks including categories such as traditional NLP, mathematics, commonsense reasoning, and more.\nWe perform experiments on four math and reasoning tasks from the challenging BIG-Bench Hard (Suzgun et al., 2022) collection. We consider two scriptable tasks: Multistep Arithmetic and Word Sorting, where a step-by-step solution (rationale) and a feedback can be generated using a script; and two unscriptable tasks: Date Understanding and Logical Deduction, where we prompt an LLM (Codex/text-davinci-003) to generate feedbacks. We prompt Codex as the IMP module for all tasks.\nFor each task, we first collect a set of gold stepby-step rationales by either scripting a solution for scriptable tasks, or using the CoT prompts from Suzgun et al. (2022) to generate a solution using LLMs. For those LLM-generated rationales, we only keep the correct ones (see Appendix A for more details) for training. Then, to better measure a model's generalization ability, we split each of the 4 tasks further into seen and unseen subtasks. We mainly categorize simpler questions as the seen subtasks to be used for model training. We describe our categorization method in Table 2.", "publication_ref": ["b41"], "figure_ref": [], "table_ref": []}, {"heading": "Models and Baselines", "text": "Models We use LLaMA-7B as M \u03b8 in our main experiments in Table 3. LLaMA (Touvron et al., 2023a) is a collection of foundation language models ranging from 7B to 65B that have shown strong performance compared to GPT-3 (175B) on many benchmarks (Zheng et al., 2023;Taori et al., 2023;Peng et al., 2023b). Due to the cost of training language models, we use the smallest 7B model. For results with LLaMA-2 models, see Appendix D. For training hyperparameters, see Appendix J.\nBaselines We compare TRIPOST training with three baselines: fine-tuning using self-generated, self-consistent rationales (LMSI, Huang et al. (2023)); fine-tuning using only ground truth rationales (ft rationale); and fine-tuning using selfimprovement demonstrations from LLMs (ft SI. demo, similar to Ye et al. (2023)). For better performance, we initialize with the model trained after ft rationale for all methods. Lastly, for a fair comparison, we restrict iterative algorithms such as TRIPOST to only have access to the same amount of input prompts as used to train baselines such as ft rationale. For more implementation details, see Appendix G and Appendix I.", "publication_ref": ["b17", "b48"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Metrics", "text": "To measure task performance, we follow prior studies on Big-Bench (Ho et al., 2023;Huang et al., 2023) and report the accuracy of the final answer extracted from the model's output. For each task, we report the accuracy on the seen subtasks and unseen subtasks, and its overall performance. To measure the model's self-improvement ability, we mainly consider two metrics: 1) how often the model tries to self-improve (SI. Freq.), and 2) how much those of self-improvement attempts contribute to the model's task performance (SI. Contrib.). We measure SI. Freq. as the number of times the model attempted to self-improve divided by the size of the test set, and SI. Contrib. as the number of times those improvement attempts actually reached the correct final answer.   PaLM-540B) to improve on tasks where it can already achieve a reasonable performance. Next, we find ft SI. demo to slightly degrade the model's performance across all tasks, which we believe is due to the capability mismatch between the LLM demonstrator and the small LM learner (Section 1). This forces the small LM to learn from \"advanced\" errors not from its own (Table 1 and Appendix B). Finally, we see that in all tasks, TRIPOST-trained models performs the best in all metrics. In general, we also observe improvement in the performance of TRIPOST-trained models as the number of iterations t increases. 3 We believe this is because, during the process of learning to self-improve, the model also learns to better understand the tasks by learning from its own mistakes Andrychowicz et al., 2018;Lightman et al., 2023). This enables the model to not only generate better initial attempts, but also improve its self-improvement ability.", "publication_ref": ["b16", "b17", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Main Results", "text": "In Table 4, we further explore the contribution of M \u03b8 's self-improvement ability by describing how its overall performance improved. We find that in two out of the four datasets, TRIPOST-trained models generate an more accurate initial attempt than the baselines (denoted as Directly Correct), and in all cases, TRIPOST-trained models had measurable self-improvement contributions in both seen and unseen tasks (cf. Figure 1 and Table A4). This suggests that TRIPOST-training can 1) help the model better understand the tasks and generate better initial attempts, and 2) help distill self-improving ability into the model. We believe that the combination of both factors improve the model's overall performance in Table 3.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": ["tab_3", "tab_3", "tab_1"]}, {"heading": "TRIPOST-auto", "text": "In Table 5, we explore another way of training M \u03b8 with TRIPOST. Instead of re-balancing the training dataset using a fixed p as in Section 3.4, we can simply include all the edited improvement tuples x imp and the directly correct attempts x T generated by M \u03b8 . We denote this method as TRIPOST-auto, as it automatically \"balances\" its training data to be proportional to its current performance, because p can be interpreted as how often the model's attempts were incorrect and needed editing. TRI-POST-auto training included no less x imp compared to TRIPOST (but generally more x T , resulting in p < 0.43), and we find that the model now rarely attempts to self-improve. However, this unexpectedly leads to even better overall performance, especially on unscriptable tasks. We believe this indicates that 1) learning to always generate a useful feedback and the corresponding improvement is    harder than learning to directly generate a correct attempt, and 2) using LLM-generated feedbacks, which covers more error cases than a Python script, is effective in improving a model's performance.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Analysis", "text": "To investigate the factors that can influence how TRIPOST-trained models learned to attempt selfimprovement, we focus our analysis on the Multistep Arithmetic and Logical Deduction datatset.\nWe also mainly study TRIPOST with p = 0.43, which has both a measurable self-improvement contribution and improvement in its task performance (see Table 3 and Table 4) 4 . 4 In practice, we implement p by specifying the ratio of the number of \"self-improvement samples vs. directly correct samples vs. gold samples\". For example, a ratio of 1.5 : 1.0 : 1.0 corresponds to p = 0.43.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Ablation Studies", "text": "We perform ablation studies for each of the three stages in TRIPOST to better understand their contribution to model's overall performance. In Table 6, we report the task accuracy when: interaction between M \u03b8 and LLM is removed, so that M \u03b8 is distilled with purely LLM demonstrations (-interaction); data filtering is removed (-filtering); dataset balancing is changed to using its own performance (+auto-balance); and the weights for SL are changed to be the same for all tokens (weighed SL). We find that all components are important for TRIPOST to work well, and the choice of fixing p presents a trade-off between a model's self-improvement ability and its task performance (notibly, both TRIPOST and TRIPOST-auto improve upon the baselines).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proportion of SI. Training Data", "text": "In Table 7, we investigate how much improvement demonstration (x imp ) is needed to elicit a measurable self-improvement contribution from M \u03b8 . We find that when a large proportion (e.g. p = 0.70) of the training data contains x imp , the model often attempts to self-improve but does not always result in an overall better performance. This is because many of the \"improvement\" attempts result in failures (e.g. changing an already correct attempt to become an incorrect one), and the best performance is achieved typically when p is low. Despite this, we find that for all other cases with p \u2264 0.43, TRI-POST-trained model achieved a better performance than the baseline methods (see Table 4).", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7", "tab_3"]}, {"heading": "Number of TRIPOST Iterations", "text": "In most of our experiments, we trained TRIPOST up to t = 3 iterations. This is because we found that LLMs and our Python scripts start to struggle with generating feedback or improving M \u03b8 attempts after three iterations. In Figure 3, we present how the number of self-improving trajectories collected (x imp , after filtering) changes as TRIPOST iteration increases. We found that as M \u03b8 improves its performance over time, it 1) poses a greater challenge for our FBK module to generate feedback and/or the IMP module to generate improvement, and 2) generates fewer incorrect attempts for TRI-POST to edit. This is especially impactful for Multistep Arithmetic, as our feedback scripts can only consider a fixed number of error types. This also shows that even LLMs can struggle at generating useful feedbacks or correct improvements, which supports our findings in Section 3.5 that learning to generate feedback and improvements may be harder than to directly generate a correct solution.\nLastly, we note that TRIPOST can, in principle, be applied as an online RL algorithm, where one does not restrict the input prompts to be a fixed set as in Section 3. We believe this could be beneficial to improve the model's performance and genearlization ability beyond TRIPOST(t = 3).", "publication_ref": [], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "Related Work", "text": "Prompting LLMs to Self-Improve Recently, many work (Bai et al., 2022;Madaan et al., 2023) have discovered LLM's capability to self-improve by letting it revise its own answer after prompting it to generate feedbacks. Following these work  2023) finds that LLMs can be prompted to act as an \"optimization function\", which can be used to automatically perform prompt engineering. Our work focuses on distilling the self-improvement ability of LLMs into a smaller model, which was initially not capable of selfimprovement (Figure 1).\nTraining LMs to Self-Improve Besides prompting methods, recent work also explored approaches to train a LM to self-improve. LMSI (Huang et al., 2023) trains LMs (e.g., PaLM-540B) with self-generated, self-consistent answers to improve their task performance, yet we found such method ineffective for small LMs.  2022) considered using multiple small LMs to generate feedback and improvement, which also relates to model ensemble methods (Dietterich, 2000). For example, Welleck et al. (2022) trains a \"corrector\" to improve answers generated by a given fixed generator. This method gathers improved attempts by sampling from the generator and pairing highscoring attempts with low-scoring ones. It also does not provide reasonings (e.g., feedbacks) for each improvement.  first trains a feedback model by using a set of predefined rules that perturbs an original solution, and then trains a separate model to generate answers conditioned on the feedback. Our work leverages LLMs to train a single model capable of generating both feedback and improvement, and also does not require any predefined rules (e.g., using LLMs as the FBK module). Saunders et al. (2022); Ye et al. (2023) has attempted to equip a single small model to selfimprove by training on LLM demonstrations, but found that it had little to no effect for small models on math/reasoning tasks. Our work presents analyses of how these previous methods can fail, and proposes TRIPOST that can train a small model to self-improve and achieve better task performance.\nKnowledge Distillation Learning from experts' demonstrations or reasoning (e.g., from GPT-4) has shown to be successful at improving the performance of smaller models in various tasks (Mukherjee et al., 2023;Laskin et al., 2022;Peng et al., 2023b;Ho et al., 2023;Ye et al., 2023;Huang et al., 2023;Jung et al., 2024). Distillation methods (Hinton et al., 2015;Ba and Caruana, 2014) generally train a target model using expert demonstrations unaware of the target model's capability. While TRI-POST also use LLMs to demonstrate generating a feedback or an improvement, these demonstrations are always conditioned on the output of the smaller model. In this view, our approach combines merits from reinforcement learning with knowledge distillation techniques, where small models are distilled with demonstrations that are created by its own exploration augmented by LLMs' supervision.", "publication_ref": ["b25", "b17", "b10", "b44", "b36", "b48", "b27", "b16", "b48", "b17", "b18", "b15", "b1"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Conclusion", "text": "We introduce TRIPOST, a training algorithm that distills the ability to self-improve to a small model and help it achieve better task performance. TRI-POST first creates improving trajectories using interactions between a smaller model and an LLM, then post-process the collected trajectories, and finally train the smaller model to self-improve using weighted SL. We evaluated TRIPOST on four math and reasoning tasks from the Big-Bench Hard collection and found that it can help small models achieve better task performance. In our analysis, we find that 1) the interactive process of learning from and correcting its own mistakes is crucial for small models to learn to self-improve and 2) learning to always generate a useful feedback and a corresponding improvement can be much harder than learning to directly generate a correct answer. These findings suggest that other data formats, beyond the traditional (input, answer) pair, could be better suited for training a language model to solve a downstream task. We believe this also opens new possibilities for future work to leverage LLMs to improve the performance of smaller, faster models.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "Model Sizes In all of our experiments, we used a single A100 and mainly tested TRIPOST on 7B models, the smallest in the LLaMA-1 and LLaMA-2 family (Touvron et al., 2023a,b). However, with the recently introduced flash attention technique (Dao et al., 2022;Dao, 2023) which can be used to reduce memory usage during training, we plan to extend our experiments to use models with more than 7B parameters.\nDatasets We focused our experiments on math and reasoning tasks because 1) prior work (Ye et al., 2023) had found it difficult to train a 7-13B to self-improve on those tasks and 2) measuring performance improvement is more well defined (for example, as compared to creative story writing).\nHowever, we note that as TRIPOST is task agnostic, in theory it can be applied to other tasks such as knowledge-grounded dialogue generation (Yoshino et al., 2023) or dialogue safety (Dinan et al., 2019). We intend to leave this for future work.\nLLM Usage While attempts for some tasks can be parsed and evaluated using a Python script (e.g., multistep arithmetic and word sorting), it quickly becomes unmanageable for tasks where reasonings mostly take the form of free text (e.g., date understanding and logical deduction). Therefore, we use LLMs such as GPT-3 and Codex (and ChatGPT, see Appendix F), which are highly performant at a reasonable cost. Specifically, we mainly use textdavinci-003 as the feedback module and Codex as the improvement module, as we found this to be the most cost-performant configuration in our experiments. However, since the ability of LLMs to generate feedback or improvements is crucial for TRIPOST to collect training data, this presents a trade-off between the cost of using more performant LLMs (e.g., GPT-4) and the training outcome of TRI-POST, for example on harder tasks such as GSM8k (Cobbe et al., 2021). We hope that with advances in making LLMs more available (Zhang et al., 2022a), such a trade-off would diminish.", "publication_ref": ["b9", "b8", "b48", "b49", "b11", "b7", "b50"], "figure_ref": [], "table_ref": []}, {"heading": "Ethical Considerations", "text": "Our work describes an algorithm to improve small models' performance on math and reasoning tasks, by distilling them the ability to self-improve using interaction records with LLMs. Generally, while most algorithms are not designed for unethical usage, there is often potential for abuse in their applications. In our experiments, we apply TRIPOST to four math and reasoning tasks from the Big-Bench Hard collection (Suzgun et al., 2022). However, because training algorithms are typically taskagnostic, it is possible to use them for unethical tasks, such as scamming and generating harmful responses (Welbl et al., 2021;Gehman et al., 2020). We do not condone the use of TRIPOST for any unlawful or morally unjust purposes.\nDenny Zhou, and Jason Wei. 2022. Challenging", "publication_ref": ["b13"], "figure_ref": [], "table_ref": []}, {"heading": "A More Details on Datasets and Preprocessing", "text": "We use four tasks from the Big-Bench Hard collection (Suzgun et al., 2022) for our experiments: multistep arithmetic, word sorting, date understanding, and logical deduction. Since these tasks do not provide ground truth step-by-step rationale, we either generate them using a script (for multistep arithmetic and word sorting), or prompt Codex (Chen et al., 2021) in a few-shot setting using examples from Suzgun et al. (2022). For rationales generated using prompting, we only keep the ones that reached the correct answer and passed a simple consistency check (e.g. for multiple choice questions, we ensure that the final selected choice in the last step appeared in the second last step). We provide example rationales used for each task in Table A8, Table A9, Table A10, and Table A11. Since Big-Bench (Srivastava et al., 2023) did not provide an official training/validation/test split, we generated our own splits with statistics shown in Table A1.  (Suzgun et al., 2022).", "publication_ref": ["b7", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "B Analyzing Errors Made by Codex and LLaMA-7B", "text": "To detail the different type and amount of errors made by an LLM (e.g., Codex) and a smaller model (e.g., LLaMA-7B), we manually examine incorrect attempts generated by the two models in the Multistep Arithmetics dataset. We use Codex with few-shot prompting, and LLaMA-7B after supervised finetuning on ground-truth step-by-step solutions (denoted as LLaMA+ft). We randomly sample 50 generated attempts with incorrect answers, and carefully review each step in those attempts.\nFor each incorrect step, we apply the principle of error-carried-forward and categorize the first error encountered according to Table A2. We present our analysis in Figure A1 and Table A3. Figure A1 shows that calculation errors take up more than 50% of the time for both Codex and the finetuned LLaMA-7B. However, Codex also makes many algebriac errors (such as forgetting to change sign after adding brackets), while LLaMA-7B often hallucinates by adding or deleting terms from previous calculations. Furthermore, Table A3 shows that, compared to the fine-tuned LLaMA-7B, Codex generates longer solutions while producing fewer errors per step. These findings suggest that supervised finetuning a smaller LM (e.g., LLaMA-7B) based on correcting LLM-generated errors may be inefficient, as it forces the smaller model to learn from attempts and mistakes very different from its own (see Section 1 and Appendix C for more details).", "publication_ref": [], "figure_ref": ["fig_1", "fig_1"], "table_ref": ["tab_9", "tab_1", "tab_1"]}, {"heading": "C More Details on the Prior Study", "text": "In the prior study mentioned in Section 1, we experimented with distilling a smaller model (e.g. LLaMA-7B) with self-improvement demonstration using just the LLMs. We found that not only can the smaller model not self-improve by few-shot prompting, they also still fail to do so after training on the LLM self-improvement demonstrations (also discussed in Section 1). In Figure 1 we presented the performance gap between prompting Codex (175B) and finetuning/prompting LLaMA (7B) with self-improvement demonstrations, and in Table A4 we show the detailed numerical results.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": ["tab_3"]}, {"heading": "D Additional Results on LLaMA-2", "text": "In Table A5 we present the results of using the LLaMA-2 7B model (Touvron et al., 2023b) for TRIPOST training. We used the same procedure as testing with the LLaMA-1 model in our main experiments (Section 3), except that we used p = 0.26 across all settings with LLaMA-2 instead of p = 0.43. This is because we found that the LLaMA-2 baseline (ft rationale) achieves almost twice the performance compared to its LLaMA-1 counterpart. As the LLaMA-2 models make fewer mistakes, we decrease p accordingly to prevent TRIPOST from terminating early due to lack of data. In general, Table A5 shows a similar trend as discussed in Section 3 that 1) fine-tuning on LLM demonstrations of self-improvement did not help improve math/reasoning task performance, and 2) TRIPOST can further improve upon the baselines.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5", "tab_5"]}, {"heading": "E Effect of Weighted SL", "text": "Besides balancing the training dataset, we also found it important to use a weighted cross-entropy Error Name", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definition Example", "text": "Calculation Error errors in performing basic arithmetic operations (addition, subtraction, multiplication, division)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "+ 3 = 7", "text": "Algebraic Error errors in algebraic manipulation, such as forgetting to change signs when adding brackets or forgetting the correct order of operations 1 \u2212 2 + 3 = 1 \u2212 (2 + 3)\nCopy Error mis-copying an operand or an operator from previous steps 7 + 1 + (...) = 7 \u2212 1 + (...) Hallucation adding or deleting an operand or an operator from previous steps 7 + (...) = 7 \u2212 1 + (...) Other Error errors that do not fall into the above categories  loss to emphasize learning the improvement-related tokens (x fb i or x att i+1 ) of each training sample. In Table A6, we find that using a weight too low (w = 1.0) can result in the model rarely attempting to self-improve, while using a weight too high (w = 3.0) does not result in better performance. We believe that this has a similar effect of adjusting p in Section 4.2: some incentive is needed for the model to learn to self-improve, while too much emphasis on trying to self-improve can result in a worse performance.\nWhile we also experimented with alternatives such as masking easier tokens (x att i in a single-step improvement triplet), we believe there is a rich set of techniques that can be used to train the model to focus on harder inputs. This includes boosting algorithms (Schapire, 1999;He et al., 2019), automatic loss reweighing methods (Kanai et al., 2023;Wang et al., 2022Wang et al., , 2020, as well as importance-sampling based methods (Katharopoulos and Fleuret, 2019). We leave this for future work as it is orthogonal to our main contributions.", "publication_ref": ["b37", "b14", "b19", "b20"], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "F Prompting Details", "text": "Besides prompting to generate rationales (e.g. for date understanding), we also use prompting to gen-   erate feedbacks and improvements given the initial attempt. For scriptable tasks such as multistep arithmetic and word sorting, we use a script to generate the feedback by first parsing each step in the attempt, and check their correctness/consistency with other steps using a set of predefined rules. This is similar to Welleck et al. (2022), but we also generalize this to unscriptable tasks such as date understanding and logical deduction by few-shot prompting GPT-3 (text-davinci-003) (Brown et al., 2020) and Codex (Chen et al., 2021) to generate feedbacks and improvements. We found that being able to generate useful feedback is critical for gathering successful improvement trajectories, and we discovered that ChatGPT (OpenAI, 2022) is less effective than GPT-3 or Codex in our case. We provide examples of the feedbacks generated for each task in Table A12, and the prompts used to generate feedback or improvements in Table A13, Table A14, Table A15, and Table A16. Note that we used a form-type of prompting for generating feedback because it can more easily ensure that our (formatted) feedback will contain all the elements we need.\nWhen an answer is correct, we manually attach the phrase \"Step 1 to step x is correct, and the final response is also correct.\" as the termination feedback, where \"x\" is the last step number. This termination condition is also used during inference.\nG More Details on Baselines LMSI Huang et al. (2023) proposed LMSI, a method to improve PaLM-540B (Chowdhery et al., 2022) on math and reasoning tasks by training it on self-generated and consistent step-by-step rationales. First, LMSI generates multiple step-bystep solutions using a high temperature (\u03c4 = 1.2).\nThen, LMSI only keeps the answers that are selfconsistent (by majority voting) in the final answer. Finally, LMSI further augments these solutions with mixed formats, such as removing all the intermediate steps and only keep the final answer. To be comparable with other methods in Table 3 that have access to the ground truth answer, we modify the second step to only keep the answers that are correct. In addition, since small models such as LLaMA-7B performed poorly in these tasks without fine-tuning, we perform LMSI after training the model on the collected silver step-by-step solutions in Appendix A.\nft. SI demo Following Ye et al. (2023), ft. SI demo finetunes a model on LLM-generated selfimprovement demonstrations. For all tasks, we experimented with LLMs \u2208 {ChatGPT, Codex} and reported one with better performance (often Codex). In details, we first prompt a LLM (e.g. Codex) to generate an initial attempt, and then reused TRIPOST with the same LLM as the FBK and IMP to generate a feedback and an improvement. For a fair comparison in Table 3, we also balanced the collected data using the same p = 0.43 as with TRIPOST. Finally, train the small LM using (unweighted) SL on the collected data.\nH Running LMSI(t > 1)\nLMSI described in (Huang et al., 2023) was not applied as an iterative algorithm. However, since LMSI training only relies on self-generated and self-consistent answers, it can be ran iteratively similar to TRIPOST. We present this comparison in Table A7, and find that LMSI(t \u2265 1) struggles when the base model (ft rationale) has a weak task performance. We believe this is because LMSI is mainly a self-training algorithm designed for LLMs such as PaLM-540B (Chowdhery et al., 2022), which can often generate correct or near-correct solutions. However, TRIPOST is a training algorithm designed for smaller LMs, where models learns to self-improve from its interaction records with expert LLMs. Table A7: Comparing TRIPOST(t > 1) with LMSI(t > 1). For simplicity, we show total accuracy for each task.", "publication_ref": ["b44", "b7", "b48", "b17"], "figure_ref": [], "table_ref": ["tab_1", "tab_3", "tab_5", "tab_6", "tab_1", "tab_1", "tab_7", "tab_7"]}, {"heading": "I Implementation Details", "text": "We combine techniques from prompting-based selfimprovement (Madaan et al., 2023;Bai et al., 2022) and active learning (Zhang et al., 2022b;Lightman et al., 2023) to collect a set of self-improving trajectories. Specifically, we first either use a script or few-shot prompting (see Appendix F for more details) to gather feedbacks on a given attempt, and then use prompting to generate improvements conditioned on the previous attempt, the feedback, and all the steps in the previous attempt before the first error step (see Tables A13 to A16 for example). This is to ensure that the improved attempt is making modifications on the previous attempt, rather than creating an entirely new attempt.\nTo edit the original attempt given the script/LLM-generated feedback, we 1) find the first x fb * i feedback that differs from the M \u03b8generated feedback x fb i (usually i = 1); 2) replace x fb * i with x fb i ; 3) remove all the attempts, feedback, and improvement after after x fb i from the trajectory. After this, we prompt an LLM in the improvement module IMP to generate an improvement as described above and in Appendix F.\nTo filter out some of the unhelpful feedbacks or incorrectly \"improved\" attempts, we mainly check 1) whether the final attempt reached the correct answer; 2) if there is at least one difference between the previous attempt and the improved attempt; and 3) if the final answer is consistent with the second last step. We only keep the data that have passed all checks. The effect of this filtering is discussed in our ablation studies in Section 4.1.", "publication_ref": ["b25", "b52"], "figure_ref": [], "table_ref": []}, {"heading": "J Model/Training hyperparameters", "text": "In our main experiments, we used a single A100 GPU with DeepSpeed (Rasley et al., 2020) Zero-2 optimization. We used AdamW (Loshchilov and Hutter, 2019) as the optimizer. For each iteration of the TRIPOST algorithm, we train the model for 4 epochs (line 17). We use a linear learning rate schedule with 20% of warmup steps and a peak learning rate of 1e-6. We use a maximum sequence length of 1024 tokens, batch size = 1 and gradient accumulation step = 4. On average, three iterations of TRIPOST take about 12 hours to train.", "publication_ref": ["b35", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Example Question and Generated Rationale for Multistep Arithmetic Q: ((7 --1 + -1 + -4) -(-7 + 7 * 3 * -9)) = Answer: Let's think step by step. Recall that the order of operations in mathematics is as follows:\n(1) Parentheses, (2) exponents, (3) multiplication and division (from left to right), (4) addition and multiplication (from left to right). So, remember to always compute the expressions inside parentheses or brackets first.\n(1) This equation can be written as \"(A -B)\", where A = (7 --1 + -1 + -4) and B = (-7 + 7 * 3 * -9).\n(2) Let's calculate A = (7 --1 + -1 + -4) = (((7 --1) + -1) + -4) = ((8 + -1) + -4) = (7 + -4) = 3.\n(3) Let's calculate B = (-7 + 7 * 3 * -9) = (-7 + ((7 * 3) * -9)) = (-7 + (21 * -9)) = (-7 + -189) = -196. (4) Then, the final equation is (A -B) = (3 --196) = 199. (Final response) So the answer is 199.\nTable A8: Examples of generated rationales for Multistep Arithmetic using Python scripts. Question and prompts are written in gray italics, and the generated step-by-step rationale is shown in black.\nExample Question and Generated Rationale for Word Sorting Q: Sort the following words alphabetically: List: sioux fortescue purloin percept helmsman Attempt: Let's think step by step.\n(1) The first letter: \"sioux\"=\"s\", \"fortescue\"=\"f\" (6), \"purloin\"=\"p\" (16), \"percept\"=\"p\" (16), \"helmsman\"=\"h\" (8) (2) We now have: (6) \"fortescue\" < (8) \"helmsman\" < (16) [\"purloin\" ? \"percept\"] < ( 19) \"sioux\".\n(3) Now sort this subpart [\"purloin\" ? \"percept\"] by looking at their second letters: \"purloin\"=\"u\" (21), \"percept\"=\"e\" (5). (3.1) We now have: (5) \"percept\" < ( 21) \"purloin\". (3.2) Hence, we have \"percept\" < \"purloin\". (4) Hence, we have \"fortescue\" < \"helmsman\" < \"percept\" < \"purloin\" < \"sioux\". (Final response) So the answer is: fortescue helmsman percept purloin sioux.\nTable A9: Examples of generated rationales for Word Sorting using Python scripts. Question and prompts are written in gray italics, and the generated step-by-step rationale is shown in black.\nExample Question and Generated Rationale for Date Understanding Q: Jane scheduled 3 appointments with 5 poeple for tomorrow (Tue, 7/9/1972 (1) If Jane scheduled 3 appointments with 5 people for tomorrow (Tuesday, 7/9/1972), then today's date is Monday, 7/8/1972.\n(2) The date one year ago from today is 7/8/1971. (Final response) So the answer is (B).  (Chen et al., 2021). Question and prompts are written in gray italics, and the generated step-by-step rationale is shown in black.\nExample Question and Generated Rationale for Logical Deduction Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are three books: a white book, a green book, and an orange book. The green book is to the right of the white book. The orange book is the rightmost. Options: (A) The white book is the leftmost. (B) The green book is the leftmost. (C) The orange book is the leftmost. Attempt: Let's think step by step. Let \"??\" represent 0 or more objects, and \"?\" represent exactly 1 object.\n(1) The green book is to the right of the white book: \"(left) ?? white ?? green ?? (right)\".\n(2) The orange book is the rightmost: \"(left) ?? orange (right)\".\n(3) There are in total three books: a white book, a green book, and an orange book. (4) Combining (1) and (2) we get the following ordering: \"(left) ?? white ?? green ?? orange (right)\".\n(5) Combining (3) and (4) we get the following ordering: \"(left) white green orange (right)\". (6) According to this ordering, the leftmost book is the white book. (Final response) So the answer is (A).\nTable A11: Examples of generated rationales for Logical Deduction by prompting Codex (Chen et al., 2021). Question and prompts are written in gray italics, and the generated step-by-step rationale is shown in black.", "publication_ref": ["b7", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Task Example", "text": "Multistep Arithmetic \u2020 Q: ((-8 + -2 * 1 * 3) * (-2 * -1 * 5 -2)) = Answer: Let's think step by step. Recall that the order of operations in mathematics ...\n(1) This equation can be written as \"(A * B)\", where A = (-8 + -2 * 1 * 3) and B = ...\n(2) Let's calculate A = (-8 + -2 * 1 * 3) = ... = -14.\n(3) Let's calculate B = (-2 * -1 * 5 -2) = ... = (2 * 5 -2) = 10. Then, the final equation is (A * B) = (-14 * 10) = -140. (Final response) So the answer is -140. Feedback:\nStep (1) to (2) are correct. In step (3) the part \" (2 * 5 -2) = 10 \" is incorrect. This is because there is a calculation error, since (2 * 5 -2) is not equal to 10. Word Sorting \u2020 Q: Sort the following words alphabetically: List: cole estonia brian redneck coda Answer: Let's think step by step.\n(1) The first letter: \"cole\"=\"c\" (3), \"estonia\"=\"e\" (5), ..., \"coda\"=\"c\" (3).\n(2) We now have: (2) \"brian\" < (3) [\"cole\" ? \"coda\"] < ( 5) \"estonia\" < ( 18) \"redneck\".\n(3) Now sort this subpart [\"cole\" ? \"coda\"] by looking at their second letters: \"cole\"=\"o\" (15), \"coda\"=\"o\" ( 15).\n(3.1) We now have: (15) [\"cole\" ? \"coda\"].\n(3.2) Now sort this subpart [\"cole\" ? \"coda\"] by looking at their third letters: \"cole\"=\"l\" (12), \"coda\"=\"d\" ( 4).\n(3.2.1) We now have: ( 12) \"cole\" < ( 4) \"coda\". ... (Final response) So the answer is: cole coda estonia brian redneck. Feedback:\nStep (1) to (3.2) are correct. In step (3.2.1) the part \" ( 12) \"cole\" < ( 4) \"coda\" \" is incorrect. This is because words are not sorted in ascending order. (1) The motorcycle is the oldest: \"(oldest) motorcycle ?? (newest)\".\n(2) The bus is newer than the tractor: \"(newest) bus ?? tractor ?? (oldest)\".\n(3) There are in total three vehicles: a motorcycle, a bus, and a tractor.\n(4) Combining ( 1) and (2) we get the following ordering: \"(newest) bus ?? tractor ?? (oldest)\"\n(5) Combining ( 3) and ( 4) we get the following ordering: \"(oldest) motorcycle bus (newest) tractor\". (6) According to this ordering, the vehicle that is the newest is the tractor. (Final response) So the answer is (C). Feedback:\nStep (1) to (2) are correct. In step (3) the part \"(newest) bus ?? tractor ?? (oldest)\" is incorrect. This is because it is missing the motorcycle from step (1).\nTable A12: Examples of an incorrect attempt generated by a small model, followed by its feedback generated by either an LLM (Codex or text-davinci-003) or a script (indicated by \u2020 ) for each task. Input question is written in black, a generated attempt is written in blue, and its feedback is written in cyan. Table A13: Prompt used for generating improvement for Multistep Arithmetic. The generated feedback is then formatted as \"Step 1 to step {earliest error step -1} is correct. In step {earliest error step} the part '{error segment}' is incorrect. This is because '{error reason}'.\" In general, we used three-shot prompting. Parts that will be generated are highlighted in blue. Due to limited space, we present one example used for each task. Please refer to our code repository for the full prompt.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "5146", "text": "Prompt Type", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Prompt Example", "text": "Get Improvement Q: Sort the following words alphabetically: List: cosh inferno every foyer coterie mcfadden halifax Answer: Let's think step by step.\n(1) The first letter: \"cosh\"=\"c\" (3), \"inferno\"=\"i\" ( 9), \"every\"=\"e\" (5), \"foyer\"=\"f\" (6), \"coterie\"=\"c\" (3), \"mcfadden\"=\"m\" (13), \"halifax\"=\"h\" ( 8).\n(2) We now have: (3) \"cosh\" ? \"coterie\" < ( 5) \"every\" < ( 6) \"foyer\" < ( 8) \"halifax\" < ( 9) \"inferno\" < ( 13) \"mcfadden\".\n(3) Now sort this subpart [\"cosh\" ? \"coterie\"] by looking at their second letters: \"cosh\"=\"o\" (15), \"coterie\"=\"o\" ( 15).\n(3.1) We now have: (15) [\"cosh\" ? \"coterie\"].\n(3.2) Now sort this subpart [\"cosh\" ? \"coterie\"] by looking at their third letters: \"cosh\"=\"s\" ( 19), \"coterie\"=\"c\" (3).\n(3.3) We now have: (3) \"coterie\" < ( 19) \"cosh\".\n(3.4) Hence, we have \"coterie\" < \"cosh\".\n(4) Hence, we have \"coterie\" < \"cosh\" < \"every\" < \"foyer\" < \"halifax\" < \"inferno\" < \"mcfadden\". (Final response) So the answer is: coterie cosh every foyer halifax inferno mcfadden.\n[END] Feedback:\nStep (1) to (3.1) are correct. In step (3.2) the part \" \"coterie\"=\"c\" (3) \" is incorrect. This is because the third letter of \"coterie\" should be \"t\". Updated Answer: Let's think step by step.\n(1) The first letter: \"cosh\"=\"c\" (3), \"inferno\"=\"i\" (9), \"every\"=\"e\" (5), \"foyer\"=\"f\" (6), \"coterie\"=\"c\" (3), \"mcfadden\"=\"m\" (13), \"halifax\"=\"h\" (8).\n(2) We now have: (3) [\"cosh\" ? \"coterie\"] < (5) \"every\" < (6) \"foyer\" < (8) \"halifax\" < ( 9) \"inferno\" < ( 13) \"mcfadden\".\n(3) Now sort this subpart [\"cosh\" ? \"coterie\"] by looking at their second letters: \"cosh\"=\"o\" (15), \"coterie\"=\"o\" (15).\n(3.1) We now have: (15) [\"cosh\" ? \"coterie\"].\n(3.2) Now sort this subpart [\"cosh\" ? \"coterie\"] by looking at their third letters: \"cosh\"=\"s\" (19), \"coterie\"=\"t\" (20).\n(3.3) We now have: ( 19) \"cosh\" < (20) \"coterie\".\n(3.4) Hence, we have \"cosh\" < \"coterie\".\n(4) Hence, we have \"cosh\" < \"coterie\" < \"every\" < \"foyer\" < \"halifax\" < \"inferno\" < \"mcfadden\". (Final response) So the answer is: cosh coterie every foyer halifax inferno mcfadden.\n[END]\nTable A14: Prompt used for generating improvement for Word Sorting. The generated feedback is then formatted as \"Step 1 to step {earliest error step -1} is correct. In step {earliest error step} the part '{error segment}' is incorrect. This is because '{error reason}'.\" In general, we used three-shot prompting. Parts that will be generated are highlighted in blue. Due to limited space, we present one example used for each task. Please refer to our code repository for the full prompt. Table A15: Prompt used for generating feedback and improvement for Date Understanding. The generated feedback is then formatted as \"Step 1 to step {first error step -1} is correct. In step {first error step} the part '{error part}' is incorrect. This is because '{error reason}'.\" In general, we used three-shot prompting. Parts that will be generated are highlighted in blue. Due to limited space, we present one example used for each task. Please refer to our code repository for the full prompt.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "5148", "text": "Prompt Type", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Prompt Example", "text": "Get Feedback Q: The following paragraphs each describe a set of three objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a branch, there are three birds: a hummingbird, an owl, and a falcon. The falcon is to the right of the owl. The hummingbird is to the left of the owl. Options: (A) The hummingbird is the second from the left. (B) The owl is the second from the left. (C) The falcon is the second from the left. Answer: Let's think step by step. Let \"??\" represents 0 or more objects, and \"?\" represents exactly 1 object.\n(1) The falcon is to the right of the owl: \"(left) ?? owl ?? falcon ?? (right)\".\n(2) The hummingbird is to the left of the owl: \"(left) ?? hummingbird ?? owl ?? (right)\".\n(3) There are in total three birds: a hummingbird, an owl, and a falcon.\n(4) Combining ( 1) and ( 2) we get the following ordering: \"(left) ?? hummingbird ?? owl ?? falcon ?? (right)\".\n(5) Combining ( 3) and ( 4) we get the following ordering: \"(left) hummingbird owl falcon (right)\". (3) There are in total three birds: a hummingbird, an owl, and a falcon.\n(4) Combining ( 1) and ( 2) we get the following ordering: \"(left) ?? hummingbird ?? owl ?? falcon ?? (right)\".\n(5) Combining ( 3) and ( 4) we get the following ordering: \"(left) hummingbird owl falcon (right)\". (6) According to this ordering, the bird that is second from the left is the falcon. (Final response) The falcon is the second from the left. So the answer is (C). Feedback:\nStep (1) to (5) are correct. In step (6) the part \"the bird that is second from the left is the falcon\" is incorrect. This is because it is inconsistent with the ordering in step (5), which established that \"(left) hummingbird owl falcon (right)\". Updated Answer: Let's think step by step. Let \"??\" represents 0 or more objects, and \"?\" represents exactly 1 object.\n(1) The falcon is to the right of the owl: \"(left) ?? owl ?? falcon ?? (right)\".\n(2) The hummingbird is to the left of the owl: \"(left) ?? hummingbird ?? owl ?? (right)\".\n(3) There are in total three birds: a hummingbird, an owl, and a falcon.\n(4) Combining ( 1) and (2) we get the following ordering: \"(left) ?? hummingbird ?? owl ?? falcon ?? (right)\".\n(5) Combining ( 3) and ( 4) we get the following ordering: \"(left) hummingbird owl falcon (right)\". (6) According to this ordering, the bird that is second from the left is the owl. (Final response) The owl is the second from the left. So the answer is (B).\n[END]\nTable A16: Prompt used for generating feedback and improvement for Logical Deduction. The generated feedback is then formatted as \"Step 1 to step {first error step -1} is correct. In step {first error step} the part '{error part}' is incorrect. This is because '{error reason}'.\" In general, we used three-shot prompting. Parts that will be generated are highlighted in blue. Due to limited space, we present one example used for each task. Please refer to our code repository for the full prompt.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "", "journal": "", "year": "2018", "authors": "Marcin Andrychowicz; Filip Wolski; Alex Ray; Jonas Schneider; Rachel Fong; Peter Welinder"}, {"ref_id": "b1", "title": "Do deep nets really need to be deep?", "journal": "", "year": "2014", "authors": "Jimmy Lei; Rich Ba;  Caruana"}, {"ref_id": "b2", "title": "", "journal": "", "year": "", "authors": "Yuntao Bai; Saurav Kadavath; Sandipan Kundu; Amanda Askell; Jackson Kernion; Andy Jones; Anna Chen; Anna Goldie; Azalia Mirhoseini; Cameron Mckinnon; Carol Chen; Catherine Olsson; Christopher Olah; Danny Hernandez; Dawn Drain; Deep Ganguli; Dustin Li; Eli Tran-Johnson; Ethan Perez; Jamie Kerr; Jared Mueller; Jeffrey Ladish; Joshua Landau; Kamal Ndousse; Kamile Lukosuite; Liane Lovitt; Michael Sellitto; Nelson Elhage; Nicholas Schiefer; Noemi Mercado"}, {"ref_id": "b3", "title": "Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners", "journal": "Curran Associates, Inc", "year": "", "authors": "Tom Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared D Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell; Sandhini Agarwal; Ariel Herbert-Voss; Gretchen Krueger; Tom Henighan; Rewon Child; Aditya Ramesh; Daniel Ziegler; Jeffrey Wu; Clemens Winter; Chris Hesse; Mark Chen; Eric Sigler; Mateusz Litwin"}, {"ref_id": "b4", "title": "Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis", "journal": "", "year": "", "authors": "Mark Chen; Jerry Tworek; Heewoo Jun; Qiming Yuan; Henrique Ponde De Oliveira Pinto; Jared Kaplan; Harri Edwards; Yuri Burda; Nicholas Joseph; Greg Brockman; Alex Ray; Raul Puri; Gretchen Krueger; Michael Petrov; Heidy Khlaaf; Girish Sastry; Pamela Mishkin; Brooke Chan; Scott Gray; Nick Ryder; Mikhail Pavlov; Alethea Power; Lukasz Kaiser; Mohammad Bavarian; Clemens Winter"}, {"ref_id": "b5", "title": "", "journal": "Ilya Sutskever", "year": "", "authors": "Jan Carr; Josh Leike; Vedant Achiam; Evan Misra; Alec Morikawa; Matthew Radford; Miles Knight; Mira Brundage; Katie Murati; Peter Mayer; Bob Welinder; Dario Mcgrew; Sam Amodei;  Mccandlish"}, {"ref_id": "b6", "title": "", "journal": "", "year": "", "authors": "Aakanksha Chowdhery; Sharan Narang; Jacob Devlin; Maarten Bosma; Gaurav Mishra; Adam Roberts; Paul Barham;  Hyung Won; Charles Chung; Sebastian Sutton; Parker Gehrmann; Kensen Schuh; Sasha Shi; Joshua Tsvyashchenko; Abhishek Maynez; Parker Rao; Yi Barnes; Noam Tay; Vinodkumar Shazeer; Emily Prabhakaran; Nan Reif; Ben Du; Reiner Hutchinson; James Pope; Jacob Bradbury; Michael Austin; Guy Isard; Pengcheng Gur-Ari; Toju Yin; Anselm Duke; Sanjay Levskaya; Sunipa Ghemawat; Henryk Dev; Xavier Michalewski; Vedant Garcia; Kevin Misra; Liam Robinson; Denny Fedus; Daphne Zhou; David Ippolito; Hyeontaek Luan; Katherine Lim ; Oleksandr Polozov; Zongwei Lee; Xuezhi Zhou; Brennan Wang; Mark Saeta; Orhan Diaz; Michele Firat; Jason Catasta; Kathy Wei; Douglas Meier-Hellstern;  Eck"}, {"ref_id": "b7", "title": "Training verifiers to solve math word problems", "journal": "", "year": "2021", "authors": "Karl Cobbe; Vineet Kosaraju; Mohammad Bavarian; Mark Chen; Heewoo Jun; Lukasz Kaiser; Matthias Plappert; Jerry Tworek; Jacob Hilton; Reiichiro Nakano; Christopher Hesse; John Schulman"}, {"ref_id": "b8", "title": "FlashAttention-2: Faster attention with better parallelism and work partitioning", "journal": "ArXiv", "year": "2023", "authors": "Tri Dao"}, {"ref_id": "b9", "title": "FlashAttention: Fast and memory-efficient exact attention with IO-awareness", "journal": "", "year": "2022", "authors": "Tri Dao; Daniel Y Fu; Stefano Ermon; Atri Rudra; Christopher R\u00e9"}, {"ref_id": "b10", "title": "Ensemble methods in machine learning", "journal": "", "year": "2000", "authors": "G Thomas;  Dietterich"}, {"ref_id": "b11", "title": "Build it break it fix it for dialogue safety: Robustness from adversarial human attack", "journal": "", "year": "2019", "authors": "Emily Dinan; Samuel Humeau; Bharath Chintagunta; Jason Weston"}, {"ref_id": "b12", "title": "Read, revise, repeat: A system demonstration for human-in-the-loop iterative text revision", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Wanyu Du; Myung Zae; Vipul Kim;  Raheja"}, {"ref_id": "b13", "title": "RealToxi-cityPrompts: Evaluating neural toxic degeneration in language models", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Suchin Samuel Gehman; Maarten Gururangan; Yejin Sap; Noah A Choi;  Smith"}, {"ref_id": "b14", "title": "Gradient boosting machine: A survey", "journal": "", "year": "2019", "authors": "Zhiyuan He; Danchen Lin; Thomas Lau; Mike Wu"}, {"ref_id": "b15", "title": "Distilling the knowledge in a neural network", "journal": "", "year": "2015", "authors": "Geoffrey Hinton; Oriol Vinyals; Jeff Dean"}, {"ref_id": "b16", "title": "Large language models are reasoning teachers", "journal": "", "year": "2023", "authors": "Namgyu Ho; Laura Schmid; Se-Young Yun"}, {"ref_id": "b17", "title": "Large language models can self-improve", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Jiaxin Huang; Shixiang Gu; Le Hou; Yuexin Wu; Xuezhi Wang; Hongkun Yu; Jiawei Han"}, {"ref_id": "b18", "title": "Impossible distillation: from low-quality model to high-quality dataset & model for summarization and paraphrasing", "journal": "", "year": "2024", "authors": "Jaehun Jung; Peter West; Liwei Jiang; Faeze Brahman; Ximing Lu; Jillian Fisher; Taylor Sorensen; Yejin Choi"}, {"ref_id": "b19", "title": "One-vs-the-rest loss to focus on important samples in adversarial training", "journal": "", "year": "2023", "authors": "Sekitoshi Kanai; Masanori Shin'ya Yamaguchi; Hiroshi Yamada; Kentaro Takahashi; Yasutoshi Ohno;  Ida"}, {"ref_id": "b20", "title": "Not all samples are created equal: Deep learning with importance sampling", "journal": "", "year": "2019", "authors": "Angelos Katharopoulos; Fran\u00e7ois Fleuret"}, {"ref_id": "b21", "title": "Large language models are zero-shot reasoners", "journal": "", "year": "2023", "authors": "Takeshi Kojima; Shane Shixiang; Machel Gu; Yutaka Reid; Yusuke Matsuo;  Iwasawa"}, {"ref_id": "b22", "title": "Maxime Gazeau, Himanshu Sahni, Satinder Singh, and Volodymyr Mnih. 2022. In-context reinforcement learning with algorithm distillation", "journal": "", "year": "", "authors": "Michael Laskin; Luyu Wang; Junhyuk Oh; Emilio Parisotto; Stephen Spencer; Richie Steigerwald;  Strouse"}, {"ref_id": "b23", "title": "John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Let's verify step by step", "journal": "", "year": "", "authors": "Vineet Hunter Lightman; Yura Kosaraju; Harri Burda;  Edwards"}, {"ref_id": "b24", "title": "Decoupled weight decay regularization", "journal": "", "year": "2019", "authors": "Ilya Loshchilov; Frank Hutter"}, {"ref_id": "b25", "title": "Self-refine: Iterative refinement with self-feedback", "journal": "", "year": "2023", "authors": "Aman Madaan; Niket Tandon; Prakhar Gupta; Skyler Hallinan; Luyu Gao; Sarah Wiegreffe; Uri Alon"}, {"ref_id": "b26", "title": "Think about it! improving defeasible reasoning by first modeling the question scenario", "journal": "", "year": "2021", "authors": "Aman Madaan; Niket Tandon; Dheeraj Rajagopal; Peter Clark; Yiming Yang; Eduard Hovy"}, {"ref_id": "b27", "title": "Orca: Progressive learning from complex explanation traces of gpt-4", "journal": "", "year": "2023", "authors": "Subhabrata Mukherjee; Arindam Mitra; Ganesh Jawahar; Sahaj Agarwal; Hamid Palangi; Ahmed Awadallah"}, {"ref_id": "b28", "title": "OpenAI: Introducing ChatGPT", "journal": "", "year": "2022", "authors": " Openai"}, {"ref_id": "b29", "title": "OpenAI. 2023. GPT-4 technical report", "journal": "", "year": "", "authors": ""}, {"ref_id": "b30", "title": "", "journal": "", "year": "", "authors": "Long Ouyang; Jeff Wu; Xu Jiang; Diogo Almeida; Carroll L Wainwright; Pamela Mishkin; Chong Zhang; Sandhini Agarwal; Katarina Slama"}, {"ref_id": "b31", "title": "Training language models to follow instructions with human feedback", "journal": "", "year": "2022-01", "authors": "Jacob Schulman; Fraser Hilton; Luke Kelton; Maddie Miller; Amanda Simens; Peter Askell; Paul Welinder;  Christiano"}, {"ref_id": "b32", "title": "Refiner: Reasoning feedback on intermediate representations", "journal": "", "year": "2023", "authors": "Debjit Paul; Mete Ismayilzada; Maxime Peyrard; Beatriz Borges; Antoine Bosselut; Robert West; Boi Faltings"}, {"ref_id": "b33", "title": "Check your facts and try again: Improving large language models with external knowledge and automated feedback", "journal": "", "year": "2023", "authors": "Baolin Peng; Michel Galley; Pengcheng He; Hao Cheng; Yujia Xie; Yu Hu; Qiuyuan Huang; Lars Liden; Zhou Yu; Weizhu Chen; Jianfeng Gao"}, {"ref_id": "b34", "title": "Pengcheng He, Michel Galley, and Jianfeng Gao. 2023b. Instruction tuning with gpt-4", "journal": "", "year": "", "authors": "Baolin Peng; Chunyuan Li"}, {"ref_id": "b35", "title": "Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters", "journal": "Association for Computing Machinery", "year": "2020", "authors": "Jeff Rasley; Samyam Rajbhandari; Olatunji Ruwase; Yuxiong He"}, {"ref_id": "b36", "title": "Self-critiquing models for assisting human evaluators", "journal": "", "year": "2022", "authors": "William Saunders; Catherine Yeh; Jeff Wu; Steven Bills; Long Ouyang"}, {"ref_id": "b37", "title": "A brief introduction to boosting", "journal": "Morgan Kaufmann Publishers Inc", "year": "1999", "authors": "Robert E Schapire"}, {"ref_id": "b38", "title": "", "journal": "", "year": "2016", "authors": "Tom Schaul; John Quan; Ioannis Antonoglou; David Silver"}, {"ref_id": "b39", "title": "Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. 2022. Peer: A collaborative language model", "journal": "", "year": "", "authors": "Timo Schick; Jane Dwivedi-Yu; Zhengbao Jiang; Fabio Petroni; Patrick Lewis; Gautier Izacard; Qingfei You"}, {"ref_id": "b40", "title": "Reflexion: Language agents with verbal reinforcement learning", "journal": "", "year": "2023", "authors": "Noah Shinn; Federico Cassano; Beck Labash; Ashwin Gopinath; Karthik Narasimhan; Shunyu Yao"}, {"ref_id": "b41", "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models", "journal": "", "year": "2023", "authors": "Aarohi Srivastava; Abhinav Rastogi; Abhishek Rao; Abu Awal Md Shoeb; Abubakar Abid; Adam Fisch; Adam R Brown; Adam Santoro; Aditya Gupta; Adri\u00e0 Garriga-Alonso"}, {"ref_id": "b42", "title": "", "journal": "", "year": "", "authors": "Mirac Suzgun; Nathan Scales; Nathanael Sch\u00e4rli; Sebastian Gehrmann; Yi Tay;  Hyung Won; Aakanksha Chung; Quoc V Chowdhery; Ed H Le;  Chi"}, {"ref_id": "b43", "title": "Lisa Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang. 2021. Challenges in detoxifying language models", "journal": "Association for Computational Linguistics", "year": "", "authors": "Johannes Welbl; Amelia Glaese; Jonathan Uesato; Sumanth Dathathri; John Mellor"}, {"ref_id": "b44", "title": "Generating sequences by learning to self-correct", "journal": "", "year": "2022", "authors": "Sean Welleck; Ximing Lu; Peter West; Faeze Brahman; Tianxiao Shen; Daniel Khashabi; Yejin Choi"}, {"ref_id": "b45", "title": "", "journal": "", "year": "", "authors": "Chengrun Yang; Xuezhi Wang; Yifeng Lu; Hanxiao Liu; Quoc V Le; Denny Zhou"}, {"ref_id": "b46", "title": "Re3: Generating longer stories with recursive reprompting and revision", "journal": "", "year": "2022", "authors": "Kevin Yang; Yuandong Tian; Nanyun Peng; Dan Klein"}, {"ref_id": "b47", "title": "Graphbased, self-supervised program repair from diagnostic feedback", "journal": "", "year": "2020", "authors": "Michihiro Yasunaga; Percy Liang"}, {"ref_id": "b48", "title": "Selfee: Iterative self-revising llm empowered by selffeedback generation", "journal": "", "year": "2023", "authors": "Seonghyeon Ye; Yongrae Jo; Doyoung Kim; Sungdong Kim; Hyeonbin Hwang; Minjoon Seo"}, {"ref_id": "b49", "title": "Overview of the tenth dialog system technology challenge: Dstc10", "journal": "", "year": "2023", "authors": "Koichiro Yoshino; Yun-Nung Chen; Paul Crook; Satwik Kottur; Jinchao Li; Behnam Hedayatnia; Seungwhan Moon; Zhengcong Fei; Zekang Li; Jinchao Zhang; Yang Feng; Jie Zhou; Seokhwan Kim; Yang Liu; Di Jin; Alexandros Papangelis; Karthik Gopalakrishnan; Dilek Hakkani-Tur; Babak Damavandi; Alborz Geramifard; Chiori Hori; Ankit Shah; Chen Zhang; Haizhou Li; Jo\u00e3o Sedoc; Luis F D'haro; Rafael Banchs; Alexander Rudnicky"}, {"ref_id": "b50", "title": "", "journal": "", "year": "2022", "authors": "Susan Zhang; Stephen Roller; Naman Goyal; Mikel Artetxe; Moya Chen; Shuohui Chen; Christopher Dewan; Mona Diab; Xian Li; Xi Victoria Lin; Todor Mihaylov; Myle Ott; Sam Shleifer; Kurt Shuster; Daniel Simig; Punit Singh Koura; Anjali Sridhar; Tianlu Wang; Luke Zettlemoyer"}, {"ref_id": "b51", "title": "The wisdom of hindsight makes language models better instruction followers", "journal": "", "year": "2023", "authors": "Tianjun Zhang; Fangchen Liu; Justin Wong; Pieter Abbeel; Joseph E Gonzalez"}, {"ref_id": "b52", "title": "A survey of active learning for natural language processing", "journal": "", "year": "2022", "authors": "Zhisong Zhang; Emma Strubell; Eduard Hovy"}, {"ref_id": "b53", "title": "and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena", "journal": "", "year": "", "authors": "Lianmin Zheng; Wei-Lin Chiang; Ying Sheng; Siyuan Zhuang; Zhanghao Wu; Yonghao Zhuang; Zi Lin; Zhuohan Li; Dacheng Li; Eric Xing; Hao Zhang; Joseph E Gonzalez"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Compared to LLMs, smaller models have difficulty performing self-improvement on math or logical tasks, such as Multistep Arithmetics and Logical Deduction from the Big-Bench. +ft: finetuned on groundtruth rationales; +SI. prompt: prompted to perform self-improvement; +ft SI. demo further finetuned +ft on LLM self-improvement demonstrations.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Feedbackfinal answer is correct[END]. Feedback: In step (2) the part \"(4-3) = -1\" is incorrect. This is an calculation error, ...", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "X gold \u222a B into triplets x imp or x T", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "l = {3, 4} \u00d7 d = {2} l = {3, 4} \u00d7 d = {3} and number of operands (l) // l = 3, d = 2 l = {5, 6} \u00d7 d = {2, 3} Word Sorting number of words to sort (l) Q: orange apple banana pear l = {2, 3, ..., 7} l = {8, 9, ..., 16} // l = 4 Date Understanding number of steps to solve (l) Q: Today is 01/02, what's the l = {1, 2} l \u2265 3 date yesterday? // l = 1 Logical Deduction number of options (l) Q: John runs ... Who runs fastest? l = {3, 5} l = {7} Options: (A).. (B).. (C).. // l = 3Table 2: Categorization of the datasets into seen and unseen tasks. seen tasks are chosen to be easier and are used for training. Example questions are abbreviated, for complete examples please refer to Appendix A.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 3 :3Figure 3: Improvement demonstrations become more difficult to collect as TRIPOST iteration increases.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": ",Yang et al. (2022);Peng et al. (2023a);Shinn et al. (2023);Schick et al. (2022);Yang et al. (2023) has utilized such a capability to improve LLM's performance on various tasks. For example,Yang et al. (2022) recursively prompts an LLM to generate a longer story, andMadaan et al. (2023) iteratively prompts an LLM to improve its answers on a wide range of tasks such as sentiment reversal and dialogue response generation. More generally,Yang et al. (", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Many work such as Paul et al. (2023); Welleck et al. (2022); Madaan et al. (2021); Yasunaga and Liang (2020); Du et al. (", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Attempt: ...... Feedback: ...... Update: ...... Feedback: ...... Overview of TRIPOST algorithm. TRIPOST consists of three stages: interactive trajectory editing where we use our FBK and IMP module to edit trajectories generated by a smaller model M \u03b8 ; data post-processing where we filter out erroneous trajectories and create a re-balanced dataset; and model training where we train M \u03b8 using weighted supervised learning on the post-processed dataset.", "figure_data": "b) data post-processingQuestionedited trajectoryoutputloss weight high...Feedback:...Update:...lowfilterpromptQuestion Attempt: Feedback: Update:Question Feedback: Attempt: splitAttemptUpdate: Feedback: Attempt: Feedback: ... Question...Update:...FBKre-balancetraining dataFigure 2:"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "TRIPOST's evaluation results on the four datasets. First, we find LMSI(Huang et al., 2023) to be roughly on-par with ft. rationale only when the performance of the base model (i.e., ft. rationale) is already high on the training questions (the seen subtask). This is understandable, as LMSI was originally designed for LLM (e.g.,", "figure_data": "MethodMultistep Arithmetic  \u2020 seen unseen totalWord Sorting  \u2020 seen unseen totalDate Understanding seen unseen totalLogical Deduction seen unseen totalLMSI10.830.004.33 67.725.5626.83 14.559.0912.99 61.11 20.00 48.10ft rationale39.751.4816.78 73.495.8228.50 33.35 21.21 29.87 62.698.6745.78ft SI. demo29.170.0011.67 53.541.9819.26 27.27 18.18 24.68 54.63 15.00 41.67OursTRIPOST(t = 1) 41.67 TRIPOST(t = 2) 49.58 TRIPOST(t = 3) 52.500.84 1.39 2.5017.17 74.02 20.67 74.02 22.50 77.175.16 7.14 5.9528.23 32.73 13.64 27.27 57.88 22.00 46.52 29.55 35.46 25.00 32.47 58.80 18.00 45.25 29.82 40.00 29.55 37.01 63.89 15.00 48.42"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Overall performance of TRIPOST on four BIG-Bench hard datasets. For each dataset, we train our models on the seen tasks, and evaluate their performance on both seen and unseen tasks. For all TRIPOST runs, we use the same hyperparameters (e.g., p = 0.43). Total accuracy (total) is accuracy weighted based on the number of test samples. \u2020 denotes that the task uses scripted rationale/feedback. Results are averaged over three runs.", "figure_data": "DatasetSI. Contrib. seen unseen totalDirectly Correct Total Acc.Multistep Arithmetic 1.390.281.6720.8322.50Word Sorting1.850.522.3727.4429.82Date Understanding 1.951.293.2533.7637.01Logical Deduction8.230.638.8639.5648.52"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Analyzing how TRIPOST-trained models improved the overall task performance. Total accuracy is first decomposed into attempts that are directly correct (Directly Correct) and attempts with self-improvement (SI. Contrib.). SI. Contrib. is then further decomposed into its accuracy contribution on the seen and unseen subtasks.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Freq SI. Cont. total SI. Freq SI. Cont. total SI. Freq SI. Cont. total SI. Freq SI. Cont. total", "figure_data": "Multistep Arithmetic  \u2020 0.00 SI. TRIPOST(t = 1) Method 0.00 17.171.58Word Sorting  \u2020 0.5228.23Date Understanding 0.00 0.00 27.27Logical Deduction 8.86 2.85 46.52TRIPOST(t = 2) TRIPOST(t = 3)1.33 3.671.11 1.6720.67 22.502.90 4.380.52 2.3729.55 29.821.94 10.380.65 3.2532.47 37.0129.72 23.4211.39 8.8645.25 48.42TRIPOST-auto(t = 1)0.000.0020.000.000.0030.340.000.0032.471.900.6351.27TRIPOST-auto(t = 2)0.000.0023.330.000.0029.550.000.0056.820.630.0055.06TRIPOST-auto(t = 3)0.000.0024.330.000.0030.340.000.0068.830.630.6356.96"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Overall performance of TRIPOST without explicit re-balancing. TRIPOST-auto uses the same training procedure as TRIPOST, except that the proportion of x imp used for training is determined automatically using the model's current task performance.", "figure_data": "MethodMultistep Arithmetic SI. Contrib. Total Acc. SI. Contrib. Total Acc. Logical DeductionTRIPOST -interaction1.67 0.2822.50 11.678.86 0.0048.42 41.67-filtering0.3320.677.5948.27+auto-balance -weighed SL0.00 0.0024.33 21.330.63 1.9056.96 43.67"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "TRIPOST ablation studies.", "figure_data": "DatasetpSelf-Improvement Total Acc. Freq. Contrib.0.05 0.000.0023.170.20 0.000.0024.33Multistep Arithmetic0.43 3.671.6722.500.56 8.612.5020.000.70 18.883.6118.670.05 0.000.0049.370.20 0.630.0052.63Logical Deduction0.43 23.428.8648.420.56 20.257.5945.570.70 59.4931.6445.57"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Varying the proportion of x SI used during TRIPOST training.", "figure_data": ""}, {"figure_label": "A2", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Categorization of errors commonly made by Codex or LLaMA-7B in the Multistep Arithmetics dataset. LMs of different sizes make different types of errors. In the Multistep Arithmetics dataset, more than half of the errors made by Codex or a finetuned LLaMA-7B belong to Calculation Error. However, the second most common error is Arithmetic Error for Codex, and Copy Error for LLaMA-7B.", "figure_data": "51.3%31.6% 9.4% 6.8%55.0%7.6% 9.2% 26.0%Calculation Error Algebraic Error Copy Error Hallucination Other Error(a) Codex(b) LLaMA+ft (7B)Figure A1: Codex LLaMA+ft (7B)Avg. Char per Question113.8102.4Avg. Char per Attempt920.0650.1Percent Steps with Errors31.735.1Table A3: LMs of different sizes make different amount of errors. In the Multistep Arithmetics dataset, Codex makes less errors per step compared to a finetuned LLaMA-7B, while answering longer questions and gen-erating longer solutions.Dataset MethodSI. Contrib. Total Acc.Codex (175B)-31.33MS.A.+ SI. prompting LLaMA+ft (7B) + SI. prompting + ft SI. demo Codex (175B)2.00 -0.00 0.28 -33.33 \u2191 16.78 11.60 \u2193 11.67 \u2193 81.01L.D.+ SI. prompting LLaMA+ft (7B) + SI. prompting + ft SI. demo4.43 -0.00 0.0085.44 \u2191 45.78 43.67 \u2193 41.67 \u2193"}, {"figure_label": "A4", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Compared to LLMs, smaller models have difficulty performing self-improvement (SI.) on mathematical/logical tasks, such as Multistep Arithmetics (MS.A.) and Logical Deduction (L.D.).", "figure_data": ""}, {"figure_label": "A5", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "", "figure_data": ": Using TRIPOST with LLaMA-2 7B model. Overall, LLaMA-2 performs better than its LLaMA-1 counterpart, and TRIPOST further improves LLaMA-2's task performance.DatasetwSelf-Improvement Total Acc. Freq. Contrib.1.0 0.000.0021.33Multistep Arithmetic1.5 3.671.6722.503.0 3.331.3822.001.0 10.131.9043.67Logical Deduction1.5 23.428.8648.423.0 19.629.4946.84"}, {"figure_label": "A6", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Varying the SL weights w used during TRI-POST training.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "x = (x att 0 , x fb 1 , x att 1 , x fb 2 , x att 2 , ..., x fb m ),", "formula_coordinates": [3.0, 97.29, 668.17, 165.42, 15.17]}, {"formula_id": "formula_1", "formula_text": "x SI = (x att 0 , x fb 1 , x att 1 , ..., x fb m )", "formula_coordinates": [3.0, 306.14, 373.59, 129.36, 14.85]}, {"formula_id": "formula_2", "formula_text": "(x att 0 , ..., x att i\u22121 , x fb * i ).", "formula_coordinates": [3.0, 369.58, 633.28, 91.41, 15.17]}, {"formula_id": "formula_3", "formula_text": "x edited = (x att 0 , ..., x att i\u22121 , x fb * i , x att * i ).", "formula_coordinates": [3.0, 334.33, 727.32, 161.91, 15.17]}], "doi": "10.18653/v1/2022.in2writing-1.14"}