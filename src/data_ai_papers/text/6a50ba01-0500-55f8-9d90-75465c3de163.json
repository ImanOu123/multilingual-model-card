{"title": "Large-Scale Behavioral Targeting", "authors": "Ye Chen; Dmitry Pavlov; John F Canny", "pub_date": "", "abstract": "Behavioral targeting (BT) leverages historical user behavior to select the ads most relevant to users to display. The state-of-the-art of BT derives a linear Poisson regression model from fine-grained user behavioral data and predicts click-through rate (CTR) from user history. We designed and implemented a highly scalable and efficient solution to BT using Hadoop MapReduce framework. With our parallel algorithm and the resulting system, we can build above 450 BT-category models from the entire Yahoo's user base within one day, the scale that one can not even imagine with prior systems. Moreover, our approach has yielded 20% CTR lift over the existing production system by leveraging the well-grounded probabilistic model fitted from a much larger training dataset. Specifically, our major contributions include: (1) A MapReduce statistical learning algorithm and implementation that achieve optimal data parallelism, task parallelism, and load balance in spite of the typically skewed distribution of domain data. (2) An in-place feature vector generation algorithm with linear time complexity O(n) regardless of the granularity of sliding target window. (3) An in-memory caching scheme that significantly reduces the number of disk IOs to make large-scale learning practical. (4) Highly efficient data structures and sparse representations of models and data to enable fast model updates. We believe that our work makes significant contributions to solving large-scale machine learning problems of industrial relevance in general. Finally, we report comprehensive experimental results, using industrial proprietary codebase and datasets.", "sections": [{"heading": "INTRODUCTION", "text": "Behavioral targeting (BT) leverages historical user behavior to select the most relevant ads to display. A wellgrounded statistical model of BT predicts click-through rate (CTR) of an ad from user behavior, such as ad clicks and views, page views, search queries and clicks. Behavioral data is intrinsically in large scale (e.g., Yahoo! logged 9 terabytes of display ad data with about 500 billion entries 1 on August, 2008), albeit sparse; particularly ad click is a very rare event (e.g., the population CTR of automotive display ads is about 0.05%). Consequently, to fit a BT predictor with low generalization error requires vast amounts of data (e.g., our experiments showed that the generalization error monotonically decreased as the data size increased to cover all users). In practice, it is also desired to refresh models often and thus to constrain the running time of training, given the dynamic nature of user behavior and the volatility of ads and pages.\nIn this paper we present a scalable and efficient solution to behavioral targeting using Hadoop [1] MapReduce [9] framework. First, we introduce the background of BT for online display advertising. Second, we describe linear Poisson regression, a probabilistic model for count data such as online user behavioral data. We then focus on the design of such grid-based algorithms that can scale to the entire Yahoo! user base, with a special effort to share our experiences in addressing practical challenges during implementation and experiments. Finally, we show the experimental results on some industrial datasets, compared with the existing system in production. The contribution of this work includes the successful experiences at Yahoo! in parallelizing statistical machine learning algorithms to deal effectively with webscale data using MapReduce framework, the theoretical and empirical insights into modeling very large user base.", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "BACKGROUND", "text": "Behavioral targeting is yet another application of modern statistical machine learning methods to online advertising. But unlike other computational advertising techniques, BT does not primarily rely on contextual information such as query (\"sponsored search\") and web page (\"content match\"). Instead, BT learns from past user behavior, especially the implicit feedback (i.e., ad clicks) to match the best ads to users. This makes BT enjoy a broader applicability such as graphical display ads, or at least a valuable user dimension complementary to other contextual advertising techniques.\nIn today's practice, behaviorally targeted advertising inventory comes in the form of some kind of demand-driven taxonomy. Two hierarchical examples are \"Finance, Investment\" and \"Technology, Consumer Electronics, Cellular Telephones\". Within a category of interest, a BT model derives a relevance score for each user from past activity. Should the user appear online during a targeting time window, the ad serving system will qualify this user (to be shown an ad in this category) if the score is above a certain threshold. One de facto measure of relevance is CTR, and the threshold is predetermined in such a way that both a desired level of relevance (measured by the cumulative CTR of a collection of targeted users) and the volume of targeted ad impressions (also called reach) can be achieved. It is obvious that revenue is a function of CTR and reach.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "LINEAR POISSON REGRESSION", "text": "We describe a linear Poisson regression model for behavioral count data [5]. The natural statistical model of count data is the Poisson distribution, and we adopt the linear mean parameterization. Specifically, let y be the observed count of a target event (i.e., ad click or view within a category), \u03bb be the expected count or the mean parameter of the Poisson, w be the weight vector to be estimated, and x be the \"bag-of-words\" representation of feature event counts for a user. The probability density is:\np(y) = \u03bb y exp(\u2212\u03bb) y! , where \u03bb = w x.(1)\nGiven a collection of user behavioral data\nD = {(xi, yi)} n i=1\nwhere n is the number of training examples, we wish to find a weight vector w that maximizes the data log likelihood:\n= X i \" yi log (w xi) \u2212 w xi \u2212 log (yi!) \" ,(2)\nwhere i denotes a user or a training example 2 . Taking the derivative of the log likelihood with respect to wj yields:\n\u2202 \u2202wj = X i \" yi \u03bbi xij \u2212 xij \u00ab ,(3)\nwhere wj is the coefficient and xij is the regressor, respectively, of a feature indexed by j. The log likelihood function is globally concave; therefore the maximum likelihood estimator (MLE) of w can be uniquely found by gradient descent or, more efficiently, by the Newton-Raphson method.\nBetter yet, we adapt a multiplicative recurrence proposed by Lee and Seung [11] to our optimization problem by constraining w \u2265 0. The multiplicative update rule is:\nw j \u2190 wj P i y i \u03bb i xij P i xij , where \u03bbi = w xi.(4)\nThe assumption of non-negative weights is intuitive in that it imposes a positive effect of one-unit change in regressors (feature event counts) on the conditional mean (expected target event count), while yielding an efficient recurrence.\nFor predicting ad CTR of a user i within a target category k, we use the unbiased estimator constructed from the conditional Poisson means of clicks and views respectively, with Laplacian smoothing addressing data sparseness,\nCTR ik = \u03bb click ik + \u03b1 \u03bb view ik + \u03b2 ,(5)\nwhere \u03b1 and \u03b2 are the smoothing constants that can be defined globally for all categories or specifically to each category. Note that \u03b1/\u03b2 gives the default CTR of a \"new user\" 3 without any history, a natural choice is letting \u03b1/\u03b2 be the cumulative CTR of all users in that category (also called population CTR). Since \u03bbi = w xi, the CTR prediction is inherently personalized (one Poisson mean for each user) and dynamic (xi can be updated in real time as event stream comes in). The performance of a BT model is evaluated mainly via click-view ROC curve and, of greater business relevance, the CTR lift at a certain operating point of reach. Finally, we comment on the choice of linear parameterization of the Poisson mean \u03bb = w x (identity link function), instead of the canonical exponential form \u03bb = exp(w x) (log link function). Recall that we assume wj \u2265 0 \u2200j, the identity link is thus possible for Poisson since \u03bb \u2265 0. In model training, it can be readily shown that fitting a Poisson regression with the log link function involves computing exp(w xi) \u2200i in each recurrence, instead of w xi \u2200i in the linear case. This slight efficiency gain by the latter can become non-trivial when model needs to be refreshed very often, while i may iterate over billions of examples. The rationale behind, however, comes more from the practical considerations in online prediction. Keep in mind that an online BT system needs to make ad serving decisions in real time (in the order of millisecond); thus it is generally impractical to score users from scratch (i.e., computing \u03bb = w x from the entire history of x). In practice, expected clicks and views for all users are computed offline a priori (e.g., daily) and updated incrementally online, usually with some form of decay. The simple linear form of the predictor makes incremental scoring possible by maintaining the additivity of the update function. More precisely, given a new event \u2206xj of feature j taking place, the predictor can be updated as:\n\u03bb = \u03bb\u03b4 \u2206t + wj\u2206xj, (6\n)\nwhere \u03bb is the new estimator, \u03bb is the previous one, and \u03b4 \u2206t is an exponential decay with a factor of \u03b4 and \u2206t time elapsed. On the other hand, the exponential form increments the score as:\n\u03bb = exp h (log \u03bb)\u03b4 \u2206t + wj\u2206xj i ,(7)\nwhere the logarithmic and exponential operations are extra computing cost.", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}, {"heading": "LARGE-SCALE IMPLEMENTATION", "text": "In this section we describe the parallel algorithms of fitting the Poisson regression model using Hadoop MapReduce framework. Our focus, however, is to elaborate on various innovations in addressing practical challenges in large-scale learning [7].", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}, {"heading": "Data Reduction and Information Loss", "text": "Most practical learning algorithms with web-scale data are IO-bound and particularly scan-bound. In the context of BT, one needs to preprocess 20-30 terabytes (TB) raw data feeds of display ads and searches (one month's worth). A good design should reduce data size at the earliest opportunity. Data reduction is achieved by projection, aggregation and merging. In the very first scan of raw data, only relevant fields are extracted. We next aggregate event counts of a user (identified by cookie) over a configurable period of time and then merge counts with a same composite key 4 (cookie, time) into a single entry. On the other hand, the data preprocessing step should have minimum information loss and redundancy. For example, the time span of aggregation shall satisfy the time resolution required for model training and evaluation (e.g., next one-minute prediction). This step neither decays counts nor categorizes ads, hence loosely coupled with specific modeling logics. A marginal redundancy is introduced by using (cookie, time) as key, given that examples only need to be identified at the cookie level. In practice, we found this is a good trade-off, since using the composite key reduces the length of an entry and hence scales to very long history user data by alleviating any restrictions on the buffer size of a record. After preprocessing, the data size is reduced to 2-3TB.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Feature Selection and Inverted Indexing", "text": "Behavioral data is heterogeneous and sparse. A datadriven approach is to use granular events as features, such as ad clicks, page views and search queries. With such a fine-grained feature space, feature selection is not only theoretically sound (to overcome the curse of dimensionality), but also practically critical; since for instance the dimensionality of search queries can be unbounded. We adopt a simple frequency-based feature selection method. It does so by counting entity frequency in terms of touching cookies and selecting most frequent entities into our feature space. An entity refers to the name (unique identifier) of an event (e.g., ad id or query). Entity is one level higher than feature since the latter is uniquely identified by a (feature type, entity) pair. For example, a same ad id can be used by an ad click or view feature; similarly, a query term may denote a search feature, an organic or sponsored result click feature. In the context of BT, we consider three types of entities: ad, page, and search. Thus the output of feature selection is three dictionaries (or vocabularies), which collectively (offset by feature types) define an inverted index of features.\nWe select features simply based on frequency instead of other more sophisticated information-theoretic measures such 4 We use composite key here in a logical sense, while physically Hadoop only provides a simple key/value storage architecture. We implemented composite key as a single concatenated key. as mutual information. Not only does the simple frequency counting enjoy computational efficiency; but also because we found empirically frequency-based method works best for sparse data. The joint distribution term in mutual information will be dominated by frequent features anyway. And also, mutual information estimates can be very noisy in sparse data.\nWe found several design tricks worked very well in practice. First, frequency is counted in cookie rather than event occurrence. This effectively imposes a robot filtering mechanism by enforcing one vote for each cookie. Second, to select the most frequent entities we apply a predefined threshold instead of fixing top-N from a sorted frequency table. The threshold is probed a priori given a desired feature dimensionality. But once an optimal threshold is determined, it can be used consistently regardless of the size of training data. The thresholding approach is more statistically sound since more features (thus more model parameters) require more data to fit; and vice versa, more data favors more features in a more expressive model. Third, MapReduce framework enforces sorting on key for input to reducers, which is required for reducers to efficiently fetch relevant partitions. In many cases such as thresholding, however, the logic within reducer does not require input to be sorted. Sorting some data type can be expensive (e.g., arbitrarily long string as cookies). To avoid this unnecessary overhead, an optimization is to swap key and value in mapper, given that (1) key data type is sorting-expensive, while value data type is not; and (2) swapping key and value does not compromise the data needs and computing cost in reducer. The last MapReduce job of feature selection that hashes selected entities into maps (dictionaries) is an example of this optimization. Indeed, our implementation does even better. Once the frequency of an entity is summed, the threshold is applied immediately locally and in-memory; and hence the long tail of the frequency table is cut from the traffic to the last hashing step.", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}, {"heading": "Feature Vector Generation in O(1n)", "text": "For iterative algorithms for optimization commonly used in statistical learning, one needs to scan the training data multiple times. Even for a fast-convergent method such as the multiplicative recurrence in our case, the algorithm requires 15-20 passes of training data to converge the MLE of model parameters. Consequently, great efforts should be made to design a data structure optimized for sequential access, along both the user and feature dimensions; while materializing any data reduction and pre-computing opportunities. Behavioral count data is very sparse by nature, thus a sparse vector representation should also be used. Specifically, an example (xi, yi) consists of two vectors, one for features xi and the other for targets yi 5 . Each vector is represented as a pair of arrays of same length NNZ (number of nonzero entries); one of integer type for feature/target indices and the other of float type for values (float for possible decaying), with a same array index coupling an (index, value) pair. This data structure can be viewed as a flattened adaptation of Yale sparse matrix representation [10]. We further split the representations of features and targets for fast access of both, particularly the tensor product yi \u2297 xi, the major computational cost for model updates.\nWith the inverted index built from feature selection, we reference a feature name by its index in generating examples and onwards. The original feature name can be of any data type with arbitrary length; after indexing the algorithm now efficiently deals with integer index consistently. Several pre-computations are carried out in this step. First, feature counts are further aggregated into a typically larger time window (e.g., one-month feature window and one-day target window); and target counts are aggregated from categorized feature counts. Second, decay counts exponentially over time to account for the freshness of user behavior. Third, realize causal or non-causal approaches to generating examples. The causal approach collects features before targets temporally; while the non-causal approach generates targets and features from a same period of history. Although the causal method seems more intuitive and must be followed in evaluation, the non-causal way has advantages in increasing the number of effective examples for training and hence more suitable for short-term modeling. The data size now, one that will be directly consumed by weight initialization and updates, is 200-300 gigabytes (GB) with binary and compressed storage format.\nIt is generally intractable to use algorithms of time complexity higher than linear O(n) in solving large-scale machine learning problems of industrial relevance. Moreover, unlike traditional complexity analysis, the scalar c of a linear complexity O(cn) must be seriously taken into account when n is easily in the order of billion. BT is a problem of such scale. Our goal in time complexity is O(1n), where n is the number of examples keyed on (cookie, time). Recall that ad click is a very rare event, while it is a target event thus carrying arguably the most valuable information in predicting CTR. The size of a sliding target window should be relatively small for the following reasons. Empirically, a large window (e.g., one-week) effectively discards many (feature, target) co-occurrences given that a typical user session lasts less than one hour. Theoretically, for a large window and hence large target event counts, the assumed Poisson approaches a Gaussian with a same mean and may suffer from overdispersion [3]. In online prediction on the other hand, one typically estimates target counts in a window of several minutes (time interval between successive ad servings). Suppose that the number of sliding windows is t, a na\u00efve algorithm would scan the data t times and thus have a complexity of O(tn). When t increases, O(tn) becomes unacceptable. For example, per-minute sliding over one week for short-term modeling gives 10, 080 windows.\nWe develop an algorithm of O(1n) smoothed complexity [12] for generating examples, regardless of the number of sliding windows t. The essence of the algorithm is the following: (1) cache in memory all inputs of a cookie; (2) sort events by time and hence forming an event stream; (3) precompute the boundaries of feature/target sliding windows; (4) maintain three iterators on the event stream, referencing previous featureBegin, current featureBegin and targetBegin, respectively; (5) use one pair of objects (e.g., TreeMap in Java) to respectively hold the feature and target vectors, but share the object pair for all examples. As the feature and target windows slide forward, advance the iterators accordingly to generate an example for the current window using in-place increment, decrement, and decay. In the worst case, one scan of each of the three iterators is sufficient for generating all examples for the cookie in question. In a typical causal setup, the target window is much smaller than the feature window; hence the smoothed complexity is O(1n). The formalism and schematic of the algorithm are shown in Algorithm 1 and Figure 1, respectively. Note that we let FeatureVector collectively denote the 2-tuple of input feature and target feature vectors.  ", "publication_ref": ["b8", "b1", "b10"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Data-driven Weight Initialization", "text": "Model initialization involves assigning initial weights (coefficients of regressors) by scanning the training set D once. To exploit the sparseness of the problem, one shall use some data-driven approach instead of simply uniformly or randomly assigning weights to all parameters, as many gradientbased algorithms do. A data-driven initialization will drastically speed up weights convergence since, for example, under the multiplicative update rule as Eq. ( 4) those weights with no data support will remain zeros as initialized. We define a unigram(j) as one occurrence of feature j, and a bigram(k, j) as one co-occurrence of target k and feature j. The basic idea is to allocate the weight w kj as normalized co-occurrences of (k, j), i.e., a bigram-based initialization. Here normalization can be performed per example through its total feature counts, and globally through unigram and/or bigram counts. We implement two weight initialization methods under different motivations. The first method uses feature-specific normalization by total feature unigram counts over all examples, motivated by the idea of tf-idf,\nw kj \u2190 P i y ik x ij P j x ij P i xij . (8\n)\nThe second method uses target-specific normalizer involving total unigram and bigram counts, motivated by the highly skewed distribution of traffic over categories,\nw kj \u2190 P i (y ik xij) P i y ik P j \u02c6P i (y ik x ij ) P i x ij \u02dc.(9)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Parallel Multiplicative Recurrence", "text": "We wish to estimate the MLE of a dense weight matrix from a sparse data matrix D. We adopt a highly effective multiplicative update rule arising from non-negative matrix factorization (NMF) [11], given that D contains count data and weights are assumed to be non-negative. More precisely, let W be a d \u00d7 m weight matrix where d is the number of targets and m is the number of features. A dense view of D is a n \u00d7 (d + m) matrix which can be further blocked into a n \u00d7 d target counts matrix Y and a n \u00d7 m feature counts matrix X, i.e., D = [Y X]. The MLE of W can be regarded as the solution to an NMF problem Y \u2248 W X where the quality of factorization is measured by data log likelihood [4]. Since both Y and X are given, we directly apply the multiplicative algorithm in [11] to compute W thus yielding our recurrence in Eq. (4).\nThe computational performance of the multiplicative update in Eq. ( 4) is dominated by counting bigrams (perexample normalized as in the numerator of the multiplicative factor), while the global normalizing unigram counts (the denominator of the multiplicative factor) are pre-computed in feature vector generation. Iterative learning algorithms typically encounter parallelization bottleneck in synchronizing model parameters after each iteration [6]. In solving Poisson MLE in our case, for each iteration the final multiplicative update of the weight vector w k of a target variable k has to be carried out in a single node to output this weight vector. Notice that the number of targets d can be arbitrarily small; and the traffic distribution over targets (categories) is by nature highly skewed. Consequently, an unwise parallel design would suffer from suboptimal task parallelism and poor load balance. Our algorithm successfully addresses the above parallelization challenges as follows.", "publication_ref": ["b9", "b2", "b9", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Scalable Data Structures", "text": "To achieve optimal task parallelism, we represent the weight matrix W as d dense vectors (arrays) of length m, each w k for a target variable k. First, using weight vectors is more scalable in terms of memory footprint than matrix representation. Assume that d = 500 and m = 200, 000, a dense W in float requires 400 megabytes (MB) memory. Reading the entire matrix in memory, as one previous standalone implementation does, is unscalable for clusters of commodity machines. For example, in Yahoo's Hadoop clusters, each node has 8GB RAM which is typically shared by 8 JVM processes and hence 1GB per JVM. The vector representation scales in both target and feature dimensions. A weight vector is read in memory on demand and once at a time; and hence d can be arbitrarily large. The memory footprint of a vector becomes bounded, e.g., a 200MB RAM can hold a vector of 50 million float weights. A three-month worth of Yahoo's behavioral data without feature selection contains features well below 10 million. Second, the weight vector data structure facilitates task parallelism since a node only needs to retrieve those vectors relevant to the data being processed. Third, the dense representation of w k makes the dot product \u03bb ik = w k xi very efficient. Recall that feature vector uses sparse array data structure. Given the relevant w k as a dense array in memory, one loop of the xi sparse array is sufficient for computing the dot product, with a smoothed complexity of O(mx) where mx is the typical NNZ in xi. A dot product of two sparse vectors of high dimensionality is generally inefficient since random access is not in constant time as in dense array. Even a sort-merge implementation would only yield a complexity of O(mw) where mw is the typical NNZ in w k and mx mw < m. The choice of sparse representation for feature vector is thus readily justified by its much higher sparseness than weight vector 6 and the even higher dimensionality of n.", "publication_ref": ["b4"], "figure_ref": [], "table_ref": []}, {"heading": "Fine-grained Parallelization", "text": "For updating the weight matrix W = [w kj ] d\u00d7m iteratively, we distribute the computation of counting bigrams by the composite key (k, j) which defines an entry w kj in W . A na\u00efve alternative is distributing either rows by k or columns by j; both however suffer from typically unbalanced traffics (some k or j dominates the running time) and the overhead of synchronizing bigram(k, j). By distributing (k, j), the algorithm yields an optimal parallelization independent of the characteristics of domain data, with no application-level parallelization needed. Distributing composite keys (k, j) effectively pre-computes total bigram counts of all examples in a fully parallel fashion before synchronizing weight vectors; and thus making the last synchronization step as computationally light-weighted as possible. This, indeed, is the key to a successful parallel implementation of iterative learning algorithms. In our implementation, the weights synchronization along with update only takes less than two minutes. Recall that MapReduce framework only provides a single-key storage architecture. In order to distribute (k, j) keys, we need an efficient function to construct a one-value composite key from two simple keys and to recover the sim-ple keys back when needed. Specifically, we define the following operators for this purpose: (1) bigramKey(k, j) = a long integer obtained by bitwise left-shift 32-bit of k and then bitwise OR by j; (2) k = an integer obtained from the high-order 32-bit of bigramKey(k, j); (3) j = an integer obtained from the low-order 32-bit of bigramKey(k, j).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "In-memory Caching", "text": "The dense weight vector representation is highly scalable, but raises challenges in disk IO. Consider a na\u00efve implementation that reads weight vectors from disk on demand as it sequentially processes examples. Suppose that there are n examples, d targets, and on average each example contains dx targets. File IO generally dominates the running time of large-scale computing. In the worst case of dx = d, the na\u00efve algorithm thus has a complexity of O(dn), which obviously is of no use in practice. We tackle this problem via in-memory caching. Caching weight vectors is, however, not the solution; since a small subset of examples will require all weight vectors sit in memory. The trick is to cache input examples. Now suppose that there are l caches for the n examples. After reading each cache into memory, the algorithm maintains a hash map of (target index, array index). This hash map effectively records all relevant targets for the cached examples, and meanwhile provides constant-time lookup from target index to array index to retrieve target counts. In the worst case of all caches hitting d targets, our algorithm yields a complexity of O(dl), where l n. We argue that caching input data is generally a very sound strategy for grid-based framework. For example, a Hadoop cluster of 2, 000 nodes can distribute 256GB data into 128MB blocks with each node processing only one block on average, and thus l = 1 to 2. In-memory caching is also applied to the output of the first mapper that emits (bigramKey(k, j), bigram(k, j)) pairs for each example i, while aggregating bigram counts into a same bigram key for each cache. This output caching reduces disk writes and network traffic, similar to the function of combiner; while leveraging data locality in memory proactively.\nThe parallel algorithm, data structures, and in-memory caching for multiplicative update are also applied to model initialization. Notice that the multiplicative factor in Eq. (4) has an identical form as the first initialization method in Eq. ( 8), except that the per-example normalizer becomes the expected target counts instead of the total feature unigram counts. We show our parallel design formally in Algorithm 2, and schematically in Figure 2.", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "EXPERIMENTS", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dataset and Parameters", "text": "We conducted a comprehensive set of large-scale experiments using the enormous Yahoo's user behavioral data, to evaluate the prediction accuracy and scalability of our parallel Poisson regression model. In each experiment reported below, the controlled parameters are the ones we found empirically superior, as follows. The training data was collected from a 5-week period of time (2008-09-30 to 2008-11-03) where the first four weeks formed the explanatory variables x and the last week was for generating the response variable y. We used all 512 buckets 7 of user data, which gave above 7 A bucket is a random partition of cookies, where the par- The training examples were generated in a causal fashion; with a target window of size one-day, sliding over a one-week period, and preceded by a 4-week feature window (also sliding along with the target window). We leveraged six types of features: ad clicks and views (sharing a same dictionary of ads), page views (from a dictionary of pages), search queries, algorithmic and sponsored result clicks (sharing a same dictionary of queries). For feature selection, we set the frequency thresholds in terms titioning is done by a hash function of cookie string. Legend: 1. Variables: x for feature counts, y for target counts, \u03bb for expected target counts, w for model weights; 2. Indices: i for example, j for feature, k for target; 3. <key>: distributing by a single key; 4. <key1, key2>: distributing by a composite key. of touching cookies to be: 50,000 for ads, 10,000 for pages, and 20,000 for queries. This gave about 150K features comprised of 40K ads (\u00d72), 40K pages, and 10K queries (\u00d73).\nFor robot filtering, we removed examples with the number of distinct events above 5,000. After model initialization using the second method as described in Section 4.4, we performed 17 iterations of multiplicative updates to converge weights. Model evaluation was done using 32 buckets of data (a 1/16 sample) from the next day (2008-11-04) following the training period. To simulate online prediction, we set the exponential decay ratio \u03b4 = 14-day p 1/2 (i.e., a half-life of 14-day); and a 6-minute latency between a 5-week feature window and a 6-minute target window, sliding over one day.\nThe prediction accuracy is measured by two metrics: (1) the relative CTR lift over a baseline at a certain operating point of view recall (also called reach in ad targeting terminology), where the CTR is normalized by the population CTR to eliminate potential variances across buckets and time; and (2) the area under the click-view ROC curve. A click-view ROC curve plots the click recall vs. the view recall, from the testing examples ranked in descending order of predicted CTR. Each point on the ROC curve gives the precision in a relative sense (click recall/view recall) corresponding to a particular view recall. The higher the area under the ROC curve, the more accurate the predictor; and a random predictor would give a ROC area of 0.5 (a diagonal ROC). Since we built models for 60 major BT categories in one batch, we report average CTR lift weighted by views in each category, and average ROC area weighted by clicks in each category. The scalability of our algorithm is measured by the increase in running time (in hours) with respect to input size. All our experiments were run on a 500-node Hadoop cluster of commodity machines (2\u00d7 Quad Core 2GHz CPU and 8GB RAM).", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}, {"heading": "The Baseline Model", "text": "We compared our results with a baseline model using linear regression, which is a prior solution to BT [8]. The linear regression model has two groups of covariates: intensity and recency. Intensity is an aggregated event count, possibly with decay; while recency is the time elapsed since a user had a event most recently. Both covariate groups contain the same six event types as in our Poisson model; but counts are aggregated into the category being modeled and restricted to that category. The response variable is a binary variable indicating clicking (y = 1) or not (y = 0); thus the regression model predicts the propensity for clicking (similar as CTR but unbounded). The linear regression model was trained with quadratic loss; but constrained on the signs of coefficients based on domain knowledge, i.e., all intensity coefficients are non-negative except for ad views and all recency coefficients are non-positive except for ad views. The model has two components: a long-term model was trained to predict the click propensity for the next day using at least one-month worth of user history; and a shortterm model was trained to predict the click propensity in the next hour using at least one-week worth of data. The final clickability score is the product of the long-term and short-term scores (the independence assumption).", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data Size", "text": "Recall that ad click is a very rare event while carries probably the most important user feedback information. The feature space of granular events, on the other hand, has an extremely high dimensionality. It is therefore desirable to use more data for training. One major objective of our large-scale implementation is to scale up to the entire Yahoo's user data. To evaluate the effect of the size of training data on the prediction and computational performances, we varied input data size from 32 buckets to 512 buckets. The results show, as in Table 1, that as the data size increases, the prediction accuracy increases monotonically, while the run-time grows sub-linearly. One prior implementation contains a nonparallel training routine, and trivially parallelized (data parallelism only) feature vector generation and evaluation routines. For the same batch of 60 BT-category models trained on 256 buckets of data, 50K features, and with only one target window of size one-week (non-sliding), the running time was 29 hours. The majority of the time (over 85%) was spent on the nonparallel weight initialization and updates restricted to a single machine, while feature vector generation and evaluation were distributed across about 100 nodes using Torque and Moab. It only took our fully parallel implementation 7.43 hours, a 4\u00d7 speed-up; even with 150K features and daily sliding target window. It is important to note that the prior implementation was not able to handle as large feature space or sliding window in tractable time primarily because of scalability limitations.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Feature Selection", "text": "When abundant data is available, a high-dimensional feature space may yield better model expressiveness. But as the number of features keeps increasing, the model becomes overfitting. In practice, to find the optimal number of features is largely an empirical effort. This experiment reflects such an effort. We examined different numbers and combinations of features, as summarized in Table 2; and the results are shown in Table 3.  The results show that 150K is the empirically optimal number of features given other parameters controlled as described in Section 5.1. This optimum is primarily a function of the size of training data. A similar study was performed on a 64-bucket training set using the prior nonparallel solution discussed in Section 5.3.1; and we found that 50K features was the optimal point for a 1/8 sample. As shown in the column of queries in Table 2, we controlled the number of queries unchanged in this experiment. This is because we found, from a prior study, that the contribution of query features to CTR prediction is insignificant relative to ads and pages. The run-time results shown in Table 3 confirm that the running time is approximately a constant w.r.t. the dimensionality of feature space. This suggests that our implementation is scalable along the feature dimension, which was made possible by in-memory caching input examples, reading weight vectors on-demand, and computing updates in batch, as discussed in Section 4.5.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2", "tab_3", "tab_2", "tab_3"]}, {"heading": "Feature Vector Generation", "text": "As explained in Section 4.3, one key to a scalable solution to BT is a linear-time algorithm for feature vector generation. We developed such an algorithm by in-place incrementing and decrementing a shared map data structure; and hence typically one scan of the input data suffices for generating all examples. In this experiment, we verified the scalability of the feature vector generation routine, and the prediction performances resulted from different sizes of sliding target window. Over a one-week target period, we generated examples with a sliding target window of sizes 15minute, one-hour, one-day, and one-week, respectively. The results are illustrated in Table 4.\nThe results show that, as the target window size reduces from one-week to 15-minute, the run-time for feature vector generation remains approximately constant; even though the number of active examples increases by 13 folds at the high end relative to the low. Here an active example is defined as the one having at least one ad click or view in any category being modeled. The total run-time does increase since the downstream modeling routines need to process more examples, but at a much lower rate than that of the number ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Stratified Sampling", "text": "The training examples can be categorized into three groups: (1) the ones with ad clicks (in any BT-categoy being modeled), ( 2  The negative examples only impact the denominator in the update formula as in Eq. (4). Since the denominator does not depend on \u03bbi, it can be pre-computed as a normalizer in the multiplicative factor; and then the multiplicative recurrence only needs to iterate over the active examples. Our implementation exploits this sparseness, thus the runtime is only sensitive to the view-only sampling rate. Table 5 shows that a small sampling rate for negative examples (0 or 0.2) combined with a large view-only sampling rate (0.5 or 1) yields superior results, which confirms the argument about different information contents in sub-populations.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "Latency", "text": "In the offline evaluations reported on so far, we placed a 6-minute latency (also called gap) window between a 5-week feature window and a 6-minute target window. Assuming a uniform distribution of a target event over the target window, the expected latency was 9 minute. In other words, we disregarded any event happening during the 9-minute latency window for predicting that particular target event. This was to simulate an online prediction environment where the data pipeline was not real-time after an user event was triggered and before the production scoring system saw that event. However, user activities within the session where an ad is served, especially some task-based information, are considered very relevant [2]. The objective of this experiment is to validate the potential of latency removal. The models were trained in the same way as described in Section 5.1, but evaluated with no gap and a one-minute sliding target window. As the results show, in Table 6, the latencyreduced evaluation yields a significantly higher prediction accuracy than the non real-time setup, by a 17% improvement in CTR lift and a 1.5% edge in ROC area. The ROC curves for the category \"Technology\" before and after latency removal are plotted in Figure 3.  ", "publication_ref": ["b0"], "figure_ref": ["fig_5"], "table_ref": ["tab_7"]}, {"heading": "DISCUSSION", "text": "Behavioral targeting is intrinsically a large-scale machine learning problem from the following perspectives. (1) To fit a BT predictive model with low generalization error and a desired level of statistical confidence requires massive behavioral data, given the sparseness the problem. (2) The dimensionality of feature space for the state-of-the-art BT model is very high. The linear Poisson regression model uses granular events (e.g., individual ad clicks and search queries) as features, with a dimensionality ranging from several hundred thousand to several million. (3) The number of BT models to be built is large. There are over 450 BT-category models for browser and login cookies need to be trained on a regular basis. Furthermore, the solution to training BT models has to be very efficient, because: (1) user interests and behavioral patterns change over time; and (2) cookies and features (e.g., ads and pages) are volatile objects.\nOur grid-based solution to BT successfully addresses the above challenges through a truly scalable, efficient and flexible design and implementation. For example, the existing standalone modeling system could only manage to train 60 BT-category models using about one week end-to-end time. Our solution can build over 450 BT models within one day. The scalability achieved further allows for frequent model refreshes and short-term modeling. Finally, scientific experimentation and breakthroughs in BT requires such a scalable and flexible platform to enable a high speed of innovation.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Determining ad targeting information and/or ad creative information using past search queries", "journal": "", "year": "2004-03-31", "authors": "S Agarwal; P Renaker; A Smith"}, {"ref_id": "b1", "title": "Regression Analysis of Count Data", "journal": "Cambridge University Press", "year": "1998", "authors": "A C Cameron; P K Trivedi"}, {"ref_id": "b2", "title": "GaP: a factor model for discrete data", "journal": "", "year": "2004", "authors": "J Canny"}, {"ref_id": "b3", "title": "Granular data for behavioral targeting", "journal": "", "year": null, "authors": "J Canny; S Zhong; S Gaffney; C Brower; P Berkhin; G H John"}, {"ref_id": "b4", "title": "Scalable collaborative filtering algorithms for mining social networks", "journal": "", "year": "2008", "authors": "E Chang"}, {"ref_id": "b5", "title": "Large-scale behavioral targeting for advertising over a network", "journal": "", "year": "2009-01-09", "authors": "Y Chen; D Pavlov; P Berkhin; J Canny"}, {"ref_id": "b6", "title": "Model for generating user profiles in a behavioral targeting system", "journal": "Patent", "year": "2006-03-29", "authors": "C Y Chung; J M Koran; L.-J Lin; H Yin"}, {"ref_id": "b7", "title": "Mapreduce: Simplified data processing on large clusters", "journal": "Communications of the ACM", "year": "2008", "authors": "J Dean; S Ghemawat"}, {"ref_id": "b8", "title": "A comparison of several bandwidth and profile reduction algorithms", "journal": "ACM Transactions on Mathematical Software (TOMS)", "year": "1976", "authors": "N E Gibbs; W G Poole; Jr ; P K Stockmeyer"}, {"ref_id": "b9", "title": "Algorithms for non-negative matrix factorization", "journal": "", "year": "2000", "authors": "D D Lee; H S Seung"}, {"ref_id": "b10", "title": "Smoothed analysis of algorithms: Why the simplex algorithm usually takes polynomial time", "journal": "Journal of the ACM", "year": "2004", "authors": "D A Spielman; S.-H Teng"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "2Depending on the approach to generating training examples (xi, yi), one user could contribute to multiple examples.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Feature vector generation in O(1n)", "figure_data": ""}, {"figure_label": "2235678441", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Algorithm 2 : 2 begin 3 return 5 Function 6 begin 7 if outputCacheSize \u2265 upperBound then 8 outputP i x ij ; 44 L 12235678441Parallel multiplicative recurrence Input: cookieIndex, FeatureVector Output: updated w k , \u2200k MapReduce 1: PoissonMultBigram; 1 Function: bigramKey(k, j) a long by bitwise-left-shift 32-bit k and 4 bitwise-OR j; end : cacheOutput(key, value) i \u2208 inputCache do 17 \u03bb ik \u2190 w k x i ; 18 y ik \u2190 y ik /\u03bb ik ; 19 cacheOutput(bigramKey(k, j),y ik x ij ), \u2200k, j; hash map of targetIndex, 30 targetArrayIndex ; randomized feature/target partitioning if specified; 31 end 32 Mapper \u2192 bigramKey(k, j), y ik x ij ; by k (routing entries with a same target to a 41 single reducer); Reducer \u2192 w k ; 42 begin 43 w kj \u2190 w kj P i (yikxij/\u03bbik) -norm or L 2 -norm regularization if specified; 45 end 46 500 millions training examples and approximately 3TB preprocessed and compressed data.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 2 :2Figure 2: Parallel multiplicative recurrence", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 3 :3Figure 3: ROC plots before and after latency removal for the \"Technology\" category", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Feature vector generation /* We denote a datum in MapReduce as key, value , use ':' as field delimiter within key or value, and '...' for repetition of foregoing fields. */", "figure_data": "1Data structure: FeatureVector2begin/* Array notation: dataType[arrayLength]*/3int[targetLength] targetIndexArray;4float[targetLengh] targetValueArray;5int[inputLength] inputIndexArray;6float[inputLength] inputValueArray;7endInput: cookie:timePeriod,featureType:featureName:featureCount...Output: cookieIndex, FeatureVector8MapReduce: PoissonFeatureVector;9Mapper \u2192 cookie,timePeriod:featureType:featureName:featureCount... ;10Reducer \u2192 cookieIndex, FeatureVector ;11begin/* For a cookie*/12compute boundaries of t pairs of feature/target windows;13cache events and sort values by timePeriod;14initialize iterators and TreeMaps;/* Slide window forward*/15for i \u2190 1 to t do16advance prevFeatureBegin to decrement featurecounts in-place;17decay feature counts incrementally;18advance currFeatureBegin to increment featurecounts in-place;advance currTargetBegin to increment target counts;"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": "The Effect of Training Data Size# Buckets3264128256512CTR lift0.1583 0.2003 0.2287 0.2482 0.2598ROC area0.8193 0.8216 0.8234 0.8253 0.8267Run-time2.953.786.957.4314.07"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "The Parameters of Feature Selection", "figure_data": "Total number Ads (\u00d72) Pages Queries (\u00d73)60K10K10K10K90K20K20K10K150K40K40K10K270K80K80K10K1.2M100K1M10K"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ": The Effect of Feature Dimensionality# Features60K90K150K 270K 1.2MCTR lift0.2197 0.2420 0.2598 0.2584 0.2527ROC area0.8257 0.8258 0.8267 0.8267 0.8261Run-time14.8713.5214.0713.0816.42"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Linear-time Feature Vector Generation Size of tgt. win.", "figure_data": "15-min 1-hour 1-day 1-weekCTR lift0.18290.2266 0.2598 \u22120.0086ROC area0.80310.8145 0.82670.7858Act. ex. (10 6 )2, 1761, 469535158Run-time (fv-gen)1.51.571.431.38Run-time (total)31.0327.3714.079.23of examples increasing. As for prediction accuracy, one-daysliding gives the best CTR lift and ROC area."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": ") the ones with zero ad clicks but nonzero ad views, and (3) the ones with neither ad clicks nor views, or so-called negative examples. It is plausible that the first group carries the most valuable information for predicting CTR, followed by the second group and then the third. It has computational advantages to sample less important examples. In this experiment, we tested different stratified sampling schemes, where all nonzero-click examples were kept, view-only and negative examples were sampled independently at different rates. The results are summarized in Table5.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "", "figure_data": "Stratified SamplingSampling ratesCTR lift ROC area Run-timeneg = 0; view = 10.25980.826714.07neg = 0.2; view = 10.27350.824312.77neg = 0.5; view = 10.26120.820813neg = 1; view = 10.24380.816211.88neg = 0; view = 0.50.25790.82808.9neg = 0; view = 0.20.24620.82667.57neg = 0; view = 0\u22120.03280.77365.38"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "The Effect of Latency Removal", "figure_data": "Latency6-min gap 6-min target 1-min target no-gapCTR lift0.25980.4295ROC area0.82670.8413"}], "formulas": [{"formula_id": "formula_0", "formula_text": "p(y) = \u03bb y exp(\u2212\u03bb) y! , where \u03bb = w x.(1)", "formula_coordinates": [2.0, 99.78, 463.7, 193.13, 21.52]}, {"formula_id": "formula_1", "formula_text": "D = {(xi, yi)} n i=1", "formula_coordinates": [2.0, 223.41, 493.65, 68.99, 10.72]}, {"formula_id": "formula_2", "formula_text": "= X i \" yi log (w xi) \u2212 w xi \u2212 log (yi!) \" ,(2)", "formula_coordinates": [2.0, 93.02, 533.79, 199.89, 21.86]}, {"formula_id": "formula_3", "formula_text": "\u2202 \u2202wj = X i \" yi \u03bbi xij \u2212 xij \u00ab ,(3)", "formula_coordinates": [2.0, 120.89, 594.91, 172.02, 24.55]}, {"formula_id": "formula_4", "formula_text": "w j \u2190 wj P i y i \u03bb i xij P i xij , where \u03bbi = w xi.(4)", "formula_coordinates": [2.0, 360.94, 83.58, 194.97, 26.0]}, {"formula_id": "formula_5", "formula_text": "CTR ik = \u03bb click ik + \u03b1 \u03bb view ik + \u03b2 ,(5)", "formula_coordinates": [2.0, 395.37, 207.78, 160.55, 23.66]}, {"formula_id": "formula_6", "formula_text": "\u03bb = \u03bb\u03b4 \u2206t + wj\u2206xj, (6", "formula_coordinates": [2.0, 396.06, 600.88, 155.94, 10.13]}, {"formula_id": "formula_7", "formula_text": ")", "formula_coordinates": [2.0, 552.0, 603.14, 3.92, 7.86]}, {"formula_id": "formula_8", "formula_text": "\u03bb = exp h (log \u03bb)\u03b4 \u2206t + wj\u2206xj i ,(7)", "formula_coordinates": [2.0, 372.83, 667.37, 183.09, 12.05]}, {"formula_id": "formula_9", "formula_text": "w kj \u2190 P i y ik x ij P j x ij P i xij . (8", "formula_coordinates": [5.0, 133.95, 271.33, 155.03, 28.86]}, {"formula_id": "formula_10", "formula_text": ")", "formula_coordinates": [5.0, 288.99, 284.08, 3.92, 7.86]}, {"formula_id": "formula_11", "formula_text": "w kj \u2190 P i (y ik xij) P i y ik P j \u02c6P i (y ik x ij ) P i x ij \u02dc.(9)", "formula_coordinates": [5.0, 104.5, 339.73, 188.41, 24.01]}], "doi": ""}