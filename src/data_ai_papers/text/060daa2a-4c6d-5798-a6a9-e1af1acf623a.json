{"title": "Mind Your Outliers! Investigating the Negative Impact of Outliers on Active Learning for Visual Question Answering", "authors": "Siddharth Karamcheti Ranjay; Krishna Li; Fei-Fei Christopher; D Manning", "pub_date": "", "abstract": "Active learning promises to alleviate the massive data needs of supervised machine learning: it has successfully improved sample efficiency by an order of magnitude on traditional tasks like topic classification and object recognition. However, we uncover a striking contrast to this promise: across 5 models and 4 datasets on the task of visual question answering, a wide variety of active learning approaches fail to outperform random selection. To understand this discrepancy, we profile 8 active learning methods on a per-example basis, and identify the problem as collective outliers -groups of examples that active learning methods prefer to acquire but models fail to learn (e.g., questions that ask about text in images or require external knowledge). Through systematic ablation experiments and qualitative visualizations, we verify that collective outliers are a general phenomenon responsible for degrading pool-based active learning. Notably, we show that active learning sample efficiency increases significantly as the number of collective outliers in the active learning pool decreases. We conclude with a discussion and prescriptive recommendations for mitigating the effects of these outliers in future work.", "sections": [{"heading": "Introduction", "text": "Today, language-equipped vision systems such as VizWiz, TapTapSee, BeMyEyes, and CamFind are actively being deployed across a broad spectrum of users. 1 As underlying methods improve, these systems will be expected to operate over diverse visual environments and understand myriad language inputs (Bigham et al., 2010;Tellex et al., 2011;Mei et al., 2016;Anderson et al., 2018b;Park et al., 2019). Visual Question Answering (VQA), the task of answering questions about Figure 1: We systematically evaluate active learning on VQA datasets and isolate their inability to perform better than random sampling due to the presence of collective outliers. Active learning methods prefer to acquire these outliers, which are hard and often impossible for models to learn. We show that Dataset Maps, like the one shown here, can heuristically identify these collective outliers as examples assigned low model confidence and prediction variability during training. visual inputs, is a popular benchmark used to evaluate progress towards such open-ended systems (Agrawal et al., 2015;Krishna et al., 2017;Gordon et al., 2018;Hudson and Manning, 2019). Unfortunately, today's VQA models are data hungry: Their performance scales monotonically with more train-ing data (Lu et al., 2016;Lin and Parikh, 2017), motivating the need for data acquisition mechanisms such as active learning, which maximize performance while minimizing expensive data labeling.\nWhile active learning is often key to effective data acquisition when such labeled data is difficult to obtain (Lewis and Catlett, 1994;Tong and Koller, 2001;Culotta and McCallum, 2005;Settles, 2009), we find that 8 modern active learning methods Siddhant and Lipton, 2018;Lowell et al., 2019) show little to no improvement in sample efficiency across 5 models on 4 VQA datasets -indeed, in some cases performing worse than randomly selecting data to label. This finding is in stark contrast to the successful application of active learning methods on a variety of traditional tasks, such as topic classification (Siddhant and Lipton, 2018;Lowell et al., 2019), object recognition (Deng et al., 2018), digit classification , and named entity recognition (Shen et al., 2017). Our negative results hold even when accounting for common active learning ailments: cold starts, correlated sampling, and uncalibrated uncertainty. We mitigate the cold start challenge of needing a representative initial dataset by varying the size of the seed set in our experiments. We account for sampling correlated data within a given batch by including Core-Set selection (Sener and Savarese, 2018) in the set of active learning methods we evaluate. Finally, we use deep Bayesian active learning to calibrate model uncertainty to high-dimensional data (Houlsby et al., 2011;Gal and Ghahramani, 2016;.\nAfter concluding that negative results are consistent across all experimental conditions, we investigate active learning's ineffectiveness on VQA as a data problem and identify the existence of collective outliers (Han and Kamber, 2000) as the source of the problem. Leveraging recent advances in model interpretability, we build Dataset Maps , which distinguish between collective outliers and useful data that improve validation set performance (see Figure 1). While global outliers deviate from the rest of the data and are often a consequence of labeling error, collective outliers cluster together; they may not individually be identifiable as outliers but collectively deviate from other examples in the dataset. For instance, VQA-2 (Goyal et al., 2017) is riddled with collections of hard questions that require external knowledge to answer (e.g., \"What is the symbol on the hood often associated with?\") or that ask the model to read text in the images (e.g., \"What is the word on the wall?\"). Similarly, GQA (Hudson and Manning, 2019) asks underspecified questions (e.g., \"what is the person wearing?\" which can have multiple correct answers). Collective outliers are not specific to VQA, but can similarly be found in many open-ended tasks, including visual navigation (Anderson et al., 2018b) (e.g., \"Go to the grandfather clock\" requires identifying rare grandfather clocks), and open-domain question answering (Kwiatkowski et al., 2019), amongst others.\nUsing Dataset Maps, we profile active learning methods and show that they prefer acquiring collective outliers that models are unable to learn, explaining their poor improvements in sample efficiency relative to random sampling. Building on this, we use these maps to perform ablations where we identify and remove outliers iteratively from the active learning pool, observing correlated improvements in sample efficiency. This allows us to conclude that collective outliers are, indeed, responsible for the ineffectiveness of active learning for VQA. We end with prescriptive suggestions for future work in building active learning methods robust to these types of outliers.", "publication_ref": ["b4", "b65", "b49", "b3", "b52", "b1", "b37", "b20", "b28", "b47", "b45", "b41", "b68", "b12", "b59", "b61", "b46", "b61", "b46", "b13", "b60", "b58", "b27", "b18", "b24", "b21", "b28", "b3", "b39"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Our work tests the utility of multiple recent active learning methods on the open-ended understanding task of VQA. We draw on the dataset analysis literature to identify collective outliers as the bottleneck hindering active learning methods in this setting.\nActive Learning. Active learning strategies have been successfully applied to image recognition (Joshi et al., 2009;Sener and Savarese, 2018), information extraction (Scheffer et al., 2001;Finn and Kushmerick, 2003;Jones et al., 2003;Culotta and McCallum, 2005), named entity recognition (Hachey et al., 2005;Shen et al., 2017), semantic parsing (Dong et al., 2018), and text categorization (Lewis and Gale, 1994;Hoi et al., 2006). However, these same methods struggle to outperform a random baseline when applied to the task of VQA (Lin and Parikh, 2017;Jedoui et al., 2019). To study this discrepancy, we systematically apply 8 diverse active learning methods to VQA, including methods that use model uncertainty (Abramson and Freund, 2004;Collins et al., 2008;Joshi et al., 2009), Bayesian uncertainty (Gal and Ghahramani, 2016;Kendall and Gal, 2017), disagreement (Houlsby et al., 2011;, and Core-Set selection (Sener and Savarese, 2018).\nVisual Question Answering. Progress on VQA has been heralded as a marker for progress on general open-ended understanding tasks, resulting in several benchmarks (Agrawal et al., 2015;Malinowski et al., 2015;Ren et al., 2015a;Goyal et al., 2017;Krishna et al., 2017;Suhr et al., 2019;Hudson and Manning, 2019) and models (Zhou et al., 2015;Fukui et al., 2016;Lu et al., 2016;Zhu et al., 2016;Wu et al., 2016;Anderson et al., 2018a;Tan and Bansal, 2019;Chen et al., 2020). To ensure that our negative results are not dataset or model-specific, we sample 4 datasets and 5 representative models, each utilizing unique visual and linguistic features and employing different inductive biases.\nInterpreting and Analyzing Datasets. Given the prevalence of large datasets in modern machine learning, it is critical to assess dataset properties to remove redundancies (Gururangan et al., 2018;Li and Vasconcelos, 2019) or biases (Torralba and Efros, 2011;Khosla et al., 2012;Bolukbasi et al., 2016), both of which negatively impact sample efficiency. Prior work has used training dynamics to find examples which are frequently forgotten (Krymolowski, 2002;Toneva et al., 2019) versus those that are easy to learn (Bras et al., 2020). This work suggests using two model-specific measures confidence and prediction variance -as indicators of a training example's \"learnability\" (Chang et al., 2017;. Dataset Maps , a recently introduced framework uses these two measures to profile datasets to find learnable examples. Unlike prior datasets analyzed by Dataset Maps that have a small number of global outliers as hard examples, we discover that VQA datasets contain copious amounts of collective outliers, which are difficult or even impossible for models to learn.", "publication_ref": ["b32", "b58", "b56", "b16", "b31", "b12", "b23", "b60", "b15", "b42", "b26", "b45", "b29", "b0", "b11", "b32", "b18", "b35", "b27", "b58", "b1", "b48", "b54", "b21", "b37", "b62", "b28", "b76", "b17", "b47", "b77", "b73", "b2", "b64", "b9", "b22", "b43", "b69", "b36", "b5", "b38", "b67", "b7", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Active Learning Experimental Setup", "text": "We adopt the standard pool-based active learning setup from prior work (Lewis and Gale, 1994;Settles, 2009;Lin and Parikh, 2017), consisting of a model M, initial seed set of labeled examples (x i , y i ) \u2208 D seed used to initialize M, an unlabeled pool of data D pool , and an acquisition function A(x, M). We run active learning over a series of acquisition iterations  T where at each iteration we acquire a batch of B new examples per:x \u2208 D pool to label per x = arg max x\u2208D pool A(x, M).\nAcquiring an example often refers to using an oracle or human expert to annotate a new example with a correct label. We follow prior work to simulate an oracle using existing datasets, forming D seed from a fixed percentage of the full dataset, and using the remainder as D pool Lin and Parikh, 2017;Siddhant and Lipton, 2018). We re-train M after each acquisition iteration.\nPrior work has noted the impact of seed set size on active learning performance (Lin and Parikh, 2017;Misra et al., 2018;Jedoui et al., 2019). We run multiple active learning evaluations with varying seed set sizes (ranging from 5% to 50% of the full pool size). We keep the size of each acquisition batch B to a constant 10% of the overall pool size.", "publication_ref": ["b42", "b59", "b45", "b45", "b61", "b45", "b50", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Models", "text": "Visual Question Answering (VQA) requires reasoning over two modalities: images and text. Most models use feature \"backbones\" (e.g., features from object recognition models pretrained on Ima-geNet, and pretrained word vectors for text). For image features we use grid-based features from ResNet-101 , or object-based features from Faster R-CNN (Ren et al., 2015b) finetuned on Visual Genome (Anderson et al., 2018a). We evaluate with a representative sample of existing VQA models, including the following: 2\nLogReg is a logistic regression model that uses either ResNet-101 or Faster R-CNN image features with mean-pooled GloVe question embeddings (Pennington et al., 2014). Although these models are not as performant as the subsequent models, logistic regression has been effective on VQA (Suhr et al., 2019), and is pervasive in the active learning literature (Schein and Ungar, 2007;Yang and Loog, 2018;Mussmann and Liang, 2018).\nLSTM-CNN is a standard model introduced with VQA-1 (Agrawal et al., 2015). We use more performant ResNet-101 features instead of the original VGGNet features as our visual backbone. BUTD (Bottom-Up Top-Down Attention) uses object-based features in tandem with attention over objects (Anderson et al., 2018a). BUTD won the 2017 VQA Challenge (Teney et al., 2018), and has been a consistent baseline for recent work in VQA.\nLXMERT is a large multi-modal transformer model that uses BUTD's object features and contextualized BERT  language features (Tan and Bansal, 2019). LXMERT is pretrained on a corpus of aligned image-and-textual data spanning MS COCO, Visual Genome, VQA-2, NLVR-2, and GQA (Lin et al., 2014;Krishna et al., 2017;Goyal et al., 2017;Suhr et al., 2019;Hudson and Manning, 2019), initializing a cross-modal representation space conducive to fine-tuning. 3", "publication_ref": ["b55", "b2", "b53", "b62", "b57", "b74", "b51", "b1", "b2", "b66", "b44", "b37", "b21", "b62", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "Acquisition Functions", "text": "Several active learning methods have been developed to account for different aspects of the machine learning training pipeline: while some acquire examples with high aleotoric uncertainty (Settles, 2009) (having to do with the natural uncertainty in the data) or epistemic uncertainty  (having to do with the uncertainty in the modeling/learning process), others attempt to acquire examples that reflect the distribution of data in the pool (Sener and Savarese, 2018). We sample a diverse set of these methods:\nRandom Sampling serves as our baseline passive approach for acquiring examples.\nLeast Confidence acquires examples with lowest model prediction probability (Settles, 2009).\nEntropy acquires examples with the highest entropy in the model's output (Settles, 2009). MC-Dropout Entropy (Monte-Carlo Dropout with Entropy acquisition) acquires examples with high entropy in the model's output averaged over multiple passes through a neural network with different dropout masks (Gal and Ghahramani, 2016). This process is a consequence of a theoretical casting of dropout as approximate Bayesian inference in deep Gaussian processes. BALD (Bayesian Active Learning by Disagreement) builds upon Monte-Carlo Dropout by proposing a decision theoretic objective; it acquires examples that maximise the decrease in expected posterior entropy (Houlsby et al., 2011;Siddhant and Lipton, 2018) -capturing \"disagreement\" across different dropout masks.", "publication_ref": ["b59", "b58", "b59", "b59", "b18", "b27", "b61"], "figure_ref": [], "table_ref": []}, {"heading": "Core-Set", "text": "Selection samples examples that capture the diversity of the data pool (Sener and Savarese, 2018;Coleman et al., 2020). It acquires examples to minimize the distance between an example in the unlabeled pool to its closest labeled example. Since Core-Set selection operates over a representation space (and not an output distribution, like prior strategies) and VQA models operate over two modalities, we employ three Core-Set variants: Core-Set (Language) and Core-Set (Vision) operate over their respective representation spaces while Core-Set (Fused) operates over the \"fused\" vision and language representation space.", "publication_ref": ["b58", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Results", "text": "We evaluate the 8 active learning strategies across the 5 models described in the previous section. Figures 2-5 show a representative sample of active learning results across datasets. Due to space constraints, we only visualize 4 active learning strategies -Least-Confidence, BALD, CoreSet-Fused, and the Random Baseline -using 3 models (LSTM-CNN, BUTD, LXMERT). 4 Results and trends are consistent across the different acquisition functions, models and seed set sizes (see the appendix for results with other models, acquisition functions, and seed set sizes). We now go on to provide descriptions of the datasets we evaluate against, and the corresponding results. Strategies perform on par with or worse than the random baseline, when using 10% of the full dataset as the seed set.\n4 0 K 8 0 K 1 2 0 K 1 6 0 K 2 0 0 K 2 4 0 K 2 8 0 K 3 2 0 K 3 6 0 K 4 0 0 K Number of Training Examples 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Validation Accuracy LSTM-CNN -VQA-2 Random Baseline Least-Confidence BALD Core-Set (Fused) 4 0 K 8 0 K 1 2 0 K 1 6 0 K 2 0 0 K 2 4 0 K 2 8 0 K 3 2 0 K 3 6 0 K 4 0 0 K Number of Training Examples 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Validation Accuracy BUTD -VQA-2 4 0 K 8 0 K 1 2 0 K 1 6 0 K 2 0 0 K 2 4 0 K 2 8 0 K 3 2 0 K 3 6 0 K 4 0 0 K Number of Training Examples 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Validation Accuracy LXMERT -VQA-2\nFigure 3: Results for the full VQA-2 dataset, also using 10% of the full dataset as a seed set. Similar to the plot above, all active learning methods perform similar to a random baseline.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Simplified VQA Datasets", "text": "One complexity of VQA is the size of the output space and the number of examples present (Agrawal et al., 2015;Goyal et al., 2017); VQA-2 has 400k training examples, and in excess of 3k possible answers (see Table 1). However, prior work in active learning focuses on smaller datasets like the 10-class MNIST dataset , binary classification (Siddhant and Lipton, 2018), or small-cardinality (\u2264 20 classes) text categorization (Lowell et al., 2019). To ensure our results and conclusions are not due to the size of the output space, we build two meaningful, but narrow-domain VQA datasets from subsets of VQA-2. These simplified datasets reduce the complexity of the underlying learning problem and provide a fair comparison to existing active learning literature.\nVQA-Sports. We generate VQA-Sports by compiling a list of 20 popular sports (e.g., soccer, football, tennis, etc.) in VQA-2, and restricting the set of questions to those with answers in this list. We picked the sports categories by ranking the GloVe vector similarity between the word \"sports\" to answers in VQA-2, and selected the 20 most commonly occurring answers.\nVQA-Food. We generate the VQA-Food dataset similarly, compiling a list of the 20 commonly occurring food categories by GloVe vector similarity to the word \"food.\"\nResults. Figure 2 presents results for VQA-Sports, with an initial seed set restricted to 10% of the total pool (500 examples). The appendix reports similar results on VQA-Food. For LSTM-CNN, Least-Confidence appears to be slightly more sample efficient, while all other strategies perform on par with or worse than random. For BUTD, all methods are on par with random; for LXMERT, they perform worse than random. Generally on VQA-Sports, active learning performance varies, but fails to outperform random acquisition.", "publication_ref": ["b1", "b21", "b61", "b46"], "figure_ref": ["fig_0"], "table_ref": ["tab_1"]}, {"heading": "VQA-2", "text": "VQA-2 is the canonical dataset for evaluating VQA models (Goyal et al., 2017). In keeping with prior work (Anderson et al., 2018a;Tan and Bansal, 2019), we filter the training set to only include answers that appear at least 9 times, resulting in 3130 unique answers. Unlike traditional VQA-2 evaluation, which treats the task as a multi-label binary classification problem, we follow prior active learning work on VQA (Lin and Parikh, 2017), which formulates it as a multi-class classification problem, enabling the use of acquisition functions such as uncertainty sampling and BALD.  the right of?\". We use the standard GQA training set of 943k questions, 900k of which we use for the active learning pool.\nResults. Figure 5 shows results on GQA using a seed set of 10% of the full pool (90k examples). Despite its notable differences in question structure to VQA-2, active learning still performs on par with or slightly worse than random.", "publication_ref": ["b21", "b2", "b64", "b45"], "figure_ref": [], "table_ref": []}, {"heading": "Analysis via Dataset Maps", "text": "The previous section shows that active learning fails to improve over random acquisition on VQA across models and datasets. A simple question remains -why? One hypothesis is that sample inefficiency stems from the data itself: there is only a 2% gain in validation accuracy when training on half versus the whole dataset. Working from this, we characterize the underlying datasets using Dataset Maps  and discover that active learning methods prefer sampling \"hard-tolearn\" examples, leading to poor performance.    (Han and Kamber, 2000) -they often present as fundamental subproblems of a broader task. For instance (Figure 7), in VQA-2, we identify clusters of hard-to-learn examples that require optical character recognition (OCR) for reasoning about text (e.g., \"What is the first word on the black car?\"); another cluster requires external knowledge to answer (\"What is the symbol on the hood often associated with?\"). In GQA, we identify different clusters of collective outliers; one cluster stems from innate underspecification (e.g., \"what is on the shelf?\" with multiple objects present on the shelf); another cluster requires multiple reasoning hops difficult for current models (e.g., \"What is the vehicle that is driving down the road the box is on the side of?\"). We sample 100 random \"hard-to-learn\" examples from both VQA-2 and GQA and find that 100% Ablating Outliers. To verify that collective outliers are responsible for the degradation of active learning performance, we re-run our experiments using active learning pools with varying numbers of outliers removed. To remove these outliers, we sort and remove all examples in the data pool using the product of their model confidence and prediction variability (x and y-axis values of the Dataset Maps). We systematically remove examples with a low product value and observe how active learning performance changes (see Figure 8).\nWe observe a 2-3x improvement in sample efficiency when removing 50% of the entire data pool, consisting mainly of collective outliers (Figure 8c). This improvement decreases if we only remove 25% of the full pool (Figure 8b), and further degrades if we remove only 10% (Figure 8a). This ablation demonstrates that active learning methods are more sample efficient than the random baseline when collective outliers are absent from the unlabelled pool.", "publication_ref": ["b24"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Discussion and Future Work", "text": "This paper asks a simple question -why does the modern neural active learning toolkit fail when applied to complex, open ended tasks? While we focus on VQA, collective outliers are abundant in tasks such as natural language inference (Bowman et al., 2015;Williams et al., 2018) and opendomain question answering (Kwiatkowski et al., 2019), amongst others. More insidious is their nature; collective outliers can take multiple forms, requiring external domain knowledge or \"commonsense\" reasoning, containing underspecification, or requiring capabilities beyond the scope of a given model (e.g., requiring OCR ability). While we perform ablations in this work removing collective outliers, demonstrating that active learning fails as collective outliers take up larger portions of the dataset, this is only an analytical tool; these outliers are, and will continue to be, pervasive in open-ended datasets -and as such, we will need to develop better tools for learning (and performing active learning) in their presence.\nSelective Classification. One potential direction for future work is to develop systems that abstain when they encounter collective outliers. Historical artificial intelligence systems, such as SHRDLU (Winograd, 1972) and QUALM (Lehnert, 1977), were designed to flag input sequences that they were not designed to parse. Ideas from those methods can and should be resurrected using modern techniques; for example, recent work suggests that a simple classifier can be trained to identify out-ofdomain data inputs, provided a seed out-of-domain dataset (Kamath et al., 2020). Active learning methods can be augmented with a similar classifier, which re-calibrates active learning uncertainty scores with this classifier's predictions. Other work learns to identify novel utterances by learning to intelligently set thresholds in representation space (Karamcheti et al., 2020), a powerful idea especially if combined with other representation-centric active learning methods like Core-Set Sampling (Sener and Savarese, 2018).\nActive Learning with Global Reasoning. Another direction for future work to explore is to leverage Dataset Maps to perform more global, holistic reasoning over datasets, to intelligently identify promising examples -in a sense, baking part of the analysis done in this work directly into the active learning algorithms. A possible instantiation of this idea would be in training a discriminator to differentiate between \"learnable\" examples (upper half of each Dataset Map) from the \"unlearnable\", collective outliers with low confidence and low variability. Between each active learning acquisition iteration, one can generate an updated Dataset Map, thereby reflecting what models are learning as they obtain new labeled examples.\nMachine learning systems deployed in realworld settings will inevitably encounter open-world datasets, ones that contain a mixture of learnable and unlearnable inputs. Our work provides a framework to study when models encounter such inputs. Overall, we hope that our experiments serve as a catalyst for future work on evaluating active learning methods with inputs drawn from open-world datasets.", "publication_ref": ["b6", "b70", "b39", "b71", "b40", "b33", "b34", "b58"], "figure_ref": [], "table_ref": []}, {"heading": "Reproducibility", "text": "All code for data preprocessing, model implementation, and active learning algorithms is made available at https://github.com/siddk/vqa-outliers. Additionally, this repository also contains the full set of results and dataset maps as well.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Overview", "text": "Due to the broad scope of our experiments and analysis, we were unable to fit all our results in the main body of the paper. Furthermore, given the limited length provided by the appendix, we provide only salient implementation details and other representative results here; however, we make all code, models, data, results, active learning implementations available at this link: https: //github.com/siddk/vqa-outliers.\nGenerally, any combination of {active learning strategy \u00d7 model \u00d7 seed set size \u00d7 analysis/acquisition plot} is present in this paper, and is available in the public code repository.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Implementation Details", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.1 Models & Training", "text": "Where applicable, we implement our models based on publicly available PyTorch implementations. For the LSTM-CNN model, we base our implementation off of this repository: https://github.com/ Shivanshu-Gupta/Visual-Question-Answering, while for the Bottom-Up Top-Down Attention Model, we use this repository: https://github.com/ hengyuan-hu/bottom-up-attention-vqa, keeping default hyperparameters the same.\nLogistic Regression. When implementing Logistic Regression, we base our PyTorch implementation on the broadly used Scikit-Learn (https: //scikit-learn.org) implementation, using the default parameters (including L2 weight decay). We optimize our models via stochastic gradient descent.\nLXMERT. As mentioned in Section 3, the default LXMERT checkpoint and fine-tuning code made publicly available in Tan and Bansal (2019) (associated code repository: https://github.com/ airsplay/lxmert) is pretrained on data from VQA-2 and GQA, leaking information that could substantially affect our active learning results. To mitigate this, we contacted the authors, who kindly provided us with a checkpoint of the model without VQA pretraining.\nHowever, in addition to this model obtaining different results from those reported in the original work, the provided pretrained checkpoint behaves slightly differently during fine-tuning, requiring different hyperparameters than provided in the original repository. We perform a coarse grid search over hyperparameters, using the LXMERT implementation provided by HuggingFace Transformers (Wolf et al., 2019), and find that using an AdamW optimizer rather than the BERT-Adam Optimizer used in the original work without any special learning rate scheduling results in the best fine-tuning performance.", "publication_ref": ["b72"], "figure_ref": [], "table_ref": []}, {"heading": "B.2 Acquisition Functions", "text": "We use standard implementations of the 8 active learning strategies described, borrowing from prior implementations (Mussmann and Liang, 2018) and existing code repositories (https://github.com/ google/active-learning). We provide additional details below.\nMonte-Carlo Dropout. For our implementations of the deep Bayesian active learning methods (Monte-Carlo Dropout w/ Entropy, BALD), we follow Gal and Ghahramani (2016) and estimate a Dropout distribution via test-time dropout, running multiple forward passes through our neural networks, with different, randomly sampled Dropout masks. We use a value of k = 10 forward passes to form our Dropout distribution. Amortized Core-Set Selection. In the original Core-Set selection active learning work introduced by Sener and Savarese (2018), it is shown that Core-Set selection for active learning can be reduced to a version of the k-centers problem, which can be solved approximately (2-OPT) with a greedy algorithm. However, running this algorithm on highdimensional representations, across large pools can be prohibitive; Core-Set selection is batch-aware, requiring recomputing distances from each \"clustercenter\" (points in the set of acquired examples) to all points in the active learning pool after each acquisition in a batch. While we can run this out completely for smaller datasets (and indeed, this is what we do for our small datasets VQA-Sports and VQA-Food), a single acquisition iteration for a large dataset for the full VQA-2 dataset takes approximately 20 GPU-hours on the resources we have available, or up to 9 days for a single Core-Set selection run. For GQA, performing exact Core-Set selection takes at least twice as long.\nTo still capture the spirit of Core-Set diversitybased selection in our evaluation, we instead introduce an amortized implementation of Core-Set selection, which is comprised of two steps. We first downsample the high-dimensional representations (of either the fused language and text, or either unimodal representations) via Principal Component Analysis (PCA) to make the distance computation faster by an order of magnitude. Then, rather than updating distances from examples in our acquired set to points in our pool after each acquisitionx, we delay updates, instead only refreshing the distance computation every 2000 acquisitions (roughly 5% of an acquisition batch for VQA-2). This allows us to report results for Core-Set selection with the three different proposed representations (Fused, Language-Only, Vision-Only) for VQA-2; unfortunately, for GQA and LXMERT (due to the high cost of training), even running this amortized version of Core-Set selection is prohibitive, so we report a subset of results, and omit the rest.", "publication_ref": ["b51", "b18", "b58"], "figure_ref": [], "table_ref": []}, {"heading": "C Active Learning Results", "text": "We include further results from our study of active learning applied to VQA, including results on VQA-Food (not included in the main body), active learning results for the two logistic regression models -Log-Reg (ResNet-101) and Log-Reg (Faster R-CNN), as well as with the 4 acquisition strategies not included in the main body of the paper -Entropy, Monte-Carlo Dropout w/ Entropy, Core-Set (Language), and Core-Set (Vision).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1 VQA-Food", "text": "Figure 9 shows results on VQA-Food with the LSTM-CNN, BUTD, and LXMERT models, with a seed set comprised of 10% of the total pool. The results are mostly similar to those reported in the paper; strategies track or underperform random sampling, with the exception of Least-Confidence for the LSTM-CNN model -however, this is the sole exception, and the LSTM-CNN has the highest training variance of all the models we try.", "publication_ref": [], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "C.2 Logistic Regression (ResNet-101)", "text": "Figure 10 shows active learning results for the Lo-gReg (ResNet-101) model on VQA-Sports (seed set = 10%), and VQA-2 (seed set = 10%, 50%). Results are similar to those reported in the paper, with active learning failing to outperform random acqusition.", "publication_ref": [], "figure_ref": ["fig_8"], "table_ref": []}, {"heading": "C.3 Logistic Regression (Faster R-CNN)", "text": "Figure 11 presents the same set of experiments as the prior section, except with the LogReg (Faster R-CNN) model. While the object-based Faster R-CNN representation enables much higher performance than the ResNet-101 representation, active learning results are consistent with those reported in the paper.", "publication_ref": [], "figure_ref": ["fig_9"], "table_ref": []}, {"heading": "C.4 Other Acquisition Strategies", "text": "Figure 12 presents results for the four other active learning strategies we implement -Entropy, Monte Carlo Dropout w/ Entropy, Core-Set (Language), and Core-Set (Vision) -for the BUTD model. Results are across VQA-Sports (seed set = 10%), and VQA-2 (seed set = 10%, 50%) -despite the unique features of each strategy, the trends remain consistent with those in the paper.    Figure 12: Results with the BUTD on VQA-Sports, VQA-2 and GQA using the alternative 4 acquisition strategies not included in the main body of the paper. Unsurprisingly, results are consistent with those reported in the paper.   Given that the map for GQA is similar to the map for VQA-2, it is not surprising that the active learning acquisitions follow a similar trend, preferring to select \"hard-to-learn\" examples.", "publication_ref": [], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "D Dataset Maps & Acquisitions", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "The authors are fully committed to maintaining this repository, in terms of both functionality and ease of use, and will actively monitor both email and Github Issues should there be problems.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We thank Kaylee Burns, Eric Mitchell, Stephen Mussman, Dorsa Sadigh, and our anonymous ACL reviewers for their useful feedback on earlier versions of this paper. We are also grateful to Hao Tan for providing us with the LXMERT checkpoint trained without access to VQA datasets, as well as for general LXMERT fine-tuning pointers.\nSiddharth Karamcheti is graciously supported by the Open Philanthropy Project AI Fellowship. Christopher D. Manning is a CIFAR Fellow.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Active learning for visual object recognition", "journal": "", "year": "2004", "authors": "Yotam Abramson; Yoav Freund"}, {"ref_id": "b1", "title": "VQA: Visual question answering", "journal": "International Journal of Computer Vision", "year": "2015", "authors": "Aishwarya Agrawal; Jiasen Lu; Stanislaw Antol; Margaret Mitchell; C Lawrence Zitnick; Devi Parikh; Dhruv Batra"}, {"ref_id": "b2", "title": "Bottom-up and top-down attention for image captioning and visual question answering", "journal": "", "year": "2018", "authors": "X Peter Anderson; C He; Damien Buehler; Mark Teney; Stephen Johnson; Lei Gould;  Zhang"}, {"ref_id": "b3", "title": "Visionand-language navigation: Interpreting visuallygrounded navigation instructions in real environments", "journal": "", "year": "2018", "authors": "Peter Anderson; Qi Wu; Damien Teney; Jake Bruce; Mark Johnson; Niko S\u00fcnderhauf; Ian Reid; Stephen Gould; Anton Van Den;  Hengel"}, {"ref_id": "b4", "title": "VizWiz: nearly realtime answers to visual questions", "journal": "", "year": "2010", "authors": "P Jeffrey; Chandrika Bigham; Hanjie Jayant; Greg Ji; Andrew Little;  Miller; C Robert; Robin Miller; Aubrey Miller; Brandyn Tatarowicz; Samual White; Tom White;  Yeh"}, {"ref_id": "b5", "title": "Man is to computer programmer as woman is to homemaker? Debiasing word embeddings", "journal": "", "year": "2016", "authors": "Tolga Bolukbasi; Kai-Wei Chang; Y James; Venkatesh Zou; Adam T Saligrama;  Kalai"}, {"ref_id": "b6", "title": "A large annotated corpus for learning natural language inference", "journal": "", "year": "2015", "authors": "Samuel Bowman; Gabor Angeli; Christopher Potts; Christopher D Manning"}, {"ref_id": "b7", "title": "Adversarial filters of dataset biases", "journal": "", "year": "2020", "authors": "Swabha Ronan Le Bras; Chandra Swayamdipta; Rowan Bhagavatula; Matthew Zellers; Ashish Peters; Yejin Sabharwal;  Choi"}, {"ref_id": "b8", "title": "Active bias: Training more accurate neural networks by emphasizing high variance samples", "journal": "", "year": "2017", "authors": " Haw-Shiuan; Erik Chang; Andrew Learned-Miller;  Mccallum"}, {"ref_id": "b9", "title": "Uniter: Universal image-text representation learning", "journal": "", "year": "2020", "authors": "Yen-Chun Chen; Linjie Li; Licheng Yu; Ahmed El Kholy; Faisal Ahmed; Zhe Gan; Yu Cheng; Jingjing Liu"}, {"ref_id": "b10", "title": "Selection via proxy: Efficient data selection for deep learning", "journal": "", "year": "2020", "authors": "Cody Coleman; Christopher Yeh; Stephen Mussmann; Baharan Mirzasoleiman; Peter Bailis; Percy Liang; Jure Leskovec; Matei Zaharia"}, {"ref_id": "b11", "title": "Towards scalable dataset construction: An active learning approach", "journal": "", "year": "2008", "authors": "Brendan Collins; Jia Deng; Kai Li; Li Fei-Fei"}, {"ref_id": "b12", "title": "Reducing labeling effort for structured prediction tasks", "journal": "", "year": "2005", "authors": "Aron Culotta; Andrew Mccallum"}, {"ref_id": "b13", "title": "Adversarial active learning for sequences labeling and generation", "journal": "", "year": "2018", "authors": "Yue Deng; Kawai Chen; Yilin Shen; Hongxia Jin"}, {"ref_id": "b14", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b15", "title": "Confidence modeling for neural semantic parsing", "journal": "", "year": "2018", "authors": "Li Dong; Chris Quirk; Mirella Lapata"}, {"ref_id": "b16", "title": "Active learning selection strategies for information extraction", "journal": "", "year": "2003", "authors": "Aidan Finn; Nicolas Kushmerick"}, {"ref_id": "b17", "title": "Multimodal compact bilinear pooling for visual question answering and visual grounding", "journal": "EMNLP", "year": "2016", "authors": "Akira Fukui; Dong Huk Park; Daylen Yang; Anna Rohrbach; Trevor Darrell; Marcus Rohrbach"}, {"ref_id": "b18", "title": "Dropout as a Bayesian approximation: Representing model uncertainty in deep learning", "journal": "", "year": "2016", "authors": "Yarin Gal; Zoubin Ghahramani"}, {"ref_id": "b19", "title": "Deep Bayesian active learning with image data", "journal": "", "year": "2017", "authors": "Yarin Gal; R Islam; Zoubin Ghahramani"}, {"ref_id": "b20", "title": "IQA: Visual question answering in interactive environments", "journal": "", "year": "2018", "authors": "Daniel Gordon; Aniruddha Kembhavi; Mohammad Rastegari; Joseph Redmon; Dieter Fox; Ali Farhadi"}, {"ref_id": "b21", "title": "Making the V in VQA matter: Elevating the role of image understanding in visual question answering", "journal": "", "year": "2017", "authors": "Yash Goyal; Tejas Khot; Douglas Summers-Stay; Dhruv Batra; Devi Parikh"}, {"ref_id": "b22", "title": "Annotation artifacts in natural language inference data", "journal": "", "year": "2018", "authors": "Swabha Suchin Gururangan; Omer Swayamdipta; Roy Levy; Samuel Schwartz; Noah A Bowman;  Smith"}, {"ref_id": "b23", "title": "Investigating the effects of selective sampling on the annotation task", "journal": "", "year": "2005", "authors": "Ben Hachey; Beatrice Alex ; Markus Becker"}, {"ref_id": "b24", "title": "Data Mining: Concepts and Techniques", "journal": "Morgan Kaufmann", "year": "2000", "authors": "Jiawei Han; Micheline Kamber"}, {"ref_id": "b25", "title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"ref_id": "b26", "title": "Batch mode active learning and its application to medical image classification", "journal": "", "year": "2006", "authors": "C H Steven; Rong Hoi; Jianke Jin; Michael R Zhu;  Lyu"}, {"ref_id": "b27", "title": "Bayesian active learning for classification and preference learning", "journal": "", "year": "2011", "authors": "Neil Houlsby; Ferenc Husz\u00e1r; Zoubin Ghahramani; M\u00e1t\u00e9 Lengyel"}, {"ref_id": "b28", "title": "GQA: A new dataset for real-world visual reasoning and compositional question answering", "journal": "", "year": "2019", "authors": "A Drew; Christopher D Hudson;  Manning"}, {"ref_id": "b29", "title": "Deep Bayesian active learning for multiple correct outputs", "journal": "", "year": "2019", "authors": "Khaled Jedoui; Ranjay Krishna; Michael Bernstein; Li Fei-Fei"}, {"ref_id": "b30", "title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning", "journal": "", "year": "2017", "authors": "Justin Johnson; Bharath Hariharan; Laurens Van Der Maaten; Li Fei-Fei; Lawrence Zitnick; Ross Girshick"}, {"ref_id": "b31", "title": "Active learning for information extraction with multiple view feature sets", "journal": "", "year": "2003", "authors": "Rosie Jones; Rayid Ghani; Tom Mitchell; Ellen Riloff"}, {"ref_id": "b32", "title": "Multi-class active learning for image classification", "journal": "", "year": "2009", "authors": "J Ajay; Fatih Joshi; Nikolaos Porikli;  Papanikolopoulos"}, {"ref_id": "b33", "title": "Selective question answering under domain shift", "journal": "", "year": "2020", "authors": "Amita Kamath; Robin Jia; Percy Liang"}, {"ref_id": "b34", "title": "Learning adaptive language interfaces through decomposition", "journal": "", "year": "2020", "authors": "Siddharth Karamcheti; Dorsa Sadigh; Percy Liang"}, {"ref_id": "b35", "title": "What uncertainties do we need in Bayesian deep learning for computer vision?", "journal": "", "year": "2017", "authors": "Alex Kendall; Yarin Gal"}, {"ref_id": "b36", "title": "Undoing the damage of dataset bias", "journal": "", "year": "2012", "authors": "Aditya Khosla; Tinghui Zhou; Tomasz Malisiewicz; Alexei A Efros; Antonio Torralba"}, {"ref_id": "b37", "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "journal": "International Journal of Computer Vision", "year": "2017", "authors": "Ranjay Krishna; Yuke Zhu; Oliver Groth; Justin Johnson; Kenji Hata; Joshua Kravitz; Stephanie Chen; Yannis Kalantidi; Li-Jia Li; David A Shamma; Michael S Bernstein; Fei-Fei Li"}, {"ref_id": "b38", "title": "Distinguishing easy and hard instances", "journal": "", "year": "2002", "authors": "Yuval Krymolowski"}, {"ref_id": "b39", "title": "Natural questions: A benchmark for question answering research", "journal": "", "year": "2019", "authors": "Tom Kwiatkowski; Jennimaria Palomaki; Olivia Redfield; Michael Collins; Ankur Parikh; Chris Alberti; Danielle Epstein; Illia Polosukhin; Matthew Kelcey; Jacob Devlin; Kenton Lee; Kristina N Toutanova; Llion Jones; Ming-Wei Chang; Andrew Dai; Jakob Uszkoreit; Quoc Le; Slav Petrov"}, {"ref_id": "b40", "title": "The Process of Question Answering", "journal": "", "year": "1977", "authors": "Wendy Lehnert"}, {"ref_id": "b41", "title": "Heterogeneous uncertainty sampling for supervised learning", "journal": "", "year": "1994", "authors": "D David; Jason Lewis;  Catlett"}, {"ref_id": "b42", "title": "A sequential algorithm for training text classifiers", "journal": "In ACM Special Interest Group on Information Retreival", "year": "1994", "authors": "D David; William A Lewis;  Gale"}, {"ref_id": "b43", "title": "Repair: Removing representation bias by dataset resampling", "journal": "", "year": "2019", "authors": "Yi Li; Nuno Vasconcelos"}, {"ref_id": "b44", "title": "Microsoft COCO: Common objects in context", "journal": "", "year": "2014", "authors": "Tsung-Yi Lin; Michael Maire; Serge Belongie; James Hays; Pietro Perona; Deva Ramanan; Piotr Doll\u00e1r; C Lawrence Zitnick"}, {"ref_id": "b45", "title": "Active learning for visual question answering: An empirical study", "journal": "", "year": "2017", "authors": "Xiao Lin; Devi Parikh"}, {"ref_id": "b46", "title": "Practical obstacles to deploying active learning", "journal": "EMNLP", "year": "2019", "authors": "David Lowell; Zachary C Lipton; Byron C "}, {"ref_id": "b47", "title": "Hierarchical question-image co-attention for visual question answering", "journal": "", "year": "2016", "authors": "Jiasen Lu; Jianwei Yang; Dhruv Batra; Devi Parikh"}, {"ref_id": "b48", "title": "Ask your neurons: A neural-based approach to answering questions about images", "journal": "", "year": "2015", "authors": "Mateusz Malinowski; Marcus Rohrbach; Mario Fritz"}, {"ref_id": "b49", "title": "Listen, attend, and walk: Neural mapping of navigational instructions to action sequences", "journal": "", "year": "2016", "authors": "Hongyuan Mei; Mohit Bansal; Matthew R Walter"}, {"ref_id": "b50", "title": "Learning by asking questions", "journal": "", "year": "2018", "authors": "Ishan Misra; Ross Girshick; Rob Fergus; Martial Hebert; Abhinav Gupta; Laurens Van Der Maaten"}, {"ref_id": "b51", "title": "On the relationship between data efficiency and error in active learning", "journal": "", "year": "2018", "authors": "Stephen Mussmann; Percy Liang"}, {"ref_id": "b52", "title": "AI-based request augmentation to increase crowdsourcing participation", "journal": "", "year": "2019", "authors": "Junwon Park; Ranjay Krishna; Pranav Khadpe; Li Fei-Fei; Michael Bernstein"}, {"ref_id": "b53", "title": "GloVe: Global vectors for word representation", "journal": "", "year": "2014", "authors": "Jeffrey Pennington; Richard Socher; Christopher D Manning"}, {"ref_id": "b54", "title": "Exploring models and data for image question answering", "journal": "", "year": "2015", "authors": "Mengye Ren; Ryan Kiros; Richard Zemel"}, {"ref_id": "b55", "title": "Faster R-CNN: Towards real-time object detection with region proposal networks", "journal": "", "year": "2015", "authors": "Kaiming Shaoqing Ren; Ross B He; Jian Girshick;  Sun"}, {"ref_id": "b56", "title": "Active hidden Markov models for information extraction", "journal": "", "year": "2001", "authors": "Tobias Scheffer; Christian Decomain; Stefan Wrobel"}, {"ref_id": "b57", "title": "Active learning for logistic regression: An evaluation", "journal": "", "year": "2007", "authors": "A Schein; Lyle H Ungar"}, {"ref_id": "b58", "title": "Active learning for convolutional neural networks: A core-set approach", "journal": "", "year": "2018", "authors": "Ozan Sener; Silvio Savarese"}, {"ref_id": "b59", "title": "Active learning literature survey", "journal": "", "year": "2009", "authors": "Burr Settles"}, {"ref_id": "b60", "title": "Deep active learning for named entity recognition", "journal": "", "year": "2017", "authors": "Yanyao Shen; Hyokun Yun; C Zachary; Yakov Lipton; Animashree Kronrod;  Anandkumar"}, {"ref_id": "b61", "title": "Deep Bayesian active learning for natural language processing: Results of a large-scale empirical study", "journal": "", "year": "2018", "authors": "Aditya Siddhant;  Zachary C Lipton"}, {"ref_id": "b62", "title": "A corpus for reasoning about natural language grounded in photographs", "journal": "", "year": "2019", "authors": "Alane Suhr; Stephanie Zhou; Ally Zhang; Iris Zhang; Huajun Bai; Yoav Artzi"}, {"ref_id": "b63", "title": "Dataset cartography: Mapping and diagnosing datasets with training dynamics", "journal": "EMNLP", "year": "2020", "authors": "Swabha Swayamdipta; Roy Schwartz; Nicholas Lourie; Yizhong Wang; Hannaneh Hajishirzi; Noah A Smith; Yejin Choi"}, {"ref_id": "b64", "title": "LXMERT: Learning cross-modality encoder representations from transformers", "journal": "", "year": "2019", "authors": "Mohit Hao Hao Tan;  Bansal"}, {"ref_id": "b65", "title": "Understanding natural language commands for robotic navigation and mobile manipulation", "journal": "", "year": "2011", "authors": "Stefanie Tellex; Thomas Kollar; Steven Dickerson; R Matthew; Ashis Gopal Walter; Seth J Banerjee; Nicholas Teller;  Roy"}, {"ref_id": "b66", "title": "Tips and tricks for visual question answering: Learnings from the 2017 challenge", "journal": "", "year": "2018", "authors": "Damien Teney; Peter Anderson; Xiaodong He; Anton V D Hengel"}, {"ref_id": "b67", "title": "An empirical study of example forgetting during deep neural network learning", "journal": "", "year": "2019", "authors": "Mariya Toneva; Alessandro Sordoni; Remi Tachet; Adam Combes; Yoshua Trischler; Geoffrey J Bengio;  Gordon"}, {"ref_id": "b68", "title": "Support vector machine active learning with applications to text classification", "journal": "Journal of machine learning research", "year": "2001", "authors": "Simon Tong; Daphne Koller"}, {"ref_id": "b69", "title": "Unbiased look at dataset bias", "journal": "", "year": "2011", "authors": "Antonio Torralba; Alexei A Efros"}, {"ref_id": "b70", "title": "A broad-coverage challenge corpus for sentence understanding through inference", "journal": "", "year": "2018", "authors": "Adina Williams; Nikita Nangia; Samuel Bowman"}, {"ref_id": "b71", "title": "Understanding Natural Language", "journal": "Academic Press", "year": "1972", "authors": "Terry Winograd"}, {"ref_id": "b72", "title": "HuggingFace's transformers: State-of-the-art natural language processing", "journal": "", "year": "2019", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; R'emi Louf; Morgan Funtowicz; Jamie Brew"}, {"ref_id": "b73", "title": "Ask me anything: Free-form visual question answering based on knowledge from external sources", "journal": "", "year": "2016", "authors": "Qi Wu; Peng Wang; Chunhua Shen; Anthony Dick; Anton Van Den;  Hengel"}, {"ref_id": "b74", "title": "A benchmark and comparison of active learning for logistic regression", "journal": "Pattern Recognition", "year": "2018", "authors": "Yazhou Yang; Marco Loog"}, {"ref_id": "b75", "title": "Stacked attention networks for image question answering", "journal": "", "year": "2016", "authors": "Zichao Yang; Xiaodong He; Jianfeng Gao; Li Deng; Alex Smola"}, {"ref_id": "b76", "title": "Simple baseline for visual question answering", "journal": "", "year": "2015", "authors": "Bolei Zhou; Yuandong Tian; Sainbayar Sukhbaatar; Arthur Szlam; Rob Fergus"}, {"ref_id": "b77", "title": "Visual7W: Grounded question answering in images", "journal": "", "year": "2016", "authors": "Yuke Zhu; Oliver Groth; Michael Bernstein; Li Fei-Fei"}, {"ref_id": "b78", "title": "Target-driven visual navigation in indoor scenes using deep reinforcement learning", "journal": "", "year": "2017", "authors": "Yuke Zhu; Roozbeh Mottaghi; Eric Kolve; J Joseph; Abhinav Lim; Li Gupta; Ali Fei-Fei;  Farhadi"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure2: Results for varied active learning methods on VQA-Sports, a simplified VQA dataset. Strategies perform on par with or worse than the random baseline, when using 10% of the full dataset as the seed set.", "figure_data": ""}, {"figure_label": "45", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 4 :Figure 5 :45Figure4: Results on VQA-2 using 50% of the dataset as a seed set. While methods are relatively better when using a larger seed set-confirming results from(Lin and Parikh, 2017)-no methods outperform random.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 6 :6Figure 6: We visualize the difference in acquisition preferences between random and active learning acquisitions (least confidence and BALD) across multiple iterations. Active learning methods prefer to sample impossible examples which models are unable to learn, hurting sample efficiency relative to the random baseline.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 7 :7Figure 7: Example groups of collective outliers in the VQA-2 and GQA datasets.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Mapping VQA Datasets. A Dataset Map) is a model-specific graph for profiling the learnability of individual training examples. Dataset Maps present holistic pictures of classification datasets relative to the training dynamics of a given model; as a model trains for multiple epochs and sees the same examples repeatedly, the mapping process logs statistics about the confidence assigned to individual predictions. Maps then visualize these statistics against two axes: the y-axis plots the average model confidence assigned to the correct answer over training epochs, while the x-axis plots the spread, or variability, of these values. This introduces a 2D representation of a dataset (viewed through its relationship with individual model) where examples are placed on the map by coarse statistics describing their \"learnability\". We show the Dataset Map for BUTD trained on VQA-2 in Figure1. For our work, we build this map post-hoc, training on the entire pool as a means for analyzing what active learning is doing -treating it as a diagnostic tool for identifying the root cause why active learning seems to fail for VQA.In an ideal setting, the majority of examples in the training set should lie in the upper half of the graph -i.e., the mean confidence assigned to the correct answer should be relatively high. Examples towards the upper-left side represent the \"easy-tolearn\" examples, as the variability in the confidence assigned by the model over time is fairly low.A curious feature of VQA-2 and other VQA datasets is the presence of the 25-30% of examples in the bottom-left of the map (shown in red in Figure1) -examples that have low confidence and variability. In other words, models are unable to learn a large proportion of training examples. While prior work attributes examples in this quadrant to \"labeling errors\" (Swayamdipta et al., 2020), labeling errors in VQA are sparse, and cannot account for the density of such examples in these maps. Interpreting Acquisitions. We profile the acquisitions made by each active learning method, contextualizing the acquired examples via their placement on the associated Dataset Map. We segregate training examples into four buckets using the map's y-axis: easy (\u2265 0.75), medium (\u2265 0.50), hard (\u2265 0.25), and impossible (\u2265 0.00). Ideally, active learning should be robust to \"hard-to-learn\" examples, focusing instead on learnable, high uncertainty examples towards the upper-right portion of the Dataset Map. Instead, we find that active learning methods acquire a large proportion of impossible examples early on and concentrate on the easier examples only after the impossible examples dwindle (see Figure 6). In contrast, the random baseline acquires examples proportional to each bucket's density in the underlying map; acquiring easier examples earlier and performing on par with or better than all others.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "of the examples belong to one of the two aforementioned collectives. Since hard-to-learn examples constitute 25-30% of the data pool, active learning methods cannot avoid them. Uncertainty-based methods (e.g., Least-Confidence, Entropy, Monte-Carlo Dropout) identify them as valid acquisition targets because models lack the capacity to correctly answer these examples, assigning low confidence and high uncertainty. Disagreementbased methods (e.g., BALD) are similar; model confidence is generally low but high variance (lower middle/lower right of the Dataset Maps). Finally, diversity methods (e.g., Core-Set selection) identify these examples as different enough from the existing pool to warrant acquisition, but fail to learn meaningful representations, fueling a vicious cycle wherein they continue to pick these examples.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 9 :9Figure 9: Results for the representative active learning methods on VQA-Food, a simplified VQA dataset similar to VQA-Food, across LSTM-CNN, BUTD, and LXMERT.", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 10 :10Figure10: Active learning results using the Logistic Regression (ResNet-101) model on VQA-Sports (10% seed set), and VQA-2 (10% and 50% seed set). Most strategies either track or underperform random acquisition.", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 11 :11Figure11: Active learning results using the Logistic Regression (Faster R-CNN) model on VQA-Sports (10% seed set), and VQA-2 (10% and 50% seed set). While the Faster R-CNN representation leads to better validation accuracies, active learning performance remains consistent.", "figure_data": ""}, {"figure_label": "13", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 13 :13Figure 13: Dataset Maps for the Bottom-Up Top-Down Attention model on VQA-Sports, VQA-Food, and GQA respectively. Note that VQA-Sports and VQA-Food have fewer \"hard-to-learn\" examples.", "figure_data": ""}, {"figure_label": "1516", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 15 :Figure 16 :1516Figure 15: Acquisitions with the BUTD Model on VQA-Food. Despite the sparsity of hard examples, active learning strategies still tend towards them. BALD is high-variance, selecting examples all over the map.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ": We evaluate active learning on 4 VQA datasets.We display the total available training examples, effec-tive pool sizes we use [in brackets], and the total num-ber of possible answers for each dataset."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Active learning performs relatively better with larger seed sets but still underperforms random. Surprisingly, when initialized with 50% of the pool as the seed set, the gain in validation accuracy after acquiring the entire pool of examples (400k examples total) is only 2%. This is an indication that the lack of sample efficiency might be a result of the underlying data, a problem we explore in the next section.", "figure_data": "VQA-2External knowledge: What does the symbol on the blanket mean?OCR: What is the first word on the black car?Multi-hop reasoning:GQAUnderspecification: What is on the shelf?What is the vehicle that is driving down the road the boxis on the side of?"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Sports and VQA-Food are generally easier, with fewer \"hard-to-learn\" examples, active learning still has a bias for picking those examples. For GQA, our earlier analysis is confirmed; active learning is picking the collective outliers populating the bottom half of the Dataset Map.", "figure_data": "Validation Accuracy0.4 0.5 0.6 0.7 0.8 0.9 1.0LSTM-CNN -VQA-Food Random Baseline Least-Confidence BALD Core-Set (Fused)Validation Accuracy0.4 0.5 0.6 0.7 0.8 0.9 1.0BUTD -VQA-Food Random Baseline Least-Confidence BALD Core-Set (Fused)Validation Accuracy0.4 0.5 0.6 0.7 0.8 0.9 1.0LXMERT -VQA-Food Random Baseline Least-Confidence BALD0.30.30.30.20.20.2To provide further context around active learn-ing acquisitions across datasets, Figures 13-16 present Dataset Maps and acquisitions for the BUTD Model across VQA-Sports, VQA-Food, and GQA respectively. Interesting to note is that while VQA-4 0 0 8 0 1 . 2 K 1 . 6 K 2 K 2 . 4 K 2 . 8 K 3 . 2 K 3 . 6 K 4 K Number of Training Examples 4 0 0 8 0 0 1 . 2 K 1 . 6 K 2 K 2 . 4 K 2 . 8 K 3 . 2 K 3 . 6 K 4 K Number of Training Examples 4 0 0 8 0 0 1 . 2 K 1 . 6 K 2 K 2 . 4 K 2 . 8 K 3 . 2 K 4 K 3 . 6 K Number of Training Examples"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Acquisitions with the BUTD Model on VQA-Sports. The dataset has fewer \"hard-to-learn\" examples, but active learning strategies pick the medium-hard examples, which still negatively impact performance.", "figure_data": "Acquisitions by Difficulty0 100 200 300 400 500Impossible [p > 0.0] Hard [p > 0.25] Medium [p > 0.5] Easy [p > 0.75] Random BaselineAcquisitions by Difficulty0 100 200 300 400 500Least-ConfidenceAcquisitions by Difficulty0 100 200 300 400 500BALD5 0 0 1 K 1 . 5 K 2 K 2 . 5 K 3 K 3 . 5 K 4 K 4 . 5 K Number of Training Examples5 0 0 1 K 1 . 5 K 2 K 2 . 5 K 3 K 3 . 5 K 4 K 4 . 5 K Number of Training Examples5 0 0 1 K 1 . 5 K 2 K 2 . 5 K 3 K 3 . 5 K 4 K 4 . 5 K Number of Training Examples0 50 100 150 200 250 300 350 400 Figure 14: 4 0 0 Acquisitions by Difficulty 8 0 0 1 . 2 K Random Baseline Impossible [p > 0.0] Hard [p > 0.25] Medium [p > 0.5] Easy [p > 0.75] 1 . 6 K 2 K 2 . 4 K 2 . 8 K 3 . 2 K 3 . 6 K Number of Training ExamplesAcquisitions by Difficulty0 50 100 150 200 250 300 350 400Least-Confidence 1 . 2 K 8 0 0 4 0 0 1 . 6 K 2 K 2 . 4 K 2 . 8 K 3 . 2 K 3 . 6 K Number of Training Examples4 0 0 8 0 0 1 . 2 K 1 . 6 K 2 K 2 . 4 K 2 . 8 K 3 . 2 K 3 . 6 K Number of Training Examples"}], "formulas": [{"formula_id": "formula_0", "formula_text": "4 0 K 8 0 K 1 2 0 K 1 6 0 K 2 0 0 K 2 4 0 K 2 8 0 K 3 2 0 K 3 6 0 K 4 0 0 K Number of Training Examples 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Validation Accuracy LSTM-CNN -VQA-2 Random Baseline Least-Confidence BALD Core-Set (Fused) 4 0 K 8 0 K 1 2 0 K 1 6 0 K 2 0 0 K 2 4 0 K 2 8 0 K 3 2 0 K 3 6 0 K 4 0 0 K Number of Training Examples 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Validation Accuracy BUTD -VQA-2 4 0 K 8 0 K 1 2 0 K 1 6 0 K 2 0 0 K 2 4 0 K 2 8 0 K 3 2 0 K 3 6 0 K 4 0 0 K Number of Training Examples 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Validation Accuracy LXMERT -VQA-2", "formula_coordinates": [5.0, 69.23, 237.33, 450.12, 141.68]}], "doi": ""}