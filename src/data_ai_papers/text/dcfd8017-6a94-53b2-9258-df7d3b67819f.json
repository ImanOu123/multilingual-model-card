{"title": "Specifying Object Attributes and Relations in Interactive Scene Generation", "authors": "Oron Ashual; Lior Wolf", "pub_date": "2019-11-17", "abstract": "We introduce a method for the generation of images from an input scene graph. The method separates between a layout embedding and an appearance embedding. The dual embedding leads to generated images that better match the scene graph, have higher visual quality, and support more complex scene graphs. In addition, the embedding scheme supports multiple and diverse output images per scene graph, which can be further controlled by the user. We demonstrate two modes of per-object control: (i) importing elements from other images, and (ii) navigation in the object space, by selecting an appearance archetype.", "sections": [{"heading": "Introduction", "text": "David Marr has defined vision as the process of discovering from images what is present in the world, and where it is [15]. The combination of what and where captures the essence of an image at the semantic level and therefore, also plays a crucial role when defining the desired output of image synthesis tools.\nIn this work, we employ scene graphs with per-object location and appearance attributes as an accessible and easyto-manipulate way for users to express their intentions, see Fig. 1. The what aspect is captured hierarchically: objects are defined as belonging to a certain class (horse, tree, boat, etc.) and as having certain appearance attributes. These attributes can be (i) selected from a predefined set obtained by clustering previously seen attributes, or (ii) copied from a sample image. The where aspect, is captured by what is often called a scene graph, i.e., a graph where the scene objects are denoted as nodes, and their relative position, such as \"above\" or \"left of\", are represented as edge types.\nOur method employs a dual encoding for each object in the image. The first part encodes the object's placement and captures a relative position and other global image features, as they relate to the specific object. It is generated based on the scene graph, by employing a graph convolution net-work, followed by the concatenation of a random vector z. The second part encodes the appearance of the object and can be replaced, e.g., by importing it from the same object as it appears in another image, without directly changing the other objects in the image. This copying of objects between images is done in a semantic way, and not at the pixel level.\nIn the scene graph that we employ, each node is equipped with three types of information: (i) the type of object, encoded as a vector of a fixed dimension, (ii) the location attributes of the objects, which denote the approximate location in the generated image, using a coarse 5 \u00d7 5 grid and its size, discretized to ten values, and (iii) the appearance embedding mentioned above. The edges denote relations: \"right of\", \"left of\", \"above\", \"below\", \"surrounding\", and \"inside\". The method is implemented within a convenient user interface, which supports a dynamic placement of objects and the creation of a scene graph. The edge relations are inferred automatically, given the relative position of the objects. This eliminates the need for mostly unnecessary user intervention. Rendering is done in real time, supporting the creation of novel scenes in an interactive way, see Fig. 1.\nThe neural network that we employ has multiple subparts, as can be seen in Fig. 2: (i) A graph convolutional network that converts the input scene graph to a per-object embedding to their location. (ii) A CNN that converts the location embedding of each object to an object's mask. (iii) A parallel network that converts the location embedding to a bounding box location, where the object mask is placed. (iv) An appearance embedding CNN that converts image information into an embedding vector. This process is done off-line and when creating a new image, the vectors can be imported from other images, or selected from a set of archetypes. (v) A multiplexer that combines the object masks and the appearance embedding information, to create a one multidimensional tensor, where different groups of layers denote different objects. (vi) An encoder-decoder residual network that creates the output image.\nOur method is related to the recent work of [9], who create images based on scene graphs. Their method also uses a graph convolutional network to obtain masks, a mul-Figure 1. An example of the image creation process. (top row) the schematic illustration panel of the user interface, in which the user arranges the desired objects. (2nd row) the scene graph that is inferred automatically based on this layout. (3rd row) the layout that is created from the scene graph. (bottom row) the generated image. Legend for the GUI colors in the top row: purple -adding an object, green -resizing it, red -replacing its appearance. (a) A simple layout with a sky object, a tree and a grass object. All object appearances are initialized to a random archetype appearance. (b) A giraffe is added. (c) The giraffe is enlarged. (d) The appearance of the sky is changed to a different archetype. (e) A small sheep is added. (f) An airplane is added. (g) The tree is enlarged.\ntiplexer that combines the layout information and a subsequent encoder-decoder architecture for obtaining the final image. There are, however, important differences: (i) by separating the layout embedding from the appearance embedding, we allow for much more control and freedom to the object selection mechanism, (ii) by adding the location attributes as input, we allow for an intuitive and more direct user control, (iii) the architecture we employ enables better quality and higher resolution outputs, (iv) by adding stochasticity before the masks are created, we are able to generate multiple results per scene graph, (v) this effect is amplified by the ability of the users to manipulate the resulting image, by changing the properties of each individual object, (vi) we introduce a mask discriminator, which plays a crucial role in generating plausible masks, (vii) another novel discriminator captures the appearance encoding in a counterfactual way, and (viii) we introduce feature matching based on the discriminator network and (ix) a perceptual loss term to better capture the appearance of an object, even if the pose or shape of that object has changed.", "publication_ref": ["b14", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Previous Work", "text": "Image generation techniques based on GANs [3] are constantly improving in resolution, visual quality, the diversity of generated images, and the ability to cover the entire visual domain presented during training. In this work, we address conditional image generation, i.e., the creation of images that match a specific input. Earlier work in conditional image generation includes class based image generation [16], which generates an image that matches a given textual description [18,26]. In many cases, the conditioning signal is a source image, in which case the problem is often referred to as image translation. Pix2pix [7] is a fully supervised method that requires pairs of matching samples from the two domains. The Pix2pixHD architecture that was recently presented by [25] is highly influential and many recent video or image mapping works employ elements of it, including our work.\nImage generation based on scene graphs was recently presented in [9]. A scene graph representation is often used for retrieval based on text [10,17], and a few datasets include this information, e.g., COCO-stuff [2] and the visual Figure 2. The architecture of our composite network, including the subnetworks G, M, B, A, R, and the process of creating the layout tensor t. The scene graph is passed to the network G to create the layout embedding ui of each object. The bounding box bi is created from this embedding, using network B. A random vector zi is concatenated to ui, and the network M computes the mask mi. The appearance information, as encoded by the network A, is then added to create the tensor t with c + d5 channels, c being the number of classes. The autoencoder R generates the final image p from this tensor.\ngenome [12]. Also related is the synthesis of images from a given input layout of bounding boxes (and not one that is inferred by a network from a scene graph), which was very recently studied by [28] for small 64x64 images. In another line of work, images are generated to match input sentences, without constructing the scene graph as an intermediate representation [18,26,6].\nRecently, an interactive tool was introduced based on the novel notion of GAN dissection [1]. This tool allows for the manipulation of well-localized neurons that control the occurrence of specific objects across the image using a drawing interface. By either adding or reducing the activations of these neurons, objects can be added, expanded, reduced or removed. The manipulation that we offer here is both more semantic (less related to specific locations and more to spatial relations between the objects), and more precise, in the sense that we provide full control over the exact instance of the object and not just of the desired class.", "publication_ref": ["b2", "b15", "b17", "b25", "b6", "b24", "b8", "b9", "b16", "b1", "b11", "b27", "b17", "b25", "b5", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Method", "text": "Each object i in the input scene graph is associated with a single node n i = [o i , l i ], where o i \u2208 R d1 is a learned encoding of the object class and l i \u2208 {0, 1} d2+d3 is a location vector. The object class embedding o i is one of c possible embedding vectors, c being the number of classes, and o i is set according to the class of object i, denoted c i . The embedding size d 1 is set arbitrarily to 128. The first d 2 = 25 bits of l i denote a coarse image location using a 5 \u00d7 5 grid, and the rest denotes the size of the object, using a scale of d 3 = 10 values. The edge information e ij \u2208 R d1 exists for a subset of the possible pairs of nodes, and encodes, using a learned embedding, the relations between the nodes. In other words, the values of e ij are taken from a learned dictionary with six possible values, each associated with one type of pairwise relation.\nThe location of each generated object is given as a pseudo-binary mask m i (output of a sigmoid) and a bound-\ning box b i = [x 1 , y 1 , x 2 , y 2 ] \u2208 [0, 1] 4\n, which encodes the coordinates of the bounding box as a ratio of the image dimensions. The mask, but not the bounding box, is also determined by a per-object random vector z i \u223c N(0, 1) d4 to create a variation in the generated masks, where d 4 = 64 was set arbitrarily, without testing other values.\nThe method employs multiple ways of embedding input information. The class identity and every inter-object relation, both taking a discrete value, are captured by embedding vectors of dimension d 1 , which are learned as part of the end-to-end training. The object appearance a i \u2208 R d5 of object i seen during training, is obtained by applying a CNN A to a (ground truth) cropped image I i of that object, resized to a fixed resolution of 64 \u00d7 64. d 5 was set arbitrarily to 32 to reflect that it has less information than that of the entire object, which is embedded in R d1 .\nThe way in which the data flows through the subnetworks, as depicted in Fig. 2, is captured by the equations:\nu i = G({n i }, {e ij }) (1) m i = M (u i , z i ) (2) b i = B(u i )(3)\na i = A(I i ) (4) t = T ({c i , m i , b i , a i }) (5) p = R(t)(6)\nwhere G is the graph convolutional network [9,20] that generates the per-object layout embedding, M and B are the networks that generate the object's mask and its bounding box, respectively, T is the fixed (unlearned) function that maps the various object embeddings to a tensor t. Finally, R is the encoder-decoder network that outputs an image p \u2208 R H\u00d7W \u00d73 based on t. The exact architecture of each network is provided in the appendix.\nThe function T constructs the tensor t as a sum of per-object tensors t i \u2208 R H\u00d7W \u00d7(d5+c) , where c is the number of objects. First, the mask m i is shifted and scaled, according to the bounding box b i , resulting in a mask m HW i of size H \u00d7W . Then, a first tensor t 1 i of size H \u00d7W \u00d7d 5 is formed as the tensor product of m HW i and a i . Similarly, a second tensor t 2 i \u2208 R H\u00d7W \u00d7c is formed as the tensor product of m HW i and the one hot vector of length c encoding class c i . The tensor t i is a concatenation of the two tensors t 1 i and t 2 i along the third dimension.\nFor performing adversarial training of the appearance embedding network A, we create two other tensors: t and t . The first one is obtained by employing the ground truth bounding box b i of object i and the ground truth segmentation mask m i . The second one is obtained by incorporating the same ground truth bounding box and mask in a counterfactual way, by replacing a i with a k , where a k is an appearance embedding of an object image I k of a different object from the same class c i , i.e., a k = A(I k ), c i = c k and\nt = T ({c i , m i , b i , a i }) (7) t = T ({c i , m i , b i , a k })(8)\nDuring training, in half of the training samples, the location and size information vectors l i are zeroed, in order to allow the network to generate layouts, even when this information is not available.", "publication_ref": ["b8", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Training Loss Terms", "text": "The loss used to optimize the networks contains multiple terms, which is not surprising, given the need to train five networks (not including the adversarial discriminators mentioned below) and two vector embeddings (o i and e ij ). (9) where in our experiments we set \u03bb 1 = \u03bb 2 = \u03bb 6 = \u03bb 7 = 10, \u03bb 3 = \u03bb 4 = 1, \u03bb 5 = 0.1.\nL = L Rec +\u03bb 1 L box +\u03bb 2 L perceptual +\u03bb 3 L D-mask +\u03bb 4 L D-image + \u03bb 5 L D-object + \u03bb 6 L FM-mask + \u03bb 7 L FM-image\nThe reconstruction loss L Rec is the L1 difference between the reconstructed image p and the ground truth training image. The box loss L box is the MSE between the computed b i (summed over all objects) and the ground truth bounding box b i . Note that unlike [9], we do not employ a mask loss, since our mask contains a stochastic element (Eq. 2). The perceptual loss [8] compares the generated image with the ground truth training image p , using the activations F u of the VGG network [21] at layer u in a set of predefined layers U .\nL perceptual = u\u2208U 1 u ||F u (p) \u2212 F u (p )|| 1\nOur method employs three discriminators D mask , D object , and D image . The mask discriminator employs a Least Squares GAN (LS-GAN [14]) and is conditioned on the object's class c i . Recall that m i is the real mask of object i and m i , the generated mask, which depends on a random variable z i . The GAN loss associated with the mask discriminator is given by\nL D\u2212mask = [log D mask (m i , c i )]+ E z\u223cN (0,1) 64 [log(1 \u2212 D mask (M (u i , z), c i )] (10)\nFor the purpose of training D mask , we minimize \u2212L D\u2212mask . The second discriminator D image , is used for training in an adversarial manner three networks R, M , and A. The loss of L D-image is a compound loss that is given as\nL D-image = L real \u2212 L fake-image \u2212 L fake-layout + L alt-appearance where L real = log D image (t , p ) (11) L fake-image = log(1 \u2212 D image (t , p)) (12) L fake-layout = log(1 \u2212 D image (t, p )) (13) L alt-appearance = log(1 \u2212 D image (t , p ))(14)\nThe goal of the compound loss is to make sure that the generated image p, given a ground truth layout tensor t is indistinguishable from the real image p , and that this is true, even if the layout tensor t is based on estimated bounding boxes and masks (unlike t ). In addition, we would like the ground truth image to be a poor match for a counterfactual appearance vector, as given in t .\nFollowing [25] we use a multi-scale LS-GAN with two scales. In other words, L D-image is computed at the full scale and at half scale (using two different discriminators), and both terms are summed up to obtain the actual L D-image .\nThe third discriminator, D object , guarantees that the generated objects, one by one, look real. For this purpose, we crop p using the bounding boxes b i to create object images I i . Recall that I i are ground truth crops of images, obtained from the ground truth image p , using the ground truth bounding boxes b .\nL D-object = n i=1 log D object (I i ) \u2212 log D object (I i )(15)\nD object maximizes this loss during training. The mask feature matching loss L FM-mask and the image feature matching loss L FM-image are similar to the perceptual loss, i.e., they are based on the L1 difference in the activation. However, instead of the VGG loss, the discriminators are used, as in [19]. In these losses, all layers are used. L FM-mask compares the activations of the generated mask m i and the real mask m i (the discriminator D mask also takes the class c i as input). The other feature matching loss L FM-image compares the activations of D image (t, p) with those of the ground truth layout tensor and image D image (t , p ).  ", "publication_ref": ["b8", "b8", "b7", "b20", "b13", "b24", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Generating the archetypes", "text": "The GUI enables the user to select from preexisting object appearances, as well as copying the appearance vec-tor a i from another image. The existing object appearances are given as 100 archetypes per object class. These are obtained, by applying the learned network A to all objects in a (a) given class in the training set and employing k-means clustering, in order to obtain 100 class means.\n(b) (c) (d) (e) (f) (g)\nIn the GUI, the archetypes are presented linearly along a slider. The order along the slider is obtained by applying a 1-D t-SNE [24] embedding to the 100 archetypes.", "publication_ref": ["b23"], "figure_ref": [], "table_ref": []}, {"heading": "Inferring the scene graph from the layout panel", "text": "The GUI lets the users place objects on a schematic layout, see Fig. 1. Each object is depicted as a string in one of ten different font sizes, in order to capture the size element of l i . The location in the layout determines the 5 \u00d7 5 grid placement, which is encoded in the other part of l i .\nNote, however, that the locations and sizes are provided as indications of the structure of the graph layout and not as absolute locations (or scene layout). The generating network maintains freedom in the object placements to match the semantic properties of the objects in the scene.\nThe coarse placement by the user is more intuitive and less laborious than specifying a scene graph. To avoid adding unwanted work for the users, the edge labels are inferred, based on the relative position and size of the objects. An object which is directly to the left of another object, for example, is labeled \"left of\". The order in which objects are inserted to the layout determines the reference directing. If object i is inserted before object j, then \"i is to the left of j\" and not \"j is to the right of i\". The inside and surrounding relations are determined similarly, by considering objects of different sizes, whose centers are nearby.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Training details", "text": "All networks are trained using ADAM [11] solver with beta1 = 0.5 for 1 million iterations. The learning rate was set to 1e \u22124 for all components except L D-mask , where we set it to a smaller learning rate of 1e \u22125 . The different learning rates help us to stabilize the mask network. We use batch sizes of 32, 16, 4 in our 64 \u00d7 64, 128 \u00d7 128, 256 \u00d7 256 resolutions respectively. Notice that since each image contains up to 8 objects, each batch contains up to 8 \u00d7 32 = 256 different objects.", "publication_ref": ["b10"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We compare our results with the state of the art methods of [9] and [28], using various metrics from the literature. In addition, we perform an ablation analysis to study the relative contribution of various aspects of our method. Our experiments are conducted on the COCO-Stuff dataset [2], which, using the same split as the previous works, contains approximately 25,000 train images, 1000 validation, and 2000 test images.\nWe employ two modes of experiments: either using the ground truth (GT) layout or the inferred layout. The first mode is the only one suitable for the method of [29]. Whenever possible, we report the statistics reported in the previous work. Some of the statistics reported for [9] are computed by us, based on the published model. We report results for three resolutions 64 2 , 128 2 , and 256 2 . The literature reports numerical results only for the first resolution. While [9] presents visual results for 128x128, our attempts Figure 6. Duplicating an object's appearance in the generated image. Images are created based on the scene graph, such that the appearance is taken from one of five unrelated images. In this example, the sky's appearance is generated from the reference image, while all other objects use the same random appearance archetype. to train their method using the published code on this resolution, resulted in sub-par performance, despite some effort. We, therefore, prefer not to provide these non-competitive baseline numbers. The code of [29] is not yet available.\nWe employ multiple acceptable literature evaluation metrics for evaluating the generated images. The inception score [19] measures both the quality of the generated images and their diversity. As has been done in previous works, a pre-trained inception network [22] is employed in order to obtain the network activations used to compute the score. Larger inception scores are better. The FID [5] measures the distance between the distribution of the generated images and that of the real test images, both modelled as a multivariate Gaussian. Lower FID scores are better. The inception score of [9] for the complete pipeline is taken from their paper. The other scores are not reported there. The inception score for [28] is the one reported by the authors.\nb [9] and [28] report numerical results only for a resolution of 64x64.\nc Not reported and cannot be computed due to lack of code/results. d The accuracy reported by [28] is incompatible (different classifiers).\nTable 1. A quantitative comparison using various image generation scores. In order to support a fair comparison, our model does not use location attributes and employs random appearance attributes.\nLess common, but relevant to our task, is the classification accuracy score, used by [29]. A ResNet-101 model [4] is trained to classify the 171 objects available in the training datasets, after cropping and resizing them to a fixed size of 224x224 pixels. On the test image, we report the accuracy of this classifier applied to the object images that are generated, using the bounding box of the image's layout. A higher accuracy means that the method creates more realistic, or at least identifiable, objects.\nWe also report a diversity score [27], which is based on the perceptual similarity [10] between two images. This score is used to measure the distance between pairs of images that are generated given the same input. Ideally, the user would be able to obtain multiple, diverse, alternative outputs to choose from. Specifically, the activations of AlexNet [13] are used together with the LPIPS visual similarity metric [27]. A higher diversity score is better.\nIn addition, we also report three scores for evaluating the quality of the bounding boxes. The IoU score is the ratio between the area of the ground truth bounding box that is also covered by the generated bounding box (the intersection), and the area covered by either box (the union). We also report recall scores at two different thresholds. R@0.5 measures the ratio of object bounding boxes with an IoU of at least 0.5, and similarly for R@0.3.\nTab. 1 compares our method with the baselines and the real test images using the inception, FID, and classification accuracy scores. We make sure not to use information that the baseline method of [9] is not using and use zero location attributes and appearance attributes that are randomly sam-  2. The diversity score of [27]. The results of [9] are computed by us and are considerably higher than those reported for the same method by [28]. The results of [28]   As can be seen, our method obtains a significant lead in all these scores over the baseline methods, whenever such a comparison can be made. This is true both when the ground truth layout is used and when the layout is generated. As expected, the ground truth layout obtains better scores.\nSample results of our 256x256 model are shown in Fig. 3, using test images from the COCO-stuff datasets. Each row presents the scene layout, the ground truth image from which the layout was extracted, our method's results, where the object attributes present a random archetype and the location attributes are zeroed (l i = 0), our results when using the ground truth layout of the image (including masks and bounding boxes), our results where the appearance attributes of each object are copied from the ground truth image and the location vectors are zero, and our results where the location attributes coarsely describe the objects' locations and the appearance attributes are randomly selected from the archetypes. In addition, we present the result of the baseline method of [9] at the 64x64 resolution for which a model was published.\nAs can be seen, our model produces realistic results across all settings, which are more pleasing than the baseline method. Using ground truth location and appearance attributes, the resulting image better matches the test image.\nTab. 2 reports the diversity of our method in comparison to the two baseline methods. The source of stochasticity we employ (the random vector z i used in Eq. 2) produces a higher diversity than the two baseline methods (which also include a random element), even when not changing the location vector l 1 or appearance attributes a i . Varying either one of these factors adds a sizable amount of diversity. In the experiments of the table, the location attributes, when varied, are sampled using per-class Gaussian distribution that fit to the location vectors of the training set images.\nFig. 4 presents samples obtained when sampling the appearance attributes. In each case, for all i, l i = 0 and the object's appearance embedding a i is sampled uniformly between the archetypes. This results in a considerable visual diversity. Fig. 5 presents results in which the appearance is fixed to the mean appearance vector for all objects of that class and the location attribute vectors l i are sampled from the Gaussian distributions mentioned above. In almost all cases, the generated images are visually pleasing. In some cases, the location attributes sampled are not compatible with a realistic image. Note, however, that in our method, the default value for l i is zero and not a random vector.\nTab. 3 presents a comparison with the method of [9], regarding the placement accuracy of the bounding boxes. Even when not using the location attribute vectors l i , our bounding box placement better matches the test images. As expected, adding the location vectors improves the results.\nThe ability of our method to copy the appearance of an existing image object is demonstrated in Fig. 6. In this example, we generate the same test scene graph, while varying a single object in accordance with five different options extracted from images unseen during training. Despite the variability of the appearance that is presented in the five sources, the generated images mostly maintain their visual quality. These results are presented at a resolution of 256x256, which is the default resolution for our GUI. At this resolution, the system processes a graph in 16.3ms.\nUser study Following [9], we perform a user study to compare with the baseline method the realism of the generated image, the adherence to the scene graph, as well as to verify that the objects in the scene graph appear in the output image. The user study involved n = 20 computer graphics and computer vision students. Each student was shown the output images for 30 random test scene-graphs from the COCO-stuff dataset and was asked to select the preferable method, according to two criteria: \"which image is more realistic\" and \"which image better reflects the scene graph\". In addition, the list of objects in the scene graph was presented, and the users were asked to count the number of objects that appear in each of the images. The two images, one for the method of [9] and one for our method, were presented in a random order. To allow for a fair comparison, the appearance archetypes were selected at random, the location vectors were set to zero for all objects, and we have used images from our 64 \u00d7 64 resolution model. The results, listed in Tab. 4, show that our method significantly outperforms the baseline method in all aspects tested.", "publication_ref": ["b8", "b27", "b1", "b28", "b8", "b8", "b28", "b18", "b21", "b4", "b8", "b27", "b8", "b27", "b27", "b28", "b3", "b26", "b9", "b12", "b26", "b8", "b26", "b8", "b27", "b27", "b8", "b8", "b8", "b8"], "figure_ref": ["fig_1", "fig_2", "fig_3"], "table_ref": []}, {"heading": "Ablation analysis", "text": "The relative importance of the various losses is approximated, by removing it from the method and training the 128x128 model. For this study, we use both the inception and the FID scores. The results are reported in Tab. 5. As can be seen, removing each of the losses results in a noticeable degradation. Removing the perceptual loss is extremely detrimental. Out of the three discriminators, removing the mask discriminator is the most damaging, since, due to the random component z i , we do not have a direct loss on the mask. Finally, replacing our image discriminator with the one in [9], results in some loss of accuracy.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We present an image generation tool in which the input consists of a scene graph with the potential addition of location information. Each object is associated both with a location embedding and with an appearance embedding. The latter can be extracted from another image, allowing for a duplication of existing objects to a new image, where their layout is drastically changed. In addition to the dual encoding, our method presents both a new architecture and new loss terms, which leads to an improved performance over the existing baselines.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgement", "text": "This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant ERC CoG 725974).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Network architecture", "text": "The graph convolutional network used is the same as the one used in [9], which was modified in order to support the added node information. We concatenate the embedding of the objects to the location attributes creating vectors in R 128+35 . These vectors, together with the relation embedding is feed-forward into a fully-connected layer which results in a vector embedding in R 128 for each of the objects and each of the relations. The network then follows the architecture of [9], using a shared symmetric function to calculate the object vector from all the relations it participate in. Our graph convolution has overall five layers.\nTo describe the rest of the networks, we follow a semiconventional shorthand notation for defining architectures. Let C k denote a Convolution layer of k filters with a kernel size of 7x7 and a stride of 1, followed by instance normalization [23] and a ReLU activation function. Similarly, we use D k to a layer which uses a stride of 2, reflection padding, and k filters. In addition, we use B k to denote a 3x3 upsample-convolution-BatchNormalization-ReLU layer with k filters and a stride and padding of 1. We use V k to define Residual blocks with two 3x3 convolutional layers, both with k filters. Moreover, U k denotes a layer with k filters of size 3x3 and a fractional stride of 0.5, followed by instance normalization. CB k denotes a stride-2 k-filter, 4x4 convolution followed by batch normalization. GA denotes a global average pooling layer. Fully connected layers with k hidden units followed by a ReLU activation are denoted by L k . The ReLU is not applied to the L k layer, if it is the top layer.\nThe discriminators call for an even more elaborate terminology. Let C i\u2212k\u2212o denote a 3x3 Convolution layer with i input channels and k output filters, a stride of o and a padding of 1. In addition, LR denotes Leaky-ReLU with negative slope of 0.2, IN denotes Instance Normalization, and AP k denotes a 3x3 Average Pool stride 2 and padding of 1. Also, let C k\u2212s denote a Convolution layer with k filters, stride of s, kernel size of 4x4, and a padding of size 2. D k\u2212s denotes a 4x4 Convolution-InstanceNorm-LeakyReLU layer with k filters, a stride of s padding of 2 and a LeakyReLU with a negative slope of 0.2.\nThe different components of the networks can be described as:   ", "publication_ref": ["b8", "b8", "b22"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Gan dissection: Visualizing and understanding generative adversarial networks", "journal": "", "year": "2018", "authors": "David Bau; Jun-Yan Zhu; Hendrik Strobelt; Zhou Bolei; Joshua B Tenenbaum; William T Freeman; Antonio Torralba"}, {"ref_id": "b1", "title": "Coco-stuff: Thing and stuff classes in context", "journal": "", "year": "2007", "authors": "Holger Caesar; R R Jasper; Vittorio Uijlings;  Ferrari"}, {"ref_id": "b2", "title": "Generative adversarial nets", "journal": "Curran Associates, Inc", "year": "2014", "authors": "Ian Goodfellow; Jean Pouget-Abadie; Mehdi Mirza; Bing Xu; David Warde-Farley; Sherjil Ozair; Aaron Courville; Yoshua Bengio"}, {"ref_id": "b3", "title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"ref_id": "b4", "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium", "journal": "Curran Associates, Inc", "year": "2017", "authors": "Martin Heusel; Hubert Ramsauer; Thomas Unterthiner; Bernhard Nessler; Sepp Hochreiter"}, {"ref_id": "b5", "title": "Inferring semantic layout for hierarchical textto-image synthesis", "journal": "", "year": "2018", "authors": "Seunghoon Hong; Dingdong Yang; Jongwook Choi; Honglak Lee"}, {"ref_id": "b6", "title": "Image-to-image translation with conditional adversarial networks", "journal": "", "year": "2017", "authors": "Phillip Isola; Jun-Yan Zhu; Tinghui Zhou; Alexei A Efros"}, {"ref_id": "b7", "title": "Perceptual losses for real-time style transfer and super-resolution", "journal": "", "year": "2016", "authors": "Justin Johnson; Alexandre Alahi; Li Fei-Fei"}, {"ref_id": "b8", "title": "Image generation from scene graphs", "journal": "", "year": "2009", "authors": "Justin Johnson; Agrim Gupta; Li Fei-Fei"}, {"ref_id": "b9", "title": "Image retrieval using scene graphs", "journal": "", "year": "2015", "authors": "Justin Johnson; Ranjay Krishna; Michael Stark; Li-Jia Li; David A Shamma; Michael S Bernstein; Li Fei-Fei"}, {"ref_id": "b10", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2016", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b11", "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "journal": "International Journal of Computer Vision", "year": "2016", "authors": "Ranjay Krishna; Yuke Zhu; Oliver Groth; Justin Johnson; Kenji Hata; Joshua Kravitz; Stephanie Chen; Yannis Kalantidis; Li-Jia Li; David A Shamma; Michael S Bernstein; Li Fei-Fei"}, {"ref_id": "b12", "title": "Imagenet classification with deep convolutional neural networks", "journal": "Curran Associates, Inc", "year": "2012", "authors": "Alex Krizhevsky; Ilya Sutskever; Geoffrey E Hinton"}, {"ref_id": "b13", "title": "Least squares generative adversarial networks", "journal": "", "year": "2017", "authors": "Xudong Mao; Qing Li; Haoran Xie; Y K Raymond; Zhen Lau; Stephen Paul Wang;  Smolley"}, {"ref_id": "b14", "title": "Vision: A Computational Investigation into the Human Representation and Processing of Visual Information", "journal": "Henry Holt and Co., Inc", "year": "1982", "authors": "David Marr"}, {"ref_id": "b15", "title": "Conditional generative adversarial nets", "journal": "", "year": "2014", "authors": "Mehdi Mirza; Simon Osindero"}, {"ref_id": "b16", "title": "Semantic image retrieval via active grounding of visual situations", "journal": "", "year": "2018", "authors": "Max H Quinn; Erik Conser; Jordan M Witte; Melanie Mitchell"}, {"ref_id": "b17", "title": "Generative adversarial text to image synthesis", "journal": "", "year": "2016", "authors": "Scott Reed; Zeynep Akata; Xinchen Yan; Lajanugen Logeswaran; Bernt Schiele; Honglak Lee"}, {"ref_id": "b18", "title": "Improved techniques for training gans", "journal": "", "year": "2016", "authors": "Tim Salimans; Ian J Goodfellow; Wojciech Zaremba; Vicki Cheung; Alec Radford; Xi Chen"}, {"ref_id": "b19", "title": "The graph neural network model", "journal": "IEEE Transactions on Neural Networks", "year": "2009", "authors": "Franco Scarselli; Marco Gori; Ah Chung Tsoi; Markus Hagenbuchner; Gabriele Monfardini"}, {"ref_id": "b20", "title": "Very deep convolutional networks for large-scale image recognition", "journal": "", "year": "2014", "authors": "Karen Simonyan; Andrew Zisserman"}, {"ref_id": "b21", "title": "Going deeper with convolutions", "journal": "", "year": "2008", "authors": "Christian Szegedy; Wei Liu; Yangqing Jia; Pierre Sermanet; Scott Reed; Dragomir Anguelov; Dumitru Erhan; Vincent Vanhoucke; Andrew Rabinovich"}, {"ref_id": "b22", "title": "stance normalization: The missing ingredient for fast stylization", "journal": "", "year": "2016", "authors": "Dmitry Ulyanov; Andrea Vedaldi; Victor Lempitsky"}, {"ref_id": "b23", "title": "Visualizing data using t-SNE", "journal": "Journal of Machine Learning Research", "year": "2008", "authors": "Laurens Van Der Maaten; Geoffrey Hinton"}, {"ref_id": "b24", "title": "High-resolution image synthesis and semantic manipulation with conditional gans", "journal": "", "year": "2018", "authors": "Ting-Chun Wang; Ming-Yu Liu; Jun-Yan Zhu; Andrew Tao; Jan Kautz; Bryan Catanzaro"}, {"ref_id": "b25", "title": "Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks", "journal": "", "year": "2017", "authors": "Han Zhang; Tao Xu; Hongsheng Li; Shaoting Zhang; Xiaolei Huang; Xiaogang Wang; Dimitris N Metaxas"}, {"ref_id": "b26", "title": "The unreasonable effectiveness of deep features as a perceptual metric", "journal": "", "year": "2018", "authors": "Richard Zhang; Phillip Isola; Alexei A Efros; Eli Shechtman; Oliver Wang"}, {"ref_id": "b27", "title": "Image generation from layout. CoRR, abs/1811.11389", "journal": "", "year": "2005", "authors": "Bo Zhao; Lili Meng; Weidong Yin; Leonid Sigal"}, {"ref_id": "b28", "title": "Multiple source domain adaptation with adversarial learning", "journal": "", "year": "2018", "authors": "Han Zhao; Shanghang Zhang; Guanhang Wu; Jo ; P Costeira; M F Jos\u00e9; Geoffrey J Moura;  Gordon"}], "figures": [{"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 .3Figure 3. Image generation based on a given scene graph. Each row is a different example. (a) the scene graph, (b) the ground truth image, from which the layout was extracted,(c) our results when we used the ground truth layout of the image, similar to[28], (d) our method's results, where the appearance attributes present a random archetype and the location attributes coarsely describe the ground truth bounding box, (e) our results when we use the ground truth image to generate the appearance attributes, and the location attributes are zeroed li = 0, (f) our results where li = 0, and the appearance attributes are sampled from the archetypes, and (g) the results of[9].", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 .4Figure 4. The diversity obtained when keeping the location attributes li fixed at zero and sampling different appearance archetypes. (a) the scene graph, (b) the ground truth image, from which the layout was extracted, (c-g) generated images.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 .5Figure 5. The diversity obtained when keeping the appearance vectors fixed and sampling from the location distribution. (a) the scene graph, (b) the ground truth image from which the layout was extracted, (c-g) generated images.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Comparison of predicted bounding boxes", "figure_data": "User Study[9]OursMore realistic output16.7%83.3%Better adherence to scene graph19.3%80.7%Ratio of observed objects27.31% 45.38%among all COCO objectsRatio of observed objects46.49% 65.23%among all COCO stuffTable 4. User study resultsModelInceptionFIDFull method10.4 \u00b1 0.475.4No Lperceptual6.2 \u00b1 0.1125.1No LD-mask5.2 \u00b1 0.1183.6No LD-image7.4 \u00b1 0.2122.5No LD-object8.7 \u00b1 0.194.5Using Dimage of [9]8.1 \u00b1 0.3114.2Table 5. Ablation Studypled (see 3.2). [29] employs bounding boxes and not masks.However, we follow the same comparison (to masked basedmethods) given in their paper."}], "formulas": [{"formula_id": "formula_0", "formula_text": "ing box b i = [x 1 , y 1 , x 2 , y 2 ] \u2208 [0, 1] 4", "formula_coordinates": [3.0, 308.86, 334.11, 156.41, 11.23]}, {"formula_id": "formula_1", "formula_text": "u i = G({n i }, {e ij }) (1) m i = M (u i , z i ) (2) b i = B(u i )(3)", "formula_coordinates": [3.0, 314.28, 564.11, 103.26, 39.54]}, {"formula_id": "formula_2", "formula_text": "a i = A(I i ) (4) t = T ({c i , m i , b i , a i }) (5) p = R(t)(6)", "formula_coordinates": [3.0, 427.55, 564.11, 115.2, 38.84]}, {"formula_id": "formula_3", "formula_text": "t = T ({c i , m i , b i , a i }) (7) t = T ({c i , m i , b i , a k })(8)", "formula_coordinates": [4.0, 119.42, 301.09, 166.94, 25.56]}, {"formula_id": "formula_4", "formula_text": "L = L Rec +\u03bb 1 L box +\u03bb 2 L perceptual +\u03bb 3 L D-mask +\u03bb 4 L D-image + \u03bb 5 L D-object + \u03bb 6 L FM-mask + \u03bb 7 L FM-image", "formula_coordinates": [4.0, 60.08, 474.76, 225.79, 24.75]}, {"formula_id": "formula_5", "formula_text": "L perceptual = u\u2208U 1 u ||F u (p) \u2212 F u (p )|| 1", "formula_coordinates": [4.0, 60.63, 608.3, 225.73, 23.55]}, {"formula_id": "formula_6", "formula_text": "L D\u2212mask = [log D mask (m i , c i )]+ E z\u223cN (0,1) 64 [log(1 \u2212 D mask (M (u i , z), c i )] (10)", "formula_coordinates": [4.0, 318.83, 123.68, 226.29, 30.93]}, {"formula_id": "formula_7", "formula_text": "L D-image = L real \u2212 L fake-image \u2212 L fake-layout + L alt-appearance where L real = log D image (t , p ) (11) L fake-image = log(1 \u2212 D image (t , p)) (12) L fake-layout = log(1 \u2212 D image (t, p )) (13) L alt-appearance = log(1 \u2212 D image (t , p ))(14)", "formula_coordinates": [4.0, 308.86, 224.16, 236.25, 97.89]}, {"formula_id": "formula_8", "formula_text": "L D-object = n i=1 log D object (I i ) \u2212 log D object (I i )(15)", "formula_coordinates": [4.0, 326.15, 544.01, 218.96, 30.32]}, {"formula_id": "formula_9", "formula_text": "(b) (c) (d) (e) (f) (g)", "formula_coordinates": [6.0, 151.99, 511.45, 370.57, 8.64]}], "doi": ""}