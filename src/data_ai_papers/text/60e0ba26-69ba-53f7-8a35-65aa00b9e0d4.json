{"title": "SimFair: A Unified Framework for Fairness-Aware Multi-Label Classification", "authors": "Tianci Liu; Haoyu Wang; Yaqing Wang; Xiaoqian Wang; Lu Su; Jing Gao", "pub_date": "2023-02-22", "abstract": "Recent years have witnessed increasing concerns towards unfair decisions made by machine learning algorithms. To improve fairness in model decisions, various fairness notions have been proposed and many fairness-aware methods are developed. However, most of existing definitions and methods focus only on single-label classification. Fairness for multilabel classification, where each instance is associated with more than one labels, is still yet to establish. To fill this gap, we study fairness-aware multi-label classification in this paper. We start by extending Demographic Parity (DP) and Equalized Opportunity (EOp), two popular fairness notions, to multilabel classification scenarios. Through a systematic study, we show that on multi-label data, because of unevenly distributed labels, EOp usually fails to construct a reliable estimate on labels with few instances. We then propose a new framework named Similarity s-induced Fairness (s\u03b3-SimFair). This new framework utilizes data that have similar labels when estimating fairness on a particular label group for better stability, and can unify DP and EOp. Theoretical analysis and experimental results on real-world datasets together demonstrate the advantage of s\u03b3-SimFair over existing methods on multi-label classification tasks.", "sections": [{"heading": "Introduction", "text": "Nowadays, machine learning algorithms play increasingly more important roles in decision-making for a broad spectrum of applications, such as applicant screening in job markets, credit risk analysis, and recommendation systems. However, recent studies (Barocas and Selbst 2016;Buolamwini and Gebru 2018;Dressel and Farid 2018) have discovered that machine learning algorithms tend to make discriminatory decisions. For example, a dataset may contain records of physicians most of whom are male. As a result, a job screening algorithm trained on this dataset may unfairly predict if a person is suitable for a physician position based on their gender, instead of education background or professional experience. Obviously, such favorable prediction for male applicants is unfair to female applicants.\nFormally, the algorithmic fairness issue refers to the phenomenon that machine learning algorithms make discriminatory decisions across different demographic subgroups and give favorable predictions for some particular subgroups. Intuitively, discriminatory decisions are associated with some demographic features contained in the data, such as age, gender, and race. These features are referred to as sensitive features. Ideally, a fair model should be able to make decisions independent of sensitive features. Towards this end, different fairness notions (Pedreshi, Ruggieri, and Turini 2008;Dwork et al. 2011;Hardt, Price, and Srebro 2016; Chouldechova and Roth 2020) have been proposed. Among them, Demographic Parity (DP) (Pedreshi, Ruggieri, and Turini 2008) and Equalized Opportunity (EOp) (Hardt, Price, and Srebro 2016) are two of the most widely-used definitions. DP requires a model's decision to be independent of sensitive features, achieving a population-level fairness (Edwards and Storkey 2015;Madras et al. 2018;Creager et al. 2019). However, Dwork et al. (2011) showed that such population level fairness does not necessarily guarantee fairness in all label groups. To address this limitation, Hardt, Price, and Srebro (2016) proposed to take label information into consideration and defined EOp and its stronger version Equalized Odds (EO). Specifically, EOp requires the decision to be independent of sensitive features conditionally in the label group receiving an favorable outcome (Hardt, Price, and Srebro 2016). Examples of favorable outcomes include \"being admitted to a position\" in job screening, and \"approval of credit card application\". For brevity, we refer to the label group in which each individual receives the favorable outcome as the advantaged group. With a more restrictive fairness definition, EO further requires that the decision is independent of sensitive features in each label group, including not only the advantaged group but also the groups receiving other outcomes.\nSome methods have been proposed based on the aforementioned fairness definitions. However, they are focused only on scenarios where each instance is associated with a single target label (Hardt, Price, and Srebro 2016;Woodworth et al. 2017;Zafar et al. 2017). In many real-world applications, multiple labels need to be predicted for an instance. For example, in job screening, an applicant may apply for multiple positions, and the admission decision of each position is a target label of the applicant. Similarly, undergraduates usually submit applications to multiple programs when applying to graduate schools, and thus associate themselves with multiple target labels of admission. Scenarios where each instance is associated with more than one target labels are termed as multi-label classification (Zhang and Zhou 2014). Obviously, fairness concerns also exist in multi-label classification scenarios. One straightforward approach toward fairness in a multi-label scenario is to decompose multi-label classification into multiple binary classification tasks, each of which judges whether a label is associated with an instance or not, and then apply existing fairness metric separately on each binary classification task (Zhang et al. 2018). However, this naive approach ignores one unique property of multi-label classification, i.e., the correlations among labels. Again, take job screening as an example. Applicants usually apply for positions with similar requirements of skill sets and experiences at the same time, and thus the application outcomes (labels) are correlated. Ignoring such correlations among labels would lead to unsatisfactory classification and let alone fairness results. On the other hand, existing multi-label classification methods consider the correlations among labels but cannot enforce fairness in the predictions.\nTherefore, it is critical to define fairness directly in the context of multi-label classification. Unfortunately, we did not find existing work along this direction. This motivates us to study this problem. In a multi-label scenario, since different target labels usually occur together, it is more natural to treat their combinations as an advantaged outcome (label). For example, the advantaged group in the job screening example with two possible positions can be the applicants who \"received offers of position A and position B\". Note that this definition allows us to define more general and complex advantaged groups by specifying more than one favorable labels and requires fairness on all of them.\nIn practice, the discussed fairness objective is usually achieved by incorporating some fairness notions into optimization (Mohler et al. 2018;Scutari, Panero, and Proissl 2021). Such an optimization is non-trivial when tackling fairness issue based on this extended concept of advantaged group in multi-label classification, where collected data is usually not evenly distributed among different labels (Dekel and Shamir 2010). When few instances are in the advantaged group (i.e., the group that has the favorable label), it may introduce unreliable fairness constraints into optimization and degrade the fairness performance. In this work, we show that the aforementioned optimization challenges can be alleviated by utilizing information sharing among labels. Intuitively, we group data with different but similar labels to alleviate data shortage issue, and then enable an EOp-like framework to incorporate fairness constraints on advantaged groups.This will be formalized in Section 3. We refer to our framework as Similarity s-induced Fairness (s \u03b3 -SimFair), highlighting the crucial requirement of a similarity measure between different labels in the data grouping step.\nThe proposed framework s \u03b3 -SimFair is principled in the sense that it unifies DP and EOp, bringing the flexibility of leveraging a population level fairness or fairness on some particular label groups (e.g., the advantaged group) per desires. The DP and EOp are two extreme cases of s \u03b3 -SimFair. When treating all labels as equally similar and ignoring their differences, we end up with one label group of data, in which s \u03b3 -SimFair becomes DP. On the contrary, if two labels are similar only if they are the same, then each label group involves only one label, in which s \u03b3 -SimFair becomes EOp. Moreover, s \u03b3 -SimFair is able to enforce a restrictive fairness notion on the advantaged group even when data is inadequate by utilizing information from other similar label groups.\nOur main contributions are summarized below.\n\u2022 To the best of our knowledge, we are the first to investigate fairness in a multi-label classification setting. We extend DP and EOp to the multi-label classification setting, and recognize the challenge of achieving EOp based on both theoretical and empirical studies.\n\u2022 To handle the recognized challenge, we propose a novel framework, namely s \u03b3 -SimFair, to achieve the fairness objective for multi-label classification even when imbalanced label distributions exist. We further support the proposed framework with rigorous theoretical analysis.\n\u2022 The comprehensive experiments show that the proposed framework s \u03b3 -SimFair is able to achieve competitive and even better performance in term of DP and EOp compared to that of directly incorporating DP and EOp into optimization respectively.", "publication_ref": ["b2", "b4", "b12", "b29", "b13", "b18", "b29", "b18", "b14", "b25", "b10", "b13", "b18", "b18", "b18", "b35", "b39", "b43", "b41", "b27", "b34", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work Algorithmic Fairness", "text": "Most existing fairness definitions fall into two categories: group fairness (Pedreshi, Ruggieri, and Turini 2008;Dwork et al. 2011;Hardt, Price, and Srebro 2016;Chouldechova and Roth 2020) and individual fairness (Dwork et al. 2011).\nGroup fairness requires that the probability of being assigned to a group by a model is independent of sensitive features such as gender, age and race. For example, Demographic Parity (DP) requires that the prediction is independent of sensitive features, while Equalized Odds (EO) and Equalized Opportunity (EOp) require that the prediction is conditionally independent of sensitive features in each or some label group. When labels are binary, this is equivalent to requiring an equality of true and false positive rates across different demographic subgroups. Modifications of DP and EO (EOp) have also been studied. For example, in Pleiss et al. (2017), a relaxed condition is required by replacing EO with some calibration. Individual fairness, on the other hand, requires that a model treats similar individuals similarly (Dwork et al. 2011). In this work we focus on group fairness. In order to correct the unfairness of models, many methods have also been proposed, which can be classified into one of the following three categories: pre-processing biased datasets, in-processing models during training, and post-processing the outputs of models. In-processing is usually the most effective way to intervene an unfair model (Petersen et al. 2021), which can be done by penalizing unfair predictions directly (Mohler et al. 2018;Scutari, Panero, and Proissl 2021), or by disentangling some intermediate representations (on which final predictions are made) from sensitive features (Locatello et al. 2019;Creager et al. 2019). Nevertheless, penalty-based methods are still good and effective starting points to mitigate unfairness (Mary, Calauzenes, and El Karoui 2019;Kamishima et al. 2012).", "publication_ref": ["b29", "b13", "b18", "b8", "b13", "b31", "b13", "b30", "b27", "b34", "b24", "b10", "b26", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Multi-label Classification", "text": "Multi-label classification is a general family of classification tasks where each instance is associated with multiple target labels. This task has very broad applications (El Kafrawy, Mausad, and Esmail 2015), such as recommendation systems (Zheng, Mobasher, and Burke 2014;Zhang et al. 2020), multi-object detection (Gong et al. 2019;Zhao et al. 2020), and text classification (Yang et al. 2009;Nam et al. 2014).\nMethods for multi-label classification can be grouped into two categories (Zhang and Zhou 2014;Tsoumakas, Katakis, and Vlahavas 2006): problem transformation and algorithm adaptation. Problem transformation tackles multi-label classification by transforming the task into other well-defined tasks. One possible transformation is binary relevance (Boutell et al. 2004), which ignores all dependencies among different targets and predicts each target separately. Classifier chain, the other extreme case, learns the joint distribution of different labels by applying the chain rule of probability (Read et al. 2011). In summary, these problem transformation multi-label classification tasks into other well-established learning problems and adopt existing methods to solve them (Tsoumakas and Vlahavas 2007;F\u00fcrnkranz et al. 2008). Algorithm adaptation, on the other hand, modify existing algorithms such as kNN (Zhang and Zhou 2007) and decision tree (Clare and King 2001) to model multi-label data directly. We refer readers to Zhang and Zhou (2014); Tsoumakas, Katakis, and Vlahavas (2006) for more details.\nDeep learning has advanced multi-label classification as well (Liu et al. 2021). Recently, Chen, Xue, and Gomes (2018); Bai, Kong, and Gomes (2020) revisited the Multivariate Probit (MP) model (Chib and Greenberg 1998) with the equipment of deep learning tools. MP model assumes that the joint distribution of labels is controlled by a multivariate Gaussian random variable, and the probability of a label is determined by the cumulative density function (CDF) at the value of this Gaussian variable. The correlations in the Gaussian variable allows the model to capture pairwise dependencies in a multi-label setting. Chen, Xue, and Gomes ( 2018) parameterized the MP model with a deep neural network resulting in the deep Multivariate Probit model (DMVP), and Bai, Kong, and Gomes (2020) proposed to combine DMVP and variational autoencoder (Kingma and Welling 2014) to obtain better performance.", "publication_ref": ["b15", "b45", "b40", "b17", "b44", "b37", "b28", "b43", "b34", "b3", "b32", "b35", "b16", "b42", "b9", "b43", "b34", "b23", "b7", "b0", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Methodology", "text": "In this section, we propose s \u03b3 -SimFair, a flexible framework to unify Demographic Parity (DP) and Equalized Opportunity (EOp). We start with deriving DP and EOp in multi-label scenarios. Then we provide a systematic study on the challenges of estimating EOp in multi-label scenarios. We propose s \u03b3 -SimFair based on the these studies to achieve the fairness objective even when imbalanced label distributions exist.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "Notations Throughout this paper, we use bold capital letters (e.g., X) to denote matrices, bold lowercase letters (e.g., x) to denote (column) vectors, and calligraphic letters (e.g., X ) to denote spaces. Finally, capital P denotes a probability and lowercase p denotes a distribution. We summarize notations used in this paper in appendix A for better readability.\nConsider a dataset that contains N samples D = {(x (i) , a (i) , y (i) )} N i=1 . Without loss of generality, we assume each sample is associated with M non-sensitive features x (i) \u2208 X = R M , a K-way scalar sensitive feature a (i) \u2208 A = {1, . . . , K} where K is the number of demographic subgroups (e.g., K = 2 if gender is the sensitive feature that takes female and male), and L binary labels y (i) \u2208 Y = {0, 1} L . We further assume N samples are drawn from an unknown underlying distribution p over space (X \u00d7 A \u00d7 Y), and use (x, a, y) \u223c p to denote a random sample. To avoid ambiguity, for y = (y 1 , . . . , y L ), we call y a label, and y l \u2208 {0, 1} the l-th target, where y l = 1 indicates the presence of l-th target. We use h : X \u2192 Y to denote a multi-label classifier that predicts label based on non-sensitive features. Under these settings, L = 1 corresponds to single-label classification, and L > 1 corresponds to multi-label classification.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Multi-Label Classification Prediction", "text": "We consider a wide family of multi-label classifiers that satisfy h = f \u2022 g :\nX \u2192 [0, 1] L \u2192 Y.\nIn particular, a classifier first predicts\u1ef9 = g(x), the probability of the presence of L targets given x. Then l-th target prediction is given by\u0177 l = 1(\u1ef9 l \u2265 0.5) elementwisely, in which f denotes this elementwise thresholding function. This family of classifiers is capable of capturing dependencies between different targets by predicting\u1ef9 given x jointly as shown in Chen, Xue, and Gomes (2018); Bai, Kong, and Gomes (2020).", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "DP and EOp on Multi-Label Classification", "text": "DP and EOp Condition In this section, we establish DP and EOp condition in multi-label scenarios. For classifier h = f \u2022 g : X \u2192 Y and random sample (x, a, y) \u223c p, h is fair in terms of ( 1) DP if\u0177 \u22a5 a; and (2) EOp if\u0177 \u22a5 a | y adv , where y adv \u2208 Y denotes some advantaged label where only favorable outcomes (e.g., \"received offer\" in the job screening example) present. In essence, DP requires predictions to be independent with sensitive variables, and EOp requires conditional independence to hold on label y adv . As assumed, prediction\u0177 depends on predicted probability\u1ef9 elmentwisely, therefore distribution of\u0177 is fully parameterized by\u1ef9. Proposition 3.1 gives a condition for DP and EOp to hold in multilabel classification.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proposition 3.1 (DP and EOp condition for multi-label classifier). For a multi-label classifier that takes the form", "text": "h = f \u2022 g, where\u1ef9 = g(x)\nis the predicted probability and y = f (\u1ef9) is computed elementwisely, DP and EOp hold if for any k \u2208 A\nDP: E[\u1ef9 | a = k] = E[\u1ef9] EOp: E[\u1ef9 | a = k, y = y adv ] = E[\u1ef9 | y = y adv ]. (1)\nProof. See appendix B. Remark 1. Proposition 3.1 indicates that on multi-label data where labels are correlated, for classifier h, we can still evaluate its fairness performances in the same way as evaluating traditional single label classifiers by comparing the averaged predicted probability on different subgroups. Moreover, we can construct estimations with finite samples\nE[\u1ef9 | a = k] \u2248 N i=1\u1ef9 (i) 1(a (i) = k) N i=1 1(a (i) = k) E[\u1ef9] \u2248 1 N N i=1\u1ef9 (i) E[\u1ef9 | a = k, y = y adv ] \u2248 N i=1\u1ef9 (i) 1(a (i) = k)1(y = y adv ) N i=1 1(a (i) = k)1(y = y adv ) (2) E[\u1ef9 | y = y adv ] \u2248 N i=1\u1ef9 (i) 1(y = y adv ) N i=1 1(y = y adv )(3)\nEstimation Challenge of EOp In multi-label scenarios, a long-tailed phenomenon, i.e., most labels only associate with few samples (Dekel and Shamir 2010), brings additional challenges for EOp estimation. Without sufficient samples, EOp is barely able to construct reliable estimates for fairness and correspondingly may not achieve fairness objective in an in-processing framework on such datasets. Mathematically, the challenge of estimating EOp stems from terms 2) and (3). When these summations are close to 0, the two estimates are unstable or even undefined. More formally, the conditional expectation in eqn ( 1) is\nN i=1 1(a (i) = k)1(y (i) = y adv ) and N i=1 1(y (i) = y adv ) in eqn (\nE[\u1ef9 | a = k, y = y adv ] = \u1ef9p(\u1ef9 | a = k, y = y adv ) d\u1ef9 = \u1ef9p(\u1ef9, a = k, y = y adv ) d\u1ef9 P (a = k, y = y adv ) .\nHere P (a = k, y = y adv ) = E[1(a = k)1(y = y adv )], and\n\u1ef9p(\u1ef9, a = k, y = y adv ) d\u1ef9 = \u1ef91(a = k)1(y = y adv )p(\u1ef9, a, y) da dy d\u1ef9 = E[\u1ef91(a = k)1(y = y adv )].\nThis implies\nE[\u1ef9 | a = k, y = y adv ] = E[\u1ef91(a = k)1(y = y adv )] E[1(a = k)1(y = y adv )](4)\nE[\u1ef9 | y = y adv ] = E[\u1ef91(y = y adv )] E[1(y = y adv )] .(5)\nHenceforth, eqn ( 1) is equivalent to\nE[\u1ef91(y = y adv )] E[1(y = y adv )] = E[\u1ef91(a = k)1(y = y adv )] E[1(a = k)1(y = y adv )](6)\nfor \u2200 k \u2208 A. If event 1(y = y adv ) = 1 happens with low probability, i.e., few samples are from label group y adv , EOp is difficult and even impossible to estimate from eqn (2) and ( 3) directly.\nSimilarity s-induced Fairness (s \u03b3 -SimFair)\nMotivated by the above analysis, we propose a new framework to help achieve DP or EOp, where hard 1(y = y adv ) \u2208 {0, 1} is relaxed to some similarity function s(y, y adv ) \u2208 [0, 1]. Informally, we loosen the membership of the advantaged group requirement in EOp and use a soft conditioning.\nFor any random sample (x, a, y), fairness of its prediction is always taken in consideration, but as the affinity of y to y adv decreases, it will be down-weighted when estimating fairness violations with respect to y adv . Definition 1 (s \u03b3 -SimFair). Given a similarity function s :\nY \u00d7 Y \u2192 [0, 1], a multi-label classifier h satisfies Similarity s-induced Fairness (s \u03b3 -SimFair) if for \u2200 k \u2208 A, E[\u1ef9s(y, y adv )] E[s(y, y adv )] = E[\u1ef91(a = k)s(y, y adv )] E[1(a = k)s(y, y adv )] .(7)\nSame as DP and EOp, terms involved in eqn ( 7) can be estimated with\nE[\u1ef9s(y, y adv )] E[s(y, y adv )] \u2248 i\u1ef9 (i) s(y (i) , y adv ) i s(y (i) , y adv )(8)\nE[\u1ef91(a = k)s(y, y adv )] E[1(a = k)s(y, y adv )] \u2248 i\u1ef9 (i) 1(a (i) = k)s(y (i) , y adv ) i 1(a (i) = k)s(y (i) , y adv ) .(9)\nIn this paper, we adopt the Jaccard score to define similarity s. In essence, we use the cardinality ratio between the intersection and union of pair (y, y ) to measure their similarity, then apply some monotonic transformation for scaling. Formally, for y \u2208 Y with y l = 1 represents the presence of the l-th target, we denote cate(y) = {l : y l = 1, l = 1, . . . , L}, i.e., the collection of indices of present targets, and define\nJac(y, y adv ) = |cate(y) \u2229 cate(y adv )| |cate(y) \u222a cate(y adv )| s \u03b3 (y, y adv ) = exp (\u03b3 (Jac(y, y adv ) \u2212 1))\nwhere \u03b3 is a scaling parameter. It is worth mentioning that the choice of s is not unique and can be task-or data-specific. For any multi-label classifier h satisfying s \u03b3 -SimFair, its violation of DP will be arbitrarily small if \u03b3 is sufficiently small; and its violation of EOp will be arbitrarily small if \u03b3 is sufficiently large. More generally, its violation of DP is arbitrarily close to its violation of s \u03b3 -SimFair for sufficiently small \u03b3, and its violation of EOp is arbitrarily close to its violation of s \u03b3 -SimFair for sufficiently large \u03b3.  \n) (h), is defined as K k=1 E[\u1ef9s\u03b3(y, y adv )] E[s\u03b3(y, y adv )] \u2212 E[\u1ef91(a = k)s\u03b3(y, y adv )] E[1(a = k)s\u03b3(y, y adv )] (10\n)\nwhere \u2022 is the L 2 norm. In words, we count how the fairness conditions in eqn ( 7) are violated in all demographic subgroup a = k. When K = 2 (i.e., the sensitive feature is binary), it can also be writen as\nE[\u1ef91(a = 1)s\u03b3(y, y adv )] E[1(a = 1)s\u03b3(y, y adv )] \u2212 E[\u1ef91(a = 2)s\u03b3(y, y adv )] E[1(a = 2)s\u03b3(y, y adv )] .(11)\nDP and EOp, as discussed, are special cases of s \u03b3 -SimFair so we omit their forms.\nIn-processing with s \u03b3 -SimFair We use s \u03b3 -SimFair to improve fairness of classifier h in an in-processing framework. Specifically, on each mini-batch during training, we estimate the fairness violation (defined in eqn ( 10) or ( 11)) by eqn ( 8) and ( 9). The estimate defines the regularization term as parts of training loss. In particular, we train h with stochastic gradient descent-based methods by minimizing min\nh mlc (h) + \u03bb s\u03b3 (y,y adv ) (h). (12\n)\nHere mlc (h) is the loss for multi-label classification, and s\u03b3 (y,y adv ) (h) estimates the violation of s \u03b3 -SimFair. Hyperparameter \u03bb \u2265 0 balances the two losses.", "publication_ref": ["b11"], "figure_ref": [], "table_ref": []}, {"heading": "Multivariate Probit Variational AutoEncoder (MPVAE)", "text": "We use MPVAE as a backbone model to illustrate and verify the performance of our work. MPVAE is a multi-label classification method without fairness constraint enforcement, and we adapt it with a fairness penalty to ensure s \u03b3 -SimFair. MPVAE is a variational autoencoder structured model that is capable to capture pairwise dependency in label y. It learns two encoders to map x and y into a shared representation space and decode with the same decoder. A Multivariate Probit (MP) model is used to predict\u0177, and model correlations between different labels y d and y d . Fig. 1 illustrates the structure of MPVAE, where green color marks the additional fairness penalty. Algorithm 1 in appendix C provides a concise summary of updating MPVAE with one step on a minibatch. Due to the page limitation, we refer readers to Bai, Kong, and Gomes (2020) for details about MPVAE.", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Experiments", "text": "In this section, we evaluate s \u03b3 -SimFair with the goal of providing insights from three aspects:\n\u2022 How does s \u03b3 -SimFair approximate DP and EOp?\n\u2022 How does s \u03b3 -SimFair help achieve DP and EOp?\n\u2022 How does s \u03b3 -SimFair affect fairness-accuracy tradeoff?\nIn the following, we will discuss experiment settings first and then present the details about the evaluation from these three aspects.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Datasets and Experiment Setup", "text": "Datasets Due to the lack of existing work in fairness-aware multi-label classification, we transform two tabular datasets  that are ubiquitous in fairness literature into multi-label settings. Towards this goal, we select some features and treat them as additional targets. To help focus on the challenge brought by multi-label, we use binary sensitive features, but as defined in eqn (3), our methods can easily generalize to where more complicated sensitive features are used 1 .\n[) | ] [) | ] ) ) [) ] [) ]\n\u2022 Adult (Kohavi 1996) is a widely-used fairness dataset from UCI repository that contains 48,842 samples. Original Adult dataset contains 112 features and a binary label income level, which denotes whether an individual's yearly income is greater than $50K dollars or not. We further use workclass and occupation as two other targets. In terms of sensitive features, we follow Reddy et al. (2021) and binarize age into 25-44 years old and else. This allows us to construct two balanced demographic subgroups.\n\u2022 Credit (Yeh and Lien 2009) is another popular fairness dataset from UCI repository. It contains 30,000 samples, each sample is associated with 24 features and a binary label indicates the existence of default payments. We treat education level as an additional target, and use gender as the sensitive feature.\nBaselines We compare MPVAE h trained with proposed s \u03b3 -SimFair regularizer with three baseline methods: (1) No regularizer: use mlc loss only by setting \u03bb = 0 in eqn ( 12  (example-F1) as defined below\nmicro-F1 = 2 L l=1 N i=1 y (i) l\u0177 (i) l L l=1 N i=1 (y (i) l +\u0177 (i) l ) macro-F1 = 1 L L l=1 2 N i=1 y (i) l\u0177 (i) l N i=1 (y (i) l +\u0177 (i) l ) example-F1 = 1 N N i=1 2 L l=1 y (i) l\u0177 (i) l L l=1 (y (i) l +\u0177 (i) l ) .\nThese metrics compute either F1-score over the label matrix or averaged F1-score over targets or samples. Implementation Details For s \u03b3 -SimFair, we use \u03b3 = 1, 5, 10 to illustrate when it approximates DP or EOp. We also vary \u03bb, the coefficient of fairness loss s\u03b3 (y,y adv ) (h), from 1 to 5000 to study the trade-off between fairness and accuracy (in terms of micro-, macro-, and example-F1). We randomly choose 70% data for training and 30% for testing. Other hyperparameters for MPVAE training such as batch size, epochs, and learning rates are fixed throughout all experiments. A full list of hyperparameters used in this paper is provided in appendix D.", "publication_ref": ["b22", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "Estimate DP and EOp with s \u03b3 -SimFair", "text": "We first evaluate how well s \u03b3 -SimFair can approximate DP and EOp to answer RQ1. To do so, we train a MPVAE without any regularizers for 20 epochs on Adult and Credit datasets and evaluate how it violates DP and EOp. We choose the largest label group (i.e., the label that appears most frequent) as the advantaged group. This allows us to construct a reliable estimate of EOp, which could be used as the ground truth.\nFigure 2 shows how fairness violations estimated by s \u03b3 -SimFair change under different \u03b3, with DP and EOp marked on the left and right y-axis. From the figure, the starting points of s \u03b3 -SimFair curves at \u03b3 = 0.1 locate close to DP, and the ending points at \u03b3 = 10 are close to EOp; these observations justify the effectiveness of s \u03b3 -SimFair in approximating DP and EOp, consistent with theoretical analysis.\nNext, we study the robustness of three estimators by varying the numbers of samples in the advantaged group to different levels and evaluating how estimates of DP and EOp change. Results summarized in Table 1 are averaged over 10 independent replications. Empirically, EOp estimator degrades drastically as the size of observed advantaged group decreases. s \u03b3 -SimFair with large \u03b3(= 5), in contrary, produces more stable EOp estimates when EOp estimator fails.  In terms of DP, both its own and s \u03b3 -SimFair estimator produce similarly stable results, which is reasonable as we only decrease the size of the advantaged group.", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": ["tab_3"]}, {"heading": "Performance of Regularization", "text": "After showing that s \u03b3 -SimFair can approximate DP and EOp well, we evaluate how well it can help achieve DP and EOp. We start with reporting fairness violations of MPVAE trained with DP, EOp, and s \u03b3 -SimFair regularizers. On each dataset, two potential advantaged groups are considered. The first group is the largest label group as in the last subsection, and the second group is chosen to be a small label group but we can still estimate EOp on the test set. For Adult dataset, since it has more labels, we choose the 18-th largest label group, which is the smallest one that has more than 100 test samples from the advantaged group out of 152 possible labels. For Credit dataset, we choose the 9-th largest, this group has at least 10 test samples from the advantaged label out of 13 possible labels. Throughout experiments, we fix \u03bb = 10 and run 10 replications to smooth out randomness.\nTable 2 shows resultant DP and EOp achieved by different methods. In all experiments, s \u03b3 -SimFair performs competitive to DP regularizer and better than EOp regularizer in terms of minimizing these metrics as objectives. Notably, when the advantaged group is small, the vanilla EOp regularizer mitigates EOp violation poorly, but s \u03b3 -SimFair still reduces it significantly. Moreover, s \u03b3 -SimFair maintains a better DP-EOp balance, even they are known to be incompatible (Barocas, Hardt, and Narayanan 2017). For example, s 1 -SF regularzier helps achieve better DP and EOp simultaneously than a DP regularizer on the largest label group on Adult dataset. We interpret this observation as a byproduct of the biased estimation given by s \u03b3 -SimFair. As s \u03b3 -SimFair is biased towards DP (EOp) when estimating EOp (DP), such bias implicitly considers the other metric and hence strikes a batter balance. These results clearly establish the power of s \u03b3 -SimFair in minimizing DP and EOp.\nTo better reveal the limitation of EOp regularizer, we further evaluate how fair a model can be achieved by the use of different methods. To do so, we choose a large \u03bb = 5000. Note that such large \u03bb, will be shown shortly, significantly impedes accuracy. Here we sacrifice all accuracy to check the potential of different methods.  We run 3 replications on top 18 largest label groups in Adult dataset and top 9 largest label groups in Credit dataset as advantaged group separately 2 . Figure 3 shows resultant DP and EOp achieved by different methods. Compared to EOp regularizer, which performs the worst on all labels, s \u03b3 -SimFair is much more stable. In extreme cases, s \u03b3 -SimFair, as a good approximation of EOp, also encounter failure ultimately, but it is much more robust.  ", "publication_ref": ["b1"], "figure_ref": ["fig_5"], "table_ref": ["tab_5"]}, {"heading": "Fairness-Accuracy Tradeoff", "text": "We end up this section with a study on the tradeoff between fairness and accuracy on the two label groups from the previous section. Hyperparameter \u03bb varies from 1 to 5000 and results are averaged over 10 replications. Due to the page limit, we only report EOp-accuracy tradeoffs on Credit dataset in Figure 4 here and defer other figures to appendix E. Nevertheless, conclusions drawn here apply to all experiments.\n2 As described above, these groups have sufficient test samples to check violations. Overall, micro-and example-F1 are much more robust to fairness requirement than macro-F1. On Credit dataset, they are even improved slightly when a small fairness regularization is added. We hypothesize that fairness regularization indirectly adds smooth conditions and penalizes unstable predictions. s \u03b3 -SimFair has similar tradeoff patterns compared with the DP regularizer and does not encounter instability as EOp regularizer does. In addition, on small label groups where EOp regularizer fails, its s \u03b3 -SimFair approximation succeeds in achieving low EOp violation, and performs one of the best in handling tradeoffs.", "publication_ref": [], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "Conclusions", "text": "In this paper, We study the important problem of enforcing fairness on multi-label classification. Given the ubiquitous imbalanced issue with respect to label groups, we propose s \u03b3 -SimFair, an effective framework that helps achieve existing group fairness metric: DP and EOp. We first establish a formal extension of DP and EOp condition to multi-label scenarios, then prove that (extended) DP and EOp can be exactly expressed by s \u03b3 -SimFair, and can be approximated arbitrarily well. Experiments on two real-world datasets echos with theoretical analysis and reveals limitations of EOp regularizer. s \u03b3 -SimFair, in contrary, shows strong robustness against the challenges EOp regularizer cannot overcome.\ns \u03b3 -SimFair is a general tool. The concept and technique derived in this paper can be applied to multi-class classification as well, so long as a proper similarity function can be defined in the label space Y. In the future, we plan to further conduct theoretical analysis on s \u03b3 -SimFair regularizer and convergence, and apply s \u03b3 -SimFair in a post-processing framework.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Notation Table", "text": "We summarize notations used in this paper in table 3.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Notations Meaning", "text": "x \u2208 X Non-sensitive feature and non-sensitive feature space. a \u2208 A Sensitive feature and sensitive feature space. y \u2208 Y = {0, 1} L Label and label space.\nh = f \u2022 g : X \u2192 Y A composited multi-label classifier, f : X \u2192 [0, 1] L and g : [0, 1] L \u2192 Y. y = h(x) = g(f (x)) Predicted label (Prediction). y = f (x)\nPredicted probability vector. s \u03b3 (y, y )\nSimilarity between label y and y . \u03b3 \u2265 0\nScaling hyperparameter in similarity. \u03bb \u2265 0\nCoefficient of fairness penalty. mlc (h)\nMulti-label classification loss.\ns\u03b3 (y,y adv ) (h) s \u03b3 -SimFair violation (penalty). \nh = f \u2022 g, where\u1ef9 = g(x)\nis the predicted probability and\u0177 = f (\u1ef9) is computed elementwisely, DP and EOp hold if for any k \u2208 A\nDP: E[\u1ef9 | a = k] = E[\u1ef9] EOp: E[\u1ef9 | a = k, y = y adv ] = E[\u1ef9 | y = y adv ].(13)\nProof. Here we derive the condition of EOp in eqn ( 13), the proof for DP can be obtained in the same way. Note that the conditional distribution of prediction y in k-th demographic subgroup of the advantaged group is given by\np(\u0177 | a = k, y = y adv ) = p(\u0177, x | a = k, y = y adv ) dx = p(\u0177 | x, a = k, y = y adv )p(x | a = k, y = y adv ) dx (a) = p(\u0177 | x)p(x | a = k, y = y adv ) dx = g(x)p(x | a = k, y = y adv ) dx = E[\u1ef9 | a = k, y = y adv ],\nwhere Proof. The EOp case can be seen by taking special s(y, y ) = 1(y = y ) in eqn (7). To prove the DP case, note that for constant function s(y, y ) = c, the left hand side of eqn ( 7) becomes E[c\u1ef9]/E[c] = E[\u1ef9], and the right hand side is\nE[c\u1ef91(a = k)] E[c1(a = k)] = E[\u1ef91(a = k)] E[1(a = k)] = 1 P (a = k) \u1ef91(a = k)p(\u1ef9, a) da\u1ef9 = \u1ef9 1 P (a = k) p(1(a = k)p(\u1ef9, a)) da d\u1ef9 = \u1ef9p(\u1ef9 | a = k) d\u1ef9 = E[\u1ef9 | a = k]. Now eqn (7) requires E[\u1ef9 | a = k] = E[\u1ef9]\nfor all k, which is exactly the condition of DP.\nProof of Proposition 3.3 Proposition B.3 (s \u03b3 -SimFair helps achieve DP and EOp). For any multi-label classifier h satisfying s \u03b3 -SimFair, its violation of DP will be arbitrarily small if \u03b3 is sufficiently small; and its violation of EOp will be arbitrarily small if \u03b3 is sufficiently large. More generally, its violation of DP is arbitrarily close to its violation of s \u03b3 -SimFair for sufficiently small \u03b3, and its violation of EOp is arbitrarily close to its violation of s \u03b3 -SimFair for sufficiently large \u03b3.\nProof. Let y, y be two arbitrary label vectors, and s * (y, y ) denote the limit of their similarity s \u03b3 (y, y ) as \u03b3 \u2192 0 or \u03b3 \u2192 \u221e. Equivalently speaking, s * (y, y ) is constant 1 or indicator function 1(y = y ), which are special s \u03b3 by Proposition 3.2. This allows us to express and upper bound the difference between violations of DP (or EOp) and s \u03b3 -SimFair on subgroup with a = k and label y adv defined in eqn (10) by .\ns * (y,y adv ) (f ) \u2212 s\u03b3 (y,y adv ) (f ) = E[\u1ef9s * (y, y adv )] E[s * (y, y adv )] \u2212 E[\u1ef91(a = k)s * (y, y adv )] E[1(a = k)s * (y, y adv )] \u2212 E[\u1ef9s \u03b3 (y, y adv )] E[s \u03b3 (y, y adv )] \u2212 E[\u1ef91(a = k)s \u03b3 (y, y adv )] E[1(a = k)s \u03b3 (y, y adv )] (a) \u2264 E[\u1ef9s * (y, y adv )] E[s * (y, y adv )] \u2212 E[\u1ef91(a = k)s * (y, y adv )] E[1(a = k)s * (y, y adv )] \u2212 E[\u1ef9s \u03b3 (y, y adv )] E[s \u03b3 (y, y adv )] \u2212 E[\u1ef91(a = k)s \u03b3 (y, y adv )] E[1(a = k)s \u03b3 (y, y adv )](\nIn other words, the first term in eqn ( 14) can be bounded with arbitrarily small \u03b5 as \u03b3 \u2192 0 (for DP case with s * (y, y ) = 1) or \u03b3 \u2192 \u221e (for EOp case with s * (y, y ) = 1(y = y )). Similar upper bound can be derived for the second term in the same way. Put together, we have\ns * (y,y adv ) (f ) \u2212 s\u03b3 (y,y adv ) (f ) \u2264 2\u03b5,(15)\nwhere \u03b5 depends on \u03b3 and can be made arbitrarily small. The first part of the proposition is proved by further assuming s\u03b3 (y, y adv ) = 0, i.e.,\nE[\u1ef9s \u03b3 (y, y adv )] E[s \u03b3 y, y adv )] = E[\u1ef91(a = k)s \u03b3 (y, y adv )] E[1(a = k)s \u03b3 (y, y adv )] .(16)\nThis completes our proof.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C Algorithm", "text": "Here we provide a concise illustration of MPVAE traininig with s \u03b3 -SimFair regularizer. In each step, we take one minibatch randomly selected from the dataset and update MPVAE with loss defined in eqn ( 12). Algorithm 1 summarizes this step.\nAlgorithm 1: One update of MPVAE with s \u03b3 -SimFair regularizer.\nInput: mini-batch {(x (i) , a (i) , y (i) )} n i=1 , advantaged group y adv , MPVAE h, hyperparameters \u03b3 and \u03bb. Output: Updated MPVAE h 1: Compute the empirical multi-label classification loss on the minibatc\u0125 i) , y (i) ; h).\nmlc = 1 n n i=1 mlc (x (\n2: For each sample, compute predictions from label and feature branches\u1ef9 (i) y ,\u1ef9 (i) x , and s (i) \u03b3 = s \u03b3 (y (i) , y adv ). 3: Compute empirical fairness loss\u02c6 s\u03b3 (y,y adv ) on the minibatch with eqn (10) or (11). Estimate each term by eqn (8) or (9). 4: Take one updates on h with Adam (Kingma and Ba 2014) to minimize the final empirical los\u015d =\u02c6 mlc + \u03bb\u02c6 s\u03b3 (y,y adv )", "publication_ref": ["b20"], "figure_ref": [], "table_ref": []}, {"heading": "D Experimental Details", "text": "Here we present the hyperparameters we used in experiments for reproducibility. For experiments run 10 replications, we used random seeds from 1 to 10; for experiments that only had 3 replications, we used seeds from 1 to 3. We used the same fixed hyperparameters except \u03bb and \u03b3 throughout all experiments. We made one modification to MPVAE training by clipping the gradient norm to stabilize the training, other training strategies are adopted from Bai, Kong, and Gomes (2020) and details can be found therein. Hyperparameters that are different from Bai, Kong, and Gomes (2020) ", "publication_ref": ["b0", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "E More Experimental Results", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Fairness-Accuracy Tradeoff", "text": "In this section we show full EOp-and DP-accuracy tradeoff in Figure 5 and 6 following the same logic as in Section 4. Note that whereas EOp-accuracy tradeoff on Adult dataset has different curvatures, but the conclusion does not change. DP-accuracy tradeoff has the similar trends as the EOp-accuracy tradeoff, so we omit reiterating observations. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgement", "text": "This work is supported in part by the US National Science Foundation under grant NSF IIS-2226108 and NSF IIS-2141037. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments on Multiclass Sensitive Feature", "text": "In this section we report results of s \u03b3 -SimFair on multiclass sensitive feature, where corresponding regularizers are constructed based on eqn (10). We take Adult dataset as an example and use race as the sensitive feature. All other experiment settings are same as before.   ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Disentangled Variational Autoencoder based Multi-Label Classification with Covariance-Aware Multivariate Probit Model", "journal": "", "year": "2020", "authors": "J Bai; S Kong; C Gomes"}, {"ref_id": "b1", "title": "Fairness in machine learning", "journal": "Nips tutorial", "year": "2017", "authors": "S Barocas; M Hardt; A Narayanan"}, {"ref_id": "b2", "title": "Big data's disparate impact", "journal": "Calif. L. Rev", "year": "2016", "authors": "S Barocas; A D Selbst"}, {"ref_id": "b3", "title": "Learning multi-label scene classification", "journal": "Pattern recognition", "year": "2004", "authors": "M R Boutell; J Luo; X Shen; C M Brown"}, {"ref_id": "b4", "title": "Gender shades: Intersectional accuracy disparities in commercial gender classification", "journal": "PMLR", "year": "2018", "authors": "J Buolamwini; T Gebru"}, {"ref_id": "b5", "title": "Building classifiers with independency constraints", "journal": "IEEE", "year": "2009", "authors": "T Calders; F Kamiran; M Pechenizkiy"}, {"ref_id": "b6", "title": "End-to-end learning for the deep multivariate probit model", "journal": "PMLR", "year": "2018", "authors": "D Chen; Y Xue; C Gomes"}, {"ref_id": "b7", "title": "Analysis of multivariate probit models", "journal": "Biometrika", "year": "1998", "authors": "S Chib; E Greenberg"}, {"ref_id": "b8", "title": "A snapshot of the frontiers of fairness in machine learning", "journal": "Communications of the ACM", "year": "2020", "authors": "A Chouldechova; A Roth"}, {"ref_id": "b9", "title": "Knowledge Discovery in Multi-label Phenotype Data", "journal": "Springer", "year": "2001", "authors": "A Clare; R D King"}, {"ref_id": "b10", "title": "Flexibly Fair Representation Learning by Disentanglement", "journal": "", "year": "2019", "authors": "E Creager; D Madras; J.-H Jacobsen; M A Weis; K Swersky; T Pitassi; R Zemel"}, {"ref_id": "b11", "title": "Multiclass-Multilabel Classification with More Classes than Examples", "journal": "PMLR", "year": "2010", "authors": "O Dekel; O Shamir"}, {"ref_id": "b12", "title": "The accuracy, fairness, and limits of predicting recidivism", "journal": "Science Advances", "year": "2018", "authors": "J Dressel; H Farid"}, {"ref_id": "b13", "title": "Fairness Through Awareness", "journal": "", "year": "2011", "authors": "C Dwork; M Hardt; T Pitassi; O Reingold; R Zemel"}, {"ref_id": "b14", "title": "Censoring Representations with an Adversary", "journal": "", "year": "2015", "authors": "H Edwards; A Storkey"}, {"ref_id": "b15", "title": "Experimental comparison of methods for multi-label classification in different application domains", "journal": "International Journal of Computer Applications", "year": "2015", "authors": "P El Kafrawy; A Mausad; H Esmail"}, {"ref_id": "b16", "title": "Multilabel classification via calibrated label ranking", "journal": "Machine learning", "year": "2008", "authors": "J F\u00fcrnkranz; E H\u00fcllermeier; E Loza Menc\u00eda; K Brinker"}, {"ref_id": "b17", "title": "Using multilabel classification to improve object detection. neurocomputing", "journal": "", "year": "2019", "authors": "T Gong; B Liu; Q Chu; N Yu"}, {"ref_id": "b18", "title": "Equality of Opportunity in Supervised Learning", "journal": "", "year": "2016", "authors": "M Hardt; E Price; N Srebro"}, {"ref_id": "b19", "title": "Fairness-aware classifier with prejudice remover regularizer", "journal": "Springer", "year": "2012", "authors": "T Kamishima; S Akaho; H Asoh; J Sakuma"}, {"ref_id": "b20", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "D P Kingma; J Ba"}, {"ref_id": "b21", "title": "Auto-Encoding Variational Bayes", "journal": "", "year": "2014", "authors": "D P Kingma; M Welling"}, {"ref_id": "b22", "title": "Scaling up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid", "journal": "AAAI Press", "year": "1996", "authors": "R Kohavi"}, {"ref_id": "b23", "title": "The Emerging Trends of Multi-Label Learning", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2021", "authors": "W Liu; H Wang; X Shen; I Tsang"}, {"ref_id": "b24", "title": "On the Fairness of Disentangled Representations", "journal": "", "year": "2019", "authors": "F Locatello; G Abbati; T Rainforth; S Bauer; B Sch\u00f6lkopf; O Bachem"}, {"ref_id": "b25", "title": "Learning Adversarially Fair and Transferable Representations", "journal": "PMLR", "year": "2018", "authors": "D Madras; E Creager; T Pitassi; R Zemel"}, {"ref_id": "b26", "title": "Fairnessaware learning for continuous attributes and treatments", "journal": "PMLR", "year": "2019", "authors": "J Mary; C Calauzenes; N Karoui"}, {"ref_id": "b27", "title": "A penalized likelihood method for balancing accuracy and fairness in predictive policing", "journal": "IEEE", "year": "2018", "authors": "G Mohler; R Raje; J Carter; M Valasik; J Brantingham"}, {"ref_id": "b28", "title": "Large-scale multi-label text classification-revisiting neural networks. In Joint european conference on machine learning and knowledge discovery in databases", "journal": "Springer", "year": "2014", "authors": "J Nam; J Kim; E Loza Menc\u00eda; I Gurevych; J F\u00fcrnkranz"}, {"ref_id": "b29", "title": "Discrimination-aware data mining", "journal": "", "year": "2008", "authors": "D Pedreshi; S Ruggieri; F Turini"}, {"ref_id": "b30", "title": "Post-processing for Individual Fairness", "journal": "", "year": "2021", "authors": "F Petersen; D Mukherjee; Y Sun; M Yurochkin"}, {"ref_id": "b31", "title": "On Fairness and Calibration", "journal": "", "year": "2017", "authors": "G Pleiss; M Raghavan; F Wu; J Kleinberg; K Q Weinberger"}, {"ref_id": "b32", "title": "Classifier chains for multi-label classification. Machine learning", "journal": "", "year": "2011", "authors": "J Read; B Pfahringer; G Holmes; E Frank"}, {"ref_id": "b33", "title": "Benchmarking bias mitigation algorithms in representation learning through fairness metrics", "journal": "", "year": "2021", "authors": "C Reddy; D Sharma; S Mehri; A Romero-Soriano; S Shabanian; S Honari"}, {"ref_id": "b34", "title": "Achieving Fairness with a Simple Ridge Penalty", "journal": "Citeseer", "year": "2006", "authors": "M Scutari; F Panero; M Proissl; G Tsoumakas; I Katakis; I Vlahavas"}, {"ref_id": "b35", "title": "Random k-labelsets: An ensemble method for multilabel classification", "journal": "PMLR", "year": "2007", "authors": "G Tsoumakas; I Vlahavas;  Springer; B Woodworth; S Gunasekar; M I Ohannessian; N Srebro"}, {"ref_id": "b36", "title": "A unified view of multilabel performance measures", "journal": "PMLR", "year": "2017", "authors": "X.-Z Wu; Z.-H Zhou"}, {"ref_id": "b37", "title": "Effective multi-label active learning for text classification", "journal": "", "year": "2009", "authors": "B Yang; J.-T Sun; T Wang; Z Chen"}, {"ref_id": "b38", "title": "The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. Expert systems with applications", "journal": "", "year": "2009", "authors": "I.-C Yeh; C Lien"}, {"ref_id": "b39", "title": "Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment", "journal": "", "year": "2017", "authors": "M B Zafar; I Valera; M Gomez Rodriguez; K P Gummadi"}, {"ref_id": "b40", "title": "A multi-label classification method using a hierarchical and transparent representation for paper-reviewer recommendation", "journal": "ACM Transactions on Information Systems (TOIS)", "year": "2020", "authors": "D Zhang; S Zhao; Z Duan; J Chen; Y Zhang; J Tang"}, {"ref_id": "b41", "title": "Binary relevance for multi-label learning: an overview. Frontiers of", "journal": "Computer Science", "year": "2018", "authors": "M.-L Zhang; Y.-K Li; X.-Y Liu; X Geng"}, {"ref_id": "b42", "title": "Ml-knn: A Lazy Learning Approach to Multi-Label Learning", "journal": "Pattern recognition", "year": "2007", "authors": "M.-L Zhang; Z.-H Zhou"}, {"ref_id": "b43", "title": "A Review on Multi-Label Learning Algorithms", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": "2014", "authors": "M.-L Zhang; Z.-H Zhou"}, {"ref_id": "b44", "title": "Adaptive object detection with dual multi-label prediction", "journal": "Springer", "year": "2020", "authors": "Z Zhao; Y Guo; H Shen; J Ye"}, {"ref_id": "b45", "title": "Context recommendation using multi-label classification", "journal": "", "year": "2014", "authors": "Y Zheng; B Mobasher; R Burke"}, {"ref_id": "b46", "title": "ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)", "journal": "IEEE", "year": "", "authors": ""}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "s \u03b3 -SimFair Unifies DP and EOp One key characteristic of s \u03b3 -SimFair is that it can be seen as an unification of DP and EOp, as formalized by Proposition 3.2 and 3.3. Proposition 3.2 (DP and EOp are special cases of s \u03b3 -SimFair). Consider s \u03b3 -SimFair defined in eqn (7), if similarity s is a constant function s(y, y ) = c for some c, then s \u03b3 -SimFair implies DP; if s is an indicator function s(y, y ) = 1(y = y ), then s \u03b3 -SimFair implies EOp. Proof. See appendix B. Proposition 3.3 (s \u03b3 -SimFair helps achieve DP and EOp).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Proof.See appendix B. Remark 2. Proposition 3.2 reveals the connection between s \u03b3 -SimFair and DP (EOp). Proposition 3.3 further shows that s \u03b3 -SimFair condition indeed helps achieve DP and EOp, establishing a theoretical foundation of borrowing information from similar labels.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "s \u03b3 -SimFair Regularized Model Training Fairness Violation Violation of s \u03b3 -SimFair denoted by s\u03b3 (y,y adv", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 1 :1Figure 1: Framework of training MPVAE (Bai, Kong, and Gomes 2020) with fairness regularization (in green). Blocks in blue mark the label branch and blocks in yellow mark the (non-sensitive) feature branch. During training, MPVAE predicts two probability vectors\u1ef9 on two branches separately. Both of them are used to construct the s \u03b3 -SimFair regularizer. During testing, only yellow blocks (prediction from the feature branch) are accessible.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 2 :2Figure 2: s \u03b3 -SimFair can estimate DP and EOp with different hyperparameter \u03b3, DP and EOp estimates are marked on left and right y-axes.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 3 :3Figure 3: Achieved DP and EOp as the advantaged group becomes smaller. An extremely large \u03bb = 5000 is used to enforce fairness mitigation. Compared to EOp regularizer, s \u03b3 -SimFair is more robust to the sample size.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 4 :4Figure4: EOp-accuracy tradeoffs on Credit dataset. EOp regularizer is unstable and ineffective when the advantaged group is small, s \u03b3 -SimFair, on the other hand, preserves similar tradeoff trend as DP on both large and small label groups.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "a) holds because of the conditional independence\u0177 \u22a5 (y, a) | x. Similar derivation gives us p(\u0177 | y = y adv ) = E[\u1ef9 | y = y adv ]. This indicates that the conditional independence requirement in EOp can be fulfilled if corresponding conditional expectations match. Proof of Proposition 3.2 Proposition B.2 (DP and EOp are special cases of s \u03b3 -SimFair). Consider s \u03b3 -SimFair defined in eqn (7), if similarity s is a constant function s(y, y ) = c for some c, then s \u03b3 -SimFair implies DP; if s is an indicator function s(y, y ) = 1(y = y ), then s \u03b3 -SimFair implies EOp.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "b) \u2264 E[\u1ef9s * (y, y adv )] E[s * (y, y adv )] \u2212 E[\u1ef9s \u03b3 (y, y adv )] E[s \u03b3 (y, y adv )] + E[\u1ef91(a = k)s * (y, y adv )] E[1(a = k)s * (y, y adv )] \u2212 E[\u1ef91(a = k)s \u03b3 (y, y adv )] E[1(a = k)s \u03b3 (y, y adv )] , where (a) holds from the reverse triangle inequality and (b) holds from the triangle inequality. Next, sequence s \u03b3 (y, y ) converges to s * (y, y ) monotonically, we have the following almost surely convergenc\u1ebd ys \u03b3 (y, y adv ) a.s. \u2212 \u2212 \u2192\u1ef9s * (y, y adv ). Recall that\u1ef9 is the predicted probability vector so E[ \u1ef9 ] < \u221e, and s \u03b3 (y, y adv ), s * (y, y adv ) \u2208 [0, 1], we have max( \u1ef9s \u03b3 (y, y adv ) , \u1ef9s * (y, y adv ) ) \u2264 \u1ef9 Let \u03b3 * denote 0 (for DP) or \u221e (for EOp). According to Dominated Convergence Theorem lim \u03b3\u2192\u03b3 * E[\u1ef9s \u03b3 (y, y adv )] \u2212 E[\u1ef9s * (y, y adv )] = 0, and lim \u03b3\u2192\u03b3 * E[s \u03b3 (y, y adv )] \u2212 E[s * (y, y adv )] = 0. Without loss of generality we further assume E[s \u03b3 (y, y adv )] and E[s * (y, y adv )] are positive 3 . This gives us E[\u1ef9s \u03b3 (y, y adv )] E[s \u03b3 (y, y adv )] \u2192 E[\u1ef9s * (y, y adv )] E[s * (y, y adv )]", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "DP, EOp, and s \u03b3 -SimFair estimates (denoted as s \u03b3 -SF) on Adult and Credit datasets. Certain portions of samples in the advantaged group are kept (col. y adv obs.(%)) to check the robustness of different estimators. Results are averaged over 10 replications. Estimates of DP and EOp on 100% portion of samples are considered as the ground truth (marked with asterisk). s \u03b3 -SimFair estimator is more robust than EOp estimator.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "|y adv | Metric DP reg s1-SF reg s5-SF reg s10-SF reg EOp reg No reg", "figure_data": "AdultNo.1 No.18DP 0.038 EOp 0.051 DP 0.038 EOp 0.0760.031 0.042 0.038 0.0720.038 0.030 0.043 0.0370.043 0.034 0.045 0.0270.045 0.035 0.094 0.0660.111 0.161 0.111 0.095CreditNo.1 No.9DP 0.018 EOp 0.026 DP 0.018 EOp 0.2020.018 0.026 0.018 0.1920.017 0.025 0.019 0.1930.018 0.025 0.019 0.1970.018 0.026 0.030 0.2410.029 0.038 0.030 0.241"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "DP and EOp violations of MPVAE trained with DP, EOp, and s \u03b3 -SimFair regularziers. On each dataset, a large and a small advantaged groups (measured by their ranking in col. |y adv |) are tested. Results are averaged over 10 replications, best results are in bold.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Main notations used in this paper.", "figure_data": "B Omitted Proofs"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "are listed in table 4.", "figure_data": "Epochs Ranking loss coefficient Latent dimension Max. gradient norm20100325"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Hyperparameter settings of our experiments. Other hyperparameters are adopted fromBai, Kong, and Gomes (2020).", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "X \u2192 [0, 1] L \u2192 Y.", "formula_coordinates": [3.0, 319.5, 283.52, 68.26, 10.31]}, {"formula_id": "formula_1", "formula_text": "h = f \u2022 g, where\u1ef9 = g(x)", "formula_coordinates": [3.0, 319.5, 566.41, 104.18, 8.96]}, {"formula_id": "formula_2", "formula_text": "DP: E[\u1ef9 | a = k] = E[\u1ef9] EOp: E[\u1ef9 | a = k, y = y adv ] = E[\u1ef9 | y = y adv ]. (1)", "formula_coordinates": [3.0, 344.27, 603.25, 213.73, 24.43]}, {"formula_id": "formula_3", "formula_text": "E[\u1ef9 | a = k] \u2248 N i=1\u1ef9 (i) 1(a (i) = k) N i=1 1(a (i) = k) E[\u1ef9] \u2248 1 N N i=1\u1ef9 (i) E[\u1ef9 | a = k, y = y adv ] \u2248 N i=1\u1ef9 (i) 1(a (i) = k)1(y = y adv ) N i=1 1(a (i) = k)1(y = y adv ) (2) E[\u1ef9 | y = y adv ] \u2248 N i=1\u1ef9 (i) 1(y = y adv ) N i=1 1(y = y adv )(3)", "formula_coordinates": [4.0, 61.54, 93.05, 230.96, 85.45]}, {"formula_id": "formula_4", "formula_text": "N i=1 1(a (i) = k)1(y (i) = y adv ) and N i=1 1(y (i) = y adv ) in eqn (", "formula_coordinates": [4.0, 64.52, 277.29, 227.98, 27.87]}, {"formula_id": "formula_5", "formula_text": "E[\u1ef9 | a = k, y = y adv ] = \u1ef9p(\u1ef9 | a = k, y = y adv ) d\u1ef9 = \u1ef9p(\u1ef9, a = k, y = y adv ) d\u1ef9 P (a = k, y = y adv ) .", "formula_coordinates": [4.0, 68.83, 347.44, 208.84, 45.08]}, {"formula_id": "formula_6", "formula_text": "\u1ef9p(\u1ef9, a = k, y = y adv ) d\u1ef9 = \u1ef91(a = k)1(y = y adv )p(\u1ef9, a, y) da dy d\u1ef9 = E[\u1ef91(a = k)1(y = y adv )].", "formula_coordinates": [4.0, 74.97, 418.37, 197.3, 57.01]}, {"formula_id": "formula_7", "formula_text": "E[\u1ef9 | a = k, y = y adv ] = E[\u1ef91(a = k)1(y = y adv )] E[1(a = k)1(y = y adv )](4)", "formula_coordinates": [4.0, 81.72, 494.95, 210.78, 21.44]}, {"formula_id": "formula_8", "formula_text": "E[\u1ef9 | y = y adv ] = E[\u1ef91(y = y adv )] E[1(y = y adv )] .(5)", "formula_coordinates": [4.0, 104.46, 519.79, 188.04, 21.44]}, {"formula_id": "formula_9", "formula_text": "E[\u1ef91(y = y adv )] E[1(y = y adv )] = E[\u1ef91(a = k)1(y = y adv )] E[1(a = k)1(y = y adv )](6)", "formula_coordinates": [4.0, 89.16, 560.17, 203.34, 21.44]}, {"formula_id": "formula_10", "formula_text": "Y \u00d7 Y \u2192 [0, 1], a multi-label classifier h satisfies Similarity s-induced Fairness (s \u03b3 -SimFair) if for \u2200 k \u2208 A, E[\u1ef9s(y, y adv )] E[s(y, y adv )] = E[\u1ef91(a = k)s(y, y adv )] E[1(a = k)s(y, y adv )] .(7)", "formula_coordinates": [4.0, 319.5, 115.58, 238.85, 50.69]}, {"formula_id": "formula_11", "formula_text": "E[\u1ef9s(y, y adv )] E[s(y, y adv )] \u2248 i\u1ef9 (i) s(y (i) , y adv ) i s(y (i) , y adv )(8)", "formula_coordinates": [4.0, 360.16, 193.74, 197.84, 24.01]}, {"formula_id": "formula_12", "formula_text": "E[\u1ef91(a = k)s(y, y adv )] E[1(a = k)s(y, y adv )] \u2248 i\u1ef9 (i) 1(a (i) = k)s(y (i) , y adv ) i 1(a (i) = k)s(y (i) , y adv ) .(9)", "formula_coordinates": [4.0, 332.6, 220.94, 225.41, 23.26]}, {"formula_id": "formula_13", "formula_text": "Jac(y, y adv ) = |cate(y) \u2229 cate(y adv )| |cate(y) \u222a cate(y adv )| s \u03b3 (y, y adv ) = exp (\u03b3 (Jac(y, y adv ) \u2212 1))", "formula_coordinates": [4.0, 356.93, 328.33, 163.64, 38.24]}, {"formula_id": "formula_14", "formula_text": ") (h), is defined as K k=1 E[\u1ef9s\u03b3(y, y adv )] E[s\u03b3(y, y adv )] \u2212 E[\u1ef91(a = k)s\u03b3(y, y adv )] E[1(a = k)s\u03b3(y, y adv )] (10", "formula_coordinates": [5.0, 70.06, 81.78, 218.7, 43.88]}, {"formula_id": "formula_15", "formula_text": ")", "formula_coordinates": [5.0, 288.77, 108.09, 3.73, 7.77]}, {"formula_id": "formula_16", "formula_text": "E[\u1ef91(a = 1)s\u03b3(y, y adv )] E[1(a = 1)s\u03b3(y, y adv )] \u2212 E[\u1ef91(a = 2)s\u03b3(y, y adv )] E[1(a = 2)s\u03b3(y, y adv )] .(11)", "formula_coordinates": [5.0, 66.39, 176.33, 226.11, 21.44]}, {"formula_id": "formula_17", "formula_text": "h mlc (h) + \u03bb s\u03b3 (y,y adv ) (h). (12", "formula_coordinates": [5.0, 118.38, 309.27, 169.97, 14.66]}, {"formula_id": "formula_18", "formula_text": ")", "formula_coordinates": [5.0, 288.35, 309.59, 4.15, 8.64]}, {"formula_id": "formula_19", "formula_text": "[) | ] [) | ] ) ) [) ] [) ]", "formula_coordinates": [5.0, 469.1, 65.44, 36.33, 92.96]}, {"formula_id": "formula_20", "formula_text": "micro-F1 = 2 L l=1 N i=1 y (i) l\u0177 (i) l L l=1 N i=1 (y (i) l +\u0177 (i) l ) macro-F1 = 1 L L l=1 2 N i=1 y (i) l\u0177 (i) l N i=1 (y (i) l +\u0177 (i) l ) example-F1 = 1 N N i=1 2 L l=1 y (i) l\u0177 (i) l L l=1 (y (i) l +\u0177 (i) l ) .", "formula_coordinates": [6.0, 98.87, 205.09, 148.76, 90.03]}, {"formula_id": "formula_21", "formula_text": "h = f \u2022 g : X \u2192 Y A composited multi-label classifier, f : X \u2192 [0, 1] L and g : [0, 1] L \u2192 Y. y = h(x) = g(f (x)) Predicted label (Prediction). y = f (x)", "formula_coordinates": [10.0, 111.13, 149.56, 753.92, 32.01]}, {"formula_id": "formula_22", "formula_text": "h = f \u2022 g, where\u1ef9 = g(x)", "formula_coordinates": [10.0, 439.31, 327.41, 109.7, 8.96]}, {"formula_id": "formula_23", "formula_text": "DP: E[\u1ef9 | a = k] = E[\u1ef9] EOp: E[\u1ef9 | a = k, y = y adv ] = E[\u1ef9 | y = y adv ].(13)", "formula_coordinates": [10.0, 201.28, 358.07, 356.72, 24.43]}, {"formula_id": "formula_24", "formula_text": "p(\u0177 | a = k, y = y adv ) = p(\u0177, x | a = k, y = y adv ) dx = p(\u0177 | x, a = k, y = y adv )p(x | a = k, y = y adv ) dx (a) = p(\u0177 | x)p(x | a = k, y = y adv ) dx = g(x)p(x | a = k, y = y adv ) dx = E[\u1ef9 | a = k, y = y adv ],", "formula_coordinates": [10.0, 186.24, 428.23, 239.53, 109.15]}, {"formula_id": "formula_26", "formula_text": "E[c\u1ef91(a = k)] E[c1(a = k)] = E[\u1ef91(a = k)] E[1(a = k)] = 1 P (a = k) \u1ef91(a = k)p(\u1ef9, a) da\u1ef9 = \u1ef9 1 P (a = k) p(1(a = k)p(\u1ef9, a)) da d\u1ef9 = \u1ef9p(\u1ef9 | a = k) d\u1ef9 = E[\u1ef9 | a = k]. Now eqn (7) requires E[\u1ef9 | a = k] = E[\u1ef9]", "formula_coordinates": [11.0, 54.0, 74.59, 347.28, 122.66]}, {"formula_id": "formula_27", "formula_text": "s * (y,y adv ) (f ) \u2212 s\u03b3 (y,y adv ) (f ) = E[\u1ef9s * (y, y adv )] E[s * (y, y adv )] \u2212 E[\u1ef91(a = k)s * (y, y adv )] E[1(a = k)s * (y, y adv )] \u2212 E[\u1ef9s \u03b3 (y, y adv )] E[s \u03b3 (y, y adv )] \u2212 E[\u1ef91(a = k)s \u03b3 (y, y adv )] E[1(a = k)s \u03b3 (y, y adv )] (a) \u2264 E[\u1ef9s * (y, y adv )] E[s * (y, y adv )] \u2212 E[\u1ef91(a = k)s * (y, y adv )] E[1(a = k)s * (y, y adv )] \u2212 E[\u1ef9s \u03b3 (y, y adv )] E[s \u03b3 (y, y adv )] \u2212 E[\u1ef91(a = k)s \u03b3 (y, y adv )] E[1(a = k)s \u03b3 (y, y adv )](", "formula_coordinates": [11.0, 80.01, 332.64, 436.26, 91.9]}, {"formula_id": "formula_29", "formula_text": "s * (y,y adv ) (f ) \u2212 s\u03b3 (y,y adv ) (f ) \u2264 2\u03b5,(15)", "formula_coordinates": [12.0, 234.38, 98.67, 323.62, 11.42]}, {"formula_id": "formula_30", "formula_text": "E[\u1ef9s \u03b3 (y, y adv )] E[s \u03b3 y, y adv )] = E[\u1ef91(a = k)s \u03b3 (y, y adv )] E[1(a = k)s \u03b3 (y, y adv )] .(16)", "formula_coordinates": [12.0, 211.93, 150.2, 346.07, 24.2]}, {"formula_id": "formula_31", "formula_text": "mlc = 1 n n i=1 mlc (x (", "formula_coordinates": [12.0, 252.59, 323.35, 84.35, 30.32]}], "doi": ""}