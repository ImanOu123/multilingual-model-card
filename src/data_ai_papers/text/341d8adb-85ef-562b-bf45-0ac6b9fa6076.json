{"title": "Inferring Network Structure from Co-Occurrences", "authors": "Michael G Rabbat Electrical; M\u00e1rio A T Figueiredo; Robert D Nowak Electrical", "pub_date": "", "abstract": "We consider the problem of inferring the structure of a network from cooccurrence data: observations that indicate which nodes occur in a signaling pathway but do not directly reveal node order within the pathway. This problem is motivated by network inference problems arising in computational biology and communication systems, in which it is difficult or impossible to obtain precise time ordering information. Without order information, every permutation of the activated nodes leads to a different feasible solution, resulting in combinatorial explosion of the feasible set. However, physical principles underlying most networked systems suggest that not all feasible solutions are equally likely. Intuitively, nodes that co-occur more frequently are probably more closely connected. Building on this intuition, we model path co-occurrences as randomly shuffled samples of a random walk on the network. We derive a computationally efficient network inference algorithm and, via novel concentration inequalities for importance sampling estimators, prove that a polynomial complexity Monte Carlo version of the algorithm converges with high probability. Recently, Caffo et al. [14] have proposed a method, based on central limit theorem-like arguments, for automatically adapting the number of Monte Carlo samples used at each EM iteration. They", "sections": [{"heading": "Introduction", "text": "The study of complex networked systems is an emerging field impacting nearly every area of engineering and science, including the important domains of biology, cognitive science, sociology, and telecommunications. Inferring the structure of signalling networks from experimental data precedes any such analysis and is thus a basic and fundamental task. Measurements which directly reveal network structure are often beyond experimental capabilities or are excessively expensive. This paper addresses the problem of inferring the structure of a network from co-occurrence data: observations which indicate nodes that are activated in each of a set of signaling pathways but do not directly reveal the order of nodes within each pathway. Co-occurrence observations arise naturally in a number of interesting contexts, including biological and communication networks, and networks of neuronal colonies.\nBiological signal transduction networks describe fundamental cell functions and responses to environmental stress [1]. Although it is possible to test for individual, localized interactions between gene pairs, this approach (called genetic epistatic analysis) is expensive and time-consuming. Highthroughput measurement techniques such as microarrays have successfully been used to identify the components of different signal transduction pathways [2]. However, microarray data only reflects order information at a very coarse, unreliable level. Developing computational techniques for inferring pathway orders is a largely unexplored research area [3].\nA similar problem has been studied in telecommunication networks [4]. In this context, each path corresponds to a transmission between an origin and destination. The origin and destination are observed, in addition to the activated switches/routers carrying the transmission through the network.\nHowever, due to the geographically distributed nature of the measurement infrastructure and the rapidity at which transmissions are completed, it is not possible to obtain precise ordering information.\nAnother exciting potential application arises in neuroimaging [5,6]. Functional magnetic resonance imaging provides images of brain activity with high spatial resolution but has relatively poor temporal resolution. Treating distinct brain regions as nodes in a functional brain network that co-activate when a subject performs different tasks may lead to a similar network inference problem.\nGiven a collection of co-occurrences, a feasible network (consistent with the observations) is easily obtained by assigning an order to the elements of each co-occurrence, thereby specifying a path through the hypothesized network. Since any arbitrary order of each co-occurrence leads to a feasible network, the number of feasible solutions is proportional to the number of permutations of all the co-occurrence observations. Consequently we are faced with combinatorial explosion of the feasible set, and without additional assumptions or side information there is no reason to prefer one particular feasible network over the others. See the supplementary document [7] for further discussion.\nDespite the apparent intractability of the problem, physical principles governing most networks suggest that not all feasible solutions are equally plausible. Intuitively, nodes that co-occur more frequently are more likely to be connected in the underlying network. This intuition has been used as a stepping stone by recent approaches proposed in the context of telecommunications [4], and in learning networks of collaborators [8]. However, because of their heuristic nature, these approaches do not produce easily interpreted results and do not readily lend themselves to analysis or to the incorporation of side information.\nIn this paper, we model co-occurrences as randomly permuted samples of a random walk on the underlying network. The random permutation accounts for lack of observed order. We refer to this process as the shuffled Markov model. In this framework, network inference amounts to maximum likelihood estimation of the parameters governing the random walk (initial state distribution and transition matrix). Direct maximization is intractable due to the highly non-convex log-likelihood function and exponential feasible set arising from simultaneously considering all permutations of all co-occurrences. Instead, we derive a computationally efficient EM algorithm, treating the random permutations as hidden variables. In this framework the likelihood factorizes with respect to each pathway/observation, so that the computational complexity of the EM algorithm is determined by the E-step which is only exponential in the longest path. In order to handle networks with long paths, we propose a Monte Carlo E-step based on a simple, linear complexity importance sampling scheme. Whereas the exact E-step has computational complexity which is exponential in path length, we prove that a polynomial number of importance samples suffices to retain desirable convergence properties of the EM algorithm with high probability. In this sense, our Monte Carlo EM algorithm breaks the curse of dimensionality using randomness.\nIt is worth noting that the approach described here differs considerably from that of learning the structure of a directed graphical model or Bayesian network [9,10]. The aim of graphical modelling is to find a graph corresponding to a factorization of a high-dimensional distribution which predicts the observations well. These probabilistic models do not directly reflect physical structures, and applying such an approach to co-occurrences would ignore physical constraints inherent to the observations: co-occurring vertices must lie along a path in the network.", "publication_ref": ["b0", "b1", "b2", "b3", "b4", "b5", "b6", "b3", "b7", "b8", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Model Formulation and EM Algorithm", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Shuffled Markov Model", "text": "We model a network as a directed graph G = (V, E), where V = {1, . . . , |V |} is the vertex (node) set and E \u2286 V 2 is the set of edges (direct connections between vertices). An observation, y \u2282 V , is a subset of vertices co-activated when a particular stimulus is applied to the network (e.g., collection of signaling proteins activated in response to an environmental stress). Given a set of T observations, Y = {y (1) , . . . , y (T ) }, each corresponding to a path, where\ny (m) = {y (m) 1 , . . . , y (m)\nNm }, we say that a graph (V, E) is feasible w.r.t. Y if for each y (m) \u2208 Y there is an ordered path z (m) = (z\n(m) 1 , . . . , z (m)\nNm ) and a permutation \u03c4 (m) = (\u03c4\n(m) 1 , . . . , \u03c4 (m) Nm ) such that z (m) t = y (m) \u03c4 (m) t\n, and\n(z t\u22121 , z t ) \u2208 E, for t = 2, ..., N m .\nThe (unobserved) ordered paths, Z = {z (1) , ..., z (T ) }, are modelled as T independent samples of a first-order Markov chain with state set V . The Markov chain is parameterized by the initial state distribution \u03c0 and the (stochastic) transition matrix A. We assume that the support of the transition matrix is determined by the adjacency structure of the graph; i.e., A i,j > 0 \u21d4 (i, j) \u2208 E. Each observation y (m) results from shuffling the elements of z (m) via an unobserved permutation \u03c4 (m) , drawn uniformly from S Nm (the set of all permutations of N m objects); i.e., z\n(m) t = y (m) \u03c4 (m) t , for t = 1, . . . , N m .\nAll the \u03c4 (m) are assumed mutually independent and independent of all the z (m) . Under this model, the log-likelihood of the set of observations Y is\nlog P [Y|A, \u03c0] = T m=1 \uf8eb \uf8ed log \uf8eb \uf8ed \u03c4 \u2208S Nm P [y (m) |\u03c4 , A, \u03c0] \uf8f6 \uf8f8 \u2212 log(N m !) \uf8f6 \uf8f8 .(1)\nwhere\nP [y|\u03c4 , A, \u03c0] = \u03c0 y\u03c4 1 N t=2 A y\u03c4 t\u22121\n,y\u03c4 t , and network inference consists in computing the maximum likelihood (ML) estimates (A ML , \u03c0 ML ) = arg max A,\u03c0 log P [Y|A, \u03c0]. With the ML estimates in hand, we may determine the most likely permutation for each y (m) and obtain a feasible reconstruction from the ordered paths. In general, log P [Y|A, \u03c0] is a non-concave function of (A, \u03c0), so finding (A ML , \u03c0 ML ) is not easy. Next, we derive an EM algorithm for this purpose, by treating the permutations as missing data.", "publication_ref": ["b0", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "EM Algorithm", "text": "Let w (m) = (w\n(m) 1 , ..., w (m)\nNm ) be a binary representation of z (m) , defined by w (1) , . . . , x (T ) } be the binary representation for Y, defined in a similar way:\n(m) t = (w (m) t,1 , ..., w (m) t,|V | ) \u2208 {0, 1} |V | , with (w (m) t,i = 1) \u21d4 (z (m) t = i); let W = {w (1) , ..., w (T ) }. Let X = {x\nx (m) = (x (m) 1 , ..., x (m) Nm ), where x (m) t = (x (m) t,1 , ..., x (m) t,|V | ) \u2208 {0, 1} |V | , with (x (m) t,i = 1) \u21d4 (y (m) t = i).\nFinally, let R = {r (1) , . . . , r (T ) } be the collection of permutation matrices corresponding to T = {\u03c4 (1) , . . . , \u03c4 (T ) }; i.e., (r (m)\nt,t = 1) \u21d4 (\u03c4 (m) t = t ).\nWith this notation in place, the complete log-likelihood can be written as log P [X , R|A, \u03c0] = log P [X |R, A, \u03c0] + log P [R], where\nlog P [X |R, A, \u03c0] = T m=1 log P [x (m) |r (m) , A, \u03c0] = T m=1 |V | i,j=1 Nm t ,t =1 Nm t=2 r (m) t,t r (m) t\u22121,t x (m) t ,i x (m) t ,j log A i,j + T m=1 |V | i=1 Nm t =1 r (m) 1,t x (m) t ,i log \u03c0 i , (2)\nand P [R] is the probability of the set of permutations, which is constant and thus dropped, since the permutations are independent and equiprobable.  Since the permutations are (a priori) equiprobable, we have P [r (m) ", "publication_ref": ["b0", "b0", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "The EM algorithm proceeds by (the E-step) computing", "text": "Q A, \u03c0; A k , \u03c0 k = E log P [X , R|A, \u03c0] X , A k , \u03c0 k ,\n] = (N m !) \u22121 , P r (m) 1,t = 1] = (N m \u2212 1)!/N m ! = 1/N m , and P [r (m) |r (m) 1,t = 1] = 1/(N m \u2212 1)!.\nUsing these facts, the mutual independence among different observations, and Bayes law, it is not hard to show that\nr (m) 1,t = \u03b3 (m) t Nm t =1 \u03b3 (m) t with \u03b3 (m) t = r: r 1,t =1 P x (m) r, A k , \u03c0 k ,(3)\nwhere each term P x (m) r, A k , \u03c0 k is easily computed after using r to \"unshuffle\" x (m) :\nP x (m) r, A k , \u03c0 k = P y (m) \u03c4 , A k , \u03c0 k = \u03c0 k y (m) \u03c4 1 Nm t=2 A k y (m) \u03c4 t\u22121 ,y (m) \u03c4 t .\nThe computation of\u1fb1 (m) t ,t is similar to that ofr (m) 1,t ; the key observations are that P [r\n(m) t,t r (m) t\u22121,t = 1] = (N m \u2212 2)!/N m ! and P [r (m) |r (m) t,t r (m) t\u22121,t = 1] = 1/(N m \u2212 2)!, leading t\u014d \u03b1 (m) t ,t = \u03b3 (m) t ,t Nm t =1 \u03b3 (m) t , with \u03b3 (m) t ,t = r P [x (m) |r, A k , \u03c0 k ] Nm t=2 r t,t r t\u22121,t . (4)\nComputing\n{r (m) 1,t } and {\u1fb1 (m) t ,t } requires O N m ! operations.\nFor large N m , this is a heavy load; in Section 3, we describe a sampling approach for computing approximations tor 1,t and\u1fb1 t ,t . Maximization of Q A, \u03c0; A k , \u03c0 k w.r.t. A and \u03c0, under the normalization constraints, leads to the M-step:\nA k+1 i,j = T m=1 Nm t ,t =1\u1fb1 (m) t ,t x (m) t ,i x (m) t ,j |S| j=1 T m=1 Nm t ,t =1\u1fb1 (m) t ,t x (m) t ,i x (m) t ,j and \u03c0 k+1 i = T m=1 Nm t =1r (m) 1,t x (m) t ,i |S| i=1 T m=1 Nm t =1r (m) 1,t x (m) t ,i .\n(5) Standard convergence results for the EM algorithm due to Boyles and Wu [11,12] guarantee that the sequence {(A k , \u03c0 k )} converges monotonically to a local maximum of the likelihood.", "publication_ref": ["b10", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Handling Known Endpoints", "text": "In some applications, (one or both of) the endpoints of each path are known and only the internal nodes are shuffled. For example, in telecommunications problems, the origin and destination of each transmission are known, but not the network connectivity. In estimating biological signal transduction pathways, a physical stimulus (e.g., hypotonic shock) causes a sequence of protein interactions, resulting in another observable physical response (e.g., a change in cell wall structure); in this case, the stimulus and response act as fixed endpoints, the goal is to infer the order of the sequence of protein interactions. Knowledge of the endpoints of each path imposes the constraints r \n\u03c0 i = 1 T T m=1 x (m)\n1,i . Thus, EM only needs to be used to estimate A. In this setup, the E-step has a similar form as (4) but with sums over r replaced by sums over permutation matrices satisfying r 1,1 = 1 and r N,N = 1. The M-step update for A k+1 remains unchanged.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Large Scale Inference via Importance Sampling", "text": "For long paths, the combinatorial nature of the exact E-step -summing over all permutations of each sequence in (3) and ( 4) -may render exact computation intractable. This section presents a Monte Carlo importance sampling (see, e.g., [13]) version of the E-step, along with finite sample bounds guaranteeing that a polynomial complexity Monte Carlo EM algorithm retains desirable convergence properties of the EM algorithm; i.e., monotonic convergence to a local maximum.", "publication_ref": ["b12"], "figure_ref": [], "table_ref": []}, {"heading": "Monte Carlo E-Step by Importance Sampling", "text": "To lighten notation in this section we drop the superscripts from (A k , \u03c0 k ), using simply (A, \u03c0) for the current parameter estimates. Moreover, since the statistics\u1fb1 1,t depend only on the mth co-activation observation, y (m) , we focus on a particular length-N path observation y = (y 1 , y 2 , . . . , y N ) and drop the superscript (m).\nA na\u00efve Monte Carlo approximation would be based on random permutations sampled from the uniform distribution on S N . However, the reason we resort to approximation techniques in the first place is that S N is large, but typically only a small fraction of its elements have non-negligible posterior probability, P [\u03c4 |y, A, \u03c0]. Although we would ideally sample directly from the posterior, this would require determining its value for all N ! permutations. Instead, we propose the following sequential scheme for sampling a permutation using the current parameter estimates, (A, \u03c0). To ensure the same element is not sampled twice we introduce a vector of binary flags, f = (f 1 , f 2 , . . . , f |V | ) \u2208 {0, 1} |V | . Given a probability distribution p = (p 1 , p 2 , . . . , p |V | ) on the vertex set, V , denote by p|f the restriction of p to those elements i \u2208 V for which f i = 1; i.e.,\n(p|f ) i = p i f i |V | j=1 p j f j , for i = 1, 2, . . . , |V |. (6\n)\nOur sampling scheme proceeds as follows:\nStep 1: Initialize f so that f i = 1 if y t = i for some t = 1, . . . , N , and f i = 0 otherwise. Sample an element v from V according to the distribution \u03c0|f on V . Find t such that y t = v. Set \u03c4 1 = t. Set f v = 0 to prevent y t from being sampled again (ensure \u03c4 is a permutation). Set i = 2.\nStep 2: Let A v denote the vth row of the transition matrix. Sample an element v from V according to the distribution\nA v |f on V . Find t such that y t = v . Set \u03c4 i = t. Set f v = 0.\nStep 3: While i < N , update v \u2190 v and i \u2190 i + 1 and repeat Step 2; otherwise, stop.\nRepeating this sampling procedure L times yields a collection of iid permutations \u03c4 1 , \u03c4 2 , . . . , \u03c4 L , where the superscript now identifies the sample number; the corresponding permutation matrices are r 1 , r 2 , . . . , r L . Samples generated according to the scheme described above are drawn from a distribution R[\u03c4 |x, A, \u03c0] on S N which is different from the posterior P [\u03c4 |x, A, \u03c0]. Importance sample estimates correct for this disparity and are given by the expressions\nr 1,t = L =1 u r 1,t L =1 u and \u03b1 t ,t = L =1 u N t=2 r t,t r t\u22121,t L =1 u ,(7)\nwhere the correction factor (or weight) for sample r is given by\nu = P [r |x, A, \u03c0] R[r |x, A, \u03c0] = P [\u03c4 |y, A, \u03c0] R[\u03c4 |y, A, \u03c0] = N t=2 N t =t A y \u03c4 t\u22121 ,y \u03c4 t .(8)\nA detailed derivation of the exact form of the induced distribution, R, and the correction factor, u , based on the sequential nature of the sampling scheme, along with further discussion and comparison with alternative sampling schemes can be found in the supplementary document [7]. In fact, terms in the product ( 8) are readily available as a byproduct of Step 2 (denominator of A v |f ).", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "Monotonicity and Convergence", "text": "Standard EM convergence results directly apply when the exact E-step is used [11,12]. Let \u03b8 k = (A k , \u03c0 k ). By choosing \u03b8 k+1 according to (5) we have \u03b8 k+1 = arg max \u03b8 Q(\u03b8; \u03b8 k ), and the monotonicity property, Q(\u03b8 k+1 ; \u03b8 k ) \u2265 Q(\u03b8 k ; \u03b8 k ), is satisfied. Together with the fact that the marginal log-likelihood (1) is continuous in \u03b8 and bounded above, the monotonicity property guarantees that the exact EM iterates converge monotonically to a local maximum of log P [Y|\u03b8].\nWhen the Monte Carlo E-step is used, we no longer have monotonicity since now the M-step solves\n\u03b8 k+1 = arg max \u03b8 Q(\u03b8; \u03b8 k ),\nwhere Q is defined analogously to Q but with\u1fb1 ). To assure the Monte Carlo EM algorithm (MCEM) converges, the number of importance samples, L, must be chosen carefully so that Q approximates Q well enough; otherwise the MCEM may be swamped with error.\nt ,t at a rate which is exponential in the number of importance samples used. These bounds are based on rather novel concentration inequalities for importance sampling estimators, which may be of interest in their own right (see the supplementary document [7] for details). Then, accounting for the explicit form of Q in our problem, the result follows from application of the union bound and the assumptions that\nA k i,j , \u03c0 k i \u2265 \u03b8 min .\nIn fact, by making a slightly stronger assumption it can be shown that the MCEM update is probably monotonic (i.e., (0, \u03b4)-PAM, not approximately monotonic) if L m importance samples are used for the mth observation, where L m also depends polynomially on N m and T . See the supplementary document [7] for further discussion and for the full proof of Theorem 1.\nRecall that exact E-step computation requires N m ! operations for the mth observation (enumerating all permutations). The bound above stipulates that the number of importance samples required for a PAM update is on the order of N 4 m log N 2 m . Generating one importance sample using the sequential procedure described above requires N m operations. In contrast to the (exponential complexity) exact EM algorithm, this clearly demonstrates that the MCEM converges with high probability while only having polynomial computational complexity, and, in this sense, the MCEM meaningfully breaks the curse of dimensionality by using randomness to preserve the monotonic convergence property.", "publication_ref": ["b10", "b11", "b6", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Results", "text": "The performance of our algorithm for network inference from co-occurrences (NICO, pronounced \"nee-koh\") has been evaluated on both simulated data and on a biological data set. In these experiments, network structure is inferred by first executing the EM algorithm to infer the parameters (A, \u03c0) of a Markov chain. Then, inserting edges in the inferred graph based on the most likely order of each path according to (A, \u03c0) ensures the resulting graph is feasible with respect to the observations. Because the EM algorithm is only guaranteed to converge to a local maximum, we rerun the algorithm from multiple random initializations and chose the mostly likely of these solutions. To gauge the performance of our algorithm we use the edge symmetric difference error: the total number of false positives (edges in the inferred network which do not exist in the true network) plus the number of false negatives (edges in the true network not appearing in the inferred network).\nWe simulate co-occurrence observations in the following fashion. A random graph on 50 vertices is sampled. Disjoint sets of vertices are randomly chosen as path origins and destinations, paths are generated between each origin-destination pair using the shortest path algorithm with either unit weight per edge (\"shortest path\") or a random weight on each edge (\"random routing\"), and then co-occurrence observations are formed from each path. We keep the number of origins fixed at 5 and vary the number of destinations between 5 and 40 to see how the number of observations effects performance. NICO performance is compared against the frequency method (FM) described in [4].    1: Edge symmetric differences between inferred networks and the network one would obtain using co-occurrence measurements arranged in the correct order. Performance is averaged over 100 different network realizations. For each configuration 10 NICO and FM solutions are obtained via different initializations. We then choose the NICO solution yielding the largest likelihood, and compare with both the sparsest (fewest edges) and clairvoyant best (lowest error) FM solution.\nizations. For each network/path realization, the EM algorithm is executed with 10 random initializations. Exact E-step calculation is used for observations with N m \u2264 12, and importance sampling is used for longer paths. The longest observation in our data has N m = 19. The FM uses simple pairwise frequencies of co-occurrence to assign an order independently to each path observation. Of the 10 NICO solutions (different random initializations), we use the one based on parameter estimates yielding the highest likelihood score which also always gives the best performance. Because it is a heuristic, the FM does not provide a similar mechanism for ranking solutions from different initializations. We plot FM performance for two schemes; one based on choosing the sparsest FM solution (the one with the fewest edges), and one based on clairvoyantly choosing the FM solution with lowest error. NICO consistently outperforms even the clairvoyant best FM solution.\nOur method has also been applied to infer the stress-activated protein kinease (SAPK)/Jun Nterminal kinase (JNK) and NF\u03baB signal transduction pathways 1 (biological networks). The clustering procedure described in [2] is applied to microarray data in order to identify 18 co-occurrences arising from different environmental stresses or growth factors (path source) and terminating in the production of SAPK/JNK or NF\u03baB proteins. The reconstructed network (combined SAPK/JNK and NF\u03baB signal transduction pathways) is depicted in Figure 2. This structure agrees with the signalling pathways identified using traditional experimental techniques which test individually for each possible edge (e.g., \"MAPK\" and \"NF-\u03baB Signaling\" on http://www.cellsignal.com).", "publication_ref": ["b3", "b1"], "figure_ref": ["fig_2", "fig_8"], "table_ref": []}, {"heading": "Conclusion", "text": "This paper describes a probabilistic model and statistical inference procedure for inferring network structure from incomplete \"co-occurrence\" measurements. Co-occurrences are modelled as samples of a first-order Markov chain subjected to a random permutation. We describe exact and Monte Carlo EM algorithms for calculating maximum likelihood estimates of the Markov chain parameters (initial state distribution and transition matrix), treating the random permutations as hidden variables. Standard results for the EM algorithm guarantee convergence to a local maximum. Although our exact EM algorithm has exponential computational complexity, we provide finite-sample bounds guaranteeing convergence of the Monte Carlo EM variation to a local maximum with high probability and with only polynomial complexity. Our algorithm is easily extended to compute maximum a posteriori estimates, applying a Dirichlet prior to the initial state distribution and to each row of the Markov transition matrix. Co-occurrences are obtained from gene expression data via the clustering algorithm described in [2], and then network is inferred using NICO.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "The authors of this paper would like to thank D. Zhu   ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Systems Biology in Practice: Concepts, Implementation and Application", "journal": "John Wiley & Sons", "year": "2005", "authors": "E Klipp; R Herwig; A Kowald; C Wierling; H Lehrach"}, {"ref_id": "b1", "title": "Network constrained clustering for gene microarray data", "journal": "Bioinformatics", "year": "2005", "authors": "D Zhu; A O Hero; H Cheng; R Khanna; A Swaroop"}, {"ref_id": "b2", "title": "A computational approach for ordering signal transduction pathway components from genomics and proteomics data", "journal": "BMC Bioinformatics", "year": "2004-10", "authors": "Y Liu; H Zhao"}, {"ref_id": "b3", "title": "Understanding the topology of a telephone network via internally-sensed network tomography", "journal": "", "year": "2005", "authors": "M G Rabbat; J R Treichler; S L Wood; M G Larimore"}, {"ref_id": "b4", "title": "Classes of network connectivity and dynamics", "journal": "Complexity", "year": "2002", "authors": "O Sporns; G Tononi"}, {"ref_id": "b5", "title": "Organization, development and function of complex brain networks", "journal": "Trends in Cognitive Science", "year": "2004", "authors": "O Sporns; D R Chialvo; M Kaiser; C C Hilgetag"}, {"ref_id": "b6", "title": "Supplement to inferring network structure from co-occurrences", "journal": "", "year": "2006-10", "authors": "M G Rabbat; M A T Figueiredo; R D Nowak"}, {"ref_id": "b7", "title": "cGraph: A fast graph-based method for link analysis and queries", "journal": "", "year": "2003-08", "authors": "J Kubica; A Moore; D Cohn; J Schneider"}, {"ref_id": "b8", "title": "Learning Bayesian networks: The combination of knowledge and statistical data", "journal": "", "year": "1995", "authors": "D Heckerman; D Geiger; D Chickering"}, {"ref_id": "b9", "title": "Being Bayesian about Bayesian network structure: A Bayesian approach to structure discovery in Bayesian networks", "journal": "", "year": "2003", "authors": "N Friedman; D Koller"}, {"ref_id": "b10", "title": "On the convergence of the EM algorithm", "journal": "J. Royal Statistical Society B", "year": "1983", "authors": "R A Boyles"}, {"ref_id": "b11", "title": "On the convergence properties of the EM algorithm", "journal": "Ann. of Statistics", "year": "1983", "authors": "C F J Wu"}, {"ref_id": "b12", "title": "Monte Carlo Statistical Methods", "journal": "Springer Verlag", "year": "1999", "authors": "C Robert; G Casella"}, {"ref_id": "b13", "title": "Ascent-based Monte Carlo EM", "journal": "J. Royal Statistical Society B", "year": "2005", "authors": "B S Caffo; W Jank; G L Jones"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "the expected value of log P [X , R|A, \u03c0] w.r.t. the missing R, conditioned on the observations and on the current model estimate (A k , \u03c0 k ). Examining log P [X , R|A, \u03c0] reveals that it is linear w.r.t. simple functions of R: (a) the first row of each r (m) , i.e., r (m) 1,t ; (b) sums of transition indicators, i.e., \u03b1 t\u22121,t . Consequently, the E-step reduces to computing the conditional expectations of r (m) 1,t and \u03b1 (m) t ,t , denotedr", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "t ,t , respectively, and plugging them into the complete log-likelihood (2), which yields Q A, \u03c0; A k , \u03c0 k .", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "= 1 .1Under the first constraint, estimates of the initial state probabilities are simply given by", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 11Figure1plots the edge error for synthetic data generated using (a) shortest path routing, and (b) random routing. Each curve is the average performance over 100 different network and path real-", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "FigureFigure1: Edge symmetric differences between inferred networks and the network one would obtain using co-occurrence measurements arranged in the correct order. Performance is averaged over 100 different network realizations. For each configuration 10 NICO and FM solutions are obtained via different initializations. We then choose the NICO solution yielding the largest likelihood, and compare with both the sparsest (fewest edges) and clairvoyant best (lowest error) FM solution.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 2 :2Figure 2: Inferred topology of the combined SAPK/JNK and NF\u03baB signal transduction pathways.Co-occurrences are obtained from gene expression data via the clustering algorithm described in[2], and then network is inferred using NICO.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "y (m) = {y (m) 1 , . . . , y (m)", "formula_coordinates": [2.0, 402.62, 662.73, 93.41, 13.95]}, {"formula_id": "formula_1", "formula_text": "(m) 1 , . . . , z (m)", "formula_coordinates": [2.0, 150.55, 690.39, 54.74, 13.95]}, {"formula_id": "formula_2", "formula_text": "(m) 1 , . . . , \u03c4 (m) Nm ) such that z (m) t = y (m) \u03c4 (m) t", "formula_coordinates": [2.0, 327.35, 690.39, 155.66, 18.47]}, {"formula_id": "formula_3", "formula_text": "(z t\u22121 , z t ) \u2208 E, for t = 2, ..., N m .", "formula_coordinates": [2.0, 108.0, 709.56, 134.93, 9.65]}, {"formula_id": "formula_4", "formula_text": "(m) t = y (m) \u03c4 (m) t , for t = 1, . . . , N m .", "formula_coordinates": [3.0, 108.0, 127.51, 396.0, 30.19]}, {"formula_id": "formula_5", "formula_text": "log P [Y|A, \u03c0] = T m=1 \uf8eb \uf8ed log \uf8eb \uf8ed \u03c4 \u2208S Nm P [y (m) |\u03c4 , A, \u03c0] \uf8f6 \uf8f8 \u2212 log(N m !) \uf8f6 \uf8f8 .(1)", "formula_coordinates": [3.0, 152.5, 173.87, 351.5, 34.34]}, {"formula_id": "formula_6", "formula_text": "P [y|\u03c4 , A, \u03c0] = \u03c0 y\u03c4 1 N t=2 A y\u03c4 t\u22121", "formula_coordinates": [3.0, 136.37, 215.64, 143.33, 14.84]}, {"formula_id": "formula_7", "formula_text": "(m) 1 , ..., w (m)", "formula_coordinates": [3.0, 173.64, 322.06, 51.18, 13.95]}, {"formula_id": "formula_8", "formula_text": "(m) t = (w (m) t,1 , ..., w (m) t,|V | ) \u2208 {0, 1} |V | , with (w (m) t,i = 1) \u21d4 (z (m) t = i); let W = {w (1) , ..., w (T ) }. Let X = {x", "formula_coordinates": [3.0, 108.0, 322.06, 396.0, 42.26]}, {"formula_id": "formula_9", "formula_text": "x (m) = (x (m) 1 , ..., x (m) Nm ), where x (m) t = (x (m) t,1 , ..., x (m) t,|V | ) \u2208 {0, 1} |V | , with (x (m) t,i = 1) \u21d4 (y (m) t = i).", "formula_coordinates": [3.0, 108.0, 354.01, 396.0, 27.02]}, {"formula_id": "formula_10", "formula_text": "t,t = 1) \u21d4 (\u03c4 (m) t = t ).", "formula_coordinates": [3.0, 234.58, 395.22, 111.82, 14.21]}, {"formula_id": "formula_11", "formula_text": "log P [X |R, A, \u03c0] = T m=1 log P [x (m) |r (m) , A, \u03c0] = T m=1 |V | i,j=1 Nm t ,t =1 Nm t=2 r (m) t,t r (m) t\u22121,t x (m) t ,i x (m) t ,j log A i,j + T m=1 |V | i=1 Nm t =1 r (m) 1,t x (m) t ,i log \u03c0 i , (2)", "formula_coordinates": [3.0, 124.98, 427.29, 379.02, 66.62]}, {"formula_id": "formula_12", "formula_text": "Q A, \u03c0; A k , \u03c0 k = E log P [X , R|A, \u03c0] X , A k , \u03c0 k ,", "formula_coordinates": [3.0, 108.0, 527.8, 396.0, 23.49]}, {"formula_id": "formula_13", "formula_text": "] = (N m !) \u22121 , P r (m) 1,t = 1] = (N m \u2212 1)!/N m ! = 1/N m , and P [r (m) |r (m) 1,t = 1] = 1/(N m \u2212 1)!.", "formula_coordinates": [3.0, 108.0, 642.96, 396.0, 29.89]}, {"formula_id": "formula_14", "formula_text": "r (m) 1,t = \u03b3 (m) t Nm t =1 \u03b3 (m) t with \u03b3 (m) t = r: r 1,t =1 P x (m) r, A k , \u03c0 k ,(3)", "formula_coordinates": [3.0, 153.0, 689.17, 351.0, 31.18]}, {"formula_id": "formula_15", "formula_text": "P x (m) r, A k , \u03c0 k = P y (m) \u03c4 , A k , \u03c0 k = \u03c0 k y (m) \u03c4 1 Nm t=2 A k y (m) \u03c4 t\u22121 ,y (m) \u03c4 t .", "formula_coordinates": [4.0, 169.12, 91.09, 273.77, 30.31]}, {"formula_id": "formula_16", "formula_text": "(m) t,t r (m) t\u22121,t = 1] = (N m \u2212 2)!/N m ! and P [r (m) |r (m) t,t r (m) t\u22121,t = 1] = 1/(N m \u2212 2)!, leading t\u014d \u03b1 (m) t ,t = \u03b3 (m) t ,t Nm t =1 \u03b3 (m) t , with \u03b3 (m) t ,t = r P [x (m) |r, A k , \u03c0 k ] Nm t=2 r t,t r t\u22121,t . (4)", "formula_coordinates": [4.0, 108.0, 135.78, 599.26, 70.4]}, {"formula_id": "formula_17", "formula_text": "{r (m) 1,t } and {\u1fb1 (m) t ,t } requires O N m ! operations.", "formula_coordinates": [4.0, 155.18, 219.89, 198.82, 14.21]}, {"formula_id": "formula_18", "formula_text": "A k+1 i,j = T m=1 Nm t ,t =1\u1fb1 (m) t ,t x (m) t ,i x (m) t ,j |S| j=1 T m=1 Nm t ,t =1\u1fb1 (m) t ,t x (m) t ,i x (m) t ,j and \u03c0 k+1 i = T m=1 Nm t =1r (m) 1,t x (m) t ,i |S| i=1 T m=1 Nm t =1r (m) 1,t x (m) t ,i .", "formula_coordinates": [4.0, 108.0, 282.05, 398.43, 31.82]}, {"formula_id": "formula_19", "formula_text": "\u03c0 i = 1 T T m=1 x (m)", "formula_coordinates": [4.0, 190.97, 474.66, 82.79, 14.73]}, {"formula_id": "formula_20", "formula_text": "(p|f ) i = p i f i |V | j=1 p j f j , for i = 1, 2, . . . , |V |. (6", "formula_coordinates": [5.0, 207.85, 157.64, 292.28, 27.03]}, {"formula_id": "formula_21", "formula_text": ")", "formula_coordinates": [5.0, 500.13, 164.38, 3.87, 8.97]}, {"formula_id": "formula_22", "formula_text": "A v |f on V . Find t such that y t = v . Set \u03c4 i = t. Set f v = 0.", "formula_coordinates": [5.0, 143.87, 273.71, 282.28, 20.64]}, {"formula_id": "formula_23", "formula_text": "r 1,t = L =1 u r 1,t L =1 u and \u03b1 t ,t = L =1 u N t=2 r t,t r t\u22121,t L =1 u ,(7)", "formula_coordinates": [5.0, 169.22, 382.74, 334.78, 30.77]}, {"formula_id": "formula_24", "formula_text": "u = P [r |x, A, \u03c0] R[r |x, A, \u03c0] = P [\u03c4 |y, A, \u03c0] R[\u03c4 |y, A, \u03c0] = N t=2 N t =t A y \u03c4 t\u22121 ,y \u03c4 t .(8)", "formula_coordinates": [5.0, 173.45, 440.84, 330.55, 30.47]}, {"formula_id": "formula_25", "formula_text": "\u03b8 k+1 = arg max \u03b8 Q(\u03b8; \u03b8 k ),", "formula_coordinates": [5.0, 108.0, 633.9, 117.77, 17.54]}, {"formula_id": "formula_26", "formula_text": "A k i,j , \u03c0 k i \u2265 \u03b8 min .", "formula_coordinates": [6.0, 108.0, 330.06, 66.68, 12.33]}], "doi": ""}