{"title": "Less is More: CLIPBERT for Video-and-Language Learning via Sparse Sampling", "authors": "Jie Lei; Linjie Li; Luowei Zhou; Zhe Gan; Tamara L Berg; Mohit Bansal; Jingjing Liu", "pub_date": "2021-02-11", "abstract": "The canonical approach to video-and-language learning (e.g., video question answering) dictates a neural model to learn from offline-extracted dense video features from vision models and text features from language models. These feature extractors are trained independently and usually on tasks different from the target domains, rendering these fixed features sub-optimal for downstream tasks. Moreover, due to the high computational overload of dense video features, it is often difficult (or infeasible) to plug feature extractors directly into existing approaches for easy finetuning. To provide a remedy to this dilemma, we propose a generic framework CLIPBERT that enables affordable endto-end learning for video-and-language tasks, by employing sparse sampling, where only a single or a few sparsely sampled short clips from a video are used at each training step. Experiments on text-to-video retrieval and video question answering on six datasets demonstrate that CLIP-BERT outperforms (or is on par with) existing methods that exploit full-length videos, suggesting that end-to-end learning with just a few sparsely sampled clips is often more accurate than using densely extracted offline features from full-length videos, proving the proverbial less-is-more principle. Videos in the datasets are from considerably different domains and lengths, ranging from 3-second genericdomain GIF videos to 180-second YouTube human activity videos, showing the generalization ability of our approach. Comprehensive ablation studies and thorough analyses are provided to dissect what factors lead to this success. Our code is publicly available. 1 * Equal contribution.", "sections": [{"heading": "Introduction", "text": "Humans communicate with each other in this interactive and dynamic visual world via languages, signs, and gestures. The ability to jointly understand both visual and  textual clues is an essential ability for intelligent agents to interpret multimodal signals in the physical world. A wide range of tasks based on real-life videos have been designed to test such ability, including text-to-video retrieval [75,28,54], video captioning [54,75,82], video question answering [74,23,33,34], and video moment retrieval [2,18,35]. The de facto paradigm to tackle these cross-modal tasks is to first extract dense video features from pre-trained vision models [21,4] and text features from pre-trained language models [50,11], then apply multimodal fusion to wrangle together these fixed representations in a shared embedding space (Figure 1 (top)).\nExisting approaches [23,33,83,31] following this paradigm have achieved strong success, yet suffer from two main drawbacks: (i) Disconnection in tasks/domains: offline feature extractors are often trained on tasks and domains different from the target task. For example, features learned for action recognition from human activity videos [26] are incongruently applied to downstream video question answering on generic-domain GIF videos [23].\n(ii) Disconnection in multimodal features: learned features from different modalities are independent of each other. For instance, action recognition models [62,66,4] are typically trained from pure video data without textual input, yet are applied to video-and-language tasks. End-to-end taskspecific finetuning offers a way to mitigate these inherent disconnections. However, extracting features from the full sequence of video frames, as in most existing work, casts excessive demand on memory and computation, rendering it difficult or even infeasible to directly plug feature extractors into a video+language learning framework for efficient end-to-end finetuning.\nMotivated by this, we propose CLIPBERT, a generic and efficient framework for end-to-end video-and-language learning (Figure 1 (bottom)). Two aspects distinguish CLIP-BERT from previous work. First, in contrast to densely extracting video features (adopted by most existing methods), CLIPBERT sparsely samples only one single or a few short clips from the full-length videos at each training step. The hypothesis is that visual features from sparse clips already capture key visual and semantic information in the video, as consecutive clips usually contain similar semantics from a continuous scene. Thus, a handful of clips are sufficient for training, instead of using the full video. Then, predictions from multiple densely-sampled clips are aggregated to obtain the final video-level prediction during inference, which is less computational demanding. This sparse-training-then-dense-inference strategy greatly reduces memory needs and computations, allowing economical end-to-end learning from raw video frame pixels and language tokens.\nThe second differentiating aspect concerns the initialization of model weights (i.e., transfer through pre-training). In recent literature, image-text pre-training (e.g., using COCO Captions [5] or Visual Genome Captions [29]) has been applied to image-text tasks [61,44,6,58,22,36,81], and video-text pre-training (e.g., using HowTo100M [46]) to video-related tasks [59,83,15,37]. There has been no study to cross-examine the effect of image-text pre-training on video-text tasks. Intuitively, visual features learned through pre-training from large-scale image datasets should also help video understanding tasks that rely on visual clues in static video frames. To investigate this, we use 2D architectures (e.g., ResNet-50 [21]) instead of 3D features [62,4,51,73] as our visual backbone for video encoding, allowing us to harness the power of image-text pretraining for video-text understanding along with the advantages of low memory cost and runtime efficiency. Empirically, we observe that the knowledge learned in image-text pre-training indeed helps video-text tasks; this simple strategy helps us achieve better or comparable performance to previous state of the art on text-to-video retrieval and video question answering tasks.\nOur contributions are three-fold: (i) We propose CLIPBERT, a new end-to-end learning framework for video+language tasks. Experiments show that CLIP-BERT achieves superior (or on par) performance than existing methods across diverse video-text tasks, where the average video length ranges from a few seconds to three minutes. (ii) Our work suggests \"less is more\": the proposed end-to-end training strategy with a single or a few (less) sparsely sampled clips is often more accurate than traditional approaches that employ densely extracted video features. (iii) We demonstrate that image-text pre-training benefits video-text tasks. We also provide comprehensive ablation studies to reveal the key factors that lead to the success of CLIPBERT, in hope of inspiring more future work.", "publication_ref": ["b74", "b27", "b53", "b53", "b74", "b81", "b73", "b22", "b32", "b33", "b1", "b17", "b34", "b20", "b3", "b49", "b10", "b22", "b32", "b82", "b30", "b25", "b22", "b61", "b65", "b3", "b4", "b28", "b60", "b43", "b5", "b57", "b21", "b35", "b80", "b45", "b58", "b82", "b14", "b36", "b20", "b61", "b3", "b50", "b72"], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "Related Work", "text": "Video and Language Understanding. Popular video-andlanguage tasks include text-to-video retrieval [75,28,54], video captioning [75,82,28,54,40], video question answering [74,23,33], and moment retrieval [2,18,35]. Standard approaches [23,74,17,80,33,12,31,32] leverage offline extracted video and text features from action recognition models [26,66,4,73], image recognition models [10,21], and language models [47,50,11,42]. Aligned with the success of transformer-based [64] language pre-training [11,42,76,52,30,8] and image-text pre-training [61,44,6,36,22,81,16,7], video-text pretraining [59,83,15,37,45,46] has shown promising results on video-and-language tasks. Beyond using fixed features and same-domain data (i.e., video-text pre-training only for video-text tasks), our work focuses on end-to-end training and applying image-text pre-training for video-text tasks.\nAction Recognition. Modern video action recognition architectures are typically designed with deep 2D [56,60,21] or 3D [62,4,73] convolutional networks. These backbones are often computation and memory heavy, making it extremely difficult to directly process videos of considerable length. To ease this difficulty, instead of training on full-length videos, models are often trained with randomly sampled short clips from the videos [55,62,51,73,67,14,13,66]. At inference time, predictions from multiple uniformly sampled clips are aggregated together as the final video-level prediction. In relation to these works, we adopt a similar strategy to perform sparse training and dense inference to reduce overhead on video processing, but focus on video-and-language tasks with cross-modal modeling of video and language, in contrast to pure video modeling.", "publication_ref": ["b74", "b27", "b53", "b74", "b81", "b27", "b53", "b39", "b73", "b22", "b32", "b1", "b17", "b34", "b22", "b73", "b16", "b79", "b32", "b11", "b30", "b31", "b25", "b65", "b3", "b72", "b9", "b20", "b46", "b49", "b10", "b41", "b63", "b10", "b41", "b75", "b51", "b29", "b7", "b60", "b43", "b5", "b35", "b21", "b80", "b15", "b6", "b58", "b82", "b14", "b36", "b44", "b45", "b55", "b59", "b20", "b61", "b3", "b72", "b54", "b61", "b50", "b72", "b66", "b13", "b12", "b65"], "figure_ref": [], "table_ref": []}, {"heading": "CLIPBERT with Sparse Sampling", "text": "We propose CLIPBERT, a general framework that enables end-to-end learning on video and language data, by learning joint representations directly from video frame pixels and raw text tokens, instead of from offline-extracted single-modality features. Figure 1 (bottom) gives an overview of CLIPBERT framework. It adopts a sparse sampling strategy using only a single or a few sampled clips at each training step, instead of full-length videos. Each sampled clip is independently encoded with a vision backbone model, the visual features from which are then fed to a cross-modal module that extracts relations between the clip and its associated text representations. Independent predictions from all the sampled clips are fused together (e.g., through mean-pooling) to derive a consensus at the video level. A task-specific loss is calculated based on this consensus to learn model parameters. During inference, CLIP-BERT densely samples a sequence of clips from the video and aggregates their predictions as the final prediction.\nMost existing work [23,33,83,31] models offlineextracted dense video features and text features. Formally, we denote a video-text pair as V (for video) and S (for text sequence). The video V is further denoted as a list of N clips of equal duration [c 1 , c 2 , ..., c N ]. This standard paradigm can be formulated as:\np = H([F SG v (c 1 ), F SG v (c 2 ), ..., F SG v (c N )], F SG l (S)),(1)\nwhere F SG v and F SG l are vision and language encoder, respectively. The superscript SG denotes Stop Gradient, meaning that gradients cannot be back-propagated through the two encoders. H is a cross-modal encoder and predictor, which models the relations between the encoded video/language inputs and makes predictions. p is the video-level prediction. A task-specific loss function L task is then applied to calculate the loss value l task based on this prediction and its corresponding ground-truth q:\nl task = L task (p, q).(2)\nSparse Sampling for Training. Instead of using the fulllength video with N clips, CLIPBERT sparsely (and randomly) samples N train clips {c \u03c4i } Ntrain i=1 from V for training. N train is typically much smaller than N . We model a sampled clip c \u03c4i together with text input S to produce a prediction p \u03c4i :\np \u03c4i = H(F v (c \u03c4i ), F l (S)),(3)\nwhere F v and F l are vision/language encoders. Different from Equation 1 that uses offline vision/language encoders, CLIPBERT is end-to-end trainable, allowing task-specific loss to further finetune the encoders, learning better representations. Independent predictions from all sampled clips are aggregated to derive a consensus. The loss value l task is calculated based on this video-level consensus:  where G is the prediction/score aggregation function (e.g., mean-pooling). When N train is larger than one, this formulation can be regarded as a form of multiple instance learning (MIL) [70]. At inference, we uniformly sample N test clips of the same duration as training clips, then aggregate predictions from all N test clips to form our final prediction. CLIPBERT's sparse training strategy can be interpreted as a type of data augmentation: different subsets of clips from the same video are used at different training steps, which improves the model's generalization ability. In this sense, it is analogous to random cropping [56,21] commonly used in image classification tasks. It also takes inspiration from action recognition methods [55,62,66,14], where a video classifier is trained on sampled clips. Model Architecture. Figure 2 gives an overview of CLIP-BERT architecture. For the vision encoder F v , we use a 2D CNN architecture ResNet-50 [21] instead of 3D architectures (such as C3D [62] or I3D [4]), because 2D models typically consume less memory and run faster. Besides, 2D CNNs have proved to work reasonably well on video understanding tasks such as action recognition [66,51]. Specifically, we take the first 5 Conv blocks of ResNet-50 [21] and add an extra convolution layer to reduce its output feature depth, as well as a 2\u00d72 max-pooling layer for spatial down-sampling, following Pixel-BERT [22]. For each sampled clip, we uniformly sample T frames and obtain T feature maps. A temporal fusion layer M (e.g., mean-pooling) is applied to aggregate the frame-level feature maps into a single clip-level feature map. We then add a row-wise and a column-wise position embedding to each feature vector based on their 2D position. These embeddings are the same trainable position embeddings as in BERT [11]. Collectively, these two position embeddings are indicative of 2D spatial locations of the features, which can be viewed as a 2D position embedding. The resulting feature map is flattened into an embedding sequence to represent the clip.\nl task = L task (G(p \u03c41 , p \u03c42 , ..., p \u03c4 N train ), q),(4)\nWe use a trainable word embedding layer as our language encoder F l to encode language tokens and add trainable position embeddings to encode positional information of the tokens. Next, we add different type embeddings [11] to both clip and text embeddings to indicate their source type. These two sequences are then concatenated as inputs to a 12-layer transformer [64,11] for cross-modal fusion. Special tokens [CLS] and [SEP] are added in this process following [11]. Given a downstream task, we add a task-specific prediction head with the last-layer [CLS] representation as input (e.g., using a two-layer MLP with softmax to produce scores for text-to-video retrieval).\nWeight Initialization and Pre-training. We initialize the ResNet-50 layers with weights from grid-feat [24,53]. It is trained on Visual Genome [29] for object detection and attribute classification, and produces effective grid features for image VQA tasks [3,20]. Input frames are resized to have a maximum longer side of L while keeping the aspect ratios, and the shorter side is zero-padded to be L as well [48]. We initialize the transformer and word embedding layers from BERT-base model [11], pre-trained on BookCorpus [84] and English Wikipedia. These weights are trained separately for their individual single-modality tasks, thus simply combining them together in a single framework for downstream task training may result in suboptimal performance. Although pre-training the whole model end-to-end with large-scale video-text datasets such as HowTo100M [46] are appealing, we are restricted by the enormous computation cost. 2 Luckily, as we use 2D CNN as our vision encoder, CLIPBERT is able to directly take image-text pairs as inputs for training. Thus, we leverage large-scale image-text datasets (COCO Captions [5] and Visual Genome Captions [29]) to perform cross-modal pretraining [61,44,22]. Specifically, we use masked language modeling [11] and image-text matching [61,44] objectives to optimize the model. By default, we finetune our model from this pre-trained weights for downstream video-text tasks. The impact of different weight initialization strategies is examined in Section 4.3.\nImplementation Details. We perform image-text pretraining on COCO Captions [5] and Visual Genome Captions [29]. These two datasets contain a total of 5.6M training image-text pairs on 151K images. This is the same data used in UNITER's [6] in-domain pre-training. We use input image size L=768, and the resulting feature map from the vision encoder contains 144 pixels. To improve generalization and reduce computation cost, during pre-training, we follow Pixel-BERT [22] to use pixel random sampling that samples 100 pixels from the encoded feature map as the input to the transformer layers. Note that we only apply pixel random sampling for pre-training, and always use the full feature map for downstream tasks to avoid misalignment in training and inference [22]. We use WordPiece embeddings [71] and keep at most 20 tokens from the caption. We then randomly mask 15% of the tokens for masked language modeling. For each image-caption pair, with a probability of 0.5, we replace the ground-truth caption with a randomly sampled caption from another image to form a negative pair for image-text matching. We use AadmW [43] to optimize end-to-end model training, with an initial learning rate of 5e-5, \u03b2 1 =0.9, \u03b2 2 =0.98, and use learning rate warmup over the first 10% training steps followed by linear decay to 0. Our model is implemented in PyTorch [49] and transformers [69]. It is trained for 40 epochs with mixed precision, on 8 NVIDIA V100 GPUs with a batch size of 32 per GPU. The whole training process takes 4 days to complete.\nFor downstream finetuning, we use the same training and optimizer configurations except that the default input image size is set to 448 (due to the typically lower resolution of videos compared to images). Since downstream datasets vary in scale and domain, we use task-specific learning rates and training epochs based on validation performance.", "publication_ref": ["b22", "b32", "b82", "b30", "b69", "b55", "b20", "b54", "b61", "b65", "b13", "b20", "b61", "b3", "b65", "b50", "b20", "b21", "b10", "b10", "b63", "b10", "b10", "b23", "b52", "b28", "b2", "b19", "b47", "b10", "b83", "b45", "b1", "b4", "b28", "b60", "b43", "b21", "b10", "b60", "b43", "b4", "b28", "b5", "b21", "b21", "b70", "b42", "b48", "b68"], "figure_ref": ["fig_1", "fig_2"], "table_ref": []}, {"heading": "Experiments", "text": "In this section, we evaluate CLIPBERT on two popular video-and-language tasks, text-to-video retrieval and video question answering, across six different datasets. We also provide extensive ablation studies to analyze the key factors that contribute to CLIPBERT's success.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Downstream Tasks", "text": "Text-to-Video Retrieval. (i) MSRVTT [75] contains 10K YouTube videos with 200K descriptions. We follow [77,46], using 7k train+val videos for training and report results on the 1K test set [77]. We also create a local val set by sampling 1K video-caption pairs from unused test videos for our ablation study. (ii) DiDeMo [2] contains 10K Flickr videos annotated with 40K sentences. (iii) ActivityNet Captions [28] contains 20K YouTube videos annotated with 100K sentences. The training set contains 10K videos, and we use val1 set with 4.9K videos to report results. For MSRVTT, we evaluate standard sentenceto-video retrieval. For DiDeMo and ActivityNet Captions, we follow [80,41] to evaluate paragraph-to-video retrieval, where all sentence descriptions for a video are concatenated   to form a paragraph for retrieval. We use average recall at K (R@K) and median rank (MdR) to measure performance. MSRVTT multiple-choice test [77] is a multiple-choice task with videos as questions, and captions as answers.\nVideo\nEach video contains 5 captions, with only one positive match. For all the QA tasks, we use standard train/val/test splits and report accuracy to measure performance. Figure 3 shows a comparison of average video length of evaluated datasets. Videos across datasets demonstrate considerable difference in domains and lengths, ranging from 3-second generic-domain GIF videos in TGIF-QA to 180second human activity videos in ActivityNet Captions.", "publication_ref": ["b74", "b76", "b45", "b76", "b27", "b79", "b40", "b76"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Analysis of Sparse Sampling", "text": "We conduct comprehensive ablation studies concerning various aspects of CLIPBERT's design in this section and Section 4.3. If not otherwise stated, we randomly sample a single frame (N train =1 and T =1) from full-length videos for training, and use the middle frame (N test =1) for inference, with input image size L=448. All ablation results are on MSRVTT retrieval local val and MSRVTT-QA val sets.\nDo we need larger input image size? We compare models with different input image sizes L \u2208 {224, 448, 768, 1000}, results shown in Table 1. Compared to the model with L=224, larger input resolution improves performance on the retrieval task, while maintaining a similar performance on the QA task. The best performance is achieved at around L=448. Further increasing the resolution does not provide significant performance boost. [24] shows that increasing input image size from 448 to 1333 always improves image VQA [3] performance with no sign of saturation, while we observe the performance converges at around 448  for MSRVTT retrieval and QA. This is potentially because VQA images are typically of higher raw resolution than MSRVTT videos (we are only able to obtain videos at a maximum height of 240 pixels). We expect higher resolution videos could further improve model performance.\nDo we need densely sampled frames? A common practice for video understanding and video+language understanding is to model densely sampled frames from the original video (e.g., [4,73] sample frames at 25 frames per second). To understand the impact of using densely sampled frames, we conduct a set of controlled experiments. Specifically, we randomly sample a fixed-length 1-second clip from the video, then evenly sample T ={1, 2, 4, 8, 16} frames within this clip for training. For inference, we use the middle clip of the video. When multiple frames are used (i.e., T >1), we use mean pooling for temporal fusion. We also experiment with variants using additional 3D convolutions before mean pooling: (i) Conv3D: a standard 3D convolution layer with kernel size 3, stride 1; (ii) Conv(2+1)D: a spatial and temporal separable 3D convolution [63,73]. Adding 3D convolutions to 2D convolutions essentially leads to a design similar to Top-Heavy S3D architecture [73], which shows better performance than full 3D convolutions on video action recognition and runs faster.\nResults are shown in Table 2. Overall, models that use 3D convolutions perform worse than models that adopt a simple mean pooling. For mean pooling, we observe that using two frames provides a notable improvement over using a single frame. However, models that use more than two frames perform similarly compared to the one using two frames, suggesting that two frames already represent enough local temporal information for the tasks.\nDo more clips at inference help? At inference, we aggregate prediction scores from multiple densely sampled clips as the final score. To show how this strategy affects performance, we evenly sample N test \u2208 {1, 2, 4, 8, 16} clips from a video and average their individual predictions at inference. For this experiment, we provide two models trained with    [45]. In mean pooling and max pooling, the cross-clip pooling is performed over logits, followed by a softmax operator. In LogSumExp, logits from each clip are first fed through an element-wise exponential operator, followed by a cross-clip mean pooling. The aggregated output is further normalized by its own sum to make it eligible as a probability distribution. For simplicity, we always use the same aggregation function for training and inference. For a fair comparison, all models use a single frame per clip for training and 16 clips for inference, i.e., T =1 and N test =16.\nResults are shown in Table 3. In general, adding more clips helps, and the second added clip gives the most performance gain. For example, for models with LogSum-Exp, N train =2 improves retrieval R1 score of N train =1 by 2.8%, while N train =16 improves only 1.9% upon N train =2, even though it adds much more clips. As for score aggregation function G, LogSumExp works the best.\nSparse Random Sampling vs. Dense Uniform Sampling. At each training step, CLIPBERT randomly samples only a single or a few short clips from a full-length video. Intuitively, this sparse random sampling strategy can be interpreted as a type of data augmentation where different subsets of clips for a video are used to calculate the loss at different training steps. To show the effectiveness of this approach, we compare CLIPBERT with a variant that uses uniformly sampled dense clips. Specifically, we use the same CLIPBERT architecture as before but always uses 16 uniformly sampled clips for training.  metrics in both retrieval and QA tasks. Meanwhile, using 4 clips is much more memory and computation efficient than using 16 clips, as we show in the next paragraph. In addition to these two sampling approaches, it is also possible to choose clips using content-based methods such as [72].\nHowever, this requires an extra non-trivial selection step, and may also remove some of the data augmentation effect brought by random sampling.\nMemory and Computation Cost. Figure 5 shows a comparison of memory and computation cost w.r.t. different numbers of clips (N train ) or frames (T ) at training. We observe that using more clips or more frames at training considerably increases memory demand and computational cost. For example, in Figure 5 (a), we see that the maximum allowed batch size for a single NVIDIA V100 GPU is 190 when N train =2, compared to that of 16 when N train =16. Meanwhile, in Figure 5 (b), we see that the time cost increases almost linearly with N train , while the performance improvement on MSRVTT retrieval is less significant. These comparisons demonstrate the efficiency and effectiveness of the proposed sparse training strategy.  [10], frame-wise action recognition model TSN [66,9] pre-trained on Kinetics-700 [57,4], or detection model grid-feat [24] pre-trained on Visual Genome [29]. For transformer and word embedding layers, we use weights from random initialization or pre-trained BERT BASE model [11].\nFor random initialization, we use the default setup in Py-Torch [49] and Transformer [68] libraries for CNN and transformer layers, respectively. Results are summarized in Table 5. We notice that randomly initializing CNN leads to massive performance degradation or even training failure, we hypothesize that it is mostly because of the difficulty of training large models on relatively small datasets (e.g.,  MSRVTT retrieval train split: 7K videos). The best performance is achieved using image-text pre-trained weights, clearly indicating the benefit of utilizing image-text pretraining for video-text tasks.\nImpact of End-to-End Training. The standard paradigm for video-and-language understanding is to train models with offline extracted features, in which the task objective does not affect the video and text encoding process. In this work, we train our model in an end-to-end manner, allowing the model to finetune feature representations by leveraging task supervision. In Table 6, we compare our model with variants that freeze portions of the parameters. Overall, the best performance is achieved by our model, showing the importance of end-to-end training. Note that all the models in Table 6 are finetuned from our end-to-end image-text pretrained model, which partly resolves the multimodal feature disconnection issue in Section 1. Thus, we expect smaller improvement from further end-to-end finetuning. 11.9 33.6 -13.0 FSE [80] 13.9 36.0 -11.0 CLIPBERT 4\u00d71 19.9 44.5 56.   Similarly, on DiDeMo and ActivityNet Captions paragraph-to-video retrieval tasks (Table 7b and Table 7c), we notice our best CLIPBERT models outputform CE [41] by 4.3% and 3.1% on R1, respectively, despite CE's use of appearance, scene, motion, face, audio, OCR, ASR features densely extracted from 11 different models. ActivityNet Caption videos are on average 180-second long. In Table 7c we show CLIPBERT performs competitively with existing methods that model long-range relations in this dataset. Es-pecially, CLIPBERT obtains 0.8% higher R1 than HSE [80] and is competitive compared to MMT [15] that uses extra audio features 3 , even though CLIPBERT 4\u00d72 * (N test =20) samples only 8-second clips from 180-second videos at each training step, and uses only 40-second content for inference. We expect CLIPBERT's performance to be further improved by sampling more clips during training and inference. Meanwhile, we also encourage future work to explore combining extra input signals, such as audio, into the CLIP-BERT framework for better performance.\nVideo Question Answering. Table 8 shows evaluation results on video question answering. Across all three tasks, CLIPBERT achieves significant performance gain. In Table 8a, CLIPBERT 1\u00d71 outperforms prior state-of-the-art QueST [25] by 6.9%, 6.8%, and 0.6% on TGIF-QA Action, Transition, and FrameQA tasks, respectively. This is especially surprising considering CLIPBERT 1\u00d71 uses only a single randomly sampled frame from the videos at each training step, while QueST uses 10 uniformly sampled frames. Moreover, when using only a single frame (the middle frames of the videos) for inference, CLIPBERT 1\u00d71 (N test =1) already far outperforms QueST on Action and Transition tasks, and is on par with QueST on FrameQA.\nIn ", "publication_ref": ["b23", "b2", "b3", "b72", "b62", "b72", "b72", "b44", "b71", "b9", "b65", "b8", "b56", "b3", "b23", "b28", "b10", "b48", "b67", "b79", "b40", "b79", "b14", "b2"], "figure_ref": ["fig_4", "fig_4", "fig_4"], "table_ref": ["tab_2", "tab_5", "tab_7", "tab_10", "tab_12", "tab_12", "tab_14", "tab_14", "tab_14", "tab_15"]}, {"heading": "Conclusion", "text": "We present CLIPBERT, a generic framework for end-toend video-and-language learning, which adopts sparse sampling to use only a few sampled short clips from the videos at each training step. Experiments across diverse tasks show that CLIPBERT outperforms (or is on par with) state-ofthe-art methods with densely sampled offline features, suggesting that the \"less is more\" principle is highly effective in practice. Comprehensive ablation studies reveal several key factors that lead to this success, including sparse sampling, end-to-end training, and image-text pre-training.\nOur CLIPBERT is quite generic, once trained, it can be easily adopted and transferred for various downstream tasks. In particular, in this work, we focus on text-to-video retrieval and video question answering.  Text-to-video Retrieval. We use a two-layer MLP with the last layer [CLS] token hidden state for a two way (i.e., matched or not matched) classification for retrieval. We use LogSumExp loss for training. Denote the two-way classification logit output for clip \u03c4 i from the video associated with the j-th example as g (j)\n\u03c4i \u2208 R 2 , where i = 1, . . . , N train for training (i = 1, . . . , N test for inference; see Section 3 of the main paper). The LogSumExp prediction p (j) \u2208 R 2 is defined as:\np (j) = Ntrain i=1 e g (j) \u03c4 i sum( Ntrain i=1 e g (j) \u03c4 i ) .(5)\nWe then use a negative log likelihood loss for training:\nL = \u2212 1 |D| |D| j=1 logp (j) [y j ],(6)\nwhere D is the dataset, y j is the index of the ground-truth answer for the j-th example.\nWe conduct experiments on three popular text-to-video retrieval datasets, MSRVTT [75], DiDeMo [2], and Activi-tyNet Captions [28]. Table 10   Video Question Answering. Similar to text-to-video retrieval task, we take the last layer [CLS] token hidden state through a two-layer MLP for classification. We use Log-SumExp to aggregate prediction from multiple clips to calculate loss. The formulation of LogSumExp loss is simlar to Equation 5 except that the dimension of g \u03c4i equals to the number of answer candidates.\nWe conduct experiments on three video QA datasets, TGIF-QA [23], MSRVTT-QA [74], and MSRVTT MC Test [77]. For TGIF-QA, we evaluate three sub-tasks, i.e., Action, Transition, and FrameQA. We train a separate model for each of the evaluated TGIF-QA tasks. For MSRVTT MC Test, as it uses the same training set as the MSRVTT retrieval task, we directly use the trained retrieval model to rank the five candidate answers.  ", "publication_ref": ["b74", "b1", "b27", "b22", "b73", "b76"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "", "text": "Acknowledgements: This research was partially done when Jie was an intern with Microsoft. He was later supported at UNC by NSF Award #1562098, DARPA KAIROS Grant #FA8750-19-2-1004, ARO-YIP Award #W911NF-18-1-0336, and Microsoft Investigator Fellowship. The views contained in this article are those of the authors and not of the funding agency.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Additional Experiments", "text": "Visual Question Answering. As CLIPBERT is designed based on 2D CNN, and is pre-trained on image-text corpus, it is also directly applicable to image-text downstream tasks, such as image-based question answering. We show CLIPBERT's performance on VQA 2.0 dataset [19] in Table 9. The model is finetuned from the image-text pretrained weights on 8GPUs for 13 epochs, with batch size 32 and learning rate 5e-5. CLIPBERT shows a reasonable performance compared to the strong pre-training baselines. Note that CLIPBERT uses grid features [24,22] instead of the commonly used region features, which is much more computation efficient, e.g., extracting grid features is around 80\u00d7 faster than extracting region features according to the computation time reported in [24].", "publication_ref": ["b18", "b23", "b21", "b23"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Bottom-up and top-down attention for image captioning and visual question answering", "journal": "", "year": "2018", "authors": "Peter Anderson; Xiaodong He; Chris Buehler; Damien Teney; Mark Johnson; Stephen Gould; Lei Zhang"}, {"ref_id": "b1", "title": "Localizing moments in video with natural language", "journal": "", "year": "2009", "authors": "Lisa Anne Hendricks; Oliver Wang; Eli Shechtman; Josef Sivic; Trevor Darrell; Bryan Russell"}, {"ref_id": "b2", "title": "Vqa: Visual question answering", "journal": "", "year": "2015", "authors": "Stanislaw Antol; Aishwarya Agrawal; Jiasen Lu; Margaret Mitchell; Dhruv Batra; Lawrence Zitnick; Devi Parikh"}, {"ref_id": "b3", "title": "Quo vadis, action recognition? a new model and the kinetics dataset", "journal": "", "year": "2007", "authors": "Joao Carreira; Andrew Zisserman"}, {"ref_id": "b4", "title": "Microsoft coco captions: Data collection and evaluation server", "journal": "arXiv", "year": "2015", "authors": "Xinlei Chen; Hao Fang; Tsung-Yi Lin; Ramakrishna Vedantam; Saurabh Gupta; Piotr Doll\u00e1r; C Lawrence Zitnick"}, {"ref_id": "b5", "title": "Uniter: Learning universal image-text representations", "journal": "", "year": "2020", "authors": "Yen-Chun Chen; Linjie Li; Licheng Yu; Ahmed El Kholy; Faisal Ahmed; Zhe Gan; Yu Cheng; Jingjing Liu"}, {"ref_id": "b6", "title": "Unifying vision-and-language tasks via text generation", "journal": "arXiv", "year": "2021", "authors": "Jaemin Cho; Jie Lei; Hao Tan; Mohit Bansal"}, {"ref_id": "b7", "title": "Electra: Pre-training text encoders as discriminators rather than generators", "journal": "", "year": "", "authors": "Kevin Clark; Minh-Thang Luong; V Quoc; Christopher D Le;  Manning"}, {"ref_id": "b8", "title": "Openmmlab's next generation video understanding toolbox and benchmark", "journal": "", "year": "", "authors": ""}, {"ref_id": "b9", "title": "Imagenet: A large-scale hierarchical image database", "journal": "", "year": "2009", "authors": "Jia Deng; Wei Dong; Richard Socher; Li-Jia Li; Kai Li; Li Fei-Fei"}, {"ref_id": "b10", "title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2007", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b11", "title": "Heterogeneous memory enhanced multimodal attention model for video question answering", "journal": "", "year": "2008", "authors": "Chenyou Fan; Xiaofan Zhang; Shu Zhang; Wensheng Wang; Chi Zhang; Heng Huang"}, {"ref_id": "b12", "title": "X3d: Expanding architectures for efficient video recognition", "journal": "", "year": "", "authors": "Christoph Feichtenhofer"}, {"ref_id": "b13", "title": "Slowfast networks for video recognition", "journal": "", "year": "2008", "authors": "Christoph Feichtenhofer; Haoqi Fan; Jitendra Malik; Kaiming He"}, {"ref_id": "b14", "title": "Multi-modal transformer for video retrieval", "journal": "", "year": "2008", "authors": "Valentin Gabeur; Chen Sun; Karteek Alahari; Cordelia Schmid"}, {"ref_id": "b15", "title": "Large-scale adversarial training for visionand-language representation learning", "journal": "", "year": "", "authors": "Zhe Gan; Yen-Chun Chen; Linjie Li; Chen Zhu; Yu Cheng; Jingjing Liu"}, {"ref_id": "b16", "title": "Motion-appearance co-memory networks for video question answering", "journal": "", "year": "2008", "authors": "Jiyang Gao; Runzhou Ge; Kan Chen; Ram Nevatia"}, {"ref_id": "b17", "title": "Tall: Temporal activity localization via language query", "journal": "", "year": "2017", "authors": "Jiyang Gao; Chen Sun; Zhenheng Yang; Ram Nevatia"}, {"ref_id": "b18", "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering", "journal": "", "year": "2017", "authors": "Yash Goyal; Tejas Khot; Douglas Summers-Stay; Dhruv Batra; Devi Parikh"}, {"ref_id": "b19", "title": "Vizwiz grand challenge: Answering visual questions from blind people", "journal": "", "year": "2018", "authors": "Danna Gurari; Qing Li; Abigale J Stangl; Anhong Guo; Chi Lin; Kristen Grauman; Jiebo Luo; Jeffrey P Bigham"}, {"ref_id": "b20", "title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"ref_id": "b21", "title": "Pixel-bert: Aligning image pixels with text by deep multi-modal transformers", "journal": "", "year": "2009", "authors": "Zhicheng Huang; Zhaoyang Zeng; Bei Liu; Dongmei Fu; Jianlong Fu"}, {"ref_id": "b22", "title": "Tgif-qa: Toward spatio-temporal reasoning in visual question answering", "journal": "", "year": "2005", "authors": "Yunseok Jang; Yale Song; Youngjae Yu; Youngjin Kim; Gunhee Kim"}, {"ref_id": "b23", "title": "In defense of grid features for visual question answering", "journal": "", "year": "", "authors": "Huaizu Jiang; Ishan Misra; Marcus Rohrbach; Erik Learned-Miller; Xinlei Chen"}, {"ref_id": "b24", "title": "Divide and conquer: Question-guided spatiotemporal contextual attention for video question answering", "journal": "", "year": "2020", "authors": "Jianwen Jiang; Ziqiang Chen; Haojie Lin; Xibin Zhao; Yue Gao"}, {"ref_id": "b25", "title": "The kinetics human action video dataset", "journal": "", "year": "2002", "authors": "Will Kay; Joao Carreira; Karen Simonyan; Brian Zhang; Chloe Hillier; Sudheendra Vijayanarasimhan; Fabio Viola; Tim Green; Trevor Back; Paul Natsev"}, {"ref_id": "b26", "title": "Hadamard product for low-rank bilinear pooling", "journal": "", "year": "2016", "authors": "Jin-Hwa Kim; Woosang Kyoung-Woon On; Jeonghee Lim; Jung-Woo Kim; Byoung-Tak Ha;  Zhang"}, {"ref_id": "b27", "title": "Dense-captioning events in videos", "journal": "", "year": "2004", "authors": "Ranjay Krishna; Kenji Hata; Frederic Ren; Li Fei-Fei; Juan Carlos Niebles"}, {"ref_id": "b28", "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "journal": "IJCV", "year": "2007", "authors": "Ranjay Krishna; Yuke Zhu; Oliver Groth; Justin Johnson; Kenji Hata; Joshua Kravitz; Stephanie Chen; Yannis Kalantidis; Li-Jia Li; David A Shamma"}, {"ref_id": "b29", "title": "Albert: A lite bert for self-supervised learning of language representations", "journal": "", "year": "", "authors": "Zhenzhong Lan; Mingda Chen; Sebastian Goodman; Kevin Gimpel; Piyush Sharma; Radu Soricut"}, {"ref_id": "b30", "title": "Hierarchical conditional relation networks for video question answering", "journal": "", "year": "2009", "authors": "Vuong Thao Minh Le; Svetha Le; Truyen Venkatesh;  Tran"}, {"ref_id": "b31", "title": "Mart: Memory-augmented recurrent transformer for coherent video paragraph captioning", "journal": "", "year": "", "authors": "Jie Lei; Liwei Wang; Yelong Shen; Dong Yu; Tamara L Berg; Mohit Bansal"}, {"ref_id": "b32", "title": "Tvqa: Localized, compositional video question answering", "journal": "", "year": "2018", "authors": "Jie Lei; Licheng Yu; Mohit Bansal; Tamara L Berg"}, {"ref_id": "b33", "title": "Tvqa+: Spatio-temporal grounding for video question answering", "journal": "", "year": "", "authors": "Jie Lei; Licheng Yu; Tamara L Berg; Mohit Bansal"}, {"ref_id": "b34", "title": "Tvr: A large-scale dataset for video-subtitle moment retrieval", "journal": "", "year": "2020", "authors": "Jie Lei; Licheng Yu; Tamara L Berg; Mohit Bansal"}, {"ref_id": "b35", "title": "Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training", "journal": "", "year": "", "authors": "Gen Li; Nan Duan; Yuejian Fang; Ming Gong; Daxin Jiang; Ming Zhou"}, {"ref_id": "b36", "title": "Hero: Hierarchical encoder for video+ language omni-representation pre-training", "journal": "", "year": "2008", "authors": "Linjie Li; Yen-Chun Chen; Yu Cheng; Zhe Gan; Licheng Yu; Jingjing Liu"}, {"ref_id": "b37", "title": "Beyond rnns: Positional self-attention with co-attention for video question answering", "journal": "", "year": "2019", "authors": "Xiangpeng Li; Jingkuan Song; Lianli Gao; Xianglong Liu; Wenbing Huang; Xiangnan He; Chuang Gan"}, {"ref_id": "b38", "title": "Object-semantics aligned pre-training for vision-language tasks", "journal": "", "year": "", "authors": "Xiujun Li; Xi Yin; Chunyuan Li; Pengchuan Zhang; Xiaowei Hu; Lei Zhang; Lijuan Wang; Houdong Hu; Li Dong; Furu Wei"}, {"ref_id": "b39", "title": "Tgif: A new dataset and benchmark on animated gif description", "journal": "", "year": "2016", "authors": "Yuncheng Li; Yale Song; Liangliang Cao; Joel Tetreault; Larry Goldberg; Alejandro Jaimes; Jiebo Luo"}, {"ref_id": "b40", "title": "Use what you have: Video retrieval using representations from collaborative experts", "journal": "", "year": "2020", "authors": "Yang Liu; Samuel Albanie"}, {"ref_id": "b41", "title": "A robustly optimized bert pretraining approach. arXiv", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov;  Roberta"}, {"ref_id": "b42", "title": "Decoupled weight decay regularization", "journal": "", "year": "2019", "authors": "Ilya Loshchilov; Frank Hutter"}, {"ref_id": "b43", "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks", "journal": "", "year": "2004", "authors": "Jiasen Lu; Dhruv Batra; Devi Parikh; Stefan Lee"}, {"ref_id": "b44", "title": "End-to-end learning of visual representations from uncurated instructional videos", "journal": "", "year": "2004", "authors": "Antoine Miech; Jean-Baptiste Alayrac; Lucas Smaira; Ivan Laptev; Josef Sivic; Andrew Zisserman"}, {"ref_id": "b45", "title": "Howto100m: Learning a text-video embedding by watching hundred million narrated video clips", "journal": "", "year": "2008", "authors": "Antoine Miech; Dimitri Zhukov; Jean-Baptiste Alayrac; Makarand Tapaswi; Ivan Laptev; Josef Sivic"}, {"ref_id": "b46", "title": "Distributed representations of words and phrases and their compositionality", "journal": "", "year": "2013", "authors": "Tomas Mikolov; Ilya Sutskever; Kai Chen; Greg S Corrado; Jeff Dean"}, {"ref_id": "b47", "title": "Revisiting modulated convolutions for visual counting and beyond. arXiv", "journal": "", "year": "2020", "authors": "Vedanuj Duy-Kien Nguyen; Xinlei Goswami;  Chen"}, {"ref_id": "b48", "title": "Pytorch: An imperative style, high-performance deep learning library", "journal": "", "year": "2019", "authors": "Adam Paszke; Sam Gross; Francisco Massa; Adam Lerer; James Bradbury; Gregory Chanan; Trevor Killeen; Zeming Lin; Natalia Gimelshein; Luca Antiga; Alban Desmaison; Andreas Kopf; Edward Yang; Zachary Devito"}, {"ref_id": "b49", "title": "Glove: Global vectors for word representation", "journal": "", "year": "2014", "authors": "Jeffrey Pennington; Richard Socher; Christopher D Manning"}, {"ref_id": "b50", "title": "Learning spatiotemporal representation with pseudo-3d residual networks", "journal": "", "year": "2017", "authors": "Zhaofan Qiu; Ting Yao; Tao Mei"}, {"ref_id": "b51", "title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "JMLR", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b52", "title": "Faster r-cnn: Towards real-time object detection with region proposal networks", "journal": "", "year": "2015", "authors": "Kaiming Shaoqing Ren; Ross He; Jian Girshick;  Sun"}, {"ref_id": "b53", "title": "Niket Tandon, and Bernt Schiele. A dataset for movie description", "journal": "", "year": "2015", "authors": "Anna Rohrbach; Marcus Rohrbach"}, {"ref_id": "b54", "title": "Two-stream convolutional networks for action recognition in videos", "journal": "", "year": "2014", "authors": "Karen Simonyan; Andrew Zisserman"}, {"ref_id": "b55", "title": "Very deep convolutional networks for large-scale image recognition", "journal": "", "year": "2015", "authors": "Karen Simonyan; Andrew Zisserman"}, {"ref_id": "b56", "title": "A short note on the kinetics-700-2020 human action dataset. arXiv", "journal": "", "year": "2020", "authors": "Lucas Smaira; Jo\u00e3o Carreira; Eric Noland; Ellen Clancy; Amy Wu; Andrew Zisserman"}, {"ref_id": "b57", "title": "Vl-bert: Pre-training of generic visuallinguistic representations", "journal": "", "year": "", "authors": "Weijie Su; Xizhou Zhu; Yue Cao; Bin Li; Lewei Lu; Furu Wei; Jifeng Dai"}, {"ref_id": "b58", "title": "Videobert: A joint model for video and language representation learning", "journal": "", "year": "2019", "authors": "Chen Sun; Austin Myers; Carl Vondrick; Kevin Murphy; Cordelia Schmid"}, {"ref_id": "b59", "title": "Going deeper with convolutions", "journal": "", "year": "2015", "authors": "Christian Szegedy; Wei Liu; Yangqing Jia; Pierre Sermanet; Scott Reed; Dragomir Anguelov; Dumitru Erhan; Vincent Vanhoucke; Andrew Rabinovich"}, {"ref_id": "b60", "title": "Lxmert: Learning crossmodality encoder representations from transformers", "journal": "", "year": "2009", "authors": "Hao Tan; Mohit Bansal"}, {"ref_id": "b61", "title": "Learning spatiotemporal features with 3d convolutional networks", "journal": "", "year": "2015", "authors": "Du Tran; Lubomir Bourdev; Rob Fergus; Lorenzo Torresani; Manohar Paluri"}, {"ref_id": "b62", "title": "A closer look at spatiotemporal convolutions for action recognition", "journal": "", "year": "2018", "authors": "Du Tran; Heng Wang; Lorenzo Torresani; Jamie Ray; Yann Lecun; Manohar Paluri"}, {"ref_id": "b63", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b64", "title": "Translating videos to natural language using deep recurrent neural networks", "journal": "", "year": "2015", "authors": "Subhashini Venugopalan; Huijuan Xu; Jeff Donahue; Marcus Rohrbach; Raymond Mooney; Kate Saenko"}, {"ref_id": "b65", "title": "Temporal segment networks: Towards good practices for deep action recognition", "journal": "", "year": "2007", "authors": "Limin Wang; Yuanjun Xiong; Zhe Wang; Yu Qiao; Dahua Lin; Xiaoou Tang; Luc Van Gool"}, {"ref_id": "b66", "title": "Abhinav Gupta, and Kaiming He. Non-local neural networks", "journal": "", "year": "2018", "authors": "Xiaolong Wang; Ross Girshick"}, {"ref_id": "b67", "title": "Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface's transformers: State-of-the-art natural language processing. arXiv", "journal": "", "year": "2019", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; R\u00e9mi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao;  Gugger"}, {"ref_id": "b68", "title": "Transformers: State-of-the-art natural language processing", "journal": "", "year": "2020", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; R\u00e9mi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger; Quentin Drame; Alexander M Lhoest;  Rush"}, {"ref_id": "b69", "title": "Deep multiple instance learning for image classification and autoannotation", "journal": "", "year": "2015", "authors": "Jiajun Wu; Yinan Yu; Chang Huang; Kai Yu"}, {"ref_id": "b70", "title": "Google's neural machine translation system: Bridging the gap between human and machine translation", "journal": "", "year": "2016", "authors": "Yonghui Wu; Mike Schuster; Zhifeng Chen; V Quoc; Mohammad Le; Wolfgang Norouzi; Maxim Macherey; Yuan Krikun; Qin Cao; Klaus Gao;  Macherey"}, {"ref_id": "b71", "title": "Adaframe: Adaptive frame selection for fast video recognition", "journal": "", "year": "2019", "authors": "Zuxuan Wu; Caiming Xiong; Chih-Yao Ma; Richard Socher; Larry S Davis"}, {"ref_id": "b72", "title": "Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification", "journal": "", "year": "2018", "authors": "Saining Xie; Chen Sun; Jonathan Huang; Zhuowen Tu; Kevin Murphy"}, {"ref_id": "b73", "title": "Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion", "journal": "", "year": "2005", "authors": "Dejing Xu; Zhou Zhao; Jun Xiao; Fei Wu; Hanwang Zhang"}, {"ref_id": "b74", "title": "Msr-vtt: A large video description dataset for bridging video and language", "journal": "", "year": "2009", "authors": "Jun Xu; Tao Mei; Ting Yao; Yong Rui"}, {"ref_id": "b75", "title": "Xlnet: Generalized autoregressive pretraining for language understanding", "journal": "", "year": "2019", "authors": "Zhilin Yang; Zihang Dai; Yiming Yang; Jaime Carbonell; R Russ; Quoc V Salakhutdinov;  Le"}, {"ref_id": "b76", "title": "A joint sequence fusion model for video question answering and retrieval", "journal": "", "year": "2005", "authors": "Youngjae Yu; Jongseok Kim; Gunhee Kim"}, {"ref_id": "b77", "title": "Video captioning and retrieval models with semantic attention", "journal": "", "year": "2016", "authors": "Youngjae Yu; Hyungjin Ko; Jongwook Choi; Gunhee Kim"}, {"ref_id": "b78", "title": "End-to-end concept word detection for video captioning, retrieval, and question answering", "journal": "", "year": "2017", "authors": "Youngjae Yu; Hyungjin Ko; Jongwook Choi; Gunhee Kim"}, {"ref_id": "b79", "title": "Cross-modal and hierarchical modeling of video and text", "journal": "", "year": "2008", "authors": "Bowen Zhang; Hexiang Hu; Fei Sha"}, {"ref_id": "b80", "title": "Unified vision-language pretraining for image captioning and vqa", "journal": "", "year": "", "authors": "Luowei Zhou; Hamid Palangi; Lei Zhang; Houdong Hu; Jason J Corso; Jianfeng Gao"}, {"ref_id": "b81", "title": "Towards automatic learning of procedures from web instructional videos", "journal": "", "year": "2018", "authors": "Luowei Zhou; Chenliang Xu; Jason J Corso"}, {"ref_id": "b82", "title": "Actbert: Learning global-local video-text representations", "journal": "", "year": "2009", "authors": "Linchao Zhu; Yi Yang"}, {"ref_id": "b83", "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "journal": "", "year": "2015", "authors": "Yukun Zhu; Ryan Kiros; Rich Zemel; Ruslan Salakhutdinov; Raquel Urtasun; Antonio Torralba; Sanja Fidler"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Comparison between popular video-and-language learning paradigm (top) and CLIPBERT (bottom). In contrast to most existing methods that utilize offline (stop gradient) extracted dense video features and text features, CLIPBERT uses sparsely sampled clips and raw text tokens for end-to-end modeling.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: Overview of CLIPBERT architecture. For simplicity, we only show an illustration of producing prediction for a single sampled clip. When multiple clips are used, their predictions are fused together as the final prediction.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: Average video length in different datasets.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure 5: Memory and computation cost comparison w.r.t. different numbers of clips (Ntrain) or frames (T ) at training. (a): Maximum allowed batch size with fixed T =1. (b): Time cost for a single forward and backward pass with fixed T =1, batch size 8. (c): Maximum allowed batch size with fixed Ntrain=1. (d): Time cost for a single forward and backward pass with fixed Ntrain=1, batch size 8. All experiments are conducted on a single NVIDIA V100 GPU with 32GB memory. MSRVTT retrieval performance is also added in (b, d) for reference. Best viewed in color.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "CLIPBERT 4\u00d72 * (Ntest=20) 21.3 49.0 63.5 6.0 (c) ActivityNet Captions val1 set.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Impact of input image size L.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": "Sampling Method N trainMSRVTT Retrieval R1 R5 R10 MdR QA Acc. MSRVTT-Dense Uniform1615.5 39.6 55.09.035.88112.7 34.5 48.8 11.036.24Sparse Random215.5 38.4 52.69.036.59415.7 41.9 55.38.036.67"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Sparse random sampling vs. dense uniform sampling. All models use Ntest=16 clips for inference.Do more clips in training help?We randomly sample N train clips and aggregate scores from the clips with aggregation function G as the final score to calculate the training loss. When multiple clips are used, information from these samples is aggregated through multiple instance learning to maximize the utility of these clips. To understand how this strategy affects model performance, we evaluate model variants that use N train \u2208 {1, 2, 4, 8, 16} at training. We also consider 3 different commonly used score aggregation", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "shows the comparison. Sparse random sampling with only 4 clips outperforms dense uniformly sampling with 16 clips across all", "figure_data": "Weight InitializationMSRVTT RetrievalMSRVTT-CNNtransformerR1 R5 R10 MdRQA Acc.randomrandom0.3 0.4 0.9 506.028.05randomBERTBASE0.0 0.2 0.7 505.031.72TSN, K700 BERTBASE5.7 22.1 33.123.035.40ImageNetBERTBASE7.2 23.3 35.621.035.01grid-featBERTBASE7.4 21.0 30.726.035.27image-text pre-training10.2 28.6 40.517.035.73"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Impact of weight initialization strategy.", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Impact of end-to-end training.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "For evaluation, we compare CLIPBERT with state-ofthe-art methods on text-to-video retrieval and video question answering tasks. We denote models using different sampling methods at training as CLIPBERT N train \u00d7T , (randomly sample N train 1-second clips for training, each contains T uniformly sampled frames of size L=448). We use LogSumExp to aggregate scores from multiple clips. At inference time, if not otherwise stated, we aggregate scores from N test =16 uniformly sampled clips.", "figure_data": "MethodR1 R5 R10 MdRMethodR1 R5 R10 MdRHERO [37] ASR, PT 20.5 47.6 60.9-CE [41]16.1 41.1-8.3JSFusion [77]10.2 31.2 43.2 13.0S2VT [65]HT [46] PT14.9 40.2 52.8 9.0ActBERT [83] PT16.3 42.8 56.9 10.0HERO [37] PT16.8 43.4 57.7-CLIPBERT 4\u00d7119.8 45.1 57.5 7.0CLIPBERT 8\u00d7222.0 46.8 59.9 6.0(a) MSRVTT 1K test set.Main Conclusions from the analyses in Section 4.2 andSection 4.3 are summarized as follows: (i) Larger input im-age size helps improve model performance, but the gain di-minishes when image size is larger than 448; (ii) Sparselysampling 2 frames from each clip performs on par withdense sampling 16 frames, showing that just one or twoframes are sufficient for effective video-and-language train-ing; mean-pooling is more effective than 3D Conv whenfusing information from different frames; (iii) More clipsat inference helps improve model performance; aggrega-tion strategy of predictions across clips affects final per-formance; (iv) Sparse (random) sampling is more effec-tive than dense uniform sampling while being more memoryand computation efficient; (v) Image-text pre-training bene-fits video-text tasks; and (vi) End-to-end training improvesmodel performance.4.4. Comparison to State-of-the-art"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Comparison with state-of-the-art methods on text-to-video retrieval. CLIPBERT models with different training input sampling methods are denoted by Ntrain\u00d7T . We use Ntest=16 if not otherwise stated. We gray out models that used features other than appearance and motion for a fair comparison, e.g., CE used appearance, scene, motion, face, audio, OCR, ASR features from 11 different models. PT indicates the model is pre-trained on HowTo100M. * denotes models use 2-second clips instead of the default 1-second clips.", "figure_data": "MethodAction Transition FrameQAMethodAccuracyMethodAccuracyST-VQA [23]60.867.149.3ST-VQA [23] (by [12])30.9SNUVL [78] (by [77])65.4Co-Memory [17]68.274.351.5Co-Memory [17] (by [12])32.0ST-VQA [23] (by [77])66.1PSAC [38]70.476.955.7AMU [74]32.5CT-SAN [79] (by [77])66.4Heterogeneous Memory [12] 73.977.853.8Heterogeneous Memory [12]33.0MLB [27] (by [77])76.1HCRN [31]75.081.455.9HCRN [31]35.6JSFusion [77]83.4QueST [25]75.981.059.7CLIPBERT 4\u00d7137.0ActBERT [83] PT85.7CLIPBERT 1\u00d71 (Ntest=1) 82.987.559.4CLIPBERT 8\u00d7237.4CLIPBERT 4\u00d7187.9CLIPBERT 1\u00d7182.887.860.3CLIPBERT 8\u00d7288.2(a) TGIF-QA test set.(b) MRSVTT-QA test set.(c) MRSVTT multiple-choice test."}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "Comparison with state-of-the-art methods on video question answering.", "figure_data": "Text-to-Video Retrieval. Table 7 summarizes results ontext-to-video retrieval. In Table 7a, CLIPBERT achievessignificant performance gain over existing methods onMSRVTT retrieval, including HT [46], ActBERT [83], andHERO [37], which are pre-trained on 136M clip-captionpairs from HowTo100M [46]. Under a fair compari-son, CLIPBERT 4\u00d71 outperforms HERO [37] by 3.0%on R@1. Note that HERO uses SlowFast [14] featuresextracted from full-length videos at a very dense framerate of 21 FPS (i.e., on average 310 frames per videofor MSRVTT), while CLIPBERT 4\u00d71 uses only 4 ran-domly sampled frames. When more frames are used, CLIP-BERT 8\u00d72 achieves even higher performance, surpass-ing HERO by 5.2%. Compared to the HERO ASR modelthat uses extra input from Automatic Speech Recognition(ASR), CLIPBERT still obtains 1.5% higher R1 score."}, {"figure_label": "8b", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "CLIPBERT 4\u00d71 outperforms HCRN[31] by 1.4% on MSRVTT-QA. Note that HCRN adopts a sophisticated hierarchical relation modeling network over the entire video sequence of 24 clips at training time, while we use only four randomly sampled frames. Using more frames further increases this performance gap to 1.8%. Table8cshows CLIPBERT 8\u00d72 improves ActBERT[83] model pre-trained on HowTo100M by 2.5%, on MSRVTT multiple choice test task.", "figure_data": ""}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "Comparison with state-of-the-art methods on VQA. G stands for grid features, R stands for region features.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_19", "figure_caption": "shows the training details for models on each of the datasets.", "figure_data": "Dataset#Epochs Bsz \u00d7Grad-Accu \u00d7#GPUs LRMSRVTT2016\u00d71\u00d785e-5DiDeMo208\u00d74\u00d785e-5ActivityNet Captions8016\u00d72\u00d785e-5"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_20", "figure_caption": "Training details for text-to-video retrieval tasks. Bsz is short for batch size. Grad-Accu stands for gradient accumulation steps. LR means initial learning rate.", "figure_data": ""}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_21", "figure_caption": "shows the training details for models on TGIF-QA tasks and MSRVTT-QA.", "figure_data": "Dataset#Epochs Bsz\u00d7Grad-Accu \u00d7#GPUs LRTGIF-QA Action5532\u00d71\u00d781e-4TGIF-QA Transition1532\u00d71\u00d781e-4TGIF-QA FrameQA1532\u00d71\u00d781e-4MSRVTT-QA1016\u00d71\u00d745e-5"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_22", "figure_caption": "Training details for video question answering tasks. Bsz is short for batch size. Grad-Accu stands for gradient accumulation steps. LR means initial learning rate.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "p = H([F SG v (c 1 ), F SG v (c 2 ), ..., F SG v (c N )], F SG l (S)),(1)", "formula_coordinates": [3.0, 55.3, 344.96, 231.06, 12.69]}, {"formula_id": "formula_1", "formula_text": "l task = L task (p, q).(2)", "formula_coordinates": [3.0, 128.2, 483.25, 158.16, 11.03]}, {"formula_id": "formula_2", "formula_text": "p \u03c4i = H(F v (c \u03c4i ), F l (S)),(3)", "formula_coordinates": [3.0, 115.73, 586.73, 170.63, 9.65]}, {"formula_id": "formula_3", "formula_text": "l task = L task (G(p \u03c41 , p \u03c42 , ..., p \u03c4 N train ), q),(4)", "formula_coordinates": [3.0, 83.33, 698.02, 203.04, 13.94]}, {"formula_id": "formula_4", "formula_text": "Video", "formula_coordinates": [5.0, 50.11, 254.51, 24.54, 8.96]}, {"formula_id": "formula_5", "formula_text": "p (j) = Ntrain i=1 e g (j) \u03c4 i sum( Ntrain i=1 e g (j) \u03c4 i ) .(5)", "formula_coordinates": [9.0, 369.61, 339.6, 175.5, 32.34]}, {"formula_id": "formula_6", "formula_text": "L = \u2212 1 |D| |D| j=1 logp (j) [y j ],(6)", "formula_coordinates": [9.0, 372.95, 400.4, 172.16, 31.18]}], "doi": ""}