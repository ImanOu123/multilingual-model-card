{"title": "The Translation-invariant Wishart-Dirichlet Process for Clustering Distance Data", "authors": "Julia E Vogt; Volker Roth", "pub_date": "", "abstract": "We present a probabilistic model for clustering of objects represented via pairwise dissimilarities. We propose that even if an underlying vectorial representation exists, it is better to work directly with the dissimilarity matrix hence avoiding unnecessary bias and variance caused by embeddings. By using a Dirichlet process prior we are not obliged to fix the number of clusters in advance. Furthermore, our clustering model is permutation-, scale-and translation-invariant, and it is called the Translation-invariant Wishart Dirichlet (TIWD) process. A highly efficient MCMC sampling algorithm is presented. Experiments show that the TIWD process exhibits several advantages over competing approaches.", "sections": [{"heading": "Introduction", "text": "The Bayesian clustering approach presented in this work aims at identifying subsets (or \"clusters\") of objects represented as columns/rows in a dissimilarity matrix. The underlying idea is that objects grouped together in such a cluster can be reasonably well described as a homogeneous sub-population. Our focus on dissimilarity matrices implies that we do not have access to a vectorial representation of the objects. Such underlying vectorial representa-Appearing in Proceedings of the 27 th International Conference on Machine Learning, Haifa, Israel, 2010. Copyright 2010 by the author(s)/owner(s). tion may or may not exist, depending on whether the dissimilarity matrix can be embedded (without distortion) in a vector space. One way of dealing with such problems would be to explicitly construct an Euclidean embedding (or possibly a distorted embedding), and to apply a traditional clustering method in the Euclidean space. We argue, however, that even under the assumption that there exists an Euclidean embedding it is better not to embed the data, since any such choice might induce an unnecessary bias and variance in the clustering process. Technically speaking, such embeddings break the symmetry induced by the translation-and rotation-invariance which reflects the information loss incurred when moving from vectors to pairwise dissimilarities. We propose a clustering model which works directly on dissimilarity matrices. It is invariant against label-and object permutations and against scale transformations. The model is fully probabilistic in nature, which means that on output we are given samples from a distribution over partitions. Further, the use of a Dirichlet process prior unburdens the user from explicitly fixing the number of clusters. We present a highly efficient sampling algorithm which avoids costly matrix operations by carefully exploiting the structure of the clustering problem. Invariance against label permutations is a common cause of the so-called \"label switching\" problem in mixture models, (Jasr et al., 2005). By formulating the model as a partition process this switching problem is circumvented. This paper is structured as follows: we start with a review of the Dirichlet cluster process for Gaussian mixtures in (McCullagh & Yang, 2008). This model is generalized to relational data by enforcing translation invariance. We call this new model the Translation-invariant Wishart-Dirichlet (WD) cluster process. We then develop an efficient sampling algorithm which makes it possible to apply the method to large-scale datasets.", "publication_ref": ["b7", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Gauss-Dirichlet Cluster Process", "text": "Let [n] := {1, . . . , n} denote an index set, and B n the set of partitions of [n]. A partition B \u2208 B n is an equivalence relation B : [n] \u00d7 [n] \u2192 {0, 1} that may be represented in matrix form as B(i, j) = 1 if y(i) = y(j) and B(i, j) = 0 otherwise, with y being a function that maps [n] to some label set L. Alternatively, B may be represented as a set of disjoint non-empty subsets called \"blocks\" b. A partition process is a series of distributions P n on the set B n in which P n is the marginal distribution of P n+1 . Such a process is called exchangeable if each P n is invariant under permutations of object indices, see (Pitman, 2006).\nA Gauss-Dirichlet cluster process consists of an infinite sequence of points in R d , together with a random partition of integers into k blocks. A sequence of length n can be sampled as follows (MacEachern, 1994;Dahl, 2005;McCullagh & Yang, 2008): fix the number of mixture modes k, generate mixing proportions \u03c0 = (\u03c0 1 , . . . , \u03c0 k ) from an exchangeable Dirichlet distribution Dir(\u03be/k, . . . , \u03be/k), generate a label sequence {y(1), . . . , y(n)} from a multinomial distribution and forget the labels introducing the random partition B of [n] induced by y. Integrating out \u03c0, one arrives at a Dirichlet-Multinomial prior over partitions\nP n (B|\u03be, k) = k! (k \u2212 k B )! \u0393(\u03be) b\u2208B \u0393(n b + \u03be/k) \u0393(n + \u03be)[\u0393(\u03be/k)] k B , (1)\nwhere k B \u2264 k denotes the number of blocks present in the partition B and n b is the size of block b. The limit as k \u2192 \u221e is well defined and known as the Ewens process (a.k.a. Chinese Restaurant process), see for instance (Ewens, 1972;Neal, 2000;Blei & Jordan, 2006). Given such a partition B, a sequence of n-dimensional observations x i \u2208 R n , i = 1, . . . , d is arranged as columns of the (n\u00d7d) matrix X, and this X is generated from a zero-mean Gaussian distribution with covariance matrix\n\u03a3 B = I n \u2297 \u03a3 0 + B \u2297 \u03a3 1 , with cov(X ir , X js |B) = \u03b4 ij \u03a3 0rs + B ij \u03a3 1rs ,(2)\nwhere \u03a3 0 is the usual (d \u00d7 d) \"pooled\" within-class covariance matrix and \u03a3 1 the (d \u00d7 d) between-class matrix, respectively, and \u03b4 ij denotes the Kronecker symbol. Since the partition process is invariant under permutations, we can always think of B being block-diagonal. For spherical covariance matrices (i.e. scaled identity matrices), \u03a3 0 = \u03b1I d , \u03a3 1 = \u03b2I d , the covariance structure reduces to\n\u03a3 B = I n \u2297 \u03b1I d + B \u2297 \u03b2I d = (\u03b1I n + \u03b2B) \u2297 I d =: \u03a3 B \u2297 I d ,\nwith cov(X ir , X js |B) = (\u03b1\u03b4 ij + \u03b2B ij )\u03b4 rs .\n(3) Thus, the columns of X are independent n-dimensional vectors x i \u2208 R n distributed according to a normal distribution with covariance matrix \u03a3 B = \u03b1I n + \u03b2B. Further, the distribution factorizes over the blocks b \u2208 B. Introducing the symbol i b := {i : i \u2208 b} defining an index-vector of all objects assigned to block b, the joint distribution reads\np(X, B|\u03b1, \u03b2, \u03be, k) = P n (B|\u03be, k) \u2022 b\u2208B d j=1 N (X i b j |\u03b1I n b + \u03b21 n b 1 t n b ) ,(4)\nwhere n b is the size of block b and 1 n b a n b -vector of ones.\nIn the following we will use the abbreviations 1 b := 1 n b and I b := I n b to avoid double subscripts. Note that this distribution is expressed in terms of the partition without resorting to labels, so label switching cannot occur.", "publication_ref": ["b12", "b8", "b3", "b10", "b4", "b11", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Wishart-Dirichlet Cluster Process", "text": "We now extend the Gauss-Dirichlet cluster process to a sequence of inner-product and distance matrices. Assume that the random matrix X n\u00d7d follows the zero-mean Gaussian distribution specified in (2), with\n\u03a3 0 = \u03b1I d , \u03a3 1 = \u03b2I d .\nThen, conditioned on the partition B, the inner product matrix S = XX t /d follows a (possibly singular) Wishart distribution in d degrees of freedom, S \u223c W d (\u03a3 B ), (Srivastava, 2003). If we directly observe the dot products S, it suffices to consider the conditional probability of partitions, P n (B|S), which has the same functional form for ordinary and singular Wishart distributions:\nP n (B|S, \u03b1, \u03b2, \u03be, k) \u221d W d (S|\u03a3 B ) \u2022 P n (B|\u03be, k) \u221d |\u03a3 B | \u2212 d 2 exp \u2212 d 2 tr(\u03a3 \u22121 B S) \u2022 P n (B|\u03be, k),(5)\nFor the following derivation it is suitable to re-parametrize the model in terms of (\u03b1, \u03b8) instead of (\u03b1, \u03b2), where \u03b8 := \u03b2/\u03b1, and in terms of W := \u03a3 \u22121 B . Due to the block structure in B, P n (B|S, \u2022) factorizes over the blocks b \u2208 B:\nP n (B|S, \u03b1, \u03b8, \u03be, k) \u221d P n (B|\u03be, k) \u2022 b\u2208B |W b | d 2 exp \u2212 b\u2208B d 2 tr(W b S bb ) ,(6)\nwhere W b , S bb denote the submatrices corresponding to the b-th diagonal block in B or W , see Figure 1. The above factorization property can be exploited to derive an efficient inference algorithm for this model. The key observation is that the inverse matrix W b = \u03a3 \u22121 b can be analytically computed as\nW b = (\u03b1I b + \u03b21 b 1 t b ) \u22121 = 1 \u03b1 I b \u2212 \u03b8 1+n b \u03b8 1 b 1 t b . (7)\nThus, the contribution of block b to the trace is\ntr(W b S bb ) = 1 \u03b1 tr(S bb ) \u2212 \u03b8 1+n b \u03b8S bb ,(8)\nwhereS bb = 1 t b S bb 1 b denotes the sum of the b-th diagonal block of S. A similar trick can be used for the determinant which is the product of the eigenvalues: the k B smallest eigenvalues of W are given by \u03bb b = \u03b1 \u22121 (1 + \u03b8n b ) \u22121 . The remaining n \u2212 k B eigenvalues are equal to \u03b1 \u22121 . Thus, the determinant reads\n|W | = b\u2208B \u03bb b = \u03b1 \u2212n b\u2208B (1 + \u03b8n b ) \u22121 .(9)", "publication_ref": ["b13"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Scale Invariance", "text": "Note that the re-parametrization using (\u03b1, \u03b8) leads to a new semantics of (1/\u03b1) as a scale parameter: we excluded \u03b1 from the partition-dependent terms in the product over the blocks in ( 9), which implies that the conditional for the partition becomes\nP n (B|\u2022) \u221d P n (B|\u03be, k) \u2022 b\u2208B (1 + \u03b8n b ) \u22121 \u2212d/2 \u2022 exp \u2212 1 \u03b1 d 2 b\u2208B tr(W b S bb ) . (10\n)\nNote that (1/\u03b1) simply rescales the observed matrix S, and we can make the model scale invariant by introducing a prior distribution and integrating out \u03b1. The conditional posterior for \u03b1 follows an inverse Gamma distribution\np(\u03b1|r, s) = s r \u0393(r) 1 \u03b1 r+1 exp \u2212 s \u03b1 ,(11)\nwith shape parameter r = n \u2022 d/2 \u2212 1 and scale s =  8) and (10). Using an inverse Gamma prior with parameters r 0 , s 0 , the posterior is of the same functional form with r p = r + r 0 + 1 and s p = s + s 0 , and we can integrate out \u03b1 analytically. Dropping all terms independent of the partition structure we arrive at\nP n (B|\u2022) \u221d P n (B|\u03be, k)|W | d/2 (\u03b1=1) (s + s 0 ) r+r0+1 , (12)\nwhere |W | (\u03b1=1) = b\u2208B (1 + \u03b8n b ) \u22121 follows from (9).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Centering Problem", "text": "In practice, however, there are two problems with the model described above: (i) we often do not directly observe S, but only a matrix of distances D. In the following we will assume that the (suitably pre-processed) matrix D contains squared Euclidean distances with components D ij = S ii + S jj \u2212 2S ij ; (ii) even if we observe a dotproduct matrix, we usually have no information about the mean vector \u00b5. Note that we assumed that there exists a matrix X with XX t = S such that the columns of X are independent copies drawn from a zero-mean Gaussian in\nR n : x \u223c N (\u00b5 = 0 n , \u03a3 = \u03a3 B ).\nThis assumption is crucial, since general mean vectors correspond to a noncentral Wishart model (Anderson, 1946), which imposes severe computational problems due to the appearance of the hypergeometric function. Both of the above problems are related in that they have to do with the lack of information about geometric transformations: assume we only observe S without access to the vectorial representations X n\u00d7d . Then we have lost the information about orthogonal transformations X \u2190 XO with OO t = I d , i.e. about rotations and reflections of the rows in X. If we only observe D, we have additionally lost the information about translations of the rows. Our sampling model implies that the means in each row are expected to converge to zero as the number of replications d goes to infinity. Thus, if we had access to X and if we are not sure that the above zero-mean assumption holds, it might be a plausible strategy to subtract the empirical row means, X n\u00d7d \u2190 X n\u00d7d \u2212 (1/d)X n\u00d7d 1 d 1 t d , and then to construct a candidate matrix S by computing the pairwise dot products. This procedure should be statistically robust if d n, since then the empirical means are probably close to their expected values. Such a matrix S fulfills two requirements for selecting candidate dot product matrices: first, S should be \"typical\" with respect to the assumed Wishart model with \u00b5 = 0, thereby avoiding any bias introduced by a particular choice. Second, the choice should be robust in a statistical sense: if we are given a second observation from the same data source, the two selected prototypical matrices S 1 and S 2 should be similar. For small d, this procedure is dangerous since it can introduce a strong bias even if the model is correct.\nConsider now case (ii) where we observe S without access to X. Case (i) needs no special treatment, since it can be reduced to case (ii) by first constructing a positive semidefinite matrix S which fulfills D ij = S ii + S jj \u2212 2S ij . For \"correcting\" the matrix S just as described above we would need a procedure which effectively subtracts the empirical row means from the rows of X. Unfortunately, there exists no such matrix transformation that operates directly on S without explicit construction of X. It is important to note that the \"usual\" centering transformation S \u2190 QSQ with Q ij = \u03b4 ij \u2212 1 n as used in kernel PCA and related algorithms does not work here: in kernel PCA the rows of X are assumed to be i.i.d. replications in R d . Consequently, the centered matrix S c is built by subtracting the column means: X n\u00d7d \u2190 X n\u00d7d \u2212 (1/n)1 n 1 t n X n\u00d7d and S c = XX t = QSQ. Here, we need to subtract the row means, and therefore it is inevitable to explicitly construct X, which implies that we have to choose a certain orthogonal transformation O. It might be reasonable to consider only rotations and to use the principle components as coordinate axes. This is essentially the kernel PCA embedding procedure: compute S c = QSQ and its eigenvalue decom-position S c = V \u039bV t , and then project on the principle axes: X = V \u039b 1/2 . The problem with this vector-space embedding is that it is statistically robust in the above sense only if d is small, because otherwise the directions of the principle axes might be difficult to estimate, and the estimates for two replicated observations might highly fluctuate, leading to different row-mean normalizations. Note that this condition for fixing the rotation contradicts the above condition d n that justifies the subtraction of the means. Further, row-mean normalization will change the pairwise dissimilarities D ij (even if the model is correct!), and this change can be drastic if d is small.\nThe cleanest solution might be to consider the dissimilarities D (which are observed in case (i) and computed as D ij = S ii + S jj \u2212 2S ij in case (ii)) as the \"reference\" quantity, and to avoid an explicit choice of S and X altogether. Therefore, we propose to encode the translation invariance directly into the likelihood, which means that the latter becomes constant on all matrices S that fulfill\nD ij = S ii + S jj \u2212 2S ij .", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "The Translation-invariant WD-Process", "text": "A squared Euclidean distance matrix D is characterized by the property of being of negative type, which means that x t Dx = \u22122x t Sx \u2264 0 for any x : x t 1 = 0. This condition is equivalent to the absence of negative eigenvalues in S c = QSQ = \u2212(1/2)QDQ. The distribution of D has been formally studied in (McCullagh, 2009), where it was shown that if S follows a standard Wishart generated from an underlying zero-mean Gaussian process, S \u223c W d (\u03a3 B ), \u2212D follows a generalized Wishart distribution, \u2212D \u223c W(1, 2\u03a3 B ) = W(1, \u2212\u2206) defined with respect to the transformation kernel K = 1, where\n\u2206 ij = \u03a3 B ii + \u03a3 B jj \u2212 2\u03a3 B ij .\nTo understand the role of the transformation kernel it is useful to introduce the notion of a generalized Gaussian distribution with kernel K = 1: X \u223c N (1, \u00b5, \u03a3). For any transformation L with L1 = 0, the meaning of the general Gaussian notation is:\nLX \u223c N (L\u00b5, L\u03a3L t ). (13\n)\nIt follows that under the kernel\nK = 1, two parameter set- tings (\u00b5 1 , \u03a3 1 ) and (\u00b5 2 , \u03a3 2 ) are equivalent if L(\u00b5 1 \u2212\u00b5 2 ) = 0 and L(\u03a3 1 \u2212 \u03a3 2 )L t = 0, i.e. if \u00b5 1 \u2212 \u00b5 2 \u2208 1, and (\u03a3 1 \u2212 \u03a3 2 ) \u2208 {1 n v t + v1 t n : v \u2208 R n }, a space which is usually denoted by sym 2 (1 \u2297 R n ).\nIt is also useful to introduce the distributional symbol S \u223c W(K, \u03a3) for the generalized Wishart distribution of the random matrix S = XX t when X \u223c N (K, 0, \u03a3). The key observation in (McCullagh, 2009) is that D ij = S ii + S jj \u2212 2S ij defines a linear transformation on symmetric matrices with kernel sym 2 (1 \u2297 R n ) which implies that the distances follow a generalized Wishart distribution with kernel 1: \u2212D \u223c W(1, 2\u03a3 B ) = W(1, \u2212\u2206). In the multi-dimensional case with spherical within-and between covariances we generalize the above model to Gaussian random matrices X \u223c N (\u00b5, \u03a3 B \u2297 I d ). Note that the d columns of this matrix are i.i.d. copies. The distribution of the matrix of squared Euclidean distances D then follows a generalized Wishart with d degrees of freedom \u2212D \u223c W d (1, \u2212\u2206). This distribution differs from a standard Wishart in that the inverse matrix W = \u03a3 \u22121 B is substituted by the matrix W = W \u2212 (1 t W 1) \u22121 W 11 t W and the determinant | \u2022 | is substituted by a generalized det(\u2022)-symbol which denotes the product of the nonzero eigenvalues of its matrix-valued argument (note that W is rank-deficient). The conditional probability of a partition then reads\nP (B|D, \u2022) \u221d W d (\u2212D|1, \u2212\u2206) \u2022 P n (B|\u03be, k) \u221d det( W ) d 2 exp d 4 tr( W D) \u2022 P n (B|\u03be, k). (14\n)\nNote that in spite of the fact that this probability is written as a function of W = \u03a3 \u22121 B , it is constant over all choices of \u03a3 B which lead to the same \u2206, i.e. independent under translations of the row vectors in X. For the purpose of inferring the partition B, this invariance property means that we can simply use our block-partition covariance model \u03a3 B and assume that the (unobserved) matrix S follows a standard Wishart distribution parametrized by \u03a3 B . We do not need to care about the exact form of S, since the conditional posterior for B depends only on D.\nScale invariance can be built into the model with the same procedure as described above for the simple (i.e. not translation invariant) WD-process. The posterior of \u03b1 again follows an inverse Gamma distribution, and after introducing a prior with parameters (s 0 , r 0 ) and integrating out \u03b1 we arrive at an expression analogous to (12) with s = d 4 tr( W D):\nP (B|\u2022) \u221d P n (B|\u03be, k) det( W (\u03b1=1) ) d 2 (s+s 0 ) n d 2 +r0 . (15)", "publication_ref": ["b9", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Efficient Inference via Gibbs Sampling", "text": "In Gibbs sampling one iteratively samples parameter values from the full conditionals. Our model includes the following parameters: the partition B, the scale \u03b1, the covariance parameter \u03b8, the number k of clusters in the population, the Dirichlet rate \u03be and the degrees of freedom d. We propose to fix d, \u03be and k: the degrees of freedom d might be estimated by the rank of S, which is often known from a pre-processing procedure. Note that d is not a very critical parameter, since all likelihood contributions are basically raised to the power of d. Thus, d might be used as an annealing-type parameter for \"freezing\" a representative partition in the limit d \u2192 \u221e. Concerning the number k of clusters in the population, there are two possibilities.\nEither one assumes k = \u221e, which results in the Ewensprocess model, or one expects a finite k. Our framework is applicable to both scenarios. Estimation of k, however is nontrivial if no precise knowledge about \u03be is available. Unfortunately, this is usually the case, and k = \u221e might be a plausible assumption in many applications. Alternatively, one might fix k to a large constant which serves as an upper bound of the expected number, which can be viewed as truncating the Ewens process. The Dirichlet rate \u03be is difficult to estimate, since it only weakly influences the likelihood. Consistent ML-estimators only exist for k = \u221e: \u03be = k B / log n, and even in this case the variance only decays like 1/ log(n), cf. (Ewens, 1972). In practice, we should not expect to be able to reliably estimate \u03be. Rather, we should have some intuition about \u03be, maybe guided by the observation that under the Ewens process model the probability of two objects belonging to the same cluster is 1/(1 + \u03be). We can then either define an appropriate prior distribution, or we can fix \u03be. Due to the weak effect of \u03be on conditionals, these approaches are usually very similar.\nThe scale \u03b1 can be integrated out analytically. The likelihood in \u03b8 is not of recognized form, and we propose to use a discretized prior set {p(\u03b8 j )} J j=1 for which we compute the posteriors {p(\u03b8 j |\u2022)} J j=1 . A new value of \u03b8 is then sampled from the categorical distribution defined by {p(\u03b8 j |\u2022)} J j=1 . We define a sweep of the Gibbs sampler as one complete update of (B, \u03b8). The most time consuming part in a sweep is the update of B by re-estimating the assignments to blocks for a single object (characterized by a row/column in D), given the partition of the remaining objects. Therefore we have to compute the membership probabilities in all existing blocks (and in a new block) by evaluating equation ( 15), which looks formally similar to (12), but a factorization over blocks is no longer obvious. Every time a new partition is analyzed, a naive implementation requires O(n 3 ) costs for computing the determinant of W and the product W D. In one sweep we need to compute k B such probabilities for n objects, summing up to costs of O(n 4 k B ). However, a more efficient algorithm exists: Theorem 1. Assuming k B blocks in the actual partition and a fixed maximum iteration number in numerical rootfinding, a sweep of the Gibbs sampler for the translationinvariant WD model can be computed in O(n 2 +nk 2 B ) time.\nProof. Assume we want to compute the membership probabilities of the l-th object, given the partition of the remaining objects and all other parameter values. We first have to downdate all quantities which depend on object l and its current block and compute the assignment probabilities for all blocks (and a new one). From the resulting categorical distribution we sample a new assignment (say block c) and update all quantities depending on object l and block c. We repeat this procedure for all objects l = 1, . . . , n.\nSince up-and down-dating are reverse to each other but otherwise identical operations, it suffices to consider the updating situation. To compute the membership probabilities we have to assign the new object to a block and evaluate (15) for the augmented matrix D * , which has one additional column and row. For notational simplicity we will drop the subscript * . Eq. (15) has two components: the prior P (B|\u03be, k) and the likelihood term which requires us to compute det( W (\u03b1=1) ) and tr( W D). With identity \u0393(x + 1) = x\u0393(x) in ( 1), the contribution of the prior is n c + \u03be/k for existing clusters and \u03be(1 \u2212 k B /k) for a new cluster (one simply sets k = \u221e for the Ewens-process).\nFor the likelihood term, consider first the generalized determinant det( W ) in ( 15). Since W = W \u2212 (1 t W 1) \u22121 W 11 t W , we have to compute \u03c1 := (1 t W 1) \u22121 for the augmented matrix W after assigning the new object l to block c. Analyzing ( 7) one derives \u03c1 \u22121 = b\u2208B n b \u03bb b , where \u03bb b = (1 + \u03b8n b ) \u22121 are the k B smallest eigenvalues of W (\u03b1=1) , see eq. ( 9). Thus, we increase n c , recompute \u03bb c and update \u03c1. Given \u03c1, we need to compute the eigenvalues of W \u2212 \u03c1W 11 t W =: W \u2212 \u03c1vv t , where the latter term defines a rank-one update of W . Analyzing the characteristic polynomial, it is easily seen that the (sizeordered) eigenvalues\u03bb i of W fulfill three conditions, see (Golub & Van Loan, 1989): (i) the smallest eigenvalue is zero:\u03bb 1 = 0; (ii) the largest n \u2212 k B eigenvalues are identical to their counterparts in W :\u03bb i = \u03bb i , i = k B +1, . . . , n; (iii) for the remaining eigenvalues with indices i = 2, . . . , k B it holds that if \u03bb i is a repeated eigenvalue of W ,\u03bb i = \u03bb i . Otherwise, they are the simple roots of the secular equation f (y) = \u03c1 + \nW D) = tr(W D) \u2212 \u03c1 \u2022 tr(W 11 t W D) = tr(W D) \u2212 \u03c1 \u2022 1 t W DW 1. (16\n)\nWe first precompute \u2200a \u2208 B:D ia = j\u2208a D ij , which induces O(n) costs since there are n summations in total. The first term in ( 16) is tr(W D) = b\u2208B tr(D bb ) \u2212 \n\u03c1 ab\u2208B 1 t a W a D ab W b 1 b =: \u03c1 ab\u2208B \u03a6 ab , \u03a6 ab =D ab \u2212 \u03b3 aDab \u2212 \u03b3 bDab + \u03b3 a \u03b3 bDab . (17\n)\nSince we have already updated \u03b3 andD, it requires O(k B ) time to update the c-th row. In a sweep, the costs for the trace sum up to O(n 2 +nk 2 B ):\nfor i = 1 to n do \u2200a \u2208 B:D ia = j\u2208a D ij O(n) for c = 1 to k B do UpdateD O(k B ) Recompute c-th term in tr(W D) O(1) Compute \u2200a \u2208 B : \u03a6 ac = \u03a6 ca O(k B ) end for end for\nThe sweep is completed by resampling \u03b8 from a discrete set with J levels which induces costs of O(k 2 B ).\nFrom the above theorem it follows that the worst case complexity in one sweep is O(n 3 ) in the infinite mixture (i.e. Ewens process-) model, since k B \u2264 n, and O(n 2 ) for the truncated Dirichlet process with k B \u2264 k < \u221e. If the \"true\" k is finite, but one still uses the infinite model, it is very unlikely to observe the worst-case O(n 3 ) behaviour in practice: if the sampler is initialized with a one-block partition (i.e. k B = 1), the trace of k B typically shows an \"almost monotone\" increase during burn-in, see Figure 3.", "publication_ref": ["b4", "b5"], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "Model Extensions", "text": "One possible extension of the TIWD cluster process is to include some preprocessing step. From the model assumptions S \u223c W(\u03a3 B ) it follows that if \u03a3 B contains k B blocks and if the separation between the clusters (i.e. \u03b8) is not too small, there will be only k B dominating eigenvalues in S. Thus, one might safely apply kernel PCA to the centered matrix S c = \u2212(1/2)QDQ, i.e. compute S c = V \u039bV t , consider only the firstk \"large\" eigenvalues in \u039b for computing a low-rank approximationS c = V\u039bV t , and switch back to dissimilarities viaD ij = (S c ) ii + (S c ) jj \u2212 2(S c ) ij . Such preprocessing might be particularly helpful in cases where S c = \u2212(1/2)QDQ contains some negative eigenvalues which are of relatively small magnitude. Then, the low-rank approximation might be positive semi-definite so thatD contains squared Euclidean distances. Such situations occur frequently if the dissimilarities stem from pairwise comparison procedures which can be interpreted as approximations to models which are guaranteed to produce Mercer kernels. A popular example are classical string alignments which might be viewed as approximations of probabilistic alignments using pairwise hidden Markov models. We present such an example in section 4. The downside of kernel PCA are the added costs of O(n 3 ), but randomized approximation methods have been introduced which significantly reduce these costs. In our TIWD software we have implemented a \"symmetrized\" version of the random projection algorithm for low-rank matrix approximation proposed in (Vempala, 2004) which uses the idea proposed in (Belabbas & Wolfe, 2007).\nAnother extension of the model concerns semi-supervised situations where for a subset of n m observations class labels, i.e. assignments to k m groups, are known. We denote this subset by the set of row indices A = {1, . . . , n m }.\nTraditional semi-supervised learning methods assume that at least one labeled object per class is observed, i.e. that the number of classes is known. This assumption, however, is questionable in many real world examples. We overcome this limitation by simply fixing the assignment to blocks for objects in A during the sampling procedure, and re-estimating only the assignments for the unlabeled objects in B = {n m + 1, . . . , n}. Using an Ewens process model with k = \u221e (or a truncated version thereof with k > k m ), the model has the freedom to introduce new classes if some objects do not resemble any labeled observation. We present such an example below, where we consider protein sequences with experimentally confirmed labels (the \"true\" labels) and others with only machine predicted labels (which we treat as unlabeled objects).", "publication_ref": ["b14", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In a first experiment we compare the proposed TIWD cluster process with several hierarchical clustering methods on synthetic data, generated as follows: (i) a random blockpartition matrix B of size n = 500 is sampled with k B = 10; (ii) d = 100 samples from N (0 n , \u03a3) are drawn, with \u03a3 = \u03b1I n +\u03b1\u03b8B, \u03b1 = 2 and different \u03b8-values; (iii) squared Euclidean distances are stored in the matrix D (n\u00d7n) .\nA two-dimensional kernel PCA projection of an example distance matrix is shown in the left panels of Fig. 2 (large \u03b8 \u2194 clear cluster separation in the upper panel, and small \u03b8 \u2194 highly overlapping clusters in the lower panel). 5000 Gibbs sweeps are computed for the TIWD cluster process (after a burn-in phase of 2000 sweeps), followed by an annealing procedure to \"freeze\" a certain partition, cf. section 3.4. For comparing the performance, several hierarchical clustering methods are applied: \"Wards\", \"complete linkage\", \"single linkage\", \"average linkage\", see (Jain & Dubes, 1988), and the resulting trees are cut at the same number of clusters as found by TIWD. The right panels show the agreement of the inferred partitions with the true labels, measured in terms of the adjusted rand index. If the clusters are well-separated, all methods perform very well, but for highly overlapping clusters, TIWD shows signifi-cant advantages over the hierarchical methods. In a second experiment we investigate the scalability of the algorithm. The \"small \u03b8\"-experiment above (lower panels in Fig. 2) is repeated for n = 8000. Figure 3 depicts the trace of the number of blocks k B during sampling. The sampler stabilizes after roughly 500 sweeps. Note the remarkable stability of the sampler (compared to the usual situations in \"traditional\" mixture models), which follows from the fact that no label-switching can appear. On a standard computer, this experiment took roughly two hours, which leads us to the conclusion that the proposed sampling algorithm is so efficient (at least for moderate k) that memory constraints are probably more severe than time constraints on standard hardware. In a next experiment we analyze the influence of encoding the translation invariance into the likelihood (our TIWD model) versus the standard WD process and row-mean subtraction as described in section 3.2. A similar random procedure for generating distance matrices is used, but this time we vary the number of replications d and the mean vector \u00b5. If \u00b5 = 0 n , both the standard WD process and the TIWD process are expected to perform well, which is confirmed in the 1st and 3rd panel (left and right box-plots). Row-mean subtraction, however, introduces significant bias and variance. For nonzero mean vectors (2nd and 4th panel), standard WD completely fails to detect the cluster structure, and row-mean subtraction can only partially overcome this problem. The TIWD process clearly outperforms the other models for nonzero mean vectors.  In a last experiment we consider a semi-supervised application example in which we study all globin-like protein sequences from the UniProtKB database (with experimentally confirmed annotations) and the TrEMBL database (with unconfirmed annotations). The former set consists of 1168 sequences which fall into 114 classes. These sequences form the \"supervised\" subset, and their assignments to blocks in the partition matrix are \"clamped\" in the Gibbs sampler. The latter set contains 2603 sequences which are treated as the \"unlabeled\" observations. Pairwise local string alignment scores s ij are computed between all sequences and transformed into dissimilarities using an exponential transform. The resulting dissimilarity matrix D is not guaranteed to be of negative type (and indeed, \u2212QDQ has some small negative eigenvalues). We overcome this problem by using the randomized low-rank approximation technique according to (Vempala, 2004;Belabbas & Wolfe, 2007), cf. section 3.5, which effectively translates D into a matrixD which is of negative type. The Ewens process model makes it possible to assign the unlabeled objects to existing classes or to new ones. Finally, almost all unlabeled objects are assigned to existing classes, with the exception of three new classes which have an interesting biological interpretation. Two of these classes contain globinlike bacterial sequences from Actinomycetales, a very special group of obligate aerobic bacteria which have to cope with oxidative stress. The latter might explain the existence of redox domains in the globin sequences, like the Ferredoxin reductase-type (FAD)-binding domain observed in all sequences in one of the clusters and the additional Nicotinamide adenine dinucleotide (NAD)-binding domain present in all sequences in the second new cluster, see Figure 5. Some of the latter sequences appear to be similar to another class that also contains Actinomycetales (see the large \"off diagonal\" probabilities surrounded by the blue circle) which, however, share a different pattern around some heme binding sites. The third new class contains short sequence fragments which show a certain variant of the hemoglobin beta subunit. With the exception of the above mentioned similarity of one of the Actino-bacterial classes to another one, the three new classes show no similarity to any of the other classes, which nicely demonstrates the advantage of a semi-supervised learning model that is flexible enough to allow the creation of new groups. ", "publication_ref": ["b6", "b14", "b1"], "figure_ref": ["fig_4", "fig_4", "fig_5"], "table_ref": []}, {"heading": "Conclusion", "text": "We introduced a flexible probabilistic model for clustering dissimilarity data. It contains an exchangeable partition process prior which avoids label-switching problems. The likelihood component follows a generalized Wishart model for squared Euclidean distance matrices which is invariant under translations and rotations, under permutations, and under scaling transformations. We call this clustering model the Translation Invariant Wishart-Dirichlet (TIWD) cluster process. The main contributions of this work are threefold: (i) On the modelling side, we propose that it is better to work directly on the distances, without computing an explicit dot-product-or vector-space-representation, since such embeddings add unnecessary bias and variance to the inference process. Experiments on simulated data corroborate this proposition by showing that the TIWD model significantly outperforms alternative approaches. In particular if the clusters are only poorly separated, the full probabilistic nature of the TIWD model has clear advantages over hierarchical approaches. (ii) On the algorithmic side we show that costly matrix operations can be avoided by carefully exploiting the inner structure of the likelihood term. We prove that a sweep of a Gibbs sampler can be computed in O(n 2 +nk 2 B ) time, as opposed to O(n 4 k B ) for a naive implementation. Experiments show that these algorithmic improvements make it possible to apply the model to large-scale datasets. (iii) A semi-supervised experiment with globin proteins revealed the strength of our partition process model which is flexible enough to introduce new classes for objects which are dissimilar to any labeled observation. We could identify an interesting class of bacterial sequences, and a subsequent analysis of their domain structure showed that these sequences indeed share some unusual structural elements.\nWe have implemented a software package for the TIWD model which links efficient C++ MCMC code to a userfriendly R-interface. We will make this package (including the datasets used in this paper) available on mloss.org.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgement. This work has been partially supported by the FP7 EU project SIMBAD.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "The non-central wishart distribution and certain problems of multivariate statistics", "journal": "Ann. Math. Statist", "year": "1946", "authors": "T W Anderson"}, {"ref_id": "b1", "title": "Fast low-rank approximation for covariance matrices", "journal": "", "year": "2007", "authors": "M A Belabbas; P J Wolfe"}, {"ref_id": "b2", "title": "Variational inference for Dirichlet process mixtures", "journal": "Bayesian Analysis", "year": "2006", "authors": "D Blei; M Jordan"}, {"ref_id": "b3", "title": "Sequentially-allocated merge-split sampler for conjugate and non-conjugate Dirichlet process mixture models", "journal": "", "year": "2005", "authors": "D B Dahl"}, {"ref_id": "b4", "title": "The sampling theory of selectively neutral alleles", "journal": "Theoretical Population Biology", "year": "1972", "authors": "W J Ewens"}, {"ref_id": "b5", "title": "", "journal": "The Johns Hopkins University Press", "year": "1989", "authors": "G H Golub; C F Van Loan;  Computations"}, {"ref_id": "b6", "title": "Algorithms for Clustering Data", "journal": "Prentice Hall", "year": "1988", "authors": "A Jain; R Dubes"}, {"ref_id": "b7", "title": "Markov chain monte carlo methods and the label switching problem in Bayesian mixture modeling", "journal": "Statist. Sci", "year": "2005", "authors": "A Jasr; C C Holmes; D A Stephens"}, {"ref_id": "b8", "title": "Estimating normal means with a conjugatestyle Dirichlet process prior", "journal": "Communication in Statistics: Simulation and Computation", "year": "1994", "authors": "S N Maceachern"}, {"ref_id": "b9", "title": "Marginal likelihood for distance matrices", "journal": "Statistica Sinica", "year": "2009", "authors": "P Mccullagh"}, {"ref_id": "b10", "title": "How many clusters?", "journal": "Bayesian Analysis", "year": "2008", "authors": "P Mccullagh; J Yang"}, {"ref_id": "b11", "title": "Markov chain sampling methods for Dirichlet process mixture models", "journal": "Journal of Computational and Graphical Statistics", "year": "2000", "authors": "R M Neal"}, {"ref_id": "b12", "title": "Ecole d'Ete de Probabilites de Saint-Flour XXXII-2002", "journal": "Springer", "year": "2006", "authors": "J Pitman"}, {"ref_id": "b13", "title": "Singular Wishart and multivariate beta distributions", "journal": "Annals of Statistics", "year": "2003", "authors": "M S Srivastava"}, {"ref_id": "b14", "title": "The Random Projection Method", "journal": "AMS", "year": "2004", "authors": "S Vempala"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. Example of the block structure of B and W (left) and the definition of sub-matrices in S and D (right) for kB = 3.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "bb ), cf. eqs. (", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "fulfilling the relations \u03bb i <\u03bb i+1 < \u03bb i+1 . Note that f can be evaluated in O(k B ) time, and with a fixed maximum number of iterations in the root-finding procedure, det( W ) can be computed in O(k B ). A sweep involves n \"new\" objects and k B blocks. Thus, the costs sum up to O(nk 2 B ): for i = 1 to n do for c = 1 to k B do n c \u2190 n c + 1, recompute \u03bb c and update \u03c1 O(1) Find roots of secular equation O(k B ) end for end for For the trace tr( W D) we have to compute tr(", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "\u03b81+n b \u03b8D bb , so we first updateD by recomputing its c-th row/column: update \u03b3 c = n c \u03bb c and \u2200a \u2208 B :D ac \u2190 D ac +D ia + D ii \u03b4 a,c O(k B ) time, and update the c-th term in tr(W D) in constant time. DefiningD ab := 1 t a D ab 1 b and \u03b3 a := na\u03b8 1+na\u03b8 , the second term in (16) reads", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 2 .2Figure2. TIWD vs. hierarchical clustering (\"Wards\", \"complete linkage\", \"single linkage\", \"average linkage\") on synthetic data (k = 10, n = 500, d = 100, repeated 20 times).", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 3 .3Figure 3. Traceplot of the number of blocks kB during the Gibbs sweeps for a large synthetic dataset. (10 clusters, n = 8000).", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 4 .4Figure 4. Comparison of WD and TIWD cluster process on synthetic data. \"WD\": standard WD, \"WD R\": WD with row-mean subtraction. Left to right: (i) d = 3, \u00b5 = 0; (ii) d = 3, \u00b5i \u223c N (40, 0.1); (iii) and (iv): same for d = 100.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 5 .5Figure 5. Co-membership probabilities of globin proteins.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "P n (B|\u03be, k) = k! (k \u2212 k B )! \u0393(\u03be) b\u2208B \u0393(n b + \u03be/k) \u0393(n + \u03be)[\u0393(\u03be/k)] k B , (1)", "formula_coordinates": [2.0, 64.19, 412.97, 225.25, 23.89]}, {"formula_id": "formula_1", "formula_text": "\u03a3 B = I n \u2297 \u03a3 0 + B \u2297 \u03a3 1 , with cov(X ir , X js |B) = \u03b4 ij \u03a3 0rs + B ij \u03a3 1rs ,(2)", "formula_coordinates": [2.0, 76.35, 556.24, 213.09, 24.6]}, {"formula_id": "formula_2", "formula_text": "\u03a3 B = I n \u2297 \u03b1I d + B \u2297 \u03b2I d = (\u03b1I n + \u03b2B) \u2297 I d =: \u03a3 B \u2297 I d ,", "formula_coordinates": [2.0, 109.89, 679.76, 138.91, 24.59]}, {"formula_id": "formula_3", "formula_text": "p(X, B|\u03b1, \u03b2, \u03be, k) = P n (B|\u03be, k) \u2022 b\u2208B d j=1 N (X i b j |\u03b1I n b + \u03b21 n b 1 t n b ) ,(4)", "formula_coordinates": [2.0, 334.68, 147.39, 206.76, 30.32]}, {"formula_id": "formula_4", "formula_text": "\u03a3 0 = \u03b1I d , \u03a3 1 = \u03b2I d .", "formula_coordinates": [2.0, 307.44, 315.48, 234.0, 21.61]}, {"formula_id": "formula_5", "formula_text": "P n (B|S, \u03b1, \u03b2, \u03be, k) \u221d W d (S|\u03a3 B ) \u2022 P n (B|\u03be, k) \u221d |\u03a3 B | \u2212 d 2 exp \u2212 d 2 tr(\u03a3 \u22121 B S) \u2022 P n (B|\u03be, k),(5)", "formula_coordinates": [2.0, 324.39, 416.56, 217.05, 30.31]}, {"formula_id": "formula_6", "formula_text": "P n (B|S, \u03b1, \u03b8, \u03be, k) \u221d P n (B|\u03be, k) \u2022 b\u2208B |W b | d 2 exp \u2212 b\u2208B d 2 tr(W b S bb ) ,(6)", "formula_coordinates": [2.0, 330.73, 506.12, 210.71, 30.62]}, {"formula_id": "formula_7", "formula_text": "W b = (\u03b1I b + \u03b21 b 1 t b ) \u22121 = 1 \u03b1 I b \u2212 \u03b8 1+n b \u03b8 1 b 1 t b . (7)", "formula_coordinates": [2.0, 319.05, 704.85, 222.39, 14.18]}, {"formula_id": "formula_8", "formula_text": "tr(W b S bb ) = 1 \u03b1 tr(S bb ) \u2212 \u03b8 1+n b \u03b8S bb ,(8)", "formula_coordinates": [3.0, 92.99, 92.11, 196.45, 14.14]}, {"formula_id": "formula_9", "formula_text": "|W | = b\u2208B \u03bb b = \u03b1 \u2212n b\u2208B (1 + \u03b8n b ) \u22121 .(9)", "formula_coordinates": [3.0, 83.61, 199.69, 205.84, 12.72]}, {"formula_id": "formula_10", "formula_text": "P n (B|\u2022) \u221d P n (B|\u03be, k) \u2022 b\u2208B (1 + \u03b8n b ) \u22121 \u2212d/2 \u2022 exp \u2212 1 \u03b1 d 2 b\u2208B tr(W b S bb ) . (10", "formula_coordinates": [3.0, 62.2, 313.87, 223.1, 31.89]}, {"formula_id": "formula_11", "formula_text": ")", "formula_coordinates": [3.0, 285.29, 324.99, 4.15, 8.64]}, {"formula_id": "formula_12", "formula_text": "p(\u03b1|r, s) = s r \u0393(r) 1 \u03b1 r+1 exp \u2212 s \u03b1 ,(11)", "formula_coordinates": [3.0, 99.3, 411.6, 190.14, 15.56]}, {"formula_id": "formula_13", "formula_text": "P n (B|\u2022) \u221d P n (B|\u03be, k)|W | d/2 (\u03b1=1) (s + s 0 ) r+r0+1 , (12)", "formula_coordinates": [3.0, 65.73, 529.59, 223.71, 14.68]}, {"formula_id": "formula_14", "formula_text": "R n : x \u223c N (\u00b5 = 0 n , \u03a3 = \u03a3 B ).", "formula_coordinates": [3.0, 307.44, 68.64, 134.65, 11.23]}, {"formula_id": "formula_15", "formula_text": "D ij = S ii + S jj \u2212 2S ij .", "formula_coordinates": [4.0, 55.44, 303.34, 99.75, 9.65]}, {"formula_id": "formula_16", "formula_text": "\u2206 ij = \u03a3 B ii + \u03a3 B jj \u2212 2\u03a3 B ij .", "formula_coordinates": [4.0, 55.44, 478.64, 127.23, 10.15]}, {"formula_id": "formula_17", "formula_text": "LX \u223c N (L\u00b5, L\u03a3L t ). (13", "formula_coordinates": [4.0, 126.83, 543.55, 158.47, 11.03]}, {"formula_id": "formula_18", "formula_text": ")", "formula_coordinates": [4.0, 285.29, 545.95, 4.15, 8.64]}, {"formula_id": "formula_19", "formula_text": "K = 1, two parameter set- tings (\u00b5 1 , \u03a3 1 ) and (\u00b5 2 , \u03a3 2 ) are equivalent if L(\u00b5 1 \u2212\u00b5 2 ) = 0 and L(\u03a3 1 \u2212 \u03a3 2 )L t = 0, i.e. if \u00b5 1 \u2212 \u00b5 2 \u2208 1, and (\u03a3 1 \u2212 \u03a3 2 ) \u2208 {1 n v t + v1 t n : v \u2208 R n }, a space which is usually denoted by sym 2 (1 \u2297 R n ).", "formula_coordinates": [4.0, 55.44, 564.76, 234.0, 57.15]}, {"formula_id": "formula_20", "formula_text": "P (B|D, \u2022) \u221d W d (\u2212D|1, \u2212\u2206) \u2022 P n (B|\u03be, k) \u221d det( W ) d 2 exp d 4 tr( W D) \u2022 P n (B|\u03be, k). (14", "formula_coordinates": [4.0, 327.41, 236.33, 209.88, 30.62]}, {"formula_id": "formula_21", "formula_text": ")", "formula_coordinates": [4.0, 537.29, 247.71, 4.15, 8.64]}, {"formula_id": "formula_22", "formula_text": "P (B|\u2022) \u221d P n (B|\u03be, k) det( W (\u03b1=1) ) d 2 (s+s 0 ) n d 2 +r0 . (15)", "formula_coordinates": [4.0, 315.16, 492.04, 226.28, 14.24]}, {"formula_id": "formula_23", "formula_text": "W D) = tr(W D) \u2212 \u03c1 \u2022 tr(W 11 t W D) = tr(W D) \u2212 \u03c1 \u2022 1 t W DW 1. (16", "formula_coordinates": [5.0, 352.89, 606.61, 184.4, 25.82]}, {"formula_id": "formula_24", "formula_text": ")", "formula_coordinates": [5.0, 537.29, 615.6, 4.15, 8.64]}, {"formula_id": "formula_25", "formula_text": "\u03c1 ab\u2208B 1 t a W a D ab W b 1 b =: \u03c1 ab\u2208B \u03a6 ab , \u03a6 ab =D ab \u2212 \u03b3 aDab \u2212 \u03b3 bDab + \u03b3 a \u03b3 bDab . (17", "formula_coordinates": [6.0, 67.93, 103.49, 217.36, 27.94]}, {"formula_id": "formula_26", "formula_text": ")", "formula_coordinates": [6.0, 285.29, 113.79, 4.15, 8.64]}, {"formula_id": "formula_27", "formula_text": "for i = 1 to n do \u2200a \u2208 B:D ia = j\u2208a D ij O(n) for c = 1 to k B do UpdateD O(k B ) Recompute c-th term in tr(W D) O(1) Compute \u2200a \u2208 B : \u03a6 ac = \u03a6 ca O(k B ) end for end for", "formula_coordinates": [6.0, 65.4, 182.84, 188.1, 92.64]}], "doi": ""}