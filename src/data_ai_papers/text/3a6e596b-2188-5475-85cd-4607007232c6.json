{"title": "Retrofitting Word Vectors to Semantic Lexicons", "authors": "Manaal Faruqui; Jesse Dodge; Sujay K Jauhar; Chris Dyer; Eduard Hovy; Noah A Smith", "pub_date": "", "abstract": "Vector space word representations are learned from distributional information of words in large corpora. Although such statistics are semantically informative, they disregard the valuable information that is contained in semantic lexicons such as WordNet, FrameNet, and the Paraphrase Database. This paper proposes a method for refining vector space representations using relational information from semantic lexicons by encouraging linked words to have similar vector representations, and it makes no assumptions about how the input vectors were constructed. Evaluated on a battery of standard lexical semantic evaluation tasks in several languages, we obtain substantial improvements starting with a variety of word vector models. Our refinement method outperforms prior techniques for incorporating semantic lexicons into word vector training algorithms.", "sections": [{"heading": "Introduction", "text": "Data-driven learning of word vectors that capture lexico-semantic information is a technique of central importance in NLP. These word vectors can in turn be used for identifying semantically related word pairs (Turney, 2006;Agirre et al., 2009) or as features in downstream text processing applications (Turian et al., 2010;Guo et al., 2014). A variety of approaches for constructing vector space embeddings of vocabularies are in use, notably including taking low rank approximations of cooccurrence statistics (Deerwester et al., 1990) and using internal representations from neural network models of word sequences (Collobert and Weston, 2008).\nBecause of their value as lexical semantic representations, there has been much research on improv-ing the quality of vectors. Semantic lexicons, which provide type-level information about the semantics of words, typically by identifying synonymy, hypernymy, hyponymy, and paraphrase relations should be a valuable resource for improving the quality of word vectors that are trained solely on unlabeled corpora. Examples of such resources include Word-Net (Miller, 1995), FrameNet (Baker et al., 1998) and the Paraphrase Database (Ganitkevitch et al., 2013).\nRecent work has shown that by either changing the objective of the word vector training algorithm in neural language models (Yu and Dredze, 2014;Xu et al., 2014;Fried and Duh, 2014) or by relation-specific augmentation of the cooccurence matrix in spectral word vector models to incorporate semantic knowledge (Yih et al., 2012;Chang et al., 2013), the quality of word vectors can be improved. However, these methods are limited to particular methods for constructing vectors.\nThe contribution of this paper is a graph-based learning technique for using lexical relational resources to obtain higher quality semantic vectors, which we call \"retrofitting.\" In contrast to previous work, retrofitting is applied as a post-processing step by running belief propagation on a graph constructed from lexicon-derived relational information to update word vectors ( \u00a72). This allows retrofitting to be used on pre-trained word vectors obtained using any vector training model. Intuitively, our method encourages the new vectors to be (i) similar to the vectors of related word types and (ii) similar to their purely distributional representations. The retrofitting process is fast, taking about 5 seconds for a graph of 100,000 words and vector length 300, and its runtime is independent of the original word vector training model. Experimentally, we show that our method works well with different state-of-the-art word vector models, using different kinds of semantic lexicons and gives substantial improvements on a variety of benchmarks, while beating the current state-of-theart approaches for incorporating semantic information in vector training and trivially extends to multiple languages. We show that retrofitting gives consistent improvement in performance on evaluation benchmarks with different word vector lengths and show a qualitative visualization of the effect of retrofitting on word vector quality. The retrofitting tool is available at: https://github.com/ mfaruqui/retrofitting.", "publication_ref": ["b45", "b0", "b44", "b23", "b15", "b10", "b3", "b21", "b48", "b46", "b20", "b47", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Retrofitting with Semantic Lexicons", "text": "Let V = {w 1 , . . . , w n } be a vocabulary, i.e, the set of word types, and \u2126 be an ontology that encodes semantic relations between words in V . We represent \u2126 as an undirected graph (V, E) with one vertex for each word type and edges (w i , w j ) \u2208 E \u2286 V \u00d7 V indicating a semantic relationship of interest. These relations differ for different semantic lexicons and are described later ( \u00a74).\nThe matrixQ will be the collection of vector representationsq i \u2208 R d , for each w i \u2208 V , learned using a standard data-driven technique, where d is the length of the word vectors. Our objective is to learn the matrix Q = (q 1 , . . . , q n ) such that the columns are both close (under a distance metric) to their counterparts inQ and to adjacent vertices in \u2126. Figure 1 shows a small word graph with such edge connections; white nodes are labeled with the Q vec-tors to be retrofitted (and correspond to V \u2126 ); shaded nodes are labeled with the corresponding vectors in Q, which are observed. The graph can be interpreted as a Markov random field (Kindermann and Snell, 1980).\nThe distance between a pair of vectors is defined to be the Euclidean distance. Since we want the inferred word vector to be close to the observed valueq i and close to its neighbors q j , \u2200j such that (i, j) \u2208 E, the objective to be minimized becomes:\n\u03a8(Q) = n i=1 \uf8ee \uf8f0 \u03b1 i q i \u2212q i 2 + (i,j)\u2208E \u03b2 ij q i \u2212 q j 2 \uf8f9 \uf8fb\nwhere \u03b1 and \u03b2 values control the relative strengths of associations (more details in \u00a76.1).\nIn this case, we first train the word vectors independent of the information in the semantic lexicons and then retrofit them. \u03a8 is convex in Q and its solution can be found by solving a system of linear equations. To do so, we use an efficient iterative updating method (Bengio et al., 2006;Subramanya et al., 2010;Das and Petrov, 2011;Das and Smith, 2011). The vectors in Q are initialized to be equal to the vectors inQ. We take the first derivative of \u03a8 with respect to one q i vector, and by equating it to zero arrive at the following online update:\nq i = j:(i,j)\u2208E \u03b2 ij q j + \u03b1 iqi j:(i,j)\u2208E \u03b2 ij + \u03b1 i (1)\nIn practice, running this procedure for 10 iterations converges to changes in Euclidean distance of adjacent vertices of less than 10 \u22122 . The retrofitting approach described above is modular; it can be applied to word vector representations obtained from any model as the updates in Eq. 1 are agnostic to the original vector training model objective.\nSemantic Lexicons during Learning. Our proposed approach is reminiscent of recent work on improving word vectors using lexical resources (Yu and Dredze, 2014;Xu et al., 2014) which alters the learning objective of the original vector training model with a prior (or a regularizer) that encourages semantically related vectors (in \u2126) to be close together, except that our technique is applied as a second stage of learning. We describe the prior approach here since it will serve as a baseline.\nHere semantic lexicons play the role of a prior on Q which we define as follows:\np(Q) \u221d exp \uf8eb \uf8ed \u2212\u03b3 n i=1 j:(i,j)\u2208E \u03b2 ij q i \u2212 q j 2 \uf8f6 \uf8f8\n(2) Here, \u03b3 is a hyperparameter that controls the strength of the prior. As in the retrofitting objective, this prior on the word vector parameters forces words connected in the lexicon to have close vector representations as did \u03a8(Q) (with the role ofQ being played by cross entropy of the empirical distribution).\nThis prior can be incorporated during learning through maximum a posteriori (MAP) estimation. Since there is no closed form solution of the estimate, we consider two iterative procedures. In the first, we use the sum of gradients of the log-likelihood (given by the extant vector learning model) and the log-prior (from Eq. 2), with respect to Q for learning. Since computing the gradient of Eq. 2 has linear runtime in the vocabulary size n, we use lazy updates (Carpenter, 2008) for every k words during training. We call this the lazy method of MAP. The second technique applies stochastic gradient ascent to the log-likelihood, and after every k words applies the update in Eq. 1. We call this the periodic method. We later experimentally compare these methods against retrofitting ( \u00a76.2).", "publication_ref": ["b28", "b4", "b42", "b12", "b13", "b48", "b46", "b8"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Word Vector Representations", "text": "We now describe the various publicly available pretrained English word vectors on which we will test the applicability of the retrofitting model. These vectors have been chosen to have a balanced mix between large and small amounts of unlabeled text as well as between neural and spectral methods of training word vectors.\nGlove Vectors. Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations show interesting linear substructures of the word vector space. These vectors were trained on 6 billion words from Wikipedia and English Gigaword  and are of length 300. 1\nSkip-Gram Vectors (SG). The word2vec tool (Mikolov et al., 2013a) is fast and currently in wide use. In this model, each word's Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. The available vectors are trained on 100 billion words of Google news dataset and are of length 300. 2\nGlobal Context Vectors (GC). These vectors are learned using a recursive neural network that incorporates both local and global (document-level) context features (Huang et al., 2012). These vectors were trained on the first 1 billion words of English Wikipedia and are of length 50. 3\nMultilingual Vectors (Multi). Faruqui and Dyer (2014) learned vectors by first performing SVD on text in different languages, then applying canonical correlation analysis (CCA) on pairs of vectors for words that align in parallel corpora. The monolingual vectors were trained on WMT-2011 news corpus for English, French, German and Spanish. We use the Enligsh word vectors projected in the common English-German space. The monolingual English WMT corpus had 360 million words and the trained vectors are of length 512. 4", "publication_ref": ["b38", "b33", "b26", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Semantic Lexicons", "text": "We use three different semantic lexicons to evaluate their utility in improving the word vectors. We include both manually and automatically created lexicons. Table 1 shows the size of the graphs obtained from these lexicons.\nPPDB. The paraphrase database (Ganitkevitch et al., 2013) is a semantic lexicon containing more than 220 million paraphrase pairs of English. 5 Of these, 8 million are lexical (single word to single word) paraphrases. The key intuition behind the acquisition of its lexical paraphrases is that two words in one language that align, in parallel text, to the same word in a different language, should be synonymous. For example, if the words jailed and imprisoned are translated as the same word in another language, it may be reasonable to assume they have the same meaning. In our experiments, we instantiate an edge in E for each lexical paraphrase in PPDB. The lexical paraphrase dataset comes in different sizes ranging from S to XXXL, in decreasing order of paraphrasing confidence and increasing order of size. We chose XL for our experiments. We want to give higher edge weights (\u03b1 i ) connecting the retrofitted word vectors (q) to the purely distributional word vectors (q) than to edges connecting the retrofitted vectors to each other (\u03b2 ij ), so all \u03b1 i are set to 1 and \u03b2 ij to be degree(i) \u22121 (with i being the node the update is being applied to). 6\nWordNet. WordNet (Miller, 1995) is a large human-constructed semantic lexicon of English words. It groups English words into sets of synonyms called synsets, provides short, general definitions, and records the various semantic relations between synsets. This database is structured in a graph particularly suitable for our task because it explicitly relates concepts with semantically aligned relations such as hypernyms and hyponyms. For example, the word dog is a synonym of canine, a hypernym of puppy and a hyponym of animal. We perform two different experiments with WordNet: (1) connecting a word only to synonyms, and (2) connecting a word to synonyms, hypernyms and hyponyms. We refer to these two graphs as WN syn and WN all , respectively. In both settings, all \u03b1 i are set to 1 and \u03b2 ij to be degree(i) \u22121 .\nFrameNet. FrameNet (Baker et al., 1998;Fillmore et al., 2003) is a rich linguistic resource containing information about lexical and predicateargument semantics in English. Frames can be realized on the surface by many different word types, which suggests that the word types evoking the same frame should be semantically related. For example, the frame Cause change of position on a scale is associated with push, raise, and growth (among many others). In our use of FrameNet, two words that group together with any frame are given an edge in E. We refer to this graph as FN. All \u03b1 i are set to 1 and \u03b2 ij to be degree(i) \u22121 .", "publication_ref": ["b21", "b3", "b18"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Evaluation Benchmarks", "text": "We evaluate the quality of our word vector representations on tasks that test how well they capture both semantic and syntactic aspects of the representations along with an extrinsic sentiment analysis task.\nWord Similarity. We evaluate our word representations on a variety of different benchmarks that have been widely used to measure word similarity.\nThe first one is the WS-353 dataset (Finkelstein et al., 2001) containing 353 pairs of English words that have been assigned similarity ratings by humans.\nThe second benchmark is the RG-65 (Rubenstein and Goodenough, 1965) dataset that contain 65 pairs of nouns. Since the commonly used word similarity datasets contain a small number of word pairs we also use the MEN dataset (Bruni et al., 2012) of 3,000 word pairs sampled from words that occur at least 700 times in a large web corpus. We calculate cosine similarity between the vectors of two words forming a test item, and report Spearman's rank correlation coefficient (Myers and Well, 1995) between the rankings produced by our model against the human rankings.\nSyntactic Relations (SYN-REL). Mikolov et al. (2013b) present a syntactic relation dataset composed of analogous word pairs. It contains pairs of tuples of word relations that follow a common syntactic relation. For example, given walking and walked, the words are differently inflected forms of the same verb. There are nine different kinds of relations and overall there are 10,675 syntactic pairs of word tuples. The task is to find a word d that best fits the following relationship: \"a is to b as c is to d,\" given a, b, and c. We use the vector offset method (Mikolov et al., 2013a;Levy and Goldberg, 2014), computing q = q a \u2212 q b + q c and returning the vector from Q which has the highest cosine similarity to q.\nSynonym Selection (TOEFL). The TOEFL synonym selection task is to select the semantically closest word to a target from a list of four candidates (Landauer and Dumais, 1997). The dataset contains 80 such questions. An example is \"rug \u2192 {sofa, ottoman, carpet, hallway}\", with carpet being the most synonym-like candidate to the target.\nSentiment Analysis (SA). Socher et al. ( 2013) created a treebank containing sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarsegrained treebank of positive and negative classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We train an 2 -regularized logistic regression classifier on the average of the word vectors of a given sentence to predict the coarse-grained sentiment tag at the sentence level, and report the testset accuracy of the classifier.", "publication_ref": ["b19", "b39", "b7", "b37", "b34", "b33", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We first show experiments measuring improvements from the retrofitting method ( \u00a76.1), followed by comparisons to using lexicons during MAP learning ( \u00a76.2) and other published methods ( \u00a76.3). We then test how well retrofitting generalizes to other languages ( \u00a76.4).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Retrofitting", "text": "We use Eq. 1 to retrofit word vectors ( \u00a73) using graphs derived from semantic lexicons ( \u00a74).\nResults. Table 2 shows the absolute changes in performance on different tasks (as columns) with different semantic lexicons (as rows). All of the lexicons offer high improvements on the word similarity tasks (the first three columns). On the TOEFL task, we observe large improvements of the order of 10 absolute points in accuracy for all lexicons except for FrameNet. FrameNet's performance is weaker, in some cases leading to worse performance (e.g., with Glove and SG vectors). For the extrinsic sentiment analysis task, we observe improvements using all the lexicons and gain 1.4% (absolute) in accuracy for the Multi vectors over the baseline. This increase is statistically significant (p < 0.01, McNemar). We observe improvements over Glove and SG vectors, which were trained on billions of tokens on all tasks except for SYN-REL. For stronger baselines (Glove and Multi) we observe smaller improvements as compared to lower baseline scores (SG and GC). We believe that FrameNet does not perform as well as the other lexicons because its frames group words based on very abstract concepts; often words with seemingly distantly related meanings (e.g., push and growth) can evoke the same frame. Interestingly, we almost never improve on the SYN-REL task, especially with higher baselines, this can be attributed to the fact that SYN-REL is inherently a syntactic task and during retrofitting we are incorporating additional semantic information in the vectors. In summary, we find that PPDB gives the best improvement maximum number of times aggreagted over different vetor types, closely followed by WN all , and retrofitting gives gains across tasks and vectors. An ensemble lexicon, in which the graph is the union of the WN all and PPDB lexicons, on average performed slightly worse than PPDB; we omit those results here for brevity.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Semantic Lexicons during Learning", "text": "To incorporate lexicon information during training, and compare its performance against retrofitting, we train log-bilinear (LBL) vectors (Mnih and Teh, 2012). These vectors are trained to optimize the log-likelihood of a language model which predicts a word token w's vector given the set of words in its context (h), also represented as vectors:\np(w | h; Q) \u221d exp i\u2208h q i q j + b j (3)\nWe optimize the above likelihood combined with the prior defined in Eq. 2 using the lazy and periodic techniques described in \u00a72. Since it is costly to compute the partition function over the whole vocabulary, we use noise constrastive estimation (NCE) to estimate the parameters of the model (Mnih and Teh, 2012) using AdaGrad (Duchi et al., 2010) with a learning rate of 0.05.  We train vectors of length 100 on the WMT-2011 news corpus, which contains 360 million words, and use PPDB as the semantic lexicon as it performed reasonably well in the retrofitting experiments ( \u00a76.1). For the lazy method we update with respect to the prior every k = 100,000 words 7 and test for different values of prior strength \u03b3 \u2208 {1, 0.1, 0.01}. For the periodic method, we update the word vectors using Eq. 1 every k \u2208 {25, 50, 100} million words.  Yu and Dredze (2014), Xu et al. (2014). Spearman's correlation (3 left columns) and accuracy (3 right columns) on different tasks.", "publication_ref": ["b36", "b36", "b16", "b48", "b46"], "figure_ref": [], "table_ref": []}, {"heading": "Comparisons to Prior Work", "text": "Two previous models (Yu and Dredze, 2014;Xu et al., 2014) have shown that the quality of word vectors obtained using word2vec tool can be improved by using semantic knowledge from lexicons. Both these models use constraints among words as a regularization term on the training objective during training, and their methods can only be applied for improving the quality of SG and CBOW vectors produced by the word2vec tool. We compared the quality of our vectors against each of these. Yu and Dredze (2014). We train word vectors using their joint model training code 8 while using exactly the same training settings as specified in their best model: CBOW, vector length 100 and PPDB for enrichment. The results are shown in the top half of Table 4 where our model consistently outperforms the baseline and their model. Xu et al. (2014). This model extracts categorical and relational knowledge among words from Freebase 9 and uses it as a constraint while training. Unfortunately, neither their word embeddings nor model training code is publicly available, so we train the SG model by using exactly the same settings as described in their system (vector length 300) and on the same corpus: monolingual English Wikipedia text. 10 We compare the performance of our retrofitting vectors on the SYN-REL and WS-353 task against the best model 11 reported in their paper. As shown in the lower half of Table 4, our model outperforms their model by an absolute 5.5 points absolute on the SYN-REL task, but a slightly inferior score on the WS-353 task.", "publication_ref": ["b48", "b46", "b48", "b46"], "figure_ref": [], "table_ref": []}, {"heading": "Multilingual Evaluation", "text": "We tested our method on three additional languages: German, French, and Spanish. We used the Universal WordNet (de Melo and Weikum, 2009), an automatically constructed multilingual lexical knowledge base based on WordNet. 12 It contains words connected via different lexical relations to other words both within and across languages. We construct separate graphs for different languages (i.e., only linking words to other words in the same language) and apply retrofitting to each. Since not many word similarity evaluation benchmarks are available for languages other than English, we tested our baseline and improved vectors on one benchmark per language. We used RG-65 (Gurevych, 2005), RG-65 (Joubarne and Inkpen, 2011) and MC-30 (Hassan and Mihalcea, 2009) for German, French and Spanish, respectively. 13 We trained SG vectors for each language of length 300 on a corpus of 1 billion tokens, each extracted from Wikipedia, and evaluate them on word similarity on the benchmarks before and after retrofitting. Table 5 shows that we obtain high improvements which strongly indicates that our method generalizes across these languages.", "publication_ref": ["b14", "b24", "b25"], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "Further Analysis", "text": "Retrofitting vs. vector length. With more dimensions, word vectors might be able to capture higher orders of semantic information and retrofitting might be less helpful. We train SG vec-   tors on 1 billion English tokens for vector lengths ranging from 50 to 1,000 and evaluate on the MEN word similarity task. We retrofit these vectors to PPDB ( \u00a74) and evaluate those on the same task. Figure 2 shows consistent improvement in vector quality across different vector lengths.\nVisualization. We randomly select eight word pairs that have the \"adjective to adverb\" relation from the SYN-REL task ( \u00a75). We then take a twodimensional PCA projection of the 100-dimensional SG word vectors and plot them in R 2 . In Figure 3 we plot these projections before (left) and after (right) retrofitting. It can be seen that in the first case the direction of the analogy vectors is not consistent, but after retrofitting all the analogy vectors are aligned in the same direction.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Related Work", "text": "The use of lexical semantic information in training word vectors has been limited. Recently, word similarity knowledge (Yu and Dredze, 2014;Fried and Duh, 2014) and word relational knowledge (Xu et al., 2014; have been used to improve the word2vec embeddings in a joint training model similar to our regularization approach. In latent semantic analysis, the word cooccurrence matrix can be constructed to incorporate relational information like antonym specific polarity induction (Yih et al., 2012) and multi-relational latent semantic analysis (Chang et al., 2013).\nThe approach we propose is conceptually similar to previous work that uses graph structures to propagate information among semantic concepts (Zhu, 2005;Culp and Michailidis, 2008). Graph-based belief propagation has also been used to induce POS tags (Subramanya et al., 2010;Das and Petrov, 2011) and semantic frame associations (Das and Smith, 2011). In those efforts, labels for unknown words were inferred using a method similar to ours. Broadly, graph-based semi-supervised learning (Zhu, 2005;Talukdar and Pereira, 2010) has been applied to machine translation (Alexandrescu and Kirchhoff, 2009), unsupervised semantic role induction (Lang and Lapata, 2011), semantic document modeling (Schuhmacher and Ponzetto, 2014), language generation (Krahmer et al., 2003) and sentiment analysis (Goldberg and Zhu, 2006).", "publication_ref": ["b48", "b20", "b46", "b47", "b9", "b11", "b42", "b12", "b13", "b43", "b1", "b31", "b40", "b29", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We have proposed a simple and effective method named retrofitting to improve word vectors using word relation knowledge found in semantic lexicons. Retrofitting is used as a post-processing step to improve vector quality and is more modular than other approaches that use semantic information while training. It can be applied to vectors obtained from any word vector training method. Our experiments explored the method's performance across tasks, semantic lexicons, and languages and showed that it outperforms existing alternatives. The retrofitting tool is available at: https:// github.com/mfaruqui/retrofitting.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "This research was supported in part by the National Science Foundation under grants IIS-1143703, IIS-1147810, and IIS-1251131; by IARPA via Department of Interior National Business Center (DoI/NBC) contract number D12PC00337; and by DARPA under grant FA87501220342. Part of the computational work was carried out on resources provided by the Pittsburgh Supercomputing Center. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: the views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/NBC, DARPA, or the U.S. Government.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "A study on similarity and relatedness using distributional and wordnet-based approaches", "journal": "", "year": "2009", "authors": "Eneko Agirre; Enrique Alfonseca; Keith Hall; Jana Kravalova"}, {"ref_id": "b1", "title": "", "journal": "", "year": "2009", "authors": "Andrei Alexandrescu; Katrin Kirchhoff"}, {"ref_id": "b2", "title": "Graph-based learning for statistical machine translation", "journal": "", "year": "", "authors": ""}, {"ref_id": "b3", "title": "The berkeley framenet project", "journal": "", "year": "1998", "authors": "Collin F Baker; Charles J Fillmore; John B Lowe"}, {"ref_id": "b4", "title": "Label propagation and quadratic criterion", "journal": "", "year": "2006", "authors": "Yoshua Bengio; Olivier Delalleau; Nicolas Le Roux"}, {"ref_id": "b5", "title": "", "journal": "", "year": "2014", "authors": "Jiang Bian; Bin Gao; Tie-Yan Liu"}, {"ref_id": "b6", "title": "Knowledge-powered deep learning for word embedding", "journal": "", "year": "", "authors": ""}, {"ref_id": "b7", "title": "Distributional semantics in technicolor", "journal": "", "year": "2012", "authors": "Elia Bruni; Gemma Boleda; Marco Baroni; Nam-Khanh Tran"}, {"ref_id": "b8", "title": "Lazy sparse stochastic gradient descent for regularized multinomial logistic regression", "journal": "", "year": "2008", "authors": "Bob Carpenter"}, {"ref_id": "b9", "title": "Multi-relational latent semantic analysis", "journal": "", "year": "2013", "authors": "Kai-Wei Chang; Wen-Tau Yih; Christopher Meek"}, {"ref_id": "b10", "title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "journal": "", "year": "2008", "authors": "Ronan Collobert; Jason Weston"}, {"ref_id": "b11", "title": "Graph-based semisupervised learning", "journal": "", "year": "2008", "authors": "Mark Culp; George Michailidis"}, {"ref_id": "b12", "title": "Unsupervised partof-speech tagging with bilingual graph-based projections", "journal": "", "year": "2011", "authors": "Dipanjan Das; Slav Petrov"}, {"ref_id": "b13", "title": "Semisupervised frame-semantic parsing for unknown predicates", "journal": "", "year": "2011", "authors": "Dipanjan Das; Noah A Smith"}, {"ref_id": "b14", "title": "Towards a universal wordnet by learning from combined evidence", "journal": "", "year": "2009", "authors": "Gerard De; Melo ; Gerhard Weikum"}, {"ref_id": "b15", "title": "Indexing by latent semantic analysis", "journal": "Journal of the American Society for Information Science", "year": "1990", "authors": "S C Deerwester; S T Dumais; T K Landauer; G W Furnas; R A Harshman"}, {"ref_id": "b16", "title": "Adaptive subgradient methods for online learning and stochastic optimization", "journal": "", "year": "2010-03", "authors": "John Duchi; Elad Hazan; Yoram Singer"}, {"ref_id": "b17", "title": "Improving vector space word representations using multilingual correlation", "journal": "", "year": "2014", "authors": "Manaal Faruqui; Chris Dyer"}, {"ref_id": "b18", "title": "", "journal": "International Journal of Lexicography", "year": "2003", "authors": "Charles Fillmore; Christopher Johnson; Miriam Petruck"}, {"ref_id": "b19", "title": "Placing search in context: the concept revisited", "journal": "", "year": "2001", "authors": "Lev Finkelstein; Evgeniy Gabrilovich; Yossi Matias; Ehud Rivlin; Zach Solan; Gadi Wolfman; Eytan Ruppin"}, {"ref_id": "b20", "title": "Incorporating both distributional and relational semantics in word representations", "journal": "", "year": "2014", "authors": "Daniel Fried; Kevin Duh"}, {"ref_id": "b21", "title": "PPDB: The paraphrase database", "journal": "", "year": "2013", "authors": "Juri Ganitkevitch; Benjamin Van Durme; Chris Callison-Burch"}, {"ref_id": "b22", "title": "Seeing stars when there aren't many stars: Graph-based semi-supervised learning for sentiment categorization", "journal": "", "year": "2006", "authors": "B Andrew; Xiaojin Goldberg;  Zhu"}, {"ref_id": "b23", "title": "Revisiting embedding features for simple semisupervised learning", "journal": "", "year": "2014", "authors": "Jiang Guo; Wanxiang Che; Haifeng Wang; Ting Liu"}, {"ref_id": "b24", "title": "Using the structure of a conceptual network in computing semantic relatedness", "journal": "", "year": "2005", "authors": "Iryna Gurevych"}, {"ref_id": "b25", "title": "Cross-lingual semantic relatedness using encyclopedic knowledge", "journal": "", "year": "2009", "authors": "Samer Hassan; Rada Mihalcea"}, {"ref_id": "b26", "title": "Improving word representations via global context and multiple word prototypes", "journal": "", "year": "2012", "authors": "H Eric; Richard Huang;  Socher; D Christopher; Andrew Y Manning;  Ng"}, {"ref_id": "b27", "title": "Comparison of semantic similarity for different languages using the google n-gram corpus and second-order cooccurrence measures", "journal": "", "year": "2011", "authors": "Colette Joubarne; Diana Inkpen"}, {"ref_id": "b28", "title": "Markov Random Fields and Their Applications", "journal": "AMS", "year": "1980", "authors": "Ross Kindermann; J L Snell"}, {"ref_id": "b29", "title": "Graph-based generation of referring expressions", "journal": "Comput. Linguist", "year": "2003", "authors": "Emiel Krahmer; Andr\u00e9 Sebastian Van Erk;  Verleg"}, {"ref_id": "b30", "title": "A solution to plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "journal": "", "year": "1997", "authors": "K Thomas; Susan T Landauer;  Dumais"}, {"ref_id": "b31", "title": "Unsupervised semantic role induction with graph partitioning", "journal": "", "year": "2011", "authors": "Joel Lang; Mirella Lapata"}, {"ref_id": "b32", "title": "Linguistic regularities in sparse and explicit word representations", "journal": "", "year": "2014", "authors": "Omer Levy; Yoav Goldberg"}, {"ref_id": "b33", "title": "Efficient estimation of word representations in vector space", "journal": "", "year": "2013", "authors": "Tomas Mikolov; Kai Chen; Greg Corrado; Jeffrey Dean"}, {"ref_id": "b34", "title": "Linguistic regularities in continuous space word representations", "journal": "", "year": "2013", "authors": "Tomas Mikolov; Yih Wen-Tau; Geoffrey Zweig"}, {"ref_id": "b35", "title": "Wordnet: a lexical database for english", "journal": "Communications of the ACM", "year": "1995", "authors": "A George;  Miller"}, {"ref_id": "b36", "title": "A fast and simple algorithm for training neural probabilistic language models", "journal": "", "year": "2012", "authors": "Andriy Mnih; Yee Whye Teh"}, {"ref_id": "b37", "title": "", "journal": "", "year": "1995", "authors": "Jerome L Myers; Arnold D Well"}, {"ref_id": "b38", "title": "Glove: Global vectors for word representation", "journal": "", "year": "2014", "authors": "Jeffrey Pennington; Richard Socher; Christopher D Manning"}, {"ref_id": "b39", "title": "Contextual correlates of synonymy", "journal": "Commun. ACM", "year": "1965-10", "authors": "Herbert Rubenstein; John B Goodenough"}, {"ref_id": "b40", "title": "Knowledge-based graph document modeling", "journal": "", "year": "2014", "authors": "Michael Schuhmacher; Simone Paolo Ponzetto"}, {"ref_id": "b41", "title": "Recursive deep models for semantic compositionality over a sentiment treebank", "journal": "", "year": "2013", "authors": "Richard Socher; Alex Perelygin; Jean Wu; Jason Chuang; Christopher D Manning; Andrew Y Ng; Christopher Potts"}, {"ref_id": "b42", "title": "Efficient graph-based semi-supervised learning of structured tagging models", "journal": "", "year": "2010", "authors": "Amarnag Subramanya; Slav Petrov; Fernando Pereira"}, {"ref_id": "b43", "title": "Experiments in graph-based semi-supervised learning methods for class-instance acquisition", "journal": "", "year": "2010", "authors": "Partha Pratim Talukdar; Fernando Pereira"}, {"ref_id": "b44", "title": "Word representations: a simple and general method for semi-supervised learning", "journal": "", "year": "2010", "authors": "Joseph Turian; Lev Ratinov; Yoshua Bengio"}, {"ref_id": "b45", "title": "Similarity of semantic relations", "journal": "Comput. Linguist", "year": "2006-09", "authors": "D Peter;  Turney"}, {"ref_id": "b46", "title": "Rc-net: A general framework for incorporating knowledge into word representations", "journal": "", "year": "2014", "authors": "Chang Xu; Yalong Bai; Jiang Bian; Bin Gao; Gang Wang; Xiaoguang Liu; Tie-Yan Liu"}, {"ref_id": "b47", "title": "Polarity inducing latent semantic analysis", "journal": "", "year": "2012", "authors": "Geoffrey Wen-Tau Yih; John C Zweig;  Platt"}, {"ref_id": "b48", "title": "Improving lexical embeddings with semantic knowledge", "journal": "", "year": "2014", "authors": "Mo Yu; Mark Dredze"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Word graph with edges between related words showing the observed (grey) and the inferred (white) word vector representations.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Spearman's correlation on the MEN word similarity task, before and after retrofitting.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Approximate size of the graphs obtained from different lexicons.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Absolute performance changes for including PPDB information while training LBL vectors. Spearman's correlation (3 left columns) and accuracy (3 right columns) on different tasks. Bold indicates greatest improvement.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Spearman's correlation for word similarity evaluation using the using original and retrofitted SG vectors.", "figure_data": "7472Spearman's correlation0 56 58 62 64 66 68 70 60200400 Vector length 600 SG + Retrofitting 800 1000 SG"}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u03a8(Q) = n i=1 \uf8ee \uf8f0 \u03b1 i q i \u2212q i 2 + (i,j)\u2208E \u03b2 ij q i \u2212 q j 2 \uf8f9 \uf8fb", "formula_coordinates": [2.0, 313.2, 201.5, 231.17, 45.9]}, {"formula_id": "formula_1", "formula_text": "q i = j:(i,j)\u2208E \u03b2 ij q j + \u03b1 iqi j:(i,j)\u2208E \u03b2 ij + \u03b1 i (1)", "formula_coordinates": [2.0, 362.66, 446.7, 177.35, 30.36]}, {"formula_id": "formula_2", "formula_text": "p(Q) \u221d exp \uf8eb \uf8ed \u2212\u03b3 n i=1 j:(i,j)\u2208E \u03b2 ij q i \u2212 q j 2 \uf8f6 \uf8f8", "formula_coordinates": [3.0, 83.53, 105.39, 203.75, 45.9]}, {"formula_id": "formula_3", "formula_text": "p(w | h; Q) \u221d exp i\u2208h q i q j + b j (3)", "formula_coordinates": [5.0, 345.2, 577.09, 194.8, 23.53]}], "doi": ""}