{"title": "Studying the Use of Popular Destinations to Enhance Web Search Interaction", "authors": "Ryen W White; Mikhail Bilenko; Silviu Cucerzan", "pub_date": "", "abstract": "We present a novel Web search interaction feature which, for a given query, provides links to websites frequently visited by other users with similar information needs. These popular destinations complement traditional search results, allowing direct navigation to authoritative resources for the query topic. Destinations are identified using the history of search and browsing behavior of many users over an extended time period, whose collective behavior provides a basis for computing source authority. We describe a user study which compared the suggestion of destinations with the previously proposed suggestion of related queries, as well as with traditional, unaided Web search. Results show that search enhanced by destination suggestions outperforms other systems for exploratory tasks, with best performance obtained from mining past user behavior at query-level granularity.", "sections": [{"heading": "INTRODUCTION", "text": "The problem of improving queries sent to Information Retrieval (IR) systems has been studied extensively in IR research [4] [11]. Alternative query formulations, known as query suggestions, can be offered to users following an initial query, allowing them to modify the specification of their needs provided to the system, leading to improved retrieval performance. Recent popularity of Web search engines has enabled query suggestions that draw upon the query reformulation behavior of many users to make query recommendations based on previous user interactions [10].\nLeveraging the decision-making processes of many users for query reformulation has its roots in adaptive indexing [8]. In recent years, applying such techniques has become possible at a much larger scale and in a different context than what was proposed in early work. However, interaction-based approaches to query suggestion may be less potent when the information need is exploratory, since a large proportion of user activity for such information needs may occur beyond search engine interactions. In cases where directed searching is only a fraction of users' information-seeking behavior, the utility of other users' clicks over the space of top-ranked results may be limited, as it does not cover the subsequent browsing behavior. At the same time, user navigation that follows search engine interactions provides implicit endorsement of Web resources preferred by users, which may be particularly valuable for exploratory search tasks.\nThus, we propose exploiting a combination of past searching and browsing user behavior to enhance users' Web search interactions.\nBrowser plugins and proxy server logs provide access to the browsing patterns of users that transcend search engine interactions. In previous work, such data have been used to improve search result ranking by Agichtein et al. [1]. However, this approach only considers page visitation statistics independently of each other, not taking into account the pages' relative positions on post-query browsing paths. Radlinski and Joachims [13] have utilized such collective user intelligence to improve retrieval accuracy by using sequences of consecutive query reformulations, yet their approach does not consider users' interactions beyond the search result page.\nIn this paper, we present a user study of a technique that exploits the searching and browsing behavior of many users to suggest popular Web pages, referred to as destinations henceforth, in addition to the regular search results. The destinations may not be among the topranked results, may not contain the queried terms, or may not even be indexed by the search engine. Instead, they are pages at which other users end up frequently after submitting same or similar queries and then browsing away from initially clicked search results. We conjecture that destinations popular across a large number of users can capture the collective user experience for information needs, and our results support this hypothesis.\nIn prior work, O'Day and Jeffries [12] identified \"teleportation\" as an information-seeking strategy employed by users jumping to their previously-visited information targets, while Anderson et al. [2] applied similar principles to support the rapid navigation of Web sites on mobile devices. In [19], Wexelblat and Maes describe a system to support within-domain navigation based on the browse trails of other users. However, we are not aware of such principles being applied to Web search. Research in the area of recommender systems has also addressed similar issues, but in areas such as question-answering [9] and relatively small online communities [16]. Perhaps the nearest instantiation of teleportation is search engines' offering of several within-domain shortcuts below the title of a search result. While these may be based on user behavior and possibly site structure, the user saves at most one click from this feature. In contrast, our proposed approach can transport users to locations many clicks beyond the search result, saving time and giving them a broader perspective on the available related information.\nThe conducted user study investigates the effectiveness of including links to popular destinations as an additional interface feature on search engine result pages. We compare two variants of this approach against the suggestion of related queries and unaided Web search, and seek answers to questions on: (i) user preference and search effectiveness for known-item and exploratory search tasks, and (ii) the preferred distance between query and destination used to identify popular destinations from past behavior logs. The results indicate that suggesting popular destinations to users attempting exploratory tasks provides best results in key aspects of the information-seeking experience, while providing query refinement suggestions is most desirable for known-item tasks.\nThe remainder of the paper is structured as follows. In Section 2 we describe the extraction of search and browsing trails from user activity logs, and their use in identifying top destinations for new queries. Section 3 describes the design of the user study, while Sections 4 and 5 present the study findings and their discussion, respectively. We conclude in Section 6 with a summary.", "publication_ref": ["b3", "b10", "b9", "b7", "b0", "b12", "b11", "b1", "b18", "b8", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "SEARCH TRAILS AND DESTINATIONS", "text": "We used Web activity logs containing searching and browsing activity collected with permission from hundreds of thousands of users over a five-month period between December 2005 and April 2006. Each log entry included an anonymous user identifier, a timestamp, a unique browser window identifier, and the URL of a visited Web page. This information was sufficient to reconstruct temporally ordered sequences of viewed pages that we refer to as \"trails\". In this section, we summarize the extraction of trails, their features, and destinations (trail end-points). In-depth description and analysis of trail extraction are presented in [20].", "publication_ref": ["b19"], "figure_ref": [], "table_ref": []}, {"heading": "Trail Extraction", "text": "For each user, interaction logs were grouped based on browser identifier information. Within each browser instance, participant navigation was summarized as a path known as a browser trail, from the first to the last Web page visited in that browser. Located within some of these trails were search trails that originated with a query submission to a commercial search engine such as Google, Yahoo!, Windows Live Search, and Ask. It is these search trails that we use to identify popular destinations.\nAfter originating with a query submission to a search engine, trails proceed until a point of termination where it is assumed that the user has completed their information-seeking activity. Trails must contain pages that are either: search result pages, search engine homepages, or pages connected to a search result page via a sequence of clicked hyperlinks. Extracting search trails using this methodology also goes some way toward handling multi-tasking, where users run multiple searches concurrently. Since users may open a new browser window (or tab) for each task [18], each task has its own browser trail, and a corresponding distinct search trail.\nTo reduce the amount of \"noise\" from pages unrelated to the active search task that may pollute our data, search trails are terminated when one of the following events occurs: (1) a user returns to their homepage, checks e-mail, logs in to an online service (e.g., MySpace or del.ico.us), types a URL or visits a bookmarked page;\n(2) a page is viewed for more than 30 minutes with no activity; (3) the user closes the active browser window. If a page (at step i) meets any of these criteria, the trail is assumed to terminate on the previous page (i.e., step i -1).\nThere are two types of search trails we consider: session trails and query trails. Session trails transcend multiple queries and terminate only when one of the three termination criteria above are satisfied. Query trails use the same termination criteria as session trails, but also terminate upon submission of a new query to a search engine. Approximately 14 million query trails and 4 million session trails were extracted from the logs. We now describe some trail features.", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}, {"heading": "Trail and Destination Analysis", "text": "Table 1 presents summary statistics for the query and session trails. Differences in user interaction between the last domain on the trail (Domain n) and all domains visited earlier (Domains 1 to (n -1)) are particularly important, because they highlight the wealth of user behavior data not captured by logs of search engine interactions. Statistics are averages for all trails with two or more steps (i.e., those trails where at least one search result was clicked). The statistics suggest that users generally browse far from the search results page (i.e., around 5 steps), and visit a range of domains during the course of their search. On average, users visit 2 unique (non search-engine) domains per query trail, and just over 4 unique domains per session trail. This suggests that users often do not find all the information they seek on the first domain they visit. For query trails, users also visit more pages, and spend significantly longer, on the last domain in the trail compared to all previous domains combined. 1 These distinctions of the last domains in the trails may indicate user interest, page utility, or page relevance. 2", "publication_ref": ["b0"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Destination Prediction", "text": "For frequent queries, most popular destinations identified from Web activity logs could be simply stored for future lookup at search time. However, we have found that over the six-month period covered by our dataset, 56.9% of queries are unique, and 97% queries occur 10 or fewer times, accounting for 19.8% and 66.3% of all searches respectively (these numbers are comparable to those reported in previous studies of search engine query logs [15,17]). Therefore, a lookup-based approach would prevent us from reliably suggesting destinations for a large fraction of searches. To overcome this problem, we utilize a simple term-based prediction model.\nAs discussed above, we extract two types of destinations: query destinations and session destinations. For both destination types, we obtain a corpus of query-destination pairs and use it to construct term-vector representation of destinations that is analogous to the classic tf.idf document representation in traditional IR [14].\nThen, given a new query q consisting of k terms t 1 \u2026t k , we identify highest-scoring destinations using the following similarity function:\nWhere query and destination term weights, and computed using standard tf.idf weighting and query session-normalized smoothed tf.idf weighting, respectively. exploring alternative algorithms for the destination prediction task remains an interesting challenge for future work, results of the user study described in subsequent sections demonstrate that this simple approach provides robust, effective results.", "publication_ref": ["b14", "b16", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "STUDY", "text": "To examine the usefulness of destinations, we conducted a user study investigating the perceptions and performance of 36 subjects on four Web search systems, two with destination suggestions", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Systems", "text": "Four systems were used in this study: a baseline Web with no explicit support for query refinement (Basel system with a query suggestion method that recommend queries (QuerySuggestion), and two systems that augment baseline Web search with destination suggestions using either query trails (QueryDestination), or end-points of (SessionDestination).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "System 1: Baseline", "text": "To establish baseline performance against which other systems can be compared, we developed a masked interface to a popular engine without additional support in formulating queries. This system presented the user-constructed query to the search engine and returned ten top-ranking documents retrieved by the engine. To remove potential bias that may have been caused by subjects' prior perceptions, we removed all identifying information such as engine logos and distinguishing interface features.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "System 2: QuerySuggestion", "text": "In addition to the basic search functionality offered by QuerySuggestion provides suggestions about further refinements that searchers can make following an initial query submission. These suggestions are computed using the search engine query log over the timeframe used for trail generation each target query, we retrieve two sets of candidate sugg contain the target query as a substring. One set is composed of 100 most frequent such queries, while the second set contain frequent queries that followed the target query in query logs candidate query is then scored by multiplying its smoothed frequency by its smoothed frequency of following the target query in past search sessions, using Laplacian smoothing. Based on these scores, six top-ranked query suggestions are returned. If fewer than six suggestions are found, iterative backoff is performed using progressively longer suffixes of the target query; a similar strategy is described in [10].\nSuggestions were offered in a box positioned on the top result page, adjacent to the search results. Figure 1a position of the suggestions on the page. Figure 1b shows a zoomed view of the portion of the results page containing the suggestions offered for the query [hubble telescope]. To the left of each query and , are and query-and userweighting, respectively. While exploring alternative algorithms for the destination prediction task emains an interesting challenge for future work, results of the user study described in subsequent sections demonstrate that this simple conducted a user the perceptions and performance of 36 subjects suggestions.\nWeb search system Baseline), a search that recommends additional and two systems that augment baseline using either end-points of of session trails To establish baseline performance against which other systems can popular search additional support in formulating queries. This the search engine documents retrieved by the engine. To remove potential bias that may have been caused by subjects' prior tions, we removed all identifying information such as search\nIn addition to the basic search functionality offered by Baseline, provides suggestions about further query refinements that searchers can make following an initial query using the search over the timeframe used for trail generation. For each target query, we retrieve two sets of candidate suggestions that is composed of 100 most frequent such queries, while the second set contains 100 most query logs. Each smoothed overall frequency of following the target query Based on these are returned. If fewer than , iterative backoff is performed using progressively longer suffixes of the target query; a similar strategy Suggestions were offered in a box positioned on the top-right of the result page, adjacent to the search results. Figure 1a shows the position of the suggestions on the page. Figure 1b  suggestion is an icon similar to a progress bar that encodes its normalized popularity. Clicking a suggestion retrieves new search results for that query.", "publication_ref": ["b9"], "figure_ref": ["fig_0", "fig_0", "fig_0", "fig_0"], "table_ref": []}, {"heading": "System 3: QueryDestination", "text": "QueryDestination uses an interface similar to However, instead of showing query refinements query, QueryDestination suggests up to six destinations visited by other users who submitted queries similar to the current one, and computed as described in the previous section shows the position of the destination suggestions page. Figure 2b shows a zoomed view of the portion of the resu page destinations suggested for the query [hubble telescope]  2 to the destination name, there is a clickable icon that allows the user to execute a search for the current query within the domain displayed. We show destinations as a separate l than increasing their search result rank, since they may deviate from the original query (e.g., those focus topics or not containing the original query terms)", "publication_ref": [], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "System 4: SessionDestination", "text": "The interface functionality in SessionDestination QueryDestination. The only difference between the two systems is the definition of trail end-points for queries used in computing top destinations. QueryDestination directs users to the domains others end up at for the active or similar quer SessionDestination directs users to the domains other users visit at the end of the search session that follows the active or similar queries. This downgrades the effect of multiple query iterations (i.e., we only care where users end up after submitting all queries), rather than directing searchers to potentially irrelevant domains that may precede a query reformulation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Research Questions", "text": "We were interested in determining the value of popular destinations To do this we attempt to answer the following research questions: 3 To improve reliability, in a similar way to QuerySuggestion are only shown if their popularity exceeds a frequency threshold.", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}, {"heading": "b) Zoomed suggestions", "text": "Query suggestion presentation in QuerySuggestion.\nsuggestion is an icon similar to a progress bar that encodes its normalized popularity. Clicking a suggestion retrieves new search interface similar to QuerySuggestion. refinements for the submitted destinations frequently ted queries similar to the current one, and computed as described in the previous section. 3 Figure 2a on suggestions on search results Figure 2b shows a zoomed view of the portion of the results\n[hubble telescope].", "publication_ref": ["b2"], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "(b) Zoomed destinations", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "QueryDestination.", "text": "title of each destination (shown in Figure 2b). Next icon that allows the user search for the current query within the destination We show destinations as a separate list, rather result rank, since they may topically deviate from the original query (e.g., those focusing on related the original query terms).\nDestination is analogous to . The only difference between the two systems is points for queries used in computing top directs users to the domains others queries.\nIn contrast, directs users to the domains other users visit at that follows the active or similar . This downgrades the effect of multiple query iterations are where users end up after submitting all queries), rather than directing searchers to potentially irrelevant domains that popular destinations. attempt to answer the following research questions:\nQuerySuggestion, destinations only shown if their popularity exceeds a frequency threshold.\nRQ1: Are popular destinations preferable and more effective than query refinement suggestions and unaided Web search for: a. Searches that are well-defined (\"known-item\" tasks)? b. Searches that are ill-defined (\"exploratory\" tasks)?\nRQ2: Should popular destinations be taken from the end of query trails or the end of session trails?", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Subjects", "text": "36 subjects (26 males and 10 females) participated in our study. They were recruited through an email announcement within our organization where they hold a range of positions in different divisions. The average age of subjects was 34.9 years (max=62, min=27, SD=6.2). All are familiar with Web search, and conduct 7.5 searches per day on average (SD=4.1). Thirty-one subjects (86.1%) reported general awareness of the query refinements offered by commercial Web search engines.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Tasks", "text": "Since the search task may influence information-seeking behavior [4], we made task type an independent variable in the study. We constructed six known-item tasks and six open-ended, exploratory tasks that were rotated between systems and subjects as described in the next section. Figure 3 shows examples of the two task types.", "publication_ref": ["b3"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Known-item task", "text": "Identify three tropical storms (hurricanes and typhoons) that have caused property damage and/or loss of life.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Exploratory task", "text": "You are considering purchasing a Voice Over Internet Protocol (VoIP) telephone. You want to learn more about VoIP technology and providers that offer the service, and select the provider and telephone that best suits you. Exploratory tasks were phrased as simulated work task situations [5], i.e., short search scenarios that were designed to reflect real-life information needs. These tasks generally required subjects to gather background information on a topic or gather sufficient information to make an informed decision. The known-item search tasks required search for particular items of information (e.g., activities, discoveries, names) for which the target was welldefined. A similar task classification has been used successfully in previous work [21]. Tasks were taken and adapted from the Text Retrieval Conference (TREC) Interactive Track [7], and questions posed on question-answering communities (Yahoo! Answers, Google Answers, and Windows Live QnA). To motivate the subjects during their searches, we allowed them to select two known-item and two exploratory tasks at the beginning of the experiment from the six possibilities for each category, before seeing any of the systems or having the study described to them.\nPrior to the experiment all tasks were pilot tested with a small number of different subjects to help ensure that they were comparable in difficulty and \"selectability\" (i.e., the likelihood that a task would be chosen given the alternatives). Post-hoc analysis of the distribution of tasks selected by subjects during the full study showed no preference for any task in either category.", "publication_ref": ["b4", "b20", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Design and Methodology", "text": "The study used a within-subjects experimental design. System had four levels (corresponding to the four experimental systems) and search tasks had two levels (corresponding to the two task types).\nSystem and task-type order were counterbalanced according to a Graeco-Latin square design.\nSubjects were tested independently and each experimental session lasted for up to one hour. We adhered to the following procedure: 1. Upon arrival, subjects were asked to select two known-item and two exploratory tasks from the six tasks of each type. 2. Subjects were given an overview of the study in written form that was read aloud to them by the experimenter. 3. Subjects completed a demographic questionnaire focusing on aspects of search experience. 4. For each of the four interface conditions: a. Subjects were given an explanation of interface functionality lasting around 2 minutes. b. Subjects were instructed to attempt the task on the assigned system searching the Web, and were allotted up to 10 minutes to do so. c. Upon completion of the task, subjects were asked to complete a post-search questionnaire. 5. After completing the tasks on the four systems, subjects answered a final questionnaire comparing their experiences on the systems. 6. Subjects were thanked and compensated.\nIn the next section we present the findings of this study.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "FINDINGS", "text": "In this section we use the data derived from the experiment to address our hypotheses about query suggestions and destinations, providing information on the effect of task type and topic familiarity where appropriate. Parametric statistical testing is used in this analysis and the level of significance is set to , unless otherwise stated. All Likert scales and semantic differentials used a 5-point scale where a rating closer to one signifies more agreement with the attitude statement.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Subject Perceptions", "text": "In this section we present findings on how subjects perceived the systems that they used. Responses to post-search (per-system) and final questionnaires are used as the basis for our analysis.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Search Process", "text": "To address the first research question wanted insight into subjects' perceptions of the search experience on each of the four systems. In the post-search questionnaires, we asked subjects to complete four 5-point semantic differentials indicating their responses to the attitude statement: \"The search we asked you to perform was\". The paired stimuli offered as responses were: \"relaxing\"/\"stressful\", \"interesting\"/ \"boring\", \"restful\"/\"tiring\", and \"easy\"/\"difficult\". The average obtained differential values are shown in Table 1 for each system and each task type. The value corresponding to the differential \"All\" represents the mean of all three differentials, providing an overall measure of subjects' feelings. Each cell in Table 1 summarizes subject responses for 18 tasksystem pairs (18 subjects who ran a known-item task on Baseline (B), 18 subjects who ran an exploratory task on QuerySuggestion (QS), etc.). The most positive response across all systems for each differential-task pair is shown in bold. We applied two-way analysis of variance (ANOVA) to each differential across all four systems and two task types. Subjects found the search easier on QuerySuggestion and QueryDestination than the other systems for known-item tasks. 4 For exploratory tasks, only searches conducted on QueryDestination were easier than on the other systems. 5 Subjects indicated that exploratory tasks on the three non-baseline systems were more stressful (i.e., less \"relaxing\") than the knownitem tasks. 6 As we will discuss in more detail in Section 4.1.3, subjects regarded the familiarity of Baseline as a strength, and may have struggled to attempt a more complex task while learning a new interface feature such as query or destination suggestions.", "publication_ref": ["b3", "b4", "b5"], "figure_ref": [], "table_ref": ["tab_0", "tab_0"]}, {"heading": "Interface Support", "text": "We solicited subjects' opinions on the search support offered by QuerySuggestion, QueryDestination, and SessionDestination. The following Likert scales and semantic differentials were used:\n\u2022 Likert scale A: \"Using this system enhances my effectiveness in finding relevant information.\" (Effectiveness) 7 \u2022 Likert scale B: \"The queries/destinations suggested helped me get closer to my information goal.\" (CloseToGoal) \u2022 Likert scale C: \"I would re-use the queries/destinations suggested if I encountered a similar task in the future\" (Re-use) \u2022 Semantic differential A: \"The queries/destinations suggested by the system were: \"relevant\"/\"irrelevant\", \"useful\"/\"useless\", \"appropriate\"/\"inappropriate\".\nWe did not include these in the post-search questionnaire when subjects used the Baseline system as they refer to interface support options that Baseline did not offer. Table 2 presents the average responses for each of these scales and differentials, using the labels after each of the first three Likert scales in the bulleted list above. The values for the three semantic differentials are included at the bottom of the table, as is their overall average under \"All\". The results show that all three experimental systems improved subjects' perceptions of their search effectiveness over Baseline, although only QueryDestination did so significantly. 8 Further examination of the effect size (measured using Cohen's d) revealed that QueryDestination affects search effectiveness most positively. 9 QueryDestination also appears to get subjects closer to their information goal (CloseToGoal) than QuerySuggestion or SessionDestination, although only for exploratory search tasks. 10 Additional comments on QuerySuggestion conveyed that subjects saw it as a convenience (to save them typing a reformulation) rather than a way to dramatically influence the outcome of their search. For exploratory searches, users benefited more from being pointed to alternative information sources than from suggestions for iterative refinements of their queries. Our findings also show that our subjects felt that QueryDestination produced more \"relevant\" and \"useful\" suggestions for exploratory tasks than the other systems. 11 All other observed differences between the systems were not statistically significant. 12 The difference between performance of QueryDestination and SessionDestination is explained by the approach used to generate destinations (described in Section 2). SessionDestination's recommendations came from the end of users' session trails that often transcend multiple queries. This increases the likelihood that topic shifts adversely affect their relevance.", "publication_ref": ["b7", "b8", "b9", "b10", "b11"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "System Ranking", "text": "In the final questionnaire that followed completion of all tasks on all systems, subjects were asked to rank the four systems in descending order based on their preferences. Table 3 presents the mean average rank assigned to each of the systems. These results indicate that subjects preferred QuerySuggestion and QueryDestination overall. However, none of the differences between systems' ratings are significant. 13 One possible explanation for these systems being rated higher could be that although the popular destination systems performed well for exploratory searches while QuerySuggestion performed well for known-item searches, an overall ranking merges these two performances. This relative ranking reflects subjects' overall perceptions, but does not separate them for each task category. Over all tasks there appeared to be a slight preference for QueryDestination, but as other results show, the effect of task type on subjects' perceptions is significant.\nThe final questionnaire also included open-ended questions that asked subjects to explain their system ranking, and describe what they liked and disliked about each system:\nBaseline: Subjects who preferred Baseline commented on the familiarity of the system (e.g., \"was familiar and I didn't end up using suggestions\" (S36)). Those who did not prefer this system disliked the lack of support for query formulation (\"Can be difficult if you don't pick good search terms\" (S20)) and difficulty locating relevant documents (e.g., \"Difficult to find what I was looking for\" (S13); \"Clunky current technology\" (S30)). QuerySuggestion:\nSubjects who rated QuerySuggestion highest commented on rapid support for query formulation (e.g., \"was useful in (1) saving typing (2) coming up with new ideas for query expansion\" (S12); \"helps me better phrase the search term\" (S24); \"made my next query easier\" (S21)). Those who did not prefer this system criticized suggestion quality (e.g., \"Not relevant\" (S11); \"Popular queries weren't what I was looking for\" (S18)) and the quality of results they led to (e.g., \"Results (after clicking on suggestions) were of low quality\" (S35); \"Ultimately unhelpful\" (S1)). QueryDestination:\nSubjects who preferred this system commented mainly on support for accessing new information sources (e.g., \"provided potentially helpful and new areas / domains to look at\" (S27)) and bypassing the need to browse to these pages (\"Useful to try to 'cut to the chase' and go where others may have found answers to the topic\" (S3)). Those who did not prefer this system commented on the lack of specificity in the suggested domains (\"Should just link to site-specific query, not site itself\" (S16); \"Sites were not very specific\" (S24); \"Too general/vague\" (S28) 14 ), and the quality of the suggestions (\"Not relevant\" (S11); \"Irrelevant\" (S6)).", "publication_ref": ["b12", "b13"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "SessionDestination:", "text": "Subjects who preferred this system commented on the utility of the suggested domains (\"suggestions make an awful lot of sense in providing search assistance, and seemed to help very nicely\" (S5)). However, more subjects commented on the irrelevance of the suggestions (e.g., \"did not seem reliable, not much help\" (S30); \"Irrelevant, not my style\" (S21), and the related need to include explanations about why the suggestions were offered (e.g., \"Low-quality results, not enough information presented\" (S35)).\nThese comments demonstrate a diverse range of perspectives on different aspects of the experimental systems. Work is obviously needed in improving the quality of the suggestions in all systems, but subjects seemed to distinguish the settings when each of these systems may be useful. Even though all systems can at times offer irrelevant suggestions, subjects appeared to prefer having them rather than not (e.g., one subject remarked \"suggestions were helpful in some cases and harmless in all\" (S15)).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Summary", "text": "The findings obtained from our study on subjects' perceptions of the four systems indicate that subjects tend to prefer QueryDestination for the exploratory tasks and QuerySuggestion for the known-item searches. Suggestions to incrementally refine the current query may be preferred by searchers on known-item tasks when they may have just missed their information target. However, when the task is more demanding, searchers appreciate suggestions that have the potential to dramatically influence the direction of a search or greatly improve topic coverage.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Search Tasks", "text": "To gain a better understanding of how subjects performed during the study, we analyze data captured on their perceptions of task completeness and the time that it took them to complete each task.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Subject Perceptions", "text": "In the post-search questionnaire, subjects were asked to indicate on a 5-point Likert scale the extent to which they agreed with the following attitude statement: \"I believe I have succeeded in my performance of this task\" (Success). In addition, they were asked to complete three 5-point semantic differentials indicating their response to the attitude statement: \"The task we asked you to perform was:\" The paired stimuli offered as possible responses were \"clear\"/\"unclear\", \"simple\"/\"complex\", and \"familiar\"/ \"unfamiliar\". Table 4 presents the mean average response to these statements for each system and task type. Subject responses demonstrate that users felt that their searches had been more successful using QueryDestination for exploratory tasks than with the other three systems (i.e., there was a two-way interaction between these two variables). 15 In addition, subjects perceived a significantly greater sense of completion with knownitem tasks than with exploratory tasks. 16 Subjects also found known-item tasks to be more \"simple\", \"clear\", and \"familiar\". 17 These responses confirm differences in the nature of the tasks we had envisaged when planning the study. As illustrated by the examples in Figure 3, the known-item tasks required subjects to retrieve a finite set of answers (e.g., \"find three interesting things to do during a weekend visit to Kyoto, Japan\"). In contrast, the exploratory tasks were multi-faceted, and required subjects to find out more about a topic or to find sufficient information to make a decision. The end-point in such tasks was less well-defined and may have affected subjects' perceptions of when they had completed the task. Given that there was no difference in the tasks attempted on each system, theoretically the perception of the tasks' simplicity, clarity, and familiarity should have been the same for all systems. However, we observe a clear interaction effect between the system and subjects' perception of the actual tasks.", "publication_ref": ["b14", "b15", "b16"], "figure_ref": ["fig_2"], "table_ref": ["tab_4"]}, {"heading": "Task Completion Time", "text": "In addition to asking subjects to indicate the extent to which they felt the task was completed, we also monitored the time that it took them to indicate to the experimenter that they had finished. The elapsed time from when the subject began issuing their first query until when they indicated that they were done was monitored using a stopwatch and recorded for later analysis. A stopwatch rather than system logging was used for this since we wanted to record the time regardless of system interactions. Figure 4 shows the average task completion time for each system and each task type. As can be seen in the figure above, the task completion times for the known-item tasks differ greatly between systems. 18 Subjects attempting these tasks on QueryDestination and QuerySuggestion complete them in less time than subjects on Baseline and SessionDestination. 19 As discussed in the previous section, subjects were more familiar with the known-item tasks, and felt they were simpler and clearer. Baseline may have taken longer than the other systems since users had no additional support and had to formulate their own queries. Subjects generally felt that the recommendations offered by SessionDestination were of low relevance and usefulness. Consequently, the completion time increased slightly between these two systems perhaps as the subjects assessed the value of the proposed suggestions, but reaped little benefit from them. The task completion times for the exploratory tasks were approximately equal on all four systems 20 , although the time on Baseline was slightly higher. Since these tasks had no clearly defined termination criteria (i.e., the subject decided when they had gathered sufficient information), subjects generally spent longer searching, and consulted a broader range of information sources than in the known-item tasks.", "publication_ref": ["b17", "b18", "b19"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Summary", "text": "Analysis of subjects' perception of the search tasks and aspects of task completion shows that the QuerySuggestion system made subjects feel more successful (and the task more \"simple\", \"clear\", and \"familiar\") for the known-item tasks. On the other hand, QueryDestination was shown to lead to heightened perceptions of search success and task ease, clarity, and familiarity for the exploratory tasks. Task completion times on both systems were significantly lower than on the other systems for known-item tasks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Subject Interaction", "text": "We now focus our analysis on the observed interactions between searchers and systems. As well as eliciting feedback on each system from our subjects, we also recorded several aspects of their interaction with each system in log files. In this section, we analyze three interaction aspects: query iterations, search-result clicks, and subject engagement with the additional interface features offered by the three non-baseline systems.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Queries and Result Clicks", "text": "Searchers typically interact with search systems by submitting queries and clicking on search results. Although our system offers additional interface affordances, we begin this section by analyzing querying and clickthrough behavior of our subjects to better understand how they conducted core search activities. Table 5 shows the average number of query iterations and search results clicked for each system-task pair. The average value in each cell is computed for 18 subjects on each task type and system. discussed in the previous section, subjects using this system felt more successful in their searches yet they exhibited less of the traditional query and result-click interactions required for search success on traditional search systems. It may be the case that subjects' queries on this system were more effective, but it is more likely that they interacted less with the system through these means and elected to use the popular destinations instead. Overall, subjects submitted most queries in QuerySuggestion, which is not surprising as this system actively encourages searchers to iteratively re-submit refined queries. Subjects interacted similarly with Baseline and SessionDestination systems, perhaps due to the low quality of the popular destinations in the latter. To investigate this and related issues, we will next analyze usage of the suggestions on the three non-baseline systems.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Suggestion Usage", "text": "To determine whether subjects found additional features useful, we measure the extent to which they were used when they were provided. Suggestion usage is defined as the proportion of submitted queries for which suggestions were offered and at least one suggestion was clicked. Table 6 shows the average usage for each system and task category. Results indicate that QuerySuggestion was used more for knownitem tasks than SessionDestination 22 , and QueryDestination was used more than all other systems for the exploratory tasks. 23 For well-specified targets in known-item search, subjects appeared to use query refinement most heavily. In contrast, when subjects were exploring, they seemed to benefit most from the recommendation of additional information sources. Subjects selected almost twice as many destinations per query when using QueryDestination compared to SessionDestination. 24 As discussed earlier, this may be explained by the lower perceived relevance and usefulness of destinations recommended by SessionDestination.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "Summary", "text": "Analysis of log interaction data gathered during the study indicates that although subjects submitted fewer queries and clicked fewer search results on QueryDestination, their engagement with suggestions was highest on this system, particularly for exploratory search tasks. The refined queries proposed by QuerySuggestion were used the most for the known-item tasks. There appears to be a clear division between the systems: QuerySuggestion was preferred for known-item tasks, while QueryDestination provided most-used support for exploratory tasks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "DISCUSSION AND IMPLICATIONS", "text": "The promising findings of our study suggest that systems offering popular destinations lead to more successful and efficient searching compared to query suggestion and unaided Web search.\nSubjects seemed to prefer QuerySuggestion for the known-item tasks where the information-seeking goal was well-defined. If the initial query does not retrieve relevant information, then subjects appreciate support in deciding what refinements to make to the query. From examination of the queries that subjects entered for the known-item searches across all systems, they appeared to use the initial query as a starting point, and add or subtract individual terms depending on search results. The post-search questionnaire asked subjects to select from a list of proposed explanations (or offer their own explanations) as to why they used recommended query refinements. For both known-item tasks and the exploratory tasks, around 40% of subjects indicated that they selected a query suggestion because they \"wanted to save time typing a query\", while less than 10% of subjects did so because the suggestions \"represented new ideas\". Thus, subjects seemed to view QuerySuggestion as a time-saving convenience, rather than a way to dramatically impact search effectiveness.\nThe two variants of recommending destinations that we considered, QueryDestination and SessionDestination, offered suggestions that differed in their temporal proximity to the current query. The quality of the destinations appeared to affect subjects' perceptions of them and their task performance. As discussed earlier, domains residing at the end of a complete search session (as in SessionDestination) are more likely to be unrelated to the current query, and thus are less likely to constitute valuable suggestions. Destination systems, in particular QueryDestination, performed best for the exploratory search tasks, where subjects may have benefited from exposure to additional information sources whose topical relevance to the search query is indirect. As with QuerySuggestion, subjects were asked to offer explanations for why they selected destinations. Over both task types they suggested that destinations were clicked because they \"grabbed their attention\" (40%), \"represented new ideas\" (25%), or users \"couldn't find what they were looking for\" (20%). The least popular responses were \"wanted to save time typing the address\" (7%) and \"the destination was popular\" (3%). The positive response to destination suggestions from the study subjects provides interesting directions for design refinements. We were surprised to learn that subjects did not find the popularity bars useful, or hardly used the within-site search functionality, inviting re-design of these components. Subjects also remarked that they would like to see query-based summaries for each suggested destination to support more informed selection, as well as categorization of destinations with capability of drill-down for each category. Since QuerySuggestion and QueryDestination perform well in distinct task scenarios, integrating both in a single system is an interesting future direction. We hope to deploy some of these ideas on Web scale in future systems, which will allow log-based evaluation across large user pools.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSIONS", "text": "We presented a novel approach for enhancing users' Web search interaction by providing links to websites frequently visited by past searchers with similar information needs. A user study was conducted in which we evaluated the effectiveness of the proposed technique compared with a query refinement system and unaided Web search. Results of our study revealed that: (i) systems suggesting query refinements were preferred for known-item tasks, (ii) systems offering popular destinations were preferred for exploratory search tasks, and (iii) destinations should be mined from the end of query trails, not session trails. Overall, popular destination suggestions strategically influenced searches in a way not achievable by query suggestion approaches by offering a new way to resolve information problems, and enhance the informationseeking experience for many Web searchers.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Improving Web search ranking by incorporating user behavior information", "journal": "", "year": "2006", "authors": "E Agichtein; E Brill; S Dumais"}, {"ref_id": "b1", "title": "Adaptive Web navigation for wireless devices", "journal": "", "year": "2001", "authors": "C Anderson"}, {"ref_id": "b2", "title": "Using terminological feedback for Web search refinement: A log-based study", "journal": "", "year": "2003", "authors": "P Anick"}, {"ref_id": "b3", "title": "Experiments with interfaces to support query expansion", "journal": "J. Doc", "year": "1997", "authors": "M Beaulieu"}, {"ref_id": "b4", "title": "Experimental components for the evaluation of interactive information retrieval systems", "journal": "J. Doc", "year": "2000", "authors": "P Borlund"}, {"ref_id": "b5", "title": "Models of searching and browsing: languages, studies and applications", "journal": "", "year": "2007", "authors": " Downey"}, {"ref_id": "b6", "title": "The TREC interactive tracks: putting the user into search", "journal": "MIT Press", "year": "2005", "authors": "S T Dumais; N J Belkin"}, {"ref_id": "b7", "title": "Experience with an adaptive indexing scheme", "journal": "", "year": "1985", "authors": "G W Furnas"}, {"ref_id": "b8", "title": "FERRET: Interactive questionanswering for real-world environments", "journal": "", "year": "2006", "authors": "A Hickl"}, {"ref_id": "b9", "title": "Generating query substitutions", "journal": "", "year": "2006", "authors": "R Jones"}, {"ref_id": "b10", "title": "A case for interaction: a study of interactive information retrieval behavior and effectiveness", "journal": "", "year": "1996", "authors": "J Koenemann; N Belkin"}, {"ref_id": "b11", "title": "Orienteering in an information landscape: how information seekers get from here to there", "journal": "", "year": "1993", "authors": "V O'day; R Jeffries"}, {"ref_id": "b12", "title": "Query chains: Learning to rank from implicit feedback", "journal": "", "year": "2005", "authors": "F Radlinski; T Joachims"}, {"ref_id": "b13", "title": "Term-weighting approaches in automatic text retrieval", "journal": "Inf. Proc. Manage", "year": "1988", "authors": "G Salton; C Buckley"}, {"ref_id": "b14", "title": "Analysis of a very large Web search engine query log", "journal": "SIGIR Forum", "year": "1999", "authors": "C Silverstein"}, {"ref_id": "b15", "title": "Exploiting query repetition and regularity in an adaptive community-based Web search engine", "journal": "User Mod. User Adapt. Int", "year": "2004", "authors": "B Smyth"}, {"ref_id": "b16", "title": "U.S. versus European Web searching trends", "journal": "SIGIR Forum", "year": "2002", "authors": "A Spink"}, {"ref_id": "b17", "title": "Multitasking during Web search sessions", "journal": "Inf. Proc. Manage", "year": "2006", "authors": "A Spink"}, {"ref_id": "b18", "title": "Footprints: history-rich tools for information foraging", "journal": "", "year": "1999", "authors": "A Wexelblat; P Maes"}, {"ref_id": "b19", "title": "Investigating behavioral variability in Web search", "journal": "", "year": "2007", "authors": "R W White; S M Drucker"}, {"ref_id": "b20", "title": "Examining the effectiveness of real-time query expansion", "journal": "Inf. Proc. Manage", "year": "2007", "authors": "R W White; G Marchionini"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. Query suggestion presentation in", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 .2Figure 2. Destination presentation in QueryTo keep the interface uncluttered, the page title of is shown on hover over the page URL (shown in Figure2to the destination name, there is a clickable icon that allows the user to execute a search for the current query within the domain displayed. We show destinations as a separate l than increasing their search result rank, since they may deviate from the original query (e.g., those focus topics or not containing the original query terms)", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 .3Figure 3. Examples of known-item and exploratory tasks.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 .4Figure 4. Mean average task completion time (\u00b1 \u00b1 \u00b1 \u00b1 SEM).15 F(3,136) = 6.34, p = .00116 F(1,136) = 18.95, p < .00117 F(1,136) = 6.82, p = .028; Known-item tasks were also more \"simple\" on QS (F(3,136) = 3.93, p = .01; Tukey post-hoc test: p = .01); \u03b1 = .167", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "22 F( 2 ,2355) = 4.67, p = .01; Tukey post-hoc tests: p = .006 23 Tukey's post-hoc tests: all p \u2264 .027 24 QD: MK = 1.8, ME = 2.1; SD: MK = 1.1, ME = 1.2; F(1,231) = 5.49, p = .02; Tukey post-hoc tests: all p \u2264 .003; (M represents mean average).", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "", "figure_data": "MeasureQuery trailsSession trailsNumber of unique domains2.04.3Total page viewsAll domains Domains 1 to (n -1) Domain n (destination)4.8 1.4 3.416.2 10.1 6.2Total time spent (secs)All domains Domains 1 to (n -1) Domain n (destination)172.6 70.4 102.3621.8 397.6 224.1"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": "DifferentialBKnown-item QS QDSDBExploratory QS QDSDEasy2.61.61.72.32.52.61.92.9Restful2.82.32.42.62.82.82.42.8Interesting2.42.21.72.22.21.81.82Relaxing2.61.922.22.52.82.32.9All2.621.92.32.52.52.12.7"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": "Scale /Known-itemExploratoryDifferentialQSQDSDQSQDSDEffectiveness2.72.52.62.82.32.8CloseToGoal2.92.72.82.72.23.1Re-use2.932.42.52.53.21 Relevant2.62.52.82.423.12 Useful2.62.72.82.72.13.13 Appropriate2.62.42.52.42.42.6All {1,2,3}2.62.62.62.62.32.9"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": "SystemsBaselineQSuggestQDestSDestRanking2.472.141.922.31"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": "ScaleBKnown-item QS QDSDBExploratory QS QDSDSuccess2.01.31.41.42.82.31.42.61 Clear1.21.11.11.11.61.51.51.62 Simple1.91.41.81.82.42.92.433 Familiar2.21.92.02.22.62.52.72.7All {1,2,3}1.81.41.61.82.22.22.22.3"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Subjects submitted fewer queries and clicked on fewer search results in QueryDestination than in any of the other systems.21 As", "figure_data": "ScaleBKnown-item QS QDSDBExploratory QS QDSDQueries1.94.21.52.43.15.72.73.5Result clicks2.621.72.43.44.32.35.118 F(3,136) = 4.56, p = .004 19 Tukey post-hoc tests: all p \u2264 .021 20 F(3,136) = 1.06, p = .37"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "", "figure_data": "MeasureQSKnown-item QDSDQSExploratory QDSDUsage35.733.523.430.035.225.3"}], "formulas": [], "doi": ""}