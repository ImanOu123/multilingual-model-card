{"title": "DynamicFusion: Reconstruction and Tracking of Non-rigid Scenes in Real-Time", "authors": "Richard A Newcombe; Dieter Fox; Steven M Seitz", "pub_date": "", "abstract": "Real-time reconstructions of a moving scene with DynamicFusion; both the person and the camera are moving. The initially noisy and incomplete model is progressively denoised and completed over time (left to right).", "sections": [{"heading": "", "text": "3D scanning traditionally involves separate capture and off-line processing phases, requiring very careful planning of the capture to make sure that every surface is covered. In practice, it's very difficult to avoid holes, requiring several iterations of capture, reconstruction, identifying holes, and recapturing missing regions to ensure a complete model. Real-time 3D reconstruction systems like KinectFusion [18,10] represent a major advance, by providing users the ability to instantly see the reconstruction and identify regions that remain to be scanned. KinectFusion spurred a flurry of follow up research aimed at robustifying the tracking [9,32] and expanding its spatial mapping capabilities to larger environments [22,19,34,31,9].\nHowever, as with all traditional SLAM and dense reconstruction systems, the most basic assumption behind KinectFusion is that the observed scene is largely static. The core question we tackle in this paper is: How can we generalise KinectFusion to reconstruct and track dynamic, non-rigid scenes in real-time? To that end, we introduce DynamicFusion, an approach based on solving for a volumetric flow field that transforms the state of the scene at each time instant into a fixed, canonical frame. In the case of a moving person, for example, this transformation undoes the person's motion, warping each body configuration into the pose of the first frame. Following these warps, the scene is effectively rigid, and standard KinectFusion updates can be used to obtain a high quality, denoised reconstruction. This progressively denoised reconstruction can then be transformed back into the live frame using the inverse map; each point in the canonical frame is transformed to its location in the live frame (see Figure 1). Defining a canonical \"rigid\" space for a dynamically moving scene is not straightforward. A key contribution of our work is an approach for non-rigid transformation and fusion that retains the optimality properties of volumetric scan fusion [5], developed originally for rigid scenes. The main insight is that undoing the scene motion to enable fusion of all observations into a single fixed frame can be achieved efficiently by computing the inverse map alone. Under this transformation, each canonical point projects along a line of sight in the live camera frame. Since the optimality arguments of [5] (developed for rigid scenes) depend only on lines of sight, we can generalize their optimality results to the non-rigid case.\nOur second key contribution is to represent this volumetric warp efficiently, and compute it in real time. Indeed, even a relatively low resolution, 256 3 deformation volume would require 100 million transformation variables to be computed at frame-rate. Our solution depends on a combination of adaptive, sparse, hierarchical volumetric basis functions, and innovative algorithmic work to ensure a real-time solution on commodity hardware. As a result, Dynam-icFusion is the first system capable of real-time dense reconstruction in dynamic scenes using a single depth camera.\nThe remainder of this paper is structured as follows. After discussing related work, we present an overview of Dy-namicFusion in Section 2 and provide technical details in Section 3. We provide experimental results in Section 4 and conclude in Section 5.", "publication_ref": ["b17", "b9", "b8", "b32", "b21", "b18", "b34", "b30", "b8", "b4", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "While no prior work achieves real-time, template-free, non-rigid reconstruction, there are two categories of closely related work: 1) real-time non-rigid tracking algorithms, and 2) offline dynamic reconstruction techniques.\nReal-time non-rigid template tracking. The vast majority of non-rigid tracking research focuses on human body parts, for which specialised shape and motion templates are learnt or manually designed. The best of these demonstrate high accuracy, real-time performance capture for tracking faces [16,3], hands [21,20], complete bodies [27], or general articulated objects [23,33].\nOther techniques directly track and deform more general mesh models. [12] demonstrated the ability to track a statically acquired low resolution shape template and upgrade its appearance with high frequency geometric details not present in the original model. Recently, [37] demonstrated an impressive real-time version of a similar technique, using GPU accelerated optimisations. In that system, a dense surface model of the subject is captured while remaining static, yielding a template for use in their realtime tracking pipeline. This separation into template generation and tracking limits the system to objects and scenes that are completely static during the geometric reconstruction phase, precluding reconstruction of things that won't reliably hold still (e.g., children or pets).\nOffline simultaneous tracking and reconstruction of dynamic scenes. There is a growing literature on offline non-rigid tracking and reconstruction techniques. Several researchers have extended ICP to enable small non-rigid deformations, e.g., [1,2]. Practical advancements to pairwise 3D shape and scan alignment over larger deformations make use of reduced deformable model parametrisations [14,4]. In particular, embedded deformation graphs [25] use a sparsely sampled set of transformation basis functions that can be efficiently and densely interpolated over space. Quasi-rigid reconstruction has also been demonstrated [15,35] and hybrid systems, making use of a known kinematic structure (e.g., a human body), are able to perform non-rigid shape denoising [36]. Other work combines non-rigid mesh template tracking and temporal denoising and completion [13], but does not obtain a single consistent representation of the scene.\nMore closely related to our work are template-free tech-\nniques. An intriguing approach to template-free non-rigid alignment, introduced in [17] and [26], treats each nonrigid scan as a view from a 4D geometric observation and performs 4D shape reconstruction. [30,29] reconstruct a fixed topology geometry by performing pair-wise scan alignment. [24] use a space-time solid incompressible flow prior that results in water tight reconstructions and is effective against noisy input point-cloud data. [28] introduce animation cartography that also estimates shape and a per frame deformation by developing a dense correspondence matching scheme that is seeded with sparse landmark matches. Recent work using multiple fixed kinect cameras [8] [7] demonstrates larger scale non-rigid reconstruction by densely tracking and fusing all depth map data into a novel directional distance function representation. All of these techniques require three to four orders of magnitude more time than is available within a real-time setting.", "publication_ref": ["b15", "b2", "b20", "b19", "b26", "b22", "b33", "b11", "b37", "b0", "b1", "b13", "b3", "b24", "b14", "b35", "b36", "b12", "b16", "b25", "b29", "b28", "b23", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "DynamicFusion Overview", "text": "DynamicFusion decomposes a non-rigidly deforming scene into a latent geometric surface, reconstructed into a rigid canonical space S \u2286 R 3 ; and a per frame volumetric warp field that transforms that surface into the live frame. There are three core algorithmic components to the system that are performed in sequence on arrival of each new depth frame:\n1. Estimation of the volumetric model-to-frame warp field parameters (Section 3.3) 2. Fusion of the live frame depth map into the canonical space via the estimated warp field (Section 3.2)\n3. Adaptation of the warp-field structure to capture newly added geometry (Section 3.4)\nFigure 2 provides an overview.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Technical Details", "text": "We will now describe the components of DynamicFusion in detail. First, we describe our dense volumetric warp-field parametrisation. This allows us to model per-frame deformations in the scene. The warp-field is the key extension over static state space representations used in traditional reconstruction and SLAM systems, and its estimation is the enabler of both non-rigid tracking and scene reconstruction.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dense Non-rigid Warp Field", "text": "We represent dynamic scene motion through a volumetric warp-field, providing a per point 6D transformation W : S \u2192 SE(3). Whereas a dense 3D translation field would be sufficient to describe time varying geometry, we have found that representing the real-world transformation   d,e). To achieve this, we estimate a volumetric warp (motion) field that transforms the canonical model space into the live frame, enabling the scene motion to be undone, and all depth maps to be densely fused into a single rigid TSDF reconstruction (d,f). Simultaneously, the structure of the warp field is constructed as a set of sparse 6D transformation nodes that are smoothly interpolated through a k-nearest node average in the canonical frame (c). The resulting per-frame warp field estimate enables the progressively denoised and completed scene geometry to be transformed into the live frame in real-time (e). In (e) we also visualise motion trails for a sub-sample of model vertices over the last 1 second of scene motion together with a coordinate frame showing the rigid body component of the scene motion. In (c) we render the nearest node to model surface distance where increased distance is mapped to a lighter value.\nof objects with both translation and rotation results in significantly better tracking and reconstruction. For each canonical point v c \u2208 S, T lc = W(v c ) transforms that point from canonical space into the live, non-rigidly deformed frame of reference.\nSince we will need to estimate the warp function for each new frame, W t , its representation must be efficiently optimisable. One possibility is to densely sample the volume, e.g. representing a quantised SE(3) field at the resolution of the truncated signed distance (TSDF) geometric representation. However, a typical TSDF volume reconstruction at a relatively low resolution of 256 3 voxels would require the solution of 6 \u00d7 256 3 parameters per frame, about 10 million times more than in the original KinectFusion algorithm, which only estimates a single rigid transformation. Clearly, a completely dense parametrisation of the warp function is infeasible. In reality, surfaces tend to move smoothly in space, and so we can instead use a sparse set of transformations as bases and define the dense volumetric warp function through interpolation. Due to its computational efficiency and high quality interpolation capability we use dual-quaternion blending DQB [11], to define our warp function:\nW(x c ) \u2261 SE3(DQB(x c )) ,(1)\nwhere the weighted average over unit dual quaternion trans-\nformations is simply DQB(x c ) \u2261 k\u2208N (xc ) w k (xc)q kc k\u2208N (xc ) w k (xc)q kc ,\nwith each unit dual-quaternionq kc \u2208 R 8 . Here, N (x) are the k-nearest transformation nodes to the point x and w k : R 3 \u2192 R defines a weight that alters the radius of influence of each node and SE3(.) converts from quaternions back to an SE(3) transformation matrix. The state of the warp-field W t at time t is defined by the values of a set of n deformation nodes N t warp = {dg v , dg w , dg se3 } t . Each of the i = 1..n nodes has a position in the canonical frame dg i v \u2208 R 3 , its associated transformation T ic = dg i se3 , and a radial basis weight dg w that controls the extent of the transformation\nw i (x c ) = exp \u2212 dg i v \u2212 x c 2 / 2(dg i w ) 2\n. Each radius parameter dg i w is set to ensure the node's influence overlaps with neighbouring nodes, dependent on the sampling sparsity of nodes, which we describe in detail in section (3.4). Since the warp function defines a rigid body transformation for all supported space, both position and any associated orientation of space is transformed, e.g., the vertex v c from a surface with orientation or normal n c is transformed into the live frame as (v t , 1) = W t (v c )(v c , 1) and (n t , 0) = W t (v c )(n c , 0) . We note that scaling of space can also be represented with this warp function, since compression and expansion of space are represented by neighbouring points moving in converging and diverging directions. Finally, we note that we can factor out any rigid body transformation common to all points in the volume, e.g., due to camera motion. We therefore introduce the explicit warped model to live camera transform, T lw , and compose this onto the volumetric warp function; our complete warp-field is then given as:\nW t (x c ) = T lw SE3(DQB(x c )).\n(2)", "publication_ref": ["b10"], "figure_ref": [], "table_ref": []}, {"heading": "Dense Non-Rigid Surface Fusion", "text": "We now describe how, given the model-to-frame warp field W t , we update our canonical model geometry. Our reconstruction into the canonical space S is represented by the sampled TSDF V : S \u2192 R 2 within a voxel domain S \u2282 N 3 . The sampled function holds for each voxel x \u2208 S corresponding to the sampled point x c , a tuple V(x) \u2192 [v(x) \u2208 R, w(x) \u2208 R] holding a weighted average of all projective TSDF values observed for that point so far v(x), together with the sum of all associated weights w(x).\nWe extend the projective TSDF fusion approach originally introduced by [6] to operate over non-rigidly deforming scenes. Given the live depth image D t , we transform each voxel center x c \u2208 S by its estimated warp into the live frame (x t , 1) = W t (x c )(x c , 1) , and carry through the TSDF surface fusion operation by directly projecting the warped center into the depth frame. This allows the TSDF for a point in the canonical frame to be updated by computing the projective TSDF in the deforming frame without having to resample a warped TSDF in the live frame. The projective signed distance at the warped canonical point is:\npsdf (x c ) = K \u22121 D t (u c ) u c , 1 z \u2212 [x t ] z ,(3)\nwhere u c = \u03c0 (Kx t ) is the pixel into which the voxel center projects. We compute distance along the optical (z) axis of the camera frame using the z component denoted [.] z . K is the known 3 \u00d7 3 camera intrinsic matrix, and \u03c0 performs perspective projection. For each voxel x, we update the TSDF to incorporate the projective SDF observed in the warped frame using TSDF fusion:\nV(x) t = [v (x), w (x)] , if psdf (dc(x)) > \u2212\u03c4 V(x) t\u22121 , otherwise(4)\nwhere dc(.) transforms a discrete voxel point into the continuous TSDF domain. The truncation distance \u03c4 > 0 and the updated TSDF value is given by the weighted averaging scheme [5], with the weight truncation introduced in [18]:\nv (x) = v(x) t\u22121 w(x) t\u22121 + min(\u03c1, \u03c4 )w(x) w(x) t\u22121 + w(x) \u03c1 = psdf (dc(x)) w (x) = min(w(x) t\u22121 + w(x), w max ) .(5)\nUnlike the static fusion scenario where the weight w(x) encodes the uncertainty of the depth value observed at the projected pixel in the depth frame, we also account for uncertainty associated with the warp function at x c . In the case of the single rigid transformation in original TSDF fusion, we are certain that observed surface regions, free space, and unobserved regions transform equivalently. In our non-rigid case, the further away the point x c is from an already mapped and observable surface region, the less certain we can be about its transformation. We use the average distance from x c to its k-nearest deformation nodes as a proxy for this increase in uncertainty and scale:\nw(x) \u221d 1 k i\u2208N (xc) dg i w \u2212 x c 2 .\nWe note that our non-rigid fusion generalises the static reconstruction case used in KinectFusion, replacing the single (rigid) model-to-camera transform with a per voxel warp that transforms the associated space into the live (non-rigid) frame (see Figure 3). This technique greatly simplifies the non-rigid reconstruction process over methods where all frames are explicitly warped into a canonical frame. Furthermore, given a correct warp field, then, since all TSDF updates are computed using distances in the camera frame, the non-rigid projective TSDF fusion approach maintains the optimality guarantees for surface reconstruction from noisy observations originally proved for the static reconstruction case in [6].", "publication_ref": ["b5", "b4", "b17", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Estimating the Warp-field State W t", "text": "We estimate the current values of the transformations dg se3 in W t given a newly observed depth map D t and the current reconstruction V by constructing an energy function that is minimised by our desired parameters:\nE(W t , V, D t , E) = Data(W t , V, D t ) + \u03bbReg(W t , E) . (6)\nOur data term consists of a dense model-to-frame ICP cost Data(W t , V, D t ) which is coupled with a regularisation term Reg(W t , E) that penalises non-smooth motion fields, and ensures as-rigid-as-possible deformation between transformation nodes connected by the edge set E. The coupling of a data-term formed from linearly blended transformations with a rigid-as-possible graph based regularisation is a form of the embedded deformation graph model introduced in [25]. The regularisation parameter \u03bb enables a trade-off between relaxing rigidity over the field when given high quality data, and ensuring a smooth consistent deformation of non or noisily observed regions of space. We defined these terms in the next subsections.", "publication_ref": ["b24"], "figure_ref": [], "table_ref": []}, {"heading": "Dense Non-Rigid ICP Data-term", "text": "Our aim is to estimate all non-rigid transformation parameters T ic and T lw that warp the canonical volume into the live frame. We achieve this by performing a dense nonrigid alignment of the current surface reconstruction, extracted from the canonical volume's zero level set, into the live frame's depth map.\nSurface Prediction and Data-Association: The current zero level set of the TSDF V is extracted by marching cubes and stored as a polygon mesh with point-normal pairs in the canonical frame:V c \u2261 {V c , N c }. We non-rigidly transform this mesh into the live frame using the current warp field W t resulting in the warped point-normalsV w .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Non-rigid scene deformation", "text": "Introducing an occlusion (a) Live frame t = 0 (b) Live Frame t = 1 (c) Canonical \u2192 Live (d) Live frame t = 0 (e) Live Frame t = 1 (f) Canonical \u2192 Live Figure 3: An illustration of how each point in the canonical frame maps, through a correct warp field, onto a ray in the live camera frame when observing a deforming scene. In (a) the first view of a dynamic scene is observed. In the corresponding canonical frame, the warp is initialized to the identity transform and the three rays shown in the live frame also map as straight lines in the canonical frame. As the scene deforms in the live frame (b), the warp function transforms each point from the canonical and into the corresponding live frame location, causing the corresponding rays to bend (c). Note that this warp can be achieved with two 6D deformation nodes (shown as circles), where the left node applies a clockwise twist. In (d) we show a new scene that includes a cube that is about to occlude the bar. In the live frame (e), as the cube occludes a portion of the bar, the points in the canonical frame (f) are warped to correctly pass through the cube.\nWe obtain an initial estimate for data-association (correspondence) between the model geometry and the live frame by rendering the warped surfaceV w into the live frame shaded with canonical frame vertex positions using a rasterizing rendering pipeline. This results in a prediction of the canonical frame's geometry that is currently predicted to be visible in the live frame: P(V c ). We store this prediction as a pair of images {v, n} : \u2126 \u2192 P(V c ), where \u2126 is the pixel domain of the predicted images, storing the rendered canonical frame vertices and normals.\nGiven optimal transformation parameters for the current time frame, the predicted-to-be-visible geometry should transform close, modulo observation noise, to the live surface vl : \u2126 \u2192 R 3 , formed by back projection of the depth image [vl(u) , 1] = K \u22121 D t (u) [u , 1] . This can be quantified by a per pixel dense model-to-frame point-plane error, which we compute under the robust Tukey penalty function \u03c8 data , summed over the predicted image domain \u2126:\nData(W, V, D t ) \u2261 u\u2208\u2126 \u03c8 data n u (v u \u2212 vl\u0169) . (7)\naugment. The transformed model vertex v(u) is simpl\u1ef9 T u = W(v(u)), producing the current canonical to live frame point-normal predictionsv u =T u v(u) andn u = T u n(u), and data-association of that model point-normal is made with a live frame point-normal through perspective projection into the pixel\u0169 = \u03c0(Kv u ).\nWe note that, ignoring the negligible cost of rendering the geometryV w , the ability to extract, predict, and perform projective data association with the currently visible canonical geometry leads to a data-term evaluation that has a computational complexity with an upper bound in the number of pixels in the observation image. Furthermore, each dataterm summand depends only on a subset of the n trans-formations when computing W, and the region over which each node has a numerically significant impact on the error function is compact. In practice, the result is a computational cost similar to a single rigid body dense projective point-plane data-term evaluation (as used in KinectFusion).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Warp-field Regularization", "text": "It is crucial for our non-rigid TSDF fusion technique to estimate a deformation not only of currently visible surfaces, but over all space within S. This enables reconstruction of new regions of the scene surface that are about to come into view. However, nodes affecting canonical space within which no currently observed surface resides will have no associated data term. In any case, noise, missing data and insufficient geometric texture in the live frame -an analogue to the aperture problem in optical-flow -will result in optimisation of the transform parameters being ill-posed. How should we constrain the motion of non-observed geometry? Whilst the fully correct motion depends on object dynamics and, where applicable, the subject's volition, we make use of a simpler model of unobserved geometry: that it deforms in a piece-wise smooth way.\nWe use a deformation graph based regularization defined between transformation nodes, where an edge in the graph between nodes i and j adds a rigid-as-possible regularisation term to the total error being minimized, under the discontinuity preserving Huber penalty \u03c8 reg . The total regularisation term sums over all pair-wise connected nodes:\nReg(W, E) \u2261 n i=0 j\u2208E(i) \u03b1 ij \u03c8 reg T ic dg j v \u2212 T jc dg j v , (8\n)\nwhere E defines the regularisation graph topology, and \u03b1 ij defines the weight associated with the edge, which we set to \u03b1 ij = max(dg i w , dg j w ).\nHierarchical Deformation Tree: In original applications of the embedded deformation graph [25] approach to non-rigid tracking, E is defined as the k\u2212nearest neighbours of each node or all nodes within a specified radius. We find that either of these edge sets work well in practice, but have further found that by constructing a hierarchical deformation graph with no explicit edge connectivity between siblings, both stability of the deformation field increases while computational costs of minimising the total energy function decreases. Given the current set of deformation nodes N warp , we construct a hierarchy of regularisation nodes N reg = {r v , r se3 , r w } (construction of the hierarchy is described in section 3.4). Importantly, we do not use N reg within the warp function W; they are used to induce longer range regularisation across the warp function with reduced computational complexity. Each level of the regularisation node hierarchy also defines the node positions, transforms, and support weights. Our regularisation graph topology is then simply formed by adding edges from each node of the hierarchy (starting in N warp ) to its k\u2212nearest nodes in the next coarser level. Since the latent surface reconstruction will grow within the canonical frame (until completely observed), we need to continuously update the deformation nodes and the regularisation graph, potentially at framerate, which we describe in Section (3.4).", "publication_ref": ["b24"], "figure_ref": [], "table_ref": []}, {"heading": "Efficient Optimization", "text": "Estimation of all transformation parameters, T lw , dg se3 and r se3 , is performed by minimising the total energy E (Eq. 6). We minimise E through Gauss-Newton non-linear optimisation, which requires iteratively re-linearising E around the currently estimated deformation node parameters and forming and solving the normal equations J Jx = J e. We formulate compositional updatesx through the exponential map with a per-node twist \u03be i \u2208 se(3), requiring 6 variables per node transform, and perform linearisation around \u03be i = 0. It is crucial that our solver is efficient, since a warp field of a deforming scene (for example a person gesticulating) may require several hundred deformation nodes, corresponding to many thousands of parameters requiring solution at frame rate. Recently, [37] demonstrated a real-time solution to a related non-rigid tracking optimization using a GPU accelerated pre-conditioned conjugate gradient descent solver. We take a different approach, using instead a direct sparse Cholesky factorization of each linearised system. We note that direct solvers resolve lowfrequency residuals very effectively, which is critical to ensuring minimal drift during reconstruction.\nThe main computational complexity in minimising E involves constructing and factorizing the Gauss-Newton approximation of the Hessian:\nJ J = J d J d + \u03bbJ r J r .\nFirst, we note that the k-nearest node field induces non-zero blocks in the data term component J d J d for each pair of nodes currently involved in deforming V into an observable region of the live frame. Building the full linear system on the GPU currently hinders real-time performance due to requirements on global GPU memory read and writes. Fortunately, since the associated weight of each deformation node reduces to a very small value outside of 3dg w , any data term there can be safely ignored. Approximating further, we compute only the block diagonal terms for J d J d , as if the effect of each node on the warp function were independent, resulting in a computational cost of building the structure similar to a single rigid body transformation for the frame. This technique is reasonable since, after solution of the linearised system, the current canonical model surface is re-warped into the live frame, resulting in a form of time-lagged linearisation of the objective.\nA second optimization efficiency comes in the form of the sparse linear system that our hierarchical regularisation term induces. We construct the regularisation Hessian approximation J r J r using the linearisation of the virtual deformation node parameters, laying out the parameter blocks with a coarse to fine ordering. The resulting complete system matrix has a block arrow-head form which is efficiently factorized with a block-Cholesky decomposition.\nPrior to non-rigid optimisation, given a new frame, we first estimate the factorised transformation T lw using the dense ICP introduced in KinectFusion. This resolves the relative rigid body transformation, i.e. due to camera motion and improves data-association for the non-rigid solver.\nTo that end, we re-render the predicted surface geometryV and perform 2 or 3 iterations of the dense non-rigid optimization. Finally, we factorise out any resulting rigid body transformationT common across all deformation nodes and update T lw \u2190TT lw .\nIt is important that computation of W is fast, requiring evaluation many millions of times per frame, all residing on the GPU. We therefore pre-compute, for each updated set of deformation node positions, a discretisation of the k\u2212nearest node field required in dual-quaternion blending (DQB) with the same resolution of our volumetric TSDF: I : S \u2192 N k . Due to the sparsity of the deformation nodes relative to the sampling density of S, this is a very fine approximation and is efficiently updated on the GPU whenever the set of nodes is updated.", "publication_ref": ["b37"], "figure_ref": [], "table_ref": []}, {"heading": "Extending the Warp-field", "text": "In the preceding subsections we defined how the canonical space can be deformed through W (Section 3.1), introduced the optimisation required to estimate warp-field state through time (3.3), and showed how, given an estimated warp field, we can incrementally update the canonical surface geometry (3.2). As this model grows, so must the support of the warp function. In this subsection we de-Figure 4: The first frames and final canonical models from DynamicFusion results shown in our accompanying video, available on our project website: http://grail.cs.washington.edu/projects/dynamicfusion. scribe our approach to extending the warp-field parametrisation to ensure deformations are represented, over both the newly emerging surface geometry and soon to be observed space. This consists of incrementally updating the deformation graph nodes N warp , and then recomputing a new hierarchical edge topology E that expands the regularisation to include the new nodes.\nInserting New Deformation Nodes into N warp : After performing a non-rigid TSDF fusion step, we extract the surface estimate in the canonical frame as the polygon meshV c . Given the current set of nodes N warp , we compute the extent to which the current warp function covers the extracted geometry. This simply entails computing the normalised distance from each vertex v c \u2208V c to its supporting nodes. An unsupported surface vertex is detected when the distance min k\u2208N(xc)\ndg k v \u2212vc dg k w \u2265 1.\nThe set of all unsupported vertices is then spatially sub-sampled using a simple radius search averaging to reduce the vertices to a set of new node positionsdg v that are at least distance apart. We note that is an important parameter in DynamicFusion; while the regularisation parameter \u03bb enforces global deformation smoothness, defines the effective resolution of the motion field. Each new node center dg * v \u2208dg v requires an initialisation of its current transformation, which is obtained directly through DQB with the current warp dg * se3 \u2190 W t (dg * v ). Finally, we update the current set of deformation nodes to correspond to the current time N t warp = N t\u22121 warp \u222a {dg v ,dg se3 ,dg w }. For each new node to be inserted, we perform an efficient GPU based update to the pre-computed k nearest node field I.\nUpdating the Regularisation Graph E: Given the newly updated set of deformation nodes, we construct an L \u2265 1 level regularisation graph node hierarchy, where the l = 0 level nodes will simply be N warp . We compute the next l = 1 level of regularisation nodes by running the radius search based sub-sampling on the warp field nodes dg v to an increased decimation radius of \u03b2 l , where \u03b2 > 1, and again compute the initial node transforms through DQB with the now updated W t . We repeat this to compute the remaining levels of the hierarchy. A completely new set of regularisation edges E is then constructed, starting with edges from l = 0 (i.e. N warp ) to the nodes in N reg at l = 1. Edges are added for each node in the finer level to its k-nearest neighbours in the coarser level.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "We demonstrate the system with a range of deforming scenes captured directly from DynamicFusion in Figure ( 5 We have found that the system works reliably across a range of dynamic scenes and settings of parameters. We urge readers to view the associated video and supplementary material for additional details of the reconstruction process and to fully appreciate the live capabilities of the system. Limitations and Discussion: While DynamicFusion can easily handle closing topological surfaces (see the hands in 5b), it is currently limited in its ability to achieve dynamic reconstruction of scenes that quickly move from a closed to open topology (for example starting a reconstruction with closed hands and then opening).\nMore generally, failures common to real-time differential tracking can cause unrecoverable model corruption or result in loop closure failures. Large inter-frame motions, or motion of occluded regions, will also lead to an inaccurate surface prediction that prevents projective data-association in later frames.\nStability of the warp field and subsequent reconstruction is achieved by the combined qualities of the as-rigidas-possible regularisation, warp field parametrisation, and the use of a dense data-term. However, we have observed limits on this stability when attempting reconstruction of highly dynamic scenes. For example, reducing the regularisation weight and increasing the density of nodes enables Canonical Model for \"drinking from a cup\" (a) Canonical model warped into the live frame for \"drinking from a cup\" Canonical Model for \"Crossing fingers\" (b) Canonical model warped into the live frame for \"crossing fingers\" Figure 5: Real-time non-rigid reconstructions for two deforming scenes. Upper rows of (a) and (b) show the canonical models as they evolve over time, lower rows show the corresponding warped geometries tracking the scene. In (a) complete models of the arm and the cup are obtained. Note the system's ability to deal with large motion and add surfaces not visible in the initial scene, such as the bottom of the cup and the back side of the arm. In (b) we show full body motions including clasping of the hands where we note that the model stays consistent throughout the interaction.\ntracking scenes with more fluid deformations than shown in the results, but the long term stability can degrade and tracking will fail when the observed data term is not able to constrain the optimisation sufficiently. Finally, we show reconstruction results that are at the current limit of what can be obtained in real-time with DynamicFusion, and there are two limitations for scaling further. As in KinectFusion, volumetric TSDF memory limits geometric extent; but there are many solutions to this in the literature. More challenging is the estimation of a growing warp field. As the size and complexity of the scene increases, proportionally more is occluded from the camera, and the problem of predicting the motion of occluded areas becomes much more challenging. This is a subject of our ongoing research.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusions", "text": "In this paper we introduced DynamicFusion, the first real-time dense dynamic scene reconstruction system, removing the static scene assumption pervasive across realtime 3D reconstruction and SLAM systems. We achieved this by generalising the volumetric TSDF fusion technique to the non-rigid case, as well as developing an efficient approach to estimate a volumetric 6D warp field in real-time. DynamicFusion obtains reconstructions of objects whilst they deform and provides dense correspondence across time. We believe that DynamicFusion, like KinectFusion, will open up a number of interesting applications of realtime 3D scanning and SLAM systems in dynamic environments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "This work was funded in part by Google, the Intel Science and Technology Center for Pervasive Computing (ISTC-PC) and by ONR grant N00014-13-1-0720. We would like to thank Daniel Canelhas, Steven Lovegrove and Tanner Schmidt for many useful conversations and insights during the development of this work.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Non-Rigid Range-Scan Alignment Using Thin-Plate Splines", "journal": "", "year": "2002", "authors": "B Brown; S Rusinkiewicz"}, {"ref_id": "b1", "title": "Global Non-rigid Alignment of 3-D Scans", "journal": "ACM Trans. Graph", "year": "2002", "authors": "B J Brown; S Rusinkiewicz"}, {"ref_id": "b2", "title": "3D Shape Regression for Real-time Facial Animation", "journal": "ACM Trans. Graph", "year": "2013-07", "authors": "C Cao; Y Weng; S Lin; K Zhou"}, {"ref_id": "b3", "title": "Range Scan Registration Using Reduced Deformable Models", "journal": "Comput. Graph. Forum", "year": "2009", "authors": "W Chang; M Zwicker"}, {"ref_id": "b4", "title": "A volumetric method for building complex models from range images", "journal": "", "year": "1996", "authors": "B Curless; M Levoy"}, {"ref_id": "b5", "title": "New Methods for Surface Reconstruction from Range Images", "journal": "", "year": "1997", "authors": "B L Curless"}, {"ref_id": "b6", "title": "Temporally enhanced 3D capture of room-sized dynamic scenes with commodity depth cameras", "journal": "", "year": "2002", "authors": "M Dou; H Fuchs"}, {"ref_id": "b7", "title": "Scanning and tracking dynamic objects with commodity depth cameras", "journal": "", "year": "2013", "authors": "M Dou; H Fuchs; J.-M Frahm"}, {"ref_id": "b8", "title": "Patch Volumes: Segmentation-based Consistent Mapping with RGB-D Cameras", "journal": "", "year": "2013", "authors": "P Henry; D Fox; A Bhowmik; R Mongia"}, {"ref_id": "b9", "title": "KinectFusion: Real-Time 3D Reconstruction and Interaction Using a Moving Depth Camera", "journal": "", "year": "2011", "authors": "S Izadi; D Kim; O Hilliges; D Molyneaux; R A Newcombe; P Kohli; J Shotton; S Hodges; D Freeman; A J Davison; A Fitzgibbon"}, {"ref_id": "b10", "title": "Skinning with Dual Quaternions", "journal": "ACM", "year": "2007", "authors": "L Kavan; S Collins; J \u017d\u00e1ra; C O'sullivan"}, {"ref_id": "b11", "title": "Robust Singleview Geometry and Motion Reconstruction", "journal": "ACM Trans. Graph", "year": "2002", "authors": "H Li; B Adams; L J Guibas; M Pauly"}, {"ref_id": "b12", "title": "Temporally Coherent Completion of Dynamic Shapes", "journal": "ACM Transactions on Graphics", "year": "2002", "authors": "H Li; L Luo; D Vlasic; P Peers; J Popovi\u0107; M Pauly; S Rusinkiewicz"}, {"ref_id": "b13", "title": "Global Correspondence Optimization for Non-Rigid Registration of Depth Scans", "journal": "", "year": "2002", "authors": "H Li; R W Sumner; M Pauly"}, {"ref_id": "b14", "title": "", "journal": "3D Self-Portraits. ACM Transactions on Graphics (Proceedings SIGGRAPH Asia", "year": "2013-11", "authors": "H Li; E Vouga; A Gudym; L Luo; J T Barron; G Gusev"}, {"ref_id": "b15", "title": "Realtime Facial Animation with On-the-fly Correctives", "journal": "ACM Transactions on Graphics", "year": "2013-07", "authors": "H Li; J Yu; Y Ye; C Bregler"}, {"ref_id": "b16", "title": "Dynamic Geometry Registration", "journal": "Eurographics Association", "year": "2007", "authors": "N J Mitra; S Fl\u00f6ry; M Ovsjanikov; N Gelfand; L Guibas; H Pottmann"}, {"ref_id": "b17", "title": "KinectFusion: Real-Time Dense Surface Mapping and Tracking", "journal": "", "year": "2004", "authors": "R A Newcombe; S Izadi; O Hilliges; D Molyneaux; D Kim; A J Davison; P Kohli; J Shotton; S Hodges; A Fitzgibbon"}, {"ref_id": "b18", "title": "Real-time 3d reconstruction at scale using voxel hashing", "journal": "ACM Transactions on Graphics (TOG)", "year": "2013", "authors": "M Nie\u00dfner; M Zollh\u00f6fer; S Izadi; M Stamminger"}, {"ref_id": "b19", "title": "Efficient model-based 3D tracking of hand articulations using Kinect", "journal": "", "year": "2011", "authors": "I Oikonomidis; N Kyriazis; A Argyros"}, {"ref_id": "b20", "title": "Realtime and Robust Hand Tracking from Depth", "journal": "", "year": "2002", "authors": "C Qian; X Sun; Y Wei; X Tang; J Sun"}, {"ref_id": "b21", "title": "Moving Volume KinectFusion", "journal": "", "year": "2012", "authors": "H Roth; M Vona"}, {"ref_id": "b22", "title": "DART: Dense Articulated Real-Time Tracking", "journal": "", "year": "2014", "authors": "T Schmidt; R Newcombe; D Fox"}, {"ref_id": "b23", "title": "Space-time surface reconstruction using incompressible flow", "journal": "ACM Trans. Graph", "year": "2008", "authors": "A Sharf; D A Alcantara; T Lewiner; C Greif; A Sheffer; N Amenta; D Cohen-Or"}, {"ref_id": "b24", "title": "Embedded deformation for shape manipulation", "journal": "ACM Trans. Graph", "year": "2004", "authors": "R W Sumner; J Schmid; M Pauly"}, {"ref_id": "b25", "title": "Reconstructing Animated Meshes from Time-varying Point Clouds", "journal": "Eurographics Association", "year": "2008", "authors": "J S\u00fcssmuth; M Winter; G Greiner"}, {"ref_id": "b26", "title": "The Vitruvian manifold: Inferring dense correspondences for oneshot human pose estimation", "journal": "", "year": "2012", "authors": "J Taylor; J Shotton; T Sharp; A Fitzgibbon"}, {"ref_id": "b27", "title": "Animation Cartography&Mdash;Intrinsic Reconstruction of Shape and Motion", "journal": "ACM Trans. Graph", "year": "2002", "authors": "A Tevs; A Berner; M Wand; I Ihrke; M Bokeloh; J Kerber; H.-P Seidel"}, {"ref_id": "b28", "title": "Efficient Reconstruction of Nonrigid Shape and Motion from Realtime 3D Scanner Data", "journal": "ACM Trans. Graph", "year": "2002", "authors": "M Wand; B Adams; M Ovsjanikov; A Berner; M Bokeloh; P Jenke; L Guibas; H.-P Seidel; A Schilling"}, {"ref_id": "b29", "title": "Reconstruction of Deforming Geometry from Time-varying Point Clouds", "journal": "Eurographics Association", "year": "2007", "authors": "M Wand; P Jenke; Q Huang; M Bokeloh; L Guibas; A Schilling"}, {"ref_id": "b30", "title": "Deformation-based loop closure for large scale dense RGB-D SLAM", "journal": "", "year": "2013", "authors": "T Whelan; M Kaess; J Leonard; J Mcdonald"}, {"ref_id": "b31", "title": "IEEE/RSJ International Conference on", "journal": "", "year": "2001", "authors": ""}, {"ref_id": "b32", "title": "Kintinuous: Spatially Extended KinectFusion", "journal": "", "year": "2012", "authors": "T Whelan; J Mcdonald; M Kaess; M Fallon; H Johannsson; J J Leonard"}, {"ref_id": "b33", "title": "Real-Time Simultaneous Pose and Shape Estimation for Articulated Objects Using a Single Depth Camera", "journal": "", "year": "2002", "authors": "M Ye; R Yang"}, {"ref_id": "b34", "title": "A Memory-Efficient KinectFusion Using Octree", "journal": "", "year": "2012", "authors": "M Zeng; F Zhao; J Zheng; X Liu"}, {"ref_id": "b35", "title": "Templateless Quasi-rigid Shape Modeling with Implicit Loop-Closure", "journal": "", "year": "2013-06", "authors": "M Zeng; J Zheng; X Cheng; X Liu"}, {"ref_id": "b36", "title": "Quality Dynamic Human Body Modeling Using a Single Low-cost Depth Camera", "journal": "", "year": "2002", "authors": "Q Zhang; B Fu; M Ye; R Yang"}, {"ref_id": "b37", "title": "Real-time Non-rigid Reconstruction Using an RGB-D Camera", "journal": "ACM Trans. Graph", "year": "2006", "authors": "M Zollh\u00f6fer; M Niessner; S Izadi; C Rehmann; C Zach; M Fisher; C Wu; A Fitzgibbon; C Loop; C Theobalt; M Stamminger"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "(a) Initial Frame at t = 0s (b) Raw (noisy) depth maps for frames at t = 1s, 10s, 15s, 20s (c) Node Distance (d) Canonical Model (e) Canonical model warped into its live frame (f) Model Normals", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: DynamicFusion takes an online stream of noisy depth maps (a,b) and outputs a real-time dense reconstruction of the moving scene (d,e). To achieve this, we estimate a volumetric warp (motion) field that transforms the canonical model space into the live frame, enabling the scene motion to be undone, and all depth maps to be densely fused into a single rigid TSDF reconstruction (d,f). Simultaneously, the structure of the warp field is constructed as a set of sparse 6D transformation nodes that are smoothly interpolated through a k-nearest node average in the canonical frame (c). The resulting per-frame warp field estimate enables the progressively denoised and completed scene geometry to be transformed into the live frame in real-time (e). In (e) we also visualise motion trails for a sub-sample of model vertices over the last 1 second of scene motion together with a coordinate frame showing the rigid body component of the scene motion. In (c) we render the nearest node to model surface distance where increased distance is mapped to a lighter value.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "), and throughout the paper. In Figure (4) we show the first frames and reconstructions for further results in our accompanying video. These examples highlight the ability of DynamicFusion to (1) continuously track across large motion during reconstruction, (2) fill in initially occluded parts of the scene, and (3) generate consistent geometry despite many loop closures occurring during the capture process. Parameters: results presented were obtained live from the system with optimisation parameters \u03bb = 200, \u03c8 data = 0.01, \u03c8 reg = 0.0001; L = 4 levels in the regularisation hierarchy with \u03b2 = 4 and a decimation density of = 25mm for all reconstructions except Figure (1) where = 15mm.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "W(x c ) \u2261 SE3(DQB(x c )) ,(1)", "formula_coordinates": [3.0, 99.73, 668.96, 186.63, 9.65]}, {"formula_id": "formula_1", "formula_text": "formations is simply DQB(x c ) \u2261 k\u2208N (xc ) w k (xc)q kc k\u2208N (xc ) w k (xc)q kc ,", "formula_coordinates": [3.0, 50.11, 696.3, 236.25, 19.4]}, {"formula_id": "formula_2", "formula_text": "w i (x c ) = exp \u2212 dg i v \u2212 x c 2 / 2(dg i w ) 2", "formula_coordinates": [3.0, 351.81, 496.44, 175.86, 13.04]}, {"formula_id": "formula_3", "formula_text": "W t (x c ) = T lw SE3(DQB(x c )).", "formula_coordinates": [4.0, 91.14, 92.06, 134.27, 9.68]}, {"formula_id": "formula_4", "formula_text": "psdf (x c ) = K \u22121 D t (u c ) u c , 1 z \u2212 [x t ] z ,(3)", "formula_coordinates": [4.0, 61.19, 375.89, 225.17, 17.2]}, {"formula_id": "formula_5", "formula_text": "V(x) t = [v (x), w (x)] , if psdf (dc(x)) > \u2212\u03c4 V(x) t\u22121 , otherwise(4)", "formula_coordinates": [4.0, 52.81, 488.78, 233.56, 24.11]}, {"formula_id": "formula_6", "formula_text": "v (x) = v(x) t\u22121 w(x) t\u22121 + min(\u03c1, \u03c4 )w(x) w(x) t\u22121 + w(x) \u03c1 = psdf (dc(x)) w (x) = min(w(x) t\u22121 + w(x), w max ) .(5)", "formula_coordinates": [4.0, 71.7, 572.95, 214.66, 52.23]}, {"formula_id": "formula_7", "formula_text": "w(x) \u221d 1 k i\u2208N (xc) dg i w \u2212 x c 2 .", "formula_coordinates": [4.0, 310.06, 122.98, 235.05, 23.55]}, {"formula_id": "formula_8", "formula_text": "E(W t , V, D t , E) = Data(W t , V, D t ) + \u03bbReg(W t , E) . (6)", "formula_coordinates": [4.0, 308.86, 367.67, 248.01, 9.68]}, {"formula_id": "formula_9", "formula_text": "Data(W, V, D t ) \u2261 u\u2208\u2126 \u03c8 data n u (v u \u2212 vl\u0169) . (7)", "formula_coordinates": [5.0, 59.34, 532.21, 227.03, 20.09]}, {"formula_id": "formula_10", "formula_text": "Reg(W, E) \u2261 n i=0 j\u2208E(i) \u03b1 ij \u03c8 reg T ic dg j v \u2212 T jc dg j v , (8", "formula_coordinates": [5.0, 308.86, 641.98, 244.34, 30.94]}, {"formula_id": "formula_11", "formula_text": ")", "formula_coordinates": [5.0, 553.2, 652.71, 3.87, 8.64]}, {"formula_id": "formula_12", "formula_text": "J J = J d J d + \u03bbJ r J r .", "formula_coordinates": [6.0, 175.45, 692.21, 110.91, 11.01]}, {"formula_id": "formula_13", "formula_text": "dg k v \u2212vc dg k w \u2265 1.", "formula_coordinates": [7.0, 186.08, 383.87, 63.88, 18.4]}], "doi": ""}