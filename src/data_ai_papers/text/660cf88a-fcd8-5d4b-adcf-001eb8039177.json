{"title": "Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models", "authors": "Nora Kassner; Philipp Dufter; Hinrich Sch\u00fctze", "pub_date": "", "abstract": "Recently, it has been found that monolingual English language models can be used as knowledge bases. Instead of structural knowledge base queries, masked sentences such as \"Paris is the capital of [MASK]\" are used as probes. We translate the established benchmarks TREx and GoogleRE into 53 languages. Working with mBERT, we investigate three questions. (i) Can mBERT be used as a multilingual knowledge base? Most prior work only considers English. Extending research to multiple languages is important for diversity and accessibility. (ii) Is mBERT's performance as knowledge base language-independent or does it vary from language to language? (iii) A multilingual model is trained on more text, e.g., mBERT is trained on 104 Wikipedias. Can mBERT leverage this for better performance? We find that using mBERT as a knowledge base yields varying performance across languages and pooling predictions across languages improves performance. Conversely, mBERT exhibits a language bias; e.g., when queried in Italian, it tends to predict Italy as the country of origin.", "sections": [{"heading": "Introduction", "text": "Pretrained language models (LMs) (Peters et al., 2018;Howard and Ruder, 2018;Devlin et al., 2019) can be finetuned to a variety of natural language processing (NLP) tasks and generally yield high performance. Increasingly, these models and their generative variants are used to solve tasks by simple text generation, without any finetuning (Brown et al., 2020). This motivated research on how much knowledge is contained in LMs: Petroni et al. (2019) used models pretrained with masked language to answer fill-in-the-blank templates such as \"Paris is the capital of [MASK].\" * Equal contribution -random order.", "publication_ref": ["b20", "b9", "b5", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Query", "text": "Two most frequent predictions en X was created in MASK.\n[Japan (170), Italy (56), . . . ] de X wurde in MASK erstellt. [Deutschland (217), Japan (70), . . . ] it X\u00e8 stato creato in MASK.\n[ Italia (167), Giappone (92), . . . ] nl X is gemaakt in MASK.\n[ Nederland (172), Itali\u00eb (50), . . . ] en X has the position of MASK. [bishop (468), God (68), ...] de X hat die Position MASK.\n[WW (261), Ratsherr (108), ...] it X ha la posizione di MASK. [pastore ( 289), papa (138), ...] nl X heeft de positie van MASK. [burgemeester (400), bisschop (276) , ...] Table 1: Language bias when querying (TyQ) mBERT. Top: For an Italian cloze question, Italy is favored as country of origin. Bottom: There is no overlap between the top-ranked predictions, demonstrating the influence of language -even though the facts are the same: the same set of triples is evaluated across languages. Table 3 shows that pooling predictions across languages addresses bias and improves performance. WW = \"Wirtschaftswissenschaftler\". This research so far has been exclusively on English. In this paper, we focus on using multilingual pretrained LMs as knowledge bases. Working with mBERT, we investigate three questions. (i) Can mBERT be used as a multilingual knowledge base? Most prior work only considers English. Extending research to multiple languages is important for diversity and accessibility. (ii) Is mBERT's performance as knowledge base language-independent or does it vary from language to language? To answer these questions, we translate English datasets and analyze mBERT for 53 languages. (iii) A multilingual model is trained on more text, e.g., BERT's training data contains the English Wikipedia, but mBERT is trained on 104 Wikipedias. Can mBERT leverage this fact? Indeed, we show that pooling across languages helps performance.\nIn summary our contributions are: i) We automatically create a multilingual version of TREx and GoogleRE covering 53 languages. ii) We use an alternative to fill-in-the-blank querying -ranking entities of the type required by the template (e.g., cities) -and show that it is a better tool to investigate knowledge captured by pretrained LMs. iii) We show that mBERT answers queries across languages with varying performance: it works reasonably for 21 and worse for 32 languages. iv) We give evidence that the query language affects results: a query formulated in Italian is more likely to produce Italian entities (see Table 1). v) Pooling predictions across languages improves performance by large margins and even outperforms monolingual English BERT. Code and data are available online (https://github.com/ norakassner/mlama).", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Data", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "LAMA", "text": "We follow the LAMA setup introduced by Petroni et al. (2019). More specifically, we use data from TREx (Elsahar et al., 2018) and GoogleRE. 1 Both consist of triples of the form (object, relation, subject). The underlying idea of LAMA is to query knowledge from pretrained LMs using templates without any finetuning: the triple (Paris, capital-of, France) is queried with the template \"Paris is the capital of [MASK].\" In LAMA, TREx has 34,039 triples across 41 relations, GoogleRE 5528 triples and 3 relations. Templates for each relation have been manually created by Petroni et al. (2019). We call all triples from TREx and GoogleRE together LAMA.\nLAMA has been found to contain many \"easyto-guess\" triples; e.g., it is easy to guess that a person with an Italian sounding name is born in Italy. LAMA-UHN is a subset of triples that are hard to guess introduced by Poerner et al. (2020).", "publication_ref": ["b22", "b6", "b22", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Translation", "text": "We translate both entities and templates. We use Google Translate to translate templates in the form \"[X] is the capital of [Y]\". After translation, all templates were checked for validity (i.e., whether they contain \"[X]\", \"[Y]\" exactly once) and corrected if necessary. In addition, German, Hindi and Japanese templates were checked by native speakers to assess translation quality (see Table 2). To translate the entity names, we used Wikidata and Google knowledge graphs.\nmBERT covers 104 languages. Google Translate covers 77 of these. Wikidata and Google Knowledge Graph do not provide entity translations for all languages and not all entities are contained in the knowledge graphs. For English we can find a total of 37,498 triples which we use from now on. On average, 34% of triples could be translated (macro average over languages). We only consider languages with a coverage above 20%, resulting in the final number of languages we include in our study: 53. The macro average of translated triples in these 53 languages is 43%. Figure 1 gives statistics. We call the translated dataset mLAMA.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": ["tab_1"]}, {"heading": "Experiments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Model", "text": "We work with mBERT (Devlin et al., 2019), a model pretrained on the 104 largest Wikipedias. We denote mBERT queried in language x as mBERT [x]. As comparison we use the English BERT-Base model and refer to it as BERT. In initial experiments with XLM-R (Conneau et al., 2020) we observed worse performance, similar to Jiang et al. (2020a). Thus, for simplicity we only report results on mBERT.", "publication_ref": ["b5", "b4", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Typed and Untyped Querying", "text": "Petroni et al. (2019) use templates like \"Paris is the capital of [MASK]\" and give arg max w\u2208V p(w|t) as answer where V is the vocabulary of the LM and p(w|t) is the (log-)probability that word w gets predicted in the template t. Thus the object of a triple must be contained in the vocabulary of the language model. This has two drawbacks: it reduces the number of triples that can be considered drastically and hinders performance comparisons across LMs with different vocabularies. We refer to this procedure as UnTyQ.\nWe propose to use typed querying, TyQ: for each relation a candidate set C is created and the prediction becomes arg max c\u2208C p(c|t). For templates like \"[X] was born in [MASK]\", we know which entity type to expect, in this case cities. We observed that (English-only) BERT-base predicts city names for MASK whereas mBERT predicts years for the same template. TyQ prevents this.\nWe choose as C the set of objects across all triples for a single relation. The candidate set could also be obtained from an entity typing system (e.g., (Yaghoobzadeh and Sch\u00fctze, 2016)), but this is beyond the scope of this paper. Variants of TyQ have been used before (Xiong et al., 2020).", "publication_ref": ["b27", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "Singletoken vs. Multitoken Objects", "text": "Assuming that objects are in the vocabulary (Petroni et al., 2019) is a restrictive assumption, even more in the multilingual case as e.g., \"Hamburg\" is in the mBERT vocabulary, but French \"Hambourg\" is tokenized to [\"Ham\", \"##bourg\"]. We consider multitoken objects by including multiple [MASK] tokens in the templates. For both TyQ and UnTyQ we compute the score that a multitoken object is predicted by taking the average of the log probabilities for its individual tokens.\nGiven a template t (e.g., \"[X] was born in [Y].\") let t 1 be the template with one mask token, (i.e., \"[X] was born in [MASK].\") and t k be the template with k mask tokens (i.e., \"[X] was born in [MASK] [MASK] . . . [MASK].\"). We denote the log probability that the token w \u2208 V is predicted at ith mask token as p(m i = w|t k ), where V is the vocabulary of the LM. To compute p(e|t) for an entity e that is tokenized into l tokens 1 , 2 , . . . , l we simply average the log probabilities across tokens:\np(e|t) = 1 l l i=1 p(m i = i |t l ).\nIf k is the maximum number of tokens of any entity e \u2208 C gets split into, we consider all templates t 1 , . . . , t k , with C being the candidate set. The prediction is then the word with the highest average log probability across all templates t 1 , . . . , t k .\nNote that for UnTyQ the space of possible predictions is V \u00d7 V \u00d7 \u2022 \u2022 \u2022 \u00d7 V whereas for TyQ it is the candidate set C.", "publication_ref": ["b22"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation", "text": "We compute precision at one for each relation, i.e., 1/|T | t\u2208T 1{t object = t object } where T is the set of all triples andt object is the object predicted by TyQ or UnTyQ. Note that T is different for each language. Our final measure (p1) is then the precision at one averaged over relations (i.e., macro average). Results for multiple languages are the macro average p1 across languages. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results and Discussion", "text": "We first investigate TyQ and UnTyQ and find that TyQ is better suited for investigating knowledge in LMs. After exploring the translation quality, we use TyQ on mLAMA and observe rather stable performance for 21 and poor performance for 32 languages. When investigating the languages more closely, we find that prediction results highly depend on the language. Finally, we validate our initial hypothesis that mBERT can leverage its multilinguality by pooling predictions: pooling indeed performs better.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "UnTyQ vs. TyQ", "text": "Figure 2 shows the distribution of p1 scores for single and multitoken objects. As expected, TyQ works better, both for single and multitoken objects. With UnTyQ, performance not only depends on the model's knowledge, but on at least three extraneous factors: (i) Does the model understand the type constraints of the template (e.g., in \"X is the capital of Y\", Y must be a country)? (ii) How \"fluent\" a substitution is an object under linguistic constraints (e.g., morphology) that can be viewed as orthogonal to knowledge? Many English templates cannot be translated into a single template in many languages, e.g., \"in X\" (with X a country) has different translations in French: \"\u00e0 Chypre\", \"au Mexique\", \"en Inde\".  mining (Jiang et al., 2020b) has been investigated in the literature to approach the typing problem.\nWe had native speakers check templates for German, Hindi and Japanese, correct mistakes in the automatic translation and paraphrase the template to obtain predictions with the correct type. Table 2 shows that corrections do not yield strong improvements. We conclude that template modifications are not an effective solution for the typing problem.", "publication_ref": ["b13"], "figure_ref": ["fig_1"], "table_ref": ["tab_1"]}, {"heading": "Translation Quality", "text": "Contemporaneous work by Jiang et al. (2020a) provides manual translations of LAMA templates for 23 languages respecting grammatical gender and inflection constraints. We evaluate our machine translated templates by comparing performance on a common subset of 14 languages using TyQ querying on the TREx subset. Surprisingly, we find a performance difference of 1 percentage points (0.23 vs. 0.24, p1 averaged over languages) in favor of the machine translated templates. This indicates that the machine translated templates in combination with TyQ exhibit comparable performance but come with the benefit of larger language coverage (53 vs. 23 languages).", "publication_ref": ["b12"], "figure_ref": [], "table_ref": []}, {"heading": "Multilingual Performance", "text": "In mLAMA, not all triples are available in all languages. Thus absolute numbers are not comparable across languages and we adopt a relative performance comparison: we report p1 of a modellanguage combination divided by p1 of mBERT's performance in English (mBERT[en]) on the exact same set of triples and call this rel-p1. A rel-p1 score of 0.5 for mBERT [fi] means that p1 of mBERT on Finnish is half of mBERT[en]'s performance on the same triples. rel-p1 of English BERT is usually greater than 1 as monolingual BERT tends to outperform mBERT [en].\nFigure 3 shows that mBERT performs reasonably well for 21 languages, but for 32 languages LAMA LAMA-UHN BERT 38.5 29.0 mBERT [en] 35.0 25.7 mBERT [pooled] 41.1 32.1 rel-p1 is less than 0.6 (i.e., their p1 is 60% of English's p1). We conclude that mBERT does not exhibit a stable performance across languages. The variable performance (from 20% to almost 100% rel-p1) indicates that mBERT has no common representation for, say, \"Paris\" across languages, i.e., mBERT representations are language-dependent.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Bias", "text": "If mBERT captured knowledge independent of language, we should get similar answers across languages for the same relation. However, Table 1 shows that mBERT exhibits language-specific biases; e.g., when queried in Italian, it tends to predict Italy as the country of origin. This effect occurs for several relations: Table 4 in the supplementary presents data for ten relations and four languages.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Pooling", "text": "We investigate pooling of predictions across languages by picking the object predicted by the majority of languages. Table 3 shows that pooled mBERT outperforms mBERT[en] by 6 percentage points on LAMA, presumably in part because the language-specific bias is eliminated. mBERT[pooled] even outperforms BERT by 3 percentage points on LAMA-UHN. This indicates that mBERT can leverage the fact that it is trained on 104 Wikipedias vs. just one and even outperforms the much stronger model BERT.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Related Work", "text": "Petroni et al. ( 2019) first asked the question: can pretrained LMs function as knowledge bases? Subsequent analyses focused on different aspects, such as negation , easy to guess names (Poerner et al., 2020), integrating adapters  or finding alternatives to a \"fill-in-the-blank\" approach with singletoken answers (Bouraoui et al., 2020;Heinzerling and Inui, 2020;Jiang et al., 2020b). Other work combines pretrained LM with information retrieval (Guu et al., 2020;Lewis et al., 2020a;Izacard and Grave, 2020;  Multilingual models like mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) perform well for zero-shot crosslingual transfer (Hu et al., 2020). However, we are not aware of any prior work that analyzed to what degree pretrained multilingual models can be used as knowledge bases. There are many multilingual question answering datasets such as XQuAD (Artetxe et al., 2020), TiDy (Clark et al., 2020), MKQA (Longpre et al., 2020) and MLQA (Lewis et al., 2020b). Usually, multilingual models are finetuned to solve such tasks. Our goal is not to improve question answering or create an alternative multilingual question answering dataset, but instead to investigate which knowledge is contained in pretrained multilingual LMs without any kind of supervised finetuning.\nThere is a range of alternative multilingual knowledge bases that could be used for evaluation. Those include ConceptNet (Speer et al., 2017) or BabelNet (Navigli and Ponzetto, 2010). We decided to provide a translated versions of TREx and GoogleRE for the sake of comparability across languages. By translating manually created templates and entities we can ensure comparability across languages. This is not possible for crowd-sourced databases like ConceptNet.\nIn contemporaneous work, Jiang et al. (2020a) create and investigate a multilingual version of LAMA. They provide human template translations for 23 languages, propose several methods for multitoken decoding and code-switching, and experiment with a number of PLMs. In contrast to their work, we investigate typed querying, focus on comparabiliy and pooling across languages, and explore language biases.", "publication_ref": ["b23", "b1", "b8", "b13", "b7", "b16", "b11", "b5", "b4", "b10", "b0", "b3", "b18", "b17", "b24", "b19", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We presented mLAMA, a dataset to investigate knowledge in language models (LMs) in a multilingual setting covering 53 languages. While our results suggest that correct entities can be retrieved for many languages, there is a clear performance gap between English and, e.g., Japanese and Thai. This suggests that mBERT is not storing entity knowledge in a language-independent way. Experiments investigating language bias confirm this finding. We hope that this paper and the dataset we publish will stimulate research on investigating knowledge in LMs multilingually rather than just in English.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Language Bias", "text": "Table 4 shows the language bias for 10 relations. For each relation we aggregated the predictions across all triples and show the most common two predicted entities together with its count (in brackets). The querying language clearly affects results. The effect is drastic for relations that ask for a country (e.g., P495 or P1001). P39 yields very different results without exhibiting a clear pattern. Other relations such as P463 or P178 are rather stable.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Data Samples", "text": "Table 4 and Table 5 show randomly sampled entries from the data.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C Pretraining Data", "text": "We investigate whether performance across languages is correlated with the amount of pretraining data for each language. To this end we investigate the number of articles per language as of January 2021 2 and p1 for TyQ in Figure 6. We do not have access to the original pretraining data of mBERT. Thus, the number of articles we consider in the analysis might be different to the actual data used to train mBERT.   4: Most frequent object predictions (TyQ) in different languages. Some relations exhibit language specific biases. WW = \"Wirtschaftswissenschaftler\".\nFigure 5: Data samples continued. ", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Acknowledgements", "text": "This work was supported by the European Research Council (# 740516) and the German Federal Ministry of Education and Research (BMBF) under Grant No. 01IS18036A. The authors of this work take full responsibility for its content. The second author was supported by the Bavarian research institute for digital transformation (bidt) through their fellowship program. We thank Yannick Couzini\u00e9 and Karan Tiwana for correcting the Japanese and Hindi templates. We thank the anonymous reviewers for valuable comments.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "On the cross-lingual transferability of monolingual representations", "journal": "", "year": "2020", "authors": "Mikel Artetxe; Sebastian Ruder; Dani Yogatama"}, {"ref_id": "b1", "title": "Inducing relational knowledge from BERT", "journal": "AAAI Press", "year": "2020-02-07", "authors": "Zied Bouraoui; Jos\u00e9 Camacho-Collados; Steven Schockaert"}, {"ref_id": "b2", "title": "Mc-Candlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners", "journal": "", "year": "", "authors": "Tom B Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell; Sandhini Agarwal; Ariel Herbert-Voss; Gretchen Krueger; Tom Henighan; Rewon Child; Aditya Ramesh; Daniel M Ziegler; Jeffrey Wu; Clemens Winter; Christopher Hesse; Mark Chen; Eric Sigler; Mateusz Litwin"}, {"ref_id": "b3", "title": "TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020", "authors": "Jonathan H Clark; Jennimaria Palomaki; Vitaly Nikolaev; Eunsol Choi; Dan Garrette; Michael Collins; Tom Kwiatkowski"}, {"ref_id": "b4", "title": "Unsupervised cross-lingual representation learning at scale", "journal": "", "year": "2020", "authors": "Alexis Conneau; Kartikay Khandelwal; Naman Goyal; Vishrav Chaudhary; Guillaume Wenzek; Francisco Guzm\u00e1n; Edouard Grave; Myle Ott; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b5", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Long and Short Papers", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b6", "title": "T-REx: A large scale alignment of natural language with knowledge base triples", "journal": "", "year": "2018", "authors": "Hady Elsahar; Pavlos Vougiouklis; Arslen Remaci; Christophe Gravier; Jonathon Hare; Frederique Laforest; Elena Simperl"}, {"ref_id": "b7", "title": "REALM: retrievalaugmented language model pre-training", "journal": "", "year": "2020", "authors": "Kelvin Guu; Kenton Lee; Zora Tung; Panupong Pasupat; Ming-Wei Chang"}, {"ref_id": "b8", "title": "Language models as knowledge bases: On entity representations, storage capacity, and paraphrased queries", "journal": "Computing Research Repository", "year": "2020", "authors": "Benjamin Heinzerling; Kentaro Inui"}, {"ref_id": "b9", "title": "Universal language model fine-tuning for text classification", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Jeremy Howard; Sebastian Ruder"}, {"ref_id": "b10", "title": "XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation", "journal": "PMLR", "year": "2020-07", "authors": "Junjie Hu; Sebastian Ruder; Aditya Siddhant; Graham Neubig; Orhan Firat; Melvin Johnson"}, {"ref_id": "b11", "title": "Leveraging passage retrieval with generative models for open domain question answering. ArXiv, abs", "journal": "", "year": "1282", "authors": "Gautier Izacard; E Grave"}, {"ref_id": "b12", "title": "X-FACTR: Multilingual factual knowledge retrieval from pretrained language models", "journal": "", "year": "2020", "authors": "Zhengbao Jiang; Antonios Anastasopoulos; Jun Araki; Haibo Ding; Graham Neubig"}, {"ref_id": "b13", "title": "How can we know what language models know", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020-06", "authors": "Zhengbao Jiang; Frank F Xu"}, {"ref_id": "b14", "title": "BERT-kNN: Adding a kNN search component to pretrained language models for better QA", "journal": "Association for Computational Linguistics", "year": "2020-11-20", "authors": "Nora Kassner; Hinrich Sch\u00fctze"}, {"ref_id": "b15", "title": "Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly", "journal": "", "year": "2020", "authors": "Nora Kassner; Hinrich Sch\u00fctze"}, {"ref_id": "b16", "title": "Pre-training via paraphrasing", "journal": "", "year": "2020-12-06", "authors": "Mike Lewis; Marjan Ghazvininejad; Gargi Ghosh; Armen Aghajanyan; Sida Wang; Luke Zettlemoyer"}, {"ref_id": "b17", "title": "MLQA: Evaluating cross-lingual extractive question answering", "journal": "", "year": "2020", "authors": "Patrick Lewis; Barlas Oguz; Ruty Rinott; Sebastian Riedel; Holger Schwenk"}, {"ref_id": "b18", "title": "MKQA: A linguistically diverse benchmark for multilingual open domain question answering. CoRR, abs", "journal": "", "year": "2007", "authors": "Shayne Longpre; Yi Lu; Joachim Daiber"}, {"ref_id": "b19", "title": "Babelnet: Building a very large multilingual semantic network", "journal": "", "year": "2010-07-11", "authors": "Roberto Navigli; Simone Paolo Ponzetto"}, {"ref_id": "b20", "title": "Deep contextualized word representations", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Matthew Peters; Mark Neumann; Mohit Iyyer; Matt Gardner; Christopher Clark; Kenton Lee; Luke Zettlemoyer"}, {"ref_id": "b21", "title": "How context affects language models' factual predictions", "journal": "", "year": "2020", "authors": "Fabio Petroni; Patrick Lewis; Aleksandra Piktus; Tim Rockt\u00e4schel; Yuxiang Wu; Alexander H Miller; Sebastian Riedel"}, {"ref_id": "b22", "title": "Language models as knowledge bases?", "journal": "Association for Computational Linguistics", "year": "2019-11-03", "authors": "Fabio Petroni; Tim Rockt\u00e4schel; Sebastian Riedel; S H Patrick; Anton Lewis; Yuxiang Bakhtin; Alexander H Wu;  Miller"}, {"ref_id": "b23", "title": "E-BERT: Efficient-yet-effective entity embeddings for BERT", "journal": "", "year": "2020", "authors": "Nina Poerner; Ulli Waltinger; Hinrich Sch\u00fctze"}, {"ref_id": "b24", "title": "Conceptnet 5.5: An open multilingual graph of general knowledge", "journal": "AAAI Press", "year": "2017", "authors": "Robyn Speer; Joshua Chin; Catherine Havasi"}, {"ref_id": "b25", "title": "2020. K-adapter: Infusing knowledge into pre-trained models with adapters", "journal": "", "year": "", "authors": "Ruize Wang; Duyu Tang; Nan Duan; Zhongyu Wei; Xuanjing Huang; Jianshu Ji; Guihong Cao; Daxin Jiang; Ming Zhou"}, {"ref_id": "b26", "title": "Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model", "journal": "", "year": "2020-04-26", "authors": "Wenhan Xiong; Jingfei Du; William Yang Wang; Veselin Stoyanov"}, {"ref_id": "b27", "title": "Giappone (92) P101: \"[X] works in the field of [Y]\" art (205), science (135) Kunst (384), Film (64) psychologie (263), kunst (120) fisiologia (168), caccia (135) P106", "journal": "WW", "year": "2016", "authors": "Yadollah Yaghoobzadeh; Hinrich Sch\u00fctze"}, {"ref_id": "b28", "title": "Microsoft (177), IBM (55) Microsoft (153), Apple (99) Microsoft (200), Nintendo (69) Microsoft (217)", "journal": "Apple", "year": "", "authors": ""}, {"ref_id": "b29", "title": "Swan (32) EMI (202), Paramount Records (59) EMI (225), Swan (50) EMI (217)", "journal": "", "year": "", "authors": ""}, {"ref_id": "b30", "title": "FIFA (126), NATO (33) FIFA (118), NATO (38) FIFA (157), WWE (16) FIFA (121)", "journal": "NATO", "year": "", "authors": ""}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: x-axis is the number of translated triples, yaxis the number of languages. There are 39,567 triples in the original LAMA (TREx and GoogleRE).", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Distribution of p1 scores for 53 languages in UnTyQ vs. TyQ. Left: singletoken (object = 1 token). Right: multitoken (object > 1 token).", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure3: p1 of BERT (red) vs mBERT[x]  (blue) divided by p1 of mBERT[en] on the same set of triples in each language x. mBERT captures less factual knowledge than monolingual English BERT. While performance is reasonable for 21 languages, it is below 60% for 32 languages. Dashed line is rel-p1 of mBERT[en]  (by definition equal to 1.0). Performance of BERT varies slightly as the set of triples is different for each language. Note that the Wikipedia of Cebuano (ceb) consists mostly of machine translated articles.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Three randomly sampled data entries from mLAMA per language. Due to the automatic generation of the dataset not all of them are fully correct.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 6 :6Figure 6: Scatter plot of p1 TyQ and number of articles in the corresponding Wikipedia. There is no clear trend visible.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Effect of manual template modification on Un-TyQ. Shown is p1, number of templates modified (in brackets). Templates are modified to correct mistakes from machine translation and paraphrased to achieve the correct object type. Manual template correction has a small effect on UnTyQ.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "p1 for BERT, mBERT queried in English, mBERT pooled on LAMA and LAMA-UHN.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "p(e|t) = 1 l l i=1 p(m i = i |t l ).", "formula_coordinates": [3.0, 116.03, 481.02, 130.22, 33.71]}], "doi": "10.18653/v1/2020.acl-main.421"}