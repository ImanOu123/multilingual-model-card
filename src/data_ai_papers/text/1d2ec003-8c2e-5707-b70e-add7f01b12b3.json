{"title": "Efficient Estimation for High Similarities using Odd Sketches", "authors": "Michael Mitzenmacher; Rasmus Pagh; Ninh Pham", "pub_date": "", "abstract": "Estimating set similarity is a central problem in many computer applications. In this paper we introduce the Odd Sketch, a compact binary sketch for estimating the Jaccard similarity of two sets. The exclusive-or of two sketches equals the sketch of the symmetric difference of the two sets. This means that Odd Sketches provide a highly spaceefficient estimator for sets of high similarity, which is relevant in applications such as web duplicate detection, collaborative filtering, and association rule learning. The method extends to weighted Jaccard similarity, relevant e.g. for TF-IDF vector comparison. We present a theoretical analysis of the quality of estimation to guarantee the reliability of Odd Sketch-based estimators. Our experiments confirm this efficiency, and demonstrate the efficiency of Odd Sketches in comparison with b-bit minwise hashing schemes on association rule learning and web duplicate detection tasks.", "sections": [{"heading": "INTRODUCTION", "text": "Estimating set similarities is a fundamental problem in databases, machine learning, and information retrieval. Given the two sets, S1 and S2, where S1, S2 \u2286 \u2126 = {0, 1, . . . , D \u2212 1}, a challenge is how to quickly compute their Jaccard similarity coefficient J, a normalized measure of set similarity:\nOne can view large datasets of Web documents as collections of sets where sets and set elements correspond to documents and document words/shingles, respectively. Other examples are datasets encountered in recommender systems, where users and items can be viewed as sets and set elements. Hence, set similarity estimation is one of the key research challenges in many application areas, such as web duplicate detection [4,6,12,18], collaborate filtering [1,9], and association rule learning [8].\nMany applications of set similarity arise in large-scale datasets. For instance, a search engine needs to crawl and index billions of web-pages. Collaborative filtering engines from sites such as Amazon or NetFlix have to deal with tens of millions of users' data. Performing similarity search over such large-scale datasets is very time-consuming. If we are willing to accept an estimate of J it turns out that it is possible to get by with much less computation and storage. But how much better can it get? In this paper we address the following question:\nIf each set S is summarized in a data structure D(S) of n bits, how precise an estimate of J(S1, S2) is it possible to make based on D(S1) and D(S2)?\nOur main finding is that existing solutions, while highly efficient in general, are not optimal when J is close to 1. We present a novel solution, the Odd Sketch, that yields improved precision in the high similarity regime.\nAlthough the setting where J is close to 1 has not often been the primary focus when studying similarity measures, there are many applications where this regime is important. Consider a setting where the goal is not just to find a similar item, but to provide a short list and ranking of the most similar items. For example, in the setting of document similarity, in a sufficiently rich environment there may be hundreds of documents quite similar to another document, and the user wants to see the top ten. For such applications, we require methods that are very accurate for high similarity values J.", "publication_ref": ["b3", "b5", "b11", "b17", "b0", "b8", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Minwise Hashing Schemes", "text": "Because minwise hashing is a building block of our approach, and because b-bit minwise hashing is our primary alternative for comparison, we review both briefly.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Minwise Hashing", "text": "Minwise hashing is a powerful algorithmic technique to estimate set similarities, originally proposed by Broder et al. [4,5]. It was used to detect and cluster similar documents in the early AltaVista search engine [6]. Since then, Given a high Jaccard similarity J and two minhashes S1, S2, we expect that |S1 \u2229 S2| = Jn (filled space) and |S1\u2206S2| = 2(1 \u2212 J)n (white space). Due to the same independent hash values in the filled space, the error of the b-bit scheme corresponds to the error of the estimate of |S1\u2206S2|. Inaccuracy in just a few bit positions in the white space will yield a large relative error of the estimate of J.\nthe scheme has been applied successfully in a variety of applications, including similarity search [4,5,6], association rule learning [8], compressing social networks [7], advertising diversification [11], tracking Web spam [21], web duplicate detection [15], large-scale learning [16], and more [1,3,14].\nWe now briefly review Broder's minwise hashing scheme. Given a random permutation \u03c0 : \u2126 \u2192 \u2126, the Jaccard similarity of S1 and S2 is\nJ(S1, S2) = Pr[min(\u03c0(S1)) = min(\u03c0(S2))].\nTherefore we get an estimator for J by considering a sequence of permutations \u03c01, . . . , \u03c0 k and storing the annotated minimum values (called \"minhashes\").\nS1 = {(i, min(\u03c0i(S1))) | i = 1, . . . , k}, S2 = {(i, min(\u03c0i(S2))) | i = 1, . . . , k}.\nWe estimate J by the fraction\u0134 = |S1 \u2229 S2|/k. This estimator is unbiased, and by independence of the permutations it can be shown that Var\n[\u0134] = J(1\u2212J) k .\nObserve that a minhash can be stored as an array of length k containing the minimum for each i = 1, . . . , k. The hash value min(\u03c0(S)) in the minhash is stored as an integer of typically 32 or 64 bits. That means that Broder's scheme might use 32k or 64k bits of memory to store k hash values for any set S.", "publication_ref": ["b3", "b4", "b5", "b3", "b4", "b5", "b7", "b6", "b10", "b20", "b14", "b15", "b0", "b2", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "b-bit Minwise Hashing", "text": "At WWW'10 Li and K\u00f6nig [15] proposed b-bit minwise hashing as a space-efficient variant of Broder's minwise hash-n bits\nodd(S1) S1 xxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxx xx xx S2 odd(S2) xxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxx x 1 xx xx 1 xx xx h(x) h(x) x' x'' 1 h(x') x x x 1 h(x'')\nDifferent independent hash values Same independent hash values Figure 2: Illustration of the Odd Sketch construction. Odd Sketch starts with a 0s bit-vector of size n. We flip a bit according to each element of the minhashes S1 and S2. The contributions of elements in S1 \u2229S2 cancel out in the exclusive-or odd(S1)\u2295odd(S2), so Odd Sketches use all of the n bits to estimate the symmetric difference size |S1\u2206S2|. This reduces the variance when J is close to 1.\ning scheme. Instead of storing b = 32 or b = 64 bits for each permutation, this approach suggested using the lowest b bits. It is based on the intuition that the same hash values give the same lowest b bits whereas the different hash values give different lowest b bits with probability 1 \u2212 1/2 b . Figure 1 shows how to construct b-bit minwise sketches.\nLet min b (\u03c0(S)) denote the lowest b bits of the hash value min(\u03c0(S)). Then the b-bit minhash S b 1 is obtained from the standard minhash S1 by replacing min by min b , reducing space usage to kb. An unbiased estimator\u0134 b for J(S1, S2) and its variance can be computed as follows:\nJ b = |S b 1 \u2229 S b 2 |/k \u2212 1/2 b 1 \u2212 1/2 b , Var[\u0134 b ] = 1 \u2212 J k J + 1 2 b \u2212 1 .\nHowever, when the Jaccard similarity is high it seems that the b-bit scheme offers less information than it might be able to. As an extreme example, suppose that we get the estimat\u00ea J b = 1, i.e., all the bits of the two summaries are identical. How confident can we be that J is indeed close to 1? For example, even if we actually have J = 1 \u2212 2 k it is quite likely that the two summaries will be identical. Somehow, since the two summaries are so highly correlated, differences of just a few bit positions will lead to very different conclusions on how close J is to 1. Thus we might ask: Is it possible to do better, avoiding the limits on accuracy that comes when summaries are highly correlated?", "publication_ref": ["b14"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Our contribution", "text": "In this paper, we introduce the Odd Sketch, a compact binary sketch for estimating the Jaccard similarity of two sets. This binary sketch is similar to a Bloom filter with one hash function, constructed on the original minhashes with the \"odd\" feature that the usual disjunction is replaced by an exclusive-or operation. That is, we hash each element of the minhash into a bit-array data structure. (We will refer to the hash function used for this as the \"sketch hash function\".) Now, instead of setting a bit to 1, we flip a bit according to the sketch hash value of each element in the minhash. We apply the Odd Sketches to minhashes, which means that the Odd Sketch records for each hash value whether it is mapped to by an odd number of elements in the minhash. Figure 2 shows a high level illustration of the construction of Odd Sketches.\nA key feature of the Odd Sketch is that when we compute the exclusive-or of two Odd Sketches, the result will be equal to the Odd Sketch of the symmetric difference of the minhashes, i.e., the set of elements in one minhash but not the other. This is because the contribution of all identical elements in the minhashes cancel out. In turn, this means that we are able to base Odd Sketches of size n on minhashes of size significantly above n whenever J is close to 1 and there are many identical values in the minhashes, so that the variance induced by the minhash step is reduced.\nThe technical difficulty is to provide a good estimator for the size of a set based on its Odd Sketch. We provide a surprisingly simple, asymptotically precise expression for the expected fraction of 1s in an Odd Sketch, and show via concentration around this expectation that the resulting estimator has good precision as long as the fraction of 1s is bounded away from 1/2.\nWe note that a similar approach has previously been used to estimate the number of distinct elements in a stream, where the usual disjunction was used instead of an exclusiveor operation [2]. One of our contributions is showing that tracking the parity of the number of items that hash to a bucket is a useful technique in the context of estimating the size of set differences (rather than the size of sets).", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "ODD SKETCHES", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Construction", "text": "The Odd Sketch is a simple, linear sketch of the indicator vector of a set S. Concretely, the sketch consists of an array s of n > 2 bits. Let h : \u2126 \u2192 [n] be a hash function, which we assume here is fully random. In the Odd Sketch, which we denote by odd(S), the ith bit is given by\nsi = \u2295 x\u2208S 1 h(x)=i ,\nwhere 0 \u2264 i < n. That is, si is the parity of the number of set items that hash to the ith location.\nTo compute the sketch, we start with the zero bit vector of size n. Then we evaluate h on each x \u2208 S, and flip the bit s h(x) of the sketch, as shown in the pseudo-code in Algorithm 1.\nBecause odd(S) records the parity of the number of elements that hash to a location, it follows that the Odd Sketch of the symmetric set difference S1\u2206S2 is the exclusive-or of the Odd Sketches odd(S1) and odd(S2).\nLemma 1. odd(S1) \u2295 odd(S2) = odd(S1\u2206S2).\nIn the following section, we show how to estimate the size of a set from the number of 1s in its Odd Sketch. By Algorithm 1 Odd Sketch(S, n) Require: A set S \u2282 \u2126 and the size of sketch in bits n 1: s \u2190 [0] n 2: Pick a random hash function h : \u2126 \u2192 [n] 3: for each set element x \u2208 S do 4: s h(x) = s h(x) \u2295 1 5: end for 6: return s Lemma 1 we can use this to estimate |S1\u2206S2| from the Odd Sketches of S1 and S2. If sets S1 and S2 are minhashes of S1 and S2, then we can estimate the Jaccard similarity of original two sets from the Odd Sketches of S1 and S2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Estimation", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Estimating a set's size from its Odd Sketch", "text": "Let m and n denote the size of set S and the size of odd(S) in bits, respectively. Because our hash functions are fully random, we can think about the process of constructing odd(S) as that of independently throwing m balls into n bins, and storing as si the parity of the number of balls in bin i. We are interested in generating an estimat\u00ea m for m based on the observation of the number of odd bins in odd(S). In the following we present two estimation approaches for the estimatem. The first one is based on the Markov chain model and the second one relies on the standard Poisson approximation to the balls and bins setting. Both approaches yield the same estimate when n is sufficiently large.\nConsider the parity of number of balls landing in any specific bin (say the first) as a simple two-state Markov chain model. The first/second state corresponds to the even/odd parity. The probability of changing states is 1/n. Let pi be the probability that any specific bin has an odd number of balls after i balls have been thrown. A simple induction yields\npi = 1 \u2212 (1 \u2212 2/n) i 2 .\n(This is a standard elementary problem in Markov chains.) It helps to now introduce some notation. Let Xi be a 0-1 random variable corresponding to the parity of the number of balls that land in the ith bin after throwing m balls, and let X = i Xi. We have shown that\nE[X] = n 1 \u2212 (1 \u2212 2/n) m 2 .\nHence, a seemingly reasonable approximation for m if we see z odd bins in the sketch is to assume that z \u2248 E[X], in which case\nz \u2248 n 1 \u2212 (1 \u2212 2/n) m 2 ,\nand solving we obtain an estimatem b\u0177\nm = ln (1 \u2212 2z/n) ln (1 \u2212 2/n) . (1\n)\nThis approximation is reasonable if X is sharply concentrated around its expectation, which we show later.\nThe second estimation approach leverages the standard Poisson approximation to the balls and bins setting and provides a pratical estimate. That is, when m balls are thrown into n bins, this is very approximately the same as independently giving each bin a number of balls that is Poisson distributed with mean \u00b5 = m/n. (We discuss this further below; also, see [19,Section 5.4].) Lemma 2 provides the relationship between the Poisson distribution with mean \u00b5 and the parity of the distribution. [20]) Let Q be a random variable that has Poisson distribution with mean \u00b5. The probability p that Q is odd is (1 \u2212 e \u22122\u00b5 )/2.", "publication_ref": ["b18", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Lemma 2. (Schuster and Philippou", "text": "Proof. The probability that Q is odd is given by\np = i odd e \u2212\u00b5 \u00b5 i i! = e \u2212\u00b5 i odd \u00b5 i i! = e \u2212\u00b5 e \u00b5 \u2212 e \u2212\u00b5 2 = 1 \u2212 e \u22122\u00b5 2 .\nLet Yi be the parity of the number of balls that land in the ith bin in the setting where the number of balls are independently Poisson distributed with mean \u00b5 = m/n in each bin, and let Y = i Yi be the number of bins with an odd number of balls. Then\nE[Y ] = np = n 1 \u2212 e \u22122m/n 2 .\nHence, a seemingly reasonable approximation for m if we see z odd bins in the sketch is to assume that z \u2248 E[Y ], in which case we obtain an estimatem a\u015d\nm = \u2212 n 2 ln (1 \u2212 2z/n) .(2)\nSince the Yi are independent and identically distributed, standard Chernoff bounds provide that z \u2248 E[Y ] with high probability, as we clarify further below. We note that when n is sufficiently large in practice, we have ln (1 \u2212 2/n) \u2248 \u22122/n. In this case, the estimate is approximately the estimate derived from equation 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Estimating Jaccard similarity from Odd Sketches", "text": "Suppose we construct Odd Sketches odd(S1), odd(S2) from the minhashes S1 and S2 derived from S1 and S2. Recall that, when we construct sets S1 and S2, if we think of the sets as random variables before instantiating them, we have\nE[|S1\u2206S2|] = 2k(1 \u2212 J),\nwhere k is the number of independent permutations and J is the Jaccard similarity of S1 and S2. Moreover, |S1\u2206S2| should be closely concentrated around its expectation, since each permutation independently gives a match with probability J. Once we have instantiated S1 and S2, given odd(S1) and odd(S2), we can estimate |S1\u2206S2| for the S1 and S2 we derived, using equation 2. For notational convenience we will think of odd(S1) and odd(S2) as the sets of bit positions containing 1, which means that their exclusive-or corresponds exactly to the symmetric difference. If we us\u00ea |S1\u2206S2| to denote our estimate of |S1\u2206S2|, then\n|S1\u2206S2| = \u2212 n 2 ln(1 \u2212 2 |odd(S1)\u2206odd(S2)|/n).\nHere |odd(S1)\u2206odd(S2)| refers to the number of 1s in the structure. Using|S1\u2206S2| as a proxy for E[|S1\u2206S2|], the Jaccard similarity can be estimated as follows:\nJ odd = 1 \u2212| S1\u2206S2| 2k = 1 + n 4k ln 1 \u2212 2 |odd(S1)\u2206odd(S2)| n .\nBoth Odd Sketches and b-bit minwise hashing can be viewed as variations of the original minwise hashing scheme that reduce the number of bits used. The quality of their estimators is dependent on the quality of the original minwise estimators. In practice, both Odd Sketches and b-bit minwise hashing need to use more permutations but less storage space than the original minwise hashing scheme.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Analysis", "text": "In the previous section, we assumed that the number of odd bins in our data structure was closely concentrated around its expectation to justify various approximations. Here we justify this assumption. This is straightforward in the Poisson setting where bin parities are independent; we show how to handle the dependencies that exist in the balls-and-bins model. We also directly calculate the variance of the number of odd bins for both the Poisson and balls-and-bins setting.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Concentration", "text": "Recall our notation: we throw m = \u00b5n balls into n bins, so that the average number of balls per bin is \u00b5. We let Xi be the parity of the number of balls that land in the ith bin and X = i Xi be the number of odd bins. Similarly, let Yi be the parity of the number of balls that land in the ith bin in the setting where the number of balls are independently Poisson-distributed, and let Y = i Yi. We show that X and Y are closely concentrated around their means.\nWe use the standard approach of passing to the setting where each bin obtains independently a Poisson distributed number of balls with mean \u00b5. This is justified by, for example, [19,Corollary 5.9] where the following is shown: Lemma 3. [Corollary 5.9 of [19]] Any event that takes placed with probability p when each bin obtains an independently distributed Poisson number of balls with mean \u00b5 takes place with probability at most pe \u221a m when m = \u00b5n balls are thrown into n bins.\nSince Y is the sum of independent 0-1 random variables, by applying a Chernoff bound [19,Exercise 4.13] to Y , we have:\nPr(|Y \u2212 E[Y ]| \u2265 n) \u2264 2e \u22122n 2 .\nHence, from Lemma 3 we have\nPr(|X \u2212 E[Y ]| \u2265 n) \u2264 (2e \u221a m)e \u22122n 2 .\nDenote byX = 1 n X the fraction of bins with an odd number of balls. We find\nPr(|X \u2212 1 n E[Y ]| \u2265 ) \u2264 (2e \u221a m)e \u22122n 2 , Pr(|X \u2212 p| \u2265 ) \u2264 (2e \u221a m)e \u22122n 2 , where p = 1 \u2212 e \u22122m/n /2. The true expected fraction of odd bins is E[X]/n = 1\u2212(1\u22122/n) m 2\n, which differs from p by an o(1) amount.\nSince m corresponds to the symmetric difference between two minhashes, we have m \u2264 2k. Hence, by choosing n > c \u22122 log k for some constant c, our estimator closely concentrates around its mean with probability 1 \u2212 k \u2212\u03c9(1) .", "publication_ref": ["b18", "b18", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Variance bound", "text": "We note that the variance on the number of odd bins for the Poisson setting is trivial to calculate, since the bins are independent. Letting p = 1 \u2212 e \u22122m/n /2, the standard result (on variance of biased coin flips) gives that the variance is np(1 \u2212 p).\nFor the balls and bins case there are dependencies among the bin loads that make the variance calculation more difficult. Recall that pi is the probability that any specific bin has an odd number of balls after i balls have been thrown, and\npi = 1 \u2212 (1 \u2212 2/n) i 2 , so each Xi = 1 with probability (1 \u2212 (1 \u2212 2/n) m )/2.\nTo calculate the variance, we first calculate E[X 2 ]; the standard expansion gives\nE[X 2 ] = E[( i Xi) 2 ] = i E[X 2 i ] + 2 i<j E[XiXj] = i E[Xi] + 2 i<j E[XiXj].\nwhere we have used the fact that (Xi) 2 = Xi since Xi only takes on the values 0 and 1. The first summation is just E[X].\nTo calculate the second summation, by symmetry it suffices to consider a specific pair of variables, say X1 and X2. We consider the total number of balls that land in the combination of bins 1 and 2. If this number is odd, then clearly X1X2 = 0. If this number is 0, then clearly X1X2 = 0. If this number is even, but more than 0, then X1X2 = 1 with probability exactly 1/2. To see this, consider the last ball that lands in either bin 1 or bin 2. One of these bins must have an odd number of balls. If the new ball lands in the other bin, then both have an odd number of balls; this happens with probability 1/2. It follows that E[X1X2] is half the probability that bins 1 and 2 considered together obtain an even and positive number of balls. As with the calculation for pi, a simple induction based on the two-state Markov chain model shows that after i balls have been thrown, the probability qi that the first two bins have an even number of balls greater than 0 is\nqi = 1 + (1 \u2212 4/n) i \u2212 2(1 \u2212 2/n) i 2 .\nHence the second sum is\nn 2 1 + (1 \u2212 4/n) m \u2212 2(1 \u2212 2/n) m 2 .\nThe variance is then\nE[X 2 ] \u2212 E[X] 2 , or n 2 1 + (1 \u2212 4/n) m \u2212 2(1 \u2212 2/n) m 2 + n(1 \u2212 (1 \u2212 2/n) m ) 2 \u2212 n(1 \u2212 (1 \u2212 2/n) m ) 2 2 .\nSimplifying, this is\nn 2 (1 \u2212 4/n) m \u2212 (1 \u2212 2/n) 2m 4 + n 1 \u2212 (1 \u2212 4/n) m 4 .\nWhile this is easily seen to be O(n 2 ), the coefficient\n(1 \u2212 4/n) m \u2212 (1 \u2212 2/n) 2m 4\nof the n 2 term above is in fact O(1/n 2 ) when m = \u00b5n. (Note that both expressions in the numerator converge to and are approximately e \u22124m/n . By examining the asymptotics carefully one can show the coefficient is O(1/n 2 ).) Hence the variance here is also O(n). Indeed, the second term is\nn 1 \u2212 (1 \u2212 4/n) m 4 \u2248 n 1 \u2212 e \u22124m/n 4 = np(1 \u2212 p),\nwhich is the variance for the Poisson setting, and the first term is negative. Again, by considering the asymptotic expansions carefully one obtains that the variance in the Poisson case is larger than in the case where there are exactly m balls thrown for large enough n, as one might naturally expect.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Accuracy of the estimator", "text": "In the previous sections we bounded the variance and gave strong tail bounds for the fraction z/n of 1s in an Odd Sketch. Recall that its expected value is pm = 1\u2212(1\u22122/n) m 2 derived from the Markov chain and its practical estimate is p = 1\u2212e \u22122m/n 2 derived from the Poisson approximation. What remains is to bound the error resulting from applying the estimator from equation (2), repeated here for convenience:\nm = \u2212 n 2 ln (1 \u2212 2z/n) .\nDefining the function f (x) = \u2212 n 2 ln(1 \u2212 2x), we havem = f (z/n). There are two sources of inaccuracy: The first is that the estimator has a bias since the expected number of 1s, npm = n 1\u2212(1\u22122/n) m 2 , differs from the practical estimate np = n 1\u2212e \u22122m/n 2 . However, it can be confirmed by an easy computation that the difference can be at most 1, so this is not significant.\nThe second, and more significant, source of error is that when z/n deviates from its expectation pm, f (z/n) will deviate from f (pm). Informally, an if z/n deviates from its expectation by \u03b5, this will give an error of roughly f (z/n) \u2022 \u03b5, as long as \u03b5 is small enough, where f (x) = n 1\u22122x is the derivative of f . It is clear that a small error can be magnified significantly if z/n is close to 1/2, since f (x) goes to infinity as x \u2192 1/2. Therefore we choose parameters such that p, the practical estimate of z/n, is bounded away from 1/2 (p \u2248 0.3 gives the best results, as we see when we discuss our experiments). By the results in section 2.3.1 this means that with high probability (wrt. n) we will have z/n < 0.4 (say). As long as this is the case, since f is monotonely increasing, we have that the error is bounded by f (0.4)|z/n \u2212 pm| = 5n. This bound is pessimistic, but shows that the estimation error is (with high probability) proportional to the error in the estimate of pm. In turn, this implies that the variance of the estimator is O(n).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Weighted similarity", "text": "Odd Sketches work with any notion of similarity that can be transformed to Hamming distance of two vectors. In particular, it works with any similarity measure that can be captured using the probability that two minhashes are identical. For example, the Jaccard similarity of two vectors v, w \u2208 R d with nonnegative entries can be defined as: J(v, w) = i min(vi, wi) i max(vi, wi) , generalizing standard Jaccard similarity which corresponds to 0-1 vectors. Hash functions that result in minhash equality with probability J(v, w) can be found in [10,13,17].", "publication_ref": ["b9", "b12", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENTAL RESULTS", "text": "We implemented b-bit minwise hashing and Odd Sketch in Matlab, and conducted experiments on a 2.67 GHz Core i7 Windows machine with 3GB of RAM. We compared the performance of b-bit minwise hashing and Odd Sketch on association rule learning and web duplication detection tasks. All results are the averages of 10 runs of the algorithms.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Parameter setting", "text": "It is obvious that the performance of both b-bit minwise and Odd Sketch depends on the number of independent permutations used in the original minwise hashing scheme. The b-bit minwise scheme uses k b = n/b permutations where the storage space is n bits and b \u2265 1 is the number of bits per permutation. Since larger k b provides higher accuracy, setting b = 1 turns out to achieve the smallest variance, as will be clear from our empirical evaluation.\nIn the Odd Sketch setting, the number of independent permutations k odd is dependent on the sketch size n and the user-defined similarity threshold J0. Typically, we are interested in retrieving pairs of sets such that J > J0 (and perhaps subject these pairs to additional filtering). Moreover, we want to choose k odd as large as possible to reduce the error from the original minwise hashing step. It seems difficult to mathematically establish the optimal way of choosing k odd , but we conducted experiments that indicate that the smallest variance is achieved when the exclusive-or of two odd sketches with similarity J0 contains around 30% 1s.\nOur estimator needs the fraction of 1s in odd(S1\u2206S2) to be smaller than 1/2. If a fraction of more than 1/2 is observed this is (with high probability) a sign of very low Jaccard similarity, so we may estimate J = 0. Recall that the process of constructing odd(S1\u2206S2) corresponds to throwing |S1\u2206S2| balls into n bins. It turns out that if we choose k odd such that |S1\u2206S2| \u2248 n/2 we get the most accurate estimate when similarity is around J. In other words, we choose the parameter k odd such that the ratio\n\u03b1 = 2k odd (1\u2212J 0 ) n \u2248 1 2 .\nWe conducted experiments to evaluate this choice of ratio \u03b1. We compared the mean square error (MSE, incorporating both variance and bias) of our estimator\u0134 odd for different ratios of \u03b1 = 2k odd (1\u2212J 0 ) n in [0.25, 1], and for different sketch sizes n \u2208 [500, 1000] bits. For each choice we found that \u03b1 = 1/2 gave the smallest observed MSE. Figure 3  the average MSE of\u0134 odd , averaged over variety of values of J and n. It illustrates that Odd Sketch achieves the highest accuracy when using the ratio \u03b1 = 0.5. So we can choose k odd = n 4(1\u2212J 0 ) given a threshold J0. When we are interested in J0 \u2265 0.75, we can set k odd > n. This means that Odd Sketch can use more independent permutations than b-bit schemes. In fact, even for the inferior choice of k odd = n, Odd Sketch can achieve better performance than 1-bit minwise hashing when J0 > 0.75.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Accuracy of Estimation", "text": "This subsection presents experiments to further evaluate the accuracy of our estimation algorithm. We carried out experiments to compare the accuracy of b-bit minwise hashing and Odd Sketch. In the b-bit schemes, we set k b = n/b to achieve a space usage of n bits. For the Odd Sketch, we set k odd = n 4(1\u2212J) . We again measured the mean square error (MSE) of estimators of both approaches. We varied n in {512, 1024} bits and conducted experiments on synthetic datasets. (But note that since we apply hashing the outcome is independent of the particular set elements, and we expect the same result on any real-life dataset.) This dataset is very high-dimensional (D = 10, 000) and highly sparse (sparsity > 99.9%).\nFigure 4 shows the negative log of MSE (\u2212 log (MSE)) of estimators of the two approaches for different values J. We note that the MSE is always smaller than 1 in our experiments, so larger \u2212 log (MSE) is better. For high Jaccard similarities J \u2265 0.8, Odd Sketch provides a smaller error than the b-bit minwise approach. The difference is more dramatic when J is very high because Odd Sketch makes use of a larger number of independent permutations than the b-bit minwise schemes. This figure also shows that the 1-bit scheme has superior performance compared to the b-bit schemes for b > 1. We note that b-bit schemes for b > 1 require additional bit-manipulation to pack b bits of hash values into 64-bit (or 32-bit) words. In contrast, both Odd Sketch and 1-bit schemes only need the XOR and bitcounting operations to compare two summaries.\nOne might argue that Odd Sketch requires a more expensive preprocessing step than b-bit minwise hashing due to the use of larger number of permutations in the minwise hashing step. But even with k odd = n, where the hashing cost is identical to that of 1-bit minwise hashing, Odd Sketch  still provides better accuracy when J > 0.75, as shown in Figure 5.\nWhen the target similarity is very high, the authors of b-bit minwise hashing also discussed the idea of combining any 2 bits of a 1-bit minhash by XOR operations to increase the amount of information in each bit. This approach is called 1 2 -bit minwise hashing, and similar to Odd Sketch has a nonlinear estimator. The 1 2 -bit scheme uses k 1 2 = 2n permutations.\nWe carried out experiments to compare the mean square errors of estimators of Odd Sketch and 1 2 -bit minwise hashing, as shown in Figure 6. The figure shows that Odd Sketch achieves a considerably smaller error than 1 2 -bit minwise hashing when J > 0.85 for both choices of k. It also shows that Odd Sketch with the best choice of k odd provides higher accuracy than for k odd = 2n.\nWe conclude the accuracy evaluation of Odd Sketch by depicting the empirical cumulative distribution function (cdf) of estimators. Figure 7 shows the empirical cdfs of Odd Sketch, 1-bit scheme and 1 2 -bit scheme on 10,000 estimators of the Jaccard similarity J = 0.9. The slope of cdf of \nk odd = k 1 2 = 2n.\nOdd Sketch is steeper than that of 1 2 -bit scheme and significantly steeper than that of 1-bit scheme. This means that Odd Sketch provides superior performance compared to b-bit minwise hashing when the target similarity is high.  2 -bit scheme with J = 0.9.", "publication_ref": ["b0", "b0", "b0", "b0"], "figure_ref": ["fig_2", "fig_3", "fig_6"], "table_ref": []}, {"heading": "Association Rule Learning", "text": "Cohen et al. [8] used minwise hashing to generate the candidate sets of high Jaccard similarity in the context of learning pairwise associations. This subsection compares the performance of Odd Sketch and b-bit schemes in this setting. Since b = 1 provides the highest accuracy among b \u2265 1, we only used the 1-bit scheme in our experiment. For a more clear comparison, we used the same number of permutations for the two approaches. We measured the precision-recall ratio of both approaches on detecting the pairwise items that have Jaccard similarity larger than a  Figure 8 shows the precision-recall ratio of the Odd Sketch and the 1-bit scheme. For the high target threshold J0 = 0.9, the Odd Sketch provides significantly higher precision and recall ratio (up to 10% better) than 1-bit minwise hashing. For J0 = 0.8, the Odd Sketch is still better in precision but slightly worse in recall.\nFigure 9 demonstrates the superiority of Odd Sketch compared to 1 2 -bit minwise hashing with respect to precision. The Odd Sketch achieved up to 20% higher precision while providing similar recall. 2 -bit scheme on the mushroom dataset with J0 = 0.9.\n1 http://fimi.ua.ac.be/data/", "publication_ref": ["b7", "b0"], "figure_ref": ["fig_8", "fig_9"], "table_ref": []}, {"heading": "Web Duplicate Detection", "text": "In this experiment, we compare the performance of the two approaches on web duplicate detection tasks on the bag of words dataset 2 . We picked three datasets, including KOS blog entires (D = 6906; N = 3430), Enron Emails (D = 28, 102; N = 39, 861), and NYTimes articles (D = 102, 660; N = 300, 000). We computed all pairwise Jaccard similarities among documents, and retrieved every pair with J > J0. For the sake of comparison, we used the same number of permutations and considered the thresholds J0 = 0.85 and J0 = 0.90. We again used the precision-recall ratio as our standard measure.\nFigures 10 and 11 show the precision-recall ratio for the Odd Sketch and 1-bit minwise hashing on three datasets with J0 = 0.85 and J0 = 0.90, respectively. The Odd Sketch obtains higher relative precision ratio or at least is comparable to 1-bit scheme when J0 = 0.85. It achieves up to 7% and 1% higher than the 1-bit scheme on KOS blog entries and Enron Emails, respectively. For J0 = 0.90, the precision ratios are almost the same on three datasets. However, Odd Sketch greatly outperforms in the recall ratio. The relative recall obtained by the Odd Sketch is approximately 15% higher than the 1-bit scheme on the KOS blog entries when J0 = 0.85 and J0 = 0.90. The difference in relative recall is not significant on the other datasets. These relative gaps are around 5% and less 1% on Enron Emails and NYTimes articles, respectively.\nFigure 12 shows the observed precision-recall graphs of the Odd Sketch and the 1 2 -bit scheme. We again set k odd = k1 = 2n for the sake of fair comparison. Both approaches achieve very high precision (higher than 90%) on the three datasets. The Odd Sketch still obtains higher precision than the 1 2 -bit scheme although the difference is not dramatic. The gap of both schemes in the recall ratio is considerable on KOS blog entries and Enron Emails. The most dramatic difference is around 4% when n = 200. On the NYTimes articles dataset, the ratio curves of both schemes are overlapping when n \u2265 200.", "publication_ref": [], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "CONCLUSION", "text": "In this paper, we proposed the Odd Sketch, a compact binary sketch for estimating the Jaccard similarity of two sets. By combining the minwise hashing technique with a hash table where only the parity of the number of items hashed to bucket is stored, Odd Sketches can be combined with just an exclusive-or operation to allow a simple estimation of the Jaccard similarity that provides a highly space-efficient solution, particularly for the high similarity regime. We presented a theoretical analysis of the quality of estimate. Our experiments on synthetic and real world datasets demonstrate the efficiency of Odd Sketches in comparison with b-bit minwise hashing schemes on association rule learning and web duplicate detection tasks. We expect that there are many other additional applications where Odd Sketches can be similarly applied.    the IT University of Copenhagen. The work of the second and third author is supported by the Danish National Research Foundation under the Sapere Aude program. We thank the anonymous reviewers for their constructive comments and suggestions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "ACKNOWLEDGMENTS", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Sketching techniques for collaborative filtering", "journal": "", "year": "2009", "authors": "Y Bachrach; E Porat; J S Rosenschein"}, {"ref_id": "b1", "title": "Counting distinct elements in a data stream", "journal": "", "year": "", "authors": "Z Bar-Yossef; T Jayram; R Kumar; D Sivakumar; L Trevisan"}, {"ref_id": "b2", "title": "Finding text reuse on the web", "journal": "", "year": "2009", "authors": "M Bendersky; W B Croft"}, {"ref_id": "b3", "title": "On the resemblance and containment of documents. Sequences", "journal": "", "year": "1997", "authors": "A Z Broder"}, {"ref_id": "b4", "title": "Min-wise independent permutations", "journal": "J. Comput. Syst. Sci", "year": "2000", "authors": "A Z Broder; M Charikar; A M Frieze; M Mitzenmacher"}, {"ref_id": "b5", "title": "Syntactic clustering of the web", "journal": "Computer Networks", "year": "1997", "authors": "A Z Broder; S C Glassman; M S Manasse; G Zweig"}, {"ref_id": "b6", "title": "On compressing social networks", "journal": "", "year": "2009", "authors": "F Chierichetti; R Kumar; S Lattanzi; M Mitzenmacher; A Panconesi; P Raghavan"}, {"ref_id": "b7", "title": "Finding interesting associations without support pruning", "journal": "IEEE Trans. Knowl. Data Eng", "year": "2001", "authors": "E Cohen; M Datar; S Fujiwara; A Gionis; P Indyk; R Motwani; J D Ullman; C Yang"}, {"ref_id": "b8", "title": "Google news personalization: scalable online collaborative filtering", "journal": "", "year": "2007", "authors": "A Das; M Datar; A Garg; S Rajaram"}, {"ref_id": "b9", "title": "Exploiting asymmetry in hierarchical topic extraction", "journal": "", "year": "2006", "authors": "S Gollapudi; R Panigrahy"}, {"ref_id": "b10", "title": "An axiomatic approach for result diversification", "journal": "", "year": "2009", "authors": "S Gollapudi; A Sharma"}, {"ref_id": "b11", "title": "Finding near-duplicate web pages: a large-scale evaluation of algorithms", "journal": "", "year": "2006", "authors": "M R Henzinger"}, {"ref_id": "b12", "title": "Improved consistent sampling, weighted minhash and l1 sketching", "journal": "", "year": "2010", "authors": "S Ioffe"}, {"ref_id": "b13", "title": "Strip: stream learning of influence probabilities", "journal": "", "year": "2013", "authors": "K Kutzkov; A Bifet; F Bonchi; A Gionis"}, {"ref_id": "b14", "title": "b-bit minwise hashing", "journal": "", "year": "2010", "authors": "P Li; A C K\u00f6nig"}, {"ref_id": "b15", "title": "Hashing algorithms for large-scale learning", "journal": "", "year": "2011", "authors": "P Li; A Shrivastava; J L Moore; A C K\u00f6nig"}, {"ref_id": "b16", "title": "Consistent weighted sampling. MSR-TR-2010-73 technical report", "journal": "", "year": "2010", "authors": "M Manasse; F Mcsherry; K Talwar"}, {"ref_id": "b17", "title": "Detecting near-duplicates for web crawling", "journal": "", "year": "2007", "authors": "G S Manku; A Jain; A D Sarma"}, {"ref_id": "b18", "title": "Probability and Computing: Randomized Algorithms and Probabilistic Analysis", "journal": "Cambridge University Press", "year": "2005", "authors": "M Mitzenmacher; E "}, {"ref_id": "b19", "title": "The odds in some odd-even games", "journal": "The American Mathematical Monthly", "year": "1975", "authors": "E F Schuster; A N Philippou"}, {"ref_id": "b20", "title": "Tracking web spam with html style similarities", "journal": "", "year": "2008", "authors": "T Urvoy; E Chauveau; P Filoche; T Lavergne"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure1: Illustration of the b-bit minwise hashing construction. Given a high Jaccard similarity J and two minhashes S1, S2, we expect that |S1 \u2229 S2| = Jn (filled space) and |S1\u2206S2| = 2(1 \u2212 J)n (white space). Due to the same independent hash values in the filled space, the error of the b-bit scheme corresponds to the error of the estimate of |S1\u2206S2|. Inaccuracy in just a few bit positions in the white space will yield a large relative error of the estimate of J.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Comparison of the average MSE on different ratios \u03b1 on the synthetic dataset: (a) Fix J = 0.9 and change n = 500 \u2212 1000 bits; (b) Fix n = 800 bits and change J = 0.75 \u2212 0.95.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Comparison of the negative log of mean square error (MSE) of Odd Sketch and b-bit minwise hashing for different Jaccard similarity. In these experiments Odd Sketch used k odd = n 4(1\u2212J) permutations, and b-bit minwise hashing used k b = n b .", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure 5: Comparison of the negative log of mean square error (MSE) of Odd Sketch and b-bit minwise hashing for different Jaccard similarities. Here, Odd Sketch uses k odd = n, and b-bit minwise hashing uses k b = n b permutations.", "figure_data": ""}, {"figure_label": "61", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 6 : 1 2=61Figure 6: Comparison of the negative log of mean square error (MSE) of Odd Sketch and 1 2 -bit minwise hashing for different Jaccard similarities: (a) Different number of permutations: k odd = n 4(1\u2212J)", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 7 :7Figure 7: Comparison of the empirical cumulative distribution function (cdf ) of estimators based on Odd Sketch, 1-bit scheme, and 12 -bit scheme with J = 0.9.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 8 :8Figure 8: Comparison of the precision-recall ratio between Odd Sketches and 1-bit scheme on the mushroom dataset.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 9 :9Figure 9: Comparison of the precision-recall ratio between Odd Sketches and 12 -bit scheme on the mushroom dataset with J0 = 0.9.", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 10 :10Figure 10: Comparison of the precision-recall ratio between Odd Sketches and 1-bit minwise hashing with J0 = 0.85 on the three datasets: KOS blog entries, Enron Emails and NYTimes articles.", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Figure 11 :11Figure 11: Comparison of the precision-recall ratio between Odd Sketches and 1-bit minwise hashing with J0 = 0.90 on the three datasets: KOS blog entries, Enron Emails and NYTimes articles.", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Figure 12 :12Figure 12: Comparison of the precision-recall ratio between Odd Sketches and 12 -bit minwise hashing with J0 = 0.90 on the three datasets: KOS blog entries, Enron Emails and NYTimes articles.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "J(S1, S2) = Pr[min(\u03c0(S1)) = min(\u03c0(S2))].", "formula_coordinates": [2.0, 89.26, 498.92, 168.18, 8.39]}, {"formula_id": "formula_1", "formula_text": "S1 = {(i, min(\u03c0i(S1))) | i = 1, . . . , k}, S2 = {(i, min(\u03c0i(S2))) | i = 1, . . . , k}.", "formula_coordinates": [2.0, 99.72, 552.69, 148.79, 21.31]}, {"formula_id": "formula_2", "formula_text": "[\u0134] = J(1\u2212J) k .", "formula_coordinates": [2.0, 144.89, 604.39, 55.53, 12.59]}, {"formula_id": "formula_3", "formula_text": "odd(S1) S1 xxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxx xx xx S2 odd(S2) xxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxx x 1 xx xx 1 xx xx h(x) h(x) x' x'' 1 h(x') x x x 1 h(x'')", "formula_coordinates": [2.0, 329.28, 62.66, 214.46, 168.59]}, {"formula_id": "formula_4", "formula_text": "J b = |S b 1 \u2229 S b 2 |/k \u2212 1/2 b 1 \u2212 1/2 b , Var[\u0134 b ] = 1 \u2212 J k J + 1 2 b \u2212 1 .", "formula_coordinates": [2.0, 316.81, 522.01, 242.38, 21.52]}, {"formula_id": "formula_5", "formula_text": "si = \u2295 x\u2208S 1 h(x)=i ,", "formula_coordinates": [3.0, 139.09, 550.26, 68.52, 13.69]}, {"formula_id": "formula_6", "formula_text": "Lemma 1. odd(S1) \u2295 odd(S2) = odd(S1\u2206S2).", "formula_coordinates": [3.0, 63.76, 683.12, 181.91, 8.37]}, {"formula_id": "formula_7", "formula_text": "pi = 1 \u2212 (1 \u2212 2/n) i 2 .", "formula_coordinates": [3.0, 395.43, 459.64, 81.87, 21.52]}, {"formula_id": "formula_8", "formula_text": "E[X] = n 1 \u2212 (1 \u2212 2/n) m 2 .", "formula_coordinates": [3.0, 384.41, 542.37, 103.91, 21.52]}, {"formula_id": "formula_9", "formula_text": "z \u2248 n 1 \u2212 (1 \u2212 2/n) m 2 ,", "formula_coordinates": [3.0, 392.27, 601.03, 88.19, 21.52]}, {"formula_id": "formula_10", "formula_text": "m = ln (1 \u2212 2z/n) ln (1 \u2212 2/n) . (1", "formula_coordinates": [3.0, 388.87, 642.28, 163.13, 19.75]}, {"formula_id": "formula_11", "formula_text": ")", "formula_coordinates": [3.0, 552.0, 648.09, 3.92, 7.86]}, {"formula_id": "formula_12", "formula_text": "p = i odd e \u2212\u00b5 \u00b5 i i! = e \u2212\u00b5 i odd \u00b5 i i! = e \u2212\u00b5 e \u00b5 \u2212 e \u2212\u00b5 2 = 1 \u2212 e \u22122\u00b5 2 .", "formula_coordinates": [4.0, 107.22, 181.2, 132.1, 52.6]}, {"formula_id": "formula_13", "formula_text": "E[Y ] = np = n 1 \u2212 e \u22122m/n 2 .", "formula_coordinates": [4.0, 118.47, 316.58, 109.76, 21.51]}, {"formula_id": "formula_14", "formula_text": "m = \u2212 n 2 ln (1 \u2212 2z/n) .(2)", "formula_coordinates": [4.0, 117.96, 376.54, 174.95, 19.74]}, {"formula_id": "formula_15", "formula_text": "E[|S1\u2206S2|] = 2k(1 \u2212 J),", "formula_coordinates": [4.0, 124.39, 535.43, 97.92, 7.89]}, {"formula_id": "formula_16", "formula_text": "|S1\u2206S2| = \u2212 n 2 ln(1 \u2212 2 |odd(S1)\u2206odd(S2)|/n).", "formula_coordinates": [4.0, 80.55, 672.5, 185.61, 19.74]}, {"formula_id": "formula_17", "formula_text": "J odd = 1 \u2212| S1\u2206S2| 2k = 1 + n 4k ln 1 \u2212 2 |odd(S1)\u2206odd(S2)| n .", "formula_coordinates": [4.0, 344.37, 75.23, 185.53, 42.82]}, {"formula_id": "formula_18", "formula_text": "Pr(|Y \u2212 E[Y ]| \u2265 n) \u2264 2e \u22122n 2 .", "formula_coordinates": [4.0, 371.76, 561.17, 129.22, 11.94]}, {"formula_id": "formula_19", "formula_text": "Pr(|X \u2212 E[Y ]| \u2265 n) \u2264 (2e \u221a m)e \u22122n 2 .", "formula_coordinates": [4.0, 357.67, 592.7, 157.39, 14.76]}, {"formula_id": "formula_20", "formula_text": "Pr(|X \u2212 1 n E[Y ]| \u2265 ) \u2264 (2e \u221a m)e \u22122n 2 , Pr(|X \u2212 p| \u2265 ) \u2264 (2e \u221a m)e \u22122n 2 , where p = 1 \u2212 e \u22122m/n /2. The true expected fraction of odd bins is E[X]/n = 1\u2212(1\u22122/n) m 2", "formula_coordinates": [4.0, 316.81, 637.74, 239.1, 73.31]}, {"formula_id": "formula_21", "formula_text": "pi = 1 \u2212 (1 \u2212 2/n) i 2 , so each Xi = 1 with probability (1 \u2212 (1 \u2212 2/n) m )/2.", "formula_coordinates": [5.0, 53.8, 243.71, 221.51, 35.22]}, {"formula_id": "formula_22", "formula_text": "E[X 2 ] = E[( i Xi) 2 ] = i E[X 2 i ] + 2 i<j E[XiXj] = i E[Xi] + 2 i<j E[XiXj].", "formula_coordinates": [5.0, 95.09, 318.44, 156.52, 70.66]}, {"formula_id": "formula_23", "formula_text": "qi = 1 + (1 \u2212 4/n) i \u2212 2(1 \u2212 2/n) i 2 .", "formula_coordinates": [5.0, 104.25, 633.26, 138.21, 21.52]}, {"formula_id": "formula_24", "formula_text": "n 2 1 + (1 \u2212 4/n) m \u2212 2(1 \u2212 2/n) m 2 .", "formula_coordinates": [5.0, 107.38, 696.39, 139.24, 21.52]}, {"formula_id": "formula_25", "formula_text": "E[X 2 ] \u2212 E[X] 2 , or n 2 1 + (1 \u2212 4/n) m \u2212 2(1 \u2212 2/n) m 2 + n(1 \u2212 (1 \u2212 2/n) m ) 2 \u2212 n(1 \u2212 (1 \u2212 2/n) m ) 2 2 .", "formula_coordinates": [5.0, 340.4, 55.87, 193.97, 69.91]}, {"formula_id": "formula_26", "formula_text": "n 2 (1 \u2212 4/n) m \u2212 (1 \u2212 2/n) 2m 4 + n 1 \u2212 (1 \u2212 4/n) m 4 .", "formula_coordinates": [5.0, 336.76, 147.57, 199.21, 21.52]}, {"formula_id": "formula_27", "formula_text": "(1 \u2212 4/n) m \u2212 (1 \u2212 2/n) 2m 4", "formula_coordinates": [5.0, 384.06, 190.39, 104.12, 21.52]}, {"formula_id": "formula_28", "formula_text": "n 1 \u2212 (1 \u2212 4/n) m 4 \u2248 n 1 \u2212 e \u22124m/n 4 = np(1 \u2212 p),", "formula_coordinates": [5.0, 343.05, 275.67, 186.62, 21.52]}, {"formula_id": "formula_29", "formula_text": "m = \u2212 n 2 ln (1 \u2212 2z/n) .", "formula_coordinates": [5.0, 387.9, 462.78, 94.89, 29.51]}, {"formula_id": "formula_30", "formula_text": "\u03b1 = 2k odd (1\u2212J 0 ) n \u2248 1 2 .", "formula_coordinates": [6.0, 196.08, 643.33, 83.58, 12.59]}, {"formula_id": "formula_31", "formula_text": "k odd = k 1 2 = 2n.", "formula_coordinates": [7.0, 316.81, 284.02, 65.4, 12.12]}], "doi": "10.1145/2566486.2568017"}