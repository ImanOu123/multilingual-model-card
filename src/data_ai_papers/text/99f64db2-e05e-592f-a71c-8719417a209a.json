{"title": "First-Person Activity Forecasting with Online Inverse Reinforcement Learning", "authors": "Nicholas Rhinehart; Kris M Kitani", "pub_date": "", "abstract": "We address the problem of incrementally modeling and forecasting long-term goals of a first-person camera wearer: what the user will do, where they will go, and what goal they seek. In contrast to prior work in trajectory forecasting, our algorithm, DARKO, goes further to reason about semantic states (will I pick up an object?), and future goal states that are far in terms of both space and time. DARKO learns and forecasts from first-person visual observations of the user's daily behaviors via an Online Inverse Reinforcement Learning (IRL) approach. Classical IRL discovers only the rewards in a batch setting, whereas DARKO discovers the states, transitions, rewards, and goals of a user from streaming data. Among other results, we show DARKO forecasts goals better than competing methods in both noisy and ideal settings, and our approach is theoretically and empirically no-regret.", "sections": [{"heading": "Introduction", "text": "Our long-term aim is to develop an AI system that can learn about a person's intent and goals by continuously observing their behavior. In this work, we progress towards this aim by proposing an online Inverse Reinforcement Learning (IRL) technique to learn a decision-theoretic human activity model from video captured by a wearable camera. The use of a wearable camera is critical to our task, as human activities must be observed up close and across large environments. Imagine a person's daily activitiesperhaps they are at home today, moving about, completing tasks. Perhaps they are a scientist that conducts a long series of experiments across various stations in a laboratory, or they work in an office building where they walk about their floor, get coffee, etc. As people tend to be very mobile, a wearable camera is ideal for observing a person's behavior.\nSince our task is to continuously learn human behavior models (i.e., a policy) from observed behavior captured with a wearable camera, our task is best described as an online IRL problem. The problem is an inverse Reinforc-Figure 1: Forecasting future behavior from first-person video. Overhead map shows likely future goal states. s i is user state at time i. Histogram insets display predictions of user's long-term semantic goal (inner right) and acquired objects (inner left). ment Learning problem because the underlying reward or cost function of the person is unknown. We must infer it along with the policy from the demonstrated behaviors. Our task is also an online learning problem, because our algorithm must continuously learn as a part of a life-long process. From this perspective, we must develop an online learning approach that learns effectively over time. Figure 2: Sparse SLAM points (2a) and offline dense reconstruction (2b) using [5] for two of our dataset environments.\nWe present an algorithm that incrementally learns spatial and semantic intentions (where you will go and what you will do) of a first-person camera wearer. By tracking the goals a person achieves, the algorithm builds a set of possible futures. At any time, the user's future is predicted among this set of goals. We term our algorithm \"Discovering Agent Rewards for K-futures Online\" (DARKO), as it learns to associate rewards with semantic states and actions from demonstrations to predict among K possible goals. Contributions: To the best of our knowledge, we present the first application of ideas from online learning theory and inverse reinforcement learning to the task of continuously learning human behavior models with a wearable camera. Our proposed algorithm is distinct from traditional IRL problems as we jointly discover states, transitions, goals, and the reward function of the underlying Markov Decision Process model. Our proposed human behavior model also goes beyond first-person trajectory forecasting and allows for the prediction of future human activities that can happen outside the immediate field of view and far into the future.", "publication_ref": ["b4"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Related Work", "text": "First-person vision (FPV): Wearable cameras have been used for various human behavior understanding tasks [4,10,12,16,22] because they give direct access to detailed visual information about a person's actions. Leveraging this feature of FPV, recent work has shown that it is possible to predict where people will look during actions [12] and how people will use the environment [20]. Decision-Theoretic Modeling: Given agent demonstrations, the task of inverse reinforcement learning (IRL) is to recover a reward function of an underlying Markov Decision Process (MDP) [1]. IRL has been used to model taxi driver behavior [34] and pedestrian behavior [35,7]. In contrast to previous work, we go beyond physical trajectory forecasting by reasoning over future object interactions and predicting future goals in terms of scene types. Online Learning Theory: The theory of learning to making optimal predictions from streaming data is well studied [23] but is rarely used in computer vision, compared to the more prevalent application of supervised learning theory. We believe, however, that the utility of online learning theory is likely to increase as the amount of available data for processing is ever increasing. While the concept of online learning has been applied to inverse reinforcement learning [18], the work was primarily theoretic in nature and has found limited application. Trajectory forecasting: Physical trajectory forecasting has received much attention from the vision community. Multiple human trajectory forecasting from a surveillance camera was investigated by [14]. Other trajectory forecasting approaches use demonstrations observed from a bird'seye view; [31] infers latent goal locations and [2] employ LSTMs to jointly reason about trajectories of multiple humans. In [25], the model forecasted short-term future trajectories of a first-person camera wearer by retrieving the nearest neighbors from a dataset of first-person trajectories under an obstacle-avoidance cost function, with each trajectory representing predictions of where the user will move in view of the frame; in [26], a similar model with learned cost function is extended to multiple users. Predicting Future Behavior: In [6,21], the tasks are to recognize an unfinished event or activity. In [6], the model predicts the onset for a single facial action primitive, e.g. the completion of a smile, which may take less than a second. Similarly, [21] predicts the completion of short human to human interactions. In [9], a hierarchical structured SVM is employed to forecast actions about a second in the future, and [28] demonstrates a semi-supervised approach for forecasting human actions a second into the future. Other works predict actions several seconds into the future [3,8,11,29]. In contrast, we focus on high-level transitions over a sequence of future actions that may occur outside the frame of view, and take a longer time to complete (in our dataset, the mean time to completion is 21.4 seconds).", "publication_ref": ["b3", "b9", "b11", "b15", "b21", "b11", "b19", "b0", "b33", "b34", "b6", "b22", "b17", "b13", "b30", "b1", "b24", "b25", "b5", "b20", "b5", "b20", "b8", "b27", "b2", "b7", "b10", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "Online IRL with DARKO", "text": "Our goal is to forecast the future behaviors of a person from a continuous stream of video captured by a wearable camera. Given a continuous stream of FPV video, our ap-proach extracts a sequence of state variables {s 1 , s 2 , . . . } using a portfolio of visual sensing algorithms (e.g., SLAM, stop detection, scene classification, action and object recognition). In an online fashion, we segment this state sequence into episodes (short trajectories) by discovering terminal goal states (e.g., when a person stops). Using the most recent episode, we adaptively solve the inverse reinforcement learning problem using online updates. Solving the IRL problem in an online fashion means that we incrementally learn the underlying decision process model.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "First-Person Behavior Model", "text": "A Markov Decision Process (MDP) is commonly used to model the sequential decision process of a rational agent. In our case, we use it to describe the activity of a person with a wearable camera. In a typical reinforcement learning problem, all elements of the MDP are assumed to be known and the task is to estimate an optimal policy \u03c0(a|s), that maps a state s to an action a, by observing rewards. In our novel online inverse formulation, every element of the MDP, including the policy, is unknown and must be inferred as new video data arrives. Formally, an MDP is defined as:\nM = {S, A, T (\u2022, \u2022), R \u03b8 (\u2022, \u2022)}.\nStates: S is the state space: the set of states an agent can visit. In our online formulation, S is initially empty, and must be expanded as new states are discovered. We define a state s as a vector that includes the location of the person (3D position), the last place the person stopped (a previous goal state), and information about any object that the person might be holding. Formally, a state s \u2208 S is denoted as:\ns = [x, y, z, o 1 . . . , o |O| , h 1 , . . . h |K| ].\nThe triplet [x, y, z] is a discrete 3D position. To obtain the position, we use a monocular visual SLAM algorithm [15] to localize the agent in a continuously built map.\nThe vector o 1 . . . , o |O| encodes any objects that the person is currently holding. We include this information in the state vector because the objects a user acquires are strongly correlated to the intended activity [4]. o j = 1 if the user has object j in their possession and zero otherwise. O is a set of pre-defined objects available to the user. K is a set of pre-defined scene types available to the user, which can be larger than the true number of scene types. The vector h 1 , . . . h K encodes the last scene type the person stopped. Example scene types are kitchen and office. h i = 1 if the user last arrived at scene type i and is zero otherwise. Goals: We also define a special type of state called a goal state s \u2282 S g , to denote states where the person has achieved a goal. We assume that when a person stops, their location in the environment is a goal. We detect goal states by using a velocity-based stop detector. Whenever a goal state is encountered, the sequence of states since the last goal state to the current goal state is considered a completed episode \u03be. The set of goals states S g \u2282 S expands with each detection. We explain later how S g is used to perform goal forecasting. Actions: A is the set of actions. A can be decomposed into two parts: A = A m \u222a A c . The act of moving from one location in the environment to another location is denoted as a m \u2208 A m . Like S, A m must be built incrementally. The set A c is the set of possible acquire and release actions of each object: A c = {acquire, release} \u00d7 O. The act of releasing or picking up an object is denoted as a c \u2208 A c . Each action a c must be detected. We do so with an image-based first-person action classifier. More complex approaches could improve performance [13]. Transition Function: The transition function T : (s, a) \u2192 s represents how actions move a person from one state to the next state. T is constructed incrementally as new states are observed and new actions are performed. In our work, T is built by keeping a table of observed (s, a, s ) triplets, which describes the connectivity graph over the state space. More advanced methods could also be used to infer more complex transition dynamics [27,30]. Reward Function: R(s, a; \u03b8) is an instantaneous reward function of action a at state s. We model R as the inner product between a vector of features f (s, a) and a vector of weights \u03b8. The reward function is essential in valuebased reinforcement learning methods (in contrast to policy search methods) as it is used to compute the policy \u03c0(a|s). In the maximum entropy setting, the policy is given by \u03c0(a|s) \u221d e Q(s,a)\u2212V (s) , where the value functions V (s) and Q(s, a) are computed from the reward function by solving the Bellman equations [34]. In our context, we learn the reward function online.\nIntuitively, we would like the features f of the reward function to incorporate information such as the position in an environment or objects in possession, since it is reasonable to believe that the goal of many activities is to reach a certain room or to retrieve a certain object. To this end, we define the features of the reward to mirror the information already contained in the state s t : the position, previous scene type, and objects held. To be concrete, the feature vector f (s, a) is the concatenation of the 3-d position coordinates [x, y, z], a K-dimensional indicator vector over previous goal state type and a |O|-dimensional indicator vector over held objects. We also concatenate a |A c |-dimensional indicator vector over actions a c \u2208 A c .", "publication_ref": ["b14", "b3", "b12", "b26", "b29", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "The DARKO Algorithm", "text": "We now describe our proposed algorithm for incrementally learning all MDP parameters, most importantly the reward function, given a continuous stream of first-person video (see DARKO in Algorithm 1). The procedure begins by initializing s, reward parameters \u03b8, empty state space S, goal space S g , transition function T , and current episode \u03be.\nState Space Update: Image frames are obtained from a first-person camera (the NEWFRAME function), and SLAM is used to track the user's location (lines 4 and 5). An image-based action detection algorithm, ACTDET, detects hand-object interactions a c and decides movements a m as a function of current and previous position. While we provide an effective method for ACTDET, our focus is to integrate (rather than optimize) its outputs. Lines 7 and 8 show how the trajectory is updated and MDP parameters of state space and transition function are expanded. Line 9 represents a collection of generalized forecasting tasks (see Section 4.4), such as the computation of future goal posterior and trajectory forecasting. Goal Detection: In order to discover goals, we use a stopdetection algorithm GOALDET, by using the camera velocity computed from SLAM (Line 10). If a goal state has been detected, that terminal state is added to the set of goal states S g . The detection of a terminal state also marks the end of an episode \u03be. The previous goal state type is also updated for the next episode. Again, while we provide an effective method for GOALDET, our focus is to integrate (rather than optimize) its outputs. Online IRL: With the termination of each episode \u03be, the reward function R and corresponding policy \u03c0 are updated via the reward parameters \u03b8 (Line 13). The parameter update uses a sequence of demonstrated behavior via the episode \u03be, and the current parameters of the MDP. More specifically, ONLINEIRL (Algorithm 2) performs online gradient descent on the likelihood under the maximum entropy distribution by updating current parameters of the reward function. The gradient of the loss can be shown to be the difference between expected feature countsf and empirical feature countsf of the current episode. Computing the gradient requires solving the soft value iteration algorithm of [33]. We include a projection step to ensure \u03b8 2 \u2264 B.\nTo the best of our knowledge, this is the first work to propose an online algorithm for maximum entropy IRL in the streaming data setting. Following the standard procedure for ensuring good performance of an online algorithm, we analyze our algorithm in terms of the regret bound. The regret R t of any online algorithm is defined as:\nR t = t i=0 l t (\u03be t ; \u03b8 t ) \u2212 min \u03b8 * t i=0 l(\u03be t ; \u03b8 * ).\nThe regret is the cumulative difference between the performance of the online model using current parameter \u03b8 versus the best hindsight model using the best parameters \u03b8 * . The loss l t is a function of the t'th demonstrated trajectory, and measures how well the model explains the trajectory.\nIn our setup, the loss function is defined as l t (\u03be t ; \u03b8) = \u2212 1 |\u03bet| |\u03bet| i=0 log \u03c0 \u03b8 (a i |s i ). It can be shown 1 that the regret 1  Goal forecasting, trajectory forecasting, . . .", "publication_ref": ["b32", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Proof of the regret bound is given in the Appendix", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "10:", "text": "is goal \u2190 GOALDET(s, frame, S g ) 11:\nif is goal then \nf i = (s,a)\u2208\u03be f (s, a) 3: Compute R(s, a; \u03b8) \u2200s \u2208 S, a \u2208 A 4: \u03c0 \u2190 SOFTVALUEITERATION(R, S, S g , T ) 5:f i \u2190 E \u03c0 [f (s, a)] 6: \u03b8 \u2190 proj \u03b8 2\u2264B (\u03b8 \u2212 \u03bb(f i \u2212f i )) 7:\nreturn \u03c0, \u03b8 8: end procedure of our online algorithm is bounded as follows:\nR t \u2264 2B \u221a 2td,(1)\nwhere B is a norm bound on \u03b8, d is feature dimensionality, and t is the episode. The average regret Rt t approaches zero as t grows since the bound is sub-linear in t. Verification of this no-regret property is shown in the experiments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Generalized Activity Forecasting", "text": "Without Line 9, Algorithm 1 only describes our online IRL process to infer the reward function. In order to make incremental predictions about the person's future behaviors online, we can leverage the current MDP and reward function. An important function which lays the basis for predicting future behaviors is the state visitation function, denoted D. We now show how D can be modified to perform generalized queries about future behavior.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "State Visitation Function D", "text": "Using the current estimate of the MDP and the reward function, we can compute the policy of the agent. Using the policy, we can forward simulate a distribution of all possible futures. This distribution is called the state visitation distribution [33]. More formally, the posterior expected count of future visitation to a state s x can be defined as\nD sx|\u03be0\u2192t E P (\u03be t+1\u2192T |\u03be0\u2192t) T \u03c4 =t+1 I(s \u03c4 = s x ) . (2)\nThis quantity represents the agent's expectation to visit each state in the future given the partial trajectory. \u03be 0\u2192t indicates a partial trajectory starting at time 0 and ending at time t. The expectation is taken under the maximum causal entropy distribution, P (\u03be t+1\u2192T |\u03be 0\u2192t ), which gives the probability of a future trajectory given the current trajectory. I is the indicator function, which counts agent visits to s x . Equation 2is estimated by sampling trajectories from \u03c0 \u03b8 (a|s).", "publication_ref": ["b32"], "figure_ref": [], "table_ref": []}, {"heading": "Activity Forecasting with State Subsets", "text": "In this work, we extend the idea of state visitations to a single state s x to a more general subset of states S p . While a generalized prediction task was not particularly meaningful in the context of trajectory prediction [7,34], predictions over a subset of states now represents semantically meaningful concepts in our proposed MDP. By using the state space representation of our first-person behavior model, we can construct subsets of the state space that have interesting semantic meaning, such as \"having an object o i \" or \"all states closest to goal k with O j set of objects.\"\nFormally, we define the expected count of visitation to a subset of states S p satisfying some property p:\nD Sp|\u03be0\u2192t E P (\u03be t+1\u2192T |\u03be0\u2192t) T \u03c4 =t+1 I(s \u03c4 \u2208 S p )(3)\n= sx\u2208Sp E P (\u03be t+1\u2192T |\u03be0\u2192t) T \u03c4 =t+1 I(s \u03c4 = s x ) = sx\u2208Sp D sx|\u03be0\u2192t . (4\n)\nEquation 4 is essentially marginalizing over the state subspace of Equation 2. Derivation of two other inference tasks is given in the Appendix: (1) expected visitation prediction of performing an action after arriving at a subspace, (2) expected joint action-state subspace visitation prediction.", "publication_ref": ["b6", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "Forecasting Trajectory Length", "text": "In the following, we present a method to predict the length of the future trajectory. Formally, we can denote the expected trajectory length:\n\u03c4 \u03be t+1\u2192T |\u03be0\u2192t E P (\u03be t+1\u2192T |\u03be0\u2192t) |\u03be t+1\u2192T | (5)\nConsider evaluating D Sp|\u03be0\u2192t from Equation 4 by setting S p = S, that is, by considering the expected future visitation count to the entire state space. Then,\nD S|\u03be0\u2192t = E P (\u03be t+1\u2192T |\u03be0\u2192t) T \u03c4 =t+1 I(s \u03c4 \u2208 S) = E P (\u03be t+1\u2192T |\u03be0\u2192t) T \u03c4 =t+1 1 = E |\u03be t+1\u2192T | =\u03c4 \u03be t+1\u2192T |\u03be0\u2192t(6)\nwhere |\u03be| indicates the number of states in trajectory \u03be.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Future Goal Prediction", "text": "As previously described, we wish to predict the final goal of a person's action sequence. For example, if I went to the study to pick up a cup, how likely am I to go to the kitchen versus the living room? This problem can be posed as solving for the MAP estimate of P (g|\u03be)\u2200g \u2208 S g , the posterior over goals. It describes what goal the user seeks given their current trajectory, defined as:\nP (g|\u03be 0\u2192t ) \u221d P (g)e Vs t (g)\u2212Vs 0 (g) ,(7)\nwhere V si (g) is the value of g with respect to a partial trajectory that ends in s i . Notice that the likelihood term is exponentially proportional to the value difference between the start state s 0 and the current state s t . In this way, the likelihood encodes the progress made towards a goal g in terms of the value function.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We first present the dataset we collected. Then, we discuss our methods for goal discovery and action recognition. To reiterate, our focus is not to engineer these methods, but instead to make intelligent use of their outputs in DARKO for the purpose of behavior modeling. We compare DARKO's performance versus several baselines on the task of goal forecasting, and show DARKO's performance is superior. Then, we analyze DARKO's performance under less noisy conditions, to illustrate how it improves when provided with more robust goal discovery and action detection algorithms. Next, we illustrate DARKO 's empirical no-regret performance, which further shows it is an efficient online learning algorithm. Finally, we present trajectory length forecasting results, and find that our length forecasts exhibit low median error. Additional analyses including feature ablation and incorporating uncertainty from goal discovery are presented in the Appendix.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "First-Person Continuous Activity Dataset", "text": "We collected a dataset of sequential goal-seeking behavior in several different environments such as home, office and laboratory. The users recorded a series of activities that naturally occur in each scenario. Each user helped design the script they followed, which involved their prior assumptions about what objects they will use and what goal they will seek. An example direction from a script is \"obtain a snack and plate in kitchen, eat at dining room table.\"\nUsers wore a hat-mounted Go-Pro Hero camera with 94 \u2022 vertical, 123 \u2022 horizontal FOVs. Our dataset is comprised of 5 user environments, and includes over 250 actions with 19 objects, 17 different scene types, at least 6 activity goals per environment, and about 200 high-level activities (trajectories). In each environment, the user recorded 3-4 long sequences of high-level activities, where each sequence represents a full day of behavior. Our dataset represents over 15 days of recording.\nFor evaluation, all ground truth labels of objects (e.g. cup, backpack), actions (i.e. acquire, release) and goals (e.g. kitchen, bedroom) were first manually annotated. A goal label correspond to when a high-level direction was completed, and in which scene it was completed, e.g. (dining room, time=65s). An action label indicates when an activity was performed, e.g. (acquire, cup, time=25s). Further details are presented in the Appendix.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Goal Discovery and Action Recognition", "text": "We describe two goal discovery methods and an action recognition method that can serve as input to DARKO. With respect to Algorithm 1, these are GOALDET and ACTDET. Scene-based Goal Discovery: This model assumes that if a scene classifier is very confident in the scene type for several images frames, the camera wearer must be in a meaningful place in the environment (i.e., kitchen, bedroom, office). We use the output of a scene classifier from [32] (GoogLeNet model) on every frame from the wearable camera. If the mean scene classifier probability for a scene type is above a threshold \u03c1 g for 20 consecutive image frames, then we add the current state s t to the set of goals S g . Stop-based Goal Discovery: This model assumes that when a person stops, they are at an important location. Using SLAM's 3D camera positions, we apply a threshold on velocity to detect stops in motion. When a stop is detected, we add the current state s t to the set of goals S g . In Table 1, temporal accuracies are computed by counting detections within 3-second windows of ground truth labels as true positives; for the scene-based method, a true positive also requires the scene type to match the ground truth scene type. Stop-based discovery is reliable across all environments, thus, we use it as our primary goal discovery method. Image-based Object Recognition: We designed an object recognition approach that classifies the object the user interacts with at every temporally-labeled window. It overwrites the ground-truth object label with its detection. The approach first detects regions of person in each frame Figure 3: Goal forecasting examples: A temporal sequence of goal forecasting results is shown in each row from left to right, with the forecasted goal icons and sorted goal probabilities inset (green: P (g * |\u03be), red: P (g i = g * |\u03be)). Top: the scientist acquires a sample to boil in the gel electrophoresis room. Middle: the user gets a textbook and goes to the lounge. Bottom: the user leaves their apartment.  with [19] to focus on objects near the visible hands, which are cropped with context and fed into an image-classifier trained on ImageNet [24]. The outputs are remapped to our object set, and a final classification is produced by taking the maximum across objects. The per-action classification accuracies in  Proposed goal posterior (Sec.4.4) achieves best P g * (mean probability of true goal).", "publication_ref": ["b31", "b18", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Goal Forecasting Performance", "text": "At every time step, our method predicts the user's goal or final destination (e.g., bedroom, exit) as described in Section 4.4 and shown in Figure 3. To understand the goal prediction reliability, we compare our approach to several baseline methods for estimating the goal posterior P (g|\u03be 0\u2192t ), where g is a goal and \u03be 0\u2192t is the observed state sequence up to the current time step. Each baseline requires the state tracking and goal discovery components of DARKO. Uniform Model (Uniform): This model returns a uniform posterior over possible goals P n (g) = 1/K n known at the current episode n, defining worst case performance. Logistic Regression Model (Logistic): A logistic regression model P n (g|s t ) is fit to map states s t to goals g. Max-Margin Event Detection (MMED) [6]: A set of max-margin models P n (g|\u03c6(s t:t\u2212w )) are trained to map features \u03c6 of a w-step history of state vectors s t:t\u2212w to a goal score. We use the best performing sumL1norm features provided with the publicly available code. RNN Classifier (RNN): An RNN is trained to predict P n (g|\u03be 0\u2192t ). We experimented with a variety of parameters (see the Appendix) and report the best results. Since all methods above are online algorithms, each of the models P n is updated after every episode n. In order quantify performance with a single score, we use the mean probability assigned to the ground truth goal type g * over all episodes {\u03be n } N n=1 as P (g\n* |{\u03be n } N n=1 ) = 1 N N n=1\nTn t=1 P n (g|\u03be nt ) (also denoted P g * ). The goal forecasting performance results are summarized in Table 2 using the above metric.", "publication_ref": ["b5"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Goal Forecasting with Perfect Visual Detectors", "text": "The experimental results up to this point have exclusively used visual detectors as input (e.g., SLAM, scene classification, object recognition). While we have shown that our approach learns meaningful human activity models from real computer vision input, we would also like to understand how our online IRL method performs when decoupled from the noise of the vision-based input. We perform the same experiments described in Section 5.3 but with idealized (ground truth) inputs for goal discovery and action   recognition. We still use SLAM for localization. Table 3 summarizes the mean true goal probability for each of the dataset environments. We observe a mean absolute performance improvement of 0.27 by using idealized inputs. Our proposed model continues to perform the best against baselines methods. This performance indicates that as vision-based component technologies improve, we can expect significant improvements in the ability to predict the goals of human activities.\nWe also measure performance when the action detection is built from ground truth and the goal discovery is built from our described methods. Our expectation is that DARKO with stop-based discovery should outperform DARKO with scene-based based discovery, given the stopdetector's more reliable goal detection performance (Table 1). The results over the dataset are given in Table 4, confirming our expectation.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5", "tab_6"]}, {"heading": "Goal Forecasting Performance over Time", "text": "In additional to understanding the performance of goal prediction with a single score, we also plot the performance of goal prediction over time. We evaluate the goal forecasting performance as a function of the fraction of time until reaching the goal. In Figure 4, we plot the mean probability of the true goal at each fractional time step P (g * |\u03be t ) = 1 N N n=1 P n (g * |\u03be nt ). Using fractional trajectory length allows for a performance comparison across trajectories of different lengths.\nAs shown in Figure 4, DARKO exhibits the property of maintaining uncertainty early in the trajectory and converging to the correct prediction as time elapses in most cases. In contrast, the logistic regression, RNN, and MMED perform worse at most time steps. As it approaches the goal, our method always produces a higher confidence in the cor-  rect goal with lower variance. We tried argmax and Platt scaling [17] to perform multi-class prediction with MMED; argmax yielded higher P g * , in addition to making P g * noisier. While the RNN sees many states, its trajectorycentric hidden-state representation may not have enough data to generalize as well as the state-centric baselines.", "publication_ref": ["b16"], "figure_ref": ["fig_3", "fig_3"], "table_ref": []}, {"heading": "Empirical Regret Analysis", "text": "We empirically show that our model has no-regret with respect to the best model computed in hindsight under the MaxEntIRL loss function (negative log-loss). In particular, we compute the regret (cumulative loss difference) between our online algorithm and the best hindsight model using the batch MaxEntIOC algorithm [34] at the end of all episodes. We plot the average regret Rt t for each environment in the dataset in Figure 5. The average regret of our algorithm approaches zero over time, matching our analysis.", "publication_ref": ["b33"], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Evaluation of Trajectory Length Estimates", "text": "Our model can also be used to estimate how long it will take a person to reach a predicted goal state. We predict the  , where \u03c4 nt is the true trajectory length and\u03c4 nt (Eq. 6) is the predicted trajectory length. Proper evaluation of trajectory length towards a goal is challenging because our approach must learn valid goals in an online fashion. When a person approaches a new goal, our approach cannot accurately predict the goal because it has yet to learn that it is a valid goal state. As a result, our algorithm makes wrong goal predictions during episodes that terminate in new goal states. If we simply evaluate the mean performance, it will be dominated by the errors of the first episode terminating in a new goal state.\nWe evaluate median n over all N episodes. The median is not dominated by the errors of the first episode toward a new goal. We find most trajectory length forecasts are accurate, evidenced by the median of the normalized prediction error in Table 5. We include a partial-trajectory nearest neighbors baseline (NN). In Lab 1, the median trajectory length estimate is within 6.3% of the true trajectory length.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_8"]}, {"heading": "Conclusion", "text": "We proposed the first method for continuously modeling and forecasting a first-person camera wearer's future semantic behaviors at far-reaching spatial and temporal horizons. Our method goes beyond predicting the physical trajectory of the user to predict their future semantic goals, and models the user's relationship to objects and their environment. We have proposed several efficient and extensible methods for forecasting other semantic quantities of interest. Exciting avenues for future work include building upon the semantic state representation to model more aspects of the environment (which enables forecasting of more detailed futures), validation against human forecasting performance, and further generalizing the notion of a \"goal\" and how goals are discovered.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Reward Function Feature Ablation Analysis", "text": "In Table 6, we show the mean true goal probability when labels are used as detectors (to isolate performance in the ideal case). While the purely positional representation of state performs well, it is almost always outperformed by the full representation of rewards that include features of both the full state and action. In Lab 1, the simpler representations slightly outperform the full, due to the relative simplicity of the high-level activities in Lab 1. Here, knowledge of the state and previous goal alone is highly predictive of future goal.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_10"]}, {"heading": "B. Incorporating Detection Noise", "text": "Current paradigms in vision often yield noise in the action and goal detectors necessary for DARKO. We first describe our method for incorporating uncertainty in each goal detection, then conduct a performance analysis with synthetic noise. Then, we analyze the performance with real, noisy goal detection. We find DARKO can still perform well with forms of noisy goal and action detection. We find incorporating goal uncertainty significantly improves performance with synthetic noise, and shows improvements in the real goal detector setting. These results show that DARKO can tolerate the effects of noise, and further support the claim that it can enjoy the benefits of better scene and activity detection algorithms. Harnessing goal detection confidence: In many scenarios, probability \u03c1 g \u2208 [0, 1] may be associated with each goal detection. We designed an effective method for handling real-world uncertainty. For known perfect goals, SOFTVALUEITERATION uses V (g) = 0, \u2200g \u2208 S g . Each goal is a maxima of V (s) \u2208 (\u2212\u221e, 0], \u2200s \u2208 S and represents a reward of 1 in log space. We replace each goal value with its log-probability: V (g) = ln \u03c1 g , which has the effect of biasing the policy towards goals with greater certainty. This results from the value iteration assigning higher value to states and actions closer to more certain goals, which makes the policy likelier to visit them. For example, if the goal detector yields a false positive of bathroom in the same area as a true positive detection of kitchen, the goal prediction posteriors for both goals will suffer, unless the false positive has an associated low \u03c1 g (high uncertainty), in which case the policy is biased towards the correct goal of kitchen.\nNoise analysis: We first analyze DARKO under the effect of adding noise to the GT. We add incorrect goal detections with probabilities \u03c1 g \u223c N (0.1, 0.05), under various amounts of noise inserted uniformly at random across time: from 10%, 20%, . . . , 90% of the number of original goal detections. For every scene, at each noise amount, we sample noise 5 times, and run DARKO with and without goal uncertainty for each corrupted sample, resulting in 225 paired experiments that evaluate the average goal forecasting probability. Per-scene results are shown in Figures 6. A one-sided Wilcoxon signed-rank test supports the hypothesis that incorporating high goal uncertainty yields better goal posterior prediction performance than not incorporating the uncertainty with p < 10 \u22127 .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C. Proof of Regret Bound", "text": "Our regret bound is:\nR t \u2264 2B \u221a 2td,(8)\nwhere B is a bound on the norm of \u03b8, d is feature dimensionality, and t is the episode count (regret after the t'th episode).\nProof. By Equation 2.5 of [23], the regret of online gradient descent is bounded:\nR t \u2264 1 2\u03bb \u03b8 2 2 + \u03bb t i=1 \u2207 \u03b8t 2 2 .(9)\nBy using bounds on \u03b8 2 2 , \u2207 \u03b8t 2 2 , and a minimizing choice of \u03bb, we will prove the result. Writing the general gradient in terms of the expected features (and omitting the subscript t):\n\u2207 \u03b8 2 2 = f \u2212f 2 2 =f Tf +f Tf \u2212 2f Tf(10)\nUsing:\n0 \u2264 x \u2212 y 2 2 = x T x + y T y \u2212 2x T y 2x T y \u2264 x T x + y T y 2(\u2212x) T y \u2264 (\u2212x) T (\u2212x) + y T y \u22122x T y \u2264 x T x + y T y, \u2234 \u22122f Tf \u2264f Tf +f Tf , (Setting x =f , y =f )\nthen Equation 10 becomes:\n\u2207 \u03b8 2 2 \u2264f Tf +f Tf +f Tf +f Tf = 2f Tf + 2f Tf \u2264 4d. (Sincef ,f \u2208 [0, 1] d )(11)\nThus, using Equation 11 in Equation 9, and that the projection step of \u03b8 (constraining the set of \u03b8 to be the convex ball with radius B) ensures \u03b8 2 \u2264 B:\nR t \u2264 B 2 2\u03bb + \u03bb t i=1 4d = B 2 2\u03bb + 4\u03bbtd.", "publication_ref": ["b22"], "figure_ref": [], "table_ref": []}, {"heading": "With the minimizing choice of", "text": "\u03bb = B 2 \u221a 2td , R t \u2264 B \u221a 2td + 2Btd \u221a 2td = 2B \u221a 2td", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D. Derivation of Other Inference Tasks", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.1. Action-Subspace Visitation", "text": "To derive the action-subspace visitation, we first use the posterior expected visitation count of performing an action a y immediately after arriving at a state s x is given in Equation 12, from [33].\nD ay,sx|\u03be0\u2192t E P (\u03be t+1\u2192T |\u03be0\u2192t) T \u03c4 =t+1 I(s \u03c4 = s x )I(a \u03c4 = a y )(12)\nD ay,sx|\u03be0\u2192t = \u03c0(a y |s x )D sx|\u03be0\u2192t(13)\nOur definition of the posterior expected action subspace visitation count is given in Equation 14. This expresses the future expectation the user will perform an action a y while in a subspace S p , given their current trajectory \u03be 0\u2192t . \n. Thus, the posterior expected action subspace visitation is straightforward to compute with D sx|\u03be0\u2192t . Various inference tasks can be constructed by choosing a y and S p appropriately.", "publication_ref": ["b32"], "figure_ref": [], "table_ref": []}, {"heading": "D.2. Joint Action-State Subspace Visitation", "text": "We additionally derive the expected transition count from a subspace of states to a subspace of actions. This expresses the expectation that the user will perform an a y \u2208 A y from a s x \u2208 S p . It is defined as: \nD Ay,Sp|\u03be0\u2192t E P (\u03be t+1\u2192T |\u03be0\u2192t) T \u03c4 =t+1 I(s \u03c4 \u2208 S p )I(a \u03c4 \u2208 A y )(16)\nAgain, computing this quantity is straightforward with D sx|\u03be0\u2192t . By marginalizing D sx|\u03be0\u2192 over various action and state subspaces that have semantic meaning, different inference quantities can be expressed and computed.     ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E. Additional Dataset Information", "text": "Objects: The set of objects available in each environment is shown in Table 7. Scene types: The set of scene types in each environment is shown in Table 8. Labels: A small snippet of ground truth for Home 1 is shown in Table 9. The ground truth pairs frame indices (timestamps) with actions and goal arrivals.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_12", "tab_13", "tab_14"]}, {"heading": "F. Regret with Detectors", "text": "We additionally show the empirical regret when using our goal discovery and action detection methods in Figure 7. We observe somewhat noisier regret behavior than in the original case, as the underlying demonstrations are noisier. The number of trajectories in Office 2 is higher here due to errors in the goal forecasting method, resulting in more goals being detected, which segments the demonstrations into more trajectories.  The person's location is in green, images from the camera are inset at top left, and goal posteriors are colored according to the above colormaps. Before grabbing the mug (Figure 8a), DARKO forecasts roughly equivalent probability to bedroom and kitchen. After the user grabs the mug (Figure 8b), DARKO correctly predicts the user is likeliest to go to the kitchen.", "publication_ref": [], "figure_ref": ["fig_7", "fig_9", "fig_9"], "table_ref": []}, {"heading": "G. Visualizations", "text": "We provide example 3D visualizations of 1) goal posterior 2) future state visitation and 3) the value function.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G.1. Goal posterior visualization", "text": "To emphasize our empirical finding that modeling the interaction with objects is useful for goal posterior prediction, we show an example sequence of predictions in Figure 8. Before the user grabs the mug, our algorithm predicts roughly equivalent probability to both the bedroom and kitchen. We see that after the user grabs the mug, DARKO has high confidence that the user will go to the kitchen.", "publication_ref": [], "figure_ref": ["fig_9"], "table_ref": []}, {"heading": "G.2. Future state visitation visualizations", "text": "See Figure 9 for example visualizations of the expected future visitation counts. In order to visualize in 3 dimensions, we take the max visitation count across all states at each position. In rows 1, 3, and 4, a single demonstration is shown, which adapts to the agent's trajectory (history). In row 2, the future state distribution drastically changes after each time the agent reaches a new goal.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G.3. Value function visualizations", "text": "See Figure 10 for example visualizations of the value function over time. Note 1) the state space size changes, and 2) that the value function changes over time, as the component of state that indicates the previous goal affects the value function.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "G.4. RNN baseline settings", "text": "We experimented with a variety of settings for the RNN baseline. After each goal is detected, the RNN is refit. The settings we experimented with are cell \u2208 {GRU, Basic}, learning rate \u2208 {0.1, 0.01, 0.001, 0.0001}, hidden dimension \u2208 {8, 16, 32, 64}, epochs after each goal \u2208 {5, 10, 50, 100}. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgements: We acknowledge the support of NSF NRI Grant 1227495 and JST CREST Grant JPMJCR14E1. The authors thank Drew Bagnell for technical discussion, Katherine Lagree and Gunnar A. Sigurdsson for data collection assistance, and the anonymous reviewers for their helpful comments.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Apprenticeship learning via inverse reinforcement learning", "journal": "ACM", "year": "2004", "authors": "P Abbeel; A Y Ng"}, {"ref_id": "b1", "title": "Social lstm: Human trajectory prediction in crowded spaces", "journal": "", "year": "2016-06", "authors": "A Alahi; K Goel; V Ramanathan; A Robicquet; L Fei-Fei; S Savarese"}, {"ref_id": "b2", "title": "Recognize human activities from partially observed videos", "journal": "", "year": "2013-06", "authors": "Y Cao; D Barrett; A Barbu; S Narayanaswamy; H Yu; A Michaux; Y Lin; S Dickinson; J Mark Siskind; S Wang"}, {"ref_id": "b3", "title": "Understanding egocentric activities", "journal": "IEEE", "year": "2011", "authors": "A Fathi; A Farhadi; J M Rehg"}, {"ref_id": "b4", "title": "Towards internet-scale multi-view stereo", "journal": "", "year": "2010", "authors": "Y Furukawa; B Curless; S M Seitz; R Szeliski"}, {"ref_id": "b5", "title": "Max-margin early event detectors", "journal": "International Journal of Computer Vision", "year": "2014", "authors": "M Hoai; F De La; Torre "}, {"ref_id": "b6", "title": "Activity forecasting", "journal": "Springer", "year": "2012", "authors": "K M Kitani; B D Ziebart; J A Bagnell; M Hebert"}, {"ref_id": "b7", "title": "Anticipating human activities using object affordances for reactive robotic response", "journal": "", "year": "2016", "authors": "H S Koppula; A Saxena"}, {"ref_id": "b8", "title": "A hierarchical representation for future action prediction", "journal": "Springer", "year": "2014", "authors": "T Lan; T.-C Chen; S Savarese"}, {"ref_id": "b9", "title": "Discovering important people and objects for egocentric video summarization", "journal": "", "year": "2012", "authors": "Y J Lee; J Ghosh; K Grauman"}, {"ref_id": "b10", "title": "Prediction of human activity by discovering temporal sequence patterns", "journal": "", "year": "2014", "authors": "K Li; Y Fu"}, {"ref_id": "b11", "title": "Delving into egocentric actions", "journal": "", "year": "2015-06", "authors": "Y Li; Z Ye; J M Rehg"}, {"ref_id": "b12", "title": "Going deeper into firstperson activity recognition", "journal": "", "year": "2016", "authors": "M Ma; H Fan; K M Kitani"}, {"ref_id": "b13", "title": "A gametheoretic approach to multi-pedestrian activity forecasting", "journal": "", "year": "2016", "authors": "W.-C Ma; D.-A Huang; N Lee; K M Kitani"}, {"ref_id": "b14", "title": "Orb-slam: a versatile and accurate monocular slam system", "journal": "IEEE Transactions on Robotics", "year": "2015", "authors": "R Mur-Artal; J Montiel; J D Tard\u00f3s"}, {"ref_id": "b15", "title": "Detecting activities of daily living in first-person camera views", "journal": "IEEE", "year": "2012", "authors": "H Pirsiavash; D Ramanan"}, {"ref_id": "b16", "title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods", "journal": "Advances in large margin classifiers", "year": "1999", "authors": "J Platt"}, {"ref_id": "b17", "title": "Maximum margin planning", "journal": "ACM", "year": "2006", "authors": "N D Ratliff; J A Bagnell; M A Zinkevich"}, {"ref_id": "b18", "title": "You only look once: Unified, real-time object detection", "journal": "", "year": "2016", "authors": "J Redmon; S Divvala; R Girshick; A Farhadi"}, {"ref_id": "b19", "title": "Learning action maps of large environments via first-person vision", "journal": "", "year": "2016", "authors": "N Rhinehart; K M Kitani"}, {"ref_id": "b20", "title": "Human activity prediction: Early recognition of ongoing activities from streaming videos", "journal": "", "year": "2011", "authors": "M S Ryoo"}, {"ref_id": "b21", "title": "First-person activity recognition: What are they doing to me?", "journal": "", "year": "2013", "authors": "M S Ryoo; L Matthies"}, {"ref_id": "b22", "title": "Online learning and online convex optimization. Foundations and Trends R in Machine Learning", "journal": "", "year": "2012", "authors": "S Shalev-Shwartz"}, {"ref_id": "b23", "title": "Very deep convolutional networks for large-scale image recognition", "journal": "", "year": "2014", "authors": "K Simonyan; A Zisserman"}, {"ref_id": "b24", "title": "Egocentric future localization", "journal": "", "year": "2016-06", "authors": "H Soo Park; J.-J Hwang; Y Niu; J Shi"}, {"ref_id": "b25", "title": "Social behavior prediction from first person videos", "journal": "", "year": "2016", "authors": "S Su; J P Hong; J Shi; H S Park"}, {"ref_id": "b26", "title": "Learning to filter with predictive state inference machines", "journal": "", "year": "2016", "authors": "W Sun; A Venkatraman; B Boots; J A Bagnell"}, {"ref_id": "b27", "title": "Anticipating visual representations from unlabeled video", "journal": "", "year": "2016-06", "authors": "C Vondrick; H Pirsiavash; A Torralba"}, {"ref_id": "b28", "title": "Patch to the future: Unsupervised visual prediction", "journal": "IEEE", "year": "2014", "authors": "J Walker; A Gupta; M Hebert"}, {"ref_id": "b29", "title": "Time series analysis", "journal": "Addison-Wesley publ Reading", "year": "1994", "authors": "W W S Wei"}, {"ref_id": "b30", "title": "Inferring \"dark matter\" and \"dark energy\" from videos", "journal": "", "year": "2013", "authors": "D Xie; S Todorovic; S.-C Zhu"}, {"ref_id": "b31", "title": "Learning deep features for scene recognition using places database", "journal": "", "year": "2014", "authors": "B Zhou; A Lapedriza; J Xiao; A Torralba; A Oliva"}, {"ref_id": "b32", "title": "Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal Entropy", "journal": "", "year": "2010", "authors": "B D Ziebart"}, {"ref_id": "b33", "title": "Maximum entropy inverse reinforcement learning", "journal": "", "year": "2008", "authors": "B D Ziebart; A L Maas; J A Bagnell; A K Dey"}, {"ref_id": "b34", "title": "Planning-based prediction for pedestrians", "journal": "", "year": "2009", "authors": "B D Ziebart; N Ratliff; G Gallagher; C Mertz; K Peterson; J A Bagnell; M Hebert; A K Dey; S Srinivasa"}, {"ref_id": "b35", "title": "IEEE/RSJ International Conference on Intelligent Robots and Systems", "journal": "IEEE", "year": "2009", "authors": ""}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Algorithm 11DARKO: Discovering Agent Rewards for Kfutures Online 1: procedure DARKO(SLAM, ACTDET, GOALDET) 2: s \u2190 0, \u03b8 = 0, S = {} , S g = {}, T.INIT(), \u03be = [] y, z] \u2190 SLAM.TRACK(frame) 6: a \u2190 ACTDET([x, y, z], frame) 7: \u03be \u2190 \u03be \u2295 (s, a), S \u2190 S \u222a {s} 8:T.EXPAND(s, a), s \u2190 T (s, a)9:", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "12 :12S g \u2190 S g \u222a {s} 13: \u03c0, \u03b8 \u2190 ONLINEIRL(\u03b8, S, T , \u03be, S g ) 14: s \u2190 T (s, a = at goal), \u03be = [] 15: end if 16: end while 17: end procedure Algorithm 2 Online Inverse Reinforcement Learning 1: procedure ONLINEIRL(\u03b8, S, T , \u03be, S g ; \u03bb, B) 2:", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Goal posterior forecasting over time: P g * vs. fraction of trajectory length, across all trajectories. DARKO outperforms other methods and becomes more confident in the correct goal as the trajectories elapse.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure 5: Empirical regret. DARKO exhibits sublinear convergence in average regret. Initial noise is overcome after DARKO adjusts to the user's early behaviors.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Day,Sp|\u03be0\u2192t E P (\u03be t+1\u2192T |\u03be0\u2192t) T \u03c4 =t+1 I(s \u03c4 \u2208 S p )I(a \u03c4 = a y ) (14) = E P (\u03be t+1\u2192T |\u03be0\u2192t) \u03c4 = s x )I(a \u03c4 = a y ) \u03c4 = s x )I(a \u03c4 = a y ) = sx\u2208Sp D ay,sx|\u03be0\u2192t = sx\u2208Sp \u03c0(a y |s x )D sx|\u03be0\u2192t", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "= E P (\u03be t+1\u2192T |\u03be0\u2192t)\u03c4 = s x )I(a \u03c4 = a y ) \u03c4 = s x )I(a \u03c4 = a y ) = ay\u2208Ay sx\u2208Sp D ay,sx|\u03be0\u2192t = ay\u2208Ay sx\u2208Sp \u03c0(a y |s x )D sx|\u03be0\u2192t .", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 7 :7Figure 7: Noisy Empirical Regret. DARKO's online behavior model exhibits sublinear convergence in average regret. Initial noise is overcome after DARKO adjusts to learning about the user's early behaviors.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "(a) Goal Posterior Before Mug Acquired (b) Goal Posterior After Mug Acquired", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 8 :8Figure 8: Goal Posterior Change Visualization: Goal posteriors for two frames are visualized in the Home 1 environment.The person's location is in green, images from the camera are inset at top left, and goal posteriors are colored according to the above colormaps. Before grabbing the mug (Figure8a), DARKO forecasts roughly equivalent probability to bedroom and kitchen. After the user grabs the mug (Figure8b), DARKO correctly predicts the user is likeliest to go to the kitchen.", "figure_data": ""}, {"figure_label": "9210", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 9 : 2 Figure 10 :9210Figure 9: Future state visitation predictions changing as the agent (blue sphere) follows their trajectory. The state visitations are projected to 3D by taking the max over all states at each location. The visualizations are, by row: Office 1, Lab 1, Home 1., Office 2", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Goal Discovery and Action Recognition. The per-scene goal discovery and action recognition accuracies are shown for our methods. A 3-second window is used around every goal discovery to compute accuracy.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": "MethodHome 1 Home 2 Office 1 Office 2 Lab 1DARKO0.5240.3780.6670.3920.473MMED [6]0.4030.2990.6000.3820.297RNN0.2910.2740.3970.3130.455Logistic0.4580.2970.5690.3230.348Uniform0.1810.0980.2330.1110.113"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Goal Forecasting Results (Visual Detections):", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Goal Forecasting Results (Labelled Detections): Proposed goal posterior achieves best P g * (mean probability of true goal). Methods benefit from better detections.", "figure_data": "MethodHome 1 Home 2 Office 1 Office 2 Lab 1Scene-based0.4380.3460.5600.2380.426Stop-based0.6140.3950.6440.6250.709"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Visual goal discovery: Better goal discovery (cf. Table1) yields better P g", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Trajectory length forecasting results. Error is relative to the true length of each trajectory. Most trajectory forecasts are fairly accurate.", "figure_data": "expected trajectory length as derived in Section 4.1. For then-th episode, we use the normalized trajectory length pre-diction error defined as n =Tn t=1|\u03c4nt\u2212\u03c4nt| \u03c4nt"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Feature Ablation Results: Full state and action features (Sec. 3.1) yield best goal prediction results. Figure6: Relative improvement from incorporating goal uncertainty. Per-scene violin plots, means, and standard deviations are shown. Per-scene one-sided paired t-tests are performed, testing the hypotheses that incorporating goal uncertainty improves goal prediction performance. A * indicates p < 0.05, and ** indicates p < 0.005.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Environment Object Set Home 1 {bookbag, book, blanket, coat, laptop, mug, plate, snack, towel} Home 2 {bookbag, book, blanket, coat, guitar, laptop, mug, plate, remote, snack, towel} Office 1 {bookbag, textbook, bottle, coat, laptop, mug, paper, plate, snack} Office 2 {bookbag, textbook, bottle, coat, laptop, mug, paper, plate, snack} Lab 1 {beaker, coat, plate, pipette, tube}", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Objects available in each environment.", "figure_data": "Environment Scene Type SetHome 1{bathroom, bedroom, exit, dining room, kitchen, living room, office}Home 2{bathroom, bedroom, exit, dining room, kitchen, living room, office, television stand}Office 1{bathroom, exit, kitchen, lounge, office, printer station, water fountain}Office 2{bathroom, exit, kitchen, lounge, office, printer room, water fountain}Lab 1{cabinet stand, exit, gel room, lab bench 1, lab bench 2, refrigeration room}"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Scene types available in each environment. Arrival Release Coat Acquire Bookbag Arrive Office Acquire Mug Arrive Kitchen Release Mug", "figure_data": "Frame Index675069007200740076307700Action/"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Labels example: A small snippet of ground truth labels for Home 1.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "M = {S, A, T (\u2022, \u2022), R \u03b8 (\u2022, \u2022)}.", "formula_coordinates": [3.0, 107.89, 346.77, 120.7, 9.65]}, {"formula_id": "formula_1", "formula_text": "s = [x, y, z, o 1 . . . , o |O| , h 1 , . . . h |K| ].", "formula_coordinates": [3.0, 93.13, 457.53, 150.21, 9.96]}, {"formula_id": "formula_2", "formula_text": "R t = t i=0 l t (\u03be t ; \u03b8 t ) \u2212 min \u03b8 * t i=0 l(\u03be t ; \u03b8 * ).", "formula_coordinates": [4.0, 89.24, 571.7, 158.0, 30.32]}, {"formula_id": "formula_3", "formula_text": "f i = (s,a)\u2208\u03be f (s, a) 3: Compute R(s, a; \u03b8) \u2200s \u2208 S, a \u2208 A 4: \u03c0 \u2190 SOFTVALUEITERATION(R, S, S g , T ) 5:f i \u2190 E \u03c0 [f (s, a)] 6: \u03b8 \u2190 proj \u03b8 2\u2264B (\u03b8 \u2212 \u03bb(f i \u2212f i )) 7:", "formula_coordinates": [4.0, 314.62, 345.78, 199.12, 72.07]}, {"formula_id": "formula_4", "formula_text": "R t \u2264 2B \u221a 2td,(1)", "formula_coordinates": [4.0, 395.43, 465.8, 149.68, 18.63]}, {"formula_id": "formula_5", "formula_text": "D sx|\u03be0\u2192t E P (\u03be t+1\u2192T |\u03be0\u2192t) T \u03c4 =t+1 I(s \u03c4 = s x ) . (2)", "formula_coordinates": [5.0, 60.18, 132.7, 226.19, 30.2]}, {"formula_id": "formula_6", "formula_text": "D Sp|\u03be0\u2192t E P (\u03be t+1\u2192T |\u03be0\u2192t) T \u03c4 =t+1 I(s \u03c4 \u2208 S p )(3)", "formula_coordinates": [5.0, 55.61, 455.38, 230.76, 30.2]}, {"formula_id": "formula_7", "formula_text": "= sx\u2208Sp E P (\u03be t+1\u2192T |\u03be0\u2192t) T \u03c4 =t+1 I(s \u03c4 = s x ) = sx\u2208Sp D sx|\u03be0\u2192t . (4", "formula_coordinates": [5.0, 95.67, 491.04, 186.82, 59.79]}, {"formula_id": "formula_8", "formula_text": ")", "formula_coordinates": [5.0, 282.49, 531.08, 3.87, 8.64]}, {"formula_id": "formula_9", "formula_text": "\u03c4 \u03be t+1\u2192T |\u03be0\u2192t E P (\u03be t+1\u2192T |\u03be0\u2192t) |\u03be t+1\u2192T | (5)", "formula_coordinates": [5.0, 83.28, 700.56, 203.08, 10.57]}, {"formula_id": "formula_10", "formula_text": "D S|\u03be0\u2192t = E P (\u03be t+1\u2192T |\u03be0\u2192t) T \u03c4 =t+1 I(s \u03c4 \u2208 S) = E P (\u03be t+1\u2192T |\u03be0\u2192t) T \u03c4 =t+1 1 = E |\u03be t+1\u2192T | =\u03c4 \u03be t+1\u2192T |\u03be0\u2192t(6)", "formula_coordinates": [5.0, 330.36, 118.69, 214.76, 82.46]}, {"formula_id": "formula_11", "formula_text": "P (g|\u03be 0\u2192t ) \u221d P (g)e Vs t (g)\u2212Vs 0 (g) ,(7)", "formula_coordinates": [5.0, 358.87, 338.82, 186.25, 11.72]}, {"formula_id": "formula_12", "formula_text": "* |{\u03be n } N n=1 ) = 1 N N n=1", "formula_coordinates": [7.0, 51.31, 529.31, 235.05, 26.43]}, {"formula_id": "formula_13", "formula_text": "R t \u2264 2B \u221a 2td,(8)", "formula_coordinates": [12.0, 266.06, 110.65, 279.05, 18.63]}, {"formula_id": "formula_14", "formula_text": "R t \u2264 1 2\u03bb \u03b8 2 2 + \u03bb t i=1 \u2207 \u03b8t 2 2 .(9)", "formula_coordinates": [12.0, 234.31, 208.64, 310.8, 30.32]}, {"formula_id": "formula_15", "formula_text": "\u2207 \u03b8 2 2 = f \u2212f 2 2 =f Tf +f Tf \u2212 2f Tf(10)", "formula_coordinates": [12.0, 240.4, 289.92, 304.71, 28.15]}, {"formula_id": "formula_16", "formula_text": "0 \u2264 x \u2212 y 2 2 = x T x + y T y \u2212 2x T y 2x T y \u2264 x T x + y T y 2(\u2212x) T y \u2264 (\u2212x) T (\u2212x) + y T y \u22122x T y \u2264 x T x + y T y, \u2234 \u22122f Tf \u2264f Tf +f Tf , (Setting x =f , y =f )", "formula_coordinates": [12.0, 196.94, 356.92, 201.35, 77.84]}, {"formula_id": "formula_17", "formula_text": "\u2207 \u03b8 2 2 \u2264f Tf +f Tf +f Tf +f Tf = 2f Tf + 2f Tf \u2264 4d. (Sincef ,f \u2208 [0, 1] d )(11)", "formula_coordinates": [12.0, 227.71, 473.27, 317.4, 45.27]}, {"formula_id": "formula_18", "formula_text": "R t \u2264 B 2 2\u03bb + \u03bb t i=1 4d = B 2 2\u03bb + 4\u03bbtd.", "formula_coordinates": [12.0, 254.6, 570.27, 86.02, 57.57]}, {"formula_id": "formula_19", "formula_text": "\u03bb = B 2 \u221a 2td , R t \u2264 B \u221a 2td + 2Btd \u221a 2td = 2B \u221a 2td", "formula_coordinates": [12.0, 186.47, 640.49, 181.33, 51.26]}, {"formula_id": "formula_20", "formula_text": "D ay,sx|\u03be0\u2192t E P (\u03be t+1\u2192T |\u03be0\u2192t) T \u03c4 =t+1 I(s \u03c4 = s x )I(a \u03c4 = a y )(12)", "formula_coordinates": [13.0, 168.83, 153.57, 376.29, 30.2]}, {"formula_id": "formula_21", "formula_text": "D ay,sx|\u03be0\u2192t = \u03c0(a y |s x )D sx|\u03be0\u2192t(13)", "formula_coordinates": [13.0, 168.83, 189.8, 376.28, 9.96]}, {"formula_id": "formula_23", "formula_text": "D Ay,Sp|\u03be0\u2192t E P (\u03be t+1\u2192T |\u03be0\u2192t) T \u03c4 =t+1 I(s \u03c4 \u2208 S p )I(a \u03c4 \u2208 A y )(16)", "formula_coordinates": [13.0, 141.17, 516.18, 403.94, 30.2]}], "doi": ""}