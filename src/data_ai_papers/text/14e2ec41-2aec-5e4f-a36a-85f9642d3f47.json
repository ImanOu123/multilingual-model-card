{"title": "Vine Pruning for Efficient Multi-Pass Dependency Parsing", "authors": "Alexander M Rush; Slav Petrov", "pub_date": "", "abstract": "Coarse-to-fine inference has been shown to be a robust approximate method for improving the efficiency of structured prediction models while preserving their accuracy. We propose a multi-pass coarse-to-fine architecture for dependency parsing using linear-time vine pruning and structured prediction cascades. Our first-, second-, and third-order models achieve accuracies comparable to those of their unpruned counterparts, while exploring only a fraction of the search space. We observe speed-ups of up to two orders of magnitude compared to exhaustive search. Our pruned third-order model is twice as fast as an unpruned first-order model and also compares favorably to a state-of-the-art transition-based parser for multiple languages.", "sections": [{"heading": "Introduction", "text": "Coarse-to-fine inference has been extensively used to speed up structured prediction models. The general idea is simple: use a coarse model where inference is cheap to prune the search space for more complex models. In this work, we present a multipass coarse-to-fine architecture for graph-based dependency parsing. We start with a linear-time vine pruning pass and build up to higher-order models, achieving speed-ups of two orders of magnitude while maintaining state-of-the-art accuracies.\nIn constituency parsing, exhaustive inference for all but the simplest grammars tends to be prohibitively slow. Consequently, most high-accuracy constituency parsers routinely employ a coarse grammar to prune dynamic programming chart cells * Research conducted at Google. of the final grammar of interest (Charniak et al., 2006;Carreras et al., 2008;Petrov, 2009). While there are no strong theoretical guarantees for these approaches, 1 in practice one can obtain significant speed improvements with minimal loss in accuracy. This benefit comes primarily from reducing the large grammar constant |G| that can dominate the runtime of the cubic-time CKY inference algorithm. Dependency parsers on the other hand do not have a multiplicative grammar factor |G|, and until recently were considered efficient enough for exhaustive inference. However, the increased model complexity of a third-order parser forced  to prune with a first-order model in order to make inference practical. While fairly effective, all these approaches are limited by the fact that inference in the coarse model remains cubic in the sentence length. The desire to parse vast amounts of text necessitates more efficient dependency parsing algorithms.\nWe thus propose a multi-pass coarse-to-fine approach where the initial pass is a linear-time sweep, which tries to resolve local ambiguities, but leaves arcs beyond a fixed length b unspecified (Section 3). The dynamic program is a form of vine parsing (Eisner and Smith, 2005), which we use to compute parse max-marginals, rather than for finding the 1best parse tree. To reduce pruning errors, the parameters of the vine parser (and all subsequent pruning models) are trained using the structured prediction cascades of Weiss and Taskar (2010) to optimize for pruning efficiency, and not for 1-best prediction (Section 4). Despite a limited scope of b = 3, the vine pruning pass is able to preserve >98% of the correct arcs, while ruling out \u223c86% of all possible arcs. Subsequent i-th order passes introduce larger scope features, while further constraining the search space. In Section 5 we present experiments in multiple languages. Our coarse-to-fine first-, second-, and third-order parsers preserve the accuracy of the unpruned models, but are faster by up to two orders of magnitude. Our pruned third-order model is faster than an unpruned first-order model, and compares favorably in speed to the state-of-the-art transitionbased parser of Zhang and Nivre (2011).\nIt is worth noting the relationship to greedy transition-based dependency parsers that are also linear-time (Nivre et al., 2004) or quadratic-time (Yamada and Matsumoto, 2003). It is their success that motivates building explicitly trained, linear-time pruning models. However, while a greedy solution for arc-standard transition-based parsers can be computed in linear-time, Kuhlmann et al. (2011) recently showed that computing exact solutions or (max-)marginals has time complexity O(n 4 ), making these models inappropriate for coarse-to-fine style pruning. As an alternative, Roark and Hollingshead (2008) and Bergsma and Cherry (2010) present approaches where individual classifiers are used to prune chart cells. Such approaches have the drawback that pruning decisions are made locally and therefore can rule out all valid structures, despite explicitly evaluating O(n 2 ) chart cells. In contrast, we make pruning decisions based on global parse max-marginals using a vine pruning pass, which is linear in the sentence length, but nonetheless guarantees to preserve a valid parse structure.", "publication_ref": ["b4", "b2", "b26", "b9", "b31", "b33", "b21", "b32", "b15", "b27", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Motivation & Overview", "text": "The goal of this work is fast, high-order, graphbased dependency parsing. Previous work on constituency parsing demonstrates that performing several passes with increasingly more complex models results in faster inference (Charniak et al., 2006;Petrov and Klein, 2007). The same technique applies to dependency parsing with a cascade of models of increasing order; however, this strategy is limited by the speed of the simplest model. The algorithm for first-order dependency parsing (Eisner, 2000) already requires O(n 3 ) time, which Lee 1 2 3 4 5 6 7 8  (2002) shows is a practical lower bound for parsing of context-free grammars. This bound implies that it is unlikely that there can be an exhaustive parsing algorithm that is asymptotically faster than the standard approach. We thus need to leverage domain knowledge to obtain faster parsing algorithms. It is well-known that natural language is fairly linear, and most headmodifier dependencies tend to be short. This property is exploited by transition-based dependency parsers (Yamada and Matsumoto, 2003;Nivre et al., 2004) and empirically demonstrated in Figure 1. The heat map on the left shows that most of the probability mass of modifiers is concentrated among nearby words, corresponding to a diagonal band in the matrix representation. On the right we show the frequency of arc lengths for different modifier partof-speech tags. As one can expect, almost all arcs involving adjectives (ADJ) are very short (length 3 or less), but even arcs involving verbs and nouns are often short. This structure suggests that it may be possible to disambiguate most dependencies by considering only the \"banded\" portion of the sentence.\nWe exploit this linear structure by employing a variant of vine parsing (Eisner and Smith, 2005 vine parser only for pruning and augment it to allow arcs to remain unspecified (by including so called outer arcs). The vine parser can thereby eliminate a possibly quadratic number of arcs, while having the flexibility to defer some decisions and preserve ambiguity to be resolved by later passes. In Figure 2 for example, the vine pass correctly determined the head-word of McGwire as neared, limited the headword candidates for fans to neared and went, and decided that the head-word for went falls outside the band by proposing an outer arc. A subsequent firstorder pass needs to score only a small fraction of all possible arcs and can be used to further restrict the search space for the following higher-order passes.", "publication_ref": ["b4", "b24", "b10", "b32", "b21", "b9"], "figure_ref": ["fig_0"], "table_ref": ["tab_4"]}, {"heading": "Graph-Based Dependency Parsing", "text": "Graph-based dependency parsing models factor all valid parse trees for a given sentence into smaller units, which can be scored independently. For instance, in a first-order factorization, the units are just dependency arcs. We represent these units by an index set I and use binary vectors Y \u2282 {0, 1} |I| to specify a parse tree y \u2208 Y such that y(i) = 1 iff the index i exists in the tree. The index sets of higherorder models can be constructed out of the index sets of lower-order models, thus forming a hierarchy that we will exploit in our coarse-to-fine cascade. The inference problem is to find the 1-best parse tree arg max y\u2208Y y \u2022 w, where w \u2208 R |I| is a weight vector that assigns a score to each index i (we dis-cuss how w is learned in Section 4). A generalization of the 1-best inference problem is to find the max-marginal score for each index i. Maxmarginals are given by the function M : I \u2192 Y defined as M (i; Y, w) = arg max y\u2208Y:y(i)=1 y \u2022 w. For first-order parsing, this corresponds to the best parse utilizing a given dependency arc. Clearly there are exponentially many possible parse tree structures, but fortunately there exist well-known dynamic programming algorithms for searching over all possible structures. We review these below, starting with the first-order factorization for ease of exposition.\nThroughout the paper we make use of some basic mathematical notation. We write [c] for the enumeration {1, . . . , c} and [c] a for {a, . . . , c}. We use 1[c] for the indicator function, equal to 1 if condition c is true and 0 otherwise. Finally we use [c] + = max{0, c} for the positive part of c.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "First-Order Parsing", "text": "The simplest way to index a dependency parse structure is by the individual arcs of the parse tree. This model is known as first-order or arc-factored. For a sentence of length n the index set is:\nI 1 = {(h, m) : h \u2208 [n] 0 , m \u2208 [n]}\nEach dependency tree has y(h, m) = 1 iff it includes an arc from head h to modifier m. We follow common practice and use position 0 as the pseudo-root ( * ) of the sentence. The full set I 1 has cardinality The first-order bilexical parsing algorithm of Eisner ( 2000) can be used to find the best parse tree and max-marginals. The algorithm defines a dynamic program over two types of items: incomplete items I(h, m) that denote the span between a modifier m and its head h, and complete items C(h, e) that contain a full subtree spanning from the head h and to the word e on one side. The algorithm builds larger items by applying the composition rules shown in Figure 3. Rule 3(a) builds an incomplete item I(h, m) by attaching m as a modifier to h. This rule has the effect that y(h, m) = 1 in the final parse. Rule 3(b) completes item I(h, m) by attaching item C(m, e). The existence of I(h, m) implies that m modifies h, so this rule enforces that the constituents of m are also constituents of h.\n|I 1 | = O(n 2 ). (a) h m \u2190 I h r + C m r + 1 C (b) h e \u2190 C h m + I m e C\nWe can find the best derivation for each item by adapting the standard CKY parsing algorithm to these rules. Since both rule types contain three variables that can range over the entire sentence (h, m, e \u2208 [n] 0 ), the bottom-up, inside dynamic programming algorithm requires O(n 3 ) time. Furthermore, we can find max-marginals with an additional top-down outside pass also requiring cubic time. To speed up search, we need to filter indices from I 1 and reduce possible applications of Rule 3(a).", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Higher-Order Parsing", "text": "Higher-order models generalize the index set by using siblings s (modifiers that previously attached to a head word) and grandparents g (head words above the current head word). For compactness, we use g 1 for the head word and s k+1 for the modifier and parameterize the index set to capture arbitrary higher- order decisions in both directions:\n(c) V \u2190 0 e \u2190 C 0 e \u2212 1 + e e \u2212 1 C (d) 0 e \u2190 V \u2190 0 m + V \u2190 e m I (e) 0 e \u2190 V \u2192 0 e V \u2190 (f) 0 e \u2190 V \u2192 0 m + V \u2192 e m I (g) 0 e \u2190 C 0 e \u2212 1 + V \u2192 e \u2212 1 e C\nI k,l = {(g, s) : g \u2208 [n] l+1 0 , s \u2208 [n] k+1 }\nwhere k + 1 is the sibling order, l + 1 is the parent order, and k + l + 1 is the model order. The canonical second-order model uses I 1,0 , which has a cardinality of O(n 3 ). Although there are several possibilities for higher-order models, we use I 1,1 as our third-order model. Generally, the parsing index set has cardinality |I k,l | = O(n 2+k+l ). Inference in higher-order models uses variants of the dynamic program for first-order parsing, and we refer to previous work for the full set of rules. For second-order models with index set I 1,0 , parsing can be done in O(n 3 ) time (McDonald and Pereira, 2006) and for third-order models in O(n 4 ) time . Even though second-order parsing has the same asymptotic time complexity as first-order parsing, inference is significantly slower due to the cost of scoring the larger index set. We aim to prune the index set, by mapping each higher-order index down to a set of small set indices that can be pruned using a coarse pruning model. For example, to use a first-order model for pruning, we would map the higher-order index to the individual indices for its arc, grandparents, and siblings:\np k,l\u21921 (g, s) = {(g 1 , s j ) : j \u2208 [k + 1]} \u222a {(g j+1 , g j ) : j \u2208 [l]}\nThe first-order pruning model can then be used to score these indices, and to produce a filtered index set F (I 1 ) by removing low-scoring indices (see Section 4). We retain only the higher-order indices that are supported by the filtered index set:\n{(g, s) \u2208 I k,l : p k,l\u21921 (g, s) \u2282 F (I 1 )}", "publication_ref": ["b19"], "figure_ref": [], "table_ref": []}, {"heading": "Vine Parsing", "text": "To further reduce the cost of parsing and produce faster pruning models, we need a model with less structure than the first-order model. A natural choice, following Section 2, is to only consider \"short\" arcs:\nS = {(h, m) \u2208 I 1 : |h \u2212 m| \u2264 b}\nwhere b is a small constant. This constraint reduces the size of the set to |S| = O(nb).\nClearly, this index set is severely limited; it is necessary to have some long arcs for even short sentences. We therefore augment the index set to include outer arcs:\nI 0 = S \u222a {(d, m) : d \u2208 {\u2190, \u2192}, m \u2208 [n]} \u222a {(h, d) : h \u2208 [n] 0 , d \u2208 {\u2190, \u2192}}\nThe first set lets modifiers choose an outer headword and the second set lets head words accept outer modifiers, and both sets distinguish the direction of the arc. Figure 5 shows a right outer arc. The size of I 0 is linear in the sentence length. To parse the index set I 0 , we can modify the parse rules in Figure 3 to enforce additional length constraints (|h \u2212 e| \u2264 b for I(h, e) and |h \u2212 m| \u2264 b for C(h, m)). This way, only indices in S are explored. Unfortunately, this is not sufficient since the constraints also prevent the algorithm from producing a full derivation, since no item can expand beyond length b.\nEisner and Smith ( 2005) therefore introduce vine parsing, which includes two new items, vine left, V \u2190 (e), and vine right, V \u2192 (e). Unlike the previous items, these new items are left-anchored at the root and grow only towards the right. The items V \u2190 (e) and V \u2192 (e) encode the fact that a word e has not taken a close (within b) head word to its left or right.\nWe incorporate these items by adding the five new parsing rules shown in Figure 4. The major addition is Rule 4(e) which converts a vine left item V \u2190 (e) to a vine right item V \u2192 (e). This implies that word e has no close head to either side, and the parse has outer head arcs, y(\u2190, e) = 1 or y(\u2192, e) = 1. The other rules are structural and dictate creation and extension of vine items. Rules 4(c) and 4(d) create vine left items from items that cannot find a head word to their left. Rules 4(f) and 4(g) extend and finish vine right items. Rules 4(d) and 4(f) each leave a head word incomplete, so they may set y(e, \u2190) = 1 or y(m, \u2192) = 1 respectively. Note that for all the new parse rules, e \u2208 [n] 0 and m \u2208 {e \u2212 b . . . n}, so parsing time of this so called vine parsing algorithm is linear in the sentence length O(nb 2 ).\nAlone, vine parsing is a poor model of syntax -it does not even score most dependency pairs. However, it can act as a pruning model for other parsers. We prune a first-order model by mapping first-order indices to indices in I 0 .\np 1\u21920 (h, m) = \uf8f1 \uf8f2 \uf8f3 {(h, m)} if |h \u2212 m| \u2264 b {(\u2192, m), (h, \u2192)} if h < m {(\u2190, m), (h, \u2190)} if h > m\nThe remaining first-order indices are then given by:\n{(h, m) \u2208 I 1 : p 1\u21920 (h, m) \u2282 F (I 0 )}\nFigure 2 depicts a coarse-to-fine cascade, incorporating vine and first-order pruning passes and finishing with a higher-order parse model.", "publication_ref": [], "figure_ref": ["fig_3", "fig_1", "fig_2"], "table_ref": []}, {"heading": "Training Methods", "text": "Our coarse-to-fine parsing architecture consists of multiple pruning passes followed by a final pass of 1-best parsing. The training objective for the pruning models comes from the prediction cascade framework of Weiss and Taskar (2010), which explicitly trades off pruning efficiency versus accuracy. The models used in the final pass on the other hand are trained for 1-best prediction.", "publication_ref": ["b31"], "figure_ref": [], "table_ref": []}, {"heading": "Max-Marginal Filtering", "text": "At each pass of coarse-to-fine pruning, we apply an index filter function F to trim the index set:\nF (I) = {i \u2208 I : f (i) = 1}\nSeveral types of filters have been proposed in the literature, with most work in coarse-to-fine parsing focusing on predicates that threshold the posterior probabilities. In structured prediction cascades, we use a non-probabilistic filter, based on the maxmarginal value of the index:\nf (i; Y, w) = 1[ M (i; Y, w) \u2022 w < t \u03b1 (Y, w) ]\nwhere t \u03b1 (Y, w) is a sentence-specific threshold value. To counteract the fact that the max-marginals are not normalized, the threshold t \u03b1 (Y, w) is set as a convex combination of the 1-best parse score and the average max-marginal value:\nt \u03b1 (Y, w) = \u03b1 max y\u2208Y (y \u2022 w) + (1 \u2212 \u03b1) 1 |I| i\u2208I M (i; Y, w) \u2022 w\nwhere the model-specific parameter 0 \u2264 \u03b1 \u2264 1 is the tradeoff between \u03b1 = 1, pruning all indices i not in the best parse, and \u03b1 = 0, pruning all indices with max-marginal value below the mean.\nThe threshold function has the important property that for any parse y, if y \u2022 w \u2265 t \u03b1 (Y, w) then y(i) = 1 implies f (i) = 0, i.e. if the parse score is above the threshold, then none of its indices will be pruned.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Filter Loss Training", "text": "The aim of our pruning models is to filter as many indices as possible without losing the gold parse. In structured prediction cascades, we incorporate this pruning goal into our training objective.\nLet y be the gold output for a sentence. We define filter loss to be an indicator of whether any i with y(i) = 1 is filtered:\n\u2206(y, Y, w) = 1[\u2203i \u2208 y, M (i; Y, w) \u2022 w < t \u03b1 (Y, w)]\nDuring training we minimize the expected filter loss using a standard structured SVM setup (Tsochantaridis et al., 2006). First we form a convex, continuous upper-bound of our loss function:\n\u2206(y, Y, w) \u2264 1[y \u2022 w < t \u03b1 (Y, w)] \u2264 [1 \u2212 y \u2022 w + t \u03b1 (Y, w)] +\nwhere the first inequality comes from the properties of max-marginals and the second is the standard hinge-loss upper-bound on an indicator. Now assume that we have a corpus of P training sentences. Let the sequence (y (1) , . . . , y (P ) ) be the gold parses for each sentences and the sequence (Y (1) , . . . , Y (P ) ) be the set of possible output structures. We can form the regularized risk minimization for this upper bound of filter loss:\nmin w \u03bb w 2 + 1 P P p=1 [1 \u2212 y (p) \u2022 w + t \u03b1 (Y (p) , w)] +\nThis objective is convex and non-differentiable, due to the max inside t. We optimize using stochastic subgradient descent (Shalev-Shwartz et al., 2007). The stochastic subgradient at example p, H(w, p) is\n0 if y (p) \u2212 1 \u2265 t \u03b1 (Y, w) otherwise, H(w, p) = 2\u03bbw P \u2212 y (p) + \u03b1 arg max y\u2208Y (p) y \u2022 w + (1 \u2212 \u03b1) 1 |I (p) | i\u2208I (p) M (i; Y (p) , w)\nEach step of the algorithm has an update of the form:\nw k = w k\u22121 \u2212 \u03b7 k H(w, p)\nwhere \u03b7 is an appropriate update rate for subgradient convergence. If \u03b1 = 1 the objective is identical to structured SVM with 0/1 hinge loss. For other values of \u03b1, the subgradient includes a term from the features of all max-marginal structures at each index. These feature counts can be computed using dynamic programming. ", "publication_ref": ["b30", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "1-Best Training", "text": "For the final pass, we want to train the model for 1best output. Several different learning methods are available for structured prediction models including structured perceptron (Collins, 2002), max-margin models (Taskar et al., 2003), and log-linear models (Lafferty et al., 2001). In this work, we use the margin infused relaxed algorithm (MIRA) (Crammer and Singer, 2003;Crammer et al., 2006) with a hamming-loss margin. MIRA is an online algorithm with similar benefits as structured perceptron in terms of simplicity and fast training time. In practice, we found that MIRA with hamming-loss margin gives a performance improvement over structured perceptron and structured SVM.", "publication_ref": ["b5", "b29", "b16", "b6", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Parsing Experiments", "text": "To empirically demonstrate the effectiveness of our approach, we compare our vine pruning cascade with a wide range of common pruning methods on the Penn WSJ Treebank (PTB) (Marcus et al., 1993). We then also show that vine pruning is effective across a variety of different languages.\nFor English, we convert the PTB constituency trees to dependencies using the Stanford dependency framework (De Marneffe et al., 2006). We then train on the standard PTB split with sections 2-21 as training, section 22 as validation, and section 23 as test. Results are similar using the Yamada and Matsumoto (2003) conversion. We additionally selected six languages from the CoNLL-X shared task (Buchholz and Marsi, 2006) that cover a number of different language families: Bulgarian, Chinese, Japanese, German, Portuguese, and Swedish. We use the standard CoNLL-X training/test split and tune parameters with cross-validation.\nAll experiments use unlabeled dependencies for training and test. Accuracy is reported as unlabeled attachment score (UAS), the percentage of tokens with the correct head word. For English, UAS ignores punctuation tokens and the test set uses predicted POS tags. For the other languages we follow the CoNLL-X setup and include punctuation in UAS and use gold POS tags on the set set. Speedups are given in terms of time relative to a highly optimized C++ implementation. Our unpruned firstorder baseline can process roughly two thousand tokens a second and is comparable in speed to the greedy shift-reduce parser of Nivre et al. (2004).", "publication_ref": ["b18", "b8", "b32", "b1", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Models", "text": "Our parsers perform multiple passes over each sentence. In each pass we first construct a (pruned) hypergraph (Klein and Manning, 2005) and then perform feature computation and inference. We choose the highest \u03b1 that produces a pruning error of no more than 0.2 on the validation set (typically \u03b1 \u2248 0.6) to filter indices for subsequent rounds (similar to Weiss and Taskar (2010)). We compare a variety of pruning models:\nLENGTHDICTIONARY a deterministic pruning method that eliminates all arcs longer than the maximum length observed for each head-modifier POS pair.\nLOCAL an unstructured arc classifier that chooses indices from I 1 directly without enforcing parse constraints. Similar to the quadratic-time filter from Bergsma and Cherry (2010).\nLOCALSHORT an unstructured arc classifier that chooses indices from I 0 directly without enforcing parse constraints. Similar to the lineartime filter from Bergsma and Cherry (2010).\nFIRSTONLY a structured first-order model trained with filter loss for pruning.\nFIRSTANDSECOND a structured cascade with first-and second-order pruning models.\nVINECASCADE the full cascade with vine, firstand second-order pruning models.\nVINEPOSTERIOR the vine parsing cascade trained as a CRF with L-BFGS (Nocedal and Wright, 1999) and using posterior probabilities for filtering instead of max-marginals.\nZHANGNIVRE an unlabeled reimplementation of the linear-time, k-best, transition-based parser of Zhang and Nivre (2011). This parser uses composite features up to third-order with a greedy decoding algorithm. The reimplementation is about twice as fast as their reported speed, but scores slightly lower.\nWe found LENGTHDICTIONARY pruning to give significant speed-ups in all settings and therefore always use it as an initial pass. The maximum number of passes in a cascade is five: dictionary, vine, first-, and second-order pruning, and a final third-order 1best pass. 3 We tune the pruning thresholds for each round and each cascade separately. This is because we might be willing to do a more aggressive vine pruning pass if the final model is a first-order model, since these two models tend to often agree.", "publication_ref": ["b12", "b31", "b0", "b0", "b22", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "Features", "text": "For the non-pruning models, we use a standard set of features proposed in the discriminative graphbased dependency parsing literature (McDonald et al., 2005;Carreras, 2007;.  Included are lexical features, part-of-speech features, features on in-between tokens, as well as feature conjunctions, surrounding part-of-speech tags, and back-off features. In addition, we replicate each part-of-speech (POS) feature with an additional feature using coarse POS representations (Petrov et al., 2012). Our baseline parsing models replicate and, for some experiments, surpass previous best results.\nThe first-and second-order pruning models have the same structure, but for efficiency use only the basic features from McDonald et al. (2005). As feature computation is quite costly, future work may investigate whether this set can be reduced further. VINEPRUNE and LOCALSHORT use the same feature sets for short arcs. Outer arcs have features of the unary head or modifier token, as well as features for the POS tag bordering the cutoff and the direction of the arc.", "publication_ref": ["b20", "b3", "b25", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "A comparison between the pruning methods is shown in Table 1. The table gives relative speedups, compared to the unpruned first-order baseline, as well as accuracy, pruning efficiency, and oracle scores. Note particularly that the third-order cascade is twice as fast as an unpruned first-order model and >200 times faster than the unpruned third-order baseline. The comparison with poste-  rior pruning is less pronounced. Filter loss training is faster than VINEPOSTERIOR for first-and third-order parsing, but the two models have similar second-order speeds. It is also noteworthy that oracle scores are consistently high even after multiple pruning rounds: the oracle score of our third-order model for example is 97.4%. Vine pruning is particularly effective. The vine pass is faster than both LOCAL and FIRSTONLY and prunes more effectively than LOCALSHORT. Vine pruning benefits from having a fast, linear-time model, but still maintaining enough structure for pruning. While our pruning approach does not provide any asymptotic guarantees, Figure 6 shows that in practice our multi-pass parser scales well even for long sentences: Our first-order cascade scales almost linearly with the sentence length, while the third-order cascade scales better than quadratic. Table 2 shows that the final pass dominates the computational cost, while each of the pruning passes takes up roughly the same amount of time.\nOur second-and third-order cascades also significantly outperform ZHANGNIVRE. The transitionbased model with k = 8 is very efficient and effective, but increasing the k-best list size scales much worse than employing multi-pass pruning. We also note that while direct speed comparison are difficult, our parser is significantly faster than the published results for other high accuracy parsers, e.g. Huang and Sagae (2010) and .\nTable 3 shows our results across a subset of the CoNLL-X datasets, focusing on languages that differ greatly in structure. The unpruned models perform well across datasets, scoring comparably to the top results from the CoNLL-X competition. We see speed increases for our cascades with almost no loss in accuracy across all languages, even for languages with fairly free word order like German. This is Table 3: Speed and accuracy results for the vine pruning cascade across various languages. B is the unpruned baseline model, and V is the vine pruning cascade. The first section of the table gives results for the CoNLL-X test datasets for Bulgarian (BG), German (DE), Japanese (JA), Portuguese (PT), Swedish (SW), and Chinese (ZH). The second section gives the result for the English (EN) test set, PTB Section 23. encouraging and suggests that the outer arcs of the vine-pruning model are able to cope with languages that are not as linear as English.", "publication_ref": ["b11"], "figure_ref": ["fig_4"], "table_ref": ["tab_4"]}, {"heading": "Conclusion", "text": "We presented a multi-pass architecture for dependency parsing that leverages vine parsing and structured prediction cascades. The resulting 200-fold speed-up leads to a third-order model that is twice as fast as an unpruned first-order model for a variety of languages, and that also compares favorably to a state-of-the-art transition-based parser. Possible future work includes experiments using cascades to explore much higher-order models.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We would like to thank the members of the Google NLP Parsing Team for comments, suggestions, bugfixes and help in general: Ryan McDonald, Hao Zhang, Michael Ringgaard, Terry Koo, Keith Hall, Kuzman Ganchev and Yoav Goldberg. We would also like to thank Andre Martins for showing that MIRA with hamming-loss margin performs better than other 1-best training algorithms.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Fast and accurate arc filtering for dependency parsing", "journal": "", "year": "2010", "authors": "S Bergsma; C Cherry"}, {"ref_id": "b1", "title": "CoNLL-X shared task on multilingual dependency parsing", "journal": "", "year": "2006", "authors": "S Buchholz; E Marsi"}, {"ref_id": "b2", "title": "Tag, dynamic programming, and the perceptron for efficient, featurerich parsing", "journal": "", "year": "2008", "authors": "X Carreras; M Collins; T Koo"}, {"ref_id": "b3", "title": "Experiments with a higher-order projective dependency parser", "journal": "", "year": "2007", "authors": "X Carreras"}, {"ref_id": "b4", "title": "Multilevel coarse-to-fine PCFG parsing", "journal": "", "year": "2006", "authors": "E Charniak; M Johnson; M Elsner; J Austerweil; D Ellis; I Haxton; C Hill; R Shrivaths; J Moore; M Pozar"}, {"ref_id": "b5", "title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "journal": "", "year": "2002", "authors": "M Collins"}, {"ref_id": "b6", "title": "Ultraconservative online algorithms for multiclass problems", "journal": "The Journal of Machine Learning Research", "year": "2003", "authors": "K Crammer; Y Singer"}, {"ref_id": "b7", "title": "Online passive-aggressive algorithms", "journal": "The Journal of Machine Learning Research", "year": "2006", "authors": "K Crammer; O Dekel; J Keshet; S Shalev-Shwartz; Y Singer"}, {"ref_id": "b8", "title": "Generating typed dependency parses from phrase structure parses", "journal": "", "year": "2006", "authors": "M C De Marneffe; B Maccartney; C D Manning"}, {"ref_id": "b9", "title": "Parsing with soft and hard constraints on dependency length", "journal": "", "year": "2005", "authors": "J Eisner; N A Smith"}, {"ref_id": "b10", "title": "Bilexical grammars and their cubictime parsing algorithms", "journal": "", "year": "2000", "authors": "J Eisner"}, {"ref_id": "b11", "title": "Dynamic programming for linear-time incremental parsing", "journal": "", "year": "2010", "authors": "L Huang; K Sagae"}, {"ref_id": "b12", "title": "Parsing and hypergraphs. New developments in parsing technology", "journal": "", "year": "2005", "authors": "D Klein; C D Manning"}, {"ref_id": "b13", "title": "Efficient third-order dependency parsers", "journal": "", "year": "2010", "authors": "T Koo; M Collins"}, {"ref_id": "b14", "title": "Dual decomposition for parsing with nonprojective head automata", "journal": "", "year": "2010", "authors": "T Koo; A M Rush; M Collins; T Jaakkola; D Sontag"}, {"ref_id": "b15", "title": "Dynamic programming algorithms for transitionbased dependency parsers", "journal": "", "year": "2011", "authors": "M Kuhlmann; C G\u00f3mez-Rodr\u00edguez; G Satta"}, {"ref_id": "b16", "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "journal": "", "year": "2001", "authors": "J Lafferty; A Mccallum; F C N Pereira"}, {"ref_id": "b17", "title": "Fast context-free grammar parsing requires fast boolean matrix multiplication", "journal": "Journal of the ACM", "year": "2002", "authors": "L Lee"}, {"ref_id": "b18", "title": "Building a large annotated corpus of english: The penn treebank", "journal": "Computational linguistics", "year": "1993", "authors": "M P Marcus; M A Marcinkiewicz; B Santorini"}, {"ref_id": "b19", "title": "Online learning of approximate dependency parsing algorithms", "journal": "", "year": "2006", "authors": "R Mcdonald; F Pereira"}, {"ref_id": "b20", "title": "Online large-margin training of dependency parsers", "journal": "", "year": "2005", "authors": "R Mcdonald; K Crammer; F Pereira"}, {"ref_id": "b21", "title": "Memory-based dependency parsing", "journal": "", "year": "2004", "authors": "J Nivre; J Hall; J Nilsson"}, {"ref_id": "b22", "title": "Numerical Optimization", "journal": "Springer", "year": "1999", "authors": "J Nocedal; S J Wright"}, {"ref_id": "b23", "title": "Hierarchical search for parsing", "journal": "", "year": "2009", "authors": "A Pauls; D Klein"}, {"ref_id": "b24", "title": "Improved inference for unlexicalized parsing", "journal": "", "year": "2007", "authors": "S Petrov; D Klein"}, {"ref_id": "b25", "title": "A universal part-of-speech tagset", "journal": "", "year": "2012", "authors": "S Petrov; D Das; R Mcdonald"}, {"ref_id": "b26", "title": "Coarse-to-Fine Natural Language Processing", "journal": "", "year": "2009", "authors": "S Petrov"}, {"ref_id": "b27", "title": "Classifying chart cells for quadratic complexity context-free inference", "journal": "", "year": "2008", "authors": "B Roark; K Hollingshead"}, {"ref_id": "b28", "title": "Pegasos: Primal estimated sub-gradient solver for svm", "journal": "", "year": "2007", "authors": "S Shalev-Shwartz; Y Singer; N Srebro"}, {"ref_id": "b29", "title": "Max-margin markov networks", "journal": "", "year": "2003", "authors": "B Taskar; C Guestrin; D Koller"}, {"ref_id": "b30", "title": "Large margin methods for structured and interdependent output variables", "journal": "Journal of Machine Learning Research", "year": "2006", "authors": "I Tsochantaridis; T Joachims; T Hofmann; Y Altun"}, {"ref_id": "b31", "title": "Structured prediction cascades", "journal": "", "year": "2010", "authors": "D Weiss; B Taskar"}, {"ref_id": "b32", "title": "Statistical dependency analysis with support vector machines", "journal": "", "year": "2003", "authors": "H Yamada; Y Matsumoto"}, {"ref_id": "b33", "title": "Transition-based dependency parsing with rich non-local features", "journal": "", "year": "2011", "authors": "Y Zhang; J Nivre"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: (a) Heat map indicating how likely a particular head position is for each modifier position. Greener/darker is likelier. (b) Arc length frequency for three common modifier tags. Both charts are computed from all sentences in Section 22 of the PTB.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Parsing rules for first-order dependency parsing. The complete items C are represented by triangles and the incomplete items I are represented by trapezoids. Symmetric left-facing versions are also included.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Additional rules for vine parsing. Vine left (V \u2190 ) items are pictured as right-facing triangles and vine right (V \u2192 ) items are marked trapezoids. Each new item is anchored at the root and grows to the right.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure 5: An outer arc (1, \u2192) from the word \"As\" to possible right modifiers.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 6 :6Figure 6: Mean parsing speed by sentence length for first-, second-, and third-order parsers as well as different pruning methods for first-order parsing. [b] indicates the empirical complexity obtained from fitting ax b .", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Oracle UAS Speed PE Oracle UAS Speed PE Oracle UAS NOPRUNE 1.00 0.00 100 91.4 0.32 0.00 100 92.7 0.01 0.00 100 93.3 LENGTHDICTIONARY 1.94 43.9 99.9 91.5 0.76 43.9 99.9 92.8 0.05 43.9 99.9 93.3 LOCALSHORT 3.08 76.6 99.1 91.4 1.71 76.4 99.1 92.6 0.31 77.5 99.0 93.1 LOCAL 4.59 89.9 98.8 91.5 2.88 83.2 99.5 92.6 1.41 89.5 98.8 93.1 FIRSTONLY 3.10 95.5 95.9 91.5 2.83 92.5 98.4 92.6 1.61 92.2 98.5 93.Results comparing pruning methods on PTB Section 22. Oracle is the max achievable UAS after pruning. Pruning efficiency (PE) is the percentage of non-gold first-order dependency arcs pruned. Speed is parsing time relative to the unpruned first-order model (around 2000 tokens/sec). UAS is the unlabeled attachment score of the final parses.", "figure_data": "Setup FIRSTANDSECOND VINEPOSTERIOR VINECASCADEFirst-order Speed PE 1 Second-order Third-order --1.80 97.6 97.7 93.1 3.92 94.6 96.5 91.5 3.66 93.2 97.7 92.6 1.67 96.5 97.9 93.1 5.24 95.0 95.7 91.5 3.99 91.8 98.7 92.6 2.22 97.8 97.4 93.1k=8k=16k=64ZHANGNIVRE4.32--92.4 2.39--92.5 0.64--92.7Table 1:"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Relative speed of pruning models in a multi-pass cascade. Note that the 1-best models use richer features than the corresponding pruning models.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "First-order Second-order Third-order Setup Speed UAS Speed UAS Speed UAS BG B 1.90 90.7 0.67 92.0 0.05 92.1 V 6.17 90.5 5.30 91.6 1.99 91.9 DE B 1.40 89.2 0.48 90.3 0.02 90.8 V 4.72 89.0 3.54 90.1 1.44 90.8", "figure_data": "JAB 1.77 92.0 0.58 92.1 0.04 92.4 V 8.14 91.7 8.64 92.0 4.30 92.3PTB 0.89 90.1 0.28 91.2 0.01 91.7 V 3.98 90.0 3.45 90.9 1.45 91.5SWB 1.37 88.5 0.45 89.7 0.01 90.4 V 6.35 88.3 6.25 89.4 2.66 90.1ZHB 7.32 89.5 3.30 90.5 0.67 90.8 V 7.45 89.3 6.71 90.3 3.90 90.9ENB V 5.24 91.0 3.92 92.2 2.23 92.7 1.0 91.2 0.33 92.4 0.01 93.0"}], "formulas": [{"formula_id": "formula_0", "formula_text": "I 1 = {(h, m) : h \u2208 [n] 0 , m \u2208 [n]}", "formula_coordinates": [3.0, 350.73, 621.07, 151.74, 18.93]}, {"formula_id": "formula_1", "formula_text": "|I 1 | = O(n 2 ). (a) h m \u2190 I h r + C m r + 1 C (b) h e \u2190 C h m + I m e C", "formula_coordinates": [3.0, 313.2, 693.59, 62.4, 19.88]}, {"formula_id": "formula_2", "formula_text": "(c) V \u2190 0 e \u2190 C 0 e \u2212 1 + e e \u2212 1 C (d) 0 e \u2190 V \u2190 0 m + V \u2190 e m I (e) 0 e \u2190 V \u2192 0 e V \u2190 (f) 0 e \u2190 V \u2192 0 m + V \u2192 e m I (g) 0 e \u2190 C 0 e \u2212 1 + V \u2192 e \u2212 1 e C", "formula_coordinates": [4.0, 325.07, 66.23, 203.06, 256.69]}, {"formula_id": "formula_3", "formula_text": "I k,l = {(g, s) : g \u2208 [n] l+1 0 , s \u2208 [n] k+1 }", "formula_coordinates": [4.0, 340.63, 424.29, 171.94, 20.5]}, {"formula_id": "formula_4", "formula_text": "p k,l\u21921 (g, s) = {(g 1 , s j ) : j \u2208 [k + 1]} \u222a {(g j+1 , g j ) : j \u2208 [l]}", "formula_coordinates": [5.0, 93.99, 122.34, 182.82, 35.47]}, {"formula_id": "formula_5", "formula_text": "{(g, s) \u2208 I k,l : p k,l\u21921 (g, s) \u2282 F (I 1 )}", "formula_coordinates": [5.0, 101.42, 241.46, 167.97, 18.93]}, {"formula_id": "formula_6", "formula_text": "S = {(h, m) \u2208 I 1 : |h \u2212 m| \u2264 b}", "formula_coordinates": [5.0, 103.64, 361.53, 163.52, 18.93]}, {"formula_id": "formula_7", "formula_text": "I 0 = S \u222a {(d, m) : d \u2208 {\u2190, \u2192}, m \u2208 [n]} \u222a {(h, d) : h \u2208 [n] 0 , d \u2208 {\u2190, \u2192}}", "formula_coordinates": [5.0, 82.71, 477.66, 205.38, 35.47]}, {"formula_id": "formula_8", "formula_text": "p 1\u21920 (h, m) = \uf8f1 \uf8f2 \uf8f3 {(h, m)} if |h \u2212 m| \u2264 b {(\u2192, m), (h, \u2192)} if h < m {(\u2190, m), (h, \u2190)} if h > m", "formula_coordinates": [5.0, 313.2, 560.16, 226.79, 47.65]}, {"formula_id": "formula_9", "formula_text": "{(h, m) \u2208 I 1 : p 1\u21920 (h, m) \u2282 F (I 0 )}", "formula_coordinates": [5.0, 342.82, 640.5, 167.56, 18.93]}, {"formula_id": "formula_10", "formula_text": "F (I) = {i \u2208 I : f (i) = 1}", "formula_coordinates": [6.0, 124.95, 252.35, 120.9, 18.93]}, {"formula_id": "formula_11", "formula_text": "f (i; Y, w) = 1[ M (i; Y, w) \u2022 w < t \u03b1 (Y, w) ]", "formula_coordinates": [6.0, 87.25, 369.75, 196.29, 18.93]}, {"formula_id": "formula_12", "formula_text": "t \u03b1 (Y, w) = \u03b1 max y\u2208Y (y \u2022 w) + (1 \u2212 \u03b1) 1 |I| i\u2208I M (i; Y, w) \u2022 w", "formula_coordinates": [6.0, 84.01, 473.61, 202.49, 54.71]}, {"formula_id": "formula_13", "formula_text": "\u2206(y, Y, w) = 1[\u2203i \u2208 y, M (i; Y, w) \u2022 w < t \u03b1 (Y, w)]", "formula_coordinates": [6.0, 313.2, 133.23, 226.8, 18.93]}, {"formula_id": "formula_14", "formula_text": "\u2206(y, Y, w) \u2264 1[y \u2022 w < t \u03b1 (Y, w)] \u2264 [1 \u2212 y \u2022 w + t \u03b1 (Y, w)] +", "formula_coordinates": [6.0, 334.5, 216.95, 183.71, 35.47]}, {"formula_id": "formula_15", "formula_text": "min w \u03bb w 2 + 1 P P p=1 [1 \u2212 y (p) \u2022 w + t \u03b1 (Y (p) , w)] +", "formula_coordinates": [6.0, 316.04, 384.31, 220.61, 33.58]}, {"formula_id": "formula_16", "formula_text": "0 if y (p) \u2212 1 \u2265 t \u03b1 (Y, w) otherwise, H(w, p) = 2\u03bbw P \u2212 y (p) + \u03b1 arg max y\u2208Y (p) y \u2022 w + (1 \u2212 \u03b1) 1 |I (p) | i\u2208I (p) M (i; Y (p) , w)", "formula_coordinates": [6.0, 313.2, 479.52, 220.87, 84.85]}, {"formula_id": "formula_17", "formula_text": "w k = w k\u22121 \u2212 \u03b7 k H(w, p)", "formula_coordinates": [6.0, 370.61, 591.71, 111.98, 18.93]}], "doi": ""}