{"title": "Emotion-Cause Pair Extraction: A New Task to Emotion Analysis in Texts", "authors": "Rui Xia; Zixiang Ding", "pub_date": "", "abstract": "Emotion cause extraction (ECE), the task aimed at extracting the potential causes behind certain emotions in text, has gained much attention in recent years due to its wide applications. However, it suffers from two shortcomings: 1) the emotion must be annotated before cause extraction in ECE, which greatly limits its applications in real-world scenarios; 2) the way to first annotate emotion and then extract the cause ignores the fact that they are mutually indicative. In this work, we propose a new task: emotion-cause pair extraction (ECPE), which aims to extract the potential pairs of emotions and corresponding causes in a document. We propose a 2-step approach to address this new ECPE task, which first performs individual emotion extraction and cause extraction via multi-task learning, and then conduct emotion-cause pairing and filtering. The experimental results on a benchmark emotion cause corpus prove the feasibility of the ECPE task as well as the effectiveness of our approach.", "sections": [{"heading": "Introduction", "text": "Emotion cause extraction (ECE) aims at extracting potential causes that lead to emotion expressions in text. The ECE task was first proposed and defined as a word-level sequence labeling problem in . To solve the shortcoming of extracting causes at word level, Gui et al. (2016a) released a new corpus which has received much attention in the following study and become a benchmark dataset for ECE research. Figure 1 displays an example from this corpus, There are five clauses in a document. The emotion \"happy\" is contained in the fourth clause. We denote this clause as emotion clause, which refers to a clause that contains emotions. It has two corresponding causes: \"a policeman visited the old man with the lost money\" in the second clause, and \"told him that the thief was caught\" in the third clause. We denote them as cause clause, which refers to a clause that contains causes.\nThe ECE task was formalized as a clause-level binary classification problem in Gui et al. (2016a). The goal is to detect for each clause in a document, whether this clause is a cause given the annotation of emotion. This framework was followed by most of the recent studies in this field Gui et al., 2016a;Li et al., 2018;Xu et al., 2019;Yu et al., 2019).\nHowever, there are two shortcomings in the current ECE task. The first is that emotions must be annotated before cause extraction in the test set, which limits the applications of ECE in real-world scenarios. The second is that the way to first annotate the emotion and then extract the cause ignores the fact that emotions and causes are mutually indicative.\nIn this work, we propose a new task: emotioncause pair extraction (ECPE), which aims to extract all potential pairs of emotions and corresponding causes in a document. In Figure 1 we show the difference between the traditional ECE task and our new ECPE task. The goal of ECE is to extract the corresponding cause clause of the given emotion. In addition to a document as the input, ECE needs to provide annotated emotion at first before cause extraction. In contrast, the output of our ECPE task is a pair of emotion-cause, without the need of providing emotion annotation in advance. Take Figure 1 for example, given the annotation of emotion: \"happy\", the goal of ECE is to track the two corresponding cause clauses: \"a policeman visited the old man with the lost money\" and \"and told him that the thief was caught\". While in the ECPE task, the goal is to directly extract all pairs of emotion clause and cause clause, including (\"The old man was very happy\", \"a policeman visited the old man with the lost money\") and (\"The old man was very happy\", \"and told him that the thief was caught\"), without providing the Document Yesterday morning, a policeman visited the old man with the lost money, and told him that the thief was caught. The old man was very happy, and deposited the money in the bank. and told him that the thief was caught", "publication_ref": ["b10", "b10", "b10", "b15", "b20", "b23"], "figure_ref": ["fig_0", "fig_0", "fig_0"], "table_ref": []}, {"heading": "Emotion Cause Extraction (ECE)", "text": "happy a policeman visited the old man with the lost money happy", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Emotion-Cause Pair Extraction (ECPE)", "text": "(The old man was very happy, a policeman visited the old man with the lost money) (The old man was very happy, and told him that the thief was caught) emotion annotation \"happy\".\nTo address this new ECPE task, we propose a two-step framework.\nStep 1 converts the emotioncause pair extraction task to two individual subtasks (emotion extraction and cause extraction respectively) via two kinds of multi-task learning networks, with the goal to extract a set of emotion clauses and a set of cause clauses.\nStep 2 performs emotion-cause pairing and filtering. We combine all the elements of the two sets into pairs and finally train a filter to eliminate the pairs that do not contain a causal relationship.\nWe evaluated our approach based on a benchmark emotion cause dataset (Gui et al., 2016a) without using emotion annotations on the test data. We finally achieve the F1 score of 61.28% in emotion-cause pair extraction. The experimental results prove the feasibility of the ECPE task and the effectiveness of our approach.\nIn addition to the emotion-cause pair extraction evaluation, we also evaluate the performance on two individual tasks (emotion extraction and cause extraction). Without relying on the emotion annotations on the test set, our approach achieves comparable cause extraction performance to traditional ECE methods (slightly lower than the state-ofthe-art). In comparison with the traditional ECE methods that removes the emotion annotation dependence, our approach shows great advantages.\nThe main contributions of this work can be summarized as follows:\n\u2022 We propose a new task: emotion-cause pair extraction (ECPE). It solves the shortcomings of the traditional ECE task that depends on the annotation of emotion before extracting cause, and allows emotion cause analysis to be applied to real-world scenarios.\n\u2022 We propose a two-step framework to address the ECPE task, which first performs individual emotion extraction and cause extraction and then conduct emotion-cause pairing and filtering.\n\u2022 Based on a benchmark ECE corpus, we construct a corpus suitable for the ECPE task.\nThe experimental results prove the feasibility of the ECPE task as well as the effectiveness of our approach.\n2 Related Work  first presented the task of emotion cause extraction (ECE) and defined this task as extracting the word-level causes that lead to the given emotions in text. They constructed a smallscale Chinese emotion cause corpus in which the spans of both emotion and cause were annotated.\nBased on the same task settings, there were some other individual studies that conducted ECE research on their own corpus using rule based methods (Neviarouskaya and Aono, 2013;Li and Xu, 2014;Gao et al., 2015a,b;Yada et al., 2017) or machine learning methods (Ghazi et al., 2015;Song and Meng, 2015).  suggested that a clause may be the most appropriate unit to detect causes based on the analysis of the corpus in , and transformed the task from word-level to clause-level. They proposed a multi-label approach that detects multi-clause causes and captures the long-distance information. There were a lot of work based on this task setting. Russo et al. (2011) introduced a method based on the linguistic patterns and common sense knowledge for the identification of Italian sentences which contain a cause phrase. Gui et al. (2014) used 25 manual-ly complied rules as features, and chose machine learning models, such as SVM and CRFs, to detect causes. Gui et al. (2016a), Gui et al. (2016b) and  released a Chinese emotion cause dataset using SINA city news. This corpus has received much attention in the following study and has become a benchmark dataset for ECE research. Based on this corpus, several traditional machine learning methods (Gui et al., 2016a,b; and deep learning methods Li et al., 2018;Yu et al., 2019;Xu et al., 2019) were proposed.\nIn addition, Cheng et al. (2017) focused on cause detection for Chinese microblogs using a multiple-user structure. They formalized two cause detection tasks for microblogs (currentsubtweet-based cause detection and originalsubtweet-based cause detection) and introduced SVM and LSTM to deal with them. Chen et al. (2018b) presented a neural network-based joint approach for emotion classification and cause detection in order to capture mutual benefits across these two sub-tasks. Chen et al. (2018a) proposed a hierarchical Convolution Neural Network (Hier-CNN), which used clause-level encoder and subtweet-level encoder to incorporate the word context features and event-based features respectively.\nAll of the above work attempts to extract wordlevel or clause-level causes given the emotion annotations. While our work is different from them, we propose to extract both the emotion and the corresponding causes at the same time (i.e., emotion-cause pair extraction) and to investigate whether indicating causes can improve emotion extraction and vice versa. Since we believe that cause and emotion are not mutually independent.", "publication_ref": ["b10", "b17", "b12", "b22", "b7", "b19", "b18", "b12", "b10", "b11", "b15", "b23", "b20", "b4", "b2", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Task", "text": "First of all, we give the definition of our emotioncause pair extraction (ECPE) task. Given a document consisting of multiple clauses d = [c 1 , c 2 , ..., c |d| ], the goal of ECPE is to extract a set of emotion-cause pairs in d:\nP = {\u2022 \u2022 \u2022 , (c e , c c ), \u2022 \u2022 \u2022},(1)\nwhere c e is an emotion clause and c c is the corresponding cause clause In traditional emotion cause extraction task, the goal is to extract c c given the annotation of c e : c e \u2192 c c . In comparison, the ECPE task is new and more difficult to address, because the annotation of emotion c e is not provided before extraction.\nNote that similar as the traditional ECE task, the ECPE task is also defined at the clause level, due to the difficulty describing emotion causes at the word/phrase level. It means that the \"emotion\" and \"cause\" used in this paper refer to \"emotion clause\" and \"cause clause\" respectively.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Approach", "text": "In this work, we propose a two-step approach to address this new ECPE task:\n\u2022 Step 1 (Individual Emotion and Cause Extraction). We first convert the emotion-cause pair extraction task to two individual subtasks (emotion extraction and cause extraction respectively). Two kinds of multi-task learning networks are proposed to model the two sub-tasks in a unified framework, with the goal to extract a set of emotion clauses\nE = {c e 1 , \u2022 \u2022 \u2022 , c e m } and a set of cause clauses C = {c c 1 , \u2022 \u2022 \u2022 , c c n } for each document.\n\u2022\nStep 2 (Emotion-Cause Pairing and Filtering). We then pair the emotion set E and the cause set C by applying a Cartesian product to them. This yields a set of candidate emotion-cause pairs. We finally train a filter to eliminate the pairs that do not contain a causal relationship between emotion and cause.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Step 1: Individual Emotion and Cause Extraction", "text": "The goal of Step 1 is to extract a set of emotion clauses and a set of cause clauses for each document, respectively. To this end, we propose two kinds of multi-task learning networks, (i.e., Independent Multi-task Learning and Interactive Multi-task Learning). The latter is an enhanced version that further captures the correlation between emotion and cause on the basis of the former.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Independent Multi-task Learning", "text": "In our task, a document contains multiple clauses: d = [c 1 , c 2 , ..., c |d| )], and each c i also contains multiple words c i = [w i,1 , w i,2 , ..., w i,|c i | ]. To capture such a \"word-clause-document\" structure, we employ a Hierarchical Bi-LSTM network which contains two layers, as shown in Figure 2. The lower layer consists of a set of word-level Bi-LSTM modules, each of which corresponds to one clause, and accumulate the context information for each word of the clause. The hidden state of the jth word in the ith clause h i,j is obtained based on a bi-directional LSTM. Attention mechanism is then adopt to get a clause representation s i .\nHere we omit the details of Bi-LSTM and attention for limited space, readers can refer to Graves et al. (2013) and Bahdanau et al. (2014).\nThe upper layer consists of two components: one for emotion extraction and another for cause extraction. Each component is a clause-level Bi-LSTM which receives the independent clause representations [s 1 , s 2 , ..., s |d| ] obtained at the lower layer as inputs. The hidden states of two component Bi-LSTM, r e i and r c i , can be viewed as the context-aware representation of clause c i , and finally feed to the softmax layer for emotion prediction and cause predication:\ny e i = softmax(W e r e i + b e ),(2)\ny c i = softmax(W c r c i + b c ),(3)\nwhere the superscript e and c denotes emotion and cause, respectively.\nThe loss of the model is a weighted sum of two components:\nL p = \u03bbL e + (1 \u2212 \u03bb)L c ,(4)\nwhere L e and L c are the cross-entropy error of emotion predication and cause predication respectively, and \u03bb is a tradeoff parameter.", "publication_ref": ["b8", "b0"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Interactive Multi-task Learning", "text": "Till now, two component Bi-LSTM at the upper layer are independent to each other. However, as we have mentioned, the two sub-tasks (emotion extraction and cause extraction) are not mutually independent. On the one hand, providing emotions can help better discover the causes; on the other hand, knowing causes may also help more accurately extract emotions. Motivated by this, we furthermore propose an interactive multi-task learning network, as an enhanced version of the former one, to capture the correlation between emotion and cause. The structure is shown in Figure 3. It should be noted that the method using emotion extraction to improve cause extraction is called Inter-EC. In addition, we can also use cause extraction to enhance emotion extraction, and call this method Inter-CE. Since Inter-EC and Inter-CE are similar in structure, we only introduce Inter-EC (illustrated in Figure 3 (a) ) instead of both.\nCompared with Independent Multi-task Learning, the lower layer of Inter-EC is unchanged, and the upper layer consists of two components, which are used to make predictions for emotion extraction task and cause extraction task in an interactive manner. Each component is a clause-level Bi-LSTM followed by a softmax layer.\nThe first component takes the independent clause representations [s 1 , s 2 , ..., s |d| ] obtained at the lower layer as inputs for emotion extraction. The hidden state of clause-level Bi-LSTM r e i is used as feature to predict the distribution of the i-th clause\u0177 e i . Then we embed the predicted label of the i-th clause as a vector Y e i , which is used for the next component.\nAnother\ncomponent takes (s 1 \u2295 Y e 1 , s 2 \u2295 Y e 2 , ..., s |d| \u2295 Y e |d| )\nas inputs for cause extraction, where \u2295 represents the concatenation operation. The hidden state of clause-level Bi-LSTM r c i is used as feature to predict the distribution of the i-th clause\u0177 c i . The loss of the model is a weighted sum of two components, which is the same as Equation 4. cause pairs with causal relationship. Firstly, we apply a Cartesian product to E and C, and obtain the set of all possible pairs:\nP all = {\u2022 \u2022 \u2022 , (c e i , c c j ), \u2022 \u2022 \u2022},(5)\nSecondly, we represent each pair in P all by a feature vector composed of three kinds of features:\nx (c e i ,c c j ) = [s e i , s c j , v d ],(6)\nwhere s e and s c are the representations of the emotion clause and cause clause respectively, and v d represents the distances between the two clauses.\nA Logistic regression model is then trained to detect for each candidate pair (c e i , c c j ), whether c e i and c c j have a causal relationship:\ny (c e i ,c c j ) \u2190 \u03b4(\u03b8 T x (c e i ,c c j ) ),(7)\nwhere\u0177 (c e i ,c c j ) = 1 denotes that (c e i , c c j ) is a pair with causal relationship,\u0177 (c e i ,c c j ) = 0 denotes (c e i , c c j ) is a pair without causal relationship, and \u03b4(\u2022) is the Sigmoid function. We finally remove the pairs whose\u0177 (c e i ,c c j ) is 0 from P all , and get the final set of emotion-cause pairs.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dataset and Metrics", "text": "Since there was no directly available corpus for the ECPE task, we constructed a ECPE corpus based on the benchmark ECE corpus (Gui et al., 2016a), in which each document contains only one emotion and corresponding one or more causes. Documents having two or more emotions are split into several samples such that each contains only one emotion. In order to better meet the ECPE task settings, we merged the documents with the same text content into one document, and labeled each emotion, cause pair in this document. The proportion of documents with different number of emotion-cause pairs in the combined dataset are shown in Table 1.\nWe stochastically select 90% of the data for training and the remaining 10% for testing. In order to obtain statistically credible results, we repeat the experiments 20 times and report the average result. We use the precision, recall, and F1 score as the metrics for evaluation, which are calculated as follows:\nP = correct pairs proposed pairs ,(8)\nR = correct pairs annotated pairs ,  Table 2: Experimental results of all proposed models and variants using precision, recall, and F1-measure as metrics on the ECPE task as well as the two sub-tasks.\nF 1 = 2 \u00d7 P \u00d7 R P + R , (10\n)\nwhere proposed pairs denotes the number of emotion-cause pairs predicted by the model, annotated pairs denotes the total number of emotion-cause pairs that are labeled in the dataset and the correct pairs means the number of pairs that are both labeled and predicted as an emotioncause pair.\nIn addition, we also evaluate the performance of two sub-tasks: emotion extraction and cause extraction. The precision, recall and F1 score defined in Gui et al. (2016a) are used as the evaluation metrics.", "publication_ref": ["b10", "b10"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Experimental Settings", "text": "We use word vectors that were pre-trained on the corpora from Chinese Weibo 1 with word2vec (Mikolov et al., 2013) toolkit. The dimension of word embedding is set to 200. The number of hidden units in BiLSTM for all our models is set to 100. All weight matrices and bias are randomly initialized by a uniform distribution U(\u22120.01, 0.01).\nFor training details, we use the stochastic gradient descent (SGD) algorithm and Adam update rule with shuffled minibatch. Batch size and learning rate are set to 32 and 0.005, respectively. As for regularization, dropout is applied for word embeddings and the dropout rate is set to 0.8. Besides, we perform L2 constraints over the soft-max parameters and L2-norm regularization is set as 1e-5. 2", "publication_ref": ["b16"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation on the ECPE Task (1) Overall Performance", "text": "In Table 2, we report the experimental results of the following three proposed models on three tasks (emotion extraction, cause extraction and emotion-cause pair extraction).\n\u2022 Indep: Indep denotes the method proposed in section 4.1.1. In this method, emotion extraction and cause extraction are independently modeled by two Bi-LSTMs.\n\u2022 Inter-CE: Inter-CE denotes the method proposed in section 4.1.2, where the predictions of cause extraction are used to improve emotion extraction.\n\u2022 Inter-EC: Inter-EC denotes the method proposed in section 4.1.2, where the predictions of emotion extraction are used to enhance cause extraction.\nCompared with Indep, Inter-EC gets great improvements on the ECPE task as well as the two sub-tasks. Specifically, we find that the improvements are mainly in the recall rate on the cause extraction task, which finally lead to the great improvement in the recall rate of ECPE. This shows that the predictions of emotion extraction are helpful to cause extraction and proves the effectiveness of Inter-EC. In addition, the performance of emotion extraction also improved, which indicates that the supervision from cause extraction is also beneficial for emotion extraction.\nInter-CE also gets significant improvements on the ECPE task compared to Indep. Specifically, we find that the improvements are mainly in the precision score on the emotion extraction task, emotion extraction cause extraction emotion-cause pair extraction P R F 1 P R F 1 P R F 1 Inter-CE-Bound #0.9144 #0.8894 #0.9016 #1.0000 #1.0000 #1.0000 #0.8682 #0.8806 #0.8742 Inter-EC-Bound #1.0000 #1.0000 #1.0000 #0.7842 #0.7116 #0.7452 #0.7610 #0.7084 #0.7328  which finally lead to the significant improvement in the precision score of ECPE. This shows that the predictions of cause extraction are beneficial to emotion extraction and proves the effectiveness of Inter-CE. By comparing Inter-EC and Inter-CE, we find that the improvement of Inter-EC is mainly obtained on the cause extraction task, and the improvement of Inter-CE is mainly gained on the emotion extraction task. These results are consistent with our intuition that emotion and cause are mutually indicative. In addition, we find that the improvements of Inter-EC on the cause extraction task are much more than the improvement of Inter-CE on the emotion extraction task. We guess that it is because cause extraction is more difficult than emotion extraction, hence there is more room for extra improvement.\n(2) Upper-Bound of Emotion and Cause Interaction\nIn order to further explore the effect of sharing predictions of two sub-tasks, we designed upperbound experiments for Inter-CE and Inter-EC. The results are shown in Table 3.\n\u2022 Inter-CE-Bound: Inter-CE-Bound is a variant of Inter-CE that uses the label of cause extraction to help emotion extraction.\n\u2022 Inter-EC-Bound: Inter-EC-Bound is a variant of Inter-EC that uses the label of emotion extraction to help cause extraction.\nThe results of Inter-CE-Bound and Inter-EC-Bound are preceded by a \"#\", indicating that they cannot be compared fairly with other methods because they use annotations. Compared with Indep, the performance of Inter-EC-Bound on cause extraction and the performance of Inter-CE-Bound on emotion extraction both improve greatly. Moreover, the improvement of Inter-EC-Bound on the cause extraction task are much more than the improvement of Inter-CE-Bound on the emotion extraction task. We guess this is because the cause extraction task is more difficult than the emotion extraction task, and there is more room for improvement, which is consistent with previous section.\nBy comparing the results of Inter-EC-Bound and Inter-EC, we found that although Inter-EC performs better than Indep, it is far poorer than Inter-EC-Bound, which is caused by lots of errors in the predictions of emotion extraction. We can draw the same conclusion when comparing Inter-CE-Bound and Inter-CE.\nThese experimental results further illustrate that emotion and cause are mutually indicative, and indicate that if we can improve the performance of emotion extraction task, we can get better performance on cause extraction task and vice versa, which finally lead to the improvement on ECPE. But it should be noted it is only an upperbound experiment where the ground-truth of emotion/causes are used to predict each other.\n(3) Effect of Emotion-Cause Pair Filtering\nIn Table 4, we report the emotion-cause pair extraction performance with/without pair filtering. With/Without pair filtering indicates whether we adopt a pair filter after applying a Cartesian product in the second step. keep rate indicates the proportion of emotion-cause pairs in P all that are finally retained after pair filtering.\nAn obvious observation is that the F1 scores of  all models on the ECPE task are significantly improved by adopting the pair filter. These results demonstrate the effectiveness of the pair filter. Specifically, by introducing the pair filter, some of the candidate emotion-cause pairs in P all are filtered out, which may result in a decrease in the recall rate and an increase in precision. According to Table 4, the precision scores of almost all models are greatly improved (more than 7%), in contrast, the recall rates drop very little (less than 1%), which lead to the significant improvement in F1 score.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2", "tab_3", "tab_3"]}, {"heading": "Evaluation on the ECE task", "text": "In Table 5, we further examine our approach by comparing it with some existing approaches on the traditional ECE task. It should be noted that our Inter-EC model does not use the emotion annotations on the test data.\n\u2022 RB is a rule-based method with manually defined linguistic rules .\n\u2022 CB is a method based on common-sense knowledge (Russo et al., 2011).\n\u2022 RB+CB+ML (Machine learning method trained from rule-based features and common-sense knowledge base) uses rules and facts in a knowledge base as features and a traditional SVM classifier for classification .\n\u2022 Multi-kernel uses the multi-kernel method to identify the cause (Gui et al., 2016a).\n\u2022 Memnet denotes a deep memory network proposed by .\n\u2022 ConvMS-Memnet is a convolutional multiple-slot deep memory network proposed by .\n\u2022 CANN denotes a co-attention neural network model proposed in Li et al. (2018).\nIt can be seen that although our method does not use emotion annotations on the test data, it still achieves comparable results with most of the traditional methods for the ECE task. This indicates that our method can overcome the limitation that emotion annotations must be given at the testing phase in the traditional ECE task, but without reducing the cause extraction performance.\nIn order to compare with the traditional methods for the ECE task under the same experimental settings, we furthermore implemented a simplification of CANN (CANN-E), which removes the dependency of emotion annotation in the test data.\nIt is clear that by removing the emotion annotations, the F1 score of CANN drops dramatically (about 34.69%). In contrast, our method does not need the emotion annotations and achieve 65.07% in F1 measure, which significantly outperforms the CANN-E model by 27.1%.", "publication_ref": ["b18", "b10", "b15"], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Conclusions and Future Work", "text": "In this paper, we propose a new task: emotioncause pair extraction, which aims to extract potential pairs of emotions and corresponding causes in text. To deal with this task, we propose a two-step method, in which we first extract both emotions and causes respectively by multi-task learning, then combine them into pairs by applying Cartesian product, and finally employ a filter to eliminate the false emotion-cause pairs. Based on a benchmark ECE corpus, we construct a corpus suitable for the ECPE task. The experimental results prove the effectiveness of our method.\nThe two-step strategy may not be a perfect solution to solve the ECPE problem. On the one hand, its goal is not direct. On the other hand, the mistakes made in the first step will affect the results of the second step. In the future work, we will try to build a one-step model that directly extract the emotion-cause pairs in an end-to-end fashion.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "The work was supported by the Natural Science Foundation of China (No. 61672288), and the Natural Science Foundation of Jiangsu Province for Excellent Young Scholars (No. BK20160085). Rui Xia and Zixiang Ding contributed equally to this paper.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Neural machine translation by jointly learning to align and translate", "journal": "", "year": "2014", "authors": "Dzmitry Bahdanau; Kyunghyun Cho; Yoshua Bengio"}, {"ref_id": "b1", "title": "Hierarchical convolution neural network for emotion cause detection on microblogs", "journal": "", "year": "2018", "authors": "Ying Chen; Wenjun Hou; Xiyao Cheng"}, {"ref_id": "b2", "title": "Joint learning for emotion classification and emotion cause detection", "journal": "", "year": "2018", "authors": "Ying Chen; Wenjun Hou; Xiyao Cheng; Shoushan Li"}, {"ref_id": "b3", "title": "Emotion cause detection with linguistic constructions", "journal": "", "year": "2010", "authors": "Ying Chen; Sophia Yat Mei Lee; Shoushan Li; Chu-Ren Huang"}, {"ref_id": "b4", "title": "An emotion cause corpus for chinese microblogs with multipleuser structures", "journal": "ACM Transactions on Asian and Low-Resource Language Information Processing", "year": "2017", "authors": "Xiyao Cheng; Ying Chen; Bixiao Cheng; Shoushan Li; Guodong Zhou"}, {"ref_id": "b5", "title": "Emotion cause detection for chinese micro-blogs based on ecocc model", "journal": "", "year": "2015", "authors": "Kai Gao; Hua Xu; Jiushuo Wang"}, {"ref_id": "b6", "title": "A rulebased approach to emotion cause detection for chinese micro-blogs. Expert Systems with Applications", "journal": "", "year": "2015", "authors": "Kai Gao; Hua Xu; Jiushuo Wang"}, {"ref_id": "b7", "title": "Detecting emotion stimuli in emotion-bearing sentences", "journal": "", "year": "2015", "authors": "Diman Ghazi; Diana Inkpen; Stan Szpakowicz"}, {"ref_id": "b8", "title": "Speech recognition with deep recurrent neural networks", "journal": "IEEE", "year": "2013", "authors": "Alex Graves; Mohamed Abdel-Rahman; Geoffrey Hinton"}, {"ref_id": "b9", "title": "A question answering approach to emotion cause extraction", "journal": "", "year": "2017", "authors": "Lin Gui; Jiannan Hu; Yulan He; Ruifeng Xu; Qin Lu; Jiachen Du"}, {"ref_id": "b10", "title": "Event-driven emotion cause extraction with corpus construction", "journal": "", "year": "2016", "authors": "Lin Gui; Dongyin Wu; Ruifeng Xu; Qin Lu; Yu Zhou"}, {"ref_id": "b11", "title": "Emotion cause extraction, a challenging task with corpus construction", "journal": "", "year": "2016", "authors": "Lin Gui; Ruifeng Xu; Qin Lu; Dongyin Wu; Yu Zhou"}, {"ref_id": "b12", "title": "Emotion cause detection with linguistic construction in chinese weibo text", "journal": "", "year": "2014", "authors": "Lin Gui; Li Yuan; Ruifeng Xu; Bin Liu; Qin Lu; Yu Zhou"}, {"ref_id": "b13", "title": "A text-driven rule-based system for emotion cause detection", "journal": "", "year": "2010", "authors": "Sophia Yat Mei Lee; Ying Chen; Chu-Ren Huang"}, {"ref_id": "b14", "title": "Text-based emotion classification using emotion cause extraction. Expert Systems with Applications", "journal": "", "year": "2014", "authors": "Weiyuan Li; Hua Xu"}, {"ref_id": "b15", "title": "A co-attention neural network model for emotion cause analysis with emotional context awareness", "journal": "", "year": "2018", "authors": "Xiangju Li; Kaisong Song; Shi Feng; Daling Wang; Yifei Zhang"}, {"ref_id": "b16", "title": "Distributed representations of words and phrases and their compositionality", "journal": "", "year": "2013", "authors": "Tomas Mikolov; Ilya Sutskever; Kai Chen; Greg S Corrado; Jeff Dean"}, {"ref_id": "b17", "title": "Extracting causes of emotions from text", "journal": "", "year": "2013", "authors": "Alena Neviarouskaya; Masaki Aono"}, {"ref_id": "b18", "title": "Emocause: an easy-adaptable approach to emotion cause contexts", "journal": "", "year": "2011", "authors": "Irene Russo; Tommaso Caselli; Francesco Rubino; Ester Boldrini; Patricio Mart\u00ednez-Barco"}, {"ref_id": "b19", "title": "Detecting concept-level emotion cause in microblogging", "journal": "", "year": "2015", "authors": "Shuangyong Song; Yao Meng"}, {"ref_id": "b20", "title": "Extracting emotion causes using learning to rank methods from an information retrieval perspective", "journal": "IEEE Access", "year": "2019", "authors": "Bo Xu; Hongfei Lin; Yuan Lin; Yufeng Diao; Liang Yang; Kan Xu"}, {"ref_id": "b21", "title": "An ensemble approach for emotion cause detection with event extraction and multikernel svms", "journal": "Tsinghua Science and Technology", "year": "2017", "authors": "Ruifeng Xu; Jiannan Hu; Qin Lu; Dongyin Wu; Lin Gui"}, {"ref_id": "b22", "title": "A bootstrap method for automatic rule acquisition on emotion cause extraction", "journal": "", "year": "2017", "authors": "Shuntaro Yada; Kazushi Ikeda; Keiichiro Hoashi; Kyo Kageura"}, {"ref_id": "b23", "title": "Multiple level hierarchical network-based clause selection for emotion cause extraction", "journal": "IEEE Access", "year": "2019", "authors": "Xinyi Yu; Wenge Rong; Zhuo Zhang; Yuanxin Ouyang; Zhang Xiong"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: An example showing the difference between the ECE task and the ECPE task.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: The Model for Independent Multi-task Learning (Indep).", "figure_data": ""}, {"figure_label": "23", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "4. 2 Figure 3 :23Figure 3: Two Models for Interactive Multi-task Learning: (a) Inter-EC, which uses emotion extraction to improve cause extraction (b) Inter-CE, which uses cause extraction to enhance emotion extraction.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "The proportion of documents with different number of emotion-cause pairs in the merged dataset.", "figure_data": "emotion extractioncause extractionemotion-cause pair extractionPRF 1PRF 1PRF 1Indep0.8375 0.8071 0.8210 0.6902 0.5673 0.6205 0.6832 0.50820.5818Inter-CE 0.8494 0.8122 0.8300 0.6809 0.5634 0.6151 0.6902 0.51350.5901Inter-EC 0.8364 0.8107 0.8230 0.7041 0.6083 0.6507 0.6721 0.57050.6128"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Results of upperbound experiments for Inter-CE and Inter-EC.", "figure_data": "without emotion-cause pair filteringwith emotion-cause pair filteringPRF 1PRF 1keep rateIndep0.58940.51140.54510.68320.50820.58180.8507Inter-CE0.58830.51920.55000.69020.51350.59010.8412Inter-EC0.60190.57750.58420.67210.57050.61280.8889Inter-CE-Bound #0.8116 #0.8880#0.8477#0.8682 #0.8806 #0.87420.9271Inter-EC-Bound #0.6941 #0.7118#0.7018#0.7610 #0.7084 #0.73280.9088"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Experimental results of all proposed models and variants using precision, recall, and F1-measure as metrics on the ECPE task with or without the pair filter.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Experimental results of some existing ECE approaches and our model on the ECE task.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "P = {\u2022 \u2022 \u2022 , (c e , c c ), \u2022 \u2022 \u2022},(1)", "formula_coordinates": [3.0, 127.55, 674.6, 162.71, 12.31]}, {"formula_id": "formula_1", "formula_text": "E = {c e 1 , \u2022 \u2022 \u2022 , c e m } and a set of cause clauses C = {c c 1 , \u2022 \u2022 \u2022 , c c n } for each document.", "formula_coordinates": [3.0, 329.09, 351.73, 196.45, 27.27]}, {"formula_id": "formula_2", "formula_text": "y e i = softmax(W e r e i + b e ),(2)", "formula_coordinates": [4.0, 118.06, 591.29, 312.32, 14.19]}, {"formula_id": "formula_3", "formula_text": "y c i = softmax(W c r c i + b c ),(3)", "formula_coordinates": [4.0, 118.58, 615.96, 171.68, 14.19]}, {"formula_id": "formula_4", "formula_text": "L p = \u03bbL e + (1 \u2212 \u03bb)L c ,(4)", "formula_coordinates": [4.0, 126.96, 701.33, 163.31, 12.3]}, {"formula_id": "formula_5", "formula_text": "component takes (s 1 \u2295 Y e 1 , s 2 \u2295 Y e 2 , ..., s |d| \u2295 Y e |d| )", "formula_coordinates": [4.0, 307.28, 559.44, 218.27, 28.11]}, {"formula_id": "formula_6", "formula_text": "P all = {\u2022 \u2022 \u2022 , (c e i , c c j ), \u2022 \u2022 \u2022},(5)", "formula_coordinates": [5.0, 123.08, 426.22, 167.18, 14.19]}, {"formula_id": "formula_7", "formula_text": "x (c e i ,c c j ) = [s e i , s c j , v d ],(6)", "formula_coordinates": [5.0, 132.72, 506.75, 157.55, 16.14]}, {"formula_id": "formula_8", "formula_text": "y (c e i ,c c j ) \u2190 \u03b4(\u03b8 T x (c e i ,c c j ) ),(7)", "formula_coordinates": [5.0, 126.65, 652.59, 163.62, 16.14]}, {"formula_id": "formula_9", "formula_text": "P = correct pairs proposed pairs ,(8)", "formula_coordinates": [5.0, 359.13, 703.58, 166.42, 24.43]}, {"formula_id": "formula_11", "formula_text": "F 1 = 2 \u00d7 P \u00d7 R P + R , (10", "formula_coordinates": [6.0, 138.75, 252.9, 146.98, 24.43]}, {"formula_id": "formula_12", "formula_text": ")", "formula_coordinates": [6.0, 285.72, 260.63, 4.54, 9.46]}], "doi": ""}