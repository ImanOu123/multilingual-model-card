{"title": "EXPRESSIVENESS AND APPROXIMATION PROPERTIES OF GRAPH NEURAL NETWORKS", "authors": "Floris Geerts; Juan L Reutter", "pub_date": "2022-04-10", "abstract": "Characterizing the separation power of graph neural networks (GNNs) provides an understanding of their limitations for graph learning tasks. Results regarding separation power are, however, usually geared at specific GNN architectures, and tools for understanding arbitrary GNN architectures are generally lacking. We provide an elegant way to easily obtain bounds on the separation power of GNNs in terms of the Weisfeiler-Leman (WL) tests, which have become the yardstick to measure the separation power of GNNs. The crux is to view GNNs as expressions in a procedural tensor language describing the computations in the layers of the GNNs. Then, by a simple analysis of the obtained expressions, in terms of the number of indexes and the nesting depth of summations, bounds on the separation power in terms of the WL-tests readily follow. We use tensor language to define Higher-Order Message-Passing Neural Networks (or k-MPNNs), a natural extension of MPNNs. Furthermore, the tensor language point of view allows for the derivation of universality results for classes of GNNs in a natural way. Our approach provides a toolbox with which GNN architecture designers can analyze the separation power of their GNNs, without needing to know the intricacies of the WL-tests. We also provide insights in what is needed to boost the separation power of GNNs.Connecting GNNs and tensor languages allows us to use our analysis of tensor languages to understand the separation and approximation power of GNNs. The number of indices and summation depth needed to represent the layers in GNNs determine their separation power in terms of color refinement and Weisfeiler-Leman tests. The framework of k-MPNNs provides a handy toolbox to understand existing and new GNN architectures, and we demonstrate this by recovering several results about the power of GNNs presented recently in the literature, as well as proving new results.", "sections": [{"heading": "INTRODUCTION", "text": "Graph Neural Networks (GNNs) (Merkwirth & Lengauer, 2005;Scarselli et al., 2009) cover many popular deep learning methods for graph learning tasks (see Hamilton (2020) for a recent overview). These methods typically compute vector embeddings of vertices or graphs by relying on the underlying adjacency information. Invariance (for graph embeddings) and equivariance (for vertex embeddings) of GNNs ensure that these methods are oblivious to the precise representation of the graphs.\nSeparation power. Our primary focus is on the separation power of GNN architectures, i.e., on their ability to separate vertices or graphs by means of the computed embeddings. It has become standard to characterize GNN architectures in terms of the separation power of graph algorithms such as color refinement (CR) and k-dimensional Weisfeiler-Leman tests (k-WL), as initiated in Xu et al. (2019) and Morris et al. (2019). Unfortunately, understanding the separation power of any given GNN architecture requires complex proofs, geared at the specifics of the architecture. We provide a tensor language-based technique to analyze the separation power of general GNNs.\nTensor languages. Matrix query languages (Brijder et al., 2019;Geerts et al., 2021b) are defined to assess the expressive power of linear algebra. Balcilar et al. (2021a) observe that, by casting various GNNs into the MATLANG (Brijder et al., 2019) matrix query language, one can use existing separation results (Geerts, 2021) to obtain upper bounds on the separation power of GNNs in terms of 1-WL and 2-WL. In this paper, we considerably extend this approach by defining, and studying, a new general-purpose tensor language specifically designed for modeling GNNs. As in Balcilar et al. (2021a), our focus on tensor languages allows us to obtain new insights about GNN architectures. First, since tensor languages can only define invariant and equivariant graph functions, any GNN that can be cast in our tensor language inherits these desired properties. More importantly, the separation power of our tensor language is as closely related to CR and k-WL as GNNs are. Loosely speaking, if tensor language expressions use k + 1 indices, then their separation power is bounded by k-WL. Furthermore, if the maximum nesting of summations in the expression is t, then t rounds of k-WL are needed to obtain an upper bound on the separation power. A similar connection is obtained for CR and a fragment of tensor language that we call \"guarded\" tensor language.\nWe thus reduce problem of assessing the separation power of any specific GNN architecture to the problem of specifying it in our tensor language, analyzing the number of indices used and counting their summation depth. This is usually much easier than dealing with intricacies of CR and k-WL, as casting GNNs in our tensor language is often as simple as writing down their layer-based definition. We believe that this provides a nice toolbox for GNN designers to assess the separation power of their architecture. We use this toolbox to recover known results about the separation power of specific GNN architectures such as GINs (Xu et al., 2019), GCNs (Kipf & Welling, 2017), Folklore GNNs (Maron et al., 2019b), k-GNNs (Morris et al., 2019, and several others. We also derive new results:\nwe answer an open problem posed by Maron et al. (2019a) by showing that the separation power of Invariant Graph Networks (k-IGNs), introduced by Maron et al. (2019b), is bounded by (k \u2212 1)-WL.\nIn addition, we revisit the analysis by Balcilar et al. (2021b) of ChebNet (Defferrard et al., 2016), and show that CayleyNet (Levie et al., 2019) is bounded by 2-WL.\nWhen writing down GNNs in our tensor language, the less indices needed, the stronger the bounds in terms of k-WL we obtain. After all, (k \u2212 1)-WL is known to be strictly less separating than k-WL (Otto, 2017). Thus, it is important to minimize the number of indices used in tensor language expressions. We connect this number to the notion of treewidth: expressions of treewidth k can be translated into expressions using k + 1 indices. This corresponds to optimizing expressions, as done in many areas in machine learning, by reordering the summations (a.k.a. variable elimination).\nApproximation and universality. We also consider the ability of GNNs to approximate general invariant or equivariant graph functions. Once more, instead of focusing on specific architectures, we use our tensor languages to obtain general approximation results, which naturally translate to universality results for GNNs. We show: (k + 1)-index tensor language expressions suffice to approximate any (invariant/equivariant) graph function whose separating power is bounded by k-WL, and we can further refine this by comparing the number of rounds in k-WL with the summation depth of the expressions. These results provide a finer picture than the one obtained by Azizian & Lelarge (2021). Furthermore, focusing on \"guarded\" tensor expressions yields a similar universality result for CR, a result that, to our knowledge, was not known before. We also provide the link between approximation results for tensor expressions and GNNs, enabling us to transfer our insights into universality properties of GNNs. As an example, we show that k-IGNs can approximate any graph function that is less separating than (k \u2212 1)-WL. This case was left open in Azizian & Lelarge (2021).\nIn summary, we draw new and interesting connections between tensor languages, GNN architectures and classic graph algorithms. We provide a general recipe to bound the separation power of GNNs, optimize them, and understand their approximation power. We show the usefulness of our method by recovering several recent results, as well as new results, some of them left open in previous work. Related work. Separation power has been studied for specific classes of GNNs (Morris et al., 2019;Xu et al., 2019;Maron et al., 2019b;Chen et al., 2019;Morris et al., 2020;Azizian & Lelarge, 2021). A first general result concerns the bounds in terms of CR and 1-WL of Message-Passing Neural Networks (Gilmer et al., 2017;Morris et al., 2019;Xu et al., 2019). Balcilar et al. (2021a) use the MATLANG matrix query language to obtain upper bounds on the separation power of various GNNs. MATLANG can only be used to obtain bounds up to 2-WL and is limited to matrices. Our tensor language is more general and flexible and allows for reasoning over the number of indices, treewidth, and summation depth of expressions. These are all crucial for our main results. The tensor language introduced resembles sum-MATLANG (Geerts et al., 2021b), but with the added ability to represent tensors. Neither separation power nor guarded fragments were considered in Geerts et al. (2021b). See Section A in the supplementary material for more details. For universality, Azizian & Lelarge (2021) is closest in spirit. Our approach provides an elegant way to recover and extend their results. Azizian & Lelarge (2021) describe how their work (and hence also ours) encompasses previous works (Keriven & Peyr\u00e9, 2019;Maron et al., 2019c;Chen et al., 2019). Our results use connections between k-WL and logics (Immerman & Lander, 1990;Cai et al., 1992), and CR and guarded logics (Barcel\u00f3 et al., 2020). The optimization of algebraic computations and the use of treewidth relates to the approaches by Aji & McEliece (2000) and Abo Khamis et al. (2016).", "publication_ref": ["b3", "b3", "b4", "b2", "b2", "b2", "b3", "b2", "b2", "b1", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "BACKGROUND", "text": "We denote sets by {} and multisets by {{}}. For n \u2208 N, n > 0, [n] := {1, . . . , n}. Vectors are denoted by v, w, . . ., matrices by A, B, . . ., and tensors by S, T, . . .. Furthermore, v i is the i-th entry of vector v, A ij is the (i, j)-th entry of matrix A and S i denotes the i = (i 1 , . . . , i k )-th entry of a tensor S. If certain dimensions are unspecified, then this is denoted by a \":\". For example, A i: and A :j denote the i-th row and j-th column of matrix A, respectively. Similarly for slices of tensors.\nWe consider undirected simple graphs G = (V G , E G , col G ) equipped with a vertex-labelling col G : V G \u2192 R \u2113 . We assume that graphs have size n, so V G consists of n vertices and we often identify\nV G with [n]. For a vertex v \u2208 V G , N G (v) := {u \u2208 V G | vu \u2208 E G }.\nWe let G be the set of all graphs of size n and let G s be the set of pairs (G, v) with G \u2208 G and v \u2208 V s G . Note that G = G 0 . The color refinement algorithm (CR) (Morgan, 1965) iteratively computes vertex labellings based on neighboring vertices, as follows. For a graph G and vertex v \u2208 V G , cr (0) (G, v) := col G (v). Then, for t \u2265 0, cr (t+1) (G, v) := (cr (t) (G, v), {{cr (t) (G, u) | u \u2208 N G (v)}}). We collect all vertex labels to obtain a label for the entire graph by defining gcr (t) \n(G) := {{cr (t) (G, v) | v \u2208 V G }}.\nThe k-dimensional Weisfeiler-Leman algorithm (k-WL) (Cai et al., 1992) iteratively computes labellings of k-tuples of vertices. For a k-tuple v, its atomic type in G, denoted by atp k (G, v), is a vector in R 2 ( k 2 ) +k\u2113 . The first k 2 entries are 0/1-values encoding the equality type of v, i.e., whether v i = v j for 1 \u2264 i < j \u2264 k. The second k 2 entries are 0/1-values encoding adjacency information, i.e., whether v i v j \u2208 E G for 1 \u2264 i < j \u2264 k. The last k\u2113 real-valued entries correspond to col G (v i ) \u2208 R \u2113 for 1 \u2264 i \u2264 k. Initially, for a graph G and v \u2208 V k G , k-WL assigns the label wl (0) k (G, v) := atp k (G, v). For t \u2265 0, k-WL revises the label according to wl (t+1) k (G, v) := (wl (t) k (G, v), M ) with M := atp k+1 (G, vu), wl (t) k (G, v[u/1]), . . . , wl (t) k (G, v[u/k]) | u \u2208 V G , where v[u/i] := (v 1 , . . . , v i\u22121 , u, v i+1 , . . . , v k ). We use k-WL to assign labels to vertices and graphs by defining: vwl (t) k (G, v) := wl (t) k (G, (v, . . . , v)), for vertex-labellings, and gwl (t) k := {{wl\n(t) k (G, v) | v \u2208 V k G\n}}, for graph-labellings. We use cr (\u221e) , gcr (\u221e) , vwl (\u221e) k , and gwl (\u221e) k to denote the stable labellings produced by the corresponding algorithm over an arbitrary number of rounds. Our version of 1-WL differs from CR in that 1-WL also uses information from non-adjacent vertices; this distinction only matters for vertex embeddings (Grohe, 2021). We use the \"folklore\" k-WL of Cai et al. (1992), except Cai et al. use 1-WL to refer to CR. While equivalent to \"oblivious\" (k + 1)-WL (Grohe, 2021), used in some other works on GNNs, care is needed when comparing to our work.\nLet G be a graph with V G = [n] and let \u03c3 be a permutation of [n]. We denote by \u03c3 \u22c6 G the isomorphic copy of G obtained by applying the permutation \u03c3. Similarly, for v \u2208 V k G , \u03c3 \u22c6 v is the permuted version of v. Let F be some feature space. A function (G, v) for any permutation \u03c3. The functions cr (t) : G 1 \u2192 F and vwl (t) k : G 1 \u2192 F are equivariant, whereas gcr (t) : G 0 \u2192 F and gwl (t) k : G 0 \u2192 F are invariant, for any t \u2265 0 and k \u2265 1.\nf : G 0 \u2192 F is called invariant if f (G) = f (\u03c3 \u22c6 G) for any permutation \u03c0. More generally, f : G s \u2192 F is equivariant if f (\u03c3 \u22c6 G, \u03c3 \u22c6 v) = f", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "SPECIFYING GNNS", "text": "Many GNNs use linear algebra computations on vectors, matrices or tensors, interleaved with the application of activation functions or MLPs. To understand the separation power of GNNs, we introduce a specification language, TL, for tensor language, that allows us to specify any algebraic computation in a procedural way by explicitly stating how each entry is to be computed. We gauge the separation power of GNNs by specifying them as TL expressions, and syntactically analyzing the components of such TL expressions. This technique gives rise to Higher-Order Message-Passing Neural Networks (or k-MPNNs), a natural extension of MPNNs (Gilmer et al., 2017). For simplicity, we present TL using summation aggregation only but arbitrary aggregation functions on multisets of real values can be used as well (Section C.5 in the supplementary material).\nTo introduce TL, consider a typical layer in a GNN of the form F \u2032 = \u03c3(A\u2022F \u2022W ), where A \u2208 R n\u00d7n is an adjacency matrix, F \u2208 R n\u00d7\u2113 are vertex features such that F i: \u2208 R \u2113 is the feature vector of vertex i, \u03c3 is a non-linear activation function, and W \u2208 R \u2113\u00d7\u2113 is a weight matrix. By exposing the indices in the matrices and vectors we can equivalently write: for i \u2208 [n] and s \u2208 [\u2113]:\nF \u2032 is := \u03c3 j\u2208[n] A ij \u2022 t\u2208[\u2113] W ts \u2022 F jt .\nIn TL, we do not work with specific matrices or indices ranging over [n], but focus instead on expressions applicable to any matrix. We use index variables x 1 and x 2 instead of i and j, replace A ij with a placeholder E(x 1 , x 2 ) and F jt with placeholders P t (x 2 ), for t \u2208 [\u2113]. We then represent the above computation in TL by \u2113 expressions \u03c8 s (x 1 ), one for each feature column, as follows:\n\u03c8 s (x 1 ) = \u03c3 x2 E(x 1 , x 2 ) \u2022 t\u2208[\u2113] W ts \u2022 P t (x 2\n) . These are pure syntactical expressions. To give them a semantics, we assign to E a matrix A \u2208 R n\u00d7n , to P t column vectors F :t \u2208 R n\u00d71 , for t \u2208 [\u2113], and to x 1 an index i \u2208 [n]. By letting the variable x 2 under the summation range over 1, 2, . . . , n, the TL expression \u03c8 s (i) evaluates to F \u2032 is . As such, F \u2032 = \u03c3(A \u2022 F \u2022 W ) can be represented as a specific instance of the above TL expressions. Throughout the paper we reason about expressions in TL rather than specific instances thereof. Importantly, by showing that certain properties hold for expressions in TL, these properties are inherited by all of its instances. We use TL to enable a theoretical analysis of the separating power of GNNs; It is not intended as a practical programming language for GNNs.\nSyntax. We first give the syntax of TL expressions. We have a binary predicate E, to represent adjacency matrices, and unary vertex predicates P s , s \u2208 [\u2113], to represent column vectors encoding the \u2113-dimensional vertex labels. In addition, we have a (possibly infinite) set \u2126 of functions, such as activation functions or MLPs. Then, TL(\u2126) expressions are defined by the following grammar:\n\u03d5 := 1 x op y | E(x, y) | P s (x) | \u03d5 \u2022 \u03d5 | \u03d5 + \u03d5 | a \u2022 \u03d5 | f (\u03d5, . . . , \u03d5) | x \u03d5\nwhere op \u2208 {=, =}, x, y are index variables that specify entries in tensors, s \u2208 [\u2113], a \u2208 R, and f \u2208 \u2126. Summation aggregation is captured by x \u03d5. 1 We sometimes make explicit which functions are used in expressions in TL(\u2126) by writing TL(f 1 , f 2 , . . .) for f 1 , f 2 , . . . in \u2126. For example, the expressions \u03c8 s (x 1 ) described earlier are in TL(\u03c3).\nThe set of free index variables of an expression \u03d5, denoted by free(\u03d5), determines the order of the tensor represented by \u03d5. It is defined inductively: free(1 x op y ) = free(E(x, y)) := {x, y}, free(P s (x)) = {x}, free(\u03d5 1 \u2022 \u03d5 2 ) = free(\u03d5 1 + \u03d5 2 ) := free(\u03d5 1 ) \u222a free(\u03d5 2 ), free(a \u2022 \u03d5 1 ) := free(\u03d5 1 ), free(f (\u03d5 1 , . . . , \u03d5 p )) := \u222a i\u2208[p] free(\u03d5 i ), and free( x \u03d5 1 ) := free(\u03d5 1 ) \\ {x}. We sometimes explicitly write the free indices. In our example expressions \u03c8 s (x 1 ), x 1 is the free index variable.\nAn important class of expressions are those that only use index variables {x 1 , . . . , x k }. We denote by TL k (\u2126) the k-index variable fragment of TL(\u2126). The expressions \u03c8 s (x 1 ) are in TL 2 (\u03c3).\nSemantics. We next define the semantics of expressions in TL(\u2126). Let G = (V G , E G , col G ) be a vertex-labelled graph. We start by defining the interpretation [[\u2022, \u03bd]] G of the predicates E, P s and the (dis)equality predicates, relative to G and a valuation \u03bd assigning a vertex to each index variable:\n[[E(x, y), \u03bd]] G := if \u03bd(x)\u03bd(y) \u2208 E G then 1 else 0 [[P s (x), \u03bd]] G := col G (\u03bd(x)) s \u2208 R [[1 x op y , \u03bd]] G := if \u03bd(x) op \u03bd(y) then 1 else 0.\nIn other words, E is interpreted as the adjacency matrix of G and the P s 's interpret the vertexlabelling col G . Furthermore, we lift interpretations to arbitrary expressions in TL(\u2126), as follows:\n[[\u03d5 1 \u2022 \u03d5 2 , \u03bd]] G := [[\u03d5 1 , \u03bd]] G \u2022 [[\u03d5 2 , \u03bd]] G [[\u03d5 1 +\u03d5 2 , \u03bd]] G := [[\u03d5 1 , \u03bd]] G + [[\u03d5 2 , \u03bd]] G [[ x \u03d5 1 , \u03bd]] G := v\u2208VG [[\u03d5 1 , \u03bd[x \u2192 v]]] G [[a \u2022 \u03d5 1 , \u03bd]] G := a \u2022 [[\u03d5 1 , \u03bd]] G [[f (\u03d5 1 , . . . , \u03d5 p ), \u03bd]] G :=f ([[\u03d5 1 , \u03bd]] G , . . . , [[\u03d5 p , \u03bd]] G )\nwhere, \u03bd[x \u2192 v] is the valuation \u03bd but which now maps the index x to the vertex v \u2208 V G . For simplicity, we identify valuations with their images. For example, [[\u03d5(x)\n, v]] G denotes [[\u03d5(x), x \u2192 v]] G . To illustrate the semantics, for each v \u2208 V G , our example expressions satisfy [[\u03c8 s , v]] G = F \u2032 vs for F \u2032 = \u03c3(A \u2022 F \u2022 W )\nwhen A is the adjacency matrix of G and F represents the vertex labels.\nk-MPNNs. Consider a function f : G s \u2192 R \u2113 : (G, v) \u2192 f (G, v) \u2208 R \u2113 for some \u2113 \u2208 N.\nWe say that the function f can be represented in TL(\u2126) if there exists \u2113 expressions \u03d5 1 (x 1 , . . . , x s ), . . . , \u03d5 \u2113 (x 1 , . . . , x s ) in TL(\u2126) such that for each graph G and each s-tuple\nv \u2208 V s G : f (G, v) = [[\u03d5 1 , v]] G , . . . , [[\u03d5 \u2113 , v]] G .\nOf particular interest are kth-order MPNNs (or k-MPNNs) which refers to the class of functions that can be represented in TL k+1 (\u2126). We can regard GNNs as functions f : G s \u2192 R \u2113 . Hence, a GNN is a k-MPNN if its corresponding functions are k-MPNNs. For example, we can interpret\nF \u2032 = \u03c3(A \u2022 F \u2022 W ) as a function f : G 1 \u2192 R \u2113 such that f (G, v) := F \u2032 v:\n. We have seen that for each\ns \u2208 [\u2113], [[\u03c8 s , v]] G = F \u2032 vs with \u03c8 s \u2208 TL 2 (\u03c3). Hence, f (G, v) = ([[\u03c8 1 , v]] G , . . . , [[\u03c8 \u2113 , v]] G\n) and thus f belongs to 1-MPNNs and our example GNN is a 1-MPNN.\nTL represents equivariant or invariant functions. We make a simple observation which follows from the type of operators allowed in expressions in TL(\u2126). Proposition 3.1. Any function f :\nG s \u2192 R \u2113 represented in TL(\u2126) is equivariant (invariant if s = 0).\nAn immediate consequence is that when a GNN is a k-MPNN, it is automatically invariant or equivariant, depending on whether graph or vertex tuple embeddings are considered.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "SEPARATION POWER OF TENSOR LANGUAGES", "text": "Our first main results concern the characterization of the separation power of tensor languages in terms of the color refinement and k-dimensional Weisfeiler-Leman algorithms. We provide a finegrained characterization by taking the number of rounds of these algorithms into account. This will allow for measuring the separation power of classes of GNNs in terms of their number of layers.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "SEPARATION POWER", "text": "We define the separation power of graph functions in terms of an equivalence relation, based on the definition from Azizian & Lelarge (2021), hereby first focusing on their ability to separate vertices. 2 Definition 1. Let F be a set of functions f :\nG 1 \u2192 R \u2113 f . The equivalence relation \u03c1 1 (F ) is defined by F on G 1 as follows: (G, v), (H, w) \u2208 \u03c1 1 (F ) \u21d0\u21d2 \u2200f \u2208 F , f (G, v) = f (H, w).\nIn other words, when ((G, v), (H, w)) \u2208 \u03c1 1 (F ), no function in F can separate v in G from w in H. For example, we can view cr (t) and vwl (t) k as functions from G 1 to some R \u2113 . As such \u03c1 1 (cr (t) ) and \u03c1 1 (vwl (t) k ) measure the separation power of these algorithms. The following strict inclusions are known: , 2017;Grohe, 2021). It is also known that more rounds (t) increase the separation power of these algorithms (F\u00fcrer, 2001).\nfor all k \u2265 1, \u03c1 1 (vwl (t) k+1 ) \u2282 \u03c1 1 (vwl (t) k ) and \u03c1 1 (vwl (t) 1 ) \u2282 \u03c1 1 (cr (t) ) (Otto\nFor a fragment L of TL(\u2126) expressions, we define \u03c1 1 (L) as the equivalence relation associated with all functions f : G 1 \u2192 R \u2113 f that can be represented in L. By definition, we here thus consider expressions in TL(\u2126) with one free index variable resulting in vertex embeddings.", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}, {"heading": "MAIN RESULTS", "text": "We first provide a link between k-WL and tensor language expressions using k + 1 index variables: Theorem 4.1. For each k \u2265 1 and any collection \u2126 of functions, \u03c1 1 vwl (\u221e) k = \u03c1 1 TL k+1 (\u2126) .\nThis theorem gives us new insights: if we wish to understand how a new GNN architecture compares against the k-WL algorithms, all we need to do is to show that such an architecture can be represented in TL k+1 (\u2126), i.e., is a k-MPNN, an arguably much easier endeavor. As an example of how to use this result, it is well known that triangles can be detected by 2-WL but not by 1-WL. Thus, in order to design GNNs that can detect triangles, layer definitions in TL 3 rather than TL 2 should be used.\nWe can do much more, relating the rounds of k-WL to the notion of summation depth of TL(\u2126) expressions. We also present present similar results for functions computing graph embeddings.\nThe summation depth sd(\u03d5) of a TL(\u2126) expression \u03d5 measures the nesting depth of the summations x in the expression. It is defined inductively:\nsd(1 x op y ) = sd(E(x, y)) = sd(P s (x)) := 0, sd(\u03d5 1 \u2022 \u03d5 2 ) = sd(\u03d5 1 + \u03d5 2 ) := max{sd(\u03d5 1 ), sd(\u03d5 2 )}, sd(a \u2022 \u03d5 1 ) := sd(\u03d5 1 ), sd(f (\u03d5 1 , . . . , \u03d5 p )) := max{sd(\u03d5 i )|i \u2208 [p]\n}, and sd( x \u03d5 1 ) := sd(\u03d5 1 ) + 1. For example, expressions \u03c8 s (x 1 ) above have summation depth one. We write TL (t) k+1 (\u2126) for the class of expressions in TL k+1 (\u2126) of summation depth at most t, and use k-MPNN (t) for the corresponding class of k-MPNNs. We can now refine Theorem 4.1, taking into account the number of rounds used in k-WL.\nTheorem 4.2. For all t \u2265 0, k \u2265 1 and any collection \u2126 of functions, \u03c1 1 vwl (t) k = \u03c1 1 TL (t) k+1 (\u2126) .\nGuarded TL and color refinement. As noted by Barcel\u00f3 et al. (2020), the separation power of vertex embeddings of simple GNNs, which propagate information only through neighboring vertices, is usually weaker than that of 1-WL. For these types of architectures, Barcel\u00f3 et al. (2020) provide a relation with the weaker color refinement algorithm, but only in the special case of firstorder classifiers. We can recover and extend this result in our general setting, with a guarded version of TL which, as we will show, has the same separation power as color refinement.\nThe guarded fragment GTL(\u2126) of TL 2 (\u2126) is inspired by the use of adjacency matrices in simple GNNs. In GTL(\u2126) only equality predicates 1 xi=xi (constant 1) and 1 xi =xi (constant 0) are allowed, addition and multiplication require the component expressions to have the same (single) free index, and summation must occur in a guarded form xj E(\nx i , x j ) \u2022 \u03d5(x j ) , for i, j \u2208 [2].\nGuardedness means that summation only happens over neighbors. In GTL(\u2126), all expressions have a single free variable and thus only functions from G 1 can be represented. Our example expressions \u03c8 s (x 1 ) are guarded. The fragment GTL (t) (\u2126) consists of expressions in GTL(\u2126) of summation depth at most t.\nWe denote by MPNNs and MPNNs (t) the corresponding \"guarded\" classes of 1-MPNNs. 3\nTheorem 4.3. For all t \u2265 0 and any collection \u2126 of functions: \u03c1 1 cr (t) = \u03c1 1 GTL (t) (\u2126) .\nAs an application of this theorem, to detect the existence of paths of length t, the number of guarded layers in GNNs should account for a representation in GTL(\u2126) of summation depth of at least t. We recall that \u03c1 1 (vwl (t) 1 ) \u2282 \u03c1 1 (cr (t) ) which, combined with our previous results, implies that TL (t) 2 (\u2126) (resp., 1-MPNNs) is strictly more separating than GTL (t) (\u2126) (resp., MPNNs).\nGraph embeddings. We next establish connections between the graph versions of k-WL and CR, and TL expressions without free index variables. To this aim, we use \u03c1 0 (F ), for a set F of functions\nf : G \u2192 R \u2113 f , as the equivalence relation over G defined in analogy to \u03c1 1 : (G, H) \u2208 \u03c1 0 (F ) \u21d0\u21d2 \u2200f \u2208 F , f (G) = f (H).\nWe thus consider separation power on the graph level. For example, we can consider \u03c1 0 (gcr (t) ) and \u03c1 0 (gwl (t) k ) for any t \u2265 0 and k \u2265 1. Also here, \u03c1 0 (gwl\n(t) k+1 ) \u2282 \u03c1 0 (gwl (t) k ) but different from vertex embeddings, \u03c1 0 (gcr (t) ) = \u03c1 0 (gwl (t)\n1 ) (Grohe, 2021). We define \u03c1 0 (L) for a fragment L of TL(\u2126) by considering expressions without free index variables.\nThe connection between the number of index variables in expressions and k-WL remains to hold. Apart from k = 1, no clean relationship exists between summation depth and rounds, however. 4 Theorem 4.4. For all t \u2265 0, k \u2265 1 and any collection \u2126 of functions, we have that:\n(1) \u03c1 0 gcr (t) = \u03c1 0 TL (t+1) 2 (\u2126) = \u03c1 0 gwl (t) 1 (2) \u03c1 0 gwl (\u221e) k = \u03c1 0 TL k+1 (\u2126) .\nIntuitively, in (1) the increase in summation depth by one is incurred by the additional aggregation needed to collect all vertex labels computed by gwl (t) 1 .\nOptimality of number of indices. Our results so far tell that graph functions represented in TL k+1 (\u2126) are at most as separating as k-WL. What is left unaddressed is whether all k + 1 index variables are needed for the graph functions under consideration. It may well be, for example, that there exists an equivalent expression using less index variables. This would imply a stronger upper bound on the separation power by \u2113-WL for \u2113 < k. We next identify a large class of TL(\u2126) expressions, those of treewidth k, for which the number of index variables can be reduced to k + 1.\nProposition 4.5. Expressions in TL(\u2126) of treewidth k are equivalent to expressions in TL k+1 (\u2126).\nTreewidth is defined in the supplementary material (Section G) and a treewidth of k implies that the computation of tensor language expressions can be decomposed, by reordering summations, such that each local computation requires at most k + 1 indices (see also Aji & McEliece (2000)). As a simple example, consider \u03b8( 2) 3 such that [[\u03b8, v]] G counts the number of paths of length two starting from v. This expression has a treewidth of one. And indeed, it is equivalent to the expression\u03b8(\nx 1 ) = x2 x3 E(x 1 , x 2 ) \u2022 E(x 2 , x 3 ) in TL (\nx 1 ) = x2 E(x 1 , x 2 ) \u2022 x1 E(x 2 , x 1 ) in TL (2)\n2 (and in fact in GTL (2) ). As a consequence, no more vertices can be separated by \u03b8(x 1 ) than by cr (2) , rather than vwl 2 2 as the original expression in TL (2) 3 suggests.\nOn the impact of functions. All separation results for TL(\u2126) and fragments thereof hold irregardless of the chosen functions in \u2126, including when no functions are present at all. Function applications hence do not add expressive power. While this may seem counter-intuitive, it is due to the presence of summation and multiplication in TL that are enough to separate graphs or vertices.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "CONSEQUENCES FOR GNNS", "text": "We next interpret the general results on the separation power from Section 4 in the context of GNNs.\n1. The separation power of any vertex embedding GNN architecture which is an MPNN (t) is bounded by the power of t rounds of color refinement.\nWe consider the Graph Isomorphism Networks (GINs) (Xu et al., 2019) and show that these are MPNNs. To do so, we represent them in GTL(\u2126). Let gin be such a network; it updates vertex embeddings as follows. Initially, gin (0 t) v: := mlp (t) F (t\u22121) v:\n) : G 1 \u2192 R \u21130 : (G, v) \u2192 F (0) v: := col G (v) \u2208 R \u21130 . For layer t > 0, gin (t) : G 1 \u2192 R \u2113t is given by: (G, v) \u2192 F (\n, u\u2208NG(v) F (t\u22121) u:\n, with F (t) \u2208 R n\u00d7\u2113t and mlp (t) = (mlp (t) 1 , . . . , mlp (t) \u2113t ) : R 2\u2113t\u22121 \u2192 R \u2113t is an MLP. We denote by GIN (t) the class of GINs consisting t layers. Clearly, gin (0) can be represented in GTL (0) by considering the expressions \u03d5 (0) i (x 1 ) := P i (x 1 ) for each i \u2208 [\u2113 0 ]. To represent gin (t) , assume that we have\n\u2113 t\u22121 expressions \u03d5 (t\u22121) i (x 1 ) in GTL (t\u22121) (\u2126) representing gin (t\u22121) . That is, we have [[\u03d5 (t\u22121) i , v]] G = F (t\u22121)\nvi for each vertex v and i \u2208 [\u2113 t\u22121 ]. Then gin (t) is represented by \u2113 t expressions \u03d5 (t) i (x 1 ) defined as:\nmlp (t) i \u03d5 (t\u22121) 1 (x 1 ), . . . , \u03d5 (t\u22121) \u2113t\u22121 (x 1 ), x2 E(x 1 , x 2 ) \u2022 \u03d5 (t\u22121) 1 (x 2 ), . . . , x2 E(x 1 , x 2 ) \u2022 \u03d5 (t\u22121) \u2113t\u22121 (x 2 ) ,\nwhich are now expressions in GTL (t) (\u2126) where \u2126 consists of MLPs. We have\n[[\u03d5 (t) i , v]] G = F (t) v,i for each v \u2208 V G and i \u2208 [\u2113 t ]\n, as desired. Hence, Theorem 4.3 tells that t-layered GINs cannot be more separating than t rounds of color refinement, in accordance with known results (Xu et al., 2019;Morris et al., 2019). We thus simply cast GINs in GTL(\u2126) to obtain an upper bound on their separation power. In the supplementary material (Section D) we give similar analyses for GraphSage GNNs with various aggregation functions (Hamilton et al., 2017), GCNs (Kipf & Welling, 2017, simplified GCNs (SGCs) (Wu et al., 2019), Principled Neighborbood Aggregation (PNAs) (Corso et al., 2020), and revisit the analysis of ChebNet (Defferrard et al., 2016) given in Balcilar et al. (2021a).\n2. The separation power of any vertex embedding GNN architecture which is an k-MPNN (t) is bounded by the power of t rounds of k-WL.\nFor k = 1, we consider extended Graph Isomorphism Networks (eGINs) (Barcel\u00f3 et al., 2020). For an egin \u2208 eGIN, egin (0) : G 1 \u2192 R \u21130 is defined as for GINs, but for layer t > 0, egin (t) : G 1 \u2192 R \u2113t is defined by (G, v) \u2192 F (t) v: := mlp (t) F (t\u22121) v:\n, u\u2208NG(v) F (t\u22121) u:\n, u\u2208VG F (t\u22121) u:\n, where mlp (t) is now an MLP from R 3\u2113t\u22121 \u2192 R \u2113t . The difference with GINs is the use of u\u2208VG F (t\u22121) u: which corresponds to the unguarded summation x1 \u03d5 (t\u22121) (x 1 ). This implies that TL rather than GTL needs to be used. In a similar way as for GINs, we can represent eGIN layers in TL (t) 2 (\u2126). That is, each eGIN (t) is an 1-MPNN (t) . Theorem 4.2 tells that t rounds of 1-WL bound the separation power of t-layered extended GINs, conform to Barcel\u00f3 et al. (2020). More generally, any GNN looking to go beyond CR must use non-guarded aggregations.\nFor k \u2265 2, it is straightforward to show that t-layered \"folklore\" GNNs (k-FGNNs) (Maron et al., 2019b) are k-MPNN (t) and thus, by Theorem 4.2, t rounds of k-WL bound their separation power. One merely needs to cast the layer definitions in TL(\u2126) and observe that k+1 indices and summation depth t are needed. We thus refine and recover the k-WL bound for k-FGNNs by Azizian & Lelarge (2021). We also show that the separation power of (k + 1)-Invariant Graph Networks ((k + 1)-IGNs) (Maron et al., 2019b) are bounded by k-WL, albeit with an increase in the required rounds.\nTheorem 5.1. For any k \u2265 1, the separation power of a t-layered (k + 1)-IGNs is bounded by the separation power of tk rounds of k-WL.\nWe hereby answer open problem 1 in Maron et al. (2019a). The case k = 1 was solved in Chen et al. (2020) by analyzing properties of 1-WL. By contrast, Theorem 4.2 shows that one can focus on expressing (k +1)-IGNs in TL k+1 (\u2126) and analyzing the summation depth of expressions. The proof of Theorem 5.1 requires non-trivial manipulations of tensor language expressions; it is a simplified proof of Geerts (2020). The additional rounds (tk) are needed because (k + 1)-IGNs aggregate information in one layer that becomes accessible to k-WL in k rounds. We defer detail to Section E in the supplementary material, where we also identify a simple class of t-layered (k + 1)-IGNs that are as powerful as (k + 1)-IGNs but whose separation power is bounded by t rounds of k-WL.\nWe also consider \"augmented\" GNNs, which are combined with a preprocessing step in which higher-order graph information is computed. In the supplementary material (Section D.3) we show how TL encodes the preprocessing step, and how this leads to separation bounds in terms of k-WL, where k depends on the treewidth of the graph information used. Finally, our approach can also be used to show that the spectral CayleyNets (Levie et al., 2019) are bounded in separation power by 2-WL. This result complements the spectral analysis of CayleyNets given in Balcilar et al. (2021b).\n3. The separation power of any graph embedding GNN architecture which is a k-MPNN is bounded by the power of k-WL.\nGraph embedding methods are commonly obtained from vertex (tuple) embeddings methods by including a readout layer in which all vertex (tuple) embeddings are aggregated. For example, mlp( v\u2208V egin (t) (G, v)) is a typical readout layer for eGINs . Since egin (t) can be represented in TL (t) 2 (\u2126), the readout layer can be represented in TL (t+1)", "publication_ref": ["b3", "b2", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "2", "text": "(\u2126), using an extra summation. So they are 1-MPNNs. Hence, their separation power is bounded by gwl (t) 1 , in accordance with Theorem 4.4. This holds more generally. If vertex embedding methods are k-MPNNs, then so are their graph versions, which are then bounded by gwl (\u221e) k by our Theorem 4.4. 4. To go beyond the separation power of k-WL, it is necessary to use GNNs whose layers are represented by expressions of treewidth > k.\nHence, to design expressive GNNs one needs to define the layers such that treewidth of the resulting TL expressions is large enough. For example, to go beyond 1-WL, TL 3 representable linear algebra operations should be used. Treewidth also sheds light on the open problem from Maron et al. (2019a) where it was asked whether polynomial layers (in A) increase the separation power. Indeed, consider a layer of the form \u03c3(A 3 \u2022 F \u2022 W ), which raises the adjacency matrix A to the power three. Translated in TL(\u2126), layer expressions resemble\nx2 x3 x4 E(x 1 , x 2 ) \u2022 E(x 2 , x 3 ) \u2022 E(x 3 , x 4\n), of treewidth one. Proposition 4.5 tells that the layer is bounded by wl (3) 1 (and in fact by cr (3) ) in separation power. If instead, the layer is of the form \u03c3(C \u2022 F \u2022 W ) where C ij holds the number of cliques containing the edge ij. Then, in TL(\u2126) we get expressions containing\nx2 x3 E(x 1 , x 2 ) \u2022 E(x 1 , x 3 ) \u2022 E(x 2 , x 3 ).\nThe variables form a 3-clique resulting in expressions of treewidth two. As a consequence, the separation power will be bounded by wl (2) 2 . These examples show that it is not the number of multiplications (in both cases two) that gives power, it is how variables are connected to each other.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "FUNCTION APPROXIMATION", "text": "We next provide characterizations of functions that can be approximated by TL expressions, when interpreted as functions. We recover and extend results from Azizian & Lelarge (2021) by taking the number of layers of GNNs into account. We also provide new results related to color refinement.", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}, {"heading": "GENERAL TL APPROXIMATION RESULTS", "text": "We assume that G s is a compact space by requiring that vertex labels come from a compact set K \u2286 R \u21130 . Let F be a set of functions f : G s \u2192 R \u2113 f and define its closure F as all functions h from G s for which there exists a sequence f 1 , f 2 , . . . \u2208 F such that lim i\u2192\u221e sup G,v f i (G, v) \u2212 h(G, v) = 0 for some norm . . We assume F to satisfy two properties. First, F is concatenation-closed: if\nf 1 : G s \u2192 R p and f 2 : G s \u2192 R q are in F , then g := (f 1 , f 2 ) : G s \u2192 R p+q : (G, v) \u2192 (f 1 (G, v), f 2 (G, v)) is also in F . Second, F is function-closed, for a fixed \u2113 \u2208 N: for any f \u2208 F such that f : G s \u2192 R p , also g \u2022 f : G s \u2192 R \u2113 is in F for any continuous function g : R p \u2192 R \u2113 .\nFor such F , we let F \u2113 be the subset of functions in F from G s to R \u2113 . Our next result is based on a generalized Stone-Weierstrass Theorem (Timofte, 2005), also used in Azizian & Lelarge (2021).\nTheorem 6.1. For any \u2113, and any set F of functions, concatenation and function closed for \u2113, we have:\nF \u2113 = {f : G s \u2192 R \u2113 | \u03c1 s (F ) \u2286 \u03c1 s (f )}.\nThis result gives us insight on which functions can be approximated by, for example, a set F of functions originating from a class of GNNs. In this case, F \u2113 represent all functions approximated by instances of such a class and Theorem 6.1 tells us that this set corresponds precisely to the set of all functions that are equally or less separating than the GNNs in this class. If, in addition, F \u2113 is more separating that CR or k-WL, then we can say more. Let alg \u2208 {cr (t) , gcr (t) , vwl (t) k , gwl (\u221e) k }. Corollary 6.2. Under the assumptions of Theorem 6.1 and if \u03c1(F \u2113 ) = \u03c1(alg), then\nF \u2113 = {f : G s \u2192 R \u2113 | \u03c1(alg) \u2286 \u03c1(f )}.\nThe properties of being concatenation and function-closed are satisfied for sets of functions representable in our tensor languages, if \u2126 contains all continuous functions g : R p \u2192 R \u2113 , for any p, or alternatively, all MLPs (by Lemma 32 in Azizian & Lelarge (2021)). Together with our results in Section 4, the corollary implies that MPNNs (t) , 1-MPNNs (t) , k-MPNNs (t) or k-MPNNs can approximate all functions with equal or less separation power than cr (t) , gcr (t) , vwl (t) k or gwl (\u221e) k , respectively. Prop. 3.1 also tells that the closure consists of invariant (s = 0) and equivariant ( s > 0) functions.", "publication_ref": ["b2", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "CONSEQUENCES FOR GNNS", "text": "All our results combined provide a recipe to guarantee that a given function can be approximated by GNN architectures. Indeed, suppose that your class of GNNs is an MPNN (t) (respectively, 1-MPNN (t) , k-MPNN (t) or k-MPNN, for some k \u2265 1). Then, since most classes of GNNs are concatenation-closed and allow the application of arbitrary MLPs, this implies that your GNNs can only approximate functions f that are no more separating than cr (t) (respectively, gcr (t) , vwl (t) k or gwl (\u221e) k ). To guarantee that that these functions can indeed be approximated, one additionally has to show that your class of GNNs matches the corresponding labeling algorithm in separation power.\nFor example, GNNs in GIN (t) \u2113 are MPNNs (t) , and thus GIN (t) \u2113 contains any function f : G 1 \u2192 R \u2113 satisfying \u03c1 1 (cr (t) ) \u2286 \u03c1 1 (f ). Similarly, eGIN (t) \u2113 s are 1-MPNNs (t) , so eGIN (t) \u2113 contains any function satisfying \u03c1 1 (wl (t) 1 ) \u2286 \u03c1 1 (f ); and when extended with a readout layer, their closures consist of functions f : G 0 \u2192 R \u2113 satisfying \u03c1 0 (gcr (t) ) = \u03c1 0 (vwl (t) 1 ) \u2286 \u03c1 0 (f ). Finally, k-FGNN (t) \u2113 s are k-MPNNs (t) , so k-FGNN (t) \u2113 consist of functions f such that \u03c1 1 (vwl (t) k ) \u2286 \u03c1 1 (f ). We thus recover and extend results by Azizian & Lelarge (2021) by including layer information (t) and by treating color refinement separately from 1-WL for vertex embeddings. Furthermore, Theorem 5.1 implies that (k + 1)-IGN \u2113 consists of functions f satisfying \u03c1 1 (vwl (\u221e) k ) \u2286 \u03c1 1 (f ) and \u03c1 0 (gwl (\u221e) k ) \u2286 \u03c1 0 (f ), a case left open in Azizian & Lelarge (2021).\nThese results follow from Corollary 6.2, that the respective classes of GNNs can simulate CR or k-WL on either graphs with discrete (Xu et al., 2019;Barcel\u00f3 et al., 2020) or continuous labels (Maron et al., 2019b), and that they are k-MPNNs of the appropriate form. ", "publication_ref": ["b2", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "SUPPLEMENTARY MATERIAL A RELATED WORK CNT'D", "text": "We provide additional details on how the tensor language TL(\u2126) considered in this paper relates to recent work on other matrix query languages. Closest to TL(\u2126) is the matrix query language sum-MATLANG (Geerts et al., 2021b) whose syntax is close to that of TL(\u2126). There are, however, key differences. First, although sum-MATLANG uses index variables (called vector variables), they all must occur under a summation. In other words, the concept of free index variables is missing, which implies that no general tensors can be represented. In TL(\u2126), we can represent arbitrary tensors and the presence of free index variables is crucial to define vertex, or more generally, k-tuple embeddings in the context of GNNs. Furthermore, no notion of summation depth was introduced for sum-MATLANG. In TL(\u2126), the summation depth is crucial to assess the separation power in terms of the number of rounds of color refinement and k-WL. And in fact, the separation power of sum-MATLANG was not considered before, and neither are finite variable fragments of sum-MATLANG and connections to color refinement and k-WL studied before. Finally, no other aggregation functions were considered for sum-MATLANG. We detail in Section C.5 that TL(\u2126) can be gracefully extended to TL(\u2126, \u0398) for some arbitrary set \u0398 of aggregation functions.\nConnections to 1-WL and 2-WL and the separation power of another matrix query language, MATLANG (Brijder et al., 2019) were established in Geerts ( 2021). Yet, the design of MATLANG is completely different in spirit than that of TL(\u2126). Indeed, MATLANG does not have index variables or explicit summation aggregation. Instead, it only supports matrix multiplication, matrix transposition, function applications, and turning a vector into a diagonal matrix. As such, MATLANG can be shown to be included in TL 3 (\u2126). Similarly as for sum-MATLANG, MATLANG cannot represent general tensors, has no (free) index variables and summation depth is not considered (in view of the absence of an explicit summation).\nWe also emphasize that neither for MATLANG nor for sum-MATLANG a guarded fragment was considered. The guarded fragment is crucial to make connections to color refinement (Theorem 4.3). Furthermore, the analysis in terms of the number of index variables, summation depth and treewidth (Theorems 4.1,4.2 and Proposition 4.5), were not considered before in the matrix query language literature. For none of these matrix query languages, approximation results were considered (Section 6.1).\nMatrix query languages are used to assess the expressive power of linear algebra. Balcilar et al. (2021a) use MATLANG and the above mentioned connections to 1-WL and 2-WL, to assess the separation power of GNNs. More specifically, similar to our work, they show that several GNN architectures can be represented in MATLANG, or fragments thereof. As a consequence, bounds on their separation power easily follow. Furthermore, Balcilar et al. (2021a) propose new architectures inspired by special operators in MATLANG. The use of TL(\u2126) can thus been seen as a continuation of their approach. We note, however, that TL(\u2126) is more general than MATLANG (which is included in TL 3 (\u2126)), allows to represent more complex linear algebra computations by means summation (or other) aggregation, and finally, provides insights in the number of iterations needed for color refinement and k-WL. The connection between the number of variables (or treewidth) and k-WL is not present in the work by Balcilar et al. (2021a), neither is the notion of guarded fragment, needed to connect to color refinement. We believe that it is precisely these latter two insights that make the tensor language approach valuable for any GNN designer who wishes to upper bound their GNN architecture.", "publication_ref": ["b3", "b3", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "B DETAILS OF SECTION 3 B.1 PROOF OF PROPOSITION 3.1", "text": "Let G = (V, E, col) be a graph and let \u03c3 be a permutation of V . As usual, we define \u03c3 \u22c6 G = (V \u03c3 , E \u03c3 , col \u03c3 ) as the graph with vertex set V \u03c3 := V , edge set vw \u2208 E \u03c3 if and only if \u03c3 \u22121 (v)\u03c3 \u22121 (w) \u2208 E, and col \u03c3 (v) := col(\u03c3 \u22121 (v)). We need to show that for any expression \u03d5\n(x) in TL(\u2126) either [[\u03d5, \u03c3 \u22c6 v]] \u03c3\u22c6G = [[\u03d5, v]] G , or when \u03d5 has no free index variables, [[\u03d5]] \u03c3\u22c6G = [[\u03d5]] G .\nWe verify this by a simple induction on the structure of expressions in TL(\u2126).\n\u2022 If \u03d5(x i , x j ) = 1 xi op xj , then for a valuation \u03bd mapping x i to v i and x j to v j in V :\n[[1 xi op xj , \u03bd]] G = 1 vi op vj = 1 \u03c3(vi) op \u03c3(vj ) = [[1 xi op xj , \u03c3 \u22c6 \u03bd]] \u03c3\u22c6G ,\nwhere we used that \u03c3 is a permutation.\n\u2022 If \u03d5(x i ) = P \u2113 (x i ), then for a valuation \u00b5 mapping x i to v i in V :\n[[P \u2113 , \u00b5]] G = (col(v i )) \u2113 = (col \u03c3 (\u03c3(v i )) \u2113 = [[P \u2113 , \u03c3 \u22c6 \u03bd]] \u03c3\u22c6G ,\nwhere we used the definition of col \u03c3 .\n\u2022 Similarly, if \u03d5(x i , x j ) = E(x i , x j ), then for a valuation \u03bd assigning x i to v i and x j to v j :\n[[\u03d5, \u03bd]] G = 1 vivj \u2208E = 1 \u03c3(vi)\u03c3(vj )\u2208E \u03c3 = [[\u03d5, \u03c3 \u22c6 \u03bd]] \u03c3\u22c6G ,\nwhere we used the definition of E \u03c3 .\n\u2022 If \u03d5(x) = \u03d5 1 (x 1 ) \u2022 \u03d5 2 (x 2 ), then for a valuation \u03bd from x to V : [[\u03d5, \u03bd]] G = [[\u03d5 1 , \u03bd]] G \u2022 [[\u03d5 2 , \u03bd]] G = [[\u03d5 1 , \u03c3 \u22c6 \u03bd]] \u03c3\u22c6G \u2022 [[\u03d5 2 , \u03c3 \u22c6 \u03bd]] \u03c3\u22c6G = [[\u03d5, \u03c3 \u22c6 \u03bd]] \u03c3\u22c6G ,\nwhere we used the induction hypothesis for \u03d5 1 and \u03d5 2 . The cases \u03d5(x) = \u03d5 1 (x 1 ) + \u03d5 2 (x 2 ) and \u03d5(x) = a \u2022 \u03d5 1 (x) are dealt with in a similar way.\n\u2022 If \u03d5(x) = f (\u03d5 1 (x 1 ), . . . , \u03d5 p (x p )), then [[\u03d5, \u03bd]] G = f ([[\u03d5 1 , \u03bd]] G , . . . , [[\u03d5 p , \u03bd]] G ) = f ([[\u03d5 1 , \u03c3 \u22c6 \u03bd]] \u03c3\u22c6G , . . . , [[\u03d5 p , \u03c3 \u22c6 \u03bd]] \u03c3\u22c6G ) = [[\u03d5, \u03c3 \u22c6 \u03bd]] \u03c3\u22c6G ,\nwhere we used again the induction hypothesis for \u03d5 1 , . . . , \u03d5 p .\n\u2022 Finally, if \u03d5(x) = y \u03d5 1 (x, y) then for a valuation \u03bd of x to V :\n[[\u03d5, \u03bd]] G = v\u2208V [[\u03d5 1 , \u03bd[y \u2192 v]]] G = v\u2208V [[\u03d5 1 , \u03c3 \u22c6 \u03bd[y \u2192 v]]] \u03c3\u22c6G = v\u2208V \u03c3 [[\u03d5 1 , \u03c3 \u22c6 \u03bd[y \u2192 v]]] \u03c3\u22c6G = [[\u03d5, \u03c3 \u22c6 \u03bd]] \u03c3\u22c6G ,\nwhere we used the induction hypothesis for \u03d5 1 and that V \u03c3 = V because \u03c3 is a permutation.\nWe remark that when \u03d5 does not contain free index variables, then [[\u03d5, \u03bd]] G = [[\u03d5]] G for any valuation \u03bd, from which invariance follows from the previous arguments. This concludes the proof of Proposition 3.1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C DETAILS OF SECTION 4", "text": "In the following sections we prove Theorem 4.1, 4.2, 4.3 and 4.4. More specifically, we start by showing these results in the setting that TL(\u2126) only supports summation aggregation ( x e) and in which the vertex-labellings in graphs take values in {0, 1} \u2113 . In this context, we introduce classical logics in Section C.1 and recall and extend connections between the separation power of these logics and the separation power of color refinement and k-WL in Section C.2. We connect TL(\u2126) and logics in Section C.3, to finally obtain the desired proofs in Section C.4. We then show how these results can be generalized in the presence of general aggregation operators in Section C.5, and to the setting where vertex-labellings take values in R \u2113 in Section C.6.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1 CLASSICAL LOGICS", "text": "In what follows, we consider graphs G = (V G , E G , col G ) with col G : V G \u2192 {0, 1} \u2113 . We start by defining the k-variable fragment C k of first-order logic with counting quantifiers, followed by the definition of the guarded fragment GC of C 2 . Formulae \u03d5 in C k are defined over the set {x 1 , . . . , x k } of variables and are formed by the following grammar:\n\u03d5 := (x i = x j ) | E(x i , x j ) | P s (x i ) | \u00ac\u03d5 | \u03d5 \u2227 \u03d5 | \u2203 \u2265m x i \u03d5,\nwhere i, j \u2208 [k], E is a binary predicate, P s for s \u2208 [\u2113] are unary predicates for some \u2113 \u2208 N, and m \u2208 N. The semantics of formulae in C k is defined in terms of interpretations relative to a given graph G and a (partial) valuation \u00b5 : {x 1 , . . . , x k } \u2192 V G . Such an interpretation maps formulae, graphs and valuations to Boolean values B := {\u22a5, \u22a4}, in a similar way as we did for tensor language expressions.\nMore precisely, given a graph G = (V G , E G , col G ) and partial valuation \u00b5 :\n{x 1 , . . . , x k } \u2192 V G , we define [[\u03d5, \u00b5]] B G \u2208 B\nfor valuations defined on the free variables in \u03d5. That is, we define:\n[[x i = x j , \u00b5]] B G := if \u00b5(x i ) = \u00b5(x j ) then \u22a4 else \u22a5; [[E(x i , x j ), \u00b5]] B G := if \u00b5(x i )\u00b5(x j ) \u2208 E G then \u22a4 else \u22a5; [[P s (x i ), \u00b5]] B G := if col G (\u00b5(x i )) s = 1 then \u22a4 else \u22a5; [[\u00ac\u03d5, \u00b5]] B G := \u00ac[[\u03d5, \u00b5]] B G ; [[\u03d5 1 \u2227 \u03d5 2 , \u00b5]] B G := [[\u03d5 1 , \u00b5]] B G \u2227 [[\u03d5 2 , \u00b5]] B G ; [[\u2203 \u2265m x i \u03d5 1 , \u00b5]] B G := if |{v \u2208 V G | [[\u03d5, \u00b5[x i \u2192 v]]] B G = \u22a4}| \u2265 m then \u22a4 else \u22a5. In the last expression, \u00b5[x i \u2192 v]\ndenotes the valuation \u00b5 modified such that it maps x i to vertex v.\nWe will also need the guarded fragment GC of C 2 in which we only allow equality conditions of the form x i = x i , component expressions of conjunction and disjunction should have the same single free variable, and counting quantifiers can only occur in guarded form:\n\u2203 \u2265m x 2 (E(x 1 , x 2 ) \u2227 \u03d5(x 2 )) or \u2203 \u2265m x 1 (E(x 2 , x 1 ) \u2227 \u03d5(x 1 )). The semantics of formulae in GC is inherited from formulae in C 2 .\nFinally, we will also consider C k \u221e\u03c9 , that is, the logic C k extended with infinitary disjunctions and conjunctions. More precisely, we add to the grammar of formulae the following constructs: where the index set A can be arbitrary, even containing uncountably many indices. We define GC \u221e\u03c9 in the same way by relaxing the finite variable conditions. The semantics is, as expected:\n[[ \u03b1\u2208A \u03d5 \u03b1 , \u00b5]] B G = \u22a4 if for at least one \u03b1 \u2208 A, [[\u03d5 \u03b1 , \u00b5]] B G = \u22a4, and [[ \u03b1\u2208A \u03d5 \u03b1 , \u00b5]] B G = \u22a4 if for all \u03b1 \u2208 A, [[\u03d5 \u03b1 , \u00b5]] B G = \u22a4.\nWe define the free variables of formulae just as for TL, and similarly, quantifier rank is defined as summation depth (only existential quantifications increase the quantifier rank). For any of the above logics L we define L (t) as the set of formulae in L of quantifier rank at most t.\nTo capture the separation power of logics, we define \u03c1 1 L (t) as the equivalence relation on G 1 defined by\n(G, v), (H, w) \u2208 \u03c1 1 L (t) \u21d0\u21d2 \u2200\u03d5(x) \u2208 L (t) : [[\u03d5, \u00b5 v ]] B G = [[\u03d5, \u00b5 w ]] B H\n, where \u00b5 v is any valuation such that \u00b5(x) = v, and likewise for w. The relation \u03c1 0 is defined in a similar way, except that now the relation is only over pairs of graphs, and the characterization is over all formulae with no free variables (also called sentences). Finally, we also use, and define, the relation \u03c1 s , which relates pairs from G s : consisting of a graph and an s-tuple of vertices. The relation is defined as\n(G, v), (H, w) \u2208 \u03c1 s L (t) \u21d0\u21d2 \u2200\u03d5(x) \u2208 L (t) : [[\u03d5, \u00b5 v ]] B G = [[\u03d5, \u00b5 w ]] B\nH , where x consist of s free variables and \u00b5 v is a valuation assigning the i-th variable of x to the i-th value of v, for any i \u2208 [s].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.2 CHARACTERIZATION OF SEPARATION POWER OF LOGICS", "text": "We first connect the separation power of the color refinement and k-dimensional Weisfeiler-Leman algorithms to the separation power of the logics we just introduced. Although most of these connections are known, we present them in a bit of a more fine-grained way. That is, we connect the number of rounds used in the algorithms to the quantifier rank of formulae in the above logics. Proposition C.1. For any t \u2265 0, we have the following identities:\nPublished as a conference paper at ICLR 2022\n(1) \u03c1 1 cr (t) = \u03c1 1 GC (t) and \u03c1 0 gcr (t) = \u03c1 0 gwl (t) 1 = \u03c1 0 C 2,(t+1) ; (2) For k \u2265 1, \u03c1 1 vwl (t) k = \u03c1 1 C k+1,(t) and \u03c1 0 C k+1,(t+k) \u2286 \u03c1 0 gwl (t) k \u2286 \u03c1 0 C k+1,(t+1) . As a consequence, \u03c1 0 gwl (\u221e) k = \u03c1 0 C k+1 .\nProof. For (1), the identity \u03c1 1 cr (t) = \u03c1 1 GC (t) is known and can be found, for example, in Theorem V.10 in Grohe (2021). The identity \u03c1 0 gcr (t) = \u03c1 0 gwl (t) 1 can be found in Proposition V.4 in Grohe ( 2021). The identity \u03c1 0 gwl (t) t+1) is a consequence of the inclusion shown in (2) for k = 1.\n1 = \u03c1 0 C 2,(\nFor (2), we use that \u03c1 k wl (t) k = \u03c1 k C k+1,(t) , see e.g., Theorem V.8 in Grohe (2021). We argue that this identity holds for \u03c1 1 vwl (t) k = \u03c1 1 C k+1, (t) . Indeed, suppose that (G, v) and (H, w)\nare not in \u03c1 1 C k+1,(t) . Let \u03d5(x 1 ) be a formula in C k+1,(t) such that [[\u03d5, v]] B G = [[\u03d5, w]] B H . Con- sider the formula \u03d5 + (x 1 , . . . , x k ) = \u03d5(x 1 ) \u2227 k i=1 (x 1 = x i ). Then, [[\u03d5 + , (v, . . . , v)]] B G = [[\u03d5 + , (w, . . . , w)]] B\nH , and hence (G, (v, . . . , v)) and (H, (w, . . . , w)) are not in \u03c1 k C k+1,(t) either. This implies that wl (t) k (G, (v, . . . , v)) = wl (t) k (H, (w, . . . , w)), and thus, by definition, vwl (t) k (G, v) = vwl (t) k (H, w). In other words, (G, v) and (H, w) are not in\n\u03c1 1 vwl (t) k , from which the inclusion \u03c1 1 vwl (t) k \u2286 \u03c1 1 C k+1,(t) follows. Conversely, if (G, v) and (H, w) are not in \u03c1 1 vwl (t) k , then wl (t) k (G, (v, . . . , v)) = wl (t)\nk (H, (w, . . . , w)). As a consequence, (G, (v, . . . , v)) and (H, (w, . . . , w))\nare not in \u03c1 k C k+1,(t) either. Let \u03d5(x 1 , . . . , x k ) be a formula in C k+1,(t) such that [[\u03d5, (v, . . . , v)]] B G = [[\u03d5, (w, . . . , w)]] B H . Then it is readily shown that we can convert \u03d5(x 1 , . . . , x k ) into a formula \u03d5 \u2212 (x 1 ) in C k+1,(t) such that [[\u03d5 \u2212 , v]] B G = [[\u03d5 \u2212 , w]] B\nH , and thus (G, v) and (H, w) are not in \u03c1 1 C k+1, (t) . Hence, we also have the inclusion t) , form which the first identity in (2) follows.\n\u03c1 1 vwl (t) k \u2287 \u03c1 1 C k+1,(", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "It remains to show", "text": "\u03c1 0 C k+1,(t+k) \u2286 \u03c1 0 gwl (t) k \u2286 \u03c1 0 C k+1,(t+1) . Clearly, if (G, H) is not in \u03c1 0 gwl (t) k\nthen the multisets of labels wl (t) k (G, v) and wl (t) k (H, w) differ. It is known that with each label c one can associate a formula \u03d5 c in C k+1, (t) such that [[\u03d5 c , v]] B G = \u22a4 if and only if wl (t) k (G, v) = c. So, if the multisets are different, there must be a c that occurs more often in one multiset than in the other one. This can be detected by a fomulae of the form \u2203 =m (x 1 , . . . , x k )\u03d5 c (x 1 , . . . , x k ) which is satisfied if there are m tuples v with label c. It is now easily verified that the latter formula can be converted into a formula in C k+1, (t+k) . Hence, the inclusion \u03c1\n0 C k+1,(t+k) \u2286 \u03c1 0 gwl (t) k follows. For \u03c1 0 gwl (t) k \u2286 \u03c1 0 C k+1,(t+1) , we show that if (G, H) is in \u03c1 0 gwl (t) k , then this implies that [[\u03d5, \u00b5]] B G = [[\u03d5, \u00b5]] B\nH for all formulae in C k+1,(t+1) and any valuation \u00b5 (notice that \u00b5 is superfluous in this definition when formulas have no free variables). Assume that (G, H) is in \u03c1 0 (gwl (t) k ). Since any formula of quantifier rank t+1 is a Boolean combination of formulas of less rank or a formula of the form \u03d5 = \u2203 \u2265m x i \u03c8 where \u03c8 is of quantifier rank t, without loss of generality consider a formula of the latter form, and assume for the sake of contradiction that [\n[\u03d5, \u00b5]] B G = \u22a4 but [[\u03d5, \u00b5]] B H = \u22a5. Since [[\u03d5, \u00b5]] B G = \u22a4, there must be at least m elements satisfying \u03c8. More precisely, let v 1 , . . . , v p in G be all vertices in G such that for each valuation \u00b5[x \u2192 v i ] it holds that [[\u03c8, \u00b5[x \u2192 v i ]] B G = \u22a4.\nAs mentioned, it must be that p is at least m. Using again the fact that \u03c1 k wl (t) k = \u03c1 k C k+1,(t) , we infer that the color wl\n(t\u22121) k (G, (v i , . . . , v i )) is the same, for each such v i . Now since gwl (t\u22121) k (G) = gwl (t\u22121) k (H)\n, it is not difficult to see that there must be exactly p vertices w 1 , . . . , w p in H such that wl\n(t\u22121) k (G, (v i , . . . , v i )) = wl (t\u22121) k\n(H, (w i , . . . , w i )). Otherwise, it would simply not be the case that the aggregation step of the colors, assigned by k-WL is the same in G and H. By the connection to logic, we again know that for valuation \u00b5\n[x \u2192 w i ] it holds that [[\u03c8, \u00b5[x \u2192 w i ]] B H = \u22a4.\nIt then follows that [[\u03d5, \u00b5]] B H = \u22a4 for any valuation \u00b5, which was to be shown.\nFinally, we remark that \u03c1 0 gwl (\u221e) k = \u03c1 0 C k+1 follows from the preceding inclusions in (2).\nBefore moving to tensor languages, where we will use infinitary logics to simulate expressions in TL k (\u2126) and GTL(\u2126), we recall that, when considering the separation power of logics, we can freely move between the logics and their infinitary counterparts:\nTheorem C.2. The following identities hold for any t \u2265 0, k \u2265 2 and s \u2265 0:\n(1) \u03c1 1 GC (t) \u221e\u03c9 = \u03c1 1 GC (t) ;\n(2) \u03c1 s C k,(t) \u221e\u03c9 = \u03c1 s C k,(t) .\nProof. For identity (1), notice that we only need to prove that \u03c1 1 GC (t) \u2286 \u03c1 1 GC (t) \u221e\u03c9 , the other direction follows directly from the definition. We point out the well-known fact that two tuples (G, v) and (H, w) belong to \u03c1 1 GC (t) if and only if the unravelling of G rooted at v up to depth t is isomorphic to the unravelling of H rooted at w up to root t. Here the unravelling is the infinite tree whose root is the root node, and whose children are the neighbors of the root node (see e.g. Barcel\u00f3 et al. (2020); Otto (2019). Now for the connection with infinitary logic. Assume that the unravellings of G rooted at v and of H rooted at w up to level t are isomorphic, but assume for the sake of contradiction that there is a formula \u03d5(x) in GC (t) \u221e\u03c9 such that\n[[\u03d5, \u00b5 v ]] B G = [[\u03d5, \u00b5 w ]] B\nH , where \u00b5 v and \u00b5 w are any valuation mapping variable x to v and w, respectively. Now since G and H are finite graphs, one can construct, from formula \u03c6, a formula \u03c6 \u2032 in GC (t) \nsuch that [[\u03c8, \u00b5 v ]] B G = [[\u03c8, \u00b5 w ]] B H .\nNotice that this is in contradiction with our assumption that unravellings where isomorphic and therefore indistinguishable by formulae in GC (t) . To construct \u03c8, consider an infinitary disjunction a\u2208A \u03b1 a . Since G and H have a finite number of vertices, and the formulae have a finite number of variables, the number of different valuations from the variables to the vertices in G or H is also finite. Thus, one can replace any extra copy of \u03b1 a , \u03b1 a \u2032 such that their value is the same in G and H. The final result is a finite disjunction, and the truth value over G and H is equivalent to the original infinitary disjunction.\nFor identity (2) we refer to Corollary 2.4 in Otto (2017).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.3 FROM TL(\u2126) TO C k", "text": "\u221e\u03c9 AND GC \u221e\u03c9\nWe are now finally ready to make the connection between expressions in TL(\u2126) and the infinitary logics introduced earlier.\nProposition C.3. For any expression \u03d5(x) in TL k (\u2126) and c \u2208 R, there exists an expression\u03c6 c (x\n) in C k \u221e\u03c9 such that [[\u03d5, v]] G = c if and only if [[\u03c6 c , v]] B G = \u22a4 for any graph G = (V G , E G , col G ) in G and v \u2208 V k G . Furthermore, if \u03d5(x) \u2208 GTL(\u2126) then\u03c6 c \u2208 GC \u221e\u03c9 .\nFinally, if \u03d5 has summation depth t then\u03c6 c has quantifier rank t.\nProof. We define\u03c6 c inductively on the structure of expressions in TL k (\u2126).\n\u2022 \u03d5(x i , x j ) := 1 xi op xj . Assume first that op is \"=\". We distinguish between (a) i = j and (b) i = j. For case (a), if c = 1, then we define\u03c6 1 (x i , x j ) := (x i = x j ), if c = 0, then we define\u03c6 0 (x i , x j ) := \u00ac(x i = x j ), and if c = 0, 1, then we define\u03c6 c (x i , x j ) :\n= x i = x i .\nFor case (b), if c = 1, then we define\u03c6 1 (x i , x j ) := (x i = x i ), and for any c = 1, we define\u03c6 c (x i , x j ) := \u00ac(x i = x i ). The case when op is \" =\" is treated analogously.\n\u2022 \u03d5(x i ) := P \u2113 (x i ). If c = 1, then we define\u03c6 1 (x i ) := P \u2113 (x i ), if c = 0, then we defin\u1ebd \u03d5 0 (x i ) := \u00acP j (x i ). For all other c, we define\u03c6 c (x i , x j ) := \u00ac(x i = x i ).\n\u2022 \u03d5(x i , x j ) := E(x i , x j ). If c = 1, then we define\u03c6 1 (x i , x j ) := E(x i , x j ), if c = 0, then we define\u03c6 0 (x i , x j ) := \u00acE(x i , x j ). For all other c, we define\u03c6 c (x i , x j ) := \u00ac(x i = x i ).\n\u2022 \u03d5 := \u03d5 1 + \u03d5 2 . We observe that [[\u03d5, v]] G = c if and only if there are\nc 1 , c 2 \u2208 R such that [[\u03d5 1 , v]] G = c 1 and [[\u03d5 2 , v]] G = c 2 and c = c 1 + c 2 .\nHence, it suffices to defin\u1ebd\n\u03d5 c := c1,c2\u2208R c=c1+c2\u03c6 c1 1 \u2227\u03c6 c2 2 , where\u03c6 c1 1 and\u03c6 c2 2 are the expressions such that [[\u03d5 1 , v]] G = c 1 if and only if [[\u03c6 c1 1 , v]] B G = \u22a4 and [[\u03d5 2 , v]] G = c 2 if and only if [[\u03c6 c2 2 , v]] B G = \u22a4, which exist by induction. \u2022 \u03d5 := \u03d5 1 \u2022 \u03d5 2\n. This is case is analogous to the previous one. Indeed, [[\u03d5, v]\n] G = c if and only if there are c 1 , c 2 \u2208 R such that [[\u03d5 1 , v]] G = c 1 and [[\u03d5 2 , v]] G = c 2 and c = c 1 \u2022 c 2 .\nHence, it suffices to define\u03c6\nc := c1,c2\u2208R c=c1\u2022c2\u03c6 c1 1 \u2227\u03c6 c2 2 .\n\u2022 \u03d5 := a \u2022 \u03d5 1 . This is case is again dealt with in a similar way. Indeed, [[\u03d5, v]] G = c if and only if there is a\nc 1 \u2208 R such that [[\u03d5 1 , v]] G = c 1 and c = a \u2022 c 1 .\nHence, it suffices to defin\u1ebd\n\u03d5 c := c1\u2208R c=a\u2022c1\u03c6 c1\n1 .\n\u2022 \u03d5 := f (\u03d5 1 , . . . , \u03d5 p ) with f : R p \u2192 R. We observe that [[\u03d5, v]] G = c if and only if there \nare c 1 , . . . , c p \u2208 R such that c = f (c 1 , . . . , c p ) and [[\u03d5 i , v]] G = c i for i \u2208 [p]. Hence, it suffices to define\u03c6 c := c1,...,cp\u2208R c=f (c1,...,cp)\u03c6 c1 1 \u2227 \u2022 \u2022 \u2022 \u2227\u03c6 cp p . \u2022 \u03d5 := xi \u03d5 1 . We observe that [[\u03d5, \u00b5]] G = c implies that we can partition V G into \u2113 parts V 1 , . . . , V \u2113 , of sizes m 1 , . . . , m \u2113 , respectively, such that [[\u03d5 1 , \u00b5[x i \u2192 v]]] G = c i for each v \u2208 V i ,\n\u03d5 c := \u2113,m1,...,m \u2113 \u2208N c1,...,c \u2113 \u2208R c= \u2113 i=1 mici \u2113 i=1 \u2203 =mi x i\u03c6 ci 1 \u2227 \u2200x i \u2113 i=1\u03c6 ci 1 ,\nwhere \u2203 =mi x i \u03c8 is shorthand notation for \u2203 \u2265mi x i \u03c8 \u2227 \u00ac\u2203 \u2265mi+1 x i \u03c8, and \u2200x i \u03c8 denotes \u00ac\u2203 \u22651 x i \u00ac\u03c8.\nThis concludes the construction of\u03c6 c . We observe that we only introduce a quantifiers when \u03d5 =\nxi \u03d5 1 and hence if we assume by induction that summation depth and quantifier rank are in sync, then if \u03d5 1 has summation depth t \u2212 1 and thus\u03c6 c 1 has quantifier rank t \u2212 1 for any c \u2208 R, then \u03d5 has summation depth t, and as can be seen from the definition of\u03c6 c , this formula has quantifier rank t, as desired.\nIt remains to verify the claim about guarded expressions. This is again verified by induction. The only case requiring some attention is \u03d5(x 1 ) := x2 E(x 1 , x 2 ) \u2227 \u03d5 1 (x 2 ) for which we can defin\u1ebd\n\u03d5 c := \u2113,m1,...,m \u2113 \u2208N c1,...,c \u2113 \u2208R c= \u2113 i=1 mici m= \u2113 i=1 mi \u2203 =m x 2 E(x 1 , x 2 ) \u2227 \u2113 i=1 \u2203 =mi x 2 E(x 1 , x 2 ) \u2227\u03c6 ci 1 (x 2 ),\nwhich is a formula in GC again only adding one to the quantifier rank of the formulae\u03c6 c 1 for c \u2208 R. So also here, we have the one-to-one correspondence between summation depth and quantifier rank.\nC.4 PROOF OF THEOREM 4.1, 4.2, 4.3 AND 4.4\nProposition C.4. We have the following inclusions: For any t \u2265 0 and any collection \u2126 of functions:\n\u2022 \u03c1 1 cr (t) \u2286 \u03c1 1 GTL (t) (\u2126) ; \u2022 \u03c1 1 vwl (t) k \u2286 \u03c1 1 TL (t)\nk+1 (\u2126) ; and\n\u2022 \u03c1 0 gwl (t) k \u2286 \u03c1 0 TL (t+1) k+1 (\u2126) .\nProof. We first show the second bullet by contraposition. That is, we show that if (G, v) and (H, w) are not in \u03c1 1 TL (t) k+1 (\u2126) , then neither are they in \u03c1 1 vwl (t) k . Indeed, suppose that there exists an expression \u03d5(x 1 ) in TL (t) k+1\n(\u2126) such that [[\u03d5, v]\n] G = c = c \u2032 = [[\u03d5, w]] H . From Proposition C.3 we know that there exists a formula\u03c6 c in C k+1,(t) \u221e\u03c9 such that [[\u03c6 c , v]] B G = \u22a4 and [[\u03c6 c , w]] B H = \u22a5.\nHence, (G, v) and (H, w) do no belong to \u03c1 1 C k+1,(t) \u221e\u03c9 . Theorem C.2 implies that (G, v) and (H, w) also do not belong to \u03c1 1 C k+1,(t) . Finally, Proposition C.1 implies that (G, v) and (H, w) do not belong to \u03c1 1 (vwl (t) k , as desired. The third bullet is shown in precisely the same, but using the identities for \u03c1 0 rather than \u03c1 1 , and gwl (t) k rather than vwl (t) k . Also the first bullet is shown in the same way, using the connection between GTL (t) (\u2126), GC 2,(t) \u221e\u03c9 , GC (t) and cr (t) , as given by Proposition C.1, Theorem C.2, and Proposition C.3.\nWe next show that our tensor languages are also more separating than the color refinement and k-dimensional Weisfeiler-Leman algorithms.\nProposition C.5. We have the following inclusions: For any t \u2265 0 and any collection \u2126 of functions: t) k ; and\n\u2022 \u03c1 1 GTL (t) (\u2126) \u2286 \u03c1 1 cr (t) ; \u2022 \u03c1 1 TL (t) k+1 (\u2126) \u2286 \u03c1 1 vwl (\n\u2022 \u03c1 0 TL (t+k) k+1 (\u2126) \u2286 \u03c1 0 gwl (t) k .\nProof. For any of these inclusions to hold, for any \u2126, we need to show the inclusion without the use of any functions. We again use the connections between the color refinement and k-dimensional Weisfeiler-Leman algorithms and finite variable logics as stated in Proposition C.1. More precisely, we show for any formula \u03d5(x) \u2208 C k,(t) there exists an expression\u03c6(x) \u2208 TL (t) k such that for any \u03c6, v]] G = 0. By appropriately selecting k and t and by observing that when \u03d5(x) \u2208 GC then\u03c6(x) \u2208 GTL, the inclusions follow.\ngraph G in G, [[\u03d5, v]] B G = \u22a4 implies [[\u03c6, v]] G = 1 and [[\u03d5, v]] B G = \u22a5 implies [[\nThe construction of\u03c6(x) is by induction on the structure of formulae in C k .\n\u2022 \u03d5 := (x i = x j ). Then, we define\u03c6 := 1 xi=xj .\n\u2022 \u03d5 := P \u2113 (x i ). Then, we define\u03c6 := P \u2113 (x i ).\n\u2022 \u03d5 := E(x i , x j ). Then, we define\u03c6 := E(x i , x j ).\n\u2022 \u03d5 := \u00ac\u03d5 1 . Then, we define\u03c6 := 1 xi=xi \u2212\u03c6 1 .\n\u2022 \u03d5 := \u03d5 1 \u2227 \u03d5 2 . Then, we define\u03c6 :=\u03c6 1 \u2022\u03c6 2 .\n\u2022 \u03d5 := \u2203 \u2265m x i \u03d5 1 . Consider a polynomial p(x) := j a j x j such that p(x) = 0 for x \u2208 {0, 1, . . . , m \u2212 1} and p(x) = 1 for x \u2208 {m, m + 1, . . . , n}. Such a polynomial exists by interpolation. Then, we define\u03c6 := j a j xi\u03c6 1 j .\nWe remark that we here crucially rely on the assumption that G contains graphs of fixed size n and that TL k is closed under linear combinations and product. Clearly, if \u03d5 \u2208 GC, then the above translations results in an expression\u03c6 \u2208 GTL(\u2126). Furthermore, the quantifier rank of \u03d5 is in oneto-one correspondence to the summation depth of\u03c6.\nWe can now apply Proposition C.1. That is, if (G, v) and (H, w) are not in \u03c1 1 cr (t) then by Proposition C.1, there exists a formula \u03d5(x) in GC (t) such that [[\u03d5, v]]\nB G = \u22a4 = [[\u03d5, w]] B H = \u22a5.\nWe have just shown when we consider\u03c6, in GTL (t) , also [[\u03c6, v]] G = [[\u03c6, w]] H holds. Hence, (G, v) and (H, w) are not in \u03c1 1 GTL (t) (\u2126) either, for any \u2126. Hence, \u03c1 1 GTL (t) (\u2126) \u2286 \u03c1 1 cr (t) holds. The other bullets are shown in the same way, again by relying on Proposition C.1 and using that we can move from vwl (t) k and gwl (t) k to logical formulae, and to expressions in TL (t) k+1 and TL (t+k) k+1 , respectively, to separate (G, v) from (H, w) or G from H, respectively. Theorems 4.1, 4.2, 4.3 and 4.4 now follow directly from Propositions C.4 and C.5.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.5 OTHER AGGREGATION FUNCTIONS", "text": "As is mentioned in the main paper, our upper bound results on the separation power of tensor languages (and hence also of GNNs represented in those languages) generalize easily when other aggregation functions than summation are used in TL expressions.\nTo clarify what we understand by an aggregation function, let us first recall the semantics of summation aggregation. Let \u03d5 :=\nxi \u03d5 1 , where xi represents summation aggregation, let G = (V G , E G , col G ) be a graph, and let \u03bd be a valuation assigning index variables to vertices in V G . The semantics is then given by:\n[[ xi \u03d5 1 , \u03bd]] G := v\u2208VG [[\u03d5 1 , \u03bd[x i \u2192 v]]] G ,\nas explained in Section 3. Semantically, we can alternatively view xi \u03d5 1 as a function which takes the sum of the elements in the following multiset of real values:\n{{[[\u03d5 1 , \u03bd[x i \u2192 v]]] G | v \u2208 V G }}.\nOne can now consider, more generally, an aggregation function F as a function which assigns to any multiset of values in R a single real value. For example, F could be max, min, mean, . . .. Let \u0398 be such a collection of aggregation functions. We next incorporate general aggregation function in tensor language.\nFirst, we extend the syntax of expressions in TL(\u2126) by generalizing the construct xi \u03d5 in the grammar of TL(\u2126) expression. More precisely, we define TL(\u2126, \u0398) as the class of expressions, formed just like tensor language expressions, but in which two additional constructs, unconditional and conditional aggregation, are allowed. For an aggregation function F we define:\naggr F xj (\u03d5) and aggr F xj \u03d5(x j ) | E(x i , x j )\n, where in the latter construct (conditional aggregation) the expression \u03d5(x j ) represents a TL(\u2126, \u0398) expression whose only free variable is x j . The intuition behind these constructs is that unconditional aggregation aggr F xj (\u03d5) allows for aggregating, using aggregate function F , over the values of \u03d5 where x j ranges unconditionally over all vertices in the graph. In contrast, for conditional aggregation aggr F xj \u03d5(x j ) | E(x i , x j ) , aggregation by F of the values of \u03d5(x j ) is conditioned on the neighbors of the vertex assigned to x i . That is, the vertices for x j range only among the neighbors of the vertex assigned to x i . More specifically, the semantics of the aggregation constructs is defined as follows:\n[[aggr F xj (\u03d5), \u03bd]] G := F ({{[[\u03d5, \u03bd[x j \u2192 v]]] G | v \u2208 V G }}) \u2208 R. [[aggr F xj \u03d5(x j ) | E(x i , x j ) , \u03bd]] G := F ({{[[\u03d5, \u03bd[x j \u2192 v]]] G | v \u2208 V G , (\u03bd(x i ), v) \u2208 E G }}) \u2208 R.\nWe remark that we can also consider aggregations functions F over multisets of values in R \u2113 for some \u2113 \u2208 N. This requires extending the syntax with aggr F xj (\u03d5 1 , . . . , \u03d5 \u2113 ) for unconditional aggregation and with aggr F xj \u03d5 1 (x j ), . . . , \u03d5 \u2113 (x j ) | E(x i , x j ) for conditional aggregation. The semantics is as expected:\nF ({{(([[\u03d5 1 , \u03bd[x j \u2192 v]]] G , . . . , [[\u03d5 \u2113 , \u03bd[x j \u2192 v]]] G ) | v \u2208 V G }}) \u2208 R and F ({{(([[\u03d5 1 , \u03bd[x j \u2192 v]]] G , . . . , [[\u03d5 \u2113 , \u03bd[x j \u2192 v]]] G ) | v \u2208 V G , (\u03bd(x i ), v) \u2208 E G }}) \u2208 R.\nThe need for considering conditional and unconditional aggregation separately is due to the use of arbitrary aggregation functions. Indeed, suppose that one uses an aggregation function F for which 0 \u2208 R is a neutral value. That is, for any multiset X of real values, the equality F (X) = F (X \u228e{0}) holds. For example, the summation aggregation function satisfies this property. We then observe:\n[[aggr F xj \u03d5(x j ) | E(x i , x j ) , \u03bd]] G = F ({{[[\u03d5, \u03bd[x j \u2192 v]]] | v \u2208 V G , (\u03bd(x i ), v) \u2208 E G }} = F ({{[[\u03d5 \u2022 E(x i , x j ), \u03bd[x j \u2192 v]]] | v \u2208 V G }} = [[aggr F xj (\u03d5(x j ) \u2022 E(x i , x j )), \u03bd]] G .\nIn other words, unconditional aggregation can simulate conditional aggregation. In contrast, when 0 is not a neutral value of the aggregation function F , conditional and unconditional aggregation behave differently. Indeed, in such cases aggr F xj \u03d5(x j ) | E(x i , x j ) and aggr F xj (\u03d5(x j ) \u2022 E(x i , x j )) may evaluate to different values, as illustrated in the following example.\nAs aggregation function F we take the average avg(X) := 1", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "|X|", "text": "x\u2208X x for multisets X of real values. We remark that 0's in X contribute to the size of X and hence 0 is not a neutral element of avg. Now, let us consider the expressions\n\u03d5 1 (x i ) := aggr avg xj (1 xj =xj \u2022 E(x i , x j )) and \u03d5 2 (x i ) := aggr avg xj (1 xj =xj | E(x i , x j )).\nLet \u03bd be such that \u03bd(\nx i ) = v. Then, [[\u03d5 1 , \u03bd]] G results in applying the average to the multiset {{1 w=w \u2022 E(v, w) | w \u2208 V G }} which includes the value 1 for every w \u2208 N G (v) and a 0 for every non- neighbor w \u2208 N G (v). In other words, [[\u03d5 1 , \u03bd]] G results in |N G (v)|/|V G |. In contrast, [[\u03d5 2 , \u03bd]] G results in applying the average to the multiset {{1 w=w | w \u2208 V G , (v, w) \u2208 E G }}.\nIn other words, this multiset only contains the value 1 for each w \u2208 N G (v), ignoring any information about the non-neighbors of v. In other words,\n[[\u03d5 2 , \u03bd]] G results in |N G (v)|/|N G (v)| = 1.\nHence, conditional and unconditional aggregation behave differently for the average aggregation function.\nThis said, one could alternative use a more general variant of conditional aggregation of the form aggr\nF xj (\u03d5|\u03c8) with as semantics [[aggr F xj (\u03d5|\u03c8), \u03bd]] G := F {{[[\u03d5, \u03bd[x j \u2192 v]]] G | v \u2208 V G , [[\u03c8, \u03bd[x j \u2192 v]] G = 0}}\nwhere one creates a multiset only for those valuations \u03bd[x j \u2192 v] for which the condition \u03c8 evaluates to a non-zero value. This general form of aggregation includes conditional aggregation, by replacing \u03c8 with E(x i , x j ) and restricting \u03d5, and unconditional aggregation, by replacing \u03c8 with the constant function 1, e.g., 1 xj =xj . In order not to overload the syntax of TL expressions, we will not discuss this general form of aggregation further.\nThe notion of free index variables for expressions in TL(\u2126, \u0398) is defined as before, where now free(aggr F xj (\u03d5)) := free(\u03d5) \\ {x j }, and where free(aggr F xj \u03d5(x j ) | E(x i , x j )) := {x i } (recall that free(\u03d5(x j )) = {x j } in conditional aggregation). Moreover, summation depth is replaced by the notion of aggregation depth, agd(\u03d5), defined in the same way as summation depth except that agd(aggr F xj (\u03d5)) := agd(\u03d5) + 1 and agd(aggr F xj (\u03d5(x j ) | E(x i , x j )) := agd(\u03d5) + 1. Similarly, the fragments TL k (\u2126, \u0398) and its aggregation depth restricted fragment TL (t) k (\u2126, \u0398) are defined as before, using aggregation depth rather than summation depth.\nFor the guarded fragment, GTL(\u2126, \u0398), expressions are now restricted such that aggregations must occur only in the form aggr\nF xj (\u03d5(x j ) | E(x i , x j )), for i, j \u2208 [2].\nIn other words, aggregation only happens on multisets of values obtained from neighboring vertices.\nWe now argue that our upper bound results on the separation power remain valid for the extension TL(\u2126, \u0398) of TL(\u2126) with arbitrary aggregation functions \u0398.\nProposition C.6. We have the following inclusions: For any t \u2265 0, any collection \u2126 of functions and any collection \u0398 of aggregation functions:\n\u2022 \u03c1 1 cr (t) \u2286 \u03c1 1 GTL (t) (\u2126, \u0398) ; \u2022 \u03c1 1 vwl (t) k \u2286 \u03c1 1 TL (t)\nk+1 (\u2126, \u0398) ; and\n\u2022 \u03c1 0 gwl (t) k \u2286 \u03c1 0 TL (t+1) k+1 (\u2126, \u0398) .\nProof. It suffices to show that Proposition C.3 also holds for expressions in the fragments of TL(\u2126, \u0398) considered. In particular, we only need to revise the case of summation aggregation (that is, \u03d5 := xi \u03d5 1 ) in the proof of Proposition C.3. Indeed, let us consider the more general case when one of the two aggregating functions are used.\n\u2022 \u03d5 := aggr F xi (\u03d5 1 ). We then defin\u1ebd }} .\n\u2022 \u03d5 := aggr F xi (\u03d5 1 (x i ) | E(x j , x i ))\n. We then defin\u1ebd\n\u03d5 c := \u2113\u2208N (m1,...,m \u2113 )\u2208N \u2113 (c,c1,...,c \u2113 )\u2208C(m1,...,m \u2113 ,F ) \u2203 =m x i E(x j , x i ) \u2227 \u2113 s=1 \u2203 =ms x i E(x j , x i ) \u2227\u03c6 cs 1 (x i )\nwhere ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "It is readily verified that", "text": "[[aggr F xi (\u03d5), v]] G = c iff [[\u03c6 c , v]] B G = \u22a4, and [[aggr F xi (\u03d5(x i ) | E(x j , x i )), v]] G = c iff [[\u03c6 c , v]] B\nG = \u22a4, as desired. For the guarded case, we note that the expression\u03c6 c above yields a guarded expression as long conditional aggregation is used of the form aggr F xi (\u03d5(x i ) | E(x j , x i )) with i, j \u2208 [2], so we can reuse the argument in the proof of Proposition C.3 for the guarded case.\nWe will illustrate later on (Section D) that this generalization allows for assessing the separation power of GNNs that use a variety of aggregation functions.\nThe choice of supported aggregation functions has, of course, an impact on the ability of TL(\u2126, \u0398) to match color refinement or the k-WL procedures in separation power. The same holds for GNNs, as shown by Xu et al. (2019). And indeed, the proof of Proposition C.5 relies on the presence of summation aggregation. We note that most lower bounds on the separation power of GNNs in terms of color refinement or the k-WL procedures assume summation aggregation since summation suffices to construct injective sum-decomposable functions on multisets (Xu et al., 2019;Zaheer et al., 2017), which are used to simulate color refinement and k-WL. A more in-depth analysis of lower bounding GNNs with less expressive aggregation functions, possibly using weaker versions of color refinement and k-WL is left as future work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.6 GENERALIZATION TO GRAPHS WITH REAL-VALUED VERTEX LABELS", "text": "We next consider the more general setting in which col G : V G \u2192 R \u2113 for some \u2113 \u2208 N. That is, vertices in a graph can carry real-valued vectors. We remark that no changes to neither the syntax nor the semantics of TL expressions are needed, yet note that\n[[P s (x), \u03bd]] G := col G (\u03bd) s is now an element in R rather than 0 or 1, for each s \u2208 [\u2113].\nA first observation is that the color refinement and k-WL procedures treat each real value as a separate label. That is, two values that differ only by any small \u01eb > 0, are considered different. The proofs of Theorem 4.1, 4.2, 4.3 and 4.4 rely on connections between color refinement and k-WL and the finite variable logics GC and C k+1 , respectively. In the discrete context, the unary predicates P s (x) used in the logical formulas indicate which label vertices have. That is,\n[[P s , v]] B G = \u22a4 iff col G (v) s = 1.\nTo accommodate for real values in the context of separation power, these logics now need to be able to differentiate between different labels, that is, different real numbers. We therefore extend the unary predicates allowed in formulas. More precisely, for each dimension s \u2208 [\u2113], we now have uncountably many predicates of the form P s,r , one for each r \u2208 R. In any formula in GC or C k+1 only a finite number of such predicates may occur. The Boolean semantics of these new predicates is as expected:\n[[P s,r (x), \u03bd]] B G := if col G (\u00b5(x i )) s = r then \u22a4 else \u22a5.\nIn other words, in our logics, we can now detect which real-valued labels vertices have. Although, in general, the introduction of infinite predicates may cause problems, we here consider a specific setting in which the vertices in a graph have a unique label. This is commonly assumed in graph learning. Given this, it is easily verified that all results in Section C.2 carry over, where all logics involved now use the unary predicates P s,r with s \u2208 [\u2113] and r \u2208 R.\nThe connection between TL and logics also carries over. First, for Proposition C.3 we now need to connect TL expressions, that use a finite number of predicates P s , for s \u2208 [\u2113], with the extended logics having uncountably many predicates P s,r , for s \u2208 [\u2113] and r \u2208 R, at their disposal. It suffices to reconsider the case \u03d5(x i ) = P s (x i ) in the proof of Proposition C.3. More precisely, [[P s (x i ), \u03bd]] G can now be an arbitrary value c \u2208 R. We now simply define\u03c6 c (x i ) := P s,c (x i ). By definition\n[[P s (x i ), \u03bd]] G = c if and only if [[P s,c (x i ), \u03bd]] B\nG = \u22a4, as desired. The proof for the extended version of proposition C.5 now needs a slightly different strategy, where we build the relevant TL formula after we construct the contrapositive of the Proposition. Let us first show how to construct a TL formula that is equivalent to a logical formula on any graph using only labels in a specific (finite) set R of real numbers.\nIn other words, given a set R of real values, we show that for any formula \u03d5(x) \u2208 C k,(t) using unary predicates P s,r such that r \u2208 R, we can construct the desired\u03c6. As mentioned, we only need to reconsider the case \u03d5(x i ) := P s,r (x i ). We defin\u00ea\n\u03d5 := 1 r \u2032 \u2208R,r =r \u2032 r \u2212 r \u2032 r \u2032 \u2208R,r =r \u2032 (P s (x i ) \u2212 r \u2032 1 xi=xi ). Then, [[\u03c6, \u03bd]] G evaluates to r \u2032 \u2208R,r =r \u2032 (r \u2212 r \u2032 ) r \u2032 \u2208R,r =r \u2032 (r \u2212 r \u2032 ) = 1 [[P s,r , \u03bd]] = \u22a4 0 [[P s,r , \u03bd]] = \u22a5 .\nIndeed, if [[P s,r , \u03bd]] = \u22a4, then col G (v) s = r and hence [[P s , v]] G = r, resulting in the same nominator and denominator in the above fraction. If [[P s,r , \u03bd]] = \u22a5, then col G (v) s = r \u2032 for some value r \u2032 \u2208 R with r = r \u2032 . In this case, the nominator in the above fraction becomes zero. We remark that this revised construction still results in a guarded TL expression, when the input logical formula is guarded as well.\nComing back to the proof of the extended version of Proposition C.5, let us show the proof for the the fact that \u03c1 1 GTL (t) (\u2126) \u2286 \u03c1 1 cr (t) , the other two items being analogous. Assume that there is a pair (G, v) and (H, w) which is not in \u03c1 1 cr (t) . Then, by Proposition C.1, applied on graphs with real-valued labels, there exists a formula \u03d5(x) in GC (t) such that [\n[\u03d5, v]] B G = \u22a4 = [[\u03d5, w]] B H = \u22a5.\nWe remark that \u03d5(x) uses finitely many P s,r predicates. Let R be the set of real values used in both G and H (and \u03d5(x)). We note that R is finite. We invoke the construction sketched above, and obtain a formula\u03c6 in GTL (t) such that [[\u03c6, v]\n] G = [[\u03c6, w]] H . Hence, (G, v) and (H, w) is not in \u03c1 1 GTL (t) (\u2126)\neither, for any \u2126, which was to be shown.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D DETAILS OF SECTION 5", "text": "We here provide some additional details on the encoding of layers of GNNs in our tensor languages, and how, as a consequence of our results from Section 4, one obtains a bound on their separation power. This section showcases that it is relatively straightforward to represent GNNs in our tensor languages. Indeed, often, a direct translation of the layers, as defined in the literature, suffices.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.1 COLOR REFINEMENT", "text": "We start with GNN architectures related to color refinement, or in other words, architectures which can be represented in our guarded tensor language.\nGraphSage. We first consider a \"basic\" GNN, that is, an instance of GraphSage (Hamilton et al., 2017) in which sum aggregation is used. The initial features are given by F (0) = (f (0) 1 , . . . , f (0) d0 ) where f (0) i \u2208 R n\u00d71 is a hot-one encoding of the ith vertex label in G. We can represent the initial embedding easily in GTL (0) , without the use of any summation. Indeed, it suffices to define \u03d5 (0) i (x 1 ) :\n= P i (x 1 ) for i \u2208 [d 0 ]. We have F (0) vj = [[\u03d5 (0) j , v]] G for j \u2208 [d 0 ]\n, and thus the initial features can be represented by simple expressions in GTL (0) . Assume now, by induction, that we can also represent the features computed by a basic GNN in layer t \u2212 1. That is, let F (t\u22121) \u2208 R n\u00d7dt\u22121 be those features and for each i\n\u2208 [d t\u22121 ] let \u03d5 (t\u22121) i (x 1 ) be expressions in GTL (t\u22121) (\u03c3) representing them. We assume that, for each i \u2208 [d t\u22121 ], F (t\u22121) vi = [[\u03d5 (t\u22121) i , v]] G .\nWe remark that we assume that a summation depth of t \u2212 1 is needed for layer t \u2212 1.\nThen, in layer t, a basic GNN computes the next features as\nF (t) := \u03c3 F (t\u22121) \u2022 V (t) + A \u2022 F (t\u22121) \u2022 W (t) + B (t) ,\nwhere A \u2208 R n\u00d7n is the adjacency matrix of G, V (t) and W (t) are weight matrices in R dt\u22121\u00d7dt , B (t) \u2208 R n\u00d7dt is a (constant) bias matrix consist of n copies of b (t) \u2208 R dt , and \u03c3 is some activation function. We can simply use the following expressions\n\u03d5 (t) j (x 1 ), for j \u2208 [d t ]: \u03c3 \uf8eb \uf8ed dt\u22121 i=1 V (t) ij \u2022 \u03d5 (t\u22121) i (x 1 ) + x2 E(x 1 , x 2 ) \u2022 dt\u22121 i=1 W (t) ij \u2022 \u03d5 (t\u22121) i (x 2 ) + b (t) j \u2022 1 x1=x1 \uf8f6 \uf8f8 .\nHere, t) ij and b (t) j are real values corresponding the weight matrices and bias vector in layer t. These are expressions in GTL (t) (\u03c3) since the additional summation is guarded, and combined with the summation depth of t \u2212 1 of \u03d5 (t\u22121) i , this results in a summation depth of t for layer t. Furthermore, F (t) vi = [[\u03d5 (t) i , v]] G , as desired. If we denote by bGNN (t) the class of t-layered basic GNNs, then our results imply \u03c1 1 cr (t) \u2286 \u03c1 1 ( GTL (t) (\u2126) \u2286 \u03c1 1 bGNN (t) , and thus the separation power of basic GNNs is bounded by the separation power of color refinement. We thus recover known results by Xu et al. (2019) and Morris et al. (2019). Furthermore, if one uses a readout layer in basic GNNs to obtain a graph embedding, one typically applies a function ro : R dt \u2192 R dt in the form of ro v\u2208VG F (t) v , in which aggregation takes places over all vertices of the graph. This corresponds to an expression in TL (t+1) 2 (\u03c3, ro):\nW (t) ij , V(\n\u03d5 j := ro j x1 \u03d5 (t\u22121) j (x 1 )\n, where ro j is the projection of the readout function on the jthe coordinate. We note that this is indeed not a guarded expression anymore, and thus our results tell that\n\u03c1 0 gcr (t) \u2286 \u03c1 0 (TL (t+1) 2 (\u2126) \u2286 \u03c1 0 bGNN (t) + readout .\nMore generally, GraphSage allows for the use of general aggregation functions F on the multiset of features of neighboring vertices. To cast the corresponding layers in TL(\u2126), we need to consider the extension TL(\u2126, \u0398) with an appropriate set \u0398 of aggregation functions, as described in Section C.5. In this way, we can represent layer t by means of the following expressions\n\u03d5 (t) j (x 1 ), for j \u2208 [d t ]. \u03c3 \uf8eb \uf8ed dt\u22121 i=1 V (t) ij \u2022 \u03d5 (t\u22121) i (x 1 ) + dt\u22121 i=1 W (t) ij \u2022 aggr F x2 \u03d5 (t\u22121) i (x 2 ) | E(x 1 , x 2 ) + b (t) j \u2022 1 x1=x1 \uf8f6 \uf8f8 ,\nwhich is now an expression in GTL (t) ({\u03c3}, \u0398) and hence the bound in terms of t iterations of color refinement carries over by Proposition C.6. Here, \u0398 simply consists of the aggregation functions used in the layers in GraphSage.\nGCNs. Graph Convolution Networks (GCNs) (Kipf & Welling, 2017) operate alike basic GNNs except that a normalized Laplacian D \u22121/2 (I + A)D \u22121/2 is used to aggregate features, instead of the adjacency matrix A. Here, D \u22121/2 is the diagonal matrix consisting of reciprocal of the square root of the vertex degrees in G plus 1. The initial embedding F (0) is just as before. We use again d t to denote the number of features in layer t. In layer t > 0, a GCN computes t) ). If, in addition to the activation function \u03c3 we add the function 1\nF (t) := \u03c3(D \u22121/2 (I + A)D \u22121/2 \u2022 F (t\u22121) W (t) + B (\n\u221a x+1 : R \u2192 R : x \u2192 1 \u221a x+1\nto \u2126, we can represent the GCN layer, as follows. For j \u2208 [d t ], we define the\nGTL (t+1) (\u03c3, 1 \u221a x+1 ) expressions \u03d5 (t) j (x 1 ) := \u03c3 f 1/ \u221a x+1 x2 E(x 1 , x 2 ) \u2022 dt\u22121 i=1 W (t) ij \u2022 \u03d5 (t\u22121) i (x 1 ) \u2022 f 1/ \u221a x+1 x2 E(x 1 , x 2 ) +f 1/ \u221a x+1 x2 E(x 1 , x 2 ) \u2022 x2 E(x 1 , x 2 )\u2022f 1/ \u221a x+1 x1 E(x 2 , x 1 ) \u2022 dt\u22121 i=1 W (t) ij \u2022\u03d5 (t\u22121) i (x 2 ) ,\nwhere we omitted the bias vector for simplicity. We again observe that only guarded summations are needed. However, we remark that in every layer we now add two the overall summation depth, since we need an extra summation to compute the degrees. In other words, a t-layered GCN correspond to expressions in\nGTL (2t) (\u03c3, 1 \u221a x+1\n). If we denote by GCN (t) the class of t-layered GCNs, then our results imply t) . We remark that another representation can be provided, in which the degree computation is factored out (Geerts et al., 2021a), resulting in a better upper bound \u03c1 1 cr (t+1) \u2286 \u03c1 1 GCN (t) . In a similar way as for basic GNNs, we also have \u03c1 0 gcr (t+1) \u2286 \u03c1 0 GCN (t) + readout .\n\u03c1 1 cr (2t) \u2286 \u03c1 1 GTL (2t) (\u2126) \u2286 \u03c1 1 GCN (\nSGCs. As an other example, we consider a variation of Simple Graph Convolutions (SGCs) (Wu et al., 2019), which use powers the adjacency matrix and only apply a non-linear activation function at the end. That is, F := \u03c3(A p \u2022 F (0) \u2022 W ) for some p \u2208 N and W \u2208 R d0\u00d7d1 . We remark that SGCs actually use powers of the normalized Laplacian, that is,\nF := \u03c3 (D \u22121/2 (I + A G )D \u22121/2 )) p \u2022F (0)\n\u2022W but this only incurs an additional summation depth as for GCNs. We focus here on our simpler version. It should be clear that we can represent the architecture in TL (p) p+1 (\u2126) by means of the expressions:\n\u03d5 (t) j (x 1 ) := \u03c3 \uf8eb \uf8ed x2 \u2022 \u2022 \u2022 xp+1 p k=1 E(x k , x k+1 ) \u2022 d0 i=1 W ij \u2022 \u03d5 (0) i (x p+1 ) \uf8f6 \uf8f8 , for j \u2208 [d 1 ]\n. A naive application of our results would imply an upper bound on their separation power by p-WL. We can, however, use Proposition 4.5. Indeed, it is readily verified that these expressions have a treewidth of one, because the variables form a path. And indeed, when for example, p = 3, we can equivalently write \u03d5 (t) j (x 1 ) as\n\u03c3 x2 E(x 1 , x 2 ) \u2022 x1 E(x 2 , x 1 ) \u2022 x2 E(x 1 , x 2 ) \u2022 d0 i=1 W ij \u2022 \u03d5 (0) i (x 2 ) ,\nby reordering the summations and reusing index variables. This holds for arbitrary p. We thus obtain guarded expressions in GTL (p) (\u03c3) and our results tell that t-layered SGCs are bounded by cr (p) for vertex embeddings, and by gcr (p) for SGCs + readout.\nPrincipal Neighbourhood Aggregation. Our next example is a GNN in which different aggregation functions are used: Principal Neighborhood Aggregation (PNA) is an architecture proposed by Corso et al. (2020) in which aggregation over neighboring vertices is done by means of mean, stdv, max and min, and this in parallel. In addition, after aggregation, three different scalers are applied. Scalers are diagonal matrices whose diagonal entries are a function of the vertex degrees. Given the features for each vertex v computed in layer t \u2212 1, that is, F (t\u22121)\nv:\n\u2208 R 1\u00d7\u2113 , a PNA computes v's new features in layer t in the following way (see layer definition (8) in (Corso et al., 2020)). First, vectors G (t) v: \u2208 R 1\u00d74\u2113 are computed such that\nG (t) vj = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 mean {{mlp j (F (t\u22121)\nw:\n) | w \u2208 N G (v)}} for 1 \u2264 j \u2264 \u2113 stdv {{mlp j (F (t\u22121) w: ) | w \u2208 N G (v)}} for \u2113 + 1 \u2264 j \u2264 2\u2113 max {{mlp j (F (t\u22121) w: ) | w \u2208 N G (v)}} for 2\u2113 + 1 \u2264 j \u2264 3\u2113 min {{mlp j (F (t\u22121) w: ) | w \u2208 N G (v)}} for 3\u2113 + 1 \u2264 j \u2264 4\u2113,\nwhere mlp j : R \u2113 \u2192 R is the projection of an MLP mlp : R \u2113 \u2192 R \u2113 on the jth coordinate. Then, three different scalers are applied. The first scaler is simply the identity, the second two scalers s 1 and s 2 depend on the vertex degrees. As such, vectors H (t) v: \u2208 R 12\u2113 are constructed as follows:\nH (t) vj = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 H (t) vj for 1 \u2264 j \u2264 4\u2113 s 1 (deg G (v)) \u2022 H (t) vj for 4\u2113 + 1 \u2264 j \u2264 8\u2113 s 2 (deg G (v)) \u2022 H (t) vj for 8\u2113 + 1 \u2264 j \u2264 12\u2113,\nwhere s 1 and s 2 are functions from R \u2192 R (see (Corso et al., 2020) for details). Finally, the new vertex embedding is obtained as (t) v: = mlp \u2032 (H (t) v: ) for some MLP mlp \u2032 : R 12\u2113 \u2192 R \u2113 . The above layer definition translates naturally into expressions in TL(\u2126, \u0398), the extension of TL(\u2126) with aggregate functions (Section C.5). Indeed, suppose that for each j \u2208 [\u2113] we have TL(\u2126, \u0398) expressions\nF\n\u03d5 (t\u22121) j (x 1 ) such that [[\u03d5 (t\u22121) j , v]] G = F (t\u22121) vj for any vertex v. Then, G (t)\nvj simply corresponds to the guarded expressions\n\u03c8 (t) j (x 1 ) := aggr mean x2 (mlp j (\u03d5 (t\u22121) 1 (x 2 ), . . . , \u03d5 (t\u22121) \u2113 (x 2 )) | E(x 1 , x 2 )),\nfor 1 \u2264 j \u2264 \u2113, and similarly for the other components of G (t) v: using the respective aggregation functions, stdv, max and min. Then, H (t) vj corresponds to\n\u03be (t) j (x 1 ) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 \u03c8 (t) j (x 1 ) for 1 \u2264 j \u2264 4\u2113 s 1 (aggr sum x2 (1 x2=x2 | E(x 1 , x 2 ))) \u2022 \u03c8 (t) j (x 1 ) for 4\u2113 + 1 \u2264 j \u2264 8\u2113 s 2 (aggr sum x2 (1 x2=x2 | E(x 1 , x 2 ))) \u2022 \u03c8 (t) j (x 1 ) for 8\u2113 + 1 \u2264 j \u2264 12\u2113\n, where we use summation aggregation to compute the degree information used in the functions in the scalers s 1 and s 2 . And finally,\n\u03d5 (t) j := mlp \u2032 j (\u03be (t) 1 (x 1 ), . . . , \u03be (t) 12\u2113 (x 1 )) represents F (t)\nvj . We see that all expressions only use two index variables and aggregation is applied in a guarded way. Furthermore, in each layer, the aggregation depth increases with one. As such, a t-layered PNA can be represented in GTL (t) (\u2126, \u0398), where \u2126 consists of the MLPs and functions used in scalers, and \u0398 consists of sum (for computing vertex degrees), and mean, stdv, max and min. Proposition C.6 then implies a bound on the separation power by cr (t) .\nOther example. In the same way, one can also easily analyze GATs (Velickovic et al., 2018) and show that these can be represented in GTL(\u2126) as well, and thus bounds by color refinement can be obtained.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.2 k-DIMENSIONAL WEISFEILER-LEMAN TESTS", "text": "We next discuss architectures related to the k-dimensional Weisfeiler-Leman algorithms. For k = 1, we discussed the extended GINs in the main paper. We here focus on arbitrary k \u2265 2.\nFolklore GNNs. We first consider the \"Folklore\" GNNs or k-FGNNs for short (Maron et al., 2019b). For k \u2265 2, k-FGNNs computes a tensors. In particular, the initial tensor\nF (0) encodes atp k (G, v) for each v \u2208 V k G .\nWe can represent this tensor by the following k 2 (\u2113 + 2) expressions in TL (0) k :\n\u03d5 (0) r,s,j (x 1 , . . . , x k ) := \uf8f1 \uf8f2 \uf8f3 1 xr=xs \u2022 P j (x r ) for j \u2208 [\u2113] E(x r , x s ) for j = \u2113 + 1 1 xr=xs for j = \u2113 + 2 , for r, s \u2208 [k] and j \u2208 [\u2113 + 2]. We note: [[\u03d5 (0) r,s,j , (v 1 , . . . , v k )]] G = F (0) v1,...,v k ,r,s,j for all (r, s, j) \u2208 [k] 2 \u00d7 [\u2113 + 2], as desired. We let \u03c4 0 := [k] 2 \u00d7 [\u2113 + 2] and set d 0 = k 2 \u00d7 (\u2113 + 2).\nThen, in layer t, a k-FGNN computes a tensor t) s (F (t\u22121) v1,...,vs\u22121,w,vs+1,...,v k ,\u2022 ) ,\nF (t) v1,...,v k ,\u2022 := mlp (t) 0 F (t\u22121) v1,...,v k ,\u2022 , w\u2208VG k s=1 mlp(\nwhere mlp (t) s : R dt\u22121 \u2192 R d \u2032 t , for s \u2208 [k], and and mlp (t) 0 : R dt\u22121\u00d7d \u2032 t \u2192 R dt are MLPs. We here use \u2022 to denote combinations of indices in \u03c4 d for F (t) and in \u03c4 d\u22121 for F (t\u22121) .\nLet F (t\u22121) \u2208 R n k \u00d7dt\u22121 be the tensor computed by an k-FGNN in layer t \u2212 1. Assume that for each tuple of elements j in \u03c4 dt\u22121 we have an expression\n\u03d5 (t\u22121) j (x 1 , . . . , x k ) satisfying [[\u03d5 (t\u22121) j , (v 1 , . . . , v k )]] G = F (t\u22121)\nv1,...,v k ,j and such that it is an expression in TL (t\u22121) k+1 (\u2126). That is, we need k + 1 index variables and a summation depth of t \u2212 1 to represent layer t \u2212 1.\nThen, for layer t, for each j \u2208 \u03c4 dt , it suffices to consider the expression\n\u03d5 (t) j (x 1 , . . . , x k ) := mlp (t) 0,j \u03d5 (t\u22121) i (x 1 , . . . , x k ) i\u2208\u03c4 d t\u22121 , x k+1 k s=1 mlp (t) s,j (\u03d5 (t\u22121) i (x 1 , . . . , x s\u22121 , x k+1 , x s+1 , . . . , x k ) i\u2208\u03c4 d t\u22121 ,\nwhere mlp (t) o,j and mlp (t) s,j are the projections of the MLPs on the j-coordinates. We remark that we need k + 1 index variables, and one extra summation is needed. We thus obtain expressions in TL (t) k+1 (\u2126) for the tth layer, as desired. We remark that the expressions are simple translations of the defining layer definitions. Also, in this case, \u2126 consists of all MLPs. When a k-FGNN is used for vertex embeddings, we now simply add to each expression a factor k s=1 1 x1=xs . As an immediate consequence of our results, if we denote by k-FGNN (t) the class of t-layered k-FGNNs, then for vertex embeddings:\n\u03c1 1 vwl (t) k \u2286 \u03c1 1 TL (t) k+1 (\u2126) \u2286 \u03c1 1 k-FGNN (t)\nin accordance with the known results from Azizian & Lelarge (2021). When used for graph embeddings, an aggregation layer over all k-tuples of vertices is added, followed by the application of an MLP. This results in expressions with no free index variables, and of summation depth t + k, where the increase with k stems from the aggregation process over all k-tuples. In view of our results, for graph embeddings:\n\u03c1 0 gwl (\u221e) k \u2286 \u03c1 0 TL k+1 (\u2126) \u2286 \u03c1 0 k-FGNN\nin accordance again with Azizian & Lelarge (2021). We here emphasize that the upper bounds in terms of k-WL are obtained without the need to know how k-WL works. Indeed, one can really just focus on casting layers in the right tensor language!\nWe remark that Azizian & Lelarge (2021) define vertex embedding k-FGNNs in a different way. Indeed, for a vertex v, its embedding is obtained by aggregating of all (k \u2212 1) tuples in the remaining coordinates of the tensors. They define vwl k accordingly. From the tensor language point of view, this corresponds to the addition of k \u2212 1 to the summation depth. Our results indicate that we loose the connection between rounds and layers, as in Azizian & Lelarge (2021). This is the reason why we defined vertex embedding k-FGNNs in a different way and can ensure a correspondence between rounds and layers for vertex embeddings. ", "publication_ref": ["b2", "b2", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "D.3 AUGMENTED GNNS", "text": "Higher-order GNN architectures such as k-GNNs, k-FGNNs and k-IGNs, incur a substantial cost in terms of memory and computation (Morris et al., 2020). Some recent proposals infuse more efficient GNNs with higher-order information by means of some pre-processing step. We next show that the tensor language approach also enables to obtain upper bounds on the separation power of such \"augmented\" GNNs.\nWe first consider F -MPNNs (Barcel\u00f3 et al., 2021) in which the initial vertex features are augmented with homomorphism counts of rooted graph patterns. More precisely, let P r be a connected rooted graph (with root vertex r), and consider a graph G = (V G , E G , col G ) and vertex v \u2208 V G . Then, hom(P r , G v ) denotes the number of homomorphism from P to G, mapping r to v. We recall that a homomorphism is an edge-preserving mapping between vertex sets. Given a collection F = {P r 1 , . . . , P r \u2113 } of rooted patterns, an F -MPNN runs an MPNN on the augmented initial vertex features:F (0) v: := (F (0) v: , hom(P r 1 , G v ), . . . , hom(P r \u2113 , G v )). Now, take any GNN architecture that can be cast in GTL(\u2126) or TL 2 (\u2126) and assume, for simplicity of exposition, that a t-layer GNN corresponds to expressions in GTL (t) (\u2126) or TL (t) 2 (\u2126). In order to analyze the impact of the augmented features, one only needs to revise the expressions \u03d5 (0) j (x 1 ) that represent the initial features. In the absence of graph patterns, \u03d5 (0) j (x 1 ) := P j (x 1 ), as we have seen before. By contrast, to representF (0) vj we need to cast the computation of hom(P r i , G v ) in TL. Assume that the graph pattern P i consists of p vertices and let us identify the vertex set with [p]. Furthermore, without of loss generality, we assume that vertex \"1\" is the root vertex in P i . To obtain hom(P r i , G v ) we need to create an indicator function for the graph pattern P i and then count how many times this indicator value is equal to one in G. The indicator function for P i is simply given by the expression uv\u2208EP i E(x u , x v ). Then, counting just pours down to summation over all index variables except the one for the root vertex. More precisely, if we define\n\u03d5 Pi (x 1 ) := x2 \u2022 \u2022 \u2022 xp uv\u2208EP i E(x u , x v ), then [[\u03d5 Pi , v]] G = hom(P r i , G v\n). This encoding results in an expression in TL p . However, it is well-known that we can equivalently write \u03d5 Pi (x 1 ) as an expression\u03c6 Pi (x 1 ) in TL k+1 where k is the treewidth of the graph P i . As such, our results imply that F -MPNNs are bounded in separation power by k-WL where k is the maximal treewidth of graphs in F . We thus recover the known upper bound as given in Barcel\u00f3 et al. (2021) using our tensor language approach.\nAnother example of augmented GNN architectures are the Graph Substructure Networks (GSNs) (Bouritsas et al., 2020). By contrast to F -MPNNs, subgraph isomorphism counts rather than homomorphism counts are used to augment the initial features. At the core of a GSN thus lies the computation of sub(P r , G v ), the number of subgraphs H in G isomorphic to P (and such that the isomorphisms map r to v). In a similar way as for homomorphisms counts, we can either directly cast the computation of sub(P r , G v ) in TL resulting again in the use of p index variables. A possible reduction in terms of index variables, however, can be obtained by relying on the result (Theorem 1.1.) by Curticapean et al. (2017) in which it shown that sub(P r , G v ) can be computed in terms of homomorphism counts of graph patterns derived from P r . More precisely, Curticapean et al. (2017) define spasm(P r ) as the set of graphs consisting of all possible homomorphic images of P r . It is then readily verified that if the maximal treewidth of the graphs in spasm(P r ) is k, then sub(P r , G v ) can be cast as an expression in TL k+1 . Hence, GSNs using a pattern collection F can be represented in TL k+1 , where k is the maximal treewidth of graphs in any of the spams of patterns in F , and thus are bounded in separation power k-WL in accordance to the results by Barcel\u00f3 et al. (2021).\nAs a final example, we consider the recently introduced Message Passing Simplicial Networks (MPSNs) (Bodnar et al., 2021). In a nutshell, MPSNs are run on simplicial complexes of graphs instead of on the original graphs. We sketch how our tensor language approach can be used to assess the separation power of MPSNs on clique complexes. We use the simplified version of MPSNs which have the same expressive power as the full version of MPSNs (Theorem 6 in Bodnar et al. ( 2021)).\nWe recall some definitions. Let Cliques(G) denote the set of all cliques in G. Given two cliques c and\nc \u2032 in Cliques(G), define c \u227a c \u2032 if c \u2282 c \u2032 and there exists no c \u2032\u2032 in Cliques(G), such that c \u2282 c \u2032\u2032 \u2282 c \u2032 . We define Boundary(c, G) := {c \u2032 \u2208 Cliques(G) | c \u2032 \u227a c} and Upper(c, G) := {c \u2032 \u2208 Cliques(G) | \u2203c \u2032\u2032 \u2208 Cliques(G), c \u2032 \u227a c \u2032\u2032 and c \u227a c \u2032\u2032 }.\nFor each c in Cliques(G) we have an initial feature vector F (0) c: \u2208 R 1\u00d7\u2113 . Bodnar et al. ( 2021) initialize all initial features with the same value. Then, in layer t, for each c \u2208 Cliques(G), features are updated as follows:\nG (t) c: = F B ({{mlp B (F (t\u22121) c:\n,\nF (t\u22121)\nc \u2032 :\n) | c \u2032 \u2208 Boundary(G, c)}}) H (t) c: = F U ({{mlp U (F (t\u22121) c:\n,\nF (t\u22121) c \u2032 : , F (t\u22121) c\u222ac \u2032 : ) | c \u2032 \u2208 Upper(G, c)}}) F (t) c: = mlp(F (t\u22121) c:\n, G (t) c: , H (t) c: ), where F B and F U are aggregation functions and mlp B , mlp U and mlp are MLPs. With some effort, one can represent these computations by expressions in TL p (\u2126, \u0398) where p is largest clique in G. As such, the separation power of clique-complex MPSNs on graphs of clique size at most p is bounded by p \u2212 1-WL. And indeed, Bodnar et al. ( 2021) consider Rook's 4 \u00d7 4 graph, which contains a 4-clique, and the Shirkhande graph, which does not contain a 4-clique. As such, the analysis above implies that clique-complex MPSNs are bounded by 2-WL on the Shrikhande graph, and by 3-WL on Rook's graph, consistent with the observation in Bodnar et al. (2021). A more detailed analysis of MPSNs in terms of summation depth and for other simplicial complexes is left as future work.\nThis illustrates again that our approach can be used to assess the separation power of a variety of GNN architectures in terms of k-WL, by simply writing them as tensor language expressions. Furthermore, bounds in terms of k-WL can be used for augmented GNNs which form a more efficient way of incorporating higher-order graph structural information than higher-order GNNs.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.4 SPECTRAL GNNS", "text": "In general, spectral GNNs are defined in terms of eigenvectors and eigenvalues of the (normalized) graph Laplacian (Bruna et al., 2014;Defferrard et al., 2016;Levie et al., 2019;Balcilar et al., 2021b)). The diagonalization of the graph Laplacian is, however, avoided in practice, due to its excessive cost. Instead, by relying on approximation results in spectral graph analysis (Hammond et al., 2011), the layers of practical spectral GNNs are defined in term propagation matrices consisting of functions, which operate directly on the graph Laplacian. This viewpoint allows for a spectral analysis of spectral and \"spatial\" GNNs in a uniform way, as shown by Balcilar et al. (2021b). In this section, we consider two specific instances of spectral GNNs: ChebNet (Defferrard et al., 2016) and CayleyNet (Levie et al., 2019), and assess their separation power in terms of tensor logic. Our general results then provide bounds on their separation power in terms color refinement and 2-WL, respectively. Chebnet. The separation power of ChebNet (Defferrard et al., 2016) was already analyzed in Balcilar et al. (2021a) by representing them in the MATLANG matrix query language (Brijder et al., 2019). It was shown (Theorem 2 (Balcilar et al., 2021a)) that it is only the maximal eigenvalue \u03bb max of the graph Laplacian used in the layers of ChebNet that may result in the separation power of ChebNet to go beyond 1-WL. We here revisit and refine this result by showing that, when ignoring the use of \u03bb max , the separation power of Chebnet is bounded already by color refinement (which, as mentioned in Section 2, is weaker than 1-WL for vertex embeddings). In a nutshell, the layers of a ChebNet are defined in terms of Chebyshev polynomials of the normalized Laplacian\nL norm = I \u2212 D \u22121/2 \u2022 A \u2022 D \u22121/2\nand these polynomials can be easily represented in GTL(\u2126). One can alternatively use the graph Laplacian L = D \u2212 A in a ChebNet, which allows for a similar analysis. The distinction between the choice of L norm and L only shows in the needed summation depth (in as in similar way as for the GCNs described earlier). We only consider the normalized Laplacian here.\nMore precisely, following Balcilar et al. (2021a;b), in layer t, vertex embeddings are updated in a ChebNet according to:\nF (t) := \u03c3 p s=1 C (s) \u2022 F (t\u22121) \u2022 W (t\u22121,s) , with C (1) := I, C (2) = 2 \u03bb max L norm \u2212 I, C (s) = 2C (2) \u2022 C (s\u22121) \u2212 C (s\u22122) , for s \u2265 3,\nand where \u03bb max denotes the maximum eigenvalue of L norm . We next use a similar analysis as in Balcilar et al. (2021a). That is, we ignore for the moment the maximal eigenvalue \u03bb max and redefine C (2) as cL norm \u2212 I for some constant c. We thus see that each C (s) is a polynomial of the form p s (c, L norm ) :\n= qs i=0 a (s) i (c) \u2022 (L norm ) i with scalar functions a(s)\ni : R \u2192 R and where we interpret (L norm ) 0 = I. To upper bound the separation power using our tensor language approach, we can thus shift our attention entirely to representing\n(L norm ) i \u2022 F (t\u22121) \u2022 W (t\u22121,s) for powers i \u2208 N. Furthermore, since (L norm ) i is again a polynomial of the form q i (D \u22121/2 \u2022 A \u2022 D \u22121/2 ) := ri j=0 b ij \u2022 (D \u22121/2 \u2022 A \u2022 D \u22121/2\n) j , we can further narrow down the problem to represent s) in GTL(\u2126), for powers j \u2208 N. And indeed, combining our analysis for GCNs and SGCs results in expressions in GTL(\u2126). As an example let us consider (D\n(D \u22121/2 \u2022 A \u2022 D \u22121/2 ) j \u2022 F (t\u22121) \u2022 W (t\u22121,\n\u22121/2 \u2022 A \u2022 D \u22121/2 ) 2 \u2022 F (t\u22121) \u2022 W (t\u22121)\n, that is we use a power of two. It then suffices to define, for each output dimension j, the expressions:\n\u03c8 2 j (x 1 ) = f 1/ \u221a x x2 E(x 1 , x 2 ) \u2022 x2 E(x 1 , x 2 ) \u2022 f 1/x x1 (E(x 2 , x 1 ) \u2022 x1 E(x 2 , x 1 ) \u2022 f 1/ \u221a x ( x2 E(x 1 , x 2 )) \u2022 dt\u22121 i=1 W (t\u22121) ij \u03d5 (t\u22121) i (x 1 ) ,\nwhere the \u03d5 (t\u22121) i (x 1 ) are expressions representing layer t \u2212 1. It is then readily verified that we can use \u03c8 2 j (x 1 ) to cast layer t of a ChebNet in GTL(\u2126) with \u2126 consisting of f 1/\n\u221a x : R \u2192 R : x \u2192 1 \u221a x , f 1/x : R \u2192 R : x \u2192 1\nx , and the used activation function \u03c3. We thus recover (and slightly refine) Theorem 2 in Balcilar et al. (2021a):\nCorollary D.1. On graphs sharing the same \u03bb max values, the separation power of ChebNet is bounded by color refinement, both for graph and vertex embeddings.\nA more fine-grained analysis of the expressions is needed when interested in bounding the summation depth and thus of the number of rounds needed for color refinement. Moreover, as shown by Balcilar et al. (2021a), when graphs have non-regular components with different \u03bb max values, ChebNet can distinguish them, whilst 1-WL cannot. To our knowledge, \u03bb max cannot be computed in TL k (\u2126) for any k. This implies that it not clear whether an upper bound on the separation power can be obtained for ChebNet taking \u03bb max into account. It is an interesting open question whether there are two graphs G and H which cannot be distinguished by k-WL but can be distinguished based on \u03bb max . A positive answer would imply that the computation of \u03bb max is beyond reach for TL(\u2126) and other techniques are needed.\nCayleyNet. We next show how the separation power of CayleyNet (Levie et al., 2019) can be analyzed. To our knowledge, this analysis is new. We show that the separation power of CayleyNet is bounded by 2-WL. Following Levie et al. (2019) and Balcilar et al. (2021b), in each layer t, a CayleyNet updates features as follows:\nF (t) := \u03c3 p s=1 C (s) \u2022 F (t\u22121) W (t\u22121,s) , with C (1) := I, C (2s) := Re hL \u2212 \u0131I hL + \u0131I s , C (2s+1) := Re \u0131 hL \u2212 \u0131I hL + \u0131I s ,\nwhere h is a constant, \u0131 is the imaginary unit, and Re : C \u2192 C maps a complex number to its real part. We immediately observe that a CayleyNet requires the use of complex numbers and matrix inversion. So far, we considered real numbers only, but when our separation results are concerned, the choice between real or complex numbers is insignificant. In fact, only the proof of Proposition C.3 requires a minor modification when working on complex numbers: the infinite disjunctions used in the proof now need to range over complex numbers. For matrix inversion, when dealing with separation power, one can use different expressions in TL(\u2126) for computing the matrix inverse, depending on the input size. And indeed, it is well-known (see e.g., Csanky (1976)) that based on the characteristic polynomial of A, A \u22121 for any matrix A \u2208 R n\u00d7n can be computed as a polynomial \u22121 cn n\u22121 i=1 c i A n\u22121\u2212i if c n = 0 and where each coefficient c i is a polynomial in tr(A j ), for various j. Here, tr(\u2022) is the trace of a matrix. As a consequence, layers in CayleyNet can be viewed as polynomials in hL \u2212 \u0131I with coefficients polynomials in tr((hL \u2212 \u0131I) j ). One now needs three index variables to represent the trace computations tr((hL \u2212 \u0131I) j ). Indeed, let \u03d5 0 (x 1 , x 2 ) be the TL 2 expression representing hL \u2212 \u0131I. Then, for example, (hL \u2212 \u0131I) j can be computed in TL 3 using \u03d5 j (x 1 , x 2 ) :\n= x3 \u03d5 0 (x 1 , x 3 ) \u2022 \u03d5 j\u22121 (x 3 , x 2 )\nand hence tr((hL\u2212\u0131I) j ) is represented by x1 x2 \u03d5 j (x 1 , x 2 )\u20221 x1=x2 .. In other words, we obtain expressions in TL 3 . The polynomials in hL \u2212 \u0131I can be represented in TL 2 just as for ChebNet. This implies that each layer in CayleyNet can be represented, on graphs of fixed size, by TL 3 (\u2126) expressions, where \u2126 includes the activation function \u03c3 and the function Re. This suffices to use our general results and conclude that CayleyNets are bounded in separation power by 2-WL. An interesting question is to find graphs that can be separated by a CayleyNet but not by 1-WL. We leave this as an open problem.\nE PROOF OF THEOREM 5.1\nWe here consider another higher-order GNN proposal: the invariant graph networks or k-IGNs of Maron et al. (2019b). By contrast to k-FGNNs, k-IGNs are linear architectures. If we denote by k-IGN (t) the class of t layered k-IGNs, then following inclusions are known (Maron et al., 2019b)\n\u03c1 1 k-IGN (t) \u2286 \u03c1 1 vwl (t) k\u22121 and \u03c1 0 k-IGN \u2286 \u03c1 0 gwl (\u221e) k\u22121 .\nThe reverse inclusions were posed as open problems in Maron et al. (2019a) and were shown to hold by Chen et al. (2020) for k = 2, by means of an extensive case analysis and by relying on properties of 1-WL. In this section, we show that the separation power of k-IGNs is bounded by that of (k \u2212 1)-WL, for arbitrary k \u2265 2. Theorem 4.2 tells that we can entirely shift our attention to showing that the layers of k-IGNs can be represented in TL k (\u2126). In other words, we only need to show that k index variables are needed for the layers. As we will see below, this requires a bit of work since a naive representation of the layers of k-IGNs use 2k index variables. Nevertheless, we show that this can be reduced to k index variables only.\nBy inspecting the expressions needed to represent the layers of k-IGNs in TL k (\u2126), we obtain that a t layer k-IGN (t) require expressions of summation depth of tk. In other words, the correspondence between layers and summation depth is precisely in sync. This implies, by Theorem 4.2:\n\u03c1 1 k-IGN = \u03c1 1 vwl (\u221e) k\u22121 ,\nwhere we ignore the number of layers. We similarly obtain that \u03c1 0 k-IGN = \u03c1 0 gwl (\u221e) k\u22121 , hereby answering the open problem posed in Maron et al. (2019a). Finally, we observe that the k-IGNs used in Maron et al. (2019b) to show the inclusion \u03c1 1 k-IGN (t) \u2286 \u03c1 1 vwl (t) k\u22121 are of very simple form. By defining a simple class of k-IGNs, denoted by k-GINs, we obtain \u03c1 1 k-GIN (t) = \u03c1 1 vwl (t) k\u22121 , hereby recovering the layer/round connections.\nWe start with the following lemma:\nLemma E.1. For any k \u2265 2, a t layer k-IGNs can be represented in TL (tk) k (\u2126).\nBefore proving this lemma, we recall k-IGNs. These are architectures that consist of linear equivariant layers. Such linear layers allow for an explicit description. Indeed, following Maron et al. (2019c), let \u223c \u2113 be the equality pattern equivalence relation on\n[n] \u2113 such that for a, b \u2208 [n] \u2113 , a \u223c \u2113 b if and only if a i = a j \u21d4 b i = b j for all j \u2208 [\u2113]\n. We denote by [n] \u2113 / \u223c \u2113 the equivalence classes induced by \u223c \u2113 . Let us denote by F (t\u22121) \u2208 R n k \u00d7dt\u22121 the tensor computed by an k-IGN in layer t \u2212 1.\nThen, in layer t, a new tensor in R n k \u00d7dt is computed, as follows. For j \u2208 [d t ] and v 1 , . . . , v k \u2208 [n] k :\nF (t) v1,...,v k ,j := \u03c3 \u03b3\u2208[n] 2k /\u223c 2k w\u2208[n] k 1 (v,w)\u2208\u03b3 i\u2208[dt\u22121]\nc \u03b3,i,j F (t\u22121) w1,...,w k ,i +\n\u00b5\u2208[n] k /\u223c k 1 v\u2208\u00b5 b \u00b5,j(1)\nfor activation function \u03c3, constants c \u03b3,i,j and b \u00b5,j in R and where 1 (v,w)\u2208\u03b3 and 1 v\u2208\u00b5 are indicator functions for the 2k-tuple (v, w) to be in the equivalence class \u03b3 \u2208 [n] 2k / \u223c 2k and the k-tuple v to be in class \u00b5 \u2208 [n] k / \u223c k . As initial tensor F (0) one defines F (0) v1,...,v k ,j := atp k (G, v) \u2208 R d0 , with d 0 = 2( k 2 ) + k\u2113 where \u2113 is the number of initial vertex labels, just as for k-FGNNs. We remark that the need for having a summation depth of tk in the expressions in TL k (\u2126), or equivalently for requiring tk rounds of (k \u2212 1)-WL, can intuitively be explained that each layer of a k-IGN aggregates more information from \"neighbouring\" k-tuples than (k \u2212 1)-WL does. Indeed, in each layer, an k-IGN can use previous tuple embeddings of all possible k-tuples. In a single round of (k \u2212 1)-WL only previous tuple embeddings from specific sets of k-tuples are used. It is only after an additional k \u2212 1 rounds, that k-WL gets to the information about arbitrary k-tuples, whereas this information is available in a k-IGN in one layer directly.\nProof of Lemma E.1. We have seen how F (0) can be represented in TL k (\u2126) when dealing with k-FGNNs. We assume now that also the t \u2212 1th layer F (t\u22121) can be represented by d t\u22121 expressions in TL\n((t\u22121)k) k\n(\u2126) and show that the same holds for the tth layer.\nWe first represent F (t) in TL 2k (\u2126), based on the explicit description given earlier. The expressions use index variables x 1 , . . . , x k and y 1 , . . . , y k . More specifically, for j \u2208 [d t ] we consider the expressions:\n\u03d5 (t) j (x 1 , . . . , x k ) = \u03c3 \uf8eb \uf8ed \u03b3\u2208[n] 2k /\u223c 2k dt\u22121 i=1 c \u03b3,i,j y1 \u2022 \u2022 \u2022 y k \u03c8 \u03b3 (x 1 , . . . , x k , y 1 , . . . , y k ) \u2022 \u03d5 (t\u22121) i (y 1 , . . . , y k ) + \u00b5\u2208[n] k /\u223c k b \u00b5,j \u2022 \u03c8 \u00b5 (x 1 , . . . , x k ) \uf8f6 \uf8f8 , (2)\nwhere \u03c8 \u00b5 (x 1 , . . . , x k ) is a product of expressions of the form 1 xi op xj encoding the equality pattern \u00b5, and similarly, \u03c8 \u03b3 (x 1 , . . . , x k , y 1 , . . . , y k ) is a product of expressions of the form 1 xi op xj , 1 yi op yj and 1 xi op yj encoding the equality pattern \u03b3. These expressions are indicator functions for the their corresponding equality patterns. That is,\n[[\u03c8 \u03b3 , (v, w)]] G = 1 if (v, w) \u2208 \u03b3 0 otherwise [[\u03c8 \u00b5 , v]] G = 1 if v \u2208 \u00b5 0 otherwise\nWe remark that in the expressions \u03d5 (t) j we have two kinds of summations: those ranging over a fixed number of elements (over equality patterns, feature dimension), and those ranging over the index variables y 1 , . . . , y k . The latter are the only ones contributing the summation depth. The former are just concise representations of a long summation over a fixed number of expressions.\nWe now only need to show that we can equivalently write \u03d5 (t) j (x 1 , . . . , x k ) as expressions in TL k (\u2126), that is, using only indices x 1 , . . . , x k . As such, we can already ignore the term \u00b5\u2208\n[n] k /\u223c k b \u00b5,j \u2022 \u03c8 \u00b5 (x 1 , . . . , x k )\nsince this is already in TL k (\u2126). Furthermore, this expressions does not affect the summation depth. Furthermore, as just mentioned, we can expand expression \u03d5 (t) j into linear combinations of other simpler expressions. As such, it suffices to show that k index variables suffice for each expression of the form:\ny1 \u2022 \u2022 \u2022 y k \u03c8 \u03b3 (x 1 , . . . , x k , y 1 , . . . , y k ) \u2022 \u03d5 (t) i (y 1 , . . . , y k ),(3)\nobtained by fixing \u00b5 and i in expression (2). To reduce the number of variables, as a first step we eliminate any disequality using the inclusion-exclusion principle. More precisely, we observe that \u03c8 \u03b3 (x, y) can be written as:\n(i,j)\u2208I 1 xi=xj \u2022 (i,j)\u2208\u012a 1 xi =xj \u2022 (i,j)\u2208J 1 yi=yj \u2022 (i,j)\u2208J 1 yi =yj (i,j)\u2208K 1 xi=yj \u2022 (i,j)\u2208K 1 xi =yj = A\u2286\u012a B\u2286J C\u2286K (\u22121) |A|+|B|+|C| (i,j)\u2208I\u222aA 1 xi=xj (i,j)\u2208J\u222aB 1 yi=yj \u2022 (i,j)\u2208J\u222aC 1 xi=yj ,(4)\nfor some sets I, J and K of pairs of indices in [k] 2 , and where\u012a = [k] 2 \\ I,J = [k] 2 \\ J and K = [k] 2 \\K. Here we use that 1 xi =xj = 1\u22121 xi=xj , 1 yi =yj = 1\u22121 yi=yj and 1 xi =yj = 1\u22121 yi=yj and use the inclusion-exclusion principle to obtain a polynomial in equality conditions only.\nIn view of expression (4), we can push the summations over y 1 , . . . , y k in expression (3) to the subexpressions that actually use y 1 , . . . , y k . That is, we can rewrite expression (3) into the equivalent expression:\nA\u2286\u012a B\u2286J C\u2286K (\u22121) |A|+|B|+|C| \u2022 (i,j)\u2208I\u222aA 1 xi=xj \u2022 \uf8eb \uf8ed y1 \u2022 \u2022 \u2022 y k (i,j)\u2208J\u222aB 1 yi=yj \u2022 (i,j)\u2208K\u222aC 1 xi=yj \u2022 \u03d5 (t\u22121) i (y 1 , . . . , y k ) \uf8f6 \uf8f8 . (5)\nBy fixing A, B and C, it now suffices to argue that\n(i,j)\u2208I\u222aA 1 xi=xj \u2022 \uf8eb \uf8ed y1 \u2022 \u2022 \u2022 y k (i,j)\u2208J\u222aB 1 yi=yj \u2022 (i,j)\u2208K\u222aC 1 xi=yj \u2022 \u03d5 (t\u22121) i (y 1 , . . . , y k ) \uf8f6 \uf8f8 , (6)\ncan be equivalently expressed in TL k (\u2126).\nSince our aim is to reduced the number of index variables from 2k to k, it is important to known which variables are the same. In expression ( 6), some equalities that hold between the variables may not be explicitly mentioned. For this reason, we expand I \u222a A, J \u222a B and K \u222a C with their implied equalities. That is, 1 xi=xj is added to I \u222a A, if for any (v, w) such that\n[[ (i,j)\u2208I\u222aA 1 xi=xj \u2022 (i,j)\u2208J\u222aB 1 yi=yj \u2022 (i,j)\u2208K\u222aC 1 xi=yj , (v, w)]] G = 1 \u21d2 [[1 xi=xj , v]] G = 1\nholds. Similar implied equalities 1 yi=yj and 1 xi=yj are added to J \u222a B and K \u222a C, respectively. let us denoted by I \u2032 , J \u2032 and K \u2032 . It should be clear that we can add these implied equalities to expression (6) without changing its semantics. In other words, expression (6) can be equivalently represented by\n(i,j)\u2208I \u2032 1 xi=xj \u2022 \uf8eb \uf8ed y1 \u2022 \u2022 \u2022 y k (i,j)\u2208J \u2032 1 yi=yj \u2022 (i,j)\u2208K \u2032 1 xi=yj \u2022 \u03d5 ((t\u22121) i (y 1 , . . . , y k ) \uf8f6 \uf8f8 ,(7)\nThere now two types of index variables among the y 1 , . . . , y k : those that are equal to some x i , and those that are not. Now suppose that (j, j \u2032 ) \u2208 J \u2032 , and thus y j = y j \u2032 , and that also (i, j) \u2208 K \u2032 , and thus x i = y j . Since we included the implied equalities, we also have (i, j \u2032 ) \u2208 K \u2032 , and thus x i = y j \u2032 . There is no reason to keep (j, j \u2032 ) \u2208 J \u2032 as it is implied by (i, j) and (i, j \u2032 ) \u2208 K \u2032 . We can thus safely remove all pairs (j, j \u2032 ) from J \u2032 such that (i, j) \u2208 K \u2032 (and thus also (i, j \u2032 ) \u2208 K \u2032 ). We denote by J \u2032\u2032 be the reduced set of pairs of indices obtained from J \u2032 in this way. We have that expression (7) can be equivalently written as\n(i,j)\u2208I \u2032 1 xi=xj \u2022 \uf8eb \uf8ed y1 \u2022 \u2022 \u2022 y k (i,j)\u2208K \u2032 1 xi=yj \u2022 (i,j)\u2208J \u2032\u2032 1 yi=yj \u2022 \u03d5 (t\u22121) i (y 1 , . . . , y k ) \uf8f6 \uf8f8 ,(8)\nwhere we also switched the order of equalities in J \u2032\u2032 and K \u2032 . Our construction of J \u2032\u2032 and K \u2032 ensures that none of the variables y j with j belonging to a pair in J \u2032\u2032 is equal to some x i .\nBy contrast, the variable y j occurring in (i, j) \u2208 K \u2032 are equal to x i . We observe, however, that also certain equalities among the variables {x 1 , . . . , x k } hold, as represented by the pairs in I \u2032 . let I \u2032 (i) := {i \u2032 | (i, i \u2032 ) \u2208 I \u2032 } and define\u00ee as a unique representative element in I \u2032 (i). For example, one can take\u00ee to be smallest index in I \u2032 (i). We use this representative index (and corresponding x-variable) to simplify K \u2032 . More precisely, we replace each pair (i, j) \u2208 K \u2032 with the pair (\u00ee, j). In terms of variables, we replace x i = y j with x\u00ee = y j . Let K \u2032\u2032 be the set K \u2032\u2032 modified in that way. Expression (8) can thus be equivalently written as\n(i,j)\u2208I \u2032 1 xi=xj \u2022 \uf8eb \uf8ed y1 \u2022 \u2022 \u2022 y k (\u00ee,j)\u2208K \u2032\u2032 1 x\u00ee=yj \u2022 (i,j)\u2208J \u2032\u2032 1 yi=yj \u2022 \u03d5 (t\u22121) i (y 1 , . . . , y k ) \uf8f6 \uf8f8 ,(9)\nwhere the free index variables of the subexpression\ny1 \u2022 \u2022 \u2022 y k (\u00ee,j)\u2208K \u2032\u2032 1 x\u00ee=yj \u2022 (i,j)\u2208J \u2032\u2032 1 yi=yj \u2022 \u03d5 (t\u22121) i (y 1 , . . . , y k )(10)\nare precisely the index variables x\u00ee for (\u00ee, j) \u2208 K \u2032\u2032 . Recall that our aim is to reduce the variables from 2k to k. We are now finally ready to do this. More specifically, we consider a bijection \u03b2 : {y 1 , . . . , y k } \u2192 {x 1 , . . . , x k } in which ensure that for each\u00ee there is a j such that (\u00ee, j) \u2208 K \u2032\u2032 and \u03b2(y j ) = x\u00ee. Furthermore, among the summations y1 \u2022 \u2022 \u2022 y k we can ignore those for which \u03b2(y j ) = x\u00ee holds. After all, they only contribute for a given x\u00ee value. Let Y be those indices in [k] such that \u03b2(y j ) = x\u00ee for some\u00ee. Then, we can equivalently write expression ( 9) as\n(i,j)\u2208I \u2032 1 xi=xj \u2022 \u03b2(yi),i\u2208Y (\u00ee,j)\u2208K \u2032 1 x\u00ee=\u03b2(yj) \u2022 (i,j)\u2208J \u2032\u2032 1 \u03b2(yi)=\u03b2(yj) \u2022 \u03b2(\u03d5 (t\u22121) i (y 1 , . . . , y k )) , (11\n)\nwhere \u03b2(\u03d5 (t\u22121) i (y 1 , . . . , y k )) denotes the expression obtained by renaming of variables y 1 , . . . , y j in \u03d5 (t\u22121) i (y 1 , . . . , y k ) into x-variables according to \u03b2. This is our desired expression in TL k (\u2126). If we analyze the summation depth of this expression, we have by induction that the summation depth of \u03d5 (t\u22121) i is at most (t \u2212 1)k. In the above expression, we are increasing the summation depth with at most |Y |. The largest size of Y is k, which occurs when none of the y-variables are equal to any of the x-variables. As a consequence, we obtained an expression of summation depth at most tk, as desired.\nAs a consequence, when using k-IGNs (t) for vertex embeddings, using (G, v) \u2192 F (t) v,...,v,: one simply pads the layer expression with i\u2208[k] 1 x1=xi which does not affect the number of variables or summation depth. When using k-IGNs (t) of graph embeddings, an additional invariant layer is added to obtain an embedding from G \u2192 R dt . Such invariant layers have a similar (simpler) representation as given in equation 1 (Maron et al., 2019c), and allow for a similar analysis. One can verify that expressions in TL \nk-IGN = \u03c1 1 vwl (\u221e) k\u22121 and \u03c1 0 k-IGN = \u03c1 0 gwl (\u221e) k\u22121 hold. k-dimensional GINs.\nWe can recover a layer-based characterization for k-IGNs that compute vertex embeddings by considering a special subset of k-IGNs. Indeed, the k-IGNs used in Maron et al.\n(2019b) to show \u03c1 1 (wl (t) k\u22121 ) \u2286 \u03c1 1 (k-IGN (t)\n) are of a very special form. We extract the essence of these special k-IGNs in the form of k-dimensional GINs. That is, we define the class k-GINs to consist of layers defined as follows. The initial layers are just as for k-IGNs. Then, for t \u2265 1:\nF (t)\nv1,...,v k ,: := mlp (t) 0 F (t\u22121) v1,...,v k ,: , u\u2208VG mlp (t) 1 (F (t\u22121) u,v2,...,v k ,: ), u\u2208VG mlp (t) 1 (F (t\u22121) v1,u,...,v k ,: ) , . . . ,\nu\u2208VG mlp (t) 1 (F (t\u22121) v1,v2,...,v k\u22121 ,w,: )) ,\nwhere F (t\u22121) v1,v2,...,v k ,: \u2208 R dt\u22121 , mlp\n1 : R dt\u22121 \u2192 R bt and mlp (t)\n1 : R dt\u22121+kbt \u2192 R dt are MLPs. It is now an easy exercise to show that k-GIN (t) can be represented in TL (t) k (\u2126) (remark that the summations used increase the summation depth with one only in each layer). Combined with Theorem 4.2 and by inspecting the proof of Theorem 1 in Maron et al. (2019b), we obtain: For (i), we show the existence of a set S \u2286 C(G s , R) such that S \u2022 F \u2113 \u2286 F \u2113 and \u03c1 s (S) \u2286 \u03c1 s (F \u2113 ) hold. Similarly as in Azizian & Lelarge (2021), we define\nS := f \u2208 C(G s , R) (f, f, . . . , f ) \u2113 times \u2208 F \u2113 .\nWe remark that for s \u2208 S and f \u2208 F \u2113 , s When we know more about \u03c1 s (F \u2113 ) we can say a bit more. In the following, we let alg \u2208 {cr (t) , gcr (t) , vwl (t) k , gwl (\u221e) k } and only consider the setting where s is either 0 (invariant graph functions) or s = 1 (equivariant graph/vertex functions). Corollary 6.2. Under the assumptions of Theorem 6.1 and if \u03c1(F \u2113 ) = \u03c1(alg), then\n\u2022 f : G s \u2192 R \u2113 : (G, v) \u2192 s(G, v) \u2299 f (G, v), with \u2299 being pointwise multiplication, is also in F \u2113 . Indeed, s \u2022 f = \u2299 \u2022 (s,\nF \u2113 = {f : G s \u2192 R \u2113 | \u03c1(alg) \u2286 \u03c1(f )}.\nProof. This is just a mere restatement of Theorem 6.1 in which \u03c1 s (F \u2113 ) in the condition \u03c1 s (F \u2113 ) \u2286 \u03c1 s (f ) is replaced by \u03c1 s (alg), where s = 1 for alg \u2208 {cr (t) , vwl (t) k } and s = 0 for alg \u2208 {gcr (t) , gwl (\u221e) k }.\nTo relate all this to functions representable by tensor languages, we make the following observations. First, if we consider F to be the set of all functions that can be represented in GTL (t) (\u2126), TL (t+1)", "publication_ref": ["b4", "b4", "b3", "b3", "b3", "b3", "b3", "b3", "b4", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "2", "text": "(\u2126), TL (t) k+1 (\u2126) or TL(\u2126), then F will be automatically concatenation and function-closed, provided that \u2126 consists of all functions in p C(R p , R \u2113 ). Hence, Theorem 6.1 applies. Furthermore, our results from Section 4 tell us that for all t \u2265 0, and k\n\u2265 1, \u03c1 1 cr (t) = \u03c1 1 GTL (t) (\u2126) , \u03c1 0 gcr (t) = \u03c1 0 TL (t+1) 2 (\u2126) = \u03c1 0 gwl (t) 1 , \u03c1 1 ewl (t) k = \u03c1 1 TL (t) k+1\n(\u2126) , and \u03c1 0 TL k+1 (\u2126) = \u03c1 0 gwl (\u221e) k . As a consequence, Corollary 6.2 applies as well. We thus easily obtain the following characterizations: Proposition F.2. For any t \u2265 0 and k \u2265 1:\n\u2022 If F consists of all functions representable in GTL (t) (\u2126), then F \u2113 = {f : G 1 \u2192 R \u2113 | \u03c1 1 cr (t) \u2286 \u03c1 1 (f )}; \u2022 If F consists of all functions representable in TL (t) k+1 (\u2126), then F \u2113 = {f : G 1 \u2192 R \u2113 | \u03c1 1 vwl (t) k \u2286 \u03c1 1 (f )}; \u2022 If F consists of all functions representable in TL (t+1) 2 (\u2126), then F \u2113 = {f : G 0 \u2192 R \u2113 | \u03c1 0 gwl (t) 1 \u2286 \u03c1 0 (f )}; and finally, \u2022 If F consists of all functions representable in TL k+1 (\u2126), then F \u2113 = {f : G 0 \u2192 R \u2113 | \u03c1 0 gwl (\u221e) k \u2286 \u03c1 0 (f )},\nprovided that \u2126 consists of all functions in p C(R p , R \u2113 ).\nIn fact, Lemma 32 in Azizian & Lelarge (2021) implies that we can equivalently populate \u2126 with all MLPs instead of all continuous functions. We can thus use MLPs and continuous functions interchangeably when considering the closure of functions.\nAt this point, we want to make a comparison with the results and techniques in Azizian & Lelarge (2021). Our proof strategy is very similar and is also based on Theorem F.1. The key distinguishing feature is that we consider functions f : G s \u2192 R \u2113 f instead of functions from graphs alone. This has as great advantage that no separate proofs are needed to deal with invariant or equivariant functions. Equivariance incurs quite some complexity in the setting considered in Azizian & Lelarge (2021).\nA second major difference is that, by considering functions representable in tensor languages, and based on our results from Section 4, we obtain a more fine-grained characterization. Indeed, we obtain characterizations in terms of the number of rounds used in CR and k-WL. In Azizian & Lelarge (2021), t is always set to \u221e, that is, an unbounded number of rounds is considered. Furthermore, when it concerns functions f : G 1 \u2192 R \u2113 f , we recall that CR is different from 1-WL. Only 1-WL is considered in Azizian & Lelarge (2021). Finally, another difference is that we define the equivariant version vwl k in a different way than is done in Azizian & Lelarge (2021), because in this way, a tighter connection to logics and tensor languages can be made. In fact, if we were to use the equivariant version of k-WL from Azizian & Lelarge ( 2021), then we necessarily have to consider an unbounded number of rounds (similarly as in our gwl k case).\nWe conclude this section by providing a little more details about the consequences of the above results for GNNs. As we already mentioned in Section 6.2, many common GNN architectures are concatenation and function-closed (using MLPs instead of continuous functions). This holds, for example, for the classes GIN (t) \u2113 , eGIN (t) \u2113 , k-FGNN (t) \u2113 and k-GIN (t) \u2113 and k-IGN (t) , as described in Section 5 and further detailed in Section E and D. Here, the subscript \u2113 refers to the dimension of the embedding space.\nWe now consider a function f that is not more separating than cr (t) (respectively, gcr (t) , vwl (t) k or gwl (\u221e) k , for some k \u2265 1), and want to know whether f can be approximated by a class of GNNs. Proposition F.2 tells that such f can be approximated by a class of GNNs as long as these are at least as separating as GTL (t) (respectively, TL (t+1) 2 , TL (t) k+1 or TL (\u221e) k+1 ). This, in turn, amounts showing that the GNNs can be represented in the corresponding tensor language fragment, and that they can match the corresponding labeling algorithm in separation power. We illustrate this for the GNN architectures mentioned above.\n\u2022 In Section 5 we showed that GIN (t) \u2113 can be represented in GTL (t) (\u2126). Theorem 4.3 then implies that \u03c1 1 (cr (t) ) \u2286 \u03c1 1 (GIN (t) \u2113 ). Furthermore, Xu et al. (2019) showed that \u03c1 1 (GIN (t) \u2113 ) \u2286 \u03c1 1 (cr (t) ). As a consequence, \u03c1 1 (GIN (t) \u2113 ) = \u03c1 1 (cr (t) ). We note that the lower bound for GINs only holds when graphs carry discrete labels. The same restriction is imposed in Azizian & Lelarge (2021).\n\u2022 In Section 5 we showed that eGIN (t) \u2113 can be represented in TL (t) 2 (\u2126). Theorem 4.2 then implies that \u03c1 1 (vwl (t) 1 ) \u2286 \u03c1 1 (eGIN (t) \u2113 ). Furthermore, Barcel\u00f3 et al. (2020) showed that \u03c1 1 (eGIN (t) \u2113 ) \u2286 \u03c1 1 (vwl (t) 1 ). As a consequence, \u03c1 1 (eGIN (t) \u2113 ) = \u03c1 1 (vwl (t) 1 ). Again, the lower bound is only valid when graphs carry discrete labels.\n\u2022 In Section 5 we mentioned (see details in Section D) that k-FGNN (t) \u2113 can be represented in TL (t) k+1 (\u2126). Theorem 4.2 then implies that \u03c1 1 (vwl\n(t) k ) \u2286 \u03c1 1 (k-FGNN (t) \u2113 ). Further- more, Maron et al. (2019b) showed that \u03c1 1 (k-FGNN (t) \u2113 ) \u2286 \u03c1 1 (vwl (t) k ). As a consequence, \u03c1 1 (k-FGNN (t) \u2113 ) \u2286 \u03c1 1 (vwl (t) k ). Similarly, \u03c1 1 ((k + 1)-GIN (t) \u2113 ) = \u03c1 1 (vwl (t) k )\nfor the special class of (k + 1)-IGNs described in Section E. No restrictions are in place for the lower bounds and hence real-valued vertex-labelled graphs can be considered.\n\u2022 When GIN (t) \u2113 or eGIN (t) \u2113 are extended with a readout layer, we showed in Section 5 that these can be represented in TL (t+1) 2 (\u2126). Theorem 4.4 and the results by Xu et al. (2019) and Barcel\u00f3 et al. (2020) then imply that \u03c1 0 (vwl (t) 1 ) and \u03c1 0 (gcr (t) ) coincide with the separation power of these architectures with a readout layer. Here again, discrete labels need to be considered.\n\u2022 Similarly, when k-FGNN or (k + 1)-IGNs are used for graph embeddings, we can represent these in TL k+1 (\u2126) resulting again that their separation power coincides with that of gwl (\u221e) k . No restrictions are again in place on the vertex labels.\nSo for all these architectures, Corollary 6.2 applies and we can characterize the closures of these architectures in terms of functions that not more separating than their corresponding versions of cr or k-WL, as described in the main paper. In summary, Proposition F.3. For any t \u2265 0: (t) \u2113 = {f : G 1 \u2192 R \u2113 | \u03c1 1 (cr (t) ) \u2286 \u03c1 1 (f )} = GTL (t) (\u2126) \u2113 eGIN (t) \u2113 = {f : G 1 \u2192 R \u2113 | \u03c1 1 (vwl (t) 1 ) \u2286 \u03c1 1 (f )} = TL (t) 2 (\u2126) \u2113 and when extended with a readout layer: (t) \u2113 = eGIN (t) \u2113 = {f : G 0 \u2192 R \u2113 | \u03c1 0 (gwl (t) 1 ) \u2286 \u03c1 0 (f )} = TL (t+1)\nGIN\nGIN", "publication_ref": ["b2", "b2", "b2", "b2", "b2", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "2", "text": "(\u2126) \u2113 .\nFurthermore, for any k \u2265 1 (\u221e) k ) \u2286 \u03c1 1 (f )} = TL k+1 (\u2126) \u2113 and when converted into graph embeddings:\nk-FGNN (t) \u2113 = k-GIN (t) \u2113 = {f : G 1 \u2192 R \u2113 | \u03c1 1 (vwl (t) k ) \u2286 \u03c1 1 (f )} = TL (t) k+1 (\u2126) \u2113 (k + 1)-IGN \u2113 = {f : G 1 \u2192 R \u2113 | \u03c1 1 (vwl\nk-FGNN \u2113 = k-GIN \u2113 = (k + 1)-IGN \u2113 = {f : G 0 \u2192 R \u2113 | \u03c1 0 (gwl (\u221e)\nk ) \u2286 \u03c1 0 (f )} = TL k+1 (\u2126) \u2113 , where the closures of the tensor languages are interpreted as the closure of the graph or graph/vertex functions that they can represent. For results involving GINs or eGINs, the graphs considered should have discretely labeled vertices.\nAs a side note, we remark that in order to simulate CR on graphs with real-valued labels, one can use a GNN architecture of the form F (t) v: = F (t\u22121) v:\n, u\u2208NG(v) mlp(F (t\u22121) u:\n) , which translates in GTL (t) (\u2126) as expressions of the form \u03d5 (t) j (x 1 ) :=\n\u03d5 (t\u22121) j (x 1 ) 1 \u2264 j \u2264 d t\u22121\nx2 E(x 1 , x 2 ) \u2022 mlp j \u03d5 (t\u22121) 1 (x 1 ), . . . , \u03d5 (t\u22121) dt (x 1 ) d t\u22121 < j \u2264 d t .\nThe upper bound in terms of CR follows from our main results. To show that CR can be simulated, it suffices to observe that one can approximate the function used in Proposition 1 in Maron et al. (2019b) to injectively encode multisets of real vectors by means of MLPs. As such, a continuous version of the first bullet in the previous proposition can be obtained.\nG DETAILS ON TREEWIDTH AND PROPOSITION 4.5\nAs an extension of our main results in Section 4, we enrich the class of tensor language expressions for which connections to k-WL exist. More precisely, instead of requiring expressions to belong to TL k+1 (\u2126), that is to only use k + 1 index variables, we investigate when expressions in TL(\u2126) are semantically equivalent to an expression using k + 1 variables. Proposition 4.5 identifies a large class of such expressions, those of treewidth k. As a consequence, even when representing GNN architectures may require more than k + 1 index variables, sometimes this number can be reduced. As a consequence of our results, this implies that their separation power is in fact upper bounded by \u2113-WL for a smaller \u2113 < k. Stated otherwise, to boost the separation power of GNNs, the treewidth of the expressions representing the layers of the GNNs must have large treewidth.\nWe next introduce some concepts related to treewidth. We here closely follow the exposition given in Abo Khamis et al. (2016) for introducing treewidth by means variable elimination sequences of hypergraphs.\nIn this section, we restrict ourselves to summation aggregation.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "G.1 ELIMINATION SEQUENCES", "text": "We first define elimination sequences for hypergraphs. Later on, we show how to associate such hypergraphs to expressions in tensor languages, allowing us to define elimination sequences for tensor language expressions.\nWith a multi-hypergraph H = (V, E) we simply mean a multiset E of subsets of vertices V. An elimination hypergraph sequences is a vertex ordering \u03c3 = v 1 , . . . , v n of the vertices of H. With such a sequence \u03c3, we can associate for j = n, n\u22121, n\u22122, . . . , 1 a sequence of n multi-hypergraphs H \u03c3 n , H \u03c3 n\u22121 , . . . , H \u03c3 1 as follows. We define\nH n := (V n , E n ) := H \u2202(v n ) := {F \u2208 E n | v n \u2208 F } U n := F \u2208\u2202(vn)\nF. and for j = n \u2212 1, n \u2212 2, . . . , 1 :\nV j := {v 1 , . . . , v j } E j := (E j+1 \\ \u2202(v j+1 )) \u222a {U j+1 \\ {v j+1 }} \u2202(v j ) := {F \u2208 E j | v j \u2208 F }\nU j := F \u2208\u2202(vj ) F.\nThe induced width on H by \u03c3 is defined as max i\u2208[n] |U i | \u2212 1. We further consider the setting in which H has some distinguished vertices. As we will see shortly, these distinguished vertices correspond to the free index variables of tensor language expressions. Without loss of generality, we assume that the distinguished vertices are v 1 , v 2 , . . . , v f . When such distinguished vertices are present, an elimination sequence is just as before, except that the distinguished vertices come first in the sequence. If v 1 , . . . , v f are the distinguished vertices, then we define the induced width of the sequence as f + max f +1\u2264i\u2264n |U i \\ {v 1 , . . . , v f }| \u2212 1. In other words, we count the number of distinguished vertices, and then augment it with the induced width of the sequence, starting from v f +1 to to v n , hereby ignoring the distinguished variables in the U i 's. One could, more generally, also try to reduce the number of free index variables but we assume that this number is fixed, similarly as how GNNs operate.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G.2 CONJUNCTIVE TL EXPRESSIONS AND TREEWIDTH", "text": "We start by considering a special form of TL expressions, which we refer to as conjunctive TL expressions, in analogy to conjunctive queries in database research and logic. A conjunctive TL expression is of the form \u03d5(x) = y \u03c8(x, y).\nwhere x denote the free index variables, y contains all index variables under the scope of a summation, and finally, \u03c8(x, y) is a product of base predicates in TL. That is, \u03c8(x, y) is a product of E(z i , z j ) and P \u2113 (z i ) with z i , z j variables in x or y. With such a conjunctive TL expression, one can associate a multi-hypergraph in a canonical way (Abo Khamis et al., 2016). More precisely, given a conjunctive TL expression \u03d5(x) we define H \u03d5 as:\n\u2022 V \u03d5 consist of all index variables in x and y;\n\u2022 E \u03d5 : for each atomic base predicate \u03c4 in \u03c8 we have an edge F \u03c4 containing the indices occurring in the predicate; and \u2022 the vertices corresponding to the free index variables x form the distinguishing set of vertices.\nWe now define an elimination sequence for \u03d5 as an elimination sequence for H \u03d5 taking the distinguished vertices into account. The following observation ties elimination sequences of \u03d5 to the number of variables needed to express \u03d5. Proposition G.1. Let \u03d5(x) be a conjunctive TL expression for which an elimination sequence of induced with k \u2212 1 exists. Then \u03d5(x) is equivalent to an expression\u03c6(x) in TL k .\nProof. We show this by induction on the number of vertices in H \u03d5 which are not distinguished. For the base case, all vertices are distinguished and hence \u03d5(x) does not contain any summation and is an expression in TL k itself.\nSuppose that in H \u03d5 there are p undistinguished vertices. That is,\n\u03d5(x) = y1 \u2022 \u2022 \u2022 yp \u03c8(x, y).\nBy assumption, we have an elimination sequence of the undistinguished vertices. Assume that y p is first in this ordering. Let us write\n\u03d5(x) = y1 \u2022 \u2022 \u2022 yp \u03c8(x, y) = y1 \u2022 \u2022 \u2022 yp\u22121 \u03c8 1 (x, y \\ y p ) \u2022 yp \u03c8 2 (x, y)\nwhere \u03c8 1 is the product of predicates corresponding to the edges F \u2208 E \u03d5 \\ \u2202(y p ), that is, those not containing y p , and \u03c8 2 is the product of all predicates corresponding to the edges F \u2208 \u2202(y p ), that is, those containing the predicate y p . Note that, because of the induced width of k \u2212 1, yp \u03c8 2 (x, y) contains all indices in U p which is of size \u2264 k. We now replace the previous expression with another expression\n\u03d5 \u2032 (x) = y1 \u2022 \u2022 \u2022 yp\u22121 \u03c8 1 (x, y \\ y p ) \u2022 R p (x, y)\nWhere R p is regarded as an |U p | \u2212 1-ary predicate over the indices in U p \\ y p . It is now easily verified that H \u03d5 \u2032 is the hypergraph H p\u22121 corresponding to the variable ordering \u03c3. We note that this is a hypergraph over p \u2212 1 undistinguished vertices. We can apply the induction hypothesis and replace \u03d5 \u2032 (x) with its equivalent expression\u03c6 \u2032 (x) in TL k . To obtain the expression\u03c6(x) of \u03d5(x), it now remains to replace the new predicate R p with its defining expression. We note again that R p contains at most k \u2212 1 indices, so it will occur in\u03c6 \u2032 (x) in the form R p (x, z) where |z| \u2264 k \u2212 1.\nIn other words, one of the variables in z is not used, say z s , and we can simply replace R p (x, z) by zs \u03c8 x (x, z, z s ).\nAs a consequence, one way of showing that a conjunctive expression \u03d5(x) in TL is equivalently expressible in TL k , is to find an elimination sequence of induced width k \u2212 1. This in turn is equivalent to H \u03d5 having a treewidth of k\u22121, as is shown, e.g., in Abo Khamis et al. (2016). As usual, we define the treewidth of a conjunctive expression \u03d5(x) in TL as the treewidth of its associated hypergraph H \u03d5 .\nWe recall the definition of treewidth (modified to our setting): A tree decomposition T = (V T , E T , \u03be T ) of H \u03d5 with \u03be T : V T \u2192 2 V is such that \u2022 For any F \u2208 E, there is a t \u2208 V T such that F \u2286 \u03be T (t); and \u2022 For any v \u2208 V corresponding to a non-distinguished index variable, the set {t | t \u2208 V T , v \u2208 \u03be(t)} is not empty and forms a connected sub-tree of T .\nThe width of a tree decomposition T is given by max t\u2208VT |\u03be T (t)| \u2212 1. Now the treewidth of H \u03d5 , tw(H) is the minimum width of any of its tree decompositions. We denote by tw(\u03d5) the treewidth of H \u03d5 . Again, similar modifications are used when distinguished vertices are in place. Referring again to Abo Khamis et al. (2016), tw(\u03d5) = k \u2212 1 is equivalent to having a variable elimination sequence for \u03d5 of an induced width of k \u2212 1. Hence, combining this observation with Proposition G.1 results in: Corollary G.2. Let \u03d5(x) be a conjunctive TL expression of treewidth k\u22121. Then \u03d5(x) is equivalent to an expression\u03c6(x) in TL k .\nThat is, we have established Proposition 4.5 for conjunctive TL expressions. We next lift this to arbitrary TL(\u2126) expressions.", "publication_ref": ["b0", "b0", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "G.3 ARBITRARY TL(\u2126) EXPRESSIONS", "text": "First, we observe that any expression in TL can be written as a linear combination of conjunctive expressions. This readily follows from the linearity of the operations in TL and that equality and\nWe next inductively define new k-MPNNs from \"old\" k-MPNNs. That is, given k-MPNNs \u03d5 1 (x 1 ), . . . , \u03d5 \u2113 (x \u2113 ), the following are also k-MPNNs:\n\u2022 Function applications of the form \u03d5(x) := f (\u03d5 1 (x 1 ), . . . , \u03d5 \u2113 (x \u2113 ) are k-MPNNs, where x = x 1 \u222a \u2022 \u2022 \u2022 \u222a x \u2113 , and defined by \u03d5(G, v) := f (\u03d5 1 (G, v| x1 ), . . . , \u03d5 \u2113 (G, v| x \u2113 )) .\nHere, if \u03d5 i (G, v| xi ) \u2208 R di , then f : R d1 \u00d7 \u2022 \u2022 \u2022 \u00d7 R d \u2113 \u2192 R d for some d \u2208 N. That is, \u03d5 generates an embedding in R d . We remark that our function applications include concatenation.\n\u2022 Unconditional aggregations of the form \u03d5(x) := agg F xj (\u03d5 1 (x, x j )) are k-MPNNs, where x j \u2208 X and x j \u2208 x, and defined by \u03d5(G, v) := F { {\u03d5 1 (G, v 1 , . . . , v j\u22121 , w, v j+1 , . . . , v k ) | w \u2208 V G }} .\nHere, if \u03d5 1 generates an embedding in R d1 , then F is an aggregation function assigning to multisets of vectors in R d1 a vector in R d , for some d \u2208 N. So, \u03d5 generates an embedding in R d .\n\u2022 Conditional aggregations of the form \u03d5(x i ) := agg F xj (\u03d5 1 (x i , x j )|E(x i , x j )) are k-MPNNs, with x i , x j \u2208 X, and defined by\n\u03d5(G, v) := F { {\u03d5 1 (G, v i , w) | w \u2208 N G (v i )}} .\nAs before, if \u03d5 1 generates an embedding in R d1 , then F is an aggregation function assigning to multisets of vectors in R d1 a vector in R d , for some d \u2208 N. So again, \u03d5 generates an embedding in R d .\nAs defined in the main paper, we also consider the subclass k-MPNNs (t) by only considering k-MPNNs defined in terms of expressions of aggregation depth at most t. Our main results, phrased in terms of k-MPNNs are: \u03c1 1 (vwl (t) k ) = \u03c1 1 (k-MPNNs (t) ) and \u03c1 0 (gwl k ) = \u03c1 0 (k-MPNNs). Hence, if the embeddings computed by GNNs are k-MPNNs, one obtains an upper bound on the separation power in terms of k-WL.\nThe classical MPNNs (Gilmer et al., 2017) are subclass of 1-MPNNs in which no unconditional aggregation can be used and furthermore, function applications require input embeddings with the same single variable (x 1 or x 2 ), and only 1 xi=xi and 1 xi =xi are allowed. In other words, they correspond to guarded tensor language expressions (Section 4.2). We denote this class of 1-MPNNs by MPNNs and by MPNNs (t) when restrictions on aggregation depth are in place. And indeed, the classical way of describing MPNNs as \u03d5 (0) (x 1 ) = (P 1 (x 1 ), . . . , P \u2113 (x 1 ))\n\u03d5 (t) (x 1 ) = f (t) \u03d5 (t\u22121) (x 1 ), aggr F (t)\nx2 \u03d5 (t\u22121) (x 1 ), \u03d5 (t\u22121) (x 2 )|E(x i , x j ) correspond to 1-MPNNs that satisfy the above mentioned restrictions. Without readouts, MPNNs compute vertex embeddings and hence, our results imply \u03c1 1 (cr (t) ) = \u03c1 1 (MPNNs (t) ).\nFurthermore, MPNNs with a readout function fall into the category of 1-MPNNs:\n\u03d5 := aggr readout x1 (\u03d5 (t) (x 1 ))\nwhere unconditional aggregation is used. Hence, \u03c1 0 (gcr (t) ) = \u03c1 0 (gwl (t) 1 ) = \u03c1 0 (1-MPNNs (t+1) ). We thus see that k-MPNNs gracefully extend MPNNs and can be used for obtaining upper bounds on the separation power of classes of GNNs.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Proposition E.2. For any k \u2265 2 and any t \u2265 0: \u03c1 1 (k-GIN (t) ) = \u03c1 1 (vwl (t) k\u22121 ).\nWe can define the invariant version of k-IGNs by adding a simple readout layer of the form v1,...,v k \u2208VG mlp(F (t) v1,...,v k ,: ), as is used in Maron et al. (2019b). We obtain, \u03c1 0 (k-GIN) = \u03c1 0 (gwl (\u221e) k\u22121 ), by simply rephrasing the readout layer in TL k (\u2126).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F DETAILS OF SECTION 6", "text": "Let C(G s , R \u2113 ) be the class of all continuous functions from G s to R \u2113 . We always assume that G s forms a compact space. For example, when vertices are labeled with values in {0, 1} \u21130 , G s is a finite set which we equip with the discrete topology. When vertices carry labels in R \u21130 we assume that these labels come from a compact set K \u2282 R \u21130 . In this case, one can represent graphs in G s by elements in (R \u21130 ) 2 and the topology used is the one induced by some metric . on the reals. Similarly, we equip R \u2113 with the topology induced by some metric . .\nThe following theorem provides a characterization of the closure of a set of functions. We state it here modified to our setting. Theorem F.1 ((Timofte, 2005)). Let F \u2286 C(G s , R \u2113 ) such that there exists a set S \u2286 C(G s , R) satisfying S \u2022 F \u2286 F and \u03c1(S) \u2286 \u03c1(F ). Then,\nWe can equivalently replace \u03c1(F ) by \u03c1(S) in the expression for F .\nWe will use this theorem to show Theorem 6.1 in the setting that F consists of functions that can be represented in TL(\u2126), and more generally, sets of functions that satisfy two conditions, stated below. We more generally allow F to consist of functions f : G s \u2192 R \u2113 f , where the \u2113 f \u2208 N may depend on f . We will require F to satisfy the following two conditions:\nWe denote by F \u2113 be the subset of F of functions from G s to R \u2113 . Theorem 6.1. For any \u2113, and any set F of functions, concatenation and function closed for \u2113, we have:\nProof. The proof consist of (i) verifying the existence of a set S as mentioned Theorem F.1; and of (ii) eliminating the pointwise convergence condition\nFor showing (ii) we argue that\nIndeed, take an arbitrary f \u2208 G k \u2192 R \u2113 and consider the constant functions b i : R \u2113 \u2192 R \u2113 : x \u2192 b i with b i \u2208 R \u2113 the ith basis vector. Since F is functionclosed for \u2113, so is F \u2113 . Hence, b i := g i \u2022 f \u2208 F \u2113 as well. Furthermore, if s a : R \u2113 \u2192 R \u2113 : x \u2192 a \u00d7 x, for a \u2208 R, then s a \u2022 f \u2208 F \u2113 and thus F \u2113 is closed under scalar multiplication. Finally, consider + : R 2\u2113 \u2192 R \u2113 : (x, y) \u2192 x + y. For f and g in F \u2113 , h = (f, g) \u2208 F since F is concatenationclosed. As a consequence, the function +\nshowing that F \u2113 is also closed under addition. All combined, this shows that F \u2113 is closed under taking linear combinations and since the basis vectors of R \u2113 can be attained, F \u2113 (G, v) := R \u2113 , as desired.\ninequality predicates can be eliminated. More specifically, we may assume that \u03d5(x) in TL is of the form \u03b1\u2208A a \u03b1 \u03c8 \u03b1 (x, y), with A finite set of indices and a \u03b1 \u2208 R, and \u03c8 \u03b1 (x, y) conjunctive TL expressions. We now define tw(\u03d5) := max{tw(\u03c8 \u03b1 ) | \u03b1 \u2208 A} for expressions in TL. To deal with expressions in TL(\u2126) that may contain function application, we define tw(\u03d5) as the maximum treewidth of the expressions: (i) \u03d5 nofun (x) \u2208 TL obtained by replacing each top-level function application f (\u03d5 1 , . . . , \u03d5 p ) by a new predicate R f with free indices free(\u03d5 1 ) \u222a \u2022 \u2022 \u2022 \u222a free(\u03d5 p ); and (ii) all expressions \u03d5 1 , . . . , \u03d5 p occurring in a top-level function application f (\u03d5 1 , . . . , \u03d5 p ) in \u03d5. We note that these expression either have no function applications (as in (i)) or have function applications of lower nesting depth (in \u03d5, as in (ii)). In other words, applying this definition recursively, we end up with expressions with no function applications, for which treewidth was already defined. With this notion of treewidth at hand, Proposition 4.5 now readily follows.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "H HIGHER-ORDER MPNNS", "text": "We conclude the supplementary material by elaborating on k-MPNNs and by relating them to classical MPNNs (Gilmer et al., 2017). As underlying tensor language we use TL k+1 (\u2126, \u0398) which includes arbitrary functions (\u2126) and aggregation functions (\u0398), as defined in Section C.5.\nWe recall from Section 3 that k-MPNNs refer to the class of embeddings f : G s \u2192 R \u2113 for some \u2113 \u2208 N that can be represented in TL k+1 (\u2126, \u0398). When considering an embedding f : G s \u2192 R \u2113 , the notion of being represented is defined in terms of the existence of \u2113 expressions in TL k+1 (\u2126, \u0398), which together provide each of the \u2113 components of the embedding in R \u2113 . We remark, however, that we can alternatively include concatenation in tensor language. As such, we can concatenate \u2113 separate expressions into a single expression. As a positive side effect, for f : G s \u2192 R \u2113 to be represented in tensor language, we can then simply define it by requiring the existence of a single expression, rather than \u2113 separate ones. This results in a slightly more succinct way of reasoning about k-MPNNs.\nIn order to reason about k-MPNNs as a class of embeddings, we can obtain an equivalent definition for the class of k-MPNNs by inductively stating how new embeddings are computed out of old embeddings. Let X = {x 1 , . . . , x k+1 } be a set of k + 1 distinct variables. In the following, v denotes a tuple of vertices that have at least as many components as the highest index of variables used in expressions. Intuitively, variable x j refers to the jth component in v. We also denote the image of a graph G and tuple v by an expression \u03d5, i.e., the semantics of \u03d5 given G and v, as \u03d5(G, v) rather than by [[\u03d5, v]] G . We further simply refer to embeddings rather than expressions.\nWe first define \"atomic\" k-MPNN embeddings which extract basic information from the graph G and the given tuple v of vertices.\n\u2022 Label embeddings of the form \u03d5(x i ) := P s (x i ), with x i \u2208 X, and defined by \u03d5(G, v) := (col G (v i )) s , are k-MPNNs;\n\u2022 Edge embeddings of the form \u03d5(x i , x j ) := E(x i , x j ), with x i , x j \u2208 X, and defined by \u03d5(G, v) := 1 if v i v j \u2208 E G 0 otherwise, are k-MPNNs; and\n\u2022 (Dis-)equality embeddings of the form \u03d5(x i , x j ) := 1 xi op xj , with x i , x j \u2208 X, and defined by \u03d5(G, v) := 1 if v i op v j 0 otherwise, are k-MPNNs.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "FAQ: Questions Asked Frequently", "journal": "ACM", "year": "2016", "authors": "Mahmoud Abo Khamis; Hung Q Ngo; Atri Rudra"}, {"ref_id": "b1", "title": "The generalized distributive law", "journal": "IEEE Transactions on Information Theory", "year": "2000", "authors": "M Srinivas; Robert J Aji;  Mceliece"}, {"ref_id": "b2", "title": "Expressive power of invariant and equivariant graph neural networks", "journal": "", "year": "2021", "authors": "Waiss Azizian; Marc Lelarge"}, {"ref_id": "b3", "title": "Breaking the limits of message passing graph neural networks", "journal": "PMLR", "year": "2021", "authors": "Muhammet Balcilar; Pierre H\u00e9roux; Benoit Ga\u00fcz\u00e8re; Pascal Vasseur; S\u00e9bastien Adam; Paul Honeine"}, {"ref_id": "b4", "title": "Analyzing the expressive power of graph neural networks in a spectral perspective", "journal": "", "year": "2021", "authors": "Muhammet Balcilar; Guillaume Renton; Pierre H\u00e9roux; Benoit Ga\u00fcz\u00e8re; S\u00e9bastien Adam; Paul Honeine"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "and such that all c i 's are pairwise distinct and c = \u2113 i=1 c i \u2022 m i . It now suffices to consider the following formul\u00e3", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "\u03d5c := \u2113\u2208N (m1,...,m \u2113 )\u2208N \u2113 (c,c1,...,c \u2113 )\u2208C(m1,...,m \u2113 ,F ) m 1 , . . . , m \u2113 , F ) now consists of all (c, c 1 , . . . , c \u2113 ) \u2208 R \u2113+1 such that c = F {{c 1 , . . . , c 1 m1 times , . . . , c \u2113 , . . . , c \u2113 m \u2113 times", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "C(m 1 , . . . , m \u2113 , F ) again consists of all (c, c 1 , . . . , c \u2113 ) \u2208 R \u2113+1 such that c = F {{c 1 , . . . , c 1 m1 times , . . . , c \u2113 , . . . , c \u2113 m \u2113 times }} and m = \u2113 s=1 m s .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Other higher-order examples. It is readily verified that t-layered k-GNNs (Morris et al., 2019) can be represented in TL (t) k+1 (\u2126), recovering the known upper bound by vwl (t) k (Morris et al., 2019). It is an equally easy exercise to show that 2-WL-convolutions (Damke et al., 2020) and Ring-GNNs (Chen et al., 2019) are bounded by 2-WL, by simply writing their layers in TL 3 (\u2126). The invariant graph networks (k-IGNs) (Maron et al., 2019b) will be treated in Section E, as their representation in TL k+1 (\u2126) requires some work.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "are needed when such an invariant layer is added to previous t layers. Based on this, Theorem 4.2, Lemma E.1 and Theorem 1 inMaron et al. (2019b), imply that \u03c1 1", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "f ) with (s, f ) the concatenation of s and f and \u2299 : R 2\u2113 \u2192 R \u2113 : (x, y) \u2192 x \u2299 y being pointwise multiplication.It remains to verify \u03c1 s (S) \u2286 \u03c1 s (F \u2113 ). Assume that (G, v) and (H, w) are not in \u03c1 s (F \u2113 ). By definition, this implies the existence of a functionf \u2208 F \u2113 such thatf (G, v) = a = b =f (H, w) with a, b \u2208 R \u2113 . We argue that (G, v) and (H, w) are also not in \u03c1 s (S) either. Indeed, Proposition 1 inMaron et al. (2019b)  implies that there exists natural numbers \u03b1 \u03b1 \u03b1 = (\u03b1 1 , . . . , \u03b1 \u2113 ) \u2208 N \u2113 such that the mapping h \u03b1 \u03b1 \u03b1 :R \u2113 \u2192 R : x \u2192 \u2113 i=1 x \u03b1i i satisfies h \u03b1 \u03b1 \u03b1 (a) = a = b = h \u03b1 \u03b1 \u03b1 (b), with a, b \u2208 R. Since F (and thus also F \u2113 ) is function-closed, h \u03b1 \u03b1 \u03b1 \u2022 f \u2208 F \u2113 for any f \u2208 F \u2113 . In particular, g := h \u03b1 \u03b1 \u03b1 \u2022f \u2208 F \u2113 and concatenation-closure implies that (g, . . . , g) : G s \u2192 R \u2113 is in F \u2113 too. Hence, g \u2208 S, by definition. It now suffices to observe that g(G, v) = h \u03b1 \u03b1 \u03b1 (f (G, v)) = a = b = h \u03b1 \u03b1 \u03b1 (f (H, w)) = g(H, w), and thus (G, v) and (H, w) are not in \u03c1 s (S), as desired.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M. Bronstein. Improving graph neural network expressivity via subgraph isomorphism counting. In Graph RepresentationLearning and Beyond (GRL+)  Workshop at the 37 th International Conference on Machine Learning, 2020. URL https://arxiv.org/abs/2006.09252. 29 Robert Brijder, Floris Geerts, Jan Van den Bussche, and Timmy Weerwag. On the expressive power of query languages for matrices. ACM TODS, 44(4):15:1-15:31, 2019. URL https://doi.org/10.1145/3331445. 1, 14, 30 Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Csanky. Fast parallel matrix inversion algorithms. SIAM J. Comput., 5(4):618-623, 1976. URL Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems, volume 30, 2016. URL https://proceedings.neurips.cc/paper/2016/file/04df4d434d481c5bb723be1b6df1ee65-Pap 2, 7, 30 Martin F\u00fcrer. Weisfeiler-Lehman refinement requires at least a linear number of iterations. In Proceedings of the 28th International Colloqium on Automata, Languages and Programming, ICALP, volume 2076 of Lecture Notes in Computer Science, pp. 322-333. Springer, 2001. URL Floris Geerts, Filip Mazowiecki, and Guillermo A. P\u00e9rez. Let's agree to degree: Comparing graph convolutional networks in the message-passing framework. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 3640-3649. PMLR, 2021a. URL http://proceedings.mlr.press/v139/geerts21a.html. 26 Floris Geerts, Thomas Mu\u00f1oz, Cristian Riveros, and Domagoj Vrgoc. Expressive power of linear algebra query languages. In Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems, PODS, pp. 342-354. ACM, 2021b. URL https://doi.org/10.1145/3452021.3458314. 1, 2, 14 Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning, volume 70, pp. 1263-1272, 2017. URL http://proceedings.mlr.press/v70/gilmer17a/gilmer17a.pdf. 2, 4, 6, 42, 43 .2010.04.005. URL https://www.sciencedirect.com/science/article/pii/S1063520310000552. Occasion of His Sixtieth Birthday, pp. 59-81. Springer, 1990. URL https://doi.org/10.1007/978-1-4612-4478-3_5. 3 Nicolas Keriven and Gabriel Peyr\u00e9. Universal invariant and equivariant graph neural networks. Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph networks. In Proceedings of the 7th International Conference on Learning Representations, ICLR, 2019c. URL https://openreview.net/forum?id=Syx72jC9tm. 3, 32, 35 Christian Merkwirth and Thomas Lengauer. Automatic generation of complementary descriptors with molecular graph networks. J. Chem. Inf. Model., 45(5):1159-1168, 2005. URL https://doi.org/10.1021/ci049613b. 1 H. L. Morgan. The generation of a unique machine description for chemical structures-a technique developed at chemical abstracts service. Journal of Chemical Documentation, 5(2):107-113, 1965. URL https://doi.org/10.1021/c160017a018. 3 ArXiv, 2019. URL https://arxiv.org/abs/1910.00039. 18 Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. ICLR, 2018. URL https://openreview.net/forum?id=rJXMpikCZ. 27 Felix Wu, Amauri H. Souza Jr., Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Q. Weinberger. Simplifying graph convolutional networks. In Proceedings of the 36th International Conference on Machine Learning, ICML, volume 97 of Proceedings of Machine Learning Research, pp. 6861-6871. PMLR, 2019. URL http://proceedings.mlr.press/v97/wu19e.html. 7, 26 Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In Proceedings of the 7th International Conference on Learning Representations, ICLR, 2019. URL https://openreview.net/forum?id=ryGs6iA5Km.", "figure_data": "Spectral net-works and locally connected networks on graphs.In Proceedings of the 2ndInternational Conference on Learning Representations, ICLR, 2014.URLhttps://openreview.net/forum?id=DQNsQf-UsoDBa. 30Jin-yi Cai, Martin F\u00fcrer, and Neil Immerman.An optimal lower bound on the num-ber of variables for graph identifications.Comb., 12(4):389-410, 1992.URLhttps://doi.org/10.1007/BF01305232. 3, 6 Zhengdao Chen, Soledad Villar, Lei Chen, and Joan Bruna. between graph isomorphism testing and function approximation with GNNs. On the equivalence In Advances in Neural Information Processing Systems, volume 32, 2019. URL https://proceedings.neurips.cc/paper/2019/file/71ee911dd06428a96c143a0b135041a4-Pap 2, 3, 28 75877cb75154206c4e65e76b88a12712-Pap 8, 32 Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Li\u00f2, and Petar Veli\u010dkovi\u0107. Principal neighbourhood aggregation for graph nets. In Ad-vances in Neural Information Processing Systems, volume 33, 2020. URL https://proceedings.neurips.cc/paper/2020/file/99cad265a1768cc2dd013f0e740300ae-Pap 7, 26, 27 L. 3, 5, 6, 17 William L. Hamilton. Graph representation learning. Synthesis Lectures on Artificial Intelligence and Machine Learning, 14(3):1-159, 2020. URL https://doi.org/10.2200/S01045ED1V01Y202009AIM046. 1 Wavelets on graphs via spectral graph theory. Applied and Computational Harmonic Analysis, 30(2):129-150, 2011. ISSN 1063-5203. 30 Neil Immerman and Eric Lander. Describing graphs: A first-order approach to graph canonization. In Complexity Theory Retrospective: In Honor of Juris Hartma-Christopher Morris, Gaurav Rattan, and Petra Mutzel. Weisfeiler and Le-man go sparse: Towards scalable higher-order graph embeddings. In Ad-vances in Neural Information Processing Systems, volume 33, 2020. URL https://proceedings.neurips.cc//paper/2020/file/f81dee42585b3814de199b2e88757f5c-Pa 2, 28 nis on the 1, 2, 7, 25, 28 Martin Otto.Pablo Barcel\u00f3, Egor V Kostylev, Mikael Monet, Jorge P\u00e9rez, Juan Reutter, and Juan PabloSilva.The logical expressiveness of graph neural networks.In Proceedings ofthe 8th International Conference on Learning Representations, ICLR, 2020.URLhttps://openreview.net/forum?id=r1lZ7AEKvB. 3, 6, 7, 8, 10, 18, 38Pablo Barcel\u00f3,Floris Geerts,Juan L. Reutter,and Maksimilian Ryschkov.Graph neural networks with local graph parameters.In Advancesin Neural Information Processing Systems,volume 34,2021.URL 1, 2, 7, 10,https://proceedings.neurips.cc/paper/2021/hash/d4d8d1ac7e00e9105775a6b660dd3cbb-Abs 23, 25, 3828, 29ManzilZaheer,SatwikKottur,SiamakRavanbakhsh,BarnabasPoczos,Russ R Salakhutdinov, and Alexander J Smola.Deep sets."}], "formulas": [{"formula_id": "formula_0", "formula_text": "V G with [n]. For a vertex v \u2208 V G , N G (v) := {u \u2208 V G | vu \u2208 E G }.", "formula_coordinates": [3.0, 108.0, 284.85, 267.93, 10.66]}, {"formula_id": "formula_1", "formula_text": "(G) := {{cr (t) (G, v) | v \u2208 V G }}.", "formula_coordinates": [3.0, 351.0, 345.52, 133.53, 10.82]}, {"formula_id": "formula_2", "formula_text": "(t) k (G, v) | v \u2208 V k G", "formula_coordinates": [3.0, 415.08, 470.33, 77.43, 15.13]}, {"formula_id": "formula_3", "formula_text": "f : G 0 \u2192 F is called invariant if f (G) = f (\u03c3 \u22c6 G) for any permutation \u03c0. More generally, f : G s \u2192 F is equivariant if f (\u03c3 \u22c6 G, \u03c3 \u22c6 v) = f", "formula_coordinates": [3.0, 108.0, 579.45, 396.06, 33.24]}, {"formula_id": "formula_4", "formula_text": "F \u2032 is := \u03c3 j\u2208[n] A ij \u2022 t\u2208[\u2113] W ts \u2022 F jt .", "formula_coordinates": [4.0, 216.72, 174.17, 178.56, 14.05]}, {"formula_id": "formula_5", "formula_text": "\u03c8 s (x 1 ) = \u03c3 x2 E(x 1 , x 2 ) \u2022 t\u2208[\u2113] W ts \u2022 P t (x 2", "formula_coordinates": [4.0, 195.48, 248.73, 203.29, 12.22]}, {"formula_id": "formula_6", "formula_text": "\u03d5 := 1 x op y | E(x, y) | P s (x) | \u03d5 \u2022 \u03d5 | \u03d5 + \u03d5 | a \u2022 \u03d5 | f (\u03d5, . . . , \u03d5) | x \u03d5", "formula_coordinates": [4.0, 153.12, 416.85, 305.8, 12.21]}, {"formula_id": "formula_7", "formula_text": "[[E(x, y), \u03bd]] G := if \u03bd(x)\u03bd(y) \u2208 E G then 1 else 0 [[P s (x), \u03bd]] G := col G (\u03bd(x)) s \u2208 R [[1 x op y , \u03bd]] G := if \u03bd(x) op \u03bd(y) then 1 else 0.", "formula_coordinates": [4.0, 134.4, 616.41, 343.19, 21.58]}, {"formula_id": "formula_8", "formula_text": "[[\u03d5 1 \u2022 \u03d5 2 , \u03bd]] G := [[\u03d5 1 , \u03bd]] G \u2022 [[\u03d5 2 , \u03bd]] G [[\u03d5 1 +\u03d5 2 , \u03bd]] G := [[\u03d5 1 , \u03bd]] G + [[\u03d5 2 , \u03bd]] G [[ x \u03d5 1 , \u03bd]] G := v\u2208VG [[\u03d5 1 , \u03bd[x \u2192 v]]] G [[a \u2022 \u03d5 1 , \u03bd]] G := a \u2022 [[\u03d5 1 , \u03bd]] G [[f (\u03d5 1 , . . . , \u03d5 p ), \u03bd]] G :=f ([[\u03d5 1 , \u03bd]] G , . . . , [[\u03d5 p , \u03bd]] G )", "formula_coordinates": [4.0, 124.92, 670.41, 358.28, 33.34]}, {"formula_id": "formula_9", "formula_text": ", v]] G denotes [[\u03d5(x), x \u2192 v]] G . To illustrate the semantics, for each v \u2208 V G , our example expressions satisfy [[\u03c8 s , v]] G = F \u2032 vs for F \u2032 = \u03c3(A \u2022 F \u2022 W )", "formula_coordinates": [5.0, 108.0, 95.37, 396.0, 32.34]}, {"formula_id": "formula_10", "formula_text": "k-MPNNs. Consider a function f : G s \u2192 R \u2113 : (G, v) \u2192 f (G, v) \u2208 R \u2113 for some \u2113 \u2208 N.", "formula_coordinates": [5.0, 108.0, 139.37, 396.1, 22.36]}, {"formula_id": "formula_11", "formula_text": "v \u2208 V s G : f (G, v) = [[\u03d5 1 , v]] G , . . . , [[\u03d5 \u2113 , v]] G .", "formula_coordinates": [5.0, 230.76, 161.33, 273.13, 30.61]}, {"formula_id": "formula_12", "formula_text": "F \u2032 = \u03c3(A \u2022 F \u2022 W ) as a function f : G 1 \u2192 R \u2113 such that f (G, v) := F \u2032 v:", "formula_coordinates": [5.0, 108.0, 230.09, 301.17, 13.09]}, {"formula_id": "formula_13", "formula_text": "s \u2208 [\u2113], [[\u03c8 s , v]] G = F \u2032 vs with \u03c8 s \u2208 TL 2 (\u03c3). Hence, f (G, v) = ([[\u03c8 1 , v]] G , . . . , [[\u03c8 \u2113 , v]] G", "formula_coordinates": [5.0, 129.12, 241.13, 353.24, 12.97]}, {"formula_id": "formula_14", "formula_text": "G s \u2192 R \u2113 represented in TL(\u2126) is equivariant (invariant if s = 0).", "formula_coordinates": [5.0, 246.6, 301.25, 257.49, 12.01]}, {"formula_id": "formula_15", "formula_text": "G 1 \u2192 R \u2113 f . The equivalence relation \u03c1 1 (F ) is defined by F on G 1 as follows: (G, v), (H, w) \u2208 \u03c1 1 (F ) \u21d0\u21d2 \u2200f \u2208 F , f (G, v) = f (H, w).", "formula_coordinates": [5.0, 108.0, 489.17, 396.18, 24.01]}, {"formula_id": "formula_16", "formula_text": "for all k \u2265 1, \u03c1 1 (vwl (t) k+1 ) \u2282 \u03c1 1 (vwl (t) k ) and \u03c1 1 (vwl (t) 1 ) \u2282 \u03c1 1 (cr (t) ) (Otto", "formula_coordinates": [5.0, 140.52, 560.92, 277.44, 13.34]}, {"formula_id": "formula_17", "formula_text": "sd(1 x op y ) = sd(E(x, y)) = sd(P s (x)) := 0, sd(\u03d5 1 \u2022 \u03d5 2 ) = sd(\u03d5 1 + \u03d5 2 ) := max{sd(\u03d5 1 ), sd(\u03d5 2 )}, sd(a \u2022 \u03d5 1 ) := sd(\u03d5 1 ), sd(f (\u03d5 1 , . . . , \u03d5 p )) := max{sd(\u03d5 i )|i \u2208 [p]", "formula_coordinates": [6.0, 108.0, 162.09, 396.09, 32.49]}, {"formula_id": "formula_18", "formula_text": "x i , x j ) \u2022 \u03d5(x j ) , for i, j \u2208 [2].", "formula_coordinates": [6.0, 328.19, 366.45, 120.94, 10.66]}, {"formula_id": "formula_19", "formula_text": "f : G \u2192 R \u2113 f , as the equivalence relation over G defined in analogy to \u03c1 1 : (G, H) \u2208 \u03c1 0 (F ) \u21d0\u21d2 \u2200f \u2208 F , f (G) = f (H).", "formula_coordinates": [6.0, 108.0, 530.93, 396.0, 22.24]}, {"formula_id": "formula_20", "formula_text": "(t) k+1 ) \u2282 \u03c1 0 (gwl (t) k ) but different from vertex embeddings, \u03c1 0 (gcr (t) ) = \u03c1 0 (gwl (t)", "formula_coordinates": [6.0, 108.0, 554.44, 396.03, 25.34]}, {"formula_id": "formula_21", "formula_text": "(1) \u03c1 0 gcr (t) = \u03c1 0 TL (t+1) 2 (\u2126) = \u03c1 0 gwl (t) 1 (2) \u03c1 0 gwl (\u221e) k = \u03c1 0 TL k+1 (\u2126) .", "formula_coordinates": [6.0, 129.24, 641.73, 353.52, 13.3]}, {"formula_id": "formula_22", "formula_text": "x 1 ) = x2 x3 E(x 1 , x 2 ) \u2022 E(x 2 , x 3 ) in TL (", "formula_coordinates": [7.0, 220.71, 208.36, 180.35, 13.22]}, {"formula_id": "formula_23", "formula_text": "x 1 ) = x2 E(x 1 , x 2 ) \u2022 x1 E(x 2 , x 1 ) in TL (2)", "formula_coordinates": [7.0, 278.45, 231.76, 195.02, 13.22]}, {"formula_id": "formula_24", "formula_text": ") : G 1 \u2192 R \u21130 : (G, v) \u2192 F (0) v: := col G (v) \u2208 R \u21130 . For layer t > 0, gin (t) : G 1 \u2192 R \u2113t is given by: (G, v) \u2192 F (", "formula_coordinates": [7.0, 108.0, 436.01, 396.08, 25.69]}, {"formula_id": "formula_25", "formula_text": "\u2113 t\u22121 expressions \u03d5 (t\u22121) i (x 1 ) in GTL (t\u22121) (\u2126) representing gin (t\u22121) . That is, we have [[\u03d5 (t\u22121) i , v]] G = F (t\u22121)", "formula_coordinates": [7.0, 108.0, 491.73, 395.53, 24.7]}, {"formula_id": "formula_26", "formula_text": "mlp (t) i \u03d5 (t\u22121) 1 (x 1 ), . . . , \u03d5 (t\u22121) \u2113t\u22121 (x 1 ), x2 E(x 1 , x 2 ) \u2022 \u03d5 (t\u22121) 1 (x 2 ), . . . , x2 E(x 1 , x 2 ) \u2022 \u03d5 (t\u22121) \u2113t\u22121 (x 2 ) ,", "formula_coordinates": [7.0, 109.92, 538.48, 392.16, 13.22]}, {"formula_id": "formula_27", "formula_text": "[[\u03d5 (t) i , v]] G = F (t) v,i for each v \u2208 V G and i \u2208 [\u2113 t ]", "formula_coordinates": [7.0, 108.0, 562.0, 396.08, 24.38]}, {"formula_id": "formula_28", "formula_text": "x2 x3 x4 E(x 1 , x 2 ) \u2022 E(x 2 , x 3 ) \u2022 E(x 3 , x 4", "formula_coordinates": [8.0, 314.28, 639.69, 182.77, 12.22]}, {"formula_id": "formula_29", "formula_text": "x2 x3 E(x 1 , x 2 ) \u2022 E(x 1 , x 3 ) \u2022 E(x 2 , x 3 ).", "formula_coordinates": [8.0, 118.56, 686.49, 173.37, 12.22]}, {"formula_id": "formula_30", "formula_text": "f 1 : G s \u2192 R p and f 2 : G s \u2192 R q are in F , then g := (f 1 , f 2 ) : G s \u2192 R p+q : (G, v) \u2192 (f 1 (G, v), f 2 (G, v)) is also in F . Second, F is function-closed, for a fixed \u2113 \u2208 N: for any f \u2208 F such that f : G s \u2192 R p , also g \u2022 f : G s \u2192 R \u2113 is in F for any continuous function g : R p \u2192 R \u2113 .", "formula_coordinates": [9.0, 108.0, 228.77, 396.0, 33.97]}, {"formula_id": "formula_31", "formula_text": "F \u2113 = {f : G s \u2192 R \u2113 | \u03c1 s (F ) \u2286 \u03c1 s (f )}.", "formula_coordinates": [9.0, 133.2, 300.65, 157.77, 12.01]}, {"formula_id": "formula_32", "formula_text": "F \u2113 = {f : G s \u2192 R \u2113 | \u03c1(alg) \u2286 \u03c1(f )}.", "formula_coordinates": [9.0, 108.0, 387.81, 396.0, 21.0]}, {"formula_id": "formula_33", "formula_text": "(x) in TL(\u2126) either [[\u03d5, \u03c3 \u22c6 v]] \u03c3\u22c6G = [[\u03d5, v]] G , or when \u03d5 has no free index variables, [[\u03d5]] \u03c3\u22c6G = [[\u03d5]] G .", "formula_coordinates": [14.0, 108.0, 700.53, 395.97, 21.57]}, {"formula_id": "formula_34", "formula_text": "[[1 xi op xj , \u03bd]] G = 1 vi op vj = 1 \u03c3(vi) op \u03c3(vj ) = [[1 xi op xj , \u03c3 \u22c6 \u03bd]] \u03c3\u22c6G ,", "formula_coordinates": [15.0, 173.28, 101.49, 265.44, 11.01]}, {"formula_id": "formula_35", "formula_text": "[[P \u2113 , \u00b5]] G = (col(v i )) \u2113 = (col \u03c3 (\u03c3(v i )) \u2113 = [[P \u2113 , \u03c3 \u22c6 \u03bd]] \u03c3\u22c6G ,", "formula_coordinates": [15.0, 190.68, 150.53, 230.64, 12.85]}, {"formula_id": "formula_36", "formula_text": "[[\u03d5, \u03bd]] G = 1 vivj \u2208E = 1 \u03c3(vi)\u03c3(vj )\u2208E \u03c3 = [[\u03d5, \u03c3 \u22c6 \u03bd]] \u03c3\u22c6G ,", "formula_coordinates": [15.0, 195.36, 203.97, 221.28, 15.54]}, {"formula_id": "formula_37", "formula_text": "\u2022 If \u03d5(x) = \u03d5 1 (x 1 ) \u2022 \u03d5 2 (x 2 ), then for a valuation \u03bd from x to V : [[\u03d5, \u03bd]] G = [[\u03d5 1 , \u03bd]] G \u2022 [[\u03d5 2 , \u03bd]] G = [[\u03d5 1 , \u03c3 \u22c6 \u03bd]] \u03c3\u22c6G \u2022 [[\u03d5 2 , \u03c3 \u22c6 \u03bd]] \u03c3\u22c6G = [[\u03d5, \u03c3 \u22c6 \u03bd]] \u03c3\u22c6G ,", "formula_coordinates": [15.0, 108.0, 238.05, 364.92, 27.81]}, {"formula_id": "formula_38", "formula_text": "\u2022 If \u03d5(x) = f (\u03d5 1 (x 1 ), . . . , \u03d5 p (x p )), then [[\u03d5, \u03bd]] G = f ([[\u03d5 1 , \u03bd]] G , . . . , [[\u03d5 p , \u03bd]] G ) = f ([[\u03d5 1 , \u03c3 \u22c6 \u03bd]] \u03c3\u22c6G , . . . , [[\u03d5 p , \u03c3 \u22c6 \u03bd]] \u03c3\u22c6G ) = [[\u03d5, \u03c3 \u22c6 \u03bd]] \u03c3\u22c6G ,", "formula_coordinates": [15.0, 108.0, 300.21, 297.87, 55.65]}, {"formula_id": "formula_39", "formula_text": "[[\u03d5, \u03bd]] G = v\u2208V [[\u03d5 1 , \u03bd[y \u2192 v]]] G = v\u2208V [[\u03d5 1 , \u03c3 \u22c6 \u03bd[y \u2192 v]]] \u03c3\u22c6G = v\u2208V \u03c3 [[\u03d5 1 , \u03c3 \u22c6 \u03bd[y \u2192 v]]] \u03c3\u22c6G = [[\u03d5, \u03c3 \u22c6 \u03bd]] \u03c3\u22c6G ,", "formula_coordinates": [15.0, 182.16, 401.25, 247.04, 49.06]}, {"formula_id": "formula_40", "formula_text": "\u03d5 := (x i = x j ) | E(x i , x j ) | P s (x i ) | \u00ac\u03d5 | \u03d5 \u2227 \u03d5 | \u2203 \u2265m x i \u03d5,", "formula_coordinates": [15.0, 179.76, 720.53, 252.48, 12.49]}, {"formula_id": "formula_41", "formula_text": "{x 1 , . . . , x k } \u2192 V G , we define [[\u03d5, \u00b5]] B G \u2208 B", "formula_coordinates": [16.0, 108.0, 145.17, 395.97, 22.9]}, {"formula_id": "formula_42", "formula_text": "[[x i = x j , \u00b5]] B G := if \u00b5(x i ) = \u00b5(x j ) then \u22a4 else \u22a5; [[E(x i , x j ), \u00b5]] B G := if \u00b5(x i )\u00b5(x j ) \u2208 E G then \u22a4 else \u22a5; [[P s (x i ), \u00b5]] B G := if col G (\u00b5(x i )) s = 1 then \u22a4 else \u22a5; [[\u00ac\u03d5, \u00b5]] B G := \u00ac[[\u03d5, \u00b5]] B G ; [[\u03d5 1 \u2227 \u03d5 2 , \u00b5]] B G := [[\u03d5 1 , \u00b5]] B G \u2227 [[\u03d5 2 , \u00b5]] B G ; [[\u2203 \u2265m x i \u03d5 1 , \u00b5]] B G := if |{v \u2208 V G | [[\u03d5, \u00b5[x i \u2192 v]]] B G = \u22a4}| \u2265 m then \u22a4 else \u22a5. In the last expression, \u00b5[x i \u2192 v]", "formula_coordinates": [16.0, 108.0, 174.02, 356.64, 110.68]}, {"formula_id": "formula_43", "formula_text": "\u2203 \u2265m x 2 (E(x 1 , x 2 ) \u2227 \u03d5(x 2 )) or \u2203 \u2265m x 1 (E(x 2 , x 1 ) \u2227 \u03d5(x 1 )). The semantics of formulae in GC is inherited from formulae in C 2 .", "formula_coordinates": [16.0, 108.0, 311.57, 395.91, 22.93]}, {"formula_id": "formula_44", "formula_text": "[[ \u03b1\u2208A \u03d5 \u03b1 , \u00b5]] B G = \u22a4 if for at least one \u03b1 \u2208 A, [[\u03d5 \u03b1 , \u00b5]] B G = \u22a4, and [[ \u03b1\u2208A \u03d5 \u03b1 , \u00b5]] B G = \u22a4 if for all \u03b1 \u2208 A, [[\u03d5 \u03b1 , \u00b5]] B G = \u22a4.", "formula_coordinates": [16.0, 108.0, 420.98, 396.01, 25.84]}, {"formula_id": "formula_45", "formula_text": "(G, v), (H, w) \u2208 \u03c1 1 L (t) \u21d0\u21d2 \u2200\u03d5(x) \u2208 L (t) : [[\u03d5, \u00b5 v ]] B G = [[\u03d5, \u00b5 w ]] B H", "formula_coordinates": [16.0, 166.92, 519.02, 278.8, 13.12]}, {"formula_id": "formula_46", "formula_text": "(G, v), (H, w) \u2208 \u03c1 s L (t) \u21d0\u21d2 \u2200\u03d5(x) \u2208 L (t) : [[\u03d5, \u00b5 v ]] B G = [[\u03d5, \u00b5 w ]] B", "formula_coordinates": [16.0, 165.0, 599.18, 280.88, 13.12]}, {"formula_id": "formula_47", "formula_text": "(1) \u03c1 1 cr (t) = \u03c1 1 GC (t) and \u03c1 0 gcr (t) = \u03c1 0 gwl (t) 1 = \u03c1 0 C 2,(t+1) ; (2) For k \u2265 1, \u03c1 1 vwl (t) k = \u03c1 1 C k+1,(t) and \u03c1 0 C k+1,(t+k) \u2286 \u03c1 0 gwl (t) k \u2286 \u03c1 0 C k+1,(t+1) . As a consequence, \u03c1 0 gwl (\u221e) k = \u03c1 0 C k+1 .", "formula_coordinates": [17.0, 127.32, 82.97, 293.52, 78.85]}, {"formula_id": "formula_48", "formula_text": "1 = \u03c1 0 C 2,(", "formula_coordinates": [17.0, 282.24, 202.61, 61.16, 13.21]}, {"formula_id": "formula_49", "formula_text": "are not in \u03c1 1 C k+1,(t) . Let \u03d5(x 1 ) be a formula in C k+1,(t) such that [[\u03d5, v]] B G = [[\u03d5, w]] B H . Con- sider the formula \u03d5 + (x 1 , . . . , x k ) = \u03d5(x 1 ) \u2227 k i=1 (x 1 = x i ). Then, [[\u03d5 + , (v, . . . , v)]] B G = [[\u03d5 + , (w, . . . , w)]] B", "formula_coordinates": [17.0, 108.0, 256.73, 396.08, 38.08]}, {"formula_id": "formula_50", "formula_text": "\u03c1 1 vwl (t) k , from which the inclusion \u03c1 1 vwl (t) k \u2286 \u03c1 1 C k+1,(t) follows. Conversely, if (G, v) and (H, w) are not in \u03c1 1 vwl (t) k , then wl (t) k (G, (v, . . . , v)) = wl (t)", "formula_coordinates": [17.0, 108.0, 309.88, 396.06, 39.74]}, {"formula_id": "formula_51", "formula_text": "are not in \u03c1 k C k+1,(t) either. Let \u03d5(x 1 , . . . , x k ) be a formula in C k+1,(t) such that [[\u03d5, (v, . . . , v)]] B G = [[\u03d5, (w, . . . , w)]] B H . Then it is readily shown that we can convert \u03d5(x 1 , . . . , x k ) into a formula \u03d5 \u2212 (x 1 ) in C k+1,(t) such that [[\u03d5 \u2212 , v]] B G = [[\u03d5 \u2212 , w]] B", "formula_coordinates": [17.0, 108.0, 349.01, 396.03, 38.41]}, {"formula_id": "formula_52", "formula_text": "\u03c1 1 vwl (t) k \u2287 \u03c1 1 C k+1,(", "formula_coordinates": [17.0, 331.92, 386.69, 92.74, 13.57]}, {"formula_id": "formula_53", "formula_text": "\u03c1 0 C k+1,(t+k) \u2286 \u03c1 0 gwl (t) k \u2286 \u03c1 0 C k+1,(t+1) . Clearly, if (G, H) is not in \u03c1 0 gwl (t) k", "formula_coordinates": [17.0, 108.0, 415.61, 395.94, 26.89]}, {"formula_id": "formula_54", "formula_text": "0 C k+1,(t+k) \u2286 \u03c1 0 gwl (t) k follows. For \u03c1 0 gwl (t) k \u2286 \u03c1 0 C k+1,(t+1) , we show that if (G, H) is in \u03c1 0 gwl (t) k , then this implies that [[\u03d5, \u00b5]] B G = [[\u03d5, \u00b5]] B", "formula_coordinates": [17.0, 108.0, 487.01, 396.01, 54.49]}, {"formula_id": "formula_55", "formula_text": "[\u03d5, \u00b5]] B G = \u22a4 but [[\u03d5, \u00b5]] B H = \u22a5. Since [[\u03d5, \u00b5]] B G = \u22a4, there must be at least m elements satisfying \u03c8. More precisely, let v 1 , . . . , v p in G be all vertices in G such that for each valuation \u00b5[x \u2192 v i ] it holds that [[\u03c8, \u00b5[x \u2192 v i ]] B G = \u22a4.", "formula_coordinates": [17.0, 108.0, 574.1, 396.09, 37.24]}, {"formula_id": "formula_56", "formula_text": "(t\u22121) k (G, (v i , . . . , v i )) is the same, for each such v i . Now since gwl (t\u22121) k (G) = gwl (t\u22121) k (H)", "formula_coordinates": [17.0, 108.0, 623.93, 289.65, 35.53]}, {"formula_id": "formula_57", "formula_text": "(t\u22121) k (G, (v i , . . . , v i )) = wl (t\u22121) k", "formula_coordinates": [17.0, 246.36, 658.73, 128.27, 15.13]}, {"formula_id": "formula_58", "formula_text": "[x \u2192 w i ] it holds that [[\u03c8, \u00b5[x \u2192 w i ]] B H = \u22a4.", "formula_coordinates": [17.0, 108.0, 683.49, 396.01, 23.01]}, {"formula_id": "formula_59", "formula_text": "[[\u03d5, \u00b5 v ]] B G = [[\u03d5, \u00b5 w ]] B", "formula_coordinates": [18.0, 327.24, 270.26, 85.16, 12.88]}, {"formula_id": "formula_60", "formula_text": "such that [[\u03c8, \u00b5 v ]] B G = [[\u03c8, \u00b5 w ]] B H .", "formula_coordinates": [18.0, 340.56, 292.1, 132.33, 12.88]}, {"formula_id": "formula_61", "formula_text": ") in C k \u221e\u03c9 such that [[\u03d5, v]] G = c if and only if [[\u03c6 c , v]] B G = \u22a4 for any graph G = (V G , E G , col G ) in G and v \u2208 V k G . Furthermore, if \u03d5(x) \u2208 GTL(\u2126) then\u03c6 c \u2208 GC \u221e\u03c9 .", "formula_coordinates": [18.0, 108.0, 459.45, 396.03, 38.7]}, {"formula_id": "formula_62", "formula_text": "= x i = x i .", "formula_coordinates": [18.0, 460.29, 568.77, 43.68, 10.65]}, {"formula_id": "formula_63", "formula_text": "c 1 , c 2 \u2208 R such that [[\u03d5 1 , v]] G = c 1 and [[\u03d5 2 , v]] G = c 2 and c = c 1 + c 2 .", "formula_coordinates": [18.0, 143.88, 674.25, 360.13, 21.58]}, {"formula_id": "formula_64", "formula_text": "\u03d5 c := c1,c2\u2208R c=c1+c2\u03c6 c1 1 \u2227\u03c6 c2 2 , where\u03c6 c1 1 and\u03c6 c2 2 are the expressions such that [[\u03d5 1 , v]] G = c 1 if and only if [[\u03c6 c1 1 , v]] B G = \u22a4 and [[\u03d5 2 , v]] G = c 2 if and only if [[\u03c6 c2 2 , v]] B G = \u22a4, which exist by induction. \u2022 \u03d5 := \u03d5 1 \u2022 \u03d5 2", "formula_coordinates": [18.0, 273.12, 704.57, 101.64, 30.01]}, {"formula_id": "formula_65", "formula_text": "] G = c if and only if there are c 1 , c 2 \u2208 R such that [[\u03d5 1 , v]] G = c 1 and [[\u03d5 2 , v]] G = c 2 and c = c 1 \u2022 c 2 .", "formula_coordinates": [19.0, 143.88, 115.41, 360.18, 21.57]}, {"formula_id": "formula_66", "formula_text": "c := c1,c2\u2208R c=c1\u2022c2\u03c6 c1 1 \u2227\u03c6 c2 2 .", "formula_coordinates": [19.0, 281.04, 148.01, 92.28, 28.93]}, {"formula_id": "formula_67", "formula_text": "c 1 \u2208 R such that [[\u03d5 1 , v]] G = c 1 and c = a \u2022 c 1 .", "formula_coordinates": [19.0, 210.36, 198.69, 185.49, 10.65]}, {"formula_id": "formula_68", "formula_text": "\u03d5 c := c1\u2208R c=a\u2022c1\u03c6 c1", "formula_coordinates": [19.0, 289.32, 216.65, 65.55, 29.05]}, {"formula_id": "formula_69", "formula_text": "are c 1 , . . . , c p \u2208 R such that c = f (c 1 , . . . , c p ) and [[\u03d5 i , v]] G = c i for i \u2208 [p]. Hence, it suffices to define\u03c6 c := c1,...,cp\u2208R c=f (c1,...,cp)\u03c6 c1 1 \u2227 \u2022 \u2022 \u2022 \u2227\u03c6 cp p . \u2022 \u03d5 := xi \u03d5 1 . We observe that [[\u03d5, \u00b5]] G = c implies that we can partition V G into \u2113 parts V 1 , . . . , V \u2113 , of sizes m 1 , . . . , m \u2113 , respectively, such that [[\u03d5 1 , \u00b5[x i \u2192 v]]] G = c i for each v \u2208 V i ,", "formula_coordinates": [19.0, 135.36, 270.09, 368.8, 99.46]}, {"formula_id": "formula_70", "formula_text": "\u03d5 c := \u2113,m1,...,m \u2113 \u2208N c1,...,c \u2113 \u2208R c= \u2113 i=1 mici \u2113 i=1 \u2203 =mi x i\u03c6 ci 1 \u2227 \u2200x i \u2113 i=1\u03c6 ci 1 ,", "formula_coordinates": [19.0, 224.4, 387.29, 199.08, 48.97]}, {"formula_id": "formula_71", "formula_text": "\u03d5 c := \u2113,m1,...,m \u2113 \u2208N c1,...,c \u2113 \u2208R c= \u2113 i=1 mici m= \u2113 i=1 mi \u2203 =m x 2 E(x 1 , x 2 ) \u2227 \u2113 i=1 \u2203 =mi x 2 E(x 1 , x 2 ) \u2227\u03c6 ci 1 (x 2 ),", "formula_coordinates": [19.0, 159.36, 569.21, 293.28, 58.69]}, {"formula_id": "formula_72", "formula_text": "\u2022 \u03c1 1 cr (t) \u2286 \u03c1 1 GTL (t) (\u2126) ; \u2022 \u03c1 1 vwl (t) k \u2286 \u03c1 1 TL (t)", "formula_coordinates": [19.0, 135.36, 721.41, 119.36, 11.62]}, {"formula_id": "formula_73", "formula_text": "\u2022 \u03c1 0 gwl (t) k \u2286 \u03c1 0 TL (t+1) k+1 (\u2126) .", "formula_coordinates": [20.0, 135.36, 104.69, 130.41, 15.13]}, {"formula_id": "formula_74", "formula_text": "] G = c = c \u2032 = [[\u03d5, w]] H . From Proposition C.3 we know that there exists a formula\u03c6 c in C k+1,(t) \u221e\u03c9 such that [[\u03c6 c , v]] B G = \u22a4 and [[\u03c6 c , w]] B H = \u22a5.", "formula_coordinates": [20.0, 108.0, 157.37, 395.98, 31.18]}, {"formula_id": "formula_75", "formula_text": "\u2022 \u03c1 1 GTL (t) (\u2126) \u2286 \u03c1 1 cr (t) ; \u2022 \u03c1 1 TL (t) k+1 (\u2126) \u2286 \u03c1 1 vwl (", "formula_coordinates": [20.0, 135.36, 324.69, 119.36, 34.42]}, {"formula_id": "formula_76", "formula_text": "\u2022 \u03c1 0 TL (t+k) k+1 (\u2126) \u2286 \u03c1 0 gwl (t) k .", "formula_coordinates": [20.0, 135.36, 367.13, 130.77, 15.25]}, {"formula_id": "formula_77", "formula_text": "graph G in G, [[\u03d5, v]] B G = \u22a4 implies [[\u03c6, v]] G = 1 and [[\u03d5, v]] B G = \u22a5 implies [[", "formula_coordinates": [20.0, 108.0, 438.38, 328.68, 12.88]}, {"formula_id": "formula_78", "formula_text": "B G = \u22a4 = [[\u03d5, w]] B H = \u22a5.", "formula_coordinates": [20.0, 351.36, 708.98, 98.61, 12.88]}, {"formula_id": "formula_79", "formula_text": "[[ xi \u03d5 1 , \u03bd]] G := v\u2208VG [[\u03d5 1 , \u03bd[x i \u2192 v]]] G ,", "formula_coordinates": [21.0, 218.64, 280.17, 174.72, 12.22]}, {"formula_id": "formula_80", "formula_text": "{{[[\u03d5 1 , \u03bd[x i \u2192 v]]] G | v \u2208 V G }}.", "formula_coordinates": [21.0, 241.92, 326.13, 128.28, 10.65]}, {"formula_id": "formula_81", "formula_text": "aggr F xj (\u03d5) and aggr F xj \u03d5(x j ) | E(x i , x j )", "formula_coordinates": [21.0, 213.0, 442.97, 178.7, 13.57]}, {"formula_id": "formula_82", "formula_text": "[[aggr F xj (\u03d5), \u03bd]] G := F ({{[[\u03d5, \u03bd[x j \u2192 v]]] G | v \u2208 V G }}) \u2208 R. [[aggr F xj \u03d5(x j ) | E(x i , x j ) , \u03bd]] G := F ({{[[\u03d5, \u03bd[x j \u2192 v]]] G | v \u2208 V G , (\u03bd(x i ), v) \u2208 E G }}) \u2208 R.", "formula_coordinates": [21.0, 116.16, 567.29, 379.68, 30.85]}, {"formula_id": "formula_83", "formula_text": "F ({{(([[\u03d5 1 , \u03bd[x j \u2192 v]]] G , . . . , [[\u03d5 \u2113 , \u03bd[x j \u2192 v]]] G ) | v \u2208 V G }}) \u2208 R and F ({{(([[\u03d5 1 , \u03bd[x j \u2192 v]]] G , . . . , [[\u03d5 \u2113 , \u03bd[x j \u2192 v]]] G ) | v \u2208 V G , (\u03bd(x i ), v) \u2208 E G }}) \u2208 R.", "formula_coordinates": [21.0, 108.0, 643.77, 396.06, 21.69]}, {"formula_id": "formula_84", "formula_text": "[[aggr F xj \u03d5(x j ) | E(x i , x j ) , \u03bd]] G = F ({{[[\u03d5, \u03bd[x j \u2192 v]]] | v \u2208 V G , (\u03bd(x i ), v) \u2208 E G }} = F ({{[[\u03d5 \u2022 E(x i , x j ), \u03bd[x j \u2192 v]]] | v \u2208 V G }} = [[aggr F xj (\u03d5(x j ) \u2022 E(x i , x j )), \u03bd]] G .", "formula_coordinates": [21.0, 132.48, 720.53, 342.54, 13.57]}, {"formula_id": "formula_85", "formula_text": "\u03d5 1 (x i ) := aggr avg xj (1 xj =xj \u2022 E(x i , x j )) and \u03d5 2 (x i ) := aggr avg xj (1 xj =xj | E(x i , x j )).", "formula_coordinates": [22.0, 138.24, 219.37, 335.4, 13.25]}, {"formula_id": "formula_86", "formula_text": "x i ) = v. Then, [[\u03d5 1 , \u03bd]] G results in applying the average to the multiset {{1 w=w \u2022 E(v, w) | w \u2208 V G }} which includes the value 1 for every w \u2208 N G (v) and a 0 for every non- neighbor w \u2208 N G (v). In other words, [[\u03d5 1 , \u03bd]] G results in |N G (v)|/|V G |. In contrast, [[\u03d5 2 , \u03bd]] G results in applying the average to the multiset {{1 w=w | w \u2208 V G , (v, w) \u2208 E G }}.", "formula_coordinates": [22.0, 108.0, 244.89, 396.2, 43.54]}, {"formula_id": "formula_87", "formula_text": "[[\u03d5 2 , \u03bd]] G results in |N G (v)|/|N G (v)| = 1.", "formula_coordinates": [22.0, 254.4, 299.61, 170.37, 10.65]}, {"formula_id": "formula_88", "formula_text": "F xj (\u03d5|\u03c8) with as semantics [[aggr F xj (\u03d5|\u03c8), \u03bd]] G := F {{[[\u03d5, \u03bd[x j \u2192 v]]] G | v \u2208 V G , [[\u03c8, \u03bd[x j \u2192 v]] G = 0}}", "formula_coordinates": [22.0, 108.0, 337.13, 396.0, 25.93]}, {"formula_id": "formula_89", "formula_text": "F xj (\u03d5(x j ) | E(x i , x j )), for i, j \u2208 [2].", "formula_coordinates": [22.0, 219.72, 511.61, 148.53, 13.09]}, {"formula_id": "formula_90", "formula_text": "\u2022 \u03c1 1 cr (t) \u2286 \u03c1 1 GTL (t) (\u2126, \u0398) ; \u2022 \u03c1 1 vwl (t) k \u2286 \u03c1 1 TL (t)", "formula_coordinates": [22.0, 135.36, 602.97, 131.48, 36.46]}, {"formula_id": "formula_91", "formula_text": "\u2022 \u03c1 0 gwl (t) k \u2286 \u03c1 0 TL (t+1) k+1 (\u2126, \u0398) .", "formula_coordinates": [22.0, 135.36, 649.49, 142.53, 15.25]}, {"formula_id": "formula_92", "formula_text": "\u2022 \u03d5 := aggr F xi (\u03d5 1 (x i ) | E(x j , x i ))", "formula_coordinates": [23.0, 135.36, 206.69, 139.38, 12.97]}, {"formula_id": "formula_93", "formula_text": "\u03d5 c := \u2113\u2208N (m1,...,m \u2113 )\u2208N \u2113 (c,c1,...,c \u2113 )\u2208C(m1,...,m \u2113 ,F ) \u2203 =m x i E(x j , x i ) \u2227 \u2113 s=1 \u2203 =ms x i E(x j , x i ) \u2227\u03c6 cs 1 (x i )", "formula_coordinates": [23.0, 153.84, 231.29, 340.23, 60.37]}, {"formula_id": "formula_94", "formula_text": "[[aggr F xi (\u03d5), v]] G = c iff [[\u03c6 c , v]] B G = \u22a4, and [[aggr F xi (\u03d5(x i ) | E(x j , x i )), v]] G = c iff [[\u03c6 c , v]] B", "formula_coordinates": [23.0, 108.0, 367.25, 396.0, 24.97]}, {"formula_id": "formula_95", "formula_text": "[[P s (x), \u03bd]] G := col G (\u03bd) s is now an element in R rather than 0 or 1, for each s \u2208 [\u2113].", "formula_coordinates": [23.0, 108.0, 628.77, 396.06, 20.88]}, {"formula_id": "formula_96", "formula_text": "[[P s , v]] B G = \u22a4 iff col G (v) s = 1.", "formula_coordinates": [23.0, 108.0, 699.5, 396.08, 22.6]}, {"formula_id": "formula_97", "formula_text": "[[P s,r (x), \u03bd]] B G := if col G (\u00b5(x i )) s = r then \u22a4 else \u22a5.", "formula_coordinates": [24.0, 199.92, 131.18, 212.16, 13.12]}, {"formula_id": "formula_98", "formula_text": "[[P s (x i ), \u03bd]] G = c if and only if [[P s,c (x i ), \u03bd]] B", "formula_coordinates": [24.0, 108.0, 261.62, 181.76, 11.56]}, {"formula_id": "formula_99", "formula_text": "\u03d5 := 1 r \u2032 \u2208R,r =r \u2032 r \u2212 r \u2032 r \u2032 \u2208R,r =r \u2032 (P s (x i ) \u2212 r \u2032 1 xi=xi ). Then, [[\u03c6, \u03bd]] G evaluates to r \u2032 \u2208R,r =r \u2032 (r \u2212 r \u2032 ) r \u2032 \u2208R,r =r \u2032 (r \u2212 r \u2032 ) = 1 [[P s,r , \u03bd]] = \u22a4 0 [[P s,r , \u03bd]] = \u22a5 .", "formula_coordinates": [24.0, 108.0, 364.41, 309.0, 76.74]}, {"formula_id": "formula_100", "formula_text": "[\u03d5, v]] B G = \u22a4 = [[\u03d5, w]] B H = \u22a5.", "formula_coordinates": [24.0, 377.07, 535.82, 127.02, 12.88]}, {"formula_id": "formula_101", "formula_text": "] G = [[\u03c6, w]] H . Hence, (G, v) and (H, w) is not in \u03c1 1 GTL (t) (\u2126)", "formula_coordinates": [24.0, 108.0, 570.93, 395.94, 22.9]}, {"formula_id": "formula_102", "formula_text": "= P i (x 1 ) for i \u2208 [d 0 ]. We have F (0) vj = [[\u03d5 (0) j , v]] G for j \u2208 [d 0 ]", "formula_coordinates": [25.0, 148.89, 134.32, 242.06, 12.98]}, {"formula_id": "formula_103", "formula_text": "\u2208 [d t\u22121 ] let \u03d5 (t\u22121) i (x 1 ) be expressions in GTL (t\u22121) (\u03c3) representing them. We assume that, for each i \u2208 [d t\u22121 ], F (t\u22121) vi = [[\u03d5 (t\u22121) i , v]] G .", "formula_coordinates": [25.0, 108.0, 177.4, 396.03, 38.66]}, {"formula_id": "formula_104", "formula_text": "F (t) := \u03c3 F (t\u22121) \u2022 V (t) + A \u2022 F (t\u22121) \u2022 W (t) + B (t) ,", "formula_coordinates": [25.0, 199.2, 237.4, 213.6, 10.61]}, {"formula_id": "formula_105", "formula_text": "\u03d5 (t) j (x 1 ), for j \u2208 [d t ]: \u03c3 \uf8eb \uf8ed dt\u22121 i=1 V (t) ij \u2022 \u03d5 (t\u22121) i (x 1 ) + x2 E(x 1 , x 2 ) \u2022 dt\u22121 i=1 W (t) ij \u2022 \u03d5 (t\u22121) i (x 2 ) + b (t) j \u2022 1 x1=x1 \uf8f6 \uf8f8 .", "formula_coordinates": [25.0, 121.44, 278.08, 369.12, 53.78]}, {"formula_id": "formula_106", "formula_text": "W (t) ij , V(", "formula_coordinates": [25.0, 132.84, 341.2, 34.14, 12.98]}, {"formula_id": "formula_107", "formula_text": "\u03d5 j := ro j x1 \u03d5 (t\u22121) j (x 1 )", "formula_coordinates": [25.0, 108.0, 479.13, 395.94, 25.06]}, {"formula_id": "formula_108", "formula_text": "\u03c1 0 gcr (t) \u2286 \u03c1 0 (TL (t+1) 2 (\u2126) \u2286 \u03c1 0 bGNN (t) + readout .", "formula_coordinates": [25.0, 192.48, 520.24, 227.04, 12.86]}, {"formula_id": "formula_109", "formula_text": "\u03d5 (t) j (x 1 ), for j \u2208 [d t ]. \u03c3 \uf8eb \uf8ed dt\u22121 i=1 V (t) ij \u2022 \u03d5 (t\u22121) i (x 1 ) + dt\u22121 i=1 W (t) ij \u2022 aggr F x2 \u03d5 (t\u22121) i (x 2 ) | E(x 1 , x 2 ) + b (t) j \u2022 1 x1=x1 \uf8f6 \uf8f8 ,", "formula_coordinates": [25.0, 122.64, 577.48, 371.97, 53.78]}, {"formula_id": "formula_110", "formula_text": "F (t) := \u03c3(D \u22121/2 (I + A)D \u22121/2 \u2022 F (t\u22121) W (t) + B (", "formula_coordinates": [26.0, 108.0, 84.16, 395.94, 22.67]}, {"formula_id": "formula_111", "formula_text": "\u221a x+1 : R \u2192 R : x \u2192 1 \u221a x+1", "formula_coordinates": [26.0, 160.56, 105.65, 119.77, 15.01]}, {"formula_id": "formula_112", "formula_text": "GTL (t+1) (\u03c3, 1 \u221a x+1 ) expressions \u03d5 (t) j (x 1 ) := \u03c3 f 1/ \u221a x+1 x2 E(x 1 , x 2 ) \u2022 dt\u22121 i=1 W (t) ij \u2022 \u03d5 (t\u22121) i (x 1 ) \u2022 f 1/ \u221a x+1 x2 E(x 1 , x 2 ) +f 1/ \u221a x+1 x2 E(x 1 , x 2 ) \u2022 x2 E(x 1 , x 2 )\u2022f 1/ \u221a x+1 x1 E(x 2 , x 1 ) \u2022 dt\u22121 i=1 W (t) ij \u2022\u03d5 (t\u22121) i (x 2 ) ,", "formula_coordinates": [26.0, 108.48, 121.01, 385.56, 91.21]}, {"formula_id": "formula_113", "formula_text": "GTL (2t) (\u03c3, 1 \u221a x+1", "formula_coordinates": [26.0, 178.8, 248.09, 65.05, 15.01]}, {"formula_id": "formula_114", "formula_text": "\u03c1 1 cr (2t) \u2286 \u03c1 1 GTL (2t) (\u2126) \u2286 \u03c1 1 GCN (", "formula_coordinates": [26.0, 216.6, 273.53, 166.26, 12.61]}, {"formula_id": "formula_115", "formula_text": "F := \u03c3 (D \u22121/2 (I + A G )D \u22121/2 )) p \u2022F (0)", "formula_coordinates": [26.0, 108.0, 369.29, 395.94, 25.45]}, {"formula_id": "formula_116", "formula_text": "\u03d5 (t) j (x 1 ) := \u03c3 \uf8eb \uf8ed x2 \u2022 \u2022 \u2022 xp+1 p k=1 E(x k , x k+1 ) \u2022 d0 i=1 W ij \u2022 \u03d5 (0) i (x p+1 ) \uf8f6 \uf8f8 , for j \u2208 [d 1 ]", "formula_coordinates": [26.0, 108.0, 419.31, 345.6, 53.67]}, {"formula_id": "formula_117", "formula_text": "\u03c3 x2 E(x 1 , x 2 ) \u2022 x1 E(x 2 , x 1 ) \u2022 x2 E(x 1 , x 2 ) \u2022 d0 i=1 W ij \u2022 \u03d5 (0) i (x 2 ) ,", "formula_coordinates": [26.0, 142.44, 511.61, 327.12, 31.33]}, {"formula_id": "formula_118", "formula_text": "G (t) vj = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 mean {{mlp j (F (t\u22121)", "formula_coordinates": [26.0, 160.44, 681.27, 123.67, 44.7]}, {"formula_id": "formula_119", "formula_text": ") | w \u2208 N G (v)}} for 1 \u2264 j \u2264 \u2113 stdv {{mlp j (F (t\u22121) w: ) | w \u2208 N G (v)}} for \u2113 + 1 \u2264 j \u2264 2\u2113 max {{mlp j (F (t\u22121) w: ) | w \u2208 N G (v)}} for 2\u2113 + 1 \u2264 j \u2264 3\u2113 min {{mlp j (F (t\u22121) w: ) | w \u2208 N G (v)}} for 3\u2113 + 1 \u2264 j \u2264 4\u2113,", "formula_coordinates": [26.0, 198.84, 683.97, 251.52, 51.09]}, {"formula_id": "formula_120", "formula_text": "H (t) vj = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 H (t) vj for 1 \u2264 j \u2264 4\u2113 s 1 (deg G (v)) \u2022 H (t) vj for 4\u2113 + 1 \u2264 j \u2264 8\u2113 s 2 (deg G (v)) \u2022 H (t) vj for 8\u2113 + 1 \u2264 j \u2264 12\u2113,", "formula_coordinates": [27.0, 199.2, 118.35, 212.4, 44.67]}, {"formula_id": "formula_121", "formula_text": "F", "formula_coordinates": [27.0, 269.4, 190.8, 6.85, 9.1]}, {"formula_id": "formula_122", "formula_text": "\u03d5 (t\u22121) j (x 1 ) such that [[\u03d5 (t\u22121) j , v]] G = F (t\u22121) vj for any vertex v. Then, G (t)", "formula_coordinates": [27.0, 108.0, 226.24, 396.06, 25.01]}, {"formula_id": "formula_123", "formula_text": "\u03c8 (t) j (x 1 ) := aggr mean x2 (mlp j (\u03d5 (t\u22121) 1 (x 2 ), . . . , \u03d5 (t\u22121) \u2113 (x 2 )) | E(x 1 , x 2 )),", "formula_coordinates": [27.0, 167.64, 258.37, 276.6, 13.73]}, {"formula_id": "formula_124", "formula_text": "\u03be (t) j (x 1 ) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 \u03c8 (t) j (x 1 ) for 1 \u2264 j \u2264 4\u2113 s 1 (aggr sum x2 (1 x2=x2 | E(x 1 , x 2 ))) \u2022 \u03c8 (t) j (x 1 ) for 4\u2113 + 1 \u2264 j \u2264 8\u2113 s 2 (aggr sum x2 (1 x2=x2 | E(x 1 , x 2 ))) \u2022 \u03c8 (t) j (x 1 ) for 8\u2113 + 1 \u2264 j \u2264 12\u2113", "formula_coordinates": [27.0, 143.28, 303.51, 320.01, 44.67]}, {"formula_id": "formula_125", "formula_text": "\u03d5 (t) j := mlp \u2032 j (\u03be (t) 1 (x 1 ), . . . , \u03be (t) 12\u2113 (x 1 )) represents F (t)", "formula_coordinates": [27.0, 108.0, 377.81, 269.91, 30.64]}, {"formula_id": "formula_126", "formula_text": "F (0) encodes atp k (G, v) for each v \u2208 V k G .", "formula_coordinates": [27.0, 108.0, 576.4, 396.28, 24.14]}, {"formula_id": "formula_127", "formula_text": "\u03d5 (0) r,s,j (x 1 , . . . , x k ) := \uf8f1 \uf8f2 \uf8f3 1 xr=xs \u2022 P j (x r ) for j \u2208 [\u2113] E(x r , x s ) for j = \u2113 + 1 1 xr=xs for j = \u2113 + 2 , for r, s \u2208 [k] and j \u2208 [\u2113 + 2]. We note: [[\u03d5 (0) r,s,j , (v 1 , . . . , v k )]] G = F (0) v1,...,v k ,r,s,j for all (r, s, j) \u2208 [k] 2 \u00d7 [\u2113 + 2], as desired. We let \u03c4 0 := [k] 2 \u00d7 [\u2113 + 2] and set d 0 = k 2 \u00d7 (\u2113 + 2).", "formula_coordinates": [27.0, 108.0, 611.31, 396.04, 69.87]}, {"formula_id": "formula_128", "formula_text": "F (t) v1,...,v k ,\u2022 := mlp (t) 0 F (t\u22121) v1,...,v k ,\u2022 , w\u2208VG k s=1 mlp(", "formula_coordinates": [27.0, 154.2, 703.25, 186.86, 31.33]}, {"formula_id": "formula_129", "formula_text": "\u03d5 (t\u22121) j (x 1 , . . . , x k ) satisfying [[\u03d5 (t\u22121) j , (v 1 , . . . , v k )]] G = F (t\u22121)", "formula_coordinates": [28.0, 108.0, 127.6, 396.06, 27.29]}, {"formula_id": "formula_130", "formula_text": "\u03d5 (t) j (x 1 , . . . , x k ) := mlp (t) 0,j \u03d5 (t\u22121) i (x 1 , . . . , x k ) i\u2208\u03c4 d t\u22121 , x k+1 k s=1 mlp (t) s,j (\u03d5 (t\u22121) i (x 1 , . . . , x s\u22121 , x k+1 , x s+1 , . . . , x k ) i\u2208\u03c4 d t\u22121 ,", "formula_coordinates": [28.0, 117.96, 193.49, 376.08, 53.77]}, {"formula_id": "formula_131", "formula_text": "\u03c1 1 vwl (t) k \u2286 \u03c1 1 TL (t) k+1 (\u2126) \u2286 \u03c1 1 k-FGNN (t)", "formula_coordinates": [28.0, 211.68, 337.36, 183.55, 13.34]}, {"formula_id": "formula_132", "formula_text": "\u03c1 0 gwl (\u221e) k \u2286 \u03c1 0 TL k+1 (\u2126) \u2286 \u03c1 0 k-FGNN", "formula_coordinates": [28.0, 212.76, 406.72, 181.89, 13.34]}, {"formula_id": "formula_133", "formula_text": "\u03d5 Pi (x 1 ) := x2 \u2022 \u2022 \u2022 xp uv\u2208EP i E(x u , x v ), then [[\u03d5 Pi , v]] G = hom(P r i , G v", "formula_coordinates": [29.0, 108.0, 295.41, 282.24, 41.26]}, {"formula_id": "formula_134", "formula_text": "c \u2032 in Cliques(G), define c \u227a c \u2032 if c \u2282 c \u2032 and there exists no c \u2032\u2032 in Cliques(G), such that c \u2282 c \u2032\u2032 \u2282 c \u2032 . We define Boundary(c, G) := {c \u2032 \u2208 Cliques(G) | c \u2032 \u227a c} and Upper(c, G) := {c \u2032 \u2208 Cliques(G) | \u2203c \u2032\u2032 \u2208 Cliques(G), c \u2032 \u227a c \u2032\u2032 and c \u227a c \u2032\u2032 }.", "formula_coordinates": [29.0, 108.0, 626.33, 396.04, 33.82]}, {"formula_id": "formula_135", "formula_text": "G (t) c: = F B ({{mlp B (F (t\u22121) c:", "formula_coordinates": [29.0, 176.16, 705.4, 106.87, 12.38]}, {"formula_id": "formula_136", "formula_text": "F (t\u22121)", "formula_coordinates": [29.0, 288.0, 705.04, 25.63, 10.61]}, {"formula_id": "formula_137", "formula_text": ") | c \u2032 \u2208 Boundary(G, c)}}) H (t) c: = F U ({{mlp U (F (t\u22121) c:", "formula_coordinates": [29.0, 174.36, 704.21, 247.34, 29.89]}, {"formula_id": "formula_138", "formula_text": "F (t\u22121) c \u2032 : , F (t\u22121) c\u222ac \u2032 : ) | c \u2032 \u2208 Upper(G, c)}}) F (t) c: = mlp(F (t\u22121) c:", "formula_coordinates": [29.0, 287.52, 720.53, 150.14, 14.05]}, {"formula_id": "formula_139", "formula_text": "L norm = I \u2212 D \u22121/2 \u2022 A \u2022 D \u22121/2", "formula_coordinates": [30.0, 108.0, 493.37, 133.09, 11.86]}, {"formula_id": "formula_140", "formula_text": "F (t) := \u03c3 p s=1 C (s) \u2022 F (t\u22121) \u2022 W (t\u22121,s) , with C (1) := I, C (2) = 2 \u03bb max L norm \u2212 I, C (s) = 2C (2) \u2022 C (s\u22121) \u2212 C (s\u22122) , for s \u2265 3,", "formula_coordinates": [30.0, 108.0, 582.29, 362.73, 74.53]}, {"formula_id": "formula_141", "formula_text": "= qs i=0 a (s) i (c) \u2022 (L norm ) i with scalar functions a(s)", "formula_coordinates": [30.0, 201.21, 694.97, 205.94, 15.25]}, {"formula_id": "formula_142", "formula_text": "(L norm ) i \u2022 F (t\u22121) \u2022 W (t\u22121,s) for powers i \u2208 N. Furthermore, since (L norm ) i is again a polynomial of the form q i (D \u22121/2 \u2022 A \u2022 D \u22121/2 ) := ri j=0 b ij \u2022 (D \u22121/2 \u2022 A \u2022 D \u22121/2", "formula_coordinates": [30.0, 337.32, 721.01, 166.84, 11.81]}, {"formula_id": "formula_143", "formula_text": "(D \u22121/2 \u2022 A \u2022 D \u22121/2 ) j \u2022 F (t\u22121) \u2022 W (t\u22121,", "formula_coordinates": [31.0, 219.48, 116.93, 165.25, 11.92]}, {"formula_id": "formula_144", "formula_text": "\u22121/2 \u2022 A \u2022 D \u22121/2 ) 2 \u2022 F (t\u22121) \u2022 W (t\u22121)", "formula_coordinates": [31.0, 339.12, 148.25, 144.71, 11.86]}, {"formula_id": "formula_145", "formula_text": "\u03c8 2 j (x 1 ) = f 1/ \u221a x x2 E(x 1 , x 2 ) \u2022 x2 E(x 1 , x 2 ) \u2022 f 1/x x1 (E(x 2 , x 1 ) \u2022 x1 E(x 2 , x 1 ) \u2022 f 1/ \u221a x ( x2 E(x 1 , x 2 )) \u2022 dt\u22121 i=1 W (t\u22121) ij \u03d5 (t\u22121) i (x 1 ) ,", "formula_coordinates": [31.0, 117.96, 189.77, 376.08, 47.89]}, {"formula_id": "formula_146", "formula_text": "\u221a x : R \u2192 R : x \u2192 1 \u221a x , f 1/x : R \u2192 R : x \u2192 1", "formula_coordinates": [31.0, 108.0, 267.29, 395.97, 27.61]}, {"formula_id": "formula_147", "formula_text": "F (t) := \u03c3 p s=1 C (s) \u2022 F (t\u22121) W (t\u22121,s) , with C (1) := I, C (2s) := Re hL \u2212 \u0131I hL + \u0131I s , C (2s+1) := Re \u0131 hL \u2212 \u0131I hL + \u0131I s ,", "formula_coordinates": [31.0, 108.0, 504.29, 350.28, 74.68]}, {"formula_id": "formula_148", "formula_text": "= x3 \u03d5 0 (x 1 , x 3 ) \u2022 \u03d5 j\u22121 (x 3 , x 2 )", "formula_coordinates": [32.0, 267.57, 108.93, 128.1, 20.61]}, {"formula_id": "formula_149", "formula_text": "\u03c1 1 k-IGN (t) \u2286 \u03c1 1 vwl (t) k\u22121 and \u03c1 0 k-IGN \u2286 \u03c1 0 gwl (\u221e) k\u22121 .", "formula_coordinates": [32.0, 186.0, 292.6, 240.0, 13.22]}, {"formula_id": "formula_150", "formula_text": "\u03c1 1 k-IGN = \u03c1 1 vwl (\u221e) k\u22121 ,", "formula_coordinates": [32.0, 252.12, 446.08, 107.76, 13.22]}, {"formula_id": "formula_151", "formula_text": "[n] \u2113 such that for a, b \u2208 [n] \u2113 , a \u223c \u2113 b if and only if a i = a j \u21d4 b i = b j for all j \u2208 [\u2113]", "formula_coordinates": [32.0, 108.0, 610.97, 396.02, 22.93]}, {"formula_id": "formula_152", "formula_text": "F (t) v1,...,v k ,j := \u03c3 \u03b3\u2208[n] 2k /\u223c 2k w\u2208[n] k 1 (v,w)\u2208\u03b3 i\u2208[dt\u22121]", "formula_coordinates": [32.0, 115.56, 677.2, 209.13, 24.67]}, {"formula_id": "formula_153", "formula_text": "\u00b5\u2208[n] k /\u223c k 1 v\u2208\u00b5 b \u00b5,j(1)", "formula_coordinates": [32.0, 402.6, 678.21, 101.48, 23.85]}, {"formula_id": "formula_154", "formula_text": "((t\u22121)k) k", "formula_coordinates": [33.0, 152.04, 232.6, 27.79, 13.94]}, {"formula_id": "formula_155", "formula_text": "\u03d5 (t) j (x 1 , . . . , x k ) = \u03c3 \uf8eb \uf8ed \u03b3\u2208[n] 2k /\u223c 2k dt\u22121 i=1 c \u03b3,i,j y1 \u2022 \u2022 \u2022 y k \u03c8 \u03b3 (x 1 , . . . , x k , y 1 , . . . , y k ) \u2022 \u03d5 (t\u22121) i (y 1 , . . . , y k ) + \u00b5\u2208[n] k /\u223c k b \u00b5,j \u2022 \u03c8 \u00b5 (x 1 , . . . , x k ) \uf8f6 \uf8f8 , (2)", "formula_coordinates": [33.0, 117.96, 294.03, 386.12, 108.63]}, {"formula_id": "formula_156", "formula_text": "[[\u03c8 \u03b3 , (v, w)]] G = 1 if (v, w) \u2208 \u03b3 0 otherwise [[\u03c8 \u00b5 , v]] G = 1 if v \u2208 \u00b5 0 otherwise", "formula_coordinates": [33.0, 170.4, 465.09, 270.22, 23.04]}, {"formula_id": "formula_157", "formula_text": "[n] k /\u223c k b \u00b5,j \u2022 \u03c8 \u00b5 (x 1 , . . . , x k )", "formula_coordinates": [33.0, 108.0, 562.41, 396.0, 24.93]}, {"formula_id": "formula_158", "formula_text": "y1 \u2022 \u2022 \u2022 y k \u03c8 \u03b3 (x 1 , . . . , x k , y 1 , . . . , y k ) \u2022 \u03d5 (t) i (y 1 , . . . , y k ),(3)", "formula_coordinates": [33.0, 196.92, 639.04, 307.16, 22.1]}, {"formula_id": "formula_159", "formula_text": "(i,j)\u2208I 1 xi=xj \u2022 (i,j)\u2208\u012a 1 xi =xj \u2022 (i,j)\u2208J 1 yi=yj \u2022 (i,j)\u2208J 1 yi =yj (i,j)\u2208K 1 xi=yj \u2022 (i,j)\u2208K 1 xi =yj = A\u2286\u012a B\u2286J C\u2286K (\u22121) |A|+|B|+|C| (i,j)\u2208I\u222aA 1 xi=xj (i,j)\u2208J\u222aB 1 yi=yj \u2022 (i,j)\u2208J\u222aC 1 xi=yj ,(4)", "formula_coordinates": [33.0, 125.52, 711.69, 361.37, 22.18]}, {"formula_id": "formula_160", "formula_text": "A\u2286\u012a B\u2286J C\u2286K (\u22121) |A|+|B|+|C| \u2022 (i,j)\u2208I\u222aA 1 xi=xj \u2022 \uf8eb \uf8ed y1 \u2022 \u2022 \u2022 y k (i,j)\u2208J\u222aB 1 yi=yj \u2022 (i,j)\u2208K\u222aC 1 xi=yj \u2022 \u03d5 (t\u22121) i (y 1 , . . . , y k ) \uf8f6 \uf8f8 . (5)", "formula_coordinates": [34.0, 119.64, 197.69, 384.44, 63.13]}, {"formula_id": "formula_161", "formula_text": "(i,j)\u2208I\u222aA 1 xi=xj \u2022 \uf8eb \uf8ed y1 \u2022 \u2022 \u2022 y k (i,j)\u2208J\u222aB 1 yi=yj \u2022 (i,j)\u2208K\u222aC 1 xi=yj \u2022 \u03d5 (t\u22121) i (y 1 , . . . , y k ) \uf8f6 \uf8f8 , (6)", "formula_coordinates": [34.0, 119.64, 283.11, 384.44, 37.47]}, {"formula_id": "formula_162", "formula_text": "[[ (i,j)\u2208I\u222aA 1 xi=xj \u2022 (i,j)\u2208J\u222aB 1 yi=yj \u2022 (i,j)\u2208K\u222aC 1 xi=yj , (v, w)]] G = 1 \u21d2 [[1 xi=xj , v]] G = 1", "formula_coordinates": [34.0, 123.0, 400.65, 365.94, 21.58]}, {"formula_id": "formula_163", "formula_text": "(i,j)\u2208I \u2032 1 xi=xj \u2022 \uf8eb \uf8ed y1 \u2022 \u2022 \u2022 y k (i,j)\u2208J \u2032 1 yi=yj \u2022 (i,j)\u2208K \u2032 1 xi=yj \u2022 \u03d5 ((t\u22121) i (y 1 , . . . , y k ) \uf8f6 \uf8f8 ,(7)", "formula_coordinates": [34.0, 137.16, 477.03, 366.92, 37.47]}, {"formula_id": "formula_164", "formula_text": "(i,j)\u2208I \u2032 1 xi=xj \u2022 \uf8eb \uf8ed y1 \u2022 \u2022 \u2022 y k (i,j)\u2208K \u2032 1 xi=yj \u2022 (i,j)\u2208J \u2032\u2032 1 yi=yj \u2022 \u03d5 (t\u22121) i (y 1 , . . . , y k ) \uf8f6 \uf8f8 ,(8)", "formula_coordinates": [34.0, 137.64, 602.67, 366.44, 37.47]}, {"formula_id": "formula_165", "formula_text": "(i,j)\u2208I \u2032 1 xi=xj \u2022 \uf8eb \uf8ed y1 \u2022 \u2022 \u2022 y k (\u00ee,j)\u2208K \u2032\u2032 1 x\u00ee=yj \u2022 (i,j)\u2208J \u2032\u2032 1 yi=yj \u2022 \u03d5 (t\u22121) i (y 1 , . . . , y k ) \uf8f6 \uf8f8 ,(9)", "formula_coordinates": [35.0, 136.56, 109.35, 367.52, 37.47]}, {"formula_id": "formula_166", "formula_text": "y1 \u2022 \u2022 \u2022 y k (\u00ee,j)\u2208K \u2032\u2032 1 x\u00ee=yj \u2022 (i,j)\u2208J \u2032\u2032 1 yi=yj \u2022 \u03d5 (t\u22121) i (y 1 , . . . , y k )(10)", "formula_coordinates": [35.0, 182.88, 173.92, 321.32, 22.58]}, {"formula_id": "formula_167", "formula_text": "(i,j)\u2208I \u2032 1 xi=xj \u2022 \u03b2(yi),i\u2208Y (\u00ee,j)\u2208K \u2032 1 x\u00ee=\u03b2(yj) \u2022 (i,j)\u2208J \u2032\u2032 1 \u03b2(yi)=\u03b2(yj) \u2022 \u03b2(\u03d5 (t\u22121) i (y 1 , . . . , y k )) , (11", "formula_coordinates": [35.0, 119.64, 293.61, 380.37, 48.57]}, {"formula_id": "formula_168", "formula_text": ")", "formula_coordinates": [35.0, 500.01, 330.95, 4.19, 8.91]}, {"formula_id": "formula_169", "formula_text": "k-IGN = \u03c1 1 vwl (\u221e) k\u22121 and \u03c1 0 k-IGN = \u03c1 0 gwl (\u221e) k\u22121 hold. k-dimensional GINs.", "formula_coordinates": [35.0, 108.0, 535.36, 252.57, 34.61]}, {"formula_id": "formula_170", "formula_text": "(2019b) to show \u03c1 1 (wl (t) k\u22121 ) \u2286 \u03c1 1 (k-IGN (t)", "formula_coordinates": [35.0, 108.0, 580.84, 172.51, 13.22]}, {"formula_id": "formula_171", "formula_text": "F (t)", "formula_coordinates": [35.0, 117.96, 625.6, 13.03, 8.95]}, {"formula_id": "formula_172", "formula_text": "u\u2208VG mlp (t) 1 (F (t\u22121) v1,v2,...,v k\u22121 ,w,: )) ,", "formula_coordinates": [35.0, 361.08, 652.25, 132.96, 24.01]}, {"formula_id": "formula_174", "formula_text": "S := f \u2208 C(G s , R) (f, f, . . . , f ) \u2113 times \u2208 F \u2113 .", "formula_coordinates": [37.0, 219.12, 111.21, 173.76, 24.57]}, {"formula_id": "formula_175", "formula_text": "\u2022 f : G s \u2192 R \u2113 : (G, v) \u2192 s(G, v) \u2299 f (G, v), with \u2299 being pointwise multiplication, is also in F \u2113 . Indeed, s \u2022 f = \u2299 \u2022 (s,", "formula_coordinates": [37.0, 108.0, 140.69, 396.18, 23.05]}, {"formula_id": "formula_176", "formula_text": "F \u2113 = {f : G s \u2192 R \u2113 | \u03c1(alg) \u2286 \u03c1(f )}.", "formula_coordinates": [37.0, 108.0, 338.85, 396.0, 20.88]}, {"formula_id": "formula_177", "formula_text": "\u2265 1, \u03c1 1 cr (t) = \u03c1 1 GTL (t) (\u2126) , \u03c1 0 gcr (t) = \u03c1 0 TL (t+1) 2 (\u2126) = \u03c1 0 gwl (t) 1 , \u03c1 1 ewl (t) k = \u03c1 1 TL (t) k+1", "formula_coordinates": [37.0, 108.0, 472.96, 395.94, 26.66]}, {"formula_id": "formula_178", "formula_text": "\u2022 If F consists of all functions representable in GTL (t) (\u2126), then F \u2113 = {f : G 1 \u2192 R \u2113 | \u03c1 1 cr (t) \u2286 \u03c1 1 (f )}; \u2022 If F consists of all functions representable in TL (t) k+1 (\u2126), then F \u2113 = {f : G 1 \u2192 R \u2113 | \u03c1 1 vwl (t) k \u2286 \u03c1 1 (f )}; \u2022 If F consists of all functions representable in TL (t+1) 2 (\u2126), then F \u2113 = {f : G 0 \u2192 R \u2113 | \u03c1 0 gwl (t) 1 \u2286 \u03c1 0 (f )}; and finally, \u2022 If F consists of all functions representable in TL k+1 (\u2126), then F \u2113 = {f : G 0 \u2192 R \u2113 | \u03c1 0 gwl (\u221e) k \u2286 \u03c1 0 (f )},", "formula_coordinates": [37.0, 135.36, 542.21, 368.64, 127.09]}, {"formula_id": "formula_179", "formula_text": "(t) k ) \u2286 \u03c1 1 (k-FGNN (t) \u2113 ). Further- more, Maron et al. (2019b) showed that \u03c1 1 (k-FGNN (t) \u2113 ) \u2286 \u03c1 1 (vwl (t) k ). As a consequence, \u03c1 1 (k-FGNN (t) \u2113 ) \u2286 \u03c1 1 (vwl (t) k ). Similarly, \u03c1 1 ((k + 1)-GIN (t) \u2113 ) = \u03c1 1 (vwl (t) k )", "formula_coordinates": [38.0, 143.88, 558.88, 360.45, 39.74]}, {"formula_id": "formula_180", "formula_text": "GIN", "formula_coordinates": [39.0, 180.0, 141.43, 16.41, 9.19]}, {"formula_id": "formula_181", "formula_text": "GIN", "formula_coordinates": [39.0, 138.36, 196.99, 16.41, 9.19]}, {"formula_id": "formula_182", "formula_text": "k-FGNN (t) \u2113 = k-GIN (t) \u2113 = {f : G 1 \u2192 R \u2113 | \u03c1 1 (vwl (t) k ) \u2286 \u03c1 1 (f )} = TL (t) k+1 (\u2126) \u2113 (k + 1)-IGN \u2113 = {f : G 1 \u2192 R \u2113 | \u03c1 1 (vwl", "formula_coordinates": [39.0, 115.44, 232.13, 305.13, 29.89]}, {"formula_id": "formula_183", "formula_text": "k-FGNN \u2113 = k-GIN \u2113 = (k + 1)-IGN \u2113 = {f : G 0 \u2192 R \u2113 | \u03c1 0 (gwl (\u221e)", "formula_coordinates": [39.0, 122.76, 285.53, 265.87, 12.49]}, {"formula_id": "formula_184", "formula_text": "\u03d5 (t\u22121) j (x 1 ) 1 \u2264 j \u2264 d t\u22121", "formula_coordinates": [39.0, 195.72, 392.2, 268.33, 12.98]}, {"formula_id": "formula_185", "formula_text": "H n := (V n , E n ) := H \u2202(v n ) := {F \u2208 E n | v n \u2208 F } U n := F \u2208\u2202(vn)", "formula_coordinates": [40.0, 211.08, 133.53, 118.98, 58.14]}, {"formula_id": "formula_186", "formula_text": "U j := F \u2208\u2202(vj ) F.", "formula_coordinates": [40.0, 223.92, 253.41, 68.16, 26.46]}, {"formula_id": "formula_187", "formula_text": "\u03d5(x) = y1 \u2022 \u2022 \u2022 yp \u03c8(x, y).", "formula_coordinates": [41.0, 249.0, 104.49, 114.0, 20.61]}, {"formula_id": "formula_188", "formula_text": "\u03d5(x) = y1 \u2022 \u2022 \u2022 yp \u03c8(x, y) = y1 \u2022 \u2022 \u2022 yp\u22121 \u03c8 1 (x, y \\ y p ) \u2022 yp \u03c8 2 (x, y)", "formula_coordinates": [41.0, 207.96, 162.81, 196.1, 49.53]}, {"formula_id": "formula_189", "formula_text": "\u03d5 \u2032 (x) = y1 \u2022 \u2022 \u2022 yp\u22121 \u03c8 1 (x, y \\ y p ) \u2022 R p (x, y)", "formula_coordinates": [41.0, 213.96, 283.61, 184.11, 22.57]}, {"formula_id": "formula_190", "formula_text": "\u03d5(G, v) := F { {\u03d5 1 (G, v i , w) | w \u2208 N G (v i )}} .", "formula_coordinates": [43.0, 227.88, 320.97, 192.12, 10.66]}, {"formula_id": "formula_191", "formula_text": "\u03d5 (t) (x 1 ) = f (t) \u03d5 (t\u22121) (x 1 ), aggr F (t)", "formula_coordinates": [43.0, 162.96, 557.2, 140.47, 14.3]}, {"formula_id": "formula_192", "formula_text": "\u03d5 := aggr readout x1 (\u03d5 (t) (x 1 ))", "formula_coordinates": [43.0, 253.8, 646.57, 104.43, 13.25]}], "doi": "10.1145/2902251.2902280"}