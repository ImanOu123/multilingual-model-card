{"title": "Re-examining the Potential Effectiveness of Interactive Query Expansion", "authors": "Ian Ruthven", "pub_date": "", "abstract": "Much attention has been paid to the relative effectiveness of interactive query expansion versus automatic query expansion. Although interactive query expansion has the potential to be an effective means of improving a search, in this paper we show that, on average, human searchers are less likely than systems to make good expansion decisions. To enable good expansion decisions, searchers must have adequate instructions on how to use interactive query expansion functionalities. We show that simple instructions on using interactive query expansion do not necessarily help searchers make good expansion decisions and discuss difficulties found in making query expansion decisions.", "sections": [{"heading": "INTRODUCTION", "text": "Query expansion techniques, e.g. [1,5], aim to improve a user's search by adding new query terms to an existing query. A standard method of performing query expansion is to use relevance information from the user -those documents a user has assessed as containing relevant information. The content of these relevant documents can be used to form a set of possible expansion terms, ranked by some measure that describes how useful the terms might be in attracting more relevant documents, [13]. All or some of these expansion terms can be added to the query either by the user -interactive query expansion (IQE)or by the retrieval system -automatic query expansion (AQE).\nOne argument in favour of AQE is that the system has access to more statistical information on the relative utility of expansion terms and can make better a better selection of which terms to add to the user's query. The main argument in favour of IQE is that interactive query expansion gives more control to the user. As it is the user who decides the criteria for relevance in a search, then the user should be able to make better decisions on which terms are likely to be useful, [10].\nA number of comparative user studies of automatic versus interactive query expansion have come up with inconclusive findings regarding the relative merits of AQE versus IQE. For example, Koenemann and Belkin [10] demonstrated that IQE can outperform AQE for specific tasks, whereas Beaulieu [1] showed AQE as giving higher retrieval effectiveness in an operational environment. One reason for this discrepancy in findings is that the design of the interface, search tasks and experimental methodology can affect the uptake and effectiveness of query expansion techniques.\nMagennis and Van Rijsbergen [12] attempted to gauge the effectiveness of IQE in live and simulated user experiments. In their experiments they estimated the performance that might be gained if a user was making very good IQE decisions (the potential effectiveness of IQE) compared to that of real users making the query modification decisions (the actual effectiveness of IQE). Their conclusion was that users tend to make sub-optimal decisions on query term utility.\nIn this paper we revisit this claim to investigate more fully the potential effectiveness of IQE. In particular we investigate how good a user's query term selection would have to be to increase retrieval effectiveness over automatic strategies for query expansion. We also compare human assessment of expansion term utility with those assessments made by the system.\nThe remainder of the paper is structured as follows. In section 2 we discuss the motivation behind our investigation and that of Magennis and Van Rijsbergen. In section 3 we describe our experimental methodology and data. In section 4 we investigate the potential effectiveness of IQE and in section 5 we compare potential strategies for helping users make IQE decisions. In section 6 we summarise our findings.", "publication_ref": ["b0", "b4", "b12", "b9", "b9", "b0", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "MOTIVATION", "text": "In this section we summarise the experiments carried out by Magennis and Van Rijsbergen, section 2.1, some limitations of these experiments, section 2.2, and discuss the motivation behind our work, section 2.3", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The potential effectiveness of IQE", "text": "In [11,12] Magennis and Van Rijsbergen, based on earlier work by Harman [7], carried out an experiment to estimate how good IQE could be if performed by expert searchers.\nUsing the WSJ (1987-1992) test collection, a list of the top 20 expansion terms was created for each query, using terms taken from the top 20 retrieved documents. This list of possible expansion terms was ranked by applying the F4, [14], term reweighting formula to the set of unretrieved relevant documents. This set of documents consists of the relevant documents not yet seen by the user. This set could not be calculated in a real search environment as retrieval systems will only have knowledge of the set of documents that have been seen by the user. However, as query expansion aims to retrieve this set of documents, they form the best evidence on the utility of expansion terms.\nUsing these sets of expansion terms, Magennis and Van Rijsbergen simulated a user selecting expansion terms over four iterations of query expansion. At each iteration, from the list of 20 expansion terms, the top 0, 3, 6, 10 and 20 terms were isolated. These groups of terms simulated possible sets of expansion terms chosen by a user. By varying which group of terms was added at each iteration, all possible expansion decisions were simulated. For example, expansion by the top 3 terms at feedback iteration 1, the top 10 terms at feedback iteration 2, etc. The best simulation for each query was taken to be a measure of the best IQE decisions that could be made by a user; the potential effectiveness of IQE.", "publication_ref": ["b10", "b11", "b6", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "One of the benefits of an approach such as that taken by Magennis and Van Rijsbergen is that it is possible to isolate the effect of query expansion itself. That is, by eliminating the effects of individual searchers and search interfaces the results can be used as baseline figures with which to compare user search effectiveness. However, in [11,12], Magennis noted several limitations of this particular measurement of the potential effectiveness of IQE.\ni. only certain combinations of terms are considered, i.e. the top 3, 6, 10 or 20 terms. Other combinations of terms are possible, e.g. the top 4 terms, and these may give better retrieval performance.\nii. real searchers are unlikely to use add a consecutive set of expansion terms, i.e. the top 3, 6, 10 or 20 terms suggested by the system. It is more likely that searchers will choose terms from throughout the list of expansion terms. In this way, users can, for example, avoid poor expansion terms suggested by the system.\niii. the ranking of expansion terms is based on information from the unseen relevant documents; ones that the user has not yet viewed. In a real search environment the expansion terms will be ranked based on their presence or absence in the documents seen and assessed relevant by the user. iv. only one document collection was used. Differences in the creation of test collections, the search topics used and the documents present in the test collection may affect the results of their conclusions and restrict the generality of their conclusions.", "publication_ref": ["b10", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Aims of study", "text": "In our experiments we aim to overcome these limitations to create a more realistic evaluation of the potential effectiveness of interactive query expansion. In particular we aim to investigate how good IQE could be, how easy it is to make good IQE decisions and investigate guidelines for helping users make good IQE decisions. We also investigate what kind of IQE decisions are actually made by searchers when selecting new search terms. In the following section we describe how we obtain the query expansion results analysed in the first part of this paper. These experiments are also based on simulations of interactive query expansion decisions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENTAL SETUP", "text": "In this section we outline the experimental methodology we used to simulate query expansion decisions. The experiments themselves were carried out on the Associated Press (AP 1998), San Jose Mercury News (SJM 1991), and Wall Street Journal (WSJ 1990(WSJ -1992 collections, details of which are given in Table 1. These collections come from the TREC initiative [16]. For each query we use the top 25 retrieved documents to provide a list of possible expansion terms, as described below. Although each collection comes with a list of 50 topic (query) descriptions, we concentrate on those queries where query expansion could change the effectiveness of an existing query. This meant excluding some queries from each test collection; those queries for which there are no relevant documents, queries where no relevant documents were retrieved in the top 25 documents (as no expansion terms could be formed without at least one relevant document), and queries where all the relevant documents are found within the top 25 retrieved documents (as query expansion will not cause a change in retrieval effectiveness for these queries).\nIn our experiments we used the wpq method of ranking terms for query expansion, [13], as this has been shown to give good results for both AQE and IQE, [4]. The equation for calculating a weight for a term using wpq is shown below, where the value r t = the number of seen relevant documents containing term t, n t = the number of documents containing t, R = the number of seen relevant documents for query q, N = the number of documents in the collection. Our procedure was as follows:\n( ) ( ) ( ) \uf8f7 \uf8f7 \uf8f8 \uf8f6 \uf8ec \uf8ec \uf8ed \uf8eb \u2212 \u2212 \u2212 \u2022 + \u2212 \u2212 \u2212 \u2212 = R N r n R r r R n N r n r R r\nFor each query, i. rank the documents using a standard tf*idf weighting to obtain an initial ranking of the documents.\nii. use the relevant documents in the top 25 retrieved documents to obtain a list of possible expansion terms, using the wpq formula to rank the expansion terms.\niii. using the top 15 expansion terms, create all possible sets of expansion terms. For each query this gives 32 678 possible sets of expansion terms. This simulates all possible selections of expansion terms using the top 15 terms, including no expansion of the query. Each of these 32 678 sets of terms represents a possible IQE decision that could be made by a user. iv. using each combination of expansion terms, add the combination to the original query and use the new query to rank the documents, again using tf*idf. That is, for each query, we carry out 32 678 separate versions of query expansion.\nv. calculate the recall-precision values for each version of the query. Here we use a full-freezing approach by which we only re-rank the unseen documents -those not use to create the list of expansion terms. This is a standard method of assessing the performance of a query expansion technique based on relevance information, [3] We only use the top 15 expansion terms for query expansion as this is a computationally intensive method of creating possible queries. In a real interactive situation users may be shown more terms than this. However, it does allow us to concentrate on those terms that are considered by the system to be the best for query expansion.\nFor each query in each collection, therefore, we have a set of 32 678 possible IQE decisions that could be made by a searcher. For each possible IQE decision we can assess the effect of making this decision on the quality of the expanded query. We use this information in several ways; firstly, in section 4, we compare the possible IQE decisions against three methods of applying AQE. We then, in section 5, examine potential strategies for helping searchers make good IQE decisions. In section 5 we also compare the possible IQE decisions against human expansion decisions.", "publication_ref": ["b15", "b12", "b3", "b2"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "COMPARING QUERY EXPANSION TECHNIQUES", "text": "In this section we examine the potential effectiveness of IQE against three possible strategies for applying AQE. In this section we compare how likely a user is to make better query expansion decisions using IQE than allowing the system to perform AQE. Our three AQE techniques are: Collection independent expansion. A common approach to AQE is to add a fixed number of terms, n, to each query. Our first AQE technique simulates this by adding the top six expansion terms to all queries, irrespective of the collection used. The value of six was chosen without prior knowledge of the effectiveness of adding this number of terms to any of the queries in the test collections used.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Collection dependent expansion.", "text": "The previous approach to AQE adds the same number of expansion terms to all queries in all collections. When using a specific test collection we can calculate a better value of n; one that is specific to the test collection used. To calculate n, for each collection, we compared the average precision over all the queries used in each collection after the addition of the top n expansion terms, where n varied from 1 to 15. The value of n that gave the optimal value of average precision for the whole query set was taken to be the value of n for each query in the collection. These values could not be calculated in an operational environment, where knowledge of all queries submitted is unknown. However, it gives a stricter AQE baseline measure as the value of n is optimal for the collection used. The values for n are shown in Table 2, and is higher than the six terms added in the previous strategy. Query dependent expansion. The collection dependent expansion strategy adds a fixed number of terms to each query within a test collection. This is optimal for the entire query set but may be sub-optimal for individual queries, i.e. some queries may give better retrieval effectiveness for greater or smaller values of n. The query dependent expansion strategy calculates which value of n is optimal for individual queries. This may be implemented in an operational retrieval system by, for example, setting a threshold on the expansion term weights.\nThese three AQE methods act as baseline performance measures for comparing AQE with IQE.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Query expansion vs. no query expansion", "text": "We first compare the effect of query expansion against no query expansion; how good are different approaches to query expansion? In Table 3 we compare the AQE baselines against no query expansion: the performance of the original query with no additional query terms. Specifically, we compare how many queries in each collection give higher average precision than no query expansion; the percentage of queries that are improved by each AQE strategy. Also included in this table, in bold figures, are the average precision figures given by applying the techniques.\nAs can be seen, all AQE strategies were more likely, on average, to improve a query than harm it. That is, all techniques improved at least 50% of the queries where query expansion could make a difference to retrieval effectiveness.\nThe automatic strategy that is most specific to the query, the query dependent strategy, not only improves the highest percentage of queries-is most stable -but also gives the highest average precision over the queries -is most effective. Conversely the automatic strategy that is least effective and improves least queries is the one that is less tailored to either the query or collection -the collection independent strategy. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "14.0", "text": "We can compare these decisions against possible IQE decisions. Firstly, in row 5 of Table 3, we show the percentage of queries improved, and average precision obtained, when using the best IQE decision for each query. This set of figures gives the best possible results on each collection when using query expansion. This is the highest potential performance of IQE using the top 15 expansion terms.\nComparing the performance of the best IQE decision against the AQE decisions, it can be seen that IQE has the potential to be the most stable technique overall in that it improves most queries. It also has the potential to be the most effective query expansion technique as it gives highest overall average precision. However this is only a potential benefit, as we shall show in the remainder of this paper it may not be easy for a user to select such an optimal set of terms.\nFor example, in the row 6 of Table 3 we show the performance of a middle-performing IQE decision. This is obtained, for each query, by ranking the average precision of all 32768 possible IQE decisions and selecting the IQE decision at position 16384 (half way down the ranking). This decision is one that would be obtained if a user makes query expansion decisions that were neither good nor poor compared to other possible decisions. This result shows that even fair IQE decisions can perform relatively poorly; improving less than half of queries and giving poorer retrieval effectiveness than any of the AQE strategies.\nFinally, in row 7 of Table 3, we show the effect if a user was consistently making the worst IQE decisions possible, i.e. always choosing the combination of expansion terms that gave the lowest average precision of all possible decisions. Even though a user is unlikely to always make such poor decisions, these decisions are being made on terms selected from the top 15 expansion terms. So, although IQE can be effective it is a technique that needs to be applied carefully. In the next section we examine how likely a user is to make a good decision using IQE.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3", "tab_3", "tab_3"]}, {"heading": "AQE vs. IQE", "text": "In this section we look at how difficult it is to select a set of expansion terms that will perform better than AQE or no query expansion. We do this by comparing how many of the possible IQE decisions will give better average precision than the AQE baselines. In Table 4 we show the results of this analysis. For each collection we show how many possible IQE decisions gave greater average precision than each of the three baselines (top row in columns 2-4) and how many of the decisions gave a significantly higher average precision than the baselines (bold figures in columns 2 -4) 2 . ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "2%", "text": "What we are trying to uncover here is how likely a user is to make good IQE decisions over a range of queries. The argument for IQE, based on this analysis, is mixed. On the positive side over 50% of the possible IQE decisions give better performance than no query expansion, and over 20% of the possible decisions give significantly better performance (row 2). However, this also means that nearly half of the possible decisions will decrease retrieval performance 3 and most decisions will not make any significant difference to the existing query performance.\nCompared against the best AQE strategy (query dependent), only a small percentage (9-10%) of possible decisions are likely to be better than allowing the system to make the query expansion decisions. Based on this analysis it appears that it may be hard for users to make very good IQE decisions; ones that are better than a good AQE technique.\nThe collection independent strategy is the most realistic default AQE approach as it assumes no knowledge of collections or queries. However, although 35%-45% of possible IQE decisions are better than the collection independent strategy, this still means that searchers are more likely to make a poorer query expansion decision than the system. This is only true, however, if users lack any method of selecting good combinations of expansion terms. In the next section we analyse potential guidelines that could be given to users to help them make good IQE decisions.", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}, {"heading": "POSSIBLE GUIDELINES FOR IQE", "text": "In this section we try to assess possible instructions that could be given to users to help them make use of IQE as a general search technique.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Select more terms", "text": "One reason for asking users to engage in IQE is to give more evidence to the retrieval system regarding the information for which they are looking. Users, especially in web searches, often use very short queries [9]. Presenting lists of possible expansion terms is one way to get users to give more information, in the form of query words, to the system.\nA useful guideline to give to users, then, may be to expand the query with as many useful terms are possible. In Table 5 we compare the size of IQE decisions that lead to an increase in retrieval effectiveness (good IQE decisions, Table 5, row 4) against those that led to a decrease in retrieval effectiveness (poor IQE decisions, Table 5, row 5). As can be seen, the size of the query expansion does not distinguish good decisions from poor decisions.\nThe size of the best IQE decisions (the average size of the combinations that gave the best average precision) is similar both to the average size of the good and poor combinations (Table 5, row 3). The sizes of the average of the best AQE decisions are also within a similar range (Table 5, row 2). So giving the system more evidence does not necessarily gain any improvement in effectiveness. ", "publication_ref": ["b8"], "figure_ref": [], "table_ref": ["tab_5", "tab_5", "tab_5", "tab_5", "tab_5"]}, {"heading": "Trust the system", "text": "A second approach might be to advise users to concentrate on the terms suggested most strongly by the system. These are terms that are calculated by the system to be the most likely to improve a query, and in our experiment are the terms with the highest wpq score. In Table 6, we present the average wpq value of the terms chosen in good and poor IQE decisions, and also in the best IQE and AQE strategies.\nThe average wpq value for terms in good (row 4) and poor IQE decisions (row 5) is relatively similar. This means that sets of terms with high wpq values are not more likely to give good performance than sets of terms with lower wpq values.\nThe average value for the best AQE decisions (row 2) is generally higher than that of the IQE decisions. This, however, results in part from the fact that the query dependent AQE strategy adds a consecutive set of terms taken from the top of the expansion term ranking. As these terms are at the top of the term ranking, they will naturally have a higher wpq value.\nThe average term score for the best IQE (row 3) decision is also higher than either the good or poor IQE decisions, so there is some merit in choosing terms that the system recommends most highly -those with high wpq values. However, the lack of difference between the good and poor IQE decisions means we cannot alone recommend the user concentrates more closely on the terms suggested by the system. That is, highly scored terms are useful but the user must apply some additional strategy to select which of these terms to use for query expansion.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "Use semantics", "text": "One of the more intuitive arguments in favour of IQE is that, unlike the statistically-based query expansion techniques, humans can exploit semantic relationships for retrieval. That is, people can recognise expansion terms that are semantically related to the information for which they are seeking and expand the query using these terms. However, investigations such as the one presented in [2] indicate that searchers can find it difficult to use semantic information even when the system supports the recognition and use of semantic relationships.\nConsequently, in this section we outline a small pilot experiment designed to compare system recommendations of term utility against human assessment of the same terms.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "System analysis of expansion term utility", "text": "The system, or automatic, analysis of an expansion term is based on the overall impact of adding that term to all possible IQE decisions that do not already contain the term. That is, we estimate the likely impact of adding a new expansion term t to an existing set of expansion terms.\nFor each query, each expansion term, t, belongs to 50% (16384) of the possible IQE decisions (and does not belong to 50% possible decisions, including no query expansion). In effect these two sets of possible decisions are identical except as relates to t: adding t to each IQE decision in the latter set would give an IQE decision in the former set. By comparing the average precision of all IQE decisions that contain t, with the corresponding decisions that do not contain t, we can classify each of the top 15 expansion terms according to whether they are good, neutral or poor expansion terms. Good terms are those that are likely to improve the performance of a possible IQE decision (a set of expansion terms); neutral ones are those that generally make no difference and poor expansion terms are those that are likely to decrease the performance of a set of expansion terms.\nWe demonstrate this in Table 7, based on the TREC topic 259 'New Kennedy Assassination Theories' run on the AP collection. Each row shows what percentage of the 16384 possible decisions, not already containing the term in column 1, that are improved, worsened, or have no difference after the addition of the term. For example, the addition of the term jfk will always improve retrieval effectiveness. That is, adding the term jfk to any set of expansion terms will increase retrieval effectiveness. Conversely, adding the term frenchi will always reduce the retrieval effectiveness, and the addition of the term warren 4 will make no difference.  For simplicity, we classify terms simply by their predominant tendency. For the example in Table 7 the good terms are jfk, motorcad, marcello, and depositori. The poor terms are oswald, dealei, kwitni, theori, documentari, frenchi and bulletin, and the neutral terms are warren, theorist and belin. The term tippit is good and poor for an equal percentage of combinations and cannot be classified.", "publication_ref": ["b3"], "figure_ref": [], "table_ref": ["tab_7", "tab_7"]}, {"heading": "Human analysis of expansion term utility", "text": "The automatic classification of expansion term utility presented in the previous section was compared against a set of human classification of the same expansion terms.\nWe selected 8 queries from each collection and asked 3 human subjects to read the whole TREC topic and each of the relevant documents found within the top 25 retrieved documents. These were the relevant documents used to create the list of the top 15 expansion terms in the previous experiment. The subjects were given the full TREC topic description to provide some context for the search, and were shown the initial query that retrieved the documents. The subjects were then presented with the top 15 expansion terms. For each expansion term the subjects were asked whether they felt the term would be useful or not useful at retrieving additional relevant documents when added to the existing query 5 .\nWe asked each subject to assess each of the 24 queries rather than distributing the queries across multiple subjects. This was to preserve any strategies the individual users may be employing when selecting expansion terms [8]. However, we did not ask the subjects to read the non-relevant retrieved documents as we felt this was too great a burden on the subjects.\nThe subjects' selection of expansion terms was compared against the automatic analysis from section 5.3.1 to compare the system classification against human classification of expansion term utility. The comparison was done in three ways; first we compare how good the subjects are at detecting good expansion terms, section 5.3.2.1, how good the subjects are at eliminating poor expansion terms, section 5.3.2.2, and examine the decisions made by the subjects, section 5.3.2.3.", "publication_ref": ["b4", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Detecting good expansion terms", "text": "For each subject we examine first whether the subjects can detect good expansion terms; whether the subjects can recognise the expansion terms that are likely to be useful in combination with other expansion terms. In Table 8 we show the percentage of the good expansion terms, as classified in section 5.3.1, which were chosen by each subject as being possibly useful for query expansion. The subjects varied in their ability to identify good expansion terms, being able to identify 32% -73% of the good expansion terms.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_8"]}, {"heading": "Eliminating poor expansion terms", "text": "If the subjects are not always good at detecting good expansion terms perhaps they are better are eliminating poor expansion terms? In Table 9 we show the percentage of expansion terms that were assessed as being poor by the system but good by the subjects. As in the previous section, the subjects' ability to correctly classify expansion terms varied with at least 25% of the poor expansion terms being rated as good by the subjects. The implication here is that subjects may have difficulty spotting poor expansion terms. One reason for the poor classification of terms may be that the subjects are only choosing certain types of terms. In Table 10 we compare the cases where the system classification (column 2) agreed or disagreed with the subjects' classification (column 3) of terms. For each case we give the average collection occurrence of the terms and (the figure in parentheses) their average occurrence within the relevant documents. For example, for the terms on which subject 1 and the system agreed that the terms were useful, these terms appeared in an average of 692 documents in the AP collection and an average of 6.5 relevant documents.\nAppearing in lots of relevant documents appears initially to correlate with an assessment of good expansion term utility. However the difference in relevant document occurrence between good/poor and bad/poor misclassification is often slight.\nThe most apparent pattern from Table 10 is that subjects tend to classify terms with a high collection frequency as being good expansion terms. Conversely terms with a low collection frequency are likely to be assessed as being poor expansion terms. This is not a universal pattern (Subject 2 on the WSJ collection for example does the opposite) but it is the main pattern and suggests that searchers may not being assessing which terms are useful but which terms are recognisable.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_9", "tab_0", "tab_0"]}, {"heading": "Subjects' reasons for expansion term selection", "text": "We discussed with each subject their reasons for their classification of expansion terms. Based on the subjects' reasons for classification and the later automatic classification, we can suggest three reasons for misclassification of expansion term utility.\ni. Statistical relationships are important as well as semantic ones. Subjects tended to ignore terms if the terms appear to have been suggested for purely statistical reasons, e.g. numbers. In general this may be a sensible approach if the query does not mention specific numbers or dates. However, the documents in the static collections we used are only a sample of the possible documents on the topics investigated. In this case, strong statistical relationships may be useful for future retrieval.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "ii. Users cannot always identify semantic relationships.", "text": "Making good use of semantic information means being able to identify semantic relationships between the information need and the possible expansion terms. For specialised or unusual terms, the subjects could be unsure of the value of these terms unless the relationship between these terms and the information need was made clear in the documents. However, being able to recognise why expansion terms have been suggested, and the searcher's ability to classify terms as useful or not, does not necessarily guarantee that the terms themselves will be seen as useful. Rather, we propose that searchers need more sophisticated support in assessing the potential quality of expansion terms.\niii. Users cannot always identify useful semantic relationships for retrieval. The difficulty most subjects experienced with selecting expansion terms is that, although they felt they could identify obvious semantic relationships, they could not identify which semantic relationships were going to attract more relevant documents. In short, the subjects felt they could not identify the effect of individual expansion terms on future retrieval. Instead the subjects concentrated mainly on terms they viewed as safe; those that were semantically related to the topic description rather than the retrieved relevant documents. That is, the subjects tended to concentrate on terms for new queries rather than modified or refined queries. This type of decision-making can also be seen in other investigations, e.g. [15] which demonstrated that, although terms suggested from relevant documents can be useful terms,", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "they are often not used as a main source of additional search terms.\nIn a real interactive environment users can, of course, try out expansion terms, or add their own new terms, and see the effect on the type of documents retrieved. However, the lack of connection between expansion terms and documents used to provide those terms indicates that searchers may need more support in how to use query expansion as a general interactive technique.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSIONS", "text": "In this paper we examined the potential effectiveness of interactive query expansion. This is mainly a simulation experiment and is intended to supplement rather than replace experimental investigations of real user IQE decision-making. There are several limitations to this work: for example, we only concentrated on altering the content of the query; future investigations will compare the results obtained here when we use relevance weighting in addition to query expansion. We also do not differentiate between queries although the success of query expansion can vary greatly across queries. We will consider this in future work, our intention here is to investigate the general applicability of query expansion.\nThe experimental results initially provided a comparison between AQE and IQE techniques. From Table 3, section 4.1, IQE has the potential to be an effective technique compared with AQE. One of the main claims for IQE is that searchers can be more adept, than the system, at identifying good expansion terms. This may be particularly true for certain types of search, e.g. in [6] Fowkes and Beaulieu showed that searchers preferred IQE when dealing with complex query statements. Subjects may also be better at targeting specific aspects of the search, i.e. focussing on parts of their information need.\nHowever, the analyses presented here show that the potential benefits of IQE may not be easy to achieve. In particular searchers have difficulty identifying useful terms for effective query expansion. The implication is that simple term presentation interfaces are not sufficient in providing sufficient support and context to allow good query expansion decisions. Interfaces must support the identification of relationships between relevant material and suggested expansion terms and should support the development of good expansion strategies by the searcher.", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Experiments with interfaces to support query expansion", "journal": "Journal of Documentation", "year": "1997", "authors": "M Beaulieu"}, {"ref_id": "b1", "title": "Qualitative evaluation of thesaurus-based retrieval", "journal": "", "year": "2002", "authors": "D Blocks; C Binding; D Cunliffe; D Tudhope"}, {"ref_id": "b2", "title": "Evaluation of feedback retrieval using modified freezing, residual collection & test and control groups. The SMART retrieval system -experiments in automatic document processing", "journal": "", "year": "1971", "authors": "Y K Chang; C Cirillo; Razon ; J "}, {"ref_id": "b3", "title": "User-choices: a new yardstick for the evaluation of ranking algorithms for interactive query expansion. Information processing and management", "journal": "", "year": "1995", "authors": "E N Efthimiadis"}, {"ref_id": "b4", "title": "Query expansion. ARIST", "journal": "", "year": "1996", "authors": "E N Efthimiadis"}, {"ref_id": "b5", "title": "Interactive searching behaviour: Okapi experiment for TREC-8", "journal": "", "year": "2002", "authors": "H Fowkes; M Beaulieu"}, {"ref_id": "b6", "title": "Towards interactive query expansion", "journal": "", "year": "1988", "authors": "D Harman"}, {"ref_id": "b7", "title": "Consistency in the selection of search concepts and search terms. Information Processing and Management. 31. 2", "journal": "", "year": "1995", "authors": "M Iivonen"}, {"ref_id": "b8", "title": "Real life, real users, and real needs: A study and analysis of users on the web", "journal": "", "year": "2000", "authors": "B J Jansen; A Spink; T Saracevic"}, {"ref_id": "b9", "title": "A case for interaction: a study of interactive information retrieval behavior and effectiveness", "journal": "", "year": "1996", "authors": "J Koenemann; N J Belkin"}, {"ref_id": "b10", "title": "The potential and actual effectiveness of interactive query expansion", "journal": "", "year": "1997", "authors": "M Magennis"}, {"ref_id": "b11", "title": "The potential and actual effectiveness of interactive query expansion", "journal": "", "year": "1997", "authors": "M Magennis; C J Van Rijsbergen"}, {"ref_id": "b12", "title": "On term selection for query expansion", "journal": "Journal of Documentation", "year": "1990", "authors": "S E Robertson"}, {"ref_id": "b13", "title": "Relevance weighting of search terms", "journal": "Journal of the American Society for Information Science", "year": "1976", "authors": "S E Robertson; Sparck Jones; K "}, {"ref_id": "b14", "title": "Interaction in information retrieval: Selection and effectiveness of search terms", "journal": "Journal of the American Society for Information Science", "year": "1997", "authors": "A Spink; T Saracevic"}, {"ref_id": "b15", "title": "Overview of the sixth text retrieval conference (TREC-6). Information Processing and Management", "journal": "", "year": "2000", "authors": "E H Voorhees; D Harman"}], "figures": [{"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "", "figure_data": "Number of documents799199025774520Number of queries used323928Average words per query 12.93.72.9Average number of relevant37.858.630.3documents per query"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": "n151513"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": "BaselineAPSJM WSJCollection independent 56%72%50%18.823.818.4Collection dependent72%79%53%19.024.818.6Query dependent75%90%86%20.126.721.1IQE best94%97%96%22.329.122.4IQE middle31%38%30%18.422.918.1IQE worst0%0%0%11.915.8"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": "BaselineAPSJM WSJNo expansion59%69%53%30%38%21%Collection independent 45%36%41%9%11%12%Collection dependent47%35%43%13%9%8%Query dependent9%10%10%1%1%"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": "APSJMWSJQuery dependent6.637.108.46IQE best7.295.567.16IQE good7.357.607.27IQE poor7.617.277.44"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "", "figure_data": "APSJMWSJQuery dependent2.202.112.39IQE best2.941.912.26IQE good1.921.711.70IQE poor1.931.702.12"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": ""}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "", "figure_data": "AP73%60%63%SJM50%40%42%WSJ62%32%45%"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "", "figure_data": "AP54%36%43%SJM39%26%35%WSJ38%45%39%"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "", "figure_data": "System UserS1S2S3APGood Good 692 (6.5) 570 (6.22)666 (5.3)Poor Good 622 (4.3) 914 (4.48)601 (4.2)Good Poor 429 (4.5) 830 (4.14)578 (4.1)Poor Poor 142 (1.7)223(1.8)178 (1.6)SJM Good Good 1321 (7.5) 1831 (7.3) 1542 (7.7)Poor Good 766 (3.7) 867 (3.7)802 (3.7)Good Poor 390 (2.8) 405 (3.8)397 (3.5)Poor Poor 53 (1.4)253 (1.9)179 (1.6)WSJ Good Good 833 (5.2) 204 (2.2)765 (4.5)Poor Good 1496 (3.9) 682 (2.8)881 (3.3)Good Poor 285 (2.6) 598 (4.0)270 (2.7)Poor Poor 427 (1.8) 966 (3.0)470 (2.3)"}], "formulas": [{"formula_id": "formula_0", "formula_text": "( ) ( ) ( ) \uf8f7 \uf8f7 \uf8f8 \uf8f6 \uf8ec \uf8ec \uf8ed \uf8eb \u2212 \u2212 \u2212 \u2022 + \u2212 \u2212 \u2212 \u2212 = R N r n R r r R n N r n r R r", "formula_coordinates": [3.0, 87.63, 138.27, 190.9, 28.54]}], "doi": ""}