{"title": "A LRT Framework for Fast Spatial Anomaly Detection", "authors": "Mingxi Wu; Xiuyao Song; Chris Jermaine; Sanjay Ranka; John Gums", "pub_date": "", "abstract": "Given a spatial data set placed on an n \u00d7 n grid, our goal is to find the rectangular regions within which subsets of the data set exhibit anomalous behavior. We develop algorithms that, given any usersupplied arbitrary likelihood function, conduct a likelihood ratio hypothesis test (LRT) over each rectangular region in the grid, rank all of the rectangles based on the computed LRT statistics, and return the top few most interesting rectangles. To speed this process, we develop methods to prune rectangles without computing their associated LRT statistics.", "sections": [{"heading": "INTRODUCTION", "text": "Discovering subsets of database data that are spatially close to one another and exhibit anomalous behavior is of key importance in many application areas. For example, consider our motivating application of mining antimicrobial (antibiotic) resistance patterns. Antimicrobial resistance in nosocomial (hospital acquired) bacterial infections is a key public health problem. Antimicrobial drugs are the first and sometimes only means of attacking bacterial infection, but due to use and misuse over time, antimicrobials become less useful as bugs become resistant due to selective pressures. The result is that common, often mild, nosocomial infections such as staph can become deadly with no effective treatment. The Antimicrobial Resistance Management (ARM) database (http://www.armprogram.com) consists of antimicrobial resistance data for nearly 400 hospitals over a 15-year period, and presents an opportunity to study the epidemiology of antimicrobial resistance. Over those 15 years, the trend in resistance rates is generally upward. However, a key question that we would like to answer is: Is the trend uniformly upward over time, or are there spatial regions where a set of hospitals have significantly different trends? Knowing the answer would provide key insight into the epidemiology of antimicrobial resistance, indicating, for example, how \"mobile\" the bugs are, or how the evolution of bugs in a hospital's general region affects local resistance rates. If resistance trends show a strong geographic affinity, then it might indicate that resistance is a local phenomenon, and so local programs at individual hospitals aimed at careful antimicrobial stewardship could be useful. However, if resistance trends are uniform over a wide area, then it might indicate that antimicrobial stewardship must be a wider effort.\nApplying a Spatial Likelihood Ratio Test. Using classical statistics, we can mine the ARM database for regions of the country with anomalous or unique resistance trends. Given a spatial region, we might treat the number of resistant cases in year y as the result of a single binomial trial, with an unknown probability of resistance py and the number of experimented isolations of the bug as a known, fixed input ny. Since we are interested in trends, we could link all of the py values over time for a given region using a linear model over the years, py = y\u2206 + p0. Then, using a likelihood ratio test [12], we could check whether the trend \u2206 for the given spatial region is significantly different from the trend that would be used for the entire country. By breaking the country into a grid and checking each contiguous region in the grid for a difference in the local trend, we will locate any locally anomalous resistance trends.\nSo, what's the problem? As long as checking whether each contiguous region is anomalous is computationally inexpensive, then a brute-force search is feasible. There are O(n 4 ) rectangular regions in an n \u00d7 n grid. For example, if n = 32, then there are 278,784 regions. This is not too many regions, but the overall cost to search the grid using a brute-force method is O(cn 4 ), where c is the average cost to check a given area by computing a single likelihood ratio test. The likelihood ratio test resulting from trendbased search described above may require seconds to run for each region that is searched, requiring weeks to run on a 32 by 32 grid. Therefore, the problem is not enumerating all of the local regions to check; the problem is actually having to run the likelihood ratio test on all of them.\nOur Contributions. The primary contribution in this paper is to generalize the set of statistical models that can be used for this sort of spatial search. We propose using the classic likelihood ratio test (LRT) statistic as a score function to evaluate the \"anomalousness\" of a given spatial region with respect to the rest spatial data. The LRT is quite general: it works with virtually any underlying statistical model. A user of our framework need only supply implementations of a few specific functions to instantiate our framework. But a key problem in practice is that computing even one test statistic value can be very expensive. Thus, we propose a pruning strategy that works for almost any underlying likelihood function, that can be used to radically cut down on the number of likelihood ratio tests that must be run when searching for anomalous spatial regions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "BACKGROUND", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The LRT Statistic", "text": "The LRT is a hypothesis test that facilitates the comparison of two models: one parametric stochastic model associated with the hypothesis that there is an anomaly, and another associated with the hypothesis that there is no anomaly-the so-called \"null model\". Both parametric models are embodied by identical likelihood functions L(\u03b8|X), where X contains the values output by the underlying stochastic process, and \u03b8 is a set of parameters coming from the parameter space \u0398. The (restricted) parameter space \u03980 allowed under the null model is the complement of the parameter space allowed in the case of anomalous data, denoted as \u0398 \u2212 \u03980. To check for an anomaly using the LRT, two hypotheses H0 : \u03b8 \u2208 \u03980 and Ha : \u03b8 \u2208 \u0398 \u2212 \u03980 are compared by computing the statistic:\n\u03bb(X) = sup \u0398 0 L(\u03b8|X) sup \u0398 L(\u03b8|X) (1)\nThis statistic is computed by first computing a maximum likelihood estimate (MLE) under both parameter spaces \u03980 and \u0398, and then computing the ratio of the likelihoods obtained via the two MLEs. Wilks [12] showed that the asymptotic distribution of \u039b(X) = \u22122 log \u03bb (which we subsequently refer to as the LRT statistic) is chi-squared with (p \u2212 q) degrees of freedom under the null hypothesis that \u03b8 \u2208 \u03980. p is the number of dimensions (or free parameters) in \u0398, and q is the number of dimensions in \u03980. Thus, to check for an anomaly at confidence level \u03b1, one checks whether \u039b(X) \u2265 c, where c is a non-negative number computed by finding how far out in the tail of a chi-square distribution one has to go to find (1 \u2212 \u03b1)% of the mass. For a very simple example of the sort of case where the LRT is applicable, imagine that we wish to test whether the disease rate within a spatial area A is different than the disease rate outside of the area. We assume that the underlying stochastic process that generates the number of cases of disease is binomial: each person who lives in A has a certain, unknown probability of becoming ill in a given time period, and we wish to check whether this probability is different in A than it is outside of A. For each A, X = {kA} where kA is the number of observed diseased individuals inside of A. The set of model parameters \u03b8 contains an unknown probability or rate of infection pA, and the known number of individuals nA who live inside of A. For a given A, if the null hypothesis holds and \u03b8 \u2208 \u03980, then pA = p\u0100 and the disease rates are the same within and without the given spatial area.\nThe likelihood function L() would then be a binomial function:\nL(\u03b8|X) \u221d p k A A (1 \u2212 pA) n A \u2212k A p k\u0100 A (1 \u2212 p\u0100) n\u0100\u2212k\u0100\nThe degrees of freedom of the null distribution is one, since there is one more free parameter allowed in \u0398 than in \u03980.\nOne can easily use the LRT as a basis for spatial anomaly search. First, all contiguous, rectangular areas in a grid are searched, and the value of the LRT statistic is computed over each of them. Those areas with the greatest value for the statistic are returned to the user for further examination as potential anomalous areas.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Spatial Scan Statistic", "text": "The LRT has been used before for spatial anomaly detection. For example, the LRT test forms the basis for the spatial scan statistic (SSS), which is useful for detecting a cluster of event occurrences in a spatial area. The SSS was first proposed in the statistics literature [6]. The model underlying the SSS assumes that each subregion has a Poisson process controlling the number of event occurrences within it. SSS-related work in the KDD literature has focused on searching for an anomalous area in an efficient fashion [9,8,1]. As we will discuss subsequently, our framework is similar to the SSS in that it utilizes the LRT to perform spatial anomaly detection, but it is far more general, admitting a very wide range of stochastic models, and so work on speeding detection via SSS is not obviously relevant. For example, existing SSS algorithms generally make extensive use of the fact that adding more data to an area while keeping the ratio of the number of observed events to the area measure constant must cause the \"interestingness\" of the area to increase. This is true in the case of a Poisson model, but not in the sort of general model that we consider. Thus, new ways to speeding up the computation are required.", "publication_ref": ["b12", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "PROPOSED LRT FRAMEWORK", "text": "We begin by assuming that the spatial area over which we search for anomalies has been pre-partitioned into an n \u00d7 n spatial grid. For each rectangular area A in the grid, we wish to answer the question, \"Does A differ significantly from the remainder of the area in the grid?\" This question is answered by computing a LRT statistic that compares A to\u0100. If the value of the LRT statistic is large, then A is returned to the user as an anomalous region.\nOur framework can be thought of as a generic template, which requires that a user supplies a few functions which instantiate a particular anomaly detection problem. We now describe the process that a typical user would undertake in order to apply our framework to a particular problem.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Choosing a PDF", "text": "The first thing that a user must do is to postulate an appropriate stochastic model to describe the data generation process within each cell. A cell is a minimum spatial unit within which we assume that there is no spatial variation, and so it does not make sense to subdivide a cell spatially when performing anomaly detection. The model used on a per-cell basis may be simple, such as a classical Poisson or Bernoulli model, or it may be an arbitrary, user-defined model of significant complexity. The stochastic model for a cell c is characterized by a probability density function (PDF), denoted by f (Xc|\u03b8c). The likelihood of a model given the data is then L(\u03b8c|Xc) = f (Xc|\u03b8c). In order to make use of the LRT statistic, we must be able to calculate the likelihood of the entire grid. Assuming the generative processes within each cell are independent of each other 1 , the likelihood of the entire grid is given by:\nL * = Y c\u2208Grid L(\u03b8c|Xc)(2)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Defining the Test Set", "text": "Once the likelihood function has been defined, the second step in employing the LRT framework is specifying two competing hypotheses in such a way that LRT can be used to decide whether a testing rectangle is a spatial anomaly. Informally, given a test area A, the two competing hypotheses take the form:\n\u2022 H0 : the process generating the data in the cells of A is not substantially different from the process generating the data in the cells outside of A.\n\u2022 Ha : the process generating the data in the cells within A is substantially different from the process outside of A.\nSince the generative process is modeled via a PDF that is assumed to be the same across all cells, determining whether the generative process differs from cell-to-cell is equivalent to determining whether the parameters to the process differ within and without A. Note the word \"substantially\" in the definition of the two hypotheses. When we are trying to test whether the generative process in two different cells (or two groups of cells) differs, we are not interested in comparing every aspect of the generative process (or every parameter). There will be natural spatial variations in the generative process that we want to ignore. For example, when we are testing whether the trend of antimicrobial resistance is the same inside of an area A as it is outside of A, differences in the starting point of the trend are uninteresting.\nAs a result, our framework differentiates two types of parameters for a cell's PDF: the \"shared parameters\" and the \"local parameters\". The set of \"shared parameters\" is the subset of \u03b8 that is forced to be identical among each member of a group of cells. The shared parameters are used to model within-group cell-similarity, and it is from within the set of shared parameters where we find the particular data property or properties that are indicative of an anomaly. Those shared parameters that the user is actually interested in testing to see whether they are the same within a region A and outside of the region A are called the \"test set\", and are denoted by T . The test set is a non-empty subset of the shared parameters.\nThe \"local parameters\" are those parameters within \u03b8 that are customizable to each cell. They generally capture the uninteresting or anticipated spatial variation of the data across different cells. Sometimes, the precise values for local parameters may be known and supplied beforehand by the user (such as the number of people living in a spatial region). Other times, local parameters may be unknown before the test is run, and their values are inferred (such as the initial resistance rate when checking for different trends in antimicrobial resistance rates).\nExample. Recall that in our motivating application, we want to find a spatial area where the trend in antimicrobial resistance is different inside of the area than it is outside of the area. We assume that the observed number of resistant cases in a cell is generated via a sample from a binomial random variable, where the number of microbes observed in year y is ny, the probability of resistance in a given year y is py, and the linear function py = \u2206 \u00d7 y + p0 is used to model the trend of antimicrobial resistance rate over many years. In this case, for a given cell c, \u03b8c = {\u2206, p0, n0, n1, n2...}. When detecting anomalous regions, we want to know whether there is an area A where the rate of change in resistance over time differs within A and outside of A. In this case, the rate of change \u2206 is a shared parameter that is assumed to be uniform inside of A, and uniform outside of A. The question is whether \u2206 has a single value for the entire grid. Thus, the test set T = {\u2206}. In contrast, p0 is a local parameter that allows the trend to have a different starting point in each cell. p0 for each cell is unknown and must be inferred from the data. The number of microbes ny observed in year y is also a local parameter, but it is available to the testing framework and need not be inferred.\nIn this case, the two hypotheses that we are comparing become:\n\u2022 H0: \u2206A = \u2206\u0100 1. For each rectangle A in the grid 2. Let \u03b8G = MLE0(f (G)) 3. Let (\u03b8A, \u03b8\u0100) = MLE1(f (G), A) 4. Let \u039b = \u22122 log L(\u03b8G|XG) + 2 log L(\u03b8A|XA) +2 log L(\u03b8\u0100|X\u0100) 5.\nIf \u039b is in the top k found so far, then remember A \u2022 Ha: \u2206A = \u2206\u0100 where \u2206A denotes the shared \u2206 within the current rectangle A and \u2206\u0100 denotes the shared \u2206 outside of the rectangle A.\nIn general, the framework considers the following two competing hypotheses for each rectangular area A:\n\u2022 H0 : \u2200t \u2208 T , tA = t\u0100\n\u2022 Ha: \u2203t \u2208 T where tA = t\u0100", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Instantiating the Framework", "text": "Once the user has developed a likelihood function and determined what parameters are in the test set, he or she must implement four functions in order for our framework to be used. These functions are as follows:\n\u2022 The summarizing function f (A) that returns the set of summary data XA for a region A in the grid, as well as all local parameters whose values are known constants.\n\u2022 A likelihood function L(\u03b8c|Xc) that accepts a set of cell summary statistics as well as a set of cell parameter values, and returns the associated likelihood.\n\u2022 The null-space MLE procedure MLE0(f (A)) that performs MLE in the null parameter space for an area A. That is, this MLE procedure accepts the output of f (A): a set of summary statistics XA over some spatial region A, as well as any local parameters for cells in A whose values are known constants. It then computes the set of non-constant parameter values in \u03b8A that maximize the likelihood function L(\u03b8A|XA). This maximization is done under the constraint that for every two cells c1 and c2 in A, and for every shared parameter s, sc 1 = sc 2 .\n\u2022 The complete-space MLE procedure MLE1(A, f (G)) that accepts a region A as well as all data and constant-valued local parameters from the entire spatial grid G, and then chooses the non-constant values in \u03b8A and \u03b8\u0100 that maximize the entire grid's likelihood: L(\u03b8A|XA)\u00d7 L(\u03b8\u0100|X\u0100). This is done under the constraint that all shared parameters not in the test set must have the same value for each and every cell. However, parameters in the test set T may differ inside of A and outside of A. For t \u2208 T , we only require that tc 1 = tc 2 if both c1 and c2 are inside of A or both c1 and c2 are outside of A; otherwise, tc 1 need not equal tc 2 .\nOnce the user has defined these four functions, our algorithms then look for the k regions that most strongly reject the null hypothesis. Logically, this is done by performing the computation given in Algorithm 1.\nExample. Continuing with our antimicrobial resistance trend example, we would instantiate f , L, MLE1, and MLE0 as follows:\n1. f (A) would return the number of microbial infections in each cell in A for each year (n0, n1, ...) as well as the number of resistant microbial infections for each cell in the area for each of the years (k0, k1, ...).\n2. For a cell c in a spatial area A, L accepts each ny and ky, as well as the rate of infection for the initial year p0 and a change rate \u2206. It then computes the binomial likelihood of the rates given the cell data:\nL(\u03b8c|Xc) \u221d Y y [(p0 + y\u2206) ky \u00d7 (1.0 \u2212 p0 \u2212 y\u2206) ny \u2212ky ]\n3. MLE0 performs a binomial MLE for an input spatial area. MLE0 is run under the constraint that \u2206 is constant across all cells in the input area, and that (p0 + y\u2206) \u2208 [0, 1]. If the input area is the entire grid, this corresponds to the null hypothesis that the trend for each cell in the grid is the same, though the starting rate p0 might be different.\n4. Finally, MLE1 performs two similar binomial MLEs, except that there are two different trends \u2206A and \u2206\u0100 allowed for the areas within and without A, respectively.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "So, What's the Difficulty?", "text": "Running the naive algorithm in Figure 1 can be quite expensive, but the computational difficulty is not necessarily associated with enumerating all of the local spatial regions; even for a 100 \u00d7 100 gird this should take only a few seconds. Rather, the computational difficulty is associated with actually computing MLE1 and MLE0 for every candidate area in the grid. Thus, we will consider pruning strategies that still enumerate all of the O(n 4 ) local spatial areas in the grid, but can often inexpensively tell us before any MLEs are ever computed that it is impossible for the current LRT statistic to have a large or interesting value.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Remarks Regarding the Framework", "text": "First, we point out that this model is exceedingly general. The only constraint of any significance is that we require statistical independence of data generation across cells.\nSecond, there is the issue of statistical significance of the regions returned as the result of the search. Since our framework relies on the LRT, the null distribution is known once the likelihood function is defined and it is quite easy to associate a p value with each discovered region. The only difficulty is that an appropriate multiplehypothesis-testing correction [4] must be used to account for the fact that O(n 4 ) individual hypothesis tests have been performed.\nIf a user is uncomfortable with either the use of an asymptotic null distribution (because he or she is suspicious of relying on asymptotics) or the use of a multiple-hypothesis-testing correction (because he or she is worried that this will compromise the power of the underlying statistical test), then a Monte-Carlo method can be used to associate a p value with each result. Specifically, a large number of spatial grids can be generated under the null hypothesis, and the search for the most anomalous region can be performed in each. By counting how many of the grid replicas have a LRT statistic greater than the one obtained on the real data, one obtains an appropriate p value. The Monte Carlo method makes the pruning algorithms presented subsequently much more important. Since we are only interested in knowing how many of the grid replicas have larger LRT statistic than the one obtained on the real data, we will supply the largest LRT statistic from the real data as an initial cut off value to the remaining replicas, which may result in tremendous speedup comparing to the naive Monte Carlo method. ", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "BASIC PRUNING MECHANISM", "text": "Given the framework from the previous section, the problem becomes speeding the search through each of the O(n 4 ) possible regions in the grid. The basic idea that we will pursue is devising some way of immediately knowing-without performing any MLE-whether the LRT statistic can possibly exceed a given cutoff value for an area A.\nSo, how do we do this? At each iteration of Algorithm 1, we will consider a region A, and wish to know whether the value of \u039b associated with A exceeds the cutoff. Normally, to compute \u039b, we will invoke both MLE1 and MLE0. That is, we would compute (\u03b8A, \u03b8\u0100) = MLE1(A, f (G)), and \u03b8G = MLE0(f (G)), and then compute the quantity\n\u039b = \u22122 log L(\u03b8G|X) + 2 log L(\u03b8A|XA) + 2 log L(\u03b8\u0100|X\u0100)\nOur goal is to somehow avoid running the expensive MLEs and still obtain some idea about whether or not \u039b exceeds the cutoff. Handling \u03b8G is easy. Since the value of MLE0(f (G)) is the same no matter what the value of A, we can compute MLE0 over the entire data set once, and then simply re-use this value for every iteration of the loop that enumerates all possible rectangles.\nHowever, it is more difficult to avoid running MLE1(A, f (G)). Fortunately, we can upper-bound the values of both L(\u03b8A|XA) and L(\u03b8\u0100|X\u0100) obtained via MLE1 by using the likelihoods associated with the MLEs that we have computed previously.\nTo do this, let A = R1 \u222a R2 for non-overlapping R1 and R2, so that XA = XR 1 \u222a XR 2 . Let \u03b8A = \u03b8R 1 \u222a \u03b8R 2 be the set of parameter values relevant to area A, that were computed via MLE1(A, f (G)). Now, assume that \u03b8 R 1 was computed directly by calling MLE0(f (R1)). Then we know that:\nL(\u03b8R 1 |XR 1 ) \u2264 L(\u03b8 R 1 |XR 1 )(3)\nThis must be true since performing MLE0 on a subregion of A has essentially relaxed the constraints for the optimization task performed by MLE1 on A. Why? Recall that the shared parameters relevant to a given cell are categorized into two classes: the shared parameters that are not in the test set T , and the shared parameters that are in the test set T . \u03b8R 1 is chosen by the optimization routine MLE1 under two constraints:\n1. All cells on the grid share the same set of values for all shared parameters not in the test set. 1. All cells within R1 share the same set of values for all shared parameters not in the test set.\n2. All cells within R1 share the same set of values for all shared parameters in the test set.\nObviously, the former two constraints used by MLE1(A, f (G)) to choose \u03b8R 1 are more strict than the later two constraints used by MLE0(f (R1)) to choose \u03b8 R 1 . As a result, inequality 3 must be true. Likewise, L(\u03b8R 2 |XR 2 ) \u2264 L(\u03b8 R 2 |XR 2 ). Thus:\nL(\u03b8A|XA) \u2264 L(\u03b8 R 1 |XR 1 ) \u00d7 L(\u03b8 R 2 |XR 2 )\nThus, one can partition A into two arbitrary subregions, then invoke MLE0 on each of those subregions independently, and use the result to upper-bound the value of L(\u03b8A|XA) that would be obtained via a call to MLE1(A, f (G)). Taking this to its logical conclusion, by induction, one can always give an upper bound to the value of L(\u03b8A|XA) obtained via MLE1 for A = R1 \u222a R2 \u222a R3 \u222a ... by invoking MLE0(f (Ri)) separately for each Ri (see an example in Figure 2). A similar argument holds for L(\u03b8\u0100|X\u0100).\nOur basic tactic will be to pre-compute a number of strategicallychosen result sets from calls to MLE0, and use those to attempt to avoid expensive calls to MLE1 by upper-bounding the quantity of the result. If these upper bounds are tight, it will often be possible to prune A without ever calling MLE1.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "TIGHT BOUND CRITERIA", "text": "To bound L(\u03b8A|XA) and L(\u03b8\u0100|X\u0100) we must be able to tile both A and\u0100 with a set of regions for which an MLE0 has already been pre-computed. There are many possible precomputations and tilling methods, and the manner in which A (and\u0100) is tiled can have a significant effect upon the quality of the bound that is achieved.\nIn practice, there are two over-riding considerations when tiling a region in order to bound the value of L(\u03b8A|XA):\n1. The region should be tiled with as few tiles as possible.\n2. For a fixed number of tiles, it is generally better to have a high variance in tile sizes than it is to have a low variance in tile sizes-that is, one big tile and n \u2212 1 small tiles are better than n medium-sized tiles.\nIt is easy to argue why adding more tiles than that are strictly needed is generally a bad idea. Under MLE0, only one value for every parameter in the shared set is allowed. However, if A is covered with n tiles, then n different values of the parameters in the shared set are allowed, with one parameter value per tile. Thus, adding more and more tiles has the effect of adding more and more parameters. This tends to result in a looser and looser upper bound on L(\u03b8A|XA).\nThe reason that large tiles are preferred-even if it results in one or two large regions and a few very small regions-is illustrated in Figure 3. The data points in this figure are represented by X's and O's; the data points represented by X's are generated by a different process than the O's and so they require different shared parameter values to model them than the O's do. If a number of smaller boxes are used (as in the left of Figure 3), then it is much more likely that the data inside of those boxes are homogeneous and contain mostly X's or mostly O's, and MLE0 will produce a much looser bound than the case in the right of Figure 3.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "PRECOMPUTATION AND BOUNDING", "text": "Were we to compute an MLE for every A in the grid, there would be O(n 4 ) MLE computations. Our goal is to reduce this by at least a factor of n by performing only O(n 3 ) MLE0 pre-computations and a handful of actual MLE1(X, A) computations for those A's that are not pruned. Also, our goal is to limit those precomputation to smaller spatial regions, so they tend to be less expensive optimization problems. Since our reduced problem is to bound both L(\u03b8A|XA) and L(\u03b8\u0100|X\u0100), we consider them separately.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Precomputation and Bounding for A", "text": "We devise a method that uses at most 2 log n 2 precomputed rectangles to tile any given A. The precomputed rectangles are obtained as follows. Consider the biggest rectangle that is enclosed by a pair of the vertical grid lines. Figure 4(a) illustrates one such rectangle (m, n, j, i). The horizontal grid line which crosses the center point of the grid is used to split this rectangle into two identical subrectangles. If we recursively split the resulting subrectangles by horizontal grid lines from their midpoints until we reach the lowest resolution of the grid, our precomputation set contains\n(2n\u22121)(n+1)n 2\nrectangles, which is in the order of O(n 3 ). To use the precomputed rectangles to tile any given A, we use a divide and conquer method. Figure 5 gives the detailed algorithm. The recursive function Tile_A accepts the bottom-left (x1, y1) and top-right (x2, y2) coordinates of a rectangle A, and a pair of low and high values (initially, low is set to zero and high is set to n). It splits A in a top-down fashion following the split points used during precomputation. This method will always tile A with the biggest possible rectangle in our precomputed set and guarantees we use the smallest number of tiles. Figure 4 (c) and (d) illustrate an example where we use two level 2 rectangles (numbered 2 and 3) and one level three rectangle (numbered 7) to tile the given A.", "publication_ref": [], "figure_ref": ["fig_3", "fig_4", "fig_3"], "table_ref": []}, {"heading": "Precomputation and Bounding for\u0100", "text": "Now we turn our attention to the tiling methods for\u0100. There are two different tiling strategies that we employ:\nThe radial method. As illustrated in Figure 6 (a), in a clock-wise order, we elongate the edges of a rectangle A until the edges hit the grid borders.\u0100 is then divided into four rectangles denoted A1 to\u01004, which are used to tile\u0100. We can do the same thing in a counter-clockwise order, which is depicted in Figure 6 (b). In order to tile any given\u0100 using the radial method, the precomputed set should contain all the rectangles that share at least one corner with the grid. This set can be obtained by considering all of the intersection points on the grid. We connect each intersection point on the grid with the four corners of the grid. This produces four diagonals, each of which creates one rectangle in our precomputed set. Since there are O(n 2 ) intersection points, there are O(n 2 ) rectangles in our precomputed set.\nThe sandwich method. As illustrated in Figure 6 (c), if we elongate the two vertical edges of A in both directions until they reach the borders of the grid,\u0100 is divided into four rectangles, denoted A1 to\u01004. We use these four resulting rectangles to tile\u0100. In the same fashion, we can do this horizontally as illustrated in Figure 6 (d). In order to tile any\u0100 using this method, we need to precompute all the rectangles that share two corners with the grid. In Figure 6 (c), these rectangles are\u01002 and\u01004. Notice that these two rectangles are in the the precomputed set for the radial method, so we can reuse them. Also, since we want to avoid any additional precomputations to bound\u01001, we call the Tile_A procedure from the previous subsection to upper bound\u01001. We can upper bound A3 in similar fashion. As a result, with no additional precomputation, we can obtain the bounds using the sandwich method.\nWe have discussed four methods to bound\u0100 (Fig 6 ); in practice, we compute each and choose the tightest bound on L(\u03b8\u0100|X\u0100).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "FINAL SEARCH ALGORITHM", "text": "The final search algorithm in Figure 7 modifies the naive algorithm. At each iteration, the new algorithm obtains an upper bound for the current LRT statistic, and compares the upper bound with the current cutoff related to the k th largest \u039b discovered so far. If the upper bound is less than the cutoff, the current region is pruned. Otherwise, the exact value for the LRT statistic is computed. 1. Does our precomputation actually cut down on the number of MLEs that need to be run in a realistic setting?\n2. Second, the LRT framework has a known asymptotic null distribution. If we use this fact along with a standard Bonferroni correction to take into account the multiple hypothesis test (MHT) problem induced by running a separate hypothesis test for each area in an n \u00d7 n grid, will we (a) still be able to detect any subtle anomalies, and (b) be able to correctly recognize those cases where there is no anomaly in the underlying data?\nExperimental Setup. For our experiments over synthetic data, we used a simple binomial likelihood model. This model is chosen because performing the required MLE is fast, so we can repeatedly run and re-run tests over reasonably large grids. Our setup is as follows. For each cell c in the grid, we randomly generate a population size nc. Then the number of \"successes\" kc in the cell is generated by sampling from a Bin(nc, p) random variable for success rate p. Given a testing rectangle A, denote the \"success\" rate within A by p and within\u0100 by q. The test set is p, the local parameter is the number of binomial trials in a particular cell. The null hypothesis asserts that p = q, and the alternative hypothesis asserts that p = q.\nSince the complete parameter space only has one more dimension than the null parameter space, the null distribution is chi-  squared with one degree of freedom. For our tests, we run 50 trials on a grid size of 128 \u00d7 128. To test the scalability, we also consider a grid size of 256 \u00d7 256. Our initial cutoff value for pruning was chosen to correspond to an overall false positive rate of 5%. In all tests, we seek the top subregion. We ran two different sets of experiments where the null hypothesis was in fact true. In the first, the underlying nc values are relatively uniform, where nc was always sampled from a Normal(\u00b5 = 1\u00d710 4 , \u03c3 = 1\u00d710 3 ) distribution. The \"success\" rate for each cell was set to 0.001. In the second case, there was a single denselypopulated region, or simulated \"city\". We randomly selected an area of size 12 \u00d7 12, and within this area, nc was always sampled from a Normal(\u00b5 = 1 \u00d7 10 5 ,\u03c3 = 5 \u00d7 10 3 ) distribution. We recorded the pruning rate (the number of rectangles pruned/the total rectangles), and checked if there were false alarms.\nWe also simulated a case where the alternative hypothesis holds. The setup was the same as the standard null case, except that we randomly pick a 4 \u00d7 3 region as the \"hot spot\". In one set of tests (the \"subtle anomaly case\"), the success rate for generating each kc was set to 0.003. In the other (the \"extreme anomaly case\"), the success rate was set to 0.01. Results are given in Tables 1 and 2.\nDiscussion. In general, the results shows very high pruning power. Taking into account the precomputated MLEs, the pruning rates realized on the 128\u00d7128 grid would on average reduce the number of tests required by a factor of 31.1 to 31.4 compared to the naive algorithm. On the 256 \u00d7 256 grid, our framework would reduce the number of tests required by a factor of 63.4.\nAlso, with an overall 0.05 level test, our framework did not find any false alarms in tests where the null hypothesis held. The total lack of false alarms (even at a significance level of 0.05) may result from our application of a conservative Bonferroni correction. On the other hand, in both the \"subtle anomaly\" and \"extreme anomaly\" cases, the framework had 100% detection accuracy. Overall, these results show that the framework seems to be both safe and effective.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Experiment Two: The ARM Database", "text": "Application Description. 357 U.S. hospitals participate in the ARM program, where each year, for each (bacteria/antimicrobial drug) combination, each hospital submits the number of isolates (bacteria instances) tested, as well as the number of times that the bacteria was found to be susceptible to the given drug. As described in the introduction, our goal is to find local spatial regions where the temporal trend of antimicrobial resistance change within the region is significantly different than the trend outside of the region.\nExperimental Goal. We wish to experimentally test whether our LRT framework can be used to effectively speed the naive algorithm. There are two primary questions we wish to answer:\n1. First, what is the pruning rate of our LRT framework over the real data set, with a realistic likelihood model?\n2. Second, what is the speedup in wall-clock running time compared to the naive algorithm?\nInstantiating the LRT Framework. To make use of LRT framework, and following the notations introduced in the running example of Section 3.3, we provide instances of f , L, MLE0 and MLE1 as follows:\n1. For each cell on the grid, there may be one or more hospitals. Function f reports the number of aggregate microbial infections in each cell in a given area A for years 0, 1, ... (n0,n1,...) as well as the number of resistant infections in each year(k0,k1,...) for all the hospitals in the cell over a fixed five years span. That is, if x indexes the cell, and y indexes the year, then f reports (nxy, kxy) for every x and y.\n2. For each cell on the grid, we treat each year's data as a single binomial trial. L accepts the cell's summary statistics reported by f as well as the rate of infection for the initial year p0 and a change rate \u2206 :\nL(\u03b8c|Xc) = Y y \" ny ky \u00ab (p0+y\u2206) ky (1.0\u2212p0\u2212y\u2206) ny \u2212ky\nHere, we assume there is a linear relationship for each cell's yearly infection rates. That is, py = y\u2206 + p0, and py \u2208 [0, 1]. Since we are testing if any subset of adjacent cells exhibits a different trend than the rest of the cells, the test set is \u2206, and the local parameter is p0 and ny.\nPlugging in the cell's PDF into Equation 2, we have the entire grid's likelihood:\nL * (\u03b8|f (G)) = Y x\u2208A Y y [ \" nxy kxy \u00ab ((p0)x + y\u2206A) kxy \u00d7 (1.0 \u2212 (p0)x \u2212 y\u2206A) nxy \u2212kxy ] \u00d7 Y x\u2208\u0100 Y y [ \" nxy kxy \u00ab ((p0)x + y\u2206\u0100) kxy \u00d7 (1.0 \u2212 (p0)x \u2212 y\u2206\u0100) nxy \u2212kxy ](4)\nIn Equation 4, \u03b8 = {\u2206, nxy, (p0)x}. Assuming that all the cells within a test region A share the same trend \u2206A, and all the cells outside region A share the same trend \u2206\u0100, then we have the following competing hypotheses:\n\u2022 H0: \u2206A = \u2206\u0100\n\u2022 Ha : \u2206A = \u2206\u0100 3. MLE0 implements a numerical optimization routine for maximizing the value of L in Equation 4. Numerical methods are required due to the lack of an analytic solution.\n4. MLE1 simply invokes MLE0 on f (A) and f (\u0100) independently, since each shared parameter is also in the test set.  Experiment Setup. For the bacteria-antimicrobial combination of S. aureus and Nafcillin, we extracted data from the ARM database for 203 hospitals that provided data in the time period from 2000 to 2004, inclusive. All of the hospitals were organized into an n \u00d7 n spatial grid. We first sort the hospitals using the longitude of their physical locations. They are then grouped into n equi-depth buckets. The placement of hospitals into the n buckets determines which column of the grid each hospital belongs to. Similarly, we sort all hospitals on latitude, partition them n ways, and use the partitioning to determine the rows. We tested three different grid sizes: n = 16, 32, and 64, and use our framework to find the spatial region that most strongly rejects the null hypothesis. For each grid size, we recorded both the pruning rate and the wall-clock running time. Since MLE0 requires one second over the entire data set, there is a strong relationship between the pruning rate and running time; enumerating all of the cells in the grid is inexpensive compared to running one or more MLEs for each area in the grid. Also, due to the high cost of running the MLE, in order to compare our methods with the naive method of simply running an MLE over each area, we had to resort to sampling. For each iteration of the naive method of Algorithm 1, before we invoke MLE0 over A and\u0100, we flip a coin having a 1% chance of obtaining a \"head\" result. If the result is a \"head\", we compute the LRT statistic by running the MLE. Otherwise, we skip the rectangle. While the result of running this sampling-based algorithm will be useless, the running time is expectedly 1 100 of the time required to run the naive algorithm to completion. All results are presented in Table 3.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Discussion.", "text": "We observed a speedup of between one and two orders of magnitude with respect to the naive algorithm, with the speedup increasing with increasing grid size. The results are quite striking. For example, in the 64 \u00d7 64 case, our framework took about 12 days to finish running. On the other hand, the naive method is estimated to take 544 days, or about one and a half years! Thus, in this real application, the pruning strategies that the proposed framework utilizes can turn a computation that is totally infeasible to one that may still be expensive-but is possible in a time frame that is likely acceptable in most epidemiological settings.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiment Three: Census Data", "text": "Application. Our data comes from a 5% sample of the 2000 U.S. census data. 2 Each database record contains data describing a single person, where we know (a) the person's annual wage, (b) the person's highest educational attainment level, and (c) the place where the person works. Annual wage is an integral number. There are eighteen classes of educational attainment, such as Bachelor's degree, Master's degree, etc. For privacy reasons, the place of work attribute in this data is described in terms of the Public Use Mi-2 http://usa.ipums.org/usa/ crodata Area (PUMA), a Census Bureau-defined area of 100,000+ residents; we use the center of each PUMA to place each person into a spatial grid placed over the U.S.A. Given this data, we wish to ask the following question:\nAre fluctuations in the level of income across the country totally explained by differences in education levels across the country, or are there real differences in income level according to the spatial region where people work? This question can be re-cast in a slightly different way that is quite amenable to analysis using our proposed framework: If we condition each person's income level upon the level of education that they obtained, are there spatial subregions of the country where the income level is significantly different from the rest? Experimental Goal. We wish to experimentally test whether our LRT framework can be used to effectively speed a realistic, largescale data analysis problem-there are 1.5 million records in the set we consider. Furthermore, the problem is made even more realistic by the fact that the data are not clean-some of the data have a missing label for the level of educational attainment, which is an issue that must be dealt with in a statistically rigorous fashion; we use an EM algorithm to deal with the missing data.\nInstantiating the LRT Framework. To use the LRT framework, again we need to provide the following components:\n1. For each cell on the grid, there may be co-located many PUMAs. Function f reports each respondent's annual wage, together with the person's education level whose values is in {0, 1, . . . 18}, where 0 indicates the respondent's education level is missing. For a cell in area A, denote the set of labeled data returned by f by XL, and denote the set of data with missing education level (the unlabeled data) using XU . Each element of XL \u222a XU takes the form (x, y) where x is the reported income and y indicates educational level.\n2. For each cell, we describe the wages distribution using a Gamma mixture model, where there are 18 component distributions, each of which represents the wage distribution of an educational class. If a data point has a valid education label, we use the Gamma component distribution to get the data point's likelihood directly. Otherwise, we plug in the data point into the mixture model. We use wj to denote the weight of the j th component for a cell; this is the probability that an arbitrary person in the current cell achieves an education level of j. (\u03b1j , \u03b2j ) denotes the Gamma distribution's parameters for the j th component. Then, L is:\nL(\u03b8c|Xc) = Y (x,y)\u2208X L p(x|\u03b1y, \u03b2y) \u00d7 Y x\u2208X U 18 X j=1 wjp(x|\u03b1j, \u03b2j )\nHere, we are interested in testing if the component distribution representing each education class is the same across the country. The test set includes (\u03b1j , \u03b2j ) for all possible j, the local parameters are the weights wj's for each mixture component j in a cell.\nAgain, we can plug in the cell's PDF into Equation 2 to get the grid's likelihood function L * (\u03b8|f (G)). Using i to index the cells, we have \u03b8 = {(\u03b1 A j , \u03b2 A j ), (\u03b1\u0100 j , \u03b2\u0100 j ), wij } for every i and j. Here, the test set is (\u03b1j, \u03b2j ), the local parameter is wij .\nAssuming all the cells within a testing region A share the same education class distribution Gamma(x|\u03b1 A j , \u03b2 A j ), and all the cells without A share the same class distribution Gamma(x|\u03b1\u0100 j , \u03b2\u0100 j ) for each j \u2208 {1, . . . , 18} . Then, we have the following competing hypothesis:\n\u2022 H0 :\u03b1 A j = \u03b1\u0100 j and \u03b2 A j = \u03b2\u0100 j for all j \u2208 {1, . . . , 18}. \u2022 Ha: \u2203 \u03b1 A j = \u03b1\u0100 j or \u03b2 A j = \u03b2\u0100 j for some j \u2208 {1, . . . , 18}.\n3. MLE0 in this problem is an implementation of the EM algorithm [3], which is chosen so that we can handle the MLE over the unlabeled data in a statistically meaningful fashion, without simply throwing them away. Since EM only converges to a local maxima, our implementation performs several random restarts, though in practice we find that no matter what the starting point, the end quality of MLE0 does not change significantly enough to affect the LRT statistic.\nFor brevity, we omit the details of the derived EM.\n4. MLE1 is two invocations of MLE0 with XA and X\u0100, respectively.\nExperimental Setup. We extracted data for fourteen, contiguous U.S. states, forming a rectangle from Arizona to Indiana, covering 330 PUMAs. The 330 PUMAs were organized into an n \u00d7 n grid using a method identical to the one used for the ARM data. We used n = 16 and 32. We used our framework to look for the top subregion-that is, the one with the largest LRT statistic value. Since MLE0 relies on an iterative EM implementation, it is expensive to run. We used sampling to obtain the estimated wall-clock time for the naive method. The results are presented in Table 4.\nDiscussion. We find that we still achieve good performance compared to the naive method, even in a small grid size. However, the pruning rate and speedup, while significant, are not as dramatic as with the other two data sets. It appears that there is some particular aspect of this application which results in the bound computed by the framework not being quite as tight as in the other two cases.\nStill, in the case of the 32\u00d732 grid, the framework is able to reduce the required time from two weeks down to two days.", "publication_ref": ["b5"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "RELATED WORK", "text": "Detecting anomalous spatial regions have been a popular research topic. Two lines of research have been popular in this area. One is spatial clustering, which includes CLARANS [10], DBSCAN [5], and STING [11] etc. The other group focuses on the performance study on detecting statistical significant spatial or spatial-temporal cluster (hot spot) [8,9,2,1]; most papers in this second line of work are based on Kulldorff's spatial scan statistic (SSS) [6,7]. The SSS is used to detect non-random spatial clusters of event occurrence. Conditioned on the observed event counts, and assuming event independence, the SSS is expressed in the following form:\nCA log CA PA + (CG \u2212 CA) log CG \u2212 CA PG \u2212 PA \u2212 CG log CG PG\nwhere CA is the aggregate event count for area A, and CG is the aggregate event count for the entire spatial region. PA and PG are the population sizes for zone A and the entire spatial region, respectively. Monte Carlo methods is employed to obtain the empirical null distribution of the test statistic. Since obtaining null distribution is expensive for Kulldorff's spatial scan statistic, several methods have been proposed for addressing the performance issue [9,8,1,2]. These methods differ from our own in that they aim to actually avoid considering all O(n 4 ) rectangular areas, whereas our goal is to avoid expensive LRT statistic computations. The existing methods in the literature only apply to those relatively simple density measures that are convex or monotonic with respect to the ratio of zone population over entire population and the ratio of zone's event count over the entire event count. For such measures, the scan-statistic-based approach would be preferred to our own. Unfortunately, existing methods do not apply to the likelihood functions used in the trend or wage model considered in this paper; in these cases, our algorithms are the only option.", "publication_ref": ["b13", "b8", "b14", "b12", "b5", "b4", "b10", "b12", "b4", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSIONS", "text": "In this paper, we have considered the problem of detecting spatial anomalies in an n \u00d7 n grid using a LRT with a chi-squared asymptotic null distribution. Our focus is on using pruning to reduce the magnitude of the constant c in the underlying O(cn 4 ) computation in the case where c is large due to an expensive LRT statistic. Experiments in real-life applications show that our methods are effective at reducing c by almost two orders of magnitude.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgements. Material in this paper was supported by the National Science Foundation under grant no. 0612170.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "y1 ==low && y2 ==high) or (y1 ==low && y2 ==mid) or (y1 ==mid && y2 ==high", "journal": "", "year": "", "authors": ""}, {"ref_id": "b1", "title": "Return rect(x1, y1, x2, y2) /* rect() returns the stored rectangle log-likelihood */ //recursive case", "journal": "", "year": "", "authors": ""}, {"ref_id": "b2", "title": "Else If (y1 <mid and y2 >mid)", "journal": "", "year": "", "authors": ""}, {"ref_id": "b3", "title": "Else If (y1 \u2265mid)", "journal": "", "year": "", "authors": ""}, {"ref_id": "b4", "title": "Spatial scan statistics: approximations and performance study", "journal": "", "year": "2006", "authors": "D Agarwal; A Mcgregor; J M Phillips; S Venkatasubramanian; Z Zhu"}, {"ref_id": "b5", "title": "The hunting of the bump: On maximizing statistical discrepancy", "journal": "", "year": "2006", "authors": "D Agarwal; J M Phillips; S Venkatasubramanian"}, {"ref_id": "b6", "title": "Maximum likelihood from incomplete data via the EM algorithm", "journal": "Journal Royal Stat. Soc., Series B", "year": "1977", "authors": "A Dempster; N Laird; D Rubin"}, {"ref_id": "b7", "title": "Multiple hypothesis testing in microarray experiments", "journal": "Statistical Science", "year": "2003", "authors": "S Dudoit; J P Shaffer; J C Boldrick"}, {"ref_id": "b8", "title": "A density-based algorithm for discovering clusters in large spatial databases with noise", "journal": "", "year": "1996-09", "authors": "M Ester; H.-P Kriegel; J Sander; X Xu"}, {"ref_id": "b9", "title": "A spatial scan statistic", "journal": "Theory and Methods", "year": "1997", "authors": "M Kulldorff"}, {"ref_id": "b10", "title": "Spatial scan statistics: model, calculations, and applications", "journal": "", "year": "1999", "authors": "M Kulldorff"}, {"ref_id": "b11", "title": "A fast multi-resolution method for detection of significant spatial disease clusters", "journal": "", "year": "2003", "authors": "D B Neill; A W Moore"}, {"ref_id": "b12", "title": "Rapid detection of significant spatial clusters", "journal": "", "year": "2004", "authors": "D B Neill; A W Moore"}, {"ref_id": "b13", "title": "Clarans: A method for clustering objects for spatial data mining", "journal": "IEEE Trans. Knowl. Data Eng", "year": "2002", "authors": "R Ng; J Han"}, {"ref_id": "b14", "title": "Sting: A statistical information grid approach to spatial data mining", "journal": "", "year": "1997", "authors": "W Wang; J Yang; R R Muntz"}, {"ref_id": "b15", "title": "The large sample distribution of the likelihood ratio for testing composite hypotheses", "journal": "Annals of Mathematical Statistics", "year": "1938", "authors": "S S Wilks"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Naive top-k LRT search (Algorithm 1)", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Illustration of bounding the maximum likelihood of region A by the product of the maximum likelihoods of subregion Ais. (a) The current testing region A. (b) The testing region A tiled by four subregions.", "figure_data": ""}, {"figure_label": "23", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "2 .Figure 3 :23Figure 3: Illustration of the second tight bound criterion. (a) Three subregions evenly cover the target region. (b) Three subregions unevenly cover the target region, with one big subregion dominating the other two.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Illustration of tiling A. (a) Rectangle (m, n, i, j) is recursively split. (b) The splitting position in each level. (c) A is tiled using three precomputed rectangles. (d) The precomputed rectangles that are used (they are circled) to tile A.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure 5: The algorithm for tiling A", "figure_data": ""}, {"figure_label": "63497", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 6 : 3 . 4 . 9 .Figure 7 :63497Figure 6: Tiling methods for\u0100. (a) and (b) depict the Radial methods. (c) and (d) depict the Sandwich methods. Input: a spatial grid G, f, MLE0,M LE1, L and k Output: top-k anomalous regions. 1. Precompute the O(n 3 ) rectangles described in Section 6.1 2. Precompute the O(4n 2 ) rectangles described in Section 6.2 3. Let \u03b80 = MLE0(f (G)). 4. For each rectangle A on the grid: 5. Follow Section 6.1 to get the upper bound for log L(\u03b8A|XA) 6. Follow Section 6.2 to get the upper bound for log L(\u03b8\u0100|X\u0100) 7. Combine the results of step 3, 5, 6 to an upper bound for \u039bA 8. If the upper bound is less than the k th best, prune A 9. Else compute \u039bA; if in the top k, remember A Figure 7: Top-k LRT statistic search with pruning", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Average pruning rates and accuracy for 50 trials on a 128 \u00d7 128 grid. The pruning rate is", "figure_data": "pruned rectangle # total rectangle #.TestAvg Pruning RateAccuracyNull (standard)99.9994%no false alarmNull( city population)99.9996%no false alarmHot spot p = 0.00399.9712 %100%Hot spot p = 0.0199.9722 %100%"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Average pruning rates and accuracy for 50 random trials on a 256 \u00d7 256 grid.", "figure_data": "TestAvg Pruning RateAccuracyNull (standard)99.9999%no false alarmHot spot p = 0.00399.9996%100%"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ": LRT framework time (in days) vs. naive algorithm for\"trend\" model.n \u00d7 nLRT time LRT pruning Naive time Speedup16 \u00d7 160.1596.5286%2.617.332 \u00d7 321.1397.6303%35.931.864 \u00d7 6411.998.0431%54445.7"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "LRT framework time (in days) vs. naive algorithm for \"wage\" model.", "figure_data": "n \u00d7 nLRT time LRT pruning Naive time Speedup16 \u00d7 16 0.2978.8396%1.284.4132 \u00d7 32 2.0186.1219%14.06.97"}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u03bb(X) = sup \u0398 0 L(\u03b8|X) sup \u0398 L(\u03b8|X) (1)", "formula_coordinates": [2.0, 92.15, 225.42, 164.03, 29.85]}, {"formula_id": "formula_1", "formula_text": "L(\u03b8|X) \u221d p k A A (1 \u2212 pA) n A \u2212k A p k\u0100 A (1 \u2212 p\u0100) n\u0100\u2212k\u0100", "formula_coordinates": [2.0, 41.51, 548.18, 189.24, 13.65]}, {"formula_id": "formula_2", "formula_text": "L * = Y c\u2208Grid L(\u03b8c|Xc)(2)", "formula_coordinates": [2.0, 357.59, 537.45, 161.63, 20.66]}, {"formula_id": "formula_3", "formula_text": "\u2022 H0: \u2206A = \u2206\u0100 1. For each rectangle A in the grid 2. Let \u03b8G = MLE0(f (G)) 3. Let (\u03b8A, \u03b8\u0100) = MLE1(f (G), A) 4. Let \u039b = \u22122 log L(\u03b8G|XG) + 2 log L(\u03b8A|XA) +2 log L(\u03b8\u0100|X\u0100) 5.", "formula_coordinates": [3.0, 30.35, 18.06, 443.15, 662.27]}, {"formula_id": "formula_4", "formula_text": "\u2022 H0 : \u2200t \u2208 T , tA = t\u0100", "formula_coordinates": [3.0, 293.39, 185.64, 89.64, 8.21]}, {"formula_id": "formula_5", "formula_text": "L(\u03b8c|Xc) \u221d Y y [(p0 + y\u2206) ky \u00d7 (1.0 \u2212 p0 \u2212 y\u2206) ny \u2212ky ]", "formula_coordinates": [4.0, 42.47, 115.77, 210.75, 20.54]}, {"formula_id": "formula_6", "formula_text": "\u039b = \u22122 log L(\u03b8G|X) + 2 log L(\u03b8A|XA) + 2 log L(\u03b8\u0100|X\u0100)", "formula_coordinates": [4.0, 280.07, 306.96, 239.13, 18.65]}, {"formula_id": "formula_7", "formula_text": "L(\u03b8R 1 |XR 1 ) \u2264 L(\u03b8 R 1 |XR 1 )(3)", "formula_coordinates": [4.0, 345.71, 490.32, 173.51, 8.84]}, {"formula_id": "formula_8", "formula_text": "L(\u03b8A|XA) \u2264 L(\u03b8 R 1 |XR 1 ) \u00d7 L(\u03b8 R 2 |XR 2 )", "formula_coordinates": [5.0, 57.11, 287.52, 158.99, 8.84]}, {"formula_id": "formula_9", "formula_text": "(2n\u22121)(n+1)n 2", "formula_coordinates": [5.0, 281.27, 586.03, 47.44, 12.92]}, {"formula_id": "formula_10", "formula_text": "L(\u03b8c|Xc) = Y y \" ny ky \u00ab (p0+y\u2206) ky (1.0\u2212p0\u2212y\u2206) ny \u2212ky", "formula_coordinates": [7.0, 302.51, 300.57, 216.64, 24.62]}, {"formula_id": "formula_11", "formula_text": "L * (\u03b8|f (G)) = Y x\u2208A Y y [ \" nxy kxy \u00ab ((p0)x + y\u2206A) kxy \u00d7 (1.0 \u2212 (p0)x \u2212 y\u2206A) nxy \u2212kxy ] \u00d7 Y x\u2208\u0100 Y y [ \" nxy kxy \u00ab ((p0)x + y\u2206\u0100) kxy \u00d7 (1.0 \u2212 (p0)x \u2212 y\u2206\u0100) nxy \u2212kxy ](4)", "formula_coordinates": [7.0, 302.51, 434.01, 220.19, 84.8]}, {"formula_id": "formula_12", "formula_text": "L(\u03b8c|Xc) = Y (x,y)\u2208X L p(x|\u03b1y, \u03b2y) \u00d7 Y x\u2208X U 18 X j=1 wjp(x|\u03b1j, \u03b2j )", "formula_coordinates": [8.0, 335.27, 507.33, 151.07, 55.26]}, {"formula_id": "formula_13", "formula_text": "CA log CA PA + (CG \u2212 CA) log CG \u2212 CA PG \u2212 PA \u2212 CG log CG PG", "formula_coordinates": [9.0, 33.71, 572.52, 204.18, 20.09]}], "doi": ""}