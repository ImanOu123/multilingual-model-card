{"title": "Towards Zero-Shot Multilingual Transfer for Code-Switched Responses", "authors": "Ting-Wei Wu; Changsheng Zhao; Ernie Chang; Yangyang Shi; Pierce Chuang; Vikas Chandra; Biing Juang", "pub_date": "", "abstract": "Recent task-oriented dialog systems obtained great successes in building personal assistants for high resource language such as English, but extending these systems to a global audience is challenging due to the need for annotated data or machine translation systems in the target language. An alternative approach is to leverage existing data in a high-resource language to enable cross-lingual transfer in low-resource language models. However, this type of transfer has not been widely explored in natural language response generation. In this research, we investigate the use of state-of-the-art multilingual models such as mBART and T5 to facilitate zero-shot and few-shot transfer of codeswitched responses. We propose a new adapterbased framework that allows for efficient transfer by learning jointly the task-specific, source and target language representations. Our framework is able to successfully transfer language knowledge even when the target language corpus is limited. We present both quantitative and qualitative analyses to evaluate the effectiveness and limitations of our approach 1 . * This work was done during an internship at Meta Reality Labs.", "sections": [{"heading": "Introduction", "text": "Recent task-oriented dialog systems (ToD) have achieved great success in intelligently communicating with humans in natural languages (Chen et al., 2017;Bohus and Rudnicky, 2009). They are designed to fully assist users with widely heralded applications such as music playing, ticket ordering, or customer servicing (Zhang et al., 2020c). However, most ToD systems are primarily established for English due to its ubiquity and the abundance of high-quality human annotations (Serban et al., 2015). Extending these services to global users may take tremendous efforts, especially in low-resource languages where the collection of training corpus is labor-intensive.\nOn the other hand, given a sizeable English dialog corpus with standard dialog features shared across other languages, it is possible to transfer the knowledge and logic between languages via machine translation or cross-task alignments. Datadriven approaches (Schuster et al., 2019;Xiang et al., 2021) perform standard supervised training with translated dialogs, known as Translate-Train. Different pseudo-data pairs could be leveraged to enhance the multilingual model's robustness. Nevertheless, a fine-grained machine translation system may not exist in an extremely low-resource language. The translation errors of entities in ground truth annotations (e.g. Indian could be translated in Chinese either to an adjective of Indian or Indian people in different contexts.) can drastically influence how model is supervised. This primarily happens in dialog tasks like dialog state tracking (DST) or natural language response generation (NLG) with language-sensitive outputs.\nAnother line of approaches instead investigates cross-lingual transfer directly in pretrained multilingual language models (Tang et al., 2021;Gritta et al., 2022). In particular, the multilingual sequence-to-sequence model family (mSeq2seq), which learns to encode the hidden representation of a given input and generates relevant outputs, can achieve promising multi-task performance in different languages. Pretraining a multilingual encoder with machine translation (Schuster et al., 2019), mask context learning (Colombo et al., 2021) or learning a word alignment matrix  could possibly transfer knowledge between languages. These methods mostly share languageagnostic outputs (Shrivastava et al., 2021) for typical classification tasks to avert off-target problem, i.e., models (partially) translate its prediction into the wrong language during zero-shot transfer due to spurious correlation (Gu et al., 2019; Figure 1: Examples of data formats for multilingual ToD systems. Each system response will come with a system act with different forms depending on use cases. 1\n\u20dd E&E setting will have English sentences with English entities and dialog acts. 2 \u20dd F&E setting will still have entire English dialog acts but foreign responses with English entities embedded. 3 \u20dd F&F will have all foreign responses but with code-switched dialog acts.\n2020a). However, the cross-lingual performance of mSeq2seqs in more challenging response generation with language-specific or code-switched outputs remains mysteriously unexplored.\nHerein, we present a study on the cross-lingual transferability of mSeq2seqs and quantify how well these models could adapt to reasonable multilingual response generation under meager availability of dialog annotation in a target language (fewshot). Given a pair of designed input-output sequences, we propose Cross-lingual Dialog Fusion (XDFusion) that employs mSeq2seqs to quickly adapt to downstream NLG tasks in target lowresource languages by inserting denoising-trained language adapters and a knowledge fusion module. In particular, we first fine-tune mSeq2seq models with the English dialog generation task. Then we insert both pretrained source and target language adapters and an additional fusion module within the fine-tuned models to merge the language-specific knowledge and fine-tune with target languages. We conduct our experiments on a multilingual multidomain ToD dataset: GlobalWOZ (Ding et al., 2022). It is a multilingual extension of an English ToD dataset for DST, MultiWoZ (Budzianowski et al., 2018). Both quantitative and qualitative results show that our proposed adapter-based framework benefits from multilingual pretraining power and abundant English resources as it outperforms several baselines with deficient target language availability.\nTo this end, our contributions are the following:\n1. We investigate and benchmark the transferability of large multilingual pretrained models in the low-resource dialog generation task.\n2. We propose an adapter-based learning framework that shows large improvements in BLEU and the slow error rate by preserving English entities in a code-switched foreign language response.", "publication_ref": ["b3", "b0", "b22", "b20", "b28", "b24", "b7", "b20", "b5", "b23", "b8", "b6", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "The proposed method allows quick adaptation", "text": "of training a new fusion module to support a new language while ameliorating the limited parameter capacity of pretrained models.\n2 Problem Formulation", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data Format", "text": "We mainly follow Madotto et al. (2021) to model ToD systems as a Seq2seq generation module using annotated formats in existing ToD dialog datasets that can generate natural responses in an allocated target language. As shown in Figure 1 of a data sample, each dialog will contain several turns of user utterances (USER) and system utterances (SYS). We first define the dialog history H as the concatenation of the alternating utterances from the user and system turns, respectively, without the last system utterance which we denote as S. Each system utterance comes with a system dialog act S ACT denoted as the concatenation of the intent I and slot-value pairs (s, v) as follows:\nS ACT = I(s 1 = v 1 , . . . , s k = v k )(1)\nWithout loss of generality, we define the modularized system response generation task as inputoutput pairs to benchmark the transferability performance of mSeq2seqs:\nH + I(s 1 = v 1 , . . . , s k = v k ) S ACT \u2192 S(2)\nS ACT could be empty sometimes where the task becomes a direct mapping between dialog history to the ideal system response H \u2192 S.\nWe first refer the dataset of English sentences with English entities as E&E. Due to frequent codeswitching phenomena, besides English-only sentences, the GlobalWOZ dataset also provides two additional use cases for other foreign dialogs: Foreign sentences with foreign local entities (F&F) and foreign sentences with local English entities (F&E). The key discrepancy lies in whether local name entities in the sentences remain in English, which will determine a language-agnostic/specific S ACT , as shown in Figure 1. E&E and F&E will have language-agnostic acts while F&F will have language-specific acts which is considered more challenging in cross-lingual transfer.", "publication_ref": ["b13"], "figure_ref": [], "table_ref": []}, {"heading": "Seq2seq Model & Setting", "text": "Based on the input-output definition in Section 2.1, we can prepare the dialog dataset as D K = {(x (i) , y (i) )} N i=1 , where (x (i) , y (i) ) is a pre-defined input-output pair from one of the three settings in consideration (E&E, F&F, F&E) and K is the language of a dataset (e.g., Chinese). In this paper, we mainly employ mSeq2seqs (e.g., mBART (Tang et al., 2021), mT5 (Xue et al., 2020)), which provide suitable parameter initialization to model the new conditional distribution. Given the input text sequence x (i) = (x\n(i) 1 , ..., x (i) L\n) with length L, we leverage the Seq2seq encoder-decoder architecture to maximize the conditional log-likelihood log p \u03b8 (y|x) where y (i) = (y\n(i) 1 , ..., y (i) T\n) with length T is the output text sequence:\nL M LE (\u03b8) = N i=1 T t=1 log p \u03b8 (y (i) t |y (i) <t , x (i) ) (3) p \u03b8 (y (i) t |y (i) <t , x (i) ) = sof tmax(Wh (i) t + b) (4) h (i) t = g(y (i) t\u22121 , f (x (i) ; \u03b8); \u03b8)(5)\nFollowing the standard taxonomy for Zero-shot cross-lingual transfer and Few-shot cross-lingual transfer setting (Ding et al., 2022), we investigate the model transfer capability based on the available resources during training. In zero-shot setting, we are only given a high-quality set of humanannotated English ToD data D En . We directly train the Seq2seq model with the defined input-output pairs, including English data and data translated from English using a machine translation system. In few-shot setting where we have further access to a small budget of foreign ToD data D F o during training to induce few-shot learning. Particularly, we include a small set (100 dialogs) of foreign ToD data in a target language during training and evaluate multilingual models' performance on NLG tasks. In summary, we mainly have three experimental settings (Train data \u2192 Test data) for benchmarking based on different language datasets to use ( \u2020 indicates only 100 dialogs available):\n\u2022 Zero-shot F&F:\nD En \u2192 D F &F F o \u2022 Few-shot F&F: D En + D F &F \u2020 F o \u2192 D F &F F o \u2022 Few-shot F&E: D En + D F &E \u2020 F o \u2192 D F &E F o\n3 Model Adaptation for Cross-lingual Dialog Transfer", "publication_ref": ["b24", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Structural Fine-tuning", "text": "In the last section, we describe how we induce cross-lingual transfer by directly fine-tuning large mSeq2seqs on labeled data of response generation task in English and very few in a target language. However, models trained with extremely imbalanced data distribution may fail to generate reasonable target language responses and suffer from spurious correlation to source language (Gu et al., 2019). How to adequately extract relevant source language knowledge while preserving spaces for target language adaptation becomes crucial and challenging, more than just simple fine-tuning. We instead split the training steps into separate phases that allows more exclusive parameter updates on source and target languages independently. In the first phase, we care more about learning the task-centralized knowledge agnostic of languages. We retain the original fine-tuning step of training large mSeq2seqs with English data only that can explicitly performs well on generating high-quality responses in English.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "Language Adapters", "text": "Since the emphasis is on target language adaptation as well as avoiding catastrophic forgetting of the multilingual and task knowledge acquired from Section 3.1, adapter module is a great fit for parameter-efficient and quick fine-tuning to new tasks and domains (Rebuffi et al., 2017). Following MAD-X (Pfeiffer et al., 2020c) for cross-lingual transfer, we employ a recent efficient adapter structure to learn language-specific information for each ", "publication_ref": ["b19", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Foreign texts", "text": "Figure 2: The overview of our proposed cross-lingual transfer framework: XDFusion. We first fine-tune parameters of large pretrained mSeq2seq models with English dialogs to learn syntactic information. Additional language adapters are trained via BART/T5 denoising task while the pretrained multilingual model is kept frozen. Finally, we insert both English (En) and Foreign (Fo) language adapters in the fine-tuned Seq2seq models from Structural Fine-tuning while training the new inserted fusion module only on target language dialogs. language, independent from the original large finetuned model. Each adapter module contains a simple down-and up-projection combined with a residual connection:\nAdapter l (h l , r l ) = U l (ReLU(D l (h l ))) + r l (6)\nwhere h l is the hidden representation of subsequent layer normalization output after feed-forward layer in the transformer layer l, U l and D l are up-and down-projection matrices, r l is the hidden state directly from feed-forward layer. During training, we insert the language adapters into original large pretrained multilingual models and update their parameters only with others kept fixed. However, instead of training language adapters using MLM tasks like Pfeiffer et al. (2020c), to better align the original pretraining objective and learn Seq2seq-fashion language knowledge, we train them on unlabeled data of a language using the BART denoising task.", "publication_ref": ["b18"], "figure_ref": [], "table_ref": []}, {"heading": "Target Language Adaptation", "text": "With fine-tuned Seq2seq model from Section 3.1 as well as both source and target language adapters from Section 3.2, we could perform task-and language-specific learning to boost the performance of a specific target language with very few annotations available. To achieve the knowledge sharing between languages, we fix the parameters of large fine-tuned model \u0398 and source/target lan-guage adapters \u03d5 s , \u03d5 t , we additionally introduce an AdapterFusion module (Pfeiffer et al., 2020a) with parameters \u03a8 to combine two language adapters with cross attention and facilitate dynamic knowledge allocation to the downstream task by training target language data D t .\n\u03a8 t \u2190 argmin \u03a8 L t (D t ; \u0398, \u03d5 s , \u03d5 t , \u03a8) (7)\nBy employing two phases of knowledge extraction and composition, we only train the AdapterFusion layer which averts catastrophic forgetting on taskrelated knowledge reserving from large fine-tuned models and interference between separate language tasks and target-language adaptation. The use of parameter-efficient structure is language agnostic and seamlessly extendable to other low-resource languages by efficiently training a lightweight target language adapter and a fusion module with easily fetched unlabeled data (bitext pairs are not required). It can allow fast alignment with other languages without much parameter updating.\n4 Experimental Settings", "publication_ref": ["b16"], "figure_ref": [], "table_ref": []}, {"heading": "Dataset", "text": "We conduct our experiments on GlobalWOZ dataset (Ding et al., 2022), a large-scale multilingual ToD dataset globalized from an English-based ToD benchmark: MultiWoZ (Budzianowski et al., 2018) with four different multilingual use cases, based on the tongue of speakers and countries they travel. We mainly adopt three of all four Global-WOZ settings: an English speaker in an English country (E&E), a Foreign speaker in an English country (F&E) and a Foreign speaker in a Foreign country (F&F), described in Section 2.1 and Figure 1. There are 10,437 dialogs for each language use in GlobalWOZ. To better compare the observations in GlobalWoZ (Ding et al., 2022) experiments, we follow Ding et al. (2022) to choose English as the high-resource source language and other three languages: Chinese (Zh), Spanish (Es), Indonesian (Id) as the low-resource target (foreign) languages.\nIn each of four languages, we split 10,437 dialogs into train/validation/test sets with ratio 8:1:1 and we further subsample 100 dialogs from Zh, Es, Id train sets for few-shot training. Finally we remain Zh, Es, Id test sets untouched during training and only for testing purpose.", "publication_ref": ["b6", "b1", "b6", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Baselines", "text": "In our first set of experiments, we explore the following zero-shot baselines and strategies for training models in Chinese (Zh), Spanish (Es), Indonesian (Id) given a large amount of English training data:\n\u2022 Adapter cross-lingual fusion module which combines pretrained language adapters together.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Details", "text": "Task Our experiments are mainly conducted on Natural Language Response Generation task (NLG), a critical component in a ToD system to accurately generate relevant system responses given the dialog history and system acts, where large pretrained models serve an ideal purpose. mBART-50-large-NMT We choose mBART as our base Seq2seq pretrained model with 590M parameters from HuggingFace with a hidden_size = 1,024, which is also first fine-tuned on 50-language translation tasks (mBART-50-large-NMT) (Tang et al., 2021). We then employ the defined data format to train base models in few-shot and zero-shot setting depicted in Section 2.2. Evaluation We use sacreBLEU to evaluate the overall n-gram match between generated and ground truth responses and Slot Error Rate (SER) to measure the percentage of correct predicted slots in a generated response. Implementation details We implement our framework and all baselines within the Transformers (Wolf et al., 2019) and Adapter-Transformers (Pfeiffer et al., 2020b) library. We mainly use mBART (mBART-large-50, mBART-50-large-NMT) and mT5 (mT5-small, mT5-base) for our base pretrained multilingual models. For finetuning via mBART denoising task on unlabelled data for language adapters, we train the same amount of mC4 dataset (Xue et al., 2020)   batch size and learning rate. For zero-shot baseline (Multi-task NMT), we include CCMatrix dataset (Schwenk et al., 2021) for additional NMT training. We choose the best checkpoint for evaluation based on validation performance. We use the Adam optimizer for all parameter optimization. We follow the hidden size of pretrained models with dimensionalities of 512 (mt5-small), 768 (mt5-base), and 1024 (mBART-large-50). We run each experiment with three random seeds and take the average as the results on 8 NVIDIA A100 40GB GPUs.", "publication_ref": ["b24", "b26", "b17", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Results & Discussion", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Main Results", "text": "In Table 1, we demonstrate the main results of cross-lingual transfer capability by fine-tuning mBART on the GlobalWOZ response generation task. The inferior performance of multilingual mBART in zero-shot setting 1 \u20dd reflects the offtarget problem where generated outputs are undesirably code-switched and missing accurate slot values. Although Translate-Train ameliorates the problem by training models with pseudo-labeled translated data, noisy machine-translated entities without context-aware translation still deteriorates its performance on generating accurate local entities. From 3 \u20dd, we found sacreBLEU increases which alludes that multilingual encoders could implicitly learn to encode language-agnostic representations that are reasonable to decode even the decoder messes up the target language generation.\nFor few-shot setting, we observe that the performance increases significantly if we introduce even a small set of annotated foreign dialogs 4 \u20dd. Cotraining with English data directly that transfers    English knowledge 5 \u20dd will be more useful for the same Indo-European language family like Spanish. SPImpMem 6 \u20dd does not exhibit its power in disentangling language agnostic/specific information in our case with an extremely imbalanced dataset. The additional private memory is not well-trained with only few foreign dialogs. Eventually, our proposed adapter framework 8 \u20dd, beats all above baselines including introducing a single adapter 7 \u20dd, by efficiently manipulating denoising-trained adapters to quickly adapt language models to a target language without sacrificing much task-specific knowledge learned in the previous phase. We also found that our approach shows larger improvements in Chi- nese data, which indicates our treatment in disentangling structure and language learning is more important when source and target languages linguistically share less common.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Comparison to pretrained language models", "text": "Table 2 and 3 summarize the results of our proposed framework performance with different base models against the baseline in 5 \u20dd of Table 1. Nonsurprisingly, using mT5-small with fewer parameters have limited capacity to learn complicated structures of the fine-tuning task which leads to unsatisfying results. Interestingly, using mBART is more effective than mT5-base while they have similar amount of parameters. We conjecture that the use of special language tokens in mBART may induce better model awareness of language-specific knowledge in few-shot setting. The effectiveness of Pretraining mBART with the machine translation task has alternative trends in three languages which may conclude that it will highly depend on the domain intimacy between machine translation corpus and downstream dialogs. For F&E setting, overall we have poorer sacreBLEU (code-switched response quality) and SER (predicting English entities) than F&F setting where we could deduce that code-switched phenomena make the models harder to generalize between two languages especially with extremely imbalanced datasets. However, we still observe a larger improvement by adopting our proposed framework in F&E setting.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4", "tab_3"]}, {"heading": "Further Analyses", "text": "Data variation. In Figure 3, we vary the number of foreign dialogs to train in the final phase of language adaption. We observe each language  saturates around 1k dialogs where the dashed lines are the upper bound of performance when we include all foreign dialogs for training. It demonstrates a good few-shot performer of our model by fine-tuning the adapter fusion module that could enhance the overall performance in the low-resource language setting.\nZero-shot observation. Table 4 summarizes our extended experiment results of mBART-50-large-NMT performance on zero-shot transfer to understand the effects of some common techniques. Directly applying English-trained model to testing low-resource languages has the lowest BLEU and SER. Since the problem mainly rises from the catastrophic forgetting on the decoder side, we have proposed different additional approaches to mitigate. However, we found neither of them work better except a slight increase in decoder freezing and denoising (row 3 and 5). Special input-output formatting seems to require more efforts for adapters to digest and transform where limiting parameter updates will restrict such capability and still suffer from off-target problem. We then focus our contributions in unfreezing our limited budget where extremely few foreign dialog annotation is acceptable. ", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": ["tab_7"]}, {"heading": "Related Work", "text": "Response generation is one of critical components in ToD systems. Extensive works have proposed to enhance response quality with RNNs (Wen et al., 2015), large pretrained models (Zhang et al., 2020b;Peng et al., 2020), augmentation  or new learning objectives (Mi et al., 2019;Zhu, 2020). They are either dealing with monolingual data or still require large amounts of annotated data which cannot allow few-shot foreign language generation -a vast majority of existing multilingual systems mostly consider language-agnostic task outputs like semantic parsing or ignore real codeswitched sentences in real cases (Ding et al., 2022). Instead, DeltaLM (Ma et al., 2021) pretrains interleaved multilingual decoders for text summarization and question generation and CSRL (Wu et al., 2022) learns language-agnostic structure-aware representations for semantic role labeling. Often, due to the high cost of collecting low-resource taskoriented dialog annotations, data-based (Yi and Cheng, 2021;Xiang et al., 2021; and model-based transfer approaches (Schuster et al., 2019;Colombo et al., 2021) are popular to take advantage of high-resource language corpus for cross-lingual transfer. Nevertheless, few-shot response generation is yet largely unexplored to induce cross-lingual transfer. The most related prior work is Chen et al. (2019) which extends the Seq2seq models for response generation with private and local memory to accommodate new languages, which nevertheless cannot learn good memory modules when language data is highly imbalanced. Our work continues to explore the possibility of cross-lingual response generation with large Seq2seq models under low-resource language constraint more effectively.", "publication_ref": ["b33", "b15", "b14", "b35", "b6", "b12", "b27", "b31", "b28", "b20", "b5", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper, we explore the pretrained mSeq2seq's capability to induce high-resource language dialog knowledge for low-resource language response generation. By introducing a few foreign high-quality annotated dialogs, we observe that it is possible to learn a dynamic adapter fusion module to fuse all related knowledge in a single large multilingual model, while preserving multilingual power from high-resource language fine-tuning. We have shown that by fine-tuning on very few dialogs of a target language, our proposed model-agnostic framework is capable of producing reasonable responses and more effective than several common baselines, which could quickly adapt to a new target language without further parameter.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "While we observe marked improvements in the proposed multilingual language transfer with adapters, we recognize that there are several limitations still in the experiments. The first limitation is that the entity translation remains difficult, which is especially severe in the generated responses in the E&E setting. We think that name-entity translation is itself a task to be explored in-depth for future works.\nOn the other hand, we think that while knowledge of language is one aspect for the transfer, the structural information of the semantic representation is also another important aspect -models need to acquire the important semantic structural information on top of the language-specific syntactic information. We think that this would further improve the resulting performance.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ethics Statement", "text": "We recognize and take seriously the ethical principles of avoiding harm, trustworthiness, fairness and non-discrimination, and privacy. We take steps to minimize the potential negative impacts of our research and we are committed to ensuring that the use of our findings and technology is done in an ethical and responsible manner. We are committed to ensuring that our research and the use of machine translation technology do not perpetuate language biases, discrimination or any form of inequality.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "The ravenclaw dialog management framework: Architecture and systems", "journal": "Computer Speech Language", "year": "2009", "authors": "Dan Bohus; Alexander Rudnicky"}, {"ref_id": "b1", "title": "MultiWOZ -a largescale multi-domain Wizard-of-Oz dataset for taskoriented dialogue modelling", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Pawe\u0142 Budzianowski; Tsung-Hsien Wen; Bo-Hsiang Tseng; I\u00f1igo Casanueva; Stefan Ultes; Milica Osman Ramadan;  Ga\u0161i\u0107"}, {"ref_id": "b2", "title": "Multilingual dialogue generation with shared-private memory", "journal": "", "year": "2019", "authors": "Chen Chen; Lisong Qiu; Zhenxin Fu; Dongyan Zhao; Junfei Liu; Rui Yan"}, {"ref_id": "b3", "title": "A survey on dialogue systems", "journal": "ACM SIGKDD Explorations Newsletter", "year": "2017", "authors": "Hongshen Chen; Xiaorui Liu; Dawei Yin; Jiliang Tang"}, {"ref_id": "b4", "title": "Cross-lingual natural language generation via pre-training", "journal": "", "year": "2019", "authors": "Zewen Chi; Li Dong; Furu Wei; Wenhui Wang; Xian-Ling Mao; Heyan Huang"}, {"ref_id": "b5", "title": "Code-switched inspired losses for spoken dialog representations", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Pierre Colombo; Emile Chapuis; Matthieu Labeau; Chlo\u00e9 Clavel"}, {"ref_id": "b6", "title": "GlobalWoZ: Globalizing MultiWoZ to develop multilingual task-oriented dialogue systems", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Bosheng Ding; Junjie Hu; Lidong Bing; Mahani Aljunied; Shafiq Joty; Luo Si; Chunyan Miao"}, {"ref_id": "b7", "title": "CrossAligner & co: Zero-shot transfer methods for task-oriented cross-lingual natural language understanding", "journal": "", "year": "2022", "authors": "Milan Gritta; Ruoyu Hu; Ignacio Iacobacci"}, {"ref_id": "b8", "title": "Improved zero-shot neural machine translation via ignoring spurious correlations", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Jiatao Gu; Yong Wang; Kyunghyun Cho; O K Victor;  Li"}, {"ref_id": "b9", "title": "Cross-lingual named entity recognition using parallel corpus: A new approach using xlm-roberta alignment", "journal": "", "year": "2021", "authors": "Bing Li; Yujie He; Wenjin Xu"}, {"ref_id": "b10", "title": "Multilingual denoising pretraining for neural machine translation", "journal": "", "year": "2020", "authors": "Yinhan Liu; Jiatao Gu; Naman Goyal; Xian Li; Sergey Edunov; Marjan Ghazvininejad; Mike Lewis; Luke Zettlemoyer"}, {"ref_id": "b11", "title": "Zero-shot cross-lingual dialogue systems with transferable latent variables", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Zihan Liu; Jamin Shin; Yan Xu; Genta Indra Winata; Peng Xu; Andrea Madotto; Pascale Fung"}, {"ref_id": "b12", "title": "Deltalm: Encoder-decoder pre-training for language generation and translation by augmenting pretrained multilingual encoders", "journal": "", "year": "2021", "authors": "Shuming Ma; Li Dong; Shaohan Huang; Dongdong Zhang; Alexandre Muzio; Saksham Singhal; Xia Hany Hassan Awadalla; Furu Song;  Wei"}, {"ref_id": "b13", "title": "Continual learning in task-oriented dialogue systems", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Andrea Madotto; Zhaojiang Lin; Zhenpeng Zhou; Seungwhan Moon; Paul Crook; Bing Liu; Zhou Yu; Eunjoon Cho; Pascale Fung; Zhiguang Wang"}, {"ref_id": "b14", "title": "Meta-learning for low-resource natural language generation in task-oriented dialogue systems", "journal": "", "year": "2019", "authors": "Fei Mi; Minlie Huang; Jiyong Zhang; Boi Faltings"}, {"ref_id": "b15", "title": "Few-shot natural language generation for taskoriented dialog", "journal": "", "year": "2020", "authors": "Baolin Peng; Chenguang Zhu; Chunyuan Li; Xiujun Li; Jinchao Li; Michael Zeng; Jianfeng Gao"}, {"ref_id": "b16", "title": "Adapterfusion: Non-destructive task composition for transfer learning", "journal": "", "year": "2020", "authors": "Jonas Pfeiffer; Aishwarya Kamath; Andreas R\u00fcckl\u00e9; Kyunghyun Cho; Iryna Gurevych"}, {"ref_id": "b17", "title": "Adapterhub: A framework for adapting transformers", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Jonas Pfeiffer; Andreas R\u00fcckl\u00e9; Clifton Poth; Aishwarya Kamath; Ivan Vuli\u0107; Sebastian Ruder; Kyunghyun Cho; Iryna Gurevych"}, {"ref_id": "b18", "title": "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer", "journal": "", "year": "2020", "authors": "Jonas Pfeiffer; Ivan Vuli\u0107; Iryna Gurevych; Sebastian Ruder"}, {"ref_id": "b19", "title": "Learning multiple visual domains with residual adapters", "journal": "Curran Associates, Inc", "year": "2017", "authors": "Hakan Sylvestre-Alvise Rebuffi; Andrea Bilen;  Vedaldi"}, {"ref_id": "b20", "title": "Cross-lingual transfer learning for multilingual task oriented dialog", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Sebastian Schuster; Sonal Gupta; Rushin Shah; Mike Lewis"}, {"ref_id": "b21", "title": "CCMatrix: Mining billions of high-quality parallel sentences on the web", "journal": "", "year": "2021", "authors": "Holger Schwenk; Guillaume Wenzek; Sergey Edunov; Edouard Grave; Armand Joulin; Angela Fan"}, {"ref_id": "b22", "title": "A survey of available corpora for building data-driven dialogue systems", "journal": "ArXiv", "year": "2015", "authors": "Iulian Serban; Ryan Lowe; Peter Henderson; Laurent Charlin; Joelle Pineau"}, {"ref_id": "b23", "title": "Span pointer networks for nonautoregressive task-oriented semantic parsing", "journal": "", "year": "2021", "authors": "Akshat Shrivastava; Pierce Chuang; Arun Babu; Shrey Desai; Abhinav Arora; Alexander Zotov; Ahmed Aly"}, {"ref_id": "b24", "title": "Multilingual translation from denoising pre-training", "journal": "", "year": "2021", "authors": "Yuqing Tang; Chau Tran; Xian Li; Peng-Jen Chen; Naman Goyal; Vishrav Chaudhary; Jiatao Gu; Angela Fan"}, {"ref_id": "b25", "title": "Semantically conditioned LSTM-based natural language generation for spoken dialogue systems", "journal": "", "year": "2015", "authors": "Milica Tsung-Hsien Wen; Nikola Ga\u0161i\u0107; Pei-Hao Mrk\u0161i\u0107; David Su; Steve Vandyke;  Young"}, {"ref_id": "b26", "title": "", "journal": "", "year": "2019", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; R\u00e9mi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger; Quentin Drame; Alexander M Lhoest;  Rush"}, {"ref_id": "b27", "title": "Zero-shot crosslingual conversational semantic role labeling", "journal": "ArXiv", "year": "2022", "authors": "Han Wu; Haochen Tan; Kun Xu; Shuqi Liu; Lianwei Wu; Linqi Song"}, {"ref_id": "b28", "title": "Zero-shot deployment for cross-lingual dialogue system", "journal": "Springer-Verlag", "year": "2021-10-13", "authors": "Lu Xiang; Yang Zhao; Junnan Zhu; Yu Zhou; Chengqing Zong"}, {"ref_id": "b29", "title": "Augnlg: Few-shot natural language generation using self-trained data augmentation", "journal": "", "year": "2021", "authors": "Xinnuo Xu; Guoyin Wang; Young-Bum Kim; Sungjin Lee"}, {"ref_id": "b30", "title": "", "journal": "", "year": "", "authors": "Linting Xue; Noah Constant; Adam Roberts; Mihir Kale; Rami Al-Rfou; Aditya Siddhant; Aditya Barua"}, {"ref_id": "b31", "title": "Zero-shot entity recognition via multi-source projection and unlabeled data", "journal": "IOP Conference Series: Earth and Environmental Science", "year": "2021", "authors": "Huixiong Yi; Jin Cheng"}, {"ref_id": "b32", "title": "Improving massively multilingual neural machine translation and zero-shot translation", "journal": "", "year": "2020", "authors": "Biao Zhang; Philip Williams; Ivan Titov; Rico Sennrich"}, {"ref_id": "b33", "title": "Dialogpt: Large-scale generative pre-training for conversational response generation", "journal": "", "year": "2020", "authors": "Yizhe Zhang; Siqi Sun; Michel Galley; Yen-Chun Chen; Chris Brockett; Xiang Gao; Jianfeng Gao; Jingjing Liu; Bill Dolan"}, {"ref_id": "b34", "title": "Minlie Huang, and Xiaoyan Zhu. 2020c. Recent advances and challenges in task-oriented dialog system", "journal": "", "year": "", "authors": "Zheng Zhang; Ryuichi Takanobu; Qi Zhu"}, {"ref_id": "b35", "title": "Boosting naturalness of language in task-oriented dialogues via adversarial training", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Chenguang Zhu"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": ".0 23.5 20.3 21.1 11.3 10.4 14.3", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Performance difference of varying available foreign data amount for training. Dashed lines are the results of using all available foreign dialogs in GlobalWOZ of a target language, which are considered as the upper bound.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Fine-tune Seq2seq model with both E&E and few F&F training data. \u2022 SPImpMem (Chen et al., 2019): Insert shared and private memory modules within Seq2seq model to induce cross-lingual transfer. \u2022 Adapter (Pfeiffer et al., 2020c): Fine-tune", "figure_data": "Seq2seq model with E&E training data; theninsert and fine-tune adapter modules both at en-coder and decoder side only. \u2022 XDFusion: Our proposed approach to insertE&E: Fine-tune mSeq2seq with E&E training data only. \u2022 Translate-Train (Ding et al., 2022): Translate E&E data with label sequence translation in Dinget al. (2022) using an external machine transla-tion system. \u2022 Translate-Back: Directly translate response out-puts predicted from English-trained model backinto the target language. \u2022 Adapter (Pfeiffer et al., 2020c): Insert and fine-tune adapter modules both at encoder and de-coder side only. \u2022 Freeze-Decoder (Chi et al., 2019): Freeze the decoder part and fine-tune encoder side only. \u2022 Multi-task learning: NMT & Denoise (Liu et al., 2020): Include external out-of-domain cor-pus to perform NMT or Denoising task trainingsimultaneously with the main dialog responsegeneration task.Then we consider the following few-shot baselinesby adding a small amount of Zh, Es, Id trainingdata along with English training data."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Translate-Train (Ding et al., 2022)  Zero-shot 10.80 11.81 12.40 11.67 42.32 38.61 37.12 39.35  3   ", "figure_data": "TaskNLGID ModelMetricsSettingzhsacreBLEU (%) \u2191 es idavgzhSER (%) \u2193 es idavg1 \u20dd E&E (Tang et al., 2021) 2 \u20dd \u20dd Translate-Back 4 \u20dd F&F (Tang et al., 2021) 5 \u20dd 1 \u20dd + 4 \u20dd (Tang et al., 2021) 6 \u20dd 5 \u20dd + SPImpMem (Chen et al., 2019) Few-shot Zero-shot Zero-shot 14.89 14.10 16.70 15.23 44.19 34.79 29.24 36.07 4.44 7.34 11.00 7.59 51.94 33.02 30.97 42.48 Few-shot 22.56 15.86 20.03 19.48 17.50 30.11 16.91 21.51 Few-shot 22.76 19.47 22.34 21.52 17.31 20.69 17.00 18.33 6.78 8.51 4.31 6.53 78.77 75.31 83.04 79.04 7 \u20dd 5 \u20dd + Adapter (Pfeiffer et al., 2020c) Few-shot 23.82 21.28 23.22 22.77 15.78 21.69 15.62 17.70 8 \u20dd 7 \u20dd + Fusion (XDFusion) Few-shot 26.71 21.39 23.78 23.96 9.76 18.46 12.51 13.58fromthe public Common Crawl web scrape as Global-WOZ training data of the corresponding languagefor 10 epochs, with a batch size of 6 and learning"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "SacreBLEU and Slot Error Rate (SER) of different cross-lingual methods in NLG task of three target languages. Best scores are highlighted in bold. \u2191 indicates the higher the better while \u2193 indicates the lower the better. avg implies the average result of three languages.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Comparison of using different pretrained models for F&F testing dataset in three languages. Best scores are highlighted in bold.", "figure_data": "ModelsacreBLEU (%) \u2191 zh es idavgzhSER (%) \u2193 es idavgmT5-small w/ XDFusion5.6 8.97.6 9.87.6 7.76.9 59.5 47.4 45.6 50.8 8.8 41.9 38.5 40.6 40.3"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Comparison of using different pretrained models for F&E testing dataset in three target languages. Best scores are highlighted in bold.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Comparison of using different zero-shot approaches for F&F testing dataset in three target languages.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Figure 4: Examples of generated system responses from different models, along with its corresponding input and ground truth responses. The first example is sampled from MultiWOZ F&E Chinese (Zh) test dataset and the other is from F&F. Red words indicate the correct local entities (F&E has English entities; F&F has Foreign entities). Orange words indicate wrong code-switched responses. Green words indicate wrong foreign entities.", "figure_data": "Qualitative analysis. Figure 4 shows the gener-ated response examples from different models onthe F&E and F&F test sets. We first observe that if we directly employ mSeq2seq models trained withonly E&E data to low-resource language tasks, we can see the off-target problem causes modelsto generate English responses where the target lan-guage indicator is omitted. Instead, Translate-Trainmethod generates Chinese correctly except the en-tities are erroneous due to wrong-translated entitiesfrom model supervision. Both XDFusion and thefew-shot baseline (E&E + F&F) generate reason-able responses that correctly follow the given sys-tem acts. The results further elucidate XDFusion'shigh flexibility to generalize to new target languagewith very limited training data, by generating morefruitful responses with consistent local entities."}], "formulas": [{"formula_id": "formula_0", "formula_text": "S ACT = I(s 1 = v 1 , . . . , s k = v k )(1)", "formula_coordinates": [2.0, 341.62, 658.46, 183.53, 13.85]}, {"formula_id": "formula_1", "formula_text": "H + I(s 1 = v 1 , . . . , s k = v k ) S ACT \u2192 S(2)", "formula_coordinates": [2.0, 338.22, 741.59, 186.93, 29.85]}, {"formula_id": "formula_2", "formula_text": "(i) 1 , ..., x (i) L", "formula_coordinates": [3.0, 175.15, 463.44, 44.46, 17.16]}, {"formula_id": "formula_3", "formula_text": "(i) 1 , ..., y (i) T", "formula_coordinates": [3.0, 213.85, 505.63, 44.36, 17.16]}, {"formula_id": "formula_4", "formula_text": "L M LE (\u03b8) = N i=1 T t=1 log p \u03b8 (y (i) t |y (i) <t , x (i) ) (3) p \u03b8 (y (i) t |y (i) <t , x (i) ) = sof tmax(Wh (i) t + b) (4) h (i) t = g(y (i) t\u22121 , f (x (i) ; \u03b8); \u03b8)(5)", "formula_coordinates": [3.0, 80.66, 543.76, 209.21, 74.4]}, {"formula_id": "formula_5", "formula_text": "D En \u2192 D F &F F o \u2022 Few-shot F&F: D En + D F &F \u2020 F o \u2192 D F &F F o \u2022 Few-shot F&E: D En + D F &E \u2020 F o \u2192 D F &E F o", "formula_coordinates": [3.0, 306.14, 201.35, 190.65, 72.09]}, {"formula_id": "formula_6", "formula_text": "Adapter l (h l , r l ) = U l (ReLU(D l (h l ))) + r l (6)", "formula_coordinates": [4.0, 78.15, 439.18, 211.72, 14.4]}, {"formula_id": "formula_7", "formula_text": "\u03a8 t \u2190 argmin \u03a8 L t (D t ; \u0398, \u03d5 s , \u03d5 t , \u03a8) (7)", "formula_coordinates": [4.0, 339.35, 466.06, 185.8, 21.46]}], "doi": "10.1016/j.csl.2008.10.001"}