{"title": "Smooth Sparse Coding via Marginal Regression for Learning Sparse Representations", "authors": "Krishnakumar Balasubramanian; Kai Yu; Guy Lebanon", "pub_date": "2012-10-04", "abstract": "We propose and analyze a novel framework for learning sparse representations, based on two statistical techniques: kernel smoothing and marginal regression. The proposed approach provides a flexible framework for incorporating feature similarity or temporal information present in data sets, via non-parametric kernel smoothing. We provide generalization bounds for dictionary learning using smooth sparse coding and show how the sample complexity depends on the L1 norm of kernel function used. Furthermore, we propose using marginal regression for obtaining sparse codes, which significantly improves the speed and allows one to scale to large dictionary sizes easily. We demonstrate the advantages of the proposed approach, both in terms of accuracy and speed by extensive experimentation on several real data sets. In addition, we demonstrate how the proposed approach could be used for improving semi-supervised sparse coding.", "sections": [{"heading": "Introduction", "text": "Sparse coding is a popular unsupervised paradigm for learning sparse representations of data samples, that are subsequently used in classification tasks. In standard sparse coding, each data sample is coded independently with respect to the dictionary. We propose a smooth alternative to traditional sparse coding that incorporates feature similarity, temporal or other user-specified domain information between the samples, into the coding process.\nThe idea of smooth sparse coding is motivated by the relevance weighted likelihood principle. Our approach constructs a code that is efficient in a smooth sense and as a result leads to improved statistical accuracy over traditional sparse coding. The smoothing operation, which could be expressed as non-parametric kernel smoothing, provides a flexible framework for incorporating several types of domain information that might be available for the user. For example, for image classification task, one could use: (1) kernels in feature space for encoding similarity information for images and videos, (2) kernels in time space in case of videos for incorporating temporal relationship, and (3) kernels on unlabeled image in the semi-supervised learning and transfer learning settings.\nMost sparse coding training algorithms fall under the general category of alternating procedures with a convex lasso regression sub-problem. While efficient algorithms for such cases exist [22,11], their scalability for large dictionaries remains a challenge. We propose a novel training method for sparse coding based on marginal regression, rather than solving the traditional alternating method with lasso sub-problem. Marginal regression corresponds to several univariate linear regression followed by a thresholding step to promote sparsity. For large dictionary sizes, this leads to a dramatic speedup compared to traditional sparse coding methods (up to two orders of magnitude) without sacrificing statistical accuracy.\nWe further develop theory that extends the sample complexity result of [20] for dictionary learning using standard sparse coding to the smooth sparse coding case. We specifically show how the sample complexity depends on the L 1 norm of the kernel function used.\nOur main contributions are: (1) proposing a framework based on kernel-smoothing for incorporating feature, time or other similarity information between the samples into sparse coding, (2) providing sample complexity results for dictionary learning using smooth sparse coding, (3) proposing an efficient marginal regression training procedure for sparse coding, and (4) successful application of the proposed method in various classification tasks. Our contributions lead to improved classification accuracy in conjunction with computational speedup of two orders of magnitude.", "publication_ref": ["b21", "b10", "b19", "b0", "b1", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Related work", "text": "Our approach is related to the local regression method [13,7]. More recent related work is [15] that uses smoothing techniques in high-dimensional lasso regression in the context of temporal data. Another recent approach proposed by [26] achieves code locality by approximating data points using a linear combination of nearby basis points. The main difference is that traditional local regression techniques do not involve basis learning. In this work, we propose to learn the basis or dictionary along with the regression coefficients locally.\nIn contrast to previous sparse coding papers we propose to use marginal regression for learning the regression coefficients, which results in a significant computational speedup with no loss of accuracy. Marginal regression is a relatively old technique that has recently reemerged as a computationally faster alternative to lasso regression [5]. See also [6] for a statistical comparison of lasso regression and marginal regression.", "publication_ref": ["b12", "b6", "b14", "b25", "b4", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Smooth Sparse Coding", "text": "Notations: The notations x and X correspond to vectors and matrices respectively, in appropriately defined dimensions; the notation \u2022 p corresponds to the L p norm of a vector (we use mostly use p = 1, 2 in this paper); the notation \u2022 F corresponds to the Frobenius norm of a matrix; the notation |f | p corresponds to the L p norm of the function f : ( |f | p d\u00b5) 1/p ; the notation x i , i = 1, . . . , n corresponds to the data samples, where we assume that each sample x i is a d-dimensional vector. The explanation below uses L 1 norm for sparsity for simplicity. But the method applies more generally to any structured regularizers, for e.g., [3,8].\nThe standard sparse coding problem consists of solving the following optimization problem, min\nD\u2208R d\u00d7K \u03b2i\u2208R K ,i=1,...,n n i=1 x i \u2212 D\u03b2 i 2 2\nsubject to\nd j 2 \u2264 1 j = 1, . . . K \u03b2 i 1 \u2264 \u03bb i = 1, . . . n.\nwhere \u03b2 i \u2208 R K corresponds to the encoding of sample x i with respected to the dictionary D \u2208 R d\u00d7K and d j \u2208 R d denotes the j-column of the dictionary matrix D. The dictionary is typically over-complete, implying that K > d. Object recognition is a common sparse coding application where x i corresponds to a set of features obtained from a collection of image patches, for example SIFT features [14]. The dictionary D corresponds to an alternative coding scheme that is higher dimensional than the original feature representation. The L 1 constraint promotes sparsity of the new encoding with respect to D. Thus, every sample is now encoded as a sparse vector that is of higher dimensionality than the original representation.\nIn some cases the data exhibits a structure that is not captured by the above sparse coding setting. For example, SIFT features corresponding to samples from the same class are presumably closer to each other compared to SIFT features from other classes. Similarly in video, neighboring frames are presumably more related to each other than frames that are farther apart. In this paper we propose a mechanism to incorporate such feature similarity and temporal information into sparse coding, leading to a sparse representation with an improved statistical accuracy (for example as measured by classification accuracy).\nWe consider the following smooth version of the sparse coding problem above:\nmin\nD\u2208R d\u00d7K \u03b2i\u2208R K ,i=1,...,n n i=1 n j=1 w(x j , x i ) x j \u2212 D\u03b2 i 2 2\n(1)\nsubject to d j 2 \u2264 1 j = 1, . . . K (2) \u03b2 i 1 \u2264 \u03bb i = 1, . . . n.(3)\nwhere n j=1 w(x j , x i ) = 1 for all i. It is convenient to define the weight function through a smoothing kernel\nw(x j , x i ) = 1 h 1 K 1 \u03c1(x j , x i ) h 1\nwhere \u03c1(\u2022, \u2022) is a distance function that captures the feature similarity, h 1 is the bandwidth, and K 1 is a smoothing kernel. Traditional sparse coding minimizes the reconstruction error of the encoded samples. Smooth sparse coding, on the other hand, minimizes the reconstruction of encoded samples with respect to their neighbors (weighted by the amount of similarity). The smooth sparse coding setting leads to codes that represent a neighborhood rather than an individual sample and that have lower mean square reconstruction error (with respect to a given dictionary), due to lower estimation variance (see for example the standard theory of smoothed empirical process [4]).", "publication_ref": ["b2", "b7", "b13", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "The choice of smoothing kernel", "text": "There are several possible ways to determine the weight function w. One common choice for the kernel function is the Gaussian kernel whose bandwidth is selected using cross-validation. Other common choices for the kernel are the triangular, uniform, and tricube kernels. The bandwidth may be fixed throughout the input space, or may vary in order to take advantage of non-uniform samples. We use in our experiment the tricube kernel with a constant bandwidth.\nThe distance function \u03c1(\u2022, \u2022) may be one of the standard distance functions (for example based on the L p norm). Alternatively, \u03c1(\u2022, \u2022) may be expressed by domain experts, learned from data before the sparse coding training, or learned jointly with the dictionary and codes during the sparse coding training.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Spatio-Temporal smoothing", "text": "In spatio-temporal applications we can extend the kernel to include also a term reflecting the distance between the corresponding time or space\nw(x j , x i ) = 1 h 1 K 1 \u03c1(x j , x i ) h 1 1 h 2 K 2 j \u2212 i h 2 .\nAbove, K 2 is a univariate symmetric kernel with bandwidth parameter h 2 . One example is video sequences, where the kernel above combines similarity of the frame features and the time-stamp. Alternatively, the weight function can feature only the temporal component and omit the first term containing the distance function between the feature representation. A related approach for that situation, is based on the Fused lasso which penalizes the absolute difference between codes for neighboring points. The main drawback of that approach is that one needs to fit all the data points simultaneously whereas in smooth sparse coding, the coefficient learning step decomposes as n separate problems which provides a computational advantage (see Section 9.1.5 for more details). Also, while fused Lasso penalty is suitable for time-series data to capture relatedness between neighboring frames, it may not be immediately suitable for other situations that the proposed smooth sparse coding method could handle.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Marginal Regression for Smooth Sparse Coding", "text": "A standard algorithm for sparse coding is the alternating bi-convex minimization procedure, where one alternates between (i) optimizing for codes (with a fixed dictionary) and (ii) optimizing for dictionary (with fixed codes). Note that step (i) corresponds to regression with L 1 constraints and step (ii) corresponds to least squares with L 2 constraints. In this section we show how marginal regression could be used to obtain better codes faster (step (i)). In order to do so, we first give a brief description of the marginal regression procedure.\nMarginal Regression: Consider a regression model y = X\u03b2 + z where y \u2208 R n , \u03b2 \u2208 R p , X \u2208 R n\u00d7p with L 2 normalized columns (denoted by x j ), and z is the noise vector. Marginal regression proceeds as follows:\n\u2022 Calculate the least squares solution\u03b1 (j) = x T j y.\n\u2022 Threshold the least-square coefficients\u03b2 (j) =\u03b1 (j) 1 {|\u03b1 (j) |>t} , j = 1, . . . , p.\nMarginal regression requires just O(np) operations compared to O(p 3 + np 2 ), the typical complexity of lasso algorithms. When p is much larger than n, marginal regression provides two orders of magnitude over Lasso based formulations. Note that in sparse coding, the above speedup occurs for each iteration of the outer loop, thus enabling sparse coding for significantly larger dictionary sizes. Recent studies have suggested that marginal regression is a viable alternative for Lasso given its computational advantage over lasso. A comparison of the statistical properties of marginal regression and lasso is available in [5,6].\nApplying marginal regression to smooth sparse coding, we obtain the following scheme. The marginal least squares coefficients are\u03b1\n(k) i = n j=1 w(x j , x i ) d k 2 d T k x j .\nWe sort these coefficient in terms of their absolute values, and select the top s coefficients whose L 1 norm is bounded by \u03bb:\u03b2\n(k) i = \u03b1 (k) i k \u2208 S 0 k / \u2208 S ,where\nS = 1, . . . , s : s \u2264 d : s k=1 |\u03b1 (k) i | \u2264 \u03bb\nWe select the thresholding parameter using cross validation in each of the sparse coding iterations. Note that the same approach could be used with structured regularizers too, for example [3,8].\nMarginal regression works well when there is minimal correlation between the different dictionary atoms. In the linear regression setting, marginal regression performs much better with orthogonal data [6]. In the context of sparse coding, this corresponds to having uncorrelated or incoherent dictionaries [19]. One way to measure such incoherence is using the babel function, which bounds the maximum inner product between two different columns d i , d j : \n\u00b5 s (D) = max\nD = {D \u2208 R d\u00d7K : d j 2 2 \u2264 1, D \u22a4 D \u2212 I 2\nF \u2264 \u03b3}. We use the method of optimal directions update [17] to solve the above optimization problem. Specifically, representing the constraints using the Lagrangian and setting the derivative with respect to D to zero, we get the following update ruleD\n(t+1) = B (t+1)B \u22a4 (t+1) + 2\u03baD \u22a4 tDt + 2\u03b7diag(D \u22a4 tDt ) XB \u22a4 (t+1) + 2(\u03ba + \u03b7)D t .\nAbove,B t = [\u03b2 1 (t), . . . ,\u03b2 n (t)] is the matrix of data codes obtained in iteration t, X \u2208 R p\u00d7n is the data in matrix format, \u03ba is a regularization parameter corresponding to the incoherence constraints, and \u03b7 is a regularization parameter corresponding to the normalization constraints. Note that if \u03ba = \u03b7 = 0, the update reduces to standard least squares update with no constraints.\nA sequence of such updates corresponding to step (i) and step (ii) converges to a stationary point of the optimization problem (this can be shown using Zangwill's theorem [27]). But no provable algorithm that converges to the global minimum of the smooth sparse coding (or standard sparse coding) exists yet. Nevertheless, the main idea of this section is to speed-up the existing alternating bi-convex minimization procedure for obtaining sparse representations, by using marginal regression.", "publication_ref": ["b4", "b5", "b2", "b7", "b5", "b18", "b16", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 1 Smooth Sparse Coding via Marginal Regression", "text": "Input: Data {(x 1 , y 1 ), . . . , (x n , y n )} and kernel/similarity measure K 1 and d 1 .\nPrecompute: Compute the weight matrix w(i, j) using the kernel/similarity measure and Initialize: Set the dictionary at time zero to be D 0 .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm: repeat", "text": "Step (i): For all i = 1, . . . , n, solve marginal regression:\n\u03b1 (k) i = n j=1 w(x j , x i ) d k 2 d T k x \u0135 \u03b2 (k) j = \u03b1 (k) j j \u2208 S 0 j / \u2208 S , S = {1, . . . , s; s \u2264 d : s k=1 |\u03b1 (k) i | \u2264 \u03bb}.\nStep (ii): Update the dictionary based on codes from previous step.\nD t = arg min D\u2208D n i=1 x i \u2212 D\u03b2 i (t) 2 2 ,\nwhere\nD = {D \u2208 R d\u00d7K : d j 2 2 \u2264 1, D \u22a4 D \u2212 I 2 F \u2264 \u03b3}\nuntil convergence Output: Return the learned codes and dictionary.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sample Complexity of Smooth sparse coding", "text": "In this section, we analyze the sample complexity of the proposed smooth sparse coding framework. Specifically, since there does not exist a provable algorithm that converges to the global minimum of the optimization problem in Equation ( 1), we provide uniform convergence bounds over the dictionary space and thereby prove a sample complexity result for dictionary learning under smooth spare coding setting. We leverage the analysis for dictionary learning in the standard sparse coding setting by [20] and extend it to the smooth sparse coding setting. The main difficulty for the smooth sparse coding setting is obtaining a covering number bound for an appropriately defined class of functions (see Theorem 1 for more details). We begin by re-representing the smooth sparse coding problem in a convenient format for analysis. Let x 1 , . . . , x n be independent random variables with a common probability measure P with a density p. We denote by P n the empirical measure over the n samples, and the kernel density estimate of p is defined by\np n,h (x) = 1 nh n i=1 K x \u2212 X i 2 h . Let K h1 (\u2022) = 1 h1 K 1 ( \u2022 h ).\nWith the above notations, the reconstruction error at the point x is given by\nr \u03bb (x) = min \u03b2\u2208S \u03bb x \u2032 \u2212 D\u03b2 2 K h1 (\u03c1(x, x \u2032 )) dP n (x \u2032 )\nwhere\nS \u03bb = {\u03b2 : \u03b2 1 \u2264 \u03bb}.\nThe empirical reconstruction error is\nE Pn (r) = min \u03b2\u2208S \u03bb x \u2032 \u2212 D\u03b2 2 K h1 (\u03c1(x, x \u2032 )) dP n (x \u2032 ) dx\nand its population version is\nE P (r) = min \u03b2\u2208S \u03bb x \u2032 \u2212 D\u03b2 2 K h1 (\u03c1(x, x \u2032 )) dP(x \u2032 ) dx.\nOur goal is to show that the sample reconstruction error is close to the true reconstruction error. Specifically, to show E P (r \u03bb ) \u2264 (1 + \u03ba)E Pn (r \u03bb )+ \u01eb where \u01eb, \u03ba \u2265 0, we bound the covering number of the class of functions corresponding to the reconstruction error. We assume a dictionary of bounded babel function, which holds as a result of the relaxed orthogonality constraint used in the Algorithm 1 (see also [17]). We define the set of r functions with respect the the dictionary D (assuming data lies in the unit d-dimensional ball S d\u22121 ) by\nF \u03bb = {r \u03bb : S d\u22121 \u2192 R : D \u2208 R d\u00d7K , d i 2 \u2264 1, \u00b5 s (D) \u2264 \u03b3}.\nThe following theorem bounds the covering number of the above function class.\nTheorem 5.1. For every \u01eb > 0, the metric space (F \u03bb , | \u2022 | \u221e ) has a subset of cardinality at most\n4\u03bb|K h 1 (\u2022)|1 \u01eb(1\u2212\u03b3) dK\n, such that every element from the class is at a distance of at most \u01eb from the subset, where\n|K h1 (\u2022)| 1 = |K h1 (x)| dP. Proof. Let F \u2032 \u03bb = {r \u2032 \u03bb : S d\u22121 \u2192 R : D \u2208 d \u00d7 K, d i 2 \u2264 1}, where r \u2032 \u03bb (x) = min \u03b2\u2208S \u03bb D\u03b2 \u2212 x .\nWith this definition we note that F \u03bb is just F \u2032 \u03bb convolved with the kernel K h1 (\u2022). By Young's inequality [4] we have,\n|K h1 * (s 1 \u2212 s 2 )| p \u2264 |K h1 | 1 |s 1 \u2212 s 2 | p , 1 \u2264 p \u2264 \u221e\nfor any L p integrable functions s 1 and s 2 . Using this fact, we see that convolution mapping between metric spaces F \u2032 and F converts \u01eb |K h 1 (\u2022)|1 covers into \u01eb covers. From [20], we have that the the class F \u2032 \u03bb has \u01eb covers of size at most ( 4\u03bb \u01eb(1\u2212\u03b3) ) dK . This proves the the statement of the theorem.\nThis leads to the following generalization bound for the smooth sparse coding.\nTheorem 5.2. Let \u03b3 < 1, \u03bb > e/4 with distribution P on S d\u22121 . Then with probability at least 1 \u2212 e \u2212t over the n samples drawn according to P, for all the D with unit length columns and \u00b5 s (D) \u2264 \u03b3, we have:\nE P (r \u03bb ) \u2264 E Pn (r \u03bb ) + dK ln 4 \u221a n\u03bb|K h 1 (\u2022)|1 (1\u2212\u03b3) 2n + t 2n + 4 n\nThe above theorem follows from the previous covering number bound and the following lemma for generalization bound that is based on the result in [20] \nconcerning | \u2022 | \u221e covering numbers. Lemma 1. Let Q be a function class of [0, B] functions with covering number ( C \u01eb ) d > e B 2 under | \u2022 | \u221e norm.\nThen for every t > 0 with probability at least 1 \u2212 e \u2212t , for all q \u2208 Q, we have:\nE f \u2264 E n f + B d ln(C \u221a n) 2n + t 2n + 4 n .\nThe above theorem, shows that the generalization error scales as O(n \u22121/2 ) (assuming the other problem parameters fixed). In the case of \u03ba > 0, it is possible to obtain faster rates of O(n \u22121 ) for smooth sparse coding, similar to derivations in [1]. The following theorem gives the precise statement.\nTheorem 5.3. Let \u03b3 < 1, \u03bb > e/4, dK > 20 and n \u2265 5000. Then with probability at least 1 \u2212 e \u2212t , we have for all D with unit length and \u00b5 s (D) \u2264 \u03b3,\nE P (r \u03bb ) \u2264 1.1E Pn (r \u03bb ) + 9 dK ln 4n\u03bb|K h 1 (\u2022)|1 (1\u2212\u03b3) + t n .\nThe above theorem follows from the covering number bound above and Proposition 22 from [20]. The definition of r \u03bb (x) differs from (1) by a square term, but it could easily be incorporated into the above bounds resulting in an additive factor of 2 inside the logarithm term.", "publication_ref": ["b19", "b16", "b3", "b19", "b19", "b0", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We demonstrate the advantage of the proposed approach both in terms of speed-up and accuracy, over standard sparse coding. A detailed description of all real-world data sets used in the experiments are given in the appendix.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Speed comparison", "text": "We conducted synthetic experiments to examine the speed-up provided by sparse coding with marginal regression. The data was generated from a a 100 dimensional mixture of two Gaussian distribution that satisfies \u00b5 1 \u2212 \u00b5 2 2 = 3 (with identity covariance matrices). The dictionary size was fixed at 1024.\nWe compare the proposed smooth sparse coding algorithm, standard sparse coding with lasso [11] and marginal regression updates respectively, with a relative reconstruction error X \u2212DB F / X F convergence criterion. We experimented with different values of the relative reconstruction error (less than 10%) and report the average time.\nFrom Table 1, we see that smooth sparse coding with marginal regression takes significantly less time to achieve a fixed reconstruction error. This is due to the fact that it takes advantage of the spatial structure and use marginal regression updates. It is worth mentioning that standard sparse coding with marginal regression updates performs faster compared to the other two methods that uses lasso updates, as expected (but does not take into account the spatial structure).  ", "publication_ref": ["b10"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Experiments with Kernel in Feature space", "text": "We conducted several experiments demonstrating the advantage of the proposed coding scheme in different settings.\nConcentrating on face and object recognition from static images, we evaluated the performance of the proposed approach along with standard sparse coding and LLC [26], another method for obtaining sparse features based on locality. Also, we performed experiments on activity recognition from videos based on both space and time based kernels. As mentioned before all results are reported using tricube kernel.", "publication_ref": ["b25"], "figure_ref": [], "table_ref": []}, {"heading": "Image classification", "text": "We conducted image classification experiments on CMU-multipie, 15 Scene and Caltech-101 data sets. Following [24] , we used the following approach for generating sparse image representation: we densely sampled 16 \u00d7 16 patches from images at the pixel level on a gird with step size 8 pixels, computed SIFT features, and then computed the corresponding sparse codes over a 1024-size dictionary. We used max pooling to get the final representation of the image based on the codes for the patches. The process was repeated with different randomly selected training and testing images and we report the average per-class recognition rates (together with its standard deviation estimate) based on one-vs-all SVM classification. We used cross validation to select the regularization and bandwidth parameters.\nAs Table 2 indicates, our smooth sparse coding algorihtm resulted in significantly higher classification accuracy than standard sparse coding and LLC. In fact, the reported performance is better than previous reported results using unsupervised sparse coding techniques [24].\nDictionary size: In order to demonstrate the use of scalability of the proposed method with respect to dictionary size, we report classification accuracy with increasing dictionary sizes using smooth sparse coding. The main advantage of the proposed marginal regression training method is that one could easily run experiments with larger dictionary sizes, which typically takes a significantly longer time for other algorithms. For both the Caltech-101 and 15-scene data set, classification accuracy increases significantly with increasing dictionary sizes as seen in Table 3.  Table 3: Effect of dictionary size on classification accuracy using smooth sparse coding and marginal regression on 15 scene and Caltech -101 data set.", "publication_ref": ["b23", "b23"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Action recognition:", "text": "We further conducted an experiment on activity recognition from videos with KTH action and YouTube data set (see Appendix). Similar to the static image case, we follow the standard approach for generating sparse representations for videos as in [21]. We densely sample 16 \u00d7 16 \u00d7 10 blocks from the video and extract HoG-3d [10] features from the sampled blocks. We then use smooth sparse coding and max-pooling to generate the video representation (dictionary size was fixed at 1024 and cross-validation was used to select the regularization and bandwidth parameters). Previous approaches include sparse coding, vector quantization, and k-means on top of the HoG-3d feature set (see [21] for a comprehensive evaluation). As indicated by Table 4, smooth sparse coding results in higher classification accuracy than previously reported state-of-the-art and standard sparse coding on both datasets (see [21,12] for a description of the alternative techniques).", "publication_ref": ["b20", "b9", "b20", "b20", "b11"], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Discriminatory power", "text": "In this section, we describe another experiment that contrasts the codes obtained by sparse coding and smooth sparse coding in the context of a subsequent classification task. As in [25], we first compute the codes in both case based on patches and combine it with max-pooling to obtain the image level representation. We then compute the fisher discriminant score (ratio of within-class variance to between-class variance) for each dimension as measures of the discrimination power realized by the representations. Figure 1, graphs a histogram of the ratio of smooth sparse coding Fisher score over standard sparse coding Fisher score R(d) = F 1 (d)/F 2 (d) for 15-scene dataset (left) and Youtube dataset (right). Both histograms demonstrate the improved discriminatory power of smooth sparse coding over regular sparse coding.", "publication_ref": ["b24"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Experiments using Temporal Smoothing", "text": "In this section we describe an experiment conducted using the temporal smoothing kernel on the Youtube persons dataset. We extracted SIFT descriptors for every 16 \u00d7 16 patches sampled on a grid of step size 8 and used smooth sparse coding with time kernel to learn the codes and max pooling to get the final video representation. We avoided pre-processing steps such as face extraction or face tracking. Note that in the previous action recognition video experiment, video blocks were densely sampled and used for extracting HoG-3d features. In this experiment, on the other hand, we extracted SIFT features from individual frames and used the time kernels to incorporate the temporal information into the sparse coding process.\nFor this case, we also compared to the more standard fused-lasso based approach [18]. Note that in fused Lasso based approach, in addition to the standard L 1 penalty, an additional L 1 penalty on the difference between the neighboring frames for each dimensions is used. This tries to enforce the assumption that in a video sequence, neighboring frames are more related to one another as compared to frames that are farther apart.\nTable 5 shows that smooth sparse coding achieved higher accuracy than fused lasso and standard sparse coding. Smooth sparse coding has comparable accuracy on person recognition tasks to other methods that use face-tracking, Cited method SC SSC 92. 10 [21] 92.423 93.549 71.2 [12] 72.640 74.974   for example [9]. Another advantage of smooth sparse coding is that it is significantly faster than sparse coding and the used lasso.", "publication_ref": ["b17", "b11", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "7", "text": "Semi-supervised smooth sparse coding\nOne of the primary difficulties in some image classification tasks is the lack of availability of labeled data and in some cases, both labeled and unlabeled data (for particular domains). This motivated semi-supervised learning and transfer learning without labels [16] respectively. The motivation for such approaches is that data from a related domain might have some visual patterns that might be similar to the problem at hand. Hence, learning a high-level dictionary based on data from a different domains aids the classification task of interest. We propose that the smooth sparse coding approach might be useful in this setting. The motivation is as follows: in semi-supervised, typically not all samples from a different data set might be useful for the task at hand. Using smooth sparse coding, one can weigh the useful points more than the other points (the weights being calculated based on feature/time similarity kernel) to obtain better dictionaries and sparse representations. Other approach to handle a lower number of labeled samples include collaborative modeling or multi-task approaches which impose a shared structure on the codes for several tasks and use data from all the tasks simultaneously, for example group sparse coding [2]. The proposed approach provides an alternative when such collaborative modeling assumptions do not hold, by using relevant unlabeled data samples that might help the task at hand via appropriate weighting.\nWe now describe an experiment that examines the proposed smoothed sparse coding approach in the context of semi-supervised dictionary learning. We use data from both CMU multi-pie dataset (session 1) and faces-on-tv dataset (treated as frames) to learn a dictionary using a feature similarity kernel. We follow the same procedure described in the previous experiments to construct the dictionary. In the test stage we use the obtained dictionary for coding data from sessions 2, 3, 4 of CMU-multipie data set, using smooth sparse coding. Note that semi-supervision was used only in the dictionary learning stage (the classification stage used supervised SVM).\nTable 6 shows the test set error rate and compares it to standard sparse coding and LLC [26]. Smooth sparse coding achieves significantly lower test error rate than the two alternative techniques. We conclude that the smoothing approach described in this paper may be useful in cases where there is a small set of labeled data, such as semisupervised  ", "publication_ref": ["b15", "b1", "b25"], "figure_ref": [], "table_ref": []}, {"heading": "Discussion and Future work", "text": "We proposed a simple framework for incorporating similarity in feature space and space or time into sparse coding. The codes obtained by smooth sparse coding are significantly more discriminatory than traditional sparse coding, and lead to substantially improved classification accuracy as measured on several different image and video classification tasks.\nWe also propose in this paper modifying sparse coding by replacing the lasso optimization stage by marginal regression and adding a constraint to enforce incoherent dictionaries. The resulting algorithm is significantly faster (speedup of about two-orders of magnitude over standard sparse coding). This facilitates scaling up the sparse coding framework to large dictionaries, an area which is usually restricted due to intractable computation. We also explore promising extensions to temporal smoothing, semi-supervised learning and transfer learning. We provide bounds on the covering numbers that lead to generalization bounds for the smooth sparse coding dictionary learning problem.\nThere are several ways in which the proposed approach can be extended. First, using an adaptive or non-constant kernel bandwidth should lead to higher accuracy. It is also interesting to explore tighter generalization error bounds by directly analyzing the solutions of the marginal regression iterative algorithm. Another potentially useful direction is to explore alternative incoherence constraints that lead to easier optimization and scaling up. The face recognition experiment was conducted on the CMU Multi-PIE dataset. The dataset is challenging due to the large number of subjects and is one of the standard data sets used for face recognition experiments. The data set contains 337 subjects across simultaneous variations in pose, expression, and illumination. We ignore the 88 subjects that were considered as outliers in [24] and used the rest of the images for our face recognition experiments. We follow [24] and use the 7 frontal extreme illuminations from session one as train set and use other 20 illuminations from Sessions 2-4 as test set.", "publication_ref": ["b23", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "15 Scenes Categorization:", "text": "We also conducted scene classification experiments on the 15-Scenes data set. This data set consist of 4485 images from 15 categories, with the number of images each category ranging from 200 to 400. The categories corresponds to scenes from various settings like kitchen, living room etc. Similar to the previous experiment, we extracted patches from the images and computed the SIFT features corresponding to the patches.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Caltech-101 Data set:", "text": "The Caltech-101 data set consists of images from 101 classes like animals, vehicles, flowers, etc. The number of images per category varies from 30 to 800. Most images are of medium resolution (300 \u00d7 300). All images are used a gray-scale images. Following previous standard experimental settings for Caltech-101 data set, we use 30 images per category and test on the rest. Average classification accuracy normalized by class frequency is used for evaluation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Activity recognition", "text": "The KTH action dataset consists of 6 human action classes. Each action is performed several times by 25 subjects and is recorded in four different scenarios. In total, the data consists of 2391 video samples. The YouTube actions data set has 11 action categories and is more complex and challenging [12]. It has 1168 video sequences of varied illumination, background, resolution etc. We randomly densely sample blocks (400 cuboids) of video from the data sample and extract HOG-3d features and constructed the video features as described above.", "publication_ref": ["b11"], "figure_ref": [], "table_ref": []}, {"heading": "Youtube person data set", "text": "Similar to the experiments using the feature smoothing kernel, in this section we report results on experiment conducted using the time smoothed kernel. Specifically, we used the YouTube person data set [9] in order to recognize people, based on time-based kernel smooth sparse coding. The dataset contains 1910 sequences of 47 subjects. The architecture for this dataset is similar to [23].", "publication_ref": ["b8", "b22"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Local rademacher complexities. The Annals of Statistics", "journal": "", "year": "2005", "authors": "P L Bartlett; O Bousquet; S Mendelson"}, {"ref_id": "b1", "title": "Group sparse coding", "journal": "", "year": "2009", "authors": "S Bengio; F Pereira; Y Singer; D Strelow"}, {"ref_id": "b2", "title": "Learning efficient structured sparse models", "journal": "", "year": "2012", "authors": "A Bronstein; P Sprechmann; G Sapiro"}, {"ref_id": "b3", "title": "Combinatorial methods in density estimation", "journal": "", "year": "2001", "authors": "L Devroye; G Lugosi"}, {"ref_id": "b4", "title": "Sure independence screening for ultrahigh dimensional feature space", "journal": "JRSS: B(Statistical Methodology)", "year": "2008", "authors": "J Fan; J Lv"}, {"ref_id": "b5", "title": "A comparison of the lasso and marginal regression", "journal": "JMLR", "year": "2012", "authors": "C R Genovese; J Jin; L Wasserman; Z Yao"}, {"ref_id": "b6", "title": "Local regression: Automatic kernel carpentry", "journal": "Statistical Science", "year": "1993", "authors": "T Hastie; C Loader"}, {"ref_id": "b7", "title": "Proximal methods for sparse hierarchical dictionary learning", "journal": "", "year": "2010", "authors": "R Jenatton; J Mairal; G Obozinski; F Bach"}, {"ref_id": "b8", "title": "Face tracking and recognition with visual constraints in real-world videos", "journal": "", "year": "2008", "authors": "M Kim; S Kumar; V Pavlovic; H Rowley"}, {"ref_id": "b9", "title": "A spatio-temporal descriptor based on 3d-gradients", "journal": "", "year": "2008", "authors": "A Kl\u00e4ser; M Marsza; C Schmid"}, {"ref_id": "b10", "title": "Efficient sparse coding algorithms", "journal": "", "year": "2007", "authors": "H Lee; A Battle; R Raina; A Y Ng"}, {"ref_id": "b11", "title": "Recognizing realistic actions from videos in the wild", "journal": "", "year": "2009", "authors": "J Liu; J Luo; M Shah"}, {"ref_id": "b12", "title": "Local regression and likelihood", "journal": "Springer Verlag", "year": "1999", "authors": "C Loader"}, {"ref_id": "b13", "title": "Object recognition from local scale-invariant features", "journal": "", "year": "", "authors": "D G Lowe"}, {"ref_id": "b14", "title": "Smoothing l1-penalized estimators for high-dimensional time-course data", "journal": "Electronic Journal of Statistics", "year": "2007", "authors": "L Meier; P B\u00fchlmann"}, {"ref_id": "b15", "title": "Self-taught learning: transfer learning from unlabeled data", "journal": "", "year": "2007", "authors": "R Raina; A Battle; H Lee; B Packer; A Y Ng"}, {"ref_id": "b16", "title": "Sparse modeling with universal priors and learned incoherent dictionaries", "journal": "", "year": "2009", "authors": "I Ram\u0131rez; F Lecumberry; G Sapiro"}, {"ref_id": "b17", "title": "Sparsity and smoothness via the fused lasso", "journal": "JRSS:B", "year": "2005", "authors": "R Tibshirani; M Saunders; S Rosset; J Zhu; K Knight"}, {"ref_id": "b18", "title": "Greed is good: Algorithmic results for sparse approximation. Information Theory", "journal": "IEEE", "year": "2004", "authors": "J A Tropp"}, {"ref_id": "b19", "title": "The sample complexity of dictionary learning", "journal": "JMLR", "year": "2011", "authors": "D Vainsencher; S Mannor; A M Bruckstein"}, {"ref_id": "b20", "title": "Evaluation of local spatio-temporal features for action recognition", "journal": "", "year": "2009", "authors": "H Wang; M M Ullah; A Klaser; I Laptev; C Schmid"}, {"ref_id": "b21", "title": "Robust face recognition via sparse representation", "journal": "IEEE PAMI", "year": "2008", "authors": "J Wright; A Y Yang; A Ganesh; S S Sastry; Y Ma"}, {"ref_id": "b22", "title": "Linear spatial pyramid matching using sparse coding for image classification", "journal": "", "year": "2009", "authors": "J Yang; K Yu; Y Gong; T Huang"}, {"ref_id": "b23", "title": "Supervised translation-invariant sparse coding", "journal": "", "year": "2010", "authors": "J Yang; K Yu; T Huang"}, {"ref_id": "b24", "title": "Learning image representations from the pixel level via hierarchical sparse coding", "journal": "", "year": "2011", "authors": "K Yu; Y Lin; J Lafferty"}, {"ref_id": "b25", "title": "Nonlinear learning using local coordinate coding", "journal": "NIPS", "year": "2009", "authors": "K Yu; T Zhang; Y Gong"}, {"ref_id": "b26", "title": "Nonlinear programming: a unified approach", "journal": "", "year": "1969", "authors": "W I Zangwill"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "2 F2i\u2208{1,...,d} max \u039b\u2282{1,...,d}\\{i};|\u039b|=s j\u2208\u039b |d \u22a4 j d i |. An alternative, which leads to easier computation is enforcing the constraint D T D \u2212 I K\u00d7K when optimizing over the dictionary matrix DD = arg min D\u2208D n i=1 x i \u2212 D\u03b2 i 2 2 , where", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 1 :1Figure 1: Comparison between the histograms of Fisher discriminant score realized by sparse coding and smooth sparse coding. The images represent the histogram of the ratio of smooth sparse coding Fisher score over standard sparse coding Fisher score (left: image data set; right: video). A value greater than 1 implies that smooth sparse coding is more discriminatory.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "11Data set Description 9.1.1 CMU Multi-pie face recognition:", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "28\u00b12.12 73.20\u00b11.14 LLC 93.70\u00b12.22 82.28\u00b11.98 74.82\u00b11.65 SSC 94.14 \u00b12.01 84.10\u00b11.87 76.24\u00b12.15", "figure_data": "CMU-multipie15 sceneCaltech-101SC92.70\u00b11.2180."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Test set error accuracy for face recognition on CMU-multipie data set (left) 15 scene (middle) and Caltech-101 (right) respectively. The performance of the smooth sparse coding approach is better than the standard sparse coding and LLC in all cases.", "figure_data": "Dictionary size15 sceneCaltech-1011024 204884.10\u00b11.87 76.24 \u00b12.15 87.43\u00b11.55 78.33\u00b11.43409689.53\u00b12.00 79.11\u00b10.87"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Action recognition (accuracy) for cited method (left), Hog3d+ SC (middle) and Hog3d+ SSC (right): KTH data set(top) YouTube action dataset (bottom).", "figure_data": ""}, {"figure_label": "56", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Linear SVM accuracy for person recognition task from YouTube face video dataset. learning and transfer learning. Semi-supervised learning test set error: Dictionary learned from both CMU multi-pie and faces-on-tv data set using feature similarity kernel, used to construct sparse codes for CMU multipie data set.", "figure_data": "MethodSCLLC SSC-tricubeTest errror 6.345 6.0035.135"}], "formulas": [{"formula_id": "formula_0", "formula_text": "D\u2208R d\u00d7K \u03b2i\u2208R K ,i=1,...,n n i=1 x i \u2212 D\u03b2 i 2 2", "formula_coordinates": [2.0, 221.76, 399.29, 126.97, 35.53]}, {"formula_id": "formula_1", "formula_text": "d j 2 \u2264 1 j = 1, . . . K \u03b2 i 1 \u2264 \u03bb i = 1, . . . n.", "formula_coordinates": [2.0, 293.4, 440.25, 96.06, 32.04]}, {"formula_id": "formula_2", "formula_text": "min", "formula_coordinates": [2.0, 234.6, 656.01, 16.57, 9.96]}, {"formula_id": "formula_3", "formula_text": "D\u2208R d\u00d7K \u03b2i\u2208R K ,i=1,...,n n i=1 n j=1 w(x j , x i ) x j \u2212 D\u03b2 i 2 2", "formula_coordinates": [2.0, 214.56, 645.77, 182.41, 35.53]}, {"formula_id": "formula_4", "formula_text": "subject to d j 2 \u2264 1 j = 1, . . . K (2) \u03b2 i 1 \u2264 \u03bb i = 1, . . . n.(3)", "formula_coordinates": [2.0, 217.68, 686.85, 343.95, 31.92]}, {"formula_id": "formula_5", "formula_text": "w(x j , x i ) = 1 h 1 K 1 \u03c1(x j , x i ) h 1", "formula_coordinates": [3.0, 240.36, 83.13, 122.79, 24.21]}, {"formula_id": "formula_6", "formula_text": "w(x j , x i ) = 1 h 1 K 1 \u03c1(x j , x i ) h 1 1 h 2 K 2 j \u2212 i h 2 .", "formula_coordinates": [3.0, 205.44, 398.25, 201.12, 24.21]}, {"formula_id": "formula_7", "formula_text": "(k) i = n j=1 w(x j , x i ) d k 2 d T k x j .", "formula_coordinates": [4.0, 256.44, 192.05, 105.48, 31.21]}, {"formula_id": "formula_8", "formula_text": "(k) i = \u03b1 (k) i k \u2208 S 0 k / \u2208 S ,where", "formula_coordinates": [4.0, 221.76, 257.21, 127.54, 34.24]}, {"formula_id": "formula_9", "formula_text": "S = 1, . . . , s : s \u2264 d : s k=1 |\u03b1 (k) i | \u2264 \u03bb", "formula_coordinates": [4.0, 226.68, 290.33, 161.21, 31.45]}, {"formula_id": "formula_10", "formula_text": "\u00b5 s (D) = max", "formula_coordinates": [4.0, 205.2, 408.21, 67.29, 10.65]}, {"formula_id": "formula_11", "formula_text": "D = {D \u2208 R d\u00d7K : d j 2 2 \u2264 1, D \u22a4 D \u2212 I 2", "formula_coordinates": [4.0, 200.52, 499.61, 182.17, 18.99]}, {"formula_id": "formula_12", "formula_text": "(t+1) = B (t+1)B \u22a4 (t+1) + 2\u03baD \u22a4 tDt + 2\u03b7diag(D \u22a4 tDt ) XB \u22a4 (t+1) + 2(\u03ba + \u03b7)D t .", "formula_coordinates": [4.0, 198.12, 561.93, 218.07, 36.33]}, {"formula_id": "formula_13", "formula_text": "\u03b1 (k) i = n j=1 w(x j , x i ) d k 2 d T k x \u0135 \u03b2 (k) j = \u03b1 (k) j j \u2208 S 0 j / \u2208 S , S = {1, . . . , s; s \u2264 d : s k=1 |\u03b1 (k) i | \u2264 \u03bb}.", "formula_coordinates": [5.0, 231.24, 153.05, 202.86, 99.85]}, {"formula_id": "formula_14", "formula_text": "D t = arg min D\u2208D n i=1 x i \u2212 D\u03b2 i (t) 2 2 ,", "formula_coordinates": [5.0, 209.76, 287.33, 142.2, 31.21]}, {"formula_id": "formula_15", "formula_text": "D = {D \u2208 R d\u00d7K : d j 2 2 \u2264 1, D \u22a4 D \u2212 I 2 F \u2264 \u03b3}", "formula_coordinates": [5.0, 213.48, 321.29, 208.74, 18.99]}, {"formula_id": "formula_16", "formula_text": "p n,h (x) = 1 nh n i=1 K x \u2212 X i 2 h . Let K h1 (\u2022) = 1 h1 K 1 ( \u2022 h ).", "formula_coordinates": [5.0, 50.4, 541.85, 330.96, 59.31]}, {"formula_id": "formula_17", "formula_text": "r \u03bb (x) = min \u03b2\u2208S \u03bb x \u2032 \u2212 D\u03b2 2 K h1 (\u03c1(x, x \u2032 )) dP n (x \u2032 )", "formula_coordinates": [5.0, 201.12, 611.21, 209.78, 18.99]}, {"formula_id": "formula_18", "formula_text": "S \u03bb = {\u03b2 : \u03b2 1 \u2264 \u03bb}.", "formula_coordinates": [5.0, 260.28, 651.81, 91.44, 17.04]}, {"formula_id": "formula_19", "formula_text": "E Pn (r) = min \u03b2\u2208S \u03bb x \u2032 \u2212 D\u03b2 2 K h1 (\u03c1(x, x \u2032 )) dP n (x \u2032 ) dx", "formula_coordinates": [5.0, 188.16, 694.37, 235.61, 18.88]}, {"formula_id": "formula_20", "formula_text": "E P (r) = min \u03b2\u2208S \u03bb x \u2032 \u2212 D\u03b2 2 K h1 (\u03c1(x, x \u2032 )) dP(x \u2032 ) dx.", "formula_coordinates": [6.0, 192.0, 83.93, 228.0, 18.87]}, {"formula_id": "formula_21", "formula_text": "F \u03bb = {r \u03bb : S d\u22121 \u2192 R : D \u2208 R d\u00d7K , d i 2 \u2264 1, \u00b5 s (D) \u2264 \u03b3}.", "formula_coordinates": [6.0, 181.32, 178.73, 249.48, 19.0]}, {"formula_id": "formula_22", "formula_text": "4\u03bb|K h 1 (\u2022)|1 \u01eb(1\u2212\u03b3) dK", "formula_coordinates": [6.0, 476.52, 219.17, 58.53, 19.33]}, {"formula_id": "formula_23", "formula_text": "|K h1 (\u2022)| 1 = |K h1 (x)| dP. Proof. Let F \u2032 \u03bb = {r \u2032 \u03bb : S d\u22121 \u2192 R : D \u2208 d \u00d7 K, d i 2 \u2264 1}, where r \u2032 \u03bb (x) = min \u03b2\u2208S \u03bb D\u03b2 \u2212 x .", "formula_coordinates": [6.0, 50.4, 241.41, 484.01, 36.24]}, {"formula_id": "formula_24", "formula_text": "|K h1 * (s 1 \u2212 s 2 )| p \u2264 |K h1 | 1 |s 1 \u2212 s 2 | p , 1 \u2264 p \u2264 \u221e", "formula_coordinates": [6.0, 198.12, 293.73, 215.76, 17.04]}, {"formula_id": "formula_25", "formula_text": "E P (r \u03bb ) \u2264 E Pn (r \u03bb ) + dK ln 4 \u221a n\u03bb|K h 1 (\u2022)|1 (1\u2212\u03b3) 2n + t 2n + 4 n", "formula_coordinates": [6.0, 175.44, 417.77, 259.9, 35.8]}, {"formula_id": "formula_26", "formula_text": "concerning | \u2022 | \u221e covering numbers. Lemma 1. Let Q be a function class of [0, B] functions with covering number ( C \u01eb ) d > e B 2 under | \u2022 | \u221e norm.", "formula_coordinates": [6.0, 50.4, 475.05, 483.17, 36.36]}, {"formula_id": "formula_27", "formula_text": "E f \u2264 E n f + B d ln(C \u221a n) 2n + t 2n + 4 n .", "formula_coordinates": [6.0, 206.28, 521.97, 199.44, 30.96]}, {"formula_id": "formula_28", "formula_text": "E P (r \u03bb ) \u2264 1.1E Pn (r \u03bb ) + 9 dK ln 4n\u03bb|K h 1 (\u2022)|1 (1\u2212\u03b3) + t n .", "formula_coordinates": [6.0, 198.84, 643.13, 214.32, 30.87]}], "doi": ""}