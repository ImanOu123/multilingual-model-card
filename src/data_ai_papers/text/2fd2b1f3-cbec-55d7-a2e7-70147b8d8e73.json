{"title": "Weaker Than You Think: A Critical Look at Weakly Supervised Learning", "authors": "Dawei Zhu; Xiaoyu Shen; Marius Mosbach; Andreas Stephan; Dietrich Klakow; Alexa Ai", "pub_date": "", "abstract": "Weakly supervised learning is a popular approach for training machine learning models in low-resource settings. Instead of requesting high-quality yet costly human annotations, it allows training models with noisy annotations obtained from various weak sources. Recently, many sophisticated approaches have been proposed for robust training under label noise, reporting impressive results. In this paper, we revisit the setup of these approaches and find that the benefits brought by these approaches are significantly overestimated. Specifically, we find that the success of existing weakly supervised learning approaches heavily relies on the availability of clean validation samples which, as we show, can be leveraged much more efficiently by simply training on them. After using these clean labels in training, the advantages of using these sophisticated approaches are mostly wiped out. This remains true even when reducing the size of the available clean data to just five samples per class, making these approaches impractical. To understand the true value of weakly supervised learning, we thoroughly analyze diverse NLP datasets and tasks to ascertain when and why weakly supervised approaches work. Based on our findings, we provide recommendations for future research. 1 * Work done outside Amazon.", "sections": [{"heading": "Introduction", "text": "Weakly supervised learning (WSL) is one of the most popular approaches for alleviating the annotation bottleneck in machine learning. Instead of collecting expensive clean annotations, it leverages weak labels from various weak labeling sources such as heuristic rules, knowledge bases or lowerquality crowdsourcing (Ratner et al., 2017). These weak labels are inexpensive to obtain, but are often noisy and inherit biases from their sources. Deep learning models trained on such noisy data without regularization can easily overfit to the noisy labels (Zhang et al., 2017;T\u00e4nzer et al., 2022). Many advanced WSL techniques have recently been proposed to combat the noise in weak labels, and significant progress has been reported. On certain datasets, they even manage to match the performance of fully-supervised models (Liang et al., 2020;Ren et al., 2020;Yu et al., 2021).\nIn this paper, we take a close look at the claimed advances of these WSL approaches and find that the benefits of using them are significantly overestimated. Although they appear to require only weak labels during training, a substantial number of clean validation samples are used for various purposes such as early-stopping (Liang et al., 2020;Yu et al., 2021) and meta-learning (Ren et al., 2018;Shu et al., 2019;. We cast doubt on this practice: in real-world applications, these clean validation samples could have instead been used for training. To address our concern, we explore fine-tuning models directly on the validation splits of eight datasets provided by the WRENCH benchmark (Zhang et al., 2021b) and compare it to recent WSL algorithms. The results are shown in Figure 1. Interestingly, although all WSL models generalize better than the weak labels, simply fine-tuning on the validation splits outperforms all WSL methods in almost all cases, sometimes even by a large margin. This suggests that existing WSL approaches are not evaluated in a realistic setting and the claimed advances of these approaches may be overoptimistic. In order to determine the true benefits of WSL approaches in a realistic setting, we conduct extensive experiments to investigate the role of clean validation data in WSL. Our findings can be summarized as follows:\n\u2022 Without access to any clean validation samples, all WSL approaches analyzed in this work fail to work, performing similarly to or worse than the weak labels ( \u00a74).\n\u2022 Although increasing the amount of clean validation samples improves WSL performance ( \u00a75), these validation samples can be more efficiently leveraged by directly training on them, which can outperform WSL approaches when there are more than 10 samples per class for most datasets ( \u00a76).\n\u2022 Even when enabling WSL models to continue training on clean validation samples, they can barely beat an embarrassingly simple baseline which directly fine-tunes on weak labels followed by fine-tuning on clean samples. This stays true with as few as 5 samples per class ( \u00a77).\n\u2022 The knowledge encoded in pre-trained language models biases them to seek linguistic correlations rather than shallow rules from the weak labels; further fine-tuning the pretrained language models with contradicting examples helps reduce biases from weak labels ( \u00a78).\nAltogether, we show that existing WSL approaches significantly overestimate their benefits in a realistic setting. We suggest future work to (1) fully leverage the available clean samples instead of only using them for validation and (2) consider the simple baselines discussed in this work when comparing WSL approaches to better understand WSL's true benefits.", "publication_ref": ["b32", "b48", "b39", "b17", "b34", "b45", "b17", "b45", "b33", "b36", "b50"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Related work", "text": "Weak supervision.\nWeak supervision is proposed to ease the annotation bottleneck in training machine learning models. It uses weak sources to automatically annotate the data, making it possible to obtain a large amount of annotated data at a low cost. A comprehensive survey is done in . Ratner et al. (2017) propose to label data programmatically using heuristics such as keywords, regular expressions or knowledge bases. One drawback of weak supervision is that its annotations are noisy, i.e., some annotations are incorrect. Training models on such noisy data may result in poor generalization (Zhang et al., 2017;T\u00e4nzer et al., 2022;. One option to counter the impact of wrongly labeled samples is to re-weight the impact of examples in loss computation (Ren et al., 2018;Shu et al., 2019;. Another line of research leverages the knowledge encoded in large language models (Ren et al., 2020;Stephan et al., 2022). Methods such as BOND (Liang et al., 2020), ASTRA (Karamanolakis et al., 2021) and COSINE (Yu et al., 2021) apply teacher-student frameworks to train noise-robust models. Zhu et al. (2023) show that teacher-student frameworks may still be fragile in challenging situations and propose incorporating meta-learning techniques in such cases. Multiple benchmarks are available to evaluate weak supervision systems, e.g., WRENCH (Zhang et al., 2021b), Skweak (Lison et al., 2021), and WALNUT (Zheng et al., 2022a). In this paper, we take representative datasets from WRENCH and reevaluate existing WSL approaches in more realistic settings.\nRealistic evaluation. Certain pitfalls have been identified when evaluating machine learning models developed for low-resource situations. Earlier work in semi-supervised learning (SSL) in computer vision, for example, often trains with a few hundred training examples while retaining thousands of validation samples for model selection (Tarvainen and Valpola, 2017;Miyato et al., 2018). Oliver et al. (2018) criticize this setting and provide specific guidance for realistic SSL evaluation. Recent work in SSL has been adapted to discard the validation set and use a fixed set of hyperpa-rameters across datasets (Xie et al., 2020;Zhang et al., 2021a;Li et al., 2021). In NLP, it has been shown that certain (prompt-based) few-shot learning approaches are sensitive to prompt selection which requires separate validation samples (Perez et al., 2021). This defeats the purported goal of few-shot learning, which is to achieve high performance even when collecting additional data is prohibitive. Recent few-shot learning algorithms and benchmarks have adapted to a more realistic setting in which fine-grained model selection is either skipped (Gao et al., 2021;Alex et al., 2021;Bragg et al., 2021;Schick and Sch\u00fctze, 2022; or the number of validation samples are strictly controlled (Bragg et al., 2021;Zheng et al., 2022b). To our knowledge, no similar work exists exploring the aforementioned problems in the context of weak supervision. This motivates our work.", "publication_ref": ["b32", "b48", "b39", "b33", "b36", "b34", "b38", "b17", "b12", "b45", "b58", "b50", "b54", "b40", "b25", "b27", "b44", "b47", "b15", "b28", "b6", "b0", "b4", "b35", "b4", "b55"], "figure_ref": [], "table_ref": []}, {"heading": "Overall setup", "text": "Problem formulation. Formally, let X and Y be the feature and label space, respectively. In standard supervised learning, we have access to a training set D = {(x i , y i )} N i=1 sampled from a clean data distribution D c of random variables (X, Y ) \u2208 X \u00d7 Y. In weak supervision, we are instead given a weakly labeled dataset D w = {(x i ,\u0177 i )} N i=1 sampled from a noisy distribution D n , where\u0177 i represents labels obtained from weak labeling sources such as heuristic rules or crowd-sourcing. 2\u0177 i is noisy, i.e., it may be different from the ground-truth label y i . The goal of WSL algorithms is to obtain a model that generalizes well on D test \u223c D c despite being trained on D w \u223c D n . In recent WSL work, a set of clean samples, D v \u223c D c , is also often included for model selection. 3 Datasets. We experiment with eight datasets covering different NLP tasks in English. Concretely, we include four text classification datasets: (1) AGNews (Zhang et al., 2015), (2) IMDb (Maas et al., 2011), (3) Yelp (Zhang et al., 2015), (4) TREC (Li and Roth, 2002), two relation classification datasets: (5) SemEval (Hendrickx et al., 2010) and (6) ChemProt (Krallinger et al., 2017), and two Named-Entity Recognition (NER) datasets: (7) CoNLL-03 (Tjong Kim Sang and De Meulder, 2003) and ( 8) OntoNotes (Pradhan et al., 2013). The weak annotations are obtained from the WRENCH (Zhang et al., 2021b) (Howard and Ruder, 2018;Devlin et al., 2019). Ren et al. (2020), Zhang et al. (2021b) and Zheng et al. (2022a) show that a pre-trained language model (PLM) fine-tuned on a weakly labeled dataset often generalizes better than the weak labels synthesized by weak labeling sources.\n(2) L2R (Ren et al., 2018) uses metalearning to determine the optimal weights for each (noisy) training sample so that the model performs best on the (clean) validation set. Although this method was originally proposed to tackle artificial label noise, we find it performs on par with or better than recent weak supervision algorithms on a range of datasets. (3) MLC  uses meta-learning as well, but instead of weighting the noisy labels, it uses the meta-model to correct them. The classifier is then trained on the corrected labels. (4) BOND (Liang et al., 2020) is a noise-aware self-training framework designed for learning with weak annotations. (5) COSINE (Yu et al., 2021) underpins self-training with contrastive regularization to improve noise robustness further and achieves state-of-the-art performance on the WRENCH (Zhang et al., 2021b) benchmark.\nTo provide a fair comparison, we use RoBERTabase (Liu et al., 2019) as the common backbone PLM for all WSL approaches (re)implemented in this paper. 4 Is clean data necessary for WSL?\nRecent best-performing WSL approaches rely on a clean validation set for model selection. Figure 1 reveals that they fail to outperform a simple model that is directly fine-tuned on the validation set. Therefore, a natural question to ask is: \"Will WSL still work without accessing the clean validation set?\". If the answer is yes, then we can truly reduce the burden of data annotation and the benefits of these WSL approaches would be undisputed. This section aims to answer this question.\nSetup. We compare three different validation choices for model selection using either (1) a clean validation set from D v as in prior work, (2) weak labels fromD v obtained by annotating the validation set via weak labeling sources (the same procedure used to construct training annotations), or (3) no validation data at all. In the last setting, we randomly select 5 sets of hyperparameters from our search space (see Appendix C). We run the WSL approaches introduced in Section 3 on all eight datasets with different validation choices and measure their test performance. Each experiment is repeated 5 times with different seeds.\nWhile one may expect a certain drop in performance when switching from D v toD v , the absolute performance of a model does not determine the usefulness of a WSL method. We are more interested in whether a trained model generalizes better than the weak labels. 5 In realistic applications, it is only worth deploying trained models if they demonstrate clear advantages over the weak labels. Therefore, we report the relative performance gain of WSL approaches over the weak labels. Formally, let P W L , P \u03b1 denote the performance (accuracy, F1-score, etc.) achieved by the weak labels and a certain WSL method \u03b1, respectively. The the relative performance gain is defined as G \u03b1 = (P \u03b1 \u2212 P W L )/P W L . We consider a WSL approach to be effective and practically useful only if G \u03b1 > 0.\nResults. Figure 2 shows the relative performance gain for all considered WSL approaches. When model selection is performed on a clean validation set (green curve), all weak supervision baselines generalize better than the weak labels. Sophisticated methods like COSINE and L2R push the performance even further. This observation is consistent with previous findings (Zhang et al., 2021b;Zheng et al., 2022a). However, when using a weakly labeled validation set (yellow curve), all WSL baselines become ineffective and barely outperform the weak labels. More interestingly, models selected through the weakly labeled validation sets do not outperform models configured with random hyperparameters (purple curve). These results demonstrate that model selection on clean validation samples plays a vital role in the effectiveness of WSL methods. Without clean validation samples, existing WSL approaches do not work.", "publication_ref": ["b51", "b24", "b51", "b16", "b8", "b13", "b41", "b31", "b50", "b10", "b5", "b34", "b50", "b54", "b33", "b17", "b45", "b50", "b20", "b50", "b54"], "figure_ref": [], "table_ref": []}, {"heading": "How much clean data does WSL need?", "text": "Now that we know clean samples are necessary for WSL approaches to work, a follow-up question would be: \"How many clean samples do we need?\" Intuitively, we expect an improvement in performance as we increase the amount of clean data, but it is unclear how quickly this improvement starts to level off, i.e., we may find that a few dozen clean samples are enough for WSL approaches to perform model selection. The following section seeks to answer this question.\n$OO 9DOLGDWLRQ $FFXUDF\\ )7 Z &26,1( /5 %21' 0/& :HDNODEHOV (a) AGNews $OO 9DOLGDWLRQ $FFXUDF\\ (b) Yelp $OO 9DOLGDWLRQ $FFXUDF\\ (c) IMDb $OO 9DOLGDWLRQ $FFXUDF\\ (d) TREC $OO 9DOLGDWLRQ $FFXUDF\\ (e) SemEval $OO 9DOLGDWLRQ $FFXUDF\\ (f) ChemProt $OO 9DOLGDWLRQ )VFRUH (g) CoNLL-03 $OO 9DOLGDWLRQ )VFRUH (h) OntoNotes 5.0\nFigure 3: The impact of the number of clean validation samples on performance. We plot average performance and standard deviation over 5 runs varying the size of the clean validation data. Whenever a small proportion of validation data is provided, most WSL techniques generalize better than the weak label baseline (grey dashed line). Performance improves with additional validation samples, but this tendency usually levels out with a moderate number of validation samples.\nSetup. We apply individual WSL approaches and vary the size of clean data sub-sampled from the original validation split. For text and relation classification tasks, we draw an increasing number of clean samples N \u2208 {5, 10, 15, 20, 30, 40, 50} per class when applicable. 6 In the case of NER, as a sentence may contain multiple labels from different classes, selecting exactly N samples per class at random is impractical. Hence, for NER we sample N \u2208 {50, 100, 200, 300, 400, 500} sentences for validation. For each N , we run the same experiment 5 times. Note that the clean data is used solely for model selection in this set of experiments.\nResults. As shown in Figure 3, in most cases, a handful of validation samples already make WSL work better than the weak labels. We observe an increasing trend in performance with more validation samples, but typically this trend weakens with a moderate size of samples (~30 samples per class or~200 sentences) and adding more samples provides little benefit. There are a few exceptions. For example, on IMDb all methods except L2R consistently perform better with more validation data. On CoNLL-03, on the other hand, most methods seem to be less sensitive to the number of samples. Overall, the results suggest that a small amount 6 The validation set of SemEval is too small to support N > 20. Also, if a dataset is unbalanced, we randomly select N \u00d7 C samples, where C denotes the number of classes. This is a realistic sampling procedure when performing data annotation.\nof clean validation samples may be sufficient for current WSL methods to achieve good performance. Using thousands of validation samples, like in the established benchmarks (Zhang et al., 2021b;Zheng et al., 2022a), is neither realistic nor necessary.\n6 Is WSL useful with less clean data?\nThe previous sections have shown that current WSL approaches (1) do not improve over direct finetuning on the existing validation splits (Figure 1) and (2) require only a small amount of validation samples to be effective (Figure 3). This section investigates whether the conclusion from Figure 1 would change with less clean data, i.e., can WSL approaches outperform direct fine-tuning when less clean data is available?\nSetup. We follow the same procedure as in Section 5 to subsample the cleanly annotated validation sets and fine-tune models directly on the sampled data. In addition to the standard fine-tuning approach (Devlin et al., 2019), we also experiment with three parameter-efficient fine-tuning (PEFT) approaches as -in the few-shot setting -they have been shown to achieve comparable or even better performance than fine-tuning all parameters (Peters et al., 2019;Logan IV et al., 2022;. In particular, we include adapters (Houlsby et al., 2019), LoRA BitFit (Zaken et al., 2022  We show the average performance (Acc. and F1-score in %) difference between (parameter-efficient) fine-tuning approaches and COSINE when varying amounts of clean samples. COSINE uses the clean samples for validation, whereas fine-tuning approaches directly train on them (indicated in the legend with the subscript 'C'). For most sequence classification tasks, fine-tuning approaches work better once 10 clean samples are available for training. For NER, several hundreds of clean sentences may be required to attain better results via fine-tuning. Refer to Appendix D for a comparison with other WSL approaches.\nWe use one fixed set of hyperparameter configurations and train models for 6000 steps on each dataset. 7 We report performance at the last step and compare it with WSL approaches which use the same amount of clean data for validation.\nResults. Figure 4 shows the performance difference between the fine-tuning baselines and CO-SINE, one of the best-performing WSL approaches, when varying the number of clean samples. It can be seen that in extremely low-resource cases (less than 5 clean samples per class), COSINE outperforms fine-tuning. However, fine-tuning approaches quickly take over when more clean samples are available. LoRA performs better than CO-SINE on three out of four text classification tasks with just 10 samples per class. AGNews is the only exception, where COSINE outperforms LoRA by about 1% when 20 samples per class are available, but adapters outperform COSINE in this case. Relation extraction has the same trend where 10-20 samples per class are often enough for fine-tuning approaches to catch up. For NER tasks, all finetuning approaches outperform COSINE with as 7 The hyperparameters are randomly picked from the ranges mentioned in the original papers of corresponding methods and fixed across all experiments. We did not cherrypick them based on the test performances. In most cases the training loss converges within 300 steps. We intentionally extend training to show that we do not rely on extra data for early-stopping. We find that overfitting to the clean data does not hurt generalization. A similar observation is made in Mosbach et al. (2021). Detailed configurations are presented in Appendix D. few as 50 sentences on CoNLL-03. OntoNotes seems to be more challenging for fine-tuning and 400 sentences are required to overtake COSINE. Still, 400 sentences only account for 0.3% of the weakly labeled samples used for training COSINE. This indicates that models can benefit much more from training on a small set of clean data rather than on vast amounts of weakly labeled data. Note that the fine-tuning approaches we experiment with work out-of-the-box across NLP tasks. If one specific task is targeted, few-shot learning methods with manually designed prompts might perform even better. 8 Hence, the performance shown here should be understood as a lower bound of what one can achieve by fine-tuning. Nevertheless, we can see that even considering the lower bound of fine-tuning-based methods, the advantage of using WSL approaches vanishes when we have as few as 10 clean samples per class. For many realworld applications, this annotation workload may be acceptable, limiting the applicability of WSL approaches.\n7 Can WSL benefit from fine-tuning?\nThe WSL approaches have only used clean samples for validation so far, which is shown to be inefficient compared to training directly on them. (b) N = 50 clean samples per class for classification tasks except for SemEval due to its limited validation size. N = 500 clean samples for NER tasks Figure 5: Performance before and after continuous fine-tuning (CFT) on the clean data. The average performance and standard deviation over 5 runs are reported. Though CFT improves the performance of WSL approaches in general, the simplest baseline FT W gains the most from it. After applying CFT, FT W performs on par with or better than more sophisticated WSL approaches, suggesting these sophisticated approaches might have overestimated their actual value. Further plots are included in Appendix F.\nWe question whether enabling WSL methods to further fine-tune on these clean samples would improve their performance. In this section, we study a straightforward training approach that makes use of both clean and weak labels. 9\nSetup. Given both the weakly labeled training data and a small amount of clean data, we consider a simple two-phase training baseline. In the first phase, we apply WSL approaches on the weakly labeled training set, using the clean data for validation. In the second phase, we take the model trained on the weakly labeled data as a starting point and continue to train it on the clean data. We call this approach continuous fine-tuning (CFT). In our experiment, we apply CFT to the two bestperforming WSL approaches, COSINE and L2R, along with the most basic WSL baseline, FT W . We sample clean data in the same way as in Section 5. The training steps of the second phase are fixed at 6000. Each experiment is repeated 5 times with different seeds.", "publication_ref": ["b50", "b54", "b5", "b29", "b21", "b9", "b26"], "figure_ref": ["fig_0", "fig_0", "fig_1"], "table_ref": []}, {"heading": "Results.", "text": "Figure 5 shows the model performance before and after applying CFT. It can be seen that CFT does indeed benefit WSL approaches in most cases even with very little clean data (Figure 5a). For L2R, however, the improvement is less obvious, and there is even a decrease on Yelp and OntoNotes. This could be because L2R uses the validation loss to reweight training samples, meaning that the value of the validation samples beyond that may only be minimal. When more clean samples are provided, CFT exhibits a greater performance gain (Figure 5b). It is also noticeable that CFT reduces the performance gap among all three WSL methods substantially. Even the simplest approach, FT W , is comparable to or beats L2R and COSINE in all tasks after applying CFT. Considering that COSINE and L2R consume far more computing resources, our findings suggest that the net benefit of using sophisticated WSL approaches may be significantly overestimated and impractical for real-world use cases.\nFinally, we find the advantage of performing WSL diminishes with the increase of clean samples even after considering the boost from CFT. When 50 clean samples per class (500 sentences for NER) are available, applying WSL+CFT only results in a performance boost of less than 1% on 6 out of 8 datasets, compared with the baseline which only fine-tunes on clean samples. Note that weak la-7UDLQLQJVWHSV  bels are no free lunch. Managing weak annotation resources necessitates experts who not only have linguistic expertise for annotation but also the ability to transform that knowledge into programs to automate annotations. This additional requirement naturally reduces the pool of eligible candidates and raises the cost. In this situation, annotating a certain amount of clean samples may be significantly faster and cheaper. Thus, we believe WSL has a long way to go before being truly helpful in realistic low-resource scenarios.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "What makes FT W +CFT effective?", "text": "As seen in the previous section, combining FT W with CFT yields a strong baseline that more sophisticated WSL approaches can hardly surpass. This section examines factors that contribute to the effectiveness of this method. Specifically, we aim to answer two questions: (1) \"How does FT W resist biases despite being trained only on weak labels?\" and (2) \"How does CFT further reduce bias introduced by weak labels?\".\nSetup. To answer question (1), we modify the backbone PLM to see if its encoded knowledge plays an important role. We explore two additional PLMs that are pre-trained on less data: RoBERTasmall-1M and RoBERTa-base-10M, which are pre- trained on 1M and 10M words, respectively. 10 We report model performance on both clean labels and weak labels to see which labels the model tends to fit. To answer question (2), we vary the agreement ratio in the clean samples to see how these clean labels help combat biases from weak labels. The agreement ratio is defined as the percentage of samples whose clean labels match the corresponding weak labels. Intuitively, if the clean label for a specific training example matches its weak label, then this example may not contribute additional information to help combat bias. A higher agreement ratio should therefore indicate fewer informative samples.\nResults. Figure 6 shows the performances for different PLMs. Pre-training on more data clearly helps to overcome biases from weak labels. When the pre-training corpus is small, the model tends to fit the noisy weak labels more quickly than the clean labels and struggles to outperform weak labels throughout the entire training process. With a large pre-training corpus, however, the model can make better predictions on clean labels than weak labels in the early stages of training, even when it is only trained on weak labels. If we apply proper early-stopping before the model is eventually dragged toward weak labels, we can attain a model that generalizes significantly better than the weak labels. This indicates that pre-training provides the model with an inductive bias to seek more general linguistic correlations instead of superficial correlations from the weak labels, which aligns with previous findings in Warstadt et al. (2020). This turns out to be the key to why simple FT W works here. Figure 7 shows how the agreement ratio \u03b1 in clean samples affects the performance. Performance declines substantially for \u03b1 > 70%, showing that it is necessary to have contradictory samples in order to reap the full advantage of CFT. This is reasonable, given that having examples with clean labels that coincide with their weak labels may reinforce the unintended bias learned from the weakly labeled training set. The optimal agreement ratio lies around 50%. However, having \u03b1 = 0 also yields decent performance for most datasets except TREC, suggesting contradictory samples play a more important role here and at least a minimum set of contradictory samples are required for CFT to be beneficial.", "publication_ref": ["b42"], "figure_ref": ["fig_2", "fig_3"], "table_ref": []}, {"heading": "Conclusions and recommendations", "text": "Our extensive experiments provide strong evidence that recent WSL approaches heavily overestimate their performance and practicality. We demonstrated that they hinge on clean samples for model selection to reach the claimed performance, yet models that are simply trained on these clean samples are already better. When both clean and weak labels are available, a simple baseline (FT W +CFT) performs on par with or better than more sophisticated methods while requiring much less computation and effort for model selection.\nInspired by prior work (Oliver et al., 2018;Perez et al., 2021), our recommendations for future WSL approaches are the following:\n\u2022 Report the model selection criteria for proposed methods and, especially, how much they rely on the presence of clean data.\n\u2022 Report how many cleanly annotated samples are required for a few-shot learning approach to reach the performance of a proposed WSL approach. If thousands of weakly annotated samples are comparable to a handful of clean samples -as we have seen in Section 6 -then WSL may not be the best choice for the given low-resource setting.\n\u2022 If a proposed WSL method requires extra clean data, such as for validation, then the simple FT W +CFT baseline should be included in evaluation to claim the real benefits gained by applying the method.\nWe hope our findings and recommendations will spur more robust future work in WSL such that new methods are truly beneficial in realistic lowresource scenarios.", "publication_ref": ["b27", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "We facilitate fair comparisons and realistic evaluations of recent WSL approaches. However, our study is not exhaustive and has the following limitations.\nFirst, it may be possible to perform model selection by utilizing prior knowledge about the dataset. For example, if the noise ratio (the proportion of incorrect labels in the training set) is known in advance, it can be used to determine (a subset of) hyperparameters (Han et al., 2018;. In this case, certain WSL approaches may still work without access to extra clean data.\nSecond, in this paper we concentrate on tasks in English where strong PLMs are available. As we have shown in Section 6, training them on a small amount of data is sufficient for generalization. For low-resource languages where no PLMs are available, training may not be that effective, and WSL methods may achieve higher performance.\nThird, we experiment with datasets from the established WRENCH benchmark, where the weak labels are frequently assigned by simple rules like as regular expressions (see Appendix B for examples). However, in a broader context, weak supervision can have different forms. For example, Smith et al. (2022) generates weak labels through large language models.  use hyper-link information as weak labels for passage retrieval. We have not extended our research to more diverse types of weak labels.\nDespite the above limitations, however, we identify the pitfalls in the existing evaluation of current WSL methods and demonstrate simple yet strong baselines through comprehensive experiments on a wide range of tasks.", "publication_ref": ["b7", "b37"], "figure_ref": [], "table_ref": []}, {"heading": "A Datasets", "text": "In the following, we give a more comprehensive description of the datasets used. A subset of the commonly used WRENCH (Zhang et al., 2021b) benchmark is used, covering various aspects such as task type, coverage and dataset size. There is a total of four classification, two relation extraction and two sequence labeling datasets. See Table 2 for a detailed set of data statistics.\nAGNews (Zhang et al., 2015) is a topic classification dataset. The task is to classify news articles into four topics, namely world, sports, business and Sci-Fi/technology. Each labeling function is composed of multiple keywords to search for. The number of keywords differs from a few up to dozens.\nIMDb (Maas et al., 2011) is a dataset of movie reviews sampled from the IMDb website. The task is binary sentiment analysis. The labeling functions are composed of keyword searches and regular expressions.\nYelp (Zhang et al., 2015) is another sentiment analysis dataset, containing crowd-sourced business reviews. The labeling functions are created using keywords and a lexicon-based sentiment analysis library.\nTREC (Li and Roth, 2002) is a question classification dataset, i.e., it asks what type of response is expected. The labels are abbreviation, description and abstract concepts, entities, human beings, locations or numeric values. The labeling functions are created using regular expressions and make a lot of use of question words such as \"what\", \"where\" or \"who\".\nSemEval (Hendrickx et al., 2010) is a relation classification dataset, using nine relation types. Examples for relation labels are cause-effect, entityorigin or message-topic. Labeling functions are created using entities within a regular expression.\nChemProt (Krallinger et al., 2017) is another relation classification dataset, focusing on chemical research literature. It contains ten different types of relations, for example chemical-protein relations such as \"biological properties upregulator\". The labeling functions are created using rules.\nCoNLL-03 (Tjong Kim Sang and De Meulder, 2003) is a named entity recognition (NER) dataset, with labels for the entities \"person\", \"location\", \"organization\", and \"miscellaneous\". Labeling functions are built using previously trained keywords, regular expressions and NER models.\nOntoNotes 5.0 (Pradhan et al., 2013) is a another NER dataset, using more fine-grained entities as CoNLL-03. Here, a subset of the CoNLL weak labeling sources is combined with keyword and regular expression based weak labeling sources.", "publication_ref": ["b50", "b51", "b24", "b51", "b16", "b8", "b13", "b41"], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "B Labeling functions", "text": "Weak labeling sources are often abstracted as labeling functions and vary in aspects such as coverage, precision, or overlap (Ratner et al., 2017;Karamanolakis et al., 2021). To showcase how the weak labeling process works, a selection of examples of labeling functions is presented. More specifically, we provide examples of rules for the two classification datasets IMDb (Table 3) and TREC (Table 4), the relation classification dataset SemEval (Table 5) and the NER dataset CoNLL-03 (Table 6).", "publication_ref": ["b32", "b12"], "figure_ref": [], "table_ref": ["tab_8"]}, {"heading": "C Overall implementation details", "text": "This section summarizes the overall implementation details of WSL approaches used in our paper. Refer to Appendix D for hyperparameter configurations of PEFT approaches. We use the Py-Torch framework 11 to implement all approaches discussed in the paper. Hugging Face (Wolf et al., 2020) is used for downloading and training the RoBERTa-base model. AdapterHub (Pfeiffer et al., 2020) is used for implementing parameter-efficient fine-tuning.\nHyperparameters In this paper, we implemented five WSL methods: FT (Devlin et al., 2019), L2R (Ren et al., 2018), MLC , BOND (Liang et al., 2020), and COSINE (Yu et al., 2021). We report the search ranges of the hyperparameters in Table 7.\nWe do not search for batch size as we find it has minor effects on the final performance. Instead, a batch size of 32 is used across experiments. Also, RoBERTa-base (Liu et al., 2019) is used as the backbone PLM and AdamW (Loshchilov and Hutter, 2019) is the optimizer used across all methods.   ", "publication_ref": ["b43", "b30", "b5", "b33", "b17", "b45", "b20", "b22"], "figure_ref": [], "table_ref": ["tab_10"]}, {"heading": "D Training with clean samples D.1 Methods and implementation details", "text": "In Section 6, we apply four (parameter-efficient) fine-tuning approaches to train models on clean validation sets. Since we do not have extra data for model selection, we choose a fixed set of hyperparameters for all datasets. In the following we briefly introduce the fine-tuning approaches, together with their hyperparameter configurations.\n\u2022 Vanilla fine-tuning (Devlin et al., 2019;Liu et al., 2019) is the standard fine-tuning approaches for pre-trained language models. It works by adding a randomly initialized classifier on top of the pre-trained model and training it together with all other model parameters.\nWe use a fixed learning rate of 2e \u22125 in all experiments.\n\u2022 Adapter-based fine-tuning (Houlsby et al., 2019) adds additional feed-forward layers called adapters to each layer of the pre-trained language model. During fine-tuning, we only update the weights of these adapter layers and keep all other parameters frozen at their pretrained values. We use a fixed learning rate of 2e \u22125 in all experiments. The reduction factor is set to 16.\n\u2022 BitFit (Zaken et al., 2022) updates only the bias parameters of every layer and keeps all other weights frozen. Despite its simplicity it has been demonstrated to achieve similar results to adapter-based fine-tuning. We use a fixed learning rate of 1e \u22124 in all experiments.\n\u2022 LoRA  is a recently proposed adapter-based fine-tuning method which uses a low-rank bottleneck architecture in each of the newly added feed-forward networks. The motivation here is to perform a low rank update to the model during fine-tuning. We use a fixed learning rate of 2e \u22125 in all experiments. The \u03b1 value used in LoRa is fixed to 16.\nIn all experiments, the batch size used in all finetuning approaches is 32. The optimizer is AdamW (Loshchilov and Hutter, 2019).", "publication_ref": ["b5", "b20", "b9", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "D.2 Training on the full validation sets", "text": "In addition to training sets, the WRENCH (Zhang et al., 2021b)    both the training and validation sets with a model that is directly fine-tuned on the validation set. The following WSL methods are included in this experiment: L2R (Ren et al., 2018), MetaWN (Shu et al., 2019), BOND (Liang et al., 2020), Denoise (Ren et al., 2020), MLC , and COSINE (Yu et al., 2021). Following prior work, we select the best set of hyperparameters via the validation set when applying the WSL methods. Also, early-stopping based on the validation performance is applied. In contrast, the direct fine-tuning baseline uses a fixed set of hyperparameters across all datasets, and no early-stopping is applied (same configuration as in Appendix D.1). We train this baseline for 6000 steps. In all cases, the training losses converged much earlier than 6000 steps, but we deliberately kept training for longer to show that the good performance achieved by this baseline is not due to any fine-grained configurations.\nAs shown in Figure 1, this simple baseline outperforms all the WSL methods in all but one case.", "publication_ref": ["b50", "b33", "b36", "b17", "b34", "b45"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "D.3 Extended comparison of training on clean data and validation for WSL approaches", "text": "In Section 6, standard fine-tuning (FT) and multiple parameter-efficient fine-tuning (PEFT) are compared with the competitive WSL method CO-SINE. In this section, we provide additional plots which show the same comparison with the other WSL methods examined in this work, namely L2R, MLC, and BOND. We report average performance (Acc. and F1 in %) difference between (parameterefficient) fine-tuning methods and the specific WSL method for varying number of clean samples. The overall tendency is consistent with the results in Section 6: WSL methods perform well on a small amount of clean labeled data but PEFT outperforms WSL methods with an increasing amount of clean labeled data.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E Additional baselines that combine weak and clean data during training", "text": "Besides CFT we also explored two simple baselines that combine both the cleanly and weakly annotated data in training:\n1. WC mix : it mixes the clean data into the weakly labeled training set. We then fine-tune a PLM on this combined dataset.\n2. WC batch : in each batch, we mix the weakly and cleanly labeled data at a ratio of 50:50.\nThis makes sure that the model can access clean samples in each batch.\nWe compared these two baselines with CFT, the results are shown in Figure 9. It can be seen that when the same amount of data is accessed, CFT outperforms the two baselines in most cases, sometimes by a large margin. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G CFT with different PLMs and agreement ratios", "text": "We provide additional plots of the experiments mentioned in Section 8 on more datasets. Figure 11 shows the performance of CFT using different PLMs during training and Figure 12 shows the performance when the number of clean samples and the agreement ratio is varied.  ", "publication_ref": [], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "Acknowledgments", "text": "We thank Vagrant Gautam for thoughtful suggestions and insightful discussions. We would also like to thank our anonymous reviewers for their constructive feedback.\nThis work has been partially funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) -Project-ID 232722074 -SFB 1102 and the EU Horizon 2020 projects ROX-ANNE under grant number 833635.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "(a) N = 10 samples per class (N = 100 sentences on NER)\n2QWR1RWHV 1 FOHDQVDPSOHV &26,1( /5 )7 : &OHDQ2QO\\\n(c) N = 30 samples per class (N = 300 sentences on NER)\n2QWR1RWHV 1 FOHDQVDPSOHV &26,1( /5 )7 : &OHDQ2QO\\\n(d) N = 40 samples per class (N = 400 sentences on NER)\nFigure 10: Performance difference before and after applying CFT to WSL methods. For text classification and relation extraction tasks, we subsample N \u2208 {5, 10, 20, 30, 40, 50} examples from the validation set. For NER, we subsample N \u2208 {50, 100, 200, 300, 400, 500}. On SemEval, the original validation set is small, and sampling more than 20 samples per class is not possible. The figure shows that the performance gap between the simple baseline FT W and COSINE/L2R becomes much smaller after CFT, suggesting that we may not require sophisticated WSL methods to achieve good generalization. B2. Did you discuss the license or terms for use and / or distribution of any artifacts? Not applicable. Left blank.\nB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? Not applicable. Left blank.\nB4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Not applicable. Left blank.\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? Not applicable. Left blank. B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. Not applicable. Left blank.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C Did you run computational experiments?", "text": "Sec 4-8 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? Sec 4-8 + appendix\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "RAFT: A real-world few-shot text classification benchmark", "journal": "", "year": "2021-12", "authors": "Neel Alex; Eli Lifland; Lewis Tunstall; Abhishek Thakur; Pegah Maham; C Jess Riedel; Emmie Hine; Carolyn Ashurst; Paul Sedille"}, {"ref_id": "b1", "title": "", "journal": "", "year": "", "authors": "Devansh Arpit; Stanislaw Jastrzebski; Nicolas Ballas; David Krueger; Emmanuel Bengio; S Maxinder"}, {"ref_id": "b2", "title": "", "journal": "", "year": "", "authors": "Tegan Kanwal; Asja Maharaj;  Fischer"}, {"ref_id": "b3", "title": "A closer look at memorization in deep networks", "journal": "PMLR", "year": "2017-08-11", "authors": "Yoshua Courville; Simon Bengio;  Lacoste-Julien"}, {"ref_id": "b4", "title": "FLEX: unifying evaluation for few-shot NLP", "journal": "", "year": "2021-12-06", "authors": "Jonathan Bragg; Arman Cohan; Kyle Lo; Iz Beltagy"}, {"ref_id": "b5", "title": "BERT: pre-training of deep bidirectional transformers for language understanding", "journal": "Association for Computational Linguistics", "year": "2019-06-02", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b6", "title": "Making pre-trained language models better few-shot learners", "journal": "Long Papers", "year": "2021-08-01", "authors": "Tianyu Gao; Adam Fisch; Danqi Chen"}, {"ref_id": "b7", "title": "Co-teaching: Robust training of deep neural networks with extremely noisy labels", "journal": "", "year": "2018-12-03", "authors": "Bo Han; Quanming Yao; Xingrui Yu; Gang Niu; Miao Xu; Weihua Hu; Ivor W Tsang; Masashi Sugiyama"}, {"ref_id": "b8", "title": "SemEval-2010 task 8: Multiway classification of semantic relations between pairs of nominals", "journal": "", "year": "2010", "authors": "Iris Hendrickx; Su Nam Kim; Zornitsa Kozareva; Preslav Nakov; \u00d3 Diarmuid; Sebastian S\u00e9aghdha; Marco Pad\u00f3; Lorenza Pennacchiotti; Stan Romano;  Szpakowicz"}, {"ref_id": "b9", "title": "Parameter-efficient transfer learning for NLP", "journal": "PMLR", "year": "2019-06", "authors": "Neil Houlsby; Andrei Giurgiu; Stanislaw Jastrzebski; Bruna Morrone; Quentin De Laroussilhe; Andrea Gesmundo; Mona Attariyan; Sylvain Gelly"}, {"ref_id": "b10", "title": "Universal language model fine-tuning for text classification", "journal": "Long Papers", "year": "2018-07-15", "authors": "Jeremy Howard; Sebastian Ruder"}, {"ref_id": "b11", "title": "LoRA: Low-rank adaptation of large language models", "journal": "", "year": "2022-04-25", "authors": "Edward J Hu; Yelong Shen; Phillip Wallis; Zeyuan Allen-Zhu; Yuanzhi Li; Shean Wang; Lu Wang; Weizhu Chen"}, {"ref_id": "b12", "title": "Self-training with weak supervision", "journal": "Association for Computational Linguistics", "year": "2021-06-06", "authors": "Giannis Karamanolakis; Subhabrata Mukherjee; Guoqing Zheng; Ahmed Hassan Awadallah"}, {"ref_id": "b13", "title": "Overview of the BioCreative VI chemicalprotein interaction track", "journal": "", "year": "2017", "authors": "Martin Krallinger; Obdulia Rabal; A Saber; Mart\u0131n Akhondi; Jes\u00fas P\u00e9rez P\u00e9rez; Gael P\u00e9rez Santamar\u00eda; Georgios Rodr\u00edguez; Ander Tsatsaronis; Jos\u00e9 Intxaurrondo; Umesh Antonio L\u00f3pez;  Nandal"}, {"ref_id": "b14", "title": "DivideMix: Learning with noisy labels as semisupervised learning", "journal": "", "year": "2020-04-26", "authors": "Junnan Li; Richard Socher; Steven C H Hoi"}, {"ref_id": "b15", "title": "CoMatch: Semi-supervised learning with contrastive graph regularization", "journal": "IEEE", "year": "2021-10-10", "authors": "Junnan Li; Caiming Xiong; Steven C H Hoi"}, {"ref_id": "b16", "title": "Learning question classifiers", "journal": "", "year": "2002", "authors": "Xin Li; Dan Roth"}, {"ref_id": "b17", "title": "BOND: BERT-assisted open-domain named entity recognition with distant supervision", "journal": "ACM", "year": "2020-08-23", "authors": "Chen Liang; Yue Yu; Haoming Jiang; Siawpeng Er; Ruijia Wang; Tuo Zhao; Chao Zhang"}, {"ref_id": "b18", "title": "2021. skweak: Weak supervision made easy for NLP", "journal": "Online. Association for Computational Linguistics", "year": "", "authors": "Pierre Lison; Jeremy Barnes; Aliaksandr Hubin"}, {"ref_id": "b19", "title": "Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning", "journal": "", "year": "2022", "authors": "Haokun Liu; Derek Tam; Muqeeth Mohammed; Jay Mohta; Tenghao Huang; Mohit Bansal; Colin Raffel"}, {"ref_id": "b20", "title": "RoBERTa: A robustly optimized BERT pretraining approach", "journal": "", "year": "1907", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b21", "title": "Cutting down on prompts and parameters: Simple few-shot learning with language models", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Robert Logan; I V ; Ivana Balazevic; Eric Wallace; Fabio Petroni; Sameer Singh; Sebastian Riedel"}, {"ref_id": "b22", "title": "Decoupled weight decay regularization", "journal": "", "year": "2019-05-06", "authors": "Ilya Loshchilov; Frank Hutter"}, {"ref_id": "b23", "title": "Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity", "journal": "Association for Computational Linguistics", "year": "2022-05-22", "authors": "Yao Lu; Max Bartolo; Alastair Moore; Sebastian Riedel; Pontus Stenetorp"}, {"ref_id": "b24", "title": "Learning word vectors for sentiment analysis", "journal": "Association for Computational Linguistics", "year": "2011", "authors": "Andrew L Maas; Raymond E Daly; Peter T Pham; Dan Huang; Andrew Y Ng; Christopher Potts"}, {"ref_id": "b25", "title": "Virtual adversarial training: a regularization method for supervised and semisupervised learning", "journal": "", "year": "2018", "authors": "Takeru Miyato; Masanori Shin-Ichi Maeda; Shin Koyama;  Ishii"}, {"ref_id": "b26", "title": "On the stability of fine-tuning BERT: Misconceptions, explanations, and strong baselines", "journal": "", "year": "2021-05-03", "authors": "Marius Mosbach; Maksym Andriushchenko; Dietrich Klakow"}, {"ref_id": "b27", "title": "Realistic evaluation of deep semi-supervised learning algorithms", "journal": "", "year": "2018-12-03", "authors": "Avital Oliver; Augustus Odena; Colin Raffel; Ian J Ekin Dogus Cubuk;  Goodfellow"}, {"ref_id": "b28", "title": "True few-shot learning with language models", "journal": "", "year": "2021-12-06", "authors": "Ethan Perez; Douwe Kiela; Kyunghyun Cho"}, {"ref_id": "b29", "title": "To tune or not to tune? Adapting pretrained representations to diverse tasks", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Matthew E Peters; Sebastian Ruder; Noah A Smith"}, {"ref_id": "b30", "title": "AdapterHub: A framework for adapting transformers", "journal": "", "year": "2020", "authors": "Jonas Pfeiffer; Andreas R\u00fcckl\u00e9; Clifton Poth; Aishwarya Kamath; Ivan Vuli\u0107; Sebastian Ruder; Kyunghyun Cho; Iryna Gurevych"}, {"ref_id": "b31", "title": "Towards robust linguistic analysis using OntoNotes", "journal": "Association for Computational Linguistics", "year": "2013", "authors": "Alessandro Sameer Pradhan; Nianwen Moschitti; Hwee Tou Xue; Anders Ng; Olga Bj\u00f6rkelund; Yuchen Uryupina; Zhi Zhang;  Zhong"}, {"ref_id": "b32", "title": "Snorkel: Rapid training data creation with weak supervision", "journal": "", "year": "2017", "authors": "Alexander Ratner; H Stephen; Henry Bach; Jason Ehrenberg; Sen Fries; Christopher Wu;  R\u00e9"}, {"ref_id": "b33", "title": "Learning to reweight examples for robust deep learning", "journal": "PMLR", "year": "2018-07-10", "authors": "Mengye Ren; Wenyuan Zeng; Bin Yang; Raquel Urtasun"}, {"ref_id": "b34", "title": "Denoising multi-source weak supervision for neural text classification", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Wendi Ren; Yinghao Li; Hanting Su; David Kartchner; Cassie Mitchell; Chao Zhang"}, {"ref_id": "b35", "title": "True few-shot learning with Prompts-A real-world perspective", "journal": "Transactions of the Association for Computational Linguistics", "year": "2022", "authors": "Timo Schick; Hinrich Sch\u00fctze"}, {"ref_id": "b36", "title": "Meta-weightnet: Learning an explicit mapping for sample weighting", "journal": "", "year": "2019-12-08", "authors": "Jun Shu; Qi Xie; Lixuan Yi; Qian Zhao; Sanping Zhou; Zongben Xu; Deyu Meng"}, {"ref_id": "b37", "title": "Language models in the loop: Incorporating prompting into weak supervision", "journal": "CoRR", "year": "2022", "authors": "Ryan Smith; Jason A Fries; Braden Hancock; Stephen H Bach"}, {"ref_id": "b38", "title": "SepLL: Separating latent class labels from weak supervision noise", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Andreas Stephan; Vasiliki Kougia; Benjamin Roth"}, {"ref_id": "b39", "title": "Memorisation versus generalisation in pre-trained language models", "journal": "Long Papers", "year": "2022-05-22", "authors": "Michael T\u00e4nzer; Sebastian Ruder; Marek Rei"}, {"ref_id": "b40", "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results", "journal": "", "year": "2017-04-24", "authors": "Antti Tarvainen; Harri Valpola"}, {"ref_id": "b41", "title": "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition", "journal": "", "year": "2003", "authors": "Erik F Tjong; Kim Sang; Fien De Meulder"}, {"ref_id": "b42", "title": "Learning which features matter: RoBERTa acquires a preference for linguistic generalizations (eventually)", "journal": "", "year": "2020", "authors": "Alex Warstadt; Yian Zhang; Xiaocheng Li; Haokun Liu; Samuel R Bowman"}, {"ref_id": "b43", "title": "Transformers: State-of-the-art natural language processing", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; R\u00e9mi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger; Quentin Drame; Alexander M Lhoest;  Rush"}, {"ref_id": "b44", "title": "Unsupervised data augmentation for consistency training", "journal": "", "year": "2020-12-06", "authors": "Qizhe Xie; Zihang Dai; Eduard H Hovy; Thang Luong; Quoc Le"}, {"ref_id": "b45", "title": "Fine-tuning pretrained language model with weak supervision: A contrastive-regularized self-training approach", "journal": "", "year": "2021-06-06", "authors": "Yue Yu; Simiao Zuo; Haoming Jiang; Wendi Ren; Tuo Zhao; Chao Zhang"}, {"ref_id": "b46", "title": "BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models", "journal": "Association for Computational Linguistics", "year": "2022-05-22", "authors": "Elad Ben Zaken; Yoav Goldberg; Shauli Ravfogel"}, {"ref_id": "b47", "title": "FlexMatch: Boosting semisupervised learning with curriculum pseudo labeling", "journal": "", "year": "2021-12-06", "authors": "Bowen Zhang; Yidong Wang; Wenxin Hou; Hao Wu; Jindong Wang; Manabu Okumura; Takahiro Shinozaki"}, {"ref_id": "b48", "title": "Understanding deep learning requires rethinking generalization", "journal": "", "year": "2017-04-24", "authors": "Chiyuan Zhang; Samy Bengio; Moritz Hardt; Benjamin Recht; Oriol Vinyals"}, {"ref_id": "b49", "title": "A survey on programmatic weak supervision", "journal": "CoRR", "year": "2022", "authors": "Jieyu Zhang; Cheng-Yu Hsieh; Yue Yu; Chao Zhang; Alexander Ratner"}, {"ref_id": "b50", "title": "WRENCH: A comprehensive benchmark for weak supervision", "journal": "", "year": "2021-12", "authors": "Jieyu Zhang; Yue Yu; Yujing Nameerror; Yaming Wang; Mao Yang; Alexander Yang;  Ratner"}, {"ref_id": "b51", "title": "Character-level convolutional networks for text classification", "journal": "", "year": "2015", "authors": "Xiang Zhang; Junbo Zhao; Yann Lecun"}, {"ref_id": "b52", "title": "Calibrate before use: Improving few-shot performance of language models", "journal": "PMLR", "year": "2021-07-24", "authors": "Zihao Zhao; Eric Wallace; Shi Feng; Dan Klein; Sameer Singh"}, {"ref_id": "b53", "title": "Meta label correction for noisy label learning", "journal": "AAAI Press", "year": "2021-02-02", "authors": "Guoqing Zheng; Ahmed Hassan Awadallah; Susan T Dumais"}, {"ref_id": "b54", "title": "WALNUT: A benchmark on semi-weakly supervised learning for natural language understanding", "journal": "", "year": "2022", "authors": "Guoqing Zheng; Giannis Karamanolakis; Kai Shu; Ahmed Awadallah"}, {"ref_id": "b55", "title": "FewNLU: Benchmarking state-of-the-art methods for few-shot natural language understanding", "journal": "Association for Computational Linguistics", "year": "2022-05-22", "authors": "Yanan Zheng; Jing Zhou; Yujie Qian; Ming Ding; Chonghua Liao; Li Jian; Ruslan Salakhutdinov; Jie Tang; Sebastian Ruder; Zhilin Yang"}, {"ref_id": "b56", "title": "Hyperlink-induced pre-training for passage retrieval in open-domain question answering", "journal": "Long Papers", "year": "2022-05-22", "authors": "Jiawei Zhou; Xiaoguang Li; Lifeng Shang; Lan Luo; Ke Zhan; Enrui Hu; Xinyu Zhang; Hao Jiang; Zhao Cao; Fan Yu; Xin Jiang; Qun Liu; Lei Chen"}, {"ref_id": "b57", "title": "Is BERT robust to label noise? A study on learning with noisy labels in text classification", "journal": "", "year": "2022-05-26", "authors": "Dawei Zhu; Michael A Hedderich; Fangzhou Zhai; David Ifeoluwa Adelani; Dietrich Klakow"}, {"ref_id": "b58", "title": "Meta self-refinement for robust learning with weak supervision", "journal": "", "year": "2023-05-02", "authors": "Dawei Zhu; Xiaoyu Shen; Michael A Hedderich; Dietrich Klakow"}, {"ref_id": "b59", "title": "error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?", "journal": "", "year": "", "authors": ""}, {"ref_id": "b60", "title": "crowdworkers) or research with human participants? Left blank", "journal": "", "year": "", "authors": ""}, {"ref_id": "b61", "title": "Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators", "journal": "", "year": "", "authors": " D1"}, {"ref_id": "b62", "title": "crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)? Not applicable", "journal": "", "year": "", "authors": ""}, {"ref_id": "b63", "title": "Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?", "journal": "", "year": "", "authors": " D3"}, {"ref_id": "b64", "title": "Was the data collection protocol approved (or determined exempt) by an ethics review board? Not applicable", "journal": "", "year": "", "authors": " D4"}, {"ref_id": "b65", "title": "Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? Not applicable", "journal": "", "year": "", "authors": " D5"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure1: Performance improvement over weak labels on the test sets. Each point represents the average performance improvement of one approach over five runs. On various NLP datasets, weakly supervised methods (dots) outperform weak labels (blue line) on the test sets. However, simply fine-tuning on the available clean validation data (light green crosses) outperforms all sophisticated weakly supervised methods in almost all cases. See Appendix D.2 for experimental details.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 4 :4Figure4: Using clean data for validation vs. training. We show the average performance (Acc. and F1-score in %) difference between (parameter-efficient) fine-tuning approaches and COSINE when varying amounts of clean samples. COSINE uses the clean samples for validation, whereas fine-tuning approaches directly train on them (indicated in the legend with the subscript 'C'). For most sequence classification tasks, fine-tuning approaches work better once 10 clean samples are available for training. For NER, several hundreds of clean sentences may be required to attain better results via fine-tuning. Refer to Appendix D for a comparison with other WSL approaches.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 6 :6Figure 6: Performance curves of different PLMs during training. PLMs are trained on weak labels and evaluated on both clean and weakly labeled test sets. Pre-training on larger corpora improves performance on the clean distribution. Further plots are in Appendix G.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 7 :7Figure 7: Model performance varying the number of clean samples N and agreement ratio \u03b1. Large \u03b1 generally causes a substantial drop in performance. * : Certain combinations of \u03b1 and N are not feasible because the validation set lacks samples with clean and weak labels that coincide or differ. Further plots are in Appendix G.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "FAdditional plots on CFT with different numbers of clean samplesWe show further plots of experiments in Section 7 with different numbers of clean samples in Figure10. More specifically, it shows the results for selecting N \u2208 {10, 20, 30, 40} clean samples per class from the clean validation set for classification and N \u2208 {100, 200, 300, 400} for NER tasks. These results corroborate the analysis presented in Section 7.", "figure_data": ""}, {"figure_label": "1112", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 11 :Figure 12 :1112Figure 11: Performance curves of different PLMs during training. PLMs are trained on weak labels and evaluated on both clean and weakly labeled test sets. Pre-training on larger corpora improves performance on the clean distribution.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "benchmark. Table1summarizes the basic statistics of the datasets.", "figure_data": "DatasetTask# Class # Train # Val # TestAGNewsTopic496K12K12KIMDbSentiment220K2.5K 2.5KYelpSentiment230K3.8K 3.8KTRECQuestion64,965500500SemEvalRelation91,749178600ChemProtRelation1013K1.6K 1.6KCoNLL-03NER414K3.2K 3.4KOntoNotes 5.0 NER18115K5K23K"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Dataset statistics.", "figure_data": "Additional details on datasets are provided in Appendix A.WSL baselines. We analyze popular WSL ap-proaches including: (1) FT W represents the stan-dard fine-tuning approach 4"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": ").", "figure_data": "3HUIRUPDQFH'HOWD)7 & /R5$ & %LW)LW & $GDSWHU &3HUIRUPDQFH'HOWD3HUIRUPDQFH'HOWD3HUIRUPDQFH'HOWD$OO 9DOLGDWLRQ$OO 9DOLGDWLRQ$OO 9DOLGDWLRQ(a) AGNews(b) Yelp(c) IMDb(d) TREC3HUIRUPDQFH'HOWD3HUIRUPDQFH'HOWD3HUIRUPDQFH'HOWD3HUIRUPDQFH'HOWD$OO 9DOLGDWLRQ$OO 9DOLGDWLRQ$OO 9DOLGDWLRQ$OO 9DOLGDWLRQ(e) SemEval(f) ChemProt(g) CoNLL-03"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "N = 5 clean samples per class for classification tasks. N = 50 clean samples for NER tasks.", "figure_data": "$*1HZV 1 FOHDQVDPSOHV SHUFODVV<HOS 1 FOHDQVDPSOHV SHUFODVV,0'E 1 FOHDQVDPSOHV SHUFODVV75(& 1 FOHDQVDPSOHV SHUFODVV&KHP3URW 1 FOHDQVDPSOHV SHUFODVV6HP(YDO 1 FOHDQVDPSOHV SHUFODVV&R1// 1 FOHDQVDPSOHV2QWR1RWHV 1 FOHDQVDPSOHV$FFXUDF\\)%HIRUH &)7$IWHU &)7%HIRUH &)7$IWHU &)7%HIRUH &)7$IWHU &)7%HIRUH &)7$IWHU &)7%HIRUH &)7$IWHU &)7%HIRUH &)7$IWHU &)7%HIRUH &)7$IWHU &)7%HIRUH &)7$IWHU &)7$FFXUDF\\)&)7 1 FOHDQVDPSOHV $IWHU &)7 $*1HZV SHUFODVV (a) %HIRUH%HIRUH &)7 1 FOHDQVDPSOHV $IWHU &)7 <HOS SHUFODVV%HIRUH &)7 1 FOHDQVDPSOHV $IWHU &)7 ,0'E SHUFODVV%HIRUH &)7 1 FOHDQVDPSOHV $IWHU &)7 75(& SHUFODVV%HIRUH &)7 &KHP3URW 1 FOHDQVDPSOHV $IWHU &)7 SHUFODVV%HIRUH &)7 1 FOHDQVDPSOHV $IWHU &)7 6HP(YDO SHUFODVV%HIRUH &)7 &R1// 1 FOHDQVDPSOHV $IWHU &)7%HIRUH &)7 2QWR1RWHV $IWHU &)7 1 FOHDQVDPSOHV &26,1( /5 )7 : &OHDQ2QO\\"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Computing infrastructure and training costWe use Nvidia V100-32 GPUs for training deep learning models. All WSL approaches studied in", "figure_data": "Avg. over labeling functions (LFs)DatasetTask#Classes#LFs%Ovr. Coverage%Coverage%Overlap%Conflict%Prec.MV#Train#Dev#TestAGNewsNews Class.4969.0810.345.052.4381.6681.2396,00012,00012,000IMDbMovie Sentiment Class.2587.5823.6011.604.5069.8873.8620,0002,5002,500YelpBusiness Sentiment Class.2882.7818.3413.584.9473.0573.3130,4003,8003,800TRECQuestion Class.66895.132.551.820.8475.9262.584,965500500SemEvalWeb Text Relation Class.9164100.000.770.320.1497.6977.331,749200ChemProtChemical Relation Class.2685.625.934.403.9546.6555.1212,8611,6071,607CoNLL-03English News NER4161001004.301.4472.1960.3814,04132503453OntoNotes 5.0Multi-Domain NER18171001001.550.5454.8458.92115,8125,00022,897"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Detailed data statistics. Note that 'Class.' is an abbreviation for classification. Coverage is the amount of samples a labeling function (LF) matches. For NER datasets, labeling functions return an entity or \"O\" thus coverage is always 100%. Overlap asks how many samples have at least 2 matching labeling functions. MV (majority vote) performance is given as F1-score for the NER datasets and as accuracy on the test set otherwise.", "figure_data": "Label Labeling FunctionPOSbeautiful, handsome, talentedNEG than this, than the film, than the moviePOS.*(highly|do|would|definitely|certainly|strongly|i|we).*(recommend|nominate).*POS.*(high|timeless|priceless|HAS|great|real|instructive).*(value|quality|meaning|significance).*"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Examples of two keyword based and two regular expression based rules for the IMDb dataset.this paper can fit into one single GPU. We report the training time of the WSL methods in Table8.", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "The search range of the hyperparameters of the five WSL approaches considered in the paper. For BOND and COSINE, we set T 1 and T 2 to constant values, because we stop training once early-stopping is triggered.", "figure_data": "AGNews IMDb Yelp TREC SemEval ChemProt CoNLL-03 OntoNotes 5.0FT0.20.20.20.10.10.20.20.5L2R2.01.21.50.30.30.40.91.2MLC1.20.81.20.30.20.51.21.0BOND0.50.20.50.10.10.20.41.1COSINE0.60.20.60.20.20.30.51.5"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Running time in hours of each WSL method when trained on a weakly labeled training set. Since we also track the validation and test performance during training, the training time reported here actually overestimates the training time required for each method.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Performance vs. number of clean samples. In most cases, CFT outperforms the other two baselines, WC batch and WC mix , by a considerable margin.", "figure_data": "5R%(57DVPDOO0 SUHWUDLQHGRQa0ZRUGV5R%(57DEDVH0 SUHWUDLQHGRQa0ZRUGV5R%(57DEDVH RULJLQDOSUHWUDLQHGRQa%ZRUGV5R%(57DVPDOO0 SUHWUDLQHGRQa0ZRUGV5R%(57DEDVH0 SUHWUDLQHGRQa0ZRUGV5R%(57DEDVH RULJLQDOSUHWUDLQHGRQa%ZRUGV$FFXUDF\\WUDLQRQZHDN WHVWRQZHDN WUDLQRQZHDN WHVWRQFOHDQ$FFXUDF\\WUDLQRQZHDN WHVWRQZHDN WUDLQRQZHDN WHVWRQFOHDQZHDNODEHOVZHDNODEHOV$FFXUDF\\ $FFXUDF\\ $FFXUDF\\ $FFXUDF\\ Figure 9: 7UDLQLQJVWHSV (a) AGNews 1XPEHURIVDPSOHVSHUFODVV )7&)7 :& EDWFK :& PL[ (e) SemEval 7UDLQLQJVWHSV $FFXUDF\\ $FFXUDF\\ (a) IMDb 7UDLQLQJVWHSV 5R%(57DVPDOO0 SUHWUDLQHGRQa0ZRUGV 7UDLQLQJVWHSV 5R%(57DEDVH0 SUHWUDLQHGRQa0ZRUGV (c) TREC 5R%(57DVPDOO0 SUHWUDLQHGRQa0ZRUGV 5R%(57DEDVH0 SUHWUDLQHGRQa0ZRUGV1XPEHURIVDPSOHVSHUFODVV (b) Yelp 1XPEHURIVDPSOHVSHUFODVV (f) ChemProt 7UDLQLQJVWHSV 7UDLQLQJVWHSV 5R%(57DEDVH RULJLQDOSUHWUDLQHGRQa%ZRUGV WUDLQRQZHDN WHVWRQZHDN WUDLQRQZHDN WHVWRQFOHDQ ZHDNODEHOV 5R%(57DEDVH RULJLQDOSUHWUDLQHGRQa%ZRUGV WUDLQRQZHDN WHVWRQZHDN WUDLQRQZHDN WHVWRQFOHDQ ZHDNODEHOV$FFXUDF\\ ) $FFXUDF\\ )1XPEHURIVDPSOHVSHUFODVV (c) IMDb 1XPEHURIVHQWHQFHV (g) CoNLL-03 7UDLQLQJVWHSV 7UDLQLQJVWHSV 5R%(57DVPDOO0 SUHWUDLQHGRQa0ZRUGV 5R%(57DVPDOO0 SUHWUDLQHGRQa0ZRUGV1XPEHURIVDPSOHVSHUFODVV (d) TREC 1XPEHURIVHQWHQFHV (h) OntoNotes 5.0 7UDLQLQJVWHSV 5R%(57DEDVH0 $FFXUDF\\ $FFXUDF\\ 7UDLQLQJVWHSV (b) Yelp 7UDLQLQJVWHSV SUHWUDLQHGRQa0ZRUGV 7UDLQLQJVWHSV 5R%(57DEDVH RULJLQDOSUHWUDLQHGRQa%ZRUGV WUDLQRQZHDN WHVWRQZHDN WUDLQRQZHDN WHVWRQFOHDQ ZHDNODEHOV (d) SemEval 5R%(57DEDVH0 SUHWUDLQHGRQa0ZRUGV 5R%(57DEDVH RULJLQDOSUHWUDLQHGRQa%ZRUGV WUDLQRQZHDN WHVWRQZHDN WUDLQRQZHDN WHVWRQFOHDQ ZHDNODEHOV7UDLQLQJVWHSV7UDLQLQJVWHSV7UDLQLQJVWHSV7UDLQLQJVWHSV7UDLQLQJVWHSV7UDLQLQJVWHSV(e) ChemProt(f) CoNLL-03"}], "formulas": [{"formula_id": "formula_0", "formula_text": "$OO 9DOLGDWLRQ $FFXUDF\\ )7 Z &26,1( /5 %21' 0/& :HDNODEHOV (a) AGNews $OO 9DOLGDWLRQ $FFXUDF\\ (b) Yelp $OO 9DOLGDWLRQ $FFXUDF\\ (c) IMDb $OO 9DOLGDWLRQ $FFXUDF\\ (d) TREC $OO 9DOLGDWLRQ $FFXUDF\\ (e) SemEval $OO 9DOLGDWLRQ $FFXUDF\\ (f) ChemProt $OO 9DOLGDWLRQ )VFRUH (g) CoNLL-03 $OO 9DOLGDWLRQ )VFRUH (h) OntoNotes 5.0", "formula_coordinates": [5.0, 71.39, 95.98, 451.28, 167.93]}], "doi": "10.18653/v1/n19-1423"}