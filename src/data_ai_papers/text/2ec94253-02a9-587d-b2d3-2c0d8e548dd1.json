{"title": "ClimSim: A large multi-scale dataset for hybrid physics-ML climate emulation", "authors": "Sungduk Yu; Walter M Hannah; Liran Peng; Jerry Lin; Mohamed Aziz Bhouri; Ritwik Gupta; Bj\u00f6rn L\u00fctjens; Justus C Will; Gunnar Behrens; Julius J M Busecke; Nora Loose; Charles Stern; Tom Beucler; Bryce E Harrop; Benjamin R Hillman; Andrea M Jenney; Savannah L Ferretti; Nana Liu; Anima Anandkumar; Noah D Brenowitz; Veronika Eyring; Nicholas Geneva; Pierre Gentine; Stephan Mandt; Jaideep Pathak; Akshay Subramaniam; Carl Vondrick; Rose Yu; Laure Zanna; Tian Zheng; Ryan P Abernathey; Fiaz Ahmed; David C Bader; Pierre Baldi; Elizabeth A Barnes; Christopher S Bretherton; Peter M Caldwell; Wayne Chuang; Yilun Han; Yu Huang; Fernando Iglesias-Suarez; Sanket Jantre; Karthik Kashinath; Marat Khairoutdinov; Thorsten Kurth; Nicholas J Lutsko; Po-Lun Ma; Griffin Mooers; J David Neelin; David A Randall; Sara Shamekh; Mark A Taylor; Nathan M Urban; Janni Yuval; Guang J Zhang; Michael S Pritchard;  Uci;  Llnl;  Columbia;  Ucb;  Mit;  Dlr;  Princeton;  Unil;  Pnnl;  Snl;  Osu;  Nvidia;  Ucsd;  Nyu;  Ucla;  Csu; Allen Ai;  Tsinghua;  Bnl", "pub_date": "2024-02-07", "abstract": "Modern climate projections lack adequate spatial and temporal resolution due to computational constraints. A consequence is inaccurate and imprecise predictions of critical processes such as storms. Hybrid methods that combine physics with machine learning (ML) have introduced a new generation of higher fidelity climate simulators that can sidestep Moore's Law by outsourcing compute-hungry, short, high-resolution simulations to ML emulators. However, this hybrid ML-physics simulation approach requires domain-specific treatment and has been inaccessible to ML experts because of lack of training data and relevant, easy-to-use workflows. We present ClimSim, the largest-ever dataset designed for hybrid ML-physics research. It comprises multi-scale climate simulations, developed by a consortium of climate scientists and ML researchers. It consists of 5.7 billion pairs of multivariate input and output vectors that isolate the influence of locally-nested, high-resolution, high-fidelity physics on a host climate simulator's macro-scale physical state.", "sections": [{"heading": "", "text": "The dataset is global in coverage, spans multiple years at high sampling frequency, and is designed such that resulting emulators are compatible with downstream coupling into operational climate simulators. We implement a range of deterministic and stochastic regression baselines to highlight the ML challenges and their scoring. The data (https://huggingface.co/datasets/LEAP/ClimSim_high-res 2 ) and code (https://leap-stc.github.io/ClimSim) are released openly to support the development of hybrid ML-physics and high-fidelity climate simulations for the benefit of science and society.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction 1.Overview", "text": "Predictions from numerical physical simulations are the primary tool informing policy on climate change. However, current climate simulators poorly represent cloud and extreme rainfall physics [1,2] despite stretching the limits of the world's most powerful supercomputers. The complexity of the Earth system imposes significant restrictions on the spatial resolution we can use in these simulations [3]. Physics occurring on scales smaller than the temporal and/or spatial resolutions of climate simulations are commonly represented using empirical mathematical representations called \"parameterizations\". Unfortunately, assumptions in these parameterizations often lead to errors that can grow into inaccuracies in the future predicted climate.\nMachine learning (ML) is an attractive approach to emulate the complex nonlinear sub-resolution physics-processes occurring on scales smaller than the resolution of the climate simulator-at a lower computational complexity. Their implementation has the exciting possibility of resulting in climate simulations that are both cheaper and more accurate than they currently are [4,5]. Current climate simulators have a typical smallest resolvable scale of 80-200 km, equivalent to the size of a typical U.S. county. However, accurately representing cloud formation requires a resolution of 100 m or finer, demanding six orders of magnitude increase in computational intensity. Exploiting ML remains a conceivable solution to sidestep the limitations of classical computing [5]: resulting hybrid-ML climate simulators combine traditional numerical methods-which solve the equations governing large-scale fluid motions of Earth's atmosphere-with ML emulators of the macro-scale effects of small-scale physics. Instead of relying on heuristic assumptions about these small-scale processes, the emulators learn directly from data generated by short-duration, high-resolution simulations [4,[6][7][8][9][10][11][12][13][14][15][16][17][18]. The task is essentially a regression problem: in the climate simulation, an ML parameterization emulator returns the large-scale outputs-changes in wind, moisture, or temperature-that occur due to unresolved small-scale (sub-resolution) physics, given large-scale resolved inputs (e.g., temperature, wind velocity; see Section 4).\nWhile several proofs of concept have emerged in recent years, hybrid-ML climate simulators have yet to be advanced to operational use. Obtaining sufficient training data is a major challenge impeding interest from the ML community. This data must contain all macro-scale variables that regulate the behavior of sub-resolution physics and be compatible with downstream hybrid ML-climate simulations. Addressing this using training data from uniformly high-resolution simulations has proven to be very expensive and can lead to issues when coupled to a host climate simulation.\nA promising solution is to utilize multi-scale climate simulation methods to generate training data. Crucially, these provide a clean interface between the emulated high-resolution physics and the host climate simulator's planetary-scale dynamics [19]. In theory, this makes downstream hybrid coupled simulation approachable and tractable. In practice, the full potential of multi-scale methods remains largely untapped due to a scarcity of existing datasets, exacerbated by the combination of operational simulation code complexity and the need for domain expertise in choosing variables.\nWe introduce ClimSim, the largest and most physically comprehensive dataset for training ML emulators of atmospheric storms, clouds, turbulence, rainfall, and radiation for use in hybrid-ML climate simulations. ClimSim is a comprehensive collection of inputs and outputs from physical climate simulations using the multi-scale method. ClimSim was prepared by atmospheric scientists and climate simulator developers to lower the barriers to entry for ML experts on this important problem. Our benchmark dataset serves as a foundation for developing robust frameworks that emulate parameterizations for cloud and extreme rainfall physics, and their interaction with other sub-resolution processes. These frameworks enable online coupling within the host coarse-resolution climate simulator, ultimately improving the performance and accuracy of climate simulators used for long-term projections.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Concepts and Terminology from Earth Science", "text": "Convective Parameterization: In atmospheric science, \"convection\" refers to storm cloud and rain development, as well as the associated turbulent air motions. Convective parameterizations represent the integrated effects of these processes, such as the vertical transport of heat, moisture, and momentum within the atmosphere, and condensational heating and drying, on the temporal and spatial scale of the host climate simulator [20][21][22]. Stochastic parameterizations represent sub-resolution (\"sub-grid scale\" in the terminology of Earth science) effects as stochastic processes, dependent on grid-scale variable inputs [23,24] to capture variations arising from sub-grid scale dynamics.\nMulti-Scale Climate Simulators: Multi-scale climate simulation is a technique that represents convection without a convective parameterization, by deploying a smaller-scale, high-resolution cloud-resolving simulator nested within each host grid column of a climate simulator [25][26][27][28][29]. The smaller-scale simulator explicitly resolves the detailed behavior of clouds and their turbulent motions at both a higher spatial and temporal resolution (but with a smaller domain) than the host simulator. This improves the accuracy of the host simulations, but comes at a high computational cost [30,31]. The time-integrated and horizontally averaged influence of the resolved convection is fed upscale to the host climate simulator, and is the target of hybrid ML-climate simulation approaches.\nSignificance of Precipitation Processes for Climate Impacts: In climate simulations, changes in precipitation with warming is a particularly important issue. The frequency of extreme precipitation events increases with warming [32][33][34], with corresponding societal impacts [35]. Current climate simulators agree on the direction of this change, but exhibit large spread in the quantitative rate of increase with warming [36,37].", "publication_ref": ["b32", "b33", "b34", "b35", "b36"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "There have been several recent efforts to produce hybrid-ML emulators using multi-scale climate simulations, analogous to ClimSim [4,[10][11][12][13][14][15][16]38]. Most of these focused on simple aquaplanets [4, 10-13, 16, 38] and those that included real geography [14,15] did not include enough variables for complete land-surface coupling, to our knowledge. Most examine simple multi-layer perceptrons except for [12,15], who used a ResNet architecture, and [39] who used a variational encoder-decoder that accounts for stochasticity. Although downstream hybrid testing in real-geography settings is error-prone, [15] demonstrates some hybrid stability. Compressing input data to avoid causal confounders may improve downstream accuracy [16], and methods have been proven to enforce physical constraints [40,41].\nCompared to the training data used above, ClimSim's comprehensive variable coverage is unprecedented, including all variables needed to couple to and from a land system simulator and enforce physical constraints. Its availability across coarse-resolution, high-resolution, aquaplanet and realgeography use cases is also new to the community. Successful ML innovations with ClimSim can have a downstream impact since it is based on a state-of-the-art multi-scale climate simulator that is actively supported by a mission agency (U.S. Department of Energy).\nIn non-multi-scale settings, an important body of related work [6][7][8][9] has made exciting progress on using analogous hybrid ML approaches to reduce biases in uniform resolution climate simulations, including in an operational climate code with land coupling and downstream hybrid stability [17,18] (see Supplementary Information; SI). Other related work includes full model emulation (FME) for short-term weather prediction [42][43][44]. Whether this approach is possible for climate simulation using the high-frequency output of its state variables remains an open question. For instance, it has recently been shown that incorporating spherical geometry and resolution invariance through spherical Fourier neural operators leads to stability of long rollouts [43]. While ClimSim is focused on hybrid-ML climate simulation and we do not demonstrate FME baselines, ClimSim contains full atmospheric state variable sampling well suited for the task.", "publication_ref": ["b37", "b38", "b39", "b40", "b41", "b42", "b43", "b42"], "figure_ref": [], "table_ref": []}, {"heading": "ClimSim Dataset Construction", "text": "Experiment Outline: ClimSim presents a regression problem with mapping from a multivariate input vector, with inputs x \u2208 R di of size d i = 124 and targets y \u2208 R do of size d o =128 (Figure 1). The input represents the local vertical structure (in horizontal location and time) of macro-scale state variables in a multi-scale physical climate simulator before any adjustments from sub-grid scale convection and radiation are made. The input also includes concatenated scalars containing boundary conditions of incoming radiation at the top of the atmospheric column, and land surface model constraints at its base. The target vector contains the tendencies of the same state variables representing the redistribution of mass and water, microphysical water species conversions, and radiative heating feedbacks associated with explicitly resolved convection. This brackets the change Figure 1: The spatially-local version of ClimSim that our baselines are scored on. A spatially-global version of the problem that expands to the full list of variables would be useful to try.\nin atmospheric state after tens of thousands of computationally intensive, spatially nested simulators of explicit cloud physics have completed a temporally-nested integration. The ultimate goal is to outsource these physics to ML by mapping inputs to targets at comparable fidelity. The target vector includes scalar fields and fluxes from the bottom of the atmospheric column expected by the land surface model component that it must couple to; land-atmosphere coupling is important to predicting regional water cycle dynamics [45,46]. Importantly, ClimSim also includes the option for expanded inputs x \u2208 R di of size d i = 617 and targets y \u2208 R do of size d o = 368, which we demonstrate in one of our experiments.\nLocality vs. Nonlocality: A spatially-global version of the problem could be of practical use for improving ML via helpful spatial context [47,48]. In such a case, the problem becomes 2D \u2192 2D regression, and would encompass inputs x \u2208 R di of maximum size d i = 617 \u00d7 21,600 (grid columns) and targets, y \u2208 R do , of maximum size d o = 368 \u00d7 21,600. Here the second dimension represents the unstructured \"cube-sphere\" computational mesh used by the climate model, which is a list of grid cell locations that span the surface of the sphere [49]. In contrast to typical image-to-image translation or spatio-temporal prediction problems in ML that involve data on a structured grid (i.e. rectilinear), the task at hand is of lower dimensionality. Further details about the climate simulator configuration, simulations, and data, including complete variable lists, can be found in SI.", "publication_ref": ["b44", "b45", "b46", "b47", "b48"], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "Dataset Collection:", "text": "We ran the E3SM-MMF multi-scale climate simulator [28,29,49,50], using multiple NVIDIA A100 GPUs for a total of \u223c 9,800 GPU-hours. We saved global instantaneous values of the atmospheric state before and after high-resolution calculations occurred, isolating state updates due to explicitly-resolved moist convection, boundary layer turbulence, and radiation; details of the climate simulator configuration can be found in SI. These data were saved at 20-minute intervals (i.e. the time step of the climate model) for 10 simulated years, resulting in 5.7 billion samples for the high-resolution simulation that uses an unstructured \"cube-sphere\" horizontal grid with 21,600 grid columns spanning the globe. This grid yields an approximate horizontal grid spacing of 1.5 \u2022 , but unlike a traditional climate model that maps points across the sphere using two dimensions aligned with cardinal north/south and east/west directions, unstructured grids use a single dimension to organize the horizontal location of points. The atmospheric columns at each location and time are treated as independent samples. Thus, the total number of samples can be understood by considering that atmospheric columns at each location and time are treated as independent samples, such that 5.7 billion \u2248 21,600 horizontal locations per time step \u00d7 72-time steps per simulated day \u00d7 3,650 simulated days). It is important to note that each sample retains a 1D structure corresponding to the vertical variation across 60 levels. We also ran two additional simulations with approximately ten times less horizontal resolution, with only 384 grid columns spanning the globe, resulting in 100 million samples for each simulation. These low-resolution options allow for fast prototyping of ML models, due to smaller training data volumes and less geographic complexity. One lowresolution simulation uses an \"aquaplanet\" configuration, i.e., a lower boundary condition of specified sea surface temperature, invariant in the longitudinal dimension with no seasonal cycle. This is the simplest prototyping dataset, removing variance associated with continents and time-varying boundary conditions. The total data volume is 41.2TB for the high-resolution dataset and 744GB for each of the low-resolution datasets.\nDataset Interface: Raw model outputs emerge from the climate simulator as standard NetCDF files which can be easily parsed in any language. Each timestep yields files containing input and target vectors separately, resulting in a total of 525,600 files for each of the three datasets. To prevent redundancy, variable metadata and grid information was saved separately.\nThe raw tensors from the climate simulations are initially either 2D or 3D, depending on the variable. For 2D tensors, the dimensions represent time and horizontal location. While these variables actually depend on three physical dimensions (time and 2D space), since each location on the sphere is indexed along a single axis due to the climate model's unstructured horizontal grid, the apparent dimensionality is lower. Such variables include solar insolation, snow depth over land, surface energy fluxes, and surface precipitation rate. 3D tensors include the additional dimension representing altitude relative to the Earth's surface, for height-varying state variables like temperature, humidity, and wind vector components. Separate files are used to store each timestep and variable. ClimSim includes a total of 24 2D variables and 10 3D variables (see Table 1 in SI).\nDataset Split: The 10-year datasets are divided into: (a) a training and validation spanning the first 8 years (0001-02 to 0009-01; YYYY-MM), excluding the first simulated month for numerical spin-up, and (b) a test set spanning the remaining two years (i.e., 0009-03 to 0011-02). A one-month gap is intentionally introduced between the two sets to prevent test set contamination via temporal correlation. Both sets are stored separately in our data repositories.", "publication_ref": ["b48", "b49"], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Energy use:", "text": "The computing and energy costs of generating ClimSim could be viewed as wasteful and having a negative consequence for society through associated emissions. We emphasize that while it can appear large, the compute used is actually orders of magnitude less than what is consumed by operational climate prediction. Associated emissions are minimized given that our integrations were performed on energy-efficient GPU hardware. The cost must also be weighed against the potential social benefit of mitigating future energy consumption by eliminating end users' need for costly physics-based MMF simulations. Meanwhile, a large consortium of interested parties have helped agree on this dataset, to help ensure it is not wasted.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "To guide ML practitioners using ClimSim, we provide an example ML workflow using the lowresolution, real-geography dataset for the task described in Section 1. All but one of our baselines focuses on emulating the subset of total available input and target variables illustrated in Figure 1, with the following inputs x \u2208 R di of size d i = 124, and targets y \u2208 R do of size d o = 128 (Figure 1, Table 1), chosen for its similarity to recent attempts in the literature.\nTraining/Validation Split: We divide the 8-year training/validation set into the first 7 years (i.e., 0001-02 to 0008-01 in the raw filenames' \"year-month\" notation) for training and the subsequent 1 year (0008-02 to 0009-01) for validation.\nPreprocessing Workflow: Our preprocessing steps were (1) downsample in time by using every 7th sample, (2) collapse horizontal location and time into a single sample dimension, (3) normalize variables by subtracting the mean and dividing by the range, with these statistics calculated separately at each of the 60 vertical levels for the four variables with vertical dependence, and (4) concatenate variables into multi-variate input and output vectors for each sample (Figure 1). The heating tendency Table 1: The subset of input and target variables used in most of our experiments (Figure 1). Dimension length 60 corresponds to the total number of vertical levels (discretized altitudes) of the climate simulator. target dT /dt (i.e., time rate of temperature T ) was calculated from the raw climate simulator output as (T after \u2212 T before )/\u2206t, where \u2206t = 1200 s) is the climate simulator's known macro-scale timestep. Likewise, the moisture tendency was calculated via taking the difference of humidity state variables recorded before versus after the convection and radiation calculations. This target variable transformation is done so that we can compare the performance of our baseline models to that of previously published models that reported errors of emulated tendencies [14,39]. Additionally, this transformation implicitly normalizes the target variables leading to better convergence properties for ML algorithms. Given the domain-specific nature of the preprocessing workflow, we provide scripts in the GitHub repository for workflow reproduction.", "publication_ref": ["b38"], "figure_ref": ["fig_1", "fig_1", "fig_1", "fig_1"], "table_ref": ["tab_7", "tab_7"]}, {"heading": "Baseline Architectures", "text": "Six baseline models used in our experiment are briefly described here. Refer to SI for further details.\nConvolutional Neural Network (CNN) uses a 1D ResNet-style network. Each ResNet block contains two 1D convolutional layers and a skip connection. CNNs can learn spatial structure and have outperformed MLP and graph-based networks in [51]. The inputs and outputs for the CNN are stacked in the channel dimensions, such that the mapping is 60 \u00d7 6 \u2192 60 \u00d7 10. Accordingly, global variables have been repeated along the vertical dimension.\nEncoder-Decoder (ED) consists of an Encoder and a Decoder with 6 fully-connected hidden layers each [39]. The Encoder of ED condenses the original dimensionality of the input variables down to only 5 nodes inside the latent space. This enhances the interpretability of ED and makes the model beneficial for advanced postprocessing of multivariate climate data [39].\nHeteroskedastic Regression (HSR) [52] predicts a separate mean and standard deviation for each output variable, using a regularized MLP.\nMulti-layer Perceptron (MLP) is a fully connected, feed-forward neural network. The MLP architecture used for our experiments is optimized via an extensive hyperparameter search with 8,257 trials.\nRandomized Prior Network (RPN) is an ensemble model [53]. Each member of the RPN is built as the sum of a trainable and a non-trainable (so-called \"prior\") surrogate model; we used MLP for simplicity. Multiple replicas of the networks are constructed by independent and random sampling of both trainable and non-trainable parameters [54,55]. RPNs also resort to data bootstrapping (e.g., subsampling and randomization) in order to mitigate the uncertainty collapse of the ensemble method when tested beyond the training data points [55].\nConditional Variational Autoencoder (cVAE) uses amortized variational inference to fit a deep generative model that is conditioned on the input and can produce samples from a complex predictive distribution.  ", "publication_ref": ["b50", "b38", "b38", "b51", "b52", "b53", "b54", "b54"], "figure_ref": [], "table_ref": []}, {"heading": "Skill Boost from Expanding Features and Targets", "text": "We performed an ablation of our best performing MLP baseline to demonstrate the added value of the expanded inputs and targets available in ClimSim, i.e. using inputs x of size d i = 617 and targets y \u2208 R do of size d o = 368; see Table 1 in SI for the full list of variables. We use the same transformation described in our preprocessing workflow to compute and add condensate (cloud liquid and cloud ice) and momentum (zonal and meridional winds) tendencies to the target vector. We conducted this ablation study with both the low-resolution and the high-resolution datasets (see Section 3.1 in SI for further details regarding these MLP variants). For common elements of the target vector, using all available variables leads to a uniform improvement in prediction accuracy, especially for precipitation, in both resolutions (Figures SI7, SI8 and Table SI4). The larger errors (e.g., MAE and RMSE) observed in the high-resolution emulators are anticipated due to the increased variance of higher-resolution data. Nevertheless, the similarity of their R 2 values to those of the corresponding low-resolution emulators confirms their adequate performance.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7", "tab_18"]}, {"heading": "Evaluation Metrics", "text": "Our evaluation metrics are computed separately for each variable in the output vector. Mean absolute error (MAE) and the coefficient of determination (R 2 ) are calculated independently at each horizontal and vertical location, and then averaged horizontally and vertically to produce the summary statistics in Figure 2. For the vertically-varying fields, we first form a mass-weighting and then convert moistening and heating tendencies into common energy units in Watts per square meter as in [56]. We also report continuous ranked probability scores (CRPS) for all considered models in SI.", "publication_ref": ["b55"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Baseline Model Results", "text": "Figure 2 summarizes the error characteristics. Whereas heating and moistening rates have comparable global mean MAE, behind a common background vertical structure (Figure 2 b,c) the coefficient of determination R 2 (d,e) reveals that certain architectures (RPN, HSR, cVAE, CNN) consistently perform better in the upper atmosphere (model level < 30) whereas the highly optimized MLP model outperforms in the lower atmosphere (model level > 30) and therefore the global mean (Table 2). For the global mean MAE we see the largest averaged errors for PRECC and NETSW (mean MAE > 15 W/m\u00b2, Figure 2 and Table 2), where MLP clearly has the best the best skill compared to all other benchmark models. For the other variables, the global mean MAE is considerably smaller and the skill of the benchmarks model appears to be more similar in absolute numbers. While for the global mean R 2 we find the lowest measurable performance for dT/dt and PRECC (mean R 2 < 0.7) and in these cases, CNN gives the most skillful predictions. The other variables have larger R 2 of order 0.8 or higher, which suggests that these quantities are easier to deep-learn (Table 2). For dq/dt and PRECSC global mean R 2 is not an ideal evaluation metric due to negligible variability in dq/dt in the upper atmosphere and for PRECSC in the tropics in the dataset (Table 2).\nAdditional tables and figures that reveal the geographic and vertical structure of these errors, fit quality, and analysis of stochastic metrics, are included in SI (Sections 4.   1 for variable definitions.", "publication_ref": [], "figure_ref": ["fig_0", "fig_0", "fig_0"], "table_ref": ["tab_2", "tab_2", "tab_2", "tab_2", "tab_7"]}, {"heading": "Physics-Informed Guidance to Improve Generalizability and Coupled Performance", "text": "Physical Constraints: Mass and energy conservation are important criteria for Earth system simulation. If these terms are not conserved, errors in estimating sea level rise or temperature change over time may become as large as the signals we hope to measure. Enforcing conservation on emulated results helps constrain results to be physically plausible and reduce the potential for errors accumulating over long time scales. We discuss how to do this and enforce additional constraints, such as non-negativity for precipitation, condensate, and moisture variables in the Supporting Information.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Stochasticity and Memory:", "text": "The results of the embedded convection calculations regulating d o are chaotic, and thus worthy of stochastic architectures, as in our RPN, HSR, and cVAE baselines. These solutions are likewise sensitive to sub-grid initial state variables from an interior nested spatial dimension that has not been included in our data.\nTemporal Locality: Incorporating the previous timesteps' target or feature in the input vector inflation could be beneficial as it captures some information about this convective memory and utilizes temporal autocorrelations present in atmospheric data.\nCausal Pruning: A systematic and quantitative pruning of the input vector based on objectively assessed causal relationships to subsets of the target vector has been proposed as an attractive preprocessing strategy, as it helps remove spurious correlations due to confounding variables and optimize the ML algorithm [16].\nNormalization: Normalization that goes beyond removing vertical structure could be strategic, such as removing the geographic mean (e.g., latitudinal, land/sea structure) or composite seasonal variances (e.g., local smoothed annual cycle) present in the data. For variables exhibiting exponential variation and approaching zero at the highest level (e.g., metrics of moisture), log-normalization might be beneficial.\nExpanded Resolution and Complete Inputs and Outputs: Our baseline models have focused on the low-resolution dataset, for ease of data volume, and using only a subset of the available inputs and outputs. This illustrates the essence of the ML challenge. However, we show in our ablation study, using MLPs, that including all input variables yields generally an improved reproduction of the target variables in both the low-resolution and the high-resolution dataset (Figures SI7 and SI8 and Table SI4). Accordingly, we encourage users who discover competitive fits in this approachable limit to expand to all inputs/outputs in the high-resolution, real-geography dataset, for which successful fits become operationally relevant.\nFurther ML Approaches: Recent methods to capture multi-scale processes using neural operators that learn in a discretization-invariant manner and can predict at higher resolutions than available during training time [57] may be attractive. Their performance can be further enhanced by incorporating physics-informed losses at a higher resolution than available training data [58]. Ideas on ML modeling for sub-grid closures from adjacent fields like turbulent flow physics and reactive flows can also be leveraged for developing architectures with an inductive bias for known priors [59], easing prediction of stiff non-linear behavior [60][61][62], generative modeling with physical constraints [63,64] and for interpretability of the final trained models [60].", "publication_ref": ["b56", "b57", "b58", "b59", "b60", "b61", "b62", "b63", "b59"], "figure_ref": [], "table_ref": ["tab_18"]}, {"heading": "Limitations and Other Applications", "text": "Idealizations: A limitation of the multi-scale climate simulator used to produce ClimSim (E3SM-MMF) is that it assumes scale separation, i.e., that convection can be represented as laterally periodic within the grid size of the host simulator, and neglects sub-grid scale representations of topographic and land-surface variability. Despite these simplifications, the data adequately captures many essential aspects of the ML problem, such as stochasticity, and interactions across radiation, microphysics, and turbulence.\nHybrid testing: Inclusion of a natural path for downstream testing of learned physics emulators as fully coupled components of a hybrid-ML climate simulator is vital. However, such a workflow is not yet included in ClimSim, since there is no easy way for the ML community to run many hybridized variants of the E3SM-MMF in a distributed high-performance GPU computing infrastructure via a lightweight API. It is our eventual goal to tackle the software engineering needed to enable such a protocol, since, in the long term, it is in this downstream environment where ML researchers should expect to have their maximum impact on the field of hybrid-ML climate simulation. Meanwhile, ClimSim provides the first step.\nStochasticity: One open problem that the dataset may allow assessing is understanding the role of stochasticity in hybrid-ML simulation. While primarily used as a dataset for regression, it would be also interesting to assess and understand the degree to which different variables are better modeled as stochastic or deterministic, or if the dataset gives rise to heavy-tailed or even multi-modal conditional distributions that are important to capture. To date, these questions have been raised based on physical conjectures [e.g., 65] but remain to be addressed in the ML-based parameterization literature. For instance, precipitation distributions have long tails that are projected to lengthen under global warming [34,66]-and will thus tend to generate out-of-sample extremes. ClimSim could help construct optimal architectures to capture precipitation tails and other impactful climate variables such as surface temperature.\nInterpretability: This dataset could also be utilized to discover physically interpretable models for atmospheric convection, radiation, and microphysics. A possible workflow would apply dimensionality reduction techniques to identify dominant vertical variations, followed by symbolic regression to recover analytic expressions [67,68].\nGeneralizability: Although the impacts of global warming and inter-annual variability are absent in this initial version of ClimSim, important questions surrounding climate-convection interactions can begin to be addressed. One strategy would involve partitioning the data such that the emulator is trained on cold columns, but validated on warm columns, where warmth could be measured by surface temperatures, as in [56]. However, the results from this approach may also reflect the dependence of convection on the geographical distribution of surface temperatures in the current climate and should be interpreted with caution. To optimally engage ML researchers in solving the climate generalization problem, a multi-climate extension of ClimSim should be developed that includes physical simulations that samples future climate states and more internal variability.\nRelevance determination and active learning: While the climate simulator code offers data generation flexibility, guidance on ideal regimes to target for improved learning would benefit the domain scientists able to run it. This question can be addressed with the current data and metrics of interest provided.", "publication_ref": ["b33", "b65", "b66", "b67", "b55"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion and Future Work", "text": "We introduce ClimSim, the most physically comprehensive dataset yet published for training ML emulators of atmospheric storms, clouds, turbulence, rainfall, and radiation for use in hybrid-ML climate simulation. It contains all inputs and outputs necessary for downstream coupling in a fullcomplexity multi-scale climate simulator. We conduct a series of experiments on a subset of these variables that demonstrate the degree to which climate data scientists have been able to fit their deterministic and stochastic components.\nWe hope ML community engagement in ClimSim will advance fundamental ML methodology and clarify the path to producing increasingly skillful sub-grid physics emulators that can be reliably used for operational climate simulation. To facilitate two-way commications between ML practitioners and climate scientists, we incorporate many desired characteristics for an ideal benchmark dataset suggeted in [69]. Such interdisciplinary collaboration will open up an exciting future in which the computational limits that currently constrain climate simulation can be reconsidered.\nWe plan to soon extend ClimSim to include, first, a sampling of multiple future climate states. Second, we aim to provide a protocol for downstream hybrid simulation testing. We hope lessons learned in our chosen limit of multi-scale atmospheric simulation will have applicability in other sub-fields of Earth System Science where computational constraints are currently a barrier to including explicit representations of more systems of nested complexity. ", "publication_ref": ["b68"], "figure_ref": [], "table_ref": []}, {"heading": "References 1 Climate Simulations", "text": "Climate models divide the Earth's atmosphere, land surface, and ocean into a 3D grid, creating a discretized representation of the planet. Somewhat like a virtual Lego construction of Earth, with each brick representing a small region (grid cell). Earth system models are made up of independent component models for the atmosphere, land surface, rivers, ocean, sea ice, and glaciers. Each of these component models is developed independently and can run by itself when provided with the appropriate input data. When running as a fully coupled system the \"component coupler\" handles the flow of data between the components.\nWithin each grid cell of the component models, a series of complex calculations are performed to account for various physical processes, such as phase changes of water, radiative heat transfer, and dynamic transport (referred to as \"advection\"). Each component model uses the discretized values of many quantities (such as temperature, humidity, and wind speed) as inputs to parameterizations and fluid solvers to output those same values for a future point in time.\nThe atmosphere and ocean components are the most expensive pieces of an Earth system model, which is largely due to the computation and inter-process communication associated with their fluid dynamics solvers. Furthermore, a significant portion of the overall cost is attributed to the atmospheric physics calculations that are performed locally within each grid column. It is important to note that atmospheric physics serves as a major source of uncertainty in climate projections, primarily stemming from the challenges associated with accurately representing cloud and aerosol processes.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Model Description", "text": "The data that comprise ClimSim are from simulations with the Energy Exascale Earth System Model-Multiscale Modeling Framework version 2.1.0 (E3SM-MMF v2) [1]. Traditionally, global atmospheric models parameterize clouds and turbulence using crude, low-order models that attempt to represent the aggregate effects of these processes on larger scales. However, the complexity and nonlinearity of cloud and rainfall processes make them particularly challenging to represent accurately with parameterizations. The MMF approach replaces these conventional parameterizations with a cloud resolving model (CRM) in each cell of the global grid, so that cloud and turbulence can be explicitly represented. Each of these independent CRMs is spatially fixed and exchange coupling tendencies with a parent global grid column. This novel approach to representing clouds and turbulence can improve various aspects of the simulated climate, such as rainfall patterns [2].\nThe configuration of E3SM-MMF used here shares some details with E3SMv2. The dynamical code of E3SM uses a spectral element approach on a cubed-sphere geometry. Physics calculations are performed on an unstructured, finite-volume grid that is slightly coarser than the dynamics grid, following Hannah et al. (2021) [3], which is better aligned with the effective resolution of the dynamics grid. Cases with realistic topography include an active land model component that responds to atmospheric conditions with the appropriate fluxes of heat and momentum.\nThe embedded CRM in E3SM-MMF is adapted from the System for Atmospheric Modeling (SAM) described by   [4]. While the CRM does explicitly represent clouds and turbulence, it still cannot represent the smallest scales of turbulence and microphysics, and, therefore, these processes still need to be parameterized within each CRM grid cell. Microphysical processes are parameterized with a single-moment scheme, and sub-grid scale turbulent fluxes are parameterized using a diagnostic Smagorinsky-type closure. Convective momentum transport in the nested CRM is handled using the scalar momentum tracer approach of Tulich (2015) [5]. The CRM uses an internal timestep of 10 seconds, while the global calculations use a timestep of 20 minutes.\nDespite recent efforts to accelerate E3SM-MMF with GPUs and algorithmic techniques [6], the CRM domain size strongly affects the computational throughput and limits the type of experiments that can be conducted. However, the MMF approach is quite flexible in how the CRM size is specified. E3SM-MMF is typically run with a 2D CRM that neglects one of the horizontal dimensions, and employs relatively coarse grid spacing that cannot represent small clouds. Increasing the size of this 2D domain by adding further columns (more CRM cells) generally improves the realism of the model solution. Reducing the model grid spacing can also improve the model to a certain degree, although the number of columns often needs to be increased to avoid the degradation associated with a small CRM. Ideally, the CRM would always be used in a 3D configuration to fully capture the complex, chaotic turbulence that dictates the life cycle of each individual cloud, but this approach is generally limited to special experiments that can justify the extra computational cost. The simulations for ClimSim utilize a 2D CRM with 64 columns and 2 km horizontal grid spacing within each grid cell.\nThe atmospheric component of E3SM uses a hybrid vertical grid that is \"terrain-following\" near the surface, and transitions to be equivalent to pressure levels near the top (e.g., https://www2. cesm.ucar.edu/models/atm-cam/docs/usersguide/node25.html). The vertical levels are specified to be thin near the surface to help capture turbulent boundary layer processes, and are gradually stretched to be very coarse in the stratosphere. E3SM-MMF uses 60 levels for the global dynamics with a top level around 65 km. The CRM used for atmospheric physics uses 50 levels, ignoring the upper 10 levels, to avoid problems that arise from using the anelastic approximation with very low densities. This does not create any issues, because cloud processes are generally confined to the troposphere where the anelastic approach is valid. The hybrid grid can be converted to pressure levels using Equation 1, where P 0 = 100, 000 Pa is a reference pressure, and P s (x, t) is the surface pressure which varies in location x and time t:\nP k = A k P 0 + B k P s(1)\nA k and B k -where the subscript k denotes the index of vertical coordinate-are the fixed, prescribed coefficients that define how the \"terrain-following\" and \"pure pressure\" coordinates are blended to define the hybrid coordinate at each vertical level. A k and B k are provided as a part of the dataset with variable names of hyam and hyai or hybm and hybi, depending on whether mid-level or interface values are needed. The third character of the variable names (\"a\" and \"b\") in Equation 1 denotes A k and B k coefficients, respectively. Note that the indexing of the vertical coordinate starts from the top of the atmosphere due to the construct of A k and B k coefficients, e.g., k = 0 for the top and k = 59 for the surface in E3SM-MMF.\nIn the E3SM-MMF framework, the sequencing of atmospheric processes can be conceptualized as follows. It starts with a surface coupling step that receives fluxes from the surface component models (i.e., land, ocean, and sea ice). This is followed by a set of relatively inexpensive physics parameterizations that handle processes such as airplane emissions, boundary layer mixing, and unresolved gravity waves. The global dynamics then takes over to evolve the winds and advect tracers on the global grid. Finally, there is another set of physics calculations to handle clouds, chemistry, and radiation, which are relatively expensive. This final physics section is where the embedded CRM of E3SM-MMF is used, and is the ideal target for surrogate model emulation due to its outsized computational expense. Accordingly, this step represents the target of ClimSim.\nOne area where E3SM-MMF significantly differs from E3SMv2 is in the treatment of aerosols and chemistry. The embedded CRM in E3SM-MMF predicts the mass of water species (i.e., cloud and rain droplet mass mixing ratios) but does not predict the number concentration (i.e., number of drops per mass of air). One consequence of this limitation is that E3SM-MMF cannot represent complex cloud aerosol interactions that can impact droplet number concentrations and cloud radiative feedbacks. Therefore, E3SM-MMF cannot use the more sophisticated aerosol and chemistry package used by E3SMv2, and instead uses prescribed aerosol and ozone amounts to account for the direct radiative impact of these tracers. Current efforts are addressing this limitation for future versions of E3SM-MMF.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Model Configurations", "text": "The simulations used for ClimSim were performed on the NERSC Perlmutter machine. We have documented all code (including the code to preprocess the data, create, train, and evaluate the baseline models, and visualize data and metrics) in an openly-available GitHub repository: https://leap-stc.github.io/ClimSim.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Variable List", "text": "All variables included in our dataset are listed in Table 1.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Dataset Statistics", "text": "Here, we present some distribution statistics to aid in understanding the dataset. Detailed distributions for all variables are provided in https://github.com/leap-stc/ClimSim/tree/main/ dataset_statistics. These statistics are calculated for each vertical level individually for the vertically-resolved variables (e.g., state_t and state_q0001). For each variable (additionally, at each level for the vertically-resolved variables), a histogram is provided to visualize the distribution using 100 bins. Additionally, a text file accompanies each histogram, containing key statistical measures such as the mean, standard deviation, skewness, kurtosis, median, deciles, quartiles, minimum, maximum, and mode. The text file also includes the bin edges and the corresponding frequency  values used to generate the histogram figures. This comprehensive approach allows for a detailed analysis of the dataset's distributions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dataset Applications", "text": "Our data can benefit a broader audience beyond climate modelers wishing to explore ML for sub-grid parameterization. For climate studies, while high-frequency timestep-level outputs from simulations are rarely archived, they offer insights into convective extremes and diurnal variability. Such data opens the path to explore multi-scale interactions between rapid dynamics and broader weather and climate fluctuations. This includes a detailed examination of variables needed to constrain vertically resolved energy and water budgets and understand their variability. For the machine learning community, this dataset addresses the scarcity of large-scale regression benchmarks, common in the sciences. Such benchmarks are less common compared to prevalent industrial datasets that emphasize classification, computer vision, and NLP tasks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Target Audiences", "text": "In essence, this benchmark aims to democratize and expand access to advanced climate modeling. High-potential architectures will undergo testing in the superparameterized version of the DOE's primary climate model, E3SM. Successful integration would substantially reduce computational costs for the DOE when contemplating the deployment of MMF technology in climate prediction. E3SM's external user community, typically deterred by the extensive computational demands of superparameterized simulators, also stands to benefit. Currently, only a minority with substantial computing resources can engage with such models. A successful recipe for ClimSim could thus democratize the use of explicit convection for a broader user base. If performant architectures also prove effective in the NCAR Community Earth System Model (CESM) -the world's most widely used open source climate simulator -the user base could expand significantly. Given its software similarities to E3SM, it is logical to expect that ClimSim's learnt parameterizations will be readily adaptable to CESM. Moreover, we anticipate that a successful hybrid machine learning climate simulator will bring benefits to a diverse range of industry sectors, including those vulnerable to climate risks (such as agriculture, energy, and tourism), as well as the climate risk industry itself (such as insurance and risk assessment).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Baseline Models", "text": "This section offers a detailed depiction of six baseline models. Every facet of model designs, excluding the dimensions of the input and output layers, differs among the models. We recognize that while this approach maximizes the differentiation among baseline models, such extensive degrees of freedom complicate the complete isolation of the effects arising from optimization parameter choices and those originating from the model architecture itself. In future ClimSim releases, baseline models will share more constraints (including optimization parameters) to highlight the performance difference due to model architectures.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Multilayer Perceptron (MLP)", "text": "A multilayer perceptron (MLP) is a basic, densely connected artificial neural network. We used KerasTuner [7] with a random search algorithm for hyperparameter optimization. The following hyperparameters were optimized: the number of hidden layers (N layers ), the number of nodes per layer (N nodes ), activation function, and batch size. The search domains were:\n\u2022 N layers : [3,4,5,6,7,8,9,10,11,12,13] \u2022 Note that N nodes was selected independently for each hidden layer. For example, for N layers = k, N nodes was drawn from the search domain k times. The width of the last hidden layer was fixed at 128. The output layer utilized the linear activation function for the first 120 outputs (corresponding to the heating and moistening tendencies), and ReLU for the remaining 8 variables (corresponding positive-definite surface variables). The loss function was taken as the mean squared error (MSE), and the learning rate was defined using a cyclic scheduler, with an initial learning rate of 2.5 \u00d7 10 -4 , maximum of 2.5 \u00d7 10 -3 , and step size of 4 epochs.\nFollowing Yu et al. ( 2023) [8], we conducted the hyperparameter search in two stages. In the first stage, a total of 8,257 randomly-drawn hyperparameter configurations were trained and evaluated with a tiny subset of the full training set, sub-sampled in the time dimension with a stride of 37. In the second stage, the top 0.2% candidates (160 hyperparameter configurations) were re-trained with a larger fraction of the full training set (sub-sampled with a stride of 7), and then evaluated for our MLP baseline. After this two-step search process, the best hyperparameter configuration was identified as:  1.\nN layers = 5, N nodes = [\nTo provide some context on the amount of variance in model performance that can be attributed to random effects of optimization, the top 160 models were selected from our pool of 8,257 trials and scored on the validation set; the 5th to 95th percentile range of this ensemble is shown by the error bars in Figures 2a and SI3, and by the grey shading in Figures 2b-e, SI4, and SI5.", "publication_ref": [], "figure_ref": ["fig_1", "fig_0", "fig_0"], "table_ref": []}, {"heading": "MLP with expanded features and targets:", "text": "We built MLP with an expanded set of input and output variables, as elaborated in Section 4.2 of the main text. For the sake of clarity, we designate an MLP model employing the subset of available variables (outlined in Section 4 of the main text) as \"MLPv1,\" while an MLP model utilizing the expanded variables is referred to as \"MLPv2.\" The hyperparameter optimization for MLPv2 followed a similar process as MLPv1, with the exception that the search domain of batch size was defined as [2700, 5400, 10800, 21600, 43200, 64800, 86400, 129600, 172800]. After 11,851 search trials, the best hyperparameter configuration was identified as: MLP with the high-resolution dataset: In conjunction with the MLP featuring expanded features and targets, we also constructed MLP models using the high-resolution dataset for both MLPv1 and MLPv2. To differentiate these models from those constructed with the low-resolution dataset, we add the suffix \"-ne30\" to their names. The hyperparameters for MLPv1-ne30 and MLPv2-ne30 were optimized using the same methodology as was applied to their low-resolution counterparts. For MLPv1-ne30, after 10,296 search trials, the best hyperparameter configuration was identified as:  \nN layers = 3, N nodes = [\nN layers = 4, N nodes = [", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Randomized Prior Network (RPN)", "text": "A randomized prior network (RPN) is an ensemble model [9]. Each member of the RPN is built as the sum of a trainable and a non-trainable (so-called \"prior\") surrogate model; we used MLP for simplicity. Multiple replicas of the networks are constructed by independent and random sampling of both trainable and non-trainable parameters [10,11]. RPNs also resort to data bootstrapping in order to mitigate the uncertainty collapse of the ensemble method when tested beyond the training data points [11]. Data bootstrapping consists of sub-sampling and randomization of the data each network in the ensemble sees during training. Hyperparameters of individual MLPs (i.e., N layers , N nodes , batch size) did not need to be tuned from scratch, and were instead chosen based on the hyperparameter search mentioned in Section 3.1. RPN ensembles of 128 networks were considered justified [10].\nIn particular, individual MLPs forming the RPN were considered as fully connected neural networks with N layers = 5, N nodes = [768, 640, 512, 640, 640], and a batch size of 3,072, as in Section 3.1. We utilized ReLU activation (with a negative slope of 0.15) for all layers except for the output layer, where the linear activation function was used.\nThe MLPs were trained for a total of 13,140 stochastic gradient descent (SGD) steps using the Adam optimizer. The learning rate was initialized at 5 \u00d7 10 -4 with an exponential decay at a rate of 0.99 for every 1,000 steps. The RPN baseline has approximately 222.3 million parameters (\u223c1.74 million per MLP) and executes 0.89 GFlops on one data point.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Convolutional Neural Network (CNN)", "text": "The convolutional neural network (CNN) used is a modified version of a residual network (ResNet). Each ResNet block is composed of two, 1D convolutions (Conv1D) with a 3 \u00d7 3 kernel using \"same\" padding, and an output feature map size of 406. Each Conv1D is followed by ReLU activation and dropout (with rate = 0.175). Residuals were also 1D convolved using a 1 \u00d7 1 kernel, and added back to the output of the main ResNet block.\nThe CNN composes 12 such ResNet blocks, followed by \"flattening\" of the feature map via a 1 \u00d7 1 convolution and eLU activation. Two separate Dense layers (and their corresponding activations) map the output feature map to their respective co-domains: one to (\u2212\u221e, \u221e) assuming that verticallyresolved variables have no defined range, and another to [0, \u221e) for all globally-resolved variables. These were concatenated as the output of the network.\nA hyperparameter search was conducted on depth, width, kernel size, activation functions, loss functions, and normalization types using the Hyperband [12] strategy with the KerasTuner [7] framework. The search domains were:\n\u2022 Model depth/number of ResNet blocks: [2,15] \u2022 Model width: [32,512] \u2022 Kernel width: [3,5,7,9] \u2022 Activation function: [GeLU, eLU, ReLU, Swish]\n\u2022 Layer normalization: [True, False]\n\u2022 Dropout: [0.0, 0.5]\n\u2022 Optimizer: [SGD, Adam]\nThe CNN was trained for 10 epochs with an Adam optimizer with standard hyperparameters (\u03b2 1 = 0.9, \u03b2 2 = 0.999, \u03f5 = 1 \u00d7 10 \u22127 ). The learning rate was defined using a cyclic scheduler, with an initial learning rate of 1 \u00d7 10 -4 , a maximum of 1 \u00d7 10 -3 , and a step size of 2 \u00d7\u230a 10,091,520 12 \u230b. A scaling function of 1 2.0 x\u22121 was applied to the scheduler per step x. The hyperparameter search was conducted for 12 hours on 8 NVIDIA Tesla V100 32GB cards, with one model executing on each card. A weighted mean absolute error (MAE) was used as the loss function for optimization. We down-weighted the standard MAE loss to de-emphasize repeated scalar values provided to the network as input. The weighted MAE function is defined below:  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Heteroskedastic Regression (HSR)", "text": "We quantified the inherent stochasticity in the data D = {(x 1 , y 1 ), . . . , (x n , y n )}, and the uncertainty in our prediction by providing a distributional prediction instead of a point estimate. In hetereoskedastic regression (HSR), this predictive distribution is modeled explicitly; here as independent Gaussians with unique mean \u00b5 k and precision (inverse variance) \u03c4 k for each variable. We assumed\ny i | x i \u223c N (\u00b5(x i ), Diag(\u03c4 (x i ) \u22121 )),\nand parameterized both \u00b5 and \u03c4 as over-parameterized feed-forward neural networks (i.e., MLPs) \u00b5 \u03b8 (x) and\u03c4 \u03d5 (x), respectively. This yielded the corresponding predictive distribution\ny i | x i \u223c N (\u03bc \u03b8 (x i ), Diag(\u03c4 \u03b8 (x i ) \u22121 )),\nwhich was fitted with maximum likelihood estimation (MLE) by minimizing the objective\nL(\u03b8, \u03d5) = 1 2n n i=1 \u2225\u03c4 \u03d5 (x i ) (y i \u2212\u03bc \u03b8 (x i ))\u2225 2 2 \u2212 1 T log (\u03bc \u03b8 (x i )) .\nNote that, due to the flexibility of the neural networks, this formulation is ill-posed. It may lead to cases of extreme overfitting where\u03c4 \u03d5 (x i ) \u2248 y i ,\u03c4 \u03d5 (x i ) \u2248 0, thus making L(\u03b8, \u03d5) completely unstable. Hence, we instead minimized a modified objective that included L2-regularization via\nL \u03c1,\u03b3 (\u03b8, \u03d5) := \u03c1L(\u03b8, \u03d5) + (1 \u2212 \u03c1) \u03b3 \u2225\u03b8\u2225 2 2 + (1 \u2212 \u03b3) \u2225\u03d5\u2225 2 2 ,\nwhere \u03c1, \u03b3 \u2208 (0, 1) determines the trade-off between MLE estimation, mean regularization, and precision regularization. We follow [13] and set \u03c1 = 1 \u2212 \u03b3 to reduce the hyperparameter search domain.\nSpecifically, we used two MLPs with layer normalization and ReLU activation, and trained them with gradient-based stochastic optimization. To improve stability, the first third of training was spent on exclusively training\u03bc \u03b8 (x i ) with an MSE loss. To optimize hyperparameters, we selected a configuration from 300 trials with a random number of N layers = [2,3,4] ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conditional Variational Autoencoder (cVAE)", "text": "A conditional generative latent variable model first samples-from a prior p(z)-a point z in a low-dimensional latent space, which then informs a conditional distribution p \u03b8 (y|z, x) over the target domain. This allows for a complex and flexible predictive distribution. In our case, we used feed-forward neural networks (i.e., MLPs) \u00b5 \u03b8 (z, x) and \u03c3 \u03b8 (z, x) with combined parameters \u03b8 and model:\nz \u223c N (0, I) y|z, x \u223c N \u00b5 \u03b8 (z, x), Diag(\u03c3 \u03b8 (x) 2 ) (2)\nTo fit the model to data D = {(x 1 , y 1 ), . . . , (x n , y n )}, we minimized the negative evidence lower bound (NELBO) L \u03b8 (q) that bounds the intractable negative marginal likelihood from above via\nL \u03b8 (q) := \u2212E zi\u223cq log p \u03b8 (y i , z i |x i ) q(z i |x i ) = \u2212 log p \u03b8 (y i |x i ) + KL q \u2225 p \u03b8 (z i |y i , x i ) \u22650\n, using an approximation q to the posterior p \u03b8 (z i |y i , x i ). The conditional variational autoencoder (cVAE) [14] uses amortized variational inference to optimize \u03b8 and q jointly by approximating the latter with e.g., q \u03c8 (z i ) = N g \u03c8 (x i ), Diag(h \u03c8 (x i ) 2 ) , where we again chose g \u03c8 (x i ) and h \u03c8 (x i ) to be MLPs. This allowed us to optimize for \u03b8 and \u03c8 by minimizing\nL \u03b8 (q) \u03b2=1 = E zi\u223cq \u03c8 1 2 y i \u2212 \u00b5 \u03b8 (z i , x i ) \u03c3 \u03b8 (z i , x i ) 2 2 + 1 T log (\u03c3 \u03b8 (z i , x i )) + \u03b2KL(q \u03c8 (z i ) \u2225 p(z i )) + const\nwith a Monte Carlo approximation by first sampling z i (once) from the variational encoder q \u03c8 (z i ).\nAfter which, we decoded the predictive mean and standard deviation with the variational decoder \u00b5 \u03b8 (z, x) and \u03c3 \u03b8 (z, x). We then computed NELBO as a sum of a reconstruction term and a KL term that regularizes the latent space, averaged over all samples, and back-propagated the gradients. By letting \u03b2 be a hyperparameter, we manually determined the trade-off between reconstruction quality and latent space structure. Finally, at inference time, we used Equation 2 to sample from the predictive distribution\np \u03b8 (\u0177|x) = p \u03b8 (\u0177|x, z)p(z) dz.\nFor both the variational encoder and decoder, we used an MLP with layer normalization, ReLU activation, dropout with p = 0.05, and two branching final layers that produced the mean and standard deviation, respectively. We trained both MLPs jointly-with gradient-based stochastic optimization-on the objective described above.\nTo optimize hyperparameters, we ran 300 trials with a random number of hidden layers N layers = [2, 3,4], \nN", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Encoder-Decoder (ED)", "text": "The Encoder-Decoder (ED) is an adjusted version of the ED presented in Behrens et al. (2022) [15]. We keep all tuneable hyperparameters except for the learning rate and the node sizes of input and output layer of ED fixed to the original values that were optimized with a detailed hyperparameter search for the superparameterization of the Community Atmosphere Model version 3 in an aquaplanet setup [15]. The Encoder consists of 6 hidden fully-connected layers. The Encoder decreases progressively the dimensionality of the input variables down to 5 nodes in the latent space of the network. These 5 latent nodes are the only input to the decoding part of ED. The Decoder maps the information from the latent space back to 128 nodes in the output layer through 6 progressively wider fully-connected hidden layers [15]. We train ED over 40 epochs with a learning rate step after each 7 th epoch, which reduces the learning rate by factor 5 [15]. The adjusted initial learning rate has a value of 1 \u00d7 10 \u22124 . The batch size has a value of 714 samples. As activation functions of all hidden layers we use ReLU and the output layer of the Decoder is ELU-activated [15]. As an optimizer during training we use Adam. As a loss function of ED we use a MSE loss and as additional metric the MAE during training. The following list summarizes the key hyperparameters of ED:\n\u2022  \nMAE = 1 n n i=1 |X i \u2212 y|(3)\nRoot Mean Squared Error (RMSE):\nRMSE = 1 n n i=1 (X i \u2212 y) 2(4)\nCoefficient of Determination (R 2 ):\nR 2 = 1 \u2212 n i=1 (X i \u2212 y) 2 n i=1 (X i \u2212X) 2(5)\nIn Equations 3-5, X i and y represent the true and predicted values, respectively. The mean of the true values of the dependent variable is denoted byX.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Stochastic Metric (CRPS)", "text": "The continuous ranked probability score (CRPS) is a generalization of the MAE for distributional predictions. CRPS penalizes over-confidence in addition to inaccuracy in ensemble predictions-a lower CRPS is better. For each variable, it compares the ground truth target y with the cumulative distribution function (CDF) F of the prediction via\nCRPS(F, y) := F (x) \u2212 1 {x\u2265y} 2 dx = E[|X \u2212 y|] \u2212 1 2 E[|X \u2212 X \u2032 |],\nwhere X, X \u2032 \u223c F are independent and identically distributed (iid) samples from the distributional prediction. We use the non-parametric \"fair estimate to the CRPS\" [16], estimating F with the empirical CDF of n = 32 iid samples X i \u223c F :\nCRPS(X, y) := 1 n n i=1 |X i \u2212 y| \u2212 1 2n(n \u2212 1) n i=1 n j=1 |X i \u2212 X j | (6)\nThe first term in Equation 6is the MAE between the target and samples of the predictive distribution, while the second term is small for small predictive variances, vanishing completely for point estimates.\nNote that this definition extends to ensemble models, where we take the prediction of each ensemble member as a sample of an implicit predictive distribution.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "MAE and R 2 of the baseline models are presented in the main text (e.g., Table 1 and Figure 2 in the main text). Here, we show RMSE and CRPS in Table 3 and Figures 3, 4, and 5.\nWe also present the spatial structure of the metrics. Figure 6 shows the latitude-height structure of R 2 .        The pressure levels on Y-axis are approximated values.", "publication_ref": [], "figure_ref": ["fig_0", "fig_6", "fig_10"], "table_ref": ["tab_7", "tab_16"]}, {"heading": "Fit Quality", "text": "Scatter plots of truth versus prediction are shown in this section (Figures SI9 to SI16 in SI Section 8). While many variables exhibit consistent fit quality, some show notable variability between baselines, as seen with snow precipitation rate predictions. The performance of our optimized deterministic baseline (MLP) suggests these issues are avoidable. However, note that our prediction problem has a multi-variate and multi-dimensional nature.\n5 Guidance", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Physical Constraints", "text": "Mass and energy conservation are important criteria for Earth system modeling. If these terms are not conserved, errors in estimating sea level rise or temperature change over time may become as large as the signals we hope to measure. Enforcing conservation on emulated results helps constrain results to be physically plausible and reduce the potential for errors accumulating over long time scales.\nIn the atmospheric component of the E3SM climate model, mass is composed of \"dry air\" (i.e., well-mixed gases such as molecular nitrogen and oxygen) and water vapor. During the physics parameterizations we seek to emulate, there is no lateral exchange of mass across columns of the host model, and the model assumes that the total mass in each column and level remains unchanged. Thus, while surface pressure (state_ps) is part of the state structure we seek to emulate, that surface pressure component must be held fixed. The water mass, however, is not held fixed, requiring fictitious sources and sinks of dry air, which are corrected later in the model-outside of the \"emulated\" part of the code-and is not addressed within the emulator.\nChanges in column water mass should balance the sources and sinks of water into and out of the column through surface fluxes. The surface source of water is an input to the emulator via the cam_in structure. The surface sink of water is generated by the model, and hence emulated in our case. The net surface water flux (source minus sink) should be equal to the tendency of water mass within the column (7). The mass of water is held in five separate terms within the state structure: water vapor (q v ), cloud liquid condensate (q l ), cloud ice (q i ), rain (q r ), and snow (q s ). These terms are held as ratios of their mass to the sum of dry air plus water vapor (referred to as specific humidity).\nThe \"\u03b4\" refers to the difference (after minus before computation) in each quantity owing to the CRM physics. The layer mass (sum of dry air and water vapor) of level k is equal to the pressure thickness of that layer \u2206p i (the difference between top and bottom interface pressure for level i) divided by the gravitational acceleration g (assumed constant). The timestep length is \u03b4t. In addition to conserving water mass, we required each individual water constituent to remain greater than or equal to zero in every layer within the column. In Equation 7, E is the surface source of water (evapotranspiration) and P is the surface sink of water (precipitation):\ni (\u03b4q v + \u03b4q l + \u03b4q i + \u03b4q r + \u03b4q s ) \u2206p i g\u03b4t = E \u2212 P (7)\nFor the portion of the code that we try to emulate, the water source E is not applied such that the only surface flux to account for when constraining water conservation is the precipitation flux (P , cam_out_PRECC). Unfortunately, only the input and output state variables for water vapor (state_q0001), cloud liquid (state_q0002), and cloud ice (state_q0003) are available. Additional storage terms related to precipitating water that have not exited the column over the course of a model timestep are unavailable in the current output. Therefore, we are unable to exactly enforce water conservation. Estimates show relative errors of a couple percent resulting from the lack of these precipitation mixing ratios. We can still require that the relative error be small. To accomplish this, we compared the \"expected\" total water, based on the combination of the input and surface fluxes, to the predicted total water. In the equations below, superscript o denotes output and superscript i denotes input:\nTotal Water (Actual) = i (\u03b4q o v + \u03b4q o l + \u03b4q o i ) \u2206p i g Total Water (Expected) = i \u03b4q i v + \u03b4q i l + \u03b4q i i \u2206p i g \u2212 P \u03b4t\nRelative Error = Total Water (Expected) \u2212 Total Water (Actual) Total Water (Actual)\nWe required the model to keep the relative error small (e.g., below 5%). Anything further is beyond the limit of the current data.\nLike mass conservation, energy conservation can generally be enforced by requiring that the total change within the column is exactly balanced by the fluxes into and out of that column. Because the emulator does not predict upwelling radiative fluxes at the model top (a sink term for energy), we do not have the boundary conditions necessary to constrain column energy tendencies. However, we still required certain criteria be met for physical consistency. First, the downwelling surface shortwave radiative flux cannot exceed the downwelling shortwave flux at the model top (prescribed input pbuf_SOLIN). Likewise, the net surface shortwave flux should also be bounded between zero (100% reflection) and the surface downwelling shortwave flux (100% absorption). Additionally, the downwelling longwave flux should not exceed the blackbody radiative flux from the warmest temperature in the column.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Unit Conversion and Weighting for Interpretable Evaluation", "text": "To facilitate the objective evaluation of the model's prediction, we provided a weight tensor of shape (d o , N x ) to convert raw outputs to area-weighted outputs with consistent energy flux units [W/m 2 ]. More details are given below.\nTo ensure that our evaluation takes the Earth's spherical geometry into account, we designed an area weighting factor a that depends on the horizontal position x:\na (x) = A col (x) / \u27e8A col \u27e9 x\nwhere A col is the area of an atmospheric column and \u27e8A col \u27e9 x the horizontal average of all atmospheric columns' areas. This formula gives more weight to outputs if their grid cell has a larger horizontal area. To ensure that our evaluation is physically-consistent, we convert all predicted variables to energy flux units W/m 2 (power per unit area). This has to be done for each variable separately.\n\u2022 For heating tendencies\u1e6a [K/s], which depends on the horizontal position x and vertical level lev, this was be done using the specific heat capacity constant at constant pressure c p [J/ (K \u00d7 kg)], where \u2206p i [Pa] is the layer's pressure thickness, calculated as the difference between the pressure at the layer's top and bottom interfaces:\nT W/m 2 = c p g \u00d7 a (x) \u00d7 \u2206p i (lev) \u00d7\u1e6a [K/s]\n\u2022 For water concentration tendenciesq s \u22121 , which also depends on x and lev, this was be done using the latent heat of vaporization of water vapor at constant pressure L v [J/kg]:\nq W/m 2 = L v g \u00d7 a (x) \u00d7 \u2206p i (lev) \u00d7q s \u22121\nNote that there is some level of arbitrariness, as the exact latent heat depends on which water phase is assumed to calculate the energy transfer. Here, we chose to weigh all phases using L v to give them comparable weights in the evaluation metrics.\n\u2022 For momentum tendenciesu m/s 2 , which also depend on x and lev, we used a characteristic wind magnitude |U | [m/s] to convert these tendencies into turbulent kinetic energy fluxes, in units W/m 2 , making them comparable to\u1e6a W/m 2 andq W/m 2 :\nu W/m 2 = |U | g \u00d7 a (x) \u00d7 \u2206p i (lev) \u00d7u m/s\nNote that there is some level of arbitrariness in the choice of |U | [m/s], which could e.g., be chosen so that the variances ofu W/m 2 and\u1e6a W/m 2 are comparable.\n\u2022 Precipitation rate variables P [m/s] were also be converted to energy fluxes using L v and the density of liquid water \u03c1 w kg/m 3 (or the density of snow/ice for solid precipitation), though they do not require vertical integration:\nP W/m 2 = L v \u00d7 \u03c1 w \u00d7 a (x) \u00d7 P [m/s]\n\u2022 Finally, surface energy fluxes F W/m 2 were simply multiplied by a (x) to account for area-weighting.\nNote that while these choices ensured unit consistency, facilitating the physical interpretation of our evaluation metrics, we recommend tailoring the exact choice of physical constants to the application of interest.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Additional Guidance", "text": "Stochasticity and Memory: The results of the embedded convection calculations regulating d o come from a chaotic dynamical system and thus could be worthy of architectures and metrics beyond the deterministic baselines in this paper. These solutions are likewise sensitive to sub-grid initial state variables from an interior nested spatial dimension that have not been included in our data.\nTemporal Locality: Incorporating the previous timesteps' target or feature in the input vector inflation could be beneficial as it captures some information about this convective memory and utilizes temporal autocorrelations present in atmospheric data.\nCausal Pruning: A systematic and quantitative pruning of the input vector based on objectively assessed causal relationships to subsets of the target vector has been proposed as an attractive preprocessing strategy, as it helps remove spurious correlations due to confounding variables and optimize the machine learning (ML) algorithm [17].\nNormalization: Normalization that goes beyond removing vertical structure could be strategic, such as removing the geographical mean (e.g., latitudinal, land/sea structure) or composite seasonal variances (e.g., local smoothed annual cycle) present in the data. For variables exhibiting exponential variation and approaching zero at the highest level (e.g., metrics of moisture), log-normalization might be beneficial.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Other Related Work", "text": "Several benchmark datasets have been developed to facilitate AI tasks in weather and climate. ClimateNet [18] and Extremeweather [19] were both designed for AI-based feature detection of extreme weather events in forecasts of Earth's future climate made using conventional climate models.\nWeatherBench [20] provides data specifically designed for data-driven weather forecasting, focusing on periods ranging from 3 to 5 days into the future. PDEBench [21] provides data from numerical simulations of several partial differential equations (PDEs) for benchmarking AI PDE emulators. ClimateBench [22] was designed for emulators that produce annual mean global predictions of temperature and precipitation given greenhouse gas concentrations and emissions. ClimART [23] was designed for the development of radiative energy transfer parameterization emulators for use in weather and climate modeling. These benchmark datasets play a vital role in advancing AI and ML research within the weather and climate domains.\nClimSim, a dataset for parameterization emulators trained on high-resolution data from small-scale embedded models, is unique compared to other benchmark datasets designed for emulators in climate simulation (ClimateBench, ClimART, and PDEBench). While PDEBench provides data for developing AI emulators of the same PDEs commonly used in climate simulation, ClimSim is uniquely tailored to address the challenging task of replacing a sophisticated parameterization for the combined effects of clouds, rain, radiation, and storms. Specifically, models trained using ClimSim will learn to emulate the nonlinear effect of clouds, rain, and storms resolved on the 1 km (20 s) space (time) scale, which is a collection of hundreds of equations rather than one, to represent their upscale impacts on the 100 km (30 min) scale. Hybrid simulation is also the goal of ClimART, which is designed specifically for the narrower and less computationally costly task of radiative energy transfer parameterization, rather than cloud and rain emulators. ClimateBench, on the other hand, is not an attempt at hybrid simulation, but rather for \"whole-model\" emulators that reproduce the annual mean global predictions of climate that a conventional climate model would simulate given unseen greenhouse gas concentrations and emissions. This does not attempt to sidestep Moore's Law or admit previously unattainable resolution, i.e., any error or bias related to the parameterizations used to create the training data are part of what is learned by the emulator.\nIn contrast, the goal of ClimSim is to develop an emulator for the explicitly resolved effect of clouds and storms on climate, so that, down the road, the emulator can be used to replace parameterizations in a climate model, enabling more realistic climate simulation without the typical computational overhead. ClimSim builds off work by a few climate scientists who have been exploring since 2017 to apply ML for hybrid multi-scale climate modeling. [24] first demonstrated that using simple ML models, and a simple atmosphere test-bed, certain atmospheric patterns of convective heating and moistening could be effectively predicted, particularly in the tropics and mid-latitude storm tracks. However, when these models were integrated into broader climate simulations, except for lucky fits that demonstrated the exciting potential for success [25], issues related to stability arose, a common problem when constructing hybrid climate models. Various methods were tried to improve the stability, such as coupling multiple models together and searching for better model architectures [26,27]. These efforts led to improved error rates in the predictions. More recently, researchers have expanded this work into real-world settings, using more advanced ML architectures [28][29][30]. Wang et al. (2022) [31] even managed to create a deep-learning model that showed hybrid stability over a decade under real-world conditions. While this hybrid model had a few biases, it was successful in capturing some aspects of climate variability. Additionally, work has been done to compress input data to avoid causal confounders while maintaining accuracy [17], use latent representations that account for stochasticity [15], and enforce physical constraints within these models [32], all of which could potentially improve their reliability. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "1.", "text": "For what purpose was the dataset created? Our benchmark dataset was created to serve as a foundation for developing robust frameworks that emulate parameterizations for cloud and extreme rainfall physics and their interaction with other sub-resolution processes.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "2.", "text": "Who created the dataset and on behalf of which entity? The dataset was developed by a consortium of climate scientists and ML researchers listed in the author list. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Distribution", "text": "1. Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? Yes, the dataset is open to the public.\n2. How will the dataset will be distributed (e.g., tarball on website, API, GitHub)? The dataset will be distributed through Hugging Face and the code used for developing baseline models through GitHub.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "3.", "text": "Have any third parties imposed IP-based or other restrictions on the data associated with the instances? No.\n4. Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? No.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Maintenance", "text": "1. Who will be supporting/hosting/maintaining the dataset? NSF-STC LEAP will support, host, and maintain the dataset.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "2.", "text": "How can the owner/curator/manager of the dataset be contacted (e.g., email address)?\nThe owner/curator/manager(s) of the dataset can be contacted through following emails: Sungduk Yu (sungduk@uci.edu), Michael S. Pritchard (mspritch@uci.edu) and LEAP (leap@columbia.edu).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "3.", "text": "Is there an erratum? No. If errors are found in the future, we will release errata on the main web page for the dataset (https://leap-stc.github.io/ClimSim).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "4.", "text": "Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? Yes, the datasets will be updated whenever necessary to ensure accuracy, and announcements will be made accordingly. These updates will be posted on the main web page for the dataset (https://leap-stc.github.io/ClimSim).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "5.", "text": "If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were the individuals in question told that their data would be retained for a fixed period of time and then deleted?) N/A 6. Will older version of the dataset continue to be supported/hosted/maintained? Yes, older versions of the dataset will continue to be maintained and hosted.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "7.", "text": "If others want to extend/augment/build on/contribute to the dataset, is there a mechanisms for them to do so? No.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "5.", "text": "Did you collect the data from the individuals in questions directly, or obtain it via third parties or other sources (e.g., websites)? We obtained the dataset from computer simulations of Earth's climate.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Uses", "text": "1. Has the dataset been used for any tasks already? No, this dataset has not been used for any tasks yet.\n2. What (other) tasks could be the dataset be used for? Please refer to Section 5 in the main manuscript for other applications.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "3.", "text": "Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? The current composition of the datasets are self-sufficient to build a climate emulator. However, it misses some extra variables, which are not essential for such climate emulators but necessary to strictly enforce physical constraints (see Section 4.5 of the main text). We plan to include these extra variables in the next release. Any changes in the next release and update to user guidelines will be documented and shared through the dataset webpage (https://leap-stc.github.io/ClimSim).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "4.", "text": "Are there tasks for which the dataset should not be used? No.   SI1.         ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Acknowledgements", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Counts", "text": "Heating tendency, T/ t (level=17)    ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Counts", "text": "Moistening tendency, q/ t (level=17)\nFigure 15: Hexagonally-binned representation of 3D (vertically-resolved) target variables comparing the climate model simulation (\"true\"; x-axis) with the ML model prediction (\"predicted\"; y-axis) at four different vertical levels. The color of each hexagonal bin corresponds to the number of data points enclosed.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Climate Change 2021: The Physical Science Basis. Contribution of Working Group I to the Sixth Assessment Report of the Intergovernmental Panel on Climate Change", "journal": "IPCC", "year": "2021", "authors": ""}, {"ref_id": "b1", "title": "An assessment of earth's climate sensitivity using multiple lines of evidence", "journal": "Rev. Geophys", "year": "2020", "authors": "S Sherwood; M J Webb; J D Annan; K C Armour; P M Forster; J C Hargreaves; G Hegerl; S A Klein; K D Marvel; E J Rohling"}, {"ref_id": "b2", "title": "Climate goals and computing the future of clouds", "journal": "Nat. Clim. Change", "year": "2017", "authors": "T Schneider; J Teixeira; C S Bretherton; F Brient; K G Pressel; C Sch\u00e4r; A P Siebesma"}, {"ref_id": "b3", "title": "Could machine learning break the convection parameterization deadlock?", "journal": "Geophys. Res. Lett", "year": "2018", "authors": "P Gentine; M Pritchard; S Rasp; G Reinaudi; G Yacalis"}, {"ref_id": "b4", "title": "Reflections and projections on a decade of climate science", "journal": "Nat. Clim. Change", "year": "2021", "authors": "V Eyring; V Mishra; G P Griffith; L Chen; T Keenan; M R Turetsky; S Brown; F Jotzo; F C Moore; S Van Der Linden"}, {"ref_id": "b5", "title": "Correcting coarse-grid weather and climate models by machine learning from global storm-resolving simulations", "journal": "J. Adv. Model. Earth Syst", "year": "2022", "authors": "C S Bretherton; B Henn; A Kwa; N D Brenowitz; O Watt-Meyer; J Mcgibbon; W A Perkins; S K Clark; L Harris"}, {"ref_id": "b6", "title": "Correcting a 200 km resolution climate model in multiple climates by machine learning from 25 km resolution simulations", "journal": "Journal of Advances in Modeling Earth Systems", "year": "2022", "authors": "S K Clark; N D Brenowitz; B Henn; A Kwa; J Mcgibbon; W A Perkins; O Watt-Meyer; C S Bretherton; L M Harris"}, {"ref_id": "b7", "title": "Machine-learned climate model corrections from a global storm-resolving model: Performance across the annual cycle", "journal": "J. Adv. Model. Earth Syst", "year": "2023", "authors": "A Kwa; S K Clark; B Henn; N D Brenowitz; J Mcgibbon; O Watt-Meyer; W A Perkins; L Harris; C S Bretherton"}, {"ref_id": "b8", "title": "Improving the reliability of ml-corrected climate models with novelty detection", "journal": "", "year": "2023", "authors": "C H Sanford; A Kwa; O Watt-Meyer; S K Clark; N D Brenowitz; J Mcgibbon; C S Bretherton"}, {"ref_id": "b9", "title": "Deep learning to represent subgrid processes in climate models", "journal": "", "year": "2018", "authors": "S Rasp; M S Pritchard; P Gentine"}, {"ref_id": "b10", "title": "Interpreting and stabilizing machine-learning parameterizations of convection", "journal": "J. Atmos. Sci", "year": "2020", "authors": "N D Brenowitz; T Beucler; M Pritchard; C S Bretherton"}, {"ref_id": "b11", "title": "A moist physics parameterization based on deep learning", "journal": "J. Adv. Model. Earth Syst", "year": "2020", "authors": "Y Han; G J Zhang; X Huang; Y Wang"}, {"ref_id": "b12", "title": "A fortran-keras deep learning bridge for scientific computing", "journal": "", "year": "2020", "authors": "J Ott; M Pritchard; N Best; E Linstead; M Curcic; P Baldi"}, {"ref_id": "b13", "title": "Assessing the potential of deep learning for emulating cloud superparameterization in climate models with realgeography boundary conditions", "journal": "J. Adv. Model. Earth Syst", "year": "2021", "authors": "G Mooers; M Pritchard; T Beucler; J Ott; G Yacalis; P Baldi; P Gentine"}, {"ref_id": "b14", "title": "Stable climate simulations using a realistic general circulation model with neural network parameterizations for atmospheric moist physics and radiation processes", "journal": "Geosci. Model Dev", "year": "2022", "authors": "X Wang; Y Han; W Xue; G Yang; G J Zhang"}, {"ref_id": "b15", "title": "Causally-informed deep learning to improve climate models and projections", "journal": "", "year": "2023", "authors": "F Iglesias-Suarez; P Gentine; B Solino-Fernandez; T Beucler; M Pritchard; J Runge; V Eyring"}, {"ref_id": "b16", "title": "Stable machine-learning parameterization of subgrid processes for climate modeling at a range of resolutions", "journal": "Nature Comm", "year": "2020", "authors": "J Yuval; P A O'gorman"}, {"ref_id": "b17", "title": "Use of neural networks for stable, accurate and physically consistent parameterization of subgrid atmospheric processes with good performance at reduced precision", "journal": "Geophys. Res. Lett", "year": "2021", "authors": "J Yuval; P A O'gorman; C N Hill"}, {"ref_id": "b18", "title": "Coupled online learning as a way to tackle instabilities and biases in neural network parameterizations: general algorithms and lorenz 96 case study (v1. 0)", "journal": "Geosci. Model Dev", "year": "2020", "authors": "S Rasp"}, {"ref_id": "b19", "title": "Atmospheric convection", "journal": "", "year": "1994", "authors": "K A Emanuel"}, {"ref_id": "b20", "title": "Atmosphere, clouds, and climate", "journal": "", "year": "2012", "authors": "D Randall"}, {"ref_id": "b21", "title": "Clouds and climate: Climate science's greatest challenge", "journal": "", "year": "2020", "authors": "A P Siebesma; S Bony; C Jakob; B Stevens"}, {"ref_id": "b22", "title": "Influence of a stochastic moist convective parameterization on tropical climate variability", "journal": "Geophys. Res. Lett", "year": "2000", "authors": "J W ; -B Lin; J D Neelin"}, {"ref_id": "b23", "title": "Rethinking convective quasiequilibrium: observational constraints for stochastic convective schemes in climate models", "journal": "Phil. Trans. Royal Soc. A", "year": "2008", "authors": "J D Neelin; O Peters; J W B Lin; K Hales; C E Holloway"}, {"ref_id": "b24", "title": "Crcp: A cloud resolving convection parameterization for modeling the tropical convecting atmosphere", "journal": "Phys. D: Nonlinear Phenom", "year": "1999", "authors": "W W Grabowski; P K Smolarkiewicz"}, {"ref_id": "b25", "title": "Structure of the madden-julian oscillation in the superparameterized cam", "journal": "J. Atmos. Sci", "year": "2009", "authors": "J J Benedict; D A Randall"}, {"ref_id": "b26", "title": "Beyond deadlock", "journal": "Geophys. Res. Lett", "year": "2013", "authors": "D A Randall"}, {"ref_id": "b27", "title": "Initial results from the super-parameterized e3sm", "journal": "Journal of Advances in Modeling Earth Systems", "year": "2020", "authors": "W M Hannah; C R Jones; B R Hillman; M R Norman; D C Bader; M A Taylor; L Leung; M S Pritchard; M D Branson; G Lin"}, {"ref_id": "b28", "title": "Unprecedented cloud resolution in a gpu-enabled full-physics atmospheric climate simulation on olcf's summit supercomputer", "journal": "Int. J. High Perform. Compu. Appl", "year": "2022", "authors": "M R Norman; D C Bader; C Eldred; W M Hannah; B R Hillman; C R Jones; J M Lee; L Leung; I Lyngaas; K G Pressel"}, {"ref_id": "b29", "title": "Breaking the cloud parameterization deadlock", "journal": "Bull. Am. Meteorol. Soc", "year": "2003", "authors": "D Randall; M Khairoutdinov; A Arakawa; W Grabowski"}, {"ref_id": "b30", "title": "Evaluation of the simulated interannual and subseasonal variability in an amip-style simulation using the csu multiscale modeling framework", "journal": "J. Clim", "year": "2008", "authors": "M Khairoutdinov; C Demott; D Randall"}, {"ref_id": "b31", "title": "Testing the clausius -clapeyron constraint on changes in extreme precipitation under co2 warming", "journal": "Clim. Dyn", "year": "2007", "authors": "P Pall; M R Allen; D A Stone"}, {"ref_id": "b32", "title": "Detection of continental-scale intensification of hourly rainfall extremes", "journal": "Nat. Clim. Change", "year": "2018", "authors": "S B Guerreiro; H J Fowler; R Barbero; S Westra; G Lenderink; S Blenkinsop; E Lewis; X F Li"}, {"ref_id": "b33", "title": "Precipitation extremes and water vapor: Relationships in current climate and implications for climate change", "journal": "Current Clim. Change Rep", "year": "2022", "authors": "J D Neelin; C Martinez-Villalobos; S N Stechmann; F Ahmed; G Chen; J M Norris; Y.-H Kuo; G Lenderink"}, {"ref_id": "b34", "title": "Contribution of historical precipitation change to us flood damages", "journal": "", "year": "2021", "authors": "F V Davenport; M Burke; N S Diffenbaugh"}, {"ref_id": "b35", "title": "Two modes of change of the distribution of rain", "journal": "J. Clim", "year": "2014", "authors": "A G Pendergrass; D L Hartmann"}, {"ref_id": "b36", "title": "Regionally high risk increase for precipitation extreme events under global warming", "journal": "Sci. Rep", "year": "2023", "authors": "C Martinez-Villalobos; J D Neelin"}, {"ref_id": "b37", "title": "Systematic sampling and validation of machine Learning-Parameterizations in climate models", "journal": "", "year": "2023-09", "authors": "J Lin; S Yu; T Beucler; P Gentine; D Walling; M Pritchard"}, {"ref_id": "b38", "title": "Non-linear dimensionality reduction with a variational encoder decoder to understand convective processes in climate models", "journal": "J. Adv. Model. Earth Syst", "year": "2022", "authors": "G Behrens; T Beucler; P Gentine; F Iglesias-Suarez; M Pritchard; V Eyring"}, {"ref_id": "b39", "title": "Enforcing analytic constraints in neural networks emulating physical systems", "journal": "Phys. Rev. Lett", "year": "2021", "authors": "T Beucler; M Pritchard; S Rasp; J Ott; P Baldi; P Gentine"}, {"ref_id": "b40", "title": "Scale-mae: A scale-aware masked autoencoder for multiscale geospatial representation learning", "journal": "", "year": "2023", "authors": "C J Reed; R Gupta; S Li; S Brockman; C Funk; B Clipp; K Keutzer; S Candido; M Uyttendaele; T Darrell"}, {"ref_id": "b41", "title": "Fourcastnet: A global data-driven high-resolution weather model using adaptive fourier neural operators", "journal": "", "year": "2022", "authors": "J Pathak; S Subramanian; P Harrington; S Raja; A Chattopadhyay; M Mardani; T Kurth; D Hall; Z Li; K Azizzadenesheli; P Hassanzadeh; K Kashinath; A Anandkumar"}, {"ref_id": "b42", "title": "Spherical fourier neural operators: Learning stable dynamics on the sphere", "journal": "", "year": "2023", "authors": "B Bonev; T Kurth; C Hundt; J Pathak; M Baust; K Kashinath; A Anandkumar"}, {"ref_id": "b43", "title": "Graphcast: Learning skillful medium-range global weather forecasting", "journal": "", "year": "2022", "authors": "R Lam; A Sanchez-Gonzalez; M Willson; P Wirnsberger; M Fortunato; A Pritzel; S Ravuri; T Ewalds; F Alet; Z Eaton-Rosen; W Hu; A Merose; S Hoyer; G Holland; J Stott; O Vinyals; S Mohamed; P Battaglia"}, {"ref_id": "b44", "title": "Soil moisture-atmosphere interactions during the 2003 european summer heat wave", "journal": "J. Clim", "year": "2007", "authors": "E M Fischer; S I Seneviratne; P L Vidale; D L\u00fcthi; C Sch\u00e4r"}, {"ref_id": "b45", "title": "Investigating soil moisture-climate interactions in a changing climate: A review", "journal": "Earth-Sci. Rev", "year": "2010", "authors": "S I Seneviratne; T Corti; E L Davin; M Hirschi; E B Jaeger; I Lehner; B Orlowsky; A J Teuling"}, {"ref_id": "b46", "title": "Non-local parameterization of atmospheric subgrid processes with neural networks", "journal": "J. Adv. Model. Earth Syst", "year": "2022", "authors": "P Wang; J Yuval; P A O'gorman"}, {"ref_id": "b47", "title": "Multiscale neural operator: Learning fast and grid-independent pde solvers", "journal": "", "year": "2022", "authors": "B L\u00fctjens; C H Crawford; C D Watson; C Hill; D Newman"}, {"ref_id": "b48", "title": "Checkerboard patterns in e3smv2 and e3sm-mmfv2", "journal": "Geosci. Model Dev", "year": "2022", "authors": "W M Hannah; K G Pressel; M Ovchinnikov; G S Elsaesser"}, {"ref_id": "b49", "title": "Separating physics and dynamics grids for improved computational efficiency in spectral element earth system models", "journal": "J. Adv. Model. Earth Syst", "year": "2021", "authors": "W M Hannah; A M Bradley; O Guba; Q Tang; J.-C Golaz; J Wolfe"}, {"ref_id": "b50", "title": "Climart: A benchmark dataset for emulating atmospheric radiative transfer in weather and climate models", "journal": "", "year": "2021", "authors": "S R Cachay; V Ramesh; J N S Cole; H Barker; D Rolnick"}, {"ref_id": "b51", "title": "Understanding pathologies of deep heteroskedastic regression", "journal": "", "year": "2023", "authors": "E Wong-Toi; A Boyd; V Fortuin; S Mandt"}, {"ref_id": "b52", "title": "Randomized prior functions for deep reinforcement learning", "journal": "", "year": "2018", "authors": "I Osband; J Aslanides; A Cassirer"}, {"ref_id": "b53", "title": "Scalable uncertainty quantification for deep operator networks using randomized priors", "journal": "Comput. Methods Appl. Mech. Eng", "year": "2022", "authors": "Y Yang; G Kissas; P Perdikaris"}, {"ref_id": "b54", "title": "Scalable bayesian optimization with high-dimensional outputs using randomized prior networks", "journal": "", "year": "2023", "authors": "M A Bhouri; M Joly; R Yu; S Sarkar; P Perdikaris"}, {"ref_id": "b55", "title": "Climate-invariant machine learning", "journal": "", "year": "2021", "authors": "T Beucler; M Pritchard; J Yuval; A Gupta; L Peng; S Rasp; F Ahmed; P A O'gorman; J D Neelin; N J Lutsko; P Gentine"}, {"ref_id": "b56", "title": "Fourier neural operator for parametric partial differential equations", "journal": "", "year": "2021", "authors": "Z Li; N Kovachki; K Azizzadenesheli; B Liu; K Bhattacharya; A Stuart; A Anandkumar"}, {"ref_id": "b57", "title": "Physics-informed neural operator for learning partial differential equations", "journal": "", "year": "2023", "authors": "Z Li; H Zheng; N Kovachki; D Jin; H Chen; B Liu; K Azizzadenesheli; A Anandkumar"}, {"ref_id": "b58", "title": "Reynolds averaged turbulence modelling using deep neural networks with embedded invariance", "journal": "J. Fluid Mech", "year": "2016", "authors": "J Ling; A Kurzawski; J Templeton"}, {"ref_id": "b59", "title": "Embedded training of neural-network subgrid-scale turbulence models", "journal": "Phys. Rev. Fluids", "year": "2021", "authors": "J F Macart; J Sirignano; J B Freund"}, {"ref_id": "b60", "title": "Generalization capability of convolutional neural networks for progress variable variance and reaction rate subgrid-scale modeling", "journal": "", "year": "2021", "authors": "V Xing; C Lapeyre; T Jaravel; T Poinsot"}, {"ref_id": "b61", "title": "Perspective on machine learning for advancing fluid mechanics", "journal": "Phys. Rev. Fluids", "year": "2019", "authors": "M P Brenner; J D Eldredge; J B Freund"}, {"ref_id": "b62", "title": "Turbulence enrichment using physics-informed generative adversarial networks", "journal": "", "year": "2020", "authors": "A Subramaniam; M L Wong; R D Borker; S Nimmagadda; S K Lele"}, {"ref_id": "b63", "title": "Deep fluids: A generative network for parameterized fluid simulations", "journal": "Comput. Graph. Forum", "year": "2019", "authors": "B Kim; V C Azevedo; N Thuerey; T Kim; M Gross; B Solenthaler"}, {"ref_id": "b64", "title": "Toward stochastic moist convective parameterization in general circulation models", "journal": "Geophys. Res. Lett", "year": "2003", "authors": "J W ; -B Lin; J D Neelin"}, {"ref_id": "b65", "title": "Precipitation extremes under climate change", "journal": "Current Clim. Change Rep", "year": "2015", "authors": "P A O'gorman"}, {"ref_id": "b66", "title": "Data-driven equation discovery of ocean mesoscale closures", "journal": "Geophys. Res. Lett", "year": "2020", "authors": "L Zanna; T Bolton"}, {"ref_id": "b67", "title": "Data-driven equation discovery of a cloud cover parameterization", "journal": "", "year": "2023", "authors": "A Grundner; T Beucler; P Gentine; V Eyring"}, {"ref_id": "b68", "title": "A vision for the development of benchmarks to bridge geoscience and data science", "journal": "", "year": "2017", "authors": "I Ebert-Uphoff; D R Thompson; I Demir; Y R Gel; A Karpatne; M Guereque; V Kumar; E Cabral-Cano; P Smyth"}, {"ref_id": "b69", "title": "Pierre Gentine 3 , Stephan Mandt 1", "journal": "Savannah L. Ferretti", "year": "", "authors": ""}, {"ref_id": "b70", "title": "3 Columbia, 4 UCB, 5 MIT, 6 DLR, 7 Princeton, 8 UNIL, 9 PNNL, 10 SNL, 11 OSU, 12 NVIDIA, 13 UCSD, 14 NYU, 15 UCLA, 16 CSU, 17 Allen AI, 18 Tsinghua", "journal": "", "year": "", "authors": " Uci"}, {"ref_id": "b71", "title": "What do the instance that comprise the dataset represent (e.g., documents, photos, people, countries?) Each instance includes both input and output vector pairs. These inputs and outputs are instantaneous snapshots of atmospheric states surrounding detailed numerical calculations to be emulated", "journal": "", "year": "", "authors": ""}, {"ref_id": "b72", "title": "How many instances are there in total (of each type, if appropriate)? The high-resolution dataset (ClimSim_high-res) includes 5,676,480,000 instances, and each low-resolution dataset (ClimSim_low-res and ClimSim_low-res_aqua-planet) includes 100,915", "journal": "", "year": "", "authors": ""}, {"ref_id": "b73", "title": "Does the dataset contain all possible instances or is it a sample of instances from a larger set? The datasets contain 80% of all possible instances. The rest 20% are reserved as the holdout test set, which will be released once enough models using ClimSim are developed by independent groups", "journal": "", "year": "", "authors": ""}, {"ref_id": "b74", "title": "Is there a label or target associated with each instance? Yes, each instance includes both input and target (prediction) variables", "journal": "", "year": "", "authors": ""}, {"ref_id": "b75", "title": "We have a hard split between the training/validation set and the test set. The first 8 simulation years-worth dataset is reserved for the training/validation set, and the last 2 simulation yearsworth dataset is reserved for the test set. However, we do not have specific recommendations on the split within the training/validation set", "journal": "", "year": "", "authors": ""}, {"ref_id": "b76", "title": "Are there any errors, sources of noise, or redundancies in the dataset? There is one redundancy", "journal": "", "year": "", "authors": ""}, {"ref_id": "b77", "title": "Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets", "journal": "", "year": "", "authors": ""}, {"ref_id": "b78", "title": "Does the dataset contain data that might be considered confidential", "journal": "", "year": "", "authors": ""}, {"ref_id": "b79", "title": "Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety", "journal": "", "year": "", "authors": ""}, {"ref_id": "b80", "title": "Collection Process 1. How was the data associated with each instance acquired? The data associated with each instance is acquired from a series of simulations of a global climate model called E3SM-MMF", "journal": "", "year": "", "authors": ""}, {"ref_id": "b81", "title": "What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)? We used many NVIDIA A100 GPU nodes in a high-performance computing cluster called Perlmutter", "journal": "", "year": "", "authors": ""}, {"ref_id": "b82", "title": "students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)? Regular employees (e.g., scientists and postdocs) at UC Irvine, LLNL, and SNL were involved in the data collection process", "journal": "", "year": "", "authors": ""}, {"ref_id": "b83", "title": "Does the dataset relate to people", "journal": "", "year": "", "authors": ""}, {"ref_id": "b84", "title": "Energy exascale earth system model v2.1.0", "journal": "", "year": "2023", "authors": "D Project"}, {"ref_id": "b85", "title": "Robust effects of cloud superparameterization on simulated daily rainfall intensity statistics across multiple versions of the community earth system model", "journal": "J. Adv. Model. Earth Syst", "year": "2016", "authors": "G J Kooperman; M S Pritchard; M A Burt; M D Branson; D A Randall"}, {"ref_id": "b86", "title": "Separating physics and dynamics grids for improved computational efficiency in spectral element earth system models", "journal": "J. Adv. Model. Earth Syst", "year": "2021", "authors": "W M Hannah; A M Bradley; O Guba; Q Tang; J.-C Golaz; J Wolfe"}, {"ref_id": "b87", "title": "Cloud resolving modeling of the arm summer 1997 iop: Model formulation, results, uncertainties, and sensitivities", "journal": "J. Atmos. Sci", "year": "2003", "authors": "M Khairoutdinov; D Randall"}, {"ref_id": "b88", "title": "A strategy for representing the effects of convective momentum transport in multiscale models: Evaluation using a new superparameterized version of the weather research and forecast model (sp-wrf)", "journal": "J. Adv. Model. Earth Syst", "year": "2015", "authors": "S N Tulich"}, {"ref_id": "b89", "title": "Unprecedented cloud resolution in a gpu-enabled full-physics atmospheric climate simulation on olcf's summit supercomputer", "journal": "Int. J. High Perform. Compu. Appl", "year": "2022", "authors": "M R Norman; D C Bader; C Eldred; W M Hannah; B R Hillman; C R Jones; J M Lee; L Leung; I Lyngaas; K G Pressel"}, {"ref_id": "b90", "title": "Kerastuner", "journal": "", "year": "2019", "authors": "T O'malley; E Bursztein; J Long; F Chollet; H Jin; L Invernizzi"}, {"ref_id": "b91", "title": "Two-step hyperparameter optimization method: Accelerating hyperparameter search by using a fraction of a training dataset", "journal": "Artif. Intell. Earth Sys", "year": "2023", "authors": "S Yu; M Pritchard; P.-L Ma; B Singh; S Silva"}, {"ref_id": "b92", "title": "Randomized prior functions for deep reinforcement learning", "journal": "", "year": "2018", "authors": "I Osband; J Aslanides; A Cassirer"}, {"ref_id": "b93", "title": "Scalable uncertainty quantification for deep operator networks using randomized priors", "journal": "Comput. Methods Appl. Mech. Eng", "year": "2022", "authors": "Y Yang; G Kissas; P Perdikaris"}, {"ref_id": "b94", "title": "Scalable bayesian optimization with high-dimensional outputs using randomized prior networks", "journal": "", "year": "2023", "authors": "M A Bhouri; M Joly; R Yu; S Sarkar; P Perdikaris"}, {"ref_id": "b95", "title": "Hyperband: A novel bandit-based approach to hyperparameter optimization", "journal": "", "year": "2018", "authors": "L Li; K Jamieson; G Desalvo; A Rostamizadeh; A Talwalkar"}, {"ref_id": "b96", "title": "Understanding pathologies of deep heteroskedastic regression", "journal": "", "year": "2023", "authors": "E Wong-Toi; A Boyd; V Fortuin; S Mandt"}, {"ref_id": "b97", "title": "Auto-encoding variational bayes", "journal": "", "year": "2014", "authors": "D P Kingma; M Welling"}, {"ref_id": "b98", "title": "Non-linear dimensionality reduction with a variational encoder decoder to understand convective processes in climate models", "journal": "J. Adv. Model. Earth Syst", "year": "2022", "authors": "G Behrens; T Beucler; P Gentine; F Iglesias-Suarez; M Pritchard; V Eyring"}, {"ref_id": "b99", "title": "Fair scores for ensemble forecasts", "journal": "Q. J. R. Meteorol. Soc", "year": "2014", "authors": "C A T Ferro"}, {"ref_id": "b100", "title": "Causally-informed deep learning to improve climate models and projections", "journal": "", "year": "2023", "authors": "F Iglesias-Suarez; P Gentine; B Solino-Fernandez; T Beucler; M Pritchard; J Runge; V Eyring"}, {"ref_id": "b101", "title": "Climatenet: an expert-labeled open dataset and deep learning architecture for enabling high-precision analyses of extreme weather", "journal": "Geosci. Model Dev", "year": "2021", "authors": "D Prabhat; K Kashinath; M Mudigonda; S Kim; L Kapp-Schwoerer; A Graubner; E Karaismailoglu; L Kleist; T Kurth; A Greiner; A Mahesh; K Yang; C Lewis; J Chen; A Lou; S Chandran; B Toms; W Chapman; K Dagon; C A Shields; T O'brien; M Wehner; W Collins"}, {"ref_id": "b102", "title": "Extremeweather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events", "journal": "", "year": "2017", "authors": "E Racah; C Beckham; T Maharaj; S E Kahou; C Prabhat;  Pal"}, {"ref_id": "b103", "title": "Weatherbench: A benchmark data set for data-driven weather forecasting", "journal": "J. Adv. Model. Earth Syst", "year": "2020", "authors": "S Rasp; P D Dueben; S Scher; J A Weyn; S Mouatadid; N Thuerey"}, {"ref_id": "b104", "title": "Pdebench: An extensive benchmark for scientific machine learning", "journal": "", "year": "2023", "authors": "M Takamoto; T Praditia; R Leiteritz; D Mackinlay; F Alesiani; D Pfl\u00fcger; M Niepert"}, {"ref_id": "b105", "title": "Climatebench v1.0: A benchmark for data-driven climate projections", "journal": "J. Adv. Model. Earth Syst", "year": "2022", "authors": "D Watson-Parris; Y Rao; D Olivi\u00e9; \u00d8 Seland; P Nowack; G Camps-Valls; P Stier; S Bouabid; M Dewey; E Fons; J Gonzalez; P Harder; K Jeggle; J Lenhardt; P Manshausen; M Novitasari; L Ricard; C Roesch"}, {"ref_id": "b106", "title": "Climart: A benchmark dataset for emulating atmospheric radiative transfer in weather and climate models", "journal": "", "year": "2021", "authors": "S R Cachay; V Ramesh; J N S Cole; H Barker; D Rolnick"}, {"ref_id": "b107", "title": "Could machine learning break the convection parameterization deadlock?", "journal": "Geophys. Res. Lett", "year": "2018", "authors": "P Gentine; M Pritchard; S Rasp; G Reinaudi; G Yacalis"}, {"ref_id": "b108", "title": "Deep learning to represent subgrid processes in climate models", "journal": "", "year": "2018", "authors": "S Rasp; M S Pritchard; P Gentine"}, {"ref_id": "b109", "title": "Interpreting and stabilizing machine-learning parameterizations of convection", "journal": "J. Atmos. Sci", "year": "2020", "authors": "N D Brenowitz; T Beucler; M Pritchard; C S Bretherton"}, {"ref_id": "b110", "title": "A fortran-keras deep learning bridge for scientific computing", "journal": "", "year": "2020", "authors": "J Ott; M Pritchard; N Best; E Linstead; M Curcic; P Baldi"}, {"ref_id": "b111", "title": "A moist physics parameterization based on deep learning", "journal": "J. Adv. Model. Earth Syst", "year": "2020", "authors": "Y Han; G J Zhang; X Huang; Y Wang"}, {"ref_id": "b112", "title": "Assessing the potential of deep learning for emulating cloud superparameterization in climate models with realgeography boundary conditions", "journal": "J. Adv. Model. Earth Syst", "year": "2021", "authors": "G Mooers; M Pritchard; T Beucler; J Ott; G Yacalis; P Baldi; P Gentine"}, {"ref_id": "b113", "title": "Stable climate simulations using a realistic general circulation model with neural network parameterizations for atmospheric moist physics and radiation processes", "journal": "Geosci. Model Dev", "year": "2022", "authors": "X Wang; Y Han; W Xue; G Yang; G J Zhang"}, {"ref_id": "b114", "title": "Non-local parameterization of atmospheric subgrid processes with neural networks", "journal": "J. Adv. Model. Earth Syst", "year": "2022", "authors": "P Wang; J Yuval; P A O'gorman"}, {"ref_id": "b115", "title": "Enforcing analytic constraints in neural networks emulating physical systems", "journal": "Phys. Rev. Lett", "year": "2021", "authors": "T Beucler; M Pritchard; S Rasp; J Ott; P Baldi; P Gentine"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: (a) Summary, where dT /dt and dq/dt are the tendencies of temperature and specific humidity, respectively, and were vertically integrated with mass weighting. (b,c) retain the vertical structure of MAE and (d,e) R 2 . Error bars and grey shadings show the the 5-to 95-percentile range of MLP. Refer toTable 1 for variable definitions.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: The architecture of the MLP baseline model.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "def mae_adjusted ( y_true , y_pred ) : abs_error = K . abs ( y_pred -y_true ) vertical_weights = K . mean ( abs_error [ : ,: ,0 : 2 ] ) * ( 120 / 128 ) scalar_weights = K . mean ( abs_error [ : ,: ,2 : 10 ] ) * ( 8 / 128 ) return vertical_weights + scalar_weightsThe CNN baseline has approximately 13.2 million parameters and executes 1.59 GFlops on one data point. The architecture is visualized below in Figure2.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 2 :2Figure 2: The ResNet-style CNN baseline is comprised of multiple ResNet blocks (i.e., DoubleConv), and applies different activation to the outputs for vertically-resolved and global variables. The channel dimensions are [in_channels, hidden_ch, out_ch, vert_ch, scalar_ch] = [6, 406, 10, 8, 2].", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "nodes = [256, 512, 1024, 2048], size of the latent space = [4, 8, 16, 32], \u03b2 (log-uniform in [0.01, 10]), optimizer = [SGD, Adam] with (\u03b2 1 = 0.9, \u03b2 2 = 0.0999), learning rate \u03bb (log-uniform in [10 -6 , 10 -3 ]), L2 regularization \u03b1 (log-uniform in [10 -6 , 10 -3 ]), and batch size = [1024, 2048, 4096, 8192, 16384]. Each run was trained for 5 epochs total on one NVIDIA GeForce RTX 4080 16GB. The run with the lowest CRPS on the validation data yielded N layers = 3, N nodes = 1,024, and a batch size of 4,096, trained with Adam. In a second step, we fixed these hyperparameters and further fine-tuned \u03b2, \u03bb, and \u03b1 by training for 20 epochs every time, for 10 trials. We found the best model with \u03b2 = 0.5, \u03bb = 5 \u00d7 10 \u22125 , \u03b1 = 10 \u22123 . The cVAE baseline has approximately 4.9 million parameters and executes 4.88 MFlops per data point.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 3 :3Figure 3: Averaged (a) MAE, (b) RMSE, (c) R 2 , and (d) CRPS. Each metric is calculated at each grid point, then horizontally-averaged and (for dT /dt and dq/dt) vertically-averaged. For MAE, RMSE, and CRPS, the units of non-energy flux variables are converted to a common energy unit, W/m 2 , following Section 5.2. Negative values are not shown for R 2 . Error bars show the 5-to 95-percentile range of MLP.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "CRPS (dT/dt)", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 4 :4Figure 4: Vertical structures of horizontally-averaged (a) MAE, (b) RMSE, (c) R 2 , and (d) CRPS of dT /dt. For MAE, RMSE, and CRPS, the units of non-energy flux variables are converted to a common energy unit, W/m 2 , following Section 5.2. Negative values are not shown for R 2 . Grey shadings show the 5-to 95-percentile range of MLP.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 5 :5Figure 5: The vertical structures of horizontally-averaged (a) MAE, (b) RMSE, (c) R 2 , and (d) CRPS of dq/dt. For MAE, RMSE, and CRPS, the units of non-energy flux variables are converted to a common energy unit, W/m 2 , following Section 5.2. Negative values are not shown for R 2 . Grey shadings show the 5-to 95-percentile range of MLP.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 6 :6Figure 6: R 2 of daily-mean, zonal-mean (a) heating tendency and (b) moistening tendency. Yellow contours surround regions of > .9R 2 while orange contours surround regions of > .7R 2 . Negative values are not plotted (white). Sin(latitude) is used for x-axis to account for the curvature of Earth.The pressure levels on Y-axis are approximated values.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "3 .3Who funded the creation of the dataset? The main funding body is the National Science Foundation (NSF) Science and Technology Center (STC) Learning the Earth with Artificial Intelligence and Physics (LEAP). Other funding sources of individual authors are listed in the acknowledgment section of the main text.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "dq/dt dql/dt dqi/dt du/dt dv/dt NETSW FLWDS PRECSC PRECC SOLS SOLL SOLSD SOLLD Output", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "Figure 7 :7Figure 7: Equivalent to Figure S3, but for comparing the MLPv1 (subset emulation) and the MLPv2 (full vector emulation). In addition, MLP models trained with the high-resolution dataset (ne30) are shown here: MLPv1-ne30 and MLPv2-ne30. Bars show the median of the performance of top-20 models selected from the hyperparamter search (>8,000 trials), and magenta error bars show the range of the top-20 model performance.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "Figure 8 :8Figure 8: Equivalent to Figure 2, but for comparing the MLP v1 (subset emulation) and the MLP v2 (full vector emulation). In addition, MLP models trained with the high-resolution dataset (ne30) are shown here: MLPv1-ne30 and MLPv2-ne30. Out of the top model pools, MLP models shown in this figure are randomly chosen for visualizatoin.", "figure_data": ""}, {"figure_label": "16", "figure_type": "figure", "figure_id": "fig_17", "figure_caption": "Figure 16 :16Figure 16: Hexagonally-binned representation of 3D (vertically-resolved) target variables comparing the climate model simulation (\"true\"; x-axis) with the ML model prediction (\"predicted\"; y-axis) at four different vertical levels. The color of each hexagonal bin corresponds to the number of data points enclosed.", "figure_data": ""}, {"figure_label": "3217", "figure_type": "figure", "figure_id": "fig_18", "figure_caption": "8. 3 2 (Figure 17 :3217Figure 17: Global maps of R 2 of baseline models (built on the low-res, real-geography dataset). Grey shading shows locations with negative R 2 values.", "figure_data": ""}, {"figure_label": "18", "figure_type": "figure", "figure_id": "fig_19", "figure_caption": "Figure 18 :18Figure 18: Global maps of R 2 of baseline models (built on the low-res, real-geography dataset). Grey shading shows locations with negative R 2 values.", "figure_data": ""}, {"figure_label": "19", "figure_type": "figure", "figure_id": "fig_20", "figure_caption": "Figure 19 :19Figure 19: Global maps of R 2 of baseline models (built on the low-res, real-geography dataset). Grey shading shows locations with negative R 2 values.", "figure_data": ""}, {"figure_label": "20", "figure_type": "figure", "figure_id": "fig_21", "figure_caption": "Figure 20 :20Figure 20: Global maps of R 2 of baseline models (built on the low-res, real-geography dataset). Grey shading shows locations with negative R 2 values.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ": MAE and R 2 for target variables averaged globally and temporally (from 0009-03 to 0011-02). Variables include heating tendency (dT /dt), moistening tendency (dq/dt), net surface shortwave flux (NETSW), downward surface longwave flux (FLWDS), snow rate (PRECSC), rain rate (PRECC), visible direct solar flux (SOLS), near-IR direct solar flux (SOLL), visible diffused solar flux (SOLSD), and near-IR diffused solar flux (SOLLD). Units of non-energy flux variables are converted to a common energy unit, W/m 2 . Best model performance for each variable is bolded."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "3, 8.1, and 8.2 in SI).", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Model Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2 Model Configurations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Code Access . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Variable List . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Dataset Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4 Dataset Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.5 Target Audiences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Multilayer Perceptron (MLP) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Randomized Prior Network (RPN) . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Convolutional Neural Network (CNN) . . . . . . . . . . . . . . . . . . . . . . . . Feb 2024 3.4 Heteroskedastic Regression (HSR) . . . . . . . . . . . . . . . . . . . . . . . . . . 3.5 Conditional Variational Autoencoder (cVAE) . . . . . . . . . . . . . . . . . . . . 3.6 Encoder-Decoder (ED) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.7 Inference Cost . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.1 Deterministic Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.2 Stochastic Metric (CRPS) . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Fit Quality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Physical Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Unit Conversion and Weighting for Interpretable Evaluation . . . . . . . . . . . . 5.3 Additional Guidance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2 Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.3 Maintenance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.4 Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.5 Collection Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.6 Uses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 Extra Figures and Tables 8.1 MLP with Expanded Target Variables . . . . . . . . . . . . . . . . . . . . . . . . 8.2 Scatter Plots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.3 Global Maps of R 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "figure_data": "Contents 1 Climate Simulations 1.1 2 Dataset and Code Access 2.1 3 Baseline Models 4.1 5 Guidance 5.1 6 Other Related Work 7 Datasheet 3.1 4 Baseline Model Evaluations 7.1"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "E3SM-MMF is unique among climate models in that it can leverage hybrid CPU/GPU architectures on machines such as NERSC Perlmutter (https://www.nersc.gov/systems/perlmutter), which has NVIDIA A100 GPUs per node. All simulations were configured to run with 4 MPI ranks and 16 OpenMP threads per node. The low-resolution (real geography and aquaplanet) cases used 2 nodes, and the high-resolution (real geography) case used 32 nodes. The throughput of these configurations was roughly 11.5 simulated years per day (sypd) for low-resolution cases and 3.3 sypd for the high-resolution case, averaged over multiple batch submissions. The total simulation length in all cases was 10 model years and 2 model months.Boundary conditions over maritime regions are constrained by prescribed sea surface temperatures and sea ice amount. Various input data are needed for the cases with realistic topography, such as ozone concentrations and sea surface temperatures, which have been generated to reflect a climatological average of the 2005-2014 period. The aquaplanet configuration does not have a land component, but otherwise has similar input requirements using idealized data to produce a climate that is symmetric along lines of constant latitude.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Overview of input variables (first column) and output variables (second column) of the E3SM-MMF physics calculations (including the CRM) that are stored in ClimSim. The other columns indicate the variable name, dimensions, units, and a brief description. IR is short for infrared, which is also often referred to as \"longwave\" radiation among atmospheric scientists.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "N nodes : [128, 256, 384, 512, 640, 768, 896, 1024] \u2022 Activation function: [ReLU, LeakyReLU (\u03b1 = 0.15), eLU (\u03b1 = 1.0)]", "figure_data": "\u2022 Batch size: [48, 96, 192, 384, 768, 1152, 1536, 2304, 3072]\u2022 Optimizer: [Adam, RAdam, RMSprop, SGD]"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_12", "figure_caption": ", N nodes = [256, 512, 1,024, 2,048], \u03b3 (log-uniform in [0.001, 0.1]), optimizer = [SGD, Adam] with hyperparameters (\u03b2 1 = 0.9, \u03b2 2 = 0.999), learning rate \u03bb (log-uniform in [10 -6 , 10 -3 ]), and batch size =[1024, 2048, 4096, 8192,  16384]. Each run was trained for 12 epochs total on one NVIDIA GeForce RTX 4080 16GB. We chose the run with the lowest CRPS on the validation data, yielding N layers = 4, N nodes = 1,024, \u03b3 = 2.2 \u00d7 10 \u22122 , \u03bb = 7 \u00d7 10 \u22126 , and a batch size of 16,384, trained with Adam. The HSR baseline has approximately 6.63 million parameters and executes 6.85 MFlops per data point.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "we shuffle the training data set before each epoch. ED baseline has approximately 832,000 parameters, with 415,000 parameters in the Encoder and 417,000 parameters in the Decoder. In total, ED executes 1.66 MFlops per data point, with 829 kFLops per data point for the Encoder and 832 kFlops per data point for the Decoder.", "figure_data": "7 th epoch\u2022 Batch size: 714\u2022 Latent space width: 5 Nodes\u2022 Encoder node size: [124, 463, 463, 232, 116, 58, 29, 5]\u2022 Decoder node size: [5, 29, 58, 116, 232, 463, 463, 128]\u2022 Encoder activation functions: [Input, ReLU, ReLU, ReLU, ReLU, ReLU, ReLU, ReLU]\u2022 Decoder activation functions: [Input, ReLU, ReLU, ReLU, ReLU, ReLU, ReLU, ELU]\u2022 Optimizer: AdamTo prevent overfitting 3.7 Inference CostCNNEDHSRMLPRPNcVAENumber of Parameters 13,200,000 832,000 6,630,000 1,750,000 222,300,000 4,900,000 MFlops Per Data Point 1590 1.66 6.85 3.50 890 4.88"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "The number of learnable parameters and Megaflops (MFlops) per data point for each of the six baseline models.", "figure_data": "4 Baseline Model Evaluations4.1 Metrics4.1.1 Deterministic MetricsMean Absolute Error (MAE):"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "Globally-averaged RMSE and CRPS. Each metric is calculated at each grid point, then horizontally-averaged and (for dT /dt and dq/dt) vertically-averaged. The units of non-energy flux variables are converted to a common energy unit, W/m 2 , following Section 5.2. Best model performance for each variable is highlighted in bold.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "8 Extra Figures and Tables 8.1 MLP with Expanded Target Variables", "figure_data": "(Variables) MLPv1MLPv2MLPv1-ne30 MLPv2-ne30MAEdT/dt dq/dt dq l /dt dq i /dt du/dt dv/dt NETSW FLWDS PRECSC PRECC SOLS SOLL SOLSD SOLLD2.688 4.503 N/A N/A N/A N/A 13.47 5.118 2.645 33.89 7.942 10.30 4.587 4.8342.305 4.030 0.689 0.384 1.34E-04 1.09E-04 8.339 4.134 1.539 23.74 5.774 8.190 3.230 3.9772.799 4.231 N/A N/A N/A N/A 15.47 5.318 3.115 42.49 8.484 10.582 5.056 4.9632.886 4.068 0.697 0.330 2.68E-04 2.66E-04 11.04 4.891 3.009 29.62 6.866 8.993 4.360 4.553R2dT/dt dq/dt dq l /dt dq i /dt du/dt dv/dt NETSW FLWDS PRECSC PRECC SOLS SOLL SOLSD SOLLD0.590 -N/A N/A N/A N/A 0.982 0.927 --1.494 0.962 0.948 0.955 0.8660.663 -----0.993 0.945 -0.833 0.978 0.964 0.976 0.9050.626 -N/A N/A N/A N/A 0.977 0.914 -0.117 -0.115 0.963 0.953 0.950 0.8740.536 -----0.988 0.924 -0.117 -0.115 0.976 0.965 0.965 0.899RMSEdT/dt dq/dt dq l /dt dq i /dt du/dt dv/dt NETSW FLWDS PRECSC PRECC SOLS SOLL SOLSD SOLLD4.437 7.337 26.95 6.803 4.656 73.16 17.39 21.96 9.474 10.143.756 6.521 1.192 0.812 2.80E-04 2.25E-04 17.24 5.532 2.955 53.47 12.84 17.89 6.837 8.4865.199 7.550 30.48 7.136 7.791 119.8 18.51 22.71 10.42 10.624.958 7.135 1.489 0.940 6.45E-04 6.72E-04 21.18 6.540 7.509 83.22 14.74 19.27 8.724 9.526"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "Similar to Table2in the main text but for comparing MAR, R2, and RMSE of different MLP models: MLP v1 (subset emulation) and the MLP v2 (full vector emulation) built with the low-resolution (ne4) and the high-resolution datasets (ne30). dq l /dt, dq i /dt, du/dt, and dv/dt correspond to the tendencies of state_q0002, state_q0003, state_u, and state_v, respectively, in Table", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "P k = A k P 0 + B k P s(1)", "formula_coordinates": [19.0, 264.43, 322.21, 240.33, 12.45]}, {"formula_id": "formula_1", "formula_text": "N layers = 5, N nodes = [", "formula_coordinates": [23.0, 107.58, 317.81, 90.04, 12.67]}, {"formula_id": "formula_2", "formula_text": "N layers = 3, N nodes = [", "formula_coordinates": [23.0, 107.58, 517.12, 88.65, 12.67]}, {"formula_id": "formula_3", "formula_text": "N layers = 4, N nodes = [", "formula_coordinates": [23.0, 122.67, 645.46, 90.54, 12.67]}, {"formula_id": "formula_4", "formula_text": "\u2022 Optimizer: [SGD, Adam]", "formula_coordinates": [25.0, 135.32, 519.59, 110.67, 12.01]}, {"formula_id": "formula_5", "formula_text": "y i | x i \u223c N (\u00b5(x i ), Diag(\u03c4 (x i ) \u22121 )),", "formula_coordinates": [26.0, 232.31, 445.83, 147.39, 19.14]}, {"formula_id": "formula_6", "formula_text": "y i | x i \u223c N (\u03bc \u03b8 (x i ), Diag(\u03c4 \u03b8 (x i ) \u22121 )),", "formula_coordinates": [26.0, 228.36, 501.86, 155.28, 19.14]}, {"formula_id": "formula_7", "formula_text": "L(\u03b8, \u03d5) = 1 2n n i=1 \u2225\u03c4 \u03d5 (x i ) (y i \u2212\u03bc \u03b8 (x i ))\u2225 2 2 \u2212 1 T log (\u03bc \u03b8 (x i )) .", "formula_coordinates": [26.0, 174.13, 543.2, 263.75, 31.2]}, {"formula_id": "formula_8", "formula_text": "L \u03c1,\u03b3 (\u03b8, \u03d5) := \u03c1L(\u03b8, \u03d5) + (1 \u2212 \u03c1) \u03b3 \u2225\u03b8\u2225 2 2 + (1 \u2212 \u03b3) \u2225\u03d5\u2225 2 2 ,", "formula_coordinates": [26.0, 185.28, 628.71, 241.45, 19.81]}, {"formula_id": "formula_9", "formula_text": "z \u223c N (0, I) y|z, x \u223c N \u00b5 \u03b8 (z, x), Diag(\u03c3 \u03b8 (x) 2 ) (2)", "formula_coordinates": [27.0, 230.19, 281.22, 274.57, 33.38]}, {"formula_id": "formula_10", "formula_text": "L \u03b8 (q) := \u2212E zi\u223cq log p \u03b8 (y i , z i |x i ) q(z i |x i ) = \u2212 log p \u03b8 (y i |x i ) + KL q \u2225 p \u03b8 (z i |y i , x i ) \u22650", "formula_coordinates": [27.0, 139.62, 347.16, 325.43, 37.38]}, {"formula_id": "formula_11", "formula_text": "L \u03b8 (q) \u03b2=1 = E zi\u223cq \u03c8 1 2 y i \u2212 \u00b5 \u03b8 (z i , x i ) \u03c3 \u03b8 (z i , x i ) 2 2 + 1 T log (\u03c3 \u03b8 (z i , x i )) + \u03b2KL(q \u03c8 (z i ) \u2225 p(z i )) + const", "formula_coordinates": [27.0, 108.14, 448.08, 395.71, 28.93]}, {"formula_id": "formula_12", "formula_text": "p \u03b8 (\u0177|x) = p \u03b8 (\u0177|x, z)p(z) dz.", "formula_coordinates": [27.0, 240.19, 590.81, 131.62, 10.71]}, {"formula_id": "formula_13", "formula_text": "N", "formula_coordinates": [27.0, 120.69, 684.75, 6.56, 11.9]}, {"formula_id": "formula_14", "formula_text": "MAE = 1 n n i=1 |X i \u2212 y|(3)", "formula_coordinates": [29.0, 257.96, 158.11, 246.8, 31.2]}, {"formula_id": "formula_15", "formula_text": "RMSE = 1 n n i=1 (X i \u2212 y) 2(4)", "formula_coordinates": [29.0, 247.69, 226.53, 257.07, 31.2]}, {"formula_id": "formula_16", "formula_text": "R 2 = 1 \u2212 n i=1 (X i \u2212 y) 2 n i=1 (X i \u2212X) 2(5)", "formula_coordinates": [29.0, 249.78, 296.58, 254.98, 33.77]}, {"formula_id": "formula_17", "formula_text": "CRPS(F, y) := F (x) \u2212 1 {x\u2265y} 2 dx = E[|X \u2212 y|] \u2212 1 2 E[|X \u2212 X \u2032 |],", "formula_coordinates": [29.0, 215.23, 453.8, 181.55, 46.55]}, {"formula_id": "formula_18", "formula_text": "CRPS(X, y) := 1 n n i=1 |X i \u2212 y| \u2212 1 2n(n \u2212 1) n i=1 n j=1 |X i \u2212 X j | (6)", "formula_coordinates": [29.0, 175.52, 550.96, 329.24, 34.1]}, {"formula_id": "formula_19", "formula_text": "i (\u03b4q v + \u03b4q l + \u03b4q i + \u03b4q r + \u03b4q s ) \u2206p i g\u03b4t = E \u2212 P (7)", "formula_coordinates": [33.0, 211.54, 404.87, 293.22, 27.72]}, {"formula_id": "formula_20", "formula_text": "Total Water (Actual) = i (\u03b4q o v + \u03b4q o l + \u03b4q o i ) \u2206p i g Total Water (Expected) = i \u03b4q i v + \u03b4q i l + \u03b4q i i \u2206p i g \u2212 P \u03b4t", "formula_coordinates": [33.0, 142.66, 589.86, 255.21, 58.02]}, {"formula_id": "formula_21", "formula_text": "a (x) = A col (x) / \u27e8A col \u27e9 x", "formula_coordinates": [34.0, 251.66, 359.77, 108.19, 17.29]}, {"formula_id": "formula_22", "formula_text": "T W/m 2 = c p g \u00d7 a (x) \u00d7 \u2206p i (lev) \u00d7\u1e6a [K/s]", "formula_coordinates": [34.0, 225.22, 500.99, 197.45, 23.79]}, {"formula_id": "formula_23", "formula_text": "q W/m 2 = L v g \u00d7 a (x) \u00d7 \u2206p i (lev) \u00d7q s \u22121", "formula_coordinates": [34.0, 225.95, 570.65, 191.32, 23.79]}, {"formula_id": "formula_24", "formula_text": "u W/m 2 = |U | g \u00d7 a (x) \u00d7 \u2206p i (lev) \u00d7u m/s", "formula_coordinates": [34.0, 219.4, 696.91, 199.89, 24.04]}, {"formula_id": "formula_25", "formula_text": "P W/m 2 = L v \u00d7 \u03c1 w \u00d7 a (x) \u00d7 P [m/s]", "formula_coordinates": [35.0, 237.79, 149.39, 172.31, 18.92]}], "doi": "10.11578/E3SM/dc.20230110.5"}