{"title": "On the Consistency of Ranking Algorithms", "authors": "John C Duchi; Lester W Mackey; Michael I Jordan", "pub_date": "", "abstract": "We present a theoretical analysis of supervised ranking, providing necessary and sufficient conditions for the asymptotic consistency of algorithms based on minimizing a surrogate loss function. We show that many commonly used surrogate losses are inconsistent; surprisingly, we show inconsistency even in low-noise settings. We present a new value-regularized linear loss, establish its consistency under reasonable assumptions on noise, and show that it outperforms conventional ranking losses in a collaborative filtering experiment.", "sections": [{"heading": "", "text": "The goal in ranking is to order a set of inputs in accordance with the preferences of an individual or a population. In this paper we consider a general formulation of the supervised ranking problem in which each training example consists of a query q, a set of inputs x, sometimes called results, and a weighted graph G representing preferences over the results. The learning task is to discover a function that provides a queryspecific ordering of the inputs that best respects the observed preferences. This query-indexed setting is natural for tasks like web search in which a different ranking is needed for each query. Following existing literature, we assume the existence of a scoring function f (x, q) that gives a score to each result in x; the scores are sorted to produce a ranking (Herbrich et al., 2000;Freund et al., 2003). We assume simply that the observed preference graph G is a directed acyclic graph (DAG). Finally, we cast our work in a decisiontheoretic framework in which ranking procedures are evaluated via a loss function L(f (x, q), G).\nIt is important to distinguish between the loss function used for evaluating learning procedures from the losslike functions used to define specific methods (generally via an optimization algorithm). In prior work the former (evaluatory) loss has often been taken to be a pairwise 0-1 loss that sums the number of misordered pairs of results. Recent work has considered losses that penalize errors on more highly ranked instances more strongly. J\u00e4rvelin & Kek\u00e4l\u00e4inen (2002) suggest using discounted cumulative gain, which assumes that each result x i is given a score y i and that the loss is a weighted sum of the y i of the predicted order. Rudin (2009) uses a p-norm to emphasize the highest ranked instances. Here we employ a general graph-based loss L(f (x, q), G) which is equal to zero if f (q, x) obeys the order specified by G-that is, f i (x, q) > f j (x, q) for each edge (i \u2192 j) \u2208 G, where f i (x, q) is the score assigned to the ith object in x-and is positive otherwise. We make the assumption that L is edgewise, meaning that L depends only on the relative order of f i (x, q) rather than on its values. Such losses are natural in settings with feedback in the form of ordered preferences, for example when learning from click data.\nAlthough we might wish to base a learning algorithm on the direct minimization of the loss L, this is generally infeasible due to the non-convexity and discontinuity of L. In practice one instead employs a surrogate loss that lends itself to more efficient minimization. This issue is of course familiar from the classification literature, where a deep theoretical understanding of the statistical and computational consequences of the choices of various surrogate losses has emerged (Zhang, 2004;Bartlett et al., 2006). There is a relative paucity of such understanding for ranking. In the current paper we aim to fill this gap, taking a step toward bringing the ranking literature into line with that for classification. We provide a general theoretical analysis of the consistency of ranking algorithms that are based on a surrogate loss function.\nThe paper is organized as follows. In Section 1, we define the consistency problem formally and present a theorem that provides conditions under which consistency is achieved for ranking algorithms. In Section 2 we show that finding consistent surrogate losses is difficult in general, and we establish results showing that many commonly used ranking loss functions are inconsistent, even in low-noise settings. We complement this in Section 3 by presenting losses that are consistent in these low-noise settings. We finish with experiments and conclusions in Sections 4 and 5.", "publication_ref": ["b5", "b4", "b6", "b9", "b13", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Consistency for Surrogate Losses", "text": "Our task is to minimize the risk of the scoring function f . The risk is the expected loss of f across all queries q, result sets x, and preference DAGs G:\nR(f ) = E X,Q,G L(f (X, Q), G).\n(1)\nGiven a query q and result set x, we define G to be the set of possible preference DAGs and p to be (a version of) the vector of conditional probabilities of each DAG. That is,\np = [p G ] G\u2208G = [P(G | x, q)] G\u2208G .\nIn what follows, we suppress dependence of p, G, and G on the query q and results x, as they should be clear from context. We assume that the cardinality of any result set x is bounded above by M < \u221e. We further define the conditional risk of f given x and q to be\n\u2113(p, f (x, q)) = G\u2208G p G L(f (x, q), G) = G\u2208G P(G | x, q)L(f (x, q), G). (2\n)\nWith this definition, we see the risk of f is equal to\nE X,Q G\u2208G P(G | X, Q)L(f (X, Q), G) = E X,Q \u2113(p, f ).\nWe overload notation so that \u03b1 takes the value of f (x, q) in \u2113(p, \u03b1). The minimal risk, or Bayes' risk, is the minimal risk over all measurable functions,\nR * = inf f R(f ) = E X,Q inf \u03b1 \u2113(p, \u03b1).\nIt is infeasible to directly minimize the true risk in Eq. (1), as it is non-convex and discontinuous. As is done in classification (Zhang, 2004;Bartlett et al., 2006), we thus consider a bounded-below surrogate \u03d5 to minimize in place of L. For each G, we write\n\u03d5(\u2022, G) : R |G| \u2192 R. The \u03d5-risk of the function f is R \u03d5 (f ) = E X,Q,G [\u03d5(f (X, Q), G)] = E X,Q G\u2208G P(G | X, Q)\u03d5(f (X, Q), G) ,\nwhile the optimal \u03d5-risk is R * \u03d5 = inf f R \u03d5 (f ). To develop a theory of consistency for ranking methods, we pursue a treatment that parallels that of Zhang (2004) for classification. Using the conditional risk in Eq. (2), we define a function to measure the discriminating ability of the surrogate \u03d5. Let G(m) denote the set of possible DAGs G over m results, noting that |G(m\n)| \u2264 3 ( m 2 ) . Let \u2206 |G(m)| \u2282 R |G(m)| denote the prob- ability simplex. For \u03b1, \u03b1 \u2032 \u2208 R m we define H m (\u03b5) = inf p\u2208\u2206,\u03b1 G\u2208G(m) p G \u03d5(\u03b1, G) \u2212 inf \u03b1 \u2032 G\u2208G(m) p G \u03d5(\u03b1 \u2032 , G) : \u2113(p, \u03b1) \u2212 inf \u03b1 \u2032 \u2113(p, \u03b1 \u2032 ) \u2265 \u03b5 .(3)\nH m measures surrogate risk suboptimality as a function of true risk suboptimality. A reasonable surrogate loss should declare any setting of {p, \u03b1} suboptimal that the true loss declares suboptimal, which corresponds to H m (\u03b5) > 0 whenever \u03b5 > 0. We will see soon that this condition is the key to consistency.\nDefine\nH(\u03b5) = min m\u2264M H m (\u03b5). We immediately have H \u2265 0, H(0) = 0, and H(\u03b5) is non-decreasing on 0 \u2264 \u03b5 < \u221e, since individual H m (\u03b5) are non-decreasing in \u03b5.\nWe have the following lemma (a simple consequence of Jensen's inequality), which we prove in Appendix A.\nLemma 1. Let \u03b6 be a convex function such that \u03b6(\u03b5) \u2264 H(\u03b5). Then for all f , \u03b6(R(f ) \u2212 R * ) \u2264 R \u03d5 (f ) \u2212 R * \u03d5 .\nCorollary 26 from Zhang (2004) then shows as a consequence of Lemma 1 that if H(\u03b5) > 0 for all \u03b5 > 0, there is a nonnegative concave function \u03be, right continuous at 0 with \u03be(0\n) = 0, such that R(f ) \u2212 R * \u2264 \u03be(R \u03d5 (f ) \u2212 R * \u03d5 ).(4)\nClearly, if lim n R \u03d5 (f n ) = R * \u03d5 , we have consistency: lim n R(f n ) = R * . Though it is not our focus, it is possible to use Eq. (4) to get strong rates of convergence if \u03be grows slowly. The remainder of this paper concentrates on finding conditions relating the surrogate loss \u03d5 to the risk \u2113 to make H(\u03b5) > 0 for \u03b5 > 0.\nWe achieve this goal by using conditions based on the edge structure of the observed DAGs. Given a probability vector p \u2208 R |G| over a set of DAGs G, we recall Eq. (2) and define the set of optimal result scores A(p) to be all \u03b1 attaining the infimum of \u2113(p, \u03b1),\nA(p) = {\u03b1 : \u2113(p, \u03b1) = inf \u03b1 \u2032 \u2113(p, \u03b1 \u2032 )}. (5\n)\nThe infimum is attained since \u2113 is edgewise as described earlier, so A(p) is not empty. The following definition captures the intuition that the surrogate loss \u03d5 should maintain ordering information. For this definition and the remainder of the paper, we use the following shorthand for the conditional \u03d5-risk:\nW (p, \u03b1) G\u2208G p G \u03d5(\u03b1, G).(6)\nDefinition 2. Let \u03d5 be a bounded-below surrogate loss with \u03d5(\u2022, G) continuous for all G. \u03d5 is edge-consistent with respect to the loss L if for all p,\nW * (p) inf \u03b1 W (p, \u03b1) < inf \u03b1 {W (p, \u03b1) : \u03b1 \u2208 A(p)} .\nDefinition 2 captures an essential property for the surrogate loss \u03d5: if \u03b1 induces an edge (i \u2192 j) via \u03b1 i > \u03b1 j so that the conditional risk \u2113(p, \u03b1) is not minimal, then the conditional surrogate risk W (p, \u03b1) is not minimal.\nWe now provide three lemmas and a theorem that show that if the surrogate loss \u03d5 satisfies edgeconsistency, then its minimizer asymptotically minimizes the Bayes risk. As the lemmas are direct analogs of results in Tewari & Bartlett (2007) and Zhang (2004), we put their proofs in Appendix A.\nLemma 3. W * (p) is continuous on \u2206. Lemma 4. Let \u03d5 be edge-consistent. Then W (p, \u03b1 (n) ) \u2192 W * (p) implies that \u2113(p, \u03b1 (n) ) \u2192 inf \u03b1 \u2113(p, \u03b1) and \u03b1 (n) \u2208 A(p) eventually. Lemma 5. Let \u03d5 be edge-consistent. For every \u03b5 > 0 there exists a \u03b4 > 0 such that if p \u2208 \u2206, \u2113(p, \u03b1) \u2212 inf \u03b1 \u2032 \u2113(p, \u03b1 \u2032 ) \u2265 \u03b5 implies W (p, \u03b1) \u2212 W * (p) \u2265 \u03b4.\nTheorem 6. Let \u03d5 be a continuous, bounded-below loss function and assume that the size of the result sets is upper bounded by a constant M . Then \u03d5 is edgeconsistent if and only the following holds: Whenever f n is a sequence of scoring functions such that\nR \u03d5 (f n ) p \u2192 R * \u03d5 , then R(f n ) p \u2192 R * .", "publication_ref": ["b13", "b0", "b13", "b13", "b11", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Proof", "text": "We begin by proving that if \u03d5 is edgeconsistent, the implication holds. By Lemma 5 and the definition of H m in Eq. (3), we have that if \u03b5 > 0, then there is some \u03b4 > 0 such that H m (\u03b5) \u2265 \u03b4 > 0. Thus H(\u03b5) = min m\u2264M H m (\u03b5) > 0, and Eq. (4) then immediately implies that R(f n ) p \u2192 R * . Now suppose that \u03d5 is not edge-consistent, that is, there is some p so that W * (p) = inf \u03b1 {W (p, \u03b1) : \u03b1 \u2208 A(p)}. Let \u03b1 (n) \u2208 A(p) be a sequence such that W (p, \u03b1 (n) ) \u2192 W * (p). If we simply define the risk to be the expected loss on one particular example x and set f n (x) = \u03b1 (n) , then R \u03d5 (f n ) = W (p, \u03b1 (n) ). Further, by assumption there is some \u03b5 > 0 such that \u2113(p, \u03b1\n(n) ) \u2265 inf \u03b1 \u2113(p, \u03b1) + \u03b5 for all n. Thus R(f n ) = \u2113(p, \u03b1 (n) ) \u2192 R * = inf \u03b1 \u2113(p, \u03b1).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Difficulty of Consistency", "text": "In this section, we explore the difficulty of finding edgeconsistent ranking losses in practice. We first show that unless P = N P many useful losses cannot be edge-consistent in general. We then show that even in low-noise settings, common losses used for ranking are not edge-consistent. We focus our attention on pairwise losses, which impose a separate penalty for each edge that is ordered incorrectly; this generalizes the disagreement error described by Dekel et al. (2004). We assume we have a set of non-negative penalties a G ij indexed by edge (i \u2192 j) and graph G so that\nL(\u03b1, G) = i<j a G ij 1 (\u03b1i\u2264\u03b1j ) + i>j a G ij 1 (\u03b1i<\u03b1j ) .(7)\nWe distinguish the cases i < j and i > j to avoid minor technical issues created by doubly penalizing 1 (\u03b1i=\u03b1j ) .\nIf we define a ij G\u2208G a G ij p G , then\n\u2113(p, \u03b1) = i<j a ij 1 (\u03b1i\u2264\u03b1j ) + i>j a ij 1 (\u03b1i<\u03b1j ) .(8)", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}, {"heading": "General inconsistency results", "text": "Finding an efficiently minimizable surrogate loss that is also consistent for Eq. ( 8) for all p is unlikely, as indicated by the next lemma. The result is a consequence of the fact that the feedback arc-set problem is N P -complete (Karp, 1972); we defer its proof to Appendix A.\nLemma 7. Define \u2113(p, \u03b1) as in Eq. (8). Finding an \u03b1 minimizing \u2113 is N P -hard.\nSince many convex functions are minimizable in polynomial time or can be straightforwardly transformed into a formulation that is minimizable in polylogarithmic time (Ben-Tal & Nemirovski, 2001), most convex surrogates are inconsistent unless P = N P .", "publication_ref": ["b7", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Low-noise inconsistency", "text": "In this section we show that, surprisingly, many common convex surrogates are inconsistent even in lownoise settings. Inspecting Eq. ( 7), a natural choice for a surrogate loss is one of the form (Herbrich et al., 2000;Freund et al., 2003;Dekel et al., 2004)\n\u03d5(\u03b1, G) = (i\u2192j)\u2208G h(a G ij )\u03c6(\u03b1 i \u2212 \u03b1 j ) (9)\nwhere \u03c6 \u2265 0 is a non-increasing function, and h is a function of the penalties a G ij . In this case, the conditional surrogate risk is W (p, \u03b1) = i =j h ij \u03c6(\u03b1 i \u2212 \u03b1 j ), where we define\nh ij G\u2208G h(a G ij )p G .\nIf \u03d5 from Eq. ( 9) is edge-consistent, then \u03c6 must be differentiable at 0 with \u03c6 \u2032 (0) < 0. This is a consequence of Bartlett et al.'s (2006) analysis of binary classification and the correspondence between binary classification and pairwise ranking; for the binary case, consistency requires \u03c6 \u2032 (0) < 0. Similarly, we must have h \u2265 0 on R + and strictly increasing. For the remainder of this section, we make the unrestrictive assumption that \u03c6 decreases more slowly in the positive direction than it increases in the negative. Formally, we use the recession function (Rockafellar, 1970, Thm. 8.5) of \u03c6,\n\u03c6 \u2032 \u221e (d) sup t>0 \u03c6(td) \u2212 \u03c6(0) t = lim t\u2192\u221e \u03c6(td) \u2212 \u03c6(0) t .\nThe assumption, satisfied for bounded below \u03c6, is\nAssumption A. \u03c6 \u2032 \u221e (1) \u2265 0 or \u03c6 \u2032 \u221e (\u22121) = \u221e.\nWe now define precisely what we mean by a low-noise setting. For any (G, p), let G be the difference graph, that is, the graph with edge weights max{a ij \u2212 a ji , 0} on edges (i \u2192 j), where a ij = G\u2208G a G ij p G , and if a ij \u2264 a ji then the edge (i \u2192 j) \u2208 G (see Fig. 1). We define the following low-noise condition based on self-reinforcement of edges in the difference graph. Definition 8. We say (G, p) is low-noise when the corresponding difference graph G satisfies the following reverse triangle inequality: whenever there is an edge (i \u2192 j) and an edge (j \u2192 k) in G, then the weight a ik \u2212 a ki on (i \u2192 k) is greater than or equal to the path weight a ij \u2212 a ji + a jk \u2212 a kj on (i \u2192 j \u2192 k).\nIt is not difficult to see that if (G, p) satisfies Def. 8, its difference graph G is a DAG. Indeed, the definition ensures that all global preference information in G (the sum of weights along any path) conforms with and reinforces local preference information (the weight on a single edge). Reasonable ranking methods should be consistent in this setting, but this is not trivial.\nIn the lemmas to follow, we consider simple 3-node DAGs that admit unique minimizers for their conditional risks. In particular, we consider DAGs on nodes 1, 2, and 3 that induce only the four penalty values a 12 , a 13 , a 23 , and a 31 (see Fig. 1). In this case, if a 13 > a 31 , any \u03b1 minimizing \u2113(p, \u03b1) clearly will have \u03b1 1 > \u03b1 2 > \u03b1 3 . We now show under some very general conditions that if \u03d5 is edge-consistent, \u03c6 is non-convex.\nLet \u03c6 \u2032 (x) denote an element of the subgradient set \u2202\u03c6(x). The subgradient conditions for optimality of are that\nW (p, \u03b1) = h 12 \u03c6(\u03b1 1 \u2212 \u03b1 2 ) + h 13 \u03c6(\u03b1 1 \u2212 \u03b1 3 ) (10) + h 23 \u03c6(\u03b1 2 \u2212 \u03b1 3 ) + h 31 \u03c6(\u03b1 3 \u2212 \u03b1 1 )\n0 = h 12 \u03c6 \u2032 (\u03b1 1 \u2212 \u03b1 2 ) + h 13 \u03c6 \u2032 (\u03b1 1 \u2212 \u03b1 3 ) \u2212 h 31 \u03c6 \u2032 (\u03b1 3 \u2212 \u03b1 1 ) 0 = \u2212h 12 \u03c6 \u2032 (\u03b1 1 \u2212 \u03b1 2 ) + h 23 \u03c6 \u2032 (\u03b1 2 \u2212 \u03b1 3 ). (11\n)\nWe begin by showing that under Assumption A on \u03c6, there is a finite minimizer of W (p, \u03b1). The lemma is technical and its proof is in Appendix A.\nLemma 9. There is a constant C < \u221e and a vector \u03b1 * minimizing W (p, \u03b1) with \u03b1 * \u221e \u2264 C.\nWe use the following lemma to prove our main theorem about inconsistency of pairwise convex losses.\nLemma 10 (Inconsistency of convex losses). Suppose that a 13 > a 31 > 0, a 12 > 0, a 23 > 0. Let \u2113(p, \u03b1) be\na 12 1 (\u03b11\u2264\u03b12) + a 13 1 (\u03b11\u2264\u03b13) + a 23 1 (\u03b12\u2264\u03b13) + a 31 1 (\u03b13<\u03b11)\nand W (p, \u03b1) be defined as in Eq. (10). For convex \u03c6 with \u03c6 \u2032 (0) < 0, W * (p) = inf \u03b1 {W (p, \u03b1) : \u03b1 / \u2208 A(p)} whenever either of the following conditions is satisfied:\nCond 1: h 23 < h 31 h 12 h 13 + h 12 Cond 2: h 12 < h 31 h 23 h 13 + h 23 .\nProof Lemma 9 shows that the optimal W * (p) is attained by some finite \u03b1. Thus, we fix an \u03b1 * satisfying Eq. (11), and let \u03b4 ij = \u03b1 * i \u2212 \u03b1 * j and g ij = \u03c6 \u2032 (\u03b4 ij ) for i = j. We make strong use of the monotonicity of subgradients, that is, \u03b4 ij > \u03b4 kl implies g ij \u2265 g kl (e.g. Rockafellar, 1970, Theorem 24.1). By Eq. (11),\ng 13 \u2212 g 12 = h 31 h 13 g 31 \u2212 1 + h 12 h 13 g 12 (12a) g 13 \u2212 g 23 = h 31 h 13 g 31 \u2212 1 + h 23 h 13 g 23 . (12b)\nSuppose for the sake of contradiction that \u03b1 * \u2208 A(p).\nAs \u03b4 13 = \u03b4 12 + \u03b4 23 , we have that \u03b4 13 > \u03b4 12 and \u03b4 13 > \u03b4 23 . The convexity of \u03c6 implies that if \u03b4 13 > \u03b4 12 , then g 13 \u2265 g 12 . If g 12 \u2265 0, we thus have that g 13 \u2265 0 and by Eq. (11), g 31 \u2265 0. This is a contradiction since \u03b4 31 < 0 gives g 31 \u2264 \u03c6 \u2032 (0) < 0. Hence, g 12 < 0. By identical reasoning, we also have that g 23 < 0. Now, \u03b4 23 > 0 > \u03b4 31 implies that g 23 \u2265 g 31 , which combined with Eq. (12a) and the fact that g 23 = (h 12 /h 23 )g 12 (by Eq. ( 11)) gives\ng 13 \u2212 g 12 \u2264 h 31 h 13 g 23 \u2212 1 + h 12 h 13 g 12 = h 31 h 12 h 23 \u2212 h 13 \u2212 h 12 g 12 h 13 .\nSince g 12 /h 13 < 0, we have that g 13 \u2212g 12 < 0 whenever h 31 h 12 /h 23 > h 13 + h 12 . But when \u03b4 13 > \u03b4 12 , we must have g 13 \u2265 g 12 , which yields a contradiction under Condition 1.\nSimilarly, \u03b4 12 > 0 > \u03b4 31 implies that g 12 \u2265 g 31 , which with g 12 = (h 23 /h 12 )g 23 and Eq. (12b) gives\ng 13 \u2212 g 23 \u2264 h 31 h 13 g 12 \u2212 1 + h 23 h 13 g 23 = h 31 h 23 h 12 \u2212 h 13 \u2212 h 23 g 23 h 13 .\nSince g 23 /h 13 < 0, we further have that g 13 \u2212 g 23 < 0 whenever h 31 h 23 /h 12 > h 13 + h 23 . This contradicts \u03b4 13 > \u03b4 23 under Condition 2.\nLemma 10 allows us to construct scenarios under which arbitrary pairwise surrogate losses with convex \u03c6 are inconsistent. Assumption A only to specify an optimal \u03b1 with \u03b1 \u221e < \u221e, and can be weakened to\nW (p, \u03b1) \u2192 \u221e as (\u03b1 i \u2212 \u03b1 j ) \u2192 \u221e.\nThe next theorem is our main negative result on the consistency of pairwise surrogate losses.\nTheorem 11. Let \u03d5 be a loss that can be written as\n\u03d5(\u03b1, G) = (i\u2192j)\u2208G h(a G ij )\u03c6(\u03b1 i \u2212 \u03b1 j )\nfor h continuous and increasing with h(0) = 0. Even in the low-noise setting, for \u03c6 convex and satisfying Assumption A, \u03d5 is not edge-consistent.", "publication_ref": ["b5", "b4", "b3"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "Proof", "text": "Assume for the sake of contradiction that \u03d5 is edge-consistent.\nRecall that for \u03c6 convex, \u03c6 \u2032 (0) < 0, and we can construct graphs G 1 and G 2 so that the resulting expected loss satisfies\nCondition 1 of Lemma 10. Let G = {G 1 , G 2 } where G 1 = ({1, 2, 3}, {(1 \u2192 2) , (1 \u2192 3)}) and G 2 = ({1, 2, 3}, {(2 \u2192 3) , (3 \u2192 1)}). Fix any weights a G1\n12 , a G1 13 , a G2 31 with a G1 13 > a G1 12 > 0 and a G1 13 > a G2 31 > 0, and let p = (.5, .5). As h is continuous with h(0) = 0, there exists some \u03b5 > 0 such that h(\u03b5) < 2h 31 h 12 /(h 13 + h 12 ), where h ij = G\u2208G h(a G ij )p G . Take a G2 23 = min{\u03b5, (a G1 13 \u2212 a G1 12 )/2}. Then we have h 23 = h(a G2 23 )/2 \u2264 h(\u03b5)/2 < h 31 h 12 /(h 13 + h 12 ). Hence Condition 1 of Lemma 10 is satisfied, so \u03d5 is not edge-consistent.\nMoreover,\na G2 23 \u2264 (a G1 13 \u2212 a G1 12 )/2 < a G1 13 \u2212 a G1\n12 implies that G is a DAG satisfying the low-noise condition.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Margin-based inconsistency", "text": "Given the difficulties encountered in the previous section, it is reasonable to consider a reformulation of our surrogate loss. A natural alternative is a marginbased loss, which encodes a desire to separate ranking scores by a large margins dependent on the preferences in a graph. Similar losses have been proposed, e.g., by Shashua & Levin (2002). In particular, we now consider losses of the form\n\u03d5(\u03b1, G) = (i\u2192j)\u2208G \u03c6 \u03b1 i \u2212 \u03b1 j \u2212 h(a G ij ) , (13\n)\nwhere h is continuous and h(0) = 0. It is clear from the reduction to binary classification that h must be increasing for the loss in Eq. ( 13) to be edge-consistent. When \u03c6 is a decreasing function, this intuitively says that the larger a ij is, the larger \u03b1 i should be when compared to \u03b1 j . Nonetheless, as we show below, such a loss is inconsistent even in low-noise settings.\nTheorem 12. Let \u03d5 be a loss that can be written as\n\u03d5(\u03b1, G) = (i\u2192j)\u2208G \u03c6(\u03b1 i \u2212 \u03b1 j \u2212 h(a G ij ))\nfor h continuous and increasing with h(0) = 0. Even in the low-noise setting, for \u03c6 convex and satisfying Assumption A, \u03d5 is not edge-consistent.\nProof Assume for the sake of contradiction that \u03d5 is edge-consistent. As noted before, \u03c6 \u2032 (0) < 0, and since \u03c6 is differentiable almost everywhere (Rockafellar, 1970, Theorem 25.3), \u03c6 is differentiable at \u2212c for some c > 0 in the range of h. Considering the four-graph setting with graphs containing one edge each,\nG 1 = ({1, 2, 3}, {(1 \u2192 2)}), G 2 = ({1, 2, 3}, {(2 \u2192 3)}), G 3 = ({1, 2, 3}, {(1 \u2192 3)})\n, and G 4 = ({1, 2, 3}, {(3 \u2192 1)}), choose constant edge weights a G1 12 = a G2 13 = a G3 23 = a G4 31 = h \u22121 (c) > 0, and set p = (.25, .01, .5, .24). In this setting, . Hence, by Lemma 10, W * (p) = inf \u03b1 {W (p, \u03b1) : \u03b1 / \u2208 A(p)}, a contradiction.\nW (p, \u03b1) = p G1\u03c6 (\u03b1 1 \u2212 \u03b1 2 ) + p G2\u03c6 (\u03b1 2 \u2212 \u03b1 3 ) + p G3\u03c6 (\u03b1 1 \u2212 \u03b1 3 ) + p G4\u03c6 (\u03b1 3 \u2212 \u03b1 1 ), for\u03c6(x) = \u03c6(x \u2212 c). Notably,\u03c6 is convex, satisfies Assumption A, and\u03c6 \u2032 (0) = \u03c6 \u2032 (\u2212c) < 0. Moreover, a 13 \u2212 a 31 = h \u22121 (c)(p G3 \u2212 p G4 ) \u2265 h \u22121 (c)(p G1 + p G2 ) = a 12 + a 23 > 0, so G is a", "publication_ref": ["b10"], "figure_ref": [], "table_ref": []}, {"heading": "Conditions for Consistency", "text": "The prospects for consistent surrogate ranking appear bleak given the results of the previous section. Nevertheless, we demonstrate in this section that there exist surrogate losses that yield consistency under some restrictions on problem noise. We consider a new lossspecifically, a linear loss in which we penalize (\u03b1 j \u2212 \u03b1 i ) proportional to the weight a ij in the given graph G.\nTo keep the loss well-behaved and disallow wild fluctuations, we also regularize the \u03b1 values. That is, our loss takes the form\n\u03d5(\u03b1, G) = (i\u2192j)\u2208G a G ij (\u03b1 j \u2212 \u03b1 i ) + \u03bd i r(\u03b1 i ). (14)\nWe assume that r is strictly convex and 1-coercive, that is, that r asymptotically grows faster than any linear function. These conditions imply that the loss of Eq. ( 14) is bounded below. Moreover, we have the basis for consistency:\nTheorem 13. Let the loss take the form of a generalized disagreement error of Eq. ( 7) and the surrogate loss take the form of Eq. ( 14) where \u03bd > 0 and r is strictly convex and 1-coercive. If the pair (G, p) induces a difference graph G that is a DAG, then\nW * (p) < inf \u03b1 {W (p, \u03b1) : \u03b1 / \u2208 A(p)} \u21d4 j a ij \u2212 a ji > j a kj \u2212 a jk for i, k s.t. a ik > a ki .\nProof We first note that G is a DAG if and only if A(p) = {\u03b1 : \u03b1 i > \u03b1 j for i < j with a ij > a ji , \u03b1 i \u2265 \u03b1 j for i > j with a ij > a ji }.\n(For a proof see Lemma 16, though essentially all we do is write out \u2113(p, \u03b1).) We have that\nW (p, \u03b1) = i \u03b1 i j (a ji \u2212 a ij ) + \u03bdr(\u03b1 i ) .\nStandard subgradient calculus gives that at optimum, r \u2032 (\u03b1 i ) = j a ij \u2212 a ji \u03bd .\nSince r is strictly convex, r \u2032 is a strictly increasing setvalued map with increasing inverse s(g) = {\u03b1 : g \u2208 \u2202r(\u03b1)}. Optimality is therefore attained uniquely at\n\u03b1 * i = s j a ij \u2212 a ji \u03bd . (15\n)\nNote that for any i, k, \u03b1 * i > \u03b1 * k if and only if s\nj aij \u2212aji \u03bd > s j a kj \u2212a jk \u03bd\n, which in turn occurs if and only if j aij \u2212aji \u03bd > j a kj \u2212a jk \u03bd . Hence, the optimal \u03b1 * of Eq. ( 15) is in A(p) if and only if\nj a ij \u2212 a ji \u03bd > j a kj \u2212 a jk \u03bd when a ik > a ki . (16)\nThus, W * (p) = inf \u03b1 {W (p, \u03b1) : \u03b1 / \u2208 A(p)} whenever Eq. ( 16) is violated. On the other hand, suppose Eq. ( 16) is satisfied. Then for all \u03b1 satisfying\n\u03b1 \u2212 \u03b1 * \u221e < min {i,k:a ik >a ki } 1 2 (\u03b1 * i \u2212 \u03b1 * k ),\nwe have \u03b1 \u2208 A(p), and inf \u03b1 {W (p, \u03b1) : \u03b1 / \u2208 A(p)} > W * (p) since \u03b1 * is the unique global minimum. We now prove a simple lemma showing that low-noise settings satisfy the conditions of Theorem 13. Lemma 14. If (G, p) is low noise, then for the associated difference graph G, whenever a ik > a ki , j a ij \u2212 a ji > j a kj \u2212 a jk .\nProof Fix (i, k) with a ik > a ki . There are two cases for a third node j: either a ij \u2212 a ji > 0 or a ij \u2212 a ji \u2264 0. In the first case, there is an edge\n(i \u2192 j) \u2208 G. If (k \u2192 j) \u2208 G, the low-noise condition implies a ij \u2212 a ji \u2265 a kj \u2212 a jk + a ik \u2212 a ki > a kj \u2212 a jk . Otherwise, a kj \u2212 a jk \u2264 0 < a ij \u2212 a ji . In the other case, a ij \u2212 a ji \u2264 0. If the inequality is strict, then (j \u2192 i) \u2208 G, so the low-noise condition implies that a ji \u2212 a ij < a ji \u2212 a ij + a ik \u2212a ki \u2264 a jk \u2212a kj , or a kj \u2212a jk < a ij \u2212a ji .\nOtherwise, a ij = a ji , and the low-noise condition guarantees that\n(j \u2192 k) / \u2208 G, so a kj \u2212 a jk \u2264 0 = a ij \u2212 a ji .\nThe inequality in the statement of the lemma is strict, because a ik \u2212 a ki > 0 = a kk \u2212 a kk . The converse of the lemma is, in general, false. Combining the above lemma with Theorem 13, we have Corollary 15. The linear loss of Eq. ( 14) is edgeconsistent if (G, p) is low noise for all query-result pairs.\nWith the above corollary, we have a consistent loss: the value-regularized linear loss is edge (and hence asymptotically) consistent in low-noise settings. It is not difficult to see that the value regularization from r is necessary; if r is not in the objective in Eq. ( 14), then \u03d5(\u2022, G) can be sent to \u2212\u221e with \u03b1 \u2208 A(p).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Relationship to prior work", "text": "One of the main results on consistency to date is due to Cossock & Zhang (2008), who work in a setting in which each item x i to be ranked is associated with a score y i . In this setting we can show that the resulting graphs (G, p) satisfy our low-noise condition. Indeed, for every observed pair of results and scores (x i , y i ), (x j , y j ) , we can construct a graph G = ({x i , x j }, {(i \u2192 j)}) and set a G ij = y i \u2212 y j . Then in the limit, we have a ij =\u0233 i \u2212\u0233 j , where\u0233 i is the true score of item x i , and clearly a ik = a ij + a jk so that G satisfies the low-noise condition.\nAnother related line of work is due to Xia et al. (2008), who introduce a notion of order-preserving probability spaces. These are inherently different from our work, which we show by considering graphs on nodes 1, 2, and 3. First, consider a low-noise setting in which the difference graph G consists of edges (1 \u2192 2) and (1 \u2192 3). Our losses are indifferent to whether we order result 2 ahead of or behind 3, and this cannot be captured by an order-preserving probability space.\nConversely, consider an order-preserving probability space over the three nodes, where the data we receive consists of full orderings of 1, 2, 3. To translate this into our framework, we must convert each of these orderings into a DAG G with associated edge weights. We assume that the weight on each edge is only a function of the distance between the entries in the ordering. Suppose we observe two orderings \u03c0 = {1 \u2192 2 \u2192 3} and \u03c0 = {2 \u2192 3 \u2192 1} with probabilities p \u03c0 > p \u03c0 \u2265 0 and p \u03c0 + p \u03c0 = 1, which is an order preserving probability space (see Def. 3 and Theorem 5 in Xia et al., 2008). If we assume w.l.o.g. that any adjacent pair in the list has edge weight equal to one and that pairs of distance equal to two in the list have edge weight w 2 , then there is no way to set w 2 so that the resulting (G, p) satisfies the low-noise condition in Def. 8. The associated difference graph G will have edge weights a 12 = p \u03c0 \u2212 w 2 p \u03c0 , a 13 = w 2 p \u03c0 \u2212 p \u03c0 , and a 23 = p \u03c0 + p \u03c0 . To satisfy the low-noise condition in Def. 8 and have the ordering \u03c0 minimize the true loss, we must have\nw 2 p \u03c0 \u2212 p \u03c0 = a 13 \u2265 a 12 + a 23 = p \u03c0 \u2212 w 2 p \u03c0 + p \u03c0 + p \u03c0 so that w 2 p \u03c0 + w 2 p \u03c0 \u2265 2p \u03c0 + 2p \u03c0 . That is, w 2 \u2265 2.\nOn the other hand, we must have a 12 > 0 so that p \u03c0 > w 2 p \u03c0 or w 2 < p \u03c0 /p \u03c0 ; taking p \u03c0 \u2193 .5 and p \u03c0 \u2191 .5, we have w 2 \u2264 1. Thus no construction that assigns a fixed weight to edges associated with permutations can transform an order-preserving probability space into graphs satisfying the low-noise conditions here. Nonetheless, our general consistency result, Theorem 6, implicitly handles order-preserving probability spaces, which assume that graphs G contain all results and the loss L(f (x, q), G) = 0 if f agrees with G on all orderings and is 1 otherwise.", "publication_ref": ["b2", "b12", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "While the focus of this work is a theoretical investigation of consistency, we have also conducted experiments that study the value-regularized linear loss our analysis suggests. We perform experiments on a collaborative filtering task in which the goal is to recommend movies to a user based on the user's and other users' movie ratings. We use one of the Movie-Lens datasets (GroupLens Lab, 2008), which consists of 100,000 ratings, on a scale of 1 to 5, for 1682 different movies by 943 users. In this case, our \"query\" is a user u, and the set of possible results consists of all 1682 movies. We learn a linear model so that f i (x, u) = w T d(x i , u), where d is a mapping from movie x i and user u to a feature vector. We use features that have proven successful in settings such as the Netflix challenge, including the age of the movie, its genre(s), the average rating of the user for other movies in the same genre(s), the average rating of the movie, and ratings given to the movie by users similar to and dissimilar from u in rating of other movies.\nTo create pairs to train our models, we randomly sample pairs (x i , x j ) of movies rated by a user. Each sampled pair of rated movies then gets a per-user weight a u ij that we set to be the difference in their ratings.\nAs discussed in Sec. 3.1, this guarantees that (G, p) is low noise. We sample across users to get n samples total. We then learn the weight vector w using one of three methods: the value-regularized linear method in this paper, a pairwise hinge loss (Herbrich et al., 2000), and a pairwise logistic loss (Dekel et al., 2004). Specifically, the surrogates are d(xj ,u)\u2212d(xi,u)) ,\ni,j,u a u ij w T (d(x j , u) \u2212 d(x i , u)) + \u03b8 i,u (w T d(x i , u)) 2 i,j,u a u ij 1 \u2212 w T (d(x i , u) \u2212 d(x j , u)) + i,j,u a u ij log 1 + e w T (\nwhere [z] + = max{z, 0}. We set \u03b8 = 10 \u22124 (it needed simply be a small number), and also added \u2113 2 - regularization in the form of \u03bb w 2 to each problem. We cross-validated \u03bb separately for each loss.\nWe partitioned the data into five subsets, and, in each of 15 experiments, we used one subset for validation, one for testing, and three for training. In every experiment, we subsampled 40,000 rated movie pairs from the test set for final evaluation. Once we had learned a vector w for each of the three methods, we computed its average generalized pairwise loss (Eq. ( 7)).\nWe show the results in Table 1. The leftmost column contains the number of pairs that were subsampled for training, and the remaining columns show the average pairwise loss on the test set for each of the methods (with standard error in parentheses). Each number is the mean of 15 independent training runs, and bold denotes the lowest loss. It is interesting to note that the linear loss always achieves the lowest test loss averaged across all tests. In fact, it achieved the lowest test loss of all three methods in all but one of our experimental runs. (We use these three losses to focus exclusively on learning in a pairwise setting- Cossock & Zhang (2008) learn using relevance scores, while Xia et al. (2008) require full ordered lists of results as training data rather than pairs.) Finally, we note that there is a closed form for the minimizer of the linear loss, which makes it computationally attractive.", "publication_ref": ["b5", "b3", "b2", "b12"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Discussion", "text": "In this paper we have presented results on both the difficulty and the feasibility of surrogate loss consistency for ranking. We have presented the negative result that many natural candidates for surrogate ranking are not consistent in general or even under low-noise restrictions, and we have presented a class of surrogate losses that achieve consistency under reasonable noise restrictions. We have also demonstrated the potential usefulness of the new loss functions in practice. This work thus takes a step toward bringing the consistency literature for ranking in line with that for classification. A natural next step in this agenda is to establish rates for ranking algorithms; we believe that our analysis can be extended to the analysis of rates. Finally, given the difficulty of achieving consistency using surrogate losses that decompose along edges, it may be beneficial to explore non-decomposable losses.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Auxiliary Proofs", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Lemma 1", "text": "The proof of this lemma is analogous to Zhang's Theorem 24. Jensen's inequality implies\n\u03b6 E(\u2113(p, f ) \u2212 inf \u03b1 \u2113(p, \u03b1)) \u2264 E\u03b6(\u2113(p, f ) \u2212 inf \u03b1 \u2113(p, \u03b1)) \u2264 E X,Q H(\u2113(p, f ) \u2212 inf \u03b1 \u2113(p, \u03b1)) \u2264 E X,Q G p G \u03d5(f (X, Q), G) \u2212 inf \u03b1 \u2032 G p G \u03d5(\u03b1 \u2032 , G) = R \u03d5 (f ) \u2212 R * \u03d5 .\nThe second to last line is a consequence of the fact that\nH(\u2113(p, \u03b1) \u2212 inf \u03b1 \u2032 \u2113(p, \u03b1 \u2032 )) \u2264 G p G \u03d5(\u03b1, G) \u2212 inf \u03b1 \u2032 G p G \u03d5(\u03b1 \u2032 , G\n) for p \u2208 \u2206 and any \u03b1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Lemma 3", "text": "The proof of this lemma is entirely similar to the proofs of lemma 16 from (Tewari & Bartlett, 2007) and lemma 27 from (Zhang, 2004), but we include it for completeness.\nLet p (n) be a sequence converging to p and B r be a closed ball of radius r in R |G| . Since\nG p (n) G \u03d5(\u03b1, G) \u2192 G \u03d5(\u03b1, G) uniformly in \u03b1 \u2208 B r (we have equicontinuity), inf \u03b1\u2208Br G\u2208G p (n) G \u03d5(\u03b1, G) \u2192 inf \u03b1\u2208Br G\u2208G p G \u03d5(\u03b1, G).\nWe then have\nW * (p (n) ) \u2264 inf \u03b1\u2208Br G p (n) G \u03d5(\u03b1, G) \u2192 inf \u03b1\u2208Br G\u2208G p G \u03d5(\u03b1, G\n), and hence (as the infimum is bounded below) we can let r \u2192 \u221e and have lim sup\nn W * (p (n) ) \u2264 inf \u03b1 G\u2208G p G \u03d5(\u03b1, G) = W * (p).\nTo bound the limit infimum, assume without loss of generality that there is some subset\nG \u2032 \u2282 G so that p G = 0 for all G \u2208 G \u2032 . Defin\u0113 W (p, \u03b1) = G\u2208G \u2032 p G \u03d5(\u03b1, G) andW * (p) = inf \u03b1W (p, \u03b1).\nNote thatW * (p (n) ) is eventually bounded so that each sequence of surrogate loss terms p This completes the proof that W * (p (n) ) \u2192 W * (p).", "publication_ref": ["b11", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Lemma 4", "text": "Suppose that \u2113(p, \u03b1 (n) ) \u2192 inf \u03b1 \u2113(p, \u03b1). Then there is a subsequence \u03b1 (nj ) and \u03b5 > 0 such that \u2113(p, \u03b1 (nj ) ) \u2265 \u2113(p, \u03b1) + \u03b5. This in turn, since there is a finite set of orderings of the entries in \u03b1, implies that \u03b1 (nj ) \u2208 A(p). But by the definition of edge-consistency,\nW (p, \u03b1 (nj ) ) > inf \u03b1 {W (p, \u03b1) : \u03b1 \u2208 A(p)} > W * (p).\nThe W (p, \u03b1 (nj ) ) are thus bounded uniformly away from W * (p), a contradiction.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Lemma 5", "text": "We prove this by contradiction and using the continuity result in Lemma 3. Assume that the statement does not hold, so that there is a sequence (p (n) \n, \u03b1 (n) ) with \u2113(p (n) , \u03b1 (n) ) \u2212 inf \u03b1 \u2032 \u2113(p (n) , \u03b1 \u2032 ) \u2265 \u03b5 but W (p (n) , \u03b1 (n) ) \u2212 W * (p (n) ) \u2192 0.\nBecause \u2206 |G| is compact, we choose a convergent subsequence n j so that p (nj ) \u2192 p for some p \u2208 \u2206 |G| . By Lemma 3 and our assumption that W (p (nj ) , \u03b1 (nj ) ) \u2212 W * (p (nj ) ) \u2192 0, W (p (nj ) , \u03b1 (nj ) ) \u2192 W * (p). Similar to the proof of the previous lemma, we assume that there is a set\nG \u2032 \u2282 G with p G > 0 for G \u2208 G \u2032 . We have lim sup nj G\u2208G p G \u03d5(\u03b1 (nj ) , G) = lim sup nj G\u2208G \u2032 p (nj ) G \u03d5(\u03b1 (nj ) , G) \u2264 lim nj G\u2208G p (nj ) G \u03d5(\u03b1 (nj ) , G) = W * (p).\nThe above proves that W (p, \u03b1 (nj ) ) \u2192 W * (p), and Lemma 4 implies that \u2113(p, \u03b1 (nj ) ) \u2192 inf \u03b1 \u2113(p, \u03b1) and \u03b1 (nj ) \u2208 A(p) eventually. The continuity of \u2113 in p and the fact that p (nj ) \u2192 p, however, contradicts\n\u2113(p (n) , \u03b1 (n) ) \u2212 inf \u03b1 \u2113(p (n) , \u03b1) \u2265 \u03b5.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Lemma 7", "text": "As stated earlier, this is a straightforward consequence of the fact that the feedback arc set problem (Karp, 1972) is N P -complete. In the feedback arc set problem, we are given a directed graph G = (V, E) and integer k and need to determine whether there is a subset E \u2032 \u2286 E with |E \u2032 | \u2264 k such that E \u2032 contains at least one edge from every directed cycle in G, or, equivalently, that G \u2032 = (V, E \\ E \u2032 ) is a DAG. Now consider the problem of deciding whether there exists a \u03b1 with \u2113(p, \u03b1) \u2264 k, and let G be the graph over all the nodes in the graphs in G with average edge weights a ij = G\u2208G a G ij p G . Since \u03b1 induces an order of the nodes in this expected graph G, this is equivalent to finding an ordering of the nodes i 1 , . . . , i n (denoted i 1 \u227a i 2 \u227a \u2022 \u2022 \u2022 \u227a i n ) in G such that the sum of the back edges is less than k, i.e., i j a ij \u2264 k.\nFurther, it is clear that removing all the back edges (edges (i \u2192 j) in the expected graph G such that i j in the order) leaves a DAG. Now given a graph G = (V, E), we can construct the expected graph G directly from G with weights a ij = 1 if (i \u2192 j) \u2208 E and 0 otherwise (to be pedantic, set p ij = 1/|E| and let the ij th graph in the set of possible graphs G be simply the edge (i \u2192 j) with weight a ij = |E|). Then it is clear that there is a \u03b1 such that \u2113(p, \u03b1) \u2264 k if and only if there is a feedback arc set E \u2032 with |E \u2032 | \u2264 k.", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Lemma 9", "text": "Let (\u03b1 (n) ) \u221e n=1 be a sequence with \u03b1 (n) \u2208 A(p) = {\u03b1 : \u03b1 1 > \u03b1 2 > \u03b1 3 } such that W (p, \u03b1 (n) ) \u2192 W * (p). Now suppose that lim sup n (\u03b1\n(n) i \u2212 \u03b1 (n)\nj ) = \u221e for some i < j. Since \u03b1 (n) \u2208 A(p), this implies lim sup n (\u03b1\n(n) 1 \u2212 \u03b1 (n) 3 ) = \u221e.\nBut \u03c6 is convex with \u03c6 \u2032 (0) < 0 and so is unbounded above, so certainly lim sup n \u03c6(\u03b1\n(n) 3 \u2212 \u03b1 (n) 1 ) = \u221e.\nThe assumption that \u03c6 \u2032 \u221e (1) = 0 or \u03c6 \u2032 \u221e (\u22121) = \u2212\u221e then implies that lim sup n W (p, \u03b1 (n) ) = \u221e. This contradiction gives that there must be some C with |\u03b1 (n) i \u2212 \u03b1 (n) j | \u2264 C for all i, j, n. W (p, \u03b1) is shift invariant with respect to \u03b1, so without loss of generality we can let \u03b1 (n) 3 = 0, and thus |\u03b1 (n) i | \u2264 C. Convex functions are continuous on compact domains (Rockafellar, 1970, Chapter 10), and inf \u03b1: \u03b1 \u221e \u2264C W (p, \u03b1) = W * (p), which proves the lemma.\nLemma 16. Let G be the difference graph for the pair (G, p). G is a DAG if and only if A(p) = {\u03b1 : \u03b1 i > \u03b1 j for i < j with a ij > a ji , \u03b1 i \u2265 \u03b1 j for i > j with a ij > a ji }.\nProof We first prove that if G is a DAG, then A(p) is the set above. Assume without loss of generality that the nodes in G are topologically sorted in the order 1, 2, . . . , m, so that a ij \u2265 a ji for i < j. (a ij \u2212 a ji )1 (\u03b1i\u2264\u03b1j ) + a ji .\nClearly, if a ij > a ji for some i < j, then any minimizing \u03b1 must satisfy \u03b1 i > \u03b1 j . If a ij \u2212 a ji = 0 for some j > i, then the relative order of \u03b1 i and \u03b1 j does not affect \u2113(p, \u03b1). Now suppose that G is not a DAG but that A(p) takes the form described in the statement of the lemma. In this case, there are (at least) two nodes i and j with a path going from i to j and from j to i in G. Let the nodes on the path from i to j be l 1 , . . . , l k and from j to i be l \u2032 1 , . . . , l \u2032 k \u2032 . By assumption, we must have that for any \u03b1 \u2208 A(p),\n\u03b1 i \u2265 \u03b1 l1 \u2265 \u2022 \u2022 \u2022 \u2265 \u03b1 l k \u2265 \u03b1 j \u2265 \u03b1 l \u2032 1 \u2265 \u2022 \u2022 \u2022 \u2265 \u03b1 l \u2032 k \u2032 \u2265 \u03b1 i .\nOne of the inequalities must be strict because we will have some l or l \u2032 > i, which is clearly a contradiction.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "JD and LM were supported by NDSEG fellowships. We thank the reviewers for their helpful feedback.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "classification, and risk bounds", "journal": "Journal of the American Statistical Association", "year": "2006", "authors": "P Bartlett; M Jordan; J Mcauliffe;  Convexity"}, {"ref_id": "b1", "title": "Lectures on Modern Convex Optimization", "journal": "SIAM", "year": "2001", "authors": "A Ben-Tal; A Nemirovski"}, {"ref_id": "b2", "title": "Statistical analysis of Bayes optimal subset ranking", "journal": "IEEE Transaction on Information Theory", "year": "2008", "authors": "D Cossock; T Zhang"}, {"ref_id": "b3", "title": "Log-linear models for label ranking", "journal": "", "year": "2004", "authors": "O Dekel; C Manning; Y Singer"}, {"ref_id": "b4", "title": "Efficient boosting algorithms for combining preferences", "journal": "", "year": "2003", "authors": "Y Freund; R Iyer; R E Schapire; Y Singer"}, {"ref_id": "b5", "title": "Large margin rank boundaries for ordinal regression", "journal": "MIT Press", "year": "2000", "authors": "R Herbrich; T Graepel; K Obermayer"}, {"ref_id": "b6", "title": "Cumulated gain-based evaluation of IR techniques", "journal": "ACM Transactions on Information Systems", "year": "2002", "authors": "K J\u00e4rvelin; J Kek\u00e4l\u00e4inen"}, {"ref_id": "b7", "title": "Reducibility among combinatorial problems", "journal": "Plenum Press", "year": "1972", "authors": "R M Karp"}, {"ref_id": "b8", "title": "Convex Analysis", "journal": "Princeton University Press", "year": "1970", "authors": "R T Rockafellar"}, {"ref_id": "b9", "title": "The p-norm push: a simple convex ranking algorithm that concentrates at the top of the list", "journal": "Journal of Machine Learning Research", "year": "2009", "authors": "C Rudin"}, {"ref_id": "b10", "title": "Ranking with large margin principle: Two approaches", "journal": "", "year": "2002", "authors": "A Shashua; A Levin"}, {"ref_id": "b11", "title": "On the consistency of multiclass classification methods", "journal": "Journal of Machine Learning Research", "year": "2007", "authors": "A Tewari; P Bartlett"}, {"ref_id": "b12", "title": "Listwise approach to learning to rank -theory and algorithm", "journal": "", "year": "2008", "authors": "F Xia; T Y Liu; J Wang; W Zhang; H Li"}, {"ref_id": "b13", "title": "Statistical analysis of some multi-category large margin classification methods", "journal": "Journal of Machine Learning Research", "year": "2004", "authors": "T Zhang"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. The two DAGs above occur with probability 12 , giving the difference graph G on the left, assuming that a13 > a31.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "is also eventually bounded. Thus we have the desired uniform convergence, andlim inf n W * (p (n) ) \u2265 lim inf nW * (p (n) ) = W * (p).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "aij 1 (\u03b1i\u2264\u03b1j ) + a ji 1 (\u03b1j <\u03b1i)", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": ".008) .479 (.010) .465 (.006) 40000 .477 (.008) .478 (.010) .464 (.006) 80000 .480 (.007) .478 (.009) .462 (.005) 120000 .477 (.008) .477 (.009) .463 (.006) 160000 .474 (.007) .474 (.007) .461 (.004) Test losses for different surrogate losses.", "figure_data": "Train pairsHingeLogisticLinear20000.478 ("}], "formulas": [{"formula_id": "formula_0", "formula_text": "R(f ) = E X,Q,G L(f (X, Q), G).", "formula_coordinates": [2.0, 105.32, 273.8, 134.2, 10.38]}, {"formula_id": "formula_1", "formula_text": "p = [p G ] G\u2208G = [P(G | x, q)] G\u2208G .", "formula_coordinates": [2.0, 145.88, 328.18, 143.56, 10.47]}, {"formula_id": "formula_2", "formula_text": "\u2113(p, f (x, q)) = G\u2208G p G L(f (x, q), G) = G\u2208G P(G | x, q)L(f (x, q), G). (2", "formula_coordinates": [2.0, 70.75, 407.91, 214.44, 48.9]}, {"formula_id": "formula_3", "formula_text": ")", "formula_coordinates": [2.0, 285.19, 436.28, 4.24, 9.62]}, {"formula_id": "formula_4", "formula_text": "E X,Q G\u2208G P(G | X, Q)L(f (X, Q), G) = E X,Q \u2113(p, f ).", "formula_coordinates": [2.0, 55.44, 490.92, 241.13, 20.88]}, {"formula_id": "formula_5", "formula_text": "R * = inf f R(f ) = E X,Q inf \u03b1 \u2113(p, \u03b1).", "formula_coordinates": [2.0, 99.2, 559.53, 146.47, 17.26]}, {"formula_id": "formula_6", "formula_text": "\u03d5(\u2022, G) : R |G| \u2192 R. The \u03d5-risk of the function f is R \u03d5 (f ) = E X,Q,G [\u03d5(f (X, Q), G)] = E X,Q G\u2208G P(G | X, Q)\u03d5(f (X, Q), G) ,", "formula_coordinates": [2.0, 55.44, 649.11, 225.81, 65.78]}, {"formula_id": "formula_7", "formula_text": ")| \u2264 3 ( m 2 ) . Let \u2206 |G(m)| \u2282 R |G(m)| denote the prob- ability simplex. For \u03b1, \u03b1 \u2032 \u2208 R m we define H m (\u03b5) = inf p\u2208\u2206,\u03b1 G\u2208G(m) p G \u03d5(\u03b1, G) \u2212 inf \u03b1 \u2032 G\u2208G(m) p G \u03d5(\u03b1 \u2032 , G) : \u2113(p, \u03b1) \u2212 inf \u03b1 \u2032 \u2113(p, \u03b1 \u2032 ) \u2265 \u03b5 .(3)", "formula_coordinates": [2.0, 307.44, 157.59, 242.14, 86.16]}, {"formula_id": "formula_8", "formula_text": "H(\u03b5) = min m\u2264M H m (\u03b5). We immediately have H \u2265 0, H(0) = 0, and H(\u03b5) is non-decreasing on 0 \u2264 \u03b5 < \u221e, since individual H m (\u03b5) are non-decreasing in \u03b5.", "formula_coordinates": [2.0, 307.44, 331.81, 233.99, 34.38]}, {"formula_id": "formula_9", "formula_text": "Lemma 1. Let \u03b6 be a convex function such that \u03b6(\u03b5) \u2264 H(\u03b5). Then for all f , \u03b6(R(f ) \u2212 R * ) \u2264 R \u03d5 (f ) \u2212 R * \u03d5 .", "formula_coordinates": [2.0, 307.44, 395.75, 233.99, 23.4]}, {"formula_id": "formula_10", "formula_text": ") = 0, such that R(f ) \u2212 R * \u2264 \u03be(R \u03d5 (f ) \u2212 R * \u03d5 ).(4)", "formula_coordinates": [2.0, 360.22, 465.67, 181.21, 33.72]}, {"formula_id": "formula_11", "formula_text": "A(p) = {\u03b1 : \u2113(p, \u03b1) = inf \u03b1 \u2032 \u2113(p, \u03b1 \u2032 )}. (5", "formula_coordinates": [2.0, 348.07, 656.15, 189.12, 17.18]}, {"formula_id": "formula_12", "formula_text": ")", "formula_coordinates": [2.0, 537.19, 658.36, 4.24, 9.62]}, {"formula_id": "formula_13", "formula_text": "W (p, \u03b1) G\u2208G p G \u03d5(\u03b1, G).(6)", "formula_coordinates": [3.0, 114.02, 114.5, 175.41, 20.89]}, {"formula_id": "formula_14", "formula_text": "W * (p) inf \u03b1 W (p, \u03b1) < inf \u03b1 {W (p, \u03b1) : \u03b1 \u2208 A(p)} .", "formula_coordinates": [3.0, 62.46, 185.27, 219.96, 16.73]}, {"formula_id": "formula_15", "formula_text": "Lemma 3. W * (p) is continuous on \u2206. Lemma 4. Let \u03d5 be edge-consistent. Then W (p, \u03b1 (n) ) \u2192 W * (p) implies that \u2113(p, \u03b1 (n) ) \u2192 inf \u03b1 \u2113(p, \u03b1) and \u03b1 (n) \u2208 A(p) eventually. Lemma 5. Let \u03d5 be edge-consistent. For every \u03b5 > 0 there exists a \u03b4 > 0 such that if p \u2208 \u2206, \u2113(p, \u03b1) \u2212 inf \u03b1 \u2032 \u2113(p, \u03b1 \u2032 ) \u2265 \u03b5 implies W (p, \u03b1) \u2212 W * (p) \u2265 \u03b4.", "formula_coordinates": [3.0, 55.44, 342.86, 234.01, 89.59]}, {"formula_id": "formula_16", "formula_text": "R \u03d5 (f n ) p \u2192 R * \u03d5 , then R(f n ) p \u2192 R * .", "formula_coordinates": [3.0, 89.81, 502.55, 165.25, 14.99]}, {"formula_id": "formula_17", "formula_text": "(n) ) \u2265 inf \u03b1 \u2113(p, \u03b1) + \u03b5 for all n. Thus R(f n ) = \u2113(p, \u03b1 (n) ) \u2192 R * = inf \u03b1 \u2113(p, \u03b1).", "formula_coordinates": [3.0, 55.44, 693.65, 233.99, 24.74]}, {"formula_id": "formula_18", "formula_text": "L(\u03b1, G) = i<j a G ij 1 (\u03b1i\u2264\u03b1j ) + i>j a G ij 1 (\u03b1i<\u03b1j ) .(7)", "formula_coordinates": [3.0, 323.27, 229.53, 218.16, 22.28]}, {"formula_id": "formula_19", "formula_text": "\u2113(p, \u03b1) = i<j a ij 1 (\u03b1i\u2264\u03b1j ) + i>j a ij 1 (\u03b1i<\u03b1j ) .(8)", "formula_coordinates": [3.0, 325.51, 311.66, 215.91, 20.73]}, {"formula_id": "formula_20", "formula_text": "\u03d5(\u03b1, G) = (i\u2192j)\u2208G h(a G ij )\u03c6(\u03b1 i \u2212 \u03b1 j ) (9)", "formula_coordinates": [3.0, 347.84, 635.99, 193.59, 23.12]}, {"formula_id": "formula_21", "formula_text": "h ij G\u2208G h(a G ij )p G .", "formula_coordinates": [3.0, 379.7, 706.54, 95.72, 13.01]}, {"formula_id": "formula_22", "formula_text": "\u03c6 \u2032 \u221e (d) sup t>0 \u03c6(td) \u2212 \u03c6(0) t = lim t\u2192\u221e \u03c6(td) \u2212 \u03c6(0) t .", "formula_coordinates": [4.0, 69.81, 211.47, 205.25, 23.81]}, {"formula_id": "formula_23", "formula_text": "Assumption A. \u03c6 \u2032 \u221e (1) \u2265 0 or \u03c6 \u2032 \u221e (\u22121) = \u221e.", "formula_coordinates": [4.0, 55.44, 262.44, 203.3, 13.05]}, {"formula_id": "formula_24", "formula_text": "W (p, \u03b1) = h 12 \u03c6(\u03b1 1 \u2212 \u03b1 2 ) + h 13 \u03c6(\u03b1 1 \u2212 \u03b1 3 ) (10) + h 23 \u03c6(\u03b1 2 \u2212 \u03b1 3 ) + h 31 \u03c6(\u03b1 3 \u2212 \u03b1 1 )", "formula_coordinates": [4.0, 69.2, 689.06, 220.22, 25.65]}, {"formula_id": "formula_25", "formula_text": "0 = h 12 \u03c6 \u2032 (\u03b1 1 \u2212 \u03b1 2 ) + h 13 \u03c6 \u2032 (\u03b1 1 \u2212 \u03b1 3 ) \u2212 h 31 \u03c6 \u2032 (\u03b1 3 \u2212 \u03b1 1 ) 0 = \u2212h 12 \u03c6 \u2032 (\u03b1 1 \u2212 \u03b1 2 ) + h 23 \u03c6 \u2032 (\u03b1 2 \u2212 \u03b1 3 ). (11", "formula_coordinates": [4.0, 307.44, 308.76, 236.17, 27.51]}, {"formula_id": "formula_26", "formula_text": ")", "formula_coordinates": [4.0, 537.0, 325.91, 4.42, 9.62]}, {"formula_id": "formula_27", "formula_text": "a 12 1 (\u03b11\u2264\u03b12) + a 13 1 (\u03b11\u2264\u03b13) + a 23 1 (\u03b12\u2264\u03b13) + a 31 1 (\u03b13<\u03b11)", "formula_coordinates": [4.0, 307.44, 482.73, 233.5, 11.01]}, {"formula_id": "formula_28", "formula_text": "Cond 1: h 23 < h 31 h 12 h 13 + h 12 Cond 2: h 12 < h 31 h 23 h 13 + h 23 .", "formula_coordinates": [4.0, 307.44, 548.22, 234.0, 24.28]}, {"formula_id": "formula_29", "formula_text": "g 13 \u2212 g 12 = h 31 h 13 g 31 \u2212 1 + h 12 h 13 g 12 (12a) g 13 \u2212 g 23 = h 31 h 13 g 31 \u2212 1 + h 23 h 13 g 23 . (12b)", "formula_coordinates": [4.0, 337.35, 668.47, 204.07, 52.16]}, {"formula_id": "formula_30", "formula_text": "g 13 \u2212 g 12 \u2264 h 31 h 13 g 23 \u2212 1 + h 12 h 13 g 12 = h 31 h 12 h 23 \u2212 h 13 \u2212 h 12 g 12 h 13 .", "formula_coordinates": [5.0, 86.95, 204.18, 170.97, 52.17]}, {"formula_id": "formula_31", "formula_text": "g 13 \u2212 g 23 \u2264 h 31 h 13 g 12 \u2212 1 + h 23 h 13 g 23 = h 31 h 23 h 12 \u2212 h 13 \u2212 h 23 g 23 h 13 .", "formula_coordinates": [5.0, 86.95, 353.42, 170.97, 52.17]}, {"formula_id": "formula_32", "formula_text": "W (p, \u03b1) \u2192 \u221e as (\u03b1 i \u2212 \u03b1 j ) \u2192 \u221e.", "formula_coordinates": [5.0, 55.44, 517.47, 142.9, 10.47]}, {"formula_id": "formula_33", "formula_text": "\u03d5(\u03b1, G) = (i\u2192j)\u2208G h(a G ij )\u03c6(\u03b1 i \u2212 \u03b1 j )", "formula_coordinates": [5.0, 95.84, 578.99, 153.19, 23.13]}, {"formula_id": "formula_34", "formula_text": "Condition 1 of Lemma 10. Let G = {G 1 , G 2 } where G 1 = ({1, 2, 3}, {(1 \u2192 2) , (1 \u2192 3)}) and G 2 = ({1, 2, 3}, {(2 \u2192 3) , (3 \u2192 1)}). Fix any weights a G1", "formula_coordinates": [5.0, 55.44, 69.55, 485.99, 648.74]}, {"formula_id": "formula_35", "formula_text": "a G2 23 \u2264 (a G1 13 \u2212 a G1 12 )/2 < a G1 13 \u2212 a G1", "formula_coordinates": [5.0, 307.44, 189.43, 172.18, 13.72]}, {"formula_id": "formula_36", "formula_text": "\u03d5(\u03b1, G) = (i\u2192j)\u2208G \u03c6 \u03b1 i \u2212 \u03b1 j \u2212 h(a G ij ) , (13", "formula_coordinates": [5.0, 329.16, 366.46, 207.84, 23.13]}, {"formula_id": "formula_37", "formula_text": ")", "formula_coordinates": [5.0, 537.0, 368.35, 4.42, 9.62]}, {"formula_id": "formula_38", "formula_text": "\u03d5(\u03b1, G) = (i\u2192j)\u2208G \u03c6(\u03b1 i \u2212 \u03b1 j \u2212 h(a G ij ))", "formula_coordinates": [5.0, 341.75, 516.06, 165.37, 23.13]}, {"formula_id": "formula_39", "formula_text": "G 1 = ({1, 2, 3}, {(1 \u2192 2)}), G 2 = ({1, 2, 3}, {(2 \u2192 3)}), G 3 = ({1, 2, 3}, {(1 \u2192 3)})", "formula_coordinates": [5.0, 307.44, 671.72, 233.99, 22.65]}, {"formula_id": "formula_40", "formula_text": "W (p, \u03b1) = p G1\u03c6 (\u03b1 1 \u2212 \u03b1 2 ) + p G2\u03c6 (\u03b1 2 \u2212 \u03b1 3 ) + p G3\u03c6 (\u03b1 1 \u2212 \u03b1 3 ) + p G4\u03c6 (\u03b1 3 \u2212 \u03b1 1 ), for\u03c6(x) = \u03c6(x \u2212 c). Notably,\u03c6 is convex, satisfies Assumption A, and\u03c6 \u2032 (0) = \u03c6 \u2032 (\u2212c) < 0. Moreover, a 13 \u2212 a 31 = h \u22121 (c)(p G3 \u2212 p G4 ) \u2265 h \u22121 (c)(p G1 + p G2 ) = a 12 + a 23 > 0, so G is a", "formula_coordinates": [6.0, 55.44, 90.42, 233.99, 87.44]}, {"formula_id": "formula_41", "formula_text": "\u03d5(\u03b1, G) = (i\u2192j)\u2208G a G ij (\u03b1 j \u2212 \u03b1 i ) + \u03bd i r(\u03b1 i ). (14)", "formula_coordinates": [6.0, 66.48, 378.1, 222.93, 23.13]}, {"formula_id": "formula_42", "formula_text": "W * (p) < inf \u03b1 {W (p, \u03b1) : \u03b1 / \u2208 A(p)} \u21d4 j a ij \u2212 a ji > j a kj \u2212 a jk for i, k s.t. a ik > a ki .", "formula_coordinates": [6.0, 63.14, 542.99, 218.6, 44.02]}, {"formula_id": "formula_43", "formula_text": "W (p, \u03b1) = i \u03b1 i j (a ji \u2212 a ij ) + \u03bdr(\u03b1 i ) .", "formula_coordinates": [6.0, 77.09, 698.46, 190.71, 20.73]}, {"formula_id": "formula_44", "formula_text": "\u03b1 * i = s j a ij \u2212 a ji \u03bd . (15", "formula_coordinates": [6.0, 371.84, 162.24, 165.16, 25.03]}, {"formula_id": "formula_45", "formula_text": ")", "formula_coordinates": [6.0, 537.0, 170.81, 4.42, 9.62]}, {"formula_id": "formula_46", "formula_text": "j aij \u2212aji \u03bd > s j a kj \u2212a jk \u03bd", "formula_coordinates": [6.0, 329.29, 207.1, 109.51, 16.47]}, {"formula_id": "formula_47", "formula_text": "j a ij \u2212 a ji \u03bd > j a kj \u2212 a jk \u03bd when a ik > a ki . (16)", "formula_coordinates": [6.0, 326.22, 261.02, 215.19, 25.03]}, {"formula_id": "formula_48", "formula_text": "\u03b1 \u2212 \u03b1 * \u221e < min {i,k:a ik >a ki } 1 2 (\u03b1 * i \u2212 \u03b1 * k ),", "formula_coordinates": [6.0, 346.18, 334.62, 161.5, 23.2]}, {"formula_id": "formula_49", "formula_text": "(i \u2192 j) \u2208 G. If (k \u2192 j) \u2208 G, the low-noise condition implies a ij \u2212 a ji \u2265 a kj \u2212 a jk + a ik \u2212 a ki > a kj \u2212 a jk . Otherwise, a kj \u2212 a jk \u2264 0 < a ij \u2212 a ji . In the other case, a ij \u2212 a ji \u2264 0. If the inequality is strict, then (j \u2192 i) \u2208 G, so the low-noise condition implies that a ji \u2212 a ij < a ji \u2212 a ij + a ik \u2212a ki \u2264 a jk \u2212a kj , or a kj \u2212a jk < a ij \u2212a ji .", "formula_coordinates": [6.0, 307.44, 514.64, 234.0, 85.06]}, {"formula_id": "formula_50", "formula_text": "(j \u2192 k) / \u2208 G, so a kj \u2212 a jk \u2264 0 = a ij \u2212 a ji .", "formula_coordinates": [6.0, 307.44, 614.74, 182.71, 10.47]}, {"formula_id": "formula_51", "formula_text": "w 2 p \u03c0 \u2212 p \u03c0 = a 13 \u2265 a 12 + a 23 = p \u03c0 \u2212 w 2 p \u03c0 + p \u03c0 + p \u03c0 so that w 2 p \u03c0 + w 2 p \u03c0 \u2265 2p \u03c0 + 2p \u03c0 . That is, w 2 \u2265 2.", "formula_coordinates": [7.0, 55.44, 666.12, 233.99, 22.65]}, {"formula_id": "formula_52", "formula_text": "i,j,u a u ij w T (d(x j , u) \u2212 d(x i , u)) + \u03b8 i,u (w T d(x i , u)) 2 i,j,u a u ij 1 \u2212 w T (d(x i , u) \u2212 d(x j , u)) + i,j,u a u ij log 1 + e w T (", "formula_coordinates": [7.0, 315.64, 600.87, 218.74, 80.71]}, {"formula_id": "formula_53", "formula_text": "\u03b6 E(\u2113(p, f ) \u2212 inf \u03b1 \u2113(p, \u03b1)) \u2264 E\u03b6(\u2113(p, f ) \u2212 inf \u03b1 \u2113(p, \u03b1)) \u2264 E X,Q H(\u2113(p, f ) \u2212 inf \u03b1 \u2113(p, \u03b1)) \u2264 E X,Q G p G \u03d5(f (X, Q), G) \u2212 inf \u03b1 \u2032 G p G \u03d5(\u03b1 \u2032 , G) = R \u03d5 (f ) \u2212 R * \u03d5 .", "formula_coordinates": [9.0, 55.67, 137.78, 226.19, 100.76]}, {"formula_id": "formula_54", "formula_text": "H(\u2113(p, \u03b1) \u2212 inf \u03b1 \u2032 \u2113(p, \u03b1 \u2032 )) \u2264 G p G \u03d5(\u03b1, G) \u2212 inf \u03b1 \u2032 G p G \u03d5(\u03b1 \u2032 , G", "formula_coordinates": [9.0, 65.95, 260.19, 223.48, 25.29]}, {"formula_id": "formula_55", "formula_text": "G p (n) G \u03d5(\u03b1, G) \u2192 G \u03d5(\u03b1, G) uniformly in \u03b1 \u2208 B r (we have equicontinuity), inf \u03b1\u2208Br G\u2208G p (n) G \u03d5(\u03b1, G) \u2192 inf \u03b1\u2208Br G\u2208G p G \u03d5(\u03b1, G).", "formula_coordinates": [9.0, 55.44, 402.08, 233.29, 60.28]}, {"formula_id": "formula_56", "formula_text": "W * (p (n) ) \u2264 inf \u03b1\u2208Br G p (n) G \u03d5(\u03b1, G) \u2192 inf \u03b1\u2208Br G\u2208G p G \u03d5(\u03b1, G", "formula_coordinates": [9.0, 55.44, 474.08, 234.0, 26.85]}, {"formula_id": "formula_57", "formula_text": "n W * (p (n) ) \u2264 inf \u03b1 G\u2208G p G \u03d5(\u03b1, G) = W * (p).", "formula_coordinates": [9.0, 80.53, 523.56, 196.59, 22.76]}, {"formula_id": "formula_58", "formula_text": "G \u2032 \u2282 G so that p G = 0 for all G \u2208 G \u2032 . Defin\u0113 W (p, \u03b1) = G\u2208G \u2032 p G \u03d5(\u03b1, G) andW * (p) = inf \u03b1W (p, \u03b1).", "formula_coordinates": [9.0, 55.44, 574.03, 241.56, 58.16]}, {"formula_id": "formula_59", "formula_text": "W (p, \u03b1 (nj ) ) > inf \u03b1 {W (p, \u03b1) : \u03b1 \u2208 A(p)} > W * (p).", "formula_coordinates": [9.0, 314.5, 177.53, 219.88, 16.74]}, {"formula_id": "formula_60", "formula_text": ", \u03b1 (n) ) with \u2113(p (n) , \u03b1 (n) ) \u2212 inf \u03b1 \u2032 \u2113(p (n) , \u03b1 \u2032 ) \u2265 \u03b5 but W (p (n) , \u03b1 (n) ) \u2212 W * (p (n) ) \u2192 0.", "formula_coordinates": [9.0, 307.44, 280.8, 234.0, 35.24]}, {"formula_id": "formula_61", "formula_text": "G \u2032 \u2282 G with p G > 0 for G \u2208 G \u2032 . We have lim sup nj G\u2208G p G \u03d5(\u03b1 (nj ) , G) = lim sup nj G\u2208G \u2032 p (nj ) G \u03d5(\u03b1 (nj ) , G) \u2264 lim nj G\u2208G p (nj ) G \u03d5(\u03b1 (nj ) , G) = W * (p).", "formula_coordinates": [9.0, 307.44, 367.34, 248.19, 82.6]}, {"formula_id": "formula_62", "formula_text": "\u2113(p (n) , \u03b1 (n) ) \u2212 inf \u03b1 \u2113(p (n) , \u03b1) \u2265 \u03b5.", "formula_coordinates": [9.0, 307.44, 513.14, 147.9, 11.75]}, {"formula_id": "formula_63", "formula_text": "(n) i \u2212 \u03b1 (n)", "formula_coordinates": [10.0, 101.6, 354.53, 43.71, 14.69]}, {"formula_id": "formula_64", "formula_text": "(n) 1 \u2212 \u03b1 (n) 3 ) = \u221e.", "formula_coordinates": [10.0, 215.94, 370.06, 73.5, 14.8]}, {"formula_id": "formula_65", "formula_text": "(n) 3 \u2212 \u03b1 (n) 1 ) = \u221e.", "formula_coordinates": [10.0, 203.52, 395.91, 85.92, 14.8]}, {"formula_id": "formula_66", "formula_text": "\u03b1 i \u2265 \u03b1 l1 \u2265 \u2022 \u2022 \u2022 \u2265 \u03b1 l k \u2265 \u03b1 j \u2265 \u03b1 l \u2032 1 \u2265 \u2022 \u2022 \u2022 \u2265 \u03b1 l \u2032 k \u2032 \u2265 \u03b1 i .", "formula_coordinates": [10.0, 314.95, 342.47, 218.97, 13.43]}], "doi": ""}