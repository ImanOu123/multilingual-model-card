{"title": "Understanding Dataset Difficulty with V-Usable Information", "authors": "Kawin Ethayarajh; Yejin Choi; Swabha Swayamdipta", "pub_date": "2022-06-15", "abstract": "Estimating the difficulty of a dataset typically involves comparing state-of-the-art models to humans; the bigger the performance gap, the harder the dataset is said to be. However, this comparison provides little understanding of how difficult each instance in a given distribution is, or what attributes make the dataset difficult for a given model. To address these questions, we frame dataset difficulty-w.r.t. a model V-as the lack of V-usable information (Xu et al., 2019), where a lower value indicates a more difficult dataset for V. We further introduce pointwise V-information (PVI) for measuring the difficulty of individual instances w.r.t. a given distribution. While standard evaluation metrics typically only compare different models for the same dataset, V-usable information and PVI also permit the converse: for a given model V, we can compare different datasets, as well as different instances/slices of the same dataset. Furthermore, our framework allows for the interpretability of different input attributes via transformations of the input, which we use to discover annotation artefacts in widely-used NLP benchmarks. 1  We use the terms \"V-usable information\" and \"V-information\" from Xu et al. (2019), interchangeably.", "sections": [{"heading": "Introduction", "text": "Datasets are designed to act as proxies for real-world tasks, yet most bear limited semblance to the tasks they purport to reflect (Torralba & Efros, 2011;Recht et al., 2019). Understanding dataset difficulty is therefore imperative to understanding progress in AI. In practice, however, estimating dataset difficulty is often limited to an informal comparison of state-of-the-art model performance to that of humans; the bigger the performance gap, the harder the dataset is said to be (Ethayarajh & Jurafsky, 2020;Ma et al., 2021). However,  such performance metrics offer little understanding of the differential difficulty of individual instances, or of which attributes in the input a given model finds useful.\nTo understand why a dataset is difficult, we extend recent work in information theory (Xu et al., 2019). To illustrate, consider a model family V that can learn to map a sentence X with its sentiment Y . Even if X were to be encrypted, the information X contains about Y would not be removed; in other words, the Shannon mutual information would be unchanged (Shannon, 1948). However, encryption makes predicting the sentiment a lot more difficult for V. But why? Intuitively, the task is easier when X is unencrypted because the information it contains is usable by V; when X is encrypted, the information still exists but becomes unusable. This quantity-V-usable information-reflects the ease with which V can predict Y given X. Xu et al. (2019) show that it can be measured using the predictive V-information framework, which generalizes Shannon information to consider computational constraints.\nOur work extends the above framework by framing dataset difficulty as the lack of V-usable information. 1 The higher \u2022 Some of the most difficult instances in SNLI and a popular grammaticality detection benchmark, CoLA (Warstadt et al., 2018), are mislabelled.\n\u2022 In a popular dataset for hate speech detection (Davidson et al., 2017), just 50 (potentially) offensive words contain most of the BERT-usable information about the label; less subtle bias may be going undetected.", "publication_ref": ["b54", "b42", "b12", "b31", "b58", "b48", "b58", "b56", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "V-Usable Information", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Background", "text": "Consider a model family V, which can be trained to map text input X to its label Y . If we encrypted the text, or translated it into a language with a very complex grammar, it would be harder to predict Y given X using the same V. How might we measure this increase in difficulty? Shannon (1948)'s mutual information I(X; Y ) is not an option-it would not change after X is encrypted, as it allows for unbounded computation, including any needed to decrypt the text.\nIntuitively, the task is easier when X is unencrypted because the information it contains is usable by V; when X is en- 2 Our code and data are available here.\ncrypted, this information still exists but becomes unusable. This quantity, called V-usable information, provides an estimate the difficulty of a dataset w.r.t. V. It can be measured under a framework called predictive V-information, which generalizes Shannon information to measure how much information can be extracted from X about Y when constrained to functions V, written as I V (X \u2192 Y ) (Xu et al., 2019). The greater the I V (X \u2192 Y ), the easier the dataset is for V. If V is the set of all functions-i.e., under unbounded computation-V-information reduces to Shannon information.\nProcessing the input with \u03c4 (e.g., by decrypting the text) can make prediction easier, allowing I V (\u03c4 (X) \u2192 Y ) \u2265 I V (X \u2192 Y ). Although this violates the data processing inequality, it explains the usefulness of certain types of processing, such as representation learning. Compared to X, the learned representations cannot have more Shannon information with Y , but they can have more usable information.", "publication_ref": ["b48", "b58"], "figure_ref": [], "table_ref": []}, {"heading": "Definitions", "text": "As defined in Xu et al. (2019): Definition 2.1. Let X, Y denote random variables with sample spaces X , Y respectively. Let \u2205 denote a null input that provides no information about Y . Given predictive family V \u2286 \u2126 = {f :\nX \u222a \u2205 \u2192 P (Y)}, the predictive V-entropy is H V (Y ) = inf f \u2208V E[\u2212 log 2 f [\u2205](Y )](1)\nand the conditional V-entropy is\nH V (Y |X) = inf f \u2208V E[\u2212 log 2 f [X](Y )](2)\nWe use log 2 to measure the entropies in bits of information, though one could also use log e and measure them in nats instead.\nPut simply, f [X] and f [\u2205] produce a probability distribution over the labels. The goal is to find the f \u2208 V that maximizes the log-likelihood of the label data with (Eq. 2) and without the input (Eq. 1). f [\u2205] models the label entropy, so \u2205 can be set to an empty string for most NLP tasks. Although predictive family has a technical definition 3 , most neural models, provided they are finetuned without any frozen parameters, easily meet this definition. Further, as per Xu et al. (2019): Definition 2.2. Let X and Y denote random variables with sample spaces X and Y, respectively. Given a predictive family V, the V-information is\nI V (X \u2192 Y ) = H V (Y ) \u2212 H V (Y |X)(3)\nBecause we are estimating this quantity on a finite dataset, the estimate can differ from the true V-information. Xu et al. (2019) provide PAC bounds for this error, where less complex V and larger datasets yield tighter bounds. Xu et al. (2019) also list several useful properties of V-information:\n\u2022 Non-Negativity: I V (X \u2192 Y ) \u2265 0 \u2022 Independence: If X is independent of Y , I V (X \u2192 Y ) = 0. \u2022 Montonicity: If U \u2286 V, then H U (Y ) \u2265 H V (Y ) and H U (Y |X) \u2265 H V (Y |X).\nTraining with the cross-entropy loss finds the f \u2208 V that maximizes the log-likelihood of Y given X (Xu et al., 2019). Thus, H V (Y |X) can be easily computed by standard training or by finetuning a pre-trained model. 4 We estimate\nH V (Y |X) by calculating E[\u2212 log f [X](Y )\n] on an identically distributed held-out set, where Y is the gold label.\nSince training with cross-entropy ultimately aims to find the infimum over the data distribution, not just the training set, it is important not to overfit the model to the training instances; this is of added significance for estimating H V (Y |X). We estimate H V (Y ) by training or finetuning another model where X is replaced by \u2205, intended to fit the label distribution. As such, computing V-information involves training or finetuning only two models.", "publication_ref": ["b58", "b58", "b58", "b58", "b58"], "figure_ref": [], "table_ref": []}, {"heading": "Assumptions", "text": "Implicit in estimating the V-information is the assumption that the data used to find the optimal f \u2208 V and the data used to estimate H V (Y |X) are identically distributed, since V-information is ultimately a function of two random variables X, Y . This dependence on the data distribution makes V-information well-suited for estimating and interpreting dataset difficulty. However, it is still possible to estimate the difficulty of sub-populations or subsets of the data, though it would be imprecise to refer to this measure as V-information (see \u00a73.1 for details). We also assume that the difference between the empirical V-information (calculated using some finite dataset) and the true V-information (calculated over the distributions) is negligible, though this may not hold, for example, if the dataset is too small (see Appendix A).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Implications", "text": "V-usable information allows us to compare i. different models V by computing I V (X \u2192 Y ) for the same X, Y (Fig. 2), ii. different datasets {(x, y)} by computing I\nV (X \u2192 Y )\nfor the same V (Fig. 1), and iii. different input variables X i by computing I V (X i \u2192 Y ) for the same V and Y (Fig. 4; \u00a74). Comparing the V-usable information estimate to accuracy in SNLI. In the first three epochs, estimates on the test set are similar across all models (top), but due to over-fitting, the estimates diverge and decline. The test accuracy (bottom) for each model loosely tracks the V-information estimate for that model, since extracting information makes prediction easier.\nWhile common classification metrics, such as accuracy or F 1 score, are often used for the above comparisons, V-usable information offers a theoretically rigorous framework, making it better suited for interpretability. The V-usable information is measured in bits / nats (depending on the log base), allowing for standardized comparisons across models and datasets. Additionally, consider the case where X and Y are independent: here, model accuracy would be no greater than the majority class frequency, but this frequency varies across datasets. V-information avoids this problem by factoring in the label entropy H V (Y ); if X, Y are independent, then the V-information is provably zero.\nSay we wish to compare two predictive families, V and U, such that U \u2286 V. Assuming both families can model the label distribution, the task will at least as easy for the larger family. This provably obviates the need to evaluate simpler function families (e.g., linear functions) when estimating dataset difficulty. Our experiments show that this bears out in practice as well (Appendix B).", "publication_ref": [], "figure_ref": ["fig_2", "fig_1"], "table_ref": []}, {"heading": "V-Usable Information in Practice", "text": "We consider the natural language inference (NLI) task, which involves predicting whether a text hypothesis entails, contradicts or is neutral to a text premise. We first apply the V-information framework to estimate the difficulty of a largescale NLI dataset, Stanford NLI (SNLI; Bowman et al., 2015), across different state-of-the-art models. The four models we use are GPT2-small (Radford et al., 2019), BERT-base-cased (Devlin et al., 2019), DistilBERT-baseuncased (Sanh et al., 2019), and BART-base (Lewis et al., 2020). Figure 2 shows the V-information estimate for all four, as well as their accuracy on the SNLI train and heldout (test) sets, across 10 training epochs. See Appendix B for results with larger models.\nModel performance tracks V-information. As seen in Figure 2, the model with the most V-information on the SNLI test set is also the most accurate. This is intuitive, since extracting more information makes prediction easier.\nOverall, BART-base extracts the most V-information, followed by BERT-base, DistilBERT-base, and GPT2-small; accuracy follows the same trend.\nV-information is more sensitive to over-fitting than heldout performance. At epoch 10, the V-information is at its lowest for all models, although the SNLI test accuracy has only declined slightly from its peak, as seen in Figure 2. This is because the models start becoming less certain about the correct label long before they start predicting the wrong label. This causes H V (Y |X) to rise-and thus I V (X \u2192 Y ) to decline-even while most of the probability mass is still placed on the correct label. This suggests that, compared to performance metrics like test accuracy, V-information can more readily inform us of over-fitting.\nDifferent datasets for the same task can have different amounts of V-usable information. We consider the MultiNLI dataset (Williams et al., 2018), a multi-genre counterpart of SNLI. Despite both being proxies for the NLI task, SNLI and MultiNLI have significantly different amounts of BERT-usable information, as shown in Figure 1. The V-information framework provides a principled means of measuring this difference in levels of difficulty; MultiNLI is expected to be more difficult than SNLI due to the diversity of genres it considers. Also shown is CoLA (Warstadt et al., 2018), a dataset for linguistic acceptability where each sentence is labeled as grammatical or not; this task is seemingly more difficult than NLI for BERT.", "publication_ref": ["b3", "b41", "b6", "b45", "b28", "b57", "b56"], "figure_ref": ["fig_2", "fig_2", "fig_2", "fig_1"], "table_ref": []}, {"heading": "Measuring Pointwise Difficulty", "text": "While V-information provides an aggregate measure of dataset difficulty ( \u00a72), a closer analysis requires measuring the degree of usable information in individual instances (w.r.t. a given distribution). We extend the V-information framework to introduce a new measure called pointwise V-information (PVI) for individual instances. The higher the PVI, the easier the instance is for V, under the given distribution.\nDefinition 3.1 (Pointwise V-Information). Given random variables X, Y and a predictive family V, the pointwise V-information (PVI) of an instance (x, y) is\nPVI(x \u2192 y) = \u2212 log 2 g[\u2205](y) + log 2 g [x](y) (4) where g \u2208 V s.t. E[\u2212 log g[\u2205](Y )] = H V (Y ) and g \u2208 V s.t. E[\u2212 log g [X](Y )] = H V (Y |X).\nIf V were, for instance, the BERT function family, g and g would be the models after finetuning BERT with and without the input respectively. For a held-out instance (x, y), PVI(x \u2192 y) is the difference in the log-probability these models place on the gold label. PVI is to V-information what PMI is to Shannon information:\nI(X; Y ) = E x,y\u223cP (X,Y ) [PMI(x, y)] I V (X \u2192 Y ) = E x,y\u223cP (X,Y ) [PVI(x \u2192 y)](5)\nGiven this relationship, our understanding of V-information extends to PVI as well: higher PVI instances are easier for V and vice-versa. A higher PVI increases the odds of being predicted correctly-this is intuitive because a correct prediction of a non-majority-class instance requires that some information be extracted from the instance. Although the V-information cannot be negative, the PVI can be-much like how PMI can be negative even though Shannon information cannot. A negative PVI simply means that the model is better off predicting the majority class than considering X, which can happen for many reasons (e.g., mislabelling). Examples with negative PVI can still be predicted correctly, as long as g places most of the probability mass on the correct label. Algorithm 1 shows our computation of PVI and V-information (by averaging over PVI).\nThe PVI of an instance (x, y) w.r.t. V should only depend on the distribution of the random variables. Sampling more from P (X, Y ) during finetuning should not change PVI(x \u2192 y). However, an instance can be drawn from different distributions, in which case we would expect its PVI to differ. For example, say we have restaurant reviews and movie reviews, along with their sentiment. The instance (\"That was great!\", positive) could be drawn from either distribution, but we would expect its PVI to be different in each (even though V is the same).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Implications", "text": "In addition to the comparisons that V-information allows us to make ( \u00a72.4), PVI allows us to compare: iv. different instances (x, y) by computing PVI (x \u2192 y) for the same X, Y, V (Tables 1, 4; Fig. 11) v. different slices or subsets of the data by computing the average PVI over instances in each slice (Table 2; Fig. 5).\nAlgorithm 1 After finetuning on a dataset of size n, the V-information and PVI can be calculated in O(n) time.\nInput: training data Dtrain = {(input xi, gold label yi)} m i=1 , held- out data Dtest = {(input xi, gold label yi)} n i=1 , model V do g \u2190 Finetune V on Dtrain \u2205 \u2190 empty string (null input) g \u2190 Finetune V on {(\u2205, yi) | (xi, yi) \u2208 Dtrain} HV (Y ), HV (Y |X) \u2190 0, 0 for (xi, yi) \u2208 Dtest do HV (Y ) \u2190 HV (Y ) \u2212 1 n log 2 g[\u2205](yi) HV (Y |X) \u2190 HV (Y |X) \u2212 1 n log 2 g [xi](yi) PVI(xi \u2192 yi) \u2190 \u2212 log 2 g[\u2205](yi) + log 2 g [xi](yi) end for IV (X \u2192 Y ) = 1 n i PVI(xi \u2192 yi) = HV (Y ) \u2212 HV (Y |X) end do\nNote that the average PVI of a slice of data is not its Vinformation, since we optimize the model w.r.t. the entire distribution. However, since in practice one often wishes to understand the relative difficulty of different subpopulations w.r.t. the training distribution, calculating the average PVIas opposed to the V-information of the subpopulation itselfis more useful.", "publication_ref": [], "figure_ref": ["fig_1", "fig_3"], "table_ref": ["tab_3"]}, {"heading": "PVI in Practice", "text": "PVI can be used to find mislabelled instances. Correctly predicted instances have higher PVI values than incorrectly predicted ones. For the held-out sets in SNLI, MultiNLI and CoLA, the difference in mean PVI between instances correctly and incorrectly predicted by BERT-base is 3.03, 2.87, and 2.45 bits respectively. These differences are statistically significant (p < 0.001). Table 1 shows the most difficult (lowest PVI) instances from CoLA; we further find that some of these are in fact mislabelled (see Appendix C for an analysis of SNLI).\nThe PVI threshold at which predictions become incorrect is similar across datasets. In Figure 3, we plot the PVI distribution of correctly and incorrectly predicted instances in each dataset. As expected, high-PVI instances are predicted correctly and low-PVI instances are not. Notably, the point at which instances start being incorrectly predicted is similar across datasets (PVI \u2248 0.5). Such a pattern could not be observed with a performance metric because the label spaces are different, evincing why the V-information framework is so useful for cross-dataset comparison.\nPVI estimates are highly consistent across models, training epochs, and random initializations. The crossmodel Pearson correlation between PVI estimates of SNLI instances is very high (r > 0.80). However, the cross-model Pearson correlation is lower for CoLA (0.40 < r < 0.65); see Fig. 9 in Appendix D. This is because, as visualized in", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sentence", "text": "Label PVI", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Wash you!", "text": "No -4.616 Who achieved the best result was Angela.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "No -4.584", "text": "Sue gave to Bill a book.\nNo -3.649 Only Churchill remembered Churchill giving the Blood, Sweat and Tears speech.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "No -3.571", "text": "Cynthia chewed.\nNo -3.510 It is a golden hair.\nYes -3.251 I won't have some money.\nNo -3.097 You may pick every flower, but leave a few for Mary.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "No -2.875", "text": "I know which book Mag read, and which book Bob said that you hadn't.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Yes -2.782", "text": "John promise Mary to shave himself.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Yes -2.609", "text": "Table 1. The 10 hardest (lowest PVI) instances in the CoLA indomain test set for grammaticality detection (label indicates grammaticality), according to BERT-base. Examples in red are assessed to be mislabelled by authors of this work. For e.g., 'Cynthia chewed.' might be grammatical because the verb 'chew' could be intransitive in this usage. This suggests that PVI could be used to identify mislabelled examples. All of these examples were predicted incorrectly by BERT-base.\nFigure 1, CoLA has less usable information, making difficulty estimates noisier. In the limit, if a dataset contained no usable information, then we would expect the correlation between PVI estimates across different models and seeds to be close to zero. It is also worth noting, however, that a high degree of cross-model correlation-as with SNLI-does not preclude comparisons between different models on the same dataset. Rather, it suggests that in SNLI, a minority of instances is responsible for distinguishing one model's performance from another. This is not surprising-given the similar complexity and architecture of these models, we would expect most instances to be equally easy. Moreover, despite the performance of Transformer-based models varying across random initializations (Dodge et al., 2019;Mosbach et al., 2020), we find that PVI estimates are quite stable: the correlation across seeds is r > 0.85 (for SNLI finetuned BERT-base, across 4 seeds); see Table 6 in Appendix D. It also concurs with human judgments of difficulty; see Fig. 10 in Appendix D.", "publication_ref": ["b9", "b33"], "figure_ref": ["fig_1", "fig_1"], "table_ref": ["tab_5"]}, {"heading": "Uncovering Dataset Artefacts", "text": "A key limitation of standard evaluation metrics (e.g. accuracy) is the lack of interpretability-there is no straightforward way to understand why a dataset is as difficult as it is. V-usable information offers an answer by allowing comparison of different input variables X i under the same V and Y , as implicated in \u00a72.4. We consider two approaches for this: applying input transformations ( \u00a74.1), and slicing Figure 3. The distribution of PVI for correctly and incorrectly predicted instances in each dataset. Note that the point at which instances start being incorrectly predicted is similar across datasets (\u223c 0.5 bits). In contrast, because the label space is different across CoLA and the other two datasets, such a comparison could not be made with a performance-based metric.\nthe dataset ( \u00a74.2).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Input Transformations", "text": "Our first approach involves applying different transformations \u03c4 i (X) to isolate an attribute a, followed by calculating I V (\u03c4 i (X) \u2192 Y ) to measure how much information (usable by V) the attribute contains about the label. For example, by shuffling the tokens in X, we can isolate the influence of the word order attribute.\nGiven that a transformation may make information more accessible (e.g., decrypting some encrypted text; c.f. \u00a72), it is possible for I V (\u03c4 i (X) \u2192 Y ) \u2265 I V (X \u2192 Y ), so the latter shouldn't be treated as an upper bound. Such transformations were applied by O' Connor & Andreas (2021) to understand what syntactic features Transformers use in next-token prediction; we take this a step further, aiming to discover annotation artefacts, compare individual instances, and ultimately understand the dataset itself. We present our findings on SNLI, CoLA, as well as DWMW17 (Davidson et al., 2017), a dataset for hate speech detection, where input posts are labeled as hate speech, offensive, or neither.\nWe apply transformations to the SNLI input to isolate different attributes (see Appendix E for an example): shuffled (shuffle tokens randomly), hypothesis-only (only include the hypothesis), premise-only (only include the premise), Figure 4. The amount of V-usable information contained in different input attributes about the gold labels in SNLI. The token identity alone (regardless of order) provides most of the information for all models (see SHUFFLED). The PREMISE, which can be shared by multiple instances, is useless alone; the HYPOTHE-SIS, which is unique to an instance, is quite useful even without a premise, suggesting it may contain annotation artefacts.\noverlap (tokens in both the premise and hypothesis).\nToken identity alone provides most of the usable information in SNLI. Figure 4 shows that the token identity alone-isolated by shuffling the input-contains most of the usable information for all models. The premise, which is often shared by multiple instances, is useless alone; the hypothesis, which is unique to an instance, is useful even without a premise. This corroborates the well-known annotation artefacts in SNLI (Gururangan et al., 2018;Poliak et al., 2018), which are spurious correlations exploited by models to predict the correct answer for the wrong reasons.\nHate speech detection might have lexical biases. Automatic hate speech detection is an increasingly important part of online moderation, but what causes a model to label speech as offensive? We find that in DWMW17, the text contains 0.724 bits of BERT-usable information about the label. Additionally, if one removed all the tokens, except for 50 (potentially) offensive ones-comprising common racial and homophobic slurs 5 -from the input post hoc, there still remains 0.490 bits of BERT-usable information. In other words, just 50 (potentially) offensive words contain most of the BERT-usable information in DWMW17. Our findings corroborate prior work which shows that certain lexical items (e.g., swear words, identity mentions) are responsible for hate speech prediction (Dixon et al., 2018;Dinan et al., 2019). Allowing models to do well by simply pattern-matching may permit subtleties in hate speech to go undetected, perpetuating harm towards minority groups (Blodgett et al., 2020 The average amount of usable information (i.e., mean PVI, in bits) that each attribute contains about each class in SNLI, according to BERT-base. Some attributes are more useful for a particular class: e.g., the degree of premise-hypothesis overlap is most useful for predicting 'entailment'. Note that the mean PVI for a particular class is different from the V-information.", "publication_ref": ["b5", "b16", "b40", "b8", "b7", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Slicing Datasets", "text": "Certain attributes are more useful for certain classes.\nComparing the usefulness of an attribute across classes can be useful for identifying systemic annotation artefacts. This can be done by simply averaging the PVI over the slice of data whose difficulty we are interested in measuring. Note that the equivalence between V-information and expected PVI only holds when the model used to estimate PVI is trained over the entire dataset, which means that the average PVI of a slice of data is not its V-information. It would not make sense to estimate the V-information of a slice because it would require training on examples from just one class, in which case the V-information would be zero. Thus the only usable difficulty measure is the mean PVI.\nWe do this for SNLI in Table 2. We see that the tokens in the premise-hypothesis overlap contains much more BERTusable information about the 'entailment' class than 'contradiction' or 'neutral'. This is unsurprising, given that the simplest means of entailing a premise is to copy it into the hypothesis and provide some additional detail. If there is no inherent reason for an attribute to be more/less usefulsuch as overlap for entailment-there may be an artefact at work. Even when there is an inherent reason for an attribute to be useful for a particular slice of the data, an attribute being exceptionally useful may also be evidence of a dataset artefact. For example, if the premise-overlap hypothesis provided almost all the usable information needed for entailment, it may be because the crowdworkers who created the dataset took a shortcut by copying the premise to create the hypothesis.\nIn Appendix F, we show how similar comparisons can be made between instances.\nCertain subsets of each class are more difficult than others. In Figure 5, we bin the examples in each SNLI class by the level of hypothesis-premise overlap and plot the average PVI. We see entailment instances with no hypothesispremise overlap are the most difficult (i.e., lowest mean PVI) while contradiction instances with no overlap are the easiest (i.e., highest mean PVI). This is not surprising, since annotation artefacts in SNLI arise from constructing entailment and contradiction via trivial changes to the premise (Gururangan et al., 2018).\nWe additionally consider slices in the dataset based on dataset cartography (Swayamdipta et al., 2020), which uses training dynamics to differentiate instances via their (1) confidence (i.e., mean probability of the correct label across epochs), and (2) variability (i.e., variance of the former).\nThe result is a dataset map revealing three regions: easy-tolearn, hard-to-learn, and ambiguous w.r.t the trained model. Slices of the dataset based on cartographic regions have distinct ranges of average PVI (Fig. 13 in Appendix H).", "publication_ref": ["b16", "b52"], "figure_ref": ["fig_3", "fig_1"], "table_ref": []}, {"heading": "Token-level Artefacts", "text": "Transforming X and then measuring the V-information to discover all token-level signals and artefacts is untenable, since we would need to finetune one new model per token. Instead, we compute the change in the V-information estimate after removing t, which yields modified input x \u00act . We use the same model g but evaluate only on a slice of the data, D C,t , which contains the token t and belongs to the class C of interest. This simplifies to measuring the increase in conditional entropy:\n1 |D C,t | D C,t [ \u2212 log 2 g [x \u00act ](y) + log 2 g [x](y)]\nToken-level signals and artefacts can be discovered using leave-one-out. Table 3 shows that auxiliary verbs (e.g., be, did) and prepositions are most indicative of ungrammatical sentences in CoLA; in contrast, grammatical sentences have no strong indicators, with no word on av-WARNING: The following content contains language from the DWMW17 dataset that is offensive in nature.\nDWMW17 (Davidson et al., 2017) Hate Speech Offensive Neither Table 3. Token-level annotation artefacts in DWMW17 and CoLA. These are the tokens whose omission leads to the greatest average increase in conditional entropy for each class (given in parentheses). Note that certain racial slurs are correctly identified as 'hate speech' but in-group variants of the same terms fall under 'offensive' instead. The full lists are available in Appendix G.\nerage increasing the conditional entropy above 0.30 upon omission.\nIn DWMW17, racial and homophobic slurs are the top indicators of 'hate speech'. However, in-group variants of the same racial slur-commonly used in African-American Vernacular English (AAVE)-fall under 'offensive' instead.\nThe fact that AAVE terms are marked as 'offensive' supports previous findings by that hate speech detection datasets may themselves be biased (Sap et al., 2019). In SNLI, we found many of the token-level artefacts matching those found using descriptive statistics in Gururangan et al. (2018). The complete word lists are available in Appendix G.", "publication_ref": ["b5", "b46", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Conditioning Out Information", "text": "What if we wanted to measure how much BERT-usable information offensive words contain about the label in DWMW17 beyond that which is captured in the sentiment? In other words, if we already had access to the sentiment polarity of a text (positive/negative/neutral), how many additional bits of information would the offensive words provide? We cannot estimate this by simply subtracting I V (offensive \u2192 Y ) from I V (sentiment \u2192 Y ), since that difference could potentially be negative. Acquiring another random variable should not decrease the amount of information we have about the label (at worst, it should be useless).\nTo capture this intuition, Hewitt et al. (2021) proposed conditional V-information, which allows one to condition out any number of random variables. Given a set of random variables B that we want to condition out, it is defined as:\nI V (X \u2192 Y |B) = H V (Y |B) \u2212 H V (Y |B \u222a {X}) (6)\nThe conditional entropy with respect to multiple variables is the only new concept here. It is estimated in practice by concatenating the text inputs represented by B and X, which in our example is the sentiment polarity (one of 'negative'/'neutral'/'positive') and the sequence of offensive words in the input. The actual model family need not change to accommodate the longer text, as long as it remains under the input token limit. 6 We find that offensive words contain 0.482 bits of BERT-usable information about the label beyond that which is contained in text sentiment. 7 This is close to all of the BERT-usable information that the offensive words contain about the label (0.490 bits), suggesting that the predictive power of (potentially) offensive words is not mediated through sentiment in DWMW17.", "publication_ref": ["b18"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "While prior literature has acknowledged that not all data instances are equal (Vodrahalli et al., 2018;Swayamdipta et al., 2020), there have been few efforts to estimate dataset difficulty formally and directly. As a notable exception, Zhang et al. (2020) proposed DIME, an informationtheoretic measure to estimate a lower bound on the lowest possible (i.e., model-agnostic) 0-1 error. Model-agnostic approaches do not explain why some datasets are easier for some models, and have limited interpretability. In contrast, V-information and PVI are specific to a model family V.\nVarious techniques have been proposed to differentiate data instances within a dataset. Text-based heuristics such as word identity (Bengio et al., 2009) or input length (Spitkovsky et al., 2010;Gururangan et al., 2018) have sometimes been used as proxies for instance difficulty, but offer limited insight into difficulty w.r.t. models. Other approaches consider training loss (Han et al., 2018;Arazo et al., 2019;Shen & Sanghavi, 2019), confidence (Hovy et al., 2013), prediction variance (Chang et al., 2017), and area under the curve (Pleiss et al., 2020). Estimates relying on model training dynamics (Toneva et al., 2018;Swayamdipta et al., 2020), gradient magnitudes (Vodrahalli et al., 2018), or loss magnitudes (Han et al., 2018) are sensitive to factors such as variance during steps of training. Influence functions (Koh & Liang, 2017), forgetting events (Toneva et al., 2018), and the Data Shapley (Ghorbani & Zou, 2019;Jia et al., 2019) can all be used to assign pointwise estimates of importance to data instances based on their contribution to the decision boundary. Moreover, although these methods all capture some aspect of difficulty, they do not lend themselves to interpreting datasets as readily as the predictive V-information framework.\nGiven its dependence on training behavior across time, cartography (Swayamdipta et al., 2020) offers complementary benefits to V-information. It can be non-trivial to measure differences between, say a CoLA data map and an SNLI data map, w.r.t BERT. In contrast, V-information provides a formal framework to make dataset difficulty estimates as an aggregate to compare datasets w.r.t a model. Other work has offered insight by splitting the data into \"easy\" and \"hard\" sets with respect to some attribute and studying changes in model performance, but these methods do not offer a pointwise estimate of difficulty (Sugawara et al., 2018;Rondeau & Hazen, 2018;Sen & Saffari, 2020).\nItem response theory (IRT; Embretson & Reise, 2013) allows the difficulty of instances to be learned via parameters in a probabilistic model meant to explain model performance (Lalor et al., 2018;Rodriguez et al., 2021). However, it does not formally relate dataset difficulty to the model being evaluated. Estimating instance difficulty is also evocative of instance selection for active learning (Lewis & Catlett, 1994;Fu et al., 2013;Liu & Motoda, 2002); however these estimates could change as the dataset picks up new instances. In contrast, PVI estimates are relatively stable, especially when the dataset has higher V-information. Uncertainty sampling, for example, picks the instances that the partially trained model is least certain about (Lewis & Gale, 1994;Nigam et al., 2000), which could be interpreted as a measure of difficulty. However, once an instance is used for training, the model may become much more certain about it, meaning that the uncertainty values are unstable.\nInterpretability of the role of certain attributes in trained models have lately led to the discovery of many dataset artefacts in NLP. Our approach to discovering dataset artefacts can also complement existing approaches to artefact discovery (Gardner et al., 2021;Pezeshkpour et al., 2021;Le Bras et al., 2020). Rissanen data analysis (Perez et al., 2021) offers a complimentary method for interpretability w.r.t attributes; it involves calculating the minimum description length (MDL): how many bits are needed to transmit the gold labels from a sender to a recipient when both have access to the same model and inputs. Since the framework depends on the order of instances (i.e., what data has been transmitted thus far), it is unsuitable for estimating dataset difficulty. In contrast, V-information is defined w.r.t. a data distribution, so it is (in theory) agnostic to data and its ordering in fine-tuning.\nV-information (Xu et al., 2019)  as well as to condition out information for probing-based interpretability techniques (Hewitt et al., 2021;Pimentel & Cotterell, 2021). However, to the best of our knowledge, ours is the first approach to use V-usable information for estimating the difficulty of NLP datasets.", "publication_ref": ["b55", "b52", "b59", "b1", "b50", "b16", "b17", "b0", "b49", "b20", "b4", "b39", "b53", "b52", "b55", "b17", "b53", "b15", "b21", "b52", "b51", "b44", "b47", "b11", "b24", "b43", "b26", "b13", "b29", "b27", "b34", "b14", "b37", "b25", "b36", "b58", "b18", "b38"], "figure_ref": [], "table_ref": []}, {"heading": "Future Work", "text": "There has been much work in the way of model interpretability, but relatively little in the way of dataset interpretability. Our framework will allow datasets to be probed, helping us understand what exactly we are testing for in models and how pervasive annotation artefacts really are. By identifying the attributes responsible for difficulty, it will be possible to build challenge sets in a more principled way and reduce artefacts in existing datasets. By studying which attributes contain information that is unusable by existing SOTA models, model creators may make more precise changes to architectures. More immediate directions of future work include:\n1. Understanding how changes to the data distribution change the difficulty of individual examples.\n2. Extending V-information to open-ended text generation, which does not induce explicit distributions over the output space. This may requiring truncating the output space (e.g., using beam search with fixed width).\n3. Applying V-information to estimate dataset difficulty in other modalities (e.g., image, audio, tabular, etc.).\nThere is nothing limiting the use of V-information to the NLP domain. For example, one could create a set of image filters-for different colors and objects-use them to transform the image, and then measure the drop in usable information.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We provided an information-theoretic perspective to understanding and interpreting the difficulty of various NLP datasets. We extended predictive V-information to estimate difficulty at the dataset level, and then introduced pointwise V-information (PVI) for measuring the difficulty of individual instances. We showed that instances with lower PVI had lower levels of annotator agreement and were less likely to be predicted correctly. We then demonstrated how systemic and token-level annotation artefacts in a dataset could be discovered by manipulating the input before calculating these measures. Our studies indicate that V-information offers a new, efficient means of interpreting NLP datasets.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Training Data", "text": "In Figure 6, we plot the V-information estimate on the SNLI test set as BERT-base is trained on increasing amounts of training data. This is to test the assumption that the training set is sufficiently large to find the function f \u2208 V that minimizes the conditional entropy. Although this assumption is impossible to validate with complete certainty, since we don't have access to the true distribution, if the V-information estimate plateaus before all the training data is used, it suggests that the training set size is not a limiting factor to the estimation. We find that this is indeed the case with SNLI, where 80% of the training data on averages provides the same estimate as using the entire training set. In cases when this assumption does not hold, readers may want to consider measuring the Bayesian mutual information instead (Pimentel & Cotterell, 2021).\nFigure 6. The V-information estimate on the SNLI test set when BERT-base is trained on increasing fractions of the training data, drawn as a random sample (with replacement). Here we plot the average and standard deviation across four samples for each fraction. In Figure 7, we plot the V-information estimate for the SNLI test and train sets. In Figure 8, we plot the V-information estimate on the CoLA in-domain held-out set for the four models that we previously studied, as well as a larger model, RoBERTa-large (Liu et al., 2019). Despite the increase in scale, the trends observed in \u00a72 still hold. A man plays the trombone on the sidewalk. N -9.966 A woman in a striped shirt holds an infant.", "publication_ref": ["b38", "b30"], "figure_ref": ["fig_4", "fig_5"], "table_ref": []}, {"heading": "B. Larger Models", "text": "A person is watching TV. N -9.612 A person swimming in a swimming pool.\nA person embraces the cold N -9.152 Women enjoying a game of table tennis.\nWomen are playing ping pong. E -8.713 A boy dressed for summer in a green shirt and kahki shorts extends food to a reindeer in a petting zoo.\nA boy alien dressed for summer in a green shirt and kahki shorts E -8.486\nTwo skateboarders, one wearing a black t-shirt and the other wearing a white t-shirt, race each other.\nTwo snowboarders race. E -8.087\nAn Asian woman dressed in a colorful outfit laughing.\nThe woman is not laughing. E -7.903\nAn older gentleman looks at the camera while he is building a deck.\nAn older gentleman in overalls looks at the camera while he is building a stained red deck in front of a house.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E -7.709", "text": "A man wearing black pants, an orange and brown striped shirt, and a black bandanna in a \"just thrown a bowling ball\" stance.\nThe bandana is expensive. C -7.685\nTwo girls kissing a man with a black shirt and brown hair on the cheeks.\nTwo girls kiss. C -7.582 In Table 4, we list the 10 hardest instances in the SNLI test set according to BERT-base. All three classes-entailment, neutral, and contradiction-are represented in this list, with entailment being slightly over-represented. We see that some of the examples are in fact mislabelled-e.g., 'PREMISE: An Asian woman dressed in a colorful outfit laughing. HYPOTHESIS: The women is not laughing.' is labelled as 'entailment' even though the correct label is 'contradiction'.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "D. Consistency of PVI estimates", "text": "Cross-Model Correlations Figure 9 shows a heatmap for Cross-model Pearson's r between PVI estimates made by different finetuned models, on the SNLI and CoLA test sets; these results support the findings in \u00a72.5.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Di.BERT GPT2 BERT BART", "text": "Di.BERT Since CoLA has less usable information for all these models, the correlations are lower. All correlations are highly statistically significant (p < 0.001).\nFigure 10. Examples that human annotators find easier (as measured by the fraction of annotators, in the range [0.5, 1.0], that agree with the gold label) also have higher PVI on average.\nHuman Agreement In Figure 10, we plot the average PVI at different levels of annotator agreement. We find that there is a concurrence between what humans find difficult and what examples are difficult according to PVI.", "publication_ref": [], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "Cross-Epoch Correlations", "text": "In Table 5, we list the cross-epoch Pearson correlation between PVI estimates made by the same model on the SNLI test set over the course of finetuning. The correlation is high (r > 0.80 during the first 5 epochs), suggesting that when an instance is easy(difficult) early on, it tends to remain easy(difficult). ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "Cross-Seed Correlations In", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E. Transformations", "text": "WARNING: The following content contains language from the DWMW17 dataset that is offensive in nature.\nIn Table 7, we provide an instance from the SNLI test set in its original form and after various attribute-specific transformations have been applied to it. These only capture a small subset of the space of possible transformations.\nFor DWMW17, we hand-picked a set of 50 potentially offensive words based on a cursory review of the dataset to see how much information these terms alone contain about the label: 'nigga', 'niggas', 'niggah', 'niggahs', 'hoe', 'hoes', 'bitch ', 'bitches', 'whitey', 'white trash', 'cracker', 'crackers', 'beaner', 'beaners', 'pussy', 'pussies', 'fag', 'fags', 'faggot', 'faggots', 'ho', 'hos', 'redneck', 'rednecks', 'porn', 'fuck', 'fucks', 'fucker', 'fuckers', 'motherfucker', 'motherfuckers', 'nigger', 'niggers', 'coon', 'coons', 'niggaz', 'nig', 'nigs', 'slut', 'sluts', 'wigger', 'wiggers', 'fucked', 'fucking', 'wigga', 'wiggas', 'retard', 'retards', and 'retarded'.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F. Instance-wise Comparisons", "text": "Certain attributes are responsible for the difficulty of certain examples. Figure 11 is an example of how we might do a fine-grained comparison of instances to understand why one may be more difficult for a given model. We compare two SNLI 'neutral' instances from the test set to try to understand why #9627 is easier for BERT than #7717 (i.e., why PVI(x 9627 \u2192 y 9627 ) > PVI(x 7717 \u2192 y 7717 )), finding that it is likely due to the former's hypothesis being more informative. While different instances can be compared w.r.t. the same attribute, different attributes cannot be compared w.r.t. the same instance, since the models used to estimate the attribute-specific V-information I V (\u03c4 a (X) \u2192 Y ) are chosen to maximize the likelihood of all the data. This is why, for example, the PVI of #7717 is higher after its tokens have been shuffled even though the average PVI (i.e., dataset-level V-information) declines after shuffling tokens.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "G. Token-Level Artefacts", "text": "WARNING: The following content contains language from the DWMW17 dataset that is offensive in nature. In Table 7. Given an NLI instance (see 'Original'), each transformation isolates some attribute from the input. The headers 'PREMISE' and 'HYPOTHESIS' were added by us to transform the two sentence inputs into a single text input for all models that were evaluated.\nthe V-information estimate. These are token-level artefacts of each class in the dataset. In the DWMW17 hate speech detection dataset, racial and homophobic slurs are artefacts of hate speech, while ableist and sexual slurs are artefacts of offensive speech. In-group AAVE terms are also predictive of offensive speech in DWMW17 even when they are used non-offensively, hinting at possible bias in the dataset (Sap et al., 2019). In CoLA, auxiliary verbs and prepositions are artefacts of ungrammatical sentences; grammatical sentences don't have any artefacts. For SNLI, we recover many of the token-level artefacts found by Gururangan et al. (2018) using descriptive statistics-even uncommon ones, such as 'cat' for contradiction.  1) confidence (i.e., mean probability of the correct label across epochs), and (2) variability (i.e., variance of the former). The result is a dataset map revealing three regions:", "publication_ref": ["b46", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "H. Relation to Dataset Cartography", "text": "\u2022 Easy-to-learn (high confidence, low variability) instances are the most frequent instances in the dataset, those which high capacity models like BERT predict correctly throughout training.\n\u2022 Hard-to-learn (low confidence, low variability) instances correspond to those which are predicted incorrectly throughout training; these were shown to correspond to mislabeled examples often.\n\u2022 Ambiguous (high variability) instances correspond to those which the model often changes its prediction for; these examples are the ones which are most responsible for high test performance, both in and out of distribution.\nFigure 12 shows that PVI values track closely to the confidence axis of a SNLI-DistilBERT-base data map 8 (Swayamdipta et al., 2020). Data maps and PVI estimates offer orthogonal perspectives to instance difficulty, the former capturing behavior of instances as training proceeds. Moreover, V-information can estimate dataset difficulty as an aggregate ( \u00a72), which is not the case for training dynamics metrics, which offer only point estimates. Both approaches can be helpful for discovering Figure 11. The PVI of two SNLI 'neutral' instances (#7717 and #9627) w.r.t. BERT-base after attribute-specific transformations, as well as the V-information estimate (i.e., average PVI over the data) for each attribute. The latter instance is easier for BERT, likely because its hypothesis is much more informative due to being so different from its premise. Note that it makes sense to compare instances w.r.t. the same attribute, but not different attributes w.r.t. the same instance, since the models used to estimate the attribute V-information IV (\u03c4a(X) \u2192 Y ) are chosen to maximize the likelihood of all the data. data artefacts. Predictive V-information estimates, however, offer the unique capability of transforming the input to discover the value of certain attributes in an efficient manner.\nIn Figure 13, we report the average PVI estimates of the three regions discovered via data maps:\n\u2022 Easy-to-learn (high confidence, low variability) instances correspond to the highest average PVI, indicating that they have the highest amount of DistilBERT-usable information.\n\u2022 Hard-to-learn (low confidence, low variability) instances correspond to the lowest average PVI, indicating that they have the lowest amount of DistilBERT-usable information. This is not surprising, since they also correspond to mislabeled instances, which can be difficult to extract usable information from.\n\u2022 Ambiguous (high variability) instances correspond to lower average PVI, indicating that there is some usable information, but not as much as those of the easy-to-learn instances, w.r.t DistilBERT.\nFor each of the bars in the plot, we consider 10% of the dataset belonging to each region (with the highest corresponding measures of confidence and variability). . Average PVI of the 10% of the most ambiguous, and the 10% of the hardest-to-learn, and 10% of the easiest-to-learn regions of the SNLI / DistilBERT-base data map (Fig. 12). Hardto-learn instances are frequently mislabeled, and therefore also reflect the lowest average PVI values. The highest average PVI values are possessed by the easy-to-learn instances, which are the most common class of instances in the SNLI dataset. Ambiguous instances are those that the model changes it decision on frequently through training; these correspond to lower average PVI values than easy-to-learn instances.\nWARNING: The following content contains language from the DWMW17 dataset that is offensive in nature.\nDWMW17 (Davidson et al., 2017) Hate Speech Offensive Neither  (-0.195) be (1.895) of (-0.195) in (1.618) that (-0.379) did (1.558) the (-0.481)\nThe (1.427)\nTable 8. Token-level annotation artefacts in each dataset. These are the tokens whose omission leads to the greatest average increase in conditional entropy for each class (given in parentheses).", "publication_ref": ["b52", "b5"], "figure_ref": ["fig_1", "fig_1", "fig_1", "fig_1"], "table_ref": []}, {"heading": "Acknowledgements", "text": "We thank Dan Jurafsky, Nelson Liu, Daniel Khashabi, and the anonymous reviewers for their helpful comments.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Unsupervised label noise modeling and loss correction", "journal": "PMLR", "year": "2019", "authors": "E Arazo; D Ortego; P Albert; N O'connor; K Mcguinness"}, {"ref_id": "b1", "title": "Curriculum learning", "journal": "", "year": "2009", "authors": "Y Bengio; J Louradour; R Collobert; Weston ; J "}, {"ref_id": "b2", "title": "Language (technology) is power: A critical survey of \"bias\" in NLP", "journal": "", "year": "2020-07", "authors": "S L Blodgett; S Barocas; Iii Daum\u00e9; H Wallach; H "}, {"ref_id": "b3", "title": "A large annotated corpus for learning natural language inference", "journal": "", "year": "2015", "authors": "S Bowman; G Angeli; C Potts; C D Manning"}, {"ref_id": "b4", "title": "Active bias: Training more accurate neural networks by emphasizing high variance samples", "journal": "", "year": "2017", "authors": "H.-S Chang; E Learned-Miller; A Mccallum"}, {"ref_id": "b5", "title": "Automated hate speech detection and the problem of offensive language", "journal": "", "year": "2017", "authors": "T Davidson; D Warmsley; M Macy; I Weber"}, {"ref_id": "b6", "title": "Pre-training of deep bidirectional transformers for language understanding", "journal": "Association for Computational Linguistics", "year": "2019-06", "authors": "J Devlin; M.-W Chang; K Lee; K Toutanova;  Bert"}, {"ref_id": "b7", "title": "Build it break it fix it for dialogue safety: Robustness from adversarial human attack", "journal": "", "year": "2019-11", "authors": "E Dinan; S Humeau; B Chintagunta; Weston ; J "}, {"ref_id": "b8", "title": "Measuring and mitigating unintended bias in text classification", "journal": "", "year": "2018", "authors": "L Dixon; J Li; J Sorensen; N Thain; L Vasserman"}, {"ref_id": "b9", "title": "Show your work: Improved reporting of experimental results", "journal": "", "year": "2019", "authors": "J Dodge; S Gururangan; D Card; R Schwartz; N A Smith"}, {"ref_id": "b10", "title": "Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping", "journal": "", "year": "2020", "authors": "J Dodge; G Ilharco; R Schwartz; A Farhadi; H Hajishirzi; N Smith"}, {"ref_id": "b11", "title": "Item response theory", "journal": "Psychology Press", "year": "2013", "authors": "S E Embretson; S P Reise"}, {"ref_id": "b12", "title": "Utility is in the eye of the user: A critique of nlp leaderboards", "journal": "", "year": "2020", "authors": "K Ethayarajh; D Jurafsky"}, {"ref_id": "b13", "title": "A survey on instance selection for active learning. Knowledge and information systems", "journal": "", "year": "2013", "authors": "Y Fu; X Zhu; B Li"}, {"ref_id": "b14", "title": "Competency problems: On finding and removing artifacts in language data", "journal": "", "year": "2021", "authors": "M Gardner; W Merrill; J Dodge; M E Peters; A Ross; S Singh; N Smith"}, {"ref_id": "b15", "title": "Data shapley: Equitable valuation of data for machine learning", "journal": "PMLR", "year": "2019", "authors": "A Ghorbani; J Zou"}, {"ref_id": "b16", "title": "Annotation artifacts in natural language inference data", "journal": "", "year": "2018", "authors": "S Gururangan; S Swayamdipta; O Levy; R Schwartz; S Bowman; N A Smith"}, {"ref_id": "b17", "title": "Co-teaching: Robust training of deep neural networks with extremely noisy labels", "journal": "", "year": "2018", "authors": "B Han; Q Yao; X Yu; G Niu; M Xu; W Hu; I W Tsang; M Sugiyama"}, {"ref_id": "b18", "title": "Conditional probing: measuring usable information beyond a baseline", "journal": "", "year": "2021", "authors": "J Hewitt; K Ethayarajh; P Liang; C D Manning"}, {"ref_id": "b19", "title": "spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing", "journal": "", "year": "2017", "authors": "M Honnibal; I Montani"}, {"ref_id": "b20", "title": "Learning whom to trust with MACE", "journal": "Association for Computational Linguistics", "year": "2013-06", "authors": "D Hovy; T Berg-Kirkpatrick; A Vaswani; E Hovy"}, {"ref_id": "b21", "title": "Towards efficient data valuation based on the shapley value", "journal": "PMLR", "year": "2019", "authors": "R Jia; D Dao; B Wang; F A Hubis; N Hynes; N M G\u00fcrel; B Li; C Zhang; D Song; C J Spanos"}, {"ref_id": "b22", "title": "Understanding black-box predictions via influence functions", "journal": "", "year": "", "authors": "P W Koh; P Liang"}, {"ref_id": "b23", "title": "Verified uncertainty calibration", "journal": "", "year": "2019", "authors": "A Kumar; P Liang; T Ma"}, {"ref_id": "b24", "title": "Understanding deep learning performance through an examination of test set difficulty: A psychometric case study", "journal": "Association for Computational Linguistics", "year": "2018-11", "authors": "J P Lalor; H Wu; T Munkhdalai; Yu ; H "}, {"ref_id": "b25", "title": "Adversarial filters of dataset biases", "journal": "PMLR", "year": "2020", "authors": "Le Bras; R Swayamdipta; S Bhagavatula; C Zellers; R Peters; M Sabharwal; A Choi; Y "}, {"ref_id": "b26", "title": "Heterogeneous uncertainty sampling for supervised learning", "journal": "Elsevier", "year": "1994", "authors": "D D Lewis; J Catlett"}, {"ref_id": "b27", "title": "A sequential algorithm for training text classifiers", "journal": "Springer", "year": "1994", "authors": "D D Lewis; W A Gale"}, {"ref_id": "b28", "title": "Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension", "journal": "", "year": "2020", "authors": "M Lewis; Y Liu; N Goyal; M Ghazvininejad; A Mohamed; O Levy; V Stoyanov; L Zettlemoyer;  Bart"}, {"ref_id": "b29", "title": "On issues of instance selection", "journal": "Data Mining and Knowledge Discovery", "year": "2002", "authors": "H Liu; H Motoda"}, {"ref_id": "b30", "title": "A robustly optimized bert pretraining approach", "journal": "", "year": "2019", "authors": "Y Liu; M Ott; N Goyal; J Du; M Joshi; D Chen; O Levy; M Lewis; L Zettlemoyer; V Stoyanov;  Roberta"}, {"ref_id": "b31", "title": "Dynaboard: An evaluation-as-a-service platform for holistic next-generation benchmarking", "journal": "", "year": "2021", "authors": "Z Ma; K Ethayarajh; T Thrush; S Jain; L Wu; R Jia; C Potts; A Williams; D Kiela"}, {"ref_id": "b32", "title": "Explanation in artificial intelligence: Insights from the social sciences", "journal": "Artificial Intelligence", "year": "2019-02", "authors": "T Miller"}, {"ref_id": "b33", "title": "On the stability of fine-tuning bert: Misconceptions, explanations, and strong baselines", "journal": "", "year": "2020", "authors": "M Mosbach; M Andriushchenko; D Klakow"}, {"ref_id": "b34", "title": "Text classification from labeled and unlabeled documents using EM", "journal": "Machine learning", "year": "2000", "authors": "K Nigam; A K Mccallum; S Thrun; T Mitchell"}, {"ref_id": "b35", "title": "What context features can transformer language models use?", "journal": "Long Papers", "year": "2021-08", "authors": "J O'connor; J Andreas"}, {"ref_id": "b36", "title": "Rissanen data analysis: Examining dataset characteristics via description length", "journal": "", "year": "2021", "authors": "E Perez; D Kiela; K Cho"}, {"ref_id": "b37", "title": "Combining feature and instance attribution to detect artifacts", "journal": "", "year": "2021", "authors": "P Pezeshkpour; S Jain; S Singh; B C Wallace"}, {"ref_id": "b38", "title": "A bayesian framework for information-theoretic probing", "journal": "", "year": "2021", "authors": "T Pimentel; R Cotterell"}, {"ref_id": "b39", "title": "Identifying mislabeled data using the area under the margin ranking", "journal": "", "year": "2020", "authors": "G Pleiss; T Zhang; E R Elenberg; K Q Weinberger"}, {"ref_id": "b40", "title": "Hypothesis only baselines in natural language inference", "journal": "", "year": "2018-06", "authors": "A Poliak; J Naradowsky; A Haldar; R Rudinger; B Van Durme"}, {"ref_id": "b41", "title": "Language models are unsupervised multitask learners", "journal": "", "year": "2019", "authors": "A Radford; J Wu; R Child; D Luan; D Amodei; I Sutskever"}, {"ref_id": "b42", "title": "Do imagenet classifiers generalize to imagenet?", "journal": "PMLR", "year": "2019", "authors": "B Recht; R Roelofs; L Schmidt; V Shankar"}, {"ref_id": "b43", "title": "Evaluation examples are not equally informative: How should that change NLP leaderboards?", "journal": "Long Papers", "year": "2021-08", "authors": "P Rodriguez; J Barrow; A M Hoyle; J P Lalor; R Jia; J Boyd-Graber"}, {"ref_id": "b44", "title": "Systematic error analysis of the stanford question answering dataset", "journal": "", "year": "2018", "authors": "M.-A Rondeau; T J Hazen"}, {"ref_id": "b45", "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter", "journal": "", "year": "2019", "authors": "V Sanh; L Debut; J Chaumond; T Wolf"}, {"ref_id": "b46", "title": "The risk of racial bias in hate speech detection", "journal": "", "year": "2019", "authors": "M Sap; D Card; S Gabriel; Y Choi; N A Smith"}, {"ref_id": "b47", "title": "What do models learn from question answering datasets?", "journal": "", "year": "2020", "authors": "P Sen; A Saffari"}, {"ref_id": "b48", "title": "A mathematical theory of communication. The Bell system technical journal", "journal": "", "year": "1948", "authors": "C E Shannon"}, {"ref_id": "b49", "title": "Learning with bad training data via iterative trimmed loss minimization", "journal": "PMLR", "year": "2019", "authors": "Y Shen; S Sanghavi"}, {"ref_id": "b50", "title": "From baby steps to leapfrog: How \"less is more\" in unsupervised dependency parsing", "journal": "Association for Computational Linguistics", "year": "2010", "authors": "V I Spitkovsky; H Alshawi; D Jurafsky"}, {"ref_id": "b51", "title": "What makes reading comprehension questions easier?", "journal": "", "year": "2018", "authors": "S Sugawara; K Inui; S Sekine; A Aizawa"}, {"ref_id": "b52", "title": "Dataset cartography: Mapping and diagnosing datasets with training dynamics", "journal": "", "year": "2020", "authors": "S Swayamdipta; R Schwartz; N Lourie; Y Wang; H Hajishirzi; N A Smith; Y Choi"}, {"ref_id": "b53", "title": "An empirical study of example forgetting during deep neural network learning", "journal": "", "year": "2018", "authors": "M Toneva; A Sordoni; R T Des Combes; A Trischler; Y Bengio; Gordon ; G J "}, {"ref_id": "b54", "title": "Unbiased look at dataset bias", "journal": "IEEE", "year": "2011", "authors": "A Torralba; A A Efros"}, {"ref_id": "b55", "title": "Are all training examples created equal? an empirical study", "journal": "", "year": "2018", "authors": "K Vodrahalli; K Li; J Malik"}, {"ref_id": "b56", "title": "Neural network acceptability judgments", "journal": "", "year": "2018", "authors": "A Warstadt; A Singh; S R Bowman"}, {"ref_id": "b57", "title": "A broadcoverage challenge corpus for sentence understanding through inference", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "A Williams; N Nangia; S Bowman"}, {"ref_id": "b58", "title": "A theory of usable information under computational constraints", "journal": "", "year": "2019", "authors": "Y Xu; S Zhao; J Song; R Stewart; S Ermon"}, {"ref_id": "b59", "title": "DIME: An Information-Theoretic difficulty measure for AI datasets", "journal": "", "year": "2020-10", "authors": "P Zhang; H Wang; N Naik; C Xiong; R Socher"}, {"ref_id": "b60", "title": "HYPOTHESIS: The kids are fighting outside. #9627: PREMISE: A group of people watching a boy getting interviewed by a man. HYPOTHESIS: A group of people are sleeping on Pluto", "journal": "", "year": "", "authors": ""}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Work done during an internship at AI2. 1 Stanford University 2 Allen Institute for Artificial Intelligence 3 Paul G. Allen School of Computer Science, University of Washington. Correspondence to: Kawin Ethayarajh <kawin@stanford.edu>. Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s).", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 .1Figure1. The Stanford NLI dataset contains more BERT-usable information than the MultiNLI and CoLA datasets, making it easier for BERT-base. Above, the distribution of instance difficulty (PVI) in the held-out sets for each; dotted lines denote the average PVI.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 .2Figure2. Comparing the V-usable information estimate to accuracy in SNLI. In the first three epochs, estimates on the test set are similar across all models (top), but due to over-fitting, the estimates diverge and decline. The test accuracy (bottom) for each model loosely tracks the V-information estimate for that model, since extracting information makes prediction easier.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 .5Figure5. The mean PVI of SNLI instances according to BERTbase, broken down by the overlap length (i.e., the number of tokens shared by the hypothesis and premise). Entailment examples with no overlap are the most difficult (i.e., lowest mean PVI).", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 7 .7Figure 7. Comparing accuracy and the V-information estimate on the SNLI train and test set w.r.t. various models.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 8 .8Figure 8. Comparing accuracy and the V-information estimate on the CoLA in-domain train and held-out set w.r.t. various models.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Swayamdipta et al. (2020) introduced dataset cartography, a method to automatically analyze and diagnose datasets with respect to a trained model. It offers a complimentary understanding of datasets and their properties, taking into account the behavior of a model towards different data instances during training. This behavior-training dynamics-helps differentiate instances via their (", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 12 .12Figure 12. Relationship between PVI and the training dynamicsbased data map (Swayamdipta et al., 2020) for SNLI held-out (test) set, computed for the DistilBERT-base architecture. As in Swayamdipta et al. (2020), Y -axis corresponds to confidence, i.e. the mean probabilities of the true class across training epochs, and X-axis corresponds to variability, i.e. the standard deviation of the true class probabilities across the same. Colours indicate binned values of PVI. PVI estimates track closely with confidence.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": ").", "figure_data": "Entailment Neutral Contradictionoriginal1.1881.0641.309shuffled1.1300.9841.224hypothesis only0.5730.5530.585premise only0.032-0.016-0.016overlap0.4150.1770.298Table 2."}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "The 10 hardest (lowest PVI) instances in the SNLI test set, according to BERT-base. 'E' denotes entailment, 'N' neutral, and 'C' contradiction. Instances that are possibly mislabelled are colored red.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Figure 9. Cross-model Pearson's r between PVI estimates made by different finetuned models, on the SNLI and CoLA test sets. For SNLI, the estimates are consistent: what one model finds difficult, others find difficult as well.", "figure_data": "SNLI1.000CoLA Di.BERT GPT2 BERT BART1.01.000 0.821 0.846 0.8150.9751.000 0.466 0.629 0.5210.9GPT2 BERT BART0.821 1.000 0.809 0.822 0.846 0.809 1.000 0.832 0.815 0.822 0.832 1.0000.825 0.850 0.875 0.900 0.925 0.9500.466 1.000 0.322 0.416 0.629 0.322 1.000 0.446 0.521 0.416 0.446 1.0000.4 0.5 0.6 0.7 0.8"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_5", "figure_caption": ", we list the Pearson correlation between PVI estimates made by BERT across different training runs. The correlation is high (r > 0.87), suggesting that what a model finds difficult is not due to chance.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Cross-epoch  Pearson correlation between PVI estimates made on the SNLI test set while finetuning various models on the SNLI training set. The estimates are stable: when an instance is easy(difficult) early on, it generally remains easy(difficult). For all models studied, the cross-epoch correlation does not dip below 0.80 for the first five epochs.", "figure_data": "BERT-baseEpoch/Epoch12351011.000 0.908 0.871 0.838 0.76220.908 1.000 0.929 0.883 0.79530.871 0.929 1.000 0.879 0.79650.838 0.883 0.879 1.000 0.833100.762 0.795 0.796 0.833 1.000BART-baseEpoch/Epoch12351011.000 0.925 0.885 0.853 0.75420.925 1.000 0.952 0.906 0.80730.885 0.952 1.000 0.914 0.81450.853 0.906 0.914 1.000 0.862100.754 0.807 0.814 0.862 1.000DistilBERT-baseEpoch/Epoch12351011.000 0.928 0.884 0.828 0.76620.928 1.000 0.952 0.890 0.82530.884 0.952 1.000 0.900 0.81950.828 0.890 0.900 1.000 0.860100.766 0.825 0.819 0.860 1.000GPT2Epoch/Epoch12351011.000 0.931 0.887 0.855 0.74720.931 1.000 0.961 0.918 0.81330.887 0.961 1.000 0.933 0.82750.855 0.918 0.933 1.000 0.874100.747 0.813 0.827 0.874 1.000"}, {"figure_label": "86", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "we list the tokens in the SNLI, CoLA, and DWMW17 datasets that, when dropped out, cause the greatest decrease in Cross-model Pearson correlation between PVI estimates made after one epoch of finetuning BERT on SNLI with different seeds. Estimates are stable: what a model finds difficult is mostly not due to chance.", "figure_data": "seedRun 1 Run 2 Run 3 Run 4Run 11.000 0.8770.884 0.885Run 20.877 1.0000.887 0.882Run 30.884 0.8871.000 0.895Run 40.885 0.8820.895 1.000AttributeTransformationTransformed InputOriginalPREMISE: Two girls kissing a man with a black shirt and brown hair on the cheeks.HYPOTHESIS: Two girls kiss.Shuffledshuffle tokens randomlyPREMISE: girls two a kissing man with a black cheeks shirt and hair brown on the. HYPOTHESIS: kiss two . girlsHypothesis-only only include hypothesisHYPOTHESIS: Two girls kiss.Premise-onlyonly include premisePREMISE: Two girls kissing a man with a black shirt and brown hair on the cheeks.Overlaphypothesis-premise overlap PREMISE: Two girls [MASK] [MASK] [MASK] [MASK] [MASK] [MASK][MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] . HYPOTHESIS:Two girls [MASK] ."}], "formulas": [{"formula_id": "formula_0", "formula_text": "X \u222a \u2205 \u2192 P (Y)}, the predictive V-entropy is H V (Y ) = inf f \u2208V E[\u2212 log 2 f [\u2205](Y )](1)", "formula_coordinates": [2.0, 307.44, 373.28, 234.0, 44.42]}, {"formula_id": "formula_1", "formula_text": "H V (Y |X) = inf f \u2208V E[\u2212 log 2 f [X](Y )](2)", "formula_coordinates": [2.0, 349.76, 443.26, 191.68, 14.66]}, {"formula_id": "formula_2", "formula_text": "I V (X \u2192 Y ) = H V (Y ) \u2212 H V (Y |X)(3)", "formula_coordinates": [2.0, 350.3, 661.52, 191.14, 9.65]}, {"formula_id": "formula_3", "formula_text": "\u2022 Non-Negativity: I V (X \u2192 Y ) \u2265 0 \u2022 Independence: If X is independent of Y , I V (X \u2192 Y ) = 0. \u2022 Montonicity: If U \u2286 V, then H U (Y ) \u2265 H V (Y ) and H U (Y |X) \u2265 H V (Y |X).", "formula_coordinates": [3.0, 66.9, 135.97, 222.54, 57.47]}, {"formula_id": "formula_4", "formula_text": "H V (Y |X) by calculating E[\u2212 log f [X](Y )", "formula_coordinates": [3.0, 55.44, 249.55, 175.9, 9.65]}, {"formula_id": "formula_5", "formula_text": "V (X \u2192 Y )", "formula_coordinates": [3.0, 244.22, 641.97, 46.38, 9.65]}, {"formula_id": "formula_6", "formula_text": "PVI(x \u2192 y) = \u2212 log 2 g[\u2205](y) + log 2 g [x](y) (4) where g \u2208 V s.t. E[\u2212 log g[\u2205](Y )] = H V (Y ) and g \u2208 V s.t. E[\u2212 log g [X](Y )] = H V (Y |X).", "formula_coordinates": [4.0, 307.08, 115.76, 234.36, 43.24]}, {"formula_id": "formula_7", "formula_text": "I(X; Y ) = E x,y\u223cP (X,Y ) [PMI(x, y)] I V (X \u2192 Y ) = E x,y\u223cP (X,Y ) [PVI(x \u2192 y)](5)", "formula_coordinates": [4.0, 339.24, 250.11, 202.2, 24.9]}, {"formula_id": "formula_8", "formula_text": "Input: training data Dtrain = {(input xi, gold label yi)} m i=1 , held- out data Dtest = {(input xi, gold label yi)} n i=1 , model V do g \u2190 Finetune V on Dtrain \u2205 \u2190 empty string (null input) g \u2190 Finetune V on {(\u2205, yi) | (xi, yi) \u2208 Dtrain} HV (Y ), HV (Y |X) \u2190 0, 0 for (xi, yi) \u2208 Dtest do HV (Y ) \u2190 HV (Y ) \u2212 1 n log 2 g[\u2205](yi) HV (Y |X) \u2190 HV (Y |X) \u2212 1 n log 2 g [xi](yi) PVI(xi \u2192 yi) \u2190 \u2212 log 2 g[\u2205](yi) + log 2 g [xi](yi) end for IV (X \u2192 Y ) = 1 n i PVI(xi \u2192 yi) = HV (Y ) \u2212 HV (Y |X) end do", "formula_coordinates": [5.0, 55.44, 94.38, 235.49, 145.67]}, {"formula_id": "formula_9", "formula_text": "1 |D C,t | D C,t [ \u2212 log 2 g [x \u00act ](y) + log 2 g [x](y)]", "formula_coordinates": [7.0, 332.3, 616.37, 185.48, 27.42]}, {"formula_id": "formula_10", "formula_text": "I V (X \u2192 Y |B) = H V (Y |B) \u2212 H V (Y |B \u222a {X}) (6)", "formula_coordinates": [8.0, 320.94, 88.35, 220.5, 9.65]}], "doi": "10.1145/1553374.1553380"}