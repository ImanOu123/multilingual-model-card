{"title": "Abstract Visual Reasoning with Tangram Shapes", "authors": "Anya Ji; Noriyuki Kojima; Noah Rush; Alane Suhr; Wai Keen Vong; Robert D Hawkins; Yoav Artzi", "pub_date": "", "abstract": "We introduce KILOGRAM, a resource for studying abstract visual reasoning in humans and machines. Drawing on the history of tangram puzzles as stimuli in cognitive science, we build a richly annotated dataset that, with >1k distinct stimuli, is orders of magnitude larger and more diverse than prior resources. It is both visually and linguistically richer, moving beyond whole shape descriptions to include segmentation maps and part labels. We use this resource to evaluate the abstract visual reasoning capacities of recent multi-modal models. We observe that pre-trained weights demonstrate limited abstract reasoning, which dramatically improves with fine-tuning. We also observe that explicitly describing parts aids abstract reasoning for both humans and models, especially when jointly encoding the linguistic and visual inputs.", "sections": [{"heading": "Introduction", "text": "Reference is a core function of natural language that relies on shared conventions and visual concepts. For example, in English, a speaker may use the term dog to refer to a particular animal of the species canis familiaris, or, through abstraction, to an object with a less strongly conventionalized name, such as the shape at the top of Figure 1. A speaker might refer to such a shape as looking like a dog, and even point to its parts, like its head and tail, despite having few visual features in common with the ordinary referent.\nComprehension and generation of references are critical for systems to engage in natural language interaction, and have been studied extensively with focus on ordinary references (e.g., Viethen and Dale, 2008;Mitchell et al., 2010;FitzGerald et al., 2013;Mao et al., 2016;Yu et al., 2016), in contrast to the visual abstraction illustrated in Figure 1. We address this gap by adopting an influential paradigm for probing human coordination in the cognitive science literature: reference games with abstract tangram shapes (e.g. Clark and Wilkes-Gibbs, 1986;Fox Tree, 1999;Hawkins et al., 2020). Unlike photographs of natural objects, where there is often a single canonical label, tangrams are fundamentally ambiguous. While some shapes fall under strong existing conventions and elicit consensus about appropriate names (e.g., Figure 1, top), others are characterized by weaker conventions (e.g., Figure 1, bottom) and every speaker may arrive at a distinct but valid description (Zettersten and Lupyan, 2020;Hupet et al., 1991). While such diversity is a key consideration motivating their use as stimuli, existing behavioral studies have typically been limited to a relatively small set of 10-20 shapes, highly restricting the overall diversity of the stimulus class. It also limits their applicability for training and analyzing vision and language models, where significantly more data is necessary.\nIn this paper, we significantly expand this resource. We introduce KILOGRAM, 1 a large collec-tion of tangrams with rich language annotations. KILOGRAM dramatically improves on existing resources along two dimensions. First, we curate and digitize 1,016 shapes, creating a set that is two orders of magnitude larger than collections used in existing work. This set dramatically increases coverage over the full range of naming variability, providing a more comprehensive view of human naming behavior. Second, rather than treating each tangram as a single whole shape, our images are vector graphics constructed from the original component puzzle pieces. This decomposition enables reasoning about both whole shapes and their parts.\nWe use this new collection of digitized tangram shapes to collect a large dataset of textual descriptions, reflecting a high diversity of naming behaviors. While existing work has focused on naming the complete shape, we also ask participants to segment and name semantically meaningful parts. We use crowdsourcing to scale our annotation process, collecting multiple annotations for each shape, thereby representing the distribution of annotations it elicits, rather than a single sample. In total, we collect 13,404 annotations, each describing a complete object and its segmented parts.\nThe potential of KILOGRAM is broad. For example, it enables the data-driven scaling of studies of human interactions and models of whole-part reasoning in language and vision models. In this paper, we use KILOGRAM to evaluate the visual reasoning capacities of recent pre-trained multi-modal models, focusing on generalizing concepts to abstract shapes. We observe limited generalization of this type in pre-trained models, but significant improvements following fine-tuning with our data. We also see how explicitly referring to and visualizing parts can help reference resolution. Data and code, as well as a data viewer are available at: https://lil.nlp.cornell.edu/kilogram/.", "publication_ref": ["b55", "b39", "b18", "b36", "b57", "b10", "b19", "b23", "b58", "b27"], "figure_ref": ["fig_0", "fig_0", "fig_0", "fig_0"], "table_ref": []}, {"heading": "Background and Related Work", "text": "Abstract or ambiguous visual stimuli have been widely used to investigate how human partners coordinate when talking about things in the absence of strong naming conventions going back to Krauss and Weinheimer (1964). Tangrams as stimuli were introduced by Clark and Wilkes-Gibbs (1986). These shapes are all built from the same seven primitives, but elicit a wide range of figurative descriptions that conceptualize shapes in different ways (Schober and Clark, 1989;Hor-ton and Gerrig, 2002;Duff et al., 2006;Holler and Wilkin, 2011;Horton and Slaten, 2012;Ibarra and Tanenhaus, 2016;Shore et al., 2018;Atkinson et al., 2019;Castillo et al., 2019;Bangerter et al., 2020). It has been observed that some shapes are easier or harder to describe (Hupet et al., 1991;Zettersten and Lupyan, 2020;Brashears and Minda, 2020), a property known as nameability or codability, which has also been studied with non-tangram shapes (e.g., line drawings; Snodgrass and Vanderwart, 1980;Cycowicz et al., 1997;Du\u00f1abeitia et al., 2018). Even though diversity is a key consideration in working with tangrams, existing stimuli sets are relatively small, limiting their usefulness as NLP benchmarks, where scale is critical. Even the largest studies of variability in naming (e.g., Murfitt and McAllister, 2001) have used a relatively small set of 60 tangrams. Fasquel et al. (2022) present a resource that is related and complementary to ours, including 332 PNG-formatted tangrams with whole-shape naming annotations in French.\nContemporary pre-trained vision and language approaches can be categorized along an axis characterizing how they encode the data, from jointly encoding the two inputs (Lu et al., 2019;Chen et al., 2020; to encoding them separately (Radford et al., 2021;Jia et al., 2021). Joint encoding aims to capture tighter interaction between the input modalities compared to separate encoding, but is generally more computationally expensive, and can only operate on multi-modal input. We study recent models on both ends: ViLT  for joint encoding and CLIP (Radford et al., 2021) for separate encoding.\nThese models are typically evaluated on image captioning (e.g., Chen et al., 2015) or visual question answering (e.g., Antol et al., 2015) benchmarks. Several benchmarks, such as NLVR (Suhr et al., 2017(Suhr et al., , 2019 and Winoground (Thrush et al., 2022), aim for more focused evaluations with a focus on compositionality. We build on these efforts, but target generalization through abstraction using visually ambiguous stimuli. This is inspired by the role of abstraction in human cognition. Abstraction is a key step in human perception (Biederman, 1987) that is critical for generalization (Gentner and Markman, 1997;Medin et al., 1993;Shepard, 1987), and forms the shared foundation on which human language communication is layered (Lupyan and Winter, 2018;McCarthy et al., 2021;Wong et al., 2022). Our focus on part de- composition is aligned with how part identification plays an important role in human abstraction (Tversky and Hemenway, 1984).", "publication_ref": ["b31", "b10", "b44", "b12", "b24", "b26", "b28", "b47", "b1", "b7", "b2", "b27", "b58", "b6", "b49", "b11", "b40", "b17", "b34", "b9", "b43", "b43", "b8", "b0", "b50", "b51", "b52", "b3", "b20", "b38", "b46", "b35", "b37", "b56", "b53"], "figure_ref": [], "table_ref": []}, {"heading": "Data Collection", "text": "We scan a large set of tangram puzzles to vector graphics, and crowdsource annotations of natural language descriptions and part segmentations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Collecting Tangram Puzzles", "text": "Tangram puzzles are made of seven primitive shapes (Elffers, 1977), which can be combined in a large variety of configurations evoking different concepts. We scan 1,004 tangrams depicting a broad set of concepts to vector graphic SVGs from Slocum (2003). Appendix A.1 shows example tangrams, Appendix A.2 details on our process. 2 We also manually add 12 tangrams commonly used in previous studies (Hawkins et al., 2020).", "publication_ref": ["b15", "b48", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Whole-Part Annotation", "text": "We design a two-stage crowdsourcing task to elicit natural language English descriptions for each tangram, both of the whole shape and of its parts (Figure 2). First, in the whole-shape description stage, the worker is shown a tangram image in grayscale and asked to complete the prompt \"This shape, as a whole, looks like ____.\" In the part annotation stage, the worker is asked to select one or more puzzle pieces, and complete the prompt \"The part(s) you selected look(s) like ____.\" These pieces are then colored and the annotation appears in the corresponding color. The annotator can delete annotations, annotate a part as UNKNOWN when they are not sure about its semantics, and add pieces to existing 2 The scanned documents are authorized for use for educational and research purposes (\"fair use\") as per U.S. copyright law (Title 17, \u00a7108, United States Code). This use does not require permission or usage fees, including for publication. Copyright and use agreement are attached to the data.  parts. All pieces must be annotated to submit the task, yielding a complete segmentation map. We use Amazon Mechanical Turk for data collection. Workers are required to be located in the United States with at least a 98% HIT acceptance rate, must pass a qualification task, and complete a survey about their language proficiency (see Appendix A.3 for further details). To prevent a small group of workers from dominating the data, each annotator is only allowed to annotate each tangram once, and cannot annotate more than 200 distinct tangrams. Workers are paid 0.14 USD per task. 3 We first collect 10,053 annotations for the 1,004 scanned tangrams, at least 10 annotations for each tangram (mean=10.01). Following this stage of annotation, we collect additional annotations for a subset of the tangrams to create a set with denser language and part segmentation annotation. We sample 62 tangrams to be representative of the different levels of diversity in annotations we observe in the initially collected data. Appendix A.4 describes the sampling procedure. We also add the 12 tangrams from previous studies for a total of 74 tangrams for dense annotation. We conduct additional annotation tasks to have at least 50 annotations for each of the 74 tangrams selected for dense annotation (mean=53.66). 4 The dense annotation gives us a better estimate of the distribution of language for the 74 selected tangrams, for example to use as reference texts in generation tasks.\nIn total, we collect 13,404 annotations for 1,016 tangrams at a total cost of 2,172.94 USD. We lowercase and stem to compute vocabulary size, and tokenize on white spaces to compute description length. Table 1   of 297 MTurk workers participate in the annotation, with 98.0% of the workers speaking English as their first language. Those who do not speak English as their first language still rate their English proficiency level as native or close to native. 1.0% of the workers speak more than one language, among which the most common are Spanish, German, Japanese, and Chinese.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": ["tab_1"]}, {"heading": "Standard Data Splits", "text": "We split the dataset for analysis and learning experiments. For analysis, we create two overlapping sets: FULL and DENSE. FULL includes 1,016 tangrams, each with 10-11 annotations (mean=10.11). It includes the 10,053 annotations initially collected for the scanned 1,004 tangrams. For the 12 commonly used tangrams, we sample 10 annotations from the later collection effort. DENSE includes all annotations for the 74 densely annotated tangrams, with at least 50, and 53.66 on average annotations per tangram. We also define the set DENSE10 to include only the annotations from the sparse set for the densely annotated tangrams. For learning experiments, we split according to tangrams to create training (692 tangrams), development (125), test (125), and test-dense sets (74). All densely annotated tangrams are in test-dense. The other three sets are split randomly.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data Analysis", "text": "The language and concepts annotators use reflect varying degrees of consensus around conventions for describing the appearance of shapes and their parts. For analysis, we preprocess the annotations by lowercasing, tokenizing, lemmatizing, and removing stop words using NLTK (Bird, 2004). We use the larger FULL set for our analyses (Section 3.3), unless otherwise noted.\nFor a broad overview of the types of concepts evoked, we manually tag 250 randomly sampled annotations: 30.8% use human-like concepts (e.g., dancer), 31.2% animate but non-human concepts (e.g., dog), and 38.0% non-animate concepts (e.g., house). We examine how part words differ across whole-shape concepts by extracting head words from whole-shape and part descriptions. Figure 3 shows the distribution of part head words for each of 272 whole-shape head words with >10 occurrences, ranked in order of frequency. Figure A.2 in the appendix illustrates how the most common part word head is used in different tangrams.\nA central problem of visual abstraction is the degree of ambiguity or subjectivity that a shape evokes across different people (Murthy et al., 2022): some descriptions have higher consensus than others. We define three measures of variability along different dimensions: shape naming divergence (SND), part naming divergence (PND), and part segmentation agreement (PSA). Table 2 lists the mean and standard deviation for these three measures over the sparsely and densely annotated data.\nShape Naming Divergence (SND) A tangram's SND quantifies the variability among whole-shape annotations. SND is an operationalization of nameability, a criteria that is commonly used to measure how consistent is naming of an object across individuals (e.g., Zettersten and Lupyan, 2020).\nFormally, a whole-shape annotation is a sequence of M tokensx = \u27e8x 1 , . . . , x M \u27e9. Given a tangram with N annotationsx (j) , j = 1, . . . , N , each of length M (j) , we define w (j) i for each token x (j) i in annotationx (j) as the proportion of other annotations of that tangram that do not contain x (j) i :\nw (j) i = 1 N \u2212 1 N j \u2032 =1 1[x (j) i / \u2208x j \u2032 ] ,(1)\nwhere 1 is an indicator function. The divergence of annotationx (j) is W\n(j) = 1 M (j) k j=0 w (j) i . The divergence of a tangram is W = 1 N N j=0 W (j) .\nFor example, the SNDs of the tangrams in Figure 1 computed only with the two annotations displayed are 0.00 (top) and 1.00 (bottom).\nMean SND is relatively high in our data, with 0.91 on FULL (Table 2). We observe relatively similar values for DENSE and DENSE10, albeit with lower standard deviation for DENSE, as expected with more annotations. Annotators often use words that are unique to their annotation. We observe perfect consensus for only one tangram, and mostly similar annotations with relatively few deviations for a few others. Figure 5 shows several examples.  collected in the second step of the annotation task. PND is computed identically to SND, but with the concatenation of all part names of an annotation as the input textx. For example, the PNDs of the two tangrams in Figure 1 computed with only the two annotations displayed are 0.19 (top) and 1.00 (bottom). In general, part descriptions are more similar than whole-shape descriptions with mean PND of 0.76 (Table 2).\nPart Segmentation Agreement (PSA) Annotators segment the tangrams into parts by grouping the tangram puzzle pieces. PSA quantifies the agreement between part segmentations as the maximum number of pieces that does not need to be moved to another group in order to edit one segmentation to another. We compute PSA as a linear sum assignment problem with maximum weight matching. For each pair of segmentations, we create a cost matrix, where the number of rows is the number of parts in one annotation and the number of columns is the number of parts in the second annotation. The value of each matrix element is the number of matching puzzle pieces between the two corresponding parts in the two annotations. The tangram PSA is the mean of costs for all annotation pairs. For example, the PSAs of the two tangrams in Figure 1 computed with only the two annotations displayed are 6.00 (top) and 3.00 (bottom).\nThe mean PSA in our data is 5.30 (Table 2), with an approximately normal distribution of values. Some tangrams have strong segmentation cues, such that annotators reach perfect consensus, while others elicit significant segmentation disagreement.", "publication_ref": ["b4", "b41", "b58"], "figure_ref": ["fig_9", "fig_0", "fig_3", "fig_0", "fig_0"], "table_ref": ["tab_3", "tab_3", "tab_3", "tab_3"]}, {"heading": "Dense Annotations", "text": "The comparison of FULL, DENSE, and DENSE10 illustrates how well our data approximates the real distribution of annotations for each tangram, and the advantage of DENSE. Figure 4 shows the complete distribution of values. Comparing DENSE10 and DENSE, the rankings of the tangrams are largely the same with the additional annotations: for SND, Spearman's rank correlation coefficient is r(72) = .78, p \u226a .001; for PND, r(72) = .87, p \u226a .001; for PSA, r(72) = .76, p \u226a .001. The tangrams sampled for DENSE represent well the distribution of tangrams along the different measures, as illustrated by the red highlights in Figure 4. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Visual Reasoning with Tangrams", "text": "We use KILOGRAM to evaluate the reasoning of CLIP (Radford et al., 2021) and ViLT  through a reference game task, where the model is given a textual description and selects the corresponding image from a set of images. Formally, given a textual descriptionx and a set of k images I = {I 1 , . . . , I k }, the task is to select the image I i \u2208 I corresponding tox. We cast the task as computing a similarity score f (x, I i ) between the descriptionx and an image I i . We select the corresponding image as I * = arg max I i \u2208I f (x, I i ).", "publication_ref": ["b43"], "figure_ref": [], "table_ref": []}, {"heading": "Reference Game Generation", "text": "We randomly generate reference games for an annotated text-image pair (x, I) by sampling additional k \u2212 1 images from data under several constraints. We do not include repeating images in the set of k images or images that have identical whole-shape text annotations. This avoids obvious ambiguity that is impossible to resolve in the target selection.\nWe also require all images to be annotated with the same number of parts. This reduces the chance of the model relying on simple part counting to discriminate between target images when including parts in the text (condition PARTS below). Appendix A.8 shows the impact of these constraints through analyzing experiments not using them.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Models", "text": "We instantiate f using CLIP or ViLT, two models based on the Transformer architecture (Vaswani et al., 2017). We provide a brief review of the models, and refer the reader to the respective papers for further details. CLIP uses two separate encoders to generate separate fixed-dimension representations of the text and images. It uses contrastive pre-training with a symmetric cross entropy loss on a large amount of aligned, but noisy web image-text data. We implement the scoring function f with CLIP by encoding the textx and all images I \u2208 I separately, and then computing the dot-product similarity score of the text with each image. This is identical to the CLIP pre-training objective, which potentially makes CLIP suitable for our task out of the box.\nViLT uses a single encoder that takes as input both the text and image inputs together. ViLT pre-training also uses aligned image-text data, but from existing benchmarks (Lin et al., 2014;Krishna et al., 2016;Ordonez et al., 2011; Sharma a person wearing a robe a person wearing a robe with a head, a collar, and a body Figure 6: Illustration of the language and vision modalities under the different experimental conditions. et al., 2018). It is pre-trained using multiple selfsupervised objectives, including image-text matching via a binary classification head, which is suitable for our task out of the box. We implement f using this classification head. Given a textx and an image I \u2208 I, we compute their similarity using the matching classification head.", "publication_ref": ["b54", "b33", "b32", "b42"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Conditions", "text": "We study several input variants. Figure 6 illustrates the modalities under the different conditions, and Appendix A.5 shows complete example inputs. For the textual descriptionx, we experiment with including the whole-shape description only (WHOLE) or adding part names (PARTS) by combining with the whole-shape description using the template <whole shape> with <part>, <part>, ..., and <part>. This tests the ability of models to benefit from part names. We consider two image I conditions: coloring all parts with the same color (BLACK) or coloring parts differently (COLOR). The color choice in COLOR corresponds to the position of the part name inx, when the text includes part names (PARTS).\nWe experiment with the original pre-trained model weights, and with contrastive fine-tuning on our data using a symmetric cross entropy loss (Radford et al., 2021). During fine-tuning only, we consider a data augmentation condition (AUG), where we augment the data by creating examples that include only a subset of the part names in the text and coloring only the parts corresponding to the included parts names in the image, while all other parts remain black. We generate partial part examples for all possible subsets of parts for each example. Appendix A.5 illustrates the generated examples. When generating reference games for the augmented data, we constrain all the examples within a reference game to have the same number of parts in their full annotation, otherwise the task could be solved by counting parts. Part names are shuffled when creating the augmented data, and part colors correspond to the sequential position of the part name in the templated text.", "publication_ref": ["b43"], "figure_ref": [], "table_ref": []}, {"heading": "Implementation Details", "text": "We set the size of the reference game context to k = 10 throughout our experiments. During contrastive fine-tuning, we create a text-image matching matrix of size k\u00d7k for each generated reference game in our training data by randomly selecting a text description for each tangram distractor from its annotations. We compute matching loss in both directions, from text to images and vice versa. In practice, this is equivalent to creating 2k reference games in both directions, and provides more informative updates. For all experiments, we use an ensemble of three models combined by element-wise multiplication of their outputs. Appendix A.7 provides model-specific implementation details. Appendix A.9 provides a reproducibility list.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Estimating Human Performance", "text": "We conduct an initial estimation of expected human performance on the same evaluation task by recruiting an independent group of 217 human participants. Each participant is randomly assigned to one of the four conditions and shown a random sequence of 20 trials from that condition, preventing leakage across conditions. On each trial, we present an annotation from our development set along with the corresponding context of ten tangrams and ask the participant to click the tangram that was being described. We randomly sample one referential context per annotation, which provides coverage over all 125 tangrams and over 600 unique descriptions in each condition. Before the actual test trials, each participant is provided with a fixed set of 10 practice trials with feedback indicating whether they have selected the correct tangram, and if not, we highlight the correct answer. Performance in the practice trials is not considered in our analysis. Appendix A.6 provides further details.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results and Analysis", "text": "Table 3 shows development and test reference game accuracies under different experimental setups, including for human studies. Figure 7 shows the accuracy distribution for human participants.   While both models perform better than a random baseline (10%) out of the box, we generally observe poor performance with the pre-trained weights (PT). CLIP slightly outperforms ViLT throughout, potentially because it is trained with a contrastive objective similar to a reference game. Whereas ViLT's matching loss is aligned with our goal, it is only one of several losses in its objective. We observe no reliable improvement from adding part information, either textual or visual. The low performance on WHOLE+BLACK indicates the models fail to generalize familiar concepts to abstract shapes and the lack of consistent improvement with part information indicates an inability to reason about the correspondence of text and colored parts.\nFine-tuning (FT) dramatically improves performance for both models. Adding part names to the text description improves both models (PARTS+BLACK). However, segmentation information in the form of part coloring without part names (WHOLE+COLOR) shows no benefit. Although ViLT does not benefit from color information alone, the combination with part names (PARTS+COLOR) shows significant added improvement in performance over having access to part information in one of the modalities. Overall, we observe small consistent differences in performance between the two models, except when having access to both part names and colors (PARTS+COLOR), which ViLT effectively uses following fine tuning. This may be because ViLT's tight integration of the modalities in its single encoder allows it to take advantage of the part correspondence information provided when both part names and colors are given.\nHuman performance follows a similar trend to the fine-tuned models: adding part names and segmentation helps performance, and their benefit is most pronounced when both are provided. Human performance is significantly higher than pre-trained (PT) models across all four conditions. Fine-tuning (FT) closes this gap. Indeed, in the PARTS+COLOR condition, ViLT significantly outperforms mean human performance. To better analyze human results, we fit a two-component Gaussian mixture model to the distribution of individual participants' accuracies (Figure 7). We observe two components for all conditions except WHOLE+BLACK, indicating two distinct sub-populations. For example, for PARTS+COLOR, the low-performing subpopulation has a mean accuracy of 52.5%, while the high-performing has a mean of 83.8%, significantly outperforming the fine-tuned ViLT. It is possible that the lower-performance sub-population is not making full use of the additional information.\nData augmentation (AUG) improves performance for CLIP, but not for ViLT, which even shows a small decrease in performance, although still significantly outperforming CLIP. We hypothesize that the presence of training examples with partial part information complicates resolving the correspondence between parts and their name, resulting in overall lower ViLT performance. We leave further study of this hypothesis for future work.\nThe augmentation condition fine-tunes the models to handle examples with partial part information, and allows to study the impact of gradually adding part information. We apply the augmentation process to the development data to generate the data for this analysis. Figure 8 shows the effect of gradually adding part information on the probability of the correct prediction, separated by the total number of parts in the example. Overall, part information is beneficial, but with a diminishing return as more part information is added. We observe this for both models, but with a much faster rate for CLIP, which overall shows much lower performance. ViLT is able to benefit from increasing part information, with the benefit diminishing only after four parts are provided.", "publication_ref": [], "figure_ref": ["fig_4", "fig_4", "fig_5"], "table_ref": ["tab_6"]}, {"heading": "Discussion", "text": "KILOGRAM provides a new window into the visual abstraction capacity of grounded language models and their ability to generalize concepts beyond their photographic appearance, an integral component of human concept representations (Fan et al., 2015). Our experiments show that there is significant room to improve pre-trained models, which should be able to perform zero-shot reference game tasks without fine-tuning as well as humans do (Clark and Wilkes-Gibbs, 1986). The improved performance after fine-tuning indicates the multi-modal architecture itself has the potential for higher performance, which current pre-training regimes likely do not support. In particular, ViLT's improved performance as a function of additional part information suggests that more structured concept alignment may play a role in this effort (e.g., between parts expressed as lexical items and the corresponding elements of the image). While we focused on the task of reference resolution, KILOGRAM is also well-suited for production tasks (e.g., generating human-like distributions of descriptions or coloring named parts on a blank tangram) as well as instruction-following tasks (e.g., placing pieces in the described configuration to reconstruct a tangram). More broadly, our data emphasizes the need for maintaining well-calibrated distributions over the many different possible ways that people may conceptualize or talk about things, rather than collapsing to a \"best\" prediction.", "publication_ref": ["b16", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "Although randomly constructed reference games provide an interpretable evaluation metric, they also pose several limitations. Performance is limited by the fact that descriptions were elicited for isolated images. These descriptions do not reflect the kind of pragmatic reasoning commonly deployed by human speakers in reference games to resolve ambiguities (Goodman and Frank, 2016). In other words, annotators were not able to anticipate the necessary level of detail to disambiguate the object from a specific context of distractors, hence the descriptions may be underinformative. Randomly generated reference games may include ambiguities that make them impossible to solve (e.g., two objects that could both plausibly be described as a bird). The possible performance ceiling on these games is likely below 100%. Extending the data through interactive reference games is an important direction for future work. Likewise, our studies of baseline human performance on this task are preliminary. We found that participants clustered into higher-and lower-performing groups, likely reflecting attentional and motivational factors (e.g., some participants may not have fully attended to the provided part information). A better understanding of human behavior is critical before making any clear conclusions comparing humans and model performance. Ultimately, models only outperformed mean human performance significantly only after fine-tuning on approximately 6,600 example reference games.\nOur resource contribution and analysis are focused on English. While the data collection design does not make language-specific assumptions, it depends on the availability of proficient speakers, which is limited in contemporary crowdsourcing services for certain languages. Our large collection of visual stimuli is well suited to extend our data collection to other languages and cultures, which may display different abstractions. This is an important direction for future work. Extending our analysis to other languages depends on the availability of pre-trained models in these languages, which may be limited by the availability of aligned language vision data and the computational resources required for pre-training. ", "publication_ref": ["b21"], "figure_ref": [], "table_ref": []}, {"heading": "A Appendix", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1 Examples from KILOGRAM", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Collecting Tangrams", "text": "We scan all the pages of tangram solutions from Slocum (2003) into JPEG files to extract SVG files of individual tangrams. We use heuristics based on edge and corner detection (Harris et al., 1988) to extract individual tangrams into separate files by detecting the four corners of each puzzle and adding padding. 5 We heuristically detect the individual standard pieces in each tangram using corner detection. Because the shapes are standard, we can test if an extracted shape is an expected puzzle's piece and if we obtain the expected number of such shapes. We resize each tangram and all its pieces to a standard size, and label the ID of each puzzle piece consistently across all tangrams. We heuristically and manually validate the outputs, and prune solutions that fail to vectorize properly, for example if the process fails to recover exactly seven pieces.", "publication_ref": ["b48", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "A.3 Crowdsourcing Qualifications and Survey", "text": "The qualifier includes three multiple choice questions aimed to ensure that (a) the annotator describes the abstract shape meaningfully instead of simply describing its geometry; (b) each part description only contains one part (body and arms instead of body with arms); and (c) the part descriptions correspond to the description of the whole shape. We provide a short video tutorial of the task and examples of invalid annotations for workers to view before completing the qualifier. We also collect basic non-identifying demographic data from each worker, including the languages that they speak and their proficiency, if English is their first language, and where they learned English. We retain the correspondence of anonymized hashed worker IDs to the annotations and language information they provide.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.4 Dense Annotation Sampling", "text": "The set DENSE is made of 62 tangrams sampled from FULL and 12 tangrams commonly used in prior work. We sample the 62 tangrams from FULL to represent the diversity of tangrams using the first set of annotations we collect. We plot the annotated tangrams by average log perplexity of whole-shape descriptions with 1 100 smoothing and PSA and apply a 5 \u00d7 5 grid to the plot (Figure A.3). Using perplexity and PSA allows us to sample a set of tangrams with diverse degrees of annotation and segmentation agreement. With a relatively high smoothing factor, we are able to spread out the data points, because the majority of the data set has high divergence in descriptions. We randomly pick 12 periphery points to collect more annotations for outliers, uniformly sample 25 from all the 1004 tangrams, and randomly sample 25, one from each grid, to represent the entire distribution.\nWe calculate average log perplexity of wholeshape annotations for each tangram.\nLet x (1) , . . . ,x (N ) be annotations for a tangram, where each annotation is a sequence of tokensx (j) = \u27e8x 1 , . . . , x M (j) \u27e9 of length M (j) . We create a language model p (j) for every annotationx (j) using all other N \u2212 1 annotations for the tangram:\np (j) (x) = C x\u2208x (j \u2032 \u0338 =j) + k total j \u2032 \u0338 =j + kV ,(2)\nwhere C x\u2208x (j \u2032 \u0338 =j) is the number of occurrences of x in the other annotations for the tangram, k is the smoothing factor, total j \u2032 \u0338 =j is the total number of words used in the other annotations for the tangram and V is the vocabulary size of all whole-shape annotations across all tangrams. The log perplexity for annotationx (j) is log P P\n(j) = \u2212 1 M (j) M (j) i=1 log 2 p(x (j) i ).\nThe log perplexity for the tangram is the average of perplexity values for all its annotations log P P = 1 N N j=1 log P P (j) . We lowercase, stem, and remove stop words before computing the log perplexity.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.5 Example Inputs for Experimental Conditions", "text": "Figure A.4 shows how one annotation, including both text and image, appears under the different experimental conditions. For conditions with PARTS annotations, we generate simple English sentences combining the whole shape description with part descriptions using the template <whole shape> with <part>, <part>, ..., and <part>. We add an indefinite article to each singular part description. BLACK images are tangrams with all pieces colored black with white borders. COLOR images are tangrams with each part colored with one of the CSS preset colors in the order of coral, gold, lightskyblue, lightpink, mediumseagreen, darkgrey, lightgrey that correspond to the parts in the annotation. For the augmented condition (AUG), text inputs are whole annotations combined with each possible subset of the part descriptions. Image inputs are tangrams colored in the same way as colored images, but the parts excluded from the subset of part descriptions are colored black instead. All part descriptions in the annotations are randomly shuffled and not consistently associated with any particular color in the images, so that the coloring solely serves as an indication of the ordering of parts in the combined text.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.6 Human Performance Baseline Details", "text": "We recruited an independent group of 233 human participants from the Prolific crowdsourcing platform (https://www.prolific.co/), and asked them to perform the same reference game task we used for model evaluation. Each participant was randomly assigned to one of the four conditions and shown a random sequence of 20 trials from that condition. On each trial, we showed a text annotation from the development set along with the corresponding context of ten tangrams and asked the participant to click the tangram that was being described. The information that was available varied across condition, just as in the model evaluations.\nThe tangrams were either presented to participants in black-and-white (BLACK) or colored according to their segmentation map (COLOR), and the language was either the whole-shape description alone (WHOLE) or with the parts included (PARTS). In the PARTS+COLOR condition, the parts text was colored to match the image to facilitate visual comparison, providing the same alignment information available to the models. We took several steps to ensure high-quality responses. First, participants began with a fixed set of 10 practice trials to familiarize with the task. For these practice trials, we provided feedback indicating whether they have selected the correct tangram, and if not, we highlight the correct answer. To assess whether participants were paying attention as opposed to responding randomly, we inserted an unambiguous \"catch trial\" where the target was the square tangram and the description was square. We excluded 16 participants who failed to select the correct target on this trial, yielding a final sample size of 217 participants out of the 233 recruited.\nBecause our aim was to obtain overall accuracy estimates for each condition, we did not require judgements for every individual annotation and context in the test set. However, we were able to ensure good coverage of the dataset, including annotations from all 125 tangrams and over 600 unique descriptions in each condition.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.7 Model-specific Implementation Details", "text": "For experiments with CLIP, we use the ViT-B/32 variant. We fine-tune using an Adam optimizer with learning rate 5e-8 and weight decay 1e-6. At the end of each epoch, the training data is shuffled and rebatched. We train the models up to 200 epochs and use patience of 50 epochs to select the model with the highest image prediction accuracy on a non-augmented validation set taken from the training data. All images are resized to CLIP's default input resolution of 224 \u00d7 224, with white padding to make to rectangle images square. The total number of trainable parameters in CLIP is 151.2M. CLIP models are fine-tuned with either a single GeForce RTX 2080 Ti GPU with 11GB memory or a single Titan RTX GPU with 24GB memory. Fine-tuning takes approximately 40 minutes per epoch for augmented setups (AUG) and roughly 3 minutes for other setups.\nFor ViLT experiments, we fine-tune with an AdamW optimizer with learning rate 1e-4 and weight decay 1e-2. We use a cosine learning rate schedule with warm-up over the first epoch. We train the models up to 30 epochs with a patience of 10 epochs and follow the same model selection criterion as for CLIP. All images are resized to 384 \u00d7 384. The total number of trainable parameters in ViLT is 87.4M. ViLT models are fine-tuned with a single Titan RTX GPU with 24 GB memory. Fine-tuning takes up to 5.5 hours per epoch for augmented setups (AUG) and roughly 15 minutes for other setups.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.8 Random Generation of Reference Games", "text": "In our main experiments (Section 5), we randomly generate reference games subject to constraints (Section 5.1). In particular, we ensure that distractors contained the same total number of parts. We explore the impact of these constraints by repeating our experiments on reference games generated without the constraints. Without the constraints, part counting can help the model disqualify distractors and significantly narrow down the set of likely referents. This is because images with a different    ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "This research was supported by ARO W911NF21-1-0106, NSF under grant No. 1750499, and a gift from Open Philanthropy. NK is supported by Masason Fellowship, AS by a Facebook PhD Fellowship and an NSF GRF under grant No. 1650441, and RDH by a CV Starr Fellowship. We thank Rob Goldstone, Judith Fan, Cathy Wong, and the anonymous reviewers for their helpful comments and suggestions. We are grateful for the contributions of the workers on Mechanical Turk.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "number of parts colored compared to the number of parts in the text description can be easily ignored without considering the semantics of the text or images. Table A.1 shows development accuracies for games generated without constraints, both for training and testing. Generally, the success rate achieved on unconstrained contexts is much higher compared to contexts generated with constraints (Figure 3). However, when analyzing the performance of this model on part-controlled contexts (Figure A.5), we observe roughly similar performance to the games generated with constraints (Figure 8), even though we would expect a significant performance increase given the results in Table A.1. We even observe a more pronounced decrease in performance when more parts are added, illustrating further difficulty generalizing. We conclude that the model trained on games generated without constraints (Table A.1) likely learns to rely on part-counting heuristics and may be less reliable in other settings.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.9 Reproducibility Checklist", "text": "For all reported experimental results:\n\u2022 A clear description of the mathematical setting, algorithm, and/or model: yes; see Section 5.\n\u2022 Submission of a zip file containing source code, with specification of all dependencies, including external libraries, or a link to such resources: yes; attached to our submission.\n\u2022 Description of computing infrastructure used: yes; see Appendix A.7.\n\u2022 The average runtime for each model or algorithm (e.g., training, inference, etc.) or estimated energy cost: yes; see Appendix A.7.\n\u2022 Number of parameters in each model: yes; see Appendix A.7.\n\u2022 Corresponding validation performance for each reported test result: yes; see Appendix 3 and Appendix A.1 for results on the development set.\n\u2022 Explanation of evaluation metrics, with links to code used: yes; see Section 5 for an explanation of the reference game metric. An implementation is included in the attached code zipfile.\nFor all experiments with hyperparameter search:\n\u2022 We performed a minimal manual search for learning rate and weight decay, and used the same values for all experiments (described in Section A.7).\nFor all datasets used:\n\u2022 Relevant details such as languages, and number of examples and label distributions: yes; see Section 3.\n\u2022 Details of train/test/validation splits: yes; see Section 3.3.\n\u2022 Explanation of any data that were excluded, and all pre-processing steps: yes; see Section 3 and Section A.2.\n\u2022 A zip file containing data or link to a downloadable version of the data: yes; attached to our submission.\n\u2022 For new data collected, a complete description of the data collection process, such as instructions to annotators and methods for quality control: yes; see Section 3.2 and Section A.3.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "VQA: Visual question answering", "journal": "", "year": "2015", "authors": "Stanislaw Antol; Aishwarya Agrawal; Jiasen Lu; Margaret Mitchell; Dhruv Batra; C Lawrence Zitnick; Devi Parikh"}, {"ref_id": "b1", "title": "Social group effects on the emergence of communicative conventions and language complexity", "journal": "Journal of Language Evolution", "year": "2019", "authors": "Mark Atkinson; J Gregory; Kenny Mills;  Smith"}, {"ref_id": "b2", "title": "Lexical entrainment without conceptual pacts? revisiting the matching task", "journal": "Journal of Memory and Language", "year": "2020", "authors": "Adrian Bangerter; Eric Mayor; Dominique Knutsen"}, {"ref_id": "b3", "title": "Recognition-by-components: a theory of human image understanding. Psychological review", "journal": "", "year": "1987", "authors": "Irving Biederman"}, {"ref_id": "b4", "title": "Nltk: The natural language toolkit", "journal": "ArXiv", "year": "2004", "authors": "Steven Bird"}, {"ref_id": "b5", "title": "The OpenCV Library. Dr. Dobb's Journal of Software Tools", "journal": "", "year": "2000", "authors": "G Bradski"}, {"ref_id": "b6", "title": "The effects of feature verbalizablity on category learning", "journal": "", "year": "2020", "authors": "Bailey Brashears; John Paul; Minda "}, {"ref_id": "b7", "title": "Interaction promotes the adaptation of referential conventions to the communicative context", "journal": "Cognitive science", "year": "2019", "authors": "Luc\u00eda Castillo; Kenny Smith;  Holly P Branigan"}, {"ref_id": "b8", "title": "Microsoft COCO captions: Data collection and evaluation server", "journal": "CoRR", "year": "2015", "authors": "Xinlei Chen; Hao Fang; Tsung-Yi Lin; Ramakrishna Vedantam; Saurabh Gupta; Piotr Doll\u00e1r; C Lawrence Zitnick"}, {"ref_id": "b9", "title": "Uniter: Universal image-text representation learning", "journal": "", "year": "2020", "authors": "Yen-Chun Chen; Linjie Li; Licheng Yu; Ahmed El Kholy; Faisal Ahmed; Zhe Gan; Yu Cheng; Jingjing Liu"}, {"ref_id": "b10", "title": "Referring as a collaborative process", "journal": "Cognition", "year": "1986", "authors": "H Herbert; Deanna Clark;  Wilkes-Gibbs"}, {"ref_id": "b11", "title": "Picture naming by young children: norms for name agreement, familiarity, and visual complexity", "journal": "Journal of experimental child psychology", "year": "1997", "authors": "Yael M Cycowicz; M Friedman; Joan Gay Rothstein;  Snodgrass"}, {"ref_id": "b12", "title": "Development of shared information in communication despite hippocampal amnesia", "journal": "Nature neuroscience", "year": "2006", "authors": "C Melissa; Julie Duff; Daniel Hengst;  Tranel;  Cohen"}, {"ref_id": "b13", "title": "", "journal": "", "year": "", "authors": "Jon Andoni Du\u00f1abeitia; Davide Crepaldi; S Antje"}, {"ref_id": "b14", "title": "Multipic: A standardized set of 750 drawings with norms for six european languages", "journal": "Quarterly Journal of Experimental Psychology", "year": "2018", "authors": "Boris Meyer; Christos New; Eva Pliatsikas; Marc Smolka;  Brysbaert"}, {"ref_id": "b15", "title": "Tangram: The Ancient Chinese Puzzle", "journal": "Penguin Books", "year": "1977", "authors": "Joost Elffers"}, {"ref_id": "b16", "title": "Common object representations for visual recognition and production", "journal": "Cognitive Science", "year": "2015", "authors": "Judith E Fan; Daniel Yamins; Nicholas B Turk-Browne"}, {"ref_id": "b17", "title": "A modified procedure for naming 332 pictures and collecting norms: Using tangram pictures in psycholinguistic studies. Behavior research methods", "journal": "", "year": "2022", "authors": "Alicia Fasquel; Ang\u00e8le Brunelli\u00e8re; Dominique Knutsen"}, {"ref_id": "b18", "title": "Learning distributions over logical forms for referring expression generation", "journal": "", "year": "2013", "authors": "Nicholas Fitzgerald; Yoav Artzi; Luke Zettlemoyer"}, {"ref_id": "b19", "title": "Listening in on monologues and dialogues. Discourse processes", "journal": "", "year": "1999", "authors": "Jean E Fox Tree"}, {"ref_id": "b20", "title": "Structure mapping in analogy and similarity", "journal": "American Psychologist", "year": "1997", "authors": "Dedre Gentner; Arthur B Markman"}, {"ref_id": "b21", "title": "Pragmatic language interpretation as probabilistic inference", "journal": "Trends in Cognitive Sciences", "year": "2016", "authors": "D Noah; Michael C Goodman;  Frank"}, {"ref_id": "b22", "title": "A combined corner and edge detector", "journal": "Citeseer", "year": "1988", "authors": "Chris Harris; Mike Stephens"}, {"ref_id": "b23", "title": "Characterizing the dynamics of learning in repeated reference games", "journal": "Cognitive science", "year": "2020", "authors": "Robert D Hawkins; Michael C Frank; Noah D Goodman"}, {"ref_id": "b24", "title": "Co-speech gesture mimicry in the process of collaborative referring during face-to-face dialogue", "journal": "Journal of Nonverbal Behavior", "year": "2011", "authors": "Judith Holler; Katie Wilkin"}, {"ref_id": "b25", "title": "Speakers' experiences and audience design: Knowing when and knowing how to adjust utterances to addressees", "journal": "Journal of Memory and Language", "year": "2002", "authors": "S William; Richard J Horton;  Gerrig"}, {"ref_id": "b26", "title": "Anticipating who will say what: The influence of speakerspecific memory associations on reference resolution", "journal": "Memory & cognition", "year": "2012", "authors": "S William; Daniel G Horton;  Slaten"}, {"ref_id": "b27", "title": "The effects of the codability and discriminability of the referents on the collaborative referring procedure", "journal": "British Journal of Psychology", "year": "1991", "authors": "Michel Hupet; Xavier Seron; Yves Chantraine"}, {"ref_id": "b28", "title": "The flexibility of conceptual pacts: Referring expressions dynamically shift to accommodate new conceptualizations", "journal": "Frontiers in psychology", "year": "2016", "authors": "Alyssa Ibarra; K Michael;  Tanenhaus"}, {"ref_id": "b29", "title": "2021. Scaling up visual and vision-language representation learning with noisy text supervision", "journal": "PMLR", "year": "", "authors": "Chao Jia; Yinfei Yang; Ye Xia; Yi-Ting Chen; Zarana Parekh; Hieu Pham; Quoc Le; Yun-Hsuan Sung; Zhen Li; Tom Duerig"}, {"ref_id": "b30", "title": "Vilt: Vision-and-language transformer without convolution or region supervision", "journal": "PMLR", "year": "2021", "authors": "Wonjae Kim; Bokyung Son; Ildoo Kim"}, {"ref_id": "b31", "title": "Changes in reference phrases as a function of frequency of usage in social interaction: A preliminary study", "journal": "Psychonomic Science", "year": "1964", "authors": "M Robert; Sidney Krauss;  Weinheimer"}, {"ref_id": "b32", "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations", "journal": "International Journal of Computer Vision", "year": "2016", "authors": "Ranjay Krishna; Yuke Zhu; Oliver Groth; Justin Johnson; Kenji Hata; Joshua Kravitz; Stephanie Chen; Yannis Kalantidis; Li-Jia Li; David A Shamma; Michael S Bernstein; Li Fei-Fei"}, {"ref_id": "b33", "title": "Microsoft coco: Common objects in context", "journal": "", "year": "2014", "authors": "Tsung-Yi Lin; Michael Maire; Serge Belongie; James Hays; Pietro Perona; Deva Ramanan; Piotr Doll\u00e1r; C Lawrence Zitnick"}, {"ref_id": "b34", "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural information processing systems", "journal": "", "year": "2019", "authors": "Jiasen Lu; Dhruv Batra; Devi Parikh; Stefan Lee"}, {"ref_id": "b35", "title": "Language is more abstract than you think, or, why aren't languages more iconic?", "journal": "Philosophical Transactions of the Royal Society B: Biological Sciences", "year": "2018", "authors": "Gary Lupyan; Bodo Winter"}, {"ref_id": "b36", "title": "Generation and comprehension of unambiguous object descriptions", "journal": "IEEE", "year": "2016", "authors": "Junhua Mao; Jonathan Huang; Alexander Toshev; Oana Camburu; Alan L Yuille; Kevin Murphy"}, {"ref_id": "b37", "title": "Learning to communicate about shared procedural abstractions", "journal": "ArXiv", "year": "2021", "authors": "William Mccarthy; X D Robert; Haoliang Hawkins; Cameron Wang; Judith E Holdaway;  Fan"}, {"ref_id": "b38", "title": "Respects for similarity", "journal": "Psychological Review", "year": "1993", "authors": "Douglas L Medin; Robert L Goldstone; Dedre Gentner"}, {"ref_id": "b39", "title": "Natural reference to objects in a visual domain", "journal": "", "year": "2010", "authors": "Margaret Mitchell"}, {"ref_id": "b40", "title": "The effect of production variables in monolog and dialog on comprehension by novel listeners", "journal": "Language and Speech", "year": "2001", "authors": "Tara Murfitt; Jan Mcallister"}, {"ref_id": "b41", "title": "Shades of confusion: Lexical uncertainty modulates ad hoc coordination in an interactive communication task", "journal": "Cognition", "year": "2022", "authors": "K Sonia;  Murthy; L Thomas; Robert D Griffiths;  Hawkins"}, {"ref_id": "b42", "title": "Im2text: Describing images using 1 million captioned photographs. Advances in neural information processing systems", "journal": "", "year": "2011", "authors": "Vicente Ordonez; Girish Kulkarni; Tamara Berg"}, {"ref_id": "b43", "title": "Learning transferable visual models from natural language supervision", "journal": "PMLR", "year": "2021", "authors": "Alec Radford; Jong Wook Kim; Chris Hallacy; Aditya Ramesh; Gabriel Goh; Sandhini Agarwal; Girish Sastry; Amanda Askell; Pamela Mishkin; Jack Clark"}, {"ref_id": "b44", "title": "Understanding by addressees and overhearers", "journal": "Cognitive psychology", "year": "1989", "authors": "F Michael;  Schober; H Herbert;  Clark"}, {"ref_id": "b45", "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning", "journal": "Long Papers", "year": "2018", "authors": "Piyush Sharma; Nan Ding; Sebastian Goodman; Radu Soricut"}, {"ref_id": "b46", "title": "Toward a universal law of generalization for psychological science", "journal": "Science", "year": "1987", "authors": "Roger N Shepard"}, {"ref_id": "b47", "title": "KTH tangrams: A dataset for research on alignment and conceptual pacts in taskoriented dialogue", "journal": "", "year": "2018", "authors": "Todd Shore; Theofronia Androulakaki; Gabriel Skantze"}, {"ref_id": "b48", "title": "The Tangram Book: The Story of the Chinese Puzzle with over 2000 Puzzles to Solve", "journal": "Sterling Publishing", "year": "2003", "authors": "J Slocum"}, {"ref_id": "b49", "title": "A standardized set of 260 pictures: norms for name agreement, image agreement, familiarity, and visual complexity", "journal": "Journal of experimental psychology. Human learning and memory", "year": "1980", "authors": "Joan Gay Snodgrass; Mary Vanderwart"}, {"ref_id": "b50", "title": "A corpus of natural language for visual reasoning", "journal": "", "year": "2017", "authors": "Alane Suhr; Mike Lewis; James Yeh; Yoav Artzi"}, {"ref_id": "b51", "title": "A corpus for reasoning about natural language grounded in photographs", "journal": "", "year": "2019", "authors": "Alane Suhr; Stephanie Zhou; Ally Zhang; Iris Zhang; Huajun Bai; Yoav Artzi"}, {"ref_id": "b52", "title": "Winoground: Probing vision and language models for visio-linguistic compositionality", "journal": "", "year": "2022", "authors": "Tristan Thrush; Ryan Jiang; Max Bartolo; Amanpreet Singh; Adina Williams; Douwe Kiela; Candace Ross"}, {"ref_id": "b53", "title": "Objects, parts, and categories", "journal": "Journal of Experimental Psychology: General", "year": "1984", "authors": "Barbara Tversky; Kathleen Hemenway"}, {"ref_id": "b54", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Lukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b55", "title": "The use of spatial relations in referring expression generation", "journal": "", "year": "2008", "authors": "Jette Viethen; Robert Dale"}, {"ref_id": "b56", "title": "", "journal": "", "year": "2022", "authors": "Catherine Wong; William Mccarthy; Gabriel Grand; Yoni Friedman; Joshua B Tenenbaum; Jacob Andreas; Robert D Hawkins; Judith E Fan"}, {"ref_id": "b57", "title": "Modeling context in referring expressions", "journal": "", "year": "2016", "authors": "Licheng Yu; Patrick Poirson; Shan Yang; Alexander C Berg; Tamara L Berg"}, {"ref_id": "b58", "title": "Finding categories through words: More nameable features improve category learning", "journal": "Cognition", "year": "2020", "authors": "Martin Zettersten; Gary Lupyan"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "*Figure 1 :1Figure 1: Two example tangrams, each with two different annotations. Each annotation includes a wholeshape description (bold), segmentation to parts (in color), and naming of parts (linked to each part). The top example shows low variability with near-perfect agreement, while the bottom shows high variability with divergence of language and segmentation.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: The two phases of our annotation task.", "figure_data": ""}, {"figure_label": "34", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :Figure 4 :34Figure 3: Part distributions for different head words. Whole-shape head words (shown in descending order of frequency from left) elicit a variety of part head word distributions. Colors are randomly assigned to part head words, but are fixed across all bars. Grey indicates part head words with < 0.005 frequency.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure 5: SND, PND, and PSA correlations computed over the FULL set. Representative examples of different SND and PSA values are illustrated on the right. Densely annotated examples are highlighted in red.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 7 :7Figure 7: The distribution of each human participant's mean accuracy in the four conditions. The white dashed lines are the estimated means of a two-component Gaussian mixture model.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 8 :8Figure 8: Mean probability assigned to the correct image using fine-tuned CLIP (left) or fine-tuned ViLT (right) on the development set, by number of parts included in text and colored in the images. Curves are separated by total number of parts in the annotation of the target example. Error bands are bootstrapped 95% confidence intervals.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure A. 11Figure A.1 shows example tangrams from our data. Figure A.2 shows examples of the use of the part name head, the most common part head word in the data. All data can be browsed on the data visualization dashboard: https://lil.nlp.cornell. edu/kilogram/.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure A. 1 :1Figure A.1: Example tangrams from our dataset.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure A. 2 :2Figure A.2: Example tangrams containing the part description head. Each example includes a tangram and its whole-shape description. We highlight the segmentation corresponding to head in each tangram.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure A. 3 :3Figure A.3: Sampled tangrams for dense annotation collection: 12 purple points picked from the periphery, 25 red points randomly sampled from each grid, and 25 green points uniformly sampled from all points.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Data statistics for the complete dataset.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "shows basic data statistics. A total", "figure_data": "FULLDENSEDENSE10SND 0.91 \u00b10.11 0.93\u00b10.06 0.90\u00b10.15 PND 0.76\u00b10.19 0.79\u00b10.15 0.73\u00b10.20 PSA 5.30\u00b10.62 5.09\u00b10.53 5.34\u00b10.77"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Mean and standard deviation of our analysis measures on the three sets.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Part Naming Divergence (PND) SND measures annotation divergence for part name annotations", "figure_data": "p(part | whole)0.00 0.25 0.50 0.75 1.00personsnakefishtablehorseflowersnailbatbottlefoxtentwhalepyramidchairhooksailboatmonkarmwormsquirrelclawlightkiteangelhelmetcakeringmailboxcactusbarnhalfpriestmushroomtankcameralionsubmarinespidereagleknighttrain"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "", "figure_data": "WHOLE+BLACK.47PARTS+BLACK.29.54WHOLE+COLOR.39.60PARTS+COLOR.52.840.000.250.500.751.00Participant Accuracy: Reference game accuracies (%) for the different ex-perimental conditions with pre-trained (PT) or fine-tuned (FT) models, as well as for human subjects."}], "formulas": [{"formula_id": "formula_0", "formula_text": "w (j) i = 1 N \u2212 1 N j \u2032 =1 1[x (j) i / \u2208x j \u2032 ] ,(1)", "formula_coordinates": [4.0, 338.7, 496.93, 186.45, 36.63]}, {"formula_id": "formula_1", "formula_text": "(j) = 1 M (j) k j=0 w (j) i . The divergence of a tangram is W = 1 N N j=0 W (j) .", "formula_coordinates": [4.0, 306.14, 547.5, 220.19, 33.19]}, {"formula_id": "formula_2", "formula_text": "p (j) (x) = C x\u2208x (j \u2032 \u0338 =j) + k total j \u2032 \u0338 =j + kV ,(2)", "formula_coordinates": [13.0, 360.86, 386.56, 164.17, 27.07]}, {"formula_id": "formula_3", "formula_text": "(j) = \u2212 1 M (j) M (j) i=1 log 2 p(x (j) i ).", "formula_coordinates": [13.0, 306.14, 501.5, 218.28, 35.62]}], "doi": "10.1109/ICCV.2015.279"}