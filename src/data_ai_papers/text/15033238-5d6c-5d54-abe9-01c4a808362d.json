{"title": "Online Learning for Latent Dirichlet Allocation", "authors": "Matthew D Hoffman; David M Blei; Francis Bach", "pub_date": "", "abstract": "We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by fitting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA finds topic models as good or better than those found with batch VB, and in a fraction of the time.", "sections": [{"heading": "Introduction", "text": "Hierarchical Bayesian modeling has become a mainstay in machine learning and applied statistics. Bayesian models provide a natural way to encode assumptions about observed data, and analysis proceeds by examining the posterior distribution of model parameters and latent variables conditioned on a set of observations. For example, research in probabilistic topic modeling-the application we will focus on in this paper-revolves around fitting complex hierarchical Bayesian models to large collections of documents. In a topic model, the posterior distribution reveals latent semantic structure that can be used for many applications.\nFor topic models and many other Bayesian models of interest, however, the posterior is intractable to compute and researchers must appeal to approximate posterior inference. Modern approximate posterior inference algorithms fall in two categories-sampling approaches and optimization approaches. Sampling approaches are usually based on Markov Chain Monte Carlo (MCMC) sampling, where a Markov chain is defined whose stationary distribution is the posterior of interest. Optimization approaches are usually based on variational inference, which is called variational Bayes (VB) when used in a Bayesian hierarchical model. Whereas MCMC methods seek to generate independent samples from the posterior, VB optimizes a simplified parametric distribution to be close in Kullback-Leibler divergence to the posterior. Although the choice of approximate posterior introduces bias, VB is empirically shown to be faster than and as accurate as MCMC, which makes it an attractive option when applying Bayesian models to large datasets [1,2,3].\nNonetheless, large scale data analysis with VB can be computationally difficult. Standard \"batch\" VB algorithms iterate between analyzing each observation and updating dataset-wide variational parameters. The per-iteration cost of batch algorithms can quickly become impractical for very large datasets. In topic modeling applications, this issue is particularly relevant-topic modeling promises Figure 1: Top: Perplexity on held-out Wikipedia documents as a function of number of documents analyzed, i.e., the number of E steps. Online VB run on 3.3 million unique Wikipedia articles is compared with online VB run on 98,000 Wikipedia articles and with the batch algorithm run on the same 98,000 articles. The online algorithms converge much faster than the batch algorithm does. Bottom: Evolution of a topic about business as online LDA sees more and more documents.\nto summarize the latent structure of massive document collections that cannot be annotated by hand. A central research problem for topic modeling is to efficiently fit models to larger corpora [4,5].\nTo this end, we develop an online variational Bayes algorithm for latent Dirichlet allocation (LDA), one of the simplest topic models and one on which many others are based. Our algorithm is based on online stochastic optimization, which has been shown to produce good parameter estimates dramatically faster than batch algorithms on large datasets [6]. Online LDA handily analyzes massive collections of documents and, moreover, online LDA need not locally store or collect the documentseach can arrive in a stream and be discarded after one look.\nIn the subsequent sections, we derive online LDA and show that it converges to a stationary point of the variational objective function. We study the performance of online LDA in several ways, including by fitting a topic model to 3.3M articles from Wikipedia without looking at the same article twice. We show that online LDA finds topic models as good as or better than those found with batch VB, and in a fraction of the time (see figure 1). Online variational Bayes is a practical new method for estimating the posterior of complex hierarchical Bayesian models.", "publication_ref": ["b0", "b1", "b2", "b3", "b4", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Online variational Bayes for latent Dirichlet allocation", "text": "Latent Dirichlet Allocation (LDA) [7] is a Bayesian probabilistic model of text documents. It assumes a collection of K \"topics.\" Each topic defines a multinomial distribution over the vocabulary and is assumed to have been drawn from a Dirichlet, \u03b2 k \u223c Dirichlet(\u03b7). Given the topics, LDA assumes the following generative process for each document d. First, draw a distribution over topics \u03b8 d \u223c Dirichlet(\u03b1). Then, for each word i in the document, draw a topic index z di \u2208 {1, . . . , K} from the topic weights z di \u223c \u03b8 d and draw the observed word w di from the selected topic, w di \u223c \u03b2 z di . For simplicity, we assume symmetric priors on \u03b8 and \u03b2, but this assumption is easy to relax [8].\nNote that if we sum over the topic assignments z, then we get p(w di |\u03b8 d , \u03b2) = k \u03b8 dk \u03b2 kw . This leads to the \"multinomial PCA\" interpretation of LDA; we can think of LDA as a probabilistic factorization of the matrix of word counts n (where n dw is the number of times word w appears in document d) into a matrix of topic weights \u03b8 and a dictionary of topics \u03b2 [9]. Our work can thus be seen as an extension of online matrix factorization techniques that optimize squared error [10] to more general probabilistic formulations.\nWe can analyze a corpus of documents with LDA by examining the posterior distribution of the topics \u03b2, topic proportions \u03b8, and topic assignments z conditioned on the documents. This reveals latent structure in the collection that can be used for prediction or data exploration. This posterior cannot be computed directly [7], and is usually approximated using Markov Chain Monte Carlo (MCMC) methods or variational inference. Both classes of methods are effective, but both present significant computational challenges in the face of massive data sets.Developing scalable approximate inference methods for topic models is an active area of research [3,4,5,11].\nTo this end, we develop online variational inference for LDA, an approximate posterior inference algorithm that can analyze massive collections of documents. We first review the traditional variational Bayes algorithm for LDA and its objective function, then present our online method, and show that it converges to a stationary point of the same objective function.", "publication_ref": ["b6", "b7", "b8", "b9", "b6", "b2", "b3", "b4", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Batch variational Bayes for LDA", "text": "In Variational Bayesian inference (VB) the true posterior is approximated by a simpler distribution q(z, \u03b8, \u03b2), which is indexed by a set of free parameters [12,13]. These parameters are optimized to maximize the Evidence Lower BOund (ELBO):\nlog p(w|\u03b1, \u03b7) \u2265L(w, \u03c6, \u03b3, \u03bb) E q [log p(w, z, \u03b8, \u03b2|\u03b1, \u03b7)] \u2212 E q [log q(z, \u03b8, \u03b2)].(1)\nMaximizing the ELBO is equivalent to minimizing the KL divergence between q(z, \u03b8, \u03b2) and the posterior p(z, \u03b8, \u03b2|w, \u03b1, \u03b7). Following [7], we choose a fully factorized distribution q of the form\nq(z di = k) = \u03c6 dw di k ; q(\u03b8 d ) = Dirichlet(\u03b8 d ; \u03b3 d ); q(\u03b2 k ) = Dirichlet(\u03b2 k ; \u03bb k ),(2)\nThe posterior over the per-word topic assignments z is parameterized by \u03c6, the posterior over the perdocument topic weights \u03b8 is parameterized by \u03b8, and the posterior over the topics \u03b2 is parameterized by \u03bb. As a shorthand, we refer to \u03bb as \"the topics.\" Equation 1 factorizes to L(w, \u03c6, \u03b3, \u03bb)\n= d E q [log p(w d |\u03b8 d , z d , \u03b2)] + E q [log p(z d |\u03b8 d )] \u2212 E q [log q(z d )] + E q [log p(\u03b8 d |\u03b1)] \u2212 E q [log q(\u03b8 d )] + (E q [log p(\u03b2|\u03b7)] \u2212 E q [log q(\u03b2)])/D .(3)\nNotice we have brought the per-corpus terms into the summation over documents, and divided them by the number of documents D. This step will help us to derive an online inference algorithm.\nWe now expand the expectations above to be functions of the variational parameters. This reveals that the variational objective relies only on n dw , the number of times word w appears in document d. When using VB-as opposed to MCMC-documents can be summarized by their word counts,\nL = d w n dw k \u03c6 dwk (E q [log \u03b8 dk ] + E q [log \u03b2 kw ] \u2212 log \u03c6 dwk ) \u2212 log \u0393( k \u03b3 dk ) + k (\u03b1 \u2212 \u03b3 dk )E q [log \u03b8 dk ] + log \u0393(\u03b3 dk ) + ( k \u2212 log \u0393( w \u03bb kw ) + w (\u03b7 \u2212 \u03bb kw )E q [log \u03b2 kw ] + log \u0393(\u03bb kw ))/D + log \u0393(K\u03b1) \u2212 K log \u0393(\u03b1) + (log \u0393(W \u03b7) \u2212 W log \u0393(\u03b7))/D d (n d , \u03c6 d , \u03b3 d , \u03bb),(4)\nwhere W is the size of the vocabulary and D is the number of documents. L can be optimized using coordinate ascent over the variational parameters \u03c6, \u03b3, \u03bb [7]:\n\u03c6 dwk \u221d exp{E q [log \u03b8 dk ] + E q [log \u03b2 kw ]}; \u03b3 dk = \u03b1 + w n dw \u03c6 dwk ; \u03bb kw = \u03b7 + d n dw \u03c6 dwk .(5)\nThe expectations under q of log \u03b8 and log \u03b2 are\nE q [log \u03b8 dk ] = \u03a8(\u03b3 dk ) \u2212 \u03a8( K i=1 \u03b3 di ); E q [log \u03b2 kw ] = \u03a8(\u03bb kw ) \u2212 \u03a8( W i=1 \u03bb ki ),(6)\nwhere \u03a8 denotes the digamma function (the first derivative of the logarithm of the gamma function).\nThe updates in equation 5 are guaranteed to converge to a stationary point of the ELBO. By analogy to the Expectation-Maximization (EM) algorithm [14], we can partition these updates into an \"E\" step-iteratively updating \u03b3 and \u03c6 until convergence, holding \u03bb fixed-and an \"M\" step-updating \u03bb given \u03c6. In practice, this algorithm converges to a better solution if we reinitialize \u03b3 and \u03c6 before each E step. Algorithm 1 outlines batch VB for LDA.", "publication_ref": ["b11", "b12", "b6", "b6", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 1 Batch variational Bayes for LDA", "text": "Initialize \u03bb randomly. while relative improvement in L(w, \u03c6, \u03b3, \u03bb) > 0.00001 do E step:\nfor d = 1 to D do Initialize \u03b3 dk = 1. (The constant 1 is arbitrary.) repeat Set \u03c6 dwk \u221d exp{E q [log \u03b8 dk ] + E q [log \u03b2 kw ]} Set \u03b3 dk = \u03b1 + w \u03c6 dwk n dw until 1 K k |change in\u03b3 dk | < 0.00001 end for M step: Set \u03bb kw = \u03b7 + d n dw \u03c6 dwk end while 2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Online variational inference for LDA", "text": "Algorithm 1 has constant memory requirements and empirically converges faster than batch collapsed Gibbs sampling [3]. However, it still requires a full pass through the entire corpus each iteration. It can therefore be slow to apply to very large datasets, and is not naturally suited to settings where new data is constantly arriving. We propose an online variational inference algorithm for fitting \u03bb, the parameters to the variational posterior over the topic distributions \u03b2. Our algorithm is nearly as simple as the batch VB algorithm, but converges much faster for large datasets.\nA good setting of the topics \u03bb is one for which the ELBO L is as high as possible after fitting the per-document variational parameters \u03b3 and \u03c6 with the E step defined in algorithm 1. Let \u03b3(n d , \u03bb) and \u03c6(n d , \u03bb) be the values of \u03b3 d and \u03c6 d produced by the E step. Our goal is to set \u03bb to maximize\nL(n, \u03bb) d (n d , \u03b3(n d , \u03bb), \u03c6(n d , \u03bb), \u03bb),(7)\nwhere\n(n d , \u03b3 d , \u03c6 d , \u03bb)\nis the dth document's contribution to the variational bound in equation 4. This is analogous to the goal of least-squares matrix factorization, although the ELBO for LDA is less convenient to work with than a simple squared loss function such as the one in [10].\nOnline VB for LDA (\"online LDA\") is described in algorithm 2. As the tth vector of word counts n t is observed, we perform an E step to find locally optimal values of \u03b3 t and \u03c6 t , holding \u03bb fixed. We then compute\u03bb, the setting of \u03bb that would be optimal (given \u03c6 t ) if our entire corpus consisted of the single document n t repeated D times. D is the number of unique documents available to the algorithm, e.g. the size of a corpus. (In the true online case D \u2192 \u221e, corresponding to empirical Bayes estimation of \u03b2.) We then update \u03bb using a weighted average of its previous value and\u03bb.\nThe weight given to\u03bb is given by \u03c1 t (\u03c4 0 + t) \u2212\u03ba , where \u03ba \u2208 (0.5, 1] controls the rate at which old values of\u03bb are forgotten and \u03c4 0 \u2265 0 slows down the early iterations of the algorithm. The condition that \u03ba \u2208 (0.5, 1] is needed to guarantee convergence. We show in section 2.3 that online LDA corresponds to a stochastic natural gradient algorithm on the variational objective L [15,16].\nThis algorithm closely resembles one proposed in [16] for online VB on models with hidden datathe most important difference is that we use an approximate E step to optimize \u03b3 t and \u03c6 t , since we cannot compute the conditional distribution p(z t , \u03b8 t |\u03b2, n t , \u03b1) exactly.", "publication_ref": ["b2", "b9", "b14", "b15", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Mini-batches.", "text": "A common technique in stochastic learning is to consider multiple observations per update to reduce noise [6,17]. In online LDA, this means computing\u03bb using S > 1 observations:\n\u03bb kw = \u03b7 + D S s n tsk \u03c6 tskw ,(8)\nwhere n ts is the sth document in mini-batch t. The variational parameters \u03c6 ts and \u03b3 ts for this document are fit with a normal E step. Note that we recover batch VB when S = D and \u03ba = 0.\nHyperparameter estimation. In batch variational LDA, point estimates of the hyperparameters \u03b1 and \u03b7 can be fit given \u03b3 and \u03bb using a linear-time Newton-Raphson method [7]. We can likewise Algorithm 2 Online variational Bayes for LDA Define \u03c1 t (\u03c4 0 + t) \u2212\u03ba Initialize \u03bb randomly. for t = 0 to \u221e do E step:\nInitialize \u03b3 tk = 1. (The constant 1 is arbitrary.) repeat Set \u03c6 twk \u221d exp{E q [log \u03b8 tk ] + E q [log \u03b2 kw ]} Set \u03b3 tk = \u03b1 + w \u03c6 twk n tw until 1 K k |change in\u03b3 tk | < 0.00001 M step: Compute\u03bb kw = \u03b7 + Dn tw \u03c6 twk Set \u03bb = (1 \u2212 \u03c1 t )\u03bb + \u03c1 t\u03bb .\nend for incorporate updates for \u03b1 and \u03b7 into online LDA:\n\u03b1 \u2190 \u03b1 \u2212 \u03c1 t\u03b1 (\u03b3 t ); \u03b7 \u2190 \u03b7 \u2212 \u03c1 t\u03b7 (\u03bb),(9)\nwhere\u03b1(\u03b3 t ) is the inverse of the Hessian times the gradient \u2207 \u03b1 (n t , \u03b3 t , \u03c6 t , \u03bb),\u03b7(\u03bb) is the inverse of the Hessian times the gradient \u2207 \u03b7 L, and \u03c1 t (\u03c4 0 + t) \u2212\u03ba as elsewhere.", "publication_ref": ["b5", "b16", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Analysis of convergence", "text": "In this section we show that algorithm 2 converges to a stationary point of the objective defined in equation 7. Since variational inference replaces sampling with optimization, we can use results from stochastic optimization to analyze online LDA. Stochastic optimization algorithms optimize an objective using noisy estimates of its gradient [18]. Although there is no explicit gradient computation, algorithm 2 can be interpreted as a stochastic natural gradient algorithm [16,15].\nWe begin by deriving a related first-order stochastic gradient algorithm for LDA. Let g(n) denote the population distribution over documents n from which we will repeatedly sample documents:\ng(n) 1 D D d=1 I[n = n d ].(10)\nI[n = n d ] is 1 if n = n d\nand 0 otherwise. If this population consists of the D documents in the corpus, then we can rewrite equation 7 as\nL(g, \u03bb) DE g [ (n, \u03b3(n, \u03bb), \u03c6(n, \u03bb), \u03bb)|\u03bb].(11)\nwhere is defined as in equation 3. We can optimize equation 11 over \u03bb by repeatedly drawing an observation n t \u223c g, computing \u03b3 t \u03b3(n t , \u03bb) and \u03c6 t \u03c6(n t , \u03bb), and applying the update\n\u03bb \u2190 \u03bb + \u03c1 t D\u2207 \u03bb (n t , \u03b3 t , \u03c6 t , \u03bb)(12)\nwhere \u03c1 t (\u03c4 0 + t) \u2212\u03ba as in algorithm 2. If we condition on the current value of \u03bb and treat \u03b3 t and \u03c6 t as random variables drawn at the same time as each observed document n t , then\nE g [D\u2207 \u03bb (n t , \u03b3 t , \u03c6 t , \u03bb)|\u03bb] = \u2207 \u03bb d (n d , \u03b3 d , \u03c6 d , \u03bb). Thus, since \u221e t=0 \u03c1 t = \u221e and \u221e t=0 \u03c1 2\nt < \u221e, the analysis in [19] shows both that \u03bb converges and that the gradient \u2207 \u03bb d (n d , \u03b3 d , \u03c6 d , \u03bb) converges to 0, and thus that \u03bb converges to a stationary point. 1 The update in equation 12 only makes use of first-order gradient information. Stochastic gradient algorithms can be sped up by multiplying the gradient by the inverse of an appropriate positive definite matrix H [19]. One choice for H is the Hessian of the objective function. In variational inference, an alternative is to use the Fisher information matrix of the variational distribution q (i.e., the Hessian of the log of the variational probability density function), which corresponds to using a natural gradient method instead of a (quasi-) Newton method [16,15]. Following the analysis in [16], the gradient of the per-document ELBO can be written as\n\u2202 (nt,\u03b3t,\u03c6t,\u03bb) \u2202\u03bb kw = W v=1 \u2202Eq[log \u03b2 kv ] \u2202\u03bb kw (\u2212\u03bb kv /D + \u03b7/D + n tv \u03c6 tvk ) = W v=1 \u2212 \u2202 2 log q(\u03b2 k ) \u2202\u03bb kv \u2202\u03bb kw (\u2212\u03bb kv /D + \u03b7/D + n tv \u03c6 tvk ),(13)\nwhere we have used the fact that E q [log \u03b2 kv ] is the derivative of the log-normalizer of q(log \u03b2 k ). By definition, multiplying equation 13 by the inverse of the Fisher information matrix yields\n\u2212 \u2202 2 log q(log \u03b2 k ) \u2202\u03bb k \u2202\u03bb T k \u22121 \u2202 (nt,\u03b3t,\u03c6t,\u03bb) \u2202\u03bb k w = \u2212\u03bb kw /D + \u03b7/D + n tw \u03c6 twk . (14\n)\nMultiplying equation 14 by \u03c1 t D and adding it to \u03bb kw yields the update for \u03bb in algorithm 2. Thus we can interpret our algorithm as a stochastic natural gradient algorithm, as in [16].", "publication_ref": ["b17", "b15", "b14", "b18", "b0", "b18", "b15", "b14", "b15", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Related work", "text": "Comparison with other stochastic learning algorithms. In the standard stochastic gradient optimization setup, the number of parameters to be fit does not depend on the number of observations [19]. However, some learning algorithms must also fit a set of per-observation parameters (such as the per-document variational parameters \u03b3 d and \u03c6 d in LDA). The problem is addressed by online coordinate ascent algorithms such as those described in [20,21,16,17,10]. The goal of these algorithms is to set the global parameters so that the objective is as good as possible once the perobservation parameters are optimized. Most of these approaches assume the computability of a unique optimum for the per-observation parameters, which is not available for LDA.\nEfficient sampling methods. Markov Chain Monte Carlo (MCMC) methods form one class of approximate inference algorithms for LDA. Collapsed Gibbs Sampling (CGS) is a popular MCMC approach that samples from the posterior over topic assignments z by repeatedly sampling the topic assignment z di conditioned on the data and all other topic assignments [22].\nOne online MCMC approach adapts CGS by sampling topic assignments z di based on the topic assignments and data for all previously analyzed words, instead of all other words in the corpus [23]. This algorithm is fast and has constant memory requirements, but is not guaranteed to converge to the posterior. Two alternative online MCMC approaches were considered in [24]. The first, called incremental LDA, periodically resamples the topic assignments for previously analyzed words. The second approach uses particle filtering instead of CGS. In a study in [24], none of these three online MCMC algorithms performed as well as batch CGS.\nInstead of online methods, the authors of [4] used parallel computing to apply LDA to large corpora. They developed two approximate parallel CGS schemes for LDA that gave similar predictive performance on held-out documents to batch CGS. However, they require parallel hardware, and their complexity and memory costs still scale linearly with the number of documents.\nExcept for the algorithm in [23] (which is not guaranteed to converge), all of the MCMC algorithms described above have memory costs that scale linearly with the number of documents analyzed. By contrast, batch VB can be implemented using constant memory, and parallelizes easily. As we will show in the next section, its online counterpart is even faster.", "publication_ref": ["b18", "b19", "b20", "b15", "b16", "b9", "b21", "b22", "b23", "b23", "b3", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We ran several experiments to evaluate online LDA's efficiency and effectiveness. The first set of experiments compares algorithms 1 and 2 on static datasets. The second set of experiments evaluates online VB in the setting where new documents are constantly being observed. Both algorithms were implemented in Python using Numpy. The implementations are as similar as possible. 2  We use perplexity on held-out data as a measure of model fit. Perplexity is defined as the geometric mean of the inverse marginal probability of each word in the held-out set of documents:\nperplexity(n test , \u03bb, \u03b1) exp \u2212( i log p(n test i |\u03b1, \u03b2))/( i,w n test iw )(15)\nwhere n i test denotes the vector of word counts for the ith document. Since we cannot directly compute log p(n test i |\u03b1, \u03b2), we use a lower bound on perplexity as a proxy:\nperplexity(n test , \u03bb, \u03b1) \u2264 exp \u2212( i E q [log p(n test i , \u03b8 i , z i |\u03b1, \u03b2)] \u2212 E q [log q(\u03b8 i , z i )])( i,w n test iw ) .(16)\nThe per-document parameters \u03b3 i and \u03c6 i for the variational distributions q(\u03b8 i ) and q(z i ) are fit using the E step in algorithm 2. The topics \u03bb are fit to a training set of documents and then held fixed. In all experiments \u03b1 and \u03b7 are fixed at 0.01 and the number of topics K = 100.\nThere is some question as to the meaningfulness of perplexity as a metric for comparing different topic models [25]. Held-out likelihood metrics are nonetheless well suited to measuring how well an inference algorithm accomplishes the specific optimization task defined by a model. Evaluating learning parameters. Online LDA introduces several learning parameters: \u03ba \u2208 (0.5, 1], which controls how quickly old information is forgotten; \u03c4 0 \u2265 0, which downweights early iterations; and the mini-batch size S, which controls how many documents are used each iteration. Although online LDA converges to a stationary point for any valid \u03ba, \u03c4 0 , and S, the quality of this stationary point and the speed of convergence may depend on how the learning parameters are set.\nWe evaluated a range of settings of the learning parameters \u03ba, \u03c4 0 , and S on two corpora: 352,549 documents from the journal Nature 3 and 100,000 documents downloaded from the English ver-sion of Wikipedia 4 . For each corpus, we set aside a 1,000-document test set and a separate 1,000-document validation set. We then ran online LDA for five hours on the remaining documents from each corpus for \u03ba \u2208 {0.5, 0.6, 0.7, 0.8, 0.9, 1.0}, \u03c4 0 \u2208 {1, 4, 16, 64, 256, 1024}, and S \u2208 {1, 4, 16, 64, 256, 1024, 4096, 16384}, for a total of 288 runs per corpus. After five hours of CPU time, we computed perplexity on the test sets for the topics \u03bb obtained at the end of each fit. Table 1 summarizes the best settings for each corpus of \u03ba and \u03c4 0 for a range of settings of S. The supplement includes a more exhaustive summary. The best learning parameter settings for both corpora were \u03ba = 0.5, \u03c4 0 = 64, and S = 4096. The best settings of \u03ba and \u03c4 0 are consistent across the two corpora. For mini-batch sizes from 256 to 16384 there is little difference in perplexity scores.\nSeveral trends emerge from these results. Higher values of the learning rate \u03ba and the downweighting parameter \u03c4 0 lead to better performance for small mini-batch sizes S, but worse performance for larger values of S. Mini-batch sizes of at least 256 documents outperform smaller mini-batch sizes.\nComparing batch and online on fixed corpora. To compare batch LDA to online LDA, we evaluated held-out perplexity as a function of time on the Nature and Wikipedia corpora above. We tried various mini-batch sizes from 1 to 16,384, using the best learning parameters for each mini-batch size found in the previous study of the Nature corpus. We also evaluated batch LDA fit to a 10,000document subset of the training corpus. We computed perplexity on a separate validation set from the test set used in the previous experiment. Each algorithm ran for 24 hours of CPU time.\nFigure 2 summarizes the results. On the larger Nature corpus, online LDA finds a solution as good as the batch algorithm's with much less computation. On the smaller Wikipedia corpus, the online algorithm finds a better solution than the batch algorithm does. The batch algorithm converges quickly on the 10,000-document corpora, but makes less accurate predictions on held-out documents.\nTrue online. To demonstrate the ability of online VB to perform in a true online setting, we wrote a Python script to continually download and analyze mini-batches of articles chosen at random from a list of approximately 3.3 million Wikipedia articles. This script can download and analyze about 60,000 articles an hour. It completed a pass through all 3.3 million articles in under three days. The amount of time needed to download an article and convert it to a vector of word counts is comparable to the amount of time that the online LDA algorithm takes to analyze it.\nWe ran online LDA with \u03ba = 0.5, \u03c4 0 = 1024, and S = 1024. Figure 1 shows the evolution of the perplexity obtained on the held-out validation set of 1,000 Wikipedia articles by the online algorithm as a function of number of articles seen. Shown for comparison is the perplexity obtained by the online algorithm (with the same parameters) fit to only 98,000 Wikipedia articles, and that obtained by the batch algorithm fit to the same 98,000 articles.\nThe online algorithm outperforms the batch algorithm regardless of which training dataset is used, but it does best with access to a constant stream of novel documents. The batch algorithm's failure to outperform the online algorithm on limited data may be due to stochastic gradient's robustness to local optima [19]. The online algorithm converged after analyzing about half of the 3.3 million articles. Even one iteration of the batch algorithm over that many articles would have taken days.", "publication_ref": ["b24", "b18"], "figure_ref": ["fig_2"], "table_ref": ["tab_0"]}, {"heading": "Discussion", "text": "We have developed online variational Bayes (VB) for LDA. This algorithm requires only a few more lines of code than the traditional batch VB of [7], and is handily applied to massive and streaming document collections. Online VB for LDA approximates the posterior as well as previous approaches in a fraction of the time. The approach we used to derive an online version of batch VB for LDA is general (and simple) enough to apply to a wide variety of hierarchical Bayesian models.", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgments D.M. Blei is supported by ONR 175-6343, NSF CAREER 0745520, AFOSR 09NL202, the Alfred P. Sloan foundation, and a grant from Google. F. Bach is supported by ANR (MGA project). 4 For the Wikipedia articles, we removed all words not from a fixed vocabulary of 7,995 common words. This vocabulary was obtained by removing words less than 3 characters long from a list of the 10,000 most common words in Project Gutenberg texts obtained from http://en.wiktionary.org/wiki/Wiktionary:Frequency lists.", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Variational inference for large-scale models of discrete choice", "journal": "", "year": "2008", "authors": "M Braun; J Mcauliffe"}, {"ref_id": "b1", "title": "Variational methods for the Dirichlet process", "journal": "", "year": "2004", "authors": "D Blei; M Jordan"}, {"ref_id": "b2", "title": "On smoothing and inference for topic models", "journal": "", "year": "2009", "authors": "A Asuncion; M Welling; P Smyth; Y W Teh"}, {"ref_id": "b3", "title": "Distributed inference for latent Dirichlet allocation", "journal": "", "year": "2007", "authors": "D Newman; A Asuncion; P Smyth; M Welling"}, {"ref_id": "b4", "title": "Parallel inference for latent Dirichlet allocation on graphics processing units", "journal": "", "year": "2009", "authors": "Feng Yan; Ningyi Xu; Yuan Qi"}, {"ref_id": "b5", "title": "The tradeoffs of large scale learning", "journal": "", "year": "2008", "authors": "L Bottou; O Bousquet"}, {"ref_id": "b6", "title": "Latent Dirichlet allocation", "journal": "Journal of Machine Learning Research", "year": "2003-01", "authors": "D Blei; A Ng; M Jordan"}, {"ref_id": "b7", "title": "Rethinking lda: Why priors matter", "journal": "", "year": "2009", "authors": "Hanna Wallach; David Mimno; Andrew Mccallum"}, {"ref_id": "b8", "title": "Variational extentions to EM and multinomial PCA", "journal": "", "year": "2002", "authors": "W Buntine"}, {"ref_id": "b9", "title": "Online learning for matrix factorization and sparse coding", "journal": "Journal of Machine Learning Research", "year": "2010", "authors": "J Mairal; F Bach; J Ponce; G Sapiro"}, {"ref_id": "b10", "title": "Efficient methods for topic model inference on streaming document collections", "journal": "", "year": "2009", "authors": "L Yao; D Mimno; A Mccallum"}, {"ref_id": "b11", "title": "Introduction to variational methods for graphical models", "journal": "", "year": "1999", "authors": "M Jordan; Z Ghahramani; T Jaakkola; L Saul"}, {"ref_id": "b12", "title": "A variational Bayesian framework for graphical models", "journal": "", "year": "2000", "authors": "H Attias"}, {"ref_id": "b13", "title": "Maximum likelihood from incomplete data via the EM algorithm", "journal": "Journal of the Royal Statistical Society, Series B", "year": "1977", "authors": "A Dempster; N Laird; D Rubin"}, {"ref_id": "b14", "title": "Stochastic approximations and efficient learning. The Handbook of Brain Theory and Neural Networks", "journal": "The MIT Press", "year": "2002", "authors": "L Bottou; N Murata"}, {"ref_id": "b15", "title": "Online model selection based on the variational Bayes", "journal": "Neural Computation", "year": "2001", "authors": "M A Sato"}, {"ref_id": "b16", "title": "Online EM for unsupervised models", "journal": "", "year": "2009", "authors": "P Liang; D Klein"}, {"ref_id": "b17", "title": "A stochastic approximation method", "journal": "The Annals of Mathematical Statistics", "year": "1951", "authors": "H Robbins; S Monro"}, {"ref_id": "b18", "title": "Online learning and stochastic approximations", "journal": "Cambridge University Press", "year": "1998", "authors": "L Bottou"}, {"ref_id": "b19", "title": "A view of the EM algorithm that justifies incremental, sparse, and other variants", "journal": "", "year": "1998", "authors": "R M Neal; G E Hinton"}, {"ref_id": "b20", "title": "On-line EM algorithm for the normalized Gaussian network", "journal": "Neural Computation", "year": "2000", "authors": "M A Sato; S Ishii"}, {"ref_id": "b21", "title": "Finding scientific topics. Proc. National Academy of Science", "journal": "", "year": "2004", "authors": "T Griffiths; M Steyvers"}, {"ref_id": "b22", "title": "Modeling and predicting personal information dissemination behavior", "journal": "ACM", "year": "2005", "authors": "X Song; C Y Lin; B L Tseng; M T Sun"}, {"ref_id": "b23", "title": "Online inference of topics with latent Dirichlet allocation", "journal": "", "year": "2009", "authors": "K R Canini; L Shi; T L Griffiths"}, {"ref_id": "b24", "title": "Reading tea leaves: How humans interpret topic models", "journal": "", "year": "2009", "authors": "J Chang; J Boyd-Graber; S Gerrish; C Wang; D Blei"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "(n d , \u03c6 d , \u03b3 d , \u03bb) denotes the contribution of document d to the ELBO.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure2: Held-out perplexity obtained on the Nature (left) and Wikipedia (right) corpora as a function of CPU time. For moderately large mini-batch sizes, online LDA finds solutions as good as those that the batch LDA finds, but with much less computation. When fit to a 10,000-document subset of the training corpus batch LDA's speed improves, but its performance suffers.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Best settings of \u03ba and \u03c4 0 for various mini-batch sizes S, with resulting perplexities on Nature and Wikipedia corpora.", "figure_data": "Best parameter settings for Nature corpusS1416642561024 4096 16384\u03ba0.90.80.80.70.60.50.50.5\u03c4 01024 1024 1024 1024 1024 256641Perplexity 1132 1087 1052 1053 1042 1031 1030 1046Best parameter settings for Wikipedia corpusS1416642561024 4096 16384\u03ba0.90.90.80.70.60.50.50.5\u03c4 01024 1024 1024 1024 1024 1024 641Perplexity 67564061159558858458058425001000Batch size 900Batch sizePerplexity200000001 00016 00256 01024 04096 16384 800 Perplexity00001 00016 00256 01024 04096 163841500batch10K batch98K 700batch10K batch98K60010 1Time in seconds (log scale) 10 2 10 3 10 4Time in seconds (log scale) 10 1 10 2 10 3 10 4"}], "formulas": [{"formula_id": "formula_0", "formula_text": "log p(w|\u03b1, \u03b7) \u2265L(w, \u03c6, \u03b3, \u03bb) E q [log p(w, z, \u03b8, \u03b2|\u03b1, \u03b7)] \u2212 E q [log q(z, \u03b8, \u03b2)].(1)", "formula_coordinates": [3.0, 143.52, 311.01, 360.48, 9.76]}, {"formula_id": "formula_1", "formula_text": "q(z di = k) = \u03c6 dw di k ; q(\u03b8 d ) = Dirichlet(\u03b8 d ; \u03b3 d ); q(\u03b2 k ) = Dirichlet(\u03b2 k ; \u03bb k ),(2)", "formula_coordinates": [3.0, 144.52, 351.1, 359.48, 10.32]}, {"formula_id": "formula_2", "formula_text": "= d E q [log p(w d |\u03b8 d , z d , \u03b2)] + E q [log p(z d |\u03b8 d )] \u2212 E q [log q(z d )] + E q [log p(\u03b8 d |\u03b1)] \u2212 E q [log q(\u03b8 d )] + (E q [log p(\u03b2|\u03b7)] \u2212 E q [log q(\u03b2)])/D .(3)", "formula_coordinates": [3.0, 174.43, 401.61, 329.57, 25.62]}, {"formula_id": "formula_3", "formula_text": "L = d w n dw k \u03c6 dwk (E q [log \u03b8 dk ] + E q [log \u03b2 kw ] \u2212 log \u03c6 dwk ) \u2212 log \u0393( k \u03b3 dk ) + k (\u03b1 \u2212 \u03b3 dk )E q [log \u03b8 dk ] + log \u0393(\u03b3 dk ) + ( k \u2212 log \u0393( w \u03bb kw ) + w (\u03b7 \u2212 \u03bb kw )E q [log \u03b2 kw ] + log \u0393(\u03bb kw ))/D + log \u0393(K\u03b1) \u2212 K log \u0393(\u03b1) + (log \u0393(W \u03b7) \u2212 W log \u0393(\u03b7))/D d (n d , \u03c6 d , \u03b3 d , \u03bb),(4)", "formula_coordinates": [3.0, 142.48, 495.54, 361.52, 69.39]}, {"formula_id": "formula_4", "formula_text": "\u03c6 dwk \u221d exp{E q [log \u03b8 dk ] + E q [log \u03b2 kw ]}; \u03b3 dk = \u03b1 + w n dw \u03c6 dwk ; \u03bb kw = \u03b7 + d n dw \u03c6 dwk .(5)", "formula_coordinates": [3.0, 108.0, 610.44, 396.0, 19.92]}, {"formula_id": "formula_5", "formula_text": "E q [log \u03b8 dk ] = \u03a8(\u03b3 dk ) \u2212 \u03a8( K i=1 \u03b3 di ); E q [log \u03b2 kw ] = \u03a8(\u03bb kw ) \u2212 \u03a8( W i=1 \u03bb ki ),(6)", "formula_coordinates": [3.0, 140.66, 645.34, 363.34, 14.11]}, {"formula_id": "formula_6", "formula_text": "for d = 1 to D do Initialize \u03b3 dk = 1. (The constant 1 is arbitrary.) repeat Set \u03c6 dwk \u221d exp{E q [log \u03b8 dk ] + E q [log \u03b2 kw ]} Set \u03b3 dk = \u03b1 + w \u03c6 dwk n dw until 1 K k |change in\u03b3 dk | < 0.00001 end for M step: Set \u03bb kw = \u03b7 + d n dw \u03c6 dwk end while 2.", "formula_coordinates": [4.0, 108.0, 130.98, 219.08, 143.99]}, {"formula_id": "formula_7", "formula_text": "L(n, \u03bb) d (n d , \u03b3(n d , \u03bb), \u03c6(n d , \u03bb), \u03bb),(7)", "formula_coordinates": [4.0, 216.59, 400.61, 287.41, 11.18]}, {"formula_id": "formula_8", "formula_text": "(n d , \u03b3 d , \u03c6 d , \u03bb)", "formula_coordinates": [4.0, 140.02, 419.09, 58.73, 9.68]}, {"formula_id": "formula_9", "formula_text": "\u03bb kw = \u03b7 + D S s n tsk \u03c6 tskw ,(8)", "formula_coordinates": [4.0, 246.25, 656.5, 257.75, 13.47]}, {"formula_id": "formula_10", "formula_text": "Initialize \u03b3 tk = 1. (The constant 1 is arbitrary.) repeat Set \u03c6 twk \u221d exp{E q [log \u03b8 tk ] + E q [log \u03b2 kw ]} Set \u03b3 tk = \u03b1 + w \u03c6 twk n tw until 1 K k |change in\u03b3 tk | < 0.00001 M step: Compute\u03bb kw = \u03b7 + Dn tw \u03c6 twk Set \u03bb = (1 \u2212 \u03c1 t )\u03bb + \u03c1 t\u03bb .", "formula_coordinates": [5.0, 127.93, 144.22, 187.88, 90.66]}, {"formula_id": "formula_11", "formula_text": "\u03b1 \u2190 \u03b1 \u2212 \u03c1 t\u03b1 (\u03b3 t ); \u03b7 \u2190 \u03b7 \u2212 \u03c1 t\u03b7 (\u03bb),(9)", "formula_coordinates": [5.0, 228.09, 291.07, 275.91, 9.65]}, {"formula_id": "formula_12", "formula_text": "g(n) 1 D D d=1 I[n = n d ].(10)", "formula_coordinates": [5.0, 250.06, 460.34, 253.94, 14.56]}, {"formula_id": "formula_13", "formula_text": "I[n = n d ] is 1 if n = n d", "formula_coordinates": [5.0, 108.0, 482.51, 104.26, 9.65]}, {"formula_id": "formula_14", "formula_text": "L(g, \u03bb) DE g [ (n, \u03b3(n, \u03bb), \u03c6(n, \u03bb), \u03bb)|\u03bb].(11)", "formula_coordinates": [5.0, 215.57, 513.97, 288.43, 9.68]}, {"formula_id": "formula_15", "formula_text": "\u03bb \u2190 \u03bb + \u03c1 t D\u2207 \u03bb (n t , \u03b3 t , \u03c6 t , \u03bb)(12)", "formula_coordinates": [5.0, 240.39, 564.65, 263.61, 9.68]}, {"formula_id": "formula_16", "formula_text": "E g [D\u2207 \u03bb (n t , \u03b3 t , \u03c6 t , \u03bb)|\u03bb] = \u2207 \u03bb d (n d , \u03b3 d , \u03c6 d , \u03bb). Thus, since \u221e t=0 \u03c1 t = \u221e and \u221e t=0 \u03c1 2", "formula_coordinates": [5.0, 108.0, 604.5, 384.48, 14.11]}, {"formula_id": "formula_17", "formula_text": "\u2202 (nt,\u03b3t,\u03c6t,\u03bb) \u2202\u03bb kw = W v=1 \u2202Eq[log \u03b2 kv ] \u2202\u03bb kw (\u2212\u03bb kv /D + \u03b7/D + n tv \u03c6 tvk ) = W v=1 \u2212 \u2202 2 log q(\u03b2 k ) \u2202\u03bb kv \u2202\u03bb kw (\u2212\u03bb kv /D + \u03b7/D + n tv \u03c6 tvk ),(13)", "formula_coordinates": [6.0, 175.26, 114.62, 328.74, 34.8]}, {"formula_id": "formula_18", "formula_text": "\u2212 \u2202 2 log q(log \u03b2 k ) \u2202\u03bb k \u2202\u03bb T k \u22121 \u2202 (nt,\u03b3t,\u03c6t,\u03bb) \u2202\u03bb k w = \u2212\u03bb kw /D + \u03b7/D + n tw \u03c6 twk . (14", "formula_coordinates": [6.0, 173.07, 188.16, 326.78, 25.07]}, {"formula_id": "formula_19", "formula_text": ")", "formula_coordinates": [6.0, 499.85, 195.43, 4.15, 8.64]}, {"formula_id": "formula_20", "formula_text": "perplexity(n test , \u03bb, \u03b1) exp \u2212( i log p(n test i |\u03b1, \u03b2))/( i,w n test iw )(15)", "formula_coordinates": [7.0, 162.67, 477.17, 341.33, 13.79]}, {"formula_id": "formula_21", "formula_text": "perplexity(n test , \u03bb, \u03b1) \u2264 exp \u2212( i E q [log p(n test i , \u03b8 i , z i |\u03b1, \u03b2)] \u2212 E q [log q(\u03b8 i , z i )])( i,w n test iw ) .(16)", "formula_coordinates": [7.0, 108.0, 528.03, 403.68, 25.87]}], "doi": ""}