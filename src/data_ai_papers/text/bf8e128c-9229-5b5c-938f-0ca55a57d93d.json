{"title": "The Data Interaction Game", "authors": "Ben Mccamish; Vahid Ghadakchi; Arash Termehchy; Behrouz Touri; Liang Huang", "pub_date": "", "abstract": "As many users do not precisely know the structure and/or the content of databases, their queries do not exactly reflect their information needs. The database management systems (DBMS) may interact with users and leverage their feedback on the returned results to learn the information needs behind users' queries. Current query interfaces assume that users follow a fixed strategy of expressing their information needs, that is, the likelihood by which a user submits a query to express an information need remains unchanged during her interaction with the DBMS. Using a realworld interaction workload, we show that users learn and modify how to express their information needs during their interactions with the DBMS. We also show that users' learning is accurately modeled by a well-known reinforcement learning mechanism. As current data interaction systems assume that users do not modify their strategies, they cannot discover the information needs behind users' queries effectively. We model the interaction between users and DBMS as a game with identical interest between two rational agents whose goal is to establish a common language for representing information needs in form of queries. We propose a reinforcement learning method that learns and answers the information needs behind queries and adapts to the changes in users' strategies and prove that it improves the effectiveness of answering queries stochastically speaking. We analyze the challenges of efficient implementation of this method over large-scale relational databases and propose two efficient adaptations of this algorithm over largescale relational databases. Our extensive empirical studies over real-world query workloads and large-scale relational databases indicate that our algorithms are efficient. Our empirical results also show that our proposed learning mechanism is more effective than the state-of-the-art query answering method.", "sections": [{"heading": "INTRODUCTION", "text": "Most users do not know the structure and content of databases and concepts such as schema or formal query languages sufficiently well to express their information needs precisely in the form of queries [14,29,30]. They may convey their intents in easy-to-use but inherently ambiguous forms, such as keyword queries, which are open to numerous interpretations. Thus, it is very challenging for a database management system (DBMS) to understand and satisfy the intents behind these queries. The fundamental challenge in the interaction of these users and DBMS is that the users and DBMS represent intents in different forms.\nMany such users may explore a database to find answers for various intents over a rather long period of time. For these users, database querying is an inherently interactive and continuing process. As both the user and DBMS have the same goal of the user receiving her desired information, the user and DBMS would like to gradually improve their understandings of each other and reach a common language of representing intents over the course of various queries and interactions. The user may learn more about the structure and content of the database and how to express intents as she submits queries and observes the returned results. Also, the DBMS may learn more about how the user expresses her intents by leveraging user feedback on the returned results. The user feedback may include clicking on the relevant answers [52], the amount of time the user spends on reading the results [23], user's eye movements [28], or the signals sent in touch-based devises [34]. Ideally, the user and DBMS should establish as quickly as possible this common representation of intents in which the DBMS accurately understands all or most user's queries.\nResearchers have developed systems that leverage user feedback to help the DBMS understand the intent behind ill-specified and vague queries more precisely [10,11]. These systems, however, generally assume that a user does not modify her method of expressing intents throughout her interaction with the DBMS. For example, they maintain that the user picks queries to express an intent according to a fixed probability distribution. It is known that the learning methods that are useful in a static setting do not deliver desired outcomes in a setting where all agents may modify their strategies [17,24]. Hence, one may not be able to use current techniques to help the DBMS understand the users' information need in a rather long-term interaction.\nTo the best of our knowledge, the impact of user learning on database interaction has been generally ignored. In this paper, we propose a novel framework that formalizes the interaction between the user and the DBMS as a game with identical interest between two active and potentially rational agents: the user and DBMS. The common goal of the user and DBMS is to reach a mutual understanding on expressing information needs in the form of keyword queries. In each interaction, the user and DBMS receive certain payoff according to how much the returned results are relevant to the intent behind the submitted query. The user receives her payoff by consuming the relevant information and the DBMS becomes aware of its payoff by observing the user's feedback on the returned results. We believe that such a game-theoretic framework naturally models the long-term interaction between the user and DBMS. We explore the user learning mechanisms and propose algorithms for DBMS to improve its understanding of intents behind the user queries effectively and efficiently over large databases. In particular, we make the following contributions:\n\u2022 We model the long term interaction between the user and DBMS using keyword queries as a particular type of game called a signaling game [15] in Section 2. \u2022 Using extensive empirical studies over a real-world interaction log, we show that users modify the way they express their information need over their course of interactions in Section 3. We also show that this adaptation is accurately modeled by a wellknown reinforcement learning algorithm [44] in experimental game-theory. \u2022 Current systems generally assume that a user does not learn and/or modify her method of expressing intents throughout her interaction with the DBMS. However, it is known that the learning methods that are useful in static settings do not deliver desired outcomes in the dynamic ones [4]. We propose a method of answering user queries in a natural and interactive setting in Section 4 and prove that it improves the effectiveness of answering queries stochastically speaking, and converges almost surely. We show that our results hold for both the cases where the user adapts her strategy using an appropriate learning algorithm and the case where she follows a fixed strategy. \u2022 We describe our data interaction system that provides an efficient implementation of our reinforcement learning method on large relational databases in Section 5. In particular, we first propose an algorithm that implements our learning method called Reservoir. Then, using certain mild assumptions and the ideas of sampling over relational operators, we propose another algorithm called Poisson-Olken that implements our reinforcement learning scheme and considerably improves the efficiency of Reservoir. \u2022 We report the results of our extensive empirical studies on measuring the effectiveness of our reinforcement learning method and the efficiency of our algorithms using real-world and large interaction workloads, queries, and databases in Section 6. Our results indicate that our proposed reinforcement learning method is more effective than the start-of-the-art algorithm for longterm interactions. They also show that Poisson-Olken can process queries over large databases faster than the Reservoir algorithm.", "publication_ref": ["b13", "b29", "b30", "b52", "b22", "b28", "b34", "b9", "b10", "b16", "b23", "b14", "b44", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "A GAME-THEORETIC FRAMEWORK", "text": "Users and DBMSs typically achieve a common understanding gradually and using a querying/feedback paradigm. After submitting each query, the user may revise her strategy of expressing intents based on the returned result. If the returned answers satisfy her intent to a large extent, she may keep using the same query to articulate her intent. Otherwise, she may revise her strategy and choose another query to express her intent in the hope that the new query will provide her with more relevant answers. We will describe this behavior of users in Section 3 in more details. The user may also inform the database system about the degree by which the returned answers satisfy the intent behind the query using explicit or implicit feedback, e.g., click-through information [23].\nThe DBMS may update its interpretation of the query according to the user's feedback.\nIntuitively, one may model this interaction as a game between two agents with identical interests in which the agents communicate via sharing queries, results, and feedback on the results. In each interaction, both agents will receive some reward according to the degree by which the returned result for a query matches its intent. The user receives her rewards in the form of answers relevant to her intent and the DBMS receives its reward through getting positive feedback on the returned results. The final goal of both agents is to maximize the amount of reward they receive during the course of their interaction. Next, we describe the components and structure of this interaction game for relational databases.\nBasic Definitions: We fix two disjoint (countably) infinite sets of attributes and relation symbols. Every relation symbol R is associated with a set of attribute symbols denoted as sort(R). Let dom be a countably infinite set of constants, e.g., strings. An instance I R of relation symbol R with n = |sort(R)| is a (finite) subset of dom n . A schema S is a set of relation symbols. A database (instance) of S is a mapping over S that associates with each relation symbol R in S an instance of I R . In this paper, we assume that dom is a set of strings.", "publication_ref": ["b22"], "figure_ref": [], "table_ref": []}, {"heading": "Intent", "text": "An intent represents an information need sought after by the user. Current keyword query interfaces over relational databases generally assume that each intent is a query in a sufficiently expressive query language in the domain of interest, e.g., Select-Project-Join subset of SQL [14,30]. Our framework and results are orthogonal to the language that precisely describes the users' intents. Table 1 illustrates a database with schema Univ(Name, Abbreviation, State, Type, Ranking) that contains information about university rankings. A user may want to find the information about university MSU in Michigan, which is precisely represented by the intent e 2 in Table 2(a), which using the Datalog syntax [1] is: ans(z) \u2190 U niv(x, 'MSU ', 'MI ', y, z).", "publication_ref": ["b13", "b30", "b0"], "figure_ref": [], "table_ref": ["tab_0", "tab_1"]}, {"heading": "Query", "text": "Users' articulations of their intents are queries. Many users do not know the formal query language, e.g., SQL, that precisely describes their intents. Thus, they may prefer to articulate their intents in languages that are easy-to-use, relatively less complex, and ambiguous such as keyword query language [14,30]. In the proposed game-theoretic frameworks for database interaction, we assume that the user expresses her intents as keyword queries. More formally, we fix a countably infinite set of terms, i.e., keywords, T . A keyword query (query for short) is a nonempty (finite) set of terms in T . Consider the database instance in Table 1. Table 2 depicts a set of intents and queries over this database. Suppose the user wants to find the information about Michigan State University in Michigan, i.e. the intent e 2 . Because the user does not know any formal database query language and may not be sufficiently familiar with the content of the data, she may express intent e 2 using q 2 : 'MSU'.\nSome users may know a formal database query language that is sufficiently expressive to represent their intents. Nevertheless, because they may not know precisely the content and schema of the database, their submitted queries may not always be the same as their intents [11,32]. For example, a user may know how to write a SQL query. But, since she may not know the state abbreviation MI, she may articulate intent e 2 as ans(t) \u2190 U niv(x, 'MSU ', y, z, t), which is different from e 2 . We plan to extend our framework for these scenarios in future work. But, in this paper, we assume that users articulate their intents as keyword queries.", "publication_ref": ["b13", "b30", "b10", "b32"], "figure_ref": [], "table_ref": ["tab_0", "tab_1"]}, {"heading": "User Strategy", "text": "The user strategy indicates the likelihood by which the user submits query q given that her intent is e. In practice, a user has finitely many intents and submits finitely many queries in a finite period of time. Hence, we assume that the sets of the user's intents and queries are finite. We index each user's intent and query by 1 \u2264 i \u2264 m and 1 \u2264 j \u2264 n, respectively. A user strategy, denoted as U , is a m \u00d7 n row-stochastic matrix from her intents to her queries. The matrix on the top of Table 3(a) depicts a user strategy using intents and queries in Table 2. According to this strategy, the user submits query q 2 to express intents e 1 , e 2 , and e 3 .    ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2", "tab_1"]}, {"heading": "DBMS Strategy", "text": "The DBMS interprets queries to find the intents behind them. It usually interprets queries by mapping them to a subset of SQL [14,26,36]. Since the final goal of users is to see the result of applying the interpretation(s) on the underlying database, the DBMS runs its interpretation(s) over the database and returns its results. Moreover, since the user may not know SQL, suggesting possible SQL queries may not be useful. A DBMS may not exactly know the language that can express all users' intents. Current usable query interfaces, including keyword query systems, select a query language for the interpreted intents that is sufficiently complex to express many users' intents and is simple enough so that the interpretation and running its outcome(s) are done efficiently [14].\nAs an example consider current keyword query interfaces over relational databases [14]. Given constant v in database I and keyword w in keyword query q, let match(v, w) be a function that is true if w appears in v and false otherwise. A majority of keyword query interfaces interpret keyword queries as Select-Project-Join queries that have below certain number of joins and whose where clauses contain only conjunctions of match functions [26,36]. Using a larger subset of SQL, e.g. the ones with more joins, makes it inefficient to perform the interpretation and run its outcomes. Given schema S, the interpretation language of the DBMS, denoted as L, is a subset of SQL over S. We precisely define L for our implementation of DBMS strategy in Section 5. To interpret a keyword query, the DBMS searches L for the SQL queries that represent the intent behind the query as accurately as possible.\nBecause users may be overwhelmed by the results of many interpretations, keyword query interfaces use a deterministic realvalued scoring function to rank their interpretations and deliver only the results of top-k ones to the user [14]. It is known that such a deterministic approach may significantly limit the accuracy of interpreting queries in long-term interactions in which the information system utilizes user's feedback [3,25,49]. Because the DBMS shows only the result of interpretation(s) with the highest score(s) to the user, it receives feedback only on a small set of interpretations. Thus, its learning remains largely biased toward the initial set of highly ranked interpretations. For example, it may never learn that the intent behind a query is satisfied by an interpretation with a relatively low score according to the current scoring function.\nTo better leverage users feedback during the interaction, the DBMS must show the results of and get feedback on a sufficiently diverse set of interpretations [3,25,49]. Of course, the DBMS should ensure that this set of interpretations are relatively relevant to the query, otherwise the user may become discouraged and give up querying. This dilemma is called the exploitation versus exploration trade-off. A DBMS that only exploits, returns top-ranked interpretations according to its scoring function. Hence, the DBMS may adopt a stochastic strategy to both exploit and explore: it randomly selects and shows the results of intents such that the ones with higher scores are chosen with larger probabilities [3,25,49]. In this approach, users are mostly shown results of interpretations that are relevant to their intents according to the current knowledge of the DBMS and provide feedback on a relatively diverse set of interpretations. More formally, given Q is a set of all keyword queries, the DBMS strategy D is a stochastic mapping from Q to L. To the best of our knowledge, to search L efficiently, current keyword query interfaces limit their search per query to a finite subset of L [14,26,36]. In this paper, we follow a similar approach and assume that D maps each query to only a finite subset of L. The matrix on the bottom of Table 3(a) depicts a DBMS strategy for the intents and queries in Table 2. Based on this strategy, the DBMS uses a exploitative strategy and always interprets query q 2 as e 2 . The matrix on the bottom of Table 3(b) depicts another DBMS strategy for the same set of intents and queries. In this example, DBMS uses a randomized strategy and does both exploitation and exploration. For instance, it explores e 1 and e 2 to answer q 2 with equal probabilities, but it always returns e 2 in the response to q 1 .", "publication_ref": ["b13", "b26", "b36", "b13", "b13", "b26", "b36", "b13", "b2", "b25", "b49", "b2", "b25", "b49", "b2", "b25", "b49", "b13", "b26", "b36"], "figure_ref": [], "table_ref": ["tab_2", "tab_1", "tab_2"]}, {"heading": "Interaction & Adaptation", "text": "The data interaction game is a repeated game with identical interest between two players, the user and the DBMS. At each round of the game, i.e., a single interaction, the user selects an intent according to the prior probability distribution \u03c0 . She then picks the query q according to her strategy and submits it to the DBMS. The DBMS observes q and interprets q based on its strategy, and returns the results of the interpretation(s) on the underlying database to the user. The user provides some feedback on the returned tuples and informs the DBMS how relevant the tuples are to her intent. In this paper, we assume that the user informs the DBMS if some tuples satisfy the intent via some signal, e.g., selecting the tuple, in some interactions. The feedback signals may be noisy, e.g., a user may click on a tuple by mistake. Researchers have proposed models to accurately detect the informative signals [25]. Dealing with the issue of noisy signals is out of the scope of this paper.\nThe goal of both the user and the DBMS is to have as many satisfying tuples as possible in the returned tuples. Hence, both the user and the DBMS receive some payoff, i.e., reward, according to the degree by which the returned tuples match the intent. This payoff is measured based on the user feedback and using standard effectiveness metrics [37]. One example of such metrics is precision at k, p@k, which is the fraction of relevant tuples in the top-k returned tuples. At the end of each round, both the user and the DBMS receive a payoff equal to the value of the selected effectiveness metric for the returned result. We denote the payoff received by the players at each round of the game, i.e., a single interaction, for returning interpretation e \u2113 for intent e i as r (e i , e \u2113 ). This payoff is computed using the user's feedback on the result of interpretation e \u2113 over the underlying database.\nNext, we compute the expected payoff of the players. Since DBMS strategy D maps each query to a finite set of interpretations, and the set of submitted queries by a user, or a population of users, is finite, the set of interpretations for all queries submitted by a user, denoted as L s , is finite. Hence, we show the DBMS strategy for a user as an n \u00d7 o row-stochastic matrix from the set of the user's queries to the set of interpretations L s . We index each interpretation in L s by 1 \u2264 \u2113 \u2264 o. Each pair of the user and the DBMS strategy, (U ,D), is a strategy profile. The expected payoff for both players with strategy profile (U ,D) is as follows.\nu r (U , D) = m i=1 \u03c0 i n j=1 U i j o \u2113=1 D j \u2113 r (e i , e \u2113 ),(1)\nThe expected payoff reflects the degree by which the user and DBMS have reached a common language for communication. This value is high for the case in which the user knows which queries to pick to articulate her intents and the DBMS returns the results that satisfy the intents behind the user's queries. Hence, this function reflects the success of the communication and interaction. For example, given that all intents have equal prior probabilities, intuitively, the strategy profile in Table 3(b) shows a larger degree of mutual understanding between the players than the one in Table 3(a). This is reflected in their values of expected payoff as the expected payoffs of the former and latter are 2 3 and 1 3 , respectively. We note that the DBMS may not know the set of users' queries beforehand and does not compute the expected payoff directly. Instead, it uses query answering algorithms that leverage user feedback, such that the expected payoff improves over the course of several interactions as we will show in Section 4.\nNone of the players know the other player's strategy during the interaction. Given the information available to each player, it may modify its strategy at the end of each round (interaction). For example, the DBMS may reduce the probability of returning certain interpretations that has not received any positive feedback from the user in the previous rounds of the game. Let the user and DBMS strategy at round t \u2208 N of the game be U (t) and D(t), respectively. In round t \u2208 N of the game, the user and DBMS have access to the information about their past interactions. The user has access to her sequence of intents, queries, and results, the DBMS knows the sequence of queries and results, and both players have access to the sequence of payoffs (not expected payoffs) up to round t \u2212 1. It depends on the degree of rationality and abilities of the user and the DBMS how to leverage these pieces of information to improve the expected payoff of the game. For example, it may not be reasonable to assume that the user adopts a mechanism that requires instant access to the detailed information about her past interactions as it is not clear whether users can memorize this information for a longterm interaction. A data interaction game is represented as tuple\n(U (t), D(t), \u03c0 , (e u (t \u2212 1)), (q(t \u2212 1)), (e d (t \u2212 1)), (r (t \u2212 1)\n)) in which U (t) and D(t) are respectively the strategies of the user and DBMS at round t, \u03c0 is the prior probability of intents in U , (e u (t \u2212 1)) is the sequence of intents, (q(t \u2212 1)) is the sequence of queries, (e d (t \u2212 1)) is the sequence of interpretations, and (r (t \u2212 1))) is the sequence of payoffs up to time t. Table 4 contains the notation and concept definitions introduced in this section for future reference.", "publication_ref": ["b25", "b37"], "figure_ref": [], "table_ref": ["tab_2", "tab_2", "tab_3"]}, {"heading": "USER LEARNING MECHANISM", "text": "It is well established that humans show reinforcement behavior in learning [40,46]. Many lab studies with human subjects conclude ", "publication_ref": ["b40", "b46"], "figure_ref": [], "table_ref": []}, {"heading": "Notation Definition e i A user's intent q j", "text": "A query submitted by the user \u03c0 i\nThe prior probability that the user queries for e i r (e i , e \u2113 )\nThe reward when the user looks for e i and the DBMS returns e \u2113 U\nThe user strategy U i j\nThe probability that user submits q j for intent e i D\nThe DBMS strategy D j \u2113\nThe probability that DBMS intent e \u2113 for query q j (U , D)\nA strategy profile u r (U , D) The expected payoff of the strategy profile (U , D) computed using reward metric r based to Equation 1that one can model human learning using reinforcement learning models [40,46]. The exact reinforcement learning method used by a person, however, may vary based on her capabilities and the task at hand. We have performed an empirical study of a real-world interaction log to find the reinforcement learning method(s) that best explain the mechanism by which users adapt their strategies during interaction with a DBMS.", "publication_ref": ["b40", "b46"], "figure_ref": [], "table_ref": []}, {"heading": "Reinforcement Learning Methods", "text": "To provide a comprehensive comparison, we evaluate six reinforcement learning methods used to model human learning in experimental game theory and/or Human Computer Interaction (HCI) [9,44]. These methods mainly vary based on 1) the degree by which the user considers past interactions when computing future strategies, 2) how they update the user strategy, and 3) the rate by which they update the user strategy. Win-Keep/Lose-Randomize keeps a query with non-zero reward in past interactions for an intent. If such a query does not exist, it picks a query randomly. Latest-Reward reinforces the probability of using a query to express an intent based on the most recent reward of the query to convey the intent. Bush and Mosteller's and Cross's models increases (decreases) the probability of using a query based its past success (failures) of expressing an intent. A query is successful if it delivers a reward more than a given threshold, e.g., zero. Roth and Erev's model uses the aggregated reward from past interactions to compute the probability by which a query is used. Roth and Erev's modified model is similar to Roth and Erev's model, with an additional parameter that determines to what extent the user forgets the reward received for a query in past interactions. The details of algorithms are in Appendix A.", "publication_ref": ["b8", "b44"], "figure_ref": [], "table_ref": []}, {"heading": "Empirical Analysis", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Interaction Logs.", "text": "We use an anonymized Yahoo! interaction log for our empirical study, which consists of queries submitted to a Yahoo! search engine in July 2010 [50]. Each record in the log consists of a time stamp, user cookie id, submitted query, the top 10 results displayed to the user, and the positions of the user clicks on the returned answers. Generally speaking, typical users of Yahoo! are normal users who may not know advanced concepts, such as formal query language and schema, and use keyword queries to find their desired information. Yahoo! may generally use a combination of structured and unstructured datasets to satisfy users' intents. Nevertheless, as normal users are not aware of the existence of schema and mainly rely on the content of the returned answers to (re)formulate their queries, we expect that the users' learning mechanisms over this dataset closely resemble their learning mechanisms over structured data. We have used three different contiguous subsamples of this log whose information is shown in Table 5. The duration of each subsample is the time between the time-stamp of the first and last interaction records. Because we would like to specifically look at the users that exhibit some learning throughout their interaction, we have collected only the interactions in which a user submits at least two different queries to express the same intent. The records of the 8H-interaction sample appear at the beginning of the the 43H-interaction sample, which themselves appear at the beginning of the 101H-interaction sample.", "publication_ref": ["b50"], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Intent & Reward.", "text": "Accompanying the interaction log is a set of relevance judgment scores for each query and result pair. Each relevance judgment score is a value between 0 and 4 and shows the degree of relevance of the result to the query, with 0 meaning not relevant at all and 4 meaning the most relevant result. We define the intent behind each query as the set of results with non-zero relevance scores. We use the standard ranking quality metric NDCG for the returned results of a query as the reward in each interaction as it models different levels of relevance [37]. The value of NDCG is between 0 and 1 and it is 1 for the most effective list. Estimation. Some models, e.g., Cross's model, have some parameters that need to be trained. We have used a set of 5,000 records that appear in the interaction log immediately before the first subsample of Table 5 and found the optimal values for those parameters using grid search and the sum of squared errors.", "publication_ref": ["b37"], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "3.2.4", "text": "Training & Testing. We train and test a single user strategy over each subsample and model, which represents the strategy of the user population in each subsample. The user strategy in each model is initialized with a uniform distribution, so that all queries are equally likely to be used for an intent. After estimating parameters, we train the user strategy using each model over 90% of the total number of records in each selected subsample in the order by which the records appear in the interaction log. We use the value of NDCG as reward for the models that use rewards to update the user strategy after each interaction. We then test the accuracy of the prediction of using a query to express an intent for each model over the remaining 10% of each subsample using the user strategy computed at the end of the training phase. Each intent is conveyed using only a single query in the testing portions of our subsamples. Hence, no learning is done in the testing phase and we do not update the user strategies. We report the mean squared errors over all intents in the testing phase for each subsample and model in Figure 1. A lower mean squared error implies that the model more accurately represents the users' learning method. We have excluded the Latest Reward results from the figure as they are an order of magnitude worse than the others.  5 3.2.5 Results. Win-Keep/Lose-Randomize performs surprisingly more accurate than other methods for the 8H-interaction subsample. It indicates that in short-term and/or beginning of their interactions, users may not have enough interactions to leverage a more complex learning scheme and use a rather simple mechanism to update their strategies. Both Roth and Erev's methods use the accumulated reward values to adjust the user strategy gradually. Hence, they cannot precisely model user learning over a rather short interaction and are less accurate than relatively more aggressive learning models such as Bush and Mosteller's and Cross's over this subsample. Both Roth and Erev's deliver the same result and outperform other methods in the 43-H and 101-H subsamples. Win-Keep/Lose-Randomize is the least accurate method over these subsamples. Since larger subsamples provide more training data, the predication accuracy of all models improves as the interaction subsamples becomes larger. The learned value for the forget parameter in the Roth and Erev's modified model is very small and close to zero in our experiments, therefore, it generally acts like the Roth and Erev's model.\nLong-term communications between users and DBMS may include multiple sessions. Since Yahoo! query workload contains the time stamps and user ids of each interaction, we have been able to extract the starting and ending times of each session. Our results indicate that as long as the user and DBMS communicate over sufficiently many of interactions, e.g., about 10k for Yahoo! query workload, the users follow the Roth and Erev's model of learning. Given that the communication of the user and DBMS involve sufficiently many interactions, we have not observed any difference in the mechanism by which users learn based on the numbers of sessions in the user and DBMS communication.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": ["tab_4"]}, {"heading": "Conclusion.", "text": "Our analysis indicates that users show a substantially intelligent behavior when adopting and modifying their strategies over relatively medium and long-term interactions. They leverage their past interactions and their outcomes, i.e., have an effective long-term memory. This behavior is most accurately modeled using Roth and Erev's model. Hence, in the rest of the paper, we set the user learning method to this model.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "LEARNING ALGORITHM FOR DBMS", "text": "Current systems generally assume that a user does not learn and/or modify her method of expressing intents throughout her interaction with the DBMS. However, it is known that the learning methods that are useful in static settings do not deliver desired outcomes in the dynamic ones [4]. Moreover, it has been shown that if the players do not use the right learning algorithms in games with identical interests, the game and its payoff may not converge to any desired states [45]. Thus, choosing the correct learning mechanism for the DBMS is crucial to improve the payoff and converge to a desired state. The following algorithmic questions are of interest:\ni. How can a DBMS learn or adapt to a user's strategy? ii. Mathematically, is a given learning algorithm effective? iii. What would be the asymptotic behavior of a given learning algorithm? Here, we address the first and the second questions above. Dealing with the third question is far beyond the scope and space of this paper. A summary of the notations introduced in Section 2 and used in this section can be found in Table 4.", "publication_ref": ["b3", "b45"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "DBMS Reinforcement Learning", "text": "We adopt Roth and Erev's learning method for adaptation of the DBMS strategy, with a slight modification. The original Roth and Erev method considers only a single action space. In our work, this would translate to having only a single query. Instead we extend this such that each query has its own action space or set of possible intents. The adaptation happens over discrete time t = 0, 1, 2, 3, . . . instances where t denotes the tth interaction of the user and the DBMS. We refer to t simply as the iteration of the learning rule. For simplicity of notation, we refer to intent e i and result s \u2113 as intent i and \u2113, respectively, in the rest of the paper. Hence, we may rewrite the expected payoff for both user and DBMS as:\nu r (U , D) = m i=1 \u03c0 i n j=1 U i j o \u2113=1 D j \u2113 r i \u2113 ,\nwhere r : [m] \u00d7 [o] \u2192 R + is the effectiveness measure between the intent i and the result, i.e., decoded intent \u2113. With this, the reinforcement learning mechanism for the DBMS adaptation is as follows. a. Let R(0) > 0 be an n \u00d7 o initial reward matrix whose entries are strictly positive.\nb. Let D(0) be the initial DBMS strategy with D j \u2113 (0) =\nR j \u2113 (0) o \u2113=1 R j \u2113 (0) > 0 for all j \u2208 [n] and \u2113 \u2208 [o]. c. For iterations t = 1, 2, . . ., do i.\nIf the user's query at time t is q(t), DBMS returns a result E(t) \u2208 E with probability:\nP(E(t) = i \u2032 | q(t)) = D q(t )i \u2032 (t).\nii. User gives a reward r ii \u2032 given that i is the intent of the user at time t. Note that the reward depends both on the intent i at time t and the result i \u2032 . Then, set\nR j \u2113 (t + 1) = R j \u2113 (t) + r i \u2113 if j = q(t) and \u2113 = i \u2032 R j \u2113 (t) otherwise .(2)\niii. Update the DBMS strategy by\nD ji (t + 1) = R ji (t + 1) o \u2113=1 R j \u2113 (t + 1) ,(3)\nfor all j \u2208 [n] and i \u2208 [o].\nIn the above algorithm R(t) is simply the reward matrix at time t. One may use an available offline scoring function, e.g. [11,26], to compute the initial reward R(0) which possibly leads to an intuitive and relatively effective initial point for the learning process [49].", "publication_ref": ["b10", "b26", "b49"], "figure_ref": [], "table_ref": []}, {"heading": "Analysis of the Learning Rule", "text": "We show in Section 3 that users modify their strategies in data interactions. Nevertheless, ideally, one would like to use a learning mechanism for the DBMS that accurately discovers the intents behind users' queries whether or not the users modify their strategies, as it is not certain that all users will always modify their strategies. Also, in some relevant applications, the user's learning is happening in a much slower time-scale compared to the learning of the DBMS. So, one can assume that the user's strategy is fixed compared to the time-scale of the DBMS adaptation. Therefore, first, we consider the case that the user is not adapting her strategy, i.e., she has a fixed strategy during the interaction. Then, we consider the case that the user's strategy is adapting to the DBMS's strategy but perhaps on a slower time-scale in Section 4.3.\nWe provide an analysis of the reinforcement mechanism provided above and will show that, statistically speaking, the adaptation rule leads to improvement of the interaction effectiveness. To simplify our analysis, we assume that the user gives feedback only on one result in the returned list of answers. Hence, we assume that the cardinality of the returned list of answers is 1. For the analysis of the learning mechanism in Section 4 and for simplification, denote\nu(t) := u r (U , D(t)),(4)\nfor an effectiveness measure r as u r is defined in (1). We recall that a random process {X (t)} is a submartingale [19] if it is absolutely integrable (i.e. E(|X (t)|) < \u221e for all t) and\nE(X (t + 1) | F t ) \u2265 X (t),\nwhere F t is the history or \u03c3 -algebra generated by X 1 , . . . , X t 1 . In other words, a process {X (t)} is a sub-martingale if the expected value of X (t + 1) given the history X (t), X (t \u2212 1), . . . , X (0), is not strictly less than the value of X (t). Note that submartingales are nothing but the stochastic counterparts of monotonically increasing sequences. As in the case of bounded (from above) monotonically increasing sequences, submartingales pose the same property, i.e. any submartingale {X (t)} with E(|X (t)|) < B for some B \u2208 R + and all t \u2265 0 is convergent almost surely, i.e. lim t \u2192\u221e X (t) exists almost surely.\nThe main result in this section is that the sequence of the utilities {u(t)} (which is indeed a stochastic process as {D(t)} is a stochastic process) defined by ( 4) is a submartignale when the reinforcement learning rule in Section 4 is utilized. As a result the proposed reinforcement learning rule stochastically improves the efficiency of communication between the DBMS and the user. More importantly, this holds for an arbitrary reward/effectiveness measure r . This is rather a very strong result as the algorithm is robust to the choice of the reward mechanism.\nTo show this, we discuss an intermediate result. For simplicity of notation, we fix the time t and we use superscript + to denote 1 In this case, simply we have E(\nX (t + 1) | F t ) = E(X (t + 1) | X (t ), . . . , X (1)).\nvariables at time (t + 1) and drop the dependencies at time t for variables depending on time t. \nE(D + j \u2113 | F t ) \u2212 D j \u2113 = D j \u2113 \u2022 m i=1 \u03c0 i U i j r i l R j + r il \u2212 o \u2113 \u2032 =1 D j \u2113 \u2032 r i \u2113 \u2032 R j + r i \u2113 \u2032 , whereR j = o \u2113 \u2032 =1 R j \u2113 \u2032 .\nTo show the main result, we use the following result in martingale theory.", "publication_ref": ["b0", "b18", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Theorem 4.2. [43]", "text": "A random process {X (t)} converges almost surely if X (t) is bounded, i.e., E(|X (t)|) < B for some B \u2208 R + and all t \u2265 0 and\nE(X (t + 1)|F t ) \u2265 X (t) \u2212 \u03b2(t)\n(5) where \u03b2(t) \u2265 0 is a summable sequence almost surely, i.e., t \u03b2(t) < \u221e with probability 1.\nUsing Lemma 4.1 and the above result, we show that up to a summable disturbance, the proposed learning mechanism is stochastically improving.\nTheorem 4.3. Let {u(t)} be the sequence given by (4). Then,\nE(u(t + 1 | F t ) \u2265 E(u(t) | F t ) \u2212 \u03b2(t),\nfor some non-negative random process {\u03b2(t)} that is summable (i.e. The above result implies that the effectiveness of the DBMS, stochastically speaking, increases as time progresses when the learning rule in Section 4 is utilized. Not only that, but this property is true for cases where the feedback is not simply a 0/1 value, e.g., the selected answer may be partially relevant to the desired intent. This is indeed a desirable property for any DBMS learning scheme.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "User and DBMS Adaptations", "text": "We also consider the case that the user also adapts to the DBMS's strategy. At the first glance, it may seem that if the DBMS adapts using a reasonable learning mechanism, the user's adaptation can only result in a more effective interaction as both players have identical interests. Nevertheless, it is known from the research in algorithmic game theory that in certain two-player games with identical interest in which both players adapt their strategies to improve their payoff, well-known learning methods do not converge to any (desired) stable state and cycle among several unstable states [17,45]. Here, we focus on the identity similarity measure, i.e. we assume that m = o and the user gives a boolean feedback:\nr i \u2113 = 1 if i = \u2113, 0 otherwise .\nIn this case, we assume that the user adapts to the DBMS strategy at time steps 0 < t 1 < \u2022 \u2022 \u2022 < t k < \u2022 \u2022 \u2022 and in those time-steps the DBMS is not adapting as there is no reason to assume the synchronicity between the user and the DBMS. The reinforcement learning mechanism for the user is as follows: a. Let S(0) > 0 be an m \u00d7n reward matrix whose entries are strictly positive.\nb. Let U (0) be the initial user's strategy with\nU i j (0) = S i j (0) n j \u2032 =1 S i j \u2032 (0) for all i \u2208 [m] and j \u2208 [n] and let U (t k ) = U (t k \u2212 1) = \u2022 \u2022 \u2022 = U (t k \u22121 + 1) for all k.\nc. For all k \u2265 1, do the following: i. The user picks a random intent t \u2208 [m] with probability \u03c0 i (independent of the earlier choices of intent) and subsequently selects a query j \u2208 [n] with probability\nP(q(t k ) = j | i(t k ) = i) = U i j (t k ).\nii. The DBMS uses the current strategy D(t k ) and interpret the query by the intent i \u2032 (t) = i \u2032 with probability\nP(i \u2032 (t k ) = i \u2032 | q(t k ) = j) = D ji \u2032 (t k ).\niii. User gives a reward 1 if i = i \u2032 and otherwise, gives no rewards, i.e.\nS + i j = S i j (t k ) + 1 if j = q(t k ) and i(t k ) = i \u2032 (t k ) S i j (t k ) otherwise\nwhere S + i j = S i j (t k + 1). iv. Update the user's strategy by\nU i j (t k + 1) = S i j (t k + 1) n j \u2032 =1 S i j \u2032 (t k + 1) ,(6)\nfor all i \u2208 [m] and j \u2208 [n].\nIn the above scheme S(t) is the reward matrix at time t for the user.\nNext, we provide an analysis of the reinforcement mechanism provided above and will show that, statistically speaking, our proposed adaptation rule for DBMS, even when the user adapts, leads to improvement of the effectiveness of the interaction. With a slight abuse of notation, let\nu(t) := u r (U , D(t)) = u r (U (t), D(t)),(7)\nfor an effectiveness measure r as u r is defined in (1).\nLemma 4.4. Let t = t k for some k \u2208 N. Then, for any i \u2208 [m] and j \u2208 [n], we have\nE(U + i j | F t ) \u2212 U i j = \u03c0 i U i j n \u2113=1 S i \u2113 + 1 (D ji \u2212 u i (t))(8)\nwhere\nu i (t) = n j=1 U i j (t)D ji (t).\nUsing Lemma 4.4, we show that the process {u(t)} is a sub-martingale.\nTheorem 4.5. Let t = t k for some k \u2208 N. Then, we have\nE(u(t + 1) | F t ) \u2212 u(t) \u2265 0 (9\n)\nwhere u(t) is given by (7).\nCorollary 4.6. The sequence {u(t)} given by (4) converges almost surely.\nThe authors in [27] have also analyzed the effectiveness of a 2-player signaling game in which both players use Roth and Erev's model for learning. However, they assume that both players learn at the same time-scale. Our result in this section holds for the case where users and DBMS learn at different time-scales, which may arguably be the dominant case in our setting as generally users may learn in a much slower time-scale compared to the DBMS.", "publication_ref": ["b16", "b45", "b6", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "EFFICIENT QUERY ANSWERING OVER RELATIONAL DATABASES", "text": "An efficient implementation of the algorithm proposed in Section 4 over large relational databases poses two challenges. First, since the set of possible interpretations and their results for a given query is enormous, one has to find efficient ways of maintaining users' reinforcements and updating DBMS strategy. Second, keyword and other usable query interfaces over databases normally return the top-k tuples according to some scoring functions [14,26]. Due to a series of seminal works by database researchers [22], there are efficient algorithms to find such a list of answers. Nevertheless, our reinforcement learning algorithm uses a randomized semantic for answering algorithms in which candidate tuples are associated a probability for each query that reflects the likelihood by which it satisfies the intent behind the query. The tuples must be returned randomly according to their associated probabilities. Using (weighted) sampling to answer SQL queries with aggregation functions approximately and efficiently is an active research area [12,29]. However, there has not been any attempt on using a randomized strategy to answer so-called point queries over relational data and achieve a balanced exploitation-exploration trade-off efficiently.", "publication_ref": ["b13", "b26", "b21", "b11", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Maintaining DBMS Strategy", "text": "5.1.1 Keyword Query Interface. We use the current architecture of keyword query interfaces over relational databases that directly use schema information to interpret the input keyword query [14]. A notable example of such systems is IR-Style [26]. As it is mentioned in Section 2.4, given a keyword query, these systems translate the input query to a Select-Project-Join query whose where clause contains function match. The results of these interpretations are computed, scored according to some ranking function, and are returned to the user. We provide an overview of the basic concepts of such a system. We refer the reader to [14,26] for more explanation.\nTuple-set: Given keyword query q, a tuple-set is a set of tuples in a base relation that contain some terms in q. After receiving q, the query interface uses an inverted index to compute a set of tuplesets. For instance, consider a database of products with relations Product(pid, name), Customer(cid, name), and ProductCustomer(pid, cid) where pid and cid are numeric strings. Given query iMac John, the query interface returns a tuple-set from Product and a tupleset from Customer that match at least one term in the query. The query interface may also use a scoring function, e.g., traditional TF-IDF text matching score, to measure how exactly each tuple in a tuple-set matches some terms in q.\nCandidate Network: A candidate network is a join expression that connects the tuple-sets via primary key-foreign key relationships. A candidate network joins the tuples in different tuple-sets and produces joint tuples that contain the terms in the input keyword query. One may consider the candidate network as a join tree expression whose leafs are tuple-sets. For instance, one candidate network for the aforementioned database of products is Product \u25b7\u25c1 ProductCustomer \u25b7\u25c1 Customer. To connect tuple-sets via primary key-foreign key links, a candidate network may include base relations whose tuples may not contain any term in the query, e.g., ProductCustomer in the preceding example. Given a set of tuple-sets, the query interface uses the schema of the database and progressively generates candidate networks that can join the tuple-sets. For efficiency considerations, keyword query interfaces limit the number of relations in a candidate network to be lower than a given threshold. For each candidate network, the query interface runs a SQL query and return its results to the users.There are algorithms to reduce the running time of this stage, e.g., run only the SQL queries guaranteed to produce top-k tuples [26]. Keyword query interfaces normally compute the score of joint tuples by summing up the scores of their constructing tuples multiplied by the inverse of the number of relations in the candidate network to penalize long joins. We use the same scoring scheme. We also consider each (joint) tuple to be candidate answer to the query if it contains at least one term in the query.", "publication_ref": ["b13", "b26", "b13", "b26", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "Managing", "text": "Reinforcements. The aforementioned keyword query interface implements a basic DBMS strategy of mapping queries to results but it does not leverage users' feedback and adopts a deterministic strategy without any exploration. A naive way to record users' reinforcement is to maintain a mapping from queries to tuples and directly record the reinforcements applied to each pair of query and tuple. In this method, the DBMS has to maintain the list of all submitted queries and returned tuples. Because many returned tuples are the joint tuples produced by candidate networks, it will take an enormous amount of space and is inefficient to update. Hence, instead of recording reinforcements directly for each pair of query and tuple, we construct some features for queries and tuples and maintain the reinforcement in the constructed feature space. More precisely, we construct and maintain a set of n-gram features for each attribute value in the base relations and each input query. N-grams are contiguous sequences of terms in a text and are widely used in text analytics and retrieval [37]. In our implementation, we use up to 3-gram features to model the challenges in managing a large set of features. Each feature in every attribute value in the database has its associated attribute and relation names to reflect the structure of the data. We maintain a reinforcement mapping from query features to tuple features. After a tuple gets reinforced by the user for an input query, our system increases the reinforcement value for the Cartesian product of the features in the query and the ones in the reinforced tuple. According to our experiments in Section 6, this reinforcement mapping can be efficiently maintained in the main memory by only a modest space overhead.\nGiven an input query q, our system computes the score of each tuple t in every tuple-set using the reinforcement mapping: it finds the n-gram features in t and q and sums up their reinforcement values recorded in the reinforcement mapping. Our system may use a weighted combination of this reinforcement score and traditional text matching score, e.g., TF-IDF score, to compute the final score. One may also weight each tuple feature proportional to its inverse frequency in the database similar to some traditional relevance feedback models [37]. Due to the space limit, we mainly focus on developing an efficient implementation of query answering based on reinforcement learning over relational databases and leave using more advanced scoring methods for future work. The scores of joint tuples are computed as it is explained in Section 5.1.1. We will explain in Section 5.2, how we convert these scores to probabilities and return tuples. Using features to compute and record user feedback has also the advantage of using the reinforcement of a pair of query and tuple to compute the relevance score of other tuples for other queries that share some features. Hence, reinforcement for one query can be used to return more relevant answers to other queries.", "publication_ref": ["b37", "b37"], "figure_ref": [], "table_ref": []}, {"heading": "Efficient Exploitation & Exploration", "text": "We propose the following two algorithms to generate a weighted random sample of size k over all candidate tuples for a query.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Reservoir.", "text": "To provide a random sample, one may calculate the total scores of all candidate answers to compute their sampling probabilities. Because this value is not known beforehand, one may use weighted reservoir sampling [13] to deliver a random sample without knowing the total score of candidate answers in a single scan of the data as follows. \nfor all i = 1 \u2208 k do insert t into A[i] with probability Sc(t ) W\nReservoir generates the list of answers only after computing the results of all candidate networks, therefore, users have to wait for a long time to see any result. It also computes the results of all candidate networks by performing their joins fully, which may be inefficient. We propose the following optimizations to improve its efficiency and reduce the users' waiting time.", "publication_ref": ["b12"], "figure_ref": [], "table_ref": []}, {"heading": "Poisson-Olken.", "text": "Poisson-Olken algorithm uses Poisson sampling to output progressively the selected tuples as it processes each candidate network. It selects the tuple t with probability\nSc(t ) M ,\nwhere M is an upper bound to the total scores of all candidate answers. To compute M, we use the following heuristic. Given candidate network CN , we get the upper bound for the total score of all tuples generated from CN : ) is an upper bound to the scores of tuples generated by CN . Since each tuple generated by CN must contain one tuple from each tuple-set in CN , the maximum number of tuples in CN is \u03a0 T S \u2208C N |T S |. It is very unlikely that all tuples of every tuple-set join with all tuples in every other tuple-set in a candidate network. Hence, we divide this value by 2 to get a more realistic estimation. We do not consider candidate networks with cyclic joins, thus, each tuple-set appears at most once in a candidate network. The value of M is the sum of the aforementioned values for all candidate networks with size greater than one and the total scores of tuples in each tuple-set. Since the scores of tuples in each tuple-set is kept in the main memory, the maximum and total scores and the size of each tuple-set is computed efficiently before computing the results of any candidate network.\nM C N = 1 n ( T S \u2208C N Sc max (T S))\nBoth Reservoir and the aforementioned Poisson sampling compute the full joins of each candidate network and then sample the output. This may take a long time particularly for candidate networks with some base relations. There are several join sampling methods that compute a sample of a join by joining only samples the input tables and avoid computing the full join [13,31,41]. To sample the results of join R 1 \u25b7\u25c1 R 2 , most of these methods must know some statistics, such as the number of tuples in R 2 that join with each tuple in R 1 , before performing the join. They precompute these statistics in a preprocessing step for each base relation. But, since R 1 and/or R 2 in our candidate networks may be tuples sets, one cannot know the aforementioned statistics unless one performs the full join.\nHowever, the join sampling algorithm proposed by Olken [41] finds a random sample of the join without the need to precompute these statistics. Given join R 1 \u25b7\u25c1 R 2 , let t \u22ca R 2 denote the set of tuples in R 2 that join with t \u2208 R 1 , i.e., the right semi-join of t and R 2 . Also, let |t \u22ca R 2 | t \u2208R 1 max be the maximum number of tuples in R 2 that join with a single tuple t \u2208 R 1 . The Olken algorithm first randomly picks a tuple t 1 from R 1 . It then randomly selects the tuple t 2 from t 1 \u22caR 2 . It accepts the joint tuple t 1 \u25b7\u25c1 t 2 with probability\n|t 1 \u22caR 2 | |t \u22caR 2 | t \u2208R 1 max\nand rejects it with the remaining probability. To avoid scanning R 2 multiple times, Olken algorithm needs an index over R 2 . Since the joins in our candidate networks are over only primary and foreign keys, we do not need too many indexes to implement this approach.\nWe extend the Olken algorithm to sample the results of a candidate network without doing its joins fully as follows. Given candidate network R 1 \u25b7\u25c1 R 2 , our algorithm randomly samples tuple t 1 \u2208 R 1 with probability\nSc(t 1 ) t \u2208R 1 (Sc(t ))\n, where Sc(t) is the score of tuple t, if R 1 is a tuple-set. Otherwise, if R 1 is a base relation, it picks the tuple with probability 1 |R 1 | . The value of t \u2208R (Sc(t)) for each tuple set R is computed at the beginning of the query processing and the value of |R| for each base relation is calculated in a preprocessing step. The algorithm then samples tuple t 2 from t 1 \u22ca R 2 with probability\nSc(t 2 ) t \u2208t 1 \u22caR 2 (Sc(t )) if R 2 is a tuple-set and 1 |t 1 \u22caR 2 | if R 2", "publication_ref": ["b12", "b31", "b41", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "is a base relation. It accepts the joint tuple with probability", "text": "t \u2208t 1 \u22caR 2 Sc(t ) max ( t \u2208s \u22caR 2 , s \u2208R 1 Sc(t ))\nand rejects it with the remaining probability.\nTo compute the exact value of max ( t \u2208s\u22caR 2 ,s \u2208R 1 Sc(t)), one has to perform the full join of R 1 and R 2 . Hence, we use an upper bound on max ( t \u2208s\u22caR 2 ,s \u2208R 1 Sc(t)) in Olken algorithm. Using an upper bound for this value, Olken algorithm produces a correct random sample but it may reject a larger number of tuples and generate a smaller number of samples. To compute an upper bound on the value of max ( t \u2208s\u22caR 2 ,s \u2208R 1 Sc(t)), we precompute the value of |t \u22ca B i | t \u2208B j max before the query time for all base relations B i and B j with primary and foreign keys of the same domain of values.\nAssume that B 1 and B 2 are the base relations of tuple-sets R 1 and R 2 , respectively. We\nhave |t \u22ca R 2 | t \u2208R 1 max \u2264 |t \u22ca B 2 | t \u2208B 1 max . Because max ( t \u2208s\u22caR 2 ,s \u2208R 1 Sc(t)) \u2264 max t \u2208R 2 (Sc(t))|t \u22ca R 2 | t \u2208R 1 max , we have max ( t \u2208s\u22caR 2 ,s \u2208R 1 Sc(t)) \u2264 max t \u2208R 2 (Sc(t))|t \u22ca B 2 | t \u2208B 1\nmax . Hence, we use\nt \u2208t 1 \u22caR 2 Sc(t ) max t \u2208R 2 (Sc(t ))|t \u22caB 2 | t \u2208B 1 max\nfor the probability of acceptance.\nWe iteratively apply the aforementioned algorithm to candidate networks with multiple joins by treating the join of each two relations as the first relation for the subsequent join in the network.\nThe following algorithm adopts a Poisson sampling method to return a sample of size k over all candidate networks using the aforementioned join sampling algorithm. We show binomial distribution with parameters n and p as B(n, p). We denote the aforementioned join algorithm as Extended-Olken. Also, ApproxTotalScore denotes the approximated value of total score computed as explained at the beginning of this section.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 2 Poisson-Olken", "text": "x \u2190 k W \u2190\nAppr oxT ot al Scor e k while x > 0 do for all candidate network CN do if CN is a single tuple-set then for all t \u2208 CN do output t with probability\nSc(t ) W\nif a tuple t is picked then\nx \u2190 x \u2212 1 else let CN = R 1 \u25b7\u25c1 . . . \u25b7\u25c1 R n for all t \u2208 R 1 do Pick value X from distribution B(k, Sc(t ) W ) Pipeline X copies of t to the Olken algorithm if Olken accepts m tuples then x \u2190 x \u2212 m\nThe expected value of produced tuples in the Poisson-Olken algorithm is close to k. However, as opposed to reservoir sampling, there is a non-zero probability that Poisson-Olken may deliver fewer than k tuples. To drastically reduce this chance, one may use a larger value for k in the algorithm and reject the appropriate number of the resulting tuples after the algorithm terminates [13]. The resulting algorithm will not progressively produce the sampled tuples, but, as our empirical study in Section 6 indicates, it is faster than Reservoir over large databases with relatively many candidate networks as it does not perform any full join.\n6 EMPIRICAL STUDY 6.1 Effectiveness 6.1.1 Experimental Setup. It is difficult to evaluate the effectiveness of online and reinforcement learning algorithms for information systems in a live setting with real users because it requires a very long time and a large amount of resources [24,25,42,47,49]. Thus, most studies in this area use purely simulated user interactions [25,42,47]. A notable expectation is [49], which uses a real-world interaction log to simulate a live interaction setting. We follow a similar approach and use Yahoo! interaction log [50] to simulate interactions using real-world queries and dataset.\nUser Strategy Initialization: We train a user strategy over the Yahoo! 43H-interaction log whose details are in Section 3 using Roth and Erev's method, which is deemed the most accurate to model user learning according to the results of Section 3. This strategy has 341 queries and 151 intents. The Yahoo! interaction log contains user clicks on the returned intents, i.e. URLs. However, a user may click a URL by mistake [49]. We consider only the clicks that are not noisy according to the relevance judgment information that accompanies the interaction log. According to the empirical study reported in Section 3.2, the parameters of number and length of sessions and the amount of time between consecutive sessions do not impact the user learning mechanism in long-term communications. Thus, we have not organized the generated interactions into sessions.\nMetric: Since almost all returned results have only one relevant answer and the relevant answers to all queries have the same level of relevance, we measure the effectiveness of the algorithms using the standard metric of Reciprocal Rank (RR) [37]. RR is 1 r where r is the position of the first relevant answer to the query in the list of the returned answers. RR is particularly useful where each query in the workload has a very few relevant answers in the returned results, which is the case for the queries used in our experiment.\nAlgorithms: We compare the algorithm introduced in Section 4.1 against the state-of-the-art and popular algorithm for online learning in information retrieval called UCB-1 [3,39,42,49]. It has been shown to outperform its competitors in several studies [39,42]. It calculates a score for an intent e given the tth submission of query q as: Score t (q, e) = W q, e, t X q, e, t + \u03b1 2l n t X q, e, t , in which X is how many times an intent was shown to the user, W is how many times the user selects a returned intent, and \u03b1 is the exploration rate set between [0, 1]. The first term in the formula prefers the intents that have received relatively more positive feedback, i.e., exploitation, and the second term gives higher scores to the intents that have been shown to the user less often and/or have not been tried for a relatively long time, i.e., exploration. UCB-1 assumes that users follow a fixed probabilistic strategy. Thus, its goal is to find the fixed but unknown expectation of the relevance of an intent to the input query, which is roughly the first term in the formula; by minimizing the number of unsuccessful trials.\nParameter Estimation: We randomly select 50% of the intents in the trained user strategy to learn the exploration parameter \u03b1 in UCB-1 using grid search and sum of squared errors over 10,000 interactions that are after the interactions in the 43H-interaction log. We do not use these intents to compare algorithms in our simulation. We calculate the prior probabilities, \u03c0 in Equation 1, for the intents in the trained user strategy that are not used to find the parameter of UCB-1 using the entire Yahoo! interaction log. DBMS Strategy Initialization: The DBMS starts the interaction with an strategy that does not have any query. Thus, the DBMS is not aware of the set of submitted queries apriori. When the DBMS sees a query for the first time, it stores the query in its strategy, assigns equal probabilities for all intents to be returned for this query, returns some intent(s) to answer the query, and stores the user feedback on the returned intent(s) in the DBMS strategy. If the DBMS has already encountered the query, it leverages the previous user's feedback on the results of this query and returns the set of intents for this query using our proposed learning algorithm. Retrieval systems that leverage online learning perform some filtering over the initial set of answers to make efficient and effective exploration possible [25,49]. More precisely, to reduce the set of alternatives over a large dataset, online and reinforcement learning algorithms apply a traditional selection algorithm to reduce the number of possible intents to a manageable size. Otherwise, the learning algorithm has to explore and solicit user feedback on numerous items, which takes a very long time. For instance, online learning algorithms used in searching a set of documents, e.g., UCB-1, use traditional information retrieval algorithms to filter out obviously non-relevant answers to the input query, e.g., the documents with low TF-IDF scores. Then, they apply the exploitation-exploration paradigm and solicit user feedback on the remaining candidate answers. The Yahoo! interaction workload has all queries and intents anonymized, thus we are unable to perform a filtering method of our own choosing. Hence, we use the entire collection of possible intents in the portion of the Yahoo! query log used for our simulation. This way, there 4521 intent per query that can be returned, which is close to the number of answers a reinforcement learning algorithm may consider over a large data set after filtering [49]. The DBMS strategy for our method is initialized to be completely random.", "publication_ref": ["b12", "b23", "b25", "b42", "b47", "b49", "b25", "b42", "b47", "b49", "b50", "b49", "b37", "b0", "b2", "b39", "b42", "b49", "b39", "b42", "b25", "b49", "b49"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": ". We simulate the interaction of a user population that starts with our trained user strategy with UCB-1 and our algorithm. In each interaction, an intent is randomly picked from the set of intents in the user strategy by its prior probability and submitted to UCB-1 and our method. Afterwards, each algorithm returns a list of 10 answers and the user clicks on the top-ranked answer that is relevant to the query according to the relevance judgment information. The details of simulation is reported in our technical report [38]. We run our simulations for one million interactions.\nFigure 2 shows the accumulated Mean Reciprocal Rank (MRR) over all queries in the simulated interactions. Our method delivers a higher MRR than UCB-1 and its MRR keeps improving over the duration of the interaction. UCB-1, however, increases the MRR at a much slower rate. Since UCB-1 is developed for the case where users do not change their strategies, it learns and commits to a fixed probabilistic mapping of queries to intents quite early in the interaction. Hence, it cannot learn as effectively as our algorithm where users modify their strategies using a randomized method, such as Roth and Erev's. As our method is more exploratory than UCB-1, it enables users to provide feedback on more varieties of intents than they do for UCB-1. This enables our method to learn more accurately how users express their intents in the long-run.\nWe have also observed that our method allows users to try more varieties of queries to express an intent and learn the one(s) that convey the intent effectively. As UCB-1 commits to a certain mapping of a query to an intent early in the interaction, it may not return sufficiently many relevant answers if the user tries this query to express another intent. This new mapping, however, could be promising in the long-run. Hence, the user and UCB-1 strategies may stabilize in less than desirable states. Since our method does not commit to a fixed strategy that early, users may try this query for another intent and reinforce the mapping if they get relevant answers. Thus, users have more chances to try and pick a query for an intent that will be learned and mapped effectively to the intent by the DBMS. We have discussed and proposed solutions for mitigating the startup period of our algorithm in Appendix E.  [20]. After submitting each query and getting some results, we simulate user feedback using the relevance information in the Bing log.\nQuery Processing: We have used Whoosh inverted index (whoosh.readthedocs.io) to index each table in databases. Whoosh recognizes the concept of table with multiple attributes, but cannot perform joins between different tables. Because the Poisson-Olken algorithm needs indexes over primary and foreign keys used to build candidate network, we have build hash indexes over these tables in Whoosh. Given an index-key, these indexes return the tuple(s) that match these keys inside Whoosh. To provide a fair comparison between Reservoir and Poisson-Olken, we have used these indexes to perform join for both methods. We also precompute and maintain all 3-grams of the tuples in each database as mentioned in Section 5.1. We have implemented our system using both Reservoir and Poisson algorithms. We have limited the size of each candidate network to 5. Our system returns 10 tuples in each interaction for both methods.\nHardware Platform: We run experiments on a server with 32 2.6GHz Intel Xeon E5-2640 processors with 50GB of main memory.", "publication_ref": ["b38", "b19"], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Results. Table 6 depicts the time for processing candidate networks and reporting the results for both Reservoir and", "text": "Poisson-Olken over TV-Program and Play databases over 1000 interactions. These results also show that Poisson-Olken is able to significantly improve the time for executing the joins in the candidate network, shown as performing joins in the table, over Reservoir in both databases. The improvement is more significant for the larger database, TV-Program. Poisson-Olken progressively produces tuples to show to user. But, we are not able to use this feature for all interactions. For a considerable number of interactions, Poisson-Olken does not produce 10 tuples, as explained in Section 5.2. Hence, we have to use a larger value of k and wait for the algorithm to finish in order to find a randomize sample of the answers as explained at the end of Section 5.2. Both methods have spent a negligible amount of time to reinforce the features, which indicate that using a rich set of features one can perform and manage reinforcement efficiently.  [2,7,18,33,48]. In these systems, a user explicitly teaches the system by labeling a set of examples potentially in several steps without getting any answer to her information need. Thus, the system is broken into two steps: first it learns the information need of the user by soliciting labels on certain examples from the user and then once the learning has completed, it suggests a query that may express the user's information need. These systems usually leverage active learning methods to learn the user intent by showing the fewest possible examples to the user [18]. However, ideally one would like to have a query interface in which the DBMS learns about the user's intents while answering her (vague) queries as our system does. As opposed to active learning methods, one should combine and balance exploration and learning with the normal query answering to build such a system. Moreover, current query learning systems assume that users follow a fixed strategy for expressing their intents. Also, we focus on the problems that arise in the long-term interaction that contain more than a single query and intent. A review of other related works is in the appendix C.", "publication_ref": ["b1", "b6", "b17", "b33", "b48", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSION", "text": "Many users do not know how to express their information needs.\nA DBMS may interact with these users and learn their information needs. We showed that users learn and modify how they express their information needs during their interaction with the DBMS and modeled the interaction between the user and the DBMS as a game, where the players would like to establish a common mapping from information needs to queries via learning. As current query interfaces do not effectively learn the information needs behind queries in such a setting, we proposed a reinforcement learning algorithm for the DBMS that learns the querying strategy of the user effectively. We provided efficient implementations of this learning mechanisms over large databases.\nBush and Mosteller's Model: Bush and Mosteller's model increases the probability that a user will choose a given query to express an intent by an amount proportional to the reward of using that query and the current probability of using this query for the intent [8]. If a user receives reward r for using q(t) at time t to express intent e i , the model updates the probabilities of using queries in the user strategy as follows.\nU i j (t + 1) = U i j (t) + \u03b1 BM \u2022 (1 \u2212 U i j (t)) q j = q(t) \u2227 r \u2265 0 U i j (t) \u2212 \u03b2 BM \u2022 U i j (t) q j = q(t) \u2227 r < 0 (10\n)\nU i j (t + 1) = U i j (t) \u2212 \u03b1 BM \u2022 U i j (t) q j q(t) \u2227 r \u2265 0 U i j (t) + \u03b2 BM \u2022 (1 \u2212 U i j (t) q j q(t) \u2227 r < 0 (11) \u03b1 BM \u2208 [0, 1] and \u03b2 BM \u2208 [0, 1] are parameters of the model.\nSince effectiveness metrics in interaction are always greater than zero, \u03b2 BM is never used in our experiments.\nCross's Model: Cross's model modifies the user's strategy similar to Bush and Mosteller's model [16], but uses the amount of the received reward to update the user strategy. Given a user receives reward r for using q(t) at time t to express intent e i , we have:\nU i j (t + 1) = U i j (t) + R(r ) \u2022 (1 \u2212 U i j (t)) q j = q(t) U i j (t) \u2212 R(r ) \u2022 U i j (t) q j q(t)(12)\nR(r ) = \u03b1 C \u2022 r + \u03b2 C(13)\nParameters \u03b1 C \u2208 [0, 1] and \u03b2 C \u2208 [0, 1] are used to compute the adjusted reward R(r ) based on the value of actual reward r .\nRoth and Erev's Model: Roth and Erev's model reinforces the probabilities directly from the reward value r that is received when the user uses query q(t) [44]. Its most important difference with other models is that it explicitly accumulates all the rewards gained by using a query to express an intent. S i j (t) in matrix S(t) maintains the accumulated reward of using query q j to express intent e i over the course of interaction up to round (time) t.\nS i j (t + 1) = S i j (t) + r q j = q(t) S i j (t) q j q(t)(14)\nU i j (t + 1) = S i j (t + 1) n j \u2032 S i j \u2032 (t + 1)(15)\nEach query not used in a successful interaction will be implicitly penalized as when the probability of a query increases, all others will decrease to keep U row-stochastic.\nRoth and Erev's Modified Model: Roth and Erev's modified model is similar to the original Roth and Erev's model, but it has an additional parameter that determines to what extent the user takes in to account the outcomes of her past interactions with the system [21]. It is reasonable to assume that the user may forget the results of her much earlier interactions with the system. This is accounted for by the forget parameter \u03c3 \u2208 [0, 1]. Matrix S(t) has the same role it has for the Roth and Erev's model.\nS i j (t + 1) = (1 \u2212 \u03c3 ) \u2022 S i j (t) + E(j, R(r ))(16)\nE(j, R(r )) = R(r ) \u2022 (1 \u2212 \u03f5) q j = q(t) R(r ) \u2022 (\u03f5) q j q(t)(17)\nR(r ) = r \u2212 r min (18)\nU i j (t + 1) = S i j (t + 1) n j \u2032 S i j \u2032 (t + 1)(19)\nIn the aforementioned formulas, \u03f5 \u2208 [0, 1] is a parameter that weights the reward that the user receives, n is the maximum number of possible queries for a given intent e i , and r min is the minimum expected reward that the user wants to receive. The intuition behind this parameter is that the user often assumes some minimum amount of reward is guaranteed when she queries the database. The model uses this minimum amount to discount the received reward. We set r min to 0 in our analysis, representing that there is no expected reward in an interaction.\nLatest-Reward: The Latest-Reward method reinforces the user strategy based on the previous reward that the user has seen when querying for an intent e i . All other queries have an equal probability to be chosen for a given intent. Let a user receive reward r \u2208 [0, 1] by entering query q j to express intent e i . The Latest-Reward method sets the probability of using q j to convey e i in the user strategy, U i j , to r and distribute the remaining probability mass 1 \u2212 r evenly between other entries related to intent e i , in U ik , where k j. Then on the complement A c of A, D + j \u2113 (\u03c9) = D j \u2113 (\u03c9). Let A i, \u2113 \u2032 \u2286 A be the subset of A such that the intent of the user is i and the pair (j, \u2113 \u2032 ) is reinforced. Note that the collection of sets {A i, \u2113 \u2032 } for i, \u2113 \u2032 \u2208 [m], are pairwise mutually exclusive and their union constitute the set A.", "publication_ref": ["b7", "b15", "b44", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "B MISSING PROOFS", "text": "We note that\nD + j \u2113 = m i=1 R j \u2113 + r il R j + r i \u2113 1 A i, \u2113 + o \u2113 \u2032 =1 \u2113 \u2032 \u2113 R j l R j + r i \u2113 \u2032 1 A i, \u2113 \u2032 + D j \u2113 1 A c .\nTherefore, we have\nE(D + j \u2113 | F t ) = m i=1 \u03c0 i U i j D j \u2113 R j \u2113 + r i l R j + r i \u2113 + m i=1 \u03c0 i U i j \u2113 \u2113 \u2032 D j \u2113 \u2032 R j l R j + r i \u2113 \u2032 + (1 \u2212 p)D j \u2113 ,\nwhere p = P(A | F ). Note that D j \u2113 = R j\u012b R j and hence,\nE(D + j \u2113 | F t ) \u2212 D j \u2113 = m i=1 \u03c0 i U i j D j \u2113 r i \u2113Rj \u2212 R j l R j (R j + r i \u2113 ) \u2212 m i=1 \u03c0 i U i j \u2113 \u2113 \u2032 D j \u2113 \u2032 R j \u2113 r i \u2113 \u2032 R j (R j + r i \u2113 \u2032 ) .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Replacing", "text": "R jl R j with D j \u2113 and rearranging the terms in the above expression, we get the result. \u25a1 Proof of Theorem 4.3: Let u + := u(t + 1), u := u(t),\nu j := u j (U (t), D(t)) = m i=1 o \u2113=1 \u03c0 i U i j D j \u2113 r i \u2113(t ) ,\nand also defineR j := m \u2113 \u2032 =1 R j \u2113 \u2032 . Note that u j is the efficiency of the jth signal/query.\nUsing the linearity of conditional expectation and Lemma 4.1, we have:\nE(u + | F t ) \u2212 u = m i=1 n j=1 \u03c0 i U i j o \u2113=1 r i \u2113 \u2032 E(D + j \u2113 | F t ) \u2212 D j \u2113 = m i=1 n j=1 o \u2113=1 \u03c0 i U i j D j \u2113 r i \u2113 m i \u2032 =1 \u03c0 \u2032 i U i \u2032 j r i \u2032 l R j + r i \u2032 \u2113 \u2212 o \u2113 \u2032 =1 D j \u2113 \u2032 r i \u2032 \u2113 \u2032 R j + r i \u2032 \u2113 \u2032 .(20)\nNow, let y j \u2113 = m i=1 \u03c0 i U i j r i \u2113 and z j \u2113 = m i=1 \u03c0 i U i j r i l R j +r i \u2113 . Then, we get from the above expression that\nE(u + | F t ) \u2212 u = n j=1 o \u2113=1 D j \u2113 y i \u2113 z j \u2113 \u2212 o \u2113=1 D j \u2113 y j \u2113 o \u2113 \u2032 =1 D j \u2113 \u2032z j \u2113 \u2032 . (21)\nNow, we express the above expression as\nE(u + | F t ) \u2212 u = V t +\u1e7c t (22\n)\nwhere\nV t = n j=1 1 R j o \u2113=1 D j \u2113 y 2 j \u2113 \u2212 o l =1 D j \u2113 y j \u2113 2 , and\u1e7c t = n j=1 o \u2113=1 D j \u2113 y j \u2113 o \u2113 \u2032 =1 D j \u2113 \u2032z j \u2113 \u2032 \u2212 m \u2113=1 D j \u2113 y j \u2113zj \u2113 .(23)\nFurther,z j \u2113 = i=1 \u03c0 i U i j r 2 i l R j (R j +r i \u2113 ) . We claim that V t \u2265 0 for each t and {\u1e7c t } is a summable sequence almost surely. Then, from (22) and Theorem 4.2, we get that {u t } converges almost surely and it completes the proof. Next, we validate our claims.\nWe first show that V t \u2265 0, \u2200t . Note that D is a row-stochastic matrix and hence, o \u2113=1 D j \u2113 = 1. Therefore, by the Jensen's inequality [19], we have:\no \u2113=1 D j \u2113 (y j \u2113 ) 2 \u2265 o \u2113=1 (D j \u2113 y j \u2113 ) 2 . Hence, V \u2265 0.\nWe next claim that {\u1e7c t } is a summable sequence with probability one. It can be observed from (23) that\nV t \u2264 o j=1 o 2 n R 2 j .(24)\nsince y j \u2113 \u2264 1,z j \u2113 \u2264R \u22122 j for each j \u2208 [n], \u2113 \u2208 [m] and D is a row-stochastic matrix. To prove the claim, it suffices to show that for each j \u2208 [m], the sequence { 1 R 2 j (t ) } is summable. Note that for each j \u2208 [m] and for each t, we haveR j (t + 1) =R j (t) + \u03f5 t where \u03f5 t \u2265 \u03f5 > 0 with probability p t \u2265 p > 0. Therefore, using the Borel-Cantelli Lemma for adapted processes [19] we have { 1 R 2 j (t ) } is summable which concludes the proof. \u25a1 Proof of Lemma 4.4: Fix i \u2208 [m], j \u2208 [n] and k \u2208 N. Let B be the event that at the t k 'th iteration, user reinforces a pair (i, \u2113) for some \u2113 \u2208 [n]. Then, on the complement B c of B, P + i j (\u03c9) = P i j (\u03c9). Let B 1 \u2286 B be the subset of B such that the pair (i, j) is reinforced and B 2 = B \\ B 1 be the event that some other pair (i, \u2113) is reinforced for \u2113 i.\nWe note that\nU + i j = S i j + 1 n \u2113=1 S i \u2113 + 1 1 B 1 + S i j n \u2113=1 S i \u2113 + 1 1 B 2 + U i j 1 B c .\nTherefore, we have\nE(U + i j | F k t ) = \u03c0 i U i j D ji S i j + 1 n \u2113=1 S i \u2113 + 1 + \u2113 j \u03c0 i U i \u2113 D \u2113i S i j n \u2113 \u2032 =1 S i \u2113 \u2032 + 1 + (1 \u2212 p)U i j ,\nwhere p = U (B | F k t ) = \u2113 \u03c0 i U i j D ji . Note that U i j = S i j n \u2113=1 S i \u2113 and hence,\nE(U + i j | F t ) \u2212 U i j = 1 n \u2113 \u2032 =1 S i \u2113 \u2032 + 1 \u03c0 i U i j D ji \u2212 \u03c0 i U i j \u2113 U i \u2113 D \u2113i .\nwhich can be rewritten as in (8). \u25a1 Proof of Theorem 4.5: Fix t = t k for some k \u2208 N. Let u + := u(t + 1), u := u(t), u i := u i (U (t), D(t)) and also defineS i := m \u2113 \u2032 =1 S i \u2113 \u2032 + 1. Then, using the linearity of conditional expectation and Lemma 4.1, we have:\nE(u + | F t ) \u2212 u = m i=1 n j=1 \u03c0 i D ji E(U + i j | F t ) \u2212 U i j = m i=1 n j=1 \u03c0 i D ji \u03c0 i U i j m \u2113 \u2032 =1 S j \u2113 \u2032 + 1 D ji \u2212 u i = m i=1 \u03c0 2 \u0129 S i n j=1 U i j (D ji ) 2 \u2212 (u i ) 2 . (25\n)\nNote that U is a row-stochastic matrix and hence, m i=1 U i j = 1. Therefore, by the Jensen's inequality [19], we have:\nn j=1 U i j (D ji ) 2 \u2265 n j=1 D ji U i j 2 = (u i ) 2 .\nReplacing this in the right-hand-side of (25), we conclude that E(u + | F t )\u2212u \u2265 0 and hence, the sequence {u(t)} is a submartingale. \u25a1 Proof of Corollary 4.6: Note from Theorem 4.3 and 4.5 that the sequence {u(t)} satisfies all the conditions of Theorem 4.2. Hence, proven. \u25a1", "publication_ref": ["b21", "b18", "b18", "b7", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "C FURTHER RELATED WORK", "text": "Database Interaction Sampling has been used to approximate the results of SQL queries with aggregation functions and achieve the fast response time needed by interactive database interfaces [12,29]. However, we use sampling techniques to learn the intent behind imprecise point queries and answer them effectively and efficiently.\nReinforcement Learning There is a recent interest in using exploitation-exploration paradigm to improve the understanding of users intents in an interactive document retrieval [24]. The exploitation-exploration trade-off has been also considered in finding keyword queries for data integration [51]. These methods, however, does not consider the impact of user learning throughout the interaction.\nGame-theoretic Models in Information Systems Researchers have also leveraged economical models to build query interfaces that return desired results to the users using the fewest possible interactions [53]. In particular, researchers have recently applied game-theoretic approaches to model the actions taken by users and document retrieval systems in a single session [35]. They propose a framework to find out whether the user likes to continue exploring the current topic or move to another topic. We, however, explore the development of common representations of intents between the user and DMBS. We also investigate the interactions that may contain various sessions and topics. Moreover, we focus on structured rather than unstructured data. Avestani et al. have used signaling games to create a shared lexicon between multiple autonomous systems [5]. Our work, however, focuses on modeling users' information needs and development of mutual understanding between users and the DBMS. Moreover, as opposed to the autonomous systems, a DBMS and user may update their information about the interaction in different time scales.", "publication_ref": ["b11", "b29", "b23", "b51", "b53", "b35", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "D MORE INFORMATION ABOUT THE SETTING OF EFFICIENCY ANALYSIS", "text": "Freebase is built based on the information about entities in the Wikipedia (wikipedia.org) articles. Each entity in Freebase database contains the URL of its corresponding article in Wikipedia. For our queries, we have used a sample of Bing (bing.com) query log whose relevant answers according to the click-through information, after filtering our noisy clicks, are in the Wikipedia articles [20]. We use two subsets of this sample whose relevant answers are in the TV-Program and Play databases. The set of queries over TV-Program has 621 (459 unique) queries with the average number of 3.65 keywords per query and the one over Play has 221 (141 unique) queries with the average number of 3.66 keywords per query. We use the frequencies of queries to calculate the prior probabilities of submission. After submitting each query and getting some results, we simulate user feedback using the relevance information in the Bing query log.", "publication_ref": ["b19"], "figure_ref": [], "table_ref": []}, {"heading": "E MITIGATING STARTUP PERIOD", "text": "Because our proposed learning algorithm is more exploratory than UCB-1, it may have a longer startup period than UCB-1's. One method is for the DBMS to use a less exploratory learning algorithm, such as UCB-1, at the beginning of the interaction. After a certain number of interactions, the DBMS can switch to our proposed learning algorithm. The DBMS can distinguish the time of switching to our algorithm by observing the amount of positive reinforcement it receives from the user. If the user does not provide any or very small number of positive feedback on the returned results, the DBMS is not yet ready to switch to a relatively more exploratory algorithm. If the DBMS observes a relatively large number of positive feedback on sufficiently many queries, it has already provided a relatively accurate answers to many queries. Finally, one may use a relatively large value of reinforcement in the database learning algorithm at the beginning of the interaction to reduce its degree of exploration. The DBMS may switch to a relatively small value of reinforcement after it observes positive feedback on sufficiently many queries.\nWe have implemented the latter of these methods by increasing the value of reinforcement by some factor. Figure 3 shows the results of applying this technique in our proposed DBMS learning algorithm over the Yahoo! query workload. The value of reinforcement is initially 3 and 6 times larger than the default value proposed in Section 4 until a threshold satisfaction value is reached, at which point the reinforcement values scales back down to its original rate. We notice that by increasing the reinforcement value by some factor, the startup period is reduced. However, there are some drawbacks to this method. Although we don't see it here, by increasing the rate of reinforcement in the beginning, some amount of exploration may be sacrificed. Thus more exploitation will occur in the beginning of the series of interactions. This may lead to behavior similar to UCB-1 and perform too much exploitation and not enough exploration. Finding the correct degree of reinforcement is an interesting area for future work.", "publication_ref": [], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "A USER LEARNING METHODS", "text": "Win-Keep/Lose-Randomize This method uses only the most recent interaction for an intent to determine the queries used to express the intent in the future [6]. Assume that the user conveys an intent e by a query q. If the reward of using q is above a specified threshold \u03c4 the user will use q to express e in the future. Otherwise, the user randomly picks another query uniformly at random to express e.", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Foundations of Databases: The Logical Level", "journal": "Addison-Wesley", "year": "1994", "authors": "Serge Abiteboul; Richard Hull; Victor Vianu"}, {"ref_id": "b1", "title": "Learning and verifying quantified boolean queries by example", "journal": "", "year": "2013", "authors": "Azza Abouzied; Dana Angluin; Christos H Papadimitriou; Joseph M Hellerstein; Avi Silberschatz"}, {"ref_id": "b2", "title": "Finite-time analysis of the multiarmed bandit problem", "journal": "Machine learning", "year": "2002", "authors": "Peter Auer; Nicolo Cesa-Bianchi; Paul Fischer"}, {"ref_id": "b3", "title": "The nonstochastic multiarmed bandit problem", "journal": "SIAM journal on computing", "year": "2002", "authors": "Peter Auer; Nicolo Cesa-Bianchi; Yoav Freund; Robert E Schapire"}, {"ref_id": "b4", "title": "Shared lexicon for distributed annotations on the Web", "journal": "", "year": "2005", "authors": "Paolo Avesani; Marco Cova"}, {"ref_id": "b5", "title": "The Role of Forgetting in the Evolution and Learning of Language", "journal": "Journal of Experimental and Theoretical Artificial Intelligence", "year": "2008", "authors": "J A Barrett; K Zollman"}, {"ref_id": "b6", "title": "Learning Join Queries from User Examples", "journal": "TODS", "year": "2015", "authors": "Angela Bonifati; Radu Ciucanu; Slawomir Staworko"}, {"ref_id": "b7", "title": "A stochastic model with applications to learning", "journal": "The Annals of Mathematical Statistics", "year": "1953", "authors": "R Robert; Frederick Bush;  Mosteller"}, {"ref_id": "b8", "title": "Reinforcement Learning in Information Searching", "journal": "Information Research: An International Electronic Journal", "year": "2013", "authors": "Yonghua Cen; Liren Gan; Chen Bai"}, {"ref_id": "b9", "title": "Query Recommendations for Interactive Database Exploration", "journal": "Springer-Verlag", "year": "2009", "authors": "Gloria Chatzopoulou; Magdalini Eirinaki; Neoklis Polyzotis"}, {"ref_id": "b10", "title": "Probabilistic Information Retrieval Approach for Ranking of Database Query Results", "journal": "TODS", "year": "2006", "authors": "Surajit Chaudhuri; Gautam Das; Vagelis Hristidis; Gerhard Weikum"}, {"ref_id": "b11", "title": "Approximate Query Processing: No Silver Bullet", "journal": "", "year": "2017-05-14", "authors": "Surajit Chaudhuri; Bolin Ding; Srikanth Kandula"}, {"ref_id": "b12", "title": "On Random Sampling over Joins", "journal": "ACM", "year": "1999", "authors": "Surajit Chaudhuri; Rajeev Motwani; Vivek Narasayya"}, {"ref_id": "b13", "title": "Keyword Search on Structured and Semi-structured Data", "journal": "", "year": "2009", "authors": "Yi Chen; Wei Wang; Ziyang Liu; Xuemin Lin"}, {"ref_id": "b14", "title": "Signaling games and stable equilibria", "journal": "Quarterly Journal of Economics", "year": "1987", "authors": "I Cho; D Kreps"}, {"ref_id": "b15", "title": "A stochastic learning model of economic behavior", "journal": "The Quarterly Journal of Economics", "year": "1973", "authors": "G John;  Cross"}, {"ref_id": "b16", "title": "On Learning Algorithms for Nash Equilibria", "journal": "Springer-Verlag", "year": "2010", "authors": "Constantinos Daskalakis; Rafael Frongillo; Christos H Papadimitriou; George Pierrakos; Gregory Valiant"}, {"ref_id": "b17", "title": "Explore-byexample: An Automatic Query Steering Framework for Interactive Data Exploration", "journal": "", "year": "2014", "authors": "Kyriaki Dimitriadou; Olga Papaemmanouil; Yanlei Diao"}, {"ref_id": "b18", "title": "Probability: theory and examples", "journal": "Cambridge university press", "year": "2010", "authors": "Rick Durrett"}, {"ref_id": "b19", "title": "Evaluating Evidences for Keyword Query Disambiguation in Entity Centric Database Search", "journal": "", "year": "2010", "authors": "Elena Demidova; Xuan Zhou; Irina Oelze; Wolfgang Nejdl"}, {"ref_id": "b20", "title": "On the Need for Low Rationality", "journal": "", "year": "1995", "authors": "Ido Erev; Alvin E Roth"}, {"ref_id": "b21", "title": "Optimal Aggregation Algorithms for Middleware", "journal": "ACM", "year": "2001", "authors": "Ronald Fagin; Amnon Lotem; Moni Naor"}, {"ref_id": "b22", "title": "Eye-tracking Analysis of User Behavior in WWW Search", "journal": "", "year": "2004", "authors": "Laura A Granka; Thorsten Joachims; Geri Gay"}, {"ref_id": "b23", "title": "Online Learning to Rank for Information Retrieval: SIGIR 2016 Tutorial", "journal": "", "year": "2016", "authors": "Artem Grotov;  Maarten De Rijke"}, {"ref_id": "b24", "title": "", "journal": "ACM", "year": "", "authors": ""}, {"ref_id": "b25", "title": "Balancing exploration and exploitation in listwise and pairwise online learning to rank for information retrieval", "journal": "Information Retrieval", "year": "2013", "authors": "Katja Hofmann; Shimon Whiteson; Maarten De Rijke"}, {"ref_id": "b26", "title": "Efficient IR-Style Keyword Search over Relational Databases", "journal": "", "year": "2003", "authors": "Vagelis Hristidis; Luis Gravano; Yannis Papakonstantinou"}, {"ref_id": "b27", "title": "Reinforcement learning in signaling game", "journal": "", "year": "2011", "authors": "Yilei Hu; Brian Skyrms; Pierre Tarr\u00e8s"}, {"ref_id": "b28", "title": "User See, User Point: Gaze and Cursor Alignment in Web Search", "journal": "", "year": "2012", "authors": "Jeff Huang; Ryen White; Georg Buscher"}, {"ref_id": "b29", "title": "Overview of Data Exploration Techniques", "journal": "", "year": "2015", "authors": "Stratos Idreos; Olga Papaemmanouil; Surajit Chaudhuri"}, {"ref_id": "b30", "title": "Making Database Systems Usable", "journal": "", "year": "2007", "authors": "H V Jagadish; Adriane Chapman; Aaron Elkiss; Magesh Jayapandian; Yunyao Li; Arnab Nandi; Cong Yu"}, {"ref_id": "b31", "title": "Quickr: Lazily Approximating Complex AdHoc Queries in BigData Clusters", "journal": "", "year": "2016", "authors": "Srikanth Kandula; Anil Shanbhag; Aleksandar Vitorovic; Matthaios Olma; Robert Grandl; Surajit Chaudhuri; Bolin Ding"}, {"ref_id": "b32", "title": "SnipSuggest: Context-aware Autocompletion for SQL", "journal": "", "year": "2010", "authors": "Nodira Khoussainova; Yongchul Kwon; Magdalena Balazinska; Dan Suciu"}, {"ref_id": "b33", "title": "Query From Examples: An Iterative, Data-Driven Approach to Query Construction", "journal": "PVLDB", "year": "2015", "authors": "Hao Li; Chee-Yong Chan; David Maier"}, {"ref_id": "b34", "title": "dbTouch in action database kernels for touch-based data exploration", "journal": "", "year": "1262", "authors": "Erietta Liarou; Stratos Idreos"}, {"ref_id": "b35", "title": "Win-Win Search: Dual-Agent Stochastic Game in Session Search", "journal": "", "year": "2014", "authors": "Jiyun Luo; Sicong Zhang; Hui Yang"}, {"ref_id": "b36", "title": "SPARK: Top-k Keyword Query in Relational Databases", "journal": "", "year": "2007", "authors": "Yi Luo; Xumein Lin; Wei Wang; Xiaofang Zhou"}, {"ref_id": "b37", "title": "An Introduction to Information Retrieval", "journal": "Cambridge University Press", "year": "2008", "authors": "Christopher Manning; Prabhakar Raghavan; Hinrich Schutze"}, {"ref_id": "b38", "title": "A Signaling Game Approach to Databases Querying and Interaction", "journal": "", "year": "2016", "authors": "Ben Mccamish; Arash Termehchy; Behrouz Touri"}, {"ref_id": "b39", "title": "An online learning framework for refining recency search results with user click feedback", "journal": "ACM Transactions on Information Systems (TOIS)", "year": "2012", "authors": "Taesup Moon; Wei Chu; Lihong Li; Zhaohui Zheng; Yi Chang"}, {"ref_id": "b40", "title": "The Neuroscience of Reinforcement Learning", "journal": "", "year": "2009", "authors": "Yael Niv"}, {"ref_id": "b41", "title": "Random Sampling from Databases. Ph.D. Dissertation. University of California", "journal": "", "year": "1993", "authors": "Frank Olken"}, {"ref_id": "b42", "title": "Learning diverse rankings with multi-armed bandits", "journal": "ACM", "year": "2008", "authors": "Filip Radlinski; Robert Kleinberg; Thorsten Joachims"}, {"ref_id": "b43", "title": "A convergence theorem for non negative almost supermartingales and some applications", "journal": "Springer", "year": "1985", "authors": "Herbert Robbins; David Siegmund"}, {"ref_id": "b44", "title": "Learning in extensive-form games: Experimental data and simple dynamic models in the intermediate term", "journal": "Games and economic behavior", "year": "1995", "authors": "E Alvin; Ido Roth;  Erev"}, {"ref_id": "b45", "title": "Some topics in two-person games", "journal": "", "year": "1964", "authors": "S Lloyd;  Shapley"}, {"ref_id": "b46", "title": "Reinforcement learning and human behavior", "journal": "Current Opinion in Neurobiology", "year": "2014", "authors": "Hanan Shteingart; Yonatan Loewenstein"}, {"ref_id": "b47", "title": "Ranked bandits in metric spaces: learning diverse rankings over large document collections", "journal": "Journal of Machine Learning Research", "year": "2013-02", "authors": "Aleksandrs Slivkins; Filip Radlinski; Sreenivas Gollapudi"}, {"ref_id": "b48", "title": "", "journal": "Query by Output. In SIGMOD", "year": "2009", "authors": "Q Tran; C Chan; S Parthasarathy"}, {"ref_id": "b49", "title": "Gathering additional feedback on search results by multi-armed bandits with respect to production ranking", "journal": "", "year": "2015", "authors": "Aleksandr Vorobev; Damien Lefortier; Gleb Gusev; Pavel Serdyukov"}, {"ref_id": "b50", "title": "Yahoo! webscope dataset anonymized Yahoo! search logs with relevance judgments version 1", "journal": "", "year": "2011-01", "authors": "! Yahoo"}, {"ref_id": "b51", "title": "Actively soliciting feedback for query answers in keyword searchbased data integration", "journal": "VLDB Endowment", "year": "2013", "authors": "Zhepeng Yan; Nan Zheng; Zachary G Ives; Partha Pratim Talukdar; Cong Yu"}, {"ref_id": "b52", "title": "The K-armed Dueling Bandits Problem", "journal": "J. Comput. Syst. Sci", "year": "2012", "authors": "Yisong Yue; Josef Broder; Robert Kleinberg; Thorsten Joachims"}, {"ref_id": "b53", "title": "Information Retrieval as Card Playing: A Formal Model for Optimizing Interactive Retrieval Interface", "journal": "", "year": "2015", "authors": "Yinan Zhang; Chengxiang Zhai"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Accuracies of learning over the subsamples of Table53.2.5 Results. Win-Keep/Lose-Randomize performs surprisingly more accurate than other methods for the 8H-interaction subsample. It indicates that in short-term and/or beginning of their interactions, users may not have enough interactions to leverage a more complex learning scheme and use a rather simple mechanism to update their strategies. Both Roth and Erev's methods use the accumulated reward values to adjust the user strategy gradually. Hence, they cannot precisely model user learning over a rather short interaction and are less accurate than relatively more aggressive learning models such as Bush and Mosteller's and Cross's over this subsample. Both Roth and Erev's deliver the same result and outperform other methods in the 43-H and 101-H subsamples. Win-Keep/Lose-Randomize is the least accurate method over these subsamples. Since larger subsamples provide more training data, the predication accuracy of all models improves as the interaction subsamples becomes larger. The learned value for the forget parameter in the Roth and Erev's modified model is very small and close to zero in our experiments, therefore, it generally acts like the Roth and Erev's model.Long-term communications between users and DBMS may include multiple sessions. Since Yahoo! query workload contains the time stamps and user ids of each interaction, we have been able to extract the starting and ending times of each session. Our results indicate that as long as the user and DBMS communicate over sufficiently many of interactions, e.g., about 10k for Yahoo! query workload, the users follow the Roth and Erev's model of learning. Given that the communication of the user and DBMS involve sufficiently many interactions, we have not observed any difference in the mechanism by which users learn based on the numbers of sessions in the user and DBMS communication.", "figure_data": ""}, {"figure_label": "41", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Lemma 4 . 1 .41For any \u2113 \u2208 [m] and j \u2208 [n], we have", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "\u221et=0 \u03b2(t) < \u221e almost surely). Hence, {u(t)} converges almost surely.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "1 2 \u03a02T S \u2208C N |T S | in which Sc max (T S) is the maximum query score of tuples in the tuple-set T S and |T S | is the size of each tuple-set. The term 1 n ( T S \u2208C N Sc max (T S)", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 2 :2Figure 2: Mean reciprocal rank for 1,000,000 interactions", "figure_data": ""}, {"figure_label": "41", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Proof of Lemma 4 . 1 :41Fix \u2113 \u2208 [m] and j \u2208 [n]. Let A be the event that at the t'th iteration, we reinforce a pair (j, \u2113 \u2032 ) for some \u2113 \u2032 \u2208 [m].", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 3 :3Figure 3: Mean reciprocal rank for 1,000,000 interactions with different degrees of reinforcements", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "A database instance of relation Univ", "figure_data": "NameAbbreviation State TypeRankMissouri State UniversityMSUMOpublic 20Mississippi State University MSUMSpublic 22Murray State UniversityMSUKYpublic 14Michigan State UniversityMSUMIpublic 18"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": "Intents and Queries2(a) IntentsIntent#Intente 1ans(z) \u2190 U niv(x, 'M SU ', 'M S ', y, z)e 2ans(z) \u2190 U niv(x, 'M SU ', 'M I ', y, z)e 3ans(z) \u2190 U niv(x, 'M SU ', 'MO ', y, z)2(b) QueriesQuery#Queryq 1'MSU MI'q 2'MSU'"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Two strategy profiles over the intents and queries in Table2. User and DBMS strategies at the top and bottom, respectively.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Summary of the notations used in the model.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Subsamples of Yahoo! interaction log", "figure_data": "Duration #Interactions #Users #Queries #Intents8H6222721116243H123234056341151101H195468795161397648293.2.3 Parameter"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Average candidate networks processing times in seconds for 1000 interactions", "figure_data": "DatabaseReservoir Poisson-OlkenPlay0.0780.042TV Program 0.2980.1717 RELATED WORKQuery learning: Database community has proposed several sys-tems that help the DBMS learn the user's information need by show-ing examples to the user and collecting her feedback"}], "formulas": [{"formula_id": "formula_0", "formula_text": "u r (U , D) = m i=1 \u03c0 i n j=1 U i j o \u2113=1 D j \u2113 r (e i , e \u2113 ),(1)", "formula_coordinates": [4.0, 362.07, 167.02, 196.14, 28.96]}, {"formula_id": "formula_1", "formula_text": "(U (t), D(t), \u03c0 , (e u (t \u2212 1)), (q(t \u2212 1)), (e d (t \u2212 1)), (r (t \u2212 1)", "formula_coordinates": [4.0, 317.96, 584.92, 199.98, 10.82]}, {"formula_id": "formula_2", "formula_text": "u r (U , D) = m i=1 \u03c0 i n j=1 U i j o \u2113=1 D j \u2113 r i \u2113 ,", "formula_coordinates": [6.0, 372.13, 412.93, 131.36, 28.95]}, {"formula_id": "formula_3", "formula_text": "R j \u2113 (0) o \u2113=1 R j \u2113 (0) > 0 for all j \u2208 [n] and \u2113 \u2208 [o]. c. For iterations t = 1, 2, . . ., do i.", "formula_coordinates": [6.0, 318.6, 514.15, 240.01, 46.37]}, {"formula_id": "formula_4", "formula_text": "P(E(t) = i \u2032 | q(t)) = D q(t )i \u2032 (t).", "formula_coordinates": [6.0, 383.03, 577.85, 109.82, 12.51]}, {"formula_id": "formula_5", "formula_text": "R j \u2113 (t + 1) = R j \u2113 (t) + r i \u2113 if j = q(t) and \u2113 = i \u2032 R j \u2113 (t) otherwise .(2)", "formula_coordinates": [6.0, 342.34, 629.52, 215.87, 22.84]}, {"formula_id": "formula_6", "formula_text": "D ji (t + 1) = R ji (t + 1) o \u2113=1 R j \u2113 (t + 1) ,(3)", "formula_coordinates": [6.0, 384.17, 670.51, 174.03, 24.8]}, {"formula_id": "formula_7", "formula_text": "u(t) := u r (U , D(t)),(4)", "formula_coordinates": [7.0, 138.7, 381.79, 155.35, 9.78]}, {"formula_id": "formula_8", "formula_text": "E(X (t + 1) | F t ) \u2265 X (t),", "formula_coordinates": [7.0, 130.48, 438.85, 86.79, 9.78]}, {"formula_id": "formula_9", "formula_text": "X (t + 1) | F t ) = E(X (t + 1) | X (t ), . . . , X (1)).", "formula_coordinates": [7.0, 146.11, 701.19, 140.84, 7.82]}, {"formula_id": "formula_10", "formula_text": "E(D + j \u2113 | F t ) \u2212 D j \u2113 = D j \u2113 \u2022 m i=1 \u03c0 i U i j r i l R j + r il \u2212 o \u2113 \u2032 =1 D j \u2113 \u2032 r i \u2113 \u2032 R j + r i \u2113 \u2032 , whereR j = o \u2113 \u2032 =1 R j \u2113 \u2032 .", "formula_coordinates": [7.0, 317.96, 126.39, 215.5, 62.82]}, {"formula_id": "formula_11", "formula_text": "E(X (t + 1)|F t ) \u2265 X (t) \u2212 \u03b2(t)", "formula_coordinates": [7.0, 386.19, 257.04, 103.69, 9.78]}, {"formula_id": "formula_12", "formula_text": "E(u(t + 1 | F t ) \u2265 E(u(t) | F t ) \u2212 \u03b2(t),", "formula_coordinates": [7.0, 371.93, 351.97, 132.2, 9.78]}, {"formula_id": "formula_13", "formula_text": "r i \u2113 = 1 if i = \u2113, 0 otherwise .", "formula_coordinates": [7.0, 392.07, 607.71, 91.5, 17.7]}, {"formula_id": "formula_14", "formula_text": "U i j (0) = S i j (0) n j \u2032 =1 S i j \u2032 (0) for all i \u2208 [m] and j \u2208 [n] and let U (t k ) = U (t k \u2212 1) = \u2022 \u2022 \u2022 = U (t k \u22121 + 1) for all k.", "formula_coordinates": [8.0, 63.7, 102.29, 230.34, 52.24]}, {"formula_id": "formula_15", "formula_text": "P(q(t k ) = j | i(t k ) = i) = U i j (t k ).", "formula_coordinates": [8.0, 114.59, 204.82, 118.39, 10.4]}, {"formula_id": "formula_16", "formula_text": "P(i \u2032 (t k ) = i \u2032 | q(t k ) = j) = D ji \u2032 (t k ).", "formula_coordinates": [8.0, 109.89, 247.59, 127.79, 12.49]}, {"formula_id": "formula_17", "formula_text": "S + i j = S i j (t k ) + 1 if j = q(t k ) and i(t k ) = i \u2032 (t k ) S i j (t k ) otherwise", "formula_coordinates": [8.0, 79.54, 292.01, 182.31, 23.0]}, {"formula_id": "formula_18", "formula_text": "U i j (t k + 1) = S i j (t k + 1) n j \u2032 =1 S i j \u2032 (t k + 1) ,(6)", "formula_coordinates": [8.0, 115.1, 348.94, 178.95, 24.75]}, {"formula_id": "formula_19", "formula_text": "u(t) := u r (U , D(t)) = u r (U (t), D(t)),(7)", "formula_coordinates": [8.0, 108.48, 476.34, 185.56, 9.78]}, {"formula_id": "formula_20", "formula_text": "E(U + i j | F t ) \u2212 U i j = \u03c0 i U i j n \u2113=1 S i \u2113 + 1 (D ji \u2212 u i (t))(8)", "formula_coordinates": [8.0, 91.0, 536.45, 203.05, 24.8]}, {"formula_id": "formula_21", "formula_text": "u i (t) = n j=1 U i j (t)D ji (t).", "formula_coordinates": [8.0, 130.16, 574.82, 86.83, 28.67]}, {"formula_id": "formula_22", "formula_text": "E(u(t + 1) | F t ) \u2212 u(t) \u2265 0 (9", "formula_coordinates": [8.0, 126.44, 654.47, 164.44, 9.78]}, {"formula_id": "formula_23", "formula_text": ")", "formula_coordinates": [8.0, 290.87, 657.13, 3.17, 4.09]}, {"formula_id": "formula_24", "formula_text": "for all i = 1 \u2208 k do insert t into A[i] with probability Sc(t ) W", "formula_coordinates": [9.0, 373.55, 484.38, 154.39, 22.59]}, {"formula_id": "formula_25", "formula_text": "Sc(t ) M ,", "formula_coordinates": [9.0, 539.74, 608.1, 19.45, 14.85]}, {"formula_id": "formula_26", "formula_text": "M C N = 1 n ( T S \u2208C N Sc max (T S))", "formula_coordinates": [9.0, 440.2, 655.2, 118.0, 11.64]}, {"formula_id": "formula_27", "formula_text": "|t 1 \u22caR 2 | |t \u22caR 2 | t \u2208R 1 max", "formula_coordinates": [10.0, 257.39, 437.69, 34.46, 18.38]}, {"formula_id": "formula_28", "formula_text": "Sc(t 1 ) t \u2208R 1 (Sc(t ))", "formula_coordinates": [10.0, 153.8, 530.57, 35.99, 16.61]}, {"formula_id": "formula_29", "formula_text": "Sc(t 2 ) t \u2208t 1 \u22caR 2 (Sc(t )) if R 2 is a tuple-set and 1 |t 1 \u22caR 2 | if R 2", "formula_coordinates": [10.0, 53.62, 602.37, 240.43, 25.88]}, {"formula_id": "formula_30", "formula_text": "t \u2208t 1 \u22caR 2 Sc(t ) max ( t \u2208s \u22caR 2 , s \u2208R 1 Sc(t ))", "formula_coordinates": [10.0, 54.99, 628.95, 79.98, 18.27]}, {"formula_id": "formula_31", "formula_text": "have |t \u22ca R 2 | t \u2208R 1 max \u2264 |t \u22ca B 2 | t \u2208B 1 max . Because max ( t \u2208s\u22caR 2 ,s \u2208R 1 Sc(t)) \u2264 max t \u2208R 2 (Sc(t))|t \u22ca R 2 | t \u2208R 1 max , we have max ( t \u2208s\u22caR 2 ,s \u2208R 1 Sc(t)) \u2264 max t \u2208R 2 (Sc(t))|t \u22ca B 2 | t \u2208B 1", "formula_coordinates": [10.0, 317.96, 154.11, 240.25, 38.75]}, {"formula_id": "formula_32", "formula_text": "t \u2208t 1 \u22caR 2 Sc(t ) max t \u2208R 2 (Sc(t ))|t \u22caB 2 | t \u2208B 1 max", "formula_coordinates": [10.0, 347.18, 192.94, 83.88, 20.37]}, {"formula_id": "formula_33", "formula_text": "Sc(t ) W", "formula_coordinates": [10.0, 481.61, 408.78, 16.32, 14.85]}, {"formula_id": "formula_34", "formula_text": "x \u2190 x \u2212 1 else let CN = R 1 \u25b7\u25c1 . . . \u25b7\u25c1 R n for all t \u2208 R 1 do Pick value X from distribution B(k, Sc(t ) W ) Pipeline X copies of t to the Olken algorithm if Olken accepts m tuples then x \u2190 x \u2212 m", "formula_coordinates": [10.0, 360.1, 433.92, 191.63, 86.68]}, {"formula_id": "formula_35", "formula_text": "U i j (t + 1) = U i j (t) + \u03b1 BM \u2022 (1 \u2212 U i j (t)) q j = q(t) \u2227 r \u2265 0 U i j (t) \u2212 \u03b2 BM \u2022 U i j (t) q j = q(t) \u2227 r < 0 (10", "formula_coordinates": [14.0, 57.74, 177.14, 232.88, 24.91]}, {"formula_id": "formula_36", "formula_text": ")", "formula_coordinates": [14.0, 290.62, 188.03, 3.42, 4.09]}, {"formula_id": "formula_37", "formula_text": "U i j (t + 1) = U i j (t) \u2212 \u03b1 BM \u2022 U i j (t) q j q(t) \u2227 r \u2265 0 U i j (t) + \u03b2 BM \u2022 (1 \u2212 U i j (t) q j q(t) \u2227 r < 0 (11) \u03b1 BM \u2208 [0, 1] and \u03b2 BM \u2208 [0, 1] are parameters of the model.", "formula_coordinates": [14.0, 59.33, 216.0, 236.09, 42.2]}, {"formula_id": "formula_38", "formula_text": "U i j (t + 1) = U i j (t) + R(r ) \u2022 (1 \u2212 U i j (t)) q j = q(t) U i j (t) \u2212 R(r ) \u2022 U i j (t) q j q(t)(12)", "formula_coordinates": [14.0, 73.68, 341.72, 220.37, 22.93]}, {"formula_id": "formula_39", "formula_text": "R(r ) = \u03b1 C \u2022 r + \u03b2 C(13)", "formula_coordinates": [14.0, 139.93, 372.23, 154.12, 11.4]}, {"formula_id": "formula_40", "formula_text": "S i j (t + 1) = S i j (t) + r q j = q(t) S i j (t) q j q(t)(14)", "formula_coordinates": [14.0, 111.0, 499.09, 183.05, 22.93]}, {"formula_id": "formula_41", "formula_text": "U i j (t + 1) = S i j (t + 1) n j \u2032 S i j \u2032 (t + 1)(15)", "formula_coordinates": [14.0, 127.35, 529.0, 166.7, 34.63]}, {"formula_id": "formula_42", "formula_text": "S i j (t + 1) = (1 \u2212 \u03c3 ) \u2022 S i j (t) + E(j, R(r ))(16)", "formula_coordinates": [14.0, 104.66, 700.33, 189.38, 9.78]}, {"formula_id": "formula_43", "formula_text": "E(j, R(r )) = R(r ) \u2022 (1 \u2212 \u03f5) q j = q(t) R(r ) \u2022 (\u03f5) q j q(t)(17)", "formula_coordinates": [14.0, 369.59, 97.82, 188.61, 22.93]}, {"formula_id": "formula_44", "formula_text": "U i j (t + 1) = S i j (t + 1) n j \u2032 S i j \u2032 (t + 1)(19)", "formula_coordinates": [14.0, 391.5, 150.78, 166.7, 34.63]}, {"formula_id": "formula_45", "formula_text": "D + j \u2113 = m i=1 R j \u2113 + r il R j + r i \u2113 1 A i, \u2113 + o \u2113 \u2032 =1 \u2113 \u2032 \u2113 R j l R j + r i \u2113 \u2032 1 A i, \u2113 \u2032 + D j \u2113 1 A c .", "formula_coordinates": [14.0, 349.72, 498.4, 171.27, 48.74]}, {"formula_id": "formula_46", "formula_text": "E(D + j \u2113 | F t ) = m i=1 \u03c0 i U i j D j \u2113 R j \u2113 + r i l R j + r i \u2113 + m i=1 \u03c0 i U i j \u2113 \u2113 \u2032 D j \u2113 \u2032 R j l R j + r i \u2113 \u2032 + (1 \u2212 p)D j \u2113 ,", "formula_coordinates": [14.0, 358.58, 563.8, 158.91, 59.26]}, {"formula_id": "formula_47", "formula_text": "E(D + j \u2113 | F t ) \u2212 D j \u2113 = m i=1 \u03c0 i U i j D j \u2113 r i \u2113Rj \u2212 R j l R j (R j + r i \u2113 ) \u2212 m i=1 \u03c0 i U i j \u2113 \u2113 \u2032 D j \u2113 \u2032 R j \u2113 r i \u2113 \u2032 R j (R j + r i \u2113 \u2032 ) .", "formula_coordinates": [14.0, 333.41, 648.24, 220.13, 59.26]}, {"formula_id": "formula_48", "formula_text": "u j := u j (U (t), D(t)) = m i=1 o \u2113=1 \u03c0 i U i j D j \u2113 r i \u2113(t ) ,", "formula_coordinates": [15.0, 92.53, 127.8, 162.24, 28.96]}, {"formula_id": "formula_49", "formula_text": "E(u + | F t ) \u2212 u = m i=1 n j=1 \u03c0 i U i j o \u2113=1 r i \u2113 \u2032 E(D + j \u2113 | F t ) \u2212 D j \u2113 = m i=1 n j=1 o \u2113=1 \u03c0 i U i j D j \u2113 r i \u2113 m i \u2032 =1 \u03c0 \u2032 i U i \u2032 j r i \u2032 l R j + r i \u2032 \u2113 \u2212 o \u2113 \u2032 =1 D j \u2113 \u2032 r i \u2032 \u2113 \u2032 R j + r i \u2032 \u2113 \u2032 .(20)", "formula_coordinates": [15.0, 61.4, 208.2, 232.65, 94.05]}, {"formula_id": "formula_50", "formula_text": "E(u + | F t ) \u2212 u = n j=1 o \u2113=1 D j \u2113 y i \u2113 z j \u2113 \u2212 o \u2113=1 D j \u2113 y j \u2113 o \u2113 \u2032 =1 D j \u2113 \u2032z j \u2113 \u2032 . (21)", "formula_coordinates": [15.0, 79.11, 335.94, 214.93, 45.31]}, {"formula_id": "formula_51", "formula_text": "E(u + | F t ) \u2212 u = V t +\u1e7c t (22", "formula_coordinates": [15.0, 129.23, 401.15, 161.4, 12.21]}, {"formula_id": "formula_52", "formula_text": ")", "formula_coordinates": [15.0, 290.62, 406.24, 3.42, 4.09]}, {"formula_id": "formula_53", "formula_text": "V t = n j=1 1 R j o \u2113=1 D j \u2113 y 2 j \u2113 \u2212 o l =1 D j \u2113 y j \u2113 2 , and\u1e7c t = n j=1 o \u2113=1 D j \u2113 y j \u2113 o \u2113 \u2032 =1 D j \u2113 \u2032z j \u2113 \u2032 \u2212 m \u2113=1 D j \u2113 y j \u2113zj \u2113 .(23)", "formula_coordinates": [15.0, 53.8, 430.72, 240.25, 75.39]}, {"formula_id": "formula_54", "formula_text": "o \u2113=1 D j \u2113 (y j \u2113 ) 2 \u2265 o \u2113=1 (D j \u2113 y j \u2113 ) 2 . Hence, V \u2265 0.", "formula_coordinates": [15.0, 53.8, 612.35, 174.54, 42.74]}, {"formula_id": "formula_55", "formula_text": "V t \u2264 o j=1 o 2 n R 2 j .(24)", "formula_coordinates": [15.0, 148.23, 681.66, 145.81, 28.67]}, {"formula_id": "formula_56", "formula_text": "U + i j = S i j + 1 n \u2113=1 S i \u2113 + 1 1 B 1 + S i j n \u2113=1 S i \u2113 + 1 1 B 2 + U i j 1 B c .", "formula_coordinates": [15.0, 341.87, 257.42, 191.74, 24.8]}, {"formula_id": "formula_57", "formula_text": "E(U + i j | F k t ) = \u03c0 i U i j D ji S i j + 1 n \u2113=1 S i \u2113 + 1 + \u2113 j \u03c0 i U i \u2113 D \u2113i S i j n \u2113 \u2032 =1 S i \u2113 \u2032 + 1 + (1 \u2212 p)U i j ,", "formula_coordinates": [15.0, 359.24, 302.13, 157.59, 52.54]}, {"formula_id": "formula_58", "formula_text": "E(U + i j | F t ) \u2212 U i j = 1 n \u2113 \u2032 =1 S i \u2113 \u2032 + 1 \u03c0 i U i j D ji \u2212 \u03c0 i U i j \u2113 U i \u2113 D \u2113i .", "formula_coordinates": [15.0, 333.44, 394.4, 209.03, 45.41]}, {"formula_id": "formula_59", "formula_text": "E(u + | F t ) \u2212 u = m i=1 n j=1 \u03c0 i D ji E(U + i j | F t ) \u2212 U i j = m i=1 n j=1 \u03c0 i D ji \u03c0 i U i j m \u2113 \u2032 =1 S j \u2113 \u2032 + 1 D ji \u2212 u i = m i=1 \u03c0 2 \u0129 S i n j=1 U i j (D ji ) 2 \u2212 (u i ) 2 . (25", "formula_coordinates": [15.0, 347.55, 507.52, 207.23, 94.87]}, {"formula_id": "formula_60", "formula_text": ")", "formula_coordinates": [15.0, 554.78, 586.07, 3.42, 4.09]}, {"formula_id": "formula_61", "formula_text": "n j=1 U i j (D ji ) 2 \u2265 n j=1 D ji U i j 2 = (u i ) 2 .", "formula_coordinates": [15.0, 369.26, 639.22, 138.16, 30.69]}], "doi": "10.1145/3183713.3196899"}