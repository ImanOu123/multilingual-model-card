{"title": "BSP-Net: Generating Compact Meshes via Binary Space Partitioning", "authors": "Zhiqin Chen; Andrea Tagliasacchi; Hao Zhang", "pub_date": "2020-12-07", "abstract": "Polygonal meshes are ubiquitous in the digital 3D domain, yet they have only played a minor role in the deep learning revolution. Leading methods for learning generative models of shapes rely on implicit functions, and generate meshes only after expensive iso-surfacing routines. To overcome these challenges, we are inspired by a classical spatial data structure from computer graphics, Binary Space Partitioning (BSP), to facilitate 3D learning. The core ingredient of BSP is an operation for recursive subdivision of space to obtain convex sets. By exploiting this property, we devise BSP-Net, a network that learns to represent a 3D shape via convex decomposition. Importantly, BSP-Net is unsupervised since no convex shape decompositions are needed for training. The network is trained to reconstruct a shape using a set of convexes obtained from a BSPtree built on a set of planes. The convexes inferred by BSP-Net can be easily extracted to form a polygon mesh, without any need for iso-surfacing. The generated meshes are compact (i.e., low-poly) and well suited to represent sharp geometry; they are guaranteed to be watertight and can be easily parameterized. We also show that the reconstruction quality by BSP-Net is competitive with state-of-the-art methods while using much fewer primitives. Code is available at https://github.com/czq142857/BSP-NET-original.", "sections": [{"heading": "Introduction", "text": "Recently, there has been an increasing interest in representation learning and generative modeling for 3D shapes. Up to now, deep neural networks for shape analysis and synthesis have been developed mainly for voxel grids [15,19,50,52], point clouds [1,34,35,57,58], and implicit functions [5,14,23,29,54]. As the dominant 3D shape representation for modeling, display, and animation, polygonal meshes have not figured prominently amid these developments. One of the main reasons is that the nonuniformity and irregularity of triangle tessellations do not naturally support conventional convolution and pooling operations [20]. However, compared to voxels and point clouds, meshes can provide a more seamless and coherent Figure 1: (a) 3D shape auto-encoding by BSP-Net quickly reconstructs a compact, i.e., low-poly, mesh, which can be easily textured. The mesh edges reproduce sharp details in the input (e.g., edges of the legs), yet still approximate smooth geometry (e.g., circular table-top). (b) State-of-theart methods regress an indicator function, which needs to be iso-surfaced, resulting in over-tessellated meshes which only approximate sharp details with smooth surfaces. surface representation; they are more controllable, easier to manipulate, and are more compact, attaining higher visual quality using fewer primitives; see Figure 1.\nFor visualization purposes, the generated voxels, point clouds, and implicits are typically converted into meshes in post-processing, e.g., via iso-surface extraction by Marching Cubes [27]. Few deep networks can generate polygonal meshes directly, and such methods are limited to genus-zero meshes [18,28,46], piece-wise genus-zero [13] meshes, meshes sharing the same connectivity [12,43], or meshes with very low number of vertices [7]. Patch-based approaches can generate results which cover a 3D shape with planar polygons [48] or curved [17] mesh patches, but their visual quality is often tampered by visible seams, incoherent patch connections, and rough surface appearance. It is difficult to texture or manipulate such mesh outputs.\nIn this paper, we develop a generative neural network which outputs polygonal meshes natively. Specifically, parameters or weights that are learned by the network can predict multiple planes which fit the surfaces of a 3D shape, resulting in a compact and watertight polygonal mesh; see Figure 1. We name our network BSP-Net, since each facet is associated with a binary space partitioning (BSP), and the shape is composed by combining these partitions. BSP-Net learns an implicit field: given n point coordinates and a shape feature vector as input, the network outputs values indicating whether the points are inside or outside the shape. The construction of this implicit function is illustrated in Figure 2, and consists of three steps: 1 a collection of plane equations implies a collection of p binary partitions of space; see Figure 2-top; 2 an operator T p\u00d7c groups these partitions to create a collection of c convex shape primitives/parts; 3 finally, the part collection is merged to produce the implicit field of the output shape.\nFigure 3 shows the network architecture of BSP-Net corresponding to these three steps: 1 given the feature code, an MLP produces in layer L 0 a matrix P p\u00d74 of canonical parameters that define the implicit equations of p planes: ax + by + cz + d = 0; these implicit functions are evaluated on a collection of n point coordinates x n\u00d74 in layer L 1 ; 2 the operator T p\u00d7c is a binary matrix that enforces a selective neuron feed from L 1 to the next network layer L 2 , forming convex parts; 3 finally, layer L 3 assembles the parts into a shape via either sum or min-pooling.\nAt inference time, we feed the input to the network to obtain components of the BSP-tree, i.e., leaf nodes (planes P) and connections (binary weights T). We then apply classic Constructive Solid Geometry (CSG) to extract the explicit polygonal surfaces of the shapes. The mesh is typically compact, formed by a subset of the p planes directly from the network, leading to a significant speed-up over the previous networks during inference, and without the need for expensive iso-surfacing -current inference time is about 0.5 seconds per generated mesh. Furthermore, meshes generated by the network are guaranteed to be watertight, possibly with sharp features, in contrast to smooth shapes produced by previous implicit decoders [5,23,29].\nBSP-Net is trainable and characterized by interpretable network parameters defining the hyper-planes and their formation into the reconstructed surface. Importantly, the network training is self-supervised as no ground truth convex shape decompositions are needed. BSP-Net is trained to reconstruct all shapes from the training set using the same set of convexes constructed in layer L 2 of the network. As a result, our network provides a natural correspondence between all the shapes at the level of the convexes. BSP-Net does not yet learn semantic parts. Grouping of the convexes into semantic parts can be obtained manually, or learned otherwise as semantic shape segmentation is a well-studied problem. Such a grouping only need to be done on each convex once to propagate the semantic understanding to all shapes containing the same semantic parts.", "publication_ref": ["b14", "b18", "b49", "b51", "b0", "b33", "b34", "b56", "b57", "b4", "b13", "b22", "b28", "b53", "b19", "b26", "b17", "b27", "b45", "b12", "b11", "b42", "b6", "b47", "b16", "b4", "b22", "b28"], "figure_ref": ["fig_0", "fig_0", "fig_1"], "table_ref": []}, {"heading": "Contributions.", "text": "\u2022 BSP-Net is the first deep generative network which directly outputs compact and watertight polygonal meshes with arbitrary topology and structure variety.\n\u2022 The learned BSP-tree allows us to infer both shape segmentation and part correspondence.\n\u2022 By adjusting the encoder of our network, BSP-Net can also be adapted for shape auto-encoding and single-view 3D reconstruction (SVR).\n\u2022 To the best of our knowledge, BSP-Net is among the first to achieve structured SVR, reconstructing a segmented 3D shape from a single unstructured object image.\n\u2022 Last but not the least, our network is also the first which can reconstruct and recover sharp geometric features.\nThrough extensive experiments on shape auto-encoding, segmentation, part correspondence, and single-view reconstruction, we demonstrate state-of-the-art performances by BSP-Net. Comparisons are made to leading methods on shape decomposition and 3D reconstruction, using conventional distortion metrics, visual similarity, as well as a new metric assessing the capacity of a model in representing sharp features. In particular, we highlight the favorable fidelity-complexity trade-off exhibited by our network.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related work", "text": "Large shape collections such as ShapeNet [2] and Part-Net [31] have spurred the development of learning techniques for 3D data processing. In this section, we cover representative approaches based on the underlying shape representation learned, with a focus on generative models.\nGrid models. Early approaches generalized 2D convolutions to 3D [6,15,25,50,51], and employed volumetric grids to represent shapes in terms of coarse occupancy functions, where a voxel evaluates to zero if it is outside and one otherwise. Unfortunately, these methods are typically limited to low resolutions of at most 64 3 due to the cubic growth in memory requirements. To generate finer results, differentiable marching cubes operations have been proposed [27], as well as hierarchical strategies [19,37,44,47,48] that alleviate the curse of dimensionality affecting dense volumetric grids. Another alternative is to use multi-view images [26,42] and geometry images [40,41], which allow standard 2D convolution, but such methods are only suitable on the encoder side of a network architecture, while we focus on decoders. Finally, recent methods that perform sparse convolutions [16] on voxel grids are similarly limited to encoders. Surface models. As much of the semantics of 3D models is captured by their surface, the boundary between inside/outside space, a variety of methods have been proposed to represent shape surfaces in a differentiable way. Amongst these we find a category of techniques pioneered by Point-Net [34] that express surfaces as point clouds [1,9,11,34,35,55,58], and techniques pioneered by AtlasNet [17] that adopt a 2D-to-3D mapping process [49,41,46,55]. An interesting alternative is to consider mesh generation as the process of estimating vertices and their connectivity [7], but these methods do not guarantee watertight results, and hardly scale beyond a hundred vertices. Implicit models. A very recent trend has been the modeling of shapes as a learnable indicator function [5,23,29], rather than a sampling of it, as in the case of voxel methods. The resulting networks treat reconstruction as a classification problem, and are universal approximators [21] whose reconstruction precision is proportional to the network complexity. However, at inference time, generating a 3D model still requires the execution of an expensive iso-surfacing operation whose performance scales cubically in the desired resolution. In contrast, our network directly outputs a lowpoly approximation of the shape surface.\nShape decomposition. BSP-Net generates meshes using a part-based approach, hence techniques that learn shape decompositions are of particular relevance. There are methods that decompose shapes as oriented boxes [45,32], axis aligned gaussians [14], super-quadrics [33], or a union of indicator functions, in BAE-NET [4]. The architecture of our network draws inspiration from BAE-NET, which is designed to segment a shape by reconstructing its parts in different branches of the network. For each shape part, BAE-NET learns an implicit field by means of a binary classifier. In contrast, BSP-Net explicitly learns a tree structure built on plane subdivisions for bottom-up part assembly.\nAnother similar work is CvxNet [8], which decomposes shapes as a collection of convex primitives. However, BSP-Net differs from CvxNet in several significant ways: 1 we target low-poly reconstruction with sharp features, while they target smooth reconstruction; 2 their network always outputs K convexes, while the \"right\" number of primitives is learnt automatically in our method; 3 our optimization routine is completely different from theirs, as their compositional tree structure is hard-coded.\nStructured models. There have been recent works on learning structured 3D models, in particular, linear [60] or hierarchical [24,59,30,32] organization of part bounding boxes. While some methods learn part geometries separately [24,30], others jointly embed/encode structure and geometry [53,13]. What is common about all of these methods is that they are supervised, and were trained on shape collections with part segmentations and labels. In contrast, BSP-Net is unsupervised. On the other hand, our network is not designed to infer shape semantics; it is trained to learn convex decompositions. To the best of our knowledge, there is only one prior work, Im2Struct [32], which infers part structures from a single-view image. However, this work only produces a box arrangement; it does not reconstruct a structured shape like BSP-Net.\nBinary and capsule networks. The discrete optimization for the tree structures in BSP-Net bears some resemblance to binary [22] and XNOR [36] neural networks. However, only one layer of BSP-Net employs binary weights, and our training method differs, as we use a continuous relaxation of the weights in early training. Further, as our network can be thought of as a simplified scene graph, it holds striking similarities to the principles of capsule networks [38], where low-level capsules (hyperplanes) are aggregated in higher (convexes) and higher (shapes) capsule representations. Nonetheless, while [38] addresses discriminative tasks (encoder), we focus on generative tasks (decoder).", "publication_ref": ["b1", "b30", "b5", "b14", "b24", "b49", "b50", "b26", "b18", "b36", "b43", "b46", "b47", "b25", "b41", "b39", "b40", "b15", "b33", "b0", "b8", "b10", "b33", "b34", "b54", "b57", "b16", "b48", "b40", "b45", "b54", "b6", "b4", "b22", "b28", "b20", "b44", "b31", "b13", "b32", "b3", "b7", "b59", "b23", "b58", "b29", "b31", "b23", "b29", "b52", "b12", "b31", "b21", "b35", "b37", "b37"], "figure_ref": [], "table_ref": []}, {"heading": "Method", "text": "We seek a deep representation of geometry that is both trainable and interpretable. This is achieved by devising a network architecture that provides a differentiable Binary Space Partitioning tree (BSP-tree) representation 1 , a classical spatial data structure originated from graphics [39,10]. This representation is easily trainable as it encodes geometry via implicit functions, and interpretable since its outputs are a collection of convex polytopes. While we generally target 3D geometry, we employ 2D examples to explain the technique without loss of generality.\nWe achieve our goal via a network containing three main modules, which act on feature vectors extracted by an encoder corresponding to the type of input data (e.g. the features produced by ResNet for images or 3D CNN for voxels). In more details, a first layer that extracts hyperplanes conditional on the input data, a second layer that groups hyperplanes in the form of half-spaces to create parts (convexes), and a third layer assembles parts together to reconstruct the overall object; see Figure 3.\nLayer 1: hyperplane extraction. Given a feature vector f , we apply a multi-layer perceptron P to obtain plane parameters P p\u00d74 , where p is the number of planes -i.e. P = P \u03c9 (f ). For any point x = (x, y, z, 1), the product D = xP T is a vector of signed distances to each planethe ith distance is negative if x is inside, and positive if it is outside, the ith plane, with respect to the plane normal.\nLayer 2: hyperplane grouping. To group hyperplanes into geometric primitives we employ a binary matrix T p\u00d7c . Via a max-pooling operation we aggregate input planes to form a set of c convex primitives:\nC * j (x) = max i (D i T ij ) < 0 inside > 0 outside. (1\n)\nNote that during training the gradients would flow through only one (max) of the planes. Hence, to ease training, we employ a version that replaces max with summation:\nC + j (x) = i relu(D i )T ij = 0 inside > 0 outside.(2)\nLayer 3: shape assembly. This layer groups convexes to create a possibly non-convex output shape via min-pooling:\nS * (x) = min j (C + j (x)) = 0 inside > 0 outside. (3\n)\nNote that the use of C + in the expression above is intentional. We avoid using C * due to the lack of a memory efficient implementation of the operator in TensorFlow 1. Again, to facilitate learning, we distribute gradients to all convexes by resorting to a (weighted) summation:\nS + (x)= \uf8ee \uf8f0 j W j 1 \u2212 C + j (x) [0,1] \uf8f9 \uf8fb [0,1] = 1 \u2248 in [0, 1) \u2248 out,(4)\nwhere W c\u00d71 is a weight vector, and [\u2022] [0,1] performs clipping. During training we will enforce W\u22481. Note that the inside/outside status here is only approximate. For example, when W=1, and all C + j =0.5, one is outside of all convexes, but inside their composition.\nTwo-stage training. Losses evaluated on (4) will be approximate, but have better gradient than (3). Hence, we develope a two-stage training scheme where: 1 in the continuous phase, we try to keep all weights continuous and compute an approximate solution via S + (x) -this would generate an approximate result as can be observed in Figure 4 (b); 2 in the next discrete phase, we quantize the weights and use a perfect union to generate accurate results by fine-tuning on S * (x) -this creates a much finer reconstruction as illustrated in Figure 4 (c,d).\nOur two-stage training strategy is inspired by classical optimization, where smooth relaxation of integer problems is widely accepted, and mathematically principled.", "publication_ref": ["b0", "b38", "b9"], "figure_ref": ["fig_1", "fig_2", "fig_2"], "table_ref": []}, {"heading": "Training Stage 1 -Continuous", "text": "We initialize T and W with random zero-mean Gaussian noise having \u03c3 = 0.02, and optimize the network via:\narg min \u03c9,T,W L + rec + L + T + L + W .(5)\nGiven query points x, our network is trained to match S(x) to the ground truth indicator function, denoted by F(x|G), in a least-squares sense:\nL + rec = E x\u223cG (S + (x) \u2212 F(x|G)) 2 ,(6)\nwhere x\u223cG indicates a sampling that is specific to the training shape G -including random samples in the unit box as well as samples near the boundary \u2202G; see [5]. An edge between plane i and convex j is represented by T ij =1, and the entry is zero otherwise. We perform a continuous relaxation of a graph adjacency matrix T, where we require its values to be bounded in the [0, 1] range:\nL + T = t\u2208T max(\u2212t, 0) + t\u2208T max(t \u2212 1, 0).(7)\nNote that this is more effective than using a sigmoid activation, as its gradients do not vanish. Further, we would like W to be close to 1 so that the merge operation is a sum:\nL + W = j |W j \u2212 1|.(8)\nHowever, we remind the reader that we initialize with W \u22480 to avoid vanishing gradients in early training.", "publication_ref": ["b4"], "figure_ref": [], "table_ref": []}, {"heading": "Training Stage 2 -Discrete", "text": "In the second stage, we first quantize T by picking a threshold \u03bb = 0.01 and assign t=(t>\u03bb)?1:0. Experimentally, we found the values learnt for T to be small, which led  to our choice of a small threshold value. With the quantized T, we fine-tune the network by:\narg min \u03c9 L * recon + L * overlap ,(9)\nwhere we ensure that the shape is well reconstructed via:\nL * recon = E x\u223cG [F(x|G) \u2022 max(S * (x), 0)] (10) +E x\u223cG [(1 \u2212 F(x|G)) \u2022 (1 \u2212 min(S * (x), 1))] . (11)\nThe above loss function pulls S * (x) towards 0 if x should be inside the shape; it pushes S * (x) beyond 1 otherwise. Optionally, we can also discourage overlaps between the convexes. We first compute a mask M such that M (x, j)=1 if x is in convex j and x is contained in more than one convex, and then evaluate:\nL * overlap = \u2212E x\u223cG E j M (x, j)C + j (x) .(12)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithmic and training details", "text": "In our 2D experiments, we use p=256 planes and c=64 convexes. We use a simple 2D convolutional encoder where each layer downsamples the image by half, and doubles the number of feature channels. We use the centers of all pixels as samples. In our 3D experiments, we use p=4, 096 planes and c=256 convexes. The encoder for voxels is a 3D CNN encoder where each layer downsamples the grid by half, and doubles the number of feature channels. It takes a volume of size 64 3 as input. The encoder for images is ResNet-18 without pooling layers that receives images of size 128 2 as input. All encoders produce feature codes |f |=256. The dense network P \u03c9 has widths {512, 1024, 2048, 4p} where the last layer outputs the plane parameters.\nWhen training the auto-encoder for 3D shapes, we adopt the progressive training from [5], on points sampled from grids that are increasingly denser (16 3 , 32 3 , 64 3 ). Note that the hierarchical training is not necessary for convergence, but results in an \u2248 3\u00d7 speedup in convergence. In Stage 1, we train the network on 16 3 grids for 8 million iterations with batch size 36, then 32 3 for 8 million iterations with batch size 36, then 64 3 for 8 million iterations with batch size 12. In Stage 2, we train the network on 64 3 grids for 8 million iterations with batch size 12.\nFor single-view reconstruction, we also adopt the training scheme in [5], i.e., train an auto-encoder first, then only train the image encoder of the SVR model to predict latent codes instead of directly predicting the output shapes. We train the image encoder for 1,000 epochs with batch size 64. We run our experiments on a workstation with an Nvidia GeForce RTX 2080 Ti GPU. When training the autoencoder (one model on the 13 ShapeNet categories), Stage 1 takes about \u22483 days and Stage 2 takes \u22482 days; training the image-encoder requires \u22481 day.", "publication_ref": ["b4", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Results and evaluation", "text": "We study the behavior of BSP-Net on a synthetic 2D shape dataset (Section 4.1), and evaluate our auto-encoder (Section 4.2), as well as single view reconstruction (Section 4.3) compared to other state-of-the-art methods.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Auto-encoding 2D shapes", "text": "To illustrate how our network works, we created a synthetic 2D dataset. We place a diamond, a cross, and a hollow diamond with varying sizes over 64 \u00d7 64 images; see Figure 4(a). The order of the three shapes is sorted so that the diamond is always on the left and the hollow diamond is The color assignment was performed on a few shapes: once a convex is colored in one shape, we can propagate the color to the other shapes by using the learnt convex id.   Figure 5 visualizes the planes used to construct the individual convexes -we visualize planes i in convex j so that T ij =1 and P 2 i1 + P 2 i2 + P 2 i3 >\u03b5 for a small threshold \u03b5 (to ignore planes with near-zero gradients). Note how BSP-Net creates a natural semantic correspondence across inferred convexes. For example, the hollow diamond in Figure 4(d) is always made of the same four convexes in the same relative positions -this is mainly due to the static structure in T : different shapes need to share the same set of convexes and their associated hyper-planes.", "publication_ref": [], "figure_ref": ["fig_2", "fig_3", "fig_2"], "table_ref": []}, {"heading": "Auto-encoding 3D shapes", "text": "For 3D shape autoencoding, we compare BSP-Net to a few other shape decomposition networks: Volumetric Primitives (VP) [45], Super Quadrics (SQ) [33], and Branched Auto Encoders (BAE) [4]. Note that for the segmentation task, we also evaluate on BAE*, the version of BAE that uses the values of the predicted implicit function, and not just the classification boundaries -please note that the surface reconstructed by BAE and BAE* are identical.\nSince all these methods target shape decomposition tasks, we train single class networks, and evaluate segmentation as well as reconstruction performance. We use the ShapeNet (Part) Dataset [56], and focus on five classes: airplane, car, chair, lamp and table. For the car class, since none of the networks separates surfaces (as we perform volumetric modeling), we reduce the parts from (wheel, body, hood, roof) \u2192 (wheel, body); and analogously for lamps (base, pole, lampshade, canopy) \u2192 (base, pole, lampshade) and tables (top, leg, support) \u2192 (top, leg).\nAs quantitative metrics for reconstruction tasks, we report symmetric Chamfer Distance (CD, scaled by \u00d71000) and Normal Consistency (NC) computed on 4k surface sampled points. We also report the Light Field Distance (LFD) [3] -the best-known visual similarity metric from computer graphics. For segmentation tasks, we report the typical mean per-label Intersection Over Union (IOU).\nSegmentation. Table 2 shows the per category segmentation results. As we have ground truth part labels for the point clouds in the dataset, after training each network, we obtain the part label for each primitive/convex by voting: for each point we identify the nearest primitive to it, and then the point will cast a vote for that primitive on the corresponding part label. Afterwards, for each primitive, we assign to it the part label that has the highest number of votes. We use 20% of the dataset for assigning part labels, and we use all the shapes for testing. At test time, for each point in the point cloud, we find its nearest primitive, and assign the part label of the primitive to the point. In the comparison to BAE, we employ their one-shot training scheme [4,Sec.3.1]. Note that BAE-NET* is specialized to the segmentation task, while our work mostly targets part-based reconstruction; as such, the IoU performance in Table 2 is an upper bound of segmentation performance.\nFigure 6 shows semantic segmentation and part correspondence implied by BSP-Net autoencoding, showing how individual parts (left/right arm/leg, etc.) are matched. In our method, all shapes are corresponded at the primitive (convexes) level. To reveal shape semantics, we manually group convexes belonging to the same semantic part and assign them the same color. Note that the color assignment is done on each convex once, and propagated to all the shapes.\nReconstruction comparison. BSP-Net achieves significantly better reconstruction quality, while maintaining high segmentation accuracy; see Table 1 and Figure 7, where we color each primitive based on its inferred part label. BAE-NET was designed for segmentation, thus produces poorquality part-based 3D reconstructions. Note how BSP-Net is able to represent complex parts such as legs of swivel chairs in Figure 7, while none of the other methods can.", "publication_ref": ["b44", "b32", "b3", "b55", "b2", "b3"], "figure_ref": ["fig_4", "fig_5", "fig_5"], "table_ref": ["tab_2", "tab_2", "tab_1"]}, {"heading": "Single view reconstruction (SVR)", "text": "We compare our method with AtlasNet [17], IM-NET [5] and OccNet [29] on the task of single view reconstruction. We report quantitative results in Table 3 and Table 4, and qualitative results in Figure 8. We use the 13 categories in ShapeNet [2] that have more than 1,000 shapes each, and the rendered views from 3D-R2N2 [6]. We train one model on all categories, using 80% of the shapes for training and 20% for testing, in a similar fashion to Atlas-Net [17]. For other methods, we download the pre-trained models released by the authors. Since the pre-trained Oc-cNet [29] model has a different train-test split than others, we evaluate it on the intersection of the test splits.", "publication_ref": ["b16", "b4", "b28", "b1", "b5", "b16", "b28"], "figure_ref": [], "table_ref": ["tab_4", "tab_5"]}, {"heading": "Edge Chamfer Distance (ECD).", "text": "To measure the capacity of a model to represent sharp features, we introduce a new metric. We first compute an \"edge sampling\" of the surface by generating 16k points S={s i } uniformly distributed on the surface of a model, and then compute sharpness as: Figure 8: Single-view 3D reconstructioncomparison to AtlasNet [17], IM-NET [5], and OccNet [29]. Middle column shows mesh tessellations of the reconstruction; last column shows the edge sampling used in the ECD metric. \u03c3(s i ) = min j\u2208N\u03b5(si) |n i \u2022 n j |, where N \u03b5 (s) extracts the indices of the samples in S within distance \u03b5 from s, and n is the surface normal of a sample. We set \u03b5=0.01, and generate our edge sampling by retaining points such that \u03c3(s i )<0.1; see Figure 8. Given two shapes, the ECD between them is nothing but the Chamfer Distance between the corresponding edge samplings.\nAnalysis. Our method achieves comparable performance to the state-of-the-art in terms of Chamfer Distance. As for visual quality, our method also outperforms most other   methods, which is reflected by the superior results in terms of Light Field Distance. Similarly to Figure 6, we manually color each convex to show part correspondences in Figure 9.\nWe visualize the triangulations of the output meshes in Figure 8: our method outputs meshes with a smaller number of polygons than state-of-the-art methods. Note that these methods cannot generate low-poly meshes, and their vertices are always distributed quasi-uniformly. Finally, note that our method is the only one amongst those tested capable of representing sharp edges -this can be observed quantitatively in terms of Edge Chamfer Distance, where BSP-Net performs much better. Note that At-lasNet could also generate edges in theory, but the shape is not watertight and the edges are irregular, as it can be seen in the zoom-ins of Figure 8. We also analyze these metrics aggregated on the entire testing set in Table 4. In this final analysis, we also include OccNet 128 and IM-NET 256 , which are the original resolutions used by the authors. Note the average number of polygons inferred by our method is 654 (recall #polygons \u2264 #triangles in polygonal meshes).", "publication_ref": ["b16", "b4", "b28"], "figure_ref": ["fig_4", "fig_6"], "table_ref": ["tab_5"]}, {"heading": "Conclusion, limitation, and future work", "text": "We introduce BSP-Net, an unsupervised method which can generate compact and structured polygonal meshes in the form of convex decomposition. Our network learns a BSP-tree built on the same set of planes, and in turn, the same set of convexes, to minimize a reconstruction loss for the training shapes. These planes and convexes are defined by weights learned by the network. Compared to state-ofthe-art methods, meshes generated by BSP-Net exhibit superior visual quality, in particular, sharp geometric details, when comparable number of primitives are employed.\nThe main limitation of BSP-Net is that it can only decompose a shape as a union of convexes. Concave shapes, e.g., a teacup or ring, have to be decomposed into many small convex pieces, which is unnatural and leads to wasting of a considerable amount of representation budget (planes and convexes). A better way to represent such shapes is to do a difference operation rather than union. How to generalize BSP-Net to express a variety of CSG operations is an interesting direction for future work.\nCurrent training times for BSP-Net are quite significant: 6 days for 4, 096 planes and 256 convexes for the SVR task trained across all categories; inference is fast however. While most shapes only need a small number of planes to represent, we cannot reduce the total number of planes as they are needed to well represent a large set of shapes. It would be ideal if the network can adapt the primitive count based on the complexity of the input shapes; this may call for an architectural change to the network.\nWhile its applicability to RGBD data could leverage the auto-decoder ideas explored by [23], the generalization of our method beyond curated datasets [2], and the ability to train from only RGB images are of critical importance.", "publication_ref": ["b22", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We thank the anonymous reviewers for their valuable comments. This work was supported in part by an NSERC grant (No. 611370), a Google Faculty Research Award, and Google Cloud Platform research credits.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Learning representations and generative models for 3d point clouds", "journal": "", "year": "2018", "authors": "Panos Achlioptas; Olga Diamanti; Ioannis Mitliagkas; Leonidas J Guibas"}, {"ref_id": "b1", "title": "ShapeNet: An information-rich 3D model repository", "journal": "", "year": "2008", "authors": "Angel X Chang; Thomas Funkhouser; Leonidas Guibas; Pat Hanrahan; Qixing Huang; Zimo Li; Silvio Savarese; Manolis Savva; Shuran Song; Hao Su; Jianxiong Xiao; Li Yi; Fisher Yu"}, {"ref_id": "b2", "title": "On visual similarity based 3d model retrieval", "journal": "", "year": "2003", "authors": "Ding-Yun Chen; Xiao-Pei Tian; Yu-Te Shen; Ming Ouhyoung"}, {"ref_id": "b3", "title": "BAE-NET: Branched autoencoder for shape co-segmentation. ICCV", "journal": "", "year": "2019", "authors": "Zhiqin Chen; Kangxue Yin; Matthew Fisher; Siddhartha Chaudhuri; Hao Zhang"}, {"ref_id": "b4", "title": "Learning implicit fields for generative shape modeling", "journal": "", "year": "2007", "authors": "Zhiqin Chen; Hao Zhang"}, {"ref_id": "b5", "title": "3d-r2n2: A unified approach for single and multi-view 3d object reconstruction", "journal": "", "year": "2016", "authors": "B Christopher; Danfei Choy; Junyoung Xu; Kevin Gwak; Silvio Chen;  Savarese"}, {"ref_id": "b6", "title": "Scan2mesh: From unstructured range scans to 3d meshes", "journal": "", "year": "2019", "authors": "Angela Dai; Matthias Nie\u00dfner"}, {"ref_id": "b7", "title": "Cvxnets: Learnable convex decomposition", "journal": "", "year": "", "authors": "Boyang Deng; Kyle Genova; Soroosh Yazdani; Sofien Bouaziz; Geoffrey Hinton; Andrea Tagliasacchi"}, {"ref_id": "b8", "title": "A point set generation network for 3d object reconstruction from a single image", "journal": "", "year": "2017", "authors": "Haoqiang Fan; Hao Su; Leonidas J Guibas"}, {"ref_id": "b9", "title": "On visible surface generation by a priori tree structures", "journal": "", "year": "1980", "authors": "Henry Fuchs; M Zvi; Bruce F Kedem;  Naylor"}, {"ref_id": "b10", "title": "Multiresolution tree networks for 3d point cloud processing", "journal": "", "year": "2018", "authors": "Matheus Gadelha; Rui Wang; Subhransu Maji"}, {"ref_id": "b11", "title": "Sparse data driven mesh deformation", "journal": "", "year": "2019", "authors": "Lin Gao; Yu-Kun Lai; Jie Yang; Ling-Xiao Zhang; Leif Kobbelt; Shihong Xia"}, {"ref_id": "b12", "title": "SDM-NET: Deep generative network for structured deformable mesh", "journal": "ACM Trans. on Graphics", "year": "2019", "authors": "Lin Gao; Jie Yang; Tong Wu; Hongbo Fu; Yu-Kun Lai; Hao Zhang"}, {"ref_id": "b13", "title": "Learning shape templates with structured implicit functions", "journal": "", "year": "2019", "authors": "Kyle Genova; Forrester Cole; Daniel Vlasic; Aaron Sarna; William T Freeman; Thomas Funkhouser"}, {"ref_id": "b14", "title": "Learning a predictable and generative vector representation for objects", "journal": "", "year": "2016", "authors": "Rohit Girdhar; F David; Mikel Fouhey; Abhinav Rodriguez;  Gupta"}, {"ref_id": "b15", "title": "and Laurens van der Maaten. 3d semantic segmentation with submanifold sparse convolutional networks", "journal": "", "year": "2018", "authors": "Benjamin Graham; Martin Engelcke"}, {"ref_id": "b16", "title": "Atlasnet: A papier-m\u00e2ch\u00e9 approach to learning 3d surface generation", "journal": "", "year": "2007", "authors": "Thibault Groueix; Matthew Fisher; Vladimir G Kim; Bryan Russell; Mathieu Aubry"}, {"ref_id": "b17", "title": "Multi-chart generative surface modeling", "journal": "ACM Trans. on Graphics (TOG)", "year": "2018", "authors": "Haggai Heli Ben Hamu; Itay Maron; Gal Kezurer; Yaron Avineri;  Lipman"}, {"ref_id": "b18", "title": "Hierarchical surface prediction for 3d object reconstruction", "journal": "", "year": "2017", "authors": "Christian H\u00e4ne; Shubham Tulsiani; Jitendra Malik"}, {"ref_id": "b19", "title": "MeshCNN: A network with an edge", "journal": "ACM Trans. on Graphics", "year": "", "authors": "Rana Hanocka; Amir Hertz; Noa Fish; Raja Giryes; Shachar Fleishman; Daniel Cohen-Or"}, {"ref_id": "b20", "title": "Approximation capabilities of multilayer feedforward networks", "journal": "Neural Networks", "year": "1991", "authors": "Kurt Hornik"}, {"ref_id": "b21", "title": "Ran El-Yaniv, and Yoshua Bengio. Binarized neural networks", "journal": "", "year": "2016", "authors": "Itay Hubara; Matthieu Courbariaux; Daniel Soudry"}, {"ref_id": "b22", "title": "DeepSDF: Learning continuous signed distance functions for shape representation", "journal": "", "year": "2008", "authors": "Jeong Joon Park; Peter Florence; Julian Straub; Richard Newcombe; Steven Lovegrove"}, {"ref_id": "b23", "title": "Grass: Generative recursive autoencoders for shape structures", "journal": "ACM Trans. on Graphics", "year": "2017", "authors": "Jun Li; Kai Xu; Siddhartha Chaudhuri; Ersin Yumer; Hao Zhang; Leonidas Guibas"}, {"ref_id": "b24", "title": "Deep marching cubes: Learning explicit surface representations", "journal": "", "year": "2018", "authors": "Yiyi Liao; Simon Donn\u00e9; Andreas Geiger"}, {"ref_id": "b25", "title": "Learning efficient point cloud generation for dense 3d object reconstruction", "journal": "", "year": "2018", "authors": "Chen-Hsuan Lin; Chen Kong; Simon Lucey"}, {"ref_id": "b26", "title": "Marching cubes: A high resolution 3d surface construction algorithm", "journal": "", "year": "1987", "authors": "William E Lorensen; Harvey E Cline"}, {"ref_id": "b27", "title": "and Yaron Lipman. Convolutional neural networks on surfaces via seamless toric covers", "journal": "ACM Trans. on Graphics", "year": "2017", "authors": "Meirav Haggai Maron; Noam Galun; Miri Aigerman; Nadav Trope; Ersin Dym; Vladimir G Yumer;  Kim"}, {"ref_id": "b28", "title": "Occupancy networks: Learning 3D reconstruction in function space", "journal": "", "year": "2007", "authors": "Lars Mescheder; Michael Oechsle; Michael Niemeyer; Sebastian Nowozin; Andreas Geiger"}, {"ref_id": "b29", "title": "Structurenet: Hierarchical graph networks for 3d shape generation", "journal": "SIGGRAPH Asia", "year": "2019", "authors": "Kaichun Mo; Paul Guerrero; Li Yi; Hao Su; Peter Wonka; Niloy Mitra; Leonidas J Guibas"}, {"ref_id": "b30", "title": "PartNet: A largescale benchmark for fine-grained and hierarchical part-level 3D object understanding", "journal": "", "year": "2019", "authors": "Kaichun Mo; Shilin Zhu; Angel X Chang; Li Yi; Subarna Tripathi; Leonidas J Guibas; Hao Su"}, {"ref_id": "b31", "title": "Im2struct: Recovering 3d shape structure from a single RGB image", "journal": "", "year": "2018", "authors": "Chengjie Niu; Jun Li; Kai Xu"}, {"ref_id": "b32", "title": "Superquadrics revisited: Learning 3d shape parsing beyond cuboids", "journal": "", "year": "2019", "authors": "Despoina Paschalidou; Ali Osman Ulusoy; Andreas Geiger"}, {"ref_id": "b33", "title": "PointNet: Deep learning on point sets for 3D classification and segmentation", "journal": "", "year": "2017", "authors": "Charles R Qi; Hao Su; Kaichun Mo; Leonidas J Guibas"}, {"ref_id": "b34", "title": "Point-Net++: Deep hierarchical feature learning on point sets in a metric space", "journal": "", "year": "2017", "authors": "Charles R Qi; Li Yi; Hao Su; Leonidas J Guibas"}, {"ref_id": "b35", "title": "Xnor-net: Imagenet classification using binary convolutional neural networks", "journal": "In ECCV", "year": "2016", "authors": "Mohammad Rastegari; Vicente Ordonez; Joseph Redmon; Ali Farhadi"}, {"ref_id": "b36", "title": "Octnetfusion: Learning depth fusion from data", "journal": "", "year": "2017", "authors": "Gernot Riegler; Ali Osman Ulusoy; Horst Bischof; Andreas Geiger"}, {"ref_id": "b37", "title": "Dynamic routing between capsules", "journal": "", "year": "2017", "authors": "Sara Sabour; Nicholas Frosst; Geoffrey E Hinton"}, {"ref_id": "b38", "title": "Study for applying computer-generated images to visual simulation", "journal": "", "year": "1969", "authors": "R Schumacher"}, {"ref_id": "b39", "title": "Deep learning 3d shape surfaces using geometry images", "journal": "", "year": "2016", "authors": "Ayan Sinha; Jing Bai; Karthik Ramani"}, {"ref_id": "b40", "title": "Surfnet: Generating 3d shape surfaces using deep residual networks", "journal": "", "year": "2017", "authors": "Ayan Sinha; Asim Unmesh; Qixing Huang; Karthik Ramani"}, {"ref_id": "b41", "title": "Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition", "journal": "", "year": "2015", "authors": "Hang Su; Subhransu Maji"}, {"ref_id": "b42", "title": "Variational autoencoders for deforming 3D mesh models", "journal": "", "year": "2018", "authors": "Qingyang Tan; Lin Gao; Yu-Kun Lai; Shihong Xia"}, {"ref_id": "b43", "title": "Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs", "journal": "", "year": "2017", "authors": "Maxim Tatarchenko; Alexey Dosovitskiy; Thomas Brox"}, {"ref_id": "b44", "title": "Learning shape abstractions by assembling volumetric primitives", "journal": "", "year": "2017", "authors": "Shubham Tulsiani; Hao Su; Leonidas J Guibas; Alexei A Efros; Jitendra Malik"}, {"ref_id": "b45", "title": "Pixel2mesh: Generating 3d mesh models from single rgb images", "journal": "", "year": "2018", "authors": "Nanyang Wang; Yinda Zhang; Zhuwen Li; Yanwei Fu; Wei Liu; Yu-Gang Jiang"}, {"ref_id": "b46", "title": "Octree-based Convolutional Neural Networks for 3D Shape Analysis. SIGGRAPH", "journal": "", "year": "2017", "authors": "Peng-Shuai Wang; Yang Liu; Yu-Xiao Guo; Chun-Yu Sun; Xin Tong;  O-Cnn"}, {"ref_id": "b47", "title": "Adaptive O-CNN: A Patch-based Deep Representation of 3D Shapes", "journal": "SIGGRAPH Asia", "year": "2018", "authors": "Peng-Shuai Wang; Chun-Yu Sun; Yang Liu; Xin Tong"}, {"ref_id": "b48", "title": "Deep geometric prior for surface reconstruction", "journal": "", "year": "2019", "authors": "Francis Williams; Teseo Schneider; Claudio Silva; Denis Zorin; Joan Bruna; Daniele Panozzo"}, {"ref_id": "b49", "title": "Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling", "journal": "", "year": "2016", "authors": "Jiajun Wu; Chengkai Zhang; Tianfan Xue; Bill Freeman; Josh Tenenbaum"}, {"ref_id": "b50", "title": "Learning shape priors for single-view 3d completion and reconstruction", "journal": "", "year": "2018", "authors": "Jiajun Wu; Chengkai Zhang; Xiuming Zhang; Zhoutong Zhang; William T Freeman; Joshua B Tenenbaum"}, {"ref_id": "b51", "title": "3d shapenets: A deep representation for volumetric shapes", "journal": "", "year": "2015", "authors": "Zhirong Wu; Shuran Song; Aditya Khosla; Fisher Yu; Linguang Zhang; Xiaoou Tang; Jianxiong Xiao"}, {"ref_id": "b52", "title": "SAGNet: Structure-aware generative network for 3D-shape modeling", "journal": "SIGGRAPH", "year": "2019", "authors": "Zhijie Wu; Xiang Wang; Di Lin; Dani Lischinski; Daniel Cohen-Or; Hui Huang"}, {"ref_id": "b53", "title": "DISN: deep implicit surface network for high-quality single-view 3d reconstruction", "journal": "NeurIPS", "year": "2019", "authors": "Qiangeng Xu; Weiyue Wang; Duygu Ceylan; Radom\u00edr Mech; Ulrich Neumann"}, {"ref_id": "b54", "title": "Foldingnet: Point cloud auto-encoder via deep grid deformation", "journal": "", "year": "2018", "authors": "Yaoqing Yang; Chen Feng; Yiru Shen; Dong Tian"}, {"ref_id": "b55", "title": "A scalable active framework for region annotation in 3D shape collections", "journal": "SIGGRAPH Asia", "year": "2016", "authors": "Li Yi; G Vladimir; Duygu Kim;  Ceylan; Mengyan Shen; Hao Yan; Cewu Su; Qixing Lu; Alla Huang; Leonidas Sheffer;  Guibas"}, {"ref_id": "b56", "title": "LOGAN: Unpaired shape transform in latent overcomplete space", "journal": "ACM Trans. on Graphics (TOG)", "year": "2019", "authors": "Kangxue Yin; Zhiqin Chen; Hui Huang; Daniel Cohen-Or; Hao Zhang"}, {"ref_id": "b57", "title": "P2P-NET: Bidirectional point displacement net for shape transform", "journal": "ACM Trans. on Graphics (TOG)", "year": "2018", "authors": "Kangxue Yin; Hui Huang; Daniel Cohen-Or; Hao Zhang"}, {"ref_id": "b58", "title": "SCORES: Shape composition with recursive substructure priors", "journal": "ACM Trans. on Graphics (TOG)", "year": "2018", "authors": "Chenyang Zhu; Kai Xu; Siddhartha Chaudhuri; Renjiao Yi; Hao Zhang"}, {"ref_id": "b59", "title": "3D-PRNN: Generating shape primitives with recurrent neural networks", "journal": "", "year": "2017", "authors": "Chuhang Zou; Ersin Yumer; Jimei Yang; Duygu Ceylan; Derek Hoiem"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: An illustration of \"neural\" BSP-tree.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: The network corresponding to Figure 2.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Evaluation in 2Dauto-encoder trained on the synthetic 2D dataset. We show auto-encoding results and highlight mistakes made in Stage 1 with red circles, which are resolved in Stage 2. We further show the effect of enabling the (optional) overlap loss. Notice that in the visualization we use different (possibly repeating) colors to indicate different convexes.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure 5: Examples of L 2 outputa few convexes from the first shape in Figure 4, and the planes to construct them. Note how many planes are unused.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 6 :6Figure 6: Segmentation and correspondencesemantics implied from autoencoding by BSP-Net. Colors shown here are the result of a manual grouping of learned convexes.The color assignment was performed on a few shapes: once a convex is colored in one shape, we can propagate the color to the other shapes by using the learnt convex id.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 7 :7Figure 7: Segmentation and reconstruction / Qualitative.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 9 :9Figure 9: Structured SVR by BSP-Net reconstructs each shape with corresponding convexes. Convexes belonging to the same semantic parts are manually grouped and assigned the same color, resulting in semantic part correspondence.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Surface reconstruction quality and comparison for 3D shape autoencoding. Best results are marked in bold.", "figure_data": "planecar chair lamp table meanVP [45]37.6 41.9 64.7 62.2 62.156.9SQ [33]48.9 49.5 65.6 68.3 77.766.2BAE [4]40.6 46.9 72.3 41.6 68.259.8Ours74.2 69.5 80.9 52.3 90.379.3Ours + L  *  overlap74.5 69.7 82.1 53.4 90.379.8BAE* [4]75.4 73.5 85.2 73.9 86.481.8"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Segmentation: comparison in per-label IoU.always on the right -this is to mimic the structure of shape datasets such as ShapeNet[2]. After training Stage 1, our network has already achieved a good approximate S + reconstruction, however, by inspecting S * , the output of our inference, we can see there are several imperfections. After the fine-tuning in Stage 2, our network achieves near perfect reconstructions. Finally, the use of overlap losses significantly improves the compactness of representation,", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "OccNet 32 IM-NET 32 Ours Atlas0 Atlas25 OccNet 32 IM-NET 32 Ours Atlas0 Atlas25 OccNet 32 IM-NET 32", "figure_data": "Chamfer Distance (CD)Edge Chamfer Distance (ECD)Light Field Distance (LFD)Atlas0 Atlas25 Oursairplane0.5870.4401.5342.211 0.7590.3960.5751.4940.815 0.487 5129.36 4680.377760.427581.13 4496.91bench1.0860.8883.2201.933 1.2260.6580.8572.1311.400 0.475 4387.28 4220.104922.894281.18 3380.46cabinet1.2311.1731.0991.902 1.1883.6762.82110.8049.521 0.435 1369.90 1558.451187.081347.97989.12car0.7990.6880.8701.390 0.8411.3851.2798.4286.085 0.702 1870.42 1754.871790.001932.78 1694.81chair1.6291.2581.4841.783 1.3401.4401.9514.2623.545 0.872 3993.94 3625.233354.003473.62 2961.20display1.5161.2852.1712.370 1.8562.2672.9116.0595.509 0.697 2940.36 3004.442565.073232.06 2533.86lamp3.8583.24812.5286.387 3.4802.4582.6908.5104.308 2.144 7566.25 7162.208038.986958.52 6726.92speaker2.3281.9572.6623.120 2.6169.1995.32411.2719.889 1.075 2054.18 2075.692393.501955.40 1748.26rifle1.0010.7152.0152.052 0.8880.2880.3181.4631.882 0.231 6162.03 6124.896615.206070.86 4741.70couch1.4711.2331.2462.344 1.6452.2533.81710.1798.531 0.869 2387.09 2343.111956.262184.28 1880.21table1.9961.3763.7342.778 1.6431.1221.7163.9003.097 0.515 3598.59 3286.053371.203347.12 2627.82phone1.0480.9751.1832.268 1.383 10.45911.58516.02114.684 1.477 1817.61 1816.221995.981964.46 1555.47vessel1.1790.9661.6912.385 1.5850.7820.88912.3753.253 0.588 4551.17 4430.045066.994494.14 3931.73mean1.4871.1702.5382.361 1.4321.8662.0696.2454.617 0.743 3644.91 3436.143795.233700.22 2939.15"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Single view reconstructioncomparison to the state of the art. Atlas25 denotes AtlasNet with 25 square patches, while Atlas0 uses a single spherical patch. Subscripts to OccNet and IM-NET show sampling resolution. For fair comparisons, we use resolution 32 3 so that OccNet and IM-NET output meshes with comparable number of vertices and faces.", "figure_data": "CD ECDLFD#V#FAtlas01.487 1.866 3644.91744614888Atlas251.170 2.069 3436.1425004050OccNet 322.538 6.245 3795.2315113017OccNet 641.950 6.654 3254.55675613508OccNet 1281.945 6.766 3224.33 2727054538IM-NET 322.361 4.617 3700.2212042404IM-NET 641.467 4.426 2940.56500710009IM-NET 128 1.387 1.971 2810.47 2050441005IM-NET 256 1.371 2.273 2804.77 82965 165929Ours1.432 0.743 2939.1510731910"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Low-poly analysisthe dataset-averaged metrics in single view reconstruction. We highlight the number of vertices #V and triangles #F in the predicted meshes.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "C * j (x) = max i (D i T ij ) < 0 inside > 0 outside. (1", "formula_coordinates": [4.0, 84.22, 395.98, 198.27, 23.3]}, {"formula_id": "formula_1", "formula_text": ")", "formula_coordinates": [4.0, 282.49, 403.1, 3.87, 8.64]}, {"formula_id": "formula_2", "formula_text": "C + j (x) = i relu(D i )T ij = 0 inside > 0 outside.(2)", "formula_coordinates": [4.0, 76.13, 480.8, 210.23, 26.71]}, {"formula_id": "formula_3", "formula_text": "S * (x) = min j (C + j (x)) = 0 inside > 0 outside. (3", "formula_coordinates": [4.0, 83.84, 559.04, 198.65, 23.3]}, {"formula_id": "formula_4", "formula_text": ")", "formula_coordinates": [4.0, 282.49, 566.15, 3.87, 8.64]}, {"formula_id": "formula_5", "formula_text": "S + (x)= \uf8ee \uf8f0 j W j 1 \u2212 C + j (x) [0,1] \uf8f9 \uf8fb [0,1] = 1 \u2248 in [0, 1) \u2248 out,(4)", "formula_coordinates": [4.0, 58.35, 665.1, 228.01, 48.05]}, {"formula_id": "formula_6", "formula_text": "arg min \u03c9,T,W L + rec + L + T + L + W .(5)", "formula_coordinates": [4.0, 370.29, 348.47, 174.82, 18.76]}, {"formula_id": "formula_7", "formula_text": "L + rec = E x\u223cG (S + (x) \u2212 F(x|G)) 2 ,(6)", "formula_coordinates": [4.0, 351.03, 417.31, 194.08, 12.69]}, {"formula_id": "formula_8", "formula_text": "L + T = t\u2208T max(\u2212t, 0) + t\u2208T max(t \u2212 1, 0).(7)", "formula_coordinates": [4.0, 338.09, 528.42, 207.02, 22.3]}, {"formula_id": "formula_9", "formula_text": "L + W = j |W j \u2212 1|.(8)", "formula_coordinates": [4.0, 385.04, 600.8, 160.08, 22.13]}, {"formula_id": "formula_10", "formula_text": "arg min \u03c9 L * recon + L * overlap ,(9)", "formula_coordinates": [5.0, 114.58, 399.26, 171.78, 18.14]}, {"formula_id": "formula_11", "formula_text": "L * recon = E x\u223cG [F(x|G) \u2022 max(S * (x), 0)] (10) +E x\u223cG [(1 \u2212 F(x|G)) \u2022 (1 \u2212 min(S * (x), 1))] . (11)", "formula_coordinates": [5.0, 64.74, 447.92, 221.62, 26.67]}, {"formula_id": "formula_12", "formula_text": "L * overlap = \u2212E x\u223cG E j M (x, j)C + j (x) .(12)", "formula_coordinates": [5.0, 72.54, 566.4, 213.82, 13.15]}], "doi": ""}