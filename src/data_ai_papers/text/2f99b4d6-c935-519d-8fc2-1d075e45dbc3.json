{"title": "Submodular Optimization with Submodular Cover and Submodular Knapsack Constraints", "authors": "Rishabh Iyer; Jeff Bilmes", "pub_date": "2013-11-12", "abstract": "We investigate two new optimization problems -minimizing a submodular function subject to a submodular lower bound constraint (submodular cover) and maximizing a submodular function subject to a submodular upper bound constraint (submodular knapsack). We are motivated by a number of real-world applications in machine learning including sensor placement and data subset selection, which require maximizing a certain submodular function (like coverage or diversity) while simultaneously minimizing another (like cooperative cost). These problems are often posed as minimizing the difference between submodular functions [14,37] which is in the worst case inapproximable. We show, however, that by phrasing these problems as constrained optimization, which is more natural for many applications, we achieve a number of bounded approximation guarantees. We also show that both these problems are closely related and an approximation algorithm solving one can be used to obtain an approximation guarantee for the other. We provide hardness results for both problems thus showing that our approximation factors are tight up to log-factors. Finally, we empirically demonstrate the performance and good scalability properties of our algorithms.", "sections": [{"heading": "Introduction", "text": "A set function f : 2 V \u2192 R is said to be submodular [8] if for all subsets S, T \u2286 V , it holds that f (S) + f (T ) \u2265 f (S \u222a T ) + f (S \u2229 T ). Defining f (j|S) f (S \u222a j) \u2212 f (S) as the gain of j \u2208 V in the context of S \u2286 V , then f is submodular if and only if f (j|S) \u2265 f (j|T ) for all S \u2286 T and j / \u2208 T . The function f is monotone iff f (j|S) \u2265 0, \u2200j / \u2208 S, S \u2286 V . For convenience, we assume the ground set is V = {1, 2, \u2022 \u2022 \u2022 , n}. While general set function optimization is often intractable, many forms of submodular function optimization can be solved near optimally or even optimally in certain cases, and hence submodularity is also often called the discrete analog of convexity [34]. Submodularity, moreover, is inherent in a large class of real-world applications, particularly in machine learning, therefore making them extremely useful in practice. In this paper, we study a new class of discrete optimization problems that have the following form: Problem 1 (SCSC): min{f (X) | g(X) \u2265 c}, and Problem 2 (SCSK): max{g(X) | f (X) \u2264 b}, where f and g are monotone non-decreasing submodular functions that also, w.l.o.g., are normalized (f (\u2205) = g(\u2205) = 0) 1 , and where b and c refer to budget and cover parameters respectively. The corresponding constraints are called the submodular cover [47] and submodular knapsack [1] respectively and hence we refer to Problem 1 as Submodular Cost Submodular Cover (henceforth SCSC) and Problem 2 as Submodular Cost Submodular Knapsack (henceforth SCSK). Our motivation stems from an interesting class of problems that require minimizing a certain submodular function f while simultaneously maximizing another submodular function g. We shall see that these naturally occur in applications like sensor placement, data subset selection, and many other machine learning applications. A standard approach used in literature [14,37,22] has been to transform these problems into minimizing the difference between submodular functions (also called DS optimization): Problem 0: min X\u2286V f (X) \u2212 g(X) .\n(1)\nWhile a number of heuristics are available for solving Problem 0, and while these heuristics often work well in practice [14,37], in the worst-case it is NP-hard and inapproximable [14], even when f and g are monotone.\nAlthough an exact branch and bound algorithm has been provided for this problem [22], its complexity can be exponential in the worst case. On the other hand, in many applications, one of the submodular functions naturally serves as part of a constraint. For example, we might have a budget on a cooperative cost, in which case Problems 1 and 2 become applicable.", "publication_ref": ["b7", "b33", "b46", "b0", "b13", "b36", "b21", "b13", "b36", "b13", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Motivation", "text": "The utility of Problems 1 and 2 become apparent when we consider how they occur in real-world applications and how they subsume a number of important optimization problems.\nSensor Placement and Feature Selection: Often, the problem of choosing sensor locations A from a given set of possible locations V can be modeled [27,14] by maximizing the mutual information between the chosen variables A and the unchosen set V \\A (i.e.,f (A) = I(X A ; X V \\A )). Note that, while the symmetric mutual information is not monotone, it can be shown to approximately monotone [27]. Alternatively, we may wish to maximize the mutual information between a set of chosen sensors X A and a quantity of interest C (i.e., f (A) = I(X A ; C)) assuming that the set of features X A are conditionally independent given C [27,14]. Both these functions are submodular. Since there are costs involved, we want to simultaneously minimize the cost g(A). Often this cost is submodular [27,14]. For example, there is typically a discount when purchasing sensors in bulk (economies of scale). Moreover, there may be diminished cost for placing a sensor in a particular location given placement in certain other locations (e.g., the additional equipment needed to install a sensor in, say, a precarious environment could be re-used for multiple sensor installations in similar environments). Hence this becomes a form of Problem 2 above. An alternate view of this problem is to find a set of sensors with minimal cooperative cost, under a constraint that the sensors cover a certain fraction of the possible locations, naturally expressed as Problem 1.\nData subset selection: A data subset selection problem in speech and NLP involves finding a limited vocabulary which simultaneously has a large coverage. This is particularly useful, for example in speech recognition and machine translation, where the complexity of the algorithm is determined by the vocabulary size. The motivation for this problem is to find the subset of training examples which will facilitate evaluation of prototype systems [33]. This problem then occurs in the form of Problem 1, where we want to find a small vocabulary subset (which is often submodular [33]), subject to a constraint that the subset acoustically spans the entire data set (which is also often submodular [30,31]). This can also be phrased as Problem 2, where we ask for maximizing the acoustic coverage and diversity subject to a bounded vocabulary size constraint. Privacy Preserving Communication: Given a set of random variables X 1 , \u2022 \u2022 \u2022 , X n , denote I as an information source, and P as private information that should be filtered out. Then one way of formulating the problem of choosing a information containing but privacy preserving set of random variables can be posed as instances of Problems 1 and 2, with f (A) = H(X A |I) and g(A) = H(X A |P), where H(\u2022|\u2022) is the conditional entropy. An alternative strategy would be to formulate the problem with f (A) = H(X A ) + H(X A |I) and g(A) = H(X A ) + H(X A |P). Machine Translation: Another application in machine translation is to choose a subset of training data that is optimized for given test data set, a problem previously addressed with modular functions [35]. Defining a submodular function with ground set over the union of training and test sample inputs V = V tr \u222a V te , we can set f : 2 Vtr \u2192 R + to f (X) = f (X|V te ), and take g(X) = |X|, and b \u2248 0 in Problem 2 to address this problem. We call this the Submodular Span problem. Probabilistic Inference: Many problems in computer vision and graphical model inference involve finding an assignment to a set of random variables. The most-probable explanation (MPE) problem finds the assignment that maximizes the probability. In computer vision and high-tree-width Markov random fields, this has been addressed using graph-cut algorithms [2], which are applicable when the MRF's energy function is submodular and limited degree, and more general submodular function minimization in the case of higher degree. Moreover, many useful non-submodular energy functions have also recently been phrased as forms of constrained submodular minimization [20] which still have bounded approximation guarantees. Some of these non-submodular energy functions can be modeled through Problems 1 and 2, where f is still the submodular energy function to be minimized while g represents a submodular constraint. For example, in image co-segmentation [40], V = V 1 \u222a V 2 can represent the set of pixels in two images, f is the submodular energy function of the two images, while g represents the similarity between the two histograms of the hypothesized foreground regions in the two images, a function shown to be submodular in [40] 2 . Apart from the real-world applications above, both Problems 1 and 2 generalize a number of well-studied discrete optimization problems. For example the Submodular Set Cover problem (henceforth SSC) [47] occurs as a special case of Problem 1, with f being modular and g is submodular. Similarly the Submodular Cost Knapsack problem (henceforth SK) [42] is a special case of problem 2 again when f is modular and g submodular. Both these problems subsume the Set Cover and Max k-Cover problems [6]. When both f and g are modular, Problems 1 and 2 are called knapsack problems [23].Furthermore, Problem 1 also subsumes the cardinality constrained submodular minimization problem [43] and more generally the problem of minimizing a submodular function subject to a knapsack constraints. It also subsumes the problem of minimizing a submodular function subject to a matroid span constraint (by setting g as the rank function of the matroid, and c as the rank of the matroid). This in turn subsumes the minimum submodular spanning tree problem [10], when f is monotone submodular.", "publication_ref": ["b26", "b13", "b26", "b26", "b13", "b26", "b13", "b32", "b32", "b29", "b30", "b34", "b1", "b19", "b39", "b39", "b46", "b41", "b5", "b22", "b42", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Our Contributions", "text": "The following are some of our contributions. We show that Problems 1 and 2 are intimately connected, in that any approximation algorithm for either problem can be used to provide guarantees for the other problem as well. We then provide a framework of combinatorial algorithms based on optimizing, sometimes iteratively, subproblems that are easy to solve. These subproblems are obtained by computing either upper or lower bound approximations of the cost functions or constraining functions. We also show that many combinatorial algorithms like the greedy algorithm for SK [42] and SSC [47] (both of which seemingly use different techniques) also belong to this framework and provide the first constant-factor bi-criterion approximation algorithm for SSC [47] and hence the general set cover problem [6]. We then show how with suitable choices of approximate functions, we can obtain a number of bounded approximation guarantees and show the hardness for Problems 1 and 2, which in fact match some of our approximation guarantees up to log-factors. Our guarantees and hardness results depend on the curvature of the submodular functions [3]. We observe a strong asymmetry in the results that the factors change polynomially based on the curvature of f but only by a constant-factor with the curvature of g, hence making the SK and SSC much easier compared to SCSK and SCSC. Finally we empirically evaluate the performance of our algorithms showing that the typical case behavior is much better than these worst case bounds.", "publication_ref": ["b41", "b46", "b46", "b5", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Background and Main Ideas", "text": "We first introduce several key concepts used throughout the paper. Given a submodular function f , we define the total curvature, \u03ba f , and the curvature of f with respect to a set X, \u03ba f (X), as 3 :\n\u03ba f = 1 \u2212 min j\u2208V f (j|V \\j) f (j) , and \u03ba f (X) = 1 \u2212 min{min j\u2208X f (j|X\\j) f (j) , min j / \u2208X f (j|X) f (j) }.(2)\nThe total curvature \u03ba f is then \u03ba f (V ) [3,45]. Intuitively, the curvature 0 \u2264 \u03ba f \u2264 1 measures the distance of f from modularity and \u03ba f = 0 if and only if f is modular (or additive, i.e., f (X) = j\u2208X f (j)). Totally normalized [4] and saturated functions like matroid rank have a curvature \u03ba f = 1. A number of approximation guarantees in the context of submodular optimization have been refined via the curvature of the submodular function [3,18,17]. For example, when maximizing a monotone submodular function under cardinality upper bound constraints, the bound of 1 \u2212 e \u22121 has been refined to 1\u2212e \u2212\u03ba f \u03ba f [3]. Similar bounds have also been shown in the context of constrained submodular minimization [18,17,16]. In this paper, we shall witness the role of curvature also in determining the approximations and the hardness of problems 1 and 2.\nAlgorithm 1 General algorithmic framework to address both Problems 1 and 2\n1: for t = 1, 2, \u2022 \u2022 \u2022 , T do 2:\nChoose surrogate functionsf t and\u011d t for f and g respectively, tight at X t\u22121 .", "publication_ref": ["b2", "b44", "b3", "b2", "b17", "b16", "b2", "b17", "b16", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "3:", "text": "Obtain X t as the optimizer of Problem 1 or 2 withf t and\u011d t instead of f and g.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "4: end for", "text": "The main idea of this paper is a framework of algorithms based on choosing appropriate surrogate functions for f and g to optimize over. This framework is represented in Algorithm 1. We would like to choose surrogate functionsf t and\u011d t such that using them, Problems 1 and 2 become easier. If the algorithm is just single stage (not iterative), we represent the surrogates asf and\u011d. The surrogate functions we consider in this paper are in the forms of bounds (upper or lower) and approximations. Our algorithms using upper and lower bounds are analogous to the majorization/ minimization algorithms proposed in [18,16], where we provided a unified framework of fast algorithms for submodular optimization. We show there in that this framework subsumes a large class of known combinatorial algorithms and also providing a generic recipe for different forms of submodular function optimization. We extend these ideas to the more general context of problems 1 and 2, to obtain a fast family of algorithms. The other type of surrogate functions we consider are those obtained from other approximations of the functions. One such classical approximation is the ellipsoidal approximations [11]. While computing this approximation is time consuming, it turns out to provide the tightest theoretical guarantees. Modular lower bounds: Akin to convex functions, submodular functions have tight modular lower bounds. These bounds are related to the subdifferential \u2202 f (Y ) of the submodular set function f at a set Y \u2286 V , which is defined [8] as:\n\u2202 f (Y ) = {y \u2208 R n : f (X) \u2212 y(X) \u2265 f (Y ) \u2212 y(Y ) for all X \u2286 V } (3)\nFor a vector x \u2208 R V and X \u2286 V , we write x(X)\n= j\u2208X x(j). Denote a subgradient at Y by h Y \u2208 \u2202 f (Y ).\nThe extreme points of \u2202 f (Y ) may be computed via a greedy algorithm: Let \u03c0 be a permutation of V that assigns the elements in Y to the first |Y | positions (\u03c0(i) \u2208 Y if and only if i \u2264 |Y |). Each such permutation defines a chain with elements S \u03c0 0 = \u2205, S \u03c0 i = {\u03c0(1), \u03c0(2), . . . , \u03c0(i)} and S \u03c0 |Y | = Y . This chain defines an\nextreme point h \u03c0 Y of \u2202 f (Y ) with entries h \u03c0 Y (\u03c0(i)) = f (S \u03c0 i ) \u2212 f (S \u03c0 i\u22121 ).(4)\nDefined as above, h \u03c0 Y forms a lower bound of f , tight at Y -i.e., h \u03c0 Y (X) = j\u2208X h \u03c0 Y (j) \u2264 f (X), \u2200X \u2286 V and h \u03c0 Y (Y ) = f (Y ). Modular upper bounds: We can also define superdifferentials \u2202 f (Y ) of a submodular function [20,15] at Y :\n\u2202 f (Y ) = {y \u2208 R n : f (X) \u2212 y(X) \u2264 f (Y ) \u2212 y(Y ); for all X \u2286 V } (5)\nIt is possible, moreover, to provide specific supergradients [15,18,16] that define the following two modular upper bounds (when referring either one, we use m f X ):\nm f X,1 (Y ) f (X) \u2212 j\u2208X\\Y f (j|X\\j) + j\u2208Y \\X f (j|\u2205), m f X,2 (Y ) f (X) \u2212 j\u2208X\\Y f (j|V \\j) + j\u2208Y \\X f (j|X). Then m f X,1 (Y ) \u2265 f (Y ) and m f X,2 (Y ) \u2265 f (Y ), \u2200Y \u2286 V and m f X,1 (X) = m f X,2 (X) = f (X)\n. MM algorithms using upper/lower bounds: Using the modular upper and lower bounds above in Algorithm 1, provide a class of Majorization-Minimization (MM) algorithms, akin to the algorithms proposed in [18,16] for submodular optimization and in [37,14] for DS optimization (Problem 0 above). An appropriate choice of the bounds ensures that the algorithm always improves the objective values for Problems 1 and 2. In particular, choosingf t as a modular upper bound of f tight at X t , or\u011d t as a modular lower bound of g tight at X t , or both, ensures that the objective value of Problems 1 and 2 always improves at every iteration as long as the corresponding surrogate problem can be solved exactly. Unfortunately, Problems 1 and 2 are NP-hard even if f or g (or both) are modular [6], and therefore the surrogate problems themselves cannot be solved exactly. Fortunately, the surrogate problems are often much easier than the original ones and can admit log or constant-factor guarantees. In practice, moreover, these factors are almost 1. In order to guarantee improvement from a theoretical stand-point however, the iterative schemes can be slightly modified using the following trick. Notice that the only case when the true valuation of X t is better than X t+1 is when the surrogate valuation of X t is better than X t+1 (since X t+1 is only near-optimal and not optimal). In such case, we terminate the algorithm at X t . What is also fortunate and perhaps surprising, as we show in this paper below, is that unlike the case of DS optimization (where the problem is inapproximable in general [14]), the constrained forms of optimization (Problems 1 and 2) do have approximation guarantees. Ellipsoidal Approximation: We also consider ellipsoidal approximations (EA) of f . The main result of Goemans et. al [11] is to provide an algorithm based on approximating the submodular polyhedron by an ellipsoid. They show that for any polymatroid function f , one can compute an approximation of the form w f (X) for a certain modular weight vector\nw f \u2208 R V , such that w f (X) \u2264 f (X) \u2264 O( \u221a n log n) w f (X), \u2200X \u2286 V .\nA simple trick then provides a curvature-dependent approximation [17] -we define the \u03ba f -curve-normalized version of f as follows:\nf \u03ba (X) f (X) \u2212 (1 \u2212 \u03ba f ) j\u2208X f (j) \u03ba f .(6)\nThe function f \u03ba essentially contains the zero curvature component of the submodular function f and the modular upper bound j\u2208X f (j) contains all the linearity. The main idea is to then approximate only the polymatroidal part and retain the linear component. This simple trick improves many of the approximation bounds. We then have the following lemma.\nLemma 2.1. [17] Given a polymatroid function f with a curvature \u03ba f < 1, the submodular function f ea (X) = \u03ba f w f \u03ba (X) + (1 \u2212 \u03ba f ) j\u2208X f (j) satisfies:\nf ea (X) \u2264 f (X) \u2264 O \u221a n log n 1 + ( \u221a n log n \u2212 1)(1 \u2212 \u03ba f ) f ea (X), \u2200X \u2286 V (7)\nf ea is multiplicatively bounded by f by a factor depending on \u221a n and the curvature. The dependence on the curvature is evident from the fact that when \u03ba f = 0, we get a bound of O(1), which is not surprising since a modular f is exactly represented as f (X) = j\u2208X f (j). We shall use the result above in providing approximation bounds for Problems 1 and 2. In particular, the surrogate functionsf or\u011d in Algorithm 1 can be the ellipsoidal approximations above, and the multiplicative bounds transform into approximation guarantees for these problems.", "publication_ref": ["b17", "b15", "b10", "b7", "b19", "b14", "b14", "b17", "b15", "b17", "b15", "b36", "b13", "b5", "b13", "b10", "b16", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Relation between SCSC and SCSK", "text": "In this section, we show a precise relationship between Problems 1 and 2. From the formulation of Problems 1 and 2, it is clear that these problems are duals of each other. Indeed, in this section we show that the problems are polynomially transformable into each other. Algorithm 2 Approx. algorithm for SCSK using an approximation algorithm for SCSC using Linear search.\n1: Input: An SCSK instance with budget b, an [\u03c3, \u03c1] approx. algo. for SCSC, & > 0. 2: Output: [(1 \u2212 )\u03c1, \u03c3] approx. for SCSK. 3: c \u2190 g(V ),X c \u2190 V . 4: while f (X c ) > \u03c3b do 5: c \u2190 (1 \u2212 )c 6:X c \u2190 [\u03c3, \u03c1]\napprox. for SCSC using c. 7: end while 8: ReturnX c Algorithm 3 Approx. algorithm for SCSC using an approximation algorithm for SCSK using Linear search.  We first introduce the notion of bicriteria algorithms. An algorithm is a [\u03c3, \u03c1] bi-criterion algorithm for Problem 1 if it is guaranteed to obtain a set X such that f (X) \u2264 \u03c3f (X * ) (approximate optimality) and g(X) \u2265 c = \u03c1c (approximate feasibility), where X * is an optimizer of Problem 1. Similarly, an algorithm is a [\u03c1, \u03c3] bi-criterion algorithm for Problem 2 if it is guaranteed to obtain a set X such that g(X) \u2265 \u03c1g(X * ) and f (X) \u2264 b = \u03c3b, where X * is the optimizer of Problem 2. In a bi-criterion algorithm for Problems 1 and 2, typically \u03c3 \u2265 1 and \u03c1 \u2264 1. We call these type of approximation algorithms, bi-criterion approximation algorithms of type 1. We can also view the bi-criterion approximations from another angle. We can say that for Problem 1,X is a feasible solution (i.e., it satisfies the constraint), and is a [\u03c3, \u03c1] bi-criterion approximation, if f (X) \u2264 \u03c3f (X * ), whereX * is the optimal solution to the problem min{f (X)|g(X) \u2265 c/\u03c1}. Similarly for Problem 2, we can say thatX is a feasible solution (i.e., it satisfies the constraint), and is a [\u03c1, \u03c3] bi-criterion approximation, if g(X) \u2265 \u03c1g(X * ), whereX * is the optimal solution to the problem max{g(X)|f (X) \u2264 b\u03c3}. We call these the bi-criterion approximation algorithms of type 2. It is easy to see that these algorithms can easily be transformed into each other. For example, for problem 1, a bi-criterion algorithm of type-I can obtain a guarantee of type-II if we run it till a covering constraint of c /\u03c1 (where c in this case, is the approximate covering constraint which the type-I algorithm needs to satisfy -note that it need not be the actual covering constraint of the problem). Similarly an algorithm of type-II can obtain a guarantee of type-I if run till a covering constraint of \u03c1c (in this case, c is the actual 'covering' constraint, since a type-II approximate algorithm provides a feasible set). We can similarly transform these guarantees for problem 2. In particular, a bi-criterion algorithm of type-I can be used to obtain a guarantee of type-II if we run it till a budget of b \u03c3 (again, here b is the approximate budget of the type-I algorithm). Similarly an algorithm of type-II can obtain a guarantee of type-I if run till a covering constraint of b/\u03c3 (in this case, b is the budget of the original problem). Though both type-I and type-II guarantees are easily transformable into each other, through the rest of this paper whenever we refer to bi-criterion approximations, we shall consider only the type-I approximations. A non-bicriterion algorithm for Problem 1 is when \u03c1 = 1 and a non-bicriterion algorithm for Problem 2 is when \u03c3 = 1. Algorithms 2 and 3 provide the schematics for using an approximation algorithm for one of the problems for solving the other. The main idea of Algorithm 2 is to start with the ground set V and reduce the value of c (which governs SCSC), until the valuation of f just falls below \u03c3b. At that point, we are guaranteed to get a ((1 \u2212 )\u03c1, \u03c3) solution for SCSK. Similarly in Algorithm 3, we increase the value of b starting at the empty set, until the valuation at g falls above \u03c1c. At this point we are guaranteed a ((1 + )\u03c3, \u03c1) solution for SCSC. In order to avoid degeneracies, we assume that f (V ) \u2265 b \u2265 min j f (j) and g(V ) \u2265 c \u2265 min j g(j), else the solution to the problem is trivial.\n1: Input: An SCSC instance with cover c, an [\u03c1, \u03c3] approx. algo. for SCSK, & > 0. 2: Output: [(1 + )\u03c3, \u03c1] approx. for SCSC. 3: b \u2190 argmin j f (j),X b \u2190 \u2205. 4: while g(X b ) < \u03c1c do 5: b \u2190 (1 + )b 6:X b \u2190 [\u03c1, \u03c3] approx.\nTheorem 3.1. Algorithm 2 is guaranteed to find a setX c which is a [(1 \u2212 )\u03c1, \u03c3] approximation of SCSK in at most log 1/(1\u2212 ) [g(V )/ min j g(j)] calls to the [\u03c3, \u03c1] approximate algorithm for SCSC. Similarly, Algorithm 3 is guaranteed to find a setX b which is a [(1 + )\u03c3, \u03c1] approximation of SCSC in log 1+ [f (V )/ min j f (j)] calls to a [\u03c1, \u03c3] approximate algorithm for SCSK.\nProof. We start by proving the first part, for Algorithm 2. Notice that Algorithm 2 converges when f (X c ) just falls below \u03c3b. Hence f (X c ) \u2264 \u03c3b (is approximately feasible) and at the previous step c = c/(1 \u2212 ), we have that f (X c ) > \u03c3b. Denoting X * c as the optimal solution for SCSC at c , we have that f (X * c ) > b (a fact which follows from the observation thatX c is a [\u03c3, \u03c1] approximation of SCSC at c). Hence if X * is the optimal solution of SCSK, it follows that g(X * ) < c . The reason for this is that, suppose, g(X * ) \u2265 c . Then it follows that X * is a feasible solution for SCSC at c and hence f (X * ) \u2265 f (X * c ) > b. This contradicts the fact that X * is an optimal solution for SCSK (since it is then not even feasible). Next, notice thatX c satisfies that g(X c ) \u2265 \u03c1c, using the fact thatX c is obtained from a (\u03c3, \u03c1) bi-criterion algorithm for SCSC. Hence,\ng(X c ) \u2265 \u03c1c = \u03c1(1 \u2212 )c > \u03c1(1 \u2212 )g(X * ) (8)\nHence the Algorithm 2 is a ((1 \u2212 )\u03c1, \u03c3) approximation for SCSK. In order to show the converge rate, notice that c \u2265 min j g(j) > 0. Since \u03c3 \u2265 1 and b \u2265 min j f (j), we can guarantee that this algorithm will stop before c reaches min j g(j). The reason is that, when c = min j g(j), the minimum value of f (X) such that g(X) \u2265 c is min j f (j), which is smaller than b. Moreover, since \u03c3 > 1, it implies that the algorithm would have terminated before this point.\nThe proof for the second part of the statement, for Algorithm 3, is omitted since it is shown using a symmetric argument.\nAlgorithm 4 Approx. algorithm for SCSK using an approximation algorithm for SCSC using Binary search.   end if 12: end while 13: ReturnX cmin Algorithm 5 Approx. algorithm for SCSC using an approximation algorithm for SCSK using Binary search.    . Moreover, when f and g are integral, the analysis is much simpler. In particular, notice that once c max \u2212 c min = 1, the algorithm will stop at the next iteration (this is because at this point, c = (c max + c min )/2 is equivalent to c = c max ). Hence, the number of iterations is bounded by log 2 g(V ), and we can exactly obtain a [\u03c1, \u03c3] bi-criterion approximation algorithm.\nOutput: [(1 \u2212 )\u03c1, \u03c3] approx. for SCSK. 3: c min \u2190 min j g(j), c max \u2190 g(V ) 4: while c max \u2212 c min \u2265 c max do 5: c \u2190 [c max + c min ]/2 6:X c \u2190 [\u03c3, \u03c1] approx. for SCSC using c. 7: if f (X c ) > \u03c3b\nb min \u2190 argmin j f (j), b max \u2190 f (V ). 4: while b max \u2212 b min \u2265 b min do 5: b \u2190 [b max + b min ]/2 6:X b \u2190 [\u03c1, \u03c3] approx. for SCSK using b 7: if g(X b ) < \u03c1c\nThe proof for the second part of the statement, for Algorithm 3, similarly follows using a symmetric argument.\nWhen f and g are integral, this removes the dependence on the factors and could potentially be much faster in practice. We also remark that a specific instance of such a transformation has been used [26], for a specific class of functions f and g. We shall show in section 4.3 that their algorithm is in fact a special case of Algorithm 4 through a specific construction to convert the non-submodular problem into an instance of a submodular one. Algorithm 2 and Algorithm 3 indeed provide an interesting theoretical result connecting the complexity of Problems 1 and 2. In the next section however, we provide distinct algorithms for each problem when f and g are polymatroid functions -this turns out to be faster than having to resort to the iterative reductions above.", "publication_ref": ["b25"], "figure_ref": [], "table_ref": []}, {"heading": "Approximation Algorithms", "text": "We consider several algorithms for Problems 1 and 2, which can all be characterized by the framework of Algorithm 1, using the surrogate functions of the form of upper/lower bounds or approximations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Approximation Algorithms for SCSC", "text": "We first describe our approximation algorithms designed specifically for SCSC, leaving to \u00a74.2 the presentation of our algorithms slated for SCSK. We first investigate a special case, the submodular set cover (SSC), and then provide two algorithms, one of them (ISSC) is very practical with a weaker theoretical guarantee, and another one (EASSC) which is slow but has the tightest guarantee. Submodular Set Cover (SSC): We start by considering a classical special case of SCSC (Problem 1) where f is already a modular function and g is a submodular function. This problem occurs naturally in a number of problems related to active/online learning [12] and summarization [31,32]. This problem was first investigated by Wolsey [47], wherein he showed that a simple greedy algorithm achieves bounded (in fact, log-factor) approximation guarantees. We show that this greedy algorithm can naturally be viewed in the framework of our Algorithm 1 by choosing appropriate surrogate functionsf t and\u011d t . The idea is to use the modular function f as its own surrogatef t and choose the function\u011d t as a modular lower bound of g. Akin to the framework of algorithms in [18], the crucial factor is the choice of the lower bound (or subgradient). Define the greedy subgradient (equivalently the greedy permutation) as:\n\u03c0(i) \u2208 argmin f (j) g(j|S \u03c0 i\u22121 ) j / \u2208 S \u03c0 i\u22121 , g(S \u03c0 i\u22121 \u222a j) < c .(9)\nOnce we reach an i where the constraint g(S \u03c0 i\u22121 \u222a j) < c can no longer be satisfied by any j / \u2208 S \u03c0 i\u22121 , we choose the remaining elements for \u03c0 arbitrarily. Let the corresponding subgradient be referred to as h \u03c0 . Let N be the minimum i such that g(S \u03c0 i ) \u2265 c and \u03b8 i = min j /\n\u2208S \u03c0 i\u22121 f (j) g(j|S \u03c0 i\u22121 )\n. Then we have the following lemma, which is an extension of [47]:\nLemma 4.1. Choosing the surrogate functionf as f and\u011d as h \u03c0 (with \u03c0 defined in Eqn. (9)) in Algorithm 1, at the end of the first iteration, we are guaranteed to obtain a set X g such that\nf (X g ) f (X * ) \u2264 1 + log e min{\u03bb 1 , \u03bb 2 , \u03bb 3 } (10\n)\nwhere\n\u03bb 1 = max{ 1 1\u2212\u03bag(S \u03c0 i ) | i : c(S \u03c0 i ) < 1}, \u03bb 2 = \u03b8 N \u03b81 and \u03bb 3 = g(V )\u2212g(\u2205) g(V )\u2212g(S \u03c0 N \u22121 )\n. Furthermore if g is integral,\nf (X g ) f (X * ) \u2264 H(max j g(j)), where H(d) = d i=1 1 i for a positive integer d.\nProof. The permutation \u03c0 is chosen based on a greedy ordering associated with the submodular set cover problem, and therefore h \u03c0 (S \u03c0 N ) = g(S \u03c0 N ) \u2265 c (where the inequality follows from the definition of N ), and thus S \u03c0 N is a feasible solution in the surrogate problem (where g is replaced by h \u03c0 ). The resulting knapsack problem can be addressed using the greedy algorithm [23], but this exactly corresponds to the greedy algorithm of submodular set cover [47] and hence the guarantee follows from Theorem 1 in [47].\nThe surrogate problem in this instance is a simple knapsack problem that can be solved nearly optimally using dynamic programming [44]. As stated in the proof, the greedy algorithm for the submodular set cover problem [47] is in fact equivalent to using the greedy algorithm for the knapsack problem [23], which is in fact suboptimal. When g is integral, the guarantee of the greedy algorithm is H g H(max j g(j)), where [47] (henceforth we will use H g for this quantity). This factor is tight up to lower-order terms [6]. Furthermore, since this algorithm directly solves SSC, we call it the primal greedy. We could also solve SSC by looking at its dual, which is SK [42]. Although SSC does not admit any constant-factor approximation algorithms [6], we can obtain a constant-factor bi-criterion guarantee: Lemma 4.2. Using the greedy algorithm for SK [42] as the approximation oracle in Algorithm 3 provides a [1 + , 1 \u2212 e \u22121 ] bi-criterion approximation algorithm for SSC, for any > 0.\nH(d) = d i=1 1 i\nProof. We call this the dual greedy. This result follows immediately from the guarantee of the submodular cost knapsack problem [42] and Theorem 3.1.\nWe remark that we can also use a simpler version of the greedy iteration at every iteration [31,24] and we obtain a guarantee of (1 + , 1/2(1 \u2212 e \u22121 )). In practice, however, both these factors are almost 1 and hence the simple variant of the greedy algorithm suffices. An interesting connection between the greedy algorithm and the induced orderings, allows us to further simplify this dual algorithm. A nice property of the greedy algorithm for the submodular knapsack problem is that it can be completely parameterized by the chain of sets (this holds for the greedy algorithm of [31,24] for knapsack constraints and the basic greedy algorithm of [38] under cardinality constraints). In particular, having computed the greedy chain of sets, and given a value of b or the budget, we can easily find the corresponding set in O(log n) time using binary search. Moreover, this also implies that we can do the transformation algorithms by just iterating through the chain of sets once. In particular, the linear search over the different values of is equivalent to the linear search over the different chain of sets. Moreover, we could also do the much faster binary search. Hence the complexity of the dual greedy algorithm is almost identical to the primal greedy one for the submodular set cover problem. It is also important to put the bicriterion result into perspective. Notice that the bicriterion guarantee suggests that we find only an approximate feasible solution. In particular, the dual greedy algorithm provides a type-I bi-criterion approximation, -that the solution obtained by running the algorithm with a cover constraint of (1 \u2212 1/e)c is competitive to the optimal solution with a cover constraint of c. However, we can also obtain a type-II bi-criterion approximation by running the dual greedy algorithm until it satisfies the cover constraint of c. In this case, we would obtain a feasible solution. The guarantee, however, would say that the resulting solution would be competitive to the optimal solution obtained with a cover constraint of c/(1 \u2212 e \u22121 ). Furthermore, these factors are in practice close to 1, and the primal and dual greedy algorithms would both perform very well empirically. Iterated Submodular Set Cover (ISSC): We next investigate an algorithm for the general SCSC problem when both f and g are submodular. The idea here is to iteratively solve the submodular set cover problem which can be done by replacing f by a modular upper bound at every iteration. In particular, this can be seen as a variant of Algorithm 1, where we start with X 0 = \u2205 and choosef t (X) = m f X t ,2 (X) at every iteration (alternatively, we can choosef t (X) = m f X t ,1 (X)). At the first iteration with X 0 = \u2205, either variant then corresponds to the set cover problem with the simple modular upper bound f (X) \u2264 m f \u2205 (X) = j\u2208X f (j)\nwhere m f X t refers to either variant. The surrogate problem at each iteration becomes: minimize m f X t (X) subject to g(X) \u2265 c.\nHence, each iteration is an instance of SSC and can be solved nearly optimally using the greedy algorithm. We can continue this algorithm for T iterations or until convergence. An analysis very similar to the ones in [14,18] will reveal polynomial time convergence. Since each iteration is only the greedy algorithm, this approach is also highly practical and scalable. Since there are two approaches to solve the set cover problem (the primal approach of Lemma 4.1 and the dual greedy approach of Lemma 4.2), we have two forms of ISSC, the primal ISSC and the dual ISSC. The following shows the resulting theoretical guarantees: Theorem 4.3. The primal ISSC algorithm obtains an approximation factor of\nKgHg 1+(Kg\u22121)(1\u2212\u03ba f ) \u2264 n 1+(n\u22121)(1\u2212\u03ba f ) H g\nwhere K g = 1 + max{|X| : g(X) < c} and H g is the approximation factor of the submodular set cover using g. Similarly the dual ISSC obtains a bi-criterion guarantee of\n(1+ )Kg 1+(Kg\u22121)(1\u2212\u03ba f ) , 1 \u2212 e \u22121 .\nProof. The first part of the result follows directly from Theorem 5.4 in [18]. In particular, the result from [18] ensures a guarantee of\n\u03b2|X * | 1 + (|X * | \u2212 1)(1 \u2212 \u03ba f ) (11)\nfor the problem of min{f (X)|X \u2208 C} where \u03b2 is the approximation guarantee of solving a modular function over C where C is the feasible set. In this case, \u03b2 = H g and |X * | \u2264 K g . When using the dual greedy approach at every iteration, we can use a similar form of the result in the bi-criterion sense. Consider only the first iteration of this algorithm (due to the monotonicity of the algorithm, we will only improve the objective value). We are then guaranteed to obtain a setX such that (denote X 1 as the solution after the first iteration)\nf (X) \u2264 f (X 1 ) \u2264 m f \u2205 (X 1 ) \u2264 (1 + )m f \u2205 (X * ) \u2264 K g (1 + ) 1 + (K g \u2212 1)(1 \u2212 \u03ba f ) f (X * ) (12\n)\nThe inequalities above follow from the fact that the modular upper bound m f \u2205 (X) satisfies [17],\nm f \u2205 (X) \u2264 f (X) \u2264 |X| 1 + (|X| \u2212 1)(1 \u2212 \u03ba f ) f (X)(13)\nand the fact that X 1 which is the solution obtained through the dual set cover with a cover constraint (1 \u2212 e \u22121 )c satisfies m f \u2205 (X 1 ) \u2264 (1 + )m f \u2205 (X * ). In the above, X * is the optimal solution to the problem min{f (X)|g(X) \u2265 c}. We could also run the dual set cover algorithm to obtain a feasible solution (i.e., a type-II guarantee). In this case, the guarantee would compete with the optimal solution satisfying a cover constraint of c/(1 \u2212 e \u22121 ).\nFrom the above, it is clear that K g \u2264 n. Notice also that H g is essentially a log-factor. We also see an interesting effect of the curvature \u03ba f of f . When f is modular (\u03ba f = 0), we recover the approximation guarantee of the submodular set cover problem. Similarly, when f has restricted curvature, the guarantees can be much better. For example, using square-root over modular function f (X) = k i=1 w i (X), which is common model used in applications [14,29,19], the worst case guarantee is H g K g . This follows directly from the results in [17]. Moreover, the approximation guarantee already holds after the first iteration, so additional iterations can only further improve the objective.\nWe remark here that a special case of ISSC, using only the first iteration (i.e., the simple modular upper bound of f ) was considered in [46,5]. Our algorithm not only possibly improves upon theirs, but our approximation guarantee is also more explicit than theirs. In particular, they show a guarantee of \u03bd f H g , where \u03bd f = min{ i\u2208X f (i) f (X) |g(X) = g(V )}. Since this factor \u03bd f itself involves an optimization problem, it is not clear how to efficiently compute this factor. Moreover given a submodular function f , it is also not evident how good this factor is. While our guarantee is an upper bound of \u03bd f H g , it is much more explicit in its dependence on the parameters of the problem. It can also be computed efficiently and has an intuitive significance related to the curvature of the function. Furthermore, our bound is also tight since with, for example, f (X) = min{|X|, 1}, our exactly matches the bound of [46,5]. Lastly our algorithm also potentially improves upon theirs thanks to its iterative nature. For another variant of this algorithm, we can replace g with its greedy modular lower bound at every iteration. Then, rather than solving every iteration through the greedy algorithm, we can solve every iteration as a knapsack problem (minimizing a modular function over a modular lower bound constraint) [23], using say, a dynamic programming based approach. This could potentially improve over the greedy variant, but at a potentially higher computational cost. Ellipsoidal Approximation based Submodular Set Cover (EASSC): In this setting, we use the ellipsoidal approximation discussed in \u00a72. We can compute the \u03ba f -curve-normalized version of f (f \u03ba , see \u00a72), and then compute its ellipsoidal approximation \u221a w f \u03ba . We then define the functionf (X) = f ea (X) = \u03ba f w f \u03ba (X) + (1 \u2212 \u03ba f ) j\u2208X f (j) and use this as the surrogate functionf for f . We choose\u011d as g itself. The surrogate problem becomes:\nmin \uf8f1 \uf8f2 \uf8f3 \u03ba f w f \u03ba (X) + (1 \u2212 \u03ba f ) j\u2208X f (j) g(X) \u2265 c \uf8fc \uf8fd \uf8fe . (14\n)\nWhile functionf (X) = f ea (X) is not modular, it is a weighted sum of a concave over modular function and a modular function. Fortunately, we can use the result from [39], where they show that any function of the form of w 1 (X) + w 2 (X) can be optimized over any polytope P with an approximation factor of \u03b2(1 + ) for any > 0, where \u03b2 is the approximation factor of optimizing a modular function over P. The complexity of this algorithm is polynomial in n and 1 . The main idea of their paper is to reduce the problem of minimizing w 1 (X) + w 2 (X), into log n problems of minimizing a modular function over the polytope. We use their algorithm to minimize f ea (X) over the submodular set cover constraint and hence we call this algorithm EASSC. Again we have the two variants, primal EASSC and dual EASSC, which essentially use at every iteration the primal and dual forms of set cover. \n\u221a n log nHg 1+( \u221a n log n\u22121)(1\u2212\u03ba f ) )\n, where H g is the approximation guarantee of the set cover problem. Moreover, the dual EASSC obtains a bi-criterion approximation of O(\n\u221a n log n 1+( \u221a n log n\u22121)(1\u2212\u03ba f ) ), 1 \u2212 e \u22121 .\nProof. The idea of the proof is to use the result from [39] where they show that any function of the form \u03bb 1 m 1 (X) + \u03bb 2 m 2 (X) where \u03bb 1 \u2265 0, \u03bb 2 \u2265 0 and m 1 and m 2 are positive modular functions has a FPTAS, provided a modular function can easily be optimized over C. Note that our function is exactly of that form. Hence,f (X) can be approximately optimized over C. It now remains to show that this translates into the approximation guarantee. From Lemma 2.1, we know that there exists af such that f (X) \u2264 f (X) \u2264 \u03b2(n)f (X), \u2200X where\n\u03b2(n) = O \u221a n log n ( \u221a n log n \u2212 1)(1 \u2212 \u03ba f ) + 1) . (15\n)\nThen, ifX is the 1 + approximately optimal solution for minimizingf over {X : g(X) \u2265 c}, we have that:\nf (X) \u2264 \u03b2(n)f (X) \u2264 H g \u03b2(n)(1 + )f (X * ) \u2264 H g \u03b2(n)(1 + )f (X * ),(16)\nwhere X * is the optimal solution. We can set to any constant, say 1, and we get the result. The dual guarantee again follows in a very similar manner thanks to the guarantee for the dual SSC.\nIf the function f has \u03ba f = 1, we can use a much simpler algorithm. In particular, since the ellipsoidal approximation is of the form of f ea (X) = w f (X), we can minimize (f ea (X)) 2 = w f (X) at every iteration, giving a surrogate problem of the form\nmin{w f (X)|g(X) \u2265 c}. (17\n)\nThis is directly an instance of SSC, and in contrast to EASSC, we just need to solve SSC once. We call this algorithm EASSCc. This guarantee is tight up to log factors when \u03ba f = 1. Proof. LetX be a set such that,\nw(X) \u2265 H g min{w(X)|g(X) \u2265 c} (18\n)\nThen denote \u03b1(n) = O( \u221a n log n). f (X) \u2264 \u03b1(n) w(X) \u2264 H g \u03b1(n) w(X * ) \u2264 H g \u03b1(n)f (X * )(19)\nIn the above, X * = argmin{f (X)|g(X) \u2265 c}.\nThe result for the dual variant can also be similarly shown.", "publication_ref": ["b11", "b30", "b31", "b46", "b17", "b46", "b22", "b46", "b46", "b43", "b46", "b22", "b46", "b5", "b41", "b5", "b41", "b41", "b30", "b23", "b30", "b23", "b37", "b13", "b17", "b17", "b17", "b16", "b13", "b28", "b18", "b16", "b45", "b4", "b45", "b4", "b22", "b38", "b38"], "figure_ref": [], "table_ref": []}, {"heading": "Approximation Algorithms for SCSK", "text": "In this section, we describe our approximation algorithms for SCSK. We note the dual nature of the algorithms in this current section to those given in \u00a74.1. We first investigate a special case, the submodular knapsack (SK), and then provide three algorithms, two of them (Gr and ISK) being practical with slightly weaker theoretical guarantee, and another one (EASK) which is not scalable but has the tightest guarantee. Submodular Cost Knapsack (SK): We start with a special case of SCSK (Problem 2), where f is a modular function and g is a submodular function. In this case, SCSK turns into the SK problem for which the greedy algorithm with partial enumeration provides a 1 \u2212 e \u22121 approximation [42]. The greedy algorithm can be seen as an instance of Algorithm 1 with\u011d being the modular lower bound of g andf being f , which is already modular. In particular, we then get back the framework of [18], where the authors show that choosing a permutation based on a greedy ordering, exactly analogous to Eqn. ( 9), provides the bounds. In particular, define:\n\u03c0(i) \u2208 argmax g(j|S \u03c0 i\u22121 ) f (j) j / \u2208 S \u03c0 i\u22121 , f (S \u03c0 i\u22121 \u222a {j}) \u2264 b ,(20)\nwhere the remaining elements are chosen arbitrarily. A slight catch however is that for the analysis to work, [42] needs to consider n 3 instances of such orderings (partial enumeration), chosen by fixing the first three elements in the permutation [18]. We can however just choose the simple greedy ordering in one stage, to get a slightly worse approximation factor of 1 \u2212 e \u22121/2 [18,31,24]. Lemma 4.6. [18] Choosing the surrogate functionf as f and\u011d as h \u03c0 in Algorithm 1 yields a set X g :\nProblem Algorithm Approx. factor * Hardness * Bi-Criterion factor # (Bi-Criterion \u03c3 \u03c1 ) # SSC Primal Greedy H(maxj f (j)) * log n * Dual Greedy (1 + , 1 \u2212 1 e ) # 1 \u2212 1/e # SCSC Primal ISSC nHg 1+(n\u22121)(1\u2212\u03ba f ) * \u2126( \u221a n 1+( \u221a n\u22121)(1\u2212\u03ba f ) ) * ,# Dual ISSC [ (1+ )n 1+(n\u22121)(1\u2212\u03ba f ) , 1 \u2212 1/e] # Primal EASSC O( \u221a n log nHg 1+( \u221a n log n\u22121)(1\u2212\u03ba f ) ) * Dual EASSC [O( \u221a n log n 1+( \u221a n log n\u22121)(1\u2212\u03ba f ) , 1 \u2212 1/e] # Primal EASSCc O( \u221a n log n Hg) * Dual EASSCc [O( \u221a n log n), 1 \u2212 1/e] # SK Greedy 1 \u2212 1/e * 1 \u2212 1/e * SCSK Greedy 1/n * \u2126( \u221a n 1+( \u221a n\u22121)(1\u2212\u03ba f ) ) * ,# ISK [1 \u2212 e \u22121 , K f 1+(K f \u22121)(1\u2212\u03ba f ) ] # Primal EASK [1 + , O( \u221a n log nHg 1+( \u221a n log n\u22121)(1\u2212\u03ba f ) )] # Dual EASK [(1 \u2212 1/e)(1 + ), O( \u221a n log n 1+( \u221a n log n\u22121)(1\u2212\u03ba f ) )] # EASKc [1 \u2212 1/e, O( \u221a n log n)] #\nmax{ max i:f (i)\u2264b g(i), g(X g )} \u2265 1/2(1 \u2212 1/e)g(X * ).(21)\nLet \u03c0 ijk be a permutation with i, j, k in the first three positions, and the remaining arrangement greedy.\nRunning O(n 3 ) restarts of one iteration of Algorithm 1 yields sets X ijk with\nmax i,j,k\u2208V f (X ijk ) \u2265 (1 \u2212 1/e)f (X * ).(22)\nGreedy (Gr): A similar greedy algorithm can provide approximation guarantees for the general SCSK problem, with submodular f and g. Unlike the knapsack case in (20), however, at iteration i we choose an element j /\n\u2208 S i\u22121 : f (S \u03c0 i\u22121 \u222a {j}) \u2264 b which maximizes g(j|S i\u22121 )\n. In terms of Algorithm 1, this is analogous to choosing a permutation, \u03c0 such that:\n\u03c0(i) \u2208 argmax{g(j|S \u03c0 i\u22121 )|j / \u2208 S \u03c0 i\u22121 , f (S \u03c0 i\u22121 \u222a {j}) \u2264 b}.(23)\nTheorem 4.7. The greedy algorithm for SCSK obtains an approx. factor of 1 \u03bag (1 \u2212 (\nK f \u2212\u03bag K f ) k f ) \u2265 1 K f , where K f = max{|X| : f (X) \u2264 b} and k f = min{|X| : f (X) \u2264 b & \u2200j \u2208 X, f (X \u222a j) > b}.\nProof. The proof of this result follows directly from [3,18]. In particular, it holds for any down monotone constraint. It is easy to see that the constraint {f (X) \u2264 b} is down-monotone when f is a monotone submodular function.\nIn the worst case, k f = 1 and K f = n, in which case the guarantee is 1/n. The bound above follows from a simple observation that the constraint {f (X) \u2264 b} is down-monotone for a monotone function f . However, in this variant, we do not use any specific information about f . In particular it holds for maximizing a submodular function g over any down monotone constraint [3]. Hence it is conceivable that an algorithm that uses both f and g to choose the next element could provide better bounds. We do not, however, currently have the analysis for this. Iterated Submodular Cost Knapsack (ISK): Here, we choosef t (X) as a modular upper bound of f , tight at X t . Let\u011d t = g. Then at every iteration, we solve:\nmax{g(X)|m f X t (X) \u2264 b},(24)\nwhich is a submodular maximization problem subject to a knapsack constraint (SK). As mentioned above, greedy can solve this nearly optimally. We start with X 0 = \u2205, choosef 0 (X) = j\u2208X f (j) and then iteratively continue this process until convergence (note that this is an ascent algorithm). We have the following theoretical guarantee:\nTheorem 4.8. Algorithm ISK obtains a set X t such that g(X t ) \u2265 (1 \u2212 e \u22121 )g(X), whereX is the optimal solution of max g(X\n) | f (X) \u2264 b(1+(K f \u22121)(1\u2212\u03ba f ) K f\nand where K f = max{|X| : f (X) \u2264 b}.\nProof. We are given thatX is the optimal solution to the problem:\nmax g(X) | f (X) \u2264 b(1 + (K f \u2212 1)(1 \u2212 \u03ba f ) K f(25)\nX is also a feasible solution to the problem:\nmax \uf8f1 \uf8f2 \uf8f3 g(X) | j\u2208X f (j) \u2264 b \uf8fc \uf8fd \uf8fe (26)\nThe reason for this is that:\nj\u2208X f (j) \u2264 K f 1 + (K f \u2212 1)(1 \u2212 \u03ba f ) f (X) \u2264 K f 1 + (K f \u2212 1)(1 \u2212 \u03ba f ) b(1 + (K f \u2212 1)(1 \u2212 \u03ba f )) K f \u2264 b\nNow, at the first iteration we are guaranteed to find a set X 1 such that g(X 1 ) \u2265 (1 \u2212 1/e)g(X). The further iterations will only improve the objective since this is a ascent algorithm.\nIt is worth pointing out that the above bound holds even after the first iteration of the algorithm. It is interesting to note the similarity between this approach and the iterated submodular set cover ISSC. Notice that the bound above is a form of a bicriterion approximation factor of type-II. In particular, we obtain a feasible solution at the end of the algorithm. We can obtain a type-I bicriterion approximation bound, by running this for a larger budget constraint. In particular, we run the above algorithm with a budget constraint of\nbK f 1+(K f \u22121)(1\u2212\u03ba f ) instead of b.\nThe following guarantee then follows.\nLemma 4.9. The ISK algorithm of type-I is guaranteed to obtain a set X which has a bicriterion approximation factor of [1 \u2212 e \u22121 ,\nK f 1+(K f \u22121)(1\u2212\u03ba f ) ], where K f = max{|X| : f (X) \u2264 b}.\nProof. This result directly follows from the Lemma above, and the transformation from a type-II approximation algorithm to a type-I one.\nEllipsoidal Approximation based Submodular Cost Knapsack (EASK): Choosing the Ellipsoidal Approximation f ea of f as a surrogate function, we obtain a simpler problem:\nmax \uf8f1 \uf8f2 \uf8f3 g(X) \u03ba f w f \u03ba (X) + (1 \u2212 \u03ba f ) j\u2208X f (j) \u2264 b \uf8fc \uf8fd \uf8fe .(27)\nIn order to solve this problem, we look at its dual problem (i.e., Eqn. ( 14)) and use Algorithm 2 to convert the guarantees. Recall that Eqn. ( 14) itself admits two variants of the ellipsoidal approximation, which we had referred to as the primal EASSC and the dual EASSC. Hence, we call the combined algorithms, primal and dual EASK, which admit the following guarantees:\nLemma 4.10. The primal EASK obtains a guarantee of 1 + , O(\n\u221a n log nHg 1+( \u221a n log n\u22121)(1\u2212\u03ba f ) )\n. Similarly, the dual EASK obtains a guarantee of\n(1 + )(1 \u2212 1/e), O( \u221a n log n 1+( \u221a n log n\u22121)(1\u2212\u03ba f ) ) .\nProof. This Lemma directly follows from Theorem 3.1 and Theorem 4.4.\nThese factors are akin to those of Theorem 4.4, except for the additional factor of 1 + . Unlike the greedy algorithm, ISSC and ISK, note that both EASK and EASSC are enormously costly and complicated algorithms. Hence, it also seems at first thought that EASK would need to run multiple versions of EASSC at each conversion round of Algorithm 2. Fortunately, however, we need to compute the Ellipsoidal Approximation just once and the algorithm can reuse it for different values of c. Furthermore, since construction of the approximation is often the bottleneck, this scheme is likely to be as costly as EASSC in practice.\nIn the case when the submodular function has a curvature \u03ba f = 1, we can actually provide a simpler algorithm without needing to use the conversion algorithm (Algorithm 2). In this case, we can directly choose the ellipsoidal approximation of f as w f (X) and solve the surrogate problem:\nmax{g(X) : w f (X) \u2264 b 2 }.(28)\nThis surrogate problem is a submodular cost knapsack problem, which we can solve using the greedy algorithm. We call this algorithm EASKc. This guarantee is tight up to log factors if \u03ba f = 1. Proof. LetX be the approximate optimizer of Eqn. (28). First we show that g(X) \u2265 (1 \u2212 1/e)g(X * ) where X * is the optimizer of problem 2. Notice that f (X * ) \u2264 b since it feasible in problem 2. Also, w f (X * ) \u2264 f (X * ) \u2264 b and hence X * is feasible in Eqn. (28). Hence it holds that g(X) \u2265 (1 \u2212 1/e)g(X * ). We now show that f (X) \u2264 b \u221a n log n. Notice that,\nf (X) \u2264 \u221a n log n w f (X) \u2264 b \u221a n log n (29)", "publication_ref": ["b41", "b17", "b41", "b17", "b17", "b30", "b23", "b17", "b19", "b2", "b17", "b2", "b27", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "Extensions beyond SCSC and SCSK", "text": "SCSC is in fact more general and can be extended to more flexible and complicated constraints which can arise naturally in many applications [26,13]. Notice first that {g(X) \u2265 \u03b1} \u21d4 {g (X) = g (V )} (30) where g (X) = min{g(X), \u03b1}. We can also have \"and\" constraints as g 1 (X) = g 1 (V ) and g 2 (X) = g 2 (V ). These have a simple equivalence:\n{g 1 (X) = g 1 (V ) \u2227 g 2 (X) = g 2 (V )} \u21d4 {g(X) = g(V )} (31)\nwhen g(X) = g 1 (X) + g 2 (X) [26]. Moreover, we can also handle k 'and' constraints, by defining g(X) = k i=1 g i (X). Similarly we can have \"or\" constraints, i.e., g 1 (X) = g 1 (V ) or g 2 (X) = g 2 (V ). These also have a nice equivalence:\n{g 1 (X) = g 1 (V ) \u2228 g 2 (X) = g 2 (V )} \u21d4 {g(X) = g(V )} (32)\nby defining g(X) = g 1 (X)g 2 (V ) + g 2 (X)g 1 (V ) \u2212 g 1 (X)g 2 (X) [13]. We can also extend these recursively to multiple 'or' constraints. Hence our algorithms can directly solve all these variants of SCSC. SCSK can also be extended to handle more complicated forms of functions g. In particular, consider the function g(X) = min{g 1 (X), g 2 (X), . . . , g k (X)} ( 33)\nwhere the functions g 1 , g 2 , . . . , g k are submodular. Although g(X) in this case is not submodular, this scenario occurs naturally in many applications, particularly sensor placement [26]. The problem in [26] is in fact a special case of Problem 2, using a modular function f . Often, however, the budget functions involve a cooperative cost, in which case f is submodular. Using Algorithm 2, however, we can easily solve this by iteratively solving the dual problem. Notice that the dual problem is in the form of Problem 1 with a non-submodular constraint g(X) \u2265 c. It is easy to see that this is equivalent to the constraint g i (X) \u2265 c, \u2200i = 1, 2, . . . , k, which can be solved thanks to the techniques above. We can also handle multiple constraints in SCSK. In particular, consider multiple 'and' constraints -{f i (X) \u2264 b i , i = 1, 2, \u2022 \u2022 \u2022 , k}, for monotone submodular functions f i and g. A first observation is that the greedy algorithm can almost directly extended to these cases, since we do not use any specific property of the constraints, while providing the approximation guarantees. Hence Theorem 4.7 can directly be extended to this case. We can also provide bi-criterion approximation guarantees with 'and' constraints. The approximate feasibility for a [\u03c1, \u03c3] bi-criterion approximation in this setting would be to have a set X such that f i (X) \u2264 \u03c3b i . Algorithm ISK can easily then be used in this scenario, and at every iteration we would solve a monotone submodular maximization problem subject to multiple linear (or knapsack constraints). Surprisingly this problem also has a constant factor (1 \u2212 1/e) approximation guarantee [28]. Hence we can retain the same approximation guarantees as in Theorem 4.8. We can also use algorithm EASKc, and obtain a curvature-independent approximation bound for this problem. The reason for this is the Ellipsoidal Approximation is of the form w fi (X), for each i and squaring it will lead to knapsack constraints. If we add the curvature terms (i.e., try to implement EASK in this setting), we obtain a much more complicated class of constraints, which we do not currently know how to handle. We can also extend SCSC and SCSK to non-monotone submodular functions. In particular, recall that the submodular knapsack has constant factor approximation guarantees even when g is non-monotone submodular [7]. Hence, we can obtain bi-criterion approximation guarantees for the Submodular Set Cover (SSC) problem, by solving the Submodular Knapsack (SK) problem multiple times. We can similarly do SCSC and SCSK when f is monotone submodular and g is non-monotone submodular, by extending ISSC, EASSC, ISK and EASK (note that in all these cases, we need to solve a submodular set cover or submodular knapsack problem with a non-monotone g). These algorithms, however, do not extend if f is non-monotone, and we do not currently know how to implement these.", "publication_ref": ["b25", "b12", "b29", "b25", "b12", "b25", "b25", "b27", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Hardness", "text": "In this section, we provide the hardness for Problems 1 and 2. The lower bounds serve to show that the approximation factors above are almost tight.\nTheorem 4.12. For any \u03ba > 0, there exists submodular functions with curvature \u03ba such that no polynomial time algorithm for Problems 1 and 2 achieves a bi-criterion factor better than \u03c3 \u03c1 = n 1/2\u2212 1+(n 1/2\u2212 \u22121)(1\u2212\u03ba) for any > 0.\nProof. We prove this result using the hardness construction from [11,43]. The main idea of their proof technique is to construct two submodular functions f (X) and f R (X) that with high probability are indistinguishable. Thus, also with high probability, no algorithm can distinguish between the two functions and the gap in their values provides a lower bound on the approximation. We shall see that this lower bound in fact matches the approximation factors up to log factors and hence this is the hardness of the problem. Define two monotone submodular functions f (X) = \u03ba f min{|X|, \u03b1} + (1 \u2212 \u03ba f )|X| and f R (X) = \u03ba f min{\u03b2 + |X \u2229R|, |X|, \u03b1} + (1 \u2212 \u03ba f )|X|, where R \u2286 V is a random set of cardinality \u03b1. Let \u03b1 and \u03b2 be an integer such that \u03b1 = x \u221a n/5 and \u03b2 = x 2 /5 for an x 2 = \u03c9(log n). Both f and f R have curvature \u03ba f . We also assume a very simple function g(X) = |X|. Given an arbitrary > 0, set x 2 = n 2 = \u03c9(log n). Then the ratio between f \u03ba f R (R) and g \u03ba f (R) is\nn 1/2\u2212 1+(n 1/2\u2212 \u22121)(1\u2212\u03ba f ) .\nA Chernoff bound analysis very similar to [43] reveals that any algorithm that uses a polynomial number of queries can distinguish h \u03ba and f \u03ba R with probability only n \u2212\u03c9 (1) , and therefore cannot reliably distinguish the functions with a polynomial number of queries. We first prove the first part of this theorem (i.e., for SCSC). In this case, we consider the problem min{h(X)||X| \u2265 \u03b1} (34) with h chosen as f and f R respectively. It is easy to see that R is the optimal set in both cases. However the ratio between the two is\n\u03b3(n) = n 1/2\u2212 1 + (n 1/2\u2212 \u2212 1)(1 \u2212 \u03ba f )(35)\nNow suppose there exists an algorithm which is guaranteed to obtain an approximation factor better than \u03b3(n). Then by running this algorithm, we are guaranteed to find different answers for Eqn (34) above. This must imply that this algorithm can distinguish between f R and g which is a contradiction. Hence no algorithm for Problem 1 can obtain an approximation guarantee\nn 1/2\u2212 1+(n 1/2\u2212 \u22121)(1\u2212\u03ba f ) .\nIn order to extend the result to the bi-criterion case, we need to show that no bi-criterion approximation algorithm can obtain a factor better than \u03c3 \u03c1 = \u03b3(n). Assume there exists a bi-criterion algorithm with a factor [\u03c3, \u03c1] such that:\n\u03c3 \u03c1 < \u03b3(n) = n 1/2\u2212 1 + (n 1/2\u2212 \u2212 1)(1 \u2212 \u03ba f )(36)\nThen, we are guaranteed to obtain a set S in equation ( 34) such that h(S) \u2264 \u03c3OP T and |S| \u2265 \u03c1\u03b1, where \u03c3 \u2265 1 and \u03c1 \u2264 1. Now we run this algorithm with h = f and h = f R respectively. With h = f , it is easy to see that the algorithm obtains a set S 1 such that f (S 1 ) \u2264 \u03c3\u03b1 and |S 1 | \u2265 \u03c1\u03b1. Similarly, with h = f R , the algorithm finds a set S 2 such that f (S 2 ) \u2264 \u03c3(\u03ba\u03b2 + (1 \u2212 \u03ba)\u03b1) and |S 2 | \u2265 \u03c1\u03b1. Since f R and f are indistinguishable, S 1 = S 2 = S. We first assume that |S| < \u03b1. Then f (S) = |S| \u2265 \u03c1\u03b1. We then have that,\nf R (S) \u2264 \u03c3(\u03ba\u03b2 + (1 \u2212 \u03ba)\u03b1 (37\n)\n\u2264 \u03c3n 2 (1 + (n 1/2\u2212 \u2212 1)(1 \u2212 \u03ba f ) (38) < \u03c1\u03b3(n)n 2 (1 + (n 1/2\u2212 \u2212 1)(1 \u2212 \u03ba f )(39)\n< \u03c1n 1/2+ (40)  This implies that the algorithm can distinguish between f R and f which is a contradiction. Now consider the second case when |S| \u2265 \u03b1. In this case, f (S) = \u03b1. Again the chain of inequalities above shows that f R (S) < \u03c1\u03b1 < \u03b1 since \u03c1 \u2264 1. This again implies that the algorithm can distinguish between f R and f which is a contradiction. Hence no bi-criterion approximation algorithm can obtain a factor better than \u03b3(n). To show the second part (for SCSK), we can simply invoke Theorem 3.1 and argue that any [\u03c1, \u03c3] bi-criterion approximation algorithm for problem 2 with\n\u03c3 \u03c1 < \u03b3(n) 1 + (42)\ncan be used in algorithm 3, to provide a [\u03c3, \u03c1] bi-criterion algorithm with \u03c3 \u03c1 < \u03b3(n). This contradicts the first part above and hence the same hardness applies to problem 2 as well.\nThe above result shows that EASSC and EASK meet the bounds above to log factors. We see an interesting curvature-dependent influence on the hardness. We also see from our approximation guarantees also that the curvature of f plays a more influential role than the curvature of g on the approximation quality. In particular, as soon as f becomes modular, the problem becomes easy, even when g is submodular. This is not surprising since the submodular set cover problem and the submodular cost knapsack problem both have constant factor guarantees.", "publication_ref": ["b10", "b42", "b42", "b0", "b33", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In this section, we empirically compare the performance of the various algorithms discussed in this paper. We are motivated by the speech data subset selection application [30,33,21] with the submodular function f encouraging limited vocabulary while g tries to achieve acoustic variability. A natural choice of the function f is a function of the form |\u0393(X)|, where \u0393(X) is the neighborhood function on a bipartite graph constructed between the utterances and the words [33]. For the coverage function g, we use two types of coverage: one is a facility location function \nBoth these functions are defined in terms of a similarity matrix S = {s ij } i,j\u2208V , which we define on the TIMIT corpus [9], using the string kernel metric [41] for similarity. Since some of our algorithms, like the Ellipsoidal Approximations, are computationally intensive, we restrict ourselves to 50 utterances. We compare our different algorithms on Problems 1 and 2 with f being the bipartite neighborhood and g being the facility location and saturated sum respectively. Since the primal and dual variants of the submodular set cover problem are similar, we just use the primal variants of ISSC and EASSC. Furthermore, in our experiments, we observe that the neighborhood function f has a curvature \u03ba f = 1. Thus, it suffices to use the simpler versions of algorithm EA (i.e., algorithm EASSCc and EASKc). The results are shown in Figure 3.\nWe observe that on the real-world instances, all our algorithms perform almost comparably. This implies, moreover, that the iterative variants, viz. Gr, ISSC and ISK, perform comparably to the more complicated EA-based ones, although EASSC and EASK have better theoretical guarantees. We also compare against a baseline of selecting random sets (of varying cardinality), and we see that our algorithms all perform much better. In terms of the running time, computing the Ellipsoidal Approximation for |\u0393(X)| with |V | = 50 takes about 5 hours while all the iterative variants (i.e., Gr, ISSC and ISK) take less than a second. This difference is much more prominent on larger instances (for example |V | = 500).", "publication_ref": ["b29", "b32", "b20", "b32", "b8", "b40"], "figure_ref": ["fig_11"], "table_ref": []}, {"heading": "Discussions and related work", "text": "In this paper, we propose a unifying framework for problems 1 and 2 based on suitable surrogate functions.\nWe provide a number of iterative algorithms which are very practical and scalable (like Gr, ISK and ISSC), and also algorithms like EASSC and EASK, which though more intensive, obtain tight approximation bounds. Finally, we empirically compare our algorithms, and show that the iterative algorithms compete empirically with the more complicated and theoretically better approximation algorithms.\nTo our knowledge, this paper provides the first general framework of approximation algorithms for Problems 1 and 2 for monotone submodular functions f and g. A number of papers, however, investigate related problems and approaches. For example, [8] investigates an exact algorithm for solving problem 1, with equality instead of inequality. However, since problem 1 subsumes the problem of minimizing a monotone submodular function subject to a cardinality equality constraint, and is hence NP hard [36]. Hence this algorithm in the worst case, must be exponential. Furthermore, a similar problem was considered in [25] with one specific instance of a function f , which is not submodular. They also use, a considerably different algorithm. Also, an algorithm equivalent to the first iteration of ISSC was proposed in [46,5] and ISSC not only generalizes this, but we also provide a more explicit approximation guarantee (we provide an elaborate discussion on this in the section describing ISSC). We also point out that, a special case of SCSK was considered in [29], with f being submodular, and g modular (we called this the submodular span problem).\nThe authors there use an algorithm very similar to Algorithm 2, to convert this problem into an instance of minimizing a submodular function subject to a knapsack constraint, for which they use the algorithm of [43].\nUnfortunately, the algorithm of [43] does not scale very well. Our algorithms for this problem, on the other hand, would continue to scale very well in practice.\nSimilarly, a number of approximation algorithms have been shown for Problem 0 [14,37,22]. The algorithms in [14,37] are scalable and practical, but lack theoretical guarantees. The algorithm of [22] though exact, employs a branch and bound technique which is often inefficient in practice (the timing analysis from [22] also depicts that). These facts are not surprising, since problem 0 is not only NP hard but also inapproximable. Moreover, these algorithms are not comparable to ours, since we directly model the hard constraints and our bicriteria results give worst-case bounds on the deviation from the constraints and the optimal solution. This is often important, since there are hard constraints in many practical applications (in the form of power constraints, or budget constraints). Casting it as Problem 0, however, no longer has guarantees on the deviation from the constraints. For future work, we would like to empirically evaluate our algorithms on many of the real world problems described above, particularly the limited vocabulary data subset selection application for speech corpora, and the machine translation application.", "publication_ref": ["b7", "b35", "b24", "b45", "b4", "b28", "b42", "b42", "b13", "b36", "b21", "b13", "b36", "b21", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgments: Special thanks to Kai Wei and Stefanie Jegelka for discussions, to Bethany Herwaldt for going through an early draft of this manuscript and to the anonymous reviewers for useful reviews. This material is based upon work supported by the National Science Foundation under Grant No. (IIS-1162606), a Google and a Microsoft award, and by the Intel Science and Technology Center for Pervasive Computing.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "The submodular knapsack polytope. Discrete Optimization", "journal": "", "year": "2009", "authors": "A Atamt\u00fcrk; V Narayanan"}, {"ref_id": "b1", "title": "Interactive graph cuts for optimal boundary and region segmentation of objects in n-d images", "journal": "", "year": "2001", "authors": "Y Boykov; M Jolly"}, {"ref_id": "b2", "title": "Submodular set functions, matroids and the greedy algorithm: tight worstcase bounds and some generalizations of the Rado-Edmonds theorem", "journal": "Discrete Applied Mathematics", "year": "1984", "authors": "M Conforti; G Cornuejols"}, {"ref_id": "b3", "title": "Decomposition of submodular functions", "journal": "Combinatorica", "year": "1983", "authors": "W H Cunningham"}, {"ref_id": "b4", "title": "On minimum submodular cover with submodular cost", "journal": "Journal of Global Optimization", "year": "2011", "authors": "H Du; W Wu; W Lee; Q Liu; Z Zhang; D.-Z Du"}, {"ref_id": "b5", "title": "A threshold of ln n for approximating set cover", "journal": "Journal of the ACM (JACM)", "year": "1998", "authors": "U Feige"}, {"ref_id": "b6", "title": "Maximizing non-monotone submodular functions", "journal": "SIAM J. COMPUT", "year": "2011", "authors": "U Feige; V Mirrokni; J Vondr\u00e1k"}, {"ref_id": "b7", "title": "Submodular functions and optimization", "journal": "Elsevier Science", "year": "2005", "authors": "S Fujishige"}, {"ref_id": "b8", "title": "Timit, acoustic-phonetic continuous speech corpus", "journal": "", "year": "1993", "authors": "J Garofolo; F Lamel; L ; J W Fiscus; D Pallet; N Dahlgren"}, {"ref_id": "b9", "title": "Approximability of combinatorial problems with multi-agent submodular cost functions", "journal": "", "year": "2009", "authors": "G Goel; C Karande; P Tripathi; L Wang"}, {"ref_id": "b10", "title": "Approximating submodular functions everywhere", "journal": "", "year": "2009", "authors": "M Goemans; N Harvey; S Iwata; V Mirrokni"}, {"ref_id": "b11", "title": "Interactive submodular set cover", "journal": "", "year": "2010", "authors": "A Guillory; J Bilmes"}, {"ref_id": "b12", "title": "Simultaneous learning and covering with adversarial noise", "journal": "", "year": "2011", "authors": "A Guillory; J Bilmes"}, {"ref_id": "b13", "title": "Algorithms for approximate minimization of the difference between submodular functions, with applications", "journal": "", "year": "2012", "authors": "R Iyer; J Bilmes"}, {"ref_id": "b14", "title": "The submodular Bregman and Lov\u00e1sz-Bregman divergences with applications", "journal": "", "year": "2012", "authors": "R Iyer; J Bilmes"}, {"ref_id": "b15", "title": "Mirror descent like algorithms for submodular optimization", "journal": "", "year": "2012", "authors": "R Iyer; S Jegelka; J Bilmes"}, {"ref_id": "b16", "title": "Curvature and Optimal Algorithms for Learning and Minimizing Submodular Functions", "journal": "", "year": "2013", "authors": "R Iyer; S Jegelka; J Bilmes"}, {"ref_id": "b17", "title": "Fast semidifferential based submodular function optimization", "journal": "", "year": "2013", "authors": "R Iyer; S Jegelka; J Bilmes"}, {"ref_id": "b18", "title": "Submodularity beyond submodular energies: coupling edges in graph cuts", "journal": "", "year": "2011", "authors": "S Jegelka; J Bilmes"}, {"ref_id": "b19", "title": "Submodularity beyond submodular energies: coupling edges in graph cuts", "journal": "", "year": "2011", "authors": "S Jegelka; J A Bilmes"}, {"ref_id": "b20", "title": "On fast approximate submodular minimization", "journal": "", "year": "2011", "authors": "S Jegelka; H Lin; J Bilmes"}, {"ref_id": "b21", "title": "Prismatic algorithm for discrete dc programming problems", "journal": "", "year": "2011", "authors": "Y Kawahara; T Washio"}, {"ref_id": "b22", "title": "Knapsack problems", "journal": "Springer Verlag", "year": "2004", "authors": "H Kellerer; U Pferschy; D Pisinger"}, {"ref_id": "b23", "title": "A note on the budgeted maximization on submodular functions", "journal": "", "year": "2005", "authors": "A Krause; C Guestrin"}, {"ref_id": "b24", "title": "Near-optimal sensor placements: Maximizing information while minimizing communication cost", "journal": "", "year": "2006", "authors": "A Krause; C Guestrin; A Gupta; J Kleinberg"}, {"ref_id": "b25", "title": "Robust submodular observation selection", "journal": "Journal of Machine Learning Research", "year": "2008", "authors": "A Krause; B Mcmahan; C Guestrin; A Gupta"}, {"ref_id": "b26", "title": "Near-optimal sensor placements in Gaussian processes: Theory, efficient algorithms and empirical studies", "journal": "JMLR", "year": "2008", "authors": "A Krause; A Singh; C Guestrin"}, {"ref_id": "b27", "title": "Maximizing submodular set functions subject to multiple linear constraints", "journal": "", "year": "2009", "authors": "A Kulik; H Shachnai; T Tamir"}, {"ref_id": "b28", "title": "Submodularity in Natural Language Processing: Algorithms and Applications", "journal": "", "year": "2012", "authors": "H Lin"}, {"ref_id": "b29", "title": "How to select a good training-data subset for transcription: Submodular active selection for sequences", "journal": "", "year": "2009", "authors": "H Lin; J Bilmes"}, {"ref_id": "b30", "title": "Multi-document summarization via budgeted maximization of submodular functions", "journal": "", "year": "2010", "authors": "H Lin; J Bilmes"}, {"ref_id": "b31", "title": "A class of submodular functions for document summarization", "journal": "", "year": "2011-06", "authors": "H Lin; J Bilmes"}, {"ref_id": "b32", "title": "Optimal selection of limited vocabulary speech corpora", "journal": "", "year": "2011", "authors": "H Lin; J Bilmes"}, {"ref_id": "b33", "title": "Submodular functions and convexity. Mathematical Programming", "journal": "", "year": "1983", "authors": "L Lov\u00e1sz"}, {"ref_id": "b34", "title": "Intelligent selection of language model training data", "journal": "Association for Computational Linguistics", "year": "2010", "authors": "R C Moore; W Lewis"}, {"ref_id": "b35", "title": "Size-constrained submodular minimization through minimum norm base", "journal": "", "year": "2011", "authors": "K Nagano; Y Kawahara; K Aihara"}, {"ref_id": "b36", "title": "A submodular-supermodular procedure with applications to discriminative structure learning", "journal": "", "year": "2005", "authors": "M Narasimhan; J Bilmes"}, {"ref_id": "b37", "title": "Best algorithms for approximating the maximum of a submodular set function", "journal": "Mathematics of Operations Research", "year": "1978", "authors": "G Nemhauser; L Wolsey"}, {"ref_id": "b38", "title": "Approximation algorithms for offline risk-averse combinatorial optimization", "journal": "", "year": "2010", "authors": "E Nikolova"}, {"ref_id": "b39", "title": "Cosegmentation of image pairs by histogram matching-incorporating a global constraint into MRFs", "journal": "IEEE", "year": "2006", "authors": "C Rother; T Minka; A Blake; V Kolmogorov"}, {"ref_id": "b40", "title": "Efficient computation of gapped substring kernels on large alphabets", "journal": "Journal of Machine Learning Research", "year": "2006", "authors": "J Rousu; J Shawe-Taylor"}, {"ref_id": "b41", "title": "A note on maximizing a submodular set function subject to a knapsack constraint", "journal": "Operations Research Letters", "year": "2004", "authors": "M Sviridenko"}, {"ref_id": "b42", "title": "Submodular approximation: Sampling-based algorithms and lower bounds", "journal": "", "year": "2008", "authors": "Z Svitkina; L Fleischer"}, {"ref_id": "b43", "title": "Approximation algorithms", "journal": "springer", "year": "2004", "authors": "V V Vazirani"}, {"ref_id": "b44", "title": "Submodularity and curvature: the optimal algorithm", "journal": "RIMS Kokyuroku Bessatsu", "year": "2010", "authors": "J Vondr\u00e1k"}, {"ref_id": "b45", "title": "Greedy approximations for minimum submodular cover with submodular cost", "journal": "Computational Optimization and Applications", "year": "2010", "authors": "P.-J Wan; D.-Z Du; P Pardalos; W Wu"}, {"ref_id": "b46", "title": "An analysis of the greedy algorithm for the submodular set covering problem", "journal": "Combinatorica", "year": "1982", "authors": "L A Wolsey"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "for SCSK using b. 7: end while 8: ReturnX b .", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Bicriterion transformation algorithms for SCSC and SCSK using Linear search.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "then    ", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "then    ", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "12: end while 13: ReturnX bmax", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 2 :2Figure 2: Bicriterion transformation algorithms for SCSC and SCSK using Binary search.Theorem 3.1 implies that the complexity of Problems 1 and 2 are identical, and a solution to one of them provides a solution to the other. Furthermore, as expected, the hardness of Problems 1 and 2 are also almost identical. When f and g are polymatroid functions, moreover, we can provide bounded approximation guarantees for both problems, as shown in the next section. Alternatively we can also do a binary search instead of a linear search to transform Problems 1 and 2. This essentially turns the factor of O(1/ ) into O(log 1/ ). Algorithms 4 and 5 show the transformation algorithms using binary search. While the binary search also ensures the same performance guarantees, it does so more efficiently.Theorem 3.2. Algorithm 4 is guaranteed to find a setX c which is a [(1 \u2212 )\u03c1, \u03c3] approximation of SCSK in at most log 2 [g(V )/ minj g(j)] calls to the [\u03c3, \u03c1] approximate algorithm for SCSC. Similarly, Algorithm 5 is guaranteed to find a setX b which is a [(1 + )\u03c3, \u03c1] approximation of SCSC in log 2 [f (V )/ minj f (j)] calls to a [\u03c1, \u03c3] approximate algorithm for SCSK. When f and g are integral, moreover, Algorithm 4 obtains a [\u03c1, \u03c3] bi-criterion approximate solution for SCSK in log 2 g(V ) iterations, and similarly Algorithm 5 obtains a [\u03c3, \u03c1] bi-criterion approximate solution for SCSC in log 2 g(V ) iterations. Proof. To show this theorem, we use the result from Theorem 3.1. Let c = c min and c = c max . An important observation is that throughout the algorithm, the values of c min satisfy f (X cmin ) \u2264 \u03c3b and f (X cmax ) > \u03c3b. Hence, f (X c ) \u2264 \u03c3b and f (X c ) > \u03c3b. Moreover, notice that c /c = c max /c min = 1/(1 \u2212 ). Hence using the proof of Theorem 3.1 the approximation guarantee follows. In order to show the complexity, notice that c max \u2212 c min is decreasing throughout the algorithm. At the beginning, c max \u2212 c min \u2264 g(V ) and at convergence, c max \u2212 c min \u2265 c max /2 \u2265 min j g(j)/2. The bound at convergence holds since, let c max and c min be the values at the previous step. It holds that c max \u2212c min \u2265 c max . Moreover, c max \u2212 c min = (c max \u2212 c min )/2 \u2265 c max /2 \u2265 c max . Hence the number of iterations is bounded by log 2 [2g(V )/ minj g(j)]. Moreover, when f and g are integral, the analysis is much simpler. In particular,", "figure_data": ""}, {"figure_label": "44", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Theorem 4 . 4 .44The primal EASSC obtains a guarantee of O(", "figure_data": ""}, {"figure_label": "45", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Corollary 4 . 5 .45The primal EASSCc obtains an approximation guarantee of O( \u221a n log n H g ). Similarly, the dual EASSCc obtains a bicriterion guarantee of [O( \u221a n log n), 1 \u2212 e \u22121 ].", "figure_data": ""}, {"figure_label": "411", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Corollary 4 . 11 .411Algorithm EASKc obtains a bi-criterion guarantee of [1 \u2212 e \u22121 , O( \u221a n log n)].", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 3 :3Figure 3: The two figures show the performance of the algorithms in the text on Problems 1 and 2 (in both cases, higher the better).", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "g 1 (1", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Input: An SCSK instance with budget b, an [\u03c3, \u03c1] approx. algo. for SCSC, & > 0.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Input: An SCSC instance with cover c, an [\u03c1, \u03c3] approx. algo. for SCSK, & > 0. 2: Output: [(1 + )\u03c3, \u03c1] approx. for SCSC.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Worst case approximation factors, hardness for Problems 1 and 2 and their special cases.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u03ba f = 1 \u2212 min j\u2208V f (j|V \\j) f (j) , and \u03ba f (X) = 1 \u2212 min{min j\u2208X f (j|X\\j) f (j) , min j / \u2208X f (j|X) f (j) }.(2)", "formula_coordinates": [4.0, 134.89, 134.14, 405.11, 22.31]}, {"formula_id": "formula_1", "formula_text": "1: for t = 1, 2, \u2022 \u2022 \u2022 , T do 2:", "formula_coordinates": [4.0, 92.31, 296.34, 106.53, 20.34]}, {"formula_id": "formula_2", "formula_text": "\u2202 f (Y ) = {y \u2208 R n : f (X) \u2212 y(X) \u2265 f (Y ) \u2212 y(Y ) for all X \u2286 V } (3)", "formula_coordinates": [4.0, 168.47, 559.7, 371.53, 11.72]}, {"formula_id": "formula_3", "formula_text": "= j\u2208X x(j). Denote a subgradient at Y by h Y \u2208 \u2202 f (Y ).", "formula_coordinates": [4.0, 286.85, 584.7, 255.08, 11.15]}, {"formula_id": "formula_4", "formula_text": "extreme point h \u03c0 Y of \u2202 f (Y ) with entries h \u03c0 Y (\u03c0(i)) = f (S \u03c0 i ) \u2212 f (S \u03c0 i\u22121 ).(4)", "formula_coordinates": [5.0, 72.0, 73.58, 468.0, 33.83]}, {"formula_id": "formula_5", "formula_text": "\u2202 f (Y ) = {y \u2208 R n : f (X) \u2212 y(X) \u2264 f (Y ) \u2212 y(Y ); for all X \u2286 V } (5)", "formula_coordinates": [5.0, 167.36, 173.86, 372.64, 11.37]}, {"formula_id": "formula_6", "formula_text": "m f X,1 (Y ) f (X) \u2212 j\u2208X\\Y f (j|X\\j) + j\u2208Y \\X f (j|\u2205), m f X,2 (Y ) f (X) \u2212 j\u2208X\\Y f (j|V \\j) + j\u2208Y \\X f (j|X). Then m f X,1 (Y ) \u2265 f (Y ) and m f X,2 (Y ) \u2265 f (Y ), \u2200Y \u2286 V and m f X,1 (X) = m f X,2 (X) = f (X)", "formula_coordinates": [5.0, 71.64, 232.0, 381.79, 78.81]}, {"formula_id": "formula_7", "formula_text": "w f \u2208 R V , such that w f (X) \u2264 f (X) \u2264 O( \u221a n log n) w f (X), \u2200X \u2286 V .", "formula_coordinates": [5.0, 229.85, 533.05, 312.08, 16.47]}, {"formula_id": "formula_8", "formula_text": "f \u03ba (X) f (X) \u2212 (1 \u2212 \u03ba f ) j\u2208X f (j) \u03ba f .(6)", "formula_coordinates": [5.0, 223.04, 584.22, 316.96, 24.72]}, {"formula_id": "formula_9", "formula_text": "f ea (X) \u2264 f (X) \u2264 O \u221a n log n 1 + ( \u221a n log n \u2212 1)(1 \u2212 \u03ba f ) f ea (X), \u2200X \u2286 V (7)", "formula_coordinates": [6.0, 159.08, 100.52, 380.92, 30.44]}, {"formula_id": "formula_10", "formula_text": "1: Input: An SCSK instance with budget b, an [\u03c3, \u03c1] approx. algo. for SCSC, & > 0. 2: Output: [(1 \u2212 )\u03c1, \u03c3] approx. for SCSK. 3: c \u2190 g(V ),X c \u2190 V . 4: while f (X c ) > \u03c3b do 5: c \u2190 (1 \u2212 )c 6:X c \u2190 [\u03c3, \u03c1]", "formula_coordinates": [6.0, 92.31, 345.95, 193.12, 81.41]}, {"formula_id": "formula_11", "formula_text": "1: Input: An SCSC instance with cover c, an [\u03c1, \u03c3] approx. algo. for SCSK, & > 0. 2: Output: [(1 + )\u03c3, \u03c1] approx. for SCSC. 3: b \u2190 argmin j f (j),X b \u2190 \u2205. 4: while g(X b ) < \u03c1c do 5: b \u2190 (1 + )b 6:X b \u2190 [\u03c1, \u03c3] approx.", "formula_coordinates": [6.0, 333.87, 344.82, 193.12, 83.68]}, {"formula_id": "formula_12", "formula_text": "Theorem 3.1. Algorithm 2 is guaranteed to find a setX c which is a [(1 \u2212 )\u03c1, \u03c3] approximation of SCSK in at most log 1/(1\u2212 ) [g(V )/ min j g(j)] calls to the [\u03c3, \u03c1] approximate algorithm for SCSC. Similarly, Algorithm 3 is guaranteed to find a setX b which is a [(1 + )\u03c3, \u03c1] approximation of SCSC in log 1+ [f (V )/ min j f (j)] calls to a [\u03c1, \u03c3] approximate algorithm for SCSK.", "formula_coordinates": [7.0, 71.6, 310.25, 468.4, 47.29]}, {"formula_id": "formula_13", "formula_text": "g(X c ) \u2265 \u03c1c = \u03c1(1 \u2212 )c > \u03c1(1 \u2212 )g(X * ) (8)", "formula_coordinates": [7.0, 217.87, 485.43, 322.14, 11.72]}, {"formula_id": "formula_14", "formula_text": "Output: [(1 \u2212 )\u03c1, \u03c3] approx. for SCSK. 3: c min \u2190 min j g(j), c max \u2190 g(V ) 4: while c max \u2212 c min \u2265 c max do 5: c \u2190 [c max + c min ]/2 6:X c \u2190 [\u03c3, \u03c1] approx. for SCSC using c. 7: if f (X c ) > \u03c3b", "formula_coordinates": [8.0, 92.31, 149.88, 193.12, 69.46]}, {"formula_id": "formula_15", "formula_text": "b min \u2190 argmin j f (j), b max \u2190 f (V ). 4: while b max \u2212 b min \u2265 b min do 5: b \u2190 [b max + b min ]/2 6:X b \u2190 [\u03c1, \u03c3] approx. for SCSK using b 7: if g(X b ) < \u03c1c", "formula_coordinates": [8.0, 333.87, 161.87, 185.81, 57.47]}, {"formula_id": "formula_16", "formula_text": "\u03c0(i) \u2208 argmin f (j) g(j|S \u03c0 i\u22121 ) j / \u2208 S \u03c0 i\u22121 , g(S \u03c0 i\u22121 \u222a j) < c .(9)", "formula_coordinates": [9.0, 186.05, 469.6, 353.95, 24.51]}, {"formula_id": "formula_17", "formula_text": "\u2208S \u03c0 i\u22121 f (j) g(j|S \u03c0 i\u22121 )", "formula_coordinates": [9.0, 290.71, 526.62, 58.82, 16.27]}, {"formula_id": "formula_18", "formula_text": "f (X g ) f (X * ) \u2264 1 + log e min{\u03bb 1 , \u03bb 2 , \u03bb 3 } (10", "formula_coordinates": [9.0, 234.56, 594.82, 301.01, 23.89]}, {"formula_id": "formula_19", "formula_text": ")", "formula_coordinates": [9.0, 535.57, 603.14, 4.43, 8.74]}, {"formula_id": "formula_20", "formula_text": "\u03bb 1 = max{ 1 1\u2212\u03bag(S \u03c0 i ) | i : c(S \u03c0 i ) < 1}, \u03bb 2 = \u03b8 N \u03b81 and \u03bb 3 = g(V )\u2212g(\u2205) g(V )\u2212g(S \u03c0 N \u22121 )", "formula_coordinates": [9.0, 101.06, 629.94, 304.89, 16.35]}, {"formula_id": "formula_21", "formula_text": "f (X g ) f (X * ) \u2264 H(max j g(j)), where H(d) = d i=1 1 i for a positive integer d.", "formula_coordinates": [9.0, 73.2, 645.98, 302.63, 16.03]}, {"formula_id": "formula_22", "formula_text": "H(d) = d i=1 1 i", "formula_coordinates": [10.0, 72.0, 189.5, 67.85, 14.56]}, {"formula_id": "formula_23", "formula_text": "KgHg 1+(Kg\u22121)(1\u2212\u03ba f ) \u2264 n 1+(n\u22121)(1\u2212\u03ba f ) H g", "formula_coordinates": [11.0, 405.22, 212.52, 149.01, 15.28]}, {"formula_id": "formula_24", "formula_text": "(1+ )Kg 1+(Kg\u22121)(1\u2212\u03ba f ) , 1 \u2212 e \u22121 .", "formula_coordinates": [11.0, 349.34, 241.75, 108.83, 15.28]}, {"formula_id": "formula_25", "formula_text": "\u03b2|X * | 1 + (|X * | \u2212 1)(1 \u2212 \u03ba f ) (11)", "formula_coordinates": [11.0, 257.49, 298.42, 282.51, 24.8]}, {"formula_id": "formula_26", "formula_text": "f (X) \u2264 f (X 1 ) \u2264 m f \u2205 (X 1 ) \u2264 (1 + )m f \u2205 (X * ) \u2264 K g (1 + ) 1 + (K g \u2212 1)(1 \u2212 \u03ba f ) f (X * ) (12", "formula_coordinates": [11.0, 144.43, 415.55, 391.14, 23.23]}, {"formula_id": "formula_27", "formula_text": ")", "formula_coordinates": [11.0, 535.57, 422.29, 4.43, 8.74]}, {"formula_id": "formula_28", "formula_text": "m f \u2205 (X) \u2264 f (X) \u2264 |X| 1 + (|X| \u2212 1)(1 \u2212 \u03ba f ) f (X)(13)", "formula_coordinates": [11.0, 207.22, 474.78, 332.79, 23.23]}, {"formula_id": "formula_29", "formula_text": "min \uf8f1 \uf8f2 \uf8f3 \u03ba f w f \u03ba (X) + (1 \u2212 \u03ba f ) j\u2208X f (j) g(X) \u2265 c \uf8fc \uf8fd \uf8fe . (14", "formula_coordinates": [12.0, 190.74, 328.36, 344.83, 34.08]}, {"formula_id": "formula_30", "formula_text": ")", "formula_coordinates": [12.0, 535.57, 342.38, 4.43, 8.74]}, {"formula_id": "formula_31", "formula_text": "\u221a n log nHg 1+( \u221a n log n\u22121)(1\u2212\u03ba f ) )", "formula_coordinates": [12.0, 341.77, 489.38, 86.13, 20.69]}, {"formula_id": "formula_32", "formula_text": "\u221a n log n 1+( \u221a n log n\u22121)(1\u2212\u03ba f ) ), 1 \u2212 e \u22121 .", "formula_coordinates": [12.0, 101.36, 517.89, 131.27, 20.33]}, {"formula_id": "formula_33", "formula_text": "\u03b2(n) = O \u221a n log n ( \u221a n log n \u2212 1)(1 \u2212 \u03ba f ) + 1) . (15", "formula_coordinates": [12.0, 214.8, 623.59, 320.77, 30.44]}, {"formula_id": "formula_34", "formula_text": ")", "formula_coordinates": [12.0, 535.57, 637.5, 4.43, 8.74]}, {"formula_id": "formula_35", "formula_text": "f (X) \u2264 \u03b2(n)f (X) \u2264 H g \u03b2(n)(1 + )f (X * ) \u2264 H g \u03b2(n)(1 + )f (X * ),(16)", "formula_coordinates": [13.0, 162.94, 96.0, 377.06, 11.72]}, {"formula_id": "formula_36", "formula_text": "min{w f (X)|g(X) \u2265 c}. (17", "formula_coordinates": [13.0, 255.53, 196.79, 280.04, 10.81]}, {"formula_id": "formula_37", "formula_text": ")", "formula_coordinates": [13.0, 535.57, 198.86, 4.43, 8.74]}, {"formula_id": "formula_38", "formula_text": "w(X) \u2265 H g min{w(X)|g(X) \u2265 c} (18", "formula_coordinates": [13.0, 233.47, 306.17, 302.1, 9.65]}, {"formula_id": "formula_39", "formula_text": ")", "formula_coordinates": [13.0, 535.57, 306.17, 4.43, 8.74]}, {"formula_id": "formula_40", "formula_text": "Then denote \u03b1(n) = O( \u221a n log n). f (X) \u2264 \u03b1(n) w(X) \u2264 H g \u03b1(n) w(X * ) \u2264 H g \u03b1(n)f (X * )(19)", "formula_coordinates": [13.0, 71.64, 320.83, 468.36, 43.97]}, {"formula_id": "formula_41", "formula_text": "\u03c0(i) \u2208 argmax g(j|S \u03c0 i\u22121 ) f (j) j / \u2208 S \u03c0 i\u22121 , f (S \u03c0 i\u22121 \u222a {j}) \u2264 b ,(20)", "formula_coordinates": [13.0, 179.7, 583.58, 360.3, 24.45]}, {"formula_id": "formula_42", "formula_text": "Problem Algorithm Approx. factor * Hardness * Bi-Criterion factor # (Bi-Criterion \u03c3 \u03c1 ) # SSC Primal Greedy H(maxj f (j)) * log n * Dual Greedy (1 + , 1 \u2212 1 e ) # 1 \u2212 1/e # SCSC Primal ISSC nHg 1+(n\u22121)(1\u2212\u03ba f ) * \u2126( \u221a n 1+( \u221a n\u22121)(1\u2212\u03ba f ) ) * ,# Dual ISSC [ (1+ )n 1+(n\u22121)(1\u2212\u03ba f ) , 1 \u2212 1/e] # Primal EASSC O( \u221a n log nHg 1+( \u221a n log n\u22121)(1\u2212\u03ba f ) ) * Dual EASSC [O( \u221a n log n 1+( \u221a n log n\u22121)(1\u2212\u03ba f ) , 1 \u2212 1/e] # Primal EASSCc O( \u221a n log n Hg) * Dual EASSCc [O( \u221a n log n), 1 \u2212 1/e] # SK Greedy 1 \u2212 1/e * 1 \u2212 1/e * SCSK Greedy 1/n * \u2126( \u221a n 1+( \u221a n\u22121)(1\u2212\u03ba f ) ) * ,# ISK [1 \u2212 e \u22121 , K f 1+(K f \u22121)(1\u2212\u03ba f ) ] # Primal EASK [1 + , O( \u221a n log nHg 1+( \u221a n log n\u22121)(1\u2212\u03ba f ) )] # Dual EASK [(1 \u2212 1/e)(1 + ), O( \u221a n log n 1+( \u221a n log n\u22121)(1\u2212\u03ba f ) )] # EASKc [1 \u2212 1/e, O( \u221a n log n)] #", "formula_coordinates": [14.0, 109.22, 72.18, 393.06, 261.4]}, {"formula_id": "formula_43", "formula_text": "max{ max i:f (i)\u2264b g(i), g(X g )} \u2265 1/2(1 \u2212 1/e)g(X * ).(21)", "formula_coordinates": [14.0, 205.38, 414.2, 334.62, 17.12]}, {"formula_id": "formula_44", "formula_text": "max i,j,k\u2208V f (X ijk ) \u2265 (1 \u2212 1/e)f (X * ).(22)", "formula_coordinates": [14.0, 233.54, 477.12, 306.46, 16.73]}, {"formula_id": "formula_45", "formula_text": "\u2208 S i\u22121 : f (S \u03c0 i\u22121 \u222a {j}) \u2264 b which maximizes g(j|S i\u22121 )", "formula_coordinates": [14.0, 116.22, 528.81, 233.53, 12.32]}, {"formula_id": "formula_46", "formula_text": "\u03c0(i) \u2208 argmax{g(j|S \u03c0 i\u22121 )|j / \u2208 S \u03c0 i\u22121 , f (S \u03c0 i\u22121 \u222a {j}) \u2264 b}.(23)", "formula_coordinates": [14.0, 188.65, 562.19, 351.35, 12.69]}, {"formula_id": "formula_47", "formula_text": "K f \u2212\u03bag K f ) k f ) \u2265 1 K f , where K f = max{|X| : f (X) \u2264 b} and k f = min{|X| : f (X) \u2264 b & \u2200j \u2208 X, f (X \u222a j) > b}.", "formula_coordinates": [14.0, 72.0, 585.61, 468.0, 26.65]}, {"formula_id": "formula_48", "formula_text": "max{g(X)|m f X t (X) \u2264 b},(24)", "formula_coordinates": [15.0, 251.05, 178.01, 288.95, 14.12]}, {"formula_id": "formula_49", "formula_text": ") | f (X) \u2264 b(1+(K f \u22121)(1\u2212\u03ba f ) K f", "formula_coordinates": [15.0, 164.66, 271.42, 122.48, 15.34]}, {"formula_id": "formula_50", "formula_text": "max g(X) | f (X) \u2264 b(1 + (K f \u2212 1)(1 \u2212 \u03ba f ) K f(25)", "formula_coordinates": [15.0, 203.75, 319.71, 760.99, 23.22]}, {"formula_id": "formula_51", "formula_text": "max \uf8f1 \uf8f2 \uf8f3 g(X) | j\u2208X f (j) \u2264 b \uf8fc \uf8fd \uf8fe (26)", "formula_coordinates": [15.0, 243.69, 375.57, 296.31, 34.08]}, {"formula_id": "formula_52", "formula_text": "j\u2208X f (j) \u2264 K f 1 + (K f \u2212 1)(1 \u2212 \u03ba f ) f (X) \u2264 K f 1 + (K f \u2212 1)(1 \u2212 \u03ba f ) b(1 + (K f \u2212 1)(1 \u2212 \u03ba f )) K f \u2264 b", "formula_coordinates": [15.0, 107.63, 441.33, 396.74, 26.8]}, {"formula_id": "formula_53", "formula_text": "bK f 1+(K f \u22121)(1\u2212\u03ba f ) instead of b.", "formula_coordinates": [15.0, 131.39, 571.05, 120.56, 15.34]}, {"formula_id": "formula_54", "formula_text": "K f 1+(K f \u22121)(1\u2212\u03ba f ) ], where K f = max{|X| : f (X) \u2264 b}.", "formula_coordinates": [15.0, 153.49, 604.35, 223.63, 15.34]}, {"formula_id": "formula_55", "formula_text": "max \uf8f1 \uf8f2 \uf8f3 g(X) \u03ba f w f \u03ba (X) + (1 \u2212 \u03ba f ) j\u2208X f (j) \u2264 b \uf8fc \uf8fd \uf8fe .(27)", "formula_coordinates": [16.0, 189.79, 106.41, 350.21, 34.08]}, {"formula_id": "formula_56", "formula_text": "\u221a n log nHg 1+( \u221a n log n\u22121)(1\u2212\u03ba f ) )", "formula_coordinates": [16.0, 362.59, 204.49, 86.59, 20.69]}, {"formula_id": "formula_57", "formula_text": "(1 + )(1 \u2212 1/e), O( \u221a n log n 1+( \u221a n log n\u22121)(1\u2212\u03ba f ) ) .", "formula_coordinates": [16.0, 207.03, 223.78, 180.18, 20.33]}, {"formula_id": "formula_58", "formula_text": "max{g(X) : w f (X) \u2264 b 2 }.(28)", "formula_coordinates": [16.0, 249.58, 389.23, 290.42, 10.81]}, {"formula_id": "formula_59", "formula_text": "f (X) \u2264 \u221a n log n w f (X) \u2264 b \u221a n log n (29)", "formula_coordinates": [16.0, 223.1, 521.85, 316.9, 16.4]}, {"formula_id": "formula_60", "formula_text": "{g 1 (X) = g 1 (V ) \u2227 g 2 (X) = g 2 (V )} \u21d4 {g(X) = g(V )} (31)", "formula_coordinates": [17.0, 190.44, 104.31, 349.56, 9.65]}, {"formula_id": "formula_61", "formula_text": "{g 1 (X) = g 1 (V ) \u2228 g 2 (X) = g 2 (V )} \u21d4 {g(X) = g(V )} (32)", "formula_coordinates": [17.0, 190.44, 175.96, 349.56, 9.65]}, {"formula_id": "formula_62", "formula_text": "n 1/2\u2212 1+(n 1/2\u2212 \u22121)(1\u2212\u03ba f ) .", "formula_coordinates": [18.0, 172.22, 226.9, 80.44, 16.18]}, {"formula_id": "formula_63", "formula_text": "\u03b3(n) = n 1/2\u2212 1 + (n 1/2\u2212 \u2212 1)(1 \u2212 \u03ba f )(35)", "formula_coordinates": [18.0, 235.48, 340.06, 304.52, 24.98]}, {"formula_id": "formula_64", "formula_text": "n 1/2\u2212 1+(n 1/2\u2212 \u22121)(1\u2212\u03ba f ) .", "formula_coordinates": [18.0, 312.0, 408.78, 80.38, 16.18]}, {"formula_id": "formula_65", "formula_text": "\u03c3 \u03c1 < \u03b3(n) = n 1/2\u2212 1 + (n 1/2\u2212 \u2212 1)(1 \u2212 \u03ba f )(36)", "formula_coordinates": [18.0, 225.81, 470.68, 314.19, 24.98]}, {"formula_id": "formula_66", "formula_text": "f R (S) \u2264 \u03c3(\u03ba\u03b2 + (1 \u2212 \u03ba)\u03b1 (37", "formula_coordinates": [18.0, 212.36, 586.32, 323.22, 9.65]}, {"formula_id": "formula_67", "formula_text": ")", "formula_coordinates": [18.0, 535.57, 586.32, 4.43, 8.74]}, {"formula_id": "formula_68", "formula_text": "\u2264 \u03c3n 2 (1 + (n 1/2\u2212 \u2212 1)(1 \u2212 \u03ba f ) (38) < \u03c1\u03b3(n)n 2 (1 + (n 1/2\u2212 \u2212 1)(1 \u2212 \u03ba f )(39)", "formula_coordinates": [18.0, 240.97, 601.16, 299.04, 28.64]}, {"formula_id": "formula_69", "formula_text": "\u03c3 \u03c1 < \u03b3(n) 1 + (42)", "formula_coordinates": [19.0, 284.54, 367.52, 255.46, 22.31]}], "doi": ""}