{"title": "Tracking Everything Everywhere All at Once", "authors": "Qianqian Wang; Yen-Yu Chang; Ruojin Cai; Zhengqi Li; Bharath Hariharan; Aleksander Holynski; Noah Snavely", "pub_date": "2023-09-12", "abstract": "2 Google Research 3 UC Berkeley Figure 1: We present a new method for estimating full-length motion trajectories for every pixel in every frame of a video, as illustrated by the motion paths shown above. For clarity, we only show sparse trajectories for foreground objects, though our method computes motion for all pixels. Our method yields accurate, coherent long-range motion even for fast-moving objects, and robustly tracks through occlusions as shown in the dog and swing examples. For context, in the second row we depict the moving object at different moments in time.", "sections": [{"heading": "Introduction", "text": "Motion estimation methods have traditionally followed one of two dominant approaches: sparse feature tracking and dense optical flow [55]. While each type of method has proven effective for their respective applications, neither representation fully models the motion of a video: pairwise optical flow fails to capture motion trajectories over long temporal windows, and sparse tracking does not model the motion of all pixels.\nA number of approaches have sought to close this gap, i.e., to estimate both dense and long-range pixel trajectories in a video. These range from methods that simply chain together two-frame optical flow fields, to more recent approaches that directly predict per-pixel trajectories across multiple frames [23]. Still, these methods all use limited context when estimating motion, disregarding information that is either temporally or spatially distant. This locality can result in accumulated errors over long trajectories and spatiotemporal inconsistencies in the motion estimates. Even when prior methods do consider long-range context [55], they op-erate in the 2D domain, resulting in a loss of tracking during occlusion events. All in all, producing both dense and longrange trajectories remains an open problem in the field, with three key challenges: (1) maintaining accurate tracks across long sequences, (2) tracking points through occlusions, and (3) maintaining coherence in space and time.\nIn this work, we propose a holistic approach to video motion estimation that uses all the information in a video to jointly estimate full-length motion trajectories for every pixel. Our method, which we dub OmniMotion, uses a quasi-3D representation in which a canonical 3D volume is mapped to per-frame local volumes through a set of local-canonical bijections. These bijections serve as a flexible relaxation of dynamic multi-view geometry, modeling a combination of camera and scene motion. Our representation guarantees cycle consistency, and can track all pixels, even while occluded (\"Everything, Everywhere\"). We optimize our representation per video to jointly solve for the motion of the entire video \"All at Once\". Once optimized, our representation can be queried at any continuous coordinate in the video to receive a motion trajectory spanning the entire video.\nIn summary, we propose an approach that: 1) produces globally consistent full-length motion trajectories for all points in an entire video, 2) can track points through occlusions, and 3) can tackle in-the-wild videos with any combination of camera and scene motion. We demonstrate these strengths quantitatively on the TAP video tracking benchmark [15], where we achieve state-of-the-art performance, outperforming all prior methods by a large margin.", "publication_ref": ["b54", "b22", "b54", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Sparse feature tracking. Tracking features [4,42] across images is essential for a wide range of applications such as Structure from Motion (SfM) [1,56,59] and SLAM [17]. While sparse feature tracking [13,43,57,67] can establish long-range correspondence, this correspondence is limited to a set of distinctive interest points, and often restricted to rigid scenes. Hence, below we focus on work that can produce dense pixel motion for general videos.\nOptical flow. Optical flow has traditionally been formulated as an optimization problem [6,7,24,75]. However, recent advances have enabled direct prediction of optical flow using neural networks with improved quality and efficiency [20,25,26,61]. One leading method, RAFT [66], estimates flow through iterative updates of a flow field based on 4D correlation volumes. While optical flow methods allow for precise motion estimation between consecutive frames, they are not suited to long-range motion estimation: chaining pairwise optical flow into longer trajectories results in drift and fails to handle occlusions, while directly computing optical flow between distant frames (i.e., larger displacements) often results in temporal inconsistency [8,75].\nMulti-frame flow estimation methods [27,29,52,70] can address some limitations of two-frame optical flow, but still struggle to handle long-range motion.\nFeature matching. While optical flow methods are typically intended to operate on consecutive frames, other techniques can estimate dense correspondences between distant pairs of video frames [41]. Several methods learn such correspondences in a self-or weakly-supervised manner [5,10,13,37,53,71,73,78] using cues like cycle consistency [28,74,83], while others [18,30,62,68,69] use stronger supervision signals such as geometric correspondences generated from 3D reconstruction pipelines [39,56]. However, pairwise matching approaches typically do not incorporate temporal context, which can lead to inconsistent tracking over long videos and poor occlusion handling. In contrast, our method produces smooth trajectories through occlusions.\nPixel-level long-range tracking. A notable recent approach, PIPs [23], estimates multi-frame trajectories through occlusions by leveraging context within a small temporal window (8 frames). However, to produce motion for videos longer than this temporal window, PIPs still must chain correspondences, a process that (1) is prone to drift and (2) will lose track of points that remain occluded beyond the 8-frame window. Concurrent to our work, several works develop learning-based methods for predicting long-range pixel-level tracks in a feedforward manner. MFT [46] learns to select the most reliable sequences of optical flows to perform longrange tracking. TAPIR [16] tracks points by employing a matching stage inspired by TAP-Net [15] and a refinement stage inspired by PIPs [23]. CoTracker [31] proposes a flexible and powerful tracking algorithm with a transformer architecture to track points throughout a video. Our contribution is complementary to these works: the output of any of these methods can be used as the input supervision when optimizing a global motion representation.\nVideo-based motion optimization. Most conceptually related to our approach are classical methods that optimize motion globally over an entire video [2,12,36,54,55,60,63,72]. Particle video, for instance, produces a set of semi-dense long-range trajectories (called particles) from initial optical flow fields [55]. However, it does not track through occlusions; an occluded entity will be treated as a different particle when it re-appears. Rubinstein et al. [54] further proposed a combinatorial assignment approach that can track through occlusion and generate longer trajectories. However, this method only produces semi-dense tracks for videos with simple motion, whereas our method estimates long-range motion for all pixels in general videos. Also related is Parti-cleSfM [82], which optimizes long-range correspondences from pairwise optical flows. Unlike our approach, Parti-cleSfM focuses on camera pose estimation within an SfM framework, where only correspondences from static regions are optimized, and dynamic objects are treated as outliers.\nNeural video representations. While our method shares similarities with recent methods that model videos using coordinate-based multi-layer perceptrons (MLPs) [44,58,65], prior work has primarily focused on problems such as novel view synthesis [38,40,47,48,77] and video decomposition [32,81]. In contrast, our work targets the challenge of dense, long-range motion estimation. Though some methods for dynamic novel view synthesis produce 2D motion as a by-product, these systems require known camera poses and the resulting motion is often erroneous [21]. Some dynamic reconstruction methods [9,76,79,80] can also produce 2D motion, but these are often object-centric with a focus on articulated objects. Alternatively, video decompositionbased representations such as Layered Neural Atlases [32] and Deformable Sprites [81] solve for a mapping between each frame and a global texture atlas. Frame-to-frame correspondence can be derived by inverting this mapping, but this process is expensive and unreliable. Furthermore, these methods are limited to representing videos using a limited number of layers with fixed ordering, restricting their ability to model complex, real-world videos.", "publication_ref": ["b3", "b41", "b0", "b55", "b58", "b16", "b12", "b42", "b56", "b66", "b5", "b6", "b23", "b74", "b19", "b24", "b25", "b60", "b65", "b7", "b74", "b26", "b28", "b51", "b69", "b40", "b4", "b9", "b12", "b36", "b52", "b70", "b72", "b77", "b27", "b73", "b82", "b17", "b29", "b61", "b67", "b68", "b38", "b55", "b22", "b45", "b15", "b14", "b22", "b30", "b1", "b11", "b35", "b53", "b54", "b59", "b62", "b71", "b54", "b53", "b81", "b43", "b57", "b64", "b37", "b39", "b46", "b47", "b76", "b31", "b80", "b20", "b8", "b75", "b78", "b79", "b31", "b80"], "figure_ref": [], "table_ref": []}, {"heading": "Overview", "text": "We propose a test-time optimization approach for estimating dense and long-range motion from a video sequence. Our method takes as input a collection of frames and pairwise noisy motion estimates (e.g., optical flow fields), and uses these to solve for a complete, globally consistent motion representation for the entire video. Once optimized, our representation can be queried with any pixel in any frame to produce a smooth, accurate motion trajectory across the full video. Our method identifies when points are occluded, and even tracks points through occlusions. In the following sections, we describe our underlying representation, dubbed OmniMotion (Sec. 4), then describe our optimization process (Sec. 5) for recovering this representation from a video.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "OmniMotion representation", "text": "As discussed in Sec. 1, classical motion representations, such as pairwise optical flow, lose track of objects when they are occluded, and can produce inconsistencies when correspondences are composed over multiple frames. To obtain accurate, consistent tracks even through occlusion, we therefore need a global motion representation, i.e., a data structure that encodes the trajectories of all points in a scene. One such global representation is a decomposition of a scene into a set of discrete, depth-separated layers [32,81]. However, most real-world scenes cannot be represented as a set of fixed, ordered layers: e.g., consider the simple case of an object rotating in 3D. At the other extreme is full 3D reconstruction that disentangles 3D scene geometry, camera pose and scene motion. This, however, is an extremely illposed problem. Thus, we ask: can we accurately track realworld motion without explicit dynamic 3D reconstruction?\nWe answer this question using our proposed representation, OmniMotion (illustrated in Fig. 2). OmniMotion represents the scene in a video as a canonical 3D volume that is mapped to local volumes for each frame through local-canonical bijections. The local-canonical bijections are parametrized as neural networks and capture both camera and scene motion without disentangling the two. As such, the video can be considered as a rendering of the resulting local volumes from a fixed, static camera.\nBecause OmniMotion does not explicitly disentangle camera and scene motion, the resulting representation is not a physically accurate 3D scene reconstruction. Instead we call it a quasi-3D representation. This relaxation of dynamic multi-view geometry allows us to sidestep ambiguities that make dynamic 3D reconstruction challenging. Yet we retain properties needed for consistent and accurate long-term tracking through occlusion: first, by establishing bijections between each local frame and a canonical frame, OmniMotion guarantees globally cycle-consistent 3D mappings across all local frames, which emulates the one-to-one correspondences between real-world, metric 3D reference frames. Second, OmniMotion retains information about all scene points that are projected onto each pixel, along with their relative depth ordering, enabling points to be tracked even when they are temporarily occluded from view.\nIn the following sections, we describe our quasi-3D canonical volume and 3D bijections, and then describe how they can be used to compute motion between any two frames.", "publication_ref": ["b31", "b80"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Canonical 3D volume", "text": "We represent a video's content using a canonical volume G that acts as a three-dimensional atlas of the observed scene. As in NeRF [44], we define a coordinate-based network F \u03b8 over G that maps each canonical 3D coordinate u \u2208 G to a density \u03c3 and color c. The density stored in G is key, as it tells us where the surfaces are in canonical space. Together with the 3D bijections, this allows us to track surfaces over multiple frames as well as reason about occlusion relationships. The color stored in G allows us to compute a photometric loss during optimization.", "publication_ref": ["b43"], "figure_ref": [], "table_ref": []}, {"heading": "3D bijections", "text": "We define a continuous bijective mapping T i that maps 3D points x i from each local coordinate frame L i to the canonical 3D coordinate frame as u = T i (x i ), where i is the frame index. Note that the canonical coordinate u is time-independent and can be viewed as a globally consistent \"index\" for a particular scene point or 3D trajectory across time. By composing these bijective mappings and their inverses, we can map a 3D point from one local 3D coordinate For a given : that map between each frame's local volume Li and the canonical volume G. Any local 3D location xi in frame i can be mapped to its corresponding canonical location u through T i , and then mapped back to another frame j as xj through the inverse mapping T \u22121 j . Each location u in G is associated with a color c and density \u03c3, computed using a coordinate-based MLP F \u03b8 . (b) To compute the corresponding 2D location for a given query point pi mapped from frame i to j, we shoot a ray into Li and sample a set of points {x k i } K k=1 , which are then mapped first to the canonical space to obtain their densities, and then to frame j to compute their corresponding local 3D locations {x k j } K k=1 . These points {x k j } K k=1 are then alpha-composited and projected to obtain the 2D corresponding locationpj.\n! Frame Frame j # ( , ) # ! ! \" ! # ! $ \" ! \" ! % # % $ \u2026 % \" \u2026 Alpha composite Project ! * # % canonical 3D volume(\nframe (L i ) to another (L j ):\nx j = T \u22121 j \u2022 T i (x i ).(1)\nBijective mappings ensure that the resulting correspondences between 3D points in individual frames are all cycle consistent, as they arise from the same canonical point.\nTo allow for expressive maps that can capture real-world motion, we parameterize these bijections as invertible neural networks (INNs). Inspired by recent work on homeomorphic shape modeling [35,49], we use Real-NVP [14] due to its simple formulation and analytic invertibility. Real-NVP builds bijective mappings by composing simple bijective transformations called affine coupling layers. An affine coupling layer splits the input into two parts; the first part is left unchanged, but is used to parametrize an affine transformation that is applied to the second part.\nWe modify this architecture to also condition on a perframe latent code \u03c8 i [35,49]. Then all invertible mappings T i are parameterized by the same invertible network M \u03b8 , but with different latent codes:\nT i (\u2022) = M \u03b8 (\u2022; \u03c8 i ).", "publication_ref": ["b34", "b48", "b13", "b34", "b48"], "figure_ref": [], "table_ref": []}, {"heading": "Computing frame-to-frame motion", "text": "Given this representation, we now describe how we can compute 2D motion for any query pixel p i in frame i. Intuitively, we \"lift\" the query pixel to 3D by sampling points on a ray, \"map\" these 3D points to a target frame j using bijections T i and T j , \"render\" these mapped 3D points from the different samples through alpha compositing, and finally \"project\" back to 2D to obtain a putative correspondence.\nSpecifically, since we assume that camera motion is subsumed by the local-canonical bijections T i , we simply use a fixed, orthographic camera. The ray at p i can then be defined as\nr i (z) = o i +zd, where o i = [p i , 0] and d = [0, 0, 1].\nWe sample K samples on the ray {x k i }, which are equivalent to appending a set of depth values {z k i } K k=1 to p i . Despite not being a physical camera ray, it captures the notion of multiple surfaces at each pixel and suffices to handle occlusion.\nNext we obtain densities and colors for these samples by mapping them to the canonical space and then querying the density network F \u03b8 . Taking the k-th sample x k i as an example, its density and color can be written as\n(\u03c3 k , c k ) = F \u03b8 (M \u03b8 (x k i ; \u03c8 i ))\n. We can also map each sample along the ray to a corresponding 3D location x k j in frame j (Eq. 1). We can now aggregate the correspondences x k j from all samples to produce a single correspondencex j . This aggregation is similar to how the colors of sample points are aggregated in NeRF: we use alpha compositing, with the alpha value for the k-th sample as \u03b1 k = 1 \u2212 exp(\u2212\u03c3 k ). We then computex j as:\nx j = K k=1 T k \u03b1 k x k j , where T k = k\u22121 l=1 (1 \u2212 \u03b1 l )(2)\nA similar process is used to composite c k to get the imagespace color\u0108 i for p i .x j is then projected using our stationary orthographic camera model to yield the predicted 2D corresponding locationp j for the query location p i .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Optimization", "text": "Our optimization process takes as input a video sequence and a collection of noisy correspondence predictions (from an existing method) as guidance, and generates a complete, globally consistent motion estimate for the entire video.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Collecting input motion data", "text": "For most of our experiments, we use RAFT [66] to compute input pairwise correspondence. We also experimented with another dense correspondence method, TAP-Net [15], and demonstrate in our evaluation that our approach consistently works well given different types of input correspondence. Taking RAFT as an example, we begin by exhaustively computing all pairwise optical flows. Since optical flow methods can produce significant errors under large displacements, we apply cycle consistency and appearance consistency checks to filter out spurious correspondences. We also optionally augment the flows through chaining, when deemed reliable. Additional details about our flow collection process are provided in the supplemental material. Despite the filtering, the (now incomplete) flow fields remain noisy and inconsistent. We now introduce our optimization method that consolidates these noisy, incomplete pairwise motion into complete and accurate long-range motion.", "publication_ref": ["b65", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Loss functions", "text": "Our primary loss function is a flow loss. We minimize the mean absolute error (MAE) between the predicted flo\u0175 f i\u2192j =p j \u2212 p i from our optimized representation and the supervising input flow f i\u2192j derived from running optical flow:\nL flo = fi\u2192j \u2208\u2126 f ||f i\u2192j \u2212 f i\u2192j || 1 (3)\nwhere \u2126 f is the set of all the filtered pairwise flows. In addition, we minimize a photometric loss defined as the mean squared error (MSE) between the predicted color\u0108 i and the observed color C i in the source video frame:\nL pho = (i,p)\u2208\u2126p ||\u0108 i (p) \u2212 C i (p)|| 2 2 (4)\nwhere \u2126 p is the set of all pixel locations over all frames. Last, to ensure temporal smoothness of the 3D motion estimated by M \u03b8 , we apply a regularization term that penalizes large accelerations. Given a sampled 3D location x i in frame i, we map it to frame i\u22121 and frame i+1 using Eq. 1, yielding 3D points x i\u22121 and x i+1 respectively, and then minimize 3D acceleration as in [38]:\nL reg = (i,x)\u2208\u2126x ||x i+1 + x i\u22121 \u2212 2x i || 1 (5)\nwhere \u2126 x is the union of local 3D spaces for all frames. Our final combined loss can be written as:\nL = L flo + \u03bb pho L pho + \u03bb reg L reg (6)\nwhere weights \u03bb control the relative importance of each term. The intuition behind this optimization is to leverage the bijections to a single canonical volume G, photo consistency, and the natural spatiotemporal smoothness provided by the coordinate-based networks M \u03b8 and F \u03b8 to reconcile inconsistent pairwise flow and fill in missing content in the correspondence graphs.", "publication_ref": ["b37"], "figure_ref": [], "table_ref": []}, {"heading": "Balancing supervision via hard mining", "text": "The exhaustive pairwise flow input maximizes the useful motion information available to the optimization stage. However, this approach, especially when coupled with the flow-filtering process, can result in an unbalanced collection of motion samples in dynamic regions. Rigid background regions typically have many reliable pairwise correspondences, while fast-moving and deforming foreground objects often have many fewer reliable correspondences after filtering, especially between distant frames. This imbalance can lead the network to focus entirely on dominant (simple) background motions, and ignore the challenging moving objects that represent a small portion of the supervisory signal.\nTo address this issue, we propose a simple strategy for mining hard examples during training. Specifically, we periodically cache flow predictions and compute error maps by calculating the Euclidean distance between the predicted and input flows. During optimization, we guide sampling such that regions with high errors are sampled more frequently. We compute these error maps on consecutive frames, where we assume our supervisory optical flow is most reliable. Please refer to the supplement for more details.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Implementation details", "text": "Network. Our mapping network M \u03b8 consists of six affine coupling layers. We apply positional encoding [44,65] with 4 frequencies to each layer's input coordinates before computing the scale and translation. We use a single 2-layer MLP with 256 channels implemented as a GaborNet [19] to compute the latent code \u03c8 i for each frame i. The input to this MLP is the time t i . The dimensionality of latent code \u03c8 i is 128. The canonical representation F \u03b8 is also implemented as a GaborNet, but with 3 layers of 512 channels.\nRepresentation. We normalize all pixel coordinates p i to the range [\u22121, 1], and set the near and far depth range to 0 and 2, defining a local 3D space for each frame as\n[\u22121, 1] 2 \u00d7 [0, 2].\nWhile our method can place content at arbitrary locations in the canonical space G, we initialize mapped canonical locations given by M \u03b8 to be roughly within a unit sphere to ensure well-conditioned input to the density/color network F \u03b8 . To improve numerical stability during training, we apply the contraction operation in mip-NeRF 360 [3] to canonical 3D coordinates u before passing them to F \u03b8 .\nTraining. We train our representation on each video sequence with Adam [33] for 200K iterations. In each training batch, we sample 256 correspondences from 8 pairs of images, for a total of 1024 correspondences. We sample K = 32 points on each ray using stratified sampling. More training details can be found in the supplemental material.", "publication_ref": ["b43", "b64", "b18", "b2", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Benchmarks", "text": "We evaluate our method on the TAP-Vid benchmark [15], which is designed to evaluate the performance of point tracking across long video clips. TAP-Vid consists of both realworld videos with accurate human annotations of point tracks and synthetic videos with perfect ground-truth point tracks. Each point track is annotated through the entire video, and is labeled as occluded when not visible.\nDatasets. We evaluate on the following datasets from TAP-Vid: 1) DAVIS, a real dataset of 30 videos from the DAVIS 2017 validation set [50], with clips ranging from 34-104 frames and an average of 21.7 point track annotations per video. 2) Kinetics, a real dataset of 1,189 videos each with 250 frames from the Kinetics-700-2020 validation set [11] with an average of 26.3 point track annotations per video. To make evaluation tractable for test-time optimization approaches like ours, we randomly sample a subset of 100 videos and evaluate all methods on this subset. 3) RGB-Stacking [34], a synthetic dataset of 50 videos each with 250 frames and 30 tracks. We exclude the synthetic Kubric dataset [22] as it is primarily intended for training. For quantitative evaluation, we adhere to the TAP benchmark protocol and evaluate all methods at 256\u00d7256 resolution, but all qualitative results are run at a higher resolution (480p).\nEvaluation metrics. Following the TAP-Vid benchmark, we report both the position and occlusion accuracy of predicted tracks. We also introduce a new metric measuring temporal coherence. Our evaluation metrics include:\n\u2022 < \u03b4 x avg measures the average position accuracy of visible points across 5 thresholds: 1, 2, 4, 8, and 16 pixels. The accuracy < \u03b4 x at each threshold \u03b4 x is the fraction of points that are within \u03b4 x pixels of their ground truth position.\n\u2022 Average Jaccard (AJ) evaluates both occlusion and position accuracy on the same thresholds as < \u03b4 x avg . It categorizes predicted point locations as true positives, false positives, and false negatives, and is defined as the ratio of true positives to all points. True positives are points within the threshold of visible ground truth points. False positives are points that are predicted as visible, but where the ground truth is occluded or beyond the threshold. False negatives are ground truth visible points that are predicted as occluded or are beyond the threshold.\n\u2022 Occlusion Accuracy (OA) evaluates the accuracy of the visibility/occlusion prediction at each frame.\n\u2022 Temporal Coherence (TC) evaluates the temporal coherence of the tracks by measuring the L 2 distance between the acceleration of groundtruth tracks and predicted tracks. The acceleration is measured as the flow difference between two adjacent frames for visible points.", "publication_ref": ["b14", "b49", "b10", "b33", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Baselines", "text": "We compare OmniMotion to various types of dense correspondence methods, including optical flow, feature matching, and multi-frame trajectory estimation as follows:\nRAFT [66] is a state-of-the-art two-frame flow method. We consider two ways to use RAFT to generate multi-frame trajectories at test time: 1) chaining RAFT predictions between consecutive frames into longer tracks, which we call RAFT-C, and 2) directly computing RAFT flow between any (non-adjacent) query and target frames (RAFT-D). When generating trajectories using RAFT-D, we always use the previous flow prediction as initialization for the current frame.\nPIPs [23] is a method for estimating multi-frame point trajectories that can handle occlusions. By default, the method uses a temporal window of 8 frames, and longer trajectories must be generated through chaining. We used the official implementation of PIPs to perform chaining. [5] uses a multi-scale contrastive random walk to learn space-time correspondences by encouraging cycle consistency across time. Similar to RAFT, we report both chained and direct correspondence computation as Flow-Walk-C and Flow-Walk-D, respectively. TAP-Net [15] uses a cost volume to predict the location of a query point in a single target frame, along with a scalar occlusion logit.", "publication_ref": ["b65", "b22", "b4", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Flow-Walk", "text": "Deformable Sprites [81] is a layer-based video decomposition method. Like our method, it uses a per-video test-time optimization. However, it does not directly produce frameto-frame correspondence, as the mapping from each frame to a canonical texture image is non-invertible. A nearest neighbor search in texture image space is required to find correspondence. Layered Neural Atlases [32] shares similarities to Deformable Sprites, but requires semantic segmentation masks as input, so we opt to compare to Deformable Sprites.\nPIPs, TAP-Net and Deformable Sprites directly predict occlusion, but RAFT and Flow-Walk do not. Therefore we follow prior work [15,78] and use a cycle consistency check with a threshold of 48 pixels to produce occlusion predictions for these methods. For our method, we detect occlusion by first mapping the query point to its corresponding 3D location in the target frame, then checking the transmittance of that 3D location in the target frame.", "publication_ref": ["b80", "b31", "b14", "b77"], "figure_ref": [], "table_ref": []}, {"heading": "Comparisons", "text": "Quantitative comparisons. We compare our method to baselines on the TAP-Vid benchmark in Tab. 1. Our method achieves the best position accuracy, occlusion accuracy, and   [50]. The leftmost image shows query points in the first frame, and the right three images show tracking results over time. Notably, our method tracks successfully through the occlusion events in swing and india, while baseline methods fail. Our method additionally detects occlusion (marked as cross \"+\") and gives plausible location estimates even when a point is occluded. Please refer to the supplemental video for better visualizations of tracking accuracy and coherence. temporal coherence consistently across different datasets. Our method works well with different input pairwise correspondences from RAFT and TAP-Net, and provides consistent improvements over both of these base methods.\nCompared to approaches that directly operate on (nonadjacent) pairs of query and target frames like RAFT-D, TAP-Net, and Flow-Walk-D, our method achieves significantly better temporal coherence due to our globally consistent representation. Compared to flow chaining methods like RAFT-C, PIPs, and Flow-Walk-C, our method has better tracking performance, especially on longer videos. Chaining methods accumulate errors over time and are not robust to oc-clusion. Although PIPs considers a wider temporal window (8 frames) for better occlusion handling, it fails to track a point if it stays occluded beyond the entire temporal window. In contrast, OmniMotion can track points through extended occlusions. Our method also outperforms the test-time optimization approach Deformable Sprites [81]. Deformable Sprites decomposes a video using a predefined two or three layers with fixed ordering and models the background as a 2D atlas with a per-frame homography, limiting its ability to fit to videos with complex camera and scene motion.\nQualitative comparisons. We compare our method qualitatively to our baselines in Fig. 3. We highlight our ability to   identify and track through (long) occlusion events while also providing plausible locations for points during occlusion, as well as handling large camera motion parallax. Please refer to the supplementary video for animated comparisons.\nMethod Kinetics DAVIS RGB-Stacking AJ \u2191 < \u03b4 x avg \u2191 OA \u2191 TC \u2193 AJ \u2191 < \u03b4 x avg \u2191 OA \u2191 TC \u2193 AJ \u2191 < \u03b4 x avg \u2191 OA \u2191 TC \u2193 RAFT-C [", "publication_ref": ["b49", "b80"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Ablations and analysis", "text": "Ablations. We perform ablations to verify the effectiveness of our design decisions in Tab. 2. No invertible is a model variant that replaces our invertible mapping network M \u03b8 with separate forward and backward mapping networks between local and canonical frames (i.e., without the strict cycle consistency guarantees of our proposed bijections).\nWhile we additionally add a cycle consistency loss for this ablation, it still fails to construct a meaningful canonical space, and can only represent simple motions of the static background. No photometric is a version that omits the photometric loss L pho ; the reduced performance suggests the importance of photoconsistency for refining motion estimates. Uniform sampling replaces our hard-mining sampling strategy with a uniform sampling strategy, which we found leads to an inability to capture fast motion.\nAnalysis. In Fig. 4, we show pseudo-depth maps generated from our model to demonstrate the learned depth ordering.\nNote that these maps do not correspond to physical depth, nonetheless, they demonstrate that using only photometric and flow signals, our method is able to sort out the relative ordering between different surfaces, which is crucial for tracking through occlusions. More ablations and analyses are provided in the supplemental material.", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Limitations", "text": "Like many motion estimation methods, our method struggles with rapid and highly non-rigid motion as well as thin structures. In these scenarios, pairwise correspondence methods can fail to provide enough reliable correspondences for our method to compute accurate global motion.\nIn addition, due to the highly non-convex nature of the underlying optimization problem, we observe that our optimization process can be sensitive to initialization for certain difficult videos. This can result in sub-optimal local minima, e.g., incorrect surface ordering or duplicated objects in canonical space that can sometimes be hard to correct through optimization.\nFinally, our method in its current form can be computationally expensive. First, the flow collection process involves computing all pairwise flows exhaustively, which scales quadratically with respect to the sequence length. However, we believe the scalability of this process can be improved by exploring more efficient alternatives to exhaustive matching, e.g., vocabulary trees or keyframe-based matching, drawing inspiration from the Structure from Motion and SLAM literature. Second, like other methods that utilize neural implicit representations [44], our method involves a relatively long optimization process. Recent research in this area [45,64] may help accelerate this process and allow further scaling to longer sequences.", "publication_ref": ["b43", "b44", "b63"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We proposed a new test-time optimization method for estimating complete and globally consistent motion for an entire video. We introduced a new video motion representation called OmniMotion which includes a quasi-3D canonical volume and per-frame local-canonical bijections. OmniMotion can handle general videos with varying camera setups and scene dynamics, and produce accurate and smooth longrange motion through occlusions. Our method achieves significant improvements over prior state-of-the-art methods both qualitatively and quantitatively.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Preparing pairwise correspondences", "text": "Our method uses pairwise correspondences from existing methods, such as RAFT [66] and TAP-Net [15], and consolidates them into dense, globally consistent, and accurate correspondences that span an entire video. As a preprocessing stage, we exhaustively compute all pairwise correspondences (i.e., between every pair of frames i and j) and filter them using cycle consistency and appearance consistency checks.\nWhen computing the flow field between a base frame i and a target frame j as i \u2192 j, we always use the flow prediction for the previous target frame (i \u2192 j \u2212 1) as initialization for the optical flow model (when possible). We find this improves flow predictions between distant frames. Still, the flow predictions between distant frames can contain significant errors, and therefore we filter out flow vector estimates with cycle consistency errors (i.e., forward-backward flow consistency error) greater than 3 pixels.\nDespite this filtering process, we still frequently observe a persistent type of error that remains undetected by cycle consistency checks. This type of spurious correspondence, illustrated in Fig. 5, occurs because flow networks can struggle to estimate motion for regions that undergo significant deformation between the two input frames, and instead opt to interpolate motion from the surrounding areas. In the example in Fig. 5, this leads to flow on the foreground person \"locking on\" to the background layer instead. This behavior results in incorrect flows that survive the cycle consistency check, since they are consistent with a secondary layer's motion (e.g., background motion). To address this issue, we additionally use an appearance check: we extract dense features for each pixel using DINO [10] and filter out correspondences whose features' cosine similarity is < 0.5. In practice, we apply the cycle consistency check for all pairwise flows and supplement it with an appearance check when the two frames are more than 3 frames apart. We found this filtering process consistently eliminates major errors in flow fields across different sequences without per-sequence tuning. The results of our filtering approach, after both cycle and appearance consistency checks, are illustrated in Fig. 6.\nOne drawback of such a filtering process is that it will also remove correct flows for regions that become occluded in the target frame. For certain correspondence methods (such as RAFT), including these motion signals during occlusion events can result in better final motion estimates. Therefore, we devise a simple strategy for detecting reliable flow in occluded regions. For each pixel, we compute its forward flow to a target frame (a), cycle flow (flow back to the source frame from the target pixel) (b), and a second forward flow (c). This process effectively amounts to a 2-pass cycle consistency check: the consistency between (a) and (b) forms a standard cycle consistency check, and the consistency between (b) and (c) forms a secondary, supplementary  one. We identify pixels where (a) and (b) are inconsistent but (b) and (c) are consistent and deem these to be occluded pixels. We found this approach effective in identifying reliable flows for occluded regions-particularly when the two frames are close to each other. Therefore, we allow these correspondences to bypass cycle consistency checks if they span a temporal distance of less than 3 frames. Our experiments use this added signal for the variant of our method that uses RAFT flow, but not for the TAP-Net variant, as we found the predicted correspondences from the latter were  less reliable near occlusion events. We can also optionally augment the supervising input flow by chaining sequences of correspondences that are deemed reliable (i.e., those that satisfy the cycle consistency and appearance consistency checks). This helps densify the set of correspondences, creating supervision between distant frames where the direct flow estimates were deemed unreliable and therefore discarded during filtering. We found this process to be beneficial especially for challenging sequences with rapid motion or large displacements, where optical flow estimates between non-adjacent frames are less reliable.", "publication_ref": ["b65", "b14", "b9"], "figure_ref": ["fig_4", "fig_4", "fig_5"], "table_ref": []}, {"heading": "B. Additional ablations", "text": "In addition to the ablations in the main paper, we provide the following ablations and report the results in Table 3: 1) Plain 2D: Rather than using a quasi-3D representation with bijections to model motion, we utilized a simple 8-layer MLP with 256 neurons that takes the query pixel location, query time, and target time as input and outputs the corresponding location in the target frame. Although we applied positional encoding with 8 frequencies to the input to enable better fitting, this ablation failed to capture the holistic motion of the video, instead only capturing simpler motion patterns for the rigid background. 2) No flow loss: we remove the flow loss and only rely on photometric information for training. We find this approach is effective only for sequences with small motion, where a photometric loss can provide useful signals to adjust motion locally. For sequences with relatively large motion, this method fails to provide correct results. 3) We also vary the number of samples K for each ray from 32 to 16 and 8. The resulting ablations, named #Samples K=8 and #Samples K=16, demonstrate that using a denser sampling strategy tends to produce better results.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "C. Additional implementation details", "text": "We provide additional implementation details below and will release our code upon acceptance.\nError map sampling. We cache the flow predictions generated by our model every 20k steps and use them to mine hard examples for effective training. Specifically, for each frame in the video sequence, we compute the optical flow between that frame and its subsequent frame, except for the final frame where we compute the flow between it and the previous frame. We then compute the L 2 distance between the predicted flow and supervising input flow, where each pixel in the video is now associated with a flow error. In each training batch, we randomly sampled half of the query pixels using weights proportional to the flow errors and the other half using uniform sampling weights.\nTraining details. In addition to the photometric loss L pho introduced in the main paper, we include an auxiliary loss term that supervises the relative color between a pair of pixels in a frame:\nL pgrad = \u2126p ||(\u0108 i (p 1 ) \u2212\u0108 i (p 2 )) \u2212 (C i (p 1 ) \u2212 C i (p 2 ))|| 1 (7) Here, (\u0108 i (p 1 ) \u2212\u0108 i (p 2 )\n) is the difference in predicted color between a pair of pixels, and (C i (p 1 ) \u2212 C i (p 2 )) is the corresponding difference between ground-truth observations. This loss is akin to spatial smoothness regularizations or gradient losses that supplement pixel reconstruction losses in prior work [32,51], but instead computed between pairs of randomly sampled, potentially distant pixels p 1 and p 2 , rather than between adjacent pixels. We apply the same gradient loss to the flow prediction as well. We found that including these gradient losses helps improve the spatial consistency of estimates, and more generally improves the training process. We also use distortion loss introduced in mip-NeRF 360 [3] to suppress floaters.\nWe train our network with the Adam optimizer with base learning rates of 3 \u00d7 10 \u22124 , 1 \u00d7 10 \u22124 , and 1 \u00d7 10 \u22123 for the density/color network F \u03b8 , the mapping network M \u03b8 , and the MLP that computes the latent code, respectively. We decrease the learning rate by a factor of 0.5 every 20k step. To select correspondences during training, we begin by sampling correspondences from pairs of frames with a maximum interval of 20, and gradually increase the window size during training. Specifically, we expand the window by one every 2k steps.\nIn our loss formulation, we compute the flow loss L flo as a weighted sum of the mean absolute error (MAE) between each pair of correspondences in a training batch. The weight is determined by the frame interval, and is given by w = 1/ cos(\u2206/N \u2032 \u2022 \u03c0/2), where \u2206 is the frame interval, and N \u2032 is the current window size. The coefficient \u03bb pho for the photometric loss initially starts at 0 and linearly increases to 10 over the first 50k steps of training. After 50k steps, \u03bb pho stays fixed at 10. This design is motivated by our observation that the photometric loss is not effective in fixing large motion errors early on in the training process, but is effective in refining the motion. The coefficient \u03bb reg for smoothness regularization is set to 20. We use the same set of network As mentioned in the main paper, this architecture is fully invertible, i.e., it can be queried in either direction, from (u, v, w) to (x, y, z) and vice-versa.\narchitecture and training hyperparameters when evaluating different datasets in the TAP-Net benchmark.\nWhen sampling on each ray, we use a stratified sampling strategy and sample K = 32 points on each ray between the near and far depth range. Additionally, when mapping a 3D location from one local volume to another, we encourage it to be mapped within our predefined depth range to avoid degenerate solutions.\nDuring training, we use alpha compositing to propagate the training signal to all samples along a ray. However, at inference time, we instead compute the corresponding location using the single sample with the largest alpha value, which we found to produce quantitatively similar but visually better results.\nNetwork architecture for M \u03b8 . We illustrate the architecture for our invertible network M \u03b8 that maps between local and canonical coordinate frames in Fig. 7. M \u03b8 is comprised of six affine coupling layers with alternating split patterns (only the first layer is highlighted in Fig. 7). The learnable component in each affine coupling layer is an MLP that computes a scale and a translation from a frame latent code \u03c8 i and the first part of the input coordinates. This scale and translation is then applied to the second part of the input coordinate. This process subsequently is repeated for each of the other coordinates. The MLP network in each affine coupling layer has 3 layers with 256 channels. We found that applying positional encoding [44] to the MLP's input coordinates improved its fitting ability, and we set the number of frequencies to 4.", "publication_ref": ["b31", "b50", "b2", "b43"], "figure_ref": ["fig_6", "fig_6"], "table_ref": []}, {"heading": "Deformable sprites evaluation. Because the Deformable", "text": "Sprites method defines directional mappings from image space to atlas space, we must approximate the inverses of these mappings in order to establish corresponding point estimates between pairs of frames. We do this by performing a nearest neighbor search: all points in the target frame are mapped to the atlas, and the closest atlas coordinate to the source point's mapping is chosen as the corresponding pixel. Furthermore, occlusion estimates are extracted using the following process: (1) initialize the layer assignment of source point tracks to the layer which has the higher opacity at the source frame, (2) at a given target frame index, denote the point as occluded if its originally assigned layer has lower opacity than the other layer.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgements. We thank Jon Barron, Richard Tucker, Vickie Ye, Zekun Hao, Xiaowei Zhou, Steve Seitz, Brian Curless, and Richard Szeliski for their helpful input and assistance. This work was supported in part by an NVIDIA academic hardware grant and by the National Science Foundation (IIS-2008313 and IIS-2211259). Qianqian Wang was supported in part by a Google PhD Fellowship.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Building Rome in a day", "journal": "Communications of the ACM", "year": "2011", "authors": "Sameer Agarwal; Yasutaka Furukawa; Noah Snavely; Ian Simon; Brian Curless; M Steven; Richard Seitz;  Szeliski"}, {"ref_id": "b1", "title": "Label propagation in video sequences", "journal": "", "year": "2010", "authors": "Vijay Badrinarayanan; Fabio Galasso; Roberto Cipolla"}, {"ref_id": "b2", "title": "Unbounded anti-aliased neural radiance fields", "journal": "", "year": "2022", "authors": "T Jonathan; Ben Barron; Dor Mildenhall;  Verbin; P Pratul; Peter Srinivasan;  Hedman"}, {"ref_id": "b3", "title": "Computer vision and image understanding", "journal": "", "year": "2008", "authors": "Herbert Bay; Andreas Ess; Tinne Tuytelaars; Luc Van Gool"}, {"ref_id": "b4", "title": "Learning pixel trajectories with multiscale contrastive random walks", "journal": "", "year": "2008", "authors": "Zhangxing Bian; Allan Jabri; Alexei A Efros; Andrew Owens"}, {"ref_id": "b5", "title": "A framework for the robust estimation of optical flow", "journal": "", "year": "1993", "authors": "J Michael; Padmanabhan Black;  Anandan"}, {"ref_id": "b6", "title": "Large displacement optical flow", "journal": "", "year": "2009", "authors": "Thomas Brox; Christoph Bregler; Jitendra Malik"}, {"ref_id": "b7", "title": "Large displacement optical flow: descriptor matching in variational motion estimation", "journal": "Trans. Pattern Analysis and Machine Intelligence", "year": "2010", "authors": "Thomas Brox; Jitendra Malik"}, {"ref_id": "b8", "title": "Neural surface reconstruction of dynamic scenes with monocular rgb-d camera", "journal": "", "year": "", "authors": "Hongrui Cai; Wanquan Feng; Xuetao Feng; Yan Wang; Juyong Zhang"}, {"ref_id": "b9", "title": "Emerging properties in self-supervised vision transformers", "journal": "", "year": "2021", "authors": "Mathilde Caron; Hugo Touvron; Ishan Misra; Herv\u00e9 J\u00e9gou; Julien Mairal; Piotr Bojanowski; Armand Joulin"}, {"ref_id": "b10", "title": "Quo vadis, action recognition? a new model and the kinetics dataset", "journal": "", "year": "2017", "authors": "Joao Carreira; Andrew Zisserman"}, {"ref_id": "b11", "title": "A video representation using temporal superpixels", "journal": "", "year": "2013", "authors": "Jason Chang; Donglai Wei; John W Fisher"}, {"ref_id": "b12", "title": "Superpoint: Self-supervised interest point detection and description", "journal": "", "year": "2018", "authors": "Daniel Detone; Tomasz Malisiewicz; Andrew Rabinovich"}, {"ref_id": "b13", "title": "Density estimation using Real NVP", "journal": "", "year": "2016", "authors": "Laurent Dinh; Jascha Sohl-Dickstein; Samy Bengio"}, {"ref_id": "b14", "title": "Tap-vid: A benchmark for tracking any point in a video", "journal": "", "year": "2006", "authors": "Carl Doersch; Ankush Gupta; Larisa Markeeva; Adria Recasens Continente; Kucas Smaira; Yusuf Aytar; Joao Carreira; Andrew Zisserman; Yi Yang"}, {"ref_id": "b15", "title": "Tapir: Tracking any point with per-frame initialization and temporal refinement", "journal": "", "year": "2023", "authors": "Carl Doersch; Yi Yang; Mel Vecerik; Dilara Gokay; Ankush Gupta; Yusuf Aytar; Joao Carreira; Andrew Zisserman"}, {"ref_id": "b16", "title": "Simultaneous localization and mapping: part i", "journal": "IEEE Robotics & Automation Magazine", "year": "2006", "authors": "Hugh Durrant-Whyte; Tim Bailey"}, {"ref_id": "b17", "title": "D2-net: A trainable cnn for joint description and detection of local features", "journal": "", "year": "2019", "authors": "Mihai Dusmanu; Ignacio Rocco; Tomas Pajdla; Marc Pollefeys; Josef Sivic; Akihiko Torii; Torsten Sattler"}, {"ref_id": "b18", "title": "Multiplicative filter networks", "journal": "", "year": "2021", "authors": "Rizal Fathony; Anit Kumar Sahu; Devin Willmott; J Zico Kolter"}, {"ref_id": "b19", "title": "Flownet: Learning optical flow with convolutional networks", "journal": "", "year": "2015", "authors": "Philipp Fischer; Alexey Dosovitskiy; Eddy Ilg; Philip H\u00e4usser; Caner Haz\u0131rba\u015f; Vladimir Golkov; Patrick Van Der; Daniel Smagt; Thomas Cremers;  Brox"}, {"ref_id": "b20", "title": "Monocular dynamic view synthesis: A reality check", "journal": "", "year": "", "authors": "Hang Gao; Ruilong Li; Shubham Tulsiani; Bryan Russell; Angjoo Kanazawa"}, {"ref_id": "b21", "title": "Kubric: A scalable dataset generator", "journal": "", "year": "2022", "authors": "Klaus Greff; Francois Belletti; Lucas Beyer; Carl Doersch; Yilun Du; Daniel Duckworth; J David; Dan Fleet; Florian Gnanapragasam; Charles Golemo;  Herrmann"}, {"ref_id": "b22", "title": "Particle video revisited: Tracking through occlusions using point trajectories", "journal": "Springer", "year": "2008", "authors": "W Adam; Zhaoyuan Harley; Katerina Fang;  Fragkiadaki"}, {"ref_id": "b23", "title": "Determining optical flow", "journal": "Artificial intelligence", "year": "1981", "authors": "K P Berthold; Brian G Horn;  Schunck"}, {"ref_id": "b24", "title": "Liteflownet: A lightweight convolutional neural network for optical flow estimation", "journal": "", "year": "2018", "authors": "Tak-Wai Hui; Xiaoou Tang; Chen Change Loy"}, {"ref_id": "b25", "title": "Flownet 2.0: Evolution of optical flow estimation with deep networks", "journal": "", "year": "2017", "authors": "Eddy Ilg; Nikolaus Mayer; Tonmoy Saikia; Margret Keuper; Alexey Dosovitskiy; Thomas Brox"}, {"ref_id": "b26", "title": "Multi-frame optical flow estimation using subspace constraints", "journal": "", "year": "1999", "authors": "Michal Irani"}, {"ref_id": "b27", "title": "Space-time correspondence as a contrastive random walk", "journal": "", "year": "2020", "authors": "Allan Jabri; Andrew Owens; Alexei Efros"}, {"ref_id": "b28", "title": "Unsupervised learning of multi-frame optical flow with occlusions", "journal": "", "year": "2018", "authors": "Joel Janai; Fatma Guney; Anurag Ranjan; Michael Black; Andreas Geiger"}, {"ref_id": "b29", "title": "Cotr: Correspondence transformer for matching across images", "journal": "", "year": "2021", "authors": "Wei Jiang; Eduard Trulls; Jan Hosang; Andrea Tagliasacchi; Kwang Moo Yi"}, {"ref_id": "b30", "title": "It is better to track together", "journal": "", "year": "", "authors": "Nikita Karaev; Ignacio Rocco; Benjamin Graham; Natalia Neverova; Andrea Vedaldi; Christian Rupprecht;  Cotracker"}, {"ref_id": "b31", "title": "Layered neural atlases for consistent video editing", "journal": "In ACM Trans. Graphics (SIGGRAPH Asia)", "year": "2021", "authors": "Yoni Kasten; Dolev Ofri; Oliver Wang; Tali Dekel"}, {"ref_id": "b32", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b33", "title": "Beyond pick-and-place: Tackling robotic stacking of diverse shapes", "journal": "", "year": "2021", "authors": "Alex X Lee; Coline Manon Devin; Yuxiang Zhou; Thomas Lampe; Konstantinos Bousmalis; Jost Tobias Springenberg; Arunkumar Byravan; Abbas Abdolmaleki; Nimrod Gileadi; David Khosid"}, {"ref_id": "b34", "title": "Cadex: Learning canonical deformation coordinate space for dynamic surface representation via neural homeomorphism", "journal": "", "year": "2022", "authors": "Jiahui Lei; Kostas Daniilidis"}, {"ref_id": "b35", "title": "Track to the future: Spatio-temporal video segmentation with long-range motion cues", "journal": "", "year": "2011", "authors": "Jos\u00e9 Lezama; Karteek Alahari; Josef Sivic; Ivan Laptev"}, {"ref_id": "b36", "title": "Joint-task self-supervised learning for temporal correspondence", "journal": "", "year": "2019", "authors": "Xueting Li; Sifei Liu; Xiaolong Shalini De Mello; Jan Wang; Ming-Hsuan Kautz;  Yang"}, {"ref_id": "b37", "title": "Neural scene flow fields for space-time view synthesis of dynamic scenes", "journal": "", "year": "2021", "authors": "Zhengqi Li; Simon Niklaus; Noah Snavely; Oliver Wang"}, {"ref_id": "b38", "title": "Megadepth: Learning singleview depth prediction from internet photos", "journal": "", "year": "2018", "authors": "Zhengqi Li; Noah Snavely"}, {"ref_id": "b39", "title": "Dynibar: Neural dynamic image-based rendering", "journal": "", "year": "", "authors": "Zhengqi Li; Qianqian Wang; Forrester Cole; Richard Tucker; Noah Snavely"}, {"ref_id": "b40", "title": "Sift flow: Dense correspondence across scenes and its applications", "journal": "Trans. Pattern Analysis and Machine Intelligence", "year": "2010", "authors": "Ce Liu; Jenny Yuen; Antonio Torralba"}, {"ref_id": "b41", "title": "Distinctive image features from scaleinvariant keypoints", "journal": "Int. J. of Computer Vision", "year": "2004", "authors": "G David;  Lowe"}, {"ref_id": "b42", "title": "An iterative image registration technique with an application to stereo vision", "journal": "", "year": "1981", "authors": "D Bruce; Takeo Lucas;  Kanade"}, {"ref_id": "b43", "title": "NeRF: Representing scenes as neural radiance fields for view synthesis", "journal": "Communications of the ACM", "year": "2005", "authors": "Ben Mildenhall; P Pratul; Matthew Srinivasan; Jonathan T Tancik; Ravi Barron; Ren Ramamoorthi;  Ng"}, {"ref_id": "b44", "title": "Instant neural graphics primitives with a multiresolution hash encoding", "journal": "In ACM Trans. Graphics (SIGGRAPH)", "year": "", "authors": "Thomas M\u00fcller; Alex Evans; Christoph Schied; Alexander Keller"}, {"ref_id": "b45", "title": "Long-term tracking of every pixel", "journal": "", "year": "", "authors": "Michal Neoral; Jon\u00e1\u0161\u0161er\u1ef3ch ; Ji\u0159\u00ed Matas;  Mft"}, {"ref_id": "b46", "title": "Nerfies: Deformable neural radiance fields", "journal": "", "year": "2021", "authors": "Keunhong Park; Utkarsh Sinha; Jonathan T Barron; Sofien Bouaziz; Dan B Goldman; M Steven; Ricardo Seitz;  Martin-Brualla"}, {"ref_id": "b47", "title": "HyperNeRF: A higherdimensional representation for topologically varying neural radiance fields", "journal": "In ACM Trans. Graphics (SIGGRAPH)", "year": "", "authors": "Keunhong Park; Utkarsh Sinha; Peter Hedman; Jonathan T Barron; Sofien Bouaziz; Dan B Goldman; Ricardo Martin-Brualla; Steven M Seitz"}, {"ref_id": "b48", "title": "Neural parts: Learning expressive 3d shape abstractions with invertible neural networks", "journal": "", "year": "2021", "authors": "Despoina Paschalidou; Angelos Katharopoulos; Andreas Geiger; Sanja Fidler"}, {"ref_id": "b49", "title": "The 2017 DAVIS challenge on video object segmentation", "journal": "", "year": "2008", "authors": "Jordi Pont-Tuset; Federico Perazzi; Sergi Caelles; Pablo Arbel\u00e1ez; Alex Sorkine-Hornung; Luc Van Gool"}, {"ref_id": "b50", "title": "Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer", "journal": "", "year": "2020", "authors": "Ren\u00e9 Ranftl; Katrin Lasinger; David Hafner; Konrad Schindler; Vladlen Koltun"}, {"ref_id": "b51", "title": "A fusion approach for multi-frame optical flow estimation", "journal": "", "year": "2019", "authors": "Zhile Ren; Orazio Gallo; Deqing Sun; Ming-Hsuan Yang; Erik B Sudderth; Jan Kautz"}, {"ref_id": "b52", "title": "Neighbourhood consensus networks", "journal": "", "year": "2018", "authors": "Ignacio Rocco; Mircea Cimpoi; Relja Arandjelovi\u0107; Akihiko Torii; Tomas Pajdla; Josef Sivic"}, {"ref_id": "b53", "title": "Towards longer long-range motion trajectories", "journal": "", "year": "2012", "authors": "Michael Rubinstein; Ce Liu; William T Freeman"}, {"ref_id": "b54", "title": "Particle video: Long-range motion estimation using point trajectories", "journal": "Int. J. of Computer Vision", "year": "2008", "authors": "Peter Sand; Seth Teller"}, {"ref_id": "b55", "title": "Structurefrom-motion revisited", "journal": "", "year": "2016", "authors": "L Johannes; Jan-Michael Schonberger;  Frahm"}, {"ref_id": "b56", "title": "Good features to track", "journal": "", "year": "1994", "authors": "Jianbo Shi"}, {"ref_id": "b57", "title": "Implicit neural representations with periodic activation functions", "journal": "", "year": "", "authors": "Vincent Sitzmann; Julien Martel; Alexander Bergman; David Lindell; Gordon Wetzstein"}, {"ref_id": "b58", "title": "Modeling the world from internet photo collections", "journal": "Int. J. of Computer Vision", "year": "2008", "authors": "Noah Snavely; M Steven; Richard Seitz;  Szeliski"}, {"ref_id": "b59", "title": "Layered image motion with explicit occlusions, temporal consistency, and depth ordering", "journal": "", "year": "2010", "authors": "Deqing Sun; Erik Sudderth; Michael Black"}, {"ref_id": "b60", "title": "Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume", "journal": "", "year": "2018", "authors": "Deqing Sun; Xiaodong Yang; Ming-Yu Liu; Jan Kautz"}, {"ref_id": "b61", "title": "Loftr: Detector-free local feature matching with transformers", "journal": "", "year": "2021", "authors": "Jiaming Sun; Zehong Shen; Yuang Wang; Hujun Bao; Xiaowei Zhou"}, {"ref_id": "b62", "title": "Dense point trajectories by GPU-accelerated large displacement optical flow", "journal": "Springer", "year": "2010", "authors": "Narayanan Sundaram; Thomas Brox; Kurt Keutzer"}, {"ref_id": "b63", "title": "Block-nerf: Scalable large scene neural view synthesis", "journal": "", "year": "", "authors": "Matthew Tancik; Vincent Casser; Xinchen Yan; Sabeek Pradhan; Ben Mildenhall; P Pratul; Jonathan T Srinivasan; Henrik Barron;  Kretzschmar"}, {"ref_id": "b64", "title": "Fourier features let networks learn high frequency functions in low dimensional domains", "journal": "", "year": "2020", "authors": "Matthew Tancik; Pratul Srinivasan; Ben Mildenhall; Sara Fridovich-Keil; Nithin Raghavan; Utkarsh Singhal; Ravi Ramamoorthi; Jonathan Barron; Ren Ng"}, {"ref_id": "b65", "title": "Raft: Recurrent all-pairs field transforms for optical flow", "journal": "", "year": "2006", "authors": "Zachary Teed; Jia Deng"}, {"ref_id": "b66", "title": "Detection and tracking of point", "journal": "Int. J. of Computer Vision", "year": "1991", "authors": "Carlo Tomasi; Takeo Kanade"}, {"ref_id": "b67", "title": "Glu-net: Global-local universal network for dense flow and correspondences", "journal": "", "year": "2020", "authors": "Prune Truong; Martin Danelljan; Radu Timofte"}, {"ref_id": "b68", "title": "Learning accurate dense correspondences and when to trust them", "journal": "", "year": "2021", "authors": "Prune Truong; Martin Danelljan; Luc Van Gool; Radu Timofte"}, {"ref_id": "b69", "title": "Modeling temporal coherence for optical flow", "journal": "", "year": "2011", "authors": "Sebastian Volz; Andres Bruhn; Levi Valgaerts; Henning Zimmer"}, {"ref_id": "b70", "title": "Tracking emerges by colorizing videos", "journal": "", "year": "2018", "authors": "Carl Vondrick; Abhinav Shrivastava; Alireza Fathi; Sergio Guadarrama; Kevin Murphy"}, {"ref_id": "b71", "title": "Layered representation for motion analysis", "journal": "", "year": "1993", "authors": "Y A John;  Wang;  Edward H Adelson"}, {"ref_id": "b72", "title": "Learning feature descriptors using camera pose supervision", "journal": "", "year": "2020", "authors": "Qianqian Wang; Xiaowei Zhou; Bharath Hariharan; Noah Snavely"}, {"ref_id": "b73", "title": "Learning correspondence from the cycle-consistency of time", "journal": "", "year": "2019", "authors": "Xiaolong Wang; Allan Jabri; Alexei A Efros"}, {"ref_id": "b74", "title": "DeepFlow: Large displacement optical flow with deep matching", "journal": "", "year": "2013", "authors": "Philippe Weinzaepfel; Jerome Revaud; Zaid Harchaoui; Cordelia Schmid"}, {"ref_id": "b75", "title": "Casa: Category-agnostic skeletal animal reconstruction", "journal": "", "year": "", "authors": "Yuefan Wu; Zeyuan Chen; Shaowei Liu; Zhongzheng Ren; Shenlong Wang"}, {"ref_id": "b76", "title": "Space-time neural irradiance fields for free-viewpoint video", "journal": "", "year": "2021", "authors": "Wenqi Xian; Jia-Bin Huang; Johannes Kopf; Changil Kim"}, {"ref_id": "b77", "title": "Rethinking self-supervised correspondence learning: A video frame-level similarity perspective", "journal": "", "year": "2021", "authors": "Jiarui Xu; Xiaolong Wang"}, {"ref_id": "b78", "title": "Lasr: Learning articulated shape reconstruction from a monocular video", "journal": "", "year": "2021", "authors": "Gengshan Yang; Deqing Sun; Varun Jampani; Daniel Vlasic; Forrester Cole; Huiwen Chang; Deva Ramanan; T William; Ce Freeman;  Liu"}, {"ref_id": "b79", "title": "Viser: Videospecific surface embeddings for articulated 3d shape reconstruction", "journal": "", "year": "2021", "authors": "Gengshan Yang; Deqing Sun; Varun Jampani; Daniel Vlasic; Forrester Cole; Ce Liu; Deva Ramanan"}, {"ref_id": "b80", "title": "Deformable sprites for unsupervised video decomposition", "journal": "", "year": "2008", "authors": "Vickie Ye; Zhengqi Li; Richard Tucker; Angjoo Kanazawa; Noah Snavely"}, {"ref_id": "b81", "title": "ParticleSfM: Exploiting dense point trajectories for localizing moving cameras in the wild", "journal": "", "year": "", "authors": "Wang Zhao; Shaohui Liu; Hengkai Guo; Wenping Wang; Yong-Jin Liu"}, {"ref_id": "b82", "title": "Learning dense correspondence via 3d-guided cycle consistency", "journal": "", "year": "2016", "authors": "Tinghui Zhou; Philipp Krahenbuhl; Mathieu Aubry; Qixing Huang; Alexei A Efros"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Method overview. (a) Our OmniMotion representation is comprised of a canonical 3D volume G and a set of bijections T i", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Qualitative comparison of our method and baselines on DAVIS [50]. The leftmost image shows query points in the first frame, and", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Pseudo-depth maps extracted from our representation, where blue indicates closer objects and red indicates further.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure 5: Erroneous correspondences after cycle consistency check. The red bounding box highlights a common type of incorrect correspondences from flow networks like RAFT [66] that remains undetected by cycle consistency check. The left images are query frames with query points and the right images are target frames with the corresponding predictions. Only correspondences on the foreground object are shown for better clarity.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 6 :6Figure6: Correspondences from RAFT[66] after both cycle and appearance checks. The left column shows a single query frame, and the right column displays target frames with increasing frame distances to the query frame from top to bottom. The filtered correspondences are reliable without significant errors.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 7 :7Figure 7: Network architecture for the mapping network M \u03b8 . We show the first affine coupling layer, which is representative of the subsequent layers, except for the different splitting patterns used.As mentioned in the main paper, this architecture is fully invertible, i.e., it can be queried in either direction, from (u, v, w) to (x, y, z) and vice-versa.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Quantitative comparison of our method and baselines on the TAP-Vid benchmark[15]. We refer to our method as Ours, and present two variants, Ours (TAP-Net) and Ours (RAFT), which are optimized using input pairwise correspondences from TAP-Net[15] and RAFT[66], respectively. Both Ours and Deformable Sprites[81] estimate global motion via test-time optimization on each individual video, while all other methods estimate motion locally in a feed-forward manner. Our method notably improves the quality of the input correspondences, achieving the best position accuracy, occlusion accuracy, and temporal coherence among all methods tested.", "figure_data": "MethodAJ \u2191 < \u03b4 x avg \u2191 OA \u2191 TC \u2193No invertible12.521.476.50.97No photometric42.358.384.10.83Uniform sampling 47.861.883.60.88Full51.767.585.3 0.74"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Ablation study on DAVIS[50].", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "! Frame Frame j # ( , ) # ! ! \" ! # ! $ \" ! \" ! % # % $ \u2026 % \" \u2026 Alpha composite Project ! * # % canonical 3D volume(", "formula_coordinates": [4.0, 66.62, 78.97, 468.16, 130.87]}, {"formula_id": "formula_1", "formula_text": "x j = T \u22121 j \u2022 T i (x i ).(1)", "formula_coordinates": [4.0, 127.39, 352.55, 159.64, 13.38]}, {"formula_id": "formula_2", "formula_text": "T i (\u2022) = M \u03b8 (\u2022; \u03c8 i ).", "formula_coordinates": [4.0, 159.67, 569.15, 76.09, 9.68]}, {"formula_id": "formula_3", "formula_text": "r i (z) = o i +zd, where o i = [p i , 0] and d = [0, 0, 1].", "formula_coordinates": [4.0, 319.05, 344.17, 210.37, 9.68]}, {"formula_id": "formula_4", "formula_text": "(\u03c3 k , c k ) = F \u03b8 (M \u03b8 (x k i ; \u03c8 i ))", "formula_coordinates": [4.0, 308.86, 439.81, 236.25, 22.74]}, {"formula_id": "formula_5", "formula_text": "x j = K k=1 T k \u03b1 k x k j , where T k = k\u22121 l=1 (1 \u2212 \u03b1 l )(2)", "formula_coordinates": [4.0, 336.81, 553.7, 208.97, 30.55]}, {"formula_id": "formula_6", "formula_text": "L flo = fi\u2192j \u2208\u2126 f ||f i\u2192j \u2212 f i\u2192j || 1 (3)", "formula_coordinates": [5.0, 102.37, 380.97, 184.66, 20.84]}, {"formula_id": "formula_7", "formula_text": "L pho = (i,p)\u2208\u2126p ||\u0108 i (p) \u2212 C i (p)|| 2 2 (4)", "formula_coordinates": [5.0, 97.08, 462.44, 189.95, 22.6]}, {"formula_id": "formula_8", "formula_text": "L reg = (i,x)\u2208\u2126x ||x i+1 + x i\u22121 \u2212 2x i || 1 (5)", "formula_coordinates": [5.0, 89.25, 587.22, 197.78, 20.56]}, {"formula_id": "formula_9", "formula_text": "L = L flo + \u03bb pho L pho + \u03bb reg L reg (6)", "formula_coordinates": [5.0, 105.68, 648.94, 181.35, 9.81]}, {"formula_id": "formula_10", "formula_text": "[\u22121, 1] 2 \u00d7 [0, 2].", "formula_coordinates": [5.0, 308.86, 574.22, 64.75, 10.31]}, {"formula_id": "formula_11", "formula_text": "Method Kinetics DAVIS RGB-Stacking AJ \u2191 < \u03b4 x avg \u2191 OA \u2191 TC \u2193 AJ \u2191 < \u03b4 x avg \u2191 OA \u2191 TC \u2193 AJ \u2191 < \u03b4 x avg \u2191 OA \u2191 TC \u2193 RAFT-C [", "formula_coordinates": [8.0, 58.35, 76.93, 478.69, 38.76]}, {"formula_id": "formula_12", "formula_text": "L pgrad = \u2126p ||(\u0108 i (p 1 ) \u2212\u0108 i (p 2 )) \u2212 (C i (p 1 ) \u2212 C i (p 2 ))|| 1 (7) Here, (\u0108 i (p 1 ) \u2212\u0108 i (p 2 )", "formula_coordinates": [14.0, 308.86, 250.14, 236.92, 44.29]}], "doi": ""}