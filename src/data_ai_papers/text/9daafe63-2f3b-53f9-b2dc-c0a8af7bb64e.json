{"title": "A Bound on the Label Complexity of Agnostic Active Learning", "authors": "Steve Hanneke", "pub_date": "", "abstract": "We study the label complexity of pool-based active learning in the agnostic PAC model. Specifically, we derive general bounds on the number of label requests made by the A 2 algorithm proposed by Balcan, Beygelzimer & Langford (Balcan et al., 2006). This represents the first nontrivial general-purpose upper bound on label complexity in the agnostic PAC model.", "sections": [{"heading": "Introduction", "text": "In active learning, a learning algorithm is given access to a large pool of unlabeled examples, and is allowed to request the label of any particular example from that pool. The objective is to learn an accurate classifier while requesting as few labels as possible. This contrasts with passive (semi)supervised learning, where the examples to be labeled are chosen randomly. In comparison, active learning can often significantly decrease the work load of human annotators by more carefully selecting which examples from the unlabeled pool should be labeled. This is of particular interest for learning tasks where unlabeled examples are available in abundance, but labeled examples require significant effort to obtain.\nIn the passive learning literature, there are well-known bounds on the number of training examples necessary and sufficient to learn a near-optimal classifier with high probability (i.e., the sample complexity) (Vapnik, 1998;Blumer et al., 1989;Kulkarni, 1989;Benedek & Itai, 1988;Long, 1995). This quantity depends largely on the VC dimension of the concept space being learned (in a distribution-independent analysis) or the metric entropy (in a distribution-dependent analysis). However, significantly less is presently known about the analogous quantity for active learning: namely, the label complexity, or number of label requests that are necessary and sufficient to learn. This knowledge gap is especially marked in the agnostic learning setting, where class labels can be noisy, and we have no assumption about the amount or type of noise. Building a thorough understanding of label complexity, along with the quantities on which it depends, seems essential to fully exploit the potential of active learning.\nIn the present paper, we study the label complexity by way of bounding the number of label requests made by a recently proposed active learning algorithm, A 2 (Balcan et al., 2006), which provably learns in the agnostic PAC model. The bound we find for this algorithm depends critically on a particular quantity, which we call the disagreement coefficient, depending on the concept space and example distribution. This quantity is often simple to calculate or bound for many concept spaces. Although we find that the bound we derive is not always tight for the label complexity, it represents a significant step forward, since it is the first nontrivial general-purpose bound on label complexity in the agnostic PAC model. The rest of the paper is organized as follows. In Section 2, we briefly review some of the related literature, to place the present work in context. In Section 3, we continue with the introduction of definitions and notation. Section 4 discusses a variety of simple examples to help build intuition. Moving on in Section 5, we state and prove the main result of this paper: an upper bound on the number of label requests made by A 2 , based on the disagreement coefficient. Following this, in Section 6, we prove a lower bound for A 2 with the same basic dependence on disagreement coefficient. We conclude in Section 7 with some open problems.", "publication_ref": ["b11", "b3", "b7", "b2", "b10", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Background", "text": "The recent literature on the label complexity of active learning has been bringing us steadily closer to understanding the nature of this problem. Within that literature, there is a mix of positive and negative results, as well as a wealth of open problems.\nWhile studying the noise-free (realizable) setting, Dasgupta defines a quantity \u03c1 called the splitting index (Dasgupta, 2005). \u03c1 is dependent on the concept space, data distribution, and a (new) parameter \u03c4 he defines, as well as the target function itself. It essentially quantifies how easy it is to reduce the diameter of the concept space. He finds that under the assumption that there is no noise, roughly\u00d5( d \u03c1 ) label requests are sufficient (where d is VC dimension), and \u2126( 1 \u03c1 ) are necessary for learning (for respectively appropriate \u03c4 values). Thus, it appears that something like splitting index may be an important quantity to consider when bounding the label complexity. However, at present the only published analysis using splitting index is restricted to the noise-free (realizable) case. Additionally, one can construct simple examples where the splitting index is O(1) (for \u03c4 = O(\u01eb 2 )), but agnostic learning requires \u2126 1 \u01eb label requests (even when the noise rate is zero). See Appendix A for an example of this. Thus, agnostic active learning seems to be a fundamentally more difficult problem than realizable active learning.\nIn studying the possibility of active learning in the presence of arbitrary classification noise, Balcan, Beygelzimer, & Langford propose the A 2 algorithm (Balcan et al., 2006). The strategy behind A 2 is to induce confidence intervals for the error rates of all concepts, and remove any concepts whose estimated error rate is larger than the smallest estimate to a statistically significant extent. This guarantees that with high probability we do not remove the best classifier in the concept space. The key observation that sometimes leads to improvements over passive learning is that, since we are only interested in comparing the error estimates, we do not need to request the label of any example whose label is not in dispute among the remaining classifiers. Balcan et al. analyze the number of label requests A 2 makes for some example concept spaces and distributions (notably linear separators under the uniform distribution on the unit sphere). However, other than fallback guarantees, they do not derive a general bound on the number of label requests, applicable to any concept space and distribution. This is the focus of the present paper.\nIn addition to the above results, there are a number of known lower bounds, than which there cannot be a learning algorithm guarateeing a number of label requests smaller. In particular, Kulkarni proves that, even if we allow arbitrary binary-valued queries and there is no noise, any algorithm that learns to accuracy 1 \u2212 \u01eb can guarantee no better than \u2126(log N (2\u01eb)) queries (Kulkarni et al., 1993), where N (2\u01eb) is the size of a minimal 2\u01eb-cover (defined below). Another known lower bound is due to K\u00e4\u00e4ri\u00e4inen, who proves that in agnostic active learning, for most nontrivial concept spaces and distributions, if the noise rate is \u03bd, then any algorithm that with probability 1 \u2212 \u03b4 outputs a classifier with error at most \u03bd + \u01eb can guarantee no better than \u2126 \u03bd 2 \u01eb 2 log 1 \u03b4 label requests (K\u00e4\u00e4ri\u00e4inen, 2006). In particular, these lower bounds imply that we can reasonably expect even the tightest general upper bounds on the label complexity to have some term related to log N (\u01eb) and some term related to \u03bd 2 \u01eb 2 log 1 \u03b4 .", "publication_ref": ["b4", "b0", "b8", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Notation and Definitions", "text": "Let X be an instance space, comprising all possible examples we may ever encounter. C is a set of measurable functions h : X \u2192 {\u22121, 1}, known as the concept space. D XY is any probability distribution on X \u00d7 {\u22121, 1}. In the active learning setting, we draw (X, Y ) \u223c D XY , but the Y value is hidden from the learning algorithm until requested.\nFor convenience, we will abuse notation by saying X \u223c D, where D is the marginal distribution of D XY over X ; we then say the learning algorithm (optionally) requests the label Y of X (which was implicitly sampled at the same time as X); we may sometimes denote this label Y by Oracle(X). For any h \u2208 C and distribution D \u2032 over X \u00d7 {\u22121, 1}, let er D \u2032 (h) = Pr (X,Y )\u223cD \u2032 {h(X) = Y }, and for S = {(x 1 , y 1 ), (x 2 , y 2 ), . . . ,\n(x m , y m )} \u2208 (X \u00d7 {\u22121, 1}) m , er S (h) = 1 m m i=1 |h(x i ) \u2212 y i |/2\n. When D \u2032 = D XY (the distribution we are learning with respect to), we abbreviate this by er(h) = er DXY (h). The noise rate, denoted \u03bd, is defined as \u03bd = inf h\u2208C er(h). Our objective in agnostic active learning is to, with probability \u2265 1\u2212\u03b4, output a classifier h with er(h) \u2264 \u03bd +\u01eb without making many label requests.\nLet \u03c1 D (\u2022, \u2022) be the pseudo-metric on C induced by D, s.t. \u2200h, h \u2032 \u2208 C, \u03c1 D (h, h \u2032 ) = Pr X\u223cD {h(X) = h \u2032 (X)}. An \u01eb-cover of C with respect to D is any set V \u2286 C such that \u2200h \u2208 C, \u2203h \u2032 \u2208 V : \u03c1 D (h, h \u2032 ) \u2264 \u01eb. We additionally let N (\u01eb) denote the size of a minimal \u01eb-cover of C with respect to D. It is known that N (\u01eb) < 2 2e \u01eb ln 2e \u01eb d ,\nwhere d is the VC dimension of C (Haussler, 1992). To focus on learnable cases, we assume d < \u221e.\nDefinition 1. For a set V \u2286 C, define the region of disagreement\nDIS(V ) = {x \u2208 X |\u2203h 1 , h 2 \u2208 V : h 1 (x) = h 2 (x)}. Definition 2. The disagreement rate \u2206(V ) of a set V \u2286 C is defined as \u2206(V ) = Pr X\u223cD {X \u2208 DIS(V )}. Definition 3. For h \u2208 C, r > 0, let B(h, r) = {h \u2032 \u2208 C : \u03c1 D (h \u2032 , h) \u2264 r}\nand define the disagreement rate at radius r\n\u2206 r = sup h\u2208C \u2206(B(h, r)). Definition 4. The disagreement coefficient is the in- fimum value of \u03b8 > 0 such that \u2200r > \u03bd + \u01eb, \u2206 r \u2264 \u03b8r.\nThe disagreement coefficient plays a critical role in the bounds of the following sections, which are increasing in this \u03b8. Roughly speaking, it quantifies how quickly the region of disagreement can grow as a function of the radius of the version space.", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}, {"heading": "Examples", "text": "The canonical example of the potential improvements in label complexity of active over passive learning is the thresholds concept space. Specifically, consider the concept space of thresholds t z on the interval\n[0, 1] (for z \u2208 [0, 1]), such that t z (x) = +1 iff x \u2265 z. Further- more, suppose D is uniform on [0, 1]. In this case, it is clear that the disagreement coefficient is at most 2, since the region of disagreement of B(t z , r) is roughly {x \u2208 [0, 1] : |x \u2212 z| \u2264 r}.\nThat is, since the disagreement region grows at rate 1 in two disjoint directions as r increases, the disagreement coefficient \u03b8 = 2.\nAs a second example, consider the disagreement coefficient for intervals on [0, 1]. As before, let X = [0, 1] and D be uniform, but this time C is the set of intervals\nI [a,b] such that for x \u2208 [0, 1], I [a,b] (x) = +1 iff x \u2208 [a, b] (for a, b \u2208 [0, 1], a \u2264 b).\nIn contrast to thresholds, the space of intervals serves as a canonical example of situations where active learning does not help compared to passive learning. This fact clearly shows itself in the disagreement coefficient, which is 1 \u03bd+\u01eb here, since \u2206 r = 1 for all r > \u03bd + \u01eb. To see this, note that the set B(I [0,0] , r) contains all concepts of the form I [a,a] . Note that 1 \u03bd+\u01eb is the largest possible value for \u03b8.\nAn interesting extension of the intervals example is the space of p-intervals, or all intervals I [a,b] such that b \u2212 a \u2265 p \u2208 ((\u03bd + \u01eb)/2, 1/8). These spaces span the range of difficulty, with active learning becoming easier as p increases. This is reflected in the \u03b8 value, since here \u03b8 = 1 2p . When r < 2p, every interval in B(I [a,b] , r) has its lower and upper boundaries within r of a and b, respectively; thus, \u2206 r \u2264 4r. However, when r \u2265 2p, every interval of width p is in B(I [0,p] , r), so \u2206 r = 1.\nAs an example that takes a (small) step closer to realistic learning scenarios, consider the following theorem. Theorem 1. If X is the surface of the origin-centered unit sphere in R d for d > 2, C is the space of homogeneous linear separators 1 , and D is the uniform distribution on X , then the disagreement coefficient \u03b8 satisfies\n1 4 min \u03c0 \u221a d, 1 \u03bd + \u01eb \u2264 \u03b8 \u2264 min \u03c0 \u221a d, 1 \u03bd + \u01eb .\nProof. First we represent the concepts in C as weight vectors w \u2208 R d in the usual way. For w 1 , w 2 \u2208 C, by examining the projection of D onto the subspace spanned by {w 1 , w 2 }, we see that \u03c1 D (w 1 , w 2 ) = arccos(w1\u2022w2) \u03c0\n. Thus, for any w \u2208 C and r \u2264 1/2, B(w, r) = {w \u2032 : w \u2022 w \u2032 \u2265 cos(\u03c0r)}. Some simple trigonometry gives us that\nDIS(B(w, r)) = {x \u2208 X : |x \u2022 w| \u2264 sin(\u03c0r)}. Let A d = 2\u03c0 d/2 \u0393( d 2 )\ndenote the surface area of the unit sphere in R d , and let\nC d (h) = 1 2 A d I 2h\u2212h 2 d\u22121 2\n, 1 2 denote the surface area of a height-h spherical cap (Li, 2011), where\nI x (a, b) = \u0393(a+b) \u0393(a)\u0393(b) x 0 t a\u22121 (1 \u2212 t) b\u22121 dt = \u0393(a+b) a\u0393(a)\u0393(b) x a 0 1 \u2212 u 1/a b\u22121 du is the regularized in- complete beta function. Then we can express \u2206 r as \u2206 r = 1\u2212 2C d (1 \u2212 sin(\u03c0r)) A d = 1\u2212I cos 2 (\u03c0r) d \u2212 1 2 , 1 2 . As I x (a, b) = 1\u2212I 1\u2212x (b, a) and \u0393 1 2 = \u221a \u03c0, this equals 2\u0393 d 2 \u221a \u03c0\u0393 d\u22121 2 sin(\u03c0r) 0 1 \u2212 x 2 d\u22123 2 dx. ( * ) As d 3 \u2264 2\u0393( d 2 ) \u221a \u03c0\u0393( d\u22121 2 ) \u2264 \u221a d \u2212 2, we see ( * ) is at most \u221a d \u2212 2 sin(\u03c0r) \u2264 \u221a d\u03c0r.\nFor the lower bound, \u2206 1/2 = 1 \u21d2 \u03b8 \u2265 min 2, 1 \u03bd+\u01eb , so we need only consider \u03bd + \u01eb < 1 8 . Supposing \u03bd + \u01eb < r < 1 8 , we have that ( * ) is at least\nd 3 sin(\u03c0r) 0 1 \u2212 x 2 d 2 dx \u2265 \u03c0 3 sin(\u03c0r) 0 d \u03c0 e \u2212d\u2022x 2 dx \u2265 1 2 min 1 2 , \u221a d sin(\u03c0r) \u2265 1 4 min 1, \u221a d\u03c0r .\nGiven knowledge of the disagreement coefficient for C under D, the following lemma allows us to extend this to a bound for any D \u2032 \u03bb-close to D. The proof is straightforward, and left as an exercise.\nInput: concept space C, accuracy parameter \u01eb \u2208 (0, 1), confidence parameter \u03b4 \u2208 (0, 1)\nOutput: classifier\u0125 \u2208 C Letn = log 2 64 \u01eb 2 d ln 8 \u01eb + ln 8 \u01eb\u03b4 log 2 4 \u01eb , and let \u03b4 \u2032 = \u03b4/n 0. V 0 \u2190 C, S 0 \u2190 \u2205, i \u2190 0, j 1 \u2190 0, k \u2190 1 1. While \u2206(V i ) (min h\u2208Vi U B(S i , h, \u03b4 \u2032 ) \u2212 min h\u2208Vi LB(S i , h, \u03b4 \u2032 )) > \u01eb 2. V i+1 \u2190 {h \u2208 V i : LB(S i , h, \u03b4 \u2032 ) \u2264 min h \u2032 \u2208Vi U B(S i , h \u2032 , \u03b4 \u2032 )} 3. i \u2190 i + 1 4. If \u2206(V i ) < 1 2 \u2206(V j k ) 5. k \u2190 k + 1; j k \u2190 i 6. S \u2032 i \u2190 Rejection sample 2 i\u2212j k samples x from D satisfying x \u2208 DIS(V i ) 7. S i \u2190 {(x, Oracle(x)) : x \u2208 S \u2032 i } 8. Return\u0125 = arg min h\u2208Vi U B(S i , h, \u03b4 \u2032 ) Figure 1. The A 2 algorithm. Lemma 1. Suppose D \u2032 is such that, \u2203\u03bb \u2208 (0, 1] s.t. for all measurable sets A \u2286 X , \u03bbD(A) \u2264 D \u2032 (A) \u2264 1 \u03bb D(A). If \u2206 r ,\u03b8,\u2206 \u2032\nr , and \u03b8 \u2032 are the disagreement rates at radius r and disagreement coefficients for D and D \u2032 respectively, then \u03bb\u2206 \u03bbr \u2264 \u2206 \u2032 r \u2264 1 \u03bb \u2206 r/\u03bb , and thus\n\u03bb 2 \u03b8 \u2264 \u03b8 \u2032 \u2264 1 \u03bb 2 \u03b8.", "publication_ref": ["b9"], "figure_ref": [], "table_ref": []}, {"heading": "Upper Bounds for the A 2 Algorithm", "text": "To prove bounds on the label complexity, we will additionally need to use some known results on finite sample rates of uniform convergence.\nDefinition 5. Let d be the VC dimension of C. For m \u2208 N, and S \u2208 (X\n\u00d7 {\u22121, 1}) m , define G(m, \u03b4) = 1 m + ln 4 \u03b4 + d ln 2em d m . U B(S, h, \u03b4) = min{er S (h) + G(|S|, \u03b4), 1}, LB(S, h, \u03b4) = max{er S (h) \u2212 G(|S|, \u03b4), 0}.\nBy convention, G(0, \u03b4) = 1. The following lemma is due to Vapnik (Vapnik, 1998).\nLemma 2. For any distribution D i over X \u00d7 {\u22121, 1}, and any m \u2208 N, with probability at least 1 \u2212 \u03b4 over the\ndraw of S \u223c D m i , every h \u2208 C satisfies |er S (h) \u2212 er Di (h)| \u2264 G(m, \u03b4).\nIn particular, this means\ner Di (h) \u2212 2G(|S|, \u03b4) \u2264 LB(S, h, \u03b4) \u2264 er Di (h) \u2264 U B(S, h, \u03b4) \u2264 er Di (h) + 2G(|S|, \u03b4). Furthermore, for \u03b3 > 0, if m \u2265 4 \u03b3 2 2d ln 4 \u03b3 + ln 4 \u03b4 , then G(m, \u03b4) < \u03b3.\nWe use a (somewhat simplified) version of the A 2 algorithm, presented by Balcan et. al (Balcan et al., 2006). The algorithm is given in Figure 1.\nThe motivation behind the A 2 algorithm is to maintain a set of concepts V i that we are confident contains any concepts with minimal error rate. If we can guarantee with statistical significance that a concept h 1 \u2208 V i has error rate worse than another concept h 2 \u2208 V i , then we can safely remove the concept h 1 since it is suboptimal. To achieve such a statistical guarantee, the algorithm employs two-sided confidence intervals on the error rates of each classifier in the concept space; however, since we are only interested in the relative differences between error rates, on each iteration we obtain this confidence interval for the error rate when D is restricted to the region of disagreement DIS(V i ). This restriction to the region of disagreement is the primary source of any improvements A 2 achieves over passive learning. We measure the progress of the algorithm by the reduction in the disagreement rate \u2206(V i ); the key question in studying the number of label requests is bounding the number of random labeled examples from the region of disagreement that are sufficient to remove enough concepts from V i to significantly reduce the measure of the region of disagreement.\nTheorem 2. If \u03b8 is the disagreement coefficient for C, then with probability at least 1 \u2212 \u03b4, given the inputs C, \u01eb, and \u03b4, A 2 outputs\u0125 \u2208 C with er(\u0125) \u2264 \u03bd + \u01eb, and the number of label requests made by A 2 is at most\nO \u03b8 2 \u03bd 2 \u01eb 2 + 1 d log 1 \u01eb + log 1 \u03b4 log 1 \u01eb .\nProof. Let \u03ba be the value of k and \u03b9 be the value of i when the algorithm halts. By convention, let . \u2206(V i ) \u2264 \u01eb also suffices to break from the loop, so \u03ba \u2264 log 2 2 \u01eb . Thus, \u03b9 \u2264 n. Lemma 2 and a union bound imply that, with probability \u2265 1 \u2212 \u03b4, for every i and every\nj \u03ba+1 = \u03b9 + 1. Let \u03b3 i = max h\u2208Vi (U B(S i , h, \u03b4 \u2032 ) \u2212 LB(S i , h, \u03b4 \u2032 )\nh \u2208 C, |er Si (h) \u2212 er Di (h)| \u2264 G(|S i |, \u03b4 \u2032 ), where D i is the con- ditional distribution of D XY given that X \u2208 DIS(V i ).\nFor the remainder of this proof, we assume that these inequalities hold for all such S i and h \u2208 C. In particular, this means we never remove the best classifier from V i . Additionally, \u2200h 1 , h 2 \u2208 V i we must have \u2206(V i )(er Di (h 1 ) \u2212 er Di (h 2 )) = er(h 1 ) \u2212 er(h 2 ). Combined with the nature of the halting criterion, this implies that er(\u0125) \u2264 \u03bd + \u01eb, as desired.\nThe rest of the proof bounds the number of label requests made by A 2 . Let h * \u2208 V i be such that er(h * ) \u2264 \u03bd + \u01eb. We consider two cases: large and small \u2206(V i ). Informally, when \u2206(V i ) is relatively large, the concepts far from h * are responsible for most of the disagreements, and since these must have relatively large error rates, we need only a few examples to remove them. On the other hand, when \u2206(V i ) is small, the halting condition is easy to satisfy.\nWe begin with the case where \u2206(\nV i ) is large. Specifi- cally, let i \u2032 = max{i \u2264 \u03b9 : \u2206(V i ) > 8\u03b8(\u03bd + \u01eb)}. (If no such i \u2032 exists, we can skip this case). Then \u2200i \u2264 i \u2032 , let V (\u03b8) i = h \u2208 V i : \u03c1 D (h, h * ) > \u2206(V i ) 2\u03b8 . Since for h \u2208 V i , \u03c1 D (h, h * )/\u2206(V i ) \u2264 er Di (h) + er Di (h * ) \u2264 er Di (h) + \u03bd+\u01eb \u2206(Vi) , we have V (\u03b8) i \u2286 h \u2208 V i : er Di (h) > 1 2\u03b8 \u2212 \u03bd + \u01eb \u2206(V i ) \u2286 h \u2208 V i : er Di (h)\u2212 1 8\u03b8 > er Di (h * ) + 3 8\u03b8 \u2212 2 \u03bd + \u01eb \u2206(V i ) \u2286 h \u2208 V i : er Di (h) \u2212 1 8\u03b8 > er Di (h * ) + 1 8\u03b8 . LetV i denote the latter set. By Lemma 2, S i of size O \u03b8 2 d log \u03b8 + log 1 \u03b4 \u2032 suffices to guarantee ev- ery h \u2208V i has LB(S i , h, \u03b4 \u2032 ) > U B(S i , h * , \u03b4 \u2032 ) in step 2. V (\u03b8) i \u2286V i and \u2206(V i \\ V (\u03b8) i ) \u2264 \u2206 \u2206(V i ) 2\u03b8 \u2264 1 2 \u2206(V i ), so in particular, any value of k for which j k \u2264 i \u2032 + 1 satisfies |S j k \u22121 | = O \u03b8 2 d log \u03b8 + log 1 \u03b4 \u2032 . To handle the remaining case, suppose \u2206(V i ) \u2264 8\u03b8(\u03bd + \u01eb).\nIn this case,\nS i of size O \u03b8 2 (\u03bd+\u01eb) 2 \u01eb 2 d log 1 \u01eb + log 1 \u03b4 \u2032 suffices to make \u03b3 i \u2264 \u01eb \u2206(Vi)\n, satisfying the halting condition. Therefore, every k for which j k > i \u2032 + 1 satisfies\n|S j k \u22121 | = O \u03b8 2 (\u03bd+\u01eb) 2 \u01eb 2 d log 1 \u01eb + log 1 \u03b4 \u2032 . Since for k > 1, j k \u22121 i=j (k\u22121) |S i | \u2264 2|S j k \u22121 |, we have that \u03b9 i=1 |S i | = O \u03b8 2 (\u03bd+\u01eb) 2 \u01eb 2 d log 1 \u01eb + log 1 \u03b4 \u2032 \u03ba . Noting that \u03ba = O(log 1 \u01eb ) and log 1 \u03b4 \u2032 = O d log 1 \u01eb + log 1 \u03b4 completes the proof.\nNote that we can get an easy improvement to the bound by replacing C with an \u01eb 2 -cover of C, using bounds for a finite concept space instead of VC bounds, and running the algorithm with accuracy parameter \u01eb 2 . This yields a similar, but sometimes much tighter, label complexity bound of\nO \u03b8 2 \u03bd 2 \u01eb 2 + 1 log N (\u01eb/2) log 1 \u01eb \u03b4 log 1 \u01eb .", "publication_ref": ["b11", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Lower Bounds for the A 2 Algorithm", "text": "In this section, we prove a lower bound on the worstcase number of label requests made by A 2 . As mentioned in Section 2, there are known lower bounds of \u2126 \u03bd 2 \u01eb 2 log 1 \u03b4 and \u2126 (log N (2\u01eb)), than which no algorithm can guarantee better (Kulkarni et al., 1993;K\u00e4\u00e4ri\u00e4inen, 2006). However, this leaves open the question of whether the \u03b8 2 factor in the bound is necessary.\nThe following theorem shows that it is for A 2 .\nTheorem 3. For any C and D, there exists an oracle with \u03bd = 0 such that, if \u03b8 is the disagreement coefficient, with probability 1\u2212\u03b4, the version of A 2 presented above makes a number of label requests at least\n\u2126 \u03b8 2 d log \u03b8 + log 1 \u03b4 .\nProof. The bound clearly holds if \u03b8 = 0, so assume \u03b8 > 0. By definition of disagreement coefficient, there is some\n\u03b1 0 > 0 such that \u2200\u03b1 \u2208 (0, \u03b1 0 ), \u2203r \u03b1 \u2208 (\u01eb, 1], h \u03b1 \u2208 C such that \u2206(B(h \u03b1 , r \u03b1 )) \u2265 \u2206 r\u03b1 \u2212 \u03b1 \u2265 \u03b8r \u03b1 \u2212 2\u03b1 > 0.\nFor some such \u03b1, let Oracle(x) = h \u03b1 (x) for all x \u2208 X .\nClearly \u03bd = 0. As before, we assume all bound evaluations in the algorithm are valid, which occurs with\nprobability \u2265 1 \u2212 \u03b4. Since LB(S i , h \u03b1 , \u03b4 \u2032 ) = 0 and U B(S i , h \u03b1 , \u03b4 \u2032 ) = G(|S i |, \u03b4 \u2032 ), if A 2 halts without re- moving any h \u2208 B(h \u03b1 , r \u03b1 ), then \u2203i : U B(S i , h \u03b1 , \u03b4 \u2032 ) \u2264 \u01eb \u2206(B(h\u03b1,r\u03b1)) \u2264 \u01eb \u03b8r\u03b1\u22122\u03b1 \u2264 r\u03b1 \u03b8r\u03b1\u22122\u03b1 .\nOn the other hand, suppose A 2 removes some h \u2208 B(h \u03b1 , r \u03b1 ) before halting, and in particular suppose the first time this happens is for some set S i . In this case,\nU B(S i , h \u03b1 , \u03b4 \u2032 ) < LB(S i , h, \u03b4 \u2032 ) \u2264 er Di (h) \u2264 er(h) \u2206(B(h\u03b1,r\u03b1)) \u2264 r\u03b1 \u03b8r\u03b1\u22122\u03b1 .\nIn either case, by definition of G(|S i |, \u03b4 \u2032 ), we must\nhave |S i | = \u2126 \u03b8 \u2212 2\u03b1 r\u03b1 2 d log \u03b8 \u2212 2\u03b1 r\u03b1 + log 1 \u03b4 \u2032 .\nSince this is true for any such \u03b1, taking the limit as \u03b1 \u2192 0 proves the bound.\nTheorems 2 and 3 show that the variation in worst-case number of label requests made by A 2 for different C and D is largely determined by the disagreement coefficient (and VC dimension). Furthermore, they give us a good estimate of the number of label requests made by A 2 . One natural question to ask is whether Theorem 2 is also tight for the label complexity of the learning problem. The following example indicates this is not the case. In particular, this means that A 2 can sometimes be suboptimal.\nSuppose X = [0, 1] n , and C is the space of axisaligned rectangles on X . That is, each h \u2208 C can be expressed as n pairs ((\na 1 , b 1 ), (a 2 , b 2 ), . . . , (a n , b n )), such that \u2200x \u2208 X , h(x) = 1 iff \u2200i, a i \u2264 x i \u2264 b i .\nFurthermore, suppose D is the uniform distribution on X . We see immediately that \u03b8 = 1 \u01eb+\u03bd , since \u2200r > 0, \u2206 r = 1. We will show the bound is not tight for the case when \u03bd = 0. 2 In this case, the bound value\nis \u2126 1 \u01eb 2 n log 1 \u01eb + log 1 \u03b4 .\nTheorem 4. When \u03bd = 0, the agnostic active learning label complexity of axis-aligned rectangles on [0, 1] n with respect to the uniform distribution is at most\nO n log n \u01eb\u03b4 + 1 \u01eb log 1 \u03b4 .\nA proof sketch for Theorem 4 is included in Appendix B. This clearly shows that the bound based on A 2 is sometimes not tight with respect to the true label complexity of learning problems. Furthermore, when \u01eb < 1 en , this problem has log N (\u01eb/2) \u2265 n, so the improvements offered by learning with an \u01eb 2 -cover cannot reduce the slack by much here (see Lemma 3 in Appendix B).", "publication_ref": ["b8", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Open Problems", "text": "Whether or not one can modify A 2 in a general way to improve this bound is an interesting open problem. One possible strategy would be to use Occam bounds, and adaptively set the prior for each iteration, while also maintaining several different types of bounds simultaneously. However, it seems that in order to obtain the dramatic improvements needed to close the gap demonstrated by Theorem 4, we need a more aggressive strategy than sampling randomly from DIS(V i ). For example, Balcan, Broder & Zhang (Balcan et al., 2007) present an algorithm for linear separa-tors which samples from a carefully chosen subregion of DIS(V i ). Though their analysis is for a restricted noise model, we might hope a similar idea is possible in the agnostic model. The end of Appendix A contains another interesting example that highlights this issue.\nOne important aspect of active learning that has not been addressed here is the value of unlabeled examples. Specifically, given an overabundance of unlabeled examples, can we use them to decrease the number of label requests required, and by how much? The splitting index bounds of Dasgupta (Dasgupta, 2005) can be used to study these types of questions in the noisefree setting; however, we have yet to see a thorough exploration of the topic for agnostic learning, where the role of unlabeled examples appears fundamentally different (at least in A 2 ).\nA. Realizable vs. Agnostic with \u03bd = 0\nThe following example indicates that agnostic active learning with \u03bd = 0 is sometimes fundamentally more difficult than realizable learning.\nLet \u01eb < 1/4, N = 1 2\u01eb . Let X = Z, and define D such that, for x \u2208 X : 0 < x \u2264 N , D(x) = \u01eb 4N and D(\u2212x) = 1\u2212\u01eb/4 N . D has zero probability elsewhere. In particular, note that 3 2 \u01eb < D(\u2212x) \u2264 4\u01eb and \u01eb 2 2 \u2264 D(x) \u2264 \u01eb 2 . Define concept space C = {h 1 , h 2 , . . .}, where \u2200i, j \u2208 {1, 2, . . .}, h i (0) = \u22121 and\nh i (\u2212j) = 2I[i = j] \u2212 1 h i (j) = 2I[j \u2265 i] \u2212 1.\nNote that this creates a learning problem where informative examples exist (x \u2208 {1, . . . , N }) but are rare. Theorem 5. For the learning problem described above, the realizable active learning label complexity is O log 1 \u01eb .\nProof. By Chernoff and union bounds, drawing \u0398 1 \u01eb 2 log 1 \u01eb\u03b4 unlabeled examples suffices to guarantee, with probability at least 1\u2212\u03b4, we have at least one unlabeled example of x, for all x \u2208 {1, 2, . . . , N }; suppose this happens. Suppose f \u2208 C is the target function. If f / \u2208 {h 1 , h 2 , . . . , h N }, querying the label of x = N suffices to show er(h N +1 ) = 0, so we output h N +1 . On the other hand, if we find f (N ) = +1, we can perform binary search among {1, 2, . . . , N } to find the smallest i > 0 such that f (i) = +1. In this case, we must have h i = f , so we output h i after O(log N ) queries. Theorem 6. For the learning problem described above, any agnostic active learning algorithm requires \u2126 1 \u01eb label requests, even if the oracle always agrees with some f \u2208 C, (i.e., even if \u03bd = 0). Proof. Suppose A is a correct agnostic learning algorithm. The idea of the proof is to assume A is guaranteed to make fewer than (1 \u2212 2\u03b4)N queries with probability \u2265 1 \u2212 \u03b4 when the target function is some particular f \u2208 C, and then show that by adding noise we can force A to output a concept with error more than \u01eb-worse than optimal with probability > \u03b4. Thus, either A cannot guarantee fewer than (1 \u2212 2\u03b4)N queries for that particular f , or A is not a correct agnostic learning algorithm.\nSpecifically, suppose that when the target function f = h N +1 , with probability \u2265 1\u2212\u03b4 A returns an \u01eb-good concept after making \u2264 q < (1 \u2212 2\u03b4)N label requests. If A is successful, then whatever concept it outputs labels all of {\u22121, \u22122, . . . , \u2212N } as \u22121. So in particular, letting the random variable R = (R 1 , R 2 , . . .) denote the sequence of examples A requests the labels of when Oracle agrees with h N +1 , this implies that with probability at least 1 \u2212 \u03b4, if Oracle(R i ) = h N +1 (R i ) for i \u2208 {1, 2, . . . , min{q, |R|}}, then A outputs a concept labeling all of {\u22121, \u22122, . . . , \u2212N } as \u22121. Now suppose instead of h N +1 , we pick the target function f \u2032 as follows. Let f \u2032 be identical to h N +1 on all of X except a single x \u2208 {\u22121, \u22122, . . . , \u2212N } where f \u2032 (x) = +1; the value of x for which this happens is chosen uniformly at random from {\u22121, \u22122, . . . , \u2212N }. Note that f \u2032 / \u2208 C. Also note that any concept in C other than h \u2212x is > \u01eb-worse than h \u2212x . Now consider the behavior of A when Oracle answers queries with this f \u2032 instead of h N +1 . Let Q = (Q 1 , Q 2 , . . .) denote the random sequence of examples A queries the labels of when Oracle agrees with f \u2032 . In particular, note that if R i = x for i \u2264 min{q, |R|},\nthen Q i = R i for i \u2264 min{q, |Q|}. E f \u2032 [Pr{A outputs h \u2212x }] \u2264 E R [Pr x {\u2203i \u2264 q : R i = x}] + \u03b4 < 1 \u2212 \u03b4.\nBy the probabilistic method, we have proven that there exists some fixed oracle such that A fails with probability > \u03b4. This contradicts the premise that A is a correct agnostic learning algorithm.\nAs an interesting aside, note that if we define C \u01eb = {h 1 , h 2 , . . . , h N }, dependent on \u01eb, then the agnostic label complexity is O log 1 \u01eb\u03b4 when \u03bd = 0. This is because we can run the realizable learning algorithm to find f = h i , and then sample \u0398 log 1 \u03b4 labeled copies of the example \u2212i; by observing that they are all labeled +1, we effectively verify that h i is at most \u01ebworse than optimal. To make this a correct agnostic algorithm, we can simply be prepared to run A 2 if any of the \u0398 log 1 \u03b4 samples of \u2212i are labeled \u22121 (which they won't be for \u03bd = 0). However, since the disagreement coefficient \u03b8 = \u0398 1 \u01eb , Theorem 3 implies A 2 does not achieve this improvement. See Appendix B for a similar example.", "publication_ref": ["b1", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "B. Axis-Aligned Rectangles", "text": "Proof Sketch of Theorem 4. To keep things simple, we omit the precise constants. Consider the following algorithm. 3 0. Sample \u0398 1 \u01eb log 1 \u03b4 labeled examples from D XY 1. If none of them are positive, return the \"all negative\" concept 2. Else let x be one of the positive examples 3. For i = 1, 2, . . . , n 4. Rejection sample unlabeled set U i of size \u0398 n \u01eb\u03b4 log n \u03b4 2 from the conditional of D given\n\u2200j = i, x j \u2212 O \u01eb\u03b4 n log 1 \u03b4 \u2264 X j \u2264 x j + O \u01eb\u03b4 n log 1 \u03b4 5. Findb i = max{z i : z \u2208 U i \u222a{x}, Oracle(z) = +1}\nby binary search in {z i : z \u2208 U i \u222a {x}, z i \u2265 x i } 6. Find\u00e2 i = min{z i : z \u2208 U i \u222a{x}, Oracle(z) = +1} by binary search in {z i : z \u2208 U i \u222a {x}, z i \u2264 x i } 7. Let\u0125 = ((\u00e2 1 ,b 1 ), (\u00e2 2 ,b 2 ), . . . , (\u00e2 n ,b n )) 8. Sample \u0398 1 \u01eb log 1 \u03b4 labeled examples T from D XY 9. If er T (\u0125) > 0, run A 2 from the start and return its output 10.Else return\u0125 The correctness of the algorithm in the agnostic setting is clear from examining the three ways to exit the algorithm. First, any oracle with Pr X\u223cD {Oracle(X) = +1} > \u01eb will, with probability \u2265 1 \u2212 O(\u03b4) have a positive example in the initial \u0398 1 \u01eb log 1 \u03b4 sample. So if the set has no positives, we can be confident the \"all negative\" concept has error \u2264 \u01eb. If we return in step 9, we know from Theorem 2 that A 2 will, with probability 1 \u2212 O(\u03b4), output a concept with error \u2264 \u03bd + \u01eb. The remaining possibility is to return in step 10. An\u0177 h with er(\u0125) > \u01eb will, with probability \u2265 1 \u2212 O(\u03b4), have er T (\u0125) > 0 in step 9. So we can be confident th\u00ea h output in step 10 has er(\u0125) \u2264 \u01eb.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "I am grateful to Nina Balcan for helpful discussions, and to Ye Nan for pointing out a mistake in the original proof of Theorem 1. This research was sponsored through a generous grant from the Commonwealth of Pennsylvania. The views and conclusions contained in this document are those of the author and should not be interpreted as representing the official policies, either expressed or implied, of the sponsoring body, or other institution or entity.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "To bound the number of label requests, note that the two binary searches we perform for each i (steps 5 and 6) require only O (log |U i |) label requests each, so the entire For loop uses only O n log n \u01eb\u03b4 label requests. We additionally have the two labeled sets of size O 1 \u01eb log 1 \u03b4 , so if we do not return in step 9, the total number of label requests is at most O n log n \u01eb\u03b4 + 1 \u01eb log 1 \u03b4 . It only remains to show that when \u03bd = 0, we do not return in step 9. Let f = ((a 1 , b 1 ), (a 2 , b 2 ), . . . , (a n , b n )) be a rectangle with er(f ) = 0. Note that er(\u0125)\nIn particular, if we do not return in step 1, with probability 1 \u2212 O(\u03b4), \u2200j, x j \u2208 [a j + \u03b3, b j \u2212 \u03b3]. Suppose this happens. In particular, this means the oracle's labels for all z \u2208 U i are completely determined by whether a i \u2264 z i \u2264 b i . We can essentially think of this as two \"threshold\" learning problems for each i: one above x i and one below x i . The binary searches find threshold values consistent with each U i . In particular, by standard passive sample complexity arguments,\nTherefore, the probability\u0125 makes a mistake on T of size O 1 \u01eb log 1 \u03b4 is at most O(\u03b4). Otherwise, we have er T (\u0125) = 0 in step 9, so we return in step 10.\nLemma 3. If C is the space of axis-aligned rectangles on [0, 1] n , and D is the uniform distribution, then for \u01eb < 1 en , log 2 N (\u01eb/2) \u2265 n.\nProof. Since N (\u01eb/2) is at least the size of any \u01eb-separated set, we can prove this lower bound by constructing an \u01eb-separated set of size 2 n . In particular, consider the set of all rectangles ((a 1 , b 1 ), (a 2 , b 2 ), . . . , (a n , b n )) satsifying \u2200i,\nSo the region in which these two disagree contains x \u2208 X :", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Agnostic active learning", "journal": "", "year": "2006", "authors": "M.-F Balcan; A Beygelzimer; J Langford"}, {"ref_id": "b1", "title": "Margin based active learning", "journal": "", "year": "2007", "authors": "M.-F Balcan; A Broder; T Zhang"}, {"ref_id": "b2", "title": "Learnability by fixed distributions", "journal": "", "year": "1988", "authors": "G Benedek; A Itai"}, {"ref_id": "b3", "title": "Learnability and the vapnikchervonenkis dimension", "journal": "Journal of the Association for Computing Machinery", "year": "1989", "authors": "A Blumer; A Ehrenfeucht; D Haussler; M Warmuth"}, {"ref_id": "b4", "title": "Coarse sample complexity bounds for active learning", "journal": "", "year": "2005", "authors": "S Dasgupta"}, {"ref_id": "b5", "title": "Decision theoretic generalizations of the PAC model for neural net and other learning applications", "journal": "Information and Computation", "year": "1992", "authors": "D Haussler"}, {"ref_id": "b6", "title": "Active learning in the nonrealizable case", "journal": "", "year": "2006", "authors": "M K\u00e4\u00e4ri\u00e4inen"}, {"ref_id": "b7", "title": "On metric entropy, vapnikchervonenkis dimension, and learnability for a class of distributions", "journal": "", "year": "1989", "authors": "S R Kulkarni"}, {"ref_id": "b8", "title": "Active learning using arbitrary binary valued queries", "journal": "", "year": "1993", "authors": "S R Kulkarni; S K Mitter; J N Tsitsiklis"}, {"ref_id": "b9", "title": "Concise formulas for the area and volume of a hyperspherical cap", "journal": "Asian Journal of Mathematics and Statistics", "year": "2011", "authors": "S Li"}, {"ref_id": "b10", "title": "On the sample complexity of PAC learning halfspaces against the uniform distribution", "journal": "IEEE Transactions on Neural Networks", "year": "1995", "authors": "P M Long"}, {"ref_id": "b11", "title": "Statistical learning theory", "journal": "John Wiley & Sons, Inc", "year": "1998", "authors": "V Vapnik"}], "figures": [], "formulas": [{"formula_id": "formula_0", "formula_text": "(x m , y m )} \u2208 (X \u00d7 {\u22121, 1}) m , er S (h) = 1 m m i=1 |h(x i ) \u2212 y i |/2", "formula_coordinates": [2.0, 307.44, 409.58, 234.0, 30.37]}, {"formula_id": "formula_1", "formula_text": "Let \u03c1 D (\u2022, \u2022) be the pseudo-metric on C induced by D, s.t. \u2200h, h \u2032 \u2208 C, \u03c1 D (h, h \u2032 ) = Pr X\u223cD {h(X) = h \u2032 (X)}. An \u01eb-cover of C with respect to D is any set V \u2286 C such that \u2200h \u2208 C, \u2203h \u2032 \u2208 V : \u03c1 D (h, h \u2032 ) \u2264 \u01eb. We additionally let N (\u01eb) denote the size of a minimal \u01eb-cover of C with respect to D. It is known that N (\u01eb) < 2 2e \u01eb ln 2e \u01eb d ,", "formula_coordinates": [2.0, 307.44, 512.58, 234.0, 79.19]}, {"formula_id": "formula_2", "formula_text": "DIS(V ) = {x \u2208 X |\u2203h 1 , h 2 \u2208 V : h 1 (x) = h 2 (x)}. Definition 2. The disagreement rate \u2206(V ) of a set V \u2286 C is defined as \u2206(V ) = Pr X\u223cD {X \u2208 DIS(V )}. Definition 3. For h \u2208 C, r > 0, let B(h, r) = {h \u2032 \u2208 C : \u03c1 D (h \u2032 , h) \u2264 r}", "formula_coordinates": [2.0, 307.44, 648.82, 234.0, 73.31]}, {"formula_id": "formula_3", "formula_text": "\u2206 r = sup h\u2208C \u2206(B(h, r)). Definition 4. The disagreement coefficient is the in- fimum value of \u03b8 > 0 such that \u2200r > \u03bd + \u01eb, \u2206 r \u2264 \u03b8r.", "formula_coordinates": [3.0, 55.44, 131.0, 233.98, 75.88]}, {"formula_id": "formula_4", "formula_text": "[0, 1] (for z \u2208 [0, 1]), such that t z (x) = +1 iff x \u2265 z. Further- more, suppose D is uniform on [0, 1]. In this case, it is clear that the disagreement coefficient is at most 2, since the region of disagreement of B(t z , r) is roughly {x \u2208 [0, 1] : |x \u2212 z| \u2264 r}.", "formula_coordinates": [3.0, 55.44, 348.27, 234.01, 76.48]}, {"formula_id": "formula_5", "formula_text": "I [a,b] such that for x \u2208 [0, 1], I [a,b] (x) = +1 iff x \u2208 [a, b] (for a, b \u2208 [0, 1], a \u2264 b).", "formula_coordinates": [3.0, 55.44, 485.42, 234.0, 28.99]}, {"formula_id": "formula_6", "formula_text": "1 4 min \u03c0 \u221a d, 1 \u03bd + \u01eb \u2264 \u03b8 \u2264 min \u03c0 \u221a d, 1 \u03bd + \u01eb .", "formula_coordinates": [3.0, 321.82, 158.6, 206.43, 26.02]}, {"formula_id": "formula_7", "formula_text": "DIS(B(w, r)) = {x \u2208 X : |x \u2022 w| \u2264 sin(\u03c0r)}. Let A d = 2\u03c0 d/2 \u0393( d 2 )", "formula_coordinates": [3.0, 307.44, 289.19, 212.57, 36.47]}, {"formula_id": "formula_8", "formula_text": "C d (h) = 1 2 A d I 2h\u2212h 2 d\u22121 2", "formula_coordinates": [3.0, 401.32, 325.87, 107.07, 14.33]}, {"formula_id": "formula_9", "formula_text": "I x (a, b) = \u0393(a+b) \u0393(a)\u0393(b) x 0 t a\u22121 (1 \u2212 t) b\u22121 dt = \u0393(a+b) a\u0393(a)\u0393(b) x a 0 1 \u2212 u 1/a b\u22121 du is the regularized in- complete beta function. Then we can express \u2206 r as \u2206 r = 1\u2212 2C d (1 \u2212 sin(\u03c0r)) A d = 1\u2212I cos 2 (\u03c0r) d \u2212 1 2 , 1 2 . As I x (a, b) = 1\u2212I 1\u2212x (b, a) and \u0393 1 2 = \u221a \u03c0, this equals 2\u0393 d 2 \u221a \u03c0\u0393 d\u22121 2 sin(\u03c0r) 0 1 \u2212 x 2 d\u22123 2 dx. ( * ) As d 3 \u2264 2\u0393( d 2 ) \u221a \u03c0\u0393( d\u22121 2 ) \u2264 \u221a d \u2212 2, we see ( * ) is at most \u221a d \u2212 2 sin(\u03c0r) \u2264 \u221a d\u03c0r.", "formula_coordinates": [3.0, 307.44, 349.73, 234.0, 177.0]}, {"formula_id": "formula_10", "formula_text": "d 3 sin(\u03c0r) 0 1 \u2212 x 2 d 2 dx \u2265 \u03c0 3 sin(\u03c0r) 0 d \u03c0 e \u2212d\u2022x 2 dx \u2265 1 2 min 1 2 , \u221a d sin(\u03c0r) \u2265 1 4 min 1, \u221a d\u03c0r .", "formula_coordinates": [3.0, 310.78, 577.11, 228.91, 53.41]}, {"formula_id": "formula_11", "formula_text": "Output: classifier\u0125 \u2208 C Letn = log 2 64 \u01eb 2 d ln 8 \u01eb + ln 8 \u01eb\u03b4 log 2 4 \u01eb , and let \u03b4 \u2032 = \u03b4/n 0. V 0 \u2190 C, S 0 \u2190 \u2205, i \u2190 0, j 1 \u2190 0, k \u2190 1 1. While \u2206(V i ) (min h\u2208Vi U B(S i , h, \u03b4 \u2032 ) \u2212 min h\u2208Vi LB(S i , h, \u03b4 \u2032 )) > \u01eb 2. V i+1 \u2190 {h \u2208 V i : LB(S i , h, \u03b4 \u2032 ) \u2264 min h \u2032 \u2208Vi U B(S i , h \u2032 , \u03b4 \u2032 )} 3. i \u2190 i + 1 4. If \u2206(V i ) < 1 2 \u2206(V j k ) 5. k \u2190 k + 1; j k \u2190 i 6. S \u2032 i \u2190 Rejection sample 2 i\u2212j k samples x from D satisfying x \u2208 DIS(V i ) 7. S i \u2190 {(x, Oracle(x)) : x \u2208 S \u2032 i } 8. Return\u0125 = arg min h\u2208Vi U B(S i , h, \u03b4 \u2032 ) Figure 1. The A 2 algorithm. Lemma 1. Suppose D \u2032 is such that, \u2203\u03bb \u2208 (0, 1] s.t. for all measurable sets A \u2286 X , \u03bbD(A) \u2264 D \u2032 (A) \u2264 1 \u03bb D(A). If \u2206 r ,\u03b8,\u2206 \u2032", "formula_coordinates": [4.0, 55.44, 83.48, 381.72, 214.29]}, {"formula_id": "formula_12", "formula_text": "\u03bb 2 \u03b8 \u2264 \u03b8 \u2032 \u2264 1 \u03bb 2 \u03b8.", "formula_coordinates": [4.0, 137.47, 324.95, 69.94, 23.43]}, {"formula_id": "formula_13", "formula_text": "\u00d7 {\u22121, 1}) m , define G(m, \u03b4) = 1 m + ln 4 \u03b4 + d ln 2em d m . U B(S, h, \u03b4) = min{er S (h) + G(|S|, \u03b4), 1}, LB(S, h, \u03b4) = max{er S (h) \u2212 G(|S|, \u03b4), 0}.", "formula_coordinates": [4.0, 82.6, 433.05, 179.63, 96.02]}, {"formula_id": "formula_14", "formula_text": "draw of S \u223c D m i , every h \u2208 C satisfies |er S (h) \u2212 er Di (h)| \u2264 G(m, \u03b4).", "formula_coordinates": [4.0, 55.44, 585.42, 181.08, 39.37]}, {"formula_id": "formula_15", "formula_text": "er Di (h) \u2212 2G(|S|, \u03b4) \u2264 LB(S, h, \u03b4) \u2264 er Di (h) \u2264 U B(S, h, \u03b4) \u2264 er Di (h) + 2G(|S|, \u03b4). Furthermore, for \u03b3 > 0, if m \u2265 4 \u03b3 2 2d ln 4 \u03b3 + ln 4 \u03b4 , then G(m, \u03b4) < \u03b3.", "formula_coordinates": [4.0, 55.44, 652.66, 234.0, 64.89]}, {"formula_id": "formula_16", "formula_text": "O \u03b8 2 \u03bd 2 \u01eb 2 + 1 d log 1 \u01eb + log 1 \u03b4 log 1 \u01eb .", "formula_coordinates": [4.0, 331.1, 621.22, 186.66, 24.91]}, {"formula_id": "formula_17", "formula_text": "j \u03ba+1 = \u03b9 + 1. Let \u03b3 i = max h\u2208Vi (U B(S i , h, \u03b4 \u2032 ) \u2212 LB(S i , h, \u03b4 \u2032 )", "formula_coordinates": [4.0, 307.44, 682.77, 233.99, 23.57]}, {"formula_id": "formula_18", "formula_text": "h \u2208 C, |er Si (h) \u2212 er Di (h)| \u2264 G(|S i |, \u03b4 \u2032 ), where D i is the con- ditional distribution of D XY given that X \u2208 DIS(V i ).", "formula_coordinates": [5.0, 55.44, 118.37, 234.0, 40.95]}, {"formula_id": "formula_19", "formula_text": "V i ) is large. Specifi- cally, let i \u2032 = max{i \u2264 \u03b9 : \u2206(V i ) > 8\u03b8(\u03bd + \u01eb)}. (If no such i \u2032 exists, we can skip this case). Then \u2200i \u2264 i \u2032 , let V (\u03b8) i = h \u2208 V i : \u03c1 D (h, h * ) > \u2206(V i ) 2\u03b8 . Since for h \u2208 V i , \u03c1 D (h, h * )/\u2206(V i ) \u2264 er Di (h) + er Di (h * ) \u2264 er Di (h) + \u03bd+\u01eb \u2206(Vi) , we have V (\u03b8) i \u2286 h \u2208 V i : er Di (h) > 1 2\u03b8 \u2212 \u03bd + \u01eb \u2206(V i ) \u2286 h \u2208 V i : er Di (h)\u2212 1 8\u03b8 > er Di (h * ) + 3 8\u03b8 \u2212 2 \u03bd + \u01eb \u2206(V i ) \u2286 h \u2208 V i : er Di (h) \u2212 1 8\u03b8 > er Di (h * ) + 1 8\u03b8 . LetV i denote the latter set. By Lemma 2, S i of size O \u03b8 2 d log \u03b8 + log 1 \u03b4 \u2032 suffices to guarantee ev- ery h \u2208V i has LB(S i , h, \u03b4 \u2032 ) > U B(S i , h * , \u03b4 \u2032 ) in step 2. V (\u03b8) i \u2286V i and \u2206(V i \\ V (\u03b8) i ) \u2264 \u2206 \u2206(V i ) 2\u03b8 \u2264 1 2 \u2206(V i ), so in particular, any value of k for which j k \u2264 i \u2032 + 1 satisfies |S j k \u22121 | = O \u03b8 2 d log \u03b8 + log 1 \u03b4 \u2032 . To handle the remaining case, suppose \u2206(V i ) \u2264 8\u03b8(\u03bd + \u01eb).", "formula_coordinates": [5.0, 55.44, 357.81, 234.0, 307.93]}, {"formula_id": "formula_20", "formula_text": "S i of size O \u03b8 2 (\u03bd+\u01eb) 2 \u01eb 2 d log 1 \u01eb + log 1 \u03b4 \u2032 suffices to make \u03b3 i \u2264 \u01eb \u2206(Vi)", "formula_coordinates": [5.0, 55.44, 648.7, 234.0, 46.38]}, {"formula_id": "formula_21", "formula_text": "|S j k \u22121 | = O \u03b8 2 (\u03bd+\u01eb) 2 \u01eb 2 d log 1 \u01eb + log 1 \u03b4 \u2032 . Since for k > 1, j k \u22121 i=j (k\u22121) |S i | \u2264 2|S j k \u22121 |, we have that \u03b9 i=1 |S i | = O \u03b8 2 (\u03bd+\u01eb) 2 \u01eb 2 d log 1 \u01eb + log 1 \u03b4 \u2032 \u03ba . Noting that \u03ba = O(log 1 \u01eb ) and log 1 \u03b4 \u2032 = O d log 1 \u01eb + log 1 \u03b4 completes the proof.", "formula_coordinates": [5.0, 55.44, 66.79, 485.99, 656.34]}, {"formula_id": "formula_22", "formula_text": "O \u03b8 2 \u03bd 2 \u01eb 2 + 1 log N (\u01eb/2) log 1 \u01eb \u03b4 log 1 \u01eb .", "formula_coordinates": [5.0, 334.69, 217.56, 179.49, 25.79]}, {"formula_id": "formula_23", "formula_text": "\u2126 \u03b8 2 d log \u03b8 + log 1 \u03b4 .", "formula_coordinates": [5.0, 367.39, 438.83, 114.1, 23.2]}, {"formula_id": "formula_24", "formula_text": "\u03b1 0 > 0 such that \u2200\u03b1 \u2208 (0, \u03b1 0 ), \u2203r \u03b1 \u2208 (\u01eb, 1], h \u03b1 \u2208 C such that \u2206(B(h \u03b1 , r \u03b1 )) \u2265 \u2206 r\u03b1 \u2212 \u03b1 \u2265 \u03b8r \u03b1 \u2212 2\u03b1 > 0.", "formula_coordinates": [5.0, 307.44, 499.27, 233.99, 28.99]}, {"formula_id": "formula_25", "formula_text": "probability \u2265 1 \u2212 \u03b4. Since LB(S i , h \u03b1 , \u03b4 \u2032 ) = 0 and U B(S i , h \u03b1 , \u03b4 \u2032 ) = G(|S i |, \u03b4 \u2032 ), if A 2 halts without re- moving any h \u2208 B(h \u03b1 , r \u03b1 ), then \u2203i : U B(S i , h \u03b1 , \u03b4 \u2032 ) \u2264 \u01eb \u2206(B(h\u03b1,r\u03b1)) \u2264 \u01eb \u03b8r\u03b1\u22122\u03b1 \u2264 r\u03b1 \u03b8r\u03b1\u22122\u03b1 .", "formula_coordinates": [5.0, 307.44, 558.14, 234.01, 53.81]}, {"formula_id": "formula_26", "formula_text": "U B(S i , h \u03b1 , \u03b4 \u2032 ) < LB(S i , h, \u03b4 \u2032 ) \u2264 er Di (h) \u2264 er(h) \u2206(B(h\u03b1,r\u03b1)) \u2264 r\u03b1 \u03b8r\u03b1\u22122\u03b1 .", "formula_coordinates": [5.0, 307.44, 632.2, 234.0, 31.49]}, {"formula_id": "formula_27", "formula_text": "have |S i | = \u2126 \u03b8 \u2212 2\u03b1 r\u03b1 2 d log \u03b8 \u2212 2\u03b1 r\u03b1 + log 1 \u03b4 \u2032 .", "formula_coordinates": [5.0, 307.44, 671.5, 234.0, 23.8]}, {"formula_id": "formula_28", "formula_text": "a 1 , b 1 ), (a 2 , b 2 ), . . . , (a n , b n )), such that \u2200x \u2208 X , h(x) = 1 iff \u2200i, a i \u2264 x i \u2264 b i .", "formula_coordinates": [6.0, 55.44, 219.0, 234.0, 28.99]}, {"formula_id": "formula_29", "formula_text": "is \u2126 1 \u01eb 2 n log 1 \u01eb + log 1 \u03b4 .", "formula_coordinates": [6.0, 55.44, 289.04, 112.76, 14.33]}, {"formula_id": "formula_30", "formula_text": "O n log n \u01eb\u03b4 + 1 \u01eb log 1 \u03b4 .", "formula_coordinates": [6.0, 119.86, 348.98, 105.14, 23.54]}, {"formula_id": "formula_31", "formula_text": "h i (\u2212j) = 2I[i = j] \u2212 1 h i (j) = 2I[j \u2265 i] \u2212 1.", "formula_coordinates": [7.0, 101.2, 479.71, 142.49, 31.99]}, {"formula_id": "formula_32", "formula_text": "then Q i = R i for i \u2264 min{q, |Q|}. E f \u2032 [Pr{A outputs h \u2212x }] \u2264 E R [Pr x {\u2203i \u2264 q : R i = x}] + \u03b4 < 1 \u2212 \u03b4.", "formula_coordinates": [7.0, 307.44, 542.94, 224.02, 56.93]}, {"formula_id": "formula_33", "formula_text": "\u2200j = i, x j \u2212 O \u01eb\u03b4 n log 1 \u03b4 \u2264 X j \u2264 x j + O \u01eb\u03b4 n log 1 \u03b4 5. Findb i = max{z i : z \u2208 U i \u222a{x}, Oracle(z) = +1}", "formula_coordinates": [8.0, 55.44, 358.34, 234.0, 35.74]}], "doi": ""}