{"title": "World-to-Words: Grounded Open Vocabulary Acquisition through Fast Mapping in Vision-Language Models", "authors": "Ziqiao Ma; Jiayi Pan; Joyce Chai", "pub_date": "", "abstract": "The ability to connect language units to their referents in the physical world, referred to as grounding, is crucial to learning and understanding grounded meanings of words. While humans demonstrate fast mapping in new word learning, it remains unclear whether modern vision-language models can truly represent language with their grounded meanings, and how grounding may further bootstrap new word learning. To this end, we introduce Grounded Open Vocabulary Acquisition (GOVA) to examine grounding and bootstrapping in openworld language learning. As an initial attempt, we propose World-to-Words (W2W), a novel visually-grounded language model by pre-training on image-text pairs highlighting grounding as an objective. Through extensive experiments and analysis, we demonstrate that W2W is a more coherent and fast grounded word learner, and that the grounding ability acquired during pre-training helps the model to learn unseen words more rapidly and robustly. 1   ", "sections": [{"heading": "Introduction", "text": "Language is learned through sensorimotor experience in the physical world (Bisk et al., 2020). The ability to connect language units to their referents in the physical world, referred to as grounding, plays an important role in learning and understanding grounded meanings of words (Harnad, 1990). As shown in Figure 1, a human reader would easily ground noun phrases to the corresponding entities captured in the image. Even when the term \"incinerator\" is new to human learners, they can still locate the object of interest through the language and visual context, and acquire its meaning. In fact, this ability to bootstrap new word learning with only minimal information, known as fast mapping, is demonstrated abundantly in cognitive  literature on human language acquisition (Carey and Bartlett, 1978;Carey, 1978;Golinkoff et al., 2000;Smith and Yu, 2008).\nRecently, there has been a substantial effort on pre-training vision-language models (VLMs) (Du et al., 2022a). Despite the exciting performance of these models on a variety of downstream vision and language tasks, it remains unclear whether these models can truly understand or produce language with their grounded meanings in the perceived world, and how grounding may further bootstrap new word learning. These questions are of interest from both a scientific and an engineering point of view. From a scientific perspective, grounding is crucial to language learners, as children attend to intended objects in the environment when producing (Tanenhaus et al., 1995;Meyer et al., 1998) and comprehending (Smith et al., 2007) utterances. From an engineering perspective, even with the availability of grounded vision language datasets (image-text pairs with fine-grained wordobject mappings) (Plummer et al., 2015), the costly grounding annotation can hardly cover the whole vocabulary space during the training time. Building upon the pre-trained models, it's important for the agent to have the ability to learn grounded new words in a few shots of raw image-text pairs without word-object mappings.\nTo this end, we introduce Grounded Open Vocabulary Acquisition (GOVA), a scalable formulation to examine grounding and bootstrapping in openworld language learning. In this formulation, lan-guage learning is a combination of learning to predict a word in a linguistic context as well as learning to ground the word in the physical world. Under this formulation, we explore the framework in which the model first acquires the grounding ability during pre-training, and then transfers this ability to learn unseen words without grounding supervision. As an initial step, we developed World-to-Words (W2W), a novel visually grounded language model motivated by recent advances in detection transformers (DETR) (Carion et al., 2020;Kamath et al., 2021). Compared to many existing VLMs, W2W performs language modeling upon explicit object representations. The model first acquires the ability to ground during pre-training, and then transfers this intrinsic ability to learn unseen words when grounded supervision is no longer available.\nOur empirical results show that learning to map words to their referents plays a significant role in grounded word acquisition. By pre-training with fine-grained word-object mappings, W2W demonstrates stronger performance in learning grounded meanings of words, both seen and unseen, yet with orders of magnitude fewer data compared to other competitive VLM baselines. The pre-trained model can further provide a foundation for efficient learning of new grounded words with a few examples. We further present an in-depth analysis to understand potential predictors of W2W in word learning, which demonstrates intriguing behaviors in comparison to human language learning. Our findings will provide a stepping stone for future work on grounded language learning in an open world.", "publication_ref": ["b1", "b25", "b4", "b3", "b20", "b63", "b14", "b69", "b48", "b64", "b52", "b5"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Grounded Open Vocabulary Acquisition (GOVA)", "text": "We start by introducing the settings of grounded word acquisition and few-shot learning of new words tasks, which are two key components of the Grounded Open Vocabulary Acquisition (GOVA) task formulation. We further present a unified evaluation protocol and introduce the dataset we curated for this problem.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Grounded Word Acquisition", "text": "Many vision-language tasks have been developed in the past, e.g., visual question answering, visual commonsense reasoning, etc. However, these tasks are mainly focused on the end task performance without scrutinizing whether words are grounded to their corresponding visual entities. We\nTwo boats of people, a smaller yellow <mask> with two people and a larger white boat with six people.\nTwo boats of people, a smaller yellow boat with two people and a larger white boat with six people.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Input Output", "text": "Figure 2: An instance of the word grounding task. Models are tasked to predict the missing word boat and localize the corresponding smaller yellow boat in the image coherently.\nconsider a formulation that directly examines if vision-language models have the ability to acquire grounded meanings of words, specifically, through both language modeling and object localization.\nFigure 2 shows an instance of the word acquisition task. A model is presented with an image x img \u2208 I and an incomplete caption x cap \u2208 T with one of its groundable words w (e.g., nouns and adjectives) replaced by a MASK. The model is tasked to predict this missing word w \u2208 V based on all available context and localize the corresponding objects\nO w = {o 1 , o 2 , \u2022 \u2022 \u2022 , o n }\nin the image by proposing the bounding boxes of them. Overall, a model capable of solving the grounded word acquisition task is a function f :\nI \u00d7 T \u2192 V \u00d7 R 4n .\nThe language modeling part takes the form of a cloze test, which predicts an open vocabulary word and is widely adopted to evaluate pre-trained language models (Paperno et al., 2016;Petroni et al., 2019;Jin et al., 2020). However, language modeling alone fails to provide a comprehensive evaluation of language grounding. For example in Figure 2, a model may correctly produce the word \"boat,\" but mistakenly attributes the evidence to the larger white boat in the image. To address this limitation, we require models to localize the corresponding object in the image. This design is motivated by the disentanglement of object detection into object localization and class recognition (Singh et al., 2018;Zareian et al., 2021;. It enables vision models to develop a sense of objectness without relying on a predefined set of object classes, thereby potentially allowing them to generalize to unseen objects. Further comparison with related task setups is discussed in Section 5 and illustrated in Figure 8 in the Appendix.", "publication_ref": ["b50", "b51", "b61", "b78"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation Metric", "text": "In language model evaluation, the commonly used measures for assessing performance are the stan-dard hit-rate-at-k (HR@k) measure and perplexity (Salazar et al., 2020;Jin et al., 2020). In masked language modeling, the log perplexity of a word w is defined as the log pseudo-perplexity:\nlog PPL(w) = \u2212 log P (w|x img , xcap)(1)\nIn object detection evaluation, especially for phrase grounding where multiple referents are possible (Kamath et al., 2021), Any-Protocol and All-Protocol are commonly adopted. Assuming n ground truth bounding boxes\nB = {b 1 , b 2 , \u2022 \u2022 \u2022 , b n } and m predicted bounding boxes B = { b 1 , b 2 , \u2022 \u2022 \u2022 , b m }, the intersection-over-union (IoU)\nin both protocols is defined as:\nIoUany = 1 n i\u2208{1,2,\u2022\u2022\u2022 ,n} max j\u2208{1,2,\u2022\u2022\u2022 ,m} IoU(bi, bj) (2) IoU all = IoU(\u222aB, \u222a B)(3)\nHowever, these metrics only capture unimodal performance without concerning the correctness of cross-modal mapping. We design two new metrics to combine language and vision performance:\n\u2022 Grounded hit-rate (G-HR@k), the proportion of tests with the masked word appearing in the top-k candidates and a localization IoU over 0.5. \u2022 Grounded perplexity (G-PPL) as follows:\nlog G-PPL(w) = \u221e if IoU = 0 log PPL(w) \u2212 log IoU else (4)", "publication_ref": ["b60"], "figure_ref": [], "table_ref": []}, {"heading": "Few-Shot Learning of New Words", "text": "Although there are grounding datasets available, i.e., image-text pairs with word-object mapping annotation (Plummer et al., 2015), it is impractical to obtain such fine-grained annotation on a large scale and to cover the whole vocabulary space V.\nWe therefore explore grounded new word learning as a few-shot learning problem, especially under the setting of incremental class learning (Mandziuk and Shastri, 1999;Kemker et al., 2018). An intuitive illustration of the few-shot new word learning framework is provided in Figure 3. Under this framework, a computational model is developed in two stages. During the pre-training stage, the model receives image-caption pairs, with finegrained word-object annotation for a set of base words V seen \u2286 V. After pre-training, the model is provided with few samples of raw text-image pairs, each containing a set of unseen words V unseen \u2286 V that the model has to acquire.\nSomeone is slicing a loaf of bread using a knife on a wooden cutting board.\nI am slicing the pizza with a knife and stacking the pieces onto the plate. Tests are performed after each training stage. It's important to note that the unseen words may not be completely new, e.g., the models may have encountered these words in its language encoder initialized with pre-trained language models. We consider them \"unseen\" because the model never sees these words paired with their referent, i.e., the grounded meanings of the words are unknown.", "publication_ref": ["b52", "b43", "b31"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Dataset Curation", "text": "We build our dataset based on the Flickr30K Entities dataset (Plummer et al., 2015), which contains image-text pairs with dense annotations between groundable phrases and bounding boxes of objects. The groundable phrases and regions are defined by the dataset, as chunks of text that refer to object bounding boxes. To construct word grounding instances, we use Stanza (Qi et al., 2020)  3 Computational Models", "publication_ref": ["b52", "b54"], "figure_ref": [], "table_ref": []}, {"heading": "The World-to-Words (W2W) Model", "text": "Humans demonstrate fast mapping, the ability to learn new words with only minimal information (Carey and Bartlett, 1978;Carey, 1978;Golinkoff et al., 2000). Motivated by how visual grounding helps humans in bootstrapping new words, we propose a computational framework that first acquires the ability to ground during pretraining, and then transfers this intrinsic ability to learn unseen words when grounded supervision is no longer available. We introduce World-to-Words (W2W), a novel visually-grounded language model with an end-to-end design as illustrated in Figure 4.\nModel Architecture. Similarly to dual-stream vision-language models, W2W encodes the textual input with a pre-trained language model (Liu et al., 2019), and encodes image input with convolutional backbone (He et al., 2016) with 2D positional encoding added. The text and image representations are linearly projected onto a joint semantic space and concatenated. The multimodal representation is then forwarded into a cross-encoder with selfattention layers. The cross-encoded representations in the final layer are sent into an object decoder, together with a set of learnable object queries. The object decoder produces an object embedding for each input object query, which can be considered as a representation of the proposed object. The object representations are further forwarded to the text decoder, which allows language modeling to explicitly attend to the perceived objects. We discuss the pre-training objectives, especially how the model acquires grounding in the following paragraphs. Other details are available in Appendix B.", "publication_ref": ["b4", "b3", "b20", "b41", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "Masked Language Modeling (MLM).", "text": "As an intrinsic task, we follow the majority of existing pretrained vision-language models to perform masked language modeling with a two-layer MLP. Words in input text are randomly masked out, and the model predicts the masked words conditioned on the corrupted sentence and image. Words in groundable phrases are masked with a probability of 0.4 and those in non-groundable regions are masked with a lower probability of 0.1.\nObject Localization (OL). Each object representation will be decoded by a shared three-layer MLP to produce a bounding box. We follow prior detection transformers (DETR) (Carion et al., 2020;Kamath et al., 2021) to perform bipartite matching between proposed boxes and ground truth boxes with a Hungarian loss (Kuhn, 1955). The predicted boxes are optimized towards ground truth using the generalized intersection-over-union (GIoU) loss (Rezatofighi et al., 2019) and the L1 loss.\nGrounding. The notion of Grounding is realized by grounded pre-training through word-region alignment (WRA) which enables fine-grained cross-modal mapping between words and objects. It consists of two levels of alignment: positional alignment and semantic alignment. In positional alignment, the model learns to map each object representation to words in the sentence, which could possibly be a MASK or an additional no-object label \u2205 (Yu and Siskind, 2013;Kamath et al., 2021). We use a fully-connected layer to predict the distribution over token positions with cross-entropy loss.\nIn semantic alignment, the model learns to bring word representations closer to the object representations that they ground to, and push the unrelated pairs farther. We use a contrastive loss over the final layers of the object and text decoders. Table 1: Test results on the seen and unseen words, obtained immediately after pre-training. Unless noted explicitly as fine-tuned (FT), all results reflect the performance of models without fine-tuning. Evaluations under both All and Any-protocols are provided in the table as (All/Any) pairs. For models depending on a frozen pre-trained object detector, we can only provide evaluation under All-Protocol. We note that the unseen words are only unseen to W2W models, as pre-trained baselines have encountered them all during development. We report the results for reference.", "publication_ref": ["b5", "b34", "b58", "b76"], "figure_ref": [], "table_ref": []}, {"heading": "Baselines Groundless", "text": "objectives in the loss function. We refer to this groundless model as W2W w/o G . Like a typical pretrained VLM, e.g., VisualBERT (Li et al., 2019), W2W w/o G performs language modeling based on the object features, without explicit cross-modal referential grounding. We apply W2W w/o G on GOVA task by fine-tuning the model on the pre-training dataset with grounding objective until convergence.\nPre-trained Baselines. For the majority of the pre-trained VLMs, the unseen words are known during pre-training. Also, the primary focus of this work is to understand grounding and bootstrapping in grounded word acquisition. It's not our goal to scale up or re-train all variants of pretraining frameworks. Therefore, we compare our model to the pre-trained VLMs with equal or reasonably larger scales for only reference and analysis purposes. We choose representative baselines in phrase grounding, as presented in Table 1: \u2022 \"Detect-and-Recognize\" Baseline: Models under this framework rely on a pre-trained frozen object detector, and then learn to predict words from proposed objects. We choose the fine-tuned VisualBERT (Li et al., 2019) for this type. \u2022 \"Produce-and-Localize\" Baseline: Models under this framework rely on a pre-trained visionlanguage model to predict the missing word, and then perform referring expression comprehension and propose objects. We combine ViLT (Kim et al., 2021) and MDETR (Kamath et al., 2021) for their competitive performance in vision-conditioned language modeling and phrase grounding individually.  ", "publication_ref": ["b38", "b38", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Pre-training Results on Seen Words", "text": "The main results for the pre-training stage are summarized in Table 1. Our direct observation is the strong performance of W2W in terms of both grounded metrics, Top-1 Grounded Hit-Rate (G-HR@1) and Grounded Perplexity (G-PPL). W2W significantly outperforms the groundless baseline W2W w/o G and pre-trained baselines, even for systems pre-trained with a significantly larger amount of data and computing, as shown in Table 2. While W2W produces correct predictions of the missing words as well as the locations of the corresponding bounding boxes, it turns out to be challenging for baselines to achieve them both. For \"Detect-and-Recognize\" baseline (VisualBERT), we observe a comparable object localization performance empowered by the frozen object detector. However, it suffers from a poor language modeling ability (as demonstrated by HR@1 and PPL, weaker than a finetuned RoBERTa). For the \"Produce-and-Localize\" baseline (ViLT+MDETR), we observe a strong language modeling performance due to the scale of ViLT. Yet, correct word grounding remains difficult, as can be seen from the poor localization performance. These results demonstrate that the GOVA task is challenging, and W2W is competitive in learning grounded word meanings during pre-training.\nBootstrapping through Grounded Objectives.\nWe further provide a cross-time analysis to under-  3% for the unseen words, which are very close to its performance on the seen set and surpass baselines that have seen these words. Moreover, as anticipated, since these words are held out during pre-training, W2W fails to correctly unmask these unseen words, leading to a high log perplexity of 11.01 and low HR of 4.2, compared to that of 1.26 and 66.9 on the seen words. Figure 5 shows an example of such word-agnostic grounding.\nThree men seated on a <MASK> in a small village.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "animal \u2022 W2W", "text": "\u2022 Ground Truth: elephant\nFigure 5: Although the word \"elephant\" is unseen to W2W, the model is still able to localize the object in the image referred to by the MASK.\nThis performance disparity in language modeling and referent localization on unseen words suggests that W2W has developed a certain level of word-agnostic grounding, i.e., to locate the most likely referent of a word through both the linguistic context and the visual context, even if the word itself is never seen during pre-training. A similar situation is faced by human language learners when inferring the grounded meaning of a new word, as we described earlier in Figure 1. Our experiment demonstrates that, through grounded pre-training, it is possible for a vision-language system to acquire word-agnostic grounding ability, which opens up the opportunity to enable human-like fast mapping when learning new words.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Few-Shot New Words Acquisition", "text": "In this section, we task W2W to acquire unseen words from a few samples of raw image-text pairs, without any bounding boxes or word-object mappings annotation. As we have demonstrated the model's word-agnostic grounding, we seek to explore if this ability can be transferred to facilitate learning unseen words when a large amount of data and grounded supervision are no longer available. Specifically, we perform few-shot learning on the pre-trained W2W with only masked language modeling (MLM) as the learning objective. More hyperparameter details are available in Appendix B.2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Learning New Words through Incremental", "text": "Learning. We first explore the multi-class incremental learning setting, in which the pre-trained model is tasked to acquire the 31 unseen words from a few-shot learning session. The experiment is repeated with sample sizes of 8, 16, 24, and 32 immediately after pre-training. As shown in Figure 6, even with as few as 8 samples per word, W2W can significantly bring down the grounded perplexity of unseen words, while mostly maintaining the grounded perplexity of the seen words without catastrophic forgetting. Compared to W2W without the grounding objective, the full W2W demonstrates better acquisition performance for unseen words.\nIt's important to note that these few shot examples are text/image pairs without explicit grounding annotation. Our W2W is able to quickly acquire grounded meanings of the new words (e.g., only with 8 examples) with a performance close to that of seen words. We further perform a word-specific controlled study with a one-class incremental learning setting. We present results on two unseen words (pizza and circular) in  ", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Predictors of Model Behaviors", "text": "There has been an interest to identify predictors that can explain/anticipate the performance or behavior of pre-trained language models (Chang and Bergen, 2022). This exploration not only offers valuable insights for future model development, but also serves as a cognitive inquiry to evaluate the extent to which language models align with human language acquisition patterns. In this section, we present the first work of this nature on visionlanguage models. Specifically, we note that the W2W model relies on a RoBERTa encoder, which might have already been equipped with prior linguistic knowledge. To assess the cognitive alignment of vision-language models to human language acquisition, we additionally pre-trained the W2W and W2W w/o G models with a randomly initialized RoBERTa encoder.\nTo comprehensively capture various aspects of words, we carefully select eight distinct predictors that encompass intrinsic psycho-linguistic characteristics, distribution patterns within the training corpus, and visual representations within the training images. We select 3 psycho-linguistic predictors, each collected and normalized from the MRC Database (Coltheart, 1981):\n\u2022 Familiarity, the degree of familiarity or exposure people have to words; \u2022 Concreteness, the degree to which words have a perceptible physical referent or are associated with tangible objects or experiences; \u2022 Imageability, the degree to which words elicit people's mental imagery.\nAnother 3 linguistic predictors are considered:\n\u2022 Unigram perplexity;\n\u2022 RoBERTa perplexity, where RoBERTa is finetuned on the captions to serve as the upper bound of unimodal language model performance; \u2022 # Co-occur phrases, the average number of co-occurring groundable phrases in a caption.\nWe finally choose 2 perceptual predictors:\n\u2022 # Co-occur objects, the average number of co-occurring objects in an image; \u2022 Bbox size, the average proportion of an image occupied by the bounding boxes of the referents.\nTo assess the statistical significance of each predictor, we performed linear regressions with likelihood ratio tests on different variants of models. Similar to Chang and Bergen (2022), we compare the overall regression including the target predictor to a regression that included all predictors except the target. We additionally present the beta weights (with signs) to capture the magnitude and direction of the correlation. Figure 7  Correlation with Linguistic and Perceptual Predictors. Our findings revealed a positive correlation between the unigram and RoBERTa log perplexity and the models' log perplexity, both for grounded and ungrounded scenarios. This indicates that vision-language models still heavily rely on distributional statistics, similar to unimodal models. While the ungrounded perplexity showed little correlation with perceptual predictors, the Any   IoU demonstrated a significant correlation with the number of co-occurring objects and average sizes of bounding boxes. This suggests concepts that are visually salient and less perceptually ambiguous are easier to localize and acquire, consistent with human learners (Smith and Yu, 2008).\nCorrelation with Psycho-linguistic Predictors.\nCounter-intuitively, there was a positive alignment between the human perceived familiarity of words and the machine's perplexities, i.e., the more familiar humans are with a word, the more perplexed models get. This contrasts with the ideal cognitive plausibility of language acquisition in humans. This discrepancy implies that current visionlanguage models may not fully achieve cognitive plausibility, which might be explained by the fact that many concepts (e.g., wild animals, musical instruments) appear abundantly in internet images but not in daily lives. In terms of imageability, it aligned well with human intuition, exhibiting a positive correlation with Any IoU and a negative correlation with perplexities. However, the concreteness predictor surprisingly exhibited the opposite correlation. This discrepancy could be attributed to the nuanced distinction between imageability and concreteness. For instance, while \"hat\" is concrete because it refers to a tangible object, it also possesses visual diversity due to its generality (e.g., many types of hats which look very differently), making it challenging to acquire. Conversely, \"blue\" is more imageable as it easily evokes a color, relatively stable, despite not referring to a specific tangible object. To learn the meaning of \"hat,\" a human language learner may benefit from physically interacting with the object, and understand that the hat is an item to cover for the head, regardless of its visual appearance. To address this gap, a potential future direction could involve developing language learning agents that acquire words through physical interactions rather than passive perception, allowing for a more comprehensive understanding of word meanings.", "publication_ref": ["b63"], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "Related Work", "text": "Vision-Language Mapping Mapping plays a central role in classic lexicon acquisition problem (Gleitman and Landau, 1994;Clark, 1995). Primarily, researchers focused on grounding words to their meaning symbols, building learning mechanisms using specific mental biases to simulate children's word acquisition, and giving computational accounts for psycholinguistic phenomena (Siskind, 1996;Regier, 2005;Goodman et al., 2007;Fazly et al., 2010). Early efforts along this line incorporate visual grounding either by learning a statistical or neural mapping from object categories (Roy and Pentland, 2002;Yu, 2005;Xu and Tenenbaum, 2007;Yu and Ballard, 2007;Yu and Siskind, 2013) and more complicated visual features (Qu and Chai, 2010;Mao et al., 2019Mao et al., , 2021Pratt et al., 2020) to linguistic labels. These studies are usually in a closed world with limited vocabulary (Krahmer and van Deemter, 2019), and words are usually isolated from the natural context of use. More recently, multi-modal understanding tasks, e.g., object retrieval (Guadarrama et al., 2014;Hu et al., 2016), referring expression comprehension and grounding (Liu et al., 2014;Yu et al., 2016;Mao et al., 2016;Wu et al., 2020), and phrase grounding (Plummer et al., 2015) map referring expressions to corresponding objects. Our setup is closely related to this line as we position grounding as an explicit word-referent mapping problem. The difference is that, our work goes beyond grounding to study open-vocabulary acquisition through fast mapping, a more complicated but realistic challenge faced by AI agents.\nVision-Language Pre-training Distributional word representations can be acquired through language modeling, and developing language models from visual data has been extensively studied by the community (Chrupa\u0142a et al., 2015;Lazaridou et al., 2015;Li et al., 2017;Sur\u0131s et al., 2020). Recent years have seen increasing research to enrich language representations with visually-augmented language modeling (Tan and Bansal, 2020; and to learn multimodal representations with vision-language pre-training (VLP) (Du et al., 2022a). We are particularly interested in VLP models with fine-grained grounding objectives, e.g., Word-Region Alignment (WRA). These models either pre-train with weakly supervised alignment algorithms like optimal transport that matches words with patches (Kim et al., 2021) or proposals from a frozen detector (Chen et al., 2020;Su et al., 2020), or perform explicit word grounding by pre-training a language-conditioned detector (Kamath et al., 2021;Li et al., 2022;Dou et al., 2022). Our model falls along this line, which jointly performs language modeling, object localization, and grounding during pre-training, rather than relying upon a preexisting object detector.\nVision-Language Tasks To evaluate visionlanguage systems, many downstream tasks have been formulated. Some related formulations are summarized in Table 5 in Appendix. While demonstrating some vision-language capabilities, these down-stream tasks provide limited insights into whether these models truly capture the grounded meaning of words with respect to the external environment. Our task design specifically targets the machine's ability to predict words and ground words to perception. More akin to our formulation is the vision-based language modeling task (Jin et al., 2020) in a continual learning setting. Our work differs mainly in two aspects. First, the task proposed by Jin et al. (2020) only predicts masked tokens based on the visual context, which leaves the referential uncertainty (i.e., grounding) unattended (e.g., in Figure 2, correct prediction of the word \"boat\" does not guarantee correct grounding). Also, this work primarily focuses on compositionality, while we seek to address few-shot grounded word learning when unseen words are encountered after pre-training.\nOpen-Vocabulary Object Detection Early works formulate fast mapping of new words as a zero-shot object classification problem, which aims to generalize from known object labels to unknown ones (Socher et al., 2013;Frome et al., 2013;Elhoseiny et al., 2013;Lazaridou et al., 2014). The setting later extends to a localization task, referred to as zero-shot object detection (ZSD) (Bansal et al., 2018;Zhu et al., 2019Rahman et al., 2020). More recently, open-vocabulary object detection (OVD) (Zareian et al., 2021;Gu et al., 2022;Du et al., 2022b;Minderer et al., 2022) combines ZSD with weakly supervised object detection (WSD) to address the unrealistic constrain of traditional zero-shot settings. OVD assumes the availability of coarse-grained image-caption pairs, and attempts to generalize from limited fine-grained annotation of object categories to unseen ones. Nevertheless, this line of work positions words as object categories and isolates them from their linguistic context (e.g., sentences).\nOur setup instead challenges models to perform language modeling in human-generated captions.", "publication_ref": ["b19", "b11", "b62", "b57", "b21", "b17", "b59", "b74", "b72", "b75", "b76", "b55", "b44", "b45", "b53", "b33", "b23", "b27", "b40", "b77", "b46", "b71", "b52", "b10", "b36", "b37", "b67", "b68", "b14", "b32", "b9", "b66", "b39", "b65", "b18", "b16", "b35", "b0", "b80", "b56", "b78", "b22", "b15", "b49"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion and Future Work", "text": "The connection between language and their referents captures the grounded meaning of words, and an explicit treatment is key to empowering efficient open-world language learning abilities in humans and AI agents. This work introduces Grounded Open Vocabulary Acquisition (GOVA), a scalable formulation to examine grounding and fast mapping in open-world grounded language learning. We propose World-to-Words (W2W), a novel visually grounded language model to investigate a paradigm where the model initially acquires grounding ability during pre-training and subsequently applies this ability to quickly learn new words without explicit grounding supervision. Our empirical findings highlight the significance of visual grounding in neural word acquisition. Especially, we find that pre-trained W2W can serve as a foundation for fast mapping of novel grounded words via fewshot learning. We also conduct a comprehensive analysis to explore potential predictors influencing the performance of vision-language models, revealing both consistent and surprising behaviors with respect to human language learning patterns. These insights pave the way for future research in grounded language learning in the open world.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "In this work, we limit ourselves to object-centric grounding, which ignored that language can ground events, attributes, manners, mental states, etc. The grounded meaning of some groundable words, especially ADVs, NUMs, VERBs, and PRONs, cannot be fully captured by the bounding boxes alone. Future work should explore better task formulations to study the acquisition of their grounded meanings. An exciting future work along this line is to extend the setting from images to videos and physical interactions with the environment, and to incorporate the rich temporal dynamics of the world for language acquisition. In addition, we ignored the social aspects of language learning, where children infer the referents of words from their caregivers through communication (Carpenter et al., 1998;Bloom, 2000). Future work could also investigate grounded word acquisition from natural dialogue.", "publication_ref": ["b6", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Ethics Statement", "text": "This project does not involve any research artifacts generated through human subject studies. Despite the considerable promise of W2W, it is crucial to examine its ethical and societal implications. The computational model relies on pre-trained language models and extensive text-image datasets, which could contain hidden biases that may result in fairness problems within the algorithms. By recognizing and actively addressing these implications, we aim to increase awareness among practitioners if the model is deployed as a language-learning agent in the future.\nA GOVA Dataset Details", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1 Illustrated Comparison of Setting", "text": "We present an illustrated comparison of task formulations related to language grounding and grounded language learning in Figure 8. Among these task formulations, our Grounded Open Vocabulary Acquisition (GOVA) task is the only one that challenges vision-language systems to perform visually grounded and object-centric language modeling. The formulation is natural and simple, with fundamental requirements on computational models to perform masked language modeling and object localization, and thus is particularly good for zeroshot analysis.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Evaluation Protocols Explained", "text": "We present an adequate evaluation protocol for grounded word acquisition in the main paper. This section provides more in-depth explanation for the metrics and implementation details for reproducibility purposes.\nPerplexity Metric Details We follow prior practice in cloze tests (Salazar et al., 2020;Jin et al., 2020) to evaluate the perplexity of a word w. We use log pseudo-perplexity in masked language modeling, defined as log PPL(w) = \u2212 log P (w|x img , x cap )\nHowever, the majority of the language models employ sub-word tokenization methods to segment and encode text. In particular, one lexical word can be segmented into several tokens, and different tokenizers can lead to different tokens for the same input. We thus introduce a tokenizer-dependent measure for perplexity. For tokenizer T , we represent the N tokens of word w as T (w) and\nlog PPL(w) = \u2212 1 N t\u2208T (w) log P (t|x img , x cap )\nIoU Metric Details we face the same challenge as Kamath et al. (2021) where multiple referents are possible for a masked word. In a similar manner, we adopt the Any-Protocol and All-Protocol to evaluate the grounded detection task. The intersection-over-union (IoU) under All-Protocols is defined as the IoU between the joint bounding box of ground truth and predicted bounding boxes: IoU all = IoU(\u222aB, \u222a B)", "publication_ref": ["b60"], "figure_ref": [], "table_ref": []}, {"heading": "A.3 Word List", "text": "\u2022 60 words are in the seen-set, each with 80 test cases: baby, ball, beach, bench, bike, black, blond, blue, boy, brown, building, car, child, dark, dog, dress, face, female, field, floor, food, girl, glasses, grass, gray, green, guitar, guy, hair, hand, hat, head, horse, jacket, jeans, lady, large, little, long, man, orange, pants, person, player, red, shirt, sidewalk, sign, small, snow, street, striped, table, top, wall, water ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.2 Hyper-parameter Decisions", "text": "We include the major hyper-parameter tuning decisions for reproducibility purpose. For more details, please refer to the supplementary codes.\n\u2022 Learning Rate:\n- ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.4 Evaluation on GOVA W2W", "text": "For our proposed W2W model, given a GOVA test, with its corresponding image and textual cloze pair passing into the model, the bounding box predictions are generated by keeping only the bounding box proposals that are mapped to at least one masked token within the cloze, while the masked token prediction results are directly decoded from its language modeling head.\nVisualBERT For the \"Detect-and-Recognize\" baseline model VisualBERT, we use phrasegrounding fine-tuned version of VisualBERT to perform object localization, and, as it lacks the language modeling head, another vanilla pre-trained VisualBERT to perform mask token prediction. Specifically, for the bounding box localization part, we treat it as a standard phrase grounding task and follow (Li et al., 2019) to select the top-1 bounding box prediction in the last masked token as the output.\nViLT+MDETR For the \"Produce-and-Localize\" baseline model ViLT + MDETR, in stage one, we feed the input image and text into ViLT, collecting its top-1 cloze token prediction result. Then, at stage two, the input image and ViLT-completed text are fed into MDETR, performing phrase-grounding to localize the object associated with the original cloze. Finally, the cloze token prediction result from ViLT together with the bounding box proposals from MDETR are used for GOVA evaluation.", "publication_ref": ["b38"], "figure_ref": [], "table_ref": []}, {"heading": "D Addendum to Results", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.1 Ablation Study", "text": "We Following the analysis of Chang and Bergen (2022) in unimodal language models, we present the KL-Divergence between the model predictions and the unigram distribution in Figure 9. An immediate observation is that all variants converge to the shallow unigram statistics at around 10 2 steps of pre-training. This aligns with the findings of Chang and Bergen (2022) that unimodal language models would converge to unigram before acquiring more complicated contextual representations. We noticed that in both text-only and W2W w/o O cases where MLM is the only pre-training objective, the models tend to stay around the unigram word distribution even with 10 4 steps of training. However, variants with an object-centric representation quickly departed from the unigram distribution. Comparatively, models with language model initialization moves quickly away from the unigram distribution, and models with a grounded objective have a marginally faster deviation. These results confirm that vision-language models can benefit from unimodal pre-training on a large corpus, and that performing language modeling upon object representations is crucial. We note that we compare the KL-Divergence from unigram only to understand the models' behaviors, and the metric itself does not serve as an evaluation of a system's performance in grounded open vocabulary acquisition.", "publication_ref": [], "figure_ref": ["fig_8"], "table_ref": []}, {"heading": "D.2 Addendum to Results in Multi-Class Incremental Learning", "text": "We present additional results in Table 6.   ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_13"]}, {"heading": "D.3 Learning New Words through One-Class", "text": "Incremental Learning.\nWe further perform a more controlled study with a word-specific one-class incremental learning setting. The pre-trained model is tasked to acquire one single unseen word from a few-shot learning session with |V unseen | = 1. The results of this section are obtained from the test immediately following the new session. We present the test result in Table 7. Again, we observe that with as few as 8 samples, W2W can achieve a satisfyingly low grounded perplexity. In the majority of the cases, W2W demonstrates the better ability to acquire unseen words over the groundless baseline.  ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_15"]}, {"heading": "Acknowledgments", "text": "This work was supported in part by NSF IIS-1949634, NSF SES-2128623, and by the Automotive Research Center (ARC) at the University of Michigan. The authors would like to thank the anonymous reviewers for their valuable feedback.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "boat, people, small, large, yellow, white, green, pink. boat, people, small, large, yellow, white, green, pink.\nTwo boats of people kayaking, a smaller yellow boat with two people and a larger white boat with six people.\nTwo boats of people kayaking, a smaller yellow boat with two people and a larger white boat with six people. a smaller yellow boat a smaller yellow boat a smaller yellow boat a smaller yellow boat Two boats of people kayaking, a smaller yellow <mask> with two people and a larger white boat with six people.\nTwo boats of people kayaking, a smaller yellow boat with two people and a larger white boat with six people.\nTwo boats of people kayaking, a smaller yellow <mask> with two people and a larger white boat with six people.\nTwo boats of people kayaking, a smaller yellow boat with two people and a larger white boat with six people.   Object Localization (OL). We follow MDETR to decode object embeddings with a three-layer MLP to produce bounding boxes. Similar to most prior work, we apply a filter over boxes with confidence below 0.7. In our framework, this means that the object corresponds to the no-object label \u2205 (Figure 4) with a probability over 0.3. We strictly follow DETR to perform bipartite matching between proposed boxes and ground truth boxes with a Hungarian loss. The predicted boxes are optimized towards ground truth by the generalized intersectionover-union (GIoU) loss and the L1 loss.\nGrounding. In positional alignment, the model learns to map each object representation to tokens in the sentence with a fixed length of 257, which could possibly be a MASK or an additional no-object label \u2205 (Figure 4). The object and the token are considered a match given a mapping probability over 0.1. We use a fully-connected layer to predict the distribution over token positions with crossentropy loss. In semantic alignment, the model learns to bring word embeddings closer to the object embeddings that they ground to, and push the unrelated pairs farther. We strictly follow the contrastive loss function defined in MDETR for every object and groundable token for this purpose.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "ACL 2023 Responsible NLP Checklist", "text": "A For every submission:\nA1. Did you describe the limitations of your work?\nSection 7, Limitations A2. Did you discuss any potential risks of your work?\nThis study does not contain any human subjects or human studies. The study proposes a problem formulation and a computational framework, which is not deployable to any real-world applications in the foreseeable future. B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nWill be included along with the code release B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\nWill be included along with the code release B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Not applicable. Left blank.\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? Not applicable. Left blank.\nB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Zero-shot object detection", "journal": "", "year": "2018", "authors": "Ankan Bansal; Karan Sikka; Gaurav Sharma; Rama Chellappa; Ajay Divakaran"}, {"ref_id": "b1", "title": "Experience grounds language", "journal": "", "year": "2020", "authors": "Yonatan Bisk; Ari Holtzman; Jesse Thomason; Jacob Andreas; Yoshua Bengio; Joyce Chai; Mirella Lapata; Angeliki Lazaridou; Jonathan May; Aleksandr Nisnevich; Nicolas Pinto; Joseph Turian"}, {"ref_id": "b2", "title": "How children learn the meanings of words", "journal": "MIT press", "year": "2000", "authors": "Paul Bloom"}, {"ref_id": "b3", "title": "The child as word learner. Linguistic theory and psychological reality", "journal": "", "year": "1978", "authors": "Susan Carey"}, {"ref_id": "b4", "title": "Acquiring a single new word", "journal": "Papers and Reports on Child Language Development", "year": "1978", "authors": "Susan Carey; Elsa Bartlett"}, {"ref_id": "b5", "title": "End-to-end object detection with transformers", "journal": "Springer", "year": "2020", "authors": "Nicolas Carion; Francisco Massa; Gabriel Synnaeve; Nicolas Usunier; Alexander Kirillov; Sergey Zagoruyko"}, {"ref_id": "b6", "title": "Social cognition, joint attention, and communicative competence from 9 to 15 months of age. Monographs of the society for research in child development", "journal": "", "year": "1998", "authors": "Malinda Carpenter; Katherine Nagell; Michael Tomasello; George Butterworth; Chris Moore"}, {"ref_id": "b7", "title": "Fiber: Fill-in-the-blanks as a challenging video understanding evaluation framework", "journal": "", "year": "2022", "authors": "Santiago Castro; Ruoyao Wang; Pingxuan Huang; Ian Stewart; Oana Ignat; Nan Liu; Jonathan Stroud; Rada Mihalcea"}, {"ref_id": "b8", "title": "Word acquisition in neural language models", "journal": "Transactions of the Association for Computational Linguistics", "year": "2022", "authors": "A Tyler;  Chang;  Benjamin K Bergen"}, {"ref_id": "b9", "title": "Uniter: Universal image-text representation learning", "journal": "", "year": "2020", "authors": "Yen-Chun Chen; Linjie Li; Licheng Yu; Ahmed El Kholy; Faisal Ahmed; Zhe Gan; Yu Cheng; Jingjing Liu"}, {"ref_id": "b10", "title": "Learning language through pictures", "journal": "Association for Computational Linguistics", "year": "2015", "authors": "Grzegorz Chrupa\u0142a; \u00c1kos K\u00e1d\u00e1r; Afra Alishahi"}, {"ref_id": "b11", "title": "The lexicon in acquisition", "journal": "Cambridge University Press", "year": "1995", "authors": "V Eve;  Clark"}, {"ref_id": "b12", "title": "The mrc psycholinguistic database", "journal": "The Quarterly Journal of Experimental Psychology Section A", "year": "1981", "authors": ""}, {"ref_id": "b13", "title": "", "journal": "", "year": "", "authors": "Zi-Yi Dou; Aishwarya Kamath; Zhe Gan; Pengchuan Zhang; Jianfeng Wang; Linjie Li; Zicheng Liu; Ce Liu; Yann Lecun; Nanyun Peng"}, {"ref_id": "b14", "title": "A survey of vision-language pre-trained models", "journal": "", "year": "2022", "authors": "Yifan Du; Zikang Liu; Junyi Li; Wayne Xin Zhao"}, {"ref_id": "b15", "title": "Learning to prompt for open-vocabulary object detection with visionlanguage model", "journal": "", "year": "2022", "authors": "Yu Du; Fangyun Wei; Zihe Zhang; Miaojing Shi; Yue Gao; Guoqi Li"}, {"ref_id": "b16", "title": "Write a classifier: Zero-shot learning using purely textual descriptions", "journal": "", "year": "2013", "authors": "Mohamed Elhoseiny; Babak Saleh; Ahmed Elgammal"}, {"ref_id": "b17", "title": "A probabilistic computational model of cross-situational word learning", "journal": "Cognitive Science", "year": "2010", "authors": "Afsaneh Fazly; Afra Alishahi; Suzanne Stevenson"}, {"ref_id": "b18", "title": "Devise: A deep visual-semantic embedding model", "journal": "", "year": "2013", "authors": "Andrea Frome; Greg S Corrado; Jon Shlens; Samy Bengio; Jeff Dean; Marc'aurelio Ranzato; Tomas Mikolov"}, {"ref_id": "b19", "title": "The acquisition of the lexicon", "journal": "mit Press", "year": "1994", "authors": "Barbara Lila R Gleitman;  Landau"}, {"ref_id": "b20", "title": "Becoming a word learner: A debate on lexical acquisition", "journal": "Oxford University Press", "year": "2000", "authors": "Roberta Michnick Golinkoff; Kathryn Hirsh-Pasek; Lois Bloom; B Linda; Amanda L Smith; Nameera Woodward; Michael Akhtar; George Tomasello;  Hollich"}, {"ref_id": "b21", "title": "A bayesian framework for cross-situational word-learning", "journal": "", "year": "2007", "authors": "Noah Goodman; Joshua Tenenbaum; Michael Black"}, {"ref_id": "b22", "title": "Open-vocabulary object detection via vision and language knowledge distillation", "journal": "", "year": "2022", "authors": "Xiuye Gu; Tsung-Yi Lin; Weicheng Kuo; Yin Cui"}, {"ref_id": "b23", "title": "Open-vocabulary object retrieval", "journal": "", "year": "2014", "authors": "Sergio Guadarrama; Erik Rodner; Kate Saenko; Ning Zhang; Ryan Farrell; Jeff Donahue; Trevor Darrell"}, {"ref_id": "b24", "title": "LVIS: A dataset for large vocabulary instance segmentation", "journal": "", "year": "2019", "authors": "Agrim Gupta; Piotr Dollar; Ross Girshick"}, {"ref_id": "b25", "title": "The symbol grounding problem", "journal": "Physica D: Nonlinear Phenomena", "year": "1990", "authors": "Stevan Harnad"}, {"ref_id": "b26", "title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"ref_id": "b27", "title": "Natural language object retrieval", "journal": "", "year": "2016", "authors": "Ronghang Hu; Huazhe Xu; Marcus Rohrbach; Jiashi Feng; Kate Saenko; Trevor Darrell"}, {"ref_id": "b28", "title": "Ram Nevatia, and Xiang Ren. 2020. Visually grounded continual learning of compositional phrases", "journal": "Online. Association for Computational Linguistics", "year": "", "authors": "Xisen Jin; Junyi Du; Arka Sadhu"}, {"ref_id": "b29", "title": "Ishan Misra, and Nicolas Carion. 2021. Mdetr-modulated detection for end-to-end multi-modal understanding", "journal": "", "year": "", "authors": "Aishwarya Kamath; Mannat Singh; Yann Lecun; Gabriel Synnaeve"}, {"ref_id": "b30", "title": "ReferItGame: Referring to objects in photographs of natural scenes", "journal": "Association for Computational Linguistics", "year": "2014", "authors": "Sahar Kazemzadeh; Vicente Ordonez; Mark Matten; Tamara Berg"}, {"ref_id": "b31", "title": "Measuring catastrophic forgetting in neural networks", "journal": "", "year": "2018", "authors": "Ronald Kemker; Marc Mcclure; Angelina Abitino; Tyler Hayes; Christopher Kanan"}, {"ref_id": "b32", "title": "Vilt: Vision-and-language transformer without convolution or region supervision", "journal": "PMLR", "year": "2021", "authors": "Wonjae Kim; Bokyung Son; Ildoo Kim"}, {"ref_id": "b33", "title": "Computational generation of referring expressions: An updated survey", "journal": "", "year": "2019", "authors": "Emiel Krahmer;  Kees Van Deemter"}, {"ref_id": "b34", "title": "The hungarian method for the assignment problem", "journal": "", "year": "1955", "authors": " Harold W Kuhn"}, {"ref_id": "b35", "title": "Is this a wampimuk? cross-modal mapping between distributional semantics and the visual world", "journal": "Long Papers", "year": "2014", "authors": "Angeliki Lazaridou; Elia Bruni; Marco Baroni"}, {"ref_id": "b36", "title": "Combining language and vision with a multimodal skip-gram model", "journal": "", "year": "2015", "authors": "Angeliki Lazaridou;  Nghia The; Marco Pham;  Baroni"}, {"ref_id": "b37", "title": "Learning visual n-grams from web data", "journal": "", "year": "2017", "authors": "Ang Li; Allan Jabri; Armand Joulin; Laurens Van Der Maaten"}, {"ref_id": "b38", "title": "Visualbert: A simple and performant baseline for vision and language", "journal": "", "year": "2019", "authors": "Liunian Harold Li; Mark Yatskar; Da Yin; Cho-Jui Hsieh; Kai-Wei Chang"}, {"ref_id": "b39", "title": "Grounded language-image pre-training", "journal": "", "year": "2022", "authors": "Pengchuan Liunian Harold Li; Haotian Zhang; Jianwei Zhang; Chunyuan Yang; Yiwu Li; Lijuan Zhong; Lu Wang; Lei Yuan; Jenq-Neng Zhang;  Hwang"}, {"ref_id": "b40", "title": "Probabilistic labeling for efficient referential grounding based on collaborative discourse", "journal": "Association for Computational Linguistics", "year": "2014", "authors": "Changsong Liu; Lanbo She; Rui Fang; Joyce Y Chai"}, {"ref_id": "b41", "title": "Roberta: A robustly optimized bert pretraining approach", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b42", "title": "Imaginationaugmented natural language understanding", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Yujie Lu; Wanrong Zhu; Xin Wang; Miguel Eckstein; William Yang Wang"}, {"ref_id": "b43", "title": "Incremental class learning-an approach to longlife and scalable learning", "journal": "IEEE", "year": "1999", "authors": "Jacek Mandziuk; Lokendra Shastri"}, {"ref_id": "b44", "title": "The neurosymbolic concept learner: Interpreting scenes, words, sentences from natural supervision", "journal": "", "year": "2019", "authors": "Jiayuan Mao; Chuang Gan; Pushmeet Kohli; Joshua B Tenenbaum; Jiajun Wu"}, {"ref_id": "b45", "title": "Grammar-based grounded lexicon learning", "journal": "", "year": "2021", "authors": "Jiayuan Mao; Freda H Shi; Jiajun Wu; Roger P Levy; Joshua B Tenenbaum"}, {"ref_id": "b46", "title": "Generation and comprehension of unambiguous object descriptions", "journal": "", "year": "2016", "authors": "Junhua Mao; Jonathan Huang; Alexander Toshev; Oana Camburu; Alan L Yuille; Kevin Murphy"}, {"ref_id": "b47", "title": "Pointer sentinel mixture models", "journal": "", "year": "2017", "authors": "Stephen Merity; Caiming Xiong; James Bradbury; Richard Socher"}, {"ref_id": "b48", "title": "Viewing and naming objects: Eye movements during noun phrase production. Cognition", "journal": "", "year": "1998", "authors": "S Antje; Astrid M Meyer; Willem Jm Sleiderink;  Levelt"}, {"ref_id": "b49", "title": "Simple open-vocabulary object detection with vision transformers", "journal": "", "year": "2022", "authors": "Matthias Minderer; Alexey Gritsenko; Austin Stone; Maxim Neumann; Dirk Weissenborn; Alexey Dosovitskiy; Aravindh Mahendran; Anurag Arnab; Mostafa Dehghani; Zhuoran Shen"}, {"ref_id": "b50", "title": "The LAMBADA dataset: Word prediction requiring a broad discourse context", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Denis Paperno; Germ\u00e1n Kruszewski; Angeliki Lazaridou; Ngoc Quan Pham; Raffaella Bernardi; Sandro Pezzelle; Marco Baroni; Gemma Boleda; Raquel Fern\u00e1ndez"}, {"ref_id": "b51", "title": "Language models as knowledge bases?", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Fabio Petroni; Tim Rockt\u00e4schel; Sebastian Riedel; Patrick Lewis; Anton Bakhtin; Yuxiang Wu; Alexander Miller"}, {"ref_id": "b52", "title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models", "journal": "", "year": "2015", "authors": "A Bryan; Liwei Plummer; Chris M Wang; Juan C Cervantes; Julia Caicedo; Svetlana Hockenmaier;  Lazebnik"}, {"ref_id": "b53", "title": "Grounded situation recognition", "journal": "Springer", "year": "2020", "authors": "Sarah Pratt; Mark Yatskar; Luca Weihs; Ali Farhadi; Aniruddha Kembhavi"}, {"ref_id": "b54", "title": "Stanza: A python natural language processing toolkit for many human languages", "journal": "", "year": "2020", "authors": "Peng Qi; Yuhao Zhang; Yuhui Zhang; Jason Bolton; Christopher D Manning"}, {"ref_id": "b55", "title": "Context-based word acquisition for situated dialogue in a virtual world", "journal": "Journal of Artificial Intelligence Research", "year": "2010", "authors": "Shaolin Qu; Joyce Yue Chai"}, {"ref_id": "b56", "title": "Improved visual-semantic alignment for zero-shot object detection", "journal": "", "year": "2020", "authors": "Shafin Rahman; Salman Khan; Nick Barnes"}, {"ref_id": "b57", "title": "The emergence of words: Attentional learning in form and meaning", "journal": "Cognitive science", "year": "2005", "authors": "Terry Regier"}, {"ref_id": "b58", "title": "Generalized intersection over union: A metric and a loss for bounding box regression", "journal": "", "year": "2019", "authors": "Hamid Rezatofighi; Nathan Tsoi; Junyoung Gwak; Amir Sadeghian; Ian Reid; Silvio Savarese"}, {"ref_id": "b59", "title": "Learning words from sights and sounds: A computational model", "journal": "Cognitive science", "year": "2002", "authors": "K Deb; Alex P Roy;  Pentland"}, {"ref_id": "b60", "title": "Masked language model scoring", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Julian Salazar; Davis Liang; Toan Q Nguyen; Katrin Kirchhoff"}, {"ref_id": "b61", "title": "R-fcn-3000 at 30fps: Decoupling detection and classification", "journal": "", "year": "2018", "authors": "Bharat Singh; Hengduo Li; Abhishek Sharma; Larry S Davis"}, {"ref_id": "b62", "title": "A computational study of cross-situational techniques for learning word-tomeaning mappings", "journal": "Cognition", "year": "1996", "authors": "Jeffrey Mark Siskind"}, {"ref_id": "b63", "title": "Infants rapidly learn word-referent mappings via cross-situational statistics", "journal": "Cognition", "year": "2008", "authors": "Linda Smith; Chen Yu"}, {"ref_id": "b64", "title": "From the outside-in: Embodied attention in toddlers", "journal": "Springer", "year": "2007", "authors": "B Linda; Chen Smith; Alfredo Yu;  Pereira"}, {"ref_id": "b65", "title": "Zero-shot learning through cross-modal transfer", "journal": "", "year": "2013", "authors": "Richard Socher; Milind Ganjoo; D Christopher; Andrew Manning;  Ng"}, {"ref_id": "b66", "title": "Vl-bert: Pre-training of generic visual-linguistic representations", "journal": "", "year": "2020", "authors": "Weijie Su; Xizhou Zhu; Yue Cao; Bin Li; Lewei Lu; Furu Wei; Jifeng Dai"}, {"ref_id": "b67", "title": "Learning to learn words from visual scenes", "journal": "", "year": "2020", "authors": "D\u0131dac Sur\u0131s; Dave Epstein; Heng Ji; Shih-Fu Chang; Carl Vondrick"}, {"ref_id": "b68", "title": "Vokenization: Improving language understanding with contextualized, visual-grounded supervision", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Hao Tan; Mohit Bansal"}, {"ref_id": "b69", "title": "Integration of visual and linguistic information in spoken language comprehension", "journal": "Science", "year": "1995", "authors": "K Michael;  Tanenhaus; J Michael; Kathleen M Spivey-Knowlton; Julie C Eberhard;  Sedivy"}, {"ref_id": "b70", "title": "Visually-augmented language modeling", "journal": "", "year": "2022", "authors": "Weizhi Wang; Li Dong; Hao Cheng; Haoyu Song; Xiaodong Liu; Xifeng Yan; Jianfeng Gao; Furu Wei"}, {"ref_id": "b71", "title": "Phrasecut: Language-based image segmentation in the wild", "journal": "", "year": "2020", "authors": "Chenyun Wu; Zhe Lin; Scott Cohen; Trung Bui; Subhransu Maji"}, {"ref_id": "b72", "title": "Word learning as bayesian inference", "journal": "Psychological review", "year": "2007", "authors": "Fei Xu; Joshua B Tenenbaum"}, {"ref_id": "b73", "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "journal": "Transactions of the Association for Computational Linguistics", "year": "2014", "authors": "Peter Young; Alice Lai; Micah Hodosh; Julia Hockenmaier"}, {"ref_id": "b74", "title": "The emergence of links between lexical acquisition and object categorization: A computational study", "journal": "Connection science", "year": "2005", "authors": "Chen Yu"}, {"ref_id": "b75", "title": "A unified model of early word learning: Integrating statistical and social cues", "journal": "Neurocomputing", "year": "2007", "authors": "Chen Yu; H Dana;  Ballard"}, {"ref_id": "b76", "title": "Grounded language learning from video described with sentences", "journal": "Long Papers", "year": "2013", "authors": "Haonan Yu; Jeffrey Mark Siskind"}, {"ref_id": "b77", "title": "Modeling context in referring expressions", "journal": "Springer", "year": "2016", "authors": "Licheng Yu; Patrick Poirson; Shan Yang; Alexander C Berg; Tamara L Berg"}, {"ref_id": "b78", "title": "Open-vocabulary object detection using captions", "journal": "", "year": "2021", "authors": "Alireza Zareian; Kevin Dela Rosa; Derek Hao Hu; Shih-Fu Chang"}, {"ref_id": "b79", "title": "Regionclip: Region-based language-image pretraining", "journal": "", "year": "2022", "authors": "Yiwu Zhong; Jianwei Yang; Pengchuan Zhang; Chunyuan Li; Noel Codella; Liunian Harold Li; Luowei Zhou; Xiyang Dai; Lu Yuan; Yin Li"}, {"ref_id": "b80", "title": "Zero shot detection. IEEE Transactions on Circuits and Systems for Video Technology", "journal": "", "year": "2019", "authors": "Pengkai Zhu; Hanxiao Wang; Venkatesh Saligrama"}, {"ref_id": "b81", "title": "Don't even look once: Synthesizing features for zero-shot detection", "journal": "", "year": "2020", "authors": "Pengkai Zhu; Hanxiao Wang; Venkatesh Saligrama"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Even when the term \"incinerator\" (highlighted yellow) is new to human learners, they can still locate the most likely referent (indicated by the yellow bounding box) in the perceived world by grounding.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "FewFigure 3 :3Figure 3: An illustration of the few-shot new word learning paradigm. The model first pre-trains on a grounding dataset with a set of base words (V seen ), and then attempts to acquire a set of unseen words (V unseen ) in a small number of raw text-image pairs. Tests are performed after each training session.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 6 :6Figure6: The log G-PPL (All-Protocol) of seen and unseen words in multi-class incremental learning, each unseen word with a sample size ranging from 8 to 32.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "displays heatmaps indicating the statistical significance (in terms of negative logarithmic p-values) of each predictor concerning Log G-PPL, Log PPL, and Any IoU. Insignificant tests are omitted from the figure.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 7 :7Figure 7: Heatmaps for statistical significance for each predictor towards the Log G-PPL, Log PPL, and Any IoU.The beta weights and their signs are presented outside of the parentheses, and the negative log p-values are presented in the parentheses. Insignificant tests with p > 0.05, i.e., \u2212 log(p) < 1.30, are discarded.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Assuming n ground truth bounding boxes B = {b 1 , b 2 , \u2022 \u2022 \u2022 , b n } and m predicted bounding boxes B = { b 1 , b 2 , \u2022 \u2022 \u2022 , b m }. The intersection-overunion (IoU) under Any-Protocols is defined as the average IoU of the best matching predicted bounding box for each ground truth object:", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 9 :9Figure 9: KL-divergence between model's token prediction and the unigram distribution of the training corpus.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "", "figure_data": "Two boats of people, a smaller <mask> boat with two people and a <mask> white boat with six people.Q L K L V LText DecoderTwo boats of people, a smaller yellow boat with two people and a larger white boat with six people.\u00d8Q O K O V OObject DecoderFigure 4: An overview of the W2W architecture, a visually grounded language model pre-trained with three objectives: masked language modeling (MLM), object localization (OL), and grounding through word-region alignment (WRA).to parse thecaption, enumerate every word in the groundablephrase, and identify those with a POS tag of NOUNor ADJ. These groundable words are replaced byMASK one at a time and matched to their correspond-ing bounding boxes.The dataset is divided into 4 splits: pre-trainingset, unseen words training set, seen words test set,and unseen words test set. We start by selecting 31unseen words and holding out all text-image pairscontaining these words from the training split ofFlickr30K Entities. The hold-out text-image pairsare further divided into the training and test setsfor unseen words. The remaining training splitof Flickr30K Entities is used for the pre-trainingset. To prevent frequent words (e.g., \"man\") fromdominating the test results of the seen words, wechoose 60 seen words and sample an equal numberof test instances for each word from the test splitof Flickr30K Entities. More details and statisticsof the dataset are available in Appendix A."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "The baselines for comparisons and references. ITM stands for Image Text Matching, and all the other abbreviations follow Section 2.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "stand the role of grounded objectives in pre-training efficiency. The results of different training steps are provided in Table3. From the table, we observe that W2W outperforms both of its groundless variants in language modeling, object localization, and jointly under the grounded perplexity. What's even more striking is that W2W achieves better performance with 10 times less training data compared to the model trained without the grounding objective (i.e., the WRA objective).", "figure_data": "These results confirmthe crucial role of explicit word-object alignmentin efficient grounded word learning. This can beexplained by that the grounded objectives attemptto align the vision and language semantic spaces,which ideally benefit both visually conditioned lan-guage modeling and language-conditioned objectlocalization. Although it is possible to build amapping between word and object representationsthrough cross-modal probing and fine-tuning af-ter pre-training, these methods are not comparableto systems with grounded objectives in terms ofefficiency and performance.# StepsMetricsW2WW2W w/o G (FT)10kIoU (\u2191) log PPL (\u2193) log G-PPL (\u2193) 2.22 / 2.23 46.7 / 46.2 1.4636.9 / 35.3 1.53 2.52 / 2.5750kIoU (\u2191) log PPL (\u2193) log G-PPL (\u2193) 1.80 / 1.82 58.1 / 57.1 1.2639.6 / 38.8 1.44 2.34 / 2.38100kIoU (\u2191) log PPL (\u2193) log G-PPL (\u2193) 1.79 / 1.81 58.7 / 57.6 1.2640.0 / 38.2 1.41 2.34 / 2.38"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Comparison of W2W and its non-grounding version at different training steps. W2W w/o G is evaluated using fine-tuning. Both Any and All-protocols are provided in the table as (All/Any) pairs. Pre-training Results on Unseen Words: Word-Agnostic Grounding One important finding of the pre-trained model is the surprising performance in localizing the unseen words behind the MASKs. As shown in Table 1, W2W achieves a high Any-IoU of 56.3% and Any-localization accuracy of 61.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "The complete results are available in Appendix D.", "figure_data": "# Sampleslog G-PPL (pizza) log G-PPL (circular)W2WW2W w/o GW2WW2W w/o G010.709.5915.2115.128 16 24 321.47 1.07 1.19 0.902.21 2.54 1.25 1.181.59 1.07 1.55 1.232.25 2.25 1.81 1.61"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Few-shot Learning Details.Since no bounding box or word-object mappings annotation is available, we train W2W with only masked language modeling (MLM) in few-sample new word learning. We reduce the batch size to 8 considering the fewer number of samples, and set the convergence criteria to a fixed number, i.e., 50 steps. All the rest of the experimental settings remain the same as pre-training.", "figure_data": "B.2 C Experiment ReproducibilityC.1 W2W Implementation DetailsOur W2W model mainly consists of one cross-modal transformer with inputs from uni-modal en-coders from image and text domain. Specially,we select the ResNet-50 (He et al., 2016) pre-trained on ImageNet from TIMM 4 as the image en-coder, and RoBERTa-base (Liu et al., 2019) from huggingface 5 as the text encoder. The cross-modal encoder and two decoders each consists of4 transformer blocks with 8 attention heads, aninput and output dimensionality of 512, and an, white, woman, yellow,inner-layer dimensionality of 2,048. Besides, 50young.learnable object queries are included to query the cross-modal decoder to generate bounding box pro-posals.\u2022 31 words are in the unseen-set, each with 50 test cases 2 : aged, bamboo, barefoot, brush, button, cafe, cheese, circular, classroom, crosswalk, di-verse, doctor, donkey, elephant, fluffy, foreign,gym, heart, newborn, pan, pizza, product, se-curity, sink, star, steep, stove, student, teacher,telephone, warm.B Computational Model DetailsB.1 Pre-training ObjectivesMasked Language Modeling (MLM). The MLM head can be placed at multiple possibleplaces, and our design is an exploration after pre-liminary experiments on smaller-scale training. Westrictly follow the setup of RoBERTa to implementthe MLM head with a two-layer MLP, based on the implementation of huggingface 3 . Words ingroundable phrases are masked with a probabil-ity of 0.4 and those in non-groundable regions aremasked with a lower probability of 0.1. For a tokenselected to mask, we follow RoBERTa to assign aprobability of 80% to replace with MASK, 10% witha random token, and 10% to do nothing."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Computational ResourcesOur W2W models is pre-trained on 8 NVidia A40 GPUs. With mixed-precision pre-training and a batch size of 128, W2W was trained for 150,000 steps where each step takes about 1.4 second.", "figure_data": "C.3Image Encoder: frozen-Text Encoder: 1 \u00d7 10 \u22125 -Multi-modal Transformer: 1 \u00d7 10 \u22124 \u2022 Batch Size: 128\u2022 Pre-training Loss Coefficients:-MLM Loss: 32-Cross Entropy for Positional Alignment: 1-Contrastive Loss for Semantic Alignment: 1-L1 Localization Loss: 5-GIoU Localization Loss: 2\u2022 Few-shot Learning:-Batch size: 8-Other Hyper-parameters: Same as Pre-training"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "The log G-PPL (All-Protocol) of seen and unseen words in multi-class incremental learning, each unseen word with a sample size ranging from 8 to 32.", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "The log G-PPL (All-Protocol) of unseen words in one-class incremental learning, each unseen word with a sample size ranging from 8 to 32.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "O w = {o 1 , o 2 , \u2022 \u2022 \u2022 , o n }", "formula_coordinates": [2.0, 306.14, 384.11, 102.44, 18.93]}, {"formula_id": "formula_1", "formula_text": "I \u00d7 T \u2192 V \u00d7 R 4n .", "formula_coordinates": [2.0, 378.41, 423.13, 87.16, 20.55]}, {"formula_id": "formula_2", "formula_text": "log PPL(w) = \u2212 log P (w|x img , xcap)(1)", "formula_coordinates": [3.0, 109.96, 134.57, 179.78, 16.88]}, {"formula_id": "formula_3", "formula_text": "B = {b 1 , b 2 , \u2022 \u2022 \u2022 , b n } and m predicted bounding boxes B = { b 1 , b 2 , \u2022 \u2022 \u2022 , b m }, the intersection-over-union (IoU)", "formula_coordinates": [3.0, 70.5, 212.85, 218.64, 53.13]}, {"formula_id": "formula_4", "formula_text": "IoUany = 1 n i\u2208{1,2,\u2022\u2022\u2022 ,n} max j\u2208{1,2,\u2022\u2022\u2022 ,m} IoU(bi, bj) (2) IoU all = IoU(\u222aB, \u222a B)(3)", "formula_coordinates": [3.0, 91.15, 275.77, 198.58, 47.98]}, {"formula_id": "formula_5", "formula_text": "log G-PPL(w) = \u221e if IoU = 0 log PPL(w) \u2212 log IoU else (4)", "formula_coordinates": [3.0, 83.38, 455.19, 206.36, 34.92]}, {"formula_id": "formula_6", "formula_text": "log PPL(w) = \u2212 1 N t\u2208T (w) log P (t|x img , x cap )", "formula_coordinates": [15.0, 80.16, 607.42, 199.66, 31.35]}], "doi": "10.18653/v1/2020.emnlp-main.703"}