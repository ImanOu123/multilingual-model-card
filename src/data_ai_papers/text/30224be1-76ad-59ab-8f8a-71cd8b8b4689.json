{"title": "Synthesizing Aspect-Driven Recommendation Explanations from Reviews", "authors": "Trung-Hoang Le; Hady W Lauw", "pub_date": "", "abstract": "Explanations help to make sense of recommendations, increasing the likelihood of adoption. However, existing approaches to explainable recommendations tend to rely on rigid, standardized templates, customized only via fill-in-the-blank aspect sentiments. For more flexible, literate, and varied explanations covering various aspects of interest, we synthesize an explanation by selecting snippets from reviews, while optimizing for representativeness and coherence. To fit target users' aspect preferences, we contextualize the opinions based on a compatible explainable recommendation model. Experiments on datasets of several product categories showcase the efficacies of our method as compared to baselines based on templates, review summarization, selection, and text generation.", "sections": [{"heading": "Introduction", "text": "Explainable recommendations are motivated by the need for not only personalized recommendations, but also the accompanying explanations. Many recommender systems are based on matrix factorization, and the learnt latent factors often lack scrutability (users may not comprehend reasons behind certain recommendations). To induce greater interpretability from the latent factors, the crux of many models (e.g., [Zhang et al., 2014;Wang et al., 2018a]) is to contextualize the latent factors along known aspects and to align a recommendation, and correspondingly its explanation, to aspect sentiments.\nProblem. An explanation is typically generated post hoc to the recommendation model. Our scope is singularly the provision of explanations, presuming that item recommendation is addressed by a separate model. While there could be various forms of explanation (content-based collaborative filtering, rules , topics [Tan et al., 2016], or social , etc.), we focus on natural language explanations, i.e., a collection of sentences highlighting product aspects of interest to the target user. Hence, we assume an aspect demand is specified as input, listing the number of sentences required for each aspect. Therefore, the goal is to meet this aspect demand with sentences representative of product quality and user preferences.\nExisting works in explainable recommendations rely on templated explanation, i.e., substituting words within a prespecified sentence. For instance, EFM [Zhang et al., 2014] has standardized templates for positive and negative opinions, each time substituting only the [aspect], e.g.,:\nYou might be interested in [battery life], on which this product performs well.\nYou might be interested in [lens], on which this product performs poorly.\nTo increase variation beyond \"well\", \"poorly\", MTER [Wang et al., 2018a] further specifies an <opinion phrase>, e.g.,:\nIts [battery life] is <long>.\nAs exemplified above, templated explanations could be repetitive, robotic, and limited in their expressiveness. They tend to read less naturally than a human-created sentence.\nA product review contains sentences that recount a user's experience with the product, which often go some way towards explaining her choices post-adoption. Leveraging this explanatory quality, but intending to explain a predicted recommendation pre-adoption, we propose to \"synthesize\" an explanation by taking snippets from various reviews and putting them together in a coherent manner. Fitted to the recommendation, this synthesis benefits from the expressiveness of human-created review sentences, and yet is still flexible enough to produce varied explanations given the wide array of combinatorial selections from rich review corpora. Moreover, since a candidate sentence may bear in-built sentiment potentially incompatible to a user's own, we expand candidate selection to all aspect-relevant sentences by incorporating opinion contextualization for sentiment compatibility. Contributions. We make several contributions in this work. As our first contribution, we propose a framework called Synthesizing Explanation for Explainable Recommendation or SEER. Section 3 describes this framework, expressing the objective and constraints in terms of integer linear programming. As the problem proves NP-hard, our second contribution is to further describe a heuristic approximation. Section 4 expands on how the synthesized explanation could be contextualized with compatible opinions. As a third contribution, in Section 6 we conduct experiments on four product categories to evaluate the efficacy of our synthesis approach, as opposed to comparative baseline approaches based U , P, A, O, T set of all users, items, aspects, opinions, and reviews T j \u2208 T set of observed text reviews on product p j S j \u2286 T j set of all sentences on product p j t ij \u2208 T j a review of user u i on product p j M explainable recommendation model Z aspect-level sentiments z ijk \u2208 Z sentiment of user u i on item p j about aspect a k D aspect demand \u03c4 solution set of selected sentences \u0393 ss variable indicates whether sentence s representing s \u03b3 s variable indicates whether sentence s is selected \u03b6 i variable indicates whether a review t i j is part of \u03c4 \u03c3 si observed indicator of whether sentence s is in t i j \u03c0 sk observed indicator of whether sentence s expresses a k s(w) sentence s after substituting opinion phrase w ", "publication_ref": ["b17", "b15", "b15", "b17", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Problem Formulation", "text": "Table 1 lists the notations used in this paper. U and P are the universal sets of m users and n products respectively. User u i \u2208 U may assign to a product p j \u2208 P a rating r ij \u2208 R + and a text review t ij . Let R be the observed user-item rating matrix, and T be the set of observed text reviews. Let A and O be the universal sets of aspects and opinion phrases. We assume the occurrence of aspect a \u2208 A and opinion phrase o \u2208 O can be detected from a review sentence as described in [Zhang et al., 2014].\nCompatible Recommendation Models. Our objective is to synthesize an explanation based on the outputs of compatible explainable recommendation models (see Section 5 for examples). An explainable recommendation model M produces both personalized recommendations and aspect-level sentiments Z \u2208 R m\u00d7n\u00d7v + to facilitate their explanations. z ijk \u2208 Z indicates user u i 's sentiment for aspect a k of p j .\nProblem Statement. Given aspect-level sentiments Z, and a product p j recommended to user u i by a model M, we output an explanation in the form of a collection of sentences \u03c4 based on the aspect demand D. Let aspect demand D \u2208 N v be a vector, where each element D k is a non-negative integer indicating the number of sentences demanded for aspect a k \u2208 A, and v = |A|. It follows that the sentences should reflect the aspect-level sentiments of the user specified in Z.\nEvaluation. A question arises on how to evaluate a recommendation explanation, aside from the goal of meeting the aspect demand. In the literature, recommendation accuracy is measured in terms of how well the prediction approaches the ground truth (held-out rating). An analogous approach would then be to compare an explanation against a ground truth. Intuitively, the review that a user writes for a product a posteriori would have been a \"perfect\" explanation if we were recommending the same product a priori. Thus, in the experiments we will compare synthesized explanations in terms of similarity to held-out reviews. ", "publication_ref": ["b17"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "SEER Framework", "text": "As seen in Figure 1, our framework is to synthesize an explanation by selecting snippets (sentences) from a product's existing reviews. Here we discuss the objective of the selection, and offer optimal as well as approximate formulations.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Optimization Objective", "text": "When recommending product p j to user u i , we construct an explanation from T j (reviews of product p j ). The solution \u03c4 ideally consists of D k sentences for each a k \u2208 A, selected from review sentences S j (the union of sentences from T j ). Representativeness. To explain the aspect a k of p j well, we aim for the most representative among sentences in S j pertinent to a k . Suppose that how well a sentence s could \"represent\" another sentence s is reflected by a cost \u03b4 ss \u2208 R + (lower is better). This may encode application-specific semantic notion of similarity, and for generality we consider these as a given. In Section 6, we experiment with several definitions, including unsupervised (e.g., cosine similarity between tfidf vectors), as well as supervised notions (e.g., paraphrase identification, textual entailment [Lan and Xu, 2018]).\nOur task is to select D k most representative ones to place into the solution set \u03c4 . To encode this selection, let \u0393 ss be a binary variable (the outcome to be determined) indicating whether a selected sentence s \u2208 \u03c4 (i.e., \u03b3 s = 1) represents another sentence s \u2208 S j . We thus want to minimize the representation cost below, where we prefer a solution \u03c4 with sentences similar to many of the same aspect.\nr cost(\u03c4 ) = s\u2208\u03c4 s \u2208Sj \u03b4 ss \u2022 \u0393 ss (1)\nCoherence. In addition to capturing the aspects well, the explanation should be compact and coherent. Intuitively, a document by fewer authors would be more coherent than by many. Hence, we attach a cost \u03b8 i (given) to using a review t i j \u2208 T j , rather than to individual sentences. This way, the selection favors selecting sentences that may have come from the same review, presumably enhancing coherence. We define the coherence cost below, where \u03b6 i is a binary variable of whether a review t i j \u2208 T j (i.e., \u03b6 i = 1) is part of the solution set \u03c4 (i.e., one or more of its sentences are selected).\nc cost(\u03c4 ) =\nt i j \u2208Tj \u03b8 i \u2022 \u03b6 i (2)\nThe given cost \u03b8 i also serves to contextualize the explanation to a specific user, as defined shortly in Section 4.\nOverall Cost. The overall cost is thus:\ncost(\u03c4 ) = c cost(\u03c4 ) + r cost(\u03c4 )(3)\nThe two components have an inherent trade off. Adding a sentence may lower r cost if the new sentence is more similar to other sentences, but that risks increasing the c cost if the new sentence comes from a review not currently in the solution. On the other hand, fewer reviews may constrain the selection of representative sentences. Hence, we need an effective algorithm to find the optimal aggregate of the two.", "publication_ref": ["b11"], "figure_ref": [], "table_ref": []}, {"heading": "Optimal Formulation via ILP", "text": "To find an optimal solution \u03c4 , we express the problem as Integer Linear Programming (ILP). ( 4a) is the objective (Eq. 3). \u03b3 s is a binary indicator whether the sentence s \u2208 S j is a part of \u03c4 . Constraints ( 4b) and ( 4c) ensure that sentence s \u2208 S j must be represented by one of the sentences s in the solution set (\u03b3 s = 1). (4d) means a review must be selected when we select any of its sentences. \u03c3 si is an observed binary indicator of whether s is in the review t i j . (4e) ensures a sentence is represented by another of the same aspect. Binary \u03c0 sk indicates whether s is of aspect a k . (4f) satisfies aspect demand. min:\nt i j \u2208Tj \u03b8 i \u2022 \u03b6 i + s,s \u2208Sj \u03b4 ss \u2022 \u0393 ss (4a) s.t: s\u2208Sj \u0393 ss = 1, \u2200s \u2208 S j (4b) \u0393 ss \u2264 \u03b3 s , \u2200s, s \u2208 S j (4c) \u03b3 s \u2022 \u03c3 si \u2264 \u03b6 i , \u2200t i j \u2208 T j , s \u2208 S j (4d) \u0393 ss \u2264 a k \u2208A \u03c0 sk \u2022 \u03c0 s k , \u2200s, s \u2208 S j (4e) s\u2208Sj \u03b3 s \u2022 \u03c0 sk = D k , \u2200a k \u2208 A (4f) \u03b6 i , \u03b3 s , \u0393 ss \u2208 {0, 1}, \u2200t i j \u2208 T j ; \u2200s, s \u2208 S j (4g)\nNP-hardness. Though SEER-ILP is theoretically optimal, it may be intractable for large problem sizes.\nProof. The proof sketch is based on a reduction from the Uncapacitated Facility Location Problem (UFLP) [Cornu\u00e9jols et al., 1983] involving a set of facilities and a set of customers.\nThere is a cost to open each facility (favoring fewer facilities) and a cost to serve a customer from an open facility (favoring facility closer to customer). We reduce UFLP to our problem where there is only a single aspect. Each customer is now a sentence s to be represented. Each facility is a review with opening cost \u03b8 i , associated with one representing sentence s.\nThe service cost is thus \u03b4 ss . Our problem specifies the number of sentences to be selected for that aspect. If we solve for all demands from 1 to m, where m is the total number of facilities, we arrive at a solution for UFLP with the lowest total cost at any number of facilities. Since UFLP is known to be NP-hard, our more general formulation is NP-hard.\nAlgorithm 1 SEER-Greedy 1: Initialize \u03c4 = \u2205; S = S j ; T = T j ; D = D; 2: while S = \u2205 do 3:\nfor t i j \u2208 T do 4:\nFind \u03c4 i \u2286 t i j that represent the most number of unmet aspects in D, which minimize the average covering cost of sentences:\n\u03b8 i + s\u2208\u03c4 i s \u2208S \u03b4 ss \u2022\u0393 ss s\u2208\u03c4 i s \u2208S \u0393 ss", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}, {"heading": "5:", "text": "\u03c4 := \u03c4 \u222a \u03c4 i ; T := T \\t i j 6:\nS := S\\S , where S are \u03c4 i covering sentences 7:\nD := D\\{a}, where {a} are \u03c4 i representing aspects 8: return \u03c4", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Approximation via Greedy Algorithm", "text": "We therefore seek an approximation to cater to large problems. Non-metric UFLP has a greedy solution [Hochbaum, 1982] with an approximation ratio of 1 + log(n) based on a mapping to Minimum Weight Set Cover (MWSC). Our problem is different from UFLP in several respects, chiefly the aspect demands, precluding direct reuse of that particular greedy solution. Even when confined to one aspect, there is no existing solution with provable guarantee for MWSC with constraint on the number of sets [Golab et al., 2015].\nOur proposed greedy solution is Algorithm 1. Sentences in S j are the coverable elements. A covering set is a review t i j with its selected sentences \u03c4 i to cover a subset of S; its weight is\n\u03b8 i + s\u2208\u03c4 i s \u2208S \u03b4 ss \u2022 \u0393 ss s\u2208\u03c4 i s \u2208S \u0393 ss\nEnumerating all subsets is exponential. In practice, it is sufficient to sort s \u2208 S in terms of \u03b4 ss and investigate the first k sentences for various k [Hochbaum, 1982]. We greedily pick the lowest-weight set until all the sentences are covered. Unique to our scenario is the selection of \u03c4 i from the sentences in t i j , by maximizing the representation of aspects, which always lowers the cost of representation. If there are multiple sentences that can represent an aspect, we seek the permutation with the lowest cost. To ensure coverage, the last sentence should cover all remaining sentences of the aspect. \nO(|S j | log |S j |). Since t avg \u2022 |T j | is equivalent to |S j |, the overall complexity of SEER-Greedy is O(|S j | 3 + |T j ||S j | 2 log |S j |).", "publication_ref": ["b9", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Opinion Contextualization", "text": "The goal is an explanation with compatible opinions to the ones the target user would have (as encoded in the Z). Contextualizing the sentences to fit the target user's aspect sentiments is done via two complementary mechanisms. Sentence Selection. One means is to employ \u03b8 i that favors more compatible reviewers in Equation 2. \u03b8 i is defined as a function of the similarity between z ij: (a vector of aspectlevel sentiments by target user u i on p j ) and z i j: , e.g., \u03b8 i = 1 \u2212 cos(z ij: , z i j: ) 2 Alternatively, our framework could admit other definitions for \u03b8 i as well. Opinion Substitution. To \"extend\" beyond the original pool of review sentences, we contextualize candidate sentences by allowing substitution of the original opinion phrase with another more attuned to the target user's sentiments. After removing the opinion to be substituted, this turns into a sentence completion task, which is an NLP problem in its own right. For concreteness, we allude to a specific solution, but a fuller consideration is beyond the scope of this work. Context2Vec [Melamud et al., 2016] pays attention to the entire sentential context, with two LSTMs for sentence-level representation: one reads from the left (lLS) and the other from the right (rLS). Their concatenation passes through a 2layer perceptron with ReLU activation to get its context representation. L 1 , L 2 are fully connected linear operations.\nw l = L 2 (ReLU (L 1 (lLS(w 1:l\u22121 ) \u2295 rLS(w |s|:l+1 )))\nAs Context2Vec only considers the surrounding words, the sentence completion is irrespective of the user's aspect-level sentiment. To \"personalize\" the explanation, we use our modification, called Aspect-Sentiment Context2Vec or ASC2V, for predicting opinionated word based on sentence context, and z ijk , i.e., u i 's sentiment for aspect a k of p j . To infuse this information explicitly, we construct an aspect-sentiment vector as of dimensionality |A|. If the sentence is of aspect a k , we set the k th dimension to the value of z ijk , and 0 otherwise. We use 1-layer perceptron with tanh activation to project aspect sentiment information into the same space as context word embedding. L 3 is fully connected linear operation.\nw = tanh(L 3 ( as))\nThis w is the starting token for both lLS and rLS (see Figure 2). We rank candidate opinions based on cosine similarity if current cost < min r cost then 7:\nw best := w; min r cost := current cost 8:\n\u03c4 := (\u03c4 \\{s}) \u222a {s(w best )} 9: return \u03c4 of their embeddings with the context vector. For the example \"This is a camera\", if z ijk expresses positive sentiment, \"great\" should be ranked highly. If negative, a different opinion may apply.\nASC2V contextualizes sentences within the synthesized explanation to further improve the objective in Equation 3. Let O ijs be top-k predicted opinions for sentence s based on ASC2V (for experiments, we use k = 10). As shown in Algorithm 2, we substitute each opinion phrase w \u2208 O ijs (line 4) into s by s(w) and keep the one minimizing r cost (line 7). c cost is not affected as only the opinion, but not the sentence, changes. This computation is efficient at O(|\u03c4 | \u2022 k), as the solution size |\u03c4 | and number of opinions k are usually relatively small.", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Compatible Recommendation Models", "text": "The class of compatible models are broadly defined. [Zhang et al., 2014;Bauman et al., 2017] are based on matrix factorization, while [Chen et al., 2016;Wang et al., 2018a] are based on tensor factorization. Others combine matrix factorization with topic modeling [Wu and Ester, 2015]. Several works enhance their explainable models by using graphs [He et al., 2015] or trees [Gao et al., 2019]. As concrete examples, in Section 6, we experiment with two models, EFM and MTER, which were established methods for templated explanations.\nExplicit Factor Model or EFM [Zhang et al., 2014] reconstructs the observed rating matrix R, user attention matrix X, and product quality matrix Y . Each x ik \u2208 X indicates the importance of aspect a k to user u i , while each y jk \u2208 Y is the summative quality of product p j on aspect a k . EFM decomposes the observations X, Y, and R into latent factors, minimizing the function  as a tensor G, where g ijk \u2208 G reflects the aggregate sentiment scores across all mentions by user u i of aspect a k in product p j 's reviews. The rating r ij is appended as an additional aspect to the tensor G, i.e., g ijv = r ij . G is decomposed using Tucker decomposition [Kolda and Bader, 2009]. Let\u011c be its reconstruction after minimizing the function\n||P Q T \u2212 R|| 2 F + \u03bb x ||\u03b7 1 \u03c8 T \u2212 X|| 2 F + \u03bb y ||\u03b7 2 \u03c8 T \u2212 Y || 2 F where P = [\u03b7 1 \u03c6 1 ] and Q = [\u03b7 2 \u03c6 2 ]\n||\u011c \u2212 G|| F \u2212 \u03bb ui\u2208U (ui,pj ,p j ) ln \u03c3(\u011d ijv \u2212\u011d ij v )\nwhere (u i , p j , p j ) is a pairwise ranking observation where u i prefers p j to p j . We synthesize an explanation based on the non-rating aspects of\u011c, i.e., z ij(0:v\u22121) =\u011d ij(0:v\u22121) .", "publication_ref": ["b17", "b1", "b2", "b15", "b17", "b8", "b7", "b17", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Comparable Methods", "text": "Our baselines comprise methods that could still serve the purpose of recommendation explanation despite not having been designed specifically for that. For one, we could select whole review(s) as explanations. [Tsaparas et al., 2011] selects a set of reviews that maximize the coverage of a specified list of aspects. [Lappas et al., 2012] finds a characteristic set of reviews that best mirror the global distribution of sentiments in the corpus. We could employ extractive summarization [Barrios et al., 2016] that combines sentences from reviews based on representativeness objective. However, our problem is distinct in incorporating a target user's \"would-be\"' aspect sentiments in arriving at an explanation with compatible sentiments.\nFor text generation, recent works utilize LSTM with attention.  takes into account the user, item, and given rating. [Ni and McAuley, 2018] incorporates the user and item, as well as starter phrases. [Ni et al., 2019] uses history reviews and keywords as attributes. Our synthesis approach selects human-created sentences, rather than generate sentences from abstract representations.\nOther methods address different problems and are not comparable.  conditions review generation on latent factors, [Lu et al., 2018] extends on review textual features, while [Truong and Lauw, 2019] extends on images, whereas [Chen et al., 2019] conditions on aspects. [Wang et al., 2018b] applies reinforcement approach for selecting sentences that agree with predicting ratings. Synthetic reviews were considered for unrelated applications, such as simulating spam [Sun et al., 2013].", "publication_ref": ["b12", "b0", "b14", "b14", "b14", "b4", "b16", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "Comparisons are tested with one-tailed paired-sample Student's t-test at 0.05 level. Experiments were run on machine with Intel Xeon E5-2650v4 2.20 GHz CPU and 256GB RAM.    [Wang et al., 2018a]. For each category, we retrieve the most common aspects covering 90% of opinion phrases and filter out users and items with fewer than five reviews. The remaining are split into training, validation, and test at a ratio of 0.6 : 0.2 : 0.2 for every user chronologically. Sentences in validation and test with opinions or aspects that had not appeared in training were excluded. Table 2 shows some basic statistics of the datasets.\nBase Models. SEER uses aspect-level sentiments Z from two compatible explainable recommendation model (see Section 5). For EFM 2 , as in the original work, the latent factor and explicit factor dimensions are 60 and 40. For MTER, we adopt the default setting of the author's implementation 3 . It is not our intention to compare these two, as our model works with any compatible base recommendation method. Evaluation Metrics. We use ROUGE [Lin and Hovy, 2003], a well-known metric for text matching and text summarization, to assess how well the synthesized explanations approach the ground-truth reviews. To cater to words as well as phrases, we report ROUGE-1 (1-gram) as well as ROUGE-L (longest common subsequence) summatively in terms of the F-Measure.", "publication_ref": ["b15", "b14"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Explanation Synthesis", "text": "Optimal vs. Approximation. For the optimal SEER-ILP, within 100 seconds, the CPLEX 4 solver can solve optimally for \u2265 95% of problem instances. Running on the same instances, SEER-Greedy achieves identical coverage of aspects (100%) at an overall cost that is just 1% higher than optimal, yet consumes merely 4% (i.e., a couple of seconds) of the   3). Subsequently, we run both variants on 100% of the problem instances. For ILP, the result would reflect either the optimal or the best solution up to that point. Representativeness Cost. For the representativeness cost \u03b4 ss in Equation 1, we explore several options. One is based on the cosine similarity of sentences s and s . Each sentence is represented by tfidf vectors based on the vocabulary of product's sentences. For \u03b4 ss , we take\n\u03b4 ss = 1 \u2212 cos(s, s ) 2\nWe also try two other models: SSE [Nie and Bansal, 2017] for paraphrase identification and ESIM [Chen et al., 2017] for textual entailment. Table 4 shows tfidf to perform the best in terms of ROUGE-L. We will use it subsequently. One reason is the corpus SSE and ESIM trained on was not optimized for review sentences. In any case, we consider \u03b4 ss as given.", "publication_ref": ["b3"], "figure_ref": [], "table_ref": ["tab_4", "tab_5"]}, {"heading": "Opinion Contextualization", "text": "We hide the ground-truth opinion from the held-out test review and evaluate the ranking of candidates in O using IR metrics: MRR (the reciprocal rank of the true opinion, averaged across held-out reviews) and Recall@10 or R@10 (fraction of held-out reviews with the true opinion in the top-10).\nWe compare our ASC2V with two baselines. Context2Vec or C2V [Melamud et al., 2016] with only on the sentence (no aspect sentiment). RC2V uses random aspect sentiment. For ASC2V, we train with similar setting as C2V, using RMSprop for optimization. Table 5 shows both variants of ASC2V significantly outperform C2V. RC2V, which adds no meaningful information, fluctuates around C2V. Indeed aspect-level sentiments are useful for opinion contextualization.", "publication_ref": ["b14"], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Comparison to Baselines", "text": "We compare the explanations generated by SEER to several categories of baselines. For parity, we control for the explanation length. The first category is template explanation, comprising the original explanations by EFM [Zhang et al., 2014] and MTER [Wang et al., 2018a]. Next is review summarization represented by TEXT RANK [Barrios et al., 2016] and review selection with four methods: REPRESENTATIVE selects the review with lowest representative cost (see Equation 1); COMPREHENSIVE selects the review of highest aspect coverage [Tsaparas et al., 2011]; CHARACTERISTIC selects the review whose aspect sentiment distribution most resembles a product's reviews [Lappas et al., 2012]; CHARACTERISTIC+ that also takes into account the aspect demand by considering distributions of demanded aspects only. The last category is review generation with ATT2SEQ     18.68 11.15 19.29 11.37 19.04 11.97 19.16 11.25 REPRESENTATIVE 18.22 11.11 19.24 11.27 19.72 12.60 19.45 11.80 COMPREHENSIVE 21.90 13.44 22.16 13.16 23.41 15.12 22.33 13.73 CHARACTERISTIC 13.18 7.65     and ROUGE-L (R-L) 5 . The template-based approaches perform poorly because a standard template cannot reflect varied reviews. Benefitting from paying attention to the aspect demand, CHARACTERISTIC+ performs better than CHARAC-TERISTIC. However, both still perform worse than COMPRE-HENSIVE that maximizes coverage of aspect demand. REP-RESENTATIVE outperforms CHARACTERISTIC since it optimizes for representativeness yet is still lower than COMPRE-HENSIVE. TEXT RANK underperforms COMPREHENSIVE, because of redundant sentences that repeat aspects while the latter considers a whole review covering various aspects. The review generation approaches tend to produce short and repetitive sentences. They do not reflect aspect-level senti-ments fully: ATT2SEQ uses ratings but no aspects, whereas EXPANSION NET and AP-REF2SEQ use aspects but may not reflect sentiments well.", "publication_ref": ["b17", "b15", "b0", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Qualitative Study", "text": "Case Study. As an illustration, Table 8 shows the explanations for a Computer instance. The ground truth review reveals aspect demand involving mouse and size. EFM describes the product having good performance on the two aspects. MTER opinions are difficult to understand. ATT2SEQ does not cover the aspect demand. EXPANSION NET generates short sentences repetitively. TEXT RANK tends to select popular repetitive aspects. Our SEER-ILP produces readable explanations that reflect not only the aspects, but also the user opinions. When used with EFM or MTER, it generates slightly different phrases, e.g., \"perfect\" vs. \"good\" size. User Study. To test the efficacy of the explanations from human perspective, we randomly select 5 user-product pairs from each category to get 20 examples in total and design a survey involving five participants who are not the authors. There are two questions. The first looks into the language quality, e.g., readable and easy to understand. The second, which also appeared in [Wang et al., 2018a], looks into appropriateness of recommendation. Q1: Are the explanatory sentences well-formed and understandable? Q2: Does the explanation help you understand why the given product is being recommended to the given user? Each question is applied to a given explanation. Each participant chooses from five-point Likert scale, from 1 (strongly disagree) to 5 (strongly agree). To compare to the proposed SEER-ILP, we choose MTER and AP-REF2SEQ as representative baselines, as these two were designed specifically for recommendation explanation and achieve high performance in terms of ROUGE-L. As reported in Table 9, SEER-ILP outperforms the two baselines significantly. For Q1, MTER with simple template is difficult to understand, while AP-REF2SEQ achieves better results (\u2265 3) comparing to MTER which shows its ability to generate readable text. However, AP-REF2SEQ-generated text is short and too general which make their explanations less informative than those of SEER-ILP.", "publication_ref": ["b15"], "figure_ref": [], "table_ref": ["tab_12", "tab_13"]}, {"heading": "Conclusion", "text": "We propose an innovative post hoc strategy for providing natural language explanations for personalized recommendations. Our approach synthesizes an explanation by selecting representative sentences from a product's reviews, contextualizing the opinions based on aspect-level sentiments from a class of compatible explainable recommendation models. SEER performs well against competitive baselines including templates, review summarization, selection, and generation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "This research is supported by the National Research Foundation, Prime Minister's Office, Singapore under its NRF Fellowship Programme (Award No. NRF-NRFF2016-07).\nProceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence  ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Variations of the similarity function of textrank for automated summarization", "journal": "", "year": "2016", "authors": " Barrios"}, {"ref_id": "b1", "title": "Aspect based recommendations: Recommending items with the most valuable aspects based on user reviews", "journal": "ACM", "year": "2017", "authors": "[ Bauman"}, {"ref_id": "b2", "title": "Learning to rank features for recommendation over multiple categories", "journal": "ACM", "year": "2016", "authors": "[ Chen"}, {"ref_id": "b3", "title": "Enhanced LSTM for natural language inference", "journal": "ACL", "year": "2017", "authors": "[ Chen"}, {"ref_id": "b4", "title": "Co-attentive multi-task learning for explainable recommendation", "journal": "AAAI Press", "year": "2019", "authors": "[ Chen"}, {"ref_id": "b5", "title": "The uncapacitated facility location problem", "journal": "", "year": "1983", "authors": " Cornu\u00e9jols"}, {"ref_id": "b6", "title": "Learning to generate product reviews from attributes", "journal": "", "year": "2017", "authors": " Dong"}, {"ref_id": "b7", "title": "Explainable recommendation through attentive multiview learning", "journal": "IEEE", "year": "2015", "authors": "[ Gao"}, {"ref_id": "b8", "title": "Trirank: Review-aware explainable recommendation by modeling aspects", "journal": "ACM", "year": "2015", "authors": "[ He"}, {"ref_id": "b9", "title": "Heuristics for the fixed cost median problem", "journal": "Math. Program", "year": "1982", "authors": "; Hochbaum;  Dorit;  Hochbaum"}, {"ref_id": "b10", "title": "Tensor decompositions and applications", "journal": "SIAM review", "year": "2009", "authors": "G Tamara;  Kolda; W Brett;  Bader"}, {"ref_id": "b11", "title": "Neural network models for paraphrase identification, semantic textual similarity, natural language inference, and question answering", "journal": "", "year": "2018", "authors": "; Wuwei Xu; Wei Lan;  Xu"}, {"ref_id": "b12", "title": "Theodoros Lappas, Mark Crovella, and Evimaria Terzi. Selecting a characteristic set of reviews", "journal": "ACM", "year": "2012", "authors": " Lappas"}, {"ref_id": "b13", "title": "Neural rating regression with abstractive tips generation for recommendation", "journal": "Association for Computing Machinery", "year": "2017", "authors": "[ Li"}, {"ref_id": "b14", "title": "Personalized review generation by expanding phrases and attending on aspect-aware representations", "journal": "Association for Computational Linguistics", "year": "2003", "authors": "Hovy ; Lin; Eduard Lin; ; Hovy;  Lu"}, {"ref_id": "b15", "title": "Rating-boosted latent topics: Understanding users and items with ratings and reviews", "journal": "ACM", "year": "2011", "authors": ""}, {"ref_id": "b16", "title": "A reinforcement learning framework for explainable recommendation", "journal": "IEEE", "year": "2018", "authors": ""}, {"ref_id": "b17", "title": "Flame: A probabilistic model combining aspect based opinion mining and collaborative filtering", "journal": "ACM", "year": "2014", "authors": "Yao Ester; Martin Wu; ; Ester;  Zhang"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Architecture of proposed framework SEER", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Complexity Analysis. In Algorithm 1, the two outer loops (lines 2-3) may require O(|S j | \u2022 |T j |). The inner cost is dominated by line 4. Computing the cost is O(t avg \u2022 |S j |), where t avg is the average length of reviews. Sorting the covered sentences is", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2: ASC2V Architecture", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Algorithm 22Opinion Substitution 1: Initialize min r cost := r cost(\u03c4 ) 2: for s \u2208 \u03c4 do 3: \u03c4 := \u03c4 \\{s}; w best := get opinion(s) 4: for w \u2208 O ijs do 5: current cost := r cost(\u03c4 \u222a {s(w)}) 6:", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "", "figure_data": ": Main Notationson templates, review summarization and selection, as well astext generation."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "are users' and products' latent factors respectively. Each is the concatenation of aspect-based factors (\u03b7 1 , \u03b7 2 ) influenced by X, Y and hidden factors (\u03c6 1 , \u03c6 2 ) influenced by ratings. \u03c8 are the latent factors of aspects. Coefficients \u03bb x and \u03bb y weigh the relative impor-", "figure_data": "Dataset #User #Product #Aspect #Opinion #Review #SentenceComputer 19,8188,6065,3544,243 163,894 512,703Camera 4,7702,6802,3212,367 37,856 151,382Toy2,6721,9848181,225 26,59857,260Cellphone 2,3401,3908821,256 19,10951,469"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Data statistics    ", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Performance ratios of SEER-Greedy to SEER-ILP (%)", "figure_data": "ComputerCameraToyCellphoneEFMSEERtfidf SEERSSE SEERESIM15.14  \u00a7 14.48 13.8014.74  \u00a7 14.01 13.5116.36  \u00a7 15.39 14.8114.96  \u00a7 14.40 14.10MTERSEERtfidf SEERSSE SEERESIM15.15  \u00a7 14.49 13.7914.71  \u00a7 14.03 13.5216.28  \u00a7 15.37 14.8415.03  \u00a7 14.42 14.10\u00a7 denotes statistically significant improvements. Highest values are in bold"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Comparison of representative costs: ROUGE-L Datasets. Experiments use four public datasets of Amazon reviews 1 [McAuley et al., 2015] of varying categories: Computer and Accessories (Computer), Camera and Photo (Camera), Toys and Games (Toy), Cell Phones and Accessories (Cellphone). Preprocessing follows", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": ".706 0.409 0.643 0.514 0.707 0.366 0.624 ASC2VEFM 0.475 \u00a7 0.713 \u00a7 0.416 \u00a7 0.652 \u00a7 0.526 \u00a7 0.726 \u00a7 0.384 \u00a7 0.649 \u00a7 ASC2VMTER 0.473 \u00a7 0.709 \u00a7 0.418 \u00a7 0.653 \u00a7 0.528 \u00a7 0.724 \u00a7 0.388 \u00a7 0.651 \u00a7 \u00a7 denotes statistically significant improvements by ASC2V Highest values (among ASC2V, RC2V, and C2V) are in bold", "figure_data": "ModelComputer MRR R@10 MRR R@10 MRR R@10 MRR R@10 Camera Toy CellphoneC2V0.460 0.695 0.411 0.645 0.515 0.705 0.365 0.621RC2V0.462 0"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Opinion Contextualization time taken by SEER-ILP on average (see Table", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Comparison to Baselines: Coverage", "figure_data": "ComputerCameraToyCellphoneModelR-1 R-L R-1 R-L R-1 R-L R-1 R-LATT2SEQ16.69 10.35 15.90 9.13 16.51 10.41 16.42 9.76EXPANSION NET11.68 1.25 19.23 5.19 24.41 4.68 14.13 3.11AP-REF2SEQ16.94 12.29 17.04 12.94 21.72 14.50 19.15 12.99TEXT RANK"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "\u00a7 15.05 \u00a7 25.11 \u00a7 14.72 \u00a7 \u00a7 25.33 \u00a7 16.30 \u00a7 24.66 \u00a7 14.87 \u00a7 SEER-ILP 25.12 \u00a7 15.14 \u00a7 25.23 \u00a7 14.74 \u00a7 25.43 \u00a7 16.36 \u00a7 24.76 \u00a7 14.96 \u00a7 MTER TEMPLATE 16.88 11.68 16.43 11.14 13.22 12.03 17.61 11.86 SEER-Greedy 24.90 \u00a7 15.08 \u00a7 25.01 \u00a7 14.65 \u00a7 25.25 \u00a7 16.24 \u00a7 24.74 \u00a7 14.94 \u00a7 SEER-ILP 25.12 \u00a7 15.15 \u00a7 25.22 \u00a7 14.71 \u00a7 25.33 \u00a7 16.28 \u00a7 24.85 \u00a7 15.03 \u00a7 \u00a7 denotes statistically significant improvements by our models Highest values in bold", "figure_data": "14.05 7.92 15.76 9.80 14.27 8.41CHARACTERISTIC+ 18.33 10.87 18.06 10.32 21.25 13.56 19.05 11.34EFMTEMPLATE SEER-Greedy14.17 8.41 14.43 8.39 13.37 8.06 14.22 8.41 24.89"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "The mouse has worked great for about 1-year. The mouse was great for a while. The size is perfect for my hand ATT2SEQ I really like the mouse. The mouse is very comfortable and the mouse is fine. I haven't had any problems with the wireless signalEXPANSION NETThe mouse is a plus. The size is great and the size is perfect", "figure_data": ": Comparison to Baselines: ROUGE-1 and ROUGE-Lthat generates text from user, product, and rating as attributes;EXPANSION NET [Ni and McAuley, 2018] that generates textfrom aspect words as starter phrases; and AP-REF2SEQ [Niet al., 2019] that generates text from user & item reviews andaspect words.Coverage. Table 6 shows the coverage, i.e., the proportionof the met aspect demand. Coverage is not necessarily 1 dueto the limited number of candidate sentences for selectionor aspects that have not appeared before. Both SEER vari-ants outperform baselines in coverage (MTER has identicalcoverage). The template methods respond to aspect demand.EFM produces duplicate sentences for an aspect, resulting inlower coverage than MTER that produces multiple sentencesby varying opinion phrases. Methods that do not benefit fromaspect demands (including ATT2SEQ, TEXT RANK, REPRE-SENTATIVE, and CHARACTERISTIC) underperform the othermethods that do. Review selection methods are limited towhat can be covered by a review. Among these, COMPRE-HENSIVE achieves the highest aspect coverage. As the re-view with the closest aspect sentiment distribution does notnecessarily have the highest aspect coverage, the coverage ofCHARACTERISTIC+ is lower than COMPREHENSIVE.Ground Truth Recovery. As Table 7 shows, SEER vari-ants (ILP and Greedy) significantly outperform all the base-lines, with the highest F-Measure for both ROUGE-1 (R-1)"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Example Explanations on a Computer instance \u00a7 4.45 \u00a7 3.80 \u00a7 3.75 \u00a7 4.45 \u00a7 3.50 \u00a7 \u00a7 denotes statistically significant improvements. Highest values are in bold", "figure_data": "Model12Annotator 345AverageMTER2.10 2.35 2.75 3.00 2.352.51Q1AP-REF2SEQ 3.15 3.00 3.60 3.75 3.50 SEER-ILP 3.95  \u00a7 4.25  \u00a7 4.00  \u00a7 3.85  \u00a7 4.10  \u00a73.40 3.75  \u00a7MTER1.75 1.50 2.40 2.80 2.002.09Q2AP-REF2SEQ 1.95 3.05 3.20 3.40 3.10 SEER-ILP 3.552.94"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Result analysis of user study", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "r cost(\u03c4 ) = s\u2208\u03c4 s \u2208Sj \u03b4 ss \u2022 \u0393 ss (1)", "formula_coordinates": [2.0, 372.18, 510.4, 185.82, 20.06]}, {"formula_id": "formula_1", "formula_text": "t i j \u2208Tj \u03b8 i \u2022 \u03b6 i (2)", "formula_coordinates": [2.0, 434.57, 657.15, 123.43, 21.51]}, {"formula_id": "formula_2", "formula_text": "cost(\u03c4 ) = c cost(\u03c4 ) + r cost(\u03c4 )(3)", "formula_coordinates": [3.0, 109.35, 77.61, 187.65, 8.96]}, {"formula_id": "formula_3", "formula_text": "t i j \u2208Tj \u03b8 i \u2022 \u03b6 i + s,s \u2208Sj \u03b4 ss \u2022 \u0393 ss (4a) s.t: s\u2208Sj \u0393 ss = 1, \u2200s \u2208 S j (4b) \u0393 ss \u2264 \u03b3 s , \u2200s, s \u2208 S j (4c) \u03b3 s \u2022 \u03c3 si \u2264 \u03b6 i , \u2200t i j \u2208 T j , s \u2208 S j (4d) \u0393 ss \u2264 a k \u2208A \u03c0 sk \u2022 \u03c0 s k , \u2200s, s \u2208 S j (4e) s\u2208Sj \u03b3 s \u2022 \u03c0 sk = D k , \u2200a k \u2208 A (4f) \u03b6 i , \u03b3 s , \u0393 ss \u2208 {0, 1}, \u2200t i j \u2208 T j ; \u2200s, s \u2208 S j (4g)", "formula_coordinates": [3.0, 75.96, 336.55, 221.04, 153.56]}, {"formula_id": "formula_4", "formula_text": "\u03b8 i + s\u2208\u03c4 i s \u2208S \u03b4 ss \u2022\u0393 ss s\u2208\u03c4 i s \u2208S \u0393 ss", "formula_coordinates": [3.0, 421.72, 127.11, 96.22, 21.77]}, {"formula_id": "formula_5", "formula_text": "\u03b8 i + s\u2208\u03c4 i s \u2208S \u03b4 ss \u2022 \u0393 ss s\u2208\u03c4 i s \u2208S \u0393 ss", "formula_coordinates": [3.0, 374.88, 391.24, 120.05, 28.13]}, {"formula_id": "formula_6", "formula_text": "O(|S j | log |S j |). Since t avg \u2022 |T j | is equivalent to |S j |, the overall complexity of SEER-Greedy is O(|S j | 3 + |T j ||S j | 2 log |S j |).", "formula_coordinates": [3.0, 315.0, 591.52, 243.01, 32.57]}, {"formula_id": "formula_7", "formula_text": "w l = L 2 (ReLU (L 1 (lLS(w 1:l\u22121 ) \u2295 rLS(w |s|:l+1 )))", "formula_coordinates": [4.0, 72.02, 515.38, 206.92, 9.96]}, {"formula_id": "formula_8", "formula_text": "w = tanh(L 3 ( as))", "formula_coordinates": [4.0, 136.52, 668.13, 77.94, 9.65]}, {"formula_id": "formula_9", "formula_text": "||P Q T \u2212 R|| 2 F + \u03bb x ||\u03b7 1 \u03c8 T \u2212 X|| 2 F + \u03bb y ||\u03b7 2 \u03c8 T \u2212 Y || 2 F where P = [\u03b7 1 \u03c6 1 ] and Q = [\u03b7 2 \u03c6 2 ]", "formula_coordinates": [4.0, 315.0, 585.52, 233.46, 29.9]}, {"formula_id": "formula_10", "formula_text": "||\u011c \u2212 G|| F \u2212 \u03bb ui\u2208U (ui,pj ,p j ) ln \u03c3(\u011d ijv \u2212\u011d ij v )", "formula_coordinates": [5.0, 79.21, 219.84, 192.56, 22.41]}, {"formula_id": "formula_11", "formula_text": "\u03b4 ss = 1 \u2212 cos(s, s ) 2", "formula_coordinates": [6.0, 132.45, 265.72, 84.89, 22.31]}], "doi": ""}