{"title": "OpenGAN: Open-Set Recognition via Open Data Generation", "authors": "Shu Kong; Deva Ramanan", "pub_date": "2021-10-13", "abstract": "D fake features K-way softmax closed vs. fake closed-set images G D G closed-set images outlier images D outlier images closed-set images D closed vs. open fake images outlier images closed-set images fake images closed vs. open G closed vs. open (a) GAN (b) Outlier Exposure (c) OpenGAN pix (d) OpenGAN f ea Figure 1: We explore open-set recognition, which requires the ability to discriminate open-set test examples outside K classes of interest. (a) Past work has suggested that GAN discriminators can serve as open-set likelihood functions, but this does not work well due to instable training of GANs [56, 53, 48, 66, 37]. (b) Outlier Exposure [31] exploits some outlier data to learn a binary discriminator D for openset discrimination. Because outliers observed during training will not exhaustively span the open-world, the discriminator D tends to generalize poorly to diverse open data [57]. (c) We introduce OpenGAN, which augments training outliers with fake open data synthesized by a generator G trained to fool the discriminator D. Importantly, we find that a small number of outliers stabilizes training by enabling effective model selection of the discriminator D. (d) Because we are concerned with accurate discrimination rather than realistic pixel generation, we find it more efficient to generate (and discriminate) features from the off-the-shelf K-way classification network. This allows OpenGAN to be implemented via a lightweight discriminator head built on top of an existing K-way network, enabling closedworld systems to be readily modified for open-set recognition.", "sections": [{"heading": "Introduction", "text": "Machine learning systems that operate in the real openworld invariably encounter test-time data that is unlike training examples, such as anomalies or rare objects that were insufficiently (or never) observed during training. Fig. 2 illustrates two cases in which a state-of-the-art semantic segmentation network misclassifies a \"stroller\"/ \"street-market\" -a rare occurrence in either training or testing -as a \"motorcycle\"/\"building\". This failure could be catastrophic for an autonomous vehicle.\nAddressing the open-world has been explored through anomaly detection [69,31] and out-of-distribution detection [36]. In K-way classification, this task can be crisply formulated as open-set recognition, which requires discriminating open-set data that belongs to a (K+1) th \"other\" . Contemporary benchmarks such as Cityscapes [14] focus on K classes of interest for evaluation, ignoring a sizeable set of \"other\" pixels that include vulnerable objects like wheelchairs and strollers (upper row). As a result, most state-of-the-art segmentators [61] also ignore these pixels during training, resulting in a stroller mislabeled as a \"motorcycle\" (top) and a street-market mislabeled as a \"building\". Such misclassifications may be critical for AVs because these objects may require different plans for obstacle avoidance (e.g., \"yield\" or \"slow-down\"). Fig. 4 shows our approach, which explicitly augments state-of-the-art segmentors with open-set reasoning.\nclass, outside the K closed-set classes [54]. Typically, open-set discrimination assumes no examples from the \"other\" class available during training [6,64,44]. In this setup, one elegant approach is to learn the closedset data distribution with a GAN, making use of the GAN-discriminator as the open-set likelihood function (Fig. 1a) [56,53,48,66,37]. However, this does not work well due to instable training of GANs. Recent work has shown that outlier exposure (Fig. 1b), or the ability to train on some outlier data as open-training examples, can work surprisingly well via the training of a simple open-vs-closed binary discriminator [18,31]. However, such discriminators fail to generalize to diverse open-set data [57] because they overfit to the available set of training outliers, which are often biased and fail to exhaustively span the openworld. Motivated by above, we introduce OpenGAN, a simple approach that dramatically improves open-set classification accuracy by incorporating several key insights. First, we show that using outlier data as a valset to select the \"right\" GAN-discriminator does achieve the state-of-the-art on open-set discrimination. Second, with outlier exposure, we augment the available set of open-training data by adversarially generating fake open examples that fool the binary discriminator (Fig. 1c). Third and most importantly, rather than defining discriminators on pixels, we define them on off-the-shelf (OTS) features computed by the closed-world K-way classification network (Fig. 1d). We find such discriminators generalize much better. Our formulation differs in three ways from other openset approaches that employ GANs. (1) Our goal is not to generate realistic pixel images, but rather to learn a robust open-vs-closed discriminator that naturally serves as an open-set likelihood function. Because of this, our ap-proach might be better characterized as a discriminative adversarial network! (2) We train the discriminator with both fake data (synthesized from the generator) and real open-training examples (cf. outlier exposure [31]). (3) We train GANs on OTS features rather than RGB pixels. We show that OpenGAN significantly outperforms prior work for open-set recognition across a variety of tasks including image classification and pixel segmentation. Moreover, we demonstrate that our technical insights improve the accuracy of other GAN-based open-set methods: training them on OTS features and selecting their discriminators via validation as the open-set likelihood function.", "publication_ref": ["b69", "b31", "b36", "b13", "b61", "b54", "b5", "b64", "b44", "b56", "b53", "b48", "b66", "b37", "b17", "b31", "b57", "b1", "b31", "b2"], "figure_ref": ["fig_0", "fig_2"], "table_ref": []}, {"heading": "Related Work", "text": "Open-Set Recognition. There are multiple lines of work addressing open-set discrimination, such as anomaly detection [12,36,69], outlier detection [53,48], and open-set recognition [54,22]. The typical setup for these problems assumes that one does not have access to training examples of open-set data. As a result, many approaches first train a closed-world K-way classification network on the closed-set [30,6] and then exploit the trained network for open-set discrimination [54,35,44]. Some others train \"ground-up\" models for both closed-world K-way classification and open-set discrimination by synthesizing fake open data during training, oftentimes sacrificing the classification accuracy on the closed-set [21,42,64,59]. To recognize open-set examples, they resort to post-hoc functions like density estimation [69,67], uncertainty modeling [20,36], and input image reconstruction [48,23,17,59].\nWe also explore open-set recognition through K-way classification networks, but we show OpenGAN, a simple and direct method of training an open-vs-closed classifier on adverserial data, performs significantly better than prior work.\nOpen-Set Recognition with GANs. As GANs can learn data distributions [25], conceptually, a GAN-discriminator trained on the closed-set naturally serves as an open-set likelihood function. However, this does not work well [56,53,48,66,37], presumably due to instable training of GANs. As a result, previous GAN-based methods focus on 1) generating fake open-set data to augment the training set, and 2) relying on the reconstruction error for open-set recognition [62,56,53,1,16]. With OpenGAN, we show that GAN-discriminator can achieve the state-of-the-art for open-set discrimination once we perform model selection on a valset of outlier examples. Therefore, unlike prior approaches, OpenGAN directly uses the discriminator as the open-set likelihood function. Moreover, our final version of OpenGAN generates features rather than pixel images.\nOpen-Set Recognition with Outlier Exposure. [18,31,52] reformulate the problem with the concept of \"outlier exposure\" which allows methods to access some  [42,44]. However, [18,31] demonstrate that outlier exposure, or the ability to train on some outlier examples, can greatly improve open-set discrimination via the training of a simple open-vs-closed binary classifier (Fig. 1b). Because it is challenging to construct a training set that exhaustively spans the open-world, such a classifier may overfit to the outlier data and not sufficiently generalize [57]. We demonstrate that OpenGAN alleviates this challenge by generating fake open-set training examples using a generator that is adversarially trained to fool the classifier. Importantly, with model selection on a valset, OpenGAN is also effective under the classic setup which assumes no availability of open-training data.", "publication_ref": ["b11", "b36", "b69", "b53", "b48", "b54", "b21", "b30", "b5", "b54", "b35", "b44", "b20", "b42", "b64", "b59", "b69", "b67", "b19", "b36", "b48", "b22", "b16", "b59", "b24", "b56", "b53", "b48", "b66", "b37", "b62", "b56", "b53", "b0", "b15", "b17", "b31", "b52", "b42", "b44", "b17", "b31", "b57"], "figure_ref": [], "table_ref": []}, {"heading": "Methodology", "text": "Let x be a data example, which can be an RGB image or its feature representation. We will show that using the latter performs better. Let D closed (x) be the closed-world distribution over x -that is, closed-set data from the K closed-set classes. Let D open (x) be the open-set data distribution of examples which do not belong to the closed-set.\nBinary Classifier. We train a binary classifier D from both closed-and open-set data:\nmax D E x\u223cD closed [ log D(x)]+\u03bb o \u2022E x\u223cDopen [ log (1\u2212D(x))]\nwhere D(x) = p(y=\"closed-set\"|x). Intuitively, we tune  [31], but underperforms when they fail to span the open-world [57].\nSynthetic Open Data. One solution to the above is to exploit synthetic training data, which might improve the generalizability of classifier D. Assume we have a generator network G(z) that produces synthetic images given (Gaussian normal) random noise inputs z \u223c N . We can naively add them to the pool of negative or open-set examples that D should not fire on. But these synthetic images might be too easy for D to categorize as open-set data [41,13]. A natural solution is to adversarially train the generator G to produce difficult examples that fool the classifier D using a GAN loss:\nmin G Ez\u223cN log (1 \u2212 D(G(z)))(1)\nBecause a perfectly trained generator G would generate realistic closed-set images, eventually making the discriminator D inapplicable for open-set discrimination. We find that the following two techniques easily resolve this issue.\nOpenGAN trains with both the real open&closed-set data and the fake open-data into a single (GAN-like) minimax optimization over D and G:\nmax D min G Ex\u223cD closed [ log D(x)] + \u03bbo \u2022 Ex\u223cD open [ log (1 \u2212 D(x))] + \u03bbG \u2022 Ez\u223cN [ log (1 \u2212 D(G(z)))](2)\nwhere \u03bb G controls the contribution of generated fake opendata by G. When there are no open training examples (i.e., \u03bb o =0), the above minimax optimization can still train a discriminator D for open-set classification. In this case, training an OpenGAN is equivalent to training a normal GAN and using its discriminator as the open-set likelihood function. While past work suggests that GAN-discriminators do not work well as open-set likelihood functions, we show they do achieve the state-of-the-art once selected with a valset (detailed below). To distinguish our contribution on the crucial step of model selection via validation, we call this method OpenGAN-0.\nOpen Validation. Model selection is challenging for GANs. Typically, one resorts to visual inspection of generated images from different model checkpoints to select the generator G [25]. In our case, we must carefully select the discriminator D. We experimented with many approaches such as using the last model checkpoint or selecting the one with minimum training error, but neither works, because adversarial training will eventually lead to a discriminator D that is incapable of discriminating closed-set data and fake open-set data generated by G (details in the supplemental). We find it crucial to use a validation set of real outlier data to select D, when D achieves the best open-vs-closed classification accuracy on the valset. We find the performance to be quite robust to the val-set of outlier examples, even when they are drawn from a different distribution from those encountered at test-time (Table 3 and 4).", "publication_ref": ["b31", "b57", "b41", "b12", "b24"], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Comparison to Prior GAN Methods", "text": "We compare OpenGAN to numerous prior art that use GANs for open-set discrimination.\nDiscriminator vs. Generator. GANs mostly aim at generating realistic images [2,9]. As a result, prior work in open-set recognition has focused on using GANs to generate realistic open-training images [21,34,42]. These additional images are used to augment the training set for learning an open-set model, which oftentimes is designed for both the closed-world K-way classification and open-set discrimination [21,34,42]. In our case, we do not learn a separate open-set model but directly use the already-trained discriminator as the open-set likelihood function.\nFeatures vs. Pixels. GANs are typically used to generate realistic pixel images. As a result, many GAN-based open-set methods focus on generating realistic images to augment the closed-set training data [48,66,31]. However, generating high-dimensional realistic images is challenging per se [2,9] and may not be necessary to open-set recognition [48]. As such, we build GANs over OTS feature embeddings learned by the closed-world K-way classification networks, e.g., over pre-logit features from the penultimate layer. This allows for exploiting an enormous amount of engineering effort \"for free\" (e.g., network design).\nClassification vs. Reconstruction. We note that most, if not all, GAN-based methods largely rely on the reconstruction error for open-set discrimination [56,53,48,66,44]. The underlying assumption is that closed-set data produces lower reconstruction error than the open-set. While this seems reasonable, it is challenging to reconstruct complex, high-resolution images [2,9], like the Cityscapes images shown in Fig. 2. On the contrary, we directly use the discriminator as the open-set likelihood function. Crucially, such a baseline has been shown to perform poorly in large body of prior work [56,53,48,66,37]. To the best of our knowledge, ours is the first result to demonstrate the strong performance of GAN-discriminators, thanks in large part to model selection via open validation (Section 3.1).", "publication_ref": ["b1", "b8", "b20", "b34", "b42", "b20", "b34", "b42", "b48", "b66", "b31", "b1", "b8", "b48", "b56", "b53", "b48", "b66", "b44", "b1", "b8", "b56", "b53", "b48", "b66", "b37"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Experiment", "text": "We We follow a \"less biased\" protocol [57] that constructs the open train&test-sets with cross-dataset images [60].\n\u2022 Setup-III examines the open-set discrimination at pixel level in semantic segmentation, which evaluates pixellevel open-vs-closed classification accuracy [7,29]. Implementation. We describe how to train the closedworld K-way classification networks which compute OTS features used for training OpenGAN f ea (Fig. 1d) and other methods (e.g., OpenMax [6] and C2AE [44]). For training K-way networks under Setup-I and II, we train a ResNet18 model [28] exclusively on the closed-train-set (with K-way cross-entropy loss). Under Setup-III, we use HRNet [61] as an OTS network, which is a top ranked model for semantic segmentation on Cityscapes [14]. We choose the penultimate/pre-logit layer of each K-way network to extract OTS features. Other layers also apply but we do not explore them in this work. Over the features, we train OpenGAN f ea discriminator (2MB), as well as the generator (2MB), with a multi-layer perceptron architecture. For comparison, we also train a ground-up OpenGAN pix over pixels with a CNN architecture (\u223c14MB) [68]. We train our OpenGAN models using GAN techniques [49]. Compared to the segmentation network HRNet (250MB), OpenGAN f ea is quite lightweight that induces minimal compute overhead. We conduct experiments with Py-Torch [45] on a single Titan X GPU. Code is available at https://github.com/aimerykong/OpenGAN Evaluation Metric. To evaluate open-set discrimination that measures the open-vs-closed binary classification performance, we follow the literature [35,44] and use the area under ROC curve (AUROC) [15]. AUROC is a calibrationfree and threshold-less metric, simplifying comparisons between methods and reliable in large open-closed imbalance situation. For open-set recognition that measures (K+1)way classification accuracy (K closed-set classes plus the (K+1) th open-set class), we report the macro average F1score over all the (K+1) classes on the valsets [54,6].", "publication_ref": ["b57", "b60", "b6", "b28", "b5", "b44", "b27", "b61", "b13", "b68", "b49", "b45", "b35", "b44", "b14", "b54", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Compared Methods", "text": "We compare the following representative baselines and state-of-the-art methods for open-set recognition.\nBaselines. First, we explore classic generative models learned on the closed-train-set, including Nearest Neighbors (NNs) [50] and Gaussian Mixture Models (GMMs) which were found to perform quite well over L2-normalized OTS features [33]. We refer the reader to the supplemental for details of GMMs as they are strong yet underexplored baseline in the literature. Both models can be used for open-set discrimination by thresholding NN distances or likelihoods. We further examine the idea of outlier exposure [31] that learns an open-vs-closed binary classifier (CLS). Lastly, following classic work in semantic segmentation [19], we evaluate a (K+1)-way classifier trained with outlier exposure, in which we use the softmax score corresponding to the (K+1) th \"other\" class as the open-set like-Table 1: Open-set discrimination (Setup-I) measured by area under ROC curve (AUROC)\u2191. We report methods marked by * with their best reported numbers in the compared papers. Recall that OpenGAN-0 does not train on outlier data (i.e., \u03bb0=0 in Eq. 2) and only selects discriminator checkpoints on the validation set. OpenGAN-0 f ea clearly performs the best. Defined on the off-the-shelf (OTS) features of closed-world K-way networks, NN f ea and OpenGAN-0 f ea work much better than their pixel version (NN pix and OpenGAN-0 pix ).  [30] and Entropy [58] (derived from softmax probabilities), and calibrated MSP (MSP c ) [36]. OpenMax [6] fits logits to Weibull distributions [55] [20,38]. The estimated uncertainties are used as open-set likelihoods. We implement MCdrop via 500 samples.\nGANs. GOpenMax [21] and OSRCI [42] train GANs to generate fake images to augment closed-set data for openset recognition. Other types of GANs can also be used for open-set recognition, such as BiGANs [66], on which we show our technical insights (e.g., training on OTS features and directly using the discriminator) also apply (Table 2).\nWhen possible, we train the methods using their opensource code. We implement NN, CLS and OpenGAN on both RGB images (marked with pix ) and OTS features (marked with f ea ) for comparison. For fair comparison, we tune all the models for all methods on the same val-sets.", "publication_ref": ["b50", "b33", "b31", "b18", "b30", "b58", "b36", "b5", "b55", "b19", "b38", "b20", "b42", "b66"], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Setup-I: Open-Set Discrimination", "text": "Datasets. MNIST/CIFAR/SVHN/TinyImageNet are widely used in the open-set literature, and we follow the literature to experiment with these datasets [42,44]. For each of the first three datasets that have ten classes, we randomly split 6 (4) classes of train/val-sets as the closed (open) train/val-sets respectively. For TinyImageNet that has 200 classes, we randomly split 20 (180) classes of train/val-sets as the closed (open) train/val-set. On each dataset and for each method, we repeat five times with different random splits and report the average AUROC on the val-set [42,44]. As all methods have small standard deviations in their performance (<0.02), we omit them for brevity.\nResults. As this setup assumes no open training data, we cannot train discriminative classifiers like CLS. But we can still train OpenGAN-0 that uses GAN-discriminator (with  [66], which learns a BiGAN with both the reconstruction error and the GANdiscriminator. We compare BiGAN's performance by either using the reconstruction error (BiGAN r ) or its discriminator (BiGAN d ) for open-set recognition. We also compare building BiGANs on either pixels (BiGAN pix ) or features (BiGAN f ea ). Table 2 lists detailed comparisons under Setup-I (all models are selected on the val-sets). Clearly, our conclusions hold regardless of the base GAN architecture: 1) using OTS features rather than pixels (cf. BiGAN f ea vs BiGAN pix ), and 2) more importantly, using discriminators instead of reconstruction errors (cf. BiGAN d vs. BiGAN r ).", "publication_ref": ["b42", "b44", "b42", "b44", "b66"], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Setup-II: Cross-Dataset Open-Set Recognition", "text": "Using cross-dataset examples as the open-set is another established protocol [36,35,31,18]. We follow the \"less bi-Table 3: Open-set recognition (Setup-II) measured by AUROC\u2191, and macro-averaged F1-score\u2191 over all (K+1) classes. We use Tiny-ImageNet (K=200) as the closed-set, and four different datasets as the open-sets. To report a method on a specific open-test-set out of four (first column), we perform four runs in which we use one of the four datasets as a validation set for training/tuning, and then average the performance measures over the four runs with a superscript marking the standard deviation. Methods such as Nearest Neighbor (NN) do not need tuning and hence have zero deviations. We provide a summary number in the bottom macro row by averaging the results over all open-test-sets. Detailed results in Table 4. Clearly, a binary classifier trained on features (CLS f ea ) already outperforms prior methods. However, when trained on pixels, CLS pix works poorly in AUROC due to overfitting to high-dimensional raw images, but performs decently in F1. To note, without handling the open-set, the K-way model (trained only on the closed-set TinyImageNet) achieves 0.553 F1-score over (K+1) classes, suggesting that, when K is large (K=200 here), F1-score can hardly reflect open-set discrimination performance which is better measured by AUROC. While largely underexplored in the literature, training a (K+1)-way model works quite well. Clearly, OpenGAN f ea works the best in both AUROC and F1-score. Please refer to Fig. 3(f-i 4). We use bilinear interpolation to resize all images to 64x64 to match TinyImageNet image resolution.\nResults. Table 3 shows detailed results. First, methods perform much better on features than pixels (e.g., CLS f ea vs. CLS pix ); and our OpenGAN performs the best. Perhaps surprisingly, OpenMax, a classic open-set, does not work well in this setup. This is consistent with the results in [18,57] [31]) are less effective. Visualization. Fig. 1 shows some synthesized images by GAN pix , and we visualize more in the supplement. To intuitively illustrate how the synthesized images help better span the open-world, we analyze why a simple discriminator works so well when trained on the OTS features. We visualize the features in Fig. 3 (a) and \"decision landscape\" in Fig. 3 (b-e), demonstrating that the closed-and open-set images are clearly separated in the feature space.", "publication_ref": ["b36", "b35", "b31", "b17", "b17", "b57", "b31"], "figure_ref": [], "table_ref": ["tab_7", "tab_7"]}, {"heading": "Setup-III: Open-Set Semantic Segmentation", "text": "Open-set semantic segmentation has been explored in recent work [7,29], which creates synthetic open-set pix-els by pasting virtual objects (e.g., cropped from PAS-CAL VOC masks [19]) on Cityscapes images. In this work, we do not generate synthetic pixels but instead repurpose \"other\" pixels (outside the set of K classes) that already exist in Cityscapes. Interestingly, classic semantic segmentation benchmarks evaluate these \"other\" pixels as a separate background class [19], but Cityscapes ignores them in its evaluation (as do many other contemporary datasets [10,4,51,43]). The historically-ignored pixels include vulnerable objects (e.g., strollers in Fig. 2), and can be naturally evaluated as open-set examples.\nDatasets. Cityscapes [14] contains 1024x2048 highresolution urban scene images with 19 class labels for semantic segmentation. We construct our train-and val-sets from its 2,975 training images, in which we use the last 10 images as val-set and the rest as train-set. We use its official 500 validation images as our test-set. The \"other\" pixels (cf. Fig. 2) are the open-set examples in this setup. We refer readers to the supplemental for details, such as model architecture, batch construction, weight tuning, etc.\nPixel Generation. As Cityscapes has high-resolution images (1024x2048), it is nontrivial to train OpenGAN pix , especially its special form OpenGAN-0 pix , which must learn to generate high-resolution images. We find the successful training of OpenGAN-0 pix depends on the resolution of images to be generated: we train OpenGAN-0 pix by generating patches (64x64), not full-resolution images.\nResults. Table 5 lists quantitative results. As we train OpenGAN and CLS with open pixels, we diagnose in Fig. 5 the open-set performance by varying the number of training images that provide the open-training pixels, along with closed-training pixels from all training images. First, these results show that OpenGAN f ea substantially outperforms all other methods. Generally speaking, the methods that process features outperform those that process pixels (e.g., OpenGAN and CLS in Fig. 5). This suggests that OTS features (from the segmentation network) serve as a pow-  [61] except the ones operating on pixels (as marked by pix ). Our approach OpenGAN f ea clearly performs the best. Fig. 5 analyzes OpenGAN trained with varied number of open-set pixels, when built on either pixels or OTS features.\nMSP [30] Entropy [58] OpenMax [6] C2AE [44] MSPc [36] MCdrop [20] GDM [35] GMM [33] ", "publication_ref": ["b6", "b28", "b18", "b18", "b9", "b3", "b51", "b43", "b13", "b61", "b30", "b58", "b5", "b44", "b36", "b19", "b35", "b33"], "figure_ref": ["fig_0", "fig_0"], "table_ref": ["tab_8"]}, {"heading": "Conclusion", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Model Architecture", "text": "We describe the network architectures of OpenGAN. Because our final version OpenGAN f ea operates on off-theshelf (OTS) features, we use multi-layer perceptron (MLP) networks for the generator and discriminator. Because OpenGAN pix operates on pixels, we make use of convolutional neural network (CNN) architectures. We begin with the former.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "OpenGAN pix architecture", "text": "OpenGAN pix 's generator and discriminator follow the CycleGAN architecture [68]. We change the stride size in the convolution layers to adapt the networks to specific image resolution (e.g., CIFAR 32x32 and TinyImageNet 64x64). The generator and discriminator in OpenGAN pix have model sizes as \u223c14MB and \u223c11MB, respectively. We find it important to ensure that OpenGAN pix has a larger capacity than OpenGAN f ea to generate high-dimensional RGB raw images.", "publication_ref": ["b68"], "figure_ref": [], "table_ref": []}, {"heading": "Setup for Open-Set Semantic Segmentation", "text": "In this work, we use Cityscapes to study open-set semantic segmentation. Prior work suggests pasting virtual objects (e.g., cropped from PASCAL VOC masks [19]) on Cityscapes images as open-set pixels [7,29]. We notice that Cityscapes ignores a sizeable portion of pixels in its benchmark, as demonstrated by Figure 7  We highlight these pixels over an image (left) and its semantic annotations (right). Cityscapes labels these pixels as rectification-border (artifacts at the image borders caused by stereo rectification), ego-vehicle (a part of the car body at the bottom of the image including car logo and hood) and out-of-roi (narrow strip of 5 pixels along the image borders). These noise-pixels can be easily identified without machinelearned methods. Therefore, we do not evaluate on these pixels.", "publication_ref": ["b18", "b6", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "Data Setup.", "text": "Cityscapes training set has 2,975 images. We use the first 2,965 images for training, and hold out the last 10 as validation set for model selection. We use the 500 Cityscapes validation images as our test set. Here are the statistics for the full train/val/test sets.\n\u2022 train-set for closed-pixels: 2,965 images providing 334M closed-set pixels.\n\u2022 train-set for open-pixels: 2,965 images providing 44M open-set pixels.\n\u2022 val-set for closed-pixels: 10 images providing 1M closed-set pixels.\n\u2022 val-set for open-pixels: 10 images providing 0.2M open-set pixels.\n\u2022 test-set for closed-pixels: 500 images providing 56M pixels.\n\u2022 test-set for open-pixels: 500 images providing 2M pixels.\nNote that, we exclude the pixels labeled with rectification-border (artifacts at the image borders caused by stereo rectification), ego-vehicle (a part of the car body at the bottom of the image including car logo and hood) and out-of-roi (narrow strip of 5 pixels along the image borders). These pixels can be easily localized using camera information. We demonstrate such pixels in Figure 8. In this sense, these pixels are not unknown open-set pixels but known noises caused by sensors and viewpoint. Therefore, we do not include them for openset evaluation. Feature setup.\n\u2022 M pix , where M \u2208 {CLS, OpenGAN}, corresponds to a model defined on raw pixels.\n\u2022 M f ea corresponds to a model defined on embedding features at the penultimate layer of underlying semantic segmentation network (i.e., HRNet as introduced below).\n\u2022 HRNet [61] is a top-ranked semantic segmentation model on Cityscapes. It has a multiscale pyramid head that produce high-resolution segmentation prediction. We extract embedding features at its penultimate layer (720-dimensional before the 19-way classifier). We also tried other layers but we did not observe significant difference in their performance. \u2022 run the OpenGAN f ea generator (being trained on-thefly) to synthesize 2,500 \"fake\" open-set pixel features.\nSimilarly, to train OpenGAN pix which is fullyconvolutional, we construct a batch of 10,000 pixels as below.\n\u2022 We feed a random real image to the OpenGAN pix discriminator, and penalize predictions on 5000 random closed pixels and 2500 random open pixels.\n\u2022 We run the OpenGAN pix generator (being trained onthe-fly) to synthesize a \"fake\" image. We feed this \"fake\" image to the discriminator along with open-set labels. We penalize 2500 random \"fake\" pixels.", "publication_ref": ["b61"], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "Model Selection", "text": "Due to the unstable training of GANs [2], model selection is crucial and challenging. GANs are typically used for generating realistic images, so model selection for GANs focuses on selecting generators. To do so, one relies heavily on manual inspection of visual results over the generated images from different model epochs [25]. In contrast, we must select the discriminator, rather than the generator, because we use the discriminator as an open-set likelihood function for open-set recognition. It is important to note that, in theory, a perfectly trained discriminator would not be capable of recognizing fake open-set data because of the equilibrium in the discriminator/generator game [3]. Although such an equilibrium hardly exist in practice, we find it crucial to select GAN discriminators to be used as openset likelihood function. For model selection, we further find it crucial to use a validation set that consists of both real open and closed data. We present this study below.\nModel selection is crucial. In Figure 9 For the scatter plot, the ideal case is that the traintime and test-time performance is linearly correlated, i.e., all dots appear in the diagonal line (from origin to top right). But their performances on the two sets are not correlated, suggesting that using the synthesized data for model selection is not sufficient. Instead, we find it crucial to use a validation set of real open examples to select the openset discriminator. Our observation is consistent to what reported in [31]. It is worth noting that the models selected on the validation set do generalize to test sets. This has been demonstrated in Table 3 and 4 in the main paper.", "publication_ref": ["b1", "b24", "b2", "b31"], "figure_ref": ["fig_8"], "table_ref": []}, {"heading": "Hyper-Parameter Tuning", "text": "Strictly following the practice of machine learning, we tune hyper-parameters on the same validation set. We now study parameter tuning through open-set semantic segmentation (Setup-III). We select the best OpenGAN model according to the performance on the validation set (10 images).\nIn training OpenGAN, a training batch contains both real closed-and open-set pixels, and synthesized fake open pixels. Correspondingly, our loss function has three terms (refer to Eq. 2 in the main paper). Therefore, we tune the hyper-parameter \u03bb o and \u03bb G as below to balance the terms in the loss function that exploits real open data and generated data: In Table 6, we show the performance on the test set of OpenGAN pix and OpenGAN f ea with varied open training images. For each selected model, we mark the corresponding \u03bb G that yields the best performance (on validation set). Roughly speaking, it is preferable to set a lower weight \u03bb G when we have more real open-set training data. However, we do not see a clear correlation between the weight \u03bb G and test-time performance. We believe this is due to the random initialization which affects adversarial learning.\nWe also study how models trained with different \u03bb G perform on validation set and test set, and if the model selected on the validation set can reliably perform well on the testing set. Figure 11 plots the performance as a function of \u03bb G on validation set and test set. Hereby we choose the OpenGAN f ea model trained with 1000 open training images. We can see the performance on the validation set reliably reflects the performance on the test set. This confirms that model selection on the validation set is reliable.", "publication_ref": [], "figure_ref": ["fig_11"], "table_ref": ["tab_13"]}, {"heading": "Statistical Models for Open-Set", "text": "Our previous work introduces a lightweight statistical pipeline that repurposes off-the-shef (OTS) deep features for open-set recognition [33]. For the completeness of this paper, we briefly introduce this pipeline: (1) extracting OTS features (with appropriate processing detailed below) of closed-set training examples using the underlying  We color the dots from blue\u2192 red to marks models saved at epoch-0\u219250, respectively. We use the three datasets (under Setup-I) following the typical setup of open-set recognition. The ideal correlation is that all the dots lie in the diagonal from bottom-left to top-right. However, there is no correlation between training (fake-vs-real) and validation (open-vs-closed) performance. Moreover, because the dots appear to be on the right part in the plots, this means that fake-vs-real classification (as denoted by the x-axis) is much easier than open-vs-closed classification (as denoted by the y-axis). These scatter plots demonstrate that ( 1  we vary the hyper-parameter \u03bbG to train OpenGAN models. Recall that \u03bbG controls the contribution of synthesized data in the loss function. We conduct model selection on the val-set (10 images), and report here the performance (AUROC\u2191) on the test set (500 images). We also mark the \u03bbG for each of the selected models. It seems to be preferable to set a lower weight \u03bbG (for the term exploiting synthesized data examples) when we have more real open-set data, but we do not see a tight correlation between \u03bbG and test-time performance. We believe this is because of the (random) initialization of model weights that has a non-trivial impact on training GANs and their final performance.    [30] and logits [6,27,44] which can be thought of as features extracted at top layers. Similar to [35], we find it crucial to analyze features from intermediate layers for open-set recognition, because logits and softmax may be too invariant to be effective for open-set recognition (Fig. 13). One immediate challenge to extract features from an intermediate layer is their high dimensionality, e.g.  We verify if such a street-shop appears in the training set. We manually search for a similar street-shop in the training set, and find the one (bottom-row) most similar to the testing example in terms of size. Importantly, we did not find any other street-shops in the training set that sell clothes like the testing example shown in the top row. In this sense, the testing image in the top row does contain a real open-set example (i.e., the street-shop) in terms of not only size, but also novel content.\nof size 512x7x7 from ResNet18 [28]. To reduce feature dimension, we simply (max or average) pool the feature activations spatially into a 512-dim feature vectors [63]. We can further reduce dimension by apploying PCA, which can reduce dimensionality by 10\u00d7 (from 512-dimensional to 50 dimensional) without sacrificing performance. We find this dimensionality particularly important for learning secondorder covariance statistics as in GMM, described below. Finally, following [24,26], we find it crucial to L2-normalize extracted features (Fig. 13). We refer the reader to [33] for quantitative results. Note in this paper, we do not L2normalize features for training OpenGANs.\nStatistical models. Given the above extracted features, we use various generative statistical methods to learn the confidence/probability that a test example belongs to the closed-set classes. Such statistical methods include simple Normalizing the pre-logits features separates them even better. These plots intuitively demonstrate the benefit of L2-normalization and using OTS features rather than the highly-invariant logits. parametric models such as class centroids [40] and classconditional Gaussian models [35,27], non-parametric models such as NN [8,32], and mixture models such as (classconditional) GMMs and k-means [11]. A statistical model labels a test example as open-set when the inverse probability (e.g., of the most-likely class-conditional GMM) or distance (e.g., to the closest class centroid) is above a threshold. One benefit of such simple statistical models is that they are interpretable and relatively easier to diagnose failures. For example, one failure mode is an open-set sample being misclassified as a closed-set class. This happens when open-set data lie close to a class-centroid or Gaussian component mean (see Fig. 13). Note that a single statistical model may have several hyperparameters -GMM can have multiple Gaussian components and different structures of second-order covariance, e.g., either a single scalar, a diagonal matrix or a general covariance per component. We make use of validation set to determine these hyperparameters, as opposed to prior works that conduct model selection either unrealistically on the test-set [44] or on large-scale val-set which could be arguably used for training [35]. We refer the reader to [33] for detailed analysis.\nLightweight Pipeline. We re-iterate that the above feature extraction and statistical models result in a lightweight pipeline for open-set recognition. To understand this, we analyze the number of parameters involved in the pipeline. Assume we learn a GMM over 512x7x7 feature activations, and specify a general covariance and five Gaussian components. If we learn the GMM directly on the feature activations, the number of parameters from the second-order covariance alone is at the scale of (512 * 7 * 7) 2 . With the help of our feature extraction (including spatial pooling and PCA), we have 50-dim feature vectors, and the number of parameters in the covariance matrices is now at the scale of 50 2 . This means a huge reduction (10 5 \u00d7) in space usage! We count the total number of parameters in this GMM: 3.3\u00d7 10 4 32-bit float parameters including PCA and GMM's five components, amounting to 128KB storage space. Moreover, given that PCA just runs once for all classes, even when we learn such GMMs for each of 19 classes (such as defined in Cityscapes), it merely requires 594KB storage space! Compared to the modern networks such as HRNet (>250MB), our statistical pipeline for open-set recognition adds a negligible (0.2%) amount of compute, making it quite practical for implementation on autonomy stacks.", "publication_ref": ["b33", "b30", "b5", "b26", "b44", "b35", "b27", "b63", "b23", "b25", "b33", "b40", "b35", "b26", "b7", "b32", "b10", "b44", "b35", "b33"], "figure_ref": ["fig_14", "fig_14", "fig_14"], "table_ref": []}, {"heading": "Further Quantitative Results", "text": "While in the main paper we compare OpenGAN to many methods and cannot include more due to space issues, we list a few more in this appendix including Entropy [58], GMM [33], CGDL [59], OpenHybrid [67], and RPL++ [13]. Except for Entropy which is a classic method, the rest were published recently. Table 7 lists the comparisons under Setup-I. Please refer to Section 4.2 of the main paper for the detailed setup. Numbers are comparable to Table 1 in the main paper. In summary, our OpenGAN outperforms all these prior methods under this setup, achieving the state-of-the-art. ", "publication_ref": ["b58", "b33", "b59", "b67", "b12"], "figure_ref": [], "table_ref": ["tab_16"]}, {"heading": "Visualization of Generated Images", "text": "In this section, we visualize some synthesized examples for intuitive demonstration.\nGenerating Small Images. Recall that OpenGAN-0 pix trains a normal GAN and uses its discriminator as the openset likelihood function. As demonstrated in the main paper, OpenGAN-0 pix performs surprisingly well under Setup-I (i.e., using CIFAR10, MNIST and SVHN datasets) and Setup-II (using TinyImageNet as the closed-set and other datasets as the open-set). OpenGAN-0 pix also enables us to generate visual results for intuitive inspection. In Figure 14, we display real and synthesized \"fake\" images under Setup-I on each of the three datasets. In Figure 15, we display real and fake images under Setup-II by using tiny-ImageNet as the closed-set and other datasets as the openset. We can see the generated images look realistic in terms of color and tone. But they are not strictly open-set images as they contain synthesized known contains (e.g., the digits in the synthesized images are closed-set digits). This intuitively demonstrates that a perfectly trained discriminator will not be capable of discriminating open and closed-sets due to the nature of the min-max game in training GANs. However, from the low confidence scores of classifying the generated fake data as closed-set shown in Figure 14, we can see the discriminator almost naively recognizes these synthesized examples as \"fake\" data. This shows the synthesized data are insufficient to be used for model selection. Moreover, from the classification confidence scores on the closed-testing and open-testing images in each datasets, we can see the discriminator is not calibrated. In other words, we cannot naively set threshold as 0.5 for open-vs-closed classification. This is largely hidden by AUROC metric which is calibration-free. This implies a potential limitation and suggests future work to calibrate the open-set discriminators.\nGenerating Cityscapes Patches. In the main paper (Fig. 6), we have shown some generated patches. In the appendix, we provide more in Figure 16. As OpenGAN-0 f ea generates features instead of pixel patches, we \"synthesize\" the patches analytically -for a generated feature, from training pixels represented as OTS features, we find the nearest-neighbor pixel feature (w.r.t L1 distance), and use the RGB patch centered at that pixel as the \"synthesized\" patch. We can see OpenGAN-0 pix synthesizes re-alistic patches w.r.t color and tone, but it (0.549 AUROC) significantly unperforms OpenGAN-0 f ea (0.709 AUROC) for open-set segmentation. The \"synthesized\" patches by OpenGAN-0 f ea capture many open-set objects, such as bridges, vehicle logo and back of traffic sign, all of which are outside the 19 classes defined in Cityscapes. This intuitively explains why OpenGAN-0 f ea (0.709 AUROC) works much better than OpenGAN-0 pix (0.549 AUROC).", "publication_ref": [], "figure_ref": ["fig_2", "fig_16", "fig_2"], "table_ref": []}, {"heading": "Visual Results of Open-Set Segmentation", "text": "On the task of open-set semantic segmentation, first, we show in Figure 12 ", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Failure Cases and Limitations", "text": "As we use an discriminator as the open-set likelihood function, straightforwardly, failure cases happen when the classification is not correct, as shown by the marked confidence scores in Figure 14, as well as the thresholded perpixel predictions in figures from 17 through 23.\nHereby we point out other failure cases and limitations. First, as we have explained in the main paper, the GANdiscriminator will eventually become incapable of discriminating closed-set and fake/open-set images due to the nature of GANs that strikes an equilibrium between the discriminator and generator. Although we empirically show superior performance by model selection on a validation set, there surely exists risks that the validation set is biased in an unknown way which could catastrophically hurt the final open-set recognition performance. This is also true even with outlier training examples. Therefore, in the real openworld practitioners should be aware of such a bias, and exploit prior knowledge in constructing \"reliable\" training and validation sets in trainign OpenGANs. Second, as we adopt adversarial training for OpenGANs, it is straightforward to ask if OpenGAN is robust to adversarial perturbations on the input images. We have not investigated this point yet, and we leave it as future work. data. On each of the three datasets, we show some random images that are closed-training image (for training GANs), synthesized \"fake\" images, closed-testing images (from known classes) and open-testing images (from unknown classes). We also mark the probability for each image of being classified as closed-set by the discriminator. We can see the synthesized images look realistic in terms of color, tone and shape. But the discriminator can easily recognize these fake images (as indicated by the low probability). Moreover, although the discriminator achieves good open-vs-closed classification performance measured by AUROC (which is calibration-free), the confidence scores (probability) are not calibrated well. This implies that the discriminator may need to be calibrated for real-world application. ", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Real OpenGAN-0 pix", "text": "OpenGAN-0 f ea Figure 16: Visuals of real Cityscapes image patches (left), synthesized patches by OpenGAN-0 pix (mid) and GOpenGAN-0 f ea (right). As OpenGAN-0 f ea generates feature vectors instead of RGB patches, we \"synthesize\" the patches in an analytical way -for a generated feature, we find the nearest-neighbor per-pixel feature (w.r.t L1 distance) from the training images, and then find the RGB patch centered at the associated pixel with the per-pixel feature. The real patch is our \"synthesized\" patch for that generated feature. The synthesized patches by OpenGAN-0 pix do look realistic in terms of color and tone, but OpenGAN-0 pix (0.549 AUROC) does not work as well as OpenGAN-0 f ea (0.709 AUROC). The \"synthesized\" patches by OpenGAN-0 f ea do capture some unknown open-set objects, such as bridge, back of traffic sign and unknown static objects, none of which belong to any of the 19 classes defined in Cityscapes for semantic segmentation (cf. Figure 7).       ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgement", "text": "This work was supported by the CMU Argo AI Center for Autonomous Vehicle Research.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Ganomaly: Semi-supervised anomaly detection via adversarial training", "journal": "", "year": "2018", "authors": "Samet Akcay; Amir Atapour-Abarghouei; Toby P Breckon"}, {"ref_id": "b1", "title": "Wasserstein generative adversarial networks", "journal": "", "year": "2017", "authors": "Martin Arjovsky; Soumith Chintala; L\u00e9on Bottou"}, {"ref_id": "b2", "title": "Generalization and equilibrium in generative adversarial nets (gans)", "journal": "", "year": "2017", "authors": "Sanjeev Arora; Rong Ge; Yingyu Liang; Tengyu Ma; Yi Zhang"}, {"ref_id": "b3", "title": "Semantickitti: A dataset for semantic scene understanding of lidar sequences", "journal": "", "year": "2019", "authors": "Jens Behley; Martin Garbade; Andres Milioto"}, {"ref_id": "b4", "title": "Towards open world recognition", "journal": "", "year": "2015", "authors": "Abhijit Bendale; Terrance Boult"}, {"ref_id": "b5", "title": "Towards open set deep networks", "journal": "", "year": "2008", "authors": "Abhijit Bendale; Terrance E Boult"}, {"ref_id": "b6", "title": "The fishyscapes benchmark: Measuring blind spots in semantic segmentation", "journal": "", "year": "2019", "authors": "Hermann Blum; Paul-Edouard Sarlin; Juan Nieto; Roland Siegwart; Cesar Cadena"}, {"ref_id": "b7", "title": "In defense of nearest-neighbor based image classification", "journal": "", "year": "2008", "authors": "Oren Boiman; Eli Shechtman; Michal Irani"}, {"ref_id": "b8", "title": "Large scale gan training for high fidelity natural image synthesis", "journal": "", "year": "2019", "authors": "Andrew Brock; Jeff Donahue; Karen Simonyan"}, {"ref_id": "b9", "title": "nuscenes: A multimodal dataset for autonomous driving", "journal": "", "year": "2020", "authors": "Holger Caesar; Varun Bankiti; Alex H Lang; Sourabh Vora; Venice Erin Liong; Qiang Xu; Anush Krishnan; Yu Pan; Giancarlo Baldan; Oscar Beijbom"}, {"ref_id": "b10", "title": "Deep quantization network for efficient image retrieval", "journal": "", "year": "2016", "authors": "Yue Cao; Mingsheng Long; Jianmin Wang; Han Zhu; Qingfu Wen"}, {"ref_id": "b11", "title": "Anomaly detection: A survey", "journal": "ACM computing surveys (CSUR)", "year": "2009", "authors": "Varun Chandola; Arindam Banerjee; Vipin Kumar"}, {"ref_id": "b12", "title": "Learning open set network with discriminative reciprocal points", "journal": "", "year": "2005", "authors": "Guangyao Chen; Limeng Qiao; Yemin Shi; Peixi Peng; Jia Li; Tiejun Huang; Shiliang Pu; Yonghong Tian"}, {"ref_id": "b13", "title": "The cityscapes dataset for semantic urban scene understanding", "journal": "", "year": "2007", "authors": "Marius Cordts; Mohamed Omran; Sebastian Ramos; Timo Rehfeld; Markus Enzweiler; Rodrigo Benenson; Uwe Franke; Stefan Roth; Bernt Schiele"}, {"ref_id": "b14", "title": "The relationship between precision-recall and roc curves", "journal": "", "year": "2006", "authors": "Jesse Davis; Mark Goadrich"}, {"ref_id": "b15", "title": "Image anomaly detection with generative adversarial networks", "journal": "Springer", "year": "2018", "authors": "Lucas Deecke; Robert Vandermeulen; Lukas Ruff; Stephan Mandt; Marius Kloft"}, {"ref_id": "b16", "title": "Iterative energy-based projection on a normal data manifold for anomaly localization", "journal": "", "year": "", "authors": "David Dehaene; Oriel Frigo; S\u00e9bastien Combrexelle; Pierre Eline"}, {"ref_id": "b17", "title": "Reducing network agnostophobia", "journal": "", "year": "2006", "authors": "Manuel Akshay Raj Dhamija; Terrance G\u00fcnther;  Boult"}, {"ref_id": "b18", "title": "The pascal visual object classes challenge: A retrospective", "journal": "IJCV", "year": "2007", "authors": "Mark Everingham; Ali Eslami; Luc Van Gool; K I Christopher; John Williams; Andrew Winn;  Zisserman"}, {"ref_id": "b19", "title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning", "journal": "", "year": "2006", "authors": "Yarin Gal; Zoubin Ghahramani"}, {"ref_id": "b20", "title": "Generative openmax for multi-class open set classification", "journal": "", "year": "2004", "authors": "Zongyuan Ge; Sergey Demyanov; Zetao Chen; Rahil Garnavi"}, {"ref_id": "b21", "title": "Recent advances in open set recognition: A survey", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "", "authors": "Chuanxing Geng;  Sheng-Jun; Songcan Huang;  Chen"}, {"ref_id": "b22", "title": "Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection", "journal": "", "year": "2019", "authors": "Dong Gong; Lingqiao Liu; Vuong Le; Budhaditya Saha"}, {"ref_id": "b23", "title": "Multi-scale orderless pooling of deep convolutional activation features", "journal": "", "year": "2014", "authors": "Yunchao Gong; Liwei Wang; Ruiqi Guo; Svetlana Lazebnik"}, {"ref_id": "b24", "title": "Generative adversarial nets", "journal": "", "year": "2003", "authors": "Ian Goodfellow; Jean Pouget-Abadie; Mehdi Mirza; Bing Xu; David Warde-Farley; Sherjil Ozair; Aaron Courville; Yoshua Bengio"}, {"ref_id": "b25", "title": "End-to-end learning of deep visual representations for image retrieval", "journal": "", "year": "2017", "authors": "Albert Gordo; Jon Almazan; Jerome Revaud; Diane Larlus"}, {"ref_id": "b26", "title": "Your classifier is secretly an energy based model and you should treat it like one", "journal": "", "year": "2019", "authors": "Will Grathwohl; Kuan-Chieh Wang; J\u00f6rn-Henrik Jacobsen; David Duvenaud; Mohammad Norouzi; Kevin Swersky"}, {"ref_id": "b27", "title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"ref_id": "b28", "title": "Mantas Mazeika, Mohammadreza Mostajabi", "journal": "Jacob Steinhardt, and Dawn Song", "year": "", "authors": "Dan Hendrycks; Steven Basart"}, {"ref_id": "b29", "title": "A benchmark for anomaly segmentation", "journal": "", "year": "2019", "authors": ""}, {"ref_id": "b30", "title": "A baseline for detecting misclassified and out-of-distribution examples in neural networks", "journal": "", "year": "2008", "authors": "Dan Hendrycks; Kevin Gimpel"}, {"ref_id": "b31", "title": "Deep anomaly detection with outlier exposure", "journal": "", "year": "2006", "authors": "Dan Hendrycks; Mantas Mazeika; Thomas Dietterich"}, {"ref_id": "b32", "title": "Nearest neighbors distance ratio open-set classifier", "journal": "", "year": "2017", "authors": "Pedro R Mendes J\u00fanior; Roberto M De Souza; Rafael Werneck; V Bernardo;  Stein; V Daniel;  Pazinato;  Waldir R De Almeida; A B Ot\u00e1vio; Ricardo Penatti; Anderson Torres;  Rocha"}, {"ref_id": "b33", "title": "An empirical exploration of open-set recognition via lightweight statistical pipelines", "journal": "", "year": "2021", "authors": "Shu Kong; Deva Ramanan"}, {"ref_id": "b34", "title": "Training confidence-calibrated classifiers for detecting outof-distribution samples", "journal": "", "year": "2018", "authors": "Kimin Lee; Honglak Lee; Kibok Lee; Jinwoo Shin"}, {"ref_id": "b35", "title": "A simple unified framework for detecting out-of-distribution samples and adversarial attacks", "journal": "", "year": "2018", "authors": "Kimin Lee; Kibok Lee; Honglak Lee; Jinwoo Shin"}, {"ref_id": "b36", "title": "Enhancing the reliability of out-of-distribution image detection in neural networks", "journal": "", "year": "2006", "authors": "Shiyu Liang; Yixuan Li; Rayadurgam Srikant"}, {"ref_id": "b37", "title": "Generative adversarial active learning for unsupervised outlier detection", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": "2004", "authors": "Yezheng Liu; Zhe Li; Chong Zhou; Yuanchun Jiang; Jianshan Sun; Meng Wang; Xiangnan He"}, {"ref_id": "b38", "title": "A general framework for uncertainty estimation in deep learning", "journal": "IEEE Robotics and Automation Letters", "year": "2020", "authors": "Antonio Loquercio; Mattia Segu; Davide Scaramuzza"}, {"ref_id": "b39", "title": "Visualizing data using t-sne", "journal": "Journal of Machine Learning Research", "year": "2008-11", "authors": "Laurens Van Der Maaten; Geoffrey Hinton"}, {"ref_id": "b40", "title": "Metric learning for large scale image classification: Generalizing to new classes at near-zero cost", "journal": "", "year": "2012", "authors": "Thomas Mensink; Jakob Verbeek; Florent Perronnin; Gabriela Csurka"}, {"ref_id": "b41", "title": "Do deep generative models know what they don", "journal": "", "year": "2018", "authors": "Eric Nalisnick; Akihiro Matsukawa; Yee Whye Teh; Dilan Gorur; Balaji Lakshminarayanan"}, {"ref_id": "b42", "title": "Open set learning with counterfactual images", "journal": "", "year": "2004", "authors": "Lawrence Neal; Matthew Olson; Xiaoli Fern; Weng-Keen Wong; Fuxin Li"}, {"ref_id": "b43", "title": "The mapillary vistas dataset for semantic understanding of street scenes", "journal": "", "year": "2017", "authors": "Gerhard Neuhold; Tobias Ollmann; Samuel Rota Bulo; Peter Kontschieder"}, {"ref_id": "b44", "title": "C2AE: class conditioned auto-encoder for open-set recognition", "journal": "", "year": "2019", "authors": "Poojan Oza; M Vishal;  Patel"}, {"ref_id": "b45", "title": "Automatic differentiation in pytorch", "journal": "", "year": "2017", "authors": "Adam Paszke; Sam Gross; Soumith Chintala; Gregory Chanan; Edward Yang; Zachary Devito; Zeming Lin; Alban Desmaison; Luca Antiga; Adam Lerer"}, {"ref_id": "b46", "title": "Generative-discriminative feature representations for open-set recognition", "journal": "", "year": "2020", "authors": "Pramuditha Perera; I Vlad; Rajiv Morariu; Varun Jain; Curtis Manjunatha; Vicente Wigington;  Ordonez;  Patel"}, {"ref_id": "b47", "title": "Deep transfer learning for multiple class novelty detection", "journal": "", "year": "2019", "authors": "Pramuditha Perera; M Vishal;  Patel"}, {"ref_id": "b48", "title": "Generative probabilistic novelty detection with adversarial autoencoders", "journal": "", "year": "2005", "authors": "Stanislav Pidhorskyi; Ranya Almohsen; Gianfranco Doretto"}, {"ref_id": "b49", "title": "Unsupervised representation learning with deep convolutional generative adversarial networks", "journal": "", "year": "2016", "authors": "Alec Radford; Luke Metz; Soumith Chintala"}, {"ref_id": "b50", "title": "Efficient algorithms for mining outliers from large data sets", "journal": "", "year": "2000", "authors": "Sridhar Ramaswamy; Rajeev Rastogi; Kyuseok Shim"}, {"ref_id": "b51", "title": "Playing for data: Ground truth from computer games", "journal": "", "year": "2016", "authors": "Vibhav Stephan R Richter; Stefan Vineet; Vladlen Roth;  Koltun"}, {"ref_id": "b52", "title": "Deep semi-supervised anomaly detection", "journal": "", "year": "", "authors": "Lukas Ruff; A Robert; Nico Vandermeulen; Alexander G\u00f6rnitz; Emmanuel Binder; Klaus-Robert M\u00fcller; Marius M\u00fcller;  Kloft"}, {"ref_id": "b53", "title": "Adversarially learned one-class classifier for novelty detection", "journal": "", "year": "2005", "authors": "Mohammad Sabokrou; Mohammad Khalooei; Mahmood Fathy; Ehsan Adeli"}, {"ref_id": "b54", "title": "Toward open set recognition", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2012", "authors": "J Walter; Anderson Scheirer;  De Rezende; Archana Rocha; Terrance E Sapkota;  Boult"}, {"ref_id": "b55", "title": "Meta-recognition: The theory and practice of recognition score analysis", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2011", "authors": "J Walter; Anderson Scheirer;  Rocha; J Ross; Terrance E Micheals;  Boult"}, {"ref_id": "b56", "title": "Unsupervised anomaly detection with generative adversarial networks to guide marker discovery", "journal": "Springer", "year": "2005", "authors": "Thomas Schlegl; Philipp Seeb\u00f6ck; Ursula Sebastian M Waldstein; Georg Schmidt-Erfurth;  Langs"}, {"ref_id": "b57", "title": "A less biased evaluation of out-of-distribution sample detectors", "journal": "", "year": "2007", "authors": "Alireza Shafaei; Mark Schmidt; James J Little"}, {"ref_id": "b58", "title": "Unsupervised risk estimation using only conditional independence structure", "journal": "", "year": "2008", "authors": "Jacob Steinhardt; S Percy;  Liang"}, {"ref_id": "b59", "title": "Conditional gaussian distribution learning for open set recognition", "journal": "", "year": "2020", "authors": "Xin Sun; Zhenning Yang; Chi Zhang; Keck-Voon Ling; Guohao Peng"}, {"ref_id": "b60", "title": "Unbiased look at dataset bias", "journal": "", "year": "2011", "authors": "Antonio Torralba; Alexei A Efros"}, {"ref_id": "b61", "title": "Deep high-resolution representation learning for visual recognition", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2004", "authors": "Jingdong Wang; Ke Sun; Tianheng Cheng; Borui Jiang; Chaorui Deng; Yang Zhao; Dong Liu; Yadong Mu; Mingkui Tan; Xinggang Wang; Wenyu Liu; Bin Xiao"}, {"ref_id": "b62", "title": "Learning discriminative reconstructions for unsupervised outlier removal", "journal": "", "year": "2015", "authors": "Yan Xia; Xudong Cao; Fang Wen; Gang Hua; Jian Sun"}, {"ref_id": "b63", "title": "Multi-scale recognition with dag-cnns", "journal": "", "year": "2015", "authors": "Songfan Yang; Deva Ramanan"}, {"ref_id": "b64", "title": "Classificationreconstruction learning for open-set recognition", "journal": "", "year": "2019", "authors": "Ryota Yoshihashi; Wen Shao; Rei Kawakami; Shaodi You; Makoto Iida; Takeshi Naemura"}, {"ref_id": "b65", "title": "Efficient gan-based anomaly detection", "journal": "", "year": "2018", "authors": "Houssam Zenati; Chuan Sheng Foo; Bruno Lecouat; Gaurav Manek; Vijay Ramaseshan Chandrasekhar"}, {"ref_id": "b66", "title": "Adversarially learned anomaly detection", "journal": "", "year": "2005", "authors": "Houssam Zenati; Manon Romain; Chuan-Sheng Foo; Bruno Lecouat; Vijay Chandrasekhar"}, {"ref_id": "b67", "title": "Hybrid models for open set recognition", "journal": "", "year": "2005", "authors": "Hongjie Zhang; Ang Li; Jie Guo; Yanwen Guo"}, {"ref_id": "b68", "title": "Unpaired image-to-image translation using cycleconsistent adversarial networks", "journal": "", "year": "2017", "authors": "Jun-Yan Zhu; Taesung Park; Phillip Isola; Alexei A Efros"}, {"ref_id": "b69", "title": "Deep autoencoding gaussian mixture model for unsupervised anomaly detection", "journal": "", "year": "2002", "authors": "Bo Zong; Qi Song; Wei Martin Renqiang Min; Cristian Cheng; Daeki Lumezanu; Haifeng Cho;  Chen"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure2: We motivate open-set recognition with safety concerns in autonomous vehicles (AVs). Contemporary benchmarks such as Cityscapes[14] focus on K classes of interest for evaluation, ignoring a sizeable set of \"other\" pixels that include vulnerable objects like wheelchairs and strollers (upper row). As a result, most state-of-the-art segmentators[61] also ignore these pixels during training, resulting in a stroller mislabeled as a \"motorcycle\" (top) and a street-market mislabeled as a \"building\". Such misclassifications may be critical for AVs because these objects may require different plans for obstacle avoidance (e.g., \"yield\" or \"slow-down\"). Fig.4shows our approach, which explicitly augments state-of-the-art segmentors with open-set reasoning.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "\u03bb o to balance the closed-and open-set training examples. This simple method is effective when the open-training examples are sufficiently representative of testing-time openset data", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Fig. 44qualitatively compares OpenGAN and the entropy method (more visual results are in the supplemental). The visualization shows OpenGAN sufficiently recognize open-set pixels. It also implies failure happens when OpenGAN misclassifies open-vs-closed pixels. Fig. 16 compares some generated patches by OpenGAN-0 f ea and OpenGAN-0 f ea , intuitively showing why using OTS features leads to better performance for open-set recognition.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "We propose OpenGAN for open-set recognition by incorporating two technical insights, 1) training an open-vsclosed classifier on OTS features rather than pixels, and 2) adversarialy synthesizing fake open data to augment the set of open-training data. With OpenGAN, we show using GAN-discriminator does achieve the state-of-the-art on open-set discrimination, once being selected using a val-set of real outlier examples. This is effective even when the outlier validation examples are sparsely sampled or strongly biased. OpenGAN significantly outperforms prior art on both open-set image recognition and semantic segmentation. Outline As elaborated in the main paper, our proposed Open-GAN trains an open-vs-closed binary classifier for openset recognition. Our three major technical insights are (1) model selection of a GAN-discriminator as the open-set likelihood function via validation, (2) augment the available set of real open training examples with adversarially synthesized \"fake\" data, and (3) training OpenGAN on offthe-shelf (OTS) features rather than pixel images. We expand on the techniques of OpenGAN in the appendix, including architecture design, model selection and additional details for training. We also provide additional comparisons to recently published methods and qualitative results. Below is the outline. Section 6: Model architectures for both OpenGAN f ea and OpenGAN pix . Section 7: Detailed setup for open-set semantic segmentation, such as data statistics (e.g., the number of openset pixels in the testing set) and batch construction during training. Section 8: Model selection that is performed on a validation set. Section 9: Hyper-parameter tuning which is performed on a validation set. Section 10: Statistical methods for open-set recognition that learn generative models (e.g., Gaussian Mixture) over off-the-shelf deep features. Section 11: More quantitative comparisons to several approaches published recently. Section 12: Visuals of synthesized images generated by OpenGAN-0 pix and OpenGAN-0 f ea , intuitively demonstrating their effectiveness and limitations. Section 13: Visual results of open-set semantic segmentation. Section 14: Failure Cases and Limitations.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "For open-set image classification, the image features have dimension D = 512 from ResNet18 (the K-way classification networks under Setup-I and II). For open-set segmentation, the per-pixel features have dimension D = 720 at the penultimate layer of HRnet (a top-ranked semantic segmentation model used in this work under Setup-III).", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 8 :8Figure 8: Void pixels in Cityscapes that are not from the closed-set classes nor open-set. We highlight these pixels over an image (left) and its semantic annotations (right). Cityscapes labels these pixels as rectification-border (artifacts at the image borders caused by stereo rectification), ego-vehicle (a part of the car body at the bottom of the image including car logo and hood) and out-of-roi (narrow strip of 5 pixels along the image borders). These noise-pixels can be easily identified without machinelearned methods. Therefore, we do not evaluate on these pixels.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": ", we plot the open-set classification performance as a function of training epochs. We study both OpenGAN-0 f ea and OpenGAN-0 pix on the three datasets as typically used in open-set recognition (under Setup-I). Recall that OpenGAN-0 is to train a normal GAN and use its discriminator as open-set likelihood function for open-set recognition. Clearly, we can see that long training time does not necessarily improve open-set classification performance. We posit that this is due to the unstable training of GANs. This motivates robust model selection using a validation set. Synthesized data are not sufficient for model selection. To study how each checkpoint models perform in training (fake-vs-real classification) and testing (open-vsclosed classification), we scatter-plot Figure 10, where we render the dots with colors to indicate the model epoch (blue\u2192 red dots represent model epoch-0\u219250, respectively).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "\u2022The term exploiting real open data has a weight \u03bb o = 1. We do not tune this as we presume the sparsely sampled open-set examples are equally important as the real closed-set examples. \u2022 The term using the generated \"fake\" data has varied parameter \u03bb G \u2208 [0.05, 0.10, 0.15, 0.20, . . . , 0.80, 0.85, 0.90]. We mainly focus on tuning \u03bb G to study how the synthesized data help training.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 9 :9Figure 9: Open-set image recognition performance vs. training epochs. We show the performance (AUROC) by OpenGAN-0 pix and OpenGAN-0 f ea on the val-sets of the three datasets which are widely studied in the open-set recognition literature (Setup-I). Recall that OpenGAN-0 is to train a normal GAN and use its discriminator as open-set likelihood function for open-set recognition. We can see that best open-set discrimination performance is achieved by intermediate checkpoints of GAN discriminators, and longer training does not necessarily improve performance. This is due to the unstable training of GANs with the min-max game. This motivates the need for robust model selection.", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 10 :10Figure 10: Scatter plot of training performance (fake-vs-real classification) and testing performance (open-vs-closed classification).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": ") intermediate discriminators can perform quite well in open-set discrimination (i.e., on the validation set consisting of real open and closed-set images), and (2) synthesized data are insufficient to be used for model selection.", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 11 :11Figure 11: Tuning hyper-parameter \u03bbG. We plot the open-set discrimination performance (AUROC) as a function of \u03bbG, which controls the contribution of generated data examples in the loss function. The model we report here is OpenGAN f ea -1000 that is trained with 1000 open-set training images. The validation set and test set contain 10 and 500 images. Although the validation set has much fewer images than the test set, the open-set classification performances align well on the two sets.", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Figure 12 :12Figure 12: street-shop as open-set.Figure 4 in the main pa-", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Figure 44Figure 12: street-shop as open-set.Figure 4 in the main paper shows open-set pixel recognition results on a street-shop on a testing image (top-row).We verify if such a street-shop appears in the training set. We manually search for a similar street-shop in the training set, and find the one (bottom-row) most similar to the testing example in terms of size. Importantly, we did not find any other street-shops in the training set that sell clothes like the testing example shown in the top row. In this sense, the testing image in the top row does contain a real open-set example (i.e., the street-shop) in terms of not only size, but also novel content.", "figure_data": ""}, {"figure_label": "13", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "Figure 13 :13Figure 13: t-SNE plots [39] of open vs closed-set testing data, as encoded by different features from a ResNet18 network trained from scratch on the TinyImageNet dataset for 200-way classification. To better view the clustering results, we show zoom-out scatter plots in which we show closed-set data in black, and color open-set examples using their class labels provided by the respective datasets. Left: Logit features mix open and closed data, suggesting that methods based on them (Entropy, SoftMax and OpenMax) may struggle in openset discrimination. Mid: Pre-logit features at the penultimate layer show better separation between closed-and open-set data. Right:Normalizing the pre-logits features separates them even better. These plots intuitively demonstrate the benefit of L2-normalization and using OTS features rather than the highly-invariant logits.", "figure_data": ""}, {"figure_label": "14", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "Figure 14 :14Figure 14: Demonstration of visuals along with the classification confidence scores as probabilities of being recognized as closed-set", "figure_data": ""}, {"figure_label": "15", "figure_type": "figure", "figure_id": "fig_16", "figure_caption": "Figure 15 :15Figure 15: Demonstration of visuals along with the OpenGAN-0 pix classification confidence scores as probabilities of being recognized as closed-set data. These visual results are generated under Setup-II, where the TinyImageNet is the closed-set for 200-way classification, and other datasets are treated as the open-set. The discriminator of the OpenGAN-0 pix is seleced over the CIFAR train-set. (a) The discriminator recognizes the closed-set training examples with a high confidence score. (b) OpenGAN-0 pix synthesizes fake images that look realistic in terms of color, tone and shape, but not content. The discriminator can easily recognize these fake images (as indicated by the low probability). The discriminator generalizes well in terms of recognizing closed-set examples from the validation and test sets as shown in (c) and (d), and open-set examples from other datasets as shown in (e), (f), and (h).", "figure_data": ""}, {"figure_label": "17", "figure_type": "figure", "figure_id": "fig_17", "figure_caption": "Figure 17 :17Figure 17: Qualitative results of a testing image from Cityscapes. [1 st row] the input image, its per-pixel semantic labels, the semantic segmentation result by HRnet and open-set pixels colored by white. [2 nd row] visual results as per-pixel scores of being classified as openset pixel by SoftMax, Entropy, C2AE and our OpenGAN-0 f ea . [3 rd row] visual results by our OpenGAN f ea and CLS, trained with 2900 or 10 open training images, respectively. [4 th row] visual results by thresholding OpenGAN-2900 with 0.6, 0.7. 0.8 and 0.9 respectively. OpenGAN clearly captures most open-set pixels (cf. the white pixels in top-right open-set map).", "figure_data": ""}, {"figure_label": "18", "figure_type": "figure", "figure_id": "fig_18", "figure_caption": "Figure 18 :18Figure 18: Qualitative results of a testing image from Cityscapes. [1 st row] the input image, its per-pixel semantic labels, the semantic segmentation result by HRnet and open-set pixels colored by white. [2 nd row] visual results as per-pixel scores of being classified as openset pixel by SoftMax, Entropy, C2AE and our OpenGAN-0 f ea . [3 rd row] visual results by our OpenGAN f ea and CLS, trained with 2900 or 10 open training images, respectively. [4 th row] visual results by thresholding OpenGAN-2900 with 0.6, 0.7. 0.8 and 0.9 respectively. OpenGAN clearly captures most open-set pixels (cf. the white pixels in top-right open-set map).", "figure_data": ""}, {"figure_label": "19", "figure_type": "figure", "figure_id": "fig_19", "figure_caption": "Figure 19 :19Figure 19: Qualitative results of a testing image from Cityscapes. [1 st row] the input image, its per-pixel semantic labels, the semantic segmentation result by HRnet and open-set pixels colored by white. [2 nd row] visual results as per-pixel scores of being classified as openset pixel by SoftMax, Entropy, C2AE and our OpenGAN-0 f ea . [3 rd row] visual results by our OpenGAN f ea and CLS, trained with 2900 or 10 open training images, respectively. [4 th row] visual results by thresholding OpenGAN-2900 with 0.6, 0.7. 0.8 and 0.9 respectively. OpenGAN clearly captures most open-set pixels (cf. the white pixels in top-right open-set map).", "figure_data": ""}, {"figure_label": "20", "figure_type": "figure", "figure_id": "fig_20", "figure_caption": "Figure 20 :20Figure 20: Qualitative results of a testing image from Cityscapes. [1 st row] the input image, its per-pixel semantic labels, the semantic segmentation result by HRnet and open-set pixels colored by white. [2 nd row] visual results as per-pixel scores of being classified as openset pixel by SoftMax, Entropy, C2AE and our OpenGAN-0 f ea . [3 rd row] visual results by our OpenGAN f ea and CLS, trained with 2900 or 10 open training images, respectively. [4 th row] visual results by thresholding OpenGAN-2900 with 0.6, 0.7. 0.8 and 0.9 respectively. OpenGAN clearly captures most open-set pixels (cf. the white pixels in top-right open-set map).", "figure_data": ""}, {"figure_label": "21", "figure_type": "figure", "figure_id": "fig_21", "figure_caption": "Figure 21 :21Figure 21: Qualitative results of a testing image from Cityscapes. [1 st row] the input image, its per-pixel semantic labels, the semantic segmentation result by HRnet and open-set pixels colored by white. [2 nd row] visual results as per-pixel scores of being classified as openset pixel by SoftMax, Entropy, C2AE and our OpenGAN-0 f ea . [3 rd row] visual results by our OpenGAN f ea and CLS, trained with 2900 or 10 open training images, respectively. [4 th row] visual results by thresholding OpenGAN-2900 with 0.6, 0.7. 0.8 and 0.9 respectively. OpenGAN clearly captures most open-set pixels (cf. the white pixels in top-right open-set map).", "figure_data": ""}, {"figure_label": "22", "figure_type": "figure", "figure_id": "fig_22", "figure_caption": "Figure 22 :22Figure 22: Qualitative results of a testing image from Cityscapes. [1 st row] the input image, its per-pixel semantic labels, the semantic segmentation result by HRnet and open-set pixels colored by white. [2 nd row] visual results as per-pixel scores of being classified as openset pixel by SoftMax, Entropy, C2AE and our OpenGAN-0 f ea . [3 rd row] visual results by our OpenGAN f ea and CLS, trained with 2900 or 10 open training images, respectively. [4 th row] visual results by thresholding OpenGAN-2900 with 0.6, 0.7. 0.8 and 0.9 respectively. OpenGAN clearly captures most open-set pixels (cf. the white pixels in top-right open-set map).", "figure_data": ""}, {"figure_label": "23", "figure_type": "figure", "figure_id": "fig_23", "figure_caption": "Figure 23 :23Figure 23: Qualitative results of a testing image from Cityscapes. [1 st row] the input image, its per-pixel semantic labels, the semantic segmentation result by HRnet and open-set pixels colored by white. [2 nd row] visual results as per-pixel scores of being classified as openset pixel by SoftMax, Entropy, C2AE and our OpenGAN-0 f ea . [3 rd row] visual results by our OpenGAN f ea and CLS, trained with 2900 or 10 open training images, respectively. [4 th row] visual results by thresholding OpenGAN-2900 with 0.6, 0.7. 0.8 and 0.9 respectively. OpenGAN clearly captures most open-set pixels (cf. the white pixels in top-right open-set map).", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "outlier data as open-training examples. In this setting, simply train-ing a binary open-vs-closed classifier works surprisingly well. However, such classifiers easily overfit to the available set of open-training data and generalize poorly, e.g., in a \"cross-dataset\" setting where open-set testing data differs from open-training data [57]. It appears fundamentally challenging to collect outlier data to curate an exhaustive training set of open-set examples. Our approach, Open-GAN, attempts to address this issue by augmenting the training set with adversarial fake open-training examples.", "figure_data": "3. OpenGAN for Open-Set RecognitionGenerally, solutions to open-set recognition contain two steps: (1) open-set discrimination that classifies testing ex-amples into closed and open sets based on the open-set like-lihoods, and (2) K-way classification on closed-set from step (1) [54, 5, 44]. The core problem to open-set recogni-tion is the first step, i.e., open-set discrimination. Typically, open-set discrimination assumes that open-set examples are not available during training"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "MSP MSPc MCdrop GDM OpenMax GOpenMax OSRCI C2AE CROSR RPL Hybrid GDFR NN pix NN f ea", "figure_data": "OpenGAN OpenGANDataset MNIST SVHN CIFAR TinyImgNet .577 .713 [30] [36] .977 .985 .886 .891 .757 .808[20] .984 .989 [35] .884 .866 .732 .752 .675 .712[6] .981 .894 .811 .576[21]  *  .984 .896 .675 .580[42]  [50] .988 .989 .991 .996 .995 -.931 .910 .922 .899 .968 .947 .935 .534 .699 .895 .883 .901 .950 .807 .544 .586 .748 .589 .809 .793 .608 .528[50] .981 .888 .801 .692-0 pix .987 .881 .971 .795-0 f ea .999 .988 .973 .907lihood. Likelihoods. Many methods compute open-set likeli-hood on OTS features, including Max Softmax Probability (MSP)"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Our technical insights apply to other GAN-based open-There are many other GAN-based open-set methods, such as training BiGANs [56, 65, 66] or adversarial autoencoders [48, 53] on raw images, and using the reconstruction error as open-set likelihood [56, 53, 66, 1, 16]. We show our technical insights apply to different GAN architectures for open-set recognition: (1) using GAN-discriminator as the open-set likelihood function instead of pixel reconstruction errors, and (2) training them on OTS features rather than raw pixels. We hereby analyze a typical BiGAN-based method", "figure_data": "set discrimination methods: 1) using BiGAN-discriminator as theopen likelihood function works better than using reconstructionerrors (BiGAN f ea d on OTS features works much better than pixels (BiGAN f ea vs. BiGAN f ea r ), and 2) learning BiGANs vs. d BiGAN pix d ). The results are comparable to Table 7.dataset MNISTBiGAN pix r .976BiGAN f ea r .998BiGAN pix d .986BiGAN f ea d .999SVHN.822.976.880.993CIFAR.924.967.968.973"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": ") for ROC curves, and F1-scores vs. thresholds on the open-set likelihood. GAN pix GAN f ea CIFAR AUROC .769 .000 .669 .011 .927 .000 .961 .013 .767 .020 .791 .007 .809 .005 .961 .007 .754 .367 .880 .091 .928 .113 .981 .027 .980 .011 F1 .548 .002 .507 .001 .525 .000 .544 .002 .564 .002 .553 .003 .564 .001 .519 .003 .545 .032 .558 .017 .555 .027 .563 .035 .585 .003 SVHN AUROC .695 .000 .691 .014 .994 .000 .990 .016 .657 .018 .863 .013 .783 .009 .999 .006 .701 .224 .948 068 .955 .052 .980 .014 .991 .013 F1 .567 .002 .551 .002 .545 .000 .574 .002 .565 .001 .572 .002 .572 .001 .575 .002 .572 .027 .564 .015 .578 .014 .574 .009 .583 .008 MNIST AUROC .764 .000 .690 .019 .901 .000 .964 .021 .755 .008 .832 .017 .801 .009 .957 .007 .986 .327 .944 .015 .961 .083 .983 .068 .989 .014 F1 .559 .001 .536 .013 .553 .000 .547 .008 .575 .001 .564 .001 .563 .001 .552 .002 .565 .020 .586 .021 .583 .010 .569 .016 .582 .005 Citysc. AUROC .789 .000 .693 .021 .715 .000 .867 .016 .814 .010 .851 .003 .868 .003 .513 .005 .646 .332 .971 .050 .828 .032 .933 .026 .978 .013 F1 .579 .002 .514 .002 .583 .000 .572 .003 .589 .002 .583 .001 .571 .001 .546 .003 .589 .007 .561 .029 .587 .006 .588 .007 .587 .000 We use t-SNE to visualize the embedding space through the OTS features computed by the K-way network trained on TinyImageNet train-set. Images from the other datasets are open-set examples. Clearly, closed and open examples are well separated in the feature space. We further visualize the \"landscape\" of the OpenGAN f ea open-set discriminator, by (b) projecting the OTS features into 2D using PCA; (c) coloring them with their closed/open labels; (d) rendering them with their open-set likelihoods computed by OpenGAN f ea ; (e) smoothing with Gaussian Filtering overlaid with the OpenGAN's decision boundary. We further compare OpenGAN f ea (tuned on SVHN) and MSP in (f-g) for open-set discrimination by ROC curves, and in (h-i) for open-set recognition by curves of the F1-score vs. thresholds of open likelihold. We render the density of open and closed testing data using shadows in (g) and (i). In these plots, we use each of the four cross-dataset open-test-sets (unseen in training) as an independent open-set to draw the curves. The curves clearly show that OpenGAN significantly outperforms MSP on open-set discrimination (AUROC) and open-set recognition (F1). ased\" protocol introduced in [57], which uses three datasets for benchmarking that reduces dataset-level bias [60]. This protocol tests the generalization of open-set methods to diverse open testing examples. Datasets. We use TinyImageNet as the closed-set for K-way classification (K=200). Images of each class are split into 500/50/50 images as the train/val/test sets. Following [57], we construct open train/val and test sets using different datasets [60], including MNIST (MN), SVHN (SV), CIFAR (CF) and Cityscapes (CS). For example, we use MNIST train-set to tune/train a model, and test it on CI-FAR test-set as open-test set. This allows for analyzing how open-set methods generalize to diverse open testing examples (cf. Table", "figure_data": "MSP OpenMax NN f ea GMMC2AEMSPc MCdrop GDM CLS pix (K+1) CLS f ea OpenOpenopen-test metric[30][6][50][33][44][36][20][35]averageAUROC .754 F1 .560.686 .527.884 .552.945 .559.748 .569.834 .568.815 .567.857 .548.772 .568.936 .565.918 .576.969 .573.984 .584(a) t-SNE visualization of features@penultimate layer(b) 2D PCA(d) rendered by confidence(f) OpenGAN : ROC curve fea(h) MSP: ROC curveTrue Positive Rateopen-set: SVHN (SV) open-set: Cityscapes (CS) open-set: CIFAR (CF) open-set: MNIST (MN)True Positive Rateopen-set: SVHN (SV) open-set: Cityscapes (CS) open-set: CIFAR (CF) open-set: MNIST (MN)closed-set: TinyImageNetFalse Positive RateFalse Positive Rateopen-set: CIFAR open-set: Cityscapes(c) closed-vs-open(e) landscape by smoothingmeasure(g) OpenGAN : F1 & density feameasure(i) MSP: F1 & densityopen-set: MNIST open-set: SVHNmacro F1open-set: CF+SV+MN+CS closed-set: TinyImageNetmacro F1open-set: CF+SV+MN+CS closed-set: TinyImageNetFigure 3: (a)threshold on open-set likelihoodthreshold on open-set likelihood"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": ". We conjecture that OpenMax cannot effectively recognize cross-dataset open-set examples represented by logit features (computed by the K-way network) which are too invariant to be adequate for open-set recognition. Moreover, the (K+1)-way classifier also works quite well, even outperforming the open-vs-closed binary classifiers (CLS) in AUROC. Next we analyze why the binary classifier CLS (as widely done since", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Diagnostic analysis for cross-dataset open-set discrimination measured by AUROC\u2191. In this setup, the TinyImageNet train/val/test sets serve as the closed train/val/test sets, and open train/test sets are the other two different datasets. Following outlier exposure [30], we train/tune CLS and OpenGAN on a cross-dataset as the open train-set. Recall that we do not train OpenGAN-0 on any open examples, although we tune it on the respective cross-dataset open train-set. CLS and OpenGAN use their last-epoch checkpoints to report performance. For better comparison, we report the average AUROC performance across all open-val-sets in the last column. We color the entries that have AUROC <0.9 with red, implying these models overfit to the open-train-set and generalize poorly on the other open-test-set. OpenGAN f ea clearly performs the best; while CLS (esp. CLS pix which operates on pixels) generalizes poorly. Perhaps surprisingly, OpenGAN-0 performs equally well although it does not train on open taining data. .999 .999 .101 .895 .935 .999 .453 .972 .411 .340 .999 .113 .317 .512 .100 .999 .634 OpenGAN-0 pix .999 .998 .550 .999 .999 .999 .993 .999 .999 .968 .999 .911 .999 .999 .915 .999 .958 OpenGAN pix .999 .999 .989 .933 .974 .999 .997 .967 .976 .998 .999 .835 .967 .928 .950 .999 .969 CLS f ea .999 .933 .916 .699 .940 .999 .979 .863 .893 .961 .999 .781 .881 .926 .949 .968 .918 OpenGAN-0 f ea .999 .998 .997 .999 .964 .996 .996 .946 .952 .992 .994 .934 .994 .995 .992 .997 .984 OpenGAN f ea .999 .999 .990 .973 .974 .999 .996 .971 .976 .998 .999 .967 .973 .968 .970 .999 .984 Further Analysis. Table 4 lists detailed results of Open-GAN, CLS (\u03bb G =0 in Eq. 2) and OpenGAN-0 (\u03bb o =0 in Eq. 2), when trained/tuned and tested on different crossdataset open-set examples. All methods perform better on OTS features than pixels (cf. CLS f ea vs. CLS pix); and work almost perfectly when trained and tested with the same open-set dataset, e.g., column-cf under \"CIFARtrain (cf)\" where we use CIFAR images as the open-set data. However, when tested on a different dataset of open-set examples, CLS performs quite poorly (especially when built on pixels) because it overfits easily to highdimensional pixel images [57]. In contrast, with fake opendata generated adversarially, OpenGAN and its special form OpenGAN-0 perform and generalize much better. Nevertheless, this implies a failure mode of OpenGAN, because the open-set data used in training could be quite different from those in testing, potentially leading to an OpenGAN that perform poorly in the real open world. Perhaps surprisingly, OpenGAN-0 f ea performs as well as OpenGAN f ea , although it does not train on open-set data. This further shows the merit of generating fake open examples to augment heavily-biased open-set training data, and our technique insights (as previously analyzed under Setup-I): 1) using GAN-discriminator as the likelihood function, and 2) training GANs on OTS features rather than pixels.", "figure_data": "open-val-setCIFAR10 (CF)SVHN (SV)MNIST (MN)Cityscapes (CS)avg.open-test-setCFSVMNCSCFSVMNCSCFSVMNCSCFSVMNCSCLS"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Comparison in open-set semantic segmentation on Cityscapes (AUROC \u2191). All methods are implemented on top of the segmentation network HRNet", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "HRNet-(K+1) OpenGAN-0 f ea CLS f ea OpenGAN f ea Qualitative results of two testing images, on which a state-of-the-art network (HRNet) misclassifies the unknown categories stroller/street-shop as motorcycle/building. From left to right of each row: the input image, its per-pixel semantic labels (in which white regions are open-set pixels), the semantic segmentation result by HRNet, open-set likelihoods by Entropy, our OpenGAN f ea , and its thresholded open-pixel map (threshold=0.7). OpenGAN clearly captures most open-set pixels (the white ones). Note that the street-shop is a real open-set example because Cityscapes train-set does not have another street-shop like this size and content (i.e., selling clothes). This confirms the effectiveness of Outlier Exposure, even with a modest amount of outliers[31].Bayesian networks (MCdrop and MSP c ) outperform the baseline MSP, showing that uncertainties can be reasonably used for open-set recognition. Lastly, we train a \"groundup\" (K+1)-way HRNet model that treats \"other\" pixels as the (K+1) th background class[19], shown by HRNet-(K+1) in Table5. It performs better than other typical openset methods but much lower than the simple open-vs-closed binary classifier CLS f ea , presumably because the (K+1)-", "figure_data": ".721.697.751.722.755.767.743.765.755.709.861.885Figure 4: 0.9RealOpenGAN-0 pixOpenGAN-0 f ea0.8 0.7 AUROC0.60.5OpenGAN-0010110 number of open training images 2103"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "6.1. OpenGAN f ea architectureOpenGAN f ea consists of a generator and a discriminator. OpenGAN f ea is compact in terms of model size (\u223c2MB), because it adopts MLP network over OTS features which are low-dimensional (e.g., 512-dim vectors) compared to pixel images. The MLP architectures are described below \u2022 The MLP discriminator in OpenGAN f ea takes a D-dimensional feature as the input.", "figure_data": "Its architec-ture has a set of fully-connected layers (fc marked with input-dimension and output-dimension), Batch Normalization layers (BN) and LeakyReLU lay-ers (hyper-parameter as 0.2): fc (D\u219264 * 8),BN, LeakyReLU, fc (64 * 8\u219264 * 4),BN, LeakyReLU, fc (64 * 4\u219264 * 2),BN, LeakyReLU, fc (64 * 2\u219264 * 1), BN,LeakyReLU, fc (64 * 1\u21921), Sigmoid.\u2022 The MLP generator synthesizes a D-dimensional feature given a 64-dimensional random vec-tor: fc (64\u219264 * 8), BN, LeakyReLU,fc (64 * 8\u219264 * 4), BN, LeakyReLU,fc (64 * 4\u219264 * 2), BN, LeakyReLU,fc (64 * 2\u219264 * 4), BN, LeakyReLU, fc(64 * 4\u2192D), Tanh."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_11", "figure_caption": ". As a result, many methods also ignore them in training. Therefore, instead of introducing artificial open-set examples, we use the historically-ignored pixels in Cityscapes as the real open-set examples. We hereby describe in detail our configuration for open-set semantic segmentation setup and experiments on Cityscapes.", "figure_data": "els are ignored in Cityscapes benchmark. ground parking dynamic bridge rail track tunnel caravan trailer guard rail pole group 10 6 10 7 10 8 10 9 10 10 road build. veget. car sidewalk sky \"Other\" pixels are ignored in Cityscapes benchmark. pole person terrain fence wall traffic sign bicycle bus truck traffic light train rider motorcycle static ground parking dynamic bridge rail track tunnel caravan trailer guard rail pole group number of pixelsFigure 7: Cityscapes annotates a sizeable portion of pixels that do not belong to one of the K closed-set classes on which theCityscapes benchmark evaluates. As a result, many methods alsoignore them during training [61]. We repurpose these historically-ignored pixels as open-set examples that are from the (K+1) th\"other\" class, allowing for a large-scale exploration of open-setrecognition via semantic segmentation.1"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Batch Construction. To fully shuffle open-and closedset training pixels, we cache all the open-set training pixel features extracted from HRNet. We construct a batch consisting of 10,000 pixels for training OpenGAN f ea .", "figure_data": "so, weTo do\u2022 randomly sample a real image, run HRNet over it and randomly extract 5,000 closed-set training pixel fea-tures;\u2022 randomly sample 2,500 open-set training features from cache;"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Hyper-parameter tuning for open-set semantic segmentation on Cityscapes. Given a fixed number of open training images,", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "Open-set discrimination (Setup-I) measured by area under ROC curve (AUROC)\u2191. Numbers are comparable to Table1in the main paper. Recall that OpenGAN-0 does not train on outlier data (i.e., \u03bb0=0 in Eq. 2) and only selects discriminator checkpoints on the validation set. OpenGAN-0 f ea clearly performs the best, achieving the state-of-the-art.MSP Entropy OpenMax MSPc GOpenMax OSRCI MCdrop GDM GMM C2AE CGDL RPL-WRN OpenHybrid OpenGAN-0 f ea", "figure_data": "Dataset [30] [58][6][36][21][42][20] [35] [33] [44] [59][13][67](ours)MNIST .977 .988 SVHN .886 .895 CIFAR .757 .788.981 .894 .811.985 .891 .808.984 .896 .675.988 .910 .699.984 .989 .993 .989 .994 .884 .866 .914 .922 .935 .732 .752 .817 .895 .903.996 .968 .901.995 .947 .950.999 .988 .973"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "that our testing set contains real open-set examples never-before-seen in training; please refer to the caption for details. Then, we show more visual results in figures from 17 through 23. From these figures, we can see OpenGAN f ea captures most open-set pixels, outperforming the other methods notably.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "max D E x\u223cD closed [ log D(x)]+\u03bb o \u2022E x\u223cDopen [ log (1\u2212D(x))]", "formula_coordinates": [3.0, 51.77, 576.17, 234.59, 14.58]}, {"formula_id": "formula_1", "formula_text": "min G Ez\u223cN log (1 \u2212 D(G(z)))(1)", "formula_coordinates": [3.0, 367.43, 167.35, 177.68, 15.02]}, {"formula_id": "formula_2", "formula_text": "max D min G Ex\u223cD closed [ log D(x)] + \u03bbo \u2022 Ex\u223cD open [ log (1 \u2212 D(x))] + \u03bbG \u2022 Ez\u223cN [ log (1 \u2212 D(G(z)))](2)", "formula_coordinates": [3.0, 345.14, 281.09, 199.97, 40.55]}], "doi": ""}