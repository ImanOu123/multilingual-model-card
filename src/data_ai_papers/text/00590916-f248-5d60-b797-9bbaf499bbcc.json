{"title": "M4: Multi-Generator, Multi-Domain, and Multi-Lingual Black-Box Machine-Generated Text Detection", "authors": "Yuxia Wang; Jonibek Mansurov; Petar Ivanov; Jinyan Su; Artem Shelmanov; Akim Tsvigun; Chenxi Whitehouse; \u2020 Osama; Mohammed Afzal; Tarek Mahmoud; Toru Sasaki; Thomas Arnold; Alham Fikri; Nizar Habash; Iryna Gurevych; Preslav Nakov; Hugo Touvron; Thibaut Lavril; Gautier Izacard; Xavier Martinet; Marie-Anne Lachaux; Timoth\u00e9e Lacroix; Baptiste Rozi\u00e8re; Naman Goyal; Eric Hambro; Faisal Azhar; Aur\u00e9lien Rodriguez; Armand Joulin; Louis Martin; Kevin Stone; Peter Al- Bert; Amjad Almahairi; Yasmine Babaei; Nikolay Bashlykov; Soumya Batra; Prajjwal Bhargava; Shruti Bhosale; Dan Bikel; Lukas Blecher; Cristian Canton Ferrer; Moya Chen; Guillem Cucurull; David Esiobu; Jude Fernandes; Jeremy Fu; Wenyin Fu; Brian Fuller; Cynthia Gao; Vedanuj Goswami; An- Thony Hartshorn; Saghar Hosseini; Rui Hou; Hakan Inan; Marcin Kardas; Viktor Kerkez; Madian Khabsa; Isabel Kloumann; Artem Korenev; Singh Koura; Jenya Lee; Di- Ana Liskovich; Yinghai Lu; Yuning Mao; Xavier Mar- Tinet; Todor Mihaylov; Pushkar Mishra; Igor Moly- Bog; Yixin Nie; Andrew Poulton; Jeremy Reizen- Stein; Rashi Rungta; Kalyan Saladi; Alan Schel- Ten; Ruan Silva; Eric Michael Smith; Ranjan Sub- Ramanian; Ellen Tan; Binh Tang; Ross Taylor; Adina Williams; Jian Xiang Kuan; Puxin Xu; Zheng Yan; Iliyan Zarov; Yuchen Zhang; An- Gela Fan; Melanie Kambadur; Sharan Narang; Aure- Lien Rodriguez; Robert Stojnic", "pub_date": "", "abstract": "Large language models (LLMs) have demonstrated remarkable capability to generate fluent responses to a wide variety of user queries. However, this has also raised concerns about the potential misuse of such texts in journalism, education, and academia. In this study, we strive to create automated systems that can detect machine-generated texts and pinpoint potential misuse. We first introduce a large-scale benchmark M4, which is a multi-generator, multi-domain, and multi-lingual corpus for machine-generated text detection. Through an extensive empirical study of this dataset, we show that it is challenging for detectors to generalize well on instances from unseen domains or LLMs. In such cases, detectors tend to misclassify machine-generated text as human-written. These results show that the problem is far from solved and that there is a lot of room for improvement. We believe that our dataset will enable future research towards more robust approaches to this pressing societal problem. The dataset is available at https://github.com/mbzuai-nlp/M4.", "sections": [{"heading": "Introduction", "text": "Large language models (LLMs) are becoming mainstream and easily accessible, ushering in an explosion of machine-generated content over various channels, such as news, social media, questionanswering forums, educational, and even academic contexts. Recently introduced LLMs, such as Chat-GPT, GPT-4, LLaMA 2 (Touvron et al., 2023b), and Jais (Sengupta et al., 2023), generate remarkably fluent responses to a wide variety of user queries. The high quality of the generated texts makes them attractive for replacing human labor in many scenarios. However, this raises concerns regarding their potential misuse, e.g., to spread disinformation or to cause disruptions in the education system (Tang et al., 2023).\nSince humans perform only slightly better than chance when classifying machine-generated vs. human-written texts (Mitchell et al., 2023), we aim to facilitate the development of automatic detectors to mitigate the potential misuse of LLMs. In particular, we construct a diverse resource that could be used for training and testing various models for detecting machine-generated text (MGT).\nPrevious efforts in detecting MGT (i) focused on only one or two particular languages, typically only on English, (ii) used a single generator, e.g., just ChatGPT (Guo et al., 2023;Shijaku and Canhasi, 2023), (iii) leveraged fine-tuned LLMs for specific tasks, e.g., machine translation or text summarization (Shamardina et al., 2022), or (iv) considered only one specific domain e.g., news (Zellers et al., 2019;Macko et al., 2023). In contrast, here we encompass multiple languages, various LLMs, and several diverse domains, aiming to enable more general machine-generated text detection. Our dataset serves as the basis for SemEval-2024 Task 8 (Wang et al., 2024).\nOur contributions are as follows:\n\u2022 We construct M4: a large-scale multigenerator, multi-domain, and multi-lingual corpus for detecting machine-generated texts in a black-box scenario where there is no access to a potential generator or its outputs except for plain text.\n\u2022 We study the performance of automatic detectors from various perspectives: (a) different detectors across different domains for a specific LLM generator, (b) different detectors across different generators for a specific domain, (c) interactions of domains and generators in a multilingual setting, and (d) the performance of the detector on data generated from different time periods. From these experiments, we draw a number of observations, which can inform future research.\n\u2022 We release our data and code freely, and we plan to keep our repository constantly growing, adding new generators, domains, and languages over time. The remainder of the paper is organized as follows: Section 2 discusses related work. Section 3 describes the process of collecting the corpus from multiple generators (including davinci-text-003, ChatGPT, GPT4, Cohere, Dolly2, and BLOOMz), multiple domains (including Wikipedia, WikiHow, Reddit, QA, news, paper abstracts, and peer reviews), and multiple languages (Arabic, Bulgarian, Chinese, English, Indonesian, Russian, and Urdu) for machine-generated text detection. Section 4 presents the seven detectors we experiment with. Section 5 evaluates their performance across domains given a generator (ChatGPT or davinci) and across generators given a domain (arXiv or Wikipedia), as well as across different languages. Finally, Section 6 concludes and points to possible directions for future work.", "publication_ref": ["b28", "b20", "b26", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "White-Box vs. Black-Box Detection We categorize the detection strategies into black-box and white-box, contingent on the level of access to the LLM that is suspected to have generated the target text. White-box methods focus on zero-shot detection without any additional training overhead (Sadasivan et al., 2023). Some use watermarking techniques (Szyller et al., 2021;He et al., 2022;Kirchenbauer et al., 2023;Zhao et al., 2023) and others rely on the expected per-token log probability of texts (Krishna et al., 2022;Mitchell et al., 2023). Black-box detectors only need API-level access to the LLM (i.e., when only the generated text is available) and typically extract and select features based on training text samples originating from both human and machine-generated sources.\nIn this study, we focus on black-box techniques because they aim to solve the task for the more practical and general use case. However, we note that their effectiveness heavily depends on the quality and the diversity of the training corpus.\nRelated Corpora Recently, a growing body of research has concentrated on amassing responses generated by LLMs. TuringBench (Uchendu et al., 2021) comprises 200K human-and machinegenerated pieces of text from 19 generative models. However, it is outdated, as the most advanced model used in this research is GPT-3. Guo et al. (2023) collected the HC3 dataset, which consists of nearly 40K questions and their corresponding answers from human experts and ChatGPT (English and Chinese), covering a wide range of domains (computer science, finance, medicine, law, psychology, and open-domain). Shijaku and Canhasi (2023) gathered TOEFL essays written by examined people and such generated by ChatGPT (126 essays for each).\nThe RuATD Shared Task 2022 involved artificial texts in Russian generated by various language models fine-tuned for specific domains or tasks such as machine translation, paraphrase generation, text summarization, and text simplification (Shamardina et al., 2022). We pay more attention to zero-shot generations of LLMs, such as the subset of RuATD generated by ruGPT-3.\nIn general, previous studies have concentrated on detecting machine-generated texts in one or two languages, for a specific LLM such as ChatGPT, or within a single domain such as news (Zellers et al., 2019;Macko et al., 2023). Our work broadens this scope to include multiple languages and a variety of widely-used LLMs across different domains.\nBlack-box Detectors are usually binary classifiers based on three types of features: statistical distributions (Guo et al., 2023;Shijaku and Canhasi, 2023), e.g., GLTR-like word rankings (Gehrmann et al., 2019), linguistic patterns (such as vocabulary, part-of-speech tags, dependency parsing, sentiment analysis, and stylistic features), and factverification features (Tang et al., 2023). Classification models involve deep neural networks, such as RoBERTa (Guo et al., 2023), or more traditional algorithms, such as logistic regression, support vector machines, Na\u00efve Bayes, and decision trees.\nThere are also widely-used off-the-shelf MGT detectors, such as the OpenAI detector, 1 GPTZero, 2 and ZeroGPT. 3 According to the limited public information about them, these detectors are trained on collections of human-written texts and texts generated by various LLMs. For example, the training data of the OpenAI detector contains generations from 34 LLMs from various organizations, including OpenAI itself. For our M4 dataset, we selected a diverse set of state-of-the-art black-box methods and features, including one off-the-shelf detector.", "publication_ref": ["b23", "b27", "b8", "b12", "b32", "b14", "b20", "b29", "b26", "b31", "b26", "b6", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "The M4 Dataset", "text": "We gather human-written texts from a diverse range of sources across various domains and languages. For English we have Wikipedia (the March 2022 version), WikiHow (Koupaee and Wang, 2018), Reddit (ELI5), arXiv, and PeerRead (Kang et al., 2018), for Chinese we have Baike/Web QA question answering (QA), for Russian we have RuATD (Shamardina et al., 2022), for Arabic Wikipedia, and we use news for Urdu, Indonesian, and Bulgarian. Details about the data sources are provided in Appendix A.1 and A.2.\nFor machine generation, we prompt the following multilingual LLMs: GPT-4, ChatGPT, GPT-3.5 (text-davinci-003), Cohere, Dolly-v2 (Conover et al., 2023), and BLOOMz 176B (Muennighoff et al., 2022). The models are asked to write articles given a title (Wikipedia), abstracts given a paper title (arXiv), peer reviews based on the title and the abstract of a paper (PeerRead), news briefs based on a title (news), also to summarize Wikipedia articles (Arabic), and to answer questions (e.g., Reddit and Baike/Web QA). 4", "publication_ref": ["b13", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Collection", "text": "Prompt Diversity For each generator, we carefully designed multiple (2-8) prompts in various styles, aiming to produce diverse outputs that are more aligned to divergent generations in real-world application scenarios. For example, on simple domains of Wikipedia and WikiHow, two prompts are applied. For arXiv and Reddit, as well as for ChatGPT, we use five prompts and four prompts for PeerRead. We generate varying tones of responses with prompts such as answer the question (1) \"like I am five years old\"; (2) \"in an expert confident voice\"; (3) \"in a formal academic and scientific writing voice\"; etc. Table 7 in Appendix A gives some statistics about the prompts used to generate the data collection, and Table 8 shows the hyper-parameters for the various generators.\nData Cleaning Simple artifacts in MGTs, such as multiple newlines and bullet points, could assist detectors, as teir presence in the training data may discourage detectors from learning more generalized signals.\nTherefore, we performed minimal cleaning of the human-written and the machine-generated texts: (i) in a human-written WikiHow text, we removed multiple commas at the beginning of a new line (like \"\"\"\"\"\", we believe that ...\") and repeating newlines (\"\\n\\n\\n\\n\\n text begin \\n\\n\\n\\n\\n\"); (ii) in machine-generated WikiHow texts, we removed bullet points (as there were no bullet points in human-written texts); (iii) in human-written Wikipedia articles, we removed references (e.g., [1], [2]), URLs, multiple newlines, as well as paragraphs whose length was less than 50 characters; and (iv) in human-written arXiv abstracts, we removed newlines stemming from PDF conversion.\nQuality Control Unlike other tasks, where the data quality can be evaluated through the agreement between annotators over gold labels, we naturally obtain gold labels along with the collection of machine-generated texts. Therefore, we checked the data quality by randomly sampling 10-20 cases for each domain/generator and manually assessing the plausibility of generated texts. This can effectively circumvent incoherent, disorganized, and illogical generations that are easy to distinguish from human-written ones due to improper prompts or hyper-parameter settings of the generators (e.g., some generators repeat newly generated snippets to satisfy the minimum setup of new tokens). Moreover, in order to mimic human-written texts, we control the length of MGTs.\nIt should be highlighted that we did not pick examples. The quality control we exercised was model-level rather than example-level. We checked for cases where a model fundamentally failed, e.g., by generating visibly very bad output (e.g., very repetitive, English instead of foreign language output, etc.). This was very high-level checking (whether to keep a certain model in M4 or not); at the individual example level, we just checked whether the output had at least 1000 characters in length. Thus, we believe any biases that we might have introduced are minimal.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_12", "tab_13"]}, {"heading": "Statistics", "text": "The overall statistics about our M4 dataset for different tasks and languages are given in Table 1. We collected \u223c 147k human-machine parallel data in total, with 102k for English and 45k for other languages: 9k for Chinese, Russian, and Bulgarian; and 6k for Urdu, Indonesian, and Arabic respectively, in addition to over 10M non-parallel human-written texts.  Train, Dev, and Test Splits: For all languages and domains, given a generator (e.g., ChatGPT), we keep 500\u00d72 (500 human-written examples and 500 machine-generated texts) for development, 500\u00d72 for testing, and the rest for training (typically, 2000\u00d72, but in some cases a bit less).", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Data Analysis", "text": "We performed analysis of our dataset in terms of vocabulary richness at the n-gram level, as well as in terms of human performance on the task of detecting machine-generated content.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "N-gram Analysis", "text": "We compared the uni-gram and the bi-gram distributions of human-written vs. machine-generated texts and found that the former had a richer vocabulary than each of the six generators; see Table 9 in Appendix A.4 for detail. Dolly-v2 had the largest number of unique uni-and bi-grams, followed by davinci, ChatGPT, and BLOOMz, and Cohere had the least. The combination of all generators had comparable vocabulary to humans. When comparing across domains, we observed that Wikipedia, which covers a wide range of topics, contains the highest number of unique unigrams, followed by WikiHow and Reddit. In contrast, arXiv and PeerRead, which are specific to academic papers and peer reviews, exhibited fewer unique uni-grams and bi-grams. Within the same domain, we calculated the overlap of unique unigrams and bi-grams between human and machinegenerated texts. This overlap ranges in 20-35% for unigrams and in 10-20% for bi-grams. These variations can provide distinctive signals for black-box machine-generated text detection approaches.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_14"]}, {"heading": "Human Evaluation", "text": "From the Reddit and the arXiv (ChatGPT) test sets, for each domain, we sampled the first 50 (human, machine) pairs of texts and shuffled them into two groups, where two texts from the same pair would go in different groups. The annotators were then asked to focus on one group, which meant that they had to make a decision looking at each example individually, rather than having a pair of examples and deciding which one in the pair was humanwritten and which one was machine-generated (as some previous work did). This ensures a realistic scenario. For Reddit, we had 29 examples by humans and 21 by machines for group 1, and (21 human, 29 machine) for group 2; and (human:26, machine:24) for arXiv group 1, (human:24, machine:26) for arXiv group 2.\nWe had a total of six human annotators, who came from different countries and were native speakers of different languages. They were all proficient in English and all had NLP background: three PhD students, two MSc students, and 1 postdoc. Annotator 3 was an English native speaker who is also proficient in Arabic. Annotators 1 and 4 were Chinese native speakers, annotators 2 and 6 ware Russian native speakers, and annotator 5 was a Bulgarian native speaker.\nEach annotator made a guess about 17 unique examples for Reddit (finished by six annotators) and 25 examples for arXiv (finished by four). 5 The results are shown in Table 2. Interestingly, the English native speaker did not perform as well as some other annotators.  We can further see in Table 2 that annotator 4 performed much better than annotator 1, even though they were both Chinese native speakers; this may be because annotator 4 had better understanding of how LLM generations work. Moreover, annotator 6 was the best rater, and he was also the one who was very familiar with LLM generation mechanisms, achieving higher guessing accuracy than annotator 2.\nThus, the annotators' proficiency in English may affect the evaluation, but for equal language proficiency, the degree of understanding of the LLM generation styles or patterns will also impact the quality of the annotator's guess.\nOn average, the accuracy of the human guesses was 0.77 for Reddit and 0.72 for arXiv. This indicates that it is not easy for humans to detect machine-generated text, especially for nonnative English speakers who are not familiar with the ChatGPT generation patterns (e.g., annotators 1,3,5). Besides, it is harder to classify the texts from arXiv than from Reddit. This is consistent with the findings in Clark et al. (2021) We hypothesize that our human annotators depended less on content signals and more on stylistic cues when identifying MGT for the arXiv domain, which results in the accuracy disparity between the two domains. Overall, it is challenging for general readers to understand and to follow abstracts of academic papers, but it is much easier to read Reddit answers.\nWe further compared the human performance to an XLM-R detector fine-tuned on in-domain training data. The classifier achieved near-perfect accuracy across the two domains, outperforming all human annotators. These findings strongly indicate the potential for automated in-domain black-box detection.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": ["tab_3", "tab_3"]}, {"heading": "Detectors", "text": "We evaluated seven detectors; see Table 11 for their hyper-parameter settings.\nRoBERTa This detector is based on the pretrained RoBERTa model (Liu et al., 2019), which we fine-tuned to detect machine-generated texts.\nELECTRA We further fine-tuned ELEC-TRA (Clark et al., 2020). Its pre-training objective is more aligned with our MGT task: it was pretrained to predict whether a token in a corrupted input was replaced by a plausible alternative sampled from a small generator network.\nXLM-R We fine-tuned XLM-RoBERTa, a multilingual variant of RoBERTa (Conneau et al., 2019).", "publication_ref": ["b17", "b2", "b3"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Logistic Regression with GLTR Features", "text": "We trained a logistic regression model based on 14 GLTR features from (Gehrmann et al., 2019), which are based on the observation that most LLM decoding strategies sample high-probability tokens from the head of the distribution. Thus, word ranking information about an LLM can be used to distinguish machine-generated texts from human-written ones. We selected two categories of these features: (i) the number of tokens in the top-10, top-100, top-1000, and 1000+ ranks from the LM predicted probability distributions (4 features), and (ii) the Frac(p) distribution over 10 bins ranging from 0.0 to 1.0 (10 features). Frac(p) describes the fraction of probability for the actual word divided by the maximum probability of any word at this position.", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "Stylistic Features", "text": "We trained an SVM classifier based on stylistic features from (Li et al., 2014): (i) character-based features, e.g., number of characters, letters, special characters, etc., (ii) syntactic features, e.g., number of punctuation and function words, (iii) structural features, e.g., total number of sentences, and (iv) word-based features, e.g., total number of words, average word length, average sentence length, etc.   12 and 13)\nNEws LAndscape (NELA) We trained an SVM classifier using the NELA features (Horne et al., 2019), which cover six aspects: (i) style: the style and the structure of the article; (ii) complexity: how complex the writing is; (iii) bias: overall bias and subjectivity; (iv) affect: sentiment and emotional patterns; (v) moral: based on the Moral Foundation Theory (Graham et al., 2012); and (vi) event: time and location.\nGPTZero Finally, we used the GPTZero system without any adaptation. It was trained on a large diverse corpus of human-written and AI-generated texts, focussing on English. The system can analyze texts ranging from individual sentences to entire documents.", "publication_ref": ["b15", "b9", "b7"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Experiments and Results", "text": "In this section, we first describe our experiments, which come in three settings: (i) same generator, cross-domain evaluation, (ii) same domain, crossgenerator evaluation, and (iii) cross-lingual, crossgenerator evaluation. As mentioned in the previous section, we also experiment with GPTZero in a zero-shot setting, as it has not seen our data (even though it might have been trained on some domains involved in our data). We further discuss the evaluation results of these experiments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Same-Generator, Cross-Domain", "text": "Given a specific text generator, such as ChatGPT and davinci-003, we train a detector using data from one domain and evaluate it on the test set from the same domain (in-domain evaluation) and other domains (out-of-domain evaluation). The results are shown in Figure 1 and Tables 12 and 13.\nIn-domain detection is easy and can be done with very high accuracy, sometimes very close to a perfect score of 100%. This is especially the case for the RoBERTa detector, which reaches 100% accuracy for detecting ChatGPT-generated text on arXiv, 99.7% on Wikipedia, 99.7% on WikiHow, and 98.0% on PeerRead. The only dataset where the best score for the RoBERTa detector is achieved when training on a different domain is Reddit. We can further see that the results with davinci-003 show the same pattern: all in-domain evaluation scores are usually very high, approaching 100%. Other detectors also show high performance in the in-domain evaluation setting, but they usually overfit less to a particular domain. For example, the LR-GLTR detector shows only 79.6% accuracy on WikiHow when the davinci-003 generator was used, while the score for the RoBERTa-based detector exceeds 99%.\nThe best performance in the out-of-domain evaluation is often achieved by fine-tuning ELEC-TRA for the task. We attribute this to the specific pre-training objective of this model, which is based on the detection of replaced tokens. ELECTRA shows slightly lower performance than RoBERTa for the in-domain evaluation, but achieves huge improvements in the out-domain evaluation setting. For example, in the case of training on Wikipedia to detect davinci-003 on Reddit, the RoBERTa's performance is close to random guessing, while ELEC-TRA achieves 87.9% accuracy. Another strong approach for out-of-domain detection is LR-GLTR, which outperforms ELECTRA in some scenarios, such as detecting ChatGPT on the Wikipedia domain.  14 and 15) Out-of-domain detection might be hard. This is especially noticeable when training on arXiv and detecting artificial texts for Reddit or training on arXiv and detecting for Wikipedia. This is expected as these pairs of domains are very different. There are some domains that offer better generalization than others. The RoBERTa-based detector and the detector based on NELA features are the most vulnerable in this regard. RoBERTa overfits to the training domain, while the NELA features are not tailored to machine-generated text detection, but rather initiated for fake news detection.\nThe best training domain for out-of-domain generalization is Reddit. Training on Reddit ELI5 usually yields the best out-of-domain performance. Wikipedia is also often a good domain for training. Training on arXiv and PeerRead yields the worst generalization across other domains because the writing style of academic papers is very specific.\nThe most challenging domain for machinegenerated text detection is WikiHow, while Peer-Read is the easiest one.\nThe GPT-3.5 (davinci-003) generator is harder to detect than ChatGPT. Aggregating the results across all domains and both generators, we can see that the accuracy for ChatGPT is usually higher than that for davinci-003. This indicates that ChatGPT may leave more distinctive signals in generated texts than davinci-003.\nFeature Analysis. We conducted feature analysis of in-domain detectors using LIME (Ribeiro et al., 2016), and we found that detectors did not overfit to MGT artifacts and leveraged word distribution for classification. See Figure 4 in Appendix G for more detail.", "publication_ref": ["b22"], "figure_ref": ["fig_0"], "table_ref": ["tab_1", "tab_1"]}, {"heading": "Same-Domain, Cross-Generator", "text": "Given a specific domain, we train the detector using the training data from one generator and we evaluate it on the test data from the same and also from other generators. The accuracy on arXiv and Wikipedia is shown in Figure 2 (see Table 14 and 15 in Section D for precision, recall, and F1).\nRoBERTa performs the best among five detectors. It is the best on both arXiv (95.9%: average accuracy) and Wikipedia (99.4%), followed by LR-GLTR (84.0/80.7%), stylistic features (80.4/82.8%), and ELECTRA (72.5/76.6%); NELA features are the worst (73.7/64.3%). We can see that apart from the main diagonal, most scores for the detector using NELA features are around or lower than 50.0%, particularly on arXiv. This indicates that they are not suitable for distinguishing machine-generated and human-written texts. Moreover, the accuracy for Wikipedia is higher than for arXiv, especially for RoBERTa pre-trained using Wikipedia data. This suggests that arXiv is somewhat harder to detect than Wikipedia, and exposure bias on pre-training can impact a detectors' domain-specific performance.\nThe highest accuracy is for the same generator. Akin to the trend of cross-domain evaluation, training and testing using the same generator always yields the best accuracy for both arXiv and Wikipedia across the five detectors. Even for NELA, and detection over generations by BLOOMz, the accuracy mostly remains over 90.0. Performance drops substantially when the training and the test data are generated from different LLMs because of different distributions between the outputs of different generators.  BLOOMz-generated text is much different from ChatGPT, davinci, and Cohere. For all detectors in both arXiv and Wikipedia, BLOOMz shows the lowest cross-generator accuracy. Specifically, when training on BLOOMz and testing on other generators, or when training on other generators and testing on BLOOMz, it shows low recall (<0.5) for machine-generated texts. This means that there are many false negative examples, namely, many machine-generated texts are misclassified as human-written ones. Most accuracy scores are \u226450.0%, i.e., similar or even worse than a random guess. This indicates that the distribution of BLOOMz outputs is very different from the other three generators. We assume that this is because BLOOMz is primarily fine-tuned for NLP downstream data.\nMoreover, we found that, for all detectors, when training on Cohere, the accuracy for ChatGPT is comparable to the accuracy on Cohere itself, and similarly high accuracy occurs when training on ChatGPT and testing on Cohere. This suggests that ChatGPT and Cohere share some generative patterns.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": ["tab_1"]}, {"heading": "Zero-shot Evaluation: GPTZero", "text": "Table 3 shows that, from the perspective of the domain, GPTZero performs the best on Wikipedia, while the worst results are on arXiv where, for all generators, the F1 score is below 50%. From the perspective of generators, GPTZero shows the best performance on ChatGPT and the worst performance on BLOOMz. The recall for BLOOMz is close to 0% across all domains, which is consistent with the results for other detectors. GPTZero also demosntrated low performance for Dolly v2. GPTZero may have been trained on generations of ChatGPT and on data from domains such as Wikipedia and Reddit, thus showing remarkable scores for them. At the same time, zero-shot detection for unseen domains and generators poses a major challenge for GPTZero.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Multilingual Evaluation", "text": "In this section, we discuss the results for our multilingual experiments with the XLM-R detector across seven languages. For multilingual evaluation, we used ChatGPT and davinci-003 as generators. The results are shown in Table 4 (see Section E in the Appendix for more detail).\nWe constructed the English training, development, and test sets by combining English texts across all domains: Wikipedia, WikiHow, Reddit ELI5, arXiv, and PeerRead. Then, the All row refers to the combination of all training data in Arabic, Bulgarian, Chinese, English, Indonesian, Russian, and Urdu from the same generator. We aim to evaluate the performance of a detector over each monolingual test set from a single domain when fully leveraging the available training data, thus observing the benefits brought by the interaction of multiple languages and domains.\nWe can see in Table 4 that the best accuracy is achieved when training and testing on the same language and using the same generator, while when training on one generator and testing on another one, the highest scores tend to appear in the row of All, i.e., when using the training data for all languages, except for Bulgarian (training on Bulgarian is best, if we want to test on Bulgarian).\nWe can also see that it is difficult for XLM-R to detect machine-generated text in a language that it has never seen during training. For example, it struggles to detect Russian, Urdu, and Indonesian machine/human-generated text when it was not trained on them. Interestingly, XLM-R still demonstrates good performance for Arabic even when trained on English data only.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_9", "tab_9"]}, {"heading": "Time Domain Evaluation", "text": "LLMs are constantly improving over time. This raises the question of the robustness of detectors for the same generator across different time points. With this in mind, we compared ChatGPT output generated in March 2023 (from our M4 dataset) vs. September 2023 on the Reddit-ELI5 domain and using XLM-R as a detector, and the same prompts and questions as for the M4 dataset. The results are shown in Table 5, where we can see that the detector trained on the earlier version can effectively classify generations produced by the September 2023 version. This implies that a detector may remain effective even when applied to a newer generator trained using fresh data.   Table 6: Impact of text length on detection accuracy on arXiv using XLM-R.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_10"]}, {"heading": "Impact of Text Length", "text": "Finally, we investigated the impact of text length on detection accuracy. We truncated arXiv articles at the first 1,000, 500, 250, and 125 characters and compared the accuracy of XLM-R detectors trained and tested on such truncated articles for machine-generated content produced by ChatGPT.\nThe results are shown in Table 6. We can see that as the length decreases from 1,000 to 125, the accuracy drops by 4.5 points. This illustrates the negative impact of smaller text length on detection performance; more experiments on the arXiv and the Reddit datasets are presented in Figure 3 in the Appendix.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion and Future Work", "text": "We presented M4, a large-scale multi-generator, multi-domain, and multi-lingual dataset for machine-generated text detection. We further experimented with this dataset performing a number of cross-domain, cross-generator, cross-lingual, and zero-shot experiments using seven detectors.\nWe found that detectors struggle to differentiate between machine-generated and human-written texts if the texts come from a domain, a generator, or a language that the model has not seen during training. Our results show that the problem is far from solved and that there is a lot of room for improvement. We hope that our release of M4, which we make freely available to the community, will enable future research towards more robust approaches to the pressing societal problem of fighting malicious machine-generated text. We have already created an extension of M4 for SemEval-2024 Task 8 (Wang et al., 2024), 6 which features additional languages, domains, and three new task (re)formulations.\nIn future work, we plan to expand our M4 dataset continuously by introducing new LLM generators, by exploring different domains, by incorporating new languages, and by diversifying the range of tasks and prompts used. We believe that this is a good, practical way to keep the dataset up-to-date in response to the ongoing progress in LLMs. Our aim is to maintain a dataset that remains relevant as LLMs continue to evolve.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ethics and Broader Impact", "text": "Below, we discuss some potential ethical concerns about the present work.\nData Collection, Licenses, and User Privacy. Creating the M4 dataset does not involve scraping raw data from websites. Instead, we used preexisting corpora that have been publicly released and approved for research purposes, with clear dataset licenses, which are listed in Table 1. To the best of our knowledge, all included datasets adhere to ethical guidelines and minimize privacy concerns. Since the human-written data has already been published and made publicly available for research purposes, we see no additional privacy risks in releasing that data as part of our M4 dataset.\nThe human text components of M4 are publicly available and can be freely accessed and used for research purposes. However, researchers must acknowledge the original sources of the text and comply with the respective licensing terms.\nThe machine-generated text components of our M4 dataset are subject to the licensing terms of the underlying LLMs. For text generated using LLMs, researchers must comply with the respective licensing terms of those LLMs:\n\u2022 davinci-003, ChatGPT, GPT-4: no specific license. They welcome research publications related to the OpenAI API. 7\n\u2022 Dolly-v2: Apache 2.0 8\n\u2022 Cohere: no specific license. They point out that CUSTOMER RETAINS ALL OWNER-SHIP AND INTELLECTUAL PROPERTY RIGHTS IN AND TO CUSTOMER DATA. 9\n\u2022 BLOOMz: Apache 2.0 10\nPotential Biases We recognize the potential for biases in our M4 dataset, stemming from both the original human-written corpora and the Large Language Models (LLMs) used for generation. This is an important issue, and we put efforts to minimize such biases. However, we are aware that unethical usage of our dataset may still lead to biased applications: even if our original dataset was completely unbiased, external parties may extract a biased subset, which would be out of our control.\nHaving already realized these concerns, we have implemented the following measures: a. We provide comprehensive documentation about our M4 dataset, including detailed information about the sources of all human-written corpora, the generation process for obtaining the machine-generated text, including the full prompts and the measures we took to cleanse the output, and the potential biases that may exist. We believe that this transparency would allow researchers to understand the origins of the data and to make informed decisions about how to use it.\nb. We further acknowledge and transparently discuss these limitations and debiasing techniques that could be used to address these limitations. We hope that the strong emphasis on transparency in our methodology by explicitly stating the sources of human-written corpora and the generation processes for the corresponding machine-generated text could help clarify the dataset's origins and potential biases.\nRobustly Secure System The M4 dataset is intended for the development of detection systems to mitigate misuse, particularly in the context of malicious content generated using LLMs. While we encourage extensive and responsible use of the datasets to advance this critical area of research, we also emphasize the importance of adhering to the licensing terms of the original human-written corpora and the corresponding LLMs.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Limitations", "text": "In this section, we discuss some perceived limitations of our study.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "M4 Dataset Generalization and Biases", "text": "Generalization: Machine-generated outputs exhibit a high degree of sensitivity to the prompts. While our M4 dataset was collected with diverse prompts for a variety of generators, domains, and languages, to cover typical use cases of generators, it has limitations as a general resource, as it is neither sufficient to train a detector that can generalize well over all possible domains and generators, nor is it enough to act as a standard benchmark that can accurately evaluate the effectiveness of a detection method.\nUp-to-Date: Detecting machine-generated text is a very challenging task when we do not know in advance the potential generator and the domain: as our findings show, human-written and machinegenerated text cannot be distinguished in certain situations, e.g., we saw issues when using text generated by BLOOMz. Therefore, we regard M4 as a useful repository of machine-generated text for researchers who want to improve and to evaluate their detectors from multiple dimensions. Moreover, the LLMs are constantly evolving, and thus any dataset collected for machine-generated text detection can become outdated relatively fast. With this in mind, we have constantly been extending the M4 dataset (e.g., with a recent collection of GPT-4 responses), and we expect to grow our repository to enable better training and more up-to-date detectors.\nBias: Biases may exist in both human-written and machine-generated texts, and it is possible that our M4 dataset may be influenced by biases from human collection, thus affecting the detection outcomes. We leave the analysis of such biases to our future work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Feasibility of Black-Box Machine-Generated Text Detection", "text": "A growing body of work shows that machinegenerated text detection might gradually become harder and even nearly impossible: as LLMs evolve, the gap between machine-generated and human-written text might narrow (Tang et al., 2023;Sadasivan et al., 2023). Liang et al. (2023) further suggested that GPT detectors are biased against non-native English writers. These findings continue to release unpromising signals for black-box detection approaches. Yet, alternatives such as watermarking or white-box methods remain impractical for proprietary LLMs, where general users and practitioners cannot access the model-internal parameters. Current black-box approaches may be less effective and may demonstrate poor generalization for unseen domains, generators, and languages; however, this reveals the need to study more general methods to improve the detection of the potential misuse cases of LLMs.", "publication_ref": ["b28", "b23", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Appendix A Data Collection and Analysis", "text": "A.1 English Corpora Wikipedia We use the Wikipedia dataset available on HuggingFace and randomly choose 3,000 articles, each of which surpasses a character length limit of 1,000. We prompt LLMs to generate Wikipedia articles given titles, with the requirement that the output articles should contain at least 250 words. For generation with Dolly-v2, we set the minimum number of generated tokens to be 300 to satisfy the minimal character length of 1,000.\nReddit ELI5 dataset (Fan et al., 2019) is a collection of English question-answering (QA) pairs, gathered to facilitate open-domain and long-form abstractive QA. The data is derived from three categories: ExplainLikeimfive for general topics, AskScience for scientific queries, and AskHistorians for historical inquiries. Each pair is composed of a question (a title + a detailed description) and corresponding answers.\nWe filtered out answers with less than 1,000 characters, retaining questions whose title ends with a question mark without detailed descriptions. Finally, we selected 1,000 QA pairs with top user ratings for each category, resulting in a total number of 3,000 pairs. We have to note that most recently, Reddit changed the terms, we are actively investigating how to deal with Reddit. We will delete it from the repository and the paper, if we are not allowed to use it after the discussion. What should be highlighted is that we started using it before the license changed.\nWikiHow dataset (Koupaee and Wang, 2018) is built from the online WikiHow knowledge base. It consists of articles with a title, a headline (the concatenation of all bold lines of all paragraphs), and text (the concatenation of all paragraphs except the bold lines). We randomly chose 3,000 articles with the length of more than 1,000 characters and prompted LLMs with titles and headlines to generate artificial articles.\nPeerRead Reviews We sampled 586 academic papers published in top-tier NLP and machine learning conferences from the PeerRead corpus (Kang et al., 2018). Each paper contains metadata, including title, abstract, and multiple human-written reviews. Given a paper, we prompt LLMs to generate peer reviews with four different instructions; two depend only on the title and another two involve both the title and the abstract. Two prompts specify the review format of first describing what problem or question the considered paper addresses, and then providing its strengths and weaknesses. Other two prompts do not contain a review format specification. 11 This results in 584 \u00d7 4 = 2,344 machine-generated texts for each generator and 5,798 human-written reviews in total.\nArxiv Abstract parallel dataset is constructed from a Kaggle corpu. We sample 3,000 abstracts with a minimum length of 1,000 characters and prompt LLMs to produce machine-generated abstracts based on their titles.", "publication_ref": ["b5", "b13", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Corpora in Other Languages", "text": "Arabic Wikipedia. Similarly to English Wikipedia, we randomly selected 3,000 Arabic articles with a length exceeding 1,000 characters and prompted the LLMs to generate artificial articles based on their titles.\nBulgarian True & Fake News is sampled from the Hack the Fake News datathon organized in 2017 by the Data Science Society in Bulgaria. It is a mixture of real and fake news. The human partition consists of 3,000 news articles with a length of more than 1,000 characters. Machine-generated texts are obtained by prompting LLMs with titles of human-written articles.\nChinese QA is constructed from 3,000 (question, answer) pairs sampled from Baike and the Web QA corpus. The length of each answer is more than 100 Chinese characters. We prompt LLMs with a combination of a brief title and a detailed description for each question.\nIndonesian News 2018 is constructed from a corpus of Indonesian news articles collected from seven different news websites in 2018. We picked news from CNN Indonesia since this source was found to provide the cleanest data. We selected 3,000 texts from the corpus and generated artificial news articles by prompting ChatGPT with a title.\nRussian RuATD is sourced from the RuATD Shared Task 2022 (Shamardina et al., 2022) devoted to artificial text detection in Russian. Shamardina et al. (2022) gathered a vast human and machine-generated corpora from various text generators. However, these generators are either task-specific or domain-specific. We leverage their human-written texts collected from publicly available resources and re-generate the machine-authored data using the open-domain state-of-the-art multilingual LLMs. The data involves six domains: (1) texts of different historical periods, (2) social media posts, (3) normative Russian, (4) web texts, (5) subtitles, and (6) bureaucratic texts with a complex discourse structure and various specific named entities.\nUrdu News is derived from Urdu News Data 1M -a collection of one million news articles from four distinct categories: Business & Economics, Science & Technology, Entertainment, and Sports. These articles were gathered from four reputable news agencies in Pakistan (Hussain et al., 2021). Each entry in this dataset includes a headline, a category, and a news article text. To ensure the data balance over four categories, we randomly sampled 750 news articles from each, resulting in 3,000 examples in total. Using the headlines as prompts, we generated the content of artificial news articles.", "publication_ref": ["b10"], "figure_ref": [], "table_ref": []}, {"heading": "A.3 LLM Generation", "text": "Prompt Diversity In terms of the prompt diversity, multiple (2-8) prompts are used to produce diverse outputs that are more aligned to divergent generations in real-world application scenarios.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Prompts of PeerRead", "text": "\u2022 Please write a peer review for the paper of + title;\n\u2022 Write a peer review by first describing what problem or question this paper addresses, then strengths and weaknesses, for the paper + title;\n\u2022 Please write a peer review for the paper of + title, its main content is as below: + abstract;\n\u2022 Write a peer review by first describing what problem or question this paper addresses, then strengths and weaknesses, for the paper + title, its main content is as below: + abstract.\nGenerator Hyper-parameters Table 8 shows hyper-parameters we set for various generators. In general, we follow the default setting, except for the length of new generations in order to satisfy the minimum character length of 1,000. We also prompted LLaMa (Touvron et al., 2023a) and FlanT5 (Chung et al., 2022), but removed all generations due to the poor quality.      ", "publication_ref": ["b0"], "figure_ref": [], "table_ref": ["tab_13"]}, {"heading": "A.4 N-gram Analysis", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.2 Computation Resources and Cost", "text": "We spent $600 on calling OpenAI APIs for ChatGPT and davinci-003 generations, $40 for calling GPTZero. Around 2,500 GPU hours were spent on Dolly-v2 and BLOOMz generation.\nC Results: Same-Generator, Cross-Domain  Table 17: Cross-language experiments. Accuracy (Acc) and F1 scores (for machine-generated class) based on XLM-R over test sets across different languages generated by davinci-003. We average performance across 5 runs (standard deviation in the parenthesis).", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "F Results: Impact of Text Length", "text": "Figure 3: Impact of text length on detection accuracy over arXiv and Reddit generated by ChatGPT, davinci and Cohere. With the character length decreasing from 1000 to 125 (by eight times), F1-score with respect to machine-generated text decreases for all subsets, demonstrating negative impacts of short text on the detection performance. Human Henry died in a joust against the captain of his Scottish Guard, Gabriel, the Count of Montgomery.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "H Examples of M4", "text": "The fateful run occurred at the end of a tournament day, after Montgomery had almost unhorsed the king. Henry instead on another tilt. Montgomery's lance struck the king's helmet and shattered, with a long splinter running through the king's visor through his eye and into or near his brain. The king initially survived injury, and was attended to by two of the most celebrated physicians in Europe, Ambroise Par\u00e9 and Andreas Vesalius. The queen, Catherine de Medici, ordered four prisoners executed with wood driven into their brains so that the physicians would have the chance to study the king's wound in detail on the corpses. Despite this rather extraordinary measure, the king deteriorated steadily. Vesalius' personal account is consistent with the development of meningitis or encephalitis. After 11 days, the king died. During these 11 days, Montgomery is supposed to have come to the king's side, and asked to have his head and right hand cut off in punishment. The king told him that he had jousted well and bravely and that the accident was not his fault. Following Henry's death, Catherine essentially ruled through a series of three of her sons. Montgomery retired to his estate in Normandy. From there, his history gets a bit complicated. He converted to Protestantism and took part in the religious fighting in Bourges and Rouen that followed the St. Bartholomew's Day Massacre in 1572. He escaped to England and remained there for about a year, later returning to attempt to lead a Huguenot insurrection in Normandy. He was captured by royal forces and executed in 1574. Sourced, in part, \"The Death of Henry II of France\" Journal of Neurosurgery 12/1992 Prompt I will ask you a question. For this question, provide me more than 200 words answer in an amateurish mediocre voice. Question: What happened to Henry II's jousting opponent after accidentally killing him in 1559?\nMachine Well, let me tell you, it was quite a tragic event for both Henry II and his opponent Gabriel de Montgomery. It all went down in 1559, during a jousting match at the Hotel des Tournelles in Paris. Henry was an experienced jouster, but Montgomery was a newcomer to the sport, and unfortunately, he was no match for the King's lance. During one of their runs, Henry's lance struck Montgomery's helmet, shattering it and sending a jagged piece of wood into his eye and brain. The young man was rushed to a nearby hospital, but it was too late. He died just a few days later, leaving behind a wife and children. As for Henry, he was devastated by the accident and reportedly went into a deep depression. He blamed himself for Montgomery's death and was haunted by guilt for the rest of his life. He even imposed a penance on himself, vowing to fast and do charitable works for the rest of his days. The incident also led to changes in the sport of jousting. After Montgomery's death, many rules were put in place to make the sport safer, such as requiring jousters to wear full face and head protection. Overall, it was a tragic event that had a lasting impact on the sport and on the King who inadvertently caused the death of his opponent.\nLanguage English LLM Generator CHATGPT Domain Arxiv", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Title Polymer Quantum Mechanics and its Continuum Limit", "text": "Human A rather non-standard quantum representation of the canonical commutation relations of quantum mechanics systems, known as the polymer representation has gained some attention in recent years, due to its possible relation with Planck scale physics. In particular, this approach has been followed in a symmetric sector of loop quantum gravity known as loop quantum cosmology.\nHere we explore different aspects of the relation between the ordinary Schroedinger theory and the polymer description. The paper has two parts. In the first one, we derive the polymer quantum mechanics starting from the ordinary Schroedinger theory and show that the polymer description arises as an appropriate limit. In the second part we consider the continuum limit of this theory, namely, the reverse process in which one starts from the discrete theory and tries to recover back the ordinary Schroedinger quantum mechanics. We consider several examples of interest, including the harmonic oscillator, the free particle and a simple cosmological model.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Field Content", "text": "Prompt Generate a 150-220-word abstract for work with title: Polymer Quantum Mechanics and its Continuum Limit. Describe the results obtained, problem work attempts to solve, and the key ideas and methodology in a formal academic and scientific writing voice. Use the first plural person form. Use active voice.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Machine", "text": "In this paper, we investigate the continuum limit of polymer quantum mechanics. The aim of our work is to better understand the quantum behavior of polymers and its relationship to the classical theory. Our research attempts to clarify the nature of the continuum limit and determine the correctness of the approximations made. Specifically, we study the connectivity of the polymer strands. We find that a continuous path can be defined in the polymer configuration space, which is essential for the polymer to display continuum behavior. We show that the approximation of continuous randomness in the spatial variable can certainly be used when the polymer is stretched out over a significant distance. For a coil-like polymer, our results demonstrate that its behavior is non-classical. Our methodology involves a careful combination of intuition, formalism, and numerical simulations. We evaluate a path integral representation of polymer quantum mechanics in the continuum limit, and use numerical simulations to support our results. The ideas proposed in this work are relevant to the understanding of the thermodynamic properties of polymers, as well as to quantum computing and information. The findings of this work present a theoretical framework for the investigation of other polymer properties that are still poorly understood.\nLanguage English LLM Generator DAVINCI-003 Domain Arxiv Title Effects of Dirac sea on pion propagation in asymmetric nuclear matter", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Human", "text": "We study pion propagation in asymmetric nuclear matter (ANM). One of the interesting consequences of pion propagation in ANM is the mode splitting for the different charged states of pions. First we describe the pion-nucleon dynamics using the non-chiral model where one starts with pseudoscalar (PS) \u03c0N coupling and the pseudovector (PV) representation is obtained via suitable non-linear field transformations. For both of these cases the effect of the Dirac sea is estimated. Subsequently, we present results using the chiral effective Lagrangian where the short-distance behavior (Dirac vacuum) is included by re-defining the field parameters as done in the modern effective field theory approach developed recently. The results are compared with the previous calculations for the case of symmetric nuclear matter (SNM). Closed form analytical results are presented for the effective pion masses and dispersion relations by making hard nucleon loop (HNL) approximation and suitable density expansion.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Prompt", "text": "Rephrase the abstract of an article with title 'Effects of Dirac sea on pion propagation in asymmetric nuclear matter'. 'We study pion propagation in asymmetric nuclear matter (ANM). One of the interesting consequences of pion propagation in ANM is the mode splitting for the different charged states of pions. First we describe the pion-nucleon dynamics using the non-chiral model where one starts with pseudoscalar (PS) \u03c0N coupling and the pseudovector (PV) representation is obtained via suitable non-linear field transformations. For both of these cases the effect of the Dirac sea is estimated. Subsequently, we present results using the chiral effective Lagrangian where the short-distance behavior (Dirac vacuum) is included by re-defining the field parameters as done in the modern effective field theory approach developed recently. The results are compared with the previous calculations for the case of symmetric nuclear matter (SNM). Closed form analytical results are presented for the effective pion masses and dispersion relations by making hard nucleon loop (HNL) approximation and suitable density expansion.'", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Machine", "text": "The propagation of pions in asymmetric nuclear matter (ANM) is studied in this article. We consider two approaches: a non-chiral model where the pseudoscalar/pseudovector coupling is determined using non-linear field transformations, and a chiral effective Lagrangian incorporating the short-distance behaviors known as the Dirac vacuum. In both cases, the consequences from these approaches on the mode splitting of the charged pions are evaluated. Using a hard nucleon loop approximation, accompanied by parameters deduced from a density expansion, closedform analytical results for the effective pion masses and dispersion relations are presented in comparison to calculations for symmetric nuclear matter.\nLanguage English LLM Generator COHERE Domain Wikipedia Title Shed (deity)\nHuman Shed was a deity from ancient Egyptian religion. Popularly called \"the Savior\", he is first recorded after the Amarna Period. Representing the concept of salvation, Shed is identified with Horus, particularly Horus the Child.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Field Content", "text": "Rather than have formal worship in a temple or as an official cult, he appears to have been a god that ordinary Egyptians looked to save them from illness, misfortune or danger. He is shown on the Metternich Stela as vanquishing danger in the form of a serpent, a scorpion and a crocodile. The rise of \"Savior\" names in personal piety during the Amarna period has been interpreted as the popular response of ordinary people to the attempts by Akhenaten to proscribe the ancient religion of Egypt. Shed has also been viewed as a form of the Canaanite god Resheph. Shed can be depicted as a young prince overcoming snakes, lions and crocodiles. Shed has been viewed as a form of savior, a helper for those in need when state authority or the king\u015b help is wanting. The increased reliance on divine assistance could even extend to saving a person from the Underworld, even to providing a substitute, and lengthening a person\u015b time in this world. In the New Kingdom Shed \"the savior\" is addressed on countless stelae by people searching or praising him for help.References Egyptian gods Savior gods Prompt Write a Wikipedia article with the title \"Shed (deity)\", the article should at least have 250 words Machine Shed (deity) Shed is a deity in the sqlrunen mythos. Shed is described as a giantess, a daughter of the primordial mother, Nott. In most traditions, Shed is a single entity, but in some traditions, Shed is a collective of giantesses. Shed is the mother of the first race of giants, the primordial beings of chaos and destruction. Shed mothered the giants through her mating with the primordial god, Odin. In some traditions, the giantess Gunnlod or Hyr is described as the mother of the giants, while Shed is described as the mother of the gods. Shed is the spouse of the primordial god, Odin. Odin and Shed had many offspring, including the gods, the giants, the Lamonts, the Maquas, the Frost Giants, and the Ice Giants. Shed is described as a beautiful giantess, with golden hair, a beautiful face, and a beautiful body. She is said to have worn a long golden gown and a gold crown. She is described as the most beautiful of all the giantesses. Shed is the goddess of fertility and reproduction. She is also the goddess of the harvest. In some traditions, Shed is also the goddess of war and death. Shed is the mother of the gods. She gave birth to the gods through her union with the primordial god, Odin. The gods were born from her body, and she nourished them with her milk. Shed is the protector of the gods. She is said to have fought alongside the gods in their war against the giants. She is also said to have fought against the dragons and the serpents. Shed is the goddess of the hunt. She is said to have accompanied the gods on their hunting expeditions. She is also said to have provided them with food and shelter. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Field Content", "text": "Prompt Please, generate wikihow article with length above 1000 characters from title 'How to Workaround Damaged Laptop Display Hinges' and headline 'Read the entire article, and check that your specific laptop can be repaired in the way presented., Cut two strips of the fuzzy Velcro and round the corners (rounding the corners will make it less likely that the Velcro will detach after gluing)., Read the instructions for the epoxy glue; mix it and use it to glue the two strips of fuzzy Velcro, one on the back of the display, and one on the bottom of the laptop, such that the two strips are co-linear (in other words, the alignment of one strip, when extended, coincides with the alignment of the other strip)., Wait for the epoxy glue to harden fully.,Close the laptop display, and cut a strip of the hooked Velcro more or less to the length required when sticking it to the two glued strips in their entirety., After the glue has hardened to maximum strength, check that the Velcro can now prevent the display from falling into the closed position., Take the fishing line, and tie it into a loop which exactly encircles the laptop when the display is open to the working position., Check that the fishing line loop is strong enough to prevent the display from falling into the fully open position (fishing line comes in a variety of strengths, use a relatively strong line which will not stretch under the load)., Glue a small strip of two-sided foam tape (like used for mounting things) on the edge of the laptop where the fishing line should rest., Optionally, you can use a plastic bag closure which has been notched (see images), in order to more easily adjust the exact size of the fishing line loop (and therefore, the exact working position of the laptop display).'\nMachine Have you ever experienced a damaged laptop display hinge? If so, you know how frustrating it can be when your display doesn't stay in the correct position. Fortunately, there's a workaround that you can use to fix this issue without having to replace the entire laptop hinge. This wikihow article will guide you through the process of using Velcro and fishing line to strengthen your laptop display hinges, keeping your display in the right position while you work. 1. Read the entire article, and check that your specific laptop can be repaired in the way presented. Before getting started with any repairs, it's important to read the entire article to make sure that this method will work for your laptop model. Some laptops may have different hinge designs that won't be compatible with the Velcro and fishing line method we're using here.\n2. Cut two strips of the fuzzy Velcro and round the corners.\nTo begin the repair process, you'll need to cut two strips of fuzzy Velcro and round the corners of each strip. This will make it less likely that the Velcro will detach after gluing.\n3. Glue the Velcro to the laptop display and bottom. Next, mix your epoxy glue according to the instructions, and use it to glue the two strips of fuzzy Velcro -one on the back of the display and one on the bottom of the laptop. Make sure that the two strips are co-linear, meaning that the alignment of one strip, when extended, coincides with the alignment of the other strip. 4. Wait for the glue to dry. After gluing the Velcro to the laptop, you'll need to wait for the epoxy glue to harden fully. This may take several hours, so be patient and allow the glue to dry completely. 5. Cut a strip of hooked Velcro to length.\nOnce the glue has dried, close your laptop display and cut a strip of hooked Velcro more or less to the length required when sticking it to the two glued strips in their entirety. 6. Check that the Velcro can prevent the display from falling into the closed position.\nAfter the glue has hardened to maximum strength, check that the Velcro can now prevent the display from falling into the closed position. If it's still loose, you may need to repeat steps 3-5 with stronger epoxy glue and larger strips of Velcro. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Abstract", "text": "Integrating text and knowledge into a unified semantic space has attracted significant research interests recently. However, the ambiguity in the common space remains a challenge, namely that the same mention phrase usually refers to various entities. In this paper, to deal with the ambiguity of entity mentions, we propose a novel Multi-Prototype Mention Embedding model, which learns multiple sense embeddings for each mention by jointly modeling words from textual contexts and entities derived from a knowledge base. In addition, we further design an efficient language model based approach to disambiguate each mention to a specific sense.\nIn experiments, both qualitative and quantitative analysis demonstrate the high quality of the word, entity and multi-prototype mention embeddings. Using entity linking as a study case, we apply our disambiguation method as well as the multi-prototype mention embeddings on the benchmark dataset, and achieve state-of-the-art performance.\nHuman -Strengths: Good ideas, simple neural learning, interesting performance (altough not striking) and finally large set of applications. -Weaknesses: amount of novel content. Clarity in some sections. The paper presents a neural learning method for entity disambiguation and linking. It introduces a good idea to integrate entity, mention and sense modeling within the smame neural language modeling technique. The simple training procedure connected with the modeling allows to support a large set of application. The paper is clear formally, but the discussion is not always at the same level of the technical ideas. The empirical evaluation is good although not striking improvements of the performance are reported. Although it seems an extension of (Yamada et al., CoNLL 2016), it adds novel ideas and it is of a releant interest.\nThe weaker points of the paper are: -The prose is not always clear. I found Section 3 not as clear. Some details of Figure 2 are not explained and the terminology is somehow redundant: for example, why do you refer to the dictionary of mentions? or the dictionary of entity-mention pairs? are these different from text anchors and types for annotated text anchors? -Tha paper is quite close in nature to Yamada et al., 2016) and the authors should at least outline the differences. One general observation on the current version is: The paper tests the Multiple Embedding model against entity linking/disambiguation tasks. However, word embeddings are not only used to model such tasks, but also some processes not directly depending on entities of the KB, e.g. parsing, coreference or semantic role labeling. The authors should show that the word embeddings provided by the proposed MPME method are not weaker wrt to simpler wordspaces in such other semantic tasks, i.e. those involving directly entity mentions. I did read the author's response.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Prompt", "text": "Write a peer review by first describing what problem or question this paper addresses, then strengths and weaknesses, for the paper: title", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Machine", "text": "The paper \"Bridge Text and Knowledge by Learning Multi-Prototype Entity Mention Embedding\" proposes a new method for utilizing external knowledge sources to improve entity detection and disambiguation in natural language processing tasks. The authors specifically address the issue of how to effectively incorporate large-scale knowledge bases such as Wikipedia into existing machine learning models. The strengths of this paper are its clear and concise explanation of the proposed methodology, use of established evaluation metrics, and consideration of potential limitations and future directions. The authors provide a detailed description of their multi-prototype entity mention embedding (MPEME) approach, which includes the use of different prototypes to represent entities with multiple meanings in Wikipedia. They evaluate their approach on two established datasets and demonstrate improved performance compared to baseline models. Additionally, the authors note potential limitations of their approach such as the requirement for large knowledge bases and potential bias in the selection of prototypes. One weakness of this paper is that it assumes knowledge bases such as Wikipedia are comprehensive and reliable sources of information. There is a growing body of research showing that knowledge bases can contain errors or biases, which may affect the accuracy of this approach. The authors acknowledge this issue but do not offer a solution, which limits the generalizability of their findings. Additionally, the paper provides limited discussion on how this approach can be scaled up for larger knowledge bases, which is an important consideration for practical applications.\nOverall, the paper provides a valuable contribution to the field by addressing an important challenge of incorporating external knowledge sources into NLP models. The proposed MPEME method shows promise for improving entity detection and disambiguation, but further research is needed to fully explore its potential and limitations. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers and the program committee chairs for their very helpful and insightful comments, which have helped us improve the paper.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Human", "text": "Especially check that the fishing line can be placed in a location which doesnt get too much in the way of your normal use (heavy users of the mouse buttons or touchpad might have problems here), and that there are locations more or less in the center of the laptop where gluing the Velcro will not affect the use of the laptop (that no access doors or ventilation holes are sealed). , Try to make the line of the two strips be as perpendicular as possible to the axis of the display hinges. , This can take 24 hours or even more. See the instructions for the particular epoxy you have used. Dont confuse the setting time of the epoxy (which can be as quick as 5 minutes) with the time until it reaches maximum strength.\n, Actually, what is more important is that the length of the hooked strip totally utilize the glued strips when the laptop display is open. Do not use the Velcro with an open display before the glue has hardened to its maximum strength. , Laptops with large or heavy displays may require gluing multiple strips of Velcro, or wider and/or stronger Velcro (Velcro comes in a range of sizes and bonding strengths). The laptop in the images has a 12.1\" display. , I used an \"improved blood knot\" for this. Instructions can be found in various places on the Internet. , If it is a bother, make the adhesive on the outside less sticky by touching it repeatedly.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Field", "text": "Content prompt Write a peer review by first describing what problem or question this paper addresses, then strengths and weaknesses, for the paper title, its main content is as below: abstract Machine In this paper, the authors tackle the problem of ambiguity in entity mentions when integrating text and knowledge into a unified semantic space. Specifically, they propose a Multi-Prototype Mention Embedding model that learns multiple sense embeddings for each mention by jointly modeling words from textual contexts and entities from a knowledge base. They also design an efficient language model-based approach to disambiguate each mention to a specific sense.\nThe authors evaluate their proposed model on a benchmark dataset for entity linking and report state-of-the-art performance.\nOverall, the paper presents a well-motivated, well-executed, and well-evaluated solution to a relevant research problem. The main contributions of the paper are the Multi-Prototype Mention Embedding model and the efficient language model-based disambiguation approach.\nThe authors clearly describe the model and how it addresses the problem of mention ambiguity.\nThe evaluation of the model is thorough, including both qualitative and quantitative analyses.\nOne of the strengths of the paper is the use of a benchmark dataset, which allows for fair comparison with previous approaches. Additionally, the authors provide detailed explanations of the implementation and evaluation of their model, making the paper easily replicable. The experiments demonstrate the high quality of the word, entity, and multi-prototype mention embeddings produced by the proposed model. However, there are a few weaknesses in the paper. While the evaluation of the proposed method is thorough and convincing, it would have been beneficial if the authors had provided more in-depth analysis of the results to show exactly where and why their method outperforms previous approaches. Additionally, the paper would have benefited from a more detailed discussion of the limitations and potential future directions of the proposed method.\nOverall, the paper is a strong contribution to the field of entity linking and the integration of text and knowledge into a unified semantic space. The proposed Multi-Prototype Mention Embedding model and the efficient language model-based disambiguation approach provide a promising solution to the problem of mention ambiguity.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Prompt", "text": "Generate an abstract for a Wikipedia article with a title: \"{title}\". Abstract: \\n Tambahkan \"Jakarta, CNN Indonesia -\" di awal artikel.\nMachine Jakarta, CNN Indonesia -Ahmad Dhani, personel grup band Dewa 19, mengaku tak memiliki informasi mengenai sosok Sri Bintang yang tengah ramai dibicarakan di media sosial. Seperti diketahui, Sri Bintang menjadi perbincangan publik setelah dirinya mengunggah video yang menyebutkan bahwa personel band Dewa 19, Ahmad Dhani, pernah menyebut Sri Bintang sebagai sosok yang berperan dalam pembuatannya dari awal. Klaim tersebut kemudian dibantah oleh Ahmad Dhani melalui sebuah video yang diunggahnya di akun media sosialnya. \"Saya sendiri nggak tahu siapa Sri Bintang itu. Jadi saya gak bisa bicara, saya gak bisa komen, kenapa orang lain mau ngomongin saya tentang Sri Bintang,\" ujar Ahmad Dhani dalam video tersebut. Ahmad Dhani juga mengatakan bahwa beberapa orang yang mengklaim tahu tentang perjalanannya dalam industri musik sebenarnya tidak memiliki informasi yang tepat. \"Saya sekali lagi ingin tegaskan bahwa ada beberapa orang yang bicara itu nggak tahu apa yang mereka bicarakan. Intinya, kita perlu memperhatikan orang yang kita ngomongin,\" tegasnya. Kendati begitu, Ahmad Dhani mengungkapkan bahwa dirinya berharap Sri Bintang bisa berkarya dan memperlihatkan kemampuan yang dimilikinya di industri musik tanah air. Ahmad Dhani juga berharap bahwa sosok Sri Bintang tidak menjadi obyek perdebatan yang tak berkesudahan di antara para penggemar musik dan publik di Indonesia. \"Setidaknya semua artis harus diberikan kesempatan yang sama untuk berkarya. Saya berharap Sri Bintang bisa berkarya dengan baik dan melebihi ekspektasi dari semua orang,\" harapnya. Meski begitu, klaim dari Sri Bintang mengenai Ahmad Dhani telah menjadi perhatian banyak pengguna media sosial. Banyak netizen yang menyatakan bahwa hal tersebut merupakan sebuah bukti bahwa industri musik di Indonesia selalu menarik perhatian masyarakat dan penuh dengan kontroversi. Maka dari itu, publik diharapkan mampu menghormati karya dari masing-masing musisi dan menghindari terjadinya perdebatan yang tak perlu di media sosial. ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "", "journal": "", "year": "2022", "authors": " Hyung Won; Le Chung; Shayne Hou; Barret Longpre; Yi Zoph; William Tay; Eric Fedus; Xuezhi Li; Mostafa Wang; Siddhartha Dehghani; Albert Brahma;  Webson; Shane Shixiang; Zhuyun Gu; Mirac Dai; Xinyun Suzgun; Aakanksha Chen; Sharan Chowdhery; Gaurav Narang; Adams Mishra; Vincent Y Yu; Yanping Zhao; Andrew M Huang; Hongkun Dai; Slav Yu; Ed H Petrov; Jeff Chi; Jacob Dean; Adam Devlin; Denny Roberts; Quoc V Zhou; Jason Le;  Wei"}, {"ref_id": "b1", "title": "All that's 'human' is not gold: Evaluating human evaluation of generated text", "journal": "Long Papers", "year": "2021", "authors": "Elizabeth Clark; Tal August; Sofia Serrano; Nikita Haduong; Suchin Gururangan; Noah A Smith"}, {"ref_id": "b2", "title": "Electra: Pre-training text encoders as discriminators rather than generators", "journal": "", "year": "2020", "authors": "Kevin Clark; Minh-Thang Luong; V Quoc; Christopher D Le;  Manning"}, {"ref_id": "b3", "title": "Unsupervised cross-lingual representation learning at scale. CoRR, abs", "journal": "", "year": "1911", "authors": "Alexis Conneau; Kartikay Khandelwal; Naman Goyal; Vishrav Chaudhary; Guillaume Wenzek; Francisco Guzm\u00e1n; Edouard Grave; Myle Ott; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b4", "title": "", "journal": "", "year": "", "authors": "Mike Conover; Matt Hayes; Ankit Mathur; Jianwei Xie; Jun Wan; Sam Shah; Ali Ghodsi; Patrick Wendell; Matei Zaharia"}, {"ref_id": "b5", "title": "ELI5: long form question answering", "journal": "Long Papers", "year": "2019-07-28", "authors": "Angela Fan; Yacine Jernite; Ethan Perez; David Grangier; Jason Weston; Michael Auli"}, {"ref_id": "b6", "title": "GLTR: Statistical detection and visualization of generated text", "journal": "", "year": "2019", "authors": "Sebastian Gehrmann; Hendrik Strobelt; Alexander Rush"}, {"ref_id": "b7", "title": "Moral foundations theory: The pragmatic validity of moral pluralism", "journal": "Advances in Experimental Social Psychology", "year": "2012", "authors": "Jesse Graham; Jonathan Haidt; Sena Koleva; Matt Motyl; Ravi Iyer; Sean Wojcik; Peter Ditto ; Biyang Guo; Xin Zhang; Ziyuan Wang; Minqi Jiang; Jinran Nie; Yuxuan Ding; Jianwei Yue"}, {"ref_id": "b8", "title": "Protecting intellectual property of language generation apis with lexical watermark", "journal": "AAAI Press", "year": "2022-02-22", "authors": "Xuanli He; Qiongkai Xu; Lingjuan Lyu; Fangzhao Wu; Chenguang Wang"}, {"ref_id": "b9", "title": "Robust fake news detection over time and attack", "journal": "ACM Transactions on Intelligent Systems and Technology (TIST)", "year": "2019", "authors": "Jeppe Benjamin D Horne; Sibel N\u00f8rregaard;  Adali"}, {"ref_id": "b10", "title": "Urdu news dataset 1m", "journal": "", "year": "2021", "authors": "Khalid Hussain; Nimra Mughal; Irfan Ali; Saif Hassan; Sher Muhammad Daudpota"}, {"ref_id": "b11", "title": "A dataset of peer reviews (PeerRead): Collection, insights and NLP applications", "journal": "Long Papers", "year": "2018", "authors": "Dongyeop Kang; Waleed Ammar; Bhavana Dalvi; Madeleine Van Zuylen; Sebastian Kohlmeier; Eduard Hovy; Roy Schwartz"}, {"ref_id": "b12", "title": "A watermark for large language models", "journal": "CoRR", "year": "2023", "authors": "John Kirchenbauer; Jonas Geiping; Yuxin Wen; Jonathan Katz; Ian Miers; Tom Goldstein"}, {"ref_id": "b13", "title": "Wikihow: A large scale text summarization dataset", "journal": "", "year": "2018", "authors": "Mahnaz Koupaee; William Yang Wang"}, {"ref_id": "b14", "title": "RankGen: Improving text generation with large ranking models", "journal": "", "year": "2022", "authors": "Kalpesh Krishna; Yapei Chang; John Wieting; Mohit Iyyer"}, {"ref_id": "b15", "title": "Authorship authentication using short messages from social networking sites", "journal": "IEEE Computer Society", "year": "2014-11-05", "authors": "Jenny S Li; John V Monaco; Li-Chiou Chen; Charles C Tappert"}, {"ref_id": "b16", "title": "Gpt detectors are biased against non-native english writers", "journal": "", "year": "2023", "authors": "Weixin Liang; Mert Yuksekgonul; Yining Mao; Eric Wu; James Zou"}, {"ref_id": "b17", "title": "Roberta: A robustly optimized bert pretraining approach", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b18", "title": "", "journal": "", "year": "", "authors": "Dominik Macko; Robert Moro; Adaku Uchendu; Jason Lucas; Michiharu Yamashita; Mat\u00fa\u0161 Pikuliak; Ivan Srba; Thai Le; Dongwon Lee; Jakub Simko"}, {"ref_id": "b19", "title": "MULTITuDE: Large-scale multilingual machine-generated text detection benchmark", "journal": "", "year": "2023", "authors": "Maria Bielikova"}, {"ref_id": "b20", "title": "Detectgpt: Zero-shot machine-generated text detection using probability curvature", "journal": "CoRR", "year": "2023", "authors": "Eric Mitchell; Yoonho Lee; Alexander Khazatsky; Christopher D Manning; Chelsea Finn"}, {"ref_id": "b21", "title": "Crosslingual generalization through multitask finetuning", "journal": "", "year": "", "authors": "Niklas Muennighoff; Thomas Wang; Lintang Sutawika; Adam Roberts; Stella Biderman; Teven Le Scao; M Bari; Sheng Shen"}, {"ref_id": "b22", "title": "why should i trust you?\" explaining the predictions of any classifier", "journal": "", "year": "2016", "authors": "Sameer Marco Tulio Ribeiro; Carlos Singh;  Guestrin"}, {"ref_id": "b23", "title": "Can ai-generated text be reliably detected?", "journal": "CoRR", "year": "2023", "authors": "Aounon Vinu Sankar Sadasivan; Sriram Kumar; Wenxiao Balasubramanian; Soheil Wang;  Feizi"}, {"ref_id": "b24", "title": "Osama Mohammed Afzal, Samta Kamboj, Onkar Pandit, Rahul Pal, Lalit Pradhan, Zain Muhammad Mujahid", "journal": "", "year": "", "authors": "Neha Sengupta; Sunil Kumar Sahu; Bokang Jia; Satheesh Katipomu; Haonan Li; Fajri Koto; William Marshall; Gurpreet Gosal; Cynthia Liu; Zhiming Chen"}, {"ref_id": "b25", "title": "", "journal": "", "year": "", "authors": "Tatiana Shamardina; Vladislav Mikhailov; Daniil Cherniavskii; Alena Fenogenova; Marat Saidov; Anastasiya Valeeva; Tatiana Shavrina; Ivan Smurov; Elena Tutubalina"}, {"ref_id": "b26", "title": "Chatgpt generated text detection", "journal": "", "year": "2023", "authors": "Rexhep Shijaku; Ercan Canhasi"}, {"ref_id": "b27", "title": "DAWN: dynamic adversarial watermarking of neural networks", "journal": "ACM", "year": "2021-10-20", "authors": "Sebastian Szyller; Samuel Buse Gul Atli; N Marchal;  Asokan"}, {"ref_id": "b28", "title": "The science of detecting llm-generated texts", "journal": "", "year": "2023", "authors": "Ruixiang Tang; Yu-Neng Chuang; Xia Hu"}, {"ref_id": "b29", "title": "TURINGBENCH: A benchmark environment for Turing test in the age of neural text generation", "journal": "", "year": "2001", "authors": "Adaku Uchendu; Zeyu Ma; Thai Le; Rui Zhang; Dongwon Lee"}, {"ref_id": "b30", "title": "Alham Fikri Aji, Nizar Habash, Iryna Gurevych, and Preslav Nakov. 2024. SemEval-2024 task 8: Multidomain, multimodal and multilingual machine-generated text detection", "journal": "", "year": "", "authors": "Yuxia Wang; Jonibek Mansurov; Petar Ivanov; Jinyan Su; Artem Shelmanov; Akim Tsvigun; Chenxi Whitehouse; Osama Mohammed Afzal; Tarek Mahmoud; Toru Sasaki; Thomas Arnold"}, {"ref_id": "b31", "title": "Defending against neural fake news", "journal": "", "year": "2019-12-08", "authors": "Rowan Zellers; Ari Holtzman; Hannah Rashkin; Yonatan Bisk; Ali Farhadi; Franziska Roesner; Yejin Choi"}, {"ref_id": "b32", "title": "Protecting language generation models via invisible watermarking. CoRR, abs/2302.03162. Table 19: Examples generated by BLOOMz across different domains. Field Content Domain Wikipedia Title Edgehill, King George County, Virginia Human Edgehill is an unincorporated community in King George County, Virginia, United States. The main roads in Office Hall are U.S. Route 301 and Virginia State Route 205. References Unincorporated communities in Virginia Unincorporated", "journal": "", "year": "2023", "authors": "Xuandong Zhao; Yu-Xiang Wang; Lei Li"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Accuracy of cross-domain experiments: given generations from ChatGPT (top) or davinci (bottom), train on a single domain and test across domains across five detectors. (see more detail in Tables12 and 13)", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Accuracy of cross-generator experiments: train and test on arXiv (top) and Wikipedia (bottom) across five detectors, over single machine-text generator vs. human. (see detail in Tables14 and 15)", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Statistics about our M4 dataset, which includes non-parallel human data and parallel human and machinegenerated texts.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ": Human evaluation on 100 examples from Reddit and arXiv (human, ChatGPT). The XLM-R de-tector fine-tuned on in-domain data demonstrated much better results than human annotators."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Arx iv Pee rRe ad Re dd it Wik iHo w Wik ipe dia", "figure_data": "Arxiv100.0 52.4 95.9 75.7 91.5 (a) RoBERTa90 100E(/(&75$F/5*/75G6W\\OLVWLFH1(/$PeerRead99.0 98.0 75.1 66.0 58.270 80WikiHow Reddit Train96.9 84.4 89.3 99.7 18.3 99.5 80.6 89.7 82.4 79.140 50 60Wikipedia55.6 60.7 48.7 48.2 99.720 30$U[ LY 3HH U5H DG 5H GGL W :LN L+R Z :LN LSH GLD$U[ LY 3HH U5H DG 5H GGL W :LN L+R Z :LN LSH GLD$U[ LY 3HH U5H DG 5H GGL W :LN L+R Z :LN LSH GLD$U[ LY 3HH U5H DG 5H GGL W :LN L+R Z :LN LSH GLDArxiv Reddit Train PeerRead53.3 98.7 51.9 50.2 51.6 52.4 91.2 93.6 88.1 42.8 99.4 26.3 54.4 55.3 55.560 70 80 9061.2 97.5 95.2 66.2 68.2 51.9 99.0 50.9 50.0 51.1 94.6 58.3 52.7 49.2 50.470 80 9053.4 90.2 88.5 76.0 86.7 50.2 94.6 86.3 73.5 84.5 85.2 71.2 45.1 50.2 47.170 80 90WikiHow WikipediaArx iv Pee rRe ad Re dd it Wik iHo w Wik ipe dia 95.0 31.7 58.6 99.4 46.4 74.8 56.7 49.0 47.8 99.630 50 4056.8 70.2 81.0 99.0 60.3 Arx iv Pee rRe ad Re dd it Wik iHo w Wik ipe dia 62.2 84.4 87.9 76.3 83.560 5072.6 76.0 77.7 79.6 88.2 71.5 72.7 68.2 73.5 90.3 Arx iv Pee rRe ad Re dd it Wik iHo w Wik ipe dia60 50Arx iv Pee rRe ad Re dd it Wik iHo w Wik ipe dia"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "ChatGPT 26.2 41.5 86.4 91.6 49.4 62.1 87.2 93.1 70.8 82.7 davinci 0.2 0.4 60.4 74.3 45.2 59.4 53.8 70.0 96.2 97.9 Cohere 18.6 31.4 30.2 44.5 68.0 77.9 69.0 81.7 84.4 91.3 Dolly v.2 5.4 10.3 52.8 66.7 13.6 21.1 29.4 45.4 18.6 31.3", "figure_data": "arXiv Rec F1 Rec Reddit F1 Rec WikiHow Wikipedia PeerRead F1 Rec F1 Rec F1BLOOMz 0.4 0.8 7.6 13.8 0.0 0.0 2.0 3.9 5.8 10.9"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Zero-shot detection with GPTZero: recall (Rec) and F1-score with respect to generators and domains.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Accuracy (%) based on XLM-R on test sets across different languages over ChatGPT and davinci-003.", "figure_data": "Test \u2192 Train \u2193 MarchMarch 2023 Acc Precision Recall F1 Acc Precision Recall F1 September 2023 99.5 99.0 100 99.5 99.4 99.0 99.8 99.4September 96.0 10092.0 95.8 99.5 99.0100 99.5"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Impact of ChatGPT update over time.", "figure_data": "Ac-curacy (Acc), Precision, Recall, and F1 scores(%) with respect to machine generations for Reddit from March 2023 and September 2023 ChatGPT generations based on XLM-R as a detector.250 96.8 96.4 94.5 125 Length \u2192 Full Length 1,000 500 Accuracy 99.0 98.9 Precision 98.2 97.8 94.2 94.4 92.5 Recall 99.8 100.0 99.8 98.6 96.8 F1 99.0 98.9 96.9 96.5 94.6"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Statistics about the prompts for different domains and LLMs. One prompt is used for non-English text, and multiple prompts are used for English. The number of prompts for different domains varies as shown in the last column. Given a domain, some models might not follow all designed instructions, leading to less variety of prompts.", "figure_data": "Source/ DomainLanguageDavinci003ChatGPTCohereGeneratorDolly-v2BLOOMzWikipediaEnglishmax_tokens=1000 max_tokens=1000 max_tokens=1000min_new_tokens=300, max_new_tokens=1000defaultReddit ELI5Englishdefaultdefaultdefaultmin_new_tokens=180 max_new_tokens=600min_new_tokens=180WikiHowEnglishmax_tokens=2000 defaultdefaultmin_new_tokens=200 max_new_tokens=1000min_new_tokens=200PeerReadEnglishdefaultdefaultdefaultdefaultmin_new_tokens=150arXiv abstractEnglishmax_tokens=3000 defaultdefaultmin_new_tokens=180 max_new_tokens=600min_new_tokens=180, max_new_tokens=420, repitition_penalty=1.15, length_penalty=10Baike/Web QAChinesedefaultdefault---RuATDRussianmax_tokens=1700 default---Urdu-newsUrdu-temperature=0---id_newspapers_2018Indonesian -default---Arabic-WikipediaArabic-default---Bulgarian True & Fake News Bulgarianmax_tokens=3000 default---"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Hyperparameters used to generate data. We only specify parameter values that are different from defaults.", "figure_data": "Domain\u2193Word (unigram) Human ChatGPT davinc-003 Cohere Dolly-v2 BLOOMzHumanbigrams ChatGPT davinc-003 CohereDolly-v2 BLOOMzWikipedia Reddit WikiHow PeerRead arXiv144,523 69,406 84,651 24,317 36,20245,275 27,403 49,723 11,314 18,29159,038 47,092 33,292 24,134 47,307 29,062 7,693 8,812 29,024 22,77765,059 36,173 46,743 29,851 35,80834,304 1,000,870 28,794 586,341 40,082 820,026 11,597 225,007 29,989 263,781295,007 253,075 501,998 102,638 145,954400,072 258,210 315,567 183,926 457,188 243,356 51,636 61,310 186,561 149,892385,074 308,695 357,007 230,282 251,770141,328 212,334 277,770 92,858 209,053All domains 252,24495,775115,482 87,428139,98196,789 2,364,143 1,047,2931,145,593 733,902 1,220,512775,387All252,244275,4552,364,1433,074,950"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Statistics about the number of unique uni-grams (word types) and bi-grams of human-written and machine generated texts (English).", "figure_data": "Domain\u2193Word (unigram) Human ChatGPT davinc-003 Cohere Dolly-v2 BLOOMz Human ChatGPT davinc-003 Cohere Dolly-v2 BLOOMz bigramsWikipedia Reddit WikiHow PeerRead arXiv334 250 369 142 128158 140 277 151 121189 159 250 90 96142 107 143 82 97167 142 160 178 13077 134 174 133 159683 482 867 244 208274 247 580 262 199337 292 514 146 142259 191 270 129 168296 254 294 332 21993 164 225 154 218All domains228170160115154136457315293207277172All228147457252"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "", "figure_data": "B Detector Hyper-parametersB.1 Detector Hyper-parametersDetector\u2193Learning rate # epochs Batch sizeMaximum iterationsCRoBERTa-base ELECTRA-base XLM-R-base LR-GLTR Linear-SVM1e-6 1e-6 2e-5 default -10 10 5 --64 64 16 default ----1,000 20,000----0.8"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "Hyper-parameter settings for five detectors. LR-GLTR is based on the sklearn logistic regression model, all hyper-parameters follow the default setting except for maximum training iterations=1,000. The Linear-SVM detector uses all default parameters provided in the sklearn implementation except the penalty parameter of the error term C and the max iterations.", "figure_data": ""}, {"figure_label": "13", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "Same-generator, cross-domain experiments: train on a single domain of davinci-003 vs Human and test across domains. Evaluation accuracy (Acc), precision (Prec), recall and F1 scores(%) with respect to machine generations across four detectors.", "figure_data": "D Results: Same-Domain, Cross-Generator"}, {"figure_label": "16", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "Cross-language experiments. Accuracy (Acc) and F1 scores (for machine-generated class) based on XLM-R over test sets across different languages generated by ChatGPT. We average performance across 5 runs (standard deviation in the parenthesis).", "figure_data": "Generator\u2193 Test Domain \u2192 Train Domain \u2193 All domains (en) davinci-003 Baike/Web QA (zh) RuATD (ru) Bulgarian News (bg) AllAll domain (en) F1 95.8 (1.9) Acc 96.0 (1.8) 66.4 (7.6) 74.8 (4.2) 62.8 (3.0) 62.0 (8.1) 64.8 (3.1) 67.2 (9.1) 96.3 (0.7) 96.4 (0.6)Baike/Web QA (zh) Acc F1 79.5 (4.1) 82.9 (2.9) 98.9 (0.4) 98.9 (0.4) 49.6 (9.3) 58.6 (3.2) 59.0 (8.7) 29.4 (23.6) 59.0 (3.6) 32.0 (11.3) 99.6 (0.2) RuATD (ru) Bulgarian News (bg) Acc F1 Acc F1 60.5 (3.0) 65.3 (5.1) 65.8 (3.2) 69.3 (6.6) 59.5 (0.6) 70.0 (0.6) 48.6 (3.3) 61.3 (3.7) 95.3 (1.6) 95.4 (1.4) 86.5 (5.1) 86.0 (6.5) 99.6 (0.2) 98.7 (0.5) 98.7 (0.5) 92.8 (2.1) 93.2 (2.0) 85.2 (3.2) 87.0 (2.3)ChatGPTAll domains (en) Baike/Web QA (zh) RuATD (ru) Bulgarian News (bg) IDN (id) Urdu-News (ur) Arabic Wikipedia (ar) 72.8 (4.7) 90.2 (0.9) 61.6 (5.5) 56.7 (3.0) 74.2 (4.9) 61.0 (14.3) 29.5 (37.4) 55.6 (7.7) 89.4 (1.0) 93.0 (0.9) 72.2 (2.8) 93.5 (1.1) 68.6 (0.5) 75.7 (7.6) 75.1 (2.2) 78.3 (11.2) 70.1 (21.1) 53.8 (1.5) 15.5 (5.8) 92.6 (1.1) 54.1 (1.8) 51.5 (5.2) 93.1 (1.2) 58.8 (2.2) 67.7 (3.7) 67.5 (14.5) 84.7 (3.9) 82.4 (5.8) 17.5 (23.6) 50.6 (0.8) 5.1 (7.0) 50.0 (0.1) 66.6 (0.1) 50.8 (0.7) 67.0 (0.3) 50.0 (0.0) 66.7 (0.0) 77.0 (2.8) 83.9 (6.9) 85.5 (5.1) 62.0 (2.3) 70.2 (1.1) All 91.3 (0.6) 90.8 (0.6) 94.5 (1.2) 94.3 (1.4) 86.1 (2.5) 85.4 (2.9)66.0 (3.2) 57.7 (3.4) 82.2 (4.5) 95.4 (1.3) 58.7 (13.9) 23.6 (35.0) 64.3 (7.6) 65.0 (5.0) 84.9 (3.2) 95.3 (1.4) 50.2 (0.2) 66.8 (0.1) 64.6 (5.9) 73.6 (3.0) 82.6 (2.2) 84.9 (1.5)"}, {"figure_label": "18", "figure_type": "table", "figure_id": "tab_19", "figure_caption": "Data format of M4: English examples sampled across different domains and LLM generators.", "figure_data": "FieldContentLanguage LLM Generator CHATGPT English Domain Reddit ELI5QuestionWhat happened to Henry II's jousting opponent after accidentally killing him in 1559?"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_20", "figure_caption": "Credit Card., You dont want people tapping into your Wii and stealing your games, do you? On the back of the Wii Points Card, this same code can be found by scratching out the grey section with a coin or similar. You will be informed if you entered an invalid (wrong) code. Go back to the main Shop Channel menu now. Nintendo, from time to time, offers freebies via a special channel called \"WiiWare\". There are over 50 games to list, so be patient if the option you selected is taking a while to load. Dont forget to say you want to download to your Wii, unless you want your classics transportable, for example if you plan to demo the classics to a friend with a Wii. Nintendo added an animation to this process that shows Mario or Luigi collecting coins. This is why you want a secure connection!, Note that a tax is added to your purchase to comply with federal law.Prompt Please, generate wikihow article with length above 1000 characters from title 'How to Buy Virtual Console Games for Nintendo Wii' and headline 'Find a few Wii Points cards from game retailers like GameStop. Make sure your Wii is online and on a secure connection if possible.,Open the Wii Shop Channel, and click Add Wii Points at the bottom right of your screen after the channel finishes loading. Click \"From Card\". Now, your Wii should be asking for a code with 5 segments, separated by dashes. Enter this code into the Wii now. After a few prompts, you will have Wii points added to your Wii, which is what you use to purchase games. Click \"Virtual Console\". Choose how you want the games to be displayed. Scroll through the list for games that you want.Click on a game you want to play, And hit Yes on the confirmation box. Wait while the game is downloaded. After you have bought the games you want to play, go back to the Wii Menu. Select the new games you just bought, and start them like a regular channel. Open the Wii Shop Channel, and click Add Wii Points. Click \"With a Credit Card\". Key in your Credit Card Number. Let Nintendo authorize the purchase.,Go back to the Points Card process above and skip down to Step 7.", "figure_data": "Language LLM Generator DAVINCI-003 English Domain WikihowTitleHow to Buy Virtual Console Games for Nintendo WiiHumanThey are about $20 a card. Or, if you want to just buy points with your credit card, Skip down to the section, With a"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_21", "figure_caption": "7. Tie a fishing line loop around the laptop. Take the fishing line and tie it into a loop which exactly encircles the laptop when the display is open to the working position. Be sure to check that the fishing line loop is strong enough to prevent the display from falling into the fully open position. 8. Glue foam tape to the laptop edge. To finalize the repair, glue a small strip of two-sided foam tape on the edge of the laptop where the fishing line should rest. Optionally, you can also use a plastic bag closure to adjust the exact size of the fishing line loop and the working position of the laptop display. With these simple steps, you can work around damaged laptop display hinges and fix the issue without having to replace the entire hinge. Keep in mind that this repair method may not work for all laptop models, so be sure to check if your laptop can be repaired in this way before getting started.", "figure_data": "FieldContentLanguage LLM Generator CHATGPT English Domain PeerReadTitleBridge Text and Knowledge by Learning Multi-Prototype Entity Mention EmbeddingGood luck!"}, {"figure_label": "20", "figure_type": "table", "figure_id": "tab_22", "figure_caption": "Examples of other languages across different domains by ChatGPT and davinci-text-003.Human\u0414\u0435\u043d\u044c \u0431\u044b\u043b \u043f\u043e\u0441\u0432\u044f\u0449\u0435\u043d \u043f\u0440\u0430\u0437\u0434\u043d\u0438\u043a\u0443 \u043d\u0430 \u00ab\u043b\u0438\u043d\u0438\u0438 \u0434\u0432\u0438\u0436\u0435\u043d\u0438\u044f\u00bb \u0435\u0433\u043e \u0441\u044b\u043d\u0430. \u0413\u043d\u0435\u0442\u0443\u0449\u0435\u0435 \u043e\u0441\u0442\u0430\u043b\u043e\u0441\u044c \u0432\u043f\u0435\u0447\u0430\u0442\u043b\u0435\u043d\u0438\u0435 \u043e\u0442 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0445 (\u043e\u0442 \u0442\u0440\u0435\u0445 \u043b\u0438\u0446) \u044f\u0432\u043d\u043e \u0438 (\u043e\u0434\u043d\u043e \u0438\u0437 \u043d\u0438\u0445) \u0434\u0430\u0436\u0435 \u043e\u0444\u043e\u0440\u043c\u043b\u0435\u043d\u043d\u043e \u043f\u0440\u0438\u0431\u043b\u0438\u0436\u0435\u043d\u043d\u043e\u0433\u043e \u043a \u0435\u0433\u043e \u0436\u0438\u0437\u043d\u0438 \u0441 \u043e\u0441\u043e\u0431\u044b\u043c\u0438 \u043f\u0440\u0430\u0432\u0430\u043c\u0438 \u043d\u0430 \u0431\u043b\u0438\u0437\u043e\u0441\u0442\u044c. \u0423\u0433\u0430\u0448\u0430\u044e \u0432\u0441\u043f\u044b\u0445\u043d\u0443\u0432\u0448\u0443\u044e \u043f\u043e\u0442\u0440\u0435\u0431\u043d\u043e\u0441\u0442\u044c \u0441\u0443\u0434\u0438\u0442\u044c \u0438 \u043e\u0441\u0443\u0436\u0434\u0430\u0442\u044c \u043e\u0434\u043d\u0443 \u0436\u0435\u043d\u0449\u0438\u043d\u0443 \u0438 \u043e\u0434\u043d\u0443 \u043c\u0443\u0436\u0441\u043a\u0443\u044e \u0444\u0438\u0433\u0443\u0440\u0443, \u0438\u0445 \u043b\u0438\u0446\u0430, \u0433\u043b\u0430\u0437\u0430, \u0432\u044b\u0440\u0430\u0436\u0435\u043d\u0438\u044f, \u043c\u0430\u043d\u0435\u0440\u044b \u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c, \u0433\u043e\u0432\u043e\u0440\u0438\u0442\u044c, \u0434\u0432\u0438\u0433\u0430\u0442\u044c\u0441\u044f. \u041e\u0441\u0442\u0430\u043b\u043e\u0441\u044c \u0431\u043e\u043b\u044c\u043d\u044b\u043c \u043c\u0435\u0441\u0442\u043e\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0438\u0441\u043a\u0430\u0436\u0435\u043d\u043d\u043e\u0435 \u0438\u0445 \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435\u043c \u0438 \u0442\u043e\u043d\u043e\u043c \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a \u043d\u0438\u043c \u0433\u043b\u0430\u0432\u044b \u0434\u043e\u043c\u0430. \u0416\u0435\u043d\u0430 \u0438 \u043c\u0430\u0442\u044c \u0435\u0435, \u0438 \u043c\u0435\u0441\u0442\u043e, \u043a\u0430\u043a\u043e\u0435 \u043e\u043d\u0438 \u0437\u0430\u043d\u044f\u043b\u0438 \u0432 \u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u0438 \u0432\u043d\u0435\u0448\u043d\u0435\u0433\u043e \u0438 \u0432\u043d\u0443\u0442\u0440\u0435\u043d\u043d\u0435\u0433\u043e \u043b\u0438\u043a\u0430 \u0418\u0433\u043e\u0440\u044f, \u0445\u043e\u0447\u0435\u0442\u0441\u044f \u0432\u0435\u0440\u0438\u0442\u044c, \u043d\u0435 \u0441\u043c\u043e\u0433\u0443\u0442 \u0432 \u0433\u043b\u0430\u0432\u043d\u044b\u0445 \u0447\u0435\u0440\u0442\u0430\u0445 \u043b\u0438\u043a\u0430 \u0435\u0433\u043e \u0432\u043d\u0443\u0442\u0440\u0435\u043d\u043d\u0435\u0439 \u0436\u0438\u0437\u043d\u0438 \u0432\u043d\u0435\u0441\u0442\u0438 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0432 \u0441\u0442\u043e\u0440\u043e\u043d\u0443 \u0440\u0430\u0437\u0440\u0443\u0448\u0438\u0442\u0435\u043b\u044c\u043d\u0443\u044e \u0438\u043b\u0438 \u0437\u0430\u0441\u0442\u043e\u0439\u043d\u0443\u044e.\u0412\u043d\u0435\u0448\u043d\u0435 \u043e\u0431\u043b\u0435\u0433\u0447\u0430\u0442 \u0438 \u043f\u0440\u0438\u0443\u043a\u0440\u0430\u0441\u044f\u0442 \u0436\u0438\u0437\u043d\u044c. \u041e\u0434\u043d\u0438 \u044f\u043c\u043e\u0447\u043a\u0438 \u0432 \u0443\u043b\u044b\u0431\u043a\u0435 \u0436\u0435\u043d\u044b \u043d\u0430 \u0435\u0435 \u0434\u0435\u0432\u0438\u0447\u0435\u0441\u043a\u0438-\u0441\u0432\u0435\u0436\u0435\u043c \u043b\u0438\u0446\u0435, \u043f\u0440\u0438 \u0442\u0432\u0435\u0440\u0434\u043e\u043c \u0434\u043e \u0436\u0435\u0441\u0442\u043a\u043e\u0441\u0442\u0438 \u0432\u0437\u0433\u043b\u044f\u0434\u0435 \u044f\u0441\u043d\u044b\u0445 \u0441\u0435\u0440\u044b\u0445 \u0433\u043b\u0430\u0437, \u0434\u0435\u043b\u0430\u044e\u0442 \u043f\u043e\u043d\u044f\u0442\u043d\u043e\u0439 \u0435\u0435 \u043f\u0440\u0438\u0432\u043b\u0435\u043a\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0434\u043b\u044f \u043c\u044f\u0433\u043a\u043e\u0441\u0435\u0440\u0434\u0435\u0447\u043d\u043e\u0433\u043e, \u043d\u043e \u0432 \u043a\u0430\u043a\u043e\u0439-\u0442\u043e \u0447\u0430\u0441\u0442\u0438 \u0441\u0432\u043e\u0435\u0433\u043e \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0430 \u0443\u0433\u0440\u044e\u043c\u043e\u0433\u043e \u043c\u0443\u0436\u0430. \u0412\u0441\u043f\u043e\u043c\u043d\u0438\u043b\u043e\u0441\u044c, \u043a\u0430\u043a 4 \u0433\u043e\u0434\u0430 \u0442\u043e\u043c\u0443 \u043d\u0430\u0437\u0430\u0434 \u043f\u043e\u0441\u043b\u0435 \u0437\u043d\u0430\u043a\u043e\u043c\u0441\u0442\u0432\u0430 \u0435\u0433\u043e \u0441 \u043e\u0434\u043d\u043e\u0439 \u043a\u0440\u0430\u0441\u0438\u0432\u043e\u0439, \u0432 \u0432\u043e\u0437\u0440\u0430\u0441\u0442\u0435 \u043c\u0435\u0436\u0434\u0443 30-40 \u0433\u043e\u0434\u0430\u043c\u0438, \u0436\u0435\u043d\u0449\u0438\u043d\u043e\u0439, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u0439 \u0434\u0443\u0448\u0435\u0432\u043d\u043e\u0433\u043e \u043c\u0438\u0440\u0430 \u0438 \u0432\u0437\u0433\u043b\u044f\u0434\u043e\u043c \u043d\u0430 \u0436\u0438\u0437\u043d\u044c, \u043f\u043e-\u043c\u043e\u0435\u043c\u0443, \u0434\u043e\u043b\u0436\u043d\u0430 \u0431\u044b\u043b\u0430 \u0431\u044b \u0440\u0430\u0437\u0431\u0443\u0434\u0438\u0442\u044c \u0438\u043d\u0442\u0435\u0440\u0435\u0441 \u043a \u0441\u0435\u0431\u0435 \u0443 \u0418\u0433\u043e\u0440\u044f, \u043e\u043d \u0441\u043e\u0433\u043b\u0430\u0441\u0438\u043b\u0441\u044f \u0441\u043e \u043c\u043d\u043e\u0439, \u0447\u0442\u043e \u00ab\u043e\u043d\u0430, \u043f\u043e-\u0432\u0438\u0434\u0438\u043c\u043e\u043c\u0443, \u0445\u043e\u0440\u043e\u0448\u0438\u0439 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u00bb. \u041d\u043e \u0442\u0443\u0442 \u0436\u0435 \u043f\u0440\u0438\u0431\u0430\u0432\u0438\u043b: -\u0420\u0430\u0437\u0432\u0435 \u0432\u044b \u043d\u0435 \u0432\u0438\u0434\u0438\u0442\u0435, \u0442\u0435\u0442\u044f \u0412\u0430\u0432\u0430, \u0447\u0442\u043e \u043c\u043d\u0435 \u043b\u0443\u0447\u0448\u0435 \u0432\u0441\u0435\u0433\u043e \u043e\u0434\u043d\u043e\u043c\u0443, \u043a\u0430\u043a \u044f \u0442\u0435\u043f\u0435\u0440\u044c \u0436\u0438\u0432\u0443. \u0414\u0438\u043a\u043e \u0438 \u043e\u0441\u043a\u043e\u0440\u0431\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u0442\u044c \u0441\u0435\u0431\u0435 \u0432\u043e\u0442 \u0432 \u044d\u0442\u0438\u0445 \u0441\u0442\u0435\u043d\u0430\u0445 \u043a\u0430\u043a\u0443\u044e-\u0442\u043e \u0434\u0440\u0443\u0433\u0443\u044e \u0436\u0435\u043d\u0443, \u043a\u0440\u043e\u043c\u0435 \u0442\u043e\u0439, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0443 \u043c\u0435\u043d\u044f \u043e\u0442\u043d\u044f\u0442\u0430 \u0441\u043c\u0435\u0440\u0442\u044c\u044e. \u041f\u043e\u0436\u0430\u043b\u0443\u0439\u0441\u0442\u0430, \u043f\u0435\u0440\u0435\u0444\u043e\u0440\u043c\u0443\u043b\u0438\u0440\u0443\u0439\u0442\u0435 \u044d\u0442\u043e\u0442 \u0442\u0435\u043a\u0441\u0442, \u0447\u0442\u043e\u0431\u044b \u043e\u043d \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u043b \u0431\u043e\u043b\u0435\u0435 1000 \u0441\u0438\u043c\u0432\u043e\u043b\u043e\u0432. \u0422\u0435\u043a\u0441\u0442: '\u0414\u0435\u043d\u044c \u0431\u044b\u043b \u043f\u043e\u0441\u0432\u044f\u0449\u0435\u043d \u043f\u0440\u0430\u0437\u0434\u043d\u0438\u043a\u0443 \u043d\u0430 \u00ab\u043b\u0438\u043d\u0438\u0438 \u0434\u0432\u0438\u0436\u0435\u043d\u0438\u044f\u00bb \u0435\u0433\u043e \u0441\u044b\u043d\u0430. \u0413\u043d\u0435\u0442\u0443\u0449\u0435\u0435 \u043e\u0441\u0442\u0430\u043b\u043e\u0441\u044c \u0432\u043f\u0435\u0447\u0430\u0442\u043b\u0435\u043d\u0438\u0435 \u043e\u0442 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0445 (\u043e\u0442 \u0442\u0440\u0435\u0445 \u043b\u0438\u0446) \u044f\u0432\u043d\u043e \u0438 (\u043e\u0434\u043d\u043e \u0438\u0437 \u043d\u0438\u0445) \u0434\u0430\u0436\u0435 \u043e\u0444\u043e\u0440\u043c\u043b\u0435\u043d\u043d\u043e \u043f\u0440\u0438\u0431\u043b\u0438\u0436\u0435\u043d\u043d\u043e\u0433\u043e \u043a \u0435\u0433\u043e \u0436\u0438\u0437\u043d\u0438 \u0441 \u043e\u0441\u043e\u0431\u044b\u043c\u0438 \u043f\u0440\u0430\u0432\u0430\u043c\u0438 \u043d\u0430 \u0431\u043b\u0438\u0437\u043e\u0441\u0442\u044c. \u0423\u0433\u0430\u0448\u0430\u044e \u0432\u0441\u043f\u044b\u0445\u043d\u0443\u0432\u0448\u0443\u044e \u043f\u043e\u0442\u0440\u0435\u0431\u043d\u043e\u0441\u0442\u044c \u0441\u0443\u0434\u0438\u0442\u044c \u0438 \u043e\u0441\u0443\u0436\u0434\u0430\u0442\u044c \u043e\u0434\u043d\u0443 \u0436\u0435\u043d\u0449\u0438\u043d\u0443 \u0438 \u043e\u0434\u043d\u0443 \u043c\u0443\u0436\u0441\u043a\u0443\u044e \u0444\u0438\u0433\u0443\u0440\u0443, \u0438\u0445 \u043b\u0438\u0446\u0430, \u0433\u043b\u0430\u0437\u0430, \u0432\u044b\u0440\u0430\u0436\u0435\u043d\u0438\u044f, \u043c\u0430\u043d\u0435\u0440\u044b \u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c, \u0433\u043e\u0432\u043e\u0440\u0438\u0442\u044c, \u0434\u0432\u0438\u0433\u0430\u0442\u044c\u0441\u044f. \u041e\u0441\u0442\u0430\u043b\u043e\u0441\u044c \u0431\u043e\u043b\u044c\u043d\u044b\u043c \u043c\u0435\u0441\u0442\u043e\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0438\u0441\u043a\u0430\u0436\u0435\u043d\u043d\u043e\u0435 \u0438\u0445 \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435\u043c \u0438 \u0442\u043e\u043d\u043e\u043c \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u0435 \u043a \u043d\u0438\u043c \u0433\u043b\u0430\u0432\u044b \u0434\u043e\u043c\u0430. \u0416\u0435\u043d\u0430 \u0438 \u043c\u0430\u0442\u044c \u0435\u0435, \u0438 \u043c\u0435\u0441\u0442\u043e, \u043a\u0430\u043a\u043e\u0435 \u043e\u043d\u0438 \u0437\u0430\u043d\u044f\u043b\u0438 \u0432 \u043e\u0431\u043d\u043e\u0432\u043b\u0435\u043d\u0438\u0438 \u0432\u043d\u0435\u0448\u043d\u0435\u0433\u043e \u0438 \u0432\u043d\u0443\u0442\u0440\u0435\u043d\u043d\u0435\u0433\u043e \u043b\u0438\u043a\u0430 \u0418\u0433\u043e\u0440\u044f, \u0445\u043e\u0447\u0435\u0442\u0441\u044f \u0432\u0435\u0440\u0438\u0442\u044c, \u043d\u0435 \u0441\u043c\u043e\u0433\u0443\u0442 \u0432 \u0433\u043b\u0430\u0432\u043d\u044b\u0445 \u0447\u0435\u0440\u0442\u0430\u0445 \u043b\u0438\u043a\u0430 \u0435\u0433\u043e \u0432\u043d\u0443\u0442\u0440\u0435\u043d\u043d\u0435\u0439 \u0436\u0438\u0437\u043d\u0438 \u0432\u043d\u0435\u0441\u0442\u0438 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0435 \u0432 \u0441\u0442\u043e\u0440\u043e\u043d\u0443 \u0440\u0430\u0437\u0440\u0443\u0448\u0438\u0442\u0435\u043b\u044c\u043d\u0443\u044e \u0438\u043b\u0438 \u0437\u0430\u0441\u0442\u043e\u0439\u043d\u0443\u044e. \u0412\u043d\u0435\u0448\u043d\u0435 \u043e\u0431\u043b\u0435\u0433\u0447\u0430\u0442 \u0438 \u043f\u0440\u0438\u0443\u043a\u0440\u0430\u0441\u044f\u0442 \u0436\u0438\u0437\u043d\u044c. \u041e\u0434\u043d\u0438 \u044f\u043c\u043e\u0447\u043a\u0438 \u0432 \u0443\u043b\u044b\u0431\u043a\u0435 \u0436\u0435\u043d\u044b \u043d\u0430 \u0435\u0435 \u0434\u0435\u0432\u0438\u0447\u0435\u0441\u043a\u0438-\u0441\u0432\u0435\u0436\u0435\u043c \u043b\u0438\u0446\u0435, \u043f\u0440\u0438 \u0442\u0432\u0435\u0440\u0434\u043e\u043c \u0434\u043e \u0436\u0435\u0441\u0442\u043a\u043e\u0441\u0442\u0438 \u0432\u0437\u0433\u043b\u044f\u0434\u0435 \u044f\u0441\u043d\u044b\u0445 \u0441\u0435\u0440\u044b\u0445 \u0433\u043b\u0430\u0437, \u0434\u0435\u043b\u0430\u044e\u0442 \u043f\u043e\u043d\u044f\u0442\u043d\u043e\u0439 \u0435\u0435 \u043f\u0440\u0438\u0432\u043b\u0435\u043a\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0434\u043b\u044f \u043c\u044f\u0433\u043a\u043e\u0441\u0435\u0440\u0434\u0435\u0447\u043d\u043e\u0433\u043e, \u043d\u043e \u0432 \u043a\u0430\u043a\u043e\u0439-\u0442\u043e \u0447\u0430\u0441\u0442\u0438 \u0441\u0432\u043e\u0435\u0433\u043e \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0430 \u0443\u0433\u0440\u044e\u043c\u043e\u0433\u043e \u043c\u0443\u0436\u0430. \u0412\u0441\u043f\u043e\u043c\u043d\u0438\u043b\u043e\u0441\u044c, \u043a\u0430\u043a 4 \u0433\u043e\u0434\u0430 \u0442\u043e\u043c\u0443 \u043d\u0430\u0437\u0430\u0434 \u043f\u043e\u0441\u043b\u0435 \u0437\u043d\u0430\u043a\u043e\u043c\u0441\u0442\u0432\u0430 \u0435\u0433\u043e \u0441 \u043e\u0434\u043d\u043e\u0439 \u043a\u0440\u0430\u0441\u0438\u0432\u043e\u0439, \u0432 \u0432\u043e\u0437\u0440\u0430\u0441\u0442\u0435 \u043c\u0435\u0436\u0434\u0443 30-40 \u0433\u043e\u0434\u0430\u043c\u0438, \u0436\u0435\u043d\u0449\u0438\u043d\u043e\u0439, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043a\u0430\u0440\u0442\u0438\u043d\u043e\u0439 \u0434\u0443\u0448\u0435\u0432\u043d\u043e\u0433\u043e \u043c\u0438\u0440\u0430 \u0438 \u0432\u0437\u0433\u043b\u044f\u0434\u043e\u043c \u043d\u0430 \u0436\u0438\u0437\u043d\u044c, \u043f\u043e-\u043c\u043e\u0435\u043c\u0443, \u0434\u043e\u043b\u0436\u043d\u0430 \u0431\u044b\u043b\u0430 \u0431\u044b \u0440\u0430\u0437\u0431\u0443\u0434\u0438\u0442\u044c \u0438\u043d\u0442\u0435\u0440\u0435\u0441 \u043a \u0441\u0435\u0431\u0435 \u0443 \u0418\u0433\u043e\u0440\u044f, \u043e\u043d \u0441\u043e\u0433\u043b\u0430\u0441\u0438\u043b\u0441\u044f \u0441\u043e \u043c\u043d\u043e\u0439, \u0447\u0442\u043e \u00ab\u043e\u043d\u0430, \u043f\u043e-\u0432\u0438\u0434\u0438\u043c\u043e\u043c\u0443, \u0445\u043e\u0440\u043e\u0448\u0438\u0439 \u0447\u0435\u043b\u043e\u0432\u0435\u043a\u00bb. \u041d\u043e \u0442\u0443\u0442 \u0436\u0435 \u043f\u0440\u0438\u0431\u0430\u0432\u0438\u043b: -\u0420\u0430\u0437\u0432\u0435 \u0432\u044b \u043d\u0435 \u0432\u0438\u0434\u0438\u0442\u0435, \u0442\u0435\u0442\u044f \u0412\u0430\u0432\u0430, \u0447\u0442\u043e \u043c\u043d\u0435 \u043b\u0443\u0447\u0448\u0435 \u0432\u0441\u0435\u0433\u043e \u043e\u0434\u043d\u043e\u043c\u0443, \u043a\u0430\u043a \u044f \u0442\u0435\u043f\u0435\u0440\u044c \u0436\u0438\u0432\u0443. \u0414\u0438\u043a\u043e \u0438 \u043e\u0441\u043a\u043e\u0440\u0431\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u0438\u0442\u044c \u0441\u0435\u0431\u0435 \u0432\u043e\u0442 \u0432 \u044d\u0442\u0438\u0445 \u0441\u0442\u0435\u043d\u0430\u0445 \u043a\u0430\u043a\u0443\u044e-\u0442\u043e \u0434\u0440\u0443\u0433\u0443\u044e \u0436\u0435\u043d\u0443, \u043a\u0440\u043e\u043c\u0435 \u0442\u043e\u0439, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0443 \u043c\u0435\u043d\u044f \u043e\u0442\u043d\u044f\u0442\u0430 \u0441\u043c\u0435\u0440\u0442\u044c\u044e.'", "figure_data": "FieldContentLanguage LLM Generator CHATGPT Arabic Domain WikipediaTitleHuman"}], "formulas": [{"formula_id": "formula_0", "formula_text": "Language English LLM Generator COHERE Domain Wikipedia Title Shed (deity)", "formula_coordinates": [25.0, 92.87, 677.63, 113.45, 46.72]}], "doi": "10.48550/arXiv.2210.11416"}