{"title": "TAG, Dynamic Programming, and the Perceptron for Efficient, Feature-rich Parsing", "authors": "Xavier Carreras; Michael Collins; Terry Koo", "pub_date": "", "abstract": "We describe a parsing approach that makes use of the perceptron algorithm, in conjunction with dynamic programming methods, to recover full constituent-based parse trees. The formalism allows a rich set of parse-tree features, including PCFGbased features, bigram and trigram dependency features, and surface features. A severe challenge in applying such an approach to full syntactic parsing is the efficiency of the parsing algorithms involved. We show that efficient training is feasible, using a Tree Adjoining Grammar (TAG) based parsing formalism. A lower-order dependency parsing model is used to restrict the search space of the full model, thereby making it efficient. Experiments on the Penn WSJ treebank show that the model achieves state-of-the-art performance, for both constituent and dependency accuracy.", "sections": [{"heading": "Introduction", "text": "In global linear models (GLMs) for structured prediction, (e.g., (Johnson et al., 1999;Lafferty et al., 2001;Collins, 2002;Altun et al., 2003;Taskar et al., 2004)), the optimal label y * for an input x is\ny * = arg max y\u2208Y(x) w \u2022 f (x, y)(1)\nwhere Y(x) is the set of possible labels for the input x; f (x, y) \u2208 R d is a feature vector that represents the pair (x, y); and w is a parameter vector. This paper describes a GLM for natural language parsing, trained using the averaged perceptron. The parser we describe recovers full syntactic representations, similar to those derived by a probabilistic context-free grammar (PCFG). A key motivation for the use of GLMs in parsing is that they allow a great deal of flexibility in the features which can be included in the definition of f (x, y).\nc 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved.\nA critical problem when training a GLM for parsing is the computational complexity of the inference problem. The averaged perceptron requires the training set to be repeatedly decoded under the model; under even a simple PCFG representation, finding the arg max in Eq. 1 requires O(n 3 G) time, where n is the length of the sentence, and G is a grammar constant. The average sentence length in the data set we use (the Penn WSJ treebank) is over 23 words; the grammar constant G can easily take a value of 1000 or greater. These factors make exact inference algorithms virtually intractable for training or decoding GLMs for full syntactic parsing.\nAs a result, in spite of the potential advantages of these methods, there has been very little previous work on applying GLMs for full parsing without the use of fairly severe restrictions or approximations. For example, the model in (Taskar et al., 2004) is trained on only sentences of 15 words or less; reranking models (Collins, 2000;Charniak and Johnson, 2005) restrict Y(x) to be a small set of parses from a first-pass parser; see section 1.1 for discussion of other related work.\nThe following ideas are central to our approach:\n(1) A TAG-based, splittable grammar. We describe a novel, TAG-based parsing formalism that allows full constituent-based trees to be recovered. A driving motivation for our approach comes from the flexibility of the feature-vector representations f (x, y) that can be used in the model. The formalism that we describe allows the incorporation of: (1) basic PCFG-style features; (2) the use of features that are sensitive to bigram dependencies between pairs of words; and (3) features that are sensitive to trigram dependencies. Any of these feature types can be combined with surface features of the sentence x, in a similar way to the use of surface features in conditional random fields (Lafferty et al., 2001). Crucially, in spite of these relatively rich representations, the formalism can be parsed efficiently (in O(n 4 G) time) using dynamic-programming algorithms described by Eisner (2000) (unlike many other TAGrelated approaches, our formalism is \"splittable\" in the sense described by Eisner, leading to more efficient parsing algorithms).\n(2) Use of a lower-order model for pruning. The O(n 4 G) running time of the TAG parser is still too expensive for efficient training with the perceptron. We describe a method that leverages a simple, first-order dependency parser to restrict the search space of the TAG parser in training and testing. The lower-order parser runs in O(n 3 H) time where H \u226a G; experiments show that it is remarkably effective in pruning the search space of the full TAG parser.\nExperiments on the Penn WSJ treebank show that the model recovers constituent structures with higher accuracy than the approaches of (Charniak, 2000;Collins, 2000;, and with a similar level of performance to the reranking parser of (Charniak and Johnson, 2005). The model also recovers dependencies with significantly higher accuracy than state-of-the-art dependency parsers such as (Koo et al., 2008;Mc-Donald and Pereira, 2006).", "publication_ref": ["b14", "b16", "b9", "b0", "b24", "b24", "b8", "b2", "b16", "b10", "b3", "b8", "b2", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Previous work has made use of various restrictions or approximations that allow efficient training of GLMs for parsing. This section describes the relationship between our work and this previous work.\nIn reranking approaches, a first-pass parser is used to enumerate a small set of candidate parses for an input sentence; the reranking model, which is a GLM, is used to select between these parses (e.g., (Ratnaparkhi et al., 1994;Johnson et al., 1999;Collins, 2000;Charniak and Johnson, 2005)). A crucial advantage of our approach is that it considers a very large set of alternatives in Y(x), and can thereby avoid search errors that may be made in the first-pass parser. 1 Another approach that allows efficient training of GLMs is to use simpler syntactic representations, in particular dependency structures (McDon-ald et al., 2005). Dependency parsing can be implemented in O(n 3 ) time using the algorithms of Eisner (2000). In this case there is no grammar constant, and parsing is therefore efficient. A disadvantage of these approaches is that they do not recover full, constituent-based syntactic structures; the increased linguistic detail in full syntactic structures may be useful in NLP applications, or may improve dependency parsing accuracy, as is the case in our experiments. 2 There has been some previous work on GLM approaches for full syntactic parsing that make use of dynamic programming. Taskar et al. (2004) describe a max-margin approach; however, in this work training sentences were limited to be of 15 words or less. Clark and Curran (2004) describe a log-linear GLM for CCG parsing, trained on the Penn treebank. This method makes use of parallelization across an 18 node cluster, together with up to 25GB of memory used for storage of dynamic programming structures for training data. Clark and Curran (2007) describe a perceptronbased approach for CCG parsing which is considerably more efficient, and makes use of a supertagging model to prune the search space of the full parsing model. Recent work Finkel et al., 2008) describes log-linear GLMs applied to PCFG representations, but does not make use of dependency features.", "publication_ref": ["b21", "b14", "b8", "b2", "b10", "b24", "b5", "b6", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "The TAG-Based Parsing Model", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Derivations", "text": "This section describes the idea of derivations in our parsing formalism. As in context-free grammars or TAGs, a derivation in our approach is a data structure that specifies the sequence of operations used in combining basic (elementary) structures in a grammar, to form a full parse tree. The parsing formalism we use is related to the tree adjoining grammar (TAG) formalisms described in (Chiang, 2003;Shen and Joshi, 2005). However, an important difference of our work from this previous work is that our formalism is defined to be \"splittable\", allowing use of the efficient parsing algorithms of Eisner (2000).\nA derivation in our model is a pair E, D where E is a set of spines, and D is a set of dependencies These structures do not have substitution nodes, as is common in TAGs. 3 Instead, the spines consist of a lexical anchor together with a series of unary projections, which usually correspond to different X-bar levels associated with the anchor. The operations used to combine spines are similar to the TAG operations of adjunction and sister adjunction. We will call these operations regular adjunction (r-adjunction) and sister adjunction (s-adjunction). As one example, the cake spine shown above can be s-adjoined into the VP node of the ate spine, to form the tree shown in figure 1(a). In contrast, if we use the r-adjunction operation to adjoin the cake tree into the VP node, we get a different structure, which has an additional VP level created by the r-adjunction operation: the resulting tree is shown in figure 1(b). The r-adjunction operation is similar to the usual adjunction operation in TAGs, but has some differences that allow our grammars to be splittable; see section 2.3 for more discussion.\nWe now give formal definitions of the sets E and D. Take x to be a sentence consisting of n + 1 words, x 0 . . . x n , where x 0 is a special root symbol, which we will denote as * . A derivation for the input sentence x consists of a pair E, D , where:\n\u2022 E is a set of (n + 1) tuples of the form i, \u03b7 , where i \u2208 {0 . . . n} is an index of a word in the sentence, and \u03b7 is the spine associated with the word x i . The set E specifies one spine for each of the (n + 1) words in the sentence. Where it is clear from context, we will use \u03b7 i to refer to the spine in E corresponding to the i'th word.\n\u2022 D is a set of n dependencies. Each dependency is a tuple h, m, l . Here h is the index of the head-word of the dependency, corresponding to the spine \u03b7 h which contains a node that is being adjoined into. m is the index of the modifier-word of the dependency, corresponding to the spine \u03b7 m which is being adjoined into \u03b7 h . l is a label.\nThe label l is a tuple POS, A, \u03b7 h , \u03b7 m , L . \u03b7 h and \u03b7 m are the head and modifier spines that are being combined. POS specifies which node in \u03b7 h is being adjoined into. A is a binary flag specifying whether the combination operation being used is sadjunction or r-adjunction. L is a binary flag specifying whether or not any \"previous\" modifier has been r-adjoined into the position POS in \u03b7 h . By a previous modifier, we mean a modifier m \u2032 that was adjoined from the same direction as m (i.e., such that h < m \u2032 < m or m < m \u2032 < h).\nIt would be sufficient to define l to be the pair POS, A -the inclusion of \u03b7 h , \u03b7 m and L adds redundant information that can be recovered from the set E, and other dependencies in D-but it will be convenient to include this information in the label. In particular, it is important that given this definition of l, it is possible to define a function GRM(l) that maps a label l to a triple of nonterminals that represents the grammatical relation between m and h in the dependency structure. For example, in the tree shown in figure 1(a), the grammatical relation between cake and ate is the triple GRM(l) = VP VBD NP . In the tree shown in figure 1(b), the grammatical relation between cake and ate is the triple GRM(l) = VP VP NP .\nThe conditions under which a pair E, D forms a valid derivation for a sentence x are similar to those in conventional LTAGs. Each i, \u03b7 \u2208 E must be such that \u03b7 is an elementary tree whose anchor is the word x i . The dependencies D must form a directed, projective tree spanning words 0 . . . n, with * at the root of this tree, as is also the case in previous work on discriminative approches to dependency parsing (McDonald et al., 2005). We allow any modifier tree \u03b7 m to adjoin into any position in any head tree \u03b7 h , but the dependencies D must nevertheless be coherent-for example they must be consistent with the spines in E, and they must be nested correctly. 4 We will al-low multiple modifier spines to s-adjoin or r-adjoin into the same node in a head spine; see section 2.3 for more details.", "publication_ref": ["b4", "b23", "b10", "b18"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "A Global Linear Model", "text": "The model used for parsing with this approach is a global linear model. For a given sentence x, we define Y(x) to be the set of valid derivations for x, where each y \u2208 Y(x) is a pair E, D as described in the previous section. A function f maps (x, y) pairs to feature-vectors f (x, y) \u2208 R d . The parameter vector w is also a vector in R d . Given these definitions, the optimal derivation for an input sentence x is y * = arg max y\u2208Y(x) w \u2022 f (x, y).\nWe now come to how the feature-vector f (x, y) is defined in our approach. A simple \"first-order\" model would define\nf (x, y) = i,\u03b7 \u2208E(y) e(x, i, \u03b7 ) + h,m,l \u2208D(y) d(x, h, m, l ) (2)\nHere we use E(y) and D(y) to respectively refer to the set of spines and dependencies in y. The function e maps a sentence x paired with a spine i, \u03b7 to a feature vector. The function d maps dependencies within y to feature vectors. This decomposition is similar to the first-order model of McDonald et al. (2005), but with the addition of the e features.\nWe will extend our model to include higherorder features, in particular features based on sibling dependencies (McDonald and Pereira, 2006), and grandparent dependencies, as in (Carreras, 2007). If y = E, D is a derivation, then:\n\u2022 S(y) is a set of sibling dependencies. Each sibling dependency is a tuple h, m, l, s . For each h, m, l, s \u2208 S the tuple h, m, l is an element of D; there is one member of S for each member of D. The index s is the index of the word that was adjoined to the spine for h immediately before m (or the NULL symbol if no previous adjunction has taken place).\n\u2022 G(y) is a set of grandparent dependencies of type 1. Each type 1 grandparent dependency is a tuple h, m, l, g . There is one member of G for every member of D. The additional information, the index g, is the index of the word that is the first modifier to the right of the spine for m.\nthat are further from the head. \u2022 Q(y) is an additional set of grandparent dependencies, of type 2. Each of these dependencies is a tuple h, m, l, q . Again, there is one member of Q for every member of D. The additional information, the index q, is the index of the word that is the first modifier to the left of the spine for m.\nThe feature-vector definition then becomes:\nf (x, y) = X i,\u03b7 \u2208E(y) e(x, i, \u03b7 ) + X h,m,l \u2208D(y) d(x, h, m, l ) + X h,m,l,s \u2208S(y) s(x, h, m, l, s ) + X h,m,l,g \u2208G(y) g(x, h, m, l, g ) + X h,m,l,q \u2208Q(y) q(x, h, m, l, q )(3)\nwhere s, g and q are feature vectors corresponding to the new, higher-order elements. 5", "publication_ref": ["b18", "b17", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Recovering Parse Trees from Derivations", "text": "As in TAG approaches, there is a mapping from derivations E, D to parse trees (i.e., the type of trees generated by a context-free grammar). In our case, we map a spine and its dependencies to a constituent structure by first handling the dependen-cies on each side separately and then combining the left and right sides. First, it is straightforward to build the constituent structure resulting from multiple adjunctions on the same side of a spine. As one example, the structure in figure 2(a) is formed by first s-adjoining the spine with anchor cake into the VP node of the spine for ate, then r-adjoining spines anchored by today and quickly into the same node, where all three modifier words are to the right of the head word. Notice that each r-adjunction operation creates a new VP level in the tree, whereas s-adjunctions do not create a new level. Now consider a tree formed by first r-adjoining a spine for luckily into the VP node for ate, followed by sadjoining the spine for John into the S node, in both cases where the modifiers are to the left of the head. In this case the structure that would be formed is shown in figure 2(b).\nNext, consider combining the left and right structures of a spine. The main issue is how to handle multiple r-adjunctions or s-adjunctions on both sides of a node in a spine, because our derivations do not specify how adjunctions from different sides embed with each other. In our approach, the combination operation preserves the height of the different modifiers from the left and right directions. To illustrate this, figure 3 shows the result of combining the two structures in figure 2. The combination of the left and right modifier structures has led to flat structures, for example the rule VP \u2192 ADVP VP NP in the above tree.\nNote that our r-adjunction operation is different from the usual adjunction operation in TAGs, in that \"wrapping\" adjunctions are not possible, and r-adjunctions from the left and right directions are independent from each other; because of this our grammars are splittable.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Parsing Algorithms", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Use of Eisner's Algorithms", "text": "This section describes the algorithm for finding y * = arg max y\u2208Y(x) w \u2022 f (x, y) where f (x, y) is defined through either the first-order model (Eq. 2) or the second-order model (Eq. 3).\nFor the first-order model, the methods described in (Eisner, 2000) can be used for the parsing algorithm. In Eisner's algorithms for dependency parsing each word in the input has left and right finitestate (weighted) automata, which generate the left and right modifiers of the word in question. We make use of this idea of automata, and also make direct use of the method described in section 4.2 of (Eisner, 2000) that allows a set of possible senses for each word in the input string. In our use of the algorithm, each possible sense for a word corresponds to a different possible spine that can be associated with that word. The left and right automata are used to keep track of the last position in the spine that was adjoined into on the left/right of the head respectively. We can make use of separate left and right automata-i.e., the grammar is splittable-because left and right modifiers are adjoined independently of each other in the tree. The extension of Eisner's algorithm to the second-order model is similar to the algorithm described in (Carreras, 2007), but again with explicit use of word senses and left/right automata. The resulting algorithms run in O(Gn 3 ) and O(Hn 4 ) time for the first-order and second-order models respectively, where G and H are grammar constants.", "publication_ref": ["b10", "b10", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Efficient Parsing", "text": "The efficiency of the parsing algorithm is important in applying the parsing model to test sentences, and also when training the model using discriminative methods. The grammar constants G and H introduced in the previous section are polynomial in factors such as the number of possible spines in the model, and the number of possible states in the finite-state automata implicit in the parsing algorithm. These constants are large, making exhaustive parsing very expensive.\nTo deal with this problem, we use a simple initial model to prune the search space of the more complex model. The first-stage model we use is a first-order dependency model, with labeled dependencies, as described in (McDonald et al., 2005). As described shortly, we will use this model to compute marginal scores for dependencies in both training and test sentences. A marginal score \u00b5(x, h, m, l) is a value between 0 and 1 that reflects the plausibility of a dependency for sentence x with head-word x h , modifier word x m , and label l. In the first-stage pruning model the labels l are triples of non-terminals representing grammatical relations, as described in section 2.1 of this paper-for example, one possible label would be VP VBD NP , and in general any triple of nonterminals is possible.\nGiven a sentence x, and an index m of a word in that sentence, we define DMAX(x, m) to be the highest scoring dependency with m as a modifier:\nDMAX(x, m) = max h,l \u00b5(x, h, m, l)\nFor a sentence x, we then define the set of allowable dependencies to be\n\u03c0(x) = { h, m, l : \u00b5(x, h, m, l) \u2265 \u03b1DMAX(x, m)}\nwhere \u03b1 is a constant dictating the beam size that is used (in our experiments we used \u03b1 = 10 \u22126 ).\nThe set \u03c0(x) is used to restrict the set of possible parses under the full TAG-based model. In section 2.1 we described how the TAG model has dependency labels of the form POS, A, \u03b7 h , \u03b7 m , L , and that there is a function GRM that maps labels of this form to triples of non-terminals. The basic idea of the pruned search is to only allow dependencies of the form h, m, POS, A, \u03b7 h , \u03b7 m , L if the tuple h, m, GRM( POS, A, \u03b7 h , \u03b7 m , L ) is a member of \u03c0(x), thus reducing the search space for the parser.\nWe now turn to how the marginals \u00b5(x, h, m, l) are defined and computed. A simple approach would be to use a conditional log-linear model (Lafferty et al., 2001), with features as defined by McDonald et al. (2005), to define a distribution P (y|x) where the parse structures y are dependency structures with labels that are triples of nonterminals. In this case we could define \u00b5(x, h, m, l) = y:(h,m,l)\u2208y P (y|x) which can be computed with inside-outside style algorithms, applied to the data structures from (Eisner, 2000). The complexity of training and applying such a model is again O(Gn 3 ), where G is the number of possible labels, and the number of possible labels (triples of non-terminals) is around G = 1000 in the case of treebank parsing; this value for G is still too large for the method to be efficient. Instead, we train three separate models \u00b5 1 , \u00b5 2 , and \u00b5 3 for the three different positions in the non-terminal triples. We then take \u00b5(x, h, m, l) to be a product of these three models, for example we would calculate\n\u00b5(x, h, m, VP VBD NP ) = \u00b5 1 (x, h, m, VP ) \u00d7 \u00b5 2 (x, h, m, VBD ) \u00d7\u00b5 3 (x, h, m, NP )\nTraining the three models, and calculating the marginals, now has a grammar constant equal to the number of non-terminals in the grammar, which is far more manageable. We use the algorithm described in (Globerson et al., 2007) to train the conditional log-linear model; this method was found to converge to a good model after 10 iterations over the training data.", "publication_ref": ["b18", "b16", "b18", "b10", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Implementation Details", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Features", "text": "Section 2.2 described the use of feature vectors associated with spines used in a derivation, together with first-order, sibling, and grandparent dependencies. The dependency features used in our experiments are closely related to the features described in (Carreras, 2007), which are an extension of the McDonald and Pereira (2006) features to cover grandparent dependencies in addition to first-order and sibling dependencies. The features take into account the identity of the labels l used in the derivations. The features could potentially look at any information in the labels, which are of the form POS, A, \u03b7 h , \u03b7 m , L , but in our experiments, we map labels to a pair (GRM( POS, A, \u03b7 h , \u03b7 m , L ), A). Thus the label features are sensitive only to the triple of nonterminals corresponding to the grammatical relation involved in an adjunction, and a binary flag specifiying whether the operation is s-adjunction or r-adjunction.\nFor the spine features e(x, i, \u03b7 ), we use feature templates that are sensitive to the identity of the spine \u03b7, together with contextual features of the string x. These features consider the identity of the words and part-of-speech tags in a window that is centered on x i and spans the range x (i\u22122) . . . x (i+2) .", "publication_ref": ["b1", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Extracting Derivations from Parse Trees", "text": "In the experiments in this paper, the following three-step process was used: (1) derivations were extracted from a training set drawn from the Penn WSJ treebank, and then used to train a parsing model; (2) the test data was parsed using the resulting model, giving a derivation for each test data sentence; (3) the resulting test-data derivations were mapped back to Penn-treebank style trees, using the method described in section 2.1. To achieve step (1), we first apply a set of headfinding rules which are similar to those described in (Collins, 1997). Once the head-finding rules have been applied, it is straightforward to extract   ), Finkel et al. (2008, Charniak (2000), Collins (2000), , Charniak and Johnson (2005), and Huang (2008). (CJ05 is the performance of an updated model at http://www.cog.brown.edu/mj/software.htm.) \"s24\" denotes results on section 24 of the treebank.   (Yamada and Matsumoto, 2003) to extract dependencies from parse trees from our model. KCC08 unlabeled is from (Koo et al., 2008), a model that has previously been shown to have higher accuracy than (McDonald and Pereira, 2006). KCC08 labeled is the labeled dependency parser from (Koo et al., 2008); here we only evaluate the unlabeled accuracy.\nderivations from the Penn treebank trees. Note that the mapping from parse trees to derivations is many-to-one: for example, the example trees in section 2.3 have structures that are as \"flat\" (have as few levels) as is possible, given the set D that is involved. Other similar trees, but with more VP levels, will give the same set D. However, this issue appears to be benign in the Penn WSJ treebank. For example, on section 22 of the treebank, if derivations are first extracted using the method described in this section, then mapped back to parse trees using the method described in section 2.3, the resulting parse trees score 100% precision and 99.81% recall in labeled constituent accuracy, indicating that very little information is lost in this process.", "publication_ref": ["b7", "b11", "b3", "b8", "b2", "b13", "b25", "b15", "b17", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Part-of-Speech Tags, and Spines", "text": "Sentences in training, test, and development data are assumed to have part-of-speech (POS) tags. POS tags are used for two purposes: (1) in the features described above; and ( 2  dictionary listing the spines that have been seen with this POS tag in training data; during parsing we only allow spines that are compatible with this dictionary. (For test or development data, we used the part-of-speech tags generated by the parser of (Collins, 1997). Future work should consider incorporating the tagging step within the model; it is not challenging to extend the model in this way.)  1 shows the results for the method.", "publication_ref": ["b7"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Experiments", "text": "Our experiments show an improvement in performance over the results in (Collins, 2000;Charniak, 2000). We would argue that the Collins (2000) method is considerably more complex than ours, requiring a first-stage generative model, together with a reranking approach. The Charniak (2000) model is also arguably more complex, again using a carefully constructed generative model. The accuracy of our approach also shows some improvement over results in . This work makes use of a PCFG with latent variables that is trained using a split/merge procedure together with the EM algorithm. This work is in many ways complementary to ours-for example, it does not make use of GLMs, dependency features, or of representations that go beyond PCFG productions-and some combination of the two methods may give further gains. Charniak and Johnson (2005), and Huang (2008), describe approaches that make use of nonlocal features in conjunction with the Charniak (2000) model; future work may consider extending our approach to include non-local features. Finally, other recent work Finkel et al., 2008) has had a similar goal of scaling GLMs to full syntactic parsing. These models make use of PCFG representations, but do not explicitly model bigram or trigram dependencies. The results in this work (88.3%/88.0% F 1 ) are lower than our F 1 score of 91.1%; this is evidence of the benefits of the richer representations enabled by our approach.\nTable 2 shows the accuracy of the model in recovering unlabeled dependencies. The method shows improvements over the method described in (Koo et al., 2008), which is a state-of-the-art second-order dependency parser similar to that of (McDonald and Pereira, 2006), suggesting that the incorporation of constituent structure can improve dependency accuracy.\nTable 3 shows the effect of the beam-size on the accuracy and speed of the parser on the development set. With the beam setting used in our experiments (\u03b1 = 10 \u22126 ), only 0.34% of possible dependencies are considered by the TAG-based model, but 99% of all correct dependencies are included. At this beam size the best possible F 1 constituent score is 98.5. Tighter beams lead to faster parsing times, with slight drops in accuracy.", "publication_ref": ["b8", "b3", "b3", "b2", "b13", "b3", "b11", "b15", "b17"], "figure_ref": [], "table_ref": ["tab_2", "tab_4"]}, {"heading": "Conclusions", "text": "We have described an efficient and accurate parser for constituent parsing. A key to the approach has been to use a splittable grammar that allows efficient dynamic programming algorithms, in combination with pruning using a lower-order model. The method allows relatively easy incorporation of features; future work should leverage this in producing more accurate parsers, and in applying the parser to different languages or domains.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Hidden markov support vector machines", "journal": "", "year": "2003", "authors": "Y Altun; I Tsochantaridis; T Hofmann"}, {"ref_id": "b1", "title": "Experiments with a higher-order projective dependency parser", "journal": "", "year": "2007", "authors": "X Carreras"}, {"ref_id": "b2", "title": "Coarse-to-fine n-best parsing and maxent discriminative reranking", "journal": "", "year": "2005", "authors": "E Charniak; M Johnson"}, {"ref_id": "b3", "title": "A maximum-entropy-inspired parser", "journal": "", "year": "2000", "authors": "E Charniak"}, {"ref_id": "b4", "title": "Statistical parsing with an automatically extracted tree adjoining grammar", "journal": "CSLI Publications", "year": "2003", "authors": "D Chiang"}, {"ref_id": "b5", "title": "Parsing the wsj using ccg and log-linear models", "journal": "", "year": "2004", "authors": "S Clark; J R Curran"}, {"ref_id": "b6", "title": "Perceptron training for a wide-coverage lexicalized-grammar parser", "journal": "", "year": "2007", "authors": "Stephen Clark; James R Curran"}, {"ref_id": "b7", "title": "Three generative, lexicalised models for statistical parsing", "journal": "", "year": "1997", "authors": "M Collins"}, {"ref_id": "b8", "title": "Discriminative reranking for natural language parsing", "journal": "", "year": "2000", "authors": "M Collins"}, {"ref_id": "b9", "title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "journal": "", "year": "2002", "authors": "M Collins"}, {"ref_id": "b10", "title": "Bilexical grammars and their cubic-time parsing algorithms", "journal": "Kluwer Academic Publishers", "year": "2000", "authors": "J Eisner"}, {"ref_id": "b11", "title": "Efficient, feature-based, conditional random field parsing", "journal": "", "year": "2008", "authors": "J R Finkel; A Kleeman; C D Manning"}, {"ref_id": "b12", "title": "Exponentiated gradient algorithms for log-linear structured prediction", "journal": "", "year": "2007", "authors": "A Globerson; T Koo; X Carreras; M Collins"}, {"ref_id": "b13", "title": "Forest reranking: Discriminative parsing with non-local features", "journal": "", "year": "2008", "authors": "L Huang"}, {"ref_id": "b14", "title": "Estimators for stochastic unification-based grammars", "journal": "", "year": "1999", "authors": "M Johnson; S Geman; S Canon; Z Chi; S Riezler"}, {"ref_id": "b15", "title": "Simple semi-supervised dependency parsing", "journal": "", "year": "2008", "authors": "Terry Koo; Xavier Carreras; Michael Collins"}, {"ref_id": "b16", "title": "Conditonal random fields: Probabilistic models for segmenting and labeling sequence data", "journal": "", "year": "2001", "authors": "J Lafferty; A Mccallum; F Pereira"}, {"ref_id": "b17", "title": "Online learning of approximate dependency parsing algorithms", "journal": "", "year": "2006", "authors": "R Mcdonald; F Pereira"}, {"ref_id": "b18", "title": "Online large-margin training of dependency parsers", "journal": "", "year": "2005", "authors": "R Mcdonald; K Crammer; F Pereira"}, {"ref_id": "b19", "title": "Improved inference for unlexicalized parsing", "journal": "", "year": "2007", "authors": "S Petrov; D Klein"}, {"ref_id": "b20", "title": "Discriminative loglinear grammars with latent variables", "journal": "", "year": "2007", "authors": "S Petrov; A Pauls; D Klein"}, {"ref_id": "b21", "title": "A maximum entropy model for parsing", "journal": "", "year": "1994", "authors": "A Ratnaparkhi; S Roukos; R Ward"}, {"ref_id": "b22", "title": "Hpsg parsing with shallow dependency constraints", "journal": "", "year": "2007", "authors": "Kenji Sagae; Yusuke Miyao; Jun'ichi Tsujii"}, {"ref_id": "b23", "title": "Incremental ltag parsing", "journal": "", "year": "2005", "authors": "L Shen; A K Joshi"}, {"ref_id": "b24", "title": "Max-margin parsing", "journal": "", "year": "2004", "authors": "B Taskar; D Klein; M Collins; D Koller; C Manning"}, {"ref_id": "b25", "title": "Statistical dependency analysis with support vector machines", "journal": "", "year": "2003", "authors": "H Yamada; Y Matsumoto"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Two example trees.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 2: Two Example Trees S", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ": Results for different methods. PPK07, FKM08,CH2000, CO2000, PK07, CJ05 and H08 are results on section23 of the Penn WSJ treebank, for the models of"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Table showing unlabeled dependency accuracy for sections 23 and 24 of the treebank, using the method of", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Effect of the beam size, controlled by \u03b1, on the performance of the parser on the development set (1,699 sentences). In each case \u03b1 refers to the beam size used in both training and testing the model. \"active\": percentage of dependencies that remain in the beam out of the total number of", "figure_data": "labeled dependencies (1,000 triple labels times 1,138,167 un-labeled dependencies); \"coverage\": percentage of correct de-pendencies in the beam out of the total number of correct de-pendencies. \"oracle F1\": maximum achievable score of con-stituents, given the beam. \"speed\": parsing time in min:secfor the TAG-based model (this figure does not include the timetaken to calculate the marginals using the lower-order model);\"F1\": score of predicted constituents."}], "formulas": [{"formula_id": "formula_0", "formula_text": "y * = arg max y\u2208Y(x) w \u2022 f (x, y)(1)", "formula_coordinates": [1.0, 122.0, 546.6, 169.82, 20.27]}, {"formula_id": "formula_1", "formula_text": "f (x, y) = i,\u03b7 \u2208E(y) e(x, i, \u03b7 ) + h,m,l \u2208D(y) d(x, h, m, l ) (2)", "formula_coordinates": [4.0, 94.21, 319.66, 197.61, 56.29]}, {"formula_id": "formula_2", "formula_text": "f (x, y) = X i,\u03b7 \u2208E(y) e(x, i, \u03b7 ) + X h,m,l \u2208D(y) d(x, h, m, l ) + X h,m,l,s \u2208S(y) s(x, h, m, l, s ) + X h,m,l,g \u2208G(y) g(x, h, m, l, g ) + X h,m,l,q \u2208Q(y) q(x, h, m, l, q )(3)", "formula_coordinates": [4.0, 305.42, 498.38, 221.42, 94.29]}, {"formula_id": "formula_3", "formula_text": "DMAX(x, m) = max h,l \u00b5(x, h, m, l)", "formula_coordinates": [6.0, 109.65, 97.07, 144.67, 17.53]}, {"formula_id": "formula_4", "formula_text": "\u03c0(x) = { h, m, l : \u00b5(x, h, m, l) \u2265 \u03b1DMAX(x, m)}", "formula_coordinates": [6.0, 72.14, 160.21, 229.39, 10.91]}, {"formula_id": "formula_5", "formula_text": "\u00b5(x, h, m, VP VBD NP ) = \u00b5 1 (x, h, m, VP ) \u00d7 \u00b5 2 (x, h, m, VBD ) \u00d7\u00b5 3 (x, h, m, NP )", "formula_coordinates": [6.0, 76.4, 694.94, 211.17, 44.89]}], "doi": ""}