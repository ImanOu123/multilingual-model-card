{"title": "Factorization with Uncertainty", "authors": "Michal Irani; P Anandan", "pub_date": "", "abstract": "Factorization using Singular Value Decomposition (SVD) is often used for recovering 3D shape and motion from feature correspondences across multiple views. SVD is powerful at nding the global solution to the associated least-square-error minimization problem. However, this is the correct error to minimize only when the x and y positional errors in the features are uncorrelated and identically distributed. But this is rarely the case in real data. Uncertainty in feature position depends on the underlying spatial intensity structure in the image, which has strong directionality to it. Hence, the proper measure to minimize is covariance-weighted squared-error (or the Mahalanobis distance). In this paper, we describe a new approach t o c o variance-weighted factorization, which can factor noisy feature correspondences with high degree of directional uncertainty i n to structure and motion. Our approach is based on transforming the raw-data into a covariance-weighted data space, where the components of noise in the di erent directions are uncorrelated and identically distributed. Applying SVD to the transformed data now m i nimizes a meaningful objective function. We empirically show that our new algorithm gives good results for varying degrees of directional uncertainty. In particular, we s h o w that unlike other SVD-based factorization algorithms, our method does not degrade with increase in directionality of uncertainty, e v en in the extreme when only normal-ow data is available. It thus provides a uni ed approach for treating corner-like points together with points along linear structures in the image.", "sections": [{"heading": "Introduction", "text": "Factorization is often used for recovering 3D shape and motion from feature correspondences across multiple frames 8, 4{7]. Singular Value Decomposition (SVD) directly obtains the global minimum of the squared-error between the noisy data and the model. This is in contrast to iterative non-linear optimization methods which m a y c o n verge to a local minimum. However, SVD requires that the noise in the x and y positions of features are uncorrelated and have identical distributions. But, it is rare that the positional errors of feature tracking algorithms are uncorrelated in their x and y coordinates. Quality of feature matching depends on the spatial variation of the intensity pattern around each feature. This a ects the positional inaccuracy both in the x and in the y components in a correlated fashion. This dependency can be modeled by directional uncertainty ( w h i c h v aries from point to point, as is shown in Fig. 1).\nWhen the uncertainty in a feature position is isotropic, but di erent features have di erent variances, then scalar-weighted SVD can be used to minimize a The uncertainty in all directions is small, since the underlying intensity structure shows variation in multiple directions. (b) Uncertainty of a point on a at curve, almost a straight line. Note that the uncertainty in the direction of the line is large, while the uncertainty in the direction perpendicular to the line is small. This is because it is hard to localize the point along the line. weighted squared error measure 1]. However, under directional uncertainty n o i s e assumptions (which is the case in reality), the error minimized by SVD is no longer meaningful. The proper measure to minimize is the covariance-weighted error (the Mahalanobis distance). This issue was either ignored by researchers 8, 4, 1, 7], or else was addressed using other minimization approaches 3,5]. Morris and Kanade 3] have suggested a uni ed approach for recovering the 3D structure and motion from point and line features, by taking into account t h e i r directional uncertainty. H o wever, they solve their objective function using an iterative non-linear minimization scheme. The line factorization algorithm of Quan and Kanade 5] is SVD-based. However, it requires a preliminary step of 2D projective reconstruction, which is necessary for rescaling the line directions in the image before further factorization can be applied. This step is then followed by three sequential SVD minimization steps, each applied to di erent i n termediate results. This algorithm requires at least seven di erent directions of lines.\nIn this paper we present a new approach to factorization, which i n troduces directional uncertainty into the SVD minimization framework. The input is the noisy positions of image features and their inverse covariance matrices which represent the uncertainty in the data. Following the approach of Irani 2], we write the image position vectors as row v ectors, rather than as column vectors as is typically done in factorization methods. This allows us to use the inverse covariance matrices to transform the input position vectors into a new data space (the \\covariance-weighted space\"), where the noise is uncorrelated and identically distributed. In the new covariance-weighted data space, corner points and points on lines all have the same reliability, and their new positional components are uncorrelated. (This is in contrast with the original data space, where corner points and points on lines had di erent reliability, and their x and y components were correlated.)\nWe apply SVD factorization to the covariance-weighted data to obtain a global optimum. This minimizes the Mahalanobis distance in the original data space. However, the covariance-weighted data space has double the rank of the original data space. To obtain the required additional rank-halving, we use a least-squares minimization step within the double-rank subspace.\nOur approach a l l o ws the recovery of 3D motion for all frames and the 3D shape for all points, even when the uncertainty o f p o i n t position is highly elliptic (for example, point on a line). It can handle reliable corner-like p o i n t correspondences and partial correspondences of points on lines (e.g., normal ow), all within a single SVD like framework. In fact, we can handle extreme cases when the only image data available is normal ow.\nIrani 2] used con dence-weighted subspace projection directly on spatiotemporal brightness derivatives, in order to constrain multi-frame correspondence estimation. The con dences she used encoded directional uncertainty a ssociated with each pixel. That formulation can be seen a special case of the covariance-weighted factorization presented in this paper.\nOur approach t h us extends the use of the powerful SVD factorization technique with a proper treatment of directional uncertainty in the data. Di erent input features can have di erent directional uncertainties with di erent ellipticities (i.e., di erent c o variance matrices). However, our extension does not allow arbitrary changes in the uncertainty of a single feature over multiple frames. We are currently able to handle the case where the change in the covariance matrices of all of the image features can be modeled by a global 2D a ne transformation, which v aries from frame to frame.\nThe rest of the paper is organized as follows: Section 2 contains a short review of SVD factorization and formulates the problem for the case of directional uncertainty. Section 3 describes the transition from the raw data space, where noise is correlated and non-uniform, to the covariance-weighted data space, where noise is uniform and uncorrelated, giving rise to meaningful SVD subspace projection. Section 4 explains how the covariance-weighted data can be factored into 3D motion and 3D shape. Section 5 extends the solution presented in Sections 3 and 4, to a more general case when the directional uncertainty o f a p o i n t c hanges across views. Section 6 provides experimental results and empirical comparison of our factorization method to other common SVD factorization methods. Section 7 concludes the paper.", "publication_ref": ["b2", "b4"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Problem Formulation", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "SVD Factorization", "text": "A set of P points are tracked across F images with coordinates (u 0 f p v 0 f p ) j f = 1 : : : F p = 1 : : : P . The point coordinates are transformed to objectcentered coordinates by subtracting their center of mass: (u 0 f p v 0 f p ) is replaced by (u f p v f p ) = ( u 0 f p ; u f v 0 f p ; v f ) for all f and p, where u f and v f are the centroids of point positions in each frame: u f = 1 P P p u 0 f p , v f = 1 P P p v 0 f p .\nTwo F P measurement matrices U and V are constructed by s t a c king all the measured correspondences as follows: The rows of M encode the motion for each frame (rotation in the case of orthography), and the columns of S contain the 3D position of each point i n t h e reconstructed scene.\nWhen there are errors in the measurement matrix W, then each position (u f p v f p ) T ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Scalar Uncertainty", "text": "The model in Section 2.1 (as well as in 8]) weights equally the contribution of each point feature to the nal shape and motion matrices. However, when the noise E f p is isotropic, but with di erent variances for the di erent points f 2 p j p = 1 Pg, t h e n E f p N(0 2 p I 2 2 ). In such cases, applying SVD to the weighted-matrix W = W ;1 , where ;1 = diag( ;1 1 ::: ;1 P ), will minimize the correct error function: ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Directional Uncertainty", "text": "So far we h a ve assumed that the noise in u f p is uncorrelated with the noise in v f p . In real image sequences, however, this is not the case. Tracking algorithms introduce non-uniform correlated error in the tracked positions of points which depends on the local image structure. For example, a corner point p will be tracked with high reliability both in u f p and in v f p , while a p o i n t p on a line will be tracked with high reliability in the direction of the gradient (\\normal ow\"), but with low reliability in the tangent direction (see Fig. 1). This leads to non-uniform correlated noise in u f p and v f p . W e model the correlated noise E f p by: E f p N(0 Q ;1 f p ) where Q f p is the 2 2 i n verse covariance matrix of the noise at point p in image-frame f. T h e c o variance matrix determines an ellipse whose major and minor axes indicate the directional uncertainty in the location (u f p v f p ) T of a point p in frame f (see Fig. 1, as well as 3] for some examples).\nAssuming that the noise at di erent points is independent, then the maximum likelihood solution is obtained by nding matrices M and S which minimize the following objective function:\nErr(M S) = P f p (E T f p Q f p E f p ) = X f p (u f p ; m T f s p ) (v f p ; n T f s p ) Q f p \" u f p ; m T f s p v f p ; n T f s p #! : (1)\nEq. ( 1) implies that in the case of directional uncertainty, the metric that we w ant to use in the minimization is the Mahalanobis distance, and not the Frobenius (least-squares) norm, w h i c h is the distance minimized by the SVD process. Morris and Kanade 3] have addressed this problem and suggested an approach to recovering M and S which is based on minimizing the Mahalanobis distance. However, their approach uses an iterative non-linear minimization scheme.\nIn the next few sections we present our approach t o SVD-based factorization, which minimizes the Mahalanobis error. Our approach combines the bene ts of SVD-based factorization for getting a good solution, with the proper treatment of directional uncertainty 1 . H o wever, unlike 3], our approach cannot handle arbitrary changes in covariance matrices of a single feature over multiple frames.\nIt can only handle frame-dependent 2D a ne deformations of the covariance matrices across di erent views (see Section 5).\n3 From Raw-Data Space to Covariance-Weighted Space\nIn this section we show how by transforming the noisy data (i.e., correspondences) from the raw-data space to a new covariance-weighted space, we can minimize the Mahalanobis distance de ned in Eq. (1), while still retaining the bene ts of SVD minimization. In particular, we w i l l show that minimizing the Frobenius Norm in the new data space (e.g., via SVD) is equivalent to minimizing the Mahalanobis distance i n t h e r aw-data space. This transition is made possible by rearranging the raw feature positions in a slightly modi ed matrix form: U j V ] F 2P , namely the matrices U and V stacked horizontally (as opposed to vertically in W = U V , w h i c h is the standard matrix form used in the traditional factorization methods (see Section 2.1)). This modi ed matrix representation is necessary to introduce covariance-weights into the SVD process, and was originally proposed by Irani 2], who used it for applying con dence-weighted subspace projection to spatio-temporal brightness derivatives for computing optical ow across multiple frames.\nFor simplicity, w e start by i n vestigating the simpler case when the directional uncertainty o f a p o i n t d o e s n o t c hange over time (i.e., frames), namely, w h e n t h e 2 2 i n verse covariance matrix Q f p of a point p is frame-independent: 8f Q f p Q p . Later, in Section 5, we will extend the approach to handle the case when the covariance matrices undergo frame-dependent 2D-a ne changes. Because Q p is positive semi-de nite, its eigenvalue decomposition has the form Q p = T , where 2 2 is a real orthonormal matrix, and 2 2 = diag( max min ). Let C p = 1 2 and f p f p ] 1 2 = u f p v f p ] 1 2 C p 2 2 . Therefore, f p is the component o f u f p v f p ] in the direction of the highest certainty (scaled by i t s certainty), and f p is the component in the direction of the lowest certainty (scaled by its certainty). For example, in the case of a point p which l i e s o n a line, f p would correspond to the component in the direction perpendicular to the line (i.e., the direction of the normal ow), and f p would correspond to the component in the direction tangent the line (the direction of in nite uncertainty). In the case of a perfect line (i.e., zero certainty in the direction of the line), then f p = 0. When the position of a point can be determined with nite certainty i n both directions (e.g., for corner points), then C p is a regular matrix. Otherwise, when there is in nite uncertainty in at least one direction (e.g., as in lines or uniform image regions), then C p is singular. Let p , p , u p and v p befourF 1 v ectors corresponding to a point p across all frames:\np = 2 4 1p\n. . .  \nj ] F 2P = U j V ] F 2P C 2P 2P (3\n)\nwhere C is a 2P x 2 P matrix, constructed from all 2 x 2 matrices C p = c p1 c p2 c p3 c p4 (p = 1 P), as follows: Note that matrix contains the components of all point positions in their directions of highest certainty, a n d contains the components of all point positions in their directions of lowest certainty. These directions vary from point t o p o i n t and are independent. Furthermore, f p and f p are also independent, and the noise in those two components is now uncorrelated. This will be shown and used below.\nLet R denote the rank of W = U V 2F P (when W is noiseless, and the camera is an a ne camera, then R 3 see Section 2.1). A review of di erent ranks R for di erent camera and world models can be found in 2]. Then the rank of U and the rank of V is each a t m o s t R. Hence, the rank of U j V ] F 2P is at most 2R (for an a ne camera, in the absence of noise, 2R 6). Therefore, according to Eq. (3), the rank of j ] i s a l s o a t m o s t 2 R.\nThe problem of minimizing the Mahalanobis distance of Eq. (1) can be restated as follows: Given noisy positions (u f p v f p ) T j f = 1 F p = 1 P , nd new positions (b u f p b v f p ) T j f = 1 F p = 1 P that minimize the following error function:\nErr (b u f p b v f p ) T = X f p (u f p ; b u f p ) (v f p ; b v f p ) Q f p u f p ; b u f p v f p ; b v f p : (4)\nBecause Q f p = Q p = C p C T p , w e can rewrite this error term as: \n= X f p (u f p ; b u f p ) (v f p ; b v f p ) C p (u f p ; b u f p ) (v f p ; b v f p ) C p T = k U ; b U j V ; b V ] C k 2 F = k U j V ]C ; b U j b V ] C k 2 F =", "publication_ref": ["b0"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "Summary of the Algorithm", "text": "We summarize the steps of the algorithm:\nStep 1: Project the covariance-weighted data j ] = U j V ]C onto a 2R-dimensional subspace (i.e., a rank-2R matrix) b j b ] (for an a ne camera 2R 6). This step is guaranteed to obtain the closest 2R-dimensional subspace because of the global optimum property o f S V D .\nStep 2: Further enforce the rank-R solution by enforcing that b\nU b V = c M U c M V b S.\nThis additional subspace projection is achieved within the j ] s p a c e , a n d is obtained with simple least squares minimization applied to the linear set of equations (6). Note that the Rank-R subspace obtained by the second step is contained inside the Rank-2R subspace obtained in the rst step. We cannot prove that the optimal Rank-R solution is guaranteed to lie within this Rank-2R subspace. However, the bulk of the optimization task is performed in Step 1, which takes the noisy high-dimensional data into the Rank-2R subspace in an optimal fashion. Moreover, both steps of our algorithm are linear. Our empirical results presented in Section 6 indicate that our two-step algorithm accurately recovers the motion and shape, while taking into account v arying degrees of directional uncertainty.", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}, {"heading": "Frame-Dependent Directional Uncertainty", "text": "So far we h a ve assumed that all frames share the same 2 2 i n verse covariance matrix Q p for a point p, i.e., 8f Q f p Q p and thus C f p C p . This assumption, however, is very restrictive, as image motion induces changes in these matrices.\nFor example, a rotation in the image plane induces a rotation on C f p (for all points p). Similarly, a scaling in the image plane induces a scaling in C f p , a n d so forth for skew in the image plane. (Note, however, that a shift in the image plane does not change C f p .)\nThe assumption 8f C f p C p was needed in order to obtain the separable matrix form of Eq. (3), thus deriving the result that the rank of j ] i s a t most 2R. S u c h a separation can not be achieved for inverse covariance matrices Q f p which c hange arbitrarily and independently. H o wever, a similar result can be obtained for the case when all the inverse covariance matrices of all points change over time in a \\similar way\".\nLet fQ p j p = 1 Pg be \\reference\" inverse covariance matrices of all the points (in Section 5.2 we explain how t h e s e a r e c hosen). Let fC p j p = 1 Pg be de ned such that C p C T p = Q p (C p is uniquely de ned by the eigenvalue decomposition, same as de ned in Section 3). In this section we show that if there exist 2 2 \\deformation\" matrices fA f j f = 1 : : : F g such t h a t : 8p 8f: C f p = A f C p (7) then the approach presented in Sections 3 and 4 still applies. Such 2 2 matrices fA f g can account for global 2D a ne deformations in the image plane (rotation, scale, and skew). Note that while C f p is di erent i n e v ery frame f and at every point p, they are not arbitrary. F or a given point p, a l l i t s 2 2 matrices C f p across all views share the same 2 2 reference matrix C p (which captures the common underlying local image structure and degeneracies in the vicinity of p), while for a g i v en frame (view) f, the matrices C f p of all points within that view share the same 2 2 \\a ne\" deformation A f (which captures the common image distortion induced on the local image structure by the common camera motion). Of course, there are many scenarios in which Eq. (7) will not su ce to model the changes in the inverse covariance matrices. However, the formulation in Eq. ( 7) does cover a wide range of scenarios, and can be used as a rst-order approximation to the actual changes in the inversecovariance matrices in the more general case. In Section 5.2 we discuss how w e choose the matrices fC p g and fA f g.\nWe next show t h a t under the assumptions of Eq. ( 7), the rank of j ] is still at most 2R. Let f p f p ] 1 2 = u f p v f p ] 1 2 C f p 2 2 (this is the same de nition as in Section 3, only here we u s e C f p instead of C p ). Then: V ] is at most 2R. Therefore, the rank of j ] i s a t m o s t 2 R even in the case of \\a ne-deformed\" inverse covariance matrices.\nf p f p ] = u f p v", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Generalized Factorization Algorithm", "text": "The factorization algorithm summarized in Section 4.1 can be easily generalized to handle the case of a ne-deformed directional uncertainty. Given matrices fA f j f = 1 Fg and fC p j p = 1 Pg, such that C f p = A f C p , then the algorithm is as follows:\nStep 0: For each point p and each f r a m e f compute: Given a collection of inverse covariance matrices, fQ f p j f = 1 F p = 1 Pg, Eq. ( 7) is not guaranteed to hold. However, we will look for the optimal collection of matrices fA f j f = 1 Fg and fC p j p = 1 Pg such t h a t the error P f p kC f p ; A f C p k is minimized (where C f p C T f p = Q f p ). These matrices fA f g and fC p g can then be used in the generalized factorization algorithm of Sec- :\nWhen all the C f p 's do satisfy Eq. ( 7), then the rank of E is 2, and it can be factored into the following two rank-2 matrices:\nE = 2 6 4 A 1 . . . A F 3 7 5 2F 2 C 1 j j C N ] 2 2P :\nWhen the entries of E (the matrices fC f p g) do not exactly satisfy Eq. ( 7), then we r e c o ver an optimal set of f b A f g and f b C p g (and hence b\nC f p = b A f b C p )\n, by applying SVD to the 2F 2P matrix E, and setting to zero all but the two highest singular values. Note that fA f g and fC p g are determined only up to a global 2 2 a ne transformation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Results", "text": "This section describes our experimental evaluation of the covariance weighted factorization algorithm described in this paper. In particular, we demonstrate two k ey properties of this algorithm: (i) that its factorization of multi-frame position data into shape and motion is accurate regardless of the degree of ellipticity in the uncertainty of the data { i.e., whether the data consists of \\corner-like\" points, \\line-like\" points (i.e., points that lie on linear image structures), or both, and (ii) that in particular, the shape recovery is completely unhampered even when the positional uncertainty of a feature point along one direction is very large (even in nite, such as in the direction of pure normal ow). We a l s o contrast its performance with two \\bench-marks\" { regular SVD (with no uncertainty t a k en into account see Section 2.1) and scalar-weighted SVD, which allows a scalar uncertainty (see Section 2.2). We performed experiments with synthetically generated data, in order to obtain a quantitative comparison of the di erent methods against ground truth under varying conditions.\nIn our experiments, we randomly generated 3D points and a ne motion matrices to create ground-truth positional data of multiple features in multiple frames. We then added elliptic Gaussian noise to this data. We v aried the ellipticity of the noise to go gradually from being fully circular to highly elliptic, up to the extreme case when the uncertainty a t e a c h point is in nite in one of the directions.\nSpeci cally, we varied the shape of the uncertainty ellipse by varying the parameter r = p max = min , where max and min correspond to the major and minor axes of the uncertainty ellipse (these are the eigenvalues of the covariance matrix of the noise in feature positions). In the rst set of experiments, the same value r was used for all the points for a given run of the experiment. The orientation of the ellipse for each point was chosen independently at random.\nIn addition, we included a set of trials in which min = 0 ( r = 1) for all the points. This corresponds to the case when only \\normal ow\" information is available (i.e., in nite uncertainty along the tangential direction).\nWe ran 20 trials for each setting of the parameter r . F or each t r i a l o f our experiment, we randomly created a cloud of 100 3D-points, with uniformly distributed coordinates. This de ned the ground-truth shape matrix S. W e r a ndomly created 20 a ne motion matrices, which together de ne the ground-truth motion matrix M. The a ne motion matrices were used to project each o f t h e 100 points into the di erent views, to generate the noiseless feature positions.\nFor each trial run of the experiment, for each p o i n t in our input dataset, we randomly generated image positional noise f p with directional uncertainty as speci ed above. The noise in the direction of max (the least uncertain direction) varied between 1% and 2% of the feature positions, whereas the noise in the direction of min (the most uncertain direction), varied between 1% and 30% of the feature positions. This noise vector was added to the true position vector (u f p v f p ) T to create the noisy input matrices U and V .\nThe noisy input data was then fed to three algorithm: the covariance-weighted factorization algorithm described in this paper, the regular SVD algorithm, and the scalar-weighted SVD algorithm, for which the scalar-weight a t each point was chosen to be equal to p max min (which is equivalent to taking the determinant of the matrix C f p at each point). Each algorithm outputs a shape matrix\u015c and a motion matrixM. These matrices were then compared against coordinate system as S and M. These errors were then averaged over the 20 trials for each setting of the parameter r . Fig. 2.a and 2.b display the errors in the recovered motion and shape for all three algorithms as a function of the degree of ellipticity in the uncertainty r = p max = min . In this particular case, the behavior of regular SVD and scalar-weighted SVD is very similar, because all points within a single trial (for a particular nite r ), have t h e same con dence (i.e., the same scalar-weight).\nNote how the error in the recovered shape and motion increases rapidly for the regular SVD and for the scalar-weighted SVD, while the covariance-weighted SVD consistently retains very high accuracy (i.e., very small error) in the recovered shape and motion. The error is kept low and uniform even when the elliptical uncertainty is in nite (r = 1 i.e., when only normal-ow information is available). This point is out of the displayed range of this graph, but is visually displayed (for a similar experiment) in Fig. 3.\nIn the second set of experiments, we divided the input set of points into two equal subsets of points. For one subset, we maintained a circular uncertainty through all the runs (i.e., for those points r = 1) , while for the other subset we gradually varied the shape of the ellipse in the same manner as in the previous experiment above (i.e., for those points r is varied from 1 to 1). In this case, the quality of the reconstruction motion for the scalar-weighted SVD showed comparable results (although still inferior) to the covariance-weighted SVD (see Fig. 2.c), and signi cantly better results than the regular SVD. The reason for this behavior is that \\good\" points (with r = 1 ) a r e w eighted highly in the scalar-weighted SVD (as opposed to the regular SVD, where all points are weighted equally). However, while the recovered shape of the circularly symmetric (\\good\") points is quite accurate and degrades gracefully with noise, the error in shape for the \\bad\" elliptical points (points with large r ) increases rapidly with the increase of r , both in the scalar-weighted SVD and in the regular SVD. The error in shape for this group of points (i.e., half of the total number of points) is shown in Fig. 2.d . Note how, in contrast, the covariance-weighted SVD maintains high quality of reconstruction both in the motion and in shape.\nIn order to visualize the results (i.e., visually compare the shape reconstructed by the di erent algorithms for di erent t ypes of noise), we repeated these experiments, but this time instead of applying it to a random shape, we applied it to a well de ned shape { a cube. We used randomly generated a ne motion matrices to determine the positions of 726 cube points in 20 di erent views, then corrupted them with random noise as before. Sample displays of the reconstructed cube by c o variance-weighted algorithm vs. the regular SVD algorithm are shown in Fig. 3 for three interesting cases: case of circular Gaussian noise r = 1 for all the points (Figs. The covariance-weighted SVD (top row) consistently maintains high accuracy of shape recovery, e v en in the case of pure normal-ow. The shape reconstruction obtained by regular SVD (bottom row), on the other hand, degrades severely with the increase in the degree of elliptical uncertainty. Scalar-weighted SVD reconstruction was not added here, because when all the points are equally reliable, then scalar-weighted SVD coincides with regular-SVD (see Fig. 2.b), yet it is not de ned for the case of in nite uncertainty (because then all the weights are equal to zero).", "publication_ref": [], "figure_ref": ["fig_7", "fig_9", "fig_7", "fig_7", "fig_9", "fig_7"], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper we have introduced a new algorithm for performing covarianceweighted factorization of multiframe correspondence data into shape and motion. Unlike the regular SVD algorithms which minimize the Frobenius norm error in the data, or the scalar-weighted SVD which minimizes a scalar-weighted version of that norm, our algorithm minimizes the covariance weighted error (or the Mahalanobis distance). This is the proper measure to minimize when the uncertainty in feature position is directional. Our algorithm transforms the raw input data int o a c o variance-weighted data space, and applies SVD in this transformed data space, where the Frobenius norm now minimizes a meaningful objective function. This SVD step projects the covariance-weighted data to a 2 R-dimensional subspace. We complete the process with an additional linear estimation step to recover the rank R shape and motion estimates.\nA fundamental advantage of our algorithm is that it can handle input data with any level of ellipticity in the directional uncertainty { i.e., from purely circular uncertainty to highly elliptical uncertainty, e v en including the case of points along lines where the uncertainty along the line direction is in nite. It can also simultaneously use data which c o n tains points with di erent l e v els of directional uncertainty. W e empirically show that our algorithm recovers shape and motion accurately, even when the more conventional SVD algorithms perform poorly. However, our algorithm cannot handle arbitrary changes in the uncertainty of a single feature over multiple frames (views). It can only account f o r frame dependent 2D a ne deformations in the covariance matrices.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Factorization as a rank 1 problem, CVPR'99", "journal": "", "year": "", "authors": "P M Q Aguiar; J M F Moura"}, {"ref_id": "b1", "title": "Multi-frame optical ow estimation using subspace constraints, ICCV'99", "journal": "", "year": "1999", "authors": "M Irani"}, {"ref_id": "b2", "title": "A uni ed factorization algorithm for points, line segments and planes with uncertain models", "journal": "", "year": "1998", "authors": "D D Morris; T Kanade"}, {"ref_id": "b3", "title": "A paraperspective factorization method for shape and motion recovery", "journal": "IEEE Trans. PAMI , V ol", "year": "1997", "authors": "C J Poelman; T Kanade"}, {"ref_id": "b4", "title": "A factorization method for a ne structure from line correspondences, CVPR'96", "journal": "", "year": "1996-06", "authors": "L Quan; T Kanade"}, {"ref_id": "b5", "title": "A ne Analysis of Image Sequences", "journal": "Cambridge University Press", "year": "1995", "authors": "L S Shapiro"}, {"ref_id": "b6", "title": "A factorization based algorithm for multi-image projective structure and motion, ECCV'96, V ol", "journal": "", "year": "", "authors": "P Sturm; B Triggs"}, {"ref_id": "b7", "title": "Shape and motion from image streams under orthography: a factorization method, IJCV , V ol", "journal": "", "year": "1992", "authors": "C Tomasi; T Kanade"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Fig. 1 .1Fig. 1. Directional uncertainty indicated by ellipse. (a) Uncertainty of a sharp corner point. The uncertainty in all directions is small, since the underlying intensity structure shows variation in multiple directions. (b) Uncertainty of a point on a at curve, almost a straight line.Note that the uncertainty in the direction of the line is large, while the uncertainty in the direction perpendicular to the line is small. This is because it is hard to localize the point along the line.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "W ; M S ) k F = kW ; M S k F where S = S ;1 . Applying SVD-factorization to W will give c M and b S , f r o m which b S = b S can be recovered. This approach i s k n o wn as weighted-SVD or weighted-factorization 1].", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "4 FactoringS4the F 2P matrix containing all the fb u f p b v f p g, a n d b j b ] = b U j b V ]C. Therefore:Minimizing the Mahalanobis distance of Eq. (4) is equivalent to nding the rank-2R matrix b j b ] closest to j ] i n t h e Frobenius norm. This minimization can be done by applying SVD subspace projection to the matrix j ], to obtain the optimal b j b ]. This is done by applying SVD to the known j ] matrix, and setting to zero all but the highest 2R singular values. However, note that although optimal, bj b ] = b U j b V ]C isin general a rank-2R matrix, and does not guaranty t h a t c W = b U b V is a rank-R matrix. In Section 4 we show h o w w e complete the process by making the transition from the optimal rank-2R matrix b j b ] to the rank-R solution c Shape and Motion The process of nding the rank-2R b j b ], as outlined in Section 3, does not yet guarantee that the corresponding b U and b V can be decomposed into rankthis section we complete the process and recover c M and b S by enforcing this matrix constraint o n b U and b V . Note that if C were an invertible matrix, then we could have recovered b U j b V ] = b j b ]C ;1 , and then proceeded with applying standard SVD to b U b V to impose the rank-R constraint and recover c M and b S. H o wever, C is in general not invertible (e.g., because of points with high aperture problem). Imposing the rank-R constraint o n b U = c M U b S and b V = c M V b S must therefore be done in the b j b ] space (i.e., without inverting C): b j b ] F 2P = c of b j b ] has the matrix form S 0 0 S . H o wever, if we are able to decompose b j b ] into the matrix form of Eq. (5)can be determined only up to an a ne transformation) will provide the desired rank-R solution. Because b j b ] F 2P is a rank-2R matrix, it can be written as a bilinear product of an F 2R matrix H and a 2R 2P matrix G: b j b ] F 2P = H F 2R G 2R 2P : This decomposition is not unique. For any i n vertible 2R 2R matrix D, b j b ] = (H D ;1 )(DG) is also a valid decomposition. We seek a matrix D which will bring DGinto a form an arbitrary R P matrix. This is a linear system of equations in the unknown components of S and D. W e therefore linearly solve for S and D, f r o m which the desired solution is obtained by: b", "figure_data": ""}, {"figure_label": "22", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "2 : 222Use the same algorithm (Steps 1 and 2) as in Section 4.1 (with the matrices fC p j p = 1 Pg, but apply it to the matrix e U j e V ] instead of U j V ]. These two steps yield the matrices b Choosing the Matrices A f and C p", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "tion 5. 1 .1Let E bea2F 2P matrix which c o n tains all the individual 2 2 matrices fC f p j f = 1 F p = 1 Pg:", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Fig. 2 .2Fig. 2. Plots of error in motion and shape w.r.t. ground truth for all three algorithms (Covariance-weighted S V D , s c alar-weighted SVD, regular SVD). (a,b) Plots for the case when all points have the similar elliptical uncertainty, which is gradually increased ( a = motion erro r , b = s h a p e error). (c,d) Plots for the case when half of the points havexed circular uncertainty, and the other half have varying elliptical uncertainty (c = motion error, d = shape error). The displayed shape e r r or in this case is the computed error for the group of elliptic points (the \\bad\" points).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "3.a and 3.d), case of elliptic Gaussian noise with r = 2 0 (Figs. 3.b and 3.e), and the case of pure \\normal ow\", when min = 0 ( r = 1) (Figs. 3.c and 3.f). (For visibility s a k e, only 3 sides of the cube are displayed).", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Fig. 3 .3Fig. 3. Reconstructed shape of the cube by the Covariance-weighted SVD (top row) vs. the regular SVD (bottom row). For visibility sake, only 3 sides of the cube are displayed. (a,d) case of circularly symmetric noise. (b,e) case of elliptical noise with ratio r = 2 0 . (c,f) case of pure \\normal ow\" (only line-like features) r = 1. Note that the quality of shape r econstruction of the covariance weighted factorization method does not degrade with the increase in the degree of ellipticity, while in the case of regular SVD, it degrades rapidly.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "Err(M S) = P f p (E T f p Q f p E f p ) = X f p (u f p ; m T f s p ) (v f p ; n T f s p ) Q f p \" u f p ; m T f s p v f p ; n T f s p #! : (1)", "formula_coordinates": [5.0, 134.76, 215.46, 345.84, 55.51]}, {"formula_id": "formula_1", "formula_text": "p = 2 4 1p", "formula_coordinates": [6.0, 164.28, 341.82, 41.64, 33.07]}, {"formula_id": "formula_2", "formula_text": "j ] F 2P = U j V ] F 2P C 2P 2P (3", "formula_coordinates": [6.0, 240.96, 516.0, 235.4, 18.06]}, {"formula_id": "formula_3", "formula_text": ")", "formula_coordinates": [6.0, 476.36, 517.35, 4.24, 13.8]}, {"formula_id": "formula_4", "formula_text": "Err (b u f p b v f p ) T = X f p (u f p ; b u f p ) (v f p ; b v f p ) Q f p u f p ; b u f p v f p ; b v f p : (4)", "formula_coordinates": [7.0, 141.72, 317.22, 338.88, 33.07]}, {"formula_id": "formula_5", "formula_text": "= X f p (u f p ; b u f p ) (v f p ; b v f p ) C p (u f p ; b u f p ) (v f p ; b v f p ) C p T = k U ; b U j V ; b V ] C k 2 F = k U j V ]C ; b U j b V ] C k 2 F =", "formula_coordinates": [7.0, 152.76, 376.14, 311.04, 87.09]}, {"formula_id": "formula_6", "formula_text": "U b V = c M U c M V b S.", "formula_coordinates": [9.0, 411.36, 165.78, 69.72, 28.03]}, {"formula_id": "formula_7", "formula_text": "f p f p ] = u f p v", "formula_coordinates": [10.0, 217.92, 244.59, 82.68, 15.34]}, {"formula_id": "formula_8", "formula_text": "E = 2 6 4 A 1 . . . A F 3 7 5 2F 2 C 1 j j C N ] 2 2P :", "formula_coordinates": [11.0, 253.2, 490.5, 154.32, 53.28]}, {"formula_id": "formula_9", "formula_text": "C f p = b A f b C p )", "formula_coordinates": [11.0, 405.72, 560.58, 57.66, 15.55]}], "doi": ""}