{"title": "Optimal and Adaptive Algorithms for Online Boosting", "authors": "Alina Beygelzimer; Satyen Kale; Haipeng Luo", "pub_date": "2015-02-10", "abstract": "We study online boosting, the task of converting any weak online learner into a strong online learner. Based on a novel and natural definition of weak online learnability, we develop two online boosting algorithms. The first algorithm is an online version of boost-by-majority. By proving a matching lower bound, we show that this algorithm is essentially optimal in terms of the number of weak learners and the sample complexity needed to achieve a specified accuracy. This optimal algorithm is not adaptive, however. Using tools from online loss minimization, we derive an adaptive online boosting algorithm that is also parameter-free, but not optimal. Both algorithms work with base learners that can handle example importance weights directly, as well as by rejection sampling examples with probability defined by the booster. Results are complemented with an experimental study.", "sections": [{"heading": "Introduction", "text": "We study online boosting, the task of boosting the accuracy of any weak online learning algorithm. The theory of boosting in the batch setting has been studied extensively in the literature and has led to a huge practical success. See the book by Schapire and Freund [2012] for a thorough discussion.\nOnline learning algorithms receive examples one by one, updating the predictor immediately after seeing each new example. In contrast to the batch setting, online learning algorithms typically don't make any stochastic assumptions about the data they observe. They are often much faster, more memory-efficient, and apply to situations where the best predictor changes over time as new examples keep coming in.\nGiven the success of boosting in batch learning, it is natural to ask about the possibility of applying boosting to online learning. Indeed, there has already been some work on online boosting [Oza and Russell, 2001, Grabner and Bischof, 2006, Liu and Yu, 2007, Grabner et al., 2008, Chen et al., 2012.\nFrom a theoretical viewpoint, recent work by Chen et al. [2012] is perhaps most interesting. They generalized the batch weak learning assumption to the online setting, and made a connection between online boosting and batch boosting that produces smooth distributions over the training examples. The resulting algorithm is guaranteed to achieve an arbitrarily small error rate as long as the number of weak learners and the number of examples are sufficiently large. No assumptions need to be made about how the data is generated. Indeed, the data can even be generated by an adversary.\nTable 1: Comparisons of our results with those of Chen et al. [2012], assuming, as in their paper, that the weak learner is derived from an online learning algorithm with an O( \u221a T ) regret bound.\nAlgorithm N T Optimal? Adaptive? Online BBM\n(Section 3.1) O( 1 \u03b3 2 ln 1 \u01eb )\u00d5( 1 \u01eb\u03b3 2 ) \u221a \u00d7 AdaBoost.OL (Section 4) O( 1 \u01eb\u03b3 2 )\u00d5( 1 \u01eb 2 \u03b3 4 ) \u00d7 \u221a\nOSBoost [Chen et al., 2012] O( 1 \u01eb\u03b3 2 )\u00d5( 1 \u01eb\u03b3 2 ) \u00d7 \u00d7\nWe present a new online boosting algorithm, based on the boost-by-majority (BBM) algorithm of [Freund, 1995]. This algorithm, called Online BBM, improves upon the work of Chen et al. [2012] in several different aspects:\n1. our assumption on online weak learners is weaker and can be seen as a direct online analogue of the weak learning assumption in standard batch boosting, 2. our algorithm doesn't require weighted online learning, instead using a sampling technique similar to the one used in boosting by filtering in the batch setting [see for example, Freund, 1992, Bradley andSchapire, 2008], and 3. our algorithm is optimal in the sense that no online boosting algorithm can achieve the same error rate with less weak learners or less examples asymptotically (see the lower bounds in Section 3.2).\nA quantitative comparison of our results with those of Chen et al. [2012] appears in Table 1, where N and T represent the number 1 of weak learners and examples needed to achieve error rate \u01eb, and \u03b3 stands for a similar concept of the \"edge\" of the weak learning oracle as in the batch setting (smaller \u03b3 means more inaccurate weak learners).\nA clear drawback of all the algorithms mentioned above is lack of adaptivity. A simple interpretation of this drawback is that all these algorithms require using \u03b3, an unknown quantity, as a parameter. More importantly, this also means that the algorithm treats each weak learner equally and ignores the fact that some weak learners are actually doing better than the others. The best example of adaptive boosting algorithm is the well-known parameter-free AdaBoost algorithm [Freund and Schapire, 1997], where each weak learner is naturally weighted by how accurate it is. In fact, adaptivity is known to be one of the key features that lead to the practical success of AdaBoost, and therefore should also be essential to the performance of online boosting algorithms. In Section 4, we thus propose AdaBoost.OL, an adaptive and parameter-free online boosting algorithm. As shown in Table 1, AdaBoost.OL is theoretically suboptimal in terms of N and T . However, empirically it generally outperforms OSBoost and sometimes even beats the optimal algorithm Online BBM (see Section 5).\nOur techniques are also very different from those of Chen et al. [2012], which rely on the smooth boosting algorithm of Servedio [2003]. As far as we know, all other work on smooth boosting [Bshouty and Gavinsky, 2003, Bradley and Schapire, 2008, Barak et al., 2009] cannot be easily generalized to the online setting, necessitating completely different methods not relying on smooth distributions. Our Online BBM algorithm builds on top of a potential based family that arises naturally in the batch setting as approximate minimax optimal algorithms for so-called drifting games [Schapire, 2001, Luo andSchapire, 2014]. The decomposition of each example in that framework naturally allows us to generalize it to the online setting where example comes one by one. On the other hand, AdaBoost.OL is derived by viewing boosting from a different angle: loss minimization [Mason et al., 2000, Schapire andFreund, 2012]. The theory of online loss minimization is the key tool for developing AdaBoost.OL.\nFinally, in Section 5, experiments on benchmark data are conducted to show that our new algorithms indeed improve over previous work.", "publication_ref": ["b20", "b18", "b12", "b15", "b13", "b6", "b6", "b10", "b3", "b11", "b21", "b4", "b3", "b0", "b16", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Setup and Assumptions", "text": "We describe the formal setup of the task of online classification by boosting. At each time step t = 1, . . . , T , an adversary chooses an example (x t , y t ) \u2208 X \u00d7 {\u22121, 1}, where X is the domain, and reveals x t to the online learner. The learner makes a prediction on its label\u0177 t \u2208 {\u22121, 1}, and suffers the 0-1 loss 1{\u0177 t = y t }. As is usual with online algorithms, this prediction may be randomized.\nFor parameters \u03b3 \u2208 (0, 1 2 ), \u03b4 \u2208 (0, 1), and a constant S > 0, the learner is said to be a weak online learner with edge \u03b3 and excess loss S if, for any T and for any input sequence of examples (x t , y t ) for t = 1, 2, . . . , T chosen adaptively, it generates predictions\u0177 t such that with probability at least 1 \u2212 \u03b4,\nT t=1 1{\u0177 t = y t } \u2264 ( 1 2 \u2212 \u03b3)T + S. (1)\nThe excess loss requirement is necessary since an online learner can't be expected to predict with any accuracy with too few examples. Essentially, the excess loss S yields a kind of sample complexity bound: the weak learner starts obtaining a distinct edge of \u2126(\u03b3) over random guessing when T \u226b S \u03b3 . Typically, the dependence of the high probability bound on \u03b4 is polylogarithmic in 1 \u03b4 ; thus in the following we will avoid explicitly mentioning \u03b4.\nFor a given parameter \u01eb > 0, the learner is said to be a strong online learner with error rate \u01eb if it satisfies the same conditions as a weak online learner except that its edge is 1 2 \u2212 \u01eb, or in other words, the fraction of mistakes made, asymptotically, is \u01eb. Just as for the weak learner, the excess loss S yields a sample complexity bound: the fraction of mistakes made by the strong learner becomes O(\u01eb) when T \u226b S \u01eb . Our main theorem is the following: Theorem 1. Given a weak online learning algorithm with edge \u03b3 and excess loss S and any target error rate \u01eb > 0, there is a strong online learning algorithm with error rate \u01eb which uses O( 1 \u03b3 2 ln( 1 \u01eb )) copies of the weak online learner, and has excess loss\u00d5\n( S \u03b3 + 1 \u03b3 2 ); thus its sample complexity is O( 1 \u01eb ( S \u03b3 + 1 \u03b3 2 )). Furthermore, if S \u2265\u03a9( 1 \u03b3 )\n, then the number of weak online learners is optimal up to constant factors, and the sample complexity is optimal up to polylogarithmic factors.\nThe requirement that S \u2265\u03a9( 1 \u03b3 ) in the lower bound is not very stringent; this is precisely the excess loss one obtains when using standard online learning algorithms with regret bound O( \u221a T ), as is explained in the discussion following Lemma 2. Furthermore, since we require the bound (1) to hold with high probability, typical analyses of online learning algorithms will have an\u00d5( \u221a T ) deviation term, which also leads to S \u2265\u03a9( 1 \u03b3 ). As the theorem indicates, the strong online learner (hereafter referred to as \"booster\") works by maintaining N copies of the weak online learner, for some positive integer N to be specified later. Denote the weak online learners WL i for i = 1, 2, . . . , N . At time step t, the prediction of i-th weak online learner is given by WL i (x t ) \u2208 {\u22121, 1}. Note the slight abuse of notation here: WL i is not a function, rather it is an algorithm with an internal state that is updated as it is fed training examples. Thus, the prediction WL i (x t ) depends on the internal state of WL i , and for notational convenience we avoid reference to the internal state.\nIn each round t, the booster works by taking a weighted majority vote of the weak learners' predictions. Specifically, the booster maintains weights \u03b1 i t \u2208 R for i = 1, . . . , N corresponding to each weak learner, and its final prediction will then be 2\u0177 t = sign( N i=1 \u03b1 i t WL i (x t )), where sign(\u2022) is 1 if the argument is nonnegative and \u22121 otherwise. After making the prediction, the true label y t is revealed by the environment. The booster then updates WL i by passing the training example (x t , y t ) to WL i with a carefully chosen sampling probability p i t (and not passing the example with the remaining probability). The sampling probability p i t is obtained by computing a weight w i t and setting 3 p i t = w i t w i \u221e , where w i = w i 1 , w i 2 , . . . , w i T . At the same time the booster updates \u03b1 i t as well, and then it is ready to make a prediction for the next round.\nWe introduce some more notation to ease the presentation. Let z i t = y t WL i (x t ) and\ns i t = s i\u22121 t + \u03b1 i t z i t with s 0 t = 0. Define z i = z i 1 , z i 2 , . . . , z i T .\nFinally, a martingale concentration bound using (1) yields the following bound (proof deferred to Appendix A). The bound can be seen as a weighted version of (1) which is necessary for the rest of the analysis.\nLemma 1. There is a constantS = 2S +\u00d5( 1 \u03b3 ) such that for any T , with high probability, for every weak learner WL i we have\nw i \u2022 z i \u2265 \u03b3 w i 1 \u2212S w i \u221e .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Handling Importance Weights", "text": "Typical online learning algorithms can handle importance weighted examples: each example (x t , y t ) comes with a weight p t \u2208 [0, 1], and the loss on that example is scaled by p t , i.e. the loss for predicting\u0177 t is p t 1{\u0177 t = y t }. Consider the following natural extension to the definition of online weak learners which incorporates importance weighted examples: we now require that for any sequence of weighted examples (x t , y t ) with weight p t \u2208 [0, 1] for t = 1, 2, . . . , T , the online learner generates predictions\u0177 t such that with probability at least 1 \u2212 \u03b4,\nT t=1 p t 1{\u0177 t = y t } \u2264 ( 1 2 \u2212 \u03b3) T t=1 p t + S.(2)\nHaving access to such a weak learner makes the boosting algorithm simpler: we now simply pass every example (x t , y t ) to every weak learner WL i using the probability p i t =\nw i t w i \u221e as importance\nweights. The advantage is that the bound (2) immediately implies the following inequality for any weak learner WL i , which can be seen as a (stronger) analogue of Lemma 1.\nw i \u2022 z i \u2265 2\u03b3 w i 1 \u2212 2S w i \u221e .(3)\nSince the analysis depends only on the bound in Lemma 1, if we use the importance-weighted version of the boosting algorithm, then we can simply use inequality (3) instead in the analysis, which gives a slightly tighter version of Theorem 1, viz. the excess loss can now be bounded by O( S \u03b3 ).\nIn the rest of the paper, for simplicity of exposition we assume that the p i t 's are used as sampling probabilities rather than importance weights, and give the analysis using the bound from Lemma 1. In experiments, however, using the p i t 's as importance weights rather than sampling probabilities led to better performance.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Discussion of Weak Online Learning Assumption", "text": "We now justify our definition of weak online learning, viz. inequality (1). In the standard batch boosting case, the corresponding weak learning assumption (see for example Schapire and Freund [2012]) made is that there is an algorithm which, given a training set of examples and an arbitrary distribution on it, generates a hypothesis that has error at most 1 2 \u2212 \u03b3 on the training data under the given distribution. This statement can be interpreted as making the following two implicit assumptions:\n1. (Richness.) Given an edge parameter \u03b3 \u2208 (0, 1 2 ), there is a set of hypotheses, H, such that given any training set (possibly, a multiset) of examples U , there is some hypothesis h \u2208 H with error at most 1 2 \u2212 \u03b3, i.e.\n(x,y)\u2208U 1{h(x) = y} \u2264 ( 1 2 \u2212 \u03b3)|U |.\n2. (Agnostic Learnability.) For any \u01eb \u2208 (0, 1), there is an algorithm which, given any training set (possibly, a multiset) of examples U , can compute a nearly optimal hypothesis h \u2208 H, i.e.\n(x,y)\u2208U 1{h(x) = y} \u2264 inf h \u2032 \u2208H (x,y)\u2208U 1{h \u2032 (x) = y} + \u01eb|U |.\nOur weak online learning assumption can be seen as arising from a direct generalization of the above two assumptions to the online setting. Namely, the richness assumption stays the same, whereas the agnostic learnability of H assumption is replaced by an agnostic online learnability of H assumption (c.f. Ben-David et al. [2009]). I.e., there is an online learning algorithm which, given any sequence of examples, (x t , y t ) for t = 1, 2, . . . , T , generates predictions\u0177 t such that\nT t=1 1{\u0177 t = y t } \u2264 inf h\u2208H T t=1 1{h(x t ) = y t } + R(T ),\nwhere R : N \u2192 R + is the regret, a non-decreasing, sublinear function of the number of prediction periods T . Since online learning algorithms are typically randomized, we assume the above bound holds with high probability. The following lemma shows that richness and agnostic online learnability immediately imply our online weak learning assumption (1):\nLemma 2. Suppose the sequence of examples (x t , y t ) is obtained from a data set for which there exists a hypothesis class H that is both rich for edge parameter 2\u03b3 and agnostically online learnable with regret R(\u2022). Then, the agnostic online learning algorithm for H satisfies the weak learning assumption (1), with edge \u03b3 and excess loss S = max T (R(T ) \u2212 \u03b3T ).\nProof. For the given sequence of examples (x t , y t ) for t = 1, 2, . . . , T , the richness with edge parameter 2\u03b3 and agnostic online learnability assumptions on H imply that with high probability, the predictions\u0177 t generated by the agnostic online learning algorithm for H satisfy\nT t=1 1{\u0177 t = y t } \u2264 ( 1 2 \u2212 2\u03b3)T + R(T ).\nIt only remains to show that Various agnostic online learning algorithms are known that have a regret bound of O( T ln( 1 \u03b4 )); for example, a standard experts algorithm on a finite hypothesis space such as Hedge. If we use such an online learning algorithm as a weak online learner, then a simple calculation implies, via Lemma 2, that it has excess loss \u0398(\n( 1 2 \u2212 2\u03b3)T + R(T ) \u2264 ( 1 2 \u2212 \u03b3)T + S, or equivalently, R(T ) \u2264 \u03b3T + S,\nln( 1 \u03b4 ) \u03b3\n). Thus, by Theorem 1, we obtain an online boosting algorithm with near-optimal sample complexity.", "publication_ref": ["b20"], "figure_ref": [], "table_ref": []}, {"heading": "An Optimal Algorithm", "text": "In this section, we generalize a family of potential based batch boosting algorithms to the online setting. With a specific potential, an online version of boost-by-majority is developed with optimal number of weak learners and near-optimal sample complexity. Matching lower bounds will be shown at the end of the section.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Potential Based Family and Boost-By-Majority", "text": "In the batch setting, many boosting algorithms can be understood in a unified framework called drifting games [Schapire, 2001]. Here, we generalize the analysis and propose a potential based family of online boosting algorithms.\nPick a sequence of N + 1 non-increasing potential functions \u03a6 i (s) such that\n\u03a6 N (s) \u2265 1{s \u2264 0}, \u03a6 i\u22121 (s) \u2265 ( 1 2 \u2212 \u03b3 2 )\u03a6 i (s \u2212 1) + ( 1 2 + \u03b3 2 )\u03a6 i (s + 1). (4\n)\nThen the algorithm is simply to set \u03b1 i t = 1 and\nw i t = 1 2 (\u03a6 i (s i\u22121 t \u2212 1) \u2212 \u03a6 i (s i\u22121 t + 1)).\nThe following theorem states the error rate bound of this general scheme.\nLemma 3. For any T and N , with high probability, the number of mistakes made by the algorithm described above is bounded as follows:\nT t=1 1{\u0177 t = y t } \u2264 \u03a6 0 (0)T +S i w i \u221e .\nProof. The key property of the algorithm is that for any fixed i and t, one can verify the following:\n\u03a6 i (s i t )+w i t (z i t \u2212\u03b3) = \u03a6 i (s i\u22121 t +z i t )+w i t (z i t \u2212\u03b3) = ( 1 2 \u2212 \u03b3 2 )\u03a6 i (s i\u22121 t \u22121)+( 1 2 + \u03b3 2 )\u03a6 i (s i\u22121 t +1) \u2264 \u03a6 i\u22121 (s i\u22121 t )\nby plugging the formula of w i t , realizing that z i t can only be \u22121 or 1, and using the definition of \u03a6 i\u22121 (s) from Eq. (4). t = 1 to T , we get\nT t=1 \u03a6 i (s i t ) + w i \u2022 z i \u2212 \u03b3 w i 1 \u2264 T t=1 \u03a6 i\u22121 (s i\u22121 t ).\nUsing Lemma 1, we get\nT t=1 \u03a6 i (s i t ) \u2264 T t=1 \u03a6 i\u22121 (s i\u22121 t ) +S w i \u221e .\nwhich relates the sums of all examples' potential for two successive weak learners. We can therefore apply this inequality iteratively to arrive at:\nT t=1 \u03a6 N (s N t ) \u2264 T t=1 \u03a6 0 (0) +S w i \u221e .\nThe proof is completed by noting that\n\u03a6 N (s N t ) \u2265 1{s N t \u2264 0} = 1{\u0177 t = y t } since y t\u0177t = sign(s N t ) by definition.\nNote that theS w i \u221e term becomes a penalty for the final error rate. Therefore, we naturally want this penalty term to be relatively small. This is not necessarily true for any choice of the potential function. For example, if \u03a6 i (s) is the exponential potential that leads to a variant of AdaBoost in the batch setting [see Schapire and Freund, 2012, Chap. 13], then the weight w i t could be exponentially large.\nFortunately, there is indeed a set of potential functions that produces small weights, which, in the batch setting, corresponds to an algorithm called boost-by-majority (BBM) Freund [1995]. All we need to do is to let Eq. (4) hold with equality, and direct calculation shows:\n\u03a6 i (s) = \u230a N\u2212i\u2212s 2 \u230b k=0 N \u2212 i k 1 2 + \u03b3 2 k 1 2 \u2212 \u03b3 2 N \u2212i\u2212k ,and\nw i t = 1 2 N \u2212 i k i t 1 2 + \u03b3 2 k i t 1 2 \u2212 \u03b3 2 N \u2212i\u2212k i t (5\n)\nwhere\nk i t = \u230a N \u2212i\u2212s i\u22121 t +1 2\n\u230b and n k is defined to be 0 if k < 0 or k > n. In other words, imagine flipping a biased coin whose probability of heads is 1 2 + \u03b3 2 for N \u2212 i times. Then \u03a6 i (s) is exactly the probability of seeing at most (N \u2212 i \u2212 s)/2 heads and w i t is half of the probability of seeing k i t heads. We call this algorithm Online BBM. The pseudocode is given in Algorithm 1.\nOne can see that the weights produced by this algorithm are small since trivially w i t \u2264 1/2. To get a better result, however, we need a better estimate of w i \u221e stated in the following lemma.", "publication_ref": ["b19", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 1 Online BBM", "text": "1: for t = 1 to T do 2:\nReceive example x t .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "3:", "text": "Predict\u0177 t = sign( N i=1 WL i (x t )), receive label y t .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "4:", "text": "Set s 0 t = 0.\n5:\nfor i = 1 to N do 6:\nSet s i t = s i\u22121 t + y t WL i (x t ).\n7:\nSet k i t = \u230a N \u2212i\u2212s i\u22121 t +1 2 \u230b. 8: Set w i t = N \u2212i k i t 1 2 + \u03b3 2 k i t 1 2 \u2212 \u03b3 2 N \u2212i\u2212k i t .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "9:", "text": "Pass training example (x t , y t ) to WL i with probability p i t =\nw i t w i \u221e .\n10:\nend for 11: end for Lemma 4. If w i t is defined as in Eq. (5), then we have\nw i t = O(1/ \u221a N \u2212 i) for any i < N .\nThis lemma was essentially proven before by Freund [1993, Lemma 2.3.10]. We give an alternative and simpler proof in Appendix B in the supplementary material by using Berry-Esseen theorem directly. We are now ready to state the main results of Online BBM.\nTheorem 2. For any T and N , with high probability, the number of mistakes made by the Online BBM algorithm is bounded as follows:\nexp(\u2212 1 2 N \u03b3 2 )T +\u00d5( \u221a N (S + 1 \u03b3 )).(6)\nThus, in order to achieve error rate \u01eb, it suffices to use N = \u0398( 1 \u03b3 2 ln 1 \u01eb ) weak learners, which gives an excess loss bound of\u0398( S \u03b3 + 1 \u03b3 2 ).\nProof. A direct application of Hoeffding's inequality gives \u03a6 0 (0) \u2264 exp(\u2212N \u03b3 2 /2). With Lemma 4 we have\ni w i \u221e = O N \u22121 i=1 1 \u221a N \u2212 i = O( \u221a N ).\nApplying Lemma 3 proves Eq. (6). Now if we set N = 2 \u03b3 2 ln 1 \u01eb , then\nT t=1 1{\u0177 t = y t } \u2264 \u01ebT +\u00d5( \u221a N (S + 1 \u03b3 )) = \u01ebT +\u00d5( S \u03b3 + 1 \u03b3 2 ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Matching Lower Bounds", "text": "We give lower bounds for the number of weak learners and the sample complexity in this section that show that our Online BBM algorithm is optimal up to logarithmic factors.\nTheorem 3. For any \u03b3 \u2208 (0, 1 4 ), S \u2265\nln( 1 \u03b4 )\n\u03b3 , \u03b4 \u2208 (0, 1) and \u01eb \u2208 (0, 1), there is a weak online learning algorithm with edge \u03b3 and excess loss S satisfying (1) with probability at least 1 \u2212 \u03b4, such that to achieve error rate \u01eb, an online boosting algorithm needs at least \u2126( 1 \u03b3 2 ln 1 \u01eb ) weak learners and a sample complexity of \u2126( S \u01eb\u03b3 ) = \u2126( 1 \u01eb ( S \u03b3 + 1 \u03b3 2 )). Proof. The proof of both lower bounds use a similar construction. In either case, all examples' labels are generated uniformly at random from {\u22121, 1}, and in time period t, each weak learner outputs the correct label y t independently of all other weak learners and other examples with a certain probability p t to be specified later. Thus, for any T , by the Azuma-Hoeffding inequality, with probability at least 1 \u2212 \u03b4, the predictions\u0177 t made by the weak learner satisfy\nT t=1 1{y t =\u0177 t } \u2264 T t=1 (1 \u2212 p t ) + 2T ln( 1 \u03b4 ) \u2264 T t=1 (1 \u2212 p t ) + \u03b3T + ln( 1 \u03b4 ) 2\u03b3 (7\n)\nwhere the last inequality follows by the arithmetic mean-geometric mean inequality. We will now carefully choose p t so that inequality (7) implies inequality (1).\nFor the lower bound on the number of weak learners, we set p t = 1 2 + 2\u03b3, so that inequality (7) implies that with probability at least 1 \u2212 \u03b4, the predictions\u0177 t made by the weak learner satisfy\nT t=1 1{y t =\u0177 t } \u2264 ( 1 2 \u2212 \u03b3)T + ln( 1 \u03b4 ) 2\u03b3 \u2264 ( 1 2 \u2212 \u03b3)T + S.\nThus, the weak online learner has edge \u03b3 with excess loss S. In this case, the Bayes optimal output of a booster using N weak learners is to simply take a majority vote of all the weak learners [see for instance Schapire and Freund, 2012, Chap. 13.2.6], and the probability that the majority vote is incorrect is \u0398(exp(\u22128N \u03b3 2 )). Setting this error to \u01eb and solving for N gives the desired lower bound. Now we turn to the lower bound on the sample complexity. We divide the whole process into two phases: for t \u2264 T 0 = S 4\u03b3 , we set p t = 1 2 , and for t > T 0 , we set p t = 1 2 + 2\u03b3. Now, if T \u2264 T 0 , inequality (7) implies that with probability at least 1 \u2212 \u03b4, the predictions\u0177 t made by the weak learner satisfy\nT t=1 1{y t =\u0177 t } \u2264 ( 1 2 + \u03b3)T + ln( 1 \u03b4 ) 2\u03b3 \u2264 ( 1 2 \u2212 \u03b3)T + S (8) using the fact that T \u2264 T 0 = S 4\u03b3 and S \u2265 ln( 1 \u03b4 ) \u03b3 .\nNext, if T > T 0 , let T \u2032 = T \u2212 T 0 , and again inequality (7) implies that with probability at least 1 \u2212 \u03b4, the predictions\u0177 t made by the weak learner satisfy\nT t=1 1{y t =\u0177 t } \u2264 1 2 T 0 + ( 1 2 \u2212 2\u03b3)T \u2032 + \u03b3T + ln( 1 \u03b4 ) 2\u03b3 = ( 1 2 \u2212 \u03b3)T + 2\u03b3T 0 + ln( 1 \u03b4 ) 2\u03b3 \u2264 ( 1 2 \u2212 \u03b3)T + S, (9) since S \u2265 ln( 1 \u03b4 )\n\u03b3 . Inequalities (8) and ( 9) imply that the weak online learner has edge \u03b3 with excess loss S.\nHowever, in the first phase (i.e. t \u2264 T 0 ), since the predictions of the weak learners are uncorrelated with the true labels, it is clear that no matter what the booster does, it makes a mistake with probability 1 2 . Thus, it will make \u2126(T 0 ) mistakes with high probability in the first phase, and thus to achieve \u01eb error rate, it needs at least \u2126(T 0 /\u01eb) = \u2126( S \u01eb\u03b3 ) examples.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "An Adaptive Algorithm", "text": "Although the Online BBM algorithm is optimal, it is unfortunately not adaptive since it requires the knowledge of \u03b3 as a parameter, which is unknown ahead of time. As discussed in the introduction, adaptivity is essential to the practical performance of boosting algorithms such as AdaBoost.\nIn this section we thus study adaptive online boosting algorithms using the theory of online loss minimization as the main tool. It is known that boosting can be viewed as trying to find a linear combination of weak hypotheses to minimize the total loss of the training examples, usually using functional gradient descent [see for details Schapire and Freund, 2012, Chap. 7]. AdaBoost, for instance, minimizes the exponential loss. Here, as discussed before, we intuitively want to avoid using exponential loss since it could lead to large weights. Instead, we will consider logistic loss \u2113(s) = ln(1 + exp(\u2212s)), which results in an algorithm called AdaBoost.L in the batch setting [Schapire and Freund, 2012, Chap. 7].\nIn the online setting, we conceptually define N different \"experts\" giving advice on what to predict on the current example x t . In round t, expert i predicts by combining the first i weak learners:\u0177 i t = sign( i j=1 \u03b1 j t WL j (x t )). Now as in AdaBoost.L, the weight w i t for WL i is obtained by computing the logistic loss of the prediction of expert i \u2212 1, i.e. \u2113(s i\u22121 t ), and then setting w i t to be the negative derivative of the loss:\nw i t = \u2212\u2113 \u2032 (s i\u22121 t ) = 1 1 + exp(s i\u22121 t ) \u2208 [0, 1].\nIn terms of the weight of WL i , i.e. \u03b1 i t , ideally we wish to mimic AdaBoost.L and use a fixed \u03b1 i for all t such that the total logistic loss is minimized:\n\u03b1 i = arg min \u03b1 T t=1 \u2113(s i\u22121 t + \u03b1z i t )\n. Of course this is not possible because \u03b1 i depends on the future unknown examples. Nevertheless, it turns out that we can almost achieve that using tools from online learning theory. Indeed, one of the fundamental topics in online learning is exactly how to perform almost as well as the best fixed choice (\u03b1 i ) in the hindsight.\nSpecifically, it turns out that it suffices to restrict \u03b1 to the feasible set [\u22122, 2]. Then consider the following simple one dimensional online learning problem: on each round t, algorithm predicts \u03b1 i t from a feasible set [\u22122, 2]; the environment then reveals loss function f t (\u03b1) = \u2113(s i\u22121 t + \u03b1z i t ) and the algorithm suffers loss f t (\u03b1 i t ). There are many so-called \"low-regret\" algorithms in the literature (see the survey by Shalev-Shwartz [2011]) for this problem ensuring\nT t=1 f t (\u03b1 i t ) \u2212 min \u03b1\u2208[\u22122,2] T t=1 f t (\u03b1) \u2264 R i T ,\nwhere R i T is sublinear in T so that on average it goes to 0 when T is large and the algorithm is thus doing almost as well as the best constant choice \u03b1 i . The simplest low-regret algorithm in this case is perhaps online gradient descent Zinkevich [2003]:\n\u03b1 i t+1 = \u03a0 \u03b1 i t \u2212 \u03b7 t f \u2032 t (\u03b1 i t ) = \u03a0 \u03b1 i t + \u03b7 t z i t 1 + exp(s i t )\n, where \u03b7 t is a time-varying learning rate and \u03a0 represents projection onto the set [\u22122, 2], i.e., \u03a0(\u2022) = max{\u22122, min{2, \u2022}}. Since the loss function is actually 1-Lipschitz (|f \u2032 t (\u03b1)| \u2264 1), if we set \u03b7 t to be 4/ \u221a t, then standard analysis shows R i T = 4 \u221a T .\nAlgorithm 2 AdaBoost.OL 1: Initialize: \u2200i : v i 1 = 1, \u03b1 i 1 = 0. 2: for t = 1 to T do 3:\nReceive example x t .", "publication_ref": ["b22", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "4:", "text": "for i = 1 to N do 5:\nSet\u0177 i t = sign( i j=1 \u03b1 j t WL j (x t )).\n6: end for 7:\nRandomly pick i t with Pr[i t = i] \u221d v i t .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "8:", "text": "Predict\u0177 t =\u0177 it t , receive label y t .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "9:", "text": "Set s 0 t = 0.\n10:\nfor i = 1 to N do 11:\nSet z i t = y t WL i (x t ).\n12:\nSet s i t = s i\u22121 t + \u03b1 i t z i t .\n13:\nSet \u03b1 i t+1 = \u03a0 \u03b1 i t + \u03b7tz i t 1+exp(s i t )\nwith \u03b7 t = 4/ \u221a t.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "14:", "text": "Pass example (x t , y t ) to WL i with probability 4 p i t = w i t = 1/(1 + exp(s i\u22121 t )).\n15:\nSet v i t+1 = v i t \u2022 exp(\u22121{y t =\u0177 i t }).\n16:\nend for 17: end for Finally, it remains to specify the algorithm's final prediction\u0177 t . In Online BBM, we simply used the advice of expert N . Unfortunately the algorithm described in this section cannot guarantee that expert N will always make highly accurate predictions. However, as we will show in the proof of Theorem 4, the algorithm does ensure that at least one of the N experts will have high accuracy.\nTherefore, what we really need to do is to decide which expert to follow on each round, and try to predict almost as well as the best fixed expert in the hindsight. This is again another classic online learning problem (called expert or hedge problem), and can be solved, for instance, by the well-known Hedge algorithm [Littlestone andWarmuth, 1994, Freund andSchapire, 1997]. The idea is to pick an expert on each round randomly with different importance weights according to their previous performance.\nWe call the final resulting algorithm AdaBoost.OL (O stands for online and L stands for logistic loss), and summarize it in Algorithm 2. Note that as promised, AdaBoost.OL is an adaptive online boosting algorithm and does not require knowing \u03b3 in advance. In fact, in the analysis we do not even assume that the weak learners satisfy the bound (1). Instead, define the quantities \u03b3 i w i \u2022z i 2 w i 1 for each weak learner WL i . This can be interpreted as the (weighted) edge over random guessing that WL i obtains. Note that \u03b3 i may even be negative, which means flipping the sign of WL i 's predictions performs better than random guessing. Nevertheless, the algorithm can still make accurate predictions even with negative \u03b3 i since it will end up choosing negative weights \u03b1 i t in that case. The performance of AdaBoost.OL is provided below.\nTheorem 4. For any T and N , with high probability, the number of mistakes made by AdaBoost.OL is bounded by\n2T i \u03b3 2 i +\u00d5 N 2 i \u03b3 2 i .\nProof. Let the number of mistakes made by expert i be M i T t=1 1{y t =\u0177 i t }, also define M 0 = T for convenience. Note that AdaBoost.OL is using a variant of the Hedge algorithm with 1{y t = y i t } being the loss of expert i on round t (Line 7 and 15). So by standard analysis [see e.g. Cesa-Bianchi and Lugosi, 2006, Corollary 2.3], and the Azuma-Hoeffding inequality, we have with high probability\nT t=1 1{y t =\u0177 t } \u2264 2 min i M i + 2 ln(N ) +\u00d5( \u221a T ). (10\n)\nNow, whenever expert i\u22121 makes a mistake (i.e. s i\u22121 t \u2264 0), we have w i t = 1/(1+exp(s i\u22121 t )) \u2265 1/2 and therefore\nw i 1 \u2265 M i\u22121 /2. (11\n)\nNote that Eq. ( 11) holds even for i = 1 by the definition of M 0 . We now bound the difference between the logistic loss of two successive experts, \u2206 i T t=1 \u2113(s i t ) \u2212 \u2113(s i\u22121 t ) . Online gradient descent (Line 13) ensures that\nT t=1 \u2113(s i t ) \u2264 min \u03b1\u2208[\u22122,2] T t=1 \u2113(s i\u22121 t + \u03b1z i t ) + 4 \u221a T ,(12)\nas discussed previously. On the other hand, direct calculation shows \u2113(s\ni\u22121 t + \u03b1z i t ) \u2212 \u2113(s i\u22121 t ) = ln 1 + w i t (e \u2212\u03b1z i t \u2212 1) \u2264 w i t (e \u2212\u03b1z i t \u2212 1). With \u03c3 i T t=1 w i t w i 1 1{z i t = 1} = 1 2 + \u03b3 i , we thus have min \u03b1\u2208[\u22122,2] T t=1 \u2113(s i\u22121 t + \u03b1z i t ) \u2212 \u2113(s i\u22121 t ) \u2264 min \u03b1\u2208[\u22122,2] w i 1 (\u03c3 i e \u2212\u03b1 + (1 \u2212 \u03c3 i )e \u03b1 \u2212 1) \u2264 \u2212 1 2 w i 1 (2\u03c3 i \u2212 1) 2 (13) = \u22122\u03b3 2 i w i 1 (14\n)\n\u2264 \u2212\u03b3 2 i M i\u22121 . (15\n)\nHere, inequality (13) follows from Lemma 5 and inequality (15) from inequality (11). The above inequality and inequality (12) imply that\n\u2206 i \u2264 \u2212\u03b3 2 i M i\u22121 + 4 \u221a T .\nSumming over i = 1, . . . , N and rearranging gives\nN i=1 \u03b3 2 i M i\u22121 + T t=1 \u2113(s N t ) \u2264 T t=1 \u2113(0) + 4N \u221a T which implies that min i M i \u2264 min i M i\u22121 \u2264 ln(2) i \u03b3 2 i T + 4N i \u03b3 2 i \u221a T since M i \u2264 M 0 for all i, \u2113(s N t )\n\u2265 0 for all t and \u2113(0) = ln(2). Using this bound in inequality (10), we get\nT t=1 1{y t =\u0177 t } \u2264 2 ln(2)T i \u03b3 2 i +\u00d5 N \u221a T i \u03b3 2 i + ln(N ) \u2264 2T i \u03b3 2 i +\u00d5 N 2 i \u03b3 2 i ,\nwhere the last inequality follows from the bound cN\n\u221a T i \u03b3 2 i \u2264 T 2 i \u03b3 2 i + c 2 N 2 2 i \u03b3 2 i\n, where c is the hidde\u00f1\nO(1) factor in the\u00d5( N \u221a T i \u03b3 2 i\n) term, using the arithmetic mean-geometric mean inequality.\nFor the case when the weak learners do satisfy the bound (1), we get the following bound on the number of errors:\nTheorem 5. If the weak learners satisfy (1), then for any T and N , with high probability, the number of mistakes made by AdaBoost.OL is bounded by\n8T \u03b3 2 N +\u00d5 N \u03b3 2 + S \u03b3 ,\nThus, in order to achieve error rate \u01eb, it suffices to use N \u2265 8 \u01eb\u03b3 2 weak learners, which gives an excess loss bound of\u00d5( S \u03b3 + 1 \u01eb\u03b3 4 ). Proof. The proof is on the same lines as that of Theorem 4. The only change is that in inequality ( 14), we use the bound \u03b3 2 i \u2265 \u03b3 2 4 \u2212 \u03b3S 2 w i 1 which follows from Lemma 1 using the fact that a \u2265 b \u2212 c implies a 2 \u2265 b 2 \u2212 2bc for non-negative a, b and c, and the fact that w i \u221e \u2264 1. This leads to the following change in inequality ( 15\n): min \u03b1\u2208[\u22122,2] T t=1 \u2113(s i\u22121 t + \u03b1z i t ) \u2212 \u2113(s i\u22121 t ) \u2264 \u2212 \u03b3 2 4 M i\u22121 + \u03b3S.\nContinuing using this bound in the proof and simplifying, we get the stated bound on the number of errors.\nThe following lemma is a simple calculation:\nLemma 5. For any \u03c3 \u2208 [0, 1], min \u03b1\u2208[\u22122,2] \u03c3e \u2212\u03b1 + (1 \u2212 \u03c3)e \u03b1 \u2264 1 \u2212 1 2 (2\u03c3 \u2212 1) 2 .\nProof. It suffice to prove the bound for \u03c3 \u2265 1 2 ; the bound for \u03c3 < 1 2 follows by simply using the bound for 1 \u2212 \u03c3. For \u03c3 \u2208 [0.5, 0.95], setting \u03b1 = 1 2 ln( \u03c3\n1\u2212\u03c3 ) \u2208 [\u22122, 2] gives \u03c3e \u2212\u03b1 + (1 \u2212 \u03c3)e \u03b1 = 4\u03c3(1 \u2212 \u03c3) \u2264 1 \u2212 1 2 (2\u03c3 \u2212 1) 2 , since \u221a 1 \u2212 x \u2264 1 \u2212 1 2 x for x \u2208 [0, 1]. For \u03c3 \u2208 (0.95, 1], setting \u03b1 = 1 2 ln( 0.95 0.05 ) \u2208 [\u22122, 2] we have \u03c3e \u2212\u03b1 + (1 \u2212 \u03c3)e \u03b1 \u2264 0.95e \u2212\u03b1 + 0.05e \u03b1 = \u221a 0.19 \u2264 1 2 \u2264 1 \u2212 1 2 (2\u03c3 \u2212 1) 2 .\nAlthough the number of weak learners and excess loss for Adaboost.OL are suboptimal, the adaptivity of AdaBoost.OL is an appealing feature and leads to good performance in experiments. The possibility of obtaining an algorithm that is both adaptive and optimal is left as an open question. All experiments were done on a diverse collection of 13 publically available datasets. The datasets come from the UCI repository, KDD Cup challenges, and the HCRC Map Task Corpus. A description of these datasets is given in Table 2. For each dataset, we performed a random split with 80% of the data used for training and the remaining 20% for testing. We tuned the learning rate, the number of weak learners, and the edge parameter \u03b3 (for all but AdaBoost.OL) using progressive validation 0-1 loss on the training set. Reported is the 0-1 loss on the test set.\nIt should be noted that the VW baseline is already a strong learner. The results obtained are given in Table 3. As can be seen, for most datasets, Online BBM had the best performance. The average improvement of Online BBM over the baseline was 5.14%. For AdaBoost.OL, it was 2.57%. Using sampling in AdaBoost.OL (i.e. AdaBoost.OL.S) boosts the average to 2.67%. The average improvement for OSBoost.OCP was 1.98%, followed by OSBoost with 1.13%. \nw i \u2022 z i \u2265 2\u03b3 w i 1 \u2212 2S w i \u221e \u2212\u00d5( w i 1 w i \u221e ) \u2265 2\u03b3 w i 1 \u2212 2S w i \u221e \u2212 \u03b3 w i 1 \u2212\u00d5( w i \u221e \u03b3 ) = \u03b3 w i 1 \u2212 2S w i \u221e \u2212\u00d5( w i \u221e \u03b3\n).\nThe second inequality above follows from the arithmetic mean-geometric mean inequality. This gives us the desired bound. The high probability bound for all weak learners follows by taking a union bound.", "publication_ref": ["b14", "b11"], "figure_ref": [], "table_ref": ["tab_0", "tab_1"]}, {"heading": "B Proof of Lemma 4", "text": "Proof. Let X \u223c B(m, p) be a binomial random variable where m = N \u2212 i and p = 1/2 + \u03b3/2. Also let q = 1 \u2212 p and F X be the CDF of X. By the definition of w i t , we have w i t \u2264 1 2 max k Pr{X = k}. We will approximate X by a Gaussian random variable G \u223c N (mp, mpq) with density function f and CDF F G . Note that\n| Pr{X = k} \u2212 k k\u22121 f (G)dG| = | (F X (k) \u2212 F X (k \u2212 1)) \u2212 (F G (k) \u2212 F G (k \u2212 1)) | \u2264 |F X (k) \u2212 F G (k)| + |F X (k \u2212 1) \u2212 F G (k \u2212 1)|.\nSo by applying Berry-Esseen theorem to the above two CDF differences between X and G, we arrive at\nPr{X = k} \u2212 k k\u22121 f (G)dG \u2264 2C(p 2 + q 2 ) \u221a mpq ,\nwhere C is the universal constant stated in Berry-Esseen theorem. It remains to point out that\nPr{X = k} \u2264 k k\u22121 f (G)dG + 2C(p 2 + q 2 ) \u221a mpq \u2264 max G\u2208R f (G) + 2C(p 2 + q 2 ) \u221a mpq = 1 \u221a 2\u03c0mpq + 2C(p 2 + q 2 ) \u221a mpq = O 1 \u221a m , since pq = 1/4 \u2212 \u03b3 2 /4 \u2265 3/16.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Proof of Lemma 1", "text": "Proof. Fix a weak learner, say WL i . Let U = {t : (x t , y t ) passed to WL i }.\nSince inequality (1) holds even for adaptive adversaries, with high probability we have\nNow fix the internal randomness\nis the expectation conditioned on all the randomness of the booster until (and not including) round t. Define \u03c3 = T t=1 p i t . We now show using martingale concentration bounds that with high probability,\nand\nHere, the\u00d5(\u2022) notation suppresses dependence on log log(T ).\nTo prove inequality (17), consider the martingale difference sequence\nNote that |X t | \u2264 1, and the conditional variance satisfies\nThen, by Lemma 2 of Bartlett et al. [2008], for any \u03b4 < 1/e and assuming T \u2265 4, with probability at least 1 \u2212 log 2 (T )\u03b4, we have T ) . This implies inequality (17). Inequality ( 18) is proved similarly. Note that these high probability bounds are conditioned on the internal randomness of WL i . By taking an expectation of this conditional probability over the internal randomness of WL i , we conclude that inequalities (17) and ( 18) hold with high probability unconditionally.\nVia a union bound, inequalities ( 16), ( 17) and (18) all hold simultaneously with high probability, which implies that", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "The uniform hardcore lemma via approximate bregman projections", "journal": "", "year": "2009", "authors": "Boaz Barak; Moritz Hardt; Satyen Kale"}, {"ref_id": "b1", "title": "High-probability regret bounds for bandit online linear optimization", "journal": "", "year": "2008", "authors": "L Peter; Varsha Bartlett; Thomas Dani; Sham Hayes; Alexander Kakade; Ambuj Rakhlin;  Tewari"}, {"ref_id": "b2", "title": "Agnostic Online Learning", "journal": "", "year": "2009", "authors": "Shai Ben-David; D\u00e1vid P\u00e1l; Shai Shalev-Shwartz"}, {"ref_id": "b3", "title": "FilterBoost: Regression and classification on large datasets", "journal": "", "year": "2008", "authors": "Joseph K Bradley; Robert E Schapire"}, {"ref_id": "b4", "title": "On boosting with polynomially bounded distributions", "journal": "The Journal of Machine Learning Research", "year": "2003", "authors": "H Nader; Dmitry Bshouty;  Gavinsky"}, {"ref_id": "b5", "title": "Prediction, Learning, and Games", "journal": "Cambridge University Press", "year": "2006", "authors": "Nicol\u00f2 Cesa; - Bianchi; G\u00e1bor Lugosi"}, {"ref_id": "b6", "title": "An Online Boosting Algorithm with Theoretical Justifications", "journal": "", "year": "2012", "authors": "Hsuan-Tien Shang-Tse Chen; Chi-Jen Lin;  Lu"}, {"ref_id": "b7", "title": "Boosting with Online Binary Learners for the Multiclass Bandit Problem", "journal": "", "year": "2014", "authors": "Hsuan-Tien Shang-Tse Chen; Chi-Jen Lin;  Lu"}, {"ref_id": "b8", "title": "An improved boosting algorithm and its implications on learning complexity", "journal": "", "year": "1992-07", "authors": "Yoav Freund"}, {"ref_id": "b9", "title": "Data Filtering and Distribution Modeling Algorithms for Machine Learning", "journal": "", "year": "1993", "authors": "Yoav Freund"}, {"ref_id": "b10", "title": "Boosting a weak learning algorithm by majority", "journal": "Information and Computation", "year": "1995", "authors": "Yoav Freund"}, {"ref_id": "b11", "title": "A decision-theoretic generalization of on-line learning and an application to boosting", "journal": "Journal of Computer and System Sciences", "year": "1997-08", "authors": "Yoav Freund; Robert E Schapire"}, {"ref_id": "b12", "title": "On-line boosting and vision", "journal": "", "year": "2006", "authors": "Helmut Grabner; Horst Bischof"}, {"ref_id": "b13", "title": "Semi-supervised on-line boosting for robust tracking", "journal": "", "year": "2008", "authors": "Helmut Grabner; Christian Leistner; Horst Bischof"}, {"ref_id": "b14", "title": "The weighted majority algorithm", "journal": "Information and Computation", "year": "1994", "authors": "Nick Littlestone; Manfred K Warmuth"}, {"ref_id": "b15", "title": "Gradient feature selection for online boosting", "journal": "", "year": "2007", "authors": "Xiaoming Liu; Ting Yu"}, {"ref_id": "b16", "title": "A Drifting-Games Analysis for Online Learning and Applications to Boosting", "journal": "", "year": "2014", "authors": "Haipeng Luo; Robert E Schapire"}, {"ref_id": "b17", "title": "Functional gradient techniques for combining hypotheses", "journal": "MIT Press", "year": "2000", "authors": "Llew Mason; Jonathan Baxter; Peter Bartlett; Marcus Frean"}, {"ref_id": "b18", "title": "Online bagging and boosting", "journal": "", "year": "2001", "authors": "C Nikunj; Stuart Oza;  Russell"}, {"ref_id": "b19", "title": "Drifting games", "journal": "", "year": "2001-06", "authors": "Robert E Schapire"}, {"ref_id": "b20", "title": "Boosting: Foundations and Algorithms", "journal": "MIT Press", "year": "2012", "authors": "Robert E Schapire; Yoav Freund"}, {"ref_id": "b21", "title": "Smooth boosting and learning with malicious noise", "journal": "Journal of Machine Learning Research", "year": "2003", "authors": "Rocco A Servedio"}, {"ref_id": "b22", "title": "Online learning and online convex optimization. Foundations and Trends in Machine Learning", "journal": "", "year": "2011", "authors": "Shai Shalev-Shwartz"}, {"ref_id": "b23", "title": "Online convex programming and generalized infinitesimal gradient ascent", "journal": "", "year": "2003", "authors": "Martin Zinkevich"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "which is true by the definition of S. This concludes the proof.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Below, d is the number of unique features in the dataset, and s is the average number of features per example.While the focus of this paper is a theoretical investigation of online boosting, we have also performed experiments to evaluate our algorithms.We extended the Vowpal Wabbit open source machine learning system VW to include the algorithms studied in this paper. We used VW's default base learning algorithm as our weak learner, tuning only the learning rate. The online boosting algorithms implemented were Online BBM, AdaBoost.OL, OSBoost (using uniform weighting on the weak learners) and OSBoost.OCP from[Chen et al., 2012], all using importance weighted examples in VW. We also implemented AdaBoost.OL.S, which is the version of AdaBoost.OL where examples sent to VW are sampled rather than weighted.", "figure_data": "Datasetinstancessd20news18,845 93.9 101,631a9a48,841 13.9123activity165,632 18.520adult48,842 12.0105bio145,750 73.474census299,284 32.0401covtype581,011 11.954letter20,000 15.616maptaskcoref158,546 40.45,944nomao34,465 82.3174poker946,799 10.010rcv1781,265 75.743,001vehv2binary299,254 48.61055 Experiments"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Performance of various online boosting algorithms on various datasets. The lowest loss attained for each dataset is bolded. The baseline is the loss obtained by running the weak learner, VW, on the data. \u221e and 1{WL i (x t ) = y t } =", "figure_data": "DatasetVW baseline Online BBM AdaBoost.OL AdaBoost.OL.S OSBoost.OCP OSBoost20news0.08120.07750.07770.07770.07910.0801a9a0.15090.14950.14970.14970.15090.1505activity0.01330.01140.01280.01270.01300.0133adult0.15430.15260.15360.15360.15390.1544bio0.00350.00310.00320.00320.00330.0034census0.04710.04690.04690.04690.04690.0470covtype0.25630.23470.24950.24500.24700.2521letter0.22950.19230.20780.20780.21480.2150maptaskcoref0.10910.10770.10830.10830.10930.1091nomao0.06410.06270.06350.06350.06270.0633poker0.45550.43120.45550.45550.45550.4555rcv10.04870.04850.04840.04840.04880.0488vehv2binary0.02920.02860.02910.02910.02840.0286"}], "formulas": [{"formula_id": "formula_0", "formula_text": "(Section 3.1) O( 1 \u03b3 2 ln 1 \u01eb )\u00d5( 1 \u01eb\u03b3 2 ) \u221a \u00d7 AdaBoost.OL (Section 4) O( 1 \u01eb\u03b3 2 )\u00d5( 1 \u01eb 2 \u03b3 4 ) \u00d7 \u221a", "formula_coordinates": [2.0, 160.92, 134.02, 271.65, 53.93]}, {"formula_id": "formula_1", "formula_text": "T t=1 1{\u0177 t = y t } \u2264 ( 1 2 \u2212 \u03b3)T + S. (1)", "formula_coordinates": [3.0, 233.28, 376.03, 306.63, 33.92]}, {"formula_id": "formula_2", "formula_text": "( S \u03b3 + 1 \u03b3 2 ); thus its sample complexity is O( 1 \u01eb ( S \u03b3 + 1 \u03b3 2 )). Furthermore, if S \u2265\u03a9( 1 \u03b3 )", "formula_coordinates": [3.0, 72.0, 604.75, 912.42, 37.16]}, {"formula_id": "formula_3", "formula_text": "s i t = s i\u22121 t + \u03b1 i t z i t with s 0 t = 0. Define z i = z i 1 , z i 2 , . . . , z i T .", "formula_coordinates": [4.0, 72.0, 334.51, 467.96, 28.2]}, {"formula_id": "formula_4", "formula_text": "w i \u2022 z i \u2265 \u03b3 w i 1 \u2212S w i \u221e .", "formula_coordinates": [4.0, 238.68, 426.55, 134.66, 20.72]}, {"formula_id": "formula_5", "formula_text": "T t=1 p t 1{\u0177 t = y t } \u2264 ( 1 2 \u2212 \u03b3) T t=1 p t + S.(2)", "formula_coordinates": [4.0, 218.52, 569.23, 321.39, 34.04]}, {"formula_id": "formula_6", "formula_text": "w i t w i \u221e as importance", "formula_coordinates": [4.0, 445.32, 625.2, 94.72, 17.7]}, {"formula_id": "formula_7", "formula_text": "w i \u2022 z i \u2265 2\u03b3 w i 1 \u2212 2S w i \u221e .(3)", "formula_coordinates": [5.0, 233.16, 107.35, 306.75, 20.72]}, {"formula_id": "formula_8", "formula_text": "(x,y)\u2208U 1{h(x) = y} \u2264 ( 1 2 \u2212 \u03b3)|U |.", "formula_coordinates": [5.0, 242.28, 413.07, 154.58, 25.72]}, {"formula_id": "formula_9", "formula_text": "(x,y)\u2208U 1{h(x) = y} \u2264 inf h \u2032 \u2208H (x,y)\u2208U 1{h \u2032 (x) = y} + \u01eb|U |.", "formula_coordinates": [5.0, 198.84, 486.26, 241.58, 26.33]}, {"formula_id": "formula_10", "formula_text": "T t=1 1{\u0177 t = y t } \u2264 inf h\u2208H T t=1 1{h(x t ) = y t } + R(T ),", "formula_coordinates": [5.0, 194.16, 599.95, 224.54, 34.04]}, {"formula_id": "formula_11", "formula_text": "T t=1 1{\u0177 t = y t } \u2264 ( 1 2 \u2212 2\u03b3)T + R(T ).", "formula_coordinates": [6.0, 221.52, 179.59, 169.82, 34.04]}, {"formula_id": "formula_12", "formula_text": "( 1 2 \u2212 2\u03b3)T + R(T ) \u2264 ( 1 2 \u2212 \u03b3)T + S, or equivalently, R(T ) \u2264 \u03b3T + S,", "formula_coordinates": [6.0, 72.0, 237.87, 317.18, 40.2]}, {"formula_id": "formula_13", "formula_text": "ln( 1 \u03b4 ) \u03b3", "formula_coordinates": [6.0, 252.0, 324.39, 20.33, 22.44]}, {"formula_id": "formula_14", "formula_text": "\u03a6 N (s) \u2265 1{s \u2264 0}, \u03a6 i\u22121 (s) \u2265 ( 1 2 \u2212 \u03b3 2 )\u03a6 i (s \u2212 1) + ( 1 2 + \u03b3 2 )\u03a6 i (s + 1). (4", "formula_coordinates": [6.0, 193.44, 554.62, 341.86, 35.21]}, {"formula_id": "formula_15", "formula_text": ")", "formula_coordinates": [6.0, 535.3, 563.02, 4.61, 10.91]}, {"formula_id": "formula_16", "formula_text": "w i t = 1 2 (\u03a6 i (s i\u22121 t \u2212 1) \u2212 \u03a6 i (s i\u22121 t + 1)).", "formula_coordinates": [6.0, 299.16, 590.35, 171.26, 20.72]}, {"formula_id": "formula_17", "formula_text": "T t=1 1{\u0177 t = y t } \u2264 \u03a6 0 (0)T +S i w i \u221e .", "formula_coordinates": [6.0, 211.08, 656.59, 190.7, 34.16]}, {"formula_id": "formula_18", "formula_text": "\u03a6 i (s i t )+w i t (z i t \u2212\u03b3) = \u03a6 i (s i\u22121 t +z i t )+w i t (z i t \u2212\u03b3) = ( 1 2 \u2212 \u03b3 2 )\u03a6 i (s i\u22121 t \u22121)+( 1 2 + \u03b3 2 )\u03a6 i (s i\u22121 t +1) \u2264 \u03a6 i\u22121 (s i\u22121 t )", "formula_coordinates": [7.0, 72.0, 95.35, 468.03, 21.2]}, {"formula_id": "formula_19", "formula_text": "T t=1 \u03a6 i (s i t ) + w i \u2022 z i \u2212 \u03b3 w i 1 \u2264 T t=1 \u03a6 i\u22121 (s i\u22121 t ).", "formula_coordinates": [7.0, 192.48, 156.67, 227.9, 34.04]}, {"formula_id": "formula_20", "formula_text": "T t=1 \u03a6 i (s i t ) \u2264 T t=1 \u03a6 i\u22121 (s i\u22121 t ) +S w i \u221e .", "formula_coordinates": [7.0, 211.2, 211.99, 190.46, 34.04]}, {"formula_id": "formula_21", "formula_text": "T t=1 \u03a6 N (s N t ) \u2264 T t=1 \u03a6 0 (0) +S w i \u221e .", "formula_coordinates": [7.0, 221.88, 286.75, 169.1, 33.92]}, {"formula_id": "formula_22", "formula_text": "\u03a6 N (s N t ) \u2265 1{s N t \u2264 0} = 1{\u0177 t = y t } since y t\u0177t = sign(s N t ) by definition.", "formula_coordinates": [7.0, 72.0, 351.07, 317.33, 37.64]}, {"formula_id": "formula_23", "formula_text": "\u03a6 i (s) = \u230a N\u2212i\u2212s 2 \u230b k=0 N \u2212 i k 1 2 + \u03b3 2 k 1 2 \u2212 \u03b3 2 N \u2212i\u2212k ,and", "formula_coordinates": [7.0, 72.0, 515.76, 360.26, 58.53]}, {"formula_id": "formula_24", "formula_text": "w i t = 1 2 N \u2212 i k i t 1 2 + \u03b3 2 k i t 1 2 \u2212 \u03b3 2 N \u2212i\u2212k i t (5", "formula_coordinates": [7.0, 199.68, 572.4, 335.62, 31.6]}, {"formula_id": "formula_25", "formula_text": ")", "formula_coordinates": [7.0, 535.3, 583.78, 4.61, 10.91]}, {"formula_id": "formula_26", "formula_text": "k i t = \u230a N \u2212i\u2212s i\u22121 t +1 2", "formula_coordinates": [7.0, 104.16, 609.0, 83.19, 23.8]}, {"formula_id": "formula_27", "formula_text": "Set k i t = \u230a N \u2212i\u2212s i\u22121 t +1 2 \u230b. 8: Set w i t = N \u2212i k i t 1 2 + \u03b3 2 k i t 1 2 \u2212 \u03b3 2 N \u2212i\u2212k i t .", "formula_coordinates": [8.0, 77.88, 168.0, 227.54, 41.68]}, {"formula_id": "formula_28", "formula_text": "w i t w i \u221e .", "formula_coordinates": [8.0, 405.24, 206.28, 26.66, 17.82]}, {"formula_id": "formula_29", "formula_text": "w i t = O(1/ \u221a N \u2212 i) for any i < N .", "formula_coordinates": [8.0, 341.4, 265.66, 164.02, 27.41]}, {"formula_id": "formula_30", "formula_text": "exp(\u2212 1 2 N \u03b3 2 )T +\u00d5( \u221a N (S + 1 \u03b3 )).(6)", "formula_coordinates": [8.0, 227.04, 374.86, 312.87, 23.73]}, {"formula_id": "formula_31", "formula_text": "i w i \u221e = O N \u22121 i=1 1 \u221a N \u2212 i = O( \u221a N ).", "formula_coordinates": [8.0, 210.0, 475.82, 198.38, 38.65]}, {"formula_id": "formula_32", "formula_text": "T t=1 1{\u0177 t = y t } \u2264 \u01ebT +\u00d5( \u221a N (S + 1 \u03b3 )) = \u01ebT +\u00d5( S \u03b3 + 1 \u03b3 2 ).", "formula_coordinates": [8.0, 170.52, 544.99, 271.82, 33.92]}, {"formula_id": "formula_33", "formula_text": "ln( 1 \u03b4 )", "formula_coordinates": [9.0, 253.68, 71.55, 20.21, 15.36]}, {"formula_id": "formula_34", "formula_text": "T t=1 1{y t =\u0177 t } \u2264 T t=1 (1 \u2212 p t ) + 2T ln( 1 \u03b4 ) \u2264 T t=1 (1 \u2212 p t ) + \u03b3T + ln( 1 \u03b4 ) 2\u03b3 (7", "formula_coordinates": [9.0, 141.24, 213.19, 394.06, 34.04]}, {"formula_id": "formula_35", "formula_text": ")", "formula_coordinates": [9.0, 535.3, 224.38, 4.61, 10.91]}, {"formula_id": "formula_36", "formula_text": "T t=1 1{y t =\u0177 t } \u2264 ( 1 2 \u2212 \u03b3)T + ln( 1 \u03b4 ) 2\u03b3 \u2264 ( 1 2 \u2212 \u03b3)T + S.", "formula_coordinates": [9.0, 184.8, 310.39, 243.38, 33.92]}, {"formula_id": "formula_37", "formula_text": "T t=1 1{y t =\u0177 t } \u2264 ( 1 2 + \u03b3)T + ln( 1 \u03b4 ) 2\u03b3 \u2264 ( 1 2 \u2212 \u03b3)T + S (8) using the fact that T \u2264 T 0 = S 4\u03b3 and S \u2265 ln( 1 \u03b4 ) \u03b3 .", "formula_coordinates": [9.0, 72.0, 469.87, 467.91, 65.12]}, {"formula_id": "formula_38", "formula_text": "T t=1 1{y t =\u0177 t } \u2264 1 2 T 0 + ( 1 2 \u2212 2\u03b3)T \u2032 + \u03b3T + ln( 1 \u03b4 ) 2\u03b3 = ( 1 2 \u2212 \u03b3)T + 2\u03b3T 0 + ln( 1 \u03b4 ) 2\u03b3 \u2264 ( 1 2 \u2212 \u03b3)T + S, (9) since S \u2265 ln( 1 \u03b4 )", "formula_coordinates": [9.0, 72.0, 560.83, 467.91, 66.68]}, {"formula_id": "formula_39", "formula_text": "w i t = \u2212\u2113 \u2032 (s i\u22121 t ) = 1 1 + exp(s i\u22121 t ) \u2208 [0, 1].", "formula_coordinates": [10.0, 212.04, 324.58, 187.7, 28.86]}, {"formula_id": "formula_40", "formula_text": "\u03b1 i = arg min \u03b1 T t=1 \u2113(s i\u22121 t + \u03b1z i t )", "formula_coordinates": [10.0, 329.76, 375.07, 156.49, 15.56]}, {"formula_id": "formula_41", "formula_text": "T t=1 f t (\u03b1 i t ) \u2212 min \u03b1\u2208[\u22122,2] T t=1 f t (\u03b1) \u2264 R i T ,", "formula_coordinates": [10.0, 222.84, 522.19, 167.3, 34.04]}, {"formula_id": "formula_42", "formula_text": "\u03b1 i t+1 = \u03a0 \u03b1 i t \u2212 \u03b7 t f \u2032 t (\u03b1 i t ) = \u03a0 \u03b1 i t + \u03b7 t z i t 1 + exp(s i t )", "formula_coordinates": [10.0, 185.28, 616.15, 227.31, 29.48]}, {"formula_id": "formula_43", "formula_text": "Set s i t = s i\u22121 t + \u03b1 i t z i t .", "formula_coordinates": [11.0, 112.32, 236.83, 97.22, 14.96]}, {"formula_id": "formula_44", "formula_text": "Set \u03b1 i t+1 = \u03a0 \u03b1 i t + \u03b7tz i t 1+exp(s i t )", "formula_coordinates": [11.0, 112.32, 251.04, 133.97, 19.62]}, {"formula_id": "formula_45", "formula_text": "Set v i t+1 = v i t \u2022 exp(\u22121{y t =\u0177 i t }).", "formula_coordinates": [11.0, 112.32, 285.43, 156.26, 20.12]}, {"formula_id": "formula_46", "formula_text": "2T i \u03b3 2 i +\u00d5 N 2 i \u03b3 2 i .", "formula_coordinates": [11.0, 261.96, 643.35, 96.5, 29.65]}, {"formula_id": "formula_47", "formula_text": "T t=1 1{y t =\u0177 t } \u2264 2 min i M i + 2 ln(N ) +\u00d5( \u221a T ). (10", "formula_coordinates": [12.0, 196.2, 141.07, 338.78, 34.04]}, {"formula_id": "formula_48", "formula_text": ")", "formula_coordinates": [12.0, 534.98, 152.26, 4.81, 10.91]}, {"formula_id": "formula_49", "formula_text": "w i 1 \u2265 M i\u22121 /2. (11", "formula_coordinates": [12.0, 270.6, 208.03, 264.38, 20.72]}, {"formula_id": "formula_50", "formula_text": ")", "formula_coordinates": [12.0, 534.98, 210.1, 4.81, 10.91]}, {"formula_id": "formula_51", "formula_text": "T t=1 \u2113(s i t ) \u2264 min \u03b1\u2208[\u22122,2] T t=1 \u2113(s i\u22121 t + \u03b1z i t ) + 4 \u221a T ,(12)", "formula_coordinates": [12.0, 203.52, 278.95, 336.27, 33.92]}, {"formula_id": "formula_52", "formula_text": "i\u22121 t + \u03b1z i t ) \u2212 \u2113(s i\u22121 t ) = ln 1 + w i t (e \u2212\u03b1z i t \u2212 1) \u2264 w i t (e \u2212\u03b1z i t \u2212 1). With \u03c3 i T t=1 w i t w i 1 1{z i t = 1} = 1 2 + \u03b3 i , we thus have min \u03b1\u2208[\u22122,2] T t=1 \u2113(s i\u22121 t + \u03b1z i t ) \u2212 \u2113(s i\u22121 t ) \u2264 min \u03b1\u2208[\u22122,2] w i 1 (\u03c3 i e \u2212\u03b1 + (1 \u2212 \u03c3 i )e \u03b1 \u2212 1) \u2264 \u2212 1 2 w i 1 (2\u03c3 i \u2212 1) 2 (13) = \u22122\u03b3 2 i w i 1 (14", "formula_coordinates": [12.0, 72.0, 322.51, 467.96, 120.68]}, {"formula_id": "formula_53", "formula_text": ")", "formula_coordinates": [12.0, 534.98, 424.54, 4.81, 10.91]}, {"formula_id": "formula_54", "formula_text": "\u2264 \u2212\u03b3 2 i M i\u22121 . (15", "formula_coordinates": [12.0, 297.24, 440.31, 237.74, 20.64]}, {"formula_id": "formula_55", "formula_text": ")", "formula_coordinates": [12.0, 534.98, 442.3, 4.81, 10.91]}, {"formula_id": "formula_56", "formula_text": "\u2206 i \u2264 \u2212\u03b3 2 i M i\u22121 + 4 \u221a T .", "formula_coordinates": [12.0, 250.92, 492.46, 110.06, 28.37]}, {"formula_id": "formula_57", "formula_text": "N i=1 \u03b3 2 i M i\u22121 + T t=1 \u2113(s N t ) \u2264 T t=1 \u2113(0) + 4N \u221a T which implies that min i M i \u2264 min i M i\u22121 \u2264 ln(2) i \u03b3 2 i T + 4N i \u03b3 2 i \u221a T since M i \u2264 M 0 for all i, \u2113(s N t )", "formula_coordinates": [12.0, 72.0, 547.15, 335.89, 108.08]}, {"formula_id": "formula_58", "formula_text": "T t=1 1{y t =\u0177 t } \u2264 2 ln(2)T i \u03b3 2 i +\u00d5 N \u221a T i \u03b3 2 i + ln(N ) \u2264 2T i \u03b3 2 i +\u00d5 N 2 i \u03b3 2 i ,", "formula_coordinates": [12.0, 130.08, 656.74, 352.7, 39.54]}, {"formula_id": "formula_59", "formula_text": "\u221a T i \u03b3 2 i \u2264 T 2 i \u03b3 2 i + c 2 N 2 2 i \u03b3 2 i", "formula_coordinates": [13.0, 322.32, 66.74, 105.05, 27.49]}, {"formula_id": "formula_60", "formula_text": "O(1) factor in the\u00d5( N \u221a T i \u03b3 2 i", "formula_coordinates": [13.0, 72.0, 86.66, 124.97, 24.28]}, {"formula_id": "formula_61", "formula_text": "8T \u03b3 2 N +\u00d5 N \u03b3 2 + S \u03b3 ,", "formula_coordinates": [13.0, 255.0, 190.66, 103.22, 25.79]}, {"formula_id": "formula_62", "formula_text": "): min \u03b1\u2208[\u22122,2] T t=1 \u2113(s i\u22121 t + \u03b1z i t ) \u2212 \u2113(s i\u22121 t ) \u2264 \u2212 \u03b3 2 4 M i\u22121 + \u03b3S.", "formula_coordinates": [13.0, 175.44, 312.1, 261.02, 56.46]}, {"formula_id": "formula_63", "formula_text": "Lemma 5. For any \u03c3 \u2208 [0, 1], min \u03b1\u2208[\u22122,2] \u03c3e \u2212\u03b1 + (1 \u2212 \u03c3)e \u03b1 \u2264 1 \u2212 1 2 (2\u03c3 \u2212 1) 2 .", "formula_coordinates": [13.0, 72.0, 435.58, 336.26, 41.93]}, {"formula_id": "formula_64", "formula_text": "1\u2212\u03c3 ) \u2208 [\u22122, 2] gives \u03c3e \u2212\u03b1 + (1 \u2212 \u03c3)e \u03b1 = 4\u03c3(1 \u2212 \u03c3) \u2264 1 \u2212 1 2 (2\u03c3 \u2212 1) 2 , since \u221a 1 \u2212 x \u2264 1 \u2212 1 2 x for x \u2208 [0, 1]. For \u03c3 \u2208 (0.95, 1], setting \u03b1 = 1 2 ln( 0.95 0.05 ) \u2208 [\u22122, 2] we have \u03c3e \u2212\u03b1 + (1 \u2212 \u03c3)e \u03b1 \u2264 0.95e \u2212\u03b1 + 0.05e \u03b1 = \u221a 0.19 \u2264 1 2 \u2264 1 \u2212 1 2 (2\u03c3 \u2212 1) 2 .", "formula_coordinates": [13.0, 72.0, 503.5, 445.36, 110.81]}, {"formula_id": "formula_65", "formula_text": "w i \u2022 z i \u2265 2\u03b3 w i 1 \u2212 2S w i \u221e \u2212\u00d5( w i 1 w i \u221e ) \u2265 2\u03b3 w i 1 \u2212 2S w i \u221e \u2212 \u03b3 w i 1 \u2212\u00d5( w i \u221e \u03b3 ) = \u03b3 w i 1 \u2212 2S w i \u221e \u2212\u00d5( w i \u221e \u03b3", "formula_coordinates": [18.0, 181.08, 106.51, 249.87, 64.16]}, {"formula_id": "formula_66", "formula_text": "| Pr{X = k} \u2212 k k\u22121 f (G)dG| = | (F X (k) \u2212 F X (k \u2212 1)) \u2212 (F G (k) \u2212 F G (k \u2212 1)) | \u2264 |F X (k) \u2212 F G (k)| + |F X (k \u2212 1) \u2212 F G (k \u2212 1)|.", "formula_coordinates": [18.0, 120.36, 324.67, 371.18, 52.04]}, {"formula_id": "formula_67", "formula_text": "Pr{X = k} \u2212 k k\u22121 f (G)dG \u2264 2C(p 2 + q 2 ) \u221a mpq ,", "formula_coordinates": [18.0, 204.0, 405.19, 207.62, 29.0]}, {"formula_id": "formula_68", "formula_text": "Pr{X = k} \u2264 k k\u22121 f (G)dG + 2C(p 2 + q 2 ) \u221a mpq \u2264 max G\u2208R f (G) + 2C(p 2 + q 2 ) \u221a mpq = 1 \u221a 2\u03c0mpq + 2C(p 2 + q 2 ) \u221a mpq = O 1 \u221a m , since pq = 1/4 \u2212 \u03b3 2 /4 \u2265 3/16.", "formula_coordinates": [18.0, 72.0, 462.67, 359.06, 123.32]}], "doi": ""}