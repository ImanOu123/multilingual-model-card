{"title": "Automatic Detection of Fragments in Dynamically Generated Web Pages", "authors": "Lakshmish Ramaswamy; Arun Iyengar; Ling Liu; Fred Douglis", "pub_date": "", "abstract": "Dividing web pages into fragments has been shown to provide significant benefits for both content generation and caching. In order for a web site to use fragment-based content generation, however, good methods are needed for dividing web pages into fragments. Manual fragmentation of web pages is expensive, error prone, and unscalable. This paper proposes a novel scheme to automatically detect and flag fragments that are cost-effective cache units in web sites serving dynamic content. We consider the fragments to be interesting if they are shared among multiple documents or they have different lifetime or personalization characteristics. Our approach has three unique features. First, we propose a hierarchical and fragment-aware model of the dynamic web pages and a data structure that is compact and effective for fragment detection. Second, we present an efficient algorithm to detect maximal fragments that are shared among multiple documents. Third, we develop a practical algorithm that effectively detects fragments based on their lifetime and personalization characteristics. We evaluate the proposed scheme through a series of experiments, showing the benefits and costs of the algorithms. We also study the impact of adopting the fragments detected by our system on disk space utilization and network bandwidth consumption.", "sections": [{"heading": "INTRODUCTION", "text": "The amount of information on the World Wide Web continues to grow at an astonishing speed. The number of dynamic web pages that are typically generated by programs executing at request time is also increasing at a rapid pace. Web caching technologies to date have been successful for efficient delivery of static web pages but they have not been so effective for delivering dynamic web content due to their frequent changing nature and their diversified freshness requirements.\nSeveral efforts have been made to address the problem of efficient serving of dynamic pages, among which Fragment-based publishing and caching of web pages [2, 10,11,13] stands out; it has been successfully commercialized in recent years. Conceptually, a fragment is a portion of a web page which has a distinct theme or functionality and is distinguishable from the other parts of the page. A web page has references to these fragments, which are stored independently on the server and in caches. In the fragmentbased publishing scheme, the cacheability and the lifetime are specified at a fragment level rather than at the page level.\nThe advantages of the fragment-based schemes are apparent and have been conclusively demonstrated [11,13]. By separating the non-personalized content from the personalized content and marking them as such, it increases the cacheable content of the web sites. Furthermore, with the fragment-based solution, a whole web page need not be invalidated when only a part of that page expires. Hence the amount of data that gets invalidated at the caches is reduced. In addition, the information that is shared across web pages needs to be stored only once, which improves disk space utilization at the caches.\nFragment-based caching solutions typically rely on the web administrator or the web page designer to manually fragment the pages on the web site. Manual markup of fragments in dynamic web pages is both labor-intensive and error-prone. More importantly, identification of fragments by hand does not scale as it requires manual revision of the fragment markups in order to incorporate any new or enhanced features of dynamic content into an operational fragment-based solution framework. Furthermore, the manual approach to fragment detection becomes unmanageable and unrealistic for edge caches that deal with multiple content providers. Thus there is a growing demand for techniques and systems that can automatically detect \"interesting\" fragments in dynamic web pages, and that are scalable and robust for efficiently delivering dynamic web content. By interesting we mean that the fragments detected are cost-effective for fragment-based caching.\nAutomatic detection of fragments presents two unique challenges. First, compared with static web pages, dynamically generated web pages have three distinct characteristics. On the one hand, dynamic web pages seldom have a single theme or functionality and they typically contain several pieces of information with varying freshness or sharability requirements. On the other hand, most of the dynamic and personalized web pages are not completely dynamic or personalized. Often the dynamic and personalized content are embedded in relatively static web page templates [5]. Furthermore, dynamic web pages from the same web site tend to share information among themselves.\nFigure 1 shows a dynamic web page generated through a fragmentbased publishing system. This Football Sport Today Page was one of the web pages hosted by IBM for a sporting event. It contains five interesting fragments that are cost-effective candidates for fragment-based caching: (1) the latest football results on the women's final, (2) the latest medal tally, (3) a daily schedule for women's football, (4) the navigation menu with the IBM logo for the sport site on the top of the page and (5) the sport links menu on the left side of the page. These fragments differ from each other in terms of their themes, functionalities, and invalidation patterns. The latest results fragment changes at a different rate than the latest medal tally fragment, which in turn changes more frequently than the fragment containing the daily schedule. In contrast, the navigational menu on the top of the page and the sport links menu on the left side of the page are relatively static and are likely to be shared by many dynamic pages generated in response to queries on sport events hosted from the web site.\nSecond, it is apparent from the above example that humans can easily identify fragments with different themes or functionality based on their prior knowledge in the domain of the content (such as sports in this example). However, in order for machines and programs to automate the fragment detection process, we need mechanisms that on the one hand can correctly identify fragments with different themes or functionality without human involvement, and on the other hand are efficient and effective for detecting and flagging such fragments through a cross-comparison of multiple pages from a web site.\nIn this paper, we propose a novel scheme to automatically detect and flag fragments in dynamic web pages which are cost-effective for fragment-based caching. We analyze web pages with respect to their information sharing behavior, personalization characteristics, and the change frequencies over time. Based on this analysis, our system detects and flags the \"interesting\" fragments in a web site.\nWe consider a fragment interesting if it has good sharability with other pages served from the same web site or it has distinct lifetime characteristics. This paper contains three original contributions:\n\u2022 First, we propose an efficient fragment-aware data structure to model dynamic web pages, including an augmented fragment tree with shingles encoding and a fast algorithm for computing shingles incrementally. This data structure forms the first step towards the efficient detection of fragments.\n\u2022 Second, we present an efficient algorithm for detecting fragments that are shared among M documents, which we call the Shared Fragment Detection Algorithm. This algorithm has two distinctive features: (1) it uses node buckets to speed up the comparison and the detection of exactly or approximately shared fragments across multiple pages.\n(2) it introduces sharing factor, minimum fragment size, and minimum matching factor as the three performance parameters to measure and tune the performance and the quality of the algorithm in terms of the fragments detected.\n\u2022 Third, we present an effective algorithm for detecting fragments that have different lifetime characteristics, which we call the Lifetime-Personalization based (L-P) Fragment Detection Algorithm. A unique characteristic of the L-P algorithm is that it detects fragments which are most beneficial to caching based on the nature and the pattern of the changes occurring in dynamic web pages.\nWe discuss several performance enhancements to these basic algorithms, and report our experiments for evaluation of the proposed fragment detection scheme, showing the effectiveness and the cost of our approach.", "publication_ref": ["b8", "b9", "b11", "b9", "b11", "b3"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "CANDIDATE FRAGMENTS", "text": "Our goal for automatic fragment detection is to find interesting fragments in dynamic web pages, which exhibit potential benefits and thus are cost-effective as cache units. We refer to these interesting fragments as candidate fragments in the rest of the paper.\nThe web documents considered here are well-formed HTML documents [8] although the approach can be applied to XML documents as well. Documents that are not well formed can be converted to well-formed documents through document normalization, for example using HTML Tidy [3].\nConcretely, we introduce the notion of candidate fragments as follows:\n\u2022 Each Web page of a web site is a candidate fragment.\n\u2022 A part of a candidate fragment is itself a candidate fragment if any one of the two conditions is satisfied:\n-The part is shared among \"M\" already existing candidate fragments, where M > 1. -The part has different personalization and lifetime characteristics than those of its encompassing (parent or ancestor) candidate fragment.\nA formal definition of candidate fragments for web pages of a web site is given below: DEFINITION 1. (Candidate Fragment) Let NW denote the set of web pages available on a web site S and CF (x) denote the set of all the fragments contained in fragment x. A fragment y is referred to as an ancestor fragment of another fragment x iff y directly or transitively contains fragment x. Let AF (x) denote all the ancestor fragments of the fragment x and F S denotes the set of fragments corresponding to the set of documents Di in NW , F S = \u222a NW i=1 CF (Di). For any document D from web site S, a fragment x in F S(D) is called a candidate fragment if one of the following two conditions is satisfied:\n1. x is a maximal Shared fragment, namely:\n\u2022 x is shared among M distinct fragments F1, . . . , FM ,\nwhere M > 1, Fi \u2208 F S, and if i = j then Fi = Fj ; and \u2022 there exists no fragment y such that y \u2208 AF (x), and y is also shared among the M distinct fragments F1, . . . , FM .", "publication_ref": ["b6", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "2.", "text": "x is a fragment that has distinct personalization and lifetime characteristics. Namely, \u2200z \u2208 AF (x), x has different personalization and lifetime characteristics than z.\nWe observe that this is a recursive definition with the base condition being that each web page is a fragment. It is also evident from the definition that the two conditions are independent. These conditions define fragments that benefit caching from two different and independent perspectives. We call the fragments satisfying Condition 1 Shared fragments, and the fragments satisfying Condition 2 L-P fragments (denoting Lifetime-Personalization based fragments). Lifetime characteristics of a fragment govern the time duration for which the fragment, if cached, would stay fresh (in tune with the value at the server). The personalization characteristics of a fragment correspond to the variations of the fragment in relation to cookies or parameters of the URL.\nIt can be observed that the two independent conditions in the candidate fragment definition correspond well to the two aims of fragment caching. By identifying and creating fragments out of the parts that are shared across more than one fragment, we aim to avoid unnecessary duplication of information at the caches. By creating fragments that have different lifetime and personalization properties we not only improve the cacheable content but also minimize the amount and frequency of the information that needs to be invalidated.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "FRAGMENT DETECTION: THE BASICS", "text": "In this section we discuss the basic design of our automated fragment detection system, including the system architecture, the efficient fragment-aware data structure for automating fragment detection, and the important configurable parameters in our system.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "System Overview", "text": "The primary goal of our system is to detect and flag candidate fragments from dynamic pages of a given web site. The fragment detection process is divided into three steps. First, the system is conceived to construct an Augmented Fragment Tree (AF tree) for the dynamic pages fed into the fragment detection system. Second, the system applies the fragment detection algorithms on the augmented fragment trees to detect the candidate fragments in the given web pages. In the third step, the system collects statistics about the fragments such as the size, how many pages share the fragment, access rates, etc. These statistics aid the administrator in deciding whether to enable fragmentation. Figure 2 gives a sketch of the system architecture.\nWe provide two independent fragment detection algorithms: one for detecting Shared fragments and the other for detecting Lifetime Personalization based (L-P) fragments. Both algorithms can be collocated with a server-side cache or an edge cache, and work on the dynamic web page dumps from the web site.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Figure 2: Fragment Detection System Architecture", "text": "The algorithm for detecting Shared fragments works on a collection of different dynamic pages generated from the same web site, whereas the L-P fragment detection algorithm works on different versions of each web page, which can be obtained from a single query being repeatedly submitted to the given web site. For example, in order to detect L-P fragments, we need to locate parts of a fragment that have different lifetime and personalization characteristics. This can be done by comparing different versions of the dynamic web page and detecting the parts that have changed over time and the parts that have remained constant. While the in-put to the L-P fragment detection algorithm differs from the shared fragment detection algorithm, both algorithms work directly on the augmented fragment tree representation of its input web pages. The output of our fragment detection algorithms is a set of fragments that are shared among a given number of documents or that have different lifetime or personalization characteristics. This fragmentation information will then be served as recommendations to the fragment caching policy manager or the respective web administrator (see Figure 2).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Augmented Fragment Trees with Shingles Encoding", "text": "Detecting interesting fragments in web pages requires efficient traversal of web pages. Thus a compact data structure for representing the dynamic web pages is critical to efficient and accurate fragment detection. Of the several document models that have been proposed, the most popular model is the Document Object Model (DOM) [1], which models web pages using a hierarchical graph. However, the DOM tree structure is less efficient for fragment detection for a number of reasons. First, our fragment detection algorithms compare pages to detect those fragments whose contents are shared among multiple pages or whose contents have distinctive expiration times. The DOM tree of a reasonably sized HTML page has a few thousand nodes. Many of the nodes in such a tree correspond to text formatting tags that do not contribute to the content-based fragment detection algorithms. Second and more importantly, the nodes of the DOM do not contain sufficient information needed for fast and efficient comparison of documents and their parts. These motivate us to introduce the concept of an augmented fragment tree (AF tree), which removes the text formatting tag nodes in the fragment tree and adds annotation information necessary for fragment detection.\nAn augmented fragment (AF) tree with shingles encoding is a hierarchical representation of a web (HTML or XML) document with the following three characteristics: First, it is a compact DOM tree with all the text-formatting tags (e.g., <Big>, <Bold>, <I>) removed. Second, the content of each node is fingerprinted with Shingles encoding [6,7,18]. Shingles are fingerprints with the property that if a document changes by a small amount, its Shingles encoding also changes by a small amount. Third, each node is augmented with additional information for efficient comparison of different documents and different fragments of documents. Concretely each node in the AF tree is annotated with the following fields:\n\u2022 Node Identifier (NodeID): A vector indicating the location of the node in the tree. \u2022 NodeValue: A string indicating the value of the node. The value of a leaf node is the text itself, and the value of an internal node is NULL (empty string). \u2022 SubtreeValue: A string that is defined recursively. For a leaf node, the SubtreeValue is equal to its NodeValue. For all internal nodes, the SubtreeValue is a concatenation of the Sub-treeValues of all its children nodes and its own NodeValue. The SubtreeValue of a node can be perceived as the fragment (content region) of a web document anchored at this subtree node. \u2022 SubtreeSize: An integer whose value is the length of Sub-treeValue in bytes. This represents the size of the structure in the document being represented by this node. \u2022 SubtreeShingles: An encoding of the SubtreeValue for fast comparison. SubtreeShingles is a vector of integers representing the shingles of the SubtreeValue.\nWe use Shingles because they have the property that if a document changes by a small amount, its Shingles also change by a small amount. Other fingerprinting techniques such as MD5 do not behave similarly. Figure 3 illustrates the high sensitivity of Shingles by comparing it with the MD5 hash through an example of two strings. The first and the second strings in Figure 3 are essentially the same strings with small perturbations (the portions that differ in the two strings have been highlighted). The MD5 hashes of the two strings are totally different, whereas the shingles of the two strings vary just by a single value out of the 8 values in the shingles set (shingle values that are present in one set but are absent in the other have been underlined in the diagram). This property of shingles has made it popular in estimating the resemblance and containment of documents [6].\nFragment based publishing of web pages improves the scalability of web services. In this paper we provide efficient techniques to automatically detect fragments in web pages. We believe that automating fragment detection is crucial for the success of fragment based web page publication.  ", "publication_ref": ["b0", "b4", "b5", "b16", "b4"], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "AF Tree Construction", "text": "The first step of our fragment detection process is to convert web pages to their corresponding AF trees. The AF tree can be constructed in two steps. The first step is to transform a web document to its DOM tree and prune the fragment tree by eliminating the text formatting nodes. The result of the first step is a specialized DOM tree that contains only the content structure tags (e.g., like <TABLE>, <TR>, <P>). The second step is to annotate the fragment tree obtained in the first step with NodeID, Node-Value, SubtreeValue, SubtreeSize and SubtreeShingles. Once the SubtreeValue is known, we can use a shingles encoding algorithm to compute its SubtreeShingles. We briefly discuss the basic algorithm [6] to compute the shingles for a given string.", "publication_ref": ["b4"], "figure_ref": [], "table_ref": []}, {"heading": "The Basic Shingling Algorithm", "text": "Any string can be considered as a sequence of tokens. The tokens might be words or characters. Let Str = T1T2T3...TN , where Ti is a token and N is the total number of tokens in Str. Then a shingles set of window length W and sample size S is constructed as follows. The set of all subsequences of length W of the string Str is computed. SubSq = {T1T2...TW , T2T3...TW +1, ..., TN\u2212W +1TN \u2212W +2...TN }. Each of these subsequences is hashed to a number between (0, 2 K ) to obtain a token-ID. A hash function similar to Rabin's function [23] could be employed for this purpose. The parameter K governs the size of the hash value set to which the subsequences are mapped. If the parameter K is set to a small value many subsequences might be mapped to the same token-ID, leading to collisions. Larger values of K are likely to avoid these collisions of subsequence, but increase the size of the hash value set. We now have (N \u2212 W + 1) token-IDs, each corresponding to one subsequence. Of these (N \u2212 W + 1) token-IDs, the minimum S are selected as the (W, S) shingles of string Str. The parameters W , S, and K can be used to tune the performance and quality of the shingles encoding.\nThe basic shingles computation algorithm is suitable for computing shingles for two independent documents. However, computing the shingles on the SubtreeValues independently at each node would entail unnecessary computations and is inefficient. This is simply because the content of every node in an AF tree is also a part of the content of its parent node. Therefore computing the SubtreeShingles of each node independently leads to a much higher cost due to duplicated shingles computation than computing the SubtreeShingles of a parent node incrementally. We propose an incremental shingles computation method and call it the Hierarchical Shingles Computing scheme (the HiSh scheme for short).", "publication_ref": ["b21"], "figure_ref": [], "table_ref": []}, {"heading": "Efficient Shingles Encoding -The HiSh Algorithm", "text": "In this section we describe a novel method to compute shingles incrementally for strings with hierarchical structures such as trees. By incremental we mean the HiSh algorithm reuses the previously computed shingles in the subsequent computation of shingles. Consider a string A = A1A2A3...AnAn+1...Am with m tokens, m \u2264 1. Let B and C be two non-overlapping substrings of A such that A is a concatenation of B and C. Let B = A1A2...An and C = An+1An+2...Am. Now we describe how to incrementally compute the (W, S) shingles of A, if (W, S) shingles of B and C are available. Let Shng(A, W, S), Shng(B, W, S) and Shng(C, W, S) denote the (W, S) shingles of the strings A, B and C respectively. We define the Overlapping Sequences to be those subsequences which begin in B and end in C. These are the subsequences that are not completely present in either shingles of B or shingles of C. Let the hashes of these subsequences be represented by the set OvlpHsh = {Hsh(A (n\u2212W +2,n+1) ), Hsh(A (n\u2212W +3,W +2) ), ..., Hsh(A (n,n+W \u22121) )}. Then we can obtain the (W, S) shingles of A as follows:\nShng(A, W, S) = Min S {Shng(B, W, S) \u00cb Shng(C, W, S) \u00cb OvlpHsh}\nHere MinS (Z) represents the operation of selecting the S minimum values from values in set Z.\nAs the shingles of B and C are available, the only extra computations needed are to compute the hashes of overlapping sequences. This is the central idea of the HiSh algorithm. Figure 4 illustrates the working of the HiSh scheme on an example string. In this example, (8, 4) shingles of the string B and string C are pre-computed and available, and we want to compute the (8, 4) shingles of the concatenation of the two strings. The HiSh algorithm computes the overlapping subsequences between the two strings (which is shown as Overlap in the figure) and computes the shingles on this overlapping string. Finally, the algorithm selects the minimum 4 values from all the three strings to yield the shingles of the entire string.\nOur experiments (see Section 6.4) indicate that the HiSh optimization can reduce the number of hashes computed in constructing the AF tree by as much as 9 times and improve the shingles computation time by about 6 times for 20-Kbyte documents, when compared to the basic algorithm. The performance gain will be greater for larger documents.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "DETECTING SHARED FRAGMENTS", "text": "This section discusses our algorithm to detect shared fragments. Given a collection of N dynamic web pages generated in response  to distinct queries over a web site, let AFi (1 \u2264 i \u2264 N ) denote the AF tree of the i th page. We call a fragment F \u2208 AFi a maximal shared fragment if it is shared among M (M < N) distinct fragments (pages) and there is no ancestor fragment of F which is shared by the same M fragments (pages). Here M is a system-defined parameter. With this definition in mind, the immediate question is how to efficiently detect such shared fragments, ensuring that the fragments detected are cost-effective cache units and beneficial for fragment-based caching. Our experiences with fragment-based solutions show that any shared fragment detection algorithm should address the following two fundamental challenges. First, one needs to define the measurement metrics of sharability. In a dynamic web site it is common to find web pages sharing portions of content that are similar but not exactly the same. In many instances the differences among these portions of content are superficial (e.g., they have only formatting differences). Thus a good automatic fragment detection system should be able to detect these approximately shared candidate fragments. Different quantifications of what is meant by \"shared\" can lead to different quality and performance of the fragment detection algorithms. The second challenge is the need for an efficient and yet scalable implementation strategy to compare the fragments (and the pages) and identify the maximal shared fragments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Approximate Sharability Measures", "text": "The Shared fragment detection algorithm operates on various web pages from the same web site and detects candidate fragments that are \"approximately\" shared. We introduce three measurement parameters to define the appropriateness of such approximately shared fragments. These parameters can be configured based on the needs of a specific application. The accuracy and the performance of the algorithm are dependent on the values of these parameters.\n\u2022 Minimum Fragment Size(M inF ragSize): This parameter specifies the minimum size of the detected fragment.\n\u2022 Sharing Factor(ShareF actor): This indicates the minimum number of pages that should share a segment in order for it to be declared a fragment.\n\u2022 Minimum Matching Factor(M inM atchF actor): This parameter specifies the minimum overlap between the SubtreeShingles to be considered as a shared fragment.\nThe parameter M inF ragSize is used to exclude very small segments of web pages from being detected as candidate fragments. This threshold on the size of the documents is necessary because the overhead of storing the fragments and composing the page would be high if the fragments are too small. The parameter ShareF actor defines the threshold on the number of documents that have shared each candidate fragment. Finally, we use the parameter M inM atchF actor to model the significance of the difference between two fragments being compared. Two fragments being compared are considered as sharing significant content if the overlap between their SubtreeShingles is greater than or equal to M inM atchF actor.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Detecting Shared Fragments with Node Buckets", "text": "The shared fragment detection algorithm detects the shared fragments in two steps as shown in Figure 5. First, the algorithm creates a sorted pool of the nodes in the AF trees of all the web pages examined using node buckets. Then, the algorithm groups those nodes that are similar to each other together and runs the condition test for maximal shared fragments. If the number of nodes in the group exceeds the minimum number of pages specified by the ShareF actor parameter, and the corresponding fragment is indeed a maximal shared fragment, the algorithm declares the node group as a shared fragment and assigns it a fragment identifier.\nStep 1: Putting Nodes into a sorted pool of node buckets More concretely, our algorithm uses the bucket structures to create a sorted pool of nodes. The algorithm creates NB buckets. Each bucket Bkti is initialized with bucket size Bsi, and is associated with a pre-assigned range of the SubtreeSizes, denoted as (M inSize(Bkti), M axSize(Bkti)). The AF trees are processed starting from the root of each tree, and a node is placed into an appropriate bucket based on its SubtreeSize, such that the Sub-treeSizes of all nodes in bucket Bkti are between M inSize(Bkti) and M axSize(Bkti). If in the process of putting nodes into buckets, a bucket grows out of its current size Bsi, it will be split into two or more buckets. Similarly, if the first step results in a pool of buckets with uneven distribution of nodes per bucket, a merge operation will be used to merge two or more buckets into one. After all the AF trees have been processed and the nodes entered into their corresponding buckets, each buckets is sorted based on the SubtreeSize of the nodes in the bucket. At the end of the process we have a set of buckets containing nodes, each of which is sorted based on the SubtreeSize of the node. The STEP 1 in Figure 5 shows the working of this step on two AF trees A and B. The nodes of the two trees are put into 5 buckets based on their SubtreeSizes. The buckets are sorted, and the buckets BT3, BT4 and BT5 are merged to obtain a set of sorted buckets.\nThere are three system-supplied parameters: (1) the number of buckets (NB) employed for this purpose, (2) the size Bi of each bucket, and (3) the range of each bucket (M inSize(Bkti), M axSize(Bkti)). Various factors may affect the decision on how to set these parameters, including the number of AF trees examined, the average number of nodes in each AF tree and the range of the SubtreeSizes of all the nodes. The performance of this step would be better if the nodes are evenly distributed in all the available buckets. One way to achieve such balanced distribution of nodes across all buckets is to set the ranges of the buckets at the lower end of the size spectrum to be smaller, and let the range of the buckets progressively increase for the buckets at the higher end of the size spectrum. This strategy is motivated by the following observations. First, it is expected that the number of nodes at a lower level of the AF trees would be larger than the number of nodes at a higher level. Second, the SubtreeSizes of the nodes at the lower level is expected to be smaller than the SubtreeSizes of the nodes in the higher levels of the AF tree. Therefore, we encourage having smaller ranges for the buckets at the lower end of the size spectrum, and gradually increase the range of the buckets at the higher end of the size spectrum.  Step 2: Identifying maximal shared fragments through grouping of similar nodes The output of the first step is a sorted pool of buckets in descending order of the ranges of buckets, and each bucket contains a list of nodes sorted by their SubtreeSizes. The task of the second step is to compare nodes and group nodes that are similar to each other together and then identify those groups of nodes that satisfy the definition of maximally shared fragments. This step processes the nodes in the buckets in decreasing order of their sizes. It starts with the node having the largest SubtreeSize, which is contained in the bucket with the highest M axSize value. For each node being processed, the algorithm compares the node against a subset of the other nodes. This subset is constructed as follows. If we are processing a node Ai, then the subset of nodes that Ai is compared against should include all nodes whose sizes are larger than P % of the SubtreeSize of Ai, where P ranges from 0% to 100%. Let CSet(Ai) denote the subset of nodes with respect to node Ai. We can use the following formula to compute CSet(Ai).\nCSet(Ai) = {Aj|SubtreeSize(Aj) \u2265 P \u00d7 SubtreeSize(Ai) 100 }\nIt is important to note that the value setting of the parameter P has implications on both the performance and the accuracy of the algorithm. If P is too low, it increases the number of comparisons performed by the algorithm. If P is very close to 100, then the number of comparisons decrease; however, it might lead the comparison process to miss some nodes that are similar. In practice we have found a value of 90% to be appropriate for most web sites. When comparing the node being processed with the nodes in its CSet, the algorithm compares the SubtreeShingles of the nodes. All such nodes whose shingles overlap more than the minimal matching factor specified by M inM atchF actor with the shingles of the node being processed are grouped together.\nStep 2 of Figure 5 demonstrates the comparison and grouping of the nodes in the sorted buckets.\nIf this group has at least ShareF actor nodes then we have the possibility of detecting it as a fragment. However before we declare the group as a candidate fragment, we need to ensure that the fragment corresponding to this group of nodes is indeed a maximally shared fragment and not a trivial fragment. To ease the decision on whether a group of nodes with similar shingles is a maximally shared fragment, we mark the descendent of each declared fragment with the fragment-ID assigned to the fragment. When similar nodes are detected, we check whether the ancestors of all of the nodes belong to the same fragment. If so, we reject the node group as a trivial fragment. Otherwise we declare the node group as a candidate fragment, assign it a fragment-ID and mark all of the descendant nodes with the fragment-ID. Once we declare a nodegroup as a candidate fragment, we remove all the nodes belonging to that group from the buckets. The algorithm proceeds by processing the next largest node in the node group in the same manner.", "publication_ref": [], "figure_ref": ["fig_3", "fig_3", "fig_3"], "table_ref": []}, {"heading": "DETECTING L-P FRAGMENTS", "text": "The L-P fragments are lifetime personalization based fragments. Typically, the L-P fragments have different lifetime and personalization characteristics than their encompassing (parent) fragment. One way to detect the L-P fragments is to compare various versions of the same web page and track the changes occurring over different versions of the web page. The nature and the pattern of the changes may provide useful lifetime and personalization information that is helpful for detecting the L-P fragments.\nThe first challenge in developing an efficient L-P fragment detection algorithm is to identify the logical units in a given web page that may change over different versions, and to discover the nature of the change. Web pages can undergo a variety of changes between versions. Parts of a web page might be deleted or moved around in the web page, and new parts may be added. Therefore a simple algorithm that only compares the parts appearing at the same relative position in different versions of the web page is unlikely to yield accurate fragments.\nThe second challenge is to detect candidate fragments that are most beneficial to caching. Suppose we have a structure such as a table in the web page being examined. Suppose the properties of the structure remain constant over different versions of the web page, but the contents of the structure have changed over different versions. Now there are two possible ways to detect fragments: Either the whole table (structure) can be made a fragment or the substructures in the table (structure) can be made fragments. Which of these would be most beneficial to caching depends upon what percentages of the substructures are changing and how they are changing (frequency and amount of changes).\nIn the design of our L-P fragment detection algorithm, we take a number of steps to address these two challenges. First, we augment the nodes of each AF tree with an additional field N odeStatus, which takes one value from the set of three choices {U nChanged, V alueChanged, P ositionChanged}. Second, we provide a shingles-based similarity function to compare different versions of a web page, and determine the portions of a web page that have distinct lifetime and personalization characteristics. Third, we construct the Object Dependency Graph (ODG) [11] for each web document examined on top of all candidate fragments detected. An Object Dependency Graph is a graphical representation of the containment relationship between the fragments of a web site. The nodes of the ODG correspond to the fragments of the web site and the edges denote the containment relationship among them. Finally, we propose to use the following configurable parameters to measure the quality of the L-P fragments in terms of cache benefit and to tune the performance of the algorithm:\n\u2022 Minimum Fragment Size(M inF ragSize): This parameter indicates the minimum size of the detected fragment.\n\u2022 Child Change Threshold(ChildChangeT hreshold): This parameter indicates the minimum fraction of children of a node that should change in value before the parent node itself can be declared as V alueChanged. This parameter can take a value between 0.0 and 1.0.\nThe L-P fragment detection algorithm works on the AF trees of different versions of web pages. It installs the first version (in chronological order) available as the base version. The algorithm compares each subsequent version to the base version and identifies candidate fragments. A new base version is installed whenever the web page undergoes a drastic change when compared with the current base version. In each step, the algorithm executes in two phases. In the first phase the algorithm marks the nodes that have changed in value or in position between the two versions of the AF tree. In the second phase the algorithm outputs the L-P fragments which are then merged to obtain the object dependency graph.", "publication_ref": ["b9"], "figure_ref": [], "table_ref": []}, {"heading": "Phase 1: Comparing the AF trees and detecting the changes", "text": "Concretely, if we have two AF trees A and B corresponding to two versions of a web page, our algorithm compares each node of the tree B, to a node from A which is most similar to it. We employ the SubtreeShingles of the nodes for similarity comparison. Let ShingleSim(Ai, Bj) denote the similarity function based on similarity of shingles of Ai and Bj . We can compute ShingleSim(Ai, Bj ) using the following formula:\nShingleSim(A i , B j ) = SubtreeShingles(A i )\u2229SubtreeShingles(B j ) SubtreeShingles(A i )\u222aSubtreeShingles(B j )\nIf we are processing node Bj from AF tree B, we obtain a node Ai from tree A such that ShingleSim(Ai, Bj ) \u2265 OvlpT hrshld, and there exists no A h such that ShingleSim(A h , Bj) > ShingleSim (Ai, Bj) where OvlpT hrshld denotes a user-specified threshold for the quantity ShingleSim, which can take a value between 0 and 1.0. If no such node is found in tree A, then it means that there is no node in A that is similar to the node Bj . This means that this node corresponds to a part that has been added in this version. Hence, the node Bj is marked as V alueChanged.\nIf a node Ai is found similar to node Bj , the algorithm begins comparing node Bj with node Ai. The algorithm compares the SubtreeValues and the NodeIDs of the two nodes. If both Subtree-Value and NodeID of the two nodes exactly match then the node is marked U nChanged. If the NodeIDs of the two nodes differ, then it means that the node has changed its position in the tree and hence it is marked as P ositionChanged.\nIf the SubtreeValues of the nodes Ai and Bj do not exactly match then the algorithm checks whether they are leaf nodes. If so, they are marked as V alueChanged. Otherwise, the algorithm recursively processes each child node of Bj in the same manner described above marking them as V alueChanged, P ositionChanged or U nChanged.\nThe algorithm addresses the second issue of discovering the fragments based on the extent of changes it is undergoing by calculating the fraction of Bj 's children that are marked as V alueChanged. If this fraction exceeds a preset threshold, which we call the ChildChangeT hreshold, then Bj itself is marked as V alueChanged. The algorithm recursively marks all the nodes in the tree in the first phase. In the second phase, the algorithm scans the tree again from the root and outputs the nodes that are marked as V alueChanged or P ositionChanged. In this pass the algorithm descends into a node's children if the node is marked as P ositionChanged or U nChanged. If the node is marked as V alueChanged, the algorithm outputs it as a L-P fragment, but does not descend into its children. This ensures that we detect maximum-sized fragments that change between versions.\nFigure 6 demonstrates the execution of one step in the L-P fragment detection algorithm. In the figure we compare the nodes of the AF tree of version 2 with the appropriate nodes of the AF tree of version 1. For example the node B7 is compared with A7 although these nodes appear at different positions in the two AF trees. We also indicate the N odeStatus of each node in version 2. In this example we set the ChildChangeT hreshold to be 0.5. The node A6 is marked as V alueChanged as both of its children have changed in value. The figure also indicates the fragments discovered in the second pass of the algorithm.\nIn summary, our L-P fragment detection algorithm detects the parts of a web page that change in value and parts of web pages changing their position between versions. Only the nodes that have changed in value are counted when deciding about the status of the parent node. The nodes that have changed only in position are as good as being unchanged for this purpose. This is because when a node just changes its relative position within its parent node, the value of the parent node would not change to a considerable extent.", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "EXPERIMENTAL EVALUATION", "text": "We have performed a range of experiments to evaluate our automatic fragment detection scheme. In this section we report four sets of experiments. The first and second sets of experiments test the two fragment detection algorithms, showing the benefits and effectiveness of the algorithms. The third set studies the impact of the fragments detected by our system on improving the caching efficiency, and the fourth set of experiments evaluates the Hierarchical Shingles computation scheme.\nThe input to the schemes is a collection of web pages including different versions of each page. Therefore we periodically fetched web pages from the web sites of BBC (http://news.bbc.co.uk), IBM's portal for marketing (http://www.ibm.com/us), Internetnews (http://www.internetnews.com) and Slashdot (http://www.slashdot.org) and created a web 'dump' for each web site. While most of these sites share information across their web pages and hence are good candidates for Shared fragment detection, the Slashdot web page forms a good candidate for L-P fragment detection for reasons explained in Section 6.2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Detecting Shared Fragments", "text": "In our first set of experiments, we study the behavior of our Shared fragment detection algorithm. The data sets used in this experimental study were web page dumps from BBC, Internet news and IBM. Due to space limitations, we primarily report the results obtained from our experiments on the BBC web site.\nBBC is a well-known news portal. Primarily, the web pages on the BBC web site can be classified into two categories: web pages reporting complete news and editorial articles (henceforth referred to as the 'article' pages) and the 'lead' pages listing the top news of the hour under different categories such as 'World', 'Americas' 'UK' etc. We observed that there is considerable information sharing among the lead pages. Therefore, the BBC web site is a good case study for detecting shared fragments.\nOur data set for the BBC web site was a web dump of 75 distinct web pages from the web site collected on 14 th July 2002. The web dump included 31 'lead' pages and 44 'article' pages.\nFigure 7 illustrates the number of Shared fragments detected at two different values of M inF ragSize and M inM atchF actor (recall that M inF ragSize is the minimum size of the detected fragment and M inM atchF actor is the minimum percentage of shingles overlap). When M inF ragSize was set to 30 bytes and M inM atchF actor was set to 70%, the number of fragments detected was 350. The number of fragments increased to 358 when the M inM atchF actor was set to 90% and to 359 when the M inM atchF actor was set to 100%. In all of our experiments we observed an increase in the number of detected fragments with increasing M inM atchF actor. This phenomenon can be explained as follows. When M inM atchF actor is set to a high value, the algorithm looks for (almost) perfect matches. Suppose we had a node A in the AF tree of one document, with children B, C and D. Suppose the same node is present in the AF tree of another document, but in this case it has children B, C and E. Obviously, the nodes in the two trees don't match perfectly. If M inM atchF actor of the algorithm were set to 90%, then the nodes B and C would be detected as fragments.\nIf on the other hand M inM atchF actor were set to 70%, then the parent node A would be detected as one single fragment. So when the M inM atchF actor is set to higher values, the number of fragments detected increases. However, the size of the detected fragments falls with increasing M inM atchF actor. The pie chart in Figure 9 indicates the percentage of fragments according to the number of pages sharing the fragments for the BBC data set. We see a large number of fragments (a little over 50%) are being shared by exactly two pages. 13% of the fragments were shared among exactly 3 pages, and 11% of the pages were shared by 10 pages or more. All 75 pages shared one fragment, and 3 fragments were shared by 69 pages. The mean of the number of pages sharing each of the detected fragments was 13.8.  A similar type of behavior was observed in all three data sets. A large percentage of the detected fragments were shared by a small number of pages, but a few fragments were shared by almost all the web pages of the site.", "publication_ref": [], "figure_ref": ["fig_5", "fig_8"], "table_ref": []}, {"heading": "BBC", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Detecting L-P Fragments", "text": "We now present the experimental evaluation of the L-P fragment detection algorithm. Though we experimented with a number of web sites, due to space limitations, we restrict our discussion to the web site from Slashdot (http://www.slashdot.org).\nSlashdot is a well known web site providing IT, electronics and business news. The front page of the Slashdot web site carries headlines and synopses of the articles on the site. The page indicates the number of comments posted by other users under each article. Thus, as new comments are added to existing articles and new articles are added to the web site, the page changes in small ways relative to the entire content of the page. It therefore forms a good case for L-P fragment detection, as well as other techniques that identify similarity across pages. The same Slashdot data set has been used in another study of similarity across pages at the level of unstructured bytes, finding that different versions of the Slashdot home page within a short time frame are extremely compressible relative to each other [17].\nThis web page provides a good case study to detect L-P fragments for a number of reasons. First, this web page is highly dynamic. Not only are there parts of the page that change every few minutes, the web page experiences major changes every couple of hours. Second, various portions of the web page have different lifetime characteristics. Third, the web page experiences many different kinds of changes like additions, deletions, value updates etc. Furthermore, there are parts of the web page that are personalized to each user.\nTable 1 provides a synopsis of the results of the L-P fragment detection experiments. A total of 79 fragments were detected when the ChildChangeT hreshold was set to 0.50, and 285 fragments were detected when ChildChangeT hreshold was set to 0.70. We observe that higher numbers of fragments are detected when ChildChangeT hreshold is set to higher values. ChildChangeT hreshold indicates the threshold for the percentage of the children to change in value before the parent itself is flagged as changed. If this threshold is set at higher values, it is more likely that nodes that are located deeper in the tree are flagged as fragments. As there are more nodes deeper in the tree, the number of fragments detected is higher. Equivalently, the average size of the fragment decreases as ChildChangeT hreshold increases. Therefore we note that when ChildChangeT hreshold is set at higher values, larger numbers of small fragments are detected. When it is set to lower values, fewer numbers of large fragments are detected.\nIn both cases, the depth of fragmentation was 3. When ChildChangeT hreshold was set to 0.50, the number of fragments detected at depths 1, 2 and 3 were respectively 10, 7 and 62. ", "publication_ref": ["b15"], "figure_ref": [], "table_ref": []}, {"heading": "Impact on Caching", "text": "Having discussed the experimental evaluation of our fragment detection system with regard to its accuracy and efficiency, we now study the impact of fragment caching on the performance of the cache, the server and the network when web sites incorporate fragments detected by our system into their respective web pages.\nWe start out by studying the savings in the disk space requirements of a fragment cache when the web pages incorporate the fragments discovered by our fragment detection system in comparison to a page cache that stores entire pages. Earlier we had explained the experimental evaluation of our shared fragment detection system on the BBC data set. We now compare the disk space needed to store the web pages in the data set when they are stored at the page granularity with disk space requirements for storing these web pages when they are fragmented as determined by our system. The graph shows that caching at the fragment level requires 22% to 31% less disk space than the conventional page level caching. The graph also shows that the improvements are higher when M inM atchF actor is set to low values. This is because when M inM atchF actor is set to low values, larger size fragments are discovered. When they are stored only once rather than being replicated, the savings obtained in terms of the disk space are higher.\nNext we study the effects of L-P fragments detected by our system on the load on the network connecting the cache and the server. As we discussed in Section 2, incorporating L-P fragments into web pages reduces the amount of data invalidated at the caches, which in turn reduces the load on the origin servers and the backbone network. In order to study the impact of the L-P fragments on the server and network load, we use the L-P fragments detected by our algorithm on the Slashdot web site.\nTo study the load on the network we also need the access and the invalidation patterns of the web pages. As we did not have accurate traces indicating the patterns of access and invalidations, we make certain assumptions, which aid us to create a model for accesses and invalidations of these web pages.\nFirst, we assume that the requests for web pages arrive according to a Poisson process, as supported by past analysis [19]. We assume the request arrival rate to be 100 requests per second. Second, we also model the invalidation processes of individual fragments as Poisson processes. We assume that the invalidation process of each fragment is independent of any other fragment in the web page. The invalidation rates in our experiments vary from 0.0001 invalidations per second to 5 invalidations per second.\nFigure 11 indicates the total bytes transferred as a function of the number of requests arriving at the cache, at fragment invalidation rates of 0.001 and 0.1 invalidations per second. The X-axis The number of bytes transferred for page-level caching is always higher than for fragment-level caching. When the invalidation rates are high, this effect is more pronounced. This experiment demonstrates the effectiveness of caching the fragments discovered by our fragment detection system in reducing the load on the network connecting the cache to the origin server.", "publication_ref": ["b17"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Improving Fragment Detection Efficiency", "text": "In this section we evaluate the performance enhancements proposed by us. We have proposed a number of techniques to improve the performance of the fragment detection process including an incremental scheme to compute the SubtreeShingles of the nodes in the AF trees (HiSh algorithm). Due to space constraints, we restrict our discussion to the experimental evaluation of the HiSh algorithm.  Figure 12 shows the total number of hash computations involved in constructing the AF tree. For a document with 1.7K characters in its content string, the number of hash computations needed for the HiSh scheme is 2.6 times less than the number of hashes computed in the direct computation. For a document whose con-tent string is 19K characters, the number of hashes computed in the HiSh scheme is almost 8.5 times less than the number of hashes computed in the direct computation. We note that the benefits of the HiSh algorithm are greater for larger documents.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "RELATED WORK", "text": "Fragment-based publishing, delivery and caching of dynamic data have received considerable attention from the research community in recent years [11,13]. Edge Side Includes [2] is a markup language to define web page components for page assembly at the edge caches. ESI provides mechanisms for specifying the cacheability properties at fragment level. Mohapatra et al. [21] discuss a fragment-based mechanism to manage quality of service for dynamic web content. Chan and Woo [12] use the structural similarity existing among various pages of a single site to efficiently deltaencode multiple web pages over time. Naaman et al. [22] present analytical and simulation based studies to compare ESI and deltaencoding, finding that ESI has potential performance advantages due to its ability to deliver only changing fragments. In addition to the above work, there is a considerable amount of literature in the more general area of the generation, delivery and caching of dynamic content [9,10]. None of these previous papers addresses the problem of how to automatically detect fragments in web pages, however.\nThe work of Bar-Yossef and Rajagopalan [5] is related to our research on automated fragment detection, although the authors were addressing a different problem. They discuss the problem of template detection through discovery of pagelets in the web pages. However, our work differs from the work on template detection both in context and content. First, the work on template detection is aimed towards improving the precision of search algorithms. Our work is aimed at detecting fragments that are most beneficial to caching and content generation. Second, the syntactic definition of a pagelet in their paper is based on the number of hyperlinks in the HTML parse tree elements. They define a pagelet as an HTML element in the parse tree of a web page such that none of its children have at least K hyperlinks and none of its ancestors is a pagelet. This definition is very different from our working definition of a candidate fragment provided in Section 2. Further, their definition of pagelets forbids recursion. In contrast we permit embedded fragments. Third, our system has two algorithms: one to detect Shared fragments and another to detect L-P fragments. Both of these detect embedded fragments.\nThere has been significant work in identifying web objects that are identical, either at the granularity of entire pages or images [4,16,20] or pieces of pages [24], using MD5 or SHA-1 hashes to detect and eliminate redundant data storage and transfer. While the motivations of these researches are similar to that of the shared fragment detection algorithm, they are more restrictive in the sense that they work on full HTML pages and can only detect and eliminate pages (or byte-blocks) which are exact replicas. Pages that are similar at the level of entire web pages [25,14] or pieces of web pages [17] can be identified using resemblance detection [6] and then delta-encoded. While these techniques have the potential to reduce transfer sizes, decomposing web pages into separately cached fragments accomplishes similar reductions in size without the need for explicit version management.\nIn addition to these, discovering and extracting objects from web pages has received considerable attention from the research community [8,15]. While these projects aim at extracting objects based on the nature of the information they contain, our work concentrates on discovering fragments based on their lifetime, personalization and sharing characteristics.", "publication_ref": ["b9", "b11", "b19", "b10", "b20", "b7", "b8", "b3", "b2", "b14", "b18", "b22", "b23", "b12", "b15", "b4", "b6", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSIONS", "text": "There has been heavy demand for technologies to ensure timely delivery of fresh dynamic content to end-users [9,10,11]. Fragmentbased generation and caching of dynamic web content is widely recognized as an effective technique to address this problem. However, past work in the area has not adequately addressed the problem of how to divide web pages into fragments. Manual fragmentation of web pages by a web administrator or web page designer is expensive and error-prone; it also does not scale well.\nIn this paper we have presented a novel scheme to automatically detect and flag \"interesting\" fragments in dynamically generated web pages that are cost-effective cache units. A fragment is considered to be interesting if it is shared among multiple pages or if it has distinct lifetime or personalization characteristics. This scheme is based on analysis of the web pages dynamically generated at given web sites with respect to their information sharing behavior, personalization properties and change patterns. Our approach has three unique features. First, we propose a hierarchical and fragment-aware model of the dynamic web pages and a data structure that is compact and effective for fragment detection. Second, we present an efficient algorithm to detect maximal fragments that are shared among multiple documents. Third, we develop an algorithm that effectively detects fragments based on their lifetime and personalization characteristics. We evaluate the proposed scheme through a series of experiments, showing the benefits and costs of the algorithms. We also report our study on the impact of adopting the fragments detected by our system on disk space utilization and network bandwidth consumption.", "publication_ref": ["b7", "b8", "b9"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Document Object Model -W3C Recommendation", "journal": "", "year": "", "authors": ""}, {"ref_id": "b1", "title": "", "journal": "", "year": "", "authors": " Html Tidy"}, {"ref_id": "b2", "title": "Replica-Aware Caching for Web Proxies", "journal": "Computer Communications", "year": "2002", "authors": "H Bahn; H Lee; S H Noh; S L Min; K Koh"}, {"ref_id": "b3", "title": "Template Detection via Data Mining and its Applications", "journal": "", "year": "2002-05", "authors": "Z Bar-Yossef; S Rajagopalan"}, {"ref_id": "b4", "title": "On resemblance and Containment of Documents", "journal": "", "year": "1997", "authors": "A Broder"}, {"ref_id": "b5", "title": "Syntactic Clustering of the Web", "journal": "", "year": "1997-04", "authors": "A Broder; S C Glassman; M S Manasse; G Zweig"}, {"ref_id": "b6", "title": "A Fully Automated Object Extraction System for the World Wide Web", "journal": "", "year": "2001", "authors": "D Buttler; L Liu"}, {"ref_id": "b7", "title": "View Invalidation for Dynamic Content Caching in Multi tiered Architectures", "journal": "", "year": "2002-09", "authors": "K S Candan; D Agrawal; W.-S Li; O Po; W.-P Hsiung"}, {"ref_id": "b8", "title": "A Scalable System for Consistently Caching Dynamic Web Data", "journal": "", "year": "1999-03", "authors": "J Challenger; A Iyengar; P Dantzig"}, {"ref_id": "b9", "title": "Publishing System for Efficiently Creating Dynamic Web Content", "journal": "", "year": "2000-05", "authors": "J Challenger; A Iyengar; K Witting; C Ferstat; P Reed"}, {"ref_id": "b10", "title": "Cache-Based Compaction: A New Technique for Optimizing Web Transfer", "journal": "", "year": "", "authors": "M C Chan; T W C Woo"}, {"ref_id": "b11", "title": "Proxy-Based Accelaration of Dynamically Generated Content on the World Wide Web: An Approach and Implementation", "journal": "", "year": "2002-06", "authors": "A Datta; K Dutta; H Thomas; D Vandermeer; K Suresha;  Ramamritham"}, {"ref_id": "b12", "title": "Application-Specific Delta Encoding Via Resemblance Detection", "journal": "", "year": "2003-06", "authors": "F Douglis; A Iyengar"}, {"ref_id": "b13", "title": "Visual Based Content Understanding towards Web Adaptation", "journal": "", "year": "2002", "authors": "X.-D Gu; J Chen; W.-Y Ma; G.-L Chen"}, {"ref_id": "b14", "title": "Aliasing on the World Wide Web: Prevalence and Performance Implications", "journal": "", "year": "2002-05", "authors": "T Kelly; J Mogul"}, {"ref_id": "b15", "title": "Redundancy Elimination Within Large Collections of Files", "journal": "", "year": "2004-06", "authors": "P Kulkarni; F Douglis; J Lavoie; J Tracey"}, {"ref_id": "b16", "title": "Finding Similar Files in a Large File System", "journal": "", "year": "1994-01", "authors": "U Manber"}, {"ref_id": "b17", "title": "Network Behavior of a Busy Web Server and its Clients", "journal": "", "year": "1995", "authors": "J "}, {"ref_id": "b18", "title": "Design, Implementation, and Evaluation of Duplicate Transfer Detection in HTTP", "journal": "", "year": "2004-03", "authors": "J Mogul; Y Chan; T Kelly"}, {"ref_id": "b19", "title": "A Framework for Managing QoS and Improving Performance of Dynamic Web Content", "journal": "", "year": "2001-11", "authors": "P Mohapatra; H Chen"}, {"ref_id": "b20", "title": "Evaluation of ESI and Class-Based Delta Encoding", "journal": "", "year": "", "authors": "M Naaman; H Garcia-Molina; A Paepcke"}, {"ref_id": "b21", "title": "Fingerprinting by Random Polynomials", "journal": "", "year": "1981", "authors": "M O Rabin"}, {"ref_id": "b22", "title": "Value-Based Web Caching", "journal": "", "year": "2003", "authors": "S C Rhea; K Liang; E Brewer"}, {"ref_id": "b23", "title": "Improved File Synchronization Techniques for Maintaining Large Replicated Collections Over Slow Networks", "journal": "", "year": "2004-03", "authors": "T Suel; P Noel; D Trendafilov"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Fragments in a Web Page", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Example of Shingles versus MD5", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: HiSh Algorithm", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure 5: Shared Fragment Detection Algorithm", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 6 :6Figure 6: L-P Fragment Detection Algorithm", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 7 :7Figure 7: Number of Fragments Detected for BBC Data set", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 8 :8Figure 8: Maximum Size of the Detected FragmentsFigure8illustrates this effect. The graph indicates the maximum size of the detected fragments for various data sets when M inM atchF actor was set to 70% and 90% . For the BBC web site, the change in the size of the largest detected fragment is rather drastic. The size falls from 5633 bytes to 797 bytes when M inM atchF actor increases from 70% to 90%.The pie chart in Figure9indicates the percentage of fragments according to the number of pages sharing the fragments for the BBC data set. We see a large number of fragments (a little over 50%) are being shared by exactly two pages. 13% of the fragments were shared among exactly 3 pages, and 11% of the pages were shared by 10 pages or more. All 75 pages shared one fragment, and 3 fragments were shared by 69 pages. The mean of the number of pages sharing each of the detected fragments was 13.8.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 9 :9Figure 9: Distribution of Fragment Sharing for the BBC Data set", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 10 :10Figure 10: Total Storage Requirements for Page and Fragment Caches Figure 10 indicates the total storage requirements as a function of the number of pages both for page caches and fragment caches.The graph shows that caching at the fragment level requires 22% to 31% less disk space than the conventional page level caching. The graph also shows that the improvements are higher when M inM atchF actor is set to low values. This is because when M inM atchF actor is set to low values, larger size fragments are discovered. When they are stored only once rather than being replicated, the savings obtained in terms of the disk space are higher.Next we study the effects of L-P fragments detected by our system on the load on the network connecting the cache and the server. As we discussed in Section 2, incorporating L-P fragments into web pages reduces the amount of data invalidated at the caches, which in turn reduces the load on the origin servers and the backbone network. In order to study the impact of the L-P fragments on the server and network load, we use the L-P fragments detected by our algorithm on the Slashdot web site.To study the load on the network we also need the access and the invalidation patterns of the web pages. As we did not have accurate traces indicating the patterns of access and invalidations, we make certain assumptions, which aid us to create a model for accesses and invalidations of these web pages.First, we assume that the requests for web pages arrive according to a Poisson process, as supported by past analysis[19]. We assume the request arrival rate to be 100 requests per second. Second, we also model the invalidation processes of individual fragments as Poisson processes. We assume that the invalidation process of each fragment is independent of any other fragment in the web page. The invalidation rates in our experiments vary from 0.0001 invalidations per second to 5 invalidations per second.Figure11indicates the total bytes transferred as a function of the number of requests arriving at the cache, at fragment invalidation rates of 0.001 and 0.1 invalidations per second. The X-axis", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 11 :11Figure 11: Bytes Transferred between Server and Cache", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Figure 12 :12Figure 12: Number of Hashes Computed in Direct and HiSh schemes", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Automatic Detection of Fragments in Websites", "figure_data": "String B String BString C String CAutomatic Detection of Fragments in WebsitesOverlap OverlapShingles of String B Shingles of String BShingles of Overlap Shingles of OverlapShingles of String C Shingles of String C{300, 434 1093, 2764} {300, 434 1093, 2764}{193, 243, 1432, 3456 } {193, 243, 1432, 3456 }{104, 470, 1956, 3464} {104, 470, 1956, 3464}Min Min{104, 193, 243, 300} {104, 193, 243, 300}Shingles of Concatenated String Shingles of Concatenated String"}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u2022 x is shared among M distinct fragments F1, . . . , FM ,", "formula_coordinates": [3.0, 86.88, 486.17, 206.0, 8.97]}, {"formula_id": "formula_1", "formula_text": "Shng(A, W, S) = Min S {Shng(B, W, S) \u00cb Shng(C, W, S) \u00cb OvlpHsh}", "formula_coordinates": [5.0, 68.76, 433.8, 209.02, 27.13]}, {"formula_id": "formula_2", "formula_text": "CSet(Ai) = {Aj|SubtreeSize(Aj) \u2265 P \u00d7 SubtreeSize(Ai) 100 }", "formula_coordinates": [6.0, 316.81, 549.78, 241.36, 20.85]}, {"formula_id": "formula_3", "formula_text": "ShingleSim(A i , B j ) = SubtreeShingles(A i )\u2229SubtreeShingles(B j ) SubtreeShingles(A i )\u222aSubtreeShingles(B j )", "formula_coordinates": [7.0, 318.84, 361.34, 233.88, 14.85]}], "doi": ""}