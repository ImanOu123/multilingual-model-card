{"title": "Hierarchical Data-driven Descent for Efficient Optimal Deformation Estimation", "authors": "Yuandong Tian; Srinivasa G Narasimhan", "pub_date": "", "abstract": "Real-world surfaces such as clothing, water and human body deform in complex ways. The image distortions observed are high-dimensional and non-linear, making it hard to estimate these deformations accurately. The recent datadriven descent approach [17] applies Nearest Neighbor estimators iteratively on a particular distribution of training samples to obtain a globally optimal and dense deformation field between a template and a distorted image. In this work, we develop a hierarchical structure for the Nearest Neighbor estimators, each of which can have only a local image support. We demonstrate in both theory and practice that this algorithm has several advantages over the nonhierarchical version: it guarantees global optimality with significantly fewer training samples, is several orders faster, provides a metric to decide whether a given image is \"hard\" (or \"easy\") requiring more (or less) samples, and can handle more complex scenes that include both global motion and local deformation. The proposed algorithm successfully tracks a broad range of non-rigid scenes including water, clothing, and medical images, and compares favorably against several other deformation estimation and tracking approaches that do not provide optimality guarantees.", "sections": [{"heading": "Introduction", "text": "Accurately finding dense correspondence between images capturing deforming objects is important for many vision tasks, such as 3D reconstruction, image alignment and tracking. However, estimating the parameters of nonrigid deformation is hard due to its high-dimensionality and strong nonconvexity. Continuous optimization approaches (e.g. gradient descent or Newton's method) require no training but often suffer from local minima, while regressionbased approaches (e.g., Nearest Neighbor) have guaranteed solutions, but need a lot of training samples.\nRecently, Tian and Narasimhan [17] proposed Datadriven Descent which combines the best properties of both continuous optimization and regression. They show that if a generative model for deformation is available, then the training samples can be generated by simply deforming the template using parameters from a particular distribu-  [17]. The number of samples needed is a constant for the first few iterations, and then decays double exponentially for our algorithm. tion. Then a sequence of Nearest Neighbor predictions will achieve the globally optimal solution that warps the test image to the template. Furthermore, to achieve the accuracy of 1/\u01eb, the number of samples needed is O(C d log 1/\u01eb) for d dimensional warping, much less than O(1/\u01eb d ) required for general regressions. Intuitively, this approach captures the group-like structure in deformation and uses the training samples which are far away from the test image for prediction. Their approach shows good empirical results for local deformation, but fails to capture general deformation that contains both global and local components (e.g., cloth moving and deforming).\nIteration\nIn this paper, we develop a top-down hierarchical structure for deformation estimation with global optimality guarantee. First, the deformation field is parameterized so that the deformation happening within a local image patch can be predicted by the content of that patch, reducing the dimensionality. Then, we model the relationship between the image content and the deformation parameters using a novel criterion. With this criterion, all patches at different locations and scales can be regarded as predictors with guaranteed worst-case precisions. Finally, combining these predic-tors together in a top-down hierarchical manner leads to an overall predictor that can handle large and high-dimensional deformation with both local and global components.\nOur contributions are three-fold. First, our approach brings down sample complexity to O(C d 1 + C 2 log 1/\u01eb), which varies very slowly with respect to the accuracy. In particular, the number of samples required in each iteration stays constant for the first few iterations (layers of hierarchy), and then decays double exponentially (Fig. 1). Practically, our unoptimized Matlab implementation is fast, achieving 3-4 fps on real images. Second, compared to [17], our sample complexity guarantee is based on much weaker assumptions that can be verified with an efficient algorithm. As a result, our constant C 1 is much smaller than the constant C in [17]. Third, our work provides a rigorous theoretical analysis and interesting insights for topdown coarse-to-fine hierarchical structures. We believe this can be useful for analyzing many other hierarchies proposed in the computer vision community.\nOur work not only has strong theoretical foundations, but also demonstrates good quantitative and qualitative results on real video sequences containing different types of deformation, including clothing and water surface deformations as well as medical images of internal organs. Our approach outperforms optimization-based approaches such as Lucas-Kanade [1] and Free-form registration [9] (both with coarse-to-fine implementations), regression-based approaches such as Nearest Neighbor and Explicit Shape Regression [3], feature-based approaches such as SIFT [6], tracking-based approaches such as KLT [13], and hybrid methods such as Data-driven Descent [17].", "publication_ref": ["b16", "b16", "b16", "b16", "b0", "b8", "b2", "b5", "b12", "b16"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Related Works", "text": "Optimization-based approaches (e.g., [1,7,9]) usually reach a local minimum using gradient descent or Newton's method. Random initialization is used to improve the quality of solutions on a heuristic basis. Regression-based approaches aim to learn a mapping from the distorted image to the deformation parameters using labeled training samples. The actual form of mapping could be Nearest Neighbor, Linear [7], Random Forest [14], Boosted Random Fern [3], etc. Feature-based approaches (e.g., SIFT [6]) find correspondence by matching local features, and have to balance between distinctiveness and invariance under deformation.\nHierarchical structures have been used extensively in vision. Typical scenarios include coarse-to-fine optimization [9] for a better local solution, interest point detection [6], multi-resolutional feature extraction [5], biologically plausible framework for object recognition [12] and so on. Recently, it is also used in Deep Learning, showing state-of-art performance in image classification [4]. However, as far as we know, none of the previous works provide theoretical performance guarantees.  ", "publication_ref": ["b0", "b6", "b8", "b6", "b13", "b2", "b5", "b8", "b5", "b4", "b11", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "The Image Deformation Model", "text": "Denote T as the template image and I p as the distorted image with deformation parameters p. The deformation field W (x; p) maps the pixel location x on the template to the pixel location W (x; p) on the distorted image I p :\nI p (W (x; p)) = T (x)(1)\nWe locally parameterize the deformation field W (x; p) at any 2D point x by a weighted linear combination of displacements p = [p(1), p(2), . . . , p(K)] T on K landmarks (Fig. 2):\nW (x; p) = x + B(x)p(2)\nwhere\nB(x) = [b 1 (x), b 2 (x), . . . , b K (x)\n] is a Kdimensional row vector of weighting factors on location x from K landmarks, p is a K-by-2 matrix storing 2K displacement components and p(i) is a 2-dimensional deformation vector for landmark i. Due to strong correlations between nearby landmark displacements, the dimensionality d of the warping field could be much lower than 2K. Naturally, at any location x, its weights from all K landmarks add to 1 ( i b i (x) = 1), and the weight b i (\u2022) at i-th landmark location l i is 1 while others are zero. Practically, B(x) can be any interpolation function, e.g., Thinplate Spline [2], B-spline [9], local linear interpolation, etc.\nMany previous works [17,7,11] also assume similar form of W (x; p). However, their parameters p, usually given by dimensionality reduction procedures (e.g., PCA), is not localized to spatial landmarks, while Eqn. 2 is an over-parameterization of the deformation field W (x; p), which leads to further reduction of training samples needed.\nGenerating training samples. From Eqn. 1, given the parameter p, one can generate the deformed image I p from the template T . This is done by assigning every pixel y of the deformed image I p with the pixel value on location x = W \u22121 (y; p) of the template T . Choosing different parameters {p i } gives many training samples {(p i , I pi )}.\nThe task now becomes how to properly distribute the training samples and how many samples are needed (i.e., sam-ple complexity) to achieve the globally optimal prediction of the unknown parameter for a distorted test image. This is the core of our contribution and will be described next.", "publication_ref": ["b1", "b8", "b16", "b6", "b10"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "The Relationship between Image Evidence and Distortion Parameters", "text": "Suppose we have training samples {p, I p } and want to predict the parameter for a test image I with an unknown true parameter p 1 . The simplest way is to use the Nearest Neighbor predictor: find I p2 in the training set that is closest to I, and return the parameter p 2 as the prediction.\nTo make this approach work, we need to assume a positive correlation between the image difference \u2206I \u2261 ||I p1 \u2212 I p2 || in terms of a certain image metric and the parameter difference \u2206p \u2261 ||p 1 \u2212 p 2 || \u221e in terms of maximal absolute difference between landmark displacements. Intuitively, this means that if two images are close, so are their parameters and vice versa. This can be represented by the following Lipchitz conditions proposed in [17]:\nL 1 \u2206I \u2264 \u2206p \u2264 L 2 \u2206I (3)\nwhere, L 1 and L 2 are two constants that are dependent on the template T . [17] shows that the ratio L 2 /L 1 is a charactistic for samples complexity for guaranteed Nearest Neighbor prediction. For simple images that contain one salient object with a clear background, L 2 /L 1 is small and a few samples suffice. For difficult images with repetitive patterns, L 2 /L 1 is large and a lot of samples are needed to distinguish among locally similar-looking structures.", "publication_ref": ["b16", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Relaxed Lipchitz Condition", "text": "One shortcoming of Eqn. 3 is that it must hold for arbitrarily small \u2206I and \u2206p. Thus it fails in two situations:\n\u2022 Noisy images. Adding noise to a distorted image I p changes its appearance but not its parameters. As a result, \u2206p \u2248 0 but \u2206I is finite. This makes L 1 \u2192 0.\n\u2022 Repetitive Patterns. If an image resembles itself after some transformation, then \u2206p is finite but \u2206I \u2248 0. This makes L 2 \u2192 +\u221e.\nIn both cases, [17] gives a trivial (infinite) bound on sample complexity and global optimality cannot be guaranteed.\nIn this paper, we relax this condition using a patch-based approach. Denote R = R(x, r) as a square centered at x with size 2r. I(R) is the patch within R and S = S(x, r) is the subset of landmarks whose displacements p(S) influence the patch content I(R). p(S) is a |S| by 2 matrix obtained by choosing S rows from p. We assume I(R) and p(S) satisfy the following relaxed Lipchitz Condition: Assumption 1 (Relaxed Lipchitz Condition) There exists 0 < \u03b1(x, r) \u2264 \u03b3(x, r) < 1 and 0 < A(x, r) < \u0393(x, r) so that for any p 1 and p 2 with\n||p 1 || \u221e \u2264 r, ||p 2 || \u221e \u2264 r: \u2206p \u2264 \u03b1r =\u21d2 \u2206I \u2264 Ar, \u2206p \u2265 \u03b3r =\u21d2 \u2206I \u2265 \u0393r (4) for \u2206p \u2261 ||p 1 (S) \u2212 p 2 (S)|| \u221e and \u2206I \u2261 ||I p1 (R) \u2212 I p2 (R)||.\nHere ||x|| \u221e \u2261 max i |x i |. Visually, the first part of Eqn. 4 says all (\u2206p, \u2206I) left to the vertical line \u03b1r have to be in the red-shaded box; while the second part says all (\u2206p, \u2206I) right to the vertical line \u03b3r have to be in the blue-shaded box (Fig. 3(a)). The condition A < \u0393 means that the bottom of blue is always above the top of red. When they touch (Fig. 3(b)), the minimal, or tightest \u03b3 is achieved for a given \u03b1, which is the monotonous curve \u03b3 = \u03b3(\u03b1).\nDifferent from the Lipchitz conditions (Eqn. 3), one important aspect of Eqn. 4 is that \u2206I and \u2206p are only correlated up to the scale of r. This weaker condition allows Eqn. 4 to account for noise and parameter changes outside the subset S that may influence the patch I(R) without altering p(S). This also accounts for the case in which two slightly different parameters share the same image appearance. In both cases, the image-dependent pair (\u03b1, \u03b3) is still well-behaved while L 2 /L 1 is not. Another aspect is that Eqn. 4 is assumed for every patch located at x with scale r, while Eqn. 3 is a single condition for the entire image.\nBesides, Eqn. 4 holds only for deformation within the acceptance range r, i.e., ||p|| \u221e \u2264 r). This is a practical condition because if p(S) is large, the image content I(R) is no longer related to the local patch deformation p(S).\nDegrees of Freedom on patches. Since p(S) is a |S|by-2 matrix, there are at most 2|S| apparent degrees of freedom for patch I(R). How large is |S|? If landmarks are distributed uniformly (e.g., on a regular grid), |S| is proportional to Area(R), which gives 2|S| \u221d r 2 .\nOn the other hand, if the overall effective degree of freedom is d, then no matter how large 2|S| is, p(S) contains dependent displacements and the effective degree of freedom in R never exceeds d. Therefore, we assume: Assumption 2 (Degrees of Freedom for Patches) The local degrees of freedom of a patch (x, r) is min(d, 2|S|).", "publication_ref": ["b16"], "figure_ref": ["fig_3", "fig_3"], "table_ref": []}, {"heading": "Guaranteed Prediction using Nearest Neighbor", "text": "Now let us study how the relaxed Lipchitz condition helps Nearest Neighbor prediction. We wish to know how well patch (x, r) can predict the deformation p(S) within its acceptance range r (i.e., ||p|| \u221e \u2264 r). Without any training samples, we can trivially set the predictionp(S) = 0 and get a worst-case guaranteed prediction error of r. Now the problem is: if we want to obtain a slightly better prediction, how many training samples do we need?\nTheorem 1 gives the answer. It shows that if the relaxed Lipchitz condition (Eqn. 4) holds, then a Nearest Neighbor prediction with 1/\u03b1 samples per dimension will always reduce the error by a factor of \u03b3 < 1:\nTheorem 1 (Guaranteed Nearest Neighbor) Given a distorted image I p with ||p|| \u221e \u2264 r, then with If the local deformation is d-dimensional with d < 2|S|, then it turns out that only a small fraction of the hypercube are sampled and c ss \u23081/\u03b1\u2309 d samples suffices. See [16] for detailed derivation of c ss .\nN (x, r) = min c ss \u23081/\u03b1\u2309 d , \u23081/\u03b1\u2309 2|S|(\nFrom Theorem 1, the exponent of Eqn. 5 is the degrees of freedom mentioned in Assumption 2, which demonstrates the curse of dimensionality. From Eqn. 5 and Eqn. 6, now both \u03b1 and \u03b3 have their physical meanings: \u03b1 is the inverse of sample complexity per dimension, while \u03b3 is the inverse of prediction accuracy. Ideally we want \u03b1 to be large for lower sample complexity, and \u03b3 to be small for higher accuracy. However, the constraint \u03b1 \u2264 \u03b3 and the minimal curve \u03b3 = \u03b3(\u03b1) show there is a trade-off. Like L 2 /L 1 in Eqn. 3, this trade-off reflects the difficulty level of images for deformation prediction (See Sec. 6 for details).", "publication_ref": ["b15"], "figure_ref": [], "table_ref": []}, {"heading": "Construction of Hierarchical Structure", "text": "According to Theorem 1, different image patches (x, r) show different characteristics in their prediction guarantees: large patches (large r) can deal with large deformation but have low prediction precision, while small patches (small r) only deals with small deformation but enjoys high prediction precision. Therefore, in order to estimate large deformation with high precision, a natural way is to build a coarse-to-fine hierarchy of predictions as follows: the coarse layer (large patch) reduces the prediction residue by a certain extent so that it is within the acceptance range of the fine layer (small patch), where the prediction is refined.\nFrom this argument, we construct the hierarchical structure as follows. Within the same layer t, scale of patches is fixed and denoted as r t . When going from top to bottom (t becomes large), the scale r t of patches shrinks towards zero. The shrinking factor\u03b3 = r t+1 /r t is set to b\u0113\n\u03b3 \u2261 max (x,r) \u03b3(x, r) < 1 (7)\nAlgorithm 1 Hierarchical Deformation Estimation.  for Patch (x j , r t ) within layer t do 7:\nS j = S(x j , r t ), R j = R(x j , r t )", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "8:", "text": "Find the Nearest Neighbor i * for patch I(R j ):\ni * = arg min i\u2208T r(x,r) ||I c (R j ) \u2212 I i (R j )|| 9:\nSet the estimationp j\u2192i (S j ) = p i * (S j ). Aggregation:p(i) = mean i\u2208Sjpj\u2192i (S j ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "12:", "text": "Update:p t (i) =p t\u22121 (i) +p(i) for all landmarks. 13: end for 14: Return final predictionsp T (i) for all landmarks. Fig. 4 and Alg. 1 illustrate the algorithm that estimates the unknown parameter p given the test image I test . For the first iteration, the test image I test is directly compared with the training samples generated from the entire image with scale r 1 to obtain the Nearest Neighbor predictionp 1 . Then for the second iteration, we have a slightly less distorted image I test (W (x,p 1 )), from which we estimate p\u2212p 1 . Since ||p\u2212p 1 || \u221e is smaller than ||p|| \u221e , its predictions can be localized to smaller patches. Then this procedure is iterated until the lowest layer. Similar to [17], Alg. 1 will converge to the globally optimal solution (Theorem 2), while the required number of samples is O(C d 1 + C 2 log 1/\u01eb) (Theorem 3). Note that a less distorted image I test (W (x;p t\u22121 )), as the input of layer t, is not necessarily the same as a distorted image I p\u2212p t\u22121 generated directly from the template image. However, their difference decreases when r t \u2192 0 and global convergence guarantee still holds (See [16]).\nTheorem 2 (The Global Convergence Theorem) If ||p|| \u221e \u2264 r 1 , then the predictionp t (i) satisfies:  Images with a salient object and clean background require only a few samples per dimension. Bottom Row: Images with repetitive patterns require more samples per dimension. In both cases, our bound is smaller than that given by Data-Driven Descent.\n||p t (i) \u2212 p(i)|| \u221e \u2264\u03b3 t r 1 \u2192 0 when t \u2192 +\u221e (8)\nProof Sketch From Theorem 1, from the top layer, after each layer the residue is contracted by at least\u03b3. Then\u03b3 < 1 implies that the error diminishes from top to bottom.", "publication_ref": ["b16", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Theorem 3 (The Number of Samples Needed)", "text": "The total number N of samples needed is bounded by:\nN \u2264 C 3 C d 1 + C 2 log 1/\u03b3 1/\u01eb (9\n)\nwhere C 1 = 1/ min (x,r) \u03b1(x, r), C 2 = 2 1/(1\u2212\u03b3 2 ) and C 3 = 2 + c ss (\u2308 1 2 log 1/\u03b3 2K/d\u2309 + 1).\nProof Sketch From Assumption 2, the areas of patches, as well as |S|, decrease by a factor of\u03b3 2 from top to bottom. Therefore, the number of samples needed stays the same until 2|S| \u2248 d, and then goes down double-exponentially. Theorem 1 gives the number of samples at any level of the hierarchy. The supplementary report [16] shows that the summation of the samples at all levels is a fast decaying series bounded by Eqn. 9.", "publication_ref": ["b15"], "figure_ref": [], "table_ref": []}, {"heading": "Empirical Upper Bounds For Images", "text": "Given a spectific template and a specific family of deformation, we can generate many deformed images and their parameters (p i , I pi ), compute all-pair image/parameter distances {\u2206p i , \u2206I i } and estimate the monotonous curve \u03b3 = \u03b3(\u03b1) like Fig. 3. This curve can help predict the theoretical difficulties of images for deformation estimation. For simplicity, we set a constant and convergent contraction factor\u03b3 = 0.95 and compute the largest \u03b1 0.95 = \u03b3 \u22121 (0.95). Therefore, simple images have high \u03b1 0.95 , indicating low sample complexity per dimension (1/\u03b1 0.95 ), and vice versa.\nWe randomly generate 1000 deformed samples and compute all-pair distances. The deformation is 2D translation and in-plane rotation (d Fig. 5 shows each template and its 1/\u03b1 0.95 . Note that images with a salient object and uniform background requires fewer samples, while images with repetitive patterns and cluttered backgrounds require more. In contrast, L 2 /\u03b3L 1 , as suggested in [17], is much higher in both cases.\nWith regard to total sample complexity N , Theorem 1 tells that for easy images, 1/\u03b1 0.95 \u2248 5 and N \u2248 [5 \u2022 (2 + \u221a\n2)] 4 = 84926 (See [16] for details), while for hard images, 1/\u03b1 0.95 \u2248 12 and N \u2248 [12 \u2022 (2 + \u221a 2)] 4 = 2817654. Although practically N may be much smaller, it gives a sense of difficulty levels of images.\nAlgorithm 2 Find Local Lipschitz Constants.\nINPUT Parameter distances {\u2206p i } with \u2206p i \u2264 \u2206p i+1 . Image distances {\u2206I i } and scale r.\n\u2206I + i = max 1\u2264j\u2264i \u2206I j , for i = 1 . . . M . \u2206I \u2212 i = min i\u2264j\u2264M \u2206I j , for i = 1 . . . M . for i = 1 to M do Find minimal j so that \u2206I \u2212 j > \u2206I + i . if i \u2264 j then\nStore a curve point (\u03b1, \u03b3) = (\u2206p i , \u2206p j )/r. end if end for  [17]. Accuracy of our approach increases much faster than [17] with the same number of samples. To obtain the same level of accuracy of our approach with 400 samples, [17] requires 10000 samples or more. Our approach also has lower variance in performance. Right: Convergence behavior of our approach with different number of training samples.", "publication_ref": ["b16", "b15", "b16", "b16", "b16"], "figure_ref": ["fig_3", "fig_7"], "table_ref": []}, {"heading": "Experiments on Simulated Data", "text": "Our algorithm works well for synthetic data. For all the experiments, our approach adopts a hierarchical structure using a grid of 256 landmarks with\u03b3 = 0.7 and T = 8 layers. For bases functions, we use Thin-Plate Spline [2] proper normalization. While our theory gives an upper bound of the sample complexity, practically 350 training samples over all layers suffice for good performance.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "Convergence Behavior", "text": "We artificially distorted 100 images with a 20dimensional global warping field specified in [17]. For each image, its 10 distorted versions are generated with random parameters, which are estimated using Data-driven Descent (TN) [17] and using our approach.\nFig. 6 shows the performance comparison. Our algorithm obtains much better performance and lower variance compared to TN with the same number of training samples. Note that the strong drop in error shows that our method achieves very high accuracy by adding very few samples once it starts to work. This coincides with Fig. 1.", "publication_ref": ["b16", "b16"], "figure_ref": ["fig_9", "fig_0"], "table_ref": []}, {"heading": "Deformation Estimation on Repetitive Patterns", "text": "We further test our approach on synthetic data containing distorted repetitive patterns, and compare it with previ- The overall degree of freedom for this dataset is very high (50 dimensions are needed to achieve < 1 pixel reconstruction error). It is in general impossible to have sufficient number of samples for global optimality conditions to be satisfied. However, practically our method still works well.\nWe compare our approach to the following previous methods: Lucas-Kanade (LK) [1], Data-driven Descent (TN) [17], Free-form registration (FF) [9], Explicit Shape Regression (ESR) [3] and SIFT matching with outlier removal using RANSAC (SR) [6]. LK and TN use a local parametric deformation model. LK uses local affine bases of size 100-by-100, and TN uses a 20-dimensional smooth bases of size 57-by-40 [17]. LK, FF and TN compute dense deformations and our hierarchy outputs 256 predicted landmarks, from which 49 landmark locations are interpolated. The KLT tracker [13] requires temporal information and will be compared in the real video sequence.\nFor one image, the RMS error is computed between the estimated landmark locationsp and ground truth locations\np as RM S = 1 K K i=1 ||p(i) \u2212p(i)|| 2 .\nFor multiple images, averaged RMS is reported.\nTable 1 compares the performance. Due to repetitive patterns, previous approaches fail to estimate the landmarks correctly. SIFT matching fails completely. The prediction of ESR is restricted to be on the linear shape subspace spanned by the training samples. Thus, it is insufficient to use the template to capture the subspace of a complex deformation field. LK and FF are stuck in local maxima despite their coarse-to-fine implementations. Our approach obtains the best performance. Fig. 7 shows the progression of our algorithm. In terms of speed, our approach is second only to ESR, which uses a fast boosting framework.\nInfluence of multiple layers. It is interesting to see how the performance changes if we switch off the first L layers of predictors. As shown in Table 2, the first two layers have less contribution on the performance than the rest of the layers. On the other hand, the lower 6 layers indeed  [1], Data-driven Descent (TN) [17], Freeform registration (FF) [9], Explicit Shape Regression [3] and SIFT matching with outlier removal using RANSAC (SR) [6]. Ours is the best performer and second best in time cost per frame. help the performance. Fig. 8 demonstrates how prediction from coarse layers (large patch) help the lower layer (small patch) find correct correspondences in repetitive patterns, justifying the hierarchy.", "publication_ref": ["b0", "b16", "b8", "b2", "b5", "b16", "b12", "b0", "b16", "b8", "b2", "b5"], "figure_ref": ["fig_10"], "table_ref": ["tab_3"]}, {"heading": "Real Experiments", "text": "We also apply our framework to real world scenarios such as water distortion, cloth deformation and registration of medical images. In Fig. 9, contour tracking is achieved by interpolating contour points from frame correspondences, while the contour of the first frame is manually labeled. In Fig. 10, tracked mesh is shown.\nThe three water distortion sequences (Row 1-2 in Fig. 9, Row 1 in Fig. 10) and one cloth sequence (Row 3 in Fig. 9) are from [17]. Two cloth sequences (Row 2-3 in Fig. 10) are from [15] and [8]. The medical sequence of cardiac magnetic resonance images (4th row in Fig. 9) is from [18]. We captured the cloth sequence in the 5th row of Fig. 9.\nFor the sequences on the 4th row of Fig. 9 and the 1st row of Fig. 10, we use temporal information by adding training samples generated from perturbing the final estimation of the previous frame. This slows down the processing to 0.3-0.5fps, yet is still faster than previous approaches. For other sequences, our algorithm runs at around 3-4 fps.\nNote that our method successfully estimates the deformations. In comparison, SIFT+RANSAC only obtains a sparse set of distinctive matches, not enough for estimating a nonrigid deformation (even if we are using Thin-Plate Spline). TN can capture detailed local deformations but not global shifts of the cloth without modeling the relationship between local patches. KLT trackers lose the target quickly and localize contour inaccurately.\nWe also quantitatively measure the landmark localization error using the densely labeled dataset provided in [17], which contains 30 labeled frames, each with 232 landmarks. In terms of RMS, LK gives 5.20, FF gives 3.93, TN gives 2.51 while our approach gives 3.29. Our framework is only second to TN, which is much slower.\nWe have tested our algorithm on existing datasets of deformable objects proposed by [10,11]. Although no groundtruth is available, our performance is close to their published results (e.g. 4.10 mean pixel distance difference in cushion video [10] and 4.43 in bed-sheet video [11]). All video sequences are 404-by-504.", "publication_ref": ["b16", "b14", "b7", "b17", "b16", "b9", "b10", "b9", "b10"], "figure_ref": ["fig_0", "fig_0", "fig_0", "fig_0"], "table_ref": []}, {"heading": "Sample Frame", "text": "Our approach TN KLT SIFT+RANSAC Template Figure 9. Example contour localization results given by our approach, TN [17], KLT [13], and SIFT matching with RANSAC [6]. Each row is a video sequence, two from underwater imaging, two from cloth deformation and the final one is from medical imaging. For each dataset, one sample frame is shown. The contours are drawn manually for the template image (1st column), and are transferred to every video frame after the correspondence was found. Our approach is stable and better than other approaches. (Best viewed in color)", "publication_ref": ["b16", "b12", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Sample Frame", "text": "Our approach TN KLT FF SIFT+RANSAC Figure 10. Example dense correspondence results given by our approach, TN, KLT, FF and SIFT matching with RANSAC. Each row is a video, two from cloth deformation and one from underwater imaging. The mesh is a regular grid on the template.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "", "text": "Acknowledgement. This research was supported in parts by ONR grant N00014-11-1-0295, a Microsoft Research PhD fellowship and a University Transportation Center T-SET grant.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Lucas-kanade 20 years on: A unifying framework", "journal": "IJCV", "year": "2004", "authors": "S Baker; I Matthews"}, {"ref_id": "b1", "title": "Principal warps: Thin-plate splines and the decomposition of deformations", "journal": "PAMI", "year": "1989", "authors": "F L Bookstein"}, {"ref_id": "b2", "title": "Face alignment by explicit shape regression", "journal": "", "year": "2006", "authors": "X Cao; Y Wei; F Wen; J Sun"}, {"ref_id": "b3", "title": "Imagenet classification with deep convolutional neural networks", "journal": "", "year": "2012", "authors": "A Krizhevsky; I Sutskever; G Hinton"}, {"ref_id": "b4", "title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "journal": "", "year": "2006", "authors": "S Lazebnik; C Schmid; J Ponce"}, {"ref_id": "b5", "title": "Distinctive image features from scale-invariant keypoints", "journal": "IJCV", "year": "2004", "authors": "D Lowe"}, {"ref_id": "b6", "title": "Active appearance models revisited", "journal": "IJCV", "year": "2004", "authors": "I Matthews; S Baker"}, {"ref_id": "b7", "title": "Optimal templates for non-rigid surface reconstruction", "journal": "", "year": "2012", "authors": "M Moll; L V Gool"}, {"ref_id": "b8", "title": "Nonrigid registration using free-form deformations: application to breast MR images", "journal": "Medical Imaging", "year": "1999", "authors": "D Rueckert; L Sonoda; C Hayes; D Hill; M Leach; D Hawkes"}, {"ref_id": "b9", "title": "Convex optimization for deformable surface 3-d tracking", "journal": "", "year": "2007", "authors": "M Salzmann; R Hartley; P Fua"}, {"ref_id": "b10", "title": "Closedform solution to non-rigid 3d surface registration", "journal": "", "year": "2008", "authors": "M Salzmann; F Moreno-Noguer; V Lepetit; P Fua"}, {"ref_id": "b11", "title": "Object recognition with features inspired by visual cortex", "journal": "", "year": "2005", "authors": "T Serre; L Wolf; T Poggio"}, {"ref_id": "b12", "title": "Good features to track", "journal": "", "year": "1994", "authors": "J Shi; C Tomasi"}, {"ref_id": "b13", "title": "Real-time human pose recognition in parts from single depth images", "journal": "", "year": "2011", "authors": "J Shotton; A Fitzgibbon; M Cook; T Sharp; M Finocchio; R Moore; A Kipman; A Blake"}, {"ref_id": "b14", "title": "Non-Rigid Structure from Locally-Rigid Motion", "journal": "", "year": "2010", "authors": "J Taylor; A Jepson; K Kutulakos"}, {"ref_id": "b15", "title": "Detailed derivation of theory of hierarchical data-driven descent", "journal": "", "year": "2013", "authors": "Y Tian; S Narasimhan"}, {"ref_id": "b16", "title": "Globally optimal estimation of nonrigid image distortion", "journal": "IJCV", "year": "2008", "authors": "Y Tian; S G Narasimhan"}, {"ref_id": "b17", "title": "Shape prior modeling using sparse representation and online dictionary learning", "journal": "Springer", "year": "2012", "authors": "S Zhang; Y Zhan; Y Zhou; M Uzunbas; D Metaxas"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. Illustrations of order of training sample complexity required for estimating d dimensional deformation. (a): To achieve a guaranteed accuracy 1/\u01eb, traditional regression-based approaches (e.g. Nearest Neighbor) require O(1/\u01eb d ) training samples. Datadriven Descent [17] requires O(C d log 1/\u01eb), decoupling the dimensionality from the accuracy. Our hierarchical framework for deformation estimation achieves O(C d 1 + C2 log 1/\u01eb) with constant C1 much smaller than C and C2 independent of dimensionality. (b) Sample complexity per iteration. A constant number of samples per iteration is needed in[17]. The number of samples needed is a constant for the first few iterations, and then decays double exponentially for our algorithm.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 .2Figure 2. Local parameterization of deformation. (a)-(b) The deformation field is controlled by a set of landmarks on the template image. By moving these landmarks, a deformed image is created. (c) Local parameterization. Each parameter p(i) encodes the 2D displacement of the landmark i. (d) Displacement on any pixel x is interpolated using displacements of nearby landmarks.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 .3Figure 3. Relaxed Lipchitz Condition (Eqn. 4). (a) Four constants (\u03b1, \u03b3, A, \u0393) capture the correlations between \u2206I and \u2206p. When \u2206p is small (\u2264 \u03b1r), \u2206I is small as well (\u2264 Ar). Conversely, with large parameter difference (\u2265 \u03b3r), the image difference is also large (\u2265 \u0393r). (b) Given \u03b1, there exists a minimal \u03b3. (c) For a monotonic relationship between \u2206I and \u2206p, \u03b1 = \u03b3 \u2208 [0, 1].", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "1 :1INPUT Training samples T r(x, r) \u2261 {(p i , I i )} for each image patch (x, r). 2: INPUT Test image I test with unknown parameters p. 3: Set an initial estimationp 0 = 0. 4: for t = 1 to T do 5: Set the current image I c (x) = I test (W (x;p t\u22121 )).", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 5 .5Figure5. Exemplar images and the theoretical bounds for the number of samples needed per dimension. For each bracket, the first number is our bound (given by 1/\u03b10.95), while the second number from Data-Driven Descent (given by L2/\u03b3L1 with \u03b3 = 0.95). Top Row: Images with a salient object and clean background require only a few samples per dimension. Bottom Row: Images with repetitive patterns require more samples per dimension. In both cases, our bound is smaller than that given by Data-Driven Descent.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "= 3 )3up to \u00b1\u03c0/8. We propose Alg. 2 which only costs O(M log M ) to compute the curve \u03b3 = \u03b3(\u03b1), while brute-force search costs O(M 3 ).", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 6 .6Figure 6. Performance of the proposed algorithm. Left: Performance comparison with[17]. Accuracy of our approach increases much faster than[17] with the same number of samples. To obtain the same level of accuracy of our approach with 400 samples,[17] requires 10000 samples or more. Our approach also has lower variance in performance. Right: Convergence behavior of our approach with different number of training samples.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 7 .7Figure 7. Demonstration of the iterative procedure of our algorithm. Starting from initialization, the algorithm applies predictors of different layers to estimate the landmark locations. Numbers on top show RMS errors.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Work flow of our hierarchical algorithm for deformation estimation. On Layer 1, a global prediction is made and the estimation is updated. On Layer 2, local deformation is estimated and aggregated. The procedure repeats until the last layer.", "figure_data": "Layer 1Layer 2Layer 3S 1S 2Initialization NN Prediction EstimationNN Prediction for subset S 1 p 1p2 NN Prediction for subset S 2 EstimationFigure 4. [N1=3, N2=24][N1=4, N2=16][N1=5,N2=24] [N1=5, N2=19][N1=5, N2=28][N1=6, N2=24][N1=6, N2=24](a) Easy Images for deformation estimation. N1 = #sample per dim\u23081/\u03b1\u2309in our methodN2 = #sample per dim\u2308L2/\u03b3L1\u2309given by [Tian and Narasimhan, IJCV 2012][N1=16, N2=30][N1=15, N2=24][N1=14, N2=41][N1=14, N2=40][N1=13, N2=41][N1=13, N2=45][N1=12, N2=40](b) Hard Images for deformation estimation."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Figure 8. Performance changes if the first K layers are switched off. When more layers are switched off, the algorithm is unable to identify global deformation and is essentially the same as local template matching at each landmark.", "figure_data": "Use all layerslayer 1-3 offlayer 1-5 offlayer 1-6 off3.749.3110.8811.19LKTNESRFFSROursRMS14.79 6.44 8.98 7.29 98.94 5.63sec/frame11770.012351.250.10Table 1. Performance comparison of different approaches, includ-ing Lucas-Kanade (LK)"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Performance on synthetic data if the first L layer of predictors are switched off, showing the bottom layers play a critical role for performance.", "figure_data": "L0123456RMS 5.63 5.20 5.14 5.83 6.72 7.95 8.74"}], "formulas": [{"formula_id": "formula_0", "formula_text": "I p (W (x; p)) = T (x)(1)", "formula_coordinates": [2.0, 383.34, 317.75, 161.78, 10.32]}, {"formula_id": "formula_1", "formula_text": "W (x; p) = x + B(x)p(2)", "formula_coordinates": [2.0, 379.43, 387.01, 165.68, 9.96]}, {"formula_id": "formula_2", "formula_text": "B(x) = [b 1 (x), b 2 (x), . . . , b K (x)", "formula_coordinates": [2.0, 340.93, 404.65, 153.32, 10.71]}, {"formula_id": "formula_3", "formula_text": "L 1 \u2206I \u2264 \u2206p \u2264 L 2 \u2206I (3)", "formula_coordinates": [3.0, 122.91, 317.18, 163.46, 17.04]}, {"formula_id": "formula_4", "formula_text": "||p 1 || \u221e \u2264 r, ||p 2 || \u221e \u2264 r: \u2206p \u2264 \u03b1r =\u21d2 \u2206I \u2264 Ar, \u2206p \u2265 \u03b3r =\u21d2 \u2206I \u2265 \u0393r (4) for \u2206p \u2261 ||p 1 (S) \u2212 p 2 (S)|| \u221e and \u2206I \u2261 ||I p1 (R) \u2212 I p2 (R)||.", "formula_coordinates": [3.0, 308.86, 234.3, 236.25, 59.97]}, {"formula_id": "formula_5", "formula_text": "N (x, r) = min c ss \u23081/\u03b1\u2309 d , \u23081/\u03b1\u2309 2|S|(", "formula_coordinates": [4.0, 85.64, 276.81, 192.98, 19.8]}, {"formula_id": "formula_6", "formula_text": "\u03b3 \u2261 max (x,r) \u03b3(x, r) < 1 (7)", "formula_coordinates": [4.0, 383.9, 235.95, 161.21, 17.04]}, {"formula_id": "formula_7", "formula_text": "i * = arg min i\u2208T r(x,r) ||I c (R j ) \u2212 I i (R j )|| 9:", "formula_coordinates": [4.0, 314.62, 370.22, 197.06, 22.59]}, {"formula_id": "formula_8", "formula_text": "||p t (i) \u2212 p(i)|| \u221e \u2264\u03b3 t r 1 \u2192 0 when t \u2192 +\u221e (8)", "formula_coordinates": [4.0, 324.5, 699.17, 220.61, 18.91]}, {"formula_id": "formula_9", "formula_text": "N \u2264 C 3 C d 1 + C 2 log 1/\u03b3 1/\u01eb (9", "formula_coordinates": [5.0, 111.45, 504.24, 171.04, 18.91]}, {"formula_id": "formula_10", "formula_text": ")", "formula_coordinates": [5.0, 282.49, 506.85, 3.87, 8.91]}, {"formula_id": "formula_11", "formula_text": "where C 1 = 1/ min (x,r) \u03b1(x, r), C 2 = 2 1/(1\u2212\u03b3 2 ) and C 3 = 2 + c ss (\u2308 1 2 log 1/\u03b3 2K/d\u2309 + 1).", "formula_coordinates": [5.0, 50.11, 528.7, 236.25, 28.07]}, {"formula_id": "formula_12", "formula_text": "\u2206I + i = max 1\u2264j\u2264i \u2206I j , for i = 1 . . . M . \u2206I \u2212 i = min i\u2264j\u2264M \u2206I j , for i = 1 . . . M . for i = 1 to M do Find minimal j so that \u2206I \u2212 j > \u2206I + i . if i \u2264 j then", "formula_coordinates": [6.0, 60.08, 109.47, 165.4, 64.53]}, {"formula_id": "formula_13", "formula_text": "p as RM S = 1 K K i=1 ||p(i) \u2212p(i)|| 2 .", "formula_coordinates": [6.0, 308.86, 491.06, 166.49, 19.8]}], "doi": ""}