{"title": "Summarizing Itemset Patterns: A Profile-Based Approach *", "authors": "Xifeng Yan; Hong Cheng; Jiawei Han; Dong Xin", "pub_date": "", "abstract": "Frequent-pattern mining has been studied extensively on scalable methods for mining various kinds of patterns including itemsets, sequences, and graphs. However, the bottleneck of frequent-pattern mining is not at the efficiency but at the interpretability, due to the huge number of patterns generated by the mining process. In this paper, we examine how to summarize a collection of itemset patterns using only K representatives, a small number of patterns that a user can handle easily. The K representatives should not only cover most of the frequent patterns but also approximate their supports. A generative model is built to extract and profile these representatives, under which the supports of the patterns can be easily recovered without consulting the original dataset. Based on the restoration error, we propose a quality measure function to determine the optimal value of parameter K. Polynomial time algorithms are developed together with several optimization heuristics for efficiency improvement. Empirical studies indicate that we can obtain compact summarization in real datasets.", "sections": [{"heading": "INTRODUCTION", "text": "Mining frequent patterns is an important data mining problem with broad applications, including association rule mining, indexing, classification, and clustering (see e.g., [2,27,26,8,15]). Recent studies on frequent-pattern mining have seen significant performance improvements on efficient identification of various kinds of patterns, e.g., itemsets, sequences, and graphs ( [2,3,14,7]). Patterns with other interesting measures were also examined extensively (see e.g., [11,5,20,25,19]). However, the major challenge of frequent-pattern mining is not at the efficiency but at the interpretability: the huge number of frequent patterns makes the patterns themselves difficult to explore, thus hampering the individual and global analysis of discovered patterns.\nThere are two sources leading to the interpretability issue. First, the rigid definition of frequent patterns often generates a large number of redundant patterns, most of which are slightly different. A pattern is frequent if and only if it occurs in at least \u03c3 fraction of a dataset. According to this definition, any subset of a frequent itemset is frequent. This downward closure property leads to an explosive number of frequent patterns. For example, a frequent itemset with n items may generate 2 n sub-itemsets, all of which are frequent. The introduction of closed frequent itemsets [20] and maximal frequent itemsets [11,5] can partially alleviate this redundancy problem. A frequent pattern is closed if and only if a super-pattern with the same support does not exist. A frequent pattern is maximal if and only if it does not have a frequent super-pattern. Unfortunately, for any pattern \u03b1, as long as there is a small disturbance on the transactions containing \u03b1, it may generate hundreds of subpatterns with different supports. We term the original pattern, \u03b1, a master pattern and its deviated subpatterns, derivative patterns. Intuitively, it is more interesting to examine the master patterns, rather than the derivative patterns. Unfortunately, there is no clear boundary between master patterns and their derivatives.\nSecondly, as long as the number of discovered patterns is beyond tens or hundreds, it becomes difficult for a user to examine them directly. A user-friendly program should present the top-k distinct patterns first and arrange the remaining patterns in a tree structure so that a user can start quickly from a small set of representative patterns. The patterns delivered by the existing top-k mining algorithms such as [12] are the most frequent closed itemsets, but not distinct ones. Users often prefer distinct patterns with little overlap for interactive exploration.\nIn this paper, we intend to solve the pattern interpretability issue by summarizing patterns using K representatives. There are three subproblems around this summarization task: what is the format of these representatives? how can we find these representatives? and what is the measure of their quality?\nA number of proposals have been made to construct a concise and lossless representation of frequent patterns. For example, Pasquier et al. [20] introduced the concept of closed frequent patterns, and Calders et al. [6] proposed mining all non-derivable frequent itemsets. These kinds of patterns are concise in the sense that all of the frequent patterns can be derived from them. Unfortunately, the number of patterns generated in these two approaches is still too large to handle.\nResearchers have also developed lossy compression methods to summarize frequent patterns: maximal patterns by Gunopulos [11], top-k patterns by Han et al. [12], errortolerant patterns by Yang et al. [28], and condensed pattern bases by Pei et al. [22]. Nevertheless, the discrimination between patterns is not emphasized in these studies. Generally speaking, a user may not only be interested in a small set of patterns, but also patterns that are significantly different. A recent approach proposed by Afrati et al. [1] uses K itemsets to cover a collection of frequent itemsets. Their solution is interesting but leaves the support integration issue open: It is unknown how to cover the support information in a summarization. In this paper, we investigate this issue and further advance the summarization concept.\nGiven a set I of items o1, . . . , o d and a transaction dataset D = {t1, . . . , tn}, where each transaction is a subset of I. The pattern collection F is a set of patterns \u03b11, . . . , \u03b1m, \u03b1i \u2286 I. We are interested in partitioning the pattern set into K groups such that the similarity within each group is maximized and the similarity between the groups is minimized. Frequent patterns are distinguished from each other not only because they have different composition, but also because they have different supports. Suppose \u03b1i and \u03b1j exhibit strong similarity on these two criteria. It is likely that \u03b1i and \u03b1j can be merged to one pattern, \u03b1i \u222a \u03b1j. This is the intuition behind merging two patterns (or clustering them in the same group). We develop a generative model M to measure these two similarity criteria simultaneously. Based on this model, we are able to evaluate the quality of such merging by measuring the probability that \u03b1i and \u03b1j are generated by M.\nUsing the above generative model, we can arrange all of the patterns in a tree structure using a hierarchical agglomerative clustering method, where patterns with the highest similarity are grouped together first. In this hierarchical tree, a user can flexibly explore patterns with different summarization granularity. Our methods can successfully compress thousands of frequent patterns into hundreds or even tens of distinct patterns.\nOur major contributions are outlined as follows.\n1. We propose a statistical model which is good not only at summarizing patterns, but also at integrating their supports. After compressing numerous patterns to K representatives, our methods are able to recover these patterns and their supports from the K representatives.", "publication_ref": ["b1", "b26", "b25", "b7", "b14", "b1", "b2", "b13", "b6", "b10", "b4", "b19", "b24", "b18", "b19", "b10", "b4", "b11", "b19", "b5", "b10", "b11", "b27", "b21", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "2.", "text": "A principled similarity measure based on Kullback-Leibler divergence is developed to group highly correlated patterns together. We show that the summarization algorithm based on this measure can complete in polynomial time.\n3. We use K representatives to summarize the pattern set and devise a mechanism to estimate the support of frequent patterns from these K representatives only. In addition, the estimation error is used to evaluate the summarization quality. We monitor quality changes over different Ks in order to determine the optimal number of representatives for a given pattern set. To the best of our knowledge, ours is the first algorithm that can guide the selection of K, thus eliminating the obstacle for the applicability of pattern summarization.\n4. Empirical studies indicate that the method can build very compact pattern summarization in many real data sets. For example, on a typical mushroom dataset 1 , the method can summarize thousands of frequent patterns accurately using around 30 patterns.\nThe rest of the paper is organized as follows. Section 2 introduces the concept of pattern profile for similar frequent patterns. The details of the similarity measure, the quality evaluation function, as well as the summarization algorithms are introduced in Section 3. We report our experimental results in Section 4, discuss related work in Section 5, and conclude our study in Section 6.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "PATTERN PROFILE", "text": "Let I be a set of items o1, o2, . . . , o d . A subset of I is called an itemset. A transaction dataset is a collection of itemsets, D = {t1, . . . , tn}, where ti \u2286 I. For any itemset \u03b1, we write the transactions that contain \u03b1 as D\u03b1 = {ti|\u03b1 \u2286 ti and ti \u2208 D}. Frequent itemsets have the Apriori property: any subset of a frequent itemset is frequent [2]. Since the number of subsets of a large frequent itemset is explosive, it is more efficient to mine closed frequent itemsets only.  Note that in Definition 1, for a transaction that contributes to the support of a pattern, it must contain the entire pattern. The rigid definition of closed frequent patterns causes a severe problem. A small disturbance within the transactions may result in hundreds of subpatterns that could have different supports. There exists significant pattern redundancy since many patterns are actually derived from the same pattern. On the other hand, a pattern could be missed if its support is below \u03c3 while its derivative subpatterns pass the threshold. In this situation, it is necessary to assemble small correlated subpatterns together to recover it. If we relax the exact matching criterion in the support definition and allow one item missing in a pattern, abcd becomes the only pattern in D1.\nDefinition 1 (Frequent Itemset). For a transaction dataset D, an itemset \u03b1 is frequent if |D\u03b1| |D| \u2265 \u03c3,\nInspired by this example, we find that it is very important to group similar itemsets together to eliminate redundant patterns. We can merge the itemsets in each group to a master pattern. A master pattern is the union of all the itemsets within a group. Before we formalize the format of pattern summarization, let us first examine what a good summarization means.\nSuppose \u03b1 and \u03b2 can be grouped together to form a master pattern, \u03b1 \u222a \u03b2. That means the supports of \u03b1, \u03b2, and \u03b1 \u222a \u03b2 should not be significantly different from each other. Or more fundamentally, D\u03b1 and D \u03b2 should be highly correlated so that the transactions containing \u03b1 will likely contain \u03b2, and vice versa. Although the support similarity is one of the major criteria in determining whether we should summarize two patterns into one, there is another measure that determines how good a summarization is. Let us compare D1 with another dataset D2 shown in Figure 2. For these two datasets, we can summarize all the subpatterns of abcd as abcd because their supports are very close to each other. However, the quality of these two summarizations is different. If we allow approximate matching between a pattern and a transaction, pattern abcd is more likely contained by transactions \"abc\" in D1 (one item missing) than by transactions \"a\" in D2 (three items missing). That means, the summarization of D1 as abcd has better quality. This intuition will be clarified when we explain our pattern profile model below.\nAccording to the above discussion, we propose using a probability profile to describe a representative, instead of using an itemset only. The profile has a probability distribution on items. Suppose patterns \u03b11, \u03b12, . . . , \u03b1 l are grouped together to form a master pattern \u03b11 \u222a \u03b12 \u222a . . . \u222a \u03b1 l . We can estimate the distribution vector that generates the dataset\nD \u2032 = l i=1 D\u03b1 i . P (D \u2032 |\u03b8) = t j \u2208D \u2032 d i=1 p(xi = t i j ),(1)\nwhere t i j is the value of xi in the j th transaction and \u03b8 is a set of probability {p(xi)}. When t i j = 1, it means that the j th transaction has item oi.\nAccording to maximum likelihood estimation (MLE), the \"best\" generative model should maximize the log likelihood L(\u03b8|D \u2032 ) = log P (D \u2032 |\u03b8), which leads to\n\u2202L(\u03b8|D \u2032 ) \u2202\u03b8 = 0 (2)\nThe well-known result is\np(xi = 1) = t j \u2208D \u2032 t i j |D \u2032 | .(3)\nThat is, p(xi = 1) is the relative frequency of item oi in D \u2032 . We use a probability distribution vector derived from MLE to describe the item distribution in a set of transactions.\nHere we formulate the concept of pattern profile.  If we want to build a pattern profile for abcd in D1 and D2, we can derive the distribution vectors for the sample datasets in Figures 1 and 2. The resulting profiles are shown in Table 1. For example, p(a) = 50+1000 50+100+1000 = 0.91. Table 1 demonstrates that the profile derived from D1 has higher quality since it is much closer to a perfect profile, a profile with p(xi = 1) = 1.0 for oi \u2208 \u03c6. In addition, without accessing the original dataset, we can conclude that bcd in D1 is more frequent than the other size-3 subpatterns of abcd . Pattern profile actually provides more information than the master pattern itself; it encodes the distribution of subpatterns. The key difference between our profile model and the itemset model proposed by Afrati et al. [1] is that we do not represent a collection of patterns using the maximal pattern only; instead, we apply a profile model that can accommodate the patterns themselves as well as their supports. Conceptually, our profile model is similar to the well-known PSSM model in the DNA sequence alignment [10].\nThe support of a pattern in a dataset D can be regarded as the average probability of observing a pattern from a transaction,\np(\u03b1) = t\u2208D p(\u03b1|t) * p(t),\nwhere p(t) = 1 |D| and p(\u03b1|t) = 1 if \u03b1 \u2286 t, 0 otherwise. In our profile model, we can regard the probability of observing a pattern as the probability that the pattern is generated by its corresponding profile times the probability of observing this profile from a transaction,\np(\u03b1|t) \u223c p(\u03b1|M) * p(M|t),(4)\nwhere we assume the conditional independence p(\u03b1|M, t) = p(\u03b1|M). According to this model, we can estimate the support for a given pattern \u03b1 from the profile it belongs to.\nDefinition 5 (Estimated Support). Let M be a profile over a set of patterns {\u03b11, \u03b12, . . . , \u03b1 l }. The estimated support of \u03b1 k is written as\u015d(\u03b1 k ),\ns(\u03b1 k ) = s(M) \u00d7 o i \u2208\u03b1 k p(xi = 1),(5)\nwhere\ns(M) = |D\u03b1 1 \u222a...\u222aD\u03b1 l | |D|\n, p is the distribution vector of M and xi is the boolean random variable indicating the selection of item oi in pattern \u03b1 k . Surprisingly, the calculation of an estimated support is only involved with d + 1 real values: the d-dimensional distribution vector of a profile and the number of transactions that support the profile. This result becomes one of the most distinguishing features in our summarization model. It means that we can use very limited information in a profile to recover the supports of a rather large set of patterns.", "publication_ref": ["b1", "b0", "b9", "b0"], "figure_ref": ["fig_1", "fig_1"], "table_ref": ["tab_0", "tab_0"]}, {"heading": "PATTERN SUMMARIZATION", "text": "Our pattern profile model shows how to represent a set of patterns in a compact way and how to recover their supports without accessing the original dataset. However, the problem of selecting a set of similar patterns for summarization is not yet solved. We formalize this summarization problem as follows.\nDefinition 6 (Pattern Summarization). Given a set of patterns F = {\u03b11, \u03b12, . . . , \u03b1m} that are mined from a database D = {t1, t2, . . . , tn}, pattern summarization is to find K pattern profiles based on the pattern set F .\nA potential solution to the summarization problem is to group frequent patterns into several clusters such that the similarity within clusters is maximized and the similarity between clusters is minimized. Once the clustering is done, we can calculate a profile for each cluster.\nWe can construct a specific profile for each pattern that only contains that pattern itself. Using this representation, we can measure the distance between two patterns based on the divergence between their profiles. The distance between two patterns should reflect the correlation between the transactions that support these two patterns. Namely, if two patterns \u03b1 and \u03b2 are correlated, D\u03b1 and D \u03b2 likely have large overlap; and the non-overlapping parts exhibit high similarity. Several measures are available to fulfill this requirement. A well-known one is the Kullback-Leibler divergence between the distribution vectors in the profiles of \u03b1 (M\u03b1) and \u03b2 (M \u03b2 ),\nKL(p||q) = d i=1 x i \u2208{0,1} p(xi) log p(xi) q(xi) , (6\n)\nwhere p is the distribution vector of M\u03b1 and q is the distribution vector of M \u03b2 . When p(xi) and q(xi) have zero probability, KL(p||q) = \u221e. In order to avoid this situation, we smooth the probability of p(xi) (and q(xi)) with a background prior,\np \u2032 (xi) = \u03bbu + (1 \u2212 \u03bb)p(xi),\nwhere \u03bb is a smoothing parameter, 0 < \u03bb < 1, and u could be the background distribution of item oi.\nWhen KL(p||q) is small, it means that the two distribution vectors p and q are similar, and vice versa. This could be justified by Taylor's formula with remainder,\nKL(p(xi)||q(xi)) = \u03b8log \u03b8 \u03b7 + (1 \u2212 \u03b8)log 1 \u2212 \u03b8 1 \u2212 \u03b7 = \u03b8 \u2212 \u03b7 + \u03b8o( \u03b8 \u03b7 \u2212 1) + \u03b7 \u2212 \u03b8 + (1 \u2212 \u03b8)o( 1 \u2212 \u03b8 1 \u2212 \u03b7 \u2212 1) = \u03b8o( \u03b8 \u2212 \u03b7 \u03b7 ) + (1 \u2212 \u03b8)o( \u03b7 \u2212 \u03b8 1 \u2212 \u03b7 ),\nwhere \u03b8 = p(xi = 1) and \u03b7 = q(xi = 1). When 0 < \u03b8 < 1 and 0 < \u03b7 < 1, it implies that\nKL(p(xi)||q(xi)) < \u01eb \u21d4 p(xi) \u223c q(xi).\nNote that for any oi \u2208 \u03b1, p(xi = 1) = 1 and for any oi \u2208 \u03b2, q(xi = 1) = 1 (after smoothing, both of them are close to 1, but not equal to 1). When two distribution vectors p and q of patterns \u03b1 and \u03b2 are similar, transactions containing \u03b1 will likely contain \u03b2 too, and vice versa. It indicates that these two patterns are strongly correlated, i.e., p(xi) \u223c q(xi) \u21d4 D\u03b1 \u223c D \u03b2 . Likely, patterns \u03b1 and \u03b2 are derivative patterns of the same pattern \u03b1 \u222a \u03b2. Therefore, KL-divergence can serve as a distance measure for the pattern summarization task. This conclusion can further be justified by the connection of KL-divergence with the following generative model.\nGiven several profiles {M\u03b1}, and a pattern \u03b2, we have to decide how likely \u03b2 is generated by a profile M\u03b1, or how likely D \u03b2 is generated by M\u03b1. If D \u03b2 is generated by M\u03b1, then we cannot tell the difference between D \u03b2 and the transactions D\u03b1 covered by M\u03b1. In this case, we can put patterns \u03b1 and \u03b2 in one cluster. The best profile in {M\u03b1} should maximize\nP (D \u03b2 |M\u03b1) = t j \u2208D \u03b2 d i=1 p(xi = t i j ),\nwhere p is the distribution vector of M\u03b1. It is equivalent to maximizing L(M\u03b1) = log P (D \u03b2 |M\u03b1). Let q be the distribution vector of pattern \u03b2.\nL(M\u03b1) n = 1 n t j \u2208D \u03b2 d i=1 log p(xi = t i j ) = d i=1 ni n log p(xi = 1) + n \u2212 ni n log p(xi = 0) = d i=1 x i \u2208{0,1} q(xi) log p(xi),(7)\nwhere n = |D \u03b2 | and ni is the number of transactions (in D \u03b2 ) having item oi. We may add the entropy of q,\nH(q(x)) = \u2212 d i=1 x i \u2208{0,1} q(xi) log q(xi),\non the left and right side of Eq. (7). Hence,\nL(M\u03b1) n + H(q(x)) = d i=1 q(xi = 1) log p(xi = 1) q(xi = 1) + q(xi = 0) log p(xi = 0) q(xi = 0) = \u2212KL(q(x)||p(x))(8)\nAccording to Eq. ( 8), the best profile maximizing P (D \u03b2 |M\u03b1) is the profile that minimizes KL(q(x)||p(x)). That means that KL-divergence can measure how likely a pattern is generated from a profile, indicating that it is a reasonable distance measure for grouping profiles.\nIn the rest of this section, we will introduce our summarization methods based on hierarchical clustering and K-means clustering, as well as the potential optimization heuristics. We are not going to explore the details of clustering algorithms, since essentially any clustering algorithm based on KL-divergence can be used. We will focus on the practical issues raised by pattern summarization.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Hierarchical Agglomerative Clustering", "text": "Hierarchical clustering produces a dendrogram where two clusters are merged together at each level. The dendrogram allows a user to explore the pattern space in a top-down manner and provides a global view of patterns. calculate the KL-divergence between C and the remaining clusters; 10:\nk = k \u2212 1; 11: return; Algorithm 1 outlines the pattern summarization process using a hierarchical clustering approach. At the beginning, it calculates the KL-divergence between any pair of patterns. In the following iterations, it repeats Lines 3-10 by selecting the two clusters which have the smallest KL-divergence (Line 4) and merging them into one cluster. The iteration procedure terminates when the total number of clusters becomes K.\nA cluster C is defined as a collection of patterns, C \u2286 F . The set of transactions that contain a pattern in C is written as DC = \u222a\u03b1D\u03b1, \u03b1 \u2208 C and the master pattern in C is written as IC = \u222a\u03b1\u03b1, \u03b1 \u2208 C. The newly merged cluster inherits the transactions that support the original clusters and the patterns that are owned by the original clusters. It has a newly built profile over the merged transactions (Line 8).\nThe profiling construction has to scan the dataset once, thus taking O(nd) for each merge operation, where n is the number of transactions in D, and d is the size of the global itemset I. The initial KL-divergence construction (Line 2) takes O(m 2 d), where m is the number of patterns. For each cluster Ci, we can maintain a distance list between Ci and other clusters and sort them in increasing order. Whenever a new cluster C is generated, the two merged clusters are deleted from the distance lists in time O(m). A new distance list is created for C and sorted in time O(mlogm). Note that we need not insert the distance from C in the existing distance lists. The minimum KL-divergence can be found by checking the first element in O(m) distance lists. Therefore, hierarchical clustering itself can be done in O(m 2 logm). Hence, Algorithm 1 can finish in O(m\n2 logm + m 2 d + mnd).\nWhen the dataset is very large, it may be expensive to recompute the profile by scanning the whole dataset. Fortunately, it is effective to profile a new cluster through sampling based on Hoeffding bound [13].", "publication_ref": ["b12"], "figure_ref": [], "table_ref": []}, {"heading": "K-means Clustering", "text": "One major computation cost in hierarchical clustering is the pair-wise KL-divergence calculation. In each iteration, Algorithm 1 has to calculate O(m) times of KL-divergence. In total, Algorithm 1 has to do O(m 2 ) KL-divergence computation. One approach to eliminate the quadratic cost is to adopt K-means clustering. Using K-means, we can achieve very fast clustering for a large number of patterns. 1: randomly select K patterns as the initial clusters; 2: for each pattern \u03b1 do assign its membership to the cluster that has the smallest KL-divergence KL(M\u03b1||MC j ); 3: update the profiles of newly formed clusters by Eq. (3); 4: repeat Lines 2-3 until small change in MC 1 , . . . MC K or the summarization quality does not increase; 5: return; Algorithm 2 outlines the major steps of the K-means algorithm. In the initial step, Algorithm 2 randomly selects K patterns as the initial cluster centers. In the following iterations, it reassigns patterns to clusters according to the KL-divergence criterion. The profiles of newly formed clusters are then updated (Line 3 Algorithm 2). This procedure will terminate until there is only small change in MC 1 , . . . , and MC K or it meets other stop conditions, e.g., the summarization quality does not increase any more (see Section 3.4).\nConceptually, Algorithm 2 is similar to distributional clustering [4] and divisive clustering [9]. The first difference is that we treat each dimension as a separate distribution while other approaches put all dimensions together and create a multinomial model. The second difference is that our quality evaluation function (see Section 3.4) is different from the mutual information loss function proposed by Dhillon et al. [9]. The third difference is that distributional clustering or divisive clustering mixes the profiles directly by assigning equal weight to each instance, while we prefer to recompute the profiles through the original dataset. In the experiment section, we will illustrate the performance difference between these two strategies.\nThe time complexity of Algorithm 2 is O((mkd + knd)r), where m is the number of patterns, k is the number of clusters, d is the number of distinct items, n is the number of transactions, and r is the number of iterations. Generally Kmeans clustering can complete clustering faster than hierarchical clustering. However, it cannot provide a hierarchical clustering tree for pattern navigation. Another drawback of K-means is that its output is highly related with the seed selection (Line 1, Algorithm 2), which is undesirable in some applications.", "publication_ref": ["b3", "b8", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Optimization Heuristics", "text": "We develop two optimization heuristics to speed up the clustering process.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Closed Itemsets vs. Frequent Itemsets", "text": "We can start the clustering process either from closed frequent itemsets or from frequent itemsets. The following lemma shows that either way generates the same result. Proof. If \u03b1 \u2286 \u03b2 and s(\u03b1) = s(\u03b2), then D\u03b1 = D \u03b2 , which leads to the above lemma.\nAny frequent itemset must have a corresponding closed frequent itemset. According to Lemma 1, their profiles have zero KL-divergence, indicating that the clustering based on frequent itemsets will be the same as the clustering based on closed frequent itemsets. Therefore, we can summarize closed frequent itemsets instead of frequent itemsets. Since the number of closed frequent itemsets is usually less than that of frequent itemsets, summarizing closed itemsets can significantly reduce the number of patterns involved in clustering, thus improving efficiency.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Approximate Profiles", "text": "Another issue is whether we should rebuild the profile from scratch as Algorithm 1 (Line 8) and Algorithm 2 (Line 3) do. The profile updating dominates the computation in both algorithms since it has to access the original dataset. A potential solution is to mix two profiles directly without checking the original dataset in Algorithm 1,\np(x) = |Cs| |Cs| + |Ct| ps(x) + |Ct| |Cs| + |Ct| pt(x),(9)\nor weigh each pattern's profile equally in Algorithm 2,\np(x) = 1 |C| \u03b1\u2208C p\u03b1(x). (10\n)\nThis approximation can significantly improve clustering efficiency. However, it may affect the summarization quality since the mixed profile may no longer reflect the real distribution. Traditional clustering algorithms usually assume the instances are sampled independently (i.i.d). However, in our case, the i.i.d. assumption for frequent patterns is not valid. Most of the frequent patterns are not independent at all. It may be incorrect to have an equal weight for each pattern or weigh two clusters according to their sizes.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Quality Evaluation", "text": "One way to evaluate the quality of a profile is to calculate the probability of generating its master pattern from its profile, p(\u03c6) = o i \u2208\u03c6 p(xi = 1). The closer p(\u03c6) to 1, the better the quality. Because the master patterns from different profiles have different sizes, it is pretty hard to combine multiple p(\u03c6)s to give a global quality assessment. We are going to examine an additional measure in this section.\nThrough Eq. (5) in Section 2, we show that the support of a pattern covered by one profile can be estimated through its distribution vector directly. A high quality profile should have this estimation as close as possible to the real support. When we apply this measure to a set of profiles, we encounter an ambiguity issue since one pattern may have different estimated supports according to different profiles. Suppose we are given minimum information: a set of profiles, each of which is a triple distribution vector, master pattern, support . The information about which pattern belongs to which profile is not given. For any pattern \u03b1, it could be a subset of several master patterns. In this situation, we may get multiple support estimations for \u03b1. Which one should we select? A simple strategy is to select the maximum one,\ns(\u03b1 k ) = max M s(M) \u00d7 o i \u2208\u03b1 k p M (xi = 1). (11\n)\nThis strategy is consistent with the support recovery for a frequent pattern given a set of closed frequent patterns. Let F be a set of closed frequent patterns. For any frequent pattern \u03b1, its support is the same as the maximum support of its super-pattern \u03b2, \u03b1 \u2286 \u03b2 and \u03b2 \u2208 F .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definition 7 (Restoration Error)", "text": ". Given a set of profiles M1, . . . , MK and a testing pattern set T = {\u03b11, \u03b12, . . . , \u03b1 l }, the quality of a pattern summarization can be evaluated by the following average relative error, called restoration error,\nJ = 1 |T | \u03b1 k \u2208T |s(\u03b1 k ) \u2212\u015d(\u03b1 k )| s(\u03b1 k ) . (12\n)\nRestoration error measures the average relative error between the estimated support of a pattern and its real support. If this measure is small enough, it means that the estimated support of a pattern is quite close to its real support. A profile with small restoration error can provide very accurate support estimation.\nThe measure in the above definition is determined by the testing pattern set. We may choose the original patterns (which have been summarized into K master patterns) as the testing case. We can also assess the quality over the itemsets that are estimated to be frequent, i.e.,\nJc = 1 |T \u2032 | \u03b1 k \u2208T \u2032 |\u015d(\u03b1 k ) \u2212 s(\u03b1 k )| s(\u03b1 k ) ,(13)\nwhere T \u2032 is the collection of the itemsets generated by the master patterns in profiles and\u015d(\u03b1 k ) \u2265 \u03c3. The measure J tests \"frequent patterns\", some of which may be estimated as \"infrequent\", while Jc tests \"estimated frequent patterns\", some of which are actually \"infrequent\". Therefore, these two measures are complementary to each other. As long as Jc is relatively small, we can obtain the support of a generated itemset with high accuracy and determine whether it is frequent or not. The following lemma shows that if we summarize closed frequent itemsets using K profiles, the subsets generated by the master patterns in these K profiles will cover all of the frequent itemsets.\nLemma 2. Let {M1, . . . , MK } be a set of profiles learned over a collection of closed frequent itemsets {\u03b11, . . . , \u03b1m} using hierarchical clustering or K-means clustering. For any frequent itemset \u03c0, there must exist a profile M k such that \u03c0 \u2286 \u03c6 k , where \u03c6 k is the master itemset of M k .\nProof. For any frequent itemset \u03c0, there exists a closed frequent itemset \u03b1i such that \u03c0 \u2286 \u03b1i. \u03b1i must belong to one cluster, say M k . Hence, \u03b1i \u2286 \u03c6 k , where \u03c6 k is the master itemset of M k . Therefore, \u03c0 \u2286 \u03c6 k .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Optimal Number of Profiles", "text": "The summarization quality is related to the setting of K, i.e., the number of profiles. A smaller K is always preferred. Nevertheless, when the summarization is too coarse, it may not provide any valuable information. In order to determine the optimal number of profiles, we can apply a constraint on the summarization result. For example, for any profile M = (p, \u03c6, \u03c1), we require p(xi) \u2265 0.9 for any i such that oi \u2208 \u03c6. The optimal number is the smallest K that does not violate this constraint. In this section, we examine the summarization quality change to determine the optimal value of K.\nWhen two distribution vectors p and q calculated from patterns \u03b1 and \u03b2 are close to each other, transactions containing \u03b1 will likely contain \u03b2 too, and vice versa. Thus, D\u03b1 is similar to D \u03b2 and D\u03b1 \u222a D \u03b2 is similar to both D\u03b1 and D \u03b2 . Let r be the probability distribution vector over D\u03b1 \u222a D \u03b2 . When p and q are close, the mixture r will be close to them too. Therefore, the support estimation of any pattern according to Eq. (11) will not change much when we merge \u03b1 and \u03b2. It implies that the merge of two similar profiles will not significantly change the summarization quality.\nOn the other hand, if a clustering algorithm has to merge two profiles M\u03b1 and M \u03b2 that have a large KL-divergence, it may dramatically change the estimated support of a given pattern. Therefore, when we gradually decrease the value of K, we will observe the deterioration of the summarization quality. By checking the derivative of the quality over K, \u2202J \u2202K , we can find the optimal value of K practically: If J increases suddenly from K * to K * \u22121, K * is likely to be a good choice for the optimal number of profiles. Figure 3 shows the summarization quality along the number of profiles for a real dataset, Mushroom. The support threshold is set at 25%. We use hierarchical clustering to summarize this pattern set. The curve indicates that there are three optimal candidates to choose: 30, 53, and 76. A user can select one of them for examination based on their   ", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "EMPIRICAL STUDY", "text": "In this section, we provide the empirical evaluation for the effectiveness of our summarization algorithm. We use two kinds of datasets in our experiments: three real datasets and a series of synthetic datasets. The clustering algorithms are implemented in Visual C++. All of our experiments are performed on a 3.2GHZ, 1GB-memory, Intel PC running Windows XP.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Real Datasets", "text": "Mushroom. The first dataset, Mushroom, is available in the machine learning repository of UC Irvine. We obtained a variant from FIMI repository. This dataset consists of 8124 hypothetical mushroom samples with 119 distinct features. Each sample has 23 features. A support threshold of 25% was used to collect 688 closed frequent patterns (5545 2 frequent itemsets).\nFigure 5 shows the average restoration error over the closed frequent patterns (J) and the average restoration error over the frequent itemsets generated by the resulting profiles (Jc). The two restoration errors J and Jc are quite close. This indicates that we can use the K representative profiles to properly estimate the supports of the original closed pat- terns as well as the supports of the patterns not in the original set but derivable from the profiles. We also examined the standard deviation of the two restoration errors and found they are pretty close to J or Jc. From this aspect, our summarization method is stable and accurate in the restoration of the patterns and their supports.  Figure 6 shows the average restoration error (J) over hierarchical clustering and K-means clustering with or without applying the profile approximation heuristics. In KMeans-Apx and Hierarchical-Apx, we do not go back to the dataset to rebuild the cluster profilers. Instead, we directly interpolate the existing profiles as described in Section 3.3.2. Overall, the 688 closed patterns can be successfully summarized into 30 profiles with good quality -the average restoration error at 30 profiles is less than 0.1. In other words, the error of estimating the support for a pattern is less than 10% of that pattern's real support, and even as low as 5% when we summarize them into 53 profiles or over. BMS-Webview1. The second dataset, BMS-Webview1, is a web click-stream dataset from a web retailer company: Gazelle.com. The data was provided by Blue Martini Software [17]. In this experiment, we set the minimum support threshold at 0.1% and got 4,195 closed itemsets. BMS-Webview1 is completely different from the Mushroom dataset. It consists of many small frequent itemsets over a large set of items (itemsets of size 1-3 make up 84.55% of the total 4,195 patterns versus 14.10% for Mushroom), which makes the summarization more difficult.\nFigure 7 shows the average restoration error over hierarchical clustering and K-means clustering with or without  applying the profile approximation heuristics. As shown in the figure, when we use the profile approximation heuristics in K-means, the summarization quality is much worse than that of building the profiles from scratch. The restoration at K = 100 is 259% for K-means with the profile approximation while it is around 60% for hierarchical clustering or K-means without profile approximation.\nOverall, the summarization quality for BMS-Webview1 patterns is worse than that of Mushroom. When we use 1,200 profiles to summarize the patterns, the restoration error is 17%. The difference is due to the underlying distribution of patterns. There is much redundancy between patterns in Mushroom. By examining the 688 closed patterns of Mushroom, we can easily identify \"rough\" groups of patterns, where patterns in each group look very similar in composition and differ in support by only a very small number. Intuitively, these patterns can be summarized accurately. In BMS-Webview1, patterns are much shorter and sparser. Apparently, it is not good to put two itemsets into one cluster while they have very little overlap in composition. Such pattern distribution can be also explained from the data characteristics, i.e., the click-stream dataset usually contains random and short user access sessions, where the pattern explosive problem is not as serious as in dense datasets like Mushroom.  Replace. The third dataset, Replace, is a program trace dataset collected from the \"replace\" program, one of Siemens Programs, which are widely used in software engineering research [16]. We recorded the program call and transition information of 4,395 correct executions. Each type of program calls and transitions is taken as one item. Overall, there are 66 kinds of calls and transitions. The frequent patterns mined in this dataset may reveal the normal program execution structures, which can be compared with the abnormal executions for bug isolation.\nWe set the minimum support threshold at 3% in this experiment and obtained 4, 315 closed frequent execution structures. Figure 8 shows the average restoration error over hierarchical clustering and K-means clustering.\nAll methods except K-means clustering with profile approximation achieve good summarization quality. The quality of the three methods is quite close and the restoration error is about 6% when we use 200 clusters. The curves indicate that there are two optimal K values to choose: 40 and 80, since the error decreases significantly at these points compared with their neighboring points. For K-means with profile approximation, we further examine the summarization quality. Though the average restoration error does not decrease as we increase the number of profiles, the standard deviation of the error does lower -the standard deviation is 21.37% at K = 20 versus 7.39% at K = 200. It means that the summarization quality improves as we use more profiles.", "publication_ref": ["b16", "b15"], "figure_ref": ["fig_12", "fig_14", "fig_16", "fig_18"], "table_ref": []}, {"heading": "Synthetic Datasets", "text": "In this experiment, we want to study how the underlying distribution in patterns can affect the summarization quality. We used a series of synthetic datasets with different distributions to test it. The synthetic data generator is provided by IBM and is available at http://www.almaden.ibm. com/software/ quest/Resources/index.shtml. Users can specify parameters like the number of transactions, the number of distinct items, the average number of items in a transaction, etc., to generate various kinds of data.\nWe generated seven transaction datasets, where we vary the number of items to control the distribution of patterns in these datasets. Each dataset has 10,000 transactions, each of which has an average of 20 items. The number of distinct items in each dataset varies from 40, 60, up to 160. Since it may not be fair to compare the result using a fixed support threshold in these datasets, we intentionally obtained the top-500 frequent closed patterns from each dataset and summarize them into 50 and 100 profiles using hierarchical clustering. Figure 9 shows the average restoration error J.  As shown in Figure 9, the summarization quality deteriorates as the number of distinct items in the datasets in-creases. When the number of items is small, the dataset has a dense distribution. There exists much redundancy between patterns. So, they can be summarized with small restoration errors; while for a dataset with a large number of items, the patterns are sparsely distributed. Thus, it is harder to summarize them with reasonably good quality. This experiment shows that the summarization quality has close relation with the patterns themselves. This result is also observed in the previous real datasets. Dense data with high redundancy can be summarized with good quality while sparse patterns with little overlap cannot be grouped together very well.  We also tested the running time of our pattern summarization methods over six synthetic datasets by varying the number of transactions from 1,000, 10,000 up to 50,000. A set of about 1,100 closed patterns is obtained from each dataset using a minimum support of 10%. We tested hierarchical clustering and K-means clustering with or without applying the profile approximation heuristics over these datasets. Figure 10 shows the running time. The running time of Hierarchical-Apx and KMeans-Apx does not change with the transaction number because we simply interpolate the profiles as described in Section 3.3.2. The running time of hierarchical clustering and K-means clustering without using profile approximation heuristics increases linearly with the number of transactions. This figure shows that profile approximation can really improve the efficiency.", "publication_ref": [], "figure_ref": ["fig_20", "fig_20", "fig_6"], "table_ref": []}, {"heading": "RELATED WORK", "text": "Lossless methods have been proposed to reduce the output size of frequent itemset patterns. Pasquier et al. [20] developed the concept of closed frequent patterns, and Calders et al. [6] proposed mining non-derivable frequent itemsets. These kinds of patterns are concise in the sense that all of the frequent patterns can be derived from these representations. Lossy compression methods were also developed in parallel: maximal patterns by Gunopulos [11], error-tolerant patterns by Yang et al. [28] and Pei et al. [23], and Top-k patterns by Han et al. [12]. These methods can reduce the pattern set size further. For example, in maximal pattern mining, all of the frequent subpatterns are removed so that the resulting pattern set is very compact. Besides lossless and lossy methods, other concepts like support envelopes [24] were also proposed to explore association patterns.\nOur pattern approximation model is also related to the probabilistic models developed by Pavlov et al. [21] for query approximations, where frequent patterns and their supports are used to estimate query selectivity. Mielik\u00e4inen and Mannila [18] proposed an approximation solution based on ordering patterns. The closest work to our study is a novel pattern approximation approach proposed by Afrati et al. [1], which uses k frequent (or border) itemsets to cover a collection of frequent itemsets. Their result can be regarded as a generalization of maximal frequent itemsets. In [1] Afrati et al. mentioned the support integration issue: It is unknown how to integrate the support information with the approximation. In this paper, we solved this problem, thus advancing the summarization concept. Interestingly, the K representatives mined by our approach can be regarded as a generalization of closed frequent itemsets.", "publication_ref": ["b19", "b5", "b10", "b27", "b22", "b11", "b23", "b20", "b17", "b0", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSIONS", "text": "We have examined how to summarize a collection of itemset patterns using only K representatives. The summarization will solve the interpretability issue caused by the huge number of frequent patterns. Surprisingly, our profile model is able to recover frequent patterns as well as their supports, thus answering the support integration issue raised by Afrati et al. [1]. We also solved the problem of determining the optimal value of K by monitoring the change of the support restoration error. Empirical studies indicate that we can obtain very compact summarization in real datasets. Our approach belongs to a post-mining process; we are working on algorithms that can directly apply our profiling model to the mining process.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Approximating a collection of frequent sets", "journal": "", "year": "2004", "authors": "F Afrati; A Gionis; H Mannila"}, {"ref_id": "b1", "title": "Mining association rules between sets of items in large databases", "journal": "", "year": "1993", "authors": "R Agrawal; T Imielinski; A Swami"}, {"ref_id": "b2", "title": "Mining sequential patterns", "journal": "", "year": "1995", "authors": "R Agrawal; R Srikant"}, {"ref_id": "b3", "title": "Distributional clustering of words for text classification", "journal": "", "year": "1998", "authors": "L Baker; A Mccallum"}, {"ref_id": "b4", "title": "Efficiently mining long patterns from databases", "journal": "", "year": "1998", "authors": "R Bayardo"}, {"ref_id": "b5", "title": "Mining all non-derivable frequent itemsets", "journal": "", "year": "2002", "authors": "T Calders; B Goethals"}, {"ref_id": "b6", "title": "Finding frequent substructures in chemical compounds", "journal": "", "year": "1998", "authors": "L Dehaspe; H Toivonen; R King"}, {"ref_id": "b7", "title": "Frequent sub-structure-based approaches for classifying chemical compounds", "journal": "", "year": "2003", "authors": "M Deshpande; M Kuramochi; G Karypis"}, {"ref_id": "b8", "title": "A divisive information-theoretic feature clustering algorithm for text classification", "journal": "J. of Machine Learning Research", "year": "2003", "authors": "I Dhillon; S Mallela; R Kumar"}, {"ref_id": "b9", "title": "Biological Sequence Analysis : Probabilistic Models of Proteins and Nucleic Acids", "journal": "Cambridge University Press", "year": "1999", "authors": "R Durbin; S Eddy; A Krogh; G Mitchison"}, {"ref_id": "b10", "title": "Data mining, hypergraph transversals, and machine learning", "journal": "", "year": "1997", "authors": "D Gunopulos; H Mannila; R Khardon; H Toivonen"}, {"ref_id": "b11", "title": "Mining top-k frequent closed patterns without minimum support", "journal": "", "year": "2002", "authors": "J Han; J Wang; Y Lu; P Tzvetkov"}, {"ref_id": "b12", "title": "Probability inequalities for sums of bounded random variables", "journal": "J. American Statistical Associations", "year": "1963", "authors": "W Hoeffding"}, {"ref_id": "b13", "title": "Substructure discovery in the subdue system", "journal": "", "year": "1994", "authors": "L Holder; D Cook; S Djoko"}, {"ref_id": "b14", "title": "Mining spatial motifs from protein structure graphs", "journal": "", "year": "2004", "authors": "J Huan; W Wang; D Bandyopadhyay; J Snoeyink; J Prins; A Tropsha"}, {"ref_id": "b15", "title": "Experiments of the effectiveness of dataflow-and controlflow-based test adequacy criteria", "journal": "", "year": "1994", "authors": "M Hutchins; H Foster; T Goradia; T Ostrand"}, {"ref_id": "b16", "title": "Cup 2000 organizers' report: Peeling the onion. SIGKDD Explorations", "journal": "", "year": "2000", "authors": "R Kohavi; C Brodley; B Frasca; L Mason; Z Zheng"}, {"ref_id": "b17", "title": "The pattern ordering problem", "journal": "", "year": "2003", "authors": "T Mielik\u00e4inen; H Mannila"}, {"ref_id": "b18", "title": "Alternative interest measures for mining associations", "journal": "IEEE Trans. Knowledge and Data Engineering", "year": "2003", "authors": "E Omiecinski"}, {"ref_id": "b19", "title": "Discovering frequent closed itemsets for association rules", "journal": "", "year": "1999", "authors": "N Pasquier; Y Bastide; R Taouil; L Lakhal"}, {"ref_id": "b20", "title": "Beyond independence: Probabilistic models for query approximation on binary transaction data", "journal": "IEEE Trans. Knowledge and Data Engineering", "year": "2003", "authors": "D Pavlov; H Mannila; P Smyth"}, {"ref_id": "b21", "title": "On computing condensed frequent pattern bases", "journal": "", "year": "2002", "authors": "J Pei; G Dong; W Zou; J Han"}, {"ref_id": "b22", "title": "Fault-tolerant frequent pattern mining: Problems and challenges", "journal": "", "year": "2001", "authors": "J Pei; A Tung; J Han"}, {"ref_id": "b23", "title": "Support envelopes: a technique for exploring the structure of association patterns", "journal": "", "year": "2004", "authors": "M Steinbach; P Tan; V Kumar"}, {"ref_id": "b24", "title": "Selecting the right interestingness measure for association patterns", "journal": "", "year": "2002", "authors": "P Tan; V Kumar; J Srivastava"}, {"ref_id": "b25", "title": "Clustering transactions using large items", "journal": "", "year": "1999", "authors": "K Wang; C Xu; B Liu"}, {"ref_id": "b26", "title": "Graph indexing: A frequent structure-based approach", "journal": "", "year": "2004", "authors": "X Yan; P Yu; J Han"}, {"ref_id": "b27", "title": "Efficient discovery of error-tolerant frequent itemsets in high dimensions", "journal": "", "year": "2001", "authors": "C Yang; U Fayyad; P S Bradley"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "where |D\u03b1| |D| is called the support of \u03b1 in D, written s(\u03b1), and \u03c3 is the minimum support threshold, 0 \u2264 \u03c3 \u2264 1.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Definition 2 (2Closed Frequent Itemset). A frequent itemset \u03b1 is closed if there does not exist an itemset \u03b2 such that \u03b1 \u2286 \u03b2 and D\u03b1 = D \u03b2 .", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 1 Figure 1 :11Figure 1 shows a sample dataset, where the first column represents the transactions and the second the number of transactions. For example, 50 transactions have only items a, c, and d ; and 100 transactions have only items b, c, and d.There are 1, 150 transactions in D1. If we set the minimum support at 40%, itemset abcd is frequent, and so are its sub-itemsets. There are 15 frequent itemsets, among which 4 are closed. As one can see, the number of closed frequent itemsets is much less than that of frequent itemsets.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 2 :2Figure 2: D2", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Definition 3 (3Bernoulli Distribution Vector). Let I = {o1, . . . , o d } be a set of items, and xi be a boolean random variable indicating the selection of oi. p(x) = [p(x1), . . . , p(x d )] is a Bernoulli distribution vector over d dimensions, where x1, . . . , and x d are independent boolean random variables.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Definition 4 (4Pattern Profile). Let \u03b11, \u03b12, . . . , \u03b1 l be a set of patterns and D \u2032 = i D\u03b1 i . A profile M over \u03b11, \u03b12, . . ., and \u03b1 l is a triple p, \u03c6, \u03c1 . p is a probability distribution vector learned through Eq. (3). Pattern \u03c6 = \u222ai\u03b1i is taken as the master pattern of \u03b11, . . . , \u03b1 l . \u03c1 = |D \u2032 | |D| is regarded as the support of the profile, also written as s(M).", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Algorithm 11Pattern Summarization: Hierarchical Clustering Input: Transaction dataset D, Pattern set F = {\u03b11, . . . , \u03b1m}, Number of representatives K, Output: A set of pattern profiles MC 1 , . . . , MC K . 1: initialize k = m clusters, each of which has one pattern; 2: compute the pairwise KL divergence among C1, . . . , C k , dij = KL(MC i ||MC j ); 3: while (k > K) 4:select dst such that s, t = argmin i,j dij;5: merge clusters Cs and Ct to a new cluster C; 6: DC = DC s \u222a DC t ; 7: IC = IC s \u222a IC t ; 8: update the profile of C over DC by Eq. (3); 9:", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Algorithm 22Pattern Summarization: K-means Input: Transaction dataset D, Pattern set F = {\u03b11, . . . , \u03b1m}, Number of representatives K, Output: A set of pattern profiles MC 1 , . . . , MC k .", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Lemma 1 .1Given patterns \u03b1 and \u03b2, if \u03b1 \u2286 \u03b2 and their supports are equal, then KL(M \u03b2 ||M\u03b1)=KL(M\u03b1||M \u03b2 ) = 0.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 3: Mushroom 25%: J", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 4 :4Figure 4: Mushroom 25%: \u25b3J", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 44shows the quality change along the number of profiles. The derivative of J clearly indicates three huge quality changes.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Figure 5 :5Figure 5: Mushroom: Hierarchical Clustering", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "Figure 6 :6Figure 6: Mushroom", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_16", "figure_caption": "Figure 7 :7Figure 7: BMS-Webview1", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_18", "figure_caption": "Figure 8 :8Figure 8: Replace", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_20", "figure_caption": "Figure 9 :9Figure 9: Synthetic Data: Hierarchical Clustering", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_22", "figure_caption": "Figure 10 :10Figure 10: Synthetic Data: Efficiency", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Pattern Profile", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "Definition 1 (Frequent Itemset). For a transaction dataset D, an itemset \u03b1 is frequent if |D\u03b1| |D| \u2265 \u03c3,", "formula_coordinates": [2.0, 316.81, 372.67, 242.94, 23.29]}, {"formula_id": "formula_1", "formula_text": "D \u2032 = l i=1 D\u03b1 i . P (D \u2032 |\u03b8) = t j \u2208D \u2032 d i=1 p(xi = t i j ),(1)", "formula_coordinates": [3.0, 324.49, 65.93, 231.43, 57.11]}, {"formula_id": "formula_2", "formula_text": "\u2202L(\u03b8|D \u2032 ) \u2202\u03b8 = 0 (2)", "formula_coordinates": [3.0, 409.92, 198.42, 146.0, 22.12]}, {"formula_id": "formula_3", "formula_text": "p(xi = 1) = t j \u2208D \u2032 t i j |D \u2032 | .(3)", "formula_coordinates": [3.0, 389.51, 236.73, 166.41, 24.87]}, {"formula_id": "formula_4", "formula_text": "p(\u03b1) = t\u2208D p(\u03b1|t) * p(t),", "formula_coordinates": [3.0, 382.2, 702.54, 108.32, 18.46]}, {"formula_id": "formula_5", "formula_text": "p(\u03b1|t) \u223c p(\u03b1|M) * p(M|t),(4)", "formula_coordinates": [4.0, 111.78, 117.02, 181.13, 8.97]}, {"formula_id": "formula_6", "formula_text": "s(\u03b1 k ) = s(M) \u00d7 o i \u2208\u03b1 k p(xi = 1),(5)", "formula_coordinates": [4.0, 108.78, 214.99, 184.12, 19.5]}, {"formula_id": "formula_7", "formula_text": "s(M) = |D\u03b1 1 \u222a...\u222aD\u03b1 l | |D|", "formula_coordinates": [4.0, 81.13, 241.85, 89.95, 14.56]}, {"formula_id": "formula_8", "formula_text": "KL(p||q) = d i=1 x i \u2208{0,1} p(xi) log p(xi) q(xi) , (6", "formula_coordinates": [4.0, 95.76, 692.48, 193.22, 28.6]}, {"formula_id": "formula_9", "formula_text": ")", "formula_coordinates": [4.0, 288.98, 701.17, 3.93, 8.97]}, {"formula_id": "formula_10", "formula_text": "p \u2032 (xi) = \u03bbu + (1 \u2212 \u03bb)p(xi),", "formula_coordinates": [4.0, 382.24, 115.85, 108.24, 11.04]}, {"formula_id": "formula_11", "formula_text": "KL(p(xi)||q(xi)) = \u03b8log \u03b8 \u03b7 + (1 \u2212 \u03b8)log 1 \u2212 \u03b8 1 \u2212 \u03b7 = \u03b8 \u2212 \u03b7 + \u03b8o( \u03b8 \u03b7 \u2212 1) + \u03b7 \u2212 \u03b8 + (1 \u2212 \u03b8)o( 1 \u2212 \u03b8 1 \u2212 \u03b7 \u2212 1) = \u03b8o( \u03b8 \u2212 \u03b7 \u03b7 ) + (1 \u2212 \u03b8)o( \u03b7 \u2212 \u03b8 1 \u2212 \u03b7 ),", "formula_coordinates": [4.0, 316.81, 196.33, 226.8, 68.52]}, {"formula_id": "formula_12", "formula_text": "KL(p(xi)||q(xi)) < \u01eb \u21d4 p(xi) \u223c q(xi).", "formula_coordinates": [4.0, 360.98, 303.42, 150.76, 8.97]}, {"formula_id": "formula_13", "formula_text": "P (D \u03b2 |M\u03b1) = t j \u2208D \u03b2 d i=1 p(xi = t i j ),", "formula_coordinates": [4.0, 368.08, 527.99, 136.56, 28.35]}, {"formula_id": "formula_14", "formula_text": "L(M\u03b1) n = 1 n t j \u2208D \u03b2 d i=1 log p(xi = t i j ) = d i=1 ni n log p(xi = 1) + n \u2212 ni n log p(xi = 0) = d i=1 x i \u2208{0,1} q(xi) log p(xi),(7)", "formula_coordinates": [4.0, 325.05, 606.26, 230.87, 94.55]}, {"formula_id": "formula_15", "formula_text": "H(q(x)) = \u2212 d i=1 x i \u2208{0,1} q(xi) log q(xi),", "formula_coordinates": [5.0, 94.15, 74.62, 156.86, 28.6]}, {"formula_id": "formula_16", "formula_text": "L(M\u03b1) n + H(q(x)) = d i=1 q(xi = 1) log p(xi = 1) q(xi = 1) + q(xi = 0) log p(xi = 0) q(xi = 0) = \u2212KL(q(x)||p(x))(8)", "formula_coordinates": [5.0, 55.13, 127.76, 237.78, 63.29]}, {"formula_id": "formula_17", "formula_text": "2 logm + m 2 d + mnd).", "formula_coordinates": [5.0, 468.97, 286.11, 86.94, 10.02]}, {"formula_id": "formula_18", "formula_text": "p(x) = |Cs| |Cs| + |Ct| ps(x) + |Ct| |Cs| + |Ct| pt(x),(9)", "formula_coordinates": [6.0, 84.24, 654.11, 208.66, 20.86]}, {"formula_id": "formula_19", "formula_text": "p(x) = 1 |C| \u03b1\u2208C p\u03b1(x). (10", "formula_coordinates": [6.0, 127.97, 696.74, 160.84, 24.25]}, {"formula_id": "formula_20", "formula_text": ")", "formula_coordinates": [6.0, 288.81, 702.54, 4.1, 8.97]}, {"formula_id": "formula_21", "formula_text": "s(\u03b1 k ) = max M s(M) \u00d7 o i \u2208\u03b1 k p M (xi = 1). (11", "formula_coordinates": [6.0, 353.73, 397.97, 198.1, 19.51]}, {"formula_id": "formula_22", "formula_text": ")", "formula_coordinates": [6.0, 551.82, 397.97, 4.1, 8.97]}, {"formula_id": "formula_23", "formula_text": "J = 1 |T | \u03b1 k \u2208T |s(\u03b1 k ) \u2212\u015d(\u03b1 k )| s(\u03b1 k ) . (12", "formula_coordinates": [6.0, 376.29, 544.74, 175.53, 25.47]}, {"formula_id": "formula_24", "formula_text": ")", "formula_coordinates": [6.0, 551.82, 550.55, 4.1, 8.97]}, {"formula_id": "formula_25", "formula_text": "Jc = 1 |T \u2032 | \u03b1 k \u2208T \u2032 |\u015d(\u03b1 k ) \u2212 s(\u03b1 k )| s(\u03b1 k ) ,(13)", "formula_coordinates": [6.0, 372.1, 695.46, 183.82, 25.87]}], "doi": ""}