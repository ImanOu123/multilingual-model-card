{"title": "Discriminative Learning of Sum-Product Networks", "authors": "Robert Gens; Pedro Domingos", "pub_date": "", "abstract": "Sum-product networks are a new deep architecture that can perform fast, exact inference on high-treewidth models. Only generative methods for training SPNs have been proposed to date. In this paper, we present the first discriminative training algorithms for SPNs, combining the high accuracy of the former with the representational power and tractability of the latter. We show that the class of tractable discriminative SPNs is broader than the class of tractable generative ones, and propose an efficient backpropagation-style algorithm for computing the gradient of the conditional log likelihood. Standard gradient descent suffers from the diffusion problem, but networks with many layers can be learned reliably using \"hard\" gradient descent, where marginal inference is replaced by MPE inference (i.e., inferring the most probable state of the non-evidence variables). The resulting updates have a simple and intuitive form. We test discriminative SPNs on standard image classification tasks. We obtain the best results to date on the CIFAR-10 dataset, using fewer features than prior methods with an SPN architecture that learns local image structure discriminatively. We also report the highest published test accuracy on STL-10 even though we only use the labeled portion of the dataset.", "sections": [{"heading": "Introduction", "text": "Probabilistic models play a crucial role in many scientific disciplines and real world applications. Graphical models compactly represent the joint distribution of a set of variables as a product of factors normalized by the partition function. Unfortunately, inference in graphical models is generally intractable. Low treewidth ensures tractability, but is a very restrictive condition, particularly since the highest practical treewidth is usually 2 or 3 [2,9]. Sum-product networks (SPNs) [23] overcome this by exploiting context-specific independence [7] and determinism [8]. They can be viewed as a new type of deep architecture, where sum layers alternate with product layers. Deep networks have many layers of hidden variables, which greatly increases their representational power, but inference with even a single layer is generally intractable, and adding layers compounds the problem [3]. SPNs are a deep architecture with full probabilistic semantics where inference is guaranteed to be tractable, under general conditions derived by Poon and Domingos [23]. Despite their tractability, SPNs are quite expressive [16], and have been used to solve difficult problems in vision [23,1].\nPoon and Domingos introduced an algorithm for generatively training SPNs, yet it is generally observed that discriminative training fares better. By optimizing P (Y|X) instead of P (X, Y) conditional random fields retain joint inference over dependent label variables Y while allowing for flexible features over given inputs X [22]. Unfortunately, the conditional partition function Z(X) is just as prone to intractability as with generative training. For this reason, low treewidth models (e.g. chains and trees) of Y are commonly used. Research suggests that approximate inference can make it harder to learn rich structured models [21]. In this paper, discriminatively training SPNs will allow us to combine flexible features with fast, exact inference over high treewidth models.\nWith inference and learning that easily scales to many layers, SPNs can be viewed as a type of deep network. Existing deep networks employ discriminative training with backpropagation through softmax layers or support vector machines over network variables. Most networks that are not purely feed-forward require approximate inference. Poon and Domingos showed that deep SPNs could be learned faster and more accurately than deep belief networks and deep Boltzmann machines on a generative image completion task [23]. This paper contributes a discriminative training algorithm that could be used on its own or with generative pre-training.\nFor the first time we combine the advantages of SPNs with those of discriminative models. In this paper we will review SPNs and describe the conditions under which an SPN can represent the conditional partition function. We then provide a training algorithm, demonstrate how to compute the gradient of the conditional log-likelihood of an SPN using backpropagation, and explore variations of inference. Finally, we show state-of-the-art results where a discriminatively-trained SPN achieves higher accuracy than SVMs and deep models on image classification tasks.", "publication_ref": ["b1", "b8", "b22", "b6", "b7", "b2", "b22", "b15", "b22", "b0", "b21", "b20", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Sum-Product Networks", "text": "SPNs were introduced with the aim of identifying the most expressive tractable representation possible. The foundation for their work lies in Darwiche's network polynomial [14]. We define an unnormalized probability distribution \u03a6(x) \u2265 0 over a vector of Boolean variables X. The indicator function [.] is one when its argument is true and zero otherwise; we abbreviate [X i ] and [X i ] as x i and x i . To distinguish random variables from indicator variables, we use roman font for the former and italic for the latter. Vectors of variables are denoted by bold roman and bold italic font, respectively. The network polynomial of \u03a6(x) is defined as x \u03a6(x) (x), where (x) is the product of indicators that are one in state x. For example, the network polynomial of the Bayesian network\nX 1 \u2192 X 2 is P (x 1 )P (x 2 |x 1 )x 1 x 2 + P (x 1 )P (x 2 |x 1 )x 1x2 + P (x 1 )P (x 2 |x 1 )x 1 x 2 + P (x 1 )P (x 2 |x 1 )x 1x2 .\nTo compute P (X 1 = true, X 2 = false), we access the corresponding term of the network polynomial by setting indicators x 1 andx 2 to one and the rest to zero. To find P (X 2 = true), we fix evidence on X 2 by setting x 2 to one andx 2 to zero and marginalize X 1 by setting both x 1 andx 1 to one. Notice that there are two reasons we might set an indicator x i = 1: (1) evidence {X i = true}, in which case we setx i = 0 and (2) marginalization of X i , wherex i = 1 as well. In general the role of an indicator x i is to determine whether terms compatible with variable state X i = true are included in the summation, and similarly forx i . With this notation, the partition function Z can be computed by setting all indicators of all variables to one.\nThe network polynomial has size exponential in the number of variables, but in many cases it can be represented more compactly using a sum-product network [23,14]. Definition 1. (Poon & Domingos, 2011) A sum-product network (SPN) over variables X 1 , . . . , X d is a rooted directed acyclic graph whose leaves are the indicators x 1 , . . . , x d andx 1 , . . . ,x d and whose internal nodes are sums and products. Each edge (i, j) emanating from a sum node i has a non-negative weight w ij . The value of a product node is the product of the values of its children. The value of a sum node is j\u2208Ch(i) w ij v j , where Ch(i) are the children of i and v j is the value of node j.  If we could replace the exponential sum over variable states in the partition function with the linear evaluation of the network, inference would be tractable. For example, the SPN in Figure 1 represents the joint probability of three Boolean variables P\n(X 1 , X 2 , X 3 ) in the Bayesian network X 2 \u2190 X 1 \u2192 X 3 using six indicators S[x 1 ,x 1 , x 2 ,x 2 , x 3 ,x 3 ].\nTo compute P (X 1 = true), we could sum over the joint states of X 2 and X 3 , evaluating the network a total of four times S[1, 0, 0, 1, 0, 1]+. . .+ S[1, 0, 1, 0, 1, 0]. Instead, we set the indicators so that the network sums out both X 2 and X 3 . An indicator setting of S[1,0,1,1,1,1] computes the sum over all states compatible with our evidence e = {X1 = true} and requires only one evaluation.\nHowever, not every SPN will have this property. If a linear evaluation of an SPN with indicators set to represent evidence equals the exponential sum over all variable states consistent with that evidence, the SPN is valid. The scope of a node is defined as the set of variables that have indicators among the node's descendants. To \"appear in a child\" means to be among that child's descendants. If a sum node is incomplete, the SPN will undercount the true marginals. Since an incomplete sum node has scope larger than a child, that child will be non-zero for more than one state of the sum (e.g. if\nS[x 1 ,x 1 , x 2 ,x 2 ] = (x 1 + x 2 ), S[1, 0, 1, 1] < S[1, 0, 1, 0] + S[1, 0, 0, 1]).\nIf a product node is inconsistent, the SPN will overcount the marginals as it will incorporate impossible states (e.g. x 1 \u00d7x 1 ) into its computation.\nPoon and Domingos show how to generatively train the parameters of an SPN. One method is to compute the likelihood gradient and optimize with gradient descent (GD). They also show how to use expectation maximization (EM) by considering each sum node as the marginalization of a hidden variable [17]. They found that online EM using most probable explanation (MPE or \"hard\") inference worked the best for their image completion task.\nGradient diffusion is a key issue in training deep models. It is commonly observed in neural networks that when the gradient is propagated to lower layers it becomes less informative [3]. When every node in the network takes fractional responsibility for the errors of a top level node, it becomes difficult to steer parameters out of local minima. Poon and Domingos also saw this effect when using gradient descent and EM to train SPNs. They found that online hard EM could provide a sparse but strong learning signal to synchronize the efforts of upper and lower nodes. Note that hard training is not exclusive to EM. In the next section we show how to discriminatively train SPNs with hard gradient descent.", "publication_ref": ["b13", "b22", "b13", "b16", "b2"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Discriminative Learning of SPNs", "text": "We define an SPN S[y, h|x] that takes as input three disjoint sets of variables H, Y, and X (hidden, query, and given). We denote the setting of all h indicator functions to 1 as S[y, 1|x], where the bold 1 is a vector. We do not sum over states of given variables X when discriminatively training SPNs. Given an instance, we treat X as constants. This means that one ignores X variables in the scope of a node when considering completeness and consistency. Since adding a constant as a child to a product node cannot make that product inconsistent, a variable x can be the child of any product node in a valid SPN. To maintain completeness, x can only be the child of a sum node that has scope outside of Y or H. The parameters of an SPN can be learned using an online procedure as in Algorithm 1 as proposed by Poon and Domingos. The three dimensions of the algorithm are generative vs. discriminative, the inference procedure, and the weight update. Poon and Domingos discussed generative gradient descent with marginal inference as well as EM with marginal and MPE inference. In this section we will derive discriminative gradient descent with marginal and MPE inference, where hard gradient descent can also be used for generative training. EM is not typically used for discriminative training as it requires modification to lower bound the conditional likelihood [25] and there may not be a closed form for the M-step.", "publication_ref": ["b24"], "figure_ref": [], "table_ref": []}, {"heading": "Discriminative Training with Marginal Inference", "text": "A component of the gradient of the conditional log likelihood takes the form\n\u2202 \u2202w log P (y|x) = \u2202 \u2202w log h \u03a6(Y = y, H = h|x) \u2212 \u2202 \u2202w log y ,h \u03a6(Y = y , H = h|x) = 1 S[y, 1|x] \u2202S[y, 1|x] \u2202w \u2212 1 S[1, 1|x] \u2202S[1, 1|x] \u2202w\nwhere the two summations are separate bottom-up evaluations of the SPN with indicators set as S[y, 1|x] and S[1, 1|x], respectively.\nThe partial derivatives of the SPN with respect to all weights can be computed with backpropagation, detailed in Algorithm 2. After performing a bottom-up evaluation of the SPN, partial derivatives are passed from parent to child as follows from the chain rule and described in [15]. The form of backpropagation presented takes time linear in the number of nodes in the SPN if product nodes have a bounded number of children.\nOur gradient descent update then follows the direction of the partial derivative of the conditional log likelihood with learning rate \u03b7: \u2206w = \u03b7 \u2202 \u2202w log P (y|x). After each gradient step we optionally renormalize the weights of a sum node so they sum to one. Empirically we have found this to produce the best results. The second SPN evaluation that marginalizes H and Y can reuse computation from the first, for example, when Y is modeled by a root sum node. In this case the values of all non-root nodes are equivalent between the two evaluations. For any architecture, one can memoize values of nodes that do not have a query variable indicator as a descendant. \n+ + + f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f\nFigure 2: Positive and negative terms in the hard gradient. The root node sums out the variable Y, the two sum nodes on the left sum out the hidden variable H 1 , the two sum nodes on the right sum out H 2 , and a circled 'f' denotes an input variable X i . Dashed lines indicate negative elements in the gradient.\nWe define a max-product network (MPN) M [y, h|x] based on the max-product semiring. This network compactly represents the maximizer polynomial max x \u03a6(x) (x), which computes the MPE [15]. To convert an SPN to an MPN, we replace each sum node by a max node, where weights on children are retained. The gradient of the conditional log likelihood with MPE inference is then\n\u2202 \u2202w logP (y|x) = \u2202 \u2202w log max h \u03a6(Y = y, H = h|x) \u2212 \u2202 \u2202w log max y ,h \u03a6(Y = y , H = h|x)\nwhere the two maximizations are computed by M [y, 1|x] and M [1, 1|x]. MPE inference also consists of a bottom-up evaluation followed by a top-down pass. Inference yields a branching path through the SPN called a complete subcircuit that includes an indicator (and therefore assignment) for every variable [15]. Analogous to Viterbi decoding, the path starts at the root node and at each max (formerly sum) node it only travels to the max-valued child. At product nodes, the path branches to all children. We define W as the multiset of weights traversed by this path 1 . The value of the MPN takes the form of a product wi\u2208W w ci i , where c i is the number of times w i appears in W . The partial derivatives of the MPN with respect to all nodes and weights is computed by Algorithm 2 modified to accommodate MPNs: (1) S becomes M , (2) when n is a sum node, the body of the forall loop is run once for j as the max-valued child.\nThe partial derivative of the logarithm of an MPN with respect to a weight takes the form\n\u2202 log M \u2202wi = \u2202 log M \u2202M \u2202M \u2202wi = 1 M \u2202M \u2202wi = ci \u2022 w c i \u22121 i w j \u2208W \\{w i } w c j j w j \u2208W w c j j = ci wi\nThe gradient of the conditional log likelihood with MPE inference is therefore \u2206c i /w i , where \u2206c i = c i \u2212 c i is the difference between the number of times w i is traversed by the two MPE inference paths in M [y, 1|x] and M [1, 1|x], respectively. The hard gradient update is then \u2206w i = \u03b7 \u2202 \u2202wi logP (y|x) = \u03b7 \u2206ci wi . The hard gradient for a training instance (x d , y d ) is illustrated in Figure 2. In the first two expressions, the complete subcircuit traveled by each MPE inference is shown in bold. Product nodes do not have weighted children, so they do not appear in the gradient, depicted in the last expression We can also easily add regularization to SPN training. An L2 weight penalty takes the familiar form of \u2212\u03bb||w|| 2 and partial derivatives \u22122\u03bbw i can be added to the gradient. With an appropriate optimization method, an L1 penalty could also be used for learning with marginal inference on dense SPN architectures. However, sparsity is not as important for SPNs as it is for Markov random fields, where a non-zero weight can have outsize impact on inference time; with SPNs inference is always linear with respect to model size.\nA summary of the variations of Algorithm 1 is provided in Tables 1 and 2. The generative hard gradient can be used in place of online EM for datasets where it would be prohibitive to store inference results from past epoch. For architectures that have high fan-in sum nodes, soft inference may be able to separate groups of modes faster than hard inference, which can only alter one child of a sum node at a time.\nWe observe the similarity between the updates of hard EM and hard gradient descent. In particular, if we reparameterize the SPN so that each child of a sum node is weighted by w i = e w i , the form of  \nS l \u2202M \u2202Mn = k\u2208P a(n) \u2202M \u2202M k l\u2208Ch(k)\\{n} M l Product \u2202S \u2202Sn = k\u2208P a(n) w kn \u2202S \u2202S k \u2202M \u2202Mn = k\u2208P a(n) w kn \u2202M \u2202M k : w kn \u2208 W 0 : otherwise Weight \u2202S \u2202w ki = \u2202S \u2202S k S i \u2202M \u2202w ki = \u2202M \u2202M k M i\n\u2206w i = \u03b7 ci wi Gen. EM P (H k = i|x, y) \u221d w ki \u2202S[x,y] \u2202S k P (H k = i|x, y) = 1 : w ki \u2208 W 0 : otherwise Disc. GD \u2206w = \u03b7 1 S[y,1|x] \u2202S[y,1|x] \u2202w \u2212 \u2206w i = \u03b7 \u2206ci wi 1 S[1,1|x]\n\u2202S [1,1|x] \u2202w the partial derivative of the log MPN becomes\n\u2202 log M \u2202w i = 1 M \u2202M \u2202w i = ci w j \u2208W e c j \u2022w j w j \u2208W e c j \u2022w j = ci\nThis means that the hard gradient update for weights in logspace is \u2206w i = \u2206c i , which resembles structured perceptron [13].", "publication_ref": ["b14", "b14", "b14", "b0", "b12"], "figure_ref": ["fig_2", "fig_2"], "table_ref": ["tab_0"]}, {"heading": "Experiments", "text": "We have applied discriminative training of SPNs to image classification benchmarks. CIFAR-10 and STL-10 are standard datasets for deep networks and unsupervised feature learning. Both are 10-class small image datasets. We achieve the best results to date on both tasks.\nWe follow the feature extraction pipeline of Coates et al. [10], which was also used recently to learn pooling functions [20]. The procedure consists of extracting 4 \u00d7 10 5 6x6 pixel patches from the training set images, ZCA whitening those patches [19], running k-means for 50 rounds, and then normalizing the dictionary to have zero mean and unit variance. We then use the dictionary to extract K features at every 6x6 pixel site in the image (unit stride) with the \"triangle\" encoding f k (x) = max{0,z \u2212 z k }, where z k = ||x \u2212 c k || 2 , c k is the k-th item in the dictionary, andz is the average z k . For each image of CIFAR-10, for example, this yields a 27 \u00d7 27 \u00d7 K feature vector that is finally downsampled by max-pooling to a G \u00d7 G \u00d7 K feature vector.  We experiment with a simple architecture that allows for discriminative learning of local structure. This architecture cannot be generatively trained as it violates consistency over X. Inspired by the successful star models in Felzenszwalb et al. [18], we construct a network with C classes, P parts per class, and T mixture components per part. A part is a pattern of image patch features that can occur anywhere in the image (e.g. an arrangement of patches that defines a curve). Each part filter f cpt is of dimension W \u00d7 W \u00d7 K and is initialized to 0. The root of the SPN is a sum node with a child S c for each class c in the dataset multiplied by the indicator for that state of the label variable Y. S c is a product over P nodes S cp , where each S cp is a sum node over T nodes S cpt . The hidden variables H represent the choice of cluster in the mixture over a part and its position (S cp and S cpt , respectively). Finally, S cpt sums over positions i, j in the image of the logistic function e xij \u2022 fcpt where the given variable x ij is the same dimension as f and parts can overlap.", "publication_ref": ["b9", "b19", "b18", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "GxGxK", "text": "Notice that the mixture S cp models an additional level of spatial structure on top of the image patch features learned by k-means. Coates and Ng [12] also learn higher-order structure, but whereas our method learns structure discriminatively in the context of a parts-based model, their unsupervised algorithm greedily groups features based on correlation and is unable to learn mixtures. Compared with the pooling functions in Jia et al. [20] that model independent translation of patch features, our architecture models how nearby features move together. Other deep probabilistic architectures should be able to model high-level structure, but considering the difficulty in training these models with approximate inference, it is hard to make full use of their representational power. Unlike the star model of Felzenswalb et al. [18] that learns filters over predefined HOG image features, our SPN learns on top of learned image features that can model color and detailed patterns.\nGenerative SPN architectures on the same features produce unsatisfactory results as generative training is led astray by the large number of features, very few of which differentiate labels. In the generative SPN paper [23], continuous variables are modeled with univariate Gaussians at the leaves (viewed as a sum node with infinite children but finite weight sum). With discriminative training, X can be continuous because we always condition on it, which effectively folds it into the weights.\nAll networks are learned with stochastic gradient descent regularized by early stopping. We found that using marginal inference for the root node and MPE inference for the rest of the network worked best. This allows the SPN to continue learning the difference between classes even when it correctly classifies a training instance. The fraction of the training set reserved for validation with CIFAR-10 and STL-10 were 10% and 20%, respectively. Learning rates, P , and T were chosen based on validation set performance.", "publication_ref": ["b11", "b19", "b17", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Results on CIFAR-10", "text": "CIFAR-10 consists of 32x32 pixel images: 5 \u00d7 10 4 for training and 10 4 for testing. We first compare discriminative SPNs with other methods as we vary the size of the dictionary K. The results are seen in Figure 4. To fairly compare with recent work [10,20] we also set G = 4. In general, we observe that SPNs can achieve higher performance using half as many features as the next best approach, the learned pooling function. We hypothesize that this is because the SPN architecture allows us to discriminatively train large moveable parts, image structure that cannot be captured by larger dictionaries. In Jia et al. [20] the pooling functions blur individual features (i.e. a 6x6 pixel dictionary item), from which the classifier may have trouble inferring the coordination of image parts.\nWe then experimented with a finer grid and fewer dictionary items (G = 7, K = 400). Pooling functions destroy information, so it is better if less is done before learning. Finer grids are less feasible for the method in Jia et al. [20] as the number of rectangular pooling functions grows O(G 4 ). Our best test accuracy of 83.96% was achieved with W = 3, P = 200, and T = 2, chosen   ", "publication_ref": ["b9", "b19", "b19", "b19"], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "Method", "text": "Dictionary Accuracy Logistic Regression [24] 36.0% SVM [5] 39.5% SIFT [5] 65.6% mcRBM [24] 68.3% mcRBM-DBN [24] 71.0% Convolutional RBM [10] 78.9% K-means (Triangle) [10] 4000, 4x4 grid 79.6 % HKDES [4] 80.0% 3-Layer Learned RF [12] 1600, 9x9 grid 82.0% Learned Pooling [20] 6000, 4x4 grid 83.11% Discriminative SPN 400, 7x7 grid 83.96%  [11] 54.9% (\u00b1 0.4%) 1-layer Sparse Coding [11] 59.0% (\u00b1 0.8%) 3-layer Learned Receptive Field [12] 60.1% (\u00b1 1.0%) Discriminative SPN 62.3% (\u00b1 1.0%) by validation set performance. This architecture achieves the highest published test accuracy on the CIFAR-10 dataset, remarkably using one fifth the number of features of the next best approach. We compare top CIFAR-10 results in Table 3, highlighting the dictionary size of systems that use the feature extraction from Coates et al. [10].", "publication_ref": ["b23", "b4", "b4", "b23", "b23", "b9", "b9", "b3", "b11", "b19", "b10", "b10", "b11", "b9"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Results on STL-10", "text": "STL-10 has larger 96x96 pixel images and less labeled data (5,000 training and 8,000 test) than CIFAR-10 [10]. The training set is mapped to ten predefined folds of 1,000 images. We experimented on the STL-10 dataset in a manner similar to CIFAR-10, ignoring the 10 5 items of unlabeled data. Ten models were trained on the pre-specified folds, and test accuracy is reported as an average. With K=1600, G=8, W =4, P =10, and T =3 we achieved 62.3% (\u00b1 1.0% standard deviation among folds), the highest published test accuracy as of writing. Notably, this includes approaches that make use of the unlabeled training images. Like Coates and Ng [12], our architecture learns local relations among different feature maps. However, the SPN is able to discriminatively learn latent mixtures, which can encode a more nuanced decision boundary than the linear classifier used in their work. After we carried out our experiments, Bo et al. [6] reported a higher accuracy with their unsupervised features and a linear SVM. Just as with the features of Coates et al. [10], we anticipate that using an SPN instead of the SVM would be beneficial by learning spatial structure that the SVM cannot model.", "publication_ref": ["b9", "b11", "b5", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "Sum-product networks are a new class of probabilistic model where inference remains tractable despite high treewidth and many hidden layers. This paper introduced the first algorithms for learning SPNs discriminatively, using a form of backpropagation to compute gradients. Discriminative training allows for a wider variety of SPN architectures than generative training, because completeness and consistency do not have to be maintained over evidence variables. We proposed both \"soft\" and \"hard\" gradient algorithms, using marginal inference in the \"soft\" case and MPE inference in the \"hard\" case. The latter successfully combats the diffusion problem, allowing deep networks to be learned. Experiments on image classification benchmarks illustrate the power of discriminative SPNs.\nFuture research directions include applying other discriminative learning paradigms to SPNs (e.g. max-margin methods), automatically learning SPN structure, and applying discriminative SPNs to a variety of structured prediction problems.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgments: This research was partly funded by ARO grant W911NF-08-1-0242, AFRL contract FA8750-09-C-0181, NSF grant IIS-0803481, and ONR grant N00014-12-1-0312. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ARO, AFRL, NSF, ONR, or the United States Government.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Sum-product networks for modeling activities with stochastic structure", "journal": "CVPR", "year": "2012", "authors": "M Amer; S Todorovic"}, {"ref_id": "b1", "title": "Thin junction trees", "journal": "Advances in Neural Information Processing Systems", "year": "2002", "authors": "F Bach; M I Jordan"}, {"ref_id": "b2", "title": "Learning deep architectures for AI", "journal": "", "year": "2009", "authors": "Y Bengio"}, {"ref_id": "b3", "title": "Object recognition with hierarchical kernel descriptors", "journal": "IEEE", "year": "2011", "authors": "L Bo; K Lai; X Ren; D Fox"}, {"ref_id": "b4", "title": "Kernel descriptors for visual recognition", "journal": "", "year": "2010", "authors": "L Bo; X Ren; D Fox"}, {"ref_id": "b5", "title": "Unsupervised feature learning for RGB-D based object recognition", "journal": "ISER", "year": "2012", "authors": "L Bo; X Ren; D Fox"}, {"ref_id": "b6", "title": "Context-specific independence in bayesian networks", "journal": "", "year": "1996", "authors": "C Boutilier; N Friedman; M Goldszmidt; D Koller"}, {"ref_id": "b7", "title": "On probabilistic inference by weighted model counting. Artificial Intelligence", "journal": "", "year": "2008", "authors": "M Chavira; A Darwiche"}, {"ref_id": "b8", "title": "Efficient principled learning of thin junction trees", "journal": "MIT Press", "year": "2008", "authors": "A Chechetka; C Guestrin"}, {"ref_id": "b9", "title": "An analysis of single-layer networks in unsupervised feature learning", "journal": "", "year": "2011", "authors": "A Coates; H Lee; A Y Ng"}, {"ref_id": "b10", "title": "The importance of encoding versus training with sparse coding and vector quantization", "journal": "", "year": "2011", "authors": "A Coates; A Y Ng"}, {"ref_id": "b11", "title": "Selecting receptive fields in deep networks. NIPS", "journal": "", "year": "2011", "authors": "A Coates; A Y Ng"}, {"ref_id": "b12", "title": "Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms", "journal": "ACL", "year": "2002", "authors": "M Collins"}, {"ref_id": "b13", "title": "A differential approach to inference in Bayesian networks", "journal": "Journal of the ACM", "year": "2003", "authors": "A Darwiche"}, {"ref_id": "b14", "title": "Modeling and Reasoning with Bayesian Networks", "journal": "Cambridge University Press", "year": "2009", "authors": "A Darwiche"}, {"ref_id": "b15", "title": "Shallow vs. deep sum-product networks", "journal": "", "year": "2011", "authors": "O Delalleau; Y Bengio"}, {"ref_id": "b16", "title": "Maximum likelihood from incomplete data via the EM algorithm", "journal": "Journal of the Royal Statistical Society, Series B", "year": "1977", "authors": "A P Dempster; N M Laird; D B Rubin"}, {"ref_id": "b17", "title": "A discriminatively trained, multiscale, deformable part model", "journal": "Ieee", "year": "2008", "authors": "P Felzenszwalb; D Mcallester; D Ramanan"}, {"ref_id": "b18", "title": "Independent component analysis: algorithms and applications", "journal": "", "year": "2000", "authors": "A Hyv\u00e4rinen; E Oja"}, {"ref_id": "b19", "title": "Beyond spatial pyramids: Receptive field learning for pooled image features", "journal": "", "year": "2012", "authors": "Y Jia; C Huang; T Darrell"}, {"ref_id": "b20", "title": "Structured learning with approximate inference", "journal": "", "year": "2007", "authors": "A Kulesza; F Pereira"}, {"ref_id": "b21", "title": "Conditional random fields: Probabilistic models for segmenting and labeling data", "journal": "Morgan Kaufmann", "year": "2001", "authors": "J Lafferty; A Mccallum; F Pereira"}, {"ref_id": "b22", "title": "Sum-product networks: A new deep architecture", "journal": "", "year": "2011", "authors": "H Poon; P Domingos"}, {"ref_id": "b23", "title": "Modeling pixel means and covariances using factorized third-order Boltzmann machines", "journal": "IEEE", "year": "2010", "authors": "M A Ranzato; G E Hinton"}, {"ref_id": "b24", "title": "Expectation maximization algorithms for conditional likelihoods", "journal": "ACM", "year": "2005", "authors": "J Saloj\u00e4rvi; K Puolam\u00e4ki; S Kaski"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "The value of an SPN S[x 1 ,x 1 , . . . , x d ,x d ] is the value of its root.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: SPN over Boolean variables X 1 , X 2 , X 3", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Definition 2 .2(Poon & Domingos, 2011) A sum-product network S is valid iff S(e) = \u03a6 S (e) for all evidence e.In their paper, Poon and Domingos prove that there are two conditions sufficient for validity: completeness and consistency. Definition 3. (Poon & Domingos, 2011) A sum-product network is complete iff all children of the same sum node have the same scope. Definition 4. (Poon & Domingos, 2011) A sum-product network is consistent iff no variable appears negated in one child of a product node and non-negated in another. Theorem 1. (Poon & Domingos, 2011) A sum-product network is valid if it is complete and consistent.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Algorithm 1 :1LearnSPNInput: Set D of instances over variables X and label variables Y, a valid SPN S with initialized parameters. Output: An SPN with learned weights repeat forall the d \u2208 D do UpdateWeights(S, Inference(S,x d ,y d )) until convergence or early stopping condition;", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 3 :3Figure 3: SPN architecture for experiments. Hidden variable indicators omitted for legibility.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Pooling, Jia et al. K-means (tri.), white, Coates et al. Auto-encoder, raw, Coates et al. RBM, whitened, Coates et al.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 4 :4Figure4: Impact of dictionary size K with a 4x4 pooling grid (W =3) on CIFAR-10 test accuracy", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "BackpropSPNInput: A valid SPN S, where Sn denotes the value of node n after bottom-up evaluation.", "figure_data": "else\u2202S \u2202S j \u2190 \u2202S \u2202S j + wn,j \u2202S \u2202Sn \u2202S \u2202w n,j \u2190 Sj \u2202S \u2202Sn forall the j \u2208 Ch(n) do \u2202S \u2202S j \u2190 \u2202S \u2202S j + \u2202S \u2202Sn + + + + + + + k\u2208Ch(n)\\{j} S + + + + ++ ++ + ++ ++ + ++++ +++++ +++"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Inference procedures", "figure_data": "NodeSoft InferenceHard InferenceSum\u2202S \u2202Sn =k\u2208P a(n)\u2202S \u2202S k l\u2208Ch(k)\\{n}"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Weight updates   ", "figure_data": "UpdateSoft InferenceHard InferenceGen. GD \u2206w = \u03b7 \u2202S[x,y] \u2202w"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Test accuracies on CIFAR-10.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Comparison of average test accuracies on all folds of STL-10.", "figure_data": "MethodAccuracy (\u00b1\u03c3)1-layer Vector Quantization"}], "formulas": [{"formula_id": "formula_0", "formula_text": "X 1 \u2192 X 2 is P (x 1 )P (x 2 |x 1 )x 1 x 2 + P (x 1 )P (x 2 |x 1 )x 1x2 + P (x 1 )P (x 2 |x 1 )x 1 x 2 + P (x 1 )P (x 2 |x 1 )x 1x2 .", "formula_coordinates": [2.0, 108.0, 357.81, 395.5, 20.61]}, {"formula_id": "formula_1", "formula_text": "(X 1 , X 2 , X 3 ) in the Bayesian network X 2 \u2190 X 1 \u2192 X 3 using six indicators S[x 1 ,x 1 , x 2 ,x 2 , x 3 ,x 3 ].", "formula_coordinates": [2.0, 108.0, 634.72, 188.04, 32.24]}, {"formula_id": "formula_2", "formula_text": "S[x 1 ,x 1 , x 2 ,x 2 ] = (x 1 + x 2 ), S[1, 0, 1, 1] < S[1, 0, 1, 0] + S[1, 0, 0, 1]).", "formula_coordinates": [3.0, 108.0, 328.46, 288.7, 10.32]}, {"formula_id": "formula_3", "formula_text": "\u2202 \u2202w log P (y|x) = \u2202 \u2202w log h \u03a6(Y = y, H = h|x) \u2212 \u2202 \u2202w log y ,h \u03a6(Y = y , H = h|x) = 1 S[y, 1|x] \u2202S[y, 1|x] \u2202w \u2212 1 S[1, 1|x] \u2202S[1, 1|x] \u2202w", "formula_coordinates": [4.0, 121.29, 226.03, 370.61, 56.07]}, {"formula_id": "formula_4", "formula_text": "+ + + f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f f", "formula_coordinates": [5.0, 152.91, 92.0, 341.54, 46.23]}, {"formula_id": "formula_5", "formula_text": "\u2202 \u2202w logP (y|x) = \u2202 \u2202w log max h \u03a6(Y = y, H = h|x) \u2212 \u2202 \u2202w log max y ,h \u03a6(Y = y , H = h|x)", "formula_coordinates": [5.0, 117.42, 261.73, 378.35, 23.54]}, {"formula_id": "formula_6", "formula_text": "\u2202 log M \u2202wi = \u2202 log M \u2202M \u2202M \u2202wi = 1 M \u2202M \u2202wi = ci \u2022 w c i \u22121 i w j \u2208W \\{w i } w c j j w j \u2208W w c j j = ci wi", "formula_coordinates": [5.0, 158.75, 421.08, 294.01, 30.16]}, {"formula_id": "formula_7", "formula_text": "S l \u2202M \u2202Mn = k\u2208P a(n) \u2202M \u2202M k l\u2208Ch(k)\\{n} M l Product \u2202S \u2202Sn = k\u2208P a(n) w kn \u2202S \u2202S k \u2202M \u2202Mn = k\u2208P a(n) w kn \u2202M \u2202M k : w kn \u2208 W 0 : otherwise Weight \u2202S \u2202w ki = \u2202S \u2202S k S i \u2202M \u2202w ki = \u2202M \u2202M k M i", "formula_coordinates": [6.0, 125.02, 113.14, 357.08, 63.3]}, {"formula_id": "formula_8", "formula_text": "\u2206w i = \u03b7 ci wi Gen. EM P (H k = i|x, y) \u221d w ki \u2202S[x,y] \u2202S k P (H k = i|x, y) = 1 : w ki \u2208 W 0 : otherwise Disc. GD \u2206w = \u03b7 1 S[y,1|x] \u2202S[y,1|x] \u2202w \u2212 \u2206w i = \u03b7 \u2206ci wi 1 S[1,1|x]", "formula_coordinates": [6.0, 141.18, 202.43, 325.95, 73.02]}, {"formula_id": "formula_9", "formula_text": "\u2202 log M \u2202w i = 1 M \u2202M \u2202w i = ci w j \u2208W e c j \u2022w j w j \u2208W e c j \u2022w j = ci", "formula_coordinates": [6.0, 209.17, 305.72, 174.43, 32.38]}], "doi": ""}