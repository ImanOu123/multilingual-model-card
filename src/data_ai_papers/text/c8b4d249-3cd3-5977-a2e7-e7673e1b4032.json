{"title": "Immersions -How Does Music Sound to Artificial Ears?", "authors": "Vincent Herrmann", "pub_date": "", "abstract": "Immersions is a system that visualizes and sonifies the inner workings of a sound processing neural network in real-time. An optimization procedure generates sounds that activate certain regions in the network. This renders audible the way music sounds to this artificial ear. In addition the activations of the neurons at each point in time are visualized, creating the experience of immersing oneself into the depths of an artificial intelligence.", "sections": [{"heading": "Introduction", "text": "Although neural networks are often described as black boxes, there exist methods to make us see and hear what is going on inside a neural networks. A well known example of this is the DeepDream technique [1] that generates bizarre psychedelic but strangely familiar pictures. We follow a related approach to make audible the innards of sound processing neural nets.\nThe basic idea is the following: You start with an arbitrary sound clip (e.g. a drum loop). This clip will be modified by optimization procedure in way that stimulates a certain region in the neural net. The changed clip can then be used as a basis for a new optimization. This way you obtain sounds that are generated by the neural network in a freely associative way.\nOne of the most important properties of neural networks is that they process information on different levels of abstraction. An activation in some region of the net thus may for example be associated with short and simple sounds or noises, a different region with a more complex musical phenomenon, such as rhythm, phrase or key. For what exactly a neural networks listens depends on the data it was trained on, as well as on the task it is specialized to accomplish. This means to different nets music will sound differently. All this can be directly experienced with the Immersions project. For this I developed a setup that allows -not unlike a DJ mixing console -the generation, visualization and control of sound in real-time.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "Technical Concepts", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Contrastive Predictive Coding", "text": "In the center of this project stands an \"artificial ear\", meaning a sound processing neural network. It has been shown that analogies exist between convolutional neural networks (ConvNets) and the human auditory cortex [2]. Of course the human auditory system is not a classification network, as the used used in [2], we usually don't learn with the help of explicit labels. The question of the actual learning mechanisms in the brain is highly contested, but a promising candidate, especially for perceptual learning, might be predictive coding [3,4]. Here future stimuli are predicted and then compared to the actual occurred stimuli. The learning entails reducing the discrepancy between prediction and reality.\nOne method of using the idea of predictive coding for unsupervised learning in artificial neural networks is presented in [5] and [6]. For this so-called contrastive predictive coding you need two nets: The encoder net builds a compressed encoding z t of the signal for each time step t and the autoregressive net summarizes multiple sequential encodings into a vector c. The encoding k time steps in the future, z t+k , is predicted by a different learnt linear transformation M k of c for each time step. The loss for each mini-batch is calculated as follows:\nL CP C = \u2212 n,k log exp(z T n,k M k c n ) m exp(z T m,k M k c n )\nBoth n and m index the mini-batch dimension. For details on the model architecture used here, please see Figure 2 and appendix A.\nFor contrastive predictive coding no expensively labeled datasets are required, the model learns to make sense of any inputs it is given. The models for this project were trained on datasets consisting of about 40 hours of house music mixes or 200 hours of classical piano music.", "publication_ref": ["b1", "b1", "b2", "b3", "b4", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Visualization", "text": "To illustrate what is going on inside the model at each moment we visualize the activation of the artificial neurons. The time dimension is retained, resulting in animated pictures. The neurons have to be arranged in a 2D layout according to their position in the network (given by layer, channel and pitch). The neurons are connected with one another according to different patterns depending on the type of layer (e.g. fully connected or convolutional). This leads to a graph layout problem.\nTo find a layout for a graph of this size (tens of thousands vertices and many millions connections) a multilevel force layout method [7,8] is suited. Here the graph is modeled as a physical system, where the vertices repel each other as if electrically charged with the same polarity, and edges act like rubber bands pulling connected vertices together. To avoid local energy minima we start with few vertices and gradually add more until the layout of the complete graph is calculated. Details on the procedure can be found in appendix B.\nOnce the layout is worked out the current state of the net can be depicted by lighting up strongly activated neurons and letting others stay dark (see Figure 1).", "publication_ref": ["b6", "b7"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Input Optimization", "text": "Today's neural networks usually are completely differentiable. This means we can generate inputs for a trained model that maximize the activations of certain neurons in the network using gradient based optimization [9,1,10]. These inputs can then be directly experienced by humans and show which particular stimuli the selected neurons respond to.\nNeural nets, especially if they did not receive adversarial training [11], are susceptible to small changes in the input. These can lead to local optima that are not perceptible by humans but still have the required properties. To prevent this we apply several types of regularization: temporal shifting of the input, small pitch changes, masking of random regions in the scalogram, noisy gradients and de-noising of the input. All these methods make the input optimization more difficult in certain ways and thus enforce more robust and distinct results.", "publication_ref": ["b8", "b0", "b9", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Live Performance", "text": "The generated audio clips have a duration of about four seconds. With that they lend themselves for a loop based live performance. One loop then corresponds for example to two 4/4 measures at 120 bpm. The optimization procedure described above constantly generates new audio clips. As soon as one clip has finished playing, the latest newly calculated clip is started, resulting in an acoustic morphing.\nAll aspects of the procedure can be adjusted in real-time. For intuitive control I developed a GUI and made the most important parameters controllable by a MIDI controller. The setup described here is very flexible, other networks trained on different data or with different architectures can easily be employed.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ethical Implication", "text": "Neural networks might be the artificial systems that come yet closest to our own ways of thinking and perceiving. This work tries tries to explore this area between familiarity and strangeness. The visualization in a way anthropomorphizes, or at least biologizes, the results of calculations (the emerging shapes, although completely determined by the network architecture, could remind one of a micro-organism or a brain in a jar). You might argue that this paints a wrong and maybe even dangerous picture, buying into the current hype or promoting overconfidence in these kind of novel systems. My hope however is that this work rather invites people to think critically about the results they see and hear. We are all fascinated by neural networks not without a reason. I don't think we have even begun exhaust the many ways to examine them and gain a deeper understanding.\nFor further material, demos and code, please see https://vincentherrmann.github.io/demos/ immersions   ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Neural Layouts", "text": "The coarse graph levels for the layout calculation presented in 2.2 are achieved by iteratively merging two vertices into one with the combined weight. First we merge neighbouring vertices in the channel dimension, then in the pitch dimension. During the layout calculation the higher resolution graphs are reconstructed in the reverse order. When calculating the layout for a trained neural network, I used the variance of each activation over the validation set as the vertex weight.\nSince in a force layout each vertex interacts with every other one, to keep the computational load acceptable a Barnes-Hut tree approximation [14] has to be used in order to reduce the computational complexity. For GPU acceleration and easy integration into the software existing ecosystem I implemented the graph layout algorithm in PyTorch. The code is available at https://vincentherrmann.github.io/demos/immersions. This visualization method is very flexible and can be used for various neural network architectures. While it is possible to calculate a layout for a complete 2D convolutional network, in a 2D visualization setting the results are overcrowded an thus not very insightful. Because of that we stick with cuts through one of the spatial dimensions for the VGG [15] and ResNet [13] architectures shown in Figures 5 and 6. ", "publication_ref": ["b13", "b14", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "A Network Architecture", "text": "The encoder network receives as input the scalogram of a roughly four second long audio clip. A scalogram is the result of a constant-Q or a wavelet transformation [12] of the audio signal and comes quite close to the representation that the cochlea passes on to the brain (see [2]). Since it is a 2D representation (with the dimensions time and frequency) we can use a conventional 2D-ConvNet of the kind common in the computer vision and image processing.\nAs encoder we use a modified ResNet [13] with ReLu nonlinearities, max pooling and batch normalization. In addition to the usual quadratic kernels we also apply kernels that only span the pitch dimension and are supposed to detect overtones and harmonies (see Table 1). The autoregressive model is a ResNet as well, but with 1D convolutions (Table 2). ", "publication_ref": ["b11", "b1", "b12"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Inceptionism: Going deeper into neural networks", "journal": "", "year": "2015", "authors": "Alexander Mordvintsev; Christopher Olah; Mike Tyka"}, {"ref_id": "b1", "title": "A task-optimized neural network replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy", "journal": "Neuron", "year": "2018", "authors": "J E Alexander;  Kell; L K Daniel; Erica N Yamins;  Shook; Josh H Sam V Norman-Haignere;  Mcdermott"}, {"ref_id": "b2", "title": "Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects", "journal": "Nature neuroscience", "year": "1999", "authors": "P N Rajesh;  Rao; H Dana;  Ballard"}, {"ref_id": "b3", "title": "A theory of cortical responses", "journal": "Philosophical transactions of the Royal Society B: Biological sciences", "year": "1456", "authors": "Karl Friston"}, {"ref_id": "b4", "title": "Representation learning with contrastive predictive coding", "journal": "", "year": "2018", "authors": "Aaron Van Den Oord; Yazhe Li; Oriol Vinyals"}, {"ref_id": "b5", "title": "Wasserstein dependency measure for representation learning", "journal": "", "year": "2019", "authors": "Sherjil Ozair; Corey Lynch; Yoshua Bengio; Aaron Van Den Oord; Sergey Levine; Pierre Sermanet"}, {"ref_id": "b6", "title": "A multilevel algorithm for force-directed graph-drawing", "journal": "Journal of Graph Algorithms and Applications", "year": "2006", "authors": "Chris Walshaw"}, {"ref_id": "b7", "title": "Efficient, high-quality force-directed graph drawing", "journal": "Mathematica Journal", "year": "2005", "authors": "Yifan Hu"}, {"ref_id": "b8", "title": "Visualizing higher-layer features of a deep network", "journal": "", "year": "2009", "authors": "Dumitru Erhan; Yoshua Bengio; Aaron Courville; Pascal Vincent"}, {"ref_id": "b9", "title": "Feature visualization", "journal": "Distill", "year": "2017", "authors": "Chris Olah; Alexander Mordvintsev; Ludwig Schubert"}, {"ref_id": "b10", "title": "Learning perceptually-aligned representations via adversarial robustness", "journal": "", "year": "2019", "authors": "Logan Engstrom; Andrew Ilyas; Shibani Santurkar; Dimitris Tsipras; Brandon Tran; Aleksander Madry"}, {"ref_id": "b11", "title": "Constant q-wave propagation and attenuation", "journal": "Journal of Geophysical Research: Solid Earth", "year": "1979", "authors": "Einar Kjartansson"}, {"ref_id": "b12", "title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"ref_id": "b13", "title": "A hierarchical o (n log n) force-calculation algorithm", "journal": "nature", "year": "1986", "authors": "Josh Barnes; Piet Hut"}, {"ref_id": "b14", "title": "Very deep convolutional networks for large-scale image recognition", "journal": "", "year": "2014", "authors": "Karen Simonyan; Andrew Zisserman"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Activation visualization of the model trained on house music", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Activation visualization of the model with each layer in a different color", "figure_data": ""}, {"figure_label": "456", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :Figure 5 :Figure 6 :456Figure 4: Four steps of the layout calculation for the model architecture used in this paper", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "L CP C = \u2212 n,k log exp(z T n,k M k c n ) m exp(z T m,k M k c n )", "formula_coordinates": [2.0, 220.99, 152.12, 168.82, 29.78]}], "doi": ""}