{"title": "A Simple Theoretical Model of Importance for Summarization", "authors": "Maxime Peyrard", "pub_date": "", "abstract": "Research on summarization has mainly been driven by empirical approaches, crafting systems to perform well on standard datasets with the notion of information Importance remaining latent. We argue that establishing theoretical models of Importance will advance our understanding of the task and help to further improve summarization systems. To this end, we propose simple but rigorous definitions of several concepts that were previously used only intuitively in summarization: Redundancy, Relevance, and Informativeness. Importance arises as a single quantity naturally unifying these concepts. Additionally, we provide intuitions to interpret the proposed quantities and experiments to demonstrate the potential of the framework to inform and guide subsequent works.", "sections": [{"heading": "Introduction", "text": "Summarization is the process of identifying the most important information from a source to produce a comprehensive output for a particular user and task (Mani, 1999). While producing readable outputs is a problem shared with the field of Natural Language Generation, the core challenge of summarization is the identification and selection of important information. The task definition is rather intuitive but involves vague and undefined terms such as Importance and Information.\nSince the seminal work of Luhn (1958), automatic text summarization research has focused on empirical developments, crafting summarization systems to perform well on standard datasets leaving the formal definitions of Importance latent (Das and Martins, 2010;Nenkova and McKeown, 2012). This view entails collecting datasets, defining evaluation metrics and iteratively selecting the best-performing systems either via super-vised learning or via repeated comparison of unsupervised systems (Yao et al., 2017).\nSuch solely empirical approaches may lack guidance as they are often not motivated by more general theoretical frameworks. While these approaches have facilitated the development of practical solutions, they only identify signals correlating with the vague human intuition of Importance. For instance, structural features like centrality and repetitions are still among the most used proxies for Importance (Yao et al., 2017;Kedzie et al., 2018). However, such features just correlate with Importance in standard datasets. Unsurprisingly, simple adversarial attacks reveal their weaknesses (Zopf et al., 2016).\nWe postulate that theoretical models of Importance are beneficial to organize research and guide future empirical works. Hence, in this work, we propose a simple definition of information importance within an abstract theoretical framework. This requires the notion of information, which has received a lot of attention since the work from Shannon (1948) in the context of communication theory. Information theory provides the means to rigorously discuss the abstract concept of information, which seems particularly well suited as an entry point for a theory of summarization. However, information theory concentrates on uncertainty (entropy) about which message was chosen from a set of possible messages, ignoring the semantics of messages (Shannon, 1948). Yet, summarization is a lossy semantic compression depending on background knowledge.\nIn order to apply information theory to summarization, we assume texts are represented by probability distributions over so-called semantic units (Bao et al., 2011). This view is compatible with the common distributional embedding representation of texts rendering the presented framework applicable in practice. When applied to semantic symbols, the tools of information theory indirectly operate at the semantic level (Carnap and Bar-Hillel, 1953;Zhong, 2017).", "publication_ref": ["b45", "b43", "b10", "b53", "b72", "b72", "b32", "b76", "b61", "b61", "b1", "b5", "b75"], "figure_ref": [], "table_ref": []}, {"heading": "Contributions:", "text": "\u2022 We define several concepts intuitively connected to summarization: Redundancy, Relevance and Informativeness. These concepts have been used extensively in previous summarization works and we discuss along the way how our framework generalizes them.\n\u2022 From these definitions, we formulate properties required from a useful notion of Importance as the quantity unifying these concepts. We provide intuitions to interpret the proposed quantities.\n\u2022 Experiments show that, even under simplifying assumptions, these quantities correlates well with human judgments making the framework promising in order to guide future empirical works.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Framework", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Terminology and Assumptions", "text": "We call semantic unit an atomic piece of information (Zhong, 2017;Cruse, 1986). We note \u2126 the set of all possible semantic units.\nA text X is considered as a semantic source emitting semantic units as envisioned by Weaver (1953) and discussed by Bao et al. (2011). Hence, we assume that X can be represented by a probability distribution P X over the semantic units \u2126.", "publication_ref": ["b75", "b9", "b70", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Possible interpretations:", "text": "One can interpret P X as the frequency distribution of semantic units in the text. Alternatively, P X (\u03c9 i ) can be seen as the (normalized) likelihood that a text X entails an atomic information \u03c9 i (Carnap and Bar-Hillel, 1953). Another interpretation is to view P X (\u03c9 i ) as the normalized contribution (utility) of \u03c9 i to the overall meaning of X (Zhong, 2017).", "publication_ref": ["b5", "b75"], "figure_ref": [], "table_ref": []}, {"heading": "Motivation for semantic units:", "text": "In general, existing semantic information theories either postulate or imply the existence of semantic units (Carnap and Bar-Hillel, 1953;Bao et al., 2011;Zhong, 2017). For example, the Theory of Strongly Semantic Information produced by Floridi (2009) implies the existence of semantic units (called information units in his work). Building on this, Tsvetkov (2014) argued that the original theory of Shannon can operate at the semantic level by relying on semantic units.\nIn particular, existing semantic information theories imply the existence of semantic units in formal semantics (Carnap and Bar-Hillel, 1953), which treat natural languages as formal languages (Montague, 1970). In general, lexical semantics (Cruse, 1986) also postulates the existence of elementary constituents called minimal semantic constituents. For instance, with frame semantics (Fillmore, 1976), frames can act as semantic units.\nRecently, distributional semantics approaches have received a lot of attention (Turian et al., 2010;Mikolov et al., 2013b). They are based on the distributional hypothesis (Harris, 1954) and the assumption that meaning can be encoded in a vector space (Turney and Pantel, 2010;Erk, 2010). These approaches also search latent and independent components that underlie the behavior of words (G\u00e1bor et al., 2017;Mikolov et al., 2013a).\nWhile different approaches to semantics postulate different basic units and different properties for them, they have in common that meaning arises from a set of independent and discrete units. Thus, the semantic units assumption is general and has minimal commitment to the actual nature of semantics. This makes the framework compatible with most existing semantic representation approaches. Each approach specifies these units and can be plugged in the framework, e.g., frame semantics would define units as frames, topic models (Allahyari et al., 2017) would define units as topics and distributional representations would define units as dimensions of a vector space.\nIn the following paragraphs, we represent the source document(s) D and a candidate summary S by their respective distributions P D and P S . 1", "publication_ref": ["b5", "b1", "b75", "b21", "b5", "b52", "b9", "b20", "b65", "b51", "b28", "b66", "b18", "b23", "b50", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Redundancy", "text": "Intuitively, a summary should contain a lot of information. In information-theoretic terms, the amount of information is measured by Shannon's entropy. For a summary S represented by P S :\nH(S) = \u2212 \u03c9 i P S (\u03c9 i ) \u2022 log(P S (\u03c9 i )) (1)\nH(S) is maximized for a uniform probability distribution when every semantic unit is present only once in S: \u2200(i, j), P S (\u03c9 i ) = P S (\u03c9 j ). Therefore, we define Redundancy, our first quantity relevant to summarization, via entropy:\nRed(S) = H max \u2212 H(S)(2)\nSince H max = log |\u2126| is a constant indepedent of S, we can simply write: Red(S) = \u2212H(S).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Redundancy in Previous Works:", "text": "By definition, entropy encompasses the notion of maximum coverage. Low redundancy via maximum coverage is the main idea behind the use of submodularity (Lin and Bilmes, 2011). Submodular functions are generalizations of coverage functions which can be optimized greedily with guarantees that the result would not be far from optimal (Fujishige, 2005). Thus, they have been used extensively in summarization (Sipos et al., 2012;Yogatama et al., 2015). Otherwise, low redundancy is usually enforced during the extraction/generation procedures like MMR (Carbonell and Goldstein, 1998).", "publication_ref": ["b39", "b22", "b62", "b73", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Relevance", "text": "Intuitively, observing a summary should reduce our uncertainty about the original text. A summary approximates the original source(s) and this approximation should incur a minimum loss of information. This property is usually called Relevance.\nHere, estimating Relevance boils down to comparing the distributions P S and P D , which is done via the cross-entropy Rel(S, D) = \u2212CE(S, D):\nRel(S, D) = \u03c9 i P S (\u03c9 i ) \u2022 log(P D (\u03c9 i )) (3)\nThe cross-entropy is interpreted as the average surprise of observing S while expecting D. A summary with a low expected surprise produces a low uncertainty about what were the original sources. This is achieved by exhibiting a distribution of semantic units similar to the one of the source documents: P S \u2248 P D . Furthermore, we observe the following connection with Redundancy:\nKL(S||D) = CE(S, D) \u2212 H(S) \u2212KL(S||D) = Rel(S, D) \u2212 Red(S) (4)\nKL divergence is the information loss incurred by using D as an approximation of S (i.e., the uncertainty about D arising from observing S instead of D). A summarizer that minimizes the KL divergence minimizes Redundancy while maximizing Relevance.\nIn fact, this is an instance of the Kullback Minimum Description Principle (MDI) (Kullback and Leibler, 1951), a generalization of the Maximum Entropy Principle (Jaynes, 1957): the summary minimizing the KL divergence is the least biased (i.e., least redundant or with highest entropy) summary matching D. In other words, this summary fits D while inducing a minimum amount of new information. Indeed, any new information is necessarily biased since it does not arise from observations in the sources. The MDI principle and KL divergence unify Redundancy and Relevance.", "publication_ref": ["b33", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "Relevance in Previous Works:", "text": "Relevance is the most heavily studied aspect of summarization. In fact, by design, most unsupervised systems model Relevance. Usually, they used the idea of topical frequency where the most frequent topics from the sources must be extracted. Then, different notions of topics and counting heuristics have been proposed. We briefly discuss these developments here. Luhn (1958) introduced the simple but influential idea that sentences containing the most important words are most likely to embody the original document. Later, Nenkova et al. (2006) showed experimentally that humans tend to use words appearing frequently in the sources to produce their summaries. Then, Vanderwende et al. (2007) developed the system SumBasic, which scores each sentence by the average probability of its words.\nThe same ideas can be generalized to n-grams. A prominent example is the ICSI system (Gillick and Favre, 2009) which extracts frequent bigrams. Despite being rather simple, ICSI produces strong and still close to state-of-the-art summaries (Hong et al., 2014).\nDifferent but similar words may refer to the same topic and should not be counted separately.\nThis observation gave rise to a set of important techniques based on topic models (Allahyari et al., 2017). These approaches cover sentence clustering (McKeown et al., 1999;Radev et al., 2000;Zhang et al., 2015), lexical chains (Barzilay and Elhadad, 1999), Latent Semantic Analysis (Deerwester et al., 1990) or Latent Dirichlet Allocation (Blei et al., 2003) adapted to summarization (Hachey et al., 2006;Daum\u00e9 III and Marcu, 2006;Wang et al., 2009;Davis et al., 2012). Approaches like hLDA can exploit repetitions both at the word and at the sentence level (Celikyilmaz and Hakkani-Tur, 2010).\nGraph-based methods form another particularly powerful class of techniques to estimate the frequency of topics, e.g., via the notion of centrality (Mani and Bloedorn, 1997;Mihalcea and Tarau, 2004;Erkan and Radev, 2004). A significant body of research was dedicated to tweak and improve various components of graph-based approaches. For example, one can investigate different similarity measures (Chali and Joty, 2008). Also, different weighting schemes between sentences have been investigated (Leskovec et al., 2005;Wan and Yang, 2006). Therefore, in existing approaches, the topics (i.e., atomic units) were words, n-grams, sentences or combinations of these. The general idea of preferring frequent topics based on various counting heuristics is formalized by cross-entropy. Indeed, requiring the summary to minimize the crossentropy with the source documents implies that frequent topics in the sources should be extracted first.\nAn interesting line of work is based on the assumption that the best sentences are the ones that permit the best reconstruction of the input documents (He et al., 2012). It was refined by a stream of works using distributional similarities (Li et al., 2015;Ma et al., 2016). There, the atomic units are the dimensions of the vector spaces. This information bottleneck idea is also neatly captured by the notion of cross-entropy which is a measure of information loss. Alternatively, (Daum\u00e9 and Marcu, 2002) viewed summarization as a noisy communication channel which is also rooted in information theory ideas. (Wilson and Sperber, 2008) provide a more general and less formal discussion of relevance in the context of Relevance Theory (Lavrenko, 2008).", "publication_ref": ["b43", "b55", "b67", "b24", "b30", "b0", "b48", "b60", "b74", "b2", "b14", "b3", "b25", "b12", "b69", "b13", "b6", "b46", "b49", "b19", "b7", "b35", "b68", "b29", "b36", "b44", "b11", "b71", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "Informativeness", "text": "Relevance still ignores other potential sources of information such as previous knowledge or preconceptions. We need to further extend the contextual boundary. Intuitively, a summary is informative if it induces, for a user, a great change in her knowledge about the world. Therefore, we introduce K, the background knowledge (or preconceptions about the task). K is represented by a probability distribution P K over semantic units \u2126.\nFormally, the amount of new information contained in a summary S is given by the crossentropy Inf (S, K) = CE(S, K):\nInf (S, K) = \u2212 \u03c9 i P S (\u03c9 i ) \u2022 log(P K (\u03c9 i )) (5)\nFor Relevance the cross-entropy between S and D should be low. However, for Informativeness, the cross-entropy between S and K should be high because we measure the amount of new information induced by the summary in our knowledge.\nBackground knowledge is modeled by assigning a high probability to known semantic units. These probabilities correspond to the strength of \u03c9 i in the user's memory. A simple model could be the uniform distribution over known information:\nP K (\u03c9 i ) is 1\nn if the user knows \u03c9 i , and 0 otherwise. However, K can control other variants of the summarization task: A personalized K p models the preferences of a user by setting low probabilities to the semantic units of interest. Similarly, a query Q can be encoded by setting low probability to semantic units related to Q. Finally, there is a natural formulation of update summarization. Let U and D be two sets of documents. Update summarization consists in summarizing D given that the user has already seen U . This is modeled by setting K = U , considering U as previous knowledge.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Informativeness in Previous Works:", "text": "The modelization of Informativeness has received less attention by the summarization community. The problem of identifying stopwords originally faced by Luhn (1958) could be addressed by developments in the field of information retrieval using background corpora like TF\u2022IDF (Sparck Jones, 1972). Based on the same intuition, Dunning (1993) outlined an alternative way of identifying highly descriptive words: the loglikelihood ratio test. Words identified with such techniques are known to be useful in news summarization (Harabagiu and Lacatusu, 2005).\nFurthermore, Conroy et al. (2006) proposed to model background knowledge by a large random set of news articles. In update summarization, Delort and Alfonseca (2012) used Bayesian topic models to ensure the extraction of informative summaries. Louis (2014) investigated background knowledge for update summarization with Bayesian surprise. This is comparable to the combination of Informativeness and Redundancy in our framework when semantic units are ngrams. Thus, previous approaches to Informativeness generally craft an alternate background distribution to model the a-priori importance of units. Then, units from the document rare in the background are preferred, which is captured by maximizing the cross-entropy between the summary and K. Indeed, unfrequent units in the background would be preferred in the summary because they would be surprising (i.e., informative) to an average user.", "publication_ref": ["b63", "b16", "b27", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Importance", "text": "Since Importance is a measure that guides which choices to make when discarding semantic units, we must devise a way to encode their relative importance. Here, this means finding a probability distribution unifying D and K by encoding expectations about which semantic units should appear in a summary.\nInformativeness requires a biased summary (w.r.t. K) and Relevance requires an unbiased summary (w.r.t. D). Thus, a summary should, by using only information available in D, produce what brings the most new information to a user with knowledge K. This could formalize a common intuition in summarization that units frequent in the source(s) but rare in the background are important.\nFormally, let d i = P D (\u03c9 i ) be the probability of the unit \u03c9 i in the source D. Similarly, we note k i = P K (\u03c9 i ). We seek a function f (d i , k i ) encoding the importance of unit \u03c9 i . We formulate simple requirements that f should satisfy:\n\u2022 Informativeness: \u2200i = j, if d i = d j and k i > k j then f (d i , k i ) < f (d j , k j ) \u2022 Relevance: \u2200i = j, if d i > d j and k i = k j then f (d i , k i ) > f (d j , k j ) \u2022 Additivity: I(f (d i , k i )) \u2261 \u03b1I(d i ) + \u03b2I(k i )\n(I is the information measure from Shannon's theory (Shannon, 1948))\n\u2022 Normalization:\ni f (d i , k i ) = 1\nThe first requirement states that, for two semantic units equally represented in the sources, we prefer the more informative one. The second requirement is an analogous statement for Relevance. The third requirement is a consistency constraint to preserve additivity of the information measures (Shannon, 1948). The fourth requirement ensures that f is a valid distribution.\nTheorem 1. The functions satisfying the previous requirements are of the form:\nP D K (\u03c9 i ) = 1 C \u2022 d \u03b1 i k \u03b2 i (6) C = i d \u03b1 i k \u03b2 i , \u03b1, \u03b2 \u2208 R + (7)\nC is the normalizing constant. The parameters \u03b1 and \u03b2 represent the strength given to Relevance and Informativeness respectively which is made clearer by equation ( 11). The proof is provided in appendix B.", "publication_ref": ["b61", "b61"], "figure_ref": [], "table_ref": []}, {"heading": "Summary scoring function:", "text": "By construction, a candidate summary should approximate P D K , which encodes the relative importance of semantic units. Furthermore, the summary should be non-redundant (i.e., high entropy). These two requirements are unified by the Kullback MDI principle: The least biased summary S * that best approximates the distribution P D K is the solution of:\nS * = argmax S \u03b8 I = argmin S KL(S||P D K ) (8)\nThus, we note \u03b8 I as the quantity that scores summaries:\n\u03b8 I (S, D, K) = \u2212KL(P S , ||P D K )(9)\nInterpretation of P D K :\nP D K\ncan be viewed as an importance-encoding distribution because it encodes the relative importance of semantic units and gives an overall target for the summary.\nFor example, if a semantic unit \u03c9 i is prominent in D (P D (\u03c9 i ) is high) and not known in K (P D (\u03c9 i ) is low), then P D K (\u03c9 i ) is very high, which means very desired in the summary. Indeed, choosing this unit will fill the gap in the knowledge K while matching the sources.\nFigure 1 illustrates how this distribution behaves with respect to D and K (for \u03b1 = \u03b2 = 1).", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Summarizability:", "text": "The target distribution P D K may exhibit different properties. For example, it might be clear which semantic units should be extracted (i.e., a spiky probability distribution) or it might be unclear (i.e., many units have more or less the same importance score). This can be quantified by the entropy of the importance-encoding distribution:\nH D K = H(P D K )(10)\nIntuitively, this measures the number of possibly good summaries. If H D K is low then P D S is spiky and there is little uncertainty about which semantic units to extract (few possible good summaries). Conversely, if the entropy is high, many equivalently good summaries are possible.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Interpretation of \u03b8 I :", "text": "To better understand \u03b8 I , we remark that it can be expressed in terms of the previously defined quantities: It is worth noting that each previously defined quantity: Red, Rel and Inf are measured in bits (using base 2 for the logarithm). Then, \u03b8 I is also an information measure expressed in bits. Shannon (1948) initially axiomatized that information quantities should be additive and therefore \u03b8 I arising as the sum of other information quantities is unsurprising. Moreover, we ensured additivity with the third requirement of P D K .", "publication_ref": ["b61"], "figure_ref": [], "table_ref": []}, {"heading": "Potential Information", "text": "Relevance relates S and D, Informativeness relates S and K, but we can also connect D and K.\nIntuitively, we can extract a lot of new information from D only when K and D are different.\nWith the same argument laid out for Informativeness, we can define the amount of potential information as the average surprise of observing D while already knowing K. Again, this is given by the cross-entropy P I K (D) = CE(D, K):\nP I K (D) = \u2212 \u03c9 i P D (\u03c9 i ) \u2022 log(P K (\u03c9 i )) (13)\nPreviously, we stated that a summary should aim, using only information from D, to offer the maximum amount of new information with respect to K. P I K (D) can be understood as Potential Information or maximum Informativeness, the maximum amount of new information that a summary can extract from D while knowing K. A summary S cannot extract more than P I K (D) bits of information (if using only information from D).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental setup", "text": "To further illustrate the workings of the formula, we provide examples of experiments done with a simplistic choice for semantic units: words. Even with simple assumptions \u03b8 I is a meaningful quantity which correlates well with human judgments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data:", "text": "We experiment with standard datasets for two different summarization tasks: generic and update multi-document summarization.\nWe use two datasets from the Text Analysis Conference (TAC) shared task: TAC-2008 and TAC-2009. 2 In the update part, 10 new documents (B documents) are to be summarized assuming that the first 10 documents (A documents) have already been seen. The generic task consists in summarizing the initial document set (A).\nFor each topic, there are 4 human reference summaries and a manually created Pyramid set . In both editions, all system summaries and the 4 reference summaries were manually evaluated by NIST assessors for readability, content selection (with Pyramid) and overall responsiveness. At the time of the shared tasks, 57 systems were submitted to TAC-2008 and55 to TAC-2009. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Setup and Assumptions:", "text": "To keep the experiments simple and focused on illustrating the formulas, we make several simplistic assumptions. First, we choose words as semantic units and therefore texts are represented as frequency distributions over words. This assumption was already employed by previous works using information-theoretic tools for summarization (Haghighi and Vanderwende, 2009). While it is limiting, this remains a simple approximation letting us observe the quantities in action. K, \u03b1 and \u03b2 are the parameters of the theory and their choice is subject to empirical investigation. Here, we make simple choices: for update summarization, K is the frequency distribution over words in the background documents (A). For generic summarization, K is the uniform probability distribution over all words from the source documents. Furthermore, we use \u03b1 = \u03b2 = 1.", "publication_ref": ["b26"], "figure_ref": [], "table_ref": []}, {"heading": "Correlation with humans", "text": "First, we measure how well the different quantities correlate with human judgments. We compute the score of each system summary according to each quantity defined in the previous section: Red, Rel, Inf , \u03b8 I (S, D, K). We then compute the correlations between these scores and the manual Pyramid scores. Indeed, each quantity is a summary scoring function and could, therefore, be evaluated based on its ability to correlate with human judgments (Lin and Hovy, 2003). Thus, we also report the performances of the summary scoring functions from several standard baselines: Edmundson (Edmundson, 1969) which scores sentences based on 4 methods: term frequency, presence of cue-words, overlap with title and position of the sentence. LexRank (Erkan and Radev, 2004) is a popular graph-based approach which scores sentences based on their centrality in a sentence similarity graph. ICSI (Gillick and Favre, 2009) extracts a summary by solving a maximum coverage problem considering the most frequent bigrams in the source documents. KL and JS (Haghighi and Vanderwende, 2009) which measure the divergence between the distribution of words in the summary and in the sources. Furthermore, we report two baselines from Louis (2014) which account for background knowledge: KL back and JS back which measure the divergence between the distribution of the summary and the background knowledge K. Further details concerning baseline scoring functions can be found in appendix A.\nWe measure the correlations with Kendall's \u03c4 , a rank correlation metric which compares the orders induced by both scored lists. We report results for both generic and update summarization averaged over all topics for both datasets in table 1.\nIn general, the modelizations of Relevance (based only on the sources) correlate better with human judgments than other quantities. Metrics accounting for background knowledge work better in the update scenario. This is not surprising as the background knowledge K is more meaningful in this case (using the previous document set).\nWe observe that JS divergence gives slightly better results than KL. Even though KL is more theoretically appealing, JS is smoother and usually works better in practice when distributions have different supports (Louis and Nenkova, 2013).\nFinally, \u03b8 I significantly 3 outperforms all baselines in both the generic and the update case. Red, Rel and Inf are not particularly strong on their own, but combined together they yield a strong summary scoring function \u03b8 I . Indeed, each quantity models only one aspect of content selection, only together they form a strong signal for Importance.\nWe need to be careful when interpreting these results because we made several strong assumptions: by choosing n-grams as semantic units and by choosing K rather arbitrarily. Nevertheless, these are promising results. By investigating better text representations and more realistic K, we should expect even higher correlations. We provide a qualitative example on one topic in appendix C with a visualization of P D K in comparison to reference summaries.  ", "publication_ref": ["b38", "b17", "b19", "b24", "b26", "b41", "b42"], "figure_ref": [], "table_ref": []}, {"heading": "Comparison with Reference Summaries", "text": "Intuitively, the distribution P D K should be similar to the probability distribution P R of the humanwritten reference summaries.\nTo verify this, we scored the system summaries and the reference summaries with \u03b8 I and checked whether there is a significant difference between the two lists. 4 We found that \u03b8 I scores reference summaries significantly higher than system summaries. The p\u2212value, for the generic case, is 9.2e\u22126 and 1.1e\u22123 for the update case. Both are much smaller than the 1e\u22122 significance level. Therefore, \u03b8 I is capable of distinguishing systems summaries from human written ones. For comparison, the best baseline (JS) has the following p\u2212values: 8.2e\u22123 (Generic) and 4.5e\u22122 (Update). It does not pass the 1e\u22122 significance level for the update scenario.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion and Future Work", "text": "In this work, we argued for the development of theoretical models of Importance and proposed one such framework. Thus, we investigated a theoretical formulation of the notion of Importance. In a framework rooted in information theory, we formalized several summary-related quantities like: Redundancy, Relevance and Informativeness. Importance arises as the notion unifying these concepts. More generally, Importance is the measure that guides which choices to make when information must be discarded. The introduced quantities generalize the intuitions that have previously been used in summarization research.\nConceptually, it is straightforward to build a system out of \u03b8 I once a semantic units representation and a K have been chosen. A summarizer intends to extract or generate a summary maximizing \u03b8 I . This fits within the general optimization framework for summarization (McDonald, 2007;Peyrard and Eckle-Kohler, 2017b;Peyrard and Gurevych, 2018) The background knowledge and the choice of semantic units are free parameters of the theory. They are design choices which can be explored empirically by subsequent works. Our experiments already hint that strong summarizers can be developed from this framework. Characters, character n-grams, morphemes, words, n-grams, phrases, and sentences do not actually qualify as semantic units. Even though previous works who relied on information theoretic motivation (Lin et al., 2006;Haghighi and Vanderwende, 2009;Louis and Nenkova, 2013;Peyrard and Eckle-Kohler, 2016) used some of them as support for probability distributions, they are neither atomic nor independent. It is mainly because they are surface forms whereas semantic units are abstract and operate at the semantic level. However, they might serve as convenient approximations. Then, interesting research questions arise like Which granularity offers a good approximation of semantic units? Can we automatically learn good approximations? N-grams are known to be useful, but other granularities have rarely been considered together with information-theoretic tools.\nFor the background knowledge K, a promising direction would be to use the framework to actually learn it from data. In particular, one can apply supervised techniques to automatically search for K, \u03b1 and \u03b2: finding the values of these parame-ters such that \u03b8 I has the best correlation with human judgments. By aggregating over many users and many topics one can find a generic K: what, on average, people consider as known when summarizing a document. By aggregating over different people but in one domain, one can uncover a domain-specific K. Similarly, by aggregating over many topics for one person, one would find a personalized K.\nThese consistute promising research directions for future works.", "publication_ref": ["b47", "b58", "b59", "b37", "b26", "b42", "b56"], "figure_ref": [], "table_ref": []}, {"heading": "A Details about Baseline Scoring Functions", "text": "In the paper, we compare the summary scoring function \u03b8 I against the summary scoring functions derived from several summarizers following the methodology from Peyrard and Eckle-Kohler (2017a). Here, we give explicit formulation of the baseline scoring functions.\nEdmundson: (Edmundson, 1969) Edmundson (1969 presented a heuristic which scores sentences according to 4 different features:\n\u2022 Cue-phrases: It is based on the hypothesis that the probable relevance of a sentence is affected by the presence of certain cue words such as 'significant' or 'important'. Bonus words have positive weights, stigma words have negative weights and all the others have no weight. The final score of the sentence is the sum of the weights of its words.\n\u2022 Key: High-frequency content words are believed to be positively correlated with relevance (Luhn, 1958). Each word receives a weight based on its frequency in the document if it is not a stopword. The score of the sentence is also the sum of the weights of its words.\n\u2022 Title: It measures the overlap between the sentence and the title.\n\u2022 Location: It relies on the assumption that sentences appearing early or late in the source documents are more relevant.\nBy combining these scores with a linear combination, we can recognize the objective function:\n\u03b8 Edm. (S) = s\u2208S \u03b1 1 \u2022 C(s) + \u03b1 2 \u2022 K(s) (14) + \u03b1 3 \u2022 T (s) + \u03b1 4 \u2022 L(s)(15)\nThe sum runs over sentences and C, K, T and L output the sentence scores for each method (Cue, Key, Title and Location).\nICSI: (Gillick and Favre, 2009) A global linear optimization that extracts a summary by solving a maximum coverage problem of the most frequent bigrams in the source documents. ICSI has been among the best systems in a classical ROUGE evaluation (Hong et al., 2014).\nHere, the identification of the scoring function is trivial because it was originally formulated as an optimization task. If c i is the i-th bigram selected in the summary and w i is its weight computed from D, then:\n\u03b8 ICSI (S) = c i \u2208S c i \u2022 w i(16)\nLexRank: (Erkan and Radev, 2004) This is a well-known graph-based approach. A similarity graph G(V, E) is constructed where V is the set of sentences and an edge e ij is drawn between sentences v i and v j if and only if the cosine similarity between them is above a given threshold. Sentences are scored according to their PageRank score in G. Thus, \u03b8 LexRank is given by:\n\u03b8 LexRank (S) = s\u2208S P R G (s)(17)\nHere, P R is the PageRank score of sentence s.\nKL-Greedy: (Haghighi and Vanderwende, 2009) In this approach, the summary should minimize the Kullback-Leibler (KL) \nP X (w) represents the frequency of the word (or n-gram) w in the text X. The minus sign indicates that KL should be lower for better summaries. Indeed, we expect a good system summary to exhibit a similar probability distribution of n-grams as the sources.\nAlternatively, the Jensen-Shannon (JS) divergence can be used instead of KL. Let M be the average word frequency distribution of the candidate summary S and the source documents D distribution:\n\u2200g \u2208 S, P M (g) = 1 2 (P S (g) + P D (g)) (20)\nThen, the formula for JS is given by:\n\u03b8 JS (S) = \u2212JS(S||D) (21) = 1 2 (KL(S||M ) + KL(D||M )) (22)\nWithin our framework, the KL divergence acts as the unification of Relevance and Redundancy when semantic units are bigrams.", "publication_ref": ["b57", "b17", "b17", "b43", "b24", "b30", "b19", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "B Proof of Theorem 1", "text": "Let \u2126 be the set of semantic units. The notation \u03c9 i represents one unit. Let P T , and P K be the text representations of the source documents and background knowledge as probability distributions over semantic units. We note t i = P T (\u03c9 i ), the probability of the unit \u03c9 i in the source T . Similarly, we note k i = P K (\u03c9 i ). We seek a function f unifying T and K such that: f (\u03c9 i ) = f (t i , k i ).\nWe remind the simple requirements that f should satisfy:\n\u2022 Informativeness: \u2200i = j, if t i = t j and k i > k j then f (t i , k i ) < f (t j , k j )\n\u2022 Relevance: \u2200i = j, if t i > t j and k i = k j then f (t i , k i ) > f (t j , k j )\n\u2022 Additivity: I(f (t i , k i )) \u2261 \u03b1I(t i )+\u03b2I(k i ) (I is the information measure from Shannon's theory (Shannon, 1948))\n\u2022 Normalization:\ni f (t i , k i ) = 1\nTheorem 1 states that the functions satisfying the previous requirements are:\nP T K (\u03c9 i ) = 1 C \u2022 t \u03b1 i k \u03b2 i C = i t \u03b1 i k \u03b2 i , \u03b1, \u03b2 \u2208 R +(23)\nwith C the normalizing constant.\nProof. The information function defined by Shannon (1948) is the logarithm: I = log. Then, the Additivity criterion can be written: log(f (t i , k i )) = \u03b1 log(t i ) + \u03b2 log(k i ) + A (24)\nwith A a constant independent of t i and k i Since log is monotonous and increasing, the Informativeness and Additivity criteria can be combined: \u2200i = j, if t i = t j and k i > k j then: log f (t i , k i ) < log f (t j , k j ) \u03b1 log(t i ) + \u03b2 log(k i ) < \u03b1 log(t j ) + \u03b2 log(k j ) \u03b2 log(k i ) < \u03b2 log(k j ) But k i > k j , therefore: \u03b2 < 0\nFor clarity, we can now use \u2212\u03b2 with \u03b2 \u2208 R + . Similarly, we can combine the Relevance and Additivity criteria: \u2200i = j, if t i > t j and k i = k j then: log f (t i , k i ) > log f (t j , k j ) \u03b1 log(t i ) + \u03b2 log(k i ) > \u03b1 log(t j ) + \u03b2 log(k j ) \u03b1 log(t i ) > \u03b1 log(t j ) But t i > t j , therefore: \u03b1 > 0\nThen, we have the following form from the Additivity criterion:\nlog f (t i , k i ) = \u03b1 log(t i ) \u2212 \u03b2 log(k i ) + A f (t i , k i ) = e A e [\u03b1 log(t i )\u2212\u03b2 log(k i )] f (t i , k i ) = e A t \u03b1 i k \u03b2 i x\nFinally, the Normalization constraint specifies the constant e A :\nC = 1 e A and C = i t \u03b1 i k \u03b2 i then: A = \u2212 log( i t \u03b1 i k \u03b2 i )", "publication_ref": ["b61", "b61"], "figure_ref": [], "table_ref": []}, {"heading": "C Example", "text": "As an example, for one selected topic of TAC-2008 update track, we computed the P D K and compare it to the distribution of the 4 reference summaries.\nWe report the two distributions together in figure 2. For visibility, only the top 50 words according to P D K are considered. However, we observe a good match between the distribution of the reference summaries and the ideal distribution as defined by P D K . Furthermore, the most desired words according to P D K make sense. This can be seen by looking at one of the human-written reference summary of this topic: Reference summary for topic D0803 China sacrificed coal mine safety in its massive demand for energy. Gas explosions, flooding, fires, and cave-ins cause most accidents. The mining industry is riddled with corruption from mining officials to owners. Officials are often illegally invested in mines and ignore safety procedures for production. South Africa recently provided China with information on mining safety and technology during a conference. China is beginning enforcement of safety regulations. Over 12,000 mines have been ordered to suspend operations and 4,000 others ordered closed. This year 4,228 miners were killed in 2,337 coal mine accidents. China's mines are the most dangerous worldwide.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "This work was partly supported by the German Research Foundation (DFG) as part of the Research Training Group \"Adaptive Preparation of Information from Heterogeneous Sources\" (AIPHES) under grant No. GRK 1994/1, and via the German-Israeli Project Cooperation (DIP, grant No. GU 798/17-1). We also thank the anonymous reviewers for their comments.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Text Summarization Techniques: A Brief Survey", "journal": "International Journal of Advanced Computer Science and Applications", "year": "2017", "authors": "Mehdi Allahyari; Seyedamin Pouriyeh; Mehdi Assefi; Saeid Safaei; Elizabeth D Trippe; Juan B Gutierrez; Krys Kochut"}, {"ref_id": "b1", "title": "Towards a theory of semantic communication", "journal": "IEEE", "year": "2011", "authors": "Jie Bao; Prithwish Basu; Mike Dean; Craig Partridge; Ananthram Swami; Will Leland; James A Hendler"}, {"ref_id": "b2", "title": "Using Lexical Chains for Text Summarization", "journal": "", "year": "1999", "authors": "Regina Barzilay; Michael Elhadad"}, {"ref_id": "b3", "title": "Latent Dirichlet Allocation", "journal": "Journal of Machine Learning Research", "year": "2003", "authors": "David M Blei; Andrew Y Ng; Michael I Jordan"}, {"ref_id": "b4", "title": "The Use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries", "journal": "", "year": "1998", "authors": "Jaime Carbonell; Jade Goldstein"}, {"ref_id": "b5", "title": "An Outline of a Theory of Semantic Information", "journal": "British Journal for the Philosophy of Science", "year": "1953", "authors": "Rudolf Carnap; Yehoshua Bar-Hillel"}, {"ref_id": "b6", "title": "A Hybrid Hierarchical Model for Multi-Document Summarization", "journal": "Association for Computational Linguistics", "year": "2010", "authors": "Asli Celikyilmaz; Dilek Hakkani-Tur"}, {"ref_id": "b7", "title": "Improving the performance of the random walk model for answering complex questions", "journal": "Association for Computational Linguistics", "year": "2008", "authors": "Yllias Chali; R Shafiq;  Joty"}, {"ref_id": "b8", "title": "Topic-Focused Multi-Document Summarization Using an Approximate Oracle Score", "journal": "", "year": "2006", "authors": "John M Conroy; Judith D Schlesinger; Dianne P O'leary"}, {"ref_id": "b9", "title": "Lexical Semantics. Cambridge University Press", "journal": "", "year": "1986", "authors": "D A Cruse"}, {"ref_id": "b10", "title": "A Survey on Automatic Text Summarization. Literature Survey for the Language and Statistics II Course at CMU", "journal": "", "year": "2010", "authors": "Dipanjan Das; F T Andr\u00e9;  Martins"}, {"ref_id": "b11", "title": "A Noisychannel Model for Document Compression", "journal": "", "year": "2002", "authors": "Hal Daum\u00e9; , Iii; Daniel Marcu"}, {"ref_id": "b12", "title": "Bayesian Query-Focused Summarization", "journal": "Association for Computational Linguistics", "year": "2006", "authors": "Hal Daum\u00e9; Iii ; Daniel Marcu"}, {"ref_id": "b13", "title": "OCCAMS-An Optimal Combinatorial Covering Algorithm for Multi-document Summarization", "journal": "IEEE", "year": "2012", "authors": "T Sashka; John M Davis; Judith D Conroy;  Schlesinger"}, {"ref_id": "b14", "title": "Indexing by Latent Semantic Analysis", "journal": "", "year": "1990", "authors": "Scott Deerwester; Susan T Dumais; George W Furnas; Thomas K Landauer; Richard Harshman"}, {"ref_id": "b15", "title": "Du-alSum: A Topic-model Based Approach for Update Summarization", "journal": "Association for Computational Linguistics", "year": "2012", "authors": "Jean- ; Yves Delort; Enrique Alfonseca"}, {"ref_id": "b16", "title": "Accurate Methods for the Statistics of Surprise and Coincidence", "journal": "Computational linguistics", "year": "1993", "authors": "Ted Dunning"}, {"ref_id": "b17", "title": "New Methods in Automatic Extracting", "journal": "Journal of the Association for Computing Machinery", "year": "1969", "authors": "H P Edmundson"}, {"ref_id": "b18", "title": "What is Word Meaning, Really? (and How Can Distributional Models Help Us Describe It?", "journal": "", "year": "2010", "authors": "Katrin Erk"}, {"ref_id": "b19", "title": "LexRank: Graph-based Lexical Centrality As Salience in Text Summarization", "journal": "Journal of Artificial Intelligence Research", "year": "2004", "authors": "G\u00fcnes Erkan; Dragomir R Radev"}, {"ref_id": "b20", "title": "Frame Semantics And the Nature of Language", "journal": "Annals of the New York Academy of Sciences", "year": "1976", "authors": "Charles J Fillmore"}, {"ref_id": "b21", "title": "Philosophical Conceptions of Information", "journal": "Springer", "year": "2009", "authors": "Luciano Floridi"}, {"ref_id": "b22", "title": "Annals of discrete mathematics. Elsevier", "journal": "", "year": "2005", "authors": "Satoru Fujishige"}, {"ref_id": "b23", "title": "Exploring Vector Spaces for Semantic Relations", "journal": "", "year": "2017", "authors": "Kata G\u00e1bor; Haifa Zargayouna; Isabelle Tellier; Davide Buscaldi; Thierry Charnois"}, {"ref_id": "b24", "title": "A Scalable Global Model for Summarization", "journal": "", "year": "2009", "authors": "Dan Gillick;  Benoit Favre"}, {"ref_id": "b25", "title": "Dimensionality Reduction Aids Term Co-Occurrence Based Multi-Document Summarization", "journal": "Association for Computational Linguistics", "year": "2006", "authors": "Ben Hachey; Gabriel Murray; David Reitter"}, {"ref_id": "b26", "title": "Exploring Content Models for Multi-document Summarization", "journal": "", "year": "2009", "authors": "Aria Haghighi; Lucy Vanderwende"}, {"ref_id": "b27", "title": "Topic Themes for Multi-document Summarization", "journal": "", "year": "2005", "authors": "Sanda Harabagiu; Finley Lacatusu"}, {"ref_id": "b28", "title": "Distributional structure. Word", "journal": "", "year": "1954", "authors": "Zellig Harris"}, {"ref_id": "b29", "title": "Document Summarization Based on Data Reconstruction", "journal": "", "year": "2012", "authors": "Zhanying He; Chun Chen; Jiajun Bu; Can Wang; Lijun Zhang; Deng Cai; Xiaofei He"}, {"ref_id": "b30", "title": "A Repository of State of the Art and Competitive Baseline Summaries for Generic News Summarization", "journal": "", "year": "2014", "authors": "Kai Hong; John Conroy; Alex Benoit Favre; Hui Kulesza; Ani Lin;  Nenkova"}, {"ref_id": "b31", "title": "Information Theory and Statistical Mechanics", "journal": "Physical Review", "year": "1957", "authors": "Edwin T Jaynes"}, {"ref_id": "b32", "title": "Content Selection in Deep Learning Models of Summarization", "journal": "", "year": "2018", "authors": "Chris Kedzie; Kathleen Mckeown; Hal Daume; Iii "}, {"ref_id": "b33", "title": "On Information and Sufficiency. The Annals of Mathematical Statistics", "journal": "", "year": "1951", "authors": "Solomon Kullback; Richard A Leibler"}, {"ref_id": "b34", "title": "A generative theory of relevance", "journal": "Springer Science & Business Media", "year": "2008", "authors": "Victor Lavrenko"}, {"ref_id": "b35", "title": "Impact of Linguistic Analysis on the Semantic Graph Coverage and Learning of Document Extracts", "journal": "", "year": "2005", "authors": "Jure Leskovec; Natasa Milic-Frayling; Marko Grobelnik"}, {"ref_id": "b36", "title": "Reader-Aware Multi-document Summarization via Sparse Coding", "journal": "", "year": "2015", "authors": "Piji Li; Lidong Bing; Wai Lam; Hang Li; Yi Liao"}, {"ref_id": "b37", "title": "An Information-Theoretic Approach to Automatic Evaluation of Summaries", "journal": "", "year": "2006", "authors": "Chin-Yew Lin; Guihong Cao; Jianfeng Gao; Jian-Yun Nie"}, {"ref_id": "b38", "title": "Automatic Evaluation of Summaries Using N-gram Cooccurrence Statistics", "journal": "", "year": "2003", "authors": "Chin-Yew Lin; Eduard Hovy"}, {"ref_id": "b39", "title": "A Class of Submodular Functions for Document Summarization", "journal": "", "year": "2011", "authors": "Hui Lin; Jeff A Bilmes"}, {"ref_id": "b40", "title": "Multi-document Summarization Based on Two-level Sparse Representation Model", "journal": "", "year": "2015", "authors": "He Liu; Hongliang Yu; Zhi-Hong Deng"}, {"ref_id": "b41", "title": "A Bayesian Method to Incorporate Background Knowledge during Automatic Text Summarization", "journal": "Short Papers", "year": "2014", "authors": "Annie Louis"}, {"ref_id": "b42", "title": "Automatically Assessing Machine Summary Content Without a Gold Standard", "journal": "Computational Linguistics", "year": "2013", "authors": "Annie Louis; Ani Nenkova"}, {"ref_id": "b43", "title": "The Automatic Creation of Literature Abstracts", "journal": "IBM Journal of Research Development", "year": "1958", "authors": "Hans Peter Luhn"}, {"ref_id": "b44", "title": "An Unsupervised Multi-Document Summarization Framework Based on Neural Document Model", "journal": "", "year": "2016", "authors": "Shulei Ma; Zhi-Hong Deng; Yunlun Yang"}, {"ref_id": "b45", "title": "Advances in Automatic Text Summarization", "journal": "MIT Press", "year": "1999", "authors": "Inderjeet Mani"}, {"ref_id": "b46", "title": "Multidocument Summarization by Graph Search and Matching", "journal": "AAAI Press", "year": "1997", "authors": "Inderjeet Mani; Eric Bloedorn"}, {"ref_id": "b47", "title": "A Study of Global Inference Algorithms in Multi-document Summarization", "journal": "", "year": "2007", "authors": "Ryan Mcdonald"}, {"ref_id": "b48", "title": "Towards Multidocument Summarization by Reformulation: Progress and Prospects", "journal": "", "year": "1999", "authors": "Kathleen R Mckeown; Judith L Klavans; Vasileios Hatzivassiloglou"}, {"ref_id": "b49", "title": "Textrank: Bringing order into text", "journal": "", "year": "2004", "authors": "Rada Mihalcea; Paul Tarau"}, {"ref_id": "b50", "title": "Efficient Estimation of Word Representations in Vector Space", "journal": "CoRR", "year": "2013", "authors": "Tomas Mikolov; Kai Chen; Greg Corrado; Jeffrey Dean"}, {"ref_id": "b51", "title": "Distributed representations of words and phrases and their compositionality", "journal": "", "year": "2013", "authors": "Tomas Mikolov; Ilya Sutskever; Kai Chen; Greg S Corrado; Jeff Dean"}, {"ref_id": "b52", "title": "English as a formal language", "journal": "Edizioni di Communita", "year": "1970", "authors": "Richard Montague"}, {"ref_id": "b53", "title": "A Survey of Text Summarization Techniques. Mining Text Data", "journal": "", "year": "2012", "authors": "Ani Nenkova; Kathleen Mckeown"}, {"ref_id": "b54", "title": "The Pyramid Method: Incorporating Human Content Selection Variation in Summarization Evaluation", "journal": "ACM Transactions on Speech and Language Processing", "year": "2007", "authors": "Ani Nenkova; Rebecca Passonneau; Kathleen Mckeown"}, {"ref_id": "b55", "title": "A Compositional Context Sensitive Multi-document Summarizer: Exploring the Factors That Influence Summarization", "journal": "", "year": "2006", "authors": "Ani Nenkova; Lucy Vanderwende; Kathleen Mckeown"}, {"ref_id": "b56", "title": "A General Optimization Framework for Multi-Document Summarization Using Genetic Algorithms and Swarm Intelligence", "journal": "", "year": "2016", "authors": "Maxime Peyrard; Judith Eckle-Kohler"}, {"ref_id": "b57", "title": "A principled framework for evaluating summarizers: Comparing models of summary quality against human judgments", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Maxime Peyrard; Judith Eckle-Kohler"}, {"ref_id": "b58", "title": "Supervised learning of automatic pyramid for optimization-based multi-document summarization", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Maxime Peyrard; Judith Eckle-Kohler"}, {"ref_id": "b59", "title": "Objective function learning to match human judgements for optimization-based summarization", "journal": "", "year": "2018", "authors": "Maxime Peyrard; Iryna Gurevych"}, {"ref_id": "b60", "title": "Centroid-based Summarization of Multiple Documents: Sentence Extraction, Utility-based Evaluation, and User Studies", "journal": "", "year": "2000", "authors": "R Dragomir; Hongyan Radev; Malgorzata Jing;  Budzikowska"}, {"ref_id": "b61", "title": "A Mathematical Theory of Communication", "journal": "Bell Systems Technical Journal", "year": "1948", "authors": "Claude E Shannon"}, {"ref_id": "b62", "title": "Large-margin Learning of Submodular Summarization Models", "journal": "Association for Computational Linguistics", "year": "2012", "authors": "Ruben Sipos; Pannaga Shivaswamy; Thorsten Joachims"}, {"ref_id": "b63", "title": "A Statistical Interpretation of Term Specificity and its Application in Retrieval", "journal": "Journal of documentation", "year": "1972", "authors": "Karen Sparck Jones"}, {"ref_id": "b64", "title": "The KE Shannon and L. Floridi's Amount of Information", "journal": "Life Science Journal", "year": "2014", "authors": " Victor Yakovlevich Tsvetkov"}, {"ref_id": "b65", "title": "Word Representations: A Simple and General Method for Semi-supervised Learning", "journal": "", "year": "2010", "authors": "Joseph Turian; Lev Ratinov; Yoshua Bengio"}, {"ref_id": "b66", "title": "From Frequency to Meaning: Vector Space Models of Semantics", "journal": "Journal of artificial intelligence research", "year": "2010", "authors": "D Peter; Patrick Turney;  Pantel"}, {"ref_id": "b67", "title": "Beyond SumBasic: Taskfocused Summarization with Sentence Simplification and Lexical Expansion", "journal": "Information Processing & Management", "year": "2007", "authors": "Lucy Vanderwende; Hisami Suzuki; Chris Brockett; Ani Nenkova"}, {"ref_id": "b68", "title": "Improved Affinity Graph Based Multi-Document Summarization", "journal": "", "year": "2006", "authors": "Xiaojun Wan; Jianwu Yang"}, {"ref_id": "b69", "title": "Multi-document Summarization Using Sentence-based Topic Models", "journal": "", "year": "2009", "authors": "Dingding Wang; Shenghuo Zhu; Tao Li; Yihong Gong"}, {"ref_id": "b70", "title": "Recent Contributions to the Mathematical Theory of Communication", "journal": "ETC: A Review of General Semantics", "year": "1953", "authors": "Warren Weaver"}, {"ref_id": "b71", "title": "Relevance Theory, chapter 27", "journal": "John Wiley and Sons, Ltd", "year": "2008", "authors": "Deirdre Wilson; Dan Sperber"}, {"ref_id": "b72", "title": "Recent Advances in Document Summarization", "journal": "Knowledge and Information Systems", "year": "2017", "authors": "Jin-Ge Yao; Xiaojun Wan; Jianguo Xiao"}, {"ref_id": "b73", "title": "Extractive Summarization by Maximizing Semantic Volume", "journal": "", "year": "2015", "authors": "Dani Yogatama; Fei Liu; Noah A Smith"}, {"ref_id": "b74", "title": "Clustering Sentences with Density Peaks for Multi-document Summarization", "journal": "", "year": "2015", "authors": "Yang Zhang; Yunqing Xia; Yi Liu; Wenmin Wang"}, {"ref_id": "b75", "title": "A Theory of Semantic Information", "journal": "", "year": "2017", "authors": "Yixin Zhong"}, {"ref_id": "b76", "title": "Beyond Centrality and Structural Features: Learning Information Importance for Text Summarization", "journal": "", "year": "2016", "authors": "Markus Zopf; Eneldo Loza Menc\u00eda; Johannes F\u00fcrnkranz"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "\u03b8I (S, D, K) \u2261 \u2212Red(S) + \u03b1Rel(S, D) (11) + \u03b2Inf (S, K) (12) Equality holds up to a constant term log C independent from S. Maximizing \u03b8 I is equivalent to maximizing Relevance and Informativeness while minimizing Redundancy. Their relative strength are encoded by \u03b1 and \u03b2. Finally, H(S), CE(S, D) and CE(S, K) are the three independent components of Importance.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure1: figure1arepresents an example distribution of sources, figure1ban example distribution of background knowledge and figure1cis the resulting target distribution that summaries should approximate.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "divergence between the word distribution of the summary S and the word distribution of the documents D (i.e., \u03b8 KL = \u2212KL): \u03b8 KL (S) = \u2212KL(S||D)", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 2 :2Figure 2: Example of P D K in comparison to the word distribution of reference summaries for one topic of TAC-2008 (D0803).", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Correlation of various information-theoretic quantities with human judgments measured by Kendall's \u03c4 on generic and update summarization.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "H(S) = \u2212 \u03c9 i P S (\u03c9 i ) \u2022 log(P S (\u03c9 i )) (1)", "formula_coordinates": [3.0, 100.4, 92.81, 189.87, 22.37]}, {"formula_id": "formula_1", "formula_text": "Red(S) = H max \u2212 H(S)(2)", "formula_coordinates": [3.0, 123.69, 208.18, 166.58, 10.63]}, {"formula_id": "formula_2", "formula_text": "Rel(S, D) = \u03c9 i P S (\u03c9 i ) \u2022 log(P D (\u03c9 i )) (3)", "formula_coordinates": [3.0, 88.13, 639.25, 202.14, 22.37]}, {"formula_id": "formula_3", "formula_text": "KL(S||D) = CE(S, D) \u2212 H(S) \u2212KL(S||D) = Rel(S, D) \u2212 Red(S) (4)", "formula_coordinates": [3.0, 333.28, 104.17, 192.26, 26.11]}, {"formula_id": "formula_4", "formula_text": "Inf (S, K) = \u2212 \u03c9 i P S (\u03c9 i ) \u2022 log(P K (\u03c9 i )) (5)", "formula_coordinates": [4.0, 316.74, 259.84, 208.81, 22.37]}, {"formula_id": "formula_5", "formula_text": "P K (\u03c9 i ) is 1", "formula_coordinates": [4.0, 307.28, 427.9, 51.53, 12.97]}, {"formula_id": "formula_6", "formula_text": "\u2022 Informativeness: \u2200i = j, if d i = d j and k i > k j then f (d i , k i ) < f (d j , k j ) \u2022 Relevance: \u2200i = j, if d i > d j and k i = k j then f (d i , k i ) > f (d j , k j ) \u2022 Additivity: I(f (d i , k i )) \u2261 \u03b1I(d i ) + \u03b2I(k i )", "formula_coordinates": [5.0, 83.38, 683.72, 206.89, 82.43]}, {"formula_id": "formula_7", "formula_text": "i f (d i , k i ) = 1", "formula_coordinates": [5.0, 402.85, 102.08, 66.99, 18.81]}, {"formula_id": "formula_8", "formula_text": "P D K (\u03c9 i ) = 1 C \u2022 d \u03b1 i k \u03b2 i (6) C = i d \u03b1 i k \u03b2 i , \u03b1, \u03b2 \u2208 R + (7)", "formula_coordinates": [5.0, 347.02, 274.21, 178.52, 63.26]}, {"formula_id": "formula_9", "formula_text": "S * = argmax S \u03b8 I = argmin S KL(S||P D K ) (8)", "formula_coordinates": [5.0, 320.56, 562.08, 204.98, 20.88]}, {"formula_id": "formula_10", "formula_text": "\u03b8 I (S, D, K) = \u2212KL(P S , ||P D K )(9)", "formula_coordinates": [5.0, 344.94, 630.85, 180.61, 15.32]}, {"formula_id": "formula_11", "formula_text": "P D K", "formula_coordinates": [5.0, 307.28, 673.75, 14.02, 12.93]}, {"formula_id": "formula_12", "formula_text": "H D K = H(P D K )(10)", "formula_coordinates": [6.0, 147.34, 265.28, 142.93, 15.32]}, {"formula_id": "formula_13", "formula_text": "P I K (D) = \u2212 \u03c9 i P D (\u03c9 i ) \u2022 log(P K (\u03c9 i )) (13)", "formula_coordinates": [6.0, 317.95, 172.95, 207.59, 22.37]}, {"formula_id": "formula_14", "formula_text": "\u03b8 Edm. (S) = s\u2208S \u03b1 1 \u2022 C(s) + \u03b1 2 \u2022 K(s) (14) + \u03b1 3 \u2022 T (s) + \u03b1 4 \u2022 L(s)(15)", "formula_coordinates": [13.0, 85.68, 580.67, 204.59, 38.96]}, {"formula_id": "formula_15", "formula_text": "\u03b8 ICSI (S) = c i \u2208S c i \u2022 w i(16)", "formula_coordinates": [13.0, 364.5, 145.16, 161.04, 23.09]}, {"formula_id": "formula_16", "formula_text": "\u03b8 LexRank (S) = s\u2208S P R G (s)(17)", "formula_coordinates": [13.0, 353.32, 302.39, 172.22, 22.26]}, {"formula_id": "formula_18", "formula_text": "\u2200g \u2208 S, P M (g) = 1 2 (P S (g) + P D (g)) (20)", "formula_coordinates": [13.0, 322.83, 666.49, 202.71, 24.43]}, {"formula_id": "formula_19", "formula_text": "\u03b8 JS (S) = \u2212JS(S||D) (21) = 1 2 (KL(S||M ) + KL(D||M )) (22)", "formula_coordinates": [13.0, 316.0, 725.57, 209.55, 39.51]}, {"formula_id": "formula_20", "formula_text": "i f (t i , k i ) = 1", "formula_coordinates": [14.0, 167.57, 455.92, 65.25, 18.81]}, {"formula_id": "formula_21", "formula_text": "P T K (\u03c9 i ) = 1 C \u2022 t \u03b1 i k \u03b2 i C = i t \u03b1 i k \u03b2 i , \u03b1, \u03b2 \u2208 R +(23)", "formula_coordinates": [14.0, 126.2, 524.7, 164.07, 63.07]}, {"formula_id": "formula_22", "formula_text": "log f (t i , k i ) = \u03b1 log(t i ) \u2212 \u03b2 log(k i ) + A f (t i , k i ) = e A e [\u03b1 log(t i )\u2212\u03b2 log(k i )] f (t i , k i ) = e A t \u03b1 i k \u03b2 i x", "formula_coordinates": [14.0, 326.18, 395.54, 180.47, 63.25]}, {"formula_id": "formula_23", "formula_text": "C = 1 e A and C = i t \u03b1 i k \u03b2 i then: A = \u2212 log( i t \u03b1 i k \u03b2 i )", "formula_coordinates": [14.0, 360.18, 523.0, 112.45, 88.64]}], "doi": "10.14569/IJACSA.2017.081052"}