{"title": "Image-Based Rendering Using Image-Based Priors", "authors": "Andrew Fitzgibbon; Yonatan Wexler; Andrew Zisserman", "pub_date": "2005-02", "abstract": "Given a set of images acquired from known viewpoints, we describe a method for synthesizing the image which would be seen from a new viewpoint. In contrast to existing techniques, which explicitly reconstruct the 3D geometry of the scene, we transform the problem to the reconstruction of colour rather than depth. This retains the benefits of geometric constraints, but projects out the ambiguities in depth estimation which occur in textureless regions. On the other hand, regularization is still needed in order to generate high-quality images. The paper's second contribution is to constrain the generated views to lie in the space of images whose texture statistics are those of the input images. This amounts to an image-based prior on the reconstruction which regularizes the solution, yielding realistic synthetic views. Examples are given of new view generation for cameras interpolated between the acquisition viewpoints-which enables synthetic steadicam stabilization of a sequence with a high level of realism.", "sections": [{"heading": "Introduction", "text": "Given a small number of photographs of the same scene from several viewing positions, we want to synthesize the image which would be seen from a new viewpoint. This \"view synthesis\" (Fig. 1) problem has been widely researched in recent years. However, even the best methods do not yet produce images which look truly real. The primary source of error is in the tradeoff between the inherent ambiguity of the problem, and the loss of high-frequency detail due to the regularizations which must be applied to alleviate that ambiguity.\nIn this paper, we show how to constrain the generated images to have the same local statistics as natural images, effectively projecting the new view onto the space of real-world images. As this space is a small subspace of the space of all images, the result is strongly regularized synthetic views which preserve high-frequency details.\nStrategies for view synthesis are divided into those which explicitly compute a 3D representation of the scene, and those in which the computation of scene geometry is implicit. The first class includes texturemapped rendering of stereo reconstructions (Koch, 1995 ;Scharstein, 1999;Scharstein and Szeliski, 2002), volumetric techniques such as space carving (Broadhurst and Cipolla, 2001;Kutulakos and Seitz, 1999;Matusik et al., 2000;Seitz and Dyer, 1997;Wexler and Chellappa, 2001), and other volumetric approaches (Szeliski and Golland, 1998). Implicitgeometry techniques (Gortler et al., 1996;Levoy and Hanrahan, 1996;Matusik et al., 2002;McMillan and Bishop, 1995) assemble the pixels of the synthesized view from the rays sampled by the pixels of the input images. In a newly emergent class of technique, to which this paper is most closely related, viewdependent geometry (Debevec et al., 1996;Irani et al., 2000;Koch et al., 2001;Rademacher, 1999) is used to guide the selection of the colour at each pixel.\nWhat all these techniques have in common, whether based on lightfields or explicit 3D models, is that there is no free lunch: in order to generate a new ray which is not in the bundle one is given, one must solve a form of the stereo correspondence problem. This is a difficult inverse problem, which is poorly conditioned: for a given set of images, many different solutions will model the image data equally well. Thus, in order to select between the nearly equivalent solutions the prob-lem must be regularized by incorporating prior knowledge about the likely form of the solution. Previous work on new-view synthesis or stereo reconstruction has typically included such prior knowledge as a priori constraints on the (piecewise) smoothness of the 3D geometry, which results in artifacts at depth boundaries. In this paper, because the problem is expressed in terms of the reconstructed image rather than the reconstructed depth map, we can impose image-based priors, which can be learnt from natural images (Freeman and Pasztor, 1999;Grenander and Srivastava, 2001;Huang and Mumford, 1999;Srivastava et al., 2003).\nThe most relevant previous work is primarily in two areas: view-dependent geometry, and natural image statistics. Irani et al. (2002) expressed new view generation as the estimation of the colour at each generated pixel. Their representation implies, as does ours, a 3D geometry for the scene which is different for each synthetic viewpoint, and is thus related to view-dependent visual hull computation (Matusik et al., 2000;Wexler and Chellappa, 2001). As they note, this greatly improves the fidelity of the reconstructed image. However, it does not remove the fundamental ambiguity in the problem, which this paper directly addresses. In addition, their technique depends on the presence of a dominant plane in the scene, where this paper deals with the case of a general 3D scene with general camera motion.\nThe use of image-based priors to regularize hard inverse problems is inspired by Freeman and Pasztor's (1999) work on learning priors for Bayesian image reconstruction. Our texture representation, as a library of exemplar image patches, derives from this and from the recent tecture synthesis literature (Efros and Leung, 1999;Wei and Levoy, 2000). In this paper we extend these ideas to deal with the strongly multimodal data likelihoods present in the image-based rendering task, allowing the generation of new views which are locally similar to the input images, but globally consistent with the new viewpoint.", "publication_ref": ["b17", "b18", "b1", "b11", "b13", "b19", "b24", "b22", "b5", "b12", "b14", "b15", "b2", "b10", "b16", "b4", "b6", "b7", "b21", "b8", "b13", "b24", "b4", "b3", "b23"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Problem Statement", "text": "We are given a collection of n 2D images I 1 to I n , in which I i (x, y) is the colour at pixel (x, y) of the ith image. 1 Colour is expressed as a 3-vector in an appropriate colorspace. The images are taken by cameras in different positions represented by 3 \u00d7 4 projection matrices P 1 to P n , which are supplied. Figure 2 summarizes the situation. The projection matrix P projects homogeneous 3D points X to homogeneous 2D points x = \u03bb(x, y, 1) linearly: x = PX where the equality is up to scale. We denote by I i (X) the pixel in image i to which 3D point X projects, so\nI i (X) = I i (\u03c0(P i X)), \u03c0(x, y, w) = (x/w, y/w) (1)\nEpipolar lines: Projections of ray X(z) The stack of epipolar lines is C (i,z)\nInput images I1 I2 I3 3D Object\nView to be synthesized Pixel (x,y) to be generated, with colour V(x,y) 3 D R a y X (z )\nFigure 2. Geometric configuration. The supplied information is a set of 2D images I 1..n and their camera positions P 1..n . At each pixel in the view to be synthesized, we wish to discover the colour which is most likely to be a reprojection of a 3D object point, based on the implied projection into the source images.\nThe task of virtual view synthesis is to generate the image which would be seen by a virtual camera in a position not in the original set. Specifically, we wish to compute, for each pixel V (x, y) in a virtual image V the colour which that pixel would observe if a real camera were placed at the new location. We assume we are dealing with diffuse, opaque objects, and that any deviations from this assumption may be considered part of imaging noise. The extensions to more general lighting assumptions are exactly those in space carving (Kutulakos and Seitz, 1999), and will not be dealt with here. The objective of this work is to infer the most likely rendered view V given the set of input images I 1 , . . . , I n . In a Bayesian framework, we wish to choose the synthesised view V which maximizes the posterior p(V | I 1 , . . . , I n ). Bayes' rule allows us to write this as\np(V | I 1 , . . . , I n ) = p(I 1 , . . . , I n | V) p(V) p(I 1 , . . . , I n ) (2)\nwhere p(V) is the prior on V, and the data term p(I 1 , . . . , I n | V) measures the likelihood that the observed images could have been observed if V were the true colours at the novel viewpoint. Because we shall maximize this posterior over V, we need not compute the denominator p(I 1 , . . . , I n ), and will instead optimize the function\nq(V) = p(I 1 , . . . , I n | V) p(V) (3)\nThis likelihood has two parts: the photoconsistency likelihood p(I 1 , . . . , I n | V) and the prior p(V) which we shall call p texture (V).", "publication_ref": ["b11"], "figure_ref": [], "table_ref": []}, {"heading": "Photoconsistency Constraint", "text": "The colour consistency constraint we employ is standard in the stereo and space-carving literature. We consider each pixel V (x, y) in the synthesised view separately, so the likelihood is written as the product of per-pixel likelihoods\np(I 1 , . . . , I n | V) = (x,y) p(I 1 , .., I n | V (x, y)) (4)\nConsider the generation of new-view pixel V (x, y). This is a sample from along the ray emanating from the camera centre, which we may assume to be the origin. Let the direction of this ray be denoted d(x, y). It can be computed easily given the calibration parameters of the virtual camera. Let a 3D point along the ray be given by the function X(z) = zd(x, y) where z ranges between preset values z min and z max . For a given depth z, we can compute using (1) the set of pixels to which X(z) projects in the images I 1..n . Denote the colours of those pixels by the function\nC(i, z) = I i (X(z)). (5\n)\nLet the set of all colours at a given z value be written\nC(:, z) = {C(i, z)} n i=1 , (6\n)\nand the set, C, of all samples-at location (x, y)-be\nC = {C(i, z) | 1 \u2264 i \u2264 n, z min < z < z max }. (7)\nFigure 3 shows an example of C at one pixel in a real sequence. Because the input-image pixels whose colours form C are the only pixels which influence new-view pixel (x, y), the photoconsistency likelihood further simplifies to (writing V for V (x, y))\np(I 1 , . . . , I n | V ) = p(C | V ) (8)\nNow, by making explicit the dependence on the depth z and marginalizing (assuming p(z | V ) is uniform), we obtain\np(C | V ) = p(C | V, z) dz = p(C(:, z) | V, z) dz (9)\nThe noise on the input image colours C(i, z) will be modelled as being drawn from distributions with density functions of the form exp(\u2212\u03b2\u03c1(t)), centred at V , where \u03b2 is a constant specifying the width of the distribution. Thus the likelihood is of the form\np(C(:, z) | V, z) = n i=1 exp \u2212\u03b2\u03c1( V \u2212 C(i, z) ) (10)\nThe function \u03c1 is a robust kernel, and in this work is generally the absolute distance \u03c1(x) = |x|, corresponding to an exponential distribution on the pixel intensities. In situations (discussed later) where a Gaussian distribution is more appropriate, the kernel becomes \u03c1(x) = x 2 .  (i, z) where the frame number i varies along the vertical axis, and the depth samples z vary along the horizontal. Equivalently, row i of this image is the intensity along the epipolar line generated by x in image i. Below are shown photoconsistency likelihoods p(C | V, z) for two values of the colour V (backgrounds to the plots). As this pixel is a co-location of background and foreground, these two colours form modes of p(C | V ) when z is maximized. This multi-modality is the essence of the ambiguity in new-view synthesis, which prior knowledge must remove.\nIn order to choose the colour V , we shall be computing (Section 3.1) the modes of the function p(C(:, z) | V (x, y)). As defined above, this requires the computation of the integral (9), which is computationally undemanding. However, because the value of \u03b2 is difficult to know, and because the function is sensitive to its value, the integral must also be over a hyperprior on \u03b2, rendering it much more challenging. Approximating the marginal by the maximum gives us an approximation, denoted p photo ,  which avoids both of these problems. In the implementation, the maximum over z is computed by explicitly sampling, typically using 500 values. Figure 4 shows a plot of p(C(:, z) | V, z) for grayscale C at a typical pixel. Figure 5 shows isosurface plots of p photo (V ) in RGB space for the same pixel.\np photo (V (x, y)) \u2248 max z p(C(:, z) | V, z) (11)", "publication_ref": [], "figure_ref": ["fig_1", "fig_2", "fig_3"], "table_ref": []}, {"heading": "Incorporating the Texture Prior", "text": "The function p photo (V ) will generally be multimodal, due firstly to physical factors such as occlusion and partial pixel effects and secondly to deficiencies in the image-formation model, such as not modelling specular reflections or having an inaccurate model of imaging noise. Thus the data likelihood at the true colour may often be lower than the likelihood at other, spurious values. Consequently, selecting the maximum-likelihood V at each pixel yields images with significant artefacts, such as those shown in Fig. 1(c). We would like to constrain the generated views to lie in the space of real images by imposing a prior on the possible generated images. Defining such a prior is in the domain of the analysis of natural image statistics, an active area of recent neurophysiological and machine learning research (Grenander and Srivastava, 2001;Huang and Mumford, 1999;Srivastava et al., 2003). Because it has been observed that correlation between pixels falls off quickly as a function of distance, we can make the assumption that the probability density can be written as a product of functions operating on small neighborhoods. Let the generated image V have pixels V (x, y). Then the prior has the form\np texture (V) = x,y p texture (N (x, y)) (12\n)\nwhere the function N (x, y) is the set of colours of neighbours of (x, y). Here we use 5 \u00d7 5 neighbourhoods, so\nN (x, y) = {V (x + i, y + j) | \u22122 \u2264 i, j \u2264 2}. (13\n)\nAs the form of p texture is typically very difficult to represent analytically (Huang and Mumford, 1999), we follow (Efros and Leung, 1999;Freeman and Pasztor, 1999) and represent our texture prior as a library of texture patches. The likelihood of a particular neighbourhood is measured by computing its distance to the closest database patch. Thus, we are given a texture database of 5 \u00d7 5 image patches, denoted T = {T 1 , ..., T N } where N is typically extremely large. The definition of p texture is then\np texture (N (x, y)) = exp \u2212\u03bb min T \u2208T T \u2212 N (x, y) 2\nwhere \u03bb is a tuning parameter. This is a closest-point problem in the set of 75-d points (75 = 5 \u00d7 5 \u00d7 3) in T and may be efficiently solved using a variety of algorithms, for example vector quantization and BSP tree indexing (Wei and Levoy, 2000).", "publication_ref": ["b6", "b7", "b21", "b7", "b3", "b4", "b23"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Combining Photoconsistency and Texture", "text": "Finally, combining the data and prior terms, we have the expression for the quasi-likelihood q(V) =\nx,y p photo (V (x, y)) p texture (N (x, y)).\nIn the implementation, we minimize the negative log of q, yielding the energy formulation\nE(V) = x,y E photo (V (x, y)) + x,y E texture (N (x, y)) (14\n)\nwhere E photo measures the deviation from photoconsistency at pixel (x, y) and E texture measures the a-priori likelihood of the texture patch surrounding (x, y). From ( 11), the definition of E photo at a pixel (x, y) with 3D ray X(z) is\nE photo (V ) = min z min <z<z max n i=1 \u03c1( V \u2212 I i (X(z)) ) (15)\nThe texture energy is the negative log of p texture , giving\nE texture (N (x, y)) = \u03bb min T \u2208T T \u2212 N (x, y) 2 (16)\nThe view synthesis problem is now one of minimization of E over the space of images. This is a difficult global optimization problem, and making it tractable is the subject of the next section.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Implementation", "text": "The optimization of the energy defined above could be directly attempted using a global optimization strategy such as simulated annealing. However, both the prior and the data term E photo are expensive to evaluate, with multiple local minima at each pixel, meaning that attaining a global optimum will be difficult, and certainly time consuming. To render the optimization tractable, we exploit the simplification of the energy function conferred by estimating colour rather than depth. That is, we compute the set of modes of the photo-consistency term for each pixel, and restrict the solution for that pixel to this set. Then the texture prior is used to select the values from this set. This reduces the problem from a search over a high-dimensional space to an enumeration of the possible combinations.\nAlthough the data likelihood p(C | V ) is multimodal, there are typically far fewer modes than there are maxima of p(C(:, z) | V, z) over depth, so we can hope to explicitly compute the modes of p(C | V ) as the first step. This means that the optimization becomes a discrete labelling problem, which although still complex, can be analysed much more efficiently.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Enumerating the Minima of E photo (V )", "text": "The goal then is to generate a list of plausible colours for each rendered pixel V (x, y). One option would be to sample from p photo (V ) using MCMC, but this is computationally unattractive. A more practical alternative is to find all local minima of the energy function E photo (V ).\nOn the face of it, this seems a tall order, but as Fig. 5 indicates, there are typically few minima in a generally well-behaved space. Inspection of several such plots on a number of scenes suggests that this behaviour is typical. Finding all local minima of such functions is task for which several strategies have emerged from the computational chemistry community, and have been introduced to computer vision by Sminchisescu and Triggs (2002). The most expensive is to densely sample the space of V (here 3D RGB space), and this is the strategy used to obtain the isosurface plot shown in Fig. 5. A more efficient strategy to isolate the minima is to start gradient descent from several randomly chosen starting points, and iterate until local minima are found. Finally clustering on the locations of the minima produces a set of distinct colours which are likely at that pixel. On the images we have tested, 12 steps of gradient descent on each of 20 random starting colours V takes a total of about 0.1 seconds in Matlab, and produces between four and six colour hypotheses at each pixel.", "publication_ref": ["b20"], "figure_ref": ["fig_3", "fig_3"], "table_ref": []}, {"heading": "Texture Reference and Rectification", "text": "The second implementation issue is the source of reference textures. To build a general tool for projection of images onto natural images, a large database of images of natural scenes would be the ideal choice. In this case, however, we are operating in a limited problem domain. We expect that the newly synthesized views will be similar locally to the input views with which the algorithm is provided. Therefore, the texture library is built of patches from the input images. This provides excellent performance with a small library, and the pho-toconsistency term means that the system cannot \"overlearn\" by simply copying large patches from the nearest source image to the newly rendered view. For speed, we can also use the known z range to limit the search for matching texture windows in source image I i to the bounding box of {P i X(z) | z min < z < z max }.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Optimization", "text": "Given the modes of the photoconsistency distribution at each pixel, the optimization of ( 14) becomes a labelling problem. Each pixel is associated with an integer label l(x, y), which indicates which mode of the distribution will be used to colour that pixel, with a corresponding photoconsistency cost which is precomputed. This significantly reduces the cost of function evaluations, but the optimization is still a computationally challenging problem. For this work, we have implemented a variant of the iterated conditional modes (ICM) algorithm (Besag, 1986), alternately optimizing the photoconsistency and texture priors. The algorithm begins by selecting, for each pixel, the most likely mode of the photoconsistency function, yielding an initial estimate V 0 . Then, at each ICM iteration, each pixel is varied until the 5 \u00d7 5 window surrounding it minimizes the sum E photo + E texture at that pixel. This optimization is potentially extremely expensive, implying the evaluation of E photo (V ) for the value V in the centre of each texture patch T . However, because the minima of E photo are available, a fast approximation is obtained simply by writing E photo (V ) \u2248 V \u2212 V r \u22121 2 , where V r \u22121 is the colour obtained at the previous iteration. If all other pixels in V are fixed, the task is to choose V to minimize E photo + \u03bbE texture , approximated by (using ( 16))\nV r = argmin V min T \u2208T ( V \u2212 V r \u22121 2 + \u03bb T \u2212 N (V ) 2 ) (17\n)\nwhere N (V ) is the image neighbourhood around V . Splitting the second term into a contribution from the centre pixel of T and the remainder of the neighbourhood, it can be shown that this amounts to setting the centre pixel to a linear combination of (a) the photoconsistency mode, and (b) the value that would be predicted by sampling-based texture synthesis. If V r \u22121 is the value predicted by photoconsistency at the previous iteration, and T is the value at the centre pixel of the best matching texture patch T , then the pixel should be replaced by\nV r = V r \u22121 + \u03bbT 1 + \u03bb (18)\nFinally, replacing V r by the closest mode at each iteration ensures that the synthesized colour is always a subset of the photoconsistency minima. Note that this does not undo the good work of the robust kernel in computing the modes of E photo , but allows the texture prior to efficiently select between the robustlycomputed colour candidates at each pixel. This also prevents the algorithm from copying large sections of the texture source. Figure 6 summarizes the steps in the algorithm.", "publication_ref": ["b0"], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Choice of Robust Kernels", "text": "In the preceding, the choice of robust kernels for the photoconsistency likelihood has been mentioned several times. In practice, there is a significant tradeoff between speed and accuracy implied by choosing other than the squared-error kernel \u03c1(x) = x 2 kernel, as the mode computation can be significantly optimized for the squared-error case. The problem arises when there is significant occlusion in the sequence, as on the example pixel in Fig. 3, and it becomes necessary to produce a view which looks \"behind\" the foreground pixel. Using the squared-error kernel, the true colour (in this case, black) is not a minimum of E photo , because the column C(:, z) at the depth corresponding to the background contains some white pixels which are significant outliers to the Gaussian distribution exp(\u2212\u03c1(\u2022)).\nThe true colour is a minimum using the absolute distance \u03c1(x) = |x| or Huber kernels, which are less sensitive to such outliers. To provide a rule of thumb, the squared-error kernel is fast, and works well for interpolation, but the absolute distance kernel is needed for extrapolation.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Examples", "text": "Image sequences were captured using a hand-held camera, and the sequences were calibrated using   commercially available camera tracking software (2d3 Ltd. http://www. 2d3.com, 2002). A number of examples of the algorithm performance were produced. Single still frames are reproduced here, and complete MPEG sequences may be found at http://www.robots.ox.ac.uk/\u223cawf/ibr. The first experiment is a leave-one-out test, so that the recovered images can be compared against ground truth. Each frame of the 27-frame \"monkey\" sequence was reconstructed based on the other 26 frames. Figure 7 shows the results for a typical frame, comparing the ground truth image first to the synthesized view using photoconsistency alone, and then to the result guided by the texture prior. Visually, the fidelity is high, and the image is free of the high-frequency artifacts which the photoconsistency-maximizing view exhibits. Artifacts do occur in the background visible under the monkey's arm, where few of the source views have observed the background, meaning it does not appear as a mode of the photoconsistency distribution. The difference image in Fig. 7(d) is simply the length of the RGB difference vector at each pixel, but shows that the texture prior does not bias the generated view, for example by copying one of the texture sources.\nThe second example shows performance on a \"steadicam\" task, where the scene is re-rendered at a set of viewpoints which smoothly interpolate the first and last camera position and orientation. The reader is encouraged to consult the videos on the webpage above to confirm the absence of artifacts, and the subtle movements of the partial occlusions at the boundaries.\nFigure 1 shows example images from one sequence and illustrates the improvement obtained. The erroneous areas surrounding the ear are removed, while the remainder of the image retains its (correct) solution. At high magnification, it is in fact possible to see that the optimized solution has added back some highfrequency detail in the image. This is because the local statistics of the texture library are being applied to the rendered view.", "publication_ref": [], "figure_ref": ["fig_5", "fig_5", "fig_0"], "table_ref": []}, {"heading": "Conclusion", "text": "This paper has shown that view synthesis problems can be regularized using texture priors. This is in contrast to the depth-based priors that previous algorithms have used. Image-based priors have several advantages over the depth-based ones. First, depth priors are difficult to learn from real images, so artificial approximations are used. These approximations are equivalent to assuming very simple models of the world-for example, that it is piecewise planar-and thus introduce artifacts into the generated views. In contrast, image-based priors are easy to obtain from the world. If the problem domain is restricted, as it is here, a small number of images can be used to regularize the solution to a complex inverse problem.\nThere are many areas for further work: (1) imagebased priors as implemented here are expensive to evaluate. For a typical depth prior, evaluation of the prior in a pixel neighbourhood requires computation of the order of a few machine instructions. As image-based priors are stored in large lookup tables, the cost of evaluating them is many times higher. (2) In this paper, only one optimization strategy was investigated. It is hoped that examination of other strategies will lead to significantly quicker solutions. (3) Occlusion is handled here by the robust kernel \u03c1. More geometric handling of occlusion, analogous to space carving's improvement over voxel colouring, ought to yield better results. (4) When rendering sequences of images, it is valuable to impose temporal continuity from frame to frame. This paper has not addressed this issue, so the rendered sequences show some flicker. On the other hand this does allow the stability of the per-frame solutions to be evaluated.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "Funding for this work was provided by the DTI/EPSRC Link grant V2I. Fitzgibbon would like to thank the Royal Society for its generous support. Matrices are in fixed-width font, viz M.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "On the statistical analysis of dirty pictures", "journal": "J. Royal Stat. Soc. B", "year": "1986", "authors": "J Besag"}, {"ref_id": "b1", "title": "A statistical consistency check for the space carving algorithm", "journal": "", "year": "2001", "authors": "A Broadhurst; R Cipolla"}, {"ref_id": "b2", "title": "Modeling and rendering architecture from photographs: A hybrid geometry-and image-based approach", "journal": "", "year": "1996", "authors": "P E Debevec; C J Taylor; J Malik"}, {"ref_id": "b3", "title": "Texture synthesis by non-parametric sampling", "journal": "", "year": "1999", "authors": "A Efros; T Leung"}, {"ref_id": "b4", "title": "Learning low-level vision", "journal": "", "year": "1999", "authors": "W Freeman; E Pasztor"}, {"ref_id": "b5", "title": "The lumigraph", "journal": "", "year": "1996", "authors": "S J Gortler; R Grzeszczuk; R Szeliski; M F Cohen"}, {"ref_id": "b6", "title": "Probability models for clutter in natural images", "journal": "IEEE PAMI", "year": "2001", "authors": "U Grenander; A Srivastava"}, {"ref_id": "b7", "title": "Statistics of natural images and models", "journal": "", "year": "1999", "authors": "J Huang; D Mumford"}, {"ref_id": "b8", "title": "What does the scene look like from a scene point?", "journal": "", "year": "2002", "authors": "M Irani; T Hassner; Anandan ; P "}, {"ref_id": "b9", "title": "3D surface reconstruction from stereoscopic image sequences", "journal": "", "year": "1995", "authors": "R Koch"}, {"ref_id": "b10", "title": "Image-based rendering from uncalibrated light-fields with scalable geometry", "journal": "Springer LNCS 2032", "year": "2001", "authors": "R Koch; B Heigl;  Pollefeys; ; R Marc; T Klette;  Huang"}, {"ref_id": "b11", "title": "A theory of shape by space carving", "journal": "", "year": "1999", "authors": "K Kutulakos; S Seitz"}, {"ref_id": "b12", "title": "Light field rendering", "journal": "", "year": "1996", "authors": "M Levoy; P Hanrahan"}, {"ref_id": "b13", "title": "Image-based visual hulls", "journal": "", "year": "2000", "authors": "W Matusik; C Buehler; R Raskar; L Mcmillan; S Gortler"}, {"ref_id": "b14", "title": "Image-based 3d photography using opacity hulls", "journal": "", "year": "2002", "authors": "W Matusik; H Pfister; P A Beardsley; A Ngan; R Ziegler; L Mcmillan"}, {"ref_id": "b15", "title": "Plenoptic modeling: An imagebased rendering system", "journal": "", "year": "1995", "authors": "L Mcmillan; G Bishop"}, {"ref_id": "b16", "title": "View-dependent geometry", "journal": "", "year": "1999", "authors": "P Rademacher"}, {"ref_id": "b17", "title": "View Synthesis Using Stereo Vision", "journal": "Springer-Verlag", "year": "1999", "authors": "D Scharstein"}, {"ref_id": "b18", "title": "A taxonomy and evaluation of dense two-frame stereo correspondence algorithms", "journal": "IJCV", "year": "2002", "authors": "D Scharstein; R Szeliski"}, {"ref_id": "b19", "title": "Photorealistic scene reconstruction by voxel coloring", "journal": "", "year": "1997", "authors": "S M Seitz; C R Dyer"}, {"ref_id": "b20", "title": "Building roadmaps of local minima of visual models", "journal": "", "year": "2002", "authors": "C Sminchisescu; B Triggs"}, {"ref_id": "b21", "title": "On advances in statistical modeling of natural images", "journal": "", "year": "2003", "authors": "A Srivastava; A Lee; E Simoncelli; S Zhu"}, {"ref_id": "b22", "title": "Stereo matching with transparency and matting", "journal": "", "year": "1998", "authors": "R Szeliski; P Golland"}, {"ref_id": "b23", "title": "Fast texture synthesis using tree-structured vector quantization", "journal": "", "year": "2000", "authors": "L.-Y Wei; M Levoy"}, {"ref_id": "b24", "title": "View synthesis using convex and visual hulls", "journal": "", "year": "2001", "authors": "Y Wexler; R Chellappa"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. View synthesis. (a, b): Two from a set of 39 images taken by a hand-held camera. (c): Detail from a new view generated using state-of-the-art view synthesis. The new view is about 20 \u2022 displaced from the closest view in the original sequence. Note the spurious echo of the ear. (d): The same detail, but constrained to only generate views which have similar local statistics to the input images.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 .3Figure 3. Photoconsistency. One image is shown from a sequence of 27 captured by a hand-held camera. The circled pixel x's photoconsistency with respect to the other 26 images is illustrated in (a). The upper image in (b) shows the reprojected colours C(:, z) as columns of 26 colour samples, at each of 500 depth samples. The colours are the samples C(i, z)  where the frame number i varies along the vertical axis, and the depth samples z vary along the horizontal. Equivalently, row i of this image is the intensity along the epipolar line generated by x in image i. Below are shown photoconsistency likelihoods p(C | V, z) for two values of the colour V (backgrounds to the plots). As this pixel is a co-location of background and foreground, these two colours form modes of p(C | V ) when z is maximized. This multi-modality is the essence of the ambiguity in new-view synthesis, which prior knowledge must remove.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 .4Figure 4. The function p(C(:, z) | V, z) plotted for the pixel studied in Fig.3, with grayscale images, so V is a scalar, and \u03c1(x) = |x|. The projected graphs show the marginals (blue) and the maxima (red). The marginalization over colour (V ) has fewer minima than that over z, and the two modes corresponding to foreground and background are clearly seen.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 .5Figure 5. Minima of E photo . (a) Isosurfaces in RGB space of the photoconsistency function E photo (V ) at the pixel studied in Fig. 3. Minima are computed by gradient descent from random starting positions, of which twelve are shown (black circles), with the gradient descent trajectories plotted in black. Four modes were retained after clustering; their locations are marked by white 3D \"axes\" lines in (a), and their RGB colours are shown in (b).", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 6 .6Figure6. Pseudocode for iterative computation of new view V. The preprocessing is expensive (about 0.1 sec/pixel), the iterations cost as much as patch-based texture synthesis.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 7 .7Figure 7. Leave-one-out test. Using 26 views to render a missing view allows comparison to be made between the rendered view and ground truth. (a) Maximum-likelihood view, in which each pixel is coloured according to the highest mode of the photoconsistency function. High-frequency artifacts are visible throughout the scene. (b) View synthesized using texture prior. The artifacts are significantly reduced. (c) Ground-truth view. (d) Difference image between (b) and (c).", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 8 .8Figure 8. Steadicam test. Three novel views of the monkey scene from viewpoints not in the original sequence. The complete sequence may be found at http://www.robots.ox.ac.uk/\u223cawf/ibr.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 9 .9Figure9. 3D composite from 2D images. The camera motion from the live-action background plate is applied to the head sequence, rendering new views of the face.", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 10 .10Figure10. Tsukuba. Fine details such as the lamp arm are retained, but some ghosting is evident around the top of the lamp.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "I i (X) = I i (\u03c0(P i X)), \u03c0(x, y, w) = (x/w, y/w) (1)", "formula_coordinates": [3.0, 80.31, 512.77, 205.14, 21.59]}, {"formula_id": "formula_1", "formula_text": "Input images I1 I2 I3 3D Object", "formula_coordinates": [3.0, 138.58, 559.51, 110.08, 96.36]}, {"formula_id": "formula_2", "formula_text": "p(V | I 1 , . . . , I n ) = p(I 1 , . . . , I n | V) p(V) p(I 1 , . . . , I n ) (2)", "formula_coordinates": [3.0, 325.54, 349.36, 199.02, 24.54]}, {"formula_id": "formula_3", "formula_text": "q(V) = p(I 1 , . . . , I n | V) p(V) (3)", "formula_coordinates": [3.0, 353.17, 479.97, 171.39, 10.48]}, {"formula_id": "formula_4", "formula_text": "p(I 1 , . . . , I n | V) = (x,y) p(I 1 , .., I n | V (x, y)) (4)", "formula_coordinates": [3.0, 316.37, 653.52, 208.19, 19.93]}, {"formula_id": "formula_5", "formula_text": "C(i, z) = I i (X(z)). (5", "formula_coordinates": [4.0, 137.01, 222.81, 144.57, 10.48]}, {"formula_id": "formula_6", "formula_text": ")", "formula_coordinates": [4.0, 281.58, 223.48, 3.87, 8.97]}, {"formula_id": "formula_7", "formula_text": "C(:, z) = {C(i, z)} n i=1 , (6", "formula_coordinates": [4.0, 129.48, 268.98, 152.11, 13.07]}, {"formula_id": "formula_8", "formula_text": ")", "formula_coordinates": [4.0, 281.58, 271.05, 3.87, 8.97]}, {"formula_id": "formula_9", "formula_text": "C = {C(i, z) | 1 \u2264 i \u2264 n, z min < z < z max }. (7)", "formula_coordinates": [4.0, 87.78, 317.95, 197.68, 10.48]}, {"formula_id": "formula_10", "formula_text": "p(I 1 , . . . , I n | V ) = p(C | V ) (8)", "formula_coordinates": [4.0, 115.03, 413.35, 170.42, 10.48]}, {"formula_id": "formula_11", "formula_text": "p(C | V ) = p(C | V, z) dz = p(C(:, z) | V, z) dz (9)", "formula_coordinates": [4.0, 110.12, 486.43, 175.34, 36.08]}, {"formula_id": "formula_12", "formula_text": "p(C(:, z) | V, z) = n i=1 exp \u2212\u03b2\u03c1( V \u2212 C(i, z) ) (10)", "formula_coordinates": [4.0, 80.88, 606.3, 204.57, 41.33]}, {"formula_id": "formula_13", "formula_text": "p photo (V (x, y)) \u2248 max z p(C(:, z) | V, z) (11)", "formula_coordinates": [4.0, 337.12, 718.46, 187.44, 14.96]}, {"formula_id": "formula_14", "formula_text": "p texture (V) = x,y p texture (N (x, y)) (12", "formula_coordinates": [6.0, 110.02, 371.13, 171.29, 19.45]}, {"formula_id": "formula_15", "formula_text": ")", "formula_coordinates": [6.0, 281.31, 371.8, 4.15, 8.97]}, {"formula_id": "formula_16", "formula_text": "N (x, y) = {V (x + i, y + j) | \u22122 \u2264 i, j \u2264 2}. (13", "formula_coordinates": [6.0, 82.65, 460.16, 198.66, 25.57]}, {"formula_id": "formula_17", "formula_text": ")", "formula_coordinates": [6.0, 281.31, 476.77, 4.15, 8.97]}, {"formula_id": "formula_18", "formula_text": "p texture (N (x, y)) = exp \u2212\u03bb min T \u2208T T \u2212 N (x, y) 2", "formula_coordinates": [6.0, 74.27, 641.1, 196.42, 17.2]}, {"formula_id": "formula_19", "formula_text": "E(V) = x,y E photo (V (x, y)) + x,y E texture (N (x, y)) (14", "formula_coordinates": [6.0, 315.13, 256.93, 205.28, 30.98]}, {"formula_id": "formula_20", "formula_text": ")", "formula_coordinates": [6.0, 520.41, 278.94, 4.15, 8.97]}, {"formula_id": "formula_21", "formula_text": "E photo (V ) = min z min <z<z max n i=1 \u03c1( V \u2212 I i (X(z)) ) (15)", "formula_coordinates": [6.0, 318.71, 371.02, 205.85, 28.79]}, {"formula_id": "formula_22", "formula_text": "E texture (N (x, y)) = \u03bb min T \u2208T T \u2212 N (x, y) 2 (16)", "formula_coordinates": [6.0, 329.66, 434.78, 194.9, 17.2]}, {"formula_id": "formula_23", "formula_text": "V r = argmin V min T \u2208T ( V \u2212 V r \u22121 2 + \u03bb T \u2212 N (V ) 2 ) (17", "formula_coordinates": [7.0, 310.64, 561.73, 209.77, 32.01]}, {"formula_id": "formula_24", "formula_text": ")", "formula_coordinates": [7.0, 520.41, 584.77, 4.15, 8.97]}, {"formula_id": "formula_25", "formula_text": "V r = V r \u22121 + \u03bbT 1 + \u03bb (18)", "formula_coordinates": [8.0, 141.72, 483.68, 143.74, 25.39]}], "doi": ""}