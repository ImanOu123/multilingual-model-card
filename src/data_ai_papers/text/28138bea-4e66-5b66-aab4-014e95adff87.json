{"title": "Extrinsic Evaluation of Machine Translation Metrics", "authors": "Nikita Moghe; Tom Sherborne; Mark Steedman; Alexandra Birch", "pub_date": "", "abstract": "Automatic machine translation (MT) metrics are widely used to distinguish the quality of machine translation systems across large test sets (i.e., system-level evaluation). However, it is unclear if automatic metrics can reliably distinguish good translations from bad at the sentence level (i.e., segment-level evaluation). We investigate how useful MT metrics are at detecting segment-level quality by correlating metrics with the translation utility for downstream tasks. We evaluate the segment-level performance of widespread MT metrics (chrF, COMET, BERTScore, etc.) on three downstream cross-lingual tasks (dialogue state tracking, question answering, and semantic parsing). For each task, we have access to a monolingual task-specific model and a translation model. We calculate the correlation between the metric's ability to predict a good/bad translation with the success/failure on the final task for machine-translated test sentences. Our experiments demonstrate that all metrics exhibit negligible correlation with the extrinsic evaluation of downstream outcomes. We also find that the scores provided by neural metrics are not interpretable, in large part due to having undefined ranges. We synthesise our analysis into recommendations for future MT metrics to produce labels rather than scores for more informative interaction between machine translation and multilingual language understanding.", "sections": [{"heading": "Introduction", "text": "Although machine translation (MT) is typically seen as a standalone application, in recent years MT models have been more frequently deployed as a component of a complex NLP platform delivering multilingual capabilities such as cross-lingual information retrieval  or automated multilingual customer support (Gerz et al., 2021). When an erroneous translation is generated by the MT systems, it may add new errors in the task pipeline leading to task failure and poor user ex-perience. For example, consider the user's request in Chinese \u5251\u6865\u6709\u7259\u4e70\u52a0\u83dc\u5417\uff1f (\"Is there any good Jamaican food in Cambridge?\") machinetranslated into English as \"Does Cambridge have a good meal in Jamaica?\". The model will erroneously consider \"Jamaica\" as a location, instead of cuisine, and prompt the search engine to look up restaurants in Jamaica 1 . To avoid this breakdown, it is crucial to detect an incorrect translation before it causes further errors in the task pipeline.\nOne way to approach this breakdown detection is using segment-level scores provided by MT metrics. Recent MT metrics have demonstrated high correlation with human judgements at the system level for some language pairs (Ma et al., 2019). These metrics are potentially capable of identifying subtle differences between MT systems that emerge over a relatively large test corpus. These metrics are also evaluated on respective correlation with human judgements at the segment level, however, there is a considerable performance penalty (Ma et al., 2019;Freitag et al., 2021b). Segment-level evaluation of MT is indeed more difficult and even humans have low inter-annotator agreement on this task (Popovi\u0107, 2021). Despite MT systems being a crucial intermediate step in several applications, characterising the behaviour of these metrics under task-oriented evaluation has not been explored.\nIn this work, we provide a complementary evaluation of MT metrics. We focus on the segmentlevel performance of metrics, and we evaluate their performance extrinsically, by correlating each with the outcome of downstream tasks with respective, reliable accuracy metrics. We assume access to a parallel task-oriented dataset, a task-specific monolingual model, and a translation model that can translate from the target language into the language of the monolingual model. We consider the Translate-Test setting -where at test time, the examples from the test language are translated to the task language for evaluation. We use the outcomes of this extrinsic task to construct a breakdown detection benchmark for the metrics.\nWe use dialogue state tracking, semantic parsing, and extractive question answering as our extrinsic tasks. We evaluate nine metrics consisting of string overlap metrics, embedding-based metrics, and metrics trained using scores from human evaluation of MT. Surprisingly, we find our setup challenging for all existing metrics; demonstrating poor capability in discerning good and bad translations across tasks. We present a comprehensive analysis of the failure of the metrics through quantitative and qualitative evaluation.\nOur contributions are summarised as follows: 1) We derive a new breakdown detection task, for evaluating MT metrics, measuring how indicative segment-level scores are for downstream performance of an extrinsic cross-lingual task (Section 3). We evaluate nine metrics on three extrinsic tasks covering 39 unique language pairs. The task outputs, the breakdown detection labels, and metric outputs are publicly available. 2 2) We show that segment-level scores, from these metrics, have minimal correlation with extrinsic task performance (Section 4.1). Our results indicate that these scores are uninformative at the segment level (Section 4.3) -clearly demonstrating a serious deficiency in the best contemporary MT metrics. In addition, we find variable task sensitivity to different MT errors (Section 4.2).\n3) We propose recommendations on developing MT metrics to produce useful segment-level output by predicting labels instead of scores and suggest reusing existing post-editing datasets and explicit error annotations (See Section 5).", "publication_ref": ["b20", "b36", "b36", "b47"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Evaluation of machine translation has been of great research interest across different communities (Nakazawa et al., 2022;Fomicheva et al., 2021). Notably, the Conference on Machine Translation (WMT) has been organising annual shared tasks on automatic MT evaluation since 2006 (Koehn and Monz, 2006;Freitag et al., 2021b) that invites metric developers to evaluate their methods on outputs of several MT systems. Metric evaluation typically includes a correlation of the scores with human judgements collected for the respective translation outputs. But, designing such guidelines is challenging (Mathur et al., 2020a), leading to the development of several different methodologies and analyses over the years.\nThe human evaluation protocols include general guidelines for fluency, adequacy and/or comprehensibility (White et al., 1994) on continuous scales (Koehn and Monz, 2006;Graham et al., 2013) (direct assessments) or fine-grained annotations of MT errors (Freitag et al., 2021a,b) based on error ontology like Multidimensional Quality Metrics (MQM) (Lommel et al., 2014) or rank outputs from different MT systems for the same input (Vilar et al., 2007). Furthermore, the best way to compare MT scores with their corresponding judgements is also an open question (Callison-Burch et al., 2006;Bojar et al., 2014Bojar et al., , 2017. The new metrics claim their effectiveness by comparing their performance with competitive metrics on the latest benchmark.\nThe progress and criticism of MT evaluation are generally documented in a metrics shared task overview (Callison-Burch et al., 2007). For example, Stanojevi\u0107 et al. (2015) highlighted the effectiveness of neural embedding-based metrics; Ma et al. (2019) show that metrics struggle on segmentlevel performance despite achieving impressive system-level correlation; Mathur et al. (2020b) investigate how different metrics behave under different domains. In addition to these overviews, Mathur et al. (2020a) show that meta-evaluation regimes were sensitive to outliers and minimal changes in evaluation metrics are insufficient to claim metric efficacy. Kocmi et al. (2021) conducted a comprehensive evaluation effort to identify which metric is best suited for pairwise ranking of MT systems. Guillou and Hardmeier (2018) look at a specific phenomenon of whether metrics are capable of evaluating translations involving pronominal anaphora. Recent works have also criticised individual metrics such as COMET (Amrhein and Sennrich, 2022) and BERTScore (Hanna and Bojar, 2021). These works draw their conclusions based on some comparison with human judgement or on specific pitfalls of individual metrics. Our work focuses on the usability of the metrics as solely judged on their ability to predict downstream tasks where MT is an intermediate step (with a primary emphasis on segment-level performance). Taskbased evaluation has been well studied (Jones and Galliers (1996); Laoudi et al. (2006); Zhang et al.", "publication_ref": ["b43", "b15", "b31", "b40", "b60", "b31", "b21", "b35", "b58", "b9", "b6", "b7", "b8", "b55", "b36", "b41", "b40", "b30", "b23", "b2", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Monolingual End", "text": "Task Model English DST I am looking for a restaurant that has cheap price range and a rating of 5. Figure 1: The meta-evaluation pipeline. The predictions for the extrinsic task in the test language (Chinese, ZH) are obtained using the Translate-Test setup -the test language is translated into the task language (English, EN) before passing to the task-specific model. The input sentence (ZH) and the corresponding translations (EN) are evaluated with a metric of interest. The metric is evaluated based on the correlation of its scores with the predictions of the end task.\n(2022), inter alia) but limited to evaluating MT systems rather than MT metrics. Closer to our work is Scarton et al. (2019); Zouhar et al. (2021) which proposes MT evaluation as ranking translations based on the time to post-edit model outputs. We borrow the term of breakdown detection from Martinovski and Traum (2003) that proposes breakdown detection for dialogue systems to detect unnatural responses.", "publication_ref": ["b52", "b64", "b39"], "figure_ref": [], "table_ref": []}, {"heading": "Methodology", "text": "Our aim is to determine how reliable MT metrics are for predicting success on downstream tasks.\nOur setup uses a monolingual model (e.g., a dialogue state tracker) trained on a task language and parallel test data from multiple languages. We use MT to translate a test sentence (from a test language to the task language) and then infer a label for this example using the monolingual model. If the model predicts a correct label for the parallel task language input but an incorrect label for the translated test language input, then we have observed a breakdown due to a material error in the translation pipeline. We then study if the metric could predict if the translation is suitable for the end task. We refer to Figure 1 for an illustration. We frequently use the terms test language and task language to avoid confusion with the usage of source language and target language in the traditional machine translation setup. In Figure 1, the task language is English and the test language is Chinese. We now describe our evaluation setup and the metrics under investigation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Setup", "text": "For all the tasks described below, we first train a model for the respective tasks on the monolingual setup. We evaluate the task language examples on each task and capture the monolingual predictions of the model. We consider the Translate-Test paradigm (Hu et al., 2020), we translate the examples from each test language into the task language. The generated translations are then fed to the task-specific monolingual model. We use either (i) OPUS translation models (Tiedemann and Thottingal, 2020), (ii) M2M100 translation  or (iii) translations provided by the authors of respective datasets. Note that the examples across all the languages are parallel and we therefore always have access to the correct label for a translated sentence. We obtain the predictions for the translated data to construct a breakdown detection benchmark for the metrics. We consider only the subset of examples in the test language which were correctly predicted in the task language to avoid errors that arise from extrinsic task complexity. Therefore, all incorrect extrinsic predictions for the test language in our setup arise from erroneous translation. This isolates the extrinsic task failure as the fault of only the MT system. We use these predictions to build a binary classification benchmark-all target language examples that are correctly predicted in the extrinsic task receive a positive label (no breakdown) while the incorrect predictions receive a negative label (breakdown).\nWe consider the example from the test language as source, the corresponding machine translation as hypothesis and the human reference from the task language as reference. Thus, in Figure 1, the source is \u5251\u6865\u6709\u7259\u4e70\u52a0\u83dc\u5417\uff1f, the hypothesis is \"Does Cambridge have a good meal in Jamaica\", and the reference will be \"Is there any good Jamaican food in Cambridge\". These triples are then scored by the respective metrics. After obtaining the segment-level scores for these triples, we define a threshold for the scores, thus turning metrics into classifiers. For example, if the threshold for the metric in Figure 1 is 0.5, it would mark both examples as bad translations. We plot a histogram over the scores with ten bins for every setup and select the interval with the highest performance on the development set as a threshold. The metrics are then evaluated on how well their predictions for a good/bad translation correlate with the breakdown detection labels.", "publication_ref": ["b26", "b57"], "figure_ref": [], "table_ref": []}, {"heading": "Tasks", "text": "We choose tasks that contain outcomes belonging to a small set of labels, unlike natural language generation tasks which have a large solution space. This discrete nature of the outcomes allows us to quantify the performance of MT metrics based on standard classification metrics. The tasks also include varying types of textual units: utterances, sentences, questions, and paragraphs, allowing a comprehensive evaluation of the metrics.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Semantic Parsing (SP)", "text": "Semantic parsing transforms natural language utterances into logical forms to express utterance semantics in some machine-readable language. The original ATIS study (Hemphill et al., 1990) collected questions about flights in the USA with the corresponding SQL to answer respective questions from a relational database. We use the MultiATIS++SQL dataset from Sherborne and Lapata (2022) comprising gold parallel utterances in English, French, Portuguese, Spanish, German and Chinese (from Xu et al. (2020)) paired to executable SQL output logical forms (from Iyer et al. (2017)). The model follows Sherborne and Lapata (2023), as an encoder-decoder Transformer model based on mBART50 (Tang et al., 2021). The parser generates valid SQL queries and performance is measured as exact-match denotation accuracy-the proportion of output queries returning identical database results relative to gold SQL queries.", "publication_ref": ["b25", "b61", "b28", "b54", "b56"], "figure_ref": [], "table_ref": []}, {"heading": "Extractive Question Answering (QA)", "text": "The task of extractive question answering is predicting a span of words from a paragraph corresponding to the question. We use the XQuAD dataset (Artetxe et al., 2020) for evaluating extractive question answering. The XQuAD dataset was obtained by professionally translating examples from the development set of English SQuAD dataset (Rajpurkar et al., 2016) into ten languages: Spanish, German, Greek, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, and Hindi. We use the publicly available question answering model that finetunes RoBERTa (Liu et al., 2019) on the SQuAD training set. We use the Exact-Match metric, i.e., the model's predicted answer span exactly matches the gold standard answer span; for the breakdown detection task. The metrics scores are produced for the question and the context. A translation is considered to be faulty if either of the scores falls below the chosen threshold for every metric.", "publication_ref": ["b3", "b48", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "Dialogue State Tracking (DST)", "text": "In the dialogue state tracking task, a model needs to map the user's goals and intents in a given conversation to a set of slots and values, known as a dialogue state, based on a pre-defined ontology. MultiWoZ 2.1 (Eric et al., 2020) is a popular dataset for examining the progress in dialogue state tracking which consists of multi-turn conversations in English spanning across 7 domains. We consider the Multi 2 WoZ dataset (Hung et al., 2022) where the development and test set have been professionally translated into German, Russian, Chinese, and Arabic from the MultiWoZ 2.1 dataset. We use the dialogue state tracking model trained on the English dataset by Lee et al. (2019). We consider the Joint Goal Accuracy where the inferred label is correct only if the predicted dialogue state is exactly equal to the ground truth to provide labels for the breakdown task. We use oracle dialogue history and the metric scores are produced only for the current utterance spoken by the user.", "publication_ref": ["b12", "b27", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "Metrics", "text": "We describe the metrics based on their design principles: derived from the surface level token overlap, embedding similarity, and neural metrics trained using WMT data. We selected the following metrics as they are the most studied, frequently used, and display a varied mix of design principles.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Surface Level Overlap", "text": "BLEU (Papineni et al., 2002) is a string-matching metric that compares the token-level n-grams of the hypothesis with the reference translation. BLEU is computed as a precision score weighted by a brevity penalty. We use sentence-level BLEU in our experiments. chrF (Popovi\u0107, 2017) computes a character n-gram F-score based on the overlap between the hypothesis and the reference.", "publication_ref": ["b44", "b46"], "figure_ref": [], "table_ref": []}, {"heading": "Embedding Based", "text": "BERTScore  uses contextual embeddings from pre-trained language models to compute the similarity between the tokens in the reference and the generated translation using cosine similarity. The similarity matrix is used to compute precision, recall, and F1 scores.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Trained on WMT Data", "text": "WMT organises an annual shared task on developing MT models for several categories in machine translation (Akhbardeh et al., 2021). Human evaluation of the translated outputs from the participating machine translation models is often used to determine the best-performing MT system. In recent years, this human evaluation has followed two protocols: (i) Direct Assessment (DA) (Graham et al., 2013): where the given translation is rated from 0 to 100 based on the perceived translation quality and (ii) Expert based evaluation where the translations are evaluated by professional translators with explicit error listing based on the Multidimensional Quality Metrics (MQM) ontology. MQM ontology consists of a hierarchy of errors and translations are penalised based on the severity of errors in this hierarchy. These human evaluations are then used as training data for building new MT metrics. COMET metrics: Cross-lingual Optimized Metric for Evaluation of Translation (COMET) (Rei et al., 2020) uses a cross-lingual encoder (XLM-R (Conneau et al., 2020)) and pooling operations to predict score of the given translation. Representations for the source, hypothesis, and reference (obtained using the encoder) are combined and passed through a feedforward layer to predict a score. These metrics use a combination of WMT evaluation data across the years to produce different metrics. In all the variants, the MQM scores and DA scores are normalised to z-scores to reduce the effect of outlier annotations. COMET-DA uses direct assessments from 2017 to 2019 as training data while COMET-MQM uses direct assessments from 2017 to 2021 as training data. This metric is then fine-tuned with MQM data from Freitag et al. (2021a). UniTE metrics (Wan et al., 2022), Unified Translation Evaluation, is another neural translation metric that proposes a multi-task setup for the three strategies of evaluation: source-hypothesis, source-hypothesis-reference, and reference-hypothesis in a single model. The pre-training stage involves training the model with synthetic data constructed using a subset of WMT evaluation data. Fine-tuning uses novel attention mechanisms and aggregate loss functions to facilitate the multi-task setup.\nAll the above reference-based metrics have their corresponding reference-free versions which use the same training regimes but exclude encoding the reference. We refer to them as COMET-QE-DA, COMET-QE-MQM, and UniTE-QE respectively. COMET-QE-DA in this work uses DA scores from 2017 to 2020. We list the code sources of these metrics in Appendix B.", "publication_ref": ["b21", "b50", "b59"], "figure_ref": [], "table_ref": []}, {"heading": "Metric Evaluation", "text": "The meta-evaluation for the above metrics uses the breakdown detection benchmark. As the class distribution changes depending on the task and the language pair, we require an evaluation that is robust to class imbalance. We consider using macro-F1 and Matthew's Correlation Coefficient (MCC) (Matthews, 1975) on the classification labels. The range of macro-F1 is from 0 to 1 with equal weight to positive and negative classes. We include MCC to interpret the MT metric's standalone performance for the given extrinsic task. The range of MCC is between -1 to 1. An MCC value near 0 indicates no correlation with the class distribution. Any MCC value between 0 and 0.3 indicates negligible correlation, 0.3 to 0.5 indicates low correlation.", "publication_ref": ["b42"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "We report the aggregated results for semantic parsing, question answering, and dialogue state tracking in Table 1 with fine-grained results in Appendix D. We use a random baseline for comparison which assigns the positive and negative labels with equal probability.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Performance on Extrinsic Tasks", "text": "We find that almost all metrics perform above the random baseline on the macro-F1 metric. We use MCC to identify if this increase in macro-F1 makes the metric usable in the end task. Evaluating MCC, we find that all the metrics show negligible correlation across all three tasks. Contrary to trends where neural metrics are better than metrics based on surface overlap (Freitag et al., 2021b), we find this breakdown detection to be difficult irrespective  of the design of the metric. We also evaluate an ensemble with majority voting of the predictions from the top three metrics per task. Ensembling provides minimal gains suggesting that metrics are making similar mistakes despite varying properties of the metrics.\nComparing the reference-based versions of trained metrics (COMET-DA, COMET-MQM, UniTE) with their reference-free quality estimation (QE) equivalents, we observe that referencebased versions perform better, or are competitive to, their reference-free versions for the three tasks. We also note that references are unavailable when the systems are in production, hence reference-based metrics are unsuitable for realistic settings. We discuss alternative ways of obtaining references in Section 4.4.\nBetween the use of MQM-scores and DA-scores during fine-tuning COMET variants, we find that both COMET-QE-DA and COMET-DA are strictly better than COMET-QE-MQM and COMET-MQM for question answering and dialogue state tracking respectively, with no clear winner for semantic parsing (See Appendix D).\nThe results on per-language pair in Appendix D suggest that no specific language pairs stand out as easier/harder across tasks. As this performance is already poor, we cannot verify if neural metrics can generalise in evaluating language pairs unseen during training. Case Study: We look at Semantic Parsing with an English-trained parser tested with Chinese inputs for our case study with the well-studied COMET- DA metric. We report the number of correct and incorrect predictions made by COMET-DA across ten equal ranges of scores in Figure 2. The bars labelled on the x-axis indicate the end-point of the interval i.e., the bar labelled -0.74 contains examples that were given scores between -1.00 and -0.74.\nFirst, we highlight that the threshold is -0.028, counter-intuitively suggesting that even some correct translations receive a negative score. We expected the metric to fail in the regions around the threshold as those represent strongest confusion. For example, \"\u5468\u65e5\u4e0b\u5348\u4ece\u8fc8\u963f\u5bc6\u98de\u5f80\u514b\u5229\u592b \u5170\" is correctly translated as \"Sunday afternoon from Miami to Cleveland\" yet the metric assigns it a score of -0.1. However, the metric makes mis-  takes throughout the bins. For example, \"\u6211\u9700\u8981 \u9884\u8ba2\u4e00\u8d9f\u8054\u5408\u822a\u7a7a\u4e0b\u5468\u516d\u7684\u4ece\u8f9b\u8f9b\u90a3\u63d0\u98de\u5f80\u7ebd \u7ea6\u5e02\u7684\u822a\u73ed\" is translated as \"I need to book a flight from Cincinnati to New York City next Saturday.\" and loses the crucial information of \"United Airlines\"; yet it is assigned a high score of 0.51. This demonstrates that the metric possesses a limited perception of a good or bad translation for the end task.\nWe suspect this behaviour is due to the current framework of MT evaluation. The development of machine translation metrics largely caters towards the intrinsic task of evaluating the quality of a translated text in the target language. The severity of a translation error is dependent on the guidelines released by the organisers of the WMT metrics task or the design choices of the metric developers. Our findings agree with  that different downstream tasks will demonstrate varying levels of sensitivity to the same machine translation errors.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Qualitative Evaluation", "text": "To quantify detecting which translation errors are most crucial to the respective extrinsic tasks, we conduct a qualitative evaluation of the MT outputs and task predictions. We annotate 50 false positives and 50 false negatives for test languages Chinese (SP), Hindi (QA), and Russian (DST) respectively. The task language is English. We annotate the MT errors (if present) in these examples based on the MQM ontology. We tabulate these results in Table 2 using COMET-DA for these analyses.\nWithin the false negatives, a majority of the errors (>48%) are due to the metric's inability to detect translations containing synonyms or paraphrases of the references as valid translations. Further, omission errors detected by the metric are not crucial for DST as these translations often exclude pleasantries. Similarly, errors in fluency are not important for both DST and SP but they are crucial for QA as grammatical errors in questions produce incorrect answers. Mistranslation of named entities (NEs), especially which lie in the answer span, is a false negative for QA since QA models find the answer by focusing on the words in the context surrounding the NE rather than the error in that NE. Detecting mistranslation in NEs is crucial for both DST and SP as this error category dominates the false positives. A minor typo of Lester instead of Leicester marks the wrong location in the dialogue state which is often undetected by the metric. Addition and omission errors are also undetected for SP while mistranslation of reservation times is undetected for DST.\nWe also find that some of the erroneous predictions can be attributed to the failure of the extrinsic task model than the metric. For example, the MT model uses an alternative term of direct instead of nonstop while generating the translation for the reference \"show me nonstop flights from montreal to orlando\". The semantic parser fails to generalise despite being trained with mBART50 to ideally inherit some skill at disambiguiting semantically similar phrases. This error type accounts for 25% for SP, 20% for QA and 5% in DST of the total annotated errors. We give examples in Appendix C.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Finding the Threshold", "text": "Interpreting system-level scores provided by automatic metrics requires additional context such as the language pair of the machine translation model or another MT system for comparison 3 . In this classification setup, we rely on interpreting the segment-level score to determine whether the translation is suitable for the downstream task. We find that choosing the right threshold to identify trans-  lations requiring correction is not straightforward.\nOur current method to obtain a threshold relies on validating candidate thresholds on the development set and selecting an option with the best F1 score. These different thresholds are obtained by plotting a histogram of scores with ten bins per task and language pair. We report the mean and standard deviation of best thresholds for every language pair for every metric in Table 3. Surprisingly, the thresholds are inconsistent and biased for bounded metrics: BLEU (0-100), chrF (0-100), and BERTScore (0-1). The standard deviations across the table indicate that the threshold varies greatly across language pairs. We find that thresholds of these metrics are also not transferable across tasks. COMET metrics, except COMET-DA, have lower standard deviations. By design, the range of COMET metrics in this work is unbounded. However, as discussed in the theoretical range of COMET metrics 4 , empirically, the range for COMET-MQM lies between -0.2 to 0.2, questioning whether lower standard deviation is an indicator of threshold consistency. Some language pairs within the COMET metrics have negative thresholds. We also find that some of the use cases under the UniTE metrics have a mean negative threshold, indicating that good translations can have negative UniTE scores. Similar to Marie (2022), we suggest that the notion of negative scores for good translations, only for certain language pairs, is counter-intuitive as most NLP metrics tend to produce positive scores.\nThus, we find that both bounded and unbounded metrics discussed here do not provide segmentlevel scores whose range can be interpreted mean-  ingfully across tasks and language pairs.", "publication_ref": ["b66"], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "Reference-based Metrics in an Online Setting", "text": "In an online setting, we do not have access to references at test time. To test the effectiveness of reference-based methods here, we consider translating the translation back into the test language. For example, for an en parser, the test language ti zh is translated into mt en and then translated back to Chinese as mt zh . The metrics now consider mt en as source, mt zh as hypothesis, and ti zh as the reference. We generate these new translations using the mBART50 translation model (Tang et al., 2021) and report the results in Table 4.\nCompared to the results in Table 1, there is a further drop in performance across all the tasks and metrics. The metrics also perform worse than their reference-free counterparts. The second translation is likely to add additional errors to the existing translation. This cascading of errors confuses the metric and it can mark a perfectly useful translation as a breakdown. The only exception is that of the UniTE metric which has comparable performance (but overall poor) due to its multi-task setup.", "publication_ref": ["b56"], "figure_ref": [], "table_ref": ["tab_8", "tab_2"]}, {"heading": "Recommendations", "text": "Our experiments suggest that evaluating MT metrics on the segment level for extrinsic tasks has considerable room for improvement. We propose recommendations based on our observations:\nPrefer MQM for Human Evaluation of MT outputs: We reinforce the proposal of using the MQM scoring scheme with expert annotators for evaluating MT outputs in line with Freitag et al. (2021a). As seen in Section 4.2, different tasks have varying tolerance to different MT errors. With explicit errors marked per MT output, future classifiers can be trained on a subset of human evaluation data containing errors most relevant to the downstream application.\nMT Metrics Could Produce Labels over Scores: The observations from Section 4.2 and Section 4.3 suggest that interpreting the quality of the produced MT translation based on a number is unreliable and difficult. We recommend exploring whether segment-level MT evaluation can be approached as an error classification task instead of regression. Specifically, whether the words in the source/hypothesis can be tagged with explicit error labels. Resorting to MQM-like human evaluation will result in a rich repository of human evaluation based on an ontology of errors and erroneous spans marked across the source and hypothesis (Freitag et al., 2021a). Similarly, the post-editing datasets (Scarton et al. (2019); Fomicheva et al. (2022) , inter alia) also provide a starting point. An interesting exploration in this direction are the works by Perrella et al. (2022); Rei et al. (2022) that treat MT evaluation as a sequence-tagging problem by labelling the errors in an example. Such metrics can also be used for intrinsic evaluation by assigning weights to the labels and producing a weighted score.\nAdd Diverse References During Training: From Section 4.2, we find that both the neural metric and the task-specific model are not robust to paraphrases. We also recommend the inclusion of diverse references through automatic paraphrasing (Bawden et al., 2020) or data augmentation during the training of neural metrics.", "publication_ref": ["b52", "b16", "b45", "b49", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We propose a method for evaluating MT metrics which is reliable at the segment-level and does not depend on human judgements by using correlation MT metrics with the success of extrinsic downstream tasks. We evaluated nine different metrics on the ability to detect errors in generated translations when machine translation is used as an intermediate step for three extrinsic tasks: Semantic Parsing, Question Answering, and Dialogue State Tracking. We find that segment-level scores provided by all the metrics show negligible correlation with the success/failure outcomes of the end task across different language pairs. We attribute this result to segment scores produced by these metrics being uninformative and that different extrinsic tasks demonstrate different levels of sensitivity to different MT errors. We propose recommenda-tions to predict error types instead of error scores to facilitate the use of MT metrics in downstream tasks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "As seen in Section 4.2, sometimes the metrics are unnecessarily penalised due to errors made by the end task models. Filtering these cases would require checking every example in every task manually. We hope our results can provide conclusive trends to the metric developers focusing on segment-level MT evaluation.\nWe included three tasks to cover different types of errors in machine translations and different types of contexts in which an online MT metric is required. Naturally, this regime can be extended to other datasets, other tasks, and other languages (Ruder et al., 2021;Doddapaneni et al., 2022). Further, our tasks used stricter evaluation metrics such as exact match. Incorporating information from partially correct outputs is not trivial and will be hopefully addressed in the future. We have covered 37 language pairs across the tasks which majorly use English as one of the languages. Most of the language pairs in this study are high-resource languages. Similarly, the examples in multilingual datasets are likely to exhibit translationese -unnatural artefacts from the task language present in the test language during manual translation; which tend to overestimate the performance of the various tasks (Majewska et al., 2023;Freitag et al., 2020). We hope to explore the effect of translationese on MT evaluation  and extrinsic tasks in future. The choice of metrics in this work is not exhaustive and is dependent on the availability and ease of use of the metric provided by the authors.", "publication_ref": ["b51", "b11", "b37", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Ethics Statement", "text": "This work uses datasets, models, and metrics that are publicly available. Although the scope of this work does not allow us to have an in-depth discussion of biases associated with metrics (Amrhein et al., 2022), we caution the readers of drawbacks of metrics that cause unfair evaluation to marginalised subpopulations which are discovered or yet to be discovered. We will release the translations, metrics scores, and corresponding task outputs for reproducibility.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Language Codes", "text": "Please find the language codes in Table 5.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_10"]}, {"heading": "B Implementation Details", "text": "We provide the implementation details of metrics and models in Table 6. All models are publicly available and required no training from our side. The metrics BERTScore, COMET family and UniTE family can run on both GPU and CPU. If run on GPU, the metrics run under 5 minutes for a given task and given language pair. No hyperparameters are required. We follow the standard train-dev-test split as released by the authors for DST (Hung et al., 2022) and SP (Sherborne and Lapata, 2022). As no development set is available for the XQuAD dataset, we use the first 200 examples as development set to choose the threshold but report the performance on the full test set.", "publication_ref": ["b27", "b53"], "figure_ref": [], "table_ref": []}, {"heading": "C Errors of COMET-DA", "text": "The proportion of errors from Section 4.2 are listed in Table 2. We also provide error examples in Figure 3.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "D Task-specific results", "text": "We now list the results across every language pair for all the tasks in Tables tables 7 to 11.      ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "We thank Barry Haddow for providing us with valuable feedback on setting up this work. We thank Arushi Goel and the attendees at the MT Marathon 2022 for discussions about this work. We thank Ankita Vinay Moghe, Nikolay Bogoychev, and Chantal Amrhein for their comments on the earlier drafts. We thank the anonymous reviewers for their helpful suggestions. This work was supported in part by the UKRI Centre for Doctoral Training in Natural Language Processing, funded by the UKRI (grant EP/S022481/1) and the University of Edinburgh (Moghe). We also thank Huawei for their support (Moghe). Sherborne gratefully acknowledges the support of the UK Engineering and Physical Sciences Research Council (grant EP/W002876/1).", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Masaaki Nagata, Ajay Nagesh, Toshiaki Nakazawa, Matteo Negri, Santanu Pal, Allahsera Auguste Tapo, Marco Turchi, Valentin Vydrin, and Marcos Zampieri. 2021. Findings of the 2021 conference on machine translation (WMT21)", "journal": "", "year": "", "authors": "Farhad Akhbardeh; Arkady Arkhangorodsky; Magdalena Biesialska; Ond\u0159ej Bojar; Rajen Chatterjee; Vishrav Chaudhary; Marta R Costa-Jussa; Cristina Espa\u00f1a-Bonet; Angela Fan; Christian Federmann; Markus Freitag; Yvette Graham; Roman Grundkiewicz; Barry Haddow; Leonie Harter; Kenneth Heafield; Christopher Homan; Matthias Huck; Kwabena Amponsah-Kaakyire; Jungo Kasai; Daniel Khashabi"}, {"ref_id": "b1", "title": "ACES: Translation Accuracy Challenge Sets for Evaluating Machine Translation Metrics", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Chantal Amrhein; Nikita Moghe; Liane Guillou"}, {"ref_id": "b2", "title": "Identifying weaknesses in machine translation metrics through minimum Bayes risk decoding: A case study for COMET", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Chantal Amrhein; Rico Sennrich"}, {"ref_id": "b3", "title": "On the cross-lingual transferability of monolingual representations", "journal": "", "year": "2020", "authors": "Mikel Artetxe; Sebastian Ruder; Dani Yogatama"}, {"ref_id": "b4", "title": "Annual Meeting of the Association for Computational Linguistics", "journal": "Online. Association for Computational Linguistics", "year": "", "authors": ""}, {"ref_id": "b5", "title": "A study in improving BLEU reference coverage with diverse automatic paraphrasing", "journal": "", "year": "2020", "authors": "Rachel Bawden; Biao Zhang; Lisa Yankovskaya; Andre T\u00e4ttar; Matt Post"}, {"ref_id": "b6", "title": "Findings of the 2014 workshop on statistical machine translation", "journal": "", "year": "2014", "authors": "Ond\u0159ej Bojar; Christian Buck; Christian Federmann; Barry Haddow; Philipp Koehn; Johannes Leveling; Christof Monz; Pavel Pecina; Matt Post; Herve Saint-Amand; Radu Soricut; Lucia Specia; Ale\u0161 Tamchyna"}, {"ref_id": "b7", "title": "Results of the WMT17 metrics shared task", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Ond\u0159ej Bojar; Yvette Graham; Amir Kamran"}, {"ref_id": "b8", "title": "(meta-) evaluation of machine translation", "journal": "", "year": "2007", "authors": "Chris Callison; - Burch; Cameron Fordyce; Philipp Koehn; Christof Monz; Josh Schroeder"}, {"ref_id": "b9", "title": "Re-evaluating the role of Bleu in machine translation research", "journal": "Association for Computational Linguistics", "year": "2006", "authors": "Chris Callison; - Burch; Miles Osborne; Philipp Koehn"}, {"ref_id": "b10", "title": "Unsupervised cross-lingual representation learning at scale", "journal": "", "year": "2020", "authors": "Alexis Conneau; Kartikay Khandelwal; Naman Goyal; Vishrav Chaudhary; Guillaume Wenzek; Francisco Guzm\u00e1n; Edouard Grave; Myle Ott; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b11", "title": "Indicxtreme: A multi-task benchmark for evaluating indic languages", "journal": "", "year": "2022", "authors": "Sumanth Doddapaneni; Rahul Aralikatte; Gowtham Ramesh; Shreya Goyal; M Mitesh; Anoop Khapra; Pratyush Kunchukuttan;  Kumar"}, {"ref_id": "b12", "title": "Mul-tiWOZ 2.1: A consolidated multi-domain dialogue dataset with state corrections and state tracking baselines", "journal": "", "year": "2020", "authors": "Mihail Eric; Rahul Goel; Shachi Paul; Abhishek Sethi; Sanchit Agarwal; Shuyang Gao; Adarsh Kumar; Anuj Goyal; Peter Ku; Dilek Hakkani-Tur"}, {"ref_id": "b13", "title": "European Language Resources Association", "journal": "", "year": "", "authors": "France Marseille"}, {"ref_id": "b14", "title": "Beyond english-centric multilingual machine translation", "journal": "J. Mach. Learn. Res", "year": "2021", "authors": "Angela Fan; Shruti Bhosale; Holger Schwenk; Zhiyi Ma; Ahmed El-Kishky; Siddharth Goyal; Mandeep Baines; Onur Celebi; Guillaume Wenzek; Vishrav Chaudhary; Naman Goyal; Tom Birch; Vitaliy Liptchinsky; Sergey Edunov; Michael Auli; Armand Joulin"}, {"ref_id": "b15", "title": "The Eval4NLP shared task on explainable quality estimation: Overview and results", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Marina Fomicheva; Piyawat Lertvittayakumjorn; Wei Zhao; Steffen Eger; Yang Gao"}, {"ref_id": "b16", "title": "MLQE-PE: A multilingual quality estimation and post-editing dataset", "journal": "", "year": "2022", "authors": "Marina Fomicheva; Shuo Sun; Erick Fonseca; Chrysoula Zerva; Fr\u00e9d\u00e9ric Blain; Vishrav Chaudhary; Francisco Guzm\u00e1n; Nina Lopatina; Lucia Specia; Andr\u00e9 F T Martins"}, {"ref_id": "b17", "title": "Qijun Tan, and Wolfgang Macherey. 2021a. Experts, errors, and context: A large-scale study of human evaluation for machine translation", "journal": "", "year": "", "authors": "Markus Freitag; George Foster; David Grangier; Viresh Ratnakar"}, {"ref_id": "b18", "title": "BLEU might be guilty but references are not innocent", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Markus Freitag; David Grangier; Isaac Caswell"}, {"ref_id": "b19", "title": "Alon Lavie, and Ond\u0159ej Bojar. 2021b. Results of the WMT21 metrics shared task: Evaluating metrics with expert-based human evaluations on TED and news domain", "journal": "", "year": "", "authors": "Markus Freitag; Ricardo Rei; Nitika Mathur; Chi-Kiu Lo; Craig Stewart; George Foster"}, {"ref_id": "b20", "title": "Multilingual and cross-lingual intent detection from spoken data", "journal": "", "year": "2021", "authors": "Daniela Gerz; Pei-Hao Su; Razvan Kusztos; Avishek Mondal; Micha\u0142 Lis; Eshan Singhal; Nikola Mrk\u0161i\u0107; Tsung-Hsien Wen; Ivan Vuli\u0107"}, {"ref_id": "b21", "title": "Continuous measurement scales in human evaluation of machine translation", "journal": "", "year": "2013", "authors": "Yvette Graham; Timothy Baldwin; Alistair Moffat; Justin Zobel"}, {"ref_id": "b22", "title": "Statistical power and translationese in machine translation evaluation", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Yvette Graham; Barry Haddow; Philipp Koehn"}, {"ref_id": "b23", "title": "Automatic reference-based evaluation of pronoun translation misses the point", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Liane Guillou; Christian Hardmeier"}, {"ref_id": "b24", "title": "2021. A fine-grained analysis of BERTScore", "journal": "Online. Association for Computational Linguistics", "year": "", "authors": "Michael Hanna; Ond\u0159ej Bojar"}, {"ref_id": "b25", "title": "The ATIS spoken language systems pilot corpus", "journal": "", "year": "1990-06-24", "authors": "Charles T Hemphill; John J Godfrey; George R Doddington"}, {"ref_id": "b26", "title": "XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation", "journal": "PMLR", "year": "2020-07", "authors": "Junjie Hu; Sebastian Ruder; Aditya Siddhant; Graham Neubig; Orhan Firat; Melvin Johnson"}, {"ref_id": "b27", "title": "Multi2WOZ: A robust multilingual dataset and conversational pretraining for task-oriented dialog", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Chia-Chien Hung; Anne Lauscher; Ivan Vuli\u0107; Simone Ponzetto; Goran Glava\u0161"}, {"ref_id": "b28", "title": "Learning a neural semantic parser from user feedback", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Srinivasan Iyer; Ioannis Konstas; Alvin Cheung; Jayant Krishnamurthy; Luke Zettlemoyer"}, {"ref_id": "b29", "title": "Evaluating Natural Language Processing Systems, An Analysis and Review", "journal": "Springer", "year": "1996", "authors": ""}, {"ref_id": "b30", "title": "To ship or not to ship: An extensive evaluation of automatic metrics for machine translation", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Tom Kocmi; Christian Federmann; Roman Grundkiewicz; Marcin Junczys-Dowmunt"}, {"ref_id": "b31", "title": "Manual and automatic evaluation of machine translation between European languages", "journal": "Association for Computational Linguistics", "year": "2006", "authors": "Philipp Koehn; Christof Monz"}, {"ref_id": "b32", "title": "Task-based MT evaluation: From who/when/where extraction to event understanding", "journal": "", "year": "2006", "authors": "Jamal Laoudi; Calandra R Tate; Clare R Voss"}, {"ref_id": "b33", "title": "SUMBT: Slot-utterance matching for universal and scalable belief tracking", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Hwaran Lee; Jinsik Lee; Tae-Yoon Kim"}, {"ref_id": "b34", "title": "", "journal": "RoBERTa: A Robustly Optimized BERT Pretraining Approach. ArXiv preprint", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b35", "title": "Multidimensional quality metrics (mqm): A framework for declaring and describing translation quality metrics", "journal": "", "year": "2014", "authors": "Arle Lommel; Aljoscha Burchardt; Hans Uszkoreit"}, {"ref_id": "b36", "title": "Results of the WMT19 metrics shared task: Segment-level and strong MT systems pose big challenges", "journal": "", "year": "2019", "authors": "Qingsong Ma; Johnny Wei; Ond\u0159ej Bojar; Yvette Graham"}, {"ref_id": "b37", "title": "Cross-Lingual Dialogue Dataset Creation via Outline-Based Generation", "journal": "Transactions of the Association for Computational Linguistics", "year": "2023", "authors": "Olga Majewska; Evgeniia Razumovskaia; M Edoardo; Ivan Ponti; Anna Vuli\u0107;  Korhonen"}, {"ref_id": "b38", "title": "An Automatic Evaluation of the WMT22 General Machine Translation Task", "journal": "", "year": "2022", "authors": " Benjamin Marie"}, {"ref_id": "b39", "title": "The Error Is the Clue: Breakdown In Human-Machine Interaction", "journal": "", "year": "2003", "authors": "Bilyana Martinovski; David Traum"}, {"ref_id": "b40", "title": "Tangled up in BLEU: Reevaluating the evaluation of automatic machine translation evaluation metrics", "journal": "", "year": "2020", "authors": "Nitika Mathur; Timothy Baldwin; Trevor Cohn"}, {"ref_id": "b41", "title": "Results of the WMT20 metrics shared task", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Nitika Mathur; Johnny Wei; Markus Freitag; Qingsong Ma; Ond\u0159ej Bojar"}, {"ref_id": "b42", "title": "Comparison of the predicted and observed secondary structure of t4 phage lysozyme", "journal": "Biochimica et Biophysica Acta (BBA) -Protein Structure", "year": "1975", "authors": "Brian W Matthews"}, {"ref_id": "b43", "title": "Overview of the 9th workshop on Asian translation", "journal": "", "year": "2022", "authors": "Toshiaki Nakazawa; Hideya Mino; Isao Goto; Raj Dabre; Shohei Higashiyama; Shantipriya Parida; Anoop Kunchukuttan; Makoto Morishita; Ond\u0159ej Bojar; Chenhui Chu; Akiko Eriguchi; Kaori Abe; Yusuke Oda; Sadao Kurohashi"}, {"ref_id": "b44", "title": "Bleu: a method for automatic evaluation of machine translation", "journal": "Association for Computational Linguistics", "year": "2002", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"ref_id": "b45", "title": "MaTESe: Machine Translation Evaluation as a Sequence Tagging Problem", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Stefano Perrella; Lorenzo Proietti; Alessandro Scir\u00e8; Niccol\u00f2 Campolungo; Roberto Navigli"}, {"ref_id": "b46", "title": "chrF++: words helping character n-grams", "journal": "", "year": "2017", "authors": "Maja Popovi\u0107"}, {"ref_id": "b47", "title": "Agree to disagree: Analysis of inter-annotator disagreements in human evaluation of machine translation output", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Maja Popovi\u0107"}, {"ref_id": "b48", "title": "SQuAD: 100,000+ questions for machine comprehension of text", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Pranav Rajpurkar; Jian Zhang; Konstantin Lopyrev; Percy Liang"}, {"ref_id": "b49", "title": "COMET-22: Unbabel-IST 2022 Submission for the Metrics Shared Task", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Ricardo Rei; G C Jos\u00e9; Duarte Souza; Chrysoula Alves; Ana C Zerva; Taisiya Farinha; Alon Glushkova; Luisa Lavie; Andr\u00e9 F T Coheur;  Martins"}, {"ref_id": "b50", "title": "COMET: A neural framework for MT evaluation", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Ricardo Rei; Craig Stewart; Ana C Farinha; Alon Lavie"}, {"ref_id": "b51", "title": "XTREME-R: Towards more challenging and nuanced multilingual evaluation", "journal": "", "year": "2021", "authors": "Sebastian Ruder; Noah Constant; Jan Botha; Aditya Siddhant; Orhan Firat; Jinlan Fu; Pengfei Liu; Junjie Hu; Dan Garrette; Graham Neubig; Melvin Johnson"}, {"ref_id": "b52", "title": "Estimating postediting effort: a study on human judgements, taskbased and reference-based metrics of MT quality", "journal": "", "year": "2019", "authors": "Scarton Scarton; Mikel L Forcada; Miquel Espl\u00e0-Gomis; Lucia Specia"}, {"ref_id": "b53", "title": "Zero-shot cross-lingual semantic parsing", "journal": "Long Papers", "year": "2022", "authors": "Tom Sherborne; Mirella Lapata"}, {"ref_id": "b54", "title": "Meta-Learning a Cross-lingual Manifold for Semantic Parsing", "journal": "Transactions of the Association for Computational Linguistics", "year": "2023", "authors": "Tom Sherborne; Mirella Lapata"}, {"ref_id": "b55", "title": "Results of the WMT15 metrics shared task", "journal": "", "year": "2015", "authors": "Milo\u0161 Stanojevi\u0107; Amir Kamran; Philipp Koehn; Ond\u0159ej Bojar"}, {"ref_id": "b56", "title": "Multilingual translation from denoising pre-training", "journal": "", "year": "2021", "authors": "Yuqing Tang; Chau Tran; Xian Li; Peng-Jen Chen; Naman Goyal; Vishrav Chaudhary; Jiatao Gu; Angela Fan"}, {"ref_id": "b57", "title": "OPUS-MT -building open translation services for the world", "journal": "", "year": "2020", "authors": "J\u00f6rg Tiedemann; Santhosh Thottingal"}, {"ref_id": "b58", "title": "Human evaluation of machine translation through binary system comparisons", "journal": "Association for Computational Linguistics", "year": "2007", "authors": "David Vilar; Gregor Leusch; Hermann Ney; Rafael E Banchs"}, {"ref_id": "b59", "title": "UniTE: Unified translation evaluation", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Yu Wan; Dayiheng Liu; Baosong Yang; Haibo Zhang; Boxing Chen; Derek Wong; Lidia Chao"}, {"ref_id": "b60", "title": "The ARPA MT evaluation methodologies: Evolution, lessons, and future approaches", "journal": "", "year": "1994", "authors": "John S White; Theresa A O'connell; Francis E O'mara"}, {"ref_id": "b61", "title": "End-to-end slot alignment and recognition for crosslingual NLU", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Weijia Xu; Batool Haider; Saab Mansour"}, {"ref_id": "b62", "title": "Evaluating machine translation in cross-lingual Ecommerce search", "journal": "", "year": "2022", "authors": "Hang Zhang; Liling Tan; Amita Misra"}, {"ref_id": "b63", "title": "Bertscore: Evaluating text generation with BERT", "journal": "", "year": "2020-04-26", "authors": "Tianyi Zhang; Varsha Kishore; Felix Wu; Kilian Q Weinberger; Yoav Artzi"}, {"ref_id": "b64", "title": "Neural machine translation quality and post-editing performance", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Vil\u00e9m Zouhar; Martin Popel; Ond\u0159ej Bojar; Ale\u0161 Tamchyna"}, {"ref_id": "b65", "title": "Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?", "journal": "", "year": "", "authors": ""}, {"ref_id": "b66", "title": "error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc", "journal": "", "year": "", "authors": ""}, {"ref_id": "b67", "title": "If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used", "journal": "", "year": "", "authors": " Nltk;  Spacy;  Rouge"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Graph of predictions by COMET-DA (threshold: -0.028), categorised by the metric scores in ten intervals. Task: Semantic Parsing with English parser and test language is Chinese. The bars indicate the count of examples with incorrect parses (red) and correct parses (blue) assigned the scores for the given ranges.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "The proportion of the different types of errors erroneously detected and undetected by COMET-DA for languages mentioned in Section 4.2. False positives and false negatives are computed by excluding the examples where the extrinsic task model was at fault.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Mean and Standard Deviation of the best threshold on the development set for all the language pairs in the respective extrinsic tasks. The thresholds are inconsistent across language pairs and tasks for both bounded and unbounded metrics.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "MCC scores of reference based metrics with pseudo references when gold references are unavailable at test time. Performance is worse than metrics with oracle references and reference-free metrics (Table1)", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Language codes of languages used in this work", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "MT metrics for extrinsic Dialogue State Tracking (Multi 2 WoZ) using an English-trained state tracker. Good/Bad are the number of examples in the respective labels (Not breakdown/Breakdown) for the classification task. Reported Macro F1 scores and MCC scores quantify if the metric detects a breakdown for the extrinsic task. Metrics have negligible correlation with the outcomes of the end task.", "figure_data": "Language Good / Badar 592 / 264 696 / 169 701/ 170 721 / 152 631 / 241 701 / 173 539 / 323 443 / 389 616 / 251 606 / 266 de el es hi ru th tr vi zhRandom0.023-0.002-0.0020.0170.001-0.002-0.0020.028-0.051-0.045BLEU chrF0.135 0.1600.048 0.0830.142 0.1720.098 0.0920.162 0.2020.125 0.1060.128 0.1620.097 0.0000.108 0.1730.171 0.119BERTScore COMET-DA COMET-MQM UniTE0.139 0.193 0.096 0.0680.076 0.122 0.011 -0.0310.173 0.194 0.025 -0.0020.051 0.086 0.017 -0.0140.209 0.187 0.062 0.0430.131 0.111 -0.023 0.0470.121 0.125 -0.001 -0.0060.046 0.108 -0.050 0.0560.173 0.124 0.079 -0.0170.148 0.120 0.054 -0.023COMET-QE-DA COMET-QE-MQM UniTE-QE0.178 0.099 0.0650.084 0.050 -0.0310.142 -0.013 0.0120.068 0.025 -0.0080.125 0.090 0.0350.115 -0.025 0.0690.066 0.041 0.0730.049 -0.077 0.0560.063 0.068 -0.0090.110 0.070 -0.069"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "MCC values for different metrics for extrinsic task of Extractive Question Answering (XQuaD dataset) where the model is trained on English. Good/Bad are the number of examples in the respective labels (Not breakdown/Breakdown) for the classification task. Metrics have poor performance on the classification task as a majority report MCC < 0.3", "figure_data": "MethodardeeleshiruthtrvizhGood / Bad592 / 264 696 / 169 701/ 170 721 / 152 631 / 241 701 / 173 539 / 323 443 / 389 616 / 251 606 / 266Random0.5080.5250.5120.4920.4890.5050.4900.4680.4730.498BLEU chrF0.549 0.5790.515 0.5410.564 0.5750.543 0.5460.571 0.5950.562 0.5450.556 0.5670.487 0.4800.549 0.5570.585 0.554BERTScore COMET-DA COMET-MQM UniTE0.569 0.596 0.535 0.3700.538 0.560 0.351 0.4790.586 0.571 0.307 0.3430.523 0.543 0.225 0.3140.604 0.593 0.361 0.3080.528 0.543 0.365 0.5190.561 0.561 0.330 0.3660.523 0.549 0.429 0.4380.580 0.562 0.509 0.2820.535 0.540 0.453 0.326COMET-QE-DA COMET-QE-MQM 0.549 0.575 UniTE-QE 0.3560.534 0.510 0.2170.559 0.416 0.3440.530 0.473 0.3630.550 0.420 0.3220.544 0.384 0.5340.532 0.356 0.5250.474 0.459 0.4160.530 0.509 0.2810.495 0.492 0.523"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "macro F1 scores for different metrics for extrinsic task of Extractive Question Answering (XQuAD dataset) where the model is trained on English. Good/Bad are the number of examples in the respective labels (Not breakdown/Breakdown) for the classification task.", "figure_data": "srctgtRandomBLEUchrFBERTScoreCOMET-DACOMET-MQMUniTECOMET-QE-DACOMET-QE-MQMUniTE-QEende fr pt es zh0.465 0.440 0.466 0.463 0.4290.492 0.487 0.676 0.599 0.5740.500 0.519 0.659 0.566 0.5700.45 0.467 0.614 0.564 0.5820.436 0.473 0.555 0.630 0.5900.465 0.491 0.609 0.614 0.5770.469 0.525 0.4525 0.626 0.5860.511 0.489 0.527 0.546 0.5160.474 0.525 0.500 0.535 0.5130.481 0.509 0.588 0.574 0.490de fren fr pt es zh en de pt es zh0.490 0.409 0.462 0.479 0.468 0.489 0.385 0.472 0.492 0.3840.611 0.523 0.592 0.605 0.614 0.595 0.518 0.620 0.462 0.6410.598 0.539 0.641 0.621 0.670 0.590 0.616 0.620 0.613 0.7020.623 0.515 0.638 0.569 0.571 0.607 0.587 0.565 0.512 0.6660.624 0.595 0.684 0.666 0.614 0.630 0.541 0.543 0.627 0.6670.637 0.613 0.683 0.631 0.553 0.606 0.570 0.583 0.648 0.6580.629 0.608 0.619 0.684 0.581 0.628 0.546 0.538 0.574 0.6610.556 0.592 0.645 0.596 0.524 0.597 0.503 0.549 0.594 0.5210.620 0.522 0.619 0.576 0.532 0.574 0.476 0.534 0.568 0.5020.673 0.536 0.580 0.621 0.554 0.588 0.542 0.520 0.573 0.575pten de fr es zh0.476 0.438 0.458 0.491 0.4030.629 0.550 0.546 0.640 0.6100.676 0.575 0.603 0.646 0.6900.681 0.577 0.488 0.634 0.5510.685 0.586 0.599 0.639 0.5800.655 0.594 0.495 0.639 0.5110.705 0.481 0.574 0.459 0.6210.695 0.608 0.574 0.562 0.6210.654 0.569 0.545 0.586 0.4920.526 0.501 0.645 0.509 0.591esen de fr pt zh0.455 0.455 0.453 0.500 0.3740.530 0.530 0.542 0.506 0.5620.561 0.546 0.531 0.561 0.6440.566 0.587 0.606 0.579 0.5620.605 0.540 0.564 0.554 0.6270.601 0.521 0.568 0.564 0.5870.600 0.584 0.584 0.529 0.6870.544 0.49 0.569 0.561 0.5240.564 0.486 0.560 0.566 0.4780.529 0.513 0.556 0.581 0.662esen de fr pt zh0.455 0.455 0.453 0.500 0.3740.530 0.530 0.542 0.506 0.5620.561 0.546 0.531 0.561 0.6440.566 0.587 0.606 0.579 0.5620.605 0.540 0.564 0.554 0.6270.601 0.521 0.568 0.564 0.5870.600 0.584 0.584 0.529 0.6870.544 0.490 0.569 0.561 0.5240.564 0.486 0.560 0.566 0.4780.529 0.513 0.556 0.581 0.662"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "MT Metric performance on F1 for extrinsic semantic parsing (MultiATIS++SQL) with the parser trained in src language.", "figure_data": "srctgtRandomBLEUchrFBERTScoreCOMET-DACOMET-MQMUniTECOMET-QE-DACOMET-QE-MQMUniTE-QEende fr pt es zh0.012 -0.043 -0.067 0.002 -0.0900.008 -0.024 0.353 0.203 0.1520.016 0.039 0.328 0.133 0.146-0.096 -0.066 0.231 0.152 0.173-0.122 -0.020 0.201 0.279 0.187-0.000 -0.001 0.228 0.229 0.188-0.06 0.050 0.114 0.252 0.1720.025 -0.021 0.089 0.110 0.060-0.021 -0.021 0.209 0.107 0.035-0.027 0.017 0.187 0.166 0.078deen fr pt es zh-0.003 -0.007 -0.070 -0.035 -0.0630.226 0.046 0.184 0.230 0.2410.210 0.078 0.300 0.242 0.3400.251 0.033 0.312 0.200 0.1500.263 0.196 0.394 0.332 0.2420.328 0.226 0.406 0.264 0.1240.303 0.243 0.302 0.370 0.2580.161 0.185 0.331 0.206 0.0540.250 0.044 0.295 0.181 0.0880.349 0.078 0.206 0.256 0.112fren de pt es zh0.006 -0.087 -0.023 -0.015 -0.1160.194 0.099 0.242 0.053 0.3110.182 0.237 0.240 0.233 0.4130.220 0.180 0.177 0.118 0.3730.269 0.105 0.133 0.283 0.3650.229 0.155 0.170 0.300 0.3470.262 0.125 0.117 0.151 0.3900.195 0.026 0.100 0.229 0.1430.148 -0.043 0.115 0.177 0.0510.178 0.086 0.106 0.153 0.248pten de fr es zh0.013 -0.093 0.013 0.009 0.0610.315 0.112 0.100 0.286 0.2210.365 0.181 0.222 0.293 0.4490.378 0.159 0.061 0.278 0.2530.372 0.188 0.218 0.278 0.1610.320 0.190 0.030 0.288 0.0480.414 0.216 0.155 0.142 0.2420.402 0.183 0.053 0.076 0.0000.310 0.150 0.090 0.243 -0.0110.175 0.007 0.291 0.025 0.212esen de fr pt zh-0.063 -0.075 -0.065 0.014 -0.0050.080 0.092 0.140 0.012 0.1480.179 0.169 0.118 0.144 0.2890.136 0.175 0.214 0.169 0.1540.214 0.082 0.129 0.148 0.2540.208 0.047 0.140 0.143 0.1730.200 0.186 0.196 0.110 0.3930.095 -0.013 0.150 0.160 0.1020.128 -0.024 0.124 0.133 0.0000.058 0.033 0.112 0.166 0.363zhen de fr pt es-0.034 0.008 -0.045 -0.130 -0.0150.283 0.260 0.204 0.264 0.3400.218 0.274 0.238 0.357 0.3750.252 0.302 0.343 0.430 0.4460.302 0.314 0.330 0.327 0.4070.290 0.347 0.247 0.295 0.4170.333 0.273 0.328 0.307 0.2130.264 0.139 0.222 0.171 0.1390.324 0.199 0.259 0.205 0.2290.232 0.169 0.287 0.134 0.211"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "MT Metric performance on MCC for the classification task with extrinsic semantic parsing (Multi-ATIS++SQL) with the parser trained in src language.", "figure_data": ""}], "formulas": [], "doi": "10.18653/v1/2020.acl-main.421"}