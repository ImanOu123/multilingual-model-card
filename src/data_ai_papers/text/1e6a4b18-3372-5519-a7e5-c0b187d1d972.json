{"title": "MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks", "authors": "Lei Zhang; Yuge Zhang; Kan Ren; Dongsheng Li; Yuqing Yang; Microsoft Research; Ren Kan", "pub_date": "", "abstract": "The field of machine learning (ML) has gained widespread adoption, leading to significant demand for adapting ML to specific scenarios, which is yet expensive and non-trivial. The predominant approaches towards the automation of solving ML tasks (e.g., AutoML) are often time-consuming and hard to understand for human developers. In contrast, though human engineers have the incredible ability to understand tasks and reason about solutions, their experience and knowledge are often sparse and difficult to utilize by quantitative approaches. In this paper, we aim to bridge the gap between machine intelligence and human knowledge by introducing a novel framework MLCopilot 1 , which leverages the state-of-the-art large language models to develop ML solutions for novel tasks. We showcase the possibility of extending the capability of LLMs to comprehend structured inputs and perform thorough reasoning for solving novel ML tasks. And we find that, after some dedicated design, the LLM can (i) observe from the existing experiences of ML tasks and (ii) reason effectively to deliver promising results for new tasks. The solution generated can be used directly to achieve high levels of competitiveness.", "sections": [{"heading": "Introduction", "text": "Past decades have witnessed a great advance and rapid development of machine learning (ML), but ML algorithms are still notoriously hard to configure . For specific tasks, configuring and conducting corresponding ML solutions is non-trivial, which thus requires extensive human labor. Many challenges arise in developing practical ML solutions. First, it is of large human efforts considering the large space of ML solutions, such as feature engineering, model design, optimization details, etc. Second, ML algorithms are sensitive to even minor changes of the task context. As a result, even the same algorithm may need to be reconfigured for different application tasks. Last but not least, transferring successful experiences across different tasks is also intractable, which demands high-level reasoning abilities of human experts to derive reasonable solutions for novel tasks.\nThe predominant approaches that relieve the human effort of algorithm configuration have been some automation mechanisms such as AutoML (Automated Machine Learning) . One major branch of AutoML formulates the problem as black-box optimization, and resorts to some optimization approaches such as Bayesian optimization (BO) (Frazier, 2018) to solve it. Though the obtained results are shown to be promising, it is time-consuming to spawn multiple trials, especially for large datasets and complex tasks. Moreover, AutoML does not follow the natural pattern of ML development that humans are accustomed to, which leaves a huge gap for humans to understand and control the whole process. Specifically, it is either difficult to explain the behavior of auto-tuning, or intractable to incorporate human prior such as the knowledge of the model architectures into the process, making it less flexible for human developers. Furthermore, the ML solutions derived by these optimization-based methods may only fit to the specific domains, and the transferring ability of these results also remains an open problem Yan et al., 2022).\nContrarily, we notice two tendencies in how humans approach an ML task. Instead of jumping into solving the new task directly, humans often try to comprehend the task at hand and draw from their past experiences on relevant tasks. Additionally, humans recall their knowledge, which may have came from a textbook or prior experiences. This process differs significantly from the auto-mated approach mentioned earlier, which leads us to a natural question: can we leverage both machine intelligence and human design patterns to improve our ability to solve ML tasks? The advances of Large Language Models (LLM) (Brown et al., 2020;Chowdhery et al., 2022;Ouyang et al., 2022) have illustrated tremendous promising performance in mimicking human behaviors on conversationbased tasks. It seems plausible to utilize the power of LLM to address ML problems in a more humanlike way.\nNevertheless, several challenges remain when incorporating LLMs to achieve this goal. First, we discovered that LLMs have trouble performing ML tasks based solely on the task description, in which case the performance is no better than random generation. Attempting to leverage historical ML experience, we found that the data often reside in heterogeneous formats (e.g., code, configs and logs), which need to be canonicalized into formats that are acceptable to LLMs. Moreover, the amount of information that can be incorporated into in-context learning (Brown et al., 2020) is quite limited, and thus some retrieval strategy is desired to make the best out of it. Finally, deriving a ML solution based on historical experience is in its essence a mathematical thinking and logical reasoning problem (Patel et al., 2021), which necessitates some mechanisms to reasoning over knowledge.\nIn this paper, we explore and present a novel framework MLCopilot, which leverages LLMs to suggest solutions for novel real-world ML tasks, based on the existing experiences from historical tasks. We decompose the problem into offline and online stages. In the offline stage, MLCopilot canonicalizes historical data and creates an experience pool. LLMs are then used to elicit valuable knowledge from historical experience. In the online stage, MLCopilot retrieves experiences from the most relevant tasks from the experience pool, given the description of the target task. It then interacts with LLMs to obtain multiple suggested ML solutions in one round. We demonstrate that, with a well-designed framework, LLMs can not only elicit meaningful knowledge from historical experiences but also provide reasonable and competitive ML solutions for novel tasks.\nOur work presents a three-fold contribution, which can be summarized as follows. (i) To the best of our knowledge, we are the first to utilize LLMs as a tool to generate solutions for new ML tasks. (ii) A novel retrieve-and-prompt framework has been proposed to solve ML tasks almost instantaneously, without any time-consuming searching or optimization. (iii) We leverage the text understanding and generation capabilities of LLMs to produce interpretable 2 results for ML tasks. This approach has shown comparable or even better performance on a variety of real-world ML benchmarks.\n2 Related Work", "publication_ref": ["b13", "b44", "b27", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "Large Language Models", "text": "Large language models (LLMs) are neural networks of significant sizes (typically containing tens or hundreds of billions of parameters). They have gained the incredible ability of processing and generating natural languages, due to the training on massive amounts of text data (Radford et al., 2018(Radford et al., , 2019Brown et al., 2020). Studies show that LLMs beyond a certain scale have \"emergent abilities\" (Wei et al., 2022), and perform remarkably well in applications such as chatbots, machine translation, and text summarization (Zhao et al., 2023;Touvron et al., 2023).\nWhile LLMs have illustrated superior performance on natural language understanding and human-like text generation, they are still quite limited for complicated tasks that require reasoning (Huang and Chang, 2022) and mathematical skills (Patel et al., 2021;Thawani et al., 2021;Han et al., 2022;Saxton et al., 2019). The stream of task automation Shen et al., 2023) investigated a general approach to decompose a task into a sequence of sub-tasks, but they are orthogonal to our work, since they did not take into past experience or knowledge from other tasks when planning a new task.", "publication_ref": ["b30", "b31", "b42", "b15", "b28", "b35", "b14", "b32", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "Machine Learning and AutoML", "text": "Machine learning (ML) is a subfield of artificial intelligence (AI) that involves developing optimization algorithms that can learn from data and make predictions (Bishop and Nasrabadi, 2006) or decisions (Sutton and Barto, 2018). Although ML has been successful in many real-world applications, designing an effective ML solution for a new task can be challenging due to the numerous design choices required. AutoML  emerges as an approach to alleviate the manual effort involved. Popular methodologies include neu-  ral architecture search (NAS) (Pham et al., 2018), meta-learning (Andrychowicz et al., 2016), and Bayesian optimization (Frazier, 2018).\nAutoML is able to reach beyond-human levels in solving ML tasks, but it still faces a few drawbacks. First, most AutoML methods require many rounds of trial-and-error, which can be timeconsuming. Second, AutoML typically searches from scratch for a new task and neglects the experience on previous tasks. Finally, most AutoML methods are not interpretable due to their blackbox nature, which excludes human understanding. Some methods may address one or two of the drawbacks, but not all of them. For example, the stream of transferrable AutoML research seeks to leverage past experience to assist in searching for new tasks (Bardenet et al., 2013;Wistuba et al., 2016;Mittal et al., 2020;Yan et al., 2022;Wang et al., 2021a), but they lack interpretability and most of them only work for specific types of tasks. A recent study  aims to use Transformer model (Vaswani et al., 2017) with large-scale pretraining to deal with broader types of tasks, but it is still non-interpretable and a cost search remains required for new tasks. Most recently, (Zheng et al., 2023) tried to search for neural architectures using GPT-4 (OpenAI, 2023). It prompts LLM to explain rationales, but it only explores model architectures, and still requires costly trial-and-error.", "publication_ref": ["b3", "b29", "b0", "b13", "b2", "b43", "b24", "b44", "b39", "b38", "b48"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "The goal of MLCopilot is to assist humans in solving complex ML problems. Generally speaking, given a task which is a real-world problem for ML models to tackle, the goal of ML development is to conduct a concrete solution. The solution can be either a pipeline, configuration, or code snippet, based upon which a concrete ML model could be learned to handle the target task. The solution is also a particular sample within a complicated solution space that involves various design choices. These choices are mutually correlated and the outcome of different alternatives often influences the others and eventually affects the final performance of the overall ML solution.\nTo create reasonable ML solutions for new tasks, we can draw on experiences from previous relevant tasks. MLCopilot is designed to use historical experiences for knowledge elicitation and effectively conduct effective solutions for the given novel ML task (details in \u00a7 4). To improve comprehension and clarity, we summarize the terminologies with descriptions and examples in Table 1.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "MLCopilot", "text": "In this section, we present MLCopilot, with the formulation of the main problem and the overall architecture of our method. Then, we will describe some key components of MLCopilot in detail, including target task description, retrieval, canonicalization, and knowledge elicitation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Overall Framework", "text": "As discussed previously, to unleash the power of LLMs in solving complex ML tasks, explicitly leveraging historical experience is crucial. However, utilizing past experience is not straightforward considering the heterogeneous data format and the huge number of records. Therefore, our technical design mainly focuses on addressing two problems: (i) how to comprehend and exploit the abundant raw experiences; (ii) how to effectively solve ML tasks based on the result of (i).\nThe main idea behind MLCopilot is knowledgebased reasoning, that is to leverage LLMs to conduct reasoning and task solving based on the previous knowledge, which has been analyzed and elicited from past experiences. To this end, ML-Copilot contains two stages, including offline and online parts, both of which have been visually illustrated in Figure 1. In the offline stage, LLM has been incorporated to analyze the canonicalized historical experience data and elicit useful knowledge. And in the online stage, the user will query ML-Copilot, which is also built upon LLM, to obtain a suitable ML solution for the novel task.    ", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Offline Stage: Understanding and Reasoning", "text": "We first present the data settings and describe the corresponding preprocessing procedure briefly. Let H = {D 1 , . . . , D N H } be the raw historical data with N H previous records. The i-th record D i is defined as a three-element tuple \u27e8T i , S i , M i \u27e9 which contains a task T i \u2208 T, a solution S i \u2208 S, and the evaluated metric performance M i , e.g., classification accuracy.\nNote that, the historical data H may have heterogeneous and diverse formats. For example, the task can be described in the natural text, while the solution can be a JSON configuration, a row of tabular data, or code snippets. An experience pool P E is constructed, to canonicalize the data and store them as experiences P E = {E 1 , . . . , E N E }, where E j = C(D j ), and C(\u2022) is the canonicalization function. For the simplicity of notations, we assume all the solutions within P E come from a universal solution space. It is easy to extend the framework to scenarios with multiple solution spaces.\nKnowledge, is high-level information acquired from the experience (Dictionary, 1989), and we leverage LLM to elicit knowledge from the constructed experience pool, in the offline stage. Knowledge is the easy-to-understand summarization of previous ML experiences, which will further be utilized when inferring the final solution in the online stage. To be specific, a subset of experience is first sampled from the experience pool, then a LLM is incorporated to read and understand the ex-perience data, allowing us to \"elicit\" knowledge K from it. The process of elicitation is formulated as K = I K (P E ; LLM), which is an iterative process by interacting with LLM along with post-validation on the obtained knowledge. The detailed process of elicitation is discussed in \u00a7 4.5. All the generated knowledge is stored in a knowledge pool P K = {K 1 , . . . , K N K } with totally N K items.\nThe obtained experience pool and knowledge pool will be further utilized by MLCopilot in the online stage, to conduct reasonable, promising, and competitive ML solutions for novel tasks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Online Stage: Retrieving and Solving", "text": "The online stage of MLCopilot aims to conduct reasoning and task solving based on the off-theshelf information obtained from the offline stage. Specifically, given the user query with a task description, MLCopilot will respond with the corresponding reasonable ML solutions via retrieving relevant experiences and knowledge, and interacting with LLM by a curated prompt, in one round.\nWhen a user comes with a novel target taskT , which has never been seen in history, MLCopilot first retrieves the relevant experiences of other relevant tasks as demonstrations\u1ebc = R E (T , P E ), where R E (\u2022) is the retrieval functions for the experience pool. It also retrieves knowledgeK = R K (T , P K ) to guide the response on the new task, where R K (\u2022) is the retrieval functions for the knowledge pool. MLCopilot finally generates a recommended solution by invoking LLM once:\nS = LLM(T ,\u1ebc,K).\nThe framework is shown in Figure 1, where we illustrate an example of how MLCopilot handles a task to classify a brain tumor by leveraging previous experiences and knowledge. Next we will introduce the dedicated components in detail.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Task Description in Natural Language", "text": "Firstly, we show how the target task is described in our framework, which is the input to MLCopilot. The prior works (Feurer et al., 2015;Wang et al., 2021a) usually use meta-features designed by humans for specific types of tasks (e.g., the number of samples in the dataset) to describe a task, so as to ease the difficulty of comprehending the task. However, such design might degenerate the ability of LLM to generalize to new types of tasks. We believe that task description in natural language is more straightforward to users. It is also agnostic to task types, and does not require heuristics to design meta-features. As such, we adapt the task description without any feature engineering, and users can freely describe dataset names, characteristics, domain-specific constraints, and more. Furthermore, our experiments ( \u00a7 D) illustrated that incorporating a natural language user interface helps recall and leverage previous knowledge contained in the training corpus of LLMs.", "publication_ref": ["b12", "b39"], "figure_ref": [], "table_ref": []}, {"heading": "Retrieval", "text": "The retrieval technique has been used to (i) gather some demonstrations of the historical ML solutions to the relevant tasks and (ii) apply useful knowledge previously to further motivate and prompt the LLM to better solve the target ML task.\nWe first discuss how to retrieve experience R E as demonstrations\u1ebc. Intuitively, the most helpful experience in solving a new task should come from the most relevant tasks. The key question then becomes how relevance is defined. To this end, we first embed the task description by invoking a language model E (e.g., GPT-3 (Brown et al., 2020)) to generate a embedding vector of the textual content. Given a new taskT , MLCopilot retrieves the most relevant historical tasks from the experience pool P E by calculating the cosine similarity between the embeddings of the new task and the stored tasks. The corresponding experience to these embeddings will serve as demonstrations\u1ebc, as calculated as\nE = R E (T , P E ) = arg top-k \u27e8T,S,M \u27e9\u2208P E E(T ) \u2022 E(T ) |E(T )| \u2022 |E(T )| ,\nwhere the most relevant k entries of experience will be retrieved for subsequent demonstration. The retrieval of knowledge R K is based on matching of solution space -retrieving all the knowledge that are elicited from the same solution space as the new task, from the knowledge pool P K . This simplicity of R K is due to the fact that the knowledge produced in the offline stage of MLCopilot is concise and of high quality, whose generation procedure will be discussed in \u00a7 4.5.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Canonicalization", "text": "As mentioned previously, the data of raw ML experience are heterogeneous and of diverse formats. While some of them (e.g., task descriptions) have already been in natural text format, the ML solutions and the corresponding metric performance are often expressed in structured configurations, tabular formats, or programming languages. More importantly, they might even contain a lot of numbers, which language models or even LLMs are not good at processing and understanding (Thawani et al., 2021;Han et al., 2022;Saxton et al., 2019). To better unleash the power of LLM, we canonicalize all the data to express it in natural language.\nThe essential part of canonicalization is to convert the raw data into a well-formed natural language, as shown in the left part of Figure 2. Other than unifying the solutions in diverse formats, a crucial technique is number discretization which avoids feeding numbers to LLM directly. We follow (Thawani et al., 2021) that discretizes continuous numerical data into several intervals, and mapping each value within each interval to the same discrete value. To minimize performance loss, we discretize each numerical value based on the corresponding distribution and percentile points. More details can be found in \u00a7 5.1.", "publication_ref": ["b35", "b14", "b32", "b35"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Knowledge Elicitation", "text": "With the canonicalized experience stored in pool P E , MLCopilot can then elicit knowledge, to better support the online stage for solving novel ML tasks. It is important to note that knowledge elicitation occurs offline, prior to serving user tasks. The approach involves the following steps: (i) constructing a prompt that consists of a random subset of the experience pool (to avoid bias towards certain tasks), along with an inquiry that asks for analysis and summary; (ii) sending the prompt to LLMs to generate a knowledge \"candidate\"; (iii) validating the candidate on experience pool. The flow of this process is illustrated in the right part of Figure 2 (pseudo-code in \u00a7 A).\nWe elaborate on the validation step, which we call automated post-validation after requesting knowledge from the LLM. This step is designed to alleviate the hallucination issue (Ji et al., 2023) and raise the quality of generated knowledge. It tests the knowledge by using it to solve a set of validation tasks. If the generated knowledge is found invalid (e.g., due to hallucination), it adjusts the generation settings, such as the order of experiences in the prompt, tone of hypophora questions, and parameters of LLM invocation, and let LLM regenerate knowledge. This iterative process continues until the performance on the validation tasks has been converged, or the invocation has reached the maximum number. This process can be represented formally as K = I K (P E ; LLM).\nWe argue that our knowledge elicitation is novel and different from prior works of knowledge extraction  or knowledge generation  in natural language processing. Firstly, our knowledge is obtained from heterogeneous resources using general text completion models, without requiring predefined templates or complicated pipelines. Secondly, acquiring knowledge for ML tasks requires analysis, summarization, and high-level reasoning, which is significantly more challenging than simply extracting simple facts . Finally, the knowledge is anchored in experience data, rather than purely based on LLM's pre-training corpus , which makes the framework scalable to new scenarios.\nKnowledge elicited by MLCopilot can be beneficial not only for LLMs but also for human ML practitioners. Since the knowledge is expressed in natural language, it could potentially serve as a cookbook for ML developers. In an effort to share our findings and inspire future ML research, we have released all the knowledge obtained so far (see \u00a7 E). Hopefully this will reveal some of the \"secret sauce\" behind how ML works and promote knowledge sharing within the community.", "publication_ref": ["b18"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Experiment", "text": "We evaluate MLCopilot on a series of benchmarks, aiming to answer the following research questions: (i) Can MLCopilot outperform traditional approaches or simple interactions with LLMs? (ii) How important are individual techniques in ML-Copilot, e.g., knowledge and experience? (iii) Is the elicited knowledge informative and reasonable?", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiment Setup", "text": "Implementation details. The current implementation of MLCopilot involves maintaining dedicated experience and knowledge pools for each solution space. The historical data is sourced from the benchmarks described below, while task descriptions are crawled from benchmark websites. Numerical values in the data are discretized into five levels: \"very low\", \"low\", \"medium\", \"high\", and \"very high\". The precise value of each level is determined by analyzing the statistics of the best solutions within the solution space (see detailed analysis in \u00a7 5.3). We interact with the general-purpose GPT-3.5 model 3 (code-named \"text-davinci-003\", without additional fine-tuning), and \"text-embedding-ada-002\" to obtain embeddings for task descriptions. (Results with other LLMs can be found in \u00a7 D.3.) The temperature is set to 0 to minimize randomness. Additional details regarding prompt design can be found in \u00a7 C. Benchmarks. We selected benchmarks that have established a predetermined solution space for all possible solutions and provided performance metrics for all the solutions in the solution space (either through a lookup table or surrogate). We conducted experiments using MLCopilot on three ML benchmarks: HPO-B (Arango et al., 2021), PD1 (Wang et al., 2021b), and HyperFD (Yan et al., 2022). These benchmarks comprise numerous ML tasks and datasets, covering a broad spectrum of scenarios such as tabular data classification and regression, image classification, and object detection. Details can be found in \u00a7 B.1. Evaluation metrics. In each experiment, every compared method makes three attempts to predict successful solutions for an unseen task. Solutions are evaluated in the order they were suggested. Metric@t, where t \u2265 1, is defined as the best metric performance achieved among the first t suggested solutions. The reported performance metrics are averaged over at least 5 random seeds.", "publication_ref": ["b40", "b44"], "figure_ref": [], "table_ref": []}, {"heading": "Main Results", "text": "We show the performance of MLCopilot in Table 2. Baselines we compared with include:\n\u2022 Traditional AutoML or meta learning methods, Method HPO-B \u2191 PD1 \u2191 HyperFD (Rank \u2193 AP \u2191) nAcc@1 nAcc@2 nAcc@3 nAcc@1 nAcc@2 nAcc@3 Rank@1 Rank@2 Rank@3 AP@1 AP@  including Random, ASKL (Feurer et al., 2015), Constant (Bardenet et al., 2013;, TST-M (Wistuba et al., 2016), Hyper-STAR (Mittal et al., 2020), HyperFD (Yan et al., 2022) and FLAML-Zero (Wang et al., 2021a). Details described in \u00a7 B.2.\n\u2022 LLM-ZS directly prompts LLM to generate a zero-shot solution based solely on the task description, which is similar to using tools such as GitHub Copilot 4 or Amazon CodeWhisperer 5 .\n\u2022 LLM-FS uses the few-shot prompt technique (Brown et al., 2020) by adding some demonstrations to the prompt to enable incontext learning. The demonstrations are randomly selected from our canonicalized experience pool. Unlike MLCopilot, LLM-FS does not have access to advanced techniques such as experience and knowledge retrieval.\nMLCopilot achieved the highest normalized accuracy (nAcc) across all three trials. The improvement is particularly significant for the first attempt (nAcc@1). It is remarkable that LLM-FS has already surpassed all the traditional baselines, suggesting the large capability of LLMs on ML tasks.\nOn PD1, Normalized accuracy (nAcc) are in range [\u22122, 2] following the setting of (Wang et al., 2021b). MLCopilot remains the best out of all methods compared. Notably, \"Constant\" baseline almost outcompetes all other baselines, which casts doubt on the effectiveness of the task similarities measured by other baselines. Meanwhile, both LLM-ZS and LLM-FS fail on PD1, indicating PD1 is more challenging for LLMs.\nFor HyperFD, Following (Yan et al., 2022), we use average precisions (AP) (the higher the better) and rankings (within [1,216], the lower are better) to measure the performance. Similar to what 4 https://github.com/features/copilot 5 https://aws.amazon.com/codewhisperer/ was observed in HPO-B, LLM-FS achieves comparable performance to most baselines with a few demonstrations. It is expected that the performance of LLM-FS would improve with the inclusion of techniques from MLCopilot. However, it is worth noting that HyperFD is a private benchmark and its benchmark was released after the knowledge cutoff of GPT-3.5, making it unlikely that LLM has memorized the best solutions on this benchmark.", "publication_ref": ["b12", "b2", "b43", "b24", "b44", "b39", "b40", "b44"], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Ablation study", "text": "In the ablation study, we use nAcc@1 (or Rank@1) as the main metric for comparisons. Study of retrieval. The first question we are trying to answer is whether retrieving experience and knowledge are necessary -what happens if either of them is missing from the prompt sent to LLM? The results are shown in Table 4. While the absence of knowledge leads to a reduction in performance, the absence of demonstrations leads to a complete collapse. Examining the knowledge generated ( \u00a7 E), we found it often contains vague claims such as \"The size of the dataset can influence the configuration of eta\" (HPO-B, Space 5971). The knowledge did not clarify what is the \"influence\", which is why experience is still much needed even with the presence of knowledge.\nWe then compare different retrieval methods (\"Pipeline MLCopilot\" columns in Table 3). ML-Copilot retrieves the most relevant tasks based on the embedding of textual description. Alternatively, we can (i) measure similarities based on metafeatures; (ii) simply retrieve experiences randomly. Shown in Table 3, both meta-feature and text embedding consistently outperform random retrieval.\nWhen choosing between meta-features or text embedding, we believe that the latter has demonstrated advantages over manually designed metafeatures. This is partly due to the fact that the performance of meta-features depends largely on   the quality of their design. While meta-features have been shown to be promising for tabular datasets where they are well-studied and carefully designed (Feurer et al., 2015;Wang et al., 2021a), the design of meta-features for complex tasks in PD1 is non-trivial. In contrast, the text embedding approach has the additional advantage of not requiring any manual design for new types of tasks.\nFurthermore, text embedding is more promising for handling new and varied tasks, while meta-feature for new tasks is not easily scalable. Nevertheless, we would like to emphasize that the key factor is not solely the method of retrieving experience, but rather how the retrieved experience is utilized. When the retrieved experience is used directly, as done in ASKL, all retrieval strategies perform poorly. In contrast, MLCopilot has the ability to not only retrieve relevant experience, but also provide guidance through elicited knowledge and leverage the power of LLMs. Study of canonicalization. As shown in Table 6, the performance suffers considerably without discretization as sending continuous numerical values directly to LLM is not feasible. Furthermore, it is crucial to compute the split points based on the statistics of the best solutions. If the range is expanded to include all possible values, the split point may not fall on the sensitive points, resulting in subpar performance. This is demonstrated by \"On All\" in Table 6, which performs even worse than no discretization at all 6 . Study of knowledge. In Table 5, we conducted an ablation study to evaluate the impact of knowledge on our method. The results show that, removing knowledge retrieval in the online stage of our 6 Please note that HyperFD is not included in the ablation study as it adopts a discrete solution space.   method results in a significant decrease in the final performance. This is because knowledge is instrumental in helping LLMs arrive at the most effective solutions. Furthermore, the post-validation in elicitation procedure in the offline stage of MLCopilot also plays a vital role in enhancing its usefulness.\nBased on our qualitative study on the generated knowledge (see \u00a7 E), we found that, the knowledge serves as a helpful summary of past ML experiences while providing guidance on how to adjust parameters and settings based on task characteristics. We observe that post-validation significantly reduces the chances that trivial, vague, or hallucinated knowledge is produced, although such knowledge is still sometimes observed. For example, in the case of \"UniRef50\" task with \"Transformer\" model on PD1, the knowledge contains certain numerical examples that were not part of the demonstrations and instead the result of hallucinations.", "publication_ref": ["b12", "b39"], "figure_ref": [], "table_ref": ["tab_8", "tab_7", "tab_7", "tab_11", "tab_11", "tab_10"]}, {"heading": "Conclusion", "text": "In conclusion, this paper proposes MLCopilot, a framework that unleashes the power of LLMs to solve practical ML tasks. MLCopilot showcases the versitility of LLMs, that it can handle not only text-related tasks, but also tasks involving heterogeneous inputs and intricate reasoning. We believe this represents a significant advancement in expanding the scope of LLM applications to a broader spectrum of complex problems.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ethical considerations", "text": "The architecture of MLCopilot is meticulously engineered to ensure that the solutions it recommends always remain within the bounds of the solution space provided by the user. As a result, it acts as a safeguard against the generation of unethical solutions, provided that the defined solution space adheres to ethical standards.\nHowever, the foundational techniques outlined in this paper, including experience retrieval and knowledge elicitation, possess broader applicability across various scenarios beyond machine learning, such as task automation ) and scientific research (Boiko et al., 2023). In these contexts where the solution space extends beyond the constraints of a strictly-defined machine learning problem and where Large Language Models (LLMs) exhibit inherent limitations, the potential for unpredictability arises. Therefore, it becomes imperative to exercise ethical prudence when deploying MLCopilot in diverse cases.", "publication_ref": ["b4"], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "Potential data leakage. Since LLMs are trained on large corpus of data from Internet, it is likely that the benchmarks (especially HPO-B based on OpenML) have already been encountered during the pre-training phase of LLMs. To mitigate this potential bias, we conducted an evaluation of ML-Copilot on HyperFD (Yan et al., 2022). It is worth noting that the HyperFD dataset was introduced in a paper published after the knowledge cutoff date of GPT-3.5, and the dataset itself remains private. We empirically reveal that MLCopilot exhibits robust performance on the HyperFD dataset.\nFurthermore, our findings indicate a significant performance enhancement when the data is canonicalized (Table 6). If the data were indeed memorized during the pre-training process, LLMs would likely benefit from access to unaltered, raw data. These results provide valuable supporting evidence for the assertion that the capabilities of LLMs extend beyond mere memorization. They encompass a broader spectrum of cognitive skills, including mathematical reasoning and logical thinking. Distinction from AutoML methods. MLCopilot is not intended to serve as a replacement for established AutoML approaches. The distinction is grounded in the inherent limitations of Large Language Models (LLMs) when it comes to performing mathematical computations, as illustrated in recent work (Imani et al., 2023). Consequently, it is improbable that MLCopilot would surpass stateof-the-art Bayesian optimization methods in the pursuit of superior solutions. In Table 2 we terminated our evaluation at t = 3 (i.e., three solutions), as we observed that performance reached a point of saturation with further increases in t.\nWe argue that the true value of MLCopilot lies in the following facets: (i) it accepts arbitrary types of task descriptions; (ii) it leverages ML experiences from diverse sources, encompassing both pretraining and prompting; (iii) it exhibits an exceptional ability to rapidly produces multiple out-of-the-box solutions for a novel task. Consequently, we envision the possibility of combining MLCopilot with existing AutoML methods, opening up an intriguing avenue for future exploration. Robustness of MLCopilot. As MLCopilot has the ability to accommodate heterogeneous formats of inputs, it is worth discussing the robustness of MLCopilot in the wild. This consideration extends to situations where users submit poorly-formatted task descriptions and when the experience pool includes data with noisy accuracy labels or flawed canonicalization. A detailed assessment of ML-Copilot's robustness is presented in \u00a7 D.\nThe experiments conducted shed light on the system's robustness against certain challenges (e.g., the choice of LLMs and task description formats). But it is still important to note that its performance can degrade under specific conditions, such as when dealing with a severely limited prompt context window length.", "publication_ref": ["b44", "b17"], "figure_ref": [], "table_ref": ["tab_11", "tab_5"]}, {"heading": "A Algorithms", "text": "We summarize our method as algorithm 1 and algorithm 2.  (Vanschoren et al., 2014). Each space has a fixed ML algorithm such as random forest (Breiman, 2001), SVM (Cortes and Vapnik, 1995), or XGBoost (Chen and Guestrin, 2016), and the goal is to determine the optimal configuration of the algorithm for a given dataset. HPO-B also provides successful configurations from past tasks, which are canonicalized into experience in our case. Additionally, they have released surrogate models to expedite the evaluation of solutions that have not been attempted before.\nThe final benchmark performance is determined by averaging the normalized accuracy (nAcc) across all datasets, following the normalization protocol in (Arango et al., 2021).\nPD1. The PD1 Neural Net Tuning Dataset is proposed by HyperBO (Wang et al., 2021b), consisting of 24 classification tasks, covering image classification, next token prediction, and translation. Each task is associated with a predefined neural network (CNN (Krizhevsky et al., 2017) or transformer (Vaswani et al., 2017)), and has four configurable parameters of a SGD optimizer with Nesterov momentum (Nesterov, 1983). Due to the high cost of training neural networks, evaluating the solutions suggested by MLCopilot by running them in real-time is not feasible. So we created a surrogate model to predict the performance of suggested solutions for each task (see \u00a7 B.4 for details). As per (Wang et al., 2021b), we report normalized accuracy (nAcc).\nHyperFD. HyperFD (Yan et al., 2022) is a benchmark designed to optimize the performance of a neural face detector on an unseen dataset by properly configuring data augmentation, neural architecture, loss function, and training recipe. For this purpose, they formulated a solution space and provided both average precision (AP) and rank for every possible solution on all datasets. The benchmark was published after the claimed knowledge cutoff of GPT-3.5 (September 2021), and it has not been publicly released yet, making it unlikely that it has appeared in the training data of LLM.", "publication_ref": ["b37", "b5", "b10", "b7", "b1", "b40", "b20", "b38", "b25", "b40", "b44"], "figure_ref": [], "table_ref": []}, {"heading": "B.2 Compared Baselines", "text": "Details about the traditional baselines used in our experiment are described below.\n\u2022 Random method randomly generates a solution.\n\u2022 ASKL (Auto-sklearn 1.0) (Feurer et al., 2015) finds the most similar tasks based on manually selected meta-features of tasks and directly uses the best solutions on them.\n\u2022 Constant (Bardenet et al., 2013;) (a.k.a. Average) uses a constant set of solutions for any new task. The produced set of solutions is the one with the best average performance on the historical tasks. This method is straightforward and has no specific literature reference. We reference the two literatures for \"Constant\" as they also adopt a similar baseline for comparison.\n\u2022 TST-M (Wistuba et al., 2016) employs Gaussian processes to approximate the performance of solutions in the solution space for each task. When a new task is encountered, it combines performance predictions of solutions for different tasks and predicts the performance of each solution on the new task by averaging predictions on history tasks weighted by task similarities.\n\u2022 HyperSTAR (Mittal et al., 2020) trains a performance predictor for a joint encoding of solution and task features. HyperSTAR is originally built for vision tasks. To adapt it for non-image tasks, we incorporate handcrafted meta-features as task features.\n\u2022 HyperFD (Yan et al., 2022) is a method specifically designed for the HyperFD benchmark, which uses a sophisticated meta-feature extractor for neural face detection tasks. However, it is not a general-purpose method and is not designed to work with other types of tasks.\n\u2022 FLAML-Zero (Wang et al., 2021a) is a recent method that generates a portfolio of ML solutions through offline meta-training, minimizing overall regret across meta-training tasks. It uses meta-features to link new tasks to existing ones based on their similarity.", "publication_ref": ["b12", "b2", "b43", "b24", "b44", "b39"], "figure_ref": [], "table_ref": []}, {"heading": "B.3 Post-validation", "text": "The post-validation step that we have incorporated draws inspiration from established practices within machine learning, where a dedicated validation set is employed to enhance model performance. In our specific case, we allocate 10% of the training meta-dataset for validation purposes, allowing us to systematically filter and select the most valuable generated knowledge. This additional layer of validation contributes significantly to adcressing hallucination-related issues. Detailed steps of post-validation include a loop of: sampling hypophora question, sampling temperature, generating knowledge candidates, validating candidate knowledge, and an earlystopping mechanism that determines stagnation. This is described in algorithm 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.4 Building Surrogate Model for PD1", "text": "The metric in PD1 contains many NaN values, which correspond to network training divergence. For benchmarking purposes, it is more important to be able to distinguish the top-performing solutions, i.e., solutions above medium accuracy. To accomplish this, we adopt a two-stage surrogate approach. We use a classification model to distinguish the top-performing solutions, and then two regression models: one specially optimized for the top-performing solutions, and the other one for all solutions. We utilize XGBoost (Chen and Guestrin, 2016) for building classifiers and regressors. Default parameters are used for those models.", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "C Prompt Design", "text": "We show two example prompts used on HPO-B. One of them is used for the online stage, as shown in Table 7, when MLCopilot receives a task description given by the user and sends it to LLM to obtain a recommended solution.\nTable 8 shows an example of prompt used during the offline stage, when we generate a series of knowledge candidates. For post-validation, we use the prompt same as the online stage (example in Table 7).", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_13", "tab_14", "tab_13"]}, {"heading": "Prompt", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Space description", "text": "Here are some classification datasets along with best hyperparameter configurations to train a R language model \"Learner mlr.classif.svm from package(s) e1071\" on them.\nDemonstrations Dataset: The dataset name is \"ada_agnostic\". It contains 2 classes, 4562 instances, 49 features, 48 numeric features, 1 categorical features. The majority class size is 3430 and the minority class size is 1132. Configuration 1: cost is very small. kernel is linear. Configuration 2: cost is very small. kernel is linear. Configuration 3: cost is very small. kernel is linear. Dataset: The dataset name is \"credit-g\". It contains 2 classes, 1000 instances, 21 features, 7 numeric features, 14 categorical features. The majority class size is 700 and the minority class size is 300. Configuration 1: cost is medium. gamma is small. kernel is radial. Configuration 2: cost is medium. gamma is very small. kernel is radial. Configuration 3: cost is medium. gamma is small. kernel is radial. Dataset: The dataset name is \"ozone-level-8hr\". It contains 2 classes, 2534 instances, 73 features, 72 numeric features, 1 categorical features. The majority class size is 2374 and the minority class size is 160. Configuration 1: cost is small. gamma is small. kernel is radial. Configuration 2: cost is very small. gamma is small. kernel is radial. Configuration 3: cost is small. gamma is small. kernel is radial.      When MLCopilot is deployed, it may be impractical to require users to strictly adhere to a specific format when writing task descriptions. We must consider if MLCopilot is robust enough to handle various formats. To simulate diverse formats, we ask GPT-3.5 to rewrite the descriptions by: (i) condensing the original task descriptions; and (ii) anonymizing the descriptions by removing task names. The results are shown in Table 9. We observed fluctuations in performance when the description format changed, indicating that LLM is sensitive to prompt format. This aligns with the previous researches (Webson and Pavlick, 2022;Lu et al., 2022) suggesting that LLMs may not interpret inputs in the same way as humans.\nPerformance was particularly worse when tasks were anonymized, leading us to conjecture that task names stimulate neurons in the LLM that are important for solving the relevant tasks and also leverage the previous knowledge already memorized from the training corpus of LLMs. To further verify this, we conducted an additional experiment by randomly swapping task names between tasks, and surprisingly observed performance improvement on PD1 and HyperFD. This echoes the finding in (Min et al., 2022), which suggests that \"random labels are better than no labels at all\".", "publication_ref": ["b41", "b22", "b23"], "figure_ref": [], "table_ref": ["tab_15"]}, {"heading": "D.2 Length of prompt.", "text": "We study the effect of prompt length, which is mainly influenced by the number of retrieved ML experiences as demonstrations, to the performance. In our previous experiments, we retrieved as many experiences as possible, either until all the experiences had been retrieved or the maximum prompt length was reached. For each task, the three best solutions were demonstrated, which was an arbitrary choice in our early experiments. In this section, we vary these two factors. As shown in Figure 3, the performance generally improves as the number of demonstrations in the prompt increases, measured by prompt tokens. However, the performance soon saturates at around 1.5k tokens and fluctuates. Moreover, demonstrating more solutions for each task leverages more data and has higher potential, especially above 3k tokens.", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "D.3 Choice of LLMs.", "text": "We report performance of MLCopilot if equipped with LLMs other than GPT-3.5 (code-named \"textdavinci-003\") used in our main experiments. The models we have experimented with include:  \u2022 GPT-3.5 Turbo 7 : a cost-efficient version of GPT-3.5 that uses chat completion as its user interface.\n\u2022 GPT-3 (Brown et al., 2020) (code-named \"text-davinci-001\"): the original GPT-3 model trained without instruction finetuning.\n\u2022 LLAMA-7B (Touvron et al., 2023): a well-known open-source model with a large user community, with a relatively loose requirement of GPU memory.\nWe compare the results with the original results with text-davinci-003 (GPT-3.5) and our main baselines. As shown in Table Table 10   We discuss the cases where the experience pool is polluted during the operation of the system. We evaluated the robustness under such scenarios on HPO-B (the largest solution space).\nFirstly, we assessed the impact of noises in accuracy, by perturbing the accuracies in historical data. We added Gaussian noise to the accuracy values. The standard deviation of the Gaussian noise is 10% of the accuracy distribution. As a result of such disturbance, suboptimal configurations might pop up as the best configurations and serve as demonstrations in the prompt. After experimenting on HPO-B, we found (in Table 11) that MLCopilot does suffer from such disturbance (performance drop from 81.59 to 78.54). However, such a result is still competitive with the state-of-the-art baselines (FLAML 77.84). Moreover, if a similar disturbance was done to the input data of FLAML, its performance further drops to 73.53. Full results (top-1 normalized accuracy) are shown in the table below.\nSecondly, we investigate the effect of faulty canonicalization. In our paper, we showed (in Table 6) that canonicalization is an important component of MLCopilot, and a misconfigured canonicalization can lead to degraded performance. Following your suggestions, we introduced random noise into the canonicalization process by replacing 10% of the canonicalized data with random discrete values. That is, each parameter of the configuration has a 10% probability to be replaced with a random choice from \"very low\", \"low\", \"medium\", \"high\", \"very high\". We show the results (top-1 normalized accuracy on HPO-B) in Table 12.\nAlthough the result is still competitive with baseline FLAML, we can see that faulty canonicalization does lead to worse performance. Notably, the impact is even more severe than the setting of perturbed accuracy. We speculate that false canonicalization can be particularly misleading for the logical reasoning of large language models. We will include a discussion of these findings in our revision.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1", "tab_1", "tab_11", "tab_1"]}, {"heading": "E Knowledge", "text": "All contents in this section are generated by Large Language Models.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.1 HPO-B", "text": "HPO-B contains 16 design spaces. We finalize one set of knowledge for each space.\nSpace: 5860 1. Generally, datasets with more numeric features require larger alphas and smaller lambdas for better performance. 2. Datasets with a higher ratio of minority to majority class size require smaller alphas and larger lambdas for better performance. 3. Datasets with more features require larger alphas and smaller lambdas for better performance. 4. Datasets with more categorical features require larger alphas and larger lambdas for better performance.\nSpace: 4796\n1. For datasets with a large majority class size and a small minority class size, a larger cp and minbucket size tend to be better hyper-parameter configurations. 2. For datasets with a small majority class size and a large minority class size, a smaller cp and minbucket size tend to be better hyper-parameter configurations. 3. For datasets with a large number of numeric features, a larger cp and minbucket size tend to be better hyper-parameter configurations. 4. For datasets with a small number of numeric features, a smaller cp and minbucket size tend to be better hyper-parameter configurations. 5. For datasets with a large number of categorical features, a smaller cp and minbucket size tend to be better hyper-parameter configurations. 6. For datasets with a small number of categorical features, a larger cp and minbucket size tend to be better hyper-parameter configurations.\nSpace: 5971\n1. Generally, larger datasets require higher nrounds and larger subsample values. 2. The majority class size and minority class size of the dataset can influence the configuration of alpha, booster, colsample bylevel, colsample bytree, eta, lambda, max depth, min child weight, nrounds, and subsample. 3. The number of numeric and categorical features in the dataset can determine the booster used. 4. The size of the dataset can influence the configuration of eta, lambda, max depth, min child weight, nrounds, and subsample. 5. The size of the minority class can determine the configuration of alpha, colsample bylevel, colsample bytree, eta, lambda, max depth, min child weight, nrounds, and subsample.\nSpace: 6766\n1. For datasets with a larger majority class size, high values of alpha and low values of lambda tend to perform better. 2. For datasets with a smaller majority class size, low values of alpha and high values of lambda tend to perform better. 3. For datasets with more numeric features, medium values of alpha and low values of lambda tend to perform better. 4. For datasets with more categorical features, high values of alpha and large values of lambda tend to perform better. 5. For datasets with a larger number of features, high values of alpha and large values of lambda tend to perform better.\nSpace: 5965\n1. The larger the majority class size, the smaller the min node size and sample fraction tend to be. 2. The larger the minority class size, the larger the min node size and sample fraction tend to be. 3. The larger the number of features, the larger the mtry tends to be. 4. The larger the number of numeric features, the larger the mtry tends to be. 5. The larger the number of categorical features, the smaller the mtry tends to be. 6. The larger the number of trees, the smaller the mtry tends to be. 7. The larger the number of instances, the larger the sample fraction tends to be. 8. The replace parameter is usually set to True. 9. The respect unordered factors parameter is usually set to False. Space: 7607\n1. The min node size generally decreases as the dataset sizeincreases.\n2. The mtry is usually small for datasets with few features and large for datasets with many features. 3. The num trees is usually small for datasets with few instances and large for datasets with many instances. 4. Replace is usually set to False for small datasets and True for large datasets. 5. Respect unordered factors is usually set to False for datasets with few categorical features and True for datasets with many categorical features. 6. Sample fraction is usually set to small for datasets with few instances and large for datasets with many instances.\nSpace: 6794\n1. For datasets with a large majority class size, larger min node size and sample fraction values are usually used, while for datasets with a smaller majority class size, smaller min node size and sample fraction values are usually used. 2. For datasets with more features, larger mtry values are usually used. 3. For datasets with more numeric features, replace is usually set to True, while for datasets with more categorical features, replace is usually set to False. 4. Respect unordered factors is usually set to True when the dataset has more categorical features.\nSpace: 7609\n1. For datasets with more features, larger mtry values are preferred. 2. For datasets with more instances, larger sample fractions are preferred. 3. For datasets with more majority class instances, smaller min node sizes are preferred. 4. For datasets with more numeric features, replace is typically set to True. 5. For datasets with more categorical features, respect unordered factors is typically set to False. 6. For datasets with a more balanced class size, num trees is typically set to a smaller value.\nSpace: 5859\n1. larger datasets tend to require smaller cp values and larger minbucket values. 2. Smaller datasets tend to require larger cp values and smaller minbucket values. 3. For larger datasets, maxdepth tends to be very large or medium, whereas for Smaller datasets , maxdepth tends to be very small or small. 4. For larger datasets, minsplit tends to be very large or large, whereas for Smaller datasets , minsplit tends to be very small or small.\nSpace: 5889\n1. The larger the dataset size, the larger the mtry and num trees, and the smaller the sample fraction. 2. The larger the majority class size, the larger the mtry and num trees, and the smaller the sample fraction.\n3. The smaller the number of features, the smaller the mtry and num trees, and the larger the sample fraction. 4. The more numeric features, the larger the mtry and num trees, and the smaller the sample fraction. 5. The more categorical features, the smaller the mtry and num trees, and the larger the sample fraction. 6. The replace parameter is usually set to True. Space: 5970\n1. For datasets with more numeric features, smaller alpha and smaller lambda values tend to be the best hyper-parameter configurations. 2. For datasets with more categorical features, larger alpha and larger lambda values tend to be the best hyper-parameter configurations. 3. For datasets with majority class size significantly larger than minority class size, larger alpha and larger lambda values tend to be the best hyper-parameter configurations.\nSpace: 5527\n1. The cost parameter tends to increase as the dataset size increases. 2. The gamma parameter tends to decrease as the number of numeric features increases. 3. The kernel parameter tends to be radial for datasets with numeric features, and polynomial or linear for datasets with categorical features. 4. The degree parameter tends to increase as the number of categorical features increases.\nSpace: 5636\n1. The larger the majority class size, the smaller the cp value should be. 2. The larger the minority class size, the larger the cp value should be. 3. The larger the number of features, the smaller the maxdepth value should be. 4. The larger the number of numeric features, the larger the minbucket value should be. 5. The larger the number of categorical features, the smaller the minbucket value should be. 6. The larger the number of instances, the larger the minsplit value should be.\nSpace: 5891\n1. For datasets with many numeric features, larger cost values and smaller gamma values tend to be more effective. 2. For datasets with many categorical features, linear kernels tend to be more effective. 3. For datasets with few numeric features, small cost values and larger gamma values tend to be more effective. 4. For datasets with few categorical features, polynomial kernels tend to be more effective.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.2 PD1", "text": "We performed leave-one-out evaluation on the PD1 benchmark, which consists of 23 tasks. However, some tasks are using the same model and dataset but only different in batch size. These tasks should not appear in training tasks and test tasks at the same time (Wang et al., 2021b). Therefore, only 13 distinct sets of training tasks were available for testing. For each set of training tasks, we generated a corresponding set of knowledge, which is presented below.\nTest task: CIFAR100, Wide ResNet 1. Set the initial learning rate (LR) according to the size of the dataset and the complexity of the model. 2. Set the momentum parameter to a lower value for larger datasets and a higher value for simpler models. 3. Set the power parameter to a higher value for more complex models. 4. Set the lambda parameter to a higher value for more complex models and a lower value for simpler models.\nTest task: CIFAR10, Wide ResNet 1. Adjust the initial learning rate and momentum based on the size and complexity of the dataset: higher for large and complex datasets, lower for small and simple datasets. 2. Adjust the power and lambda parameters based on the desired speed of the learning process: higher power and lower lambda for faster learning, lower power and higher lambda for slower learning. 3. Consider any domain-specific constraints when configuring the optimizer, such as accuracy requirements.\nTest task: Fashion-MNIST, Max Pooling CNN with ReLU 1. Set the initial learning rate to a low or medium value.\n2. Set the momentum to a high or medium value. 3. Set the power to a low or medium value. 4. Set the lambda to a low or medium value. 5. Adjust the initial learning rate, momentum, power, and lambda according to the characteristics of the task, such as the dataset size, model architecture, and the complexity of the prediction task. For example, for tasks with larger datasets, a higher initial learning rate may be beneficial, while for tasks with smaller datasets, a lower initial learning rate may be more suitable.\nSimilarly, for tasks with more complex models, a higher momentum may be beneficial, while for simpler models, a lower momentum may be more suitable. Additionally, for tasks with more complex prediction tasks, a higher power may be beneficial, while for simpler tasks, a lower power may be more suitable. Finally, for tasks with more complex models, a higher lambda may be beneficial, while for simpler models, a lower lambda may be more suitable.\nTest task: Fashion-MNIST, Max Pooling CNN with Tanh 1. Choose an initial LR that is appropriate for the size of the dataset and complexity of the model. 2. Set the momentum to a value that is appropriate for the size of the dataset and complexity of the model. 3. Set the power parameter to a value that is appropriate for the size of the dataset and complexity of the model. 4. Set the lambda parameter to a value that is appropriate for the size of the dataset and complexity of the model.\nTest task: Fashion-MNIST, Simple CNN 1. Set the initial learning rate (LR) to a value that is appropriate for the size of the dataset. 2. Set the momentum to a value that is appropriate for the size of the dataset. 3. Set the power parameter to a value that is appropriate for the size of the dataset. 4. Set the lambda parameter to a value that is appropriate for the size of the dataset and the desired level of regularization.\nTest task: ImageNet, ResNet50\n1. For tasks with larger batch sizes, use a higher initial learning rate and higher momentum. For tasks with smaller batch sizes, use a lower initial learning rate and lower momentum. 2. For tasks with larger vocabularies, use a higher lambda value. For tasks with smaller vocabularies, use a lower lambda value. 3. For tasks with more complex models, use a higher power value. For tasks with simpler models, use a lower power value.\nTest task: LM1B, Transformer 1. Set the initial learning rate to a value that is suitable for the size and complexity of the dataset. 2. Set the momentum to a value that is suitable for the size and complexity of the dataset. 3. Set the power parameter to a value that is suitable for the noise and outliers in the dataset. 4. Set the lambda parameter to a value that is suitable for the noise and outliers in the dataset. 5. Consider the characteristics of the task, such as the dataset size, model architecture, and the complexity of the prediction task, when adjusting the parameters. 6. For tasks with larger datasets, a higher initial learning rate and lower momentum may be more suitable. 7. For tasks with Smaller datasets , a lower initial learning rate and higher momentum may be more suitable. 8. For tasks with more complex models, a higher initial learning rate and lower momentum may be more suitable. 9. For tasks with simpler models, a lower initial learning rate and higher momentum may be more suitable. 10. For tasks with more complex prediction tasks, a higher initial learning rate and lower momentum may be more suitable. 11. For tasks with simpler prediction tasks, a lower initial learning rate and higher momentum may be more suitable. Test task: MNIST, Simple CNN 1. Adjust the initial learning rate and momentum according to the size and complexity of the dataset: higher for large and complex datasets, lower for small and simple datasets. 2. Adjust the power and lambda parameters according to the size and complexity of the dataset: higher for large and complex datasets, lower for small and simple datasets. 3. Adjust the initial learning rate and momentum according to the task requirements: higher for tasks requiring high accuracy, lower for tasks requiring high speed.\nTest task: SVHN, Wide ResNet 1. For image classification tasks, set the initial learning rate (LR) to a higher value and the momentum to a lower value. 2. For language tasks, set the initial LR to a lower value and the momentum to a higher value. 3. For tasks with larger batch sizes, set the initial LR to a higher value and the momentum to a lower value. 4. For tasks with smaller batch sizes, set the initial LR to a lower value and the momentum to a higher value. 5. For tasks with more complex models, set the power to a higher value and the lambda to a higher value. 6. For tasks with simpler models, set the power to a lower value and the lambda to a lower value.\nTest task: UniRef50, Transformer 1. Set the initial learning rate, momentum, power, and lambda values according to the following guidelines: -For larger batch sizes and larger datasets, use a higher learning rate, higher momentum, lower power, and higher lambda. -For smaller batch sizes and Smaller datasets , use a lower learning rate, lower momentum, higher power, and lower lambda.", "publication_ref": ["b40"], "figure_ref": [], "table_ref": []}, {"heading": "Examples:", "text": "-For a CNN with max-pool and ReLU on Fashion MNIST with a batch size of 256, use an initial learning rate of 0.001, a momentum of 0.9, a power of 0.1, and a lambda of 0.01. -For a Wide ResNet on CIFAR10 with a batch size of 2048, use an initial learning rate of 0.01, a momentum of 0.9, a power of 0.5, and a lambda of 0.001. -For a Transformer on LM1B with a batch size of 2048, use an initial learning rate of 0.001, a momentum of 0.9, a power of 0.01, and a lambda of 0.001.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Test task: WMT15, xformer", "text": "1. Set the initial learning rate to a low or medium value.\n2. Set the momentum to a high or medium value. 3. Set the power to a low or medium value. 4. Set the lambda to a high or medium value. 5. Adjust the initial learning rate and momentum based on the characteristics of the task, such as the dataset size, model architecture, and the complexity of the prediction task. For example, for tasks with larger datasets, a higher initial learning rate and a lower momentum may be more suitable, while for tasks with smaller datasets, a lower initial learning rate and a higher momentum may be more suitable. Additionally, for tasks with more complex models, a higher initial learning rate and a lower momentum may be more suitable, while for tasks with simpler models, a lower initial learning rate and a higher momentum may be more suitable. Finally, for tasks with more complex prediction tasks, a higher initial learning rate and a lower momentum may be more suitable, while for tasks with simpler prediction tasks, a lower initial learning rate and a higher momentum may be more suitable.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.3 HyperFD", "text": "Similar to PD1, evaluation on HyperFD is also leave-one-out on 12 tasks. We show 12 sets of knowledge based on the choices of test tasks.\nTest task: AFLW 1. Configure crop size and anchor matching IoU threshold based on the number of faces in the dataset: -For datasets with more faces, use larger crop sizes and higher anchor matching IoU thresholds. -For datasets with fewer faces, use smaller crop sizes and lower anchor matching IoU thresholds. 2. Configure learning rate and negative to positive ratio based on the number of faces in the dataset: -For datasets with more faces, use higher learning rates and more negative to positive ratios. -For datasets with fewer faces, use lower learning rates and fewer negative to positive ratios. 3. Configure location loss weight based on the presence of facial landmarks in the dataset: -For datasets with facial landmarks, use higher location loss weights.\nTest task: ANIME 1. Set the crop size and anchor matching IoU threshold according to the number of faces in the dataset: -For datasets with more faces, use larger crop sizes and higher anchor matching IoU thresholds. -For datasets with fewer faces, use smaller crop sizes and lower anchor matching IoU thresholds. 2. Set the location loss weight according to the presence of facial landmarks in the dataset: -For datasets with facial landmarks, use higher location loss weights. -For datasets without facial landmarks, use lower location loss weights. 3. Set the learning rate and optimizer according to the negative to positive ratio in the dataset: -For datasets with higher negative to positive ratios, use higher learning rates and optimizers such as SGD or Adam.\nTest task: FaceMask 1. Set the crop size according to the number of faces in the dataset: larger crop sizes for datasets with more faces, and smaller crop sizes for datasets with fewer faces. 2. Set the anchor matching IoU threshold according to the number of faces in the dataset: higher thresholds for datasets with more faces, and lower thresholds for datasets with fewer faces. 3. Set the location loss weight according to the presence of facial landmarks in the dataset: higher weights for datasets with facial landmarks, and lower weights for datasets without facial landmarks. 4. Set the negative to positive ratio according to the number of faces in the dataset: higher ratios for datasets with more faces, and lower ratios for datasets with fewer faces. 5. Set the learning rate according to the number of faces in the dataset: higher rates for datasets with more faces, and lower rates for datasets with fewer faces.\nTest task: FDDB 1. Set the crop size to be larger and the anchor matching IoU threshold to be higher for datasets with more faces. 2. Increase the location loss weight and decrease the negative to positive ratio for datasets with more faces. 3. Use a lower learning rate and an optimizer such as Adam or SGD for datasets with facial landmarks.\nTest task: FDDB-360\n1. For datasets with more faces, use a larger crop size and a higher anchor matching IoU threshold. 2. For datasets with fewer faces, use a smaller crop size and a lower anchor matching IoU threshold. 3. For datasets with no facial landmarks, use a lower location loss weight and a higher negative to positive ratio. 4. For datasets with facial landmarks, use a higher location loss weight and a lower negative to positive ratio. 5. For datasets with more faces, use a higher learning rate and an SGD optimizer. 6. For datasets with fewer faces, use a lower learning rate and an Adam optimizer.\nTest task: MAFA 1. Set the crop size and anchor matching IoU threshold according to the number of faces per image in the dataset: larger crop sizes and higher IoU thresholds for datasets with more faces per image, and smaller crop sizes and lower IoU thresholds for datasets with fewer faces per image. 2. Set the location loss weight according to the presence of facial landmarks in the dataset: higher weights for datasets with facial landmarks, and lower weights for datasets without facial landmarks. 3. Set the negative to positive ratio according to the difficulty of the dataset: higher ratios for datasets with more challenging scenarios (e.g. weather-based degradations, motion blur, focus blur). 4. Set the learning rate and optimizer according to the size of the dataset: lower learning rates and optimizers such as Adam or SGD for datasets with more images.\nTest task: PASCAL VOC 1. Set the crop size according to the number of faces in the dataset: larger crop sizes for datasets with more faces, and smaller crop sizes for datasets with fewer faces. 2. Set the anchor matching IoU threshold according to the number of faces in the dataset: higher thresholds for datasets with more faces, and lower thresholds for datasets with fewer faces. 3. Set the location loss weight according to the presence of facial landmarks in the dataset: lower weights for datasets with no facial landmarks, and higher weights for datasets with facial landmarks. 4. Set the negative to positive ratio according to the number of faces in the dataset: higher ratios for datasets with more faces, and lower ratios for datasets with fewer faces. 5. Set the learning rate and optimizer according to the difficulty of the dataset: higher learning rates and optimizers such as SGD for more challenging datasets.\nTest task: UFDD 1. Set the crop size and anchor matching IoU threshold according to the number of faces in the dataset: larger crop size and higher IoU threshold for datasets with more faces, smaller crop size and lower IoU threshold for datasets with fewer faces. 2. Set the location loss weight and negative to positive ratio according to the number of faces in the dataset: higher location loss weight and higher negative to positive ratio for datasets with more faces, lower location loss weight and lower negative to positive ratio for datasets with fewer faces. 3. Set the learning rate and optimizer according to the presence of facial landmarks in the dataset: lower learning rate and Adam optimizer for datasets with facial landmarks, higher learning rate and SGD optimizer for datasets without facial landmarks.\nTest task: UMDAA-02\n1. Set the crop size according to the number of faces in the dataset: larger crop sizes for datasets with more faces, and smaller crop sizes for datasets with fewer faces. 2. Set the anchor matching IoU threshold according to the number of faces in the dataset: higher thresholds for datasets with more faces, and lower thresholds for datasets with fewer faces. 3. Set the location loss weight according to the presence of facial landmarks in the dataset: higher weights for datasets with facial landmarks, and lower weights for datasets without facial landmarks. 4. Set the negative to positive ratio according to the number of faces in the dataset: higher ratios for datasets with more faces, and lower ratios for datasets with fewer faces. 5. Set the learning rate and optimizer according to the difficulty of the dataset: higher learning rates and optimizers such as SGD or Adam for more challenging datasets.\nTest task: WIDER FACE 1. Set the crop size to a value that is proportional to the number of faces in the dataset. 2. Set the anchor matching IoU threshold to a value that is proportional to the number of faces in the dataset.\n3. Set the negative to positive ratio to a value that is proportional to the number of faces in the dataset. 4. Set the learning rate to a value that is proportional to the number of faces in the dataset. 5. If the dataset contains facial landmarks, set the location loss weight to a value that is proportional to the number of faces in the dataset.\nTest task: WIDER-FACE-360\n1. Set the crop size and anchor matching IoU threshold according to the number of faces in the dataset: larger crop size and higher IoU threshold for datasets with more faces, and smaller crop size and lower IoU threshold for datasets with fewer faces. 2. Set the location loss weight according to the presence of facial landmarks: higher weight for datasets with facial landmarks, and lower weight for datasets without facial landmarks. 3. Set the negative to positive ratio according to the number of faces in the dataset: higher ratio for datasets with more faces, and lower ratio for datasets with fewer faces. 4. Set the learning rate according to the number of faces in the dataset: higher rate for datasets with more faces, and lower rate for datasets with fewer faces. 5. Set the optimizer according to the number of faces in the dataset: SGD for datasets with more faces, and Adam for datasets with fewer faces.\nTest task: WIKI 1. Set the crop size to a value that is proportional to the number of faces in the dataset. 2. Set the anchor matching IoU threshold to a value that is proportional to the number of faces in the dataset. 3. Set the location loss weight to a value that is proportional to the presence of facial landmarks in the dataset. 4. Set the learning rate to a value that is inversely proportional to the negative to positive ratio in the dataset. 5. Use an optimizer such as Adam or SGD.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Learning to learn by gradient descent by gradient descent", "journal": "", "year": "2016", "authors": "Marcin Andrychowicz; Misha Denil; Sergio Gomez; W Matthew; David Hoffman; Tom Pfau; Brendan Schaul; Nando De Shillingford;  Freitas"}, {"ref_id": "b1", "title": "Hpo-b: A large-scale reproducible benchmark for black-box hpo based on openml", "journal": "", "year": "2021", "authors": "Hadi Sebastian Pineda Arango;  Samer Jomaa"}, {"ref_id": "b2", "title": "Collaborative hyperparameter tuning", "journal": "PMLR", "year": "2013", "authors": "R\u00e9mi Bardenet; M\u00e1ty\u00e1s Brendel; Bal\u00e1zs K\u00e9gl; Michele Sebag"}, {"ref_id": "b3", "title": "Pattern recognition and machine learning", "journal": "Springer", "year": "2006", "authors": "M Christopher;  Bishop; M Nasser;  Nasrabadi"}, {"ref_id": "b4", "title": "Emergent autonomous scientific research capabilities of large language models", "journal": "", "year": "2023", "authors": "A Daniil; Robert Boiko; Gabe Macknight;  Gomes"}, {"ref_id": "b5", "title": "Random forests", "journal": "Machine learning", "year": "2001", "authors": "Leo Breiman"}, {"ref_id": "b6", "title": "Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Conference on Neural Information Processing Systems (NeurIPS)", "journal": "", "year": "", "authors": "Tom B Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell; Sandhini Agarwal; Ariel Herbert-Voss; Gretchen Krueger; Tom Henighan; Rewon Child; Aditya Ramesh; Daniel M Ziegler; Jeffrey Wu; Clemens Winter; Christopher Hesse; Mark Chen; Eric Sigler; Mateusz Litwin"}, {"ref_id": "b7", "title": "Xgboost: A scalable tree boosting system", "journal": "", "year": "2016", "authors": "Tianqi Chen; Carlos Guestrin"}, {"ref_id": "b8", "title": "Towards learning universal hyperparameter optimizers with transformers", "journal": "Advances in Neural Information Processing Systems", "year": "2022", "authors": "Yutian Chen; Xingyou Song; Chansoo Lee; Zi Wang; Richard Zhang; David Dohan; Kazuya Kawakami; Greg Kochanski; Arnaud Doucet;  Marc'aurelio Ranzato"}, {"ref_id": "b9", "title": "Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, and others. 2022. Palm: Scaling language modeling with pathways", "journal": "ArXiv", "year": "", "authors": "Aakanksha Chowdhery; Sharan Narang; Jacob Devlin; Maarten Bosma; Gaurav Mishra; Adam Roberts; Paul Barham"}, {"ref_id": "b10", "title": "Supportvector networks", "journal": "Machine learning", "year": "1995", "authors": "Corinna Cortes; Vladimir Vapnik"}, {"ref_id": "b11", "title": "Oxford english dictionary", "journal": "", "year": "1989", "authors": "Oxford English Dictionary ; Simpson; Ja & Weiner; Esc "}, {"ref_id": "b12", "title": "Jost Springenberg, Manuel Blum, and Frank Hutter", "journal": "", "year": "2015", "authors": "Matthias Feurer; Aaron Klein; Katharina Eggensperger"}, {"ref_id": "b13", "title": "A tutorial on bayesian optimization", "journal": "", "year": "2018", "authors": "I Peter;  Frazier"}, {"ref_id": "b14", "title": "Luna: Language understanding with number augmentations on transformers via number plugins and pre-training", "journal": "", "year": "2022", "authors": "Hongwei Han; Jialiang Xu; Mengyu Zhou; Yijia Shao; Shi Han; Dongmei Zhang"}, {"ref_id": "b15", "title": "Towards reasoning in large language models: A survey", "journal": "", "year": "2022", "authors": "Jie Huang; Kevin Chen-Chuan Chang"}, {"ref_id": "b16", "title": "Automated machine learning: methods, systems, challenges", "journal": "Springer Nature", "year": "2019", "authors": "Frank Hutter; Lars Kotthoff; Joaquin Vanschoren"}, {"ref_id": "b17", "title": "Mathprompter: Mathematical reasoning using large language models", "journal": "", "year": "2023", "authors": "Shima Imani; Liang Du; Harsh Shrivastava"}, {"ref_id": "b18", "title": "Survey of hallucination in natural language generation", "journal": "ACM Computing Surveys", "year": "2023", "authors": "Ziwei Ji; Nayeon Lee; Rita Frieske; Tiezheng Yu; Dan Su; Yan Xu; Etsuko Ishii; Ye Jin Bang; Andrea Madotto; Pascale Fung"}, {"ref_id": "b19", "title": "Auto-weka: Automatic model selection and hyperparameter optimization in weka", "journal": "", "year": "2019", "authors": "Lars Kotthoff; Chris Thornton; H Holger; Frank Hoos; Kevin Hutter;  Leyton-Brown"}, {"ref_id": "b20", "title": "Imagenet classification with deep convolutional neural networks", "journal": "Communications of the ACM", "year": "2017", "authors": "Alex Krizhevsky; Ilya Sutskever; Geoffrey E Hin"}, {"ref_id": "b21", "title": "Chameleon: Plug-and-play compositional reasoning with large language models", "journal": "", "year": "2023", "authors": "Pan Lu; Baolin Peng; Hao Cheng; Michel Galley; Kai-Wei Chang; Ying Nian Wu; Song-Chun Zhu; Jianfeng Gao"}, {"ref_id": "b22", "title": "Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity", "journal": "Long Papers", "year": "2022", "authors": "Yao Lu; Max Bartolo; Alastair Moore; Sebastian Riedel; Pontus Stenetorp"}, {"ref_id": "b23", "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Sewon Min; Xinxi Lyu; Ari Holtzman; Mikel Artetxe; Mike Lewis; Hannaneh Hajishirzi; Luke Zettlemoyer"}, {"ref_id": "b24", "title": "Hyperstar: Task-aware hyperparameters for deep networks", "journal": "", "year": "2020", "authors": "Gaurav Mittal; Chang Liu; Nikolaos Karianakis; Victor Fragoso; Mei Chen; Yun Fu"}, {"ref_id": "b25", "title": "A method of solving a convex programming problem with convergence rate o\\bigl(k\u02c62\\bigr)", "journal": "", "year": "1983", "authors": "Yurii Evgen; ' Nesterov"}, {"ref_id": "b26", "title": "", "journal": "OpenAI. 2023. Gpt-4 technical report", "year": "", "authors": ""}, {"ref_id": "b27", "title": "Training language models to follow instructions with human feedback", "journal": "Advances in Neural Information Processing Systems", "year": "2022", "authors": "Long Ouyang; Jeffrey Wu; Xu Jiang; Diogo Almeida; Carroll Wainwright; Pamela Mishkin; Chong Zhang; Sandhini Agarwal; Katarina Slama; Alex Ray"}, {"ref_id": "b28", "title": "Are nlp models really able to solve simple math word problems?", "journal": "", "year": "2021", "authors": "Arkil Patel; Satwik Bhattamishra; Navin Goyal"}, {"ref_id": "b29", "title": "Efficient neural architecture search via parameters sharing", "journal": "PMLR", "year": "2018", "authors": "Hieu Pham; Melody Guan; Barret Zoph; Quoc Le; Jeff Dean"}, {"ref_id": "b30", "title": "Improving language understanding by generative pre-training", "journal": "", "year": "2018", "authors": "Alec Radford; Karthik Narasimhan"}, {"ref_id": "b31", "title": "Language models are unsupervised multitask learners", "journal": "", "year": "2019", "authors": "Alec Radford; Jeff Wu; Rewon Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"ref_id": "b32", "title": "Analysing mathematical reasoning abilities of neural models", "journal": "", "year": "2019", "authors": "David Saxton; Edward Grefenstette; Felix Hill; Pushmeet Kohli"}, {"ref_id": "b33", "title": "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface", "journal": "", "year": "2023", "authors": "Yongliang Shen; Kaitao Song; Xu Tan; Dongsheng Li; Weiming Lu; Yueting Zhuang"}, {"ref_id": "b34", "title": "Reinforcement learning: An introduction", "journal": "MIT press", "year": "2018", "authors": "S Richard; Andrew G Sutton;  Barto"}, {"ref_id": "b35", "title": "Representing numbers in nlp: a survey and a vision", "journal": "", "year": "2021", "authors": "Avijit Thawani; Jay Pujara; Pedro A Szekely; Filip Ilievski"}, {"ref_id": "b36", "title": "Edouard Grave, and Guillaume Lample. 2023. Llama: Open and Efficient Foundation Language Models", "journal": "ArXiv", "year": "", "authors": "Hugo Touvron; Thibaut Lavril; Gautier Izacard; Xavier Martinet; Marie-Anne Lachaux; Timoth\u00e9e Lacroix; Naman Baptiste Rozi\u00e8re; Eric Goyal; Faisal Hambro;  Azhar; Armand Aur'elien Rodriguez;  Joulin"}, {"ref_id": "b37", "title": "Openml: networked science in machine learning", "journal": "ACM SIGKDD Explorations Newsletter", "year": "2014", "authors": "Joaquin Vanschoren; Jan N Van Rijn; Bernd Bischl; Luis Torgo"}, {"ref_id": "b38", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b39", "title": "Flaml: A fast and lightweight automl library", "journal": "", "year": "2021", "authors": "Chi Wang; Qingyun Wu; Markus Weimer; Erkang Zhu"}, {"ref_id": "b40", "title": "Pre-trained gaussian processes for bayesian optimization", "journal": "", "year": "2021", "authors": "Zi Wang; George E Dahl; Kevin Swersky; Chansoo Lee; Zelda Mariet; Zachary Nado; Justin Gilmer; Jasper Snoek; Zoubin Ghahramani"}, {"ref_id": "b41", "title": "Do promptbased models really understand the meaning of their prompts?", "journal": "", "year": "2022", "authors": "Albert Webson; Ellie Pavlick"}, {"ref_id": "b42", "title": "Emergent Abilities of Large Language Models", "journal": "", "year": "2022", "authors": "Jason Wei; Yi Tay; Rishi Bommasani; Colin Raffel; Barret Zoph; Sebastian Borgeaud; Dani Yogatama; Maarten Bosma; Denny Zhou; Donald Metzler; E Chi; Tatsunori Hashimoto; P Vinyals; J Liang; W Dean;  Fedus"}, {"ref_id": "b43", "title": "Two-stage transfer surrogate model for automatic hyperparameter optimization", "journal": "Springer", "year": "2016-09-19", "authors": "Martin Wistuba; Nicolas Schilling; Lars Schmidt-Thieme"}, {"ref_id": "b44", "title": "Privacy-preserving online automl for domain-specific face detection", "journal": "", "year": "2022", "authors": "Chenqian Yan; Yuge Zhang; Quanlu Zhang; Yaming Yang; Xinyang Jiang; Yuqing Yang; Baoyuan Wang"}, {"ref_id": "b45", "title": "Generate rather than retrieve: Large language models are strong context generators", "journal": "", "year": "2022", "authors": "Wenhao Yu; Dan Iter; Shuohang Wang; Yichong Xu; Mingxuan Ju; Soumya Sanyal; Chenguang Zhu; Michael Zeng; Meng Jiang"}, {"ref_id": "b46", "title": "Deepke: A deep learning based knowledge extraction toolkit for knowledge base population", "journal": "", "year": "2022", "authors": "Ningyu Zhang; Xin Xu; Liankuan Tao; Haiyang Yu; Hongbin Ye; Shuofei Qiao; Xin Xie; Xiang Chen; Zhoubo Li; Lei Li"}, {"ref_id": "b47", "title": "", "journal": "", "year": "", "authors": "Kun Wayne Xin Zhao; Junyi Zhou; Tianyi Li; Xiaolei Tang; Yupeng Wang; Yingqian Hou; Beichen Min; Junjie Zhang; Zican Zhang;  Dong"}, {"ref_id": "b48", "title": "Can gpt-4 perform neural architecture search?", "journal": "", "year": "2023", "authors": "Mingkai Zheng; Xiu Su; Shan You; Fei Wang; Chen Qian; Chang Xu; Samuel Albanie"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Overview of MLCopilot. MLCopilot has offline and online stages. During the offline stage, it creates pools of experience and knowledge. In the online stage, it retrieves experience and knowledge based on the novel task description. Finally, MLCopilot invokes LLM and returns solutions.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Offline stage: canonicalization, knowledge elicitation.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Algorithm 1 :1Offline Stage of MLCopilot Input :Historical data H = {D 1 , . . . , D N H }. Maximum iterations rounds. Stagnation patience patience. Candidate question list Questions. Validation tasks ValTasks. Output :Experience Pool P E ; Knowledge k * . 1 P E \u2190 {C(D i )} ; /* C is canonicalization function */ 2 r * \u2190 \u2212\u221e; 3 stagnation \u2190 0; 4 for n=1 to rounds do 5 E \u2190 RandomSample(P E ); 6 q \u2190 RandomSample(Questions) ; /* Sample one hypophora question */ 7 \u03c4 \u2190Uniform(0, 1); ; /* Random temperature */ 8 k \u2190 LLM (E, q; \u03c4 ) ; /* Generate knowledge candidates */ 9 S \u2190 LLM (E, k; ValTasks; 0) ; /* Mock online stage on validation tasks */ r \u2190 Evaluate(S) ; /* Run and evaluate the solution */ if r > r * then 12 r * \u2190 r; Online Stage of MLCopilot Input :A new task descriptionT . Experience pool P E . Knowledge pool P K . Output :SolutionS. 1\u1ebc \u2190 R E (T , P E ) ; /* Retrieve experiences */ 2K \u2190 R K (T , P K ); /* Retrieve knowledge */ 3S \u2190 LLM(T ,\u1ebc,K); . HPO-B-v3 (Arango et al., 2021) comprises 16 solution spaces for 101 datasets obtained from the OpenML", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "HyperFD (lower better).", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 3 :3Figure 3: Effect of the experience number and the number of solutions demonstrated for each task. D Robustness of MLCopilot D.1 Task description in the wild.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Term DefinitionExample Task T ML problem to solve (optionally with constraints). Find an optimizer for a ResNet on ImageNet dataset. Solution space S Solution hypothesis space to the task.Optimizer: {Adam, SGD}; Learning rate: [10 \u22126 , 0.1].Solution SOne particular choice within solution space. 2-layer ResNet with SGD optimizer using LR 10 \u22123 .", "figure_data": "Experience ESuccessful solutions on historical tasks.SGD with lr 0.024 achieves 76.2% accuracy for ResNet on ImageNet.Knowledge KHigh-level information acquired from experiences.Usage of small LR makes training slower but could yield better final result."}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Terminologies used throughout this paper.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "he task is to classify a brain tumor based on a mpMRI scan. The dataset contains ~400k samples. Classify a lung tumor based on blood test report. The dataset contains around 1k samples. Solution: Model is xgboost. Using small max depth. Classify a lung tumor based on blood test report. The dataset contains around 1k samples. Solution: Model is xgboost. Using small max depth.", "figure_data": "1. DescribeMLCopilotHistorical data HtaskTask description TThe task is to classify a brain tumor based on aCanonicalizationUsermpMRI scan. The dataset contains ~400k samples.Demonstrations \u1ebc Task: Task: Shoulder X-ray classification. Diagnose type are A1, C1, D1. The dataset contains 500 samples. Demonstrations \u1ebc Task: Task: Shoulder X-ray classification. Diagnose type are A1, C1, D1. The dataset contains 500 samples.2. Retrieve ExperienceExperience pool P E Offline Elicitation (with LLM)Solution: Using pretrained EfficientNet-B0. Finetune Solution: Using pretrained EfficientNet-B0. Finetunewith a low learning rate and high weight decay. with a low learning rate and high weight decay.Knowledge K1 Knowledge K1Knowledge pool P K. CNN is commonly used for image datasets. For . CNN is commonly used for image datasets. Fortabular datasets, use decision tree algorithms. tabular datasets, use decision tree algorithms.3. Retrieve2. Use a low learning rate when finetuning on a pretrained model. Use a high learning rate to 2. Use a low learning rate when finetuning on a pretrained model. Use a high learning rate toKnowledgeconverge faster. converge faster.3. For small datasets, control regularization 3. For small datasets, control regularizationparameters like max depth to prevent overfit. parameters like max depth to prevent overfit.Solution4. Prompt LLMFinetune on a pretrained EfficientNet-B0 withlow learning rate and low weight decay.Online stage"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Main results on HPO-B, PD1 and HyperFD. nAcc, AP: the higher the better. Rank: the lower the better.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "34\u00b10.00 81.59\u00b10.94 1.40\u00b10.00 1.48\u00b10.06 79.17\u00b10.00 59.74\u00b11.89 Meta-feature 80.61\u00b10.00 83.29\u00b11.46 1.02\u00b10.00 1.41\u00b10.09 107.67\u00b10.00 50.49\u00b16.38 Random 73.67\u00b12.51 78.00\u00b12.82 0.06\u00b10.25 1.37\u00b10.12 84.23\u00b114.84 57.95\u00b110.19", "figure_data": "Retrieved byPipelineHPO-B \u2191 ASKL MLCopilotASKLPD1 \u2191 MLCopilotHyperFD \u2193 ASKL MLCopilotText embedding75."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Comparison of approaches to retrieve experience (i.e., based on what measures to retrieve the experience) and to consume the retrieved experience (ASKL: directly use the solutions for retrieved tasks on the new task; MLCopilot: use the retrieved experience as demonstrations along with knowledge to prompt LLM).", "figure_data": "Retrieve HPO-B \u2191 PD1 \u2191 HyperFD \u2193 Exp.+Know. 81.59\u00b10.94 1.48\u00b10.06 59.74\u00b11.89 Know. 62.77\u00b11.78 1.10\u00b10.00 127.75\u00b10.00 Exp. 76.21\u00b10.16 1.36\u00b10.09 63.20\u00b13.55"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Effect of retrieving experience and knowledge.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Method HPO-B \u2191 HyperBO \u2191 HyperFD \u2193 MLCopilot 81.59\u00b10.94 1.48\u00b10.06 59.74\u00b11.89 w/o Post-Val. 78.34\u00b10.71 1.44\u00b10.05 62.41\u00b13.66 w/o Know. 76.21\u00b10.16 1.36\u00b10.09 63.20\u00b13.55", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Ablation on knowledge utilization in online stage and post-validation in offline stage.", "figure_data": "Discretization HPO-B\u2191 PD1 \u2191 HyperFD\u2193 On Best 81.59\u00b10.94 1.48\u00b10.06 59.74\u00b11.89 On All 70.82\u00b10.01 1.41\u00b10.02 -\u2717 74.09\u00b10.44 1.45\u00b10.05 84.96\u00b17.16"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Discretization in canonicalization.", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Example prompt for HPO-B in online serving.", "figure_data": "PromptSpace descriptionHere are some classification datasets along with best hyper-parameter configurations to train a R language model \"Learner mlr.classif.svm from package(s) e1071\" on them.DemonstrationsDataset: The dataset name is \"wilt\". It contains 2 classes, 4839 instances, 6 features, 5 numeric features, 1 categorical features. The majority class size is 4578 and the minority class size is 261.Configuration 1: cost is medium. gamma is large. kernel is radial.Configuration 2: cost is medium. gamma is medium. kernel is radial.Configuration 3: cost is large. gamma is medium. kernel is radial.Dataset: The dataset name is \"ilpd\". It contains 2 classes, 583 instances, 11 features, 9 numeric features, 2 categorical features. The majority class size is 416 and the minority class size is 167.Configuration 1: cost is medium. gamma is medium. kernel is radial.Configuration 2: cost is very small. gamma is very large. kernel is radial.Configuration 3: cost is medium. gamma is very large. kernel is radial.Dataset: The dataset name is \"steel-plates-fault\". It contains 2 classes, 1941 instances, 34 features, 33 numeric features, 1 categorical features. The majority class size is 1268 and the minority class size is 673.Configuration 1: cost is small. kernel is linear.Configuration 2: cost is very small. kernel is linear.Configuration 3: cost is very small. kernel is linear.InstructionQ: From the examples above, what patterns can we observe about the relationship between dataset characteristics and the best hyper-parameter con-figurations? Answer MUST be concise, critical, point-by-point, line-by-line, and brief. Only in-clude relevant observations without unnecessary elaboration."}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Example prompt for HPO-B in the offline stage.", "figure_data": ""}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "Comparison of different description formats.", "figure_data": "Description Format Original Condense AnonymousHPO-B \u2191 81.59\u00b10.94 1.48\u00b10.06 PD1 \u2191 79.66\u00b10.06 1.52\u00b10.01 77.43\u00b10.04 1.21\u00b10.06 68.42\u00b110.01 HyperFD \u2193 59.74\u00b11.89 57.33\u00b13.21Misleading names75.80\u00b13.01 1.43\u00b10.0562.52\u00b13.921.61.5Q$FF#6ROVWDVNnAcc@11.3 1.4 1.2# Sols / task 1 3 57RNHQV1.1# Tokens 1000 2000 3000(a) HPO-B (higher better).(b) PD1 (higher better)."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "67\u00b11.99 1.37\u00b10.10 72.41\u00b110.48 MLCopilot (text-davinci-001) 82.13\u00b12.05 1.58\u00b10.04 71.25\u00b110.70 MLCopilot (LLAMA-7B) 79.51\u00b10.57 1.43\u00b10.08 67.47\u00b15.60 MLCopilot (text-davinci-003) 81.59\u00b10.94 1.48\u00b10.06 59.74\u00b11.89", "figure_data": "Method (with LLM) ASKL FLAML MLCopilot (gpt-3.5-turbo)HPO-B \u2191 77.01\u00b10.00 1.26\u00b10.00 PD1 \u2191 77.84\u00b10.00 1.28\u00b10.00 81.HyperFD \u2193 92.58\u00b10.00 66.42\u00b10.00"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "Performance of MLCopilot equipped with different LLMs.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_18", "figure_caption": ", MLCopilot is robust to choices of LLMs. It is compatible with all the LLMs we have tested and achieves competitive results under different settings. Also, we see a trend that when working with stronger and larger models, MLCopilot still achieves even better results.", "figure_data": "D.4 Noisy accuracy and faulty canonicalization.Methodw/ original data w/ perturbed dataFLAML MLCopilot77.84\u00b10.00 81.59\u00b10.9473.53\u00b10.00 78.54\u00b13.25"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_19", "figure_caption": "Impact of noises in accuracy.", "figure_data": "MethodnAccMLCopilot (original) MLCopilot (faulty canonicalization) 77.11\u00b11.77 81.59\u00b10.94 FLAML 77.84\u00b10.00"}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_20", "figure_caption": "Effect of faulty canonicalization.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_23", "figure_caption": "Test task: MNIST, Max Pooling CNN with ReLU 1. Set the initial learning rate to a low or medium value. 2. Set the momentum to a high or medium value. 3. Set the power to a low or medium value. 4. Set the lambda to a low or high value.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_24", "figure_caption": "Test task: MNIST, Max Pooling CNN with Tanh 1. Set the initial learning rate to a high value to ensure that the model is able to learn quickly and efficiently. 2. Set the momentum to a low value to prevent the model from overfitting. 3. Set the power and/or lambda to high values to ensure that the learning rate decays slowly and the model is able to continue learning for a longer period of time. 4. For tasks such as training a CNN with max-pool and ReLU on Fashion MNIST, set the initial learning rate to a low value to prevent the model from overfitting. 5. For tasks such as training a ResNet50 on ImageNet, set the initial learning rate to a high value to ensure that the model is able to learn quickly and efficiently. 6. For tasks such as training a Wide ResNet on CIFAR100, set the initial learning rate to a very high value to ensure that the model is able to learn quickly and efficiently. 7. For tasks such as training a Transformer on UniRef50, set the initial learning rate to a low value to prevent the model from overfitting.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "S = LLM(T ,\u1ebc,K).", "formula_coordinates": [5.0, 72.7, 70.87, 88.93, 14.01]}, {"formula_id": "formula_1", "formula_text": "E = R E (T , P E ) = arg top-k \u27e8T,S,M \u27e9\u2208P E E(T ) \u2022 E(T ) |E(T )| \u2022 |E(T )| ,", "formula_coordinates": [5.0, 70.86, 748.86, 232.42, 35.15]}], "doi": ""}