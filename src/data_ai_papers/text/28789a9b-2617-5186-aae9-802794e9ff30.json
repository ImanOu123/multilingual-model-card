{"title": "How to Combine Tree-Search Methods in Reinforcement Learning", "authors": "Yonathan Efroni Technion; Israel Gal; Dalal Technion; Israel Bruno Scherrer; Shie Mannor Technion", "pub_date": "2019-02-17", "abstract": "Finite-horizon lookahead policies are abundantly used in Reinforcement Learning and demonstrate impressive empirical success. Usually, the lookahead policies are implemented with specific planning methods such as Monte Carlo Tree Search (e.g. in AlphaZero (Silver et al. 2017b)). Referring to the planning problem as tree search, a reasonable practice in these implementations is to back up the value only at the leaves while the information obtained at the root is not leveraged other than for updating the policy. Here, we question the potency of this approach. Namely, the latter procedure is non-contractive in general, and its convergence is not guaranteed. Our proposed enhancement is straightforward and simple: use the return from the optimal tree path to back up the values at the descendants of the root. This leads to a \u03b3 h -contracting procedure, where \u03b3 is the discount factor and h is the tree depth. To establish our results, we first introduce a notion called multiple-step greedy consistency. We then provide convergence rates for two algorithmic instantiations of the above enhancement in the presence of noise injected to both the tree search stage and value estimation stage.", "sections": [{"heading": "Introduction", "text": "A significant portion of the Reinforcement Learning (RL) literature regards Policy Iteration (PI) methods. This family of algorithms contains numerous variants which were thoroughly analyzed (Puterman 1994;Bertsekas and Tsitsiklis 1995) and constitute the foundation of sophisticated state-of-the-art implementations (Mnih et al. 2016;Silver et al. 2017b). The principal mechanism of PI is to alternate between policy evaluation and policy improvement. Various well-studied approaches exist for the policy evaluation stages; these may rely on single-step bootstrap, multi-step Monte-Carlo return, or parameter-controlled interpolation of the former two. For the policy improvement stage, theoretical analysis was mostly reserved for policies that are 1-step greedy, while recent prominent implementations of multiplestep greedy policies exhibited promising empirical behavior (Silver et al. 2017b;Silver et al. 2017a).\nRelying on recent advances in the analysis of multiplestep lookahead policies (Efroni et al. 2018a;Efroni et al. 2018b), we study the convergence of a PI scheme whose Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. improvement stage is h-step greedy with respect to (w.r.t.) the value function, for h > 1. Calculating such policies can be done via Dynamic Programming (DP) or other planning methods such as tree search. Combined with sampling, the latter corresponds to the famous Monte Carlo Tree Search (MCTS) algorithm employed in (Silver et al. 2017b;Silver et al. 2017a). In this work, we show that even when partial (inexact) policy evaluation is performed and noise is added to it, along with a noisy policy improvement stage, the above PI scheme converges with a \u03b3 h contraction coefficient. While doing so, we also isolate a sufficient convergence condition which we refer to as h-greedy consistency and relate it to previous 1-step greedy relevant literature.\nA straightforward 'naive' implementation of the PI scheme described above would perform an h-step greedy policy improvement and then evaluate that policy by bootstrapping the 'usual' value function. Surprisingly, we find that this procedure does not necessarily contracts toward the optimal value, and give an example where it is indeed non-contractive. This contraction coefficient depends both on h and on the partial evaluation parameter: m in the case of m-step return, and \u03bb when eligibility trace is used. The non-contraction occurs even when the h-greedy consistency condition is satisfied.\nTo solve this issue, we propose an easy fix which we employ in all our algorithms, that relieves the convergence rate from the dependence of m and \u03bb, and allows the \u03b3 h contraction mentioned earlier in this section. Let us treat each state as a root of a tree of depth h; then our proposed fix is the following. Instead of backing up the value only at the leaves and ridding of all non-root related tree-search outputs, we reuse the tree-search byproducts and back up the optimal value of the root node children. Hence, instead of bootstrapping the 'usual' value function in the evaluation stage, we bootstrap the optimal value obtained from the h \u2212 1 horizon optimal planning problem.\nThe contribution of this work is primarily theoretical, but in Section 8 we also present experimental results on a toy domain. The experiments support our analysis by exhibiting better performance of our enhancement above compared to the 'naive' algorithm. Additionally, we identified previous practical usages of this enhancement in literature. In (Baxter, Tridgell, and Weaver 1999), the authors proposed backing up the optimal tree search value as a heuristic. They named the algorithm TDLeaf(\u03bb) and showcase its outperformance over the alternative 'naive' approach. A more recent work (Lai 2015) introduced a deep learning implementation of TDLeaf(\u03bb) called Giraffe. Testing it on the game of Chess, the authors claim (during publication) it is \"the most successful attempt thus far at using end-to-end machine learning to play chess\". In light of our theoretical results and empirical success described above, we argue that backing up the optimal value from a tree search should be considered as a 'best practice' among RL practitioners.", "publication_ref": ["b4", "b0", "b3", "b5", "b0", "b1", "b5", "b0", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "Our framework is the infinite-horizon discounted Markov Decision Process (MDP). An MDP is defined as the 5-tuple (S, A, P, R, \u03b3) (Puterman 1994), where S is a finite state space, A is a finite action space, P \u2261 P (s |s, a) is a transition kernel, R \u2261 r(s, a) \u2208 [R min , R max ] is a reward function, and \u03b3 \u2208 (0, 1) is a discount factor. Let \u03c0 : S \u2192 P(A) be a stationary policy, where P(A) is a probability distribution on A.\nLet v \u03c0 \u2208 R |S| be the value of a policy \u03c0, defined in state s as v \u03c0 (s) \u2261 E \u03c0 |s [ \u221e t=0 \u03b3 t r(s t , \u03c0(s t ))]\n, where E \u03c0 |s denotes expectation w.r.t. the distribution induced by \u03c0 and conditioned on the event {s 0 = s}. For brevity, we respectively denote the reward and value at time t by r t \u2261 r(s t , \u03c0 t (s t ))\nand v t \u2261 v(s t ). It is known that v \u03c0 = \u221e t=0 \u03b3 t (P \u03c0 ) t r \u03c0 = (I \u2212 \u03b3P \u03c0 ) \u22121 r \u03c0 , with the component-wise values [P \u03c0 ] s,s P (s | s, \u03c0(s)) and [r \u03c0 ] s r(s, \u03c0(s)).\nOur goal is to find a policy \u03c0 * yielding the optimal value v * such that v * = max \u03c0 (I \u2212 \u03b3P \u03c0 ) \u22121 r \u03c0 . This goal can be achieved using the three classical operators (with equalities holding component-wise):\n\u2200v, \u03c0, T \u03c0 v = r \u03c0 + \u03b3P \u03c0 v, (1) \u2200v, T v = max \u03c0 T \u03c0 v, (2\n)\n\u2200v, G(v) = {\u03c0 : T \u03c0 v = T v},(3)\nwhere T \u03c0 is a linear operator, T is the optimal Bellman operator and both T \u03c0 and T are \u03b3-contraction mappings w.r.t. the max norm. It is known that the unique fixed points of T \u03c0 and T are v \u03c0 and v * , respectively. The set G(v) is the standard set of 1-step greedy policies w.r.t. v. Furthermore, given v * , the set G(v * ) coincides with that of stationary optimal policies. In other words, every policy that is 1-step greedy w.r.t. v * is optimal and vice versa.\nThe most known variants of PI are Modified-PI (Puterman and Shin 1978) and \u03bb-PI (Bertsekas and Ioffe 1996). In both, the evaluation stage of PI is relaxed by performing partial-evaluation, instead of the full policy evaluation. In this work, we will generalize algorithms using both of these approaches. Modified PI performs partial evaluation using the m-return, (T \u03c0 ) m v, where \u03bb-PI uses the \u03bb-return, T \u03c0 \u03bb v, with \u03bb \u2208 [0, 1]. This operator has the following equivalent forms (see e.g. (Scherrer 2013), p.1182),\nT \u03c0 \u03bb v def = (1 \u2212 \u03bb) \u221e j=0 \u03bb j (T \u03c0 ) j+1 v (4) = v + (I \u2212 \u03b3\u03bbP \u03c0 ) \u22121 (T \u03c0 v \u2212 v). r t=0 \u03b3r t=1 \u03b3 2 v t=2 s s r s l\nFigure 1: Obtaining the h-greedy policy with a tree-search also outputs T \u03c0 h T h\u22121 v and T h\u22121 v. In this example, the red arrow depicts the h-greedy policy. The value at the root's child node s l is T h\u22121 v(s l ), which corresponds to the optimal blue trajectory starting at s l . The same holds for s r .\nThese operators correspond to the ones used in the famous TD(n) and TD(\u03bb) (Sutton, Barto, and others 1998),\n(T \u03c0 ) m v = E \u03c0 |\u2022 m\u22121 t=0 \u03b3 t r(s t , \u03c0 t (s t )) + \u03b3 m v(s h ) , T \u03c0 \u03bb v = v + E \u03c0 |\u2022 \u221e t=0 (\u03b3\u03bb) t (r t + \u03b3v t+1 \u2212 v t ) .\n3 The h-Greedy Policy and h-PI Let h \u2208 N\\{0}. An h-greedy policy (Bertsekas and Tsitsiklis 1995; Efroni et al. 2018a) \u03c0 h outputs the first optimal action out of the sequence of actions solving a non-stationary, hhorizon control problem as follows:\narg max \u03c00 max \u03c01,..,\u03c0 h\u22121 E \u03c00...\u03c0 h\u22121 |\u2022 h\u22121 t=0 \u03b3 t r(s t , \u03c0 t (s t )) + \u03b3 h v(s h ) = arg max \u03c00 E \u03c00 |\u2022 r(s 0 , \u03c0 0 (s 0 )) + \u03b3 T h\u22121 v (s 1 ) ,(5)\nwhere the notation E\n\u03c00...\u03c0 h\u22121 |\u2022\ncorresponds to conditioning on the trajectory induced by the choice of actions (\u03c0 0 (s 0 ), \u03c0 1 (s 1 ), . . . , \u03c0 h\u22121 (s h\u22121 )) and a starting state s 0 = \u2022 .\nAs the equality in (5) suggests that \u03c0 h can be interpreted as a 1-step greedy policy w.r.t. T h\u22121 v. We denote the set of h-greedy polices w.r.t v as G h (v) and is defined by\n\u2200v, G h (v) = {\u03c0 : T \u03c0 T h\u22121 v = T h v}.\nThis generalizes the definition of the 1-step greedy set of policies, generalizing, (3), and coincides with it for h = 1. Remark 1. The h-greedy policy can be obtained by solving the above formulation with DP in linear time (in h). Other than returning the policy, the last and one-before-last iterations also return T \u03c0 h T h\u22121 v and T h\u22121 v, respectively. Another, conceptually similar option would be using Model Predictive Control to solve the planning problem and again retrieve the above values of interest (Negenborn et al. 2005;Tamar et al. 2017). Given a 'nice' mathematical structure, this can be done efficiently. When the model is unknown, finding \u03c0 h together with T \u03c0 h T h\u22121 v and T h\u22121 v is possible with model-free approaches such as Q-learning (Jin et al. 2018). Alternatively, \u03c0 h (s) can be retrieved using a tree-search of depth h, starting at root s (see Figure 1). The search again returns T \u03c0 h T h\u22121 v and T h\u22121 v \"for free\" as the values at the root and its descendant nodes. While the tree-search complexity in general is exponential in h, sampling can be used. Examples for such sampling-based tree-search methods are MCTS (Browne et al. 2012) and Optimistic Tree Exploration (Munos 2014).\nAlgorithm 1 h-PI Initialize: h \u2208 N \\ {0}, v 0 = v \u03c00 \u2208 R |S| while v k changes do \u03c0 k \u2190 \u03c0 \u2208 G h (v) v k+1 \u2190 v \u03c0 k k \u2190 k + 1 end while Return \u03c0, v\nAs was discussed in (Bertsekas and Tsitsiklis 1995;Efroni et al. 2018a), one can use the h-greedy policy to derive a policy-iteration procedure called h-PI (see Algorithm 1). In it, the 1-step greedy policy from PI is replaced with the hgreedy policy. This algorithm iteratively calculates an h-step greedy policy with respect to v, and then performs a complete evaluation of this policy. Convergence is guaranteed after O(h \u22121 ) iterations (Efroni et al. 2018a).", "publication_ref": ["b4", "b5", "b6", "b0", "b3", "b6", "b2", "b0", "b0", "b0", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "h-Greedy Consistency", "text": "The h-greedy policy w.r.t v \u03c0 is strictly better than \u03c0, i.e., v \u03c0 h \u2265 v \u03c0 (Bertsekas and Tsitsiklis 1995; Efroni et al. 2018a). Using this property for proving convergence of an algorithm requires the algorithm to perform exact value estimation, which can be a hard task. Instead, in this work, we replace the less practical exact evaluation with partial evaluation; this comes with the price of more challenging analysis. Tackling this more intricate setup, we identify a key property required for the analysis to hold. We refer to it as h-greedy consistency. It will be central to all proofs in this work.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "Definition 1. A pair of value function and policy", "text": "(v, \u03c0) is h-greedy consistent if T \u03c0 T h\u22121 v \u2265 T h\u22121 v.\nIn words, (v, \u03c0) is h-greedy consistent if \u03c0 'improves', component-wise, the value T h\u22121 v. Since relaxing the evaluation stage comes with the h-greedy consistency requirement, the following question arises: while dispatching an algorithm, what is the price ensuring h-greedy consistency per each iteration? As we will see in the coming sections, it is enough to ensure h-greedy consistency only for the first iteration of our algorithms. For the rest of the iterations it holds by construction and is shown to be guaranteed in our proofs. Thus, by only initializing to an h-greedy consistent (v 0 , \u03c0 0 ), we enable guaranteeing the convergence of an algorithm that performs partial evaluation instead of exact in each its iterations. Ensuring consistency for the first iteration is straightforward, as is explained in the following remark.\nRemark 2. Choosing (v, \u03c0) which is h-greedy consistent can be done, e.g., by choosing v = Rmin 1\u2212\u03b3 (i.e., set every entrance of v to the minimal possible accumulated reward) and \u03c0 = \u03c0 h \u2208 G h (v). Furthermore, for any value-policy, (v, \u03c0), that is not h-greedy consistent, let\n\u2206 = max s T h\u22121v \u2212 T \u03c0 T h\u22121v (s) \u03b3 h\u22121 (1 \u2212 \u03b3) > 0,\nand set v =v \u2212 \u2206. Then, (v, \u03c0) is h-greedy consistent. This is a generalization to the construction given for h = 1 (see (Bertsekas and Tsitsiklis 1995), p. 46). h-greedy consistency is an h-step generalization of a notion already introduced in previous works on 1-step-based PI schemes with partial evaluation. The latter are known as 'optimistic' PI schemes and include Modified PI and \u03bb-PI (Bertsekas and Tsitsiklis 1995). There, the initial value-policy pair is assumed to be 1-greedy consistent, i.e. T \u03c01 v 0 \u2265 v 0 , e.g., (Bertsekas and Tsitsiklis 1995), p. 32 and 45, (Bertsekas 2011), p. 3, (Puterman and Shin 1978)[Theorem 2]. This property served as an assumption on the pair (v 0 , \u03c0 1 ).\nTo further motivate our interest in Definition 1, in the rest of the section we give two results that would be used in proofs later but are also insightful on their own. The following lemma gives that h-greedy consistency implies a sequence of value-function partial evaluation relations (see proof in Appendix A). Lemma 1. Let (v, \u03c0) be h-greedy consistent. Then,\nT \u03c0 T h\u22121 v \u2264 \u2022 \u2022 \u2022 \u2264 (T \u03c0 ) l T h\u22121 v \u2264 \u2022 \u2022 \u2022 \u2264 v \u03c0 .\nThe result shows that v \u03c0 is strictly bigger than T h\u22121 v. This property holds when v = v \u03c0 , i.e., when v is an exact value of some policy and was central in the analysis of h-PI (Efroni et al. 2018a). However, as Lemma 1 suggests, we only need h-greedy consistency, which is easier to have than estimating the exact value of a policy (see Remark 2).\nThe next result shows that if \u03c0 is taken to be the h-greedy policy, using partial evaluation results in a \u03b3 h contraction toward the optimal value v * (see proof in Appendix C).\nProposition 2. Let v and \u03c0 h \u2208 G h (v) be s.t. (v, \u03c0 h ) is h- greedy consistent. Then, for any m \u2265 1 and \u03bb \u2208 [0, 1], ||v * \u2212 (T \u03c0 h ) m T h\u22121 v|| \u221e \u2264 \u03b3 h ||v * \u2212 v|| \u221e and ||v * \u2212 T \u03c0 h \u03bb T h\u22121 v|| \u221e \u2264 \u03b3 h ||v * \u2212 v|| \u221e .\nIn (Efroni et al. 2018a)[Lemma 2], a similar contraction property was proved and played a central role in the analysis of the corresponding h-PI algorithm. Again, there, the requirement was v = v \u03c0 . Instead, the above result requires a weaker condition: h-greedy consistency of (v, \u03c0 h ).", "publication_ref": ["b3", "b0", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "The h-Greedy Policy Alone is Not Sufficient For Partial Evaluation", "text": "A more practical version of h-PI (Algorithm 1) would involve the mor \u03bb-return w.r.t. v k instead of the exact value. This would correspond to the update rules:\n\u03c0 k \u2190 arg max \u03c0 T \u03c0 T h\u22121 v k ,(6)\nv k+1 \u2190 (T \u03c0 k ) m v k or v k+1 \u2190 T \u03c0 k \u03bb v k .(7)\nIndeed, this would relax the evaluation task to an easier task than full policy evaluation. The next theorem suggests that for \u03c0 h \u2208 G h (v), even if (v, \u03c0 h ) is h-greedy consistent, the procedure ( 6)-( 7) does not necessarily contract toward the optimal policy, unlike the form of update in Proposition 2. To see that, note that both \u03b3 m + \u03b3 h and \u03b3(1\u2212\u03bb)\n1\u2212\u03bb\u03b3 + \u03b3 h can be larger than 1.\nTheorem 3. Let h > 1, m \u2265 1, and \u03bb \u2208 [0, 1]. Let v be a value function and \u03c0 h \u2208 G h (v) s.t. (v, \u03c0 h ) is h-greedy consistent (see Definition 1). Then, ||v * \u2212 (T \u03c0 h ) m v|| \u221e \u2264 (\u03b3 m + \u03b3 h )||v * \u2212 v|| \u221e ,(8)\n||v * \u2212 T \u03c0 h \u03bb v|| \u221e \u2264 \u03b3(1 \u2212 \u03bb) 1 \u2212 \u03bb\u03b3 + \u03b3 h ||v * \u2212 v|| \u221e . (9)\nAdditionally, there exist a \u03b3-discounted MDP, value function v, and policy\n\u03c0 h \u2208 G h (v) s.t. (v, \u03c0 h ) is h-greedy consistent,\nfor which (8) and ( 9) hold with equality.\nThe proof of the first statement is given in Appendix D, and the proof of the second statement is as follows.\nProof of second statement in Theorem 3. We prove this by constructing an example. Fix h > 1 and consider the corresponding 4-state MDP in Figure 2\n. Let v be v(s 0 ) = v(s 2 ) = v(s 3 ) = 0, v(s 1 ) = \u2212 1 1\u2212\u03b3 . Also, let \u03c0 h \u2208 G h (v). For this choice, observe that T h\u22121 v \u2264 T \u03c0 h T h\u22121 v, i.e., (v, \u03c0 h ) is h-greedy consistent.\nThe optimal policy from state s 0 is to choose the action 'up'. Thus, it is easy to see that, v * (s 0 ) = v * (s 3 ) = 1 1\u2212\u03b3 , and in the remaining of states it is easy to observe that\nv * (s 1 ) = v * (s 2 ) = 0. Now, see that for any h > 1 T h\u22121 v (s 1 ) = T h\u22121 v (s 2 ) = 0, T h\u22121 v (s 3 ) = 1\u2212\u03b3 h\u22121 1\u2212\u03b3 .\nThus, the h-greedy policy (by using (5)) is contained in the following set of actions \u03c0 h (s 0 ) \u2208 {right, up}, \u03c0 h (s 1 ) \u2208 {right, stay}, \u03c0 h (s 2 ), \u03c0 h (s 3 ) \u2208 {stay}. For example, we see that taking the action 'stay' or 'right' from state s 1 and then obtain T h\u22121 v have equal value:\nr(s 1 , 'stay ) + \u03b3(T h\u22121 v)(s 1 ) =r(s 1 , 'right ) + \u03b3(T h\u22121 v)(s 2 ) = 0.\nLet us choose an h-greedy policy, \u03c0 h , of the form: \u03c0 h (s 0 ) = right, \u03c0 h (s 1 ) = stay, \u03c0 h (s 2 ) = stay. Thus, from state s 0 , the m-return has the value\n((T \u03c0 h ) m v) (s 0 ) = m\u22121 i=0 \u03b3 t r(s i , \u03c0 h (s i )) + \u03b3 m v(s i=m ) = 1 \u2212 \u03b3 m \u2212 \u03b3 h 1 \u2212 \u03b3 + m\u22121 i=1 \u03b3 i \u2022 0 + \u03b3 m \u2212 1 1 \u2212 \u03b3 = 1 \u2212 \u03b3 m \u2212 \u03b3 h 1 \u2212 \u03b3 We thus have that ||v * \u2212 (T \u03c0 h ) m v|| \u221e = |v * (s 1,0 ) \u2212 (T \u03c0 h ) m v(s 1,0 )| = 1 1 \u2212 \u03b3 + \u03b3 m + \u03b3 h \u2212 1 1 \u2212 \u03b3 = (\u03b3 m + \u03b3 h ) 1 1 \u2212 \u03b3 (10\n)\ns 0 v(s 0 ) = 0 s 1 v(s 1 ) = \u2212 1 1\u2212\u03b3 s 2 v(s 2 ) = 0 s 3 v(s 3 ) = 0 1\u2212\u03b3 h 1\u2212\u03b3 0 0 1 1 0 Figure 2:\nThe MDP used in the proof of Theorem 3. NC-hm-PI and NC-h\u03bb-PI may result in a new value that does not contract toward v * .\nIt is also easy to see that ||v * \u2212v|| \u221e = 1 1\u2212\u03b3 . By using ( 10),\n||v * \u2212 (T \u03c0 h ) m v|| \u221e = (\u03b3 m + \u03b3 h )||v * \u2212 v|| \u221e ,\nwhich concludes the tightness result on the first result in Theorem 3. The tightness proof of ( 9) easily follows using the same construction as above; for details see Appendix D.\nAs discussed above, Theorem 3 suggests that the 'naive' partial-evaluation scheme would not necessarily lead to contraction toward the optimal value, especially for small values of h, m, \u03bb and large \u03b3; these are often values of interest. Moreover, the second statement in the theorem contrasts with the known result for h = 1, i.e., Modified PI and \u03bb-PI. There, a \u03b3-contraction was shown to exist (Scherrer 2013)[Proposition 8] and (Puterman and Shin 1978)[Theorem 2].\nFrom this point onwards, we shall refer to the algorithms given in ( 6)-( 7) and discussed in this section as Non-Contracting (NC)-hm-PI and NC-h\u03bb-PI.", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}, {"heading": "Backup the Tree-Search Byproducts", "text": "In the previous section, we proved that partial evaluation using the backed-up value function v, as given in ( 6)-( 7), is not necessarily a process converging toward the optimal value. In this section, we propose a natural respective fix: back up the value T h\u22121 v and perform the partial evaluation w.r.t. it. In the noise-free case this is motivated by Proposition 2, which reveals a \u03b3 h -contraction per each PI iteration.\nWe now introduce two new algorithms that relax h-PI's (from Algorithm 1) exact policy evaluation stage to the more practical mand \u03bb-return partial evaluation. Notice that hm-PI can be interpreted as iteratively performing h \u2212 1 steps of Value Iteration and one step of Modified PI (Puterman and Shin 1978), whereas instead of the latter, h\u03bb-PI performs one step of \u03bb-PI (Bertsekas and Ioffe 1996).\nOur algorithms also account for noisy updates in both the improvement and evaluation stages. For that purpose, we first define the following approximate improvement operator.", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 2 hm-PI", "text": "Initialize: h, m \u2208 N \\ {0}, v \u2208 R |S| while stopping criterion is false do \u03c0 k+1 \u2190 \u03c0 \u2208 G \u03b4 k+1 h (v k ) v k+1 \u2190 (T \u03c0 k+1 ) m T h\u22121 v k + k k \u2190 k + 1 end while Return \u03c0, v Algorithm 3 h\u03bb-PI Initialize: h \u2208 N \\ {0}, \u03bb \u2208 [0, 1], v \u2208 R |S| while stopping criterion is false do \u03c0 k+1 \u2190 \u03c0 \u2208 G \u03b4 k+1 h (v k ) v k+1 \u2190 T \u03c0 k+1 \u03bb T h\u22121 v k + k k \u2190 k + 1 end while Return \u03c0, v Definition 2. For\u03b4 \u2208 R |S| + , let G\u03b4 h (v) be the approximate h- greedy set of policies w.r.t. v with error\u03b4, s.t. for \u03c0 \u2208 G\u03b4 h (v), T \u03c0 T h\u22121 v \u2265 T h v \u2212\u03b4.\nAdditionally, the algorithms assume additive\u02c6 \u2208 R |S| error in the evaluation stage. We call them hm-PI and h\u03bb-PI and present them in Algorithms 2 and 3. As opposed to the non-contracting update discussed in Section 5, the evaluation stage in these algorithms uses T h\u22121 v.\nWe now provide our main result, demonstrating a \u03b3 hcontraction coefficient for both hm-PI and h\u03bb-PI.\nThe proof technique builds upon the previously introduced invariance argument (see (Efroni et al. 2018a), proof of Theorem 9). This enables working with a more convenient, shifted noise sequence. Thereby, we construct a shifted noise sequence s.t. the value-policy pair (v k , \u03c0 k+1 ) in each iteration is h-greedy consistent (see Definition 1). We thus also eliminate the h-greedy consistency assumption on the initial (v 0 , \u03c0 1 ) pair, which appears in previous works (see Remark 2). Specifically, we shift v 0 by \u2206 0 ; the latter quantifies how 'far' (v 0 , \u03c0 1 ) is from being h-greedy consistent. Notice our bound explicitly depends on \u2206 0 . The provided proof is simpler and shorter than in previous works (e.g. (Scherrer 2013)). We believe that the proof technique presented here can be used as a general 'recipe' for proving newly-devised PI procedures that use partial evaluation with more ease. Theorem\n4. Let h, m \u2208 N \\ {0}, \u03bb \u2208 [0, 1]. For noise se- quences { k } and {\u03b4 k }, k \u221e \u2264 and \u03b4 k \u221e \u2264 \u03b4. Let \u2206 0 = max{0, max s T h\u22121 v 0 \u2212 T \u03c01 T h\u22121 v 0 (s) \u03b3 h\u22121 (1 \u2212 \u03b3) }.\nThen,\nv * \u2212 v \u03c0 k+1 \u221e \u2264 \u03b3 kh ||v * \u2212 (v 0 \u2212 \u2206 0 )|| \u221e + (2\u03b3 h + \u03b4)(1 \u2212 \u03b3 kh ) (1 \u2212 \u03b3)(1 \u2212 \u03b3 h ) and hence lim sup k\u2192\u221e v * \u2212 v \u03c0 k \u221e \u2264 2\u03b3 h +\u03b4 (1\u2212\u03b3)(1\u2212\u03b3 h ) . Proof.\nWe start with the invariance argument. Consider the process with the alternative error in the evaluation stage,\nk = k \u2212 C k e, where C k = max \u03b4 k+1 +\u03b3 h\u22121 max k \u2212\u03b3 h min k \u03b3 h\u22121 (1\u2212\u03b3)\n, and e a vector of 'ones' of dimension |S|. Next, given initial value v 0 , let v 0 = v 0 \u2212 \u2206 0 . As described in Remark 2, this transformation makes (v 0 , \u03c0 1 ) h-greedy consistent. Since the greedy policy is invariant for an addition of a constant, i.e., for \u03b1 \u2208 R, G h (v + \u03b1e) = G h (v), and since T h (v + \u03b1e) = T h v + \u03b3 h \u03b1, we have that the sequence of policies generated is invariant for the offered transformation.\nNext, we use Lemma 6, which gives that the choice of C k leads to a sequence of pairs of h-greedy consistent policies and values in every iteration. Thus, we can now continue with simpler analysis than in (Efroni et al. 2018a).\nAt this stage of the proof we focus on hm-PI. Define\nd k def = v * \u2212 (v k \u2212 k ) for k \u2265 1, and d 0 def = v * \u2212 v 0 . We get d k+1 = v * \u2212 (T \u03c0 k+1 ) m T h\u22121 v k (11) \u2264 v * \u2212 T \u03c0 k+1 T h\u22121 v k (12) \u2264 v * \u2212 T h v k + max \u03b4 k+1 \u2264 (T \u03c0 * ) h v * \u2212 T h (v k \u2212 k ) \u2212 \u03b3 h min k + max \u03b4 k+1 \u2264 (T \u03c0 * ) h v * \u2212 (T \u03c0 * ) h (v k \u2212 k ) \u2212 \u03b3 h min k + max \u03b4 k+1 = \u03b3 h (P \u03c0 * ) h (v * \u2212 (v k \u2212 k )) \u2212 \u03b3 h min k + max \u03b4 k+1 = \u03b3 h (P \u03c0 * ) h d k \u2212 \u03b3 h min k + max \u03b4 k+1 .\nThe second relation holds by applying Lemma 1 on \u03c0 k+1 and v k which are h-consistent. Furthermore, by using the form of C k and simple algebraic manipulations it can be shown that \u2212\u03b3 h min k + max \u03b4 k+1 \u2264 2\u03b3 h +\u03b4 1\u2212\u03b3 . Thus,\nd k+1 \u2264 \u03b3 h (P \u03c0 * ) h d k + 2\u03b3 h + \u03b4 1 \u2212 \u03b3 . (13\n)\nIteratively applying the above relation on k, we get that\nd k \u2264 \u03b3 kh (P \u03c0 * ) kh d 0 + (2\u03b3 h + \u03b4)(1 \u2212 \u03b3 kh ) (1 \u2212 \u03b3)(1 \u2212 \u03b3 h ) \u2264 \u03b3 kh ||d 0 || \u221e + (2\u03b3 h + \u03b4)(1 \u2212 \u03b3 kh ) (1 \u2212 \u03b3)(1 \u2212 \u03b3 h ) . (14\n)\nTo conclude the proof for hm-PI notice that \nv * \u2212 v \u03c0 k+1 \u2264 v * \u2212 (v k \u2212 k ) = d k ,\n||v * \u2212 v \u03c0 k+1 || \u221e \u2264 \u03b3 kh ||d 0 || \u221e + (2\u03b3 h + \u03b4)(1 \u2212 \u03b3 kh ) (1 \u2212 \u03b3)(1 \u2212 \u03b3 h ) . Since d 0 = v * \u2212 v 0 = v * \u2212 (v 0 \u2212 \u2206 0 )\n, we obtain the first claim for hm-PI. Taking the limit easily gives the second claim, again for hm-PI:\nlim k\u2192\u221e ||v * \u2212 v \u03c0 k+1 || \u221e \u2264 2\u03b3 h + \u03b4 (1 \u2212 \u03b3)(1 \u2212 \u03b3 h ) .\nThe convergence proof for h\u03bb-PI is identical to that of hm-PI, except for a minor change: the transition from (11) to ( 12) holds due to the following argument:\nd k+1 \u2264 v * \u2212 (1 \u2212 \u03bb) i \u03bb i (T \u03c0 k+1 ) i+1 T h\u22121 v k \u2264 v * \u2212 (1 \u2212 \u03bb) i \u03bb i T \u03c0 k+1 T h\u22121 v k = v * \u2212 T \u03c0 k+1 T h\u22121 v k ,\nwhere the second relation holds by applying Lemma 1. This can be used since \u03c0 k+1 and v k are h-greedy consistent according to Lemma 6. This exemplifies the advantage of using the notion of h-greedy consistency in our proof technique.\nThanks to using T h\u22121 v in the evaluation stage, Theorem 4 guarantees a convergence rate of \u03b3 h -as to be expected when using a T h greedy operator. Compared to directly using v as is done in Section 5, this is a significant improvement since the latter does not even necessarily contract.\nA possibly more 'natural' version of our algorithms would back up the value of the root node instead of its descendants. The following remark extends on that. Remark 3. Consider a variant of hm-PI and h\u03bb-PI, which backs\n-up T \u03c0 k+1 T h\u22121 v k instead of T h\u22121 v k . Namely, in this variant, the evaluation stage for hm-PI (Algorithm 2) is v k+1 \u2190 (T \u03c0 k+1 ) m\u22121 (T \u03c0 k+1 T h\u22121 v k ) + k , and for h\u03bb-PI (Algorithm 3) it is v k+1 \u2190T \u03c0 k+1 \u03bb (T \u03c0 k+1 T h\u22121 v k ) + k . The latter is (see Appendix F) T \u03c0 \u03bb v def = (1 \u2212 \u03bb) \u221e j=0 \u03bb j (T \u03c0 ) j v = v + \u03bb(I \u2212 \u03b3\u03bbP \u03c0 ) \u22121 (T \u03c0 v \u2212 v)\n-a variation of the \u03bbreturn operator from (4), in which T \u03c0 is raised to the power of j and not j + 1. The performance of these algorithms is equivalent to that of the original hm-PI and h\u03bb-PI, as given in Theorem 4, since\n(T \u03c0 ) m\u22121 T \u03c0 = (T \u03c0 ) m andT \u03c0 \u03bb T \u03c0 = T \u03c0 \u03bb .\nYet, implementing them is potentially easier in practice, and can be considered more 'natural' due to the backup of the root optimal value rather its descendants.", "publication_ref": ["b0", "b5", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Relation to Existing Work", "text": "In the context of related theoretical work, we find two results necessitating a discussion. The first is the performance bound of Non-Stationary Approximate Modified PI (NS-AMPI) (Lesner and Scherrer 2015)[Theorem 3]. Compared to it, Theorem 4 reveals two improvements. First, it gives that hm-PI is less sensitive to errors; our bound's numerator has \u03b3 h instead of \u03b3. Second, in each iteration, hm-PI requires storing a single policy in lieu of h policies as in NS-AMPI. This makes hm-PI significantly more memory efficient. Nonetheless, there is a caveat in our work compared to (Lesner and Scherrer 2015). In each iteration, we require to approximately solve an h-finite-horizon problem, while they require solving approximate 1-step greedy problem instances.\nThe second relevant theoretical result is the performance bound of a recently introduced MCTS-based RL algorithm (Jiang, Ekwedike, and Liu 2018)[Theorem 1]. There, in the noiseless case there is no guarantee for convergence to the optimal policy 1 . Contrarily, in our setup, with \u03b4 = 0 and = 0 both hm-PI and h\u03bb-PI converge to the optimal policy. Next, we discuss related literature on empirical studies and attempt to explain observations there with the results of this work. In (Baxter, Tridgell, and Weaver 1999;Veness et al. 2009;Lanctot et al. 2014) the idea of incorporating the optimal value from the tree-search was experimented with. Most closely related to our synchronous setup is that in (Baxter, Tridgell, and Weaver 1999). There, motivated by practical reasons, the authors introduced and evaluated both NC h\u03bb-PI and h\u03bb-PI, which they respectively call TD-directed(\u03bb) and TDLeaf(\u03bb). Specifically, TD-directed(\u03bb) and TDLeaf(\u03bb) respectively back up v and T \u03c0 h T h\u22121 v. As Remark 1 suggests, T \u03c0 h T h\u22121 v can be extracted directly from the tree-search, as is also pointed out in (Baxter, Tridgell, and Weaver 1999). Interestingly, the authors show that TDLeaf(\u03bb) outperforms TD-directed(\u03bb). Indeed, Theorem 3 sheds light on this phenomenon.\nLastly, a prominent takeaway message from Theorems 3 and 4 is that AlphaGoZero (Silver et al. 2017b;Silver et al. 2017a) can be potentially improved. This is because in (Silver et al. 2017b), the authors do not back up the optimal value calculated from the tree search. As their approach relies on PI (and specifically resembles to h-PI), our analysis, which covers noisy partial evaluation, can be beneficial even in the practical setup of AlphaGoZero.", "publication_ref": ["b3", "b2", "b0", "b6", "b3", "b0", "b0", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In this section, we empirically study NC-hm-PI (Section 5) and hm-PI (Section 6) in the exact and approximate cases. Additional results can also be found in Appendix G. Our experiments demonstrate the practicalities of Theorem 3 and 4, even in the simple setup considered here.\nWe conducted our simulations on a simple N \u00d7 N deterministic grid-world problem with \u03b3 = 0.97, as was done in (Efroni et al. 2018a). The action set is {'up','down','right','left','stay'}. In each experiment, a reward r g = 1 was placed in a random state while in all other states the reward was drawn uniformly from [\u22120.1, 0.1]. In the considered problem there is no terminal state. Also, the entries of the initial value function are drawn from N (0, 1). We ran the algorithms and counted the total number of calls to the simulator. Each such \"call\" takes a state-action pair (s, a) as input, and returns the current reward and next (deterministic) state. Thus, it quantifies the total running time of the algorithm, and not the total number of iterations.\nWe begin with the noiseless case, in which k and \u03b4 k from Algorithm 2 are 0. While varying h and m, we counted the total number of queries to the simulator until convergence, which defined as ||v * \u2212 v k || \u221e \u2264 10 \u22127 . Figure 3  Total Queries to Simulator NC-hm-PI time for equal ranges of h and m. It highlights the suboptimality of NC-hm-PI compared to hm-PI. As expected, for h = 1, the results coincide for NC-hm-PI and hm-PI since the two algorithms are then equivalent. For h > 1, the performance of NC-hm-PI significantly deteriorates up to an order of magnitude compared to hm-PI. However, the gap between the two becomes less significant as m increases. This can be explained with Theorem 3: increasing m in NC-hm-PI drastically shrinks \u03b3 m in (8) and brings the contraction coefficient closer to \u03b3 h , which is that of hm-PI. In the limit m \u2192 \u221e both algorithms become h-PI.\nh = 1 h = 2 h = 3 h = 4 h = 5\nThe bottom row in Figure 3 depicts the convergence time in 1-d plots for several small values of h and a large range of m. It highlights the tradeoff in choosing m. As h increases, the optimal choice of m increases as well. Further rigorous analysis of this tradeoff in m versus h is an intriguing subject for future work.\nNext, we tested the performance of NC-hm-PI and hm-PI in the presence of evaluation noise. Specifically, \u2200k, s \u2208 S, k (s) \u223c U (\u22120.3, 0.3) and \u03b4 k (s) = 0. For NC-hm-PI, the noise was added according to v k+1 \u2190 (T \u03c0 k ) m v k + k instead of the update in the first equation in (7). The value \u03b4 k = 0 corresponds to having access to the exact model. Generally, one could leverage the model for a complete immediate solution instead of using Algorithm 2, but here we consider cases where this cannot be done due to, e.g., too large of a state-space. In this case, we can approximately estimate the value and use a multiple-step greedy operator with access to the exact model. Indeed, this setup is conceptually similar to that taken in Al- phaGoZero (Silver et al. 2017b). Figure 4 exhibits the results. The heatmap values are ||v * \u2212 v \u03c0 f || \u221e , where \u03c0 f is the algorithms' output policy after 4 \u2022 10 6 queries to the simulator. Both NC-hm-PI and hm-PI converge to a better value as h increases. However, this effect is stronger in the latter compared to the former, especially for small values of m. This demonstrates how hm-PI is less sensitive to approximation error. This behavior corresponds to the hm-PI error bound in Theorem 4, which decreases as h increases.", "publication_ref": ["b0"], "figure_ref": ["fig_1", "fig_1", "fig_2"], "table_ref": []}, {"heading": "Summary and Future Work", "text": "In this work, we formulated, analyzed and tested two approaches for relaxing the evaluation stage of h-PI -a multiplestep greedy PI scheme. The first approach backs up v and the second backs up T h\u22121 v or T \u03c0 h T h\u22121 v (see Remark 3). Although the first might seem like the natural choice, we showed it performs significantly worse than the second, especially when combined with short-horizon evaluation, i.e., small m or \u03bb. Thus, due to the intimate relation between h-PI and state-of-the-art RL algorithms (e.g., (Silver et al. 2017b)), we believe the consequences of the presented results could lead to better algorithms in the future.\nAlthough we established the non-contracting nature of the algorithms in Section 5, we did not prove they would necessarily not converge. We believe that further analysis of the non-contracting algorithms is intriguing, especially given their empirical converging behavior in the noiseless case (see Section 8, Figure 3). Understanding when the noncontracting algorithms perform well is of value, since their update rules are much simpler and easier to implement than the contracting ones.\nTo summarize, this work highlights yet another difference between 1-step based and multiple-step based PI methods, on top of the ones presented in (Efroni et al. 2018a;Efroni et al. 2018b). Namely, multiple-step based methods introduce a new degree of freedom in algorithm design: the utilization of the planning byproducts. We believe that revealing additional such differences and quantifying their pros and cons is both intriguing and can have meaningful algorithmic consequences.", "publication_ref": ["b0", "b1"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "A Proof of Lemma 1", "text": "Since (v, \u03c0) is h-greedy consistent we have that,\nT h\u22121 v \u2264 T \u03c0 T h\u22121 v.\nBy remembering that (T \u03c0 ) l\u22121 ,for any l \u2208 N \\ {0}, is a monotonic operator we have that\n(T \u03c0 ) l\u22121 T h\u22121 v \u2264 (T \u03c0 ) l T h\u22121 v.\nWe can concatenate the inequalities and conclude by observing that lim l\u2192\u221e (T \u03c0 ) l T h\u22121 v = v \u03c0 , since T \u03c0 is a contraction operator with a fixed point v \u03c0 .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Affinity of T \u03c0 and Consequences", "text": "In this section we prove, for completeness, an important property of T \u03c0 (which was also described in (Efroni et al. 2018a)\n[Appendix B]). Lemma 5. Let {v i , \u03bb i } \u221e\ni=0 be a series of value functions, v i \u2208 R |S| , and positive real numbers, \u03bb i \u2208 R + , such that \u221e i=0 \u03bb i = 1. Let T \u03c0 be a fixed policy Bellman operator and n \u2208 N. Then,\nT \u03c0 ( \u221e i=0 \u03bb i v i ) = \u221e i=0 \u03bb i T \u03c0 v i , (T \u03c0 ) n ( \u221e i=0 \u03bb i v i ) = \u221e i=0 \u03bb i (T \u03c0 ) n v i .\nProof. Using simple algebra and the definition of T \u03c0 (see Definition 1) we have that\nT \u03c0 ( \u221e i=0 \u03bb i v i ) = r \u03c0 + \u03b3P \u03c0 ( \u221e i=0 \u03bb i v i ) = r \u03c0 + \u221e i=0 \u03bb i \u03b3P \u03c0 v i = \u221e i=0 \u03bb i (r \u03c0 + \u03b3P \u03c0 v i ) = \u221e i=0 \u03bb i T \u03c0 v i .\nThe second claim is a result of the first claim and is proved by iteratively applying the first relation.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "C Proof of Proposition 2", "text": "The proof goes as follows.\nv * \u2212 (T \u03c0 h ) m T h\u22121 v \u2264 v * \u2212 T \u03c0 h T h\u22121 v (15) = v * \u2212 T h v = (T \u03c0 * ) h v * \u2212 T h v \u2264 (T \u03c0 * ) h v * \u2212 (T \u03c0 * ) h v \u2264 \u03b3 h (P \u03c0 * ) h (v * \u2212 v) \u2264 \u03b3 h ||v * \u2212 v|| \u221e .\nThe first relation holds due to Lemma 1, the second relation holds since \u03c0 h \u2208 G h (v), and the last relation holds since (P \u03c0 * ) h is a stochastic matrix. To prove similar result for the second claim we merely change the first relation, to\nv * \u2212 T \u03c0 h \u03bb T h\u22121 v = v * \u2212 (1 \u2212 \u03bb) i \u03bb i (T \u03c0 h ) i+1 T h\u22121 v (16) \u2264 v * \u2212 (1 \u2212 \u03bb) i \u03bb i T \u03c0 h T h\u22121 v = v * \u2212 T \u03c0 h T h\u22121 v,\nwhere the second relation holds according to Lemma 1 since (\u03c0 h , v) are h-greedy consistent. Furthermore,\n(T \u03c0 h ) m T h\u22121 v \u2264 v \u03c0 h \u2264 v * ,and\nT \u03c0 h \u03bb T h\u22121 v =(1 \u2212 \u03bb) i \u03bb i (T \u03c0 h ) i+1 (T \u03c0 h ) h\u22121 v \u2264(1 \u2212 \u03bb) i \u03bb i v \u03c0 = v \u03c0 \u2264 v * ,\nwhere the first inequality in both of the relations above holds due to Lemma 1, and the second inequality holds since v \u03c0 \u2264 v * for any \u03c0. Thus, we have that v * \u2212 (T \u03c0 h ) m T h\u22121 v, v * \u2212 T \u03c0 h \u03bb T h\u22121 v \u2265 0, component-wise, and we can take the max-norm on the LHS of ( 15) and ( 16) to prove the statements.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D Proof of Theorem 3", "text": "We begin with proving (8). We have that\nv * \u2212 (T \u03c0 h ) m v =v * \u2212 v \u03c0 h + v \u03c0 h \u2212 (T \u03c0 h ) m v =(T \u03c0 * ) h v * \u2212 v \u03c0 h + (T \u03c0 h ) m v \u03c0 h \u2212 (T \u03c0 h ) m v =(T \u03c0 * ) h v * \u2212 v \u03c0 h + \u03b3 m (P \u03c0 h ) m (v \u03c0 h \u2212 v) \u2264(T \u03c0 * ) h v * \u2212 v \u03c0 h + \u03b3 m (P \u03c0 h ) m (v * \u2212 v) \u2264(T \u03c0 * ) h v * \u2212 T h v + \u03b3 m (P \u03c0 h ) m (v * \u2212 v) \u2264(T \u03c0 * ) h v * \u2212 (T \u03c0 * ) h v + \u03b3 m (P \u03c0 h ) m (v * \u2212 v) =\u03b3 h (P \u03c0 * ) h (v * \u2212 v) + \u03b3 m (P \u03c0 h ) m (v * \u2212 v) \u2264(\u03b3 h + \u03b3 m ) v * \u2212 v \u221e . (17\n)\nThe forth relation holds since v * \u2265 v \u03c0 , the fifth relation holds due to Lemma 1, the sixth relation holds by the definition of the optimal Bellman operator (namely, T l v \u2265 (T \u03c0 ) l v for any v and \u03c0), and the last relation holds since (P \u03c0 * ) h , (P \u03c0 h ) h are stochastic matrices. We also have that\n(T \u03c0 h ) m v \u2212 v * = (T \u03c0 h ) m v \u2212 T m v * = (T \u03c0 h ) m v \u2212 (T \u03c0 h ) m v * = \u03b3 m (P \u03c0 h ) m (v \u2212 v * ) \u2264 \u03b3 m v \u2212 v * \u221e \u2264 (\u03b3 h + \u03b3 m ) v * \u2212 v \u221e . (18\n)\nWhere the first relation holds since v * is the fixed point of T m , the second relation holds by the definition of the optimal Bellman operator, and the forth relation holds since (P \u03c0 h ) m is a stochastic matrix. Combining ( 17) and ( 17) yields\n||v * \u2212 (T \u03c0 h ) m v|| \u221e \u2264(\u03b3 h + \u03b3 m )||v * \u2212 v|| \u221e .(19)\nThe second statement is a consequence of the first statement.\n||v * \u2212 T \u03c0 h \u03bb v|| \u221e =||v * \u2212 (1 \u2212 \u03bb) i \u03bb i (T \u03c0 h ) i+1 v|| \u221e =||(1 \u2212 \u03bb) i \u03bb i (v * \u2212 (T \u03c0 h ) i+1 v)|| \u221e \u2264(1 \u2212 \u03bb) i \u03bb i ||v * \u2212 (T \u03c0 h ) i+1 v|| \u221e \u2264 (1 \u2212 \u03bb) i \u03bb i (\u03b3 i+1 + \u03b3 h ) ||v * \u2212 v|| \u221e = \u03b3(1 \u2212 \u03bb) 1 \u2212 \u03bb\u03b3 + \u03b3 h ||v * \u2212 v|| \u221e .\nIn the first relation we use the definition of T \u03c0 h \u03bb , the third relation holds due to the triangle's inequality and the forth relation holds due to (19).\nTo conclude the proof we finish proving the tightness of (9) using the same construction given in the part of the proof that is in the paper's body:\n(T \u03c0 h \u03bb v)(s 0 ) = 1 \u2212 \u03b3 h 1 \u2212 \u03b3 + \u221e i=0 (\u03b3\u03bb) i (\u03b3(1 \u2212 \u03b3)v(s 1 )) = 1 \u2212 \u03b3 h 1 \u2212 \u03b3 \u2212 \u03b3(1 \u2212 \u03bb) (1 \u2212 \u03b3\u03bb) \u2022 1 1 \u2212 \u03b3 .\nSee that\n|(T \u03c0 h \u03bb v)(s 0 ) \u2212 v * (s 0 )| = \u03b3 h + \u03b3(1 \u2212 \u03bb) (1 \u2212 \u03b3\u03bb) 1 1 \u2212 \u03b3 . Since ||(T \u03c0 h \u03bb v) \u2212 v * || \u221e = |(T \u03c0 h \u03bb v)(s 0 ) \u2212 v * (s 0 )|, ||v * \u2212 T \u03c0 h \u03bb v|| \u221e = \u03b3 h + \u03b3(1 \u2212 \u03bb) (1 \u2212 \u03b3\u03bb) 1 1 \u2212 \u03b3 = \u03b3 h + \u03b3(1 \u2212 \u03bb) (1 \u2212 \u03b3\u03bb) ||v * \u2212 v|| \u221e E h-Greedy Consistency in Each Iteration\nThe following result is used to prove Theorem 3. According to it, the choice of C k leads to a sequence of h-greedy consistent policies and values in every iteration.\nLemma 6. Let k = k \u2212 C k e, where C k = max \u03b4 k+1 +\u03b3 h\u22121 max k \u2212\u03b3 h min k \u03b3 h\u22121 (1\u2212\u03b3)\nand e is a vector of 'ones' of dimension |S|. For both hm-PI or h\u03bb-PI, let the value function at the k-th iteration with the alternative error, k , be\nv k . Let \u03c0 k+1 \u2208 G \u03b4 k+1 h (v k ). Then, in every iteration k (v k , \u03c0 k+1 ) is h-greedy consistent; i.e., T h\u22121 v k \u2264 T \u03c0 k+1 T h\u22121 v k , and v k \u2212 k \u2264 T \u03c0 k+1 T h\u22121 v k .\nProof of Lemma 6: hm-PI part. The proof goes by induction. The induction hypothesis is that\n(\u03c0 k , v k\u22121 ) is h-greedy consistent, T h\u22121 v k\u22121 \u2264 T \u03c0 k T h\u22121 v k\u22121\n, and we show it induces both of relations. The base case holds, i.e., (\u03c0 1 , v 0 ) is h-greedy consistent, due to v 0 = v 0 \u2212 d, (see Remark 2).\nWe start by proving that (\u03c0 k+1 , v k ) for every k by proving the induction step.\nT h\u22121 v k \u2212 T \u03c0 k+1 T h\u22121 v k \u2264 T h\u22121 v k \u2212 T h v k + max \u03b4 k+1 = T h\u22121 (v k \u2212 k ) \u2212 T h (v k \u2212 k ) + \u03b3 h\u22121 max k \u2212 \u03b3 h min k + max \u03b4 k+1 = T h\u22121 (v k \u2212 k ) \u2212 T h (v k \u2212 k ),(20)\nwhere the last relation holds due to the choice of k and C k , by which we get \u03b3 h\u22121 max k \u2212 \u03b3 h min k + max \u03b4 k+1 = 0. We continue with the analysis from (20),\nT h\u22121 (v k \u2212 k ) \u2212 T h (v k \u2212 k ) = T h\u22121 (T \u03c0 k ) m T h\u22121 v k\u22121 \u2212 T h (T \u03c0 k ) m T h\u22121 v k\u22121 \u2264 T h\u22121 T \u03c0 k (T \u03c0 k ) m T h\u22121 v k\u22121 \u2212 T h (T \u03c0 k ) m T h\u22121 v k\u22121 \u2264 T h\u22121 T (T \u03c0 k ) m T h\u22121 v k\u22121 \u2212 T h (T \u03c0 k ) m T h\u22121 v k\u22121 = 0.\nIn the third relation we used Lemma 1 due to the assumption that (v k\u22121 , \u03c0 k ) is h-greedy consistent and the monotonicity of T h\u22121 , in the forth relation we used the definition of the optimal Bellman operator, i.e., T \u03c0v \u2264 Tv, and the monotonicity of T h\u22121 , and in the last relation we used T h\u22121 T = T h and recognized the two terms cancel one another.\nThis concludes that that for hm-PI the sequence of policies and alternative values are h-greedy consistent. We now prove that\nv k \u2212 k \u2212 T \u03c0 k+1 T h\u22121 v k \u2264 0 for hm-PI. v k \u2212 k \u2212 T \u03c0 k+1 T h\u22121 v k \u2264 v k \u2212 T h v k + max \u03b4 k+1 \u2264 v k \u2212 k \u2212 T h (v k \u2212 k ) \u2212 \u03b3 h min k + max \u03b4 k+1 \u2264 v k \u2212 k \u2212 T h (v k \u2212 k )(21)\nThe last relation holds due to\n\u2212 \u03b3 h min k + max \u03b4 k+1 = \u2212\u03b3 h min k + max \u03b4 k+1 \u2212 (1 \u2212 \u03b3 h )C k = max k (\u2212 1 \u2212 \u03b3 h 1 \u2212 \u03b3 ) + \u03b3 h min k ( 1 \u2212 \u03b3 h \u03b3 h\u22121 (1 \u2212 \u03b3) \u2212 1) + max \u03b4 k+1 (1 \u2212 1 \u2212 \u03b3 h \u03b3 h\u22121 (1 \u2212 \u03b3) ) \u2264 0.\nSee that the first and third terms are negative. Furthermore, min k \u2264 0 (if not, we can omit it in all previous analysis) and its coefficient is positive, the second term is also negative as well, and thus the entire expression is negative. We continue with the analysis from ( 21),\nv k \u2212 k \u2212 T \u03c0 k+1 T h\u22121 v k \u2264 v k \u2212 k \u2212 T h ((v k \u2212 k )) \u2264 (T \u03c0 k ) m T h\u22121 v k\u22121 \u2212 T h (T \u03c0 k ) m T h\u22121 v k\u22121 \u2264 (T \u03c0 k ) h (T \u03c0 k ) m T h\u22121 v k\u22121 \u2212 T h (T \u03c0 k ) m T h\u22121 v k\u22121 \u2264 T h (T \u03c0 k ) m T h\u22121 v k\u22121 \u2212 T h (T \u03c0 k ) m T h\u22121 v k\u22121 = 0.\nWhere the third relation holds due to Lemma 1, and in the forth relation we used the definition of the optimal Bellman operator, i.e., (T \u03c0 ) hv \u2264 T hv .\nSince (\u03c0 k , v k\u22121 ) is h-greedy consistent due to the first claim we get\nv k \u2212 k \u2212 T \u03c0 k+1 T h\u22121 v k \u2264 0.\nTo prove the statements for the h\u03bb-PI we merely have to perform a minor change in ( 20) and ( 21) and to use the following Lemma, which is a consequence of Lemma 1.\nLemma 7. Let \u03bb \u2208 [0, 1], l \u2208 N and (v, \u03c0) be h-greedy consistent. Then, T \u03c0 \u03bb T h\u22121 v \u2264 (T \u03c0 ) l T \u03c0 \u03bb T h\u22121 v. Proof. We have that T \u03c0 \u03bb T h\u22121 v = (1 \u2212 \u03bb) i \u03bb i (T \u03c0 ) i+1 T h\u22121 v \u2264 (1 \u2212 \u03bb) i \u03bb i (T \u03c0 ) i+1+l T h\u22121 v = (1 \u2212 \u03bb) i \u03bb i (T \u03c0 ) l (T \u03c0 ) i+1 T h\u22121 v = (T \u03c0 ) l (1 \u2212 \u03bb) i \u03bb i (T \u03c0 ) i+1 T h\u22121 v = (T \u03c0 ) l T \u03c0 \u03bb T h\u22121 v.\nWhere the third relation holds due to Lemma 1, and the forth relation holds by using Lemma 5.\nProof of Lemma 6: h\u03bb-PI part. To prove that h\u03bb-PI preserves the h-greedy consistency we start from (20) and follow similar line of proof.\nT h\u22121 (v k \u2212 k ) \u2212 T h (v k \u2212 k ) = T h\u22121 T \u03c0 k \u03bb v k\u22121 \u2212 T h T \u03c0 k \u03bb v k\u22121 \u2264 T h\u22121 T \u03c0 k T \u03c0 k \u03bb v k\u22121 \u2212 T h T \u03c0 k \u03bb v k\u22121 \u2264 T h\u22121 T T \u03c0 k \u03bb v k\u22121 \u2212 T h T \u03c0 k \u03bb v k\u22121 = 0.\nWhere the third relation holds due to Lemma 7, and in the forth relation we used the definition of the optimal Bellman operator, i.e., T \u03c0v \u2264 Tv, and the monotonicity of T h\u22121 .\nThis proves that the h-greedy consistency is preserved in h\u03bb-PI as well. To prove the second statement for h\u03bb-PI we start from (21).\nv k \u2212 k \u2212 T h (v k \u2212 k ) = T \u03c0 k \u03bb v k\u22121 \u2212 T h T \u03c0 k \u03bb v k\u22121 \u2264 (T \u03c0 k ) h T \u03c0 k \u03bb v k\u22121 \u2212 T h T \u03c0 k \u03bb v k\u22121 \u2264 T h T \u03c0 k \u03bb v k\u22121 \u2212 T h T \u03c0 k \u03bb v k\u22121 = 0.\nWhere the third relation holds due to Lemma 7 and the monotonicity of T h\u22121 , and in the forth relation we used the definition of the optimal Bellman operator, i.e,, (T \u03c0 ) hv \u2264 T hv . F A Note on the Alternative \u03bb-Return Operator\nIn Remark 3 we defined an alternative \u03bb-return operator,T \u03c0 \u03bb def = (1 \u2212 \u03bb) \u221e j=0 \u03bb j (T \u03c0 ) j v. We give here an equivalence form of this operator. Proposition 8. For any \u03c0 and \u03bb \u2208 [0, 1]T \u03c0 \u03bb v = v + \u03bb(I \u2212 \u03b3\u03bbP \u03c0 ) \u22121 (T \u03c0 v \u2212 v) Proof. This relation can be easily derived by using the equivalence in (4). We have that\nT \u03c0 \u03bb def = (1 \u2212 \u03bb) \u221e j=0 \u03bb j (T \u03c0 ) j v = (1 \u2212 \u03bb)v + (1 \u2212 \u03bb) \u221e j=1 \u03bb j (T \u03c0 ) j v = (1 \u2212 \u03bb)v + \u03bb(1 \u2212 \u03bb) \u221e j=0 \u03bb j (T \u03c0 ) j+1 v = (1 \u2212 \u03bb)v + \u03bb(1 \u2212 \u03bb) \u221e j=0 \u03bb j (T \u03c0 ) j+1 v \u03bbT \u03c0 \u03bb v = v + \u03bb(I \u2212 \u03b3\u03bbP \u03c0 ) \u22121 (T \u03c0 v \u2212 v),\nwhere in the last relation we used the equivalent form of T \u03c0 \u03bb provided in (4).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G More Experimental Results", "text": "In this section we add more empirical result on the convergence of the tested algorithms in Section 8 in the approximate case (as described in Section 8). Specifically, we plot ||v * \u2212 v k || \u221e versus the total number of queries to the simulator, where v k is the value function. This complements the plot in Section 8, there we plot ||v * \u2212 v \u03c0 f || \u221e , where v \u03c0 f is the exact value of the policy that the algorithms output.\nIn the presence of errors, the value does not converge to a point in the, but only to a region. According to Theorem 4, as h increases, hm-PI is expected to converge to a 'better' policy (i.e., closer to the optimal policy). As the results in Figure 5 demonstrate, also the value function, v, of hm-PI converges to a better region than NC-hm-PI. This would be expected since a better policy would correspond to a better value function estimate. Furthermore, it is also observed that hm-PI converges faster than NC-hm-PI. This is again expected due to the possible non-contracting nature of this algorithm.\nLastly, in Figure 6 the standard error, which corresponds to the mean results in Figure 4, is given.  ", "publication_ref": [], "figure_ref": ["fig_3", "fig_4", "fig_2"], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Temporal differences-based policy iteration and applications in neuro-dynamic programming", "journal": "", "year": "1995", "authors": "Tridgell Baxter;  Weaver; J Baxter; A Tridgell; L Weaver; D P Bertsekas; S Ioffe; D P Bertsekas; J N Tsitsiklis; D P Bertsekas; C B Browne; E Powley; D Whitehouse; S M Lucas; P I Cowling; P Rohlfshagen; S Tavener; D Perez; S Samothrakis; S Colton; Y Efroni; G Dalal; B Scherrer; S Mannor"}, {"ref_id": "b1", "title": "Multiple-step greedy policies in online and approximate reinforcement learning", "journal": "", "year": "2018", "authors": "[ Efroni"}, {"ref_id": "b2", "title": "Feedback-based tree search for reinforcement learning", "journal": "PMLR", "year": "2018", "authors": "Ekwedike Jiang;  Liu; D Jiang; E Ekwedike; H Liu; C Jin; Z Allen-Zhu; S Bubeck; M I Jordan"}, {"ref_id": "b3", "title": "From bandits to monte-carlo tree search: The optimistic principle applied to optimization and planning", "journal": "", "year": "1928", "authors": "M Lai; M Lanctot; M H Winands; T Pepels; N R Sturtevant; B Lesner; B Scherrer; V Mnih; A P Badia; M Mirza; A Graves; T Lillicrap; T Harley; D Silver; K Kavukcuoglu; R Munos; R R Negenborn; B De Schutter; M A Wiering; H Hellendoorn; M L Puterman; M C Shin"}, {"ref_id": "b4", "title": "Markov decision processes: discrete stochastic dynamic programming", "journal": "John Wiley & Sons", "year": "1994", "authors": "M L Puterman"}, {"ref_id": "b5", "title": "Mastering chess and shogi by self-play with a general reinforcement learning algorithm", "journal": "Journal of Machine Learning Research", "year": "2013", "authors": "B Scherrer; D Silver; T Hubert; J Schrittwieser; I Antonoglou; M Lai; A Guez; M Lanctot; L Sifre; D Kumaran; T Graepel"}, {"ref_id": "b6", "title": "Learning from the hindsight planepisodic mpc improvement", "journal": "IEEE", "year": "1998", "authors": "Barto Sutton; R S Sutton; A G Barto"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "exhibits the results. In its top row, the heatmaps give the convergence", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: (Top) Noiseless NC-hm-PI and hm-PI convergence time as function of h and m. (Bottom) Noiseless NC-hm-PI and hm-PI convergence time as function of a wide range of m, for several values of h. In both figures, the standard error is less than %2 of the mean.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure4: Distance from optimum (lower is better) for NChm-PI and hm-PI in the presence of evaluation noise. The heatmap values are ||v * \u2212 v \u03c0 f || \u221e , where \u03c0 f is the algorithms' output policy after 4 \u2022 10 6 queries to the simulator. The standard error of the results is given in Appendix G.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure5: hm-PI and NC-hm-PI performance for several h and m values. We measure ||v * \u2212 v k || \u221e versus total queries to simulator in each run. In this experiment we used \u2200s \u2208 S, k, k (s) \u223c U (\u22120.3, 0.3), \u03b4 k (s) = 0, as described in Section 8.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 6 :6Figure 6: Standard error versus h and m for the corresponding mean results given in Figure 4.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "which holds due to the second claim in Lemma 6 combined with Lemma 1. Since the LHS is positive, we can apply the max norm on the inequality and use (14):", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "Let v \u03c0 \u2208 R |S| be the value of a policy \u03c0, defined in state s as v \u03c0 (s) \u2261 E \u03c0 |s [ \u221e t=0 \u03b3 t r(s t , \u03c0(s t ))]", "formula_coordinates": [2.0, 54.0, 261.04, 238.5, 23.9]}, {"formula_id": "formula_1", "formula_text": "and v t \u2261 v(s t ). It is known that v \u03c0 = \u221e t=0 \u03b3 t (P \u03c0 ) t r \u03c0 = (I \u2212 \u03b3P \u03c0 ) \u22121 r \u03c0 , with the component-wise values [P \u03c0 ] s,s P (s | s, \u03c0(s)) and [r \u03c0 ] s r(s, \u03c0(s)).", "formula_coordinates": [2.0, 54.0, 321.73, 238.5, 46.71]}, {"formula_id": "formula_2", "formula_text": "\u2200v, \u03c0, T \u03c0 v = r \u03c0 + \u03b3P \u03c0 v, (1) \u2200v, T v = max \u03c0 T \u03c0 v, (2", "formula_coordinates": [2.0, 108.39, 408.43, 184.12, 30.15]}, {"formula_id": "formula_3", "formula_text": ")", "formula_coordinates": [2.0, 288.63, 424.77, 3.87, 8.64]}, {"formula_id": "formula_4", "formula_text": "\u2200v, G(v) = {\u03c0 : T \u03c0 v = T v},(3)", "formula_coordinates": [2.0, 117.2, 441.01, 175.3, 11.03]}, {"formula_id": "formula_5", "formula_text": "T \u03c0 \u03bb v def = (1 \u2212 \u03bb) \u221e j=0 \u03bb j (T \u03c0 ) j+1 v (4) = v + (I \u2212 \u03b3\u03bbP \u03c0 ) \u22121 (T \u03c0 v \u2212 v). r t=0 \u03b3r t=1 \u03b3 2 v t=2 s s r s l", "formula_coordinates": [2.0, 95.94, 58.68, 393.7, 641.97]}, {"formula_id": "formula_6", "formula_text": "(T \u03c0 ) m v = E \u03c0 |\u2022 m\u22121 t=0 \u03b3 t r(s t , \u03c0 t (s t )) + \u03b3 m v(s h ) , T \u03c0 \u03bb v = v + E \u03c0 |\u2022 \u221e t=0 (\u03b3\u03bb) t (r t + \u03b3v t+1 \u2212 v t ) .", "formula_coordinates": [2.0, 337.37, 275.2, 202.77, 64.25]}, {"formula_id": "formula_7", "formula_text": "arg max \u03c00 max \u03c01,..,\u03c0 h\u22121 E \u03c00...\u03c0 h\u22121 |\u2022 h\u22121 t=0 \u03b3 t r(s t , \u03c0 t (s t )) + \u03b3 h v(s h ) = arg max \u03c00 E \u03c00 |\u2022 r(s 0 , \u03c0 0 (s 0 )) + \u03b3 T h\u22121 v (s 1 ) ,(5)", "formula_coordinates": [2.0, 321.16, 415.83, 243.73, 50.12]}, {"formula_id": "formula_8", "formula_text": "\u03c00...\u03c0 h\u22121 |\u2022", "formula_coordinates": [2.0, 411.5, 472.9, 33.21, 14.83]}, {"formula_id": "formula_9", "formula_text": "\u2200v, G h (v) = {\u03c0 : T \u03c0 T h\u22121 v = T h v}.", "formula_coordinates": [2.0, 362.14, 550.45, 153.22, 11.72]}, {"formula_id": "formula_10", "formula_text": "Algorithm 1 h-PI Initialize: h \u2208 N \\ {0}, v 0 = v \u03c00 \u2208 R |S| while v k changes do \u03c0 k \u2190 \u03c0 \u2208 G h (v) v k+1 \u2190 v \u03c0 k k \u2190 k + 1 end while Return \u03c0, v", "formula_coordinates": [3.0, 53.64, 174.58, 176.47, 89.97]}, {"formula_id": "formula_11", "formula_text": "(v, \u03c0) is h-greedy consistent if T \u03c0 T h\u22121 v \u2265 T h\u22121 v.", "formula_coordinates": [3.0, 54.0, 518.32, 238.5, 19.7]}, {"formula_id": "formula_12", "formula_text": "\u2206 = max s T h\u22121v \u2212 T \u03c0 T h\u22121v (s) \u03b3 h\u22121 (1 \u2212 \u03b3) > 0,", "formula_coordinates": [3.0, 352.26, 117.25, 172.98, 24.52]}, {"formula_id": "formula_13", "formula_text": "T \u03c0 T h\u22121 v \u2264 \u2022 \u2022 \u2022 \u2264 (T \u03c0 ) l T h\u22121 v \u2264 \u2022 \u2022 \u2022 \u2264 v \u03c0 .", "formula_coordinates": [3.0, 348.11, 363.19, 181.28, 10.81]}, {"formula_id": "formula_14", "formula_text": "Proposition 2. Let v and \u03c0 h \u2208 G h (v) be s.t. (v, \u03c0 h ) is h- greedy consistent. Then, for any m \u2265 1 and \u03bb \u2208 [0, 1], ||v * \u2212 (T \u03c0 h ) m T h\u22121 v|| \u221e \u2264 \u03b3 h ||v * \u2212 v|| \u221e and ||v * \u2212 T \u03c0 h \u03bb T h\u22121 v|| \u221e \u2264 \u03b3 h ||v * \u2212 v|| \u221e .", "formula_coordinates": [3.0, 319.5, 483.93, 240.15, 54.14]}, {"formula_id": "formula_15", "formula_text": "\u03c0 k \u2190 arg max \u03c0 T \u03c0 T h\u22121 v k ,(6)", "formula_coordinates": [3.0, 361.3, 671.21, 196.7, 16.21]}, {"formula_id": "formula_16", "formula_text": "v k+1 \u2190 (T \u03c0 k ) m v k or v k+1 \u2190 T \u03c0 k \u03bb v k .(7)", "formula_coordinates": [3.0, 361.3, 689.46, 196.7, 13.61]}, {"formula_id": "formula_17", "formula_text": "Theorem 3. Let h > 1, m \u2265 1, and \u03bb \u2208 [0, 1]. Let v be a value function and \u03c0 h \u2208 G h (v) s.t. (v, \u03c0 h ) is h-greedy consistent (see Definition 1). Then, ||v * \u2212 (T \u03c0 h ) m v|| \u221e \u2264 (\u03b3 m + \u03b3 h )||v * \u2212 v|| \u221e ,(8)", "formula_coordinates": [4.0, 53.67, 141.45, 238.83, 48.4]}, {"formula_id": "formula_18", "formula_text": "||v * \u2212 T \u03c0 h \u03bb v|| \u221e \u2264 \u03b3(1 \u2212 \u03bb) 1 \u2212 \u03bb\u03b3 + \u03b3 h ||v * \u2212 v|| \u221e . (9)", "formula_coordinates": [4.0, 65.19, 195.19, 227.31, 22.31]}, {"formula_id": "formula_19", "formula_text": "\u03c0 h \u2208 G h (v) s.t. (v, \u03c0 h ) is h-greedy consistent,", "formula_coordinates": [4.0, 109.14, 234.92, 185.09, 9.65]}, {"formula_id": "formula_20", "formula_text": ". Let v be v(s 0 ) = v(s 2 ) = v(s 3 ) = 0, v(s 1 ) = \u2212 1 1\u2212\u03b3 . Also, let \u03c0 h \u2208 G h (v). For this choice, observe that T h\u22121 v \u2264 T \u03c0 h T h\u22121 v, i.e., (v, \u03c0 h ) is h-greedy consistent.", "formula_coordinates": [4.0, 54.0, 310.52, 239.67, 45.12]}, {"formula_id": "formula_21", "formula_text": "v * (s 1 ) = v * (s 2 ) = 0. Now, see that for any h > 1 T h\u22121 v (s 1 ) = T h\u22121 v (s 2 ) = 0, T h\u22121 v (s 3 ) = 1\u2212\u03b3 h\u22121 1\u2212\u03b3 .", "formula_coordinates": [4.0, 58.57, 380.29, 233.93, 40.52]}, {"formula_id": "formula_22", "formula_text": "r(s 1 , 'stay ) + \u03b3(T h\u22121 v)(s 1 ) =r(s 1 , 'right ) + \u03b3(T h\u22121 v)(s 2 ) = 0.", "formula_coordinates": [4.0, 97.5, 480.54, 151.49, 27.95]}, {"formula_id": "formula_23", "formula_text": "((T \u03c0 h ) m v) (s 0 ) = m\u22121 i=0 \u03b3 t r(s i , \u03c0 h (s i )) + \u03b3 m v(s i=m ) = 1 \u2212 \u03b3 m \u2212 \u03b3 h 1 \u2212 \u03b3 + m\u22121 i=1 \u03b3 i \u2022 0 + \u03b3 m \u2212 1 1 \u2212 \u03b3 = 1 \u2212 \u03b3 m \u2212 \u03b3 h 1 \u2212 \u03b3 We thus have that ||v * \u2212 (T \u03c0 h ) m v|| \u221e = |v * (s 1,0 ) \u2212 (T \u03c0 h ) m v(s 1,0 )| = 1 1 \u2212 \u03b3 + \u03b3 m + \u03b3 h \u2212 1 1 \u2212 \u03b3 = (\u03b3 m + \u03b3 h ) 1 1 \u2212 \u03b3 (10", "formula_coordinates": [4.0, 53.53, 549.9, 234.82, 152.1]}, {"formula_id": "formula_24", "formula_text": ")", "formula_coordinates": [4.0, 288.35, 686.74, 4.15, 8.64]}, {"formula_id": "formula_25", "formula_text": "s 0 v(s 0 ) = 0 s 1 v(s 1 ) = \u2212 1 1\u2212\u03b3 s 2 v(s 2 ) = 0 s 3 v(s 3 ) = 0 1\u2212\u03b3 h 1\u2212\u03b3 0 0 1 1 0 Figure 2:", "formula_coordinates": [4.0, 319.5, 56.94, 203.2, 158.95]}, {"formula_id": "formula_26", "formula_text": "||v * \u2212 (T \u03c0 h ) m v|| \u221e = (\u03b3 m + \u03b3 h )||v * \u2212 v|| \u221e ,", "formula_coordinates": [4.0, 345.9, 282.57, 185.71, 11.72]}, {"formula_id": "formula_27", "formula_text": "Initialize: h, m \u2208 N \\ {0}, v \u2208 R |S| while stopping criterion is false do \u03c0 k+1 \u2190 \u03c0 \u2208 G \u03b4 k+1 h (v k ) v k+1 \u2190 (T \u03c0 k+1 ) m T h\u22121 v k + k k \u2190 k + 1 end while Return \u03c0, v Algorithm 3 h\u03bb-PI Initialize: h \u2208 N \\ {0}, \u03bb \u2208 [0, 1], v \u2208 R |S| while stopping criterion is false do \u03c0 k+1 \u2190 \u03c0 \u2208 G \u03b4 k+1 h (v k ) v k+1 \u2190 T \u03c0 k+1 \u03bb T h\u22121 v k + k k \u2190 k + 1 end while Return \u03c0, v Definition 2. For\u03b4 \u2208 R |S| + , let G\u03b4 h (v) be the approximate h- greedy set of policies w.r.t. v with error\u03b4, s.t. for \u03c0 \u2208 G\u03b4 h (v), T \u03c0 T h\u22121 v \u2265 T h v \u2212\u03b4.", "formula_coordinates": [5.0, 54.0, 68.79, 450.72, 158.3]}, {"formula_id": "formula_28", "formula_text": "4. Let h, m \u2208 N \\ {0}, \u03bb \u2208 [0, 1]. For noise se- quences { k } and {\u03b4 k }, k \u221e \u2264 and \u03b4 k \u221e \u2264 \u03b4. Let \u2206 0 = max{0, max s T h\u22121 v 0 \u2212 T \u03c01 T h\u22121 v 0 (s) \u03b3 h\u22121 (1 \u2212 \u03b3) }.", "formula_coordinates": [5.0, 54.0, 477.41, 240.15, 51.79]}, {"formula_id": "formula_29", "formula_text": "v * \u2212 v \u03c0 k+1 \u221e \u2264 \u03b3 kh ||v * \u2212 (v 0 \u2212 \u2206 0 )|| \u221e + (2\u03b3 h + \u03b4)(1 \u2212 \u03b3 kh ) (1 \u2212 \u03b3)(1 \u2212 \u03b3 h ) and hence lim sup k\u2192\u221e v * \u2212 v \u03c0 k \u221e \u2264 2\u03b3 h +\u03b4 (1\u2212\u03b3)(1\u2212\u03b3 h ) . Proof.", "formula_coordinates": [5.0, 54.0, 551.38, 214.63, 80.34]}, {"formula_id": "formula_30", "formula_text": "k = k \u2212 C k e, where C k = max \u03b4 k+1 +\u03b3 h\u22121 max k \u2212\u03b3 h min k \u03b3 h\u22121 (1\u2212\u03b3)", "formula_coordinates": [5.0, 58.04, 643.97, 231.49, 16.3]}, {"formula_id": "formula_31", "formula_text": "d k def = v * \u2212 (v k \u2212 k ) for k \u2265 1, and d 0 def = v * \u2212 v 0 . We get d k+1 = v * \u2212 (T \u03c0 k+1 ) m T h\u22121 v k (11) \u2264 v * \u2212 T \u03c0 k+1 T h\u22121 v k (12) \u2264 v * \u2212 T h v k + max \u03b4 k+1 \u2264 (T \u03c0 * ) h v * \u2212 T h (v k \u2212 k ) \u2212 \u03b3 h min k + max \u03b4 k+1 \u2264 (T \u03c0 * ) h v * \u2212 (T \u03c0 * ) h (v k \u2212 k ) \u2212 \u03b3 h min k + max \u03b4 k+1 = \u03b3 h (P \u03c0 * ) h (v * \u2212 (v k \u2212 k )) \u2212 \u03b3 h min k + max \u03b4 k+1 = \u03b3 h (P \u03c0 * ) h d k \u2212 \u03b3 h min k + max \u03b4 k+1 .", "formula_coordinates": [5.0, 319.5, 277.82, 256.1, 128.21]}, {"formula_id": "formula_32", "formula_text": "d k+1 \u2264 \u03b3 h (P \u03c0 * ) h d k + 2\u03b3 h + \u03b4 1 \u2212 \u03b3 . (13", "formula_coordinates": [5.0, 369.43, 463.13, 184.42, 23.89]}, {"formula_id": "formula_33", "formula_text": ")", "formula_coordinates": [5.0, 553.85, 471.76, 4.15, 8.64]}, {"formula_id": "formula_34", "formula_text": "d k \u2264 \u03b3 kh (P \u03c0 * ) kh d 0 + (2\u03b3 h + \u03b4)(1 \u2212 \u03b3 kh ) (1 \u2212 \u03b3)(1 \u2212 \u03b3 h ) \u2264 \u03b3 kh ||d 0 || \u221e + (2\u03b3 h + \u03b4)(1 \u2212 \u03b3 kh ) (1 \u2212 \u03b3)(1 \u2212 \u03b3 h ) . (14", "formula_coordinates": [5.0, 347.7, 506.28, 206.15, 52.39]}, {"formula_id": "formula_35", "formula_text": ")", "formula_coordinates": [5.0, 553.85, 543.42, 4.15, 8.64]}, {"formula_id": "formula_36", "formula_text": "v * \u2212 v \u03c0 k+1 \u2264 v * \u2212 (v k \u2212 k ) = d k ,", "formula_coordinates": [5.0, 319.5, 562.82, 238.5, 23.51]}, {"formula_id": "formula_37", "formula_text": "||v * \u2212 v \u03c0 k+1 || \u221e \u2264 \u03b3 kh ||d 0 || \u221e + (2\u03b3 h + \u03b4)(1 \u2212 \u03b3 kh ) (1 \u2212 \u03b3)(1 \u2212 \u03b3 h ) . Since d 0 = v * \u2212 v 0 = v * \u2212 (v 0 \u2212 \u2206 0 )", "formula_coordinates": [5.0, 319.5, 611.37, 231.52, 40.81]}, {"formula_id": "formula_38", "formula_text": "lim k\u2192\u221e ||v * \u2212 v \u03c0 k+1 || \u221e \u2264 2\u03b3 h + \u03b4 (1 \u2212 \u03b3)(1 \u2212 \u03b3 h ) .", "formula_coordinates": [5.0, 353.02, 677.56, 171.45, 23.89]}, {"formula_id": "formula_39", "formula_text": "d k+1 \u2264 v * \u2212 (1 \u2212 \u03bb) i \u03bb i (T \u03c0 k+1 ) i+1 T h\u22121 v k \u2264 v * \u2212 (1 \u2212 \u03bb) i \u03bb i T \u03c0 k+1 T h\u22121 v k = v * \u2212 T \u03c0 k+1 T h\u22121 v k ,", "formula_coordinates": [6.0, 77.43, 96.24, 190.97, 65.58]}, {"formula_id": "formula_40", "formula_text": "-up T \u03c0 k+1 T h\u22121 v k instead of T h\u22121 v k . Namely, in this variant, the evaluation stage for hm-PI (Algorithm 2) is v k+1 \u2190 (T \u03c0 k+1 ) m\u22121 (T \u03c0 k+1 T h\u22121 v k ) + k , and for h\u03bb-PI (Algorithm 3) it is v k+1 \u2190T \u03c0 k+1 \u03bb (T \u03c0 k+1 T h\u22121 v k ) + k . The latter is (see Appendix F) T \u03c0 \u03bb v def = (1 \u2212 \u03bb) \u221e j=0 \u03bb j (T \u03c0 ) j v = v + \u03bb(I \u2212 \u03b3\u03bbP \u03c0 ) \u22121 (T \u03c0 v \u2212 v)", "formula_coordinates": [6.0, 53.44, 318.97, 452.78, 118.19]}, {"formula_id": "formula_41", "formula_text": "(T \u03c0 ) m\u22121 T \u03c0 = (T \u03c0 ) m andT \u03c0 \u03bb T \u03c0 = T \u03c0 \u03bb .", "formula_coordinates": [6.0, 87.88, 486.76, 170.75, 12.69]}, {"formula_id": "formula_42", "formula_text": "h = 1 h = 2 h = 3 h = 4 h = 5", "formula_coordinates": [7.0, 134.92, 178.51, 15.65, 44.11]}, {"formula_id": "formula_43", "formula_text": "T h\u22121 v \u2264 T \u03c0 T h\u22121 v.", "formula_coordinates": [9.0, 263.92, 89.6, 84.17, 10.81]}, {"formula_id": "formula_44", "formula_text": "(T \u03c0 ) l\u22121 T h\u22121 v \u2264 (T \u03c0 ) l T h\u22121 v.", "formula_coordinates": [9.0, 241.7, 129.66, 128.59, 10.81]}, {"formula_id": "formula_45", "formula_text": "[Appendix B]). Lemma 5. Let {v i , \u03bb i } \u221e", "formula_coordinates": [9.0, 54.0, 212.45, 101.0, 25.78]}, {"formula_id": "formula_46", "formula_text": "T \u03c0 ( \u221e i=0 \u03bb i v i ) = \u221e i=0 \u03bb i T \u03c0 v i , (T \u03c0 ) n ( \u221e i=0 \u03bb i v i ) = \u221e i=0 \u03bb i (T \u03c0 ) n v i .", "formula_coordinates": [9.0, 234.99, 256.06, 142.02, 63.51]}, {"formula_id": "formula_47", "formula_text": "T \u03c0 ( \u221e i=0 \u03bb i v i ) = r \u03c0 + \u03b3P \u03c0 ( \u221e i=0 \u03bb i v i ) = r \u03c0 + \u221e i=0 \u03bb i \u03b3P \u03c0 v i = \u221e i=0 \u03bb i (r \u03c0 + \u03b3P \u03c0 v i ) = \u221e i=0 \u03bb i T \u03c0 v i .", "formula_coordinates": [9.0, 187.42, 344.52, 236.66, 63.51]}, {"formula_id": "formula_48", "formula_text": "v * \u2212 (T \u03c0 h ) m T h\u22121 v \u2264 v * \u2212 T \u03c0 h T h\u22121 v (15) = v * \u2212 T h v = (T \u03c0 * ) h v * \u2212 T h v \u2264 (T \u03c0 * ) h v * \u2212 (T \u03c0 * ) h v \u2264 \u03b3 h (P \u03c0 * ) h (v * \u2212 v) \u2264 \u03b3 h ||v * \u2212 v|| \u221e .", "formula_coordinates": [9.0, 183.7, 471.87, 374.3, 76.64]}, {"formula_id": "formula_49", "formula_text": "v * \u2212 T \u03c0 h \u03bb T h\u22121 v = v * \u2212 (1 \u2212 \u03bb) i \u03bb i (T \u03c0 h ) i+1 T h\u22121 v (16) \u2264 v * \u2212 (1 \u2212 \u03bb) i \u03bb i T \u03c0 h T h\u22121 v = v * \u2212 T \u03c0 h T h\u22121 v,", "formula_coordinates": [9.0, 193.32, 588.59, 364.68, 64.07]}, {"formula_id": "formula_50", "formula_text": "(T \u03c0 h ) m T h\u22121 v \u2264 v \u03c0 h \u2264 v * ,and", "formula_coordinates": [9.0, 248.94, 689.84, 114.12, 10.81]}, {"formula_id": "formula_51", "formula_text": "T \u03c0 h \u03bb T h\u22121 v =(1 \u2212 \u03bb) i \u03bb i (T \u03c0 h ) i+1 (T \u03c0 h ) h\u22121 v \u2264(1 \u2212 \u03bb) i \u03bb i v \u03c0 = v \u03c0 \u2264 v * ,", "formula_coordinates": [10.0, 207.89, 72.46, 195.87, 49.56]}, {"formula_id": "formula_52", "formula_text": "v * \u2212 (T \u03c0 h ) m v =v * \u2212 v \u03c0 h + v \u03c0 h \u2212 (T \u03c0 h ) m v =(T \u03c0 * ) h v * \u2212 v \u03c0 h + (T \u03c0 h ) m v \u03c0 h \u2212 (T \u03c0 h ) m v =(T \u03c0 * ) h v * \u2212 v \u03c0 h + \u03b3 m (P \u03c0 h ) m (v \u03c0 h \u2212 v) \u2264(T \u03c0 * ) h v * \u2212 v \u03c0 h + \u03b3 m (P \u03c0 h ) m (v * \u2212 v) \u2264(T \u03c0 * ) h v * \u2212 T h v + \u03b3 m (P \u03c0 h ) m (v * \u2212 v) \u2264(T \u03c0 * ) h v * \u2212 (T \u03c0 * ) h v + \u03b3 m (P \u03c0 h ) m (v * \u2212 v) =\u03b3 h (P \u03c0 * ) h (v * \u2212 v) + \u03b3 m (P \u03c0 h ) m (v * \u2212 v) \u2264(\u03b3 h + \u03b3 m ) v * \u2212 v \u221e . (17", "formula_coordinates": [10.0, 180.59, 214.85, 373.26, 125.33]}, {"formula_id": "formula_53", "formula_text": ")", "formula_coordinates": [10.0, 553.85, 330.84, 4.15, 8.64]}, {"formula_id": "formula_54", "formula_text": "(T \u03c0 h ) m v \u2212 v * = (T \u03c0 h ) m v \u2212 T m v * = (T \u03c0 h ) m v \u2212 (T \u03c0 h ) m v * = \u03b3 m (P \u03c0 h ) m (v \u2212 v * ) \u2264 \u03b3 m v \u2212 v * \u221e \u2264 (\u03b3 h + \u03b3 m ) v * \u2212 v \u221e . (18", "formula_coordinates": [10.0, 187.41, 396.83, 366.44, 55.85]}, {"formula_id": "formula_55", "formula_text": ")", "formula_coordinates": [10.0, 553.85, 443.34, 4.15, 8.64]}, {"formula_id": "formula_56", "formula_text": "||v * \u2212 (T \u03c0 h ) m v|| \u221e \u2264(\u03b3 h + \u03b3 m )||v * \u2212 v|| \u221e .(19)", "formula_coordinates": [10.0, 214.53, 499.52, 343.47, 11.72]}, {"formula_id": "formula_57", "formula_text": "||v * \u2212 T \u03c0 h \u03bb v|| \u221e =||v * \u2212 (1 \u2212 \u03bb) i \u03bb i (T \u03c0 h ) i+1 v|| \u221e =||(1 \u2212 \u03bb) i \u03bb i (v * \u2212 (T \u03c0 h ) i+1 v)|| \u221e \u2264(1 \u2212 \u03bb) i \u03bb i ||v * \u2212 (T \u03c0 h ) i+1 v|| \u221e \u2264 (1 \u2212 \u03bb) i \u03bb i (\u03b3 i+1 + \u03b3 h ) ||v * \u2212 v|| \u221e = \u03b3(1 \u2212 \u03bb) 1 \u2212 \u03bb\u03b3 + \u03b3 h ||v * \u2212 v|| \u221e .", "formula_coordinates": [10.0, 182.33, 537.3, 246.85, 137.76]}, {"formula_id": "formula_58", "formula_text": "(T \u03c0 h \u03bb v)(s 0 ) = 1 \u2212 \u03b3 h 1 \u2212 \u03b3 + \u221e i=0 (\u03b3\u03bb) i (\u03b3(1 \u2212 \u03b3)v(s 1 )) = 1 \u2212 \u03b3 h 1 \u2212 \u03b3 \u2212 \u03b3(1 \u2212 \u03bb) (1 \u2212 \u03b3\u03bb) \u2022 1 1 \u2212 \u03b3 .", "formula_coordinates": [11.0, 203.09, 83.03, 205.82, 57.92]}, {"formula_id": "formula_59", "formula_text": "|(T \u03c0 h \u03bb v)(s 0 ) \u2212 v * (s 0 )| = \u03b3 h + \u03b3(1 \u2212 \u03bb) (1 \u2212 \u03b3\u03bb) 1 1 \u2212 \u03b3 . Since ||(T \u03c0 h \u03bb v) \u2212 v * || \u221e = |(T \u03c0 h \u03bb v)(s 0 ) \u2212 v * (s 0 )|, ||v * \u2212 T \u03c0 h \u03bb v|| \u221e = \u03b3 h + \u03b3(1 \u2212 \u03bb) (1 \u2212 \u03b3\u03bb) 1 1 \u2212 \u03b3 = \u03b3 h + \u03b3(1 \u2212 \u03bb) (1 \u2212 \u03b3\u03bb) ||v * \u2212 v|| \u221e E h-Greedy Consistency in Each Iteration", "formula_coordinates": [11.0, 54.0, 162.77, 363.19, 119.28]}, {"formula_id": "formula_60", "formula_text": "Lemma 6. Let k = k \u2212 C k e, where C k = max \u03b4 k+1 +\u03b3 h\u22121 max k \u2212\u03b3 h min k \u03b3 h\u22121 (1\u2212\u03b3)", "formula_coordinates": [11.0, 54.0, 310.9, 303.82, 16.3]}, {"formula_id": "formula_61", "formula_text": "v k . Let \u03c0 k+1 \u2208 G \u03b4 k+1 h (v k ). Then, in every iteration k (v k , \u03c0 k+1 ) is h-greedy consistent; i.e., T h\u22121 v k \u2264 T \u03c0 k+1 T h\u22121 v k , and v k \u2212 k \u2264 T \u03c0 k+1 T h\u22121 v k .", "formula_coordinates": [11.0, 53.44, 328.66, 504.56, 76.7]}, {"formula_id": "formula_62", "formula_text": "(\u03c0 k , v k\u22121 ) is h-greedy consistent, T h\u22121 v k\u22121 \u2264 T \u03c0 k T h\u22121 v k\u22121", "formula_coordinates": [11.0, 54.0, 411.42, 505.24, 24.08]}, {"formula_id": "formula_63", "formula_text": "T h\u22121 v k \u2212 T \u03c0 k+1 T h\u22121 v k \u2264 T h\u22121 v k \u2212 T h v k + max \u03b4 k+1 = T h\u22121 (v k \u2212 k ) \u2212 T h (v k \u2212 k ) + \u03b3 h\u22121 max k \u2212 \u03b3 h min k + max \u03b4 k+1 = T h\u22121 (v k \u2212 k ) \u2212 T h (v k \u2212 k ),(20)", "formula_coordinates": [11.0, 102.32, 464.05, 455.68, 45.15]}, {"formula_id": "formula_64", "formula_text": "T h\u22121 (v k \u2212 k ) \u2212 T h (v k \u2212 k ) = T h\u22121 (T \u03c0 k ) m T h\u22121 v k\u22121 \u2212 T h (T \u03c0 k ) m T h\u22121 v k\u22121 \u2264 T h\u22121 T \u03c0 k (T \u03c0 k ) m T h\u22121 v k\u22121 \u2212 T h (T \u03c0 k ) m T h\u22121 v k\u22121 \u2264 T h\u22121 T (T \u03c0 k ) m T h\u22121 v k\u22121 \u2212 T h (T \u03c0 k ) m T h\u22121 v k\u22121 = 0.", "formula_coordinates": [11.0, 125.0, 543.75, 362.0, 45.16]}, {"formula_id": "formula_65", "formula_text": "v k \u2212 k \u2212 T \u03c0 k+1 T h\u22121 v k \u2264 0 for hm-PI. v k \u2212 k \u2212 T \u03c0 k+1 T h\u22121 v k \u2264 v k \u2212 T h v k + max \u03b4 k+1 \u2264 v k \u2212 k \u2212 T h (v k \u2212 k ) \u2212 \u03b3 h min k + max \u03b4 k+1 \u2264 v k \u2212 k \u2212 T h (v k \u2212 k )(21)", "formula_coordinates": [11.0, 141.19, 639.38, 416.81, 63.14]}, {"formula_id": "formula_66", "formula_text": "\u2212 \u03b3 h min k + max \u03b4 k+1 = \u2212\u03b3 h min k + max \u03b4 k+1 \u2212 (1 \u2212 \u03b3 h )C k = max k (\u2212 1 \u2212 \u03b3 h 1 \u2212 \u03b3 ) + \u03b3 h min k ( 1 \u2212 \u03b3 h \u03b3 h\u22121 (1 \u2212 \u03b3) \u2212 1) + max \u03b4 k+1 (1 \u2212 1 \u2212 \u03b3 h \u03b3 h\u22121 (1 \u2212 \u03b3) ) \u2264 0.", "formula_coordinates": [12.0, 122.68, 70.66, 368.86, 40.11]}, {"formula_id": "formula_67", "formula_text": "v k \u2212 k \u2212 T \u03c0 k+1 T h\u22121 v k \u2264 v k \u2212 k \u2212 T h ((v k \u2212 k )) \u2264 (T \u03c0 k ) m T h\u22121 v k\u22121 \u2212 T h (T \u03c0 k ) m T h\u22121 v k\u22121 \u2264 (T \u03c0 k ) h (T \u03c0 k ) m T h\u22121 v k\u22121 \u2212 T h (T \u03c0 k ) m T h\u22121 v k\u22121 \u2264 T h (T \u03c0 k ) m T h\u22121 v k\u22121 \u2212 T h (T \u03c0 k ) m T h\u22121 v k\u22121 = 0.", "formula_coordinates": [12.0, 145.58, 153.51, 320.84, 61.39]}, {"formula_id": "formula_68", "formula_text": "v k \u2212 k \u2212 T \u03c0 k+1 T h\u22121 v k \u2264 0.", "formula_coordinates": [12.0, 245.21, 256.76, 121.59, 12.69]}, {"formula_id": "formula_69", "formula_text": "Lemma 7. Let \u03bb \u2208 [0, 1], l \u2208 N and (v, \u03c0) be h-greedy consistent. Then, T \u03c0 \u03bb T h\u22121 v \u2264 (T \u03c0 ) l T \u03c0 \u03bb T h\u22121 v. Proof. We have that T \u03c0 \u03bb T h\u22121 v = (1 \u2212 \u03bb) i \u03bb i (T \u03c0 ) i+1 T h\u22121 v \u2264 (1 \u2212 \u03bb) i \u03bb i (T \u03c0 ) i+1+l T h\u22121 v = (1 \u2212 \u03bb) i \u03bb i (T \u03c0 ) l (T \u03c0 ) i+1 T h\u22121 v = (T \u03c0 ) l (1 \u2212 \u03bb) i \u03bb i (T \u03c0 ) i+1 T h\u22121 v = (T \u03c0 ) l T \u03c0 \u03bb T h\u22121 v.", "formula_coordinates": [12.0, 54.0, 317.0, 398.71, 159.31]}, {"formula_id": "formula_70", "formula_text": "T h\u22121 (v k \u2212 k ) \u2212 T h (v k \u2212 k ) = T h\u22121 T \u03c0 k \u03bb v k\u22121 \u2212 T h T \u03c0 k \u03bb v k\u22121 \u2264 T h\u22121 T \u03c0 k T \u03c0 k \u03bb v k\u22121 \u2212 T h T \u03c0 k \u03bb v k\u22121 \u2264 T h\u22121 T T \u03c0 k \u03bb v k\u22121 \u2212 T h T \u03c0 k \u03bb v k\u22121 = 0.", "formula_coordinates": [12.0, 162.88, 535.46, 286.24, 46.08]}, {"formula_id": "formula_71", "formula_text": "v k \u2212 k \u2212 T h (v k \u2212 k ) = T \u03c0 k \u03bb v k\u22121 \u2212 T h T \u03c0 k \u03bb v k\u22121 \u2264 (T \u03c0 k ) h T \u03c0 k \u03bb v k\u22121 \u2212 T h T \u03c0 k \u03bb v k\u22121 \u2264 T h T \u03c0 k \u03bb v k\u22121 \u2212 T h T \u03c0 k \u03bb v k\u22121 = 0.", "formula_coordinates": [12.0, 186.74, 631.95, 238.52, 46.08]}, {"formula_id": "formula_72", "formula_text": "T \u03c0 \u03bb def = (1 \u2212 \u03bb) \u221e j=0 \u03bb j (T \u03c0 ) j v = (1 \u2212 \u03bb)v + (1 \u2212 \u03bb) \u221e j=1 \u03bb j (T \u03c0 ) j v = (1 \u2212 \u03bb)v + \u03bb(1 \u2212 \u03bb) \u221e j=0 \u03bb j (T \u03c0 ) j+1 v = (1 \u2212 \u03bb)v + \u03bb(1 \u2212 \u03bb) \u221e j=0 \u03bb j (T \u03c0 ) j+1 v \u03bbT \u03c0 \u03bb v = v + \u03bb(I \u2212 \u03b3\u03bbP \u03c0 ) \u22121 (T \u03c0 v \u2212 v),", "formula_coordinates": [13.0, 145.66, 155.22, 320.69, 152.01]}], "doi": ""}