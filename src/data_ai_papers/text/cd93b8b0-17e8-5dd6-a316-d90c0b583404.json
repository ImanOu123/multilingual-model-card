{"title": "StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing", "authors": "Xuekai Zhu; Jian Guan; Minlie Huang; Juan Liu", "pub_date": "", "abstract": "Non-parallel text style transfer is an important task in natural language generation. However, previous studies concentrate on the token or sentence level, such as sentence sentiment and formality transfer, but neglect long style transfer at the discourse level. Long texts usually involve more complicated author linguistic preferences such as discourse structures than sentences. In this paper, we formulate the task of non-parallel story author-style transfer, which requires transferring an input story into a specified author style while maintaining source semantics. To tackle this problem, we propose a generation model, named StoryTrans, which leverages discourse representations to capture source content information and transfer them to target styles with learnable style embeddings. We use an additional training objective to disentangle stylistic features from the learned discourse representation to prevent the model from degenerating to an auto-encoder. Moreover, to enhance content preservation, we design a mask-and-fill framework to explicitly fuse style-specific keywords of source texts into generation. Furthermore, we constructed new datasets for this task in Chinese and English, respectively. Extensive experiments show that our model outperforms strong baselines in overall performance of style transfer and content preservation. * Equal contribution. \u2020 Corresponding author \u90ed\u7ff0\u662f\u53e4\u65f6\u5019\u4e00\u540d\u624d\u5b50\u3002\u4e00\u4e2a\u590f\u65e5\u7684\u665a\u4e0a\uff0c\u4ed6\u5728\u9662\u4e2d\u4e58\u51c9\u3002\u5ffd \u7136\uff0c\u4e00\u9635\u98ce\u8d77\uff0c\u9001\u6765\u4e00\u80a1\u6c81\u4eba\u5fc3\u813e\u7684\u6e05\u9999\uff0c\u4e00\u4f4d\u5c11\u5973\u9a7e\u7740\u767d\u4e91\u4ece \u5929\u800c\u964d\uff0c\u51fa\u73b0\u5728\u90ed\u7ff0\u773c\u524d \u2026 Guo Han was a talented man in ancient times. One summer evening, he was enjoying the cool in the courtyard. Suddenly, a gust of wind brought a refreshing fragrance, and a young girl descended from the sky on a white cloud", "sections": [{"heading": "Introduction", "text": "Text style transfer aims to endow a text with a different style while keeping its main semantic content unaltered. It has a wide range of applications, such as formality transfer (Jain et al., 2019), sentiment transfer (Shen et al., 2017) and author-style imitation (Tikhonov and Yamshchikov, 2018).\nDue to the lack of parallel corpora, recent works mainly focus on unsupervised transfer by selfreconstruction. Current methods proposed to dis-Table 1: An example that transfers a vernacular story to the martial arts style of JY generated by StyleLM. The orange sentence indicates missing content in source text. The rewritten token is underlined. The red highlights are supplementary short phrases or plots to align with the target style. The English texts below the Chinese are translated versions of the Chinese samples. entangle styles from contents by removing stylistic tokens from inputs explicitly (Huang et al., 2021) or reducing stylistic features from token-level hidden representations of inputs implicitly (Lee et al., 2021). This line of work has impressive performance on single-sentence sentiment and formality transfer. However, it is yet not investigated to transfer author styles of long texts such as stories, manifesting in the author's linguistic choices at the lexical, syntactic, and discourse levels.\nIn this paper, we present the first study on story author-style transfer, which aims to rewrite a story incorporating source content and the target author style. The first challenge of this task lies in imitation of author's linguistic choices at the discourse level, such as narrative techniques (e.g., brief or detailed writing). As exemplified in Table 1, the generation text for the JinYong (JY) 1 style not only rewrites some tokens to the martial arts style (e.g., \"\u767d\u4e91\" /\"white cloud\" to \"\u767d\u5149\u4e00\u95ea\" /\"light flashing\") but also adds additional events in detail and enrich the storyline (e.g., the red highlights). In contrast to the transfer of token-level features like formality, it is more difficult to capture the intersentence relations correlated with author styles and disentangle them from contents. The second challenge is that the author styles tend to be highly associated with specific writing topics. Therefore, it is hard to transfer these style-specific contents to another style. For example, the topic \"talented man\" hardly shows up in the novels of JY, leading to the low content preservation of such contents, as shown in the orange text in Table 1.\nTo alleviate the above issues, we propose a generation framework, named StoryTrans, which learns discourse representations from source texts and then combines these representations with learnable style embeddings to generate texts of target styles. Furthermore, we propose a new training objective to reduce stylistic features from the discourse representations, which aims to pull the representations derived from different texts close in the latent space. To enhance content preservation, we separate the generation process into two stages, which first transfers the source text with the style-specific content keywords masked and then generates the whole text by imposing these keywords explicitly.\nTo support the evaluation of the proposed task, we collect new datasets in Chinese and English based on existing story corpora. 2 We conduct extensive experiments to transfer fairy tales (in Chinese) or everyday stories (in English) to typical author styles, respectively. Automatic evaluation results show that our model achieves a better overall performance in style control and content preservation than strong baselines. The manual evaluation also confirms the efficacy of our model. We summarize the key contributions of this work as follows: I. To the best of our knowledge, we present the first study on story author style transfer. We construct new Chinese and English datasets for this task. II. We propose a new generation model named Sto-ryTrans to tackle the new task, which implements content-style disentanglement and stylization based on discourse representations, then enhances content preservation by explicitly incorporating stylespecific keywords. III. Extensive experiments show that our model outperforms baselines in the overall performance of style transfer accuracy and content preservation.", "publication_ref": ["b12", "b25", "b28", "b11", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Style Transfer", "text": "Recent studies concentrated mainly on token-level style transfer of single sentences, such as formality or sentiment transfer. We categorize these studies into three following paradigms.\nThe first paradigm built a style transfer system without explicit disentanglement of style and content. This line of work used additional style signals or a multi-generator structure to control the style. Dai et al. (2019) added an extra style embedding in input for manipulating the style of texts. Yi et al. (2020) proposed a style instance encoding method for learning more discriminative and expressive style embeddings. The learnable style embedding is a flexible yet effective approach to providing style signals. Such a design helps better preserve source content. Syed et al. (2020) randomly dropped the input words, then reconstructed input for each author separately, which obtained multiple author-specific generators. The multi-generator structure is effective but also resource-consuming. However, this paradigm incurs unsatisfactory style transfer accuracy without explicit disentanglement.\nThe second paradigm disentangled the content and style explicitly in latent space, then combined the target style signal. Zhu et al. ( 2021) diluted sentence-level information in style representations. John et al. (2019) incorporated style prediction and adversarial objectives for disentangling. Lee et al. (2021) removed style information of each token with reverse attention score (Bahdanau et al., 2015) , which is estimated by a pre-trained style classifier. This paradigm utilizes adversarial loss functions or a pre-trained estimator for disentanglement. And experiment results indicate that explicit disentanglement leads to satisfactory style transfer accuracy but poor content preservation.\nThe final paradigm views style as localized features of tokens in a sentence, which locates styledependent words and replaces the target-style ones. Xu et al. (2018) employed an attention mechanism to identify style tokens and filter out such tokens. Wu et al. (2019) utilized a two-stage framework to mask all sentimental tokens and then infill them. Huang et al. (2021) aligned words of input and reference to achieve token-level transfer. To sum up, this paradigm maintains all word-level information, but it is hard to apply to the scenarios where styles are expressed beyond token level, e.g., author style.\nAbsorbing ideas from paradigm 1 and 2, we", "publication_ref": ["b2", "b26", "b13", "b16", "b0", "b32", "b30", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "\u2160. Discourse Representation Transfer", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "\u2161. Content Preservation Enhancing", "text": "Then at the peace of my soul , \u2022\u2022\u2022  ) to contain main semantics of pre-processed input (x m ). Then, the fusion module stylizes the discourse representations with target style embedding (\u015d). For content preservation enhancing (the second stage), our model enhances the content preservation of transferred texts (x m ) with stylespecific content (k). x andx denote the original story and the final output, respectively. apply explicit disentanglement by pulling close discourse representations, which is formulated into disentanglement loss. Furthermore, we design a fusion module to stylize the discourse representation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data preprocessing", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "High-Level Representation", "text": "Prior works captured the hierarchical structure of natural language texts by learning high-level representations. Li et al. (2015) and  proposed to learn hierarchical embedding representations by reconstructing masked version of sentences or paragraphs. Reimers and Gurevych (2019) derived semantical sentence embeddings by fine-tuning BERT (Devlin et al., 2019) on downstream tasks. ; Guan et al. (2021b) inserted special tokens for each sentence and devised several pre-training tasks to learn sentencelevel representations. We are inspired to use a sentence order prediction task to learn high-level discourse representations.", "publication_ref": ["b18", "b24", "b3", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Long Text Generation", "text": "In order to generate coherent long texts, recent studies usually decomposed generation into multiple stages. Fan et al. (2018); Yao et al. (2019) generated a premise, then transformed it into a passage. Tan et al. (2021) first produced domain-specific content keywords and then progressively refines them into complete passages. Borrowing these ideas , we adopted a mask-and-fill framework to enhance content preservation in text style transfer.", "publication_ref": ["b4", "b33", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "Methodology", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Task Definition and Model Overview", "text": "We formulate the story author-style transfer task as follows: assuming that S is the set of all author-styles, given a multi-sentence input x = (x 1 , x 2 , \u2022 \u2022 \u2022 , x T ) of T tokens and its author-style label s \u2208 S, the model should generate a multisentence text with a specified author-style\u015d \u2208 S while keeping the main semantics of x.\nAs illustrated in Figure 1, we split the generation process into two stages. We first identify stylespecific keywords k = (k 1 , k 2 , \u2022 \u2022 \u2022 , k l ) from x, and then mask them with special tokens \u27e8mask\u27e9. We denote the resulting masked version of x as\nx m = (x m 1 , x m 2 , \u2022 \u2022 \u2022 , x m T ).\nIn the first generation stage, we perform discourse representation transfer on x m . In the second stage, we complete the masked tokens in the output of the first stage conditioned on k in a style-unrelated manner.\nDue to the lack of parallel data, typical style transfer models tend to optimize the selfreconstruction loss with the same inputs and outputs (Xiao et al., 2021;Lee et al., 2021). Obviously, training with only the self-reconstruction loss will make the model easily ignore the target style signals and simply repeat the source inputs. Therefore, in the first stage, we devise an additional training objective, to disentangle stylistic features from intermediate discourse representations {r i } n i=1 , where n is the number of sentences. Then, we fused these style-independent discourse representations with the target style\u015d as a discourse- level guidance for the subsequent generation of the transferred text. As for discourse representations learning, we employ a sentence order prediction loss to capture inter-sentence discourse dependencies. And we use a style classifier loss to control the style of generated texts (Lee et al., 2021). In summary, the fist-stage model is trained using the following loss function:\nL 1 = L self + \u03bb 1 L dis + \u03bb 2 L sop + \u03bb 3 L style , (1)\nwhere \u03bb 1 , \u03bb 2 and \u03bb 3 are adjustable hyperparameters. L self , L dis , L sop and L style are the self-reconstruction loss, the disentanglement loss, the sequence order prediction loss and the style classifier loss, respectively. Figure 2 shows the workflow of learning objects.\nIn the second stage, we use a denoising autoencoder (DAE) loss to train another encoderdecoder model for reconstructing x:\nL 2 = \u2212 T t=1 logP (x t |x <t , {k i } l i=1 , x m ). (2\n)\nThis stage is unrelated to author styles, and helps achieve better content preservation.", "publication_ref": ["b31", "b16", "b16"], "figure_ref": ["fig_0", "fig_1"], "table_ref": []}, {"heading": "Discourse Representations Transfer", "text": "As described in Figure 2, we propose to learn discourse representations, and then reconstruct the texts from discourse representations. And we perform the disentanglement and stylizing operation based on discourse representations.\nDiscourse Representations Supposing that x m consists of n sentences, we insert a special token \u27e8Sen\u27e9 at the end of each sentence in x m (Reimers and Gurevych, 2019;Guan et al., 2021b). Let r n denote the hidden state of the encoder at the position of the n-th special token, {r i } n i=1 = Encoder(x m ). And z n is the output of the fusion module corresponding to r n . Previous studies have demonstrated that correcting the order of shuffled sentences is a simple but effective way to learn meaningful discourse representations . As shown in Figure 1, we feed z n into a pointer network (Gong et al., 2016) to predict orders. During training, we shuffled the original sentence order and feed the perturbed text into the encoder for calculating L sop .\nFusion Module To provide signals of transfer direction, we concatenate the learned discourse representations {r i } n i=1 with the style embedding s and fuse them using a multi-head attention layer, as illustrated in Figure 1. To capture discourselevel features of texts with different author-styles, we set each style embedding to a vector with the same dimension as r i . Formally, we derive the style-aware discourse representations {z i } n+1 i=1 as follows:\n{z i } n+1 i=1 = MHA(Q = K = V = {s \u2225 {r i } n i=1 }),(3)\nwhere MHA is the multi-head attention layer, Q/K/V is the corresponding query/key/value, \u2225 is the concatenation operation. Then, the decoder gets access to {z i } n+1 i=1 through the cross-attention layer, which serve as a discourse-level guidance for generating the transferred texts. Then, we feed {z i } n+1 i=1 into the decoder. Pointer Network Following Logeswaran et al. (2018); , we use a pointer network to predict the original orders of the shuffled sentences. The each position probability of sentence order is formulated as follows:\np i = softmax({z i } n i=1 W z T i ),(4)\nwhere p i is predicted probabilities of sentence i, W is a trainable parameter.", "publication_ref": ["b24", "b9", "b7", "b19"], "figure_ref": ["fig_1", "fig_0", "fig_0"], "table_ref": []}, {"heading": "First-Stage Training Objectives", "text": "Self-Reconstruction Loss We formulate selfreconstruction loss as follows:\nL self = \u2212 T t=1 logP (x m t |x m <t , {r i } n i=1 , s), (5\n)\nwhere s is the learnable embedding of s. During inference, we replace s with the embedding of the target style\u015d (i.e.,\u015d), to achieve the style transfer.\nDisentanglement Loss We disentangle the style and content on discourse representations. Inspired by prior studies on structuring latent spaces (Gao et al., 2019;Zhu et al., 2021), we devise an additional loss function L dis to pull close discourse representations from different examples in the same mini-batch, corresponding to different author styles. L dis and L self work as adversarial losses and lead the model to achieve a balance between content preservation and style transfer. We derive L dis as follows:\nL dis = 1 2b b i=1 b j=1 \u2225r i \u2212r j \u2225 2 2 ,(6)\nr = 1 n n i=1 r i (7\n)\nwhere b is the size of mini-batch.\nSentence Order Prediction Loss We formulate L sop as the cross-entropy loss between the golden and predicted orders as follows:\nL sop = \u2212 1 n n i=1 o i log(p i ),(8)\nwhere o i is a one-hot ground-truth vector of correct sentence position, and p i is predicted probabilities.", "publication_ref": ["b6", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Style Classifier Loss", "text": "We expect the transferred text to be of the target style. Hence we train a style classifier to derive the style transfer loss as follows:\nL style = \u2212Exm \u223cDecoder [logP C (s|x m )], (9)\nwhere P C is the conditional distribution over styles defined by the classifier. We train the classifier on the whole training set with the standard crossentropy loss. Then, we freeze the weights of style classifier for computing L style . On the other hand, we follow Lee et al. (2021); Dai et al. (2019) to use soft sampling to allow gradient back-propagation.", "publication_ref": ["b16", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Content Preservation Enhancing", "text": "As aforementioned, author styles have a strong correlation with contents. It is difficult to transfer such style-specific contents to other styles directly.\nSince we train the model in an auto-encoder manner, it will have no idea how to transfer those content representations that have never seen other style embeddings during training. To address the issue, we propose to mask the style-specific keywords in the source text and perform style transfer on the  masked text in the first generation stage. Then, we fill the masked tokens in the second stage. We follow Xiao et al. (2021) to use a frequencybased method to identify the style-specific keywords. Specifically, we extract style-specific keywords by (1) obtaining the top-10 words with the highest TF-IDF scores from each corpus, (2) retaining only people's names, place names, and proper nouns, (3) and filtering out those words with a high frequency in all corpora 3 . We denote the resulting word set as D s for the corpus with the style s. We extract the style-specific keywords k from the text x by selecting the words that are in D s . We detail above operation and explain it in Appendix A.\nIn the second stage, we train another model to fill the mask tokens in outputs of the first stage conditioned on the identified style-specific keywords in source inputs. During training, we concatenate the keywords in k with a special token \u27e8Key\u27e9 and feed them into the encoder paired with x m , as shown in Figure 1. The training object is formulated as Equation 2. During inference, the decoder generates the transferred textx conditioned on the output of the first stagex m in an auto-regressive manner.", "publication_ref": ["b31"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Experiments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Datasets", "text": "We construct stylized story datasets in Chinese and English, respectively. The Chinese dataset consists of three styles of texts, including fairy tales from LOT (Guan et al., 2021a), LuXun (LX), and JinYong (JY). Specifically, LuXun writes realism novels while JinYong focuses on martial arts novels. These texts of different styles have a gap in lexical, syntactic, and semantic levels. Samples of different styles are detailed in Appendix C.\nIn our experiments, we aim to transfer a fairy tale to the LX or JY style. The English dataset consists of two styles of texts, including everyday stories from ROCStories (Mostafazadeh et al., 2016) and fragments from Shakespeare's plays. We expect to transfer a five-sentence everyday story into the Shakespeare style. The statistics of datasets are shown in Table 2. The more details are described in Appendix B.", "publication_ref": ["b20"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Implementation", "text": "We take LongLM BASE (Guan et al., 2021a) and T5 BASE (Raffel et al., 2020) as the backbone model of both generation stages for Chinese and English experiments, respectively. Furthermore, the fusion module and pointer network consist of two and one layers of randomly initialized bidirectional Transformer blocks (Vaswani et al., 2017), respectively.\nWe conduct experiments on one RTX 6000 GPU. In addition, we build the style classifier based on the encoder of LongLM BASE and T5 BASE for Chinese and English, respectively.\nWe set \u03bb 1 /\u03bb 2 /\u03bb 3 in Equation 1 to 1/1/1, the batch size to 4, the learning rate to 5e-5, the maximum sequence length of the encoder and decoder to 512 for both generation stages in the Chinese experiments. And the hyper-parameters for English experiments are the same except that \u03bb 1 /\u03bb 2 /\u03bb 3 are set to 0.5/0.5/0.5 and the learning rate to 2.5e-5. More implementation details are presented in Appendix D.", "publication_ref": ["b23", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Baselines", "text": "Since no previous studies have focused on story author-style transfer, we build several baselines by adapting short-text style transfer models. For a fair comparison, we initialize all baselines using the same pre-trained parameters as our model. Specifically, we adopt the following baselines: Style Transformer: It adds an extra style embedding and a discriminator to provide style transfer rewards without disentangling content from styles (Dai et al., 2019). StyleLM: This baseline generates the target text conditioned on the given style token and corrupted version of the original text (Syed et al., 2020). Reverse Attention: It inserts a reverse attention module on the last layer of the encoder, which aims to negate the style information from the hidden states of the encoder (Lee et al., 2021).", "publication_ref": ["b2", "b26", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Automatic Evaluation", "text": "Evaluation Metrics Previous works evaluate style transfer systems mainly from three aspects including style transfer accuracy, content preservation, and sentence fluency. A good style transfer system needs to balance the contradiction between content preservation and transfer accuracy (Zhu et al., 2021;Niu and Bansal, 2018). We use a joint metric to evaluate the overall performance of models. On the other hand, previous studies usually use perplexity (PPL) of a pre-trained language model. However, in our experiments, we found that the PPL of model outputs is lower than human-written texts, suggesting that PPL is not reliable for evaluating the quality of stories. Therefore, we evaluate the fluency through manual evaluation.\nSpecifically, we adopt the following automatic metrics: (1) Style Transfer Accuracy: We use two variants of style transfer accuracy following Krishna et al. (2021), absolute accuracy (a-Acc) and relative accuracy (r-Acc). We train a style classifier and regard the classifier score as the a-Acc. And r-Acc is a binary value to indicate whether the style classifier score the output higher than the input (1/0 for a higher/lower score). We train the classifier by fine-tuning the encoder of LongLM BASE and T5 BASE on the Chinese and English training set, respectively. The classifier achieves a 99.6% and 99.41% accuracy on the Chinese and English test sets, respectively. (2) Content Preservation: We use BLEU-n (n=1,2) (Papineni et al., 2002) and BERTScore (BS) (Zhang* et al., 2020) between generated and input texts to measure their lexical and semantic similarity, respectively. And we report recall (BS-R), precision (BS-P) and F1 score (BS-F1) for BS. (3) Overall: We use the geometric mean of a-ACC and BLEU/BS-F1 score (BL-Overall/BS-Overall) to assess the overall performance of models (Krishna et al., 2020;Lee et al., 2021).", "publication_ref": ["b11", "b21", "b22", "b15", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Results on the Chinese Dataset", "text": "We show the overall performance and individual metrics results in Table 3. In terms of overall performance, Story-Trans outperforms baselines, illustrating that Sto-ryTrans can achieve a better balance between style transfer and content preservation.\nIn terms of style accuracy, StoryTrans achieves the best style transfer accuracy (a-Acc) in LX and comparable performance in JY. The bad performance of baselines indicates the necessity to perform explicit disentanglement beyond the token level. In addition, manual inspection shows that Style Transformer tends to copy the input, accounting for the highest BLEU score and BERTScore.    verse Attention degenerate into an auto-encoder, and tend to copy the input even more than their performance on the Chinese dataset. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Results on Ablation Study As shown in", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "StyleLM", "text": "1\u20e3 Professor Currie climbed up alone and reached the highest peak of the mountain range. The sound of footsteps seemed to be getting closer and closer, and there seemed to be some panting sounds after that.2\u20e3 Guo Jing propped his hands on the wall and looked from a distance. Curry and the professor stopped, and saw the door slowly closed, and the thick fog cleared away. Mr. Curry said: \"I can't tell you.\" Guo Jing said: \"Why not?\" Zhu Cong said: \"If you don't tell me, I will never let you follow me, I will follow you until you leave far away.\" Zhu Zuyin said: \"Mr. Zhu, let's go!\" He couldn't help but let go of his sleeves and climbed forward alone. 5\u20e3 Mark strode after him, neither of them seemed to want to leave him, but after a long time, they let go.\n1\u20e3 Once, when Professor Curry was climbing the highest peak of the Cairngo Mountains alone, he heard some huge footsteps and panting behind him, like someone following him. 2\u20e3\nProfessor Curry immediately stopped and looked around, but he couldn't see and touch anything due to the thick fog. 3\u20e3 He had no choice but to take a step forward, but at the same time, the weird footsteps sounded. 4\u20e3 Professor Curry couldn't help being terrified, and ran far far away in one breath. 5\u20e3 Since then, he never dared to climb Ban Makrut Mountain alone.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Source Text (ZH)", "text": "1\u20e3 \u4e00\u6b21\uff0c\u67ef\u91cc\u6559\u6388\u5728\u72ec\u81ea\u6500\u767b\u51ef\u6069\u679c\u5c71\u8109\u7684\u6700\u9ad8\u5cf0\u65f6\u53d1\u73b0\uff0c\u5728\u4ed6\u7684\u8eab\u540e\u4e0d\u65f6\u4f20\u51fa\u5de8\u5927\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u6709\u4eba\u5728\u4ee5\u5927\u4ed6\u4e09\u56db\u500d\u7684\u6b65\u4f10\u7d27\u8ddf\u5176\u540e\uff0c\u4f3c\u4e4e\u8fd8\u53ef\u4ee5\u542c\u5230\u5de8\u4eba\u5598\u606f \u7684\u58f0\u97f3\u30022\u20e3 \u67ef\u91cc\u6559\u6388\u7acb\u5373\u7ad9\u4f4f\u5de6\u53f3\u5f20\u671b\uff0c\u7531\u4e8e\u5927\u96fe\u4ec0\u4e48\u4e5f\u770b\u4e0d\u6e05\uff0c\u56db\u5468\u4e5f\u6478\u4e0d\u5230\u4efb\u4f55\u4e1c\u897f\u3002 3\u20e3 \u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u53ef\u4e0e\u6b64\u540c\u65f6\uff0c\u90a3\u602a\u5f02\u7684\u811a\u6b65\u58f0\u4e5f\u968f\u4e4b\u54cd\u8d77\u3002 4\u20e3 \u67ef\u91cc\u6559\u6388\u7981\u4e0d\u4f4f\u6bdb\u9aa8\u609a\u7136\uff0c\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u30025\u20e3 \u4ece\u90a3\u4ee5\u540e\uff0c\u4ed6\u518d\u4e5f\u4e0d\u6562\u72ec\u81ea\u6500\u767b\u73ed\u9a6c\u514b\u5f8b\u5c71\u4e86\u3002 1\u20e3 Somehow after he looked around, the sound of footsteps suddenly suppressed, and while looking around silently, he said loudly: \"Someone stepped heavily on the mountain, and I almost fainted.\" 2\u20e3 Curry suddenly realized that the thick fog seemed to be piercing my feet, and there was no other way, so he hurriedly jumped up, and looked around. He even moved a few times, and the thick fog around disappeared, and there was no one around except for \"moving\" noisy. 3\u20e3 He didn't dare to step forward again, at this moment, he suddenly heard hissing sounds from behind, and then two internal forces came up from the soles of his feet. 4\u20e3 He was so annoyed that jumping up, two streams of blood flew from under his feet, and went straight out in one breath. 5\u20e3 At the moment, he no longer dares to stay alone on the mountains, and he no longer wants to leave alone.   ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "StoryTrans", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Manual Evaluation", "text": "We randomly sampled 100 fairy tales from the Chinese dataset, and obtained 800 generated texts from StoryTrans and three baseline models. Then, we hire three Chinese native speakers to evaluate in three aspects including style transfer accuracy (Sty.), content preservation (Con.) and coherence (Coh.). We ask the annotators to judge each aspect from 1 (the worst) to 3 (the best).\nAs illustrated in Table 6, our StoryTrans received the highest style accuracy and modest performance in content preservation and coherence. More details and analysis are presented in Appendix G.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "Case Study", "text": "Table 5 shows the cases generated by StoryTrans and the best baseline. StyleLM inserts many unrelated sentences, which overwhelm the original content and impact the coherence, further leading to the content loss of sentences 3 and 4. On the contrary, StoryTrans supplement several short phrases or plots (e.g., \"\u7eb5\u8eab\u8dc3\u8d77\" /\"hurriedly jumped up\") to enrich the storyline and maintain the main content. Furthermore, StoryTrans can rewrite most sentences with the target style and maintain source semantics. In addition, StyleLM tends to discard the source entities and use words which is specific in the target style (e.g., \"\u90ed\u9756\" /\"Guo Jing\"), while StoryTrans dose not, suggesting the necessity of the mask-and-fill framework.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_9"]}, {"heading": "Stylistic Feature Visualization", "text": "We follow Syed et al. (2020) to define several stylistic features and visualize the features of the golden texts and generated texts on the Chinese test set. The stylistic features include the type and number of punctuation marks, the number of sentences, and the number of words. As shown in Figure 3, the texts generated by Reverse Attention and StyleLM have similar stylistic features to source texts. In contrast, StoryTrans can better capture different stylistic features and transfer source texts to specified styles. More details are in Appendix F.", "publication_ref": ["b26"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper, we present the first study for story author-style transfer and analyze the difficulties of this task. Accordingly, we propose a novel generation model, which explicitly disentangles the style information from high-level text representations to improve the style transfer accuracy, and achieve better content preservation by injecting style-specific contents. Automatic evaluations show StoryTrans outperform baselines on the overall performance.\nFurther analysis shows StoryTrans has a better ability to capture linguistic features for style transfer.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "In style transfer, content preservation and style transfer are adversarial. Long texts have richer contents and more abstract stylistic features. We also notice that content preservation is the main disadvantage of StoryTrans in automatics evaluation results. Case studies also indicate that StoryTrans can maintain some entities and the relations between entities. However, strong discourse-level style transfer ability endangered content preservation. In contrast, baselines such as Style Transformer have better content preservation but hardly transfer the style. We believe that StoryTrans is still a good starting point for this important and challenging task.\nDuring preliminary experiments, we also manually inspected multiple author styles besides Shakespeare, such as Mark Twain. However, we found that their styles are not as obvious as Shakespeare, as shown in the following example. Therefore, we only selected authors with relatively distinct personal styles for our transfer experiments. In future work, we will expand our research and choose more authors with distinct styles for style transfer. For example, the style distinction between the following examples is not readily apparent.\n\u2022 Everyday story in our datatset: Ashley wanted to be a unicorn for Halloween. She looked all over for a unicorn costume. She wasn't able to find one.\n\u2022 \"A Double Barrelled Detective Story\" by Mark Twain: You will go and find him. I have known his hiding-place for eleven years; it cost me five years and more of inquiry.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ethics Statement", "text": "We perform English and Chinese experiments on public datasets and corpora. Specifically, English datasets come from ROCstories and Project Gutenberg. Moreover, Chinese datasets include the LOT dataset and public corpora of JY and LX. Automatic and manual evaluation demonstrate that our model outperforms strong baselines on both Chinese and English datasets. In addition, our model can be easily applied to different languages by substituting specific pre-trained language models. As for manual evaluation, we hired three native Chinese speakers as annotators to evaluate generated texts and did not ask about personal privacy or collect the personal information of annotators. We pay 1.8 yuan (RMB) per sample in compliance with Chinese wage standards. Considering it would cost an average of 1 minute for an annotator to score a sample, the payment is reasonable. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Style-Specific Contents", "text": "We detail how we extract style-specific contents and explain how they are used from the following three aspects:\nWhat do we mean by \"style-specific content\"?\nWe refer to \"style-specific content\" as those mainly used in texts with specific styles and should be retained after style transfer. For example, \"Harry Potter\" and \"Horcrux\" are style-specific since they are used only in J.K. Rowling-style stories. When transferring J.K. Rowling-style stories to other styles, style-specific tokens shouldn't be changed. However, existing models tend to drop style-specific tokens since they are not trained to learn these tokens conditioned on other styles.\nHow do we extract \"style-specific contents\"? We extract style-specific contents by (1) obtaining top-10 salient tokens using TF-IDF, (2) reserving only people names (e.g., \"Harry Potter\"), place names (e.g., \"London\"), and proper nouns (e.g., \"Horcrux\"), and (3) filtering out high-frequency tokens in all corpus (e.g., \"London\") since these tokens can be learned conditioned on every style. We regard the remaining tokens as style-specific contents.\nAs mentioned before, we employ the TF-IDF algorithm on the corpus to obtain rough style-specific contents for different styles, respectively. The reason for using TF-IDF: it is necessary to ensure that the extracted tokens are salient to the story plots. We extract style-specific tokens from the salient tokens using the second and third steps. Then, we use a part-of-speech tagging toolkit (e.g., NLTK) to identify function words and prepositions to retain people's names, place names, and proper nouns. Note that the frequency is an empirical value observed from datasets. However, the TF-IDF algorithm chooses the important words corresponding to the special style based on word frequency. There may be some style-unrelated words that are important to the content. Therefore, we need to filter out style-unrelated words. Concretely, we use Jieba 4 /NLTK (Bird et al., 2009) to collect the word frequency for Chinese and English datasets, respectively. Moreover, we regard the words possessing a high frequency in all styles corpus as style-unrelated words. Specifically, We set tokens appearing in 10% samples in the dataset as highfrequency words. Then we filter out these words to obtain style-specific contents. The frequency value needs to be reset to apply the method to other datasets.\nHow are the \"style-specific contents\" used? One challenge of long-text style transfer is transferring discourse-level author style while preserving the main characters and storylines. It's difficult for existing models to transfer style-specific contents since they are not trained to learn these tokens conditioned on other styles. Therefore, we extract \"style-specific contents\" before style transferring and replace them with the special token \"<Mask>\". 4 https://github.com/fxsjy/jieba Then, the \"style-specific contents\" will be filled in the second stage, as shown in Figure 1.", "publication_ref": ["b1"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "B Data Pre-Processing", "text": "Due to lack of stylized author datasets, we collected several authors' corpus to construct new datasets. As for Chinese, we extracted paragraphs from 21 novels of LuXun (LX) and 15 novels of JinYong (JY), and fairy tales collected by Guan et al. (2021a). On the other hand, the English dataset consists of everyday stories from ROCStories (Mostafazadeh et al., 2016) and fragments from Shakespeare's plays. Each fragment of Shakespeare's plays comprises multiple consecutive sentences and as long as samples in ROC-Stories. We collect the Shakespeare-style texts from the Shakespeare corpus in Project Gutenberg 5 under the Project Gutenberg License 6 . We use Jieba/NLTK (Bird et al., 2009) for word tokenization for the Chinese/English dataset in data pro-processing. In addition, these data are public corpora, and we also check the information for anonymization.\nRegarding to limitation of modern language models, the length of samples is also limited. We set the max length as 384 and 90 for Chinese and English, respectively. Each sample has 4 sentences at least. We choose above length to balance the data length of different styles. Additionally, we filtered the texts which are too long to generate or too short to unveil author writing style. As Figure 4 shows, texts in the Chinese dataset spans a diverse range of length.", "publication_ref": ["b20", "b1"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "C Different Style Samples", "text": "In process of constructing datasets, we try to collect different author corpus who have a gap in writing styles. As shown in Table 8, the JY-style texts mostly describe martial arts actions and construct interesting plots, while the LX-style texts focus on realism with profound descriptive and critical significance. And the fairy tales differ from these texts in terms of topical and discourse features. In the English datasets, the Shakespeare-style texts are flamboyant and contain elaborate metaphors and ingenious ideas, which the everyday stories are written in plain language and without rhetoric.\n1\u20e3 Once, when Professor Curry was climbing the highest peak of the Cairngo Mountains alone, he heard some huge footsteps and panting behind him, like someone following him. 2\u20e3 Professor Curry immediately stopped and looked around, but he couldn't see and touch anything due to the thick fog. 3\u20e3 He had no choice but to take a step forward, but at the same time, the weird footsteps sounded. 4\u20e3 Professor Curry couldn't help being terrified, and ran far far away in one breath. 5\u20e3 Since then, he never dared to climb Ban Makrut Mountain alone. 1\u20e3 Ke Zhen'e didnt dare to do anything wrong, it was too weird. Breathing carefully and moving of his body after a few steps, his whole body suddenly grew bigger and his feet touched a ball of cotton, then he immediately bowed his head and shrank back , seemed to be afraid that the little novice will move again. Ke Zhen'e said : Mo Shixia, there is something wrong.\" The sorcery said in a deep voice, \"No, its true. The thing must be exactly the same.\" Ke Zhen'e said: \"Thats right.\"4\u20e3 Jumping up suddenly, flying forward. He stepped down and immediately jumped out of the window, which was exactly the same as that thing. That was different from things, Ke Zhen'e was startled, then jumped. The thing stood still, turned around immediately, unable to turn around in one breath, then there was a loud bang in front of him, and at the same time, there was a sound of panting and shouting from a distance. Yang Guo grabbed the horse's reins with his left hand, clamping with his leg, and then little red horse rushed out of sight. Guo Fu was so frightened that his hands and feet were sore, and she slowly walked to the corner to pick up the long sword. Using soft objects to display strength was originally the essence of the ancient tomb school martial arts. Yang Guo's internal energy was strong at this moment, and a flick of his sleeve was no less than the impact of a giant steel whip. Yang Guo hugged Guo Xiang, and rode a sweaty horse to the north. After a while, he passed Xiangyang and ran for dozens of miles. Although Huang Rong climbed to the top of the tree and looked far into the distance, she could not see any trace of him.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_14"]}, {"heading": "LX \u81ea\u300a\u65b0\u9752\u5e74\u300b\u51fa\u7248\u4ee5\u6765\uff0c\u4e00\u5207\u5e94\u4e4b\u800c\u5632\u9a82\u6539\u9769\uff0c\u540e\u6765\u53c8\u8d5e\u6210\u6539\u9769\uff0c\u540e\u6765\u53c8\u5632\u9a82\u6539\u9769\u8005\uff0c\u73b0\u5728\u62df\u6001\u7684\u5236\u670d\u65e9\u5df2\u7834\u788e\uff0c\u663e\u51fa\u81ea\u8eab\u7684\u672c", "text": "\u76f8\u6765\u4e86\uff0c\u771f\u6240\u8c13\"\u4e8b\u5b9e\u80dc\u4e8e\u96c4\u8fa9\"\uff0c\u53c8\u4f55\u5f85\u4e8e\u7eb8\u7b14\u5589\u820c\u7684\u6279\u8bc4\u3002\u6240\u4ee5\u6211\u7684\u5e94\u65f6\u7684\u6d45\u8584\u7684\u6587\u5b57\uff0c\u4e5f\u5e94\u8be5\u7f6e\u4e4b\u4e0d\u987e\uff0c\u4e00\u4efb\u5176\u6d88\u706d\u7684\uff1b\u4f46\u51e0 \u4e2a\u670b\u53cb\u5374\u4ee5\u4e3a\u73b0\u72b6\u548c\u90a3\u65f6\u5e76\u6ca1\u6709\u5927\u4e24\u6837\uff0c\u4e5f\u8fd8\u53ef\u4ee5\u5b58\u7559\uff0c\u7ed9\u6211\u7f16\u8f91\u8d77\u6765\u4e86\u3002\u8fd9\u6b63\u662f\u6211\u6240\u60b2\u54c0\u7684\u3002\u6211\u4ee5\u4e3a\u51e1\u5bf9\u4e8e\u65f6\u5f0a\u7684\u653b\u51fb\uff0c\u6587\u5b57 \u987b\u4e0e\u65f6\u5f0a\u540c\u65f6\u706d\u4ea1\uff0c\u56e0\u4e3a\u8fd9\u6b63\u5982\u767d\u8840\u8f6e\u4e4b\u917f\u6210\u75ae\u7596\u4e00\u822c\uff0c\u5018\u975e\u81ea\u8eab\u4e5f\u88ab\u6392\u9664\uff0c\u5219\u5f53\u5b83\u7684\u751f\u547d\u7684\u5b58\u7559\u4e2d\uff0c\u4e5f\u5373\u8bc1\u660e\u7740\u75c5\u83cc\u5c1a\u5728\u3002 Since the publication of \"New Youth\", everyone has ridiculed the reform in response to it, later approved of it, and then ridiculed the reformers. Now the mimetic uniform has long been broken, showing its true nature. The so-called \"facts speak louder than words\", why should they be criticized by pen and paper mouthpieces. Therefore, my timely and superficial writing should also be ignored and wiped out. However, a few friends thought that the current situation was not much different from that at that time, and they could still be preserved, so they edited them for me. This is what I am saddened by. I think any attack on the evils of the times, the writing must perish at the same time as the evils of the times, because this is like the boils and boils caused by the white blood wheel. If it is not eliminated by itself, the existence of its life also proves that the germs are still there.\nTale \u6709\u4e2a\u8d22\u4e3b\uff0c\u975e\u5e38\u559c\u6b22\u81ea\u5bb6\u7684\u4e00\u68f5\u6a58\u5b50\u6811\u3002\u8c01\u4ece\u6811\u4e0a\u6458\u4e0b\u4e00\u4e2a\u6a58\u5b50\uff0c\u4ed6\u5c31\u4f1a\u8bc5\u5492\u4eba\u5bb6\u4e0b\u5341\u516b\u5c42\u5730\u72f1\u3002\u8fd9\u5e74\uff0c\u6a58\u5b50\u53c8\u6302\u6ee1\u4e86\u679d\u5934\u3002\u8d22 \u4e3b\u7684\u5973\u513f\u998b\u7684\u76f4\u6d41\u53e3\u6c34\u3002\u5fcd\u4e0d\u4f4f\u6458\u4e86\u4e00\u4e2a\uff0c\u521a\u5c1d\u4e86\u4e00\u53e3\uff0c\u5c31\u4e0d\u7701\u4eba\u4e8b\u4e86\u3002\u8d22\u4e3b\u540e\u6094\u4e0d\u5df2\uff0c\u628a\u6811\u4e0a\u7684\u6a58\u5b50\u90fd\u6458\u4e0b\u6765\uff0c\u5206\u7ed9\u90bb\u5c45\u548c\u8def \u4eba\u3002\u6700\u540e\u4e00\u4e2a\u6a58\u5b50\u5206\u5b8c\uff0c\u5973\u513f\u5c31\u82cf\u9192\u4e86\u3002\u8d22\u4e3b\u518d\u4e5f\u4e0d\u6562\u968f\u4fbf\u8bc5\u5492\u522b\u4eba\u4e86\u3002 There was a rich man who liked his orange tree very much. Whoever plucks an orange from the tree, he will curse him to eighteen levels of hell. This year, oranges are hanging on the branches again. The rich man's daughter was drooling. Then, she couldn't help picking one, and just after a bite, she was unconscious. The rich man was remorseful, so he plucked all the oranges from the tree and gave them to neighbors and passers-by. After the last orange was given, the daughter woke up. The rich man no longer dared to curse others casually.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "ROC", "text": "Garth has a chicken farm. Each morning he must wake up and gather eggs. Yesterday morning there were 33 eggs! After gathering the eggs, he feeds the chickens. Finally he gets to eat breakfast, and go to school.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D More Implementation Details", "text": "In terms of selecting pre-trained model, LongLM base and T5 base are the generic base model for the Chinese and English generation, respectively. To optimize the models for these specific languages, we have fine-tuned them using different hyperparameter values (\u03bb 1/2/3 ). These values were determined based on the performance  observed on a validation set, which was created by pre-extracting 5% of the training data for this purpose.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E More Ablation Study Results", "text": "To explore the effect of the proposed component, we also conduct more ablation studies on Chinese datasets. As shown in Table 9, the ablation of L dis leads to better style accuracy, which show the different trends comparing with English dataset. We conjecture that L dis aims to maintain the content and reduces style information. Without L dis , the powerful L style leads the StoryTrans to degenerate to style conditional language model. Furthermore, the ablation of L style also confirms the powerful ability of style control as in previous paper. And we find that when removing L sop , the model loses the ability to transfer at the discourse level and has only learned token-level copy.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_15"]}, {"heading": "F Style Analysis of Transferred Texts", "text": "In order to investigate whether our StoryTrans indeed rephrase the expression of texts, we employ surface elements of text to show author writing styles. And the surface element are associated with statistical observations. For example, the small average length of sentences show the author preference to write a short sentence, and more question marks indicate the author accustomed to using questions. To this end, we use the number of (1) commas, (2) colons, (3) sentences in a paragraph, (4) question mark (5) left quotation mark, (6) right quotation mark, and ( 7) average number of words in a sentence to quantify surface elements into a 7 dimension vector. Then we leverage the t-SNE to visualize the golden texts and transferred texts. As shown in Figure 3, different style distribute separately across the style space. This proves JY, LX and fairy tale in Chinese dataset have a gap in writing style. And Figure 5 shows the transferred texts fall in golden texts in style space, indicating Story-Trans successfully transferred the writing style.", "publication_ref": [], "figure_ref": ["fig_2", "fig_4"], "table_ref": []}, {"heading": "G More Details of Manual Evaluation", "text": "In addition to automatic evaluation, we conduct manual evaluation on generated texts. As mentioned before, we require the annotators to score each aspect from 1 (the worst) to 3 (the best). As for payment, we pay 1.8 yuan (RMB) per sample in compliance with Chinese wage standards. Our annotators consist of undergraduate students who are experienced in reading texts written in the styles of the respective authors (JY and LX). To ensure they fully understand the evaluation metrics, we conducted case analyses with them. Our scoring rubric assigns 1, 2, or 3 points to the transferred text based on the proportion of sentences meeting the following criteria (1/3, 2/3, or 3/3):\n\u2022 Style Accuracy: whether the transferred text conforms to the corresponding style.\n\u2022 Content Preservation: whether the source content, such as character names, are retained.\n\u2022 Coherence: whether the sentences in the transferred text are semantically connected.\nAnd we compute the final score of each text by averaging the scores of three annotators.\nAs illustrated in manual evaluation, we observe that the results mainly conform with the automatic evaluation. Our StoryTrans obtained the highest score on the style accuracy in both transferred directions by a sign test compared to the other baselines, showing its stable ability of style control. Moreover, in terms of content preservation, the score of StoryTrans is comparable with StyleLM and slightly higher than Reverse Attention, demonstrating that StoryTrans can keep the main semantics of input. In terms of coherence, the score of Story-Trans is also comparable with baselines, showing some room for improvement. As discussed before, Style Transformer tends to copy the input, leading to the highest performance in content preservation and coherence. In summary, human evaluation depicts the strength of StoryTrans not only on style control but also on overall performance, indicating a balance of these metrics.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "H More Case Studies", "text": "We show more cases in Table 7. Comparing source text with Style Transformer, Style Transformer copies the input and only changes little tokens. This result also confirms with highest BLEU and BERTScore in automatic results. Like StyleLM, Reverse Attention also incorporates some target author content into generated texts. However, Reverse Attention inserts too much content that overwhelms original plots. Furthermore, some critical entities (e.g., character name, \"\u67ef\u91cc\u6559\u6388\" /\"Professor Curry\" \u2192 \"\u67ef\u9547\u6076\" /\"Ke Zhen'e\") are revised to the similar word on in target author corpus. To maintain the story coherence, these important entities should stay the same. In summary, the token-level transfer may destroy the essential plots and damage the coherence.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_12"]}, {"heading": "Acknowledgments", "text": "This work was supported by the NSFC projects (Key project with No. 61936010 ). This work was also supported by the Guoqiang Institute of Tsinghua University, with Grant No. 2020GQG0005.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? Appendix B B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Appendix B B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? 4.1 and Appendix B B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. We counted the details of the dataset and discussed the details in Section 4.1 C Did you run computational experiments?", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "4.4", "text": "C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? 4.2 Appendix B D Did you use human annotators (e.g., crowdworkers) or research with human participants? 4.5 D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? 4.5 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)? 4.5 and Appendix F D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? Appendix B and Appendix F D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? Not applicable. We do not submit the protocol to an ethics review board because our country has not yet established an ethical committee at the national level.\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? 4.5 and Appendix F", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Neural machine translation by jointly learning to align and translate", "journal": "", "year": "2015", "authors": "Dzmitry Bahdanau; Kyung Hyun Cho; Yoshua Bengio"}, {"ref_id": "b1", "title": "Natural language processing with Python: analyzing text with the natural language toolkit", "journal": "Reilly Media, Inc", "year": "2009", "authors": "Steven Bird; Ewan Klein; Edward Loper"}, {"ref_id": "b2", "title": "Style transformer: Unpaired text style transfer without disentangled latent representation", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Ning Dai; Jianze Liang; Xipeng Qiu; Xuanjing Huang"}, {"ref_id": "b3", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b4", "title": "Hierarchical neural story generation", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Angela Fan; Mike Lewis; Yann Dauphin"}, {"ref_id": "b5", "title": "Measuring nominal scale agreement among many raters", "journal": "Psychological bulletin", "year": "1971", "authors": "L Joseph;  Fleiss"}, {"ref_id": "b6", "title": "Structuring latent spaces for stylized response generation", "journal": "", "year": "2019", "authors": "Xiang Gao; Yizhe Zhang; Sungjin Lee; Michel Galley; Chris Brockett; Jianfeng Gao; Bill Dolan"}, {"ref_id": "b7", "title": "End-to-end neural sentence ordering using pointer network", "journal": "", "year": "2016", "authors": "Jingjing Gong; Xinchi Chen; Xipeng Qiu; Xuanjing Huang"}, {"ref_id": "b8", "title": "Changjie Fan, and Minlie Huang. 2021a. Lot: A benchmark for evaluating chinese long text understanding and generation", "journal": "", "year": "", "authors": "Jian Guan; Zhuoer Feng; Yamei Chen; Ruilin He; Xiaoxi Mao"}, {"ref_id": "b9", "title": "Long text generation by modeling sentence-level and discourselevel coherence", "journal": "Long Papers", "year": "2021", "authors": "Jian Guan; Xiaoxi Mao; Changjie Fan; Zitao Liu; Wenbiao Ding; Minlie Huang"}, {"ref_id": "b10", "title": "Stochastic neighbor embedding. Advances in neural information processing systems", "journal": "", "year": "2002", "authors": "E Geoffrey; Sam Hinton;  Roweis"}, {"ref_id": "b11", "title": "NAST: A non-autoregressive generator with word alignment for unsupervised text style transfer", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Fei Huang; Zikai Chen; Chen Henry Wu; Qihan Guo; Xiaoyan Zhu; Minlie Huang"}, {"ref_id": "b12", "title": "Unsupervised controllable text formalization", "journal": "", "year": "2019", "authors": "Parag Jain; Abhijit Mishra; Amar Prakash Azad; Karthik Sankaranarayanan"}, {"ref_id": "b13", "title": "Disentangled representation learning for non-parallel text style transfer", "journal": "", "year": "2019", "authors": "Vineet John; Lili Mou; Hareesh Bahuleyan; Olga Vechtomova"}, {"ref_id": "b14", "title": "Bidisha Samanta, and Partha Talukdar. 2021. Fewshot controllable style transfer for low-resource settings: A study in indian languages", "journal": "", "year": "", "authors": "Kalpesh Krishna; Deepak Nathani; Xavier Garcia"}, {"ref_id": "b15", "title": "Reformulating unsupervised style transfer as paraphrase generation", "journal": "", "year": "2020", "authors": "Kalpesh Krishna; John Wieting; Mohit Iyyer"}, {"ref_id": "b16", "title": "Enhancing content preservation in text style transfer using reverse attention and conditional layer normalization", "journal": "Long Papers", "year": "2021", "authors": "Dongkyu Lee; Zhiliang Tian; Lanqing Xue; Nevin L Zhang"}, {"ref_id": "b17", "title": "SLM: Learning a discourse language representation with sentence unshuffling", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Haejun Lee; Drew A Hudson; Kangwook Lee; Christopher D Manning"}, {"ref_id": "b18", "title": "A hierarchical neural autoencoder for paragraphs and documents", "journal": "Association for Computational Linguistics", "year": "2015", "authors": "Jiwei Li; Thang Luong; Dan Jurafsky"}, {"ref_id": "b19", "title": "Sentence ordering and coherence modeling using recurrent neural networks", "journal": "", "year": "2018", "authors": "Lajanugen Logeswaran; Honglak Lee; Dragomir Radev"}, {"ref_id": "b20", "title": "A corpus and cloze evaluation for deeper understanding of commonsense stories", "journal": "", "year": "2016", "authors": "Nasrin Mostafazadeh; Nathanael Chambers; Xiaodong He; Devi Parikh; Dhruv Batra; Lucy Vanderwende; Pushmeet Kohli; James Allen"}, {"ref_id": "b21", "title": "Polite dialogue generation without parallel data", "journal": "Transactions of the Association for Computational Linguistics", "year": "2018", "authors": "Tong Niu; Mohit Bansal"}, {"ref_id": "b22", "title": "Bleu: A method for automatic evaluation of machine translation", "journal": "Association for Computational Linguistics", "year": "2002", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"ref_id": "b23", "title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "Journal of Machine Learning Research", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b24", "title": "Sentence-bert: Sentence embeddings using siamese bert-networks", "journal": "", "year": "2019", "authors": "Nils Reimers; Iryna Gurevych"}, {"ref_id": "b25", "title": "Style transfer from non-parallel text by cross-alignment", "journal": "Curran Associates Inc", "year": "2017", "authors": "Tianxiao Shen; Tao Lei; Regina Barzilay; Tommi Jaakkola"}, {"ref_id": "b26", "title": "Adapting language models for non-parallel authorstylized rewriting", "journal": "", "year": "2020", "authors": "Bakhtiyar Syed; Gaurav Verma; Anandhavelu Balaji Vasan Srinivasan; Vasudeva Natarajan;  Varma"}, {"ref_id": "b27", "title": "Progressive generation of long text with pretrained language models", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Zichao Bowen Tan; Maruan Yang; Eric Al-Shedivat; Zhiting Xing;  Hu"}, {"ref_id": "b28", "title": "Guess who? multilingual approach for the automated generation of author-stylized poetry", "journal": "IEEE", "year": "2018", "authors": "Alexey Tikhonov; P Ivan;  Yamshchikov"}, {"ref_id": "b29", "title": "Attention is all you need", "journal": "Curran Associates, Inc", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Illia Kaiser;  Polosukhin"}, {"ref_id": "b30", "title": "Mask and infill: Applying masked language model for sentiment transfer", "journal": "", "year": "2019", "authors": "Xing Wu; Tao Zhang; Liangjun Zang; Jizhong Han; Songlin Hu"}, {"ref_id": "b31", "title": "Transductive learning for unsupervised text style transfer", "journal": "", "year": "2021", "authors": "Fei Xiao; Liang Pang; Yanyan Lan; Yan Wang; Huawei Shen; Xueqi Cheng"}, {"ref_id": "b32", "title": "Unpaired sentiment-to-sentiment translation: A cycled reinforcement learning approach", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Jingjing Xu; Xu Sun; Qi Zeng; Xiaodong Zhang; Xuancheng Ren; Houfeng Wang; Wenjie Li"}, {"ref_id": "b33", "title": "Planand-write: Towards better automatic storytelling", "journal": "", "year": "2019", "authors": "Lili Yao; Nanyun Peng; Ralph Weischedel; Kevin Knight; Dongyan Zhao; Rui Yan"}, {"ref_id": "b34", "title": "Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?", "journal": "", "year": "", "authors": ""}, {"ref_id": "b35", "title": "error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc", "journal": "", "year": "", "authors": ""}, {"ref_id": "b36", "title": "If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used", "journal": "", "year": "", "authors": " Nltk;  Spacy;  Rouge"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure1: An overview of the generative flow. For discourse representation transfer (the first stage), the encoder employs discourse representations ({r i } n i=1 ) to contain main semantics of pre-processed input (x m ). Then, the fusion module stylizes the discourse representations with target style embedding (\u015d). For content preservation enhancing (the second stage), our model enhances the content preservation of transferred texts (x m ) with stylespecific content (k). x andx denote the original story and the final output, respectively.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Illustration of loss functions during training for the first stage (a) and second stage (b). Enc, Fus, Dec and C denote the encoder, the fusion module, the decoder, and style classifier, respectively.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Stylistic features visualization of the golden texts (-Golden) and generated texts (-Trans) on the Chinese test set using t-SNE (Hinton and Roweis, 2002).", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Length distribution of texts in the Chinese dataset.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure 5: Visualization of the golden LX-style texts and transferred LX-style texts on style space using t-SNE.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Gina was looking for a place to be alone. \u2022\u2022\u2022 <mask> was looking for a place to be <mask> . <Sen> \u2022\u2022\u2022", "figure_data": "Story paragraphs extracted from Chinese and English novels $$ $ \" %<Key 1 > Gina <Key 2 > alone \u2022\u2022\u2022' $Identifying style-specific contentsEncoderDecoderand replacing them with mask tokens in the story paragraphsDiscourse Representation+! \" Style$! $!$\" $\"\u2026 \u2026$# $$Pointer NetworkEncoderMasked Story Paragraphs $ \" Pulling Close for Disentangling Style-Specific Contents % Sequence ReconstructingFusion Embedding Module1 2 Sentence Order 2 3 4 4 1 3 Shuffled Unshuffled+% Then at the <mask> of my soul,\u2022\u2022\u2022 ' $ \"Generation FlowDecoder"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Statistics of the Chinese (ZH) and English (EN) datasets. Avg Len indicates the average length of tokens of each sample.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Automatic evaluation results on the test set of the Chinese and English datasets. Bold numbers indicate best performance. ZH-LX/ZH-JY is the Chinese author LuXun/JinYong, respectively. EN-SP is the English author", "figure_data": "Shakespeare. StoryTrans achieves the best overall performance (BL/BS-Overall), with a good trad-off between style accuracy (r/a-Acc) and content preservation (BLEU-1/2 and BS-P/R/F1).r-Acca-AccBLEU-1BLEU-2BS-PBS-RBS-F1BL-OverallBS-OverallProposed Model88.6252.4132.2012.7181.7787.5184.3134.3166.47(-) L dis (-) L style (-) Lsop (-) CE75.86 50.68 78.96 92.4131.37 7.93 38.96 73.1033.49 45.00 39.45 21.6214.52 23.79 19.20 6.0982.38 84.38 82.92 79.7388.07 89.16 88.62 86.1284.92 86.5 85.47 82.5927.44 16.51 33.80 31.8251.61 26.19 57.70 77.70"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "StoryTrans outperforms Reverse Attention. Additionally, StyleLM achieves better performance in content preservation, benefiting from inputting noisy versions of golden texts. But without disentanglement, it can't strip style information. This leads to a lower overall performance than StoryTrans. As for Style Transformer, the results demonstrate that only an attention-based model hardly removes style features in overwhelming tokens information, leading to degenerate into an auto-encoder.Results on the English Dataset Similarly, Story-Trans achieves the best overall performance on the English dataset, showing its effectiveness and generalization. And StoryTrans outperforms baselines significantly in terms of style transfer accuracy. As for content preservation, Style Transformer and Re-", "figure_data": "ModelsSty.Con.LXCoh.\u03baSty.Con.JYCoh.\u03baStyle Transformer StyleLM Reverse Attention1.02 1.61 1.692.95** 1.99 1.252.91** 1.58 1.640.80 0.20 0.211.00 1.7 2.072.98** 1.92 1.252.94** 1.94 1.920.89 0.23 0.20StoryTrans1.98**1.841.670.242.43**1.691.910.23"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": ", we observe a significant drop in transfer accuracy without L dis or L style . L dis works by disentangling stylistic features from the discourse representations, while L style exerts direct supervision on styles of generated texts. Without L \u67ef\u91cc\u6559\u6388\u72ec\u81ea\u6500\u767b\u4e86\u4e0a\u53bb,\u5230\u8fbe\u4e86\u5c71\u8109\u6700\u9ad8\u5cf0\u3002 \u53ea\u542c\u5f97\u811a\u6b65\u58f0\u4f3c\u4e4e\u8d8a\u8d70\u8d8a\u8fd1,\u5176\u540e\u4f3c\u4e4e\u8fd8\u53d1\u51fa\u4e00\u4e9b\u5598\u606f\u4e4b\u58f0\u30022\u20e3 \u90ed\u9756\u53cc\u624b\u6491\u5728\u5899\u4e0a,\u8fdc\u8fdc\u671b\u53bb,\u67ef\u91cc\u548c\u6559\u6388\u7ad9\u4f4f\u4e86,\u53ea\u89c1\u90a3 \u9053\u95e8\u7f13\u7f13\u95ed\u4e0a,\u5927\u96fe\u968f\u5373\u6563\u5f00,\u4ed6\u8fc8\u5f00\u4e86\u6b65\u5b50,\u7ee7\u7eed\u524d\u8fdb,\u811a\u6b65\u58f0\u97f3\u4e5f\u968f\u4e4b\u6d88\u5931\u3002\u67ef\u91cc\u5148\u751f\u9053:\"\u6211\u53ef\u4e0d\u80fd\u8ddf\u4f60\u8bf4\u3002\"\u90ed\u9756\u9053:\"\u4e3a\u4ec0\u4e48\u4e0d\u80fd\u8bf4?\"\u6731\u806a\u9053:\"\u5982\u679c\u4f60\u4e0d\u8bf4,\u6211\u53ef\u6c38 \u8fdc\u4e0d\u8ba9\u4f60\u8ddf\u7740\u6211\u4e86,\u6211\u4f1a\u4e00\u76f4\u8ddf\u7740\u4f60,\u76f4\u5230\u4f60\u8d70\u8fdc,\u6211\u624d\u4f1a\u79bb\u5f00\u4f60\u3002\"\u6731\u7956\u836b\u9053:\"\u6731\u5148\u751f,\u8fd9\u5c31\u8d70\u7f62!\"\u8bf4\u7740\u4e5f\u4e0d\u7531\u81ea\u4e3b\u7684\u6492\u5f00\u4e86\u8863\u8896,\u72ec\u81ea\u5411\u524d\u6500\u767b\u3002 5\u20e3 \u9a6c\u514b\u5927\u8e0f\u6b65\u8ddf\u5728\u4ed6 \u8eab\u540e,\u4e24\u4eba\u4f3c\u4e4e\u90fd\u4e0d\u60f3\u79bb\u5f00\u4ed6,\u4f46\u8fc7\u4e86\u826f\u4e45,\u8fd9\u624d\u653e\u5f00\u4e86\u624b\u3002", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "1\u20e3 Ian was very conceited. 2\u20e3 Ian believed he was superior to everyone that he met. 3\u20e3 Ian wanted to show off to the world how great he was. 4\u20e3 Ian decided to enter a reality show competition to show off. 5\u20e3 Ian finished in last place and was extremely embarrassed.1\u20e3 2\u20e3 Ian . Ian that he was superior the competition , as he had vst reality . 3\u20e3 Giue you think'd it was true ? 5\u20e3 Ian . Ian not that he was the last , and it were too late", "figure_data": "1\u20e3 \u4e0d\u77e5\u600e\u4e48\u5728\u4ed6\u5f20\u671b\u4e4b\u540e,\u9a6c\u514b\u7684\u811a\u6b65\u58f0\u7a81\u7136\u538b\u6291\u8d77\u6765,\u5728\u4ed6\u5fc3\u4e2d\u9ed8\u5ff5\u5f20\u671b\u4e4b\u65f6,\u5927\u58f0\u8bf4\u9053:\"\u6709\u4eba\u5728\u4ed6\u6559\u6388\u7684\u5c71\u8109\u4e0a\u91cd\u91cd\u8e0f\u4e86\u4e00\u811a,\u6211\u51e0\u4e4e\u6709\u4e00\u9f3b\u5b50\u6655\u8fc7\u53bb\u3002\" 2\u20e3 \u67ef\u91cc\u767b\u65f6(Ours)\u7701\u609f,\u67ef\u91cc\u56db\u5468\u5927\u96fe\u4f3c\u4e4e\u90fd\u523a\u7740\u6211\u8fd9\u4e00\u811a,\u66f4\u65e0\u522b\u6cd5,\u5fd9\u7eb5\u8eab\u8dc3\u8d77,\u6559\u6388\u56db\u5468\u5f20\u671b\u770b\u65f6\u3002\u4ed6\u8fde\u79fb\u52a8\u51e0\u4e0b,\u6559\u6388\u5468\u56f4\u5927\u96fe\u4e5f\u770b\u4e0d\u89c1\u4e86,\u56db\u5468\u9664\u4e86\"\u79fb\u52a8\"\u4e4b\u5916,\u66f4\u65e0\u4eba\u534a\u70b9\u58f0\u606f\u30023\u20e3\u4ed6\u4e0d\u6562\u518d\u5411\u524d\u8e0f\u51fa,\u4fbf\u5728\u8fd9\u65f6,\u5ffd\u542c\u5f97\u8eab\u540e\u55e4\u55e4\u4e24\u54cd, \u540c\u65f6\u5f20\u671b\u3001\u4e24\u80a1\u5185\u529b\u4ece\u811a\u5e95\u4f20\u5c06\u4e0a\u6765,\u6b65\u5b50\u79bb\u811a\u5e95\u53ea\u4f59\u4e94\u516d\u4e08\u4e4b\u5904\u30024\u20e3 \u9a6c\u514b\u975e\u5e38\u6c14\u607c,\u4e0d\u7531\u5f97\u7eb5\u8eab\u800c\u8d77,\u4e24\u80a1\u8840\u6c34\u4ece\u811a\u5e95\u4e0b\u98de\u8fc7,\u4e00\u53e3\u6c14\u76f4\u5954\u51fa\u53bb\u30025\u20e3 \u5f53\u4e0b\u4ed6\u518d\u4e5f\u4e0d\u6562\u72ec\u81ea\u7559\u5728\u5c71\u8109\u4e4b\u4e0a,\u518d\u4e5f\u4e0d\u613f\u72ec\u81ea\u79bb\u5f00\u3002Source Text(EN)StoryTrans(Ours)for him."}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "StyleTransformer 1\u20e3 \u4e00\u6b21,\u67ef\u91cc\u6559\u6388\u5728\u72ec\u81ea\u6500\u767b\u51ef\u6069\u679c\u5c71\u8109\u7684\u6700\u9ad8\u5cf0\u65f6\u53d1\u73b0,\u5728\u4ed6\u7684\u8eab\u540e\u4e0d\u65f6\u4f20\u51fa\u5de8\u5927\u7684\u811a\u6b65\u58f0,\u4f3c\u4e4e\u6709\u4eba\u5728\u4ee5\u5927\u4ed6\u4e09\u56db\u500d\u7684\u6b65\u4f10\u7d27\u8ddf\u5176\u540e,\u4f3c\u4e4e\u8fd8\u53ef\u4ee5\u542c\u5230\u5de8\u4eba\u5598 \u606f\u7684\u58f0\u97f3\u3002 2\u20e3 \u67ef\u91cc\u4e60\u7acb\u5373\u7ad9\u4f4f\u5de6\u53f3\u5f20\u671b,\u7531\u4e8e\u5927\u96fe\u4ec0\u4e48\u4e5f\u770b\u4e0d\u6e05,\u56db\u5468\u4e5f\u6478\u4e0d\u5230\u4efb\u4f55\u4e1c\u897f\u3002 3\u20e3 \u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb,\u53ef\u4e0e\u6b64\u540c\u65f6,\u90a3\u602a\u5f02\u7684\u811a\u6b65\u547c\u4e5f\u968f\u4e4b\u54cd \u8d77\u3002 4\u20e3 \u67ef\u91cc\u9762\u6559\u6388\u7981\u4e0d\u4f4f\u6bdb\u9aa8\u609a\u7136,\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f,\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u3002 5\u20e3 \u4ece\u90a3\u4ee5\u540e,\u4ed6\u518d\u4e5f\u4e0d\u6562\u72ec\u81ea\u6500\u64d2\u73ed\u9a6c\u514b\u5f8b\u5c71\u4e86\u30021\u20e3 Once, when Professor Curry was climbing the highest peak of the Cairngo Mountains alone, he heard some huge footsteps and panting behind him, like someone following him. 2\u20e3 Professor Curry immediately stopped and looked around, but he couldn't see and touch anything due to the thick fog. 3\u20e3 He had no choice but to take a step forward, but at the same time, the weird footsteps sounded. 4\u20e3 Professor Curry couldn't help being terrified, and ran far far away in one breath. 5\u20e3 Since then, he never dared to climb Ban Makrut Mountain alone.", "figure_data": "Source Text1\u20e3 \u4e00\u6b21\uff0c\u67ef\u91cc\u6559\u6388\u5728\u72ec\u81ea\u6500\u767b\u51ef\u6069\u679c\u5c71\u8109\u7684\u6700\u9ad8\u5cf0\u65f6\u53d1\u73b0\uff0c\u5728\u4ed6\u7684\u8eab\u540e\u4e0d\u65f6\u4f20\u51fa\u5de8\u5927\u7684\u811a\u6b65\u58f0\uff0c\u4f3c\u4e4e\u6709\u4eba\u5728\u4ee5\u5927\u4ed6\u4e09\u56db\u500d\u7684\u6b65\u4f10\u7d27\u8ddf\u5176\u540e\uff0c\u4f3c\u4e4e\u8fd8\u53ef\u4ee5\u542c\u5230 \u5de8\u4eba\u5598\u606f\u7684\u58f0\u97f3\u30022\u20e3 \u67ef\u91cc\u6559\u6388\u7acb\u5373\u7ad9\u4f4f\u5de6\u53f3\u5f20\u671b\uff0c\u7531\u4e8e\u5927\u96fe\u4ec0\u4e48\u4e5f\u770b\u4e0d\u6e05\uff0c\u56db\u5468\u4e5f\u6478\u4e0d\u5230\u4efb\u4f55\u4e1c\u897f\u3002 3\u20e3 \u4ed6\u53ea\u597d\u8fc8\u5f00\u6b65\u5b50\u7ee7\u7eed\u524d\u8fdb\uff0c\u53ef\u4e0e\u6b64\u540c\u65f6\uff0c\u90a3\u602a\u5f02\u7684\u811a\u6b65\u58f0\u4e5f\u968f\u4e4b\u54cd\u8d77\u30024\u20e3 \u67ef\u91cc\u6559\u6388\u7981\u4e0d\u4f4f\u6bdb\u9aa8\u609a\u7136\uff0c\u4e0d\u7531\u81ea\u4e3b\u5730\u6492\u5f00\u4e24\u817f\uff0c\u4e00\u53e3\u6c14\u8dd1\u51fa\u5f88\u8fdc\u5f88\u8fdc\u30025\u20e3 \u4ece\u90a3\u4ee5\u540e\uff0c\u4ed6\u518d\u4e5f\u4e0d\u6562\u72ec\u81ea\u6500\u767b\u73ed\u9a6c\u514b\u5f8b\u5c71\u4e86\u3002Reverser Attention1\u20e3 \u67ef\u9547\u6076\u53ef\u4e0d\u6562\u9020\u6b21\u592a\u8fc7\u8be1\u5f02,\u51dd\u795e\u7559\u610f\u5468\u8eab\u547c\u5438\u4e0e\u4e3e\u52a8,\u5f85\u4ed6\u51d1\u8fd1\u6570\u6b65,\u5168\u8eab\u4fbf\u5982\u731b\u5730\u5927\u4e86\u8d77\u6765,\u53ea\u89c9\u4e24\u811a\u4f3c\u4e4e\u5df2\u7ecf\u78b0\u5230\u4e00\u56e2\u68c9\u82b1,\u7acb\u5373\u4f4e\u5934\u7f29\u4f4f,\u4f3c\u4e4e\u6015\u5c0f\u6c99\u5f25 \u53c8\u91cd\u65b0\u52a8\u8fc7\u3002\u67ef\u9547\u6076\u4f4e\u58f0\u9053:\"\u83ab\u5341\u4fa0,\u90a3\u90aa\u672f\u4f3c\u4e4e\u6709\u4e9b\u4e0d\u5bf9\u3002\"\u90aa\u672f\u6c89\u58f0\u9053:\"\u4e0d\u5bf9,\u662f\u771f\u7684\u3002\u90a3\u8fb9\u7684\u4e1c\u897f,\u4e00\u5b9a\u4e0e\u90a3\u4e1c\u897f\u4e00\u6a21\u4e00\u6837\u3002\"\u67ef\u9547\u6076\u9053:\"\u90a3\u4e48\u5c31\u5bf9\u4e86\u3002\"4\u20e3 \u731b\u5730\u7a9c\u8d77,\u98de\u8eab\u800c\u524d\u3002\u67ef\u9547\u6076\u8eab\u5b50\u5c1a\u672a\u843d\u5730,\u4e00\u811a\u8e0f\u4e0b,\u7acb\u5373\u4ece\u7a97\u53e3\u8dc3\u4e86\u51fa\u53bb,\u4e0e\u90a3\u4e1c\u897f\u4e00\u6a21\u4e00\u6837\u3002\u90a3\u4e1c\u897f\u5e76\u975e\u4e1c\u897f\u4e4b\u751f,\u5374\u662f\u4e1c\u897f\u4e4b\u5f02,\u67ef\u9547\u6076\u4e00\u6014\u4e4b\u4e0b,\u968f\u5373\u7eb5\u8eab\u800c\u8d77,\u7ad9\u5728\u9ad8\u5904\u3002\u90a3\u4e1c\u897f\u7ad9\u5b9a\u811a\u6b65,\u7acb\u5373\u8f6c\u8eab,\u4e00\u53e3\u6c14\u8f6c\u4e0d\u8fc7\u6765,\u7830\u7684\u4e00\u58f0\u5de8\u54cd,\u5728\u67ef\u9547\u6076\u9762\u524d\u54cd\u4e86\u534a\u5929,\u540c\u65f6\u8fdc\u5904\u4f20\u6765\u4e00\u9635\u6c14\u5598\u5406\u559d\u4e4b\u58f0\u3002"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "More Chinese cases generated by baselines, which are transferred from the fairy tale style to the JY style. The number before the sentences indicate their corresponding sentences in the source text in semantics. The underline sentences indicate inserted content to align with target style. The English texts below the Chinese are translated versions of the Chinese samples.", "figure_data": "AuthorsTextsJY\u6768\u8fc7\u5de6\u624b\u62a2\u8fc7\u9a6c\u7f30\uff0c\u53cc\u817f\u4e00\u5939\uff0c\u5c0f\u7ea2\u9a6c\u5411\u524d\u6025\u51b2\uff0c\u7edd\u5c18\u800c\u53bb\u3002\u90ed\u8299\u53ea\u5413\u5f97\u624b\u8db3\u9178\u8f6f\uff0c\u6162\u6162\u8d70\u5230\u5899\u89d2\u62fe\u8d77\u957f\u5251\uff0c\u5251\u8eab\u5728\u5899\u89d2\u4e0a\u731b\u529b \u78b0\u649e\uff0c\u7adf\u5df2\u5f2f\u5f97\u4fbf\u5982\u4e00\u628a\u66f2\u5c3a\u3002\u4ee5\u67d4\u7269\u65bd\u5c55\u521a\u52b2\uff0c\u539f\u662f\u53e4\u5893\u6d3e\u6b66\u529f\u7684\u7cbe\u8981\u6240\u5728\uff0c\u674e\u83ab\u6101\u4fbf\u62c2\u5c18\u3001\u5c0f\u9f99\u5973\u4f7f\u7ef8\u5e26\uff0c\u7686\u662f\u8fd9\u95e8\u529f\u592b\u3002\u6768\u8fc7\u6b64\u65f6\u5185\u52b2\u65e2\u5f3a\uff0c\u8896\u5b50\u4e00\u62c2\uff0c\u5b9e\u4e0d\u4e0b\u4e8e\u94a2\u97ad\u5de8\u6775\u4e4b\u649e\u51fb\u3002\u6768\u8fc7\u62b1\u4e86\u90ed\u8944\uff0c\u9a91\u7740\u6c57\u8840\u5b9d\u9a6c\u5411\u5317\u75be\u9a70\uff0c\u4e0d\u591a\u65f6\u4fbf\u5df2\u63a0\u8fc7\u8944\u9633\uff0c\u5954\u884c\u4e86\u6570\u5341\u91cc\uff0c\u56e0\u6b64\u9ec4\u84c9\u867d\u6500\u4e0a\u6811\u9876\u6781\u76ee\u8fdc\u773a\uff0c\u5374\u77a7\u4e0d\u89c1\u4ed6\u7684\u8e2a\u5f71\u3002"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "ShakespeareKing. Giue them the Foyles yong Osricke, Cousen Hamlet, you know the wager. Ham. Verie well my Lord, Your Grace hath laide the oddes a 'th' weaker side. King. I do not feare it, I haue seene you both: But since he is better'd, we haue therefore oddes. Laer. This is too heauy, Let me see another.", "figure_data": ""}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Samples of different authors in Chinese and English datasets. The English texts below the Chinese are translated versions of the Chinese samples.", "figure_data": ""}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "More ablation study results on Chinese datasets. (-) indicates removing the component in proposed model. ZH-LX/ZH-JY is the Chinese author LuXun/JinYong, respectively.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "x m = (x m 1 , x m 2 , \u2022 \u2022 \u2022 , x m T ).", "formula_coordinates": [3.0, 306.14, 529.7, 117.31, 20.41]}, {"formula_id": "formula_1", "formula_text": "L 1 = L self + \u03bb 1 L dis + \u03bb 2 L sop + \u03bb 3 L style , (1)", "formula_coordinates": [4.0, 80.41, 349.93, 209.46, 20.55]}, {"formula_id": "formula_2", "formula_text": "L 2 = \u2212 T t=1 logP (x t |x <t , {k i } l i=1 , x m ). (2", "formula_coordinates": [4.0, 85.73, 507.94, 199.9, 34.56]}, {"formula_id": "formula_3", "formula_text": ")", "formula_coordinates": [4.0, 285.63, 517.48, 4.24, 13.15]}, {"formula_id": "formula_4", "formula_text": "{z i } n+1 i=1 = MHA(Q = K = V = {s \u2225 {r i } n i=1 }),(3)", "formula_coordinates": [4.0, 306.14, 349.43, 221.9, 27.18]}, {"formula_id": "formula_5", "formula_text": "p i = softmax({z i } n i=1 W z T i ),(4)", "formula_coordinates": [4.0, 351.47, 570.94, 173.68, 20.96]}, {"formula_id": "formula_6", "formula_text": "L self = \u2212 T t=1 logP (x m t |x m <t , {r i } n i=1 , s), (5", "formula_coordinates": [4.0, 319.22, 689.8, 201.69, 34.56]}, {"formula_id": "formula_7", "formula_text": ")", "formula_coordinates": [4.0, 520.91, 699.34, 4.24, 13.15]}, {"formula_id": "formula_8", "formula_text": "L dis = 1 2b b i=1 b j=1 \u2225r i \u2212r j \u2225 2 2 ,(6)", "formula_coordinates": [5.0, 109.47, 229.35, 316.55, 34.7]}, {"formula_id": "formula_9", "formula_text": "r = 1 n n i=1 r i (7", "formula_coordinates": [5.0, 121.78, 267.22, 163.85, 34.7]}, {"formula_id": "formula_10", "formula_text": ")", "formula_coordinates": [5.0, 285.63, 276.76, 4.24, 13.15]}, {"formula_id": "formula_11", "formula_text": "L sop = \u2212 1 n n i=1 o i log(p i ),(8)", "formula_coordinates": [5.0, 121.68, 379.98, 168.19, 34.69]}, {"formula_id": "formula_12", "formula_text": "L style = \u2212Exm \u223cDecoder [logP C (s|x m )], (9)", "formula_coordinates": [5.0, 87.09, 507.26, 202.77, 20.96]}], "doi": "10.18653/v1/P19-1601"}