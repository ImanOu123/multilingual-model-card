{"title": "Safe and Nested Subgame Solving for Imperfect-Information Games *", "authors": "Noam Brown; Tuomas Sandholm", "pub_date": "2017-11-16", "abstract": "In imperfect-information games, the optimal strategy in a subgame may depend on the strategy in other, unreached subgames. Thus a subgame cannot be solved in isolation and must instead consider the strategy for the entire game as a whole, unlike perfect-information games. Nevertheless, it is possible to first approximate a solution for the whole game and then improve it by solving individual subgames. This is referred to as subgame solving. We introduce subgame-solving techniques that outperform prior methods both in theory and practice. We also show how to adapt them, and past subgame-solving techniques, to respond to opponent actions that are outside the original action abstraction; this significantly outperforms the prior state-of-the-art approach, action translation. Finally, we show that subgame solving can be repeated as the game progresses down the game tree, leading to far lower exploitability. These techniques were a key component of Libratus, the first AI to defeat top humans in heads-up no-limit Texas hold'em poker.", "sections": [{"heading": "Introduction", "text": "Imperfect-information games model strategic settings that have hidden information. They have a myriad of applications including negotiation, auctions, cybersecurity, and physical security.\nIn perfect-information games, determining the optimal strategy at a decision point only requires knowledge of the game tree's current node and the remaining game tree beyond that node (the subgame rooted at that node). This fact has been leveraged by nearly every AI for perfect-information games, including AIs that defeated top humans in chess [9] and Go [32]. In checkers, the ability to decompose the game into smaller independent subgames was even used to solve the entire game [30]. However, it is not possible to determine a subgame's optimal strategy in an imperfect-information game using only knowledge of that subgame, because the game tree's exact node is typically unknown. Instead, the optimal strategy may depend on the value an opponent could have received in some other, unreached subgame. Although this is counter-intuitive, we provide a demonstration in Section 2.\nRather than rely on subgame decomposition, past approaches for imperfect-information games typically solved the game as a whole upfront. For example, heads-up limit Texas hold'em, a relatively simple form of poker with 10 13 decision points, was essentially solved without decomposition [2]. However, this approach cannot extend to larger games, such as heads-up no-limit Texas hold'em-the primary benchmark in imperfect-information game solving-which has 10 161 decision points [18].\nThe standard approach to computing strategies in such large games is to first generate an abstraction of the game, which is a smaller version of the game that retains as much as possible the strategic characteristics of the original game [27,29,28]. For example, a continuous action space might be discretized. This abstract game is solved and its solution is used when playing the full game by mapping states in the full game to states in the abstract game. We refer to the solution of an abstraction (or more generally any approximate solution to a game) as a blueprint strategy.\nIn heavily abstracted games, a blueprint may be far from the true solution. Subgame solving attempts to improve upon the blueprint by solving in real time a more fine-grained abstraction for an encountered subgame, while fitting its solution within the overarching blueprint.", "publication_ref": ["b8", "b31", "b29", "b1", "b17", "b26", "b28", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "Coin Toss", "text": "In this section we provide intuition for why an imperfect-information subgame cannot be solved in isolation. We demonstrate this in a simple game we call Coin Toss, shown in Figure 1a, which will be used as a running example throughout the paper.\nCoin Toss is played between players P 1 and P 2 . The figure shows rewards only for P 1 ; P 2 always receives the negation of P 1 's reward. A coin is flipped and lands either Heads or Tails with equal probability, but only P 1 sees the outcome. P 1 then chooses between actions \"Sell\" and \"Play.\" The Sell action leads to a subgame whose details are not important, but the expected value (EV) of choosing the Sell action will be important. (For simplicity, one can equivalently assume in this section that Sell leads to an immediate terminal reward, where the value depends on whether the coin landed Heads or Tails). If the coin lands Heads, it is considered lucky and P 1 receives an EV of $0.50 for choosing Sell. On the other hand, if the coin lands Tails, it is considered unlucky and P 1 receives an EV of \u2212$0.50 for action Sell. (That is, P 1 must on average pay $0.50 to get rid of the coin). If P 1 instead chooses Play, then P 2 may guess how the coin landed. If P 2 guesses correctly, then P 1 receives a reward of \u2212$1. If P 2 guesses incorrectly, then P 1 receives $1. P 2 may also forfeit, which should never be chosen but will be relevant in later sections. We wish to determine the optimal strategy for P 2 in the subgame S that occurs after P 1 chooses Play, shown in Figure 1a. Were P 2 to always guess Heads, P 1 would receive $0.50 for choosing Sell when the coin lands Heads, and $1 for Play when it lands Tails. This would result in an average of $0.75 for P 1 . Alternatively, were P 2 to always guess Tails, P 1 would receive $1 for choosing Play when the coin lands Heads, and \u2212$0.50 for choosing Sell when it lands Tails. This would result in an average reward of $0.25 for P 1 . However, P 2 would do even better by guessing Heads with 25% probability and Tails with 75% probability. In that case, P 1 could only receive $0.50 (on average) by choosing Play when the coin lands Heads-the same value received for choosing Sell. Similarly, P 1 could only receive \u2212$0.50 by choosing Play when the coin lands Tails, which is the same value received for choosing Sell. This would yield an average reward of $0 for P 1 . It is easy to see that this is the best P 2 can do, because P 1 can average $0 by always choosing Sell. Therefore, choosing Heads with 25% probability and Tails with 75% probability is an optimal strategy for P 2 in the \"Play\" subgame. Now suppose the coin is considered lucky if it lands Tails and unlucky if it lands Heads. That is, the expected reward for selling the coin when it lands Heads is now \u2212$0.50 and when it lands Tails is now $0.50. It is easy to see that P 2 's optimal strategy for the \"Play\" subgame is now to guess Heads with 75% probability and Tails with 25% probability. This shows that a player's optimal strategy in a subgame can depend on the strategies and outcomes in other parts of the game. Thus, one cannot solve a subgame using information about that subgame alone. This is the central challenge of imperfect-information games as opposed to perfect-information games.", "publication_ref": [], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "Notation and Background", "text": "This paper focuses on two-player zero-sum games. In a two-player zero-sum extensive-form game there are two players, P = {1, 2}. H is the set of all possible nodes, represented as a sequence of actions. A(h) is the actions available in a node and P (h) \u2208 P \u222a c is the player who acts at that node, where c denotes chance. Chance plays an action a \u2208 A(h) with a fixed probability. If action a \u2208 A(h) leads from h to h , then we write h \u2022 a = h . If a sequence of actions leads from h to h , then we write h h . The set of nodes Z \u2286 H are terminal nodes. For each player i \u2208 P, there is a payoff function\nu i : Z \u2192 where u 1 = \u2212u 2 .\nImperfect information is represented by information sets (infosets). Every node h \u2208 H belongs to exactly one infoset for each player. For any infoset I i , nodes h, h \u2208 I i are indistinguishable to player i. Thus the same player must act at all the nodes in an infoset, and the same actions must be available. Let P (I i ) and A(I i ) be such that all h \u2208 I i , P (I i ) = P (h) and A(I i ) = A(h).\nA strategy \u03c3 i (I i ) is a probability vector over A(I i ) for infosets where P (I i ) = i. The probability of action a is denoted by \u03c3 i (I i , a). For all h \u2208 I i , \u03c3 i (h) = \u03c3 i (I i ). A full-game strategy \u03c3 i \u2208 \u03a3 i defines a strategy for each player i infoset. A strategy profile \u03c3 is a tuple of strategies, one for each player. The expected payoff for player i if all players play the strategy profile \u03c3 i , \u03c3 \u2212i is u i (\u03c3 i , \u03c3 \u2212i ), where \u03c3 \u2212i denotes the strategies in \u03c3 of all players other than i.\nLet \u03c0 \u03c3 (h) = h \u2022a h \u03c3 P (h ) (h , a)\ndenote the probability of reaching h if all players play according to \u03c3. \u03c0 \u03c3 i (h) is the contribution of player i to this probability (that is, the probability of reaching h if chance and all players other than i always chose actions leading to h). \u03c0 \u03c3 \u2212i (h) is the contribution of all players, and chance, other than i. We similarly define \u03c0 \u03c3 (h, h ) is the probability of reaching h given that h has been reached, and 0 if h h . This papers focuses on perfect-recall games, where a player never forgets past information. Thus, for every I i , \u2200h, h \u2208 I i , \u03c0 \u03c3 i (h) = \u03c0 \u03c3 i (h ). We define \u03c0 \u03c3 i (I i ) = \u03c0 \u03c3 i (h) for h \u2208 I i . Also, I i I i if for some h \u2208 I i and some h \u2208 I i , h h. Similarly, [25] is a strategy profile \u03c3 * where no player can improve by shifting to a different strategy, so \u03c3 * satisfies \u2200i, u i (\u03c3 * i , \u03c3\nI i \u2022 a I i if h \u2022 a h. A Nash equilibrium\n* \u2212i ) = max \u03c3 i \u2208\u03a3i u i (\u03c3 i , \u03c3 * \u2212i ). An -Nash equilibrium is a strategy profile \u03c3 * such that \u2200i, u i (\u03c3 * i , \u03c3 * \u2212i ) + \u2265 max \u03c3 i \u2208\u03a3i u i (\u03c3 i , \u03c3 * \u2212i ). A best response BR(\u03c3 \u2212i ) is a strategy for player i that is optimal against \u03c3 \u2212i . Formally, BR(\u03c3 \u2212i ) satisfies u i (BR(\u03c3 \u2212i ), \u03c3 \u2212i ) = max \u03c3 i \u2208\u03a3i u i (\u03c3 i , \u03c3 \u2212i ).\nIn a two-player zero-sum game, the exploitability exp(\u03c3 i ) of a strategy \u03c3 i is how much worse \u03c3 i does against an opponent best response than a Nash equilibrium strategy would do. Formally, exploitability of\n\u03c3 i is u i (\u03c3 * ) \u2212 u i (\u03c3 i , BR(\u03c3 i ))\n, where \u03c3 * is a Nash equilibrium.\nThe expected value of a node h when players play according to \u03c3 is v \u03c3 i (h) = z\u2208Z \u03c0 \u03c3 (h, z)u i (z) . An infoset's value is the weighted average of the values of the nodes in the infoset, where a node is weighed by the player's belief that she is in that node. Formally, v \u03c3 i\n(I i ) = h\u2208I i \u03c0 \u03c3 \u2212i (h)v \u03c3 i (h) h\u2208I i \u03c0 \u03c3 \u2212i (h) and v \u03c3 i (I i , a) = h\u2208I i \u03c0 \u03c3 \u2212i (h)v \u03c3 i (h\u2022a) h\u2208I i \u03c0 \u03c3 \u2212i (h)\n. A counterfactual best response [24] CBR(\u03c3 \u2212i ) is a best response that also maximizes value in unreached infosets. Specifically, a counterfactual best response is a best response \u03c3 i with the additional condition that if \u03c3 i (I i , a) > 0 then v \u03c3 i (I i , a) = max a v \u03c3 i (I i , a ). We further define counterfactual best response value CBV \u03c3\u2212i (I i ) as the value player i expects to achieve by playing according to CBR(\u03c3 \u2212i ), having already reached infoset I i . Formally,\nCBV \u03c3\u2212i (I i ) = v CBR(\u03c3\u2212i),\u03c3\u2212i i (I i ) and CBV \u03c3\u2212i (I i , a) = v CBR(\u03c3\u2212i),\u03c3\u2212i i (I i , a).\nAn imperfect-information subgame, which we refer to simply as a subgame in this paper, can in most cases (but not all) be described as including all nodes which share prior public actions (that is, actions viewable to both players). In poker, for example, a subgame is uniquely defined by a sequence of bets and public board cards. Figure 1b shows the public game tree of Coin Toss. Formally, an imperfect-information subgame is a set of nodes S \u2286 H such that for all h \u2208 S, if h h , then h \u2208 S, and for all h \u2208 S and all i \u2208 P, if h \u2208 I i (h) then h \u2208 S. Define S top as the set of earliest-reachable nodes in S. That is, h \u2208 S top if h \u2208 S and h \u2208 S for any h h.", "publication_ref": ["b24", "b23"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Prior Approaches to Subgame Solving", "text": "This section reviews prior techniques for subgame solving in imperfect-information games, which we build upon. Throughout this section, we refer to the Coin Toss game shown in Figure 1a.\nAs discussed in Section 1, a standard approach to dealing with large imperfect-information games is to solve an abstraction of the game. The abstract solution is a (probably suboptimal) strategy profile in the full game. We refer to this full-game strategy profile as the blueprint. The goal of subgame solving is to improve upon the blueprint by changing the strategy only in a subgame. While the blueprint is frequently a Nash equilibrium (or approximate Nash equilibrium) in some abstraction of the full game, our techniques do not assume this. The blueprint can in fact be any arbitrary strategy in the full game.\nFigure 2: The blueprint we refer to in the game of Coin Toss. The Sell action leads to a subgame that is not displayed. Probabilities are shown for all actions. The dotted line means the two P2 nodes share an infoset. The EV of each P1 action is also shown.\nAssume that a blueprint \u03c3 (shown in Figure 2) has already been computed for Coin Toss in which P 1 chooses Play 3 4 of the time with Heads and 1 2 of the time with Tails, and P 2 chooses Heads 1 2 of the time, Tails 1 4 of the time, and Forfeit 1 4 of the time after P 1 chooses Play. 2 The details of the blueprint in the Sell subgame are not relevant in this section, but the EV for choosing the Sell action is relevant. We assume that if P 1 chose the Sell action and played optimally thereafter, then she would receive an expected payoff of 0.5 if the coin is Heads, and \u22120.5 if the coin is Tails. We will attempt to improve P 2 's strategy in the subgame S that follows P 1 choosing Play.", "publication_ref": ["b1"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Unsafe Subgame Solving", "text": "We first review the most intuitive form of subgame solving, which we refer to as Unsafe subgame solving [1,14,15,12]. This form of subgame solving assumes both players played according to the blueprint prior to reaching the subgame. That defines a probability distribution over the nodes at the root of the subgame S, representing the probability that the true game state matches that node. A strategy for the subgame is then calculated which assumes that this distribution is correct.\nIn all subgame solving algorithms, an augmented subgame containing S and a few additional nodes is solved to determine the strategy for S. Applying Unsafe subgame solving to the blueprint in Coin Toss (after P 1 chooses Play) means solving the augmented subgame shown in Figure 3a. Specifically, the augmented subgame consists of only an initial chance node and S. The initial chance node reaches h \u2208 S top with probability \u03c0 \u03c3 (h) h \u2208Stop \u03c0 \u03c3 (h ) . The augmented subgame is solved and its strategy for P 2 is used in S rather than the blueprint strategy. Unsafe subgame solving lacks theoretical solution quality guarantees and there are many situations where it performs extremely poorly. Indeed, if it were applied to the blueprint of Coin Toss then P 2 would always choose Heads-which P 1 could exploit severely by only choosing Play with Tails. Despite the lack of theoretical guarantees and potentially bad performance, Unsafe subgame solving is simple and can sometimes produce low-exploitability strategies, as we show later.\nWe now move to discussing safe subgame-solving techniques, that is, ones that ensure that the exploitability of the strategy is no higher than that of the blueprint strategy.", "publication_ref": ["b0", "b13", "b14", "b11"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Subgame Resolving", "text": "In subgame Resolving [8], a safe strategy is computed for P 2 in the subgame by solving the augmented subgame shown in Figure 3b, producing an equilibrium strategy \u03c3 S . This augmented subgame differs from Unsafe subgame solving by giving P 1 the option to \"opt out\" from entering S and instead receive the EV of playing optimally against P 2 's blueprint strategy in S.\nSpecifically, the augmented subgame for Resolving differs from unsafe subgame solving as follows. For each h top \u2208 S top we insert a new P 1 node h r , which exists only in the augmented subgame, between the initial chance node and h top . The set of these h r nodes is S r . The initial chance node connects to each node h r \u2208 S r in proportion to the probability that player P 1 could reach h top if P 1 tried to do so (that is, in proportion to \u03c0 \u03c3 \u22121 (h top )). At each node h r \u2208 S r , P 1 has two possible actions. Action a S leads to h top , while action a T leads to a terminal payoff that awards the value of playing optimally against P 2 's blueprint strategy, which is CBV \u03c32 (I 1 (h top )). In the blueprint of Coin Toss, P 1 choosing Play after the coin lands Heads results in an EV of 0, and 1 2 if the coin is Tails. Therefore, a T leads to a terminal payoff of 0 for Heads and 1 2 for Tails. After the equilibrium strategy \u03c3 S is computed in the augmented subgame, P 2 plays according to the computed subgame strategy \u03c3 S 2 rather than the blueprint strategy when in S. The P 1 strategy \u03c3 S 1 is not used. Clearly P 1 cannot do worse than always picking action a T (which awards the highest EV P 1 could achieve against P 2 's blueprint). But P 1 also cannot do better than always picking a T , because P 2 could simply play according to the blueprint in S, which means action a S would give the same EV to P 1 as action a T (if P 1 played optimally in S). In this way, the strategy for P 2 in S is pressured to be no worse than that of the blueprint. In Coin Toss, if P 2 were to always choose Heads (as was the case in Unsafe subgame solving), then P 1 would always choose a T with Heads and a S with Tails.\nResolving guarantees that P 2 's exploitability will be no higher than the blueprint's (and may be better). However, it may miss opportunities for improvement. For example, if we apply Resolving to the example blueprint in Coin Toss, one solution to the augmented subgame is the blueprint itself, so P 2 may choose Forfeit 25% of the time even though Heads and Tails dominate that action. Indeed, the original purpose of Resolving was not to improve upon a blueprint strategy in a subgame, but rather to compactly store it by keeping only the EV at the root of the subgame and then reconstructing the strategy in real time when needed rather than storing the whole subgame strategy. Maxmargin subgame solving [24], discussed in Appendix A, can improve performance by defining a margin M \u03c3 S (I 1 ) = CBV \u03c32 (I 1 ) \u2212 CBV \u03c3 S 2 (I 1 ) for each I 1 \u2208 S top and maximizing min I1\u2208Stop M \u03c3 S (I 1 ). Resolving only makes all margins nonnegative. However, Maxmargin does worse in practice when using estimates of equilibrium values as discussed in Section 6.", "publication_ref": ["b7", "b23"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Reach Subgame Solving", "text": "All of the subgame-solving techniques described in Section 4 only consider the target subgame in isolation, which can lead to suboptimal strategies. For example, Maxmargin solving applied to S in Coin Toss results in P 2 choosing Heads with probability 5 8 and Tails with 3 8 in S. This results in P 1 receiving an EV of \u2212 1 4 by choosing Play in the Heads state, and an EV of 1 4 in the Tails state. However, P 1 could simply always choose Sell in the Heads state (earning an EV of 0.5) and Play in the Tails state and receive an EV of 3 8 for the entire game. In this section we introduce Reach subgame solving, an improvement to past subgame-solving techniques that considers what the opponent could have alternatively received from other subgames. 3 For example, a better strategy for P 2 would be to choose Heads with probability 3 4 and Tails with probability 1 4 . Then P 1 is indifferent between choosing Sell and Play in both cases and overall receives an expected payoff of 0 for the whole game.\nHowever, that strategy is only optimal if P 1 would indeed achieve an EV of 0.5 for choosing Sell in the Heads state and \u22120.5 in the Tails state. That would be the case if P 2 played according to the blueprint in the Sell subgame (which is not shown), but in reality we would apply subgame solving to the Sell subgame if the Sell action were taken, which would change P 2 's strategy there and therefore P 1 's EVs. Applying subgame solving to any subgame encountered during play is equivalent to applying it to all subgames independently. Thus, we must consider that the EVs from other subgames may differ from what the blueprint says because subgame solving would be applied to them as well. nodes whose outcomes are seen by both P1 and P2. Right: An augmented subgame for one of the subgames according to Reach subgame solving. If only one of the subgames is being solved, then the alternative payoff for Heads can be at most 1. However, if both are solved independently, then the gift must be split among the subgames and must sum to at most 1. For example, the alternative payoff in both subgames can be 0.5.\nAs an example of this issue, consider the game shown in Figure 4 which contains two identical subgames S 1 and S 2 where the blueprint has P 2 pick Heads and Tails with 50% probability. The Sell action leads to an EV of 0.5 from the Heads state, while Play leads to an EV of 0. If we were to solve just S 1 , then P 2 could afford to always choose Tails in S 1 , thereby letting P 1 achieve an EV of 1 for reaching that subgame from Heads because, due to the chance node C 1 , S 1 is only reached with 50% probability. Thus, P 1 's EV for choosing Play would be 0.5 from Heads and \u22120.5 from Tails, which is optimal. We can achieve this strategy in S 1 by solving an augmented subgame in which the alternative payoff for Heads is 1. In that augmented subgame, P 2 always choosing Tails would be a solution (though not the only solution).\nHowever, if the same reasoning were applied independently to S 2 as well, then P 2 might always choose Tails in both subgames and P 1 's EV for choosing Play from Heads would become 1 while the EV for Sell would only be 0.5. Instead, we could allow P 1 to achieve an EV of 0.5 for reaching each subgame from Heads (by setting the alternative payoff for Heads to 0.5). In that case, P 1 's overall EV for choosing Play could only increase to 0.5, even if both S 1 and S 2 were solved independently.\nWe capture this intuition by considering for each I 1 \u2208 S top all the infosets and actions I 1 \u2022 a I 1 that P 1 would have taken along the path to I 1 . If, at some I 1 \u2022 a I 1 where P 1 acted, there was a different action a * \u2208 A(I 1 ) that leads to a higher EV, then P 1 would have taken a suboptimal action if they reached I 1 . The difference in value between a * and a is referred to as a gift. We can afford to let P 1 's value for I 1 increase beyond the blueprint value (and in the process lower P 1 's value in some other infoset in S top ), so long as the increase to I 1 's value is small enough that choosing actions leading to I 1 is still suboptimal for P 1 . Critically, we must ensure that the increase in value is small enough even when the potential increase across all subgames is summed together, as in Figure 4. 4 A complicating factor is that gifts we assumed were present may actually not exist. For example, in Coin Toss, suppose applying subgame solving to the Sell subgame results in P 1 's value for Sell from the Heads state decreasing from 0.5 to 0.25. If we independently solve the Play subgame, we have no way of knowing that P 1 's value for Sell is lower than the blueprint suggested, so we may still assume there is a gift of 0.5 from the Heads state based on the blueprint. Thus, in order to guarantee a theoretical result on exploitability that is as strong as possible, we use in our theory and experiments a lower bound on what gifts could be after subgame solving was applied to all other subgames. Formally, let \u03c3 2 be a P 2 blueprint and let \u03c3 \u2212S 2 be the P 2 strategy that results from applying subgame solving independently to a set of disjoint subgames other than S. Since we do not want to compute \u03c3 \u2212S 2 in order to apply subgame solving to S, let g\n\u03c3 \u2212S 2 (I 1 , a ) be a lower bound of CBV \u03c3 \u2212S 2 (I 1 ) \u2212 CBV \u03c3 \u2212S 2 (I 1 , a ) that does not require knowledge of \u03c3 \u2212S 2 .\nIn our experiments we use g\n\u03c3 \u2212S 2 (I 1 , a ) = max a\u2208Az(I 1 )\u222a{a } CBV \u03c32 (I 1 , a) \u2212 CBV \u03c32 (I 1 , a ) where A z (I 1 ) \u2286 A(I 1 )\nis the set of actions leading immediately to terminal nodes. Reach subgame solving modifies the augmented subgame in Resolving and Maxmargin by increasing the alternative payoff for infoset\nI 1 \u2208 S top by I 1 \u2022a I1|P (I 1 )=P1 g \u03c3 \u2212S 2 (I 1 , a )\n. Formally, we define a reach margin as\nM \u03c3 S r (I 1 ) = M \u03c3 S (I 1 ) + I 1 \u2022a I1|P (I 1 )=P1 g \u03c3 \u2212S 2 (I 1 , a )(1)\nThis margin is larger than or equal to the one for Maxmargin, because g \u03c3 \u2212S 2 (I , a ) is nonnegative. We refer to the improved algorithms as Reach-Resolve and Reach-Maxmargin.\nIntuitively, the alternative payoff in an augmented subgame determines how important it is that P 2 \"defend\" against that P 1 infoset. If the alternative payoff is increased, then P 1 is more likely to choose the alternative payoff rather than enter the subgame, so P 2 can instead focus on lowering the value of other P 1 infosets in S top .\nUsing a lower bound on gifts is not necessary to guarantee safety. So long as we use a gift value g \u03c3 (I 1 , a ) \u2264 CBV \u03c32 (I 1 ) \u2212 CBV \u03c32 (I 1 , a ), the resulting strategy will be safe. However, using a lower bound further guarantees a reduction to exploitability when a P 1 best response reaches with positive probability an infoset I 1 \u2208 S top that has positive margin, as proven in Theorem 1. In practice, it may be best to use an accurate estimate of gifts. One option is to use\u011d\n\u03c3 \u2212S 2 (I 1 , a ) = CBV \u03c32 (I 1 ) \u2212CBV \u03c32 (I 1 , a ) in place of g \u03c3 \u2212S 2 (I 1 , a ) , whereCBV\u03c32\nis the closest P 1 can get to the value of a counterfactual best response while P 1 is constrained to playing within the abstraction that generated the blueprint. Using estimates is covered in more detail in Section 6.\nTheorem 1 shows that when subgames are solved independently and using lower bounds on gifts, Reach-Maxmargin solving has exploitability lower than or equal to past safe techniques. The theorem statement is similar to that of Maxmargin [24], but the margins are now larger (or equal) in size. Theorem 1. Given a strategy \u03c3 2 in a two-player zero-sum game, a set of disjoint subgames S, and a strategy \u03c3 S 2 for each subgame S \u2208 S produced via Reach-Maxmargin solving using lower bounds for gifts, let \u03c3 2 be the strategy that plays according to \u03c3 S 2 for each subgame S \u2208 S, and \u03c3 2 elsewhere. Moreover, let \u03c3 \u2212S 2 be the strategy that plays according to \u03c3 2 everywhere except for P 2 nodes in S, where it instead plays according to \u03c3 2 . If \u03c0\nBR(\u03c3 2 ) 1 (I 1 ) > 0 for some I 1 \u2208 S top , then exp(\u03c3 2 ) \u2264 exp(\u03c3 \u2212S 2 ) \u2212 h\u2208I1 \u03c0 \u03c32 \u22121 (h)M \u03c3 S r (I 1 ).\nSo far the described techniques have guaranteed a reduction in exploitability over the blueprint by setting the value of a T equal to the value of P 1 playing optimally to P 2 's blueprint. Relaxing this guarantee by instead setting the value of a T equal to an estimate of P 1 's value when both players play optimally leads to far lower exploitability in practice. We discuss this approach in the next section.", "publication_ref": ["b2", "b2", "b2", "b3", "b23"], "figure_ref": ["fig_2", "fig_2"], "table_ref": []}, {"heading": "Estimates for Alternative Payoffs", "text": "In this section we consider the case where we have a good estimate of what the values of subgames would look like in a Nash equilibrium. Unlike previous sections, exploitability might be higher than the blueprint when using this method; the solution quality ultimately depends on the accuracy of the estimates used. In practice this approach leads to significantly lower exploitability.\nWhen solving multiple P 2 subgames, there is a minimally-exploitable strategy \u03c3 * 2 that could, in theory, be computed by changing only the strategies in the subgames. (\u03c3 * 2 may not be a Nash equilibrium because P 2 's strategy outside the subgames is fixed, but it is the closest that can be achieved by changing the strategy only in the subgames). However, \u03c3 * 2 can only be guaranteed to be produced by solving all the subgames together, because the optimal strategy in one subgame depends on the optimal strategy in other subgames.\nStill, suppose that we know CBV \u03c3 * 2 (I 1 ) for every infoset I 1 \u2208 S top for every subgame S. Let I r,1 be the infoset in S r that leads to I 1 . By setting the P 1 alternative payoff for I r,1 to v(I r,1 , a T ) = CBV \u03c3 * 2 (I 1 ), safe subgame solving guarantees a strategy will be produced with exploitability no worse than \u03c3 * 2 . Thus, achieving a strategy equivalent to \u03c3 * 2 does not require knowledge of \u03c3 * 2 ; rather, it only requires knowledge of CBV \u03c3 * 2 (I 1 ) for infosets I 1 in the top of the subgames.\nWhile we do not know CBV \u03c3 * 2 (I 1 ) exactly without knowing \u03c3 * 2 itself, we may nevertheless be able to produce (or learn) good estimates of CBV \u03c3 * 2 (I 1 ). For example, in Section 8 we compute the solution to the game of No-Limit Flop Hold'em (NLFH), and find that in perfect play P 2 can expect to win about 37 mbb/h 5 (that is, if P 1 plays perfectly against the computed P 2 strategy, then P 1 earns \u221237; if P 2 plays perfectly against the computed P 1 strategy, then P 2 earns 37). An abstraction of the game which is only 0.02% of the size of the full game produces a P 1 strategy that can be beaten by 112 mbb/h, and a P 2 strategy that can be beaten by 21 mbb/h. Still, the abstract strategy estimates that at equilibrium, P 2 can expect to win 35 mbb/h. So even though the abstraction produces a very poor estimate of the strategy \u03c3 * , it produces a good estimate of the value of \u03c3 * . In our experiments, we estimate CBV \u03c3 * 2 (I 1 ) by calculating a P 1 counterfactual best response within the abstract game to P 2 's blueprint. We refer to this strategy asCBR(\u03c3 2 ) and its value in an infoset I 1 asCBV \u03c32 (I 1 ).\nWe then useCBV \u03c32 (I 1 ) as the alternative payoff of I 1 in an augmented subgame. In other words, rather than calculate a P 1 counterfactual best response in the full game to P 2 's blueprint strategy (which would be CBR(\u03c3 2 )), we instead calculate P 1 's counterfactual best response where P 1 is constrained by the abstraction.\nIf the blueprint was produced by conducting T iterations of CFR in an abstract game, then one could instead simply use the final iteration's strategy \u03c3 T 1 , as this converges to a counterfactual best response within the abstract game. This is what we use in our experiments in this paper.\nTheorem 2 proves that if we use estimates of CBV \u03c3 * 2 (I 1 ) as the alternative payoffs in Maxmargin subgame solving, then we can bound exploitability by the distance of the estimates from the true values. This is in contrast to the previous algorithms which guaranteed exploitability no worse than the blueprint.\nTheorem 2. Let S be a set of disjoint subgames being solved in a game with no private actions. Let \u03c3 be a blueprint and let \u03c3 * 2 be a minimally-exploitable P 2 strategy that differs from \u03c3 2 only in S. Let \u2206 = max S\u2208S,I1\u2208Stop |CBV \u03c3 * 2 (I 1 ) \u2212 CBV \u03c32 (I 1 )|. Applying Maxmargin solving to each subgame using \u03c3 as the blueprint produces a P 2 strategy with exploitability no higher than exp(\u03c3 *\n2 ) + 2\u2206.\nUsing estimates of the values of \u03c3 * tends to be do better than the theoretically safe options described in Section 4. 6 Although Theorem 2 uses Maxmargin in the proof, in practice Resolve does far better with estimates than Maxmargin. Additionally, the theorem easily extends to Reach-Maxmargin as well, and Reach-Resolve does better than Resolve regardless of whether estimates are used.\nSection B.1 discusses an improvement, which we refer to as Distributional alternative payoffs, that leads to even better performance by making the algorithm more robust to errors in the blueprint estimates.", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}, {"heading": "Nested Subgame Solving", "text": "As we have discussed, large games must be abstracted to reduce the game to a tractable size. This is particularly common in games with large or continuous action spaces. Typically the action space is discretized by action abstraction so that only a few actions are included in the abstraction. While we might limit ourselves to the actions we included in the abstraction, an opponent might choose actions that are not in the abstraction. In that case, the off-tree action can be mapped to an action that is in the abstraction, and the strategy from that in-abstraction action can be used. For example, in an auction game we might include a bid of $100 in our abstraction. If a player bids $101, we simply treat that as a bid of $100. This is referred to as action translation [16,31,10]. Action translation is the state-of-the-art prior approach to dealing with this issue. It has been used, for example, by all the leading competitors in the Annual Computer Poker Competition (ACPC).\nIn this section, we develop techniques for applying subgame solving to calculate responses to opponent off-tree actions, thereby obviating the need for action translation. That is, rather than simply treat a bid of $101 as $100, we calculate in real time a unique response to the bid of $101. This can also be done in a nested fashion in response to subsequent opponent off-tree actions. We present two methods that dramatically outperform the leading action translation technique. Additionally, these techniques can be used to solve finer-grained models as play progresses down the game tree. For exposition, we assume that P 2 wishes to respond to P 1 choosing an off-tree action.\nWe refer to the first method as the inexpensive method. 7 When P 1 chooses an off-tree action a, a subgame S is generated following that action such that for any infoset I 1 that P 1 might be in, I 1 \u2022 a \u2208 S top . This subgame may itself be an abstraction. A solution \u03c3 S is computed via subgame solving, and \u03c3 S is combined with \u03c3 to form a new blueprint \u03c3 in the expanded abstraction that now includes action a. The process repeats whenever P 1 again chooses an off-tree action.\nTo conduct safe subgame solving in response to off-tree action a, we could calculate CBV \u03c32 (I 1 , a) by defining, via action translation, a P 2 blueprint following a and best responding to it [5]. However, that could be computationally expensive and would likely perform poorly in practice because, as we show later, action translation is highly exploitable. Instead, we relax the guarantee of safety and useCBV \u03c32 (I 1 ) for the alternative payoff, whereCBV \u03c32 (I 1 ) is the value in I 1 of P 1 playing as close to optimal as possible while constrained to playing in the blueprint abstraction (which excludes action a). In this case, exploitability depends on how wellCBV \u03c32 (I 1 ) approximates CBV \u03c3 * 2 (I 1 ), where \u03c3 * 2 is an optimal P 2 strategy (see Section 6). 8 In general, we find that only a small number of near-optimal actions need to be included in the blueprint abstraction forCBV \u03c32 (I 1 ) to be close to CBV \u03c3 * 2 (I 1 ). We can then approximate a near-optimal response to any opponent action. This is particularly useful in very large or continuous action spaces.\nThe \"inexpensive\" approach cannot be combined with Unsafe subgame solving because the probability of reaching an action outside of a player's abstraction is undefined. Nevertheless, a similar approach 6 It is also possible to combine the safety of past approaches with some of the better performance of using estimates by adding the original Resolve conditions as additional constraints. 7 Following our study, the AI DeepStack used a technique similar to this form of nested subgame solving [23]. 8 We estimate\nCBV \u03c3 * 2 (I1) rather than CBV \u03c3 * 2 (I1, a) because CBV \u03c3 * 2 (I1) \u2212 CBV \u03c3 * 2 (I1, a\n) is a gift that may be added to the alternative payoff anyway. is possible with Unsafe subgame solving (as well as all the other subgame-solving techniques) by starting the subgame solving at h rather than at h \u2022 a. In other words, if action a taken in node h is not in the abstraction, then Unsafe subgame solving is conducted in the smallest subgame containing h (and action a is added to that abstraction). This increases the size of the subgame compared to the inexpensive method because a strategy must be recomputed for every action a \u2208 A(h) in addition to a. For example, if an off-tree action is chosen by the opponent as the first action in the game, then the strategy for the entire game must be recomputed. We therefore call this method the expensive method. We present experiments with both methods.", "publication_ref": ["b15", "b30", "b9", "b6", "b4", "b7", "b5", "b6", "b22", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "Our experiments were conducted on heads-up no-limit Texas hold'em, as well as two smaller-scale poker games we call No-Limit Flop Hold'em (NLFH) and No-Limit Turn Hold'em (NLTH). The description for these games can be found in Appendix E. For equilibrium finding, we used CFR+ [33].\nOur first experiment compares the performance of the subgame-solving techniques when applied to information abstraction (which is card abstraction in the case of poker). Specifically, we solve NLFH with no information abstraction on the preflop. On the flop, there are 1,286,792 infosets for each betting sequence; the abstraction buckets them into 200, 2,000, or 30,000 abstract ones (using a leading information abstraction algorithm [11]). We then apply subgame solving immediately after the flop community cards are dealt.\nWe experiment with two versions of the game, one small and one large, which include only a few of the available actions in each infoset. We also experimented on abstractions of NLTH. In that case, we solve NLTH with no information abstraction on the preflop or flop. On the turn, there are 55,190,538 infosets for each betting sequence; the abstraction buckets them into 200, 2,000, or 20,000 abstract ones. We apply subgame solving immediately after the turn community card is dealt. Tables 1, 2, and 3 show the performance of each technique. In all our experiments, exploitability is measured in the standard units used in this field: milli big blinds per hand (mbb/h). Since subgame solving begins immediately after a chance node with an extremely high branching factor (1, 755 in NLFH), the gifts for the Reach algorithms are divided among subgames inefficiently. Many subgames do not use the gifts at all, while others could make use of more. The result is that the theoretically safe version of Reach allocates gifts very conservatively. In the experiments we show results both for the theoretically safe splitting of gifts, as well as a more aggressive version where gifts are scaled up by the branching factor of the chance node (1,755). This weakens the theoretical guarantees of the algorithm, but in general did better than splitting gifts in a theoretically correct manner. However, this is not universally true. Appendix D shows that in at least one case, exploitability increased when gifts were scaled up too aggressively. In all cases, using Reach subgame solving in at least the theoretical safe method led to lower exploitability.", "publication_ref": ["b32", "b10", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Small Flop", "text": "Despite lacking theoretical guarantees, Unsafe subgame solving did surprisingly well in most games. However, it did substantially worse in Large NLFH with 30,000 buckets. This exemplifies its variability. Among the safe methods, all of the changes we introduce show improvement over past techniques. The Reach-Estimate + Distributional algorithm generally resulted in the lowest exploitability among the various choices, and in most cases beat Unsafe subgame solving.\nIn all but one case, using estimated values lowered exploitability more than Maxmargin and Resolve subgame solving. Also, in all but one case using distributional alternative payoffs lowered exploitability.\nThe second experiment evaluates nested subgame solving, and compares it to action translation. In order to also evaluate action translation, in this experiment, we create an NLFH game that includes 3 bet sizes at every point in the game tree (0.5, 0.75, and 1.0 times the size of the pot); a player can also decide not to bet. Only one bet (i.e., no raises) is allowed on the preflop, and three bets are allowed on the flop. There is no information abstraction anywhere in the game. We also created a second, smaller abstraction of the game in which there is still no information abstraction, but the 0.75\u00d7 pot bet is never available. We calculate the exploitability of one player using the smaller abstraction, while the other player uses the larger abstraction. Whenever the large-abstraction player chooses a 0.75\u00d7 pot bet, the small-abstraction player generates and solves a subgame for the remainder of the game (which again does not include any subsequent 0.75\u00d7 pot bets) using the nested subgame-solving techniques described above. This subgame strategy is then used as long as the large-abstraction player plays within the small abstraction, but if she chooses the 0.75\u00d7 pot bet again later, then the subgame solving is used again, and so on.\nTable 4 shows that all the subgame-solving techniques substantially outperform action translation. Resolve, Maxmargin, and Reach-Maxmargin use inexpensive nested subgame solving, while Unsafe and \"Reach-Maxmargin (expensive)\" use the expensive approach. In all cases, we used estimates for the alternative payoff as described in Section 7. We did not test distributional alternative payoffs in this experiment, since the calculated best response values are likely quite accurate. Reach-Maxmargin performed the best, outperforming Maxmargin and Unsafe subgame solving. These results suggest that nested subgame solving is preferable to action translation (if there is sufficient time to solve the subgame).  We used the techniques presented in this paper in our AI Libratus, which competed against four top human specialists in heads-up no-limit Texas hold'em in the January 2017 Brains vs. AI competition.\nLibratus was constructed by first solving an abstraction of the game via a new variant of Monte Carlo CFR [21] that prunes negative-regret actions [4,6,7]. Libratus applied nested subgame solving (solved with CFR+ [33]) upon reaching the third betting round, and in response to every subsequent opponent bet thereafter. This allowed Libratus to avoid information abstraction during play, and leverage nested subgame solving's far lower exploitability in response to opponent off-tree actions.\nNo-limit Texas hold'em is the most popular form of poker in the world and has been the primary benchmark challenge for AI in imperfect-information games. The competition was played over the course of 20 days, and involved 120,000 hands of poker. A prize pool of $200,000 was split among the four humans based on their performance against the AI to incentivize strong play. The AI decisively defeated the team of human players by a margin of 147 mbb / hand, with 99.98 statistical significance (see Figure 5). This was the first, and so far only, time an AI defeated top humans in no-limit poker. ", "publication_ref": ["b20", "b3", "b5", "b6", "b32"], "figure_ref": ["fig_3"], "table_ref": ["tab_2"]}, {"heading": "Conclusion", "text": "We introduced a subgame-solving technique for imperfect-information games that has stronger theoretical guarantees and better practical performance than prior subgame-solving methods. We presented results on exploitability of both safe and unsafe subgame-solving techniques. We also introduced a method for nested subgame solving in response to the opponent's off-tree actions, and demonstrated that this leads to dramatically better performance than the usual approach of action translation. This is, to our knowledge, the first time that exploitability of subgame-solving techniques has been measured in large games.\nFinally, we demonstrated the effectiveness of these techniques in practice in heads-up no-limit Texas Hold'em poker, the main benchmark challenge for AI in imperfect-information games. We developed the first AI to reach the milestone of defeating top humans in heads-up no-limit Texas Hold'em. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix: Supplementary Material A Maxmargin Solving", "text": "Maxmargin solving [24] is similar to Resolving, except that it seeks to improve P 2 's strategy in the subgame strategy as much as possible. While Resolving seeks a strategy for P 2 in S that would simply dissuade P 1 from entering S, Maxmargin solving additionally seeks to punish P 1 as much as possible if P 1 nevertheless chooses to enter S. A subgame margin is defined for each infoset in S r , which represents the difference in value between entering the subgame versus choosing the alternative payoff. Specifically, for each infoset I 1 \u2208 S top , the subgame margin is\nM \u03c3 S (I 1 ) = CBV \u03c32 (I 1 ) \u2212 CBV \u03c3 S 2 (I 1 )(2)\nIn Maxmargin solving, a Nash equilibrium \u03c3 S for the augmented subgame described in Resolving subgame solving is computed such that the minimum margin over all I 1 \u2208 S top is maximized. Aside from maximizing the minimum margin, the augmented subgames used in Resolving and Maxmargin solving are identical.\nGiven our base strategy in Coin Toss, Maxmargin solving would result in P 2 choosing Heads with probability 5 8 , Tails with probability 3 8 , and Forfeit with probability 0. The augmented subgame can be solved in a way that maximizes the minimum margin by using a standard LP solver. In order to use iterative algorithms such as the Excessive Gap Technique [26,13,20] or Counterfactual Regret Minimization (CFR) [35], one can use the gadget game described by Moravcik et al. [24]. Details on the gadget game are provided in the Appendix. Our experiments used CFR.\nMaxmargin solving is safe. Furthermore, it guarantees that if every Player 1 best response reaches the subgame with positive probability through some infoset(s) that have positive margin, then exploitability is strictly lower than that of the blueprint strategy. While the theoretical guarantees are stronger, Maxmargin may lead to worse practical performance relative to Resolving when combined with the techniques discussed in Section 6, due to Maxmargin's greater tendency to overfit to assumptions in the model.", "publication_ref": ["b23", "b25", "b12", "b19", "b34", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "B Description of Gadget Game", "text": "Solving the augmented subgame described in Maxmargin solving and Reach-Maxmargin solving will not, by itself, necessarily maximize the minimum margin. While LP solvers can easily handle this objective, the process is more difficult for iterative algorithms such as Counterfactual Regret Minimization (CFR) and the Excessive Gap Technique (EGT). For these iterative algorithms, the augmented subgame can be modified into a gadget game that, when solved, will provide a Nash equilibrium to the augmented subgame and will also maximize the minimum margin [24]. This gadget game is unnecessary when using distributional alternative payoffs, which is introduced in section B.1.\nThe gadget game differs from the augmented subgame in two ways. First, all P 1 payoffs that are reached from the initial infoset of I 1 \u2208 S r are shifted by the alternative payoff of I 1 , and there is longer an alternative payoff. Second, rather than the game starting with a chance node that determines P 1 's starting infoset, P 1 decides for herself which infoset to begin the game in. Specifically, the game begins with a P 1 node where each action in the node corresponds to an infoset I 1 in S r . After P 1 chooses to enter an infoset I 1 , chance chooses the precise node h \u2208 I 1 in proportion to \u03c0 \u03c3 \u22121 (h). By shifting all payoffs in the game by the size of the alternative payoff, the gadget game forces P 1 to focus on improving the performance of each infoset over some baseline, which is the goal of Maxmargin and Reach-Maxmargin solving. Moreover, by allowing P 1 to choose the infoset in which to enter the game, the gadget game forces P 2 to focus on maximizing the minimum margin. Figure 6 illustrates the gadget game used in Maxmargin and Reach-Maxmargin. Figure 6: An example of a gadget game in Maxmargin refinement. P 1 picks the initial infoset she wishes to enter S r in. Chance then picks the particular node of the infoset, and play then proceeds identically to the augmented subgame, except all P 1 payoffs are shifted by the size of the alternative payoff and the alternative payoff is then removed from the augmented subgame.", "publication_ref": ["b23"], "figure_ref": [], "table_ref": []}, {"heading": "B.1 Distributional Alternative Payoffs", "text": "One problem with existing safe subgame-solving techniques is that they may \"overfit\" to the alternative payoffs, even when we use estimates. Consider for instance a subgame with two different P 1 infosets I 1 and I 1 at the top. Assume P 1 's value for I 1 is estimated to be 1, and for I 1 is 10. Now suppose during subgame solving, P 2 has a choice between two different strategies. The first sets P 1 's value in the subgame for I 1 to 0.99 and for I 1 to 9.99. The second slightly increases P 1 's value for the subgame for I 1 to 1.01 but dramatically lowers the value for I 1 to 0. The safe subgame-solving methods described so far would choose the first strategy, because the second strategy leaves one of the margins negative. However, intuitively, the second strategy is likely the better option, because it is more robust to errors in the model. For example, perhaps we are not confident that 10 is the exact value, but instead believe its true value is normally distributed with 10 as the mean and a standard deviation of 1. In this case, we would prefer the strategy that lowers the value for I 1 to 0.\nTo address this problem, we introduce a way to incorporate the modeling uncertainty into the game itself. Specifically, we introduce a new augmented subgame that makes subgame solving more robust to errors in the model. This augmented subgame changes the augmented subgame used in subgame Resolving (shown in Figure 3b) so that the alternative payoffs are random variables, and P 1 is informed at the start of the augmented subgame of the values drawn from the random variables (but P 2 is not). The augmented subgame is otherwise identical. A visualization of this change is shown in Figure 7. As the distributions of the random variables narrow, the augmented subgame converges to the Resolve augmented subgame (but still maximizes the minimum margin when all margins are positive). As the distributions widen, P 2 seeks to maximize the sum over all margins, regardless of which are positive or negative. This modification makes the augmented subgame infinite in size because the random variables may be real-valued and P 1 could have a unique strategy for each outcome of the random variable. Fortunately, the special structure of the game allows us to arrive at a P 2 Nash equilibrium strategy for this infinite-sized augmented subgame by solving a much simpler gadget game.\nThe gadget game is identical to the augmented subgame used in Resolve subgame solving (shown in Figure 3b), except at each initial P 1 infoset I r,1 \u2208 S r , P 1 chooses action a S (that is, chooses to enter the subgame rather than take the alternative payoff) with probability P X I1 \u2264 v(I r,1 , a S ) , where v(I r,1 , a S ) is the expected value of action a S . (When solving via CFR, it is the expected value on each iteration, as described in CFR-BR [19]). This leads to Theorem 3, which proves that solving this simplified gadget game produces a P 2 strategy that is a Nash equilibrium in the infinite-sized augmented subgame illustrated in Figure 7. Theorem 3. Let S be a Resolve augmented subgame and S r its root. Let S be a Distributional augmented subgame similar to S , except at each infoset I r,1 \u2208 S r , P 1 observes the outcome of a random variable X I1 and the alternative payoff is equal to that outcome. If CFR is used to solve S except that the action leading to S is taken from each I r,1 \u2208 S r with probability P X I1 \u2264 v t (I r,1 , a S ) , where v t (I r,1 , a S ) is the value on iteration t of action a S , then the resulting P 2 strategy \u03c3 S 2 in S is a P 2 Nash equilibrium strategy in S.\nAnother option which also solves the game but has better empirical performance relies on the softmax (also known as Hedge) algorithm [22]. This gadget game is more complicated, and is described in detail in Appendix C. We use the softmax gadget game in our experiments.\nThe correct distribution to use for the random variables ultimately depends on the actual unknown errors in the model. In our experiments for this technique, we set X I1 \u223c N \u00b5 I1 , s 2 I1 , where \u00b5 I is the blueprint value (plus any gifts). s I1 is set as the difference between the blueprint value of I 1 , and the true (that is, unabstracted) counterfactual best response value of I 1 . Our experiments show that this heuristic works well, and future research could yield even better options.", "publication_ref": ["b18", "b21"], "figure_ref": ["fig_1", "fig_4", "fig_1", "fig_4"], "table_ref": []}, {"heading": "C Hedge for Distributional Subgame Solving", "text": "In this paper we use CFR [35] with Hedge in S r , which allows us to leverage a useful property of the Hedge algorithm [22] to update all the infosets resulting from outcomes of X I1 simultaneously. 9 When using Hedge, action a S in infoset I r,1 in the augmented subgame is chosen on iteration t with probability e \u03b7 tv (I r,1 ,a S ) e \u03b7 tv (I r,1 ,a S ) +e \u03b7 tv (I r,1 ,a T ) . Wherev(I r,1 , a T ) is the observed expected value of action a T , v(I r,1 , a S ) is the observed expected value of action a S , and \u03b7 t is a tuning parameter. Since, action a S leads to identical play by both players for all outcomes of X,v(I r,1 , a S ) is identical for all outcomes of X. Moreover,v(I r,1 , a T ) is simply the outcome of X I1 . So the probability that a S is taken across all infosets on iteration t is \u221e \u2212\u221e e \u03b7tv(Ir,1,a S ) e \u03b7tv(Ir,1,a S ) + e \u03b7tx f X I 1 (x)dx\n(3)\nwhere f X I 1 (x) is the pdf of X I1 . In other words, if CFR is used to solve the augmented subgame, then the game being solved is identical to Figure 3b except that action a S is always chosen in infoset I 1 on iteration t with probability given by (3). In our experiments, we set the Hedge tuning parameter \u03b7 as suggested in [3]:\n\u03b7 t = \u221a ln(|A(I1)|) 3 \u221a V AR(I1)t \u221a t\n, where V AR(I 1 ) t is the observed variance in the payoffs 9 Another option is to apply CFR-BR [19] only at the initial P1 nodes when deciding between a T and a S .\nthe infoset has received across all iterations up to t. In the subgame that follows S r , we use CFR+ as the solving algorithm.", "publication_ref": ["b34", "b21", "b8", "b2", "b2", "b8", "b18"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "D Scaling of Gifts", "text": "To retain the theoretical guarantees of Reach subgame solving, one must ensure that the gifts assigned to reachable subgames do not (in aggregate) exceed the original gift. That is, if g(I 1 ) is a gift at infoset I 1 , we must ensure that CBV \u03c3 * 2 (I 1 ) \u2264 CBV \u03c32 (I 1 ) + g(I 1 ). In this paper we accomplish this by increasing the margin of an infoset I 1 , where I 1 I 1 , by at most g(I 1 ). However, empirical performance may improve if the increase to margins due to gifts is scaled up by some factor. In most games we experimented on, exploitability decreased the further the gifts were scaled. However, Figure 8 shows one case in which we observe the exploitability increasing when the gifts are scaled up too far. The graph shows exploitability when the gifts are scaled by various factors. At 0, the algorithm is identical to Maxmargin. at 1, the algorithm is the theoretically correct form of Reach-Maxmargin. Optimal performance in this game occurs when the gifts are scaled by a factor of about 1, 000. Scaling the gifts by 100, 000 leads to performance that is worse than Maxmargin subgame solving. This empirically demonstrates that while scaling up gifts may lead to better performance in some cases (because an entire gift is unlikely to be used in every subgame that receives one), it may also lead to far worse performance in some cases. ", "publication_ref": [], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "E Rules for Poker Variants", "text": "Our experiments are conducted on heads-up no-limit Texas hold'em (HUNL), as well as smaller-scale variants we call no-limit flop hold'em (NLFH) and no-limit turn hold'em (NLTH). We begin by describing the rules of HUNL.\nIn the form of HUNL discussed in this paper, each player starts a hand with $20,000. One player is designated P 1 , while the other is P 2 . This assignment alternates between hands. HUNL consists of four rounds of betting. On a round of betting, each player can choose to either fold, call, or raise. If a player folds, that player immediately surrenders the pot to the opponent and the game ends. If a player calls, that players places a number of chips in the pot equal to the opponent's contribution. If a player raises, that player adds more chips to the pot than the opponent's contribution. A round of betting ends after a player calls. Players can continue to go back and forth with raises in a round until one of them runs out of chips.\nIf either player chooses to raise first in a round, they must raise a minimum of $100. If a player raises after another player has raised, that raise must be greater than or equal to the last raise. The maximum amount for a bet or raise is the remainder of that player's chip stack, which in our model is $20,000 at the beginning of a game.\nAt the start of HUNL, both players receive two private cards from a standard 52-card deck. P 1 must place a big blind of $100 in the pot, while P 2 must place a small blind of $50 in the pot. There is then a round of betting (the preflop), starting with P 2 . When the round ends, three community cards are dealt face up between the players. There is then another round of betting (the flop), starting with P 1 this time. After the round of betting completes, another community card is dealt face up, and another round of betting commences starting with P 1 (the turn). Finally, one more community card is dealt face up, and a final betting round occurs (the river), again starting with P 1 . If neither player folds before the final betting round completes, the player with the best five-card poker hand, constructed from their two private cards and the five face-up community cards, wins the pot. In the case of a tie, the pot is split evenly.\nNLTH is similar to no-limit Texas hold'em except there are only three rounds of betting (the preflop, flop, and turn) in which there are two options for bet sizes. There are also only four community cards. NLFH is similar except there are only two rounds of betting (the preflop and flop), and three community cards.\nWe experiment with two versions of NLFH, one small and one large, which include only a few of the available actions in each infoset. The small game requires 1.1 GB to store the unabstracted strategy as double-precision floats. The large game requires 4 GB. NLTH requires 35 GB to store the unabstracted strategy.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F Proof of Theorem 1", "text": "Proof. Assume M \u03c3 S r (I 1 ) \u2265 0 for every infoset I 1 and assume \u03c0\nBR(\u03c3 2 ) 1 (I * 1 ) > 0 for some I * 1 \u2208 S top and let = M r (I * 1 ). Define \u03c0 \u03c3 \u22121 (I 1 ) = h\u2208I1 \u03c0 \u03c3 \u22121 (h) and define \u03c0 \u03c3 \u22121 (I 1 , I 1 ) = h\u2208I1,h \u2208I 1 \u03c0 \u03c3 \u22121 (h, h\n). We show that for every P 1 infoset I 1 I * 1 where P (I 1 ) = P 1 ,\nCBV \u03c3 2 (I 1 ) \u2264 CBV \u03c3 \u2212S 2 (I 1 )+ I 1 \u2022a I1|P (I 1 )=P1 CBV \u03c3 \u2212S 2 (I 1 ) \u2212 CBV \u03c3 \u2212S 2 (I 1 , a ) \u2212 h\u2208I1,h * \u2208I * 1 \u03c0 \u03c32 \u22121 (h, h * ) (4)\nBy the definition of M \u03c3 S r (I * 1 ) this holds for I * 1 itself. Moreover, the condition holds for every other I 1 \u2208 S top , because by assumption every margin is nonnegative and \u03c0 \u03c32 \u22121 (I 1 , I * 1 ) = 0 for any I 1 \u2208 S top where I 1 = I * 1 . The condition also clearly holds for any I 1 with no descendants in S because then \u03c0 \u03c32 \u22121 (I 1 , I * 1 ) = 0 and \u03c3 2 (h) = \u03c3 \u2212S 2 (h) in all P 2 nodes following I 1 . This satisfies the base step. We now move on to the inductive step.\nLet Succ(I 1 , a) be the set of earliest-reachable P 1 infosets following I 1 such that P (I 1 ) = P 1 for I \u2208 Succ(I 1 , a). Formally, I 1 \u2208 Succ(I 1 , a) if P (I 1 ) = P 1 and I 1 \u2022 a I 1 and for any other I 1 \u2208 Succ(I 1 , a), I 1 I 1 . Then\nCBV \u03c3 2 (I 1 , a) = CBV \u03c3 \u2212S 2 (I 1 , a)+ I 1 \u2208Succ(I1,a) \u03c0 \u03c3 2 \u22121 (I 1 , I 1 )(CBV \u03c3 2 (I 1 ) \u2212 CBV \u03c3 \u2212S 2 (I 1 )) (5)\nAssume that every I 1 \u2208 Succ(I 1 , a) satisfies (4). Then\nCBV \u03c3 2 (I 1 , a) \u2264 CBV \u03c3 \u2212S 2 (I 1 , a) \u2212 \u03c0 \u03c32 \u22121 (I 1 , I * 1 ) + I 1 \u2208Succ(I1,a) \u03c0 \u03c32 \u22121 (I 1 , I 1 ) I 1 \u2022a I 1 |P (I 1 )=P1 CBV \u03c3 \u2212S 2 (I 1 ) \u2212 CBV \u03c3 \u2212S 2 (I 1 , a ) CBV \u03c3 2 (I 1 , a) \u2264 CBV \u03c3 \u2212S 2 (I 1 ) \u2212 CBV \u03c3 \u2212S 2 (I 1 ) \u2212 CBV \u03c3 \u2212S 2 (I 1 , a) \u2212 \u03c0 \u03c32 \u22121 (I 1 , I * 1 ) + I 1 \u2208Succ(I1,a) \u03c0 \u03c32 \u22121 (I 1 , I 1 ) I 1 \u2022a I 1 |P (I 1 )=P1 CBV \u03c3 \u2212S 2 (I 1 ) \u2212 CBV \u03c3 \u2212S 2 (I 1 , a ) Since CBV \u03c3 \u2212S 2 (I 1 ) \u2212 CBV \u03c3 \u2212S 2 (I 1 , a) \u2264 CBV \u03c3 \u2212S 2 (I 1 ) \u2212 CBV \u03c3 \u2212S 2\n(I 1 , a 1 ) so we get\nCBV \u03c3 2 (I 1 , a) \u2264 CBV \u03c3 \u2212S 2 (I 1 ) \u2212 (CBV \u03c3 \u2212S 2 (I 1 ) \u2212 CBV \u03c3 \u2212S 2 (I 1 , a) \u2212 \u03c0 \u03c32 \u22121 (I 1 , I * 1 ) + I 1 \u2208Succ(I1,a) \u03c0 \u03c32 \u22121 (I 1 , I 1 ) I 1 \u2022a I 1 |P (I 1 )=P1 CBV \u03c3 \u2212S 2 (I 1 ) \u2212 CBV \u03c3 \u2212S 2 (I 1 , a ) CBV \u03c3 2 (I 1 , a) \u2264 CBV \u03c3 \u2212S 2 (I 1 ) \u2212 \u03c0 \u03c32 \u22121 (I 1 , I * 1 ) + I 1 \u2208Succ(I1,a) \u03c0 \u03c32 \u22121 (I 1 , I 1 ) I 1 \u2022a I1|P (I 1 )=P1 CBV \u03c3 \u2212S 2 (I 1 ) \u2212 CBV \u03c3 \u2212S 2 (I 1 , a ) CBV \u03c3 2 (I 1 , a 1 ) \u2264 CBV \u03c3 \u2212S 2 (I 1 )\u2212\u03c0 \u03c32 \u22121 (I 1 , I * 1 ) + I 1 \u2022a I1|P (I 1 )=P1 CBV \u03c3 \u2212S 2 (I 1 )\u2212CBV \u03c3 \u2212S 2 (I 1 , a 1 )\nSince \u03c0 BR(\u03c3 2 ) 1\n(I * 1 ) > 0, and action a leads to I * 1 , so by definition of a best response, CBV \u03c3 2 (I 1 , a) = CBV \u03c3 2 (I 1 ). Thus,\nCBV \u03c3 2 (I 1 ) \u2264 CBV \u03c3 \u2212S 2 (I 1 )\u2212\u03c0 \u03c32 \u22121 (I 1 , I * 1 ) + I 1 \u2022a I1|P (I 1 )=P1 CBV \u03c3 \u2212S 2 (I 1 )\u2212CBV \u03c3 \u2212S 2 (I 1 , a )\nwhich satisfies the inductive step.\nApplying this reasoning to the root of the entire game, we arrive at exp(\u03c3 2 ) \u2264 exp(\u03c3 \u2212S 2 ) \u2212 \u03c0 \u03c32 \u22121 (I * 1 ) .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Theorem 2", "text": "Proof. Without loss of generality, we assume that it is player P 2 who conducts subgame solving. We define a node h in a subgame S as earliest-reachable if there does not exist a node h \u2208 S such that h \u227a h. For each earliest-reachable node h \u2208 S, let h r be its parent and a S be the action leading to h such that h r \u2022 a S = h. We require h r to be a P 1 node; if it is not, then we can simply insert a P 1 node with only a single action between h r and h. Let S r be the set of all h r for S.\nApplying subgame solving to subgames as they are reached during play is equivalent to applying subgame solving to every subgame before play begins, so we can phrase what follows in the context of all subgames being solved before play begins. Let \u03c3 2 be the P 2 strategy produced after subgame solving is applied to every subgame. We show inductively that for any P 1 infoset I 1 \u2208 S where it is P 1 's turn to move (i.e., P (I 1 ) = P 1 ), the counterfactual best response values for P 1 satisfy\nCBV \u03c3 2 (I 1 ) \u2264 CBV \u03c3 * 2 (I 1 ) + 2\u2206(6)\nDefine Succ(I 1 , a) as the set of infosets belonging to P 1 that follow action a in I 1 and where it is P 1 's turn and where P 1 has not had a turn since a, as well as terminal nodes follow action a in I 1 without P 1 getting a turn. Formally, a terminal node z \u2208 Z is in Succ(I 1 , a) if there exists a history h \u2208 I 1 such that h \u2022 a z and there does not exist a history h such that P (h ) = P 1 and h \u2022 a h \u227a z. Additionally, an infoset I 1 belonging to P 1 is in Succ(I 1 , a) if P (I 1 ) = P 1 and I 1 \u2022 a I 1 and there does not exist an earlier infoset I 1 belonging to P 1 such that P (I 1 ) = P 1 and I \u2022 a I 1 \u227a I 1 . Define Succ(I 1 ) as \u222a a\u2208A(I1) Succ(I 1 , a). Similarly, we define Succ(h, a) as the set of histories belonging to P (h), or terminals, that follow action a and where P (h) has not had a turn since a. Formally, h \u2208 Succ(h, a) if either P (h ) = P (h) or P (h ) \u2208 Z and h \u2022 a h and there does not exist a history h such that P (h ) = P (h) and h \u2022 a h \u227a h . Now we define a level L for each P 1 infoset where it is P 1 's turn and the infoset is not in the set of subgames S.\n\u2022 For immediate parents of subgames we define the level to be zero: for all I 1 \u2208 S r for any subgame S \u2208 S, L(I 1 ) = 0.\n\u2022 For infoset that are not ancestors of subgames, we define the level to be zero: L(I 1 ) = 0 for any infoset I 1 that is not an ancestor of a subgame in S.\n\u2022 For all other infosets, the level is one greater than the greatest level of its successors: L(I 1 ) = + 1 where = max I 1 \u2208Succ(I1) L(I 1 ) where L(z) = 0 for terminal nodes z.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Base case of induction", "text": "First consider infosets I 1 \u2208 S r for some subgame S \u2208 S. We define M \u03c3 2 (I 1 ) = v \u03c3 (I 1 , a S ) \u2212 CBV \u03c3 2 (I 1 , a S ). Consider a subgame S \u2208 S. Estimated-Maxmargin subgame solving arrives at a strategy \u03c3 2 such that min I1\u2208Sr M \u03c3 2 (I 1 ) is maximized. By the assumption in the theorem statement, |v \u03c3 (I 1 , a S ) \u2212 CBV \u03c3 * 2 (I 1 , a S )| \u2264 \u2206 for all I 1 \u2208 S r . Thus, \u03c3 * 2 satisfies min I1\u2208Sr M \u03c3 * 2 (I 1 ) \u2265 \u2212\u2206 and therefore min I1\u2208Sr M \u03c3 2 (I 1 ) \u2265 \u2212\u2206, because Estimated-Maxmargin subgame solving could, at least, arrive at \u03c3 2 = \u03c3 * 2 . From the definition of M \u03c3 2 (I 1 ), this implies that for all I 1 \u2208 S r , CBV \u03c3 2 (I 1 , a S ) \u2264 v \u03c3 (I 1 , a S ) + \u2206. Since by assumption v \u03c3 (I 1 , a S ) \u2264 CBV \u03c3 * 2 (I 1 , a S ) + \u2206, this gives us CBV \u03c3 2 (I 1 , a S ) \u2264 CBV \u03c3 * 2 (I 1 , a S ) + 2\u2206. Now consider infosets I 1 that are not ancestors of any subgame in S. By definition, for all h such that h I 1 or I 1 h, and P (h) = P 2 , \u03c3 * 2 (I 2 (h)) = \u03c3 2 (I 2 (h)) = \u03c3 2 (I 2 (h)). Therefore, CBV \u03c3 2 (I 1 ) = CBV \u03c3 * 2 (I 1 ). So, we have shown that (6) holds for any I 1 such that L(I 1 ) = 0.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Inductive step", "text": "Now assume that (6) holds for any P 1 infoset I 1 where P (I 1 ) = P 1 and I 1 \u2208 S and L(I 1 ) \u2264 . Consider an I 1 such that P (I 1 ) = P 1 and I 1 \u2208 S and L(I 1 ) = + 1.\nFrom the definition of CBV \u03c3 2 (I 1 , a), we have that for any action a \u2208 A(I 1 ),\nCBV \u03c3 2 (I 1 , a) = h\u2208I1 \u03c0 \u03c3 2 \u22121 (h) v CBR(\u03c3 2 ),\u03c3 2 (h \u2022 a) / h\u2208I1 \u03c0 \u03c3 2 \u22121 (h)(7)\nSince for any h \u2208 I 1 there is no P 1 action between a and reaching any h \u2208 Succ(h, a), so \u03c0 \u03c3 2 1 (h \u2022 a, h ) = 1. Thus,\nCBV \u03c3 2 (I 1 , a) = h\u2208I1 \u03c0 \u03c3 2 \u22121 (h) h \u2208Succ(h,a) \u03c0 \u03c3 2 \u22121 (h, h ) v CBR(\u03c3 2 ),\u03c3 2 (h ) / h\u2208I1 \u03c0 \u03c3 2 \u22121 (h) (8\n)\nCBV \u03c3 2 (I 1 , a) = h\u2208I1 h \u2208Succ(h,a) \u03c0 \u03c3 2 \u22121 (h ) v CBR(\u03c3 2 ),\u03c3 2 (h ) / h\u2208I1 \u03c0 \u03c3 2 \u22121 (h)(9)\nSince the game is perfect recall, h\u2208I1 h \u2208Succ(h,a) f (h ) = I 1 \u2208Succ(I1,a) h \u2208I 1 f (h ) for any function f . Thus,\nCBV \u03c3 2 (I 1 , a) = I 1 \u2208Succ(I1,a) h \u2208I 1 \u03c0 \u03c3 2 \u22121 (h ) v CBR(\u03c3 2 ),\u03c3 2 (h ) / h\u2208I1 \u03c0 \u03c3 2 \u22121 (h)(10)\nFrom the definition of CBV \u03c3 2 (I 1 ) we get CBV \u03c3 2 (I 1 , a) = I 1 \u2208Succ(I1,a)\nCBV \u03c3 2 (I 1 )\nh \u2208I 1 \u03c0 \u03c3 2 \u22121 (h ) / h\u2208I1 \u03c0 \u03c3 2 \u22121 (h)(11)\nSince ( 6) holds for all I 1 \u2208 Succ(I 1 , a), so CBV \u03c3 2 (I 1 , a) \u2264 I 1 \u2208Succ(I1,a)\n(CBV \u03c3 * 2 (I 1 ) + 2\u2206)\nh \u2208I 1 \u03c0 \u03c3 2 \u22121 (h ) / h\u2208I1 \u03c0 \u03c3 2 \u22121 (h)(12)\nSince P 2 's strategy is fixed according to \u03c3 2 outside of S, so for all I 1 \u2208 S, \u03c0 \u03c3 \u22121 (I 1 ) = \u03c0 \u03c3 \u22121 (I 1 ) = \u03c0 \u03c3 * \u22121 (I 1 ). Therefore, CBV \u03c3 2 (I 1 , a) \u2264 I 1 \u2208Succ(I1,a)\n(CBV \u03c3 * 2 (I 1 ) + 2\u2206)\nh \u2208I 1 \u03c0 \u03c3 * 2 \u22121 (h ) / h\u2208I1 \u03c0 \u03c3 * 2 \u22121 (h)(13)\nPulling out the 2\u2206 constant and applying equation ( 11) for CBV \u03c3 * 2 (I 1 , a) we get CBV \u03c3 2 (I 1 , a) \u2264 CBV \u03c3 * (I 1 , a) + 2\u2206\nI 1 \u2208Succ(I1,a) h \u2208I 1 \u03c0 \u03c3 * 2 \u22121 (h ) / h\u2208I1 \u03c0 \u03c3 * 2 \u22121 (h)(14)\nSince \nThus, ( 6) holds for I 1 as well and the inductive step is satisfied. Extending (6) to the root of the game, we see that exp(\u03c3 2 ) \u2264 exp(\u03c3 * 2 ) + 2\u2206.", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}, {"heading": "G Proof of Theorem 3", "text": "Proof. We prove inductively that using CFR in S while choosing the action leading to S from each I 1 \u2208 S r with probability P X I1 \u2264 v t (I 1 , a S ) results in play that is identical to CFR in S and CFR-BR [19] in S r , which converges to a Nash equilibrium.\nFor each P 2 infoset I 2 in S where P (I 2 ) = P 2 , there is exactly one corresponding infoset I 2 in S that is reached via the same actions, ignoring random variables. Each P 1 infoset I 1 in S where P (I 1 ) = P 1 corresponds to a set of infosets in S that are reached via the same actions, where the elements in the set differ only by the outcome of the random variables. We prove that on each iteration, the instantaneous regret for these corresponding infosets is identical (and therefore the average strategy played in the P 2 infosets over all iterations is identical).\nAt the start of the first iteration of CFR, all regrets are zero. Therefore, the base case is trivially true. Now assume that on iteration t, regrets are identical for all corresponding infosets. Then the strategies played on iteration t in S are identical as well.\nFirst, consider an infoset I 1 in S and a corresponding infoset I 1 in S. Since the remaining structure of the game is identical beyond I 1 and I 1 , and because P 2 's strategies are identical in all P 2 infosets encountered, so the immediate regret for I 1 and I 1 is identical as well.\nNext, consider a P 1 infoset I 1,x in S r in which the random variable X I1 has an observed value of x. Let the corresponding P 1 infoset in S r be I 1 . Since CFR-BR is played in this infoset, and since action a T leads to a payoff of x, so P 1 will choose action a S with probability 1 if x \u2265 a T and with probability 0 otherwise. Thus, for all infosets in S r corresponding to I 1 , action a S is chosen with probability P X I1 \u2264 v(I 1 , a S ) .\nFinally, consider a P 2 infoset I 2 in S and its corresponding infoset I 2 in S . Since in both cases action a T is taken in S r with probability P X I1 \u2264 v(I 1 , a S ) , and because P 1 plays identically between corresponding infosets in S and S , and because the structure of the game is otherwise identical, so the immediate regret for I 1 and I 1 is identical as well.", "publication_ref": ["b18"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Approximating game-theoretic optimal strategies for fullscale poker", "journal": "", "year": "2003", "authors": "Darse Billings; Neil Burch; Aaron Davidson; Robert Holte; Jonathan Schaeffer; Terence Schauenberg; Duane Szafron"}, {"ref_id": "b1", "title": "Heads-up limit hold'em poker is solved", "journal": "Science", "year": "2015-01", "authors": "Michael Bowling; Neil Burch; Michael Johanson; Oskari Tammelin"}, {"ref_id": "b2", "title": "Dynamic thresholding and pruning for regret minimization", "journal": "", "year": "2017", "authors": "Noam Brown; Christian Kroer; Tuomas Sandholm"}, {"ref_id": "b3", "title": "Regret-based pruning in extensive-form games", "journal": "", "year": "2015", "authors": "Noam Brown; Tuomas Sandholm"}, {"ref_id": "b4", "title": "Simultaneous abstraction and equilibrium finding in games", "journal": "", "year": "2015", "authors": "Noam Brown; Tuomas Sandholm"}, {"ref_id": "b5", "title": "Baby Tartanian8: Winning agent from the 2016 annual computer poker competition", "journal": "", "year": "2016", "authors": "Noam Brown; Tuomas Sandholm"}, {"ref_id": "b6", "title": "Reduced space and faster convergence in imperfectinformation games via regret-based pruning", "journal": "", "year": "2016", "authors": "Noam Brown; Tuomas Sandholm"}, {"ref_id": "b7", "title": "Solving imperfect information games using decomposition", "journal": "", "year": "2014", "authors": "Neil Burch; Michael Johanson; Michael Bowling"}, {"ref_id": "b8", "title": "", "journal": "Deep Blue. Artificial intelligence", "year": "2002", "authors": "Murray Campbell; Joseph Hoane; Feng-Hsiung Hsu"}, {"ref_id": "b9", "title": "Action translation in extensive-form games with large action spaces: axioms, paradoxes, and the pseudo-harmonic mapping", "journal": "AAAI Press", "year": "2013", "authors": "Sam Ganzfried; Tuomas Sandholm"}, {"ref_id": "b10", "title": "Potential-aware imperfect-recall abstraction with earth mover's distance in imperfect-information games", "journal": "", "year": "2014", "authors": "Sam Ganzfried; Tuomas Sandholm"}, {"ref_id": "b11", "title": "Endgame solving in large imperfect-information games", "journal": "", "year": "2015", "authors": "Sam Ganzfried; Tuomas Sandholm"}, {"ref_id": "b12", "title": "First-order algorithm with O(ln(1/ )) convergence for -equilibrium in two-person zero-sum games", "journal": "Mathematical Programming", "year": "2012", "authors": "Andrew Gilpin; Javier Pe\u00f1a; Tuomas Sandholm"}, {"ref_id": "b13", "title": "A competitive Texas Hold'em poker player via automated abstraction and real-time equilibrium computation", "journal": "", "year": "2006", "authors": "Andrew Gilpin; Tuomas Sandholm"}, {"ref_id": "b14", "title": "Better automated abstraction techniques for imperfect information games, with application to Texas Hold'em poker", "journal": "", "year": "2007", "authors": "Andrew Gilpin; Tuomas Sandholm"}, {"ref_id": "b15", "title": "A heads-up no-limit texas hold'em poker player: discretized betting models and automatically generated equilibriumfinding programs", "journal": "", "year": "2008", "authors": "Andrew Gilpin; Tuomas Sandholm; Troels Bjerre S\u00f8rensen"}, {"ref_id": "b16", "title": "A time and space efficient algorithm for approximately solving large imperfect information games", "journal": "", "year": "2014", "authors": "Eric Jackson"}, {"ref_id": "b17", "title": "Measuring the size of large no-limit poker games", "journal": "", "year": "2013", "authors": "Michael Johanson"}, {"ref_id": "b18", "title": "Finding optimal abstract strategies in extensive-form games", "journal": "AAAI Press", "year": "2012", "authors": "Michael Johanson; Nolan Bard; Neil Burch; Michael Bowling"}, {"ref_id": "b19", "title": "Theoretical and practical advances on smoothing for extensive-form games", "journal": "", "year": "2017", "authors": "Christian Kroer; Kevin Waugh; Fatma K\u0131l\u0131n\u00e7-Karzan; Tuomas Sandholm"}, {"ref_id": "b20", "title": "Monte Carlo sampling for regret minimization in extensive games", "journal": "", "year": "2009", "authors": "Marc Lanctot; Kevin Waugh; Martin Zinkevich; Michael Bowling"}, {"ref_id": "b21", "title": "The weighted majority algorithm", "journal": "Information and Computation", "year": "1994", "authors": "Nick Littlestone; M K Warmuth"}, {"ref_id": "b22", "title": "Deepstack: Expert-level artificial intelligence in heads-up no-limit poker", "journal": "", "year": "2017", "authors": "Matej Morav\u010d\u00edk; Martin Schmid; Neil Burch; Viliam Lis\u00fd; Dustin Morrill; Nolan Bard; Trevor Davis; Kevin Waugh; Michael Johanson; Michael Bowling"}, {"ref_id": "b23", "title": "Refining subgames in large imperfect information games", "journal": "", "year": "2016", "authors": "Matej Moravcik; Martin Schmid; Karel Ha; Milan Hladik; Stephen Gaukrodger"}, {"ref_id": "b24", "title": "Equilibrium points in n-person games", "journal": "Proceedings of the National Academy of Sciences", "year": "1950", "authors": "John Nash"}, {"ref_id": "b25", "title": "Excessive gap technique in nonsmooth convex minimization", "journal": "SIAM Journal of Optimization", "year": "2005", "authors": "Yurii Nesterov"}, {"ref_id": "b26", "title": "The state of solving large incomplete-information games, and application to poker", "journal": "", "year": "2010", "authors": "Tuomas Sandholm"}, {"ref_id": "b27", "title": "Abstraction for solving large incomplete-information games", "journal": "", "year": "2015", "authors": "Tuomas Sandholm"}, {"ref_id": "b28", "title": "Solving imperfect-information games", "journal": "Science", "year": "2015", "authors": "Tuomas Sandholm"}, {"ref_id": "b29", "title": "", "journal": "Checkers is solved. Science", "year": "2007", "authors": "Jonathan Schaeffer; Neil Burch; Yngvi Bj\u00f6rnsson; Akihiro Kishimoto; Martin M\u00fcller; Robert Lake; Paul Lu; Steve Sutphen"}, {"ref_id": "b30", "title": "Probabilistic state translation in extensive games with large action sets", "journal": "", "year": "2009", "authors": "David Schnizlein; Michael Bowling; Duane Szafron"}, {"ref_id": "b31", "title": "Mastering the game of go with deep neural networks and tree search", "journal": "Nature", "year": "2016", "authors": "David Silver; Aja Huang; Chris J Maddison; Arthur Guez; Laurent Sifre; George Van Den; Julian Driessche; Ioannis Schrittwieser; Veda Antonoglou; Marc Panneershelvam;  Lanctot"}, {"ref_id": "b32", "title": "Solving heads-up limit texas hold'em", "journal": "", "year": "2015", "authors": "Oskari Tammelin; Neil Burch; Michael Johanson; Michael Bowling"}, {"ref_id": "b33", "title": "Strategy grafting in extensive games", "journal": "", "year": "2009", "authors": "Kevin Waugh; Nolan Bard; Michael Bowling"}, {"ref_id": "b34", "title": "Regret minimization in games with incomplete information", "journal": "", "year": "2007", "authors": "Martin Zinkevich; Michael Johanson; H Michael; Carmelo Bowling;  Piccione"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: (a) The example game of Coin Toss. \"C\" represents a chance node. S is a Player 2 (P2) subgame. The dotted line between the two P2 nodes means that P2 cannot distinguish between them. (b) The public game tree of Coin Toss. The two outcomes of the coin flip are only observed by P1.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: The augmented subgames solved to find a P2 strategy in the Play subgame of Coin Toss.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Left: A modified game of Coin Toss with two subgames. The nodes C1 and C2 are public chance", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure 5: Libratus's performance over the course of the 2017 Brains vs AI competition.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 7 :7Figure 7: A visualization of the change in the augmented subgame from Figure 3b when using distributional alternative payoffs.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 8 :8Figure 8: Exploitability in Flop Texas Hold'em of Reach-Maxmargin as we scale up the size of gifts.", "figure_data": ""}, {"figure_label": "112121", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "I 1 1 \u03c0 \u03c3 * 2 \u2212 1 (h ) = h\u2208I1 \u03c0 \u03c3 * 2 \u2212 1112121\u2208Succ(I1,a) h \u2208I (h) we arrive atCBV \u03c3 2 (I 1 , a) \u2264 CBV \u03c3 * (I 1 , a) + 2\u2206", "figure_data": ""}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Exploitability (evaluated in the game with no information abstraction) of subgame-solving in small flop Texas hold'em. Exploitability (evaluated in the game with no information abstraction) of subgame-solving in large flop Texas hold'em.", "figure_data": "Hold'em Flop Buckets:2002,00030,000Trunk Strategy88.69 37.374 9.128Unsafe14.68 3.9580.5514Resolve60.16 17.795.407Maxmargin30.05 13.994.343Reach-Maxmargin29.88 13.904.147Reach-Maxmargin (not split)24.87 9.8072.588Estimate11.66 6.2612.423Estimate + Distributional10.44 6.2453.430Reach-Estimate + Distributional10.21 5.7982.258Reach-Estimate + Distributional (not split) 9.560 4.9241.733Large Flop Hold'em Flop Buckets:2002,000 30,000Trunk Strategy283.7 165.2 41.41Unsafe65.59 38.22 396.8Resolve179.6 101.7 23.11Maxmargin134.7 77.89 19.50Reach-Maxmargin134.0 72.22 18.80Reach-Maxmargin (not split)130.3 66.79 16.41Estimate52.62 41.93 30.09Estimate + Distributional49.56 38.98 10.54Reach-Estimate + Distributional49.33 38.52 9.840Reach-Estimate + Distributional (not split) 49.13 37.22 8.777"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Exploitability of the various subgame-solving techniques in nested subgame solving. The performance of the pseudo-harmonic action translation is also shown.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "10 AcknowledgmentsThis material is based on work supported by the National Science Foundation under grants IIS-1718457, IIS-1617590, and CCF-1733556, and the ARO under award W911NF-17-1-0082, as well as XSEDE computing resources provided by the Pittsburgh Supercomputing Center. The Brains vs. AI competition was sponsored by Carnegie Mellon University, Rivers Casino, GreatPoint Ventures, Avenue4Analytics, TNG Technology Consulting, Artificial Intelligence, Intel, and Optimized Markets, Inc. We thank Kristen Gardner, Marcelo Gutierrez, Theo Gutman-Solo, Eric Jackson, Christian Kroer, Tim Reiff, and the anonymous reviewers for helpful feedback.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "u i : Z \u2192 where u 1 = \u2212u 2 .", "formula_coordinates": [3.0, 171.94, 263.85, 120.72, 9.65]}, {"formula_id": "formula_1", "formula_text": "Let \u03c0 \u03c3 (h) = h \u2022a h \u03c3 P (h ) (h , a)", "formula_coordinates": [3.0, 108.0, 387.8, 142.0, 12.72]}, {"formula_id": "formula_2", "formula_text": "I i \u2022 a I i if h \u2022 a h. A Nash equilibrium", "formula_coordinates": [3.0, 107.64, 468.58, 95.33, 25.35]}, {"formula_id": "formula_3", "formula_text": "* \u2212i ) = max \u03c3 i \u2208\u03a3i u i (\u03c3 i , \u03c3 * \u2212i ). An -Nash equilibrium is a strategy profile \u03c3 * such that \u2200i, u i (\u03c3 * i , \u03c3 * \u2212i ) + \u2265 max \u03c3 i \u2208\u03a3i u i (\u03c3 i , \u03c3 * \u2212i ). A best response BR(\u03c3 \u2212i ) is a strategy for player i that is optimal against \u03c3 \u2212i . Formally, BR(\u03c3 \u2212i ) satisfies u i (BR(\u03c3 \u2212i ), \u03c3 \u2212i ) = max \u03c3 i \u2208\u03a3i u i (\u03c3 i , \u03c3 \u2212i ).", "formula_coordinates": [3.0, 108.0, 494.31, 396.35, 49.24]}, {"formula_id": "formula_4", "formula_text": "\u03c3 i is u i (\u03c3 * ) \u2212 u i (\u03c3 i , BR(\u03c3 i ))", "formula_coordinates": [3.0, 230.53, 552.01, 121.95, 11.23]}, {"formula_id": "formula_5", "formula_text": "(I i ) = h\u2208I i \u03c0 \u03c3 \u2212i (h)v \u03c3 i (h) h\u2208I i \u03c0 \u03c3 \u2212i (h) and v \u03c3 i (I i , a) = h\u2208I i \u03c0 \u03c3 \u2212i (h)v \u03c3 i (h\u2022a) h\u2208I i \u03c0 \u03c3 \u2212i (h)", "formula_coordinates": [3.0, 108.0, 593.94, 390.24, 45.27]}, {"formula_id": "formula_6", "formula_text": "CBV \u03c3\u2212i (I i ) = v CBR(\u03c3\u2212i),\u03c3\u2212i i (I i ) and CBV \u03c3\u2212i (I i , a) = v CBR(\u03c3\u2212i),\u03c3\u2212i i (I i , a).", "formula_coordinates": [3.0, 148.72, 682.68, 336.08, 14.15]}, {"formula_id": "formula_7", "formula_text": "\u03c3 \u2212S 2 (I 1 , a ) be a lower bound of CBV \u03c3 \u2212S 2 (I 1 ) \u2212 CBV \u03c3 \u2212S 2 (I 1 , a ) that does not require knowledge of \u03c3 \u2212S 2 .", "formula_coordinates": [7.0, 108.0, 271.89, 396.0, 28.16]}, {"formula_id": "formula_8", "formula_text": "\u03c3 \u2212S 2 (I 1 , a ) = max a\u2208Az(I 1 )\u222a{a } CBV \u03c32 (I 1 , a) \u2212 CBV \u03c32 (I 1 , a ) where A z (I 1 ) \u2286 A(I 1 )", "formula_coordinates": [7.0, 134.0, 299.7, 371.16, 15.18]}, {"formula_id": "formula_9", "formula_text": "I 1 \u2208 S top by I 1 \u2022a I1|P (I 1 )=P1 g \u03c3 \u2212S 2 (I 1 , a )", "formula_coordinates": [7.0, 108.0, 334.92, 184.58, 16.37]}, {"formula_id": "formula_10", "formula_text": "M \u03c3 S r (I 1 ) = M \u03c3 S (I 1 ) + I 1 \u2022a I1|P (I 1 )=P1 g \u03c3 \u2212S 2 (I 1 , a )(1)", "formula_coordinates": [7.0, 192.29, 358.9, 311.71, 26.25]}, {"formula_id": "formula_11", "formula_text": "\u03c3 \u2212S 2 (I 1 , a ) = CBV \u03c32 (I 1 ) \u2212CBV \u03c32 (I 1 , a ) in place of g \u03c3 \u2212S 2 (I 1 , a ) , whereCBV\u03c32", "formula_coordinates": [7.0, 108.0, 518.69, 762.05, 27.81]}, {"formula_id": "formula_12", "formula_text": "BR(\u03c3 2 ) 1 (I 1 ) > 0 for some I 1 \u2208 S top , then exp(\u03c3 2 ) \u2264 exp(\u03c3 \u2212S 2 ) \u2212 h\u2208I1 \u03c0 \u03c32 \u22121 (h)M \u03c3 S r (I 1 ).", "formula_coordinates": [8.0, 108.0, 86.63, 396.0, 28.47]}, {"formula_id": "formula_13", "formula_text": "CBV \u03c3 * 2 (I1) rather than CBV \u03c3 * 2 (I1, a) because CBV \u03c3 * 2 (I1) \u2212 CBV \u03c3 * 2 (I1, a", "formula_coordinates": [9.0, 169.51, 700.32, 286.05, 11.66]}, {"formula_id": "formula_14", "formula_text": "M \u03c3 S (I 1 ) = CBV \u03c32 (I 1 ) \u2212 CBV \u03c3 S 2 (I 1 )(2)", "formula_coordinates": [15.0, 224.2, 215.81, 279.8, 13.37]}, {"formula_id": "formula_15", "formula_text": "\u03b7 t = \u221a ln(|A(I1)|) 3 \u221a V AR(I1)t \u221a t", "formula_coordinates": [17.0, 195.07, 675.71, 80.17, 26.45]}, {"formula_id": "formula_16", "formula_text": "BR(\u03c3 2 ) 1 (I * 1 ) > 0 for some I * 1 \u2208 S top and let = M r (I * 1 ). Define \u03c0 \u03c3 \u22121 (I 1 ) = h\u2208I1 \u03c0 \u03c3 \u22121 (h) and define \u03c0 \u03c3 \u22121 (I 1 , I 1 ) = h\u2208I1,h \u2208I 1 \u03c0 \u03c3 \u22121 (h, h", "formula_coordinates": [19.0, 108.0, 355.69, 396.0, 39.69]}, {"formula_id": "formula_17", "formula_text": "CBV \u03c3 2 (I 1 ) \u2264 CBV \u03c3 \u2212S 2 (I 1 )+ I 1 \u2022a I1|P (I 1 )=P1 CBV \u03c3 \u2212S 2 (I 1 ) \u2212 CBV \u03c3 \u2212S 2 (I 1 , a ) \u2212 h\u2208I1,h * \u2208I * 1 \u03c0 \u03c32 \u22121 (h, h * ) (4)", "formula_coordinates": [19.0, 117.96, 425.84, 386.04, 44.25]}, {"formula_id": "formula_18", "formula_text": "CBV \u03c3 2 (I 1 , a) = CBV \u03c3 \u2212S 2 (I 1 , a)+ I 1 \u2208Succ(I1,a) \u03c0 \u03c3 2 \u22121 (I 1 , I 1 )(CBV \u03c3 2 (I 1 ) \u2212 CBV \u03c3 \u2212S 2 (I 1 )) (5)", "formula_coordinates": [19.0, 117.96, 593.08, 386.04, 44.7]}, {"formula_id": "formula_19", "formula_text": "CBV \u03c3 2 (I 1 , a) \u2264 CBV \u03c3 \u2212S 2 (I 1 , a) \u2212 \u03c0 \u03c32 \u22121 (I 1 , I * 1 ) + I 1 \u2208Succ(I1,a) \u03c0 \u03c32 \u22121 (I 1 , I 1 ) I 1 \u2022a I 1 |P (I 1 )=P1 CBV \u03c3 \u2212S 2 (I 1 ) \u2212 CBV \u03c3 \u2212S 2 (I 1 , a ) CBV \u03c3 2 (I 1 , a) \u2264 CBV \u03c3 \u2212S 2 (I 1 ) \u2212 CBV \u03c3 \u2212S 2 (I 1 ) \u2212 CBV \u03c3 \u2212S 2 (I 1 , a) \u2212 \u03c0 \u03c32 \u22121 (I 1 , I * 1 ) + I 1 \u2208Succ(I1,a) \u03c0 \u03c32 \u22121 (I 1 , I 1 ) I 1 \u2022a I 1 |P (I 1 )=P1 CBV \u03c3 \u2212S 2 (I 1 ) \u2212 CBV \u03c3 \u2212S 2 (I 1 , a ) Since CBV \u03c3 \u2212S 2 (I 1 ) \u2212 CBV \u03c3 \u2212S 2 (I 1 , a) \u2264 CBV \u03c3 \u2212S 2 (I 1 ) \u2212 CBV \u03c3 \u2212S 2", "formula_coordinates": [19.0, 117.96, 674.79, 361.13, 45.18]}, {"formula_id": "formula_20", "formula_text": "CBV \u03c3 2 (I 1 , a) \u2264 CBV \u03c3 \u2212S 2 (I 1 ) \u2212 (CBV \u03c3 \u2212S 2 (I 1 ) \u2212 CBV \u03c3 \u2212S 2 (I 1 , a) \u2212 \u03c0 \u03c32 \u22121 (I 1 , I * 1 ) + I 1 \u2208Succ(I1,a) \u03c0 \u03c32 \u22121 (I 1 , I 1 ) I 1 \u2022a I 1 |P (I 1 )=P1 CBV \u03c3 \u2212S 2 (I 1 ) \u2212 CBV \u03c3 \u2212S 2 (I 1 , a ) CBV \u03c3 2 (I 1 , a) \u2264 CBV \u03c3 \u2212S 2 (I 1 ) \u2212 \u03c0 \u03c32 \u22121 (I 1 , I * 1 ) + I 1 \u2208Succ(I1,a) \u03c0 \u03c32 \u22121 (I 1 , I 1 ) I 1 \u2022a I1|P (I 1 )=P1 CBV \u03c3 \u2212S 2 (I 1 ) \u2212 CBV \u03c3 \u2212S 2 (I 1 , a ) CBV \u03c3 2 (I 1 , a 1 ) \u2264 CBV \u03c3 \u2212S 2 (I 1 )\u2212\u03c0 \u03c32 \u22121 (I 1 , I * 1 ) + I 1 \u2022a I1|P (I 1 )=P1 CBV \u03c3 \u2212S 2 (I 1 )\u2212CBV \u03c3 \u2212S 2 (I 1 , a 1 )", "formula_coordinates": [20.0, 108.0, 150.02, 421.48, 144.58]}, {"formula_id": "formula_21", "formula_text": "CBV \u03c3 2 (I 1 ) \u2264 CBV \u03c3 \u2212S 2 (I 1 )\u2212\u03c0 \u03c32 \u22121 (I 1 , I * 1 ) + I 1 \u2022a I1|P (I 1 )=P1 CBV \u03c3 \u2212S 2 (I 1 )\u2212CBV \u03c3 \u2212S 2 (I 1 , a )", "formula_coordinates": [20.0, 108.0, 334.75, 407.32, 26.25]}, {"formula_id": "formula_22", "formula_text": "CBV \u03c3 2 (I 1 ) \u2264 CBV \u03c3 * 2 (I 1 ) + 2\u2206(6)", "formula_coordinates": [20.0, 236.85, 566.29, 267.15, 13.37]}, {"formula_id": "formula_23", "formula_text": "CBV \u03c3 2 (I 1 , a) = h\u2208I1 \u03c0 \u03c3 2 \u22121 (h) v CBR(\u03c3 2 ),\u03c3 2 (h \u2022 a) / h\u2208I1 \u03c0 \u03c3 2 \u22121 (h)(7)", "formula_coordinates": [21.0, 154.45, 427.64, 349.55, 23.52]}, {"formula_id": "formula_24", "formula_text": "CBV \u03c3 2 (I 1 , a) = h\u2208I1 \u03c0 \u03c3 2 \u22121 (h) h \u2208Succ(h,a) \u03c0 \u03c3 2 \u22121 (h, h ) v CBR(\u03c3 2 ),\u03c3 2 (h ) / h\u2208I1 \u03c0 \u03c3 2 \u22121 (h) (8", "formula_coordinates": [21.0, 113.23, 493.87, 386.9, 23.91]}, {"formula_id": "formula_25", "formula_text": ")", "formula_coordinates": [21.0, 500.13, 497.57, 3.87, 8.64]}, {"formula_id": "formula_26", "formula_text": "CBV \u03c3 2 (I 1 , a) = h\u2208I1 h \u2208Succ(h,a) \u03c0 \u03c3 2 \u22121 (h ) v CBR(\u03c3 2 ),\u03c3 2 (h ) / h\u2208I1 \u03c0 \u03c3 2 \u22121 (h)(9)", "formula_coordinates": [21.0, 139.89, 529.44, 364.11, 23.91]}, {"formula_id": "formula_27", "formula_text": "CBV \u03c3 2 (I 1 , a) = I 1 \u2208Succ(I1,a) h \u2208I 1 \u03c0 \u03c3 2 \u22121 (h ) v CBR(\u03c3 2 ),\u03c3 2 (h ) / h\u2208I1 \u03c0 \u03c3 2 \u22121 (h)(10)", "formula_coordinates": [21.0, 124.27, 591.49, 379.73, 25.69]}, {"formula_id": "formula_28", "formula_text": "h \u2208I 1 \u03c0 \u03c3 2 \u22121 (h ) / h\u2208I1 \u03c0 \u03c3 2 \u22121 (h)(11)", "formula_coordinates": [21.0, 337.6, 646.33, 166.4, 25.63]}, {"formula_id": "formula_29", "formula_text": "h \u2208I 1 \u03c0 \u03c3 2 \u22121 (h ) / h\u2208I1 \u03c0 \u03c3 2 \u22121 (h)(12)", "formula_coordinates": [21.0, 346.17, 699.26, 157.83, 25.63]}, {"formula_id": "formula_30", "formula_text": "h \u2208I 1 \u03c0 \u03c3 * 2 \u22121 (h ) / h\u2208I1 \u03c0 \u03c3 * 2 \u22121 (h)(13)", "formula_coordinates": [22.0, 346.17, 106.1, 157.83, 27.27]}, {"formula_id": "formula_31", "formula_text": "I 1 \u2208Succ(I1,a) h \u2208I 1 \u03c0 \u03c3 * 2 \u22121 (h ) / h\u2208I1 \u03c0 \u03c3 * 2 \u22121 (h)(14)", "formula_coordinates": [22.0, 298.36, 160.46, 205.64, 27.33]}], "doi": ""}