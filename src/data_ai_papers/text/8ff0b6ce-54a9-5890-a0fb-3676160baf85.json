{"title": "Safe Reinforcement Learning via Probabilistic Logic Shields", "authors": "Wen-Chi Yang; Giuseppe Marra; Gavin Rens; Luc De Raedt; Leuven Ai; K U Leuven;  Belgium", "pub_date": "", "abstract": "Safe Reinforcement learning (Safe RL) aims at learning optimal policies while staying safe. A popular solution to Safe RL is shielding, which uses a logical safety specification to prevent an RL agent from taking unsafe actions. However, traditional shielding techniques are difficult to integrate with continuous, end-to-end deep RL methods. To this end, we introduce Probabilistic Logic Policy Gradient (PLPG). PLPG is a model-based Safe RL technique that uses probabilistic logic programming to model logical safety constraints as differentiable functions. Therefore, PLPG can be seamlessly applied to any policy gradient algorithm while still providing the same convergence guarantees. In our experiments, we show that PLPG learns safer and more rewarding policies compared to other state-of-the-art shielding techniques.", "sections": [{"heading": "Introduction", "text": "Shielding is a popular Safe Reinforcement Learning (Safe RL) technique that aims at finding an optimal policy while staying safe [Jansen et al., 2020]. To do so, it relies on a shield, a logical component that monitors the agent's actions and rejects those that violate the given safety constraint. These rejection-based shields are typically based on formal verification, offering strong safety guarantees compared to other safe exploration techniques [Garc\u00eda and Fern\u00e1ndez, 2015]. While early shielding techniques operate completely on symbolic state spaces [Jansen et al., 2020;Alshiekh et al., 2018;Bastani et al., 2018], more recent ones have incorporated a neural policy learner to handle continuous state spaces [Hunt et al., 2021;Anderson et al., 2020;Harris and Schaub, 2020]. In this paper, we will also focus on integrating shielding with neural policy learners.\nIn current shielding approaches, the shields are deterministic, that is, an action is either safe or unsafe in a particular state. This is an unrealistic assumption as the world is inherently uncertain and safety is a matter of degree and risk rather than something absolute. For example, Fig. 1 demonstrates a scenario in which a car must detect obstacles from visual input and the sensor readings are noisy. The uncertainty arising from noisy sensors cannot be directly exploited by such rejection-based shields. In fact, it is often assumed that agents have perfect sensors [Giacobbe et al., 2021;Hunt et al., 2021], which is unrealistic. By working with probabilistic rather than deterministic shields, we will be able to cope with such uncertainties and risks.\nMoreover, even when given perfect safety information in all states, rejection-based shielding may fail to learn an optimal policy [Ray et al., 2019;Hunt et al., 2021;Anderson et al., 2020]. This is due to the way the shield and the agent interact as the learning agent is not aware of the rejected actions and continues to update its policy as if all safe actions were sampled directly from its safety-agnostic policy instead of through the shield. By eliminating the mismatch between the shield and the policy, we will be able to guarantee convergence towards an optimal policy if it exists.\nWe introduce probabilistic shields as an alternative to the deterministic rejection-based shields. Essentially, probabilistic shields take the original policy and noisy sensor readings to produce a safer policy, as demonstrated in Fig. 2 (right). By explicitly connecting action safety to probabilistic semantics, probabilistic shields provides a realistic and principled way to balance return and safety. This also allows for shielding to be applied at the level of the policy instead of at the level of individual actions, which is typically done in the literature [Hunt et al., 2021;Jansen et al., 2020].\nWe propose the concept of Probabilistic Logic Shields (PLS) and its implementation in probabilistic logic programs. Probabilistic logic shields are probabilistic shields that models safety through the use of logic. The safety specification is expressed as background knowledge and its interaction with the learning agent and the noisy sensors is encoded in a probabilistic logic program, which is an elegant and effective way of defining a shield. Furthermore, PLS can be automatically compiled into a differentiable structure, allowing for the optimization of a single loss function through the shield, enforcing safety directly in the policy. Probabilistic logic shields have several benefits:\n\u2022 A Realistic Safety Function. PLS models a more realistic, probabilistic evaluation of safety instead of a deterministic one, it allows to balance safety and reward, and in this way control the risk.  et al., 2015], A2C [Mnih et al., 2016], etc. \u2022 Convergence. Using PLS in deep RL comes with convergence guarantees unlike the use of rejection-based shields.", "publication_ref": ["b11", "b8", "b11", "b0", "b1", "b10", "b9", "b10", "b10", "b10", "b11", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "Probabilistic Logic Programming We will introduce probabilistic logic programming (PLP) using the syntax of the ProbLog system [De Raedt and Kimmig, 2015]. An atom is a predicate symbol followed by a tuple of logical variables and/or constants. A literal is an atom or its negation.\nA ProbLog theory (or program) T consists of a finite set of probabilistic facts F and a finite set of clauses BK. A probabilistic fact is an expression of the form p i :: f i where p i denotes the probability of the fact f i being true. It is assumed that the probabilistic facts are independent of one other (akin to the nodes with parents in Bayesian networks) and the dependencies are specified by clauses (or rules). For instance, 0.8 :: obstc(front). states that the probability of having an obstacle in front is 0.8. A clause is a universally quantified expression of the form h :\u2212b 1 , ..., b n where h is a literal and b 1 , ..., b n is a conjunction of literals, stating that h is true if all b i are true. The clause defining safe in Fig. 1 states that it is safe when there is no crash. Each truth value assignment of all probabilistic facts F, denoted by F k , induces a possible world w k where all ground facts in w k are true and all that are not in w k are false. Formally, the probability of a possible world w k is defined as follows.\nP (w k ) = fi\u2208w k p i fi \u2208w k (1 \u2212 p i ).(1)\nProbLog allows for annotated disjunctions (ADs). An AD is a clause with multiple heads h i that are mutually exclusive to one another, meaning that exactly one head is true when the body is true, and the choice of the head is governed by a probability distribution. An AD has the form of p 1 :: h 1 ; \u2022 \u2022 \u2022 ; p m :: h m where each h i is the head of a clause and Raedt and Kimmig, 2015;Fierens et al., 2015]. For instance, {0.1 :: act(nothing); 0.5 :: act(accel); 0.1 :: act(brake); 0.1 :: act(left); 0.2 :: act(right)} is an AD (with no conditions), stating the probability with which each action will be taken. The success probability of an atom q given a theory T is the sum of the probabilities of the possible worlds that entail q. Formally, it is defined as P (q) := w k |=q P (w k ). Given a set of atoms E as evidence, the conditional probability of a query q is P(q|E) = P (q,E) P (E) . For instance, in Fig. 1, the probability of being safe in the next state given that the agent accelerates is P(safe|act(accel)) = 0.14 0.5 = 0.28.\nm i=1 p i \u2264 1 [De\nMDP A Markov decision process (MDP) is a tuple M = S, A, T, R, \u03b3 where S and A are state and action spaces, respectively. T (s, a, s ) = P (s |s, a) defines the probability of being in s after executing a in s. R(s, a, s ) defines the immediate reward of executing a in s resulting in s . \u03b3 \u2208 [0, 1] is the discount factor. A policy \u03c0 : S \u00d7 A \u2192 [0, 1] defines the probability of taking an action in a state. We use \u03c0(a|s) to denote the probability of action a in state s under policy \u03c0 and \u03c0(s) for the probability distribution over all the actions in state s. Shielding A shield is a reactive system that guarantees safety of the learning agent by preventing it from selecting any unsafe action at run time [Jansen et al., 2020]. Usually, a shield is provided as a Markov model and a temporal logical formula, jointly defining a safety specification. The shield constrains the agent's exploration by only allowing it to take actions that satisfy the given safety specification. For example, when there is a car driving in front and the agent proposes to accelerate, a standard shield (Fig. 2, left) will reject the action as the agent may crash. Policy Gradients The objective of a RL agent is to find a policy \u03c0 \u03b8 (parameterized by \u03b8) that maximizes the total expected return along the trajectory, formally,\nJ(\u03b8) = E \u03c0 \u03b8 [ \u221e t=0 \u03b3 t r t ]\nJ(\u03b8) is assumed to be finite for all policies. An important class of RL algorithms, particularly relevant in the setting of shielding, is based on policy gradients, which maximize J(\u03b8) by repeatedly estimating the gradient \u2207 \u03b8 J(\u03b8). The general form of policy gradient methods is:\n\u2207 \u03b8 J(\u03b8) = E \u03c0 \u03b8 [ \u221e t=0 \u03a8 t \u2207 \u03b8 log \u03c0 \u03b8 (s t , a t )](2)\nwhere \u03a8 t is an empirical expectation of the return . The expected value is usually computed using Monte Carlo methods, requiring efficient sampling from \u03c0 \u03b8 .\n3 Probabilistic Logic Shields", "publication_ref": ["b5", "b5", "b7", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Probabilistic Shielding", "text": "To reason about safety, we will assume that we have a safetyaware probabilistic model P(safe|a, s), indicating the probability that an action a is safe to execute in a state s. We use bold P to distinguish it from the underlying probability distributions P (\u2022) of the MDP. Our safety-aware probabilistic model does not require that the underlying MDP is known. However, it needs to represent internally safe relevant-dynamics as the safety specification. Therefore, the safety-aware model P is not a full representation of the MDP, it is limited to safety-related properties.\nNotice that for traditional logical shields actions are either safe or unsafe, which implies that P(safe|a, s) = 1 or 0. We are interested in the safety of policies, that is, in P \u03c0 (safe|s) = a\u2208A P(safe|s, a)\u03c0(a|s).\n(3) Figure 1: A motivation example of Probabilistic Logic Shields. We encode the interaction between the base policy \u03c0, the state-abstraction H and the safety specification using a ProbLog program T . This provides a uniform language to express many aspects of the shielding process.\nThe shielded policy \u03c0 + (s) decreases the probability of unsafe actions, e.g. acceleration and increases the likelihood of being safe.\nBy marginalizing out actions according to their probability under \u03c0, the probability P \u03c0 measures how likely it is to take a safe action in s according to \u03c0. Definition 3.1. Probabilistic Shielding Given a base policy \u03c0 and a probabilistic safety model P(safe|s, a), the shielded policy is \u03c0 + (a|s) = P \u03c0 (a|s, safe) = P(safe|s, a) P \u03c0 (safe|s) \u03c0(a|s)\nIntuitively, a shielded policy is a re-normalization of the base policy that increases (resp. decreases) the probabilities of the actions that are safer (resp. less safe) than average. Proposition 1. A shielded policy is always safer than its base policy in all states, i.e. P \u03c0 + (safe|s) \u2265 P \u03c0 (safe|s) for all s and \u03c0. (A proof is in Appendix B)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Shielding with Probabilistic Logics", "text": "In this paper, we focus on probabilistic shields implemented through probabilistic logic programs. The ProbLog program defining probabilistic safety consists of three parts.\nFirst, an annotated disjunction \u03a0 s , which represents the policy \u03c0(a|s) in the logic program. For example, in Fig. 1, the policy \u03c0(a|s) is represented as the annotated disjunction \u03a0 s = {0.1 :: act(nothing); 0.5 :: act(accel); 0.1 :: act(brake); 0.1 :: act(left); 0.2 :: act(right)}.\nThe second component of the program is a set of probabilistic facts H s representing an abstraction of the current state. Such abstraction should contain information needed to reason about the safety of actions in that state. Therefore, it should not be a fair representation of the entire state s. For example, in Fig. 1, the abstraction is represented as the set of probabilistic facts: H s = { 0.8 :: obstc(front). 0.2 :: obstc(left). 0.5 :: obstc(right).}\nThe third component of the program is a safety specification BK, which is a set of clauses and ADs representing knowledge about safety. In our example, the predicates crash and safe are defined using clauses. The first clause for crash states that the probability of having a crash is 0.9 if the agent accelerates when there is an obstacle in front of it. The clause for safe states that it is safe if no crash occurs.\nTherefore, we obtain a ProbLog program T (s) = BK \u222a H s \u222a \u03a0 s , inducing a probabilistic measure P T . By querying this program, we can reason about safety of policies and actions. More specifically, we can use T (s) to obtain:\n\u2022 action safety in s: P(safe|s, a) = P T (safe|a) \u2022 policy safety in s: P \u03c0 (safe|s) = P T (safe)\n\u2022 the shielded policy in s: P \u03c0 (a|s, safe) = P T (a|safe) Notice that these three distributions can be obtained by querying the same identical program ProbLog program T .\nIt is important to realize that although we are using the probabilistic logic programming language ProbLog to reason about safety, we could also have used alternative representations such as Bayesian networks, or other StarAI models . The ProbLog representation is however convenient because it allows to easily model planning domains, it is Turing equivalent and it is differentiable. At the same time, it is important to note that the safety model in ProbLog is an abstraction, using possibly a different representation than that of the underlying MDP.\nPerception Through Neural Predicates The probabilities of both \u03a0 s and H s depend on the current state s. We compute these probabilities using two neural networks \u03c0 and H operating on real valued inputs (i.e. images or sensors). We feed then the probabilities to the ProbLog program. As we depicted in Fig. 1, both these functions take a state representation s as input (e.g. an image) and they output the probabilities the facts representing, respectively, the actions and the safe-relevant abstraction of the state. This feature is closely related to the notion of neural predicate, which can -for the purposes of this paper -be regarded as encapsulated neural networks inside logic. Since ProbLog programs are differentiable with respect to the probabilities in \u03a0 s and H s , the gradients can be seamlessly backpropagated to network parameters during learning.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Probabilistic Logic Policy Gradient", "text": "In this section, we demonstrate how to use probabilistic logic shields with a deep reinforcement learning method. We will focus on real-vector states (such as images), but the safety constraint is specified symbolically and logically. Under this setting, our goal is to find an optimal policy in the safe policy space \u03a0 safe = {\u03c0 \u2208 \u03a0|P \u03c0 (safe|s) = 1, \u2200s \u2208 S}. Notice that it is possible that no optimal policies exist in the safe policy space. In this case, our aim will be to find a policy that it as safe as possible.\nWe propose a new safe policy gradient technique, which we call Probabilistic Logic Policy Gradient (PLPG). PLPG Figure 2: A comparison between a traditional rejection-based shield (SHLD) and a Probabilistic Logic Shield (PLS). Both shields take a policy \u03c0 and a set of noisy sensor readings Hs to compute a safe action. Left: The policy must keep sampling actions until a safe action is accepted by the rejection-based shield. This requires an assumption that an action is either completely safe or unsafe. Right: We replace SHLD with PLS that proposes a safer policy \u03c0 + on the policy level without imposing the assumption. applies probabilistic logic shields and is guaranteed to converge to a safe and optimal policy if it exists.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "PLPG for Probabilistic Shielding", "text": "To integrate PLS with policy gradient, we simply replace the base policy in Eq. (2) with the shielded policy obtained through Eq. (4). We call this a shielded policy gradient.\nE \u03c0 + \u03b8 [ \u221e t=0 \u03a8 t \u2207 \u03b8 log \u03c0 + \u03b8 (a t |s t )](5)\nThis requires the shield to be differentiable and cannot be done using rejection-based shields. The gradient encourages policies \u03c0 + that are more rewarding, i.e. that has a large \u03a8, in the same way of a standard policy gradient. It does so by assuming that unsafe actions have been filtered by the shield. However, when the safety definition is uncertain, unsafe actions may still be taken and such a gradient may still end up encouraging unsafe but rewarding policies.\nFor this reason, we introduce a safety loss to penalize unsafe policies. Intuitively, a safe policy should have a small safety loss, and the loss of a completely safe policy should be zero. The loss can be expressed by interpreting the policy safety as the probability that \u03c0 + satisfies the safety constraint (using the semantic loss of [Xu et al., 2018]); more formally, L s := \u2212 log P \u03c0 + (safe|s). Then, the corresponding safety gradient is as follows.\n\u2212E \u03c0 + \u03b8 [\u2207 \u03b8 log P \u03c0 + (safe|s)](6)\nNotice that the safety gradient is not a shielding mechanism but a regret due to its loss-based nature. By combining the shielded policy gradient and the safety gradient, we obtain a new Safe RL technique. Definition 4.1. (PLPG) The probabilistic logic policy gradient \u2207 \u03b8 J(\u03b8) is\nE \u03c0 + \u03b8 [ \u221e t=0 \u03a8t\u2207 \u03b8 log \u03c0 + \u03b8 (at|st) \u2212 \u03b1\u2207 \u03b8 log P \u03c0 + \u03b8 (safe|st)] (7)\nwhere \u03b1 is the safety coefficient that indicates the weight of the safety gradient.\nWe introduce the safety coefficient \u03b1, a hyperparameter that controls the combination of the two gradients.\nBoth gradients in PLPG are essential. The shielded policy gradient forbids the agent from immediate danger and the safety gradient penalizes unsafe behavior. The interaction between shielded and loss-based gradients is still a very new topic, nonetheless, recent advances in neuro-symbolic learning have shown that both are important [Ahmed et al., 2022]. Our experiments shows that having only one but not both is practically insufficient.", "publication_ref": ["b15"], "figure_ref": [], "table_ref": []}, {"heading": "Probabilistic vs Rejection-based Shielding", "text": "The problem of finding a policy in the safe policy space cannot be solved by rejection-based shielding (cf. Fig. 2, left) without strong assumptions, such as that a state-action pair is either completely safe or unsafe. It is in fact often assumed that a set of safe state-action pairs SA safe is given. To implement a rejection-based shield, the agent must keep sampling from the base policy until an action a is accepted. This approach implicitly conditions through very inefficient rejection sampling schemes as below, which has an unclear link to probabilistic semantics [Robert et al., 1999].\n\u03c0 + (a|s) = 1 [(s,a)\u2208SAsafe] \u03c0(a|s) a \u2208A 1 [(s,a )\u2208SAsafe] \u03c0(a |s) (8)", "publication_ref": ["b13"], "figure_ref": [], "table_ref": []}, {"heading": "Convergence Under Perfect Safety Information", "text": "It is common for existing shielding approaches to integrate policy gradients with a rejection-based shield, e.g. Eq. ( 8), resulting in the following policy gradient.\n\u2207 \u03b8 J(\u03b8) = E \u03c0 + \u03b8 [ \u221e t=0 \u03a8 t \u2207 \u03b8 log \u03c0 \u03b8 (s t , a t )](9)\nIt is a known problem that this approach may result in sub-optimal policies, even though the agent has perfect safety information [Kalweit et al., 2020;Ray et al., 2019;Hunt et al., 2021;Anderson et al., 2020]. This is due to a policy mismatch between \u03c0 + and \u03c0 in Eq. (9). More specifically, Eq. ( 9) is an off-policy algorithm, meaning that the policy used to explore (i.e. \u03c0 + ) is different from the policy that is updated (i.e. \u03c0) 1 . For any off-policy policy gradient method to converge to an optimal policy (even in the tabular case), the behavior policy (to explore) and the target policy (to be updated) must appropriately visit the same state-action space, i.e., if \u03c0(a|s) > 0 then \u03c0 + (a|s) > 0 [Sutton and Barto, 2018]. This requirement is violated by Eq. (9). PLPG, on the other hand, reduces to Eq. (5) when given perfect sensor information of the state. Since it has the same form as the standard policy gradient (Eq. 9), PLPG is guaranteed to converge to an optimal policy according to the Policy Gradient Theorem [Sutton et al., 2000]. Proposition 2. PLPG, i.e. Eq. (7), converges to an optimal policy given perfect safety information in all states.\nIt should be noted that the base policy learnt by PLPG is unlikely to be equivalent to the policy that would have been learnt without a shield, all other things being equal. That is, in general, \u03c0 \u03b8 in Eq. (9) will not tend to be the same as \u03c0 \u03b8 in Eq. ( 7) as learning continues. This is because the parameters \u03b8 in Eq. ( 7) depend on \u03c0 + \u03b8 instead of \u03c0 \u03b8 , and \u03c0 \u03b8 is learnt in a way to make \u03c0 + optimal given safety imposed by the shield.", "publication_ref": ["b11", "b10", "b13", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Learning From Unsafe Actions", "text": "PLPG has a significant advantage over a rejection-based shield by learning not only from the actions accepted by the shield but also from the ones that would have been rejected. This is a result of the use of ProbLog programs that computes the safety of all available actions in the current state, allowing us to update the base policy in terms of safety without having to actually execute the other actions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Leveraging Probabilistic Logic Programming", "text": "Probabilistic logic programs can, just like Bayesian networks, be compiled into circuits using knowledge compilation [Darwiche, 2003]. Although probabilistic inference is hard (#Pcomplete), once the circuit is obtained, inference is linear in the size of the circuit [De Raedt and Kimmig, 2015;Fierens et al., 2015]. Furthermore, the circuits can be used to compute gradients and the safety model only needs to be compiled once. We show a compiled program in Appendix C. Our PLP language is still restricted to discrete actions, although it should be possible to model continuous actions using extensions thereof in future work [Nitti et al., 2016].", "publication_ref": ["b5", "b7", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "Now, we present an empirical evaluation of PLPG, when compared to other baselines in terms of return and safety.\nExperimental Setup Experiments are run in three environments. (1) Stars, the agent must collect as many stars as possible without going into a stationary fire; (2) Pacman, the agent must collect stars without getting caught by the fire ghosts that can move around, and (3) Car Racing, the agent must go around the track without driving into the grass area. We use two configurations for each domain. A detailed description is in Appendix A. We compare PLPG to three RL baselines.\n\u2022 PPO, a standard safety-agnostic agent that starts with a random initial policy [Schulman et al., 2017]. \u2022 VSRL, an agent augmented with a deterministic rejection-based shield [Hunt et al., 2021]. Its structure is shown in Fig. 2 (left).\n\u2022 -VSRL a new risk-taking variant of VSRL that has a small probability of accepting any action, akin togreedy [Fern\u00e1ndez and Veloso, 2006] As -VSRL simulates an artificial noise/distrust in the sensors, we expect that it improves VSRL's ability to cope with noise. In order to have a fair comparison, VSRL, -VSRL and PLPG agents have identical safety sensors and safety constraints, and PPO does not have any safety sensors. We do not compare to shielding approaches that require a full model of the environment since they are not applicable here. We train all agents in all domains using 600k learning steps and all experiments are repeated using five different seeds.\nIn our experiments, we answer the following questions : Q1 Does PLPG produce safer and more rewarding policies than its competitors? Q2 What is the effect of the hyper-parameters \u03b1 and ? Q3 Does the shielded policy gradient or the safety gradient have more impact on safety in PLPG? Q4 What are the performance and the computational cost of a multi-step safety look-ahead PLS?\nMetrics We compare all agents in terms of two metrics:\n(1) average normalized return, i.e. the standard per-episode normalized return averaged over the last 100 episodes;\n(2) cumulative normalized violation, i.e. the accumulated number of constraint violations from the start of learning. The absolute numbers are listed in Appendix D.", "publication_ref": ["b13", "b10", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Probabilistic Safety via Noisy Sensor Values", "text": "We consider a set of noisy sensors around the agent for safety purposes.\nFor instance, the four fire sensor readings in Fig. 3 (left) might {0.6 :: fire(0, 1). 0.1 :: fire(0, \u22121). 0.1 :: fire(\u22121, 0). 0.4 :: fire(1, 0).} These sensors provide to the shield only local information of the learning agent. PLPG agents are able to directly use the noisy sensor values. However, as VSRL and -VSRL agents require 0/1 sensor values to apply formal verification, noisy sensor readings must be discretized to {0, 1}. For example, the above fire sensor readings become {1 :: fire(0, 1). 0 :: fire(0, \u22121). 0 :: fire(\u22121, 0). 0 :: fire(1, 0).} For all domains, the noisy sensors are standard, pre-trained neural approximators of an accuracy higher than 99%. The training details are presented in Appendix A.\nEven with such accurate sensors, we will show that discretizing the sensor values, as in VSRL, is harmful for safety. Details of the pre-training procedure are in Appendix A.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Q1: Lower Violation and Higher Return", "text": "We evaluate how much safer and more rewarding PLPG is compared to baselines by measuring the cumulative safety violation and the average episodic return during the learning process. Before doing so, we must select appropriate hyperparameters for PLPG and VSRL agents. Since our goal is to find an optimal policy in the safe policy space, we select the \u03b1 \u2208 {0, 0.1, 0.5, 1, 5} and \u2208 {0, 0.005, 0.01, 0.05, 0.1, 0.2, 0.5, 1.0} values that result in the lowest violation in each domain. The hyperparameter choices are listed in Appendix D. These values are fixed for the rest of the experiments (except for Q2 where we explicitly analyze their effects). We show that PLPG achieves the lowest violation while having a comparable return to other agents.\nThe results are plotted in Fig. 4 where each data point is a policy trained for 600k steps. Fig. 4 clearly shows that PLPG is the safest in most tested domains. When augmented with perfect sensors, PLPG's violation is lower than PPO by 50.2% and lower than VSRL by 25.7%. When augmented with noisy sensors, PLPG's violation is lower than PPO by 51.3%, lower than VSRL by 24.5% and lower than -VSRL by 13.5%. Fig. 4 also illustrates that PLPG achieves a comparable (or slightly higher) return while having the lowest violation. When augmented with perfect sensors, PLPG's return is higher than PPO by 0.5% and higher than VSRL by 4.8%. When augmented with noisy sensors, PLPG's return is higher than PPO by 4.5%, higher than VSRL by 6.5% and higher than -VSRL by 6.7%.\nCar Racing has complex and continuous action effects compared to the other domains. In CR, safety sensor readings alone are not sufficient for the car to avoid driving into the grass as they do not capture the inertia of the car, which involves the velocity and the underlying physical mechanism such as friction. In domains where safety sensors are not sufficient, the agent must be able to act a little unsafe to learn, as PPO, -VSRL and PLPG can do, instead of completely relying on sensors such as VSRL. Hence, compared to the safetyagnostic baseline PPO, while -VSRL and PLPG respectively reduce violations by 37.8% and 50.2%, VSRL causes 18% more violations even when given perfect safety sensors. Note that we measure the actual violations instead of policy safety P \u03c0 (safe|s). The policy safety during the learning process is plotted in Appendix D.", "publication_ref": [], "figure_ref": ["fig_1", "fig_1", "fig_1"], "table_ref": []}, {"heading": "Q2: Selection of Hyperparameters \u03b1 and", "text": "We analyze how the hyper-parameters \u03b1 and respectively affect the performance of PLPG and VSRL in noisy environ- ments. Specifically, we measure the episodic return and cumulative constraint violation. The results are plotted in Fig. 5.\nThe effect of different values of \u03b1 to PLPG agents is clear. Increasing \u03b1 gives a convex trend in both the return and the violation counts. The optimal value of \u03b1 is generally between 0.1 and 1, where the cumulative constraint violation and the return are optimal. This illustrates the benefit of combining the shielded policy gradient and the safety gradient. On the contrary, the effect of to VSRL agents is not significant. Increasing generally improves return but worsens constraint violation. This illustrates that simply randomizing the policy is not effective in improving the ability of handling noisy environments. Notice that there is no one-to-one mapping between the two hyper-parameters, as \u03b1 controls the combination of the gradients and controls the degree of unsafe exploration.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Q3: PLPG Gradient Analysis", "text": "We analyze how the shielded policy gradient and the safety gradient interact with one another. To do this, we introduce two ablated agents, one uses only safety gradients (Eq. ( 5)) and the other uses only policy gradients (Eq. ( 6)). The results are plotted in Fig. 6 where each data point is a policy trained for 600k steps.\nFig. 6 illustrates that combining both gradients is safer than using only one gradient. When augmented with perfect sensors, the use of both gradients has a violation that is lower than using only safety or policy gradient by 24.7% and 10.8%, respectively. When augmented with noisy sensors, using both gradients has a violation that is lower than using only safety or policy gradient by 25.5% and 15.5%, respectively. Nonetheless, the importance of policy and safety gradients varies among domains. In Stars and Pacman, using only policy gradients causes fewer violations than using only  safety gradients. In CR, however, using only safety gradients causes fewer violations. This is a consequence of the inertia of the car not being fully captured by safety sensors, as discussed in Q1.", "publication_ref": [], "figure_ref": ["fig_3", "fig_3"], "table_ref": []}, {"heading": "Q4: Multi-step Safety Look-ahead", "text": "We analyze the behaviour of PLPG when we use probabilistic logic shields for multi-step safety look-ahead. In this case, the safety program requires more sensors around the agent for potential danger over a larger horizon in the future. In Pacman, for example, new sensors are required to detect whether there is a ghost N units away where N is the safety horizon. We analyze the behavior/performance in terms of return/violation and computational cost. The results are shown in Table 1.\nIncreasing the horizon causes an exponential growth in the compiled circuit size and the corresponding inference time. However, we can considerably improve both safety and return, especially when moving from one to two steps.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Safe RL. Safe RL aims to avoid unsafe consequences through the use of various safety representations [Garc\u00eda and Fern\u00e1ndez, 2015]. There are several ways to achieve this goal. One could constrain the expected cost [Achiam et al., 2017;Moldovan and Abbeel, 2012], maximize safety constraint satisfiability through a loss function [Xu et al., 2018], add a penalty to the agent when the constraint is violated [Pham et al., 2018;Tessler et al., 2019;Memarian et al., 2021], or construct a more complex reward structure using temporal logic [De Giacomo et al., 2021;Camacho et al., 2019;Jiang et al., 2021;Hasanbeig et al., 2019;Den Hengst et al., 2022]. These approaches express safety as a loss, while our method directly prevents the agent from taking actions that can potentially lead to safety violation. Probabilistic Shields. Shielding is a safe reinforcement learning approach that aims to completely avoid unsafe actions during the learning process [Alshiekh et al., 2018;Jansen et al., 2020]. Previous shielding approaches have been limited to symbolic state spaces and are not suitable for noisy environments [Jansen et al., 2020;Harris and Schaub, 2020;Hunt et al., 2021;Anderson et al., 2020]. To address uncertainty, some methods incorporate randomization, e.g. simulating future states in an emulator to estimate risk [Li and Bastani, 2020;Giacobbe et al., 2021], using -greedy exploration that permits unsafe actions [Garc\u00eda and Fern\u00e1ndez, 2019], or randomizing the policy based on the current belief state [Karkus et al., 2017]. To integrate shielding with neural policies, one can translate a neural policy to a symbolic one that can be formally verified [Bastani et al., 2018;Verma et al., 2019]. However, these methods rely on sampling and do not have a clear connection to uncertainty present in the environment while our method directly exploits such uncertainty through the use of PLP principles. Belief states can capture uncertainty but require an environment model to to keep track of the agent's belief state, which is a stronger assumption than our method [Junges et al., 2021;Carr et al., 2022]. Differentiable layers for neural policies. The use of differentiable shields has gain some attention in the field. One popular approach is to add a differentiable layer to the policy network to prevent constraint violations. Most of these methods focus on smooth physical rules [Dalal et al., 2018;Pham et al., 2018;Cheng et al., 2019] and only a few involve logical constraints. In [Kimura et al., 2021], an optimization layer is used to transform a state-value function into a policy encoded as a logical neural network. [Ahmed et al., 2022] encode differentiable and hard logical constraints for neural networks using PLP. While being similar, this last model focuses on prediction tasks and do not consider a trade-off between return and constraint satisfiability.", "publication_ref": ["b8", "b0", "b15", "b13", "b14", "b12", "b4", "b11", "b9", "b0", "b11", "b11", "b10", "b12", "b9", "b9", "b11", "b1", "b14", "b11", "b2", "b3", "b13", "b2", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We introduced Probabilistic Logic Shields, a proof of concept of a novel class of end-to-end differentiable shielding techniques. PLPG enable efficient training of safe-byconstruction neural policies. This is done by using a probabilistic logic programming layer on top of the standard neural policy. PLPG is a generalization of classical shielding [Hunt et al., 2021] that allows for both deterministic and probabilistic safety specifications. Future work will be dedicated to extending PLS to be used with a larger class of RL algorithms such as the ones with continuous policies.\n% SAFETY LOOKAHEAD xagent(stay, 0, 0). xagent(left,-1, 0).\nxagent (right,1, 0). xagent(up, 0, 1). xagent(down, 0,-1). % SAFETY CONSTRAINT crash:-act(A),\nxagent(A, X, Y), fire(X, Y). safe:-not(crash).", "publication_ref": ["b10"], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Pacman", "text": "Pacman is more complicated than the Stars in that the fire ghosts can move around and their transition models are unknown. All the other environmental parameters are the same as Stars.\nWe use the following probabilistic logic shield to perform a two-step look-ahead. The agent has 12 sensors to detect whether there is a fire ghost one or two units away. The agent is assumed to follow the policy at T = 0 and then do nothing at T = 1. The fire ghosts are assumed to be able to move to any immediate neighboring cells at T = 0 and T = 1. A crash can occur at T = 0 or T = 1. % BASE POLICY a0::act(stay); a1::act(up); a2::act(down); a3::act(left); a4::act(right). % SENSORS f0:: fire(0, 0, 1). f1:: fire(0, 0,-1). f2:: fire(0,-1, 0). f3:: fire(0, 1, 0). f4:: fire(0, 0, 2). f5:: fire(0, 0,-2). f6:: fire(0,-2, 0). f7:: fire(0, 2, 0). f8:: fire(0, 1, 1). f9:: fire(0, 1,-1). f10::fire(0,-1, 1). f11::fire(0,-1,-1). % SAFETY LOOKAHEAD fire(T, X, Y):fire(T-1, PrevX, PrevY), move(PrevX, PrevY, _, X, Y).\nagent(1, X, Y):action(A), move(0, 0, A, X, Y).\nagent(2, X, Y):-agent(1, X, Y). move(X, Y, stay, X, Y). move(X, Y, left, X-1, Y). move(X, Y, right, X+1, Y). move(X, Y, up, X, Y+1). move(X, Y, down, X, Y-1).\n% SAFETY CONSTRAINT crash:-fire(T, X, Y), agent(T, X, Y). safe:-not(crash).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.3 Car Racing", "text": "We simplify Gym's Car Racing environment [Brockman et al., 2016]. Our environment is a randomly generated car track, e.g. Fig. 8 (left). At the beginning, the agent will be put on the track, e.g. Fig. 8 (middle). The task of the agent is to drive around the track (i.e. to finish one lap) within 1000 frames. In each frame, the agent takes a discrete action: donothing, accelerate, brake, turn-left or turn-right. Each action costs a negative reward of \u22120.1. The agent gets a small reward if it continues to follow the track. The agent does not get a penalty for driving on the grass, however, if the car drives outside of the map (i.e. the black part in Fig. 8, left), the episode ends with a negative reward of \u2212100. Each state is a downsampled, grayscale image where each pixel is a value between \u22121 and 1, e.g. Fig. 8 (right).\nThe original environment has a continuous action space that consists of three parameters: steering, gas and break with the minimum values [\u22121, 0, 0] and the maximum values [+1, +1, +1]. In this paper, all agents use five discrete actions: do-nothing ([0,\n0, 0]), accelerate ([0, 1, 0]), brake ([0, 0, 0.8]), turn-left ([\u22121, 0, 0]), turn-right ([1, 0, 0]).\nWe use the following probabilistic logic shield. The act/1 predicates represent the base policy and the grass/1 predicates represent whether there is grass in front, on the left or right hand side of the agent. In Fig. 8 (middle), the sensors may produce {0 :: grass(0), 0 :: grass(1) 0 :: grass(2)}. % BASE POLICY a0::act(dn); a1::act(accel); a2::act(brake); a3::act(left); a4::act(right). % SENSORS g0::grass(front). g1::grass(left). g2::grass(right). % SAFETY LOOKAHEAD ingrass:grass(left), not(grass(right)), (act(left); act(accel)). ingrass:not(grass(left)), grass(right), (act(right); act(accel)).\n% SAFETY CONSTRAINT safe:-not(ingrass).", "publication_ref": ["b1"], "figure_ref": ["fig_4", "fig_4", "fig_4", "fig_4"], "table_ref": []}, {"heading": "A.4 Approximating Noisy Sensors", "text": "We approximate noisy sensors using convolutional neural networks. In all the environments, we use 4 convolutional layers with respectively 8,16,32,64 (5x5) filters and relu activations. The output is computed by a dense layer with sigmoid activation function. The number of output neurons depends on the ProbLog program for that experiment (see Appendix A.1). We pre-train the networks and then we fix them during the reinforcement learning process. The pre-training strategy is the following. We generated randomly 3k images for Stars and Pacman with 30 fires and 30 stars, e.g. Fig. 7 (right), and 2k images for Car Racing, e.g. Fig. 8 (right). We selected the size of the pre-training datasets in order to achieve an accuracy higher than 99% on a validation set of 100 examples.", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "B Safety Guarantees of Probabilistic Logic Shield", "text": "In Definition 3.1, a shielded policy \u03c0 + is always safer than its base policy \u03c0 for all s and \u03c0, i.e. The inequality comes from Jensen's inequality [Pishro-Nik, 2014], which states that E[g(X)] \u2265 g(E[X]) for any convex function g(X). In this proof, g(X) = X 2 .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C A Compiled Circuit", "text": "The following program is compiled to the circuit in Fig. 9.  ", "publication_ref": [], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "D Experiment Details", "text": "We run two configurations for each domain. The second configuration is more challenging than the first one. The configurations may have different normalization ranges and hyperparameters. We select the hyperparameters that result in the lowest violations in each configuration. Table 2 lists a summary of all configurations and all other tables follows this table if not explicitly specified. We train all agents in all configurations using 600k learning steps and all experiments are repeated using five different seeds. The following tables list the normalized, average return and violation values of the five runs. Table 3 lists the return/violation results of all agents (cf. the large data points in Fig. 4). Table 4 lists the return/violation results for PLPG gradient analysis (cf. the large data points in Fig. 6). Tables 5 and 6 respectively list the return/violation results for \u03b1 \u2208 {0, 0.1, 0.5, 1, 5} and \u2208 {0, 0.005, 0.01, 0.05, 0.1, 0.2, 0.5, 1.0} (cf. Fig. 5).\nWe plot episodic return, cumulative violation and stepwise policy safety in the learning process for all agents (P \u03c0 + (safe|s) for shielded agents and P \u03c0 (safe|s) for PPO). Fig. 11 plots these curves for the agents augmented with perfect sensors and Fig. 10 for the ones with noisy sensors. PPO agents are plotted in both figures as a baseline but they are not augmented with any sensors. The return and policy safety curves are smoothed by taking the exponential moving average with the coefficient of 0.05.        ", "publication_ref": [], "figure_ref": ["fig_1", "fig_3", "fig_2", "fig_10", "fig_9"], "table_ref": ["tab_4", "tab_5", "tab_6"]}, {"heading": "A Experiment settings", "text": "Experiments are run on machines that consist of Intel(R) Xeon(R) E3-1225 CPU cores and 32Gb memory. All environments in this work are available on GitHub under the MIT license. The measurements are obtained by taking the average of five seeds. All agents are trained using PPO in stable-baselines [Hill et al., 2018] with batch size=512, n epochs=15, n steps=2048, clip range=0.1, learning rate=0.0001. All policy networks and value networks have two hidden layers of size 64. All the other hyperparameters are set to default as in stable-baselines [Hill et al., 2018]. The source code will be available upon publication of the paper. We build the Stars environments using Berkeley's Pac-Man Project 2 . Our environment is a 15 by 15 grid world containing stars and fires, e.g. Fig. 7, where the agent can move around using five discrete, deterministic actions: stay, up, down, left, right. Each action costs a negative reward of \u22120.1. The task of the agent is to collect all stars in the environment. Collecting one star yields a reward of 1. When the agent finds all stars, the episode ends with a reward of 10. If the agent crashes into fire or has taken 200 steps (maximum episode length), the episode ends with no rewards. Each state is a downsampled, grayscale image where each pixel is a value between -1 and 1, e.g. Fig. 7 (right).", "publication_ref": ["b9", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "A.1 Stars", "text": "We use the following probabilistic logic shield. The act/1 predicates represent the base policy and the fire/2 predicates represent whether there is fire in an immediate neighboring grid. These predicates are neural predicates, meaning that all probabilities a i (resp. f i will be substituted by real values in [0, 1], produced by the underlying base policy (resp. perfect or noisy sensors). In Fig. 7 (right), the sensors may produce {0 :: fire(0, 1), 0 :: fire(0, \u22121), 1 :: fire(\u22121, 0), 0 :: fire(1, 0)}. % BASE POLICY a0::act(stay); a1::act(up); a2::act(down); a3::act(left); a4::act(right). % SENSORS f0::fire( 0, 1). f1::fire( 0,-1). f2::fire(-1, 0). f3::fire( 1, 0).", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Neurosymbolic reinforcement learning with formally verified exploration", "journal": "AAAI Press", "year": "2017", "authors": "[ References;  Achiam"}, {"ref_id": "b1", "title": "Ltl and beyond: Formal languages for reward function specification in reinforcement learning", "journal": "Curran Associates Inc", "year": "2016", "authors": "[ Bastani"}, {"ref_id": "b2", "title": "Safe reinforcement learning via shielding under partial observability", "journal": "", "year": "2019", "authors": "[ Carr"}, {"ref_id": "b3", "title": "Endto-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks. AAAI'19/IAAI'19/EAAI'19", "journal": "J. ACM", "year": "2003", "authors": "Richard M Murray; Joel W Burdick"}, {"ref_id": "b4", "title": "Foundations for restraining bolts: Reinforcement learning with ltlf/ldlf restraining specifications", "journal": "", "year": "2021", "authors": "De Giacomo"}, {"ref_id": "b5", "title": "Probabilistic (logic) programming concepts", "journal": "Mach. Learn", "year": "2015", "authors": "De Raedt;  Kimmig"}, {"ref_id": "b6", "title": "Statistical Relational Artificial Intelligence: Logic, Probability, and Computation", "journal": "Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool", "year": "2006", "authors": " [de Raedt"}, {"ref_id": "b7", "title": "Inference and learning in probabilistic logic programs using weighted Boolean formulas. Theory and Practice of Logic Programming", "journal": "", "year": "2015", "authors": " Fierens"}, {"ref_id": "b8", "title": "A comprehensive survey on safe reinforcement learning", "journal": "Journal of Machine Learning Research", "year": "2015", "authors": "Fern\u00e1ndez Garc\u00eda; Javier Garc\u00eda; Fernando Fern\u00e1ndez"}, {"ref_id": "b9", "title": "Reinforcement learning for temporal logic control synthesis with probabilistic satisfaction guarantees", "journal": "", "year": "2018", "authors": "Fern\u00e1ndez ; Javier Garc\u00eda; Fernando Garc\u00eda; ; Fern\u00e1ndez;  Giacobbe"}, {"ref_id": "b10", "title": "Verifiably safe exploration for end-to-end reinforcement learning", "journal": "", "year": "2020", "authors": "[ Hunt"}, {"ref_id": "b11", "title": "Temporal-logicbased reward shaping for continuing reinforcement learning tasks", "journal": "Curran Associates Inc", "year": "2017", "authors": " Jiang"}, {"ref_id": "b12", "title": "Robust Model Predictive Shielding for Safe Reinforcement Learning with Stochastic Dynamics", "journal": "", "year": "2020", "authors": "Shuo Bastani; Osbert Li; Wonjoon Bastani ; Farzan Memarian; Rudolf Goo; Ufuk Lioutikov;  Topcu"}, {"ref_id": "b13", "title": "Optlayer -practical constrained optimization for deep reinforcement learning in the real world", "journal": "Springer", "year": "1999", "authors": ""}, {"ref_id": "b14", "title": "Policy gradient methods for reinforcement learning with function approximation", "journal": "MIT Press", "year": "2000", "authors": " Sutton"}, {"ref_id": "b15", "title": "Yitao Liang, and Guy Van den Broeck. A semantic loss function for deep learning with symbolic knowledge", "journal": "", "year": "2018", "authors": ""}], "figures": [{"figure_label": "3", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 3 :3Figure 3: The domains of Stars, Pacman and Car Racing. The difference between Stars and Pacman is that fires in Stars are stable and the ones in Pacman move around.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 4 :4Figure 4: Trade-off between Violation (x-axis) and Return (yaxis). Each small data point is an agent's policy and the large data points are the average of five seeds. An ideal policy should lie in the upper-left corner.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 5 :5Figure 5: The episodic return and cumulative constraint violation of VSRL (left) and PLPG (right) agents in noisy environments. Left: The effect of is not significant. Right: Increasing \u03b1 gives a convex trend in both return and violation. The absolute numbers are listed in Appendix D.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 6 :6Figure6: Trade-off between Violation (x-axis) and Return (yaxis). Each data point is an agent's policy and the big data points are the average of five seeds.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 8 :8Figure 8: Left: A lap in Car Racing (Configuration 1). Middle: A CR image. We mark the sensors around the agent white but they not visible in training. Right: A CR state (downsampled from the middle).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "P\u03c0 + (safe|s) \u2265 P \u03c0 (safe|s) \u2200\u03c0 \u2208 \u03a0 and \u2200s \u2208 S Proof.P \u03c0 + (safe|s)Eq. (3) = a\u2208A \u03c0 + (a|s)P(safe|s, a) Eq.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 9 :9Figure 9: A circuit compiled from a program.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "/ 0.58 0.62 / 0.59 0.50 / 0.62 0.43 / 0.62 0.27 / 0.67 Car Racing1 0.93 / 0.42 0.99 / 0.19 0.82 / 0.19 0.83 / 0.18 0.47 / 0.52 Car Racing2 0.70 / 0.39 0.84 / 0.21 0.74 / 0.17 0.60 / 0.34 0.21 / 0.77 Table 5: The return and violation of different \u03b1 values in noisy environments (cf. Fig. 5.67 / 0.51 0.36 / 0.62 0.67 / 0.75 0.43 / 0.86 0.63 / 0.57 0.52 / 0.95 0.70 / 0.45 0.58 / 0.55", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Fig.5).", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 10 :10Figure 10: Performance of agents augmented with perfect sensors. Left: The average episodic return. Middle: The cumulative episodic constraint violation. Right: The step-wise policy safety, i.e. P\u03c0(safe|s).", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 11 :11Figure 11: Performance of agents augmented with noisy sensors. Left: The average episodic return. Middle: The cumulative episodic constraint violation. Right: The step-wise policy safety, i.e. P\u03c0(safe|s).", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Mar 2023 related properties. This is less demanding than requiring the full MDP be known required by many modelbased approaches [Jansen et al., 2020; Hunt et al., 2021; Carr et al., 2022]. \u2022 End-to-end Deep RL. By being differentiable, PLS allows for seamless application of probabilistic logic shields to any model-free reinforcement learning agent such as PPO [Schulman et al., 2018], TRPO [Schulman", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "/ 0.90 0.57 / 0.00 0.71 / 0.00 0.61 / 0.00 0.64 / 0.03 0.74 / 0.01 Stars2 0.50 / 0.89 0.43 / 0.58 0.44 / 0.42 0.43 / 0.59 0.46 / 0.52 0.46 / 0.35 Pacman1 0.74 / 0.92 0.72 / 0.74 0.85 / 0.65 0.71 / 0.72 0.75 / 0.65 0.71 / 0.65 Pacman2 0.56 / 0.82 0.56 / 0.62 0.59 / 0.58 0.46 / 0.67 0.51 / 0.62 0.62 / 0.59 Car Racing1 0.77 / 0.60 0.74 / 0.84 0.62 / 0.21 0.83 / 0.82 0.88 / 0.47 0.83 / 0.18", "figure_data": "Return/ViolationNo SafetyPerfect SensorsNoisy SensorsPPOVSRLPLPGVSRL-VSRLPLPGStars1 0.68 Car Racing2 0.58 / 0.55 0.55 / 0.67 0.65 / 0.17 0.67 / 0.51 0.70 / 0.45 0.74 / 0.17"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Normalized return and violation for all agents (cf. the large data points in Fig.4).", "figure_data": "Return/ViolationPerfect SensorsNoisy SensorsOnly Safety Grad Only Policy GradBothOnly Safety Grad Only Policy GradBothStars10.63 / 0.520.66 / 0.000.71 / 0.000.61 / 0.470.95 / 0.290.74 / 0.01Stars20.43 / 0.600.51 / 0.530.44 / 0.420.44 / 0.620.46 / 0.620.46 / 0.35Pacman10.72 / 0.890.87 / 0.650.85 / 0.650.54 / 0.880.84 / 0.640.71 / 0.65Pacman20.51 / 0.790.66 / 0.580.59 / 0.580.39 / 0.780.60 / 0.580.62 / 0.59Car Racing11.00 / 0.140.91 / 0.580.62 / 0.210.86 / 0.250.93 / 0.420.83 / 0.18Car Racing20.67 / 0.200.76 / 0.320.65 / 0.170.56 / 0.290.70 / 0.390.74 / 0.17"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Analysis of PLPG gradients (cf. the large data points in Fig.6).", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "The return and violation of different values in noisy environments (cf.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "P (w k ) = fi\u2208w k p i fi \u2208w k (1 \u2212 p i ).(1)", "formula_coordinates": [2.0, 106.86, 464.37, 190.15, 20.81]}, {"formula_id": "formula_1", "formula_text": "m i=1 p i \u2264 1 [De", "formula_coordinates": [2.0, 54.0, 554.86, 243.0, 22.66]}, {"formula_id": "formula_2", "formula_text": "J(\u03b8) = E \u03c0 \u03b8 [ \u221e t=0 \u03b3 t r t ]", "formula_coordinates": [2.0, 392.94, 321.15, 87.12, 30.2]}, {"formula_id": "formula_3", "formula_text": "\u2207 \u03b8 J(\u03b8) = E \u03c0 \u03b8 [ \u221e t=0 \u03a8 t \u2207 \u03b8 log \u03c0 \u03b8 (s t , a t )](2)", "formula_coordinates": [2.0, 355.36, 417.87, 202.64, 30.2]}, {"formula_id": "formula_5", "formula_text": "E \u03c0 + \u03b8 [ \u221e t=0 \u03a8 t \u2207 \u03b8 log \u03c0 + \u03b8 (a t |s t )](5)", "formula_coordinates": [4.0, 114.86, 312.83, 182.14, 30.2]}, {"formula_id": "formula_6", "formula_text": "\u2212E \u03c0 + \u03b8 [\u2207 \u03b8 log P \u03c0 + (safe|s)](6)", "formula_coordinates": [4.0, 118.04, 531.86, 178.96, 13.33]}, {"formula_id": "formula_7", "formula_text": "E \u03c0 + \u03b8 [ \u221e t=0 \u03a8t\u2207 \u03b8 log \u03c0 + \u03b8 (at|st) \u2212 \u03b1\u2207 \u03b8 log P \u03c0 + \u03b8 (safe|st)] (7)", "formula_coordinates": [4.0, 64.77, 624.45, 232.23, 26.81]}, {"formula_id": "formula_8", "formula_text": "\u03c0 + (a|s) = 1 [(s,a)\u2208SAsafe] \u03c0(a|s) a \u2208A 1 [(s,a )\u2208SAsafe] \u03c0(a |s) (8)", "formula_coordinates": [4.0, 358.4, 282.39, 199.6, 33.54]}, {"formula_id": "formula_9", "formula_text": "\u2207 \u03b8 J(\u03b8) = E \u03c0 + \u03b8 [ \u221e t=0 \u03a8 t \u2207 \u03b8 log \u03c0 \u03b8 (s t , a t )](9)", "formula_coordinates": [4.0, 354.45, 378.42, 203.55, 30.2]}, {"formula_id": "formula_10", "formula_text": "0, 0]), accelerate ([0, 1, 0]), brake ([0, 0, 0.8]), turn-left ([\u22121, 0, 0]), turn-right ([1, 0, 0]).", "formula_coordinates": [11.0, 54.0, 368.36, 243.0, 19.92]}], "doi": ""}