{"title": "Iterative Document-level Information Extraction via Imitation Learning", "authors": "Yunmo Chen; William Gantt; Weiwei Gu; Tongfei Chen; Aaron Steven White; Benjamin Van Durme", "pub_date": "", "abstract": "We present a novel iterative extraction model, ITERX, for extracting complex relations, or templates (i.e., -tuples representing a mapping from named slots to spans of text) within a document. Documents may feature zero or more instances of a template of any given type, and the task of template extraction entails identifying the templates in a document and extracting each template's slot values. Our imitation learning approach casts the problem as a Markov decision process (MDP), and relieves the need to use predefined template orders to train an extractor. It leads to state-of-the-art results on two established benchmarks -4-ary relation extraction on SCIREX and template extraction on MUC-4 -as well as a strong baseline on the new BETTER Granular task. 1   ", "sections": [{"heading": "Introduction", "text": "A variety of tasks in information extraction (IE) require synthesizing information across multiple sentences, up to the length of an entire document. The centrality of document-level reasoning to IE has been underscored by an intense research focus in recent years on problems such as argument linking (Ebner et al., 2020;Li et al., 2021, i.a.), -ary relation extraction Yao et al., 2019;Jain et al., 2020, i.a.), and -our primary focus -template extraction (Du et al., 2021b;Huang et al., 2021, i.a.).\nConstrued broadly, template extraction is general enough to subsume certain other document-level extraction tasks, including -ary relation extraction. Motivated by this consideration, we propose to treat these problems under a unified framework of generalized template extraction ( \u00a72). 2 Figure 1 shows 4-ary relations from the SCIREX dataset (Jain et al., 2020), presented as simple templates.   Lei et al. (2018)) from the SCIREX dataset. An agent reads the entire paper and iteratively generates templates, each consisting of slots for Task, Method, Dataset, and Metric.\nSince documents typically describe multiple complex events and relations, template extraction systems must be capable of predicting multiple templates per document. Existing approaches such as Du et al. (2021b) and Huang et al. (2021) rely on a linearization strategy to force models to learn to predict templates in a pre-defined order. In general, however, such orderings are arbitrary. Others have instead focused on the simplified problem of role-filler entity extraction (REE), which entails extracting all slot-filling entities but does not involve mapping them to individual templates (Patwardhan and Riloff, 2009;Du et al., 2021a, i.a.).\nWe present a new model for generalized template extraction, ITERX, that iteratively extracts multiple templates from a document without requiring a pre-defined linearization scheme. We formulate the problem as a Markov decision process (MDP, \u00a72), where an action corresponds to the generation of a single template ( \u00a73.2), and states are sets of predicted templates ( \u00a73.3). Our system is trained via imitation learning, where the agent imitates a dynamic oracle drawn from an expert policy ( \u00a73.4). Our contributions can be summarized as follows:\nCol. Isaacs said that the guerrillas attacked the \"La Eminencia\" farm located near the \"Santo Tomas\" farm, where they burned the facilities and stole food He also reported that the guerrillas killed a peasant in the city of Flores, in the northern El Pet\u00e9n department, and burned a tank truck. In early September, illegal border crossings by two people infected with COVID-19, triggered a week-long lockdown of another Yunnan border city, Ruili, and prompted at least eight prefectures and 25 counties to enter \"wartime status.\" Following the incident, Yunnan vowed to strengthen border patrols.\nTwenty people have been sentenced to prison in Southwest China's Yunnan Province for crimes relating to illegal immigration from Myanmar to China during the COVID-19 pandemic, Yunnan's high people's court reported on September 28.   and BETTER Granular (right) datasets. Event triggers (e.g. burned above) are not annotated in MUC-4 and are highlighted here only for clarity.\n\u2022 We show that generalized template extraction can be treated as a Markov decision process, and that imitation learning can be effectively used to train a model to learn this process without making explicit assumptions about template orderings. \u2022 We demonstrate state-of-the-art results with ITERX on two established benchmarks for complex relation extraction: 4-ary relation extraction on SCIREX and template extraction on MUC-4. \u2022 We introduce strong baselines for the recently introduced English BETTER Granular template extraction task.", "publication_ref": ["b10", "b45", "b9", "b22", "b25", "b9", "b19", "b33"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Problem Formulation", "text": "We propose to treat both classic template extraction and -ary relation extraction under a unified framework of generalized template extraction.\nGiven a document = ( 1 , \u2022 \u2022 \u2022 , )\nwhere each is a token, we assume that some system (or model component) generates a set of candidate mention spans X = { 1 , \u2022 \u2022 \u2022 , }, where each = [ : ] \u2208 X is contiguous with left and right span boundary indices and .\nWe define a template ontology as a set of template types T , where each type \u2208 T is associated with a set of slot types . A template instance is defined as a pair ( , {( : ), \u2022 \u2022 \u2022 }) where \u2208 T is a template type, \u2208 is a slot type associated with , and \u2286 X is a subset of all mention spans extracted from the document that fills slot type ( = \u2205 indicates that slot has no filler). 3 Taking Figure 2 (left) as an example, Template 1 has type = Arson and slots {PerpretratorIndiv : {\"guerrillas\"}, PhysicalTarget : {\"facilities\"}}.\nWe reduce the problem of extracting a single template to the problem of assigning a slot type to each extracted span \u2208 X , where some spans may be assigned a special null type ( ), indicating that they fill no slot in the current template. Given this formulation, we can equivalently specify a template instance as ( , ) where is an assignment of spans to slot types: { : } \u2208 . We denote the union of all slot types across all template types, along with the empty slot type , as S = { } \u222a \u2208T . With these definitions in hand, the problem of generalized template extraction can be stated succinctly: Given a template ontology T , a document , and a set of candidate mentions X extracted from , generate a set of template instances\n{( 1 , 1 ), \u2022 \u2022 \u2022 , ( , )}, where \u2208 T .\nAs an MDP We treat generalized template extraction as a Markov decision process (MDP), where each step of the process generates one whole template instance. For simplicity, we consider the problem of extracting templates of a specific type \u2208 T ; extracting all templates then simply requires iterating over T , where |T | is typically very small. This MDP (2 A , A, , ) comprises the following: 4\n\u2022 2 A : the set of states. In our case, this is the set of all template generation histories. Each state \u2286 A is a set of generated templates;\n\u2022 A: the set of actions or assignments: an action is the generation of a single template (an assignment of slot types to spans);\n\u2022 : the environment that dictates state transitions.\nHere, each transition simply adds a generated template to the set of all templates generated for the document: ( , ) = \u222a { };\n\u2022 ( , ): the reward from action under the current state .\nThese components are detailed in the following section. Figure 3 shows ITERX in action: the MDP 4 Our notation is consistent with prior NLP work that uses MDPs, e.g., Levenshtein Transformers (Gu et al., 2019 produces two templates sequentially, terminating on a null assignment to all input spans in X .", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}, {"heading": "Model", "text": "Our ITERX model is a parameterized agent that makes decisions under the MDP above: conditioned on an input document , spans X extracted from , and a specific template type , ITERX generates a single template of type at each step. The model consists of two parameterized components:\n\u2022 Policy : A policy ( | , X) that generates a distribution over potential assignments of spans to slots in the current template of type ;\n\u2022 State transition model : An autoregressive state encoder that maps a state (i.e., a set of predicted templates) to a continuous representation X via a state transition model , where X ( +1) = (X ( ) , ). Here the state representation X = (x 1 , \u2022 \u2022 \u2022 , x ) comprises of a vector x \u2208 R for each span .\nITERX generates a sequence of templates: It starts with initial state X (0) (see Figure 3 for a running example) comprising only span representations derived from the encoder (as no templates have been predicted) and ends when no new template is generated ( \u00a73.5). ITERX is trained via imitation learning, aiming to imitate an expert policy * derived from reference templates.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Span Extraction and Representation", "text": "ITERX takes spans as input and thus relies on a span proposal component to obtain candidate spans. 5 For all experiments, we use the neural CRFbased span finder employed for FrameNet parsing in Xia et al. (2021) and for the BETTER Abstract task in Yarmohammadi et al. (2021). 6 CRF-based span finders have been empirically shown to excel at IE tasks (Gu et al., 2022). The input document is first embedded using a pretrained Transformer encoder (Devlin et al., 2019;Raffel et al., 2020) that is fine-tuned during model training. 7 Each span = [ : ] extracted by the span extractor is encoded as a vector x enc , which is obtained by first concatenating three vectors of dimension : the embeddings of the first and the last tokens in the span, and a weighted sum 5 Although many systems since Lee et al. (2017) that require spans as input have trained span proposal modules endto-end, we found this to be unnecessary to obtain strong results and leave this extension for future work. 6 The code of the span finder can be found at github.com/hiaoxui/span-finder. We refer the reader to these papers for further details.\n7 Different encoders are used in \u00a74 for fair comparison with prior work. For documents exceeding maximum length max = 1024, tokens are encoded in chunks of size max .\nof the embeddings of all tokens within the span, using a learned global query (Lee et al., 2017). This 3 -dimensional vector is then compressed to size using a two-layer feedforward network. Lastly, to incorporate positional information, we add sinusoidal positional embeddings based on the tokenlevel offset of within the document to yield x enc .", "publication_ref": ["b44", "b46", "b18", "b7", "b36", "b24", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Policy: Generating a Single Template", "text": "A policy generates a single template given span states X = (x 1 , \u2022 \u2022 \u2022 , x ) and template type \u2208 T , conditioned on the document and all of its candidate mention spans.\nSince an action represents a set of slot type assignments for all candidate mentions, the policy ( | , X) can be factorized as\n( | , X) = ( : ) \u2208 ( | , x) .(1)\nThus we only need to model the slot type distribution for each candidate span. Here, we employ two models described below.\nIndependent Modeling We train a classifier that outputs a slot type (or ) given both the template type embedding t and the slot type embedding s, inspired by a standard practice in binary relation extraction (Ebner et al., 2020;Lin et al., 2020, i.a.).\nIt computes the probability with a two-layer feedforward network (FFN), with slots not associated with the template type (i.e., \u2209 \u222a { }) assigned 0 probability:\nind ( | , x) \u221d 1 \u2208 \u222a{ } \u2022exp(s T \u2022FFN( [t; x])) (2)\nwhere [\u2022 ; \u2022] denotes vector concatenation.\nJoint Modeling Following Chen et al. (2020), we create a model that jointly considers all candidate spans given the template type. We begin by prepending t to the sequence of span states X to yield the sequence (t, x 1 , \u2022 \u2022 \u2022 , x ). This sequence is fed to a different Transformer encoder, which naturally models interactions both between spans and between a span and the template type via selfattention (Vaswani et al., 2017):\n(t,x 1 , \u2022 \u2022 \u2022,x ) = Transformer(t, x 1 , \u2022 \u2022 \u2022, x ) (3)\nWe emphasize that the inputs to the Transformer are embeddings of spans (see \u00a73.1) and not tokens, following Chen et al. (2019Chen et al. ( , 2020. 8 For each x , we pass the representationx output by the Transformer to a linear layer with output size |S |, the total number of slot types. A softmax activation is then applied over all slot types that are valid for (i.e., \u2208 \u222a { }), with invalid types masked out, yielding the following distribution:\njoint ( | , x) \u221d 1 \u2208 \u222a{ } \u2022 exp(s Tx )(4)", "publication_ref": ["b10", "b3", "b41", "b2", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "State Transition Model", "text": "A state transition model models the environment ( , ). Recall that a state transition just consists in the generation of a single template, where the current state is the set of all templates that have been generated up to the current step.\nHere, we propose a neural model that produces a representation of . Specifically, we model as a sequence of vectors X mem ( ) \u2208 R \u00d7 -one -dimensional state vector for each of the candidate spans \u2208 X . Each state vector x mem \u2208 R acts as a span memory, tracking the use of that span across generated templates. We model state transitions using a single gated recurrent unit (GRU; Cho et al., 2014). Given the current template assignment ( : ) \u2208 of a slot type to a span , the state transition for \u2032 = \u222a { } is given as follows:\nx \u2032 mem = GRU(x mem , [s ;t]) if \u2260 ; x mem if = . (5\n)\nwheret is a template embedding given byt = t when using the independent policy model given in Equation 2 and given by Equation 3 when using the joint model. Intuitively,t is a summarized vector of the current template, akin to the role of the [CLS] token employed in BERT (Devlin et al., 2019). Here, we use a concatenation of the slot type embedding s and the template vectort as the input to the state transition GRU to track the use of the span.\nThe input representation of a span at each step is simply x = x enc + x mem -the sum of the original span embeddings x enc described in \u00a73.1 and the current memory vector x mem .", "publication_ref": ["b5", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Policy Learning", "text": "We use direct policy learning (DPL), a type of imitation learning, to train our model. DPL entails training an agent to imitate the behavior of an interactive demonstrator as given by optimal actions * drawn from some expert policy * , a proposal distribution over actions. This expert policy is computed dynamically based on the current state of the agent, as we describe below. For this reason, the interactive demonstrator is sometimes referred to as a dynamic oracle (Goldberg and Nivre, 2012).\nThe log-likelihood of the oracle action under the ITERX policy model is the reward. This ensures that the learning problem can be optimized directly using gradient descent, where the objective is given by the expected reward:\nE * \u223c * \u223c\u02dc \u221e \u2211\ufe01 =0 log ( * | , X( ))(6)\nHere, is a discount factor,\u02dcis the mixed policy, and states are repeatedly sampled from their induced state distribution\u02dc. The mixed policy\u02dcis a mixture of the expert policy and the agent's policy (Ross et al., 2011). Sampling from\u02dccan thus be described as first sampling some \u2208 {0, 1}, then sampling from the agent's parameterized policy if = 1, or sampling an action from the dynamic oracle * if = 0:\n\u223c Bernoulli( ); \u223c + (1 \u2212 ) * .(7)\nHere , the agent roll-out rate, or the agent policy mixing rate, is a hyperparameter that controls the probability of the agent following its own policy vs. the dynamic oracle. This process resembles scheduled sampling (Bengio et al., 2015), a technique commonly employed in training models for sequence generation tasks like machine translation: when updating decoder hidden states, either the gold token * or the predicted token\u02c6may be used, and the decision is made via a random draw. Here, the difference is that we are generating templates at each step instead of tokens.", "publication_ref": ["b15", "b37", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Expert Policy", "text": "We construct an expert policy based on the agent's policy. At training time, given the set of gold templates * and the current state (all templates predicted thus far), the set\u00af= * \\ contains all gold templates not yet predicted. Our expert policy is formulated as\n* ( | , X) \u221d log ( | ,X)/ if \u22080 if \u2209\u00af,(8)\nwhere is a temperature parameter. Intuitively, our expert policy seeks to \"please\" the agent: a (viable) action's probability under the expert policy is proportional to the probability under the agent's policy. Temperature controls concentration: \u2192 0 + reduces it to a point distribution over a single action and \u2192 \u221e results in equal probability assigned to all remaining gold templates.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Inference", "text": "Although many search algorithms for sequence prediction can be employed (e.g. beam search, A*), we find greedy decoding to be effective, and leave further exploration for future work. Setting the initial state (0) = \u2205, we take actions (i.e., generate templates) by greedy decoding\u02c6= arg max ( | , X) for every step. Decoding stops when all spans are assigned the null slot type in\u02c6.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We evaluate ITERX on three datasets: SCIREX (Jain et al., 2020), MUC-4 (Grishman and Sundheim, 1996), and BETTER Phase II English Granular. 9 SCIREX is a challenge dataset for 4-ary relation extraction 10 on full academic articles related to machine learning. MUC-4 and Granular are both traditional template extraction tasks, though they differ in important respects, which we discuss in Appendix C. For summary statistics, see  ", "publication_ref": ["b22"], "figure_ref": [], "table_ref": []}, {"heading": "Baselines", "text": "GTT (Du et al., 2021b) To our knowledge, this is the only prior work to have attempted full template extraction in recent years, and it is thus our primary baseline for comparison on MUC-4. 11 GTT first prepends the document text with the valid template types, then passes the result to a BERT encoder. A Transformer decoder (whose parameters are shared with the encoder) then generates a linearized sequence of template instances.\nTEMPGEN (Huang et al., 2021) ", "publication_ref": ["b9", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Metrics", "text": "The currently used metric for template extraction and REE on MUC-4 is CEAF-REE, proposed in Du et al. (2021a) and then used in Du et al. (2021b) and Huang et al. (2021). 12 CEAF-REE is based on the CEAF metric (Luo, 2005) for coreference resolution, that computes an alignment between gold and predicted entities that maximizes a measure of similarity between aligned entities (e.g. CEAF 4 in coreference resolution). This alignment is subject to the constraint that each reference entity is aligned to at most one predicted entity. The CEAF-REE implementation (henceforth, CEAF-REE impl ) employed in Du et al. (2021a,b) and Huang et al. (2021) unfortunately departs from the stated metric definition (CEAF-REE def ) in two ways: (1) it eliminates the constraint on entity alignments and (2) it treats the template type as an additional slot when reporting cross-slot averages. For maximally transparent comparisons to prior work, we report scores under both CEAF-REE def and CEAF-REE impl , obtaining state-of-the-art results on MUC-4 with each.\nHowever, we argue that neither CEAF-REE def nor CEAF-REE impl is consistent with historical evaluation of template extraction systems. CEAF-REE def errs in enforcing the entity alignment constraint: doing so effectively requires systems to perform coreference resolution, which is too strict and runs contrary to the original MUC-4 evaluation.\nIn the interest of clarity, we define a modified version of the CEAF-REE metric that avoids both pitfalls: it relaxes the entity alignment constraint and it does not include template type in cross-slot averages. We call this version CEAF-RME, where \"M\" stands for mention and emphasizes the focus on mention-level rather than entity-level (\"E\") scoring. Intuitively, relaxing this constraint amounts to placing the burden of coreference resolution on the metric: if the scorer aligns two predicted mentions to the same reference entity, the mentions are implicitly deemed coreferent.\nNote that for a CEAF-family metric, the similarity function for entities ( , ) between the reference entity and the predicted is arbitrary (Luo, 2005). In Du et al. (2021a), CEAF-REE impl uses\n\u2286 ( , ) = 1[ \u2286 ].\nWe argue that \u2286 overly penalizes models for predicting incorrect mentions, as even a single incorrect mention reduces the score to 0. Instead, a better choice is 3 ( , ) = | \u2229 | from Luo (2005): this computes a micro-average score of all mentions, and it adequately assigns partial credit to the overlap between the predicted mention set and the reference mention set. See Figure 4 for a succinct comparison among these variants. 13 A more detailed discussion can be found in Appendix D.\nFor SCIREX, we report CEAF-RME (under both 3 and \u2286 ). For MUC-4, we report all metrics so that fair comparison with prior work can be made. For BETTER Granular, we use its official metrics, described in Appendix D.     (Raffel et al., 2020) achieves roughly 2\u00d7 the performance of TEMPGEN 15 (see Table 2).\nE n t i t y a l i g n m e n t REE def RME \u03c6 REE impl RME \u03c6 3 ! relaxed relaxed relaxed strict T y p e n o t a s l o t \u2714 \u2714 \u2717 \u2714 C l u s t e r s i m i l a r i t y \u03c6 3 \u03c6 ! \u03c6 ! \u03c6 !", "publication_ref": ["b8", "b9", "b19", "b28", "b19", "b28", "b8", "b28", "b36"], "figure_ref": ["fig_1"], "table_ref": ["tab_8"]}, {"heading": "MUC-4", "text": "Under the most comparable setting, ITERX outperforms GTT under all metrics by 1-2%, both using BERT base (Table 3). With T5 enc large , ITERX obtains even better performance, with most gains coming from increased precision. 16 Furthermore, we note a consistent gap of \u2248 5% F 1 be-tween CEAF-RME \u2286 and CEAF-REE impl , 17 which we suspect is due to CEAF-REE impl 's inclusion of scores for template type into the aggregated slot F 1 : as template type scores are higher across models than slot type scores, they are liable to inflate the aggregate score.\nBETTER Granular We report scores on the English-only Phase II BETTER Granular task using the official BETTER scoring metric in Table 4. Given the complexity of the Granular task, the accompanying difficulty of developing models to perform it, and the lack of existing work on Granular, we report scores only for ITERX under T5 enc large . We intend these to serve as a solid baseline against which future work may be measured.\nTemplate Slot Combined P R F 1 P R F 1\n89.7 74.5 81.4 41.0 33.5 36.9 30.0 We next conduct ablations to examine how specific aspects of ITERX's design affect learning. Here, we focus on SCIREX as a case study, as it has the highest average templates per document of the three datasets, allowing us to best investigate the behavior of ITERX over long action sequences.\nRecall that the dynamic oracle specifies an expert policy * (Equation 8) from which expert actions * are drawn. One design decision concerns the agent roll-out rate, , which controls how often we draw from the expert policy vs. the agent policy when making updates. Another decision concerns how entropic this policy distribution should be, controlled by the temperature . Both decisions reflect a trade-off between exploration and exploitation in the space of action sequences.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_9", "tab_10"]}, {"heading": "Agent Roll-out Rate", "text": "We show how model performance changes as we increase the agent rollout rate \u2208 [0, 1] in Figure 5, where = 0 to always following the expert policy, and = 1 corresponds to always updating based on the agent's own policy. The model performs poorly under low , but improves quickly as increases, reaching a plateau past \u2265 0.5. The results are intuitive, as relying more on the expert (lower ) for learning would result in a fixed and deterministic set of states that may hinder the agent from visiting new states, which are often encountered at test time. With higher , the agent's behavior is more consistent between train and test time. ", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Temperature", "text": "We compare the following four settings for sampling from * , keeping = 0 to control for effects of policy mixing:\n\u2022 FIXED: Select the next template in the document based on the order as is given in the dataset. In this case, does not come into play. This setting corresponds to the standard practice of using fixed template linearizations (Du et al., 2021b;Huang et al., 2021).\n\u2022 \u2192 0 + (ARGMAX): Select the template that maximizes the likelihood with the systempredicted distributions over slots.", "publication_ref": ["b9", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "\u2022", "text": "= 1 (XENT): Sample a template according to the distribution defined by the cross entropy between references and predictions.\n\u2022 \u2192 \u221e (UNIFORM): Sample a template uniformly from the correct template set.\nTest set performance for each setting is shown in Table 5. The results for CEAF-RME \u2286 show a trade-off in precision and recall corresponding to the exploitation-exploration trade-off induced by , with the higher (more exploration) of XENT and UNIFORM, yielding higher recall. The trend for CEAF-RME 3 is similar, though less pronounced. Interestingly, while CEAF-RME \u2286 F 1 scores are consistent across settings, CEAF-RME 3 F 1 scores are higher under higher temperature settings. To the extent that the more entropic settings conduce to higher template and mention recall, we would expect these settings to yield more partial-credit template alignments (under the 3 similarity function) than non-random settings, which tend to focus on correct prediction of fewer templates -thus potentially missing templates entirely and receiving no partial credit.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_11"]}, {"heading": "Related Work", "text": "Template Extraction The term template extraction was originally proposed in the Message Understanding Conferences (MUC; Sundheim, 1991, i.a.) to describe the task of extracting templates from articles. Researchers later focused more heavily on sentence-level IE, especially after the release of the ACE 2005 dataset (Walker et al., 2006). But following renewed interest in document-level IE, researchers (Du et al., 2021b;Huang et al., 2021;Gantt et al., 2022, i.a.) have begun to revisit MUC and to develop new template extraction datasets (notably, BETTER Granular).\nTraditionally, template extraction comprises two sub-tasks: template identification, in which a system identifies and types all templates in a document, and slot filling or role-filler entity extraction (REE), in which the slots associated with each template are filled with extracted entities. Much recent work in this domain has turned away from the full task, focusing only on REE, which is tantamount to assuming that there is just a single aggregate template per document (Patwardhan and Riloff, 2009;Riloff, 2011, 2012;Du et al., 2021a;Huang et al., 2021).", "publication_ref": ["b43", "b9", "b19", "b33", "b8", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Document-Level Relation Extraction", "text": "Alongside template extraction, there has been considerable recent interest within IE in various challenging document-level relation extraction objectives, beyond the longstanding and dominant focus on coreference resolution. Argument linking -a generalization of semantic role labeling (SRL; Gildea and Jurafsky, 2002) in which a predicate's extrasentential arguments must also be labeled -is one notable example, and has attracted recent attention through the RAMS (Ebner et al., 2020) and WikiEvents (Li et al., 2021) benchmarks. 18 Prior benchmarks on this task include SemEval 2010 Task 10 (Ruppenhofer et al., 2010), Beyond Nombank (Gerber and Chai, 2010), ONV5 (Moor et al., 2013), and Multi-sentence AMR (O'Gorman et al., 2018). A separate line of work has concentrated on general N-ary relation extraction challenge tasks, in which entities participating in the same relation may be scattered widely throughout a document. Beyond SCIREX, PubMed Peng et al., 2017) and DocRED (Yao et al., 2019) are two other prominent benchmarks in this area.\nImitation Learning Our approach casts the problem of generalized template extraction as a Markov decision process. SEARN (Daum\u00e9 III et al., 2009) and other related work (Ross et al., 2011;Venkatraman et al., 2015;Chang et al., 2015, i.a.) have considered structured prediction under a reinforcement learning setting. Notably, in dependency parsing, Goldberg and Nivre (2012) proposed the use of a dynamic oracle to guide an agent toward the correct parse (see \u00a73.4).\nWe also employ direct policy learning for optimization of the template extraction MDP, thus reducing the problem to one of supervised sequence learning that is amenable to gradient descent. Such treatment is reminiscent of other similar techniques in NLP. Scheduled sampling (Bengio et al., 2015), for instance, trains a sequence generator with an expert policy consisting of a mixture of the predicted token and the gold token. Relatedly, Levenshtein Transformers (Gu et al., 2019) learn to edit a sequence by imitating an expert policy based on the Levenshtein edit distance.", "publication_ref": ["b14", "b10", "b26", "b13", "b29", "b31", "b34", "b45", "b6", "b37", "b42", "b15", "b0", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We have presented ITERX, a new model for generalized template extraction that iteratively generates templates via a Markov decision process. ITERX demonstrates state-of-the-art performance on two benchmarks in this domain -4-ary relation extraction on SCIREX and template extraction on MUC-4 -and establishes a strong baseline on a third benchmark, BETTER Granular. In our experiments, we have also shown that imitation learning is a viable paradigm for these problems. We hope that our findings encourage future work to confront the challenge of dealing with documents that describe multiple complex events and relations head-on, rather than veiling this difficulty behind simplified task formulations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "Although we believe our iterative extraction paradigm to be promising, we acknowledge that this work is not without limitations. First, ITERX features a significant number of hyperparameters. We found that these generally required some effort to tune for specific datasets, and that there was no single configuration that was uniformly the best across domains. We showed the impact of manipulating some of these hyperparameters in \u00a75. Second, our ITERX implementation iterates over all template types in the template ontology during training and inference, which means that runtime grows linearly in the number of template types. While our framework could in principle support template type prediction as well (which would reduce this to (1)), it does not do so in practice, and hence runtime may be long for large ontologies. However, we again stress that actual template ontologies tend to be small.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Terminology", "text": "Information Extraction is rife with vague and competing terms for similar concepts, and we recognize some hazard in introducing generalized template extraction (GTE) into this landscape. To head off possible confusion, we highlight two important differences between this problem and the well established problem of event extraction (EE).\nFirst, EE requires identifying lexical event triggers, whereas GTE does not, as template instances do not necessarily have one specific lexical anchor. A document describing a terrorist attack may only explicitly describe a series of bombings, and a document describing an epidemic may only explicitly state that thousands of people have contracted a particular disease. This property holds of all three datasets we focus on, and can be seen in both Figure 1 and Figure 2. Template anchors are not annotated either for MUC-4 or for SCIREX. And while they are annotated for BETTER Granular, they do not factor into scoring. This contrasts with major EE datasets, such as ACE or PropBank, for which typed lexical triggers must be extracted.\nSecond, we take GTE to be a fundamentally document-level task: templates concern events described over an entire document. In practice, EE has historically referred to extraction of predicateargument structures within a single sentence. One could conceivably argue that this usage has begun to change with the recent interest in argument linking datasets like RAMS (Ebner et al., 2020) and WikiEvents (Li et al., 2021), in which arguments may appear in different sentences from the one containing their predicate. Even so, these crosssentence arguments are still arguments of a particular predicate, in a particular sentence. Moreover, the overwhelming majority of arguments in these datasets are sentence-local (Ebner et al., 2020). As emphasized above, templates are not necessarily anchored to particular lexical items. For this reason, they also do not necessarily exhibit the level of locality one finds in EE.\nThese differences are what motivate the use of CEAF-REE as an evaluation metric, in contrast to the precision, recall, and F1 scores for events and arguments that are typically reported for EE. In brief, it simply is not possible to compute these for GTE in the same way as they are computed for EE. We elaborate on this point in Appendix D.", "publication_ref": ["b10", "b26", "b10"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "B Model Training and Hyperparameters", "text": "We implemented our models in PyTorch (Paszke et al., 2019) and AllenNLP (Gardner et al., 2018). We trained all our models with a single NVIDIA RTX6000 GPU. For all experiments that reproduce prior works, we trained models until full convergence under the patience settings provided in the publicly released code. For all ITERX models, we trained and tuned hyperparameters under our grid's limit of 24 hours per run, with which we were able to obtain solid performance on all datasets. We performed hyperparameter search manually and report the best performing hyperparameters and the bounds we searched in Table 6, Table 7, and Table 8.", "publication_ref": ["b32", "b12"], "figure_ref": [], "table_ref": ["tab_13", "tab_15", "tab_17"]}, {"heading": "C Dataset Details", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1 MUC-4", "text": "The MUC-4 dataset features a total of 1,700 English documents (1,300 for train and 200 each for dev and test) concerning geopolitical conflict and terorrism in South America. Documents are annotated with templates of one of six kinds -Attack, Arson, Bombing, Murder, Robbery, and ForcedWorkStoppage -and may have multiple templates (often of the same type) or no templates at all. All templates contain the same    slots. While the original data contains numerous slots, it has become standard practice to evaluate systems on just five of these (apart from the slot for the template's type), all of which take entity-valued fillers: Perpetrator(Individual), Perpetrator(Organization), Victim, Weapon, and Target.\n, BERTbase} LR 3 \u00d7 10 \u22125 {1 \u00d7 10 \u22125 , 3 \u00d7 10 \u22125 , 5 \u00d7 10 \u22125 } Encoder LR 1 \u00d7 10 \u22125 1 \u00d7 10 \u22125 0.6 [0, 1] 1.0 {0, 1.0, \u221e} 1.0 {1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.2 BETTER Granular", "text": "The BETTER Granular dataset contains documents spanning a number of domains, and, like MUC-4, focuses on six types of complex event, though covering different topics: protests, epidemics, natural disasters, acts of terrorism, incidents of corruption, and (human) migrations. However, Granular is substantially more difficult than MUC-4 in several ways. First, each template type is associated with a distinct set of slots. Second, only some of the slots take entities as fillers, whereas others take events, boolean values, or one of a fixed number of strings.  Finally, the formal evaluation setting for Granularwhich we do not adopt in this paper -is zero-shot and cross-lingual: systems trained only on English documents are evaluated exclusively on documents in a different target language. 19 The data used in our experiments is English-only and comprises the \"train,\" \"analysis,\" and \"devtest\" splits from Phase II of the BETTER program, for which the target language is Farsi.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.3 SCIREX", "text": "The 4-ary SCIREX relation extraction task seeks to idenfity entity 4-tuples that describe a metric used to evaluate a method applied to an ML task as realized by a specific dataset -e.g. (span F1, BERT, SRL, ACE 2005). The challenge of SCIREX lies not only in these pieces of information tending to be widely dispersed throughout an article, but also in the fact that only tuples describing novel work presented in the paper (and not merely cited work) are labeled as gold examples. Following Huang et al. (2021), we frame this as a template extraction task, treating each 4-tuple as a template with four slots.", "publication_ref": ["b19"], "figure_ref": [], "table_ref": []}, {"heading": "D Model Evaluation Details", "text": "A key consideration that arises in evaluating generalized template extraction is the need to align predicted and reference templates: a given predicted template may be reasonably similar to multiple different reference templates, and one must decide on a single template to use as the reference for each predicted one. Generalized template extraction is similar in this respect to coreference resolution, in which predicted entities may (partially) match multiple reference entities, and one must determine a ground truth alignment. Importantly, this consideration also renders metrics that are traditionally reported for event extraction -namely, event and argument precision, recall, and F 1 -inappropriate. This is because event extraction is fundamentally a span labeling problem, and the identity of the appropriate reference span is always clear for a given predicted span: either a reference span with the same boundary and type exists or it does not. 20 By contrast, the mapping from prediction to reference for templates is only this transparent in cases of perfectly accurate predictions.\nAll the evaluation metrics presented in this appendix are, at base, minimal extensions of precision, recall, and F 1 to cases where template alignments are both necessary and nontrivial. For CEAF-REE in particular, the various versions of the metric that we discuss (CEAF-RME 3 , CEAF-RME \u2286 , CEAF-REE def , and CEAF-REE impl ) merely reflect differences in how this alignment should be performed and whether the template type should be treated in the same way as slot types for reporting purposes.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.1 MUC-4", "text": "MUC-4 evaluation presents a special challenge, owing to its long and complicated history, and to terminological confusion. 21 Here, we discuss CEAF-REE (Du et al., 2021a), the current standard metric for MUC-4 evaluation. We begin with definitions, following with a discussion of some of its problems, and conclude with an extended presentation of our CEAF-RME variant, introduced in \u00a74.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "D.1.1 CEAF and CEAF-REE: Definitions", "text": "The CEAF-REE metric, introduced by Du et al. (2021a), has since been adopted as the standard evaluation metric for MUC-4 (Du et al., 2021b;Huang et al., 2021). To our knowledge, no official scoring script has ever been released for MUC-4, although the metrics used as part of the original evaluation are described in detail in Chinchor (1992). CEAF-REE does not attempt to implement these original metrics, but is rather a lightly adapted version of the widely used CEAF metric for corefer-ence resolution, proposed in Luo (2005). 22 CEAF computes an alignment between reference (R) and system-predicted (S) entities, with each entity represented by a set of coreferent mentions, and with the constraint that each predicted entity is aligned to at most one reference entity. This is treated as a maximum bipartite matching problem, in which one seeks the alignment that maximizes the sum of an entity-level similarity function ( , ) over all aligned entities \u2208 R and \u2208 S within a document. In principle, CEAF is agnostic to the choice of , though it is generally desirable that ( , ) = 0 when \u2208 such that \u2208 and that ( , ) = 1 when = , for reasons described in Luo (2005). In practice, the 4 similarity function is most commonly used, defined as the Dice coefficient (or F 1 score) between and : \nHere we see that 4 computes a version of macro-average over entities, whereas 3 computes a micro-average.\nThe CEAF that uses 4 is sensibly denoted CEAF 4 in coreference resolution. CEAF-REE impl differs from CEAF 4 in the following ways:\n\u2022 All entities are aligned within role, conditioned on matching template type. E.g. only predicted entities for the Victim slot in Bombing templates would be considered valid candidates for alignment with entities filling the Victim slot in reference Bombing templates.\n\u2022 A binary similarity function \u2286 is used, defined as follows: 22 Luo's motivations for proposing CEAF actually derive in large part from observed shortcomings with the original MUC-4 F 1 score. See Luo (2005) for details.\n\u2286 ( , ) := 1, if \u2286 0, otherwise(13)\nThis function \u2286 says that a model receives full credit (1.0) for a predicted entity if and only if its mentions form a subset of those in the reference entity. If even one incorrect mention is included, the model receives a score of zero for that entity.", "publication_ref": ["b9", "b19", "b4", "b28", "b28", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "D.1.2 CEAF-REE: Problems and Solutions", "text": "Our principal concerns with CEAF-REE lie with how it has so far been reported and implemented, and with challenges in extending it to the full template extraction task, in which multiple templates of the same type may be present in a document. We elaborate on two issues discussed briefly in \u00a74 and also introduce a third.\nFirst, previous work that reports CEAF-REE treats the template type merely as another slot, with template type labels treated as special kinds of \"entities\" that may fill this slot. This is not necessarily problematic in itself: template type-level metrics are valuable for evaluating system performance. However, it is problematic when reporting (micro or macro) average CEAF-REE figures across slots, as these works do. This is because incorporating the scores for template type into the average elides the distinction between roles (slots) and the kind of event being described (the template type). Moreover, the alignment between slot-filling entities is also already conditioned on a match between the template types. There are thus two distinct ways in which information about a system's predictive ability with respect to template type end up in a slot-level average CEAF-REE score. This results in reported values that are very difficult to interpret, and potentially misleading to the extent that these features of CEAF-REE implementations are not made apparent in writing.\nSecond, the constraint that at most one predicted entity be aligned to each reference entitystipulated in the metric definition (CEAF-REE def ) -is not enforced in the implementation (CEAF-REE impl ). Practically, this means that the alignment shown in Figure 6 would receive full credit, whereas it ought to receive a precision score of only 0.75, as Du et al. (2021a) describe. As we argue in \u00a74, we believe this constraint to be overly strict. But this point aside, the discrepancy between definition and implementation is problematic in itself.\nThird, full template extraction introduces a second maximum bipartite matching problem, which requires aligning predicted and reference templates of the same type, and which CEAF-REE (either CEAF-REE def or CEAF-REE impl ) is not natively Figure 6: An example alignment between predicted and reference entities from Du et al. (2021a). In past implementations of CEAF-REE, this alignment would receive full credit, rather than being penalized for precision ( = 0.75).\nequipped to handle, given that it operates at the level of slots. Du et al. (2021b) reports CEAF-REE for GTT under an optimal template alignment, but this is obtained via brute-force, enumerating and evaluating every possible alignment, including those between templates of different types. The similarity function, (call it TEMP ( , )) that they use for template alignment is itself the cross-slot average CEAF-REE impl score for predicted template and reference template . This brute-force template alignment, in conjunction with the two-level maximum bipartite matching problem, results in prohibitively long scorer execution times in cases where there are even a modest number of predicted or reference templates of the same type. 23 In addition to our implementation of CEAF-RME (see below), we also present the first correct implementation of CEAF-REE def that fully addresses the first two points above: template types are no longer treated as additional slots and the entity-level alignment constraint is enforced. On the third point, our implementation efficiently computes optimal template alignments using the Kuhn-Munkres algorithm (Kuhn, 1955;Munkres, 1957). However, even with this efficient implementation, solving the two-level maximum bipartite matching problems is still computationally intensive. 23 Only CEAF-REE def requires solving a two-level maximum-bipartite matching problem. Since CEAF-REE impl does not enforce the entity alignment constraint, these alignments will not necessarily be bipartite.", "publication_ref": ["b8", "b8", "b9", "b23", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "D.1.3 Coreference and CEAF-RME", "text": "As CEAF was designed for coreference, it is unsurprising that coreference considerations introduce a further wrinkle for CEAF-REE. None of the three models described in this work (including ITERX) performs entity coreference resolution. This clearly presents a problem because CEAF-REE def is an entity-level metric. One way to score these models is simply to treat each extracted mention as a singleton entity and use CEAF-REE def exactly as defined, and we report these scores in the main text for MUC-4. However, reporting only CEAF-REE def would be undesirable for several reasons:\n\u2022 It would render our results incomparable to past work, which reports only CEAF-REE impl .\n\u2022 It would put our work at odds with the overwhelming majority of the template extraction literature, where evaluation criteria focus on string matching between predicted and reference mentions. (The original MUC-4 evaluation only required systems to extract a single representative mention for each entity -not to identify all such mentions.)\n\u2022 The constraint that at most one predicted entity be aligned to a given reference entity would yield punishingly low scores for systems that are highly effective at extracting relevant spans, but that simply do not perform the additional step of coreference.\nFor these reasons, we disfavor a template extraction metric that requires template extraction systems to do coreference. These considerations motivate our introduction of CEAF-RME (rolefiller mention extraction) -that makes a minimal modification to CEAF-REE def to address (1) and (2) above. CEAF-RME treats system-predicted mentions as singleton entities, but deliberately relaxes the alignment constraint, potentially allowing multiple predicted singletons to map to the same reference entity, effectively pushing the burden of coreference into the metric. We believe CEAF-RME is consistent with what template extraction research has in fact historically cared about (identifying mentions that fill some slot) while correcting implementation problems with CEAF-REE that produce misleading results.\nThe micro-average CEAF-RME 3 results that we report on MUC-4 in the main body of the paper are micro-average CEAF-RME scores under an optimal template alignment (using CEAF-RME as the template similarity function), which is efficiently obtained using the Kuhn-Munkres algorithm.\nWe additionally include a version of CEAF-RME that uses \u2286 (CEAF-RME \u2286 ) for parallel comparison against CEAF-REE impl . Recall that CEAF-REE impl is essentially CEAF-RME \u2286 with the template type included as an additional slot. We reiterate that CEAF-RME 3 is the more appropriate metric since it can award partial credit for predicted entities whose mentions overlap imperfectly with those in the reference, where CEAF-RME \u2286 assigns zero credit in such cases.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.2 SCIREX", "text": "We use the same CEAF-RME 3 implementations for scoring SCIREX as we use for MUC-4. Full evaluation using the original SCIREX scoring script requires systems to perform coreference resolution, which makes it similarly inappropriate to CEAF-REE def for evaluating the systems presented in this work, none of which feature a coreference module. The CEAF-RME 3 and CEAF-RME \u2286 results presented in the main text together give a clearer picture of these models' ability to extract relevant mentions (short of clustering them) than would a coreference-based metric. We simply treat the SCIREX 4-tuples as 4-slot templates, following Huang et al. (2021).", "publication_ref": ["b19"], "figure_ref": [], "table_ref": []}, {"heading": "D.3 BETTER Granular", "text": "Evaluation for the BETTER Granular task bears some core similarity to CEAF-REE def in that relies on obtaining the alignment between system and reference templates that maximizes some similarity function that decomposes over slot fillers. And just as with (our corrected implementation of) CEAF-REE def , this is achieved via the Kuhn-Munkres algorithm. However, Granular scoring differs from CEAF-REE def in four key respects. First, the overall system score -referred to as the combined score -incorporates both a slot-level F1 score and a template-level F1 score:\nCombinedScore := TypeF1 \u00d7 SlotF1\nOnly exact matches between system and reference templates types are awarded credit. It is worth noting that because this score does not decompose over template pairs, it cannot be optimized directly using Kuhn-Munkres. In practice, what is optimized is response gain -the number of correct slot fillers minus the number of incorrect oneswhich provably yields alignments that optimize the combined score within a probabilistic error bound.\nThe remaining three key differences relate to the calculation of the slot-level F1. For one, Granular slots are not exclusively entity-valued, but may also be event-, (mixed) event-and-entity-, boolean-, and (categorical) string-valued, and different similarity functions must be employed in these different cases. For another, where CEAF-REE defines mentions by their string representation, the Granular score defines mentions based on document offsets. Finally, Granular also requires extraction of temporal and irrealis information for slots, and this in turn impacts the SlotF1 score.\nBorrowing terminology from the discussion of MUC-4 above, we describe below how ( , ) is calculated for some generic reference slot filler and system-predicted slot filler for slots of different types.\nBoolean and Categorical Values For booleanand categorical-string valued slots (i.e., slots taking on one of a predefined set of values). ( , ) = 1 if there is an exact match between the system and reference fillers and is 0 otherwise. Entities Unique among the three tasks discussed in this paper, Granular features an explicit preference for informative arguments in its scoring structure. In particular, (proper) name mentions of an entity are worth more than nominal mentions, which in turn are worth more than pronominal ones. 24 Thus, if Barack Obama were represented by the reference entity {Obama, the former President, he}, full credit would be awarded for returning only the mention Obama, less credit for the former President, and still less for he. Exact point values depend on the mentions present in the reference entity:\n\u2022 Correct name mentions always receive full credit ( ( , ) = 1)\n\u2022 Correct nominal mentions receive half-credit ( ( , ) = 0.5) if the reference entity additionally contains a name mention, and receive full credit otherwise.\n\u2022 Correct pronominal mentions receive quartercredit ( ( , ) = 0.25) if the reference entity additionally contains both a name and a nominal mention, and half-credit if only a nominal mention is featured. (Note that entities will never feature only pronominal mentions.)\nEvents Some Granular slots require events as fillers. Like entities, events are represented as sets of mentions (event anchors or triggers). Unlike entities, there is no informativity hierarchy for events. Furthermore, while event coreference is not a part of the Granular task, annotations for event coreference are nonetheless provided for scoring purposes: ( , ) = 1.0 iff contains only mentions belonging to events in the set of gold coreferent events , and is 0 otherwise, akin to REE .\nMixed Entities and Events Some slots may take a mix of events and entities as fillers. Since systems must indicate whether predicted mention clusters are entity-or event-denoting, the same similarity criteria for events and entities as described above are used to compute for events and entities that fill these slots.\nTemporal and Irrealis Information One of the features of Granular that makes it decidedly more difficult than either MUC-4 or SCIREX is the requirement to extract information relating to the time and irrealis status of an event when such information is available in the document. This information is encapsulated in special time-attachments and irrealis fields associated with each slot-filling entity or event. The former is given as a set of temporal expressions that describe the time at or during which the filler satisfied the role denoted by the slot (e.g. when individuals filling the tested-count slot in the Epidemic template were tested for the disease). The latter is given as one of a set of strings that describe whether or how the filler satisfied the role denoted by the slot: counterfactual, hypothetical, future, unconfirmed, unspecified, and non-occurrence. time-attachments and irrealis are each worth 0.25 points, where exact matches are required for full credit on either and where zero points are awarded otherwise. For slots for which time-attachments and irrealis are required, the value of appropriate to its filler type is scaled by 0.5 such that the maximum overall score ( , ) for a given filler -factoring in time-attachments, irrealis, and event or entity similarity -is 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We thank Nathaniel Weir, Elias Stengel-Eskin, and Patrick Xia for helpful comments and feedback. This work was supported in part by DARPA AIDA (FA8750-18-2-0015) and IARPA BETTER . The views and conclusions contained in this work are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, or endorsements of DARPA, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Scheduled sampling for sequence prediction with recurrent neural networks", "journal": "", "year": "2015-12-07", "authors": "Samy Bengio; Oriol Vinyals; Navdeep Jaitly; Noam Shazeer"}, {"ref_id": "b1", "title": "Learning to search better than your teacher", "journal": "", "year": "2015-07-11", "authors": "Kai-Wei Chang; Akshay Krishnamurthy; Alekh Agarwal; Hal Daum\u00e9; Iii ; John Langford"}, {"ref_id": "b2", "title": "Improving long distance slot carryover in spoken dialogue systems", "journal": "", "year": "2019", "authors": "Tongfei Chen; Chetan Naik; Hua He; Pushpendre Rastogi; Lambert Mathias"}, {"ref_id": "b3", "title": "Joint modeling of arguments for event understanding", "journal": "", "year": "2020", "authors": "Yunmo Chen; Tongfei Chen; Benjamin Van Durme"}, {"ref_id": "b4", "title": "Muc-4 evaluation metrics", "journal": "", "year": "1992", "authors": "Nancy Chinchor"}, {"ref_id": "b5", "title": "On the properties of neural machine translation: Encoder-decoder approaches", "journal": "", "year": "2014", "authors": "Kyunghyun Cho; Bart Van Merri\u00ebnboer; Dzmitry Bahdanau; Yoshua Bengio"}, {"ref_id": "b6", "title": "Search-based structured prediction", "journal": "Mach. Learn", "year": "2009", "authors": "Hal Daum\u00e9; Iii ; John Langford; Daniel Marcu"}, {"ref_id": "b7", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b8", "title": "GRIT: Generative role-filler transformers for document-level event entity extraction", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Xinya Du; Alexander Rush; Claire Cardie"}, {"ref_id": "b9", "title": "Template filling with generative transformers", "journal": "", "year": "2021", "authors": "Xinya Du; Alexander Rush; Claire Cardie"}, {"ref_id": "b10", "title": "Multi-sentence argument linking", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Seth Ebner; Patrick Xia; Ryan Culkin; Kyle Rawlins; Benjamin Van Durme"}, {"ref_id": "b11", "title": "On event individuation for document-level information extraction", "journal": "", "year": "2022", "authors": "William Gantt; Reno Kriz; Yunmo Chen; Siddharth Vashishtha; Aaron Steven White"}, {"ref_id": "b12", "title": "Allennlp: A deep semantic natural language processing platform", "journal": "", "year": "2018", "authors": "Matt Gardner; Joel Grus; Mark Neumann; Oyvind Tafjord; Pradeep Dasigi; Nelson Liu; Matthew Peters; Michael Schmitz; Luke Zettlemoyer"}, {"ref_id": "b13", "title": "Beyond Nom-Bank: A study of implicit arguments for nominal predicates", "journal": "Association for Computational Linguistics", "year": "2010", "authors": "Matthew Gerber; Joyce Chai"}, {"ref_id": "b14", "title": "Automatic labeling of semantic roles", "journal": "Computational Linguistics", "year": "2002", "authors": "Daniel Gildea; Daniel Jurafsky"}, {"ref_id": "b15", "title": "A dynamic oracle for arc-eager dependency parsing", "journal": "", "year": "2012", "authors": "Yoav Goldberg; Joakim Nivre"}, {"ref_id": "b16", "title": "Message Understanding Conference-6: A brief history", "journal": "", "year": "1996", "authors": "Ralph Grishman; Beth Sundheim"}, {"ref_id": "b17", "title": "Levenshtein transformer", "journal": "Vancouver", "year": "2019-12-08", "authors": "Jiatao Gu; Changhan Wang; Junbo Zhao"}, {"ref_id": "b18", "title": "An empirical study on finding spans", "journal": "", "year": "2022", "authors": "Weiwei Gu; Boyuan Zheng; Yunmo Chen; Tongfei Chen; Benjamin Van Durme"}, {"ref_id": "b19", "title": "Document-level entity-based extraction as template generation", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Kung-Hsiang Huang; Sam Tang; Nanyun Peng"}, {"ref_id": "b20", "title": "Peeling back the layers: Detecting event role fillers in secondary contexts", "journal": "", "year": "2011", "authors": "Ruihong Huang; Ellen Riloff"}, {"ref_id": "b21", "title": "Modeling textual cohesion for event extraction", "journal": "", "year": "2012", "authors": "Ruihong Huang; Ellen Riloff"}, {"ref_id": "b22", "title": "SciREX: A challenge dataset for document-level information extraction", "journal": "", "year": "2020", "authors": "Sarthak Jain; Madeleine Van Zuylen; Hannaneh Hajishirzi; Iz Beltagy"}, {"ref_id": "b23", "title": "The hungarian method for the assignment problem", "journal": "", "year": "1955", "authors": " Harold W Kuhn"}, {"ref_id": "b24", "title": "End-to-end neural coreference resolution", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Kenton Lee; Luheng He; Mike Lewis; Luke Zettlemoyer"}, {"ref_id": "b25", "title": "A multi-sentiment-resource enhanced attention network for sentiment classification", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Zeyang Lei; Yujiu Yang; Min Yang; Yi Liu"}, {"ref_id": "b26", "title": "Document-level event argument extraction by conditional generation", "journal": "", "year": "2021", "authors": "Sha Li; Ji Heng; Jiawei Han"}, {"ref_id": "b27", "title": "A joint neural model for information extraction with global features", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Ying Lin; Heng Ji; Fei Huang; Lingfei Wu"}, {"ref_id": "b28", "title": "On coreference resolution performance metrics", "journal": "Association for Computational Linguistics", "year": "2005", "authors": "Xiaoqiang Luo"}, {"ref_id": "b29", "title": "Predicate-specific annotations for implicit role binding: Corpus annotation, data analysis and evaluation experiments", "journal": "", "year": "2013", "authors": "Tatjana Moor; Michael Roth; Anette Frank"}, {"ref_id": "b30", "title": "Algorithms for the assignment and transportation problems", "journal": "Journal of the society for industrial and applied mathematics", "year": "1957", "authors": "James Munkres"}, {"ref_id": "b31", "title": "AMR beyond the sentence: the multi-sentence AMR corpus", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "O' Tim; Michael Gorman; Kira Regan; Ulf Griffitt; Kevin Hermjakob; Martha Knight;  Palmer"}, {"ref_id": "b32", "title": "Pytorch: An imperative style, high-performance deep learning library", "journal": "Curran Associates, Inc", "year": "2019", "authors": "Adam Paszke; Sam Gross; Francisco Massa; Adam Lerer; James Bradbury; Gregory Chanan; Trevor Killeen; Zeming Lin; Natalia Gimelshein; Luca Antiga; Alban Desmaison; Andreas Kopf; Edward Yang; Zachary Devito; Martin Raison; Alykhan Tejani; Sasank Chilamkurthy; Benoit Steiner; Lu Fang; Junjie Bai; Soumith Chintala"}, {"ref_id": "b33", "title": "A unified model of phrasal and sentential evidence for information extraction", "journal": "Association for Computational Linguistics", "year": "2009", "authors": "Siddharth Patwardhan; Ellen Riloff"}, {"ref_id": "b34", "title": "Cross-sentence n-ary relation extraction with graph LSTMs", "journal": "Transactions of the Association for Computational Linguistics", "year": "2017", "authors": "Nanyun Peng; Hoifung Poon; Chris Quirk; Kristina Toutanova; Wen-Tau Yih"}, {"ref_id": "b35", "title": "Distant supervision for relation extraction beyond the sentence boundary", "journal": "Long Papers", "year": "2017", "authors": "Chris Quirk; Hoifung Poon"}, {"ref_id": "b36", "title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "J. Mach. Learn. Res", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b37", "title": "A reduction of imitation learning and structured prediction to no-regret online learning", "journal": "", "year": "2011-04-11", "authors": "St\u00e9phane Ross; Geoffrey J Gordon; Drew Bagnell"}, {"ref_id": "b38", "title": "SemEval-2010 task 10: Linking events and their participants in discourse", "journal": "", "year": "2010", "authors": "Josef Ruppenhofer; Caroline Sporleder; Roser Morante; Collin Baker; Martha Palmer"}, {"ref_id": "b39", "title": "Bidirectional attention flow for machine comprehension", "journal": "", "year": "2017-04-24", "authors": "Min Joon Seo; Aniruddha Kembhavi; Ali Farhadi; Hannaneh Hajishirzi"}, {"ref_id": "b40", "title": "Overview of the third Message Understanding Evaluation and Conference", "journal": "", "year": "1991-05-21", "authors": "Beth M Sundheim"}, {"ref_id": "b41", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Lukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b42", "title": "Improving multi-step prediction of learned time series models", "journal": "AAAI Press", "year": "2015-01-25", "authors": "Arun Venkatraman; J Andrew Hebert;  Bagnell"}, {"ref_id": "b43", "title": "ACE 2005 Multilingual Training Corpus LDC2006T06. Linguistic Data Consortium", "journal": "", "year": "2006", "authors": "Christopher Walker; Stephanie Strassel; Julie Medero; Kazuaki Maeda"}, {"ref_id": "b44", "title": "LOME: Large ontology multilingual extraction", "journal": "", "year": "2021", "authors": "Patrick Xia; Guanghui Qin; Siddharth Vashishtha; Yunmo Chen; Tongfei Chen; Chandler May; Craig Harman; Kyle Rawlins; Aaron Steven White; Benjamin Van Durme"}, {"ref_id": "b45", "title": "DocRED: A large-scale document-level relation extraction dataset", "journal": "", "year": "2019", "authors": "Yuan Yao; Deming Ye; Peng Li; Xu Han; Yankai Lin; Zhenghao Liu; Zhiyuan Liu; Lixin Huang; Jie Zhou; Maosong Sun"}, {"ref_id": "b46", "title": "Everything is all it takes: A multipronged strategy for zero-shot cross-lingual information extraction", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Mahsa Yarmohammadi; Shijie Wu; Marc Marone; Haoran Xu; Seth Ebner; Guanghui Qin; Yunmo Chen; Jialiang Guo; Craig Harman; Kenton Murray; Aaron Steven White; Mark Dredze; Benjamin Van Durme"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: An example of multi-template extraction on a document (an NLP paper; Lei et al. (2018)) from the SCIREX dataset. An agent reads the entire paper and iteratively generates templates, each consisting of slots for Task, Method, Dataset, and Metric.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 4 :4Figure 4: A comparison of the metrics discussed. Features in blue are \"desired\" for the evaluation of our task.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "AlphaFigure 5 :5Figure5: Performance changes on CEAF-RME 3 with respect to , for which higher means higher probability of rolling out agent's policy for state update.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "function and the maximal match * between entities, the final precision and recall are computed as follows:", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": ").", "figure_data": "BiDAF Machine comprehension SQuAD CNN/DailyMail question answering exact match accuracy Extracted templates:BiDAF Machine comprehension SQuAD CNN/DailyMail question answering exact match accuracy Template 1 Task question answering Metric exact match Dataset SQuAD Method machine comprehension BiDAFBiDAF Machine comprehension SQuAD CNN/DailyMail question answering exact match accuracy Template 2 Task Method Dataset Metric machine comprehension BiDAF accuracy CNN/DailyMailaGenerated templatePolicy\u21e1X Span\u2327 TransitionX 0 Next\u21e1\u21e1\u21e1statestateOne iteration stepX (0)\u2327X (1)\u2327X (2)Figure 3: The basic iteration step of ITERX (left box), and an unrolled version on SCIREX 4-ary relation extraction"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": "SCIREXMUC-4GranularTrain Dev Test Train Dev Test Train Dev Test# documents 306 66 66 1,300 200 200 302 34 32# templates 1,627 251 271 1,114 191 209 610 57 47# temp. types166# slot types4592 + 4  \u2020"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Summary statistics of the datasets.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "This is the current state-of-the-art system for REE (the simplified slot-filling entity extraction task) on MUC-4. On SCIREX, TEMPGEN may output multiple relation instances, but only one canonical mention as the filler for each role in the relation.", "figure_data": "On MUC-4, TEMPGEN outputs a single aggregate templateper document, but allows multiple spans to fill atemplate slot. We make minimal modifications tothe TEMPGEN source code to support multi-filler,multi-template prediction on both datasets, allow-ing for direct comparison to ITERX and GTT onfull template extraction."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Results on SCIREX.", "figure_data": "Model (Encoder)CEAF-RME 3CEAF-RME \u2286CEAF-REE def\u2020CEAF-REE implPRF 1PRF 1PRF 1PRF 1TEMPGEN (BART base ) 54.2 15.8 24.5 58.3 31.0 40.5 53.1 29.5 37.9 55.7 40.0 46.4TEMPGEN (BART large ) 55.8 18.9 28.3 61.3 32.9 42.8 60.3 31.2 41.1 63.7 37.4 47.2GTT (BERT base )54.7 23.0 32.3 55.0 36.8 44.1 54.7 37.0 44.1 61.7 42.4 50.2ITERX (BERT base )41.3 27.9 33.3 47.2 45.0 46.1 41.3 45.3 43.2 52.3 51.1 51.7ITERX (BART enc base ) ITERX (T5 enc large )39.2 24.8 30.4 44.8 40.1 42.3 35.4 20.3 39.2 49.8 45.7 47.6 53.5 26.2 35.2 55.8 42.4 48.2 47.5 42.4 44.8 60.9 46.9 53.0"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Results on MUC-4. \u2020 Note that scores under CEAF-REE def are compared as if every mention forms a singleton entity. We made this assumption since neither prior work nor our model perform coreference resolution. Thus, CEAF-REE def is a somewhat inappropriate metric for these systems, but is included for completeness.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Results with different choices of temperature.", "figure_data": "ApproachCEAF-RME 3CEAF-RME \u2286PRF 1PRF 1FIXED28.7 7.3 11.7 29.9 29.9 29.9ARGMAX 26.4 6.9 11.0 29.4 30.2 29.8XENT28.7 10.7 15.6 27.8 32.3 29.9UNIFORM 29.1 11.3 16.3 27.6 32.0 29.7"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "", "figure_data": ": Hyperparameters and other reproducibility in-formation for SCIREX. \"LR\" denotes learning rate, \"Type\" indicates which policy network architecture isused (see  \u00a73.2), \"Max #Iteration\" sets the maximumnumber of iterations that the model is allowed to per-form, and \"Training Spans\" determines whether thetraining spans come from gold annotations or the inter-section of gold spans and those predicted by the spanfinding module. NameBestSearch BoundsEncoderT5 enc large{T5 enc base , T5 enc large , BART enc base , BART enc large"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "", "figure_data": ": Hyperparameters and other reproducibility in-formation for MUC-4."}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "Hyperparameters and other reproducibility information for Granular.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "Given a document = ( 1 , \u2022 \u2022 \u2022 , )", "formula_coordinates": [2.0, 70.87, 465.73, 165.68, 19.44]}, {"formula_id": "formula_1", "formula_text": "{( 1 , 1 ), \u2022 \u2022 \u2022 , ( , )}, where \u2208 T .", "formula_coordinates": [2.0, 306.42, 391.46, 169.88, 19.44]}, {"formula_id": "formula_2", "formula_text": "( | , X) = ( : ) \u2208 ( | , x) .(1)", "formula_coordinates": [4.0, 120.68, 296.0, 168.46, 27.98]}, {"formula_id": "formula_3", "formula_text": "ind ( | , x) \u221d 1 \u2208 \u222a{ } \u2022exp(s T \u2022FFN( [t; x])) (2)", "formula_coordinates": [4.0, 83.41, 502.8, 205.72, 20.1]}, {"formula_id": "formula_4", "formula_text": "(t,x 1 , \u2022 \u2022 \u2022,x ) = Transformer(t, x 1 , \u2022 \u2022 \u2022, x ) (3)", "formula_coordinates": [4.0, 77.65, 675.01, 211.49, 19.44]}, {"formula_id": "formula_5", "formula_text": "joint ( | , x) \u221d 1 \u2208 \u222a{ } \u2022 exp(s Tx )(4)", "formula_coordinates": [4.0, 345.14, 164.52, 179.27, 20.11]}, {"formula_id": "formula_6", "formula_text": "x \u2032 mem = GRU(x mem , [s ;t]) if \u2260 ; x mem if = . (5", "formula_coordinates": [4.0, 321.8, 448.35, 198.37, 28.77]}, {"formula_id": "formula_7", "formula_text": ")", "formula_coordinates": [4.0, 520.17, 458.07, 4.24, 9.46]}, {"formula_id": "formula_8", "formula_text": "E * \u223c * \u223c\u02dc \u221e \u2211\ufe01 =0 log ( * | , X( ))(6)", "formula_coordinates": [5.0, 105.45, 219.21, 183.69, 34.43]}, {"formula_id": "formula_9", "formula_text": "\u223c Bernoulli( ); \u223c + (1 \u2212 ) * .(7)", "formula_coordinates": [5.0, 145.19, 389.69, 143.94, 35.98]}, {"formula_id": "formula_10", "formula_text": "* ( | , X) \u221d log ( | ,X)/ if \u22080 if \u2209\u00af,(8)", "formula_coordinates": [5.0, 89.19, 706.24, 233.98, 29.25]}, {"formula_id": "formula_11", "formula_text": "\u2286 ( , ) = 1[ \u2286 ].", "formula_coordinates": [6.0, 313.35, 349.96, 95.08, 19.44]}, {"formula_id": "formula_12", "formula_text": "E n t i t y a l i g n m e n t REE def RME \u03c6 REE impl RME \u03c6 3 ! relaxed relaxed relaxed strict T y p e n o t a s l o t \u2714 \u2714 \u2717 \u2714 C l u s t e r s i m i l a r i t y \u03c6 3 \u03c6 ! \u03c6 ! \u03c6 !", "formula_coordinates": [6.0, 350.07, 600.3, 128.0, 100.68]}, {"formula_id": "formula_13", "formula_text": "Template Slot Combined P R F 1 P R F 1", "formula_coordinates": [7.0, 319.31, 644.54, 197.57, 25.91]}, {"formula_id": "formula_14", "formula_text": ", BERTbase} LR 3 \u00d7 10 \u22125 {1 \u00d7 10 \u22125 , 3 \u00d7 10 \u22125 , 5 \u00d7 10 \u22125 } Encoder LR 1 \u00d7 10 \u22125 1 \u00d7 10 \u22125 0.6 [0, 1] 1.0 {0, 1.0, \u221e} 1.0 {1.", "formula_coordinates": [13.0, 311.03, 345.04, 208.33, 44.26]}, {"formula_id": "formula_16", "formula_text": "\u2286 ( , ) := 1, if \u2286 0, otherwise(13)", "formula_coordinates": [15.0, 113.53, 705.18, 175.6, 27.78]}, {"formula_id": "formula_17", "formula_text": "CombinedScore := TypeF1 \u00d7 SlotF1", "formula_coordinates": [17.0, 100.1, 673.33, 159.81, 19.44]}], "doi": "10.18653/v1/W19-4111"}