{"title": "From Large Scale Image Categorization to Entry-Level Categories", "authors": "Vicente Ordonez; Jia Deng; Yejin Choi; Alexander C Berg; Tamara L Berg", "pub_date": "", "abstract": "Entry level categories -the labels people will use to name an object -were originally defined and studied by psychologists in the 1980s. In this paper we study entrylevel categories at a large scale and learn the first models for predicting entry-level categories for images. Our models combine visual recognition predictions with proxies for word \"naturalness\" mined from the enormous amounts of text on the web. We demonstrate the usefulness of our models for predicting nouns (entry-level words) associated with images by people. We also learn mappings between concepts predicted by existing visual recognition systems and entry-level concepts that could be useful for improving human-focused applications such as natural language image description or retrieval.", "sections": [{"heading": "Introduction", "text": "Computational visual recognition is beginning to work. Although far from solved, algorithms have now advanced to the point where they can recognize or localize thousands of object categories with reasonable accuracy [17,4,3,12]. While we could predict any one of many relevant labels for an object, the question of \"What should I actually call it?\" is becoming important for large-scale visual recognition. For instance, if a classifier were lucky enough to get the example in Figure 1 correct, it might output grampus griseus, while most people are more likely to simply say dolphin. This is closely related to ideas of basic and entry level categories formulated by psychologists such as Eleanor Rosch [18] and Stephan Kosslyn [11]. While objects are members of many categories -e.g. Mr Ed is a palomino, but also a horse, an equine, an odd-toed ungulate, a placental mammal, a mammal, and so on -most people looking at Mr Ed would tend to call him a \"horse\", his entry level category (unless they are fans of the show). More generally such questions are very relevant to recent work on the connection between computer vision outputs and (generating) natural language descriptions of images [8,13,16,14].\nIn this paper we consider two related problems 1) learn-", "publication_ref": ["b16", "b3", "b2", "b11", "b17", "b10", "b7", "b12", "b15", "b13"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "grampus griseus dolphin", "text": "Recognition Prediction What should I Call It? ing a mapping from specific categoriese.g., leaf nodes in WordNet [9] -to what people are likely to call them and 2) learning to map from outputs of thousands of noisy computer vision classifiers/detectors evaluated on an image to what a person is likely to call the image.\nOur proposed methods take into account several sources of structure and information: the structure of WordNet, frequencies of word use from Google n-grams, outputs of a large-scale visual recognition system, and large amounts of paired image and text data. In particular, we make use of the SBU Captioned Photo Dataset [16], which consists of 1 million images with natural language captions, as a source of natural image naming patterns. Taken together, we are able to study patterns for choice of basic level categories at a much larger scale than previous psychology experiments.\nOn a technical level, our work is related to recent work from Deng et al. [6] that tries to \"hedge\" predictions of visual content by optimally backing off in the WordNet hierarchy. One key difference is that our approach allows a reward function over the WordNet hierarchy that is not monotonic along paths from the root to leaves. This allows reward based on factors including frequency of word use that are not monotonic along such paths in WordNet. This also allows mappings to be learned from a WordNet leaf node, l, to natural word choices that are not along a path from l to the root, \"entity\". In evaluations, our results significantly out-perform those of Deng et al. [6] because although optimal in some abstract sense, they are not optimal with respect to how people describe image content.\nOur work is also related to the growing challenge of harnessing the ever increasing number of pre-trained recognition systems, thus avoiding always \"starting from scratch\" in developing new applications. It is wasteful not to take advantage of the CPU weeks [10,12], months [3,6], or even millennia [15] invested in developing recognition models for increasingly large labeled datasets [7,19,22,5,20]. However, for any specific end user application, the categories of objects, scenes, and attributes labeled in a particular dataset may not be the most useful predictions. One benefit of our work can be seen as exploring the problem of translating the outputs of a vision system trained with one vocabulary of labels (WordNet leaf nodes) to labels in a new vocabulary (commonly used visually descriptive nouns).\nEvaluations show that our models can effectively emulate the naming schemes of human observers. Furthermore, we show that using noisy vision estimates for image content, our system can output words that are significantly closer to human annotations than either the raw noisy vision estimates or the results of using the state of the art hedging system from Deng et al. [6].", "publication_ref": ["b8", "b15", "b5", "b5", "b9", "b11", "b2", "b5", "b14", "b6", "b18", "b21", "b4", "b19", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Insights into Entry-Level Categories", "text": "At first glance, the task of finding the entry-level categories may seem like a linguistic problem of finding a hypernym of any given word. Although there is a considerable conceptual connection between entry-level categories and hypernyms, there are two notable differences:\n1. Although \"bird\" is a hypernym of both \"penguin\", and \"sparrow\", \"bird\" may be a good entry-level category for \"sparrow\", but not for \"penguin\". This phenomenon -that some members of a category are more prototypical than others -has been discussed in Prototype Theory [18]. 2. Entry-level categories are not confined by (inherited) hypernyms, in part because encyclopedic knowledge is different from common sense knowledge. For example \"rhea\" is not a kind of \"ostrich\" in the strict taxonomical sense. However, due to their visual similarity, people generally refer to a \"rhea\" as an \"ostrich\". Adding to the challenge is that although extensive, WordNet is neither complete nor practically optimal for our purpose. For example, according to WordNet, \"kitten\" is not a kind of \"cat\", and \"tulip\" is not a kind of \"flower\". In fact, both of the above points have a connection to visual information of objects, as visually similar objects are more likely to belong to the same entry-level category. In this work, we present the first extensive study that (1) characterizes entry-level categories in the context of translating encyclopedic visual categories to natural names that people commonly use, and (2) provides approaches that infer entrylevel categories from a large scale image corpus, guided by semantic word knowledge.", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}, {"heading": "Paper Overview", "text": "Our paper is divided as follows. In section 2 we run experiments to gather entry-level category labels directly from people. In section 3 we learn translations between leaf node concepts and entry-level concepts. In section 4 we propose two models and a joint model that can take an image as input and predict entry-level concepts. Finally, in section 5 we provide experimental evaluations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Obtaining Natural Categories from Humans", "text": "We use Amazon Mechanical Turk to crowd source translations of ImageNet synsets into entry-level categories D = {x i , y i | x i is a leaf node, y i is a word}. Our experiments present users with a 2x5 array of images sampled from an ImageNet synset, x i , and users are asked to label the depicted concept. Results are obtained for 500 ImageNet synsets and aggregated across 8 users per task. We found agreement (measured as at least 3 of 8 users in agreement) among users for 447 of the 500 concepts, indicating that even though there are many potential labels for each synset (e.g. Sarcophaga carnaria could conceivably be labeled as fly, dipterous insect, insect, arthropod, etc) people have a preference for particular entry-level categories. This experiment expands on previous studies in psychology [18,11]. Cheap and easy online crowdsourcing enables us to gather these labels for a much larger set of (500) concepts than previous experiments. Furthermore, we use the results of our experiments to automatically learn generalizations to a substantially larger set of ImageNet synsets in section 3.", "publication_ref": ["b17", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Translating Encyclopedic Concepts to Entry-Level Concepts", "text": "Our objective in this section is to discover mappings between encyclopedic concepts (ImageNet leaf categories, e.g. Chlorophyllum molybdites) to output concepts that are more natural (e.g. mushroom). In section 3.1 we present an approach that relies on the wordnet hierarchy and frequency of words in a web scale corpus. In section 3.2 we follow an approach that uses visual recognition models learned on a paired image-caption dataset.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Language-Only Translation", "text": "For comparison purposes, we first consider a translation approach that relies only on language-based information. We hypothesize that the frequency of terms computed from massive amounts of text on the web reflects the \"naturalness\" of concepts. We use the n-gram counts of the Google 1T corpus [2] as a proxy for term \"naturalness\". Specifically, for a synset w, we quantify \"naturalness\" as the maximum log count \u03c6(w) of all of the terms in the synset.\nTo control the degree of naturalness, we constrain the translation using the hyponym/hypernym structure of Word-Net. More specifically, we define \u03c8(w, v) as a function that measures the distance between leaf node v and node w in the hypernym structure. Then the translation function \u03c4 (v, \u03bb) : V \u2192 W maps a leaf node v to a target node w by maximizing a trade-off between naturalness and semantic proximity.\n\u03c4 (v, \u03bb) = arg max w [\u03c6(w) \u2212 \u03bb\u03c8(w, v)], w \u2208 \u03a0(v) (1) \u03a0(v)\nis the set of (inherited) hypernyms including v. We find the optimal \u03bb based on a sub-set of translation pairs D = (x i , y i ) collected using MTurk (section 2). We show the relationship between \u03bb and the size of the output vocabulary |W | on the left side of Fig. 2 and the relationship between \u03bb and \u03a6(D, \u03bb) on the right side. The size of the output vocabulary increases monotonically with \u03bb. At a high level, increasing \u03bb serves to encourage mappings to be close to the input node in the WordNet hierarchy, thereby increasing the vocabulary size and limiting the generalization of concepts. Conversely, \"naturalness\", \u03a6(D, \u03bb), increases initially and then decreases as too much specificity or generalization hurts the naturalness of the outputs. For example, generalizing from \"grampus griseus\" to \"dolphin\" is good for \"naturalness\", but generalizing all the way to \"entity\" decreases \"naturalness\". In Figure 2 the red line shows accuracy for predicting the most agreed upon word for a synset, while the cyan line shows the accuracy for predicting any word collected from any user.", "publication_ref": ["b1"], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "\u03a6(D, \u03bb)", "text": "= i 1[word(\u03c4 (x i , \u03bb)) = y i ](2)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Input Concept", "text": "Ngramtranslation", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "SVMtranslation", "text": "Humantranslation  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Visually-Informed Translation", "text": "In this approach, for a given leaf synset v we sample a set of n = 100 images s = {I 1 , I 2 , ..., I n } and each image is automatically annotated with nouns N i = {n i1 , n i2 , ..., n im } using the models learned in section 4.2. We use the set of labels N = N 1 \u222a N 2 ... \u222a N n as keyword annotations for synset v and rank them using a TFIDF information retrieval where we consider each category v in our experimental setting as a document for the inverse document frequency term. We pick the most relevant keyword for each node v as the entry-level categorical translation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Predicting Entry-Level Concepts for Images", "text": "Our objective in this section is to explore approaches that can take an image as input and predict its entry-level labels. The models we propose are: 1) a method that combines \"naturalness\" measures computed from the web with direct estimates of visual content computed at leaf nodes and inferred for internal nodes (section 4.1), 2) a method that learns models for entry-level recognition from a large collection of images with associated captions (section 4.2), and 3) a joint method combining the two approaches (section 4.3).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Prediction using Propagated Visual Estimates", "text": "As our first method for predicting entry level categories for an image, we present a variation on the hedging approach [6]. In the hedging work, the output is the node with the maximum expected reward, where the reward is monotonic in the hierarchy and has been smoothed by adding a carefully chosen constant to the reward for all nodes. In our modification, we construct a non-monotonic reward \u03b3 based on naturalness and a smoothing offset that is scaled by the position in the hierarchy.\nThe image content for an image, I, is estimated using trained models from [6]. These models predict presence or absence of 7404 leaf node concepts in the ImageNet hierarchy. Following the approach of [6], we compute estimates of visual content for internal nodes by hierarchically accumulating all predictions below a node: 1\nf (v, I) = \u23a7 \u23a8 \u23a9f (v, I), if v is a leaf node v \u2208Z(v)f (v , I), if v is an internal node (3)\nWhere Z(v) is the set of all leaf nodes under node v and f (v, I) is the output of a Platt-scaled decision value from a linear SVM trained for the category corresponding to input leaf node v. Each linear SVM is trained on sift features with locally-constrained linear coding and spatial pooling on a regular 3x3 grid. Following our approach from section 3.1, we define for every node in the ImageNet hierarchy a trade-off function between \"naturalness\" (ngram counts) and specificity (relative position in the wordnet hierarchy):\n\u03b3(v,\u03bb) = [\u03c6(v) \u2212\u03bb\u03c8(v)](4)\nWhere\u03c8(v) = max w\u2208Z(v) \u03c8(v, w) measures the max height over Z(v), the set of leaf nodes under v. We parameterize this trade-off by\u03bb.\nFor entry-level category prediction on images, we would like to maximize both \"naturalness\" and content estimates. For example, text based \"naturalness\" will tell us that both cat and dog are good entry level categories, but a confident visual prediction for German shepherd for an image tells us that dog is a much better entry-level prediction than cat for that image.\nTherefore, for an input image, we want to output a set of concepts that have a large prediction for both \"naturalness\" and content estimate score. For our experiments we output the top 5 Wordnet synsets according to:\nf nat (v, I,\u03bb) = f (v, I)\u03b3(v,\u03bb) (5) f nat (v, I,\u03bb) = f (v, I)[\u03c6(v) \u2212\u03bb\u03c8(v)] (6\n)\nAs we change\u03bb we expect similar behavior to our web based concept translations (section 3.1). Again, we can tun\u00ea \u03bb to control the degree of specificity while trying to preserve \"naturalness\" using n-gram counts. We compare our framework to hedging [6] for different settings of\u03bb. For a side by side comparison we modify hedging to output the top 5 synsets based on their scoring function. Here, the working vocabulary is the unique set of predicted labels output for each method on this test set. Results demonstrate (Figure 4) that under different parameter settings we consistently obtain much higher levels of precision for predicting entrylevel categories than hedging [6].", "publication_ref": ["b5", "b5", "b5", "b5", "b5"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Prediction using Supervised Learning", "text": "In the previous section we rely on wordnet structure to compute estimates of image content, especially for internal nodes. However, this is not always a good measure of content because: 1) The wordnet hierarchy doesn't encode knowledge about some semantic relationships between objects (i.e. functional or contextual relationships), 2) Even with the vast coverage of 7404 ImageNet leaf nodes we are missing models for many potentially important entry-level categories that are not at the leaf level.\nAs an alternative, we directly train models for entry-level categories from data where people have provided entrylevel labels -in the form of nouns present in visually descriptive image captions. We postulate that these nouns represent examples of entry-level labels because they have been naturally annotated by people to describe what is present in an image. For this task, we leverage the large scale dataset of [16], containing 1 million captioned images. We transform this dataset into a set D = {X (i) , Y (i) | X (i) \u2208 X, Y (i) \u2208 Y}, where X = [0-1] S is an input space of estimates of visual content for S = 7404 ImageNet leaf node categories and Y = [0, 1] D is a set of binary output labels for D target categories.\nFor estimating the presence of objects from our set of 7404 ImageNet leaf node categories we use the same models as the previous section with one additional consideration. We run the classifiers on a set of bounding boxes B = {b k } for each training image using the window selection method of [21]. We then aggregate the results across multiple bounding boxes by max pooling of visual concepts scores. So the feature descriptor for an image I (i) is:\nX (i) = {x (i) j | x (i) j = max(f (v j , I (i) , b k )}(7)\nWheref (v j , I (i) , b k ) is the output score for the presence of the visual concept represented by the leaf node v j and bounding box b k on image I (i) .\nFor training our D target categories, we obtain labels Y from the million captions by running a POS-tagger [1] and defining Y i = [y j | image i has noun j]. The POStagger helps cleans up some word sense ambiguity due to polysemy. |D| is determined experimentally from data by learning models for the most frequent words in this dataset. This provides us with a target vocabulary that is both likely to contain entry-level categories (because we expect entrylevel category nouns to commonly occur in our visual descriptions) and to contain sufficient images for training effective recognition models. We use up to 10000 images for training each model. Since we are using human labels from real-world data, the frequency of words in our target vocabulary follows a power-law distribution. Hence we only have a very large amount of training data for a few most commonly occurring noun concepts. Specifically, we learn linear SVMs subject to platt scaling for each of our target concepts. We keep 800 of the best performing models. Our combined scoring prediction function is then (note that the operations here are pointwise operators):\nF svm (I, \u0398) = [f svm (v i , I, \u03b8 i )] (8\n)\nF svm (I, \u0398) = 1 1 \u2212 exp(a\u0398 X + b) (9) R(\u03b8 i ) = 1 2 \u03b8 i + c |D| j=1 max(0, 1 \u2212 y (j) i \u03b8 i X (j) ) 2 (10)\nWe minimize the squared hinge-loss with 1 regularization (eqn 10). The latter provides a natural way of modeling the relationships between the input and output label spaces that encourages sparseness 2 . See examples in Figure 5. Since we learn each linear SVM independently, \u03b8 i represents a row in the joint matrix \u0398. We fit Platt scaling parameters a = [a i ] and b = [b i ] for each target label i on a held out validation set.\nOne of the drawbacks of using the ImageNet hierarchy to aggregate estimates of visual concepts (section 3) is that it ignores more complex relationships between concepts.\nHere our data-driven approach to the problem implicitly discovers these relationships. For instance a concept like tree has a co-occurrence relationship with bird that may be useful for prediction. A chair is often occluded by the objects sitting on the chair, but evidence of those types of objects, e.g. people or cat or co-occurring objects, e.g. table can help us predict the presence of a chair. See figure 5 for some example learned relationships.\nGiven this large dataset of images with noisy visual predictions and text labels, we manage to learn quite good predictors of high-level content, even for categories with relatively high intra-class variation (e.g. girl, boy, market, house). We show some results of images with predicted output labels for a group of images in Figure 6.", "publication_ref": ["b15", "b20", "b0", "b1"], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Joint Prediction", "text": "Finally, we explore methods to combine our two approaches from section 4.1 and section 4.2. We start by associating the SVM based scores f svm (section 4.2) to synsets in the ImageNet hierarchy. Here we map words from our 2 We find c = 0.01 to yield good results for our problem and use this value for training all individual models. . Sample predictions from our experiments on a test set for each type of category. Note that image labels come from caption nouns, so some images marked as correct predictions might not depict the target concept whereas some images marked as wrong predictions might actually depict the target category.\nTable 2. Performance at predicting the labels agreed upon by 2 (of 3) Turkers on dataset A (random images) and Dataset B (images with high confidence scores). Precision/Recall are computed per image and averaged across each dataset, computed over 10 splits. target nouns D to the best matching synset concept. For each synset, v, we also associate its direct translation score, f nat (v, I,\u03bb) (section 4.1), illustrated in Fig 7. This means that for all WordNet synsets we have a direct translation score, and for some synsets we have a mapped SVM score f svm (v, I, \u03b8 v ) (for nodes not appearing in D we set this score to be zero). Likewise the SVM scoring function introduces some new concepts not present in the WordNet hierarchy that have a value of zero for f nat (v, I,\u03bb). We redefine the domain of our scoring function (eqns 11 and 12) and use a parameter \u03b1 to control for tradeoff between the two models (13).\nf nat (v) = f nat (v, I,\u03bb), if v \u2208 dom(f nat ) 0 otherwise (11) f svm (v) = f svm (v, I, \u03b8 v ), if v \u2208 dom(f svm ) 0 otherwise (12\n)\nf joint (v, \u03b1) = \u03b1 f nat (v) + f svm (v)(13)\nWe define our prediction function that associates a set of ImageNet nodes v 1 , v 2 , ..., v n to an input image based on the joint scores (13) as follows:\nv 1 ,v 2 , ...,v n = argmax v1,v2,...,vn i f joint (v i , \u03b1) (14)\nThis means we can select the set of n nodes that have the highest scores. We use n = 5 and find \u03b1 that minimizes the error on the average annotation F1 score per image on a training set of 1000 images with human labels.", "publication_ref": ["b1", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Evaluation", "text": "We evaluate learning general translations from encyclopedic to entry-level concepts (section 5.1) and predicting entry-level concepts for images (section 5.2).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluating Translations", "text": "We show sample results from each of our methods to learn concept translations in Figure 3 (more are included in the supplemental material). In some cases Ngramtranslation fails. For example, whinchat (a type of bird) translates to \"chat\" most likely because of the inflated counts for the most common use of \"chat\". SVM translation fails when it learns to weight context words highly, for example \"snorkeling\" \u2192 \"water\", or \"African bee\" \u2192 \"flower\" even when we try to account for common context words using IDF. Finally, even humans are not always correct, for example \"Rhea americana\" looks like an ostrich, but is not taxonomically one. Even for categories like \"marmot\" most people named it \"squirrel\". Overall, ngram translation agrees 37% of the time with human supplied translations and the SVM translation agrees 21% of the time, indicating that translation learning is non-trivial.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Evaluating Image Entry-Level Predictions", "text": "We measure the accuracy of our proposed entry-level category prediction methods by evaluating how well we can predict nouns freely associated with images by users on MTurk. We select two evaluation image sets. Dataset A: contains 1000 images selected at random from the million image dataset. Dataset B: contains 1000 images selected from images displaying high confidence in concept predic-  tions. Both sets are completely disjoint from the sets of images used for learning. For each image, we instruct 3 users on MTurk to write down any nouns that are relevant to the image content. Because these annotations are free associations we observe a large and varied set of associated nouns -3610 distinct nouns total in our evaluation sets. This makes noun prediction extremely challenging! We evaluate prediction of all nouns associated with an image by Turkers (Table 1) and prediction of nouns assigned by at least 2 of 3 Turkers (Table 2). Here N+ refers to the working vocabulary of the method -the total number of unique words output by the method for the given test set. For reference we compute the precision of one human annotator against the other two and found that on Dataset A humans were able to predict what the previous annotators labeled with 0.35 precision and with 0.45 precision for Dataset B.\nResults show precision and recall for prediction on each of our Datasets, comparing: leaf node classification performance (flat classifier), the outputs of hedging [6], and our proposed entry-level category predictors (ngram-biased mapping, SVM mapping, and a joint model). Performance at this task on Set B is in general better than performance on Dataset A, because Dataset B contains images which have confident classifier scores. Surprisingly their difference in performance is not extreme and performance on both sets is admirable for this challenging task.\nOn all datasets and tasks we find the joint model to perform the best (section 4.3), followed by supervised prediction (section 4.2), and propagated prediction (section 4.1). In addition, we greatly outperform both leaf node classification and the hedging technique [6] (approximately doubling their performance on this task).", "publication_ref": ["b5", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "Results indicate that our inferred concept translations are meaningful and that our models are able to predict entrylevel categories -the words people use to describe image content -for images. These methods could apply to many different end-user applications that require recognition outputs that are useful for human consumption, including tasks related to description generation and retrieval.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Nltk: the natural language toolkit", "journal": "", "year": "2006", "authors": "S Bird"}, {"ref_id": "b1", "title": "Web 1t 5-gram version 1", "journal": "", "year": "2006", "authors": "T Brants; A Franz"}, {"ref_id": "b2", "title": "What does classifying more than 10,000 image categories tell us", "journal": "", "year": "2010", "authors": "J Deng; A C Berg; K Li; L Fei-Fei"}, {"ref_id": "b3", "title": "Large scale visual recognition challenge", "journal": "", "year": "", "authors": "J Deng; A C Berg; S Satheesh; H Su; A Khosla; L Fei-Fei"}, {"ref_id": "b4", "title": "ImageNet: A Large-Scale Hierarchical Image Database", "journal": "", "year": "2009", "authors": "J Deng; W Dong; R Socher; L.-J Li; K Li; L Fei-Fei"}, {"ref_id": "b5", "title": "Hedging your bets: Optimizing accuracy-specificity trade-offs in large scale visual recognition", "journal": "", "year": "2008", "authors": "J Deng; J Krause; A C Berg; L Fei-Fei"}, {"ref_id": "b6", "title": "The pascal visual object classes (voc) challenge", "journal": "IJCV", "year": "2002", "authors": "M Everingham; L Van Gool; C K I Williams; J Winn; A Zisserman"}, {"ref_id": "b7", "title": "Every picture tells a story: generating sentences for images", "journal": "", "year": "2010", "authors": "A Farhadi; M Hejrati; M A Sadeghi; P Young; C Rashtchian; J Hockenmaier; D Forsyth"}, {"ref_id": "b8", "title": "WordNet: an electronic lexical database", "journal": "MIT Press", "year": "1998", "authors": ""}, {"ref_id": "b9", "title": "Object detection with discriminatively trained partbased models. tPAMI", "journal": "", "year": "2009", "authors": "P Felzenszwalb; R Girshick; D Mcallester; D Ramanan"}, {"ref_id": "b10", "title": "Pictures and names: making the connection. cognitive psychology. Cognitive Psychology", "journal": "", "year": "1984", "authors": "P Jolicoeur; M A Gluck; S M Kosslyn"}, {"ref_id": "b11", "title": "Imagenet classification with deep convolutional neural networks", "journal": "", "year": "2012", "authors": "A Krizhevsky; I Sutskever; G Hinton"}, {"ref_id": "b12", "title": "Babytalk: Understanding and generating simple image descriptions", "journal": "", "year": "2011", "authors": "G Kulkarni; V Premraj; S Dhar; S Li; Y Choi; A C Berg; T L Berg"}, {"ref_id": "b13", "title": "Collective generation of natural image descriptions", "journal": "", "year": "2012", "authors": "P Kuznetsova; V Ordonez; A Berg; T L Berg; Y Choi"}, {"ref_id": "b14", "title": "Building high-level features using large scale unsupervised learning", "journal": "", "year": "2012", "authors": "Q Le; M Ranzato; R Monga; M Devin; K Chen; G Corrado; J Dean; A Ng"}, {"ref_id": "b15", "title": "Im2text: Describing images using 1 million captioned photographs", "journal": "", "year": "2004", "authors": "V Ordonez; G Kulkarni; T L Berg"}, {"ref_id": "b16", "title": "Towards good practice in large-scale learning for image classification", "journal": "", "year": "2012", "authors": "F Perronnin; Z Akata; Z Harchaoui; C Schmid"}, {"ref_id": "b17", "title": "Principles of categorization. Cognition and Categorization", "journal": "", "year": "1978", "authors": "E Rosch"}, {"ref_id": "b18", "title": "Labelme: a database and web-based tool for image annotation", "journal": "IJCV", "year": "2008", "authors": "B C Russell; A Torralba; K P Murphy; W T Freeman"}, {"ref_id": "b19", "title": "80 million tiny images: a large dataset for non-parametric object and scene recognition", "journal": "PAMI", "year": "1958", "authors": "A Torralba; R Fergus; W T Freeman"}, {"ref_id": "b20", "title": "Segmentation as selective search for object recognition", "journal": "", "year": "2001", "authors": "K E A Van De Sande; J R R Uijlings; T Gevers; A W M Smeulders"}, {"ref_id": "b21", "title": "Sun database: Large scale scene recognition from abbey to zoo", "journal": "", "year": "2010", "authors": "J Xiao; J Hays; K Ehinger; A Oliva; A Torralba"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. Example translation between a WordNet based object category prediction and what people might call the depicted object.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 .2Figure 2. Left: shows the relationship between parameter \u03bb and the target vocabulary size. Right: shows the relationship between parameter \u03bb and agreement accuracy with human labeled synsets evaluated against the most agreed human label (red) and any human label (cyan).", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 .3Figure 3. Translations from ImageNet leaf node synset categories to entry level categories using our automatic approaches from sections 3.1 (Ngram-) and 3.2 (SVM-) and crowd-sourced human annotations from section 2 (Human-).", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 .4Figure 4. Relationship between average precision agreement and working vocabulary size (on a set of 1000 images) for the hedging method[6] (blue) and our direct translation method (red).", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 66Figure 6. Sample predictions from our experiments on a test set for each type of category. Note that image labels come from caption nouns, so some images marked as correct predictions might not depict the target concept whereas some images marked as wrong predictions might actually depict the target category.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 7 .7Figure 7. For every node in the tree we have estimates of visual content coming from two sources a) naturalness and hierarchical aggregation fnat and b) supervised learning fsvm.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 8 .8Figure 8. Example translations. 1 st col shows images. 2 nd col shows MTurk associated nouns. These represent the ground truth annotations (entry-level categories) we would like to predict (colored in blue). 3 rd col shows predicted nouns using a standard multiclass flat-classifier. 4 th col shows nouns predicted by the method of [6]. 5 th col shows our n-gram based method predictions. 6 th col shows our SVM mapping predictions and finally the 7 th column shows the labels predicted by our joint model. Matches are colored in green. Tables 1,2 show the measured improvements in recall and precision. We provide more examples in supplemental material.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u03c4 (v, \u03bb) = arg max w [\u03c6(w) \u2212 \u03bb\u03c8(w, v)], w \u2208 \u03a0(v) (1) \u03a0(v)", "formula_coordinates": [3.0, 50.11, 238.76, 236.25, 33.59]}, {"formula_id": "formula_1", "formula_text": "= i 1[word(\u03c4 (x i , \u03bb)) = y i ](2)", "formula_coordinates": [3.0, 125.99, 309.77, 160.38, 20.97]}, {"formula_id": "formula_2", "formula_text": "f (v, I) = \u23a7 \u23a8 \u23a9f (v, I), if v is a leaf node v \u2208Z(v)f (v , I), if v is an internal node (3)", "formula_coordinates": [4.0, 60.37, 172.08, 225.99, 53.8]}, {"formula_id": "formula_3", "formula_text": "\u03b3(v,\u03bb) = [\u03c6(v) \u2212\u03bb\u03c8(v)](4)", "formula_coordinates": [4.0, 116.06, 346.96, 170.3, 9.96]}, {"formula_id": "formula_4", "formula_text": "f nat (v, I,\u03bb) = f (v, I)\u03b3(v,\u03bb) (5) f nat (v, I,\u03bb) = f (v, I)[\u03c6(v) \u2212\u03bb\u03c8(v)] (6", "formula_coordinates": [4.0, 91.07, 548.36, 195.29, 34.47]}, {"formula_id": "formula_5", "formula_text": ")", "formula_coordinates": [4.0, 282.49, 572.56, 3.87, 9.38]}, {"formula_id": "formula_6", "formula_text": "X (i) = {x (i) j | x (i) j = max(f (v j , I (i) , b k )}(7)", "formula_coordinates": [4.0, 340.52, 676.23, 204.59, 14.93]}, {"formula_id": "formula_7", "formula_text": "F svm (I, \u0398) = [f svm (v i , I, \u03b8 i )] (8", "formula_coordinates": [5.0, 106.13, 662.58, 176.37, 10.71]}, {"formula_id": "formula_8", "formula_text": ")", "formula_coordinates": [5.0, 282.49, 663.02, 3.87, 9.38]}, {"formula_id": "formula_9", "formula_text": "F svm (I, \u0398) = 1 1 \u2212 exp(a\u0398 X + b) (9) R(\u03b8 i ) = 1 2 \u03b8 i + c |D| j=1 max(0, 1 \u2212 y (j) i \u03b8 i X (j) ) 2 (10)", "formula_coordinates": [5.0, 94.13, 73.0, 450.98, 642.99]}, {"formula_id": "formula_10", "formula_text": "f nat (v) = f nat (v, I,\u03bb), if v \u2208 dom(f nat ) 0 otherwise (11) f svm (v) = f svm (v, I, \u03b8 v ), if v \u2208 dom(f svm ) 0 otherwise (12", "formula_coordinates": [6.0, 60.78, 564.85, 225.58, 61.45]}, {"formula_id": "formula_11", "formula_text": ")", "formula_coordinates": [6.0, 282.21, 609.37, 4.15, 9.38]}, {"formula_id": "formula_12", "formula_text": "f joint (v, \u03b1) = \u03b1 f nat (v) + f svm (v)(13)", "formula_coordinates": [6.0, 96.56, 636.04, 189.8, 10.71]}, {"formula_id": "formula_13", "formula_text": "v 1 ,v 2 , ...,v n = argmax v1,v2,...,vn i f joint (v i , \u03b1) (14)", "formula_coordinates": [6.0, 74.57, 695.76, 211.79, 20.97]}], "doi": ""}