{"title": "Human Detection via Classification on Riemannian Manifolds", "authors": "Oncel Tuzel; Fatih Porikli; Peter Meer", "pub_date": "2007-07-46", "abstract": "We present a new algorithm to detect humans in still images utilizing covariance matrices as object descriptors. Since these descriptors do not lie on a vector space, well known machine learning techniques are not adequate to learn the classifiers. The space of d-dimensional nonsingular covariance matrices can be represented as a connected Riemannian manifold. We present a novel approach for classifying points lying on a Riemannian manifold by incorporating the a priori information about the geometry of the space. The algorithm is tested on INRIA human database where superior detection rates are observed over the previous approaches.", "sections": [{"heading": "Human Detection via Classification on Riemannian Manifolds 1. Introduction", "text": "Human detection in still images is considered among the hardest examples of object detection problems. The articulated structure and variable appearance of the human body, combined with illumination and pose variations, contribute to the complexity of the problem.\nThe leading approaches in human detection can be separated into two groups based on the search method. The first group of methods is based on sequentially applying a classifier at all the possible subwindows in a given image. In [16], a polynomial support vector machine (SVM) was learned using Haar wavelets as human descriptors. Later, the work was extended to multiple classifiers trained to detect human parts, and the responses inside the detection window are combined to give the final decision [14]. Similar to still images, in [23], a real time moving human detection algorithm was described also using Haar wavelet descriptors but extracted from space-time differences in video. Using Ad-aBoost, the most discriminative features were selected, and multiple classifiers were combined to form a rejection cascade, such that if any classifier rejects a hypothesis then it is considered a negative example. In [4], an excellent human detector was described by training an SVM classifier using densely sampled histogram of oriented gradients (similar to SIFT descriptors) inside the detection window. The performance of the proposed descriptors was shown on INRIA human database and all the previous methods had false positive rates of at least one-two orders of magnitude higher at the same detection rates. Recently in a similar approach [24], near real time detection performances were achieved by training a cascade model using histogram of oriented gradients (HOG) features.\nThe second group of methods is based on detecting human parts [5,9,19] or common shapes [12] and assembling these local features according to geometric constraints to form the final human model. In [13], parts were represented by co-occurrences of local orientation features and separate detectors were trained for each part using AdaBoost. Human location was determined by maximizing the joint likelihood of part occurrences combined according to the geometric relations. A human detection system for crowded scenes was described in [11]. The approach combined local appearance features and their geometric relations with global cues by top-down segmentation based on per pixel likelihoods. Other approaches include using silhouette information either in matching [8] or in classification framework [15].\nOur approach belongs to the first group, and is most similar to [23] and [24], but instead of Haar wavelets or HOG features we use covariance features as human descriptors. Covariance features were introduced in [21] for matching and texture classification problems, and later were extended to tracking [18]. A region was represented by the covariance matrix of image features, such as spatial location, intensity, higher order derivatives, etc. Similarly, we represent a human with several covariance matrices of overlapping regions. It is not adequate to use classical machine learning techniques to train the classifiers since the covariance matrices do not lie on a vector space.\nSymmetric positive definite matrices (nonsingular covariance matrices) can be formulated as a connected Riemannian manifold. The main contribution of this paper is a novel approach for classifying points lying on a Riemannian manifold by incorporating the a priori information about the geometry of the space. Some of the relevant papers for clustering data points lying on differentiable manifolds can be found in [1,20,22].\nThe paper is organized as follows. In Section 2, we briefly describe the covariance descriptors. In Section 3, we present an introduction to Riemannian geometry focussing on the space of symmetric positive definite matrices. In Sections 4 and 5, we describe our algorithm for classification on Riemannian manifolds and its application to human detection. The experiments are presented in Section 6.", "publication_ref": ["b15", "b13", "b23", "b3", "b24", "b4", "b8", "b19", "b11", "b12", "b10", "b7", "b14", "b23", "b24", "b21", "b17", "b0", "b20", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Covariance Descriptors", "text": "Here we present a brief overview of covariance descriptors [21] and its specialization for human detection. Let I be one-dimensional intensity or three-dimensional color image, and F be the\nW \u00d7 H \u00d7 d dimensional feature image extracted from I F (x, y) = \u03a6(I, x, y)(1)\nwhere the function \u03a6 can be any mapping such as intensity, color, gradients, filter responses, etc. For a given rectangular region R \u2282 F , let {z i } i=1..S be the d-dimensional feature points inside R. The region R is represented with the d \u00d7 d covariance matrix of the feature points\nC R = 1 S \u2212 1 S i=1 (z i \u2212 \u00b5)(z i \u2212 \u00b5) T (2\n)\nwhere \u00b5 is the mean of the points.\nFor human detection problem we define the mapping \u03a6(I, x, y) as\nx y |I x | |I y | I 2 x + I 2 y |I xx | |I yy | arctan |I x | |I y | T(3)\nwhere x and y are pixel location, I x , I xx , .. are intensity derivatives and the last term is the edge orientation. With the defined mapping the input image is mapped to a d = 8 dimensional feature image. The covariance descriptor of a region is an 8 \u00d7 8 matrix and due to symmetry only upper triangular part is stored, which has only 36 different values. The descriptor encodes information of the variances of the defined features inside the region, their correlations with each other and spatial layout.\nThere is an efficient way to compute covariance descriptors using integral images [21]. After constructing d(d+1)/2 integral images, the covariance descriptor of any rectangular region can be computed in O(d 2 ) time independent of the region size. We refer readers to [21] for more details of the descriptors and computational method.\nGiven an arbitrary sized detection window R, there are a very large number of covariance descriptors that can be computed from subwindows r 1,2,... , as shown in Figure 1. We perform sampling and consider all the subwindows r starting with minimum size of 1/10 of the width and height of the detection window R, at all possible locations. The size of r is incremented in steps of 1/10 along the horizontal or vertical, or both, until r = R. Although the approach might be considered redundant due to overlaps, there is significant evidence that the overlapping regions are an important factor in detection performances [4,24]. The boosting mechanism, that will be described later, allows us to search for the best regions.\nThe covariance descriptors are robust towards illumination changes. We would like to enhance this property to also include local illumination variations in an image. Let r be a possible feature subwindow inside the detection window R. We compute the covariance of the detection window C R and subwindow C r using integral representation. The normalized covariance matrix is computed by dividing the columns and rows of C r with the respective diagonal entries of C R . The method described is equivalent to first normalizing the feature vectors inside the region R to have zero mean and unit standard deviation, and after that computing the covariance descriptor of subwindow r. The process only requires d 2 extra division operations.", "publication_ref": ["b21", "b21", "b21", "b3", "b24"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Riemannian Geometry", "text": "We present a brief introduction to Riemannian geometry focussing on the space of symmetric positive definite matrices. See [2] for a more detailed description. We refer to points lying on a vector space with small bold letters x \u2208 R m , whereas points lying on the manifold with capital bold letters X \u2208 M.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "Riemannian Manifolds", "text": "A manifold is a topological space which is locally similar to an Euclidean space. Every point on the manifold has a neighborhood for which there exists a homeomorphism (one-to-one, onto and continuous mapping in both directions), mapping the neighborhood to R m . For differentiable manifolds, it is possible to define the derivatives of the curves on the manifold. The derivatives at a point X on the manifold lies in a vector space T X , which is the tangent space at that point. A Riemannian manifold M is a differentiable manifold in which each tangent space has an inner product <, > X which varies smoothly from point to point. The inner product induces a norm for the tangent vectors on the tangent space, such that, y 2 X =< y, y > X . The minimum length curve connecting two points on the manifold is called the geodesic, and the distance between the points d(X, Y) is given by the length of this curve. Let y \u2208 T X and X \u2208 M. From X there exists a unique geodesic starting with the tangent vector y. The exponential map, exp X : T X \u2192 M, maps the vector y to the point reached by this geodesic, and the distance of the geodesic is given by d(X, exp X (y)) = y X .\nIn general, the exponential map exp X is onto but only one-to-one in a neighborhood of X. Therefore, the inverse mapping log X : M \u2192 T X is uniquely defined only around the neighborhood of the point X. If for any Y \u2208 M, there exists several y \u2208 T X such that Y = exp X (y), then log X (Y) is given by the tangent vector with the smallest norm. Notice that both operators are point dependent where the dependence is made explicit with the subscript.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Space of Symmetric Positive Definite Matrices", "text": "The d \u00d7 d dimensional symmetric positive definite matrices (nonsingular covariance matrices), Sym + d , can be formulated as a connected Riemannian manifold and an invariant Riemannian metric on the tangent space of Sym + d is given by [17] <\ny, z > X = tr X \u2212 1 2 yX \u22121 zX \u2212 1 2 .(4)\nThe exponential map associated to the Riemannian metric\nexp X (y) = X 1 2 exp X \u2212 1 2 yX \u2212 1 2 X 1 2(5)\nis a global diffeomorphism (one-to-one, onto and continuously differentiable mapping in both directions). Therefore, the logarithm is uniquely defined at all the points on the manifold\nlog X (Y) = X 1 2 log X \u2212 1 2 YX \u2212 1 2 X 1 2 . (6\n)\nThe exp and log are the ordinary matrix exponential and logarithm operators. Not to be confused, exp X and log X are manifold specific operators which are also point dependent, For symmetric matrices, the ordinary matrix exponential and logarithm operators can be computed easily. Let \u03a3 = UDU T be the eigenvalue decomposition of a symmetric matrix. The exponential series is\nX \u2208 Sym + d .\nexp(\u03a3) = \u221e k=0 \u03a3 k k! = Uexp(D)U T (7)\nwhere exp(D) is the diagonal matrix of the eigenvalue exponentials. Similarly, the logarithm is given by\nlog(\u03a3) = \u221e k=1 (\u22121) k\u22121 k (\u03a3 \u2212 I) k = Ulog(D)U T . (8)\nThe exponential operator is always defined, whereas the logarithms only exist for symmetric matrices with positive eigenvalues, Sym + d . From the definition of the geodesic given in the previous section, the distance between two points on Sym + d is measured by substituting ( 6) into (4)\nd 2 (X, Y) = < log X (Y), log X (Y) > X = tr log 2 (X \u2212 1 2 YX \u2212 1 2 ) .(9)\nWe note that an equivalent form of the affine invariant distance metric was first given in [6], in terms of joint eigenvalues of X and Y.\nWe define an orthogonal coordinate system on the tangent space with the vector operation. The orthogonal coordinates of a vector y on the tangent space at point X is given by vec X (y) = upper(X \u2212 1 2 yX \u2212 1 2 ) (10) where upper refers to the vector form of the upper triangular part of the matrix. The mapping vec X , relates the Riemannian metric (4) on the tangent space to the canonical metric defined in R m .", "publication_ref": ["b16", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Mean of the Points on Riemannian Manifolds", "text": "Let {X i } i=1...N be the set of points on a Riemannian manifold M. Similar to Euclidean spaces, the Karcher mean [10] of points on Riemannian manifold, is the point on M which minimizes the sum of squared distances\n\u00b5 = arg min Y\u2208M N i=1 d 2 (X i , Y)(11)\nwhere in our case d 2 is the distance metric (9). Differentiating the error function with respect to Y and setting it equal to zero, gives the following gradient descent procedure [17]\n\u00b5 t+1 = exp \u00b5 t 1 N N i=1 log \u00b5 t (X i )(12)\nwhich finds a local minimum of the error function. The method iterates by computing first order approximations to the mean on the tangent space. The weighted mean computation is similar to (12). We replace inside of the exponential, the mean of the tangent vectors with the weighted mean\n1 P N i=1 wi N i=1 w i log \u00b5 t (X i ).", "publication_ref": ["b9", "b8", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Classification on Riemannian Manifolds", "text": "Let {(X i , y i )} i=1...N be the training set with respect to class labels, where X i \u2208 M, y i \u2208 {0, 1} and M is a Riemannian manifold. We want to find a function F (X) : M \u2192 {0, 1}, which divides the manifold into two based on the training set of labeled items.\nA function which divides the manifold is rather a complicated notion compared to Euclidean space. For example, consider the simplest form a linear classifier on R 2 . A point and a direction vector on R 2 defines a line which separates R 2 into two. Equivalently, on a two-dimensional differentiable manifold, we can consider a point on the manifold and a tangent vector on the tangent space of the point, which together defines a curve on the manifold via exponential map. For example, if we consider the image of the lines on the 2-torus, the curves never divide the manifold into two.\nA straightforward approach for classification would be to map the manifold to a higher dimensional Euclidean space, which can be considered as flattening the manifold. However in a general case, there is no such mapping that globally preserves the distances between the points on the manifold. Therefore a classifier trained on the flattened space does not reflect the global structure of the points.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Local Maps and Boosting", "text": "We propose an incremental approach by training several weak classifiers on the tangent space and combining them through boosting. We start by defining mappings from neighborhoods on the manifold to the Euclidean space, similar to coordinate charts. Our maps are the logarithm maps, log X , that map the neighborhood of points X \u2208 M to the tangent spaces T X . Since this mapping is a homeomorphism around the neighborhood of the point, the structure of the manifold is preserved locally. The tangent space is a vector space and we learn the classifiers on this space. The classifiers can be trained on the tangent space at any point on the manifold. The mean of the points (11) minimizes the sum of squared distances on the manifold, therefore it is a good approximation up to a first order.\nAt each iteration, we compute the weighted mean of the points where the weights are adjusted through boosting. We map the points to the tangent space at the mean and learn a weak classifier on this vector space. Since the weights of the samples which are misclassified during earlier stages of boosting increase, the weighted mean moves towards these points producing more accurate classifiers for these points. The approach minimizes the approximation error through averaging over several weak classifiers.  -Fit the function g l (x) by weighted least-square regression of zi to xi using weights wi.\nzi = y i \u2212p(X i ) p(X i )(1\u2212p(X i )) wi = p(Xi)(1 \u2212 p(Xi)). -Compute\n-Update F (X) \u2190 F (X) + 1 2 f l (X)\nwhere f l is defined in (15) and p(X) \u2190 e F (X) e F (X) +e \u2212F (X) .\n\u2022 Output the classifier sign \n[F (X)] = sign [ P L l=1 f l (X)]", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}, {"heading": "LogitBoost on Riemannian Manifolds", "text": "We start with brief description of LogitBoost algorithm [7] on vector spaces. We consider the binary classification problem, y i \u2208 {0, 1}. The probability of x being in class 1 is represented by\np(x) = e F (x) e F (x) + e \u2212F (x) F (x) = 1 2 L l=1 f l (x). (13\n)\nThe LogitBoost algorithm learns the set of regression functions {f l (x)} l=1...L (weak learners) by minimizing the negative binomial log-likelihood of the data l(y, p(x))\n\u2212 N i=1 [y i log(p(x i )) + (1 \u2212 y i )log(1 \u2212 p(x i ))](14)\nthrough Newton iterations. At the core of the algorithm, LogitBoost fits a weighted least square regression, f l (x) of training points x i \u2208 R m to response values z i \u2208 R with weights w i . The LogitBoost algorithm on Riemannian manifolds is similar to original LogitBoost, except differences at the level of weak learners. In our case, the domain of the weak learners are in M such that f l (X) : M \u2192 R. Following the discussion of the previous section, we learn the regression functions in the tangent space at the weighted mean of the points on the manifold. We define the weak learners as\nf l (X) = g l (vec \u00b5 l (log \u00b5 l (X)))(15)\nand learn the functions g l (x) : R m \u2192 R and the weighted mean of the points \u00b5 l \u2208 M. Notice that, the mapping vec (10), gives the orthogonal coordinates of the tangent vectors.\nThe algorithm is presented in Figure 2. The steps marked with ( * ) are the only differences from original LogitBoost algorithm. For functions {g l } l=1...L , it is possible to use any form of weighted least squares regression such as linear functions, regression stumps, etc., since the domain of the functions are in R m . A very large number of covariance descriptors can be computed from a single detection window and it is computationally intractable to test all of them. At each boosting iteration of kth LogitBoost level, we sample 200 subwindows among all the possible subwindows, and construct normalized covariance descriptors as described in Section 2. We learn the weak classifiers representing each subwindow, and add the best classifier which minimizes negative binomial log-likelihood ( 14) to the cascade level k.", "publication_ref": ["b6"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Human Detection", "text": "Each level of cascade detector is optimized to correctly detect at least 99.8% of the positive examples, while rejecting at least 35% of the negative examples. In addition, we enforce a margin constraint between the positive samples and the decision boundary. Let p k (X) be the probability of a sample being positive at cascade level k, evaluated through (13). Let X p be the positive example that has the (0.998N p )th largest probability among all the positive examples. Let X n be the negative example that has the (0.35N n )th smallest probability among all the negative examples.\nWe continue to add weak classifiers to cascade level k until p k (X p ) \u2212 p k (X n ) > th b , where we set th b = 0.2. \nk if p k (X) > p k (X p ) \u2212 th b > p k (X n ) or equivalently F k (X) > F k (X n ).\nWith the proposed method, any of the positive training samples in the top 99.8 percentile have at least th b more probability than the decision boundary. The process continues with the training of (k + 1)th cascade level, until k = K.\nThe method presented here is a slight modification of the LogitBoost classifier on Riemannian manifolds described in Section 4.2. We compute the weighted means of only the positive examples since negative set is not well characterized for detection tasks. Although rarely happens, if some of the features are totally correlated, there will be singularities in the covariance descriptor. We ignore those cases by adding very small identity matrix to the covariance.", "publication_ref": ["b12"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We perform the experiments on INRIA human database [4]. The database contains 1774 human annotations (3548 with reflections) and 1671 person free images. Detection on INRIA human database is challenging since it includes subjects with a wide range of variations in pose, clothing, illumination, background and partial occlusions. We perform the same separation of training -testing sets to directly compare the results with the methods of Dalal & Triggs [4] and Zhu et.al. [24]. To our knowledge, these two methods produce the best results published on the given database, and a detailed comparison with the other previous methods is given in [4].\nIn the first experiment, we compare our results with [4] and [24]. Although it has been noted that kernel SVM is computationally expensive, we consider both the linear and kernel SVM method of [4]. The method of [24] trains a boosted classifier using HOG features, and two different results were reported based on the normalization. Here we consider only the best performing result, the L2-norm.\nIn Figure 4, we plot the detection error tradeoff curves on a log-log scale. The y-axis corresponds to the miss rate, F alseN eg/(F alseN eg + T rueP os), and the xaxis corresponds to false positives per window (FPPW), F alseP os/(T rueN eg + F alseP os). The curve for our method is generated by adding one cascade level at a time. For example, in our case the rightmost marker at 7.5 * 10 \u22123 FPPW corresponds to detection using only the first 11 levels of cascade, whereas the marker positioned at 4 * 10 \u22125 FPPW corresponds to cascade of all 30 levels. The markers between the two extremes correspond to a cascade of between 11 to 30 levels.\nTo generate the result at 10 \u22125 FPPW (leftmost marker), we shifted the decision boundaries of all the cascade levels to produce less false positives at the cost of higher miss rates. We place the decision boundary to p k (X) > (p k (X n ) + p k (X p ))/2, such that the margin th b is reduced by half. See Section 5 for details. We see that at almost all the false positive rates, our miss rates are significantly lower than other approaches. The closest result to our method is the kernel SVM classifier of [4], which requires kernel evaluation at 1024 dimensional space to classify a single detection window. If we consider 10 \u22124 as an acceptable false positive rate per window, our miss rate is 6.8%, where the the second best result is 9.3%.\nSince the method removes samples which are rejected by the previous levels of cascade, during the training of last levels only very small amount of negative samples, order of 10 2 remained. At these levels, the training error did not generalize well, such that the same detection rates are not achieved on the test set. This can be seen by the dense markers around FPPW < 7 * 10 \u22125 . We believe that better detection rates can be achieved at low false positive rates with introduction of more negative images. We also note that, in our method 25% of false positives come from a single textured image, where the training set does not include a similar image.\nIn the second experiment, we consider an empirical val- idation of the presented classification algorithm on Riemannian manifolds. In Figure 5, we present the detection error tradeoff curves for four different approaches.\n\u2022 The original method, which maps the points to the tangent spaces at the weighted means. \u2022 The mean computation step is removed from the original algorithm and points are always mapped to the tangent space at the identity matrix. \u2022 We ignore the geometry of Sym + 8 , and stack the upper triangular part of the covariance matrix into a vector, such that learning is performed on the vector space.\n\u2022 We replace the covariance descriptors with HOG descriptors, and perform original LogitBoost classification. The original method outperforms all the other approaches significantly. The second best result is achieved by mapping points to the tangent space at the identity matrix followed by the vector space approaches.\nIn Figure 6, we plot the number of weak classifiers at each cascade level and the accumulated rejection rate over the cascade levels. There are very few classifiers on early levels of cascade and the first five level reject 90% of the negative examples. On average our method requires evaluation of 8.45 covariance descriptors per negative detection window, whereas on average 15.62 HOG evaluations are required in [24].\nIn Figure 7, several detection examples are shown for crowded scenes with humans having variable illumination, appearance, pose and partial occlusion. We search the images at five different scales and the white regions show all the detection results. We filter the detection results with adaptive bandwidth mean shift filtering [3], with bandwidth 1/10 of the window width and height. Black dots show the modes, and ellipses are generated by averaging the detection window sizes converging to the mode. The training of the classifiers took two days on a current state of art PC, which is a reasonable time to train a cascade model. Given a novel image, on average the method can search around 3000 detection windows per second. The most computationally expensive operation of our method is the eigenvalue decomposition to compute the logarithm of a matrix, which requires O(d 3 ) arithmetic operations. Compared to previous approaches, the search time is faster than [4] but slower than [24] which produces significantly lower detection rates.", "publication_ref": ["b3", "b3", "b24", "b3", "b3", "b24", "b3", "b24", "b3", "b24", "b2", "b3", "b24"], "figure_ref": ["fig_7", "fig_8", "fig_9"], "table_ref": []}, {"heading": "Conclusion", "text": "We presented a new approach for human detection problem utilizing covariance matrices as object descriptors and a novel learning algorithm on the Riemannian manifolds. The proposed learning algorithm is not specific to Sym + d , and can be used to train classifiers for points lying on any connected Riemannian manifold. The superior performance of the proposed approach is shown on INRIA human database, where previous methods have significantly higher miss rates at almost all the false positive rates per window.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Affine invariance revisited", "journal": "", "year": "2006", "authors": "E Begelfor; M Werman"}, {"ref_id": "b1", "title": "An Introduction to Differentiable Manifolds and Riemannian Geometry", "journal": "Academic Press", "year": "2002", "authors": "W M Boothby"}, {"ref_id": "b2", "title": "Mean shift: A robust approach toward feature space analysis", "journal": "IEEE Trans. Pattern Anal. Machine Intell", "year": "2002", "authors": "D Comaniciu; P Meer"}, {"ref_id": "b3", "title": "Histograms of oriented gradients for human detection", "journal": "", "year": "2005", "authors": "N Dalal; B Triggs"}, {"ref_id": "b4", "title": "Pictorial structures for object recognition", "journal": "Intl. J. of Computer Vision", "year": "2005", "authors": "P F Felzenszwalb; D P Huttenlocher"}, {"ref_id": "b5", "title": "A metric for covariance matrices", "journal": "Dept. of Geodesy and Geoinformatics", "year": "1999", "authors": "W F\u00f6rstner; B Moonen"}, {"ref_id": "b6", "title": "Additive logistic regression: A statistical view of boosting", "journal": "Ann. Statist", "year": "2000", "authors": "J Friedman; T Hastie; R Tibshirani"}, {"ref_id": "b7", "title": "Real-time object detection for smart vehicles", "journal": "", "year": "1999", "authors": "D Gavrila; V Philomin"}, {"ref_id": "b8", "title": "Probabilistic methods for finding people", "journal": "Intl. J. of Computer Vision", "year": "2001", "authors": "S Ioffe; D A Forsyth"}, {"ref_id": "b9", "title": "Riemannian center of mass and mollifier smoothing", "journal": "Commun. Pure Appl. Math", "year": "1977", "authors": "H Karcher"}, {"ref_id": "b10", "title": "Pedestrian detection in crowded scenes", "journal": "", "year": "2005", "authors": "B Leibe; E Seemann; B Schiele"}, {"ref_id": "b11", "title": "Multiple object class detection with a generative model", "journal": "", "year": "2006", "authors": "K Mikolajczyk; B Leibe; B Schiele"}, {"ref_id": "b12", "title": "Human detection based on a probabilistic assembly of robust part detectors", "journal": "", "year": "2004", "authors": "K Mikolajczyk; C Schmid; A Zisserman"}, {"ref_id": "b13", "title": "Example-based object detection in images by components", "journal": "IEEE Trans. Pattern Anal. Machine Intell", "year": "2001", "authors": "A Mohan; C Papageorgiou; T Poggio"}, {"ref_id": "b14", "title": "Incremental learning of object detectors using a visual shape alphabet", "journal": "", "year": "2006", "authors": "A Opelt; A Pinz; A Zisserman"}, {"ref_id": "b15", "title": "A trainable system for object detection", "journal": "Intl. J. of Computer Vision", "year": "2000", "authors": "P Papageorgiou; T Poggio"}, {"ref_id": "b16", "title": "A Riemannian framework for tensor computing", "journal": "Intl. J. of Computer Vision", "year": "2006", "authors": "X Pennec; P Fillard; N Ayache"}, {"ref_id": "b17", "title": "Covariance tracking using model update based on Lie algebra", "journal": "", "year": "2006", "authors": "F Porikli; O Tuzel; P Meer"}, {"ref_id": "b18", "title": "White dots show all the detection results. Black dots are the modes generated by mean shift smoothing and the ellipses are average detection window sizes", "journal": "", "year": "", "authors": ""}, {"ref_id": "b19", "title": "Learning to parse pictures of people", "journal": "", "year": "2002", "authors": "R Ronfard; C Schmid; B Triggs"}, {"ref_id": "b20", "title": "Nonlinear mean shift for clustering over analytic manifolds", "journal": "", "year": "2006", "authors": "R Subbarao; P Meer"}, {"ref_id": "b21", "title": "Region covariance: A fast descriptor for detection and classification", "journal": "", "year": "2006", "authors": "O Tuzel; F Porikli; P Meer"}, {"ref_id": "b22", "title": "Simultaneous multiple 3D motion estimation via mode finding on Lie groups", "journal": "", "year": "2005", "authors": "O Tuzel; R Subbarao; P Meer"}, {"ref_id": "b23", "title": "Detecting pedestrians using patterns of motion and appearance", "journal": "", "year": "2003", "authors": "P Viola; M Jones; D Snow"}, {"ref_id": "b24", "title": "Fast human detection using a cascade of histograms of oriented gradients", "journal": "", "year": "2006", "authors": "Q Zhu; S Avidan; M C Yeh; K T Cheng"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. Covariance descriptor. The d dimensional feature image F is constructed from input image I through mapping \u03a6. The detection window is R and r1, r2 are two possible descriptor subwindows.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "The tangent space of Sym + d is the space of d \u00d7 d symmetric matrices and both the manifold and the tangent spaces are m = d(d + 1)/2 dimensional.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Input: 2 \u2022-2Training set {(Xi, yi)}i=1...N , Xi \u2208 M, yi \u2208 {0, 1}\u2022 Start with weights wi = 1/N , i = 1...N , F (X) = 0 and p(Xi) = 1 Repeat for l = 1...L Compute the response values and weights", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "weighted mean of the points \u00b5 l = arg min Y\u2208M P N i=1 wid 2 (Xi, Y) (12). ( * ) -Map the data points to the tangent space at \u00b5 l xi = vec\u00b5 l (log \u00b5 l (Xi)). ( * )", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 2 .2Figure 2. LogitBoost on Riemannian Manifolds.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Forhuman detection, we combine K = 30 LogitBoost classifiers on Sym + 8 with rejection cascade, as shown in Figure 3. Weak classifiers {g l } l=1...L are linear regression functions learned on the tangent space of Sym + 8 . The tangent space is m = 36 dimensional vector space. Let N pi and N ni be the number of positive and negative images in the training set. Since any detection window sampled from a negative image is a negative sample, it is possible to generate much more negative examples than the number of negative images. Assume that we are training the kth cascade level. We classify all the possible detection windows on the negative training images with the cascade of the previous (k \u2212 1) LogitBoost classifiers. The samples which are misclassified form the possible negative set (samples classified as positive). Since the cardinality of the possible negative set is very large, we sample N n = 10000 examples from this set as the negative examples at cascade level k. At every cascade level, we consider all the positive training images as the positive training set. There is a single human at each of the positive images, so N p = N pi .", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 3 .3Figure 3. Cascade of LogitBoost classifiers. The kth LogitBoost classifier selects normalized covariance descriptors of subwindows r k,i .", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 4 .4Figure 4. Comparison with methods of Dalal & Triggs [4] and Zhu et.al. [24]. The curves for other approaches are generated from the respective papers. See text for details.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 5 .5Figure 5. Detection rates of different approaches for our method. See text for details.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 6 .6Figure 6. The number of weak classifiers at each cascade level and the accumulated rejection rate over the cascade levels. See text for details.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "W \u00d7 H \u00d7 d dimensional feature image extracted from I F (x, y) = \u03a6(I, x, y)(1)", "formula_coordinates": [4.0, 50.11, 263.4, 236.25, 44.08]}, {"formula_id": "formula_1", "formula_text": "C R = 1 S \u2212 1 S i=1 (z i \u2212 \u00b5)(z i \u2212 \u00b5) T (2", "formula_coordinates": [4.0, 94.52, 389.19, 187.97, 30.53]}, {"formula_id": "formula_2", "formula_text": ")", "formula_coordinates": [4.0, 282.49, 399.81, 3.87, 8.91]}, {"formula_id": "formula_3", "formula_text": "x y |I x | |I y | I 2 x + I 2 y |I xx | |I yy | arctan |I x | |I y | T(3)", "formula_coordinates": [4.0, 60.35, 474.89, 226.01, 26.64]}, {"formula_id": "formula_4", "formula_text": "y, z > X = tr X \u2212 1 2 yX \u22121 zX \u2212 1 2 .(4)", "formula_coordinates": [5.0, 102.19, 469.42, 184.17, 13.76]}, {"formula_id": "formula_5", "formula_text": "exp X (y) = X 1 2 exp X \u2212 1 2 yX \u2212 1 2 X 1 2(5)", "formula_coordinates": [5.0, 89.03, 515.1, 197.33, 14.71]}, {"formula_id": "formula_6", "formula_text": "log X (Y) = X 1 2 log X \u2212 1 2 YX \u2212 1 2 X 1 2 . (6", "formula_coordinates": [5.0, 87.32, 593.99, 195.17, 14.7]}, {"formula_id": "formula_7", "formula_text": ")", "formula_coordinates": [5.0, 282.49, 597.78, 3.87, 8.91]}, {"formula_id": "formula_8", "formula_text": "X \u2208 Sym + d .", "formula_coordinates": [5.0, 50.11, 653.69, 53.35, 13.92]}, {"formula_id": "formula_9", "formula_text": "exp(\u03a3) = \u221e k=0 \u03a3 k k! = Uexp(D)U T (7)", "formula_coordinates": [5.0, 354.39, 101.54, 190.72, 30.76]}, {"formula_id": "formula_10", "formula_text": "log(\u03a3) = \u221e k=1 (\u22121) k\u22121 k (\u03a3 \u2212 I) k = Ulog(D)U T . (8)", "formula_coordinates": [5.0, 319.63, 162.47, 225.48, 30.76]}, {"formula_id": "formula_11", "formula_text": "d 2 (X, Y) = < log X (Y), log X (Y) > X = tr log 2 (X \u2212 1 2 YX \u2212 1 2 ) .(9)", "formula_coordinates": [5.0, 338.88, 268.06, 206.23, 29.77]}, {"formula_id": "formula_12", "formula_text": "\u00b5 = arg min Y\u2208M N i=1 d 2 (X i , Y)(11)", "formula_coordinates": [5.0, 357.94, 528.53, 187.17, 30.53]}, {"formula_id": "formula_13", "formula_text": "\u00b5 t+1 = exp \u00b5 t 1 N N i=1 log \u00b5 t (X i )(12)", "formula_coordinates": [5.0, 344.06, 606.41, 201.06, 30.53]}, {"formula_id": "formula_14", "formula_text": "1 P N i=1 wi N i=1 w i log \u00b5 t (X i ).", "formula_coordinates": [5.0, 334.13, 695.95, 115.15, 20.09]}, {"formula_id": "formula_15", "formula_text": "zi = y i \u2212p(X i ) p(X i )(1\u2212p(X i )) wi = p(Xi)(1 \u2212 p(Xi)). -Compute", "formula_coordinates": [6.0, 347.1, 142.71, 103.03, 38.59]}, {"formula_id": "formula_16", "formula_text": "-Update F (X) \u2190 F (X) + 1 2 f l (X)", "formula_coordinates": [6.0, 347.1, 243.56, 137.85, 12.69]}, {"formula_id": "formula_17", "formula_text": "[F (X)] = sign [ P L l=1 f l (X)]", "formula_coordinates": [6.0, 334.65, 272.03, 105.26, 25.3]}, {"formula_id": "formula_18", "formula_text": "p(x) = e F (x) e F (x) + e \u2212F (x) F (x) = 1 2 L l=1 f l (x). (13", "formula_coordinates": [6.0, 317.81, 409.26, 223.16, 30.76]}, {"formula_id": "formula_19", "formula_text": ")", "formula_coordinates": [6.0, 540.96, 419.88, 4.15, 8.91]}, {"formula_id": "formula_20", "formula_text": "\u2212 N i=1 [y i log(p(x i )) + (1 \u2212 y i )log(1 \u2212 p(x i ))](14)", "formula_coordinates": [6.0, 324.89, 488.5, 220.22, 30.53]}, {"formula_id": "formula_21", "formula_text": "f l (X) = g l (vec \u00b5 l (log \u00b5 l (X)))(15)", "formula_coordinates": [6.0, 364.52, 668.97, 180.59, 12.4]}, {"formula_id": "formula_22", "formula_text": "k if p k (X) > p k (X p ) \u2212 th b > p k (X n ) or equivalently F k (X) > F k (X n ).", "formula_coordinates": [7.0, 308.86, 240.81, 236.25, 22.34]}], "doi": ""}