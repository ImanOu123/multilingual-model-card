{"title": "Nonparametric Scene Parsing: Label Transfer via Dense Scene Alignment", "authors": "Ce Liu; Jenny Yuen; Antonio Torralba", "pub_date": "", "abstract": "In this paper we propose a novel nonparametric approach for object recognition and scene parsing using dense scene alignment. Given an input image, we retrieve its best matches from a large database with annotated images using our modified, coarse-to-fine SIFT flow algorithm that aligns the structures within two images. Based on the dense scene correspondence obtained from the SIFT flow, our system warps the existing annotations, and integrates multiple cues in a Markov random field framework to segment and recognize the query image. Promising experimental results have been achieved by our nonparametric scene parsing system on a challenging database. Compared to existing object recognition approaches that require training for each object category, our system is easy to implement, has few parameters, and embeds contextual information naturally in the retrieval/alignment procedure.", "sections": [{"heading": "Introduction", "text": "Scene parsing, or recognizing and segmenting objects in an image, is one of the core problems of computer vision. Traditional approaches to object recognition begin by specifying an object model, such as template matching [28,5], constellations [9,7], bags of features [24,14,10,25], or shape models [2,3,6], etc. These approaches typically work with a fixed-number of object categories and require training generative or discriminative models for each category given training data. In the parsing stage, these systems try to align the learned models to the input image and associate object category labels with pixels, windows, edges or other image representations. Recently, context information has also been carefully modeled to capture the relationship between objects at the semantic level [11,13]. Encouraging progress has been made by these models on a variety of object recognition and scene parsing tasks.\nHowever, these learning-based methods do not, in general, scale well with the number of object categories. For example, to expand an existing system to include more object categories, we need to train new models for these categories and, typically adjust system parameters. Training can be a tedious job if we want to include thousands of object categories for a scene parsing system. In addition, the complexity of contextual relationships amongst objects also increases rapidly as the quantity of object categories ", "publication_ref": ["b27", "b4", "b8", "b6", "b23", "b13", "b9", "b24", "b1", "b2", "b5", "b10", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "expands.", "text": "Recently, the emergence of large databases of images has opened the door to a new family of methods in computer vision. Large database-driven approaches have shown the potential for nonparametric methods in several applications. Instead of training sophisticated parametric models, these methods try to reduce the inference problem for an unknown image to that of matching to an existing set of annotated images. In [21], the authors estimate the pose of a human relying on 0.5 million training examples. In [12], the proposed algorithm can fill holes on an input image by introducing elements that are likely to be semantically correct through searching a large image database. In [19], a system is designed to infer the possible object categories that may appear in an image by retrieving similar images in a large database [20]. Moreover, the authors in [27] showed that with a database of 80 million images, even simple SSD match can give semantically meaningful parsing for 32 \u00d7 32 images.\nMotivated by the recent advances in large databasedriven approaches, we designed a nonparametric scene parsing system to transfer the labels from existing samples to annotate an image through dense scene alignment, as illustrated in Figure 1. For a query image (a), our system first retrieves the top matches in the LabelMe database [20] using a combination of GIST matching [18] and SIFT flow Figure 2. An illustration of our coarse-to-fine pyramid SIFT flow matching. The green square is the searching window for p k at each pyramid level k. For simplicity only one image is shown here, where p k is on image s1, and c k and w(p k ) are on image s2. [16]. Since these top matches are labeled, we transfer the annotation (c) of the top matches to the query image and obtain the scene parsing result in (d). For comparison, the ground-truth user annotation of the query is displayed in (e). Our system is able to generate promising scene parsing results if images from the same scene category are retrieved in the annotated database.\nHowever, it is nontrivial to build an efficient and reliable scene parsing system using dense scene alignment. The SIFT flow algorithm proposed in [16] does not scale well with image dimensions. Therefore, we propose a flexible, coarse-to-fine matching scheme to find dense correspondences between two images. To account for the multiple annotation suggestions from the top matches, a Markov random field model is used to merge multiple cues (e.g. likelihood, prior and spatial smoothness) into reliable annotation. Promising experimental results are achieved on images from the LabelMe database [20].\nOur goal is to explore the performance of scene parsing through the transfer of labels from existing annotated images, rather than building a comprehensive object recognition system. We show, however, that the performance of our system outperforms existing approaches [5,23] on our dataset.", "publication_ref": ["b20", "b11", "b18", "b19", "b26", "b19", "b17", "b15", "b15", "b19", "b4", "b22"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "SIFT Flow for Dense Scene Alignment", "text": "As our goal is to transfer the labels of existing samples to parse an input image, it is essential to find the dense correspondence for images across scenes. Liu et al. [16] have demonstrated that SIFT flow is able to establish semantically meaningful correspondences between two images through matching local SIFT structures. In this section we extend the SIFT flow algorithm [16] to be more robust to matching outliers by modifying the objective function for matching, and more efficient for aligning large-scale images using a coarse-to-fine approach.", "publication_ref": ["b15", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Modified matching objective", "text": "Let p = (x, y) contain the spatial coordinate of a pixel, and w(p) = (u(p), v(p)) be the flow vector at p. Denote s 1 and s 2 as the per-pixel SIFT feature [17] for two images 1 , and \u03b5 contains all the spatial neighborhood (a four-neighbor system is used). Our modified energy function is defined as:\nE(w) = p min s 1 (p) \u2212 s 2 (p + w(p)) 1 , t + p \u03b7 |u(p)| + |v(p)| + (p,q)\u2208\u03b5 min \u03b1|u(p) \u2212 u(q)|, d + (p,q)\u2208\u03b5 min \u03b1|v(p) \u2212 v(q)|, d .(1)\nIn this objective function, truncated L1 norms are used in both the data and the smoothness terms to account for matching outliers and flow discontinuities, with t and d as the threshold, respectively. An L1 norm is also imposed on the magnitude of the flow vector as a bias towards smaller displacement when no other information is available. Notice that in [16] only an L1 norm is used for the data term and the small displacement biased is formulated as an L2 norm. This energy function is optimized by running sequential Belief Propagation (BP-S) [26] on a dual plane setup [22].", "publication_ref": ["b16", "b0", "b15", "b25", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Coarse-to-fine matching scheme", "text": "While SIFT flow has demonstrated the potential for aligning images across scenes [16], its performance scales poorly with respect to the image size. In SIFT flow, a pixel in one image can literally match to any other pixel in another image. Suppose the image has h 2 pixels, then the time and space complexity of the BP algorithm to estimate the SIFT flow is O(h 4 ). As reported in [16], the computation time for 145 \u00d7 105 images with an 80 \u00d7 80 searching neighborhood is 50 seconds. The original implementation of SIFT flow would require more than two hours to process a pair of 256 \u00d7 256 images in our database with a memory usage of 16GB to store the data term.\nTo address the performance drawback, we designed a coarse-to-fine SIFT flow matching scheme that significantly improves the performance. The procedure is illustrated in Figure 2. For simplicity, we use s to represent both s 1 and s 2 . A SIFT pyramid {s (k) } is established, where s (1) = s and s (k+1) is smoothed and downsampled from s (k) . At each pyramid level k, let p k be the coordinate of the pixel to match, c k be the offset or centroid of the searching window, and w(p k ) be the best match from BP. At the top pyramid level s (3) , the searching window is centered at p 3 (c 3 = p 3 ) with size m\u00d7m, where m is the width (height) of s (3) . The complexity of BP at this level is O(m 4 ). After BP\n0 1 2 -1 -2 5 -2 (a) (b) (c)\nFigure 3. We generalized distance transform function for truncated L1 norm [8] to pass message between neighboring nodes that have different offsets (centroids) of the searching window.\nconverges, the system propagates the optimized flow vector w(p 3 ) to the next (finer) level to be c 2 where the searching window of p 2 is centered. The size of this searching window is fixed to be n \u00d7 n with n = 11. This procedure iterates from s (3) to s (1) until the flow vector w(p 1 ) is estimated. Since n is fixed at all levels except for the top, the complexity of this coarse-to-fine algorithm is O(h 2 log h), a significant speed up compared to O(h 4 ).\nWhen the matching is propagated from an coarser level to the finer level, the searching windows for two neighboring pixels may have different offsets (centroids). We modify the the distance transform function developed for truncated L1 norm [8] to cope with this situation, with the idea illustrated in Figure 3. To compute the message passing from pixel p to its neighbor q, we first gather all other messages and data term, and apply the routine in [8] to compute the message from p to q assuming that q and p have the same offset and range. The function is then extended to be outside the range by increasing \u03b1 per step, as shown in Figure 3 (a). We take the function in the range that q is relative to p as the message. For example, if the offset of the searching window for p is 0, and the offset for q is 5, then the message from p to q is plotted in Figure 3 (c). If the offset of the searching window for q is \u22122 otherwise, the message is shown in Figure 3 (b).\nUsing the proposed coarse-to-fine matching scheme and modified distance transform function, the matching between two 256 \u00d7 256 images takes 31 seconds on a workstation with two quad-core 2.67 GHz Intel Xeon CPUs and 32 GB memory, in a C++ implementation. Further speedup (up to 50x) can be achieved through GPU implementation [4] of the BP-S algorithm since this algorithm can be parallelized. We leave this as future work.\nA natural question is whether the coarse-to-fine matching scheme can achieve the same minimum energy as the ordinary matching scheme (only one level without coarseto-fine) [16]. An experiment is conducted to compare these two algorithms (refer to Section 4.1 for more details). In general, we found that the coarse-to-fine matching outperforms the ordinary matching in terms of obtaining lower energy. This is consistent with what has been discovered in . For a query image, we first find a K-nearest neighbor set in the database using GIST matching [18]. The nearest neighbors are re-ranked using SIFT flow matching scores, and form a top M -voting candidate set. The annotations are transferred from the voting candidates to the query image.\nthe optical flow community: coarse-to-fine search not only speeds up computation but also leads to lower energy. This can be caused by the inherent self-similarity nature of SIFT features across scales: the correspondence at a coarser level is a good prediction for the correspondence at a finer level.", "publication_ref": ["b15", "b15", "b2", "b2", "b7", "b2", "b0", "b7", "b7", "b3", "b15", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Scene Parsing through Label Transfer", "text": "Now that we have a large database of annotated images and a technique of establishing dense correspondences across scenes, we can transfer the existing annotations to a query image through dense scene alignment. For a given query image, we retrieve a set of K-nearest neighbors in our database using GIST matching [18]. We then compute the SIFT flow from the query to each nearest neighbor, and use the achieved minimum energy (defined in Eqn. 1) to rerank the K-nearest neighbors. We further select the top M re-ranked retrievals to create our voting candidate set. This voting set will be used to transfer its contained annotations into the query image. This procedure is illustrated in Figure 4.\nUnder this setup, scene parsing can be formulated as the following label transfer problem. For a query image I with its corresponding SIFT image s, we have a set of voting candidates {s i , c i , w i } i=1:M , where s i , c i and w i are the SIFT image, annotation, and SIFT flow field (from s to s i ) of the ith voting candidate. c i is an integer image where c i (p) \u2208 {1, \u2022 \u2022 \u2022 , L} is the index of object category for pixel p. We want to obtain the annotation c for the query image by transferring c i to the query image according to the dense correspondence w i .\nWe build a probabilistic Markov random field model to integrate multiple labels, prior information of object category, and spatial smoothness of the annotation to parse image I. Similar to that of [23], the posterior probability is defined as:\n\u2212 log P c|I, s, {s i , c i , w i } = p \u03c8 c(p); s, {s \u2032 i } + \u03b1 p \u03bb c(p) +\u03b2 {p,q}\u2208\u03b5 \u03c6 c(p), c(q); I +log Z, (2\n)\nwhere Z is the normalization constant of the probability. This posterior contains three components, i.e. likelihood, prior and spatial smoothness.\nThe likelihood term is defined as\n\u03c8 c(p) = l = min i\u2208\u2126 p,l s(p)\u2212s i (p+w(p)) , \u2126 p,l = \u2205 \u03c4, \u2126 p,l = \u2205 (3)\nwhere \u2126 p,l = {i; c i (p + w(p)) = l} is the index set of the voting candidates whose label is l after being warped to pixel p. \u03c4 is set to be the value of the maximum difference of SIFT feature: \u03c4 = max s1,s2,p s 1 (p) \u2212 s 2 (p) .\nThe prior term is \u03bb(c(p) = l) indicates the prior probability that object category l appears at pixel p. This is obtained from counting the occurrence of each object category at each location in the training set.\n\u03bb c(p) = l = \u2212 log hist l (p)(4)\nwhere hist l (p) is the spatial histogram of object category l.\nThe smoothness term is defined to bias the neighboring pixels into having the same label if no other information is available, and the probability depends on the edge of the image: the stronger luminance edge, the more likely that the neighboring pixels may have different labels.\n\u03c6 c(p), c(q) = \u03b4[c(p) = c(q)] \u01eb+e \u2212\u03b3 I(p)\u2212I(q) 2 \u01eb+1 (5) where \u03b3 = (2 < I(p) \u2212 I(q) 2 >) \u22121 [23].\nNotice that the energy function is controlled by four parameters, K and M that decide the mode of the model, and \u03b1 and \u03b2 that control the influence of spatial prior and smoothness. Once the parameters are fixed, we again use BP-S algorithm to minimize the energy. The algorithm converges in two seconds on a workstation with two quad-core 2.67 GHz Intel Xeon CPUs.\nA significant difference between our model and that in [23] is that we have fewer parameters because of the nonparametric nature of our approach, whereas classifiers where trained in [23]. In addition, color information is not included in our model at the present as the color distribution for each object category is diverse in our database.", "publication_ref": ["b17", "b22", "b22", "b22"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Experiments", "text": "We used a subset of the LabelMe database [20] to test our system. This dataset contains 2688 fully annotated images, most of which are outdoor scenes including street, beach, mountains, fields and buildings. From these images we randomly selected 2488 for training and 200 for testing. We chose the top 33 object categories with the most labeled pixels. The pixels that are not labeled, or labeled as other object categories, are treated as the 34th category: \"unlabeled\". The per pixel frequency count of these object categories in the training set is shown at the top of Figure 5. The color of each bar is the average RGB value of the corresponding object category from the training data with  saturation and brightness boosted for visualization. The top 10 object categories are sky, building, mountain, tree, unlabeled, road, sea, field, grass, and river. The spatial priors of these object categories are displayed at the bottom of Figure 5. White means zero probability and saturated color means the highest probability. We observe that sky occupies the upper part of image grid and field occupies the lower part.\nNotice that there are only limited numbers of samples for the objects such as sun, cow, bird, and moon.", "publication_ref": ["b19"], "figure_ref": ["fig_2", "fig_2"], "table_ref": []}, {"heading": "Evaluation of the dense scene alignment", "text": "We first evaluate our coarse-to-fine SIFT flow matching for dense scene alignment. We randomly selected 10 images from the test set as the query, and check the minimum energy obtained between the query and the best SIFT flow match using coarse-to-fine scheme and ordinary scheme (non coarse-to-fine), respectively. For these 256 \u00d7 256 images, the average running time coarse-to-fine SIFT flow is 31 seconds, whereas it takes 127 minutes in average for the ordinary matching. The coarse-to-fine scheme not only runs  . Coarse-to-fine SIFT flow not only runs significantly faster, but also achieves lower energies most of the time. In this experiment, we randomly selected 10 samples in the test set and computed the lowest energy of the best match with the nearest neighbors. We tested both the coarse-to-fine algorithm proposed in this paper and the ordinary matching scheme in [16]. Except for sample #8, coarse-to-fine matching achieves lower energy than the ordinary matching algorithm.\nsignificantly faster, but also achieves lower energies most of the time compared to the ordinary matching algorithm [16] as shown in Figure 7.\nBefore evaluating the performance our system on object recognition, we want to evaluate how well SIFT flow performs in matching structures across different images and how it compares with human selected matches. Traditional optical flow is a well-defined problem and it is straightforward for humans to annotate motion for evaluation [15]. In the case of SIFT flow, however, there may not be obvious or unique best pixel-to-pixel matching as the two images may contain different objects, or the same object categories with very different instances.\nTo evaluate the matching obtained by SIFT flow, we performed a user study where we showed 11 users image pairs with preselected sparse points in the first image and asked the users to select the corresponding points in the second image. As shown on the right of Figure 8, user annotation can be ambiguous. Therefore, we use the following metric to evaluate SIFT flow: for a pixel p, we have several human annotations z i as its flow vector, and w(p) as the estimated SIFT flow vector. We compute Pr \u2203z i , z i \u2212 w(p) \u2264 r|r , namely the probability of one human annotated flow is within distance r to SIFT flow w(p). This function (or r is plotted on the left of Figure 8 (red curve). For comparison, we plot the same probability function (blue curve) for minimum L1-norm SIFT matching, i.e. SIFT flow matching without spatial terms. Clearly SIFT flow matches better to human annotation than minimum L1-norm SIFT matching.", "publication_ref": ["b15", "b15", "b14"], "figure_ref": ["fig_4", "fig_5", "fig_5"], "table_ref": []}, {"heading": "Results of scene parsing", "text": "Our scene parsing system is illustrated in Figure 6. The system retrieves a K-nearest neighbor set for the query image (a), and further selects M voting candidates with the minimum SIFT matching score. For the purpose of illustration we set M = 3 here. The RGB image, SIFT image, and annotation of the voting candidates are shown in (c) to (e), respectively. The SIFT flow field is visualized in (f) using   the same visualization scheme as in [16]. After we warp the voting candidates into the query with respect to the flow field, the warped RGB (g) and SIFT image (h) are very close to the query (a) and (b). Combining the warped annotations in (i), the system outputs the parsing of the query in (j), which is close to the ground-truth annotation in (k).\nSome label transferring results are displayed in Figure 12. The input image from the test set is displayed in column (a). We show the best match, its corresponding annotation, and the warped best match in (b), (c) and (d), respectively, to hint the annotation for the query, even though our system takes the top M matches as voting candidates. Again, the warped image (d) looks similar to the input, indicating that SIFT flow successfully matches image structures. The scene parsing results output from our system are listed in column (e) with parameter setting K = 50, M = 5, \u03b1 = 0.1, \u03b2 = 70. The ground-truth user annotation is listed in (f). Notice that the gray pixels in (f) are \"unlabeled\", but our system does not generate \"unlabeled\" output. For sample 1, 5, 6, 8 and 9, our system generates reasonable predictions for the pixels annotated as \"unlabeled\". The pixelwise recognition rate of our system is 74.75% by excluding the \"unlabeled\" class [23]. A failure example for our system is shown in Figure 13 when the system fails to retrieve images with similar object categories to the query.\nFor comparison, we downloaded and ran the code from [23] using the same training and test data with the conditional random field turned off. The overall pixel-wise recognition rate of their system on our data set is 51.67%, and the per-class rates are displayed in Figure 9 (c). For fairness we also turned off the Markov random field model in our framework by setting \u03b1 = \u03b2 = 0, and plotted the corresponding results in Figure 9 (b). Clearly, our system outperforms [23] in terms of both overall and per-class recognition rate. Overall, our system is able to predict the right object categories in the input image with a segmentation fit to image boundary, even though the best match may look different from the input, e.g. 2, 11, 12 and 17. If we divide the object categories into stuff (e.g. sky, mountains, tree, sea and field) and things (e.g. cars, sign, boat and bus) [1,13], our system generates much better results for stuff than for things. The recognition rate for the top 7 object categories (all are \"stuff\") is 82.72%. This is because in our current system we only allow one labeling for each pixel, and smaller objects tend to be overwhelmed by the labeling of larger objects. We plan to build a recursive system in our future work to further retrieve things based on the inferred stuff. We investigate the performance of our system by varying the parameters K, M , \u03b1 and \u03b2. First, we fix \u03b1 = 0.1, \u03b2 = 70 and plot the recognition rate as a function of K in Figure 10 (a) with different M . Overall, the recognition rate increases as more nearest neighbors are retrieved (K \u2191) and more voting candidates are used (M \u2191) since, obviously, multiple candidates are needed to transfer labels to the query. However, the recognition drops as K and M continue to increase as more candidates may include noise to label transfer. The maximum performance is obtained when K = 50 and M = 5. Second, we fix M = 5, and plot the recognition rate as a function of K by turning on prior and spatial terms (\u03b1 = 0.1, \u03b2 = 70) and turning them off (\u03b1 = \u03b2 = 0) in Figure 10 (b). Prior and spatial smoothness increase the performance of our system by about 7 percentage.\nLastly, we compared the performance of our system with a classifier-based system [5]. We downloaded their code and trained a classifier for each object category using the same training data. We converted our system into a binary object detector for each class by only using the per-class likelihood term. The per-class ROC curves of our system Figure 11. The ROC curve of each individual pixel-wise binary classifier. Red curve: our system after being converted to binary classifiers; blue curve: the system in [5]. We used convex hull to make the ROC curves strictly concave. The number (n, m) underneath the name of each plot is the quantity of the object instances in the test and training set, respectively. For example, (170, 2124) under \"sky\" means that there are 170 test images containing sky, and 2124 training images containing sky. Our system obtains reasonable performance for objects with sufficient samples in both training and test sets, e.g. sky, building, mountain and tree. We observe truncation in the ROC curves where there are not enough test samples, e.g. field, sea, river, grass, plant, car and sand. The performance is poor for objects without enough training samples, e.g. crosswalk, sign, boat, pole, sun and bird. The ROC does not exist for objects without any test samples, e.g. desert, cow and moon. In comparison, our system outperforms or equals [5] for all object categories except for grass, plant, boat, person and bus. The performance of [5] on our database is low because the objects have drastically different poses and appearances. The dark gray pixels in (f) are \"unlabeled\". Notice how our system generates a reasonable parsing even for these \"unlabeled\" pixels. Our system fails when no good matches can be retrieved in the database. Since the best matches do not contain river, the input image is mistakenly parsed as a scene of grass, tree and mountain in (e). The ground-truth annotation is in (f).\n(red) and theirs (blue) are plotted in Figure 11. Except for five object categories, grass, plant, boat, person and bus, our system outperforms or equals theirs.", "publication_ref": ["b15", "b22", "b22", "b22", "b0", "b12", "b4", "b4", "b4", "b4"], "figure_ref": ["fig_3", "fig_0", "fig_0", "fig_7", "fig_7", "fig_0", "fig_0", "fig_0", "fig_0"], "table_ref": []}, {"heading": "Conclusion", "text": "We presented a novel, nonparametric scene parsing system to transfer the annotations from a large database to an input image using dense scene alignment. A coarse-to-fine SIFT flow matching scheme is proposed to reliably and efficiently establish dense correspondences between images across scenes. Using the dense scene correspondences, we warp the pixel labels of the existing samples to the query. Furthermore, we integrate multiple cues to segment and recognize the query image into the object categories in the database. Promising results have been achieved by our scene alignment and parsing system on a challenging database. Compared to existing approaches that require training for each object category, our nonparametric scene parsing system is easy to implement, has only a few parameters, and embeds contextual information naturally in the retrieval/alignment procedure.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "Funding for this research was provided by the Royal Dutch/Shell Group, NGA NEGI-1582-04-0004, MURI Grant N00014-06-1-0734, NSF Career award (IIS 0747120), and a National Defense Science and Engineering Graduate Fellowship.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "On seeing stuff: the perception of materials by humans and machines", "journal": "", "year": "2001", "authors": "E H Adelson"}, {"ref_id": "b1", "title": "Shape context: A new descriptor for shape matching and object recognition", "journal": "", "year": "2000", "authors": "S Belongie; J Malik; J Puzicha"}, {"ref_id": "b2", "title": "Shape matching and object recognition using low distortion correspondence", "journal": "", "year": "2005", "authors": "A Berg; T Berg; J Malik"}, {"ref_id": "b3", "title": "Real-time connectivity constrained depth map computation using programmable graphics hardware", "journal": "", "year": "2005", "authors": "N Cornelis; L V Gool"}, {"ref_id": "b4", "title": "Histograms of oriented gradients for human detection", "journal": "", "year": "2005", "authors": "N Dalal; B Triggs"}, {"ref_id": "b5", "title": "Pictorial structures for object recognition", "journal": "IJCV", "year": "2005", "authors": "P Felzenszwalb; D Huttenlocher"}, {"ref_id": "b6", "title": "A discriminatively trained, multiscale, deformable part model", "journal": "", "year": "2008", "authors": "P Felzenszwalb; D Mcallester; D Ramanan"}, {"ref_id": "b7", "title": "Efficient belief propagation for early vision", "journal": "IJCV", "year": "2006", "authors": "P F Felzenszwalb; D P Huttenlocher"}, {"ref_id": "b8", "title": "Object class recognition by unsupervised scale-invariant learning", "journal": "", "year": "2003", "authors": "R Fergus; P Perona; A Zisserman"}, {"ref_id": "b9", "title": "Pyramid match kernels: Discriminative classification with sets of image features", "journal": "", "year": "2005", "authors": "K Grauman; T Darrell"}, {"ref_id": "b10", "title": "Beyond nouns: Exploiting prepositions and comparative adjectives for learning visual classifiers", "journal": "", "year": "2008", "authors": "A Gupta; L S Davis"}, {"ref_id": "b11", "title": "Scene completion using millions of photographs", "journal": "ACM SIGGRAPH", "year": "2007", "authors": "J Hays; A A Efros"}, {"ref_id": "b12", "title": "Learning spatial context: Using stuff to find things", "journal": "", "year": "2008", "authors": "G Heitz; D Koller"}, {"ref_id": "b13", "title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "journal": "", "year": "2006", "authors": "S Lazebnik; C Schmid; J Ponce"}, {"ref_id": "b14", "title": "Humanassisted motion annotation", "journal": "", "year": "2008", "authors": "C Liu; W T Freeman; E H Adelson; Y Weiss"}, {"ref_id": "b15", "title": "SIFT flow: dense correspondence across different scenes", "journal": "", "year": "2008", "authors": "C Liu; J Yuen; A Torralba; J Sivic; W T Freeman"}, {"ref_id": "b16", "title": "Object recognition from local scale-invariant features", "journal": "", "year": "1999", "authors": "D G Lowe"}, {"ref_id": "b17", "title": "Modeling the shape of the scene: a holistic representation of the spatial envelope", "journal": "IJCV", "year": "2001", "authors": "A Oliva; A Torralba"}, {"ref_id": "b18", "title": "Freeman. Object recognition by scene alignment", "journal": "", "year": "2007", "authors": "B C Russell; A Torralba; C Liu; R Fergus; W "}, {"ref_id": "b19", "title": "LabelMe: a database and web-based tool for image annotation", "journal": "IJCV", "year": "2008", "authors": "B C Russell; A Torralba; K P Murphy; W T Freeman"}, {"ref_id": "b20", "title": "Fast pose estimation with parameter sensitive hashing", "journal": "", "year": "2003", "authors": "G Shakhnarovich; P Viola; T Darrell"}, {"ref_id": "b21", "title": "Efficient MRF deformation model for non-rigid image matching", "journal": "", "year": "2007", "authors": "A Shekhovtsov; I Kovtun; V Hlavac"}, {"ref_id": "b22", "title": "Textonboost: Joint appearance, shape and context modeling for multi-class object recognition and segmentation", "journal": "", "year": "2006", "authors": "J Shotton; J Winn; C Rother; A Criminisi"}, {"ref_id": "b23", "title": "Video Google: a text retrieval approach to object matching in videos", "journal": "", "year": "2003", "authors": "J Sivic; A Zisserman"}, {"ref_id": "b24", "title": "Describing visual scenes using transformed dirichlet processes", "journal": "", "year": "2005", "authors": "E Sudderth; A Torralba; W T Freeman; W Willsky"}, {"ref_id": "b25", "title": "A comparative study of energy minimization methods for markov random fields", "journal": "TPAMI", "year": "2008", "authors": "R Szeliski; R Zabih; D Scharstein; O Veksler; V Kolmogorov; A Agarwala; M Tappen; C Rother"}, {"ref_id": "b26", "title": "80 million tiny images: a large dataset for non-parametric object and scene recognition", "journal": "TPAMI", "year": "2008", "authors": "A Torralba; R Fergus; W T Freeman"}, {"ref_id": "b27", "title": "Rapid object detection using a boosted cascade of simple features", "journal": "", "year": "2001", "authors": "P Viola; M Jones"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. For a query image (a), our system finds the top matches (b) (three are shown here) using a modified, corse-to-fine SIFT flow matching algorithm. The annotations of the top matches (c) are transferred and integrated to parse the input image as shown in (d). For comparison, the ground-truth user annotation of (a) is shown in (e).", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 44Figure 4. For a query image, we first find a K-nearest neighbor set in the database using GIST matching[18]. The nearest neighbors are re-ranked using SIFT flow matching scores, and form a top M -voting candidate set. The annotations are transferred from the voting candidates to the query image.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 5 .5Figure 5. Above: the per-pixel frequency counts of the object categories in our dataset (sorted in descending order). The color of each bar is the average RGB value of each object category from the training data with saturation and brightness boosted for visualization. Bottom: the spatial priors of the object categories in the database. White means zero and the saturated color means high probability.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 6 .6Figure 6. System overview. Our algorithm computes the SIFT image (b) of an query image (a), and uses GIST[18] to find its K nearest neighbors in our database. We apply coarse-to-fine SIFT flow to align the query image to the nearest neighbors, and obtain top M as voting candidates (M = 3 here). (c) to (e): the RGB image, SIFT image and user annotation of the voting candidates. (f): the inferred SIFT flow. From (g) to (i) are the warped version of (c) to (e) with respect to the SIFT flow in (f). Notice the similarity between (a) and (g), (b) and (h). Our system combines the voting from multiple candidates and generates scene parsing in (j) by optimizing the posterior. (k): the ground-truth annotation of (a).", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 77Figure7. Coarse-to-fine SIFT flow not only runs significantly faster, but also achieves lower energies most of the time. In this experiment, we randomly selected 10 samples in the test set and computed the lowest energy of the best match with the nearest neighbors. We tested both the coarse-to-fine algorithm proposed in this paper and the ordinary matching scheme in[16]. Except for sample #8, coarse-to-fine matching achieves lower energy than the ordinary matching algorithm.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 8 .8Figure 8. The evaluation of SIFT flow using human annotation. Left: the probability of one human annotated flow lies within r distance to the SIFT flow as a function of r (red curve). For comparison, we plot the same probability for direct minimum L1-norm matching (blue curve). Clearly SIFT flow matches human perception better. Right: the histogram of the standard deviation of human annotation. Human perception of scene correspondence varies from subject to subject.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 9 .9Figure 9. The per-class recognition rate of our system and the one in [23]. (a) Our system with the the parameters optimized for pixel-wise recognition rate. (b) Our system with \u03b1 = \u03b2 = 0, namely, with the Markov random field model turned off. (c) The performance of the system in [23] also with the conditional random field turned off, trained and tested on the same data sets as (a) and (b).", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 10 .10Figure 10. (a): Recognition rate as a function of the number of nearest neighbors K and the number of voting candidates M . (b): recognition rate as a function of the number of nearest neighbors K. Clearly, prior and spatial smoothness help improve the recognition rate.", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 12 .12Figure 12. Some scene parsing results output from our system. (a): query image; (b): the best match from nearest neighbors; (c): the annotation of the best match; (d): the warped version of (b) according to the SIFT flow field; (e): the inferred per-pixel parsing after combining multiple voting candidates; (f): the ground truth annotation of (a).The dark gray pixels in (f) are \"unlabeled\". Notice how our system generates a reasonable parsing even for these \"unlabeled\" pixels.", "figure_data": ""}, {"figure_label": "13", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 13 .13Figure13. Our system fails when no good matches can be retrieved in the database. Since the best matches do not contain river, the input image is mistakenly parsed as a scene of grass, tree and mountain in (e). The ground-truth annotation is in (f).", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "E(w) = p min s 1 (p) \u2212 s 2 (p + w(p)) 1 , t + p \u03b7 |u(p)| + |v(p)| + (p,q)\u2208\u03b5 min \u03b1|u(p) \u2212 u(q)|, d + (p,q)\u2208\u03b5 min \u03b1|v(p) \u2212 v(q)|, d .(1)", "formula_coordinates": [2.0, 320.04, 121.55, 225.2, 110.83]}, {"formula_id": "formula_1", "formula_text": "0 1 2 -1 -2 5 -2 (a) (b) (c)", "formula_coordinates": [3.0, 128.4, 137.69, 97.99, 57.89]}, {"formula_id": "formula_2", "formula_text": "\u2212 log P c|I, s, {s i , c i , w i } = p \u03c8 c(p); s, {s \u2032 i } + \u03b1 p \u03bb c(p) +\u03b2 {p,q}\u2208\u03b5 \u03c6 c(p), c(q); I +log Z, (2", "formula_coordinates": [3.0, 310.32, 630.17, 231.01, 51.73]}, {"formula_id": "formula_3", "formula_text": ")", "formula_coordinates": [3.0, 541.33, 661.0, 3.91, 8.97]}, {"formula_id": "formula_4", "formula_text": "\u03c8 c(p) = l = min i\u2208\u2126 p,l s(p)\u2212s i (p+w(p)) , \u2126 p,l = \u2205 \u03c4, \u2126 p,l = \u2205 (3)", "formula_coordinates": [4.0, 52.56, 105.71, 233.96, 39.01]}, {"formula_id": "formula_5", "formula_text": "\u03bb c(p) = l = \u2212 log hist l (p)(4)", "formula_coordinates": [4.0, 109.68, 247.07, 176.84, 10.62]}, {"formula_id": "formula_6", "formula_text": "\u03c6 c(p), c(q) = \u03b4[c(p) = c(q)] \u01eb+e \u2212\u03b3 I(p)\u2212I(q) 2 \u01eb+1 (5) where \u03b3 = (2 < I(p) \u2212 I(q) 2 >) \u22121 [23].", "formula_coordinates": [4.0, 50.16, 347.2, 236.36, 50.69]}], "doi": ""}