{"title": "Evaluating Factuality in Text Simplification", "authors": "Ashwin Devaraj; William Sheffield; Byron C Wallace; Junyi Jessy Li;  Science", "pub_date": "", "abstract": "Automated simplification models aim to make input texts more readable. Such methods have the potential to make complex information accessible to a wider audience, e.g., providing access to recent medical literature which might otherwise be impenetrable for a lay reader. However, such models risk introducing errors into automatically simplified texts, for instance by inserting statements unsupported by the corresponding original text, or by omitting key information. Providing more readable but inaccurate versions of texts may in many cases be worse than providing no such access at all. The problem of factual accuracy (and the lack thereof) has received heightened attention in the context of summarization models, but the factuality of automatically simplified texts has not been investigated. We introduce a taxonomy of errors that we use to analyze both references drawn from standard simplification datasets and state-of-the-art model outputs. We find that errors often appear in both that are not captured by existing evaluation metrics, motivating a need for research into ensuring the factual accuracy of automated simplification models.", "sections": [{"heading": "Introduction", "text": "Simplification methods aim to make texts more readable without altering their meaning. This may permit information accessibility to a wide range of audiences, e.g., non-native speakers (Yano et al., 1994), children (De Belder and Moens, 2010), as well as individuals with aphasia (Carroll et al., 1998) and dyslexia (Rello et al., 2013). Simplification may also help laypeople digest technical information that would otherwise be impenetrable (Damay et al., 2006;Devaraj et al., 2021).\nRecent work has made substantial progress by designing sequence-to-sequence neural models that \"translate\" complex sentences into simplified versions (Xu et al., 2016;Alva-Manchego et al., 2020).\n(1) [Original] There was no difference in operating time or perioperative complication rates.\n[Model simplified] However, there was not enough evidence to determine if there was an important difference in operative time or complication rates when compared to conventional surgery.\n(2) [Original] All studies were associated with methodological limitations.\n[Model simplified] All studies were of poor quality and had limitations in the way they were conducted.\n(3) [Original] On June 24 1979 (the 750th anniversary of the village), Glinde received its town charter.\n[Model simplified] On June 24 1979, the 750th anniversary of the village was renamed.\n(4) [Original] Others agreed with the federal court; they started marrying people in the morning.\n[Model simplified] Others agreed with the federal court; they started trying in morning.\n(5) [Original] In 2014, Mary Barra became CEO of General Motors, making her the first female CEO of a major automobile company.\n[Model simplified] Also, just one woman leads a major automobile company. Omitted main subject. An important but mostly overlooked aspect of automated simplification-especially in the conditional text generation regime-is whether outputs are faithful to the inputs that they are simplifying. Consider, for example, automatically simplifying medical texts (Devaraj et al., 2021): Presenting individuals with readable medical information that contains factual errors is probably worse than providing no such access at all. Recent work has acknowledged factuality and faithfulness as key issues to be addressed in other conditional generation tasks like summarization (Kryscinski et al., 2020a;Maynez et al., 2020;Pagnoni et al., 2021;Goyal and Durrett, 2021), yet so far little research has thoroughly studied the kinds of errors that simplification datasets and system outputs exhibit. This work seeks to close this research gap.\nTable 1 shows examples of generated outputs from existing simplification systems, and these clearly illustrate that factuality is an issue. We conduct multi-dimensional analyses based on the edit nature of simplification (Xu et al., 2015;Dong et al., 2019) and define a small typology of (potential) factual errors in the context of simplification. Inserting information can be useful to define jargon and provide explanatory content, but introducing irrelevant or erroneous content (\"hallucinating\") is bad (e.g., examples 1-2 in Table 1). Omitting information related to the main entity or event could lead to a change in how the text is understood (e.g., example 5 in Table 1). Finally, making inappropriate substitutions can result in inconsistencies (e.g., examples 3-4 in Table 1). Together these dimensions represent the precision, recall, and accuracy of information conveyed in simplified texts.\nWe collect human ratings of factuality for these aspects on two widely used simplification corpora: Wikilarge (Zhang and Lapata, 2017) and Newsela (Xu et al., 2015). Automatically aligned sentences from these two datasets are typically used to train and evaluate supervised simplification systems. We find that errors occur frequently in the validation and test sets of both datasets, although they are more common in Newsela (Section 6).\nWe then evaluate outputs from several modern simplification models (Zhang and Lapata, 2017;Dong et al., 2019;Martin et al., 2020;Maddela et al., 2021), as well as a fine-tuned T5 (Raffel et al., 2020) model. Compared to RNN-based models, Transformer-based ones tend to have less severe deletion and substitution errors; however, the pre-trained T5 produced more hallucinations on the more abstractive Newsela dataset. We find that existing quality metrics for simplification such as SARI (Xu et al., 2016) correlate poorly with factuality. Although deletion errors correlate with existing semantic similarity measures, they fail to capture insertion and substitution.\nAs an initial step towards automatic factuality assessment in simplification, we train RoBERTa (Liu et al., 2019)-based classification models using our annotated data, and use synthetically generated data to supplement training. We demonstrate that this is a challenging task.\nOur code and data can be found at https://github.com/AshOlogn/Evaluating-Factuality-in-Text-Simplification.", "publication_ref": ["b47", "b5", "b2", "b36", "b4", "b6", "b45", "b0", "b6", "b20", "b29", "b31", "b13", "b44", "b8", "b49", "b44", "b49", "b8", "b28", "b27", "b34", "b45", "b26"], "figure_ref": [], "table_ref": ["tab_0", "tab_0", "tab_0", "tab_0"]}, {"heading": "Related Work", "text": "Factuality (and the lack thereof) has been identified as critical in recent work in unsupservised simplification (Laban et al., 2021) and medical simplification (Devaraj et al., 2021). Guo et al. (2018) incorporated textual entailment into their simplification task via an auxillary loss. They showed that this improved simplifications with respect to standard metrics and human assessments of output fluency, adequacy, and simplicity, but they did not explicitly evaluate the resultant factuality of outputs, which is our focus.\nGiven the paucity of prior work investigating factuality in the context of automated simplification, the most relevant thread of research to the present effort is work on measuring (and sometimes improving) the factuality in outputs from neural summarization systems. Falke et al. (2019a) proposed using textual entailment predictions as a means to identify errors in generated summaries. Elsewhere, Kryscinski et al. (2020a) used weak supervisionheuristic transformations used to intentionally introduce factual errors-to train a model to identify inaccuracies in outputs. Maynez et al. (2020) enlisted humans to evaluate hallucinations (content found in a summary but not in its corresponding input) in automatically generated outputs. They report that for models trained on the XSUM dataset (Narayan et al., 2018), over 70% of summaries contain hallucinations. This corroborates other recent work (Falke et al., 2019a;, which has also found that ROUGE is a weak gauge of factuality. Wang et al. (2020a) proposed QAGS, which uses automated question-answering to measure the consistency between reference and generated summaries. Elsewhere, Xu et al. (2020) proposed evaluating textual factuality independent of surface realization via Semantic Role Labeling (SRL). Finally, Pagnoni et al. (2021) introduced the FRANK (meta-)benchmark for evaluating factuality metrics for summarization. While FRANK is tailored towards summarizationspecific error categories including discourse, our ontology broadly reflects the goal of simplification (retaining content with simpler language) from the perspective of information precision, recall, and accuracy.", "publication_ref": ["b22", "b6", "b14", "b10", "b20", "b29", "b30", "b10", "b41", "b46", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "Information Errors in Simplification", "text": "Above we reviewed various recently proposed frameworks and methods for assessing the factual 0: There is no new information included in the simplified text, or the new information is trivial (e.g., a single insignificant word).\n1: New information is introduced in the simplified text, but this information does not introduce a new main idea. Instead, this information merely supports the same main idea expressed in the complex sentence (e.g., elaboration, example).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "2:", "text": "A new main idea is introduced in the simplified text.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "0:", "text": "No information is removed from the complex sentence, or what is removed is very minor and insignificant to the main idea of the complex sentence. 1: Information is removed from the complex text, but its removal does not obfuscate the main idea of the complex text.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "2:", "text": "Information critical to the main idea of the complex text is removed.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "0:", "text": "There is no altered information in the simplified text at all.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "1:", "text": "There exists a piece of altered information in the simplified sentence, but the main idea is still intact.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "2:", "text": "The main idea of the complex text is altered in the simplified text.\n-1: Either the complex or simplified sentence is malformed (i.e., is a sentence fragment or gibberish characters) and cannot be understood.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Insertion Deletion Substitution", "text": "Figure 1: The full annotation scheme: 0: no/trivial change; 1: nontrivial but preserves main idea; 2: does not preserve main idea; -1: gibberish. The -1 label is applicable to all three categories.\naccuracy of automatically-generated summaries.\nWe aim in this work to similarly codify content errors in simplification.\nBelow we describe broad categories of errors 1 we observed in simplification datasets and system outputs, and then use these to design annotation guidelines that formalize accuracy assessment (Section 5). Our analysis revealed three broad categories, illustrated in Table 2:\n(1) Information Insertion: This occurs when information not mentioned in the complex sentence is inserted into-or hallucinated in-its simplified counterpart. The insertion may be as small as mentioning a proper noun not in the complex sentence, or as large as introducing a new main idea. This category is similar to extrinsic hallucination in the summarization literature (Maynez et al., 2020;Goyal and Durrett, 2021).\n(2) Information Deletion: This is when information in the complex sentence is omitted from the simplified sentence. A minor example of this is the reverse of the insertion case above, where an entity is mentioned by name in the complex sentence but only by pronoun in the simplified sentence.\n(3) Information Substitution: This is when information in the complex sentence is modified in the simplified sentence such that it changes the meaning. This category is broad, encompassing both alterations to the simplified sentence that directly contradict information in the complex sentence, and those that do not.\nBecause errors can co-occur, we adopt a multidimensional labeling scheme that requires a different label to be provided for each category. Each category label specifies the severity of the error: 0-no/trivial change; 1-nontrivial but preserves main idea; 2-doesn't preserve main idea; -1gibberish, specified in Figure 1. Table 1 shows 1 We adapt a graded labeling scheme based on content and meaning preservation. For brevity, we use the word \"error\" as a generic term to refer to all the phenomena captured by our labeling scheme, even those that may be considered acceptable in some simplification systems.", "publication_ref": ["b29", "b13"], "figure_ref": [], "table_ref": ["tab_2", "tab_0"]}, {"heading": "Category", "text": "Original  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Interpretation as Precision and Recall", "text": "In simplification one attempts to rewrite a given complex sentence to be simpler while preserving most of the information that it contains. The categories above can be interpreted as errors in information precision (the fraction of content that also appears in the complex sentence) and recall (the fraction of content in the complex sentence preserved during simplification). With this interpretation, a \"false positive\" (affecting precision) occurs when the simplified sentence contains information not present in the source, i.e., introduces a \"hallucination\". And a \"false negative\" (hindering recall) is where the simplified sentence omits key information in the source.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data and Models", "text": "We annotate data from the simplification datasets themselves (we will call these reference examples), as well as from model-generated text. Thus we assess how the distribution of errors in the references compares to that of errors in system outputs and glean insights that might relate model architecture and training choices to the kinds of errors produced.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Datasets.", "text": "We annotated examples from the Wikilarge and Newsela (Xu et al., 2015;Zhang and Lapata, 2017) datasets. These are commonly used in the literature, and so results have been reported on these corpora for a diverse collection of models. Wikilarge comprises 296K roughly-aligned sentences pairs from English Wikipedia and Simple English Wikipedia. Newsela (Xu et al., 2015) consists of 96K sentence pairs extracted from a dataset of news stories rewritten at 4 reading levels by professionals. To make analysis tractable in this work, we examine the simplest level for Newsela. We annotated 400 pairs of (complex, simplified) sentences each from the validation and test sets for Newsela. For Wikilarge, we annotated 400 pairs from the validation set and 359 from the test set (this constitutes the entire test set).\nSimplification Models. We annotated outputs generated by a collection of models on the same validation and test examples from Wikilarge and Newsela, respectively. We selected a set of models intended to be representative of different architectures and training methods.\nMore specifically, for RNN-based models we considered Dress (Zhang and Lapata, 2017) and EditNTS (Dong et al., 2019). Dress is an LSTM model trained using REINFORCE (Williams, 1992) to minimize a reward function consisting of meaning preservation, simplicity, and fluency terms. EditNTS represents each sentence pair as a sequence of edit operations and directly learns these operations to perform simplification.\nFor Transformer-based architectures we evaluated two previously proposed models: Access (Martin et al., 2020) andControlTS (Maddela et al., 2021). Access trains a randomlyinitialized Transformer to generate simplifications parametrized by control tokens influencing traits like lexical complexity and length compression. ControlTS is a hybrid method that generates simplification candidates using grammatical rules and then applies a BERT-based (Devlin et al., 2019) paraphrasing model. In addition, we also fine-tuned T5 (Raffel et al., 2020) for the simplification task, detailed in Appendix A. T5 is a Transformer-based model jointly pretrained both on unsupervised language modeling objectives and a host of supervised tasks including summarization and translation, all framed as text-to-text problems.", "publication_ref": ["b44", "b49", "b44", "b49", "b8", "b43", "b28", "b7", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "Labeling with Mechanical Turk", "text": "Annotation Procedure We use Amazon Mechanical Turk to acquire labels for reference exam-  ples from datasets, and for model-generated simplifications. To ensure that only annotators who understood our labeling scheme would be included, we released a qualification task consisting of 10 sentence pairs with perfect agreement among two of the authors, with detailed explanation of the labeling scheme, and required that annotators achieve at least 75% accuracy on this set. After worker qualification, examples were released to only qualified workers, and each example was annotated by 3 workers. The final label for each category (insertion, deletion, substitution) was set to the majority label if one existed. If every annotator provided a different label for a given category, we removed this example for purposes of this category. For example, if annotators provided insertion labels of {1, 1, 2} and deletion labels of {2, 1, 0} for a specific instance, then this would not be assigned a deletion label, but would receive a \"final\" insertion label of 1. Workers were compensated $10.00 per hour on the annotation task. Inter-annotator Agreement. We quantified the degree of inter-annotator agreement using 3 metrics, each capturing a different dimension of labeling consistency for each category: First, we report the percentage of examples that had a well-defined majority label for each category. Most annotators agreed on labels for the majority of examples (first column in Table 3), meaning that very few annotations had to be discarded for any category.\nBecause 0 was the most common label for all 3 categories, especially for the reference examples from the datasets, we also recorded the percentage of examples with majority non-zero annotations that also have a well-defined majority label. For example, the labels {0, 1, 2} are majority non-zero but do not correspond to a well-defined majority label, while {0, 1, 1} satisfies both conditions. Table 3 (column 2) indicates that even among examples where most annotators agree that there is an error, the majority agree on a specific label of 1, 2, or -1.  We also measured Krippendorff's alpha (Krippendorff, 1970) with an ordinal level of measurement (assigning the -1 label a value of 3 to indicate maximum severity). Dataset annotations for insertion enjoy moderate agreement (\u03b1 = 0.425), those for deletion imply substantial agreement (\u03b1 = 0.639), and those for substitution exhibit fair agreement (\u03b1 = 0.200) (Artstein and Poesio, 2008). The latter is possibly due to the clear majority label of 0 among substitution labels.\nThe % majority agreement scores indicate that although the annotation scheme involves a degree of subjectivity in distinguishing between minor and major errors, with proper screening crowdsource workers can label text pairs with our annotation scheme consistently enough so that a well-defined label can be assigned to the vast majority of examples.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Factuality of Reference Examples", "text": "Quantitative Analysis Table 4 reports distributions of acquired labels for information insertion, deletion, and substitution errors over the annotated reference examples. Deletion errors are far more common than insertion errors in both datasets, though Wikilarge has fewer of both than Newsela. This is unsurprising, as one of the motivations for introducing the Newsela dataset was that it contains shorter and less syntactically-complex simplifications. Reassuringly, there were very few substitution errors found in either dataset.\nTable 5 shows a clear positive correlation between length reduction and the severity of deletion errors present. As expected, sentences are shortened more substantially in Newsela than in Wikilarge. One the other hand, while Table 5 indicates that the examples with nonzero insertion labels collectively see a greater increase in length than those with no insertion errors, the mean length increase for level 2 examples is smaller than that for level 1.\nSimplifications in Newsela are more abstractive (Xu et al., 2015), i.e., simplified sentences copy fewer phrases verbatim from inputs. This can be quantified via normalized edit distance (Levenshtein, 1965), which yielded a median of 0.46 for Newsela examples compared to the 0.38 for Wikilarge (after noise filtering described in Appendix B). Table 5 indicates that on average the more erroneous the insertion or deletion, the greater the normalized edit distance between the original and simplified sentences.\nThese results suggest that while reducing sentence length and rewording can be beneficial (Klare, 1963), too much can negatively impact factuality.\nQualitative Analysis We also manually inspected insertion and deletion errors in both datasets, revealing clear patterns of deletion errors. Label 1 deletions by definition involve omissions of nonsalient details that do not much affect the meaning of the sentence, e.g.:\nOriginal: Mayfield wrote and sang on a string of message-oriented records, including \"Keep on Pushing\" and \"People Get Ready.\" Simplified: Mayfield wrote and sang on records that had a message.\n(Newsela, deletion-1)\nLabel 2 deletions have two common manifestations across the datasets. The first involves deletion of the main clause and subsequent promotion of a secondary clause:\nOriginal: \"Until you know how the sausage is made, you don't know how expensive it is to make that sausage,\" said Josh Updike, creative director of Rethink Leisure & Entertainment, which is working on several projects in China and elsewhere in Asia. Simplified: The company is working on several projects in China and Asia.\n(Newsela, deletion-2)\nAnother common type of label 2 deletion involves removing a key (though often small) phrase that effectively reframes the entire sentence, e.g.: By deleting in the Modified Version (emphasis ours), the simplified sentence erroneously states that one may add front-and back-cover passages to the list of cover texts to the unmodified version, which is implicitly forbidden in the original.\nBecause of the small number of insertion errors on Wikilarge, we were unable to identify any meaningful trends. However, we observed patterns in  Newsela for both levels 1 and 2 of insertions, pertaining to quotative phrases (e.g., inserting \"experts said\" to the beginning of a sentence even though the original sentence did not mention an expert), and temporal phrases, e.g.:\nOriginal: They could not afford to pay their son's roughly $10,000 cost for classes at the University of Texas at Austin. Simplified: When he grew up, they could not afford to pay $10,000 for him to go to the University of Texas at Austin.\n(Newsela, insertion-1)\nAnother error trend pertains to a change in specificity:\nOriginal: Mutanabbi Street has always been a hotbed of dissent. Simplified: Mutanabbi Street has always been a place where protest marches are held. (Newsela, insertion-2)\nWe observed more contextually related errors for Newsela due to its style and its simplification process. Newsela documents were edited by professionals who rewrote the entire original document, and so information inserted or deleted could move from or to adjacent sentences. This preserves information for the whole document but causes problems at the sentence level. Also, compared to Wikilarge, Newsela's news articles naturally involve more complex discourse (Van Dijk, 2013). These factors lead to relatively underspecified sentences (Li et al., 2016) in the simplified text when they are taken out-of-context during training and evaluation. This observation calls for the inclusion of document context during simplification (Sun et al., 2020), or performing decontextualization (Choi et al., 2021) before simplifying.", "publication_ref": ["b44", "b23", "b16", "b39", "b25", "b38", "b3"], "figure_ref": [], "table_ref": ["tab_6", "tab_8", "tab_8", "tab_8"]}, {"heading": "Factuality of System Outputs", "text": "Table 6 shows the distributions of insertion, deletion, and substitution errors annotated in system outputs. 2 It also shows the standard simplification evaluation metric-SARI scores (Xu et al., 2016)for the annotated set. For the three models that 2 DRESS only released their Wikilarge outputs; ControlTS had different data splits for Newsela. We could not successfully reproduce their results for Newsela. reported both Wikilarge and Newsela outputs, the relative frequency of deletion errors between the two datasets appears to be preserved in model outputs, though for the RNN models errors are milder on Newsela and amplified on Wikilarge.\nA clear relationship between dataset and system output distributions does not exist for insertion and substitution errors. For Dress and EditNTS, this is due to the fact that the minor differences in insertion errors are dwarfed by the larger number of -1 (gibberish) labels assigned to Newsela outputs. Interestingly, outputs from the T5 model were rarely labeled as -1 errors, so the difference in insertion errors is more apparent. In the case of substitution, the Newsela outputs for Dress and T5 models show much higher rates of substitution errors than the Wikilarge outputs, despite the opposite being true for the datasets themselves. EditNTS does not show the same pattern, but again, the high rate of -1 errors subsumes every other trend. One possible reason for this phenomenon could be that the higher abstractiveness of Newsela encourages models to rewrite the input sentence to a greater extent and destroy the original meaning in the process. In general the models produce substitution errors more frequently than are found in the dataset, meaning that they are introduced by the models themselves and not merely learned from the data.\nModel comparisons There are a few differences in error distributions between the RNN-based and Transformer-based models, and between pretrained vs. non-pretrained Transformer models. All three Transformer models have less severe deletion errors than the RNN models on Wikilarge, and in addition T5 has lower deletion error rates on Newsela. Perhaps the most striking trend is that the Transformer models have far lower -1 gibberish errors than RNN-based models, even Access, which is not pre-trained on the language modeling task. T5-which has been pre-trained on large amounts of data-produced more insertion errors, while Access produced more substitution errors.  Quantitative Analysis We explore the relationships between the factuality annotations of system outputs and both length reduction and normalized edit distance. We briefly describe our findings here and defer numerical details to Appendix C.\nFor every model except Access, there is a clear positive correlation between the severity of deletion errors and the degree of length reduction between the complex input and generated simplification. This is consistent with the trend observed for the datasets. No consistent relationships between length change and levels of insertion and substitution errors are exhibited by the system outputs. As in the case of length reduction, mean edit distances increase with the severity of deletion error with no consistent trends found for insertion and substitution labels.", "publication_ref": ["b45"], "figure_ref": [], "table_ref": ["tab_10"]}, {"heading": "Qualitative analysis", "text": "We also manually inspect model outputs, detailed in Appendix D, and summarize main observations here. As in the data, models also produce deletions ranging from single words and short phrases to clauses. For the two RNN models, DRESS and EditNTS, level 1 errors primarily consist of shorter deletion errors, which include pronoun errors and modifiers. Level 2 errors are almost always longer deletions, yet we did not observe the promotion of a subordinate clause to a main one as in the references, suggesting that models tend to follow syntactic rules more strictly. For T5, we additionally observe level 2 errors in which the model deletes a semantically critical word. We observed more error variability in the other two transformer models, Access and ControlTS. Models introduced varying numbers of insertion and substitution errors, but in inspection we did not observe any clear properties of these as a function of model type.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Comparison with Existing Metrics", "text": "Relationship to SARI. SARI is the most popular metric used to evaluate text simplification models (Xu et al., 2016). For each model, we report Spearman's rank correlation coefficient (Spearman, 1904) between SARI and each error category. As Table 7 reports, there is only a weak correlation between SARI and the prevalence of information errors, and both the direction and magnitude of the correlation are highly dependent on model and dataset. This lack of correlation is unsurprising since SARI uses lexical overlap between the generated text with the reference text pair to judge simplification quality. This parallels the case with ROUGE in summarization (Falke et al., 2019a;Maynez et al., 2020;.\nMeasures of Semantic Similarity. Many existing text simplification systems attempt to address the problem of meaning preservation by using a semantic similarity score either directly in their loss/reward function or in a candidate ranking step (Zhang and Lapata, 2017;Kriz et al., 2019;Zhao et al., 2020;Maddela et al., 2021). Additionally, some of these metrics have been included in recent factuality evaluation platforms in summarization (Pagnoni et al., 2021). We explore the extent to which existing similarity methods detect  information errors as outlined in our annotation scheme. We consider: (1) Jaccard similarity;\n(2) cosine similarity between averaged GloVe (Pennington et al., 2014) or ELMo (Peters et al., 2018) embeddings of the original and simplified sentences;\n(3) cosine similarity between Sentence-BERT (Reimers and Gurevych, 2019) embeddings; and (4) BERTScore (Zhang et al., 2019).\nAs Table 8 indicates, the semantic similarity measures explored capture deletion errors quite well, while being a moderate indicator of insertion errors and a very weak one for substitution errors. Since deletion and substitution errors are common in most of the models we evaluated, the results indicate that better methods are needed to detect unacceptable deletions and intrinsic hallucinations in simplification outputs.", "publication_ref": ["b45", "b37", "b10", "b29", "b49", "b18", "b50", "b27", "b31", "b32", "b33", "b35", "b48"], "figure_ref": [], "table_ref": ["tab_12", "tab_13"]}, {"heading": "Measures of Factuality.", "text": "As in text simplification, the most common evaluation metrics used in text summarization like ROUGE do not adequately account for the factuality of model generations with respect to the input texts (Kryscinski et al., 2019). For this reason, recent works have proposed model-based metrics to automatically assess factuality (Falke et al., 2019b;Durmus et al., 2020;Wang et al., 2020b;Kryscinski et al., 2020b;Goyal and Durrett, 2020). We consider the following systems: (1) FACT-CC, which is a BERT-based model trained on a synthetic dataset to classify text pairs as being factually inconsistent or not (Kryscinski et al., 2020b), and (2) DAE, which is another BERT-based model that classifies each dependency arc in the model output as entailing the source text or not (Goyal and Durrett, 2020). More specifically, for FACT-CC we use the model's probability that each simplification example is inconsistent. For DAE we use the average of the lowest k probabilities that a dependency arc in the target sentence does not entail the source for k = 1, 3, 5.\nAs Table 9 indicates, both FACT-CC and DAE's outputs correlate less with insertion and deletion annotations than even surface-level measures of  semantic similarity like Jaccard similarity, though DAE scores correlate better with substitution errors than do FACT-CC and all evaluated measures of semantic similarity.", "publication_ref": ["b19", "b11", "b9", "b42", "b21", "b12", "b21", "b12"], "figure_ref": [], "table_ref": ["tab_14"]}, {"heading": "Automatic Factuality Assessment", "text": "Since manual annotation is costly and timeconsuming, as a first step towards large-scale evaluation, we present an initial attempt at automating factuality assessment by training a model on human annotations. To supplement training, we explore methods of generating synthetic data to improve model performance.\nWe framed automatic factuality assessment as a classification task in which a separate classifier is trained for each category (Insertion, Deletion, and Substitution), for each of the levels 0, 1, and 2. We treat the annotations used in our previous analyses as the test set and have additional data annotated to function as the training set for this task. We therefore collected a total of 1004 additional examples annotated across Wikilarge, Newsela, Access outputs on Wikilarge, and T5 outputs on Newsela and Wikilarge. We fine-tuned RoBERTa (Liu et al., 2019) with a classification head. 10 indicates, the validation dataset is both small and highly imbalanced, with very few level 2 insertion and substitution errors. To alleviate this issue, we experimented with a few methods of generating synthetic insertion and substitution errors on which to pretrain the model. We accomplished this by modifying each of the complex sentences in the validation set. To generate insertion errors, we replace names with pronouns and remove phrases from the source text to create target texts (information deletions) and then swap the source and target to produce information insertions. To generate substitutions, we change numbers in the source text, negate statements, and used BERT masking to perturb information in the sentence. We generated 10K examples in total; Appendix E.1 describes these  methods in greater detail.", "publication_ref": ["b26"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Synthetic Data Generation As Table", "text": "Training and Evaluation The model is evaluated using the F1-scores with respect to each class (0,1,2), and when selecting checkpoints during training, the average of the label 1 and 2 F1 scores is used. The deletion model was trained directly on its training data, whereas the insertion and substitution models were initially pretrained on the synthetic datasets. Training details are provided in Appendix E.3.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Table 10 shows the test F1 scores achieved by the three classifiers. As expected, the deletion classifier achieved the best 1 and 2 F1 scores, likely due to the fact that the training dataset had plenty of level 1 and 2 deletion errors.\nAlthough the insertion and substitution datasets are similarly skewed, the insertion classifier significantly outperforms the substitution one. We found that using synthetic data is useful: without it, F1s for levels 1 and/or 2 are near 0 for insertion and substitution. Even with data augmentation, however, detecting errors is a challenging task.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Conclusion", "text": "We have presented an evaluation of the factuality of automated simplification corpora and model outputs, using an error typology with varied degrees of severity. We found that errors appear frequently in both references and generated outputs.\nIn the datasets, deletion errors are quite frequent, with Newsela containing more than Wikilarge. The system outputs indicate that the models also tend to delete information, which is likely a behavior learned from the training data. Model outputs contain more substitution errors than the datasets, so that behavior is probably a model bias rather than something picked up from the data. Although we examined the two commonly used sentence-level datasets, factuality errors do extend to other domains and larger units of text. Our initial analysis of factuality in medical text simplification (Devaraj et al., 2021) found errors of all three types, an indication that factual simplification is an open problem in such high-stake areas. The details of our analysis are in Appendix F.\nWe also found that factuality errors are not well captured by existing metrics used in simplification such as SARI (Xu et al., 2016). While semantic similarity metrics correlate with deletion errors, they poorly correlate with insertion or substitution. We further present an initial model for automatic factuality assessment, which we demonstrate is a challenging task.", "publication_ref": ["b6", "b45"], "figure_ref": [], "table_ref": []}, {"heading": "A Training details for the T5 simplification model", "text": "We used the T5 base architecture, which contains around 220M parameters. For both Newsela and Wikilarge, we trained the T5 model for 5 epochs with a batch size of 6 and constant learning rate of 3e-4. We prefixed each input text with the summarization prefix summarize:, since that was the task closest to simplification that the T5 model was pretrained on. Newsela simplifications were generated using nucleus sampling with p = 0.9 (Holtzman et al., 2020), and Wikilarge simplifications were generated using beam search with 6 beams.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Noise filtering on Wikilarge", "text": "To filter out noisy alignments in the Wikilarge test set (when comparing the normalized edit distances between complex and simplified sentences in Newsela and Wikilarge), we employed the same method as used by Xu et al. (2015) to produce sentence-level alignments from the Newsela dataset, that is, we only keep sentence pairs if they have a Jaccard similarity of at least 0.4 if the simplification is one sentence long and 0.2 if it is longer than one sentence.", "publication_ref": ["b44"], "figure_ref": [], "table_ref": []}, {"heading": "C Numerical Details for System Output Results", "text": "Table 11 shows the relationship between mean % length reduction from input text to model output and the level of factuality errors present in the example. Table 12 likewise shows the relationship between normalized edit distance between inputs and model outputs and factuality annotations.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0", "tab_0"]}, {"heading": "D Qualitative Analysis of System Outputs", "text": "We also manually examined system outputs for error trends. Despite output variability for every model, two primary trends were observed in deletion errors across the models for both Wikilarge and, where available, Newsela. No trends could be drawn for insertion and substitution errors because of their infrequency. The first type of deletion error, hence referred to as a \"short\", is the deletion or change of a single word or short phrase, usually a modifier (such as an adjective, adverb, or serialized noun), but occasionally a noun, noun phrase, or verb. For example:\nOriginal: The equilibrium price for a certain type of labor is the wage. Simplified: The price of a certain type of labor is the wage.\n(ControlTS, Wikilarge, deletion-1)\nWhen the word is changed rather than deleted, the replacing word is often less descriptive but can also be lateral. Shorts include pronoun errors, where a noun phrase is replaced with a pronoun. Note also that multiple, independent shorts may occur in an output and still receive a level 1 for deletion.\nThe second type of error, hence referred to as a \"long\", is the deletion of a phrase, most commonly a prepositional phrase, or a subordinate or coordinate clause. For example:\nOriginal: For Rowling, this scene is important because it shows Harry's bravery, and by retrieving Cedric's corpse, he demonstrates selflessness and compassion. Simplified: For Rowling, this scene is important because it shows Harry's bravery.\n(Dress, Wikilarge, deletion-2) Importantly, longs concerning clauses differ from the clause promotion error found in the datasets in that longs delete a subordinate or coordinate clause of the original while clause promotion errors delete the main clause of the original. Multiple, independent longs rarely occur in one output; that is, if multiple secondary clauses are deleted, they are usually nested (likely because a sentence where this could happen would have a very complex structure, at least in English.)\nAccess and ControlTS had notable variability in the errors. Despite this, shorts were the most common error for label 1, with no notable presence of longs. These shorts were often not pronoun errors. Additionally, no trends could be noted for label 2 errors in these models. By contrast, nearly all of Dress's errors fit into these two trends. Label 1 output errors primarily consisted of shorts, especially pronoun errors, though longs also occurred. Label 2 output errors were almost entirely longs. EditNTS and T5 errors closely follow the trends found in Dress, though T5 notably had several label 2 errors that were shorts, deleting a semanticallycritical word.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E Automatic Factuality Assessment", "text": "Here we describe the details of generating synthetic data and training the three annotation classifiers.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.1 Synthetic Data Generation", "text": "Name Insertion. Each name of a person in the source text is replaced one at a time with a pronoun   to create a target text. Then the source and target texts are swapped to simulate the insertion of a name in place of a pronoun. This text pair is labeled with a level 1 insertion. Phrase Insertion. Each phrase in the source text is deleted one at a time to create a shorter target text, and the source and target texts are swapped to simulate the insertion of a phrase. The insertion is labeled as a level 1 if the BERTScore of the texts is between 0.6 and 0.8, and it is labeled as 2 if it is between 0.2 and 0.4. If the score is not in either interval, the example is discarded. These thresholds were determined by manual inspection of the distribution of scores computed in Section 8. Number Alteration. We replace each number found in the source sentence one at a time with a random number of the same order of magnitude (e.g., 3 \u2192 7, 99 \u2192 74). This modification is labeled as a level 1 substitution. Statement Negation. Each auxiliary verb in the source text is negated one at a time to generate target texts. This modification is labeled as a level 1 substitution. BERT Masking. To generate level 1 substitutions, we randomly mask 2 tokens in the source text, pass the masked text through a BERT model, and fill the masked tokens with the third highest probability token in the output logits. To generate level 2 substitutions, we instead mask every fifth token in the source text and fill them with the fifth highest  probability token indicated by the logits.\nOnce synthetic examples were generated, all the label 0 examples from the original training dataset were added. In the insertion synthetic dataset, level 1 labels significantly outnumbered level 2 labels, so only a random sample of them was included in the final dataset. Table 13 shows the sizes and label distributions of the synthetic datasets. Some class imbalance was tolerated here since the number of examples for all levels was much larger than in the original training set and minority classes were oversampled during training.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "E.2 Model", "text": "We fine-tune the pretrained base RoBERTa model architecture with a classification head. The model contains 12 hidden layers, a hidden size of 768, and 12 attention heads.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.3 Training Details", "text": "The insertion and substitution models were pretrained on an 80-20 train/dev split of their synthetic datasets for 10 epochs with a batch size of 64 and learning rate of 1e-4 and evaluated on the validation split every 100 steps.\nThe best checkpoint was selected and then trained on an 80-20 split of its original dataset for 50 epochs with the same batch size and learning rate and evaluated every 10 steps. The best model from this round was finally fine-tuned on the entire training dataset for 1 epoch with the same batch size but a learning rate of 3e-5 before being evaluated on the test set.\nThe deletion classifier was trained similarly, except that the pretraining step was omitted.\nIn every stage of training, minority classes were oversampled in the training split until they matched the frequency of the most populous class.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F Case Study: Medical Texts", "text": "We present an initial analysis of factuality in the context of medical text simplification (Devaraj et al., 2021), a case where information accuracy is paramount. This task presents unique challenges given the complex, jargon-laden texts to be simplified. We evaluate a model proposed in recent work for medical text simplification (Devaraj et al., 2021). This was trained by fine-tuning BART (Lewis et al., 2020) on aligned paragraphs of technical abstracts-plain English summaries from the Cochrane library, a database of systematic reviews of clinical trials. We annotated 10 randomly selected outputs from this model with respect to the original paragraphs. 3 Because the original texts are difficult to understand, we enlist a trained annotator (a senior in Linguistics and co-author of this work) to perform this evaluation.\nTable 14 reports the number of error types observed across paragraphs. The error rate here is disconcerting: Out of the 10 paragraphs evaluated, we found 3 with at least one level-2 error, and 5 with more than one error. We provide examples or errors below. These findings further stresses the critical importance of factuality evaluation and improvement in simplification systems. 3 Note that while so far we have applied our annotation framework with respect to sentences, it is not tied to any specific linguistic unit.", "publication_ref": ["b6", "b6", "b24"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Category", "text": "Level 0 Level 1 Level 2   Devaraj et al. (2021).\nSimplified: [...] However, there was not enough evidence to determine if there was an important difference in operative time or complication rates when compared to conventional surgery. [...]", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "Deletion", "text": "Original: Two trials with a total population of 1300 amyotrophic lateral sclerosis patients who were randomized to treatment with subcutaneous injections of recombinant human ciliary neurotrophic factor or placebo were examined in this review. [...] Simplified: In this review, we found two randomisedcontrolled trials with a total population of 1300 patients who were randomized to treatment with subcutaneous injections of recombinant human ciliary neurotrophic factor or placebo. [...] ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "This work was partially supported by NSF grants IIS-1850153, IIS-2107524, IIS-1901117, as well as the National Institutes of Health (NIH), grant R01-LM012086. We also acknowledge the Texas Advanced Computing Center (TACC) at UT Austin for providing the computational resources for many of the results within this paper. We are grateful to the anonymous reviewers for their comments and feedback.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Data-driven sentence simplification: Survey and benchmark", "journal": "Computational Linguistics", "year": "2020", "authors": "Fernando Alva-Manchego; Carolina Scarton; Lucia Specia"}, {"ref_id": "b1", "title": "Inter-Coder Agreement for Computational Linguistics", "journal": "Computational Linguistics", "year": "2008", "authors": "Ron Artstein; Massimo Poesio"}, {"ref_id": "b2", "title": "Practical simplification of english newspaper text to assist aphasic readers", "journal": "", "year": "1998", "authors": "John Carroll; Guido Minnen; Yvonne Canning; Siobhan Devlin; John Tait"}, {"ref_id": "b3", "title": "Decontextualization: Making sentences stand-alone", "journal": "Transactions of the Association for Computational Linguistics", "year": "2021", "authors": "Eunsol Choi; Jennimaria Palomaki; Matthew Lamm; Tom Kwiatkowski; Dipanjan Das; Michael Collins"}, {"ref_id": "b4", "title": "SIMTEXT: Text simplification of medical literature", "journal": "", "year": "2006", "authors": "Jerwin Jan; S Damay; Gerard Jaime; D Lojico; Kimberly Amanda; L Lu; D Tarantan; E Ong"}, {"ref_id": "b5", "title": "Text simplification for children", "journal": "", "year": "2010", "authors": "Jan De Belder; Marie-Francine Moens"}, {"ref_id": "b6", "title": "Paragraph-level simplification of medical texts", "journal": "", "year": "2021", "authors": "Ashwin Devaraj; Iain Marshall; Byron Wallace; Junyi Jessy Li"}, {"ref_id": "b7", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Long and Short Papers", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b8", "title": "EditNTS: An neural programmer-interpreter model for sentence simplification through explicit editing", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Yue Dong; Zichao Li; Mehdi Rezagholizadeh; Jackie Chi Kit Cheung"}, {"ref_id": "b9", "title": "FEQA: A question answering evaluation framework for faithfulness assessment in abstractive summarization", "journal": "", "year": "2020", "authors": "Esin Durmus; He He; Mona Diab"}, {"ref_id": "b10", "title": "Ranking generated summaries by correctness: An interesting but challenging application for natural language inference", "journal": "", "year": "2019", "authors": "Tobias Falke; Leonardo F R Ribeiro; Ido Prasetya Ajie Utama; Iryna Dagan;  Gurevych"}, {"ref_id": "b11", "title": "Ranking generated summaries by correctness: An interesting but challenging application for natural language inference", "journal": "", "year": "2019", "authors": "Tobias Falke; Leonardo F R Ribeiro; Ido Prasetya Ajie Utama; Iryna Dagan;  Gurevych"}, {"ref_id": "b12", "title": "Evaluating factuality in generation with dependency-level entailment. Findings of the Association for Computational Linguistics", "journal": "", "year": "2020", "authors": "Tanya Goyal; Greg Durrett"}, {"ref_id": "b13", "title": "Annotating and modeling fine-grained factuality in summarization", "journal": "", "year": "2021", "authors": "Tanya Goyal; Greg Durrett"}, {"ref_id": "b14", "title": "Dynamic multi-level multi-task learning for sentence simplification", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Han Guo; Ramakanth Pasunuru; Mohit Bansal"}, {"ref_id": "b15", "title": "The curious case of neural text degeneration", "journal": "", "year": "2020", "authors": "Ari Holtzman; Jan Buys; Li Du; Maxwell Forbes; Yejin Choi"}, {"ref_id": "b16", "title": "Measurement of readability", "journal": "", "year": "1963", "authors": "George Roger Klare"}, {"ref_id": "b17", "title": "Estimating the reliability, systematic error and random error of interval data", "journal": "Educational and Psychological Measurement", "year": "1970", "authors": "Klaus Krippendorff"}, {"ref_id": "b18", "title": "Complexity-weighted loss and diverse reranking for sentence simplification", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Reno Kriz; Jo\u00e3o Sedoc; Marianna Apidianaki; Carolina Zheng; Gaurav Kumar; Eleni Miltsakaki; Chris Callison-Burch"}, {"ref_id": "b19", "title": "Neural text summarization: A critical evaluation", "journal": "", "year": "2019", "authors": "Wojciech Kryscinski; Nitish Shirish Keskar; Bryan Mc-Cann; Caiming Xiong; Richard Socher"}, {"ref_id": "b20", "title": "Evaluating the factual consistency of abstractive text summarization", "journal": "", "year": "2020", "authors": "Wojciech Kryscinski; Bryan Mccann; Caiming Xiong; Richard Socher"}, {"ref_id": "b21", "title": "Evaluating the factual consistency of abstractive text summarization", "journal": "", "year": "2020", "authors": "Wojciech Kryscinski; Bryan Mccann; Caiming Xiong; Richard Socher"}, {"ref_id": "b22", "title": "Keep it simple: Unsupervised simplification of multi-paragraph text", "journal": "Long Papers", "year": "2021", "authors": "Philippe Laban; Tobias Schnabel; Paul Bennett; Marti A Hearst"}, {"ref_id": "b23", "title": "Binary codes capable of correcting deletions, insertions, and reversals. Soviet physics. Doklady", "journal": "", "year": "1965", "authors": "Vladimir I Levenshtein"}, {"ref_id": "b24", "title": "BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension", "journal": "", "year": "2020", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal ; Abdelrahman Mohamed; Omer Levy; Veselin Stoyanov; Luke Zettlemoyer"}, {"ref_id": "b25", "title": "Improving the annotation of sentence specificity", "journal": "", "year": "2016", "authors": "Jessy Junyi;  Li; O' Bridget; Yi Daniel; Wenli Wu; Ani Zhao;  Nenkova"}, {"ref_id": "b26", "title": "", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b27", "title": "Controllable text simplification with explicit paraphrasing", "journal": "", "year": "2021", "authors": "Mounica Maddela; Fernando Alva-Manchego; Wei Xu"}, {"ref_id": "b28", "title": "Controllable sentence simplification", "journal": "", "year": "2020", "authors": "Louis Martin; \u00c9ric De La Clergerie; Beno\u00eet Sagot; Antoine Bordes"}, {"ref_id": "b29", "title": "On faithfulness and factuality in abstractive summarization", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Joshua Maynez; Shashi Narayan; Bernd Bohnet; Ryan Mcdonald"}, {"ref_id": "b30", "title": "Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Shashi Narayan; Shay B Cohen; Mirella Lapata"}, {"ref_id": "b31", "title": "Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics", "journal": "", "year": "2021", "authors": "Artidoro Pagnoni; Vidhisha Balachandran; Yulia Tsvetkov"}, {"ref_id": "b32", "title": "GloVe: Global vectors for word representation", "journal": "Association for Computational Linguistics", "year": "2014", "authors": "Jeffrey Pennington; Richard Socher; Christopher Manning"}, {"ref_id": "b33", "title": "Deep contextualized word representations", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Matthew E Peters; Mark Neumann; Mohit Iyyer; Matt Gardner; Christopher Clark; Kenton Lee; Luke Zettlemoyer"}, {"ref_id": "b34", "title": "Exploring the limits of transfer learning with a unified text-totext transformer", "journal": "Journal of Machine Learning Research", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b35", "title": "Sentence-BERT: Sentence embeddings using Siamese BERTnetworks", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Nils Reimers; Iryna Gurevych"}, {"ref_id": "b36", "title": "Frequent words improve readability and short words improve understandability for people with dyslexia", "journal": "Springer Berlin Heidelberg", "year": "2013", "authors": "Luz Rello; Ricardo Baeza-Yates; Laura Dempere-Marco; Horacio Saggion"}, {"ref_id": "b37", "title": "The proof and measurement of association between two things", "journal": "American Journal of Psychology", "year": "1904", "authors": "Charles Spearman"}, {"ref_id": "b38", "title": "On the helpfulness of document context to sentence simplification", "journal": "", "year": "2020", "authors": "Renliang Sun; Zhe Lin; Xiaojun Wan"}, {"ref_id": "b39", "title": "News as discourse", "journal": "", "year": "2013", "authors": " Teun A Van Dijk"}, {"ref_id": "b40", "title": "Generating (Factual?) Narrative Summaries of RCTs: Experiments with Neural Multi-Document Summarization", "journal": "", "year": "2021", "authors": "Byron C Wallace; Sayantan Saha; Frank Soboczenski; Iain J Marshall"}, {"ref_id": "b41", "title": "Asking and answering questions to evaluate the factual consistency of summaries", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Alex Wang; Kyunghyun Cho; Mike Lewis"}, {"ref_id": "b42", "title": "Asking and answering questions to evaluate the factual consistency of summaries", "journal": "", "year": "2020", "authors": "Alex Wang; Kyunghyun Cho; Mike Lewis"}, {"ref_id": "b43", "title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning", "journal": "Machine learning", "year": "1992", "authors": "J Ronald;  Williams"}, {"ref_id": "b44", "title": "Problems in current text simplification research: New data can help", "journal": "Transactions of the Association for Computational Linguistics", "year": "2015", "authors": "Wei Xu; Chris Callison-Burch; Courtney Napoles"}, {"ref_id": "b45", "title": "Optimizing statistical machine translation for text simplification", "journal": "Transactions of the Association for Computational Linguistics", "year": "2016", "authors": "Wei Xu; Courtney Napoles; Ellie Pavlick; Quanze Chen; Chris Callison-Burch"}, {"ref_id": "b46", "title": "Fact-based content weighting for evaluating abstractive summarisation", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Xinnuo Xu; Ond\u0159ej Du\u0161ek; Jingyi Li; Verena Rieser; Ioannis Konstas"}, {"ref_id": "b47", "title": "The effects of simplified and elaborated texts on foreign language reading comprehension. Language learning", "journal": "", "year": "1994", "authors": "Yasukata Yano; H Michael; Steven Long;  Ross"}, {"ref_id": "b48", "title": "Bertscore: Evaluating text generation with bert", "journal": "", "year": "2019", "authors": "Tianyi Zhang; Varsha Kishore; Felix Wu; Q Kilian; Yoav Weinberger;  Artzi"}, {"ref_id": "b49", "title": "Sentence simplification with deep reinforcement learning", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Xingxing Zhang; Mirella Lapata"}, {"ref_id": "b50", "title": "Semi-supervised text simplification with back-translation and asymmetric denoising autoencoders", "journal": "", "year": "2020", "authors": "Yanbin Zhao; Lu Chen; Zhi Chen; Kai Yu"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Original:You may add a passage of up to five words as a Front-Cover Text, and a passage of up to 25 words as a Back-Cover Text, to the end of the list of Cover Texts in the Modified Version. Simplified: You may add a passage of up to five words as a Front-Cover Text and a passage of up to 25 words as a Back-Cover Text to the end of the list of Cover Texts.(Wikilarge, deletion-2)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Insertion Original: [...] All studies were associated with methodological limitations. [...] Simplified: [...] All studies were of poor quality and had limitations in the way they were conducted. [...]SubstitutionOriginal: [...] There was no difference in operating time or perioperative complication rates.[...]    ", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Original texts from the Wiki, news, and medical domains with corresponding outputs from simplification systems. Models introduce factual errors.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ": Illustrative examples of the three categories ofinformation errors. Not from a real dataset.level-2 examples from system outputs for inser-tion (examples 1-2), substitution (examples 3-4),and deletion (example 5). Reference examples arediscussed in Section 6."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Percentage of examples with majority annotator agreement for each category and percentage of examples with a majority nonzero label in which the majority of annotators agreed on the specific label.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Insertion, deletion, and substitution error distributions (%) in Wikilarge and Newsela test datasets.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "(23.8) -19.0 (36.9) -38.3 (29.0) 0.41 (0.17) 0.51 (0.21) 0.54 (0.04)", "figure_data": "% length changeNormalized edit distanceLevel 0Level 1Level 2Level 0Level 1Level 2Insertion Wikilarge -5.0 (17.0)22.4 (36.9)7.1 (0.0)0.20 (0.20) 0.55 (0.40) 0.58 (0.0)Newsela -39.4 Deletion Wikilarge 2.8 (15.8)-22.3 (18.9) -35.9 (15.9) 0.19 (0.23) 0.35 (0.18) 0.39 (0.14)Newsela1.5 (27.6)-34.8 (23.1) -49.6 (22.8) 0.34 (0.31) 0.46 (0.13) 0.53 (0.10)"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "% length change (left) and normalized edit distances (right) in simplified sentences in each insertion and deletion error category (mean \u00b1 standard deviation).", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "SARI and error distributions in system outputs manually evaluated.", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "", "figure_data": ""}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "", "figure_data": ": Spearman's rank correlation coefficients for se-mantic similarity measures vs. each information errorcategory (Insertion, Deletion, Substitution)."}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Spearman's rank correlation coefficients for factuality measures vs. each information error category (Insertion, Deletion, Substitution).", "figure_data": ""}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "Annotated label counts in the training set, and F1 on the test set.", "figure_data": ""}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "% length change in system outputs (mean).", "figure_data": "InsertionDeletionSubstitutionModelDataset012012012DressWikilarge 0.23 0.06 0.42 0.03 0.32 0.49 0.23 0.22 0.17Newsela0.29--0.07 0.38 0.48 0.28 0.30 0.33EditNTSWikilarge 0.18 0.46-0.10 0.25 0.43 0.20 0.17 0.18Newsela0.36 0.33-0.10 0.37 0.46 0.37 0.42 0.25T5Wikilarge 0.08 0.53-0.04 0.30 0.56 0.09 0.09-Newsela0.30 0.36 0.56 0.13 0.39 0.13 0.33 0.13-AccessWikilarge 0.20 0.31 0.14 0.17 0.23 0.42 0.22 0.20 0.21ControlTS Wikilarge 0.24 0.43 0.52 0.12 0.38 0.50 0.27 0.24 0.52"}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_19", "figure_caption": "Normalized edit distances in system outputs (mean).", "figure_data": ""}, {"figure_label": "13", "figure_type": "table", "figure_id": "tab_21", "figure_caption": "Sizes and label distributions of synthetic datasets.", "figure_data": ""}, {"figure_label": "14", "figure_type": "table", "figure_id": "tab_23", "figure_caption": "Error judgments of the 10 example outputs from", "figure_data": ""}], "formulas": [], "doi": "10.1162/coli_a_00370"}