{"title": "Rates of Convergence for Sparse Variational Gaussian Process Regression", "authors": "David R Burt; Carl Edward Rasmussen; Mark Van Der Wilk", "pub_date": "", "abstract": "Excellent variational approximations to Gaussian process posteriors have been developed which avoid the O N 3 scaling with dataset size N . They reduce the computational cost to O N M 2 , with M N the number of inducing variables, which summarise the process. While the computational cost seems to be linear in N , the true complexity of the algorithm depends on how M must increase to ensure a certain quality of approximation. We show that with high probability the KL divergence can be made arbitrarily small by growing M more slowly than N . A particular case is that for regression with normally distributed inputs in D-dimensions with the Squared Exponential kernel, M = O(log D N ) suffices. Our results show that as datasets grow, Gaussian process posteriors can be approximated cheaply, and provide a concrete rule for how to increase M in continual learning scenarios.", "sections": [{"heading": "Introduction", "text": "Gaussian processes (GPs) [Rasmussen & Williams, 2006] are distributions over functions that are convenient priors in Bayesian models. They can be seen as infinitely wide neural networks [Neal, 1996], and are particularly popular in regression models, as they produce good uncertainty estimates, and have closed-form expressions for the posterior and marginal likelihood. The most well known drawback of GP regression is the computational cost of the exact calculation of these quantities, which scales as O N 3 in time and O N 2 in memory where N is the number of training examples. Low-rank approximations [Qui\u00f1onero Candela & Rasmussen, 2005]  While the computational cost of adding inducing variables is well understood, results on how many are needed to achieve a good approximation are lacking. As the dataset size increases, we cannot expect to keep the capacity of the approximation constant without the quality deteriorating. Taking into account the rate at which M must increase with N to achieve a particular approximation accuracy, as well as the cost of initializing or optimizing the inducing points, determines a more realistic sense of the costs of scaling Gaussian processes.\nApproximate GPs are often trained using variational inference [Titsias, 2009], which minimizes the KL divergence from an approximate posterior to the full posterior process [Matthews et al., 2016]. We use this KL divergence as our metric for the approximate posterior's quality. We show that under intuitive assumptions the number of inducing variables only needs to grow at a sublinear rate for the KL between the approximation and the posterior to go to zero. This shows that very sparse approximations can be used for large datasets, without introducing much bias into hyperparameter selection through evidence lower bound (ELBO) maximisation, and with approximate posteriors that are accurate in terms of their prediction and uncertainty.\nThe core idea of our proof is to use upper bounds on the KL divergence that depend on the quality of a Nystr\u00f6m approximation to the data covariance matrix. Using existing results, we show this error can be understood in terms of the spectrum of an infinite-dimensional integral operator. In the case of stationary kernels, our main result proves that priors with smoother sample functions, and datasets with more concentrated inputs admit sparser approximations.", "publication_ref": ["b24", "b21", "b22", "b28", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Main results", "text": "We assume that training inputs are drawn i.i.d. from a fixed distribution, and prove bounds of the form\nKL Q P \u2264 O g(M, N ) \u03c3 2 n \u03b4 1 + c y 2 2 \u03c3 2 n + N\nwith probability at least 1 \u2212 \u03b4, whereP is the posterior Gaussian process, Q is a variational approximation, and y are the training targets. The function g(M, N ) depends on both the kernel and input distribution, and grows linearly in N and generally decays rapidly in M . The quality of the initialization determines , which can be made arbitrarily small (e.g. an inverse power of N ) at some additional com-arXiv:1903.03571v3 [stat.ML] 3 Sep 2019 putational cost. Theorems 1 and 2 give results of this form for a collection of inducing variables defined using spectral information, theorems 3 and 4 hold for inducing points.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Background and notation", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Gaussian process regression", "text": "We consider Gaussian process regression, where we observe training data, D = {x i , y i } N i=1 with x i \u2208 X and y i \u2208 R. Our goal is to predict outputs y * for new inputs x * while taking into account the uncertainty we have about f (\u2022) due to the limited size of the training set. We follow a Bayesian approach by placing a prior over f , and a likelihood to relate f to the observed data through some noise. Our model is\nf \u223c GP(\u03bd(\u2022), k(\u2022, \u2022)), y i = f (x i ) + i , i \u223c N (0, \u03c3 2 n ),\nwhere \u03bd : X \u2192 R is the mean function and k : X \u00d7 X \u2192 R is the covariance function. We take \u03bd \u2261 0; the general case can be derived similarly after first centering the process. We use the posterior for making predictions, and the marginal likelihood for selecting hyperparameters, both of which have closed-form expressions [Rasmussen & Williams, 2006]. The log marginal likelihood is of particular interest to us, as the quality of its approximation and our posterior approximation is linked. Its form is\nL = \u2212 1 2 y T K \u22121 n y \u2212 1 2 log|K n | \u2212 N 2 log(2\u03c0) , (1\n)\nwhere K n = K ff + \u03c3 2 n I, and [K ff ] i,j = k(x i , x j ).", "publication_ref": ["b24"], "figure_ref": [], "table_ref": []}, {"heading": "Sparse variational Gaussian process regression", "text": "While all quantities of interest have analytic expressions, their computation is infeasible for large datasets due to the O N 3 time complexity of the determinant and inverse. Many approaches have been proposed that rely on a low-rank approximation to K ff [Qui\u00f1onero Candela & Rasmussen, 2005;Rahimi & Recht, 2008], which allow the determinant and inverse to be computed in O N M 2 , where M is the rank of the approximating matrix.\nWe consider the variational framework developed by Titsias [2009], which minimizes the KL divergence from the posterior process to an approximate GP\nGP k \u2022u K \u22121 uu \u00b5, k \u2022\u2022 + k \u2022u K \u22121 uu (\u03a3 \u2212 K uu )K \u22121 uu k u\u2022 , (2) where [k u\u2022 ] i = k(\u2022, z i ), [K uf ] m,i := k(z m , x i ) and [K uu ] m,n := k(z m , z n )\n. This variational distribution is determined through defining the density of the function values u \u2208 R M at inducing inputs Z = {z m } M m=1 to be q(u) = N (\u00b5, \u03a3). Z, \u00b5, and \u03a3 are variational parameters. Titsias [2009] solved the convex optimization problem for \u00b5 and \u03a3 explicitly, resulting in the evidence lower bound (ELBO): Hensman et al. [2013] proposed optimising over {\u00b5, \u03a3} numerically, which allows minibatch optimization. In both cases, L = L lower + KL(Q||P ) [Matthews et al., 2016], so any bounds on the KL-divergence we prove hold for Hensman et al. [2013] as well, as long as {\u00b5, \u03a3} is at the optimum value. Titsias [2009] suggests jointly maximizing the ELBO (eq. 3) w.r.t. the variational and hyperparameters. This comes at the cost of introducing bias in hyperparameter estimation [Turner & Sahani, 2011], notably the overestimation of the \u03c3 2 n [Bauer et al., 2016]. Adding inducing points reduces the KL gap [Titsias, 2009], and the bias is practically eliminated when enough inducing variables are used. -Gredilla & Figueiras-Vidal [2009] showed that one can specify the distribution q(u), on integral transformations of f (\u2022). Using these interdomain inducing variables can lead to sparser representations, or computational benefits [Hensman et al., 2018]. Interdomain inducing variables are defined by\nL lower = \u2212 1 2 y T Q \u22121 n y\u2212 1 2 log|Q n |\u2212 N 2 log(2\u03c0)\u2212 t 2\u03c3 2 n (3) where Q n = Q ff + \u03c3 2 n I, Q ff = K T uf K \u22121 uu K uf and t = Tr(K ff \u2212 Q ff ).", "publication_ref": ["b22", "b23", "b28", "b9", "b19", "b9", "b28", "b30", "b2", "b28", "b17", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Interdomain inducing features", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "L\u00e1zaro", "text": "u m = X f (x)g(x; z m )dx . When g(x; z m ) = \u03b4(x \u2212 z m ) the u m are inducing points.\nInterdomain features require replacing k u\u2022 and K uu in eq. 2 with integral transforms of the kernel. In later sections, we investigate particular interdomain transformations with interesting convergence properties.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Upper bounds on the marginal likelihood", "text": "Combined with eq. 3, an upper bound on eq. 1 can show when the KL divergence is small, which indicates inference has been successful and hyperparameter estimates are likely to have little bias. Titsias [2014] introduced an upper bound that can be computed in O N M 2 :\nL upper := \u2212 1 2 y T (Q n +tI) \u22121 y\u2212 1 2 log(|Q n |)\u2212 N 2 log 2\u03c0. (4)\nThis gives a data-dependent upper bound, that can be computed after seeing the data, and for given inducing inputs.", "publication_ref": ["b29"], "figure_ref": [], "table_ref": []}, {"heading": "Spectral properties of the covariance matrix", "text": "While for small datasets spectral properties of the covariance matrix can be analyzed numerically, we need a different approach for understanding these properties for a typical large dataset. The covariance operator, K, captures the limiting properties of K ff for large N . It is defined by\nKg(x ) = X g(x)k(x, x )p(x)dx,(5)\nwhere p(x) is a probability density from which the inputs are assumed to be drawn. We assume that K is compact, which is the case if k(x, x ) is bounded. Under this assumption, the spectral theorem tells us that K has a discrete spectrum. The (finite) sequence of eigenvalues of 1 N K ff converges to the (infinite) sequence of eigenvalues of K [Koltchinskii & Gin\u00e9, 2000]. Mercer's Theorem [Mercer, 1909] tells us that for continuous kernel functions,\nk(x, x ) = \u221e m=1 \u03bb m \u03c6 m (x)\u03c6 m (x ),(6)\nwhere the (\u03bb m , \u03c6 m ) \u221e i=1 are eigenvalue-eigenfunction pairs of the operator K, with the eigenfunctions orthonormal in\nL 2 (X ) p . Additionally, \u221e m=1 \u03bb m < \u221e.", "publication_ref": ["b14", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Selecting the number of inducing variables", "text": "Ideally, the number of inducing variables should be selected to make the KL(Q||P ) small. Currently, the most common advice is to increase the number of inducing variables M until the lower bound (eq. 3) no longer improves. This is a necessary, but not a sufficient condition for the ELBO to be tight and the KL to be small. Taking the upper bound (eq. 4) into consideration, we can guarantee a good approximation when difference between the upper and lower bounds converges to zero, as this upper bounds the KL.\nBoth these procedures rely on bounds computed for a given dataset, and a specific setting of variational parameters. While practically useful, they do not tell us how many inducing variables we should expect to use before observing any data. In this work, we focus on a priori bounds, and asymptotic behavior as N \u2192 \u221e and M grows as a function of N . These bounds guarantee how the variational method scales computationally for any dataset satisfying intuitive conditions. This is particularly important for continual learning scenarios, where we incrementally observe more data. With our a priori results we can guarantee that the growth in required computation will not exceed a certain rate.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Bounds on the KL divergence for eigenfunction inducing features", "text": "In this section, we prove a priori bounds on the KL divergence using inducing features that rely on spectral information about the covariance matrix or the associated operator. The results in this section form the basis for bounds on the KL divergence for inducing points (section 4). n is a lower bound for the expected value over the KL divergence when y is generated according to our prior model.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Posteriori Bounds on the KL Divergence", "text": "We first consider a posteriori bounds on the KL divergence that hold for any y, derived by looking at the difference between L upper and L lower . We will use these bounds in later sections to analyze asymptotic convergence properties.\nLemma 1. Let K ff = K ff \u2212 Q ff , t = Tr( K ff ) and \u03bb max denote the largest eigenvalue of K ff . Then,\nKL Q P \u2264 1 2\u03c3 2 n t+ \u03bb max y 2 2 \u03c3 2 n + \u03bb max \u2264 t 2\u03c3 2 n 1+ y 2 2 \u03c3 2 n +t .\nThe proof bounds the difference between a refinement of L upper also proven by Titsias [2014] and L lower through an algebraic manipulation and is given in appendix A. The second inequality is a consequence of t \u2265 \u03bb max . We typically expect y 2 2 = O(N ), which is the case when the variance of the observed ys is bounded, so if t 1/N the KL divergence will be small.", "publication_ref": ["b29"], "figure_ref": [], "table_ref": []}, {"heading": "A priori bounds: averaging over y", "text": "Lemma 1 is typically overly pessimistic, as it assumes y can be parallel to the largest eigenvector of K ff . In this section, we consider a bound that holds a priori over the training outputs, when they are drawn from the model. This allows us to bound the KL divergence for a 'typical' dataset.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Lemma 2. For any set of {x", "text": "i } N i=1 , if the outputs {y i } N i=1\nare generated according to our generative model, then\nt 2\u03c3 2 n \u2264 E y KL Q P \u2264 t \u03c3 2 n (7)\nThe lower bound tells us that even if the training data is contained in an interval of fixed length, we need to use more inducing points for problems with large N if we want to ensure the sparse approximation has converged. This is shown in Figure 1 for data uniformly sampled on the interval [0, 5] with 15 inducing points.\nSketch of Proof.\nE y KL Q P = t 2\u03c3 2 n + N (y; 0, K n ) \u00d7 log N (y; 0, K n ) N (y; 0, Q n ) dy\nThe second term on the right is a KL divergence between centered Gaussian distributions. The lower bound follows from Jensen's inequality. The proof of the upper bound (appendix B) bounds this KL divergence above by t/(2\u03c3 2 n ).", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Minimizing the upper bound: an idealized case", "text": "We now consider the set of M interdomain inducing features that minimize the upper bounds in Lemmas 1 and 2.\nTaking into account the lower bound in Lemma 2, they must be within a factor of two of the optimal features defined without reference to training outputs under the assumption of Lemma 2. Consider\nu m := N i=1 w (m) i f (x i ) where w (m) i\nis the i th entry in the m th eigenvector of K ff . That is, u m is a linear combination of inducing points placed at each data point, with weights coming from the entries of the m th eigenvector of K ff . We show in appendix C that\ncov(u m , u k ) = w (m)T K ff w (k) = \u03bb k (K ff )\u03b4 m,k ,(8)\ncov(u m , f (x i )) = K ff w (m) i = \u03bb m (K ff )w (m) i .(9)\nInference with these features can be seen as the variational equivalent of the optimal parametric projection of the model derived by Ferrari-Trecate et al. [1999].\nComputation with these features requires computing the matrices K uf and K uu . K uu contains the first M eigenvalues of K ff , K uf contains the corresponding eigenvectors. Computing the first M eigenvalues and vectors (i.e. performing atruncated SVD of K ff ) can be done in O(N 2 M ) using, for example, Lanczos iteration [Lanczos, 1950]. With these features Q ff is the optimal rank-M approximation to K ff and leads to\u03bb max = \u03bb M +1 (K ff ) and t = N m=M +1 \u03bb m (K ff ).", "publication_ref": ["b5", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Eigenfunction inducing features", "text": "We now modify the construction given in section 3.3 to no longer depend on K ff explicitly (which depends on the specific training inputs) and instead depend on assumptions about the training data. This construction is the a priori counterpart of the eigenvector inducing features, as it is defined prior to observing a specific set of training inputs.\nConsider the limit as we have observed a large amount of data, so that 1 N K ff \u2192 K. This leads us to replace the eigenvalues, {\u03bb m (K ff )} M m=1 , with the operator eigenvalues, {\u03bb m } M m=1 , and the eigenvectors,\n{w (m) } M m=1 , with the eigenfunctions, {\u03c6 m } M m=1 , yielding u m = X f (x)\u03c6 m (x)p(x)dx. (10\n)\nNote that p(x) influences u m . In appendix C, we show\ncov(u m , u k ) = \u03bb m \u03b4 m,k and cov(u m , f (x i )) = \u03bb m \u03c6 m (x i ).\nThese features can be seen as the variational equivalent of methods utilizing truncated priors proposed in Zhu et al. [1997], which are the optimal linear M dimensional parametric GP approximation defined a priori, in terms of minimizing expected mean square error.\nIn the case of the SE kernel and Gaussian inputs, closed form expressions for eigenfunctions and values are known [Zhu et al., 1997]. For Mat\u00e9rn kernels with inputs uniform on [a, b], expressions for the eigenfunctions and eigenvalues needed in order to compute K uf and K uu can be found in Youla [1957]. However, the formulas involve solving systems of transcendental equations limiting the practical applicability of these features for Mat\u00e9rn kernels.", "publication_ref": ["b35", "b35", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "A priori bounds on the KL divergence for eigenfunction features", "text": "Having developed the necessary preliminary results, we now prove the first a priori bounds on the KL divergence. We start with eigenfunction features, which can be implemented practically in certain instances discussed above.\nTheorem 1. Suppose N training inputs are drawn i.i.d. according to input density p(x). For inference with M eigenfunction inducing variables defined with respect to the prior kernel and p(x), with probability at least 1 \u2212 \u03b4,\nKL Q P \u2264 C 2\u03c3 2 n \u03b4 1 + y 2 2 \u03c3 2 n (11\n)\nwhere we have defined C = N \u221e m=M +1 \u03bb m , and the \u03bb m are the eigenvalues of the integral operator K associated to the prior kernel and p(x).\nTheorem 2. With the assumptions and notation of Theorem 1 if y is distributed according to a sample from the prior generative model, with probability at least 1 \u2212 \u03b4,\nKL Q P \u2264 C \u03b4\u03c3 2 n ,(12)\nSketch of Proof of Theorems 1 and 2. We first prove a bound on t that holds in expectation over input data matrices of size N with entries drawn i.i.d. from p(x).\nA direct computation of Q ff shows that [Q ff ] i,j = M m=1 \u03bb m \u03c6 m (x i )\u03c6 m (x j ).\nUsing the Mercer expansion of the kernel matrix and subtracting,\nK ff i,i = \u221e m=M +1 \u03bb m \u03c6 2 m (x i ).\nSumming this and taking the expectation,\nE X [t] = N \u221e m=M +1 \u03bb m E xi \u03c6 2 m (x i ) = N \u221e m=M +1 \u03bb m .\nThe second equality follows from the eigenfunctions having norm 1. Applying Markov's inequality and Lemmas 1 and 2 yields Theorems 1 and 2 respectively.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Squared exponential kernel and Gaussian inputs", "text": "For the SE kernel in one-dimension with hyperparameters (v, 2 ) and p(x) \u223c N (0, \u03c3 2 ), Zhu et al., 1997]. In this case, using the geometric series formula,\n\u03bb m = v 2a/AB m\u22121 where a = 1/(4\u03c3 2 ), b = 1/(2 2 ), c = \u221a a 2 + 2ab, A = a + b + c and B = b/A [\n\u221e m=M +1 \u03bb m = v \u221a 2a (1 \u2212 B) \u221a A B M .\nUsing this bound with Theorems 1 and 2, we see that by choosing M = O(log N ), under the assumptions of either theorem, we can obtain a bound on the KL divergence that tends to 0 as N tends to infinity.", "publication_ref": ["b35"], "figure_ref": [], "table_ref": []}, {"heading": "Mat\u00e9rn kernels and uniform measure", "text": "For Ritter et al., 1995;Seeger et al., 2008], so\nthe Mat\u00e9rn k + 1/2 kernel in one dimension, \u03bb m m \u22122k\u22122 [\n\u221e m=M +1 \u03bb m = O(M \u22122k\u22121 ).\nIn order for the bound in Theorem 2 to converge to 0, we need lim\nN \u2192\u221e N M 2k+1 \u2192 0. This holds if M = N \u03b1 for \u03b1 > 1 2k+1 .\nFor k > 0, this bound indicates the number of inducing features can grow sublinearly with the amount of data.", "publication_ref": ["b25", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "Bounds for inducing points", "text": "We have shown that using spectral knowledge of either K ff or K we obtain bounds on the KL divergence indicating that the number of inducing features can be much smaller than the number of data points. While mathematically convenient, the practical applicability of the interdomain features used is limited by computational considerations in the case of the eigenvector features and by the lack of analytic expressions for K uf in most cases for the eigenfunction features, as well not knowing the true input density, p(x).\nIn contrast, inducing points can be efficiently applied to any kernel. In this section, we show that with a good initialization based on the empirical input data distribution, inducing points lead to bounds that are only slightly weaker than the interdomain approaches suggested so far.\nProving this amounts to obtaining bounds on the trace of the error of a Nystr\u00f6m approximation to K ff . The Nystr\u00f6m approximation, popularized for kernel methods by Williams & Seeger [2001], approximates a positive semi-definite symmetric matrix by subsampling columns. If M columns,\n{c i } M i=1 , are selected from K ff , the approximation used is K ff \u2248 CC \u22121 C T , where C = [c 1 , c 2 , . . . , c M ] and C is the M \u00d7 M principal submatrix associated to the {c i } M i=1 .\nNote that if inducing points are placed at the points associated to each column in the data matrix, then K uu = C and\nK T uf = C, so Q ff = CC \u22121 C T .\nLemma 3. [Belabbas & Wolfe, 2009] Given a symmetric positive semidefinite matrix, K ff , if M columns are selected to form a Nystr\u00f6m approximation such that the probability of selecting a subset of columns, Z, is proportional to the determinant of the principal submatrix formed by these columns and the matching rows, then,\nE Z [Tr(K ff \u2212 Q ff )] \u2264 (M + 1) N m=M +1 \u03bb m (K ff ). (13\n)\nThis means that on average well-initialized inducing points lead to bounds within a factor of M + 1 of eigenvector inducing features.\nThe selection scheme described introduces negative correlations between inducing points locations, leading the z i to be well-dispersed amongst the training data, as shown in fig. 2. The strength of these negative correlations is determined by the particular kernel.\nThe proposed initialization scheme is equivalent to sampling Z according to a discrete k-Determinantal Point Process (k-DPP), defined over K ff . Belabbas & Wolfe [2009] suggested that sampling from this distribution, which has support over N M subsets of columns, may be computationally infeasible. Kulesza & Taskar [2011] provided an exact algorithm for sampling from k-DPPs given an eigendecomposition of the kernel matrix. In our setting, we require our initialisation scheme to have similar computational cost to computing the sparse GP bounds, which prohibits us from performing eigendecomposition. Instead, we rely on cheaper \" close\" sampling methods. We therefore provide the following corollary of lemma 3, proven in appendix D.\nCorollary 1. Suppose the inducing points, Z, are sampled from an k-DPP, \u03bd, i.e a distribution over subsets of X of size M satisfying, d(\u00b5, \u03bd) T V \u2264 where d(\u2022, \u2022) T V denotes total variation distance and \u00b5 is a k-DPP on K ff . Suppose the k(x, x) < v for all x \u2208 X . Then\nE Z\u223c\u03bd [t] \u2264 (M + 1) N m=M +1 \u03bb m (K ff ) + 2N v . (14)", "publication_ref": ["b33", "b3", "b3", "b15"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "A priori bounds for inducing points", "text": "We show analogues of theorems 1 and 2 for inducing points. \nKL Q P \u2264 C(M + 1) + 2N v 2\u03c3 2 n \u03b4 1 + y 2 2 \u03c3 2 n (15\n)\nwhere C = N \u221e m=M +1 \u03bb m , and \u03bb m are the eigenvalues of the integral operator K associated to kernel, k, and p(x).\nTheorem 4. With the assumptions and notation of theorem 3 and if y is distributed according to a sample from the prior generative model, with probability at least 1 \u2212 \u03b4,\nKL Q P \u2264 C(M + 1) + 2N v \u03b4\u03c3 2 n .(16)\nProof. We prove theorem 4. Theorem 3 follows the same argument, replacing the expectation over y with the bound given by lemma 1.\nE X E Z|X E y KL Q P \u2264 \u03c3 \u22122 n E X E Z|X [t] \u2264 \u03c3 \u22122 n (M + 1)E X N m=M +1 \u03bb m (K ff ) + 2N v \u2264 \u03c3 \u22122 n (M + 1)N \u221e m=M +1 \u03bb m + 2N v .\nThe first two inequalities use lemma 2 and corollary 1. The third follows from noting that the sum inside the expectation is the error in trace norm of the optimal rank M approximation to the covariance matrix for any given X, and is bounded above by the error from the rank M approximation due to eigenfunction features. We showed that this error is in expectation equal to N \u221e m=M +1 \u03bb m so this must be an upper bound on the expectation in the second to last line. 1 We apply Markov's inequality, yielding for any \u03b4 \u2208 (0, 1) with probability at least 1 \u2212 \u03b4,\nKL Q P \u2264 (M + 1)N \u221e m=M +1 \u03bb m + 2N V \u03b4\u03c3 2 n .\nFigure 3 compares the actual KL divergence, the a posteriori bound derived by L upper \u2212 L lower , and the bounds proven in theorems 3 and 4 on a dataset with normally distributed training inputs and y drawn from the generative model.", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Consequences of theorem 3 and theorem 4", "text": "We now investigate implications of our main results for sparse GP regression. Our first two corollaries consider Gaussian inputs and the squared exponential (SE) kernel, and show that in D dimensions, choosing M = O(log D (N )) is sufficient in order for the KL divergence to converge with high probability. We then briefly summarize convergence rates for other stationary kernels. Finally we point out consequences of our definition of convergence for the quality of the pointwise posterior mean and uncertainty.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Comparison of consequences of theorems", "text": "Using the explicit formula for the eigenvalues given in section 3.6, we arrive at the following corollary:\nCorollary 2. Suppose that y 2 2 \u2264 RN. Fix \u03b3 > 0, and take = \u03b4\u03c3 2 n vN \u03b3+2 . Assume the input data is normally distributed and regression in performed with a SE kernel. Under the assumptions of theorem 3, with probability 1 \u2212 \u03b4, when inference is performed with\nKL Q P \u2264 N \u2212\u03b3 2R/\u03c3 2 n + 2/N .(17)\nM = (3+\u03b3) log(N )+log D log(B \u22121 ) , where D = v \u221a 2a 2 \u221a A\u03c3 2 n \u03b4(1\u2212B)\n. The proof is given in appendix E. If the lengthscale is much shorter than the standard deviation of the data then B will be near 1, implying that M will need to be large in order for the bound to converge. Remark 1. The assumption y 2 2 \u2264 RN for some R is very weak. For example, if y is a realization of an integrable function with constant noise,\nN i=1 y 2 i \u2264 N i=1 f (x i ) 2 + N i=1 2 i + o(N )\nThe first sum is asymptotically N f (x)p(x)dx, and the second is asymptotically N \u03c3 2 n .\nThe consequence of corollary 2 is shown in fig. 4, in which we gradually increase N, choosing M = C log(N ) + C 0 , and see the KL divergence converges as an inverse power of N. The training outputs are generated from a sample from the prior generative model. Note that as theorem 4 assumes y is sampled from the prior and is not derived using the upper bound (eq. 4); it may be tighter than the a posteriori bound in cases when this upper bound is not tight.\nFor the SE kernel and Gaussian inputs, the rate that we prove M must increase for inducing points and eigenfunction features differs by a constant factor. For the Mat\u00e9rn k + 1/2 kernel in one dimension, we need to choose M = N \u03b1 with \u03b1 > 1/(2k) instead of \u03b1 > 1/(2k + 1). This difference is particularly stark in the case of the Mat\u00e9rn 3/2 kernel, for which our bounds tell us that inference with inducing points requires \u03b1 > 1/2 as opposed to \u03b1 > 1/3 for the eigenfunction features. Whether this is an artifact of the proof, the initialization scheme, or an inherent limitation for inducing points is an interesting area for future work.", "publication_ref": [], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "Multidimensional data, effect of input density and other kernels", "text": "If X = R D , it is common to choose a separable kernel, i.e a kernel that can be written as a product of kernels along each dimension. If this choice is made, and input densities factor over the dimensions, the eigenvalues of K are the product of the eigenvalues along each dimension. In the case of the SE-ARD kernel and Gaussian input distribution, we obtain an analogous statement to corollary 2 in D-dimensions.\nCorollary 3. For any fixed , \u03b4 > 0 under the assumptions of corollary 2, with a SE-ARD kernel in D dimensions, p(x) a multivariate Gaussian, M = O(log D N ) inducing points and = O(N \u2212\u03b3 ) for some fixed \u03b3 > 2, with probability at\nleast 1 \u2212 \u03b4, KL Q P \u2264 \u03b4 \u22121 .\nThe proof uses ideas from Seeger et al. [2008] and is given in appendix E. While for the SE kernel and Gaussian input density M can grow polylogarithmically in N, and the KL divergence still converges, this is not the case for regression with other kernels or input distribution.\nClosed form expressions for the eigenvalues of operators associated to many kernels and input distributions are not known. For stationary kernels and compactly supported input distributions, the asymptotic rate of decay of the eigenvalues of K is well-understood [Widom, 1963;1964;Ritter et al., 1995]. The intuitive summary of these results is that smooth kernels, with concentrated input distributions have rapidly decaying eigenvalues. In contrast, kernels such as the Mat\u00e9rn-1/2 that define processes that are not smooth have slowly decaying eigenvalues. For Lebesgue measure on [a, b] the Sacks-Ylivasker conditions of order r (appendix F), which can be roughly thought of as meaning that realizations of the process are r times differentiable with probability 1 [Ritter et al., 1995], implies an eigendecay of \u03bb m m \u22122r\u22122 . Table 1 summarizes the spectral decay of several stationary kernels, as well as the implications for the number of inducing points needed for inference to provably converge with our bounds.", "publication_ref": ["b26", "b31", "b32", "b25", "b25"], "figure_ref": [], "table_ref": []}, {"heading": "Computational complexity", "text": "We now have all the components necessary for analyzing the overall computational complexity of finding an arbitrarily good GP approximation. To understand the Table 1. The number of features needed for our bounds to converge in D\u2212dimensions. These hold for some fixed \u03b1 > 0 and any > 0.\nKERNEL INPUT DISTRIBUTION DECAY OF \u03bbm M, THEOREM 3 M, THEOREM 4 SE-KERNEL COMPACT SUPPORT O exp(\u2212\u03b1 m d log m d ) O(log D (N )) O(log D (N )) SE-KERNEL GAUSSIAN O exp(\u2212\u03b1 m d ) O(log D (N )) O(log D (N )) MAT\u00c9RN k+1/2 2 UNIFORM ON INTERVAL O M \u22122k\u22122 log(M ) 2(d\u22121)(k+1) O N 1/k+ O N 1/(2k)+\nfull computational complexity, we must consider the cost of initializing the inducing points using an exact or approximate k-DPP, as well as the O(N M 2 ) time complexity of variational inference. Recent work of Derezi\u01f9ski et al. [2019] indicate that an exact algorithm for sampling a k-DPP can be implemented in O(N log(N )poly(M )).\nWe base our method on Anari et al. [2016], who show that an k-DPP can be sampled via MCMC methods in O(N M 4 log(N ) + N M 3 log( 1)) time with memory O(N + M 2 ) (see appendix D). 3 We can choose to be any inverse power of N which only adds a constant factor to the complexity. For the SE kernel, taking M = O(log D N ) inducing points leads to a complexity of O(N log 4D+1 N ), a large computational saving compared to the O(N 3 ) cost of exact inference. For the Mat\u00e9rn k + 1 2 kernel in onedimension and the average case analysis of theorem 4 we need to take M = O(N 1/(2k)+ ) implying a computational complexity of O(N 1+2/k+4 log(N )) which is an improvement over the cost of full inference for k > 1. Improvements in methods for sampling exact or approximate k-DPPs (e.g. recent bounds on mixing time [Hermon & Salez, 2019]) or bounds on other selection schemes for Nystr\u00f6m approximations directly translate to improved bounds on the computational cost of convergent sparse Gaussian process approximations through this framework.", "publication_ref": ["b4", "b1", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Pointwise approximate posterior", "text": "In many applications, pointwise estimates of the posterior mean and variance are of interest. It is therefore desirable that the approximate variational posterior gives similar estimates of these quantities as the true posterior. Huggins et al. [2019] derived an approximation method for sparse GP inference with provable guarantees about pointwise mean and variance estimates of the posterior process and showed that approximations with moderate KL divergences can still have large deviations in mean and variance estimates. However, if the KL divergence converges to zero, estimates of mean and variance converge to the posterior values. By the 2 The kernel discussed here is the product of 1-dimensional Mat\u00e9rn kernels with the same smoothness parameter, which differs from the standard definition if D > 1. Bounds on the eigenvalues of the standard multidimensional Mat\u00e9rn kernel appear in Seeger et al. [2008].\n3 Open source implementations of approximate k-DPPs are available (e.g. [Gautier et al., 2018]). chain rule of KL divergence [Matthews et al., 2016],\nKL(\u00b5 X \u03bd X ) = KL(\u00b5 x * \u03bd x * ) + E \u00b5x * KL \u00b5 X \\x * |x * \u03bd X \\x * |x * \u2265 KL(\u00b5 x * \u03bd x * ).\nTherefore, bounds on the mean and variance of a onedimensional Gaussian with a small KL divergence imply pointwise guarantees about posterior inference when the KL divergence between processes is small. Proposition 1. Suppose q and p are one dimensional Gaussian distributions with means \u00b5 1 and \u00b5 2 and variances \u03c3 1 and \u03c3 2 , such that 2KL(q p) = \u03b5 \u2264 1 5 , then\n|\u00b5 1 \u2212 \u00b5 2 | \u2264 \u03c3 2 \u221a \u03b5 \u2264 \u03c3 1 \u221a \u03b5 1 \u2212 \u221a 3\u03b5 and |1 \u2212 \u03c3 2 1 /\u03c3 2 2 | < \u221a 3\u03b5.\nThe proof is in appendix B. If \u2192 0, proposition 1 implies \u00b5 1 \u2192 \u00b5 2 and \u03c3 1 \u2192 \u03c3 2 . Using this and theorems 3 and 4, the posterior mean and variance converge pointwise to those of the full model using M N inducing features.", "publication_ref": ["b13", "b26", "b6", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Related work", "text": "Statistical guarantees for convergence of parametric GP approximations [Zhu et al., 1997;Ferrari-Trecate et al., 1999], lead to similar conclusions about the choice of approximating rank. Ferrari-Trecate et al. [1999] showed that given N data points, using a rank M truncated SVD of the prior covariance matrix, such that \u03bb M \u03c3 2 n /N results in almost no change in the model, in terms of expected mean squared error. Our results can be considered the equivalent for variational inference, showing that theoretical guarantees can be established for non-parametric approximate inference.\nGuarantees on the quality Nystr\u00f6m approximations have been used to bound the error of approximate kernel methods, notably for kernel ridge regression [Alaoui & Mahoney, 2015;Li et al., 2016]. The specific method for selecting columns in the Nystr\u00f6m approximation plays a large role in these analyses. Li et al. [2016] use an approximate k-DPP, nearly identical to the initialization we analyze; Alaoui & Mahoney [2015] sample columns according to ridge leverage scores. The substantial literature on bounds for Nystr\u00f6m approximations [e.g. Gittens & Mahoney, 2013] motivates considering other initialization schemes for inducing points in the context of Gaussian process regression.", "publication_ref": ["b35", "b5", "b0", "b18", "b18", "b0", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We proved bounds on the KL divergence between the variational approximation of sparse GP regression to the posterior, that depend only on the decay of the eigenvalues of the covariance operator. These bounds prove the intuitive result that smooth kernels with training data concentrated in a small region admit high quality, very sparse approximations. These bounds prove that truly sparse non-parametric inference, with M N, can provide reliable estimates of the marginal likelihood and pointwise posterior.\nExtensions to models with non-conjugate likelihoods, especially within the framework of Hensman et al. [2015], pose a promising direction for future research.\nA. Proof Of Lemma 1 Titsias [2014] proves the tighter upper bound,\nL \u2264 L upper := \u2212 N 2 log(2\u03c0) \u2212 1 2 log(|Q n |) \u2212 1 2 y T Q n + \u03bb max I \u22121 y.\nSubtracting,\nL upper \u2212 L lower = t 2\u03c3 2 n + 1 2 y T Q \u22121 n \u2212 (Q n + \u03bb max I) \u22121 y . (18)\nSince Q ff is symmetric positive semidefinite, Q n is positive definite with eigenvalues bounded below by \u03c3 2 n . Write, Q n = U\u039bU T , where U is unitary and \u039b is a diagonal matrix with non-increasing diagonal entries \u03b3 1 \u2265 \u03b3 2 \u2265 . . . \u2265 \u03b3 N \u2265 \u03c3 2 n . We can rewrite the second term (ignoring the factor of one half) in Equation 18as,\n(U T y) T \u039b \u22121 \u2212 (\u039b + \u03bb max I) \u22121 (U T y). Define, z = (U T y). Since U is unitary, z = y . (U T y) T \u039b \u22121 \u2212 (\u039b + tI) \u22121 (U T y) = z T \u039b \u22121 \u2212 (\u039b + \u03bb max I) \u22121 z = i z 2 i \u03bb max \u03b3 2 i + \u03b3 i \u03bb max \u2264 y 2 \u03bb max \u03b3 2 N + \u03b3 N \u03bb max .\nThe last inequality comes from noting that the fraction in the sum attains a maximum when \u03b3 i is minimized. Since \u03c3 2 n is a lower bound on the smallest eigenvalue of Q n , we have,\ny T Q \u22121 n \u2212 (Q n + \u03bb max I) \u22121 y \u2264 \u03bb max y 2 \u03c3 4 n + \u03c3 2 n \u03bb max .\nlemma 1 follows.", "publication_ref": ["b10", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "B. KL Divergence Gaussian Distributions", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.1. KL divergence between multivariate Gaussian distributions", "text": "We make use of the formula for KL divergences between multivariate Gaussian distributions in our proof of lemma 2, and the univariate case in proposition 1.\nRecall the KL divergence from p 1 \u223c N (m 1 , S 1 ) to p 2 \u223c N (m 2 , S 2 ) both of dimension N is given by\nKL(p 1 p 2 ) = 1 2 Tr S 2 \u22121 S 1 + log |S 2 | |S 1 | +(m 1 \u2212 m 2 ) T S 2 \u22121 (m 1 \u2212 m 2 ) \u2212 N \u2265 0. (19\n)\nThe inequality is a special case of Jensen's inequality.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.2. Proof of Upper Bound in lemma 2", "text": "In the main text we showed,\nE y KL Q P = t 2\u03c3 2 n + N (y; 0, K n ) \u00d7 log N (y; 0, K n ) N (y; 0, Q n ) dy\nIn order to complete the proof, we need to show that the second term on the right hand side is bounded above by t/(2\u03c3 2 n ). Using Equation 19:\nE y KL Q P = t 2\u03c3 2 n \u2212 N 2 + 1 2 log |Q n | |K n | + 1 2 Tr Q \u22121 n (K n ) \u2264 t 2\u03c3 2 n \u2212 N 2 + 1 2 Tr Q \u22121 n (Q n + K ff ) . (20\n)\nThe inequality follows from noting the log determinant term is negative, as K n Q n (i.e. K n \u2212 Q n is positive definite).\nSimplifying the last term,\n1 2 Tr(I) + 1 2 Tr Q \u22121 n K ff ) \u2264 N/2 + t\u03bb 1 Q \u22121 n /2 \u2264 N/2 + t/(2\u03c3 2 n ).\nThe first inequality uses that for positive semi-definite symmetric matrices Tr(AB) \u2264 Tr(A)\u03bb 1 (B) which is a special case of H\u00f6lder's inequality for Schatten norms. The final line uses that the largest eigenvalue of Q \u22121 n is bounded above by \u03c3 \u22122 n . Using this in Equation 20 finishes the proof.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.3. Proof of Proposition 1", "text": "Defining = 2KL(q p), = \u03c3 2 1 + (\u00b5 1 \u2212 \u00b5 2 ) 2 \u03c3 2 2 \u2212 log \u03c3 2 1 \u03c3 2 2 \u2212 1 (21) \u2265 x \u2212 log(x) \u2212 1\nwhere we have defined\nx = \u03c3 2 1 \u03c3 2 2 . Applying the lower bound x \u2212 log(x) \u2212 1 \u2265 (x \u2212 1) 2 /2 \u2212 (x \u2212 1) 3 /3, \u2265 (x \u2212 1) 2 /2 \u2212 (x \u2212 1) 3 /3.\nA bound on |x \u2212 1| that holds for all can then be found with the cubic formula. Under the assumption that < 1 5 , we have x \u2212 log(x) < 1.2 which implies x \u2208 [0.493, 1.77]. For x in this range, we have\nx \u2212 log(x) \u2212 1 \u2265 (x \u2212 1) 2 /3 So, |x \u2212 1| \u2264 \u221a 3\nThis proves that,\n1 \u2212 \u221a 3 < \u03c3 2 1 \u03c3 2 2 < 1 + \u221a 3 .\nFrom Equation 21and\nx \u2212 log x > 1, |\u00b5 1 \u2212 \u00b5 2 | \u2264 \u03c3 2 \u221a 3 .\nUsing our bound on the ratio of the variances completes the proof of proposition 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C. Covariances for Interdomain Features", "text": "We compute the covariances for eigenvector and eigenfunction inducing features.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1. Eigenvector inducing features", "text": "Recall we have defined eigenvector inducing features by,\nu m = N i=1 w (m) i f (x i ).\nThen,\ncov(u m , u k ) = E \uf8ee \uf8f0 N i=1 w (m) i f (x i ) N j=1 w (k) j f (x j ) \uf8f9 \uf8fb = N i=1 w (m) i N j=1 w (k) j E[f (x i )f (x j )] = N i=1 w (m) i N j=1 w (k) j k(x i , x j ).\nWe now recognize this expression as w (m)T K ff w (k) . Using the defining property of eigenvectors as well as orthonor-\nmality, cov(u m , u k ) = \u03bb k (K ff )\u03b4 m,k . Similarly, cov(u m , f (x i )) = E \uf8ee \uf8f0 N j=1 w (m) j f (x j )f (x i ) \uf8f9 \uf8fb = N j=1 w (m) j E[f (x j )f (x i )] = N j=1 w (m) j k(x j , x i ).\nThis is the i th entry of the matrix vector product K ff w (m) = \u03bb m (K ff )w (m) i .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.2. Eigenfunction inducing features", "text": "Recall we have defined eigenfunction inducing features by,\nu m = \u03c6 m (x)f (x)p(x)dx. Then, cov(u m , u k ) = E \u03c6 m (x)f (x)p(x)dx \u03c6 k (x )f (x )p(x )dx = \u03c6 m (x)p(x) \u03c6 k (x )E[f (x)f (x )]p(x )dx dx = \u03c6 m (x)p(x) \u03c6 k (x )k(x, x )p(x )dx dx.\nThe expectation and integration may be interchanged by Fubini's theorem, as both integrals converge absolutely since p(x) is a probability density, the \u03c6 m (x) are in L 2 (X ) p \u2282 L 1 (X ) p and k is bounded.\nWe may then apply the eigenfunction property to the inner integral and orthonormality of eigenfunctions to the result yielding,\ncov(u m , u k ) = \u03bb k \u03c6 k (x)\u03c6 m (x)p(x)dx = \u03bb k \u03b4 m,k . With similar considerations, cov(u m , f (x i )) = E \u03c6 m (x)f (x)f (x i )p(x)dx = \u03c6 m (x)E[f (x)f (x i )]p(x)dx = \u03bb m \u03c6 m (x i ). D. Discrete k-DPPs D.1. Proof of Corollary 1 from Lemma 3 E Z\u223c\u03bd [t] = E Z\u223c\u00b5 [t] + (E Z\u223c\u03bd [t] \u2212 E Z\u223c\u00b5 [t]) \u2264 (M + 1) N M +1 \u03bb(K ff ) + Z\u2208( N M ) (\u00b5(Z) \u2212 \u03bd(Z))t(Z) \u2264 (M + 1) N M +1 \u03bb(K ff ) + N v Z\u2208( N M ) |\u00b5(Z) \u2212 \u03bd(Z)| \u2264 (M + 1) N M +1 \u03bb(K ff ) + 2N v .\nThe first inequality follows from lemma 3. The second uses the triangle inequality replace t(Z) with a bound on its maximum. The final line uses one of the definitions of total variation distance for discrete random variables.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.2. Sampling Approximate k-DPPs", "text": "Belabbas & Wolfe [2009] proposed using the Metropolis method for approximate sampling from a k-DPP. Several recent works have shown that a natural Metropolis algorithm on k-DPPs mixes quickly. In particular, Anari et al. [2016] considers the following algorithm: Theorem 5 (Anari et al. [2016], Theorem 2). Let A denote algorithm 1. Let \u03bd R denote the distribution induced by R steps of A. Let R( ) denote the minimum R such that \u00b5 \u2212 \u03bd R T V < , where \u00b5 is a k-DPP on some kernel matrix K ff . Then\nR( ) \u2264 N M log N M M ! \u2264 N M 2 log N +N M log 1 .\nAlgorithm 1 MCMC algorithm for sampling k-DPP (A) Input: Training inputs X = {x i } N i=1 , number of points to choose, M , kernel k. Returns: A sample of M inducing points drawn proportional to the determinant of K Z,Z Initialize M columns greedily in an iterative fashion call this set S 0 . for r < R do Sample i uniformly from S r and j uniformly from X \\ S r . Define T = S \\ {i} \u222a {j}, Compute p i\u2192j := 1 2 min{1, det(K T )/det(K Sr )} With probability p i\u2192j S r+1 = T otherwise, S r+1 = S end for Return: S R Taking to be any fixed inverse power of N, (i.e. = N \u2212\u03b3 , will make the second term O(N M log(N )), while by taking \u03b3 large (e.g. greater than 2), we can make 2N v small. The total cost of the algorithm is determined by the cost of the greedy initialization, plus R( ) times the per iteration cost of the algorithm. A naive implementation of the greedy initialization requires O(N M 4 ) time and O(N M ) memory, simply by computing the determinant of each of the N \u2212 m possible ways to extend the current subset (faster implementations are possible, but this suffices for our purposes). We assume that this is implemented in such a way that at the end of the initialization we have access to det(K S0 ) and a Cholesky factorization S 0 = L 0 L 0 T .\nWe take as an inductive hypothesis that at iteration r of the algorithm, we know det(K Sr ) and a Cholesky factorization, K Sr = L r L T r . We then need to show we can compute a Cholesky factorization and determinant of K T in O(M 2 ). Given the Cholesky factorization of T, det(K T ) can be computed O(M ) as it is the product of the square of the diagonal elements. It therefore remains to consider the calculation of L T , a Cholesky factor of K T .\nThe computation of L T proceeds in two steps: first we compute L S\\i using L S .\nL S = \uf8ee \uf8f0 L 1,1 0 0 2,1 2,2 0 L 3,1 3,2 L 3,3 \uf8f9 \uf8fb ,K S = \uf8ee \uf8f0 K 1,1 k 2,1 K 1,3 k 2,1 k(x i , x i ) k 2,3 K 3,1 k 3,2 K 3,3 \uf8f9 \uf8fb A direct calculation shows, L S\\i = L 1,1 0 L 3,1 L 3,3\nwhere L 3,3 L T 3,3 = L 3,3 L T 3,3 + 3,2 T 3,2 . This is a rank oneupdate to a Cholesky factorization, and can be performed using standard methods in O(M 2 ).\nWe now need to extend a Cholesky factorization from S \\ i to T, which involves adding a row. From theorem 3, with probability 1 \u2212 \u03b4 and this choice of\nKL Q P \u2264 C(M + 1) 2\u03c3 2 n \u03b4 1 + y 2 2 \u03c3 2 n + N \u2212\u03b3 (R/\u03c3 2 n + 1/N )(22)\nTake M = (3+\u03b3) log(N )+log D log(B \u22121 )\n. If M \u2265 N the KLdivergence is zero and we are done. Otherwise, C(M +1) < N 2 \u221e i=M +1 \u03bb i . By the geometric series formula,\n\u221e i=M +1 \u03bb i = v \u221a 2a \u221a A B M 1 \u2212 B Now, B M = N \u22123\u2212\u03b3 D \u22121 , so \u221e i=M +1 \u03bb i = \u03b4N \u22123\u2212\u03b3 , implying C(M +1) 2\u03b4\u03c3 2 n < N \u22121\u2212\u03b3 .\nUsing this in eq. 22 completes the proof.", "publication_ref": ["b1", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "E.2. Corollary 3", "text": "It is sufficient to consider the case of isotropic kernels and input distributions. 4 From [Seeger et al., 2008] in the isotropic case (i.e. B i = B j =: B for all i, j \u2264 D),\n\u03bb s+D\u22121 \u2264 2a A D/2 B s 1/D . DefineM = M + D \u2212 1,\nto be the number of features used for inference. 4 For the general case, the eigenvalues can be bounded above by constant times the eigenvalues of an operator with an isotropic kernel with all lengthscales equal to the shortest kernel lengthscale and the input density standard deviation set to the largest standard deviation of any one-dimensional marginal of p(x).", "publication_ref": ["b26"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We would like to thank James Hensman for providing an excellent research environment, and the reviewers for their helpful feedback and suggestions. We would also particularly like to thank Guillaume Gautier, for pointing out an error in the exact k-DPP sampling algorithm cited in an earlier version of this work, and for guiding us through recent work on sampling k-DPPs.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "In the second line, we use that B < 1, so B s 1/D obtains its minimum on the interval s \u2208 [s , s +1] at the right endpoint (i.e. monotonicity). We now define \u03b1 = \u2212 log(B). So,\nIn the second line we made the substitution t = \u03b1s 1/D , so ds = \u03b1 \u2212D Dt D\u22121 . We now recognize,\nas an incomplete gamma function, \u0393(D, \u03b1M 1/D ). From Gradshteyn & Ryzhik [2014, 8.352],\nAs M grows as a function of N and D is fixed, for N large D \u2264 \u03b1M 1/D . This implies that that the largest term in the sum on the right hand side is the final term, so\nFor any fixed D, for this choice of M for any \u03b5 > 0 for N large,\nBy choosing \u03b3 > 3 + , for some fixed > 0 the proof is complete, using a similar argument as the one used in the proof of the previous corollary.\nNote that using the bound proven in Theorem 2 (the tightest of our bounds) the exponential scaling in dimension is unavoidable. If both k and p(x) are isotropic, then the eigenvalue\ntimes. This follows from noting that this is the number of ways to write m as a sum of D non-negative integers. Using an identity and standard lower bound for binomial coefficients\nfor some constant depending on D, C(D). Using Theorem 2 we need to choose M such that \u03bb M = O(1/N ). This means choosing K log(N ) in the sum above, leading to at least \u03b1 log D (N ) features being needed for some constant \u03b1. The constant in this lower bound decays rapidly with D, while the constant in the upper bound does not. Better understanding this gap is important for understanding the performance of sparse Gaussian process approximations in high dimensions.\nIf the data actually lies on a lower dimensional manifold, we conjecture the scaling depends mainly on the dimensionality of the manifold. In particular, if the manifold is linear and axis-aligned, then the kernel matrix only depends on distances along the manifold (not in the space it is embedded in) so the eigenvalues will not be effected by the higher dimensional embedding. We conjecture that similar properties are exhibited when the data manifold is nonlinear.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F. Smoothness and Sacks-Ylivasker Conditions", "text": "In many instances the precise eigenvalues of the covariance operator are not available, but the asymptotic properties are well understood. A notable example is when the data is distributed uniformly on the unit interval. If the kernel satisfies the Sacks-Ylivasker conditions of order r:\n\u2022 k(x, x ) is r-times continuously differentiable on [0, 1] 2 Moreover, k(x, x ) has continuous partial derivatives up to order r +2 times on (0, 1) 2 \u2229(x > x ) and (0, 1) 2 \u2229 (x < x ). These partial derivatives can be continuously extended to the closure of both regions.\n\u2022 Let L denote k (r,r) (x, x ), L + denote the restriction of L to the upper triangle and L \u2212 the restriction to the lower triangle, then L\n(1,0) +", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "< L", "text": "(1,0) \u2212 on the diagonal x = x .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "\u2022 L", "text": "(2,0) + (s, \u2022) is an element of the RKHS associated to L and has norm bounded independent of s.\nNotably, Mat\u00e9rn half integer kernels of order r + 1/2 meet the S-Y condition of order r. See Ritter et al. [1995] for a more detailed explanation of these conditions and extensions to the multivariate case.", "publication_ref": ["b25"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Fast Randomized Kernel Ridge Regression with Statistical Guarantees", "journal": "", "year": "2015", "authors": "A Alaoui; M W Mahoney"}, {"ref_id": "b1", "title": "Monte Carlo Markov Chain Algorithms for Sampling Strongly Rayleigh Distributions and Determinantal Point Processes", "journal": "", "year": "2016", "authors": "N Anari; S O Gharan; A Rezaei"}, {"ref_id": "b2", "title": "Understanding probabilistic sparse Gaussian process approximations", "journal": "", "year": "2016", "authors": "M Bauer; M Van Der Wilk; C E Rasmussen"}, {"ref_id": "b3", "title": "Spectral Methods in Machine Learning and new Strategies for very Large Datasets", "journal": "PNAS", "year": "2009", "authors": "M.-A Belabbas; P J Wolfe"}, {"ref_id": "b4", "title": "Exact sampling of determinantal point processes with sublinear time preprocessing", "journal": "", "year": "2019", "authors": "M Derezi\u01f9ski; D Calandriello; M Valko"}, {"ref_id": "b5", "title": "Finitedimensional Approximation of Gaussian Processes", "journal": "", "year": "1999", "authors": "G Ferrari-Trecate; C K Williams; M Opper"}, {"ref_id": "b6", "title": "Sampling Determinantal Point Processes with Python. ArXiv e-prints", "journal": "", "year": "2018", "authors": "G Gautier; R Bardenet; M Valko;  Dppy"}, {"ref_id": "b7", "title": "Revisiting the Nystr\u00f6m method for improved large-scale machine learning", "journal": "PMLR", "year": "2013-06", "authors": "A Gittens; M Mahoney"}, {"ref_id": "b8", "title": "Table of Integrals, Series, and Products", "journal": "Academic press", "year": "2014", "authors": "I S Gradshteyn; I M Ryzhik"}, {"ref_id": "b9", "title": "Gaussian processes for big data", "journal": "", "year": "2013", "authors": "J Hensman; N Fusi; N D Lawrence"}, {"ref_id": "b10", "title": "Scalable Variational Gaussian Process Classification", "journal": "", "year": "2015", "authors": "J Hensman; A Matthews; Z Ghahramani"}, {"ref_id": "b11", "title": "Variational Fourier Features for Gaussian Processes", "journal": "", "year": "2018", "authors": "J Hensman; N Durrande; A Solin"}, {"ref_id": "b12", "title": "Modified log-Sobolev inequalities for strong-Rayleigh measures", "journal": "", "year": "2019", "authors": "J Hermon; J Salez"}, {"ref_id": "b13", "title": "Scalable Gaussian Process Inference with Finite-data Mean and Variance Guarantees", "journal": "", "year": "2019", "authors": "J H Huggins; T Campbell; M Kasprzak; T Broderick"}, {"ref_id": "b14", "title": "Random Matrix Approximation of Spectra of Integral Operators", "journal": "", "year": "2000", "authors": "V Koltchinskii; E Gin\u00e9"}, {"ref_id": "b15", "title": "k-DPPs: Fixed-size Determinantal Point Processes", "journal": "", "year": "2011", "authors": "A Kulesza; B Taskar"}, {"ref_id": "b16", "title": "An Iteration Method for the Solution of the Eigenvalue Problem of Linear Differential and Integral Operators", "journal": "", "year": "1950", "authors": "C Lanczos"}, {"ref_id": "b17", "title": "Inter-domain Gaussian Processes for Sparse Inference using Inducing Features", "journal": "", "year": "2009", "authors": "M L\u00e1zaro-Gredilla; A Figueiras-Vidal"}, {"ref_id": "b18", "title": "Fast DPP Sampling for Nystrom with Application to Kernel Methods", "journal": "", "year": "2016", "authors": "C Li; S Jegelka; S Sra"}, {"ref_id": "b19", "title": "On Sparse Variational Methods and the Kullback-Leibler Divergence between Stochastic Processes", "journal": "", "year": "2016", "authors": "A G D G Matthews; J Hensman; R Turner; Z Ghahramani"}, {"ref_id": "b20", "title": "Functions of Positive and Negative Type, and their Connection the Theory of Integral Equations", "journal": "In Phil. Trans. R. Soc. Lond. A", "year": "1909", "authors": "J Mercer"}, {"ref_id": "b21", "title": "", "journal": "Bayesian Learning for Neural Networks", "year": "1996", "authors": "R M Neal"}, {"ref_id": "b22", "title": "A Unifying View of Sparse Approximate Gaussian Process Regression", "journal": "", "year": "2005", "authors": "J Qui\u00f1onero Candela; C E Rasmussen"}, {"ref_id": "b23", "title": "Random Features for Large-scale Kernel Machines", "journal": "", "year": "2008", "authors": "A Rahimi; B Recht"}, {"ref_id": "b24", "title": "Gaussian Processes for Machine Learning", "journal": "MIT Press", "year": "2006", "authors": "C E Rasmussen; C K Williams"}, {"ref_id": "b25", "title": "Multivariate Integration and Approximation for Random Fields Satisfying Sacks-Ylvisaker Conditions", "journal": "Institute of Mathematical Statistics", "year": "1995", "authors": "K Ritter; G W Wasilkowski; H Wo\u017aniakowski"}, {"ref_id": "b26", "title": "Information Consistency of Nonparametric Gaussian Process Methods", "journal": "IEEE", "year": "2008", "authors": "M W Seeger; S M Kakade; D P Foster"}, {"ref_id": "b27", "title": "On the eigenspectrum of the Gram matrix and the generalization error of kernel-PCA", "journal": "IEEE Transactions on Information Theory", "year": "2005", "authors": "J Shawe-Taylor; C K Williams; N Cristianini; J Kandola"}, {"ref_id": "b28", "title": "Variational Learning of Inducing Variables in Sparse Gaussian Processes", "journal": "", "year": "2009", "authors": "M K Titsias"}, {"ref_id": "b29", "title": "Variational Inference for Gaussian and Determinantal Point Processes", "journal": "", "year": "2014-12", "authors": "M K Titsias"}, {"ref_id": "b30", "title": "Two Problems with Variational Expectation Maximisation for Time Series Models", "journal": "Cambridge University Press", "year": "2011", "authors": "R E Turner; M Sahani"}, {"ref_id": "b31", "title": "Asymptotic Behavior of the Eigenvalues of Certain Integral Equations. I", "journal": "American Mathematical Society", "year": "1963", "authors": "H Widom"}, {"ref_id": "b32", "title": "Asymptotic behavior of the eigenvalues of certain integral equations. II", "journal": "Springer", "year": "1964", "authors": "H Widom"}, {"ref_id": "b33", "title": "Using the Nystr\u00f6m Method to Speed Up Kernel Machines", "journal": "MIT Press", "year": "2001", "authors": "C K I Williams; M Seeger"}, {"ref_id": "b34", "title": "The Solution of a Homogeneous Wiener-Hopf Integral Equation Occurring in the Expansion of Secondorder Stationary Random Functions", "journal": "IEEE", "year": "1957", "authors": "D Youla"}, {"ref_id": "b35", "title": "Gaussian Regression and Optimal Finite Dimensional Linear Models", "journal": "Springer-Verlag", "year": "1997", "authors": "H Zhu; C K I Williams; R Rohwer; M Morciniec"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "choose M inducing variables which summarise the entire posterior, reducing the cost to O N M 2 + M 3 time and O N M + M 2 memory.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 .1Figure 1. Increasing N with fixed M increases the expected KL divergence. t/2\u03c3 2n is a lower bound for the expected value over the KL divergence when y is generated according to our prior model.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 .2Figure2. Determinant based sampling, with a SE kernel with = 2 (top) and with = .5 (middle) leads to more dispersed inducing points than uniform sampling (bottom).", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Theorem 3 .3Suppose N training inputs are drawn i.i.d according to input density p(x), and k(x, x) < v for all x \u2208 X . Sample M inducing points from the training data with the probability assigned to any set of size M equal to the probability assigned to the corresponding subset by an k-DPP with k = M . With probability at least 1 \u2212 \u03b4,", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 3 .3Figure3. Rates of convergence as M increases on fixed dataset of size N = 1000, with a SE-kernel with = .6, v = 1, \u03c3n = 1 and x \u223c N (0, 1) and y sampled from the prior.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 4 .4Figure4. We increase N and take M = C log(N ) for a onedimensional SE-kernel and normally distributed inputs. The KL divergence decays rapidly, as predicted by Corollary 2.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "j , x j ) \u2212 k T S\\i,j L \u22121T S\\i L \u22121 S\\i k S\\i,j . All of these calculations can be computed in O(M 2 ) completing the proof of the per iteration cost. E. Proof of Corollaries E.1. Corollary 2", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "KL Q P \u2264 O g(M, N ) \u03c3 2 n \u03b4 1 + c y 2 2 \u03c3 2 n + N", "formula_coordinates": [1.0, 321.38, 600.7, 193.65, 25.77]}, {"formula_id": "formula_1", "formula_text": "f \u223c GP(\u03bd(\u2022), k(\u2022, \u2022)), y i = f (x i ) + i , i \u223c N (0, \u03c3 2 n ),", "formula_coordinates": [2.0, 56.19, 252.11, 232.51, 18.44]}, {"formula_id": "formula_2", "formula_text": "L = \u2212 1 2 y T K \u22121 n y \u2212 1 2 log|K n | \u2212 N 2 log(2\u03c0) , (1", "formula_coordinates": [2.0, 78.99, 391.29, 206.58, 23.11]}, {"formula_id": "formula_3", "formula_text": ")", "formula_coordinates": [2.0, 285.57, 398.35, 3.87, 8.64]}, {"formula_id": "formula_4", "formula_text": "GP k \u2022u K \u22121 uu \u00b5, k \u2022\u2022 + k \u2022u K \u22121 uu (\u03a3 \u2212 K uu )K \u22121 uu k u\u2022 , (2) where [k u\u2022 ] i = k(\u2022, z i ), [K uf ] m,i := k(z m , x i ) and [K uu ] m,n := k(z m , z n )", "formula_coordinates": [2.0, 55.08, 612.72, 234.36, 46.9]}, {"formula_id": "formula_5", "formula_text": "L lower = \u2212 1 2 y T Q \u22121 n y\u2212 1 2 log|Q n |\u2212 N 2 log(2\u03c0)\u2212 t 2\u03c3 2 n (3) where Q n = Q ff + \u03c3 2 n I, Q ff = K T uf K \u22121 uu K uf and t = Tr(K ff \u2212 Q ff ).", "formula_coordinates": [2.0, 307.08, 93.0, 234.36, 63.69]}, {"formula_id": "formula_6", "formula_text": "u m = X f (x)g(x; z m )dx . When g(x; z m ) = \u03b4(x \u2212 z m ) the u m are inducing points.", "formula_coordinates": [2.0, 306.97, 420.22, 236.21, 44.16]}, {"formula_id": "formula_7", "formula_text": "L upper := \u2212 1 2 y T (Q n +tI) \u22121 y\u2212 1 2 log(|Q n |)\u2212 N 2 log 2\u03c0. (4)", "formula_coordinates": [2.0, 312.42, 608.95, 229.02, 23.11]}, {"formula_id": "formula_8", "formula_text": "Kg(x ) = X g(x)k(x, x )p(x)dx,(5)", "formula_coordinates": [3.0, 101.73, 118.23, 187.71, 18.15]}, {"formula_id": "formula_9", "formula_text": "k(x, x ) = \u221e m=1 \u03bb m \u03c6 m (x)\u03c6 m (x ),(6)", "formula_coordinates": [3.0, 103.34, 245.42, 186.1, 30.2]}, {"formula_id": "formula_10", "formula_text": "L 2 (X ) p . Additionally, \u221e m=1 \u03bb m < \u221e.", "formula_coordinates": [3.0, 55.44, 306.72, 161.08, 19.34]}, {"formula_id": "formula_11", "formula_text": "KL Q P \u2264 1 2\u03c3 2 n t+ \u03bb max y 2 2 \u03c3 2 n + \u03bb max \u2264 t 2\u03c3 2 n 1+ y 2 2 \u03c3 2 n +t .", "formula_coordinates": [3.0, 309.04, 342.39, 230.8, 27.66]}, {"formula_id": "formula_12", "formula_text": "i } N i=1 , if the outputs {y i } N i=1", "formula_coordinates": [3.0, 426.14, 569.01, 114.81, 17.94]}, {"formula_id": "formula_13", "formula_text": "t 2\u03c3 2 n \u2264 E y KL Q P \u2264 t \u03c3 2 n (7)", "formula_coordinates": [3.0, 362.68, 603.94, 178.76, 24.19]}, {"formula_id": "formula_14", "formula_text": "E y KL Q P = t 2\u03c3 2 n + N (y; 0, K n ) \u00d7 log N (y; 0, K n ) N (y; 0, Q n ) dy", "formula_coordinates": [4.0, 85.06, 90.91, 174.76, 57.67]}, {"formula_id": "formula_15", "formula_text": "u m := N i=1 w (m) i f (x i ) where w (m) i", "formula_coordinates": [4.0, 55.08, 324.57, 162.17, 56.31]}, {"formula_id": "formula_16", "formula_text": "cov(u m , u k ) = w (m)T K ff w (k) = \u03bb k (K ff )\u03b4 m,k ,(8)", "formula_coordinates": [4.0, 66.32, 426.51, 223.12, 11.72]}, {"formula_id": "formula_17", "formula_text": "cov(u m , f (x i )) = K ff w (m) i = \u03bb m (K ff )w (m) i .(9)", "formula_coordinates": [4.0, 66.32, 444.48, 223.12, 18.26]}, {"formula_id": "formula_18", "formula_text": "{w (m) } M m=1 , with the eigenfunctions, {\u03c6 m } M m=1 , yielding u m = X f (x)\u03c6 m (x)p(x)dx. (10", "formula_coordinates": [4.0, 307.44, 104.51, 234.0, 57.52]}, {"formula_id": "formula_19", "formula_text": ")", "formula_coordinates": [4.0, 537.29, 145.11, 4.15, 8.64]}, {"formula_id": "formula_20", "formula_text": "cov(u m , u k ) = \u03bb m \u03b4 m,k and cov(u m , f (x i )) = \u03bb m \u03c6 m (x i ).", "formula_coordinates": [4.0, 307.44, 193.71, 234.56, 9.65]}, {"formula_id": "formula_21", "formula_text": "KL Q P \u2264 C 2\u03c3 2 n \u03b4 1 + y 2 2 \u03c3 2 n (11", "formula_coordinates": [4.0, 355.47, 527.85, 181.82, 25.77]}, {"formula_id": "formula_22", "formula_text": ")", "formula_coordinates": [4.0, 537.29, 536.48, 4.15, 8.64]}, {"formula_id": "formula_23", "formula_text": "KL Q P \u2264 C \u03b4\u03c3 2 n ,(12)", "formula_coordinates": [4.0, 384.0, 647.57, 157.44, 24.19]}, {"formula_id": "formula_24", "formula_text": "A direct computation of Q ff shows that [Q ff ] i,j = M m=1 \u03bb m \u03c6 m (x i )\u03c6 m (x j ).", "formula_coordinates": [5.0, 55.08, 70.19, 234.36, 26.29]}, {"formula_id": "formula_25", "formula_text": "K ff i,i = \u221e m=M +1 \u03bb m \u03c6 2 m (x i ).", "formula_coordinates": [5.0, 113.71, 117.39, 122.17, 30.47]}, {"formula_id": "formula_26", "formula_text": "E X [t] = N \u221e m=M +1 \u03bb m E xi \u03c6 2 m (x i ) = N \u221e m=M +1 \u03bb m .", "formula_coordinates": [5.0, 61.87, 180.34, 221.15, 30.47]}, {"formula_id": "formula_27", "formula_text": "\u03bb m = v 2a/AB m\u22121 where a = 1/(4\u03c3 2 ), b = 1/(2 2 ), c = \u221a a 2 + 2ab, A = a + b + c and B = b/A [", "formula_coordinates": [5.0, 55.08, 324.96, 234.36, 45.85]}, {"formula_id": "formula_28", "formula_text": "\u221e m=M +1 \u03bb m = v \u221a 2a (1 \u2212 B) \u221a A B M .", "formula_coordinates": [5.0, 107.07, 388.43, 130.74, 40.4]}, {"formula_id": "formula_29", "formula_text": "the Mat\u00e9rn k + 1/2 kernel in one dimension, \u03bb m m \u22122k\u22122 [", "formula_coordinates": [5.0, 55.44, 522.38, 221.54, 20.91]}, {"formula_id": "formula_30", "formula_text": "\u221e m=M +1 \u03bb m = O(M \u22122k\u22121 ).", "formula_coordinates": [5.0, 65.96, 543.33, 117.44, 19.34]}, {"formula_id": "formula_31", "formula_text": "N \u2192\u221e N M 2k+1 \u2192 0. This holds if M = N \u03b1 for \u03b1 > 1 2k+1 .", "formula_coordinates": [5.0, 55.13, 557.92, 234.31, 30.31]}, {"formula_id": "formula_32", "formula_text": "{c i } M i=1 , are selected from K ff , the approximation used is K ff \u2248 CC \u22121 C T , where C = [c 1 , c 2 , . . . , c M ] and C is the M \u00d7 M principal submatrix associated to the {c i } M i=1 .", "formula_coordinates": [5.0, 307.44, 446.09, 234.0, 44.35]}, {"formula_id": "formula_33", "formula_text": "K T uf = C, so Q ff = CC \u22121 C T .", "formula_coordinates": [5.0, 307.44, 507.01, 127.26, 15.3]}, {"formula_id": "formula_34", "formula_text": "E Z [Tr(K ff \u2212 Q ff )] \u2264 (M + 1) N m=M +1 \u03bb m (K ff ). (13", "formula_coordinates": [5.0, 314.6, 607.12, 222.7, 30.47]}, {"formula_id": "formula_35", "formula_text": ")", "formula_coordinates": [5.0, 537.29, 617.85, 4.15, 8.64]}, {"formula_id": "formula_36", "formula_text": "E Z\u223c\u03bd [t] \u2264 (M + 1) N m=M +1 \u03bb m (K ff ) + 2N v . (14)", "formula_coordinates": [6.0, 68.26, 347.93, 221.18, 30.48]}, {"formula_id": "formula_37", "formula_text": "KL Q P \u2264 C(M + 1) + 2N v 2\u03c3 2 n \u03b4 1 + y 2 2 \u03c3 2 n (15", "formula_coordinates": [6.0, 66.14, 522.12, 219.16, 25.77]}, {"formula_id": "formula_38", "formula_text": ")", "formula_coordinates": [6.0, 285.29, 530.75, 4.15, 8.64]}, {"formula_id": "formula_39", "formula_text": "KL Q P \u2264 C(M + 1) + 2N v \u03b4\u03c3 2 n .(16)", "formula_coordinates": [6.0, 100.48, 643.48, 188.96, 24.19]}, {"formula_id": "formula_40", "formula_text": "E X E Z|X E y KL Q P \u2264 \u03c3 \u22122 n E X E Z|X [t] \u2264 \u03c3 \u22122 n (M + 1)E X N m=M +1 \u03bb m (K ff ) + 2N v \u2264 \u03c3 \u22122 n (M + 1)N \u221e m=M +1 \u03bb m + 2N v .", "formula_coordinates": [6.0, 307.44, 91.33, 230.11, 85.43]}, {"formula_id": "formula_41", "formula_text": "KL Q P \u2264 (M + 1)N \u221e m=M +1 \u03bb m + 2N V \u03b4\u03c3 2 n .", "formula_coordinates": [6.0, 318.55, 319.25, 204.03, 28.13]}, {"formula_id": "formula_42", "formula_text": "KL Q P \u2264 N \u2212\u03b3 2R/\u03c3 2 n + 2/N .(17)", "formula_coordinates": [6.0, 348.68, 672.45, 192.76, 18.89]}, {"formula_id": "formula_43", "formula_text": "M = (3+\u03b3) log(N )+log D log(B \u22121 ) , where D = v \u221a 2a 2 \u221a A\u03c3 2 n \u03b4(1\u2212B)", "formula_coordinates": [7.0, 55.44, 209.81, 234.0, 33.83]}, {"formula_id": "formula_44", "formula_text": "N i=1 y 2 i \u2264 N i=1 f (x i ) 2 + N i=1 2 i + o(N )", "formula_coordinates": [7.0, 96.58, 346.34, 152.47, 30.32]}, {"formula_id": "formula_45", "formula_text": "least 1 \u2212 \u03b4, KL Q P \u2264 \u03b4 \u22121 .", "formula_coordinates": [7.0, 307.44, 343.55, 135.37, 18.89]}, {"formula_id": "formula_46", "formula_text": "KERNEL INPUT DISTRIBUTION DECAY OF \u03bbm M, THEOREM 3 M, THEOREM 4 SE-KERNEL COMPACT SUPPORT O exp(\u2212\u03b1 m d log m d ) O(log D (N )) O(log D (N )) SE-KERNEL GAUSSIAN O exp(\u2212\u03b1 m d ) O(log D (N )) O(log D (N )) MAT\u00c9RN k+1/2 2 UNIFORM ON INTERVAL O M \u22122k\u22122 log(M ) 2(d\u22121)(k+1) O N 1/k+ O N 1/(2k)+", "formula_coordinates": [8.0, 74.52, 95.24, 445.83, 48.43]}, {"formula_id": "formula_47", "formula_text": "KL(\u00b5 X \u03bd X ) = KL(\u00b5 x * \u03bd x * ) + E \u00b5x * KL \u00b5 X \\x * |x * \u03bd X \\x * |x * \u2265 KL(\u00b5 x * \u03bd x * ).", "formula_coordinates": [8.0, 316.59, 186.63, 215.71, 32.41]}, {"formula_id": "formula_48", "formula_text": "|\u00b5 1 \u2212 \u00b5 2 | \u2264 \u03c3 2 \u221a \u03b5 \u2264 \u03c3 1 \u221a \u03b5 1 \u2212 \u221a 3\u03b5 and |1 \u2212 \u03c3 2 1 /\u03c3 2 2 | < \u221a 3\u03b5.", "formula_coordinates": [8.0, 307.44, 316.73, 238.39, 40.79]}, {"formula_id": "formula_49", "formula_text": "L \u2264 L upper := \u2212 N 2 log(2\u03c0) \u2212 1 2 log(|Q n |) \u2212 1 2 y T Q n + \u03bb max I \u22121 y.", "formula_coordinates": [10.0, 317.4, 462.52, 214.07, 47.42]}, {"formula_id": "formula_50", "formula_text": "L upper \u2212 L lower = t 2\u03c3 2 n + 1 2 y T Q \u22121 n \u2212 (Q n + \u03bb max I) \u22121 y . (18)", "formula_coordinates": [10.0, 317.57, 547.26, 223.87, 39.49]}, {"formula_id": "formula_51", "formula_text": "(U T y) T \u039b \u22121 \u2212 (\u039b + \u03bb max I) \u22121 (U T y). Define, z = (U T y). Since U is unitary, z = y . (U T y) T \u039b \u22121 \u2212 (\u039b + tI) \u22121 (U T y) = z T \u039b \u22121 \u2212 (\u039b + \u03bb max I) \u22121 z = i z 2 i \u03bb max \u03b3 2 i + \u03b3 i \u03bb max \u2264 y 2 \u03bb max \u03b3 2 N + \u03b3 N \u03bb max .", "formula_coordinates": [10.0, 340.06, 704.69, 168.76, 18.44]}, {"formula_id": "formula_52", "formula_text": "y T Q \u22121 n \u2212 (Q n + \u03bb max I) \u22121 y \u2264 \u03bb max y 2 \u03c3 4 n + \u03c3 2 n \u03bb max .", "formula_coordinates": [11.0, 68.27, 248.54, 208.35, 27.66]}, {"formula_id": "formula_53", "formula_text": "KL(p 1 p 2 ) = 1 2 Tr S 2 \u22121 S 1 + log |S 2 | |S 1 | +(m 1 \u2212 m 2 ) T S 2 \u22121 (m 1 \u2212 m 2 ) \u2212 N \u2265 0. (19", "formula_coordinates": [11.0, 65.4, 431.5, 219.89, 48.93]}, {"formula_id": "formula_54", "formula_text": ")", "formula_coordinates": [11.0, 285.29, 464.38, 4.15, 8.64]}, {"formula_id": "formula_55", "formula_text": "E y KL Q P = t 2\u03c3 2 n + N (y; 0, K n ) \u00d7 log N (y; 0, K n ) N (y; 0, Q n ) dy", "formula_coordinates": [11.0, 85.06, 541.38, 174.76, 57.67]}, {"formula_id": "formula_56", "formula_text": "E y KL Q P = t 2\u03c3 2 n \u2212 N 2 + 1 2 log |Q n | |K n | + 1 2 Tr Q \u22121 n (K n ) \u2264 t 2\u03c3 2 n \u2212 N 2 + 1 2 Tr Q \u22121 n (Q n + K ff ) . (20", "formula_coordinates": [11.0, 65.4, 640.33, 219.89, 76.09]}, {"formula_id": "formula_57", "formula_text": ")", "formula_coordinates": [11.0, 285.29, 699.28, 4.15, 8.64]}, {"formula_id": "formula_58", "formula_text": "1 2 Tr(I) + 1 2 Tr Q \u22121 n K ff ) \u2264 N/2 + t\u03bb 1 Q \u22121 n /2 \u2264 N/2 + t/(2\u03c3 2 n ).", "formula_coordinates": [11.0, 320.87, 114.7, 208.35, 42.54]}, {"formula_id": "formula_59", "formula_text": "Defining = 2KL(q p), = \u03c3 2 1 + (\u00b5 1 \u2212 \u00b5 2 ) 2 \u03c3 2 2 \u2212 log \u03c3 2 1 \u03c3 2 2 \u2212 1 (21) \u2265 x \u2212 log(x) \u2212 1", "formula_coordinates": [11.0, 307.44, 255.43, 234.0, 67.55]}, {"formula_id": "formula_60", "formula_text": "x = \u03c3 2 1 \u03c3 2 2 . Applying the lower bound x \u2212 log(x) \u2212 1 \u2265 (x \u2212 1) 2 /2 \u2212 (x \u2212 1) 3 /3, \u2265 (x \u2212 1) 2 /2 \u2212 (x \u2212 1) 3 /3.", "formula_coordinates": [11.0, 306.28, 328.82, 235.17, 76.92]}, {"formula_id": "formula_61", "formula_text": "x \u2212 log(x) \u2212 1 \u2265 (x \u2212 1) 2 /3 So, |x \u2212 1| \u2264 \u221a 3", "formula_coordinates": [11.0, 307.44, 475.05, 176.83, 54.91]}, {"formula_id": "formula_62", "formula_text": "1 \u2212 \u221a 3 < \u03c3 2 1 \u03c3 2 2 < 1 + \u221a 3 .", "formula_coordinates": [11.0, 368.83, 552.45, 111.21, 27.29]}, {"formula_id": "formula_63", "formula_text": "x \u2212 log x > 1, |\u00b5 1 \u2212 \u00b5 2 | \u2264 \u03c3 2 \u221a 3 .", "formula_coordinates": [11.0, 383.34, 589.58, 82.2, 40.24]}, {"formula_id": "formula_64", "formula_text": "u m = N i=1 w (m) i f (x i ).", "formula_coordinates": [12.0, 127.63, 110.3, 89.63, 30.32]}, {"formula_id": "formula_65", "formula_text": "cov(u m , u k ) = E \uf8ee \uf8f0 N i=1 w (m) i f (x i ) N j=1 w (k) j f (x j ) \uf8f9 \uf8fb = N i=1 w (m) i N j=1 w (k) j E[f (x i )f (x j )] = N i=1 w (m) i N j=1 w (k) j k(x i , x j ).", "formula_coordinates": [12.0, 70.2, 171.4, 204.49, 107.48]}, {"formula_id": "formula_66", "formula_text": "mality, cov(u m , u k ) = \u03bb k (K ff )\u03b4 m,k . Similarly, cov(u m , f (x i )) = E \uf8ee \uf8f0 N j=1 w (m) j f (x j )f (x i ) \uf8f9 \uf8fb = N j=1 w (m) j E[f (x j )f (x i )] = N j=1 w (m) j k(x j , x i ).", "formula_coordinates": [12.0, 55.44, 317.61, 207.35, 159.0]}, {"formula_id": "formula_67", "formula_text": "u m = \u03c6 m (x)f (x)p(x)dx. Then, cov(u m , u k ) = E \u03c6 m (x)f (x)p(x)dx \u03c6 k (x )f (x )p(x )dx = \u03c6 m (x)p(x) \u03c6 k (x )E[f (x)f (x )]p(x )dx dx = \u03c6 m (x)p(x) \u03c6 k (x )k(x, x )p(x )dx dx.", "formula_coordinates": [12.0, 55.13, 575.26, 221.59, 134.07]}, {"formula_id": "formula_68", "formula_text": "cov(u m , u k ) = \u03bb k \u03c6 k (x)\u03c6 m (x)p(x)dx = \u03bb k \u03b4 m,k . With similar considerations, cov(u m , f (x i )) = E \u03c6 m (x)f (x)f (x i )p(x)dx = \u03c6 m (x)E[f (x)f (x i )]p(x)dx = \u03bb m \u03c6 m (x i ). D. Discrete k-DPPs D.1. Proof of Corollary 1 from Lemma 3 E Z\u223c\u03bd [t] = E Z\u223c\u00b5 [t] + (E Z\u223c\u03bd [t] \u2212 E Z\u223c\u00b5 [t]) \u2264 (M + 1) N M +1 \u03bb(K ff ) + Z\u2208( N M ) (\u00b5(Z) \u2212 \u03bd(Z))t(Z) \u2264 (M + 1) N M +1 \u03bb(K ff ) + N v Z\u2208( N M ) |\u00b5(Z) \u2212 \u03bd(Z)| \u2264 (M + 1) N M +1 \u03bb(K ff ) + 2N v .", "formula_coordinates": [12.0, 306.97, 171.4, 238.08, 304.54]}, {"formula_id": "formula_69", "formula_text": "R( ) \u2264 N M log N M M ! \u2264 N M 2 log N +N M log 1 .", "formula_coordinates": [12.0, 307.44, 691.03, 234.0, 26.12]}, {"formula_id": "formula_70", "formula_text": "L S = \uf8ee \uf8f0 L 1,1 0 0 2,1 2,2 0 L 3,1 3,2 L 3,3 \uf8f9 \uf8fb ,K S = \uf8ee \uf8f0 K 1,1 k 2,1 K 1,3 k 2,1 k(x i , x i ) k 2,3 K 3,1 k 3,2 K 3,3 \uf8f9 \uf8fb A direct calculation shows, L S\\i = L 1,1 0 L 3,1 L 3,3", "formula_coordinates": [13.0, 55.08, 565.08, 242.94, 87.78]}, {"formula_id": "formula_71", "formula_text": "KL Q P \u2264 C(M + 1) 2\u03c3 2 n \u03b4 1 + y 2 2 \u03c3 2 n + N \u2212\u03b3 (R/\u03c3 2 n + 1/N )(22)", "formula_coordinates": [13.0, 344.17, 267.8, 197.27, 40.99]}, {"formula_id": "formula_72", "formula_text": "\u221e i=M +1 \u03bb i = v \u221a 2a \u221a A B M 1 \u2212 B Now, B M = N \u22123\u2212\u03b3 D \u22121 , so \u221e i=M +1 \u03bb i = \u03b4N \u22123\u2212\u03b3 , implying C(M +1) 2\u03b4\u03c3 2 n < N \u22121\u2212\u03b3 .", "formula_coordinates": [13.0, 307.44, 369.76, 167.89, 135.34]}, {"formula_id": "formula_73", "formula_text": "\u03bb s+D\u22121 \u2264 2a A D/2 B s 1/D . DefineM = M + D \u2212 1,", "formula_coordinates": [13.0, 307.44, 596.92, 175.66, 58.93]}], "doi": ""}