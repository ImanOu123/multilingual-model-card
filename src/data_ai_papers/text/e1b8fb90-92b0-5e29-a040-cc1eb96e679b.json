{"title": "Boosting for Comparison-Based Learning", "authors": "Micha\u00ebl Perrot; Ulrike Von Luxburg", "pub_date": "2019-05-29", "abstract": "We consider the problem of classification in a comparison-based setting: given a set of objects, we only have access to triplet comparisons of the form object x i is closer to object x j than to object x k . In this paper we introduce TripletBoost, a new method that can learn a classifier just from such triplet comparisons. The main idea is to aggregate the triplets information into weak classifiers, which can subsequently be boosted to a strong classifier. Our method has two main advantages: (i) it is applicable to data from any metric space, and (ii) it can deal with large scale problems using only passively obtained and noisy triplets. We derive theoretical generalization guarantees and a lower bound on the number of necessary triplets, and we empirically show that our method is both competitive with state of the art approaches and resistant to noise.", "sections": [{"heading": "Introduction", "text": "In the past few years the problem of comparison-based learning has attracted growing interest in the machine learning community [Agarwal et al., 2007;Jamieson and Nowak, 2011;Tamuz et al., 2011;Tschopp et al., 2011;Van Der Maaten and Weinberger, 2012;Heikinheimo and Ukkonen, 2013;Amid and Ukkonen, 2015;Kleindessner and Luxburg, 2015;Jain et al., 2016;Haghiri et al., 2017;Kazemi et al., 2018]. The motivation is to relax the assumption that an explicit representation of the objects or a distance metric between pairs of examples are available. Instead one only has access to a set of ordinal distance comparisons that can take several forms depending on the problem at hand. In this paper we focus on triplet comparisons of the form object x i is closer to object x j than to object x k , that is on relations of the form d(x i , x j ) < d(x i , x k ) where d is an unknown metric 1 .\nWe address the problem of classification with noisy triplets that have been obtained in a passive manner: the examples lie in an unknown metric space, not necessarily Euclidean, and we are only given a small set of triplet comparisons -there is no way in which we could actively ask for more triplets. Furthermore we assume that the answers to the triplet comparisons can be noisy. To deal with this problem one can try to first recover an explicit representation of the examples, a task that can be solved by ordinal embedding approaches [Agarwal et al., 2007;Van Der Maaten and Weinberger, 2012;Terada and von Luxburg, 2014;Jain et al., 2016], and then apply standard machine learning approaches. However, such embedding methods assume that the examples lie in a Euclidean space and do not scale well with the number of examples: typically they are too slow for datasets with more than 10 3 examples. As an alternative, it would be desirable to have a classification algorithm that can work with triplets directly, without taking a detour via ordinal embedding. To the best of our knowledge, for the case of passively obtained triplets, this problem has not yet been solved in the literature.\nAnother interesting question in this context is that of the minimal number of triplets required to successfully learn a classifier. It is known that to exactly recover an ordinal embedding one needs of the order \u2126(n 3 ) passively queried triplets in the worst case (essentially all of them), unless we make stronger assumptions on the underlying metric space [Jamieson and Nowak, 2011]. However, classification is a problem which seems simpler than ordinal embedding, and thus it might be possible to obtain better lower bounds.\nIn this paper we propose TripletBoost, a method for classification that is able to learn using only passively obtained triplets while not making any assumptions on the underlying metric space. To the best of our knowledge this is the first approach that is able to solve this problem. Our method is based on the idea that the triplets can be aggregated into simple triplet classifiers, which behave like decision stumps and are well-suited for boosting approaches [Schapire and Freund, 2012]. From a theoretical point of view we prove that our approach learns a classifier with low training error, and we derive generalization guarantees that ensure that its error on new examples is bounded. Furthermore we derive a new lower bound on the number of triplets that are necessary to ensure useful predictions. From an empirical point of view we demonstrate that our approach can be applied to datasets that are several order of magnitudes larger than the ones that can currently be handled by ordinal embedding methods. Furthermore we show that our method is quite resistant to noise. ", "publication_ref": ["b0", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "The TripletBoost Algorithm", "text": "In this paper we are interested in multi-class classification problems. Let (X , d) be an unknown and general metric space, typically not Euclidean. Let Y be a finite label space. Let S = {(x i , y i )} n i=1 be a set of n examples drawn i.i.d. from an unknown distribution D S defined over X \u00d7 Y. Note that we use the notation x i as a convenient way to identify an object; it does not correspond to any explicit representation that could be used by an algorithm (such as coordinates in a vector space). Let T = {(x i , x j , x k ) : (x i , y i ), (x j , y j ), (x k , y k ) \u2208 S, x j = x k } be a set of m triplets. Each ordered tuple (x i , x j , x k ) \u2208 T encodes the following relation between the three examples:\nd(x i , x j ) < d(x i , x k ) .\n(1)\nGiven the triplets in T and the label information of all points, our goal is to learn a classifier. We make two main assumptions about the data. First, we assume that the triplets are uniformly and independently sampled from the set of all possible triplets. Second, we assume that the triplets in T can be noisy (that is the inequality has been swapped, for example (x i , x k , x j ) \u2208 T while the true relation is d(x i , x j ) < d(x i , x k )), but the noise is uniform and independent from one triplet to another. In the following I a denotes the indicator function returning 1 when a property a is verified and 0 otherwise, W c is an empirical distribution over S \u00d7 Y, and w c,xi,y is the weight associated to object x i and label y.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Weak Triplet Classifiers", "text": "Rather than considering triplets as individual pieces of information we propose to aggregate them into decision stumps that we call triplet classifiers. The underlying idea is to select two reference examples, x j and x k , and to divide the space into two half-spaces: examples that are closer to x j and examples that are closer to x k . This is illustrated in Figure 1. In principle, this can be achieved with triplets only. However, a major difficulty in our setting is that our triplets are passively obtained: for most training points x i we do not know whether they are closer to x j or x k . In particular, it is impossible to evaluate the classification accuracy of such a simple classifier on the whole training set. To deal with this problem we propose to use an abstention scheme where a triplet classifier Algorithm 1 TripletBoost: Boosting with triplet classifiers.\nInput: S = {(x i , y i )} n i=1 a set of n examples, T = {(x i , x j , x k ) : x j = x k } a set of m triplets.\nOutput: H(\u2022) a strong classifier.\n1: Let W 1 be the empirical uniform distribution:\n\u2200(x i , y i ) \u2208 S, \u2200y \u2208 Y, w 1,xi,y = 1 n|Y| . 2: for c = 1, . . . , C do 3:\nChoose a triplet classifier h c according to W c (Equation (2)).", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "4:", "text": "Compute the weight \u03b1 c of h c according to its performance on W c (Equation (3)).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "5:", "text": "Update the weights of the examples to obtain a new distribution W c+1 (Equation ( 4)). 6: end for\n7: return H(\u2022) = arg max y\u2208Y C c=1 \u03b1 c I hc(\u2022) =\u03d1\u2227y\u2208hc(\u2022)\nabstains if it does not know on which side of the hyperplane the considered point lies. Given a set T of triplets and two reference points x j and x k , we define a triplet classifier as:\nh j,k (x) = o j if (x, x j , x k ) \u2208 T , o k if (x, x k , x j ) \u2208 T , \u03d1 otherwise.\nIn our multi-class setting, o j and o k will be sets of labels, that is o j , o k \u2286 Y. In Section 2.2 we describe how we choose them in a data dependent fashion to obtain classifiers with minimal error on the training set. The prediction \u03d1 simply means that the triplet classifier abstains on the example. Let H denote the set of all possible triplet classifiers.\nThe triplet classifiers are very simple and, in practice, we do not expect them to perform well at all. But we prove in Section 3.1 that, for appropriate choices of o j and o k , they are at least as good as random predictors. This is all we need to ensure that they can be used successfully in a boosting framework. The next section describes how this works.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "TripletBoost", "text": "Boosting is based on the insight that weak classifiers (that is classifiers marginally better than random predictors) are usually easy to obtain and can be combined in a weighted linear combination to obtain a strong classifier. This weighted combination can be obtained in an iterative fashion where, at each iteration, a weak-classifier is chosen and weighted so as to minimize the error on the training examples. The weights of the points are then updated to put more focus on hardto-classify examples [Schapire and Freund, 2012]. In this paper we use a well-known boosting algorithm called Ad-aBoost. MO [Schapire and Singer, 1999;Schapire and Freund, 2012]. This method can handle multi-class problems with a one-against-all approach, works with abstaining classifiers and is theoretically well founded. Algorithm 1 summarizes the main steps of our approach that we detail below. Choosing a triplet classifier. To choose a triplet classifier we proceed in two steps. In the first step, we select two reference points x j and x k such that y j = y k . This is done by randomly sampling from an empirical distribution W c,X on the examples. Here W c,X denotes the marginal distribution of W c with respect to the examples. This distribution is updated at each iteration to put more focus on those parts of the space that are hard to classify while promoting triplet classifiers that are able to separate different classes (see Equation 4).\nIn the second step, we choose o j and o k , the sets of labels that should be predicted for each half space of the triplet classifier. Given one of the half spaces, we propose to add a label to the set of predicted labels if the weight of examples of this class is greater than the weight of examples of different classes. Formally, with w c,xi,y defined as in Algorithm 1, we construct o j as follows:\no j = y : (xi,yi)\u2208S, (xi,xj ,x k )\u2208T (I y=yi \u2212 I y =yi ) w c,xi,y > 0 . (2)\nWe construct o k in a similar way. The underlying idea is that adding h c to the current combination of triplet classifiers should improve the predictions on the training set as much as possible. In Section 3.1 we show that this strategy is optimal and that it ensures that the selected triplet classifier is either a weak classifier or has a weight \u03b1 c of 0. Computing the weight of the triplet classifier. To choose the weight of the triplet classifier h c we start by computing W c,+ and W c,\u2212 , the weights of correctly and incorrectly classified examples:\nW c,+ = (xi,yi)\u2208S, hc(xi) =\u03d1 I yi\u2208hc(xi) w c,xi,yi + y =yi I y / \u2208hc(xi) w c,xi,y , W c,\u2212 = (xi,yi)\u2208S, hc(xi) =\u03d1 I yi / \u2208hc(xi) w c,xi,yi + y =yi I y\u2208hc(xi) w c,xi,y .(3)\nWe then set\n\u03b1 c = log W c,+ + 1 n W c,\u2212 + 1 n /2. The term 1\nn is a smoothing constant [Schapire and Singer, 2000]: in our setting with few, passively queried triplets it helps to avoid numerical problems that might arise when W c,+ or W c,\u2212 = 0. In Theorem 2 we show that this choice of \u03b1 c leads to a decrease in training error as the number of iterations increases. Updating the weights of the examples. In each iteration of our algorithm, a new triplet classifier h c is added to the weighted combination of classifiers, and we need to update the empirical distribution W c over the examples for the next iteration. The idea is (i) to reduce the weights of correctly classified examples, (ii) to keep constant the weights of the examples for which the current triplet classifier abstains, and (iii) to increase the weights of incorrectly classified examples. The weights are then normalized by a factor Z c so that W c+1 remains an empirical distribution over the examples. Formally, \u2200(\nx i , y i ) \u2208 S, \u2200y \u2208 Y, if h c (x i ) = \u03d1 then w c+1,xi,y = wc,x i ,y Zc and if h c (x i ) = \u03d1 then w c+1,xi,y = w c,xi,y Z c exp {\u2212\u03b1 c (I y=yi \u2212 I y =yi ) \u00d7 I y\u2208hc(xi) \u2212 I y / \u2208hc(xi)\n. ( 4)\nUsing H for prediction. Given a new example x, Triplet-Boost predicts its label as\nH(x) = arg max y\u2208Y C c=1 \u03b1 c I hc(x) =\u03d1\u2227y\u2208hc(x)(5)\nthat is the label with the highest weight as predicted by the weighted combination of selected triplet classifiers. However, recall that we are in a passive setting, and thus we assume that we are given a set of triplets T x = {(x, x j , x k ) : (x j , y j ), (x k , y k ) \u2208 S, x j = x k } associated with the example x (but there is no way to choose them). Hence, some of the triplets in T x correspond to triplet classifiers in H (that is (x, x j , x k ) \u2208 T x and the reference points for h c were x j and x k ) and some do not. In particular, it might happen that none of the triplets in T x corresponds to a triplet classifier in H and, in this case, H can only randomly predict a label. In Section 3.3 we provide a lower bound on the number of triplets necessary to avoid this behaviour. The main computational bottleneck when predicting the label of a new example x is to check whether the triplets in T ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Theoretical Analysis", "text": "In this section we show that our approach is theoretically well founded. First we prove that the triplet classifiers with nonzero weights are weak learners: they are slightly better than random predictors (Theorem 1). Building upon this result we show that, as the number of iterations increases, the training error of the strong classifier learned by TripletBoost is decreased (Theorem 2). Then, to ensure that TripletBoost does not over-fit, we derive a generalization bound showing that, given a sufficient amount of training examples, the test error is bounded (Theorem 3). Finally, we derive a lower bound on the number of triplets necessary to ensure that TripletBoost does not learn a random predictor (Theorem 4).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Triplet Classifiers and Weak Learners", "text": "We start this theoretical analysis by showing that the strategy to choose the predicted labels of triplet classifiers described in Equation ( 2) is optimal: it ensures that their error is minimal on the training set (compared to any other labelling strategy). We also show that the triplet classifiers are never worse than random predictors and in fact, that only those triplets classifiers that are weak classifiers (strictly better than random classifiers) are affected a non-zero weight. This is summarized in the next theorem. Theorem 1 (Triplet classifiers and weak learners). Let W c be an empirical distribution over S \u00d7 Y and h c be the corresponding triplet classifier chosen as described in Section 2.2. It holds that:\n1. the error of h c on W c is at most the error of a random predictor and is minimal compared to other labelling strategies, 2. the weight \u03b1 c of the classifier is non-zero if and only if, h c is a weak classifier, that is is strictly better than a random predictor.\nProof. The proof is given in Appendix E.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Boosting guarantees", "text": "From a theoretical point of view the boosting framework has been extensively investigated and it has been shown that most AdaBoost-based methods decrease the training error at each iteration [Freund and Schapire, 1997]. Another question that has attracted a lot of attention is the problem of generalization. It is known that when the training error has been minimized, AdaBoost-based methods often do not over-fit and it might even be beneficial to further increase the number of weak learners. A popular explanation is the margin theory which says that as the number of iterations increases, the confidence of the algorithm in its predictions increases and thus, the test accuracy is improved [Schapire et al., 1998;Breiman, 1999;Wang et al., 2011;Gao and Zhou, 2013]. TripletBoost is based on AdaBoost.MO, and thus it inherits the theoretical guarantees presented above. In this section, we provide two theorems which show (i) that TripletBoost reduces the training error as the number of iterations increases, and (ii) that it generalizes well to new examples.\nThe following theorem shows that, as the number of iterations increases, TripletBoost decreases the training error. Theorem 2 (Reduction of the Training Error). Let S be a set of n examples and T be a set of m triplets (obtained as described in Section 2). Let H(\u2022) be the classifier obtained after C iterations of TripletBoost (Algorithm 1) using S and T as input. It holds that:\nP (x,y)\u2208S [H(x) = y] \u2264 |Y| 2 C c=1 Z c with Z c = (1 \u2212 W c,+ \u2212 W c,\u2212 ) + (W c,+ ) \u2022 Wc,\u2212+ 1 n Wc,++ 1 n + (W c,\u2212 ) \u2022 Wc,++ 1 n Wc,\u2212+ 1 n \u2264 1.\nProof. This result, inherited from AdaBoost. MO [Schapire and Freund, 2012], is proven in Appendix F.\nThe next theorem shows that the true error of a classifier learned by TripletBoost can be bounded by a quantity related to the confidence of the classifier on the training examples, with respect to a margin, plus a term which decreases as the number of examples increases. The confidence of the classifier on the training examples is also bounded and decreases for sufficiently small margins. Theorem 3 (Generalization Guarantees). Let D S be a distribution over X \u00d7 Y, let S be a set of n examples drawn i.i.d. from D S , and let T be a set of m triplets (obtained as described in Section 2). Let H(\u2022) be the classifier obtained after C iterations of TripletBoost (Algorithm 1) using S and T as input. Let H be a set of triplet classifiers as defined in Section 2.1. Then, given a margin parameter \u03b8 > log |H| 16|Y| 2 n and a measure of the confidence of H(\u2022) in its predictions \u03b8 H (x, y), with probability at least 1 \u2212 \u03b4, we have that\nP (x,y)\u223cD S [H(x) = y] \u2264 P (x,y)\u2208S [\u03b8 H (x, y) \u2264 \u03b8] + O \uf8eb \uf8ed log 1 \u03b4 n + log |Y| 2 n\u03b8 2 log |H| log |H| n\u03b8 2 \uf8f6 \uf8f8\nFurthermore we have that\nP (x,y)\u2208S [\u03b8 H (x, y) \u2264 \u03b8] \u2264 |Y| 2 C c=1 Z c W c,+ + 1 n W c,\u2212 + 1 n \u03b8 .\nProof. This result, inherited from AdaBoost. MO [Schapire and Freund, 2012], is proven in Appendix G.\nAt a first glance it seems that this bound does not depend on m, the number of available triplets. However, this dependency is implicit: m impacts the probability that the training examples are well classified with a large margin \u03b8. If the number of triplets is small, the probability that the training examples are well classified with a given margin is small. This probability increases when the number of triplets increases. We illustrate this behaviour in Appendix A.\nTo prove a bound that explicitly depends on m would be of significant interest. However this is a difficult problem, as it requires to use an explicit measure of complexity for general metric spaces, which is beyond the scope of this paper.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Lower bound on the number of triplets", "text": "In this section we investigate the minimum number of triplets that are necessary to ensure that our algorithm performs well. Ideally, we would like to obtain a lower bound on the number of triplets that are necessary to achieve a given accuracy. In this paper we take a first step in this direction by deriving a bound on the number of triplets that are necessary to ensure that the learned classifier does not abstain on any unseen example. Theorems 4 and 5 show that it abstains with high probability if it is learned using too few triplets or if it combines too few triplet classifiers. Theorem 4 (Lower bound on the probability that a strong classifier abstains). Let n \u2265 2 be the number of training examples, p = 2n k n 3 with k \u2208 0, 3 \u2212 log(2) log(n) be the probability that a triplet is available in the triplet set T and\nC = n \u03b2 2 with \u03b2 \u2208 0, 1 + log (n\u22121) log (n)\nbe the number of classifiers combined in the learned classifier. Let A be any algorithm learning a classifier H(\u2022) = arg max y\u2208Y C c=1 \u03b1 c I y\u2208hc(\u2022) that combines several triplet classifiers using some weights \u03b1 c \u2208 R. Assume that triplet classifiers that abstain on all the training examples have a weight of 0 (that is if h c (x i ) = \u03d1 for all the examples (x i , y i ) \u2208 S then \u03b1 c = 0). Then the probability that H abstains on a test example is bounded as follows:\nP (x,y)\u223cD S [H(x) = \u03d1] \u2265 1 \u2212 p + p (1 \u2212 p) n C . (6)\nProof. The proof is given in Appendix H. We fix the triplets metric to the Euclidean distance. In Figure 2a we consider the noise free setting and we vary the proportion of available triplets from 1 to 10% of all the triplets. In Figure 2b we fix this proportion to 10% and we vary the noise level from 0 to 20%. Iris is a small scale dataset with 150 examples and 4 dimensions. In Figure 2c we fix the proportion of available triplets to 10%, the noise level to 0% and we vary the metric considered to generate the triplets.\nTo understand the implications of this theorem we consider a concrete example.\nExample 1. Assume that we build a linear combination of all possible triplet classifiers, that is C = n(n\u22121)", "publication_ref": [], "figure_ref": ["fig_2", "fig_2", "fig_2"], "table_ref": []}, {"heading": "2", "text": ". Then we have\nlim n\u2192+\u221e P (x,y)\u223cD S [H(x) = \u03d1] \u2265 \uf8f1 \uf8f2 \uf8f3 1 if k < 3 2 , exp(\u22122) if k = 3 2 , 0 if 3 2 < k,(7)\nwhere k is the parameter that controls the probability p that a particular triplet is available in the triplets set T . The bottom line is that when k < 3 2 , that is when we do not have at least \u2126(n \u221a n) random triplets, the learned classifier abstains on all the examples.\nProof. The proof is given in Appendix I.\nTheorem 4 shows that when p and C are too small, then the strong classifier abstains with high probability. However, the theorem does not guarantee that the strong classifier does not abstain when p and C are large. The next theorem takes care of this other direction under slightly stronger assumptions on the weights learned by the algorithm.\nTheorem 5 (Exact bound on the probability that a strong classifier abstains). In Theorem 4, further assume that each triplet classifier that does not abstain on at least one training example has a weight different from 0 (if for at least one example (x i , y i ) \u2208 S we have that h c (x i ) = \u03d1 then \u03b1 c = 0). Then equality holds in Equation (6).\nProof. The proof is given in Appendix J.\nTheorem 5 implies that equality holds in Example 1, thus when C = n(n\u22121) 2 we need at least k > 3 2 , that is at least \u2126(n \u221a n) random triplets, to obtain a classifier that never abstains. In Appendix I we extend Example 1 and we study the limit as n \u2192 \u221e for general values of C and p. We also provide a graphical illustration of the bound and a discussion on how this lower bound compares to existing results [Ailon, 2012;Jamieson and Nowak, 2011;Jain et al., 2016].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We propose an empirical evaluation of TripletBoost. We consider six datasets of varying scales and four baselines. Baselines. First, we consider an embedding approach. We use tSTE [Van Der Maaten and Weinberger, 2012] to embed the triplets in a Euclidean space and we use the 1-nearest neighbour algorithm for classification. We also would like to compare to alternative approaches able to learn directly from triplets (without embedding as a first step). However, to the best of our knowledge, TripletBoost is the only method able to do classification using only passively obtained triplets. The only option is to choose competing methods that have access to more information (providing them an unfair advantage). We settled for a method that uses actively chosen triplets to build a comparison-tree to retrieve nearest neighbours (CompTree) [Haghiri et al., 2017]. Finally, to put the results obtained in the triplet setting in perspective, we consider two methods that use the original Euclidean representations of the data, the 1-nearest neighbour algorithm (1NN) and AdaBoost.SAMME (SAMME) [Hastie et al., 2009]. Implementation details. For tSTE we used the implementation distributed on the authors' website and we set the embedding dimension to the original dimension of the data. This method was only considered for small datasets with less than 10 3 examples as it does not scale well to bigger datasets (Figure 3c). For CompTree we used our own implementation and the leaf size of the comparison tree is set to 1 as this is the only value for which this method can handle noise. For 1NN and SAMME we used sk-learn [Pedregosa et al., 2011]. The number of boosting iterations for SAMME is set to 10 3 . Finally for TripletBoost we set the number of iterations to 10 6 . Datasets and performance measure. We consider six datasets: Iris, Moons, Gisette, Cod-rna, MNIST, and kM-NIST. For each dataset we generate some triplets as in Equation (1) using three metrics: the Euclidean, Cosine, and Cityblock distances (details provided in Appendix C). Given a set of n examples there are n 2 (n\u22121) /2 possible triplets. We consider three different regimes where 1%, 5% or 10% of them are available, and we consider three noise levels where 0%, In Figure 3a we consider the noise free setting and we vary the proportion of triplets available from 1 to 10% of all the triplets. In Figure 3b we fix the proportion of available triplets to 5% and we vary the noise level from 0 to 20%. Figure 3c presents the training time of the triplet-based methods with training samples of increasing sizes on the Moons dataset. The proportion of triplets and the noise level were both set to 10%. The results were obtained on a single core @3.40GHz.\n10% or 20% of them are incorrect. We measure performances in terms of test accuracy (higher is better). For all the experiments we report the mean and standard deviation of 10 repetitions. Since the results are mostly consistent across the datasets we present some representative ones here and defer the others to Appendix C. Small scale regime. We first consider the datasets with less than 10 3 training examples (Figure 2). In this setting our method does not perform well when the number of triplets is too small, but gets closer to the baselines when the number of triplets increases (Figure 2a). This behaviour can be easily explained: when only 1% of the triplets are available, the triplet classifiers abstain on all but 3 or 4 examples on average and thus their performance evaluations are not reliable. Consequently their weights cannot be chosen in a satisfactory manner. This problem vanishes when the number of triplets increases. With increasing noise levels (Figure 2b) one can notice that TripletBoost is more robust than CompTree. Indeed, CompTree generates a comparison-tree based on individual triplet queries, and the greedy decisions in the tree building procedure can easily be misleading in the presence of noise. Finally, our approach is less sensitive than tSTE to changes in the metric that generated the triplets (Figure 2c). Indeed, tSTE assumes that this metric is the Euclidean distance while our approach does not make any assumptions.\nLarge scale regime. On larger datasets (Figure 3), our method does not reach the accuracy of 1NN and SAMME, who exploit a significant amount of extra information. Still, it performs quite well and is competitive with CompTree, the method that uses active rather than passive queries. The ordinal embedding methods cannot compete in this regime, as they are too slow to even finish (Figure 3c). Once again TripletBoost is quite resistant to noise (Figure 3b).", "publication_ref": [], "figure_ref": ["fig_2", "fig_2", "fig_2", "fig_2"], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper we proposed TripletBoost to address the problem of comparison-based classification. It is particularly designed for situations where triplets cannot be queried actively, and we have to live with whatever set of triplets we get. We do not make any geometric assumptions on the underlying space. From a theoretical point of view we have shown that Triplet-Boost is well founded and we proved guarantees on both the training error and the generalization error of the learned classifier. Furthermore we derived a new lower bound showing that to avoid learning a random predictor, at least \u2126(n \u221a n) triplets are needed. In practice we have shown that, given a sufficient amount of triplets, our method is competitive with state of the art methods and that it is quite resistant to noise.\nTo the best of our knowledge, TripletBoost is the first algorithm that is able to handle large scale datasets using only passively obtained triplets. This means that the comparisonbased setting could be considered for problems which were, until now, out of reach. As an illustration, consider a platform where users can watch, comment and rate movies. It is reasonable to assume that triplets of the form movie m i is closer to m j than to m k can be automatically obtained using the ratings of the users, their comments, or their interactions. In this scenario, active learning methods are not applicable since the users might be reluctant to answer solicitations. Similarly, embedding methods are too slow to handle large numbers of movies or users. However, we can use TripletBoost to solve problems such as predicting the genres of the movies. As a proof of concept we considered the 1m movielens dataset [Harper and Konstan, 2016]. It contains 1 million ratings from 6040 users on 3706 movies. We used the users' ratings to obtain some triplets about the movies and TripletBoost to learn a classifier able to predict the genres of a new movie (more details are given in Appendix D). Given a new movie, in \u223c83% of the cases the genre predicted as the most likely one is correct and, on average, the 5 genres predicted as the most likely ones cover \u223c92% of the true genres. A Discussion on Theorem 3\nAt a first glance it seems that the bound presented in Theorem 3 does not depend on m, the number of available triplets. However, this dependency is implicit: m impacts the probability that the training examples are well classified with a large margin \u03b8. If the number of triplets is small, the probability that the training examples are well classified with a given margin is small. This probability increases when the number of triplets increases.\nTo illustrate this behaviour, consider a fixed margin \u03b8. Assume that each of the n training examples is classified by exactly one triplet classifier and that each triplet classifier abstains on all but one example. In this case, the only way to have no error on the training set is to combine the n triplet classifiers that do not abstain. For simplicity consider the case where all the triplet classifiers have uniform weight 1 n in the final classifier. Then the fixed margin will not be achieved when the number of training examples increases. The first term on the right hand side of the generalization bound will be 1, that is the bound predicts that the learned classifier might not generalize. This fact remains true for any weighting scheme. In this example, the best weighting scheme classifies with a margin at least \u03b8, at most ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Illustration and Discussion on the Lower Bound of Theorems 4 and 5", "text": "When n = 100 we illustrate the bound obtained in Theorems 4 and 5 in Figure 4. This figure shows that the transition between abstaining and non-abstaining classifier depends on both the proportion of triplets available and the number of classifiers considered. In particular, when the number of combined classifiers increases, one needs a smaller number of triplets. Conversely when the number of triplets available increases, one can consider combining fewer classifiers. This illustration confirms the result obtained in Example 1 that shows that when C = n(n\u22121) 2 the number of available triplets should at least scale as \u2126(n \u221a n) to obtain a non-trivial classifier that does not abstain on most of the test examples. The number of triplets that are necessary to achieve good classification accuracy might be higher but is lower bounded by this value.\nThis lower bound does not contradict existing results [Ailon, 2012;Jamieson and Nowak, 2011;Jain et al., 2016]. They were developed in the different context of triplet recovery, where the goal is not classification, but to predict the outcome of unobserved triplet questions. For example it has been shown that to exactly recover all the triplets, the number of passively available triplets should scale in \u2126(n 3 ) [Jamieson and Nowak, 2011]. Similarly Jain et al. [2016] derive a finite error bound for approximate recovery of the Euclidean Gram matrix. Our bound shows that, in a classification setting, it might be possible to do better than that. To set a complete picture, one would need to derive an upper bound on the number of triplets necessary for good classification accuracy.", "publication_ref": [], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "C Details on the Experiments", "text": "The characteristics of the different datasets are given in Table 1. To generate the triplets we used three different metrics:\n\u2022 Euclidean distance: d(x, y) = x \u2212 y 2 , \u2022 Cityblock distance: d(x, y) = x \u2212 y 1 , \u2022 Cosine distance: d(x, y) = 1 \u2212 x,y x 2 y 2 .\nThe set T of all triplets is then defined as follows:\nT = {(x i , x j , x k ) : d(x i , x j ) < d(x i , x k )} .\nIn all the experiments we considered subsets T of T by selecting uniformly at random without replacement 1%, 5%, or 10% of the triplets. Similarly we added some noise by randomly swapping 0%, 10%, or 20% of the triplets, that is (\nx i , x k , x j ) \u2208 T while (x i , x j , x k ) \u2208 T .\nThe results that were omitted in the main paper are given in Figures 5 to 16.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": ["tab_2"]}, {"heading": "D Details on the Movielens experiment", "text": "As a proof of concept we considered the 1m movielens dataset [Harper and Konstan, 2016]. This dataset contains 1 million ratings from 6040 users on 3706 movies and each movie has one or several genres (there is 18 genres in total). To demonstrate the interest of our approach we proposed (i) to use the users' ratings to obtain some triplets of the form movie m i is closer to movie m j than to movie m k , and (ii) to use TripletBoost to learn a classifier predicting the genres of the movies.\nTo generate the triplets we propose to consider that movie m i is closer to m j than to m k if, on average, users that rated all three movies rated m i and m j more similarly than m i and m k . The underlying intuition is that users like and dislike genres of movies -for example a user that dislikes horror movies but likes comedy movies will probably give low ratings to The Ring   (2002) and Scream (1996) and a higher rating to The Big Lebowsky (1998). Formally let r u,i be the rating of user u on movie m i and r u,i,j = |r u,i \u2212 r u,j | then the triplet set T is\nT = \uf8f1 \uf8f2 \uf8f3 (m i , m j , m k ) : \uf8eb \uf8ed u\u2208U i,j,k I ru,i,j <r u,i,k \u2212 I ru,i,j >r u,i,k |U i,j,k | \uf8f6 \uf8f8 > 0 \uf8fc \uf8fd \uf8fe\nwhere U i,j,k is the set of users that rated all three movies. Each user has only rated a small number of movies and might give a high, respectively low, rating to a movie with a genre that he usually rates lower, respectively higher. Thus we only have access to a noisy subset of all the possible triplets.\nWe used a random sample of 2595 movies, and their corresponding triplets, to learn a multi-label classifier using TripletBoost (with 10 6 boosting iterations). Since we are in a multi-label setting we would like to predict how relevant each genre is for a new movie rather than a single genre. To obtain such a quantity we can simply ignore the arg max in Equation ( 5) in the main paper to obtain a classifier H(m, y) that predicts the weight of a genre y for a movie m:\nH(m, y) = C c=1 \u03b1 c I hc(m) =\u03d1\u2227y\u2208hc(m) .\nIn the test phase we used the 1111 remaining movies to measure the performance of the learned classifier. First we considered the precision of the genre predicted with the highest weight and obtained a value of 0.83168. It means that in \u223c83% of the cases the genre predicted with the highest weight is correct. We also considered the recall of the 5 genres predicted with the highest weights and obtained a value of 0.92943. It implies that, on average, the 5 genres predicted with the highest weights cover \u223c92% of the genres of the considered movie. In each row we consider a different metric to generate the triplets. In each column we consider a different proportion of triplets available from 1 to 10%. In each plot we vary the noise level from 0 to 20%.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E Proof of Theorem 1", "text": "Theorem 1 (Triplet classifiers and weak learners). Let W c be an empirical distribution over S \u00d7 Y and h c be the corresponding triplet classifier chosen as described in Section 2.2. It holds that:\n1. the error of h c on W c is at most the error of a random predictor and is minimal compared to other labelling strategies, 2. the weight \u03b1 c of the classifier is non-zero if and only if, h c is a weak classifier, that is is strictly better than a random predictor.\nProof. To prove the first claim of the theorem we first study the error of h c on W c and then we show that any labelling strategy different from our would increase the proportion of incorrectly classified examples. To prove the second claim we simply use the definition of the weights \u03b1 c . First claim First of all notice that the probability that a triplet classifier h c makes an error on W c is\nP (xi,y)\u223cWc (I y=yi \u2212 I y =yi ) = I y\u2208hc(xi) \u2212 I y / \u2208hc(xi)\nthat is, by the law of total probabilities:\nP (xi,y)\u223cWc hc(xi) =\u03d1 (I y=yi \u2212 I y =yi ) = I y\u2208hc(xi) \u2212 I y / \u2208hc(xi) P (xi,y)\u223cWc [h c (x i ) = \u03d1] + P (xi,y)\u223cWc hc(xi)=\u03d1 (I y=yi \u2212 I y =yi ) = I y\u2208hc(xi) \u2212 I y / \u2208hc(xi) P (xi,y)\u223cWc [h c (x i ) = \u03d1] .\nNotice that when h c abstains the best possible behaviour is to randomly predict the labels and thus\nP (xi,y)\u223cWc hc(xi)=\u03d1 (I y=yi \u2212 I y =yi ) = I y\u2208hc(xi) \u2212 I y / \u2208hc(xi) = 1 2 .\nHence to show that h c has at most the error of a random predictor it is sufficient to show that\nP (xi,y)\u223cWc hc(xi) =\u03d1 (I y=yi \u2212 I y =yi ) = I y\u2208hc(xi) \u2212 I y / \u2208hc(xi) = W c,\u2212 W c,+ + W c,\u2212 \u2264 1 2\nwhere W c,+ and W c,\u2212 are respectively the proportions of correctly and incorrectly classified examples.\nOn the one hand assume that h c uses x j and x k as reference points. Let S c,j be the set of all the training examples for which h c does not abstain and which are closer to x j than to x k . If y \u2208 o j then we have that:\nzi\u2208Sc,j I y=yi w c,xi,y > zi\u2208Sc,j I y =yi w c,xi,y .(8)\nSimilarly, if y / \u2208 o j then it implies that:\nzi\u2208Sc,j I y =yi w c,xi,y \u2265 zi\u2208Sc,j I y=yi w c,xi,y .(9)\nThe same result holds for o k .\nOn the other hand recall that\nW c,+ = (xi,yi)\u2208S hc(xi) =\u03d1 \uf8eb \uf8ed I yi\u2208hc(xi) w c,xi,yi + y =yi I y / \u2208hc(xi) w c,xi,y \uf8f6 \uf8f8 , W c,\u2212 = (xi,yi)\u2208S hc(xi) =\u03d1 \uf8eb \uf8ed I yi / \u2208hc(xi) w c,xi,yi + y =yi I y\u2208hc(xi) w c,xi,y \uf8f6 \uf8f8 .(10)\nCombining Equations ( 8), ( 9), and ( 10) we obtain\nW c,\u2212 \u2264 W c,+ \u21d4 2W c,\u2212 \u2264 W c,+ + W c,\u2212 \u21d4 W c,\u2212 W c,+ + W c,\u2212 \u2264 1 2\nwhich proves that h c has at most the error of a random predictor.\nTo see that our labelling strategy is optimal just notice that any other labelling strategy would either predict some labels in o j or o k that do not conform with Equation ( 8) or not predict some labels from o j or o k that do not conform with Equation (9). As a consequence W c,\u2212 would be increased and thus the error of h c would also be increased. This concludes the proof of the first claim. Second claim Recall that \u03b1 c is computed as\n\u03b1 c = 1 2 log W c,+ + 1 n W c,\u2212 + 1 n .\nAlso notice that h c is a weak classifier, that is better than a random predictor, when W c,\u2212 < W c,+ . We have that:\nW c,\u2212 < W c,+ \u21d4 W c,\u2212 + 1 n < W c,+ + 1 n \u21d4 1 < W c,+ + 1 n W c,\u2212 + 1 n \u21d4 0 < 1 2 log W c,+ + 1 n W c,\u2212 + 1 n\nand it proves the second claim.\nF Proof of Theorem 2\nTheorem 2 (Reduction of the Training Error). Let S be a set of n examples and T be a set of m triplets (obtained as described in Section 2). Let H(\u2022) be the classifier obtained after C iterations of TripletBoost (Algorithm 1) using S and T as input. It holds that:\nP (x,y)\u2208S [H(x) = y] \u2264 |Y| 2 C c=1 Z c with Z c = (1 \u2212 W c,+ \u2212 W c,\u2212 ) + (W c,+ ) \u2022 Wc,\u2212+ 1 n Wc,++ 1 n + (W c,\u2212 ) \u2022 Wc,++ 1 n Wc,\u2212+ 1 n \u2264 1.\nProof. This result is inherited from AdaBoost.MO and can be obtained directly by applying Theorem 10.4 in Chapter 10 of the book of Schapire and Freund [2012] using a so-called loss-based decoding and noticing that in our caseK = |Y| and \u03c1 = 2.\nFor the sake of completeness we detail the complete proof below. First of all, let F (x, y)\n= C c=1 \u03b1 c I hc(x) =\u03d1 I y\u2208hc(x) \u2212 I y / \u2208hc(x)\nand notice that the strong classifier H(\u2022) can be equivalently rewritten as: + exp (\u2212(I y=y \u2212 I y =y )F (x, y ))\nH(\u2022) = arg max y\u2208Y C c=1 \u03b1 c I hc(\u2022) =\u03d1\u2227y\u2208hc(\u2022) (x > y \u21d4 x \u2212 (B \u2212 x) > y \u2212 (B \u2212 y).) \u21d4 H(\u2022) = arg max y\u2208Y C c=1 \u03b1 c I hc(\u2022) =\u03d1 I y\u2208hc(\u2022) \u2212 I y / \u2208hc(\u2022) (F (\u2022, y) = C c=1 \u03b1 c I hc(\u2022) =\u03d1 I y\u2208hc(\u2022) \u2212 I y / \u2208hc(\u2022) .) \u21d4 H(\u2022) = arg max y\u2208Y F (\u2022, y) \u21d4 H(\u2022) = arg min y\u2208Y \u2212F (\u2022, y) \u21d4 H(\u2022) =\n(\u2200x > 0, x + 1 x \u2265 2.) \u2265 2.\nAssuming that the classifier H(\u2022) makes M errors, we have that:\n2M \u2264 (x,y)\u2208S y \u2208Y exp (\u2212(I y=y \u2212 I y =y )F (x, y )) .(11)\nNow, we need to show that the right hand side of Inequality ( 11) is bounded by n |Y| C c=1 Z c where n is the number of training examples and Z c is the normalization factor used in TripletBoost. Recall that, given an example (x, y) \u2208 S and a label y \u2208 Y, the weight w c+1,x,y , obtained after c iterations of our algorithm, is\nw c+1,x,y = w c,x,y Zc if h c (x) = \u03d1, w c,x,y Zc exp \u2212\u03b1 c (I y =y \u2212 I y =y ) I y \u2208hc(x) \u2212 I y / \u2208hc(x) if h c (x) = \u03d1. \u21d4 w c+1,x,y = w c,x,y Z c exp \u2212 (I y =y \u2212 I y =y ) \u03b1 c I hc(x) =\u03d1 I y \u2208hc(x) \u2212 I y / \u2208hc(x)(12)\nBy recursively using Equation ( 12), we obtain\nw C+1,x,y = w 1,x,y C c=1 Z c C c=1 exp \u2212 (I y =y \u2212 I y =y ) \u03b1 c I hc(x) =\u03d1 I y \u2208hc(x) \u2212 I y / \u2208hc(x)(13)\n(exp(x) exp(y) = exp(x + y).) \u21d4 w C+1,x,y = w 1,x,y C c=1 Z c exp \u2212 (I y =y \u2212 I y =y ) C c=1 \u03b1 c I hc(x) =\u03d1 I y \u2208hc(x) \u2212 I y / \u2208hc(x)(14)\n(F (x, y) = C c=1 \u03b1 c I hc(x) =\u03d1 I y\u2208hc(x) \u2212 I y / \u2208hc(x) .) \u21d4 w C+1,x,y = w 1,x,y C c=1 Z c exp (\u2212 (I y =y \u2212 I y =y ) F (x, y ))(15)\n( (x,y)\u2208S y \u2208Y w C+1,x,y = 1 and \u2200(x, y) \u2208 S, \u2200y \u2208 Y, w 1,x,y = 1 n|Y| .)\n\u21d4 C c=1 Z c = 1 n |Y| (x,y)\u2208S y \u2208Y exp (\u2212 (I y =y \u2212 I y =y ) F (x, y )) . (16\n)\nCombining this result with Inequality ( 11) we obtain:\n2M \u2264 n |Y| C c=1 Z c . Noticing that P (x,y)\u2208S [H(x) = y] = M n\ngives the first part of the theorem:\nP (x,y)\u2208S [H(x) = y] \u2264 |Y| 2 C c=1 Z c .\nIn the last part of the proof we show that Z\nc = (1 \u2212 W c,+ \u2212 W c,\u2212 ) + (W c,+ ) \u2022 Wc,\u2212+ 1 n Wc,++ 1 n + (W c,\u2212 ) \u2022 Wc,++ 1 n Wc,\u2212+ 1 n \u2264 1.\nUsing the result obtained in Equation ( 12) we have that \nW c,+ + 1 n W c,\u2212 + 1 n = (W c,+ ) \u2022 W c,\u2212 + 1 n W c,+ + 1 n + (W c,\u2212 ) \u2022 W c,+ + 1 n W c,\u2212 + 1 n\nSimilarly, using the fact that (x,y)\u2208S y \u2208Y w c,x,y = 1, we obtain\n(x,y)\u2208S hc(x)=\u03d1 y \u2208Y w c,x,y = 1 \u2212 W c,+ \u2212 W c,\u2212 .\nCombining these two results with Equation ( 17) gives the value of Z c :\nZ c = (1 \u2212 W c,+ \u2212 W c,\u2212 ) + (W c,+ ) \u2022 W c,\u2212 + 1 n W c,+ + 1 n + (W c,\u2212 ) \u2022 W c,+ + 1 n W c,\u2212 + 1 n .\nTo show that Z c \u2264 1 note that\nW c,+ + 1 n \u2212 W c,\u2212 + 1 n 2 \u2265 0 \u21d4 W c,+ + 1 n + W c,\u2212 + 1 n \u2212 2 W c,+ + 1 n W c,\u2212 + 1 n \u2265 0 \u21d4 W c,+ + W c,\u2212 + 2 n \u2212 W c,+ + 1 n W c,\u2212 + 1 n W c,+ + 1 n \u2212 W c,\u2212 + 1 n W c,+ + 1 n W c,\u2212 + 1 n \u2265 0 \u21d4 W c,+ + W c,\u2212 + 2 n \u2212 W c,+ W c,\u2212 + 1 n W c,+ + 1 n \u2212 1 n W c,\u2212 + 1 n W c,+ + 1 n \u2212 W c,\u2212 W c,+ + 1 n W c,\u2212 + 1 n \u2212 1 n W c,+ + 1 n W c,\u2212 + 1 n \u2265 0 \u21d4 \u2212W c,+ \u2212 W c,\u2212 \u2212 2 n + W c,+ W c,\u2212 + 1 n W c,+ + 1 n + 1 n W c,\u2212 + 1 n W c,+ + 1 n + W c,\u2212 W c,+ + 1 n W c,\u2212 + 1 n + 1 n W c,+ + 1 n W c,\u2212 + 1 n \u2264 0 \u21d4 1 \u2212 W c,+ \u2212 W c,\u2212 \u2212 2 n + W c,+ W c,\u2212 + 1 n W c,+ + 1 n + 1 n W c,\u2212 + 1 n W c,+ + 1 n + W c,\u2212 W c,+ + 1 n W c,\u2212 + 1 n + 1 n W c,+ + 1 n W c,\u2212 + 1 n \u2264 1 \u21d4 Z c \u2212 2 n + 1 n W c,\u2212 + 1 n W c,+ + 1 n + 1 n W c,+ + 1 n W c,\u2212 + 1 n \u2264 1 \u21d4 Z c \u2212 2 n + 1 n W c,\u2212 + 1 n W c,+ + 1 n + W c,+ + 1 n W c,\u2212 + 1 n \u2264 1 (\u2200x > 0, x + 1 x \u2265 2.) \u21d2 Z c \u2264 1.\nIt concludes the proof of the theorem.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G Proof of Theorem 3", "text": "Theorem 3 (Generalization Guarantees). Let D S be a distribution over X \u00d7 Y, let S be a set of n examples drawn i.i.d. from D S , and let T be a set of m triplets (obtained as described in Section 2). Let H(\u2022) be the classifier obtained after C iterations of TripletBoost (Algorithm 1) using S and T as input. Let H be a set of triplet classifiers as defined in Section 2.1. Then, given a margin parameter \u03b8 > log |H| 16|Y| 2 n and a measure of the confidence of H(\u2022) in its predictions \u03b8 H (x, y), with probability at least 1 \u2212 \u03b4, we have that\nP (x,y)\u223cD S [H(x) = y] \u2264 P (x,y)\u2208S [\u03b8 H (x, y) \u2264 \u03b8] + O \uf8eb \uf8ed log 1 \u03b4 n + log |Y| 2 n\u03b8 2 log |H| log |H| n\u03b8 2 \uf8f6 \uf8f8\nFurthermore we have that\nP (x,y)\u2208S [\u03b8 H (x, y) \u2264 \u03b8] \u2264 |Y| 2 C c=1 Z c W c,+ + 1 n W c,\u2212 + 1 n \u03b8 .\nProof. This result is inherited from AdaBoost.MO and can be obtained directly by following the steps of Exercise 10.3 in Chapter 10 of the book of Schapire and Freund [2012]. Following our notations \u03b8 f,\u03b7 (x, y) is equal to M f,\u03b7 (x, y) and the free parameter n in their proof has been chosen as 16\n\u03b8 2 log 4|Y| 2 n\u03b8 2 log |H|\n. The second part of the theorem can be obtained in a similar way by following the steps of Exercise 10.4 in Chapter 10 of the book of Schapire and Freund [2012]. For the sake of completeness we write the complete proof below.\nFirst, we define several quantities used throughout. Recall that our approach outputs a strong classifier H(\u2022) defined as\nH(x) = arg max y\u2208Y C c=1 \u03b1 c I hc(x) =\u03d1\u2227y\u2208hc(x)(18)\nAs in the proof of Theorem 2, let F (x, y) = For f \u2208 co (H), \u03b7 > 0, and (x, y) \u2208 X \u00d7 Y, let\n\u03bd f,\u03b7 (x, y) = \u2212 1 \u03b7 log \uf8eb \uf8ed 1 |Y| y \u2208Y exp \u2212 \u03b7 (I y=y \u2212 I y =y ) f (x, y ) \uf8f6 \uf8f8 . (22\n)\nWe use this quantity to define the following confidence margin for (x, y) \u2208 X \u00d7 Y:\n\u03b8 H (x, y) = \u03b8 f,\u03b7 (x, y) = 1 2 \u03bd f,\u03b7 (x, y) \u2212 max y =y \u03bd f,\u03b7 (x, y ) . (23\n)\nBound on the true risk of H(\u2022).\nNote that \u03b8 f,\u03b7 (x, y) \u2208 [\u22121, 1] since f (x, y) \u2208 [\u22121, 1]\n. Similarly, given an example (x, y) \u2208 X \u00d7 Y, if H(\u2022) makes an error then it implies that there exists a label = y such that \nF (x, y) \u2264 F (x, ) \u21d4 f (x, y) \u2264 f (x, ) (\u03b7 > 0.) \u21d4 \u03b7f (x, y) \u2264 \u03b7f (x\n\u21d4 y \u2208Y exp \u2212 \u03b7 (I y=y \u2212 I y =y ) f (x, y ) \u2265 y \u2208Y exp \u2212 \u03b7 (I =y \u2212 I =y ) f (x, y ) \u21d4 log \uf8eb \uf8ed 1 |Y| y \u2208Y exp \u2212 \u03b7 (I y=y \u2212 I y =y ) f (x, y ) \uf8f6 \uf8f8 \u2265 log \uf8eb \uf8ed 1 |Y| y \u2208Y exp \u2212 \u03b7 (I =y \u2212 I =y ) f (x, y ) \uf8f6 \uf8f8 \u21d4 \u03bd f,\u03b7 (x, y) \u2264 \u03bd f,\u03b7 (x, ) \u21d4 \u03bd f,\u03b7 (x, y) \u2212 \u03bd f,\u03b7 (x, ) \u2264 0 \u21d4 \u03b8 f,\u03b7 (x, y) \u2264 0. It implies that P (x,y)\u223cD S [H(x) = y] = P (x,y)\u223cD S [\u03b8 f,\u03b7 (x, y) \u2264 0]\nand hence, in the following, we prove an upper bound on P (x,y)\u223cD S [\u03b8 f,\u03b7 (x, y) \u2264 0]. and that given x \u2208 X and y \u2208 Y fixed, Ef f (x, y ) = f (x, y ). Hence, using Hoeffding's inequality, it holds that\nP f f (x, y ) \u2212 f (x, y ) \u2265 \u03b8 4 \u2264 2 exp \u2212T \u03b8 2 32 .\nThen, by the union bound, it holds that\nP f \u2203y \u2208 Y : f (x, y ) \u2212 f (x, y ) \u2265 \u03b8 4 \u2264 2 |Y| exp \u2212T \u03b8 2 32 . (24\n)\nConsider the following technical fact from Schapire and Freund [2012]. Define the grid\n\u03b5 \u03b8 = 4 log |Y| i\u03b8 : i = 1, . . . , 8 log |Y| \u03b8 2 \u21d4 \u03bd f,\u03b7 (x, y) \u2212 \u03b8 \u2264 max =y \u03bd f,\u03b7 (x, ) + \u03b8 \u21d4 \u2212 log \uf8eb \uf8ed 1 |Y| y \u2208Y exp \u2212 \u03b7 (I y=y \u2212 I y =y ) f (x, y ) \uf8f6 \uf8f8 \u2212 \u03b7\u03b8 \u2264 max =y \u2212 log \uf8eb \uf8ed 1 |Y| y \u2208Y exp \u2212 \u03b7 (I =y \u2212 I =y ) f (x, y ) \uf8f6 \uf8f8 + \u03b7\u03b8 \u21d4 log \uf8eb \uf8ed 1 |Y| y \u2208Y exp \u2212 \u03b7 (I y=y \u2212 I y =y ) f (x, y ) \uf8f6 \uf8f8 + \u03b7\u03b8 \u2265 min =y log \uf8eb \uf8ed 1 |Y| y \u2208Y exp \u2212 \u03b7 (I =y \u2212 I =y ) f (x, y ) \uf8f6 \uf8f8 \u2212 \u03b7\u03b8 \u21d4 y \u2208Y exp (\u03b7\u03b8) exp \u2212 \u03b7 (I y=y \u2212 I y =y ) f (x, y ) \u2265 min =y y \u2208Y exp (\u2212\u03b7\u03b8) exp \u2212 \u03b7 (I =y \u2212 I =y ) f (x, y ) \u21d4 y \u2208Y exp \u2212 \u03b7 (I y=y \u2212 I y =y ) f (x, y ) + \u03b7\u03b8 \u2265 min =y y \u2208Y exp \u2212 \u03b7 (I =y \u2212 I =y ) f (x, y ) \u2212 \u03b7\u03b8 \u21d4 y \u2208Y exp \u2212 z(y ) \u2265 min =y y \u2208Y exp \u2212 z (y )\nThen, when the strong classifier violates the margin, that is \u03b8 f,\u03b7 (x, y) \u2264 \u03b8, we have that:\ny \u2208Y exp \u2212 z(y ) \u2265 1 2 y \u2208Y exp \u2212 z(y ) + 1 2 min =y y \u2208Y exp \u2212 z (y ) \u2265 min =y \uf8ee \uf8f0 1 2 y \u2208Y exp \u2212 z(y ) + 1 2 y \u2208Y exp \u2212 z (y ) \uf8f9 \uf8fb \u2265 1 2 min =y y \u2208Y exp \u2212 z(y ) + exp \u2212 z (y ) \u2265 1 2 min =y y \u2208Y exp \u2212 \u03b7 (I y=y \u2212 I y =y ) f (x, y ) + \u03b7\u03b8 + exp \u2212 \u03b7 (I =y \u2212 I =y ) f (x, y ) \u2212 \u03b7\u03b8\n(Dropping all the terms that do not depend on y and since \u2200x \u2208 R, exp x > 0.)\n\u2265 1 2 min =y y \u2208{y, } exp \u2212 \u03b7 (I y=y \u2212 I y =y ) f (x, y ) + \u03b7\u03b8 + exp \u2212 \u03b7 (I =y \u2212 I =y ) f (x, y ) \u2212 \u03b7\u03b8 (\u2200y \u2208 {y, } , (I =y \u2212 I =y ) = \u2212(I y=y \u2212 I y =y ).) \u2265 1 2 min =y y \u2208{y, } exp \u2212 \u03b7 (I y=y \u2212 I y =y ) f (x, y ) + \u03b7\u03b8 + exp \u03b7 (I y=y \u2212 I y =y ) f (x, y ) \u2212 \u03b7\u03b8 \u2265 1 2 min =y y \u2208{y, } exp \u2212 z(y ) + exp z(y ) (\u2200x > 0, x + 1 x \u2265 2.) \u2265 2 Now assume that the strong classifier violates the margin M times, that is \u03b8 f,\u03b7 (x, y) \u2264 \u03b8 for M examples, then 2M \u2264 (x,y)\u2208S y \u2208Y exp \u2212 z(y ) \u21d4 2M \u2264 (x,y)\u2208S y \u2208Y exp \u2212 \u03b7 (I y=y \u2212 I y =y ) f (x, y ) + \u03b7\u03b8 \u21d4 2M \u2264 (x,y)\u2208S y \u2208Y exp \u2212 \u03b7 (I y=y \u2212 I y =y ) f (x, y ) exp \u03b7\u03b8 (Let \u03b7 = c \u03b1 c and recall that ( c \u03b1 c ) f (x, y) = F (x, y).) \u21d4 2M \u2264 (x,y)\u2208S y \u2208Y exp \u2212 (I y=y \u2212 I y =y ) F (x, y ) exp c \u03b1 c \u03b8 \u21d4 2M \u2264 exp c \u03b1 c \u03b8 (x,y)\u2208S y \u2208Y exp \u2212 (I y=y \u2212 I y =y ) F (x, y ) Recall that (x,y)\u2208S y \u2208Y exp \u2212 (I y=y \u2212 I y =y ) F (x, y ) = n |Y| C c=1 Z c as shown in Equation (16) in the proof of Theorem 2. 2M \u2264 exp c \u03b1 c \u03b8 n |Y| C c=1 Z c \u21d4 2M \u2264 exp c \u03b1 c \u03b8 n |Y| C c=1 Z c \u21d4 2M \u2264 n |Y| C c=1 Z c exp (\u03b1 c \u03b8) \u21d4 M n \u2264 |Y| 2 C c=1 Z c exp (\u03b1 c \u03b8)\nReplacing \u03b1 c by its value, that is \u03b1 c = 1 2 log  be the probability that a triplet is available in the triplet set T and C = n \u03b2 2 with \u03b2 \u2208 0, 1 + log (n\u22121) log (n)\nbe the number of classifiers combined in the learned classifier. Let A be any algorithm learning a classifier H(\u2022) = arg max y\u2208Y C c=1 \u03b1 c I y\u2208hc(\u2022) that combines several triplet classifiers using some weights \u03b1 c \u2208 R. Assume that triplet classifiers that abstain on all the training examples have a weight of 0 (that is if h c (x i ) = \u03d1 for all the examples (x i , y i ) \u2208 S then \u03b1 c = 0). Then the probability that H abstains on a test example is bounded as follows:\nP (x,y)\u223cD S [H(x) = \u03d1] \u2265 1 \u2212 p + p (1 \u2212 p) n C .(38)\nProof. Recall that H(\u2022) = arg max y\u2208Y C c=1 \u03b1 c I y\u2208hc(\u2022) . Each component \u03b1 c I y\u2208hc(\u2022) of H abstains for a given example x if h c (x) = \u03d1 or \u03b1 c = 0. Overall H abstains if all its components also abstain and we bound this probability here.\nRecall that S = {(x i , y i )} n i=1 is a set of n examples drawn i.i.d. from an unknown distribution D S over the space X \u00d7 Y where |Y| < \u221e and recall that C is the number of classifiers selected in H. Finally recall that each triplet is obtained with probability p independently of the other triplets. It implies that each triplet classifier h c (\u2022) abstains with probability q = 1 \u2212 p independently of the other triplet classifiers.\nFirst we start by defining several random variables. Let Y 1,1 , . . . , Y n,C be nC independent random variables encoding the event that a classifier abstains on one example:\nY i,c = 0 if h c abstains for example x i , 1 otherwise. (39\n)\nFrom the definition of the classifiers each of these random variables follow a Bernoulli distribution B (1, p). Let Y x,1 , . . . , Y x,C be C independent random variables encoding the event that a classifier abstains on a new example x:\nY x,c = 0 if h c abstains for example x, 1 otherwise. (40\n)\nFrom the definition of the classifiers each of these random variables follow a Bernoulli distribution B (1, p). Let V 1 , . . . , V C be C independent random variables encoding the probability that a triplet classifier abstains on all the training examples:\nV c = n i=1 Y i,c .(41)\nAs a sum of n independent Bernoulli trials, these random variables follow a Binomial distribution B (n, p).\nUsing the law of total probabilities we have that: Note that from the assumption that \u2200(x i , y i ) \u2208 S, h c (x i ) = \u03d1 then \u03b1 c = 0 and the definition of the random variables Y x,\u2022 we have that:\nP (x,y)\u223cD S H(x) = \u03d1 C c=1 V c Y x,c = 0 = 1.\nSimilarly, we have that: Let U 1 , . . . , U C be C independent random variables such that:\nU c = 1 if V c Y x,c = 0, 0 otherwise. . (44\n)\nThese random variables follow a Bernoulli distribution B (1, (1 \u2212 p) + p(1 \u2212 p) n ). To see that, note that V c and Y x,c are independent and thus we have that: \nP (x,y)\u223cD S [V\n= 1 \u2212 1 \u2212 n 0 p 0 (1 \u2212 p) n\u22120 p = 1 \u2212 (1 \u2212 (1 \u2212 p) n ) p = (1 \u2212 p) + p (1 \u2212 p) n .\nThe r.h.s. of Inequality (43) can then be written as:\nP (x,y)\u223cD S C c=1 V c Y x,c = 0 = P (x,y)\u223cD S C c=1 U c = c(45)\nwhich, by definition of the random variables U \u2022 , corresponds to the probability of obtaining c successes among c Bernoulli trials. In other words the random variable U = C c=1 U j follows a Binomial distribution B (c, (1 \u2212 p) + p(1 \u2212 p) n ). Following this we have that:\nP (x,y)\u223cD S C c=1 V c Y x,c = 0 = c c ((1 \u2212 p) + p(1 \u2212 p) n ) C (1 \u2212 (1 \u2212 p) + p(1 \u2212 p) n ) C\u2212C (46) = ((1 \u2212 p) + p(1 \u2212 p) n ) C . (47\n)\nHence plugging this result in Inequality (43) we have that:\nP (x,y)\u223cD S [H(x) = \u03d1] \u2265 ((1 \u2212 p) + p(1 \u2212 p) n ) C\nwhich concludes the proof.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "I Proof of Example 1", "text": "Note that here we prove a more general version of Example 1 than the one presented in the main paper. The latter follows directly by choosing \u03b2 = 1 + log (n\u22121) log (n) .\nIf \u03b2 = 1 then:\nlim n\u2192+\u221e \u2212n k\u22123+\u03b2 1 \u2212 1 \u2212 2n k\u22123 n = \uf8f1 \uf8f2 \uf8f3 0 if 0 \u2264 k < 2, exp(\u22122) \u2212 1 if k = 2, \u2212\u221e if 2 < k < 3 \u2212 log(2) log(n) . If 1 < \u03b2 \u2264 2 then: lim n\u2192+\u221e \u2212n k\u22123+\u03b2 1 \u2212 1 \u2212 2n k\u22123 n = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 0 if 0 \u2264 k < 5\u2212\u03b2 2 , \u22122 if k = 5\u2212\u03b2 2 , \u2212\u221e if 5\u2212\u03b2 2 < k < 3 \u2212 log(2) log(n) . (54\n)\nRemaining term, with l > 1. We can rewrite the remaining term as follows:\n\u2212 \u221e l=2 n \u03b2 2l 2n k\u22123 \u2212 2n k\u22123 1 \u2212 2n k\u22123 n l = \u2212 \u221e l=2 n \u03b2 2l 2n kl\u22123l 1 \u2212 1 \u2212 2n k\u22123 n l = \u2212 \u221e l=2 1 \u2212 1 \u2212 2n k\u22123 n l l n (k\u22123)l+\u03b2\nNotice that \u2200k \u2208 0, 3 \u2212 log(2) log(n) , \u2200l \u2265 2, \u2200n \u2265 2 we have that 1 \u2212 1 \u2212 2n k\u22123 n l \u2208 [0, 1]. Following this we have:\nl > \u03b2 3 \u2212 k \u21d2 lim n\u2192+\u221e \u2212n (k\u22123)l+\u03b2 = 0 \u21d2 lim n\u2192+\u221e \u2212 1 \u2212 1 \u2212 2n k\u22123 n l l n (k\u22123)l+\u03b2 = 0 l \u2264 \u03b2 3 \u2212 k \u21d2 lim n\u2192+\u221e \u2212n (k\u22123)l+\u03b2 \u2264 0 \u21d2 lim n\u2192+\u221e \u2212 1 \u2212 1 \u2212 2n k\u22123 n l l n (k\u22123)l+\u03b2 \u2264 0.\nFrom this, noticing that k < 6\u2212\u03b2 2 implies \u03b2 3\u2212k < 2, we obtain:\n0 \u2264 k < 6 \u2212 \u03b2 2 \u21d2 lim n\u2192+\u221e \u2212 \u221e l=2 n 2 2l 2n k\u22123 \u2212 2n k\u22123 1 \u2212 2n k\u22123 n l = 0 (55) 6 \u2212 \u03b2 2 \u2264 k < 3 \u2212 log(2) log(n) \u21d2 lim n\u2192+\u221e \u2212 \u221e l=2 n 2 2l 2n k\u22123 \u2212 2n k\u22123 1 \u2212 2n k\u22123 n l \u2264 0 (56)\nLimit of Equation (50). We now combine the limits obtained in Equations ( 54), ( 55) and ( 56) to obtain the following limits: If 0 \u2264 \u03b2 < 1 then 6\u2212\u03b2 2 > 3 \u2212 \u03b2 and thus the remaining term does not change the limit of the main term which implies:\nlim n\u2192+\u221e \u2212 n \u03b2 2 2n k\u22123 \u2212 2n k\u22123 1 \u2212 2n k\u22123 n \u2212 \u221e l=2 n \u03b2 2l 2n k\u22123 \u2212 2n k\u22123 1 \u2212 2n k\u22123 n l = \uf8f1 \uf8f2 \uf8f3 0 if 0 \u2264 k < 3 \u2212 \u03b2, \u22121 if k = 3 \u2212 \u03b2, \u2212\u221e if 3 \u2212 \u03b2 < k < 3 \u2212 log(2)\nlog(n) .\nIf \u03b2 = 1 then 6\u2212\u03b2 2 > 2 and thus the remaining term does not change the limit of the main term which implies:\nlim n\u2192+\u221e \u2212 n \u03b2 2 2n k\u22123 \u2212 2n k\u22123 1 \u2212 2n k\u22123 n \u2212 \u221e l=2 n \u03b2 2l 2n k\u22123 \u2212 2n k\u22123 1 \u2212 2n k\u22123 n l = \uf8f1 \uf8f2 \uf8f3 0 if 0 \u2264 k < 2, exp(\u22122) \u2212 1 if k = 2, \u2212\u221e if 2 < k < 3 \u2212 log(2) log(n) .\nIf 1 < \u03b2 \u2264 1 + log(n\u22121) log(n) then 6\u2212\u03b2 2 > 5\u2212\u03b2 2 and thus the remaining term does not change the limit of the main term which implies:\nlim n\u2192+\u221e \u2212 n \u03b2 2 2n k\u22123 \u2212 2n k\u22123 1 \u2212 2n k\u22123 n \u2212 \u221e l=2 n \u03b2 2l 2n k\u22123 \u2212 2n k\u22123 1 \u2212 2n k\u22123 n l = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 0 if 0 \u2264 k < 5\u2212\u03b2 2 , \u22122 if k = 5\u2212\u03b2 2 , \u2212\u221e if 5\u2212\u03b2 2 < k < 3 \u2212 log(2) log(n) .\nPlugging this result back into Equation ( 50) gives the lemma.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "J Proof of Theorem 5", "text": "Theorem 5 (Exact bound on the probability that a strong classifier abstains). In Theorem 4, further assume that each triplet classifier that does not abstain on at least one training example has a weight different from 0 (if for at least one example (x i , y i ) \u2208 S we have that h c (x i ) = \u03d1 then \u03b1 c = 0). Then equality holds in Equation (38).\nProof. The theorem follows directly by noticing that, in the proof of Theorem 4, if we further assume that each triplet classifier that does not abstain on at least one training example has a weight different from 0 (if for at least one example (x i , y i ) \u2208 S we have that h c (x i ) = \u03d1 then \u03b1 c = 0) then we have equality in Equation ( 42):\nP (x,y)\u223cD S H(x) = \u03d1 C c=1 V c Y x,c = 0 = 0.\nAnd thus we also have that:\nP (x,y)\u223cD S [H(x) = \u03d1] = P (x,y)\u223cD S C c=1 V c Y x,c = 0 .\nNote that, in the very unlikely event that, in one of the iterations of TripletBoost, the selected triplet classifier has an error of exactly 1 /2, Theorem 5 might not hold for our method.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "Ulrike von Luxburg acknowledges funding by the DFG through the Institutional Strategy of the University of T\u00fcbingen (DFG, ZUK 63) and the Cluster of Excellence EXC 2064/1, project number 390727645.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Example 1. In Theorem 4, we choose different values for p = 2n k\u22123 the probability of that a triplet is available and C = n \u03b2 2 the number of combined classifiers. Then we take the limit as n \u2192 \u221e to obtain the following results.\nIf 0 \u2264 \u03b2 < 1 then: 2) log(n) . If \u03b2 = 1 then:\nIf 1 < \u03b2 \u2264 1 + log (n\u22121) log (n) then:\nProof. Replacing p and C by their values in Equation ( 38) of Theorem 4 and applying Lemma 1 gives the example.\nThe next Lemma is a technical result used in the proof of Theorem 4.\nLemma 1 (Limit of the right hand side of Theorem 4.).\nand\nthen the following limits hold: If 0 \u2264 \u03b2 < 1 then:\nProof. We are looking for the limit when n \u2192 \u221e of the function defined in Equation ( 49).\nIn the following we study the limits of the two underlined terms in Equation ( 50). Main term, with l = 1. To obtain the limit of this term we proceed in two steps. First we decompose it in a product of two sub terms and we study their individual limits. Then we consider the indeterminate forms that arise after considering the products of the limits and we obtain the correct limits by upper and lower bounding the term and showing that the limits of the two bounds are identical.\nWe are interested in:\nwe have:\n\u2022 On the other hand, we have:\nTo see that note that:\nwhich is an indeterminate form that can be solved using l'H\u00f4pital's rule. Given the derivatives:\nCombining Limits ( 51) and ( 52) we obtain the following limits: If 0 \u2264 \u03b2 < 1 then:\nThe only indeterminate form arises when 1 < \u03b2 \u2264 2 and 3 \u2212 \u03b2 < k < 2. To solve this indeterminate form we propose to upper and lower bound \u2212n k\u22123+\u03b2 1 \u2212 1 \u2212 2n k\u22123 n and to show that the limits coincides. Notice that, given our assumptions on k we have that \u22122n k\u22123 \u2208 (\u22121, 0] and hence, since n \u2265 2 we can apply Bernoulli's inequalities 2 to obtain:\nFurthermore, recalling that we are only interested in the case where 1 < \u03b2 \u2264 2 and 3 \u2212 \u03b2 < k < 2, we have the following limits:\nHence we obtain:\nCombining this result with Equation (53) gives the limit of the main term:\nIf 0 \u2264 \u03b2 < 1 then:", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Generalized non-metric multidimensional scaling", "journal": "", "year": "2007", "authors": "Sameer Agarwal; Josh Wills; Lawrence Cayton; Gert Lanckriet; David Kriegman; Serge Belongie"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Example of a triplet classifier. Given two reference points xj and x k , the space is divided in two half-spaces: examples closer to xj and examples closer to x k . Triplet information is enough to reveal which half-space a point xi is in.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "x match a triplet classifier in H. A naive implementation would compare each triplet in T x to each triplet classifier, which can be as expensive as O(|T x | C). Fortunately, by first sorting the triplets and the triplet classifiers, a far more reasonable complexity of O(|T x | log(|T x |) + C log(C)) can be achieved.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure2: Moons is a small scale dataset with 500 examples and 2 dimensions. We fix the triplets metric to the Euclidean distance. In Figure2awe consider the noise free setting and we vary the proportion of available triplets from 1 to 10% of all the triplets. In Figure2bwe fix this proportion to 10% and we vary the noise level from 0 to 20%. Iris is a small scale dataset with 150 examples and 4 dimensions. In Figure2cwe fix the proportion of available triplets to 10%, the noise level to 0% and we vary the metric considered to generate the triplets.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure3: The Gisette dataset has 7000 examples and 5000 dimensions. In Figure3awe consider the noise free setting and we vary the proportion of triplets available from 1 to 10% of all the triplets. In Figure3bwe fix the proportion of available triplets to 5% and we vary the noise level from 0 to 20%. Figure3cpresents the training time of the triplet-based methods with training samples of increasing sizes on the Moons dataset. The proportion of triplets and the noise level were both set to 10%. The results were obtained on a single core @3.40GHz.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "1 \u03b8 training examples. When the number of triplets increases, the value of Z c decreases, because the proportion of examples on which the selected triplet classifier abstains, 1 \u2212 W c,+ \u2212 W c,\u2212 , decreases. Similarly, the proportion of training examples that are classified with a margin at least \u03b8 increases. Hence the first term on the right hand side of the generalization bound is greatly reduced and the learned classifier generalizes well.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": ".Figure 4 :4Figure 4: Illustration of the bound in Theorems 4 and 5 when n = 100.", "figure_data": ""}, {"figure_label": "5678910111213141516", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 5 :Figure 6 :Figure 7 :Figure 8 :Figure 9 :Figure 10 :Figure 11 :Figure 12 :Figure 13 :Figure 14 :Figure 15 :Figure 16 :5678910111213141516Figure5: Results on the Iris dataset. In each row we consider a different metric to generate the triplets. In each column we consider a different noise level from 0 to 20%. In each plot we vary the proportion of triplets available from 1 to 10%.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "arg min y\u2208Y exp (\u2212F (\u2022, y)) (arg min x exp (\u2212f (x)) = arg min x \u2212 exp (f (x)).) \u21d4 H(\u2022) = arg min y\u2208Y exp (\u2212F (\u2022, y)) \u2212 exp (F (\u2022, y)) (Adding a constant does not change the value of arg min.) \u21d4 H(\u2022) = arg min y\u2208Y exp (\u2212F (\u2022, y)) \u2212 exp (F (\u2022, y)) + y \u2208Y exp (F (\u2022, y )) \u21d4 H(\u2022) = arg min y\u2208Y y \u2208Y exp (\u2212(I y=y \u2212 I y =y )F (\u2022, y )) . Given an example (x, y) \u2208 S, the classifier H(\u2022) makes an error if and only if there exists a label = y such that y \u2208Y exp (\u2212(I =y \u2212 I =y )F (x, y )) \u2264 y \u2208Y exp (\u2212(I y=y \u2212 I y =y )F (x, y )) . It implies that, when H(\u2022) makes an error y \u2208Y exp (\u2212(I y=y \u2212 I y =y )F (x, y )) y=y \u2212 I y =y )F (x, y )) (Dropping all the terms that do not depend on y and since \u2200x \u2208 R, exp x > 0.) \u2265 1 2 y \u2208{y, } [exp (\u2212(I =y \u2212 I =y )F (x, y )) + exp (\u2212(I y=y \u2212 I y =y )F (x, y ))] (\u2200y \u2208 {y, } , (I =y \u2212 I =y ) = \u2212(I y=y \u2212 I y =y ).) I y=y \u2212 I y =y )F (x, y ))", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "wc+1,x,y = w c,x,y Z c exp \u2212 (I y =y \u2212 I y =y ) \u03b1 c I hc(x) =\u03d1 I y \u2208hc(x) \u2212 I y / \u2208hc(x) ( (x,y)\u2208S y \u2208Y w c+1,x,y = 1.) \u21d4 Z c = (x,y)\u2208S y \u2208Y w c,x,y exp \u2212 (I y =y \u2212 I y =y ) \u03b1 c I hc(x) =\u03d1 I y \u2208hc(x) \u2212 I y / \u2208hc(x) x,y exp \u2212 (I y =y \u2212 I y =y ) \u03b1 c I y \u2208hc(x) \u2212 I y / \u2208hc(x) (17) Notice that W c,+ and W c,\u2212 , the weights of correctly and incorrectly classified examples, can be rewritten as W c,+ = (x,y)\u2208S hc(x) =\u03d1 y \u2208Y I y =y\u2227y \u2208hc(x) + I y =y\u2227y / \u2208hc(x) w c,x,y , W c,\u2212 = (x,y)\u2208S hc(x) =\u03d1 y \u2208Y I y =y\u2227y / \u2208hc(x) + I y =y\u2227y \u2208hc(x) w c,x,y . x,y exp \u2212 (I y =y \u2212 I y =y ) \u03b1 c I y \u2208hc(x) \u2212 I y / \u2208hc(x) = (x,y)\u2208S hc(x) =\u03d1 y \u2208Y w c,x,y exp \u2212 (I y =y \u2212 I y =y ) 1 2 log W c,+ + 1 n W c,\u2212 + 1 n I y \u2208hc(x) \u2212 I y / \u2208hc(x) (exp(a log(b)) = b a ) I y =y \u2212I y =y ) 1 2 (I y \u2208hc(x) \u2212I y / \u2208hc(x) )) (exp(a log(b)) = b a ) = (x,y)\u2208S hc(x) =\u03d1 y \u2208Y w c,x,y I y =y\u2227y \u2208hc(x) + I y =y\u2227y / x,y I y =y\u2227y / \u2208hc(x) + I y =y\u2227y \u2208hc(x)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Cc=1 \u03b1 c I hc(x) =\u03d1 I y\u2208hc(x) \u2212 I y / \u2208hc(x) and let f : X \u00d7 Y \u2192 R be the normalized version of F :f (x, y) = C c=1 a c I hc(x) =\u03d1 I y\u2208hc(x) \u2212 I y / \u2208hc(x) . (19)where a c = \u03b1c C c=1 \u03b1c \u2208 [0, 1] since we have that \u03b1 c \u2265 0 because of our labelling strategy for the triplet classifiers. From the proof of Theorem 2, recall that we have H(x) = arg maxy\u2208Y F (x, y) = arg max y\u2208Y f (x, y). Notice that f is part of the following convex set: co (H) = f : x, y \u2192 C c=1 a c I hc(x) =\u03d1 I y\u2208hc(x) \u2212 I y / \u2208hc(x) a 1 , . . . , a C \u2265 0; C c=1 a c = 1; h 1 , . . . h C \u2208 H . (20) Let us define the following set of unweighted averages of size T : un T (H) = f : x, y \u2192 1 T T t=1 I ht(x) =\u03d1 I y\u2208ht(x) \u2212 I y / \u2208ht(x) h 1 , . . . h T \u2208 H . (21)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": ", ) \u21d4 exp (\u2212\u03b7f (x, y)) \u2212 exp (\u03b7f (x, y)) \u2265 exp (\u2212\u03b7f (x, )) \u2212 exp (\u03b7f (x, )) \u21d4 exp (\u2212\u03b7f (x, y)) \u2212 exp (\u03b7f (x, y)) + y \u2208Y exp (\u03b7f (x, y )) \u2265 exp (\u2212\u03b7f (x, )) \u2212 exp (\u03b7f (x, )) + y \u2208Y exp (\u03b7f (x, y ))", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "To this end, let us first definef (x, y) =\u03d1 I y\u2208ht(x) \u2212 I y / \u2208ht(x) \u2208 un T withh 1 , . . . ,h T drawn from H independently at random following the probability distribution defined by the weights a 1 , . . . , a C . Notice thatf (x, y) \u2208 [\u22121, 1]", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "bound on the probability that a strong classifier abstains). Let n \u2265 2 be the number of training examples, p = 2n k n 3 with k \u2208 0, 3 \u2212 log(2) log(n)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "c Y x,c = 0] = 1 \u2212 P (x,y)\u223cD S [V c Y x,c = 0] = 1 \u2212 P (x,y)\u223cD S [{V c = 0} \u2229 {Y x,c = 0}] = 1 \u2212 P (x,y)\u223cD S [V c = 0] P (x,y)\u223cD S [Y x,c = 0] (Y x,c follows a Bernoulli distribution B (1, p).) = 1 \u2212 1 \u2212 P (x,y)\u223cD S [V c = 0] p (V c follows a Binomial distribution B (n, p).)", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Nir Ailon. An active learning algorithm for ranking from pairwise preferences with an almost optimal query complexity. Journal of Machine Learning Research, 13(Jan), 2012. Ehsan Amid and Antti Ukkonen. Multiview triplet embedding: Learning attributes in multiple maps. In International Conference on Machine Learning, 2015. Sanjoy Dasgupta, and Amin Karbasi. Comparison based learning from weak oracles. arXiv preprint arXiv:1802.06942v1, 2018. Matth\u00e4us Kleindessner and Ulrike Luxburg. Dimensionality estimation without distances. In Artificial Intelligence and Statistics, 2015. Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 1998.", "figure_data": "F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,Aur\u00e9lien Bellet, Amaury Habrard, and Marc Sebban. Metric learning. Synthesis Lectures on Artificial Intelligence and Machine Learning, 9(1), 2015.R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour-napeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12, 2011.Leo Breiman. Prediction games and arcing algorithms. Neu-ral computation, 11(7), 1999.Robert E Schapire and Yoav Freund. Boosting: Foundations and algorithms. MIT press, 2012.Robert E Schapire and Yoram Singer. Improved boostingalgorithms using confidence-rated predictions. Machinelearning, 37(3), 1999.Robert E Schapire and Yoram Singer. Boostexter: Aboosting-based system for text categorization. Machinelearning, 39(2-3), 2000.Lalit Jain, Kevin G Jamieson, and Rob Nowak. Finite sampleprediction and recovery bounds for ordinal embedding. InNeural Information Processing Systems, 2016.Kevin G Jamieson and Robert D Nowak. Low-dimensionalembedding using adaptively selected ordinal data. InConference on Communication, Control, and Computing,2011."}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "d(x i , x j ) < d(x i , x k ) .", "formula_coordinates": [2.0, 130.17, 388.88, 90.67, 9.65]}, {"formula_id": "formula_1", "formula_text": "Input: S = {(x i , y i )} n i=1 a set of n examples, T = {(x i , x j , x k ) : x j = x k } a set of m triplets.", "formula_coordinates": [2.0, 315.0, 68.43, 222.86, 23.58]}, {"formula_id": "formula_2", "formula_text": "\u2200(x i , y i ) \u2208 S, \u2200y \u2208 Y, w 1,xi,y = 1 n|Y| . 2: for c = 1, . . . , C do 3:", "formula_coordinates": [2.0, 319.98, 115.35, 166.51, 34.62]}, {"formula_id": "formula_3", "formula_text": "7: return H(\u2022) = arg max y\u2208Y C c=1 \u03b1 c I hc(\u2022) =\u03d1\u2227y\u2208hc(\u2022)", "formula_coordinates": [2.0, 319.98, 215.92, 206.0, 30.2]}, {"formula_id": "formula_4", "formula_text": "h j,k (x) = o j if (x, x j , x k ) \u2208 T , o k if (x, x k , x j ) \u2208 T , \u03d1 otherwise.", "formula_coordinates": [2.0, 358.94, 304.61, 148.94, 30.87]}, {"formula_id": "formula_5", "formula_text": "o j = y : (xi,yi)\u2208S, (xi,xj ,x k )\u2208T (I y=yi \u2212 I y =yi ) w c,xi,y > 0 . (2)", "formula_coordinates": [3.0, 62.37, 203.02, 234.62, 29.19]}, {"formula_id": "formula_6", "formula_text": "W c,+ = (xi,yi)\u2208S, hc(xi) =\u03d1 I yi\u2208hc(xi) w c,xi,yi + y =yi I y / \u2208hc(xi) w c,xi,y , W c,\u2212 = (xi,yi)\u2208S, hc(xi) =\u03d1 I yi / \u2208hc(xi) w c,xi,yi + y =yi I y\u2208hc(xi) w c,xi,y .(3)", "formula_coordinates": [3.0, 55.83, 364.82, 241.17, 85.24]}, {"formula_id": "formula_7", "formula_text": "\u03b1 c = log W c,+ + 1 n W c,\u2212 + 1 n /2. The term 1", "formula_coordinates": [3.0, 101.1, 456.13, 135.25, 17.74]}, {"formula_id": "formula_8", "formula_text": "x i , y i ) \u2208 S, \u2200y \u2208 Y, if h c (x i ) = \u03d1 then w c+1,xi,y = wc,x i ,y Zc and if h c (x i ) = \u03d1 then w c+1,xi,y = w c,xi,y Z c exp {\u2212\u03b1 c (I y=yi \u2212 I y =yi ) \u00d7 I y\u2208hc(xi) \u2212 I y / \u2208hc(xi)", "formula_coordinates": [3.0, 54.0, 641.74, 243.0, 63.41]}, {"formula_id": "formula_9", "formula_text": "H(x) = arg max y\u2208Y C c=1 \u03b1 c I hc(x) =\u03d1\u2227y\u2208hc(x)(5)", "formula_coordinates": [3.0, 346.59, 85.42, 211.42, 30.2]}, {"formula_id": "formula_10", "formula_text": "P (x,y)\u2208S [H(x) = y] \u2264 |Y| 2 C c=1 Z c with Z c = (1 \u2212 W c,+ \u2212 W c,\u2212 ) + (W c,+ ) \u2022 Wc,\u2212+ 1 n Wc,++ 1 n + (W c,\u2212 ) \u2022 Wc,++ 1 n Wc,\u2212+ 1 n \u2264 1.", "formula_coordinates": [4.0, 54.0, 417.33, 243.0, 85.94]}, {"formula_id": "formula_11", "formula_text": "P (x,y)\u223cD S [H(x) = y] \u2264 P (x,y)\u2208S [\u03b8 H (x, y) \u2264 \u03b8] + O \uf8eb \uf8ed log 1 \u03b4 n + log |Y| 2 n\u03b8 2 log |H| log |H| n\u03b8 2 \uf8f6 \uf8f8", "formula_coordinates": [4.0, 321.73, 85.07, 229.54, 50.94]}, {"formula_id": "formula_12", "formula_text": "P (x,y)\u2208S [\u03b8 H (x, y) \u2264 \u03b8] \u2264 |Y| 2 C c=1 Z c W c,+ + 1 n W c,\u2212 + 1 n \u03b8 .", "formula_coordinates": [4.0, 325.77, 168.58, 221.46, 31.03]}, {"formula_id": "formula_13", "formula_text": "C = n \u03b2 2 with \u03b2 \u2208 0, 1 + log (n\u22121) log (n)", "formula_coordinates": [4.0, 315.0, 542.56, 243.0, 31.01]}, {"formula_id": "formula_14", "formula_text": "P (x,y)\u223cD S [H(x) = \u03d1] \u2265 1 \u2212 p + p (1 \u2212 p) n C . (6)", "formula_coordinates": [4.0, 338.35, 664.8, 219.65, 22.62]}, {"formula_id": "formula_15", "formula_text": "lim n\u2192+\u221e P (x,y)\u223cD S [H(x) = \u03d1] \u2265 \uf8f1 \uf8f2 \uf8f3 1 if k < 3 2 , exp(\u22122) if k = 3 2 , 0 if 3 2 < k,(7)", "formula_coordinates": [5.0, 61.95, 321.8, 235.05, 37.46]}, {"formula_id": "formula_16", "formula_text": "\u2022 Euclidean distance: d(x, y) = x \u2212 y 2 , \u2022 Cityblock distance: d(x, y) = x \u2212 y 1 , \u2022 Cosine distance: d(x, y) = 1 \u2212 x,y x 2 y 2 .", "formula_coordinates": [8.0, 63.96, 467.46, 175.17, 47.25]}, {"formula_id": "formula_17", "formula_text": "T = {(x i , x j , x k ) : d(x i , x j ) < d(x i , x k )} .", "formula_coordinates": [8.0, 217.86, 537.5, 176.28, 9.65]}, {"formula_id": "formula_18", "formula_text": "x i , x k , x j ) \u2208 T while (x i , x j , x k ) \u2208 T .", "formula_coordinates": [8.0, 54.0, 566.8, 502.62, 20.61]}, {"formula_id": "formula_19", "formula_text": "T = \uf8f1 \uf8f2 \uf8f3 (m i , m j , m k ) : \uf8eb \uf8ed u\u2208U i,j,k I ru,i,j <r u,i,k \u2212 I ru,i,j >r u,i,k |U i,j,k | \uf8f6 \uf8f8 > 0 \uf8fc \uf8fd \uf8fe", "formula_coordinates": [9.0, 164.39, 443.34, 283.22, 34.75]}, {"formula_id": "formula_20", "formula_text": "H(m, y) = C c=1 \u03b1 c I hc(m) =\u03d1\u2227y\u2208hc(m) .", "formula_coordinates": [9.0, 231.75, 597.82, 148.51, 30.2]}, {"formula_id": "formula_21", "formula_text": "P (xi,y)\u223cWc (I y=yi \u2212 I y =yi ) = I y\u2208hc(xi) \u2212 I y / \u2208hc(xi)", "formula_coordinates": [21.0, 197.6, 614.56, 207.59, 15.05]}, {"formula_id": "formula_22", "formula_text": "P (xi,y)\u223cWc hc(xi) =\u03d1 (I y=yi \u2212 I y =yi ) = I y\u2208hc(xi) \u2212 I y / \u2208hc(xi) P (xi,y)\u223cWc [h c (x i ) = \u03d1] + P (xi,y)\u223cWc hc(xi)=\u03d1 (I y=yi \u2212 I y =yi ) = I y\u2208hc(xi) \u2212 I y / \u2208hc(xi) P (xi,y)\u223cWc [h c (x i ) = \u03d1] .", "formula_coordinates": [21.0, 74.37, 652.48, 463.25, 53.11]}, {"formula_id": "formula_23", "formula_text": "P (xi,y)\u223cWc hc(xi)=\u03d1 (I y=yi \u2212 I y =yi ) = I y\u2208hc(xi) \u2212 I y / \u2208hc(xi) = 1 2 .", "formula_coordinates": [22.0, 186.03, 71.75, 239.95, 29.78]}, {"formula_id": "formula_24", "formula_text": "P (xi,y)\u223cWc hc(xi) =\u03d1 (I y=yi \u2212 I y =yi ) = I y\u2208hc(xi) \u2212 I y / \u2208hc(xi) = W c,\u2212 W c,+ + W c,\u2212 \u2264 1 2", "formula_coordinates": [22.0, 151.34, 124.04, 308.12, 29.78]}, {"formula_id": "formula_25", "formula_text": "zi\u2208Sc,j I y=yi w c,xi,y > zi\u2208Sc,j I y =yi w c,xi,y .(8)", "formula_coordinates": [22.0, 220.13, 202.47, 337.87, 20.06]}, {"formula_id": "formula_26", "formula_text": "zi\u2208Sc,j I y =yi w c,xi,y \u2265 zi\u2208Sc,j I y=yi w c,xi,y .(9)", "formula_coordinates": [22.0, 220.13, 250.16, 337.87, 20.06]}, {"formula_id": "formula_27", "formula_text": "W c,+ = (xi,yi)\u2208S hc(xi) =\u03d1 \uf8eb \uf8ed I yi\u2208hc(xi) w c,xi,yi + y =yi I y / \u2208hc(xi) w c,xi,y \uf8f6 \uf8f8 , W c,\u2212 = (xi,yi)\u2208S hc(xi) =\u03d1 \uf8eb \uf8ed I yi / \u2208hc(xi) w c,xi,yi + y =yi I y\u2208hc(xi) w c,xi,y \uf8f6 \uf8f8 .(10)", "formula_coordinates": [22.0, 177.34, 301.76, 380.66, 89.64]}, {"formula_id": "formula_28", "formula_text": "W c,\u2212 \u2264 W c,+ \u21d4 2W c,\u2212 \u2264 W c,+ + W c,\u2212 \u21d4 W c,\u2212 W c,+ + W c,\u2212 \u2264 1 2", "formula_coordinates": [22.0, 177.84, 414.79, 258.59, 51.25]}, {"formula_id": "formula_29", "formula_text": "\u03b1 c = 1 2 log W c,+ + 1 n W c,\u2212 + 1 n .", "formula_coordinates": [22.0, 251.81, 545.28, 108.38, 28.12]}, {"formula_id": "formula_30", "formula_text": "W c,\u2212 < W c,+ \u21d4 W c,\u2212 + 1 n < W c,+ + 1 n \u21d4 1 < W c,+ + 1 n W c,\u2212 + 1 n \u21d4 0 < 1 2 log W c,+ + 1 n W c,\u2212 + 1 n", "formula_coordinates": [22.0, 174.5, 594.16, 256.04, 96.38]}, {"formula_id": "formula_31", "formula_text": "P (x,y)\u2208S [H(x) = y] \u2264 |Y| 2 C c=1 Z c with Z c = (1 \u2212 W c,+ \u2212 W c,\u2212 ) + (W c,+ ) \u2022 Wc,\u2212+ 1 n Wc,++ 1 n + (W c,\u2212 ) \u2022 Wc,++ 1 n Wc,\u2212+ 1 n \u2264 1.", "formula_coordinates": [23.0, 54.0, 110.88, 332.21, 59.66]}, {"formula_id": "formula_32", "formula_text": "= C c=1 \u03b1 c I hc(x) =\u03d1 I y\u2208hc(x) \u2212 I y / \u2208hc(x)", "formula_coordinates": [23.0, 154.32, 211.98, 161.88, 14.11]}, {"formula_id": "formula_33", "formula_text": "H(\u2022) = arg max y\u2208Y C c=1 \u03b1 c I hc(\u2022) =\u03d1\u2227y\u2208hc(\u2022) (x > y \u21d4 x \u2212 (B \u2212 x) > y \u2212 (B \u2212 y).) \u21d4 H(\u2022) = arg max y\u2208Y C c=1 \u03b1 c I hc(\u2022) =\u03d1 I y\u2208hc(\u2022) \u2212 I y / \u2208hc(\u2022) (F (\u2022, y) = C c=1 \u03b1 c I hc(\u2022) =\u03d1 I y\u2208hc(\u2022) \u2212 I y / \u2208hc(\u2022) .) \u21d4 H(\u2022) = arg max y\u2208Y F (\u2022, y) \u21d4 H(\u2022) = arg min y\u2208Y \u2212F (\u2022, y) \u21d4 H(\u2022) =", "formula_coordinates": [23.0, 126.82, 241.02, 431.18, 155.26]}, {"formula_id": "formula_34", "formula_text": "(\u2200x > 0, x + 1 x \u2265 2.) \u2265 2.", "formula_coordinates": [24.0, 140.99, 87.27, 417.01, 25.71]}, {"formula_id": "formula_35", "formula_text": "2M \u2264 (x,y)\u2208S y \u2208Y exp (\u2212(I y=y \u2212 I y =y )F (x, y )) .(11)", "formula_coordinates": [24.0, 199.01, 143.66, 359.0, 20.53]}, {"formula_id": "formula_36", "formula_text": "w c+1,x,y = w c,x,y Zc if h c (x) = \u03d1, w c,x,y Zc exp \u2212\u03b1 c (I y =y \u2212 I y =y ) I y \u2208hc(x) \u2212 I y / \u2208hc(x) if h c (x) = \u03d1. \u21d4 w c+1,x,y = w c,x,y Z c exp \u2212 (I y =y \u2212 I y =y ) \u03b1 c I hc(x) =\u03d1 I y \u2208hc(x) \u2212 I y / \u2208hc(x)(12)", "formula_coordinates": [24.0, 98.87, 216.61, 459.13, 54.72]}, {"formula_id": "formula_37", "formula_text": "w C+1,x,y = w 1,x,y C c=1 Z c C c=1 exp \u2212 (I y =y \u2212 I y =y ) \u03b1 c I hc(x) =\u03d1 I y \u2208hc(x) \u2212 I y / \u2208hc(x)(13)", "formula_coordinates": [24.0, 159.38, 296.43, 398.62, 30.24]}, {"formula_id": "formula_38", "formula_text": "(exp(x) exp(y) = exp(x + y).) \u21d4 w C+1,x,y = w 1,x,y C c=1 Z c exp \u2212 (I y =y \u2212 I y =y ) C c=1 \u03b1 c I hc(x) =\u03d1 I y \u2208hc(x) \u2212 I y / \u2208hc(x)(14)", "formula_coordinates": [24.0, 103.09, 331.12, 454.91, 45.31]}, {"formula_id": "formula_39", "formula_text": "(F (x, y) = C c=1 \u03b1 c I hc(x) =\u03d1 I y\u2208hc(x) \u2212 I y / \u2208hc(x) .) \u21d4 w C+1,x,y = w 1,x,y C c=1 Z c exp (\u2212 (I y =y \u2212 I y =y ) F (x, y ))(15)", "formula_coordinates": [24.0, 103.09, 380.03, 454.91, 41.35]}, {"formula_id": "formula_40", "formula_text": "\u21d4 C c=1 Z c = 1 n |Y| (x,y)\u2208S y \u2208Y exp (\u2212 (I y =y \u2212 I y =y ) F (x, y )) . (16", "formula_coordinates": [24.0, 103.09, 442.59, 450.75, 30.94]}, {"formula_id": "formula_41", "formula_text": ")", "formula_coordinates": [24.0, 553.85, 453.33, 4.15, 8.64]}, {"formula_id": "formula_42", "formula_text": "2M \u2264 n |Y| C c=1 Z c . Noticing that P (x,y)\u2208S [H(x) = y] = M n", "formula_coordinates": [24.0, 54.0, 501.37, 291.99, 51.57]}, {"formula_id": "formula_43", "formula_text": "P (x,y)\u2208S [H(x) = y] \u2264 |Y| 2 C c=1 Z c .", "formula_coordinates": [24.0, 238.35, 560.94, 135.3, 30.2]}, {"formula_id": "formula_44", "formula_text": "c = (1 \u2212 W c,+ \u2212 W c,\u2212 ) + (W c,+ ) \u2022 Wc,\u2212+ 1 n Wc,++ 1 n + (W c,\u2212 ) \u2022 Wc,++ 1 n Wc,\u2212+ 1 n \u2264 1.", "formula_coordinates": [24.0, 233.13, 602.91, 298.04, 18.91]}, {"formula_id": "formula_45", "formula_text": "W c,+ + 1 n W c,\u2212 + 1 n = (W c,+ ) \u2022 W c,\u2212 + 1 n W c,+ + 1 n + (W c,\u2212 ) \u2022 W c,+ + 1 n W c,\u2212 + 1 n", "formula_coordinates": [25.0, 129.2, 437.7, 291.1, 73.97]}, {"formula_id": "formula_46", "formula_text": "(x,y)\u2208S hc(x)=\u03d1 y \u2208Y w c,x,y = 1 \u2212 W c,+ \u2212 W c,\u2212 .", "formula_coordinates": [25.0, 222.58, 541.06, 166.84, 28.52]}, {"formula_id": "formula_47", "formula_text": "Z c = (1 \u2212 W c,+ \u2212 W c,\u2212 ) + (W c,+ ) \u2022 W c,\u2212 + 1 n W c,+ + 1 n + (W c,\u2212 ) \u2022 W c,+ + 1 n W c,\u2212 + 1 n .", "formula_coordinates": [25.0, 149.11, 596.82, 313.79, 28.12]}, {"formula_id": "formula_48", "formula_text": "W c,+ + 1 n \u2212 W c,\u2212 + 1 n 2 \u2265 0 \u21d4 W c,+ + 1 n + W c,\u2212 + 1 n \u2212 2 W c,+ + 1 n W c,\u2212 + 1 n \u2265 0 \u21d4 W c,+ + W c,\u2212 + 2 n \u2212 W c,+ + 1 n W c,\u2212 + 1 n W c,+ + 1 n \u2212 W c,\u2212 + 1 n W c,+ + 1 n W c,\u2212 + 1 n \u2265 0 \u21d4 W c,+ + W c,\u2212 + 2 n \u2212 W c,+ W c,\u2212 + 1 n W c,+ + 1 n \u2212 1 n W c,\u2212 + 1 n W c,+ + 1 n \u2212 W c,\u2212 W c,+ + 1 n W c,\u2212 + 1 n \u2212 1 n W c,+ + 1 n W c,\u2212 + 1 n \u2265 0 \u21d4 \u2212W c,+ \u2212 W c,\u2212 \u2212 2 n + W c,+ W c,\u2212 + 1 n W c,+ + 1 n + 1 n W c,\u2212 + 1 n W c,+ + 1 n + W c,\u2212 W c,+ + 1 n W c,\u2212 + 1 n + 1 n W c,+ + 1 n W c,\u2212 + 1 n \u2264 0 \u21d4 1 \u2212 W c,+ \u2212 W c,\u2212 \u2212 2 n + W c,+ W c,\u2212 + 1 n W c,+ + 1 n + 1 n W c,\u2212 + 1 n W c,+ + 1 n + W c,\u2212 W c,+ + 1 n W c,\u2212 + 1 n + 1 n W c,+ + 1 n W c,\u2212 + 1 n \u2264 1 \u21d4 Z c \u2212 2 n + 1 n W c,\u2212 + 1 n W c,+ + 1 n + 1 n W c,+ + 1 n W c,\u2212 + 1 n \u2264 1 \u21d4 Z c \u2212 2 n + 1 n W c,\u2212 + 1 n W c,+ + 1 n + W c,+ + 1 n W c,\u2212 + 1 n \u2264 1 (\u2200x > 0, x + 1 x \u2265 2.) \u21d2 Z c \u2264 1.", "formula_coordinates": [25.0, 73.95, 646.5, 464.38, 61.39]}, {"formula_id": "formula_49", "formula_text": "P (x,y)\u223cD S [H(x) = y] \u2264 P (x,y)\u2208S [\u03b8 H (x, y) \u2264 \u03b8] + O \uf8eb \uf8ed log 1 \u03b4 n + log |Y| 2 n\u03b8 2 log |H| log |H| n\u03b8 2 \uf8f6 \uf8f8", "formula_coordinates": [26.0, 191.23, 401.93, 229.54, 50.94]}, {"formula_id": "formula_50", "formula_text": "P (x,y)\u2208S [\u03b8 H (x, y) \u2264 \u03b8] \u2264 |Y| 2 C c=1 Z c W c,+ + 1 n W c,\u2212 + 1 n \u03b8 .", "formula_coordinates": [26.0, 195.27, 486.41, 221.46, 31.03]}, {"formula_id": "formula_51", "formula_text": "\u03b8 2 log 4|Y| 2 n\u03b8 2 log |H|", "formula_coordinates": [26.0, 265.59, 547.1, 63.28, 16.01]}, {"formula_id": "formula_52", "formula_text": "H(x) = arg max y\u2208Y C c=1 \u03b1 c I hc(x) =\u03d1\u2227y\u2208hc(x)(18)", "formula_coordinates": [26.0, 223.97, 605.48, 334.03, 30.2]}, {"formula_id": "formula_53", "formula_text": "\u03bd f,\u03b7 (x, y) = \u2212 1 \u03b7 log \uf8eb \uf8ed 1 |Y| y \u2208Y exp \u2212 \u03b7 (I y=y \u2212 I y =y ) f (x, y ) \uf8f6 \uf8f8 . (22", "formula_coordinates": [27.0, 161.23, 222.78, 392.62, 33.68]}, {"formula_id": "formula_54", "formula_text": ")", "formula_coordinates": [27.0, 553.85, 236.71, 4.15, 8.64]}, {"formula_id": "formula_55", "formula_text": "\u03b8 H (x, y) = \u03b8 f,\u03b7 (x, y) = 1 2 \u03bd f,\u03b7 (x, y) \u2212 max y =y \u03bd f,\u03b7 (x, y ) . (23", "formula_coordinates": [27.0, 184.42, 286.3, 369.44, 22.31]}, {"formula_id": "formula_56", "formula_text": ")", "formula_coordinates": [27.0, 553.85, 293.36, 4.15, 8.64]}, {"formula_id": "formula_57", "formula_text": "Note that \u03b8 f,\u03b7 (x, y) \u2208 [\u22121, 1] since f (x, y) \u2208 [\u22121, 1]", "formula_coordinates": [27.0, 203.99, 318.59, 227.68, 9.65]}, {"formula_id": "formula_58", "formula_text": "F (x, y) \u2264 F (x, ) \u21d4 f (x, y) \u2264 f (x, ) (\u03b7 > 0.) \u21d4 \u03b7f (x, y) \u2264 \u03b7f (x", "formula_coordinates": [27.0, 59.23, 347.91, 498.78, 50.58]}, {"formula_id": "formula_59", "formula_text": "\u21d4 y \u2208Y exp \u2212 \u03b7 (I y=y \u2212 I y =y ) f (x, y ) \u2265 y \u2208Y exp \u2212 \u03b7 (I =y \u2212 I =y ) f (x, y ) \u21d4 log \uf8eb \uf8ed 1 |Y| y \u2208Y exp \u2212 \u03b7 (I y=y \u2212 I y =y ) f (x, y ) \uf8f6 \uf8f8 \u2265 log \uf8eb \uf8ed 1 |Y| y \u2208Y exp \u2212 \u03b7 (I =y \u2212 I =y ) f (x, y ) \uf8f6 \uf8f8 \u21d4 \u03bd f,\u03b7 (x, y) \u2264 \u03bd f,\u03b7 (x, ) \u21d4 \u03bd f,\u03b7 (x, y) \u2212 \u03bd f,\u03b7 (x, ) \u2264 0 \u21d4 \u03b8 f,\u03b7 (x, y) \u2264 0. It implies that P (x,y)\u223cD S [H(x) = y] = P (x,y)\u223cD S [\u03b8 f,\u03b7 (x, y) \u2264 0]", "formula_coordinates": [27.0, 54.0, 451.14, 485.51, 146.05]}, {"formula_id": "formula_60", "formula_text": "P f f (x, y ) \u2212 f (x, y ) \u2265 \u03b8 4 \u2264 2 exp \u2212T \u03b8 2 32 .", "formula_coordinates": [27.0, 204.08, 676.34, 203.85, 24.66]}, {"formula_id": "formula_61", "formula_text": "P f \u2203y \u2208 Y : f (x, y ) \u2212 f (x, y ) \u2265 \u03b8 4 \u2264 2 |Y| exp \u2212T \u03b8 2 32 . (24", "formula_coordinates": [28.0, 175.98, 71.19, 377.87, 24.66]}, {"formula_id": "formula_62", "formula_text": ")", "formula_coordinates": [28.0, 553.85, 79.82, 4.15, 8.64]}, {"formula_id": "formula_63", "formula_text": "\u03b5 \u03b8 = 4 log |Y| i\u03b8 : i = 1, . . . , 8 log |Y| \u03b8 2 \u21d4 \u03bd f,\u03b7 (x, y) \u2212 \u03b8 \u2264 max =y \u03bd f,\u03b7 (x, ) + \u03b8 \u21d4 \u2212 log \uf8eb \uf8ed 1 |Y| y \u2208Y exp \u2212 \u03b7 (I y=y \u2212 I y =y ) f (x, y ) \uf8f6 \uf8f8 \u2212 \u03b7\u03b8 \u2264 max =y \u2212 log \uf8eb \uf8ed 1 |Y| y \u2208Y exp \u2212 \u03b7 (I =y \u2212 I =y ) f (x, y ) \uf8f6 \uf8f8 + \u03b7\u03b8 \u21d4 log \uf8eb \uf8ed 1 |Y| y \u2208Y exp \u2212 \u03b7 (I y=y \u2212 I y =y ) f (x, y ) \uf8f6 \uf8f8 + \u03b7\u03b8 \u2265 min =y log \uf8eb \uf8ed 1 |Y| y \u2208Y exp \u2212 \u03b7 (I =y \u2212 I =y ) f (x, y ) \uf8f6 \uf8f8 \u2212 \u03b7\u03b8 \u21d4 y \u2208Y exp (\u03b7\u03b8) exp \u2212 \u03b7 (I y=y \u2212 I y =y ) f (x, y ) \u2265 min =y y \u2208Y exp (\u2212\u03b7\u03b8) exp \u2212 \u03b7 (I =y \u2212 I =y ) f (x, y ) \u21d4 y \u2208Y exp \u2212 \u03b7 (I y=y \u2212 I y =y ) f (x, y ) + \u03b7\u03b8 \u2265 min =y y \u2208Y exp \u2212 \u03b7 (I =y \u2212 I =y ) f (x, y ) \u2212 \u03b7\u03b8 \u21d4 y \u2208Y exp \u2212 z(y ) \u2265 min =y y \u2208Y exp \u2212 z (y )", "formula_coordinates": [28.0, 216.78, 117.37, 159.81, 22.31]}, {"formula_id": "formula_64", "formula_text": "y \u2208Y exp \u2212 z(y ) \u2265 1 2 y \u2208Y exp \u2212 z(y ) + 1 2 min =y y \u2208Y exp \u2212 z (y ) \u2265 min =y \uf8ee \uf8f0 1 2 y \u2208Y exp \u2212 z(y ) + 1 2 y \u2208Y exp \u2212 z (y ) \uf8f9 \uf8fb \u2265 1 2 min =y y \u2208Y exp \u2212 z(y ) + exp \u2212 z (y ) \u2265 1 2 min =y y \u2208Y exp \u2212 \u03b7 (I y=y \u2212 I y =y ) f (x, y ) + \u03b7\u03b8 + exp \u2212 \u03b7 (I =y \u2212 I =y ) f (x, y ) \u2212 \u03b7\u03b8", "formula_coordinates": [31.0, 54.0, 99.6, 487.31, 129.46]}, {"formula_id": "formula_65", "formula_text": "\u2265 1 2 min =y y \u2208{y, } exp \u2212 \u03b7 (I y=y \u2212 I y =y ) f (x, y ) + \u03b7\u03b8 + exp \u2212 \u03b7 (I =y \u2212 I =y ) f (x, y ) \u2212 \u03b7\u03b8 (\u2200y \u2208 {y, } , (I =y \u2212 I =y ) = \u2212(I y=y \u2212 I y =y ).) \u2265 1 2 min =y y \u2208{y, } exp \u2212 \u03b7 (I y=y \u2212 I y =y ) f (x, y ) + \u03b7\u03b8 + exp \u03b7 (I y=y \u2212 I y =y ) f (x, y ) \u2212 \u03b7\u03b8 \u2265 1 2 min =y y \u2208{y, } exp \u2212 z(y ) + exp z(y ) (\u2200x > 0, x + 1 x \u2265 2.) \u2265 2 Now assume that the strong classifier violates the margin M times, that is \u03b8 f,\u03b7 (x, y) \u2264 \u03b8 for M examples, then 2M \u2264 (x,y)\u2208S y \u2208Y exp \u2212 z(y ) \u21d4 2M \u2264 (x,y)\u2208S y \u2208Y exp \u2212 \u03b7 (I y=y \u2212 I y =y ) f (x, y ) + \u03b7\u03b8 \u21d4 2M \u2264 (x,y)\u2208S y \u2208Y exp \u2212 \u03b7 (I y=y \u2212 I y =y ) f (x, y ) exp \u03b7\u03b8 (Let \u03b7 = c \u03b1 c and recall that ( c \u03b1 c ) f (x, y) = F (x, y).) \u21d4 2M \u2264 (x,y)\u2208S y \u2208Y exp \u2212 (I y=y \u2212 I y =y ) F (x, y ) exp c \u03b1 c \u03b8 \u21d4 2M \u2264 exp c \u03b1 c \u03b8 (x,y)\u2208S y \u2208Y exp \u2212 (I y=y \u2212 I y =y ) F (x, y ) Recall that (x,y)\u2208S y \u2208Y exp \u2212 (I y=y \u2212 I y =y ) F (x, y ) = n |Y| C c=1 Z c as shown in Equation (16) in the proof of Theorem 2. 2M \u2264 exp c \u03b1 c \u03b8 n |Y| C c=1 Z c \u21d4 2M \u2264 exp c \u03b1 c \u03b8 n |Y| C c=1 Z c \u21d4 2M \u2264 n |Y| C c=1 Z c exp (\u03b1 c \u03b8) \u21d4 M n \u2264 |Y| 2 C c=1 Z c exp (\u03b1 c \u03b8)", "formula_coordinates": [31.0, 54.0, 248.6, 504.0, 458.73]}, {"formula_id": "formula_66", "formula_text": "P (x,y)\u223cD S [H(x) = \u03d1] \u2265 1 \u2212 p + p (1 \u2212 p) n C .(38)", "formula_coordinates": [32.0, 207.85, 249.81, 350.15, 22.62]}, {"formula_id": "formula_67", "formula_text": "Y i,c = 0 if h c abstains for example x i , 1 otherwise. (39", "formula_coordinates": [32.0, 221.46, 384.16, 332.39, 19.92]}, {"formula_id": "formula_68", "formula_text": ")", "formula_coordinates": [32.0, 553.85, 390.26, 4.15, 8.64]}, {"formula_id": "formula_69", "formula_text": "Y x,c = 0 if h c abstains for example x, 1 otherwise. (40", "formula_coordinates": [32.0, 222.27, 444.14, 331.58, 19.92]}, {"formula_id": "formula_70", "formula_text": ")", "formula_coordinates": [32.0, 553.85, 450.24, 4.15, 8.64]}, {"formula_id": "formula_71", "formula_text": "V c = n i=1 Y i,c .(41)", "formula_coordinates": [32.0, 277.5, 511.76, 280.5, 30.32]}, {"formula_id": "formula_72", "formula_text": "P (x,y)\u223cD S H(x) = \u03d1 C c=1 V c Y x,c = 0 = 1.", "formula_coordinates": [32.0, 217.09, 677.13, 177.83, 30.2]}, {"formula_id": "formula_73", "formula_text": "U c = 1 if V c Y x,c = 0, 0 otherwise. . (44", "formula_coordinates": [33.0, 247.81, 191.6, 306.04, 19.92]}, {"formula_id": "formula_74", "formula_text": ")", "formula_coordinates": [33.0, 553.85, 197.7, 4.15, 8.64]}, {"formula_id": "formula_75", "formula_text": "P (x,y)\u223cD S [V", "formula_coordinates": [33.0, 172.67, 253.24, 45.23, 15.66]}, {"formula_id": "formula_76", "formula_text": "= 1 \u2212 1 \u2212 n 0 p 0 (1 \u2212 p) n\u22120 p = 1 \u2212 (1 \u2212 (1 \u2212 p) n ) p = (1 \u2212 p) + p (1 \u2212 p) n .", "formula_coordinates": [33.0, 262.49, 376.05, 145.16, 51.48]}, {"formula_id": "formula_77", "formula_text": "P (x,y)\u223cD S C c=1 V c Y x,c = 0 = P (x,y)\u223cD S C c=1 U c = c(45)", "formula_coordinates": [33.0, 198.37, 457.01, 359.63, 30.2]}, {"formula_id": "formula_78", "formula_text": "P (x,y)\u223cD S C c=1 V c Y x,c = 0 = c c ((1 \u2212 p) + p(1 \u2212 p) n ) C (1 \u2212 (1 \u2212 p) + p(1 \u2212 p) n ) C\u2212C (46) = ((1 \u2212 p) + p(1 \u2212 p) n ) C . (47", "formula_coordinates": [33.0, 121.09, 536.69, 436.91, 45.76]}, {"formula_id": "formula_79", "formula_text": ")", "formula_coordinates": [33.0, 553.85, 573.81, 4.15, 8.64]}, {"formula_id": "formula_80", "formula_text": "P (x,y)\u223cD S [H(x) = \u03d1] \u2265 ((1 \u2212 p) + p(1 \u2212 p) n ) C", "formula_coordinates": [33.0, 208.13, 610.93, 194.75, 18.63]}, {"formula_id": "formula_81", "formula_text": "lim n\u2192+\u221e \u2212n k\u22123+\u03b2 1 \u2212 1 \u2212 2n k\u22123 n = \uf8f1 \uf8f2 \uf8f3 0 if 0 \u2264 k < 2, exp(\u22122) \u2212 1 if k = 2, \u2212\u221e if 2 < k < 3 \u2212 log(2) log(n) . If 1 < \u03b2 \u2264 2 then: lim n\u2192+\u221e \u2212n k\u22123+\u03b2 1 \u2212 1 \u2212 2n k\u22123 n = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 0 if 0 \u2264 k < 5\u2212\u03b2 2 , \u22122 if k = 5\u2212\u03b2 2 , \u2212\u221e if 5\u2212\u03b2 2 < k < 3 \u2212 log(2) log(n) . (54", "formula_coordinates": [37.0, 63.96, 72.57, 489.89, 99.48]}, {"formula_id": "formula_82", "formula_text": ")", "formula_coordinates": [37.0, 553.85, 148.1, 4.15, 8.64]}, {"formula_id": "formula_83", "formula_text": "\u2212 \u221e l=2 n \u03b2 2l 2n k\u22123 \u2212 2n k\u22123 1 \u2212 2n k\u22123 n l = \u2212 \u221e l=2 n \u03b2 2l 2n kl\u22123l 1 \u2212 1 \u2212 2n k\u22123 n l = \u2212 \u221e l=2 1 \u2212 1 \u2212 2n k\u22123 n l l n (k\u22123)l+\u03b2", "formula_coordinates": [37.0, 127.23, 194.17, 357.0, 66.28]}, {"formula_id": "formula_84", "formula_text": "l > \u03b2 3 \u2212 k \u21d2 lim n\u2192+\u221e \u2212n (k\u22123)l+\u03b2 = 0 \u21d2 lim n\u2192+\u221e \u2212 1 \u2212 1 \u2212 2n k\u22123 n l l n (k\u22123)l+\u03b2 = 0 l \u2264 \u03b2 3 \u2212 k \u21d2 lim n\u2192+\u221e \u2212n (k\u22123)l+\u03b2 \u2264 0 \u21d2 lim n\u2192+\u221e \u2212 1 \u2212 1 \u2212 2n k\u22123 n l l n (k\u22123)l+\u03b2 \u2264 0.", "formula_coordinates": [37.0, 127.46, 290.95, 357.08, 58.0]}, {"formula_id": "formula_85", "formula_text": "0 \u2264 k < 6 \u2212 \u03b2 2 \u21d2 lim n\u2192+\u221e \u2212 \u221e l=2 n 2 2l 2n k\u22123 \u2212 2n k\u22123 1 \u2212 2n k\u22123 n l = 0 (55) 6 \u2212 \u03b2 2 \u2264 k < 3 \u2212 log(2) log(n) \u21d2 lim n\u2192+\u221e \u2212 \u221e l=2 n 2 2l 2n k\u22123 \u2212 2n k\u22123 1 \u2212 2n k\u22123 n l \u2264 0 (56)", "formula_coordinates": [37.0, 134.73, 373.61, 423.27, 63.97]}, {"formula_id": "formula_86", "formula_text": "lim n\u2192+\u221e \u2212 n \u03b2 2 2n k\u22123 \u2212 2n k\u22123 1 \u2212 2n k\u22123 n \u2212 \u221e l=2 n \u03b2 2l 2n k\u22123 \u2212 2n k\u22123 1 \u2212 2n k\u22123 n l = \uf8f1 \uf8f2 \uf8f3 0 if 0 \u2264 k < 3 \u2212 \u03b2, \u22121 if k = 3 \u2212 \u03b2, \u2212\u221e if 3 \u2212 \u03b2 < k < 3 \u2212 log(2)", "formula_coordinates": [37.0, 115.89, 472.04, 380.21, 68.31]}, {"formula_id": "formula_87", "formula_text": "lim n\u2192+\u221e \u2212 n \u03b2 2 2n k\u22123 \u2212 2n k\u22123 1 \u2212 2n k\u22123 n \u2212 \u221e l=2 n \u03b2 2l 2n k\u22123 \u2212 2n k\u22123 1 \u2212 2n k\u22123 n l = \uf8f1 \uf8f2 \uf8f3 0 if 0 \u2264 k < 2, exp(\u22122) \u2212 1 if k = 2, \u2212\u221e if 2 < k < 3 \u2212 log(2) log(n) .", "formula_coordinates": [37.0, 115.89, 568.18, 380.21, 70.95]}, {"formula_id": "formula_88", "formula_text": "lim n\u2192+\u221e \u2212 n \u03b2 2 2n k\u22123 \u2212 2n k\u22123 1 \u2212 2n k\u22123 n \u2212 \u221e l=2 n \u03b2 2l 2n k\u22123 \u2212 2n k\u22123 1 \u2212 2n k\u22123 n l = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 0 if 0 \u2264 k < 5\u2212\u03b2 2 , \u22122 if k = 5\u2212\u03b2 2 , \u2212\u221e if 5\u2212\u03b2 2 < k < 3 \u2212 log(2) log(n) .", "formula_coordinates": [37.0, 115.89, 676.79, 380.21, 30.55]}, {"formula_id": "formula_89", "formula_text": "P (x,y)\u223cD S H(x) = \u03d1 C c=1 V c Y x,c = 0 = 0.", "formula_coordinates": [38.0, 217.09, 222.69, 177.83, 30.2]}, {"formula_id": "formula_90", "formula_text": "P (x,y)\u223cD S [H(x) = \u03d1] = P (x,y)\u223cD S C c=1 V c Y x,c = 0 .", "formula_coordinates": [38.0, 199.91, 276.41, 212.18, 30.2]}], "doi": ""}