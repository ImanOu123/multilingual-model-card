{"title": "Small-Text: Active Learning for Text Classification in Python", "authors": "Christopher Schr\u00f6der; Lydia M\u00fcller; Andreas Niekler; Martin Potthast", "pub_date": "", "abstract": "We introduce small-text, an easy-to-use active learning library, which offers pool-based active learning for single-and multi-label text classification in Python. It features numerous pre-implemented state-of-the-art query strategies, including some that leverage the GPU. Standardized interfaces allow the combination of a variety of classifiers, query strategies, and stopping criteria, facilitating a quick mix and match, and enabling a rapid and convenient development of both active learning experiments and applications. With the objective of making various classifiers and query strategies accessible for active learning, small-text integrates several well-known machine learning libraries, namely scikit-learn, PyTorch, and Hugging Face transformers. The latter integrations are optionally installable extensions, so GPUs can be used but are not required. Using this new library, we investigate the performance of the recently published SetFit training paradigm, which we compare to vanilla transformer fine-tuning, finding that it matches the latter in classification accuracy while outperforming it in area under the curve. The library is available under the MIT License at https://github.com/webis-de/small-text, in version 1.3.0 at the time of writing.", "sections": [{"heading": "Introduction", "text": "Text classification, like most modern machine learning applications, requires large amounts of training data to achieve state-of-the-art effectiveness. However, in many real-world use cases, labeled data does not exist and is expensive to obtain, especially when domain expertise is required. Active Learning (Lewis and Gale, 1994) solves this problem by repeatedly selecting unlabeled data instances that are deemed informative according to a so-called query strategy, and then having them labeled by an expert (see Figure 1a). A new model is then trained on all previously labeled data, and this process is repeated until a specified stopping criterion is met. Active learning aims to minimize the amount of labeled data required while maximizing the effectiveness (increase per iteration) of the model, e.g., in terms of classification accuracy. An active learning setup, as shown in Figure 1b, generally consists of up to three components on the system side: a classifier, a query strategy, and an optional stopping criterion. Meanwhile, many approaches for each of these components have been proposed and studied. Determining appropriate combinations of these approaches is only possible experimentally, and efficient implementations are often nontrivial. In addition, the components often depend on each other, for example, when a query strategy relies on parts specific to certain model classes, such as gradients (Ash et al., 2020) or embeddings (Margatina et al., 2021). The more such non-trivial combinations are used together, the more the reproduction effort increases, making a modular library essential.\nAn obvious solution to the above problems is the use of open source libraries, which, among other benefits, accelerate research and facilitate technology transfer between researchers as well as into practice (Sonnenburg et al., 2007)  tions for active learning in general already exist, few address text classification, which requires features specific to natural language processing, such as word embeddings (Mikolov et al., 2013) or language models (Devlin et al., 2019). To fill this gap, we introduce small-text, an active learning library that provides tried and tested components for both experiments and applications.\n.", "publication_ref": ["b1", "b27", "b45", "b28", "b9"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "Overview of Small-Text", "text": "The main goal of small-text is to offer state-ofthe-art active learning for text classification in a convenient and robust way for both researchers and practitioners. For this purpose, we implemented a modular pool-based active learning mechanism, illustrated in Figure 2, which exposes interfaces for classifiers, query strategies, and stopping criteria. The core of small-text integrates scikitlearn (Pedregosa et al., 2011), enabling direct use of its classifiers. Overall, the library provides thirteen query strategies, including some that are only usable on text data, five stopping criteria, and two integrations of well-known machine learning libraries, namely PyTorch (Paszke et al., 2019) and transformers (Wolf et al., 2020). The integrations ease the use of CUDA-based GPU computing and transformer models, respectively. The modular architecture renders both integrations completely optional, resulting in a slim core that can also be used in a CPU-only scenario without unnecessary dependencies. Given the ability to combine a considerable variety of classifiers and query strategies, we can easily build a vast number of combinations of active learning setups.\nThe library provides relevant text classification baselines such as SVM (Joachims, 1998) and Kim-CNN (Kim, 2014), and many more can be used through scikit-learn. Recent transformer mod-els such as BERT (Devlin et al., 2019) are available through the transformers integration. This integration also includes a wrapper that enables the use of the recently published SetFit training paradigm (Tunstall et al., 2022), which uses contrastive learning to fine-tune SBERT embeddings (Reimers and Gurevych, 2019) in a sample efficient manner.\nAs the query strategy, which selects the instances to be labeled, is the most salient component of an active learning setup, the range of alternative query strategies provided covers four paradigms at the time of writing: (i) confidence-based strategies: least confidence (Lewis and Gale, 1994;Culotta and McCallum, 2005), prediction entropy (Roy and McCallum, 2001), breaking ties (Luo et al., 2005), BALD (Houlsby et al., 2011), CVIRS (Reyes et al., 2018), and contrastive active learning (Margatina et al., 2021); (ii) embedding-based strategies: BADGE (Ash et al., 2020), BERT k-means (Yuan et al., 2020), discriminative active learning (Gissin and Shalev-Shwartz, 2019), and SEALS (Coleman et al., 2022); (iii) gradient-based strategies: expected gradient length (EGL; Settles et al., 2007), EGL-word (Zhang et al., 2017), and EGLsm (Zhang et al., 2017); and (iv) coreset strategies: greedy coreset (Sener and Savarese, 2018) and lightweight coreset (Bachem et al., 2018). Since there is an abundance of query strategies, this list will likely never be exhaustive-also because strategies from other domains, such as computer vision, are not always applicable to the text domain, e.g., when using the geometry of images (Konyushkova et al., 2015), and thus will be disregarded here.\nFurthermore, small-text includes a considerable amount of different stopping criteria: (i) stabilizing predictions (Bloodgood and Vijay-Shanker, 2009), (iv) overall-uncertainty (Zhu et al., 2008), (iii) classification-change (Zhu et al., 2008), (ii) predicted change of F-measure (Altschuler and Bloodgood, 2019), and (v) a criterion that stops after a fixed number of iterations. Stopping criteria are often neglected in active learning although they exert a strong influence on labeling efficiency.\nThe library is available via the python packaging index and can be installed with just a single command: pip install small-text. Similarly, the integrations can be enabled using the extra requirements argument of Python's setuptools, e.g., the transformers integration is installed using pip install small-text [transformers]. The robustness of the implementation rests on extensive unit and integration tests. Detailed examples, an API documentation, and common usage patterns are available in the online documentation. 1", "publication_ref": ["b36", "b35", "b52", "b18", "b9", "b37", "b24", "b7", "b41", "b26", "b15", "b38", "b27", "b1", "b55", "b12", "b6", "b44", "b57", "b57", "b43", "b3", "b21", "b4", "b58", "b58", "b0"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Library versus Annotation Tool", "text": "We designed small-text for two types of settings: (i) experiments, which usually consist of either automated active learning evaluations or shortlived setups with one or more human annotators, and (ii) real-world applications, in which the final model is subsequently applied on unlabeled or unseen data. Both cases benefit from a library which offers a wide range of well-tested functionality.\nTo clarify on the distinction between a library and an annotation tool, small-text is a library, by which we mean a reusable set of functions and classes that can be used and combined within more complex programs. In contrast, annotation tools provide a graphical user interface and focus on the interaction between the user and the system. Obviously, small-text is still intended to be used by annotation tools but remains a standalone library. In this way it can be used (i) in combination with an annotation tool, (ii) within an experiment setting, or (iii) as part of a backend application, e.g. a web API. As a library it remains compatible to all of these use cases. This flexibility is supported by the library's modular architecture which is also in concordance with software engineering best practices, where high cohesion and low coupling (Myers, 1975) are known to contribute towards highly reusable software (M\u00fcller et al., 1993;Tonella, 2001). As a result, small-text should be compatible with most annotations tools that are extensible and support text classification.\n1 https://small-text.readthedocs.io", "publication_ref": ["b31", "b30", "b47"], "figure_ref": [], "table_ref": []}, {"heading": "Code Example", "text": "In this section we show a code example to perform active learning with transformers models.\nDataset First, we create (for the sake of a simple example) a synthetic two-class spam dataset of 100 instances. The data is given by a list of texts and a list of integer labels. To define the tokenization strategy, we provide a transformers tokenizer. From these individual parts we construct a TransformersDataset object which is a dataset abstraction that can be used by the interfaces in small-text.  (Gamma et al., 1995) is responsible for creating new classifiers. Finally, we set the query strategy to least confidence (Culotta and McCallum, 2005).\nInitialization There is a chicken-and-egg problem for active learning because most query strategies rely on the model, and a model in turn is trained on labeled instances which are selected by the query strategy. This problem can be solved by either providing an initial model (e.g. through manual labeling), or by using cold start approaches (Yuan et al., 2020). In this example we simulate a user-provided initialization by looking up the respective true labels and providing an initial model: ", "publication_ref": ["b11", "b7", "b55"], "figure_ref": [], "table_ref": []}, {"heading": "1", "text": "To provide an initial model in the experimental scenario (where true labels are accessible), smalltext provides sampling methods, from which we use the balanced sampling to obtain a subset whose class distribution is balanced (or close thereto). In a real-world application, initialization would be accomplished through a starting set of labels supplied by the user. Alternatively, a cold start classifier or query strategy can be used instead.\nActive Learning Loop After the previous code examples prepared the setting by loading a dataset, configuring the active learning setup, and providing an initial model, the following code block shows the actual active learning loop. In this example, we perform five queries during each of which ten instances are queried. During a query step the query strategy samples instances to be labeled. Subsequently, new labels for each instance are provided and passed to the update method, and then a new model is trained. In this example, it is a simulated response relying on true labels, but in a real-world application this part is the user's response. 1\nIn summary, we built a full active learning setup in only very few lines of code. The actual active learning loop consists of just the previous code block and changing hyperparameters, e.g., using a different query strategy, is as easy as adapting the query_strategy variable.  In Table 1, we compare small-text to the previously mentioned libraries, and compare them based on several criteria related to active learning or to the respective code base: While all libraries provide a selection of query strategies, not all li-braries offer stopping criteria, which are crucial to reducing the total annotation effort and thus directly influence the efficiency of the active learning process (Vlachos, 2008;Laws and Sch\u00fctze, 2008;Olsson and Tomanek, 2009). We can also see a difference in the number of provided query strategies. While a higher number of query strategies is certainly not a disadvantage, it is more important to provide the most relevant strategies (either due to recency, domain-specificity, strong general performance, or because it is a baseline). Based on these criteria, small-text provides numerous recent strategies such as BADGE (Ash et al., 2020), BERT K-Means (Yuan et al., 2020), and contrastive active learning (Margatina et al., 2021), as well as the gradient-based strategies by Zhang et al. (2017), where the latter are unique to active learning for text classification. Selecting a subset of query strategies is especially important since active learning experiments are computationally expensive (Margatina et al., 2021;Schr\u00f6der et al., 2022), and therefore not every strategy can be tested in the context of an experiment or application. Finally, only small-text, lrtc, and ALToolbox focus on text, and only about half of the libraries offer access to GPU-based deep learning, which has become indispensable for text classification due to the recent advances and ubiquity of transformer-based models (Vaswani et al., 2017;Devlin et al., 2019).\nThe distinguishing characteristic of small-text is the focus on text classification, paired with a multitude of interchangeable components. It of-   Li and Roth (2002). The dataset type was abbreviated by N (News), S (Sentiment), Q (Questions). \u22c6: Predefined test sets were available and adopted.\nfers the most comprehensive set of features (as shown in Table 1) and through the integrations these components can be mixed and matched to easily build numerous different active learning setups, with or without leveraging the GPU. Finally, it allows to use concepts from natural language processing (such as transformer models) and provides query strategies unique to text classification.", "publication_ref": ["b51", "b23", "b32", "b1", "b55", "b27", "b57", "b27", "b42", "b50", "b9", "b25"], "figure_ref": [], "table_ref": ["tab_4", "tab_4"]}, {"heading": "Experiment", "text": "We perform an active learning experiment comparing an SBERT model trained with the recent sentence transformers fine-tuning paradigm (Set-Fit; (Tunstall et al., 2022)) over a BERT model trained with standard fine-tuning. SetFit is a contrastive learning approach that trains on pairs of (dis)similar instances. Given a fixed amount of differently labeled instances, the number of possible pairs is considerably higher than the size of the original set, making this approach highly sample efficient (Chuang et al., 2020;H\u00e9naff, 2020) and therefore interesting for active learning.\nSetup We reproduce the setup of our previous work (Schr\u00f6der et al., 2022) and evaluate on the datasets shown in  brevity, we refer to the first as \"BERT\" and to the second as \"SetFit\". To compare their performance during active learning, we provide an extensive benchmark over multiple computationally inexpensive uncertainty-based query strategies, which were selected due to encouraging results in our previous work. Moreover, we include BALD, BADGE, and greedy coreset-all of which are computationally more expensive, but have been increasingly used in recent work (Ein-Dor et al., 2020;Yu et al., 2022).\nResults In Table 3, the results show the summarized classification performance in terms of (i) final accuracy after the last iteration, and (ii) area under curve (AUC). We also compare strategies by ranking them from 1 (best) to 8 (worst) per model and dataset by accuracy and AUC. First, we can also confirm for SetFit the earlier finding that uncertainty-based strategies perform strong for BERT (Schr\u00f6der et al., 2022). Second, SetFit configurations result in between 0.06 and 1.7 percentage points higher mean accuracy, and also in betwen 4.2 and 6.6 higher AUC when averaged over model and query strategy. Interestingly, the greedy coreset strategy (CS) is remarkably more successful for the SetFit runs compared to the BERT runs. Detailed results per configuration can be found in the appendix, where it can be seen that SetFit reaches higher accuracy scores in most configurations, and better AUC scores in all configurations.\nDiscussion When trained with the new SetFit paradigm, models having only a third of the parameters compared to the large BERT model achieve results that are not only competitive, but slightly better regarding final accuracy and considerably better in terms of AUC. Since the final accuracy values are often within one percentage point or less to each other, it is obvious that the improvement in AUC stems from improvements in earlier queries, i.e. steeper learning curves. We suspect that this is at least partly owed to sample efficiency from SetFit's training that uses pairs of instances. Moreover, this has the additional benefit of reducing instability of transformer models (Mosbach et al., 2021) as can be exemplarily seen in Figure 3. This increasingly occurs when the training set is small (Mosbach et al., 2021), which is likely alleviated with the additional instance pairs. On the other hand, training cost increase linearly with the number of pairs per instance. In the low-data regime, however, this is a manageable additional cost that is worth the benefits.", "publication_ref": ["b5", "b14", "b42", "b10", "b54", "b42", "b29", "b29"], "figure_ref": ["fig_3"], "table_ref": ["tab_8"]}, {"heading": "Library Adoption", "text": "As recent publications have already adopted smalltext, we present four examples which have already successfully utilized it for their experiments.\nAbusive Language Detection Kirk et al. (2022) investigated the detection of abusive language using transformer-based active learning on six datasets of which two exhibited a balanced and four an imbalanced class distribution. They evalu-ated a pool-based binary active learning setup, and their main finding is that, when using active learning, a model for abusive language detection can be efficiently trained using only a fraction of the data.", "publication_ref": ["b20"], "figure_ref": [], "table_ref": []}, {"heading": "Classification of Citizens' Contributions", "text": "In order to support the automated classification of German texts from online citizen participation processes, Romberg and Escher (2022) used active learning to classify texts collected by three cities into eight different topics. They evaluated this realworld dataset both as a single-and multi-label active learning setup, finding that active learning can considerably reduce the annotation efforts. Gonsior et al. (2022) examined several alternatives to the softmax function to obtain better confidence estimates for active learning. Their setup extended small-text to incorporate additional softmax alternatives and found that confidence-based methods mostly selected outliers. As a remedy to this they proposed and evaluated uncertainty clipping.", "publication_ref": ["b40", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Softmax Confidence Estimates", "text": "Revisiting Uncertainty-Based Strategies In a previous publication, we reevaluated traditional uncertainty-based query strategies with recent transformer models (Schr\u00f6der et al., 2022). We found that uncertainty-based methods can still be highly effective and that the breaking ties strategy is a drop-in replacement for prediction entropy.\nNot only have all of these works successfully applied small-text to a variety of different problems, but each work is also accompanied by a GitHub repository containing the experiment code, which is the outcome we had hoped for. We expect that small-text will continue to gain adoption within the active learning and text classification communities, so that future experiments will increasingly rely on it by both reusing existing components and by creating their own extensions, thereby supporting the field through open reproducible research.", "publication_ref": ["b42"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We introduced small-text, a modular Python library, which offers state-of-the-art active learning for text classification. It integrates scikit-learn, PyTorch, and transformers, and provides robust components that can be mixed and matched to quickly apply active learning in both experiments and applications, thereby making active learning easily accessible to the Python ecosystem.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "Although a library can, among other things, lower the barrier of entry, save time, and speed up research, this can only be leveraged with basic knowledge of the Python programming language. All included algorithmic components are subject to their own limitations, e.g., the greedy coreset strategy quickly becomes computationally expensive as the amount labeled data increases. Moreover, some components have hyperparameters which require an understanding of the algorithm to achieve the best classification performance. In the end, we provide a powerful set of tools which still has to be properly used to achieve the best results.\nAs small-text covers numerous text classification models, query strategies, and stopping criteria, some limitations from natural language processing, text classification and active learning apply as well. For example, all included classification models rely on tokenization, which is inherently more difficult for languages which have no clear word boundaries such as Chinese, Japanese, Korean, or Thai.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ethics Statement", "text": "In this paper, we presented small-text, a library which can-like any other software-be used for good or bad. It can be used to bootstrap classification models in scenarios where no labeled data is available. This could be used for good, e.g. for spam detection, hatespeech detection, or targeted news filtering, but also for bad, e.g., for creating models that detect certain topics that are to be censored in authoritarian regimes. While such systems already exist and are of sophisticated quality, small-text is unlikely to change anything at this point. On the contrary, being open-source software, these methods can now be used by a larger audience, which contributes towards the democratization of classification algorithms.  Table 5: Final area under curve (AUC) per dataset, model, and query strategy. We report the mean and standard deviation over five runs. The best result per dataset is printed in bold. Query strategies are abbreviated as follows: prediction entropy (PE), breaking ties (BT), least confidence (LC), contrastive active learning (CA), BALD (BA), BADGE (BD), greedy coreset (CS), and random sampling (RS). The best result per dataset is printed in bold.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D Library Adoption", "text": "As mentioned in Section 7, the experiment code of previous works documents how small-text was used and can be found at the following locations: ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their constructive advice and the early adopters of the library for their invaluable feedback.\nThis research was partially funded by the Development Bank of Saxony (SAB) under project numbers 100335729 and 100400221. Computations were done (in part) using resources of the Leipzig University Computing Centre.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Supplementary Material", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Technical Environment", "text": "All experiments were conducted within a Python 3.8 environment. The system had CUDA 11.1 installed and was equipped with an NVIDIA GeForce RTX 2080 Ti (11GB VRAM).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Experiments", "text": "Each experiment configuration represents a combination of model, dataset and query strategy, and has been run for five times.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.1 Datasets", "text": "We used datasets that are well-known benchmarks in text classification and active learning. All datasets are accessible to the Python ecosystem via Python libraries that provide fast access to those datasets. We obtained CR and SUBJ using gluonnlp, and AGN, MR, and TREC using huggingface datasets.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.2 Pre-Trained Models", "text": "In the experiments, we fine-tuned (i) a large BERT model (bert-large-uncased) and (ii) an SBERT paraphrase-mpnet-base model (sentencetransformers/paraphrase-mpnet-base-v2). Both are available via the huggingface model repository.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.3 Hyperparameters", "text": "Maximum Sequence Length We set the maximum sequence length to the minimum multiple of ten, so that 95% of the given dataset's sentences contain at most that many tokens.\nTransformer Models For BERT, we adopt the hyperparameters from Schr\u00f6der et al. (2022). For SetFit, we use the same learning rate and optimizer parameters but we train for only one epoch.", "publication_ref": ["b42"], "figure_ref": [], "table_ref": []}, {"heading": "C Evaluation", "text": "In Table 4 and Table 5 we report final accuracy and AUC scores including standard deviations, measured after the last iteration. Note that results obtained through PE, BT, and LC are equivalent for binary datasets.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1 Evaluation Metrics", "text": "Active learning was evaluated using standard metrics, namely accuracy und area under the learning curve. For both metrics, the respective scikit-learn implementation was used.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Stopping active learning based on predicted change of F measure for text classification", "journal": "IEEE", "year": "2019", "authors": "Michael Altschuler; Michael Bloodgood"}, {"ref_id": "b1", "title": "Deep batch active learning by diverse, uncertain gradient lower bounds", "journal": "", "year": "2020", "authors": "Jordan T Ash; Chicheng Zhang; Akshay Krishnamurthy; John Langford; Alekh Agarwal"}, {"ref_id": "b2", "title": "Bayesian active learning for production, a systematic study and a reusable library", "journal": "", "year": "2020", "authors": "Parmida Atighehchian; Fr\u00e9d\u00e9ric Branchaud-Charron; Alexandre Lacoste"}, {"ref_id": "b3", "title": "Scalable k-Means Clustering via Lightweight Coresets", "journal": "", "year": "2018", "authors": "Olivier Bachem; Mario Lucic; Andreas Krause"}, {"ref_id": "b4", "title": "A method for stopping active learning based on stabilizing predictions and the need for user-adjustable stopping", "journal": "", "year": "2009", "authors": "Michael Bloodgood; K Vijay-Shanker"}, {"ref_id": "b5", "title": "Debiased contrastive learning", "journal": "Curran Associates, Inc", "year": "2020", "authors": "Ching-Yao Chuang; Joshua Robinson; Yen-Chen Lin; Antonio Torralba; Stefanie Jegelka"}, {"ref_id": "b6", "title": "Similarity search for efficient active learning and search of rare concepts", "journal": "", "year": "2022", "authors": "Cody Coleman; Edward Chou; Julian Katz-Samuels; Sean Culatana; Peter Bailis; Alexander C Berg; Robert Nowak; Roshan Sumbaly; Matei Zaharia; I Zeki Yalniz"}, {"ref_id": "b7", "title": "Reducing labeling effort for structured prediction tasks", "journal": "", "year": "2005", "authors": "Aron Culotta; Andrew Mccallum"}, {"ref_id": "b8", "title": "modAL: A modular active learning framework for Python", "journal": "", "year": "2018", "authors": "Tivadar Danka; Peter Horvath"}, {"ref_id": "b9", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b10", "title": "Active Learning for BERT: An Empirical Study", "journal": "", "year": "2020", "authors": "Alon Liat Ein-Dor; Ariel Halfon; Eyal Gera; Lena Shnarch; Leshem Dankin; Marina Choshen; Ranit Danilevsky; Yoav Aharonov; Noam Katz;  Slonim"}, {"ref_id": "b11", "title": "Design Patterns: Elements of Reusable Object-Oriented Software", "journal": "Addison-Wesley Longman Publishing Co., Inc", "year": "1995", "authors": "Erich Gamma; Richard Helm; Ralph Johnson; John Vlissides"}, {"ref_id": "b12", "title": "Discriminative active learning", "journal": "", "year": "2019", "authors": "Daniel Gissin; Shai Shalev-Shwartz"}, {"ref_id": "b13", "title": "To softmax, or not to softmax: that is the question when applying active learning for transformer models", "journal": "", "year": "2022", "authors": "Julius Gonsior; Christian Falkenberg; Silvio Magino; Anja Reusch; Maik Thiele; Wolfgang Lehner"}, {"ref_id": "b14", "title": "Data-efficient image recognition with contrastive predictive coding", "journal": "PMLR", "year": "2020-07", "authors": "J Olivier;  H\u00e9naff"}, {"ref_id": "b15", "title": "Bayesian active learning for classification and preference learning", "journal": "", "year": "2011", "authors": "Neil Houlsby; Ferenc Huszar; Zoubin Ghahramani; M\u00e1t\u00e9 Lengyel"}, {"ref_id": "b16", "title": "Active learning by learning", "journal": "", "year": "2015", "authors": "Wei-Ning Hsu; Hsuan-Tien Lin"}, {"ref_id": "b17", "title": "Mining and summarizing customer reviews", "journal": "Association for Computing Machinery", "year": "2004", "authors": "Minqing Hu; Bing Liu"}, {"ref_id": "b18", "title": "Text categorization with support vector machines: Learning with many relevant features", "journal": "Springer", "year": "1998-04-21", "authors": "Thorsten Joachims"}, {"ref_id": "b19", "title": "Convolutional neural networks for sentence classification", "journal": "", "year": "2014", "authors": "Yoon Kim"}, {"ref_id": "b20", "title": "Is More Data Better? Re-thinking the Importance of Efficiency in Abusive Language Detection with Transformers-Based Active Learning", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Hannah Kirk; Bertie Vidgen; Scott Hale"}, {"ref_id": "b21", "title": "Introducing geometry in active learning for image segmentation", "journal": "", "year": "2015", "authors": "Ksenia Konyushkova; Raphael Sznitman; Pascal Fua"}, {"ref_id": "b22", "title": "Atal Roghman, Christoph Sandrock, and Bernhard Sick. 2021. scikitactiveml: A Library and Toolbox for Active Learning Algorithms. Preprints.org", "journal": "", "year": "", "authors": "Daniel Kottke; Marek Herde; Tuan Pham Minh; Alexander Benz; Pascal Mergard"}, {"ref_id": "b23", "title": "Stopping criteria for active learning of named entity recognition", "journal": "", "year": "2008", "authors": "Florian Laws; Hinrich Sch\u00fctze"}, {"ref_id": "b24", "title": "A sequential algorithm for training text classifiers", "journal": "", "year": "1994", "authors": "D David; William A Lewis;  Gale"}, {"ref_id": "b25", "title": "Learning question classifiers", "journal": "", "year": "2002", "authors": "Xin Li; Dan Roth"}, {"ref_id": "b26", "title": "Active Learning to Recognize Multiple Types of Plankton", "journal": "Journal of Machine Learning Research", "year": "2005", "authors": "Tong Luo; Kurt Kramer; Dmitry B Goldgof; Lawrence O Hall; Scott Samson; Andrew Remsen; Thomas Hopkins"}, {"ref_id": "b27", "title": "Active learning by acquiring contrastive examples", "journal": "", "year": "2021", "authors": "Katerina Margatina; Giorgos Vernikos"}, {"ref_id": "b28", "title": "Efficient estimation of word representations in vector space", "journal": "", "year": "2013", "authors": "Tom\u00e1s Mikolov; Kai Chen; Greg Corrado; Jeffrey Dean"}, {"ref_id": "b29", "title": "On the stability of finetuning BERT: misconceptions, explanations, and strong baselines", "journal": "", "year": "2021", "authors": "Marius Mosbach; Maksym Andriushchenko; Dietrich Klakow"}, {"ref_id": "b30", "title": "A reverse-engineering approach to subsystem structure identification", "journal": "Journal of Software Maintenance: Research and Practice", "year": "1993", "authors": "A Hausi; Mehmet A M\u00fcller; Scott R Orgun; James S Tilley;  Uhl"}, {"ref_id": "b31", "title": "Reliable Software through Composite Design", "journal": "Petrocelli/Charter", "year": "1975", "authors": "J Glenford;  Myers"}, {"ref_id": "b32", "title": "An intrinsic stopping criterion for committee-based active learning", "journal": "", "year": "2009", "authors": "Fredrik Olsson; Katrin Tomanek"}, {"ref_id": "b33", "title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "journal": "", "year": "2004", "authors": "Bo Pang; Lillian Lee"}, {"ref_id": "b34", "title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "journal": "Association for Computational Linguistics", "year": "2005", "authors": "Bo Pang; Lillian Lee"}, {"ref_id": "b35", "title": "Pytorch: An imperative style, high-performance deep learning library", "journal": "", "year": "2019", "authors": "Adam Paszke; Sam Gross; Francisco Massa; Adam Lerer; James Bradbury; Gregory Chanan; Trevor Killeen; Zeming Lin; Natalia Gimelshein; Luca Antiga; Alban Desmaison; Andreas Kopf; Edward Yang; Zachary Devito"}, {"ref_id": "b36", "title": "Scikit-learn: Machine learning in python", "journal": "Journal of Machine Learning Research (JMLR)", "year": "2011", "authors": "Fabian Pedregosa; Ga\u00ebl Varoquaux; Alexandre Gramfort; Vincent Michel; Bertrand Thirion; Olivier Grisel; Mathieu Blondel; Peter Prettenhofer; Ron Weiss"}, {"ref_id": "b37", "title": "Sentence-BERT: Sentence embeddings using Siamese BERTnetworks", "journal": "", "year": "2019", "authors": "Nils Reimers; Iryna Gurevych"}, {"ref_id": "b38", "title": "Effective active learning strategy for multilabel learning", "journal": "Neurocomputing", "year": "2018", "authors": "Oscar Reyes; Carlos Morell; Sebasti\u00e1n Ventura"}, {"ref_id": "b39", "title": "JCLAL: A Java Framework for Active Learning", "journal": "Journal of Machine Learning Research", "year": "2016", "authors": "Oscar Reyes; Eduardo P\u00e9rez; Mar\u00eda Del Carmen Rodr\u00edguez-Hern\u00e1ndez; M Habib; Sebasti\u00e1n Fardoun;  Ventura"}, {"ref_id": "b40", "title": "Automated topic categorisation of citizens' contributions: Reducing manual labelling efforts through active learning", "journal": "Springer International Publishing", "year": "2022", "authors": "Julia Romberg; Tobias Escher"}, {"ref_id": "b41", "title": "Toward optimal active learning through sampling estimation of error reduction", "journal": "", "year": "2001", "authors": "Nicholas Roy; Andrew Mccallum"}, {"ref_id": "b42", "title": "Revisiting uncertainty-based query strategies for active learning with transformers", "journal": "", "year": "2022", "authors": "Christopher Schr\u00f6der; Andreas Niekler; Martin Potthast"}, {"ref_id": "b43", "title": "Active learning for convolutional neural networks: A core-set approach", "journal": "", "year": "2018", "authors": "Ozan Sener; Silvio Savarese"}, {"ref_id": "b44", "title": "Multiple-instance active learning", "journal": "", "year": "2007", "authors": "Burr Settles; Mark Craven; Soumya Ray"}, {"ref_id": "b45", "title": "The Need for Open Source Software in Machine Learning", "journal": "Journal of Machine Learning Research", "year": "2007", "authors": "S\u00f6ren Sonnenburg; L Mikio;  Braun;  Cheng Soon; Samy Ong; Leon Bengio; Geoffrey Bottou; Yann Holmes; Klaus-Robert Lecun; Fernando M\u00fcller; Carl Edward Pereira; Gunnar Rasmussen; Bernhard R\u00e4tsch; Alexander Sch\u00f6lkopf; Pascal Smola; Jason Vincent; Robert Weston;  Williamson"}, {"ref_id": "b46", "title": "ALiPy: Active learning in python", "journal": "", "year": "2019", "authors": "Ying-Peng Tang; Guo-Xiang Li; Sheng "}, {"ref_id": "b47", "title": "Concept analysis for module restructuring", "journal": "IEEE Trans. Software Eng", "year": "2001", "authors": "Paolo Tonella"}, {"ref_id": "b48", "title": "ALToolbox: A set of tools for active learning annotation of natural language texts", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Akim Tsvigun; Leonid Sanochkin; Daniil Larionov; Gleb Kuzmin; Artem Vazhentsev; Ivan Lazichny; Nikita Khromov; Danil Kireev"}, {"ref_id": "b49", "title": "Moshe Wasserblat, and Oren Pereg. 2022. Efficient few-shot learning without prompts", "journal": "", "year": "", "authors": "Lewis Tunstall; Nils Reimers; Unso Eun Seo Jo; Luke Bates; Daniel Korat"}, {"ref_id": "b50", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b51", "title": "A stopping criterion for active learning", "journal": "Computer Speech & Language", "year": "2008", "authors": "Andreas Vlachos"}, {"ref_id": "b52", "title": "Transformers: State-of-the-art natural language processing", "journal": "", "year": "2020", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; R\u00e9mi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger; Quentin Drame; Alexander M Lhoest;  Rush"}, {"ref_id": "b53", "title": "libact: Pool-based active learning in python", "journal": "", "year": "2017", "authors": "Yao-Yuan Yang; Shao-Chuan Lee; Yu-An Chung; Tung-En Wu; Si-An Chen; Hsuan-Tien Lin"}, {"ref_id": "b54", "title": "AcTune: Uncertainty-based active self-training for active fine-tuning of pretrained language models", "journal": "", "year": "2022", "authors": "Yue Yu; Lingkai Kong; Jieyu Zhang; Rongzhi Zhang; Chao Zhang"}, {"ref_id": "b55", "title": "Cold-start active learning through selfsupervised language modeling", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Michelle Yuan; Hsuan-Tien Lin; Jordan Boyd-Graber"}, {"ref_id": "b56", "title": "Character-level convolutional networks for text classification", "journal": "Curran Associates, Inc", "year": "2015", "authors": "Xiang Zhang; Junbo Zhao; Yann Lecun"}, {"ref_id": "b57", "title": "Active discriminative text representation learning", "journal": "", "year": "2017", "authors": "Ye Zhang; Matthew Lease; Byron C Wallace"}, {"ref_id": "b58", "title": "Multi-criteria-based strategy to stop active learning for data annotation", "journal": "", "year": "2008", "authors": "Jingbo Zhu; Huizhen Wang; Eduard Hovy"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Illustrations of (a) the active learning process, and (b) the active learning setup with the components of the active learner.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "While solu-84 Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics System Demonstrations, pages 84-95 May 2-4, 2023 \u00a92023 Association for Computational Linguistics", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: Module architecture of small-text. The core module can optionally be extended with a PyTorch and transformers integration, which enable to use GPU-based models and state-of-the-art transformer-based text classifiers of the Hugging Face transformers library, respectively. The dependencies between the module's packages have been omitted.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: An exemplary learning curve showing the difference in test accuracy for breaking ties strategy on the TREC dataset, comparing BERT and SetFit. The tubes represent the standard deviation across five runs.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "This yields a binary text classification dataset containing 50 examples of the positive class (spam) and the negative class (ham) each: Active Learning Configuration Next, we configure the classifier and query strategy. Although the active learner, query strategies, and stopping criteria components are dataset-and classifieragnostic, classifier and dataset have to match (i.e. TransformerBasedClassification must be used with TransformersDataset) owing to the different underlying data structures: Since the active learner may need to instantiate a new classifier before the training step, a factory", "figure_data": "import numpy as npfrom small_text import TransformersDataset, \\TransformerModelArgumentsfrom transformers import AutoTokenizer# Fake data example:#50 spam and 50 non-spam examplestext = np.array(['this is ham'] * 50 +['this is spam'] * 50)labels = np.array([0] * 50 + [1] * 50)transformer_model = 'bert-base-uncased'tokenizer = AutoTokenizer.from_pretrained(transformer_model)train = TransformersDataset.from_arrays(text, labels, tokenizer,target_labels=np.array([0, 1]),max_length=10)from small_text import LeastConfidence, \\TransformerBasedClassificationFactory \\as TransformerFactorynum_classes = 2model_args = TransformerModelArguments(transformer_model)clf_factory = TransformerFactory(model_args, 1 num_classes, kwargs={'device': 'cuda'})query_strategy = LeastConfidence()"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Comparison between small-text and relevant previous active learning libraries. We abbreviated the number of query strategies by \"QS\", the number of stopping criteria by \"SC\", and the low-resource-text-classification framework by lrtc. All information except \"Publication Year\" and \"Code Repository\" has been extracted from the linked GitHub repository of the respective library on February 24th, 2023. Random baselines were not counted towards the number of query strategies. Publications: 1 Reyes et al. (2016),", "figure_data": "on textclassification and has a number of built-in mod-els, datasets, and query strategies to perform ac-tive learning experiments. Another recent libraryis scikit-activeml which offers general activelearning built around scikit-learn. It comeswith 29 query strategies but provides no stoppingcriteria. GPU-based functionality can be used viaskorch, 2 a PyTorch wrapper, which is a ready-to-use adapter as opposed to our implemented clas-sifier structures but is on the other hand restrictedto the scikit-learn interfaces. ALToolbox (Tsvi-gun et al., 2022) is an active learning frameworkthat provides an annotation interface and a bench-marking mechanism to develop new query strate-gies. While it has some overlap with small-text,it is not a library, but also focuses on text data,namely on text classification and sequence tagging."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Key characteristics about the examined datasets: 1 Zhang et al. (2015), 2 Hu and Liu (2004), 3 Pang and Lee (2005), 4 Pang and Lee (2004), 5", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": "with an extended set ofquery strategies. Starting with a pool-based ac-tive learning setup with 25 initial samples, we per-form 20 queries during each of which 25 instancesare queried and labeled. Since SetFit has onlybeen evaluated for single-label classification (Tun-stall et al., 2022), we focus on single-label clas-sification as well. The goal is to compare thefollowing two models: (i) BERT (bert-large-uncased; (Devlin et al., 2019)) with 336M param-eters and (ii) SBERT (paraphrase-mpnet-base-v2; (Reimers and Gurevych, 2019)) with 110M pa-rameters. The first model is trained via vanilla fine-tuning and the second using SetFit. For the sake of"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "", "figure_data": ": The \"Rank\" columns show the mean rank whenordered by mean accuracy (Acc.) and by area undercurve (AUC). The \"Result\" columns show the meanaccuracy and AUC. All values used in this table refer tostate after the final iteration. Query strategies are abbre-viated as follows: prediction entropy (PE), breaking ties(BT), least confidence (LC), contrastive active learning(CA), BALD (BA), BADGE (BD), greedy coreset (CS),and random sampling (RS)."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": ".898 0.003 0.901 0.004 0.900 0.001 0.889 0.010 0.889 0.008 0.894 0.003 0.881 0.006 0.886 0.004 SetFit 0.900 0.002 0.902 0.004 0.902 0.002 0.892 0.006 0.887 0.010 0.896 0.003 0.896 0.003 0.877 0.005 CR BERT 0.920 0.009 0.920 0.009 0.916 0.006 0.917 0.010 0.919 0.010 0.911 0.010 0.915 0.012 0.902 0.014 SetFit 0.937 0.014 0.937 0.014 0.937 0.014 0.938 0.009 0.934 0.004 0.913 0.011 0.939 0.011 0.912 0.010 MR BERT 0.850 0.005 0.850 0.005 0.846 0.008 0.844 0.008 0.859 0.003 0.835 0.017 0.843 0.006 0.831 0.020 SetFit 0.871 0.009 0.871 0.009 0.871 0.009 0.869 0.004 0.867 0.005 0.864 0.008 0.870 0.008 0.871 0.003 SUBJ BERT 0.959 0.005 0.959 0.005 0.958 0.003 0.958 0.008 0.959 0.003 0.948 0.006 0.957 0.004 0.937 0.006 SetFit 0.962 0.004 0.962 0.004 0.962 0.004 0.960 0.002 0.966 0.002 0.942 0.002 0.963 0.003 0.932 0.005 TREC-6 BERT 0.960 0.002 0.966 0.003 0.960 0.008 0.965 0.006 0.958 0.007 0.958 0.009 0.952 0.015 0.947 0.009 SetFit 0.966 0.005 0.961 0.005 0.966 0.005 0.963 0.008 0.961 0.005 0.958 0.005 0.967 0.004 0.946 0.009", "figure_data": "Dataset ModelQuery StrategyPEBTLCCABABDCSRSAGNBERT 0"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Final accuracy per dataset, model, and query strategy. We report the mean and standard deviation over five runs. The best result per dataset is printed in bold. Query strategies are abbreviated as follows: prediction entropy (PE), breaking ties (BT), least confidence (LC), contrastive active learning (CA), BALD (BA), BADGE (BD), greedy coreset (CS), and random sampling (RS). The best result per dataset is printed in bold..009 0.839 0.014 0.836 0.009 0.821 0.015 0.819 0.012 0.840 0.003 0.804 0.012 0.825 0.011 SetFit 0.881 0.002 0.889 0.003 0.885 0.005 0.879 0.004 0.869 0.006 0.881 0.002 0.881 0.003 0.867 0.004 CR BERT 0.885 0.007 0.885 0.007 0.881 0.007 0.881 0.011 0.882 0.006 0.876 0.005 0.874 0.011 0.877 0.011 SetFit 0.925 0.001 0.925 0.001 0.925 0.001 0.927 0.003 0.924 0.005 0.910 0.005 0.930 0.002 0.908 0.008 MR BERT 0.819 0.010 0.819 0.010 0.820 0.007 0.813 0.009 0.817 0.013 0.808 0.011 0.804 0.010 0.813 0.004 SetFit 0.859 0.004 0.859 0.004 0.859 0.004 0.859 0.003 0.858 0.004 0.855 0.002 0.858 0.004 0.857 0.002 SUBJ BERT 0.944 0.008 0.944 0.008 0.943 0.007 0.940 0.009 0.939 0.009 0.929 0.005 0.934 0.006 0.924 0.007 SetFit 0.953 0.002 0.953 0.002 0.953 0.002 0.952 0.003 0.950 0.002 0.940 0.003 0.949 0.001 0.935 0.002 TREC-6 BERT 0.818 0.033 0.855 0.023 0.837 0.034 0.829 0.030 0.816 0.029 0.856 0.024 0.799 0.037 0.843 0.008 SetFit 0.910 0.008 0.934 0.005 0.919 0.008 0.917 0.013 0.907 0.017 0.934 0.010 0.927 0.008 0.927 0.004", "figure_data": "Dataset ModelQuery StrategyPEBTLCCABABDCSRSAGNBERT 0.827 0"}], "formulas": [], "doi": "10.1109/ICOSC.2019.8665646"}