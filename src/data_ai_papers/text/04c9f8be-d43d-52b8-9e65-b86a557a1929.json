{"title": "Clustering What Matters: Optimal Approximation for Clustering with Outliers", "authors": "Akanksha Agrawal; Tanmay Inamdar; Saket Saurabh; Jie Xue", "pub_date": "2023-02-18", "abstract": "Clustering with outliers is one of the most fundamental problems in Computer Science. Given a set X of n points and two numbers k, m, the clustering with outliers aims to exclude m points from X and partition the remaining points into k clusters that minimizes a certain cost function. In this paper, we give a general approach for solving clustering with outliers, which results in a fixed-parameter tractable (FPT) algorithm in k and m 1 , that almost matches the approximation ratio for its outlier-free counterpart. As a corollary, we obtain FPT approximation algorithms with optimal approximation ratios for k-Median and k-Means with outliers in general and Euclidean metrics. We also exhibit more applications of our approach to other variants of the problem that impose additional constraints on the clustering, such as fairness or matroid constraints.", "sections": [{"heading": "Introduction", "text": "Clustering is a family of problems that aims to group a given set of objects in a meaningful way-the exact \"meaning\" may vary based on the application. These are fundamental problems in Computer Science with applications ranging across multiple fields like pattern recognition, machine learning, computational biology, bioinformatics and social science. Thus, these problems have been a subject of extensive studies in the field of Algorithm Design (and its sub-fields), see for instance, the surveys on this topic (and references therein) [24,22,5].\nTwo of the central clustering problems are k-Median and k-Means. In the standard k-Median problem, we are given a set X of n points, and an integer k, and the goal is to find a set C * \u2286 X of at most k centers, such that the following cost function is minimized over all subsets C of size at most k. cost(X, C) := In k-Means, the objective function instead contains the sum of squares of distances.\nOften real world data are contaminated with a small amount of noise and these noises can substantially change the clusters that we obtain using the underlying algorithm. To circumvent the issue created by such noises, there are several studies of clustering problems with outliers, see for instance, [8,21,15,12,13,1].\nIn outlier extension of the k-Median problem, which we call k-MedianOut, we are also given an additional integer m \u2265 0 that denotes the number of outliers that we are allowed to drop. We want to find a set C of at most k centers, and a set Y \u2286 X of at most m outliers, such that cost(X \\ Y, C) := p\u2208X\\Y min c\u2208C d(p, c) is minimized over all (Y, C) satisfying the requirements. Observe that the cost of clustering for k-MedianOut equals the sum of distances of each point to its nearest center, after excluding a set of m points from consideration 2 . We remark that in a similar spirit we can define the outlier version of the k-Means problem, which we call k-MeansOut.\nIn this paper, we will focus on approximation algorithms. An algorithm is said to have an approximation ratio of \u03b1, if it is guaranteed to return a solution of cost no greater than \u03b1 times the optimal cost, while satisfying all other conditions. That is, the solution must contain at most k centers, and drop m outliers. If the algorithm is randomized, then it must return such a solution with high probability, i.e., probability at least 1 \u2212 n \u2212c for some c \u2265 1.\nFor a fixed set C of centers, the set of m outliers is automatically defined, namely the set of m points that are farthest from C (breaking ties arbitrarily). Thus, an optimal clustering for k-MedianOut, just like k-Median, can be found in n O(k) time by enumerating all center sets. On the other hand, we can enumerate all n O(m) subsets of outliers, and reduce the problem directly to k-Median. Other than these straightforward observations, there are several non-trivial approximations known for k-MedianOut, which we discuss in a subsequent paragraph.\nOur Results. In this work, we describe a general framework that reduces a clustering with outliers problem (such as k-MedianOut or k-MeansOut) to its outlier-free counterpart in an approximation-preserving fashion. More specifically, given an instance I of k-MedianOut, our reduction runs in time f (k, m, ) \u2022 n O (1) , and produces multiple instances of k-Median, such that a \u03b2-approximation for at least one of the produced instances of k-Median implies a (\u03b2 + )approximation for the original instance I of k-MedianOut. This is the main result of our paper.\nOur framework does not rely on the specific properties of the underlying metric space. Thus, for special metrics, such as Euclidean spaces, or shortest-path metrics induced by sparse graph classes, for which FPT (1 + )-approximations are known for k-Median, our framework implies matching approximation for k-MedianOut. Finally, our framework is quite versatile in that one can extend it to obtain approximation-preserving FPT reductions for related clustering with outliers problems, such as k-MeansOut, and clustering problems with fair outliers (such as [4,20]), and Matroid Median with Outliers. We conclude by giving a partial list of the corollaries of our reduction framework. The running time of each algorithm is f (k, m, ) \u2022 n O (1) for some function f that depends on the problem and the setting. Next to each result, we also cite the result that we use as a black box to solve the outlier-free clustering problem.\n\u2022 (1 + 2 e + ) \u2248 (1.74 + )-approximation (resp. 1 + 8 e + )-approximation) for k-MedianOut (resp. k-MeansOut) in general metrics [11]. These approximations are tight even for m = 0, under a reasonable complexity theoretic hypothesis, as shown in the same paper.\n\u2022 (1 + )-approximation for k-MedianOut and k-MeansOut in (i) metric spaces of constant doubling dimensions, which includes Euclidean spaces of constant dimension, (ii) metrics induced by graphs of bounded treewidth, and (iii) metrics induced by graphs that exclude a fixed graph as a minor (such as planar graphs). [11].\n\u2022 (2 + )-approximation for Matroid Median with Outliers in general metrics, where k refers to the rank of the matroid. [10] \u2022 (1 + 2 e + )-approximation for Colorful k-Median in general metrics, where m denotes the total number of outliers across all color classes [10]. The preceding two problems are orthogonal generalizations of k-MedianOut, and are formally defined in Section 4.\nOur Techniques. Our reduction is inspired from the following seemingly simple observation that relates k-MedianOut and k-Median. Let I be an instance of k-MedianOut, where we want to find a set C of k centers, such that the sum of distances of all except at most m points to the nearest center in C is minimized. By treating the outliers in an optimal solution for I as virtual centers, one obtains a solution for (k + m)-Median without outliers whose cost is at most the optimal cost of I. In other words, the optimal cost of an appropriately defined instance I of (k + m)-Median is a lower bound on the optimal cost of I. Since k-Median is a well-studied problem, at this point, one would hope that it is sufficient to restrict the attention to I. That is, if we obtain a solution (i.e., a set of k + m centers) for I, can then be modified to obtain a solution (i.e., a set of k centers and m outliers) for I. However, it is unclear whether one can do such a modification without blowing up the cost for I. Nevertheless, this connection between I and I turns out to be useful, but we need several new ideas to exploit it.\nAs in before, we start with a constant approximation for I, and perform a sampling similar to [9] to obtain a weighted set of points. This set is obtained by dividing the set of points connected to each center in the approximate solution into concentric rings, such that the \"error\" introduced in the cost by treating all points in the ring as identical is negligible. Then, we sample O((k +m) log n/ ) points from each ring, and give each point an appropriate weight. We then prove a crucial concentration bound (cf. Lemma 1), which informally speaking relates the connection cost of original set of points in a ring, and the corresponding weighted sample. In particular, for any set of k centers, with good probability, the difference between the original and the weighted costs is \"small\", even after excluding at most m outliers from both sets. Intuitively speaking, this concentration bound holds because the sample size is large enough compared to both k and m. Then, by taking the union of all such samples, we obtain a weighted set S of O(((k + m) log n/ ) 2 ) points that preserves the connection cost to any set of k centers, even after excluding m outliers with at least a constant probability. Then, we enumerate all sets Y of size m from S, and solve the resulting k-Median instance induced on S \\ Y . Finally, we argue that at least one of the resulting instances I will have the property that, a \u03b2-approximation for I implies a (\u03b2 + )-approximation for I.\nRelated Work. The first constant approximation for k-MedianOut was given by [8] for some large constant. More recently, [21,17] gave constant approximations based on iterative LP rounding technique, and the 6.387-approximation by latter is currently the best known approximation. These approximation algorithms run in polynomial time in n. [21] also give the best known polynomial approximations for related problems of k-MeansOut and Matroid Median. Now we turn to FPT approximations, which is also the setting for our results. To the best of our knowledge, there are three works in this setting, [12,15,23]. The idea of relating k-Median with m Outliers to (k + m)-Median that we discuss above is also present in these works. Even though it is not stated explicitly, the approach of [23] can be used to obtain FPT approximations in general metrics; albeit with a worse approximation ratio. However, by using additional properties of Euclidean k-MedianOut/k-MeansOut (where one is allowed to place centers anywhere in R d ) their approach yields a (1 + )-approximation in FPT time. [15] design approximation algorithms with ratio of 3+ for k-MedianOut (resp. 9+ for k-MedianOut) in time ((k +m)/ ) O(k) \u2022n O (1) . Thus, our approximation ratios of 1 + 2 e + for k-MeansOut, and 1 + 8 e + for k-MeansOut improve on these results -albeit with a slightly worse FPT running time. Furthermore, our result is essentially an approximation-preserving reduction from k-MedianOut to k-Median in the same metric, which yields (1 + )-approximations in some special settings as discussed earlier. On the other hand, it seems that a loss of 3 + (resp. 9 + ) in the approximation guarantee is inherent to the algorithm of [15].\nOn the lower bound side, [16] showed it is NP-hard to approximate k-Median (and thus k-MedianOut) within a factor 1 + 2 e \u2212 for any > 0. Recently, [10] strengthened this result assuming Gap-ETH, and showed that an (1 + 2 e \u2212 )-approximation algorithm must take at least n k g( ) time for some function g().\nBicriteria approximations relax the strict requirement of using at most k centers, or dropping at most m outliers, in order to give improved approximation ratios, or efficiency (or both). For k-MedianOut, [6] gave a 4(1 + 1/ )-approximation, while dropping m(1 + ) outliers. [18] gave a constant approximation based on local search for k-MeansOut that drops O(km log(n\u2206)) outliers, where \u2206 is the diameter of the set of points. [14] gave a (25 + )-approximation that uses k(1 + ) centers but only drops m outliers. In Euclidean spaces, they also give a (1 + )-approximation that returns a solution with k(1 + ) centers.", "publication_ref": ["b23", "b21", "b4", "b7", "b20", "b14", "b11", "b12", "b0", "b1", "b0", "b3", "b19", "b0", "b10", "b10", "b9", "b9", "b8", "b7", "b20", "b16", "b20", "b11", "b14", "b22", "b22", "b14", "b0", "b14", "b15", "b9", "b5", "b17", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "Basic notions. Let (\u0393, d) be a metric space, where \u0393 is a finite set of points, and d : \u0393\u00d7\u0393 \u2192 R is a distance function satisfying symmetry and triangle inequality. The k-median problem. In the k-Median problem, an instance is a triple I = (X, F, k), where X and F are finite sets of points in some metric space (\u0393, d), and k \u2265 1 is an integer. The points in X are called clients, and the points in F are called facilities or centers. The task is to find a subset C \u2286 F of size at most k that minimizes the cost function\ncost(X, C) := p\u2208X d(p, C).\nThe size of an instance I = (X, F, k) is defined as |I| = |X \u222a F |, which we denote by n.\nk-median with outliers. The input to k-MedianOut contains an additional integer 0 \u2264 m \u2264 n, and thus an instance is given by a 4-tuple I = (X, F, k, m). Let C \u2286 F be a set of facilities. We define cost m (X, C) := sum \u223cm {cost(p, C) : p \u2208 X}, i.e., the sum of n \u2212 m smallest distances of points in X to the set of centers C. The goal is to find a set of centers C minimizing cost m (X, C) over all sets C \u2286 F of size at most k. Given a set C \u2286 F of centers, we denote the corresponding solution by (Y, C), where Y \u2286 X is a set of m outlier points in X with largest distances realizing cost m (X, C). Given an instance I of k-MedianOut, we use OPT(I) to denote the value of an optimal solution to I.\nWeighted sets and random samples. During the course of the algorithm, we will often deal with weighted sets of points. Here, S \u2286 X is a weighted set, with each point p \u2208 S having integer weight w(p) \u2265 0. For any set C \u2286 F and 1 \u2264 m \u2264 |S|, define wcost m (S, C) := sum \u223cm {d(p, C) \u2022 w(p) : p \u2208 S}. A random sample of a finite set S refers to a random subset of S. Throughout this paper, random samples are always generated by picking points uniformly and independently.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "k-Median with Outliers", "text": "In this section, we give our FPT reduction from k-MedianOut to the standard k-Median problem. Formally, we shall prove the following theorem. \nO(m) \u2022 T (n, k) \u2022 n O(1)\n, where n is the instance size and m is the number of outliers.\nCombining the above theorem with the known (1 + 2 e + )-approximation k-median algorithm [10] that runs in (k/ ) O(k) \u2022 n O(1) time, we directly have the following result. Corollary 1. There exists a (1 + 2 e + )-approximation algorithm for k-MedianOut with running 1) , where n is the instance size and m is the number of outliers.\ntime k+m O(m) \u2022 k O(k) \u2022 n O(\nThe rest of this section is dedicated to proving Theorem 1. Let I = (X, F, k, m) be an instance of k-MedianOut. We define a (k +m)-Median instance I = (X, F \u222aX, k +m), where in addition to the original set of facilities, there is a facility co-located with each client. We have the following observation.\nObservation 1. OPT(I ) \u2264 OPT(I), i.e., the value of an optimal solution to I is a lower bound on the value of an optimal solution to I.\nProof. Let (Y * , C * ) be an optimal solution to I realizing the value OPT(I). We define a solution (Y , C ) for I as follows: let Y = X, and C = C * \u222a Y * . That is, the set of centers C is obtained by adding a facility co-located with each outlier point from Y * . Now we argue about the costs.\nSince C * \u2286 C , for each point p \u2208 Y * , d(p, C ) \u2264 d(p, C * ). On the other hand, for each q \u2208 X \\ Y * , d(q, C ) = 0, since there is a co-located center in C * . This implies that cost 0 (X, C ) \u2264 cost m (X, C).\nSince the solution (Y , C ) is feasible for the instance I , it follows that OPT(I ) is no larger than the cost cost 0 (X, C ). Now, we use \u03c4 -approximation algorithm guaranteed by the theorem, for the instance I , and obtain a set of at most\nk \u2264 k + m centers A such that cost 0 (X, A) \u2264 \u03c4 \u2022 OPT(I ) \u2264 \u03c4 \u2022 OPT(I).\nBy assumption, running this algorithm takes polynomial time. Let R = cost 0 (X,A) \u03c4 n be a lower bound on average radius, and \u03c6 = log(\u03c4 n) . For each center c i \u2208 A, let X i \u2286 X denote the set of points whose closest center in A is c i . By arbitrarily breaking ties, we can assume that the sets X i are disjoint, i.e., {X i } 1\u2264i\u2264k forms a partition of X. Now we further partition each X i into smaller groups such that the points in each group have similar distances to c i . Specifically, we define\nX i,j := B X i (c i , R) if j = 0, B X i (c i , 2 j R) \\ B X i (c i , 2 j\u22121 R) if j \u2265 1. Let s = c\u03c4 2 2 (m + k ln n + ln(1/\u03bb))\n, for some large enough constant c. We define a weighted set of points S i,j \u2286 X i,j as follows. If |X i,j | \u2264 s, then we say X i,j is small. In this case, define S i,j := X i,j and let the weight w i,j of each point p \u2208 S i,j be 1. Otherwise, |X i,j | > s and we say that X i,j is large. In this case, we take a random sample S i,j \u2286 X i,j of size s. We set the weight of every point in S i,j to be w i,j = |X i,j |/|S i,j |. For convenience, assume the weights w i,j to be integers 3 . Finally, let S = i,j S i,j . The set S can be thought of as an -coreset for the k-MedianOut instance I. Even though we do not define this notion formally, the key properties of S will be proven in Lemma 2 and 3. Thus, we will often informally refer to S as a coreset.\nProposition 1. We have |S| = O(((k + m) log n/ ) 2 ) if \u03bb is a constant.\nProof. For any p \u2208 X, d(p, A) \u2264 cost 0 (X, A) = \u03c4 n \u2022 R \u2264 2 \u03c6 R. Therefore, for any c i \u2208 A and j > \u03c6, X i,j = \u2205, and X i = \u03c6 j=0 X i,j . It follows that the number of non-empty sets X i,j is at most\n|A| \u2022 (1 + log(\u03c4 n)) = O((k + m) log n), since |A| \u2264 k + m and \u03c4 is a constant. For each non-empty X i,j , |S i,j | \u2264 2s = O((m + k log n)/ 2\n), if \u03bb is a constant. Since S = i,j S i,j , the claimed bound follows.\nProposition 2. [9,19] Let M \u2265 0 and \u03b7 be fixed constants, and let h(\u2022) be a function defined on a set V such that \u03b7 \u2264 h(p) \u2264 \u03b7 + M for all p \u2208 V . Let U \u2286 V be a random sample of size s, and \u03b4 > 0 be a parameter.\nIf s \u2265 M 2 2\u03b4 2 ln(2/\u03bb), then Pr h(V ) |V | \u2212 h(U ) |U | \u2265 \u03b4 \u2264 \u03bb,\nwhere h(U ) := u\u2208U h(u), and h(V ) := v\u2208V h(v).\nLemma 1. Let (\u0393, d) be a metric space, V \u2286 \u0393 be a finite set of points, \u03bb , \u03be > 0, q \u2265 0 be parameters, and define s = 4 \u03be 2 q + ln 2 \u03bb . Suppose U \u2286 V is a random sample of size s . Then for any fixed finite set C \u2286 F with probability at least 1 \u2212 \u03bb it holds that for any 0 \u2264 t \u2264 q, Observation 2. The following inequalities hold.\n|cost t (V, C) \u2212 wcost t (U, C)| \u2264 \u03be|V |(diam(V ) + d(V, C)),\n\u2022 t |U | |V | \u2212 1 \u2264 t \u2264 t |U | |V | \u2022 h (V ) \u2264 h(V ) \u2212 t \u2022 \u03b7(V ) \u2264 h(V ), and h (V ) \u2265 h(V ) \u2212 t \u2022 (\u03b7(V ) + diam(V )) \u2022 h (U ) \u2264 h(U ), and h (U ) \u2265 h(U ) \u2212 t |U | |V | \u2022 (\u03b7(U ) + diam(U )) \u2022 \u03b7(V ) \u2264 \u03b7(U ) \u2264 \u03b7(V ) + diam(V )\nProof. The first item is immediate from the definition t = t|U |/|V | . Consider the second item.\nFor each v \u2208 V , let g(v) := d(v, C) \u2212 \u03b7(V ). Let V \u2286 V denote a set of points of size t that have the t largest distances to the centers in C. By triangle inequality, we get that\nd(v, C) \u2264 d(v, v * ) + d(v * , C) \u2264 diam(V ) + \u03b7(V ), where v * \u2208 V is a point realizing the minimum distance \u03b7(V ) to the set of centers C. This implies that g(v) \u2264 diam(V ) for all v \u2208 V . Now, observe that h(V ) = h (V ) + v\u2208V (\u03b7(V ) + g(v)) (Since h (V ) excludes the distances of points in V ) = h (V ) + t \u2022 \u03b7(V ) + v\u2208V g(v) (1) \u2265 h (V ) + t \u2022 \u03b7(V ) (g(v) \u2265 0 for all v \u2208 V )\nBy rearranging the last inequality, we get the first part of the second item. To see the second part, observe that (1) implies that h(V\n) \u2264 h (V ) + t \u2022 \u03b7(V ) + t \u2022 diam(V ), since g(v) \u2264 diam(V ) for all v \u2208 V .\nThe proof of the third item is analogous to the proof of the first item. In addition, we need to combine the inequalities from the first item of the observation. We omit the details. The fourth item follows from the fact that U \u2286 V , and via triangle inequality.\nBy applying Proposition 2 with \u03b7 = \u03b7(V ), M = diam(V ) and \u03b4 = \u03beM/2, we know with probability at most \u03bb ,\nv\u2208V d(v, C) |V | \u2212 u\u2208U d(u, C) |U | \u2265 \u03be 2 diam(V ).\nRecall that, h(V ) = v\u2208V d(v, C) and h(U ) = u\u2208U d(u, C). Thus, with probability at least 1 \u2212 \u03bb , we have that h(V )\n|V | \u2212 h(U ) |U | \u2264 \u03be 2 diam(V ).(2)\nNow, we prove the following technical claim.\nClaim 1. Suppose (2) holds. Then we have,\nh (V ) |V | \u2212 h (U ) |U | \u2264 \u03be \u2022 (diam(V ) + d(V, C))(3)\nProof. We suppose that (2) holds, and show that (3) holds with probability 1. First, consider, h (U )\n|U | \u2212 h (V ) |V | \u2264 h(U ) |U | \u2212 h(V ) |V | + t \u2022 (\u03b7(V ) + diam(V )) |V | (From Observation 2, Part 2) \u2264 \u03be 2 diam(V ) + t |V | \u2022 (\u03b7(V ) + diam(V )) (From (2)) \u2264 \u03be 2 diam(V ) + \u03be 2 \u2022 (\u03b7(V ) + diam(V ))(4)\nwhere the last inequality follows from the assumption that\n|V | \u2265 s \u2265 4q \u03be \u2265 4t \u03be . Now, consider h (V ) |V | \u2212 h (U ) |U | \u2264 h(V ) |V | \u2212 h(U ) |U | \u2212 t\u03b7(V ) |V | + t |U | |V | \u2022 (\u03b7(U ) + diam(U )) |U | (From Observation 2, Part 3) \u2264 \u03be 2 diam(V ) \u2212 t \u2022 \u03b7(V ) |V | + t \u2022 \u03b7(U ) |V | + t \u2022 diam(U ) |V | (From (2)) \u2264 \u03be 2 diam(V ) \u2212 t \u2022 \u03b7(V ) |V | + t \u2022 (\u03b7(V ) + diam(V )) + t \u2022 diam(U ) |V | (From Observation 2, Part 4) \u2264 \u03be 2 diam(V ) + 2t \u2022 diam(V ) |V | (diam(U ) \u2264 diam(V )) \u2264 \u03bediam(V )(5)\nwhere the last inequality follows from the assumption that\n|V | \u2265 s \u2265 4q \u03be \u2265 4t \u03be .\nThus, from Claim 1, we know that since (2) holds with probability at least 1 \u2212 \u03bb , the following inequality also holds with probability at least 1 \u2212 \u03bb .\nh (V ) \u2212 h (U ) \u2022 |V | |U | \u2264 \u03be|V | \u2022 (diam(V ) + d(V, C)).\nThe preceding inequality is equivalent to the one in the lemma, because h (V ) = cost t (V, C), and h (U )\n\u2022\n|V | |U | = |V | |U | \u2022 cost t (U, C) = wcost t (U, C).\nFinally, notice that Claim 1 holds when the h function is defined with respect to any choice of t \u2208 {0, 1, . . . , q}. Therefore, with probability at least 1 \u2212 \u03bb , the inequality in the lemma holds for any t \u2208 {0, 1, . . . , q}, which completes the proof.\nNext, we show the following observation, whose proof is identical to an analogous proof in [8].\nObservation 3. The following inequalities hold.\n\u2022 i,j |X i,j |2 j R \u2264 3 \u2022 cost 0 (X, A) \u2264 3\u03c4 \u2022 OPT(I). \u2022 i,j |X i,j |diam(X i,j ) \u2264 6 \u2022 cost 0 (X, A) \u2264 6\u03c4 \u2022 OPT(I).\nProof. For any p \u2208 X i,j , it holds that 2 j R \u2264 max {2d(p, A), R} \u2264 2d(p, A) + R. Therefore, i,j\n|X i,j | \u2022 2 j R \u2264 i,j p\u2208X i,j 2 j R \u2264 i,j p\u2208X i,j 2d(p, A) + R = 2 p\u2208X d(p, A) + |X| \u2022 |R| = 2 \u2022 cost 0 (X, A) + n|R| \u2264 3 \u2022 cost 0 (X, A) (By definition of R) \u2264 3\u03c4 OPT(I ) \u2264 3\u03c4 OPT(I).\nWe also obtain the second item by observing that diam(X i,j ) \u2264 2 \u2022 2 j \u2022 R.\nNext, we show that the following lemma, which informally states that the union of the sets of sampled points approximately preserve the cost of clustering w.r.t. any set of at most k centers, even after excluding at most m outliers overall.\nLemma 2. The following statement holds with probability at least 1 \u2212 \u03bb/2: For all sets C \u2286 F of size at most k, and for all sets of non-negative integers\n{m i,j } i,j such that i,j m i,j \u2264 m, i,j cost m i,j (X i,j , C) \u2212 i,j wcost t i,j (S i,j , C) \u2264 \u2022 i,j cost m i,j (X i,j , C)(6)\nwhere t i,j = m i,j /w i,j .\nProof. Fix an arbitrary set C \u2286 F of at most k centers, and the integers {m i,j } i,j such that i,j m i,j \u2264 m as in the statement of the lemma. For each i = 1, . . . , |A|, and 0 \u2264 j \u2264 \u03c6, we invoke Lemma 1 by setting V = X i,j , and U = S i,j , \u03be = 8\u03c4 , \u03bb = n \u2212k \u03bb/(4(k + m)(1 + \u03c6)), and q = m. This implies that, the following inequality holds with probability at least 1 \u2212 \u03bb for each set X i,j , and the corresponding m i,j \u2264 m,\ncost m i,j (X i,j , C) \u2212 wcost t i,j (S i,j , C) \u2264 8\u03c4 |X i,j |(diam(X i,j ) + d(X i,j , C))(7)\nNote that the sample size required in order for this inequality to hold is\ns = 4 \u03be 2 m + ln 2 \u03bb = 4 8\u03c4 2 \u2022 m + ln 8n k (k + m)(1 + \u03c6) \u03bb \u2264 s.\nFor any i, j, if X i,j < s (i.e., X i,j is small ), then the sample S i,j is equal to X i,j , and each point in S i,j has weight equal to 1. This implies that cost m i,j (X i,j , C) = wcost t i,j (S i,j , C) for all such X i,j , and their contribution to the right hand side of inequality ( 6) is zero. Thus, it suffices to restrict the sum on the right hand side of (6) over large sets X i,j 's. Let L consist of all pairs (i, j) such that X i,j is large. We have the following claim.", "publication_ref": ["b0", "b2", "b8", "b18", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Claim 2.", "text": "(i,j)\u2208L |X i,j |d(X i,j , C) \u2264 2cost m (X, C).\nProof. Let Y denote the farthest m points in X from the set of centers C. Now, fix (i, j) \u2208 L and let q i,j := |X i,j \u2229 Y | \u2264 m denote the number of outliers in X i,j . Since |X i,j | \u2265 2m \u2265 2q i,j , the set X i,j \\ Y is non-empty, and all points X i,j \\ Y contribute towards cost m (X, C). That is,\n(i,j)\u2208L p\u2208X i,j \\Y d(p, C) \u2264 cost m (X, C)(8)\nFor any p \u2208 X i,j \\ Y , d(X i,j , C) \u2264 d(p, C) from the definition. Therefore,\n(i,j)\u2208L |X i,j | \u2022 d(X i,j , C) \u2264 (i,j)\u2208L 2|X i,j \\ Y | \u2022 d(X i,j , C) \u2264 2 \u2022 (i,j)\u2208L p\u2208X i,j \\Y d(p, C) \u2264 2 \u2022 cost m (X, C)\nHere, to see the second inequality, see that |X i,j | \u2265 2q i,j , which implies that |X i,j |\u2212q i,j \u2264 2(|X i,j |\u2212 q i,j ). The last inequality follows from (8).\nThus, by revisiting ( 6) and ( 7), we get:\n(i,j)\u2208L cost m i,j (X i,j , C) \u2212 wcost t i,j (S i,j , C) \u2264 8\u03c4 (i,j)\u2208L |X i,j |(diam(X i,j ) + d(X i,j , C)) (From (7)) \u2264 8\u03c4 \u2022 (6\u03c4 \u2022 OPT(I) + 2cost m (X, C)) (From Obs. 3 and Claim 2) = 8\u03c4 (8\u03c4 \u2022 cost m (X, C)) = \u2022 cost m (X, C)\nWhere, in the last inequality, since C is an arbitrary set of at most k centers, OPT(I) \u2264 cost m (X, C).\nNote that the preceding inequality holds for a fixed set C of centers with probability at least 1 \u2212 |A| \u2022 (1 + \u03c6)\u03bb = 1 \u2212 n \u2212k \u03bb/2, which follows from taking the union bound over all sets X i,j , 1 \u2264 i \u2264 |A| \u2264 k + m, and 0 \u2264 j \u2264 \u03c6.\nSince F has at most n k subsets of size at most k, the statement of the lemma follows from taking a union bound. Now we are ready to prove Theorem 1. We enumerate every subset T \u2286 S of size at most m. For each T , we compute a \u03b2-approximation solution for the (weighted) k-median instance (S\\T, F, k). Theorem 1 only assumes the existence of a \u03b2-approximation algorithm for unweighted k-median, which cannot be applied to weighted point sets. However, we can transform S\\T to an equivalent unweighted sets R, which contains, for each x \u2208 S\\T , w(x) copies of (unweighted) x, where w(x) is the weight of x in S\\T . It is clear that wcost(S\\T, C) = cost(R, C) for all C \u2286 F . Thus, we can apply the \u03b2-approximation k-Median algorithm on (R, F, k) to compute a center set C \u2286 F of size k such that wcost(T, C) \u2264 \u03b2 \u2022 wcost(T, C ) for any C \u2286 F of size k. We do this for all T \u2286 S of size at most m. Let C denote the set of all center sets C computed. We pick a center set C * \u2286 C that minimizes cost m (X, C * ), and return (Y * , C * ) as the solution where Y * \u2286 X consists of the m points in X farthest to the center set C * . Lemma 3. With probability at least 1 \u2212 \u03bb 2 , for all C \u2286 F of size k we have\ncost m (X, C * ) \u2264 1 + 1 \u2212 \u2022 \u03b2cost m (X, C).\nProof. The statement in Lemma 2 holds with probability at least 1\u2212\u03bb/2. Thus, it suffices to assume the statement in Lemma 2, and show cost m (X, C * ) \u2264 (1 + ) 2 \u03b2 \u2022 cost m (X, C) for any C \u2286 F of size k. Fix a subset C \u2286 F of size k. Let Y \u2286 X consist of the m points in X farthest to C, and define m i,j = |Y \u2229 X i,j |. Set t i,j = m i,j /w i,j . Note that cost m (X, C) = i,j cost m i,j (X i,j , C). Furthermore, by Lemma 2, we have i,j\nwcost t i,j (S i,j , C) \u2264 (1 + ) \u2022 i,j cost m i,j (X i,j , C) = (1 + ) \u2022 cost m (X, C).(9)\nNow let T i,j \u2286 S i,j consist of the t i,j points in S i,j farthest to C, and define T = i,j T i,j . Since |T | \u2264 m, T is considered by our algorithm and thus there exists a center set C \u2208 C that is a \u03b2-approximation solution for the (weighted) k-median instance (S\\T, F, k). We have\nwcost(S\\T, C ) \u2264 \u03b2 \u2022 wcost(S\\T, C) = \u03b2 i,j\nwcost t i,j (S i,j , C).\nNote that wcost(S\\T, C ) \u2265 i,j wcost t i,j (S i,j , C ). Furthermore, by applying Lemma 2 again, we have i,j wcost t i,j (S i,j , C ) \u2265 (1 \u2212 ) \u2022 i,j cost m i,j (X i,j , C ). It then follows that\n(1 \u2212 ) \u2022 cost m (X i,j , C ) \u2264 (1 \u2212 ) \u2022 i,j cost m i,j (X i,j , C ) \u2264 wcost(S\\T, C ). (11\n)\nFinally, we have cost m (X, C * ) \u2264 cost m (X, C ) by the construction of C * . Combining this with ( 9), (10), and ( 11), we have cost m (X, C * ) \u2264 1+ 1\u2212 \u2022 \u03b2cost m (X, C), which completes the proof.\nBy choosing \u03bb > 0 to be a sufficiently small constant, and by appropriately rescaling 4 , the above lemma shows that our algorithm outputs a (\u03b2 + )-approximation solution with a constant probability. By repeating the algorithm a logarithmic number of rounds, we can guarantee the algorithm succeeds with high probability. The number of subsets T \u2286 S of size at most m is bounded by |S| O(m) , which is ", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "Ensuring Integral Weights in the Coreset", "text": "Recall that in order to obtain the set S i,j from a large X i,j , we sample s points uniformly and independently at random (with replacement), and give each point in S i,j the weight w i,j =\n|X i,j | |S i,j | .\nIn the main body of the proof, we assumed that the quantity w i,j is integral for the sake of simplicity. However, in general |X i,j | |S i,j | may not be an integer. Here, we describe how to modify this construction to ensure integral weights.\nTo this end, let X\n(1) i,j \u2286 X i,j be an arbitrary subset of size |X i,j | mod s, and let X\n(2) i,j = X i,j \\Y i,j . From this time onward, we treat X (1) i,j and X\n(2) i,j as two separate sets of the form X \u2022,\u2022 , and proceed with the construction of the coreset.\nIn particular, observe that |X (1) i,j | < s, i.e., it is small, and |X\n(2) i,j | = t \u2022 s for some positive integer t, and thus X\n(2) i,j is large. Therefore, we let S\n(1) i,j \u2190 X (1)\ni,j , and each point is added with weight 1. On the other hand, to obtain S (2) i,j , we sample s points uniformly and independently at random from X\n(2) i,j , and set the weight of each point to be |X (2) i,j |/s, which is an integer. From this point onward, we proceed with exactly the same analysis as in the original proof, i.e., we treat X (1) i,j as a small set, and X\n(2) i,j as a large set in the analysis. Since for the small sets, the sampled set is equal to the original set, their contribution to the left hand side of the following inequality in the statement of Lemma 2, is equal to zero.\ni,j cost m i,j (X i,j , C) \u2212 i,j wcost t i,j (S i,j , C) \u2264 \u2022 i,j cost m i,j (X i,j , C)\nTherefore, the analysis of Lemma 2 goes through without any modifications. The only other minor change is that the number of points in the coreset S, which is obtained by taking the union of all S \u2022,\u2022 , is now at most twice the previous bound, which is easily absorbed in the big-oh notation.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "Extensions", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "k-Means with Outliers", "text": "This is similar to k-MedianOut, except that the cost function is the sum of squares of distances of all except m outlier points to a set of k facilities. This generalizes the well-known k-Means problem. Here, the main obstacle is that, the squares of distances do not satisfy triangle inequality, and thus it does not form a metric. However, they satisfy a relaxed version of triangle inequality\n(i.e., d(p, q) 2 \u2264 2(d(p, r) 2 + d(r, q) 2 )\n). This technicality makes the arguments tedious, nevertheless, we can follow the same approach as for k-MedianOut, to obtain optimal FPT approximation schemes. Our technique implies an optimal (1 + 8 e + )-approximation for k-MeansOut (using the result of [10] as a black-box), improving upon polynomial-time 53.002-approximation from [21], and (9 + )-approximation from [15] in time FPT in k, m and .\nIn fact, using our technique, we can get improved approximation guarantees for (k, z)-Clustering with Outliers, where the cost function involves z-th power of distances, where z \u2265 1 is fixed for a problem. Note that the cases z = 1 and z = 2 correspond to k-MedianOut and k-MeansOut respectively. We give the details for (k, z)-Clustering with Outliers in the appendix.", "publication_ref": ["b9", "b20", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Matroid Median with Outliers", "text": "A matroid is a pair M = (F, S), where F is a ground set, and S is a collection of subsets of F with the following properties: (i) \u2205 \u2208 S, (ii) If A \u2208 S, then for every subset B \u2286 A, B \u2208 S, and (iii) For any A, B \u2208 S with |B| < |A|, there exists an b \u2208 B \\ A such that B \u222a {b} \u2208 S. The rank of a matroid M is the size of the largest independent set in S. Using the definition of matroid, it can be easily seen that all inclusion-wise maximal independent sets (called bases) have the same size.\nAn instance of Matroid Median with Outliers is given by (X, F, M, m), where M = (F, S) is a matroid with rank k defined over a finite ground set F , and X, F are sets of clients and facilities, belonging to a finite metric space (\u0393, d). The objective is to find a set C \u2286 F of facilities that minimizes cost m (X, C), and C \u2208 S, i.e., C is an independent set in the given matroid. Note that an explicit description of a matroid of rank k may be as large as n k . Therefore, we assume that we are given an efficient oracle access to the matroid M. That is, we are provided with an algorithm A that, given a candidate set S \u2286 F , returns in time T (A) (which is assumed to be polynomial in |F |), returns whether S \u2208 I.\nWe can adapt our approach to Matroid Median with Outliers in a relatively straightforward manner. Recall that our algorithm needs to start with an instance of outlier-free problem (i.e., Matroid Median) that provides a lower bound on the optimal cost of the given instance. To this end, given an instance I = (X, F, M = (F, S), m) of Matroid Median with Outliers, we define an instance I = (X, F, M , 0) of Matroid Median with 0 Outliers (i.e., Matroid Median), where M = (F \u222a X, S ) is defined as follows. S = {Y \u222a C : Y \u2286 X with |Y | \u2264 m and C \u2286 F with C \u2208 S}. That is, each independent set of M is obtained by taking the union of an independent set of facilities from M, and a subset of X of size at most m. It is straightforward to show that M satisfies all three axioms mentioned above, and thus is a matroid over the ground set F \u222a X. In particular, it is the direct sum of M and a uniform matroid M m over X of rank m (i.e., where any subset of X of size at most m is independent). Note that using the oracle algorithm A, we can simulate an oracle algorithm to test whether a candidate set C \u2286 F \u222a X is independent in M . Therefore, using a (2 + )-approximation for Matroid Median [10] in time FPT in k and , we can find a set A \u2286 F \u222a X of size at most k + m that we can use to construct a coreset. The details about enumeration are similar to that for k-MedianOut, and are thus omitted.", "publication_ref": ["b9"], "figure_ref": [], "table_ref": []}, {"heading": "Colorful k-Median", "text": "This is an orthogonal generalization of k-MedianOut to ensure a certain notion of fairness in the solution (see [20]). Suppose the set of points X is partitioned into different colors X 1 X 2 . . . X . We are also given the corresponding number of outliers m 1 , m 2 , . . . , m . The goal is to find a set of at most facilities C to minimize the connection cost of all except at most m t outliers from each color class X t , i.e., we want to minimize the cost function:\nt=1 cost mt (X t , C). This follows a generalizations of the well-known k-Center problem introduced in [4] and [2,20] , called Colorful k-Center. Similar generalization of Facility Location has also been studied in [7].\nUsing our ideas, we can find an FPT approximation parameterized by k, m = t=1 m t , and . To this end, we sample sufficiently many points from each color class X t separately, and argue that it preserves the cost appropriately. The technical details follow the same outline as that for k-Median with m Outliers. In particular, during the enumeration phase-just like that for k-MedianOut-we obtain several instances of k-Median. That is, our algorithm is color-agnostic after constructing the coreset. Thus, we obtain a tight (1 + 2 e + )-approximation for this problem. This is the first non-trivial true approximation for this problem -previous work [17] only gives a pseudo-approximation, i.e., a solution with cost at most a constant times that of an optimal cost, but using slightly more than k facilities.", "publication_ref": ["b19", "b3", "b1", "b19", "b6", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "A Combination of Above Generalizations", "text": "Our technique also works for a combination of the aforementioned generalizations that are orthogonal to each other. To consider an extreme example, consider Colorful Matroid Median with different color classes (a similar version for k-Center objective has been recently studied by [3]), where we want to find a set of facilities that is independent in the given matroid, in order to minimize the sum of distances of all except m t outlier points for each color class X t . By using a combination of the ideas mentioned above, one can get FPT approximations for such generalizations.", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}, {"heading": "Concluding Remarks", "text": "In this paper, we give a reduction from k-MedianOut to k-Median that runs in time FPT in k, m, and , and preserves the approximation ratio up to an additive factor. As a consequence, we obtain improved FPT approximations for k-MedianOut in general as well as special kinds of metrics, and these approximation guarantees are known to be tight in general. Furthermore, our technique is versatile in that it also gives improved approximations for related clustering problems, such as k-MeansOut, Matroid Median with Outliers, and Colorful k-Median, among others.\nThe most natural direction is to improve the FPT running time while obtaining the tight approximation ratios. More fundamentally, perhaps, is the question whether we need an FPT dependence on the number of outliers, m; or whether it is possible to obtain approximation guarantees for k-MedianOut matching that for k-Median, with a running time that is FPT in k and alone.\nNote that the first and third line in the preceding chain of inequalities implies that (d(p, C) \u2212 d(P, C)) \u2264 diam(P ). Note that both sides of the inequality are non-negative. Thus, by taking the z-th power of both sides, the second item follows.\nConsider an instance I = ((\u0393, d), X, F, k, m) be an instance of (k, z)-Clustering with m Outliers. We define an instance I = ((\u0393, d), X, F \u222a X, k + m, 0) of (k + m, z)-Clustering (without outliers), where in addition to the original set of facilities, there is a facility co-located with each client. The following observation and its proof is analogous to Observation 1, and thus we omit the proof. Observation 4. OPT(I ) \u2264 OPT(I), i.e., the value of an optimal solution to I is a lower bound on the value of an optimal solution to I.\nThe following definitions and the construction of the coreset is analogous to that for k-median with m outliers, with appropriate modifications needed for z-th power of distances. First, we assume that there exists a \u03c4 -approximation algorithm for (k, z)-clustering problem that runs in polynomial time, where \u03c4 = O(1). Then, by using this \u03c4 -approximation algorithm for the instance I , we obtain a set of at most k \u2264 k+m centers A such that cost 0 (X, A) \u2264 \u03c4 \u2022OPT(I ) \u2264 \u03c4 \u2022OPT(I).\nLet R = cost 0 (X,A) \u03c4 n 1/z be a lower bound on average radius, and let \u03c6 = log(\u03c4 n) . For each c i \u2208 A, let X i \u2286 X denote the set of points whose closest center in A is c i . By arbitrarily breaking ties, we assume that the sets X i are disjoint, i.e., the sets {X i } 1\u2264i\u2264k form a partition of X. Now, we define the set of rings centered at each center c i as follows.\nX i,j :=\nB X i (c i , R) if j = 0 B X i (c i , 2 j R) \\ B X i (c i , 2 j\u22121 R) if j \u2265 1 Let s = c\u03c4 2 2 (c z) 2 (m + k ln n + ln(1/\u03bb))\n, for some large enough constants c, c . We define a weighted set of points S i,j \u2286 X i,j as follows. If |X i,j | \u2264 s, then say that X i,j is small, and let S i,j := X i,j , and let the weight of each point p \u2208 S i,j be 1. Otherwise, if |X i,j | > s, then say that X i,j is large. In this case, let Y i,j \u2286 X i,j be an arbitrary subset of size |X i,j | mod q. We add each point q \u2208 Y i,j to S i,j with weight 1. Furthermore, we sample s points uniformly at random (with replacement) X i,j \\ Y i,j , and add to the set S i,j with weight equal to\n|X i,j \\Y ij | s\n, which is an integer. Thus, we assume that |S i,j | \u2264 2s, and the weight of every point in S i,j is an integer. Finally, let S = i,j S i,j . Lemma 4. Let (\u0393, d) be a metric space, and let V \u2286 \u0393 be a finite set of points. Let \u03bb , \u03be > 0, q \u2265 0, be parameters, and define s = 4 \u03be 2 q + ln 2 \u03bb . If |V | \u2265 s , and U is a sample of s points picked uniformly and independently at random from V , with each point of U having weight |V |/|U |, such that the total weight w(U ) is equal to |V |, then for any fixed finite set C \u2286 \u0393, and for any 0 \u2264 t \u2264 q, with probability at least 1 \u2212 \u03bb it holds that\n|cost t (V, C) \u2212 wcost t (U, C)| \u2264 2 2z+2 \u03be|V | \u2022 (diam(V ) z + d(V, C) z ),(12)\nwhere t = t|U |/|V | .\nProof. Throughout the proof, we fix the set C and 0 \u2264 t \u2264 q as in the statement of the lemma. Next, we define the following notation. For all ). We summarize a few properties about these definitions in the following observation, which is analogous to Observation 2.\nv \u2208 V , let h(v) = cost(v, C) = d(v, C) z ,\nObservation 5.\n\u2022 t\n|U | |V | \u2212 1 \u2264 t \u2264 t |U | |V | \u2022 For any p \u2208 P , (\u03b7(V ) z \u2264 d(p, C) z = cost(p, C) z \u2264 \u03b7(V ) z + 2 z (\u03b7(V ) z + diam(V )) \u2022 h (V ) \u2264 h(V ) \u2212 t \u2022 \u03b7(V ) z \u2264 h(V ), and h (V ) \u2265 h(V ) \u2212 2 z \u2022 t \u2022 (\u03b7(V ) z + diam(V ) z ) \u2022 h (U ) \u2264 h(U ), and h (U ) \u2265 h(U ) \u2212 2 z \u2022 t |U | |V | \u2022 (\u03b7(U ) z + diam(U ) z ) \u2022 \u03b7(V ) z \u2264 \u03b7(U ) z \u2264 2 z (\u03b7(V ) z + diam(V ) z )\nProof. The first item is immediate from the definition t = t|U |/|V | . Consider the second item. For each v \u2208 V , let g(v) := d(v, C) \u2212 \u03b7(V ). Let V \u2286 V denote a set of points of size t that have the t largest distances to the centers in C. From Proposition 3, we get that for any p \u2208 V , \u03b7(V\n) z \u2264 cost(p, C) z \u2264 2 z \u2022 (\u03b7(V ) z + diam(P ) z ). This implies that g(v) \u2264 diam(V ) for all v \u2208 V . Now, observe that h(V ) = h (V ) + v\u2208V (\u03b7(V ) + g(v)) z (Since h (V ) excludes the distances of points in V ) \u2265 h (V ) + t \u2022 \u03b7(V ) z (g(v) \u2265 0 for all v \u2208 V )\nBy rearranging the last inequality, we get the first part of the third item. Also note that the first inequality also implies that h(V ) \u2264 h (V ) + 2 z t \u2022 \u03b7(V ) + 2 z v\u2208V g(v) z , via Proposition 3. Then, by recalling that g(v) \u2264 diam(V ) for all v \u2208 V , the second part of the third item follows.\nThe proof of the fourth item is analogous to that of the third item. In addition, we need to combine the inequalities from the first item of the observation. We omit the details. The fifth item follows from the fact that U \u2286 V , and via triangle inequality.\nLet \u03b7 = \u03b7(V ) z , M = 2 2z+2 (\u03b7(V ) z + diam(V ) z\n), and \u03b4 = \u03beM/2. Then, the second item of Observation 5 implies that \u03b7 \u2264 h(v) \u2264 \u03b7 + M for all v \u2208 V . Then, Proposition 2 implies that,\nPr v\u2208V cost(v, C) |V | \u2212 u\u2208U cost(u, C) |U | \u2265 \u03be 2 2 z (\u03b7(V ) z ) + diam(V ) z ) = Pr h(V ) |V | \u2212 h(U ) |U | \u2265 \u03b4 \u2264 \u03bb .\nThus, with probability at least 1 \u2212 \u03bb , we have that\nh(V ) |V | \u2212 h(U ) |U | \u2264 \u03be 2 \u2022 M (13)\nIn the rest of the proof, we condition on this event, and assume that (13) holds, and show that the inequality in the lemma holds with probability 1. First, consider,\nh (U ) |U | \u2212 h (V ) |V | \u2264 h(U ) |U | \u2212 h(V ) |V | + 2 z \u2022 t \u2022 (\u03b7(V ) z + diam(V ) z ) |V | (From Obs. 5) \u2264 \u03be 2 M + t \u2022 M |V | (From (13)) \u2264 \u03beM(14)\nwhere the last inequality follows from the assumption that\n|V | \u2265 s \u2265 4q \u03be \u2265 4t \u03be . Now, consider h (V ) |V | \u2212 h (U ) |U | \u2264 h(V ) |V | \u2212 h(U ) |U | + 2 z \u2022 t |U | |V | \u2022 (\u03b7(U ) z + diam(V ) z ) |U | (From Obs. 5, Part 4) \u2264 \u03be 2 M + 2 z \u2022 t \u2022 \u03b7(U ) z |V | + 2 z \u2022 t \u2022 diam(V ) z |V | (From (2) \u2264 \u03be 2 M + 2 2z \u2022 t \u2022 (\u03b7(V ) z + diam(V ) z ) + t \u2022 2 z \u2022 diam(V ) z |V | (From Obs. 2, Part 5) \u2264 \u03be 2 M + 2 2z+1 \u2022 t \u2022 (\u03b7(V ) z + diam(V ) z ) |V | (Since |V | \u2265 s \u2265 4q \u03be \u2265 4t \u03be ) = \u03be 2 M + \u03be 2 M = \u03beM(15)\nNote that ( 14) and ( 15) hold with probability 1, conditioned on the inequality (13) holding, which happens with probability at least 1 \u2212 \u03bb . Therefore, the following inequality holds with probability at least 1 \u2212 \u03bb :\nh (V ) \u2212 h (U ) \u2022 |V | |U | \u2264 2 2z+2 \u03be \u2022 |V | \u2022 (diam(V ) z + d(V, C) z )(16)\nwhere we recall that \u03b7(  \u2022 i,j |X i,j |(2 j R) z \u2264 (1 + 2 z ) \u2022 cost 0 (X, A) \u2264 (1 + 2 z )\u03c4 \u2022 OPT(I).\nV ) = d(V, C).\n\u2022 i,j |X i,j |diam(X i,j ) z \u2264 2 z (1 + 2 z ) \u2022 cost 0 (X, A) \u2264 2 2z+1 \u2022 \u03c4 \u2022 OPT(I).\nProof. For any p \u2208 X i,j , it holds that 2 j R \u2264 max {2d(p, A), R} \u2264 2d(p, A) + R. We also obtain the second item by observing that diam(X i,j ) \u2264 2 \u2022 2 j \u2022 R, and using an analogous argument.\nNext, we show that the following lemma, which informally states that the union of the sets of sampled points approximately preserve the cost of clustering w.r.t. any set of at most k centers, even after excluding at most m outliers overall.\nLemma 5. The following statement holds with probability at least 1 \u2212 \u03bb/2: For all sets C \u2286 F of size at most k, and for all sets of non-negative integers {m i,j } i,j such that i,j m i,j \u2264 m, i,j cost m i,j (X i,j , C) \u2212 i,j wcost m i,j (S i,j , C) \u2264 \u2022 i,j cost m i,j (X i,j , C)\nwhere t i,j = m i,j /w i,j .\nProof. Fix an arbitrary set C of at most k centers and the integers {m i,j } i,j such that i,j m i,j \u2264 m as in the statement of the lemma. For each i = 1, . . . , |A|, and 0 \u2264 j \u2264 \u03c6, we invoke Lemma 4 by setting V \u2190 X i,j , and U \u2190 S i,j , \u03be \u2190 2 9z \u03c4 , \u03bb \u2190 n \u2212k \u03bb/(4(k + m)(1 + \u03c6)), and q \u2190 m. This implies that, the following inequality holds with probability at least 1 \u2212 \u03bb for each set X i,j , and for the corresponding m i,j \u2264 m: cost m i,j (X i,j , C) \u2212 wcost t i,j (S i,j , C) \u2264 2 9z \u03c4 2 2z+2 |X i,j |(diam(X i,j ) + d(X i,j , C))\nNote that for any i, j, if X i,j < s, i.e., X i,j is small, then the sample S i,j is equal to X i,j , and each point in S i,j has weight equal to 1. This implies that cost t i,j (X i,j , C) = wcost t i,j (S i,j , C) for all such X i,j , the contribution to the right hand side of inequality ( 18) is zero. Thus, it suffices to restrict the sum on the right hand side of (18) over large sets X i,j 's. We have the following claim about the large sets X i,j , the proof of which is analogous to that of Claim 2, and is therefore omitted.\nClaim 4. i,j:X i,j is large d(X i,j , C) z \u2264 2cost m (X, C). Thus, by revisiting (18), we get: |cost m (X, C) \u2212 \u2206(C)| \u2264 i,j:X i,j is large cost t i,j (X i,j , C) \u2212 wcost t i,j (S i,j , C)\n\u2264 2 9z \u03c4 i,j:X i,j is large 2 2z+2 \u2022 |X i,j |(diam(X i,j ) z + d(X i,j , C) z ) (By setting q i,j \u2190 t i,j and q i,j \u2190 t i,j in ( 18)) Where, the last inequality follows from the fact that since C is an arbitrary set of at most k centers, OPT(I) \u2264 cost m (X, C). Note that the preceding inequality holds for a fixed set C of centers with probability at least 1 \u2212 |A| \u2022 (1 + \u03c6)\u03bb = 1 \u2212 n \u2212k \u03bb/2, which follows from taking the union bound over all sets X i,j , 1 \u2264 i \u2264 |A| \u2264 k + m, and 0 \u2264 j \u2264 \u03c6.\nSince there are at most n k subsets C of F size at most k, the statement of the lemma follows from taking a union bound.\nOnce we obtain a coreset S satisfying Lemma 5, we can perform a similar enumeration of sets of size at most m, and obtain k+m O(m) \u2022 n O(1) instances of (k, z)-Clustering. We call a \u03b2-approximation on each of these instances, and each call takes time T (n, k). The subsequent analysis is identical to that for k-MedianOut which can be used to show an analogous version of Lemma 3. We omit the details, and conclude this section with the following theorem, which generalizes Theorem 1.\nTheorem 2. Let z \u2265 1 be a fixed constant. Suppose there exists a \u03b2-approximation algorithm for (k, z)-Clustering with running time T (n, k) for some constant \u03b2 \u2265 1, and there exists a \u03c4 -approximation algorithm for (k, z)-Clustering that runs in polynomial time, where \u03c4 = O(1). Then there exists a (\u03b2 + )-approximation algorithm for (k, z)-Clustering with Outliers, with running time k+m O(m) n O(1) \u2022T (n, k), where n is the instance size and m is the number of outliers.", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments.", "text": "We thank Rajni Dabas for bringing a few minor typos to our attention. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A (k, z)-clustering with Outliers", "text": "Let z \u2265 1 be a fixed real that is not part of the input of the problem. The input of the (k, z)-Clustering problem is an instance I = ((\u0393, d), X, F, k), where (\u0393, d) is a metric space, X \u2286 \u0393 is a (finite) set of n points, called points or clients, F \u2286 \u0393 is a set of facilities, and k is a positive integer. The task is to find a set C \u2286 F of facilities (called centers) in F that minimizes the following cost function:\n(k, z)-clustering with m outliers. Here, the input contains an additional integer 1 \u2264 m \u2264 n, and the goal is to find a set X \u2286 X of n \u2212 m points, such that cost(X , C) is minimized (over choices all of X and C). Here, the set X \\ X of at most m points corresponds to the set of outliers. In another notation, we want to find a set C \u2286 F of at most k centers that minimizes cost m (X, C) := sum \u223cm {cost(p, C) : p \u2208 X}, i.e., the sum of n \u2212 m smallest distances of points in X to the set of centers C.\nFirst, we state a few properties about the z-th powers of distances, which will be subsequently useful in the analysis. Proposition 3. Let P, C \u2286 \u0393 be non-empty finite subsets of points. For any point p \u2208 P , the following holds:\nProof. Let p * \u2208 P be a point realizing the smallest distance d(P, C). It follows that for any p \u2208 P ,\n\u2264 2 max{diam(P ), d(P, C)} Now, by taking the z-th power of each term, we get the first inequality, which follows from max{a, b} \u2264 a + b.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Re, k-clustering with fair outliers", "journal": "ACM", "year": "2022", "authors": "M Almanza; A Epasto; A Panconesi; G "}, {"ref_id": "b1", "title": "A technique for obtaining true approximations for k-center with covering constraints", "journal": "Springer", "year": "2020", "authors": "G Anegg; H Angelidakis; A Kurpisz; R Zenklusen"}, {"ref_id": "b2", "title": "Techniques for generalized colorful k-center problems", "journal": "", "year": "2022", "authors": "G Anegg; L V Koch; R Zenklusen"}, {"ref_id": "b3", "title": "A constant approximation for colorful k-center", "journal": "", "year": "2019", "authors": "S Bandyapadhyay; T Inamdar; S Pai; K R Varadarajan"}, {"ref_id": "b4", "title": "Theoretical analysis of the k-means algorithm-a survey", "journal": "Springer", "year": "2016", "authors": "J Bl\u00f6mer; C Lammersen; M Schmidt; C Sohler"}, {"ref_id": "b5", "title": "Algorithms for facility location problems with outliers", "journal": "ACM/SIAM", "year": "2001", "authors": "M Charikar; S Khuller; D M Mount; G Narasimhan"}, {"ref_id": "b6", "title": "Algorithms for covering multiple submodular constraints and applications", "journal": "Journal of Combinatorial Optimization", "year": "2022", "authors": "C Chekuri; T Inamdar; K Quanrud; K Varadarajan; Z Zhang"}, {"ref_id": "b7", "title": "A constant factor approximation algorithm for k-median clustering with outliers", "journal": "", "year": "2008-01-20", "authors": "K Chen"}, {"ref_id": "b8", "title": "On coresets for k-median and k-means clustering in metric and euclidean spaces and their applications", "journal": "SIAM Journal on Computing", "year": "2009", "authors": "K Chen"}, {"ref_id": "b9", "title": "Tight FPT approximations for k-median and k-means", "journal": "", "year": "2019", "authors": "V Cohen-Addad; A Gupta; A Kumar; E Lee; J Li"}, {"ref_id": "b10", "title": "A new coreset framework for clustering", "journal": "ACM", "year": "2021", "authors": "V Cohen-Addad; D Saulpic; C Schwiegelshohn"}, {"ref_id": "b11", "title": "Improved algorithms for clustering with outliers", "journal": "", "year": "2019", "authors": "Q Feng; Z Zhang; Z Huang; J Xu; J Wang"}, {"ref_id": "b12", "title": "Approximation schemes for clustering with outliers", "journal": "ACM Transactions on Algorithms (TALG)", "year": "2019", "authors": "Z Friggstad; K Khodamoradi; M Rezapour; M R Salavatipour"}, {"ref_id": "b13", "title": "Local search yields a PTAS for k-means in doubling metrics", "journal": "SIAM J. Comput", "year": "2019", "authors": "Z Friggstad; M Rezapour; M R Salavatipour"}, {"ref_id": "b14", "title": "FPT approximation for constrained metric kmedian/means", "journal": "", "year": "2020", "authors": "D Goyal; R Jaiswal; A Kumar"}, {"ref_id": "b15", "title": "Greedy strikes back: Improved facility location algorithms", "journal": "J. Algorithms", "year": "1999", "authors": "S Guha; S Khuller"}, {"ref_id": "b16", "title": "Structural iterative rounding for generalized kmedian problems", "journal": "", "year": "2021", "authors": "A Gupta; B Moseley; R Zhou"}, {"ref_id": "b17", "title": "Local search methods for k-means with outliers", "journal": "", "year": "2017", "authors": "S Gupta; R Kumar; K Lu; B Moseley; S Vassilvitskii"}, {"ref_id": "b18", "title": "Decision theoretic generalizations of the PAC model for neural net and other learning applications", "journal": "Inf. Comput", "year": "1992", "authors": "D Haussler"}, {"ref_id": "b19", "title": "Fair colorful k-center clustering", "journal": "Springer", "year": "2020", "authors": "X Jia; K Sheth; O Svensson"}, {"ref_id": "b20", "title": "Constant approximation for k-median and kmeans with outliers via iterative rounding", "journal": "ACM", "year": "2018", "authors": "R Krishnaswamy; S Li; S Sandeep"}, {"ref_id": "b21", "title": "A survey of clustering algorithms, in Data mining and knowledge discovery handbook", "journal": "Springer", "year": "2009", "authors": "L Rokach"}, {"ref_id": "b22", "title": "Outliers-resistant cluster-ing+++", "journal": "", "year": "2020", "authors": "A Statman; L Rozenberg; D Feldman"}, {"ref_id": "b23", "title": "A comprehensive survey of clustering algorithms", "journal": "Annals of Data Science", "year": "2015", "authors": "D Xu; Y Tian"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "For any finite set S \u2286 \u0393 and a point p \u2208 \u0393, we let d(p, S) := min s\u2208S d(p, S), and let diam(S) := max x,y\u2208S d(x, y). For two non-empty sets S, C \u2286 \u0393, let d(S, C) = min p\u2208S d(p, S) = min p\u2208S min c\u2208C d(p, c). For a point p \u2208 \u0393, r \u2265 0, and a set C \u2286 \u0393, let B C (p, r) = {q \u2208 C : d(p, c) \u2264 r}. Let T be a finite (multi)set of n real numbers, for some positive integer n, and let 1 \u2264 m \u2264 n. Then, we use the notation sum \u223cm (T ) to denote the sum of n \u2212 m smallest values in T (including repetitions in case of a multi-set).", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Theorem 1 .1Suppose there exists a \u03b2-approximation algorithm for k-Median with running time T (n, k), and a \u03c4 -approximation algorithm for k + m-Median with polynomial running time, where \u03b2 and \u03c4 are constants. Then there exists a (\u03b2 + )-approximation algorithm for k-MedianOut with running time k+m", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "where t = t|U |/|V | and w(u) = |V |/|U | for all u \u2208 U .Proof. Throughout the proof, we fix the set C \u2286 F and 0 \u2264 t \u2264 q as in the statement of the lemma. Next, we define the following notation. For each v \u2208 V , let h(v) := d(v, C), and let h(V ) := v\u2208V h(v), and h(U ) := u\u2208U h(u). Analogously, let h (V ) := cost t (V, C), and h (U ) := cost t (U, C). Let \u03b7(V ) := min v\u2208V d(v, C), and \u03b7(U ) := min u\u2208U d(u, C). We summarize a few properties about these definitions in the following observation.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "(k+m) log n O(m) by Proposition 1. Note that (log n) O(m) \u2264 max{m O(m) , n O(1) }. Thus, the number of subsets T \u2286 S of size at most m is bounded byf (k, m, ) \u2022 n O(1) , where f (k, m, ) = k+m O(m) . Thus, we need to call the \u03b2-approximationk-Median algorithm f (k, m, ) \u2022 n O(1) times, which takes f (k, m, )n O(1) \u2022 T (n, k) time overall.The first call of the algorithm for obtaining a \u03c4 -approximation to the (k + m)-Median instance takes polynomial time. Besides this, the other parts of our algorithm can all be done in polynomial time. This completes the proof of Theorem 1.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "and let h(V ) := v\u2208V h(v), and h(U ) := u\u2208U h(u). Analogously, let h (V ) := cost t (V, C), and h (U ) := cost t (U, C), i.e., sum of all except t (resp. t ) largest h-values. Let \u03b7(V ) := min v\u2208V d(v, C) z , and \u03b7(U ) := min u\u2208U d(u, C", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "The preceding inequality is equivalent to the inequality in the lemma, by recalling that h (V ) = cost t (V, C), and h (U )\u2022 |V | |U | = |V | |U | \u2022cost t (U, C) = wcost t (U, C), since the weight of every sampled point in U is equal to |V |/|U |. This concludes the proof of the lemma.Next, we show the following claim.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "|X i,j | \u2022 (2 j R) z \u2264 i,j p\u2208X i,j (2 j R) z \u2264 i,j p\u2208X i,j (2d(p, A) + R) z = 2 z p\u2208X d(p, A) z + |X| \u2022 R z = 2 z \u2022 cost 0 (X, A) + n \u2022 R z \u2264 (1 + 2 z ) \u2022 cost 0 (X, A) (By definition of R) \u2264 (1 + 2 z ) \u2022 \u03c4 \u2022 OPT(I ) \u2264 (1 + 2 z ) \u2022 \u03c4 \u2022 OPT(I).(From Obs. 4)", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "\u2264 2 9z \u03c4 \u2022 ( 222z+2 \u2022 2 2z+1 \u03c4 OPT(I) + 2 2z+3 cost m (X, C) (From Claim 3 and Claim 4) \u2264 2 9z \u03c4 (2 9z \u2022 \u03c4 \u2022 cost m (X, C)) = \u2022 cost m (X, C))", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "cost(X, C) := p\u2208X d(p, C).", "formula_coordinates": [4.0, 237.03, 365.75, 121.85, 22.64]}, {"formula_id": "formula_1", "formula_text": "O(m) \u2022 T (n, k) \u2022 n O(1)", "formula_coordinates": [4.0, 185.28, 734.87, 92.97, 14.24]}, {"formula_id": "formula_2", "formula_text": "time k+m O(m) \u2022 k O(k) \u2022 n O(", "formula_coordinates": [5.0, 65.2, 118.66, 143.56, 14.24]}, {"formula_id": "formula_3", "formula_text": "Since C * \u2286 C , for each point p \u2208 Y * , d(p, C ) \u2264 d(p, C * ). On the other hand, for each q \u2208 X \\ Y * , d(q, C ) = 0, since there is a co-located center in C * . This implies that cost 0 (X, C ) \u2264 cost m (X, C).", "formula_coordinates": [5.0, 65.2, 283.77, 465.52, 26.13]}, {"formula_id": "formula_4", "formula_text": "k \u2264 k + m centers A such that cost 0 (X, A) \u2264 \u03c4 \u2022 OPT(I ) \u2264 \u03c4 \u2022 OPT(I).", "formula_coordinates": [5.0, 175.16, 362.31, 337.37, 10.75]}, {"formula_id": "formula_5", "formula_text": "X i,j := B X i (c i , R) if j = 0, B X i (c i , 2 j R) \\ B X i (c i , 2 j\u22121 R) if j \u2265 1. Let s = c\u03c4 2 2 (m + k ln n + ln(1/\u03bb))", "formula_coordinates": [5.0, 82.13, 457.26, 332.82, 54.55]}, {"formula_id": "formula_6", "formula_text": "Proposition 1. We have |S| = O(((k + m) log n/ ) 2 ) if \u03bb is a constant.", "formula_coordinates": [5.0, 65.2, 616.83, 346.18, 11.52]}, {"formula_id": "formula_7", "formula_text": "|A| \u2022 (1 + log(\u03c4 n)) = O((k + m) log n), since |A| \u2264 k + m and \u03c4 is a constant. For each non-empty X i,j , |S i,j | \u2264 2s = O((m + k log n)/ 2", "formula_coordinates": [5.0, 65.2, 669.41, 465.52, 24.18]}, {"formula_id": "formula_8", "formula_text": "If s \u2265 M 2 2\u03b4 2 ln(2/\u03bb), then Pr h(V ) |V | \u2212 h(U ) |U | \u2265 \u03b4 \u2264 \u03bb,", "formula_coordinates": [6.0, 172.98, 93.2, 195.54, 50.36]}, {"formula_id": "formula_9", "formula_text": "|cost t (V, C) \u2212 wcost t (U, C)| \u2264 \u03be|V |(diam(V ) + d(V, C)),", "formula_coordinates": [6.0, 165.52, 226.08, 264.86, 10.95]}, {"formula_id": "formula_10", "formula_text": "\u2022 t |U | |V | \u2212 1 \u2264 t \u2264 t |U | |V | \u2022 h (V ) \u2264 h(V ) \u2212 t \u2022 \u03b7(V ) \u2264 h(V ), and h (V ) \u2265 h(V ) \u2212 t \u2022 (\u03b7(V ) + diam(V )) \u2022 h (U ) \u2264 h(U ), and h (U ) \u2265 h(U ) \u2212 t |U | |V | \u2022 (\u03b7(U ) + diam(U )) \u2022 \u03b7(V ) \u2264 \u03b7(U ) \u2264 \u03b7(V ) + diam(V )", "formula_coordinates": [6.0, 81.59, 362.22, 364.04, 84.82]}, {"formula_id": "formula_11", "formula_text": "d(v, C) \u2264 d(v, v * ) + d(v * , C) \u2264 diam(V ) + \u03b7(V ), where v * \u2208 V is a point realizing the minimum distance \u03b7(V ) to the set of centers C. This implies that g(v) \u2264 diam(V ) for all v \u2208 V . Now, observe that h(V ) = h (V ) + v\u2208V (\u03b7(V ) + g(v)) (Since h (V ) excludes the distances of points in V ) = h (V ) + t \u2022 \u03b7(V ) + v\u2208V g(v) (1) \u2265 h (V ) + t \u2022 \u03b7(V ) (g(v) \u2265 0 for all v \u2208 V )", "formula_coordinates": [6.0, 65.2, 484.49, 465.52, 119.59]}, {"formula_id": "formula_12", "formula_text": ") \u2264 h (V ) + t \u2022 \u03b7(V ) + t \u2022 diam(V ), since g(v) \u2264 diam(V ) for all v \u2208 V .", "formula_coordinates": [6.0, 65.2, 630.2, 465.52, 23.24]}, {"formula_id": "formula_13", "formula_text": "v\u2208V d(v, C) |V | \u2212 u\u2208U d(u, C) |U | \u2265 \u03be 2 diam(V ).", "formula_coordinates": [6.0, 204.85, 740.1, 202.56, 25.15]}, {"formula_id": "formula_14", "formula_text": "|V | \u2212 h(U ) |U | \u2264 \u03be 2 diam(V ).(2)", "formula_coordinates": [7.0, 239.45, 93.81, 291.26, 24.43]}, {"formula_id": "formula_15", "formula_text": "h (V ) |V | \u2212 h (U ) |U | \u2264 \u03be \u2022 (diam(V ) + d(V, C))(3)", "formula_coordinates": [7.0, 203.0, 168.99, 327.72, 24.43]}, {"formula_id": "formula_16", "formula_text": "|U | \u2212 h (V ) |V | \u2264 h(U ) |U | \u2212 h(V ) |V | + t \u2022 (\u03b7(V ) + diam(V )) |V | (From Observation 2, Part 2) \u2264 \u03be 2 diam(V ) + t |V | \u2022 (\u03b7(V ) + diam(V )) (From (2)) \u2264 \u03be 2 diam(V ) + \u03be 2 \u2022 (\u03b7(V ) + diam(V ))(4)", "formula_coordinates": [7.0, 103.6, 229.48, 427.12, 82.85]}, {"formula_id": "formula_17", "formula_text": "|V | \u2265 s \u2265 4q \u03be \u2265 4t \u03be . Now, consider h (V ) |V | \u2212 h (U ) |U | \u2264 h(V ) |V | \u2212 h(U ) |U | \u2212 t\u03b7(V ) |V | + t |U | |V | \u2022 (\u03b7(U ) + diam(U )) |U | (From Observation 2, Part 3) \u2264 \u03be 2 diam(V ) \u2212 t \u2022 \u03b7(V ) |V | + t \u2022 \u03b7(U ) |V | + t \u2022 diam(U ) |V | (From (2)) \u2264 \u03be 2 diam(V ) \u2212 t \u2022 \u03b7(V ) |V | + t \u2022 (\u03b7(V ) + diam(V )) + t \u2022 diam(U ) |V | (From Observation 2, Part 4) \u2264 \u03be 2 diam(V ) + 2t \u2022 diam(V ) |V | (diam(U ) \u2264 diam(V )) \u2264 \u03bediam(V )(5)", "formula_coordinates": [7.0, 119.45, 321.69, 411.27, 193.13]}, {"formula_id": "formula_18", "formula_text": "|V | \u2265 s \u2265 4q \u03be \u2265 4t \u03be .", "formula_coordinates": [7.0, 348.53, 528.24, 89.8, 15.68]}, {"formula_id": "formula_19", "formula_text": "h (V ) \u2212 h (U ) \u2022 |V | |U | \u2264 \u03be|V | \u2022 (diam(V ) + d(V, C)).", "formula_coordinates": [7.0, 181.78, 590.69, 235.98, 24.43]}, {"formula_id": "formula_20", "formula_text": "\u2022", "formula_coordinates": [7.0, 94.55, 643.68, 3.03, 9.57]}, {"formula_id": "formula_21", "formula_text": "|V | |U | = |V | |U | \u2022 cost t (U, C) = wcost t (U, C).", "formula_coordinates": [7.0, 101.93, 640.31, 190.52, 16.27]}, {"formula_id": "formula_22", "formula_text": "\u2022 i,j |X i,j |2 j R \u2264 3 \u2022 cost 0 (X, A) \u2264 3\u03c4 \u2022 OPT(I). \u2022 i,j |X i,j |diam(X i,j ) \u2264 6 \u2022 cost 0 (X, A) \u2264 6\u03c4 \u2022 OPT(I).", "formula_coordinates": [7.0, 81.59, 751.13, 233.47, 14.17]}, {"formula_id": "formula_23", "formula_text": "|X i,j | \u2022 2 j R \u2264 i,j p\u2208X i,j 2 j R \u2264 i,j p\u2208X i,j 2d(p, A) + R = 2 p\u2208X d(p, A) + |X| \u2022 |R| = 2 \u2022 cost 0 (X, A) + n|R| \u2264 3 \u2022 cost 0 (X, A) (By definition of R) \u2264 3\u03c4 OPT(I ) \u2264 3\u03c4 OPT(I).", "formula_coordinates": [8.0, 192.99, 116.73, 337.73, 138.86]}, {"formula_id": "formula_24", "formula_text": "{m i,j } i,j such that i,j m i,j \u2264 m, i,j cost m i,j (X i,j , C) \u2212 i,j wcost t i,j (S i,j , C) \u2264 \u2022 i,j cost m i,j (X i,j , C)(6)", "formula_coordinates": [8.0, 141.82, 356.17, 388.9, 61.16]}, {"formula_id": "formula_25", "formula_text": "cost m i,j (X i,j , C) \u2212 wcost t i,j (S i,j , C) \u2264 8\u03c4 |X i,j |(diam(X i,j ) + d(X i,j , C))(7)", "formula_coordinates": [8.0, 213.78, 535.05, 316.94, 37.3]}, {"formula_id": "formula_26", "formula_text": "s = 4 \u03be 2 m + ln 2 \u03bb = 4 8\u03c4 2 \u2022 m + ln 8n k (k + m)(1 + \u03c6) \u03bb \u2264 s.", "formula_coordinates": [8.0, 165.2, 604.79, 265.52, 57.87]}, {"formula_id": "formula_27", "formula_text": "(i,j)\u2208L p\u2208X i,j \\Y d(p, C) \u2264 cost m (X, C)(8)", "formula_coordinates": [9.0, 212.5, 120.6, 318.22, 23.74]}, {"formula_id": "formula_28", "formula_text": "(i,j)\u2208L |X i,j | \u2022 d(X i,j , C) \u2264 (i,j)\u2208L 2|X i,j \\ Y | \u2022 d(X i,j , C) \u2264 2 \u2022 (i,j)\u2208L p\u2208X i,j \\Y d(p, C) \u2264 2 \u2022 cost m (X, C)", "formula_coordinates": [9.0, 226.35, 181.23, 145.03, 106.12]}, {"formula_id": "formula_29", "formula_text": "(i,j)\u2208L cost m i,j (X i,j , C) \u2212 wcost t i,j (S i,j , C) \u2264 8\u03c4 (i,j)\u2208L |X i,j |(diam(X i,j ) + d(X i,j , C)) (From (7)) \u2264 8\u03c4 \u2022 (6\u03c4 \u2022 OPT(I) + 2cost m (X, C)) (From Obs. 3 and Claim 2) = 8\u03c4 (8\u03c4 \u2022 cost m (X, C)) = \u2022 cost m (X, C)", "formula_coordinates": [9.0, 144.82, 359.66, 385.89, 106.6]}, {"formula_id": "formula_30", "formula_text": "cost m (X, C * ) \u2264 1 + 1 \u2212 \u2022 \u03b2cost m (X, C).", "formula_coordinates": [9.0, 209.08, 742.79, 177.76, 24.43]}, {"formula_id": "formula_31", "formula_text": "wcost t i,j (S i,j , C) \u2264 (1 + ) \u2022 i,j cost m i,j (X i,j , C) = (1 + ) \u2022 cost m (X, C).(9)", "formula_coordinates": [10.0, 194.89, 148.31, 335.82, 39.95]}, {"formula_id": "formula_32", "formula_text": "wcost(S\\T, C ) \u2264 \u03b2 \u2022 wcost(S\\T, C) = \u03b2 i,j", "formula_coordinates": [10.0, 154.48, 255.25, 202.14, 22.2]}, {"formula_id": "formula_34", "formula_text": "(1 \u2212 ) \u2022 cost m (X i,j , C ) \u2264 (1 \u2212 ) \u2022 i,j cost m i,j (X i,j , C ) \u2264 wcost(S\\T, C ). (11", "formula_coordinates": [10.0, 124.17, 333.06, 401.7, 22.2]}, {"formula_id": "formula_35", "formula_text": ")", "formula_coordinates": [10.0, 525.87, 333.18, 4.85, 9.57]}, {"formula_id": "formula_36", "formula_text": "|X i,j | |S i,j | .", "formula_coordinates": [10.0, 491.42, 617.35, 24.73, 17.48]}, {"formula_id": "formula_37", "formula_text": "(1) i,j \u2190 X (1)", "formula_coordinates": [11.0, 273.79, 83.26, 49.63, 16.0]}, {"formula_id": "formula_38", "formula_text": "i,j cost m i,j (X i,j , C) \u2212 i,j wcost t i,j (S i,j , C) \u2264 \u2022 i,j cost m i,j (X i,j , C)", "formula_coordinates": [11.0, 141.82, 220.54, 321.04, 22.2]}, {"formula_id": "formula_39", "formula_text": "(i.e., d(p, q) 2 \u2264 2(d(p, r) 2 + d(r, q) 2 )", "formula_coordinates": [11.0, 65.2, 416.17, 166.68, 11.52]}, {"formula_id": "formula_40", "formula_text": "B X i (c i , R) if j = 0 B X i (c i , 2 j R) \\ B X i (c i , 2 j\u22121 R) if j \u2265 1 Let s = c\u03c4 2 2 (c z) 2 (m + k ln n + ln(1/\u03bb))", "formula_coordinates": [16.0, 82.13, 372.71, 331.3, 56.52]}, {"formula_id": "formula_41", "formula_text": "|X i,j \\Y ij | s", "formula_coordinates": [16.0, 394.28, 485.21, 36.21, 16.51]}, {"formula_id": "formula_42", "formula_text": "|cost t (V, C) \u2212 wcost t (U, C)| \u2264 2 2z+2 \u03be|V | \u2022 (diam(V ) z + d(V, C) z ),(12)", "formula_coordinates": [16.0, 145.08, 616.01, 385.64, 13.33]}, {"formula_id": "formula_43", "formula_text": "v \u2208 V , let h(v) = cost(v, C) = d(v, C) z ,", "formula_coordinates": [16.0, 296.97, 677.13, 195.23, 11.52]}, {"formula_id": "formula_44", "formula_text": "|U | |V | \u2212 1 \u2264 t \u2264 t |U | |V | \u2022 For any p \u2208 P , (\u03b7(V ) z \u2264 d(p, C) z = cost(p, C) z \u2264 \u03b7(V ) z + 2 z (\u03b7(V ) z + diam(V )) \u2022 h (V ) \u2264 h(V ) \u2212 t \u2022 \u03b7(V ) z \u2264 h(V ), and h (V ) \u2265 h(V ) \u2212 2 z \u2022 t \u2022 (\u03b7(V ) z + diam(V ) z ) \u2022 h (U ) \u2264 h(U ), and h (U ) \u2265 h(U ) \u2212 2 z \u2022 t |U | |V | \u2022 (\u03b7(U ) z + diam(U ) z ) \u2022 \u03b7(V ) z \u2264 \u03b7(U ) z \u2264 2 z (\u03b7(V ) z + diam(V ) z )", "formula_coordinates": [17.0, 81.59, 66.92, 396.47, 109.38]}, {"formula_id": "formula_45", "formula_text": ") z \u2264 cost(p, C) z \u2264 2 z \u2022 (\u03b7(V ) z + diam(P ) z ). This implies that g(v) \u2264 diam(V ) for all v \u2208 V . Now, observe that h(V ) = h (V ) + v\u2208V (\u03b7(V ) + g(v)) z (Since h (V ) excludes the distances of points in V ) \u2265 h (V ) + t \u2022 \u03b7(V ) z (g(v) \u2265 0 for all v \u2208 V )", "formula_coordinates": [17.0, 65.2, 227.34, 465.52, 79.68]}, {"formula_id": "formula_46", "formula_text": "Let \u03b7 = \u03b7(V ) z , M = 2 2z+2 (\u03b7(V ) z + diam(V ) z", "formula_coordinates": [17.0, 82.13, 410.27, 226.64, 11.52]}, {"formula_id": "formula_47", "formula_text": "Pr v\u2208V cost(v, C) |V | \u2212 u\u2208U cost(u, C) |U | \u2265 \u03be 2 2 z (\u03b7(V ) z ) + diam(V ) z ) = Pr h(V ) |V | \u2212 h(U ) |U | \u2265 \u03b4 \u2264 \u03bb .", "formula_coordinates": [17.0, 132.07, 448.51, 327.83, 71.67]}, {"formula_id": "formula_48", "formula_text": "h(V ) |V | \u2212 h(U ) |U | \u2264 \u03be 2 \u2022 M (13)", "formula_coordinates": [17.0, 246.5, 557.64, 284.22, 24.43]}, {"formula_id": "formula_49", "formula_text": "h (U ) |U | \u2212 h (V ) |V | \u2264 h(U ) |U | \u2212 h(V ) |V | + 2 z \u2022 t \u2022 (\u03b7(V ) z + diam(V ) z ) |V | (From Obs. 5) \u2264 \u03be 2 M + t \u2022 M |V | (From (13)) \u2264 \u03beM(14)", "formula_coordinates": [17.0, 119.58, 629.58, 411.14, 71.73]}, {"formula_id": "formula_50", "formula_text": "|V | \u2265 s \u2265 4q \u03be \u2265 4t \u03be . Now, consider h (V ) |V | \u2212 h (U ) |U | \u2264 h(V ) |V | \u2212 h(U ) |U | + 2 z \u2022 t |U | |V | \u2022 (\u03b7(U ) z + diam(V ) z ) |U | (From Obs. 5, Part 4) \u2264 \u03be 2 M + 2 z \u2022 t \u2022 \u03b7(U ) z |V | + 2 z \u2022 t \u2022 diam(V ) z |V | (From (2) \u2264 \u03be 2 M + 2 2z \u2022 t \u2022 (\u03b7(V ) z + diam(V ) z ) + t \u2022 2 z \u2022 diam(V ) z |V | (From Obs. 2, Part 5) \u2264 \u03be 2 M + 2 2z+1 \u2022 t \u2022 (\u03b7(V ) z + diam(V ) z ) |V | (Since |V | \u2265 s \u2265 4q \u03be \u2265 4t \u03be ) = \u03be 2 M + \u03be 2 M = \u03beM(15)", "formula_coordinates": [18.0, 113.37, 65.78, 417.34, 207.3]}, {"formula_id": "formula_51", "formula_text": "h (V ) \u2212 h (U ) \u2022 |V | |U | \u2264 2 2z+2 \u03be \u2022 |V | \u2022 (diam(V ) z + d(V, C) z )(16)", "formula_coordinates": [18.0, 161.95, 329.94, 368.77, 24.43]}, {"formula_id": "formula_52", "formula_text": "V ) = d(V, C).", "formula_coordinates": [18.0, 178.33, 366.03, 65.75, 9.69]}], "doi": ""}