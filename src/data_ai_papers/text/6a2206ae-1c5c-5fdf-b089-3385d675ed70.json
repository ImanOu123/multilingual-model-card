{"title": "Task Programming: Learning Data Efficient Behavior Representations", "authors": "Jennifer J Sun; Ann Kennedy; Eric Zhan; David J Anderson; Yisong Yue; Pietro Perona", "pub_date": "2021-03-29", "abstract": "Specialized domain knowledge is often necessary to accurately annotate training sets for in-depth analysis, but can be burdensome and time-consuming to acquire from domain experts. This issue arises prominently in automated behavior analysis, in which agent movements or actions of interest are detected from video tracking data. To reduce annotation effort, we present TREBA: a method to learn annotation-sample efficient trajectory embedding for behavior analysis, based on multi-task self-supervised learning. The tasks in our method can be efficiently engineered by domain experts through a process we call \"task programming\", which uses programs to explicitly encode structured knowledge from domain experts. Total domain expert effort can be reduced by exchanging data annotation time for the construction of a small number of programmed tasks. We evaluate this trade-off using data from behavioral neuroscience, in which specialized domain knowledge is used to identify behaviors. We present experimental results in three datasets across two domains: mice and fruit flies. Using embeddings from TREBA, we reduce annotation burden by up to a factor of 10 without compromising accuracy compared to state-of-the-art features. Our results thus suggest that task programming and self-supervision can be an effective way to reduce annotation effort for domain experts.", "sections": [{"heading": "Introduction", "text": "Behavioral analysis of one or more agents is a core element in diverse fields of research, including biology [36,26], autonomous driving [6,39], sports analytics [42,43], and video games [20,3]. In a typical experimental workflow, the location and pose of agents is first extracted from each frame of a behavior video, and then labels for experimenter-defined behaviors of interest are applied on a frame-by-frame basis based on the pose and movements of the agents. In addition to reducing human effort, automated quantification of behavior can lead to more objective, pre-Correspondence to jjsun@caltech.edu. cise, and scalable measurements compared to manual annotation [1,10]. However, training behavior detection models can be data intensive and manual behavior annotation often requires specialized domain knowledge and high-frequency temporal labels. As a result, this process of generating training datasets is time-consuming and effort-intensive for experts. Therefore, methods to reduce annotation effort by domain experts are needed to accelerate behavioral studies.\nWe study alternative ways for domain experts to improve classifier accuracy beyond simply increasing the sheer volume of annotations. In particular, we propose a framework that unifies: (1) self-supervised representation learning, and\n(2) encoding explicit structured knowledge on trajectory data using expert-defined programs. Domain experts can construct these programs efficiently because keypoint trajectories in each frame are typically low dimensional, and experts can already hand-design effective features for trajectory data [36,28]. To best leverage this structured expert knowledge, we develop a framework to learn trajectory representations based on multi-task self-supervised learning, which has not been well-explored for trajectory data.\nOur Approach. Our framework, Trajectory Embedding for Behavior Analysis (TREBA), learns trajectory representations through trajectory generation alongside a set of decoder tasks based on expert-engineered programs. These programs are created by domain experts through a process we call task programming, inspired by the data programming paradigm [33]. Task programming is a process by which domain experts identify trajectory attributes relevant to the behaviors of interest under study, write programs, and apply those programs to inform representation learning (Section 3.2). This flexibility in decoder tasks allows our framework to be applicable to a variety of agents and behaviors studied across diverse fields of research.\nExpert Effort Tradeoffs. Since task programming will typically require a domain expert's time, we study the tradeoff between doing task programming and data annotation. We compare behavior classification performance with different amounts of annotated training data and programmed tasks. For example, for the domain illustrated in Figure 1, domain experts can reduce error by 13% relative to the base classifier by annotating 701k additional frames, or they can reduce error by 16% by learning a representation using 10 programmed tasks in our framework. Our approach allows experts to trade a large number of annotations for a small number of programmed tasks.\nWe study our approach across two domains in behavioral neuroscience, namely mouse and fly behavior. We chose this setting because it requires specialized domain knowledge for data annotation, and data efficiency is important for domain experts. Furthermore, decoder tasks in our framework can be efficiently programmed by experts based on simple functions describing trajectory attributes for identifying behaviors of interest. For example, for mouse social behaviors such as attack [36], important behavior attributes include the speed of each mouse and distance between mice. The corresponding task could then be to decode these attributes from the learned representations.\nOur contributions are:\n\u2022 We introduce task programming as an efficient way for domain experts to reduce annotation effort and encode structural knowledge. We develop a novel method to learn an annotation-sample efficient trajectory representation using self-supervision and programmatic supervision.\n\u2022 We study the effect of task programming, data annotation, and different decoder losses on behavior classifier performance.\n\u2022 We demonstrate these representations on three datasets in two domains, showing that our method can lead to a 10\u00d7 annotation reduction for mice, and 2\u00d7 for flies.", "publication_ref": ["b36", "b25", "b5", "b39", "b42", "b43", "b19", "b2", "b0", "b9", "b36", "b27", "b32", "b36"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Behavior Modeling. Behavior modeling using trajectory data is studied across a variety of fields [26,6,39,42,20,3]. In particular, there is an increasing effort to automatically detect and classify behavior from trajectory data [23,1,14,27,13,36]. Our experiments are based on behavior classification datasets from behavioral neuroscience [15,4,36], a field where specialized domain knowledge is important for identifying behaviors of interest.\nThe behavior analysis pipeline generally consists of the following steps: (1) tracking the pose of agents, (2) computing pose-based features, and ( 3) training behavior classifiers [4,21,36,28]. To address step 1, there are many existing pose estimation models [15,27,18,36]. In our work, we leverage two existing pose models, [36] for mice and [15] for flies, to produce trajectory data. In steps 2 and 3 of the typical behavior analysis pipeline, hand-designed trajectory features are computed from the animals' pose, and classifiers are trained to predict behaviors of interest in a fully supervised fashion [4,21,15,36]. Training fully supervised behavior classifiers requires time-consuming annotations by domain experts [1]. Instead, our proposed approach enables domain experts to trade time-consuming annotation work for task programming with representation learning.\nAnother group of work uses unsupervised methods to discover new motifs and behaviors [22,41,2,26,5]. Our work focuses on the more common case where domain experts already know what types of actions they would like to study in an experiment. We aim to improve the dataefficiency of learning expert-defined behaviors.\nRepresentation Learning. Visual representation learning has made great progress in effective representations for images and videos [17,16,7,29,25,19,38]. Selfsupervised signals are often used to train this visual representation, such as learning relative positions of image patches [11], predicting image rotations [16], predicting future patches [29], and constrastive learning on augmented images [7]. Compared to visual data, trajectory data is significantly lower dimensional in each frame, and techniques from visual representation learning often cannot be applied directly. For example, while we can create image patches that represent the same visual class, it is difficult to select a partial set of keypoints that represent the same behavior. Our framework builds upon these approaches to learn effective representations for behavioral data.\nWe investigate different decoder tasks in order to learn an effective behavior representation. One decoder task that we investigate is self-decoding: the reconstruction of input trajectories using generative modeling. Generative modeling has previously been applied to learn representations for visual data [45,38,29] and language modeling [31]; for trajectory data, we use imitation learning [40,44,43] to train our trajectory representation. The other tasks in our multitask self-supervised learning framework are created by domain experts using task programming (Section 3.2). This idea of using a human-provided function as part of training has been studied for training set creation [33,32], and controllable trajectory generation [43]. Our work explores these additional decoder tasks to further improve the learned representation over the generative loss alone.\nMulti-Task Self-Supervised Learning. We jointly optimize a family of self-supervised tasks in an encoderdecoder setup, making this work an example of multitask self-supervised learning. Multi-task self-supervised learning has been applied to other domains such as visual data [12,25], accelerometer recordings [35], audio [34] and multi-modal inputs [37,30]. Generally in each of these domains, tasks are defined ahead of time, as is the case for tasks such as frame reconstruction, colorization, finding relative position of image patches, and video-audio alignment. Most of these tasks are designed for image or video data, and cannot be applied directly to trajectory data. To construct tasks for trajectory representation learning, we propose that domain experts can use task programming to engineer decoder tasks and encode structural knowledge.", "publication_ref": ["b25", "b5", "b39", "b42", "b19", "b2", "b22", "b0", "b13", "b26", "b12", "b36", "b14", "b3", "b36", "b3", "b20", "b36", "b27", "b14", "b26", "b17", "b36", "b36", "b14", "b3", "b20", "b14", "b36", "b0", "b21", "b41", "b1", "b25", "b4", "b16", "b15", "b6", "b28", "b24", "b18", "b38", "b10", "b15", "b28", "b6", "b45", "b38", "b28", "b30", "b40", "b44", "b43", "b32", "b31", "b43", "b11", "b24", "b35", "b33", "b37", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Methods", "text": "We introduce Trajectory Embedding for Behavior Analysis (TREBA), a method to learn an annotation-sample efficient trajectory representation using self-supervision and auxiliary decoder tasks engineered by domain experts. Figure 2 provides an overview of the expert's role. In our framework, domain experts replace (a significant amount of) time-consuming manual annotation with the construction of a small number of programmed tasks, reducing total expert effort. Each task places an additional constraint on the learned trajectory embedding.\nTREBA uses the expert-programmed tasks based on a multi-task self-supervised learning approach, outlined in Figure 3. To learn task-relevant low-dimensional representations of pose trajectories, we train a network jointly on (1) reconstruction of the input trajectory (Section 3.1) and (2) expert-programmed decoder tasks (Section 3.3). The learned representation can then be used as input to behavior modeling tasks, such as behavior classification.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Trajectory Representations", "text": "Let D be a set of N unlabelled trajectories. Each trajectory \u03c4 is a sequence of states \u03c4 = {(s t )} T t=1 , where the state s i at timestep i corresponds to the location or pose of the agents at that timestep. In this study, we divide trajectories from longer recordings into segments of length T , but in general trajectory length can vary. For multiple agents, the keypoints of each agent is stacked at each timestep.\nBefore we introduce our expert-programmed tasks, we will use trajectory reconstruction as an initial selfsupervised task. Given a history of agent states, we would like our model to predict the next state. This task is usually studied with sequential generative models. We used trajectory variational autoencoders (TVAEs) [9,43] to embed the input trajectory using an RNN encoder, q \u03c6 , and an RNN decoder, p \u03b8 , to predict the next state. The TVAE loss is:  We use a prior distribution p \u03b8 (z) on z to regularize the learned embeddings; in this study, our prior is the unit Gaussian. By optimizing for the TVAE loss only, we learn an unsupervised version of TREBA. When performing subsequent behavior modeling tasks such as classification, we use the embedding mean, z \u00b5 .\nL tvae = E q \u03c6 T t=1 \u2212 log(p \u03b8 (s t+1 |s t , z)) +D KL (q \u03c6 (z|\u03c4 )||p \u03b8 (z)).(1)", "publication_ref": ["b8", "b43"], "figure_ref": [], "table_ref": []}, {"heading": "Task Programming", "text": "Task programming is the process by which domain experts create decoder tasks for trajectory self-supervised learning. This process consists of selecting attributes from trajectory data, writing programs, and creating decoder tasks based on the programs (Figure 2). Here, domain experts are people with specialized knowledge for studying behavior, such as neuroscientists or sports analysts.\nTo start, domain experts identify attributes from trajectory data relevant to the behaviors of interest under study. Behavior attributes capture information that is likely relevant to agent behavior, but is not explicitly included in the trajectory states {(s t )} T t=1 . These attributes represent structured knowledge that domain experts are implicitly or explicitly considering for behavior analysis, such as the distance between two agents, agent velocity, or the relative positioning of agent body parts.\nNext, domain experts write a program to compute these attributes on trajectory data, which can be done with existing tools such as MARS [36] or SimBA [28]. Algorithm 1 shows a sample program from the mouse social behavior domain, for measuring the \"facing angle\" between a pair of interacting mice. Each program can be used to construct decoder tasks for self-supervised learning (Section 3.3).\nOur framework is inspired by the data programming paradigm [33], which applies programs to training set creation. In comparison, our framework uses task programming to unify expert-engineered programs, which encode structured expert knowledge, with representation learning.", "publication_ref": ["b36", "b27", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 1: Sample Program for Facing Angle", "text": "Input: centroid of mouse 1 (x 1 , y 1 ), centroid of mouse 2 (x 2 , y 2 ), heading of mouse 1 (\u03c6 1 ) Working with domain experts in behavioral neuroscience, we created a set of programs to use in studying our approach. The selected programs are a subset of behavior attributes in [36] (for mouse datasets) and a subset of behavior attributes in [15] (for fly datasets). We list the programs used in Table 1, and provide more details about the programs in the Supplementary Material.\nx diff = x 2 \u2212 x 1 y diff = y 2 \u2212 y 1 \u03b8 = arctan(y diff , x diff ) Return \u03b8 \u2212 \u03c6 1 Domain Behavior", "publication_ref": ["b36", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Learning Algorithm", "text": "We develop a method to incorporate the programs from domain experts as additional learning signals for TREBA. We consider the following three approaches: (1) enforcing attribute consistency in generated trajectories (Section 3.3.1), (2) performing attribute decoding directly (Section 3.3.2), ( 3) applying contrastive loss based on program supervision (Section 3.3.3). Each of these methods applies a different loss on the low-dimensional representation z of trajectory \u03c4 . Any combinations of these decoding tasks can be combined with self-decoding from Section 3.1 to inform the trajectory embedding z.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Attribute Consistency", "text": "Let \u03bb be a set of M domain-expert-designed functions measuring agent behavior attributes, such as agent velocity or facing angle. Recall that each \u03bb j , j = 1...M takes as input a trajectory \u03c4 , and returns some expert-designed attribute \u03bb j (\u03c4 ) computed from that trajectory. For \u03bb j designed for a single frame, we apply the function to the center frame of \u03c4 . Attribute consistency aims to maintain the same behavior attribute labels for the generated trajectory as the original. Let\u03c4 be the trajectory generated by the TVAE given the same initial condition as \u03c4 and encoding z.The attribute consistency loss is:\nL attr = E \u03c4 \u223cD M j=1 1(\u03bb j (\u03c4 ) = \u03bb j (\u03c4 )) .(2)\nHere, we show the loss for categorical \u03bb j , but in general, \u03bb j can be continuous and any loss measuring differences between \u03bb j (\u03c4 ) and \u03bb j (\u03c4 ) applies, such as mean squared error. We do not require \u03bb to always be differentiable, and we use the differentiable approximation introduced in [43] to handle non-differentiable \u03bb.", "publication_ref": ["b43"], "figure_ref": [], "table_ref": []}, {"heading": "Attribute Decoding", "text": "Another option is to decode each attribute \u03bb j (\u03c4 ) directly from the learned representation z. Here we apply a shallow decoder f to the learned representation, with decoding loss:\nL decode = E \u03c4 \u223cD M j=1 1(f(q \u03c6 (z \u00b5 |\u03c4 )) = \u03bb j (\u03c4 )) . (3)\nSimilar to Eq. (2), we show the loss for categorical \u03bb j , however any type of \u03bb may be used.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Contrastive Loss", "text": "Lastly, the programmed tasks can be used to supervise contrastive learning of our representation. For a trajectory \u03c4 i , and for each \u03bb j , positive examples are those trajectories with the same attribute class under \u03bb j . For \u03bb j with continuous outputs, we create a discretized\u03bb j in which we apply fixed thresholds to divide the output space into classes. For our work, we apply two thresholds for each program such that our classes are approximately equal in size.\nWe apply a shallow decoder g to the learned representation, and let g = g(q \u03c6 (z \u00b5 |\u03c4 )) represent the decoded repre-sentation. We then apply the contrastive loss:\nL cntr. = B i=1 M j=1 \u22121 N pos(i,j) B k=1 1 i =k \u2022 1 \u03bbj (\u03c4i)=\u03bbj (\u03c4 k ) \u2022 log exp(g i \u2022 g k /t) N l=1 1 i =l \u2022 exp(g i \u2022 g l /t) ,(4)\nwhere B is the batch size, N pos(i,j) is the number of positive matches for \u03c4 i with \u03bb j , and t > 0 is a scalar temperature parameter. Our form of contrastive loss supervised by task programming is similar to the contrastive loss in [24] supervised by human annotations. A benefit of task programming is that the supervision from programs can be quickly and scalably applied to unlabelled datasets, as compared to expert supervision which can be time-consuming. We note that the unsupervised version of this contrastive loss is studied in [7], based on previous works such as [29].", "publication_ref": ["b23", "b6", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "Data Augmentation", "text": "We can perform data augmentation on trajectory data based on our expert-provided programs. Given the set of all possible augmentations, we define \u039b to be the subset of augmentations that are attribute-preserving: that is, for all \u03bb j in the set of programs, \u03bb j (\u03c4 ) = \u03bb j (\u039b m (\u03c4 )) for some augmentation \u039b m \u2208 \u039b. An example of a valid augmentation in the mouse domain is reflection of the trajectory data.\nAll losses presented above can be extended with data augmentation, by replacing \u03c4 with \u039b m (\u03c4 ) in losses. For contrastive loss, adding data augmentation corresponds to extending the batch size to 2B, with B samples from the original and augmented trajectories.\nThe augmentations we use in our experiments are reflections, rotations, translations, and a small Gaussian noise on the keypoints (mouse data only). In practice, we add the loss for each decoder with and without data augmentation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Datasets", "text": "We work with datasets from behavioral neuroscience, where there are large-scale, expert-annotated datasets from scientific experiments. We study behavior for the laboratory mouse and the fruit fly, two of the most common model organisms in behavioral neuroscience. For each organism, we first train TREBA using large unannotated datasets: for the mouse domain we use an in-house dataset comprised of approximately 100 hours of recorded diadic social interactions (Mouse100), while for the fly domain we use the Fly vs. Fly dataset [15] without annotations.\nAfter pre-training TREBA, we evaluate the suitability of our trajectory representation for supervised behavior clas-sification (classifying frame-level behaviors on continuous trajectory data), on three additional datasets:\nMARS. The MARS dataset [36] is a recently released mouse social behavior dataset collected in the same conditions as Mouse100. The dataset is annotated by neurobiologists on a frame-by-frame basis for three behaviors: sniff, attack, and mount. We use the provided train, validation, and test split (781k, 352k, and 184k frames respectively). Trajectories are extracted by the MARS tracker [36].\nCRIM13. CRIM13 [4] is a second mouse social behavior dataset manually annotated on a frame-by-frame basis by experts. To extract trajectories, we use a version of the the MARS tracker [36] fine-tuned on pose annotations on CRIM13. We select a subset of videos from which trajectories can be reliably detected for a train, validation and test split of 407k, 96k, and 142k frames respectively. We evaluated classifier performance on the same three behaviors studied in MARS (sniff, attack, mount).\nCRIM13 is a useful test of the robustness of TREBA trained on Mouse100, as the recording conditions in CRIM13 (image resolution 640 \u00d7 480, frame rate 25Hz, and non-centered cage location) are different from those of Mouse100 (image resolution 1024 \u00d7 570, frame rate 30Hz, and centered cage location).\nFly vs. Fly (Fly). We use the Aggression and Courtship videos from the Fly dataset [15]. These videos record interactions between a pair of flies annotated on a frame-byframe basis for social behaviors by domain experts. Our train, validation and test split has 1067k, 162k, 322k frames respectively. We use the trajectories tracked by [15] and evaluate on all behaviors with more than 1000 frames of annotations in the full training set (lunge, wing threat, tussle, wing extension, circle, copulation).", "publication_ref": ["b14", "b36", "b36", "b3", "b36", "b14", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Training and Evaluation Procedure", "text": "We use the attribute consistency loss (Section 3.3.1) and contrastive loss (Section 3.3.3) to train TREBA using programs. With the same programs, we find that different loss combinations result in similar performance, and that the combination of consistency and contrastive losses performs the best overall. The results for all loss combinations are provided in the Supplementary Material.\nFor the datasets in the mouse domain (MARS and CRIM13) we train TREBA on Mouse100, with 10 programs provided by mouse behavior domain experts. For the Fly dataset, we train TREBA on the training split of Fly without annotations, with 13 programs provided by fly behavior domain experts. The full list is in Table 1. We then use the trained encoder, with pre-trained frozen weights, as a trajectory feature extractor over T = 21 frames, where the representation for each frame is computed using ten frames before and after the current frame.\nWe ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Main Results", "text": "We evaluate the data efficiency of our representation for supervised behavior classification, by training a classifier to predict behavior labels given both our learned representation and one of either (1) raw keypoints or (2) domain-specific features designed by experts. The TREBA+keypoints evaluation allows us to test the effectiveness of our representation without other hand-designed features, while the TREBA+features evaluation is closer to most potential use cases. The domain-specific features for mice are the trajectory features from [36] and features for flies are the trajectory features from [4]. The input features are a superset of the programs we use in Table 1.\nOur representation is able to improve the data efficiency for both keypoints and domain-specific features, over all evaluated amounts of training data availability (Figure 4). We discuss each dataset below:\nMARS. Our representation significantly improves classification performance over keypoints alone (Figure 4 A1). We achieve the same performance as the full baseline training using only between 1% and 2% of the data. While this result is partially because our representation contains temporal information, we can also observe a significant increase in data efficiency in A2 compared to domain-specific features, which also contains temporal features. Classifiers using TREBA has the same performance as the full baseline training set with around 5% \u223c 10% of data (i.e., 10\u00d7 \u223c 20\u00d7 improved annotation efficiency).\nCRIM13. We test the transfer learning ability of our representation on CRIM13, a dataset with different image properties than Mouse100, the training set of TREBA. Our representation achieves the same performance as the baseline training with keypoints using around 5% to 10% of the training data (Figure 4 B1). With domain-specific features, TREBA uses 50% of the data annotation to have the same performance as the full training baseline (i.e., 2\u00d7 improved annotation efficiency). Our representation is able to generalize to a different dataset of the same organism.\nFly. When using keypoints only, our representation re-  quires 10% of the data (Figure 4 C1) and for features, our representation requires 50% of the data (Figure 4 C2) to achieve the same performance as full baseline training. This corresponds to 2\u00d7 improved annotation efficiency.", "publication_ref": ["b36", "b3"], "figure_ref": ["fig_2", "fig_2", "fig_2", "fig_2", "fig_2"], "table_ref": []}, {"heading": "Model Ablations", "text": "We perform the following model ablations to better characterize our approach. In this section, percentage error reduction relative to baseline is averaged over all training fractions. Additional results are in the Supplementary Material.\nVarying Programmed Tasks. We test the performance of TREBA trained with each single program provided by the domain experts in Table 1, and the average, best, and worst performance is visualized in Figure 5. On average, representations learned from a single program is better than using features alone, but using all provided programs further improves performance.\nFor a single program, there could be a large variation in performance depending on the selected program (Figure 5). While the best performing single program is close in classifier MAP to using all programs, the worst performing program may increase error, as in MARS and CRIM13. We further tested the performance using more programs.\nIn the mouse domain, we found that with three randomly selected programs, the variation between runs is much smaller compared to single programs (Supplementary Material). With three programs, we achieve comparable average error reduction from baseline features to using all pro-grams (MARS: 14.6% error reduction for 3 programs vs. 15.3% for all, CRIM13: 9.2% for 3 programs vs. 9.5% for all). For the fly domain, we found that we needed seven programs to achieve comparable performance (20.7% for 7 programs vs. 21.2% for all).\nVarying Decoder Losses. When the programmed tasks are fixed, decoder losses with different combinations of consistency (Section 3.3.1), decoding (Section 3.3.2), and contrastive (Section 3.3.3) loss are similar in performance (Supplementary Material). Additionally, we evaluate the TREBA framework without programmed tasks, with decoder tasks using trajectory generation and unsupervised contrastive loss. While self-supervised representations are also effective at reducing baseline error, we achieve the best classifier performance using TREBA with programmed tasks (Table 2). Furthermore, we found that training trajectory representations without self-decoding, using the contrastive loss from [7,8], resulted in less effective representations for classification (Supplementary Material).\nData Augmentation. We removed the losses using the data augmentation described in Section 3.3.4, and found that performance was slightly lower for all datasets than with augmentation. In particular, adding data augmentation decreases error by 1.2% on MARS, 2.5% on CRIM13, and 5.3% on Fly compared to without data augmentation.", "publication_ref": ["b6", "b7"], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Pre-Training Variations", "text": "The results shown for MARS was obtained with pre-training TREBA on Mouse100, a large in-house mouse dataset with the same image prop- erties as MARS. Figure 6 demonstrates the effect of varying TREBA training data amount with TVAE only and with programs. For both keypoints and features, we observe that TVAE (MARS) has the largest error. We see that error can be decreased by either adding more data (features + TVAE (Mouse100) with 3.9% decrease) or adding task programming (features + Programs (MARS) with 4.4% decrease). Adding both more data and task programming results in an average decrease of 5.7% error relative to TVAE (MARS) and the lowest average error.", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Conclusion", "text": "We introduce a method to learn an annotation-sample efficient Trajectory Embedding for Behavior Analysis (TREBA). To train this representation, we study selfsupervised decoder tasks as well as decoder tasks with programmatic supervision, the latter created using task programming. Our results show that TREBA can reduce annotation requirements by a factor of 10 for mice and 2 for flies. Our experiments on three datasets (two in mice and one in fruit flies) suggest that our approach is effective across different domains. TREBA is not restricted to animal behavior  and may be applied to other domains where tracking data is expensive to annotate, such as in sports analytics.\nOur experiments highlight, and quantify, the tradeoff between task programming and data annotation. The choice of which is more effective will depend on the cost of annotation and the level of expert understanding in identifying behavior attributes. Directions in creating tools to facilitate program creation and data annotation will help further accelerate behavioral studies.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We would like to thank Tomomi Karigo at Caltech for providing the mouse dataset. The Simons Foundation (Global Brain grant 543025 to PP) generously supported this work, and this work is partially supported by NIH Award #K99MH117264 (to AK), NSF Award #1918839 (to YY), and NSERC Award #PGSD3-532647-2019 (to JJS).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Supplementary Material", "text": "We provide additional details and experimental results from task programming and TREBA.\n\u2022 Section A describes the programs we use in the mouse and fly domain (Section A.1) as well as experimental results with varying number of programs (Section A.2).\n\u2022 Section B provides implementation details on the representation learning architecture for TREBA and behavior classification models.\n\u2022 Section C contains experimental results for decoder loss variations, time estimates, and classification samples.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Program Details and Experiments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1. Program Details", "text": "Programs for the Mouse Domain. We provide additional details on the programs listed in Table 1 in Section 3 of the paper. For the datasets in the mouse domain, programs are selected by domain experts based on the features used for mouse behavior classification in [36]. The experiments are recorded for a standard resident-intruder assay, where an intruder mouse is introduced to the cage of the resident mouse. Mouse 1 corresponds to the resident mouse and mouse 2 corresponds to the intruder mouse. These features are based on the anatomically defined keypoints tracked by the MARS tracker for each mouse: the nose, the ears, the base of the neck, the hips, and the base of the tail. A subset of the programs for the mouse domain is visualized in Figure 7, and all the programs we use for the mouse domain are listed below.\n\u2022 Facing angle: Relative angle between orientation of the body of the mouse to the line connecting centroids of the two mice. The facing angle is computed for both mice. This describe how closely one mouse is facing the other mouse.\n\u2022 Speed: Change in position of the centroid of the mouse across consecutive frames. The speed is computed for both mice. This property is especially important for helping identify aggressive behavior.\n\u2022 Distance between nose of mouse 1 and 2: Distance between the nose keypoints of each mouse. Distance between noses can be used for identifying when the mice are interacting during social behavior such as sniff.\n\u2022 Distance between nose of mouse 1 and tail of mouse 2: Distance between the nose keypoint of mouse 1 and base of tail keypoint of mouse 2. Distance between nose of mouse 1 and tail of mouse 2 can be used to identify when the mice are interacting during social behavior such as sniff.\n\u2022 Head-Body Angle: For one mouse, the angle formed by the nose, neck, and base of tail keypoints. This angle is computed for each mouse. This attribute helps describe the body shape of the mouse, since it varies with changes to the relative orientation of the head and body of each mouse.\n\u2022 Nose Movement: Nose movement of each mouse measured by speed relative to the movement of the centroid. This is computed for each mouse. This attribute describes the nose and head speed with respect to the center of the mouse, and can help identify aggressive behavior.\nPrograms for the Fly Domain. For the datasets in the fly domain, these programs are selected by domain experts based on the features used for fly behavior classification in [15]. For each fly, the fly tracker tracks the centroid of the body with a fitted ellipse for the body, the left wingtip keypoint and the right wingtip keypoint. A subset of the programs is visualized in Figure 8 and all the programs we use for the fly domain are listed below.\n\u2022 Angular speed: Change in heading direction of the fly across consecutive frames based on the fly body ellipse. The angular speed is computed for all flies. This attribute describes how fast the fly is turning and can help identify behaviors such as tussle and circle.\n\u2022 Minimum and maximum wing angles: The wing angle is the angle between the wing tip keypoint, the centroid, and the point on the back of the body ellipse. Programs are used to compute both the minimum and maximum wing angles for each fly. The wing attributes are especially important behaviors defined by wing position and motion, such as wing extension and wing threat. The facing angle is computed for both flies. The facing angle helps identify if a fly is facing in the direction of the other fly.\n\u2022 Speed: Change in position of the centroid of the fly across consecutive frames. The speed is computed for all flies. This property helps identify behaviors such as lunge, which usually has high speed.\n\u2022 Distance between centroid of fly 1 and 2: The distance between flies is often a good attribute to determine if the flies are interacting during social behavior.\n\u2022 Ratio between the major and minor axis: The ratio between the major and minor axis length of the fly body ellipse. This is computed for each fly. This attribute is a useful description of the body shape of the fly. When the fly is tilting up, the ratio is usually smaller, and when the fly is flat against the surface, the ratio is usually larger.", "publication_ref": ["b36", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "A.2. Program Performance Results", "text": "We evaluate the representation learned using task programming for each individual program for the mouse and fly domains (Table 3 to Table 8). The evaluation procedure is the same as the main paper, where the MAP is averaged over nine runs with three random selection for each training fraction. MAP@k% corresponds to the classifier MAP supervised with k% of the training data. We note that average error reduction discussed in this section is computed across all evaluated training fractions from the main paper (1%, 2%, 5%, 10%, 25%, 50%, 100%).\nMouse Program Evaluations. We train TREBA with each individual program from the mouse domain on Mouse100 using the consistency and contrastive losses, and evaluate the performance of domain-specific features+TREBA on MARS (Table 3) and CRIM13 (Table 5). In general, TREBA trained on a single program improves classifier performance, except for the bottom three programs and there is a high variation in performance (for example, Nose Movement Mouse 1 vs. Distance Nose-Nose). The two distance attributes, Head-Body Angle of Mouse 1, and Facing Angle of Mouse 1 generally performs the best across MARS and CRIM13 as a single program.\nThe best performing single program, Head-Body Angle Mouse 1 for MARS (Table 3) and Distance Nose-Nose for CRIM13 (Table 5), is comparable in performance to using all programs. When comparing average error reduction from baseline hand-designed features on MARS, TREBA using the top single program (Head-Body Angle Mouse 1, Table 3) achieves an error reduction of 14.5% and using all programs achieves an error reduction of 15.3%. On CRIM13, the top single program (Distance Nose-Nose, Table 5) achieves an error reduction of 9.3% and all programs achieves an error reduction of 9.5%. In contrast, the worst performing single program may reduce performance in the mouse domain. We study whether this performance variance can be reduced by adding more programs.\nWe experiment training TREBA using sets of three programs and evaluating behavior classification performance (MARS in Table 4, CRIM13 in Table 6). We note that program sets B and C are three randomly selected programs from the full mouse program list, and program set A consists of the worst performing three single programs. Despite program set A consisting of the lowest performing single programs, we see that for both MARS and CRIM13, the different sets of three programs are similar in performance and is comparable to using all programs (Table 4, Table 6). By training TREBA with three programs instead of one, the performance variation across program sets is much lower. We recommend that domain experts train with multiple programs, unless the best performing single program is known.\nFly Program Evaluations. We train TREBA on the Fly dataset without annotations using individual programs from the fly domain and evaluate on the Fly dataset (Table 7). Training with TREBA with any single expert-engineered program improves performance in the fly domain. We see that the speed and wing angle features generally perform the best. The top performing single program (Min. Wing Angle Fly 1, Table 7) achieves 19.4% average error reduction over baseline features, comparing to 21.2% using all programs. Similar to the mouse domain, if the best performing program is known ahead of time, we can achieve comparable performance to training TREBA using all programs. However, the best and worst single programs have a large variance, and we experiment with adding more programs. We start by training on sets of three programs, and found that there is a gap in performance to using all programs (Table 8). We additionally experiment with sets of seven programs to close this performance gap. Training with randomly selected three or seven programs (Table 8) has much smaller variations across program selections compared to single programs (Table 7). For the fly domain, we found that training with seven programs is able to achieve comparable performance to using all programs.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B. Additional Implementation Details", "text": "We provide hyperparameters used in training TREBA (Table 9) and the classification models (Table 10). Our code is available at https://github.com/neuroethology/TREBA.\nFor training TREBA, the TVAE consists of a bi-directional GRU with 256 units for the encoder, followed by linear layers, with a latent dimension of 32. We take the encoding mean from the encoder, z \u00b5 , as our learned representation of the trajectory. The decoder for the self-decoding task is also a GRU with 256 units followed by linear layers to predict the state in the next timestamp (by predicting the change from the current state to the next state). The decoder for the other tasks (attribute decoding, contrastive loss) consists of a fully connected neural network with 32 units. For the attribute consistency loss, we use the method proposed in [43] to train a 256 unit GRU to approximate non-differentiable programs. When the corresponding decoder is used, we weigh the consistency loss by 1.0, contrastive loss by 10.0, and the decoding loss by 1.0. We train TREBA using Adam optimizer with a learning rate of 0.0002. The input trajectories to the TREBA model are the detected keypoints for mouse and fly using the MARS tracker [36] and Fly tracker [15] respectively. At each frame, we stack the keypoints of the agents, and a trajectory in our experiment consists of 21 frames. We normalize the coordinates of pose keypoints by the image pixel dimensions.\nFor classification, we use a shallow fully connected network with two hidden layers. We decrease the size of the network as the input training fraction decreases (Table 10). The model size and other hyperparameters are chosen based on the validation split. Our results are all reported on the test split. We train the classification models using cross-entropy loss and Adam optimizer with learning rate 0.001. ", "publication_ref": ["b43", "b36", "b14"], "figure_ref": [], "table_ref": ["tab_8", "tab_8"]}, {"heading": "C. Additional Experimental Results", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1. Decoder Loss Variations", "text": "We evaluate TREBA trained using different decoder losses on supervised behavior classification. The procedure is the same as described in the main paper. We evaluate performance given both our learned representation and one of either (1) raw keypoints or (2) domain-specific features designed by experts. The input keypoints to the classification model are the detected poses from the MARS tracker [36] and the Fly tracker [15]. The input domain-specific features are the handdesigned trajectory features for mouse [36] and fly [15]. The input features are a superset of the programs we use to train TREBA (listed in Table 1 from the main paper and described in Supplementary Material Section A.1).\nWe compare the MAP of TREBA representations trained with different decoder losses in Table 11. The rows for TVAE and TVAE + Unsup. Contrast represents TREBA trained without programmed tasks and the remaining rows represents different combinations of decoder losses with programmed tasks. The average error reduction of these runs from baseline are shown in Table 2 in the main paper. Across all domains and training data amounts, we see that the learned representation improves classifier performance for both keypoints and domain-specific features. The improvements in performance are generally larger when we use keypoints, most likely because domain-specific features already contain informative features for classification. Furthermore, we experiment training TREBA without self-decoding, using unsupervised contrastive loss similar to [7,8], and we see that the classifier MAP is lower than training with self-decoding using the TVAE loss.\nTable 11 demonstrates that when using task programming, different decoder loss combinations (attribute consistency, decoding, and contrastive loss) are similar in performance in general, except when consistency loss is used alone. The lowest performing loss is when we use attribute consistency loss alone, which is applied to the generated trajectory and not directly to the representation. This result suggests that having at least one loss term directly applied on the representation (either decoding or contrastive loss) is beneficial.\nComparing task programming with TVAE loss alone, we see that TVAE loss is generally lower in performance (Table 11, Table 2 in the main paper). We note that TVAE loss alone corresponds to self-supervised learning with self-decoding only. Adding unsupervised contrastive loss to self-decoding improves performance relative to the TVAE, but we can improve the performance further using losses based on task programming (for example, TVAE+Contrastive+Consistency).\nRandom Program Inputs. We further experiment with training TREBA (Contrastive + Consistency), without expertengineered programs, using a program that returns one of three classes randomly with equal probability for each trajectory. We found that the error reduction when using a program with random outputs is between training using TVAE alone and using unsupervised contrastive loss. On MARS relative to baseline features, TREBA with a random program achieves an error reduction of 14.0%, compared to 13.7% for TVAE, 14.3% for TVAE + Unsup. Contrastive, 15.3% for all programs (Table 2 in main paper). On Fly relative to baseline features, TREBA with a random program achieves an error reduction of 13.6%, compared to 11.7% for TVAE, 16.1% for TVAE + Unsup. Contrastive, 21.2% for all programs (Table 2 in main paper). We tried adding more random programs during training, but did not observe an increase in performance. The lower performance of TREBA using random program inputs compared to all programs suggests that programs engineered using structured expert knowledge based on behavior attributes is important for improving the effectiveness of the learned representation.", "publication_ref": ["b36", "b14", "b36", "b14", "b6", "b7"], "figure_ref": [], "table_ref": ["tab_9", "tab_5", "tab_9", "tab_9", "tab_5", "tab_5", "tab_5"]}, {"heading": "C.2. Time Estimates", "text": "Based on domain expert estimates in neurobiology, behavior annotation takes 4 times the length of 30Hz videos, while task programming takes 5 to 10 minutes per program. We note that this time estimate is from domain experts familiar with data annotation and trajectory feature design. Applying this estimate to Figure 4 A2 B2 C2 in the main paper (for data efficiency with domain-specific features), in regions of low time investment (< 2 hours), it is generally better for domain experts to annotate more data. For performance at > 2 hours, task programming provides a better return on investment of the expert's time. Task programming requires an initial effort to produce the programs, which then scales to any data amount with no additional effort. Note that this time is variable depending on the domain expert and the domain, and our estimate is based on neurobiologists familiar with data annotation and trajectory feature design.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "C.3. Classification Samples", "text": "We visualize classification samples for each dataset using input domain-specific features and TREBA (MARS in Figure 9, CRIM13 in Figure 10, Fly in Figure 11). We visualize the classifier using TREBA at the training fraction such that the classifier MAP matches that of the fully-supervised baseline feature performance. Comparing the samples qualitatively, we see that the classifier output with the full training set is comparable to TREBA with 10\u00d7 reduced annotations on MARS and 2\u00d7 reduced annotations on CRIM13 and Fly. We note that the classifier trained on reduced data alone (last row of Figures 9, 10, 11) is generally less accurate, compared to the classifiers trained with either full training data or with TREBA. ", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Toward a science of computational ethology", "journal": "Neuron", "year": "2014", "authors": "J David; Pietro Anderson;  Perona"}, {"ref_id": "b1", "title": "Mapping the stereotyped behaviour of freely moving fruit flies", "journal": "Journal of The Royal Society Interface", "year": "2014", "authors": "J Gordon;  Berman; M Daniel; William Choi; Joshua W Bialek;  Shaevitz"}, {"ref_id": "b2", "title": "Customizing scripted bots: Sample efficient imitation learning for human-like behavior in minecraft", "journal": "", "year": "", "authors": "Brian Broll; Matthew Hausknecht; Dave Bignell; Adith Swaminathan"}, {"ref_id": "b3", "title": "Social behavior recognition in continuous video", "journal": "", "year": "2012", "authors": "P Xavier; Piotr Burgos-Artizzu; Dayu Doll\u00e1r;  Lin; J David; Pietro Anderson;  Perona"}, {"ref_id": "b4", "title": "Unsupervised identification of the internal states that shape natural behavior", "journal": "Nature neuroscience", "year": "2019", "authors": "J Adam; Jonathan W Calhoun; Mala Pillow;  Murthy"}, {"ref_id": "b5", "title": "Argoverse: 3d tracking and forecasting with rich maps", "journal": "", "year": "2019", "authors": "Ming-Fang Chang; John Lambert; Patsorn Sangkloy; Jagjeet Singh; Slawomir Bak; Andrew Hartnett; De Wang; Peter Carr; Simon Lucey; Deva Ramanan"}, {"ref_id": "b6", "title": "A simple framework for contrastive learning of visual representations", "journal": "ICML", "year": "2005", "authors": "Ting Chen; Simon Kornblith; Mohammad Norouzi; Geoffrey Hinton"}, {"ref_id": "b7", "title": "Big self-supervised models are strong semi-supervised learners", "journal": "", "year": "2020", "authors": "Ting Chen; Simon Kornblith; Kevin Swersky; Mohammad Norouzi; Geoffrey Hinton"}, {"ref_id": "b8", "title": "Selfconsistent trajectory autoencoder: Hierarchical reinforcement learning with trajectory embeddings", "journal": "", "year": "2018", "authors": "D John; Yuxuan Co-Reyes; Abhishek Liu; Benjamin Gupta; Pieter Eysenbach; Sergey Abbeel;  Levine"}, {"ref_id": "b9", "title": "Automated image-based tracking and its application in ecology", "journal": "Trends in ecology & evolution", "year": "2014", "authors": "I Anthony; John A Dell; Kristin Bender; Iain D Branson; Gonzalo G Couzin;  De Polavieja; Pjj Lucas; Alfonso Noldus; Pietro P\u00e9rez-Escudero;  Perona; D Andrew; Martin Straw;  Wikelski"}, {"ref_id": "b10", "title": "Unsupervised visual representation learning by context prediction", "journal": "", "year": "2015", "authors": "Carl Doersch; Abhinav Gupta; Alexei A Efros"}, {"ref_id": "b11", "title": "Multi-task selfsupervised visual learning", "journal": "", "year": "2017", "authors": "Carl Doersch; Andrew Zisserman"}, {"ref_id": "b12", "title": "Computational analysis of behavior", "journal": "Annual review of neuroscience", "year": "2016", "authors": "Kristin Se Roian Egnor;  Branson"}, {"ref_id": "b13", "title": "Learning recurrent representations for hierarchical behavior modeling", "journal": "ICLR", "year": "2017", "authors": "Eyrun Eyjolfsdottir; Kristin Branson; Yisong Yue; Pietro Perona"}, {"ref_id": "b14", "title": "Detecting social actions of fruit flies", "journal": "Springer", "year": "2011", "authors": "Eyrun Eyjolfsdottir; Steve Branson; P Xavier; Eric D Burgos-Artizzu; Jonathan Hoopfer;  Schor; J David; Pietro Anderson;  Perona"}, {"ref_id": "b15", "title": "Unsupervised representation learning by predicting image rotations", "journal": "ICLR", "year": "2018", "authors": "Spyros Gidaris; Praveer Singh; Nikos Komodakis"}, {"ref_id": "b16", "title": "Scaling and benchmarking self-supervised visual representation learning", "journal": "", "year": "2019", "authors": "Priya Goyal; Dhruv Mahajan; Abhinav Gupta; Ishan Misra"}, {"ref_id": "b17", "title": "Deepposekit, a software toolkit for fast and robust animal pose estimation using deep learning. Elife, 8:e47994", "journal": "", "year": "2019", "authors": "M Jacob; Daniel Graving; Hemal Chae; Liang Naik; Benjamin Li;  Koger; Iain D Blair R Costelloe;  Couzin"}, {"ref_id": "b18", "title": "Video representation learning by dense predictive coding", "journal": "", "year": "2019", "authors": "Tengda Han; Weidi Xie; Andrew Zisserman"}, {"ref_id": "b19", "title": "Minecraft as ai playground and laboratory", "journal": "", "year": "2019", "authors": "Katja Hofmann"}, {"ref_id": "b20", "title": "Automated measurement of mouse social behaviors using depth sensing, video tracking, and machine learning", "journal": "Proceedings of the National Academy of Sciences", "year": "2015", "authors": "Weizhe Hong; Ann Kennedy; P Xavier; Moriel Burgos-Artizzu;  Zelikowsky; G Santiago; Pietro Navonne; David J Perona;  Anderson"}, {"ref_id": "b21", "title": "B-soid: An open source unsupervised algorithm for discovery of spontaneous behaviors", "journal": "bioRxiv", "year": "2020", "authors": "I Alexander; Eric A Hsu;  Yttri"}, {"ref_id": "b22", "title": "Jaaba: interactive machine learning for automatic annotation of animal behavior", "journal": "Nature methods", "year": "2013", "authors": "Mayank Kabra; Alice A Robie; Marta Rivera-Alba; Steven Branson; Kristin Branson"}, {"ref_id": "b23", "title": "", "journal": "", "year": "2020", "authors": "Prannay Khosla; Piotr Teterwak; Chen Wang; Aaron Sarna; Yonglong Tian; Phillip Isola; Aaron Maschinot; Ce Liu; Dilip Krishnan"}, {"ref_id": "b24", "title": "Revisiting self-supervised visual representation learning", "journal": "", "year": "1920", "authors": "Alexander Kolesnikov; Xiaohua Zhai; Lucas Beyer"}, {"ref_id": "b25", "title": "Identifying behavioral structure from deep variational embeddings of animal motion", "journal": "bioRxiv", "year": "2020", "authors": "Kevin Luxem; Falko Fuhrmann; Johannes K\u00fcrsch; Stefan Remy; Pavol Bauer"}, {"ref_id": "b26", "title": "Deeplabcut: markerless pose estimation of user-defined body parts with deep learning", "journal": "Nature neuroscience", "year": "2018", "authors": "Alexander Mathis; Pranav Mamidanna; Kevin M Cury; Taiga Abe; N Venkatesh; Mackenzie Weygandt Murthy; Matthias Mathis;  Bethge"}, {"ref_id": "b27", "title": "Simple behavioral analysis (simba): an open source toolkit for computer classification of complex social behaviors in experimental animals", "journal": "BioRxiv", "year": "2020", "authors": " Simon Ro Nilsson; L Nastacia;  Goodwin; J Jia; Sophia Choong;  Hwang; R Hayden; Zane Wright; Xiaoyu Norville; Dayu Tong;  Lin; S Brandon; Neir Bentzley;  Eshel"}, {"ref_id": "b28", "title": "Representation learning with contrastive predictive coding", "journal": "", "year": "2005", "authors": "Aaron Van Den Oord; Yazhe Li; Oriol Vinyals"}, {"ref_id": "b29", "title": "Evolving losses for unsupervised video representation learning", "journal": "", "year": "", "authors": "Anelia Aj Piergiovanni; Michael S Angelova;  Ryoo"}, {"ref_id": "b30", "title": "Improving language understanding by generative pre-training", "journal": "", "year": "2018", "authors": "Alec Radford; Karthik Narasimhan; Tim Salimans; Ilya Sutskever"}, {"ref_id": "b31", "title": "Snorkel: Rapid training data creation with weak supervision", "journal": "NIH Public Access", "year": "2017", "authors": "Alexander Ratner; H Stephen; Henry Bach; Jason Ehrenberg; Sen Fries; Christopher Wu;  R\u00e9"}, {"ref_id": "b32", "title": "Data programming: Creating large training sets, quickly", "journal": "", "year": "2016", "authors": "J Alexander; Christopher M De Ratner; Sen Sa; Daniel Wu; Christopher Selsam;  R\u00e9"}, {"ref_id": "b33", "title": "", "journal": "", "year": "", "authors": "Mirco Ravanelli; Jianyuan Zhong; Santiago Pascual; Pawel Swietojanski; Joao Monteiro; ; ; Yoshua Bengio"}, {"ref_id": "b34", "title": "Multi-task self-supervised learning for robust speech recognition", "journal": "IEEE", "year": "2020", "authors": ""}, {"ref_id": "b35", "title": "Multi-task self-supervised learning for human activity detection", "journal": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies", "year": "2019", "authors": "Aaqib Saeed; Tanir Ozcelebi; Johan Lukkien"}, {"ref_id": "b36", "title": "The mouse action recognition system (mars): a software pipeline for automated analysis of social behaviors in mice", "journal": "", "year": "2011", "authors": "Cristina Segalin; Jalani Williams; Tomomi Karigo; May Hui; Moriel Zelikowsky; Jennifer J Sun; Pietro Perona; David J Anderson; Ann Kennedy"}, {"ref_id": "b37", "title": "Does visual self-supervision improve learning of speech representations?", "journal": "", "year": "2020", "authors": "Abhinav Shukla; Stavros Petridis; Maja Pantic"}, {"ref_id": "b38", "title": "Videobert: A joint model for video and language representation learning", "journal": "", "year": "2019", "authors": "Chen Sun; Austin Myers; Carl Vondrick; Kevin Murphy; Cordelia Schmid"}, {"ref_id": "b39", "title": "Scalability in perception for autonomous driving: Waymo open dataset", "journal": "", "year": "2020", "authors": "Pei Sun; Henrik Kretzschmar; Xerxes Dotiwalla; Aurelien Chouard; Vijaysai Patnaik; Paul Tsui; James Guo; Yin Zhou; Yuning Chai; Benjamin Caine"}, {"ref_id": "b40", "title": "Robust imitation of diverse behaviors", "journal": "", "year": "2017", "authors": "Ziyu Wang; Josh S Merel; E Scott;  Reed; Gregory Nando De Freitas; Nicolas Wayne;  Heess"}, {"ref_id": "b41", "title": "Mapping sub-second structure in mouse behavior", "journal": "Neuron", "year": "2015", "authors": "B Alexander;  Wiltschko; J Matthew; Giuliano Johnson; Ralph E Iurilli; Jesse M Peterson; Stan L Katon; Victoria E Pashkovski;  Abraira; P Ryan; Sandeep Robert Adams;  Datta"}, {"ref_id": "b42", "title": "Diverse generation for multi-agent sports games", "journal": "", "year": "2019", "authors": "A Raymond; Alexander G Yeh; Jonathan Schwing; Kevin Huang;  Murphy"}, {"ref_id": "b43", "title": "Learning calibratable policies using programmatic style-consistency. ICML", "journal": "", "year": "2005", "authors": "Eric Zhan; Albert Tseng; Yisong Yue; Adith Swaminathan; Matthew Hausknecht"}, {"ref_id": "b44", "title": "Generating multi-agent trajectories using programmatic weak supervision. ICLR", "journal": "", "year": "2019", "authors": "Eric Zhan; Stephan Zheng; Yisong Yue; Long Sha; Patrick Lucey"}, {"ref_id": "b45", "title": "Unpaired image-to-image translation using cycleconsistent adversarial networkss", "journal": "", "year": "2017", "authors": "Jun-Yan Zhu; Taesung Park; Phillip Isola; Alexei A Efros"}], "figures": [{"figure_label": "11", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "1 .Figure 1 .11Figure 1. Overview of our approach. Part 1: A typical behavior study starts with extraction of tracking data from videos. We show 7 keypoints for each mouse, and draw the trajectory of the nose keypoint. Part 2: Domain experts can either do data annotation (Classifier A) or task programming (Classifier B) to reduce classifier error. The middle panel shows annotated frames at 30Hz. Colors in the bottom plot represent interpolated performance based on classifier error at the circular markers (full results in Section 4.3). The size of the marker represents the error variance.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 .3Figure 3. TREBA Training and Inference Pipelines. During training, we use trajectory self-decoding and the programmed decoder tasks to train the trajectory encoder. The learned representation is used for downstream tasks such as behavior classification.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 .4Figure 4. Data Efficiency for Supervised Classification. Training data fraction vs. classifier error on MARS (left), CRIM13 (middle) and fly (right). The blue lines represent performance with baseline keypoints and features, and the orange lines are with TREBA. The shaded regions correspond to the classifier standard deviation over nine repeats. The gray dotted line marks the best observed classifier performance when trained on the baseline features (using the full training set). Note the log scale on both the x and y axes.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 6 .6Figure 6. Pre-Training Data Variations. Effect of varying pretraining data on classifier data efficiency for the MARS dataset. \"TVAE\" corresponds to training TREBA with TVAE losses only, and \"Programs\" corresponds to training with all programs.", "figure_data": ""}, {"figure_label": "78", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 7 .Figure 8 .78Figure 7. Visualizing behavior attributes for mouse dataset.", "figure_data": ""}, {"figure_label": "3456", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "3 . 4 . 5 . 6 .3456Single Program Variations on MARS. Average MAP of classifiers on MARS trained with features and with TREBA using the specified single program. The order of the programs are based on the average error reduction over all training fractions (highest error reduction at the top). Additional Program Variations on MARS. Average MAP of classifiers on MARS trained with features and with TREBA using the three programs. The programs are: (A) Nose Movement Mouse 1, Nose Movement Mouse 2, Facing Angle Mouse 2; (B) Facing Angle Mouse 1, Head-Body Angle Mouse 1, Head-Body Angle Mouse 2; (C) Speed Mouse 1, Nose Movement 1, Distance Nose-Nose. MAP@k% 10 50 100 Domain-specific features 0.792 0.858 0.873 + Distance Nose-Nose 0.811 0.876 0.889 + Distance Nose-Tail 0.807 0.881 0.891 + Speed Mouse 2 0.810 0.875 0.891 + Facing Angle Mouse 1 0.811 0.879 0.890 + Head-Body Angle Mouse 1 0.809 0.880 0.889 + Speed Mouse 1 0.808 0.876 0.889 + Head-Body Angle Mouse 2 0.802 0.870 0.881 + Nose Movement Mouse 2 0Single Program Variations on CRIM13. Average MAP of classifiers on CRIM13 trained with features and with TREBA using the specified single program. The order of the programs are based on the average error reduction over all training fractions (highest error reduction at the top). Additional Program Variations on CRIM13. Average MAP of classifiers on MARS trained with features and with TREBA using the three programs. The programs are: (A) Nose Movement Mouse 1, Nose Movement Mouse 2, Facing Angle Mouse 2; (B) Facing Angle Mouse 1, Head-Body Angle Mouse 1, Head-Body Angle Mouse 2; (C) Speed Mouse 1, Nose Movement 1, Distance Nose-Nose.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Task Programming and Data Annotation for Classifier Training. Domain experts can choose between doing task programming and/or data annotation. Task programming is the process for domain experts to engineer decoder tasks for representation learning. The programs enable learning of annotation-sample efficient trajectory features to improve performance instead of additional annotations.", "figure_data": "Domain ExpertTask ProgrammingData AnnotationSpeeddist_nose(x1, y1, x2, y2): x_diff = x2 -x1Distancey_diff = y2 -y1 dist = norm(x_diff, y_diff)MountSniffOtherExamine trajectory dataSelect behavior attributesWrite programsAdd decoder taskAnnotate frame-level behaviorClassifierFeatureModelTrainingExtractionTrainingFigure 2."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "evaluate our classifiers, with and without TREBA features, using Mean Average Precision (MAP). We compute the mean over behaviors of interest with equal weighting. Our classifiers are shallow fully-connected neural networks on the input features. To determine the relationship between classifier performance and training set size, we sub-sample the training data by randomly sampling trajectories (with lengths of 100 frames) to achieve a desired fraction of the training set size. Sampling was performed to achieve a similar class distribution as the full training set. We train each classifier nine times over three different random selections of the training data for each training fraction (1%, 2%, 5%, 10%, 25%, 50%, 75%, 100%). Additional implementation details are in the Supplementary Material.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Varying Programmed Tasks. Effect of varying number of programmed tasks on classifier data efficiency. The shaded region corresponds to the best and worst classifiers trained using a single programmed task from Table1. The grey dotted line corresponds to the value where the baseline features achieve the best performance (using the full training set). Decoder Error Reductions. Percentage error reduction relative to baseline keypoints and domain-specific features for training with different decoder losses for TREBA. The average is taken over all evaluated training fractions.", "figure_data": "Error (Log Scale)3\u00d710 1 2 \u00d7 10 1 4 \u00d7 10 1 5\u00d710 1Features + TREBA (10 programs) Features + TREBA (1 program) Features MARS Features with Program VariationsCRIM13 Features with Program VariationsFly Features with Program Variations10 110 210 1 Training Data Fraction (Log Scale)10 010 210 1 Training Data Fraction (Log Scale)10 010 210 1 Training Data Fraction (Log Scale)10 0Figure 5. Keypoint Error Reduction (%)Decoder LossMARSCRIM13FlyTVAE52.2 \u00b1 4.034.7 \u00b1 1.515.4 \u00b1 2.1TVAE+ Unsup. Contrast52.6 \u00b1 3.937.4 \u00b1 2.420.9 \u00b1 1.7TVAE+ Contrast+Consist55.1 \u00b1 3.0 41.1 \u00b1 2.1 33.7 \u00b1 1.2Features Error Reduction (%)Decoder LossMARSCRIM13FlyTVAE13.7 \u00b1 1.88.2 \u00b1 4.611.7 \u00b1 4.7TVAE+ Unsup. Contrast14.3 \u00b1 2.28.9 \u00b1 4.116.1 \u00b1 1.7TVAE+ Contrast+Consist15.3 \u00b1 2.19.5 \u00b1 3.821.2 \u00b1 4.5"}, {"figure_label": "789", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Single Program Variations on Fly. Average MAP of classifiers on Fly trained with features and with TREBA using the specified single program. The order of the programs are based on the average error reduction over all training fractions (highest error reduction at the top). Additional Program Variations on Fly. Average MAP of classifiers on Fly trained with features and with TREBA using three and seven programs. The sets of three programs are: (A) Hyperparameters for Representation Learning.", "figure_data": "MAP@k%1050100Domain-specific features0.774 0.829 0.868MAP@k%1050100+ Min. Wing Angle Fly 10.820 0.864 0.885Domain-specific features 0.774 0.829 0.868+ Speed Fly 10.818 0.856 0.878+ 3 programs (A)0.814 0.857 0.880+ Speed Fly 20.804 0.861 0.880+ 3 programs (B)0.814 0.857 0.878+ Angular Speed Fly 10.821 0.862 0.881+ 3 programs (C)0.815 0.863 0.880+ Max. Wing Angle Fly 20.814 0.859 0.882+ 7 programs (A)0.819 0.869 0.889+ Min. Wing Angle Fly 20.814 0.859 0.886+ 7 programs (B)0.820 0.863 0.885+ Distance Between Centroids 0.815 0.858 0.882+ 7 programs (C)0.815 0.860 0.882+ Facing Angle Fly 10.814 0.862 0.881+ Axis Ratio Fly 10.809 0.855 0.882+ Angular Speed Fly 20.811 0.853 0.879+ Facing Angle Fly 20.811 0.853 0.876Speed Fly 1, Min/Max Wing Angle Fly 1; (B) Speed Fly 2, Facing+ Max. Wing Angle Fly 10.811 0.855 0.883Angle Fly 1, Axis Ratio Fly 2; (C) Min Wing Angle Fly 2, Speed+ Axis Ratio Fly 20.797 0.852 0.880Fly 1, Axis Ratio Fly 1.+ All Programs0.820 0.868 0.886The sets of seven programs are: (A) Distance, Angular Speed Fly1/2, Max Wing Angle Fly 1, Min/Max Wing Angle Fly 2, FacingAngle Fly 1; (B) Distance, Speed/Angular Speed Fly 1, Max WingAngle Fly 1, Facing Angle Fly 2, Axis Ratio Fly 1/2; (C) SpeedFly 2, Min/Max Wing Angle Fly 1/2, Facing Angle Fly 1/2.DatasetBatch size z-dim Encoder Units Decoder Units Temperature t Learning RateMouse100128322562560.070.0002Fly128322562560.070.0002"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Hyperparameters for Classification Models.", "figure_data": "DatasetBatch sizeClassifier UnitsClassifier Units Classifier Units Learning Rate(100%, 75%, 50%)(25%, 10%)(5%, 2%, 1%)MARS512256, 32128, 1664, 160.001CRIM13512256, 32128, 1664, 160.001Fly512256, 32128, 1664, 160.001"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_9", "figure_caption": ".635 0.656 0.538 0.621 0.648 0.348 0.519 0.586 + TVAE 0.817 0.852 0.859 0.703 0.796 0.820 0.419 0.635 0.722 + TVAE+Unsup. Contrast 0.815 0.852 0.866 0.706 0.813 0.837 0.521 0.667 0.739 + TVAE+Consist 0.704 0.763 0.776 0.581 0.694 0.720 0.497 0.657 0.729 Decoder Loss Variations. Comparing data efficiency of TREBA trained with different decoder losses with respect to classifier MAP. Keypoints and domain-specific features represent baseline input features. TVAE represents training TREBA with self-decoding only and contrastive, decoding and consistency loss are described in Section 3.3 of the main paper.", "figure_data": "DatasetMARSCRIM13FlyMAP@k%105010010501001050100Keypoints 0.588 0+ TVAE+Contrast 0.804 0.851 0.868 0.707 0.813 0.838 0.625 0.712 0.753+ TVAE+Decode0.825 0.857 0.872 0.719 0.828 0.848 0.666 0.737 0.773+ TVAE+Contrast+Consist0.822 0.856 0.866 0.722 0.821 0.837 0.650 0.707 0.750+ TVAE+Decode+Consist0.820 0.855 0.870 0.707 0.813 0.837 0.432 0.603 0.688+ TVAE+Contrast+Decode0.821 0.859 0.871 0.693 0.811 0.830 0.645 0.738 0.775+ TVAE+Contrast+Decode+Consist 0.821 0.857 0.870 0.693 0.811 0.834 0.484 0.616 0.679+ Unsup. Contrast0.582 0.704 0.735 0.587 0.697 0.721 0.384 0.560 0.645Domain-specific features0.824 0.838 0.847 0.792 0.858 0.873 0.774 0.829 0.868+ TVAE0.850 0.866 0.869 0.808 0.874 0.885 0.791 0.852 0.880+ TVAE+Unsup. Contrast0.850 0.866 0.871 0.808 0.876 0.889 0.811 0.858 0.882+ TVAE+Consist0.824 0.841 0.853 0.775 0.854 0.871 0.812 0.856 0.882+ TVAE+Contrast0.853 0.868 0.872 0.811 0.876 0.889 0.834 0.869 0.888+ TVAE+Decode0.851 0.869 0.874 0.805 0.880 0.892 0.815 0.866 0.883+ TVAE+Contrast+Consist0.853 0.868 0.877 0.808 0.876 0.888 0.820 0.868 0.886+ TVAE+Decode+Consist0.848 0.868 0.873 0.808 0.872 0.888 0.781 0.838 0.862+ TVAE+Contrast+Decode0.846 0.867 0.876 0.811 0.878 0.892 0.810 0.862 0.885+ TVAE+Contrast+Decode+Consist 0.851 0.866 0.872 0.813 0.879 0.892 0.783 0.842 0.868+ Unsup. Contrast0.830 0.847 0.853 0.787 0.858 0.874 0.784 0.842 0.866"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Annotations on MARS Test Set. Classifier annotations on MARS dataset vs. ground truth at 30Hz. For each sample, the second row corresponds to features + TREBA trained with 10 expert-engineered programs, with \u223c 10% of the supervised behavior annotations. The third row corresponds to training using domain-specific features on the full dataset. The last row corresponds to training using domain-specific features on \u223c 10% of data, without TREBA. Annotations on CRIM13 Test Set. Classifier annotations on CRIM13 dataset vs. ground truth at 25Hz. For each sample, the second row corresponds to features + TREBA trained with 10 expert-engineered programs, with \u223c 50% of the supervised behavior annotations. The third row corresponds to training using domain-specific features on the full dataset. The last row corresponds to training using domain-specific features on \u223c 50% of data, without TREBA. Annotations on Fly Test Set. Classifier annotations on Fly dataset vs. ground truth at 30Hz. For each sample, the second row corresponds to features + TREBA trained with 13 expert-engineered programs, with \u223c 50% of the supervised behavior annotations. The third row corresponds to training using domain-specific features on the full dataset. The last row corresponds to training using domainspecific features on \u223c 50% of data, without TREBA.", "figure_data": "MARS Test Sample 1Ground truth80k annotations + 10 programs 781k annotationssniff mount attack80k annotations0200400600800 Frame Number 1000 MARS Test Sample 2120014001600Ground truth 80k annotations + 10 programs 781k annotations 80k annotations Ground truth 546k annotations + 13 programs 1067k annotationsFly Test Sample 1 (Lunge)sniff mount attack lunge0 546k annotations200400600800 Frame Number 1000120014001600Figure 9. 0 Ground truth 213k annotations + 10 programs 407k annotations 213k annotations 0 Ground truth 213k annotations + 10 programs 407k annotations 213k annotations 0 Ground truth 546k annotations + 13 programs 1067k annotations 546k annotations 0 Ground truth 546k annotations + 13 programs 1067k annotations 546k annotations Figure 10. 0 Figure 11.200 200 200 200 200400 400 400 400 400600 600 600 600 600 Fly Test Sample 3 (Wing Extension) 800 1000 Frame Number CRIM13 Test Sample 1 800 1000 Frame Number CRIM13 Test Sample 2 800 1000 Frame Number 800 1000 Frame Number Fly Test Sample 2 (Tussle) 800 1000 1200 1200 1200 1200 1200 Frame Number1400 1400 1400 1400 14001600 1600 1600 1600 1600sniff mount attack sniff mount attack tussle wing extension"}], "formulas": [{"formula_id": "formula_0", "formula_text": "L tvae = E q \u03c6 T t=1 \u2212 log(p \u03b8 (s t+1 |s t , z)) +D KL (q \u03c6 (z|\u03c4 )||p \u03b8 (z)).(1)", "formula_coordinates": [3.0, 346.07, 670.19, 199.04, 45.05]}, {"formula_id": "formula_1", "formula_text": "x diff = x 2 \u2212 x 1 y diff = y 2 \u2212 y 1 \u03b8 = arctan(y diff , x diff ) Return \u03b8 \u2212 \u03c6 1 Domain Behavior", "formula_coordinates": [4.0, 320.82, 322.61, 122.37, 71.55]}, {"formula_id": "formula_2", "formula_text": "L attr = E \u03c4 \u223cD M j=1 1(\u03bb j (\u03c4 ) = \u03bb j (\u03c4 )) .(2)", "formula_coordinates": [5.0, 87.63, 294.77, 198.73, 30.32]}, {"formula_id": "formula_3", "formula_text": "L decode = E \u03c4 \u223cD M j=1 1(f(q \u03c6 (z \u00b5 |\u03c4 )) = \u03bb j (\u03c4 )) . (3)", "formula_coordinates": [5.0, 64.53, 489.77, 221.84, 30.32]}, {"formula_id": "formula_4", "formula_text": "L cntr. = B i=1 M j=1 \u22121 N pos(i,j) B k=1 1 i =k \u2022 1 \u03bbj (\u03c4i)=\u03bbj (\u03c4 k ) \u2022 log exp(g i \u2022 g k /t) N l=1 1 i =l \u2022 exp(g i \u2022 g l /t) ,(4)", "formula_coordinates": [5.0, 316.33, 96.72, 228.78, 62.77]}], "doi": "10.1101/2020.07.26.222299"}