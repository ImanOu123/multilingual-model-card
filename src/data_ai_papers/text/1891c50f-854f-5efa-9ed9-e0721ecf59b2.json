{"title": "PAC Optimal Exploration in Continuous Space Markov Decision Processes", "authors": "Jason Pazis; Ronald Parr", "pub_date": "", "abstract": "Current exploration algorithms can be classified in two broad categories: Heuristic, and PAC optimal. While numerous researchers have used heuristic approaches such as -greedy exploration successfully, such approaches lack formal, finite sample guarantees and may need a significant amount of finetuning to produce good results. PAC optimal exploration algorithms, on the other hand, offer strong theoretical guarantees but are inapplicable in domains of realistic size. The goal of this paper is to bridge the gap between theory and practice, by introducing C-PACE, an algorithm which offers strong theoretical guarantees and can be applied to interesting, continuous space problems.", "sections": [{"heading": "Introduction and motivation", "text": "Efficient exploration is a central concept in Reinforcement Learning (RL). Contrary to the more straightforward, nonsequential, supervised active learning setting, we are often unable to collect samples from different parts of the stateaction space at will. Samples must come from trajectories which depend on the dynamics of the underlying Markov Decision Process (MDP), which means that sampling certain states can be quite improbable unless we are actively trying to reach those states. Furthermore, even in cases where we have a generative model of the environment, it may be the case that only a tiny percentage of the state space is reachable from the starting state, and learning by collecting samples from the entire space would be very inefficient.\nA number of relatively recent papers have proposed new algorithms, or analyzed known methods, to explore efficiently in unknown MDPs. Unfortunately, even after more than a decade of incremental improvements on complexity bounds, experimental results on challenging, realistic applications are absent. Instead, it appears that research on near optimal exploration is still focused on small, discrete state-action spaces. This is in stark contrast with the rest of the field, which has emphasized continuous and/or large MDPs. 1 Copyright \u00a9 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n1 While some research in PAC optimal exploration for continuous spaces exists (Kakade, Kearns, and Langford 2003), it does not offer a concrete, practical algorithm. See section 4 for more details.\nThis paper contributes C-PACE, a new algorithm for exploration in continuous state MDPs, which is guaranteed to perform within some constant of the optimal policy on all but a small number of steps with high probability. In addition to our theoretical contribution, to demonstrate the applicability of the proposed approach in realistic problems, we present experimental results on a challenging six dimensional continuous HIV treatment domain.", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "Background", "text": "A Markov Decision Process (MDP) is a 5-tuple (S, A, P, R, \u03b3), where S is the state space of the process, A is the action space, P is a Markovian transition model p(s |s, a) denotes the probability density of a transition to state s when taking action a in state s , R is a reward function R(s, a, s ) is the expected reward for taking action a in state s and transitioning to state s , and \u03b3 \u2208 [0, 1) is a discount factor for future rewards. A deterministic policy \u03c0 for an MDP is a mapping \u03c0 : S \u2192 A from states to actions; \u03c0(s) denotes the action choice in state s. The value V \u03c0 (s) of a state s under a policy \u03c0 is defined as the expected, total, discounted reward when the process begins in state s and all decisions are made according to policy \u03c0. There exists an optimal policy \u03c0 * for choosing actions which yields the optimal value function V * (s), defined recursively via the Bellman optimality equation: V * (s) = max a { s p(s |s, a) (R(s, a, s ) + \u03b3V * (s ))}. Q \u03c0 (s, a) and Q * (s, a) are similarly defined when action a is taken at the first step.\nIn reinforcement learning, a learner interacts with a stochastic process modeled as an MDP and typically observes the state and immediate reward at every step; however, the transition model P and the reward function R are not accessible. The goal is to learn an optimal policy using the experience collected through interaction with the process. At each step of interaction, the learner observes the current state s, chooses an action a, and observes the resulting next state s and the reward received r, essentially sampling the transition model and the reward function of the process. Thus experience comes in the form of (s, a, r, s ) samples.\nThere have been many definitions of sample complexity in various RL settings. For the purposes of this paper, we employ the following definition due to Kakade (2003):\nDefinition 2.1. The sample complexity of exploration of an algorithm is the number of time steps t such that V \u03c0 (s t ) < V * (s t ) \u2212 . Discrete, PAC optimal exploration algorithms measure their efficiency in terms of the number of states and actions. In continuous state-action spaces we have to use a definition which relates to the covering number of the space: Definition 2.2. An exploration algorithm is said to be efficient if its sample complexity is polynomial in the covering number of the state-action space N SA (LQ, d ) (see Definition 3.2).", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "C-PACE: Continuous PAC optimal exploration", "text": "In many interesting problems, we do not start with a representative set of samples. Instead, acquiring samples is part of the learning process, and we are not only evaluated on the quality of the resulting policy, but also on the sample complexity of the algorithm.\nWhile such criteria can be used for almost any problem, the most interesting cases from a practical perspective are problems with complicated dynamics and large state spaces, which cannot be sufficiently covered with a reasonable number of samples from a policy which selects actions uniformly at random (which is often what is used when we lack an expert policy). 2 Additionally, while not strictly necessary for such problems to be interesting, we will assume that in general we cannot reset the process to any desired state (we do not have a generative model).\nThis section introduces C-PACE, an algorithm for PAC optimal exploration in continuous state spaces. Instead of discretizing the state-action space or using a parametric approximation technique, C-PACE assumes that there exists some distance metric in which the state-action value function is smooth. For simplicity of exposition, our choice of smoothness measure will be Lipschitz continuity, although C-PACE can easily be extended to support other forms of smoothness (such as H\u00f6lder continuity), simply by pushing the complexity into the distance function. A Lipschitz continuous Q-value function means that for any two state-action pairs we have |Q(s, a)\u2212Q(s , a )| \u2264 L Q d(s, a, s , a ). Even though it may be statistically impossible to sample transitions out of a particular state-action pair more than once, Lipschitz continuity allows us to use samples from nearby state-actions to approximate Bellman's equation without introducing too much error. We will make this notion more concrete later in this section.\nSimilarly to other work in the field of PAC optimal exploration (Strehl and Littman 2005), C-PACE is based on the intuition that if we act according to the most optimistic scenario consistent with our observations, we'll either achieve good performance (if our optimistic assumption turns out to be true), or learn something new about the environment.\nC-PACE's optimism in the face of uncertainty comes in the way the estimate of Bellman's equation is calculated. Instead of performing a simple average over nearby samples, an additional distance-dependent term is added. This term accounts for the maximum difference between the value of the state-action each sample originated from, and the value of the state-action whose Bellman equation we are trying to estimate.", "publication_ref": ["b13"], "figure_ref": [], "table_ref": []}, {"heading": "Definitions and assumptions", "text": "For simplicity we assume that all rewards lie in [0, R max ] and 0 \u2264 Q max \u2264 Rmax 1\u2212\u03b3 . 3 V \u03c0 (s, T ) denotes the T step truncated, discounted value function for policy \u03c0. Assumption 3.1. We are given access to an approximate nearest neighbor algorithm 4 which when queried about the k nearest neighbors from a set of N points, returns one of cN sets of k distinct approximate nearest neighbors, where if d is the distance of the true k-th nearest neighbor, the approximate k-th nearest neighbor is no more than c m (d + c a ) away. 5 Definition 3.2. The approximate covering number N SA (LQ, d ) of a state-action space is the size of the largest minimal set C of state-action pairs, such that for any (s, a) reachable from the starting state(s), there exists\n(s , a ) \u2208 C such that LQc m [d(s, a, s , a ) + c a ] \u2264 d . Definition 3.3. A state-action pair (s i , a i ) is considered \"known\" if LQd(s i , a i , s k , a k ) \u2264 d , where (s k , a k )\nis its k-th approximate nearest neighbor in the sample set. Definition 3.4. The value x i of each sample of Bellman's equation is defined as the sampled reward plus the discounted, approximate, optimistic value of the sampled next state\u1e7c (s) = max aQ (s, a):\nx i = x (si,ai,ri,s i ) = r i + \u03b3\u1e7c (s i )\nDefinition 3.5. For state-action (s i , a i ), the approximate optimistic Q value function is defined as:\nQ(s i , a i ) = k j=1 min Q max , x j + LQd ij k (1\n)\nwhereQ max = R max + \u03b3Q max , d ij = d(s i , a i ,\ns j , a j ) and j = 1 to k are selected to be the k approximate nearest sampled neighbors to state-action (s i , a i ) with d ij \u2264 Qmax LQ . If fewer than k approximate neighbors exist within Qmax LQ , substitute the value of each missing neighbor withQ max . 6\nThe C-PACE algorithm Given the above, C-PACE can be summarized as follows: 1. From state s, select and perform an action according to arg max aQ (s, a).\n2. If (s, a) executed in step 1 is not known, add (s, a, r, s ) to the sample set, and find the fixed point solution toQ. 3. Go to step 1. We will not try to categorize C-PACE as a model-free or model-based algorithm. On the one hand, C-PACE integrates experience from all samples at every step, a trait commonly associated with model-based algorithms (Szita and Szepesv\u00e1ri 2010). On the other hand, it does not explicitly build a model, and its space requirements are less than what is typically required to build an accurate model, which have been defined as sufficient conditions for an algorithm to be considered model free by some authors (Strehl et al. 2006).\nThe rest of this section is devoted to proving that C-PACE explores efficiently. Intuitively, the proof is based on two key facts. The first is that at each step, C-PACE will either perform near optimally, or learn something new about the environment with high probability. The second is that there is only a finite number of things C-PACE can learn about the environment, so the number of suboptimal steps must be bounded.", "publication_ref": ["b16", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Basic lemmata", "text": "Lemma 3.6. (Lemma 4.5 in Kakade, Kearns, and Langford ( 2003)) All state-actions reachable from the starting state(s) will become known after adding at most kN SA (LQ, d ) samples.\nLemma 3.7. (Lemma 2 in Kearns and Singh (2002\n)) If T \u2265 1 1\u2212\u03b3 ln Rmax T , then |V \u03c0 (s, T ) \u2212 V \u03c0 (s)| \u2264 T 1\u2212\u03b3 .\nAlthough the following lemma was first stated for discrete state-action spaces, the proof readily extends to continuous state-action spaces if we allow for K to be an infinite set: Lemma 3.8. (Generalized Induced Inequality) (Lemma 8 in Strehl and Littman 2005) Let M be an MDP, K a set of state-action pairs, M an MDP equal to M on K (identical transition and reward functions), \u03c0 a policy, and T some positive integer. Let A M be the event that a state-action pair not in K is encountered in a trial generated by starting from state s 1 and following \u03c0 for T steps in M. Then:\nV \u03c0 M (s 1 , T ) \u2265 V \u03c0 M (s 1 , T ) \u2212 Q max P r(A M ) Lemma 3.9. (Lemma 56 in Li 2009) Let x 1 , x 2 , x 3 , \u2022 \u2022 \u2022 \u2208 B\nbe a sequence of m independent Bernoulli trials, each with a success probability at least \u00b5: E[x i ] \u2265 \u00b5, for some constant \u00b5 > 0. Then for any l \u2208 N and \u03b4 \u2208 (0, 1), with probability at least 1 \u2212 \u03b4, Proof. We will prove our claim by induction. All we need to prove is that B i Q 0 (s, a) \u2264 Q 0 (s, a) + i\u22121 j=0 \u03b3 j , and then take the limit as i \u2192 \u221e.\nx 1 + x 2 + \u2022 \u2022 \u2022 + x m \u2265 l if m \u2265 2 \u00b5 l + ln 1 \u03b4 . In\nThe base is given by hypothesis. Assuming that B i Q 0 (s, a) \u2264 Q 0 (s, a) + i\u22121 j=0 \u03b3 j , we'll prove that the inequality also holds for i + 1:\nB i+1 Q0(s, a) = BB i Q0(s, a) \u2264 s P (s |s, a) R(s, a, s ) + \u03b3 max a B i Q0(s , a ) \u2264 s P (s |s, a) R(s, a, s ) + \u03b3 max a Q0(s , a ) + i\u22121 j=0 \u03b3 j = s P (s |s, a) R(s, a, s ) + \u03b3 max a Q0(s , a ) + \u03b3 i\u22121 j=0 \u03b3 j \u2264 Q0(s, a) + + \u03b3 i\u22121 j=0 \u03b3 j = Q0(s, a) + i j=0 \u03b3 j\nIf we now take the limit as i \u2192 \u221e we have the original claim:\nlim i\u2192\u221e B i Q 0 (s, a) \u2264 Q 0 (s, a) + i\u22121 j=0 \u03b3 j \u2192 Q * (s, a) \u2264 Q 0 (s, a) + 1 \u2212 \u03b3\nLemma 3.11. (Second part of theorem 4.1 in Williams and Baird (1993)). Let = ||Q \u2212 BQ|| \u221e denote the Bellman error magnitude for Q, and V * \u2264 V Q . The return V \u03c0 from the greedy policy over Q satisfies:\n\u2200s \u2208 S, V \u03c0 (s) \u2265 V * (s) \u2212 1 \u2212 \u03b3 Theorem 3.12. Let \u2212 \u2265 0 and + \u2265 0 be constants such that: \u2200(s, a) \u2208 (S, A), \u2212 \u2212 \u2264 Q(s, a) \u2212 BQ(s, a) \u2264 + .\nThe return V \u03c0 from the greedy policy over Q satisfies:\n\u2200s \u2208 S, V \u03c0 (s) \u2265 V * (s) \u2212 \u2212 + + 1 \u2212 \u03b3 Proof.\nWe set Q (s, a) = Q(s, a) + \u2212 1\u2212\u03b3 , \u2200(s, a) \u2208 (S, A). It's easy to see that the performance achieved by the one step greedy policy \u03c0 over Q and \u03c0 over Q is the same: \nV \u03c0 (x) = V \u03c0 (x).", "publication_ref": ["b8", "b13", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Approximation", "text": "Given a Lipschitz continuous value function, the value of any state-action pair can be expressed in terms of any other state-action pair as Q(s j , a j ) = Q(s i , a i )+\u03be ij L Q d ij , where\nd ij = d(s i , a i , s j , a j ) and \u03be ij is a fixed but possibly un- known constant in [\u22121, 1]. For sample (s i , a i , r i , s i ), define: x (si,ai,ri,s i ),j = r i + \u03b3V (s i ) + \u03be ij L Q d ij .\nThen:\nE s i [x (si,ai,ri,s i ),j ] = E s i [r i + \u03b3V (s i )] + \u03be ij L Q d ij = Q(s i , a i ) + \u03be ij L Q d ij .\nConsider a state-action pair (s 0 , a 0 ) and its k approximate nearest neighbors (s i , a i ) for i = 1, . . . , k. We can arrive at an estimate of its value by averaging over the predicted value of all its neighbors (which may include itself if (s 0 , a 0 ) is a sampled state-action pair):Q(s 0 , a 0 ) = k i=1 x (s i ,a i ,r i ,s i ),0 k . Setting \u03be ij = 1 \u2200 i, j we arrive at the definition of the approximate optimistic value functionQ (Definition 3.5).\nIn the following, B denotes the (exact) Bellman operator, B denotes the approximate Bellman operator corresponding to the definition ofQ above, andB denotes the approximate Bellman operator defined by the right hand side of equation 1.\nThe Bellman error can be decomposed into two pieces: the maximum absolute overestimation and underestimation error s caused by using a finite number of neighbors, and the overestimation error caused by using neighbors at a nonzero distance from the point of interest d (remember that d is an input used at step 2 of the algorithm).\nThe following lemma helps bound the minimum number of neighbors k, required to guarantee a particular s with probability 1 \u2212 \u03b4: Lemma 3.13. If\nQ 2 max 2 s ln 2N SA (LQ, d ) \u03b4 \u2264 k \u2264 2N SA (LQ, d ) \u03b4 : \u2212 s \u2264BQ(s, a) \u2212 BQ(s, a) \u2264 s , w. p. 1 \u2212 \u03b4, \u2200(s, a).\nProof.Q is the (fixed) solution to the equations in Definition 3.5, andB differs from B in that it is the mean over k samples instead of the true expectation. Thus we can use Hoeffding's inequality to bound the difference between ap-plyingB and B toQ for any (s, a) (note that values inQ will always lie in [0,Q max ]):\n7 P (|BQ(s, a) \u2212 BQ(s, a)| \u2264 s ) \u2264 2e \u2212 2 2 s k Q 2 max .\nFrom the union bound, we have that the probability \u03b4 of the mean over k samples being more than s away in any of the kN SA (LQ, d ) possible combinations, is no more than the sum of the individual probabilities:\n\u03b4 \u2264 kN SA (LQ, d )2e \u2212 2 2 s k Q 2 max .\nSolving for k, we have that for a given probability of failure \u03b4 and error s , assuming k \u2264\n2N SA (LQ, d ) \u03b4\n, k needs to be at most:\nk =Q 2 max 2 s ln 2N SA (LQ, d ) \u03b4 .\nThe following lemma translates the result from Lemma 3.13 fromB toB for known states: Proof. Assuming Q is in a complete metric space, all we need to prove is thatB is a contraction in maximum norm. Suppose ||Q 1 \u2212 Q 2 || \u221e = . For any (s, a) we have:\nLemma 3.14. IfQ 2 max 2 s ln 2N SA (LQ, d ) \u03b4 \u2264 k \u2264 2N SA (LQ, d ) \u03b4 : \u2212 s \u2264Q(s,\nBQ 1 (s, a) = = 1 k k j=1 min Q max , r j + \u03b3 max a Q 1 (s , a ) + LQd ij \u2264 1 k k j=1 min Q max , r j + \u03b3 max a (Q 2 (s , a ) + ) + LQd ij \u2264 1 k k j=1 min Q max , r j + \u03b3 max a Q 2 (s , a ) + LQd ij + \u03b3 =BQ 2 (s, a) + \u03b3 \u21d2BQ 1 (s, a) \u2264BQ 2 (s, a) + \u03b3\nSimilarly we have thatBQ 2 (s, a) \u2264BQ 1 (s, a) + \u03b3 which completes our proof.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Efficient exploration", "text": "The following theorem is the main theorem of this section. It allows us to guarantee that the number of steps in which the performance of C-PACE is significantly worse than that of an optimal policy starting from the current state is at most log-linear in the covering number of the state-action space with probability 1 \u2212 \u03b4. Theorem 3.16. Let M be an MDP,\u03c0 t be the greedy policy overQ at time t, s t be the state at time t and\nQ 2 max 2 s ln 2N SA (LQ, d ) \u03b4 \u2264 k \u2264 2N SA (LQ, d ) \u03b4\n. For a trajectory of arbitrary length, with probability at least 1 \u2212 \u03b4, there will be at most:\n2Qmax K ln Rmax T kN SA (LQ, d ) + ln 2 \u03b4 time-steps t, where K = (1 \u2212 \u03b3)Q max P r(A M ), such that: 8 V\u03c0 t M (s t ) < V * M (s) \u2212 2 K + 2 T + d + 2 s 1 \u2212 \u03b3 .(2)\nProof. Let M be an MDP that is equal to M on all known state-action pairs. All other state-action pairs transition deterministically with reward R(s, a) =Q(s, a) to an absorbing state with reward 0. 9 Let A M be the event that\u03c0 t encounters an unknown state-action in T steps. At every time step, exactly one of two things can happen:\n1. P r(A M ) \u2265 K Qmax(1\u2212\u03b3)\n. From Lemma 3.9, with probability 1 \u2212 \u03b4 2 , this can happen to no more than \nV\u03c0 t M (s t ) \u2265 V\u03c0 t M (s t , T ) \u2265 V\u03c0 t M (s t , T ) \u2212 K 1 \u2212 \u03b3 \u2265 V\u03c0 t M (s t ) \u2212 K + T 1 \u2212 \u03b3 \u2265 V * M (s) \u2212 K + T + d + 2 s 1 \u2212 \u03b3 \u2265 V * M (s, T ) \u2212 K + T + d + 2 s 1 \u2212 \u03b3 \u2265 V * M (s, T ) \u2212 2 K + T + d + 2 s 1 \u2212 \u03b3 \u2265 V * M (s) \u2212 2 K + 2 T + d + 2 s 1 \u2212 \u03b3 ,\nwhere step 1 made use of the fact that rewards are assumed to be non-negative, step 2 made use of Lemma 3.8, step 3 made use of Lemma 3.7, step 4 made use of Lemma 3.14, theorem 3.12 and the definition of M , step 5 made use of the fact that rewards are assumed to be non-negative, step 6 made use of Lemma 3.8 and step 7 made use of Lemma 3.7.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "PAC optimal exploration", "text": "During the last decade, there has been a significant body of work addressing PAC optimal exploration. However, the vast majority of papers only address the discrete case, providing incremental improvements on complexity bounds. The best of these bounds offer log-linear dependence on the size of the state-action space. Unfortunately, the huge constants associated with these bounds preclude the use of the associated algorithms in non-trivially sized, discrete MDPs, which coupled with their inability to handle continuous spaces makes them inapplicable in challenging, realistic domains. Exploration in Metric State Spaces (Kakade, Kearns, and Langford 2003) is the only example we are aware of in the PAC MDP literature which tries to address exploration in continuous state spaces. While definitely a step in the right direction, the paper did not offer a concrete algorithm. Instead, an efficient black box approximate planner and local approximate modeling algorithm are assumed to exist. Unfortunately some of the assumptions regarding the abilities of these black box algorithms are overly optimistic (e.g. good model approximation everywhere not just with some probability 1 \u2212 \u03b4). Nouri and Littman (2008) and Jong and Stone (2007) present interesting algorithms for exploration in continuous state spaces, but stop short of providing PAC-optimal bounds for them. For cases where the user can come up with a good set of features but not an exploration strategy, Strehl and Littman (2008) provide an algorithm that can explore in polynomial time in environments whose dynamics can be accurately modeled by linear regression. Similarly, Brunskill et al. (2009) also use parametric models, but allow for the state-action space to be partitioned into multiple models, rather than a monolithic one.\nAn interesting algorithm for discrete MDPs which has a number of similarities with C-PACE is Model-Based Interval Estimation (Strehl and Littman 2005). Similarly to C-PACE, Model-Based Interval Estimation integrates new samples as soon as they are available, instead of integrating in batches. In addition, like C-PACE it always acts assuming the best of all possible worlds consistent with its hypothesis is true, achieving exploration in an implicit manner, rather than explicitly choosing to explore or exploit at each timestep.\nDelayed Q-learning (Strehl et al. 2006) was the first model-free algorithm for PAC optimal reinforcement learning with log-linear dependence on the number of stateactions. Its bounds have since been improved upon by a model based algorithm (Szita and Szepesv\u00e1ri 2010).", "publication_ref": ["b6", "b12", "b5", "b14", "b1", "b13", "b15", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Other forms of exploration", "text": "While PAC optimal exploration is arguably one of the most theoretically interesting forms of exploration, other forms of exploration have been proposed and used over the years.\nOne of the simplest and most commonly used approaches to exploration is the so called -greedy family of algorithms. Although they are guaranteed to explore the entire stateaction space eventually, the time required may be exponential in the size of the state-action space.\nAnother approach to exploration is that of Bayesian or PAC-Bayesian exploration (Kolter and Ng 2009). Bayesian exploration tries to optimize for a very different goal from that of typical PAC optimal methods. Its assumption is that all that matters is the cumulative discounted reward from the current state, and as such it chooses to explore unknown state-actions only when those state-actions are expected to perform better that the known state-actions. This leads to a significantly more myopic algorithm, which explores far less than other PAC optimal methods. Such an approach would be appropriate in situations where its assumptions are true, which, for example, may include the testing phase in a learning scenario with both learning and testing phases.", "publication_ref": ["b9"], "figure_ref": [], "table_ref": []}, {"heading": "Simulated HIV treatment", "text": "The simulated HIV treatment problem (Adams et al. 2004), introduced to reinforcement learning by Ernst et al. (2006), is a six dimensional continuous state space, two dimensional discrete action space problem, modeled after clinical data. The state space of the process, abbreviated as (T 1 , T 2 , T * 1 , T * 2 , V, E) measures the concentrations of healthy CD4+ T-lymphocytes, healthy macrophages, infected CD4+ T-lymphocytes, infected macrophages, free virus particles and HIV-specific cytotoxic T-cells, updated every five days. The action space of the process is comprised of the administration of two drugs over the next five days, a reverse transcriptase inhibitor and a protease inhibitor.\nThe reward of the process at time t is: 1,000E t \u2212 0.1V t \u2212 20,000 2 1t \u2212 2,000 2 2t . 1t and 2t are equal to 0.7 and 0.3 respectively when the reverse transcriptase inhibitor and protease inhibitor are administered and 0 otherwise. The agent is rewarded for having a large number of HIV-specific cytotoxic T-cells, while it is penalized for having a large number of free virus particles, and for using each of the drugs, as these drugs have severe, adverse side effects.\nIn the absence of treatment, the system of ordinary differential equations exhibits three physical equilibrium points (and several non physical ones for which one or more state variables are negative): The uninfected state S = (10 6 , 3198, 0, 0, 0, 10), which is an unstable equilibrium point (even slight perturbations lead away from this equilibrium), the \"healthy\" locally stable equilibrium point, S = (967839,621,76,6,415,353108) and the \"non-healthy\" locally stable equilibrium point, S = (163573,5,11945,46,63919,24). Simulations show that the basin of attraction of the healthy steady-state is relatively small compared to the one of the non-healthy steady state, and in the absence of drugs, perturbation of the uninfected steady state by adding as little as one single particle of virus per ml of blood plasma leads to asymptotic convergence towards the non-healthy steady state. For further details readers can consult earlier work on this domain (Ernst et al. 2006).\nIn all experiments below, the Lipschitz constant was set to 10 3 , the distance function was set to be the max-norm over the state-action space, and k = 1 (the model for this domain is deterministic). 10 At the start of every episode, the domain was initialized to the \"non-healthy\" steady state.\nA policy which selects actions uniformly at random, often used to obtain samples from typical RL domains was unable to generate samples anywhere but in the vicinity of the \"non-healthy\" steady state. Additionally several discretization based PAC-optimal exploration algorithms were unable to yield meaningful performance improvement after several days of computation, for various discretization thresholds. As such, we consider this domain a good testbed for an exploration algorithm. 11 Figure 1 (a) shows total accumulated discounted reward achieved by C-PACE as a function of training episodes. Although the performance is very poor at first, C-PACE is able to find a good policy in less than 500 episodes.\nFigure 1 (b) shows the policy followed at the 1000th episode while figures 1 (c) through (h) show the path through the state space for the the 1000th episode. By repeatedly cycling the drugs, the agent is able to take the patient from the \"non-healthy\" steady state to the \"healthy\" steady state, where the number of HIV-specific cytotoxic T-cells is high, the number of free virus particles is low, and drugs are no longer required to manage the disease.\nSince our choice of reward function and dynamics are the same as the ones used in earlier work, our results are directly comparable. Of course one would be comparing the combination of human effort to find a good exploration schedule and a \"best estimate\" algorithm (Ernst et al. 2006), or a pure planning algorithm using the model (Busoniu et al. 2011), versus the fully automated exploration of C-PACE, which needs to convince itself that no better policy exists before it starts to follow the same trajectory to the goal consistently. To the best of our knowledge, this is the first time this problem (or any other problem of comparable difficulty) has been tackled by a PAC optimal exploration algorithm.", "publication_ref": ["b0", "b3", "b3", "b3", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Future work", "text": "While these simulation results are very encouraging, there are still aspects of the real world as they would apply to the HIV treatment and many other domains, that we have not yet considered.\nFor the HIV treatment problem the number of exploration episodes needed to arrive at a good treatment strategy are fairly modest, even by real world standards. However, in our simulations, the trials were done in succession. Considering that for the HIV domain each episode lasts two years, arriving at a good policy in this way would require several centuries. An important step forward would be to develop algorithms that can exploit multiple parallel executions on the same MDP, integrating information and choosing actions to maximize global knowledge and performance. Another aspect of the real world which our exploration algorithm did not take into account is that the true cost of exploration is more than just the number of steps for which we perform worse than near optimally. This is obviously true when dealing with patients, but it is also true with mechanical systems. Even if, for example our budget allows us to crash a number of autonomous helicopters while learning to fly, that number will be prohibitively small compared to most PAC bounds. Consequently methods for safe exploration are of great real world interest (Moldovan and Abbeel 2012).\nOur analysis applies to both discrete and continuous/multidimensional action spaces. Unfortunately, selecting the action which maximizesQ over large/infinite sets of actions is far from trivial, and approximate methods could break any PAC guarantees. An effective method for efficient action selection which maintains PAC guarantees would extend the applicability of C-PACE to many interesting problems.\nFinally, the central assumption made by C-PACE is that there exists some distance function in which the value function is smooth (Lipschitz continuous in our analysis). While it is reasonable to expect that such a distance function exists, many obvious distance functions that a user might try to use may not satisfy this requirement, or may have a large Lipschitz constant. Automatic discovery of suitable distance functions is an important next step.", "publication_ref": ["b11"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper, we presented C-PACE, the first practical, model-free, PAC-optimal algorithm for exploration in continuous spaces using real trajectories for learning. C-PACE requires only a guess of the Lipschitz constant of the Qfunction for the chosen distance metric and the maximum tolerable error due to sampling as input. Using these and an approximation to the Bellman operator, it provides a straightforward and easily implemented exploration algorithm with strong performance guarantees. We have demonstrated the ability of C-PACE to explore and learn good policies for a challenging, six-dimensional problem, the first time a PAC-optimal algorithm has been applied to a problem of such size and difficulty.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We would like to thank Vincent Conitzer, Mauro Maggioni and the anonymous reviewers for helpful comments and suggestions. This work was supported by NSF IIS-1147641 and NSF IIS-1218931. Opinions, findings, conclusions or recommendations herein are those of the authors and not necessarily those of NSF.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Dynamic multidrug therapies for HIV: Optimal and STI control approaches", "journal": "In Mathematical Biosciences and Engineering", "year": "2004", "authors": "B Adams; H Banks; H.-D Kwon; H Tran"}, {"ref_id": "b1", "title": "Provably efficient learning with typed parametric models", "journal": "The Journal of Machine Learning Research", "year": "2009", "authors": "E Brunskill; B Leffler; L Li; M Littman; N Roy"}, {"ref_id": "b2", "title": "Optimistic planning for sparsely stochastic systems", "journal": "", "year": "2011", "authors": "L Busoniu; R Munos; B D Schutter; R Babuska"}, {"ref_id": "b3", "title": "Clinical data based optimal STI strategies for HIV", "journal": "", "year": "2006", "authors": "D Ernst; G.-B Stan; J Goncalves; L Wehenkel"}, {"ref_id": "b4", "title": "Machine Learning Conference of Belgium and The Netherlands (Benelearn)", "journal": "", "year": "", "authors": ""}, {"ref_id": "b5", "title": "Model-based exploration in continuous state spaces. Abstraction, Reformulation, and Approximation", "journal": "", "year": "2007", "authors": "N Jong; P Stone"}, {"ref_id": "b6", "title": "Exploration in metric state spaces", "journal": "", "year": "2003", "authors": "S Kakade; M J Kearns; J Langford"}, {"ref_id": "b7", "title": "On the sample complexity of reinforcement learning", "journal": "", "year": "2003", "authors": "S M Kakade"}, {"ref_id": "b8", "title": "Near-optimal reinforcement learning in polynomial time", "journal": "Machine Learning", "year": "2002", "authors": "M J Kearns; S P Singh"}, {"ref_id": "b9", "title": "Near-bayesian exploration in polynomial time", "journal": "", "year": "2009", "authors": "J Z Kolter; A Y Ng"}, {"ref_id": "b10", "title": "A unifying framework for computational reinforcement learning theory", "journal": "", "year": "2009", "authors": "L Li"}, {"ref_id": "b11", "title": "Safe exploration in Markov decision processes", "journal": "", "year": "2012", "authors": "T M Moldovan; P Abbeel"}, {"ref_id": "b12", "title": "Multi-resolution exploration in continuous spaces", "journal": "Advances in neural information processing systems", "year": "2008", "authors": "A Nouri; M Littman"}, {"ref_id": "b13", "title": "A theoretical analysis of model-based interval estimation", "journal": "ACM", "year": "2005", "authors": "A L Strehl; M L Littman"}, {"ref_id": "b14", "title": "Online linear regression and its application to model-based reinforcement learning", "journal": "Advances in Neural Information Processing Systems", "year": "2008", "authors": "A Strehl; M Littman"}, {"ref_id": "b15", "title": "PAC model-free reinforcement learning", "journal": "", "year": "2006", "authors": "A L Strehl; L Li; E Wiewiora; J Langford; M L Littman"}, {"ref_id": "b16", "title": "Model-based reinforcement learning with nearly tight exploration complexity bounds", "journal": "", "year": "2010", "authors": "I Szita; C Szepesv\u00e1ri"}, {"ref_id": "b17", "title": "Tight performance bounds on greedy policies based on imperfect value functions", "journal": "", "year": "1993", "authors": "R Williams; L Baird"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "the following, B denotes the (exact) Bellman operator. Lemma 3.10. Let \u2265 0 be a constant such that: \u2200(s, a) \u2208 (S, A), BQ(s, a) \u2264 Q(s, a) + . Then: \u2200(s, a) \u2208 (S, A), Q * (s, a) \u2264 Q(s, a) + 1 \u2212 \u03b3", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "From lemma 3.10 we have \u2200(s, a) \u2208 (S, A), Q (s, a) \u2265 Q * (s, a). \u2200(s, a) \u2208 (S, A), BQ (s, a) = s P (s |s, a) R(s, a, s ) + \u03b3 max a Q(s , a ) + \u2212 1 \u2212 \u03b3 = BQ(s, a) + \u03b3 \u2212 1 \u2212 \u03b3 \u21d2 Q (s, a) \u2212 BQ (s, a) = Q(s, a) \u2212 BQ(s, a) + \u2212 \u2264 \u2212 + + From lemma 3.11 we have \u2200s \u2208 S, V \u03c0 (s) \u2265 V * (s) \u2212 \u2212 + + 1\u2212\u03b3 .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "a) \u2212 BQ(s, a) \u2264 s + d , w. p. 1 \u2212 \u03b4, for all known (s, a). Proof. Follows directly from Lemma 3.13, the fact that 0 \u2264 BQ(s, a) \u2212BQ(s, a) \u2264 d for all known (s, a), and the fact that sinceQ is fixed underB,BQ(s, a) =Q(s, a), \u2200(s, a). Lemma 3.15. The set of equations in definition 3.5 has a unique fixed point.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "length T before all stateactions become known. Setting T = 1 1\u2212\u03b3 ln Rmax T we have that this can happen to no more than 2Qmax K ln Rmax T kN SA (LQ, d ) + ln 2 \u03b4 time-steps. 2. P r(A M ) < K Qmax(1\u2212\u03b3) for state s at time t. With probability 1 \u2212 \u03b4 2 :", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 1: (a) Total accumulated discounted reward per episode achieved by C-PACE versus training episodes. (b) Policy for the the 1000th episode: use of the reverse transcriptase inhibitor (blue) and protease inhibitor (red). Path through the state space for the the 1000th episode: (c) number of healthy CD4+ T-lymphocytes, (d) healthy macrophages, (e) infected CD4+ T-lymphocytes, (f) infected macrophages, (g) free virus particles, (h) HIV-specific cytotoxic T-cells.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "(s , a ) \u2208 C such that LQc m [d(s, a, s , a ) + c a ] \u2264 d . Definition 3.3. A state-action pair (s i , a i ) is considered \"known\" if LQd(s i , a i , s k , a k ) \u2264 d , where (s k , a k )", "formula_coordinates": [2.0, 319.5, 331.82, 238.5, 36.58]}, {"formula_id": "formula_1", "formula_text": "x i = x (si,ai,ri,s i ) = r i + \u03b3\u1e7c (s i )", "formula_coordinates": [2.0, 362.43, 436.73, 132.72, 11.84]}, {"formula_id": "formula_2", "formula_text": "Q(s i , a i ) = k j=1 min Q max , x j + LQd ij k (1", "formula_coordinates": [2.0, 330.02, 481.39, 224.11, 28.9]}, {"formula_id": "formula_3", "formula_text": ")", "formula_coordinates": [2.0, 554.13, 495.04, 3.87, 8.64]}, {"formula_id": "formula_4", "formula_text": "whereQ max = R max + \u03b3Q max , d ij = d(s i , a i ,", "formula_coordinates": [2.0, 319.5, 516.53, 192.63, 9.65]}, {"formula_id": "formula_5", "formula_text": ")) If T \u2265 1 1\u2212\u03b3 ln Rmax T , then |V \u03c0 (s, T ) \u2212 V \u03c0 (s)| \u2264 T 1\u2212\u03b3 .", "formula_coordinates": [3.0, 54.0, 414.17, 217.85, 23.03]}, {"formula_id": "formula_6", "formula_text": "V \u03c0 M (s 1 , T ) \u2265 V \u03c0 M (s 1 , T ) \u2212 Q max P r(A M ) Lemma 3.9. (Lemma 56 in Li 2009) Let x 1 , x 2 , x 3 , \u2022 \u2022 \u2022 \u2208 B", "formula_coordinates": [3.0, 54.0, 559.09, 238.2, 29.13]}, {"formula_id": "formula_7", "formula_text": "x 1 + x 2 + \u2022 \u2022 \u2022 + x m \u2265 l if m \u2265 2 \u00b5 l + ln 1 \u03b4 . In", "formula_coordinates": [3.0, 63.96, 621.52, 228.54, 27.21]}, {"formula_id": "formula_8", "formula_text": "B i+1 Q0(s, a) = BB i Q0(s, a) \u2264 s P (s |s, a) R(s, a, s ) + \u03b3 max a B i Q0(s , a ) \u2264 s P (s |s, a) R(s, a, s ) + \u03b3 max a Q0(s , a ) + i\u22121 j=0 \u03b3 j = s P (s |s, a) R(s, a, s ) + \u03b3 max a Q0(s , a ) + \u03b3 i\u22121 j=0 \u03b3 j \u2264 Q0(s, a) + + \u03b3 i\u22121 j=0 \u03b3 j = Q0(s, a) + i j=0 \u03b3 j", "formula_coordinates": [3.0, 335.06, 134.78, 201.89, 245.77]}, {"formula_id": "formula_9", "formula_text": "lim i\u2192\u221e B i Q 0 (s, a) \u2264 Q 0 (s, a) + i\u22121 j=0 \u03b3 j \u2192 Q * (s, a) \u2264 Q 0 (s, a) + 1 \u2212 \u03b3", "formula_coordinates": [3.0, 328.46, 417.56, 220.57, 55.1]}, {"formula_id": "formula_10", "formula_text": "\u2200s \u2208 S, V \u03c0 (s) \u2265 V * (s) \u2212 1 \u2212 \u03b3 Theorem 3.12. Let \u2212 \u2265 0 and + \u2265 0 be constants such that: \u2200(s, a) \u2208 (S, A), \u2212 \u2212 \u2264 Q(s, a) \u2212 BQ(s, a) \u2264 + .", "formula_coordinates": [3.0, 319.5, 554.27, 238.5, 55.59]}, {"formula_id": "formula_11", "formula_text": "\u2200s \u2208 S, V \u03c0 (s) \u2265 V * (s) \u2212 \u2212 + + 1 \u2212 \u03b3 Proof.", "formula_coordinates": [3.0, 319.5, 627.69, 181.8, 41.15]}, {"formula_id": "formula_12", "formula_text": "V \u03c0 (x) = V \u03c0 (x).", "formula_coordinates": [3.0, 319.5, 693.62, 78.01, 10.31]}, {"formula_id": "formula_13", "formula_text": "d ij = d(s i , a i , s j , a j ) and \u03be ij is a fixed but possibly un- known constant in [\u22121, 1]. For sample (s i , a i , r i , s i ), define: x (si,ai,ri,s i ),j = r i + \u03b3V (s i ) + \u03be ij L Q d ij .", "formula_coordinates": [4.0, 54.0, 285.97, 238.51, 41.0]}, {"formula_id": "formula_14", "formula_text": "E s i [x (si,ai,ri,s i ),j ] = E s i [r i + \u03b3V (s i )] + \u03be ij L Q d ij = Q(s i , a i ) + \u03be ij L Q d ij .", "formula_coordinates": [4.0, 62.85, 351.53, 219.89, 25.18]}, {"formula_id": "formula_15", "formula_text": "Q 2 max 2 s ln 2N SA (LQ, d ) \u03b4 \u2264 k \u2264 2N SA (LQ, d ) \u03b4 : \u2212 s \u2264BQ(s, a) \u2212 BQ(s, a) \u2264 s , w. p. 1 \u2212 \u03b4, \u2200(s, a).", "formula_coordinates": [4.0, 54.0, 640.25, 222.1, 31.23]}, {"formula_id": "formula_16", "formula_text": "7 P (|BQ(s, a) \u2212 BQ(s, a)| \u2264 s ) \u2264 2e \u2212 2 2 s k Q 2 max .", "formula_coordinates": [4.0, 319.5, 92.01, 183.73, 30.46]}, {"formula_id": "formula_17", "formula_text": "\u03b4 \u2264 kN SA (LQ, d )2e \u2212 2 2 s k Q 2 max .", "formula_coordinates": [4.0, 319.5, 168.08, 118.01, 19.0]}, {"formula_id": "formula_18", "formula_text": "2N SA (LQ, d ) \u03b4", "formula_coordinates": [4.0, 460.33, 200.58, 48.66, 15.85]}, {"formula_id": "formula_19", "formula_text": "k =Q 2 max 2 s ln 2N SA (LQ, d ) \u03b4 .", "formula_coordinates": [4.0, 365.72, 216.13, 117.43, 17.87]}, {"formula_id": "formula_20", "formula_text": "Lemma 3.14. IfQ 2 max 2 s ln 2N SA (LQ, d ) \u03b4 \u2264 k \u2264 2N SA (LQ, d ) \u03b4 : \u2212 s \u2264Q(s,", "formula_coordinates": [4.0, 319.5, 273.62, 191.16, 39.04]}, {"formula_id": "formula_21", "formula_text": "BQ 1 (s, a) = = 1 k k j=1 min Q max , r j + \u03b3 max a Q 1 (s , a ) + LQd ij \u2264 1 k k j=1 min Q max , r j + \u03b3 max a (Q 2 (s , a ) + ) + LQd ij \u2264 1 k k j=1 min Q max , r j + \u03b3 max a Q 2 (s , a ) + LQd ij + \u03b3 =BQ 2 (s, a) + \u03b3 \u21d2BQ 1 (s, a) \u2264BQ 2 (s, a) + \u03b3", "formula_coordinates": [4.0, 319.5, 464.07, 224.47, 108.69]}, {"formula_id": "formula_22", "formula_text": "Q 2 max 2 s ln 2N SA (LQ, d ) \u03b4 \u2264 k \u2264 2N SA (LQ, d ) \u03b4", "formula_coordinates": [5.0, 55.2, 161.8, 179.71, 17.87]}, {"formula_id": "formula_23", "formula_text": "2Qmax K ln Rmax T kN SA (LQ, d ) + ln 2 \u03b4 time-steps t, where K = (1 \u2212 \u03b3)Q max P r(A M ), such that: 8 V\u03c0 t M (s t ) < V * M (s) \u2212 2 K + 2 T + d + 2 s 1 \u2212 \u03b3 .(2)", "formula_coordinates": [5.0, 54.0, 192.97, 443.54, 55.47]}, {"formula_id": "formula_24", "formula_text": "1. P r(A M ) \u2265 K Qmax(1\u2212\u03b3)", "formula_coordinates": [5.0, 51.51, 328.22, 122.47, 11.59]}, {"formula_id": "formula_25", "formula_text": "V\u03c0 t M (s t ) \u2265 V\u03c0 t M (s t , T ) \u2265 V\u03c0 t M (s t , T ) \u2212 K 1 \u2212 \u03b3 \u2265 V\u03c0 t M (s t ) \u2212 K + T 1 \u2212 \u03b3 \u2265 V * M (s) \u2212 K + T + d + 2 s 1 \u2212 \u03b3 \u2265 V * M (s, T ) \u2212 K + T + d + 2 s 1 \u2212 \u03b3 \u2265 V * M (s, T ) \u2212 2 K + T + d + 2 s 1 \u2212 \u03b3 \u2265 V * M (s) \u2212 2 K + 2 T + d + 2 s 1 \u2212 \u03b3 ,", "formula_coordinates": [5.0, 78.79, 462.73, 197.2, 162.51]}], "doi": ""}