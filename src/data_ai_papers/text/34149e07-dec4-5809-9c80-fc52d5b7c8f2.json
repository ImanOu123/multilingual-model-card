{"title": "Multiple Kernel Learning, Conic Duality, and the SMO Algorithm", "authors": "Francis R Bach; Gert R G Lanckriet; Michael I Jordan", "pub_date": "", "abstract": "While classical kernel-based classifiers are based on a single kernel, in practice it is often desirable to base classifiers on combinations of multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for the support vector machine (SVM), and showed that the optimization of the coefficients of such a combination reduces to a convex optimization problem known as a quadratically-constrained quadratic program (QCQP). Unfortunately, current convex optimization toolboxes can solve this problem only for a small number of kernels and a small number of data points; moreover, the sequential minimal optimization (SMO) techniques that are essential in large-scale implementations of the SVM cannot be applied because the cost function is non-differentiable. We propose a novel dual formulation of the QCQP as a second-order cone programming problem, and show how to exploit the technique of Moreau-Yosida regularization to yield a formulation to which SMO techniques can be applied. We present experimental results that show that our SMO-based algorithm is significantly more efficient than the general-purpose interior point methods available in current optimization toolboxes.", "sections": [{"heading": "Introduction", "text": "One of the major reasons for the rise to prominence of the support vector machine (SVM) is its ability to cast nonlinear classification as a convex optimization problem, in particular a quadratic program (QP). Con-vexity implies that the solution is unique and brings a suite of standard numerical software to bear in finding the solution. Convexity alone, however, does not imply that the available algorithms scale well to problems of interest. Indeed, off-the-shelf algorithms do not suffice in large-scale applications of the SVM, and a second major reason for the rise to prominence of the SVM is the development of special-purpose algorithms for solving the QP (Platt, 1998;Joachims, 1998;Keerthi et al., 2001).\nRecent developments in the literature on the SVM and other kernel methods have emphasized the need to consider multiple kernels, or parameterizations of kernels, and not a single fixed kernel. This provides needed flexibility and also reflects the fact that practical learning problems often involve multiple, heterogeneous data sources. While this so-called \"multiple kernel learning\" problem can in principle be solved via cross-validation, several recent papers have focused on more efficient methods for kernel learning (Chapelle et al., 2002;Grandvalet & Canu, 2003;Lanckriet et al., 2004;Ong et al., 2003). In this paper we focus on the framework proposed by Lanckriet et al. (2004), which involves joint optimization of the coefficients in a conic combination of kernel matrices and the coefficients of a discriminative classifier. In the SVM setting, this problem turns out to again be a convex optimization problem-a quadratically-constrained quadratic program (QCQP). This problem is more challenging than a QP, but it can also be solved in principle by generalpurpose optimization toolboxes such as Mosek (Andersen & Andersen, 2000). Again, however, this existing algorithmic solution suffices only for small problems (small numbers of kernels and data points), and improved algorithmic solutions akin to sequential minimization optimization (SMO) are needed.\nWhile the multiple kernel learning problem is convex, it is also non-smooth-it can be cast as the minimization of a non-differentiable function subject to linear constraints (see Section 3.1). Unfortunately, as is well known in the non-smooth optimization literature, this means that simple local descent algorithms such as SMO may fail to converge or may converge to incorrect values (Bertsekas, 1995). Indeed, in preliminary attempts to solve the QCQP using SMO we ran into exactly these convergence problems.\nOne class of solutions to non-smooth optimization problems involves constructing a smooth approximate problem out of a non-smooth problem. In particular, Moreau-Yosida (MY) regularization is an effective general solution methodology that is based on inf-convolution (Lemarechal & Sagastizabal, 1997). It can be viewed in terms of the dual problem as simply adding a quadratic regularization term to the dual objective function. Unfortunately, in our setting, this creates a new difficulty-we lose the sparsity that makes the SVM amenable to SMO optimization. In particular, the QCQP formulation of Lanckriet et al. (2004) does not lead to an MY-regularized problem that can be solved efficiently by SMO techniques.\nIn this paper we show how these problems can be resolved by considering a novel dual formulation of the QCQP as a second-order cone programming (SOCP) problem. This new formulation is of interest on its own merit, because of various connections to existing algorithms. In particular, it is closely related to the classical maximum margin formulation of the SVM, differing only by the choice of the norm of the inverse margin. Moreover, the KKT conditions arising in the new formulation not only lead to support vectors as in the classical SVM, but also to a dual notion of \"support kernels\"-those kernels that are active in the conic combination. We thus refer to the new formulation as the support kernel machine (SKM).\nAs we will show, the conic dual problem defining the SKM is exactly the multiple kernel learning problem of Lanckriet et al. (2004). 1 Moreover, given this new formulation, we can design a Moreau-Yosida regularization which preserves the sparse SVM structure, and therefore we can apply SMO techniques.\nMaking this circle of ideas precise requires a number of tools from convex analysis. In particular, Section 3 defines appropriate approximate optimality conditions for the SKM in terms of subdifferentials and approximate subdifferentials. These conditions are then used in Section 4 in the design of an MY regularization for the SKM and an SMO-based algorithm. We present the results of numerical experiments with the new method in Section 5.", "publication_ref": ["b12", "b6", "b7", "b4", "b5", "b8", "b11", "b8", "b0", "b1", "b9", "b8", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Learning the kernel matrix", "text": "In this section, we (1) begin with a brief review of the multiple kernel learning problem of Lanckriet et al. (2004), (2) introduce the support kernel machine (SKM), and (3) show that the dual of the SKM is equivalent to the multiple kernel learning primal.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "Multiple kernel learning problem", "text": "In the multiple kernel learning problem, we assume that we are given n data points (x i , y i ), where x i \u2208 X for some input space X , and where y i \u2208 {\u22121, 1}. We also assume that we are given m matrices K j \u2208 R n\u00d7n , which are assumed to be symmetric positive semidefinite (and might or might not be obtained from evaluating a kernel function on the data {x i }). We consider the problem of learning the best linear combination m j=1 \u03b7 j K j of the kernels K j with nonnegative coefficients \u03b7 j 0 and with a trace constraint tr m j=1 \u03b7 j K j = m j=1 \u03b7 j tr K j = c, where c > 0 is fixed. Lanckriet et al. (2004) show that this setup yields the following optimization problem:\nmin \u03b6 \u2212 2e \u03b1 (L) w.r.t. \u03b6 \u2208 R, \u03b1 \u2208 R n s.t. 0 \u03b1 C, \u03b1 y = 0 \u03b1 D(y)K j D(y)\u03b1\ntr Kj c \u03b6, j \u2208 {1, . . . , m}, where D(y) is the diagonal matrix with diagonal y, e \u2208 R n the vector of all ones, and C a positive constant. The coefficients \u03b7 j are recovered as Lagrange multipliers for the constraints \u03b1 D(y)K j D(y)\u03b1 tr Kj c \u03b6.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "Support kernel machine", "text": "We now introduce a novel classification algorithm that we refer to as the \"support kernel machine\" (SKM). It will be motivated as a block-based variant of the SVM and related margin-based classification algorithms. But our underlying motivation is the fact that the dual of the SKM is exactly the problem (L). We establish this equivalence in the following section.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Linear classification", "text": "In this section we let X = R k . We also assume we are given a decomposition of R k as a product of m blocks: R k = R k1 \u00d7 \u2022 \u2022 \u2022 \u00d7 R km , so that each data point x i can be decomposed into m block components, i.e. x i = (x 1i , . . . , x mi ), where each x ji is in general a vector.\nThe goal is to find a linear classifier of the form y = sign(w x + b) where w has the same block decomposition w = (w 1 , . . . , w m ) \u2208 R k1+\u2022\u2022\u2022+km . In the spirit of the soft margin SVM, we achieve this by minimizing a linear combination of the inverse of the margin and the training error. Various norms can be used to combine the two terms, and indeed many different algorithms have been explored for various combinations of 1 -norms and 2 -norms. In this paper, our goal is to encourage the sparsity of the vector w at the level of blocks; in particular, we want most of its (multivariate) components w i to be zero. A natural way to achieve this is to penalize the 1 -norm of w. Since w is defined by blocks, we minimize the square of a weighted block 1 -norm, ( m j=1 d j ||w j || 2 ) 2 , where within every block, an 2 -norm is used. Note that a standard 2 -based SVM is obtained if we minimize the square of a block 2 -norm, m j=1 ||w j || 2 2 , which corresponds to ||w|| 2 2 , i.e., ignoring the block structure. On the other hand, if m = k and d j = 1, we minimize the square of the 1 -norm of w, which is very similar to the LP-SVM proposed by Bradley and Mangasarian (1998). The primal problem for the SKM is thus:\nmin 1 2 ( m j=1 d j ||w j || 2 ) 2 + C n i=1 \u03be i (P ) w.r.t. w \u2208 R k1 \u00d7 \u2022 \u2022 \u2022 \u00d7 R km , \u03be \u2208 R n + , b \u2208 R s.t. y i ( j w j x ji + b) 1 \u2212 \u03be i , \u2200i \u2208 {1, . . . , n}.", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}, {"heading": "Conic duality and optimality conditions", "text": "For a given optimization problem there are many ways of deriving a dual problem. In our particular case, we treat problem (P ) as a second-order cone program (SOCP) (Lobo et al., 1998), which yields the following dual (see Appendix A for the derivation):\nmin 1 2 \u03b3 2 \u2212 \u03b1 e (D) w.r.t. \u03b3 \u2208 R, \u03b1 \u2208 R n s.t. 0 \u03b1 C, \u03b1 y = 0 || i \u03b1 i y i x ji || 2 d j \u03b3, \u2200j \u2208 {1, . . . , m}.\nIn addition, the Karush-Kuhn-Tucker (KKT) optimality conditions give the following complementary slackness equations:\n(a) \u03b1 i (y i ( j w j x ji + b) \u2212 1 + \u03be i ) = 0, \u2200i (b) (C \u2212 \u03b1 i )\u03be i = 0, \u2200i (c) wj ||wj ||2 \u2212 i \u03b1iyixji dj \u03b3 = 0, \u2200j (d) \u03b3( d j t j \u2212 \u03b3) = 0.\nEquations (a) and (b) are the same as in the classical SVM, where they define the notion of a \"support vector.\" That is, at the optimum, we can divide the data points into three disjoint sets: I 0 = {i, \u03b1 i = 0}, I M = {i, \u03b1 i \u2208 (0, C)}, and I C = {i, \u03b1 i = C}, such that points belonging to I 0 are correctly classified points not on the margin and such that \u03be i = 0; points in I M are correctly classified points on the margin such that \u03be i = 0 and y i ( j w j x ji + b) = 1, and points in I C are points on the \"wrong\" side of the margin for which \u03be i 0 (incorrectly classified if \u03be i 1) and y i ( j w j x ji + b) = 1 \u2212 \u03be i . The points whose indices i are in I M or I C are the support vectors.\nWhile the KKT conditions (a) and (b) refer to the index i over data points, the KKT conditions (c) and (d) refer to the index j over components of the input vector. These conditions thus imply a form of sparsity not over data points but over \"input dimensions.\" Indeed, two non-zero elements (u, v) and (u , v ) of a secondorder cone K d = {(u, v) \u2208 R d \u00d7 R, ||u|| 2 v} are orthogonal if and only if they both belong to the boundary, and they are \"anti-proportional\" (Lobo et al., 1998); that is, \u2203\u03b7 > 0 such that ||u|| 2 = v, ||u || 2 = v , (u, v) = \u03b7(\u2212u , v ) (see Figure 1). Thus, if \u03b3 > 0, we have:\n\u2022 if || i \u03b1 i y i x ji || 2 < d j \u03b3, then w j = 0, \u2022 if || i \u03b1 i y i x ji || 2 = d j \u03b3, then \u2203\u03b7 j > 0, such that w j = \u03b7 j i \u03b1 i y i x ji , ||w j || 2 = \u03b7 j d j \u03b3.\nSparsity thus emerges from the optimization problem. Let J denote the set of active dimensions, i.e., J (\u03b1, \u03b3) = {j : || i \u03b1 i y i x ji || 2 = d j \u03b3}. We can rewrite the optimality conditions as\n\u2200j, w j = \u03b7 j i \u03b1 i y i x ji , with \u03b7 j = 0 if j / \u2208 J . Equation (d) implies that \u03b3 = j d j ||w j || 2 = j d j (\u03b7 j d j \u03b3), which in turn implies j\u2208J d 2 j \u03b7 j = 1.", "publication_ref": ["b10", "b10"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Kernelization", "text": "We now remove the assumption that X is a Euclidean space, and consider embeddings of the data points x i in a Euclidean space via a mapping \u03c6 : X \u2192 R f . In correspondence with our block-based formulation of the classification problem, we assume that \u03c6(x) has m distinct block components \u03c6(x) = (\u03c6 1 (x), . . . , \u03c6 m (x)). Following the usual recipe for kernel methods, we assume that this embedding is performed implicitly, by specifying the inner product in R f using a kernel function, which in this case is the sum of individual kernel functions on each block component:\nk(x i , x j ) = \u03c6(x i ) \u03c6(x j ) = m s=1 \u03c6 s (x i ) \u03c6 s (x j ) = m s=1 k s (x i , x j ).\nWe now \"kernelize\" the problem (P ) using this kernel function. In particular, we consider the dual of (P ) and substitute the kernel function for the inner products in (D):\nmin 1 2 \u03b3 2 \u2212 e \u03b1 (D K ) w.r.t. \u03b3 \u2208 R, \u03b1 \u2208 R n s.t. 0 \u03b1 C, \u03b1 y = 0 (\u03b1 D(y)K j D(y)\u03b1) 1/2 \u03b3d j , \u2200j,\nwhere K j is the j-th Gram matrix of the points {x i } corresponding to the j-th kernel.\nThe sparsity that emerges via the KKT conditions (c) and (d) now refers to the kernels K j , and we refer to the kernels with nonzero \u03b7 j as \"support kernels.\"\nThe resulting classifier has the same form as the SVM classifier, but is based on the kernel matrix combination K = j \u03b7 j K j , which is a sparse combination of \"support kernels.\"", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Equivalence of the two formulations", "text": "By simply taking d j =\ntr Kj c , we see that problem (D K ) and (L) are indeed equivalent-thus the dual of the SKM is the multiple kernel learning primal. Care must be taken here though-the weights \u03b7 j are defined for (L) as Lagrange multipliers and for (D K ) through the anti-proportionality of orthogonal elements of a second-order cone, and a priori they might not coincide: although (D K ) and (L) are equivalent, their dual problems have different formulations. It is straightforward, however, to write the KKT optimality conditions for (\u03b1, \u03b7) for both problems and verify that they are indeed equivalent. One direct consequence is that for an optimal pair (\u03b1, \u03b7), \u03b1 is an optimal solution of the SVM with kernel matrix j \u03b7 j K j .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Optimality conditions", "text": "In this section, we formulate our problem (in either of its two equivalent forms) as the minimization of a non-differentiable convex function subject to linear constraints. Exact and approximate optimality conditions are then readily derived using subdifferentials. In later sections we will show how these conditions lead to an MY-regularized algorithmic formulation that will be amenable to SMO techniques.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Max-function formulation", "text": "A rearrangement of the problem (D K ) yields an equivalent formulation in which the quadratic constraints are moved into the objective function:\nmin max j 1 2d 2 j \u03b1 D(y)K j D(y)\u03b1 \u2212 \u03b1 e (S) w.r.t. \u03b1 \u2208 R n s.t. 0 \u03b1 C, \u03b1 y = 0.\nWe let J j (\u03b1) denote 1 2d 2 j \u03b1 D(y)K j D(y)\u03b1 \u2212 \u03b1 e and J(\u03b1) = max j J j (\u03b1). Problem (S) is the minimization of the non-differentiable convex function J(\u03b1) subject to linear constraints. Let J (\u03b1) be the set of active kernels, i.e., the set of indices j such that J j (\u03b1) = J(\u03b1). We let F j (\u03b1) \u2208 R n denote the gradient of J j , that is,\nF j = \u2202Jj \u2202\u03b1 = 1 d 2 j D(y)K j D(y)\u03b1 \u2212 e.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Optimality conditions and subdifferential", "text": "Given any function J(\u03b1), the subdifferential of J at \u03b1 \u2202J(\u03b1) is defined as (Bertsekas, 1995):\n\u2202J(\u03b1) = {g \u2208 R n , \u2200\u03b1 , J(\u03b1 ) J(\u03b1) + g (\u03b1 \u2212 \u03b1)}.\nElements of the subdifferential \u2202J(\u03b1) are called subgradients. When J is convex and differentiable at \u03b1, then the subdifferential is a singleton and reduces to the gradient. The notion of subdifferential is especially useful for characterizing optimality conditions of nonsmooth problems (Bertsekas, 1995).\nThe function J(\u03b1) defined in the earlier section is a pointwise maximum of convex differentiable functions, and using subgradient calculus we can easily see that the subdifferential \u2202J(\u03b1) of J at \u03b1 is equal to the convex hull of the gradients F j of J j for the active kernels. That is:\n\u2202J(\u03b1) = convex hull{F j (\u03b1), j \u2208 J (\u03b1)}.\nThe Lagrangian for (S) is equal to L(\u03b1) = J(\u03b1) \u2212 \u03b4 \u03b1 + \u03be (\u03b1 \u2212 Ce) + b\u03b1 y, where b \u2208 R, \u03be, \u03b4 \u2208 R n + , and the global minimum of L(\u03b1, \u03b4, \u03be, b) with respect to \u03b1 is characterized by the equation\n0 \u2208 \u2202L(\u03b1) \u21d4 \u03b4 \u2212 \u03be \u2212 by \u2208 \u2202J(\u03b1).\nThe optimality conditions are thus the following: \u03b1, (b, \u03b4, \u03be) is a pair of optimal primal/dual variables if and only if:\n\u03b4 \u2212 \u03be \u2212 by \u2208 \u2202J(\u03b1) (OP T 0 ) \u2200i, \u03b4 i \u03b1 i = 0, \u03be i (C \u2212 \u03b1 i ) = 0 \u03b1 y = 0, 0 \u03b1 C.\nAs before, we define I M (\u03b1) = {i, 0 < \u03b1 i < C}, I 0 (\u03b1) = {i, \u03b1 i = 0}, I C (\u03b1) = {i, \u03b1 i = C}. We also define, following (Keerthi et al., 2001), I 0+ = I 0 \u2229{i, y i = 1} and\nI 0\u2212 = I 0 \u2229 {i, y i = \u22121}, I C+ = I C \u2229 {i, y i = 1}, I C\u2212 = I C \u2229 {i, y i = \u22121}.\nWe can then rewrite the optimality conditions as\n\u03bd \u2212 be = D(y) j\u2208J (\u03b1) d 2 j \u03b7 j F j (\u03b1) \u03b7 0, j d 2 j \u03b7 j = 1 (OP T 1 ) \u2200i \u2208 I M \u222a I 0+ \u222a I C\u2212 , \u03bd i 0 \u2200i \u2208 I M \u222a I 0+ \u222a I C\u2212 , \u03bd i 0.", "publication_ref": ["b1", "b1", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Approximate optimality conditions", "text": "Exact optimality conditions such as (OP T 0 ) or (OP T 1 ) are generally not suitable for numerical optimization. In non-smooth optimization theory, one instead formulates optimality criteria in terms of the \u03b5-subdifferential, which is defined as\n\u2202 \u03b5 J(\u03b1) = {g \u2208 R n , \u2200\u03b1 , J(\u03b1 ) J(\u03b1)\u2212\u03b5+g (\u03b1 \u2212\u03b1)}.\nWhen J(\u03b1) = max j J j (\u03b1), then the \u03b5-subdifferential contains (potentially strictly) the convex hull of the gradients F j (\u03b1), for all \u03b5-active functions, i.e., for all j such that max i J i (\u03b1) \u2212 \u03b5 J j (\u03b1). We let J \u03b5 (\u03b1) denote the set of all such kernels. So, we have C \u03b5 (\u03b1) = convex hull{F j (\u03b1), j \u2208 J \u03b5 (\u03b1)} \u2286 \u2202 \u03b5 J(\u03b1).\nOur stopping criterion, referred to as (\u03b5 1 , \u03b5 2 )optimality, requires that the \u03b5 1 -subdifferential is within \u03b5 2 of zero, and that the usual KKT conditions are met. That is, we stop whenever there exist \u03bd, b, g such that\ng \u2208 \u2202 \u03b51 J(\u03b1) (OP T 2 ) \u2200i \u2208 I M \u222a I 0+ \u222a I C\u2212 , \u03bd i 0 \u2200i \u2208 I M \u222a I 0+ \u222a I C\u2212 , \u03bd i 0 ||\u03bd \u2212 be \u2212 D(y)g|| \u221e \u03b5 2 .\nNote that for one kernel, i.e., when the SKM reduces to the SVM, this corresponds to the approximate KKT conditions usually employed for the standard SVM (Platt, 1998;Keerthi et al., 2001;Joachims, 1998). For a given \u03b1, checking optimality is hard, since even computing \u2202 \u03b51 J(\u03b1) is hard in closed form. However, a sufficient condition for optimality can be obtained by using the inner approximation C \u03b51 (\u03b1) of this \u03b5 1 -subdifferential, i.e., the convex hull of gradients of \u03b5 1 -active kernels. Checking this sufficient condition is a linear programming (LP) existence problem, i.e., find \u03b7 such that:\n\u03b7 0, \u03b7 j = 0 if j / \u2208 J \u03b51 (\u03b1), j d 2 j \u03b7 j = 1 (OP T 3 ) max i\u2208IM \u222aI0\u2212\u222aIC+ {(K(\u03b7)D(y)\u03b1) i \u2212 y i } min i\u2208IM \u222aI0+\u222aIC\u2212 {(K(\u03b7)D(y)\u03b1) i \u2212 y i } + 2\u03b5 2 ,\nwhere K(\u03b7) = j\u2208J\u03b5 1 (\u03b1) \u03b7 j K j . Given \u03b1, we can determine whether it is (\u03b5 1 , \u03b5 2 )-optimal by solving the potentially large LP (OP T 3 ). If in addition to having \u03b1, we know a potential candidate for \u03b7, then a sufficient condition for optimality is that this \u03b7 verifies (OP T 3 ), which doesn't require solving the LP. Indeed, the iterative algorithm that we present in Section 4 outputs a pair (\u03b1, \u03b7) and only these sufficient optimality conditions need to be checked.", "publication_ref": ["b12", "b7", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Improving sparsity", "text": "Once we have an approximate solution, i.e., values \u03b1 and \u03b7 that satisfy (OP T 3 ), we can ask whether \u03b7 can be made sparser. Indeed, if some of the kernels are close to identical, then some of the \u03b7's can potentially be removed-for a general SVM, the optimal \u03b1 is not unique if data points coincide, and for a general SKM, the optimal \u03b1 and \u03b7 are not unique if data points or kernels coincide. When searching for the minimum 0 -norm \u03b7 which satisfies the constraints (OP T 3 ), we can thus consider a simple heuristic approach where we loop through all the nonzero \u03b7 j and check whether each such component can be removed. That is, for all j \u2208 J \u03b51 (\u03b1), we force \u03b7 j to zero and solve the LP. If it is feasible, then the j-th kernel can be removed.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Regularized support kernel machine", "text": "The function J(\u03b1) is convex but not differentiable.\nIt is well known that in this situation, steepest descent and coordinate descent methods do not necessarily converge to the global optimum (Bertsekas, 1995). SMO unfortunately falls into this class of methods. Therefore, in order to develop an SMO-like algorithm for the SKM, we make use of Moreau-Yosida regularization. In our specific case, this simply involves adding a second regularization term to the objective function of the SKM, as follows:\nmin 1 2 ( j d j ||w j || 2 ) 2 + 1 2 j a 2 j ||w j || 2 2 + C i \u03be i (R) w.r.t. w \u2208 R k1 \u00d7 \u2022 \u2022 \u2022 \u00d7 R km , \u03be \u2208 R n + , b \u2208 R s.t. y i ( j w j x ji + b) 1 \u2212 \u03be i , \u2200i \u2208 {1, . . . , n},\nwhere (a j ) are the MY-regularization parameters.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "Dual problem", "text": "The conic dual is readily computed as:\nmin 1 2 \u03b3 2 + 1 2 j (\u00b5 j \u2212 \u03b3d j ) 2 a 2 j \u2212 i \u03b1 i w.r.t. \u03b3 \u2208 R + , \u00b5 \u2208 R m , \u03b1 \u2208 R n s.t. 0 \u03b1 i C, \u03b1 y = 0 || i \u03b1 i y i x ji || 2 \u00b5 j , \u2200j.\nIf we define the function G(\u03b1) as\nG(\u03b1) = min \u03b3\u2208R+,\u00b5\u2208R m { 1 2 \u03b3 2 + 1 2 j (\u00b5j \u2212\u03b3dj ) 2 a 2 j \u2212 i \u03b1 i , || i \u03b1 i y i x ji || 2 \u00b5 j , \u2200j},\nthen the dual problem is equivalent to minimizing G(\u03b1) subject to 0 \u03b1 C and \u03b1 y = 0. We prove in Appendix B that G(\u03b1) is differentiable and we show how to compute G(\u03b1) and its derivative in time O(m log m).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Solving the MY-regularized SKM using SMO", "text": "Since the objective function G(\u03b1) is differentiable, we can now safely envisage an SMO-like approach, which consists in a sequence of local optimizations over only two components of \u03b1. Since the \u03b5-optimality conditions for the MY-regularized SKM are exactly the same as for the SVM, but with a different objective function (Platt, 1998;Keerthi et al., 2001):\n(OP T 4 ) max i\u2208IM \u222aI0\u2212\u222aIC+ {y i \u2207G(\u03b1) i } min i\u2208IM \u222aI0+\u222aIC\u2212 {y i \u2207G(\u03b1) i } + 2\u03b5,\nchoosing the pair of indices can be done in a manner similar to that proposed for the SVM, by using the fast heuristics of Platt (1998) and Keerthi et al. (2001). In addition, caching and shrinking techniques (Joachims, 1998) that prevent redundant computations of kernel matrix values can also be employed.\nA difference between our setting and the SVM setting is the line search, which cannot be performed in closed form for the MY-regularized SKM. However, since each line search is the minimization of a convex function, we can use efficient one-dimensional root finding, such as Brent's method (Brent, 1973).", "publication_ref": ["b12", "b7", "b12", "b7", "b6", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "Theoretical bounds", "text": "In order to be able to check efficiently the approximate optimality condition (OP T 3 ) of Section 3.3, we need estimates for \u03b1 and \u03b7 from the solution of the MY-regularized SKM obtained by SMO. It turns out that the KKT conditions for the MY-regularized SKM also lead to support kernels, i.e., there is a sparse nonnegative weight vector \u03b7 such that \u03b1 is a solution of the SVM with the kernel K = j \u03b7 j K j . However, in the regularized case, those weights \u03b7 can be obtained directly from \u03b1 as a byproduct of the computation of G(\u03b1) and its derivative. Those weights \u03b7(\u03b1) do not satisfy j d 2 j \u03b7 j = 1, but can be used to define weights \u03b7(\u03b1) that do (we give expressions for \u03b7(\u03b1) and\u03b7(\u03b1) in Appendix B).\nLet \u03b5 1 , \u03b5 2 be the two tolerances for the approximate optimality conditions for the SKM. In this section, we show that if (a j ) are small enough, then an \u03b5 2 /2optimal solution of the MY-regularized SKM \u03b1, together with\u03b7(\u03b1), is an (\u03b5 1 , \u03b5 2 )-optimal solution of the SKM, and an a priori bound on (a j ) is obtained that does not depend on the solution \u03b1.\nTheorem 1 Let 0 < \u03b5 < 1. Let y \u2208 {\u22121, 1} n and K j , j = 1, . . . , m be m positive semidefinite kernel matrices. Let d j and a j , j = 1, . . . , m, be 2m strictly positive numbers. If \u03b1 is an \u03b5-optimal solution of the MYregularized SKM, then (\u03b1,\u03b7(\u03b1)) is an (\u03b5 1 , \u03b5 2 )-optimal solution of the SKM, with\n\u03b5 1 = nC max j a 2 j d 2 j (2+max j a 2 j d 2 j ) and \u03b5 2 = \u03b5+C max j a 2 j M j d 4 j , where M j = max u v |(K j ) uv |.\nCorollary 1 With the same assumptions and\n||a|| 2 \u221e min min j d 2 j \u03b51 nC 1 + (1 + \u03b51 nC ) 1/2 , \u03b5 2 /2 max j Mj C d 4 j ,\nif \u03b1 is an \u03b5 2 /2-optimal solution of the MY-regularized SKM, then (\u03b1,\u03b7(\u03b1)) is an (\u03b5 1 , \u03b5 2 )-optimal solution of the SKM.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A minimization algorithm", "text": "We solve the SKM by solving the MY-regularized SKM with decreasing values of the regularization parameters (a j ). In our simulations, the kernel matrices are all normalized, i.e., have unit diagonal, so we can choose all d j equal. We use a j (\u03ba) = \u03ba and d j (\u03ba) = (1 \u2212 \u03ba), where \u03ba is a constant in [0, 1]. When \u03ba = 1, the MYregularized SKM is exactly the SVM based on the sum of the kernels, while when \u03ba = 0, it is the non-MYregularized SKM.\nThe algorithm is as follows: given the data and precision parameters \u03b5 1 , \u03b5 2 , we start with \u03ba = 1, which solves the SVM up to precision \u03b5 2 /2 using standard SMO, and then update \u03ba to \u00b5\u03ba (where \u00b5 < 1) and solve the MY-regularized SKM with constant \u03ba using the adjusted SMO up to precision \u03b5 2 /2, and so on. At the end of every SMO optimization, we can extract weights\u03b7 j (\u03b1) from the \u03b1 solution, as shown in Appendix B, and check the (\u03b5 1 , \u03b5 2 )-optimality conditions (OP T 3 ) of the original problem (without solving the LP). Once they are satisfied, the algorithm stops.\nSince each SMO optimization is performed on a differentiable function with Lipschitz gradient and SMO is equivalent to steepest descent for the 1norm (Joachims, 1998), classical optimization results show that each of those SMO optimizations is finitely convergent (Bertsekas, 1995). Corollary 1 directly implies there are only a finite number of such optimizations; thus, the overall algorithm is finitely convergent.\nAdditional speed-ups can be easily achieved here. For example, if for successive values of \u03ba, some kernels have a zero weight, we might as well remove them from the algorithm and check after convergence if they can be safely kept out. In simulations, we use the following values for the free parameters: \u00b5 = 0.5, \u03b5 1 /n = 0.0005, \u03b5 2 = 0.0001, where the value for \u03b5 1 /n corresponds to the average value this quantity attains when solving the QCQP (L) directly using Mosek.", "publication_ref": ["b6", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Simulations", "text": "We compare the algorithm presented in Section 4.4 with solving the QCQP (L) using Mosek for two datasets, ionosphere and breast cancer, from the UCI repository, and nested subsets of the adult dataset from Platt (1998). The basis kernels are Gaussian kernels on random subsets of features, with varying widths. We vary the number of kernels m for fixed number of data points n, and vice versa. We report running time results (Athlon MP 2000+ processor, 2.5G RAM) in Figure 2. Empirically, we obtain an average scaling of m 1.1 and n 1.4 for the SMO-based approach and m 1.6 and n 4.1 for Mosek. Thus the algorithm presented in this paper appears to provide a significant improvement over Mosek in computational complexity, both in terms of the number of kernels and the number of data points.", "publication_ref": ["b12"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Conclusion", "text": "We have presented an algorithm for efficient learning of kernels for the support vector machine.  respect to the number of data points makes it possible to learn kernels for large scale problems, while the good scaling with respect to the number of basis kernels opens up the possibility of application to largescale feature selection, in which the algorithm selects kernels that define non-linear mappings on subsets of input features.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We wish to acknowledge support from a grant from Intel Corporation, and a graduate fellowship to Francis Bach from Microsoft Research.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix A. Dual of the SKM", "text": "The primal problem (P) can be put in the following equivalent form, where K k = {(u, v) \u2208 R k+1 , ||u|| 2 v} is the second-order cone of order k (we now omit the summation intervals, with the convention that index i goes from 1 to n and index j goes from 1 to m):\nThe cone K k is self-dual, so the conic Lagrangian corresponding to the problem is\nAfter computing derivatives with respect to the primal variables and setting them to zero, we readily get the dual function:\nAfter elimination of dummy variables, we obtain problem (D).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix B. Computation of G(\u03b1)", "text": "We can first maximize over each \u00b5 i ; a short calculation reveals:\nwhich implies that \nand zero otherwise. Also, for given x, it is a piecewise quadratic function of y. We thus need to minimize a piecewise quadratic differentiable strictly convex function of \u03b3, which can be done easily by inspecting all points of non-differentiability, which requires sorting the sequence (\u03b3 j ). The complexity of such an algorithm is O(m log m).\nBecause of strict convexity the minimum with respect to \u03b3 is unique and denoted \u03b3(\u03b1). In addition, this uniqueness implies that G(\u03b1) is differentiable and that its derivative is equal to:\nWe define \u03b7 j (\u03b1) = 1 a 2 j 1 \u2212 \u03b3(\u03b1) \u03b3j (\u03b1) if \u03b3 j (\u03b1) \u03b3(\u03b1), and zero otherwise. We also define\u03b7 j (\u03b1) = \u03b7 j (\u03b1)/(1 \u2212 a 2 j \u03b7 j (\u03b1)). Using the optimality conditions for \u03b3(\u03b1), it is easy to prove that j d 2 j\u03b7 j (\u03b1) = 1. The weights \u03b7 j (\u03b1) provide an estimate of the weights for the SKM, and can be used to check optimality. Corollary 1 shows that if (a j ) is small enough, then if \u03b1 is approximately optimal for the MY-regularized SKM, the pair (\u03b1,\u03b7(\u03b1)) is approximately optimal for the SKM.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "The MOSEK interior point optimizer for linear programming: an implementation of the homogeneous algorithm", "journal": "High Perf. Optimization", "year": "2000", "authors": "E D Andersen; K D Andersen"}, {"ref_id": "b1", "title": "Nonlinear programming", "journal": "Athena Scientific", "year": "1995", "authors": "D Bertsekas"}, {"ref_id": "b2", "title": "Feature selection via concave minimization and support vector machines", "journal": "Morgan Kaufmann", "year": "1998", "authors": "P S Bradley; O L Mangasarian"}, {"ref_id": "b3", "title": "Algorithms for minimization without derivatives", "journal": "Prentice-Hall", "year": "1973", "authors": "R P Brent"}, {"ref_id": "b4", "title": "Choosing multiple parameters for support vector machines", "journal": "", "year": "2002", "authors": "O Chapelle; V Vapnik; O Bousquet; S Mukherjee"}, {"ref_id": "b5", "title": "Adaptive scaling for feature selection in SVMs", "journal": "MIT Press", "year": "2003", "authors": "Y Grandvalet; S Canu"}, {"ref_id": "b6", "title": "Making large-scale support vector machine learning practical", "journal": "MIT Press", "year": "1998", "authors": "T Joachims"}, {"ref_id": "b7", "title": "Improvements to Platt's SMO algorithm for SVM classifier design", "journal": "Neural Computation", "year": "2001", "authors": "S S Keerthi; S K Shevade; C Bhattacharyya; K R K Murthy"}, {"ref_id": "b8", "title": "Learning the kernel matrix with semidefinite programming", "journal": "J. Machine Learning Research", "year": "2004", "authors": "G R G Lanckriet; N Cristianini; L E Ghaoui; P Bartlett; M I Jordan"}, {"ref_id": "b9", "title": "Practical aspects of the Moreau-Yosida regularization: Theoretical preliminaries", "journal": "SIAM J. Optim", "year": "1997", "authors": "C Lemarechal; C Sagastizabal"}, {"ref_id": "b10", "title": "Applications of second-order cone programming", "journal": "Lin. Alg. and its Applications", "year": "1998", "authors": "M S Lobo; L Vandenberghe; S Boyd; H L\u00e9bret"}, {"ref_id": "b11", "title": "Hyperkernels. Neural Information Processing Systems", "journal": "MIT Press", "year": "2003", "authors": "S Ong; A J Smola; R C Williamson"}, {"ref_id": "b12", "title": "Fast training of support vector machines using sequential minimal optimization", "journal": "MIT Press", "year": "1998", "authors": "J Platt"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure1. Orthogonality of elements of the second-order cone K2 = {w = (u, v), u \u2208 R 2 , v \u2208 R, ||u||2 v}: two elements w, w of K2 are orthogonal and nonzero if and only if they belong to the boundary and are anti-proportional.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 .2Figure 2. Running times in seconds for Mosek and SMO. (Top) Ionosphere and breast cancer data, with fixed number of data points n and varying number of kernels m. (Bottom) Adult dataset: (left) with fixed n and varying m, (right) with fixed m and varying n ( * means Mosek ran out of memory).", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "min \u03b6 \u2212 2e \u03b1 (L) w.r.t. \u03b6 \u2208 R, \u03b1 \u2208 R n s.t. 0 \u03b1 C, \u03b1 y = 0 \u03b1 D(y)K j D(y)\u03b1", "formula_coordinates": [2.0, 307.44, 375.12, 142.51, 55.54]}, {"formula_id": "formula_1", "formula_text": "min 1 2 ( m j=1 d j ||w j || 2 ) 2 + C n i=1 \u03be i (P ) w.r.t. w \u2208 R k1 \u00d7 \u2022 \u2022 \u2022 \u00d7 R km , \u03be \u2208 R n + , b \u2208 R s.t. y i ( j w j x ji + b) 1 \u2212 \u03be i , \u2200i \u2208 {1, . . . , n}.", "formula_coordinates": [3.0, 55.44, 332.4, 235.68, 52.03]}, {"formula_id": "formula_2", "formula_text": "min 1 2 \u03b3 2 \u2212 \u03b1 e (D) w.r.t. \u03b3 \u2208 R, \u03b1 \u2208 R n s.t. 0 \u03b1 C, \u03b1 y = 0 || i \u03b1 i y i x ji || 2 d j \u03b3, \u2200j \u2208 {1, . . . , m}.", "formula_coordinates": [3.0, 59.92, 492.09, 225.02, 63.55]}, {"formula_id": "formula_3", "formula_text": "(a) \u03b1 i (y i ( j w j x ji + b) \u2212 1 + \u03be i ) = 0, \u2200i (b) (C \u2212 \u03b1 i )\u03be i = 0, \u2200i (c) wj ||wj ||2 \u2212 i \u03b1iyixji dj \u03b3 = 0, \u2200j (d) \u03b3( d j t j \u2212 \u03b3) = 0.", "formula_coordinates": [3.0, 78.43, 603.93, 188.02, 61.81]}, {"formula_id": "formula_4", "formula_text": "\u2022 if || i \u03b1 i y i x ji || 2 < d j \u03b3, then w j = 0, \u2022 if || i \u03b1 i y i x ji || 2 = d j \u03b3, then \u2203\u03b7 j > 0, such that w j = \u03b7 j i \u03b1 i y i x ji , ||w j || 2 = \u03b7 j d j \u03b3.", "formula_coordinates": [3.0, 307.44, 484.12, 234.0, 46.93]}, {"formula_id": "formula_5", "formula_text": "\u2200j, w j = \u03b7 j i \u03b1 i y i x ji , with \u03b7 j = 0 if j / \u2208 J . Equation (d) implies that \u03b3 = j d j ||w j || 2 = j d j (\u03b7 j d j \u03b3), which in turn implies j\u2208J d 2 j \u03b7 j = 1.", "formula_coordinates": [3.0, 307.44, 587.91, 234.0, 45.75]}, {"formula_id": "formula_6", "formula_text": "k(x i , x j ) = \u03c6(x i ) \u03c6(x j ) = m s=1 \u03c6 s (x i ) \u03c6 s (x j ) = m s=1 k s (x i , x j ).", "formula_coordinates": [4.0, 63.15, 153.64, 218.57, 29.91]}, {"formula_id": "formula_7", "formula_text": "min 1 2 \u03b3 2 \u2212 e \u03b1 (D K ) w.r.t. \u03b3 \u2208 R, \u03b1 \u2208 R n s.t. 0 \u03b1 C, \u03b1 y = 0 (\u03b1 D(y)K j D(y)\u03b1) 1/2 \u03b3d j , \u2200j,", "formula_coordinates": [4.0, 71.06, 251.62, 202.77, 63.55]}, {"formula_id": "formula_8", "formula_text": "min max j 1 2d 2 j \u03b1 D(y)K j D(y)\u03b1 \u2212 \u03b1 e (S) w.r.t. \u03b1 \u2208 R n s.t. 0 \u03b1 C, \u03b1 y = 0.", "formula_coordinates": [4.0, 320.68, 198.41, 199.22, 46.67]}, {"formula_id": "formula_9", "formula_text": "F j = \u2202Jj \u2202\u03b1 = 1 d 2 j D(y)K j D(y)\u03b1 \u2212 e.", "formula_coordinates": [4.0, 341.81, 329.81, 144.48, 19.85]}, {"formula_id": "formula_10", "formula_text": "\u2202J(\u03b1) = {g \u2208 R n , \u2200\u03b1 , J(\u03b1 ) J(\u03b1) + g (\u03b1 \u2212 \u03b1)}.", "formula_coordinates": [4.0, 311.33, 409.93, 226.22, 18.91]}, {"formula_id": "formula_11", "formula_text": "\u2202J(\u03b1) = convex hull{F j (\u03b1), j \u2208 J (\u03b1)}.", "formula_coordinates": [4.0, 339.3, 590.31, 170.26, 17.04]}, {"formula_id": "formula_12", "formula_text": "0 \u2208 \u2202L(\u03b1) \u21d4 \u03b4 \u2212 \u03be \u2212 by \u2208 \u2202J(\u03b1).", "formula_coordinates": [4.0, 352.21, 667.2, 144.44, 17.04]}, {"formula_id": "formula_13", "formula_text": "\u03b4 \u2212 \u03be \u2212 by \u2208 \u2202J(\u03b1) (OP T 0 ) \u2200i, \u03b4 i \u03b1 i = 0, \u03be i (C \u2212 \u03b1 i ) = 0 \u03b1 y = 0, 0 \u03b1 C.", "formula_coordinates": [5.0, 76.77, 82.78, 191.35, 39.85]}, {"formula_id": "formula_14", "formula_text": "I 0\u2212 = I 0 \u2229 {i, y i = \u22121}, I C+ = I C \u2229 {i, y i = 1}, I C\u2212 = I C \u2229 {i, y i = \u22121}.", "formula_coordinates": [5.0, 55.44, 169.68, 234.0, 28.99]}, {"formula_id": "formula_15", "formula_text": "\u03bd \u2212 be = D(y) j\u2208J (\u03b1) d 2 j \u03b7 j F j (\u03b1) \u03b7 0, j d 2 j \u03b7 j = 1 (OP T 1 ) \u2200i \u2208 I M \u222a I 0+ \u222a I C\u2212 , \u03bd i 0 \u2200i \u2208 I M \u222a I 0+ \u222a I C\u2212 , \u03bd i 0.", "formula_coordinates": [5.0, 72.34, 213.36, 200.19, 65.13]}, {"formula_id": "formula_16", "formula_text": "\u2202 \u03b5 J(\u03b1) = {g \u2208 R n , \u2200\u03b1 , J(\u03b1 ) J(\u03b1)\u2212\u03b5+g (\u03b1 \u2212\u03b1)}.", "formula_coordinates": [5.0, 55.44, 374.33, 233.99, 18.91]}, {"formula_id": "formula_17", "formula_text": "g \u2208 \u2202 \u03b51 J(\u03b1) (OP T 2 ) \u2200i \u2208 I M \u222a I 0+ \u222a I C\u2212 , \u03bd i 0 \u2200i \u2208 I M \u222a I 0+ \u222a I C\u2212 , \u03bd i 0 ||\u03bd \u2212 be \u2212 D(y)g|| \u221e \u03b5 2 .", "formula_coordinates": [5.0, 84.11, 544.02, 176.65, 61.87]}, {"formula_id": "formula_18", "formula_text": "\u03b7 0, \u03b7 j = 0 if j / \u2208 J \u03b51 (\u03b1), j d 2 j \u03b7 j = 1 (OP T 3 ) max i\u2208IM \u222aI0\u2212\u222aIC+ {(K(\u03b7)D(y)\u03b1) i \u2212 y i } min i\u2208IM \u222aI0+\u222aIC\u2212 {(K(\u03b7)D(y)\u03b1) i \u2212 y i } + 2\u03b5 2 ,", "formula_coordinates": [5.0, 307.44, 115.16, 239.02, 54.06]}, {"formula_id": "formula_19", "formula_text": "min 1 2 ( j d j ||w j || 2 ) 2 + 1 2 j a 2 j ||w j || 2 2 + C i \u03be i (R) w.r.t. w \u2208 R k1 \u00d7 \u2022 \u2022 \u2022 \u00d7 R km , \u03be \u2208 R n + , b \u2208 R s.t. y i ( j w j x ji + b) 1 \u2212 \u03be i , \u2200i \u2208 {1, . . . , n},", "formula_coordinates": [5.0, 307.44, 646.72, 238.04, 50.95]}, {"formula_id": "formula_20", "formula_text": "min 1 2 \u03b3 2 + 1 2 j (\u00b5 j \u2212 \u03b3d j ) 2 a 2 j \u2212 i \u03b1 i w.r.t. \u03b3 \u2208 R + , \u00b5 \u2208 R m , \u03b1 \u2208 R n s.t. 0 \u03b1 i C, \u03b1 y = 0 || i \u03b1 i y i x ji || 2 \u00b5 j , \u2200j.", "formula_coordinates": [6.0, 83.69, 99.81, 176.99, 80.24]}, {"formula_id": "formula_21", "formula_text": "G(\u03b1) = min \u03b3\u2208R+,\u00b5\u2208R m { 1 2 \u03b3 2 + 1 2 j (\u00b5j \u2212\u03b3dj ) 2 a 2 j \u2212 i \u03b1 i , || i \u03b1 i y i x ji || 2 \u00b5 j , \u2200j},", "formula_coordinates": [6.0, 55.44, 205.47, 235.26, 39.42]}, {"formula_id": "formula_22", "formula_text": "(OP T 4 ) max i\u2208IM \u222aI0\u2212\u222aIC+ {y i \u2207G(\u03b1) i } min i\u2208IM \u222aI0+\u222aIC\u2212 {y i \u2207G(\u03b1) i } + 2\u03b5,", "formula_coordinates": [6.0, 64.43, 446.76, 216.01, 37.75]}, {"formula_id": "formula_23", "formula_text": "\u03b5 1 = nC max j a 2 j d 2 j (2+max j a 2 j d 2 j ) and \u03b5 2 = \u03b5+C max j a 2 j M j d 4 j , where M j = max u v |(K j ) uv |.", "formula_coordinates": [6.0, 307.44, 372.85, 234.01, 62.05]}, {"formula_id": "formula_24", "formula_text": "||a|| 2 \u221e min min j d 2 j \u03b51 nC 1 + (1 + \u03b51 nC ) 1/2 , \u03b5 2 /2 max j Mj C d 4 j ,", "formula_coordinates": [6.0, 315.13, 463.38, 218.61, 32.37]}], "doi": ""}