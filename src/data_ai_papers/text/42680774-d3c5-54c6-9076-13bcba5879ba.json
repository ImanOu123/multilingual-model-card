{"title": "QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations", "authors": "Chaitanya Malaviya; Peter Shaw; Ming-Wei Chang; Kenton Lee; Kristina Toutanova", "pub_date": "", "abstract": "Formulating selective information needs results in queries that implicitly specify set operations, such as intersection, union, and difference. For instance, one might search for \"shorebirds that are not sandpipers\" or \"science-fiction films shot in England\". To study the ability of retrieval systems to meet such information needs, we construct QUEST, a dataset of 3357 natural language queries with implicit set operations, that map to a set of entities corresponding to Wikipedia documents. The dataset challenges models to match multiple constraints mentioned in queries with corresponding evidence in documents and correctly perform various set operations. The dataset is constructed semi-automatically using Wikipedia category names. Queries are automatically composed from individual categories, then paraphrased and further validated for naturalness and fluency by crowdworkers. Crowdworkers also assess the relevance of entities based on their documents and highlight attribution of query constraints to spans of document text. We analyze several modern retrieval systems, finding that they often struggle on such queries. Queries involving negation and conjunction are particularly challenging and systems are further challenged with combinations of these operations. 1 65 iting annotation to the top few results of a baseline 66 information retrieval system. 67 To analyze how well retrieval systems handle 68 such queries, we present QUEST, a dataset with 69 natural language queries from four domains, that 70 are mapped to relatively comprehensive sets of en-71 tities corresponding to Wikipedia pages. We use 72 Wikipedia categories and their mapping to entities 73 in Wikipedia as a building block for our dataset 74 construction approach, but do not allow access to 75 this semi-structured data source at inference time, 76 to simulate text-based retrieval. Wikipedia cate-77 gories represent a broad set of natural language 78 descriptions of entity properties and often corre-79 spond to selective information need queries that 80 could be plausibly issued by a search engine user 81 ([At least 90% of the time based on our filtering?]). 82 The correspondence between property names and 83 document text is also often subtle and requires so-84 phisticated reasoning to determine relevance, rep-85 resenting the natural language inference challenge 86 inherent in the task, while the knowledge of cate-87 gory membership allows us to construct relatively 88 comprehensive sets of candidate entities for atomic 89 categories and their combinations. 90 Our dataset construction process is outlined in 91 Figure 1. The base queries in our dataset are 92 semi-automatically generated using Wikipedia cat-93 egory names. To construct queries, we sample 94 category names and compose them into complex 95 queries by using pre-defined templates (for exam-96 ple, A \\ B \\ C). Next, we ask crowdworkers to 97 paraphrase these automatically generated queries, 98 while ensuring that the paraphrased queries are 99 fluent and clearly describe what a user could be 00 looking for. These are then validated for natural- 01 ness and fluency by a different set of crowdworkers, 02 and filtered according to those criteria. Finally, for 03 a large subset of our dataset, we collect scalar rel-04 evance labels based on the entity documents, and 05 textual attributions mapping query constraints to 06 spans of document text, to aid the development of 07 systems that can make precise inferences based on 08 trusted sources. 09 Performing well on this dataset requires sys-10 tems that can match query constraints with cor-11", "sections": [{"heading": "Introduction", "text": "People often express their information needs with multiple preferences or constraints. Queries corresponding to such needs typically implicitly express set operations such as intersection, difference, and union. For example, a movie-goer might be looking for a science-fiction film from the 90s which does not feature aliens and a reader might be interested in a historical fiction novel set in France. Similarly, * Work done during an internship at Google. 1 The dataset is available at https://github.com/ google-research/language/tree/master/language/ quest.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Birds of Venezuelan Andes -Birds of Colombia", "text": "Birds found in the Venezuelan Andes but not in Colombia a botanist attempting to identify a species based on their recollection might search for shrubs that are evergreen and found in Panama. Further, if the set of entities that satisfy the constraints is relatively small, a reader may like to see and explore an exhaustive list of these entities. In addition, to verify and trust a system's recommendations, users benefit from being shown evidence from trusted sources .\nAddressing such queries has been primarily studied in the context of question answering with structured knowledge bases (KBs), where query constraints are grounded to predefined predicates and symbolically executed. However, KBs can be incomplete and expensive to curate and maintain. Meanwhile, advances in information retrieval may enable developing systems that can address such queries without relying on structured KBs, by matching query constraints directly to supporting evidence in text documents. However, queries that combine multiple constraints with implicit set operations are not well represented in existing retrieval benchmarks such as MSMarco (Nguyen et al., 2016) and Natural Questions (Kwiatkowski et al., 2019). Also, such datasets do not focus on retrieving an exhaustive document set, instead limiting annotation to the top few results of a baseline information retrieval system.\nTo analyze retrieval system performance on such queries, we present QUEST, a dataset with natural language queries from four domains, that are mapped to relatively comprehensive sets of entities corresponding to Wikipedia pages. We use categories and their mapping to entities in Wikipedia as a building block for our dataset construction approach, but do not allow access to this semistructured data source at inference time, to simulate text-based retrieval. Wikipedia categories represent a broad set of natural language descriptions of entity properties and often correspond to selective information need queries that could be plausibly issued by a search engine user. The relationship between property names and document text is often subtle and requires sophisticated reasoning to determine, representing the natural language inference challenge inherent in the task.\nOur dataset construction process is outlined in Figure 1. The base queries are semi-automatically generated using Wikipedia category names. To construct complex queries, we sample category names and compose them by using pre-defined templates (for example, A \u2229 B \\ C). Next, we ask crowdworkers to paraphrase these automatically generated queries, while ensuring that the paraphrased queries are fluent and clearly describe what a user could be looking for. These are then validated for naturalness and fluency by a different set of crowdworkers, and filtered according to those criteria. Finally, for a large subset of the data, we collect scalar relevance labels based on the entity documents and fine-grained textual attributions mapping query constraints to spans of document text. Such annotation could aid the development of systems that can make precise inferences from trusted sources.\nPerforming well on this dataset requires systems that can match query constraints with corresponding evidence in documents and handle set operations implicitly specified by the query (see erations are not well represented in existing re- proposed for question answering over knowledge 136 bases (Berant et al., 2013;Yih et al., 2016;Tal- (Nguyen et al., 2016) and Natural Questions (Kwiatkowski et al., 2019). Also, such datasets do not focus on retrieving an exhaustive document set, instead limiting annotation to the top few results of a baseline information retrieval system.\nTo analyze how well retrieval systems handle such queries, we present QUEST, a dataset with natural language queries from four domains, that are mapped to relatively comprehensive sets of entities corresponding to Wikipedia pages. We use Wikipedia categories and their mapping to entities in Wikipedia as a building block for our dataset construction approach, but do not allow access to this semi-structured data source at inference time, to simulate text-based retrieval. Wikipedia categories represent a broad set of natural language descriptions of entity properties and often correspond to selective information need queries that could be plausibly issued by a search engine user ([At least 90% of the time based on our filtering?]).\nThe correspondence between property names and document text is also often subtle and requires sophisticated reasoning to determine relevance, representing the natural language inference challenge inherent in the task, while the knowledge of category membership allows us to construct relatively comprehensive sets of candidate entities for atomic categories and their combinations.\nOur dataset construction process is outlined in Figure 1. The base queries in our dataset are semi-automatically generated using Wikipedia category names. To construct queries, we sample category names and compose them into complex queries by using pre-defined templates (for example, A \\ B \\ C). Next, we ask crowdworkers to paraphrase these automatically generated queries, while ensuring that the paraphrased queries are fluent and clearly describe what a user could be looking for. These are then validated for naturalness and fluency by a different set of crowdworkers, and filtered according to those criteria. Finally, for a large subset of our dataset, we collect scalar relevance labels based on the entity documents, and textual attributions mapping query constraints to spans of document text, to aid the development of systems that can make precise inferences based on trusted sources.\nPerforming well on this dataset requires systems that can match query constraints with cor-   Figure 2), while also efficiently scaling to large collections of entities. We evaluate several retrieval systems by finetuning pretrained models on our dataset. Systems are trained to retrieve multidocument sets given a query. We find that current dual encoder and cross-attention models up to the size of T5-Large (Raffel et al., 2020) are largely not effective at performing retrieval for queries with set operations. Queries with conjunctions and negations prove to be especially challenging for models and systems are further challenged with combinations of set operations. Our error analysis reveals that non-relevant false positive entities are often caused by the model ignoring negated constraints, or ignoring the conjunctive constraints in a query.", "publication_ref": ["b15", "b11", "b2", "b28", "b15", "b11", "b18"], "figure_ref": ["fig_0", "fig_0", "fig_1"], "table_ref": []}, {"heading": "Related Work", "text": "Previous work in question answering and information retrieval has focused on QA over knowledge bases as well as open-domain QA and retrieval over a set of entities or documents. We highlight how these relate to our work below.\nKnowledge Base QA Several datasets have been proposed for question answering over knowledge bases (Berant et al., 2013;Yih et al., 2016;Talmor and Berant, 2018;Keysers et al., 2020;Gu et al., 2021, inter alia). These benchmarks require retrieval of a set of entities that exist as nodes or relations in an accompanying knowledge base. Questions are optionally supplemented with logical forms. Lan et al. (2021) provide a comprehensive survey of complex KBQA datasets. Previous work has simultaneously noted that large curated KBs are incomplete (Watanabe et al., 2017). Notably, KBQA systems operate over a constrained answer schema, which limits the types of queries they can handle. Further, these schema are expensive to construct and maintain. For this reason, our work focuses on a setting where we do not assume access to a KB. We note that KBQA datasets have also been adapted to settings where a KB is incomplete or unavailable (Watanabe et al., 2017;Sun et al., 2019). This was done by either removing some subset of the data from the KB or ignoring the KB entirely. A key difference from these datasets is also that we do not focus on multihop reasoning over multiple documents. Instead, the relevance of an entity can be determined solely based on its document.\nOpen-Domain QA and Retrieval Many opendomain QA benchmarks, which consider QA over unstructured text corpora, have been proposed in prior work. Some of these, such as TREC (Craswell et al., 2020), MSMarco (Nguyen et al., 2016) and Natural Questions (Kwiatkowski et al., 2019) are constructed using \"found data\", using real user queries on search engines. Thakur et al. (2021) present a benchmark where they consider many such existing datasets. Datasets such as Hot-potQA (Yang et al., 2018), and MultiRC (Khashabi et al., 2018) have focused on multi-hop question answering. Other work has explored e-commerce datasets (for example, (Kong et al., 2022)), but these have not been released publicly. Notably, the focus of these datasets differs from ours as we focus on queries that contain implicit set operations over exhaustive answer sets. Such queries are not well represented in existing datasets because they occur in the tail of the query distributions considered.\nMulti-Answer Retrieval Related work (Min et al., 2021;Amouyal et al., 2022) also studies the problem of multi-answer retrieval, where systems are required to predict multiple distinct answers for a query. Min et al. (2021) adapt existing datasets (for example, WebQuestionsSP (Yih et al., 2016)) to study this setting and propose a new metric, MRecall@K, to evaluate exhaustive recall of multiple answers. We also consider the problem of multi-answer set retrieval, but consider queries that implicitly contain set constraints.\nIn concurrent work, RomQA (Zhong et al., 2022) proposes an open-domain QA dataset, focusing on combinations of constraints extracted from Wikidata. RomQA shares our motivation to enable answering queries with multiple constraints, which have possibly large answer sets. To make attribution to evidence feasible without human annotation, RomQA focuses on questions whose component constraints can be verified from single entity-linked sentences from Wikipedia abstracts, annotated with relations automatically through distant supervision, with high precision but possibly low recall (T-Rex corpus). In QUEST, we broaden the scope of queryevidence matching operations by allowing for attribution through more global, document-level inference. To make human annotation for attribution feasible, we limit the answer set size and the evidence for an answer to a single document.", "publication_ref": ["b2", "b28", "b23", "b8", "b13", "b26", "b26", "b22", "b3", "b15", "b11", "b24", "b27", "b9", "b10", "b14", "b0", "b14", "b28", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Dataset Generation", "text": "QUEST consists of 3357 queries paired with up to 20 corresponding entities. Each entity has an associated document derived from its Wikipedia page. The dataset is divided into 1307 queries for training, 323 for validation, and 1727 for testing.\nThe task for a system is to return the correct set of entities for a given query. Additionally, as the collection contains 325,505 entities, the task requires retrieval systems that can scale efficiently. We do not allow systems to access additional information outside of the text descriptions of entities at inference time. Category labels are omitted from all entity documents.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Atomic Queries", "text": "The base atomic queries (i.e., queries without any introduced set operations) in our dataset are derived from Wikipedia category names 2 . These are handcurated natural language labels assigned to groups of related documents in Wikipedia 3 . Category assignments to documents allow us to automatically determine the set of answer entities for queries with high precision and relatively high recall. We compute transitive closures of all relevant categories to determine their answer sets.\nHowever, repurposing these categories for constructing queries poses challenges: 1) lack of evi-  dence in documents: documents may not contain sufficient evidence for judging their relevance to a category, potentially providing noisy signal for relevance attributable to the document text, 2) low recall: entities may be missing from categories to which they belong. For about half of the dataset, we crowdsource relevance labels and attribution based on document text, and investigate recall through manual error analysis ( \u00a75). We select four domains to represent some diversity in queries: films, books, animals and plants. Focusing on four rather than all possible domains enables higher quality control. The former two model a general search scenario, while the latter two model a scientific search scenario.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introducing set operations", "text": "To construct queries with set operations, we define templates that represent plausible combinations of atomic queries. Denoting atomic queries as A, B and C, our templates and corresponding examples from different domains are listed in Table 1. Templates were constructed by composing three basic set operations (intersection, union and difference). They were chosen to ensure unambiguous interpretations of resulting queries by omitting those combinations of set operations that are non-associative.\nBelow we describe the logic behind sampling atomic queries (i.e., A, B, C) for composing com-plex queries, with different set operations. In all cases, we ensure that answer sets contain between 2-20 entities so that crowdsourcing relevance judgements is feasible. We sample 200 queries per template and domain, for a total of 4200 initial queries. The dataset is split into train + validation (80-20 split) and testing equally. In each of these sets, we sampled an equal number of queries per template. Intersection. The intersection operation for a template A\u2229B is particularly interesting and potentially challenging when both A and B have large answer sets but their intersection is small. We require the minimum answer set sizes of each A and B to be fairly large (>50 entities), while their intersection to be small (2-20 entities). Difference. Similar to intersection, we require the answer sets for both A and B to be substantial (>50 entities), but also place maximum size constraints on both A (<200 entities) and B (<10000 entities) as very large categories tend to suffer from recall issues in Wikipedia. We also limit the intersection of A and B (see reasoning in Appendix B). Union. For the union operation, we require both A and B to be well-represented through the entities in the answer set for their union A \u222a B. Hence, we require both A and B to have at least 3 entities. Further, we require their intersection to be non-zero but less than 1/3rd of their union. This is so that A and B are somewhat related queries.  For all other templates that contain compositions of the above set operations, we apply the same constraints recursively. For example, for A\u2229B \\C, we sample atomic queries A and B for the intersection operation, then sample C based on the relationship between A \u2229 B and C.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Annotation Tasks", "text": "Automatically generating queries based on templates results in queries that are not always fluent and coherent. Further, entities mapped to a query may not actually be relevant and don't always have attributable evidence for judging their relevance. We conduct crowdsourcing to tackle these issues. The annotation tasks aim at ensuring that 1) queries are fluent, unambiguous and contain diverse natural language logical connectives, (2) entities are verified as being relevant or non-relevant and (3) relevance judgements are attributed to document text for each relevant entity. Crowdsourcing is performed in three stages, described below. More annotation details and the annotation interfaces can be found in Appendix C.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Paraphrasing", "text": "Crowdworkers were asked to paraphrase a templatically generated query so that the paraphrased query is fluent, expresses all constraints in the original query, and clearly describes what a user could be looking for. This annotation was done by one worker per query.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Validation", "text": "This stage is aimed at validating the queries we obtain from the paraphrasing stage. Crowdworkers were given queries from the first stage and asked to label whether the query is 1) fluent, 2) equivalent to the original templatic query in meaning, and 3) rate its naturalness (how likely it is to be issued by a real user). This annotation was done by 3 workers per query. We excluded those queries which were rated as not fluent, unnatural or having a different meaning than the original query, based on a ma-jority vote. Based on the validation, we removed around around 11% of the queries from stage 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Relevance Labeling", "text": "Next, crowdworkers were asked to provide relevance judgements for the automatically determined answer sets of queries. Specifically, they were given a query and associated entities/documents, and asked to label their relevance on a scale of 0-3 (definitely not relevant, likely not relevant, likely relevant, definitely relevant). They were asked to ensure that relevance should mostly be inferred from the document, but they could use some background knowledge and do minimal research.\nWe also asked them to provide attributions for document relevance. Specifically, we ask them to first label whether the document provides sufficient evidence for the relevance of the entity (complete/partial/no). Then, for different phrases in the query (determined by the annotator), we ask them to mark sentence(s) in the document that indicate its relevance. The attribution annotation is broadly inspired by Rashkin et al. (2021). For negated constraints, we ask annotators to mark attributable sentences if they provide counter-evidence. Since this annotation was time-intensive, we collected these annotations for two domains (films and books). We found that relevance labeling was especially difficult for the plants and animals domains, as they required more specialized scientific knowledge. In our pilot study prior to larger scale data collection, we collected 3 relevance ratings from different annotators for 905 query and document pairs from the films domain. In 61.4% of cases, all 3 raters judged the document to be \"Definitely relevant\" or \"Likely relevant\" or all 3 raters judged the document to be \"Definitely not relevant\" or \"Likely not relevant\". The Fleiss' kappa metric on this data was found to be K=0.43. We excluded all entities which were marked as likely or definitely not relevant to a query based on the document text from its answer set. Around 23.7% of query-document pairs from stage 2 were excluded. ", "publication_ref": ["b19"], "figure_ref": [], "table_ref": []}, {"heading": "Dataset Statistics", "text": "Basic dataset statistics are reported in Table 2. The dataset contains more entities from the films domain, because this domain is more populated in Wikipedia. The average length of queries is 8.6 words and the average document length is 452 words. Documents from the films and books domains are longer on average, as they often contain plots and storylines. Around \u223c69% of entities have complete evidence and \u223c30% have partial evidence. Evidence was labeled as partial when not all phrases in the query had explicit evidence in the document (i.e., they may require background knowledge or reasoning). There are on average 33.2 words attributed for each entity with the maximum attribution text span ranging up to length 1837 words. Finally, the average answer set size is 10.5 entities.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Additional Training Examples", "text": "Beyond the annotated data, we generated additional synthetic examples for training. We found including such examples improved model performance, and we include these examples for the experiments in \u00a74. To generate these examples, we sample 5000 atomic queries from all domains, ensuring that they do not already appear as sub-queries in any of the queries in QUEST and use their corresponding entities in Wikipedia as their relevant entity set.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup", "text": "We evaluate modern retrieval systems to establish baseline performances. We also perform extensive error analysis to understand patterns of model errors and the quality of the labels in QUEST.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Task Definition", "text": "We consider a corpus, E, that contains entities across all domains in the dataset. Each entity is accompanied with a document based on its Wikipedia page. An example in our dataset consists of a query,\nx, and an annotated set of relevant entities, y \u2282 E. As described in \u00a73, for all examples |y| < 20. Our task is to develop a system that, given E and a query x, predicts a set of relevant entities,\u0177 \u2282 E.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation", "text": "Our primary evaluation metric is average F 1 , which averages per-example F 1 scores. We compute F 1 for each example by comparing the predicted set of entities,\u0177, with the annotated set, y.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Baseline Systems", "text": "We evaluated several combinations of retrievers and classifiers, as shown in Figure 3. For the retriever component, we consider a sparse BM25 retriever (Robertson et al., 2009) and a dense dual encoder retriever (denoted DE). Following , we initialize our dual encoder from a T5 (Raffel et al., 2020) encoder and train with an in-batch sampled softmax loss (Henderson et al., 2017). Once we have a candidate set, we need to determine a set of relevant entities. To classify relevance of each candidate document for the given query, we consider a cross-attention model which consists of a T5 encoder and decoder. 4 We train the cross-attention classifier using a binary cross-entropy loss with negative examples based on non-relevant documents in top 1,000 documents retrieved by BM25 and random non-relevant documents (similarly to Nogueira and Cho (2019)   we truncate document text. We discuss the impact of this and alternatives in \u00a75. Further, since T5 was pre-trained on Wikipedia, we investigate the impact of memorization in Appendix D. Additional details and hyperparameter settings are in Appendix A.", "publication_ref": ["b20", "b18", "b17"], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "Manual Error Annotation", "text": "For the best overall system, we sampled errors and manually annotated 1145 query-document pairs from the validation set. For the retriever, we sampled relevant documents not included in the top-100 candidate set and non-relevant documents ranked higher than relevant ones. For the classifier, we sampled false positive and false negative errors made in the top-100 candidate set. This annotation process included judgements of document relevance (to assess agreement with the annotations in the dataset) and whether the document (and the truncated version considered by the dual encoder or classifier) contained sufficient evidence to reasonably determine relevance. We also annotated relevance for each constraint within a query. We discuss these results in \u00a75.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results and Analysis", "text": "We report the performance of our baseline systems on the test set in Table 3. In this section, we summarize the key findings from our analysis of these results and the error annotation described in \u00a74.4.\nDual encoders outperform BM25. As shown in Table 3, the best overall system uses a T5-Large Dual Encoder instead of BM25 for retrieval. The performance difference is even more significant when comparing recall of Dual Encoders and BM25 directly. We report average recall (average per-example recall of the full set of relevant documents) and MRecall (Min et al., 2021) (the percentage of examples where the candidate set contains all relevant documents), over various candidate set sizes in Table 4. Retrieval and classification are both challenging. As we consider only the top-100 candidates from the retriever, the retriever's recall@100 sets an upper bound on the recall of the overall system. Recall@100 is only 0.476 for the T5-Large Dual Encoder, and the overall recall is further reduced by the T5-Large classifier to 0.368, despite achieving only 0.165 precision. This suggests that there is room for improvement from both stages to improve overall scores. As performance improves for larger T5 sizes for both retrieval and classification, further model scaling could be beneficial. Models struggle with intersection and difference. We also analyzed results across different templates and domains, as shown in Table 5. Different constraints lead to varying distributions over answer set sizes and the atomic categories used. Therefore, it can be difficult to interpret differences in F1 scores across templates. Nevertheless, we found the queries with set union have the highest average F1 scores. Queries with set intersection have the lowest average F1 scores, and queries with set difference also appear to be challenging.\nTo analyze why queries with conjunction and negation are challenging, we labeled the relevance of individual query constraints ( \u00a74.4), where a system incorrectly judges relevance of a non-relevant document. The results are summarized in Table 6. For a majority of false positive errors involving intersection, at least one constraint is satisfied. This could be interpreted as models incorrectly treating intersection as union when determining relevance. Similarly, for a majority of examples with set difference, the negated constraint is not satisfied. This suggests that the systems are not sufficiently sensitive to negations.  There is significant headroom to improve both precision and recall. As part of our manual error analysis ( \u00a74.4), we made our own judgements of relevance and measured agreement with the relevance annotations in QUEST. As this analysis focused on cases where our best system disagreed with the relevance labels in the dataset, we would expect agreement on these cases to be significantly lower than on randomly selected query-document pairs in the dataset. Therefore, it provides a focused way to judge the headroom and annotation quality of the dataset. For false negative errors, we judged 91.1% of the entities to be relevant for the films and books domains, and 81.4% for plants and animals. Notably, we collected relevance labels for the films and books domains and removed some entities based on these labels, as described in \u00a73, which likely explains the higher agreement for false negatives from these domains. This indicates significant headroom for improving recall as defined by QUEST, especially for the domains where we collected relevance labels.\nFor false positive errors, we judged 28.8% of the entities to be relevant, showing a larger disagreement with the relevance labels in the dataset. This is primarily due to entities not included in the entity sets derived from the Wikipedia category taxonomy (97.7%), rather than entities removed due to relevance labeling. This is a difficult issue to fully resolve, as it is not feasible to exhaustively label relevance for all entities to correct for recall issues in the Wikipedia category taxonomy. Future work can use pooling to continually grow the set  of relevant documents (Sparck Jones and Van Rijsbergen, 1975). Despite this, our analysis suggests there is significant headroom for improving precision, as we judged a large majority of the false positive predictions to be non-relevant.\nTruncating document text usually provides sufficient context. In our experiments, we truncate document text to 512 tokens for the dual encoder, and 384 tokens for the classifier to allow for the document and query to be concatenated. Based on our error analysis ( \u00a74.4), out of the documents with sufficient evidence to judge relevance, evidence occurred in this truncated context 93.2% of the time for the dual encoder, and 96.1% of the time for the classifier. This may explain the relative success of this simple baseline for handling long documents. We also evaluated alternative strategies but these performed worse in preliminary experiments 5 . Future work can evaluate efficient transformer variants (Guo et al., 2022;Beltagy et al., 2020).", "publication_ref": ["b14", "b21", "b6", "b1"], "figure_ref": [], "table_ref": ["tab_5", "tab_5", "tab_6", "tab_7", "tab_8"]}, {"heading": "Conclusion", "text": "We present QUEST, a new benchmark of queries which contain implicit set operations with corresponding sets of relevant entity documents. Our experiments indicate that such queries present a challenge for modern retrieval systems. Future work could consider approaches that have better inductive biases for handling set operations in natural language expressions (for example, Vilnis et al. (2018)). The attributions in QUEST can be leveraged for building systems that can provide finegrained attributions at inference time. The potential of pretrained generative LMs and multi-evidence aggregation methods to answer set-seeking selective queries, while providing attribution to sources, can also be investigated.", "publication_ref": ["b25"], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "Naturalness. Since our dataset relies on the Wikipedia category names and semi-automatically generated compositions, it does not represent an unbiased sample from a natural distribution of real search queries that contain implicit set operations. Further, we limit attention to non-ambiguous queries and do not address the additional challenges that arise due to ambiguity in real search scenarios. However, the queries in our dataset were judged to plausibly correspond to real user search needs and system improvements measured on QUEST should correlate with improvements on at least a fraction of natural search engine queries with set operations.\nRecall. We also note that because Wikipedia categories have imperfect recall of all relevant entities (that contain sufficient evidence in their documents), systems may be incorrectly penalised for predicted relevant entities assessed as false positive. We quantify this in section 5. We have also limited the trusted source for an entity to its Wikipedia document but entities with insufficient textual evidence in their documents may still be relevant. Ideally, multiple trusted sources could be taken into account and evidence could be aggregated to make relevance decisions. RomQA (Zhong et al., 2022) takes a step in this latter direction although the evidence attribution is not manually verified.\nAnswer Set Sizes. To ensure that relevance labels are correct and verifiable, we seek the help of crowdworkers. However, this meant that we needed to restrict the answer set sizes to 20 for the queries in our dataset, to make annotation feasible. On one hand, this is realistic for a search scenario because users may only be interested in a limited set of results. On the other hand, our dataset does not model a scenario where the answer set sizes are much larger.", "publication_ref": ["b29"], "figure_ref": [], "table_ref": []}, {"heading": "A Experiment Details and Hyperparameters", "text": "All models were fine-tuned starting from T5 1.1 checkpoints 6 . We fine-tune T5 models on 32 Cloud TPU v3 cores 7 . Fine-tuning takes less than 8 hours for all models.\nDual Encoder. We used the t5x_retrieval library 8 for implementing dual encoder models. We tuned some parameters based on results on the validation set. Relevant hyperparameters for training the dual encoder are:\n\u2022 Learning Rate: 1e-3 For the T5 input we concatenated the query and truncated document text. The T5 output is the string \"relevant\" or \"not relevant\". To classify document relevance at inference time, we applied a threshold to the probability assigned to the \"relevant\" label, which we tuned on the validation set. When classifying BM25 candidates we used a threshold of 0.9 and when classifying the dual encoder candidates we used a threshold of 0.95.\nOther  \np = r A * (1 \u2212 r \u2229 ) * |A| r A * (1 \u2212 r B * r \u2229 ) * |A| p = (1 \u2212 r \u2229 ) (1 \u2212 r B * r \u2229 )\nDiscussion While recall is simply equal to r A , precision is a more complicated function of r B and r \u2229 , and can be very low for large values of r \u2229 . Intuitively, if subtractingB from\u00c2 removes most of\u00c2, then the precision of the resulting set will be dominated by the relevant entities missing fromB. This motivates limiting the intersection of the two sets used to construct queries involving set intersection. For example, if r B = 0.95, then with r \u2229 < 0.8, we can ensure p > 0.83.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C Annotation Details", "text": "The annotation tasks in QUEST were carried out by participants who were paid contractors. They are based in Austin, TX and either have a bachelor's degree (55%) or equivalent work experience (45%). They were paid by the hour for their work and were recruited from a vendor who screened them for knowledge of US English. They were informed of how their work would be used and could opt out. They received a standard contracted wage, which complies with living wage laws in their country of employment. The annotation interfaces presented to the annotators are shown in Figures 4, 5 and 6.", "publication_ref": [], "figure_ref": ["fig_10"], "table_ref": []}, {"heading": "D Impact of Memorization of Pre-training Data", "text": "Since the T5 checkpoints we use to initialize our models were pre-trained on the C4 corpus (which includes Wikipedia), we investigate whether these models have memorized aspects of the Wikipedia category graph. We compare recall of the T5-based dual encoder model for Wikipedia documents that were created prior to the pre-training date of the T5 checkpoint compared with documents that were added after pre-training. We report these in Table 7, along with the recalls for the same sets of documents with a BM25 retriever, for a baseline  comparison. We note that the ratio of scores between the documents added before pre-training to documents added after pre-training is similar for both systems, which suggests factors other than memorization may explain the difference. For example, the documents created before vs. after the pre-training date have average lengths of 759.7 vs. 441.2 words, respectively.   ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We would like to thank Isabel Kraus-Liang, Mahesh Maddinala, Andrew Smith, Daphne Domansi, and all the annotators for their work. We would also like to thank Mark Yatskar, Dan Roth, Zhuyun Dai,  Jianmo Ni, William Cohen, Andrew McCallum,  Shib Sankar Dasgupta and Nicholas Fitzgerald for  useful discussions.   ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "We will release the code and our dataset publicly.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B1. Did you cite the creators of artifacts you used?", "text": "Appendix A B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nYes, we will use an MIT license to release our dataset.\nB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? 3 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? 3 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? 3 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. 3 C Did you run computational experiments? 4 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? Appendix A\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\nC2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? Section 4 and Appendix A C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? Appendix A C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)? Appendix A D Did you use human annotators (e.g., crowdworkers) or research with human participants? Appendix C D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? Appendix C D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)? Appendix C D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? Appendix C D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nLeft blank. D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? Appendix C", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Qampari:: An open-domain question answering benchmark for questions with many answers from multiple paragraphs", "journal": "ArXiv", "year": "2022", "authors": "Ohad Samuel Joseph Amouyal; Ori Rubin; Tomer Yoran; Jonathan Wolfson; Jonathan Herzig;  Berant"}, {"ref_id": "b1", "title": "Longformer: The long-document transformer", "journal": "", "year": "2020", "authors": "Iz Beltagy; Matthew E Peters; Arman Cohan"}, {"ref_id": "b2", "title": "Semantic parsing on Freebase from question-answer pairs", "journal": "", "year": "2013", "authors": "Jonathan Berant; Andrew Chou; Roy Frostig; Percy Liang"}, {"ref_id": "b3", "title": "Overview of the TREC 2019 deep learning track", "journal": "", "year": "2020", "authors": "Nick Craswell; Bhaskar Mitra; Emine Yilmaz; Daniel Campos; Ellen M Voorhees"}, {"ref_id": "b4", "title": "Deeper text understanding for ir with contextual neural language modeling", "journal": "", "year": "2019", "authors": "Zhuyun Dai; Jamie Callan"}, {"ref_id": "b5", "title": "Beyond iid: three levels of generalization for question answering on knowledge bases", "journal": "", "year": "2021", "authors": "Yu Gu; Sue Kase; Michelle Vanni; Brian Sadler; Percy Liang; Xifeng Yan; Yu Su"}, {"ref_id": "b6", "title": "LongT5: Efficient text-to-text transformer for long sequences", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Mandy Guo; Joshua Ainslie; David Uthus; Santiago Ontanon; Jianmo Ni; Yun-Hsuan Sung; Yinfei Yang"}, {"ref_id": "b7", "title": "Balint Miklos, and Ray Kurzweil. 2017. Efficient natural language response suggestion for smart reply", "journal": "", "year": null, "authors": "Matthew Henderson; Rami Al-Rfou; Brian Strope; Yun-Hsuan Sung; L\u00e1szl\u00f3 Luk\u00e1cs; Ruiqi Guo"}, {"ref_id": "b8", "title": "Measuring compositional generalization: A comprehensive method on realistic data", "journal": "", "year": "2020-04-26", "authors": "Daniel Keysers; Nathanael Sch\u00e4rli; Nathan Scales; Hylke Buisman; Daniel Furrer; Sergii Kashubin; Nikola Momchev; Danila Sinopalnikov; Lukasz Stafiniak; Tibor Tihon; Dmitry Tsarkov; Xiao Wang; Olivier Marc Van Zee;  Bousquet"}, {"ref_id": "b9", "title": "Looking beyond the surface: A challenge set for reading comprehension over multiple sentences", "journal": "", "year": "2018", "authors": "Daniel Khashabi; Snigdha Chaturvedi; Michael Roth; Shyam Upadhyay; Dan Roth"}, {"ref_id": "b10", "title": "Multi-aspect dense retrieval", "journal": "", "year": "2022", "authors": "Weize Kong; Swaraj Khadanga; Cheng Li; Shaleen Gupta; Mingyang Zhang; Wensong Xu; Mike Bendersky"}, {"ref_id": "b11", "title": "Natural questions: A benchmark for question answering research", "journal": "Transactions of the Association for Computational Linguistics", "year": "2019", "authors": "Tom Kwiatkowski; Jennimaria Palomaki; Olivia Redfield; Michael Collins; Ankur Parikh; Chris Alberti; Danielle Epstein; Illia Polosukhin; Jacob Devlin; Kenton Lee; Kristina Toutanova; Llion Jones; Matthew Kelcey; Ming-Wei Chang; Andrew M Dai; Jakob Uszkoreit; Quoc Le; Slav Petrov"}, {"ref_id": "b12", "title": "QED: A Framework and Dataset for Explanations in Question Answering", "journal": "Transactions of the Association for Computational Linguistics", "year": "2021", "authors": "Matthew Lamm; Jennimaria Palomaki; Chris Alberti; Daniel Andor; Eunsol Choi; Livio Baldini Soares; Michael Collins"}, {"ref_id": "b13", "title": "A survey on complex knowledge base question answering: Methods, challenges and solutions", "journal": "", "year": "2021", "authors": "Yunshi Lan; Gaole He; Jinhao Jiang; Jing Jiang; Wayne Xin Zhao; Ji-Rong Wen"}, {"ref_id": "b14", "title": "Joint passage ranking for diverse multi-answer retrieval", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Sewon Min; Kenton Lee; Ming-Wei Chang; Kristina Toutanova; Hannaneh Hajishirzi"}, {"ref_id": "b15", "title": "Ms marco: A human generated machine reading comprehension dataset", "journal": "", "year": "2016", "authors": "Tri Nguyen; Mir Rosenberg; Xia Song; Jianfeng Gao; Saurabh Tiwary; Rangan Majumder; Li Deng"}, {"ref_id": "b16", "title": "Large dual encoders are generalizable retrievers", "journal": "", "year": "2022", "authors": "Jianmo Ni; Chen Qu; Jing Lu; Zhuyun Dai; Gustavo Hernandez Abrego; Ji Ma; Vincent Zhao; Yi Luan; Keith Hall; Ming-Wei Chang; Yinfei Yang"}, {"ref_id": "b17", "title": "Passage re-ranking with bert", "journal": "", "year": "2019", "authors": "Rodrigo Nogueira; Kyunghyun Cho"}, {"ref_id": "b18", "title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "Journal of Machine Learning Research", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b19", "title": "Measuring attribution in natural language generation models", "journal": "", "year": "2021", "authors": "Vitaly Hannah Rashkin; Matthew Nikolaev; Lora Lamm; Michael Aroyo; Dipanjan Collins; Slav Das; Gaurav Petrov; Iulia Singh Tomar; David Turc;  Reitter"}, {"ref_id": "b20", "title": "The probabilistic relevance framework: Bm25 and beyond", "journal": "Foundations and Trends\u00ae in Information Retrieval", "year": "2009", "authors": "Stephen Robertson; Hugo Zaragoza"}, {"ref_id": "b21", "title": "Report on the need for and provision of an ideal information retrieval test collection", "journal": "", "year": "1975", "authors": "K ; Sparck Jones; C J Van Rijsbergen"}, {"ref_id": "b22", "title": "PullNet: Open domain question answering with iterative retrieval on knowledge bases and text", "journal": "", "year": "2019", "authors": "Haitian Sun; Tania Bedrax-Weiss; William Cohen"}, {"ref_id": "b23", "title": "The web as a knowledge-base for answering complex questions", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Alon Talmor; Jonathan Berant"}, {"ref_id": "b24", "title": "BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models", "journal": "", "year": "2021", "authors": "Nandan Thakur; Nils Reimers; Andreas R\u00fcckl\u00e9"}, {"ref_id": "b25", "title": "Probabilistic embedding of knowledge graphs with box lattice measures", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Luke Vilnis; Xiang Li; Shikhar Murty; Andrew Mc-Callum"}, {"ref_id": "b26", "title": "Question answering from unstructured text by retrieval and comprehension", "journal": "", "year": "2017", "authors": "Yusuke Watanabe; Bhuwan Dhingra; Ruslan Salakhutdinov"}, {"ref_id": "b27", "title": "HotpotQA: A dataset for diverse, explainable multi-hop question answering", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Zhilin Yang; Peng Qi; Saizheng Zhang; Yoshua Bengio; William Cohen; Ruslan Salakhutdinov; Christopher D Manning"}, {"ref_id": "b28", "title": "The value of semantic parse labeling for knowledge base question answering", "journal": "Short Papers", "year": "2016", "authors": "Matthew Wen-Tau Yih; Chris Richardson; Ming-Wei Meek; Jina Chang;  Suh"}, {"ref_id": "b29", "title": "RoMQA: A benchmark for robust, multi-evidence, multi-answer question answering", "journal": "", "year": "2022", "authors": "Victor Zhong; Weijia Shi; Wen-Tau Yih; Luke Zettlemoyer"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: The dataset construction process for QUEST. First, (1) we sample Wikipedia category names and find their corresponding set of relevant entities. (2) Then, we compose a query with set operations and have this query paraphrased by crowdworkers. (3) These queries are then validated for fluency and naturalness. (4) Finally, crowdworkers mark the entities' relevance by highlighting attributable spans in their documents.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: An example of a query and relevant entity from QUEST. The attribution for different query constraints can come from different parts of the document.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "137morand Berant, 2018;Keysers et al., 2020;Gu 138 et al., 2021, inter alia). These benchmarks re-139 quire retrieval of a set of entities that exist as nodes 140 or relations in an accompanying knowledge base. 141 Questions are optionally supplemented with logical 142 forms. Lan et al. (2021) provide a comprehensive 143 2 erations are not well represented in existing retrieval benchmarks such as MSMarco", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Query:Supernatural horror films set in Nevada Entity: Willy's Wonderland Document Text: Willy's Wonderland is a 2021 American action comedy horror film directed by Kevin Lewis from a screenplay by G. O. Parsons. The film stars Nicolas Cage, who also [...] It follows a quiet drifter who is tricked into cleaning up an abandoned family entertainment center haunted by eight murderous animatronic characters. The project was announced in October 2019, [...] When his car catches a flat tire on a remote country road, a quiet drifter ends up stranded outside of Hayesville, Nevada. Mechanic Jed Love picks [...]", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 2 :2Figure 2: An example of a query and relevant entity from QUEST. The attribution for different query constraints can come from different parts of the document.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 2 :2Figure 2: An example of a query and relevant entity from QUEST. The attribution for different query constraints can come from different parts of the document.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 3 :3Figure3: We compare several systems consisting of a retriever for efficiently selecting a set of candidates from the document corpus and a document relevance classifier for determining the final predicted document set.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "|A| \u2212 r A * r \u2229 * |A| |A| \u2212 (r \u2229 * |A|) r = r A * (1 \u2212 r \u2229 ) * |A| (1 \u2212 r \u2229 ) * |A| r = r AAnd for precision:p = |(A \\ B) \u2229 (\u00c2 \\B)| |(\u00c2 \\B)| |A| \u2212 r A * r \u2229 * |A| r A * |A| \u2212 r A * r B * r \u2229 * |A|", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 4 :4Figure 4: Annotation interface for the paraphrasing stage.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 5 :5Figure 5: Annotation interface for the validation stage.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Figure 6 :6Figure 6: Annotation interface for the relevance labeling stage.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Dutch crime comedy or romantic comedy films A \u2229 B Italian crime films set in the 1970's Films A \\ B Indian sport films that are not about cricket A \u222a B \u222a C Dutch or Swiss war films, or war films from 1945 A \u2229 B \u2229 C 2020's drama films shot in cleveland A \u2229 B \\ C Epic films about Christianity not set in Israel Moths or Insects or Arthropods of Guadeloupe A \u2229 B \u2229 C Plants the Arctic, the United Kingdom, and the Caucasus have in common A \u2229 B \\ C", "figure_data": "DomainTemplate AExample Biographical Italian bandits filmsNum. QueriesA \u222a BBooks PlantsA A \u222a B A \u2229 B A \\ B A \u222a B \u222a C A \u2229 B \u2229 C A \u2229 B \\ C A A \u222a B A \u2229 B A \\ B A \u222a B \u222a C2004 German novels 1925 Russian novels or Novels by Ivan Bunin 1991 Novels set in Iceland Novels set in the 1900s not based on real events Novels set in Nanjing, Hebei, or Jiangsu English language Harper & Brothers Children's fiction books Novels that take place in Vietnam that aren't about war plants only from Gabon Trees of Manitoba or Subarctic America Shrubs used in traditional Native American medicine Trees from the Northwestern US that can't be found in Canada61AnimalsA A \u222a B A \u2229 B A \\ BOrchids of Indonesia and Malaysia but not Thailand what are the Rodents of Cambodia Animals from Cuba or Jamaica that are extinct Neogene mammals of Africa that are Odd-toed ungulates Non-Palearctic birds of Mongolia"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Statistics of examples in QUEST across different domains.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Average Precision, Recall, and F1 of baseline systems evaluated on the test dataset.", "figure_data": "Avg. Recall@KMRecall@KRetriever2050100100020501001000BM250.104 0.153 0.197 0.395 0.020 0.030 0.037 0.087T5-Base DE 0.255 0.372 0.455 0.726 0.045 0.088 0.127 0.360T5-Large DE 0.265 0.386 0.476 0.757 0.047 0.100 0.142 0.408"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Average Recall and MRecall of various retrievers.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": ": F1 of our strongest baseline (T5-Large DE + T5-Large Classifier) across templates and domains."}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "", "figure_data": ": Analysis of false positive errors from the T5-Large classifier and cases where a non-relevant docu-ment was ranked ahead of a relevant one for the T5-Large dual encoder. For queries with conjunction, we determined the percentage of cases where 1, 2, or 3 constraints in the template were not satisfied by the predicted document (# Constraints). For queries with negation, we measured the percentage of cases where the negated constraint (Neg.) was not satisfied."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Let us assume we have two sets derived from the Wikipedia category graph,\u00c2 andB. The Wikipedia category graph can be missing some relevant entities, such that A \u2282 A andB \u2282 B, where A and B are interpreted as the hypothetical sets containing all relevant entities. We quantify the degree of missing entities by denoting recall as r A and r B , such that |\u00c2| = r A * |A| and |B| = r B * |B|. We quantify the fraction of elements in A that are also in B as r \u2229 , such that |A \u2229 B| = r \u2229 * |A|. For simplicity, we also assume that the overlap between A andB is such that |\u00c2 \u2229 B| = r A * |A \u2229 B| and |\u00c2 \u2229B| = r A * r B * |A \u2229 B|. Derivation What is the recall (r) and precision (p) of\u00c2 \\B relative to A \\ B as a function of r A , r B , and r \u2229 ?First, we derive this function for recall: 9", "figure_data": "\u2022 Learning Rate: 1e-3\u2022 Warmup Steps: 1000\u2022 Finetuning Steps: 10000\u2022 Batch Size: 1024\u2022 Max Source Length: 512\u2022 Max Target Length: 16B Set Difference and RecallNotation and Assumptionsrelevant hyperparameters for training theclassifier are:6 https://github.com/google-research/t5x/blob/main/docs/models.md 7 https://cloud.google.com/tpu/ 8 https://github.com/google-research/t5x_retrieval"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Average recall@100 on the subsets of documents created before vs after T5 pre-training.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "p = r A * (1 \u2212 r \u2229 ) * |A| r A * (1 \u2212 r B * r \u2229 ) * |A| p = (1 \u2212 r \u2229 ) (1 \u2212 r B * r \u2229 )", "formula_coordinates": [13.0, 115.05, 182.51, 128.69, 69.72]}], "doi": "10.18653/v1/2022.findings-naacl.55"}