{"title": "Statistics-Based Summarization -Step One: Sentence Compression", "authors": "Kevin Knight; Daniel Marcu", "pub_date": "", "abstract": "When humans produce summaries of documents, they do not simply extract sentences and concatenate them. Rather, they create new sentences that are grammatical, that cohere with one another, and that capture the most salient pieces of information in the original document. Given that large collections of text/abstract pairs are available online, it is now possible to envision algorithms that are trained to mimic this process. In this paper, we focus on sentence compression, a simpler version of this larger challenge. We aim to achieve two goals simultaneously: our compressions should be grammatical, and they should retain the most important pieces of information. These two goals can conflict. We devise both noisy-channel and decision-tree approaches to the problem, and we evaluate results against manual compressions and a simple baseline.", "sections": [{"heading": "Introduction", "text": "Most of the research in automatic summarization has focused on extraction, i.e., on identifying the most important clauses/sentences/paragraphs in texts (see (Mani & Maybury 1999) for a representative collection of papers). However, determining the most important textual segments is only half of what a summarization system needs to do because, in most cases, the simple catenation of textual segments does not yield coherent outputs. Recently, a number of researchers have started to address the problem of generating coherent summaries: McKeown et al. (1999), , and Jing and McKeown (1999) in the context of multidocument summarization;  in the context of revising single document extracts; and Witbrock and Mittal (1999) in the context of headline generation.\nThe approach proposed by Witbrock and Mittal (1999) is the only one that applies a probabilistic model trained directly on Headline, Document pairs. However, this model has yet to scale up to generating multiple-sentence abstracts as well as well-formed, grammatical sentences. All other approaches employ sets of manually written or semi-automatically derived Copyright c 2000, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.\nrules for deleting information that is redundant, compressing long sentences into shorter ones, aggregating sentences, repairing reference links, etc.\nOur goal is also to generate coherent abstracts. However, in contrast with the above work, we intend to eventually use Abstract, Text tuples, which are widely available, in order to automatically learn how to rewrite Texts as coherent Abstracts. In the spirit of the work in the statistical MT community, which is focused on sentence-to-sentence translations, we also decided to focus first on a simpler problem, that of sentence compression. We chose this problem for two reasons:\n\u2022 First, the problem is complex enough to require the development of sophisticated compression models: Determining what is important in a sentence and determining how to convey the important information grammatically, using only a few words, is just a scaled down version of the text summarization problem. Yet, the problem is simple enough, since we do not have to worry yet about discourse related issues, such as coherence, anaphors, etc.\n\u2022 Second, an adequate solution to this problem has an immediate impact on several applications. For example, due to time and space constraints, the generation of TV captions often requires only the most important parts of sentences to be shown on a screen (Linke-Ellis 1999;Robert-Ribes et al. 1999).\nA good sentence compression module would therefore have an impact on the task of automatic caption generation. A sentence compression module can also be used to provide audio scanning services for the blind (Grefenstette 1998). In general, since all systems aimed at producing coherent abstracts implement manually written sets of sentence compression rules (McKeown et al. 1999;Mani, Gates, & Bloedorn 1999;Barzilay, McKeown, & Elhadad 1999), it is likely that a good sentence compression module would impact the overall quality of these systems as well. This becomes particularly important for text genres that use long sentences.\nIn this paper, we present two approaches to the sentence compression problem. Both take as input a sequence of words W = w 1 , w 2 , . . . , w n (one sentence).\nFrom: AAAI-00 Proceedings. Copyright \u00a9 2000, AAAI (www.aaai.org). All rights reserved.\nAn algorithm may drop any subset of these words. The words that remain (order unchanged) form a compression. There are 2 n compressions to choose from-some are reasonable, most are not. Our first approach develops a probabilistic noisy-channel model for sentence compression. The second approach develops a decisionbased, deterministic model.", "publication_ref": ["b14", "b17", "b9", "b20", "b20", "b12", "b19", "b7", "b17", "b15", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "A noisy-channel model for sentence compression", "text": "This section describes a probabilistic approach to the compression problem. In particular, we adopt the noisy channel framework that has been relatively successful in a number of other NLP applications, including speech recognition (Jelinek 1997), machine translation (Brown et al. 1993), part-of-speech tagging (Church 1988), transliteration (Knight & Graehl 1998), and information retrieval (Berger & Lafferty 1999).\nIn this framework, we look at a long string and imagine that (1) it was originally a short string, and then (2) someone added some additional, optional text to it. Compression is a matter of identifying the original short string. It is not critical whether or not the \"original\" string is real or hypothetical. For example, in statistical machine translation, we look at a French string and say, \"This was originally English, but someone added 'noise' to it.\" The French may or may not have been translated from English originally, but by removing the noise, we can hypothesize an English source-and thereby translate the string. In the case of compression, the noise consists of optional text material that pads out the core signal. For the larger case of text summarization, it may be useful to imagine a scenario in which a news editor composes a short document, hands it to a reporter, and tells the reporter to \"flesh it out\" . . . which results in the article we read in the newspaper. As summarizers, we may not have access to the editor's original version (which may or may not exist), but we can guess at itwhich is where probabilities come in.\nAs in any noisy channel application, we must solve three problems:\n\u2022 Source model. We must assign to every string s a probability P(s), which gives the chance that s is generated as an \"original short string\" in the above hypothetical process. For example, we may want P(s) to be very low if s is ungrammatical.\n\u2022 Channel model. We assign to every pair of strings s, t a probability P(t | s), which gives the chance that when the short string s is expanded, the result is the long string t. For example, if t is the same as s except for the extra word \"not,\" then we may want P(t | s) to be very low. The word \"not\" is not optional, additional material.\n\u2022 Decoder. When we observe a long string t, we search for the short string s that maximizes P(s | t). This is equivalent to searching for the s that maximizes P(s)\n\u2022 P (t | s).\nIt is advantageous to break the problem down this way, as it decouples the somewhat independent goals of creating a short text that (1) looks grammatical, and (2) preserves important information. It is easier to build a channel model that focuses exclusively on the latter, without having to worry about the former. That is, we can specify that a certain substring may represent unimportant information, but we do not need to worry that deleting it will result in an ungrammatical structure. We leave that to the source model, which worries exclusively about well-formedness. In fact, we can make use of extensive prior work in source language modeling for speech recognition, machine translation, and natural language generation. The same goes for actual compression (\"decoding\" in noisy-channel jargon)-we can re-use generic software packages to solve problems in all these application domains.", "publication_ref": ["b8", "b3", "b4", "b10", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Statistical Models", "text": "In the experiments we report here, we build very simple source and channel models. In a departure from the above discussion and from previous work on statistical channel models, we assign probabilities P tree (s) and P expand tree (t | s) to trees rather than strings. In decoding a new string, we first parse it into a large tree t (using Collins' parser (1997)), and we then hypothesize and rank various small trees.\nGood source strings are ones that have both (1) a normal-looking parse tree, and (2) normal-looking word pairs. P tree (s) is a combination of a standard probabilistic context-free grammar (PCFG) score, which is computed over the grammar rules that yielded the tree s, and a standard word-bigram score, which is computed over the leaves of the tree. For example, the tree s =(S (NP John) (VP (VB saw) (NP Mary))) is assigned a score based on these factors:\nP tree (s) = P(TOP \u2192 S | TOP) \u2022 P(S \u2192 NP VP | S) \u2022 P(NP \u2192 John | NP) \u2022 P(VP \u2192 VB NP | VP) \u2022 P(VP \u2192 saw | VB) \u2022 P(NP \u2192 Mary | NP) \u2022 P(John | EOS) \u2022 P(saw | John) \u2022 P(Mary | saw) \u2022 P(EOS | Mary)\nOur stochastic channel model performs minimal operations on a small tree s to create a larger tree t. For each internal node in s, we probabilistically choose an expansion template based on the labels of the node and its children. For example, when processing the S node in the tree above, we may wish to add a prepositional phrase as a third child. We do this with probability P(S \u2192 NP VP PP | S \u2192 NP VP). Or we may choose to leave it alone, with probability P(S \u2192 NP VP | S \u2192 NP VP). After we choose an expansion template, then for each new child node introduced (if any), we grow a new subtree rooted at that node-for example (PP (P in) (NP Pittsburgh)). Any particular subtree is grown with probability given by its PCFG factorization, as above (no bigrams). ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Example", "text": "In this section, we show how to tell whether one potential compression is more likely than another, according to the statistical models described above. Suppose we observe the tree t in Figure 1, which spans the string abcde. Consider the compression s1, which is shown in the same figure.\nWe compute the factors P tree (s1) and P expand tree (t | s1). Breaking this down further, the source PCFG and word-bigram factors, which describe P tree (s1), are:\nP(TOP \u2192 G | TOP) P(H \u2192 a | H) P(G \u2192 H A | G) P(C \u2192 b | C) P(A \u2192 C D | A) P(D \u2192 e | D) P(a | EOS) P(e | b) P(b | a) P(EOS | e)\nThe channel expansion-template factors and the channel PCFG (new tree growth) factors, which describe P expand tree (t | s1), are:\nP(G \u2192 H A | G \u2192 H A) P(A \u2192 C B D | A \u2192 C D) P(B \u2192 Q R | B) P(Z \u2192 c | Z) P(Q \u2192 Z | Q) P(R \u2192 d | R)\nA different compression will be scored with a different set of factors. For example, consider a compression of t that leaves t completely untouched. In that case, the source costs P tree (t) are:\nP(TOP \u2192 G | TOP) P(H \u2192 a | H) P(a | EOS) P(G \u2192 H A | G) P(C \u2192 b | C) P(b | a) P(A \u2192 C D | A) P(Z \u2192 c | Z) P(c | b) P(B \u2192 Q R | B) P(R \u2192 d | R) P(d | c) P(Q \u2192 Z | Q) P(D \u2192 e | D) P(e | d) P(EOS | e)\nThe channel costs P expand tree (t | t) are:\nThe documentation is typical of Epson quality: excellent. Documentation is excellent.\nAll of our design goals were achieved and the delivered performance matches the speed of the underlying device. All design goals were achieved.\nReach's E-mail product, MailMan, is a message-management system designed initially for VINES LANs that will eventually be operating system-independent. MailMan will eventually be operating system-independent.\nAlthough the modules themselves may be physically and/or electrically incompatible, the cable-specific jacks on them provide industry-standard connections.\nCable-specific jacks provide industry-standard connections.\nIngres/Star prices start at $2,100.\nIngres/Star prices start at $2,100.\nFigure 2: Examples from our parallel corpus.\nP(G \u2192 H A | G \u2192 H A) P(A \u2192 C B D | A \u2192 C B D) P(B \u2192 Q R | B \u2192 Q R) P(Q \u2192 Z | Q \u2192 Z)\nNow we can simply compare P expand tree (s1 | t) = P tree (s1) \u2022 P expand tree (t | s1))/P tree (t) versus P expand tree (t | t) = P tree (t) \u2022 P expand tree (t | t))/P tree (t) and select the more likely one. Note that P tree (t) and all the PCFG factors can be canceled out, as they appear in any potential compression. Therefore, we need only compare compressions of the basis of the expansion-template probabilities and the word-bigram probabilities. The quantities that differ between the two proposed compressions are boxed above. Therefore, s1 will be preferred over t if and only if:\nP(e | b) \u2022 P(A \u2192 C B D | A \u2192 C D) > P(b | a) \u2022 P(c | b) \u2022 P(d | c) \u2022 P(A \u2192 C B D | A \u2192 C B D) \u2022 P(B \u2192 Q R | B \u2192 Q R) \u2022 P(Q \u2192 Z | Q \u2192 Z)", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Training Corpus", "text": "In order to train our system, we used the Ziff-Davis corpus, a collection of newspaper articles announcing computer products. Many of the articles in the corpus are paired with human written abstracts. We automatically extracted from the corpus a set of 1067 sentence pairs. Each pair consisted of a sentence t = t 1 , t 2 , . . . , t n that occurred in the article and a possibly compressed version of it s = s 1 , s 2 , . . . , s m , which occurred in the human written abstract. Figure 2 shows a few sentence pairs extracted from the corpus.\nWe decided to use such a corpus because it is consistent with two desiderata specific to summarization work: (i) the human-written Abstract sentences are grammatical; (ii) the Abstract sentences represent in a compressed form the salient points of the original newspaper Sentences. We decided to keep in the corpus uncompressed sentences as well, since we want to learn not only how to compress a sentence, but also when to do it.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Learning Model Parameters", "text": "We collect expansion-template probabilities from our parallel corpus. We first parse both sides of the parallel corpus, and then we identify corresponding syntactic nodes. For example, the parse tree for one sentence may begin (S (NP . . . ) (VP . . . ) (PP . . . )) while the parse tree for its compressed version may begin (S (NP . . . ) (VP . . . )). If these two S nodes are deemed to correspond, then we chalk up one joint event (S \u2192 NP VP, S \u2192 NP VP PP); afterwards we normalize. Not all nodes have corresponding partners; some noncorrespondences are due to incorrect parses, while others are due to legitimate reformulations that are beyond the scope of our simple channel model. We use standard methods to estimate word-bigram probabilities.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Decoding", "text": "There is a vast number of potential compressions of a large tree t, but we can pack them all efficiently into a shared-forest structure. For each node of t that has n children, we \u2022 generate 2 n \u2212 1 new nodes, one for each non-empty subset of the children, and \u2022 pack those nodes so that they are referred to as a whole. For example, consider the large tree t above. All compressions can be represented with the following forest:\nG \u2192 H A B \u2192 R A\u2192 B C H \u2192 a G \u2192 H Q \u2192 Z A\u2192 C C \u2192 b G \u2192 A A \u2192 C B D A \u2192 B Z \u2192 c B \u2192 Q R A \u2192 C B A \u2192 D R \u2192 d B \u2192 Q A\u2192 C D D \u2192 e\nWe can also assign an expansion-template probability to each node in the forest. For example, to the B \u2192 Q node, we can assign P(B \u2192 Q R | B \u2192 Q). If the observed probability from the parallel corpus is zero, then we assign a small floor value of 10 \u22126 . In reality, we produce forests that are much slimmer, as we only consider compressing a node in ways that are locally grammatical according to the Penn Treebank-if a rule of the type A \u2192 C B has never been observed, then it will not appear in the forest.\nAt this point, we want to extract a set of highscoring trees from the forest, taking into account both expansion-template probabilities and word-bigram probabilities. Fortunately, we have such a generic extractor on hand (Langkilde 2000). This extractor was designed for a hybrid symbolic-statistical natural language generation system called Nitrogen. In that application, a rule-based component converts an abstract semantic representation into a vast number of potential English renderings. These renderings are packed into a forest, from which the most promising sentences are extracted using statistical scoring.\nFor our purposes, the extractor selects the trees with the best combination of word-bigram and expansiontemplate scores. It returns a list of such trees, one for each possible compression length. For example, for the sentence Beyond that basic level, the operations of the three products vary, we obtain the following \"best\" compressions, with negative log-probabilities shown in parentheses (smaller = more likely):\nBeyond that basic level, the operations of the three products vary widely (1514588) Beyond that level, the operations of the three products vary widely (1430374) Beyond that basic level, the operations of the three products vary (1333437) Beyond that level, the operations of the three products vary (1249223) Beyond that basic level, the operations of the products vary (1181377) The operations of the three products vary widely ( 939912)\nThe operations of the products vary widely ( 872066)\nThe operations of the products vary (748761)\nThe operations of products vary (690915)", "publication_ref": ["b11"], "figure_ref": [], "table_ref": []}, {"heading": "Operations of products vary (809158)", "text": "The operations vary (522402)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Operations vary (662642)", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Length Selection", "text": "It is useful to have multiple answers to choose from, as one user may seek a 20% compression, while another seeks a 60% compression. However, for purposes of evaluation, we want our system to be able to select a single compression. If we rely on the log-probabilities as shown above, we will almost always choose the shortest compression. (Note above, however, how the threeword compression scores better than the two-word compression, as the models are not entirely happy removing the article \"the\"). To create a more fair competition, we divide the log-probability by the length of the compression, rewarding longer strings. This is commonly done in speech recognition.\nIf we plot this normalized score against compression length, we usually observe a (bumpy) U-shaped curve, as illustrated in Figure 3. In a typical more difficult case, a 25-word sentence may be optimally compressed by a 17-word version. Of course, if a user requires a shorter compression than that, she may select another region of the curve and look for a local minimum.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "A decision-based model for sentence compression", "text": "In this section, we describe a decision-based, history model of sentence compression. As in the noisy-channel approach, we again assume that we are given as input\nFinally another advantage of broadband is distance .\nFinally, another advantage of broadband is distance .\nAnother advantage of broadband is distance . Advantage of broadband is distance . Another advantage is distance . Advantage is distance . a parse tree t. Our goal is to \"rewrite\" t into a smaller tree s, which corresponds to a compressed version of the original sentence subsumed by t. Suppose we observe in our corpus the trees t and s2 in Figure 1. In this model, we ask ourselves how we may go about rewriting t into s2. One possible solution is to decompose the rewriting operation into a sequence of shift-reduce-drop actions that are specific to an extended shift-reduce parsing paradigm.\nIn the model we propose, the rewriting process starts with an empty Stack and an Input List that contains the sequence of words subsumed by the large tree t. Each word in the input list is labeled with the name of all syntactic constituents in t that start with it (see Figure 4). At each step, the rewriting module applies an operation that is aimed at reconstructing the smaller tree s2. In the context of our sentence-compression module, we need four types of operations:\n\u2022 shift operations transfer the first word from the input list into the stack; \u2022 reduce operations pop the k syntactic trees located at the top of the stack; combine them into a new tree; and push the new tree on the top of the stack. Reduce operations are used to derive the structure of the syntactic tree of the short sentence. \u2022 drop operations are used to delete from the input list subsequences of words that correspond to syntactic constituents. A drop x operations deletes from the input list all words that are spanned by constituent x in t. \u2022 assignType operations are used to change the label of trees at the top of the stack. These actions assign POS tags to the words in the compressed sentence, which may be different from the POS tags in the original sentence.\nThe decision-based model is more flexible than the channel model because it enables the derivation of trees whose skeleton can differ quite drastically from that of the tree given as input. For example, using the channel model, we are unable to obtain tree s2 from t. However, the four operations listed above enable us to rewrite a tree t into any tree s, as long as an in-order traversal of the leaves of s produces a sequence of words that occur in the same order as the words in the tree t. For example, the tree s2 can be obtained from tree t by following this sequence of actions, whose effects are shown in Figure 4: shift; assignType H; shift; assignType K; reduce 2 F; drop B; shift; assignType D; reduce 2 G.\nTo save space, we show shift and assignType operations on the same line; however, the reader should understand that they correspond to two distinct actions. As one can see, the assignType K operation rewrites the POS tag of the word b; the reduce operations modify the skeleton of the tree given as input.\nTo increase readability, the input list is shown in a format that resembles as closely as possible the graphical representation of the trees in figure 1.", "publication_ref": [], "figure_ref": ["fig_0", "fig_2", "fig_2", "fig_0"], "table_ref": []}, {"heading": "Learning the parameters of the decision-based model", "text": "We associate with each configuration of our shiftreduce-drop, rewriting model a learning case. The cases are generated automatically by a program that derives sequences of actions that map each of the large trees in our corpus into smaller trees. The rewriting procedure simulates a bottom-up reconstruction of the smaller trees.\nOverall, the 1067 pairs of long and short sentences yielded 46383 learning cases. Each case was labeled with one action name from a set of 210 possible actions: There are 37 distinct assignType actions, one for each POS tag. There are 63 distinct drop actions, one for each type of syntactic constituent that can be deleted during compression. There are 109 distinct reduce actions, one for each type of reduce operation that is applied during the reconstruction of the compressed sentence. And there is one shift operation. Given a tree t and an arbitrary configuration of the stack and input list, the purpose of the decision-based classifier is to learn what action to choose from the set of 210 possible actions.\nTo each learning example, we associated a set of 99 features from the following two classes:\nOperational features reflect the number of trees in the stack, the input list, and the types of the last five operations. They also encode information that denote the syntactic category of the root nodes of the partial trees built up to a certain time. Examples of such features are: num-berTreesInStack, wasPreviousOperationShift, syn-tacticLabelOfTreeAtTheTopOfStack, etc.\nOriginal-tree-specific features denote the syntactic constituents that start with the first unit in the input list. Examples of such features are: inputList-StartsWithA CC, inputListStartsWithA PP, etc.\nThe decision-based compression module uses the C4.5 program (Quinlan 1993) in order to learn decision trees that specify how large syntactic trees can be compressed into shorter trees. A ten-fold crossvalidation evaluation of the classifier yielded an accuracy of 87.16% (\u00b1 0.14). A majority baseline classifier that chooses the action shift has an accuracy of 28.72%.", "publication_ref": ["b18"], "figure_ref": [], "table_ref": []}, {"heading": "Employing the decision-based model", "text": "To compress sentences, we apply the shift-reduce-drop model in a deterministic fashion. We parse the sentence to be compressed (Collins 1997) and we initialize the input list with the words in the sentence and the syntactic constituents that \"begin\" at each word, as shown in Figure 4. We then incrementally inquire the learned classifier what action to perform, and we simulate the execution of that action. The procedure ends when the input list is empty and when the stack contains only one tree. An inorder traversal of the leaves of this tree produces the compressed version of the sentence given as input.\nSince the model is deterministic, it produces only one output. The advantage is that the compression is very fast: it takes only a few milliseconds per sentence. The disadvantage is that it does not produce a range of compressions, from which another system may subsequently choose. It is straightforward though to extend the model within a probabilistic framework by applying, for example, the techniques used by Magerman (1995).", "publication_ref": ["b5", "b13"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Evaluation", "text": "To evaluate our compression algorithms, we randomly selected 32 sentence pairs from our parallel corpus, which we will refer to as the Test Corpus. We used the other 1035 sentence pairs for training. Figure 5 shows three sentences from the Test Corpus, together with the compressions produced by humans, our compression algorithms, and a baseline algorithm that produces compressions with highest word-bigram scores. The examples are chosen so as to reflect good, average, and bad performance cases. The first sentence is compressed in the same manner by humans and our algorithms (the baseline algorithm chooses though not to compress this sentence). For the second example, the output of the Decision-based algorithm is grammatical, but the semantics is negatively affected. The noisy-channel algorithm deletes only the word \"break\", which affects the correctness of the output less. In the last example, the noisy-channel model is again more conservative and decides not to drop any constituents. In constrast, the decision-based algorithm compresses the input substantially, but it fails to produce a grammatical output.\nWe presented each original sentence in the Test Corpus to four judges, together with four compressions of it: the human generated compression, the outputs of the noisy-channel and decision-based algorithms, and the output of the baseline algorithm. The judges were told that all outputs were generated automatically. The order of the outputs was scrambled randomly across test cases.\nTo avoid confounding, the judges participated in two experiments. In the first experiment, they were asked to determine on a scale from 1 to 5 how well the systems did with respect to selecting the most important words in the original sentence. In the second experiment, they were asked to determine on a scale from 1 to 5 how grammatical the outputs were.\nWe also investigated how sensitive our algorithms are with respect to the training data by carrying out the same experiments on sentences of a different genre, the scientific one. To this end, we took the first sentence of the first 26 articles made available in 1999 on the cmplg archive. We created a second parallel corpus, which we will refer to as the Cmplg Corpus, by generating by ourselves compressed grammatical versions of these sentences. Since some of the sentences in this corpus were extremely long, the baseline algorithm could not produce compressed versions in reasonable time.\nThe results in Table 1 show compression rates, and mean and standard deviation results across all judges, for each algorithm and corpus. The results show that the decision-based algorithm is the most aggressive: on average, it compresses sentences to about half of their original size. The compressed sentences produced by both algorithms are more \"grammatical\" and contain more important words than the sentences produced by the baseline. T -test experiments showed these differences to be statistically significant at p < 0.01 both for individual judges and for average scores across", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "all judges. T -tests showed no significant statistical differences between the two algorithms. As Table 1 shows, the performance of the compression algorithms is much closer to human performance than baseline performance; yet, humans perform statistically better than our algorithms at p < 0.01.\nWhen applied to sentences of a different genre, the performance of the noisy-channel compression algorithm degrades smoothly, while the performance of the decision-based algorithm drops sharply. This is due to a few sentences in the Cmplg Corpus that the decisionbased algorithm over-compressed to only two or three words. We suspect that this problem can be fixed if the decision-based compression module is extended in the style of Magerman (1995), by computing probabilities across the sequences of decisions that correspond to a compressed sentence. Likewise, there are substantial gains to be had in noisy-channel modeling-we see clearly in the data many statistical dependencies and processes that are not captured in our simple initial models. More grammatical output will come from taking account of subcategory and head-modifier statistics (in addition to simple word-bigrams), and an expanded channel model will allow for more tree manipulation possibilities. Work on extending the algorithms presented in this paper to compressing multiple sentences is currently underway.", "publication_ref": ["b13"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Baseline: Arborscan and worked in, but it very large dxf. Noisy-channel: Arborscan is reliable and worked accurately in testing, but it produces very large dxf files. Decision-based: Arborscan is reliable and worked accurately in testing very large dxf files. Humans: Arborscan produces very large dxf files. Original: Many debugging features, including user-defined break points and variable-watching and message-watching windows, have been added. Baseline: Debugging, user-defined and variable-watching and message-watching, have been. Noisy-channel: Many debugging features, including user-defined points and variable-watching and message-watching windows", "journal": "", "year": "", "authors": ""}, {"ref_id": "b1", "title": "for Computational Linguistics (ACL-99)", "journal": "", "year": "1999", "authors": "R Barzilay; K Mckeown; M Elhadad"}, {"ref_id": "b2", "title": "Information retrieval as statistical translation", "journal": "", "year": "1999", "authors": "A Berger; J Lafferty"}, {"ref_id": "b3", "title": "The mathematics of statistical machine translation: Parameter estimation", "journal": "Computational Linguistics", "year": "1993", "authors": "P Brown; S Della Pietra; V Della Pietra; R Mercer"}, {"ref_id": "b4", "title": "A stochastic parts program and noun phrase parser for unrestricted text", "journal": "", "year": "1988", "authors": "K Church"}, {"ref_id": "b5", "title": "Three generative, lexicalized models for statistical parsing", "journal": "", "year": "1997", "authors": "M Collins"}, {"ref_id": "b6", "title": "Annual Meeting of the Association for Computational Linguistics (ACL-97)", "journal": "", "year": "", "authors": ""}, {"ref_id": "b7", "title": "Producing intelligent tele", "journal": "", "year": "1998", "authors": "G Grefenstette"}, {"ref_id": "b8", "title": "Statistical Methods for Speech Recognition", "journal": "The MIT Press", "year": "1997", "authors": "F Jelinek"}, {"ref_id": "b9", "title": "The decomposition of human-written summary sentences", "journal": "", "year": "1999", "authors": "H Jing; K Mckeown"}, {"ref_id": "b10", "title": "Machine transliteration", "journal": "Computational Linguistics", "year": "1998", "authors": "K Knight; J Graehl"}, {"ref_id": "b11", "title": "Forest-based statistical sentence generation", "journal": "Association for Computational Linguistics", "year": "2000", "authors": "I Langkilde"}, {"ref_id": "b12", "title": "Closed captioning in America: Looking beyond compliance", "journal": "", "year": "1999", "authors": "N Linke-Ellis"}, {"ref_id": "b13", "title": "Statistical decision-tree models for parsing", "journal": "", "year": "1995", "authors": "D Magerman"}, {"ref_id": "b14", "title": "Advances in Automatic Text Summarization", "journal": "The MIT Press", "year": "1999", "authors": "I Mani; M Maybury"}, {"ref_id": "b15", "title": "Improving summaries by revising them", "journal": "", "year": "1999", "authors": "I Mani; B Gates; E Bloedorn"}, {"ref_id": "b16", "title": "Annual Meeting of the Association for Computational Linguistics", "journal": "", "year": "", "authors": ""}, {"ref_id": "b17", "title": "Towards multidocument summarization by reformulation: Progress and prospects", "journal": "", "year": "1999", "authors": "K Mckeown; J Klavans; V Hatzivassiloglou; R Barzilay; E Eskin"}, {"ref_id": "b18", "title": "C4.5: Programs for Machine Learning", "journal": "Morgan Kaufmann Publishers", "year": "1993", "authors": "J Quinlan"}, {"ref_id": "b19", "title": "Semi-automatic captioning of TV programs, an Australian perspective", "journal": "", "year": "1999", "authors": "J Robert-Ribes; S Pfeiffer; R Ellison; D Burnham"}, {"ref_id": "b20", "title": "Ultrasummarization: A statistical approach to generating highly condensed non-extractive summaries", "journal": "Poster Session", "year": "1999", "authors": "M Witbrock; V Mittal"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Examples of parse trees.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Adjusted log-probabilities for top-scoring compressions at various lengths (lower is better).", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Example of incremental tree compression.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u2022 P (t | s).", "formula_coordinates": [2.0, 85.38, 694.51, 39.85, 9.98]}, {"formula_id": "formula_1", "formula_text": "P tree (s) = P(TOP \u2192 S | TOP) \u2022 P(S \u2192 NP VP | S) \u2022 P(NP \u2192 John | NP) \u2022 P(VP \u2192 VB NP | VP) \u2022 P(VP \u2192 saw | VB) \u2022 P(NP \u2192 Mary | NP) \u2022 P(John | EOS) \u2022 P(saw | John) \u2022 P(Mary | saw) \u2022 P(EOS | Mary)", "formula_coordinates": [2.0, 335.44, 467.22, 201.8, 64.78]}, {"formula_id": "formula_2", "formula_text": "P(TOP \u2192 G | TOP) P(H \u2192 a | H) P(G \u2192 H A | G) P(C \u2192 b | C) P(A \u2192 C D | A) P(D \u2192 e | D) P(a | EOS) P(e | b) P(b | a) P(EOS | e)", "formula_coordinates": [3.0, 69.94, 343.83, 164.23, 65.27]}, {"formula_id": "formula_3", "formula_text": "P(G \u2192 H A | G \u2192 H A) P(A \u2192 C B D | A \u2192 C D) P(B \u2192 Q R | B) P(Z \u2192 c | Z) P(Q \u2192 Z | Q) P(R \u2192 d | R)", "formula_coordinates": [3.0, 69.94, 471.6, 198.19, 51.72]}, {"formula_id": "formula_4", "formula_text": "P(TOP \u2192 G | TOP) P(H \u2192 a | H) P(a | EOS) P(G \u2192 H A | G) P(C \u2192 b | C) P(b | a) P(A \u2192 C D | A) P(Z \u2192 c | Z) P(c | b) P(B \u2192 Q R | B) P(R \u2192 d | R) P(d | c) P(Q \u2192 Z | Q) P(D \u2192 e | D) P(e | d) P(EOS | e)", "formula_coordinates": [3.0, 69.94, 594.06, 225.46, 82.11]}, {"formula_id": "formula_5", "formula_text": "P(G \u2192 H A | G \u2192 H A) P(A \u2192 C B D | A \u2192 C B D) P(B \u2192 Q R | B \u2192 Q R) P(Q \u2192 Z | Q \u2192 Z)", "formula_coordinates": [3.0, 335.44, 283.62, 132.48, 57.6]}, {"formula_id": "formula_6", "formula_text": "P(e | b) \u2022 P(A \u2192 C B D | A \u2192 C D) > P(b | a) \u2022 P(c | b) \u2022 P(d | c) \u2022 P(A \u2192 C B D | A \u2192 C B D) \u2022 P(B \u2192 Q R | B \u2192 Q R) \u2022 P(Q \u2192 Z | Q \u2192 Z)", "formula_coordinates": [3.0, 335.44, 488.98, 202.3, 42.86]}, {"formula_id": "formula_7", "formula_text": "G \u2192 H A B \u2192 R A\u2192 B C H \u2192 a G \u2192 H Q \u2192 Z A\u2192 C C \u2192 b G \u2192 A A \u2192 C B D A \u2192 B Z \u2192 c B \u2192 Q R A \u2192 C B A \u2192 D R \u2192 d B \u2192 Q A\u2192 C D D \u2192 e", "formula_coordinates": [4.0, 69.94, 445.97, 202.26, 53.82]}], "doi": ""}