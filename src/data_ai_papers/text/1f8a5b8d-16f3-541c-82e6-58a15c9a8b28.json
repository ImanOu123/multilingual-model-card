{"title": "Error Analysis and the Role of Morphology", "authors": "Marcel Bollmann; Anders S\u00f8gaard", "pub_date": "", "abstract": "We evaluate two common conjectures in error analysis of NLP models: (i) Morphology is predictive of errors; and (ii) the importance of morphology increases with the morphological complexity of a language. We show across four different tasks and up to 57 languages that of these conjectures, somewhat surprisingly, only (i) is true. Using morphological features does improve error prediction across tasks; however, this effect is less pronounced with morphologically complex languages. We speculate this is because morphology is more discriminative in morphologically simple languages. Across all four tasks, case and gender are the morphological features most predictive of error.", "sections": [{"heading": "Introduction", "text": "In error analysis, we often blame morphology (Nivre, 2007;Bender, 2009), i.e., the productive inflection and derivation of new word forms. Morphology has been argued to be a major source of error in syntactic parsing , semantic parsing (\u015eahin and Steedman, 2018), machine translation (Irvine et al., 2013;Burlot and Yvon, 2017) and a range of other tasks, in particular in morphologically complex languages (Bender, 2009;S\u00f8gaard et al., 2018;. This paper presents a large-scale study showing that morphology is, as commonly conjectured, an important source of error across tasks, but somewhat surprisingly, that morphology is less predictive of errors in morphologically complex languages.\nEnglish is a morphologically simple language, showing very limited inflection and expressing most concepts through syntactic structure instead; it is also the most-represented language at major natural language processing (NLP) venues and that with the largest amount of language resources available (Bender, 2011;Joshi et al., 2020). This ", "publication_ref": ["b24", "b0", "b17", "b4", "b0", "b33", "b1", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Random forest classifier", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Predict", "text": "Figure 1: Overview of our methodology: We map each token to a set of morphological features and, based on this representation, predict whether some NLP system (e.g., a dependency parser) was correct () or made an error () on that token. makes it easy to ignore morphology when designing model architectures. As a consequence, we frequently observe that performance of NLP systems on morphologically more complex languages lags behind that for English (e.g. Czarnowska et al., 2019;.\nComplex morphology leads to the occurrence of rare inflected word forms. Polish nouns, for example, can inflect for number and seven different cases; this makes it less likely that all of these inflected word forms appear in the training data for our NLP models. Consequently, a model that correctly handles imi\u0119 'name' (NOM.SG) might not have seen the less frequent form imionami (INST.PL), potentially resulting in errors. If the model has generally seen fewer words in instrumental case, this can lead to systematic errors on this class of inflections.\nNowadays, many NLP systems use statistically learned subword units such as byte-pair encodings  or use characters as input representations, which could allow a system to generalize to individual affixes. However, in practice, these approaches are often found to be in-sufficient at capturing morphological structure (Vania and Lopez, 2017;Bostrom and Durrett, 2020;.\nContributions In this study, we revisit two common conjectures about the role of morphology that are made in error analysis of NLP systems. Specifically, we ask whether (i) whether morphology is generally predictive of errors across tasks and languages; and (ii) whether the extent to which morphology is predictive depends on the morphological complexity of the language in question. These conjectures are common throughout the literature (Nivre, 2007;Bender, 2009;Manning, 2011).\nLooking at data from four shared tasks on semantic role labeling (Haji\u010d et al., 2009), dependency parsing (Zeman et al., 2018), verbal multi-word expression identification (Ramisch et al., 2018), and quality estimation (Fonseca et al., 2019), we map each token in the input data to a set of morphological features. Using only this feature set, and without using any orthographic or distributional representation of the input, we train random forest classifiers to predict whether a system has made an error on an input token. Figure 1 illustrates this approach.\nUsing this methodology, we find that, somewhat surprisingly, our results only support the first conjecture. In other words, (i) while morphology is helpful in predicting such errors, (ii) the degree to which morphology helps does not increase with the morphological complexity of the language. Moreover, we find and discuss task-specific differences between which morphological features are predictive of error. In general, part of speech, case and gender are most predictive of error.\nThe code for obtaining the datasets and running the experiments is made publicly available. 1", "publication_ref": ["b6", "b38", "b2", "b24", "b0", "b22", "b40", "b28", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Background", "text": "Morphology is frequently identified as a source of error during qualitative evaluations of NLP systems. Honnibal et al. (2010) observe that inflectional variants cause problems for statistical CCG tagging due to training data sparseness, and explicit morphological analysis helps, even for English. For dependency parsing, Seeker and Kuhn (2013) identify case syncretism as a source of error propagation in data from Czech, German, and Hungarian.  give a broader overview of the challenges that rich morphological structure presents for dependency parsing, and\u015eahin and Steedman (2018) discuss the importance of morphology in semantic parsing.\nMany observations of the effect of morphology come from evaluating machine translation (MT) systems. Federico et al. (2014) show that morphological errors are common for MT into Arabic and Russian and strongly affect human quality judgement. For English-Romanian MT, Peter et al. (2016) find that tense and verb form on the target side are a common source of error. Klubi\u010dka et al. (2017) find that errors in English-Croatian MT are more common for some morphological categories, such as case. In a similar vein, Burlot and Yvon (2017) evaluate morphological competence of MT systems using contrast pairs and show that systems have different strengths and weaknesses for different morphological phenomena. Beyond parsing and MT, morphology has also been shown to present a challenge for tasks such as Arabic handwriting recognition (Habash and Roth, 2011) or Russian anaphora resolution (Toldova et al., 2016).\nMost of the studies cited above predate contextual embedding models such as BERT (Devlin et al., 2019), which are now considered state-of-the-art for many NLP tasks. So far, few studies have explicitly analysed BERT with regard to morphology. Edmiston (2020) analyses morphological content in BERT-style models for five languages and finds that \"[morphological] ambiguity is negatively correlated with performance on classification, and to a significant degree in many cases\", suggesting that morphology is still a significant source of error in these models. We go significantly beyond this work by studying a much larger set of morphological variables, across several architectures and tasks, and across up to 57 languages.", "publication_ref": ["b15", "b31", "b9", "b27", "b21", "b4", "b11", "b36", "b7", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Datasets", "text": "We collect datasets from shared tasks that (i) publish system outputs along with their gold annotations, (ii) span a variety of languages, and (iii) cover different NLP tasks. Based on these criteria, we pick datasets from the following shared tasks:\n\u2022 SEM: CoNLL-2009 Shared Task on Semantic Dependencies (Haji\u010d et al., 2009), covering semantic role labeling for seven languages.\n\u2022 UDP: CoNLL-2018 Shared Task on Universal Dependencies Parsing (Zeman et al., 2018), covering syntactic parsing for 57 languages.\n\u2022 VMWE: PARSEME 2018 Shared Task on Automatic Identification of Verbal Multiword Expressions (MWE; Ramisch et al., 2018), covering nine languages.\nAdditionally, we use the following dataset for its gold annotations:\n\u2022 MT: WMT 2019 Shared Tasks on Quality Estimation (Fonseca et al., 2019), covering wordlevel quality estimation for English-German and English-Russian machine translation.\nHere, we are not interested in the system outputs from the shared task; instead, we use the gold annotations for the quality estimation, which give us token-level error labels for the underlying machine translation outputs. Section 4.2 describes in detail how we assign error labels to these datasets.", "publication_ref": ["b40", "b28", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Methodology", "text": "We train a classifier to predict errors made by NLP systems based on morphological features of the input tokens, in order to then analyze which morphological features (if any) are most predictive of such errors. We first describe how we obtain these features (Sec. 4.1) and how we classify when an NLP system has made an error (Sec. 4.2), then describe the classifier itself (Sec. 4.3).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Feature Extraction", "text": "We represent each token in the input data using a binary feature set. Each individual feature is named using the convention of {CATEGORY}={VALUE}, where the former is a feature category (such as POS for \"part of speech\") and the latter is a value within that category (e.g. VERB). We encode these features in a binary manner, i.e., for each feature in our inventory, that feature is either present or not present. Importantly, the classifier itself has no notion of \"feature categories\" as it only sees a single, binary feature vector.\nThe full feature inventory is summarized in Table 1; what follows is a description of these features and how we derived them.\nMorphological features Our morphological feature inventory consists of (i) Universal Dependencies (UD) features, (ii) lexical features, and (iii) string-based features.\nUD features include the universal part-ofspeech (POS) category and the universal feature set as defined by Universal Dependencies; e.g. U:POS=VERB or U:TENSE=PAST. 2 The UDP shared-task gold data already provides this annotation; for the other tasks, we obtain these features by running UDPipe 3 (Straka and Strakov\u00e1, 2017) with the largest pre-trained model for the language in question. 4 We complement this with the following additional lexical features: (i) SYNCRETIC specifies to what extent a token can be representative for several morphological feature sets: e.g., ask can be either U:MOOD=IND or U:MOOD=IMP, depending on context; (ii) AMBIG POS specifies to what extent the universal part-of-speech tag of the token can differ based on context: e.g., book could be either U:POS=VERB or U:POS=NOUN; and (iii) AMBIG LEX specifies whether or not the token belongs to multiple lexemes: e.g., ruling is a form of both '(to) rule' and '(the) ruling'. To determine these features for a given token, we use UDLexicons 5 (Sagot, 2018); in case a language is not covered by UDLexicons, we fall back to UniMorph 6 (Kirov et al., 2018).\nFinally, we define purely string-based features based on comparing the token with its lemma. We perform character-based string alignment using Edlib (\u0160o\u0161i\u0107 and \u0160iki\u0107, 2017) and derive the following features: (i) EDIT=PRE and EDIT=SUF when there is an edit at the beginning or the end of the sequence, respectively; (ii) EDIT=IN when there is an edit in the middle of the sequence; and (iii) EDIT=FULL when there is no character alignment between the strings. These features are intended to approximate prefixation, suffixation, infixation or other word-internal processes, and suppletion, respectively.", "publication_ref": ["b34", "b29", "b19"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Control features", "text": "To estimate the relative importance of our morphological features for the error prediction task, we additionally introduce a set of control features that are not morphologically motivated (cf. Tab. 1). These are (i) string length fea- \nxn = MATCH xi \u2208 {MATCH, MISMATCH, GAP} EDIT=IN \u2203i, j, k : i < j < k \u2227xi = MATCH \u2227xj = MATCH \u2227x k = MATCH EDIT=FULL \u2200i : xi = MATCH Control features LEN=1-3 1 \u2264 |t| \u2264 3\nwhere |t| is the string length of t\nLEN=4-6 4 \u2264 |t| \u2264 6 LEN=7-9 7 \u2264 |t| \u2264 9 LEN=10+ |t| \u2265 10 FREQ=99 P99 \u2264 f (t)\nwhere f (t) is the absolute frequency count of t\nFREQ=98 P98 \u2264 f (t) < P99\nand Pn is the n-th percentile of the frequency distribution tures, where each token is assigned exactly one such feature depending on its length; and (ii) token frequency bins. For the latter, we count token frequencies in the Universal Dependencies treebanks and assign each token a frequency feature. These features are based on frequency bins that we manually curated to provide a roughly balanced distribution of tokens to bins: e.g., FREQ=99 denotes a token that is in the 99 th percentile of the frequency distribution of all types, while FREQ=RARE denotes a token occurring less than four times overall (see Table 1 for all definitions).\nFREQ=95 P95 \u2264 f (t) < P98 FREQ=90 P90 \u2264 f (t) < P95 FREQ=UNCOMMON 4 \u2264 f (t) < P90 FREQ=RARE f (t) < 4\nPruning and statistics Since very rare features are not very informative, for any given dataset, we remove features that occur less than 10 times in that dataset. Depending on the task and language, we generate between 17 and 120 unique features this way, with an average of 68.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Classifying errors in system outputs", "text": "The target variable for our classifier is a binary label corresponding to whether or not the shared-task system has made an error on the input token. This requires comparing the outputs of a system to the gold data and classifying each token as either correct or incorrect. We will also refer to the latter as the error class. This classification follows the original evaluation criteria by the shared tasks to the extent possible.\nFor SEM, a prediction is classified as \"correct\" iff the semantic dependencies and label columns are an exact match with the gold data. For UDP, we do the same with the syntactic head and dependency relation columns; this is the same criterion that underlies the labeled attachment score (LAS) commonly used to evaluate dependency parsing. VMWE is a little more challenging since its prediction involves a set of tokens within a sentence. For each sentence, we match up each gold MWE with the predicted MWE that has the same label and the largest token overlap. We then consider a token \"incorrectly\" predicted if has a MWE annotation that does not belong to one of these matched MWEs, or if it lacks a MWE annotation that it should have according to the gold data.\nAs mentioned before, we treat the MT data a little differently: here, the gold data provides binary labels in the form of \"OK\" and \"BAD\" tags, corresponding to the correctness of some machine translation system. These tags are provided both for tokens and gaps between tokens (to account for the deletion/insertion of words in machine translation). We use the token-level tags from the gold data directly as our error classification labels. Appendix A gives an example for the error classification approach on VMWE and MT.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Training classifiers", "text": "With the extracted features (from Sec. 4.1), we can now train classifiers to predict the error variable (from Sec. 4.2). Concretely, we train random forest classifiers (Breiman, 2001) as implemented by Scikit-learn 7 (Pedregosa et al., 2011) on each output file provided by each shared task. Random forests are ensembles of decision trees and are quick to train: the average training time on our datasets was 14 seconds on CPU, with no single run taking longer than five minutes.\nAs an alternative to random forests, we also experimented with randomized logistic regression classifiers followed by stability selection (Meinshausen and B\u00fchlmann, 2010) to select predictive features. In our trials, this approach showed a worse performance (in terms of F 1 -score) compared to random forests, while also taking considerably longer to run (averaging 7 minutes per dataset). We therefore only report results with random forest classifiers.", "publication_ref": ["b3", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "Analysis", "text": "For each shared task (Sec. 3), we ran our classification pipeline (Sec. 4) separately for each combination of (i) system submission and (ii) language evaluated on. Since random forests are largely interpretable, our analysis focuses on the important features in our learned models.\nFirst, though, we look at the overall F 1 -score of the individual classifiers, which we evaluate via stratified 5-fold cross-validation on each data point (Sec. 5.1). Additionally, to better estimate the importance of morphology, we run our crossvalidation pipeline a second time without the mor-phological features, i.e., only providing the classifiers with the \"control features\" shown in Tab. 1. We refer to these two feature sets as \"full\" and \"control\" settings, respectively, and analyze their differences in F 1 -score (Sec. 5.2). 8 Finally, we analyse the importance of individual morphological features (Sec. 5.3).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "How well do the classifiers predict errors?", "text": "To evaluate how well the full classifiers learned the task, we consider their F 1 -score for predicting the \"error\" class. Across all of our datasets, we observe a mean F 1 of 0.43 with a standard deviation of \u00b10.18. Note that our setup is not comparable to most other NLP classification tasks: we evaluate a classifier trained to detect the errors of state-ofthe-art systems, which means that (i) the task is inherently hard, as those systems are optimized to fix easily detectable errors, and (ii) there is no reason to assume a priori that this task is well learnable from morphological input features alone. Therefore, we believe an F 1 score of 0.43-albeit with considerable variance in performance across tasks and languages-is a strong result.\nError rate There is one important aspect to consider: the frequency of the \"error\" class depends on the system performance of the data point we look at, and as such our class distribution can be highly imbalanced and varied. Indeed, F 1 -score and frequency of the error class correlate very strongly with Pearson's r = 0.93. Figure 2 plots this relationship. 9 This suggests that the errors introduced by state-of-the-art NLP systems, unsurprisingly, become harder and harder to predict the better the underlying systems perform.\nNote that data imbalance is in the nature of the error prediction task, as we expect errors in state-ofthe-art systems to be rare. Additionally, different 8 To complement the results and analyses presented here, we also provide a detailed table with the results for all task/language pairs in Appendix B.\n9 It might look surprising that many data points have very high error rates, with some even going above 0.95; i.e., more than 95% of all predictions in the respective file are deemed to be \"incorrect\" according to the criteria in Sec. 4.2. Spot-checking reveals that this is, however, plausible: for example, in UDP, the average labeled attachment score (LAS) on the Thai TH_PUD treebank was only 1.38 (Zeman et al., 2018, Table 15), with 23 systems achieving a LAS of only 0.77 or lower (out of 100; cf. http://universaldependencies.org/ conll18/results-las.html), which is reflected by an error rate of \u226599.23% in our data. languages have differently-sized morphological tag inventories, affecting the total number of input features for the classifier. We do not attempt to apply data balancing techniques to counteract this, since this would make the task artificially easy and our results overly optimistic.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "How important is morphology for predicting errors?", "text": "Figure 3 provides an alternative view of the F 1scores presented in Fig. 2, this time as a letter-value plot (Hofmann et al., 2017) showing quantiles of the F 1 distribution. Additionally, we compare the classifier with the full feature set to the control set where morphological features were not included. We observe that the classifiers learn best on UDP followed by SEM, while classifier F 1 is relatively poor on VMWE data. A probable explanation for this is the generally low error rate in VMWE (cf. Fig. 2). The other important observation is that classifiers in the \"control\" setting score consistently lower than the classifiers that have access to morphological features.\nImportance by language For looking at individual languages, we restrict ourselves to the UDP data. Firstly, UDP covers 57 languages-more than any other task in our comparison-and there are no languages in the other tasks that are not also contained in UDP. Secondly, our classifier performance is generally highest on UDP (cf. Fig. 3), allowing for a more meaningful interpretation of results, particularly of selected features.\nFurthermore, to factor out the effect of a data point's error rate (as discussed in Sec. 5.1), we look at the difference between the F 1 -score of the full classifier and the control classifier trained on the same data point. In other words, we define\n\u2206F 1 = F 1 (g f ) \u2212 F 1 (g c ) (1)\nwhere g f and g c are the classifiers with the full and the control feature set, respectively. This gives us a way to judge the importance of morphological features relative to the non-morphological ones while minimizing the effect of the error rate on the results, since \u2206F 1 no longer shows a strong correlation with the error rate (r = 0.29). Figure 4 (bottom half) shows the quartiles of \u2206F 1 scores by language in the UDP dataset. They span a wide range of values, with the median \u2206F 1 varying gradually between \u22120.03 (for Turkish, TUR) and 0.24 (for Nigerian Pidgin, PCM). Morphological features appear to be important for some languages while being unhelpful, and sometimes even detrimental, for others.\nMorphological complexity Are the differences in \u2206F 1 scores (in Fig. 4) somehow related to the morphological complexity of the languages? To analyze this relationship more systematically, we use the measure of morphological feature entropy (MFE) introduced by \u00c7\u00f6ltekin and Rama (2018). MFE is sensitive to both the size of a language's morphological feature inventory as well as its distribution, with a more uniform distribution of features resulting in a higher MFE. Since MFE is a treebank measure that relies on the association between tokens and morphological tags, it is affected by tokenization and annotation choices of the treebank used to calculate it; therefore, it can only be considered a rough approximation of the underlying language's complexity. Like \u00c7\u00f6ltekin \n0.1 0.2 0.3 \u2206 F1\nFigure 4: Classifier performance on UDP by language, sorted by median \u2206F 1 , where \u2206F 1 is the difference in F 1 -scores between training with the full and the control feature set (cf. Eq. 1). Bottom half shows the quartiles of the \u2206F 1 distribution, top half shows the morphological feature entropy (MFE) for the given language; color shading is also based on MFE (with darker shade = higher MFE). Full names for all language codes as well as exact numeric values can be found in Appendix B.\nand Rama (2018), we calculate the MFE score for each language on the UD treebanks. 10 The MFE score for each language is shown in the top half of Fig. 4. Surprisingly, we find a slight, negative correlation between MFE and \u2206F 1 (Pearson's r = \u22120.24). While languages with high MFE appear across the whole range of the \u2206F 1 distribution, a number of languages with low MFE-and thus deemed to be more morphologically simple, such as Thai (THA), Japanese (JPN), or Nigerian Pidgin (PCM)-are found to profit more from the inclusion of morphological features. One possible explanation is that the control features are already very strong, which we will look at more closely in Sec. 5.3. Another possible factor is that morphologically complex languages introduce a much larger set of morphological features; if, for a given language, most of them are not relevant for predicting errors in the UDP task, they might hurt the overall classifier performance.", "publication_ref": ["b14", "b5"], "figure_ref": ["fig_2", "fig_1", "fig_1", "fig_2"], "table_ref": []}, {"heading": "What morphological features are most predictive of errors?", "text": "Morphological features provide a helpful signal to the classifiers, though its overall magnitude differs 10 We use UD version 2.5 (Zeman et al., 2019).\nby language (cf. Sec. 5.2). Now, we ask which of the morphological features are particularly relevant for error prediction. Since plain feature importances of trained random forest classifiers can be misleading (Strobl et al., 2007;Parr et al., 2018), we follow the approach of explicitly removing features and retraining (Parr et al., 2018;Hooker and Mentch, 2019). Unlike the analyses above, we are not concerned with generalization here, but with identifying features that are especially predictive for the error variable on each dataset as a whole. Therefore, we do not use a cross-validation strategy, but rely on the full dataset for both training and obtaining feature importances. Concretely, for each feature category (as introduced in Sec. 4.1), we retrain the model without features from that category and note the drop in error-class F 1 -score compared to the model with the full feature set. Formally, let \u03a6 be the full feature set and \u03c6 c \u2282 \u03a6 the subset of features belonging to category c (e.g., c = U:TENSE). The importance of category c is then defined as\nf (c) = F 1 (C \u03a6 ) \u2212 F 1 (C \u03a6\\\u03c6c ) (2)\nwhere C X is a random forest classifier trained using feature set X. Higher values for f (c) mean a higher importance of category c, while negative values  mean that including c is actually detrimental to the F 1 -score.\nAverage feature importances Table 2 shows the top 10 feature categories for each task, averaged over all languages and datasets. The two control features, FREQ and LEN, always appear among the three most important categories, only trumped by U:POS for the UDP and SEM tasks. Notably, these three are the only feature categories that are guaranteed to appear with every token. It is no surprise that token frequency is strongly related to the likelihood of errors, while Zipf's law tells us that token length is strongly correlated with frequency.\nFigure 5 shows the distribution of feature importances for the top 10 categories of UDP (cf. Tab. 2b). U:POS spans a much wider range of FI values than any of the other categories, although the outliers at the upper end all come from Nigerian Pidgin (PCM). Moreover, categories with a low average FI (e.g., U:ASPECT or SYNCRETIC) do not show outliers, i.e., are of low importance across languages. This is also true for the remaining feature categories.\nIndividual part-of-speech tags Since U:POS is an important feature category across tasks (cf. Tab. 2), we also look at feature importances for individual POS tags. For this, we use the same approach as for the feature categories (cf. Eq. 2), except that we now only remove a single U:POS feature from \u03a6 at a time.\nTable 3 shows the average feature importances for individual U:POS features, though this time we restrict ourselves to the subset of languages in UDP that are also covered in SEM. 11 This way, we can better isolate the task-specific differences in FI scores, without conflating them with the dif-11 These are Catalan, Czech, German, English, Japanese, Spanish, and Chinese; cf. Appendix B. ferent language-specific distributions of part-ofspeech tags that may affect these results. We find that adverbs (ADV) are the most important partof-speech category for both tasks, while INTJ and PART are found to be important for predicting errors in UDP, but not in SEM. This aligns with our intuitions about what is hard in syntactic and semantic parsing, further supporting the validity of our approach.", "publication_ref": ["b35", "b25", "b25", "b16"], "figure_ref": ["fig_3"], "table_ref": ["tab_4", "tab_6"]}, {"heading": "Conclusion", "text": "We presented a large-scale error analysis focusing on the role of morphology. Our analysis spans a range of morphological variables, four NLP tasks, and up to 57 languages. We confirm the common conjecture that morphological variablesespecially case and gender-are predictive of errors across NLP tasks and languages. Somewhat sur-  prisingly, we found that the usefulness of morphological variables is negatively correlated with the morphological complexity of the language in question. We speculate this is because morphological information is more discriminative in morphologically simple languages.  A Examples for error classification Table 4a shows an example for how we classify errors (cf. Sec. 4.2) in the VMWE dataset on verbal multi-word expression (MWE) identification.\nIn the gold data, a single MWE ('postawienie sprawy') is annotated, while the NLP system has incorrectly identified the MWE as being 'to . . . postawienie'. The annotation \"1\" here is an ID in case there are multiple MWEs within the same sentence. We annotate both 'to', which was mistakenly identified as part of the MWE, as well as 'sprawy', which was mistakenly left out, as an error (). All remaining tokens are marked as correct (). Table 4b shows an example from the MT dataset on quality estimation for machine translation (MT). Here, the gold data provides us with \"OK\" and \"BAD\" labels for the individual tokens of the machine-generated translation as well as for the gaps between the tokens. The latter is done to be able to annotate missing passages in the machine translation output; i.e., a gap between tokens would be labelled \"BAD\" if the MT system should have produced more output at a given position in a sentence than it did. Since it is unclear to which (existing) tokens these \"gap annotations\" should be ascribed to, we do not consider them for the error classification, and only consider \"OK/BAD\" labels for the tokens that do appear in the data.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_8", "tab_8"]}, {"heading": "B Statistics and classifier results", "text": "Table 5 presents statistics and classifier results, corresponding to the analyses in Secs. 5.1 and 5.2, for each task/language pair. The column \"Avg. error rate\" corresponds to the error rates plotted in Fig. 2, while the \"MFE\" column shows the mor-phological feature entropy (cf. Sec. 5.2) for the respective language. \"Avg. F 1 \" shows the average F 1 -score after stratified 5-fold cross-validation (cf. Sec. 5.1), while \"Avg. \u2206F 1 \" corresponds to the \u2206F 1 -measure defined in Eq. ( 1 ", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": ["tab_10"]}, {"heading": "Acknowledgments", "text": "Marcel Bollmann was funded from the European Union's Horizon 2020 research and innovation programme under the Marie Sk\u0142odowska-Curie grant agreement No. 845995. Anders S\u00f8gaard was funded by a Google Focused Research Award.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Linguistically na\u00efve != language independent: Why NLP needs linguistic typology", "journal": "", "year": "2009", "authors": "Emily M Bender"}, {"ref_id": "b1", "title": "On achieving and evaluating language-independence in NLP", "journal": "Linguistic Issues in Language Technology", "year": "2011", "authors": "Emily M Bender"}, {"ref_id": "b2", "title": "Byte pair encoding is suboptimal for language model pretraining", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Kaj Bostrom; Greg Durrett"}, {"ref_id": "b3", "title": "Random forests. Machine Learning", "journal": "", "year": "2001", "authors": "Leo Breiman"}, {"ref_id": "b4", "title": "Evaluating the morphological competence of machine translation systems", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Franck Burlot; Fran\u00e7ois Yvon"}, {"ref_id": "b5", "title": "Exploiting Universal Dependencies treebanks for measuring morphosyntactic complexity", "journal": "", "year": "2018", "authors": "\u00c7agr\u0131 \u00c7\u00f6ltekin; Taraka Rama"}, {"ref_id": "b6", "title": "Don't forget the long tail! A comprehensive analysis of morphological generalization in bilingual lexicon induction", "journal": "", "year": "2019", "authors": "Paula Czarnowska; Sebastian Ruder; Edouard Grave; Ryan Cotterell; Ann Copestake"}, {"ref_id": "b7", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Long and Short Papers", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b8", "title": "A systematic analysis of morphological content in BERT models for multiple languages", "journal": "", "year": "2020", "authors": "Daniel Edmiston"}, {"ref_id": "b9", "title": "Assessing the impact of translation errors on machine translation quality with mixed-effects models", "journal": "Association for Computational Linguistics", "year": "2014", "authors": "Marcello Federico; Matteo Negri; Luisa Bentivogli; Marco Turchi"}, {"ref_id": "b10", "title": "Findings of the WMT 2019 shared tasks on quality estimation", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Erick Fonseca; Lisa Yankovskaya; F T Andr\u00e9; Mark Martins; Christian Fishel;  Federmann"}, {"ref_id": "b11", "title": "Using deep morphology to improve automatic error detection in Arabic handwriting recognition", "journal": "Association for Computational Linguistics", "year": "2011", "authors": "Nizar Habash; Ryan Roth"}, {"ref_id": "b12", "title": "", "journal": "", "year": "", "authors": "Jan Haji\u010d; Massimiliano Ciaramita"}, {"ref_id": "b13", "title": "The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages", "journal": "Association for Computational Linguistics", "year": "2009", "authors": "Adam M\u00e0rquez; Joakim Meyers; Sebastian Nivre; Jan Pad\u00f3; Pavel \u0160t\u011bp\u00e1nek; Mihai Stra\u0148\u00e1k; Nianwen Surdeanu; Yi Xue;  Zhang"}, {"ref_id": "b14", "title": "Letter-value plots: Boxplots for large data", "journal": "Journal of Computational and Graphical Statistics", "year": "2017", "authors": "Heike Hofmann; Hadley Wickham; Karen Kafadar"}, {"ref_id": "b15", "title": "Morphological analysis can improve a CCG parser for English", "journal": "", "year": "2010", "authors": "Matthew Honnibal; Jonathan K Kummerfeld; James R Curran"}, {"ref_id": "b16", "title": "Please stop permuting features: An explanation and alternatives", "journal": "", "year": "2019", "authors": "Giles Hooker; Lucas Mentch"}, {"ref_id": "b17", "title": "Measuring machine translation errors in new domains", "journal": "", "year": "2013", "authors": "Ann Irvine; John Morgan; Marine Carpuat; Hal Daum\u00e9; Iii ; Dragos Munteanu"}, {"ref_id": "b18", "title": "The state and fate of linguistic diversity and inclusion in the NLP world", "journal": "", "year": "2020", "authors": "Pratik Joshi; Sebastin Santy; Amar Budhiraja; Kalika Bali; Monojit Choudhury"}, {"ref_id": "b19", "title": "UniMorph 2.0: Universal morphology", "journal": "", "year": "2018", "authors": "Christo Kirov; Ryan Cotterell; John Sylak-Glassman; G\u00e9raldine Walther; Ekaterina Vylomova; Patrick Xia; Manaal Faruqui; Sabrina J Mielke; Arya Mc-Carthy; Sandra K\u00fcbler; David Yarowsky"}, {"ref_id": "b20", "title": "Getting the ##life out of living: How adequate are word-pieces for modelling complex morphology?", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Stav Klein; Reut Tsarfaty"}, {"ref_id": "b21", "title": "Fine-grained human evaluation of neural versus phrase-based machine translation", "journal": "The Prague Bulletin of Mathematical Linguistics", "year": "2017", "authors": "Filip Klubi\u010dka; Antonio Toral; M V\u00edctor;  S\u00e1nchez-Cartagena"}, {"ref_id": "b22", "title": "Part-of-speech tagging from 97% to 100%: is it time for some linguistics?", "journal": "Springer-Verlag", "year": "2011", "authors": "Christopher D Manning"}, {"ref_id": "b23", "title": "Stability selection", "journal": "Journal of the Royal Statistical Society: Series B (Statistical Methodology)", "year": "2010", "authors": "Nicolai Meinshausen; Peter B\u00fchlmann"}, {"ref_id": "b24", "title": "Data-driven dependency parsing across languages and domains: Perspectives from the CoNLL-2007 shared task", "journal": "", "year": "2007", "authors": "Joakim Nivre"}, {"ref_id": "b25", "title": "Beware default random forest importances", "journal": "", "year": "2018", "authors": "Terence Parr; Kerem Turgutlu; Christopher Csiszar; Jeremy Howard"}, {"ref_id": "b26", "title": "Scikit-learn: Machine learning in python", "journal": "Journal of Machine Learning Research", "year": "2011", "authors": "Fabian Pedregosa; Ga\u00ebl Varoquaux; Alexandre Gramfort; Vincent Michel; Bertrand Thirion; Olivier Grisel; Mathieu Blondel; Peter Prettenhofer; Ron Weiss; Vincent Dubourg; Jake Vanderplas; Alexandre Passos; David Cournapeau; Matthieu Brucher; Matthieu Perrot; \u00c9douard Duchesnay"}, {"ref_id": "b27", "title": "The QT21/HimL combined machine translation system", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Jan-Thorsten Peter; Tamer Alkhouli; Hermann Ney; Matthias Huck; Fabienne Braune; Alexander Fraser; Ale\u0161 Tamchyna; Ond\u0159ej Bojar; Barry Haddow; Rico Sennrich; Fr\u00e9d\u00e9ric Blain; Lucia Specia"}, {"ref_id": "b28", "title": "Edition 1.1 of the PARSEME shared task on automatic identification of verbal multiword expressions", "journal": "", "year": "2018", "authors": "Carlos Ramisch; Silvio Ricardo Cordeiro; Agata Savary; Veronika Vincze; Archna Verginica Barbu Mititelu; Maja Bhatia; Marie Buljan; Polona Candito; Voula Gantar; Tunga Giouli; Abdelati G\u00fcng\u00f6r; Uxoa Hawwari; Jolanta I\u00f1urrieta; Simon Kovalevskait\u0117; Timm Krek; Chaya Lichte; Johanna Liebeskind; Carla Monti; Behrang Parra Escart\u00edn; Renata Qasem-Izadeh; Nathan Ramisch; Ivelina Schneider; Ashwini Stoyanova; Abigail Vaidya;  Walsh"}, {"ref_id": "b29", "title": "A multilingual collection of CoNLL-u-compatible morphological lexicons", "journal": "", "year": "2018", "authors": "Beno\u00eet Sagot"}, {"ref_id": "b30", "title": "Character-level models versus morphology in semantic role labeling", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "G\u00f6zde G\u00fcl\u015fahin; Mark Steedman"}, {"ref_id": "b31", "title": "Morphological and syntactic case in statistical dependency parsing", "journal": "Computational Linguistics", "year": "2013", "authors": "Wolfgang Seeker; Jonas Kuhn"}, {"ref_id": "b32", "title": "Neural machine translation of rare words with subword units", "journal": "Long Papers", "year": "2016", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"ref_id": "b33", "title": "On the limitations of unsupervised bilingual dictionary induction", "journal": "Long Papers", "year": "2018", "authors": "Anders S\u00f8gaard; Sebastian Ruder; Ivan Vuli\u0107"}, {"ref_id": "b34", "title": "Tokenizing, POS tagging, lemmatizing and parsing UD 2.0 with UDPipe", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Milan Straka; Jana Strakov\u00e1"}, {"ref_id": "b35", "title": "Bias in random forest variable importance measures: Illustrations, sources and a solution", "journal": "BMC Bioinformatics", "year": "2007", "authors": "Carolin Strobl; Anne-Laure Boulesteix"}, {"ref_id": "b36", "title": "Error analysis for anaphora resolution in Russian: new challenging issues for anaphora resolution task in a morphologically rich language", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Svetlana Toldova; Ilya Azerkovich; Alina Ladygina; Anna Roitberg; Maria Vasilyeva"}, {"ref_id": "b37", "title": "From SPMRL to NMRL: What did we learn (and unlearn) in a decade of parsing morphologically-rich languages (MRLs)?", "journal": "", "year": "2020", "authors": "Reut Tsarfaty; Dan Bareket; Stav Klein; Amit Seker"}, {"ref_id": "b38", "title": "From characters to words to in between: Do we capture morphology?", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Clara Vania; Adam Lopez"}, {"ref_id": "b39", "title": "Edlib: a C/C++ library for fast, exact sequence alignment using edit distance", "journal": "Bioinformatics", "year": "2017", "authors": "Martin \u0160o\u0161i\u0107; Mile \u0160iki\u0107"}, {"ref_id": "b40", "title": "CoNLL 2018 shared task: Multilingual parsing from raw text to universal dependencies", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Daniel Zeman; Jan Haji\u010d; Martin Popel; Martin Potthast; Milan Straka; Filip Ginter; Joakim Nivre; Slav Petrov"}, {"ref_id": "b41", "title": "Petya Osenova, Robert \u00d6stling, Lilja \u00d8vrelid, Niko Partanen, Elena Pascual, Marco Passarotti, Agnieszka Patejuk, Guilherme Paulino-Passos, Angelika Peljak-\u0141api\u0144ska, Siyao Peng, Cenel-Augusto Perez, Guy Perrier, Daria Petrova", "journal": "", "year": "", "authors": "Daniel Zeman; Joakim Nivre; Mitchell Abrams; No\u00ebmi Aepli; \u017deljko Agi\u0107; Lars Ahrenberg; Gabriel\u0117 Aleksandravi\u010di\u016bt\u0117; Lene Antonsen; Katya Aplonova; Maria Jesus Aranzabe; Gashaw Arutie; Masayuki Asahara; Luma Ateyah; Mohammed Attia; Aitziber Atutxa; Liesbeth Augustinus; Elena Badmaeva; Miguel Ballesteros; Esha Banerjee; Sebastian Bank; Victoria Verginica Barbu Mititelu; Colin Basmov; John Batchelor; Sandra Bauer; Kepa Bellato; Yevgeni Bengoetxea;  Berzak; Ahmad Irshad; Riyaz Ahmad Bhat; Erica Bhat; Eckhard Biagetti; Agn\u0117 Bick; Rogier Bielinskien\u0117; Victoria Blokland; Lo\u00efc Bobicev; Emanuel Borges Boizou; Carl V\u00f6lker; Cristina B\u00f6rstell; Gosse Bosco; Sam Bouma; Adriane Bowman; Kristina Boyd; Aljoscha Brokait\u0117; Marie Burchardt; Bernard Candito; Gauthier Caron; Tatiana Caron; G\u00fcl\u015fen Cavalcanti; Flavio Cebiroglu Eryigit; Giuseppe G A Massimiliano Cecchini;  Celano; Savas Slavom\u00edr\u010d\u00e9pl\u00f6; Fabricio Cetin; Jinho Chalub; Yongseok Choi; Jayeol Cho; Alessandra T Chun; Silvie Cignarella; Aur\u00e9lie Cinkov\u00e1; Kaja Collomb ; Peter Dirix; Timothy Dobrovoljc; Kira Dozat; Puneet Droganova; Hanne Dwivedi; Marhaba Eckhoff; Ali Eli; Binyam Elkahky; Olga Ephrem; Toma\u017e Erina; Aline Erjavec; Wograine Etienne; Rich\u00e1rd Evelyn; Hector Farkas; Jennifer Fernandez Alcalde; Cl\u00e1udia Foster; Kazunori Freitas; Katar\u00edna Fujita; Daniel Gajdo\u0161ov\u00e1; Marcos Galbraith; Moa Garcia; Sebastian G\u00e4rdenfors; Kim Garza; Filip Gerdes; Iakes Ginter; Koldo Goenaga; Memduh Gojenola; Yoav G\u00f6k\u0131rmak; Xavier G\u00f3mez Goldberg; Berta Gonz\u00e1lez Guinovart; Bernadeta Saavedra; Matias Grici\u016bt\u0117; Normunds Grioni; Bruno Gr\u016bz\u012btis; C\u00e9line Guillaume; Nizar Guillot-Barbance; Jan Habash; Jan Haji\u010d; Mika Haji\u010d Jr; Linh H\u00e0 H\u00e4m\u00e4l\u00e4inen; Na-Rae M\u1ef9; Kim Han; Dag Harris; Johannes Haug; Felix Heinecke; Barbora Hennig; Jaroslava Hladk\u00e1; Florinel Hlav\u00e1\u010dov\u00e1; Petter Hociung; Jena Hohle; Takumi Hwang; Radu Ikeda; Elena Ion; O Irimia; Tom\u00e1\u0161 Ishola; Anders Jel\u00ednek; Fredrik Johannsen; Markus J\u00f8rgensen; H\u00fcner Juutinen; Andre Ka\u015f\u0131kara; Nadezhda Kaasen;  Kabaeva; Hiroshi Sylvain Kahane; Jenna Kanayama; Boris Kanerva; Tolga Katz; Jessica Kayadelen; V\u00e1clava Kenney; Jesse Kettnerov\u00e1; Elena Kirchner; Arne Klementieva; Kamil K\u00f6hn; Natalia Kopacewicz; Jolanta Kotsyba; Simon Kovalevskait\u0117; Sookyoung Krek; Veronika Kwak; Lorenzo Laippala; Lucia Lambertino; Tatiana Lam;  Lando; Alexei Septina Dian Larasati; John Lavrentiev; Phuong L\u00ea Lee; Alessandro H\u1ed3ng; Saran Lenci; Herman Lertpradit;  Leung; Ying Cheuk; Josie Li; Keying Li; Kyungtae Li; Maria Lim; Yuan Liovina; Nikola Li; Olga Ljube\u0161i\u0107; Olga Loginova; Teresa Lyashevskaya; Vivien Lynn; Aibek Macketanz; Michael Makazhanov; Christopher Mandl; Ruli Manning;  Manurung ; Carolyn; Antonio Spadine; Milan Stella; Jana Straka; Alane Strnadov\u00e1; Umut Suhr; Shingo Sulubacak; Zsolt Suzuki; Dima Sz\u00e1nt\u00f3; Yuta Taji; Fabio Takahashi; Takaaki Tamburini; Isabelle Tanaka; Guillaume Tellier; Liisi Thomas; Trond Torga;  Trosterud"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: F 1 -scores of trained error classifiers in relation to the frequency of error, i.e. the error rate of the original model (cf. Sec. 4.2).", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure3: Quantiles of the F 1 distribution by dataset, and whether the classifiers were trained using the full feature set from Tab. 1 (blue) or only the control features (orange).", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "FIFigure 5 :5Figure 5: Quartiles of the top 10 feature categories on UDP by average feature importance (FI).", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": ":POS={VALUE} universal part-of-speech tag, e.g. U:POS=VERB U:{FEAT}={VALUE} universal feature according to the UD specification, e.g. U:TENSE=PAST AMBIG POS =NO |Pt| = 1 where Pt is the set of all observed universal POS tags for t AMBIG POS =YES 1 < |Pt| < 5 Mt is the set of all observed morphological feature combinations for t MATCH where [x0, . . . , xn] is the sequence of edit alignments between t and l,", "figure_data": "FeatureDefinitionMorphological featuresAMBIG POS =HIGH|Pt| \u2265 5AMBIG LEX =NO|Lt| = 1where Lt is the set of all observed lemmata for tAMBIG LEX =YES|Lt| > 1SYNCRETIC=NO where SYNCRETIC=YES |Mt| = 1 1 < |Mt| < 5SYNCRETIC=HIGH|Mt| \u2265 5EDIT=SUF"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Inventory of extracted features (cf. Sec. 4.1). t always denotes the token, l its lemma.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Top 10 feature categories by average feature importance (FI) for each task. All FI scores given \u202210 \u22123", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Average feature importance (FI) for U:POS features on the subset of languages that are both in UDP and SEM.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Examples for how tokens are classified as correct () or incorrect () in our experiments. GOLD shows gold annotations, SYS shows output from an NLP system participating in the respective shared task.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Statistics and classifier results averaged over each task/language pair.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "xn = MATCH xi \u2208 {MATCH, MISMATCH, GAP} EDIT=IN \u2203i, j, k : i < j < k \u2227xi = MATCH \u2227xj = MATCH \u2227x k = MATCH EDIT=FULL \u2200i : xi = MATCH Control features LEN=1-3 1 \u2264 |t| \u2264 3", "formula_coordinates": [4.0, 77.98, 229.93, 303.24, 91.08]}, {"formula_id": "formula_1", "formula_text": "LEN=4-6 4 \u2264 |t| \u2264 6 LEN=7-9 7 \u2264 |t| \u2264 9 LEN=10+ |t| \u2265 10 FREQ=99 P99 \u2264 f (t)", "formula_coordinates": [4.0, 78.2, 323.11, 131.49, 43.22]}, {"formula_id": "formula_2", "formula_text": "FREQ=98 P98 \u2264 f (t) < P99", "formula_coordinates": [4.0, 78.2, 368.43, 156.65, 7.86]}, {"formula_id": "formula_3", "formula_text": "FREQ=95 P95 \u2264 f (t) < P98 FREQ=90 P90 \u2264 f (t) < P95 FREQ=UNCOMMON 4 \u2264 f (t) < P90 FREQ=RARE f (t) < 4", "formula_coordinates": [4.0, 78.2, 378.4, 156.65, 37.75]}, {"formula_id": "formula_4", "formula_text": "\u2206F 1 = F 1 (g f ) \u2212 F 1 (g c ) (1)", "formula_coordinates": [6.0, 361.81, 319.52, 163.73, 10.77]}, {"formula_id": "formula_5", "formula_text": "0.1 0.2 0.3 \u2206 F1", "formula_coordinates": [7.0, 81.05, 174.57, 19.59, 57.84]}, {"formula_id": "formula_6", "formula_text": "f (c) = F 1 (C \u03a6 ) \u2212 F 1 (C \u03a6\\\u03c6c ) (2)", "formula_coordinates": [7.0, 351.41, 707.8, 174.13, 11.22]}], "doi": "10.18653/v1/2020.findings-emnlp.414"}