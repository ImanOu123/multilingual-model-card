{"title": "Fast Image Search for Learned Metrics", "authors": "Prateek Jain; Brian Kulis; Kristen Grauman", "pub_date": "", "abstract": "We introduce a method that enables scalable image search for learned metrics. Given pairwise similarity and dissimilarity constraints between some images, we learn a Mahalanobis distance function that captures the images' underlying relationships well. To allow sub-linear time similarity search under the learned metric, we show how to encode the learned metric parameterization into randomized locality-sensitive hash functions. We further formulate an indirect solution that enables metric learning and hashing for vector spaces whose high dimensionality make it infeasible to learn an explicit weighting over the feature dimensions. We demonstrate the approach applied to a variety of image datasets. Our learned metrics improve accuracy relative to commonly-used metric baselines, while our hashing construction enables efficient indexing with learned distances and very large databases.", "sections": [{"heading": "Introduction", "text": "As the world's store of digital images continues to grow exponentially, and as novel data-rich approaches to computer vision begin to emerge, many interesting problems demand fast techniques capable of accurately searching very large databases of images or image features. For instance, local feature-based recognition methods require searching huge databases of patch descriptors [20], as do new methods for computing 3D models from multi-user photo databases [25]. Similarly, image-or video-based data mining [27,24] and example-based approaches to pose estimation [23,2] seek to leverage extremely large image collections, while nearest neighbor classifiers are frequently employed for recognition and shape matching [31,10]. For most such tasks, the quality of the results relies heavily on the chosen image representation and the distance metric used to compare examples.\nUnfortunately, preferred representations tend to be highdimensional [20,24], and often the best distance metric is one specialized (or learned) for the task at hand [10,31,13], rather than, say, a generic Euclidean norm or Gaussian kernel. Neither factor bodes well for large-scale image search: known data structures for efficient exact search are ineffective for high-dimensional spaces, while existing methods for approximate sub-linear time search are defined only for certain standard metrics. Thus, there is a tension when choosing an image representation and metric, where one must find a fine balance between the suitability for the problem and the convenience of the computation. We are interested in reducing this tension; to that end, in this work we develop a general algorithm that enables fast approximate search for a family of learned metrics and kernel functions.\nA good distance metric between images accurately reflects the true underlying relationships, e.g., the category labels or other hidden parameters. It should report small distances for examples that are similar in the parameter space of interest (or that share a class label), and large distances for examples that are unrelated. General-purpose measures, such as L p norms, are not necessarily well-suited for all learning problems with a given data representation.\nRecent advances in metric learning make it possible to learn distance (or kernel) functions that are more effective for a given problem, provided some partially labeled data or constraints are available [30,3,15,8,10]. By taking advantage of the prior information, these techniques offer improved accuracy when indexing or classifying examples. However, thus far they have limited applicability to very large datasets, since specialized learned distance functions preclude the direct use of known efficient search techniques. Data structures for efficient exact search are known to be ineffective for high-dimensional spaces and can (depending on the data distribution) degenerate to brute force search [9,28]; approximate search methods can guarantee sub-linear time performance, but are defined only for certain generic metrics. As such, searching for similar examples according to a learned metric currently requires an exhaustive (linear) scan of all previously seen examples, in the worst case. This is a limiting factor that thus far deters the use of metric learning with very large image databases.\nIn this work we introduce a method for fast approxi- original distance together, (c) our semi-supervised hash functions incorporate the learned constraints, so that examples constrained to be similar-or other pairs like them-will with high probability hash together. The circular red region in (b) denotes that existing LSH functions generate a hyperplane uniformly at random to separate images, in contrast, as indicated by the blue \"hourglass\" region in (c), our hash functions bias the selection of random hyperplanes to reflect the specified (dis)similarity constraints.\nmate similarity search with learned Mahalanobis metrics. We formulate randomized hash functions that incorporate side-information from partially labeled data or paired constraints, so that examples may be efficiently indexed according to the learned metric without resorting to a naive linear scan of all items. We present a straightforward solution for the case of relatively low-dimensional input vector spaces, and further derive a solution to accommodate very high-dimensional data for which explicit input space computations are infeasible. The former contribution makes fast indexing accessible for numerous existing metric learning methods (e.g., [30,3,8]), while the latter is of particular interest for commonly used image representations, such as bags-of-words, multi-dimensional multi-resolution histograms, and other high-dimensional features.\nWe demonstrate the generality of our approach by applying it to three distinct large-scale image search problems: exemplar-based recognition, pose estimation, and feature indexing. Our method allows rapid and accurate retrieval, and gains over relevant state-of-the-art techniques.", "publication_ref": ["b19", "b24", "b26", "b23", "b22", "b1", "b30", "b9", "b19", "b23", "b9", "b30", "b12", "b29", "b2", "b14", "b7", "b9", "b8", "b27", "b29", "b2", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Recent work has yielded various approaches to metric learning, including several techniques to learn a combination of existing kernels [19,29], as well as methods to learn a Mahalanobis metric [30,3,8], and methods to learn example-specific local distance functions [10]. Embedding functions can be useful both to capture (as closely as possible) a desired set of provided distances between points, as well as to provide an efficient approximation for a known but computationally expensive distance function of interest [1,13]. In contrast to learned metrics, such geometric embeddings are meant to mirror a fixed distance function and do not adapt to reflect supervised constraints.\nIn order to efficiently index multi-dimensional data, data structures based on spatial partitioning and recursive hyperplane decomposition have been developed, e.g. k \u2212 dtrees [9] and metric trees [28]. Due to the particular im-portance of indexing local patch features, several tree-based strategies have also been proposed [4,21] in the vision community. Some such data structures support the use of arbitrary metrics. However, while their expected query time requirement may be logarithmic in the database size, selecting useful partitions can be expensive and requires good heuristics; worse, in high-dimensional spaces all exact search methods are known to provide little query time improvement over a naive linear scan [17].\nAs such, researchers have considered the problem of approximate similarity search, where a user is afforded explicit tradeoffs between the guaranteed accuracy versus speed of a search. Several randomized approximate search algorithms have been developed that allow highdimensional data to be searched in time sub-linear in the size of the database, notably the locality-sensitive hashing (LSH) methods of [17,6]. Data-dependent variants of LSH have been proposed: the authors of [11] select partitions based on where data points are concentrated, while in [23] boosting is used to select feature dimensions that are most indicative of similarity in the parameter space. This tunes the hash functions according to the estimation problem of interest; however, indexed examples must be sorted according to the input space (non-learned) distance.\nWe address the problem of sub-linear time approximate similarity search for a class of learned metrics. While randomized algorithms such as LSH have been employed heavily in vision to mitigate the time complexity of identifying similar examples [22], their use has been restricted to generic measures for which the appropriate hash functions are already defined; that is, direct application to learned metrics was not possible. We instead devise a method that allows knowledge attained from partially labeled data or paired constraints to be incorporated into the hash functions (see Figure 1). Our algorithm is theoretically sound: there is provably no additional loss in accuracy relative to the learned metric beyond the quantifiable loss induced by the approximate search technique.", "publication_ref": ["b18", "b28", "b29", "b2", "b7", "b9", "b0", "b12", "b8", "b27", "b3", "b20", "b16", "b16", "b5", "b10", "b22", "b21"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Approach", "text": "The main idea of our approach is to learn a parameterization of a Mahalanobis metric based on provided labels or paired constraints for some training examples, while simultaneously encoding the learned information into randomized hash functions. These functions will guarantee that the more similar inputs are under the learned metric, the more likely they are to collide in a hash table.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Parameterized Mahalanobis Metrics", "text": "Given n points {x 1 , . . . , x n }, with all x i \u2208 \u211c d , we wish to compute a positive-definite (p.d.) d \u00d7 d matrix A to parameterize the squared Mahalanobis distance:\nd A (x i , x j ) = (x i \u2212 x j ) T A(x i \u2212 x j ),(1)\nfor all i, j = 1, . . . , n. Note that a generalized inner product (kernel) measures the pairwise similarity associated with that distance: s A (x i , x j ) = x T i Ax j . Given a set of interpoint distance constraints, one can directly learn a matrix A to yield a measure that is more accurate for a given classification or clustering problem. Many methods have been proposed for Mahalanobis metric learning [30,3,8]; we consider the information-theoretic metric learning method of [8] because it is kernelizable. Since below we will derive a new algorithm to systematically update semi-supervised hash functions in concert with this metric learner, we next briefly overview the necessary background and equations from [8].", "publication_ref": ["b29", "b2", "b7", "b7", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Information-Theoretic Metric Learning", "text": "Given an initial d \u00d7 d p.d. matrix A 0 specifying prior knowledge about inter-point distances, the learning task is posed as an optimization problem that minimizes the LogDet divergence between matrices A and A 0 , subject to a set of constraints specifying pairs of examples that are similar or dissimilar. In semi-supervised multi-class settings, the constraints are taken directly from the provided labels: points in the same class must be similar, points in different classes are constrained to be dissimilar.\nTo compute A, the LogDet divergence is minimized while enforcing desired constraints: min\nA 0 D \u2113d (A, A 0 ) s. t. d A (x i , x j ) \u2264 u (i, j) \u2208 S, d A (x i , x j ) \u2265 \u2113 (i, j) \u2208 D,(2)\nwhere D \u2113d (A, A 0 ) = tr(AA \u22121 0 ) \u2212 log det(AA \u22121 0 ) \u2212 d, S and D are sets containing pairs of points constrained to be similar and dissimilar, respectively, and \u2113 and u are large and small values, respectively (defined below).\nComputing the optimal solution to (2) involves repeatedly projecting the current solution onto a single constraint, via the update:\nA t+1 = A t + \u03b2 t A t (x it \u2212 x jt )(x it \u2212 x jt ) T A t ,(3)\nwhere x it and x jt are the constrained data points for iteration t, and \u03b2 t is a projection parameter computed by the algorithm.\nWhen the dimensionality of the data is very high, one cannot explicitly work with A, and so the update in (3) cannot be performed. However, one may still implicitly update the Mahalanobis matrix A via updates in kernel space for an equivalent kernel learning problem in which K = X T AX for X = [x 1 , . . . , x n ]. If K 0 is an input kernel matrix for the data, the appropriate update is:\nK t+1 = K t + \u03b2 t K t (e it \u2212 e jt )(e it \u2212 e jt ) T K t ,(4)\nwhere the vectors e it and e jt refer to the i t -th and j t -th standard basis vectors, respectively, and the projection parameter \u03b2 t is the same as in (3) (see [8]). Note that it is possible for the set of examples involved in constraints to be a superset of the set of examples in the input kernel.\nIn the next section we show how to constrain the distribution of randomized hash functions according to a learned parameterization, in the event that A t can be manipulated directly. Then we derive an implicit formulation that enables information-theoretic learning with high-dimensional inputs for which A t cannot be explicitly represented.", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "Hashing for Semi-Supervised Similarity Search", "text": "A family of locality-sensitive hash functions F is a distribution of functions where the following holds: for any two objects x and y,\nPr h\u2208F [h(x) = h(y)] = sim(x, y),(5)\nwhere sim(x, y) is some similarity function defined on the collection of objects [6,17]. When h(x) = h(y), x and y collide in the hash table. Because the probability that two inputs collide is equal to the similarity between them, highly similar objects are indexed together in the hash table with high probability. Existing LSH functions can accommodate the Hamming distance [17], L p norms [7], and inner products [6], and such functions have been explored previously in the vision community [22,23,14].\nIn the following we present new algorithms to construct LSH functions for learned metrics. Specifically, we introduce a family of hash functions that accommodate learned Mahalanobis distances, where we want to retrieve examples x i for an input x q for which the value d A (x i , x q ) resulting from (1) is small, or, in terms of the kernel form, for which the value of s A (x i , x q ) = x T q Ax i is high.\nExplicit Formulation. Given the matrix A for a metric learned as above 1 , such that A = G T G, we generate the following randomized hash functions h r,A , which accept an input point and return a hash key bit:\nh r,A (x) = 1, if r T Gx \u2265 0 0, otherwise ,(6)\nwhere the vector r is chosen at random from a ddimensional Gaussian distribution with zero mean and unit variance. This construction leverages earlier results showing that (i) the probability of two unit vectors having a dot product with random vector r that are opposite in sign is proportional to the angle between them [12], and (ii) the sign of r T x i is therefore a locality-sensitive function for the inner product of any two inputs x i and x j [6].\nThus by parameterizing the hash functions instead by G (which is computable since A is p.d.), we obtain the following relationship:\nPr [h r,A (x i ) = h r,A (x j )] = 1 \u2212 1 \u03c0 cos \u22121 x T i Ax j |Gx i ||Gx j | ,\nwhich sustains the LSH requirement of ( 5) for a learned Mahalanobis metric, whether A is computed using the method of [8] or otherwise [30,3]. Essentially we have shifted the random hyperplane r according to A, and by factoring it by G we allow the random hash function itself to \"carry\" the information about the learned metric. The denominator in the cosine term normalizes the learned kernel values.\nIn this case, we could equivalently transform all the data according to A prior to hashing; however, the choice of presentation here helps set up the more complex formulation we derive below. Note that (6) requires that the input dimension d be low enough that A can be explicitly handled in memory, allowing the updates in (3). Implicit Formulation. We are also interested in the case where the dimensionality d may be very high-say on the order of 10 4 to 10 6 -but the examples are sparse and therefore representable (e.g., bags of words or histogram pyramids [24,13]). Even though the examples are each sparse, the matrix A can be dense, with values for each dimension. In this case, the kernelized metric learning updates in (4) are necessary. However, this complicates the computation of hash functions, as they can no longer be computed directly as in (6) above. Thus, in this section we derive a new algorithm to make simultaneous implicit updates to both the hash functions and the metric.\nWe denote high-dimensional inputs by \u03c6(x) to mark their distinction from the dense inputs x handled earlier.\nWe are initially given c examples that participate in similarity constraints. Let \u03a6 = [\u03c6(x 1 ), . . . , \u03c6(x c )] be the d \u00d7 c matrix of those initial c data points, and let \u03c6(x i ) T \u03c6(x j ) be the initial (non-learned) kernel value between example x i and the input x j . Initially, K 0 = \u03a6 T \u03a6, and so, implicitly, A 0 = I. As in the explicit formulation above, the goal is to wrap G into the hash function, i.e. to compute r T G\u03c6(x), but now we must do so without working directly with G.\nIn the following, we will show that an appropriate hash function h r,A for inputs \u03c6(x) can be defined as:\nhr,A(\u03c6(x)) = \uf6be 1, if r T \u03c6(x) + P c i=1 \u03b3 r i \u03c6(xi) T \u03c6(x) \u2265 0 0, otherwise ,(7)\nwhere \u03c6(x i ) T \u03c6(x) is the original kernel value between x i and the query x, and \u03b3 r i are coefficients computed once (offline) during metric learning (and will be defined below). Note that while G is dense and therefore not manageable, computing r T \u03c6(x) is computationally inexpensive, as only the entries of r corresponding to non-zero entries in \u03c6(x) need to be generated. Should the inputs be highdimensional but dense, our implicit form is still valuable, as we bypass computing O(d 2 ) products with G and require only O(d) inner products for r T \u03c6(x).\nNext we present a construction to express G in terms of the initially chosen c data points, and thus a method to compute ( 7) efficiently. Our construction relies on two technical lemmas, which we list in the appendix. Recall the update rule for A from (3):\nA t+1 = A t + \u03b2 t A t v t v T t A t\n, where v t = \u03c6(y t ) \u2212 \u03c6(z t ), if points y t and z t are involved in the constraint under consideration at iteration t. We emphasize that just as this update must be implemented implicity via (4), so too we must derive an implicit update for the G t matrix required by our hash functions. Since A t is p.d., we can factorize it as A t = G T t G t , which allows us to rewrite the update as:\nA t+1 = G T t (I + \u03b2 t G t v t v T t G T t )G t .\nAs a result, if we factorize I +\u03b2 t G t v t v T t G T t , we can derive an update for G t+1 :\nGt+1 = (I + \u03b2tGtvtv T t G T t ) 1/2 Gt = (I + \u03b1tGtvtv T t G T t )Gt,(8)\nwhere the second equality follows from Lemma 1 using y = G t v t , and \u03b1 t is defined accordingly. Using (8) and Lemma 2, G t can be expressed as G t = I + \u03a6S t \u03a6 T , where S t is a c \u00d7 c matrix of coefficients that determines the contribution of each of the c points to G. Initially, S 0 is set to be zero matrix, and from there every\nS t+1 is iteratively updated in O(c 2 ) time via S t+1 = St+\u03b1t(I +StK0)(ei t \u2212ej t )(ei t \u2212ej t ) T (I +K0S T t )(I +K0St).\nUsing this result, at convergence of the metric learning algorithm we can compute G\u03c6(x) in terms of the c 2 input pairs (\u03c6(x i ), \u03c6(x j )) as follows:\nG\u03c6(x) = \u03c6(x) + \u03a6S\u03a6 T \u03c6(x) = \u03c6(x) + c i=1 c j=1 S ij \u03c6(x j )\u03c6(x i ) T \u03c6(x).\nTherefore, we have\nr T G\u03c6(x) = r T \u03c6(x) + c X i=1 c X j=1 Sij r T \u03c6(xj)\u03c6(xi) T \u03c6(x) = r T \u03c6(x) + c X i=1 \u03b3 r i \u03c6(xi) T \u03c6(x),\nStep  where \u03b3 r i = j S ij r T \u03c6(x j ), and is a notation substitution for the first equality. This notation reflects that the values of each \u03b3 r i rely only on known constrained points, and thus can be efficiently computed in the training phase, prior to hashing anything into the database. Finally, having determined the expression for r T G\u03c6(x), we arrive at our hash function definition in (7). Note the analogy between the use of r T Gx and r T G\u03c6(x) in ( 6) and ( 7), respectively.\nIn this section we presented our main technical contribution: explicit and implicit methods to construct semisupervised hash functions. We emphasize that our formulation is theoretically sound and in itself is novel; what is accomplished would not be possible with a simple merging of the metrics in [8] with LSH.", "publication_ref": ["b5", "b16", "b16", "b6", "b5", "b21", "b22", "b13", "b0", "b11", "b5", "b7", "b29", "b2", "b23", "b12", "b6", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Searching Hashed Examples", "text": "Having constructed LSH functions for learned metrics, we can apply existing methods [17,6] to perform sub-linear time approximate similarity search. Given N data points in a Hamming space and an input x q , approximate nearneighbor (ANN) techniques guarantee retrieval of example(s) within the radius (1 + \u01eb)D from x q in O(N 1/(1+\u01eb )) time, where the true nearest neighbor is at a distance of D from x q . We employ the method of [6], which requires searching M = 2N 1/(1+\u01eb) examples to obtain the first ANN. (Note that M << N for large databases.) After hashing, we only need to compute the learned kernel values between the query and the examples with which it collided. The hashed neighbors are ranked according to these scores, and this ranked list is used for k-NN classification, clustering, etc., depending on the application.\nTo generate b-bit hash keys, we select b random vectors [r 1 , . . . , r b ] to form b hash functions and concatenate the resulting bits from (6) or (7). There is a tradeoff in the selection of b: larger values will increase the accuracy of how well the keys themselves reflect the learned metric, but will increase computation time and can lead to too few collisions in the hash tables. On the other hand, lower values of b make hashing faster, but the key will only coarsely reflect our metric, and too many collisions may result.\nTable 1 summarizes the computational complexity for the main steps of our algorithm: projections during offline metric learning, computing each hash bit for a given point, and computing the ANNs for a hashed query. z is the number of non-zero entries in the query, z \u2264 d. See [18] for more details on the complexity.", "publication_ref": ["b16", "b5", "b5", "b6", "b17"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Results", "text": "The need to search for images or local image descriptors within very large databases arises frequently. In the following we apply our algorithm for image search in three distinct domains: exemplar-based recognition, pose estimation, and feature indexing. In all cases, our experimental goal is twofold: 1) to evaluate the impact on accuracy a learned metric has relative to both standard baseline metrics and state-of-the-art methods, and 2) to test how reliably our semi-supervised hash functions preserve the learned metrics in practice when performing sub-linear time database searches. We therefore report results in terms of both accuracy improvements as well as speedups realized.\nThroughout we select examples for (dis)similarity constraints randomly from among a pool of examples. For categorical data, (dis)similarity constraints are associated with points having different (same) labels; for data with parameter vectors, constraints are determined based on examples' nearness in the parameter space. We compute the distance between all pairs of a subset (\u2248 100) of the database examples according to the non-learned metric, and then let the distance constraints' lower \u2113 and upper u limits be the 1-st and 99-th percentile of those values, respectively. We measure accuracy in terms of the error of the retrieved nearest neighbors' labels, which is either a parameter vector (in the case of the pose data) or a class label (in the case of the patches and object images).\nHuman Body Pose Estimation. First we demonstrate our method applied to single-frame human body pose estimation. Example-based techniques to infer pose (e.g. [2,23]) store a large database of image examples that are labeled with their true pose (i.e., 3d joint positions or angles). A query image is indexed into the database according to image similarity, and the query's pose is estimated based on the pose parameters attached to those nearest neighbors (NN). Thus our objective for this task is to learn a metric for the image features that reports small distances for examples that are close in pose space, and to make the search scalable by hashing according to the learned metric. This is similar to the goals of the parameter-sensitive hashing (PSH) method of [23]. However our approach is distinct from [23] in that it allows one to seamlessly both hash and search according to the learned metric. As a result it may provide more accurate retrievals, as we show empirically below.\nWe use a database of half a million examples provided by the authors of [26], where PSH is employed within a pose tracker. The images were generated with Poser graphics software: human figures in a variety of clothes are rendered in many realistic poses drawn from mocap data. Each image is represented by a d = 24, 016-dimensional multi-scale edge detection histogram (EDH). The vectors' high dimension requires our implicit formulation for semi-supervised  Middle: Error as a function of the number of hash bits. Fast search with the learned metric is more accurate than the L 2 baseline. For both, the error converges around b=500 bits. Right: Hashing error relative to an exhaustive linear scan as a function of \u01eb, which controls the search time required. For each, we constrain the distance of the 10 nearest exemplars (in terms of pose parameters) to be less than \u2113. Similarly, of all the examples with a pose distance greater than a threshold \u03c4 , 10 are randomly picked and their distance to the example is constrained to be greater than u. The values of tau and c are selected with cross-validation.\nM L \u2212 H A S H I N P U T L 2 \u2212 H A S H P S H\nAs baselines, we compute results for NN search with both the Euclidean distance (L 2 ) on the EDH's, and the Hamming distance on the PSH embeddings provided by the authors of [26]. To hash with the L 2 baseline we simply apply [6]. We also use PCA to reduce the dimensionality of the EDH vectors in order to apply our explicit formulation. We measure the error for a query by the mean distance of its true joint positions to the poses in the k-NN. To give a sense of the variety of the data, a random database example is on average at a distance of 34.5 cm from a query.\nThe table in Figure 2 shows the overall errors for each method. (Throughout our approach is denoted by 'ML'.) With a linear scan, ML yields the most accurate retrievals of all methods, and with hashing it outperforms all the hashing-based techniques. The PCA-based results are relatively poor, indicating the need to use the full high-d features and thus our implicit formulation. A paired-error Ttest reveals that our improvements over PSH and L 2 are statistically significant, with 99.95% confidence.\nFigure 3 shows the NN retrieved by each method for five typical queries. In most examples, L 2 and PSH estimate the overall pose reasonably well, but suffer on one or more limbs, whereas our approach more precisely matches all limbs and yields a lower total error. While PSH does not improve over the L 2 baseline for this dataset (as it did for data in [23]), it does do nearly as well as L 2 when using about 16x fewer dimensions; it appears its main advantage here is the ability to significantly reduce the dimension.\nOur semi-supervised hash functions maintain the accuracy of the learned metric, but for orders of magnitude less search time than the linear scan. With our Matlab implementation, a linear scan requires 433.25 s per query, while our hashing technique requires just 1.39 s. On average, metric learning with hashing searches just 0.5% of the database. Figure 2 compares the error obtained by ML+hashing and L 2 +hashing when varying the number of hash bits (middle plot) and the search time allowed (right plot). For a large number of bits, the hash keys are more precise and hence the error drops (although hashing overhead increases). Similarly, since M = 2N 1/(1+\u01eb) , for higher values of \u01eb we must search fewer examples, but accuracy guarantees decrease.\nExemplar-based Object Categorization. Next we evaluate our method applied for NN object recognition with the Caltech-101, a now common benchmark. To compare these images we consider learning kernels on top of the pyramid match kernel (PMK) [13] applied to SIFT features, and the kernel designed in [31] applied to geometric blur features.  Our learned kernels significantly improve NN search accuracy relative to their non-learned counterparts, the CORR and PMK kernels. Right: Comparison of the k-NN classification error when hashing with the original and learned PMK. This plot shows the accuracy-search time tradeoff when using the original or learned hashing functions. CORR refers to the kernel proposed by Zhang et al. [31]. (Best viewed in color.)\nThe PMK uses multi-resolution histograms to estimate the correspondence between two sets of local image features. To hash with the non-learned PMK, the pyramids can be embedded in such a way that standard inner product LSH functions are applicable [14]. The pyramid inputs are sparse but extremely high-dimensional (d = O(10 6 )), thus explicitly representing A is infeasible, and the implicit form of our technique is necessary. The kernel in [31] also measures the correspondences between local features, but by averaging over the minimum distance to matching features in terms of the descriptors and their position in the image; we will refer to it as CORR. Note that we can learn kernels for both the PMK and CORR using our implicit formulation, but can only hash with the learned PMK, since explicit vector space representation (\u03c6(x)) for the CORR is unknown.\nWe first evaluate the effectiveness of metric learning itself on this dataset. We pose a k-NN classification task, and evaluate both the original (PMK or CORR) and learned kernels when used in a linear scan mode. We vary the number of training examples T per class for the database, using the remainder as test examples, and measure accuracy in terms of the mean recognition rate per class, as is standard practice for this dataset.\nFigure 4 shows our results relative to all other existing techniques (left) and specifically against the original baseline kernels for NN (middle). Our approach outperforms all existing single-kernel classifier methods when using the learned CORR kernel: we achieve 61.0% accuracy for T = 15 and 69.6% accuracy for T = 30. Our learned PMK achieves 52.2% accuracy for T = 15 and 62.1% accuracy for T = 30. The middle plot in Figure 4 reveals gains in NN retrieval accuracy; notably, our learned kernels with simple NN classification also outperform the baseline kernels when used with SVMs [31,13]. Only the results of recent multiple-metric approaches [10,29,5] (shown with dashed lines in the left plot) are more accurate, though they also incur the greater cost of applying each of the base kernels in sequence to all examples, while our method requires only one comparison to be computed per example. We hy-pothesize that using the kernels learned with our method along with the ones used in [29,5] would further boost the accuracy; this remains as the subject of future experiments. Now we consider hashing over the learned PMK. For T = 15, our learned hash functions achieve 47% accuracy, and require about 10x less computation time than a linear scan when accounting for the hash key computation (here N = 1515, which is modest compared to the pose data). The rightmost plot in Figure 4 shows the error of our learned PMK-based hashing compared to the baseline [14] as a function of \u01eb. For these data the value of b had little effect on accuracy. As with the linear scan search, we still realize significant accuracy improvements, but now with a guaranteed sub-linear time search.\nIndexing Local Patch Descriptors. Finally, we evaluate our approach on a patch matching task using data provided from the Photo Tourism project [25] and [16]. The dataset contains about 300K local patches extracted from interest points in multiple users' photos of scenes from different viewpoints. The objective is to be able to rapidly identify any matching patches from the same 3d scene point in order to provide correspondences to a structure from motion algorithm. For this application, classifying patches is not so useful; rather, one wants to find all relevant patches. Thus we measure accuracy in terms of precision and recall.\nWe add random jitter (scaling, rotations, and shifts) to all patches as prescribed in [16], extract both the raw patch intensities and SIFT descriptors, and then pose the retrieval task to the L 2 baseline and our learned metrics for each representation. To learn metrics we gather constraints from 10,000 matching and non-matching patch pairs, with a 50-50 mix taken from the Trevi and Halfdome portions of the data. All methods are tested on 100K pairs from the Notre Dame portion. The left plot in Figure 5 compares their accuracy via ROC curves for each feature and metric combination; the numbers in the legend summarize the error in terms of the false positive rate once 95% of the true positives are retrieved. ML+raw intensities yields a significant gain over L 2 +raw, while ML+SIFT also gives some improvement. 2 Finally, we consider our ML-hashing algorithm for the SIFT patches. We measure accuracy by the relevance of the NN ranking: for increasing values of k, we compute the recall rate within the top k-NN. We calculate this score with and without hashing, and before and after metric learning. In order to control k for the hashing, we consider as many nearby hash bins as necessary. In the right plot in Figure 5, we see that the learned metric outperforms the L 2 baseline, and that hashing does not noticeably degrade accuracy. When k = 1000, we search only 16.1% of the database when hashing over the learned metric, and when k = 1, we search only 0.8%, leading to substantial gains in retrieval time (about a factor of 80 vs. linear scan). Conclusions: We have introduced a method to enable efficient approximate similarity search for learned metrics, and experiments show good results for a variety of datasets. Our main contribution is a new algorithm to construct theoretically sound locality-sensitive hash functions-for both implicit and explicit parameterizations of a Mahalanobis distance. For high-dimensional data, we derive simultaneous implicit updates for both the hash function and the learned metric. Experiments with a variety of datasets clearly demonstrate our technique's accuracy and flexibility for large-scale image search tasks.", "publication_ref": ["b1", "b22", "b22", "b22", "b25", "b25", "b5", "b22", "b12", "b30", "b30", "b13", "b30", "b30", "b12", "b9", "b28", "b4", "b28", "b4", "b13", "b24", "b15", "b15", "b1"], "figure_ref": ["fig_2", "fig_3", "fig_2", "fig_5", "fig_5", "fig_5", "fig_6", "fig_6"], "table_ref": []}, {"heading": "", "text": "Acknowledgements: We thank Greg Shakhnarovich for sharing the Poser data. This research was supported in part by grants from ORAU and Microsoft Research.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix", "text": "Proofs are provided in [18]. Lemma 1. Let B = I + \u03b2yy T be p.s.d. Then B 1/2 = I + \u03b1yy T , with \u03b1 = (\u00b1 1 + \u03b2y T y \u2212 1)/y T y.\nLemma 2. For all t, if G 0 = I and S 0 = 0, then\nIn this experiment we were able to reproduce the baseline for L 2 given in [16], however we were unable to do so for their SIFT baseline, for which 6% error is obtained. We suspect this is due to our un-optimized SIFT extraction, and that ML would continue to yield similar improvements as above if provided better descriptors. ", "publication_ref": ["b17", "b15"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "BoostMap: A Method for Efficient Approximate Similarity Rankings", "journal": "", "year": "2004", "authors": "V Athitsos; J Alon; S Sclaroff; G Kollios"}, {"ref_id": "b1", "title": "Estimating 3D Hand Pose from a Cluttered Image", "journal": "", "year": "2003-06", "authors": "V Athitsos; S Sclaroff"}, {"ref_id": "b2", "title": "Learning a Mahalanobis Metric from Equivalence Constraints", "journal": "Journal of Machine Learning Research", "year": "2005-06", "authors": "A Bar-Hillel; T Hertz; N Shental; D Weinshall"}, {"ref_id": "b3", "title": "Shape Indexing Using Approximate Nearest-Neighbour Search in High Dimensional Spaces", "journal": "", "year": "1997", "authors": "J Beis; D Lowe"}, {"ref_id": "b4", "title": "Representing shape with a spatial pyramid kernel", "journal": "", "year": "2007", "authors": "A Bosch; A Zisserman; X Munoz"}, {"ref_id": "b5", "title": "Similarity Estimation Techniques from Rounding Algorithms", "journal": "", "year": "2002", "authors": "M Charikar"}, {"ref_id": "b6", "title": "Locality-Sensitive Hashing Scheme Based on p-Stable Distributions", "journal": "", "year": "2004", "authors": "M Datar; N Immorlica; P Indyk; V Mirrokni"}, {"ref_id": "b7", "title": "Information-Theoretic Metric Learning", "journal": "", "year": "2007", "authors": "J Davis; B Kulis; P Jain; S Sra; I Dhillon"}, {"ref_id": "b8", "title": "An Algorithm for Finding Best Matches in Logarithmic Expected Time", "journal": "ACM Transactions on Mathematical Software", "year": "1977-09", "authors": "J Freidman; J Bentley; A Finkel"}, {"ref_id": "b9", "title": "Learning globally-consistent local distance functions for shape-based image retrieval and classification", "journal": "", "year": "2007", "authors": "A Frome; Y Singer; F Sha; J Malik"}, {"ref_id": "b10", "title": "Mean Shift Based Clustering in High Dimensions: A Texture Classification Example", "journal": "", "year": "2003", "authors": "B Georgescu; I Shimshoni; P Meer"}, {"ref_id": "b11", "title": "Improved Approximation Algorithms for Maximum Cut and Satisfiability Problems Using Semidefinite Programming", "journal": "JACM", "year": "1995", "authors": "M Goemans; D Williamson"}, {"ref_id": "b12", "title": "The Pyramid Match Kernel: Discriminative Classification with Sets of Image Features", "journal": "", "year": "2005", "authors": "K Grauman; T Darrell"}, {"ref_id": "b13", "title": "Pyramid Match Hashing: Sub-Linear Time Indexing Over Partial Correspondences", "journal": "", "year": "2007", "authors": "K Grauman; T Darrell"}, {"ref_id": "b14", "title": "Learning distance functions for image retrieval", "journal": "", "year": "2004-06", "authors": "T Hertz; A Bar-Hillel; D Weinshall"}, {"ref_id": "b15", "title": "Discriminant embedding for local image descriptors", "journal": "", "year": "2007-10", "authors": "G Hua; M Brown; S Winder"}, {"ref_id": "b16", "title": "Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality", "journal": "", "year": "1998", "authors": "P Indyk; R Motwani"}, {"ref_id": "b17", "title": "Fast similarity search for learned metrics", "journal": "", "year": "2007", "authors": "P Jain; B Kulis; K Grauman"}, {"ref_id": "b18", "title": "Learning the kernel matrix with semidefinite programming", "journal": "In Journal of Machine Learning Research", "year": "2004", "authors": "G Lanckriet; N Cristinanini; P Bartlett; L Ghaoui; M Jordan"}, {"ref_id": "b19", "title": "Distinctive Image Features from Scale-Invariant Keypoints", "journal": "IJCV", "year": "2004", "authors": "D Lowe"}, {"ref_id": "b20", "title": "Scalable Recognition with a Vocabulary Tree", "journal": "", "year": "2006", "authors": "D Nister; H Stewenius"}, {"ref_id": "b21", "title": "Nearest-Neighbor Methods in Learning and Vision: Theory and Practice", "journal": "", "year": "2006", "authors": "G Shakhnarovich; T Darrell; P Indyk"}, {"ref_id": "b22", "title": "Fast Pose Estimation with Parameter-Sensitive Hashing", "journal": "", "year": "2003", "authors": "G Shakhnarovich; P Viola; T Darrell"}, {"ref_id": "b23", "title": "Video Data Mining Using Configurations of Viewpoint Ivariant Regions", "journal": "", "year": "2004-06", "authors": "J Sivic; A Zisserman"}, {"ref_id": "b24", "title": "Photo Tourism: Exploring Photo Collections in 3D", "journal": "ACM Press", "year": "2006", "authors": "N Snavely; S Seitz; R Szeliski"}, {"ref_id": "b25", "title": "Conditional random people:tracking humans with crfs and grid filters", "journal": "", "year": "2006", "authors": "L Taycher; G Shakhnarovich; D Demirdjian; T Darrell"}, {"ref_id": "b26", "title": "Tiny images", "journal": "", "year": "2007", "authors": "A Torralba; R Fergus; W T Freeman"}, {"ref_id": "b27", "title": "Satisfying General Proximity / Similarity Queries with Metric Trees", "journal": "Information Processing Letters", "year": "1991", "authors": "J Uhlmann"}, {"ref_id": "b28", "title": "Learning the discriminative power-invariance trade-off", "journal": "", "year": "2007", "authors": "M Varma; D Ray"}, {"ref_id": "b29", "title": "Distance Metric Learning, with Application to Clustering with Side-Information", "journal": "", "year": "2002", "authors": "E Xing; A Ng; M Jordan; S Russell"}, {"ref_id": "b30", "title": "SVM-KNN: Discriminative Nearest Neighbor Classification for Visual Category Recognition", "journal": "", "year": "2006", "authors": "H Zhang; A Berg; M Maire; J Malik"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. (a)  When learning a metric, some paired constraints can be obtained for a portion of the image database, specifying some examples that ought to be treated as similar (straight line) or dissimilar (crossed out line). (b) Whereas existing randomized LSH functions hash examples similar under the original distance together, (c) our semi-supervised hash functions incorporate the learned constraints, so that examples constrained to be similar-or other pairs like them-will with high probability hash together. The circular red region in (b) denotes that existing LSH functions generate a hyperplane uniformly at random to separate images, in contrast, as indicated by the blue \"hourglass\" region in (c), our hash functions bias the selection of random hyperplanes to reflect the specified (dis)similarity constraints.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Explicit Implicit Metric learning projection (offline) O(d 2 ) O(c 2 ) Hashing: compute h r,A (x) O(d) O(z) Search: identify the query's ANNs O(M d) O(M z)", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 .2Figure 2. Pose results. Left: Mean pose error (in cm) obtained with each method. Our approach (denoted ML) outperforms the L 2 baseline and PSH [23].", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 .3Figure 3. Examples of pose estimates. Each column contains a different pose. Top row contains query images, remaining rows show the best pose retrieved by each method. Second row shows best pose obtained by our method hash functions. We use a linear kernel over c = 50 randomly selected examples as the initial kernel (K 0 ). We hold out 1000 test query examples, and generate 1, 000, 000 similarity constraints among 50K of the remaining training examples.For each, we constrain the distance of the 10 nearest exemplars (in terms of pose parameters) to be less than \u2113. Similarly, of all the examples with a pose distance greater than a threshold \u03c4 , 10 are randomly picked and their distance to the example is constrained to be greater than u. The values of tau and c are selected with cross-validation.As baselines, we compute results for NN search with both the Euclidean distance (L 2 ) on the EDH's, and the Hamming distance on the PSH embeddings provided by the authors of[26]. To hash with the L 2 baseline we simply apply[6]. We also use PCA to reduce the dimensionality of the EDH vectors in order to apply our explicit formulation. We measure the error for a query by the mean distance of its true joint positions to the poses in the k-NN. To give a sense of the variety of the data, a random database example", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "examples per class mean recognition rate per class Caltech 101: Comparison to Existing Methods ML+CORR ML+PMK Varma and Ray (ICCV07) Bosch et al. (CIVR07) Frome et al. (ICCV07) Zhang et al.(CVPR06) Lazebnik et al. (CVPR06) Berg (thesis) Mutch & Lowe(CVPR06) Grauman & Darrell(ICCV 2005) Berg et al.(CVPR05) Wang et al.(CVPR06) Holub et al.(ICCV05) Serre et al.(CVPR05) Fei\u2212Fei et al.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 4 .4Figure 4. Caltech-101 results. Left: Comparison against existing techniques. Our method outperforms all other single metric/kernel approaches. Middle:", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 5 .5Photo Tourism: Comparison of ROC curves", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Computational complexity for the proposed method, using variables defined in the text.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "d A (x i , x j ) = (x i \u2212 x j ) T A(x i \u2212 x j ),(1)", "formula_coordinates": [3.0, 90.48, 229.61, 196.04, 12.49]}, {"formula_id": "formula_1", "formula_text": "A 0 D \u2113d (A, A 0 ) s. t. d A (x i , x j ) \u2264 u (i, j) \u2208 S, d A (x i , x j ) \u2265 \u2113 (i, j) \u2208 D,(2)", "formula_coordinates": [3.0, 97.44, 558.69, 189.08, 46.66]}, {"formula_id": "formula_2", "formula_text": "A t+1 = A t + \u03b2 t A t (x it \u2212 x jt )(x it \u2212 x jt ) T A t ,(3)", "formula_coordinates": [3.0, 66.84, 701.69, 219.68, 12.49]}, {"formula_id": "formula_3", "formula_text": "K t+1 = K t + \u03b2 t K t (e it \u2212 e jt )(e it \u2212 e jt ) T K t ,(4)", "formula_coordinates": [3.0, 325.68, 203.21, 219.56, 12.61]}, {"formula_id": "formula_4", "formula_text": "Pr h\u2208F [h(x) = h(y)] = sim(x, y),(5)", "formula_coordinates": [3.0, 359.64, 428.13, 185.6, 15.82]}, {"formula_id": "formula_5", "formula_text": "h r,A (x) = 1, if r T Gx \u2265 0 0, otherwise ,(6)", "formula_coordinates": [4.0, 93.0, 92.69, 193.52, 23.32]}, {"formula_id": "formula_6", "formula_text": "Pr [h r,A (x i ) = h r,A (x j )] = 1 \u2212 1 \u03c0 cos \u22121 x T i Ax j |Gx i ||Gx j | ,", "formula_coordinates": [4.0, 50.16, 256.61, 248.77, 26.89]}, {"formula_id": "formula_7", "formula_text": "hr,A(\u03c6(x)) = \uf6be 1, if r T \u03c6(x) + P c i=1 \u03b3 r i \u03c6(xi) T \u03c6(x) \u2265 0 0, otherwise ,(7)", "formula_coordinates": [4.0, 308.88, 102.08, 239.43, 32.0]}, {"formula_id": "formula_8", "formula_text": "A t+1 = A t + \u03b2 t A t v t v T t A t", "formula_coordinates": [4.0, 397.68, 302.69, 116.05, 12.97]}, {"formula_id": "formula_9", "formula_text": "A t+1 = G T t (I + \u03b2 t G t v t v T t G T t )G t .", "formula_coordinates": [4.0, 355.2, 399.29, 143.65, 13.57]}, {"formula_id": "formula_10", "formula_text": "Gt+1 = (I + \u03b2tGtvtv T t G T t ) 1/2 Gt = (I + \u03b1tGtvtv T t G T t )Gt,(8)", "formula_coordinates": [4.0, 311.52, 444.08, 233.51, 21.68]}, {"formula_id": "formula_11", "formula_text": "S t+1 is iteratively updated in O(c 2 ) time via S t+1 = St+\u03b1t(I +StK0)(ei t \u2212ej t )(ei t \u2212ej t ) T (I +K0S T t )(I +K0St).", "formula_coordinates": [4.0, 308.88, 530.81, 236.2, 25.2]}, {"formula_id": "formula_12", "formula_text": "G\u03c6(x) = \u03c6(x) + \u03a6S\u03a6 T \u03c6(x) = \u03c6(x) + c i=1 c j=1 S ij \u03c6(x j )\u03c6(x i ) T \u03c6(x).", "formula_coordinates": [4.0, 321.6, 599.45, 210.61, 45.85]}, {"formula_id": "formula_13", "formula_text": "r T G\u03c6(x) = r T \u03c6(x) + c X i=1 c X j=1 Sij r T \u03c6(xj)\u03c6(xi) T \u03c6(x) = r T \u03c6(x) + c X i=1 \u03b3 r i \u03c6(xi) T \u03c6(x),", "formula_coordinates": [4.0, 313.44, 657.56, 227.27, 58.9]}, {"formula_id": "formula_14", "formula_text": "M L \u2212 H A S H I N P U T L 2 \u2212 H A S H P S H", "formula_coordinates": [6.0, 35.23, 239.91, 5.87, 172.07]}], "doi": ""}