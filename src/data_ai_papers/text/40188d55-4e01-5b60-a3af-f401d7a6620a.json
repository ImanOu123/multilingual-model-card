{"title": "Bayesian Persuasion in Sequential Decision-Making *", "authors": "Jiarui Gan; Rupak Majumdar; Goran Radanovic; Adish Singla", "pub_date": "2022-05-24", "abstract": "We study a dynamic model of Bayesian persuasion in sequential decision-making settings. An informed principal observes an external parameter of the world and advises an uninformed agent about actions to take over time. The agent takes actions in each time step based on the current state, the principal's advice/signal, and beliefs about the external parameter. The action of the agent updates the state according to a stochastic process. The model arises naturally in many applications, e.g., an app (the principal) can advice the user (the agent) on possible choices between actions based on additional real-time information the app has. We study the problem of designing a signaling strategy from the principal's point of view. We show that the principal has an optimal strategy against a myopic agent, who only optimizes their rewards locally, and the optimal strategy can be computed in polynomial time. In contrast, it is NP-hard to approximate an optimal policy against a far-sighted agent. Further, if the principal has the power to threaten the agent by not providing future signals, then we can efficiently compute a threat-based strategy. This strategy guarantees the principal's payoff as if playing against an agent who is far-sighted but myopic to future signals.", "sections": [{"heading": "Introduction", "text": "Uncertainty is prevalent in models of sequential decision making. Usually, an agent relies on prior knowledge and Bayesian updates as a basic approach to dealing with uncertainties. In many scenarios, a knowledgeable principal has direct access to external information and can reveal it to influence the agent's behavior. For example, a navigation app (the principal) normally knows about the global traffic conditions and can inform a user (the agent), who then decides a particular route based on the app's advice. The additional information can help improve the quality of the agent's decision-making. Meanwhile, by strategically revealing the external information, the principal can also persuade the agent to act in a way beneficial to the principal.\nWe study the related persuasion problem in a dynamic environment. In a static setting, the interaction between the principal and the agent is modeled by Bayesian persuasion (Kamenica and Gentzkow, 2011), where the principal uses their information advantage to influence the agent's strategy in a oneshot game, by way of signaling. In this paper, we extend this setting to include interaction in an infinitehorizon Markov decision process (MDP), where rewards incurred depend on the state of the environment, the action performed, as well as an external parameter sampled from a known prior distribution at each step. The principal, who cannot directly influence the state, observes the realization of this external parameter and signals the agent about their observation. The agent chooses to perform an action based on the state and the signal, and the action updates the state according to a stochastic transition function. Both the principal and the agent aim to optimize their own rewards received in the course of the play.\nIf the objectives of the principal and the agent are completely aligned, the principal should reveal true information about the external parameter, so the more interesting case is when they are misaligned. For example, a user of an navigation app only wants to optimize their commute times but the app may want to incentivize the user to upgrade to a better service, or to increase traffic throughput when the app is provided by a social planner. We consider two major types of agents-myopic and far-sighted-and investigate the problem of optimal signaling strategy design against them. A myopic agent optimizes their payoff locally: in each step, they take an action that will give them the highest immediate reward. It can model a large number of \"short-lived\" agents each appearing instantly in a system (e.g., users of a ride-sharing app or an E-commerce website). A far-sighted agent, on the other hand, optimizes their long-run cumulative reward and considers future information disclosure.\nWe show that, in the myopic setting, an optimal signaling strategy for the principal can be computed in polynomial time through a reduction to linear programming. On the other hand, in the case of a farsighted agent, optimal signaling strategy design becomes computationally intractable: if P =NP, there exists no polynomial time approximation scheme. Our proof of computational intractability is quite general, and extends to showing the hardness of similar principal-agent problems in dynamic settings.\nTo work around the computational barrier, we focus on a special type of far-sighted agents who are advice-myopic. An advice-myopic agent optimizes their cumulative reward over time based on the history of information disclosures, but does not assume that the principal will continue to provide information in the future. We expect such behavior to be a natural heuristic in the real world when agents resort to prior knowledge, but not future information disclosure, to estimate future rewards. We then show that optimal signaling strategies can again be computed in polynomial-time. More interestingly, the solution can be used to design a threat-based signaling strategy against a far-sighted agent. We show that this threat-based strategy induces the same reaction from a far-sighted agent as from an advice-myopic one. Hence, it guarantees the principal the same payoff obtained against an advice-myopic agent, when the agent is actually far-sighted. Figure 1 shows the subtleties of optimal signaling strategies in the dynamic setting.", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Our starting point is the work on Bayesian persuasion (Kamenica and Gentzkow, 2011), which looks at optimal signaling under incomplete information in the static case. Many variants of this model have been proposed and studied ever since, with applications in security, voting, advertising, finance, etc. (e.g., Goldstein and Leitner, 2018;Badanidiyuru et al., 2018;Castiglioni et al., 2020a); also see the comprehensive surveys (Kamenica, 2019;Dughmi, 2017). Dynamic models of Bayesian persuasion were studied recently (Ely, 2017;Renault et al., 2017), and some more recent works focused on algorithmic problems from several dynamic models, such as a model built on extensive-form games (EFGs) (Celli et al., 2020) and an online persuasion model (Castiglioni et al., 2020b(Castiglioni et al., , 2021. These models are sufficiently different from ours. In the EFG model, in particular, an EFG parameterized by the state of nature (akin to our external parameter) is instantiated before the play, and a group of receivers then engage in the EFG and they infer the EFG being played according to signals from a sender. Hence, information exchange happens only once in this model, whereas it happens in every step in ours. Such one-off persuasions also appeared in several other works on Bayesian persuasion and, more broadly, on non-cooperative IRL (inverse reinforcement learning) and incentive exploration Mansour et al., 2021;Simchowitz and Slivkins, 2021).\nReversing the roles of the players in terms of who has the power to commit leads to a dual problem of Bayesian persuasion, which is often known as automated mechanism design Sandholm, 2002, 2004). In such problems, the signal receiver commits to a mechanism that specifies the action they will take upon receiving each signal, and the signal sender sends signals optimally in response. A very recent work by Zhang and Conitzer (2021) considered automated mechanism design in a dy-\ns 0 s 1 s 2 a b a (0, 0) b (0.1, 10) c (0.1, 0) External parameter \u03b8 a \u03b8 b a (1, 1) (\u22121, 0) b (\u22121, 0) (1, 1)\nFigure 1: A simple example: a principal wishes to reach s 2 while maximizing rewards. All transitions are deterministic, and every edge is labeled with the corresponding action and (in the brackets) rewards for the agent and the principal, respectively. The rewards for state-action pairs (s 0 , a) and (s 0 , b) (dashed edges) also depend on the 2-valued external parameter, as specified in the table; the value of the parameter is sampled uniformly at random at each step. Assume uniform discounting with discount 1 2 . Without signaling, the agent will always take action c in s 0 , whereby the principal obtains payoff 0. The principal can reveal information about the external parameter to attract the agent to move to s 1 . If the agent is myopic, the principal can reveal full information, which leads to the agent moving to s 1 , taking action b, and ending in s 2 ; the principal obtains payoff 6 as a result. However, if the agent is far-sighted, this will not work: the agent will end up in a loop in s 0 and s 1 , resulting in overall payoff 4/3 for the principal. To improve, the principal can use a less informative strategy in s 0 : e.g., advising the agent to take the more profitable action 10% of the time and a uniformly sampled action in {a, b} the remaining 90% of the time. The agent will be incentivized to move to s 1 then. Alternatively, the principal can also use a threat-based strategy, which yields an even higher payoff in this instance: always reveal the true information in s 0 , advise the agent to take b in s 1 , and stop providing any information if the agent does not follow the advice. The outcome of this strategy coincides with how an advice-myopic agent behaves: they will choose b at s 1 as future disclosures are not considered. namic setting similar to ours, and offered a complementary view to our work. In their work, the primary consideration is a finite-horizon setting and history-based strategies. In contrast, we focus primarily on unbounded horizons and memory-less strategies.\nThe interaction between the principal and the agent can be viewed as a stochastic game (Shapley, 1953) where one player (i.e., the principal) has the power to make a strategy commitment (Letchford and Conitzer, 2010;Letchford et al., 2012). Games where multiple agents jointly take actions in a dynamic environment have been widely studied in the literature on multi-agent reinforcement learning, but usually in settings without strategy commitment (Littman, 1994;Bu\u015foniu et al., 2010).\nMore broadly, our work also relates to the advice-based interaction framework (e.g., Torrey and Taylor, 2013;Amir et al., 2016), where the principal's goal is to communicate advice to an agent on how to act in the world. This advice-based framework is also in close relationship to the machine teaching literature (Goldman and Kearns, 1995;Singla et al., 2014;Doliwa et al., 2014;Zhu et al., 2018;Ng and Russell, 2000;Hadfield-Menell et al., 2016) where the principal (i.e., the teacher) seeks to find an optimal training sequence to steer the agent (i.e., the learner) towards the desired goal. Similarly, in environment design, the principal modifies the rewards or transitions to steer the behavior of the agent. The objective may be obtaining fast convergence (Ng et al., 1999;Mataric, 1994), or inducing a target policy of the agent (Zhang and Parkes, 2008;Zhang et al., 2009;Ma et al., 2019;Rakhsha et al., 2020b;Huang and Zhu, 2019;Rakhsha et al., 2020a). These problem settings are similar to ours in that the principal cannot directly act in the environment but can influence the agent's actions via learning signals. We see our setting and techniques as complementary to these studies; in particular, our hardness results can be extended there as well.", "publication_ref": ["b17", "b13", "b1", "b3", "b16", "b10", "b11", "b29", "b6", "b4", "b5", "b31", "b30", "b18", "b19", "b20", "b2", "b33", "b0", "b12", "b32", "b9", "b39", "b24", "b14", "b25", "b23", "b36", "b37", "b21", "b28", "b15", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "The Model", "text": "Our formal model is an MDP with reward uncertainties, given by a tuple M = S, A, P, \u0398, (\u00b5 s ) s\u2208S , R, R and involving two players: a principal and an agent. Similar to a standard MDP, S is a finite state space of the environment; A is a finite action space for the agent; P : S \u00d7 A \u00d7 S \u2192 [0, 1] is the transition dynamics of the state. When the environment is in state s and the agent takes action a, the state transitions to s \u2032 with probability P (s, a, s \u2032 ); both the principal and the agent are aware of the state throughout. Meanwhile, rewards are generated for both the principal and the agent, and are specified by the reward functions R : S \u00d7 \u0398 \u00d7 A \u2192 R and R : S \u00d7 \u0398 \u00d7 A \u2192 R, respectively. Hence, unlike in a standard MDP, here the rewards also depend on an external parameter \u03b8 \u2208 \u0398. This parameter captures an additional layer of uncertainty of the environment; it follows a distribution \u00b5 s \u2208 \u2206(\u0398) and is drawn anew every time the state changes. For all s \u2208 S, \u00b5 s is common prior knowledge shared between the principal and the agent; however, only the principal has access to the realization of \u03b8.\nCrucially, since the actions are taken only by the agent, the principal cannot directly influence the state. Instead, the principal can use their information advantage about the external parameter to persuade the agent to take certain actions, by way of signaling.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Signaling and Belief Update", "text": "Let G be a space of signals. A signaling strategy of the principal generates a distribution over G. Our primary consideration in this paper is Markovian signaling strategies, whereby signals to send only depend on the current state (independent of the history). Formally, a signaling strategy \u03c0 = (\u03c0 s ) s\u2208S of the principal consists of a function \u03c0 s : \u0398 \u2192 \u2206(G) for each state s \u2208 S. Upon observing an external parameter \u03b8, the principal will send a signal sampled from \u03c0 s (\u03b8) when the current state is s; we denote by \u03c0 s (\u03b8, g) the probability of g \u2208 G in this distribution.\nThe signal space is broadly construed. For example, one simple signaling strategy is to always reveal the true information, which always sends a deterministic signal g \u03b8 associated with the observed external parameter \u03b8 \u2208 \u0398 (i.e., a message saying \"The current external state is \u03b8\"); formally, we write \u03c0 s (\u03b8) = e g \u03b8 . 1 In contrast, if the same signal is sent irrespective of the external parameter, i.e., \u03c0 s (\u03b8) = \u03c0 s (\u03b8 \u2032 ) for all \u03b8, \u03b8 \u2032 \u2208 \u0398, then the signaling strategy is completely uninformative. Without loss of generality, we assume that signals in G are distinct from each other from the agent's point of view.\nUpon receiving a signal g, the agent updates their posterior belief about the (distribution of) the external parameter: the conditional probability of the parameter being \u03b8 is g) .\nPr(\u03b8|g, \u03c0 s ) = \u00b5s(\u03b8)\u2022\u03c0s(\u03b8,g) \u03b8 \u2032 \u2208\u0398 \u00b5s(\u03b8 \u2032 )\u2022\u03c0s(\u03b8 \u2032 ,\n(1)\nTo derive the above posterior also relies on knowledge about the principal's signaling strategy \u03c0. Indeed, we follow the Bayesian persuasion framework, whereby the principal commits to a signaling strategy \u03c0 at the beginning of the game and announces it to the agent.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Signaling Strategy Optimization", "text": "We take the principal's point of view and investigate the problem of optimal signaling strategy design: given M, find a signaling strategy \u03c0 that maximizes the principal's (discounted) cumulative reward E \u221e t=0 \u03b3 t R(s t , \u03b8 t , a t )|z, \u03c0 , where z = (z s ) s\u2208S is the distribution of the starting state, \u03b3 \u2208 [0, 1) is a discount factor, and the expectation is taken over the trajectory (s t , \u03b8 t , a t ) \u221e t=0 induced by the signaling strategy \u03c0. To completely specify this task requires a behavioral model for the agent. We will consider two major types of agents-myopic and far-sighted-and will define them separately in the next two sections; a myopic agent only cares about their instant reward in each step, whereas a far-sighted agent considers the cumulative reward with respect to a discount factor\u03b3 > 0 (which need not be equal to \u03b3).\nIn summary, the game proceeds as follows. At the beginning, the principal commits to a signaling strategy \u03c0 and announces it to the agent. Then in each step, if the environment is in state s, an external parameter \u03b8 \u223c \u00b5 s is drawn (by nature); the principal observes \u03b8, samples a signal g \u223c \u03c0 s (\u03b8), and sends g to the agent. The agent receives g, updates their belief about \u03b8 (according to (1)), and decides an action a \u2208 A to take accordingly. The state then transitions to s \u2032 \u223c P (s, a, \u2022).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "When Agent is Myopic", "text": "We first consider a myopic agent. A myopic agent aims to maximize their reward in each individual step. Upon receiving a signal g in state s, the agent will take a best action a \u2208 A, which maximizes E \u03b8\u223cPr(\u2022|g,\u03c0s) R(s, \u03b8, a). We study the problem of computing an optimal signaling strategy against a myopic agent, termed OPTSIG-MYOP.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Action Advice", "text": "According to a standard argument via the revelation principle, it is often without loss of generality to consider signaling strategies in the form of action advice. This also holds in our model. Specifically, for any signaling strategy, there exists an equivalent strategy \u03c0 which uses only a finite set G A := {g a : a \u2208 A} of signals, and each signal g a corresponds to an action a \u2208 A; moreover, \u03c0 is incentive compatible (IC), which means that the agent is also incentivized to take the corresponding action a upon receiving g a , i.e., we have E \u03b8\u223cPr(\u2022|ga,\u03c0s) R(s, \u03b8, a) \u2265 E \u03b8\u223cPr(\u2022|ga,\u03c0s) R(s, \u03b8, a \u2032 ) for all a \u2032 \u2208 A, 2 or equivalently:\n\u03b8\u2208\u0398 Pr(\u03b8|g a , \u03c0 s )\u2022 R(s, \u03b8, a) \u2212 R(s, \u03b8, a \u2032 ) \u2265 0 for all a \u2032 \u2208 A.(2)\nIn other words, \u03c0 signals which action the agent should take and it is designed in a way such that the agent cannot be better off deviating from the advised action with respect to the posterior belief. We call a signaling strategy that only uses signals in G A an action advice, and call it an IC action advice if it also satisfies (2). We refer the reader to the appendix for more details about the generality of IC action advices in our model. We can easily characterize the outcome of an IC action advice \u03c0: at each state s, since the agent is incentivized to follow the advice, with probability \u03c6 \u03c0 s (\u03b8, a) := \u00b5 s (\u03b8) \u2022 \u03c0 s (\u03b8, g a ) they will take action a while the realized external parameter is \u03b8; hence, \u03c6 \u03c0 s is a distribution over \u0398 \u00d7 A. We can then define the following set A s \u2286 \u2206(\u0398 \u00d7 A), which contains all such distributions that can be induced by some \u03c0:\nA s = {\u03c6 \u03c0 s : \u03c0 is an IC action advice} .\nIt would now be convenient to view the problem facing the principal as an (single-agent) MDP M * = S, (A s ) s\u2208S , P * , R * , where S is the same state space in M; A s defines an (possibly infinite) action space for each s; the transition dynamics\nP * : S \u00d7 \u2206(\u0398 \u00d7 A) \u00d7 S \u2192 [0, 1] and reward function R * : S \u00d7 \u2206(\u0398 \u00d7 A) \u2192 R are such that P * (s, x, s \u2032 ) = E (\u03b8,a)\u223cx P (s, a, s \u2032 ),\nand R * (s, x) = E (\u03b8,a)\u223cx R(s, \u03b8, a).\nNamely, M * is defined as if the principal can choose actions (which are (\u03b8, a) pairs) freely from A s , whereas the choice is actually realized through persuasion. A policy \u03c3 for M * maps each state s to an action x \u2208 A s , and it corresponds to an IC action advice \u03c0 in M, with \u03c6 \u03c0 s = \u03c3(s) for all s. The problem of designing an optimal action advice then translates to computing an optimal policy for M * . We show next that we can exploit a standard approach to compute an optimal policy but we need to address a key challenge as the action space of M * may contain infinitely many actions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "LP Formulation", "text": "The standard approach to computing an optimal policy for an MDP is to compute a value function V : S \u2192 R that satisfies the Bellman equation:\nV (s) = max x\u2208As R * (s, x) + \u03b3 \u2022 s \u2032 \u2208S P * (s, x, s \u2032 ) \u2022 V (s \u2032 )\nfor all s \u2208 S.\n(3)\nIt is well-known that there exists a unique solution to the above system of equations, from which an optimal policy can be extracted. In particular, one approach to computing this unique solution is by using the following LP (linear program) formulation, where V (s) are the variables; The optimal value of this LP directly gives the cumulative reward of optimal policies under a given initial state distribution z.\nmin V s\u2208S z s \u2022 V (s) (4) s.t. V (s) \u2265 R * (s, x) + \u03b3 \u2022 s \u2032 \u2208S P * (s, x, s \u2032 ) \u2022 V (s \u2032 ) for all s \u2208 S, x \u2208 A s (4a)\nThe issue with this LP formulation is that there may be infinitely many constraints as (4a) must hold for all x \u2208 A s . This differs from MDPs with a finite action space, in which case the LP formulation can be reduced to one with a finite set of constraints, where each constraint corresponds to an action. We address this issue by using the ellipsoid method as sketched below. More practically, we can also derive a concise LP formulation by exploiting the duality principle (see the appendix).\nTheorem 1. OPTSIG-MYOP is solvable in polynomial time.\nProof sketch. We show that LP (4) can be solved in polynomial time by using the ellipsoid method.\nThe key to this approach is to implement the separation oracle in polynomial time. For any given value assignment of the variables (in our problem values of V (s)), the oracle should decide correctly whether all the constraints of the LP are satisfied and, if not, output a violated one.\nTo implement the separation oracle for our problem amounts to solving the following optimization for all s \u2208 S:\nmax x\u2208As R * (s, x) + \u03b3 \u2022 s \u2032 \u2208S P * (s, x, s \u2032 ) \u2022 V (s \u2032 ) \u2212 V (s).\nBy checking if the above maximum value is positive, we can identify if (4a) is violated for some x \u2208 A s . Indeed, the set of IC action advices can be characterized by the constraints in (2), which are linear constraints if we expand Pr(\u03b8|g a , \u03c0 s ) according to (1) and eliminate the denominator (where we also treat \u03c0 s (\u03b8, g a ) as the additional variables and add the constraint x(\u03b8, a) = \u00b5 s (\u03b8) \u2022 \u03c0 s (\u03b8, g a ) for every \u03b8 and a).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "When Agent is Far-sighted", "text": "A far-sighted (FS) agent looks beyond the immediate reward and considers the cumulative reward discounted by\u03b3. We now study signaling strategy design against an FS agent.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Optimal Signaling against FS Agent", "text": "When facing an FS agent, we cannot define an inducible set A s independently for each state. The principal needs to take a global view and aim to induce the agent to use a policy that benefits the principal. We term the problem of optimal signaling strategy design against an FS agent OPTSIG-FS.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Best Response of FS Agent", "text": "We first investigate an FS agent's best response problem. When the principal commits to a signaling strategy \u03c0, the best response problem facing the agent can be formulated as an MDP M \u03c0 = S \u00d7 G, A, P \u03c0 , R \u03c0 . In each step the agent observes the state s \u2208 S of M along with a signal g \u2208 G from the principal; the tuple (s, g) constitutes a state in M \u03c0 , and we call it a meta-state to distinguish it from states in M. From the agent's perspective, after they take action a, the meta-state transitions to (s \u2032 , g \u2032 ) with probability\nP \u03c0 ((s, g), a, (s \u2032 , g \u2032 )) = P (s, a, s \u2032 ) \u2022 E \u03b8\u223c\u00b5 s \u2032 \u03c0 s \u2032 (\u03b8, g \u2032 ).(5)\nNamely, a next state s \u2032 of M is sampled from P (s, a, \u2022), then a new external parameter \u03b8 is sampled from \u00b5 s \u2032 and the principal sends a signal g \u223c \u03c0 s \u2032 (\u03b8). Meanwhile, the following reward is yielded for the agent:\nR \u03c0 ((s, g), a) = E \u03b8\u223cPr(\u2022|g,\u03c0s) R(s, \u03b8, a),(6)\nwhere the posterior belief Pr(\u03b8|g, \u03c0 s ) is defined in (1). Hence, an optimal policy \u03c3 : S \u00d7 G \u2192 A for M \u03c0 defines a best response of the agent against \u03c0. An optimal signaling strategy of the principal maximizes the cumulative reward against the agent's best response.\nInapproximability We show that OPTSIG-FS is highly intractable: even to find an approximate solution to OPTSIG-FS requires solving an NP-hard problem. Hence, it is unlikely that there exists any efficient approximation algorithm for this task, assuming that P=NP is unlikely.\nTheorem 2. Assuming that P = NP, then OPTSIG-FS does not admit any polynomial-time 1 \u03bb 1\u2212\u01ebapproximation algorithm for any constant \u01eb > 0, where \u03bb is the number of states s \u2208 S in which the prior distribution \u00b5 s is non-deterministic (i.e., supported on at least two external parameters). This holds even when |\u0398| = 2 and the discount factors \u03b3,\u03b3 \u2208 (0, 1) are fixed.\nThe proof of Theorem 2 is via a reduction from the MAXIMUM INDEPENDENT SET problem, which is known to be NP-hard to approximate (Zuckerman, 2006). The result may also be of independent interest: It can be easily adapted to show the inapproximability of similar principal-agent problems in dynamic settings. This hardness result also indicates a \"phase transition\" between the cases where\u03b3 = 0 and\u03b3 > 0 given the tractability of OPTSIG-MYOP showed in Section 3.", "publication_ref": ["b40"], "figure_ref": [], "table_ref": []}, {"heading": "Advice-myopic Agent", "text": "The intractability of OPTSIG-FS motivates us to consider advice-myopic (AM) agents, who account for their future rewards like an FS agent does, but behave myopically and ignore the principal's future signals. In other words, they always assume that the principal will disappear in the next step and rely only on their prior knowledge to estimate the future payoff. We refer to the optimal signaling strategy problem against an AM agent as OPTSIG-AM.\nEquivalence to the Myopic Setting Since an AM agent does not consider future signals, their future reward is independent of the principal's signaling strategy. This allows us to define a set of inducible distributions of (\u03b8, a) independently for each state, similarly to our approach to dealing with a myopic agent. In other words, an AM agent is equivalent to a myopic agent who adds a fixed value to their reward function, and this fixed value is the best future reward they can achieve without the help of any signals. This value is independent of the signaling strategy and can be calculated beforehand. Let R + : S \u00d7 \u0398 \u00d7 A \u2192 R be the reward function of this equivalent myopic agent. We have\nR + (s, \u03b8, a) = R(s, \u03b8, a) +\u03b3 \u2022 E s \u2032 \u223cP (s,a,\u2022) V (s \u2032 , g 0 ), (7\n)\nwhere V is the optimal value function of the agent when completely uninformative signals are given.\nIn more detail, let \u22a5: \u0398 \u2192 \u2206(G) be a completely uninformative signaling strategy, with \u22a5 (\u03b8) =\u00ea g 0 for all \u03b8 (i.e., it always sends the same signal g 0 ). Then V is the optimal value function for the MDP M \u22a5 = S \u00d7 {g 0 }, A, P \u22a5 , R \u22a5 , defined the same way as M \u03c0 in Section 4.1, with \u03c0 =\u22a5. Hence, the\nBellman equation gives V (s, g 0 ) = max a\u2208A R \u22a5 (s, \u03b8, a) +\u03b3 \u2022 E (s \u2032 ,g 0 )\u223cP \u22a5 ((s,g 0 ),a,\u2022) V (s \u2032 , g 0 ) = max a\u2208A E \u03b8\u223c\u00b5s R(s, \u03b8, a) +\u03b3 \u2022 E s \u2032 \u223cP (s,a,\u2022) V (s \u2032 , g 0 ) (8)\nfor all s \u2208 S, where the second transition follows by ( 5) and ( 6) and we also use the facts that the posterior Pr(\u2022|g, \u22a5) degenerates to the prior \u00b5 s (\u2022) as \u22a5 is uninformative, and that P \u22a5 ((s, g 0 ), a, (s \u2032 , g 0 )) = P (s, a, s \u2032 ) as the meta-state only transitions among the ones in the form (s, g 0 ).\nWe can compute V efficiently by solving the above Bellman equation. (A standard LP approach suffices given that M \u22a5 has a finite action space.) Then we obtain R + according to (7), with which we can construct an equivalent OPTSIG-MYOP instance and solve it using our algorithm in Section 3. The solution is also optimal to the original OPTSIG-AM instance as we argued above; we state this result in the theorem below and omit the proof.\nTheorem 3. OPTSIG-AM is solvable in polynomial time.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Threat-based Action Advice against FS Agent", "text": "Now that we can efficiently solve OPTSIG-AM, we will show that we can use a solution to OPTSIG-AM to efficiently design a signaling strategy against an FS agent. Interestingly, we can prove that this strategy guarantees the principal the payoff as if they are playing against an AM agent, when the agent is actually FS. The idea is to add a threat in the action advice: if the agent does not take the advised action, then the principal will stop providing any information in future steps (equivalently, switching to strategy \u22a5). Essentially, this amounts to a one-memory strategy, denoted \u031f = (\u031f s ) s\u2208S , where each \u031f s : S \u00d7 \u0398 \u00d7 G \u00d7 A \u2192 \u2206(A) also depends on the signal and the action taken in the previous step (i.e., whether the action follows the signal).\nMore formally, suppose that \u03c0 = (\u03c0 s ) s\u2208S is a solution to OPTSIG-AM and without loss of generality it is an IC action advice. We construct a one-memory strategy:\n\u031f s ((s, \u03b8), g, a) = \u03c0 s (\u03b8), if g \u2208 {g a , null} \u22a5 (\u03b8) =\u00ea g 0 , otherwise(9)\nwhere g and a are the signal and action taken in the previous step (assume that g is initialized to null in the first step); each signal g a advises the agent to take the corresponding action a, and g 0 is a signal that does not correspond to any action.\nOur key finding is that, via this simple threat-based mechanism, the strategy \u031f we design is persuasive for an FS agent: the threat it makes effectively incentivizes the FS agent to take advised actions. To show this, we first analyze the problem facing the agent when the principal commits to \u031f.\nBest Response to \u031f From an FS agent's perspective, the principal committing to \u031f results in an MDP M \u031f = S \u00d7 G, A, P \u031f , R \u031f . We have G = {g 0 } \u222a G A , so each meta-state (s, g) in M \u031f consists of a state of M and a signal from the principal. The transition dynamics depend on whether the signal sent in the current state is g 0 or not (i.e., whether the principal has switched to the threat-mode):\n\u2022 For all (s, g a ) \u2208 S\u00d7G A , the agent following the advised action a results in transition probabilities:\nP \u031f ((s, g a ), a, \u2022) = P \u03c0 ((s, g a ), a, \u2022);(10a)\nOtherwise, i.e., if any action b = a is taken, the principal will fulfill the threat and send g 0 in the next step. Hence,\nP \u031f ((s, g a ), b, (s \u2032 , g)) = g \u2032 \u2208G P \u03c0 ((s, g a ), a, (s \u2032 , g \u2032 )), if g = g 0 0, otherwise(10b)\n\u2022 For all (s, g 0 ) \u2208 S \u00d7 {g 0 }, the threat is activated in these meta-states, we have:\nP \u031f ((s, g 0 ), a, (s \u2032 , g \u2032 )) = P \u22a5 ((s, g 0 ), a, (s \u2032 , g \u2032 )) = P (s, a, s \u2032 ), if g \u2032 = g 0 0, otherwise(10c)\nSimilarly, the reward function differs in meta-states (s, g a ) and (s, g 0 ). We have\nR \u031f ((s, g), \u2022) = R \u03c0 ((s, g), \u2022), if g \u2208 G A R \u22a5 ((s, g 0 ), \u2022), otherwise(11)\nPersuasiveness of \u031f To show the persuasiveness of \u031f, we argue that the following policy \u03c3 : S\u00d7G \u2192 A of the agent, in which the agent always takes the advised action, is optimal in response to \u031f. For all s \u2208 S, we define\n\u03c3(s, g) = a, if g = g a \u2208 G \u0100 \u03c3(s, g 0 ), if g = g 0 (12)\nwhere\u03c3 is an optimal policy against \u22a5, the value function V : S \u00d7 G \u2192 R of which (as from the agent's perspective) is already defined in Section 4.2 and satisfies (8).\nTheorem 4 shows the optimality of \u03c3. Intuitively, we show that the value function of \u03c3 is at least as large as V , so the agent has no incentive to provoke the threat.\nTheorem 4. The policy \u03c3 defined in (12) is an optimal response of an FS agent to \u031f. (Hence, \u031f incentivizes an FS agent to take the advised action.)\nThe direct consequence of Theorem 4 is that \u031f guarantees the principal the best payoff they can obtain when facing an AM agent, even though the agent is FS (Corollary 5). Hence, \u031f serves as an alternative approach to deal with an FS agent. Note that this threat-based strategy may not be an optimal one-memory strategy. Indeed, with minor changes to our proof of Theorem 2, we can show that for any positive integer k the problem of computing an optimal k-memory strategy is inapproximable (see the appendix). In contrast, in the myopic and advice-myopic settings, since the agent's behavior is Markovian, the optimal signaling strategies we designed remain optimal even when we are allowed to use memory-based strategies.\nCorollary 5. By using \u031f against an FS agent, the principal's cumulative reward is the same as the highest cumulative reward they can obtain against an AM agent. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We empirically evaluate signaling strategies obtained with our algorithms. The goal is to compare the payoffs yielded for the principal. We use Python (v3.9) to implement our algorithms and Gurobi (v9.1.2) to solve all the LPs. All results were obtained on a platform with a 2 GHz Quad-Core CPU and 16 GB memory, and each is averaged over at least 20 instances. We conduct experiments on (i) general instances without any specific underlying structure, and (ii) instances generated based on a road navigation application.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "General Instances", "text": "The first set of instances are generated as follows. The transition probabilities and the initial state distribution are generated uniformly at random (and normalized to ensure that they sum up to 1). We also set an integer parameter n * , and change n * states to terminal states. The reward values are first generated uniformly at random from the range [0, 1]. Then, we tune the agent's rewards according to a parameter s, \u03b8, a). Hence, when \u03b2 = 0, the agent's rewards are independent of the principal's; when \u03b2 = 1, they are completely aligned; and when \u03b2 = \u22121, they are zero-sum. We evaluate optimal signaling strategies against a myopic and an AM agent; the latter is equivalent to our threat-based strategy against an FS agent (THREAT-FS). We use two benchmarks, which are by Figure 3: Comparison of signaling strategies in a navigation application: all results are shown as the ratios of FULLCONTROL to them on the y-axes (now that rewards are costs). All curves and axes have the same meanings as in Figure 2. All results are obtained on instances with n = 20 and m = 100 (i.e., numbers of nodes and edges in the network), where we also fix |\u0398| = 3, \u03b3 =\u03b3 = 0.8, and \u03b2 = 0.5 unless they are variables.\n\u03b2 \u2208 [\u22121, 1], resetting R(s, \u03b8, a) \u2190 (1 \u2212 |\u03b2|) \u2022 R(s, \u03b8, a) + \u03b2 \u2022 R(\nnature also the lower and upper bounds of payoffs of other strategies: i) when the principal cannot send any signal and the agent operates with only the prior knowledge (NOSIG-MYOP and NOSIG-AM/FS; AM and FS agents have the same behavior in this case); and ii) when the principal has full control over the agent (FULLCONTROL). For ease of comparison, all results are shown as their ratios to results of FULLCONTROL.\nFigure 2 summarizes the results. It is clearly seen that OPTSIG improves significantly upon NOSIG in all figures. The gap appears to increase with \u03b2, and when \u03b2 \u2265 0 (when the agent's rewards are positively correlated to that of the principal), OPTSIG is very closed to FULLCONTROL. It is also noted that differences between results obtained in the myopic setting and in the FS/AM setting are very small (e.g., compare (a) and (b)). This is mainly due to the random nature of the instances: in expectation, future rewards of all actions are the same. Hence, in the remaining figures we only present results obtained in the myopic setting. As shown in these figures, payoff improvement offered by the optimal strategies increases slowly with the number of actions and the number of external parameters. Intuitively, as these two numbers increase, the agent's decision making in each state becomes more reliant on advice from the principal. Nevertheless, the results do not appear to vary insignificantly with other parameters, such as the number of states or the number of terminal states as shown in (e) and (f) (also see the appendix for additional experiment results).", "publication_ref": [], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "Road Navigation Instances", "text": "In the navigation application, the agent wants to travel from a starting node to a destination node on a road network, and is free to choose any path. In each step, the agent picks a road at the current node and travels through it. The reward the agent receives at each step is a cost representing the travel time through the chosen road, which depends on the congestion level represented by the external parameter.\nThe principal, as a social planner, has a preference over the path the agent picks (e.g., in consideration of the overall congestion or noise levels across the city), and this is encoded in a reward function for the principal: whenever the agent picks a road, the principal also receives a cost according to this reward function. Naturally, the agent's position (the node the agent is on) defines the state of the MDP. For simplicity, we assume that the road network is a directed acyclic graph (DAG), so the agent always reaches the destination in a finite number of steps.\nTo generate an instance, we first generate a random DAG with specified numbers of nodes and edges (roads). Let these numbers be n and m, respectively (n \u2264 m \u2264 n(n\u22121)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "2", "text": "). We sample a Pr\u00fcfer sequence of length n \u2212 2 uniformly at random and then convert it into the corresponding tree. We index the nodes according to their order in a breadth-first search. The node with the smallest/largest index is chosen as the start/destination. Then we add an edge between a pair of nodes chosen uniformly at random, from the node with the smaller index to the node with the larger index, until there are m edges on the graph. In the case that some node has no outgoing edge and it is not the destination, we also add an edge linking this node to the destination, so the graph generated may actually have more than m edges. In this way the graph generated is always a DAG.\nThe results are presented in Figure 3. The results exhibit similar patterns to their counterparts in Figure 2. Nevertheless, the gaps between different strategies appear to be narrower in the FS/AM setting than those in the myopic setting, which is not obvious in Figure 2.", "publication_ref": [], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "Conclusion", "text": "We described and studied a dynamic model of persuasion in infinite horizon Markov processes. Our main results characterize the nature and computational complexity of optimal signaling against different types of agents. A limitation of the current model is that it requires common knowledge of transitions and rewards; hence, studying online versions of our problem (Castiglioni et al., 2020b) is an immediate future step. While we focus on the algorithmic aspects of persuasion, our results indicate how a social planner might influence agents optimally. In particular implementations, the planner's incentives may not be aligned with societal benefits. In these cases, a careful analysis of the persuasion mechanisms and their moral legitimacy must be considered. It can be easily verified that when no signal is used, the agent always prefers to take action a in every state s v , v \u2208 N , resulting in cumulative reward 0 for the principal. Hence, the objective value we consider remains the same after we subtract from it this benchmark cumulative reward.\ns v prob. = 1/m s u prob. = 1/m . . . . . . . . . . . . . . . . . . s \u2032 v s \u2032 u s \u2032\u2032 v s \u2032\u2032 u s X b, 0 b, 0 a, \u03c1(a, \u03b8) b \u03c1(b, \u03b8) b \u03c1(b, \u03b8) a, \u03c1(a, \u03b8) a v , u , 0 a u , v , 0 a,\u03b3 a,\u03b3 b,\u03b3 2 b,\u03b3 2 \u03b8 a \u03b8 b a 1 \u22121 b \u22121 1 value of \u03c1(\u2022, \u2022)", "publication_ref": ["b4"], "figure_ref": [], "table_ref": []}, {"heading": "Correctness of the Reduction", "text": "We show that there is a size-k independent set on G if and only if there is a signaling strategy that gives the principal payoff k \u2022\u03b3 2 /m (and the conversion between the independent set and this signaling strategy can be done in polynomial time). As a result, an efficient approximation algorithm for computing an optimal signaling strategy, if exists, can be efficiently turned into a one for MAX-IND-SET while the approximation ratio is preserved.\nThe \"only if\" direction. Suppose that there is a size-k independent set N * \u2286 N . We show that the following signaling strategy gives the principal payoff k \u2022 \u03b3 2 /m.\n\u2022 In each state s \u2032 v , v \u2208 N * , the principal always reveals the true information, recommending the agent to take the correct action with reward 1. This gives the agent reward 1 in expectation at that step.\n\u2022 In each state s \u2032 v , v \u2208 N \\ N * , the principal reveals no information, so the agent can only rely on their prior knowledge to obtain an expected reward of 0.\nIt can be verified that, given that\u03b3 < 1/2, when the above signling strategy is applied, the agent will be (strictly) incentivized to follow the following trajectory at each s v :\ns v \u2192 s \u2032 v \u2192 s \u2032\u2032 v \u2192 s X , if v \u2208 N * s v \u2192 s X , if v \u2208 N \\ N *\nfor all s \u2208 S, a \u2208 A. The second term on the right side can be rewritten as follows according to (5), where by writing \u00b5 s \u2032 \u2022 \u03c0 s \u2032 we treat \u00b5 s \u2032 = (\u00b5 s \u2032 (\u03b8)) \u03b8\u2208\u0398 as a row vector and \u03c0 s \u2032 = (\u03c0 s \u2032 (\u03b8, g)) \u03b8\u2208\u0398,g\u2208G A as a matrix.\u03b3 We can write the above equation for all s \u2208 S more concisely as\n\u2022 E (s \u2032 ,g \u2032 )\u223cP \u03c0 ((s,ga),a,\u2022) V \u03c3 (s \u2032 , g \u2032 ) =\u03b3 \u2022 E s \u2032 \u223cP (s,a,\u2022),g \u2032 \u223c\u00b5 s \u2032 \u2022\u03c0 s \u2032 V \u03c3 (s \u2032 , g \u2032 ) =\u03b3 \u2022 E s \u2032 \u223cP (s,a,\u2022) U (s \u2032 ). (15\nU = X +\u03b3 \u2022 T \u2022 U,(16)\nwhere we treat U = (U (s)) s\u2208S and X = (X(s)) s\u2208S as two column vectors, with X(s) = E ga\u223c\u00b5s\u2022\u03c0s R \u03c0 ((s, g a ), a); and T is a |S|-by-|S| matrix with\nT (s, s \u2032 ) = \u03b8\u2208\u0398 \u00b5 s (\u03b8) ga\u2208G A \u03c0 s (\u03b8, g a ) \u2022 P (s, a, s \u2032 ).\nNext, we will show that the following equation holds to complete the proof\nV (\u2022, g 0 ) \u2264 X +\u03b3 \u2022 T \u2022 V (\u2022, g 0 ),(17)\nwhere V (\u2022, g 0 ) = V (s, g 0 ) s\u2208S is a column vector. Indeed, once the above equation holds, we will have\nU \u2212 V (\u2022, g 0 ) \u2265\u03b3 \u2022 T \u2022 U \u2212 V (\u2022, g 0 )\nby subtracting it from ( 16). Since all entries of\u03b3 \u2022 T are non-negative, plugging this inequality to itself repeatedly n times gives\nU \u2212 V (\u2022, g 0 ) \u2265\u03b3 \u2022 T \u2022 U \u2212 V (\u2022, g 0 ) \u2265\u03b3 \u2022 T \u2022 \u03b3 \u2022 T \u2022 U \u2212 V (\u2022, g 0 ) . . . \u2265\u03b3 n \u2022 T n \u2022 U \u2212 V (\u2022, g 0 ) .\nNote that all the entries of T n are in [0, 1]. Hence, when n \u2192 \u221e, the right side converges to a zero vector, which implies that U (s) \u2212 V (s, g 0 ) \u2265 0 for all s \u2208 S and hence, the desired result. Now we show that (17) holds to complete the proof, i.e., for all s \u2208 S,\nV (s, g 0 ) \u2264 X(s) +\u03b3 \u2022 s \u2032 \u2208S T (s, s \u2032 ) \u2022 V (s, g 0 ). (18\n)\nExpanding the right side, we obtain the following transitions, where the expectation is taken over g a \u223c \u00b5 s \u2022 \u03c0 s : \nE R \u03c0 ((s, g a ), a) +\u03b3 \u2022 E s \u2032 \u223cP (s,a,\u2022) V (s \u2032 , g 0 ) = E E\nThe first term above can be rewritten as follows:\nE ga\u223c\u00b5s\u2022\u03c0s E \u03b8\u223cPr(\u2022|ga,\u03c0s) R (s, \u03b8, b * ) = E \u03b8\u223c\u00b5s R (s, \u03b8, b * )\nas the marginal distribution of \u03b8 is exactly the prior distribution. Hence, continuing (19), we have\nE E \u03b8\u223cPr(\u2022|ga,\u03c0s) R + (s, \u03b8, b * ) = E \u03b8\u223c\u00b5s R(s, \u03b8, b * ) +\u03b3 \u2022 E s \u2032 \u223cP (s,b * ,\u2022) V (s \u2032 , g 0 ) = V (s, g 0 ),\n(by ( 8)) so ( 18) holds and this completes the proof.\nSince \u03c3(s, g a ) = a, we have For ease of description, fix s and a and let\n\u03a6(b) := R \u031f ((s, g a ), b) +\u03b3 \u2022 E s \u2032 \u223cP (s,b,s \u2032 ) V (s \u2032 , g 0 ).\nWe have 11) and ( 6))\n\u03a6(b) = E \u03b8\u223cPr(\u2022|ga,\u03c0s) R(s, \u03b8, b) +\u03b3 \u2022 E s \u2032 \u223cP (s,b,s \u2032 ) V (s \u2032 , g 0 ) (by (\n= E \u03b8\u223cPr(\u2022|ga,\u03c0s) R + (s, \u03b8, b).\nBy definition, \u03c0 is IC with respect to R + . Thus, (2) holds (with respect to R + ), and this implies that \u03a6(a) \u2265 \u03a6(b). Hence,\n( * ) \u2265 R \u031f ((s, g a ), b) +\u03b3 \u2022 E s \u2032 \u223cP (s,b,s \u2032 ) V (s \u2032 , g 0 ) = R \u031f ((s, g a ), b) +\u03b3 \u2022 E (s \u2032 ,g \u2032 )\u223cP \u031f ((s,ga),b,\u2022) V \u03c3 (s \u2032 , g \u2032 )\nfor all b \u2208 A \\ {a}, where the last transition is due to the fact that, given signal g a , the state will only transition to the ones in the form (s \u2032 , g 0 ) after an action b = a is taken. Hence, (A.2) holds.\nCase 2. g = g 0 . Note that once the principal starts to send signal g 0 , the remaining process stays in the subset S\u00d7{g 0 } of meta-states. The MDP defined on this subset of meta-states is equivalent to M \u22a5 . By definition, \u03c3 prescribes the same action for each state in this subset as\u03c3 does, so we have\nV \u03c3 (s, g 0 ) = V (s, g 0 ) for all s \u2208 S. Consequently, V \u03c3 (s, g 0 ) = V (s, g 0 ) = max b\u2208A E \u03b8\u223c\u00b5s R(s, \u03b8, b) +\u03b3 \u2022 E s \u2032 \u223cP (s,b,\u2022) V (s \u2032 , g 0 ) (by (8)) = max b\u2208A R \u031f ((s, g 0 ), b) +\u03b3 \u2022 E (s \u2032 ,g \u2032 )\u223cP \u031f ((s,g),b,\u2022) V (s \u2032 , g \u2032 ) ,\nwhere the last transition is due to ( 11) and (10c); note that R \u031f ((s, g 0 ), b) = R \u22a5 ((s, g 0 ), b) = E \u03b8\u223c\u00b5s R(s, \u03b8, b) as the posterior Pr(\u2022|g, \u22a5) degenerates to the prior \u00b5 s (\u2022). Hence, (A.2) holds, too, which completes the proof.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.3 Proof of Corollary 5", "text": "According to Theorem 4, \u031f incentivizes an FS agent to take all advised actions. Hence, state transition will only happen among the states in S \u00d7 G A . Any trajectory generated by using \u031f will be generated with the same probability that it is generated by using \u03c0. The principal obtains the same cumulative reward, as a result.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Generality of IC Action Advices", "text": "We show that even when the agent is FS, it is without loss of generality to consider only IC action advices. This will immediately imply the generality of IC action advices in the myopic setting (which is equivalent to the FS setting with\u03b3 = 0). Specifically, let \u03c0 be an arbitrary signaling strategy of the principal, and let \u03c3 : S \u00d7 G \u2192 A be a best response of the agent to \u03c0, i.e., an optimal policy in M \u03c0 = S \u00d7 G, A, P \u03c0 , R \u03c0 (defined in Section 4.1). We construct an action advice \u03c0 \u22c6 : for each s, \u03b8, a, we set \u03c0 \u22c6 s (\u03b8, g a ) =\ng\u2208G: \u03c3(s,g)=a \u03c0 s (\u03b8, g) (Without loss of generality, we assume that \u03c0 s (\u03b8) is supported on a finite set G of signals; if G is infinite, we can change the summation above to integration.) We prove the following proposition.\nProposition 7. Assume that the agent breaks ties by taking the action advised by \u03c0 \u22c6 when there are multiple optimal actions. Then \u03c0 \u22c6 is IC, and the agent's best response to \u03c0 \u22c6 yields as much cumulative reward in M \u03c0 \u22c6 = S \u00d7 G, A, P \u03c0 \u22c6 , R \u03c0 \u22c6 as \u03c3 does in M \u03c0 , both for the agent and the principal.\nProof. We denote by u(M \u03c0 , \u03c3 \u2032 ) the agent's payoff (cumulative reward) for executing some policy \u03c3 \u2032 in M \u03c0 . We first show that \u03c0 \u22c6 is IC, i.e., the following policy \u03c3 \u22c6 : S \u00d7 G A \u2192 A is an optimal policy in M \u03c0 \u22c6 : \u03c3 \u22c6 (s, g a ) = a for all s \u2208 S, a \u2208 A. Note that, by construction, \u03c0 \u22c6 merges signals used by \u03c0, so it is not more informative than \u03c0. Hence, u(M \u03c0 \u22c6 , \u03c3 \u2032 ) \u2264 u(M \u03c0 , \u03c3) for any policy \u03c3 \u2032 , so to prove that \u03c0 \u22c6 is IC it suffices to show that u(M \u03c0 \u22c6 , \u03c3 \u22c6 ) = u(M \u03c0 , \u03c3). Indeed, suppose that the agent uses \u03c3 \u22c6 in M \u03c0 \u22c6 . Conditioned on the environment being in state s, with probability \u00b5 s (\u03b8) \u2022 \u03c0 \u22c6 s (\u03b8, g a ) the agent takes action a while the realized external parameter is \u03b8. By construction we have \u03c0 \u22c6 s (\u03b8, g a ) = g\u2208G: \u03c3(s,g)=a \u03c0 s (\u03b8, g). Hence,\n\u00b5 s (\u03b8) \u2022 \u03c0 \u22c6 s (\u03b8, g a ) = g\u2208G: \u03c3(s,g)=a \u00b5 s (\u03b8) \u2022 \u03c0 s (\u03b8, g).\nThe right side is exactly the conditional probability of the pair (a, \u03b8) when the agent uses \u03c3 in M \u03c0 . As a result, in each state s \u2208 S, every pair (\u03b8, a) \u2208 \u0398 \u00d7 A arises with the same probability in these two situations. Since the state transition and rewards incurred in the original MDP M depend only on the external parameter and the action taken in each state, the agent's cumulative reward is also the same in expectation in these two situations, and we have u(M \u03c0 \u22c6 , \u03c3 \u22c6 ) = u(M \u03c0 , \u03c3). For the same reason, this also holds for the principal's cumulative reward. Therefore, \u03c0 \u22c6 is IC, whereby we also showed that the agent's best response policy yields the same cumulative rewards for both the agent and the principal. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D Additional Experiment Results", "text": "A full set of results obtained on general instances, including those presented in Figure 2 is shown in Figure 5. For the navigation application, we also present an additional set of results in 6. These results are obtained on instances with uniform congestion levels, i.e., all roads have the same congestion level at each step (however, the costs for the roads may still be different). Despite this difference, the results exhibit very similar patterns to those shown in Figure 3. ", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Acknowledgments", "text": "This research was sponsored in part by the Deutsche Forschungsgemeinschaft project 389792660 TRR 248-CPEC. Jiarui Gan was supported by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No. 945719).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Omitted Proofs", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1 Proof of Theorem 2", "text": "We show a reduction from the MAXIMUM INDEPENDENT SET problem (MAX-IND-SET). An instance of MAX-IND-SET is given by an undirected graph G = (E, N ). The goal is to find an independent set of the maximum size: a set of nodes N \u2032 \u2286 N is said to be an independent set if for every pair of nodes v, u \u2208 N \u2032 it holds that {v, u} / \u2208 E. It is known that MAX-IND-SET admits no efficient (1/m + \u01eb)approximation algorithm unless P = NP, where m = |N | (Zuckerman, 2006). The approximation ratios we consider are all multiplicative. Since the objective value (cumulative reward) might be negative, to make the ratios meaningful, we adjust the objective value by subtracting from it a benchmark value that equals the cumulative reward of the principal when no signal is used. This is a value that can be trivially obtained by the principal and the value of an optimal solution is always at least as large and hence non-negative after the adjustment.", "publication_ref": ["b40"], "figure_ref": [], "table_ref": []}, {"heading": "The Reduction", "text": "We only consider the case where\u03b3 is a constant in (0, 1/2) but note that the reduction easily extends to the case where\u03b3 \u2208 [1/2, 1). Specifically, we can modify the reduction by inserting a constant number l = log\u03b3 1 2 of dummy states s \u2032 1 , . . . , s \u2032 l before each state in the MDP constructed below: in each s \u2032 \u2113 , any action taken results in a deterministic state transition from s \u2032 \u2113 to s \u2032 \u2113+1 (and from s \u2032 \u2113 to s when \u2113 = l). We also change all occurrence of\u03b3 in the definition of rewards to\u03b3 \u2032 . This creates an MDP equivalent to the one used in the reduction but with a discount factor\u03b3 \u2032 < 1/2.\nGiven a MAX-IND-SET instance G = (E, N ), we construct an MDP illustrated in Figure 4, where\n\u2022 For each v \u2208 N , there are three states s v , s \u2032 v , and s \u2032\u2032 v . In addition, there is a terminal state s X .\n\u2022 The initial state is sampled from a uniform distribution over states s v , v \u2208 N .\n\u2022 For each pair of adjacent nodes u and v, i.e., {u, v} \u2208 E, there is an action a u,v ; taking a u,v leads the state transitioning to s \u2032 u . There is no such an action if u and v are not adjacent.\n\u2022 The external parameter has two possible values: \u0398 = {\u03b8 a , \u03b8 b }.\n\u2022 The agent's reward for each state-action pair is annotated on the corresponding edge in Figure 4. It depends on the external parameter only in states s \u2032 v , v \u2208 N , where the values are given by a function \u03c1 presented on the right. Namely, action a (respectively, b) is more profitable when the external parameter is \u03b8 a (respectively, \u03b8 b ). We set the prior distribution of the external parameter to a uniform distribution:\n5. Hence, according to the last point above, the principal can persuade the agent only in states s \u2032 v , v \u2208 N . It is not hard to see that through persuasion the principal is able to control the agent's reward between states s \u2032 v and s \u2032\u2032 v within the range [0, 1]. In more detail, reward 0 corresponds to the case where the principal gives completely uninformative signals (e.g., always sending signal a), when the agent can only rely on the prior belief and obtains expected reward 0; and reward 1 corresponds to the case where the principal always reveals the true information, when the agent can always pick the correct action to obtain reward 1.\nFinally, we need to specify the principal's reward.\n\u2022 Let the principal's reward be 1 only for state-action pairs (s \u2032\u2032 v , b), v \u2208 N ; for all other state-action pairs we let the reward be 0. In other words, the principal's payoff depends on the number of states s \u2032\u2032 v at which the agents takes action b.\nThis results in payoff |N * | \u2022 \u03b3 2 /m = k \u2022 \u03b3 2 /m for the principal. Indeed, we argue that by following the above trajectories, the state value function of the agent is as follows; one can easily verify that the actions the agent takes according to the above trajectories are indeed (strictly) optimal with respect these state values.\n\u2022 V (s X ) = 0.\n\u2022 For all v \u2208 N * , we have\nIn particular, to see that V (s \u2032 v ) < 1 for every v \u2208 N \\ N * , even though we did not specify the trajectory after s \u2032 v , we can assume that in general the trajectory takes the following form for some l \u2208 N \u222a {+\u221e}:\nNamely, it visits l state pairs (s \u2032 u , s \u2032\u2032 u ) before it moves to the terminal state. Observe that in the section between s \u2032 u 1 and s \u2032\u2032 u l in T l , the agent obtains reward 1 (i.e., reaches some s \u2032 u , u \u2208 N * ) in at least every four steps given that N * is an independent set. Moreover, since\u03b3 2 <\u03b3 2 1\u2212\u03b3 4 =\u03b3 2 +\u03b3 6 +\u03b3 10 + . . . , when the reward\u03b3 obtained between s \u2032\u2032 u l and s X is at most the cumulative reward the agent would obtain if the trajectory continued after s X and the agent received reward 1 in every four steps starting from the second step after s \u2032\u2032 u l . Hence, overall, the cumulative reward of T l is at most that the agent would obtain if they received reward 1 in every four steps starting from the third step (i.e., after s \u2032 u 1 ), that amounts t\u00f5 \u03b3 2\n1\u2212\u03b3 4 < 1 (given the assumption that\u03b3 < 1/2).\nThe \"if\" direction. Suppose that some signaling strategy \u03c0 (not necessarily deterministic) gives the principal payoff at least k \u2022 \u03b3 2 /m. We show that there exists a size-k independent set (and it can be found efficiently).\nLet \u03c3 be an arbitrary optimal policy of the agent in response to \u03c0. Let N * \u2286 N contains vertices v such that starting from s v , the agent reaches s X with a positive probability by following \u03c3. Since the principal's cumulative reward is at least k \u2022 \u03b3 2 /m, it must be that |N * | \u2265 k. Note that given \u03c0, both \u03c3 and N * can be computed efficiently. We argue that N * is an independent set to complete the proof.\nIndeed, consider any v \u2208 N * . Since the agent reaches s X from s v with a positive probability and \u03c3 is optimal, the agent must at least weakly prefer action b to any a v,u in state s \u2032\u2032 v ; it holds for the corresponding Q-function of \u03c3 that Q \u03c3 (s\n) for all u \u2208 N . Trivially, we have\nConsequently, b is strictly worse than a in state s u , so starting from s u and applying \u03c3 the agent will reach s X with probability zero; we have u / \u2208 N * . Since u is an arbitrary vertex adjacent to v, it follows immediately that N * is an independent set. This completes the proof.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Remarks on the Reduction", "text": "First, the above reduction does not rely on any specific assumption about the tie-breaking behavior of the agent (which defines the policy the agent will choose when there are multiple best responses against \u03c0).\nSecond, the reduction can be easily adapted to show the inapproximability of several related problems on reward poisoning (policy teaching) and commitment in stochastic games. All these problems feature a two-player MDP between a principal and an agent; the players' actions jointly determine the rewards and state transition, and the principal is able to influence the agent's reward in some or all states beforehand (directly, or indirectly through committing to some strategy). The task is to find an optimal way for the principal to influence the agent, which maximizes the principal's discounted cumulative reward. Indeed, the principal's strategy in the above reduction boils down to setting the overall reward for the agent between each pair of states s \u2032 v and s \u2032\u2032 v to a value in [0, 1]. To choose the values optimally is an optimal reward poisoning (policy teaching) problem, where the attacker (teacher) has the power to change rewards in these states. This also corresponds to a stochastic game, where the principal's actions in each state s \u2032 v either give the agent a high payoff 1 or a low payoff 0, irrespective of the action the agent plays. Indeed, it is known that computing an optimal commitment in a stochastic is NP-hard (Letchford et al., 2012) but it was not known how hard it is to approximate.\nThe proof can also be modified to show the hardness of computing a k-memory strategy, simply by inserting k dummy states before each state, so that the principal will always forget what happened in the previous non-dummy state.", "publication_ref": ["b19"], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Proof of Theorem 4", "text": "We show that the value function of \u03c3 satisfies the Bellman equation. Specifically, let V \u03c3 : S \u00d7G \u2192 R be the value function of \u03c3; we will show that the following Bellman equation holds for all (s, g) \u2208 S \u00d7 G:\nwhich then implies that \u03c3 is optimal.\nSince \u03c3 is IC, we have \u03c3(s, g a ) =\u00ea a . Hence, V \u03c3 is the (unique) solution to the following system of equations:\nV \u03c3 (s, g a ) = R \u031f ((s, g a ), a) +\u03b3 \u2022 E (s \u2032 ,g \u2032 )\u223cP \u031f ((s,ga),a,\u2022) V \u03c3 (s \u2032 , g \u2032 ) = R \u03c0 ((s, g a ), a) +\u03b3 \u2022 E (s \u2032 ,g \u2032 )\u223cP \u03c0 ((s,ga),a,\u2022) V \u03c3 (s \u2032 , g \u2032 ), for all s \u2208 S, a \u2208 A (13\nwhere we replace R \u031f and P \u031f with R \u03c0 and P \u03c0 , respectively, according to (11) and (10a). Consider each (s, g) \u2208 S \u00d7 G and the following two cases with respect to g.\nCase 1. g \u2208 G A . Suppose that g = g a (a \u2208 A). The following key lemma compares V \u03c3 and V . Intuitively, V serves as a conservative estimate to the expected future reward yielded by \u03c3. Lemma 6. E \u03b8\u223c\u00b5s,g\u223c\u03c0s(\u03b8) V \u03c3 (s, g) \u2265 V (s, g 0 ) for all s \u2208 S.\nProof. For ease of description, let U (s) := E \u03b8\u223c\u00b5s,g\u223c\u03c0s(\u03b8) V \u03c3 (s, g) for all s \u2208 S.\nAccording to (13), we have V \u03c3 (s, g a ) = R \u03c0 ((s, g a ), a) +\u03b3 \u2022 E (s \u2032 ,g \u2032 )\u223cP \u03c0 ((s,ga),a,\u2022) V \u03c3 (s \u2032 , g \u2032 ),", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Interactive teaching strategies for agent training", "journal": "", "year": "2016", "authors": "Ofra Amir; Ece Kamar; Andrey Kolobov; Barbara J Grosz"}, {"ref_id": "b1", "title": "Targeting and signaling in ad auctions", "journal": "SIAM", "year": "2018", "authors": "Ashwinkumar Badanidiyuru; Kshipra Bhawalkar; Haifeng Xu"}, {"ref_id": "b2", "title": "Multi-agent reinforcement learning: An overview. Innovations in Multi-Agent Systems and Applications-1", "journal": "", "year": "2010", "authors": "Lucian Bu\u015foniu; Robert Babu\u0161ka; Bart De Schutter"}, {"ref_id": "b3", "title": "Persuading voters: It's easy to whisper, it's hard to speak loud", "journal": "", "year": "2020", "authors": "Matteo Castiglioni; Andrea Celli; Nicola Gatti"}, {"ref_id": "b4", "title": "Online Bayesian persuasion", "journal": "", "year": "2020", "authors": "Matteo Castiglioni; Andrea Celli; Alberto Marchesi; Nicola Gatti"}, {"ref_id": "b5", "title": "Multi-receiver online bayesian persuasion", "journal": "", "year": "2021", "authors": "Matteo Castiglioni; Alberto Marchesi; Andrea Celli; Nicola Gatti"}, {"ref_id": "b6", "title": "Private bayesian persuasion with sequential games", "journal": "", "year": "2020", "authors": "Andrea Celli; Stefano Coniglio; Nicola Gatti"}, {"ref_id": "b7", "title": "Complexity of mechanism design", "journal": "Morgan Kaufmann", "year": "2002", "authors": "Vincent Conitzer; Tuomas Sandholm"}, {"ref_id": "b8", "title": "Self-interested automated mechanism design and implications for optimal combinatorial auctions", "journal": "", "year": "2004", "authors": "Vincent Conitzer; Tuomas Sandholm"}, {"ref_id": "b9", "title": "Recursive teaching dimension, VC-dimension and sample compression", "journal": "Journal of Machine Learning Research", "year": "2014", "authors": "Thorsten Doliwa; Gaojian Fan; Hans Ulrich Simon; Sandra Zilles"}, {"ref_id": "b10", "title": "Algorithmic information structure design", "journal": "ACM SIGecom Exch", "year": "2017", "authors": "S Dughmi"}, {"ref_id": "b11", "title": "", "journal": "Beeps. American Economic Review", "year": "2017", "authors": "J Ely"}, {"ref_id": "b12", "title": "On the complexity of teaching", "journal": "Journal of Computer and System Sciences", "year": "1995", "authors": "Sally A Goldman; Michael J Kearns"}, {"ref_id": "b13", "title": "Stress tests and information disclosure", "journal": "Journal of Economic Theory", "year": "2018", "authors": "Itay Goldstein; Yaron Leitner"}, {"ref_id": "b14", "title": "Cooperative inverse reinforcement learning", "journal": "", "year": "2016", "authors": "Dylan Hadfield-Menell; J Stuart; Pieter Russell; Anca Abbeel;  Dragan"}, {"ref_id": "b15", "title": "Deceptive reinforcement learning under adversarial manipulations on cost signals", "journal": "", "year": "2019", "authors": "Yunhan Huang; Quanyan Zhu"}, {"ref_id": "b16", "title": "Bayesian persuasion and information design", "journal": "Annual Review of Economics", "year": "2019", "authors": "Emir Kamenica"}, {"ref_id": "b17", "title": "Bayesian persuasion", "journal": "American Economic Review", "year": "2011", "authors": "Emir Kamenica; Matthew Gentzkow"}, {"ref_id": "b18", "title": "Computing optimal strategies to commit to in extensive-form games", "journal": "", "year": "2010", "authors": "Joshua Letchford; Vincent Conitzer"}, {"ref_id": "b19", "title": "Computing optimal strategies to commit to in stochastic games", "journal": "", "year": "2012", "authors": "Joshua Letchford; Liam Macdermed; Vincent Conitzer; Ronald Parr; Charles L Isbell"}, {"ref_id": "b20", "title": "Markov games as a framework for multi-agent reinforcement learning", "journal": "Elsevier", "year": "1994", "authors": " Michael L Littman"}, {"ref_id": "b21", "title": "Policy poisoning in batch reinforcement learning and control", "journal": "", "year": "2019", "authors": "Yuzhe Ma; Xuezhou Zhang; Wen Sun; Xiaojin Zhu"}, {"ref_id": "b22", "title": "Bayesian exploration: Incentivizing exploration in Bayesian games", "journal": "Operations Research", "year": "", "authors": "Yishay Mansour; Alex Slivkins; Vasilis Syrgkanis; Zhiwei Steven Wu"}, {"ref_id": "b23", "title": "Reward functions for accelerated learning", "journal": "", "year": "1994", "authors": "J Maja;  Mataric"}, {"ref_id": "b24", "title": "Algorithms for inverse reinforcement learning", "journal": "", "year": "2000", "authors": "Y Andrew; Stuart J Ng;  Russell"}, {"ref_id": "b25", "title": "Policy invariance under reward transformations: Theory and application to reward shaping", "journal": "", "year": "1999", "authors": "Andrew Y Ng; Daishi Harada; Stuart J Russell"}, {"ref_id": "b26", "title": "Information disclosure as a means to security", "journal": "", "year": "2015", "authors": "Zinovi Rabinovich; Albert Xin Jiang; Manish Jain; Haifeng Xu"}, {"ref_id": "b27", "title": "Policy teaching in reinforcement learning via environment poisoning attacks. CoRR, abs", "journal": "", "year": "2011", "authors": "Amin Rakhsha; Goran Radanovic; Rati Devidze; Xiaojin Zhu; Adish Singla"}, {"ref_id": "b28", "title": "Policy teaching via environment poisoning: Training-time adversarial attacks against reinforcement learning", "journal": "", "year": "2020", "authors": "Amin Rakhsha; Goran Radanovic; Rati Devidze; Xiaojin Zhu; Adish Singla"}, {"ref_id": "b29", "title": "Optimal dynamic information provision", "journal": "Games and Economic Behavior", "year": "2017", "authors": "J Renault; E Solan; N Vieille"}, {"ref_id": "b30", "title": "Stochastic games", "journal": "Proceedings of the National Academy of Sciences", "year": "1953", "authors": "S Lloyd;  Shapley"}, {"ref_id": "b31", "title": "Exploration and incentives in reinforcement learning", "journal": "", "year": "2021", "authors": "Max Simchowitz; Aleksandrs Slivkins"}, {"ref_id": "b32", "title": "Near-optimally teaching the crowd to classify", "journal": "", "year": "2014", "authors": "Adish Singla; Ilija Bogunovic; G\u00e1bor Bart\u00f3k; Amin Karbasi; Andreas Krause"}, {"ref_id": "b33", "title": "Teaching on a budget: Agents advising agents in reinforcement learning", "journal": "", "year": "2013", "authors": "Lisa Torrey; Matthew Taylor"}, {"ref_id": "b34", "title": "Exploring information asymmetry in two-stage security games", "journal": "", "year": "2015", "authors": "Haifeng Xu; Zinovi Rabinovich; Shaddin Dughmi; Milind Tambe"}, {"ref_id": "b35", "title": "Automated dynamic mechanism design", "journal": "", "year": "", "authors": "Hanrui Zhang; Vincent Conitzer"}, {"ref_id": "b36", "title": "Value-based policy teaching with active indirect elicitation", "journal": "", "year": "2008", "authors": "Haoqi Zhang; David C Parkes"}, {"ref_id": "b37", "title": "Policy teaching through reward function learning", "journal": "", "year": "2009", "authors": "Haoqi Zhang; David C Parkes; Yiling Chen"}, {"ref_id": "b38", "title": "Non-cooperative inverse reinforcement learning", "journal": "", "year": "2019", "authors": "Xiangyuan Zhang; Kaiqing Zhang; Erik Miehling; Tamer Basar"}, {"ref_id": "b39", "title": "An overview of Machine teaching", "journal": "CoRR", "year": "2018", "authors": "Xiaojin Zhu; Adish Singla; Sandra Zilles; Anna N Rafferty"}, {"ref_id": "b40", "title": "Linear degree extractors and the inapproximability of max clique and chromatic number", "journal": "Association for Computing Machinery", "year": "2006", "authors": "David Zuckerman"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure2: Comparison of signaling strategies: all results are shown as ratios to FULLCONTROL on the y-axes. Meanings of x-axes are noted in the captions. Shaded areas represent standard deviations (mean \u00b1 standard deviation). In all figures, we fix |S| = |\u0398| = |A| = 10, \u03b3 =\u03b3 = 0.8, n * = 5, and \u03b2 = 0 unless they are variables.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Reduction from MAX-IND-SET. The two nodes u and v illustrated above are adjacent, i.e., {u, v} \u2208 E. All state transitions are deterministic. Annotated on each edge are the action name and the reward of the agent for the corresponding state-action pair. Only the rewards related to states s \u2032 v , v \u2208 N depend on the external parameter, whose values are given by \u03c1 presented on the right.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "U(s) = E ga\u223c\u00b5s\u2022\u03c0s V \u03c3 (s, g a ) = E ga\u223c\u00b5s\u2022\u03c0s R \u03c0 ((s, g a ), a) +\u03b3 \u2022 E s \u2032 \u223cP (s,a,\u2022) U (s \u2032 )(plug in (14) and (15))= E ga\u223c\u00b5s\u2022\u03c0s R \u03c0 ((s, g a ), a) +\u03b3 \u2022 E ga\u223c\u00b5s\u2022\u03c0s E s \u2032 \u223cP (s,a,\u2022) U (s \u2032 ).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "\u03b8\u223cPr(\u2022|ga,\u03c0s) R (s, \u03b8, a) +\u03b3 \u2022 E s \u2032 \u223cP (s,a,\u2022) V (s \u2032 , g 0 ) (by (6)) = E E \u03b8\u223cPr(\u2022|ga,\u03c0s) R (s, \u03b8, a) +\u03b3 \u2022 E s \u2032 \u223cP (s,a,\u2022) V (s \u2032 , g 0 ) = E E \u03b8\u223cPr(\u2022|ga,\u03c0s) R + (s, \u03b8, a)(by (7))\u2265 E E \u03b8\u223cPr(\u2022|ga,\u03c0s) R + (s, \u03b8, b) , for all b \u2208 A. The last transition holds as \u03c0 is IC with respect to R + (so we have (2) with R + in place of the reward function). In particular, this holds for b * \u2208 arg max b\u2208A E \u03b8\u223c\u00b5s R(s, \u03b8, b) +\u03b3 \u2022 E s \u2032 \u223cP (s,b,\u2022) V (s \u2032 , g 0 ) , so expanding R + according to (7) gives E E \u03b8\u223cPr(\u2022|ga,\u03c0s) R + (s, \u03b8, b * ) = E E \u03b8\u223cPr(\u2022|ga,\u03c0s) R(s, \u03b8, b * ) +\u03b3 \u2022 E s \u2032 \u223cP (s,b * ,\u2022) V (s \u2032 , g 0 ) = E E \u03b8\u223cPr(\u2022|ga,\u03c0s) R(s, \u03b8, b * ) +\u03b3 \u2022 E s \u2032 \u223cP (s,b * ,\u2022) V (s \u2032 , g 0 ).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "V\u03c3 (s, g a ) = R \u031f ((s, g a ), a) +\u03b3 \u2022 E (s \u2032 ,g \u2032 )\u223cP \u031f ((s,ga),a,\u2022) V \u03c3 (s \u2032 , g \u2032 ) = R \u031f ((s, g a ), a) +\u03b3 \u2022 E (s \u2032 ,g \u2032 )\u223cP \u03c0 ((s,ga),a,\u2022) V \u03c3 (s \u2032 , g \u2032 ) (by (10a)) = R \u031f ((s, g a ), a) +\u03b3 \u2022 E s \u2032 \u223cP (s,a,s \u2032 ) E \u03b8 \u2032 \u223c\u00b5 s \u2032 ,g \u2032 \u223c\u03c0 s \u2032 (\u03b8 \u2032 ) V \u03c3 (s \u2032 , g \u2032 ) (by (5)) \u2265 R \u031f ((s, g a ), a) +\u03b3 \u2022 E s \u2032 \u223cP (s,a,s \u2032 ) V (s \u2032 , g 0 ) (by Lemma 6) ( * )", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "CPDual LP Formulation for OPTSIGMYOPNote that we can rewrite (4a) asV (s) \u2265 max x\u2208As R * (s, x) + \u03b3 \u2022 s \u2032 \u2208S P * (s, x, s \u2032 ) \u2022 V (s \u2032 )for all s \u2208 S Consider the optimization problem on the right side. We can replace x with \u03c0 by usingx(\u03b8, a) = \u00b5 s (\u03b8) \u2022 \u03c0 s (\u03b8, g a ).This results in the following LP, where \u03c0 s (\u03b8, a) are the variables and the constraints ensure that \u03c0 is an IC action advice.maximize:\u03b8\u2208\u0398 a\u2208A R(s, \u03b8, a) + \u03b3 s \u2032 \u2208S P (s, a, s \u2032 ) \u2022 V (s \u2032 ) \u2022 \u00b5 s (\u03b8) \u2022 \u03c0 s (\u03b8, g a )(20)subject to:\u03b8\u2208\u0398 \u00b5 s (\u03b8) \u2022 \u03c0 s (\u03b8, g a ) \u2022 R(s, \u03b8, a) \u2265 \u03b8\u2208\u0398 \u00b5 s (\u03b8) \u2022 \u03c0 s (\u03b8, g a ) \u2022 R(s, \u03b8, b) for all a, b \u2208 A (20a) a\u2208A \u03c0 s (\u03b8, g a ) = 1 for all \u03b8 \u2208 \u0398 (20b) \u03c0 s (\u03b8, g a ) \u2265 0 for all \u03b8 \u2208 \u0398, a \u2208 A (20c)We can further write the above LP in its dual form, which gives the following dual LP, where I s (a, b), J s (\u03b8), and K s (a, \u03b8) are the dual variables corresponding to Constraints (20a), (20b), and (20c), respec-(s, a, s\u2032 ) \u2022 V (s \u2032 ) \u2022 \u00b5 s (\u03b8) = R(s, \u03b8, b) \u2212 R(s, \u03b8, a) \u2022 \u00b5 s (\u03b8) \u2022 I s (a, b) + J s (\u03b8) \u2212 K(a, \u03b8) for all \u03b8 \u2208 \u0398, a, b \u2208 A (21a) I s (a, b) \u2265 0 for all a, b \u2208 A (21b) K s (a, \u03b8) \u2265 0 for all \u03b8 \u2208 \u0398, a, b \u2208 A (21c)Since the optimal objective value of the dual LP is equal to that of the original problem, we obtain the following new formulation for (4), where the dual variables are included as additional variables. Constraint (22a) we do not require that the right side is minimized. Indeed, for any feasible solution (V, I s , J s , K s ) to (22), we haveV (s) \u2265 \u03b8\u2208\u0398 J s (\u03b8) \u2265 OPT-DUAL = max x\u2208As R * (s, x) + \u03b3 \u2022 s \u2032 \u2208S P * (s, x, s \u2032 ) \u2022 V (s \u2032 ) ,so V is a feasible solution to the original LP (4), where OPT-DUAL denotes the optimal value of (21). Conversely, any feasible solution V \u2032 to (4), corresponds to a feasible solution (V \u2032 , I \u2032 s , J \u2032 s , K \u2032 s ) to (22), where (I \u2032 s , J \u2032 s , K \u2032 s ) is a solution to (21) with V in the coefficients fixed to V \u2032 ; namely, we haveV \u2032 (s) \u2265 max x\u2208As R * (s, x) + \u03b3 \u2022 s \u2032 \u2208S P * (s, x, s \u2032 ) \u2022 V \u2032 (s \u2032 ) = OPT-DUAL = \u03b8\u2208\u0398 J \u2032 s (\u03b8).", "figure_data": ""}, {"figure_label": "56", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 5 :Figure 6 :56Figure 5: Comparison of signaling strategies: a full set of results including those in Figure 2 and additional ones.All results are shown as ratios to FULLCONTROL on the y-axes. Meanings of x-axes are noted in the captions. Shaded areas represent standard deviations (mean \u00b1 standard deviation). In all figures, we fix |S| = |\u0398| = |A| = 10, \u03b3 =\u03b3 = 0.8, n * = 5, and \u03b2 = 0 unless they are variables.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "s 0 s 1 s 2 a b a (0, 0) b (0.1, 10) c (0.1, 0) External parameter \u03b8 a \u03b8 b a (1, 1) (\u22121, 0) b (\u22121, 0) (1, 1)", "formula_coordinates": [3.0, 180.23, 77.62, 255.21, 95.35]}, {"formula_id": "formula_1", "formula_text": "Pr(\u03b8|g, \u03c0 s ) = \u00b5s(\u03b8)\u2022\u03c0s(\u03b8,g) \u03b8 \u2032 \u2208\u0398 \u00b5s(\u03b8 \u2032 )\u2022\u03c0s(\u03b8 \u2032 ,", "formula_coordinates": [4.0, 222.84, 527.57, 140.5, 17.98]}, {"formula_id": "formula_2", "formula_text": "\u03b8\u2208\u0398 Pr(\u03b8|g a , \u03c0 s )\u2022 R(s, \u03b8, a) \u2212 R(s, \u03b8, a \u2032 ) \u2265 0 for all a \u2032 \u2208 A.(2)", "formula_coordinates": [5.0, 154.92, 400.8, 368.31, 25.45]}, {"formula_id": "formula_3", "formula_text": "A s = {\u03c6 \u03c0 s : \u03c0 is an IC action advice} .", "formula_coordinates": [5.0, 213.36, 566.81, 168.5, 14.6]}, {"formula_id": "formula_4", "formula_text": "P * : S \u00d7 \u2206(\u0398 \u00d7 A) \u00d7 S \u2192 [0, 1] and reward function R * : S \u00d7 \u2206(\u0398 \u00d7 A) \u2192 R are such that P * (s, x, s \u2032 ) = E (\u03b8,a)\u223cx P (s, a, s \u2032 ),", "formula_coordinates": [5.0, 72.0, 617.88, 451.01, 51.53]}, {"formula_id": "formula_5", "formula_text": "V (s) = max x\u2208As R * (s, x) + \u03b3 \u2022 s \u2032 \u2208S P * (s, x, s \u2032 ) \u2022 V (s \u2032 )", "formula_coordinates": [6.0, 141.36, 210.96, 236.19, 25.97]}, {"formula_id": "formula_6", "formula_text": "min V s\u2208S z s \u2022 V (s) (4) s.t. V (s) \u2265 R * (s, x) + \u03b3 \u2022 s \u2032 \u2208S P * (s, x, s \u2032 ) \u2022 V (s \u2032 ) for all s \u2208 S, x \u2208 A s (4a)", "formula_coordinates": [6.0, 113.16, 326.36, 410.07, 53.97]}, {"formula_id": "formula_7", "formula_text": "max x\u2208As R * (s, x) + \u03b3 \u2022 s \u2032 \u2208S P * (s, x, s \u2032 ) \u2022 V (s \u2032 ) \u2212 V (s).", "formula_coordinates": [6.0, 176.4, 569.28, 242.42, 25.97]}, {"formula_id": "formula_8", "formula_text": "P \u03c0 ((s, g), a, (s \u2032 , g \u2032 )) = P (s, a, s \u2032 ) \u2022 E \u03b8\u223c\u00b5 s \u2032 \u03c0 s \u2032 (\u03b8, g \u2032 ).(5)", "formula_coordinates": [7.0, 180.84, 268.32, 342.39, 14.68]}, {"formula_id": "formula_9", "formula_text": "R \u03c0 ((s, g), a) = E \u03b8\u223cPr(\u2022|g,\u03c0s) R(s, \u03b8, a),(6)", "formula_coordinates": [7.0, 211.68, 344.69, 311.55, 14.12]}, {"formula_id": "formula_10", "formula_text": "R + (s, \u03b8, a) = R(s, \u03b8, a) +\u03b3 \u2022 E s \u2032 \u223cP (s,a,\u2022) V (s \u2032 , g 0 ), (7", "formula_coordinates": [8.0, 183.96, 176.16, 335.06, 14.69]}, {"formula_id": "formula_11", "formula_text": ")", "formula_coordinates": [8.0, 519.02, 179.33, 4.21, 9.75]}, {"formula_id": "formula_12", "formula_text": "Bellman equation gives V (s, g 0 ) = max a\u2208A R \u22a5 (s, \u03b8, a) +\u03b3 \u2022 E (s \u2032 ,g 0 )\u223cP \u22a5 ((s,g 0 ),a,\u2022) V (s \u2032 , g 0 ) = max a\u2208A E \u03b8\u223c\u00b5s R(s, \u03b8, a) +\u03b3 \u2022 E s \u2032 \u223cP (s,a,\u2022) V (s \u2032 , g 0 ) (8)", "formula_coordinates": [8.0, 72.0, 261.41, 451.23, 84.57]}, {"formula_id": "formula_13", "formula_text": "\u031f s ((s, \u03b8), g, a) = \u03c0 s (\u03b8), if g \u2208 {g a , null} \u22a5 (\u03b8) =\u00ea g 0 , otherwise(9)", "formula_coordinates": [8.0, 182.16, 666.32, 341.07, 27.9]}, {"formula_id": "formula_14", "formula_text": "P \u031f ((s, g a ), a, \u2022) = P \u03c0 ((s, g a ), a, \u2022);(10a)", "formula_coordinates": [9.0, 231.6, 230.93, 291.51, 13.64]}, {"formula_id": "formula_15", "formula_text": "P \u031f ((s, g a ), b, (s \u2032 , g)) = g \u2032 \u2208G P \u03c0 ((s, g a ), a, (s \u2032 , g \u2032 )), if g = g 0 0, otherwise(10b)", "formula_coordinates": [9.0, 158.76, 288.6, 364.47, 28.87]}, {"formula_id": "formula_16", "formula_text": "P \u031f ((s, g 0 ), a, (s \u2032 , g \u2032 )) = P \u22a5 ((s, g 0 ), a, (s \u2032 , g \u2032 )) = P (s, a, s \u2032 ), if g \u2032 = g 0 0, otherwise(10c)", "formula_coordinates": [9.0, 131.4, 353.52, 391.83, 28.99]}, {"formula_id": "formula_17", "formula_text": "R \u031f ((s, g), \u2022) = R \u03c0 ((s, g), \u2022), if g \u2208 G A R \u22a5 ((s, g 0 ), \u2022), otherwise(11)", "formula_coordinates": [9.0, 196.8, 418.85, 326.31, 29.52]}, {"formula_id": "formula_18", "formula_text": "\u03c3(s, g) = a, if g = g a \u2208 G \u0100 \u03c3(s, g 0 ), if g = g 0 (12)", "formula_coordinates": [9.0, 211.32, 506.6, 311.79, 28.05]}, {"formula_id": "formula_19", "formula_text": "\u03b2 \u2208 [\u22121, 1], resetting R(s, \u03b8, a) \u2190 (1 \u2212 |\u03b2|) \u2022 R(s, \u03b8, a) + \u03b2 \u2022 R(", "formula_coordinates": [10.0, 72.0, 669.2, 275.95, 10.91]}, {"formula_id": "formula_20", "formula_text": "s v prob. = 1/m s u prob. = 1/m . . . . . . . . . . . . . . . . . . s \u2032 v s \u2032 u s \u2032\u2032 v s \u2032\u2032 u s X b, 0 b, 0 a, \u03c1(a, \u03b8) b \u03c1(b, \u03b8) b \u03c1(b, \u03b8) a, \u03c1(a, \u03b8) a v , u , 0 a u , v , 0 a,\u03b3 a,\u03b3 b,\u03b3 2 b,\u03b3 2 \u03b8 a \u03b8 b a 1 \u22121 b \u22121 1 value of \u03c1(\u2022, \u2022)", "formula_coordinates": [17.0, 142.87, 75.76, 372.23, 222.59]}, {"formula_id": "formula_21", "formula_text": "s v \u2192 s \u2032 v \u2192 s \u2032\u2032 v \u2192 s X , if v \u2208 N * s v \u2192 s X , if v \u2208 N \\ N *", "formula_coordinates": [17.0, 214.2, 714.36, 173.91, 29.65]}, {"formula_id": "formula_22", "formula_text": "\u2022 E (s \u2032 ,g \u2032 )\u223cP \u03c0 ((s,ga),a,\u2022) V \u03c3 (s \u2032 , g \u2032 ) =\u03b3 \u2022 E s \u2032 \u223cP (s,a,\u2022),g \u2032 \u223c\u00b5 s \u2032 \u2022\u03c0 s \u2032 V \u03c3 (s \u2032 , g \u2032 ) =\u03b3 \u2022 E s \u2032 \u223cP (s,a,\u2022) U (s \u2032 ). (15", "formula_coordinates": [20.0, 151.44, 118.2, 367.16, 32.69]}, {"formula_id": "formula_23", "formula_text": "U = X +\u03b3 \u2022 T \u2022 U,(16)", "formula_coordinates": [20.0, 254.28, 255.08, 268.83, 10.91]}, {"formula_id": "formula_24", "formula_text": "T (s, s \u2032 ) = \u03b8\u2208\u0398 \u00b5 s (\u03b8) ga\u2208G A \u03c0 s (\u03b8, g a ) \u2022 P (s, a, s \u2032 ).", "formula_coordinates": [20.0, 188.4, 305.16, 218.42, 25.57]}, {"formula_id": "formula_25", "formula_text": "V (\u2022, g 0 ) \u2264 X +\u03b3 \u2022 T \u2022 V (\u2022, g 0 ),(17)", "formula_coordinates": [20.0, 227.16, 355.4, 295.95, 11.85]}, {"formula_id": "formula_26", "formula_text": "U \u2212 V (\u2022, g 0 ) \u2265\u03b3 \u2022 T \u2022 U \u2212 V (\u2022, g 0 )", "formula_coordinates": [20.0, 213.36, 401.0, 163.59, 11.85]}, {"formula_id": "formula_27", "formula_text": "U \u2212 V (\u2022, g 0 ) \u2265\u03b3 \u2022 T \u2022 U \u2212 V (\u2022, g 0 ) \u2265\u03b3 \u2022 T \u2022 \u03b3 \u2022 T \u2022 U \u2212 V (\u2022, g 0 ) . . . \u2265\u03b3 n \u2022 T n \u2022 U \u2212 V (\u2022, g 0 ) .", "formula_coordinates": [20.0, 193.44, 450.2, 198.39, 63.81]}, {"formula_id": "formula_28", "formula_text": "V (s, g 0 ) \u2264 X(s) +\u03b3 \u2022 s \u2032 \u2208S T (s, s \u2032 ) \u2022 V (s, g 0 ). (18", "formula_coordinates": [20.0, 195.36, 565.32, 323.24, 25.85]}, {"formula_id": "formula_29", "formula_text": ")", "formula_coordinates": [20.0, 518.6, 568.49, 4.51, 9.75]}, {"formula_id": "formula_30", "formula_text": "E R \u03c0 ((s, g a ), a) +\u03b3 \u2022 E s \u2032 \u223cP (s,a,\u2022) V (s \u2032 , g 0 ) = E E", "formula_coordinates": [20.0, 122.64, 628.08, 205.95, 36.79]}, {"formula_id": "formula_32", "formula_text": "E ga\u223c\u00b5s\u2022\u03c0s E \u03b8\u223cPr(\u2022|ga,\u03c0s) R (s, \u03b8, b * ) = E \u03b8\u223c\u00b5s R (s, \u03b8, b * )", "formula_coordinates": [21.0, 177.24, 265.2, 240.75, 14.41]}, {"formula_id": "formula_33", "formula_text": "E E \u03b8\u223cPr(\u2022|ga,\u03c0s) R + (s, \u03b8, b * ) = E \u03b8\u223c\u00b5s R(s, \u03b8, b * ) +\u03b3 \u2022 E s \u2032 \u223cP (s,b * ,\u2022) V (s \u2032 , g 0 ) = V (s, g 0 ),", "formula_coordinates": [21.0, 142.68, 328.32, 205.71, 53.69]}, {"formula_id": "formula_34", "formula_text": "\u03a6(b) := R \u031f ((s, g a ), b) +\u03b3 \u2022 E s \u2032 \u223cP (s,b,s \u2032 ) V (s \u2032 , g 0 ).", "formula_coordinates": [21.0, 185.28, 541.56, 224.66, 14.69]}, {"formula_id": "formula_35", "formula_text": "\u03a6(b) = E \u03b8\u223cPr(\u2022|ga,\u03c0s) R(s, \u03b8, b) +\u03b3 \u2022 E s \u2032 \u223cP (s,b,s \u2032 ) V (s \u2032 , g 0 ) (by (", "formula_coordinates": [21.0, 107.64, 588.84, 328.78, 14.69]}, {"formula_id": "formula_36", "formula_text": "= E \u03b8\u223cPr(\u2022|ga,\u03c0s) R + (s, \u03b8, b).", "formula_coordinates": [21.0, 131.76, 610.56, 122.06, 14.41]}, {"formula_id": "formula_37", "formula_text": "( * ) \u2265 R \u031f ((s, g a ), b) +\u03b3 \u2022 E s \u2032 \u223cP (s,b,s \u2032 ) V (s \u2032 , g 0 ) = R \u031f ((s, g a ), b) +\u03b3 \u2022 E (s \u2032 ,g \u2032 )\u223cP \u031f ((s,ga),b,\u2022) V \u03c3 (s \u2032 , g \u2032 )", "formula_coordinates": [21.0, 172.44, 673.56, 250.47, 33.53]}, {"formula_id": "formula_38", "formula_text": "V \u03c3 (s, g 0 ) = V (s, g 0 ) for all s \u2208 S. Consequently, V \u03c3 (s, g 0 ) = V (s, g 0 ) = max b\u2208A E \u03b8\u223c\u00b5s R(s, \u03b8, b) +\u03b3 \u2022 E s \u2032 \u223cP (s,b,\u2022) V (s \u2032 , g 0 ) (by (8)) = max b\u2208A R \u031f ((s, g 0 ), b) +\u03b3 \u2022 E (s \u2032 ,g \u2032 )\u223cP \u031f ((s,g),b,\u2022) V (s \u2032 , g \u2032 ) ,", "formula_coordinates": [22.0, 72.0, 114.05, 451.23, 81.68]}, {"formula_id": "formula_39", "formula_text": "\u00b5 s (\u03b8) \u2022 \u03c0 \u22c6 s (\u03b8, g a ) = g\u2208G: \u03c3(s,g)=a \u00b5 s (\u03b8) \u2022 \u03c0 s (\u03b8, g).", "formula_coordinates": [22.0, 189.96, 718.49, 215.18, 25.76]}], "doi": ""}