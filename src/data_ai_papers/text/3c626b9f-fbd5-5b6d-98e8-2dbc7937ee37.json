{"title": "Image Parsing: Unifying Segmentation, Detection, and Recognition", "authors": "Zhuowen Tu; Xiangrong Chen; Alan L Yuille; Song-Chun Zhu", "pub_date": "2005-02", "abstract": "In this paper we present a Bayesian framework for parsing images into their constituent visual patterns. The parsing algorithm optimizes the posterior probability and outputs a scene representation as a \"parsing graph\", in a spirit similar to parsing sentences in speech and natural language. The algorithm constructs the parsing graph and re-configures it dynamically using a set of moves, which are mostly reversible Markov chain jumps. This computational framework integrates two popular inference approaches-generative (top-down) methods and discriminative (bottom-up) methods. The former formulates the posterior probability in terms of generative models for images defined by likelihood functions and priors. The latter computes discriminative probabilities based on a sequence (cascade) of bottom-up tests/filters. In our Markov chain algorithm design, the posterior probability, defined by the generative models, is the invariant (target) probability for the Markov chain, and the discriminative probabilities are used to construct proposal probabilities to drive the Markov chain. Intuitively, the bottom-up discriminative probabilities activate top-down generative models. In this paper, we focus on two types of visual patterns-generic visual patterns, such as texture and shading, and object patterns including human faces and text. These types of patterns compete and cooperate to explain the image and so image parsing unifies image segmentation, object detection, and recognition (if we use generic visual patterns only then image parsing will correspond to image segmentation (Tu  and Zhu, 2002. IEEE Trans. PAMI, 24(5):657-673). We illustrate our algorithm on natural images of complex city scenes and show examples where image segmentation can be improved by allowing object specific knowledge to disambiguate low-level segmentation cues, and conversely where object detection can be improved by using generic visual patterns to explain away shadows and occlusions.", "sections": [{"heading": "Introduction", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Objectives of Image Parsing", "text": "We define image parsing to be the task of decomposing an image I into its constituent visual patterns. The output is represented by a hierarchical graph W -called the \"parsing graph\". The goal is to optimize the Bayesian posterior probability p(W | I). Figure 1 illustrates a typical example where a football scene is first divided into three parts at a coarse level: a person in the foreground, a sports field, and the spectators. These three parts are further decomposed into nine visual patterns in the second level: a face, three texture regions, some text, a point process (the band on the field), a curve process (the markings on the field), a color region, and a region for nearby people. In principle, we can continue decomposing these parts until we reach a resolution limit (e.g. there is not sufficient resolution to detect the blades of grass on the sports field). The parsing graph is similar in spirit to the parsing trees used in speech and natural language processing (Manning and Sch\u00fctze, 2003) except that it can include horizontal connections (see the dashed curves in Fig. 1) for specifying spatial relationships and boundary sharing between different visual patterns.\nFigure 1. Image parsing example. The parsing graph is hierarchical and combines generative models (downward arrows) with horizontal connections (dashed lines), which specify spatial relationships between the visual patterns. See Fig. 4 for a more abstract representation including variables for the node attributes.\nAs in natural language processing, the parsing graph is not fixed and depends on the input image(s). An image parsing algorithm must construct the parsing graph on the fly. 1 Our image parsing algorithm consists of a set of reversible Markov chain jumps (Green, 1995) with each type of jump corresponding to an operator for reconfiguring the parsing graph (i.e., creating or deleting nodes or changing the values of node attributes). These jumps combine to form an ergodic and reversible Markov chain in the space of possible parsing graphs. The Markov chain probability is guaranteed to converges to the invariant probability p(W | I) and the Markov chain will simulate fair samples from this probability. 2 Our approach is built on previous work on Data-Driven Markov Chain Monte Carlo (DDMCMC) for recognition (Zhu et al., 2000), segmentation (Tu and Zhu, 2002a), grouping (Tu and Zhu, 2002b) and graph partitioning Zhu, 2003, 2004).\nImage parsing seeks a full generative explanation of the input image in terms of generative models, p(I | W ) and p(W ), for the diverse visual patterns which occur in natural images, see Fig. 1. This differs from standard approaches to computer vision tasks-such as segmentation, grouping, and recognition-which usually involve isolated vision modules which only explain different parts (or aspects) of the image. The image  (Tu and Zhu, 2002a) which uses only generic visual patterns (i.e. only low-level visual cues). The results (b) show that low-level visual cues are not sufficient to obtain good intuitive segmentations. The limitations of using only generic visual patterns are also clear in the synthesized images (c) which are obtained by stochastic sampling from the generative models after the parameters have been estimated by DDMCMC. The right panels (d) show the segmentations obtained by human subjects who, by contrast to the algorithm, appear to use object specific knowledge when doing the segmentation (though they were not instructed to) (Martin et al., 2001). We conclude that to achieve good segmentation on these types of images requires combining segmentation with object detection and recognition.\nparsing approach enables these different modules to cooperate and compete to give a consistent interpretation of the entire image.\nThe integration of visual modules is of increasing importance as progress on the individual modules starts approaching performance ceilings. In particular, work on segmentation (Shi and Malik, 2000;Fowlkes and Malik, 2004) and edge detection (Konishi and Coughlan, 2003;Bowyer et al., 2001) has reached performance levels where there seems little room for improvement when only low-level cues are used. For example, the segmentation failures in Fig. 2 can only be resolved by combining segmentation with object detection and recognition. Combining these cues is made easier because of recent successful work on the detection and recognition of objects (Lowe, 2003;Weber et al., 2000;Ponce et al., 2004;Belongie et al., 2002;Viola and Jones, 2001;Wu et al., 2004) and the classification of natural scenes (Barnard and Forsyth, 2001;Murphy et al., 2003) using, broadly speaking, discriminative methods based on local bottom-up tests.\nBut combining different visual modules requires a common framework which ensures consistency. Despite the effectiveness of discriminative methods for computing scene components, such as object labels and categories, they can also generate redundant and conflicting results. Mathematicians have argued (Blanchard and Geman, 2003) that discriminative methods must be followed by more sophisticated processes to (i) remove false alarms, (ii) amend missing objects by global context information, and (iii) reconcile conflicting (overlapping) explanations through model comparison. In this paper, we impose such processes by using generative models for the entire image.\nAs we will show, our image parsing algorithm is able to integrate discriminative and generative methods so as to take advantage of their complementary strengths. Moreover, we can couple modules such as segmentation and object detection by our choice of the set of visual patterns used to parse the image. In this paper, we focus on two types of patterns:-generic visual patterns for low/middle level vision, such as texture and shading, and object patterns for high level vision, such as frontal human faces and text.\nThese two types of patterns illustrate different ways in which the parsing graph can be constructed (see Fig. 20 and the related discussion). The object patterns (face and text) have comparatively little variability so they can often be effectively detected as a whole by bottom-up tests and their parts can be located subsequentially. Thus their parsing sub-graphs can be constructed in a \"decompositional\" manner from whole to parts. By contrast, a generic texture region has arbitrary shape and its intensity pattern has high entropy. Detecting such a region by bottom-up tests will require an enormous number of tests to deal with all this variability, and so will be computationally impractical. Instead, the parsing subgraphs should be built by grouping small elements in a \"compositional\" manner (Bienenstock et al., 1997).\nWe illustrate our algorithm on natural images of complex city scenes and give examples where image segmentation can be improved by allowing object specific knowledge to disambiguate low-level cues, and conversely object detection can be improved by using generic visual patterns to explain away shadows and occlusions.\nThis paper is structured as follows. In Section 2, we give an overview of the image parsing framework and discuss its theoretical background. Then in Section 3, we describe the parsing graph and the generative models used for generic visual patterns, text, and faces. In Section 4 we give the control structure of the image parsing algorithm. Section 5 gives details of the components of the algorithm. Section 6 shows how we combine AdaBoost with other tests to get proposals for detecting objects including text and faces. In Section 7 we present experimental results. Section 8 addresses some open problems in further developing the image parser as a general inference engine. We summarize the paper in Section 9.", "publication_ref": ["b20", "b58", "b34", "b43", "b16", "b25", "b7", "b29", "b53", "b41", "b3", "b52", "b54", "b2", "b38", "b5", "b4"], "figure_ref": ["fig_3", "fig_0", "fig_0"], "table_ref": []}, {"heading": "Overview of Image Parsing Framework", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Bottom-Up and Top-Down Processing", "text": "A major element of our work is to integrate discriminative and generative methods for inference. In the recent computer vision literature, top-down and bottom-up procedures can be broadly categorized into two popular inference paradigms-generative methods for \"topdown\" and discriminative methods for \"bottom-up\", illustrated in Fig. 3. From this perspective, integrating generative and discriminative models is equivalent to combining bottom-up and top-down processing. 3 The role of bottom-up and top-down processing in vision has been often discussed. There is growing experimental evidence (see Thorpe et al., 1996;Figure 3. Comparison of two inference paradigms: Top-down generative methods versus bottom-up discriminative methods. The generative method specifies how the image I can be synthesized from the scene representation W. By contrast, the discriminative methods are based by performing tests Tstj(I) and are not guaranteed to yield consistent solutions, see crosses explained in the text. Li et al., 2003) that humans can perform high level scene and object categorization tasks as fast as low level texture discrimination and other so-called preattentive vision tasks. This suggests that humans can detect both low and high level visual patterns at early stages in visual processing. It contrasts with traditional bottom-up feedforward architectures (Marr, 1982) which start with edge detection, followed by segmentation/grouping, before proceeding to object recognition and other high-level vision tasks. These experiments also relate to long standing conjectures about the role of the bottom-up/top-down loops in the visual cortical areas (Mumford, 1995;Ullman, 1995), visual routines and pathways (Ullman, 1984), the binding of visual cues (Treisman, 1986), and neural network models such as the Helmholtz machine (Dayan et al., 1995). But although combining bottom-up and top-down processing is clearly important, there has not yet been a rigorous mathematical framework for how to achieve it.\nIn this paper, we combine generative and discriminative approaches to design an DDMCMC algorithm which uses discriminative methods to perform rapid inference of the parameters of generative models. From a computer vision perspective, DDMCMC combines bottom-up processing, implemented by the discriminative models, together with top-down processing by the generative models. The rest of this section gives an overview of our approach.", "publication_ref": ["b44", "b27", "b33", "b37", "b51", "b50", "b45", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Generative and Discriminative Methods", "text": "Generative methods specify how the image I is generated from the scene representation W \u2208 . It combines a prior p(W) and a likelihood function p(I | W ) to give a joint posterior probability p(W | I). These can be expressed as probabilities on graphs, where the input image I is represented on the leaf nodes and W denotes the remaining nodes and node attributes of the graph. The structure of the graph, and in particular the number of nodes, is unknown and must be estimated for each input image.\nTo perform inference using generative methods requires estimating W * = arg max P(W | I). This is often computationally demanding because there are usually no known efficient inference algorithms (certainly not for the class of P(W | I) studied in this paper).\nIn this paper, we will perform inference by stochastic sampling W from the posterior:\nW \u223c p(W | l) \u221d p(I | W ) p(W ).\n(1)\nThis enables us to estimate W * = arg max P(W | I). Stochastic sampling is attractive because it is a general technique that can be applied to any inference problem. Moreover, it generate samples that can be used to validate the model assumptions. But the dimension of the sample space for image parsing is very high and so standard sampling techniques are computationally expensive.\nBy contrast, discriminative methods are very fast to compute. They do not specify models for how the image is generated. Instead they give discriminative (conditional) probabilities q(w j | Tst j (I)) for components {w j } of W based on a sequence of bottom-up tests Tst j (I) performed on the image. The tests are based on local image features {F j,n (I)} which can be computed from the image in a cascade manner (e.g. AdaBoost filters, see Section 6), Tst j (I) = (F j,1 (I), F j,2 (I), . . . , F j,n (I)), j = 1, 2, . . . , K .\n(2)\nThe following theorem shows that the KLdivergence between the true marginal posterior p(w j | I) and the optimal discriminant approximation q(w j | Tst(I)) using test Tst(I) will decrease monotonically as new tests are added. 4\nTheorem 1. The information gained for a variable w by a new test Tst + (I) is the decrease of Kullback-Leibler divergence between p(w | I) and its best discriminative estimate q(w | Tst(I)) or the increase of mutual information between w and the tests.\nE I [KL( p(w | I) q(w | Tst(I)))] \u2212E I [KL( p(w | I) q(w | Tst(I), Tst + (I)))] = MI(w Tst, T st + ) \u2212 M I (w Tst) = E Tst,Tst + [KL(q(w | Tst, T st + ) q(w | Tst))] \u2265 0,\nwhere E I is the expectation with respect to P(I), and E Tst,Tst + is the expectation with respect to the probability on the test responses (Tst, Tst + ) induced by P(I).\nThe decrease of the Kullback-Leibler divergence equals zero if and only if Tst(I) are sufficient statistics with respect to w.\nIn practice discriminative methods, particularly standard computer vision algorithms-see Section 4.1, will typically only use a small number of features for computational practicality. Also their discriminative probabilities q(w j | Tst(I)) will often not be optimal. Fortunately the image parsing algorithm in this paper only requires the discriminative probabilities q(w j | Tst(I)) to be rough approximations to p(w j | I).\nThe difference between discriminative and generative models is illustrated in Fig. 3. Discriminative models are fast to compute and can be run in parallel because different components are computed independently (see arrows in Fig. 3). But the components {w i } may not yield a consistent solution W and, moreover, W may not specify a consistent model for generating the observed image I. These inconsistencies are indicated by the crosses in Fig. 3. Generative models ensure consistency but require solving a difficult inference problem.\nIt is an open problem whether discriminative methods can be designed to infer the entire state W for the complicated generative models that we are dealing with. Recent work (Kumar and Hebert, 2003) is a step in this direction. But mathematicians (Blanchard and Geman, 2003) have argued that this will not be practical and that discriminative models will always require additional post-processing.", "publication_ref": ["b26", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Markov Chain Kernels and Sub-Kernels", "text": "Formally, our DDMCMC image parsing algorithm simulates a Markov chain MC =< , \u03bd, K > with kernel K in space and with probability v for the starting state. An element W \u2208 is a parsing graph. We let the set of parsing graphs be finite as images have finite pixels and grey levels.\nWe proceed by defining a set of moves for reconfiguring the graph. These include moves to: (i) create nodes, (ii) delete nodes, and (iii) change node attributes. We specify stochastic dynamics for these moves in terms of transition kernels. 5 For each move we define a Markov Chain sub-kernel by a transition matrix K a (W | W : I) with a \u2208 A being an index. This represents the probability that the system makes a transition from state W to state W when sub-kernel a is applied (i.e. (3)\nThe full transition kernel is expressed as:\nK(W | W : I) = a \u03c1(a : I)K a (W | W : I), a \u03c1(a : I) = 1, \u03c1(a : I) > 0. (4\n)\nTo implement this kernel, at each time step the algorithm selects the choice of move with probability \u03c1(a : I) for move a, and then uses kernel K a (W | W ; I) to select the transition from state W to state W . Note that both probabilities \u03c1(a : I) and K a (W | W ; I) depend on the input image I. This distinguishes our DDM-CMC methods from conventional MCMC computing (Liu, 2001;Bremaud, 1999).\nThe full kernel obeys detailed balance, Eq. (3), because all the sub-kernels do. It will also be ergodic, provided the set of moves is sufficient (i.e. so that we can transition between any two states W, W \u2208 using these moves). These two conditions ensure that p(W | I) is the invariant (target) probability of the Markov Chain (Bremaud, 1999) in the finite space .\nApplying the kernel K a(t) updates the Markov chain state probability \u00b5 t (W ) at step t to \u00b5 t+1 (W ) at t + 1 6 :\n\u00b5 t+1 (W ) = W K a(t) (W | W : I)\u00b5 t (W ). (5\n)\nIn summary, the DDMCMC image parser simulates a Markov chain MC with a unique invariant probability p(W | I). At time t, the Markov chain state (i.e. the parse graph) W follows a probability \u00b5 t which is the product of the sub-kernels selected up to time t,\nW \u223c \u00b5 t (W ) = \u03bd(W o ) \u2022 K a(1) \u2022 K a(2) \u2022 \u2022 \u2022 \u2022 \u2022 K a(t) (W o , W ) \u2192 p(W | I). (6\n)\nwhere a(t) indexes the sub-kernel selected at time t. As the time t increases, \u00b5 t (W ) approaches the posterior p(W | I) monotonically (Bremaud, 1999) at a geometric rate (Diaconis and Hanlon, 1992) independent of the starting configuration. The following convergence theorem is useful for image parsing because it helps quantify the effectiveness of the different sub-kernels.\nTheorem 2. The Kullback-Leibler divergence between the posterior p(W | I) and the Markov chain state probability decreases monotonically when a sub-kernel\nK a(t) , \u2200a(t) \u2208 A is applied, KL( p(W | I) \u00b5 t (W )) \u2212 KL( p(W | I) \u00b5 t+1 (W )) \u2265 0 (7)\nThe decrease of KL-divergence is strictly positive and is equal to zero only after the Markov chain becomes stationary, i.e. \u00b5 = p.\n[Proof] See Appendix A.\nThe theorem is related to the second law of thermodynamics (Cover and Thomas, 1991), and its proof makes use of the detailed balance Eq. (3). This KL divergence gives a measure of the \"power\" of each sub-kernel K a(t) and so it suggests an efficient mechanism for selecting the sub-kernels at each time step, see Section 8. By contrast, classic convergence analysis (see Appendix B) show that the convergence of the Markov Chain is exponentially fast, but does not give measures of power of sub-kernels.", "publication_ref": ["b28", "b6", "b6", "b6", "b14", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "DDMCMC and Proposal Probabilities", "text": "We now describe how to design the sub-kernels using proposal probabilities and discriminative models. This is at the heart of DDMCMC.\nEach sub-kernel 7 is designed to be of Metropolis-Hastings form (Metropolis et al., 1953;Hastings, 1970): The Metropolis-Hastings form ensures that the subkernels obey detailed balance (after pairing) (Bremaud, 1999).\nThe proposal probabilities Q a (W | W : Tst a (I)) will be built from discriminative probabilities using tests Tst a (I) performed on the image. The design of the proposal probabilities is a trade-off. Ideally the proposals would be sampled from the posterior p(W | I), but this is impractical. Instead the trade-off requires: (i) it is possible to make large moves in at each time step, (ii) the proposals should encourage moves to states with high posterior probability, and (iii) the proposals must be fast to compute.\nMore formally, we define the scope a (W ) = {W \u2208 : K a (W | W : I) > 0} to be the set of states which can be reached from W in one time step using sub-kernel a. We want the scope S a (W ) to be large so that we can make large moves in the space at each time step (i.e. jump towards the solution and not crawl). The scope should also, if possible, include states W with high posterior p(W | I) (i.e. it is not enough for the scope to be large, it should also be in the right part of ).\nThe proposals Q a (W | W : Tst a (I)) should be chosen so as to approximate\np(W | I) W \u2208 a (W ) p(W | I) if W \u2208 a (W ), = 0, otherwise. (10\n)\nThe proposals will be functions of the discriminative models for the components of W and of the generative models for the current state W (because it is computationally cheap to evaluate the generative models for the current state). The details of the model p(W | I) will determine the form of the proposals and how large we can make the scope while keeping the proposals easy to compute and able to approximate Eq. (10). See the detailed examples given in Section 5.\nThis description gives the bare bones of DDMCMC. We refer to  for further details of these issues from an MCMC perspective. In the discus-sion section, we describe strategies to improve DDM-CMC. Preliminary theoretical results for the convergence of DDMCMC are encouraging for a special case (see Appendix C).\nFinally, in Appendix D, we address the important practical issue of how to maintain detailed balance when there are multiple routes to transition between two state W and W . We describe two ways to do this and the trade-offs involved.", "publication_ref": ["b35", "b23", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Generative Models and Bayesian Formulation", "text": "This section describes the graph structure and the generative models used for our image parsing algorithm in this paper.\nFigure 1 illustrates the general structure of a parsing graph. In this paper, we use a two-layer-graph illustrated in Fig. 4. The top node (\"root\") of the graph represents the whole scene (with a label). It has K intermediate nodes for the visual patterns (face, text, texture, and shading). Each visual pattern has a number of pixels at the bottom (\"leaves\"). In this graph no horizontal connections are considered between the visual patterns except the constraint that they share boundaries and form a partition of the image lattice (see  for an example of image parsing where horizontal connections are used, but without object patterns).\nThe number K of intermediate nodes is a random variable, and each node i = 1, . . . , K has a set of attributes (L i , \u03b6 i , i ) defined as follows. L i is the shape descriptor and determines the region R i = R(L i ) of the image pixels covered by the visual pattern of the intermediate node. Conceptually, the pixels within R i are child nodes of the intermediate node i. (Regions may contain holes, in which case the shape descriptor will have internal and external boundaries). The remaining attribute variables (\u03b6 i , i ) specify the probability models\np(I R(L i ) | \u03b6 i , L i , i ) for generating the sub-image I R(L i ) in region R(L i ). The variables \u03b6 i \u2208 {1, . . . , 66}\nindicate the visual pattern type (3 types of generic visual patterns, 1 face pattern, and 62 text character patterns), and i denotes the model parameters for the corresponding visual pattern (details are given in the following subsections). The complete scene description can be summarized by:\nW = (K , {(\u03b6 i , L i , i ) : i = 1, 2, . . . , K }).\nThe shape descriptors {L i : i = 1, . . . , K } are required to be consistent so that each pixel in the image is a child of one, and only one, of the intermediate nodes.\nThe shape descriptors must provide a partition of the image lattice = {(m, n) : 1 \u2264 m \u2264 Height(I), 1 \u2264 n \u2264 Width(I)} and hence satisfy the condition\n= K i=1 R(L i ), R(L i ) \u2229 R(L j ) = \u2205, \u2200i = j.\nThe generation process from the scene description W to I is governed by the likelihood function:\np(I | W ) = K i=1 p I R(L i ) \u03b6 i L i , i . The prior probability p(W) is defined by p(W ) = p(K ) K i=1 p(L i ) p(\u03b6 i | L i ) p( i | \u03b6 i ).\nIn our Bayesian formulation, parsing the image corresponds to computing the W * that maximizes a posteriori probability over , the solution space of W,\nW * = arg max p W \u2208 (W | I) = arg max p W \u2208 (I | W ) p(W ). (11\n)\nIt remains to specify the prior p(W ) and the likelihood function p(I | W ). We set the prior terms p(K ) and p( i | \u03b6 i ) to be uniform probabilities. The term p(\u03b6 i | L i ) is used to penalize high model complexity and was estimated for the three generic visual patterns from training data in .", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Shape Models", "text": "We use two types of shape descriptor in this paper. The first is used to define shapes of generic visual patterns and faces. The second defines the shapes of text characters.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Shape Descriptors for Generic Visual Patterns and Faces", "text": "In this case, the shape descriptor represents the boundary 8 of the image region by a list of pixels L i = \u2202R i . The prior is defined by:\np(L i ) \u221d exp{\u2212\u03b3 | R(L i ) | \u03b1 \u2212 \u03bb|L i |}.(12)\nIn this paper, we set \u03b1 = 0.9. For computational reasons, we use this prior for face shapes though more complicated priors (Cootes et al., 2001) can be applied.", "publication_ref": ["b10"], "figure_ref": [], "table_ref": []}, {"heading": "Shape Descriptors for Text Characters", "text": "We model text characters by 62 deformable templates corresponding to the ten digits and the twenty six letters in both upper and lower cases. These deformable templates are defined by 62 prototype characters and a set of deformations. The prototypes are represented by an outer boundary and, at most, two inner boundaries. Each boundary is modeled by a B-spline using twenty five control points. The prototype characters are indexed by c i \u2208 {1, . . . , 62} and their control points are represented by a matrix TP(c i ).\nWe now define two types of deformations on the templates. One is a global affine transformation, and the other is a local elastic deformation. First we allow the letters to be deformed by an affine transform M i . We put a prior p(M i ) to penalize severe rotation and distortion. This is obtained by decomposing M i as: where \u03b8 is the rotation angle, \u03c3 x and \u03c3 y denote scaling, and h is for shearing. The prior on M i is\nM i = \u03c3 x 0 0 \u03c3 y cos \u03b8 \u2212 sin \u03b8 sin \u03b8 cos \u03b8 1 h 0 1 .\np(M i ) \u221d exp \u2212 a|\u03b8 | 2 \u2212 b \u03c3 x \u03c3 y + \u03c3 y \u03c3 x 2 \u2212 ch 2 ,\nwhere a, b, c are parameters. Next, we allow local deformations by adjusting the positions of the B-spline control points. For a digit/letter c i and affine transform M i , the contour points of the template are given by\nG TP (M i , c i ) = U \u00d7 M s \u00d7 M i \u00d7 TP(c i ).\nSimilarly the contour points on the shape with control points S i are given by G s (M i , C i ) = U \u00d7 M s \u00d7 S i (U and M s are the B-Spline matrices). We define a probability distribution p(S i | M i , C i ) for the elastic deformation given by\nS i , p(S i | M i , c i ) \u221d exp{\u2212\u03b3 | R(L i ) | \u03b1 \u2212D(G S (M i , c i ) G TP (M i , c i ))}, where D(G S (M i , c i ) G TP (M i , c i ))\nis the overall distance between contour template and the deformed contour (these deformations are small so the correspondence between points on the curves can be obtained by nearest neighbor matches, see Tu and Yuille (2004) for how we can refine this). Figure 5 shows some samples drawn from the above model.\nIn summary, each deformable template is indexed by c i \u2208 {1 . . . 62} and has a shape descriptor:\nL i = (c i , M i , S i ),\nThe prior distribution on L i is specified by:\np(L i ) = p(c i ) p(M i ) p(S i | M i , c i ).\nHere p(c i ) is a uniform distribution on all the digits and letters (we do not place a prior distribution on text strings, though it is possible to do so (Klein and Manning, 2002)).", "publication_ref": ["b48", "b24"], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Generative Intensity Models", "text": "We use four families of generative intensity models for describing intensity patterns of (approximately) constant intensity, clutter/texture, shading, and face. The first three are similar to those defined in .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Constant Intensity Model \u03b6 = 1.", "text": "This assumes that pixel intensities in a region R are subject to independently and identically distributed (iid) Gaussian distribution,\np 1 I R(L) \u03b6 = 1, L , = v\u2208R(L) G(I v \u2212 \u00b5; \u03c3 2 ), = (\u00b5, \u03c3 ) 2. Clutter/Texture Model \u03b6 = 2. This is a non- parametric intensity histogram h() discretized to take G values (i.e. is expressed as a vector (h 1 , h 2 , . . . , h G )).\nLet n j be the number of pixels in R(L) with intensity value j.  \np 2 I R(L) \u03b6 = 2, L , = v\u2208R(L) h(I v ) = G j=1 h nj j , = (h 1 , h 2 , . . . , h G ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Shading", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "4.", "text": "The PCA face model \u03b6 = 4. The generative model for faces is simpler and uses Principal Component Analysis (PCA) to obtain representations of the faces in terms of principal components {B i } and covariances . Lower level features, also modeled by PCA, can be added (Moghaddam and Pentland, 1997). Figure 6 shows some faces sampled from the PCA model. We also add other features such as the occlusion process, as described in Hallinan et al. (1999).\np 4 (I R (L) \u03b6 = 4, L , ) = G I R(L) \u2212 i \u03bb i B i ; , = (\u03bb 1 , . . . , \u03bb n , ).", "publication_ref": ["b36", "b21"], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "Overview of the Algorithm", "text": "This section gives the control structure of an image parsing algorithm based on the strategy described in Section 2, and Fig. 8 shows the diagram. Our algorithm must construct the parse graph on the fly and to estimate the scene interpretation W . Figure 7 illustrates how the algorithm selects the Markov chain moves (dynamics or sub-kernels) to search through the space of possible parse graphs of the image by altering the graph structure (by deleting or adding nodes) and by changing the node attributes. An equivalent way of visualizing the algorithm is in terms of a search through the solution space see Zhu 2002a, 2002b) for more details of this viewpoint.\nWe first define the set of moves to reconfigure the graph. These are: (i) birth or death of face nodes, (ii) birth or death of text characters, (iii) splitting or merging of regions, (iv) switching node attributes (region type \u03b6 i and model parameters i ), (v) boundary evolution (altering the shape descriptors L i of nodes with adjacent regions). These moves are implemented by sub-kernels. The first four moves are reversible jumps (Green, 1995), and will be implemented by the Metropolis-Hastings Eq. (8). The fifth move, boundary evolution, is implemented by a stochastic partial differential equation.\nThe sub-kernels for these moves require proposal probabilities driven by elementary discriminative methods, which we review in the next subsection. The proposal probabilities are designed using the criteria in Section 2.4, and full details are given in Section 5.\nThe control structure of the algorithm is described in Section 4.2. The full transition kernel for the image parser is built by combining the sub-kernels, as described in Section 2.3 and Fig. 8. The algorithm proceeds (stochastically) by selecting a sub-kernel, se-  and which is composed of reversible sub-kernels K a for making different types of moves in the parse graph (e.g. giving birth to new nodes or merging nodes). At each time step the algorith-m selects a sub-kernel stochastically. The selected sub-kernel proposes a specific move (e.g. to create or delete specific nodes) and this move is then evaluated and accepted stochastically, see Eq. (8). The proposals are based on both bottom-up (discriminative) and top-down (generative) processes, see Section 2.4. The bottom-up processes compute discriminative probabilities q(w j Tst j (I)), j = 1, 2, 3, 4 from the input image I based on feature tests Tst j (I). An additional sub-kernel for boundary evolution uses a stochastic partial differential equation will be described later.\nlecting where in the graph to apply it, and then deciding whether or not to accept the operation.", "publication_ref": ["b20"], "figure_ref": ["fig_8", "fig_7", "fig_8"], "table_ref": []}, {"heading": "The Discriminative Methods", "text": "The discriminative methods give approximate posterior probabilities q(w j Tst j (I)) for the elementary components w j of W . For computational efficiency, these probabilities are based only on a small number of simple tests Tst j (I).\nWe briefly overview and classify the discriminative methods used in our implementation. Section 5 shows how these discriminative methods are composed, see crosses in Fig. 8, to give proposals for making moves in the parsing graph.\n1. Edge Cues. These cues are based on edge detectors (Canny, 1986;Bowyer et al., 2001;Konishi et al., 2003). They are used to give proposals for region boundaries (i.e. the shape descriptor attributes of the nodes). Specifically, we run the Canny detector at three scales followed by edge linking to give partitions of the image lattice. This gives a finite list of candidate partitions which are assigned weights, see Section 5.2.3 and (Tu and Zhu, 2002a). The discriminative probability is represented by this weighted list of particles. In principle, statistical edge detectors (Konishi et al., 2003) would be preferable to Canny because they give discriminative probabilities q(w j | Tst j (I)) learnt from training data.", "publication_ref": ["b8", "b7", "b25", "b25"], "figure_ref": ["fig_8"], "table_ref": []}, {"heading": "Binarization", "text": "Cues. These cues are computed using a variant of Niblack's algorithm (Niblack, 1986). They are used to propose boundaries for text characters (i.e. shape descriptors for text nodes), and will be used in conjunction with proposals for text detection. The binarization algorithm, and an example of its output, are given in Section 6. Like edge cues, the algorithm is run at different parameters settings and represents the discriminative probability by a weighted list of particles indicating candidate boundary locations.", "publication_ref": ["b39"], "figure_ref": [], "table_ref": []}, {"heading": "Face Region", "text": "Cues. These cues are learnt by a variant of AdaBoost (Schapire 2002;Viola and Jones, 2001) which outputs discriminative probabilities (Friedman et al. 1998), see Section 6. They propose the presence effaces in sub-regions of the image. These cues are combined with edge detection to propose the localization of faces in an image.", "publication_ref": ["b42", "b52", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Text Region", "text": "Cues. These cues are also learnt by a probabilistic version of AdaBoost, see Section 6. The algorithm is applied to image windows (at a range of scales). It outputs a discriminative probability for the presence of text in each window. Text region cues are combined with binarization to propose boundaries for text characters.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Shape Affinity", "text": "Cues. These act on shape boundaries, produced by binarization, to propose text characters. They use shape context cues (Belongie et al., 2004). and information features (Tu and Yuille, 2004) to propose matches between the shape boundaries and the deformable template models of text characters.", "publication_ref": ["b48"], "figure_ref": [], "table_ref": []}, {"heading": "Region Affinity", "text": "Cues. These are used to estimate whether two regions R i , R j are likely to have been generated by the same visual pattern family and model parameters. They use an affinity similarity measure (Shi and Malik, 2000) of the intensity properties I R i , I R j .", "publication_ref": ["b43"], "figure_ref": [], "table_ref": []}, {"heading": "Model Parameter and Visual Pattern Family Cues.", "text": "These are used to propose model parameters and visual pattern family identity. They are based on clustering algorithms, such as mean-shift (Comaniciu and Meer, 1999). The clustering algorithms depend on the model types and are described in Tu and Zhu (2002a).\nIn our current implementation, we conduct all the bottom-up tests Tst j (I), j = 1, 2, . . . , K at an early stage for all the discriminative models q j (w j Tst j (I)), and they are then combined to form composite tests Tst a (I) for each subkernel K a in Eqs. (8 and 9). It may be more efficient to perform these test as required, see discussion in Section 8.", "publication_ref": ["b11"], "figure_ref": [], "table_ref": []}, {"heading": "Control Structure of the Algorithm", "text": "The control strategy used by our image parser is illustrated in Fig. 8. The image parser explores the space of parsing graphs by a Markov Chain Monte Carlo sampling algorithm. This algorithm uses a transition kernel K which is composed of sub-kernels K a corresponding to different ways to reconfigure the parsing graph. These sub-kernels come in reversible pairs 9 (e.g., birth and death) and are designed so that the target probability distribution of the kernel is the generative posterior p(W | I). At each time step, a sub-kernel is selected stochastically. The sub-kernels use the Metropolis-Hasting sampling algorithm, see Eq. (8), which proceeds in two stages. First, it proposes a reconfiguration of the graph by sampling from a proposal probability. Then it accepts or rejects this reconfiguration by sampling the acceptance probability.\nTo summarize, we outline the control strategy of the algorithm below. At each time step, it specifies (stochastically) which move to select (i.e. which subkernel), where to apply it in the graph, and whether to accept the move. The probability to select moves \u03c1(a : I) was first set to be independent of I, but we got better performance by adapting it using discriminative cues to estimate the number of faces and text characters in the image (see details below). The choice of where to apply the move is specified (stochastically) by the sub-kernel. For some sub-kernels it is selected randomly and for others is chosen based on a fitness factor (see details in Section 5), which measures how well the current model fits the image data. Some annealing is required to start the algorithm because of the limited scope of the moves in the current implementation (the need for annealing will be reduced if the compositional techniques described in Barbu and Zhu (2003)) are used).\nWe improved the effectivenss of the algorithm by making the move selection adapt to the image (i.e. by making \u03c1(a : I) depend on I). In particular, we increased the probability of giving birth and death of faces and text, \u03c1(1) and \u03c1(2), if the bottom-up (AdaBoost) proposals suggested that there are many objects in the scene. For example, let N(I) be the number of proposals for faces or text above a threshold T a . Then we modify the probabilities in the table by \u03c1(a 1 ) \u2192 {(\u03c1(a 1 ) + kg(N (I))}/Z , \u03c1(a 2 ) \u2192 {\u03c1(a 2 ) + kg(N )}/Z , \u03c1(a 3 ) \u2192 \u03c1(a 3 )/Z , \u03c1(a 4 ) \u2192 \u03c1(a 4 )/Z , where g(x) = x, x \u2264 T b g(x) = T b , x \u2265 T b and Z = 1 + 2k is chosen to normalize the probability.", "publication_ref": ["b0"], "figure_ref": ["fig_8"], "table_ref": []}, {"heading": "The basic control strategy of the image parsing algorithm", "text": "1. Initialize W (e.g. by dividing the image into four regions), setting their shape descriptors, and assigning the remaining node attributes at random. 2. Set the temperature to be T init 3. Select the type a of move by sampling from a probability \u03c1(a), with \u03c1(1) = 0.2 for faces, \u03c1(2) = 0.2 for text, \u03c1(3) = 0.4 for splitting and merging, \u03c1(4) = 0.15 for switching region model (type or model parameters), and \u03c1(5) = 0.05 for boundary evolution. This was modified slightly adaptively, see caption and text. 4. If the selected move is boundary evolution, then select adjacent regions (nodes) at random and apply stochastic steepest descent, see Section 5.1. 5. If the jump moves are selected, then a new solution W is randomly sampled as follows:\n-For the birth or death of a face, see Section 5. is applied to accept or reject the proposed move.\n6. Reduce the temperature T = 1 + T init \u00d7 exp(\u2212t \u00d7 c R ), where t is the current iteration step, c is a constant and R is the size of the image. 7. Repeat the above steps and until the convergence criterion is satisfied (by reaching the maximum number of allowed steps or by lack of decrease of the negative log posterior).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Markov Chain Kernels", "text": "This section gives a detailed discussion of the individual Markov Chain kernel, their proposal probabilities, and their fitness factors.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Boundary Evolution", "text": "These moves evolve the positions of the region boundaries but preserve the graph structure. They are implemented by a stochastic partial differential equation (Langevin equation) driven by Brownian noise and can be derived from a Markov Chain (Gemam and Huang, 1986). The deterministic component of the PDE is obtained by performing steepest descent on the negative log-posterior, as derived in Zhu and Yuille (1996).\nWe illustrate the approach by deriving the deterministic component of the PDE for the evolution of the boundary between a letter T j and a generic visual pattern region R i . The boundary will be expressed in terms of the control points {S m } of the shape descriptor of the letter. Let v denote a point on the boundary, i.e. v(s) = (x(s), y(s)) on (s) = \u2202 R i \u2229 \u2202 R j . The deterministic part of the evolution equation is obtained by taking the derivative of the negative log-posterior \u2212log p(W/I) with respect to the control points.\nMore precisely, the relevant parts of the negative logposterior, see Eq. ( 3) and ( 11) are given by E(R i ) and E(T j ) where:\nE(R i ) = R i {\u2212 log p(I(x, y)|\u03b8 \u03b6 i )} dxdy + \u03b3 |R i | \u03b1 + \u03bb|\u2202 R i |. and E(T j ) = L j \u2212 log p(I(x, y)|\u03b8 \u03b6 j ) dxdy + \u03b3 |R(L j )| \u03b1 \u2212 log p(L j ).\nDifferentiating E(R i ) + E(T j ) with respect to the control points {S m } yields the evolution PDE:\nd S m dt = \u2212 \u03b4 E(R i ) \u03b4S m \u2212 \u03b4 D(T j ) \u03b4S m = \u2212 \u03b4 E(R i ) \u03b4v \u2212 \u03b4 E(T j ) \u03b4v 1 |J(S)| ds = n(v) log p I(v); \u03b8 \u03b6 i p I(v); \u03b8 \u03b6 j + \u03b1\u03b3 1 |D j | 1\u2212\u03b1 \u2212 1 |D i | 1\u2212\u03b1 \u2212\u03bb\u03ba + D G S j (s) G T (s) 1 |J(s)| ds,\nwhere J(s) is the Jacobian matrix for the spline function. (Recall that \u03b1 = 0.9 in the implementation). The log-likelihood ratio term log\np(I(v);\u03b8 \u03b6 i )\np(I(v);\u03b8 \u03b6 j ) implements the competition between the letter and the generic region models for ownership of the boundary pixels.", "publication_ref": ["b57"], "figure_ref": [], "table_ref": []}, {"heading": "Markov Chain Sub-Kernels", "text": "Changes in the graph structure are realized by Markov chain jumps implemented by four different sub-kernels.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sub-Kernel I: Birth and Death of Text.", "text": "This pair of jumps is used to create or delete text characters. We start with a parse graph W and transition into parse graph W by creating a character. Conversely, we transition from W back to W by deleting a character.\nThe proposals for creating and deleting text characters are designed to approximate the terms in Eq. (10). We obtain a list of candidate text character shapes by using AdaBoost to detect text regions followed by binarization to detect candidate text character boundaries within text regions (see Section 6.2). This list is represented by a set of particles which are weighted by the similarity to the deformable templates for text characters (see below):\nS 1r (W ) = z (\u00b5) 1r , \u03c9 (\u00b5) 1r \u00b5 = 1, 2, . . . , N 1r .\nSimilarly, we specify another set of weighted particles for removing text characters:\nS 1l (W ) = z (\u03bd) 1l , \u03c9 (\u03bd) 1l \u03bd = 1, 2, . . . , N 1l . {z (\u00b5)\n1r } and {z (\u03bd) 1l } represent the possible (discretized) shape positions and text character deformable templates for creating or removing text, and {\u03c9 (\u00b5) 1r } and {\u03c9 (\u03bd) 1l } are their corresponding weights. The particles are then used to compute proposal probabilities\nQ 1r (W |W : I) = \u03c9 1r (W ) N 1r \u00b5=1 \u03c9 (\u00b5) 1r , Q 1l (W |W , I) = \u03c9 1l (W ) N 1l \u03bd=1 \u03c9 (\u03bd) 1l .\nThe weights \u03c9 (\u00b5) 1r and \u03c9 (\u03bd) 1l for creating new text characters are specified by shape affinity measures, such as shape contexts (Belongie et al., 2002) and informative features (Tu and Yuille, 2004). For deleting text characters we calculate \u03c9 (\u03bd) 1l directly from the likelihood and prior on the text character. Ideally these weights will approximate the ratios p(W |I) p(W |I) and p(W |I) p(W |I) .", "publication_ref": ["b3", "b48"], "figure_ref": [], "table_ref": []}, {"heading": "Sub-Kernel II: Birth and Death of Face.", "text": "The sub-kernel for the birth and death of faces is very similar to the sub-kernel of birth and death of text. We use AdaBoost method discussed in Section 6.2 to detect candidate faces. Face boundaries are obtained directly from using edge detection to give candidate face boundaries. The proposal probabilities are computed similarly to those for sub-kernel I.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sub-Kernel III: Splitting and Merging Regions", "text": "This pair of jumps is used to create or delete nodes by splitting and merging regions (nodes). We start with a parse graph W and transition into parse graph W by splitting node i into nodes j and k. Conversely, we transition back to W by merging nodes j and k into node i. The selection of which region i to split is based on a robust function on p(I R i |\u03b6 i , L i , i ) (i.e. the worse the model for region R i fits the data, the more likely we are to split it). For merging, we use a region affinity measure (Shi and Malik, 2000) and propose merges between regions which have high affinity.\nFormally, we define W, W :\nW = (K , (\u03b6 k , L k , k ), W \u2212 ) W = (K + 1, (\u03b6 i , L i , i ), (\u03b6 j , L j , j ), W \u2212 )\nwhere W \u2212 denotes the attributes of the remaining K \u22121 nodes in the graph. We obtain proposals by seeking approximations to Eq. 10 as follows.\nWe first obtain three edge maps. These are given by Canny edge detectors (Canny, 1986) at different scales (see Tu and Zhu (2002a), for details). We use these edge maps to create a list of particles for splitting S 3r (W ). A list of particles for merging is denoted by S 3l (W ).\nS 3r (W ) = z (\u00b5) 3r , \u03c9 (\u00b5) 3r : \u00b5 = 1, 2, . . . , N 3r . , S 3l (W ) = z (\u03bd)\n3l , \u03c9 (\u03bd) 3l\n:\n\u03bd = 1, 2, . . . , N 3l .\nwhere {z\n(\u00b5)\n3r } and {z (\u03bd) 3l } represent the possible (discretized) positions for splitting and merging, and their weights {\u03c9 3r }, {\u03c9 3l } will be defined shortly. In other words, we can only split a region i into regions j and k along a contour z \u00b5 3r (i.e. z \u00b5 3r forms the new boundary). Similarly we can only merge regions j and k into region i by deleting a boundary contour z \u00b5 3l . For example, Figure 11 shows 7 candidate sites for splitting W and 5 sites for merging W .\nWe now define the weights {\u03c9 3r }, {\u03c9 3l }. These weights will be used to determine probabilities for the splits and merges by:\nQ 3r (W |W : I) = \u03c9 3r (W ) N 3r \u00b5=1 \u03c9 (\u00b5) 3r , Q 3l (W |W : I) = \u03c9 3l (W ) N 3l\n\u03bd=1 \u03c9 (\u03bd) 3l\n.\nAgain, we would like \u03c9 \np(W | I) p(W | I) = p I R i | \u03b6 i , L i , i p I R j | \u03b6 j , L j , j p(I R k | \u03b6 k , L k , k ) \u2022 p(\u03b6 i , L i , i ) p(\u03b6 j , L j , j ) p(\u03b6 k , L k , k ) \u2022 p(K + 1) p(k)\nThis is expensive to compute, so we approximate Where q(R i , R j ) is an affinity measure (Shi and Malik, 2000) of the similarity of the two regions R i and R j (it is a weighted sum of the intensity difference |\u012a i \u2212\u012a j |, and the chi-squared difference between the intensity histograms), q(L i ) is given by the priors on the shape descriptors, and q(\u03b6 i , i ) is obtained by clustering in parameter space (see ).\np(W | I) p(W | I) and p(W | I) p(W | I) by: \u03c9 (\u00b5) 3r = q(R i , R j ) p I R k | \u03b6 k , L k , k \u2022 [q(L i )q(\u03b6 i , i )][q(L j )q(\u03b6 j , j )] p(\u03b6 k , L k , k ) . (13\n)\n\u03c9 (\u03bd) 3l = q(R i , R j ) p I R i | \u03b6 i , L i , i p I R j | \u03b6 j , L j , j \u2022 q(L k )q(\u03b6 k , k ) p(\u03b6 i , L i , i ) p(\u03b6 j , L j , j ) , (14\n)\nFigure", "publication_ref": ["b43", "b8", "b43"], "figure_ref": [], "table_ref": []}, {"heading": "Jump II:", "text": "Switching Node Attributes. These moves switch the attributes of a node i. This involves changing the region type \u03b6 i and the model parameters\ni .\nThe move transitions between two states:\nW = ((\u03b6 i , L i , i ), W \u2212 ) W = ((\u03b6 i , L i , i ), W \u2212 )\nThe proposal, see Eq. (10), should approximate:\np(W | I) p(W | I) = p I R i \u03b6 i , L i , i p(\u03b6 i , L i , i ) p I R i \u03b6 i , L i , i p(\u03b6 i , L i , i ) .\nWe approximate this by a weight \u03c9 (\u00b5) 4 given by\n\u03c9 (\u00b5) 4 = q(L i )q(\u03b6 i , i ) p I R i | \u03b6 i , L i , i p(\u03b6 i , L i , i ) ,\nwhere q(L i )q(\u03b6 i , i ) are the same functions used in the split and merge moves. The proposal probability is the weight normalized in the candidate set,\nQ 4 (W | W : I) = \u03c9 4 (W ) N 4 \u00b5=1 \u03c9 (\u00b5) 4\n.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "AdaBoost for Discriminative Probabilities for Face and Text", "text": "This section describes how we use AdaBoost techniques to compute discriminative probabilities for detecting faces and text (strings of letters). We also describe the binarization algorithm used to detect the boundaries of text characters.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Computing Discriminative Probabilities by Adaboost", "text": "The standard AdaBoost algorithm, for example for distinguishing faces from non-faces (Viola and Jones, 2001), learns a binary-valued strong classifier-H Ada by combining a set of n binary-valued \"weak classifiers\" or feature tests Tst Ada (I) = (h 1 (I), . . . , h n (I)) using a set of weights \u03b1 Ada = (\u03b1 1 , . . . , \u03b1 n ) (Freund and Schapire, 1996). The features are selected from a pre-designed dictionary Ada . The selection of features and the tuning of weights are posed as a supervised learning problem. Given a set of labeled examples, {(I i , i ) : i = 1, 2, . . . , M} ( i = \u00b11), AdaBoost learning can be formulated as greedily optimizing the following function (Schapire, 2002).\n(\u03b1 * Ada , Tst * Ada ) = arg min Tst Ada \u2282 Ada arg min \u03b1 Ada M i=1 exp \u2212 i <\u03b1 Ada ,Tst Ada (I i )>. (16\n)\nTo obtain discriminative probabilities we use a theorem (Friedman et al., 1998) which states that the features and test learnt by AdaBoost give (asymptotically) posterior probabilities for the object labels (e.g. face or non-face). The AdaBoost strong classifier can be rederived as the log posterior ratio test.\nTheorem 3 (Friedman et al., 1998). With sufficient training samples M and features n, AdaBoost learning selects the weights \u03b1 * Ada and tests Tst * Ada to satisfy q( = +1 | I) = e <\u03b1 Ada ,Tst Ada (I i )> e <\u03b1 Ada ,Tst Ada (I i )> + e \u2212<\u03b1 Ada ,Tst Ada (I i )> .\nMoreover, the strong classifier converges asymptotically to the posterior probability ratio test H Ada (Tst Ada (I)) = sign(< \u03b1 Ada , Tst Ada (I) >)\n= sign q( = +1 | I) q( = \u22121 | I) .\nIn practice, the AdaBoost classifier is applied to windows in the image at different scales. Each window is evaluated as being face or non-face (or text versus nontext). For most images the posterior probabilities for faces or text are negligible for almost all parts of an image. So we use a cascade of tests (Viola and Jones, 2001;Wu et al., 2004) which enables us to rapidly reject many windows by setting their marginal probabilities to be zero.\nOf course, AdaBoost will only converge to approximations to the true posterior probabilities p( | I) because only a limited number of tests can be used (and there is only a limited amount of training data).\nNote that AdaBoost is only one way to learn a posterior probability, see Theorem 1. It has been found to be very effective for object patterns which have relatively rigid structures, such as faces and text (the shapes of letters are variable but the patterns of a sequence are fairly structured (Chen and Yuille, 2004).", "publication_ref": ["b52", "b17", "b42", "b18", "b18", "b52", "b54", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "AdaBoost Training", "text": "We used standard AdaBoost training methods (Freund and Schapire, 1996;Friedman et al., 1998) combined with the cascade approach using asymmetric weighting (Viola and Jones, 2001;Wu et al., 2004). The cascade enables the algorithm to rule out most of the image as face, or text, locations with a few tests and allows computational resources to be concentrated on the more challenging parts of the images.\nThe AdaBoost for text was designed to detect text segments. Our test data was extracted by hand from 162 images of San Francisco, see Fig. 12, and contained 561 text images, some of which can be seen  in Fig. 13. More than half of the images were taken by blind volunteers (which reduces bias). We divided each text image into several overlapping text segments with fixed width-to-height ration 2:1 (typically containing between two and three letters). A total of 7,000 text segments were used as the positive training set.\nThe negative examples were obtained by a bootstrap process similar to Drucker et al. (1993) . First we selected negative examples by randomly sampling from windows in the image dataset. After training with these samples, we applied the AdaBoost algorithm at a range of scales to classify all windows in the training images. Those misclassified as text were then used as negative examples for the next stage of AdaBoost. The image regions most easily confused with text were vegetation, and repetitive structures such as railings or building facades. The features used for AdaBoost were image tests corresponding to the statistics of elementary filters. The features were chosen to detect properties of text segments that were relatively invariant to the shapes of the individual letters or digits. They included averaging the intensity within image windows, and statistics of the number of edges. We refer to (Chen and Yuille, 2004) for more details.\nFigure 14(a) shows some failure examples of Adaboost for text detection. These correspond to situations such as heavy shading, blurred images, isolated digits, vertical commercial signs, and non-standard fonts. They were not included in the training examples shown in Fig. 13. The AdaBoost posteriors for faces was trained in a similar way. This time we used Haar basis functions (Viola and Jones, 2001) as elementary features. We used the FERET (Phillips et al., 1998)  In both cases, we evaluated the log posterior ratio test on testing datasets using a number of different thresholds (see (Viola and Jones, 2001)). In agreement with previous work on faces (Viola and Jones, 2001), Ad-aBoost gave very high performance with very few false positives and false negatives, see Table 1. But these low error rates are slightly misleading because of the enormous number of windows in each image, see Table 1. A small false positive rate may imply a large number of false positives for any regular image. By varying the threshold, we can either eliminate the false positives or the false negatives but not both at the same time. We illustrate this by showing the face regions and text regions proposed by AdaBoost in Fig. 15. If we attempt classification by putting a threshold then we can only correctly detect all the faces and the text at the expense of false positives. When Adaboost is integrated with the generic region models in the image parser, the generic region proposals can remove false positives and find text that Ad-aBoost misses. For example, the '9' in the right panel of Fig. 15 is not detected because our AdaBoost algorithm was trained on text segments. Instead it is detected as a generic shading region and later recognized as a letter '9', see Fig. 17. Some false positive text and faces in Fig. 15 are removed in Figs. 17 and 19.\nThe AdaBoost algorithm for text needs to be supplemented with a binarization algorithm, described below, to determine text character location. This is followed by appling shape contexts (Belongie et al., 2002) and informative features (Tu and Yuille, 2004) to the binarization results to make proposals for the presence of specific letters and digits.\nIn many cases, see Fig. 16, the results of binarization are so good that the letters and digits can be detected immeadiately (i.e. the proposals made by the binarization  stage are automatically accepted). But this will not always be the case. We note that binarization gives far better results than alternatives such as edge detection (Canny, 1986).\nThe binarization algorithm is a variant of one proposed by Niblack (1986). We binarize the image intensity using an adaptive thresholding based on a adaptive window size. Adaptive methods are needed because image windows containing text often have shading, shadow, and occlusion. Our binarization method determines the threshold T b (v) for each pixel v by the intensity distribution of its local window r (v) (centered on \u03bd).\nT b (v) = \u00b5 I r (v) + k \u2022 std(I r (v) ,\nwhere \u00b5(I r (v) ) and std(I r (\u03bd) ) are the intensity mean and standard deviation within the local window. The size of the local window is selected to be the smallest possibble window whose intensity variance is above a fixed threshold. The parameter k = \u00b10.2, where the \u00b1 allows for cases where the foreground is brighter or darker than the background.", "publication_ref": ["b17", "b18", "b52", "b54", "b15", "b9", "b52", "b40", "b52", "b52", "b3", "b48", "b8", "b39"], "figure_ref": ["fig_0", "fig_13", "fig_3", "fig_13", "fig_4", "fig_4", "fig_7", "fig_4", "fig_7", "fig_6"], "table_ref": ["tab_2", "tab_2"]}, {"heading": "Experiments", "text": "The image parsing algorithm is applied to a number of outdoor/indoor images. The speed in PCs (Pentium IV) is comparable to segmentation methods such as normalized cuts (Malik et al., 2001) or the DDMCMC algorithm in Tu and Zhu (2002a). It typically runs around 10-20 min. The main portion of the computing time is spent in segmenting the generic patterns and by boundary diffusion (Zhu and Yuille, 1996). Figures 17,18,and 19 show some challenging examples which have heavy clutter and shading effects. We present the results in two parts. One shows the segmentation boundaries for generic regions and objects, and the other shows the text and faces detected with  In the experiments, we observed that the face and text models improved the image segmentation results by comparison to our previous work (Tu and Zhu, 2002a) which only used generic region models. Conversely, the generic region models improve object detection by removing some false alarms and recovering objects which were not initially detected. We now discuss specific examples.\nIn Figure 15, we showed two images where the text and faces were detected purely bottom-up using Ad-aBoost. It is was impossible to select a threshold so that our AdaBoost algorithm had no false positives or false negatives. To ensure no false negatives, apart from the '9', we had to lower the threshold and admit false positives due to vegetation and heavy shadows (e.g. the shadow in the sign \"HEIGHTS OPTICAL\").\nThe letter '9' was not detected at any threshold. This is because our AdaBoost algorithm was trained to detect text segments, and so did not respond to a single digit.\nBy comparison, Fig. 17 shows the image parsing results for these two images. We see that the false alarms proposed by AdaBoost are removed because they are better explained by the generic region models. The generic shading models help object detection by explaining away the heavy shading on the text \"HEIGHTS OPTICAL\" and the dark glasses on the women, see Fig. 18. Moreover, the missing digit '9' is now correctly detected. The algorithm first detected it as a generic shading region and then reclassified as a digit using the sub-kernel that switches node attributes.\nThe ability to synthesize the image from the parsing graph W* is an advantage of the Bayesian approach. The synthesis helps illustrate the successes, and sometimes the weaknesses, of the generative models. Moreover, the synthesized images show how much information about the image has been captured by the models. In Table 2, we give the number of variables used in our representation W * and show that they are roughly proportional to the jpeg bytes. Most of the variables in W * are used to represent points on the segmentation boundary, and at present they are counted independently. We could reduce the coding length of W * substantially by encoding the boundary points effectively, for example, using spatial proximity. Image encoding is not the goal of our current work, however, and more sophisticated generative models would be needed to synthesize very realistic images.  ", "publication_ref": ["b31", "b57"], "figure_ref": ["fig_4", "fig_7", "fig_8"], "table_ref": ["tab_3"]}, {"heading": "Discussion", "text": "In this section, we describe two challenging technical problems for image parsing. Our current work addresses these issues.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Two mechanisms for constructing the parsing graph", "text": "In the introduction to this paper we stated that the parsing graph can be constructed in compositional and decompositional modes. The compositional mode proceeds by grouping small elements while the decompositional approach involves detecting an object as a whole and then locating its parts, see Fig. 20.\nThe compositional mode appears most effective for Fig. 20(a). Detecting the cheetah by bottom-up tests, such as those learnt by AdaBoost, seems difficult owing to the large variability of shape and photometric properties of cheetahs. By contrast, it is quite practical using Swendsen-Wang Cuts (Barbu and Zhu, 2004) to segment the image and obtain the boundary of the cheetah using a bottom-up compositional approach and a parsing tree with multiple levels. The parsing graph is constructed starting with the pixels as leaves (there are 46,256 pixels in Fig. 20(a)). The next level of the graph is obtained using local image texture similarities to construct graph nodes (113 of them) corresponding to \"atomic regions\" of the image. Then the algorithm contructs nodes (4 of them) for \"texture regions\" at the next level by grouping the atomic regions (i.e. each atomic region node will be the child of a texture region node). At each level, we compute a discriminative (pro-posal) probability for how likely adjacent nodes (e.g. pixels or atomic regions) belong to the same object or pattern. We then apply a transition kernel implementing split and merge dynamics (using the proposals). We refer to Barbu and Zhu (2004) for more detailed discussion.\nFor objects with little variability, such as the faces shown in Fig. 20(b), we can use bottom-up proposals (e.g. AdaBoost) to activate a node that represents the entire face. The parsing graph can then be constructed downwards (i.e. in the decompositional mode) by expanding the face node to create child nodes for the parts of the face. These child nodes could, in turn, be expanded to grandchild nodes representing finer scale parts. The amount of node expansion can be made adaptive to depend on the resolution of the image. For example, the largest face in Fig. 20(b) is expanded into child nodes but there is not sufficient resolution to expand the face nodes corresponding to the three smaller faces.\nThe major technical problem is to develop a mathematical criterion for which mode is most effective for which types of objects and patterns. This will enable the algorithm to adapt its search strategy accordingly.", "publication_ref": ["b1", "b1"], "figure_ref": ["fig_0", "fig_0", "fig_0", "fig_0", "fig_0"], "table_ref": []}, {"heading": "Optimal ordering strategy for tests and kernels", "text": "The control strategy of our current image parsing algorithm does not select the tests and sub-kernels in an optimal way. At each time step, the choice of subkernel is independent of the current state W (though the choice of where in the graph to apply the sub-kernel will depend on W).\nMoreover, bottom-up tests are performed which are never used by the algorithm.\nIt would be more efficient to have a control strategy which selects the sub-kernels and tests adaptively, provided the selection process requires low computational cost. We seek to find an optimal control strategy for selection which is effective for a large set of images and visual patterns. The selection criteria should select those tests and sub-kernels which maximize the gain in information.\nWe propose the two information criteria that we described in Section 2.\nThe first is stated in Theorem 1. It measures the information gained for variable w in the parsing graph by performing a new test Tst + . The information gain is \u03b4(w Tst The second is stated in Theorem 2. It measures the power of a sub-kernel K a by the decrease of the KLdivergence \u03b4(K a ) = KL( p \u00b5 t ) \u2212 KL( p \u00b5 t K a ). The amount of decrease \u03b4 a gives a measure of the power of the sub-kernel K a when informed by Tst t (I).\nWe need also take into account the computational cost of the selection procedures. See (Blanchard and Geman, 2003) for a case study for how to optimally select tests taking into account their computational costs.", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}, {"heading": "Summary and Future Work", "text": "This paper introduces a computational framework for parsing images into basic visual patterns. We formulated the problem using Bayesian probability theory and designed a stochastic DDMCMC algorithm to perform inference. Our framework gives a rigourous way to combine segmentation with object detection and recognition. We give proof of concept by implementing a model whose visual patterns include generic regions (texture and shading) and objects (text and faces). Our approach enables these different visual patterns to compete and cooperate to explain the input images.\nThis paper also provides a way to integrate discriminative and generative methods of inference. Both methods are extensively used by the vision and machine learning communities and correspond to the distinction between bottom-up and top-down processing. Discriminative methods are typically fast but can give sub-optimal and inconsistent results, see Fig. 3. By con-trast, generative methods are optimal (in the sense of Bayesian Decision Theory) but can be slow because they require extensive search. Our DDMCMC algorithm integrates both methods, as illustrated in Fig. 8, by using discriminative methods to propose generative solutions.\nThe goal of our algorithm is to construct a parse graph representing the image. The structure of the graph is not fixed and will depend on the input image. The algorithm proceeds by constructing Markov Chain dynamics, implemented by sub-kernels, for different moves to configure the parsing graph-such as creating or deleting nodes, or altering node attributes. Our approach can be scaled-up by adding new subkernels, corresponding to different vision models. This is similar in spirit to Ullman's concept of \"visual routines\" (Ullman, 1995). Overall, the ideas in this paper can be applied to any other inference problem that can be formulated as probabilistic inference on graphs.\nOther work by our group deals with a related series of visual inference tasks using a similar framework. This includes image segmentation , curve grouping (Tu and Zhu, 2002a), shape detection (Tu and Yuille, 2004), motion analysis (Barbu and Zhu, 2004), and 3D scene reconstruction (Han and Zhu, 2003). In the future, we plan to integrate these visual modules and develop a general purpose vision system.\nFinally, we are working on ways to improve the speed of the image parsing algorithm as discussed in Section 8. In particular, we expect the use of the Swendsen-Wang cut algorithms Zhu, 2003, 2004) to drastically accelerate the search. We anticipate that this, and other improvements, will reduce the running time of DDMCMC algorithms from 10-20 min (Tu and Zhu, 2002a) to well under a minute.", "publication_ref": ["b51", "b48", "b1", "b22"], "figure_ref": ["fig_8"], "table_ref": []}, {"heading": "Appendix A: Proof of Theorem 2", "text": "Proof: For notational simplicity, we ignore the dependencies on the kernels and probabilities on the input image I.\nLet \u00b5 t (W t ) be the state probability at time step t. After applying a sub-kernel K a (W t+1 | W t ), its state becomes W t+1 with probability:\n\u00b5 t+1 (W t+1 ) = W t \u00b5 t (W t )K a (W t+1 | W t ). (17)\nThe joint probability \u00b5(W t , W t+1 ) for W t and W t+1 can be expressed in two ways as:\n\u00b5(W t , W t+1 ) = \u00b5 t (W t )K a (W t+1 | W t ) = \u00b5 t+1 (W t+1 ) p MC (W t | W t+1 ), (18\n)\nwhere p MC (W t | W t+1 ) is the \"posterior\" probability for state W t at time step t conditioned on state W t+1 at time step t + 1.\nThis joint probability \u00b5(W t , W t+1 ) can be compared to the joint probability \u03c1(W t , W t+1 ) at equilibrium (i.e. when \u00b5 t (W t ) = P(W t )). We can also express \u00b5(W t , W t+1 ) in two ways:\np(W t , W t+1 ) = p(W t )K a (W t+1 | W t ) = p(W t+1 )K a (W t+1 , W t ), (19)\nwhere the second equality is obtained using the detailed balance condition on K a .\nWe calculate the Kullback-Leibler (K-L) divergence between the joint probabilities P(W t , W t+1 ) and \u00b5(W t , W t+1 ) in two ways using the first and second equalities in Eqs. ( 18) and (19). We obtain two expressions (1 & 2) for the K-L divergence:\nKL( p(W t , W t+1 ) \u00b5(W t , W t+1 )) = W t+1 W t p(W t , W t+1 ) log p(W t , W t+1 ) \u00b5(W t , W t+1 ) 1 = W t p(W t ) W t+1 K a (W t+1 | W t )) \u00d7 log p(W t ) \u2022 K a (W t+1 | W t ) \u00b5 t (W t )K a (W t+1 | W t ) = K L( p(W ) \u00b5 t (W )) (20) 2 = W t+1 W t K a (W t | W t+1 ) p(W t+1 ) \u00d7 log p(W t+1 )K a (W t | W t+1 ) \u00b5 t+1 (W t+1 ) p MC (W t | W t+1 ) = K L( p(W t+1 ) \u00b5 t+1 (W )) + E p(W t+1 ) [K L(K a (W t | W t+1 ) p MC (W t | W t+1 ))] (21)\nWe equate the two alternatives expressions for the KL divergence, using Eqs. ( 20) and ( 21), and obtain\nK L( p(W ) \u00b5 t (W )) \u2212 K L( p(W ) \u00b5 t+1 (W )) = E p(W t+1 ) K L(K a(t) (W t | W t+1 ) p MC (W t | W t+1 ))\nThis proves that the K-L divergence decreases monotonically. The decrease is zero only when K \u03b1(t) (W t | W t+1 ) = p MC (W t | W t+1 ) (because K L( p \u00b5) \u2265 0 with equality only when p = \u00b5). This occurs if and only if \u00b5 t (W ) = p(W ) (using the definition of p MC (W t | W t+1 ), given in Eq. ( 18), and the detailed balance conditions for K a(t) (W t | W t+1 )).\nFigure 21. There may be multiple routes between the two states W and W and this creates an additional computational burden for computing the acceptance rates.\nThe expected first hitting time has higher resolution than the TV-norm in Theorem 4 or the KL-divergence in Theorem 2, as it is concerned with individual states rather than the entire space . In particular, we are interested in the expected first hitting time for those states W with high probability P(W). The following theorem (Maciuca and Zhu, 2003) shows how an informed proposal q can improve the expected first hitting time for a special class of Markov Chains.\nTheorem 5 (Maciuca and Zhu, 2003). Let p(W ), W \u2208 be the invariance (target) probability of a Markov chain MC, and Q(W, W ) = q(W ) be the proposal probability in the Metropolis-Hasting Eq. (8), then\n1 min{ p(W ), q(W )} \u2264 E[\u03c4 (W )] \u2264 1 min{ p(W ), q(W )} \u2022 1 1 \u2212 p \u2212 q TV (24\n)\nwhere p \u2212 q = 1 2 x\u2208 | p(x) \u2212 q(x) |\u2264 1.\nIn this Markov Chain design, the proposal probability depends only on W not W , and is called the Metropolized independence sampler (MIS). A similar result can also be obtained for the Metropolized Gibbs sampler (MGS). This result shows that we should choose the proposal probability q so that it overlaps with p and so that q(W) is large for those states W with high probability P(W). By contrast, choosing q to be the uniform probability, q(W ) = 1/| |\u2200W \u2208 , will result in expected first hitting times of length greater than | |) for any state with P(W ) > 1/| |. In some situations, the sub-kernels can give multiple routes between states W and W . For example, one region can be split into the same two sub-regions following different bottom-up proposals. In this section, we show that detailed balance can be maintained either by using extra computation to integrate out the multiple routes, or by treating the alternative routes separately using different sub-kernels but at the price of reduced efficiency.\nSuppose there are n pairs of routes {(q(i)Q i (W, W ), q(i)Q i (W , W ), i = 1 . . . n} between two states W and W , such as the two pairs of moves in Fig. 21(a). We represent the transition kernel by K(W, W ) = Q(W, W )\u03b1(W, W ), where\nQ(W, W ) = n i=1 q(i)Q i (W, W ),(25)\nand\n\u03b1(W, W ) = min 1 Q(W , W ) p(W ) Q(W, W ) p(W )\nEvery time a move from W to W is considered we have to consider all possible routes, which can result in heavy computation. Instead, we can treat each route as a sub-kernel as described in Section 2.3. Then the Markov chain satisfies detailed balance because each pair K(W, W ) and K i (W , W ) are specified by Metropolis-Hastings 8 and hence obey the detailed balance equation p(W )K i (W, W ) = p(W )K i (W , W ), i = 1 . . . n. This reduces the computational cost. But, as the following theorem shows, this reduction comes at the price of efficiency. Theorem 6. The kernels K(W, W ) and K(W , W ) as computed in Eq. (25) satisfy K(W, W ) \u2265 n i=1 q(i)K i (W, W ), and\nK(W, W ) \u2265 n i=1 q(i)K i (W , W ), \u2200 W = W.\nThis theorem says that the Markov chain which obeys detailed balance for each pair of moves is less effective than the one which combines all the routes. Markov chain design must balance the computation cost of computing all the proposals against the effectiveness of the Markov kernel. The situation shown in Fig. 21(b) can be treated as a special case of Fig. 21(a).", "publication_ref": ["b30", "b30"], "figure_ref": ["fig_0", "fig_0", "fig_0", "fig_0"], "table_ref": []}, {"heading": "Acknowledgments", "text": "This work was supported by an NIH (NEI) grant RO1-EY 012691-04, an NSF SGER grant IIS-0240148, an NSF grant IIS-0244763 and an ONR grant N00014-02-1-0952. The authors thank the Smith-Kettlewell research institute for providing us with text training images. We thank Yingnian Wu for stimulating discussions on the Markov chain convergence theorems.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Notes", "text": "1. Unlike most graphical inference algorithms in the literature which assume fixed graphs, such as belief propagation (Yedidia, 2001). 2. For many natural images the posterior probabilities P(W | I) are strongly peaked and so fair samples are close to the posterior maximum argmax P(W | I). So in this paper we do not distinguish between sampling and inference (optimization). 3. Recently the term \"discriminative model\" has been extended\nto cover almost any approximation to the posterior distribution P(W | I), e.g. Kumar and Hebert (2003). We will use \"discriminative model\" in its traditional sense of categorization. 4. The optimal approximation occurs when g(w, | Tst(I)) equals the probability p(w, | Tst(I)) induced by P(I | W )P(W ). 5. We choose stochastic dynamics because the Markov chain probability is guaranteed to converge to the posterior P(W | I). The complexity of the problem means that deterministic algorithms for implementing these moves risk getting stuck in local minima. 6. Algorithms like belief propagation (Yedidia et al., 2001) can be derived as approximations to this update equation by using a Gibbs sampler and making independence assumptions (Yuille, 2004).\n7. Except for one that evolves region boundaries. 8. The boundary can include an \"internal boundary\" if there is a hole inside the image region explained by a different visual pattern. 9. Except for the boundary evolution sub-kernel which will be described separately, see Section 5.1.", "publication_ref": ["b55", "b26", "b55", "b56"], "figure_ref": [], "table_ref": []}, {"heading": "Appendix B: Classic Markov Chain Convergence Result", "text": "Traditional Markov chain convergence analysis is concerned with the total variance between the invariant (target) probability p(W | I) and the Markov chain state probability \u00b5 t (W ),\nIn a finite state space , a classic result relates the TV-measure to the second largest eigenvalue modulus (\u03bb slem \u2208 [0, 1)) of the kernel K. The following theorem is adopted from Diaconis and Hanlon (1992).\nTheorem 4 (Diaconis and Hanlon, 1992). For an irreducible kernel K on finite space with invariant (target) probability p(W | I) and initial state W o , then at step n ", "publication_ref": ["b14", "b14"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Graph partition by Swendsen-Wang cut", "journal": "", "year": "2003", "authors": "A Barbu; S C Zhu"}, {"ref_id": "b1", "title": "Multi-grid and multi-level Swendsen-Wang cuts for hierarchic graph partition", "journal": "", "year": "2004", "authors": "A Barbu; S C Zhu"}, {"ref_id": "b2", "title": "Learning the semantics of words and pictures", "journal": "ICCV", "year": "2001", "authors": "K Barnard; D A Forsyth"}, {"ref_id": "b3", "title": "Shape matching and object recognition using shape contexts", "journal": "IEEE Trans, on Pattern Analysis and Machine Intelligence", "year": "2002", "authors": "S Belongie; J Malik; J Puzicha"}, {"ref_id": "b4", "title": "Compositionality, MDL Priors, and Object Recognition, NIPS", "journal": "", "year": "1997", "authors": "E Bienenstock; S Geman; D Potter"}, {"ref_id": "b5", "title": "Hierarchical testing designs for pattern recognition", "journal": "", "year": "2003", "authors": "G Blanchard; D Geman"}, {"ref_id": "b6", "title": "Markov Chains: Gibbs Fields, Monte Carlo Simulation and Queues", "journal": "Springer", "year": "1999", "authors": "P Bremaud"}, {"ref_id": "b7", "title": "Edge detector evaluation using empirical ROC curves, Computer Vision and Image Understanding", "journal": "", "year": "2001", "authors": "K W Bowyer; C Kranenburg; S Dougherty"}, {"ref_id": "b8", "title": "A computational approach to edge detection", "journal": "IEEE Trans, on PAMI", "year": "1986", "authors": "J Canny"}, {"ref_id": "b9", "title": "AdaBoost learning for detecting and reading text in city scenes", "journal": "", "year": "2004", "authors": "X Chen; A L Yuille"}, {"ref_id": "b10", "title": "Active appearance models", "journal": "IEEE Trans, on PAMI", "year": "2001", "authors": "T F Cootes; G J Edwards; C J Taylor"}, {"ref_id": "b11", "title": "Mean shift analysis and applications", "journal": "", "year": "1999", "authors": "D Comaniciu; P Meer"}, {"ref_id": "b12", "title": "Elements of Information Theory", "journal": "John Wiley and Sons, Inc: NY", "year": "1991", "authors": "T M Cover; J A Thomas"}, {"ref_id": "b13", "title": "The Helmholtz Machine", "journal": "Neural Computation", "year": "1995", "authors": "P Dayan; G Hinton; R Neal; R Zemel"}, {"ref_id": "b14", "title": "Eigenanalysis for some examples of the Metropolis algorithms", "journal": "Contemporary Mathematics", "year": "1992", "authors": "P Diaconis; P Hanlon"}, {"ref_id": "b15", "title": "Boosting performance in neural networks", "journal": "Intl J. Pattern Rec. and Artificial Intelligence", "year": "1993", "authors": "H Drucker; R Schapire; P Simard"}, {"ref_id": "b16", "title": "How Much Does Globalization Help Segmentation? CVPR", "journal": "", "year": "2004", "authors": "C Fowlkes; J Malik"}, {"ref_id": "b17", "title": "Experiments with a new boosting algorithm", "journal": "", "year": "1996", "authors": "Y Freund; R Schapire"}, {"ref_id": "b18", "title": "Additive logistic regression: A statistical view of boosting", "journal": "", "year": "1998", "authors": "J Friedman; T Hastie; R Tibshirani"}, {"ref_id": "b19", "title": "Diffusion for global optimization", "journal": "SIAM J. on Control and Optimization", "year": "1986", "authors": "S Geman; C R Huang"}, {"ref_id": "b20", "title": "Reversible jump markov chain monte carlo computation and bayesian model determination", "journal": "Biometrika", "year": "1995", "authors": "P J Green"}, {"ref_id": "b21", "title": "Two and Three Dimensional Patterns of the Face", "journal": "", "year": "1999", "authors": "P Hallinan; G Gordon; A Yuille; P Giblin; D ; A K Mumford;  Peters"}, {"ref_id": "b22", "title": "Bayesian reconstruction of 3D shapes and scenes from a single image", "journal": "", "year": "2003", "authors": "F Han; S C Zhu"}, {"ref_id": "b23", "title": "Monte Carlo sampling methods using Markov chains and their applications", "journal": "Biometrika", "year": "1970", "authors": "W K Hastings"}, {"ref_id": "b24", "title": "A generative constituent-context model for improved grammar induction", "journal": "", "year": "2002", "authors": "D Klein; C D Manning"}, {"ref_id": "b25", "title": "Statistical edge detection: learning and evaluating edge cues", "journal": "IEEE Trans, on Pattern Analysis and Machine Intelligence", "year": "2003", "authors": "S Konishi; J M Coughlan; A L Yuille; S C Zhu"}, {"ref_id": "b26", "title": "Discriminative random fields", "journal": "", "year": "2003", "authors": "S Kumar; M Hebert"}, {"ref_id": "b27", "title": "Rapid natural scene categorization in the near absence of attention", "journal": "", "year": "2003", "authors": "F F Li; R Vanrullen; C Koch; P Perona"}, {"ref_id": "b28", "title": "Monte Carlo Strategies in Scientific Computing", "journal": "Springer", "year": "2001", "authors": "J S Liu"}, {"ref_id": "b29", "title": "Distinctive image features from scale-invariant keypoints", "journal": "", "year": "2003", "authors": "L D Lowe"}, {"ref_id": "b30", "title": "How do heuristics expedite markov chain search", "journal": "", "year": "2003", "authors": "R Maciuca; S C Zhu"}, {"ref_id": "b31", "title": "Contour and texture analysis for image segmentation", "journal": "Int'l Journal of Computer Vision", "year": "2001", "authors": "J Malik; S Belongie; T Leung; J Shi"}, {"ref_id": "b32", "title": "Foundations of Statistical Natural Language Processing", "journal": "MIT Press", "year": "2003", "authors": "C D Manning; H Schiitze"}, {"ref_id": "b33", "title": "", "journal": "Freeman and Co", "year": "1982", "authors": "D Marr"}, {"ref_id": "b34", "title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics", "journal": "", "year": "2001", "authors": "D Martin; C Fowlkes; D Tal; J Malik"}, {"ref_id": "b35", "title": "Equations of state calculations by fast computing machines", "journal": "J. Chem. Phys", "year": "1953", "authors": " N Metropolis; M N Rosenbluth; A W Rosenbluth; A H Teller; E Teller"}, {"ref_id": "b36", "title": "Probabilistic visual learning for object representation", "journal": "IEEE Trans. PAMI", "year": "1997", "authors": "B Moghaddam; A Pentland"}, {"ref_id": "b37", "title": "Neuronal Architectures for Pattern-theoretic Problems", "journal": "MIT Press: A Bradford Book", "year": "1995", "authors": "D B Mumford"}, {"ref_id": "b38", "title": "Using the forest to see the tree: A graphical model relating features, objects and the scenes", "journal": "NIPS", "year": "2003", "authors": "K Murphy; A Torralba; W T Freeman"}, {"ref_id": "b39", "title": "An Introduction to Digital Image Processing", "journal": "Prentice Hall", "year": "1986", "authors": "W Niblack"}, {"ref_id": "b40", "title": "The FERET database and evaluation procedure for face recognition algorithms", "journal": "Image and Vision Computing Journal", "year": "1998", "authors": "P J Phillips; H Wechsler; J Huang; P Rauss"}, {"ref_id": "b41", "title": "Toward true 3D object recognition, Reconnaissance de Formes et Intelligence Artificielle", "journal": "", "year": "2004", "authors": "J Ponce; S Lazebnik; F Rothganger; C Schmid"}, {"ref_id": "b42", "title": "The boosting approach to machine learning: An overview, MSRI Workshop on Nonlinear Estimation and Classification", "journal": "", "year": "2002", "authors": "R E Schapire"}, {"ref_id": "b43", "title": "Normalized Cuts and Image Segmentation", "journal": "IEEE Trans. PAMI", "year": "2000", "authors": "J Shi; J Malik"}, {"ref_id": "b44", "title": "Speed of processing in the human visual system", "journal": "Nature", "year": "1996", "authors": "S Thorpe; D Fize; C Marlot"}, {"ref_id": "b45", "title": "Features and objects in visual processing", "journal": "Scientific American", "year": "1986", "authors": "A Treisman"}, {"ref_id": "b46", "title": "Image segmentation by Data-driven Markov chain Monte Carlo", "journal": "IEEE Trans. PAMI", "year": "2002", "authors": "Z Tu; S C Zhu"}, {"ref_id": "b47", "title": "Parsing images into regions, curves and curve groups", "journal": "", "year": "2002", "authors": "Z W Tu; S C Zhu"}, {"ref_id": "b48", "title": "Shape matching and recognition: Using generative models and informative features", "journal": "", "year": "2004", "authors": "Z W Tu; A L Yuille"}, {"ref_id": "b49", "title": "Eigenfaces for recognition", "journal": "J. of Cognitive Neurosciences", "year": "1991", "authors": "M Turk; A Pentland"}, {"ref_id": "b50", "title": "Visual routines", "journal": "Cognition", "year": "1984", "authors": "S Ullman"}, {"ref_id": "b51", "title": "Sequence seeking and counterstreams: A model for bidirectional information flow in the cortex. In Large-Scale Neuronal Theories of the Brain. C", "journal": "MIT Press. A Bradford Book", "year": "1995", "authors": "S Ullman"}, {"ref_id": "b52", "title": "Fast and robust classification using asymmetric Adaboost and a detector cascade", "journal": "", "year": "2001", "authors": "P Viola; M Jones"}, {"ref_id": "b53", "title": "Towards Automatic Discovery of Object Categories", "journal": "", "year": "2000", "authors": "M Weber; M Welling; P Perona"}, {"ref_id": "b54", "title": "Learning a rare event detection cascade by direct feature selection", "journal": "NIPS", "year": "2004", "authors": "J Wu; J M Regh; M D Mullin"}, {"ref_id": "b55", "title": "Generalized belief propagation", "journal": "", "year": "2001", "authors": "J S Yedidia; W T Freeman; Y Weiss"}, {"ref_id": "b56", "title": "Belief Propagation and Gibbs Sampling. Submitted to Neural Computation", "journal": "", "year": "2004", "authors": "A L Yuille"}, {"ref_id": "b57", "title": "Region competition: Unifying snakes, region growing, and Bayes/MDL for multiband image segmentation", "journal": "IEEE Trans. PAMI", "year": "1996", "authors": "S C Zhu; A L Yuille"}, {"ref_id": "b58", "title": "Integrating topdown/bottom-up for object recognition by data-driven Markov chain Monte Carlo", "journal": "", "year": "2000", "authors": "S C Zhu; R Zhang; Z W Tu"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 .2Figure2. Examples of image segmentation failure by an algorithm(Tu and Zhu, 2002a)  which uses only generic visual patterns (i.e. only low-level visual cues). The results (b) show that low-level visual cues are not sufficient to obtain good intuitive segmentations. The limitations of using only generic visual patterns are also clear in the synthesized images (c) which are obtained by stochastic sampling from the generative models after the parameters have been estimated by DDMCMC. The right panels (d) show the segmentations obtained by human subjects who, by contrast to the algorithm, appear to use object specific knowledge when doing the segmentation (though they were not instructed to)(Martin et al., 2001). We conclude that to achieve good segmentation on these types of images requires combining segmentation with object detection and recognition.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "w K a (W | W : I) = 1, \u2200 W ). Kernels which alter the graph structure are grouped into reversible pairs. For example, the subkernel for node creation K a,r (W | W : I) is paired with the sub-kernel for node deletion K a,l (W | W : I). This can be combined into a paired sub-kernel K a = \u03c1 ar K a,r (W | W : I)+\u03c1 al K a,l (W | W : I)(\u03c1 ar +\u03c1 al = 1) This pairing ensures that K a (W | W : I) = 0 if, and only if, K a (W | W : I) = 0 for all states W, W \u2208 . The sub-kernels (after pairing) are constructed to obey the detailed balance condition: p(W | I)K a (W | W : I) = p(W | I)K a (W | W : I).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Ka (W | W : I) = Q a (W | W : Tst a (I)) \u00d7 min 1, p(W | I)Q a (W | W : Tst a (I)) p(W | I)Q a (W | W : Tst a (I)) , W = W (8) where a transition from W to W is proposed (stochastically) by the proposal probability Q a (W | W : Tst a (I)) and accepted (stochastically) by the acceptance probability: \u03b1(W | W : I) = min 1, p(W | I)Q a (W | W : Tst a (I)) p(W | I)Q a (W | W : Tst a (I)) . (9)", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 .4Figure 4. Abstract representation of the parsing graph used in this paper. The intermediate nodes represent the visual patterns. Their child nodes correspond to the pixels in the image.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 .5Figure 5. Random samples drawn from the shape descriptors for text characters.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Model \u03b6 = 3 and \u03b6 = 5, . . . , 66. This family of models are used to describe generic shading", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 6 .6Figure 6. Random samples drawn from the PCA face model.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 7 .7Figure 7. Examples of Markov chain dynamics that change the graph structure or the node attributes of the graph giving rise to different ways to parse the image.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 8 .8Figure8. Integrating generative (top-down) and discriminative (bottom-up) methods for image parsing. This diagram illustrates the main points of the image parser. The dynamics are implemented by an ergodic Markov chain K, whose invariant probability is the posterior p(W I), and which is composed of reversible sub-kernels K a for making different types of moves in the parse graph (e.g. giving birth to new nodes or merging nodes). At each time step the algorith-m selects a sub-kernel stochastically. The selected sub-kernel proposes a specific move (e.g. to create or delete specific nodes) and this move is then evaluated and accepted stochastically, see Eq. (8). The proposals are based on both bottom-up (discriminative) and top-down (generative) processes, see Section 2.4. The bottom-up processes compute discriminative probabilities q(w j Tst j (I)), j = 1, 2, 3, 4 from the input image I based on feature tests Tst j (I). An additional sub-kernel for boundary evolution uses a stochastic partial differential equation will be described later.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 9 .9Figure9. The evolution of the region boundaries is implemented by stochastic partial differential equations which are driven by models competing for ownership of the regions.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "\u00b53r and \u03c9 \u03bd 3l to approximate the ratios p(W | I) p(W | I) and p(W | I) p(W | I) respectively. p(W | I) p(W | I) is given by:", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "HAda (Tst Ada (I)) = sign n i=1 \u03b1 i h i (I) = sign < \u03b1 Ada , Tst Ada (I) > . (15)", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Figure 12 .12Figure 12. Some scenes from which the training text patches are extracted.", "figure_data": ""}, {"figure_label": "13", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Figure 13 .13Figure 13. Positive training examples for AdaBoost.", "figure_data": ""}, {"figure_label": "14", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "Figure 14 .14Figure 14. Some failure examples of text and faces that Adaboost misses. The text image failures (a) are challenging and were not included in our positive training examples. The face failures (b) include heavy shading, occlusion and facial features.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "database for our positive examples, see Fig. 13(b), and by allowing small rotation and translation transformation we had 5,000 positive examples. We used the same strategy as described above for text to obtain negative examples. Failure examples are shown in Fig. 14.", "figure_data": ""}, {"figure_label": "15", "figure_type": "figure", "figure_id": "fig_16", "figure_caption": "Figure 15 .15Figure15. The boxes show faces and text as detected by the AdaBoost log posterior ratio test with fixed threshold. Observe the false positives due to vegetation, tree structure, and random image patterns. It is impossible to select a threshold which has no false positives and false negatives for this image. As it is shown in our experiments later, the generative models will remove the false positives and also recover the missing text.", "figure_data": ""}, {"figure_label": "16", "figure_type": "figure", "figure_id": "fig_17", "figure_caption": "Figure 16 .16Figure 16. Example of binarization on the detected text.", "figure_data": ""}, {"figure_label": "17", "figure_type": "figure", "figure_id": "fig_18", "figure_caption": "Figure 17 .17Figure 17. Results of segmentation and recognition on two images. The results are improved compare to the purely bottom-up (AdaBoost) results displayed in Fig. 15.", "figure_data": ""}, {"figure_label": "18", "figure_type": "figure", "figure_id": "fig_19", "figure_caption": "Figure 18 .18Figure 18. Aclose-up look of an image in Fig. 17. The dark glasses are explained by the generic shading model and so the face model does not have to fit this part of the data. Otherwise the face model would have difficulty because it would try to fit the glasses to eyes. Standard AdaBoost only correctly classifies these faces at the expense of false positives, see Fig. 15. We show two examples of synthesized faces, one (Synthesis 1) with the dark glasses (modelled by shading regions) and the other (Synthesis 2) with the dark glasses removed (i.e. using the generative face model to sample parts of the face (e.g. eyes) obscured by the dark glasses.", "figure_data": ""}, {"figure_label": "19", "figure_type": "figure", "figure_id": "fig_20", "figure_caption": "Figure 19 .19Figure 19. Results of segmentation and recognition on outdoor images. Observe the ability to detect faces and text at multiple scale.", "figure_data": ""}, {"figure_label": "20", "figure_type": "figure", "figure_id": "fig_21", "figure_caption": "Figure 20 .20Figure 20. Two mechanisms for constructing the parsing graph. See text for explanation.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_22", "figure_caption": "+ ) = KL( p(w | I) q(w | Tst(I))) \u2212 KL( p(w | I) q(w | Tst t (I), F + )), where Tst(I) denotes the previous tests (and KL is the Kullback-Leibler divergence).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_23", "figure_caption": "Appendix D: Multiple Routes in the Sub-Kernels This section addresses a practical issue about detailed balance which leads to a trade-off between computation and efficiency in the algorithm design. For notational simplicity, we write terms like Q(W | W : I) & K(W | W : I) as Q(W, W ) & K(W, W ) in this section.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "An example of the split-merge sub-kernel. State W consists of three regions and proposals are computed for 7 candidate splits. One is selected, see arrow, which changes the state to W . Conversely, there are 5 candidate merges in state W and the one selected, see arrow, returns the system to state W .", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Performance of AdaBoost at different thresholds.", "figure_data": "Object False positive False negative Images SubwindowsFace6526162355,960,040Face91814162355,960,040Face75421162355,960,040Text118273520,183,316Text1879'53520,183,316"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "The number of variables in W* for each image compared to the JPG bytes.", "figure_data": "ImageStopSoccerParkingStreetWestwoodjpg bytes 23,998 19,56323,31126,17027,790|W |4,8863,9715,0136,3469,687"}], "formulas": [{"formula_id": "formula_0", "formula_text": "W \u223c p(W | l) \u221d p(I | W ) p(W ).", "formula_coordinates": [5.0, 111.79, 270.83, 126.41, 9.96]}, {"formula_id": "formula_1", "formula_text": "E I [KL( p(w | I) q(w | Tst(I)))] \u2212E I [KL( p(w | I) q(w | Tst(I), Tst + (I)))] = MI(w Tst, T st + ) \u2212 M I (w Tst) = E Tst,Tst + [KL(q(w | Tst, T st + ) q(w | Tst))] \u2265 0,", "formula_coordinates": [5.0, 76.93, 127.29, 448.68, 604.98]}, {"formula_id": "formula_2", "formula_text": "K(W | W : I) = a \u03c1(a : I)K a (W | W : I), a \u03c1(a : I) = 1, \u03c1(a : I) > 0. (4", "formula_coordinates": [6.0, 91.88, 389.28, 189.71, 44.57]}, {"formula_id": "formula_3", "formula_text": ")", "formula_coordinates": [6.0, 281.58, 415.07, 3.87, 8.97]}, {"formula_id": "formula_4", "formula_text": "\u00b5 t+1 (W ) = W K a(t) (W | W : I)\u00b5 t (W ). (5", "formula_coordinates": [6.0, 93.9, 664.55, 187.68, 19.91]}, {"formula_id": "formula_5", "formula_text": ")", "formula_coordinates": [6.0, 281.58, 665.22, 3.87, 8.97]}, {"formula_id": "formula_6", "formula_text": "W \u223c \u00b5 t (W ) = \u03bd(W o ) \u2022 K a(1) \u2022 K a(2) \u2022 \u2022 \u2022 \u2022 \u2022 K a(t) (W o , W ) \u2192 p(W | I). (6", "formula_coordinates": [6.0, 319.5, 160.14, 201.19, 26.42]}, {"formula_id": "formula_7", "formula_text": ")", "formula_coordinates": [6.0, 520.69, 176.75, 3.87, 8.97]}, {"formula_id": "formula_8", "formula_text": "K a(t) , \u2200a(t) \u2208 A is applied, KL( p(W | I) \u00b5 t (W )) \u2212 KL( p(W | I) \u00b5 t+1 (W )) \u2265 0 (7)", "formula_coordinates": [6.0, 309.37, 328.43, 215.19, 46.47]}, {"formula_id": "formula_9", "formula_text": "p(W | I) W \u2208 a (W ) p(W | I) if W \u2208 a (W ), = 0, otherwise. (10", "formula_coordinates": [7.0, 111.24, 536.07, 170.07, 38.78]}, {"formula_id": "formula_10", "formula_text": ")", "formula_coordinates": [7.0, 281.31, 565.56, 4.15, 8.97]}, {"formula_id": "formula_11", "formula_text": "p(I R(L i ) | \u03b6 i , L i , i ) for generating the sub-image I R(L i ) in region R(L i ). The variables \u03b6 i \u2208 {1, . . . , 66}", "formula_coordinates": [8.0, 70.26, 222.93, 215.19, 23.05]}, {"formula_id": "formula_12", "formula_text": "W = (K , {(\u03b6 i , L i , i ) : i = 1, 2, . . . , K }).", "formula_coordinates": [8.0, 91.22, 328.15, 167.54, 10.48]}, {"formula_id": "formula_13", "formula_text": "= K i=1 R(L i ), R(L i ) \u2229 R(L j ) = \u2205, \u2200i = j.", "formula_coordinates": [8.0, 86.55, 429.67, 187.34, 28.79]}, {"formula_id": "formula_14", "formula_text": "p(I | W ) = K i=1 p I R(L i ) \u03b6 i L i , i . The prior probability p(W) is defined by p(W ) = p(K ) K i=1 p(L i ) p(\u03b6 i | L i ) p( i | \u03b6 i ).", "formula_coordinates": [8.0, 80.23, 502.12, 181.53, 92.27]}, {"formula_id": "formula_15", "formula_text": "W * = arg max p W \u2208 (W | I) = arg max p W \u2208 (I | W ) p(W ). (11", "formula_coordinates": [8.0, 74.36, 660.08, 206.94, 34.65]}, {"formula_id": "formula_16", "formula_text": ")", "formula_coordinates": [8.0, 281.31, 685.76, 4.15, 8.97]}, {"formula_id": "formula_17", "formula_text": "p(L i ) \u221d exp{\u2212\u03b3 | R(L i ) | \u03b1 \u2212 \u03bb|L i |}.(12)", "formula_coordinates": [8.0, 340.52, 369.17, 184.04, 12.35]}, {"formula_id": "formula_18", "formula_text": "M i = \u03c3 x 0 0 \u03c3 y cos \u03b8 \u2212 sin \u03b8 sin \u03b8 cos \u03b8 1 h 0 1 .", "formula_coordinates": [8.0, 326.25, 703.03, 176.1, 25.42]}, {"formula_id": "formula_19", "formula_text": "p(M i ) \u221d exp \u2212 a|\u03b8 | 2 \u2212 b \u03c3 x \u03c3 y + \u03c3 y \u03c3 x 2 \u2212 ch 2 ,", "formula_coordinates": [9.0, 75.98, 323.6, 198.93, 26.6]}, {"formula_id": "formula_20", "formula_text": "G TP (M i , c i ) = U \u00d7 M s \u00d7 M i \u00d7 TP(c i ).", "formula_coordinates": [9.0, 70.1, 410.36, 215.35, 22.43]}, {"formula_id": "formula_21", "formula_text": "S i , p(S i | M i , c i ) \u221d exp{\u2212\u03b3 | R(L i ) | \u03b1 \u2212D(G S (M i , c i ) G TP (M i , c i ))}, where D(G S (M i , c i ) G TP (M i , c i ))", "formula_coordinates": [9.0, 70.26, 482.76, 179.13, 76.1]}, {"formula_id": "formula_22", "formula_text": "L i = (c i , M i , S i ),", "formula_coordinates": [9.0, 139.9, 669.2, 70.51, 10.48]}, {"formula_id": "formula_23", "formula_text": "p(L i ) = p(c i ) p(M i ) p(S i | M i , c i ).", "formula_coordinates": [9.0, 107.55, 719.55, 135.79, 10.48]}, {"formula_id": "formula_24", "formula_text": "p 1 I R(L) \u03b6 = 1, L , = v\u2208R(L) G(I v \u2212 \u00b5; \u03c3 2 ), = (\u00b5, \u03c3 ) 2. Clutter/Texture Model \u03b6 = 2. This is a non- parametric intensity histogram h() discretized to take G values (i.e. is expressed as a vector (h 1 , h 2 , . . . , h G )).", "formula_coordinates": [9.0, 309.37, 520.16, 215.19, 88.01]}, {"formula_id": "formula_25", "formula_text": "p 2 I R(L) \u03b6 = 2, L , = v\u2208R(L) h(I v ) = G j=1 h nj j , = (h 1 , h 2 , . . . , h G ).", "formula_coordinates": [9.0, 323.07, 648.37, 193.24, 43.48]}, {"formula_id": "formula_26", "formula_text": "p 4 (I R (L) \u03b6 = 4, L , ) = G I R(L) \u2212 i \u03bb i B i ; , = (\u03bb 1 , . . . , \u03bb n , ).", "formula_coordinates": [10.0, 96.45, 595.8, 162.3, 57.05]}, {"formula_id": "formula_27", "formula_text": "E(R i ) = R i {\u2212 log p(I(x, y)|\u03b8 \u03b6 i )} dxdy + \u03b3 |R i | \u03b1 + \u03bb|\u2202 R i |. and E(T j ) = L j \u2212 log p(I(x, y)|\u03b8 \u03b6 j ) dxdy + \u03b3 |R(L j )| \u03b1 \u2212 log p(L j ).", "formula_coordinates": [14.0, 70.26, 423.45, 192.8, 112.71]}, {"formula_id": "formula_28", "formula_text": "d S m dt = \u2212 \u03b4 E(R i ) \u03b4S m \u2212 \u03b4 D(T j ) \u03b4S m = \u2212 \u03b4 E(R i ) \u03b4v \u2212 \u03b4 E(T j ) \u03b4v 1 |J(S)| ds = n(v) log p I(v); \u03b8 \u03b6 i p I(v); \u03b8 \u03b6 j + \u03b1\u03b3 1 |D j | 1\u2212\u03b1 \u2212 1 |D i | 1\u2212\u03b1 \u2212\u03bb\u03ba + D G S j (s) G T (s) 1 |J(s)| ds,", "formula_coordinates": [14.0, 86.88, 591.81, 183.15, 141.31]}, {"formula_id": "formula_29", "formula_text": "p(I(v);\u03b8 \u03b6 i )", "formula_coordinates": [14.0, 462.96, 148.9, 29.36, 8.39]}, {"formula_id": "formula_30", "formula_text": "S 1r (W ) = z (\u00b5) 1r , \u03c9 (\u00b5) 1r \u00b5 = 1, 2, . . . , N 1r .", "formula_coordinates": [14.0, 326.71, 461.07, 181.01, 14.12]}, {"formula_id": "formula_31", "formula_text": "S 1l (W ) = z (\u03bd) 1l , \u03c9 (\u03bd) 1l \u03bd = 1, 2, . . . , N 1l . {z (\u00b5)", "formula_coordinates": [14.0, 309.37, 524.42, 196.72, 39.02]}, {"formula_id": "formula_32", "formula_text": "Q 1r (W |W : I) = \u03c9 1r (W ) N 1r \u00b5=1 \u03c9 (\u00b5) 1r , Q 1l (W |W , I) = \u03c9 1l (W ) N 1l \u03bd=1 \u03c9 (\u03bd) 1l .", "formula_coordinates": [14.0, 358.18, 625.01, 123.55, 58.38]}, {"formula_id": "formula_33", "formula_text": "W = (K , (\u03b6 k , L k , k ), W \u2212 ) W = (K + 1, (\u03b6 i , L i , i ), (\u03b6 j , L j , j ), W \u2212 )", "formula_coordinates": [15.0, 94.11, 466.47, 180.91, 26.65]}, {"formula_id": "formula_34", "formula_text": "S 3r (W ) = z (\u00b5) 3r , \u03c9 (\u00b5) 3r : \u00b5 = 1, 2, . . . , N 3r . , S 3l (W ) = z (\u03bd)", "formula_coordinates": [15.0, 88.95, 627.39, 182.65, 29.82]}, {"formula_id": "formula_35", "formula_text": "\u03bd = 1, 2, . . . , N 3l .", "formula_coordinates": [15.0, 187.17, 646.73, 73.14, 10.48]}, {"formula_id": "formula_36", "formula_text": "(\u00b5)", "formula_coordinates": [15.0, 107.72, 671.47, 9.43, 6.28]}, {"formula_id": "formula_37", "formula_text": "Q 3r (W |W : I) = \u03c9 3r (W ) N 3r \u00b5=1 \u03c9 (\u00b5) 3r , Q 3l (W |W : I) = \u03c9 3l (W ) N 3l", "formula_coordinates": [15.0, 358.95, 410.44, 114.37, 51.13]}, {"formula_id": "formula_38", "formula_text": "p(W | I) p(W | I) = p I R i | \u03b6 i , L i , i p I R j | \u03b6 j , L j , j p(I R k | \u03b6 k , L k , k ) \u2022 p(\u03b6 i , L i , i ) p(\u03b6 j , L j , j ) p(\u03b6 k , L k , k ) \u2022 p(K + 1) p(k)", "formula_coordinates": [15.0, 312.81, 526.48, 209.46, 52.57]}, {"formula_id": "formula_39", "formula_text": "p(W | I) p(W | I) and p(W | I) p(W | I) by: \u03c9 (\u00b5) 3r = q(R i , R j ) p I R k | \u03b6 k , L k , k \u2022 [q(L i )q(\u03b6 i , i )][q(L j )q(\u03b6 j , j )] p(\u03b6 k , L k , k ) . (13", "formula_coordinates": [15.0, 311.36, 598.58, 209.05, 78.98]}, {"formula_id": "formula_40", "formula_text": ")", "formula_coordinates": [15.0, 520.41, 660.67, 4.15, 8.97]}, {"formula_id": "formula_41", "formula_text": "\u03c9 (\u03bd) 3l = q(R i , R j ) p I R i | \u03b6 i , L i , i p I R j | \u03b6 j , L j , j \u2022 q(L k )q(\u03b6 k , k ) p(\u03b6 i , L i , i ) p(\u03b6 j , L j , j ) , (14", "formula_coordinates": [15.0, 328.93, 679.78, 191.48, 53.35]}, {"formula_id": "formula_42", "formula_text": ")", "formula_coordinates": [15.0, 520.41, 716.24, 4.15, 8.97]}, {"formula_id": "formula_43", "formula_text": "Figure", "formula_coordinates": [16.0, 70.26, 298.28, 21.04, 7.17]}, {"formula_id": "formula_44", "formula_text": "i .", "formula_coordinates": [16.0, 78.2, 722.23, 5.6, 9.81]}, {"formula_id": "formula_45", "formula_text": "W = ((\u03b6 i , L i , i ), W \u2212 ) W = ((\u03b6 i , L i , i ), W \u2212 )", "formula_coordinates": [16.0, 310.63, 581.48, 206.93, 11.67]}, {"formula_id": "formula_46", "formula_text": "p(W | I) p(W | I) = p I R i \u03b6 i , L i , i p(\u03b6 i , L i , i ) p I R i \u03b6 i , L i , i p(\u03b6 i , L i , i ) .", "formula_coordinates": [16.0, 326.53, 644.44, 177.23, 26.26]}, {"formula_id": "formula_47", "formula_text": "\u03c9 (\u00b5) 4 = q(L i )q(\u03b6 i , i ) p I R i | \u03b6 i , L i , i p(\u03b6 i , L i , i ) ,", "formula_coordinates": [16.0, 335.24, 707.45, 157.46, 25.71]}, {"formula_id": "formula_48", "formula_text": "Q 4 (W | W : I) = \u03c9 4 (W ) N 4 \u00b5=1 \u03c9 (\u00b5) 4", "formula_coordinates": [17.0, 70.26, 161.4, 103.43, 18.57]}, {"formula_id": "formula_49", "formula_text": "(\u03b1 * Ada , Tst * Ada ) = arg min Tst Ada \u2282 Ada arg min \u03b1 Ada M i=1 exp \u2212 i <\u03b1 Ada ,Tst Ada (I i )>. (16", "formula_coordinates": [17.0, 77.48, 589.03, 204.58, 58.36]}, {"formula_id": "formula_50", "formula_text": ")", "formula_coordinates": [17.0, 281.31, 638.42, 4.15, 8.97]}, {"formula_id": "formula_51", "formula_text": "= sign q( = +1 | I) q( = \u22121 | I) .", "formula_coordinates": [17.0, 391.84, 279.86, 99.03, 24.02]}, {"formula_id": "formula_52", "formula_text": "T b (v) = \u00b5 I r (v) + k \u2022 std(I r (v) ,", "formula_coordinates": [20.0, 110.89, 659.77, 128.2, 10.48]}, {"formula_id": "formula_53", "formula_text": "\u00b5 t+1 (W t+1 ) = W t \u00b5 t (W t )K a (W t+1 | W t ). (17)", "formula_coordinates": [24.0, 332.52, 718.38, 192.04, 20.52]}, {"formula_id": "formula_54", "formula_text": "\u00b5(W t , W t+1 ) = \u00b5 t (W t )K a (W t+1 | W t ) = \u00b5 t+1 (W t+1 ) p MC (W t | W t+1 ), (18", "formula_coordinates": [25.0, 81.98, 178.9, 199.33, 26.65]}, {"formula_id": "formula_55", "formula_text": ")", "formula_coordinates": [25.0, 281.31, 195.51, 4.15, 8.97]}, {"formula_id": "formula_56", "formula_text": "p(W t , W t+1 ) = p(W t )K a (W t+1 | W t ) = p(W t+1 )K a (W t+1 , W t ), (19)", "formula_coordinates": [25.0, 100.7, 345.89, 184.76, 26.42]}, {"formula_id": "formula_57", "formula_text": "KL( p(W t , W t+1 ) \u00b5(W t , W t+1 )) = W t+1 W t p(W t , W t+1 ) log p(W t , W t+1 ) \u00b5(W t , W t+1 ) 1 = W t p(W t ) W t+1 K a (W t+1 | W t )) \u00d7 log p(W t ) \u2022 K a (W t+1 | W t ) \u00b5 t (W t )K a (W t+1 | W t ) = K L( p(W ) \u00b5 t (W )) (20) 2 = W t+1 W t K a (W t | W t+1 ) p(W t+1 ) \u00d7 log p(W t+1 )K a (W t | W t+1 ) \u00b5 t+1 (W t+1 ) p MC (W t | W t+1 ) = K L( p(W t+1 ) \u00b5 t+1 (W )) + E p(W t+1 ) [K L(K a (W t | W t+1 ) p MC (W t | W t+1 ))] (21)", "formula_coordinates": [25.0, 70.26, 518.12, 215.62, 211.31]}, {"formula_id": "formula_58", "formula_text": "K L( p(W ) \u00b5 t (W )) \u2212 K L( p(W ) \u00b5 t+1 (W )) = E p(W t+1 ) K L(K a(t) (W t | W t+1 ) p MC (W t | W t+1 ))", "formula_coordinates": [25.0, 309.96, 168.63, 206.61, 26.7]}, {"formula_id": "formula_59", "formula_text": "1 min{ p(W ), q(W )} \u2264 E[\u03c4 (W )] \u2264 1 min{ p(W ), q(W )} \u2022 1 1 \u2212 p \u2212 q TV (24", "formula_coordinates": [26.0, 77.44, 451.83, 210.48, 50.62]}, {"formula_id": "formula_60", "formula_text": ")", "formula_coordinates": [26.0, 284.97, 485.57, 4.15, 8.97]}, {"formula_id": "formula_61", "formula_text": "Q(W, W ) = n i=1 q(i)Q i (W, W ),(25)", "formula_coordinates": [26.0, 347.46, 516.02, 177.1, 28.79]}, {"formula_id": "formula_62", "formula_text": "\u03b1(W, W ) = min 1 Q(W , W ) p(W ) Q(W, W ) p(W )", "formula_coordinates": [26.0, 335.08, 583.5, 149.87, 23.69]}, {"formula_id": "formula_63", "formula_text": "K(W, W ) \u2265 n i=1 q(i)K i (W , W ), \u2200 W = W.", "formula_coordinates": [27.0, 87.45, 241.28, 180.82, 28.79]}], "doi": ""}