{"title": "On the Practical Ability of Recurrent Neural Networks to Recognize Hierarchical Languages", "authors": "Satwik Bhattamishra; \u2660 Kabir; Navin Goyal; Microsoft Research", "pub_date": "", "abstract": "While recurrent models have been effective in NLP tasks, their performance on context-free languages (CFLs) has been found to be quite weak. Given that CFLs are believed to capture important phenomena such as hierarchical structure in natural languages, this discrepancy in performance calls for an explanation. We study the performance of recurrent models on Dyck-n languages, a particularly important and well-studied class of CFLs. We find that while recurrent models generalize nearly perfectly if the lengths of the training and test strings are from the same range, they perform poorly if the test strings are longer. At the same time, we observe that recurrent models are expressive enough to recognize Dyck words of arbitrary lengths in finite precision if their depths are bounded. Hence, we evaluate our models on samples generated from Dyck languages with bounded depth and find that they are indeed able to generalize to much higher lengths. Since natural language datasets have nested dependencies of bounded depth, this may help explain why they perform well in modeling hierarchical dependencies in natural language data despite prior works indicating poor generalization performance on Dyck languages. We perform probing studies to support our results and provide comparisons with Transformers.", "sections": [{"heading": "Introduction", "text": "Recurrent models (RNNs and more specifically LSTMs) have been used extensively across several NLP tasks such as machine translation (Sutskever et al., 2014), language modeling (Melis et al., 2017) and question answering (Seo et al., 2016). Natural languages involve phenomena such as hierarchical and long-distance dependencies. Although RNNs are known to be Turing-complete (Siegelmann and Sontag, 1992) given unbounded precision, their practical ability to model such phenomena remains unclear.\nRecently, several works (Weiss et al., 2018;Sennhauser and Berwick, 2018;Skachkova et al., 2018) have attempted to understand the capabilities of LSTMs by empirically analyzing them on different types of formal languages. Natural languages, for the most part, can be modeled by context-free languages (Jger and Rogers, 2012) and their hierarchical structure has been emphasized by Chomsky (2002). Thus studying the capabilities of RNNs in recognizing context-free languages (CFLs) can shed light on how well they can model hierarchical structures. An important family of context-free languages is the Dyck-n language 1 .\nPrevious works (Suzgun et al., 2019a;Suzgun et al., 2019c;Yu et al., 2019) showed that LSTMs achieve limited generalization performance on recognizing Dyck-2. This prompted the development of memory-augmented variants (Joulin and Mikolov, 2015;Suzgun et al., 2019c) of LSTMs which generalize well on Dyck languages but are notoriously hard to train and fail to perform well on practical NLP tasks (Shen et al., 2019). On the other hand, despite the limited performance of LSTMs on Dyck languages, several studies (Gulordava et al., 2018;Tran et al., 2018) have found that LSTMs perform well in modeling hierarchical structure in natural language data. In this work, we take a step towards bridging this gap.\nThey say all the prayers those priests preach, and the boy hears were written centuries ago.\n[ ] ( [ ( ) ] ) [ ( ) ] Our Contributions. We investigate the ability of recurrent models to learn and generalize on Dyck languages. We first evaluate the ability of LSTMs to recognize randomly sampled Dyck-n sequences and find, in contrast to prior works (Suzgun et al., 2019a;Suzgun et al., 2019c), that they can generalize nearly perfectly when the test samples are within the same lengths as seen during training. Similar to prior works, when evaluated on randomly generated samples of higher lengths we observe limited performance. Dyck languages and (deterministic) CFLs can be recognized by (deterministic) pushdown automata (PDA). We construct an RNN that directly simulates a PDA given unbounded precision. A key observation is that the higher the depth of the stack the higher is the required precision. This implies that fixed precision RNNs are expressive enough to recognize strings of arbitrary lengths if the required depth of the stack is bounded. Based on this observation, we test the hypothesis whether LSTMs can generalize to higher lengths if the depth of the inputs in the training and test set is bounded by the same value. In the bounded depth setting, LSTMs are able to generalize to much higher lengths compared to the lengths used during training. Given that natural languages in practical settings also contain nested dependencies of bounded depths (Gibson, 1991;McElree, 2001), this may help explain why LSTMs perform well in modeling natural language corpora containing nested dependencies. We then assess the generalization performance of the model across higher depths and find that although LSTMs can generalize to a certain extent, their performance degrades gradually with increasing depths. Since Transformer (Vaswani et al., 2017) is also a dominant model in NLP (Devlin et al., 2019), we include it in our experiments. To our knowledge, prior works have not empirically analyzed Transformers on formal languages, particularly context-free languages. We further conduct robustness experiments and probing studies to support our results.", "publication_ref": ["b25", "b15", "b20", "b31", "b19", "b23", "b11", "b2", "b26", "b28", "b32", "b10", "b28", "b21", "b6", "b29", "b26", "b28", "b5", "b14", "b30", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries and Motivation", "text": "The language Dyck-n parameterized by n \u2265 1 consists of well-formed words from n types of parentheses. Its derivation rules are: S \u2192 ( i S) i , S \u2192 SS, S \u2192 where i \u2208 {1, . . . , n}. Dyck-n is context-free for every n, Dyck-2 being crucial among them, since all Dyck-n for n > 2 can be reduced to Dyck-2 (Suzgun et al., 2019a). For words in Dyck-n, the required depth of the stack in its underlying PDA is the maximum number of unbalanced parentheses in a prefix. For instance, the word ( [ ( ) ] ) [ ] in Dyck-2 has maximum depth of 3 corrsponding to the prefix ( [ (.\nRNNs. RNNs are defined by the update rule h t = f (h t\u22121 , x t ), where the function f could be a feedforward network, h t is the model's memory vector usually referred to as the hidden state vector and x t denotes the input vector at the t-th step. In practice, f is usually a single layer feedforward network with tanh or ReLU activation. To mitigate the vanishing gradients problem, LSTMs (Hochreiter and Schmidhuber, 1997), a variant of RNNs with additional gates, is most commonly used in practice. In our experiments, we will primarily work with LSTMs.", "publication_ref": ["b26", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Expressiveness Results", "text": "Proposition 2.1. Any Deterministic Pushdown Automaton can be simulated by an RNN with ReLU activation.\nWe provide a proof by construction for the above result in Appendix B by using the Cantor-set like   [52,100] respectively without any restriction on the depth of the Dyck words. Our second setting also resembles the previous dataset in terms of lengths of the strings in the training and validation sets. However, in this case, we restrict all the strings to have depths in the range [1,10]. This is to test the generalization ability across lengths when the depth is bounded. In the third case, we test the generalization ability across depths, when the lengths in train and validation sets are in the same interval [2,100]. Along with LSTMs, we also report the performance of Transformers (as used in GPT (Radford et al., 2018)) on each task since it is also a dominant model in NLP Tasks. We train and evaluate our models on the Next Character Prediction Task (NCP) introduced in Gers and Schmidhuber (2001) and used in Suzgun et al. (2019a) and Suzgun et al. (2019c). Similar to an LM setup, the model is only presented with positive samples from a given language. In NCP, for a sequence of symbols s 1 , s 2 , . . . , s n , the model is presented with the sequence s 1 , s 2 , . . . , s i at the i th step and the goal of the model is to predict the set of valid characters for the (i + 1) th step, which can be represented as a k-hot vector. The model assigns a probability to each character in the vocabulary corresponding to its validity in the next step, which is achieved by applying sigmoid activation over the unnormalized scores predicted through its output layer. Following Suzgun et al. (2019b) and Suzgun et al. (2019a), we use mean squared error between the predicted probabilities and k-hot labels as the loss function. During inference, we use a threshold of 0.5 to obtain the final prediction. The models prediction is considered to be correct if and only if its output at every step is correct. The accuracy of the model over test samples is the fraction of total samples which are predicted correctly. Note that, this is a relatively stringent metric as a correct prediction is obtained only when the model's output is correct at every step as opposed to standard classification tasks. Refer to Bhattamishra et al. (2020) for a discussion on the choice of character prediction task and its relation with other tasks such as standard classification and language modeling. Details of the dataset and parameters relevant for reproducibility can be found in section C in Appendix. We have made our source code available at https://github.com/satwik77/RNNs-Context-Free.", "publication_ref": ["b18", "b26", "b28", "b27", "b26", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Table 1 shows the performance of LSTMs and Transformers on the considered languages. When inputs are randomly sampled in a given range of lengths, LSTMs can generalize well on the validation set containing inputs of the same lengths as seen during training (Bin-1A) 3 , for all considered Dycks. However, for higher lengths (Bin-2A), it struggles to generalize on these languages. For the case when the depth is bounded, LSTMs generalize very well to much higher lengths (Bin 1B, 2B, and 3B). Transformers, on the other hand, fail to generalize to higher lengths in either case, which might be attributed to the fact that at test time, it receives positional encodings that it was not trained on. To investigate the generalization ability of models across increasing depths, we trained the models up to depth 15 and evaluated on 5 validation sets with an incremental increase in depth in each set (refer to Figure 2). We found that the models were able to generalize up to a certain extent but their performance degraded gradually as we increase the depth. However, Transformers performed relatively better as compared to LSTMs. Probing. We also conducted probing experiments on LSTMs to better understand how they perform these particular tasks. We first attempt to extract the depth of the underlying stack from the cell state of an LSTM model trained to recognize Dyck-2. We found that a single layer feedforward network was easily able to extract the depth with perfect (100%) accuracy and generalize to unseen data. Figure 3a shows a visualization of t-SNE projection of the hidden state labeled by their corresponding depths. Additionally, we also try to extract the stack elements from the hidden states of the network. For Dyck-2 samples within lengths [2,50] and depths [1,10], along with training LSTM for the NCP task, we co-train auxiliary classifiers to predict each element of the stack up to depth 10, i.e. the hidden state of the LSTM that is used to predict the next set of valid characters is now also utilized in parallel to predict (by supplying 10 separate linear layers for each element) the elements of the stack. We find that not only was the model able to predict the elements in a validation set from the same distribution, it was also able to extract the elements for sequences of higher lengths ([52,150]) on which it was not trained on (see Figure 3b). This further provides evidence to show that LSTMs are able to robustly generalize to inputs of higher lengths when their depths are bounded. We also conducted a few additional robustness experiments to ensure that the model does not overfit on training distribution. Details of probing tasks as well as additional robustness experiments can be found in the Appendix.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": ["tab_1"]}, {"heading": "Discussion", "text": "LSTMs and Transformers have been effective on language modeling tasks. In practice, during language modeling on a natural language corpus, the entire input is fed sequentially to the LSTM. Hence, the length of the input processed by the LSTM is bound to be large, requiring it to model a number of Figure 3 nested dependencies. Prior works in psycholinguistics (Gibson, 1991;McElree, 2001) have pointed out that given limited working memory of humans, natural language as used in practice should have nested dependencies of bounded depth. Some works (Jin et al., 2018;Noji et al., 2016) have even sought to build parsers for natural language with depth-bounded PCFGs. Our generalization results for LSTMs on depth-bounded CFGs could help explain why they perform well on modeling hierarchical dependencies in natural language datasets. For Transformer, although it did not generalize to higher lengths, but in practice Transformers (as used in BERT and GPT) process inputs in a fixed-length context window. Our results indicate that it does not have trouble in generalizing when the train and validation sets contain inputs of the same lengths.\nOur experiments also demonstrate that the limiting factor in LSTMs is in generalizing to higher depths as opposed to its memory-augmented variants. The exact mechanism with which trained LSTMs perform the task is not entirely clear. The limited performance of LSTMs could be due to precision issues or unstable stack encoding mechanism (Stogin et al., 2020). However, given that natural language datasets are likely to have nested dependencies of bounded depth, this limitation may not play a significant role and may help explain why prior works (Gulordava et al., 2018;Tran et al., 2018) have found LSTMs to perform well in modeling hierarchical structure on natural language datasets.\nA Preliminaries Definition A.1 (Deterministic Pushdown Automata (Hopcroft et al., 2006)). A DPDA is a 7-tuple \u03a3, Q, \u0393, q 0 , Z 0 , \u03b4, F with 1. A finite alphabet \u03a3 2. A finite set of states Q 3. A finite stack alphabet 4. An initial state q 0\n5. An initial stack symbol Z 0 6. F \u2286 Q set of accepting states 7. A state transition function\n\u03b4 : \u03a3 \u00d7 Q \u00d7 \u0393 \u2192 (q, \u03b3)\nThe output of \u03b4 is a finite set of pairs (q, \u03b3), where q is a new state and \u03b3 is the string of stack symbols that replaces the top of the stack. If X is at the top of the stack, then \u03b3 = indicates that the stack is popped, \u03b3 = X denotes that the stack is unchanged and if \u03b3 = Y Z, then X is replaced by Z, and Y is pushed onto the stack.\nA machine processes an input string x \u2208 \u03a3 * one token at a time. For each input token, the machine looks at the current input, state and top of the stack to make a transition into a new state and update its stack. The machine can also take empty string as input and make a transition based on the stack and its current state. The machine halts after reading the input and a string is accepted if the state after reading the complete input is a final state.", "publication_ref": ["b5", "b14", "b9", "b16", "b24", "b6", "b29", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "A.1 Cantor-set Encodings", "text": "In our construction, we will make use of Cantor-set like encodings as introduced in (Siegelmann and Sontag, 1992). The Cantor-set like encodings in base-4 provides us a means to encode a stack of values 0s and 1s and easily apply stack operations like push, pop and top to update them. Let \u03c5 s denote the encoding of a stack. The contents of the stack can be viewed as a rational number p 4 q where 0 < p < 4 q . The i-th element from the top of the stack can be seen as the i-th element to the right of the decimal point the in a base-4 expansion. A 0 stored in the stacked is associated with a 1 in the expansion while a 1 stored in the stack is associated with 3. Hence, only numbers of the special form \u03a3 t i=1 a i 4 i where a i \u2208 {1, 3} will appear in the encoding. For inputs of the form I \u2208 {0, 1}, the standard stack operations can be applied by simple affine operations. For instance, push(I) operation can be obtained by \u03c5 s \u2192 1 4 \u03c5 s + 1 2 I + 1 4 and the pop(I) can be obtained by \u03c5 s \u2192 4\u03c5 s \u2212 2I \u2212 1. The top of the stack can be obtained by \u03c3(4\u03c5 s \u2212 2) which will be 1 if the top of the stack is 1 else 0. The emptiness of a stack can be checked by \u03c3(\u03c5 s ) which will be 1 if the stack is nonempty or else it will be 0.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Construction", "text": "We will make use of some intermediate notions to describe our construction. We will use these multiple times in our construction. Particularly, Lemma B.1 will be used to combine the information of the state vector, input and the symbol at the top of the stack. Lemma B.2 and Lemma B.3 will be used to implement the state transition and decisions related to stack operations.\nFor the feed-forward networks we use the activation as in (Siegelmann and Sontag, 1992), namely the saturated linear sigmoid activation function:\n\u03c3(x) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 0 if x < 0, x if 0 \u2264 x \u2264 1, 1 if x > 1.\n(1)\nNote that, we can easily work with the standard ReLU activation via \u03c3(x) = ReLU(x) \u2212 ReLU(x \u2212 1). Consider two sets \u03a6 and \u03a8 (such as the set of states Q and the set of inputs \u03a3), and consider the one-hot representations of their elements \u03c6 \u2208 \u03a6 and \u03c8 \u2208 \u03a8 as \u03c6 \u2208 Q |\u03a6| and \u03c8 \u2208 Q |\u03a8| respectively. We use (\u03c6, \u03c8) \u2208 Q |\u03a6|\u00d7|\u03a8| to denote a unique one-hot encoding for each pair of \u03c6 and \u03c8. More specifically, consider the enumerations \u03c0 \u03a6 : \u03a6 \u2192 {1, 2, . . . , |\u03a6|} and \u03c0 \u03a8 : \u03a8 \u2192 {1, 2, . . . , |\u03a8|}. Then given two elements \u03c6 \u2208 \u03a6 and \u03c8 \u2208 \u03a8, the vector (\u03c6, \u03c8) \u2208 Q |\u03a6|\u00d7|\u03a8| will have a 1 in position (\u03c0 \u03a8 (\u03c8)\u22121)|\u03a6|+\u03c0 \u03a6 (\u03c6) and a 0 in every other position. We now prove that given a vector [\u03c6, \u03c8] containing concatenation of one-hot representations of the elements \u03c6 and \u03c8, there exists a single-layer feedforward network that can produce the vector (\u03c6, \u03c8). Proof. Let A i for i \u2208 {1, . . . , |\u03a6|} denote a matrices of dimensions |\u03a8| \u00d7 |\u03a6| such that A i has 1s in its i-th row and 0 everywhere else. For any one-hot vector \u03c6, note that \u03c6A i = 1 if i = \u03c0 \u03a6 (\u03c6) or else it is 0. Thus, consider the transformation,\nt (\u03c6,\u03c8) = [ \u03c6 + \u03c8A 1 , \u03c6 + \u03c8A 2 , . . . , \u03c6 + \u03c8A |\u03a8| ]\nNote that, the vector t (\u03c6,\u03c8) has a value 2 exactly at the position (\u03c0 \u03a8 (\u03c8) \u2212 1)|\u03a6| + \u03c0 \u03a6 (\u03c6) and it is either 0 or 1 at the rest of the positions. Hence by making use of bias vectors, it is easy to obtain [(\u03c6, \u03c8)],\n\u03c3(t (\u03c6,\u03c8) \u2212 1) = [(\u03c6, \u03c8)]\nwhich is what we wanted to show.\nAs an example, consider two sets \u03a6 and \u03a8 such that |\u03a6| = 3 and |\u03a8| = 2. Consider two elements \u03c6 \u2208 \u03a6 and \u03c8 \u2208 \u03a8 such that \u03c0 \u03a6 (\u03c6) = 3 and \u03c0 \u03a8 (\u03c8) = 2. Hence, this implies the corresponding one-hot vectors \u03c6 = [0, 0, 1] and \u03c8 = [0, 1]. According to the construction above, the weight matrix will be,\nW = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 1 1 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb ,\nThus by construction, the output of the feedforward network \u03c3([\u03c6, \u03c8]W \u2212 1) = [0, 0, 0, 0, 0, 1]. A similar technique was employed by (Prez et al., 2019) in their Turing completeness result for Transformer.\nWe will describe another technical lemma that we will make use of to implement our transition functions and other such mappings. Consider two sets \u03a6 and \u03a8 (such as the set of states Q and the set of inputs \u03a3 or set of stack symbols \u0393), and consider the one-hot representations of their elements \u03c6 \u2208 \u03a6 and \u03c8 \u2208 \u03a8 as \u03c6 \u2208 Q |\u03a6| and \u03c8 \u2208 Q |\u03a8| respectively. Let \u03b4 : \u03a6 \u00d7 \u03a8 \u2192 \u03a6 be any transition function that takes elements of two sets as input and produces an element of one of the sets. Let [(\u03c6, \u03c8)] \u2208 Q |\u03a6|\u00d7|\u03a8| denote a unique one-hot encoding for each pair of \u03c6 and \u03c8 as defined earlier. We demonstrate that given [(\u03c6 1 , \u03c8)] as input, there exists a single layer feedforward network that produces the vector\n\u03c6 2 if \u03b4(\u03c6 1 , \u03c8) = \u03c6 2 . Lemma B.2. There exists a function O(x) : Q |\u03a6||\u03a8| \u2192 Q |\u03a6| of the form \u03c3(xW + b) such that, O([(\u03c6 1 , \u03c8)]) = \u03c6 2\nProof. This can easily implemented using a linear transformation. Consider a matrix A \u2208 Q |\u03a6||\u03a8|\u00d7|\u03a6| . Given two inputs \u03c6 i , \u03c6 k \u2208 \u03a6 and \u03c8 j \u2208 \u03a8 such that \u03b4(\u03c6 i , \u03c8 j ) = \u03c6 k , the row (\u03c0 \u03a8 (\u03c8 j ) \u2212 1)|\u03a6| + \u03c0 \u03a6 (\u03c6 i ) of the matrix A will be the one-hot vector corresponding to \u03c6 k , that is,\nA (\u03c0 \u03a8 (\u03c8 j )\u22121)|\u03a6|+\u03c0 \u03a6 (\u03c6 i ),: = \u03c6 k .\nSimilarly, a mapping \u03b8 : \u03a6 \u00d7 \u03a8 \u2192 {0, 1} n can also be implemented using linear transformation using a transformation matrix A \u2208 Q |\u03a6||\u03a8|\u00d7n where each row of the matrix A will be the corresponding mapping {0, 1} n for the pair of (\u03c6, \u03c8) corresponding to that row.", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}, {"heading": "Lemma B.3. There exists a function", "text": "O(x) : Q |\u03a6||\u03a8| \u2192 {0, 1} n of the form \u03c3(xW + b) such that, O([(\u03c6, \u03c8)]) = [{0, 1} n ]\nProof is similar to proof of Lemma B.2. Proposition B.4. For any Deterministic Pushdown Automaton, there exists an RNN that can simulate it.\nProof. The construction is straightforward and follows by induction. We will show that at the t-th timestep, given that the model has information about the state and a representation of the stack, the model can compute the next state and update the stack representation based on the input. More formally, given a sequence x 1 , x 2 , . . . , x n \u2208 \u03a3 * , consider that the hidden state vector at the t-th timestep is h t = [q t , \u03c9 t ] where q t \u2208 Q |\u03a3| is a one-hot encoding of the state vector and \u03c9 t \u2208 Q |\u0393| is a representation of the stack based on the cantor-set like encoding. Then, given an input x t+1 \u2208 \u03a3, we will show how the network can compute h t+1 = [q t+1 , \u03c9 t+1 ]. After reading the whole input, a sequence is accepted if q n is in the set of final states F or else it is rejected.\nOur construction will use a 5-layer feed forward network that takes as input the vectors h t\u22121 and x t at each timestep and produces the vector h t . The vectors h t \u2208 Q |Q|+|\u0393| will have two subvectors of size |Q| and |\u0393| containing a one-hot representation of the state of the underlying automaton and a representation of the stack encoded in the Cantor-set representation respectively. For each input symbol x \u2208 \u03a3, its corresponding input vector x \u2208 Q |\u03a3| will be a one-hot vector. If the underlying stack takes empty string as input at particular step, the RNN will take a special symbol as input which also have a unique one-hot representation similar to other input symbols.\nAs opposed to the construction of Siegelmann and Sontag (1992), which only takes 0s and 1s as input and use a scalar to encode a stack of 0s and 1s, we will encode one-hot representation of stack symbols in vectors of size |\u0393|. The push and pop operations will always be in the form of one-hot vectors and this will ensure that retrieving the top element provides a one-hot encoding of the stack symbol.\nDetails of the construction. At each timestep, the model will receive the hidden state vector h t\u22121 = [q t\u22121 , \u03c9 t\u22121 ] and the input vector x t as input. At timestep 0, the hidden state vector will be h 0 = [q 0 , \u03c9 0 ] containing the one-hot representation of the initial state and stack encoding containing Z 0 . For instance consider |Q| = 3 and |\u0393| = 4. If the one-hot encoding of q 0 is q 0 = [1, 0, 0] and one-hot encoding of Z 0 is z 0 = [1, 0, 0, 0], then \u03c9 0 = 1 4 0 + 1 2 [1, 0, 0, 0] + 1 4 = [3/4, 1/4, 1/4, 1/4]. That is, the vector z 0 is pushed to the empty stack using the Cantor-set encoding method described in section A.1. Hence, the vector h 0 = [1, 0, 0, 3/4, 1/4, 1/4, 1/4].\nThe first layer of the feedforward network \u03c3(W h h t\u22121 + W x x t + b) will produce the vector h\n(1)\nt\u22121 = [(q t\u22121 , x t ), \u03c4 top t\u22121 , \u03c9 t\u22121 ]\n, where \u03c4 top t\u22121 \u2208 Q |\u0393| denotes a one-hot vector representation of the symbol at the top of our stack representation and the subvector (q t\u22121 , x t ) \u2208 Q |Q|\u00d7|\u03a3| is a unique one-hot vector for each pair of state q and input x. Thus, the vector h\n(1) t\u22121 is of dimension |Q|.|\u03a3| + 2|\u0393|. The vector (q t\u22121 , x t ) can be obtained by using Lemma B.1 where \u03a6 = Q and \u03a8 = \u03a3. The vector corresponding to the symbol at the top of the stack can be easily obtained using the top operation (\u03c3(4\u03c9 t\u22121 \u2212 2)) defined in section A.1.\nIn the second layer, we will use Lemma B.1 again to obtain a unique one-hot vector for each combination of the state, input and stack symbol. The output of the second layer of the feedforward network will be of the form h\n(2) t\u22121 = [(q t\u22121 , x t , \u03c4 top t\u22121 ), \u03c4 top t\u22121 , \u03c9 t\u22121 ]\n, where the subvector (q t\u22121 , x t , \u03c4 top t\u22121 ) is a unique one-hot encoding for each combination of the state q \u2208 Q, input x \u2208 \u03a3 and a stack symbol \u03c4 \u2208 \u0393. Hence, the vector h\n(2) t\u22121 will be of the dimension |Q|.|\u03a3|.|\u0393|+2|\u0393|. Since we already had the vector (q t\u22121 , x t ), and the vector \u03c4 top t\u22121 , the vector (q t\u22121 , x t , \u03c4 top t\u22121 ) can be obtained using Lemma B.1 by considering \u03a6 = Q \u00d7 \u03a3 and \u03a8 = \u0393. The primary idea is that the vector (q t\u22121 , x t , \u03c4 top t\u22121 ) provides us with all the necessary information required to implement the further steps and produce q t and \u03c9 t .  process at that point. We run all of our experiments on 4 Nvidia Tesla P100 GPUs each containing 16GB memory.\nProbing Details For designing a probe for extracting the depth of the underlying stack of Dyck-2 substrings from the cell states of pretrained LSTMs, we used a single hidden layer Feed-Forward network. The hidden size of the network was kept 32 and it was trained using Adam Optimizer (Kingma and Ba, 2014) with a batch size of 200. The accuracy on the validation set was computed by only considering the predictions to be correct for a sequence if it predicted the correct depth at every step of that sequence. In the second set of experiments that aimed to predict the elements of the stack, we trained the LSTM model on the NCP task along with an auxillary loss for predicting the top-10 elements of the stack. For the computation of the auxillary loss, we added 10 parallel linear layers on the top of the LSTM's output with i-th linear layer tasked to predict if the i-th element of the stack was i) a round opening bracket or ii) a square opening bracket or iii) If no element was present at that position. For each of these 10 linear layers we compute Cross Entropy Loss which are then averaged to obtain the auxillary loss. The final loss is computed as:\nL = L N CP + \u03bbL aux (5)\nwhere L N CP is the loss obtained from the next character prediction task and L aux is the auxillary loss just described and we use \u03bb = 1 20 in our experiments. The stack extraction auxillary task is evaluated by computing Accuracy and Recall metrics for each stack element. The accuracy for the i-th element is computed by considering if the model can predict the i-th element correctly at each step of a sequence. Since there will be a fewer cases for smaller lengths containing elements at higher depths, we also report Recall for each i-th stack element, where we only consider if the model can correctly predict the sequences containing at least one occurrence of depth i.", "publication_ref": ["b22"], "figure_ref": [], "table_ref": []}, {"heading": "D Robustness Experiments", "text": "To ensure that our results didn't overfit on the training distribution we did some robustness experiments to check efficacy of the considered neural models. As a reminder, the PCFG for Dyck-n languages is given by the following derivation rules: S \u2192 ( i S) i , with probability p (6) S \u2192 SS, with probabililty q (7) S \u2192 , with probbaility 1 \u2212 (p + q) (8)\nFor the experiments described in the main paper we used p = 0.5 and q = 0.25. To check the generalization ability of our models we checked whether a model trained with the strings generated using these values can generalize on Dyck words obtained from a different distribution. Table 4 shows the accuracies obtained by LSTMs and Transformers on data generated from different distributions. It can be observed from the results that the performance of the models for all languages remain more or less the same across different distributions.\nA model trained on the Next Character Prediction task can also be used to generate strings of the language it was trained on by starting from an empty string and then exhaustively iterating over all possible valid characters predicted by the model. We used this idea to check if a pretrained LSTM model can indeed generate all possible Dyck-2 strings upto a certain length (since the number of possible strings will grow exponentially with increasing lengths). For a maximum length of 10, there exists a total of 1619 valid Dyck-2 strings. When we used a pretrained model to exhaustively generate the valid strings, we observed that it produced exactly those 1619 strings, no more and no less.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "E Additional Results", "text": "The depth generalization results for the neural models on Dyck-3 and Dyck-4, are given in Figure 4. Similar to Dyck-2, here also we see a gradual drop in performance as we move to the higher depths but Transformers perform relatively better than LSTMs. The lengths of strings in the training set and all validation sets were fixed to lie between 2 to 100.", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Acknowledgements", "text": "We thank the anonymous reviewers for their constructive comments and suggestions. We would also like to thank our colleagues at Microsoft Research and Michael Hahn for their valuable feedback and helpful discussions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "We can use the vector (q t\u22121 , x t , \u03c4 top t\u22121 ) to directly map to q t using a simple linear transformation. To obtain \u03c9 t , we will produce all three candidate stack representations corresponding to push, pop and no-operation. That is, from (q t\u22121 , x t , \u03c4 top t\u22121 ) we will obtain \u03c9 push t , \u03c9 pop t and \u03c9 no\u2212op t . Along with that, using linear transformation we will obtain three control signals c push , c pop and c no\u2212op . To obtain the final stack representation \u03c9 t , we will implement the following operation\n(2)\nWe will implement the above steps using an additional three layers of feedforward network and thus we will obtain h t = [q t , \u03c9 t ].\nThe third layer of the feedforward network will produce the vector h\n(3)\nThe vector q t in h\n(3)\nt\u22121 can be obtained using Lemma B.2 given the vector\nt\u22121 . The vector \u03c9 pop t via the Cantor set encoding method using the transformation 4\u03c9 t \u2212 2\u03c4 top t\u22121 \u2212 1 over h\n(2)\nt\u22121 . The vector \u03c9 no\u2212op t can be obtained using the Identity Transformation. The vector \u03c4 push t\u22121 can be obtained using Lemma B.2. If for a given transition the stack operation is not a push operation then \u03c4 push t\u22121 = 0. The vector c push = 1 if the current stack operation is Push and it is 0 otherwise. Similarly, the vectors c pop and c no\u2212op are 1 if the current operation is Pop or No-operation respectively and are 0 otherwise. The vectors c push , c pop , and c no\u2212op can be obtained using Lemma B.3. During any timestep, only one of them is 1 vector and rest of them are zero vectors. The vectors c op s can be seen as control signals. That is, the candidate stack representation will be used if its corresponding control signal is 1 or else the candidate stack representation will be transformed to zero vector.\nIn the fourth layer, the feedforward network will produce the following vector, h\nFor any operation denoted by op \u2208 {push, pop, no \u2212 op}, the vector\nSince we are using first order RNNs and hence multiplicative operations are not directly possible. To implement the operation in equation 3 via linear transformations with saturation linear sigmoid activation, first note that 0 \u2264 \u03c9 t \u2264 1 and thus \u03c3(\u03c9 t \u2212 1) = 0. Using that we can implement the operation in equation 3 by,\nIn the fourth layer of the feedforward network we obtain the candidate stack representation for Push operation via linear operations as described in section A.1 along with adding c push and adding \u22121 via bias vectors. For the candidate stack representations of Pop and No-op operations, we simply add their control vectors and subtract by 1 via bias vectors. In the vector h The fifth layer will simply sum the three candidate stack vectors along with their control signals to obtain, h   which is what we wanted to show.\nThe above construction is a direct simulation of Deterministic Pushdown Automaton via RNNs in real-time. The construction of Siegelmann and Sontag (1992) first takes all the inputs and then takes further processing time. However, in practice, RNNs process the inputs and produce outputs in real-time.\nNote that, the dependence on precision is primarily determined by the depth of the stack. That is, as the number of elements in the stack keep increasing, the stack representation gets exponentially smaller due to the Cantor-set encoding scheme. If for a set of input strings, the depths are bounded, then for some finite precision, an RNN based on the above construction will be able to recognize strings of arbitrary lengths.", "publication_ref": ["b22"], "figure_ref": [], "table_ref": []}, {"heading": "C Experimental Details", "text": "We run our experiments on three Context Free Languages namely Dyck-2, Dyck-3 and Dyck-4 for LSTM and Transformer based models. Three separate datasets were generated for each language, to run the ablations, details of which are given in Table 2. For each of these experiments we do extensive hyperparameter tuning before reporting the final results. Table 3 provides different hyperparameters considered in our experiments and their bounds. All in all, this resulted in about 56 different settings for each dataset for RNNs and about 144 settings for Transformers. While reporting the final scores we take an average of the accuracies corresponding the top-5 hyperparameter settings.\nAll of our models were trained using RMSProp Optimizer with a smoothing constant \u03b1 of 0.99. For each language and its corresponding datasets, we use a batch size of 32 and train for 100 epochs. In case an accuracy of 0.99 is acheived for all of the bins before completing 100 epochs we stop the training", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "On the ability and limitations of transformers to recognize formal languages", "journal": "", "year": "2020", "authors": "Satwik Bhattamishra; Kabir Ahuja; Navin Goyal"}, {"ref_id": "b1", "title": "The algebraic theory of context-free languages", "journal": "Elsevier", "year": "1959", "authors": "Noam Chomsky;  Sch\u00fctzenberger"}, {"ref_id": "b2", "title": "Syntactic structures", "journal": "Walter de Gruyter", "year": "2002", "authors": "Noam Chomsky"}, {"ref_id": "b3", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Association for Computational Linguistics", "year": "2019-06", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b4", "title": "Lstm recurrent networks learn simple context-free and context-sensitive languages", "journal": "IEEE Transactions on Neural Networks", "year": "2001", "authors": "A Felix; E Gers;  Schmidhuber"}, {"ref_id": "b5", "title": "A computational theory of human linguistic processing: Memory limitations and processing breakdown", "journal": "", "year": "1991", "authors": "Edward Albert Fletcher Gibson"}, {"ref_id": "b6", "title": "Colorless green recurrent networks dream hierarchically", "journal": "Association for Computational Linguistics", "year": "2018-06", "authors": "Kristina Gulordava; Piotr Bojanowski; Edouard Grave; Tal Linzen; Marco Baroni"}, {"ref_id": "b7", "title": "Long short-term memory", "journal": "Neural computation", "year": "1997", "authors": "Sepp Hochreiter; J\u00fcrgen Schmidhuber"}, {"ref_id": "b8", "title": "Introduction to Automata Theory, Languages, and Computation", "journal": "Addison-Wesley Longman Publishing Co., Inc", "year": "2006", "authors": "John E Hopcroft; Rajeev Motwani; Jeffrey D Ullman"}, {"ref_id": "b9", "title": "Unsupervised grammar induction with depth-bounded pcfg", "journal": "Transactions of the Association for Computational Linguistics", "year": "2018", "authors": "Lifeng Jin; Finale Doshi-Velez; Timothy Miller; William Schuler; Lane Schwartz"}, {"ref_id": "b10", "title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "journal": "", "year": "2015", "authors": "Armand Joulin; Tomas Mikolov"}, {"ref_id": "b11", "title": "Formal language theory: refining the Chomsky hierarchy", "journal": "Philosophical Transactions of the Royal Society B: Biological Sciences", "year": "1598", "authors": "Gerhard Jger; James Rogers"}, {"ref_id": "b12", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b13", "title": "On the computational power of rnns", "journal": "", "year": "2019", "authors": "A Samuel; Robert C Korsky;  Berwick"}, {"ref_id": "b14", "title": "Working memory and focal attention", "journal": "Journal of Experimental Psychology: Learning, Memory, and Cognition", "year": "2001", "authors": "Brian Mcelree"}, {"ref_id": "b15", "title": "On the state of the art of evaluation in neural language models", "journal": "", "year": "2017", "authors": "G\u00e1bor Melis; Chris Dyer; Phil Blunsom"}, {"ref_id": "b16", "title": "Using left-corner parsing to encode universal structural constraints in grammar induction", "journal": "", "year": "2016", "authors": "Hiroshi Noji; Yusuke Miyao; Mark Johnson"}, {"ref_id": "b17", "title": "On the Turing completeness of modern neural network architectures", "journal": "", "year": "2019", "authors": "Jorge Prez; Javier Marinkovi; Pablo Barcel"}, {"ref_id": "b18", "title": "Improving language understanding by generative pre-training", "journal": "", "year": "2018", "authors": "Alec Radford; Karthik Narasimhan"}, {"ref_id": "b19", "title": "Evaluating the ability of LSTMs to learn context-free grammars", "journal": "Association for Computational Linguistics", "year": "2018-11", "authors": "Luzi Sennhauser; Robert Berwick"}, {"ref_id": "b20", "title": "Bidirectional attention flow for machine comprehension", "journal": "", "year": "2016", "authors": "Minjoon Seo; Aniruddha Kembhavi; Ali Farhadi; Hannaneh Hajishirzi"}, {"ref_id": "b21", "title": "Ordered memory", "journal": "Curran Associates, Inc", "year": "2019", "authors": "Yikang Shen; Shawn Tan; Arian Hosseini; Zhouhan Lin; Alessandro Sordoni; Aaron C Courville"}, {"ref_id": "b22", "title": "On the computational power of neural nets", "journal": "ACM", "year": "1992", "authors": "T Hava; Eduardo D Siegelmann;  Sontag"}, {"ref_id": "b23", "title": "Closing brackets with recurrent neural networks", "journal": "Association for Computational Linguistics", "year": "2018-11", "authors": "Natalia Skachkova; Thomas Trost; Dietrich Klakow"}, {"ref_id": "b24", "title": "Provably stable interpretable encodings of context free grammars in rnns with a differentiable stack", "journal": "", "year": "2020", "authors": "John Stogin; Ankur Mali; C Lee Giles"}, {"ref_id": "b25", "title": "Sequence to sequence learning with neural networks", "journal": "", "year": "2014", "authors": "Ilya Sutskever; Oriol Vinyals; Quoc V Le"}, {"ref_id": "b26", "title": "LSTM networks can perform dynamic counting", "journal": "Association for Computational Linguistics", "year": "2019-08", "authors": "Mirac Suzgun; Yonatan Belinkov; Stuart Shieber; Sebastian Gehrmann"}, {"ref_id": "b27", "title": "On evaluating the generalization of LSTM models in formal languages", "journal": "", "year": "2019", "authors": "Mirac Suzgun; Yonatan Belinkov; Stuart M Shieber"}, {"ref_id": "b28", "title": "Memory-augmented recurrent neural networks can learn generalized dyck languages", "journal": "", "year": "2019", "authors": "Mirac Suzgun; Sebastian Gehrmann; Yonatan Belinkov; Stuart M Shieber"}, {"ref_id": "b29", "title": "The importance of being recurrent for modeling hierarchical structure", "journal": "Association for Computational Linguistics", "year": "2018-10", "authors": "Ke Tran; Arianna Bisazza; Christof Monz"}, {"ref_id": "b30", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b31", "title": "On the practical computational power of finite precision RNNs for language recognition", "journal": "Association for Computational Linguistics", "year": "2018-07", "authors": "Gail Weiss; Yoav Goldberg; Eran Yahav"}, {"ref_id": "b32", "title": "Learning the Dyck language with attention-based Seq2Seq models", "journal": "Association for Computational Linguistics", "year": "2019-08", "authors": "Xiang Yu; Ngoc Thang Vu; Jonas Kuhn"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Nested dependencies in English sentences and in Dyck-2.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Generalization of LSTMs and Transformers on higher depths. The lengths of strings in the training set and all validation sets were fixed to lie between 2 to 100.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "(a) Visualization of t-SNE Projections of the hidden states obtained from a pre-trained LSTM for different Dyck-2 substrings, colored by their depths.(b) Accuracies and recalls obtained on extracting top-10 elements (averaged over them) of the stack corresponding to different Dyck-2 strings using the hidden state vector of the LSTM.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Lemma B. 1 .1There exists a function O(x) : Q |\u03a6|+|\u03a8| \u2192 Q |\u03a6||\u03a8| of the form \u03c3(xW + b) such that, O([\u03c6, \u03c8]) = [(\u03c6, \u03c8)]", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 4 :4Figure 4: Generalization of LSTMs and Transformers on higher depths for (a) Dyck-3 and (b) Dyck-4. The lengths of strings in the training set and all validation sets were fixed to lie between 2 to 100.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ": The performance of neural models on considered Dyck languages. The reported scores areobtained by averaging the accuracies of 5 best performing hyperparameter configurations of each model.All models are trained on inputs with length in [2,50] and evaluated on validation sets. Bin-2A containshigher lengths without any restriction on depth. In Bin-1B, Bin-2B and Bin-3B, the test inputs have theirdepths upper bounded by 10.encoding scheme as introduced in Siegelmann and Sontag (1992) to emulate stack operations. The aboveresult was first obtained by Korsky and Berwick (2019) somewhat indirectly using the Chomsky andSch\u00fctzenberger (1959) theorem. The above Proposition implies that RNNs can recognize any deterministicCFL given unbounded precision. In the construction, the higher the required depth of the stack the higherthe required precision. This implies that fixed precision RNNs are expressive enough to recognize stringsof arbitrary lengths if the required depth of the stack is bounded. This can also be gleaned from theconstruction of Korsky and Berwick (2019) with some additional work 2 .3 ExperimentsSetup. In our experiments, we consider three Languages, namely Dyck-2, Dyck-3 and Dyck-4. For eachof the three languages, we generate 3 different types of training and validation sets. In the first case,we generate 10k strings for the training set within lengths [2, 50] and generate two validation sets eachcontaining 1k strings within lengths [2, 50] and"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "The performance of neural models on considered Dyck languages for data generated from three different distributions. Validation Set 1 was constructed from the same distribution used to generate the training data, while the other two were generated from different distributions. All the validation sets had strings with the lengths in the interval[2, 50]  and there was no restriction kept on the depth of these strings.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u03b4 : \u03a3 \u00d7 Q \u00d7 \u0393 \u2192 (q, \u03b3)", "formula_coordinates": [8.0, 256.6, 266.52, 106.16, 9.95]}, {"formula_id": "formula_1", "formula_text": "\u03c3(x) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 0 if x < 0, x if 0 \u2264 x \u2264 1, 1 if x > 1.", "formula_coordinates": [8.0, 227.52, 722.19, 136.33, 42.98]}, {"formula_id": "formula_2", "formula_text": "t (\u03c6,\u03c8) = [ \u03c6 + \u03c8A 1 , \u03c6 + \u03c8A 2 , . . . , \u03c6 + \u03c8A |\u03a8| ]", "formula_coordinates": [9.0, 190.62, 294.97, 216.3, 11.25]}, {"formula_id": "formula_3", "formula_text": "\u03c3(t (\u03c6,\u03c8) \u2212 1) = [(\u03c6, \u03c8)]", "formula_coordinates": [9.0, 245.03, 352.77, 107.48, 11.25]}, {"formula_id": "formula_4", "formula_text": "W = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 1 1 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb ,", "formula_coordinates": [9.0, 228.72, 460.27, 140.11, 68.12]}, {"formula_id": "formula_5", "formula_text": "\u03c6 2 if \u03b4(\u03c6 1 , \u03c8) = \u03c6 2 . Lemma B.2. There exists a function O(x) : Q |\u03a6||\u03a8| \u2192 Q |\u03a6| of the form \u03c3(xW + b) such that, O([(\u03c6 1 , \u03c8)]) = \u03c6 2", "formula_coordinates": [9.0, 72.0, 648.73, 416.87, 52.45]}, {"formula_id": "formula_6", "formula_text": "A (\u03c0 \u03a8 (\u03c8 j )\u22121)|\u03a6|+\u03c0 \u03a6 (\u03c6 i ),: = \u03c6 k .", "formula_coordinates": [9.0, 383.21, 741.93, 131.99, 12.2]}, {"formula_id": "formula_7", "formula_text": "O(x) : Q |\u03a6||\u03a8| \u2192 {0, 1} n of the form \u03c3(xW + b) such that, O([(\u03c6, \u03c8)]) = [{0, 1} n ]", "formula_coordinates": [10.0, 234.83, 109.2, 266.54, 36.33]}, {"formula_id": "formula_8", "formula_text": "t\u22121 = [(q t\u22121 , x t ), \u03c4 top t\u22121 , \u03c9 t\u22121 ]", "formula_coordinates": [10.0, 72.0, 554.58, 453.54, 27.28]}, {"formula_id": "formula_9", "formula_text": "(2) t\u22121 = [(q t\u22121 , x t , \u03c4 top t\u22121 ), \u03c4 top t\u22121 , \u03c9 t\u22121 ]", "formula_coordinates": [10.0, 119.81, 679.27, 154.12, 15.86]}, {"formula_id": "formula_10", "formula_text": "L = L N CP + \u03bbL aux (5)", "formula_coordinates": [13.0, 252.58, 447.46, 272.97, 10.69]}], "doi": ""}