{"title": "Learning High Fidelity Depths of Dressed Humans by Watching Social Media Dance Videos", "authors": "Yasamin Jafarian; Hyun Soo Park", "pub_date": "", "abstract": "Output depth estimationFigure 1: This paper presents a novel approach to estimate high fidelity depths of dressed humans from a single view image by leveraging a new data resource: a number of social media dance videos that span diverse appearance, clothing styles, performances, and identities. We show an example of a sequence of this data and the corresponding human mask along with the estimated depth (the darker, the closer) and the reconstructed surface.", "sections": [{"heading": "Introduction", "text": "Clothes are an integral part of our everyday life to function, express, and protect ourselves. With the increasing prevalence of VR and AR, the ability to precisely model the complex geometry of dressed humans is becoming the key to authentic social tele-presence. To capture the local geometry, e.g., wrinkle and fabric texture, photogrammetry based on massive camera infrastructure (e.g., 40-500 cameras to cover full body shape) [12,25,55] has been used, resulting in production-level rendering [9,32] and 3D fabrication [3,5]. Despite its promise, the practical deployment of such massive camera systems in our daily environment is still challenging because of its hardware requirements and computational complexity. Single view reconstruction is an immediate remedy to address this challenge where 3D representation of humans can be learned from the scanned human 3D models [1][2][3]. Nonetheless, the amount of these data is limited (e.g., a few hundreds of static models), which do not span diverse poses, appearance, and complex cloth geometry resulting in the performance degradation of 3D human reconstruction when applying to real-world imagery. In this paper, we present a method to reconstruct high fidelity 3D geometry of dressed humans in the form of depths and surface normals from a single view image by exploiting hundreds of dance videos shared in social media (e.g., Tik-Tok mobile application) as shown in Figure 1.\nThe main characteristics of these dance videos are that 1) each video depicts a sequence of diverse poses of a single person; and 2) 3D ground truth is not available, i.e., existing fully supervised methods are not applicable. We conjecture that since the geometry of dressed humans is an inherent semi-rigid structure, the local geometry of the same person approximately remains constant up to some transformations. For instance, the cloth movement on the left upper arm region undergoes, by large, a rigid transformation when its pose changes. Therefore, it is possible that the geometric consistency over different poses can be applied to learn from the real dance videos. We estimate a transformation for each body part that can warp its 3D geometry from one image to another image at a different time instant. This allows us to self-supervise the predicted geometry of the dressed humans without 3D supervision.\nWhile modern learning based depth estimators are capable of recovering holistic scene geometry, it is shown [30] that it often fails to encode fine local geometry such as complex cloth wrinkles and face profile features, which constitutes the dominant factor of realism. On the other hand, surface normals are highly responsive to fine visual structures such as texture and wrinkles [54]. We exploit the geometric relationship of depths and surface normals to learn jointly (e.g. matching the surface normal to the curvature of the depth).\nOur end-to-end trainable method takes as input an RGB image, corresponding human foreground, and human UV coordinates and outputs a high fidelity depth that captures fine wrinkles and shapes that is faithful to the input image. We design a network called HDNet that learns the spatial relationship between the image and UV coordinate to produce an intermediate surface normals. These predicted surface normals are, in turn, used to predict the high fidelity depths of dressed humans. We use a Siamese design of HD-Net to measure the self-consistency of a pair of geometric predictions. To that end, our method is semi-supervised by leveraging both 3D scanned models and real dance videos. We demonstrate that our method outperforms the state-ofthe-art human depth estimation approaches on both real and rendered images.\nWe present four core contributions: (1) a new dataset called TikTok dataset that consists of more than 300 sequences of dance videos shared in a social media mobile platform, TikTok, totaling more than 100K images along with the human mask and human UV coordinates; (2) a novel formulation that warps the 3D geometry of dressed humans from one image to the other image at a different time instant to measure self-consistency, which allows us to utilize the real dance videos; (3) HDNet design that learns to predict fine depths reflective of surface normal prediction by enforcing their geometric consistency; (4) strong qualitative and quantitative prediction on real world imagery.", "publication_ref": ["b34", "b6", "b19", "b49", "b3", "b26", "b24", "b48"], "figure_ref": [], "table_ref": []}, {"heading": "Related Works", "text": "Our fundamental contribution lies at the intersection of human body reconstruction, single view depth estimation, and human 3D datasets. Human Body Reconstruction There are two predominant representations in human body reconstruction: parametric and non-parametric. Similar to face modeling [16], parametric mesh models such as SCAPE [8] and SMPL [33] are an attractive choice of the human body representation, which can be used for single view human reconstruction [13,26,29,39,41,44,56] and synthetic data generation [51,52]. While the parametric representation effectively limits the space of solutions where learning based approaches can be readily applicable and show remarkable performance, the reconstructed geometry has limited resolution, which prevents from expressing fine details of dressed humans. This has been addressed by refining parametric models with residual geometry [6,7,28,35]. Depth [30,50] or volumetric representation [23,60] as a non-parametric representation can describe the geometry of dressed humans. Unlike parametric models, obtaining the ground truth data is challenging: Li et al. [30] addressed by exploiting a large community dataset of Mannequin Challenge, and Tang et al. [50] incorporated semantic labels (pose and segmentation) to regularize their depth estimator. Single View Depth Estimation Single view depth estimation is a core task of scene understanding where sophisticated designs of convolutional neural networks (CNNs) enable predicting scene geometry [34,49]. To capture fine details of depth reconstruction, additional cues such as surface normals have been incorporated [18,36,40,42,43,50,59,61]. Iterative least squares [50] and kernel regression [42] have been used to fuse the surface normals and depths, and coarse-to-fine learning is used to densify LiDAR data for outdoor scenes or missing depth data [43] for indoor scenes [59]. Recently, integrating the surface normal into the depth prediction [54] (e.g. identifying whether a normal representation is realistic or not using GAN [21]) has shown to be effective in restoring local geometry such as cloth wrinkles and face profile features. Unlike previous work, we focus on recovering sub-centimeter detailed geometry tailored to dressed humans by jointly learning depths and surface normals and leveraging a large dataset of social media dance videos. Human 3D Datasets While there are a number of RGBD datasets for structural scene understanding [14,17,48,57], a limited amount of data address the problem of the geometry prediction for dressed humans in the wild. A few RGBD datasets [11,15,31,47] are designed for humans action recognition. However, these data lack the geometric details such as cloth wrinkles. For human geometry, the 3D scanned models [1-3] or multiview generated models [53,58] can be used to generate photorealistic images from multiple views, which has been used for training a geometry predictor with full supervision [45,46]. However, the amount of data is still limited to a few hundreds of static models, which prevents from learning a generalizable prediction model. In this paper, we introduce a new source of data: real dance videos from the Internet to generalize the human depth estimation to different view points, human appearance, clothing styles and poses.", "publication_ref": ["b10", "b2", "b27", "b7", "b20", "b23", "b33", "b35", "b38", "b50", "b45", "b46", "b0", "b1", "b22", "b29", "b24", "b44", "b17", "b54", "b24", "b44", "b28", "b43", "b12", "b30", "b34", "b36", "b37", "b44", "b53", "b55", "b44", "b36", "b37", "b53", "b48", "b15", "b8", "b11", "b42", "b51", "b5", "b9", "b25", "b41", "b47", "b52", "b39", "b40"], "figure_ref": [], "table_ref": []}, {"heading": "Method", "text": "Given a single image of a dressed human I, we reconstruct its high fidelity depth, i.e., z = g(x; I), where x \u2208 R 2 is the xy-location in the image, and z \u2208 R + is the depth at the corresponding location.\nExisting approaches learn g directly from the ground truth data, which shows two limitations in estimating depths of clothed humans. (1) While existing depth estimators are highly responsive to predict holistic scene geometry, it is shown [30] that its expressibility is limited at encoding fine local geometry such as irregular and complex wrinkles, that constitute the dominant factor of realism. (2) It requires a large amount of 3D ground truth data (e.g., ScanNet [17] and KITTI [19,20]). Such large ground truth data for humans that span diverse appearance, cloth styles, and poses do not exist (e.g., a few hundreds of posed scanned models [1-3]).", "publication_ref": ["b24", "b11", "b13", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Self-supervised Human Depths from Videos", "text": "We present a new method to address these limitations by leveraging large video data of real humans in motion. Albeit lacking of 3D ground truth, each video depicts the movement of a single person across time where her/his geometry approximately remains constant up to local transformations.\nConsider a coordinate transform h(u) = x that maps a canonical human body surface coordinate u \u2208 R 2 (UV surface coordinate) to the corresponding point x in an image. A key feature of the UV surface coordinate is that it is invariant to poses, clothes, and appearance.\nWe parametrize a 3D point p \u2208 R 3 reconstructed by the depth prediction using the UV coordinate, i.e.,\np i (u) = zK \u22121 x = g(h i (u); I i )K \u22121 h i (u),(1)\nwhere K \u2208 R 3\u00d73 is the camera intrinsic parameter, \u2022 \u2208 P 2 is the homogeneous representation [24], and x is the pixel location in the image domain corresponding to u in the UV domain. The subscript i indicates the time instant. The green boxes in two images show the UV correspondences of the left arm. The depths of the left arm are reconstructed in 3D and transformed to the j th time to form the part based warped depths. We apply bilinear interpolation on the foreground range, resulting in the warped depths that can supervise the depth estimate at the j th time instant through the warping loss L w .\nWe transform a set of points in the k th body part at the i th time instant to the j th time instant:\np i\u2192j (u) = W k i\u2192j (p i (u)), u \u2208 U k (2)\nwhere W is a 3D part based warping function, and U k is the set of UV coordinates associated with the k th body part. The body part is defined as a region of the body where its local geometry approximately undergoes a rigid transformation, e.g., lower arm. An analogous warping is used for non-rigid tracking [37] without the part based representation, which allows mapping between consecutive frames. With the part based warping, we substantially extend the time horizon by parametrizing the 3D point using the UV coordinate, which does not require an offline iterative closest point method between the consecutive frames. We use the Special Eucliean Transform (SE3) to model\nW k i\u2192j (p i ) = R k i\u2192j p i + t k i\u2192j\nwhere R and t are the rotation and translation 1 . Among the correspondences p i (u) \u2194 p j (u), we pre-define a subset of correspondences that represent the overall transformation for each part. With the pre-defined correspondences, we compute the transformation by minimizing the following error:\nminimize R,t l p j (v l ) \u2212 W k i\u2192j (p i (v l )) 2 , v l \u2208 V k \u2282 U k ,\nTang et al.", "publication_ref": ["b18", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "Ours", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ground truth Image", "text": "Figure 3: We compare our method with Tang et al. [50] on the surface normals derived from the depths. While two methods use the surface normals to enhance the depths, unlike Tang et al., our method jointly learns surface normals and depths by supervising them with each other, which produces more realistic and less noisy prediction that preserves the detailed geometry of wrinkles and face.\nwhere V k is the subset of the UV coordinates that represent the overall transformation. We minimize the objective using least squares [10]. In practice, we choose the sparse correspondences in the subset by discretizing the UV coordinates. This transformation is computed online, i.e., the transformation changes as the depth prediction is updated at each training iteration.\nFigure 2 illustrates the self-supervision via warping the 3D geometry of humans between two frames of a video. By having the estimated depth, we use the UV coordinates to warp the local geometry for each part from the i th time instant to the j th time instant resulting in a sparse warped depth. We apply bilinear interpolation on the foreground range to get a dense warped depths that can supervise the depth estimate at the j th time instant by minimizing warping loss L w .\nWe minimize the following loss to measure geometric discrepancy between two time instances:\nL w = l (i,j)\u2208V l k u\u2208U k p j (u) \u2212 p i\u2192j (u) 2 , (3)\nwhere V l is the set of time instances within the l th video.\nEquation (3) allows us to utilize a large amount of real videos without the 3D ground truth via self-supervision, i.e., the estimated depth in one pose can be used to supervise the depth in the other pose. This makes the depth estimation responsive to real data of diverse human poses and appearances.", "publication_ref": ["b44", "b4"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Joint Learning of Surface Normal and Depth", "text": "Surface normal estimation is highly responsive to the local texture, wrinkle, and shade [50,54]. We jointly estimate surface normal and depth to benefit from each other. We estimate the surface normals of an image I, i.e., n = f (x; I) where n \u2208 S 2 is the unit surface normal vector represented in the camera coordinate system. Surface normal n(x) is the curvature that is perpendicular to the tangential plane of the corresponding 3D point p(x) (we override the notation p(u) in Equation (1)), i.e.,\nn(x) = \u2202p(x) \u2202x \u00d7 \u2202p(x) \u2202y / \u2202p(x) \u2202x \u2202p(x) \u2202y ,(4)\nwhere n denotes the surface normal estimate derived by the depth estimate.\nWe ensure geometric consistency between the predicted surface normals and the derived surface normals from the depth estimates by minimizing their geometric error:\nL s = Ii\u2208D x\u2208R(Ii) cos \u22121 n T (x) n(x) n(x) n(x) ,(5)\nwhere R(I) is the coordinate range of the image I, and D is the image dataset including the dance videos and scanned 3D models.\nNote that the relationship between surface normal and depth has been used to obtain the details of depth estimates. GeoNet [42] has leveraged the derived surface normals to refine the surface normal estimate for an indoor scene understanding. In the human domain, Tang et al. [50] uses the surface normal prediction to refine the human depth in a post-processing manner. Unlike these methods, we use the surface normal estimates to supervise the depths and the depth estimates to supervise the surface normals by enforcing their geometric consistency in the training phase. This end-to-end online pipeline enables learning the depths from the real videos without the ground truth depth. Figure 3 illustrates the comparison of the surface normal generated from the predicted depth of our method and Tang et al. [50]. Our result is realistic, which captures the wrinkles of the cloth fabric compared to Tang et al. [50].", "publication_ref": ["b44", "b48", "b36", "b44", "b44", "b44"], "figure_ref": [], "table_ref": []}, {"heading": "Network Design", "text": "We minimize the following overall loss to learn the depth and surface normal estimators from real videos and 3D scanned models:\nL = L z + \u03bb n L n + \u03bb s L s + \u03bb w L w ,(6)\nwhere \u03bb n , \u03bb s , and \u03bb w are relative weights between losses.\nIn addition to self-consistency losses (\u03bb w and \u03bb s ), we utilize the 3D ground truth data from the 3D scanned models [2]. This depth and surface normal can be learned by minimizing the following error between ground truth normal N(x) and the prediction.  where D s is the 3D scanned dataset with the ground truth depths Z(x) and surface normals N(x). Network Design and Details We design our neural network called HDNet (Human Depth Neural Network) that allows us to utilize both real videos and 3D scanned model data as shown in Figure 4(a). HDNet is composed of two estimators: surface normal and depth estimators. The surface normal estimator f (x; I) takes as input an RGB image and its foreground mask, and outputs the surface normal estimates. The depth estimator, g(x; I), in turn, takes as input a triplet of an RGB image, foreground mask, and UV coordinate, and outputs the depth estimates. The geometric consistency between the surface normal and depth is enforced by minimizing L s . For the 3D scanned model data, both estimators are supervised by the ground truth surface normal and depth (L n and L z ), respectively.\nL z = Ii\u2208Ds x\u2208R(Ii) Z(x) \u2212 g(x; I) 2 ,(7)\nL n = Ii\u2208Ds x\u2208R(Ii) N(x) \u2212 f (x; I) 2 ,(8)\nFor the real videos, we build a Siamese network with HDNet where two triplets from two time instances within the same video are used for the depth estimates as shown in 4(b). The UV coordinates from both images are used to compute the special Euclidean transformation that is used to warp the depth from one image to the other image. At each time instant, we make five image pairs by randomly selecting time instances that have the UV coordinates of at least five common visible body parts while each contains more than 50 overlapping UV coordinates.\nFor the two estimators, we use the stacked hourglasses network [38] as a backbone network. The image and its foreground mask are cropped from the input image and resized to 256\u00d7256, and h is approximated by the inverse of the UV map obtained by DensePose [22]. We use Adam optimizer [27] with the following parameters for the training. Batch size: 10; learning rate: 0.001; the number of epochs: 380; \u03bb n : 1; \u03bb s : 0.5; and \u03bb w : 5; GPU model: NVIDIA V100.", "publication_ref": ["b32", "b16", "b21"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "TikTok Dataset", "text": "We learn high fidelity human depths by leveraging a collection of social media dance videos scraped from the Tik-Tok mobile social networking application. It is by far one of the most popular video sharing applications across generations, which include short videos (10-15 seconds) of diverse dance challenges as shown in Figure 5. We manually find more than 300 dance videos that capture a single person performing dance moves from TikTok dance challenge compilations for each month, variety, type of dances, which are moderate movements that do not generate excessive motion blur. For each video, we extract RGB images at 30 frame per second, resulting in more than 100K images. We segmented these images [4], and computed the UV coordinates. The dataset and code can be found in https://www.yasamin.page/hdnet_tiktok.  Figure 5: TikTok Dataset. We present a new dataset called TikTok dataset that consists of more than 300 sequences of dance videos shared in a social media mobile platform, TikTok, totaling more than 100K images along with the human mask and human UV coordinates.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We evaluate our method both quantitatively and qualitatively compared with the state-of-the-art methods of human depth estimation and human shape recovery on real and synthetic data. Training Datasets We use two datasets for training: 340 subjects from 3D scanned model (RenderPeople) [2] with 3D ground truth and our TikTok dataset without 3D ground truth (Section 4). We render the 3D scanned mesh models from approximately 100 viewpoints sampled uniformly across a camera rig (6m diameter) that encircles each subject with 16.5mm focal length. Total 34,000 and 100,000 images are used for training from RenderPeople and Tik-Tok data, respectively. Evaluation Datasets We use three datasets to compare the performance of ours and baseline methods: Tang et al. [50], RenderPeople [2], and Vlasic et al. [53]. 1) Training dataset of Tang et al. This dataset is made of sequences of depth and RGB image pair for 25 subjects. We randomly choose around 70 frames for each subject, totaling 1300 images. 2) RenderPeople dataset This dataset is made of 3D scanned models with texture. We choose 6 subjects that are not part of our training data and render the images from 100 viewpoints, totaling 600 images. We use a ray tracing algorithm to compute the ground truth depth and render the human textured model. 3) Vlasic et al. dataset This dataset consists of 10 sequences of different people viewed from 8 views. Each video includes average of 200 frames of diverse activities such as swing dancing, samba dancing, jumping, squat, and marching. The dataset provides the RGB images and the meshes along with the camera parameters. We use a ray tracing algorithm to generate the ground truth depth from the meshes. We randomly choose total of 2000 images from this dataset. This dataset is in particular challenging because the viewpoints are substantially different from the existing datasets, i.e., a subject is viewed from an oblique view. Evaluation metric We evaluate the performance in two aspects: (1) accuracy of depths, surface normals, 3D reconstruction, and (2) impact of joint training of surface normal and depth (L s ) and integration of real dance videos (L w ). We use mean squared error and mean absolute angular error as a metric for depth and surface normal, respectively. The surface normals are computed via Equation (4) and compared with the ground truth. In addition, we measure the 3D error by reconstructing the estimated depth. Since depths are reconstructed up to scale, we scale the reconstructed depth to match to the ground truth, i.e., the predicted depth is translated to the median of ground truth and scaled to match the minimum/maximum depths.\nWe followed the evaluation protocol of Li et al. [30], i.e., no retraining of the baseline models. We categorize the baseline methods into two: human depth estima- tion [30,50], and human shape recovery [45,46]. The quantitative comparison is summarized in Table 1. i) Human shape recovery We compare our method with non-parametric human shape recovery designed for dressed humans (PIFu [45] and PIFuHD [46]) using an implicit function. Note that these methods predict not only the frontal body surface but also occluded body surface where we measure error only for the visible region. We apply a ray tracing method to identify the frontal surface where we measure the depth and surface normal.\nii) Human depth estimation We compare with depth estimation baselines that are tailored to dressed humans, which are most relevant to our work. Li et al. [30] used a large community dataset called MannequinChallenge dataset to train  the stacked hourglasses [38], and Tang et al. [50] leveraged surface normals and depths to preserve detailed dressed human shapes. Since these two methods were designed for human depths, in particular, Li et al. [30] shows strong performance on both depths and surface normals. Figure 6(a) shows the evaluation of our method compared to the baseline methods on TikTok dataset. We can get the most representative depth estimation compared to other methods. Figure 6(b) visualizes the error map of our depth prediction and other baseline methods on Tang et. al. dataset [50]. Figure 7 shows the qualitative results of our method and the baseline methods on the evaluation datasets. We normalize the error e with respect to the ground truth depth, i.e., e = |z \u2212 Z|/Z where z and Z are the zero-mean predicted and ground truth depths, respectively. Ablation study We conduct an ablation study to analyze the impact of the losses in training: L z , L w and L s . We consider three combinations: L z , L z +L s , and L z +L s +L w . We use the Tang et al. dataset [50] for the evaluation. We scale the predicted depth to match to the ground truth, i.e., the predicted depth is translated to the median of ground truth and scaled to match the minimum/maximum depths.  Table 2 summarizes the comparison of the combinations. On the one hand, L w enforces the network to learn the geometric consistency from the videos. This loss is highly effective, allows learning from a limited amount of 3D data.\nOn the other hand, L s enforces to learn to recover the details, which can further reduce the depth and surface normal errors. Our method that leverages all three losses shows the most accurate prediction in reconstructing the depths and surface normals (last row of Table 2). Figure 8 shows the qualitative results of our ablation study on two examples of TikTok data. From left to right   [50]. D. error (normalized error), N. error (rad) and R. error represent depth error, normal error, and reconstruction error respectively (mean\u00b1std).\nwe have the image, the final method results and the results without self supervision.\nWe also visualize the performance of our method on a set of web images in Figure 9. Our method is generalizable to gray scale images, paintings, and images with multiple people.", "publication_ref": ["b44", "b47", "b24", "b24", "b44", "b39", "b40", "b39", "b40", "b24", "b32", "b44", "b24", "b44", "b44", "b44"], "figure_ref": ["fig_3", "fig_3", "fig_5", "fig_6", "fig_7"], "table_ref": ["tab_1", "tab_2", "tab_2"]}, {"heading": "Conclusion", "text": "This paper presents a new method to utilize large data of video data shared in social media to predict the depths of dressed humans. Our formulation allows self-supervision of depth prediction by leveraging local transformations to enforce geometric consistency across different poses. In addition, we jointly learn the surface normal and depth to generate high fidelity depth reconstruction. A new dataset called TikTok dataset is collected, consisting of more than 300 sequences of dance videos shared in a social media mobile platform, TikTok, totaling more than 100K images. Our method produces strong qualitative and quantitative prediction on real world imagery compared to the state-of-the-art human depth estimation and human shape recovery.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgement This work was supported by a NSF NRI 2022894 and NSF CAREER 1846031.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Video based reconstruction of 3D people models", "journal": "", "year": "2018", "authors": "H Alldieck; M Magnor; W Xu; C Theobalt; G Pons-Moll"}, {"ref_id": "b1", "title": "Tex2Shape: Detailed full human body geometry from a single image", "journal": "", "year": "2019", "authors": "T Alldieck; G Pons-Moll; C Theobalt; M Magnor"}, {"ref_id": "b2", "title": "SCAPE:shape completion and animation of people", "journal": "", "year": "2005", "authors": "D Anguelov; P Srinivasan; D Koller; S Thrun; J Rodgers; J Davis"}, {"ref_id": "b3", "title": "Adaptive mesh texture for multi-view appearance modeling", "journal": "", "year": "2018", "authors": "M Armando; J.-S Franco; E Boyer"}, {"ref_id": "b4", "title": "Least-squares fitting of two 3-d point sets", "journal": "TPAMI", "year": "1987", "authors": "K Arun; T Huang; S Blostein"}, {"ref_id": "b5", "title": "Re-identification with rgb-d sensors", "journal": "", "year": "2012", "authors": "B I Barbosa; M Cristani; A Bue; L Bazzani; V Murino"}, {"ref_id": "b6", "title": "High-quality single-shot capture of facial geometry", "journal": "SIG-GRAPH", "year": "2010", "authors": "T Beeler; B Bickel; P Beardsley; B Sumner; M Gross"}, {"ref_id": "b7", "title": "Keep it smpl: Automatic estimation of 3D human pose and shape from a single image", "journal": "", "year": "2016", "authors": "F Bogo; A Kanazawa; C Lassner; P Gehler; J Romero; M J Black"}, {"ref_id": "b8", "title": "Matterport3d: Learning from rgb-d data in indoor environments", "journal": "", "year": "2017", "authors": "A Chang; A Dai; T Funkhouser; M Halber; M Niessner; M Savva; S Song; A Zeng; Y Zhang"}, {"ref_id": "b9", "title": "Utd-mhad: A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor", "journal": "", "year": "2015", "authors": "C Chen; R Jafari; N Kehtarnavaz"}, {"ref_id": "b10", "title": "Active appearance models", "journal": "TPAMI", "year": "2001", "authors": "T F Cootes; G J Edwards; C J Taylor"}, {"ref_id": "b11", "title": "Scannet: Richly-annotated 3d reconstructions of indoor scenes", "journal": "", "year": "2017", "authors": "A Dai; A X Chang; M Savva; M Halber; T Funkhouser; M Nie\u00dfner"}, {"ref_id": "b12", "title": "Geo-supervised visual depth prediction", "journal": "", "year": "", "authors": "X Fei; A Wong; S Soatto"}, {"ref_id": "b13", "title": "Vision meets robotics: The kitti dataset. IJRR", "journal": "", "year": "2013", "authors": "A Geiger; P Lenz; C Stiller; R Urtasun"}, {"ref_id": "b14", "title": "Are we ready for autonomous driving? the kitti vision benchmark suite", "journal": "", "year": "2012", "authors": "A Geiger; P Lenz; R Urtasun"}, {"ref_id": "b15", "title": "Generative adversarial nets", "journal": "", "year": "2014", "authors": "I Goodfellow; J Pouget-Abadie; M Mirza; B Xu; D Warde-Farley; S Ozair; A Courville; Y Bengio"}, {"ref_id": "b16", "title": "DensePose: Dense human pose estimation in the wild", "journal": "", "year": "2018", "authors": "R A G\u00fcler; N Neverova; I Kokkinos"}, {"ref_id": "b17", "title": "Deepcap: Monocular human performance capture using weak supervision", "journal": "", "year": "", "authors": "M Habermann; W Xu; M Zollhoefer; G Pons-Moll; C Theobalt"}, {"ref_id": "b18", "title": "Multiple View Geometry in Computer Vision", "journal": "Cambridge University Press", "year": "2004", "authors": "R Hartley; A Zisserman"}, {"ref_id": "b19", "title": "Map visibility estimation for large-scale dynamic 3D reconstruction", "journal": "", "year": "2014", "authors": "H Joo; H S Park; Y Sheikh"}, {"ref_id": "b20", "title": "Endto-end recovery of human shape and pose", "journal": "", "year": "2018", "authors": "A Kanazawa; M J Black; D W Jacobs; J Malik"}, {"ref_id": "b21", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2015", "authors": "D P Kingma; J L Ba"}, {"ref_id": "b22", "title": "DeepWrinkles: Accurate and realistic clothing modeling", "journal": "", "year": "", "authors": "Z Laehner; D Cremers; T Tung"}, {"ref_id": "b23", "title": "Unite the people: Closing the loop between 3D and 2D human representations", "journal": "", "year": "2017", "authors": "C Lassner; J Romero; M Kiefel; F Bogo; M J Black; P V Gehler"}, {"ref_id": "b24", "title": "Learning the depths of moving people by watching frozen people", "journal": "", "year": "2007", "authors": "Z Li; T Dekel; F Cole; R Tucker; N Snavely; C Liu; W T Freeman"}, {"ref_id": "b25", "title": "Ntu rgb+d 120: A large-scale benchmark for 3d human activity understanding. TPAMI", "journal": "", "year": "2019", "authors": "J Liu; A Shahroudy; M Perez; G Wang; L.-Y Duan; A C Kot"}, {"ref_id": "b26", "title": "Deep appearance models for face rendering", "journal": "", "year": "2018", "authors": "S Lombardi; J Saragih; T Simon; Y Sheikh"}, {"ref_id": "b27", "title": "SMPL: A skinned multi-person linear model. SIG-GRAPH Asia", "journal": "", "year": "2015", "authors": "M Loper; N Mahmood; J Romero; G Pons-Moll; M J Black"}, {"ref_id": "b28", "title": "Consistent video depth estimation", "journal": "", "year": "", "authors": "X Luo; J Huang; R Szeliski; K Matzen; J Kopf"}, {"ref_id": "b29", "title": "Learning to dress 3D people in generative clothing", "journal": "", "year": "", "authors": "Q Ma; J Yang; A Ranjan; S Pujades; G Pons-Moll; S Tang; M Black"}, {"ref_id": "b30", "title": "Efficiently combining positions and normals for precise 3D geometry", "journal": "", "year": "2005", "authors": "D Nehab; S Rusinkiewicz; J Davis; R Ramamoorthi"}, {"ref_id": "b31", "title": "Dynamicfusion: Reconstruction and tracking of non-rigid scenes in real-time", "journal": "", "year": "2015", "authors": "R A Newcombe; D Fox; S M Seitz"}, {"ref_id": "b32", "title": "Based on stacked hourglass networks for human pose estimation", "journal": "", "year": "2016", "authors": "A Newell; K Yang; J Deng"}, {"ref_id": "b33", "title": "Neural body fitting: Unifying deep learning and model based human pose and shape estimation", "journal": "", "year": "2018", "authors": "M Omran; C Lassner; G Pons-Moll; P Gehler; B Schiele"}, {"ref_id": "b34", "title": "RGBD-Fusion: Real-time high precision depth recovery", "journal": "", "year": "2015", "authors": "R Or-El; G Rosman; A Wetzler; R Kimmel; A Bruckstein"}, {"ref_id": "b35", "title": "Expressive body capture: 3D hands, face, and body from a single image", "journal": "", "year": "2019", "authors": "G Pavlakos; V Choutas; N Ghorbani; T Bolkart; A A A Osman; D Tzionas; M J Black"}, {"ref_id": "b36", "title": "GeoNet: Geometric neural network for joint depth and surface normal estimation", "journal": "", "year": "2018", "authors": "X Qi; R Liao; Z Liu; R Urtasun; J Jia"}, {"ref_id": "b37", "title": "DeepLiDAR: Deep surface normal guided depth prediction for outdoor scene from sparse lidar data and single color image", "journal": "", "year": "2019", "authors": "J Qiu; Z Cui; Y Zhang; X Zhang; S Liu; B Zeng; M Pollefeys"}, {"ref_id": "b38", "title": "General automatic human shape and motion capture using volumetric contour cues", "journal": "", "year": "2016", "authors": "H Rhodin; N Robertini; D Casas; C Richardt; H.-P Seidel; C Theobalt"}, {"ref_id": "b39", "title": "PIFu: Pixel-aligned implicit function for high-resolution clothed human digitization", "journal": "", "year": "2006", "authors": "S Saito; Z Huang; R Natsume; S Morishima; A Kanazawa; H Li"}, {"ref_id": "b40", "title": "Pifuhd: Multilevel pixel-aligned implicit function for high-resolution 3d human digitization", "journal": "", "year": "2020", "authors": "S Saito; T Simon; J Saragih; H Joo"}, {"ref_id": "b41", "title": "Ntu rgb+d: A large scale dataset for 3d human activity analysis", "journal": "", "year": "2016", "authors": "A Shahroudy; J Liu; T.-T Ng; G Wang"}, {"ref_id": "b42", "title": "Indoor segmentation and support inference from rgbd images", "journal": "", "year": "2012", "authors": "N Silberman; D Hoiem; P Kohli; R Fergus"}, {"ref_id": "b43", "title": "Selfsupervised human depth estimation from monocular videos", "journal": "", "year": "", "authors": "F Tan; H Zhu; Z Cui; S Zhu; M Pollefeys; P Tan"}, {"ref_id": "b44", "title": "A neural network for detailed human depth estimation from a single image", "journal": "", "year": "2008", "authors": "S Tang; F Tan; K Cheng; Z Li; S Zhu; P Tan"}, {"ref_id": "b45", "title": "BodyNet: Volumetric inference of 3D human body shapes", "journal": "", "year": "2018", "authors": "G Varol; D Ceylan; B Russell; J Yang; E Yumer; I Laptev; C Schmid"}, {"ref_id": "b46", "title": "Learning from synthetic humans", "journal": "", "year": "2017", "authors": "G Varol; J Romero; X Martin; N Mahmood; M J Black; I Laptev; C Schmid"}, {"ref_id": "b47", "title": "Articulated mesh animation from multi-view silhouettes", "journal": "", "year": "2006", "authors": "D Vlasic; I Baran; W Matusik; J Popovi\u0107"}, {"ref_id": "b48", "title": "Normalgan: Learning detailed 3d human from a single rgb-d image", "journal": "", "year": "2020", "authors": "L Wang; X Zhao; T Yu; S Wang; Y Liu"}, {"ref_id": "b49", "title": "Performance relighting and reflectance transformation with time-multiplexed illumination", "journal": "SIGGRAPH", "year": "2005", "authors": "A Wenger; A Gardner; C Tchou; J Unger; T Hawkins; P Debevec"}, {"ref_id": "b50", "title": "Monocular total capture: Posing face, body, and hands in the wild", "journal": "", "year": "2019", "authors": "D Xiang; H Joo; Y Sheikh"}, {"ref_id": "b51", "title": "SUN3D: A database of big spaces reconstructed using SfM and object labels", "journal": "", "year": "2013", "authors": "J Xiao; A Owens; A Torralba"}, {"ref_id": "b52", "title": "Detailed, accurate, human shape estimation from clothed 3d scan sequences", "journal": "", "year": "2017", "authors": "C Zhang; S Pujades; M J Black; G Pons-Moll"}, {"ref_id": "b53", "title": "Deep depth completion of a single rgb-d image", "journal": "", "year": "2018", "authors": "Y Zhang; T Funkhouser"}, {"ref_id": "b54", "title": "DeepHuman: 3D human reconstruction from a single image", "journal": "", "year": "2019", "authors": "Z Zheng; T Yu; Y Wei; Q Dai; Y Liu"}, {"ref_id": "b55", "title": "Simpose: Effectively learning densepose and surface normal of people from simulated data", "journal": "", "year": "", "authors": "T Zhu; P Karlsson; C Bregler"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure2: Given the depth estimate at the i th time instant, we use a part based transformation that warps the 3D local geometry of the image to the image at the j th time instant. The green boxes in two images show the UV correspondences of the left arm. The depths of the left arm are reconstructed in 3D and transformed to the j th time to form the part based warped depths. We apply bilinear interpolation on the foreground range, resulting in the warped depths that can supervise the depth estimate at the j th time instant through the warping loss L w .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "HDNet self-supervision using two images from different time instances.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure4: (a) Our network HDNet takes as input an image with the correspondending human foreground and UV coordinates and predicts the high fidelity depth of the human. The HDNet is composed of the depth and surface normal estimators. The surface normal estimator takes the input image and the foreground human mask and outputs the surface normal estimation. The surface normal estimation is, in turn, used as an input along with the image, foreground human mask, and part based UV coordinate to the depth estimator. We enforce the geometric consistency between the estimated depths and surface normals. (b) We build a Siamese design of HDNet to leverage real dance videos. The estimated depth of one image is warped to the other image at a different time instant using a part based transformation. We measure the geometric consistency between the predicted depth and warped depth through L w .", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "ImageFigure 6 :6Figure 6: (a) We compare our method with the baseline methods (Li et al. [30], Tang et al. [50], PIFu [45], and PIFuHD [46]) on the TikTok dataset. (b) We measure the normalized error of estimated depths on the training data of Tang et al [50].", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "ImageLi et al. Tang et al. PIFu PIFuHD Ours Ground truth", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 7 :7Figure 7: Qualitative results of Li et al. [30], Tang et al. [50], PIFu [45], PIFuHD [46] and ours. We show the results on the evaluation datasets: (1) Tang et al. training dataset [50] (first row), (2) RenderPeople dataset [2] (middle row) and Vlasic et al. dataset [53] (last row).", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 8 :8Figure8: Ablation study on loss functions. from left to right: the image, the full method results, and the results without self-supervision.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 9 :9Figure 9: Qualitative results of our method on web images. From left to right: image, predicted depth, recosntructed surface and surface normal.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": ".083 0.597\u00b10.099 0.096\u00b10.060 Lz + Ls 1.290\u00b10.874 0.523\u00b10.084 0.087\u00b10.054 Lz + Ls + Lw 1.212\u00b10.812 0.512\u00b10.076 0.083\u00b10.051", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "59\u00b11.11 0.68\u00b10.13 0.10\u00b10.05 3.54\u00b13.78 0.48\u00b10.13 0.10\u00b10.06 5.55\u00b15.98 0.76\u00b10.13 0.27\u00b10.10 Tang et al.[50] 1.21\u00b11.61 0.54\u00b10.12 0.08\u00b10.12 3.66\u00b13.29 0.59\u00b10.10 0.12\u00b10.05 2.29\u00b12.03 0.73\u00b10.09 0.23\u00b10.07 PIFu[45] 1.52\u00b11.07 0.57\u00b10.09 0.10\u00b10.06 2.28\u00b11.86 0.43\u00b10.09 0.09\u00b10.04 3.04\u00b12.97 0.69\u00b10.09 0.22\u00b10.06", "figure_data": "Tang et al. dataset [50]RenderPeople dataset [2]Vlasic et al. dataset [53]MethodD. errorN. errorR. errorD. errorN. errorR. errorD. errorN. errorR. errorLi et al. [30] 1.PIFuHD [46] 1.45\u00b10.86 0.60\u00b10.09 0.09\u00b10.05 2.33\u00b11.92 0.47\u00b10.09 0.09\u00b10.04 3.50\u00b13.43 0.80\u00b10.09 0.19\u00b10.05Ours1.21\u00b10.81 0.51\u00b10.07 0.08\u00b10.05 1.11\u00b10.75 0.27\u00b10.05 0.05\u00b10.02 1.21\u00b10.98 0.44\u00b10.07 0.13\u00b10.04"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Quantitative Results. D. error (normalized error), N. error (rad) and R. error represent depth error, normal error, and reconstruction error respectively (mean\u00b1std).", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Ablation study onTang et al. dataset  ", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "p i (u) = zK \u22121 x = g(h i (u); I i )K \u22121 h i (u),(1)", "formula_coordinates": [3.0, 80.79, 649.15, 205.57, 11.79]}, {"formula_id": "formula_1", "formula_text": "p i\u2192j (u) = W k i\u2192j (p i (u)), u \u2208 U k (2)", "formula_coordinates": [3.0, 352.42, 424.93, 192.69, 12.84]}, {"formula_id": "formula_2", "formula_text": "W k i\u2192j (p i ) = R k i\u2192j p i + t k i\u2192j", "formula_coordinates": [3.0, 308.86, 584.57, 120.95, 12.47]}, {"formula_id": "formula_3", "formula_text": "minimize R,t l p j (v l ) \u2212 W k i\u2192j (p i (v l )) 2 , v l \u2208 V k \u2282 U k ,", "formula_coordinates": [3.0, 308.86, 662.44, 241.76, 25.13]}, {"formula_id": "formula_4", "formula_text": "L w = l (i,j)\u2208V l k u\u2208U k p j (u) \u2212 p i\u2192j (u) 2 , (3)", "formula_coordinates": [4.0, 62.61, 505.97, 223.76, 23.45]}, {"formula_id": "formula_5", "formula_text": "n(x) = \u2202p(x) \u2202x \u00d7 \u2202p(x) \u2202y / \u2202p(x) \u2202x \u2202p(x) \u2202y ,(4)", "formula_coordinates": [4.0, 324.15, 118.99, 220.97, 23.54]}, {"formula_id": "formula_6", "formula_text": "L s = Ii\u2208D x\u2208R(Ii) cos \u22121 n T (x) n(x) n(x) n(x) ,(5)", "formula_coordinates": [4.0, 335.42, 220.82, 209.7, 27.6]}, {"formula_id": "formula_7", "formula_text": "L = L z + \u03bb n L n + \u03bb s L s + \u03bb w L w ,(6)", "formula_coordinates": [4.0, 356.38, 555.2, 188.74, 10.39]}, {"formula_id": "formula_8", "formula_text": "L z = Ii\u2208Ds x\u2208R(Ii) Z(x) \u2212 g(x; I) 2 ,(7)", "formula_coordinates": [4.0, 349.53, 656.99, 195.58, 21.8]}, {"formula_id": "formula_9", "formula_text": "L n = Ii\u2208Ds x\u2208R(Ii) N(x) \u2212 f (x; I) 2 ,(8)", "formula_coordinates": [4.0, 346.41, 686.55, 198.7, 21.8]}], "doi": ""}