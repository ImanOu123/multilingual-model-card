{"title": "Karde\u015f-NLU: Transfer to Low-Resource Languages with the Help of a High-Resource Cousin -A Benchmark and Evaluation for Turkic Languages", "authors": "L\u00fctfi Kerem\u015fenel; Benedikt Ebing; Konul Baghirova; Hinrich Sch\u00fctze; Goran Glava\u0161", "pub_date": "", "abstract": "Cross-lingual transfer (XLT) driven by massively multilingual language models (mmLMs) has been shown largely ineffective for lowresource (LR) target languages with little (or no) representation in mmLM's pretraining, especially if they are linguistically distant from the high-resource (HR) source language. Much of the recent focus in XLT research has been dedicated to LR language families, i.e., families without any HR languages (e.g., families of African languages or indigenous languages of the Americas). In this work, in contrast, we investigate a configuration that is arguably of practical relevance for more of the world's languages: XLT to LR languages that do have a close HR relative. To explore the extent to which a HR language can facilitate transfer to its LR relatives, we (1) introduce Karde\u015f-NLU, 1 an evaluation benchmark with language understanding datasets in five LR Turkic languages: Azerbaijani, Kazakh, Kyrgyz, Uzbek, and Uyghur; and (2) investigate (a) intermediate training and (b) fine-tuning strategies that leverage Turkish in XLT to these target languages. Our experimental results show that both-integrating Turkish in intermediate training and in downstream fine-tuning-yield substantial improvements in XLT to LR Turkic languages. Finally, we benchmark cutting-edge instruction-tuned large language models on Karde\u015f-NLU, showing that their performance is highly task-and language-dependent.", "sections": [{"heading": "Introduction", "text": "Transformer-based massively multilingual language models (mmLMs), such as mBERT (Devlin et al., 2019), XLM-R (Conneau et al., 2020a), and mT5 (Xue et al., 2021), have substantially advanced multilingual NLP. These models have enabled rapid development of language technologies * These authors contributed equally. 1 https://github.com/lksenel/Kardes-NLU for a wide range of low-resource (LR) languages by means of cross-lingual transfer (XLT) from highresource (HR) languages, using zero-shot (Wu and Dredze, 2019;Karthikeyan et al., 2020) or few-shot transfer techniques (Lauscher et al., 2020;Schmidt et al., 2022). mmLMs are, however, biased towards HR languages and XLT with mmLMs yields especially poor transfer performance for LR target languages that are (i) underrepresented in mmLMs' pretraining corpora and (ii) linguistically distant from the source language (Lauscher et al., 2020). Besides these reasons, such poor XLT is also a consequence of the curse of multilinguality (Conneau et al., 2020a;Pfeiffer et al., 2022), i.e., a reduced representational quality of supported languages, stemming from mmLMs' parameters being shared by many linguistically diverse languages.\nIn recent years, a large body of work focused on improving XLT abilities of mmLMs, ranging from models that aim to better align representation subspaces of source and target language with crosslingual supervision Conneau et al., 2020b;Minixhofer et al., 2022;Wang et al., 2022) to those that improve the mmLMs' representational capacity for individual, mostly LR languages (Pfeiffer et al., 2020;Parovi\u0107 et al., 2022;Ansell et al., 2021;Pfeiffer et al., 2022). At the same time, an incredible amount of effort has also been dedicated to the creation of new multilingual evaluation benchmarks that either encompass sets of linguistically diverse languages (Clark et al., 2020;Ponti et al., 2020; or focus on LR languages (Adelani et al., 2021;Muhammad et al., 2022;Ebrahimi et al., 2022;Armstrong et al., 2022;Winata et al., 2023;Khanuja et al., 2023, inter alia). The vast majority of existing work, however, assumes (i) zero-shot downstream transfer from (ii) English as the source. That is primarily because, on the one hand, for most tasks, training data is only available in English. On the other hand, many of the recent benchmarks cover LR language families, i.e., families without any HR languages (e.g., some African language families or indigenous languages of the Americas): this prevents the creation of high-quality silver-standard training data in a (closely) related HR language (e.g., via machine translation (MT)), as no such language exists. Contributions. 1) In this work, we contribute to the body of evaluation resources for LR XLT with Karde\u015f-NLU, 2 an evaluation benchmark covering three natural language understanding (NLU) tasks-natural language inference (NLI), semantic text similarity (STS), and commonsense reasoning, in particular choice of plausible alternatives (COPA)-for five Turkic languages-Azerbaijani (az), Kazakh (kk), Kyrgyz (ky), Uyghur (ug), and Uzbek (uz). We focus on Turkic languages because, unlike most concurrent work, we aim to explore a highly underinvestigated XLT research question: to what extent can LR languages that do have a linguistically and genealogically (close) HR relatives profit from those relatives (Snaebjarnarson et al., 2023). 2) We extend a number of established (i) intermediate training and (ii) finetuning approaches (covering both zero-shot and few-shot XLT) for improving LR XLT by incorporating Turkish as the HR sibling of the Karde\u015f-NLU languages; and show that the mixture of incorporating Turkish in intermediate training and in task-specific fine-tuning results in substantial performance gains. 3) Given the praised generalization abilities of large instruction-based language models (LLMs) (Chung et al., 2022;Ahuja et al., 2023;Asai et al., 2023), we additionally evaluate (zeroshot) two multilingual LLMs on Karde\u015f-NLU-the open mT0 (Muennighoff et al., 2023) and commercial ChatGPT-showing that their performance is highly task-and language-dependent and in some cases substantially trails that of XLT with traditionally fine-tuned \"small\" mmLMs.", "publication_ref": ["b15", "b12", "b50", "b22", "b24", "b24", "b12", "b35", "b14", "b28", "b46", "b36", "b34", "b4", "b35", "b11", "b37", "b31", "b5", "b49", "b43", "b3", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "Karde\u015f-NLU Benchmark", "text": "Language and Task Selection. We selected languages for Karde\u015f-NLU based on two criteria: (i) linguistic and genealogical diversity within the Turkic language family and (ii) availability of native speakers of those languages who are also fluent in English. 3 Our final selection contains five languages from the Common Turkic branch, covering three different sub-branches: Western Oghuz languages (Azerbaijani; Turkish, as the HR language in our experiments, also belongs to this branch), Kipchak languages (Kazakh and Kyrgyz) and Karluk languages (Uzbek and Uyghur). Moreover, Karde\u015f-NLU covers languages with two different scripts: Latin (Azerbaijani and Uzbek) and Cyrillic (Kazakh, Kyrgyz, and Uyghur). 4 We select three tasks that are (i) among the most prominent NLU tasks, included in popular NLU benchmarks (Wang et al., 2018(Wang et al., , 2019, and (ii) already have existing evaluation datasets in a number of languages (commonly translations of an original English dataset): NLI (Conneau et al., 2018;Aggarwal et al., 2022;Ebrahimi et al., 2022), STS (Cer et al., 2017), andCOPA (Gordon et al., 2012;Ponti et al., 2020).\nDataset Translation. We adopt a widely used twostep translation approach to obtain translations in which a native speaker of the target language, fluent in English, post-edits the output of MT. 5 This way, we translated English instances from the following datasets: XNLI (Conneau et al., 2018) (2000 instances from the test portion and 1000 instances from the validation portion), STS-Benchmark (Cer et al., 2017) (800 test instances and 200 validation instances), and XCOPA (Ponti et al., 2020) (500 test instances and 100 validation instances). We initially manually compared, on a small subsample of instances from all three datasets, translation (i) with Google Translate (GT) vs. the open Turkic Interlingua MT models (Mirzakhalov et al., 2021) and (ii) from English vs. from Turkish (with Turkish instances that were, in turn, machine translated from English) and have found that GT from English produces the best output. Due to MT in the first step, we instructed the annotators to pay special attention to the idiomaticity of the source English sentences during post-editing. This particularly refers to finding suitable translations for culture-specific concepts that do not have a direct translation (e.g., \"passing for white\" has no direct translation in our target languages since racial passing is not a native concept in respective cultures). Table 1 displays several instances from Karde\u015f-NLU.\nAnnotation Costs. Given the high post-editing costs, Karde\u015f-NLU contains only subsets of the original English development and test portions of STS-B and XNLI. All of our annotators were university students who were paid the equivalent of 14$ per hour for their effort. On average, postediting took 92 hours per language, bringing the total cost of creating Karde\u015f-NLU to 6,440$.", "publication_ref": ["b45", "b44", "b13", "b2", "b9", "b37", "b13", "b9"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Karde\u015f Transfer: Leveraging Turkish", "text": "We next attempt to improve XLT to LR Karde\u015f-NLU languages by explicitly incorporating Turkish as the close HR relative into the process. We try to (1) increase mmLMs' capacity for the target languages as well as their alignment with Turkish via intermediate LM training and (2) leverage Turkish as an additional source language in downstream zero-shot and few-shot transfer.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Intermediate Language Modeling", "text": "Adapting pretrained mmLMs to target distributions-different languages, domains, or datasets-through further LM-ing can bring significant performance gains (Howard and Ruder, 2018;Gururangan et al., 2020;Muller et al., 2021;Wang et al., 2022;Hung et al., 2022). Building upon these findings, we investigate the benefit of additional LM-ing in transfer to LR Karde\u015f-NLU languages. Specifically, we explore the potential benefits of incorporating Turkish into the mmLM adaptation process and the extent to which this inclusion can improve the downstream performance for LR Turkic languages. We experiment with three different intermediate training strategies detailed below: in all cases, we (1) use the standard masked language modeling (MLM) as the training objective and (2) update all of the mmLM's pretrained weights.\nTarget Language LM-ing (TLLM). In this case, we perform additional MLM-ing only on the limited-size corpora of the target language. Turkish, as the HR relative, is not leveraged in TLLM.\nBilingual Alternating LM-ing (BALM). Here we alternately update the mmLM by MLM-ing on one batch of target language data, followed by one batch of Turkish data. BALM is similar to the bilingual training procedure of Parovi\u0107 et al. (2022): they, however, opt for parameter-efficient training with adapters, whereas we update all of the mmLM's parameters.\nBilingual Joint LM-ing (BJLM). Like BALM, in BJLM we perform bilingual MLM-ing on both the LR target language and the related HR language (Turkish). However, while in BALM monolingual batches are alternated, in BJLM batches are bilingual, i.e., they consist of instances of both languages. Importantly, both languages have the same number of instances in each batch (i.e., B/2 with B as the batch size). Although such balancing leads to frequent repetition of instances from the LR language corpus, these repeating instances are, in different batches, \"regularized\" with different source-language instances, which prevents overfitting to small-sized corpora of LR languages. Schmidt et al. (2022) demonstrate the effectiveness of BJLM in task-specific few-shot fine-tuning; here, we test it in intermediate MLM-ing.\nParameter-Efficient LM-ing. Besides full finetuning, we also carried out intermediate training (for TLLM and BALM) in a parameter-efficient manner with adapters (Houlsby et al., 2019) in the vein of prior work on XLT (Pfeiffer et al., 2020;Parovi\u0107 et al., 2022). Adapter-based variants yielded consistently weaker performance compared to tuning all mmLM's parameters. For brevity, we report these results in the Appendix ( \u00a7C).", "publication_ref": ["b17", "b32", "b46", "b20", "b34", "b16", "b36", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "Downstream Cross-Lingual Transfer", "text": "We investigate two common setups for downstream cross-lingual transfer: (1) zero-shot XLT, in which we assume that we do not have any labeled task instances in the target language, and (2) few-shot transfer, in which a small number of labeled instances in the target language exists. We follow the fair XLT evaluation procedure of Schmidt et al. (2022), which does not allow for model selection based on target-language validation data. Relying on target-language validation violates the assumption of true zero-shot XLT. Moreover, Schmidt et al. (2022Schmidt et al. ( , 2023a show that any labeled targetlanguage instances are better leveraged for training. We thus use the validation portions of Karde\u015f-NLU", "publication_ref": ["b41"], "figure_ref": [], "table_ref": []}, {"heading": "Language", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Task", "text": "Instance Label Azerbaijani NLI Premise: B\u00fct\u00fcn hallarda m\u00fc\u015ft@rinin iddialar\u0131na x@l@l g@tirm@m@k \u00fc\u00e7\u00fcn m\u00fch\u00fcm add\u0131mlar at\u0131lmal\u0131d\u0131r. Neutral (In all cases, significant steps would have to be taken to avoid prejudicing the client's claims.) Hypothesis: Bu add\u0131mlara m\u00fc\u015ft@ril@rin h@qiqi\u015f@xsiyy@tinin m\u00fcst@ntiql@rd@n gizl@dilm@si daxildir (These steps include hiding the real identity of clients from investigators.)  only for training in few-shot XLT.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Kazakh", "text": "Zero-Shot Transfer. We explore three zero-shot XLT setups: (i) monolingual training on English data, (ii) monolingual training on Turkish data, machine translated from the original English training data, and (iii) bilingual training on both English and machine-translated Turkish data, with joint bilingual batches.\nFew-Shot Transfer. In few-shot fine-tuning, we additionally train on a small number of instances in the target language. We evaluate two different few-shot fine-tuning strategies: (1) in sequential transfer (Lauscher et al., 2020;Zhao et al., 2021), large(r)-scale fine-tuning on data from the source language(s)-in our case, English, Turkish, or bilingually English and Turkish-is followed by efficient target-language fine-tuning on the few shots;\n(2) in joint fine-tuning, we follow Schmidt et al. (2022) and, after initial source-only training, interleave source-and target-language instances at the batch level-the final batch loss is then the macroaverage of the language-specific losses. Note that this results in joint trilingual fine-tuning when the source datasets are both English and Turkish.  gether with the size of their corresponding monolingual corpora in CC-100. 7 The sizes of the Turkish Wikipedia and Turkish CC-100 portions are 631MB and 5.4GB, respectively. Table 2 additionally shows the average number of tokens in test instances after XLM-R tokenization. Uyghur yields substantially more tokens than the other four languages. This is because most of Uyghur's pretraining corpus in XLM-R's is in the Arabic script, whereas Uyghur instances in Karde\u015f-NLU are written in Cyrillic.\nIn downstream XLT, we use the existing training data in English and respective automatic translations to Turkish. For NLI, we train on MNLI  and (automatically translated) Turkish training data from XNLI (Conneau et al., 2018). For STS, we train on the English training portions of STS-B (Cer et al., 2017) and its existing (automatic) translation to Turkish. 8 Due to the small size of the English training data for COPA (400 instances) (Gordon et al., 2012), reported to hinder convergence of mmLM-based models (Sap et al., 2019;Ponti et al., 2020), we follow this prior work and first fine-tune on (English) SocialIQa (SIQA)-a closely related causal commonsense reasoning dataset (Sap et al., 2019) before finetuning on (English and/or Turkish) COPA data 9 .\nIntermediate Training Details. In all our main experiments, we use XLM-R (Base size) (Conneau et al., 2020a) as our mmLM. For the bilingual intermediate training procedure (e.g., BALM and BJLM), we train for a full epoch on Turkish Wikipedia: this results in multiple passes over the target language Wikipedias, given that those are substantially smaller. Thus, in the interest of fair evaluation, we train TLLM for multiple epochs: 2 for Azerbaijani and Kazakh, 5 for Kyrgyz and Uzbek, and 18 for Uyghur. We set the batch size to 32 and limit the sequence length to 128 tokens. We use the AdamW optimizer (Loshchilov and Hutter, 2019) with a fixed learning rate of 5e\u22125.\nDownstream Training Details. We adopt standard fine-tuning and add a task-specific classifier on top of the mmLM. Unless explicitly said otherwise, we perform full fine-tuning updating all parameters of the encoder together with the classifier's parameters. For NLI and STS, we encode the pair of sentences with the mmLM and feed the transformed representation of the [CLS] token to the classifier. For the multiple-choice tasks-COPA and SIQA (which we use as a \"pre-fine-tuning\" task to stabilize COPA training)-we face a varying number of answer choices per dataset (i.e., there are 3 possible answers in SIQA and 2 in COPA). We follow prior work Sap et al. 2019;Ponti et al. 2020 and encode the premise together with each answer choice. We feed the resulting output [CLS] token into a feed-forward regressor that produces a single score for each answer choice. Afterwards, the individual scores of all choices are concatenated and fed to the softmax classifier.\nWe train the models for 10 epochs with mixed precision using AdamW (Loshchilov and Hutter, 2019) with a weight decay of 0.05 and the initial learning rate set to 2e\u22125. We use a linear scheduler with 10% linear warm-up and decay. We deviate from this configuration (i) in the joint few-shot fine-tuning, where we train for 50 epochs without a scheduler, following recommendations of (Schmidt et al., 2022), and (ii) for all NLI experiments, where we train for 5 epochs due to the size of the MNLI training data (ca. 400K instances). The sequence length is limited to 128 tokens for all tasks, matching the input size of the intermediate MLM-ing. We fine-tune with a batch size of 32, except in the trilingual joint few-shot fine-tuning (English-Turkishtarget language), where we sample 10 instances per language (i.e., batch size 30). For each experiment, we execute three runs with different random seeds and report the average performance (accuracy for NLI and COPA and Pearson correlation for STS). In zero-shot XLT, we report the performance of the last checkpoint obtained at the end of the training. In few-shot XLT, we start training from the last snapshot of the source training (English, Turkish, or English and Turkish) and select the last snapshot of the second-sequential or joint-training step.", "publication_ref": ["b24", "b53", "b13", "b9", "b39", "b37", "b39", "b27", "b39", "b37", "b27"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Results and Discussion", "text": "Zero-Shot Transfer. Table 3 displays the zeroshot XLT performance for all five Karde\u015f-NLU languages on NLI, COPA and STS. Generally, we reach the best performance when Turkish is integrated into both intermediate training (rows BALM and BAJM) and as the source language in fine-tuning (columns TR and EN,TR). On average, across all five languages, BJLM combined with source fine-tuning on concatenated English and Turkish instances (EN,TR) yields a 6.6% and 2.1% boost over zero-shot XLT from English only with the vanilla XLM-R (Base) on NLI and COPA, respectively. On these two tasks, this observation holds for all individual languages except Kazakh. The gains over the vanilla zero-shot XLT for STS, however, are much smaller, with only BALM combined with English and Turkish fine-tuning surpassing the default zero-shot XLT performance of XLM-R (Base, EN) and that by a narrower margin (+0.6). We speculate that this is because (i) fine-grained sentence similarity is more sensitive to slight semantic misalignment and (ii) while our bilingual intermediate training improves the semantic links between Turkish and the target language, it is not of an adequate scale to establish alignments of such semantic precision.\nIncluding Turkish as a fine-tuning source language (TR and EN,TR) brings consistent gains over transfer from English only, regardless of the intermediate training strategy. The best results are almost always obtained when we fine-tune on both English and Turkish (EN,TR): we hypothesize that such fine-tuning establishes task-specific representational associations between the two languages and allows the transfer to benefit from both (i) XLM-R's unmatched representational quality for English and (ii) proximity of Turkish to the target languages. The effect is then further amplified when intermediate training (BALM and BJLM) increases the XLM-R's capacity for Turkish and the target language and strengthens the alignments between them. This is confirmed by the fact that intermediate training on the target language alone (TLLM) brings downstream gains (compared to Base) for NLI but not for the other two tasks.\nLooking at individual languages, we observe the least (and smallest) gains for Azerbaijani and Kazakh, the two most-resourced Karde\u015f-NLU languages, and the most (and largest) gains for the three less-resourced languages: Uyghur, Uzbek, and Kyrgyz (e.g., compared to Base transfer from EN on NLI, BJLM with transfer from EN,TR leads to gains of 5.0% for Kyrgyz, 5.1% for Uzbek, and 17.2% for Uyghur). We see the largest gains (by a wide margin) for Uyghur, despite the script mismatch between the intermediate training (Arabic script) and evaluation (Uyghur in Cyrillic script). The intermediate bilingual training for Uyghur, which improves representations of Arabic-script tokens, would thus likely yield even larger gains if the Uyghur test instances were in the Arabic script.\nFew-Shot Transfer. Table 4 summarizes the fewshot XLT results. We observe mixed results compared to the strongest zero-shot approaches: while there is a small improvement on STS (+1.0% ), we see virtually no gains for COPA (+0.1%) and NLI (-0.3%). Consistent with zero-shot XLT findings, few-shot XLT yields best results when we start the few-shot target language training from models trained on both English and Turkish (EN,TR). Additionally, we observe that few-shot XLT with models that were intermediately trained on Turkish and the target languages (BALM, BAJM) yields stronger performance than with those MLM-ed on the target language alone (TLLM). Nonetheless, there is no bilingual intermediate training strategy that is consistently best: BJLM yields better scores on COPA, whereas BALM reaches better STS per-formance; on NLI, both strategies perform comparably. Concerning the number of target language shots, we observe that we typically need at least 50 shots to match or surpass the zero-shot XLT performance. Comparing few-shot transfer procedures, we observe task-dependent variability. On NLI, sequential fine-tuning substantially outperforms the joint approach. Conversely, on COPA and STS, joint few-shot transfer shows better performance, with a more pronounced gap on STS.\nKarde\u015f-NLU: A Difficult Few-Shot XLT Benchmark. Not only does the comparison of zero-shot and few-shot results in Table 4 render Karde\u015f-NLU as a difficult few-shot XLT benchmark but also does Karde\u015f-NLU involve two tasks-STS and COPA-that are underrepresented in the current body of work on (few-shot) XLT (Lauscher et al., 2020;Zhao et al., 2021;Schmidt et al., 2022). This makes Karde\u015f-NLU a valuable evaluation resource for XLT research.\nInstruction-Based LLMs on Karde\u015f-NLU. Given the recent popularity of instruction-tuned LLMs as competent \"generalizers\" (Ouyang et al., 2022;Ahuja et al., 2023), we additionally evaluate (zeroshot) two state-of-the-art multilingual LLMs on Karde\u015f-NLU: 10 mT0 (Muennighoff et al., 2023), as the open model tuned on instructions derived from NLP tasks, and ChatGPT, as the commercial model tuned from human instructions and feedback. To this end, we slightly modify the instructions and prompts proposed by Ahuja et al. (2023): we provide further details in the Appendix \u00a7A.\nFigure 1 compares the best zero-shot XLT performance (based on XLM-R) for each language from Table 3 against zero-shot inference with mT0 and ChatGPT. The NLI results, in which both LLMs dramatically underperform our language-adapted zero-shot XLT (-23.9% and -15.1% for ChatGPT and mT0, respectively), diametrically oppose those on COPA, where both LLMs (and especially mT0) excel and surpass our best zero-shot XLT (the gap is full 10% in favor of mT0, albeit only 1.1% for ChatGPT). We believe that this is because mT0 was instruction-tuned, multilingually, on a large number of different multi-choice QA datasets (including, e.g., SIQA  1 62.1 61.5 55.7 55.8 56.1 57.5 59.7 58.9 49.9 50.3 49.3 62.9 63.2 62.5 57.6 58.2 57.7  BALM 57.2 58.3 59.4 59.1 59.5 59.7 56.1 59.9 59.1 51.1 53.9 52.5 60.5 61.7 61.9 56.8 58.6 58.5  BJLM 61.8 63.3 63.3 58.4 58.6 57.7 56.8 61.5 62.0 50.9 52.2 53.9 61.7 60.5 62.9 57.9 59.2 8 75.5 78.1 80.6 80.1 81.9 71.3 71.8 74.2 70.6 69.3 71.3 70.6 67.0 76.9 73.8 72.7 76.5  BALM 72.7 78.7 79.7 81.4 83.2 83.9 71.1 77.3 78.3 72.8 72.3 73.5 72.5 77.6 79.3 74.1 77.8 79   Table 4: Results of sequential and joint few-shot XLT on Karde\u015f-NLU: performance with 10, 50, and 100 targetlanguage shots. The best zero-shot result per task is shown in bold, the best few-shot result is underlined. The evaluation metrics are accuracy (%) for NLI and COPA, and Pearson correlation for STS. pairs, has a weaker inductive bias for both COPA and NLI. The two LLMs yield the best performance on both tasks for Azerbaijani, the most resourced language in Karde\u015f-NLU-the performance drops for the remaining languages are drastic, especially for ChatGPT. This is in line with findings from concurrent work (Ahuja et al., 2023;Asai et al., 2023) and shows that even the largest instruction-tuned LLMs are bound by the language distribution of their (pre)training data, indicating that there is still a long way to go to enable truly multilingual NLP.", "publication_ref": ["b24", "b53", "b33", "b3", "b30", "b3", "b3"], "figure_ref": ["fig_0"], "table_ref": ["tab_6", "tab_6", "tab_0", "tab_0"]}, {"heading": "Related Work", "text": "Multilingual Evaluation Benchmarks. Reliable evaluation of the multilingual abilities of mmLMs requires that they are tested against a large set of diverse languages (Joshi et al., 2020). On the one hand, multilingual benchmarks that encompass many tasks, such as XGLUE (Liang et al., 2020) and XTREME (Hu et al., 2020;, comprise diverse but predominantly highly or moderately resourced languages: their coverage of LR languages is small and varies across tasks. On the other hand, many recent efforts introduce dedicated benchmarks for specific families of LR languages (Armstrong et al., 2022;Adelani et al., 2022;Ebrahimi et al., 2022;Winata et al., 2023, inter alia). While these target truly underrepresented languages, they typically focus on a single task only, e.g., NLI or NER. With Karde\u015f-NLU we, (i) cover multiple languages from an underrepresented language family while (ii) including various tasks (NLI, COPA, and STS) that require different degrees of precision in language understanding.\nCross-Lingual Transfer with mmLMs. mmLMs still play an important role in multilingual NLU and XLT, exhibiting good performance in zero-shot XLT (Wu and Dredze, 2019;Hu et al., 2020) to HR languages. They, however, perform much worse in XLT to LR languages distant from English (as the common source). The body of work on improving XLT is threefold. The first line of work seeks to improve XLT via post-hoc alignment of representational subspaces of individual languages, guided by parallel data Conneau et al., 2020b;Wang et al., 2022;Minixhofer et al., 2022, inter alia) and driven by crosslingual supervision. These efforts, however, offer little gain for LR languages, whose representational subspaces are of low semantic quality, to begin with. The second line of work seeks to improve the representational quality for LR languages through additional language modeling training (Pfeiffer et al., 2020;Ansell et al., 2021;Parovi\u0107 et al., 2022;Pfeiffer et al., 2022), resulting in moderate downstream performance gains. Finally, the third line of work (Lauscher et al., 2020;Zhao et al., 2021;Xu and Murray, 2022;Schmidt et al., 2022Schmidt et al., , 2023a focuses on the actual downstream transfer, rather than the task-agnostic adaptation of mmLMs, investigating how to best utilize the limited number of annotated task-specific target-language instances (Lauscher et al., 2020;Schmidt et al., 2022Schmidt et al., , 2023a or tailor source-language instances to resemble target language ones (Xu and Murray, 2022).\nIn this work, we adopt the latter two ideas and seek to improve XLT to Turkic LR languages via both intermediate LM-ing and few-shot XLT: unlike most existing work, however, we seek to lever-age a close HR language (Turkish) to facilitate the transfer. The work of Snaebjarnarson et al. ( 2023) is conceptually most similar; they, however, target a single LR language (Faroese) from a HR family (Germanic branch of the Indo-European family) with many HR relatives (Scandinavian languages).\nThe three mentioned lines of work typically propose methods to improve XLT starting from a single, given source language (usually EN). Complementary to these lines of work, the work of Lin et al. (2019) and Glava\u0161 and Vuli\u0107 (2021) instead focus on identifying the best source languages to transfer from for a given target language. Their work considers linguistic and dataset related factors beyond the sole language family. Their findings are complementary to our work, suggesting that even for LR languages that do not have a closely related HR language within their family, it might still be possible to infer such a closely related HR language from another language family.", "publication_ref": ["b21", "b25", "b19", "b5", "b31", "b50", "b19", "b14", "b46", "b28", "b36", "b4", "b34", "b35", "b24", "b53", "b51", "b41", "b24", "b41", "b51", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "In this work, we contribute to the body of evaluation resources for low-resource (LR) cross-lingual transfer (XLT) by introducing Karde\u015f-NLU, an evaluation benchmark covering three NLU tasks (NLI, STS, and COPA)-for five Turkic languages: Azerbaijani, Kazakh, Kyrgyz, Uyghur, and Uzbek. Karde\u015f-NLU allows investigation of an understudied XLT approach: leveraging a high-resource (HR) language to improve transfer to linguistically and genealogically related LR languages. We extend existing intermediate training and fine-tuning approaches for improving LR XLT to integrate Turkish as the HR \"sibling\" of the Karde\u015f-NLU languages. Through comprehensive experimentation and analysis, we demonstrated that adding Turkish in task-specific fine-tuning can provide significant XLT gains for Karde\u015f-NLU languages that are further amplified by incorporating Turkish in bilingual intermediate training strategies. What is more, we also find that Karde\u015f-NLU is a difficult benchmark for few-shot XLT, observing that established fewshot transfer methods are not effective. Finally, we evaluated two cutting-edge instruction-tuned large language models-mT0 and chatGPT-on Karde\u015f-NLU, showing that their (zero-shot) performance is inferior on lower-resourced Karde\u015f-NLU languages (Uyghur, Uzbek, Kyrgyz) and greatly varies across tasks. This proves that there is still a long way to (truly) multilingual NLP. In our subsequent efforts, we will not only seek to extend Karde\u015f-NLU with additional LR Turkic languages, but also explore how to leverage HR siblings in LR XLT for other language families.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "We strove for both a representative NLU benchmark for Turkic languages and a comprehensive study of XLT to LR target languages with the help of a closely related HR language. Nonetheless, our work is limited in several aspects. Out of 23 live Turkic languages, Karde\u015f-NLU covers only five. Two main factors determined the set of initially included languages: a limited annotation budget and the ability to find native speakers. The latter is why we ended up with languages that are among the largest Turkic languages in terms of number of native speakers (Kyrgyz, as the smallest, has ca. 5M native speakers). Further, there is a mismatch between the more common Arabic script used for Uyghur and the Cyrillic script we use for it in Karde\u015f-NLU because our Uyghur annotator was unfamiliar with the Arabic script.\nThe Karde\u015f-NLU benchmark is obtained through automatic translations from the existing English test sets to the target languages. This is followed by manual annotation and curation through native speakers to ensure high quality. In order to have suitable translations for culture specific concepts, we instructed our annotators to pay special attention to the idiomaticity of the English sentences during the editing. Despite our best efforts, the resulting datasets might not perfectly reflect the cultural and social elements of the target lowresource languages since their content is tied to original English datasets.\nNext, we employed Wikipedias as corpora for our intermediate pretraining. Albeit curated, Wikipedia content is subject to biased, missing or simply incorrect information that can lead to undesired behavior in the resulting models.\nConcerning the methodology, we limited our study exclusively to mainstream approaches: (i) intermediate LM-ing for improving the representational quality of mmLMs for a language of interest and (ii) established protocols for downstream zero-shot and few-shot XLT. We acknowledge the existence of more sophisticated (and more recent) XLT methods based, e.g., on gradient manipulation (Wang and Tsvetkov, 2021;Xu and Murray, 2022) or dedicated representational alignment of lexical units (i.e., embedding spaces) (Minixhofer et al., 2022). We hope the research community will use Karde\u015f-NLU to evaluate and profile existing and future state-of-the-art XLT approaches.\nFinally, for the prompt-based evaluation of LLMs, we experiment only with a single instruction (i.e., prompt) adapted from Ahuja et al. (2023). It is reasonable to expect that some prompt engineering effort yields better results. For ChatGPT, we slightly modify the prompts from Ahuja et al. (2023) due to the fact that they perform in-context few-shot learning, whereas we carry out zero-shot prediction: NLI. You are an NLP assistant whose purpose is to solve Natural Language Inference (NLI) problems. NLI is the task of determining the inference relation between two (short, ordered) texts. For the given two sentences, you need to predict one of the following: 1. Entailment, 2. Contradiction, or 3. Neither (Neutral). Sentence 1: {PREMISE}. For NLI, the model's output is compared directly against the target label (True, False, or Neither). For COPA, it is compared against the correct alternative ({CHOICE1} or {CHOICE2}). Since the models are free to generate any text, they can theoretically perform below the random baseline (33% for NLI and 50% for COPA).\nTable 5 displays per language and average results for zero-shot evaluations on NLI and COPA for the XLM-R base versions that we experiment with, mT0 of various sizes, and ChatGPT. We also experiment with the templates that are translated to the target language using Google Translate. However, those versions overall performed worse than the English versions, most likely because of the low translation quality. We can see that mT0's performance on COPA improves drastically when it is scaled to XL and XXL versions. It should be noted that mT0's instruction tuning dataset includes the Social IQA dataset, which is similar to the COPA dataset. This might explain the larger model's strong performance on this dataset outperforms zero-shot XLM-R variants.", "publication_ref": ["b47", "b51", "b28", "b3", "b3"], "figure_ref": [], "table_ref": ["tab_11"]}, {"heading": "B Computational Resources", "text": "All the experiments were run on a single V100 with 32GB VRAM. We roughly estimate that total GPU time accumulates to 2800 hours across all experiments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C Adapter Fine-Tuning Experiments", "text": "In preliminary experiments, we investigated the adapter-based equivalents to TLLM and BALM (on STS and NLI) (Pfeiffer et al., 2020;Parovi\u0107 et al., 2022). We report per-language and averaged scores in Table 6. Full fine-tuning of the mmLM outperformed the adapter-based tuning, especially on lower-resourced languages.\nTarget Language LM-ing Adapters (TLLM-AD). We first train monolingual language adapters on target languages via MLM-ing. We then stack a task adapter on top and fine-tune it on the corresponding downstream data-English, Turkish or English and Turkish jointly-while keeping the language adapter frozen.\nBilingual Alternating LM-ing Adapters (BALM-AD). Here, we stick to Parovi\u0107 et al. 2022 and update the language adapter\u00b4s parameters alternately by one batch on the target language data followed by one batch on Turkish data. Afterwards, we fine-tune task adapters on either English, Turkish or English and Turkish jointly, while keeping the language adapter frozen.\nAdapter Training Details. We trained monolingual language adapters for 25000 steps and bilingual ones for 50000. We set the learning rate to 1e\u22124 and the batch size to 64. For task adapters, we applied the same hyperparameters used for our full fine-tuning experiments explained in section 4 but lowered the learning rate to 1e\u22124, as suggested by Pfeiffer et al. 2020.   Table 6: Zero-Shot XLT results on Karde\u015f-NLU (NLI and STS) for two adapter strategies (TLLM-AD and BALM-AD) and source fine-tuning datasets (English only, Turkish only, and English and Turkish combined). The best results for each language-task pair are shown in bold.", "publication_ref": ["b36", "b34", "b34", "b36"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "This work was partially supported by DFG (grant SCHU 2246/14-1).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": " ", "text": "3 79.0 79.2 75.7 77.7 77.8 75.7 78.7 79.3 76.9 79.1 78.9 76.4 77.0 76.7 77.8 77.7 77.2 78.0 78.3 78.2  BALM 77.3 79.0 79.2 75.4 77.2 77.3 76.5 78.1 78.1 76.7 78.9 79.2 74.8 76.0 76.3 78.0 78.4 78.1 77.6 77.5 78.0  BJLM 77.3 78.8 79.3 72.3 77.5 77.3 75.8 78.7 78.3 77.3 79.1 79.2 76.6 76.9 75.7 77.8 78.2 77.3 78.3 78.4 77", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "MasakhaNER 2.0: Africa-centric transfer learning 1680 for named entity recognition", "journal": "", "year": "", "authors": "David Adelani; Graham Neubig; Sebastian Ruder; Shruti Rijhwani; Michael Beukman; Chester Palen-Michel; Constantine Lignos; Jesujoba Alabi; Shamsuddeen Muhammad; Peter Nabende; M Cheikh; Andiswa Bamba Dione; Rooweither Bukula;  Mabuya; F P Bonaventure; Blessing Dossou; Happy Sibanda; Jonathan Buzaaba; Godson Mukiibi; Derguene Kalipe; Amelia Mbaye; Fatoumata Taylor; Chris Chinenye Kabore; Anuoluwapo Emezue; Perez Aremu; Catherine Ogayo;  Gitau"}, {"ref_id": "b1", "title": "", "journal": "Abdoulaye Diallo, Adewale Akinfaderin", "year": "", "authors": "Jade David Ifeoluwa Adelani; Graham Abbott;  Neubig; D Daniel; Julia 'souza; Constantine Kreutzer; Chester Lignos; Happy Palen-Michel; Shruti Buzaaba; Sebastian Rijhwani; Stephen Ruder;  Mayhew;  Israel Abebe Azime; H Shamsuddeen; Chris Chinenye Muhammad; Joyce Emezue; Perez Nakatumba-Nabende; Aremu Ogayo; Catherine Anuoluwapo; Derguene Gitau; Jesujoba Mbaye;  Alabi; Tajuddeen Seid Muhie Yimam; Ignatius Rabiu Gwadabe; Rubungo Andre Ezeani; Jonathan Niyongabo; Verrah Mukiibi; Iroro Otiende; Davis Orife; Samba David;  Ngom"}, {"ref_id": "b2", "title": "IndicXNLI: Evaluating multilingual inference for Indian languages", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Divyanshu Aggarwal; Vivek Gupta; Anoop Kunchukuttan"}, {"ref_id": "b3", "title": "Maxamed Axmed, Kalika Bali, and Sunayana Sitaram", "journal": "", "year": "2023", "authors": "Kabir Ahuja; Harshita Diddee; Rishav Hada; Millicent Ochieng; Krithika Ramesh; Prachi Jain; Akshay Nambi; Tanuja Ganu; Sameer Segal"}, {"ref_id": "b4", "title": "MAD-G: Multilingual adapter generation for efficient cross-lingual transfer", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Alan Ansell; Maria Edoardo; Jonas Ponti; Sebastian Pfeiffer; Goran Ruder; Ivan Glava\u0161; Anna Vuli\u0107;  Korhonen"}, {"ref_id": "b5", "title": "JamPatoisNLI: A jamaican patois natural language inference dataset", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Ruth-Ann Armstrong; John Hewitt; Christopher Manning"}, {"ref_id": "b6", "title": "", "journal": "Xinyan Velocity Yu", "year": "", "authors": "Akari Asai; Sneha Kudugunta"}, {"ref_id": "b7", "title": "Buffet: Benchmarking large language models for few-shot cross-lingual transfer", "journal": "", "year": "2023", "authors": "Sebastian Tsvetkov; Hannaneh Ruder;  Hajishirzi"}, {"ref_id": "b8", "title": "Multilingual alignment of contextual word representations", "journal": "", "year": "2020", "authors": "Steven Cao; Nikita Kitaev; Dan Klein"}, {"ref_id": "b9", "title": "SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Daniel Cer; Mona Diab; Eneko Agirre; I\u00f1igo Lopez-Gazpio; Lucia Specia"}, {"ref_id": "b10", "title": "Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models", "journal": "", "year": "", "authors": " Hyung Won; Le Chung; Shayne Hou; Barret Longpre; Yi Zoph; William Tay; Eric Fedus; Xuezhi Li;  Wang"}, {"ref_id": "b11", "title": "TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020", "authors": "Jonathan H Clark; Eunsol Choi; Michael Collins; Dan Garrette; Tom Kwiatkowski; Vitaly Nikolaev; Jennimaria Palomaki"}, {"ref_id": "b12", "title": "Unsupervised cross-lingual representation learning at scale", "journal": "", "year": "2020", "authors": "Alexis Conneau; Kartikay Khandelwal; Naman Goyal; Vishrav Chaudhary; Guillaume Wenzek; Francisco Guzm\u00e1n; Edouard Grave; Myle Ott; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b13", "title": "XNLI: Evaluating crosslingual sentence representations", "journal": "", "year": "2018", "authors": "Alexis Conneau; Ruty Rinott; Guillaume Lample; Adina Williams; Samuel Bowman; Holger Schwenk; Veselin Stoyanov"}, {"ref_id": "b14", "title": "Emerging cross-lingual structure in pretrained language models", "journal": "", "year": "2020", "authors": "Alexis Conneau; Shijie Wu; Haoran Li; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b15", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b16", "title": "Parameter-efficient transfer learning for nlp", "journal": "PMLR", "year": "2019", "authors": "Neil Houlsby; Andrei Giurgiu; Stanislaw Jastrzebski; Bruna Morrone; Quentin De Laroussilhe; Andrea Gesmundo; Mona Attariyan; Sylvain Gelly"}, {"ref_id": "b17", "title": "Universal language model fine-tuning for text classification", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Jeremy Howard; Sebastian Ruder"}, {"ref_id": "b18", "title": "Explicit alignment objectives for multilingual bidirectional encoders", "journal": "", "year": "2021", "authors": "Junjie Hu; Melvin Johnson; Orhan Firat; Aditya Siddhant; Graham Neubig"}, {"ref_id": "b19", "title": "XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation", "journal": "PMLR", "year": "2020", "authors": "Junjie Hu; Sebastian Ruder; Aditya Siddhant; Graham Neubig; Orhan Firat; Melvin Johnson"}, {"ref_id": "b20", "title": "DS-TOD: Efficient domain specialization for task-oriented dialog", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Chia-Chien Hung; Anne Lauscher; Simone Ponzetto; Goran Glava\u0161"}, {"ref_id": "b21", "title": "The state and fate of linguistic diversity and inclusion in the NLP world", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Pratik Joshi; Sebastin Santy; Amar Budhiraja; Kalika Bali; Monojit Choudhury"}, {"ref_id": "b22", "title": "Cross-lingual ability of multilingual BERT: An empirical study", "journal": "", "year": "2020", "authors": "Kaliyaperumal Karthikeyan; Zihan Wang; Stephen Mayhew; Dan Roth"}, {"ref_id": "b23", "title": "Evaluating the diversity, equity, and inclusion of NLP technology: A case study for Indian languages", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Simran Khanuja; Sebastian Ruder; Partha Talukdar"}, {"ref_id": "b24", "title": "From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Anne Lauscher; Vinit Ravishankar; Ivan Vuli\u0107; Goran Glava\u0161"}, {"ref_id": "b25", "title": "XGLUE: A new benchmark dataset for cross-lingual pre-training, understanding and generation", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Yaobo Liang; Nan Duan; Yeyun Gong; Ning Wu; Fenfei Guo; Weizhen Qi; Ming Gong; Linjun Shou; Daxin Jiang; Guihong Cao; Xiaodong Fan; Ruofei Zhang; Rahul Agrawal; Edward Cui; Sining Wei; Taroon Bharti; Ying Qiao; Jiun-Hung Chen; Winnie Wu; Shuguang Liu; Fan Yang; Daniel Campos; Rangan Majumder; Ming Zhou"}, {"ref_id": "b26", "title": "Antonios Anastasopoulos, Patrick Littell, and Graham Neubig", "journal": "", "year": "2019", "authors": "Yu-Hsiang Lin; Chian-Yu Chen; Jean Lee; Zirui Li; Yuyan Zhang; Mengzhou Xia; Shruti Rijhwani; Junxian He; Zhisong Zhang; Xuezhe Ma"}, {"ref_id": "b27", "title": "Decoupled weight decay regularization", "journal": "", "year": "2019", "authors": "Ilya Loshchilov; Frank Hutter"}, {"ref_id": "b28", "title": "WECHSEL: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Benjamin Minixhofer; Fabian Paischer; Navid Rekabsaz"}, {"ref_id": "b29", "title": "Orhan Firat, and Sriram Chellappan. 2021. A large-scale study of machine translation in Turkic languages", "journal": "Association for Computational Linguistics", "year": "", "authors": "Jamshidbek Mirzakhalov; Anoop Babu; Duygu Ataman; Sherzod Kariev; Francis Tyers; Otabek Abduraufov; Mammad Hajili; Sardana Ivanova; Abror Khaytbaev; Antonio Laverghetta; Esra Bekhzodbek Moydinboyev; Shaxnoza Onal; Ahsan Pulatova;  Wahab"}, {"ref_id": "b30", "title": "Crosslingual generalization through multitask finetuning", "journal": "", "year": "2023", "authors": "Niklas Muennighoff; Thomas Wang; Lintang Sutawika; Adam Roberts; Stella Biderman; Teven Le Scao; Sheng Bari;  Shen"}, {"ref_id": "b31", "title": "NaijaSenti: A Nigerian Twitter sentiment corpus for multilingual sentiment analysis", "journal": "", "year": "2022", "authors": "David Ifeoluwa Shamsuddeen Hassan Muhammad; Sebastian Adelani; Ibrahim Ruder; Idris Sa'id Ahmad;  Abdulmumin;  Bello Shehu; Monojit Bello; Chris Chinenye Choudhury;  Emezue; Anuoluwapo Saheed Salahudeen Abdullahi; Al\u00edpio Aremu; Pavel Jorge;  Brazdil"}, {"ref_id": "b32", "title": "When being unseen from mBERT is just the beginning: Handling new languages with multilingual language models", "journal": "", "year": "2021", "authors": "Benjamin Muller; Antonios Anastasopoulos; Beno\u00eet Sagot; Djam\u00e9 Seddah"}, {"ref_id": "b33", "title": "Training language models to follow instructions with human feedback", "journal": "Advances in Neural Information Processing Systems", "year": "2022", "authors": "Long Ouyang; Jeffrey Wu; Xu Jiang; Diogo Almeida; Carroll Wainwright; Pamela Mishkin; Chong Zhang; Sandhini Agarwal; Katarina Slama; Alex Ray"}, {"ref_id": "b34", "title": "BAD-X: Bilingual adapters improve zero-shot cross-lingual transfer", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Marinela Parovi\u0107; Goran Glava\u0161; Ivan Vuli\u0107; Anna Korhonen"}, {"ref_id": "b35", "title": "Lifting the curse of multilinguality by pre-training modular transformers", "journal": "", "year": "2022", "authors": "Jonas Pfeiffer; Naman Goyal; Xi Lin; Xian Li; James Cross; Sebastian Riedel; Mikel Artetxe"}, {"ref_id": "b36", "title": "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer", "journal": "", "year": "2020", "authors": "Jonas Pfeiffer; Ivan Vuli\u0107; Iryna Gurevych; Sebastian Ruder"}, {"ref_id": "b37", "title": "XCOPA: A multilingual dataset for causal commonsense reasoning", "journal": "", "year": "2020", "authors": "Goran Edoardo Maria Ponti; Olga Glava\u0161; Qianchu Majewska; Ivan Liu; Anna Vuli\u0107;  Korhonen"}, {"ref_id": "b38", "title": "XTREME-R: Towards more challenging and nuanced multilingual evaluation", "journal": "", "year": "2021", "authors": "Sebastian Ruder; Noah Constant; Jan Botha; Aditya Siddhant; Orhan Firat; Jinlan Fu; Pengfei Liu; Junjie Hu; Dan Garrette; Graham Neubig; Melvin Johnson"}, {"ref_id": "b39", "title": "Social IQa: Commonsense reasoning about social interactions", "journal": "", "year": "2019", "authors": "Maarten Sap; Hannah Rashkin; Derek Chen; Yejin Ronan Le Bras;  Choi"}, {"ref_id": "b40", "title": "2022. Don't stop fine-tuning: On training regimes for few-shot cross-lingual transfer with multilingual language models", "journal": "", "year": "", "authors": "Fabian David Schmidt; Ivan Vuli\u0107; Goran Glava\u0161"}, {"ref_id": "b41", "title": "Free lunch: Robust cross-lingual transfer 1683 via model checkpoint averaging", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Fabian David Schmidt; Ivan Vuli\u0107; Goran Glava\u0161"}, {"ref_id": "b42", "title": "One for all & all for one: Bypassing hyperparameter tuning with model averaging for crosslingual transfer", "journal": "", "year": "2023", "authors": "Fabian David Schmidt; Ivan Vuli\u0107; Goran Glava\u0161"}, {"ref_id": "b43", "title": "Transfer to a lowresource language via close relatives: The case study on Faroese", "journal": "", "year": "2023", "authors": "V\u00e9steinn Snaebjarnarson; Annika Simonsen; Goran Glava\u0161; Ivan Vuli\u0107"}, {"ref_id": "b44", "title": "Superglue: A stickier benchmark for general-purpose language understanding systems", "journal": "", "year": "2019", "authors": "Alex Wang; Yada Pruksachatkun; Nikita Nangia; Amanpreet Singh; Julian Michael; Felix Hill; Omer Levy; Samuel Bowman"}, {"ref_id": "b45", "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding", "journal": "", "year": "2018", "authors": "Alex Wang; Amanpreet Singh; Julian Michael; Felix Hill; Omer Levy; Samuel Bowman"}, {"ref_id": "b46", "title": "Expanding pretrained models to thousands more languages via lexicon-based adaptation", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Xinyi Wang; Sebastian Ruder; Graham Neubig"}, {"ref_id": "b47", "title": "Gradient vaccine: Investigating and improving multi-task optimization in massively multilingual models", "journal": "", "year": "2021", "authors": "Zirui Wang; Yulia Tsvetkov"}, {"ref_id": "b48", "title": "A broad-coverage challenge corpus for sentence understanding through inference", "journal": "Long Papers", "year": "2018", "authors": "Adina Williams; Nikita Nangia; Samuel Bowman"}, {"ref_id": "b49", "title": "NusaX: Multilingual parallel sentiment dataset for 10 Indonesian local languages", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Alham Genta Indra Winata; Samuel Fikri Aji; Rahmad Cahyawijaya; Fajri Mahendra; Ade Koto; Kemal Romadhony; David Kurniawan; Radityo Eko Moeljadi; Pascale Prasojo; Timothy Fung; Jey Han Baldwin; Rico Lau; Sebastian Sennrich;  Ruder"}, {"ref_id": "b50", "title": "Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT", "journal": "", "year": "2019", "authors": "Shijie Wu; Mark Dredze"}, {"ref_id": "b51", "title": "Por qu\u00e9 n\u00e3o utiliser alla spr\u00e5k? mixed training with gradient optimization in few-shot cross-lingual transfer", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Haoran Xu; Kenton Murray"}, {"ref_id": "b52", "title": "Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer", "journal": "Association for Computational Linguistics", "year": "", "authors": "Linting Xue; Noah Constant; Adam Roberts; Mihir Kale; Rami Al-Rfou; Aditya Siddhant"}, {"ref_id": "b53", "title": "A closer look at few-shot crosslingual transfer: The choice of shots matters", "journal": "", "year": "2021", "authors": "Mengjie Zhao; Yi Zhu; Ehsan Shareghi; Ivan Vuli\u0107; Roi Reichart; Anna Korhonen; Hinrich Sch\u00fctze"}, {"ref_id": "b54", "title": "Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing", "journal": "Online. Association for Computational Linguistics", "year": "", "authors": ""}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "STS Sent. 1 :1\u0411i\u0440 \u0430\u0434\u0430\u043c \u049b\u0430\u0437\u0430\u043d\u0493\u0430 \u043a\u04af\u0440i\u0448 \u0441\u043b\u0430\u044b\u043f \u0436\u0430\u0442\u044b\u0440. (A man pours rice into a pot.) 4.2 Sent. 2: \u0415\u0440 \u0430\u0434\u0430\u043c \u0442\u0430\u0431\u0430\u049b\u049b\u0430 \u043a\u04af\u0440i\u0448 \u0441\u0430\u043b\u044b\u043f \u0436\u0430\u0442\u044b\u0440. (A man is putting rice in a bowling pot.) Kyrgyz COPA Premise: \u041a\u044b\u0437 \u043a\u043e\u0434\u0434\u0443 \u0436\u0430\u0442\u0442\u0430\u043f \u043a\u0430\u043b\u0434\u044b. (The girl memorized the code.) Choice 1 Choice 1 (Cause): \u0410\u043b \u04e9\u0437\u04af\u043d\u04e9 \u04e9\u0437\u04af \u043e\u043a\u0443\u0434\u0443. (She recited it to herself.) Choice 2 (Cause): \u0410\u043b \u043c\u0443\u043d\u0443 \u0436\u0430\u0437\u0443\u0443\u043d\u0443 \u0443\u043d\u0443\u0442\u0443\u043f \u043a\u0430\u043b\u0434\u044b. (She forgot to write it down.) Uzbek STS Sent. 1: Okapi daraxtdan yemoqda. (An okapi is eating from a tree.) 0.3 Sent. 2: Sichqon suv purkagichdan ichadi. (A moose drinks from a sprinkler.) Uyghur COPA Premise: \u0414\u04d9\u0440\u04d9\u0445 \u0439\u043e\u043f\u0443\u0440\u043c\u0430\u049b\u043b\u0438\u0440\u0438\u043d\u0438 \u0442\u04e9\u043a\u0442\u0438. (The tree shed its leaves.) Choice 2 Choice 1 (Effect): \u0419\u043e\u043f\u0443\u0440\u043c\u0430\u049b \u0440\u04d9\u04a3\u0433\u0438\u0433\u04d9 \u0431\u043e\u044f\u043b\u0434\u0438. (The leaves turned colors.) Choice 2 (Effect): \u0419\u043e\u043f\u0443\u0440\u043c\u0430\u049b\u043b\u0430\u0440 \u0439\u04d9\u0440\u0433\u04d9 \u0439\u0438\u0493\u0438\u043b\u0438\u043f \u049b\u0430\u043b\u0434\u0438. (The leaves accumulated on the ground.)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": ".7 76.5 73.6 75.3 75.6 74.9 76.1 76.2 76.4 77.3 77.6 75.1 76.8 76.9 75.2 77.0 77.6 77.2 78.5 78.8 BALM 74.1 77.8 79.0 74.5 76.0 76.3 76.2 77.6 77.8 77.3 78.6 78.4 77.1 77.2 76.9 78.3 79.4 79.6 79.4 80.0 80.0 BJLM 70.9 75.8 77.3 72.8 74.9 75.4 75.2 76.9 76.8 76.1 77.7 78.1 74.0 76.2 76.6 76.8 78.3 78.5 77.9 79.3 79.4", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Examples from Karde\u015f-NLU one for each language and at least one for each task.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Experimental Setup Data. We carry out intermediate training for five Karde\u015f-NLU languages, monolingually (i.e., TLLM) or bilingually with Turkish (BALM and BAJM, see \u00a73.1) using Wikipedias of the respective languages. Table 2 summarizes the base statistics of Wikipedias of Karde\u015f-NLU languages, 6 to-", "figure_data": "azkkkyuguzscriptLatin Cyrillic Cyrillic Arabic Latinmonolingual corpus sizes (in bytes)CC-100 1.3G889M173M46M155MWiki315M 354M126M36M136MAvg no. tokens in test instances (XLM-R tokenizer)NLI4446477952COPA2224243426STS3436365640"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "TR EN TR EN,TR EN TR EN,TR EN TR EN,TR EN TR EN,TR EN TR EN,TR NLI Base 76.5 80.1 79.6 73.8 76.3 77.3 70.4 73.9 74.1 42.2 44.4 42.9 70.7 72.0 71.8 66.7 69.4 69.1 TLLM 77.3 79.0 79.2 75.3 76.3 76.8 72.4 74.1 74.4 56.7 57.1 56.9 73.1 74.3 74.8 71.0 72.2 72.4 BALM 77.3 78.8 79.3 74.4 75.3 77.0 71.6 73.4 74.0 57.4 58.7 58.0 73.1 74.5 75.0 70.8 72.1 72.7 BJLM 76.4 78.4 79.3 74.9 75.1 76.8 71.9 74.3 75.5 57.2 59.2 59.4 73.4 74.6 75.7 70.7 72.3 73.3", "figure_data": "AzerbaijaniKazakhKyrgyzUyghurUzbekAverageBase EN TR EN,COPA 60.1 61.1 60.9 60.7 60.8 59.9 59.7 60.0 59.4 51.8 52.7 52.7 57.3 59.5 60.1 57.9 58.8 58.6 TLLM 62.). ChatGPT, in contrast, beingfine-tuned based on open-ended instruction-reply"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Zero-Shot XLT results on Karde\u015f-NLU for three intermediate LM-ing strategies (TLLM, BALM, and BJLM) and source fine-tuning datasets (English only, Turkish only, and English and Turkish combined). The best results for each language-task pair are shown in bold. The evaluation metrics are accuracy (%) for NLI and COPA, and Pearson correlation for STS.", "figure_data": "Zero-ShotFew-ShotSequentialJointEN TR EN,TRENTREN,TRENTREN,TR"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "For mT0, we only use the instance-based prompts, without the task instruction, followingAhuja et al. (2023) (and accept exact matches as correct answers only): NLI. {PREMISE} Question: {HYPOTHESIS} True, False, or Neither? COPA. {PREMISE} {% if question == \"cause\" %} This happened because... {% else %} As a consequence... {% endif %} Help me pick the more plausible option: -{CHOICE1}-{CHOICE2}", "figure_data": "Abteen Ebrahimi, Manuel Mager, Arturo Oncevay, A LLMs: mT0 and ChatGPTVishrav Chaudhary, Luis Chiruzzo, Angela Fan, JohnOrtega, Ricardo Ramos, Annette Rios, Ivan VladimirMeza Ruiz, Gustavo Gim\u00e9nez-Lugo, ElisabethMager, Graham Neubig, Alexis Palmer, RolandoCoto-Solano, Thang Vu, and Katharina Kann. 2022.AmericasNLI: Evaluating zero-shot natural languageunderstanding of pretrained multilingual models intruly low-resource languages. In Proceedings of the60th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages6279-6299, Dublin, Ireland. Association for Compu-tational Linguistics.Goran Glava\u0161 and Ivan Vuli\u0107. 2021. Climbing the towerof treebanks: Improving low-resource dependencyparsing via hierarchical source selection. In Find-ings of the Association for Computational Linguis-tics: ACL-IJCNLP 2021, pages 4878-4888.Andrew Gordon, Zornitsa Kozareva, and Melissa Roem-mele. 2012. SemEval-2012 task 7: Choice of plau-sible alternatives: An evaluation of commonsensecausal reasoning. In *SEM 2012: The First JointConference on Lexical and Computational Seman-tics -Volume 1: Proceedings of the main conferenceand the shared task, and Volume 2: Proceedings ofthe Sixth International Workshop on Semantic Eval-uation (SemEval 2012), pages 394-398, Montr\u00e9al,Canada. Association for Computational Linguistics.Suchin Gururangan, Ana Marasovi\u0107, SwabhaSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,and Noah A. Smith. 2020. Don't stop pretraining:Adapt language models to domains and tasks. InProceedings of the 58th Annual Meeting of theAssociation for Computational Linguistics, pages8342-8360, Online. Association for ComputationalLinguistics."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Sentence 2: {HYPOTHESIS}. Answer: COPA. You are an AI assistant whose purpose is to perform open-domain commonsense causal reasoning. You will be provided a premise and two alternatives, where the task is to select the alternative that more plausibly has a causal relation with the premise. Answer as concisely as possible. PREMISE {% if question == \"cause\" %} This happened because... {% else %} As a consequence...", "figure_data": "{% endif %}: Alternative 1: CHOICE1 Alternative2: CHOICE2"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "TR EN TR EN,TR EN TR EN,TR EN TR EN,TR EN TR EN,TR EN TR EN,TR NLI Base 76.5 80.1 79.6 73.8 76.3 77.3 70.4 73.9 74.1 42.2 44.4 42.9 70.7 72.0 71.8 66.7 69.4 69.1 TLM 77.3 79.0 79.2 75.3 76.3 76.8 72.4 74.1 74.4 56.7 57.1 56.9 73.1 74.3 74.8 71.0 72.2 72.4 BALM 77.3 78.8 79.3 74.4 75.3 77.0 71.6 73.4 74.0 57.4 58.7 58.0 73.1 74.5 75.0 70.8 72.1 72.7 BJLM 76.4 78.4 79.3 74.9 75.1 76.8 71.9 74.3 75.5 57.2 59.2 59.4 73.4 74.6 75.7 70.7 72.3 73.3", "figure_data": "KazakhKyrgyzUyghurUzbekAverageEN TR EN,mT0small 35.334.936.836.635.335.8mT0base40.540.339.838.340.439.8mT0large40.842.542.041.941.241.7mT0XL56.955.753.049.455.654.1mT0XXL60.759.458.154.358.958.2chatGPT56.448.047.147.747.949.4Base60.1 61.1 60.9 60.7 60.8 59.9 59.7 60.0 59.4 51.8 52.7 52.7 57.3 59.5 60.1 57.9 58.8 58.6TLM62.1 62.1 61.5 55.7 55.8 56.1 57.5 59.7 58.9 49.9 50.3 49.3 62.9 63.2 62.5 57.6 58.2 57.7BALM57.2 58.3 59.4 59.1 59.5 59.7 56.1 59.9 59.1 51.1 53.9 52.5 60.5 61.7 61.9 56.8 58.6 57.9BJLM61.8 63.3 63.3 58.4 58.6 57.7 56.8 61.5 62.0 50.9 52.2 53.9 61.7 60.5 62.9 57.9 59.2 60.0COPAmT0small34.27.63.45.643.618.8mT0base32.03.65.84.239.817.1mT0large38.038.230.424.238.433.8mT0XL60.462.850.447.663.256.9mT0XXL81.274.657.861.480.671.1chatGPT73.063.456.657.055.661.1"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Zero-Shot results for the target languages and the average results across the five languages for XLM-R base, mT0 and chatGPT models. The best results for each language-task pair are shown in bold.TR EN TR EN,TR EN TR EN,TR EN TR EN,TR EN TR EN,TR EN TR EN,TR NLI TLLM 77.3 79.0 79.2 75.3 76.3 76.8 72.4 74.1 74.4 56.7 57.1 56.9 73.1 74.3 74.8 71.0 72.2 72.4 BALM 77.3 78.8 79.3 74.4 75.", "figure_data": "AzerbaijaniKazakhKyrgyzUyghurUzbekAverageEN TR EN,"}], "formulas": [], "doi": "10.1162/tacl_a_00416"}