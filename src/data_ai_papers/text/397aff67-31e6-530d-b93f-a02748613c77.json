{"title": "Pixel-Perfect Structure-from-Motion with Featuremetric Refinement", "authors": "Philipp Lindenberger; Paul-Edouard Sarlin; Viktor Larsson; Marc Pollefeys", "pub_date": "2021-08-18", "abstract": "Finding local features that are repeatable across multiple views is a cornerstone of sparse 3D reconstruction. The classical image matching paradigm detects keypoints per-image once and for all, which can yield poorly-localized features and propagate large errors to the final geometry. In this paper, we refine two key steps of structure-from-motion by a direct alignment of low-level image information from multiple views: we first adjust the initial keypoint locations prior to any geometric estimation, and subsequently refine points and camera poses as a post-processing. This refinement is robust to large detection noise and appearance changes, as it optimizes a featuremetric error based on dense features predicted by a neural network. This significantly improves the accuracy of camera poses and scene geometry for a wide range of keypoint detectors, challenging viewing conditions, and off-the-shelf deep features. Our system easily scales to large image collections, enabling pixel-perfect crowdsourced localization at scale. Our code is publicly available at github.com/cvg/pixel-perfect-sfm as an add-on to the popular SfM software COLMAP.", "sections": [{"heading": "Introduction", "text": "Mapping the world is an important requirement for spatial intelligence applications in augmented reality or robotics. Tasks like visual localization or path planning can benefit from accurate sparse or dense 3D reconstructions of the environment. These can be built from images using Structure-from-Motion (SfM), which associates observations across views to estimate camera parameters and 3D scene geometry. Sparse reconstruction based on matching local image features [10,21,23,34,51,57,59,65] is the most common due to its scalability and its robustness to appearance changes introduced by varying devices, viewpoints, and temporal conditions found in crowdsourced scenarios [2,29,35,41,47,50,58].\nSfM assumes that sparse interest points [10,21,23,34,51,59,62,84,92]  We improve the accuracy of sparse Structure-from-Motion by refining 2D keypoints, camera poses, and 3D points using the direct alignment of deep features. This featuremetric optimization leverages dense image information but can scale to scenes with thousands of images. Such refinement results in subpixelaccurate reconstructions, even in challenging conditions.\ncally selects such points for each image independently and relies on these initial detections for the remainder of the reconstruction process. However, detecting keypoints from a single view is inherently inaccurate due to appearance changes and discrete image sampling [31]. The advent of convolutional neural network (CNNs) for detection has magnified this issue, as they generally do not retain local image information and instead favor global context. Multi-view geometric optimization with bundle adjustment [4,42,82] is commonly used to refine cameras and points using reprojection errors. Dusmanu et al. [24] proposed to refine keypoint locations prior to SfM via an analogous geometric cost constrained with local optical flow. This can improve SfM, but has limited accuracy and scalability.\nIn this work, we argue that local image information is valuable throughout the SfM process to improve its accuracy. We adjust both keypoints and bundles, before and after reconstruction, by direct image alignment [18,26,52] in a learned feature space. Exploiting this locally-dense informa-  The approach first refines the 2D keypoints only from tentative matches by optimizing a direct cost over dense feature maps. The second stage operates after SfM and refines 3D points and poses with a similar featuremetric cost.\ntion is significantly more accurate than geometric optimization, while deep, high-dimensional features extracted by a CNN ensure wider convergence in challenging conditions. This formulation elegantly combines globally-discriminative sparse matching with locally-accurate dense details. It is applicable to both incremental [70,75] and global [9,12,54] SfM irrespective of the types of sparse or dense features. We validate our approach in experiments evaluating the accuracy of both 3D structure and camera poses in various conditions. We demonstrate drastic improvements for multiple hand-crafted and learned local features using offthe-shelf CNNs. The resulting system produces accurate reconstructions and scales well to large scenes with thousands of images. In the context of visual localization, it can, in addition to providing a more accurate map, also refine poses of single query images with minimal overhead.\nFor the benefit of the research community, we will release our code as an extension to COLMAP [70,71] and to the popular localization toolbox hloc [63,64]. We believe that our featuremetric refinement can significantly improve the accuracy of existing datasets [67] and push the community towards sub-pixel accurate localization at large scale.", "publication_ref": ["b8", "b19", "b21", "b32", "b49", "b55", "b57", "b63", "b0", "b27", "b33", "b39", "b45", "b48", "b56", "b8", "b19", "b21", "b32", "b49", "b57", "b60", "b82", "b90", "b29", "b2", "b40", "b80", "b22", "b16", "b24", "b50", "b68", "b73", "b7", "b10", "b52", "b68", "b69", "b61", "b62", "b65"], "figure_ref": [], "table_ref": []}, {"heading": "Related work", "text": "Image matching is at the core of SfM and visual SLAM, which typically rely on sparse local features for their efficiency and robustness. The process i) detects a small number of interest points, ii) computes their visual descriptors, iii) matches them with a nearest neighbor search, and iv) verifies the matches with two-view epipolar estimation and RANSAC. The correspondences then serve for relative or absolute pose estimation and 3D triangulation. As keypoints are sparse, small inaccuracies in their locations can result in large errors for the estimated geometric quantities.\nDifferently, dense matching [13,49,61,74,77,81,83] considers all pixels in each image, resulting in denser and more accurate correspondences. It has been successful for constrained settings like optical flow [40,76] or stereo depth estimation [90], but is not suitable for large-scale SfM due to its high computational cost due to many redundant correspondences. Several recent works [46,60,78,96] improve the matching efficiency by first matching coarsely and subsequently refining correspondences using a local search. This is however limited to image pairs and thus cannot create point tracks required by SfM.\nOur work combines the best of both paradigms by leveraging dense local information to refine sparse observations. It is inherently amenable to SfM as it can optimize all locations over multiple views in a track simultaneously.\nSubpixel estimation is a well-studied problem in correspondence search. Common approaches either upsample the input images or fit polynomials or Gaussian distributions to local image neighborhoods [28,36,39,51,69]. With the widespread interest in CNNs for local features, solutions tailored to 2D heatmaps have been recently developed, such as learning fine local sub-heatmaps [38] or estimating subpixel corrections with regression [14,80] or the soft-argmax [55,93]. Cleaner heatmaps can also arise from aggregating predictions over multiple virtual views using data augmentation [21].\nDetections or local affine frames can be combined across multiple views with known poses in a least-squares geometric optimization [25,82]. Dusmanu et al. [24] instead refine keypoints solely based on tentative matches, without assuming known geometry. This geometric formulation exhibits remarkable robustness, but is based on a local optical flow whose estimation for each correspondence is expensive and approximate. We unify both keypoint and bundle optimizations into a joint framework that optimizes a featuremetric cost, resulting in more accurate geometries and a more efficient keypoint refinement.\nDirect alignment optimizes differences in pixel intensities by implicitly defining correspondences through the motion and geometry. It therefore does not suffer from geometric noise and is naturally subpixel accurate via image interpolation. Direct photometric optimization has been successfully applied to optical flow [8,52], visual odometry [18,26,27,44], SLAM [5,72], multi-view stereo (MVS) [19,22,91], and pose refinement [73]. It generally fails for moderate displacements or appearances changes, and is thus not suitable for large-baseline SfM. One notable work by Woodford & Rosten [88] refines dense SfM+MVS models with a robust image normalization. It focuses on dense mapping with accurate initial poses and moderate appearance changes. Georgel et al. [30] instead estimate more accurate relative poses by elegantly combining photometric and geometric costs. They show that dense information can improve sparse estimation but their approach ignores appearance changes. Differently, our work improves the entire SfM pipeline starting with tentative matches and addresses larger, challenging changes.\nTo improve on the weaknesses of photometric optimization, numerous recent works align multi-dimensional image representations. Examples of this featuremetric optimization include frame tracking with handcrafted [6,56] or learned descriptors [17,53,86,87,89], optical flow [7,11], MVS [94], and dense SfM in small scenes [79]. Closer to our work, PixLoc [66] learns deep features with a large basin of convergence for wide-baseline pose refinement. It improves the accuracy of sparse matching but is designed for single images and disregards the scalability to multiple images or large scenes. Here we extend this paradigm to other steps of SfM and propose an efficient algorithm that scales to thousands of images. We show that learning task-specific wide-context features is not necessary and demonstrate highly accurate refinements with off-the-shelf features.\nIn conclusion, our work is the first to apply robust featuremetric optimization to a large-scale sparse reconstruction problem and show significant benefits for visual localization.", "publication_ref": ["b11", "b47", "b59", "b72", "b75", "b79", "b81", "b38", "b74", "b88", "b44", "b58", "b76", "b94", "b26", "b34", "b37", "b49", "b67", "b36", "b12", "b78", "b53", "b91", "b19", "b23", "b80", "b22", "b6", "b50", "b16", "b24", "b25", "b42", "b3", "b70", "b17", "b20", "b89", "b71", "b86", "b28", "b4", "b54", "b15", "b51", "b84", "b85", "b87", "b5", "b9", "b92", "b77", "b64"], "figure_ref": [], "table_ref": []}, {"heading": "Background", "text": "Given N images {I i } observing a scene, we are interested in accurately estimating its 3D structure, represented as sparse points {P j \u2208 R 3 }, intrinsic parameters {C i } of the cameras, and the poses {(R i , t i ) \u2208 SE(3)} of the images, represented as rotation matrices and translation vectors.\nA typical SfM pipeline performs geometric estimation from correspondences between sparse 2D keypoints {p u } observing the same 3D point from different views, collectively called a track. Association between observations is based on matching local image descriptors {d u \u2208 R D }, but the estimated geometry relies solely on the location of the keypoints, whose accuracy is thus critical. Keypoints are detected from local image information for each image individually, without considering multiple views simultaneously. Subsequent steps of the pipeline discover additional information about the scene, such as its its geometry or its multi-view appearance. Two approaches leverage this information to reduce the detection noise and refine the keypoints.\nGlobal refinement: Bundle adjustment [82] is the gold standard for refining structure and poses given initial estimates. It minimizes the total geometric error\nE BA = j (i,u)\u2208T (j) \u03a0 (R i P j + t i , C i ) \u2212 p u \u03b3 , (1)\nwhere T (j) is the set the images and keypoints in track j, \u03a0(\u2022) projects to the image plane, and \u2022 \u03b3 is a robust norm [33]. This formulation implicitly refines the keypoints while ensuring their geometric consistency. It however ignores the uncertainty of the initial detections and thus re-quires many observations to reduce the geometric noise. Operating on an existing reconstruction, it cannot recover observations arising from noisy keypoints that are matched correctly but discarded by the geometric verification.\nTrack refinement: To improve the accuracy of the keypoints prior to any geometric 3D estimation, Dusmanu et al. [24] optimize their locations over tentative tracks formed by raw, unverified matches. They exploit the inherent structure of the matching graph to discard incorrect matches without relying on geometric constraints. Given two-view dense flow fields {T v\u2192u } between the neighborhoods of matching keypoints u and v, this keypoint adjustment optimizes, for each tentative track j, the multi-view cost\nE j KA = (u,v)\u2208M(j) p v + T v\u2192u [p v ] \u2212 p u \u03b3 ,(2)\nwhere M(i) denotes the set of matches that forms the track and [\u2022] is a lookup with subpixel interpolation. A deep neural network is trained to regress the flow of a single point from two input patches and the flow field is interpolated from a sparse grid. This dramatically improves the keypoint accuracy, but some errors remain as the regression and interpolation are only approximate. Both bundle and keypoint adjustments are based on geometric observations, namely keypoint locations and flow, but do not account for their respective uncertainties. They thus require a large number of observations to average out the geometric noise and their accuracy is in practice limited.", "publication_ref": ["b80", "b31", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Approach", "text": "Summarizing dense image information into sparse points is necessary to perform global data association and optimization at scale. However, refining geometry is an inherently local operation, which, we show, can efficiently benefit from locally-dense pixels. Given constraints provided by coarse but global correspondences or initial 3D geometry, the dense information only needs to be locally accurate and invariant but not globally discriminative. While SfM typically discards image information as early as possible, we instead exploit it in several steps of the process thanks to direct alignment. Leveraging the power of deep features, this translates into featuremetric keypoint and bundle adjustments that elegantly integrate into any SfM pipeline by replacing their geometric counterparts. Figure 2 shows an overview.\nWe first introduce the featuremetric optimization in Section 4.1. We then describe our formulations of keypoint adjustment, in Section 4.2, and bundle adjustment, in Section 4.3, and analyze their efficiency.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Featuremetric optimization", "text": "Direct alignment: We consider the error between image intensities at two sparse observations:\nr = I i [p u ] \u2212 I j [p v ].\nLocal image derivatives implicitly define a flow from one point to the other through a gradient descent update:\nT v\u2192u [p v ] \u221d \u2212 \u2202I j \u2202p [p v ] r .(3)\nThis flow can be efficiently computed at any location in a neighborhood around v, without approximate interpolation nor descriptor matching. It naturally emerges from the direct optimization of the photometric error, which can be minimized with second-order methods in the same way as the aforementioned geometric costs. Unlike the flow regressed from a black-box neural network [24], this flow can be made consistent across multiple view by jointly optimizing the cost over all pairs of observations. ", "publication_ref": ["b22"], "figure_ref": [], "table_ref": []}, {"heading": "Keypoint adjustment", "text": "Once local features are detected, described, and matched, we refine the keypoint locations before geometrically verifying the tentative matches.\nTrack separation: Connected components in the matching graph define tentative tracks -sets of keypoints that are likely to observe the same 3D point, but whose observations have not yet been geometrically verified. Because a 3D point has a single projection on a given image plane, valid tracks cannot contain multiple keypoints detected in the same image. We can leverage this property to efficiently prune out most incorrect matches using the track separation algorithm introduced in [24]. This speeds up the subsequent optimization and reduces the noise in the estimation.\nObjective: We then adjust the locations of 2D keypoints belonging to the same track j by optimizing its featuremetric consistency along tentative matches with the cost\nE j FKA = (u,v)\u2208M(j) w uv F i(u) [p u ] \u2212 F i(v) [p v ] \u03b3 , (4)\nwhere w uv is the confidence of the correspondence (u, v), such as the similarity of its local feature descriptors d u d v .\nThis allows the optimization to split tracks connected by weak correspondences, providing robustness to mismatches. The confidence is not based on the dense features since these are not expected to disambiguate correspondences at the global image level.\nEfficiency : This direct formulation simply compares precomputed features on sparse points and is thus much more scalable than patch flow regression (Eq. 2), which performs a dense local correlation for each correspondence. All tracks are optimized independently, which is very fast in practice despite the sheer number of tentative matches.\nDrift: Because of the lack of geometric constraints, the points are free to move anywhere on the underlying 3D surface of the scene. The featuremetric cost biases the updates towards areas with low spatial feature gradients and with better-defined features. This can result in a large drift if not accounted for. Keypoints should however remain repeatable w.r.t. unrefined detections to ensure the matchability of new images, such as for visual localization. It is thus critical to limit the drift, while allowing the refinement of noisier keypoints. For each track, we freeze the location of the keypoint u with highest connectivity, as in [24], and constrain the location p u of each keypoint w.r.t. to its initial detection p 0 u , such that p u \u2212 p 0 u \u2264 K. Once all tracks are refined, the geometric estimation proceeds, typically using two-view epipolar geometric verification followed by incremental or global SfM.", "publication_ref": ["b22", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Bundle adjustment", "text": "The estimated structure and motion can then be refined with a similar featuremetric cost. Here keypoints are implicitly defined by the projections of the 3D points into the 2D image planes, and only poses and 3D points are optimized.\nObjective: We minimize for each track j the error between its observations and a reference appearance f j :\nE FBA = j (i,u)\u2208T (j) F i [\u03a0 (R i P j + t i , C i )] \u2212 f j \u03b3 .\n(5) The reference is selected at the beginning of the optimization and kept fixed from then on. This reduces the drift of the points significantly, as also noted in [5], but is more flexible than the common ray-based parametrization [26,44,88].\nThe reference is defined as the observation closest to the robust mean \u00b5 over all initial observations f j u of the track:\nf j = argmin f \u2208{f j u } \u00b5 j \u2212 f (6) with \u00b5 j = argmin \u00b5\u2208R D f \u2208{f j u } f \u2212 \u00b5 \u03b3 .(7)\nThis ensures robustness to outlier observations and accounts for the unknown topology of the feature space.   [24], especially at 1cm or with SIFT.\nEfficiency: Compared to the keypoint adjustment (Eq. 4), using a reference feature reduces the number of residuals from O(N 2 ) to O(N ). On the other hand, all tracks need to be updated simultaneously because of the interdependency caused by the camera poses. To accelerate the convergence, we form a reduced camera system based on the Schur complement and use embedded point iterations [42]. The refinement generally converges within a few camera updates.", "publication_ref": ["b3", "b24", "b42", "b86", "b22", "b40"], "figure_ref": [], "table_ref": []}, {"heading": "Implementation", "text": "Dense extractor: Our refinement can work with any offthe-shelf CNN that produces feature maps that are locally discriminative. These should be of the same resolution as the input (stride 1) to enable subpixel accuracy. The radius of convergence, or context, of such features depends on the amount of noise in the keypoints. Most detectors like SIFT have at most a few pixels of error, while others like D2-Net exhibit a much larger detection noise. In our experiments, we use S2DNet [31] for dense feature extraction, as it computes fine features very efficiently in only 4 convolutions, but also produce, if required, deeper features with a larger context. These can then be combined into a multi-level optimization scheme [26,66,86] that sequentially refines based on coarse to fine features. The convergence can thus be adjusted depending on the detector and on the image resolution. We show in Section 5.4 that other dense features work well too.\nOptimization: The optimization problems of both keypoint and bundle adjustments are solved with the Levenberg-Marquardt [45] algorithm implemented using Ceres [3]. Feature maps are stored as collections of 16\u00d716 patches centered around the initial keypoint detections. We thus constrain points to move at most K= 8 pixels. The feature lookup is implemented as bicubic interpolation. We use the Cauchy loss \u03b3 with a scale of 0.25. The robust mean in Eq. 7\nis computed with iteratively reweighted least squares [37].\nSimultaneously storing all high-dimensional feature patches incurs high memory requirements during BA. We dramatically increase its efficiency by exhaustively precomputing patches of feature distances and directly interpolate an approximate cost\u0112 ij = F i \u2212 f j \u03b3 p ij . To improve the convergence, we store and optimize its spatial derivatives \u2202\u0112ij /\u2202p ij . This reduces the residual size from D to 3 with no loss of accuracy. See Appendix C for more details.\nRun time and memory: S2DNet can extract 3-5 dense feature maps per second and both featuremetric adjustments run in less than 5 minutes for 100 images. As these features are 128-dimensional, the memory consumption can be a bottleneck. We believe that much fewer dimensions are actually required for refinement, and retraining a compact feature extractor would improve the efficiency of the optimization.", "publication_ref": ["b29", "b24", "b64", "b84", "b43", "b1", "b35"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We evaluate our featuremetric refinement on various SfM tasks with several handcrafted and learned local features and show substantial improvements for all of them. We first evaluate its accuracy on the tasks of triangulation and camera pose estimation in Sections 5.1 and 5.2, respectively. We then assess in Section 5.3 the impact of the refinement on two-view and multi-view pose estimation for end-to-end reconstruction in challenging conditions. Lastly, Section 5.4 analyzes the validity and scalability of our design decisions through an ablation study.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "3D triangulation", "text": "We first evaluate the accuracy of the refined 3D structure given known camera poses and intrinsics.\nEvaluation: We use the ETH3D benchmark [73], which  is composed of 13 indoor and outdoor scenes and provides images with millimeter-accurate camera poses and highlyaccurate ground truth dense reconstructions obtained with a laser scanner. We follow the protocol introduced in [24], in which a sparse 3D model is triangulated for each scene using COLMAP [70] with fixed camera poses and intrinsics. Following the original benchmark setup, we report the accuracy and completeness of the reconstruction, in %, as the ratio of triangulated and ground-truth dense points that are within a given distance of each other.\nBaselines: We evaluate our featuremetric refinement with the hand-crafted local features SIFT [51] and the learned ones SuperPoint [21], D2-Net [23], and R2D2 [59], using the associated publicly available code repositories. We compare our approach to the geometric optimization of [24], referred here as Patch Flow. We re-compute the numbers provided in the original paper using the code provided by the authors.\nResults: Table 1 shows that our approach results in significantly more accurate and complete 3D reconstructions compared to the traditional geometric SfM. It is more accurate than Patch Flow, especially at the strict threshold of 1cm, and exhibits similar completeness. The improvements are consistent across all local features, both indoors and outdoors. The gap with Patch Flow is especially large for SIFT, which already detects well-localized keypoints. This confirms that our featuremetric optimization better captures low-level image information and yields a finer alignment. Patch Flow is more complete for larger thresholds as it partly solves a different problem by increasing the keypoint repeatability with its large receptive field, while we focus on their localization.  ", "publication_ref": ["b71", "b22", "b68", "b49", "b19", "b21", "b57", "b22"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Camera pose estimation", "text": "We now evaluate the impact of our refinement on the task of camera pose estimation from a single image.\nEvaluation: We again follow the setup of [24] based on the ETH3D benchmark. For each scene, 10 images are randomly selected as queries. For each of them, the remaining images, excluding the 2 most covisible ones, are used to triangulate a sparse 3D partial model. Each query is then matched against its corresponding partial model and the resulting 2D-3D matches serve to estimate its absolute pose using LO-RANSAC+PnP [15] followed by geometric refinement. We compare the 130 estimated query poses to their ground truth and report the area under the cumulative translation error curve (AUC) up to 1mm, 1cm, and 10cm.\nBaselines: Patch Flow performs multi-view optimization over each partial model independently as well as over the matches between each query and its partial model. Similarly, we first refine each partial model as in Section 5.1. We then adjust the query keypoints using its tentative matches, estimate an initial pose, and refine it with featuremetric BA.", "publication_ref": ["b22", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Results:", "text": "The AUC and its cumulative plot are shown in Table 2. Our refinement substantially improves the localization accuracy for all local features, including SIFT, for which Patch Flow does not show any benefit. At all error thresholds, featuremetric optimization is consistently more accurate than its geometric counterparts. The accuracy of SuperPoint is raised far higher than other detectors, despite the high sparsity of the 3D models that it produces. This shows how more accurate keypoint detections can result in much more accurate visual localization.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "End-to-end Structure-from-Motion", "text": "While the previous experiments precisely quantify the accuracy of the refinement, they do not contain any variations of appearance or camera models. We thus turn to crowdsourced imagery and evaluate the benefits of our featuremetric optimization in an end-to-end reconstruction pipeline.\nEvaluation: We use the data, protocol, and code of the 2020 Image Matching Challenge [1,43]. It is based on large collections of crowd-sourced images depicting popular landmarks around the world. Pseudo ground truth poses are obtained with SfM [70] and used for two tasks. The stereo task evaluates relative poses estimated from image pairs by decomposing their epipolar geometry. This is a critical step of global SfM as it initializes its global optimization. The multiview task runs incremental SfM for small subsets of images, making the SfM problem much harder, and evaluates the final relative poses within each subset. For each task, we report the AUC of the pose error at the threshold of 5 \u2022 , where the pose error is the maximum of the angular errors in rotation and translation. As the evaluation server accepts at most correspondences, we cannot evaluate our method using the test data. We instead test on a subset of the publicly available validation scenes, and tune the RANSAC and matching parameters on the remaining scenes. More details on this setup are provided in the Appendix.", "publication_ref": ["b41", "b68"], "figure_ref": [], "table_ref": []}, {"heading": "Baselines:", "text": "We evaluate our refinement in combination with SIFT [51], D2-Net [23], and SuperPoint+SuperGlue [21,65]. We limit the number of detected keypoints to 2k for computational reasons, but increase this number to 4k for D2-Net as it otherwise performs poorly. In the stereo task, we adjust the keypoints using the entire exhaustive tentative match graph (4950 pairs per scene). We use LO-DEGENSAC [15,16] for match verification, the ratio test for SIFT, and the mutual check for SIFT and D2-Net. In the multiview task, we adjust keypoints for each subset independently, considering only the matches between images in the subset, and run our bundle adjustment after SfM.\nResults: Table 3 summarizes the results. For stereo, our featuremetric keypoint adjustment significantly improves the accuracy of the two-view epipolar geometries across all local features and despite the challenging conditions. In multiview setting, it also improves the accuracy of the SfM poses, especially for small sets of images. Featuremetric optimization is particularly effective in this situation, as geometric optimization cannot fully suppress the detection noise due to the small number of observations. We visualize tracks of a 5-image reconstruction in Figure 4 and highlight the accuracy of the refined SfM model.", "publication_ref": ["b49", "b21", "b19", "b63", "b13", "b14"], "figure_ref": ["fig_2"], "table_ref": ["tab_7"]}, {"heading": "Additional insights", "text": "Ablation study: Table 4 shows the performance of several variants of our featuremetric optimization on ETH3D in terms of triangulation (scene Facade only) and localization (all scenes). We compare both types of adjustments, minor tweaks, and different image representations, including  Relative run times for 1000 images:\nFigure 3: Run-times. We show the duration, in logarithmic scale, of the refinement for varying numbers of images. Our refinement is more than ten times faster than Patch Flow [24], whose run-time is dominated by the computation of the pairwise flow, which scales quadratically. Thanks to our precomputed cost patches, the featuremetric BA is fast. The KA amounts for the majority of the refinement time.\nNCC-normalized intensity patches with fronto-parallel warping. Our final configuration, based on on the dense features of S2DNet [31], performs best across all metrics. We will now show that it is also fairly efficient. Scalability: We run SfM on subsets of images of the Aachen Day-Night dataset [67,68,95]. Figure 3 shows the run times of the refinement for subsets of 10, 100 and 1000 images. The featuremetric refinement is an order of magnitude faster than Patch-Flow [24]. Precomputing distance maps reduces the peak memory requirement of the bundle adjustment from 80 GB to less than 10GB for 1000 images. As storing feature maps only requires 50 GB of disk space, this refinement can easily run on a desktop PC. We thus refined the entire Aachen Day-Night v1.1 model, composed of 7k images, in less than 2 hours. Scene partitioning [70] could further reduce the peak memory. See Appendix D for more details.", "publication_ref": ["b22", "b29", "b65", "b66", "b93", "b22", "b68"], "figure_ref": [], "table_ref": ["tab_9"]}, {"heading": "Conclusion", "text": "In this paper we argue that the recipe for accurate largescale Structure-from-Motion is to perform an initial coarse estimation using sparse local features, which are by necessity globally-discriminative, followed by a refinement using locally-accurate dense features. Since the dense feature only need to be locally-discriminative, they can afford to capture much lower-level texture, leading to more accurate corre-spondences. Through extensive experiments we show that this results in more accurate camera poses and structure; in challenging conditions and for different local features.\nWhile we optimize against dense feature maps, we keep the sparse scene representation of SfM. This ensures not only that the approach is scalable but also that the resulting 3D model is compatible with downstream applications, e.g. mapping for visual localization. Since our refinement works well even with few observations, as it does not need to average out the keypoint detection noise, it has the potential to achieve more accurate results using fewer images.\nWe thus believe that our approach can have a large impact in the localization community as it can improve the accuracy of the ground truth poses of standard benchmark datasets, of which many are currently saturated. Since this refinement is less sensitive to under-sampling, it enables benchmarking for crowd-sourced scenarios beyond densely-photographed tourism landmarks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Additional results on ETH3D", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1. Triangulation", "text": "We refine the triangulation of SuperPoint [21] keypoints for the ETH3D Courtyard scene and show in Figure 5 the distribution of triangulation errors for points observed by different numbers of images (track length). Our featuremetric refinement provides the largest improvement for points with low track length, for which the estimates of the traditional geometric BA are dominated by the noise of the keypoint detection. For larger track lengths, the refined point cloud has an accuracy close to the Faro Focus X 330 laser scanner from which the ground truth is computed.\nWe show in Figure 10 the raw and refined point clouds for SuperPoint and D2-Net. The benefits of our refinement are easily visible in 3D. Planar walls exhibit fewer noisy keypoints and the refined point clouds are more complete.", "publication_ref": ["b19"], "figure_ref": ["fig_3", "fig_0"], "table_ref": []}, {"heading": "A.2. Camera pose estimation", "text": "We analyze in Table 5 how the different kinds of adjustments impact the accuracy of camera localization. The full method presented in the main paper first refines the 3D SfM model with featuremetric keypoint and bundle adjustments. It then refines each keypoint in the query image using its tentative 2D-3D correspondences by minimizing the featuremetric error between its observation in the query and the most similar observation of the respective 3D points. Refining the query keypoints before RANSAC increases the number of inlier matches and stabilizes the pose estimation in challenging scenarios where few 3D points are matches.\nOnce an initial pose is estimated with PnP+RANSAC, we refine it via a small featuremetric bundle adjustment over the inlier correspondences. This optimizes each query keypoint against the closest descriptor within the matched track. As opposed to refining each query keypoint against all observations in the matched track, this has the benefit of scaling linearly in the number of query keypoints and yields a similar accuracy.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_11"]}, {"heading": "B. Impact of various parameters B.1. Patch size", "text": "Figure 6 shows how much our refinement displaces the detected keypoints during the triangulation of SuperPoint on Courtyard using dense features extracted from 1600x1066pixel images. When using full feature maps without any constraints in keypoint adjustment, most points are moved by more than 1 pixel, but most often by less than 8 pixels. This confirms that storing the feature maps as 16\u00d716 patches is sufficient and rather conservative.\nWe show in Figure 7 the accuracy of the triangulation for   ", "publication_ref": [], "figure_ref": ["fig_4", "fig_5"], "table_ref": []}, {"heading": "B.2. Image resolution", "text": "The image resolution at which the dense features are extracted has a large impact on the accuracy of the refinement. In Figure 8 we quantify in the impact on both triangulation accuracy and run time for the ETH3D Courtyard scene (38 images). The accuracy drops significantly when the resolution is smaller than 1600\u00d71066px, which amounts to 25% of the full image resolution. Doubling the resolution to 3200\u00d72132px yields noticeable improvements, albeit sig-  ", "publication_ref": [], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "B.3. Reference selection for keypoint adjustment", "text": "Selecting some observations as references is necessary to avoid the drift. In a given track, the keypoint adjustment selects the point that is the most connected (topological center), while the bundle adjustment selects the point closest to the robust mean in feature space (feature center). Could we use the feature center for selecting the reference of the keypoint adjustment? By minimizing the feature distance to this unique reference, we could reduce the number of residuals from quadratic (pairwise constraints) to linear (unary constraints) and thus accelerate the optimization.  Retaining pairwise constraints however allows the optimization to separate tracks that were incorrectly merged by the track separation algorithm. This is not necessary in the bundle adjustment, as tracks are already filtered by the robust geometric estimation and can thus be assumed to be correct, but is common for unverified track. We evaluate the impact of the reference selection in the keypoint adjustment and report the results in Table 6. For both SuperPoint and D2-Net, using the feature center results in lower completeness and accuracy than the topological center. It also results in a lower track length, which confirms that the topological reference allows to retain incorrectly-merged tracks. Since the feature center still performs relatively well, it could be considered in case of tighter computational constraints.\nFurthermore, Table 6 highlights the importance of the featuremetric keypoint adjustment. The benefits are larger for D2-Net, which detects very noisy keypoints. As a consequence, many correct albeit noisy matches are rejected by the geometric verification. Our keypoint adjustment not only allows more points to be triangulated, thus increasing the completeness of the model, but also increases the accuracy of the triangulated points.  ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_13", "tab_13"]}, {"heading": "B.4. Number of feature levels", "text": "Using multiple feature levels enlarges the basin of convergence but increases the computational requirements. The radius of convergence that is required depends on the noise of the keypoint detector and on the resolution of the image from which keypoints are detected. When performing detection and refinement at identical image resolutions, the optimal displacement is at most a few pixels for most keypoint detectors. In this case, the fine level of S2DNet feature maps is sufficient. We empirically measured that its radius of convergence is approximately 3 pixels, although the multiview constraints enable to refine over much larger distances.\nWe thus use a single feature level for all experiments involving SIFT, SuperPoint, and R2D2. D2-Net require a different treatment, as its detection noise is significantly larger. This is partly due to the aggressive downsampling of its CNN backbone and to the low resolution of its output heatmap. As a consequence, we employ both fine and medium feature levels for D2-Net. Both keypoint and bundle adjustments run the optimization successively at the coarser and finer levels.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.5. Dimensionality of the features", "text": "Throughout this paper, we used 128-dimensional dense features extracted by S2DNet [31]. Relying on compact features would easily reduce the memory footprint and the run time of the refinement. To demonstrate these benefits, we show in Figure 9 the relationship between the dimension, the run time of the BA, and the triangulation accuracy when retaining only the first k channels of the S2DNet features. Features with fewer dimensions yield a faster refinement. The accuracy drops moderately but we expect a smaller reduction with features explicitly trained for smaller dimensions.", "publication_ref": ["b29"], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "C. Cost map approximation", "text": "We mention in Section 4.4 that the memory efficiency of the bundle adjustment can be improved by precomputing the featuremetric cost. We provide here more details.\nDescription: Given D-dimension features, the featuremetric bundle adjustment (Eq. 5) involves residuals and Jacobian matrices of dimension D. Unlike the keypoint adjustment, which can optimize tracks independently, all bundle parameters are updated simultaneously and the memory requirements are thus prohibitive. Given the 2D reprojection p ij = \u03a0 (R i P j + t i , C i ), this formulation loads in memory the dense features F i , interpolates them at p ij , and compute the residuals r ij = F i p ij \u2212 f j for the cost E ij = r ij \u03b3 .\nTo reduce the memory footprint, we can exhaustively precompute patches of feature distances and treat them as one-dimensional residualsr ij = F i \u2212 f j p ij . The cost then becomes\u0112 ij = \u03b3(r ij ). Such distances only need to be computed once since the reference f j is kept fixed throughout the optimization. This precomputed cost reduces the peak memory by a factor D, with often D=128. It is similar to the Neural Reprojection Error recently introduced by Germain et al. [32] for camera localization.\nAnalysis: Swapping the distance computation and the sparse interpolation introduces an approximation error. We first write the bilinear or bicubic interpolation as a sum over features F k on the discrete grid:\nF [p] = k w k F k with k w k = 1 . (8\n)\nWe assume that the features are L2-normalized F k = 1, such that F [p] \u2248 1. For a squared loss function, the approximation error can then be written as:\nF \u2212 f 2 [p] \u2212 F[p] \u2212 f 2 \u2248 1 \u2212 F[p] 2 = 1 2 k l w k w l F k \u2212 F l 2 . (9\n)\nThis error is zero at points on the discrete grid and increases with the roughness of the feature space. This approximation thus displaces the local minimum of the cost by at most 1 pixel but most often by much less.\nImprovement: This approximation however degrades the correctness of the approximate Hessian matrix that the Levenberg-Marquardt algorithm [45] relies on for fast convergence. We found that also optimizing the squared spatial derivatives of this cost significantly improves the convergence. This simply amounts to augmenting the scalar residual map with dense derivative maps:   For the experiments on ETH3D, we use the evaluation code provided by Dusmanu et al. [24]. We use the original implementations of SuperPoint [21], D2-Net [23], and R2D2 [59], and extract root-normalized SIFT [51] features using COLMAP [70]. For both sparse and dense feature extraction, the images are resized so that their longest dimension is equal to 1600 pixels. The tentative matches are filtered according to the recipe described in [24].\nr ij = \uf8eb \uf8ec \uf8ec \uf8ed F i \u2212 f j \u2202 Fi\u2212f j \u2202x \u2202 Fi\u2212f j \u2202y \uf8f6 \uf8f7 \uf8f7 \uf8f8 p ij .(10)", "publication_ref": ["b30", "b43", "b22", "b19", "b21", "b57", "b49", "b68", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "D.2. Structure-from-Motion -Section 5.3", "text": "We tune the hyperparameters on the training scenes Temple Nara Japan, Trevi Fountain, and Brandenburg Gate. The results in the main paper are computed on the test scenes Sacre Coeur, Saint Peter's Square, and Reichstag, using the data and code provided by the challenge organizers.\nFor SIFT [51], we use the mutual check, a ratio test with threshold 0.85 for the multi-view and 0.9 for the stereo tasks, and DEGENSAC with an inlier threshold of 0.5px. For D2-Net [23], we use the mutual check and inlier thresholds of 2px and 0.5px for raw and refined keypoints, respectively. For SuperPoint+SuperGlue [21,65], we do not use additional match filtering and we select an inlier thresholds of 1.1px and 0.5px for raw and refined keypoints, respectively. All sparse local and dense features are extracted at full image resolution, which is generally not larger than 1024px.", "publication_ref": ["b49", "b21", "b19", "b63"], "figure_ref": [], "table_ref": []}, {"heading": "D.3. Ablation study -Section 5.4", "text": "The triangulation metrics are reported for the ETH3D scene Facade, which is the largest with 76 images. We use SuperPoint local features as they perform best in all earlier experiments and we store dense feature maps in every experiment. The localization AUC is measured over all 13 scenes in ETH3D with 10 holdout images per scene. We now detail the different baselines.\nLocalization is achieved in \"F-KA\" by first refining the keypoints, triangulating the map and finally performing query keypoint adjustment as described in section A.2. For localization with \"F-BA\", we refined the triangulated model using featuremetric bundle adjustment and then refined the pose from PnP+RANSAC using qBA.\nIn the entry \"w/ F-BA drift\", we use the robust reference (Eq. 7) to select the observation in each track which is most similar to the robust reference as the source frame. The optimizer then minimizes the error between each other observation and the current, moving reference of the source frame. Since only the index of the source frame is fixed during the optimization, this method does not account for drift, which appears to yield higher accuracy but suffers from repeatability problems during localization.\nThe baseline \"PatchFlow + F-BA\" uses the keypoint refinement from Dusmanu et al. [24] as initialization, and runs our featuremetric bundle adjustment on top of it. We used the exact same parameters for PatchFlow as presented in [24].\nThe entry \"higher resolution\" corresponds to input images at double the resolution than all the other experiments, i.e. 3200 pixels in the longest dimension.\nFor the \"photometric\" baseline, we use RGB images (while Woodford et al. [88] use grayscale images), we warp patches of 4\u00d74 pixels at the featuremap resolution (1600 pixels in the longest dimension) with fronto-parallel assumption, and apply normalized cross correlation (NCC). Identically to our featuremetric BA and to LSPBA [88], the source frame is selected as the observation closest to the robust mean.\nWe report results for dense features extracted from a VGG-16 CNN, trained on ImageNet [20], at the layer conv1 2 (64 channels) and for the fine feature map predicted by PixLoc [66] (32 channels). The model of PixLoc, trained on MegaDepth [48], was kindly provided by its authors. In DSIFT [49] (128 channels), we apply a bin size of 4 and a step size of 1 and refer to the VLFeat implementation [85] for more details.", "publication_ref": ["b22", "b22", "b86", "b86", "b18", "b64", "b46", "b47", "b83"], "figure_ref": [], "table_ref": []}, {"heading": "D.4. Scalability", "text": "All experiments were conducted on 8 CPU cores (Intel Xeon E5-2630v4) and one NVIDIA RTX 1080 Ti. The subsets from the Aachen Day-Night v1.1 model [67,68,95] were selected as the images with the largest visibility overlap, in descending order. To accelerate the feature matching, each image was matched only to its top 20 most covisible reference images in the original Aachen SfM model. We use SuperPoint [21] features and match image pairs with the mutual check and distance thresholding at 0.7. During BA, we apply the sparse Schur solver from Ceres for each linear system in LM, while we use sparse Cholesky in KA, similar to [24]. Featuremetric bundle adjustment is stopped after 30 iterations while KA runs for at most 100 iterations and stops when parameters change by less than 10 \u22124 .\nTo refine the full Aachen Day-Night model, we use Su-perPoint features matched with SuperGlue [65] from the Hierarchical Localization toolbox [63,64]. We refine the keypoints with KA, then triangulate the points with fixed poses from the reference model. Finally, we run a full bundle adjustment of the model with the proposed approximation by cost maps.", "publication_ref": ["b65", "b66", "b93", "b19", "b22", "b63", "b61", "b62"], "figure_ref": [], "table_ref": []}, {"heading": "SuperPoint raw / refined point clouds", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Accuracy -unrefined", "text": "Accuracy -refined D2-Net", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "raw / refined point clouds", "text": "Accuracy -unrefined Accuracy -refined Figure 10: Refinement on ETH3D Courtyard. In the top parts, we show for both SuperPoint (top) and D2-Net (bottom) top-down views of the sparse point clouds triangulated with raw (in red) and refined (in green) keypoints. The refined point clouds better fit the geometry of the scene, especially on planar walls. In the lower parts, we also show images in which points are colored as accurate (in green) or inaccurate (in red) at 1cm for raw (left) and refined (right) point clouds.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "", "text": "Acknowledgements: The authors thank Mihai Dusmanu, R\u00e9mi Pautrat, Marcel Geppert, and the anonymous reviewers for their thoughtful comments. Paul-Edouard Sarlin was supported by gift funding from Huawei, and Viktor Larsson by an ETH Zurich Postdoctoral Fellowship.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Building Rome in a day", "journal": "Communications of the ACM", "year": "2011", "authors": "Sameer Agarwal; Yasutaka Furukawa; Noah Snavely; Ian Simon; Brian Curless; M Steven; Richard Seitz;  Szeliski"}, {"ref_id": "b1", "title": "", "journal": "", "year": "", "authors": "Sameer Agarwal; Keir Mierle; Others Ceres"}, {"ref_id": "b2", "title": "Bundle adjustment in the large", "journal": "", "year": "2010", "authors": "Sameer Agarwal; Noah Snavely; M Steven; Richard Seitz;  Szeliski"}, {"ref_id": "b3", "title": "Photometric bundle adjustment for vision-based SLAM", "journal": "", "year": "2016", "authors": "Hatem Alismail; Brett Browning; Simon Lucey"}, {"ref_id": "b4", "title": "Robust tracking in low light and sudden illumination changes", "journal": "", "year": "2016", "authors": "Hatem Alismail; Brett Browning; Simon Lucey"}, {"ref_id": "b5", "title": "Feature-based lucaskanade and active appearance models", "journal": "IEEE Transactions on Image Processing", "year": "2015", "authors": "Epameinondas Antonakos; Joan Alabort-I Medina; Georgios Tzimiropoulos; Stefanos P Zafeiriou"}, {"ref_id": "b6", "title": "Lucas-kanade 20 years on: A unifying framework", "journal": "IJCV", "year": "2003", "authors": "Simon Baker; Ralph Gross; Iain Matthews"}, {"ref_id": "b7", "title": "Efficient initial pose-graph generation for global sfm", "journal": "", "year": "", "authors": "Daniel Barath; Dmytro Mishkin; Ivan Eichhardt; Ilia Shipachev; Jiri Matas"}, {"ref_id": "b8", "title": "SURF: Speeded up robust features", "journal": "", "year": "2006", "authors": "Herbert Bay; Tinne Tuytelaars; Luc Van Gool"}, {"ref_id": "b9", "title": "CLKN: Cascaded Lucas-Lanade networks for image alignment", "journal": "", "year": "2017", "authors": "Che-Han Chang; Chun-Nan Chou; Edward Y Chang"}, {"ref_id": "b10", "title": "Efficient and robust large-scale rotation averaging", "journal": "", "year": "2013", "authors": "Avishek Chatterjee;  Venu Madhav;  Govindu"}, {"ref_id": "b11", "title": "Universal correspondence network", "journal": "", "year": "2016", "authors": "B Christopher; Junyoung Choy; Silvio Gwak; Manmohan Savarese;  Chandraker"}, {"ref_id": "b12", "title": "UnsuperPoint: End-to-end unsupervised interest point detector and descriptor", "journal": "", "year": "2019", "authors": "Mikkel Peter Hviid Christiansen; Yury Fly Kragh; Henrik Brodskiy;  Karstoft"}, {"ref_id": "b13", "title": "Locally optimized RANSAC", "journal": "Springer", "year": "2003", "authors": "Ond\u0159ej Chum; Ji\u0159\u00ed Matas; Josef Kittler"}, {"ref_id": "b14", "title": "Two-view geometry estimation unaffected by a dominant plane", "journal": "", "year": "2005", "authors": "Ondrej Chum; Tomas Werner; Jiri Matas"}, {"ref_id": "b15", "title": "LS-Net: Learning to solve nonlinear least squares for monocular stereo", "journal": "", "year": "2018", "authors": "Ronald Clark; Michael Bloesch; Jan Czarnowski; Stefan Leutenegger; Andrew J Davison"}, {"ref_id": "b16", "title": "Davison. Semantic texture for robust dense tracking", "journal": "", "year": "2017", "authors": "Jan Czarnowski; Stefan Leutenegger; Andrew J "}, {"ref_id": "b17", "title": "Photometric bundle adjustment for dense multi-view 3d modeling", "journal": "", "year": "2014", "authors": "Ama\u00ebl Delaunoy; Marc Pollefeys"}, {"ref_id": "b18", "title": "ImageNet: A large-scale hierarchical image database", "journal": "", "year": "2009", "authors": "Jia Deng; Wei Dong; Richard Socher; Li-Jia Li; Kai Li; Li Fei-Fei"}, {"ref_id": "b19", "title": "SuperPoint: Self-supervised interest point detection and description", "journal": "", "year": "2009", "authors": "Daniel Detone; Tomasz Malisiewicz; Andrew Rabinovich"}, {"ref_id": "b20", "title": "Computing differential properties of 3-D shapes from stereoscopic images without 3-D models", "journal": "", "year": "1994", "authors": "Fr\u00e9d\u00e9ric Devernay; D Olivier;  Faugeras"}, {"ref_id": "b21", "title": "D2-Net: A trainable CNN for joint detection and description of local features", "journal": "", "year": "2006", "authors": "Mihai Dusmanu; Ignacio Rocco; Tomas Pajdla; Marc Pollefeys; Josef Sivic; Akihiko Torii; Torsten Sattler"}, {"ref_id": "b22", "title": "Multi-View Optimization of Local Feature Geometry", "journal": "", "year": "2011", "authors": "Mihai Dusmanu; Johannes L Sch\u00f6nberger; Marc Pollefeys"}, {"ref_id": "b23", "title": "Optimal multi-view correction of local affine frames", "journal": "", "year": "2019", "authors": "Ivan Eichhardt; Daniel Barath"}, {"ref_id": "b24", "title": "Direct sparse odometry. TPAMI", "journal": "", "year": "2005", "authors": "Jakob Engel; Vladlen Koltun; Daniel Cremers"}, {"ref_id": "b25", "title": "LSD-SLAM: Large-scale direct monocular SLAM", "journal": "", "year": "2014", "authors": "Jakob Engel; Thomas Sch\u00f6ps; Daniel Cremers"}, {"ref_id": "b26", "title": "A fast operator for detection and precise location of distinct points, corners and centres of circular features", "journal": "", "year": "1987", "authors": "Wolfgang F\u00f6rstner; Eberhard G\u00fclch"}, {"ref_id": "b27", "title": "Building Rome on a cloudless day", "journal": "", "year": "2010", "authors": "Jan-Michael Frahm; Pierre Fite-Georgel; David Gallup; Tim Johnson; Rahul Raguram; Changchang Wu; Yi-Hung Jen; Enrique Dunn; Brian Clipp; Svetlana Lazebnik"}, {"ref_id": "b28", "title": "A unified approach combining photometric and geometric information for pose estimation", "journal": "", "year": "2008", "authors": "P Georgel; Selim Benhimane; Nassir Navab"}, {"ref_id": "b29", "title": "S2DNet: Learning accurate correspondences for sparse-todense feature matching", "journal": "", "year": "", "authors": "Hugo Germain; Guillaume Bourmaud; Vincent Lepetit"}, {"ref_id": "b30", "title": "Neural Reprojection Error: Merging feature learning and camera pose estimation", "journal": "", "year": "2021", "authors": "Hugo Germain; Vincent Lepetit; Guillaume Bourmaud"}, {"ref_id": "b31", "title": "Robust statistics: the approach based on influence functions", "journal": "Wiley", "year": "1986", "authors": " Frank R Hampel; M Elvezio;  Ronchetti; J Peter; Werner A Rousseeuw;  Stahel"}, {"ref_id": "b32", "title": "A combined corner and edge detector", "journal": "", "year": "1988", "authors": "G Christopher; Mike Harris;  Stephens"}, {"ref_id": "b33", "title": "Reconstructing the World* in Six Days *(as Captured by the Yahoo 100 Million Image Dataset)", "journal": "", "year": "2015", "authors": "Jared Heinly; L Johannes; Enrique Schonberger; Jan-Michael Dunn;  Frahm"}, {"ref_id": "b34", "title": "Real-time correlation-based stereo vision with reduced border errors", "journal": "IJCV", "year": "2002", "authors": "Heiko Hirschm\u00fcller; Jon Peter R Innocent;  Garibaldi"}, {"ref_id": "b35", "title": "Robust regression using iteratively reweighted least-squares", "journal": "Communications in Statistics-theory and Methods", "year": "1977", "authors": "W Paul; Roy E Holland;  Welsch"}, {"ref_id": "b36", "title": "Deep ChArUco: Dark ChArUco Marker Pose Estimation", "journal": "", "year": "2019", "authors": "Danying Hu; Daniel Detone; Tomasz Malisiewicz"}, {"ref_id": "b37", "title": "Detection of intensity changes with subpixel accuracy using laplacian-gaussian masks", "journal": "TPAMI", "year": "1986", "authors": "Andres Huertas; Gerard Medioni"}, {"ref_id": "b38", "title": "FlowNet 2.0: Evolution of optical flow estimation with deep networks", "journal": "", "year": "2017", "authors": "Eddy Ilg; Nikolaus Mayer; Tonmoy Saikia; Margret Keuper; Alexey Dosovitskiy; Thomas Brox"}, {"ref_id": "b39", "title": "From structure-from-motion point clouds to fast location recognition", "journal": "", "year": "2009", "authors": "Arnold Irschara; Christopher Zach; Jan-Michael Frahm; Horst Bischof"}, {"ref_id": "b40", "title": "Pushing the envelope of modern methods for bundle adjustment", "journal": "TPAMI", "year": "2005", "authors": "Yekeun Jeong; David Nister; Drew Steedly; Richard Szeliski; In-So Kweon"}, {"ref_id": "b41", "title": "Image Matching across Wide Baselines: From Paper to Practice. IJCV", "journal": "", "year": "2020", "authors": "Yuhe Jin; Dmytro Mishkin; Anastasiia Mishchuk; Jiri Matas; Pascal Fua; Kwang Moo Yi; Eduard Trulls"}, {"ref_id": "b42", "title": "Dense visual slam for RGB-D cameras", "journal": "", "year": "2013", "authors": "Christian Kerl; J\u00fcrgen Sturm; Daniel Cremers"}, {"ref_id": "b43", "title": "A method for the solution of certain non-linear problems in least squares", "journal": "Quarterly of applied mathematics", "year": "1944", "authors": "Kenneth Levenberg"}, {"ref_id": "b44", "title": "Dualresolution correspondence networks", "journal": "", "year": "", "authors": "Xinghui Li; Kai Han; Shuda Li; Victor Prisacariu"}, {"ref_id": "b45", "title": "Worldwide pose estimation using 3D point clouds", "journal": "", "year": "2012", "authors": "Yunpeng Li; Noah Snavely; Dan Huttenlocher; Pascal Fua"}, {"ref_id": "b46", "title": "MegaDepth: Learning singleview depth prediction from internet photos", "journal": "", "year": "2018", "authors": "Zhengqi Li; Noah Snavely"}, {"ref_id": "b47", "title": "SIFT Flow: Dense correspondence across scenes and its applications", "journal": "TPAMI", "year": "2007", "authors": "Ce Liu; Jenny Yuen; Antonio Torralba"}, {"ref_id": "b48", "title": "Efficient global 2D-3D matching for camera localization in a large-scale 3D map", "journal": "", "year": "2017", "authors": "Liu Liu; Hongdong Li; Yuchao Dai"}, {"ref_id": "b49", "title": "Distinctive image features from scaleinvariant keypoints", "journal": "IJCV", "year": "2004", "authors": "G David;  Lowe"}, {"ref_id": "b50", "title": "An iterative image registration technique with an application to stereo vision", "journal": "", "year": "1981", "authors": "D Bruce; Takeo Lucas;  Kanade"}, {"ref_id": "b51", "title": "Taking a deeper look at the inverse compositional algorithm", "journal": "", "year": "2019", "authors": "Zhaoyang Lv; Frank Dellaert; James M Rehg; Andreas Geiger"}, {"ref_id": "b52", "title": "Robust rotation and translation estimation in multiview reconstruction", "journal": "", "year": "2007", "authors": "Daniel Martinec; Tomas Pajdla"}, {"ref_id": "b53", "title": "LF-Net: Learning local features from images", "journal": "", "year": "2018", "authors": "Yuki Ono; Eduard Trulls; Pascal Fua; Kwang Moo Yi"}, {"ref_id": "b54", "title": "Illumination Change Robustness in Direct Visual SLAM", "journal": "", "year": "2017", "authors": "Seonwook Park; Thomas Sch\u00f6ps; Marc Pollefeys"}, {"ref_id": "b55", "title": "Online invariance selection for local feature descriptors", "journal": "", "year": "", "authors": "R\u00e9mi Pautrat; Viktor Larsson; R Martin; Marc Oswald;  Pollefeys"}, {"ref_id": "b56", "title": "From dusk till dawn: Modeling in the dark", "journal": "", "year": "2016", "authors": "Filip Radenovic; L Johannes; Dinghuang Schonberger; Jan-Michael Ji; Ondrej Frahm; Jiri Chum;  Matas"}, {"ref_id": "b57", "title": "Yohann Cabon, and Martin Humenberger. R2D2: Repeatable and reliable detector and descriptor", "journal": "", "year": "2005", "authors": "Jerome Revaud; Philippe Weinzaepfel; C\u00e9sar De Souza; Noe Pion; Gabriela Csurka"}, {"ref_id": "b58", "title": "Efficient neighbourhood consensus networks via submanifold sparse convolutions", "journal": "", "year": "", "authors": "Ignacio Rocco; Relja Arandjelovi\u0107; Josef Sivic"}, {"ref_id": "b59", "title": "Neighbourhood consensus networks", "journal": "", "year": "2018", "authors": "Ignacio Rocco; Mircea Cimpoi; Relja Arandjelovi\u0107; Akihiko Torii; Tomas Pajdla; Josef Sivic"}, {"ref_id": "b60", "title": "Machine learning for high-speed corner detection", "journal": "", "year": "2006", "authors": "Edward Rosten; Tom Drummond"}, {"ref_id": "b61", "title": "Visual localization made easy with hloc", "journal": "", "year": "", "authors": "Paul-Edouard Sarlin"}, {"ref_id": "b62", "title": "From coarse to fine: Robust hierarchical localization at large scale", "journal": "", "year": "2019", "authors": "Paul-Edouard Sarlin; Cesar Cadena; Roland Siegwart; Marcin Dymczyk"}, {"ref_id": "b63", "title": "SuperGlue: Learning feature matching with graph neural networks", "journal": "", "year": "2020", "authors": "Paul-Edouard Sarlin; Daniel Detone; Tomasz Malisiewicz; Andrew Rabinovich"}, {"ref_id": "b64", "title": "Back to the Feature: Learning robust camera localization from pixels to pose", "journal": "", "year": "2005", "authors": "Paul-Edouard Sarlin; Ajaykumar Unagar; M\u00e5ns Larsson; Hugo Germain; Carl Toft; Viktor Larsson; Marc Pollefeys; Vincent Lepetit; Lars Hammarstrand; Fredrik Kahl; Torsten Sattler"}, {"ref_id": "b65", "title": "Fredrik Kahl, and Tomas Pajdla. Benchmarking 6DOF outdoor visual localization in changing conditions", "journal": "", "year": "2018", "authors": "Torsten Sattler; Will Maddern; Carl Toft; Akihiko Torii; Lars Hammarstrand; Erik Stenborg; Daniel Safari; Masatoshi Okutomi; Marc Pollefeys; Josef Sivic"}, {"ref_id": "b66", "title": "Image retrieval for image-based localization revisited", "journal": "", "year": "2012", "authors": "Torsten Sattler; Tobias Weyand; Bastian Leibe; Leif Kobbelt"}, {"ref_id": "b67", "title": "A taxonomy and evaluation of dense two-frame stereo correspondence algorithms", "journal": "IJCV", "year": "2002", "authors": "Daniel Scharstein; Richard Szeliski"}, {"ref_id": "b68", "title": "Structure-from-motion revisited", "journal": "", "year": "2008", "authors": "Johannes Lutz Sch\u00f6nberger; Jan-Michael Frahm"}, {"ref_id": "b69", "title": "Pixelwise view selection for unstructured multi-view stereo", "journal": "", "year": "2016", "authors": "Johannes Lutz Sch\u00f6nberger; Enliang Zheng; Marc Pollefeys; Jan-Michael Frahm"}, {"ref_id": "b70", "title": "BAD SLAM: Bundle Adjusted Direct RGB-D SLAM. In CVPR", "journal": "", "year": "2002", "authors": "Thomas Schops; Torsten Sattler; Marc Pollefeys"}, {"ref_id": "b71", "title": "A multi-view stereo benchmark with highresolution images and multi-camera videos", "journal": "", "year": "2017", "authors": "Thomas Schops; L Johannes; Silvano Schonberger; Torsten Galliani; Konrad Sattler; Marc Schindler; Andreas Pollefeys;  Geiger"}, {"ref_id": "b72", "title": "RANSAC-Flow: generic two-stage image alignment", "journal": "", "year": "", "authors": "Xi Shen; Fran\u00e7ois Darmon; Alexei A Efros; Mathieu Aubry"}, {"ref_id": "b73", "title": "Modeling the world from internet photo collections", "journal": "IJCV", "year": "2008", "authors": "Noah Snavely; M Steven; Richard Seitz;  Szeliski"}, {"ref_id": "b74", "title": "PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume", "journal": "", "year": "2018", "authors": "Deqing Sun; Xiaodong Yang; Ming-Yu Liu; Jan Kautz"}, {"ref_id": "b75", "title": "LoFTR: Detector-free local feature matching with Transformers", "journal": "CVPR", "year": "2021", "authors": "Jiaming Sun; Zehong Shen; Yuang Wang; Hujun Bao; Xiaowei Zhou"}, {"ref_id": "b76", "title": "InLoc: Indoor Visual Localization with Dense Matching and View Synthesis", "journal": "TPAMI", "year": "2019", "authors": "Hajime Taira; Masatoshi Okutomi; Torsten Sattler; Mircea Cimpoi; Marc Pollefeys; Josef Sivic; Tomas Pajdla; Akihiko Torii"}, {"ref_id": "b77", "title": "BA-Net: Dense bundle adjustment network", "journal": "", "year": "2019", "authors": "Chengzhou Tang; Ping Tan"}, {"ref_id": "b78", "title": "Sudeep Pillai, and Rares Ambrus. Neural outlier rejection for selfsupervised keypoint learning", "journal": "", "year": "", "authors": "Jiexiong Tang; Hanme Kim; Vitor Guizilini"}, {"ref_id": "b79", "title": "DAISY: An efficient dense descriptor applied to wide-baseline stereo", "journal": "TPAMI", "year": "2009", "authors": "Engin Tola; Vincent Lepetit; Pascal Fua"}, {"ref_id": "b80", "title": "Bundle adjustment -a modern synthesis", "journal": "", "year": "1999", "authors": "Bill Triggs; F Philip; Richard I Mclauchlan; Andrew W Hartley;  Fitzgibbon"}, {"ref_id": "b81", "title": "GLU-Net: Global-local universal network for dense flow and correspondences", "journal": "", "year": "", "authors": "Prune Truong; Martin Danelljan; Radu Timofte"}, {"ref_id": "b82", "title": "Disk: Learning local features with policy gradient", "journal": "", "year": "", "authors": "J Micha\u0142; Pascal Tyszkiewicz; Eduard Fua;  Trulls"}, {"ref_id": "b83", "title": "VLFeat: An open and portable library of computer vision algorithms", "journal": "", "year": "2010", "authors": "Andrea Vedaldi; Brian Fulkerson"}, {"ref_id": "b84", "title": "GN-Net: The Gauss-Newton loss for multiweather relocalization", "journal": "RA-L", "year": "2020", "authors": "Patrick Lukas Von Stumberg; Qadeer Wenzel; Daniel Khan;  Cremers"}, {"ref_id": "b85", "title": "LM-Reloc: Levenberg-Marquardt based direct visual relocalization", "journal": "", "year": "", "authors": "Patrick Lukas Von Stumberg; Nan Wenzel; Daniel Yang;  Cremers"}, {"ref_id": "b86", "title": "Large scale photometric bundle adjustment", "journal": "", "year": "2004", "authors": "J Oliver; Edward Woodford;  Rosten"}, {"ref_id": "b87", "title": "Deep probabilistic feature-metric tracking", "journal": "RA-L", "year": "2021", "authors": "Binbin Xu; Andrew J Davison; Stefan Leutenegger"}, {"ref_id": "b88", "title": "MVSNet: Depth inference for unstructured multi-view stereo", "journal": "ECCV", "year": "2018", "authors": "Yao Yao; Zixin Luo; Shiwei Li; Tian Fang; Long Quan"}, {"ref_id": "b89", "title": "Recurrent MVSNet for high-resolution multiview stereo depth inference", "journal": "", "year": "2019", "authors": "Yao Yao; Zixin Luo; Shiwei Li; Tianwei Shen; Tian Fang; Long Quan"}, {"ref_id": "b90", "title": "LIFT: Learned invariant feature transform", "journal": "", "year": "2016", "authors": "Eduard Kwang Moo Yi; Vincent Trulls; Pascal Lepetit;  Fua"}, {"ref_id": "b91", "title": "Heatmap regression via randomized rounding", "journal": "", "year": "2020", "authors": "Baosheng Yu; Dacheng Tao"}, {"ref_id": "b92", "title": "Fast-MVSNet: Sparse-to-dense multi-view stereo with learned propagation and gauss-newton refinement", "journal": "", "year": "", "authors": "Zehao Yu; Shenghua Gao"}, {"ref_id": "b93", "title": "Reference Pose Generation for Long-term Visual Localization via Learned Features and View Synthesis", "journal": "IJCV", "year": "2020", "authors": "Zichao Zhang; Torsten Sattler; Davide Scaramuzza"}, {"ref_id": "b94", "title": "Epipolar-guided pixel-level correspondences", "journal": "", "year": "", "authors": "Qunjie Zhou; Torsten Sattler; Laura Leal-Taixe"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure1: From sparse to dense. We improve the accuracy of sparse Structure-from-Motion by refining 2D keypoints, camera poses, and 3D points using the direct alignment of deep features. This featuremetric optimization leverages dense image information but can scale to scenes with thousands of images. Such refinement results in subpixelaccurate reconstructions, even in challenging conditions.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure2: Refinement pipeline. Our refinement works on top of any SfM pipeline that is based on local features. We perform a two-stage adjustment of keypoints and bundles. The approach first refines the 2D keypoints only from tentative matches by optimizing a direct cost over dense feature maps. The second stage operates after SfM and refines 3D points and poses with a similar featuremetric cost.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Refined SfM tracks. We show patches centered around reprojections of 3x 3D points observed in 4 images of the St.Peter's Square scene. Deep features and their correlation maps with a reference are robust to scale or illumination changes, yet preserve local details required for fine alignment. Points refined with our approach (in green) are consistent across multiple views while those of a standard SfM pipeline (in red) are misaligned because the initial keypoint detections (in blue) are noisy.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure 5: Triangulation errors vs. track length. The initial, unrefined output, based on geometric BA, exhibits high errors for 3D points that are observed by few images (low track length). Our refinement significantly reduces these errors and brings the accuracy of the sparse point cloud close to the ground truth acquired by Lidar (2mm accuracy).", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 6 :6Figure6: Distribution of point movements. We show the cumulative distribution of the distance traveled by the 2D keypoints during the featuremetric refinement of SuperPoint with KA and BA. 60% of the points move by fewer than 2 pixels and 99% remain within 8 pixels of the initial detections.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 7 :7Figure 7: Impact of the patch size. Smaller patches for each observation significantly reduce memory requirements but can impair the accuracy of the refinement. Patches of size 10\u00d710 offer a good trade-off with high accuracy and moderate memory consumption.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 8 :8Figure 8: Impact of the image resolution. Increasing the image resolution increases the accuracy, but at the cost of longer feature extraction time and higher VRAM requirements. For all experiments on ETH3D, we used a maximum edge length of 1600px, which is very close to saturating the accuracy while providing low run times.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 9 :9Figure 9: Impact of the feature dimensionality. Dense features computed by S2DNet can be naively reduced to accelerate the featuremetric bundle adjustment by 2 while incurring only a minor drop of triangulation accuracy.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "85.04 92.45 0.21 0.87 3.61 57.64 71.92 85.23 0.06 0.34 2.45 \u00eb Patch Flow 80.99 89.06 95.06 0.24 0.97 3.88 64.79 78.90 90.04 0.08 0.41 2.76 \u00eb ours 82.82 89.77 94.77 0.25 0.96 3.75 68.43 80.73 91.28 0.08 0.42 2.75 SuperPoint [21] 75.76 85.61 93.38 0.59 2.21 8.89 50.45 65.07 80.26 0.10 0.55 3.92 \u00eb Patch Flow 85.77 91.57 95.85 0.72 2.51 9.59 64.94 77.65 88.86 0.15 0.77 4.93 \u00eb ours 89.33 93.58 96.58 0.74 2.53 9.51 71.27 82.58 92.08 0.16 0.83 5.06 D2-Net [23] 47.18 64.94 83.37 0.47 1.87 7.07 20.87 34.55 56.53 0.03 0.19 1.78 \u00eb Patch Flow 79.10 86.64 93.26 1.45 4.53 12.95 57.34 70.71 84.12 0.21 1.06 6.02 \u00eb ours 82.49 88.83 94.35 1.36 4.13 11.80 65.71 77.95 89.22 0.21 1.01 5.63 R2D2 [59] 66.30 79.21 90.00 0.53 2.06 8.62 49.32 66.10 83.10 0.11 0.55 3.63 \u00eb Patch Flow 77.94 85.82 92.48 0.66 2.32 9.07 64.14 78.10 90.18 0.16 0.71 4.09 \u00eb ours 80.67 87.61 93.42 0.67 2.31 8.95 67.77 80.85 91.91 0.16 0.73 4.09", "figure_data": "SfM features \u00eb Refinement SIFT [51]ETH3D indoor Accuracy (%) Completeness (%) 1cm 2cm 5cm 1cm 2cm 5cm 75.62 SuperPoint -raw ETH3D outdoor Accuracy (%) Completeness (%) 1cm 2cm 5cm 1cm 2cm 5cmSuperPoint -refinedcorrect/incorrect @ 1cm"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "3D sparse triangulation. Our refinement yields significantly more accurate and complete point clouds than the common geometric SfM pipeline. It is more effective than the existing Patch Flow", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "SIFT 16.92 56.08 81.65 \u00eb Patch Flow 14.62 52.69 81.69 \u00eb ours 25.38 60.22 84.07 \u2022 SuperPoint 15.38 51.20 82.33 \u00eb Patch Flow 28.46 63.99 86.79 \u00eb ours 40.00 71.97 86.86 \u2022 D2-Net 1.54 12.16 56.10 \u00eb Patch Flow 16.92 54.70 75.16 \u00eb ours 17.69 55.03 76.26 \u2022 R2D2 11.53 52.88 82.69 \u00eb Patch Flow 25.38 61.42 84.14 \u00eb ours 27.69 63.86 86.13", "figure_data": "100Recall [%]SfM featuresAUC (%)80\u00eb Refinement \u20221mm 1cm 10cm604020010 0raw10 1 mm10 2 refined"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "SuperPoint+SuperGlue (2k) 58.78 71.01 63.02 77.36 86.76 \u00eb ours 65.89 76.51 68.87 82.09 89.73 SIFT (2k) 38.09 48.05 25.12 50.82 77.28 \u00eb ours 40.59 50.87 28.01 53.59 79.49 D2-Net (4k) 16.83 22.40 16.52 33.07 49.35 \u00eb ours 25.89 33.32 21.33 40.69 57.93", "figure_data": "SfM featuresTask 1: Stereo Task 2: Multiview(# keypoints)AUC@K \u2022AUC@5 \u2022 @N\u00eb Refinement5 \u202210 \u202251025"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "End-to-end SfM. The proposed refinement improves the accuracy of poses estimated by epipolar geometry (stereo) or a complete SfM pipeline (multiview) with crowdsourced imagery. Improvements are substantial for both standard (SIFT) and recent (SuperGlue) matching configurations, especially when few images N observe the scene.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Ablation study on ETH3D.", "figure_data": "100k53560sOursPatch Flow10k 1000 run time [s]162s215s5458s1018s10014s1010 images100 images1000 images3.9K points30K points216K pointsFeaturesF-KAF-BA0%20%40%60%80%100%"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Ablation study for pose estimation. The accuracy of the camera pose is improved by refining the map (KA and BA) and by refining the query keypoints before (qKA) and after (qBA) pose estimation. The largest improvement is brought by qKA. It increases the number of inlier matches and the likelihood of finding the correct pose with RANSAC. various patch sizes. Smaller 10\u00d710 patches achieve sufficient accuracys and require significantly less memory.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Patch Flow [24] 37.00 55.18 0.15 0.93 7.44 5.24 \u00eb F-BA 43.65 62.44 0.18 1.06 7.70 4.17 \u00eb +F-KA (feat-ref) 45.05 64.84 0.18 1.12 7.76 4.88 \u00eb +F-KA (topol-ref) 46.46 65.41 0.19 1.14 8.19 5.02 +F-KA (feat-ref) 43.35 62.54 0.19 1.18 8.36 4.49 \u00eb +F-KA (topol-ref) 44.21 64.22 0.20 1.20 8.72 4.63", "figure_data": "TriangulationAcc. (%)Compl. (%)track\u00eb Refinement1cm 2cm 1cm 2cm 5cmlengthunrefined unrefined \u00eb Patch Flow [24] \u00eb F-BA \u00eb D2-Net SuperPoint \u00eb18.03 31.97 0.07 0.49 5.03 4.17 7.68 13.98 0.02 0.17 2.19 3.29 34.64 52.36 0.16 1.00 8.10 4.99 39.30 58.59 0.15 0.94 6.99 3.29"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Additional ablation study on ETH3D Facade.", "figure_data": "i) Featuremetric keypoint adjustment significantly improvesthe completeness, especially for noisy keypoints as in D2-Net. ii) Keypoint adjustment against the topological centerin each tentative track (topol-ref) improves the point cloudin accuracy and completeness over KA towards the robustfeature center (feat-ref) because it allows to merge tracks."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "81.31 88.50 0.47 1.74 42.22 7.3 \u00eb ours (cost maps) 80.27 87.81 0.47 1.72 29.86 0.15", "figure_data": "SuperPointAcc. (%)Compl. (%) TimeMemory\u00eb Refinement1cm 2cm 1cm 2cm(s)(GB)unrefined64.27 76.47 0.37 1.44--\u00eb ours (exact)"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "Triangulation with cost map approximations. Using precomputed cost maps increase the efficiency of the bundle adjustment with a marginal loss of accuracy. This improvement results in three-dimensional residuals, which is still smaller than D when D=128. Using the spatial derivatives, we can also compute an exact, more accurate bicubic spline interpolation of the cost landscape.Evaluation: We now show experimentally that this approximation often does not, or only minimally, impairs the accuracy of the refinement. Table7reports the results of the triangulation of SuperPoint features on the ETH3D dataset. The approximation reduces the accuracy by less than 1% and does not alter the completeness. It however significantly reduces the memory consumption of the bundle adjustment, allowing it to scale to thousands of images. Note that all experiments in Sections 5.1, 5.2, and 5.3 do not use the cost map approximation as the corresponding scenes are relatively small.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "E BA = j (i,u)\u2208T (j) \u03a0 (R i P j + t i , C i ) \u2212 p u \u03b3 , (1)", "formula_coordinates": [3.0, 55.88, 623.19, 230.48, 20.56]}, {"formula_id": "formula_1", "formula_text": "E j KA = (u,v)\u2208M(j) p v + T v\u2192u [p v ] \u2212 p u \u03b3 ,(2)", "formula_coordinates": [3.0, 325.95, 240.83, 219.17, 23.27]}, {"formula_id": "formula_2", "formula_text": "r = I i [p u ] \u2212 I j [p v ].", "formula_coordinates": [3.0, 464.2, 704.17, 82.65, 10.62]}, {"formula_id": "formula_3", "formula_text": "T v\u2192u [p v ] \u221d \u2212 \u2202I j \u2202p [p v ] r .(3)", "formula_coordinates": [4.0, 108.35, 102.29, 178.01, 22.31]}, {"formula_id": "formula_4", "formula_text": "E j FKA = (u,v)\u2208M(j) w uv F i(u) [p u ] \u2212 F i(v) [p v ] \u03b3 , (4)", "formula_coordinates": [4.0, 57.77, 660.81, 228.6, 23.27]}, {"formula_id": "formula_5", "formula_text": "E FBA = j (i,u)\u2208T (j) F i [\u03a0 (R i P j + t i , C i )] \u2212 f j \u03b3 .", "formula_coordinates": [4.0, 309.85, 514.58, 234.27, 22.94]}, {"formula_id": "formula_6", "formula_text": "f j = argmin f \u2208{f j u } \u00b5 j \u2212 f (6) with \u00b5 j = argmin \u00b5\u2208R D f \u2208{f j u } f \u2212 \u00b5 \u03b3 .(7)", "formula_coordinates": [4.0, 348.75, 629.58, 196.36, 52.59]}, {"formula_id": "formula_7", "formula_text": "F [p] = k w k F k with k w k = 1 . (8", "formula_coordinates": [11.0, 343.16, 410.95, 198.08, 20.17]}, {"formula_id": "formula_8", "formula_text": ")", "formula_coordinates": [11.0, 541.24, 411.3, 3.87, 8.64]}, {"formula_id": "formula_9", "formula_text": "F \u2212 f 2 [p] \u2212 F[p] \u2212 f 2 \u2248 1 \u2212 F[p] 2 = 1 2 k l w k w l F k \u2212 F l 2 . (9", "formula_coordinates": [11.0, 325.47, 479.25, 215.77, 43.84]}, {"formula_id": "formula_10", "formula_text": ")", "formula_coordinates": [11.0, 541.24, 503.27, 3.87, 8.64]}, {"formula_id": "formula_11", "formula_text": "r ij = \uf8eb \uf8ec \uf8ec \uf8ed F i \u2212 f j \u2202 Fi\u2212f j \u2202x \u2202 Fi\u2212f j \u2202y \uf8f6 \uf8f7 \uf8f7 \uf8f8 p ij .(10)", "formula_coordinates": [11.0, 368.11, 668.16, 177.01, 47.48]}], "doi": ""}