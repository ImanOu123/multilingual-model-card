{"title": "Unbiased Gradient Estimation in Unrolled Computation Graphs with Persistent Evolution Strategies", "authors": "Paul Vicol; Luke Metz; Jascha Sohl-Dickstein", "pub_date": "", "abstract": "Unrolled computation graphs arise in many scenarios, including training RNNs, tuning hyperparameters through unrolled optimization, and training learned optimizers. Current approaches to optimizing parameters in such computation graphs suffer from high variance gradients, bias, slow updates, or large memory usage. We introduce a method called Persistent Evolution Strategies (PES), which divides the computation graph into a series of truncated unrolls, and performs an evolution strategies-based update step after each unroll. PES eliminates bias from these truncations by accumulating correction terms over the entire sequence of unrolls. PES allows for rapid parameter updates, has low memory usage, is unbiased, and has reasonable variance characteristics. We experimentally demonstrate the advantages of PES compared to several other methods for gradient estimation on synthetic tasks, and show its applicability to training learned optimizers and tuning hyperparameters.", "sections": [{"heading": "Introduction", "text": "Unrolled computation graphs arise in many scenarios in machine learning, including when training RNNs (Williams & Peng, 1990), tuning hyperparameters through unrolled computation graphs (Baydin et al., 2017;Domke, 2012;Maclaurin et al., 2015;Wu et al., 2018;Franceschi et al., 2017;Donini et al., 2019;Franceschi et al., 2018;Liu et al., 2018;Shaban et al., 2019), and training learned optimizers (Li & Malik, 2016;Andrychowicz et al., 2016;Wichrowska et al., 2017;Metz et al., 2018;2020b;a). Many methods exist for computing gradients in such computation graphs, including ones based on reverse-mode (Williams & Peng, 1990;Tallec & Ollivier, 2017b;Aicher et al., 2019;Grefenstette et al., 2019) and forward-mode (Williams & Zipser, 1989;Tallec & Ollivier, 2017a;Mujika et al., 2018; 1 University of Toronto; work done while on internship at Google. 2 Google Brain. Correspondence to: Paul Vicol <pvicol@cs.toronto.edu>.", "publication_ref": ["b34", "b3", "b36", "b24", "b33", "b9", "b12", "b34", "b29", "b35", "b28", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Proceedings of the 38 th International Conference on Machine", "text": "Learning, PMLR 139, 2021. Copyright 2021 by the author(s). Benzing et al., 2019;Marschall et al., 2019;Menick et al., 2020) gradient accumulation. These methods have different tradeoffs with respect to compute, memory, and gradient variance.\nBackpropagation through time involves backpropagating through a full unrolled sequence (e.g. of length T ) for each parameter update. Unrolling a model over full sequences faces several difficulties: 1) the memory cost scales linearly with the unroll length, because we need to store intermediate activations for backprop (though this can be reduced at the cost of additional compute (Dauvergne & Hasco\u00ebt, 2006;Chen et al., 2016)); 2) we only perform a single parameter update after each full unroll, which is computationally expensive and introduces large latency between parameter updates; 3) long unrolls can lead to exploding or vanishing gradients (Pascanu et al., 2013), and chaotic and poorly conditioned loss landscapes (Pearlmutter, 1996;Maclaurin et al., 2015;Parmas et al., 2018;Metz et al., 2019). This is especially true in meta-learning (Metz et al., 2019).\nThe most commonly-used technique to alleviate these issues is truncated backprop through time (TBPTT) (Werbos, 1990;Tallec & Ollivier, 2017b), which splits the full sequence into shorter sub-sequences and performs a backprop update after processing each sub-sequence. However, a critical drawback of TBPTT is that it yields biased gradients, that can severely impact training (e.g. only taking into account short-term dependencies). To address the poorly conditioned loss surfaces that often result from sequential computation, it can additionally be useful to minimize a smoothed version of the loss. Evolution strategies (ES) is a family of algorithms that estimate gradients using stochastic finite-differences, and which provide an unbiased estimate of the gradient of the objective smoothed with a Gaussian. ES works well on pathological meta-optimization loss surfaces (Metz et al., 2019); however, due to the computational expense of running full unrolls, ES can only practically be applied in a truncated fashion, introducing bias.\nAn alternative to BPTT is real-time recurrent learning (RTRL), which performs forward gradient accumulation (Williams & Zipser, 1989). RTRL enables online parameter updates (after each partial unroll) and does not suffer from truncation bias; however, its memory and compute requirements render it intractable for large-scale problems.", "publication_ref": ["b7", "b8", "b18", "b19", "b3", "b17", "b10", "b10", "b32", "b29", "b10", "b35"], "figure_ref": [], "table_ref": []}, {"heading": "arXiv:2112.13835v1 [cs.LG] 27 Dec 2021", "text": "Many approximations to RTRL have been proposed (Tallec & Ollivier, 2017a;Mujika et al., 2018;Benzing et al., 2019), but most have high variance, are complicated to implement, or are only applicable to a restricted class of models.\nWe introduce an approach to unbiased gradient estimation using short, truncated unrolls, called Persistent Evolution Strategies (PES). In PES, we accumulate the perturbations experienced by the outer parameters in each partial unrollrather than starting perturbations from scratch as in vanilla ES-which yields an unbiased estimate of the gradient even when using truncated sequences. PES is simple to implement, and because it is an evolution strategies-based approach, it retains desirable characteristics such as being trivially parallelizable, memory efficient, and broadly applicable to many different types of problems, including to non-differentiable target functions.", "publication_ref": ["b28", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Contributions", "text": "\u2022 We introduce a method called Persistent Evolution Strategies (PES) to obtain unbiased gradient estimates for the parameters of an unrolled system from partial unrolls of the system.\n\u2022 We prove that PES is an unbiased gradient estimate for a smoothed version of the loss, and an unbiased estimate of the true gradient for quadratic losses.\n\u2022 We provide theoretical and empirical analyses of its variance. In addition, we describe a variance reduction technique for PES, that incorporates the analytic gradient (computed with standard backprop) of the most recent unroll of the dynamical system.\n\u2022 We demonstrate the applicability of PES in several illustrative scenarios: 1) we apply PES to tune hyperparameters including learning rates and momentums, by estimating hypergradients through partial unrolls of optimization algorithms; 2) we use PES to meta-train a learned optimizer; 3) we use PES to learn policy parameters for a continuous control task.\nWe provide a Colab notebook implementation of PES.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Background", "text": "We provide an overview of notation in Appendix A.\nProblem Setup. We consider unrolled computation graphs with state s t updated based on parameters \u03b8 via the recurrence:\ns t = f (s t\u22121 , x t ; \u03b8) (1)\nwhere x t is an optional input at step t. The objective function for optimizing \u03b8 is the sum of per-timestep losses L t (s t ; \u03b8):\nL(\u03b8) = T t=1 L t (s t ; \u03b8) (2)\nThis setup is general, even encompassing situations where we want to consider only the final loss at step T , which can be expressed using a telescoping sum of loss differences between successive steps (Beatson & Adams, 2019). 1 Instances of this problem setup include training RNNs, training learned optimizers, learning policies for control tasks, and unrolled optimization, as illustrated in Figure 1.\nUnrolled Optimization. Optimization algorithms can be unrolled to yield computation graphs, in which the nodes are the model parameters at successive optimization steps.\nEstimating gradients through unrolled optimization has been used to tune hyperparameters (Domke, 2012;Maclaurin et al., 2015;Baydin et al., 2017;Donini et al., 2019;Franceschi et al., 2017) and train learned optimizers (Li & Malik, 2016;Andrychowicz et al., 2016;Wichrowska et al., 2017;Metz et al., 2019;2020b;a;.\nTruncation Bias. Truncation, or short horizon, bias poses a major challenge when unrolled optimization is decomposed into a sequence of short sequential unrolls of length K T . These challenges have been demonstrated in gradient-based hyperparameter optimization (Wu et al., 2018) and in the training of learned optimizers (Metz et al., 2019). Approaches to mitigating short horizon bias are an area of active research (Micaelli & Storkey, 2020).\nSmoothing. Unrolling optimization for many steps can lead to pathological meta-loss surfaces that exhibit neardiscontinuities and chaotic structure (Parmas et al., 2018;Metz et al., 2019). Optimization on such non-smooth landscapes fails due to exploding gradients or gets stuck in poor local minima. One effective method to address these pathologies is to smooth the meta-loss surface, e.g. descend the Gaussian-blurred objective L(\u03b8) = E\u03b8 \u223cN (\u03b8,\u03c3 2 I) [L(\u03b8)] (Staines & Barber, 2012;Metz et al., 2019). Conveniently, ES provides an unbiased estimate of the gradient of this smoothed objective. However, the ES estimate remains biased when computed on truncations.\nEvolution Strategies. Evolution Strategies (ES) (Rechenberg, 1973;Nesterov & Spokoiny, 2017) refers to a family of methods for estimating a descent direction for arbitrary black-box functions using stochastic finite differences. Since ES only requires function evaluations and not gradients, it is a zeroth-order optimization method. The vanilla ES estimator is defined as: \ng ES = 1 N \u03c3 2 N i=1 (i) L(\u03b8 + (i) )(3)", "publication_ref": ["b3", "b33", "b10", "b12", "b36", "b10", "b13", "b17", "b10", "b26", "b10", "b20", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Task", "text": "Objective:\nFigure 1. An unrolled computation graph, illustrating how both RNNs and unrolled optimization can be described using Equations 1 and 2. In RNN training, st is the hidden state of the RNN, Lt(\u2022) is the prediction cross entropy at each timestep, \u03b8 refers to the RNN parameters, and f corresponds to the forward pass of the RNN, that takes an input and the previous hidden state (st) and returns a new hidden state (st+1). In unrolled optimization, st contains the parameters of the base model and optimizer accumulators (e.g. momentum), Lt(\u2022) is a meta-objective such as validation performance, \u03b8 contains hyperparameters (e.g. the learning rate, weight decay, etc.) that govern the optimization, and f corresponds to the update step of an optimization algorithm such as SGD, RMSprop (Tieleman & Hinton, 2012), or Adam (Kingma & Ba, 2015).\nwhere (i) \u223c N (0, \u03c3 2 I). ES is trivially parallelizable, and thus highly scalable-it has seen renewed interest in recent years as a viable optimization algorithm for reinforcement learning among other black-box problems (Salimans et al., 2017;Mania et al., 2018;Ha & Schmidhuber, 2018;Houthooft et al., 2018;Cui et al., 2018;Ha, 2020). The estimator in Eq. 3 has high variance, and thus many variance reduction techniques have been proposed, including control variates (Tang et al., 2020) and antithetic sampling (Owen, 2013). Antithetic sampling involves using pairs of function evaluations \u03b8 + and \u03b8 \u2212 , yielding the following estimator:\ng ES-A = 1 N \u03c3 2 N/2 i=1 (i) (L(\u03b8 + (i) ) \u2212 L(\u03b8 \u2212 (i) ))\nwhere N is even, and (i) \u223c N (0, \u03c3 2 I). Several methods have been proposed to improve the search space for ES, including covariance matrix adaptation ES (CMA-ES) (Hansen, 2016) and Guided ES (Maheswaranathan et al., 2018). A limitation of ES is that applying it to full unrolls is often computationally costly (as we only make one update to the system parameters every full unroll), while applying ES to partial unrolls suffers from truncation bias similarly to TBPTT. In contrast, PES allows computation of gradients from partial updates without incurring truncation bias.\nHysteresis. Any approach that performs online parameter updates, including RTRL and its approximations, will suffer from hysteresis, which refers to the dependence of the state of a system on its history. This is due to the fact that if we update \u03b8, then any accumulated state (e.g. in the case of RTRL, the accumulated Jacobian dst d\u03b8 ) will be incorrect because it is computed from previous values of \u03b8. To eliminate hysteresis completely, one would need to run the full sequence for a given problem for each parameter update, which is often prohibitively expensive. In practice, hysteresis can be mitigated by using sufficiently small learning rates; this introduces a tradeoff between training stability and training speed.", "publication_ref": ["b31", "b22", "b5", "b30", "b16", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "In this section, we discuss additional related work on online learning algorithms, and on one special class of unrolled optimization problems: hyperparameter optimization (HO). Table 1 compares several approaches to gradient estimation in unrolled computation graphs, with respect to compute, memory, parallelization, unbiasedness, and smoothing. In addition, Table 4 in Appendix B provides a comparison of the HO algorithms mentioned in this section.\nOnline Learning Algorithms. Real-time recurrent learning (RTRL) performs forward-mode gradient accumulation: it does not require storage of past states, but requires matrix-matrix products and storage of a matrix G t of size dim(s t ) \u00d7 dim(\u03b8). When dim(\u03b8) is large, as in RNN training, the cost of storing G t and the cost of computing the required matrix-matrix products is prohibitive. Several approaches propose efficient variants of RTRL based on cheaper, noisy approximations of G t . Unbiased Online Recurrent Optimization (UORO) (Tallec & Ollivier, 2017a) uses an unbiased rank-1 approximation to the full matrix; Kronecker-Factored RTRL (KF-RTRL) (Mujika et al., 2018) uses a Kronecker product decomposition to approximate the RTRL update for a class of RNNs; and Optimal Kronecker Sum Approximation (OK) (Benzing et al., 2019) uses a similar approximation but with the lowest possible variance among methods within an approximation family. Cooijmans & Martens (2019) also draw a connection between UORO and REINFORCE applied to estimate the gradient of an RNN by injecting noise into the hidden states. In contrast, PES injects noise into the parameters.\nHyperparameter Optimization (HO) There are three main approaches that can be categorized based on the types of problem-specific information used: 1) black-box approaches that do not consider the internal structure of the Table 1. Comparison of approaches for learning parameters in unrolled computation graphs. S is the size of the system state (e.g. the RNN hidden state dimension, or in the case of hyperparameter optimization the inner-problem's weight dimensionality and potentially the optimizer state; P is the dimensionality of \u03b8; T is the total number of steps in a sequence/unroll; K is the truncation length; and N is the number of samples (also called particles) used for the reparameterization gradient and in ES-based algorithms; F and B are the costs of a forward and backward pass, respectively; terms in purple denote computation/memory that can be split across parallel workers. See Appendix J for details.", "publication_ref": ["b28", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Method", "text": "Compute Memory Parallel Unbiased Optimize Non-Diff. Smoothed\nBPTT (Rumelhart et al., 1985) T (F + B) T S TBPTT (Williams & Peng, 1990) K(F + B) KS ARTBP (Tallec & Ollivier, 2017b) K(F + B) KS RTRL (Williams & Zipser, 1989) P S 2 + S(F + B) SP + S 2 UORO (Tallec & Ollivier, 2017a) F + B + S 2 + P S + P Reparam. (Metz et al., 2019) N T (F + B) N T S ES (Rechenberg, 1973) N T F N S Trunc. ES (Metz et al., 2019) N KF N S\nPES (Ours) N KF N (S + P ) PES + Analytic (Ours) N KF + K(F + B) N (S + P ) + (K + 1)S\nobjective L; 2) gray-box approaches that make use of the fact that the objective is the result of an iterative optimization procedure (e.g. by using the validation performance of a model); and 3) gradient-based approaches that require access to the exact functional form of the objective L, and that require the objective to be differentiable in the hyperparameters. Black-box approaches include grid search, random search (Bergstra & Bengio, 2012), Bayesian optimization (BO) (Snoek et al., 2012), and ES (Salimans et al., 2017;Metz et al., 2019). Gray-box approaches include Freeze-Thaw BO (Swersky et al., 2014), successive halving (Jamieson & Talwalkar, 2016), Hyperband (Li et al., 2017, Population-Based Training (Jaderberg et al., 2017), and hypernetwork-based approaches to HO (Lorraine & Duvenaud, 2018;MacKay et al., 2019).\nA key advantage of gradient-based approaches is that they scale to high-dimensional hyperparameters (e.g. millions of hyperparameters) (Lorraine et al., 2020). Maclaurin et al. (2015) differentiate through unrolled optimization to tune many hyperparameters including learning rates and weight decay coefficients. These methods can perform poorly, however, when the underlying meta-loss is not smooth. Additionally they cannot optimize non-differentiable objectives, for example accuracy rather than loss.\nPES can be considered a gray-box approach as it does not require the objective to be differentiable like gradient-based approaches, but it does take into account the iterative optimization of the inner problem.", "publication_ref": ["b21", "b34", "b29", "b35", "b28", "b10", "b20", "b10", "b25", "b22", "b10", "b27", "b0", "b2", "b1", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "Persistent Evolution Strategies", "text": "In this section, we introduce a method to obtain unbiased gradient estimates from partial unrolls of a computation graph, called Persistent Evolution Strategies (PES). First, we derive the PES gradient estimator, prove that it is unbi-ased, and present a practical algorithm (Algorithm 2). Then we discuss the variance characteristics of PES, both theoretically and empirically.\nDerivation. 2 Unrolled computation graphs (as illustrated in Figure 1) depend on shared parameters \u03b8 at every timestep; in order to account for how these contribute to the overall gradient \u2207 \u03b8 L(\u03b8), we use subscripts \u03b8 t to distinguish between applications of \u03b8 at different steps, where \u03b8 t = \u03b8, \u2200t. We further define \u0398 = (\u03b8 1 , . . . , \u03b8 T ) , which is a matrix with the per-timestep \u03b8 t as its rows. For notational simplicity in the following derivation, we drop the dependence on s t and explicitly include the dependence on each \u03b8 t , writing L t (s t ; \u03b8) as either L t (\u03b8 1 , . . . , \u03b8 t ) or simply L t (\u0398). We wish to compute the gradient \u2207 \u03b8 L(\u03b8) of the total loss over all unrolls. We begin by writing this gradient in terms of the full gradient \u2202L(\u0398) \u2202 vec(\u0398) \u2208 R P T \u00d71 , and then using ES to approximate \u2202L(\u0398) \u2202 vec(\u0398) ,\ndL(\u03b8) d\u03b8 = T \u03c4 =1 \u2202L (\u0398) \u2202\u03b8 \u03c4 = I \u2297 1 \u2202L(\u0398) \u2202 vec (\u0398) , g PES = I \u2297 1 E 1 \u03c3 2 vec ( ) L (\u0398 + ) = 1 \u03c3 2 E T \u03c4 =1 \u03c4 L (\u0398 + ) ,\nwhere \u2297 denotes the Kronecker product, = ( 1 , . . . , T ) is a matrix of perturbations t to be added to the \u03b8 t at each timestep, and the expectation is over entries in drawn from an i.i.d. Gaussian with variance \u03c3 2 . This ES approximation Algorithm 1 Truncated Evolution Strategies (ES) applied to partial unrolls of a computation graph. Input: s 0 , initial state K, truncation length for partial unrolls N , number of particles \u03c3, standard deviation of perturbations \u03b1, learning rate for ES optimization Initialize s = s 0 s (i) = s 0 Initialize \u03be (i) \u2190 0 for i \u2208 {1, . . . , N } while true d\u00f4 g ES \u2190 0 for i = 1, . . . , N do\n(i) = draw from N (0, \u03c3 2 I) i odd \u2212 (i\u22121)\ni even\nL (i) K \u2190 unroll(s, \u03b8 + (i) , K) \u03be (i) \u2190 \u03be (i) + (i) g ES \u2190\u011d ES + (i)L (i) K end for g ES \u2190 1 N \u03c3 2\u011d ES s \u2190 unroll(s, \u03b8, K) \u03b8 \u2190 \u03b8 \u2212 \u03b1\u011d ES end while\nAlgorithm 2 Persistent evolution strategies (PES). Differences from ES are highlighted in purple.\nInput: s 0 , initial state K, truncation length for partial unrolls N , number of particles \u03c3, standard deviation of perturbations \u03b1, learning rate for PES optimization Initialize\ns (i) = s 0 for i \u2208 {1, . . . , N } Initialize \u03be (i) \u2190 0 for i \u2208 {1, . . . , N } while true d\u00f4 g PES \u2190 0 for i = 1, . . . , N do (i) = draw from N (0, \u03c3 2 I) i odd \u2212 (i\u22121)\ni even\ns (i) ,L (i) K \u2190 unroll(s (i) , \u03b8 + (i) , K) \u03be (i) \u2190 \u03be (i) + (i) g PES \u2190\u011d PES + \u03be (i)L(i) K end for g PES \u2190 1 N \u03c3 2\u011d PES s \u2190 unroll(s, \u03b8, K) \u03b8 \u2190 \u03b8 \u2212 \u03b1\u011d PES end while Figure 2.\nA comparison of vanilla ES and PES gradient estimators, applied to partial unrolls of a computation graph. The conditional statement for (i) is used to implement antithetic sampling. For clarity, we describe the meta-optimization updates to \u03b8 using SGD, but we typically use Adam in practice. See Appendix K for diagrammatic representations of these algorithms.\nis an unbiased estimator of the gradient of the Gaussiansmoothed objective E [L(\u0398 + )]. We next show that g PES decomposes into a sum of sequential gradient estimates,\ng PES = 1 \u03c3 2 E T \u03c4 =1 \u03c4 T t=1 L t (\u0398 + ) = 1 \u03c3 2 E T t=1 t \u03c4 =1 \u03c4 L t (\u0398 + ) (4) = E T t=1\u011d PES t, ,(5)\ng PES t, = 1 \u03c3 2 \u03be t L t (\u03b8 1 + 1 , . . . , \u03b8 t + t ) . (6\n)\nwhere \u03be t = t \u03c4 =1 \u03c4 , Equation 4 relies on L t (\u2022) being independent of \u03c4 for \u03c4 > t, and Equation 6 similarly relies on L t (\u2022) only being a function of \u03b8 \u03c4 for \u03c4 \u2264 t. The PES estimator consists of Monte Carlo estimates of Equation 5,\ng PES = 1 N N i=1 T t=1\u011d PES t, (i)(7)\nwhere (i) are samples of , and N is the number of Monte Carlo samples. Gradient estimates at each time step can be evaluated sequentially, and used to perform SGD.\nPES with Antithetic Sampling. In practice, we use antithetic sampling to reduce variance. The PES estimator with antithetic sampling, which we denote\u011d PES-A , is given by:\ng PES-A = (I \u2297 1 )E 1 2\u03c3 2 vec ( ) (L(\u0398 + ) \u2212 L(\u0398 \u2212 )) \u2248 1 2\u03c3 2 N N i=1 T t=1 \u03be (i) t L t (\u0398 + (i) ) \u2212 L t (\u0398 \u2212 (i) )\nPES is Unbiased for Quadratic Losses. See Appendix F for a proof of the following Statement 4.1.\nStatement 4.1 (PES is unbiased). Let \u03b8 \u2208 R P and L(\u03b8) = T t=1 L t (\u03b8).\nSuppose that \u2207 \u03b8 L(\u03b8) exists, and assume that L is quadratic, so that it is equivalent to its second-order Taylor series expansion:\nL(\u0398 + ) = L(\u0398) + vec( ) \u2207 vec(\u0398) L(\u0398) + 1 2 vec( ) \u2207 2 vec(\u0398) L(\u0398) vec( ). Then, bias(\u011d PES-A ) = E [\u011d PES-A ] \u2212 \u2207 \u03b8 L(\u03b8) = 0.\nAlgorithm. Based on Eq. 7, we see that we can obtain unbiased gradient estimates from partial unrolls by: 1) not resetting the particles between unrolls, and 2) accumulating the perturbations \u03be t each particle has experienced over multiple unrolls. The resulting algorithm is simple to implement, requiring only minor modifications from vanilla ES.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Scenario tr Var \u011d PES-", "text": "A Diagonal M , i.i.d. grads ||g|| 1 2 P T + 1 2 P + T Diagonal M , identical grads ||g|| 1 2T P + 1 2 P + 1 Upper-tri M , i.i.d. grads ||g|| O(T 2 + P T ) Upper-tri M , identical grads ||g|| O( P T )\nTable 2. The variance of the PES estimator depends on the covariance of gradients across timesteps. The variance of the gradient estimate is given as a function of the number of parameters P , unrolls T , and true gradient norm ||g||. Each row corresponds to different structure in the gradient matrix M (Equation 8). In the best case, subdividing a sequence into more PES unrolls T reduces the variance by a factor of 1 T . See Appendix G for details, and Figure 3 for empirical variance scaling on an RNN task.\nAlgorithm 1 describes truncated ES applied to partial unrolls, where it suffers from short horizon bias. Algorithm 2 shows PES applied to the same problem, where it provides unbiased gradient estimates. Both algorithms (Fig. 2) are shown with antithetic sampling (perturbations are paired with their negations), which drastically reduces variance.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Variance Analysis", "text": "We use the total variance, tr(Var(\u011d PES-A )), to quantify the variance of the estimator. We provide a full derivation of the variance in Appendix G, and here we present some takeaways. The variance depends on the gradients of each loss term L t with respect to each of the per-timestep parameters \u03b8 \u03c4 . To gain insight into the structure of these gradients, we can arrange them in a matrix:\nM = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 \u2207 \u03b81 L 1 \u2207 \u03b81 L 2 \u2207 \u03b81 L 3 \u2022 \u2022 \u2022 \u2207 \u03b81 L T 0 \u2207 \u03b82 L 2 \u2207 \u03b82 L 3 \u2022 \u2022 \u2022 \u2207 \u03b82 L T 0 0 \u2207 \u03b83 L 3 \u2022 \u2022 \u2022 \u2207 \u03b83 L T . . . . . . . . . . . . . . . 0 0 0 \u2022 \u2022 \u2022 \u2207 \u03b8 T L T \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb(8)\nM is upper-triangular due to the fact that \u2207 \u03b8\u03c4 L t = 0 for all \u03c4 > t. The variance of the PES estimator depends on the covariance between the gradients \u2207 \u03b8\u03c4 L t in this matrix.\nVariance Scenarios. We consider two structures for M : 1) a diagonal structure, where the gradients \u2207 \u03b8i L j = 0, \u2200i = j; and 2) an upper-triangular structure. For each of these two matrix structures, we consider two possibilities for the covariance between gradients: a) all gradients \u2207 \u03b8i L j are identical; b) all gradients are i.i.d. The total variance for each of the four resulting scenarios is shown in Table 2. It is possible for the g t to have variance larger than any of these scenarios, though we do not observe this in practice. Empirical Variance Measurements. To investigate the variance characteristics of PES empirically, we computed the variance in a toy setting. We used an LSTM with 5 hidden units and 5-dimensional embeddings, for character-level language modeling on the Penn Treebank corpus (Marcus et al., 1993) (with a vocabulary consisting of 50 unique tokens). We measured the variance of the\u011d PES-A gradient estimate on a fixed sequence of 10 4 characters. The ground-truth gradient of the smoothed objective was computed using vanilla ES with 5000 particles over the full sequence (without truncation). Figure 3 shows the variance of the PES gradient estimate using different numbers of unrolls, ranging from 1 (a single unroll for the full sequence) to 10 4 (one unroll per input token). Note that we do not update the parameters of the RNN after each unroll; we simply accumulate the gradient estimates over all partial unrolls. We plot the variance normalized by the squared norm of the ground-truth gradient. We observe an initial drop in variance, and then a linear growth. Additional empirical variance measurements are presented in Figure 16 (Appendix G).\nReducing Variance by Incorporating the Analytic Gradient. For functions L that are differentiable, we can use the analytic gradient from the most recent partial unroll (e.g., backpropagating through the last K-step unroll) to reduce the variance of the PES gradient estimates. In Appendix H, we show how we can incorporate the analytic gradient in the ES estimate for \u2202Lt(\u0398) \u2202\u03b8 , deriving the following estimator: where p t = \u2202Lt(\u0398) \u2202\u03b8t . We call the resulting estimator PES+Analytic. In Appendix H we describe the implementation of this estimator (Algorithm 4), which requires a few simple changes from the standard PES estimator. We also provide empirical variance measurements for PES+Analytic, using the same setup as was used for Figure 3; we found that it can reduce variance by 1-2 orders of magnitude, given the same number of particles as PES.\n\u2202L t (\u0398) \u2202\u03b8 \u2248 1 \u03c3 2 E \u03c4 <t \u03c4 (L t (\u0398 + ) \u2212 t p t ) + p t(", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "First, we demonstrate via a toy experiment that PES does not suffer from truncation bias, allowing it to converge to correct solutions that are not found by TBPTT or truncated ES. Then, we apply PES to several illustrative scenarios: we use PES to meta-train a learned optimizer, learn a policy for continuous control, and optimize hyperparameters. All experiments used JAX (Bradbury et al., 2018). A simplified code snippet implementing PES is provided in Appendix M.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Influence Balancing", "text": "To demonstrate the lack of truncation bias of PES in a toy setting, we use the influence balancing task introduced by Tallec & Ollivier (2017a). This simple task is particularly sensitive to short-horizon bias, as the gradient for the single parameter \u03b8 \u2208 R has the wrong sign when estimated from short unrolls. See Appendix C.2 for more details. We use vanilla SGD to update \u03b8, with gradient estimates derived from TBPTT with different truncation horizons, exact RTRL, UORO, and PES. TBPTT does not converge with short truncations K \u2208 {1, 10}; it requires much longer truncations (K = 100) to move in the right direction. In Figure 4, we show that PES achieves nearly identical performance to exact RTRL. UORO reaches the same performance as RTRL after approximately 30k iterations. Note that the purpose of this experiment is to demonstrate that PES is unbiased, and is able to match the performance of exact RTRL given sufficiently many particles (N = 10 3 ) to reduce variance; it is not intended as a comparison of total compute. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Learned Optimizer Meta-Optimization", "text": "In this section we demonstrate PES's applicability for learned optimizer training. We meta-train an MLP-based learned optimizer as described in Metz et al. (2019). This optimizer is used to train a two hidden-layer, 128 unit, MLP on CIFAR-10 with a batch size of 128. Our meta-objective is the average training loss. We train with a total number of inner-steps of T = 1000 and a truncation length of K = 4, using both PES and truncated ES. We outer-train with Adam, using a learning rate of 10 \u22124 selected via grid search over half-orders of magnitude for each method independently.\nWe use gradient clipping of 3 applied to each gradient coordinate. We outer-train on 8 TPUv2 cores with asynchronous, batched updates of size 16. To evaluate, we compute the meta-loss averaged over 20 inner initializations over the course of meta-training. Results can be found in Figure 5. Due to PES's unbiased nature, PES achieves both lower losses, and is more consistent across random initializations of the learned optimizer.", "publication_ref": ["b10"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Learning a Continuous Control Policy", "text": "Recent work (Salimans et al., 2017;Mania et al., 2018) has shown that ES-based algorithms can be a viable alternative to more complex RL algorithms. ES optimizes the parameters of a policy directly, by sampling parameters from a distribution, running an episode, and estimating the gradient; this is in contrast to standard RL algorithms that sample actions from a distribution output by a policy. Here, we demonstrate that PES can be used to train a policy for a continuous control problem using partial unrolls, improving on the efficiency of vanilla ES typically applied to full unrolls. We train a linear policy on the Swimmer-v2 MuJoCo environment, following Mania et al. (2018). For PES, the objective for each partial unroll is the sum of rewards over that unroll. We also applied vanilla ES to the partial unrolls to demonstrate that this na\u00efve strategy does not worktruncation bias occurs for these control problems as well. Figure 7 compares vanilla ES applied to full episodes, ES Figure 6. The meta-objective surface (left), and meta-objective vs inner problem timesteps (right), for the 2D regression problem in Section 5.4. We plot meta-optimization trajectories for TBPTT, UORO, RTRL, ES, and PES starting from the same initialization, (\u22124.5, \u22124.5) in log-space. All techniques except PES either suffer from truncation bias, or become stuck due to high-frequency structure in the meta-objective surface. PES is both unbiased, and smooths the outer-objective removing high-frequency structure. Ablations over the truncation length and number of particles for this task are provided in Appendix L. applied to partial episodes, PES applied to partial episodes, and variants of ES from Mania et al. (2018) and Salimans et al. (2017). To evaluate policies, we computed the average full-episode reward over 50 random environment seeds. In Figure 7, we show the mean performance of each algorithm over 6 random seeds, with standard deviation shown by the shaded region. We see that PES reaches the same performance as full-unroll ES in slightly fewer total environment steps.", "publication_ref": ["b22", "b5", "b5", "b5", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Hyperparameter Optimization", "text": "In this section we demonstrate that PES can be used for hyperparameter optimization across four different problems.\nWe show that PES performs well when the meta-loss has many local minima, does not suffer from truncation bias, can be applied to non-differentiable objectives, and can be used to optimize many hyperparameters (both continuous and discrete) simultaneously.\nToy 2D Regression. First, we used PES to meta-optimize a learning rate schedule for a toy 2D regression problem that has one global minimum, but many local minima to which truncated gradient methods could converge.\nThe inner optimization trajectories for different values of the outer-parameters are shown in Appendix C. We tuned a linear learning rate schedule parameterized by the initial and final log-learning rates, \u03b8 0 and \u03b8 1 , respectively: \u03b1 t = 1 \u2212 t T e \u03b80 + t T e \u03b81 . In Figure 6 we compare TBPTT, UORO, RTRL, ES, and PES applied to this metaoptimization task. We found that the gradient-based methods (TBPTT, UORO, and RTRL) got stuck in suboptimal regions due to high-frequency structure in the meta-loss landscape. ES makes more progress due to smoothing, but still suffers from truncation bias. PES smooths the metaobjective surface and is unbiased, converging to a substantially better solution.\nMNIST MLP. Next, we used PES to meta-learn a learning rate schedule for an MLP classifier on MNIST. Following Wu et al. (2018), we used a two-layer MLP with 100 hidden units per layer and ReLU activations and the learning rate schedule parameterization \u03b1 t = \u03b80 (1+ t Q )\n\u03b8 1 , where \u03b1 t is the learning rate at step t, \u03b8 0 is the initial learning rate, \u03b8 1 is the decay factor, and Q is a constant fixed to 5000. This schedule is used for SGD with fixed momentum 0.9. The full unrolled inner problem consists of T = 5000 optimization steps, and we apply vanilla ES and PES with truncation lengths K \u2208 {10, 100}, yielding 500 and 50 unrolls per inner problem, respectively. The meta-objective is the sum of training losses over the inner optimization trajectory. In Figure 8(a) we see that ES converges to a suboptimal region of the hyperparameter space due to truncation bias, while PES finds the correct solution. Targeting Validation Accuracy. Because PES only requires function evaluations and not gradients, it can optimize non-differentiable objectives such as accuracy rather than loss. We demonstrate this by tuning the same parameterization of learning rate schedule as before, but using the accuracy on the MNIST validation set as the meta-objective. Figure 8(b) compares the meta-optimization trajectories of ES and PES on the validation accuracy meta-objective; again ES is biased and fails to converge to the right solution, while PES works well.\nTuning Many Hyperparameters. Here, we show that PES can tune several hyperparameters simultaneously, and achieves better performance than random search with an uninformative search space, using less compute. We tuned both continuous and discrete hyperparameters: the number of units per hidden layer (discrete architectural hyperparameters) and per-parameter-block learning rates and momentum coefficients (continuous hyperparameters). We trained a 5-hidden-layer MLP (6 layers including the output layer mapping to logits) on FashionMNIST, yielding 29 hyperparameters in total. We set the maximum number of hidden units per layer to 100, and tuned sigmoid-transformed hyperparameters representing the fraction of hidden units to use. The meta-objective was the sum of validation losses over the inner optimization trajectory.  Figure 9 compares the best meta-objective values achieved by random search, vanilla ES, and PES, expressed in terms of the total number of inner iterations used (which accounts for the particles used in ES and PES). We ran each method with four random seeds, and plot the mean (solid lines) and the min/max (shaded region) performance. Each evaluation computes the mean meta-objective over 10 full inner problems using different random seeds for model initialization and data sampling. PES outperforms ES and random search, achieving lower loss using less compute.", "publication_ref": ["b36"], "figure_ref": ["fig_1", "fig_1", "fig_2"], "table_ref": []}, {"heading": "Conclusion", "text": "We introduced a method for unbiased gradient estimation in unrolled computation graphs, called Persistent Evolution Strategies (PES). PES obtains gradients from truncated unrolls-which speeds up optimization by allowing for frequent parameter updates-while not suffering from truncation bias that affects many competing approaches. We show that PES is broadly applicable, with experiments demonstrating its application to an RNN-like task, hyperparameter optimization, reinforcement learning, and meta-training of learned optimizers. This appendix is structured as follows:\n\u2022 In Section A we give an overview of the notation used in this paper.\n\u2022 In Section B we provide a table comparing several hyperparameter optimization approaches.\n\u2022 In Section C we provide experimental details.\n\u2022 In Section D we discuss telescoping sums as a way to target the final loss rather than the sum of losses as the meta-objective.\n\u2022 In Section E we provide a derivation of the PES estimator.\n\u2022 In Section F we prove that PES is unbiased.\n\u2022 In Section G we derive the variance of the PES estimator.\n\u2022 In Section H we derive a variant of the PES estimator that incorporates the analytic gradient from the most recent partial unroll to reduce variance.\n\u2022 In Section I we show the connection between PES and the framework for gradient estimation in stochastic computation graphs introduced in Schulman et al. (2015).\n\u2022 In Section J we show derivations and compute/memory costs of the methods in Table 1.\n\u2022 In Section K we provide diagrammatic representations of the ES and PES algorithms.\n\u2022 In Section L we provide an ablation study over the truncation length and number of particles for PES.\n\u2022 In Section M we provide simplified code to implement PES in JAX (Bradbury et al., 2018). The total sequence length / total unroll length of the inner problem K", "publication_ref": ["b23"], "figure_ref": [], "table_ref": []}, {"heading": "A. Notation", "text": "The truncation length for subsequences / partial unrolls S\nThe dimensionality of the state of the unrolled system, dim(s) P\nThe dimensionality of the parameters of the unrolled system, dim(\u03b8) \u03b8\nThe parameters of the unrolled system \u03b8 t\nThe parameters of the unrolled system at time t, where \u03b8 t = \u03b8, \u2200t \u0398\nA matrix whose rows are the parameters at each timestep, \u0398 = (\u03b8 1 , . . . , \u03b8 T ) s t\nThe state of the unrolled system at time t x t\nThe (optional) external input to the unrolled system at time t f\nThe update function that evolves the unrolled system N\nThe number of particles for ES and PES \u03c3 2\nThe variance of the ES/PES perturbations t A perturbation applied to the parameters \u03b8 at timestep t A matrix whose rows are the perturbations at each timestep, = ( 1 , . . . , T ) \u03be t\nThe sum of PES perturbations up to time t, \u03be\nt = 1 + \u2022 \u2022 \u2022 + t L t (\u0398)\nThe loss at timestep t, L t (\u0398) = L t (\u03b8 1 , . . . , \u03b8 t )\nL(\u03b8), L(\u0398) The total loss, L(\u03b8) = L(\u0398) = T t=1 L t (\u0398) = T t=1 L t (\u03b8 1 , . . . , \u03b8 t ) g t\nThe true gradient at step t: \u2207 \u03b8 L t (\u03b8) \ng (t, \u03c4 ) Shorthand for \u2202Lt(\u0398) \u2202\u03b8\u03c4 , used in variance expressions \u2297 Kronecker product \u03b1\nThe learning rate for the parameters \u03b8 unroll(s, \u03b8, K)\nA function that unrolls the system for K steps starting with state s, using parameters \u03b8. Returns the updated state and loss resulting from the unroll ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B. Hyperparameter Optimization Methods", "text": "Table 4 presents a comparison of several hyperparameter optimization approaches. We distinguish between black-box, gray-box, and gradient-based approaches, and focus our comparison on whether each method can tune optimization hyperparameters, regularization hyperparameters, and discrete hyperparameters, as well as whether the method requires multiple runs through the inner problem or is online (operating within the timespan of a single inner problem), and whether the method is unbiased, meaning that it will eventually converge to the optimal hyperparameters.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Method", "text": "Type ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C. Experiment Details", "text": "In this section, we provide details for the experiments from Section 5.\nComputing Infrastructure. All experiments except for learned optimizer training were run on NVIDIA P100 GPUs (using only a single GPU per experiment). The learned optimizer experiment in Section 5.2 was trained on 8 TPUv2 cores; we used asynchronous multi-TPU training for convenience, not necessity (these experiments could be run on a single GPU if desired).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1. 2D Toy Regression", "text": "The inner objective is a toy 2D function defined as:\nf (x 0 , x 1 ) = x 2 0 + 5 \u2212 \u221a 5 + sin 2 (x 1 ) exp(\u22125x 2 0 ) + 0.25|x 1 \u2212 100|(10)\nThis was manually designed to be a challenging problem for any meta-optimization method that suffers from truncation bias. In Figure 10 we visualize the outer loss surface (aka the meta-loss surface) and the inner loss surface for this task; we show the optimization trajectories on the inner loss surface corresponding to three different choices of optimization hyperparameters (shown by color-coded markers). In our experiments, the total inner problem length was T = 100, and we used truncated unrolls of length K = 10. For ES and PES, we used perturbation variance \u03c3 2 = 1, and 100 particles (50 antithetic pairs). We used Adam with learning rate 1e-2 as the outer optimizer for all methods (TBPTT, RTRL, UORO, ES, and PES). The influence balancing task considers learning a scalar parameter \u03b8 \u2208 R that governs the evolution of the following unrolled system:\ns t+1 = As t + (\u03b8, . . . , \u03b8 p positive , \u2212\u03b8, . . . , \u2212\u03b8 n \u2212 p negative ) (11\n)\nwhere A is a fixed n \u00d7 n matrix with A i,i = 0.5, A i,i+1 = 0.5 and 0 everywhere else. The vector on the right hand side consists of \u03b8 tiled n times, with p positive and n \u2212 p negative copies. In our experiments, we used n = 23 and p = 10. The loss at each step is regression on the first index in the state vector s t :\nL t = 1 2 (s 0 t \u2212 1) 2(12)\nFor the influence balancing experiment, we used n = 23 with 10 positive and 13 negative \u03b8's. The state was initialized to a vector of ones, s 0 = 1, and \u03b8 was initialized to 0.5. We used gradient descent for optimization, with learning rate 1e-4. We did not use learning rate decay as was used in (Tallec & Ollivier, 2017a), as we did not find this to be necessary for convergence. For ES and PES we used perturbation scale \u03c3 = 0.1 and 10 3 particles.", "publication_ref": ["b28"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "C.3. MNIST Experiments", "text": "MNIST Meta-Optimization. Following Wu et al. (2018), we used a two-layer MLP with 100 hidden units per layer and ReLU activations and the learning rate schedule parameterization \u03b1 t = \u03b80 (1+ t Q )\n\u03b8 1 , where \u03b1 t is the learning rate at step t, \u03b8 0 is the initial learning rate, \u03b8 1 is the decay factor, and Q is a constant fixed to 5000. This schedule is used for SGD with fixed momentum 0.9. We used mini-batches of size 100. The full unrolled inner problem consists of T = 5000 optimization steps, and we used vanilla ES and PES with truncation lengths K \u2208 {10, 100}, yielding 500 and 50 unrolls per inner problem. The meta-objective is the sum of training softmax cross-entropy losses over the inner optimization trajectory. We used Adam as the outer-optimizer, and for each method (ES and PES), we performed a grid search over the outer-learning rates {0.01, 0.03, 0.1} to find the most stable and fastest-converging setups. For both ES and PES, we used perturbation standard deviation \u03c3 = 0.1, and 1000 particles (500 antithetic pairs).\nTuning Many Hyperparameters & Comparison to Random Search. We trained on FashionMNIST with minibatch size 100 for T = 1000 inner problem steps, using truncations of length K = 10, yielding 100 unrolls per inner problem. For both ES and PES, we used \u03c3 = 0.3 and used Adam with learning rate 1e \u2212 2 as the outer optimizer. We used an MLP with ReLU activations and 5 hidden layers (6 layers including the output layer mapping the final hidden representation to logits). We tuned separate learning rates and momentum coefficients for SGD with momentum, for each weight matrix and bias vector in the network (this yields 24 hyperparameters, as we have 6 layers each with 2 parameter blocks and 2 hyperparameters tuned). We also tuned the number of units per hidden layer, by masking the output of each hidden layer, with a deterministic mask that zeros out part of the representation, effectively using only the first n units. We tune the number of units in each of the 5 hidden layers, yielding 5 discrete hyperparameters, and 29 hyperparameters in total. Because we are effectively tuning the architecture of the MLP, we apply hidden unit masking at evaluation time in addition to training time. As the meta-objective, we used the sum of validation losses over the inner optimization trajectory.\nTo tune the number of hidden units, we used an unconstrained parameterization (in the real numbers) transformed by a sigmoid to the range (0, 1) which represents the fraction of units that are used, out of the maximum number of units per layer (set to be 100 in our experiments). The number of units per layer is determined by m i * sigmoid(\u03b8 i ) where m i is the maximum number of units for hidden layer i and \u03b8 i is the unconstrained parameterization for the fraction of units to be used.\nFor random search, we sampled learning rates uniformly at random in log-space, with range (1e-8, 1e1); we sampled momentum coefficients uniformly at random in logit-space corresponding to the sigmoid-transformed range (0.01, 0.999); and we sampled the number of hidden units per layer from the logit-space corresponding to the sigmoid-transformed range (0.01, 0.999) . For ES and PES, we initialized each learning rate uniformly at random in log space in the range (1e-4, 1e-2); we initialized each momentum coefficient uniformly at random in logit-space, to have the sigmoid-transformed range (0.01, 0.9); and we initialized the number of hidden units per layer in logit-space corresponding to the sigmoidtransformed range (0.2, 0.8). These ranges are slightly smaller than the ones used for random search in order to maintain meta-optimization stability; note from Figure 9 that the performance of both ES and PES is initially poor (prior to metaoptimization), indicating that these ranges for random initialization do not increase their performance compared to random search, and thus the improvement for PES is primarily due to its adaptation of the hyperparameters. For ES and PES, we used perturbation standard deviation \u03c3 = 0.3, N = 10 particles, and Adam with learning rate 0.01 for outer optimization. We ran each method four times with different random seeds, and plotted the mean performance, with the min and max shown by the shaded regions in Figure 9. We measured the best meta-objective value achieved so far during meta-optimization, as a function of total compute, which takes into account the number of inner iterations performed, as well as the number of parallel workers (or particles); total compute corresponds to the product of inner iterations and the number of workers.\nAdditional Hyperparameter Optimization and Learned Optimizer Experiments. In Figure 12a, we tune hyperparameters for a 1.6M parameter ResNet on CIFAR-10 using ES and PES with T = 5000, K = 20, and N = 4, targeting the sum of validation losses. In Figure 12b, we train a learned optimizer on MNIST (similarly to Metz et al. (2019)). We use the same configuration as described in Section 5.2 but target a 2-hidden layer, 128 unit MLP trained on MNIST.    Tuning Regularization for UCI Regression. Here we show that truncation bias can also arise for regularization hyperparameters such as the L 2 regularization coefficient. We tune L 2 regularization for linear regression on the Yacht data from the UCI collection (Asuncion & Newman, 2007). We found the optimal L 2 coefficient using a fine-trained grid search. In Figure 13 we compare meta-optimization using ES and PES, starting from different initial L 2 coefficients; PES robustly converges to the correct solution in all cases. We used \u03c3 = 0.01, K = 1, and N = 4 for both ES and PES.", "publication_ref": ["b36", "b10"], "figure_ref": ["fig_2", "fig_2", "fig_7", "fig_7", "fig_8"], "table_ref": []}, {"heading": "C.4. Continuous Control Details", "text": "We used OpenAI Gym 3 to interface with MuJoCo. In our implementation, each antithetic pair shares a MuJoCo environment state, which is different between different antithetic pairs. The environment state is reset to the same point before running the partial unrolls of each particle in a pair, to control for randomness (e.g., the antithetic perturbations are evaluated starting from a common state). As is standard for MuJoCo environments, the length of a full episode is T = 1000; we ran full-unroll ES with K = 1000, and we used partial unrolls of length K = 100 for truncated ES and PES. We used 10 antithetic pairs for each of ES and PES. Following Mania et al. (2018), we used vanilla SGD to optimize the policy parameters. For each of ES and PES, we performed a grid search over learning rates and perturbation scales, both from the set {1.0, 0.3, 0.1, 0.01}.\nTo evaluate policies, we computed the average full-episode reward over 50 random environment seeds. In Figure 7, we show the mean performance of each algorithm over 6 random seeds, with standard deviation shown by the shaded region.\nFollowing Mania et al. (2018), we used a linear policy initialized as all 0s (the linear policy is a single weight matrix with no bias term). Also following Mania et al. (2018), we divided the rewards by their standard deviation (computed using the aggregated rewards from all antithetic pairs) before computing the ES/PES gradient estimates. We did not use state normalization, nor did we perform any heuristic selection of a subset of the best sampled perturbation directions (as used in the ARS V2 approach of Mania et al. (2018)).", "publication_ref": ["b5", "b5", "b5", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "D. Telescoping Sums", "text": "If we wish to target the final loss L T as the meta-objective, we can define p t = L t \u2212 L t\u22121 , where L \u22121 \u2261 0. This yields the telescoping sum: Targeting the final loss encourages different behavior than targeting the sum or average of the losses. Targeting the sum of losses encourages fast convergence (small t L t ), but not necessarily the smallest final loss L T , while targeting the final loss encourages finding the smallest L T potentially at the expense of slower convergence (larger t L t ).\nT t=0 p t = ( & & L 0 \u2212 L \u22121 ) + ( & & L 1 \u2212 & & L 0 ) + ( & & L 2 \u2212 & & L 1 ) + \u2022 \u2022 \u2022 + ($ $ $ L T \u22121 \u2212 $ $ $ L T \u22122 ) + (L T \u2212 $ $ $ L T \u22121 ) = L T (13\nWe performed an experiment using telescoping sums to target the final training loss, optimizing an exponential LR schedule for an MLP on Fash-ionMNIST with T = 5000, K = 20, N = 100 (Figure 14). Due to the computational expense of evaluating the loss on the full training set to obtain L t at each partial unroll, we selected a random minibatch at the start of each inner problem, which was kept fixed for the loss evaluations for that inner problem.", "publication_ref": [], "figure_ref": ["fig_9"], "table_ref": []}, {"heading": "E. Derivation of Persistent Evolution Strategies", "text": "Here we derive the PES estimator. The derivation here closely follows that in the text body, but shows additional intermediate steps in several places in the derivation. Also see Appendix I for an alternate derivation using stochastic computation graphs (Schulman et al., 2015).", "publication_ref": ["b23"], "figure_ref": [], "table_ref": []}, {"heading": "E.1. Notation", "text": "Notation Shift Unrolled computation graphs (as illustrated in Figure 1) depend on shared parameters \u03b8 at every timestep. In order to account for how these contribute to the overall gradient \u2207 \u03b8 L(\u03b8), we use subscripts \u03b8 t to distinguish between applications of \u03b8 at different steps, where \u03b8 t = \u03b8, \u2200t (see Figure 15). We further define \u0398 = (\u03b8 1 , . . . , \u03b8 T ) , which is a matrix with the per-timestep \u03b8 t as its rows. For notational simplicity in the following derivation, we drop the dependence on s t and explicitly include the dependence on each \u03b8 t , writing L t (s t ; \u03b8) as either L t (\u03b8 1 , . . . , \u03b8 t ) or simply L t (\u0398), with an implicit initial state s 0 . Thus, L(\u03b8) = ", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "E.2. PES is ES Over the Parameters at Each Unroll Step", "text": "We wish to compute the gradient \u2207 \u03b8 L(\u03b8) of the total loss over all unrolls. We begin by writing this gradient in terms of the full gradient \u2202L(\u0398) \u2202 vec(\u0398) \u2208 R P T \u00d71 , where P is the number of parameters, and T is the total number of unrolls, and then using ES to approximate \u2202L(\u0398) \u2202 vec(\u0398) . First, note that we can write:\ndL(\u03b8) d\u03b8 = dL(\u0398) d\u03b8 = \u2202L(\u0398) \u2202\u03b8 1 U 1 d\u03b8 1 d\u03b8 + \u2202L(\u0398) \u2202\u03b8 2 U 1 d\u03b8 2 d\u03b8 + \u2022 \u2022 \u2022 + \u2202L(\u0398) \u2202\u03b8 T 1 d\u03b8 T d\u03b8 = T \u03c4 =1 \u2202L (\u0398) \u2202\u03b8 \u03c4 = I \u2297 1 \u2202L(\u0398) \u2202 vec (\u0398)\nwhere \u2297 denotes the Kronecker product, I has dimension P \u00d7 P , 1 has dimension 1 \u00d7 T , and thus I \u2297 1 has dimension P \u00d7 P T . Note that because \u2202L(\u0398) \u2202 vec(\u0398) has dimension P T \u00d7 1, the product I \u2297 1\n\u2202L(\u0398)\n\u2202 vec(\u0398) will be P \u00d7 1. Next, we will apply ES to approximate the last RHS expression above:\ndL(\u03b8) d\u03b8 \u2248 g PES = I \u2297 1 E 1 \u03c3 2 vec ( ) L (\u0398 + ) = 1 \u03c3 2 E I \u2297 1 vec ( ) L (\u0398 + ) = 1 \u03c3 2 E T \u03c4 =1 \u03c4 L (\u0398 + ) ,\nwhere = ( 1 , . . . , T ) is a matrix of perturbations t to be added to the \u03b8 t at each timestep and the expectation is over entries in drawn from an i.i.d. Gaussian with variance \u03c3 2 . This ES approximation is an unbiased estimator of the gradient of the Gaussian-smoothed objective E [L(\u0398 + )].\nWe next show that g PES decomposes into a sum of sequential gradient estimates,\ng PES = 1 \u03c3 2 E T \u03c4 =1 \u03c4 L (\u0398 + ) = 1 \u03c3 2 E T \u03c4 =1 \u03c4 T t=1 L t (\u0398 + ) = 1 \u03c3 2 E T t=1 T \u03c4 =1 \u03c4 L t (\u0398 + ) (14) = 1 \u03c3 2 E T t=1 t \u03c4 =1 \u03c4 L t (\u0398 + ) (15) = 1 \u03c3 2 E T t=1 \u03be t L t (\u0398 + ) (16) = E T t=1\u011d PES t, ,(17)\ng PES t, = 1 \u03c3 2 \u03be t L t (\u0398 + ) = 1 \u03c3 2 \u03be t L t (\u03b8 1 + 1 , . . . , \u03b8 t + t ) . (18\n)\nwhere \u03be t = t \u03c4 =1 \u03c4 , Equation 15 relies on L t (\u2022) being independent of \u03c4 for \u03c4 > t, and Equation 18 similarly relies on L t (\u2022) only being a function of \u03b8 \u03c4 for \u03c4 \u2264 t.\nThe PES estimator consists of Monte Carlo estimates of Equation 17,\ng PES = 1 N N i=1 T t=1\u011d PES t, (i)(19)\nwhere (i) are samples of , and N is the number of Monte Carlo samples. Gradient estimates at each time step can be evaluated sequentially, and used to perform SGD.\nConcrete Example. To illustrate how the expressions in the derivation above yield the desired gradient estimate, here we provide a concrete example using two-dimensional \u03b8 with three steps of unrolling. The matrix \u0398 is:\n\u0398 = \uf8ee \uf8f0 \u2212 \u2212\u2212 \u2212\u03b8 1 \u2212 \u2212\u2212 \u2212 \u2212 \u2212\u2212 \u2212\u03b8 2 \u2212 \u2212\u2212 \u2212 \u2212 \u2212\u2212 \u2212\u03b8 3 \u2212 \u2212\u2212 \u2212 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb \u2202L(\u0398) \u2202vec(\u0398) = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 \u2202L(\u0398) \u2202\u03b8 (1) 1", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "\u2202L(\u0398) \u2202\u03b8", "text": "(1) 2", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "\u2202L(\u0398) \u2202\u03b8", "text": "(1) 3", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "\u2202L(\u0398) \u2202\u03b8", "text": "(2) 1", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "\u2202L(\u0398) \u2202\u03b8", "text": "(2) 2", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "\u2202L(\u0398) \u2202\u03b8", "text": "(2) 3\n\uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb\nThe Kronecker product is:\nI \u2297 1 = 1 0 0 1 \u2297 1 1 1 = 1 1 1 0 0 0 0 0 0 1 1 1\n. Thus, we have:\n(I \u2297 1 ) \u2202L(\u0398) \u2202vec(\u0398) = \uf8ee \uf8ef \uf8f0 \u2202L(\u0398) \u2202\u03b8 (1) 1 + \u2202L(\u0398) \u2202\u03b8 (1) 2 + \u2202L(\u0398) \u2202\u03b8 (1) 3 \u2202L(\u0398) \u2202\u03b8 (2) 1 + \u2202L(\u0398) \u2202\u03b8 (2) 2 + \u2202L(\u0398) \u2202\u03b8 (2) 3 \uf8f9 \uf8fa \uf8fb = \uf8ee \uf8ef \uf8f0 \u2202L(\u0398) \u2202\u03b8 (1) 1 \u2202L(\u0398) \u2202\u03b8 (2) 1 \uf8f9 \uf8fa \uf8fb \u2202L(\u0398) \u2202\u03b8 1 + \uf8ee \uf8ef \uf8f0 \u2202L(\u0398) \u2202\u03b8 (1) 2 \u2202L(\u0398) \u2202\u03b8 (2) 2 \uf8f9 \uf8fa \uf8fb \u2202L(\u0398) \u2202\u03b8 2 + \uf8ee \uf8ef \uf8f0 \u2202L(\u0398) \u2202\u03b8 (1) 3 \u2202L(\u0398) \u2202\u03b8 (2) 3 \uf8f9 \uf8fa \uf8fb \u2202L(\u0398) \u2202\u03b8 3 = T \u03c4 =1 \u2202L(\u0398) \u2202\u03b8 \u03c4 = dL(\u0398) d\u03b8\nSimilarly, to see how the PES derivation works, consider a matrix of perturbations and its vectorization vec( ) as follows:\n= \uf8ee \uf8f0 \u2212 \u2212\u2212 \u2212 1 \u2212 \u2212\u2212 \u2212 \u2212 \u2212\u2212 \u2212 2 \u2212 \u2212\u2212 \u2212 \u2212 \u2212\u2212 \u2212 3 \u2212 \u2212\u2212 \u2212 \uf8f9 \uf8fb = \uf8ee \uf8ef \uf8f0 (1) 1 (2) 1 (1) 2 (2) 2 (1) 3 (2) 3 \uf8f9 \uf8fa \uf8fb vec( ) = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 (1) 1 (1) 2 (1) 3 (2) 1 (2) 2 (2) 3 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb\nThen,\n(I \u2297 1 )vec( ) = (1) 1 + (1) 2 + (1) 3 (2) 1 + (2) 2 + (2) 3 = (1) 1 (2) 1 1 + (1) 2 (2) 2 2 + (1) 3 (2) 3 3 = T \u03c4 =1 \u03c4\nThis shows how the following statements are equivalent in our derivation:\n1 \u03c3 2 E I \u2297 1 vec ( ) L (\u0398 + ) = 1 \u03c3 2 E T \u03c4 =1 \u03c4 L (\u0398 + ) F. Proof that PES is Unbiased Statement F.1. Let \u03b8 \u2208 R n and L(\u03b8) = T t=1 L t (\u03b8).\nSuppose that \u2207 \u03b8 L(\u03b8) exists, and assume that L is quadratic, so that it is equivalent to its second-order Taylor series expansion:\nL(\u0398 + ) = L(\u0398) + vec( ) \u2207 vec(\u0398) L(\u0398) + 1 2 vec( ) \u2207 2 vec(\u0398) L(\u0398) vec( )\nConsider the PES estimator (using antithetic sampling) below:\ng PES-A = (I \u2297 1 )E 1 2\u03c3 2 vec( )(L(\u0398 + ) \u2212 L(\u0398 \u2212 )) ,\nwhere \u223c N (0, I\u03c3 2 ). Then, bias(\u011d\nPES-A ) = E [\u011d PES-A ] \u2212 \u2207 \u03b8 L(\u03b8) = 0.\nProof. Using the assumption that L is quadratic and due to antithetic sampling, we can simplify this expression L(\u0398 + ) \u2212 L(\u0398 \u2212 ) as follows:\nL(\u0398 + ) \u2212 L(\u0398 \u2212 ) =(\u00a8L(\u0398) + vec( ) \u2207 vec(\u0398) L(\u0398) + @ @ @ @ @ @ @ @ @ @ @ @ @ @\n1 2 vec( ) \u2207 2 vec(\u0398) L(\u0398) vec( ))(20)\n\u2212 (\u00a8L(\u0398) \u2212 vec( ) \u2207 vec(\u0398) L(\u0398) + @ @ @ @ @ @ @ @ @ @ @ @ @ @\n1 2 vec( ) \u2207 2 vec(\u0398) L(\u0398) vec( ))(21)\n=2 vec( ) \u2207 vec(\u0398) L(\u0398)(22)\nThus, we have:\u011d\nPES-A = (I \u2297 1 )E 1 2\u03c3 2 2 vec( ) vec( ) \u2207 vec(\u0398) L(\u0398)(23)\n= (I \u2297 1 ) 1 \u03c3 2 E vec( ) vec( ) \u03c3 2 I \u2207 vec(\u0398) L(\u0398)(24)\n= (I \u2297 1 )\u2207 vec(\u0398) L(\u0398) (25) = \u2207 \u03b8 L(\u03b8)(26)\nThus, E[\u011d PES-A ] = \u2207 \u03b8 L(\u03b8) and bias(\u011d PES-A ) = 0.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G. PES Variance", "text": "In this section, we derive the variance of PES. The antithetic PES estimator assuming quadratic L is as follows:\ng PES-A = 1 \u03c3 2 T t=1 \u03be t vec ( 1...t ) \u2207 vec(\u03981...t) L t (\u0398)(27)\nFor simplicity in the following derivation, we consider a Monte-Carlo estimate using a single particle pair. For N particles, the variance will be scaled by a factor of 1 N . We use the total variance tr(Var(\u011d PES )) to quantify the variance of the estimator:\ntr(Var(\u011d PES-A )) = tr(E[\u011d PES-A\u011dPES-A ] \u2212 E[\u011d PES-A ]E[\u011d PES-A ] ),(28)\n= E[\u011d PES-A \u011d PES-A ] 1 \u2212 E[\u011d PES-A ] E[\u011d PES-A ] 2 (29)\nTerm 2 is easy to compute, because the estimator is unbiased, so E [\u011d PES-A ] = \u2207 \u03b8 L(\u0398). Thus,\n2 = E[\u011d PES-A ] E[\u011d PES-A ] = \u2207 \u03b8 L(\u0398) \u2207 \u03b8 L(\u0398) = ||\u2207 \u03b8 L(\u0398)|| 2(30)\nTo derive term 1 , we will expand out\u011d PES-A \u011d PES-A into a sum of simple sub-expressions and use the linearity of expectation to combine them. To simplify notation, we use the shorthand v t \u2261 vec ( 1...t ) and g t \u2261 \u2207 vec(\u03981...t) L t (\u0398). First, note that:\ng PES-A \u011d PES-A = 1 \u03c3 4 \uf8eb \uf8ec \uf8ed T t=1 \u03be t vec ( 1...t ) vt \u2207 vec(\u03981...t) L t (\u0398) g t \uf8f6 \uf8f7 \uf8f8 \uf8eb \uf8ec \uf8ed T t=1 \u03be t vec ( 1...t ) vt \u2207 vec(\u03981...t) L t (\u0398) g t \uf8f6 \uf8f7 \uf8f8 (31) = 1 \u03c3 4 \u03be 1 v 1 g 1 + \u2022 \u2022 \u2022 + \u03be T v T g T \u03be 1 v 1 g 1 + \u2022 \u2022 \u2022 + \u03be T v T g T (32) = 1 \u03c3 4 \uf8eb \uf8ec \uf8edg 1 v 1 \u03be 1 \u03be 1 v 1 g 1 a + g 1 v 1 \u03be 1 \u03be 2 v 2 g 2 b + \u2022 \u2022 \u2022 + g T v T \u03be T \u03be T v T g T \uf8f6 \uf8f7 \uf8f8 (33)\nThere are two types of terms in Eq. 33: terms of type a , which have the form g i v i \u03be i \u03be i v i g i , and terms of type b , which have the form g i v i \u03be i \u03be j v j g j where i = j. We will derive the expectations of each of these two types of terms separately, and then combine the resulting sub-expressions.\nTerms of Type a . As the first step in expanding out each term of type a , note that:\nv t g t = vec ( 1...t ) \u2207 vec(\u03981...t) L t (\u0398) = t \u03c4 =1 \u03c4 \u2207 \u03b8\u03c4 L t (\u0398)(34)\nAlso, note that \u03be i \u03be i can be expanded as follows:\n\u03be i \u03be i = ( 1 + \u2022 \u2022 \u2022 + i ) ( 1 + \u2022 \u2022 \u2022 + i ) = i m=1 m m + m\u2264i,n\u2264i,m =n m n(35)\nThus, we have:\ng i v i \u03be i \u03be i v i g i = i m=1 m \u2207 \u03b8m L i (\u0398) i n=1 n n i m=1 m \u2207 \u03b8m L i (\u0398) I (36\n)\n+ i m=1 m \u2207 \u03b8m L i (\u0398) \uf8eb \uf8ed m\u2264i,n\u2264i,m =n m n \uf8f6 \uf8f8 i m=1 m \u2207 \u03b8m L i (\u0398) II (37\n)\nWe see that there are two types of terms in I with non-zero expectation:\nE \u2207 \u03b8m L i (\u0398) m m m m \u2207 \u03b8m L i (\u0398) = \u2207 \u03b8m L i (\u0398) E m m m m (P +2)\u03c3 4 I \u2207 \u03b8m L i (\u0398)(38)\n= (P + 2)\u03c3 4 ||\u2207 \u03b8m L i (\u0398)|| 2(39)\nTo compute E m m m m in Eq. 38, we used the following identity, which was derived in Appendix A.2 of (Maheswaranathan et al., 2018). The m are assumed to be drawn from N (0, \u03a3), where in our case \u03a3 = \u03c3 2 I: There are i terms of this type, that make the following contribution to I :\n(P + 2)\u03c3 4 i m=1 ||\u2207 \u03b8m L i (\u0398)|| 2(44)\nThe second type of term in I with non-zero expectation has the form\n\u2207 \u03b8m L i (\u0398) m n n m \u2207 \u03b8m L i (\u0398) where m = n.\nComputing the expectation, we have:\nE \u2207 \u03b8m L i (\u0398) m n n m \u2207 \u03b8m L i (\u0398) = \u2207 \u03b8m L i (\u0398) E m n n m P \u03c3 4 I \u2207 \u03b8m L i (\u0398)(45)\nWhere E m n n m in Eq. 45 is computed as follows:\nE m, n m n n m = E m E n m n n m = E m [ m E n n n P \u03c3 2 m ] = P \u03c3 2 E n [ n n ] \u03c3 2 I = P \u03c3 4 I(46)\nIn Eq. 46, we obtained E m m m = P \u03c3 2 via:\nE m m m = E m tr( m m ) = E m tr( m m ) = tr E m m m = tr(\u03c3 2 I) = P \u03c3 2 (47)\nThe total contribution of terms of this type is:\ni m=1 n,m\u2208{1,...,i},n =m \u2207 \u03b8m L i (\u0398) E m n n m \u2207 \u03b8m L i (\u0398) = i m=1 n,m\u2208{1,...,i},n =m P \u03c3 4 ||\u2207 \u03b8m L i (\u0398)|| 2 (48) = (i \u2212 1)P \u03c3 4 i m=1 ||\u2207 \u03b8m L i (\u0398)|| 2(49)\nSo far, we have:\nI = (P + 2)\u03c3 4 i m=1 ||\u2207 \u03b8m L i (\u0398)|| 2 + (i \u2212 1)P \u03c3 4 i m=1 ||\u2207 \u03b8m L i (\u0398)|| 2 (50) = (iP + 2)\u03c3 4 i m=1 ||\u2207 \u03b8m L i (\u0398)|| 2(51)\nNext, we need to compute:\nII = i m=1 m \u2207 \u03b8m L i (\u0398) \uf8eb \uf8ed m\u2264i,n\u2264i,m =n m n \uf8f6 \uf8f8 i m=1 m \u2207 \u03b8m L i (\u0398)\nHere, the terms with nonzero expectation have the form\n\u2207 \u03b8m L i (\u0398) m m n n \u2207 \u03b8n L i (\u0398) or \u2207 \u03b8n L i (\u0398) n m n m \u2207 \u03b8m L i (\u0398), both of which have expectation: E \u2207 \u03b8m L i (\u0398) m m n n \u2207 \u03b8n L i (\u0398) = \u2207 \u03b8m L i (\u0398) E m m n n \u03c3 4 I \u2207 \u03b8n L i (\u0398) (52) = \u03c3 4 \u2207 \u03b8n L i (\u0398) \u2207 \u03b8m L i (\u0398)(53)\nWe have the following contribution from terms of this type, where the factor of 2 accounts for the two conditions ( n m n m and m m n n ):\n2\u03c3 4 m =n \u2207 \u03b8m L i (\u0398) \u2207 \u03b8n L i (\u0398)(54)\nThen, we have:\nE g i v i \u03be i \u03be i v i g i = (iP + 2)\u03c3 4 i m=1 ||\u2207 \u03b8m L i (\u0398)|| 2 + 2\u03c3 4 m\u2264i,n\u2264i,m =n \u2207 \u03b8m L i (\u0398) \u2207 \u03b8n L i (\u0398)(55)\nTerms of Type b . Next, we consider terms of type b , which have the form g i v i \u03be i \u03be j v j g j where i = j. Note that we can expand \u03be i \u03be j as follows:\n\u03be i \u03be j = ( 1 + \u2022 \u2022 \u2022 + i ) ( 1 + \u2022 \u2022 \u2022 + j ) = r m=1 m m + m\u2208{1,...,i},n\u2208{1,...,j},m =n m n(56)\nwhere we define r = min(i, j). Plugging in this expansion for \u03be i \u03be j , we have:\ng i v i \u03be i \u03be j v j g j = i m=1 m \u2207 m L i (\u0398) r m=1 m m j n=1 n \u2207 n L j (\u0398) I (57) + i m=1 m \u2207 \u03b8m L i (\u0398) \uf8eb \uf8ed m\u2264i,n\u2264j,m =n m n \uf8f6 \uf8f8 j n=1 n \u2207 n L j (\u0398) II (58\n)\nExpanding I , there are two types of terms of interest: ones of the form \u2207 \u03b8m L i (\u0398) m m m m \u2207 \u03b8m L j (\u0398), and ones of the form\n\u2207 \u03b8n L i (\u0398) n m m n \u2207 \u03b8n L j (\u0398).\nThe expectation of the first type of term is:\nE \u2207 \u03b8m L i (\u0398) m m m m \u2207 \u03b8m L j (\u0398) = \u2207 \u03b8m L i (\u0398) E m m m m (P +2)\u03c3 4 I \u2207 \u03b8m L j (\u0398)(59)\n= (P + 2)\u03c3 4 \u2207 \u03b8m L i (\u0398) \u2207 \u03b8m L j (\u0398)(60)\nThe total contribution from terms like this is:\n(P + 2)\u03c3 4 r m=1 \u2207 \u03b8m L i (\u0398) \u2207 \u03b8m L j (\u0398)(61)\nThe expectation of the second type of term is:\n\u2207 \u03b8n L i (\u0398) E n m m n \u2207 \u03b8n L j (\u0398) = \u03c3 4 \u2207 \u03b8n L i (\u0398) \u2207 \u03b8n L j (\u0398)(62)\nThe total contribution from terms of this type is:\n\u03c3 4 r m=1 m\u2264r,n\u2264r,n =m \u2207 \u03b8n L i (\u0398) \u2207 \u03b8n L j (\u0398)(63)\nNext, we look at terms in expression II . We have two types of terms that have nonzero expectation:\nE \u2207 \u03b8m L i (\u0398) m m n n \u2207 \u03b8n L j (\u0398) = \u03c3 4 \u2207 \u03b8m L i (\u0398) \u2207 \u03b8n L j (\u0398)(64)\nand:\nE \u2207 \u03b8n L i (\u0398) n m n m \u2207 \u03b8m L j (\u0398) = \u03c3 4 \u2207 \u03b8n L i (\u0398) \u2207 \u03b8m L j (\u0398)(65)\nThe contribution from these terms is:\n2\u03c3 4 m\u2264i,n\u2264j,m =n \u2207 \u03b8m L i (\u0398) \u2207 \u03b8n L j (\u0398)(66)\nThus, we have:\nE g i v i \u03be i \u03be j v j g j = (P + 2)\u03c3 4 r m=1 \u2207 \u03b8m L i (\u0398) \u2207 \u03b8m L j (\u0398) + \u03c3 4 r m=1 m,n\u2264r,n =m \u2207 \u03b8n L i (\u0398) \u2207 \u03b8n L j (\u0398) (67) + 2\u03c3 4 m\u2264i,n\u2264j,m =n \u2207 \u03b8m L i (\u0398) \u2207 \u03b8n L j (\u0398)(68)\nCombining Terms for E \u011d PES-A \u011d PES-A . Putting these components together, we have the following overall expression:\nE \u011d PES-A \u011d PES-A = T i=1 \uf8eb \uf8ed (iP + 2) i m=1 ||\u2207 \u03b8m L i (\u0398)|| 2 + 2 m =n \u2207 \u03b8m L i (\u0398) \u2207 \u03b8n L i (\u0398) \uf8f6 \uf8f8 (69\n)\n+ i =j (P + 2) r m=1 \u2207 \u03b8m L i (\u0398) \u2207 \u03b8m L j (\u0398) + r m=1 n =m \u2207 \u03b8n L i (\u0398) \u2207 \u03b8n L j (\u0398)(70)\n+ 2 m =n \u2207 \u03b8m L i (\u0398) \u2207 \u03b8n L j (\u0398)(71)\nTo obtain tr(Var(\u011d PES-A )), we subtract the following from the expression above:\nE \u011d PES-A E \u011d PES-A = \u2207 \u03b8 L(\u0398) \u2207 \u03b8 L(\u0398)(72)\n= T t=1 t \u03c4 =1 \u2207 \u03b8\u03c4 L t (\u0398) T t=1 t \u03c4 =1 \u2207 \u03b8\u03c4 L t (\u0398)(73)\nG.1. Considering the Dependence on T\nThe variance depends on the gradients of each loss term L t with respect to each of the per-timestep parameters \u03b8 \u03c4 . To gain insight into the structure of these gradients, we can arrange them in a matrix:\nM = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 \u2207 \u03b81 L 1 \u2207 \u03b81 L 2 \u2207 \u03b81 L 3 \u2022 \u2022 \u2022 \u2207 \u03b81 L T \u2207 \u03b82 L 1 \u2207 \u03b82 L 2 \u2207 \u03b82 L 3 \u2022 \u2022 \u2022 \u2207 \u03b82 L T \u2207 \u03b83 L 1 \u2207 \u03b83 L 2 \u2207 \u03b83 L 3 \u2022 \u2022 \u2022 \u2207 \u03b83 L T . . . . . . . . . . . . . . . \u2207 \u03b8 T L 1 \u2207 \u03b8 T L 2 \u2207 \u03b8 T L 3 \u2022 \u2022 \u2022 \u2207 \u03b8 T L T \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 \u2207 \u03b81 L 1 \u2207 \u03b81 L 2 \u2207 \u03b81 L 3 \u2022 \u2022 \u2022 \u2207 \u03b81 L T 0 \u2207 \u03b82 L 2 \u2207 \u03b82 L 3 \u2022 \u2022 \u2022 \u2207 \u03b82 L T 0 0 \u2207 \u03b83 L 3 \u2022 \u2022 \u2022 \u2207 \u03b83 L T . . . . . . . . . . . . . . . 0 0 0 \u2022 \u2022 \u2022 \u2207 \u03b8 T L T \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb (74\n)\nThe RHS is upper-triangular due to the fact that \u2207 \u03b8\u03c4 L t = 0 for all \u03c4 > t. The variance of the PES estimator depends on the covariance between the gradients \u2207 \u03b8\u03c4 L t in this matrix.\nWe consider two structures for the matrix: 1) a diagonal structure, where the gradients \u2207 \u03b8i L j = 0, \u2200i = j; and 2) an upper-triangular structure as shown in the RHS of Eq. 74. For each of these two matrix structures, we will consider two scenarios for the covariance between gradients: a) all gradients \u2207 \u03b8i L j are identical; b) all gradients are i.i.d.", "publication_ref": ["b4"], "figure_ref": [], "table_ref": []}, {"heading": "G.1.1. DIAGONAL STRUCTURE", "text": "We denote the gradient of\nL by g = \u2207 \u03b8 L(\u0398) = T t=1 \u2207 \u03b8 L t (\u0398) = T t=1 g t . (Note that \u2207 \u03b8 L t (\u0398) = $ $ $ \u2207 \u03b81 L t + $ $ $ \u2207 \u03b82 L t + \u2022 \u2022 \u2022 + \u2207 \u03b8t L t = \u2207 \u03b8t L t due to the diagonal structure.)\nIn the diagonal case, we have:\nE \u011d PES-A \u011d PES-A = T i=1 \uf8eb \uf8ed (iP + 2) i m=1\n||\u2207 \u03b8m L i (\u0398)|| 2 + @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @\n2 m\u2264i,n\u2264j,m =n \u2207 \u03b8m L i (\u0398) \u2207 \u03b8n L i (\u0398) \uf8f6 \uf8f8 (75\n)\n+ i =j (P + 2) $ $ $ $ $ $ $ $ $ $ $ $ $ r m=1 \u2207 \u03b8m L i (\u0398) \u2207 \u03b8m L j (\u0398) + $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ r m=1 n\u2264r,n =m \u2207 \u03b8n L i (\u0398) \u2207 \u03b8n L j (\u0398) (76) + 2 m\u2264i,n\u2264j,m =n \u2207 \u03b8m L i (\u0398) \u2207 \u03b8n L j (\u0398)(77)\nThus, we have:\nE \u011d PES-A \u011d PES-A = T i=1 (iP + 2) i m=1 ||\u2207 \u03b8m L i (\u0398)|| 2 + 2 i =j m\u2264i,n\u2264j,m =n \u2207 \u03b8m L i (\u0398) \u2207 \u03b8n L j (\u0398)(78)\n= T i=1 (iP + 2) ||\u2207 \u03b8i L i (\u0398)|| 2 + 2 i =j \u2207 \u03b8i L i (\u0398) \u2207 \u03b8j L j (\u0398)(79)\nTo go from Eq. 78 to Eq. 79, we use the fact that \u2207 \u03b8m L i (\u0398) = 0 for m = i and \u2207 \u03b8n L j (\u0398) = 0 for n = j. Next, note that when M is diagonal, we have:\nE[\u011d PES-A ] E[\u011d PES-A ] = T t=1 t \u03c4 =1 \u2207 \u03b8\u03c4 L t (\u0398) T t=1 t \u03c4 =1 \u2207 \u03b8\u03c4 L t (\u0398)(80)\n= T t=1 \u2207 \u03b8t L t (\u0398) T t=1 \u2207 \u03b8t L t (\u0398)(81)\n= T t=1 ||\u2207 \u03b8t L t (\u0398)|| 2 + i =j \u2207 \u03b8i L i (\u0398) \u2207 \u03b8j L j (\u0398)(82)\nThus, the total variance is:\ntr(Var(\u011d PES-A )) = E \u011d PES-A \u011d PES-A \u2212 E[\u011d PES-A ] E[\u011d PES-A ](83)\n= T i=1 (iP + 2) ||\u2207 \u03b8i L i (\u0398)|| 2 + 2 i =j \u2207 \u03b8i L i (\u0398) \u2207 \u03b8j L j (\u0398)(84)\n\u2212 T i=1 ||\u2207 \u03b8t L t (\u0398)|| 2 \u2212 i =j \u2207 \u03b8i L i (\u0398) \u2207 \u03b8j L j (\u0398)(85)\n= T i=1 (iP + 1) ||\u2207 \u03b8i L i (\u0398)|| 2 + i =j \u2207 \u03b8i L i (\u0398) \u2207 \u03b8j L j (\u0398)(86)\nScenario 1: All the \u2207 \u03b8i L i are equal. Recall our notation for the total gradient, g = \u2207 \u03b8 L(\u0398) = T t=1 \u2207 \u03b8 L t (\u0398) = T t=1 g t . If we assume that the gradients for each unroll are identical to each other, then:\ng t = 1 T g (87\n)\n||g t || 2 = 1 T g 2 = 1 T 2 ||g|| 2 (88) So, T t=1 ||g t || 2 (tP + 1) + i =j g i g j = T t=1 1 T 2 ||g|| 2 (tP + 1) + i\u2264T,j\u2264T,i =j 1 T 2 ||g|| 2 (89) = 1 T 2 ||g|| 2 T + P T (T + 1) 2 + 1 T 2 ||g|| 2 (T 2 \u2212 T ) (90) = 1 T 2 ||g|| 2 T 2 + P T 2 + P T 2 (91) = ||g|| 2 P 2T + P 2 + 1 (92)\nScenario 2: All the \u2207 \u03b8i L j are i.i.d. If we assume that the gradients for each unroll are i.i.d., then:\nE ||g|| 2 = T E ||g t || 2 (93) E ||g t || 2 = 1 T E ||g|| 2 (94) E ||g t || 2 = 1 T E ||g|| 2 (95) Thus, T t=1 ||g t || 2 (tP + 1) + i =j g i g j = T t=1 1 T ||g|| 2 (tP + 1) + i =j 1 T ||g|| 2 (96) = 1 T ||g|| 2 T + P T (T + 1) 2 + 1 T ||g|| 2 T 2 \u2212 T (97) = 1 T ||g|| 2 T + T 2 \u2212 T + P T (T + 1) 2 (98) = 1 T ||g|| 2 T 2 + P T (T + 1) 2 (99) = ||g|| 2 T + P (T + 1) 2 (100) = ||g|| 2 P T 2 + P 2 + T (101) G.1.2", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": ". UPPER-TRIANGULAR STRUCTURE", "text": "Scenario 1: All the \u2207 \u03b8i L j are equal. Suppose all the terms in the matrix are equal, e.g., \u2207 \u03b8i L j = h, \u2200i, j. The total gradient g = \u2207 \u03b8 L(\u0398) is equal to the sum of the gradients in the upper-triangular matrix. Thus, g = T (T +1) 2 h, so we can write:\nh = 2 T (T + 1) g (102\n)\nWe have:\nE \u011d PES-A \u011d PES-A = T i=1 \uf8eb \uf8ed (iP + 2) i m=1 ||h|| 2 + 2 m\u2264i,n\u2264i,m =n ||h|| 2 \uf8f6 \uf8f8 I (103\n) + i =j \uf8eb \uf8ed (P + 2) r m=1 ||h|| 2 + r m=1 m\u2264r,n\u2264r,n =m ||h|| 2 + 2 m\u2264i,n\u2264j,m =n ||h|| 2 \uf8f6 \uf8f8 II (104\n)\nTerm I is as follows:\nI = T i=1 (iP + 2)i ||h|| 2 + 2(i 2 \u2212 i) ||h|| 2 (105) = ||h|| 2 T i=1 (P + 2)i 2 (106) = ||h|| 2 (P + 2) T (T + 1)(2T + 1) 6 (107\n)\nTerm II is as follows:\nII = i =j (P + 2) r m=1 ||h|| 2 (P +2)r||h|| 2 + r m=1 n =m ||h|| 2 r(r\u22121)||h|| 2 + 2 m =n ||h|| 2 2(ij\u2212r)||h|| 2 (108) = ||h|| 2 i\u2264T,j\u2264T,i =j (P r + r 2 \u2212 r + 2ij) (109) = 2 ||h|| 2 T i=1 i\u22121 j=1 (P \u2212 1)j + j 2 + 2ij (110) = 2 ||h|| 2 \uf8eb \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ed T i=1 i\u22121 j=1 (P \u2212 1)j a + T i=1 i\u22121 j=1 j 2 b + T i=1 i\u22121 j=1 2ij c \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f8 (111)\nNext we derive each of the terms that arise in Eq. 111. a =\nT i=1 i\u22121 j=1\n(P \u2212 1)j = (P \u2212 1)\nT i=1 i\u22121 j=1 j (112) = (P \u2212 1) T i=1 i(i \u2212 1) 2 (113) = (P \u2212 1) 2 T (T + 1)(2T + 1) 6 \u2212 T (T + 1) 2 (114) b = T i=1 i\u22121 j=1 j 2 = T i=1 i(i \u2212 1)(2i \u2212 1) 6 (115) = T i=1 1 6 2i 3 \u2212 3i 2 + i (116) = 1 3 T i=1 i 3 \u2212 1 2 T i=1 i 2 + 1 6 T i=1 i (117) = 1 3 T 2 (T + 1) 2 4 \u2212 1 2 T (T + 1)(2T + 1) 6 + 1 6 T (T + 1) 2 (118) = 1 12 T 2 (T + 1) 2 \u2212 T (T + 1)(2T + 1) + T (T + 1) (119) c = T i=1 i\u22121 j=1 2ij = T i=1 i i\u22121 j=1 j (120) = T i=1 i i(i \u2212 1) 2 (121) = T i=1 1 2 i(i 2 \u2212 i) (122) = 1 2 T i=1 i 3 \u2212 i 2 (123) = 1 2 T 2 (T + 1) 2 4 \u2212 T (T + 1)(2T + 1) 6 (124\n)\nCombining all these terms, we obtain the following expression for the total variance:\n||h|| 2 P T (T + 1)(2T + 1) 3 \u2212 P T (T + 1) 2 + 5T 2 (T + 1) 2 12 \u2212 T (T + 1)(2T + 1) 6 + 2T (T + 1) 3 (125)\nCombining terms, we obtain:\n||h|| 2 5 12 T 4 + 2 3 P T 3 + 1 2 P T 2 \u2212 1 6 P T + 1 2 T 3 + 7 12 T 2 + 1 2 T (126\n)\nWe are interested in the scaling behavior as a function of the total gradient norm ||g|| 2 , where\n||h|| 2 = 2 T (T + 1) 2 ||g|| 2 (127)\nBecause the denominator in Eq. 127 is O(T 4 ), we will have terms in the total variance that scale as:\n||g|| 2 O (1) + O P T + O P T 2 \u2212 O P T 3 + O 1 T + O 1 T 2 + O 1 T 3 (128)\nScenario 2: All the \u2207 \u03b8i L j are i.i.d. In this case, by direct analogy to Equation 93, we have:\nE ||h|| 2 = 2 T (T + 1) E ||g|| 2 (129)\nHere, the denominator is of order O(T 2 ), while the numerator is of order O(T 4 ), yielding variance that scales as O(T 2 ):\n||g|| 2 O T 2 + O (P T ) + O (P ) \u2212 O P T + O (T ) + O (1) + O 1 T (130)\nFigure 16 shows the empirical variance for several potential scenarios. We performed an analysis similar to that in Section 4, measuring the variance of the PES gradient with respect to the number of unrolls for a small LSTM on the Penn TreeBank (PTB) dataset. We constructed synthetic data sequences to illustrate different scenarios: in Figure 16a we used a 10 3 length sequence consisting of characters sampled uniformly at random from the PTB vocabulary, simulating the first scenario; in Figure 16b we used a 10 3 length sequence consisting of a single repeated character, simulating the second scenario; Figure 16c shows the variance for real data-the first 10 3 characters of PTB-which exhibits characteristics of both synthetic scenarios. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "H. Reducing Variance by Incorporating the Analytic Gradient", "text": "For functions L that are differentiable, we can use the analytic gradient from the most recent partial unroll (e.g., backpropagating through the last K-step unroll) to reduce the variance of the PES gradient estimates. Below, we show how we can incorporate the analytic gradient in the ES estimate for \u2202Lt(\u0398) \u2202\u03b8 :\n\u2202L t (\u0398) \u2202\u03b8 \u2248 1 \u03c3 2 E \uf8ee \uf8f0 \uf8eb \uf8ed \u03c4 \u2264t \u03c4 \uf8f6 \uf8f8 L t (\u0398 + ) \uf8f9 \uf8fb (131) = 1 \u03c3 2 E \u03c4 <t \u03c4 L t (\u0398 + ) + 1 \u03c3 2 E [ t L t (\u0398 + )] (132) = 1 \u03c3 2 E \u03c4 <t \u03c4 L t (\u0398 + ) + \u2202L t (\u0398) \u2202\u03b8 t \u2261p t (133) = 1 \u03c3 2 E \u03c4 <t \u03c4 L t (\u0398 + ) + p t \u2212 1 \u03c3 2 E \u03c4 <t \u03c4 t p t =0 (134) = 1 \u03c3 2 E \u03c4 <t \u03c4 (L t (\u0398 + ) \u2212 t p t ) + p t (135)\nWe call the resulting estimator PES+Analytic. Algorithm 4 describes the implementation of this estimator, which requires a few simple changes from the standard PES estimator. We repeated the empirical variance measurement described in Section 4 and Appendix G using the PES+Analytic estimator, for each of the three scenarios from Appendix G, shown in Figure 18. Similarly to the other variance measurements, we report variance normalized by the squared norm of the true gradient. We found that variance increases with the number of unrolls, but the PES+Analytic variance is 1-2 orders of magnitude smaller than the standard PES variance.  ", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "I. Connection to Gradient Estimation in Stochastic Computation Graphs", "text": "In this section, we show how PES can be derived using the framework for gradient estimation in stochastic computation graphs introduced in (Schulman et al., 2015). We follow their notation for this exposition: in Figure 19, squares represent deterministic nodes, which are functions of their parents; circles represent stochastic nodes which are distributed conditionally on their parents, and nodes not in squares or circles represent inputs. For notational simplicity, in the following exposition we consider 1-dimensional \u03b8. We represent the unrolled computation graph in terms of an input node \u03b8, that gives rise to a stochastic variable \u03b8 t at each time step; the sampled \u03b8 t is used to compute the state s t , which is a deterministic function of the previous state s t\u22121 and the current parameters \u03b8 t . The losses L t are designated as cost nodes, and our objective is\nL = t L t .\nFigure 19. Unrolled stochastic computation graph for PES, in the notation of (Schulman et al., 2015).\nTheorem 1 from (Schulman et al., 2015) gives the following general form for the gradient of the sum of cost nodes in such a stochastic computation graph. Here, C is the set of cost nodes; S is the set of stochastic nodes; DEPS w denotes the set of nodes that w depends on; a \u227a D b indicates that node a depends deterministically on node b (note that this relationship holds as long as there are no stochastic nodes along a path from a to b; in our case, \u03b8 \u227a D \u03b8 t holds for all t); andQ w is the sum of cost nodes downstream from node w.\n\u2202 \u2202\u03b8\nE c\u2208C c = E \uf8ee \uf8f0 w\u2208S,\u03b8\u227a D w \u2202 \u2202\u03b8 log p(w|DEPS w ) Q w + c\u2208C,\u03b8\u227a D c \u2202 \u2202\u03b8 c(DEPS c ) \uf8f9 \uf8fb (136)\nFor the computation graph in Figure 19, \u03b8 does not deterministically influence any of the cost nodes L t , so the second term in the expectation in Eq. 136 will be 0. In addition, each stochastic node \u03b8 t , depends only on \u03b8, e.g. DEPS \u03b8t = {\u03b8}, \u2200t. Thus, our gradient estimate is:\n\u2202 \u2202\u03b8 E T t=1 L t = E T t=1 \u2202 \u2202\u03b8 log p(\u03b8 t |\u03b8) Q \u03b8t (137\n)\nQ \u03b8t is the sum of cost nodes downstream of \u03b8 t , thusQ \u03b8t = T i=t L i . Now, each \u03b8 t \u223c N (\u03b8, \u03c3 2 ), so we have:\nlog p(\u03b8 t | \u03b8) = log N (\u03b8 t | \u03b8, \u03c3 2 ) = log 1 \u221a 2\u03c0\u03c3 \u2212 1 2\u03c3 2 (\u03b8 t \u2212 \u03b8) 2 (138) Then, \u2202 \u2202\u03b8 log p(\u03b8 t | \u03b8) = \u2212 1 2\u03c3 2 \u2022 2(\u03b8 t \u2212 \u03b8) \u2022 (\u22121) (139) = 1 \u03c3 2 (\u03b8 t \u2212 \u03b8) (140) = 1 \u03c3 2 (\u03b8 + t \u2212 \u03b8) (141) = 1 \u03c3 2 t (142\n)\nwhere we used the reparameterization \u03b8 t = \u03b8 + t with t \u223c N (0, \u03c3 2 ). Plugging this into Eq. 137, we have:\n\u2202 \u2202\u03b8 E T t=1 L t = E T t=1 1 \u03c3 2 tQ\u03b8t (143) = 1 \u03c3 2 E [ 1 (L 1 + L 2 + \u2022 \u2022 \u2022 + L T ) + 2 (L 2 + L 3 + \u2022 \u2022 \u2022 + L T ) + \u2022 \u2022 \u2022 + T L T ] (144) = 1 \u03c3 2 E[ 1 L 1 + ( 1 + 2 )L 2 + ( 1 + 2 + 3 )L 3 + \u2022 \u2022 \u2022 + ( 1 + \u2022 \u2022 \u2022 + T )L T ] (145) = 1 \u03c3 2 E T t=1 t \u03c4 =1 \u03c4 L t (146)\nEq. 146 recovers the PES estimator.\nJ. Derivations and Compute/Memory Costs BPTT, TBPTT, ARTBP. Backpropagating through a full unroll of T steps requires T forward and backward passes, yielding compute T (F + B); all T states must be stored in memory to be available for gradient computation during backprop, yielding memory cost T S. Similarly, because TBPTT unrolls the computation graph for K steps, it requires K forward and backward passes, yielding computation K(F + B), and requires storing K states in memory, yielding memory cost KS. ARTBP is identical to TBPTT except that it randomly samples the truncation length in a theoretically-justified way to reduce or eliminate truncation bias. In theory, the sampled truncation lengths must allow for maximum length T , yielding worst-case compute T (F + B) and memory cost T S. However, in practice this is often intractable, so truncation lengths may be sampled within a restricted range centered around K-this is no longer unbiased, but yields average case compute K(F + B) and memory cost KS (which is reported in Table 1).\nRTRL. We begin by deriving RTRL, which simply corresponds to forward-mode differentiation. Let the state be s t \u2208 R S and the parameters be \u03b8 \u2208 R P . We have a dynamical system defined by: Here, G t is S \u00d7 P , H t is S \u00d7 S, and F t is S \u00d7 P . RTRL maintains the Jacobian G t , which requires memory SP ; furthermore, instantiating the matrices H t and F t requires memory S 2 and SP , respectively, so the total memory cost of RTRL is 2SP + S 2 . The matrix multiplication H t G t\u22121 has computational complexity S 2 P . The cost of computing the Jacobian F t is approximately min{S(F + B), P (F + B)}, depending on which of S or P is smaller-dimensional (and correspondingly whether we use forward-mode or reverse-mode automatic differentiation to compute the rows/columns of the Jacobian). Similarly, the cost of computing the Jacobian H t is approximately S(F + B) (using either forward or reverse mode autodiff). Thus, the total computational cost of RTRL is: S 2 P + S(F + B) + min{S(F + B) + P (F + B)}.\ns t = f (s t\u22121 , x t , \u03b8)(147\nNote that, in general, it matters which of s t or \u03b8 is higher dimensional. In the case of unrolled optimization, S is usually larger than P , causing RTRL to be particularly memory-intensive due to the S \u00d7 S Jacobian H t . The computation and memory costs we have derived here are expressed in a general form for state and parameter dimensions S and P , respectively.\nIn the case of RNN training, most prior work (such as (Tallec & Ollivier, 2017a;Mujika et al., 2018;Benzing et al., 2019)) assumes that the RNN parameters are of dimensionality S 2 , where S is the size of the hidden state. 4\nUORO. Unbiased Online Recurrent Optimization (UORO) (Tallec & Ollivier, 2017a) \nHere, \u2202Lt \u2202st is 1 \u00d7 S, H t is S \u00d7 S,s t is S \u00d7 1,\u03b8 t is 1 \u00d7 P , and F t is S \u00d7 P . This leads to a total computation cost of F + B + S 2 + P . We require one pass of backprop to compute the partial derivative, a vector-matrix product size S by S \u00d7 S (S 2 ), then element-wise operations on the full parameter space (P ). The memory cost of storing boths t and\u03b8 t is S + P .\nReparameterization. The reparameterization gradient estimator is\u011d reparam = 1 N N i=1 \u2207 \u03b8 L(\u03b8 + \u03c3 (i) ), where (i) \u223c N (0, I). With respect to computational complexity, this is equivalent to BPTT: its compute cost is T (F + B) and its memory cost is T S.\nES. ES applied to an unroll of length K requires performing K forward passes-it does not require any backward passes, since ES is not gradient-based (e.g., it is a zeroth-order optimization algoritm). Because ES does not require backprop, it does not need to store the intermediate states in memory, only the most recent state, yielding memory cost S that is independent of the unroll length. Using ES with N particles yields total compute and memory costs N KF and N S, respectively.", "publication_ref": ["b23", "b23", "b23", "b28", "b14", "b28"], "figure_ref": ["fig_2", "fig_2", "fig_2"], "table_ref": []}, {"heading": "PES.", "text": "As PES is an evolutionary strategies-based method, it also does not require backward passes; applied to unrolls of length K, PES has compute cost KF . In addition to storing the current state of size S as in ES, PES also maintains a perturbation accumulator for each particle; thus, the memory cost of a single PES chain is S + P . Using PES with N particles yields total compute and memory costs N KF and N (S + P ), respectively.\nPES+Analytic. Similarly to standard PES, we need to maintain a collection of N states, each of size S, and N perturbation accumulators, each of size P , yielding memory cost N (S + P ); unrolling each state for K steps requires computational cost N KF . To incorporate the analytic gradient, we need to maintain one additional particle that is unrolled using the mean \u03b8 rather than a perturbed version \u03b8 + ; this adds memory cost S. The main computational and memory overhead comes from the gradient computation through the partial unroll of length K: similarly to TBPTT, this requires storing K intermediate states, yielding memory cost KS, and requires K forward and K backward operations, yielding computational cost K(F + B). Combined with the memory and computational cost of standard PES, we have total compute cost N KF + K(F + B) and total memory cost N (S + P ) + (K + 1)S.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "K. Diagrammatic Representation of Algorithms", "text": "Figure 20 provides diagrammatic representations of ES and PES. For each partial unroll, vanilla ES starts from a shared initial state s (0) that is evolved in parallel using perturbed parameters \u03b8 + (i) . After each truncated unroll, the mean parameters \u03b8 are used to update the state, which then becomes the initial state for the next truncated unroll; no information is passed between truncated unrolls for vanilla ES. In contrast, PES maintains a set of states s (i) that are evolved in parallel, each according to a different perturbation of the parameters \u03b8 in each truncated unroll. Intuitively, these states maintain their history between truncated unrolls, since we accumulate the perturbations experienced by each state over the course of meta-optimization; when we reach the end of an inner problem, the states are reset to the same initialization, and the perturbation accumulators are reset to 0. ", "publication_ref": [], "figure_ref": ["fig_15"], "table_ref": []}, {"heading": "ES PES", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "ES Gradient Estimate", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "L. Ablation Studies", "text": "In this section, we show an ablation study over the the number of particles N , and the truncation length K (which controls the number of unrolls per inner-problem). In Figure 21 we show the sensitivity of PES to these meta-parameters for a version of the 2D regression problem (from Section 5.4) with total inner problem length T = 10, 000.\n10 1 10 2 10 3 10 4 10 5\nInner Problem Steps Figure 21. Ablation over meta-parameters for PES applied to the toy 2D regression task with total number of inner steps T = 10, 000.\nHere we vary the truncation length K and number of particles N ; all other meta-parameters are fixed: we used Adam with learning rate 3e-2 for meta-optimization, and perturbation standard deviation 0.1. (a) Decreasing K yields shorter truncations, which allow for more frequent meta-updates, improving performance compared to longer truncations. For these runs, N = 10 4 . (b) As in standard ES, increasing the particle count for PES reduces variance and can yield substantial improvements in terms of inner iterations performed, or wall-clock time. For these runs, K = 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "M. Implementation", "text": "Code Listing 1 presents a simple JAX implementation of the toy 2D regression meta-learning problem from Section 5.4, in a self-contained, runnable example. PES is easy to implement efficiently in JAX by making use of the construct jax.vmap (or jax.pmap in settings with multiple workers) to parallelize the unrolling computations over N particles.\nListing 1. Simplified PES implementation in JAX, for the 2D regression problem from Section 5.4. from functools import partial import jax import jax.numpy as jnp def loss(x):\n\"\"\"Inner loss.\"\"\" return jnp.sqrt( \"\"\"Unroll the inner problem for K steps.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Args:", "text": "x_init: the initial state for the unroll theta: a 2-dimensional array of outer parameters (log_init_lr, log_final_lr) t0: initial time step to unroll from T: maximum number of steps for the inner problem K: number of steps to unroll", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We thank Sergey Ioffe and Niru Maheswaranathan for very helpful discussions and feedback on the paper. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "References", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Algorithm 3 Original persistent evolution strategies (PES) estimator, identical to Section 4.\nInput: s 0 , initial state K, truncation length for partial unrolls N , number of particles \u03c3, standard deviation of perturbations \u03b1, learning rate for PES optimization Initialize s = s 0 Initialize s (i) = s 0 for i \u2208 {1, . . . , N } Initialize \u03be (i) \u2190 0 for i \u2208 {1, . . . , N } while true do s,\ni even \ni even", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Returns:", "text": "L: the loss resulting from the unroll x_curr: the updated state at the end of the unroll \"\"\" L = 0.0 initial_state = (L, x_init, theta, t0, T, K) state, outputs = jax.lax.scan(update, initial_state, None, length=K) (L, x_curr, theta, t_curr, T, K) = state return L, x_curr @partial(jax.jit, static_argnums=(5,6,7,8)) def pes_grad(key, xs, pert_accum, theta, t0, T, K, sigma, N):\n\"\"\"Compute PES gradient estimate.\nArgs: key: JAX PRNG key xs: Nx2 array of particles/states to be updated pert_accum: Nx2 array of accumlated perturbations for each particle theta: a 2-dimensional array of outer parameters (log_init_lr, log_final_lr) t0: initial time step for the current unroll T: maximum number of steps for the inner problem K: truncation length for the unroll sigma: standard deviation of the Gaussian perturbations N: number of perturbations (as N//2 antithetic pairs)\nReturns: theta_grad: PES gradient estimate xs: Nx2 array of updates particles/states pert_accum: Nx2 array of updated perturbations for each particle \"\"\" # Generate antithetic perturbations pos_perts = jax.random.normal(key, (N//2, theta.shape[0])) * sigma # Antithetic positives neg_perts = -pos_perts # Antithetic negatives perts = jnp.concatenate([pos_perts, neg_perts], axis=0) # Unroll the inner problem for K steps using the antithetic perturbations of theta L, xs = jax.vmap(unroll, in_axes=(0,0,None,None,None))(xs, theta + perts, t0, T, K) # Add the perturbations from this unroll to the perturbation accumulators pert_accum = pert_accum + perts # Compute the PES gradient estimate theta_grad = jnp.mean(pert_accum * L.reshape(-1, 1) / (sigma ** 2), axis=0) return theta_grad, xs, pert_accum opt_params = { 'lr': 1e-2, 'b1': 0.99, 'b2': 0.999, 'eps': 1e-8, 'm': jnp.zeros(2), 'v': jnp.zeros(2) } def outer_optimizer_step(params, grads, opt_params, t): theta_grad, xs, pert_accum = pes_grad(skey, xs, pert_accum, theta, t, T, K, sigma, N) theta, opt_params = outer_optimizer_step(theta, theta_grad, opt_params, i) t += K if i % 100 == 0: L, _ = unroll(jnp.array([1.0, 1.0]), theta, 0, T, T) # Run a full unroll to get the cost print(i, jnp.exp(theta), theta_grad, L)", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Stochastic hyperparameter optimization through hypernetworks", "journal": "", "year": "2018", "authors": "J Lorraine; D Duvenaud"}, {"ref_id": "b1", "title": "Optimizing millions of hyperparameters by implicit differentiation", "journal": "", "year": "2020", "authors": "J Lorraine; P Vicol; D Duvenaud"}, {"ref_id": "b2", "title": "Self-tuning networks: Bilevel optimization of hyperparameters using structured best-response functions", "journal": "", "year": "2019", "authors": "M Mackay; P Vicol; J Lorraine; D Duvenaud; R Grosse"}, {"ref_id": "b3", "title": "Gradientbased hyperparameter optimization through reversible learning", "journal": "", "year": "2015", "authors": "D Maclaurin; D Duvenaud; Adams ; R "}, {"ref_id": "b4", "title": "Guided evolutionary strategies: Augmenting random search with surrogate gradients", "journal": "", "year": "2018", "authors": "N Maheswaranathan; L Metz; G Tucker; D Choi; J Sohl-Dickstein"}, {"ref_id": "b5", "title": "Simple random search provides a competitive approach to reinforcement learning", "journal": "", "year": "2018", "authors": "H Mania; A Guy; B Recht"}, {"ref_id": "b6", "title": "Building a large annotated corpus of English: The Penn Treebank", "journal": "Computational Linguistics", "year": "1993", "authors": "M Marcus; B Santorini; M A Marcinkiewicz"}, {"ref_id": "b7", "title": "A unified framework of online learning algorithms for training recurrent neural networks", "journal": "", "year": "2019", "authors": "O Marschall; K Cho; C Savin"}, {"ref_id": "b8", "title": "A practical sparse approximation for real time recurrent learning", "journal": "", "year": "2020", "authors": "J Menick; E Elsen; U Evci; S Osindero; K Simonyan; A Graves"}, {"ref_id": "b9", "title": "Meta-learning update rules for unsupervised representation learning", "journal": "", "year": "2018", "authors": "L Metz; N Maheswaranathan; B Cheung; J Sohl-Dickstein"}, {"ref_id": "b10", "title": "Understanding and correcting pathologies in the training of learned optimizers", "journal": "", "year": "2019", "authors": "L Metz; N Maheswaranathan; J Nixon; D Freeman; J Sohl-Dickstein"}, {"ref_id": "b11", "title": "Tasks, stability, architecture, and compute: Training more effective learned optimizers, and using them to train themselves", "journal": "", "year": "2020", "authors": "L Metz; N Maheswaranathan; C D Freeman; B Poole; J Sohl-Dickstein"}, {"ref_id": "b12", "title": "Using a thousand optimization tasks to learn hyperparameter search strategies", "journal": "", "year": "2020", "authors": "L Metz; N Maheswaranathan; R Sun; C D Freeman; B Poole; J Sohl-Dickstein"}, {"ref_id": "b13", "title": "Non-greedy gradient-based hyperparameter optimization over long horizons", "journal": "", "year": "2020", "authors": "P Micaelli; A Storkey"}, {"ref_id": "b14", "title": "Approximating realtime recurrent learning with random Kronecker factors", "journal": "", "year": "2018", "authors": "A Mujika; F Meier; A Steger"}, {"ref_id": "b15", "title": "Random gradient-free minimization of convex functions", "journal": "Foundations of Computational Mathematics", "year": "2017", "authors": "Y Nesterov; V Spokoiny"}, {"ref_id": "b16", "title": "Monte Carlo Theory, Methods and Examples", "journal": "", "year": "2013", "authors": "A B Owen"}, {"ref_id": "b17", "title": "Flexible model-based policy search robust to the curse of chaos", "journal": "", "year": "2018", "authors": "P Parmas; C E Rasmussen; J Peters; K Doya;  Pipps"}, {"ref_id": "b18", "title": "On the difficulty of training recurrent neural networks", "journal": "", "year": "2013", "authors": "R Pascanu; T Mikolov; Y Bengio"}, {"ref_id": "b19", "title": "An investigation of the gradient descent process in neural networks", "journal": "", "year": "1996", "authors": "B Pearlmutter"}, {"ref_id": "b20", "title": "Evolutionsstrategie: Optimierung technischer Systeme nach Prinzipien der biologischen Evolution", "journal": "Frommann-Holzboog", "year": "1973", "authors": "I Rechenberg"}, {"ref_id": "b21", "title": "Learning internal representations by error propagation", "journal": "", "year": "1985", "authors": "D E Rumelhart; G E Hinton; R J Williams"}, {"ref_id": "b22", "title": "Evolution strategies as a scalable alternative to reinforcement learning", "journal": "", "year": "2017", "authors": "T Salimans; J Ho; X Chen; S Sidor; I Sutskever"}, {"ref_id": "b23", "title": "Gradient estimation using stochastic computation graphs", "journal": "", "year": "2015", "authors": "J Schulman; N Heess; T Weber; P Abbeel"}, {"ref_id": "b24", "title": "Truncated back-propagation for bilevel optimization", "journal": "", "year": "2019", "authors": "A Shaban; C.-A Cheng; N Hatch; B Boots"}, {"ref_id": "b25", "title": "Practical Bayesian optimization of machine learning algorithms", "journal": "", "year": "2012", "authors": "J Snoek; H Larochelle; R P Adams"}, {"ref_id": "b26", "title": "", "journal": "", "year": "2012", "authors": "J Staines; D Barber"}, {"ref_id": "b27", "title": "", "journal": "", "year": "2014", "authors": "K Swersky; J Snoek; R P Adams"}, {"ref_id": "b28", "title": "Unbiased online recurrent optimization", "journal": "", "year": "2017", "authors": "C Tallec; Y Ollivier"}, {"ref_id": "b29", "title": "Unbiasing truncated backpropagation through time", "journal": "", "year": "2017", "authors": "C Tallec; Y Ollivier"}, {"ref_id": "b30", "title": "Variance reduction for evolution strategies via structured control variates", "journal": "", "year": "2020", "authors": "Y Tang; K Choromanski; A Kucukelbir"}, {"ref_id": "b31", "title": "Lecture 6.5-RMSprop: Divide the gradient by a running average of its recent magnitude", "journal": "", "year": "2012", "authors": "T Tieleman; G Hinton"}, {"ref_id": "b32", "title": "Backpropagation through time: What it does and how to do it", "journal": "Proceedings of the IEEE", "year": "1990", "authors": "P J Werbos"}, {"ref_id": "b33", "title": "Learned optimizers that scale and generalize", "journal": "", "year": "2017", "authors": "O Wichrowska; N Maheswaranathan; M W Hoffman; S G Colmenarejo; M Denil; N De Freitas; J Sohl-Dickstein"}, {"ref_id": "b34", "title": "An efficient gradient-based algorithm for on-line training of recurrent network trajectories", "journal": "Neural Computation", "year": "1990", "authors": "R J Williams; J Peng"}, {"ref_id": "b35", "title": "A learning algorithm for continually running fully recurrent neural networks", "journal": "Neural Computation", "year": "1989", "authors": "R J Williams; D Zipser"}, {"ref_id": "b36", "title": "Understanding short-horizon bias in stochastic meta-optimization", "journal": "", "year": "2018", "authors": "Y Wu; M Ren; R Liao; R Grosse"}], "figures": [{"figure_label": "5", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 5 .5Figure 5. Training learned optimizers. We find that PES achieves better performance compared to truncated ES. Curves of the same color denote different initializations of the learned optimizer. See Section 5.2 for details.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 8 .8Figure8. Meta-optimization of a learning rate schedule for an MLP on MNIST. We tune the initial learning rate and decay factor, both parameterized in log-space. Here we show the meta-loss landscape, and the optimization trajectories taken by ES and PES, for unroll lengths K of 10 and 100. The meta-objective in (a) is the training loss while the meta-objective in (b) is the validation accuracy. In both visualizations, darker colors denote better values. Note that most gradient-based approaches are unable to target accuracy.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 9 .9Figure9. Meta-optimization of per-parameter-block learning rates and momentum coefficients (29 hyperparameters total).", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 10 .10Figure 10. Optimization landscape for the toy 2D regression problem. (a) The outer loss (e.g. meta-loss) surface, showing the meta-objective values (e.g. the sum of losses over the inner optimization trajectory) for different settings of the two hyperparameters controlling the initial and final (log) learning rates of a linear decay schedule; (b) the inner loss surface, showing color-coded optimization trajectories corresponding to the hyperparameters highlighted in (a); (c) a close-up of the inner loss surface in the region where the parameters (x0, x1) are initialized at the start of the inner problem.", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 11 .11Figure 11. Longer run of influence balancing, with log-scaled x-axis.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Tuning LR & momentum for a Myrtle.ai ResNet on CIFAR-10.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Learned optimizer trained on MNIST.", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 12 .12Figure 12. CIFAR-10 experiment for hyperparameter optimization and MNIST experiment for learned optimizer training.", "figure_data": ""}, {"figure_label": "13", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 13 .13Figure13. Using ES and PES to tune the L2 regularization coefficient for linear regression on the UCI Yacht dataset.", "figure_data": ""}, {"figure_label": "14", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 14 .14Figure 14. Telescoping sum for FashionMNIST final training loss (colors show log-final-loss).", "figure_data": ""}, {"figure_label": "15", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 15 .15Figure 15. Shift in notation, dropping the dependence on st and explicitly including the dependence on each \u03b8t.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Tt=1 L t (s t ; \u03b8) = T t=1 L t (\u03b8 1 , . . . , \u03b8 t ) = T t=1 L t (\u0398).", "figure_data": ""}, {"figure_label": "17", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Figure 17 .17Figure17. A comparison of the PES and PES+Analytic gradient estimators, applied to partial unrolls of a computation graph. The conditional statement for(i)  is used to implement antithetic sampling. For clarity, we describe the meta-optimization updates to \u03b8 using SGD, but we typically use Adam in practice.", "figure_data": ""}, {"figure_label": "18", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "Figure 18 .18Figure18. Empirical variance measurements for three scenarios, incorporating the analytic gradient from the most recent unroll to reduce variance.", "figure_data": ""}, {"figure_label": "20", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "Figure 20 .20Figure 20. Left: Evolution strategies (ES). Right: Persistent evolution strategies (PES).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_16", "figure_caption": "x[0] ** 2 + 5) -jnp.sqrt(5) + jnp.sin(x[1]) ** 2 * \\ jnp.exp(-5 * x[0] ** 2) + 0.25 * jnp.abs(x[1] -100) # Gradient of inner loss loss_grad = jax.grad(loss) def update(state, i): \"\"\"Performs a single inner problem update, e.g., a single unroll step. \"\"\" (L, x, theta, t_curr, T, K) = state lr = jnp.exp(theta[0]) * (T -t_curr) / T + jnp.exp(theta[1]) * t_curr / T x = x -lr * loss_grad(x) L += loss(x) * (t_curr < T) t_curr += 1 return (L, x, theta, t_curr, T, K), x @partial(jax.jit, static_argnums=(3,4)) def unroll(x_init, theta, t0, T, K):", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Loss curves for the influence balancing task. TBPTT with short truncations diverges, while PES performs nearly identically to exact RTRL. See Section 5.1 for experiment details.", "figure_data": "Loss10 3 10 11TBPTT 1 TBPTT 10 TBPTT 100 ESPES UORO RTRL10 510 1301000 Iteration 20003000Figure 4.9)"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Asuncion, A. and Newman, D. UCI machine learning repository, 2007. Baydin, A. G., Cornish, R., Rubio, D. M., Schmidt, M., and Wood, F. Online learning rate adaptation with hypergradient descent. arXiv preprint arXiv:1703.04782, 2017. Beatson, A. and Adams, R. P. Efficient optimization of loops and limits with randomized telescoping sums. arXiv preprint arXiv:1905.07006, 2019. URL http://github.com/google/jax. Chen, T., Xu, B., Zhang, C., and Guestrin, C. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016. Cooijmans, T. and Martens, J. On the variance of unbiased online recurrent optimization. arXiv preprint arXiv:1902.02405, 2019. Cui, X., Zhang, W., T\u00fcske, Z., and Picheny, M. Evolutionary stochastic gradient descent for optimization of deep neural networks. In Advances in Neural Information Processing Systems, pp. 6048-6058, 2018.Franceschi, L., Frasconi, P., Salzo, S., Grazzi, R., and Pontil, M. Bilevel programming for hyperparameter optimization and meta-learning. arXiv preprint arXiv:1806.04910, 2018.Grefenstette, E., Amos, B., Yarats, D., Htut, P. M., Molchanov, A., Meier, F., Kiela, D., Cho, K., and Chintala, S.Generalized inner loop meta-learning. arXiv preprint arXiv:1910.01727, 2019. Parallel architecture and hyperparameter search via successive halving and classification. arXiv preprint arXiv:1805.10255, 2018. Li, K. and Malik, J. Learning to optimize. arXiv preprint arXiv:1606.01885, 2016. Li, K. and Malik, J. Learning to optimize neural nets. arXiv preprint arXiv:1703.00441, 2017.", "figure_data": "Unbiased Gradient Estimation in Unrolled Computation Graphswith Persistent Evolution StrategiesSupplementary MaterialHa, D. Neuroevolution for deep reinforcement learningBenzing, F., Gauy, M. M., Mujika, A., Martinsson, A., and Paul Vicol Luke Metz Jascha Sohl-Dickstein problems. In Genetic and Evolutionary Computation Conference Companion, pp. 404-427, 2020.Steger, A. Optimal Kronecker-sum approximation of real time recurrent learning. arXiv preprint arXiv:1902.03993, 2019.Ha, D. and Schmidhuber, J. World models. arXiv preprint arXiv:1803.10122, 2018.Bergstra, J. and Bengio, Y. Random search for hyper-parameter optimization. The Journal of Machine Learn-Hansen, N. The CMA evolution strategy: A tutorial. arXiv preprint arXiv:1604.00772, 2016.ing Research, 13(1):281-305, 2012.Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary,C., Maclaurin, D., and Wanderman-Milne, S. JAX: Com-posable transformations of Python+NumPy programs,2018. Dauvergne, B. and Hasco\u00ebt, L. The data-flow equationsof checkpointing in reverse automatic differentiation. InInternational Conference on Computational Science, pp.566-573, 2006.Domke, J. Generic methods for optimization-based model-ing. In Artificial Intelligence and Statistics, pp. 318-326,2012.Donini, M., Franceschi, L., Pontil, M., Majumder, O., andFrasconi, P. Scheduling the learning rate via hypergradi-ents: New insights and a new algorithm. arXiv preprintarXiv:1910.08525, 2019."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "summarizes the notation used in this paper.", "figure_data": "SymbolMeaningESEvolution strategiesPESPersistent evolution strategies(T)BPTT(Truncated) backpropagation through timeRTRLReal time recurrent learningUOROUnbiased online recurrent optimizationT"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Table of notation, defining the terms we use in this paper.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Empirical variance measurements for three scenarios.", "figure_data": "10 5 10 6N=10 N=30N=100 N=100010 3N=10 N=30N=100 N=100010 3N=10 N=30N=100 N=1000Variance10 2 10 3 10 4Variance10 1 10 2Variance10 1 10 210 110 010 010 010 010 1 # Unrolls 10 210 310 010 1 # Unrolls 10 210 310 010 1 # Unrolls 10 210 3(a) Random sequence(b) Single character repeated(c) Real PTB sequenceFigure 16."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_11", "figure_caption": ". In order to optimize this objective, we need the gradient \u2207 \u03b8 L =", "figure_data": ")and our objective is L =T t=1 L t T t=1dLt d\u03b8 . The loss atstep t is a function of s t , so we have:dL t (s t ) d\u03b8=\u2202L t \u2202s tds t d\u03b8(148)Using Eq. 147 and the chain rule, we have:ds t d\u03b8=df (s t\u22121 , x t , \u03b8) d\u03b8(149)01=\u2202s t \u2202s t\u22121ds t\u22121 d\u03b8+\u2202s t \u2202x tU dx t d\u03b8+\u2202s t \u2202\u03b8\u00a1 \u00a1 \u00a1 ! d\u03b8 d\u03b8(150)=\u2202s t \u2202s t\u22121ds t\u22121 d\u03b8+\u2202s t \u2202\u03b8(151)Thus, we have the recurrence relation:ds t d\u03b8=\u2202s t \u2202s t\u22121ds t\u22121 d\u03b8+\u2202s t \u2202\u03b8(152)GtHtGt\u22121Ft"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "approximates RTRL by maintaining a rank-1 estimate of the Jacobian G t as:G t \u2248s t\u03b8 t (153)wheres t and\u03b8 t are vectors of dimensions S and P , respectively. Ultimately, we are interested in the gradient \u2202Lt \u2202\u03b8 . Using the UORO approximation to G t , we can write the gradient as follows:", "figure_data": "\u2202L t \u2202\u03b8=\u2202L t \u2202s tds t d\u03b8(154)=\u2202L t \u2202s tG t(155)=\u2202L t \u2202s t(H t G t\u22121 + F t )(156)=\u2202L t \u2202s t(H t (s t\u03b8 t ) + F t )(157)=\u2202L t \u2202s t(H t (s t\u03b8 t )) +\u2202L t \u2202s tF t(158)=\u2202L t \u2202s t(H t (s t\u03b8 t )) +\u2202L t \u2202s t\u2202s t \u2202\u03b8(159)=\u2202L t \u2202s t(H t (s t\u03b8 t )) +\u2202L t \u2202\u03b8(160)=\u2202L t \u2202s tH tstt +\u2202L t \u2202\u03b81\u00d71\u03b81\u00d7P"}], "formulas": [{"formula_id": "formula_0", "formula_text": "s t = f (s t\u22121 , x t ; \u03b8) (1)", "formula_coordinates": [2.0, 132.63, 673.51, 156.81, 9.68]}, {"formula_id": "formula_1", "formula_text": "L(\u03b8) = T t=1 L t (s t ; \u03b8) (2)", "formula_coordinates": [2.0, 380.96, 81.51, 160.48, 30.2]}, {"formula_id": "formula_2", "formula_text": "g ES = 1 N \u03c3 2 N i=1 (i) L(\u03b8 + (i) )(3)", "formula_coordinates": [2.0, 360.06, 668.24, 181.38, 30.32]}, {"formula_id": "formula_3", "formula_text": "g ES-A = 1 N \u03c3 2 N/2 i=1 (i) (L(\u03b8 + (i) ) \u2212 L(\u03b8 \u2212 (i) ))", "formula_coordinates": [3.0, 70.89, 411.69, 203.11, 31.18]}, {"formula_id": "formula_4", "formula_text": "PES (Ours) N KF N (S + P ) PES + Analytic (Ours) N KF + K(F + B) N (S + P ) + (K + 1)S", "formula_coordinates": [4.0, 72.44, 258.5, 287.69, 19.29]}, {"formula_id": "formula_5", "formula_text": "dL(\u03b8) d\u03b8 = T \u03c4 =1 \u2202L (\u0398) \u2202\u03b8 \u03c4 = I \u2297 1 \u2202L(\u0398) \u2202 vec (\u0398) , g PES = I \u2297 1 E 1 \u03c3 2 vec ( ) L (\u0398 + ) = 1 \u03c3 2 E T \u03c4 =1 \u03c4 L (\u0398 + ) ,", "formula_coordinates": [4.0, 330.92, 525.31, 185.22, 92.93]}, {"formula_id": "formula_6", "formula_text": "(i) = draw from N (0, \u03c3 2 I) i odd \u2212 (i\u22121)", "formula_coordinates": [5.0, 100.11, 218.58, 162.67, 22.74]}, {"formula_id": "formula_7", "formula_text": "L (i) K \u2190 unroll(s, \u03b8 + (i) , K) \u03be (i) \u2190 \u03be (i) + (i) g ES \u2190\u011d ES + (i)L (i) K end for g ES \u2190 1 N \u03c3 2\u011d ES s \u2190 unroll(s, \u03b8, K) \u03b8 \u2190 \u03b8 \u2212 \u03b1\u011d ES end while", "formula_coordinates": [5.0, 65.4, 244.44, 149.11, 98.75]}, {"formula_id": "formula_8", "formula_text": "s (i) = s 0 for i \u2208 {1, . . . , N } Initialize \u03be (i) \u2190 0 for i \u2208 {1, . . . , N } while true d\u00f4 g PES \u2190 0 for i = 1, . . . , N do (i) = draw from N (0, \u03c3 2 I) i odd \u2212 (i\u22121)", "formula_coordinates": [5.0, 315.63, 158.85, 197.37, 83.08]}, {"formula_id": "formula_9", "formula_text": "s (i) ,L (i) K \u2190 unroll(s (i) , \u03b8 + (i) , K) \u03be (i) \u2190 \u03be (i) + (i) g PES \u2190\u011d PES + \u03be (i)L(i) K end for g PES \u2190 1 N \u03c3 2\u011d PES s \u2190 unroll(s, \u03b8, K) \u03b8 \u2190 \u03b8 \u2212 \u03b1\u011d PES end while Figure 2.", "formula_coordinates": [5.0, 54.94, 245.05, 439.09, 114.59]}, {"formula_id": "formula_10", "formula_text": "g PES = 1 \u03c3 2 E T \u03c4 =1 \u03c4 T t=1 L t (\u0398 + ) = 1 \u03c3 2 E T t=1 t \u03c4 =1 \u03c4 L t (\u0398 + ) (4) = E T t=1\u011d PES t, ,(5)", "formula_coordinates": [5.0, 86.09, 444.76, 378.07, 99.87]}, {"formula_id": "formula_11", "formula_text": "g PES t, = 1 \u03c3 2 \u03be t L t (\u03b8 1 + 1 , . . . , \u03b8 t + t ) . (6", "formula_coordinates": [5.0, 86.09, 547.87, 199.47, 22.31]}, {"formula_id": "formula_12", "formula_text": ")", "formula_coordinates": [5.0, 285.57, 554.93, 3.87, 8.64]}, {"formula_id": "formula_13", "formula_text": "g PES = 1 N N i=1 T t=1\u011d PES t, (i)(7)", "formula_coordinates": [5.0, 122.0, 640.42, 167.44, 30.32]}, {"formula_id": "formula_14", "formula_text": "g PES-A = (I \u2297 1 )E 1 2\u03c3 2 vec ( ) (L(\u0398 + ) \u2212 L(\u0398 \u2212 )) \u2248 1 2\u03c3 2 N N i=1 T t=1 \u03be (i) t L t (\u0398 + (i) ) \u2212 L t (\u0398 \u2212 (i) )", "formula_coordinates": [5.0, 307.44, 444.22, 241.29, 58.32]}, {"formula_id": "formula_15", "formula_text": "Statement 4.1 (PES is unbiased). Let \u03b8 \u2208 R P and L(\u03b8) = T t=1 L t (\u03b8).", "formula_coordinates": [5.0, 307.44, 546.26, 232.43, 24.68]}, {"formula_id": "formula_16", "formula_text": "L(\u0398 + ) = L(\u0398) + vec( ) \u2207 vec(\u0398) L(\u0398) + 1 2 vec( ) \u2207 2 vec(\u0398) L(\u0398) vec( ). Then, bias(\u011d PES-A ) = E [\u011d PES-A ] \u2212 \u2207 \u03b8 L(\u03b8) = 0.", "formula_coordinates": [5.0, 307.44, 595.66, 235.93, 38.67]}, {"formula_id": "formula_17", "formula_text": "A Diagonal M , i.i.d. grads ||g|| 1 2 P T + 1 2 P + T Diagonal M , identical grads ||g|| 1 2T P + 1 2 P + 1 Upper-tri M , i.i.d. grads ||g|| O(T 2 + P T ) Upper-tri M , identical grads ||g|| O( P T )", "formula_coordinates": [6.0, 59.88, 73.63, 214.19, 86.62]}, {"formula_id": "formula_18", "formula_text": "M = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 \u2207 \u03b81 L 1 \u2207 \u03b81 L 2 \u2207 \u03b81 L 3 \u2022 \u2022 \u2022 \u2207 \u03b81 L T 0 \u2207 \u03b82 L 2 \u2207 \u03b82 L 3 \u2022 \u2022 \u2022 \u2207 \u03b82 L T 0 0 \u2207 \u03b83 L 3 \u2022 \u2022 \u2022 \u2207 \u03b83 L T . . . . . . . . . . . . . . . 0 0 0 \u2022 \u2022 \u2022 \u2207 \u03b8 T L T \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb(8)", "formula_coordinates": [6.0, 63.38, 476.94, 226.06, 65.93]}, {"formula_id": "formula_19", "formula_text": "\u2202L t (\u0398) \u2202\u03b8 \u2248 1 \u03c3 2 E \u03c4 <t \u03c4 (L t (\u0398 + ) \u2212 t p t ) + p t(", "formula_coordinates": [6.0, 309.07, 675.58, 231.45, 38.04]}, {"formula_id": "formula_20", "formula_text": "t = 1 + \u2022 \u2022 \u2022 + t L t (\u0398)", "formula_coordinates": [14.0, 118.89, 459.96, 337.84, 24.59]}, {"formula_id": "formula_21", "formula_text": "L(\u03b8), L(\u0398) The total loss, L(\u03b8) = L(\u0398) = T t=1 L t (\u0398) = T t=1 L t (\u03b8 1 , . . . , \u03b8 t ) g t", "formula_coordinates": [14.0, 107.93, 488.28, 363.67, 28.5]}, {"formula_id": "formula_22", "formula_text": "g (t, \u03c4 ) Shorthand for \u2202Lt(\u0398) \u2202\u03b8\u03c4 , used in variance expressions \u2297 Kronecker product \u03b1", "formula_coordinates": [14.0, 117.77, 567.82, 316.57, 41.42]}, {"formula_id": "formula_23", "formula_text": "f (x 0 , x 1 ) = x 2 0 + 5 \u2212 \u221a 5 + sin 2 (x 1 ) exp(\u22125x 2 0 ) + 0.25|x 1 \u2212 100|(10)", "formula_coordinates": [15.0, 157.69, 631.15, 383.75, 19.54]}, {"formula_id": "formula_24", "formula_text": "s t+1 = As t + (\u03b8, . . . , \u03b8 p positive , \u2212\u03b8, . . . , \u2212\u03b8 n \u2212 p negative ) (11", "formula_coordinates": [16.0, 124.83, 390.71, 224.36, 23.04]}, {"formula_id": "formula_25", "formula_text": ")", "formula_coordinates": [16.0, 349.19, 391.06, 4.15, 8.64]}, {"formula_id": "formula_26", "formula_text": "L t = 1 2 (s 0 t \u2212 1) 2(12)", "formula_coordinates": [16.0, 169.35, 497.38, 183.99, 22.31]}, {"formula_id": "formula_27", "formula_text": "T t=0 p t = ( & & L 0 \u2212 L \u22121 ) + ( & & L 1 \u2212 & & L 0 ) + ( & & L 2 \u2212 & & L 1 ) + \u2022 \u2022 \u2022 + ($ $ $ L T \u22121 \u2212 $ $ $ L T \u22122 ) + (L T \u2212 $ $ $ L T \u22121 ) = L T (13", "formula_coordinates": [18.0, 102.65, 571.9, 434.64, 30.2]}, {"formula_id": "formula_28", "formula_text": "dL(\u03b8) d\u03b8 = dL(\u0398) d\u03b8 = \u2202L(\u0398) \u2202\u03b8 1 U 1 d\u03b8 1 d\u03b8 + \u2202L(\u0398) \u2202\u03b8 2 U 1 d\u03b8 2 d\u03b8 + \u2022 \u2022 \u2022 + \u2202L(\u0398) \u2202\u03b8 T 1 d\u03b8 T d\u03b8 = T \u03c4 =1 \u2202L (\u0398) \u2202\u03b8 \u03c4 = I \u2297 1 \u2202L(\u0398) \u2202 vec (\u0398)", "formula_coordinates": [19.0, 81.92, 507.19, 433.05, 38.45]}, {"formula_id": "formula_29", "formula_text": "\u2202L(\u0398)", "formula_coordinates": [19.0, 395.99, 567.07, 22.49, 6.12]}, {"formula_id": "formula_30", "formula_text": "dL(\u03b8) d\u03b8 \u2248 g PES = I \u2297 1 E 1 \u03c3 2 vec ( ) L (\u0398 + ) = 1 \u03c3 2 E I \u2297 1 vec ( ) L (\u0398 + ) = 1 \u03c3 2 E T \u03c4 =1 \u03c4 L (\u0398 + ) ,", "formula_coordinates": [19.0, 189.12, 602.07, 214.57, 82.17]}, {"formula_id": "formula_31", "formula_text": "g PES = 1 \u03c3 2 E T \u03c4 =1 \u03c4 L (\u0398 + ) = 1 \u03c3 2 E T \u03c4 =1 \u03c4 T t=1 L t (\u0398 + ) = 1 \u03c3 2 E T t=1 T \u03c4 =1 \u03c4 L t (\u0398 + ) (14) = 1 \u03c3 2 E T t=1 t \u03c4 =1 \u03c4 L t (\u0398 + ) (15) = 1 \u03c3 2 E T t=1 \u03be t L t (\u0398 + ) (16) = E T t=1\u011d PES t, ,(17)", "formula_coordinates": [20.0, 177.98, 108.01, 688.33, 204.36]}, {"formula_id": "formula_32", "formula_text": "g PES t, = 1 \u03c3 2 \u03be t L t (\u0398 + ) = 1 \u03c3 2 \u03be t L t (\u03b8 1 + 1 , . . . , \u03b8 t + t ) . (18", "formula_coordinates": [20.0, 177.98, 315.61, 359.31, 22.31]}, {"formula_id": "formula_33", "formula_text": ")", "formula_coordinates": [20.0, 537.29, 322.67, 4.15, 8.64]}, {"formula_id": "formula_34", "formula_text": "g PES = 1 N N i=1 T t=1\u011d PES t, (i)(19)", "formula_coordinates": [20.0, 248.0, 398.12, 293.44, 30.32]}, {"formula_id": "formula_35", "formula_text": "\u0398 = \uf8ee \uf8f0 \u2212 \u2212\u2212 \u2212\u03b8 1 \u2212 \u2212\u2212 \u2212 \u2212 \u2212\u2212 \u2212\u03b8 2 \u2212 \u2212\u2212 \u2212 \u2212 \u2212\u2212 \u2212\u03b8 3 \u2212 \u2212\u2212 \u2212 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb \u2202L(\u0398) \u2202vec(\u0398) = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 \u2202L(\u0398) \u2202\u03b8 (1) 1", "formula_coordinates": [20.0, 221.12, 510.31, 170.36, 196.04]}, {"formula_id": "formula_36", "formula_text": "\uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb", "formula_coordinates": [20.0, 392.68, 586.4, 6.64, 119.95]}, {"formula_id": "formula_37", "formula_text": "I \u2297 1 = 1 0 0 1 \u2297 1 1 1 = 1 1 1 0 0 0 0 0 0 1 1 1", "formula_coordinates": [21.0, 162.98, 68.62, 226.29, 20.69]}, {"formula_id": "formula_38", "formula_text": "(I \u2297 1 ) \u2202L(\u0398) \u2202vec(\u0398) = \uf8ee \uf8ef \uf8f0 \u2202L(\u0398) \u2202\u03b8 (1) 1 + \u2202L(\u0398) \u2202\u03b8 (1) 2 + \u2202L(\u0398) \u2202\u03b8 (1) 3 \u2202L(\u0398) \u2202\u03b8 (2) 1 + \u2202L(\u0398) \u2202\u03b8 (2) 2 + \u2202L(\u0398) \u2202\u03b8 (2) 3 \uf8f9 \uf8fa \uf8fb = \uf8ee \uf8ef \uf8f0 \u2202L(\u0398) \u2202\u03b8 (1) 1 \u2202L(\u0398) \u2202\u03b8 (2) 1 \uf8f9 \uf8fa \uf8fb \u2202L(\u0398) \u2202\u03b8 1 + \uf8ee \uf8ef \uf8f0 \u2202L(\u0398) \u2202\u03b8 (1) 2 \u2202L(\u0398) \u2202\u03b8 (2) 2 \uf8f9 \uf8fa \uf8fb \u2202L(\u0398) \u2202\u03b8 2 + \uf8ee \uf8ef \uf8f0 \u2202L(\u0398) \u2202\u03b8 (1) 3 \u2202L(\u0398) \u2202\u03b8 (2) 3 \uf8f9 \uf8fa \uf8fb \u2202L(\u0398) \u2202\u03b8 3 = T \u03c4 =1 \u2202L(\u0398) \u2202\u03b8 \u03c4 = dL(\u0398) d\u03b8", "formula_coordinates": [21.0, 72.05, 104.19, 451.59, 65.94]}, {"formula_id": "formula_39", "formula_text": "= \uf8ee \uf8f0 \u2212 \u2212\u2212 \u2212 1 \u2212 \u2212\u2212 \u2212 \u2212 \u2212\u2212 \u2212 2 \u2212 \u2212\u2212 \u2212 \u2212 \u2212\u2212 \u2212 3 \u2212 \u2212\u2212 \u2212 \uf8f9 \uf8fb = \uf8ee \uf8ef \uf8f0 (1) 1 (2) 1 (1) 2 (2) 2 (1) 3 (2) 3 \uf8f9 \uf8fa \uf8fb vec( ) = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 (1) 1 (1) 2 (1) 3 (2) 1 (2) 2 (2) 3 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb", "formula_coordinates": [21.0, 177.1, 205.77, 250.26, 84.24]}, {"formula_id": "formula_40", "formula_text": "(I \u2297 1 )vec( ) = (1) 1 + (1) 2 + (1) 3 (2) 1 + (2) 2 + (2) 3 = (1) 1 (2) 1 1 + (1) 2 (2) 2 2 + (1) 3 (2) 3 3 = T \u03c4 =1 \u03c4", "formula_coordinates": [21.0, 143.31, 315.23, 308.96, 43.95]}, {"formula_id": "formula_41", "formula_text": "1 \u03c3 2 E I \u2297 1 vec ( ) L (\u0398 + ) = 1 \u03c3 2 E T \u03c4 =1 \u03c4 L (\u0398 + ) F. Proof that PES is Unbiased Statement F.1. Let \u03b8 \u2208 R n and L(\u03b8) = T t=1 L t (\u03b8).", "formula_coordinates": [21.0, 55.44, 391.58, 376.73, 80.53]}, {"formula_id": "formula_42", "formula_text": "L(\u0398 + ) = L(\u0398) + vec( ) \u2207 vec(\u0398) L(\u0398) + 1 2 vec( ) \u2207 2 vec(\u0398) L(\u0398) vec( )", "formula_coordinates": [21.0, 142.7, 494.15, 311.48, 22.31]}, {"formula_id": "formula_43", "formula_text": "g PES-A = (I \u2297 1 )E 1 2\u03c3 2 vec( )(L(\u0398 + ) \u2212 L(\u0398 \u2212 )) ,", "formula_coordinates": [21.0, 175.0, 549.82, 246.88, 22.31]}, {"formula_id": "formula_44", "formula_text": "PES-A ) = E [\u011d PES-A ] \u2212 \u2207 \u03b8 L(\u03b8) = 0.", "formula_coordinates": [21.0, 197.93, 585.6, 145.19, 12.1]}, {"formula_id": "formula_45", "formula_text": "1 2 vec( ) \u2207 2 vec(\u0398) L(\u0398) vec( ))(20)", "formula_coordinates": [21.0, 348.93, 653.29, 192.51, 22.31]}, {"formula_id": "formula_46", "formula_text": "1 2 vec( ) \u2207 2 vec(\u0398) L(\u0398) vec( ))(21)", "formula_coordinates": [21.0, 361.11, 678.15, 180.33, 22.31]}, {"formula_id": "formula_47", "formula_text": "=2 vec( ) \u2207 vec(\u0398) L(\u0398)(22)", "formula_coordinates": [21.0, 201.88, 704.67, 339.56, 9.96]}, {"formula_id": "formula_48", "formula_text": "PES-A = (I \u2297 1 )E 1 2\u03c3 2 2 vec( ) vec( ) \u2207 vec(\u0398) L(\u0398)(23)", "formula_coordinates": [22.0, 184.73, 89.64, 356.71, 22.31]}, {"formula_id": "formula_49", "formula_text": "= (I \u2297 1 ) 1 \u03c3 2 E vec( ) vec( ) \u03c3 2 I \u2207 vec(\u0398) L(\u0398)(24)", "formula_coordinates": [22.0, 207.37, 116.25, 334.07, 31.59]}, {"formula_id": "formula_50", "formula_text": "= (I \u2297 1 )\u2207 vec(\u0398) L(\u0398) (25) = \u2207 \u03b8 L(\u03b8)(26)", "formula_coordinates": [22.0, 207.37, 153.6, 334.07, 24.63]}, {"formula_id": "formula_51", "formula_text": "g PES-A = 1 \u03c3 2 T t=1 \u03be t vec ( 1...t ) \u2207 vec(\u03981...t) L t (\u0398)(27)", "formula_coordinates": [22.0, 197.11, 266.01, 344.33, 30.2]}, {"formula_id": "formula_52", "formula_text": "tr(Var(\u011d PES-A )) = tr(E[\u011d PES-A\u011dPES-A ] \u2212 E[\u011d PES-A ]E[\u011d PES-A ] ),(28)", "formula_coordinates": [22.0, 170.79, 341.28, 370.65, 11.62]}, {"formula_id": "formula_53", "formula_text": "= E[\u011d PES-A \u011d PES-A ] 1 \u2212 E[\u011d PES-A ] E[\u011d PES-A ] 2 (29)", "formula_coordinates": [22.0, 234.64, 360.59, 306.8, 28.57]}, {"formula_id": "formula_54", "formula_text": "2 = E[\u011d PES-A ] E[\u011d PES-A ] = \u2207 \u03b8 L(\u0398) \u2207 \u03b8 L(\u0398) = ||\u2207 \u03b8 L(\u0398)|| 2(30)", "formula_coordinates": [22.0, 170.12, 424.41, 371.32, 11.98]}, {"formula_id": "formula_55", "formula_text": "g PES-A \u011d PES-A = 1 \u03c3 4 \uf8eb \uf8ec \uf8ed T t=1 \u03be t vec ( 1...t ) vt \u2207 vec(\u03981...t) L t (\u0398) g t \uf8f6 \uf8f7 \uf8f8 \uf8eb \uf8ec \uf8ed T t=1 \u03be t vec ( 1...t ) vt \u2207 vec(\u03981...t) L t (\u0398) g t \uf8f6 \uf8f7 \uf8f8 (31) = 1 \u03c3 4 \u03be 1 v 1 g 1 + \u2022 \u2022 \u2022 + \u03be T v T g T \u03be 1 v 1 g 1 + \u2022 \u2022 \u2022 + \u03be T v T g T (32) = 1 \u03c3 4 \uf8eb \uf8ec \uf8edg 1 v 1 \u03be 1 \u03be 1 v 1 g 1 a + g 1 v 1 \u03be 1 \u03be 2 v 2 g 2 b + \u2022 \u2022 \u2022 + g T v T \u03be T \u03be T v T g T \uf8f6 \uf8f7 \uf8f8 (33)", "formula_coordinates": [22.0, 91.69, 485.08, 449.75, 112.91]}, {"formula_id": "formula_56", "formula_text": "v t g t = vec ( 1...t ) \u2207 vec(\u03981...t) L t (\u0398) = t \u03c4 =1 \u03c4 \u2207 \u03b8\u03c4 L t (\u0398)(34)", "formula_coordinates": [22.0, 177.13, 685.21, 364.31, 30.2]}, {"formula_id": "formula_57", "formula_text": "\u03be i \u03be i = ( 1 + \u2022 \u2022 \u2022 + i ) ( 1 + \u2022 \u2022 \u2022 + i ) = i m=1 m m + m\u2264i,n\u2264i,m =n m n(35)", "formula_coordinates": [23.0, 142.95, 91.52, 398.49, 30.55]}, {"formula_id": "formula_58", "formula_text": "g i v i \u03be i \u03be i v i g i = i m=1 m \u2207 \u03b8m L i (\u0398) i n=1 n n i m=1 m \u2207 \u03b8m L i (\u0398) I (36", "formula_coordinates": [23.0, 110.17, 162.85, 427.12, 46.66]}, {"formula_id": "formula_59", "formula_text": ")", "formula_coordinates": [23.0, 537.29, 173.58, 4.15, 8.64]}, {"formula_id": "formula_60", "formula_text": "+ i m=1 m \u2207 \u03b8m L i (\u0398) \uf8eb \uf8ed m\u2264i,n\u2264i,m =n m n \uf8f6 \uf8f8 i m=1 m \u2207 \u03b8m L i (\u0398) II (37", "formula_coordinates": [23.0, 186.83, 213.7, 350.47, 52.68]}, {"formula_id": "formula_61", "formula_text": ")", "formula_coordinates": [23.0, 537.29, 227.64, 4.15, 8.64]}, {"formula_id": "formula_62", "formula_text": "E \u2207 \u03b8m L i (\u0398) m m m m \u2207 \u03b8m L i (\u0398) = \u2207 \u03b8m L i (\u0398) E m m m m (P +2)\u03c3 4 I \u2207 \u03b8m L i (\u0398)(38)", "formula_coordinates": [23.0, 121.68, 306.02, 419.76, 24.88]}, {"formula_id": "formula_63", "formula_text": "= (P + 2)\u03c3 4 ||\u2207 \u03b8m L i (\u0398)|| 2(39)", "formula_coordinates": [23.0, 292.91, 336.0, 248.54, 12.62]}, {"formula_id": "formula_64", "formula_text": "(P + 2)\u03c3 4 i m=1 ||\u2207 \u03b8m L i (\u0398)|| 2(44)", "formula_coordinates": [23.0, 236.01, 499.95, 305.43, 30.2]}, {"formula_id": "formula_65", "formula_text": "\u2207 \u03b8m L i (\u0398) m n n m \u2207 \u03b8m L i (\u0398) where m = n.", "formula_coordinates": [23.0, 338.49, 548.62, 204.69, 10.62]}, {"formula_id": "formula_66", "formula_text": "E \u2207 \u03b8m L i (\u0398) m n n m \u2207 \u03b8m L i (\u0398) = \u2207 \u03b8m L i (\u0398) E m n n m P \u03c3 4 I \u2207 \u03b8m L i (\u0398)(45)", "formula_coordinates": [23.0, 124.66, 582.72, 416.78, 24.85]}, {"formula_id": "formula_67", "formula_text": "E m, n m n n m = E m E n m n n m = E m [ m E n n n P \u03c3 2 m ] = P \u03c3 2 E n [ n n ] \u03c3 2 I = P \u03c3 4 I(46)", "formula_coordinates": [23.0, 78.78, 641.62, 462.66, 26.92]}, {"formula_id": "formula_68", "formula_text": "E m m m = E m tr( m m ) = E m tr( m m ) = tr E m m m = tr(\u03c3 2 I) = P \u03c3 2 (47)", "formula_coordinates": [23.0, 114.85, 702.6, 426.59, 12.69]}, {"formula_id": "formula_69", "formula_text": "i m=1 n,m\u2208{1,...,i},n =m \u2207 \u03b8m L i (\u0398) E m n n m \u2207 \u03b8m L i (\u0398) = i m=1 n,m\u2208{1,...,i},n =m P \u03c3 4 ||\u2207 \u03b8m L i (\u0398)|| 2 (48) = (i \u2212 1)P \u03c3 4 i m=1 ||\u2207 \u03b8m L i (\u0398)|| 2(49)", "formula_coordinates": [24.0, 71.89, 95.95, 469.55, 67.36]}, {"formula_id": "formula_70", "formula_text": "I = (P + 2)\u03c3 4 i m=1 ||\u2207 \u03b8m L i (\u0398)|| 2 + (i \u2212 1)P \u03c3 4 i m=1 ||\u2207 \u03b8m L i (\u0398)|| 2 (50) = (iP + 2)\u03c3 4 i m=1 ||\u2207 \u03b8m L i (\u0398)|| 2(51)", "formula_coordinates": [24.0, 157.47, 209.82, 383.97, 64.88]}, {"formula_id": "formula_71", "formula_text": "II = i m=1 m \u2207 \u03b8m L i (\u0398) \uf8eb \uf8ed m\u2264i,n\u2264i,m =n m n \uf8f6 \uf8f8 i m=1 m \u2207 \u03b8m L i (\u0398)", "formula_coordinates": [24.0, 143.52, 321.14, 303.62, 33.76]}, {"formula_id": "formula_72", "formula_text": "\u2207 \u03b8m L i (\u0398) m m n n \u2207 \u03b8n L i (\u0398) or \u2207 \u03b8n L i (\u0398) n m n m \u2207 \u03b8m L i (\u0398), both of which have expectation: E \u2207 \u03b8m L i (\u0398) m m n n \u2207 \u03b8n L i (\u0398) = \u2207 \u03b8m L i (\u0398) E m m n n \u03c3 4 I \u2207 \u03b8n L i (\u0398) (52) = \u03c3 4 \u2207 \u03b8n L i (\u0398) \u2207 \u03b8m L i (\u0398)(53)", "formula_coordinates": [24.0, 55.44, 374.58, 486.17, 79.12]}, {"formula_id": "formula_73", "formula_text": "2\u03c3 4 m =n \u2207 \u03b8m L i (\u0398) \u2207 \u03b8n L i (\u0398)(54)", "formula_coordinates": [24.0, 231.83, 516.72, 309.61, 22.21]}, {"formula_id": "formula_74", "formula_text": "E g i v i \u03be i \u03be i v i g i = (iP + 2)\u03c3 4 i m=1 ||\u2207 \u03b8m L i (\u0398)|| 2 + 2\u03c3 4 m\u2264i,n\u2264i,m =n \u2207 \u03b8m L i (\u0398) \u2207 \u03b8n L i (\u0398)(55)", "formula_coordinates": [24.0, 94.29, 586.79, 447.15, 30.55]}, {"formula_id": "formula_75", "formula_text": "\u03be i \u03be j = ( 1 + \u2022 \u2022 \u2022 + i ) ( 1 + \u2022 \u2022 \u2022 + j ) = r m=1 m m + m\u2208{1,...,i},n\u2208{1,...,j},m =n m n(56)", "formula_coordinates": [24.0, 119.35, 682.73, 422.09, 30.94]}, {"formula_id": "formula_76", "formula_text": "g i v i \u03be i \u03be j v j g j = i m=1 m \u2207 m L i (\u0398) r m=1 m m j n=1 n \u2207 n L j (\u0398) I (57) + i m=1 m \u2207 \u03b8m L i (\u0398) \uf8eb \uf8ed m\u2264i,n\u2264j,m =n m n \uf8f6 \uf8f8 j n=1 n \u2207 n L j (\u0398) II (58", "formula_coordinates": [25.0, 111.39, 95.63, 430.05, 103.99]}, {"formula_id": "formula_77", "formula_text": ")", "formula_coordinates": [25.0, 537.29, 160.88, 4.15, 8.64]}, {"formula_id": "formula_78", "formula_text": "\u2207 \u03b8n L i (\u0398) n m m n \u2207 \u03b8n L j (\u0398).", "formula_coordinates": [25.0, 91.96, 231.45, 144.47, 10.62]}, {"formula_id": "formula_79", "formula_text": "E \u2207 \u03b8m L i (\u0398) m m m m \u2207 \u03b8m L j (\u0398) = \u2207 \u03b8m L i (\u0398) E m m m m (P +2)\u03c3 4 I \u2207 \u03b8m L j (\u0398)(59)", "formula_coordinates": [25.0, 120.8, 254.46, 420.65, 24.88]}, {"formula_id": "formula_80", "formula_text": "= (P + 2)\u03c3 4 \u2207 \u03b8m L i (\u0398) \u2207 \u03b8m L j (\u0398)(60)", "formula_coordinates": [25.0, 292.91, 284.79, 248.54, 11.72]}, {"formula_id": "formula_81", "formula_text": "(P + 2)\u03c3 4 r m=1 \u2207 \u03b8m L i (\u0398) \u2207 \u03b8m L j (\u0398)(61)", "formula_coordinates": [25.0, 217.15, 328.08, 324.29, 30.2]}, {"formula_id": "formula_82", "formula_text": "\u2207 \u03b8n L i (\u0398) E n m m n \u2207 \u03b8n L j (\u0398) = \u03c3 4 \u2207 \u03b8n L i (\u0398) \u2207 \u03b8n L j (\u0398)(62)", "formula_coordinates": [25.0, 156.55, 390.91, 384.9, 12.69]}, {"formula_id": "formula_83", "formula_text": "\u03c3 4 r m=1 m\u2264r,n\u2264r,n =m \u2207 \u03b8n L i (\u0398) \u2207 \u03b8n L j (\u0398)(63)", "formula_coordinates": [25.0, 207.27, 436.25, 334.17, 30.55]}, {"formula_id": "formula_84", "formula_text": "E \u2207 \u03b8m L i (\u0398) m m n n \u2207 \u03b8n L j (\u0398) = \u03c3 4 \u2207 \u03b8m L i (\u0398) \u2207 \u03b8n L j (\u0398)(64)", "formula_coordinates": [25.0, 155.65, 506.93, 385.79, 12.69]}, {"formula_id": "formula_85", "formula_text": "E \u2207 \u03b8n L i (\u0398) n m n m \u2207 \u03b8m L j (\u0398) = \u03c3 4 \u2207 \u03b8n L i (\u0398) \u2207 \u03b8m L j (\u0398)(65)", "formula_coordinates": [25.0, 156.3, 552.95, 385.14, 12.69]}, {"formula_id": "formula_86", "formula_text": "2\u03c3 4 m\u2264i,n\u2264j,m =n \u2207 \u03b8m L i (\u0398) \u2207 \u03b8n L j (\u0398)(66)", "formula_coordinates": [25.0, 213.76, 598.98, 327.68, 22.21]}, {"formula_id": "formula_87", "formula_text": "E g i v i \u03be i \u03be j v j g j = (P + 2)\u03c3 4 r m=1 \u2207 \u03b8m L i (\u0398) \u2207 \u03b8m L j (\u0398) + \u03c3 4 r m=1 m,n\u2264r,n =m \u2207 \u03b8n L i (\u0398) \u2207 \u03b8n L j (\u0398) (67) + 2\u03c3 4 m\u2264i,n\u2264j,m =n \u2207 \u03b8m L i (\u0398) \u2207 \u03b8n L j (\u0398)(68)", "formula_coordinates": [25.0, 65.67, 659.71, 475.77, 59.33]}, {"formula_id": "formula_88", "formula_text": "E \u011d PES-A \u011d PES-A = T i=1 \uf8eb \uf8ed (iP + 2) i m=1 ||\u2207 \u03b8m L i (\u0398)|| 2 + 2 m =n \u2207 \u03b8m L i (\u0398) \u2207 \u03b8n L i (\u0398) \uf8f6 \uf8f8 (69", "formula_coordinates": [26.0, 91.46, 97.89, 445.83, 33.76]}, {"formula_id": "formula_89", "formula_text": ")", "formula_coordinates": [26.0, 537.29, 111.83, 4.15, 8.64]}, {"formula_id": "formula_90", "formula_text": "+ i =j (P + 2) r m=1 \u2207 \u03b8m L i (\u0398) \u2207 \u03b8m L j (\u0398) + r m=1 n =m \u2207 \u03b8n L i (\u0398) \u2207 \u03b8n L j (\u0398)(70)", "formula_coordinates": [26.0, 173.27, 137.96, 368.17, 30.55]}, {"formula_id": "formula_91", "formula_text": "+ 2 m =n \u2207 \u03b8m L i (\u0398) \u2207 \u03b8n L j (\u0398)(71)", "formula_coordinates": [26.0, 193.19, 184.12, 348.25, 20.14]}, {"formula_id": "formula_92", "formula_text": "E \u011d PES-A E \u011d PES-A = \u2207 \u03b8 L(\u0398) \u2207 \u03b8 L(\u0398)(72)", "formula_coordinates": [26.0, 143.97, 253.57, 397.47, 11.98]}, {"formula_id": "formula_93", "formula_text": "= T t=1 t \u03c4 =1 \u2207 \u03b8\u03c4 L t (\u0398) T t=1 t \u03c4 =1 \u2207 \u03b8\u03c4 L t (\u0398)(73)", "formula_coordinates": [26.0, 250.52, 276.55, 290.92, 30.2]}, {"formula_id": "formula_94", "formula_text": "M = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 \u2207 \u03b81 L 1 \u2207 \u03b81 L 2 \u2207 \u03b81 L 3 \u2022 \u2022 \u2022 \u2207 \u03b81 L T \u2207 \u03b82 L 1 \u2207 \u03b82 L 2 \u2207 \u03b82 L 3 \u2022 \u2022 \u2022 \u2207 \u03b82 L T \u2207 \u03b83 L 1 \u2207 \u03b83 L 2 \u2207 \u03b83 L 3 \u2022 \u2022 \u2022 \u2207 \u03b83 L T . . . . . . . . . . . . . . . \u2207 \u03b8 T L 1 \u2207 \u03b8 T L 2 \u2207 \u03b8 T L 3 \u2022 \u2022 \u2022 \u2207 \u03b8 T L T \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 \u2207 \u03b81 L 1 \u2207 \u03b81 L 2 \u2207 \u03b81 L 3 \u2022 \u2022 \u2022 \u2207 \u03b81 L T 0 \u2207 \u03b82 L 2 \u2207 \u03b82 L 3 \u2022 \u2022 \u2022 \u2207 \u03b82 L T 0 0 \u2207 \u03b83 L 3 \u2022 \u2022 \u2022 \u2207 \u03b83 L T . . . . . . . . . . . . . . . 0 0 0 \u2022 \u2022 \u2022 \u2207 \u03b8 T L T \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb (74", "formula_coordinates": [26.0, 94.89, 378.09, 442.41, 65.93]}, {"formula_id": "formula_95", "formula_text": ")", "formula_coordinates": [26.0, 537.29, 406.98, 4.15, 8.64]}, {"formula_id": "formula_96", "formula_text": "L by g = \u2207 \u03b8 L(\u0398) = T t=1 \u2207 \u03b8 L t (\u0398) = T t=1 g t . (Note that \u2207 \u03b8 L t (\u0398) = $ $ $ \u2207 \u03b81 L t + $ $ $ \u2207 \u03b82 L t + \u2022 \u2022 \u2022 + \u2207 \u03b8t L t = \u2207 \u03b8t L t due to the diagonal structure.)", "formula_coordinates": [26.0, 55.44, 550.22, 487.93, 24.58]}, {"formula_id": "formula_97", "formula_text": "E \u011d PES-A \u011d PES-A = T i=1 \uf8eb \uf8ed (iP + 2) i m=1", "formula_coordinates": [26.0, 75.49, 604.94, 172.58, 33.53]}, {"formula_id": "formula_98", "formula_text": "2 m\u2264i,n\u2264j,m =n \u2207 \u03b8m L i (\u0398) \u2207 \u03b8n L i (\u0398) \uf8f6 \uf8f8 (75", "formula_coordinates": [26.0, 323.09, 604.94, 214.2, 33.76]}, {"formula_id": "formula_99", "formula_text": ")", "formula_coordinates": [26.0, 537.29, 618.87, 4.15, 8.64]}, {"formula_id": "formula_100", "formula_text": "+ i =j (P + 2) $ $ $ $ $ $ $ $ $ $ $ $ $ r m=1 \u2207 \u03b8m L i (\u0398) \u2207 \u03b8m L j (\u0398) + $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ r m=1 n\u2264r,n =m \u2207 \u03b8n L i (\u0398) \u2207 \u03b8n L j (\u0398) (76) + 2 m\u2264i,n\u2264j,m =n \u2207 \u03b8m L i (\u0398) \u2207 \u03b8n L j (\u0398)(77)", "formula_coordinates": [26.0, 157.3, 645.61, 384.14, 68.45]}, {"formula_id": "formula_101", "formula_text": "E \u011d PES-A \u011d PES-A = T i=1 (iP + 2) i m=1 ||\u2207 \u03b8m L i (\u0398)|| 2 + 2 i =j m\u2264i,n\u2264j,m =n \u2207 \u03b8m L i (\u0398) \u2207 \u03b8n L j (\u0398)(78)", "formula_coordinates": [27.0, 75.37, 89.89, 466.07, 30.55]}, {"formula_id": "formula_102", "formula_text": "= T i=1 (iP + 2) ||\u2207 \u03b8i L i (\u0398)|| 2 + 2 i =j \u2207 \u03b8i L i (\u0398) \u2207 \u03b8j L j (\u0398)(79)", "formula_coordinates": [27.0, 157.73, 126.43, 383.71, 30.55]}, {"formula_id": "formula_103", "formula_text": "E[\u011d PES-A ] E[\u011d PES-A ] = T t=1 t \u03c4 =1 \u2207 \u03b8\u03c4 L t (\u0398) T t=1 t \u03c4 =1 \u2207 \u03b8\u03c4 L t (\u0398)(80)", "formula_coordinates": [27.0, 153.15, 203.88, 388.29, 30.2]}, {"formula_id": "formula_104", "formula_text": "= T t=1 \u2207 \u03b8t L t (\u0398) T t=1 \u2207 \u03b8t L t (\u0398)(81)", "formula_coordinates": [27.0, 238.34, 241.09, 303.11, 30.2]}, {"formula_id": "formula_105", "formula_text": "= T t=1 ||\u2207 \u03b8t L t (\u0398)|| 2 + i =j \u2207 \u03b8i L i (\u0398) \u2207 \u03b8j L j (\u0398)(82)", "formula_coordinates": [27.0, 238.34, 275.92, 303.11, 30.55]}, {"formula_id": "formula_106", "formula_text": "tr(Var(\u011d PES-A )) = E \u011d PES-A \u011d PES-A \u2212 E[\u011d PES-A ] E[\u011d PES-A ](83)", "formula_coordinates": [27.0, 142.54, 338.92, 398.9, 11.62]}, {"formula_id": "formula_107", "formula_text": "= T i=1 (iP + 2) ||\u2207 \u03b8i L i (\u0398)|| 2 + 2 i =j \u2207 \u03b8i L i (\u0398) \u2207 \u03b8j L j (\u0398)(84)", "formula_coordinates": [27.0, 206.39, 359.52, 335.05, 30.55]}, {"formula_id": "formula_108", "formula_text": "\u2212 T i=1 ||\u2207 \u03b8t L t (\u0398)|| 2 \u2212 i =j \u2207 \u03b8i L i (\u0398) \u2207 \u03b8j L j (\u0398)(85)", "formula_coordinates": [27.0, 225.76, 396.05, 315.68, 30.55]}, {"formula_id": "formula_109", "formula_text": "= T i=1 (iP + 1) ||\u2207 \u03b8i L i (\u0398)|| 2 + i =j \u2207 \u03b8i L i (\u0398) \u2207 \u03b8j L j (\u0398)(86)", "formula_coordinates": [27.0, 206.39, 435.98, 335.05, 30.55]}, {"formula_id": "formula_110", "formula_text": "g t = 1 T g (87", "formula_coordinates": [27.0, 253.17, 523.96, 284.12, 22.31]}, {"formula_id": "formula_111", "formula_text": ")", "formula_coordinates": [27.0, 537.29, 531.02, 4.15, 8.64]}, {"formula_id": "formula_112", "formula_text": "||g t || 2 = 1 T g 2 = 1 T 2 ||g|| 2 (88) So, T t=1 ||g t || 2 (tP + 1) + i =j g i g j = T t=1 1 T 2 ||g|| 2 (tP + 1) + i\u2264T,j\u2264T,i =j 1 T 2 ||g|| 2 (89) = 1 T 2 ||g|| 2 T + P T (T + 1) 2 + 1 T 2 ||g|| 2 (T 2 \u2212 T ) (90) = 1 T 2 ||g|| 2 T 2 + P T 2 + P T 2 (91) = ||g|| 2 P 2T + P 2 + 1 (92)", "formula_coordinates": [27.0, 55.44, 548.06, 486.0, 171.29]}, {"formula_id": "formula_113", "formula_text": "E ||g|| 2 = T E ||g t || 2 (93) E ||g t || 2 = 1 T E ||g|| 2 (94) E ||g t || 2 = 1 T E ||g|| 2 (95) Thus, T t=1 ||g t || 2 (tP + 1) + i =j g i g j = T t=1 1 T ||g|| 2 (tP + 1) + i =j 1 T ||g|| 2 (96) = 1 T ||g|| 2 T + P T (T + 1) 2 + 1 T ||g|| 2 T 2 \u2212 T (97) = 1 T ||g|| 2 T + T 2 \u2212 T + P T (T + 1) 2 (98) = 1 T ||g|| 2 T 2 + P T (T + 1) 2 (99) = ||g|| 2 T + P (T + 1) 2 (100) = ||g|| 2 P T 2 + P 2 + T (101) G.1.2", "formula_coordinates": [28.0, 55.13, 90.63, 486.31, 286.54]}, {"formula_id": "formula_114", "formula_text": "h = 2 T (T + 1) g (102", "formula_coordinates": [28.0, 264.72, 428.36, 272.4, 22.31]}, {"formula_id": "formula_115", "formula_text": ")", "formula_coordinates": [28.0, 537.12, 435.42, 4.32, 8.64]}, {"formula_id": "formula_116", "formula_text": "E \u011d PES-A \u011d PES-A = T i=1 \uf8eb \uf8ed (iP + 2) i m=1 ||h|| 2 + 2 m\u2264i,n\u2264i,m =n ||h|| 2 \uf8f6 \uf8f8 I (103", "formula_coordinates": [28.0, 84.77, 482.29, 452.35, 52.68]}, {"formula_id": "formula_117", "formula_text": ") + i =j \uf8eb \uf8ed (P + 2) r m=1 ||h|| 2 + r m=1 m\u2264r,n\u2264r,n =m ||h|| 2 + 2 m\u2264i,n\u2264j,m =n ||h|| 2 \uf8f6 \uf8f8 II (104", "formula_coordinates": [28.0, 166.58, 496.23, 374.86, 95.58]}, {"formula_id": "formula_118", "formula_text": ")", "formula_coordinates": [28.0, 537.12, 553.07, 4.32, 8.64]}, {"formula_id": "formula_119", "formula_text": "I = T i=1 (iP + 2)i ||h|| 2 + 2(i 2 \u2212 i) ||h|| 2 (105) = ||h|| 2 T i=1 (P + 2)i 2 (106) = ||h|| 2 (P + 2) T (T + 1)(2T + 1) 6 (107", "formula_coordinates": [28.0, 208.33, 625.11, 333.11, 91.88]}, {"formula_id": "formula_120", "formula_text": ")", "formula_coordinates": [28.0, 537.12, 701.74, 4.32, 8.64]}, {"formula_id": "formula_121", "formula_text": "II = i =j (P + 2) r m=1 ||h|| 2 (P +2)r||h|| 2 + r m=1 n =m ||h|| 2 r(r\u22121)||h|| 2 + 2 m =n ||h|| 2 2(ij\u2212r)||h|| 2 (108) = ||h|| 2 i\u2264T,j\u2264T,i =j (P r + r 2 \u2212 r + 2ij) (109) = 2 ||h|| 2 T i=1 i\u22121 j=1 (P \u2212 1)j + j 2 + 2ij (110) = 2 ||h|| 2 \uf8eb \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ed T i=1 i\u22121 j=1 (P \u2212 1)j a + T i=1 i\u22121 j=1 j 2 b + T i=1 i\u22121 j=1 2ij c \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f8 (111)", "formula_coordinates": [29.0, 169.41, 111.02, 372.03, 183.77]}, {"formula_id": "formula_122", "formula_text": "T i=1 i\u22121 j=1", "formula_coordinates": [29.0, 178.22, 394.11, 29.4, 30.32]}, {"formula_id": "formula_123", "formula_text": "T i=1 i\u22121 j=1 j (112) = (P \u2212 1) T i=1 i(i \u2212 1) 2 (113) = (P \u2212 1) 2 T (T + 1)(2T + 1) 6 \u2212 T (T + 1) 2 (114) b = T i=1 i\u22121 j=1 j 2 = T i=1 i(i \u2212 1)(2i \u2212 1) 6 (115) = T i=1 1 6 2i 3 \u2212 3i 2 + i (116) = 1 3 T i=1 i 3 \u2212 1 2 T i=1 i 2 + 1 6 T i=1 i (117) = 1 3 T 2 (T + 1) 2 4 \u2212 1 2 T (T + 1)(2T + 1) 6 + 1 6 T (T + 1) 2 (118) = 1 12 T 2 (T + 1) 2 \u2212 T (T + 1)(2T + 1) + T (T + 1) (119) c = T i=1 i\u22121 j=1 2ij = T i=1 i i\u22121 j=1 j (120) = T i=1 i i(i \u2212 1) 2 (121) = T i=1 1 2 i(i 2 \u2212 i) (122) = 1 2 T i=1 i 3 \u2212 i 2 (123) = 1 2 T 2 (T + 1) 2 4 \u2212 T (T + 1)(2T + 1) 6 (124", "formula_coordinates": [29.0, 153.23, 394.11, 388.21, 322.88]}, {"formula_id": "formula_124", "formula_text": ")", "formula_coordinates": [30.0, 537.12, 237.89, 4.32, 8.64]}, {"formula_id": "formula_125", "formula_text": "||h|| 2 P T (T + 1)(2T + 1) 3 \u2212 P T (T + 1) 2 + 5T 2 (T + 1) 2 12 \u2212 T (T + 1)(2T + 1) 6 + 2T (T + 1) 3 (125)", "formula_coordinates": [30.0, 85.44, 291.2, 456.0, 23.89]}, {"formula_id": "formula_126", "formula_text": "||h|| 2 5 12 T 4 + 2 3 P T 3 + 1 2 P T 2 \u2212 1 6 P T + 1 2 T 3 + 7 12 T 2 + 1 2 T (126", "formula_coordinates": [30.0, 163.8, 348.33, 373.32, 22.31]}, {"formula_id": "formula_127", "formula_text": ")", "formula_coordinates": [30.0, 537.12, 355.39, 4.32, 8.64]}, {"formula_id": "formula_128", "formula_text": "||h|| 2 = 2 T (T + 1) 2 ||g|| 2 (127)", "formula_coordinates": [30.0, 238.78, 411.74, 302.66, 25.51]}, {"formula_id": "formula_129", "formula_text": "||g|| 2 O (1) + O P T + O P T 2 \u2212 O P T 3 + O 1 T + O 1 T 2 + O 1 T 3 (128)", "formula_coordinates": [30.0, 120.16, 472.15, 421.28, 22.31]}, {"formula_id": "formula_130", "formula_text": "E ||h|| 2 = 2 T (T + 1) E ||g|| 2 (129)", "formula_coordinates": [30.0, 227.15, 533.16, 314.29, 22.31]}, {"formula_id": "formula_131", "formula_text": "||g|| 2 O T 2 + O (P T ) + O (P ) \u2212 O P T + O (T ) + O (1) + O 1 T (130)", "formula_coordinates": [30.0, 140.61, 596.2, 400.83, 22.31]}, {"formula_id": "formula_132", "formula_text": "\u2202L t (\u0398) \u2202\u03b8 \u2248 1 \u03c3 2 E \uf8ee \uf8f0 \uf8eb \uf8ed \u03c4 \u2264t \u03c4 \uf8f6 \uf8f8 L t (\u0398 + ) \uf8f9 \uf8fb (131) = 1 \u03c3 2 E \u03c4 <t \u03c4 L t (\u0398 + ) + 1 \u03c3 2 E [ t L t (\u0398 + )] (132) = 1 \u03c3 2 E \u03c4 <t \u03c4 L t (\u0398 + ) + \u2202L t (\u0398) \u2202\u03b8 t \u2261p t (133) = 1 \u03c3 2 E \u03c4 <t \u03c4 L t (\u0398 + ) + p t \u2212 1 \u03c3 2 E \u03c4 <t \u03c4 t p t =0 (134) = 1 \u03c3 2 E \u03c4 <t \u03c4 (L t (\u0398 + ) \u2212 t p t ) + p t (135)", "formula_coordinates": [31.0, 146.72, 392.25, 394.72, 197.31]}, {"formula_id": "formula_133", "formula_text": "L = t L t .", "formula_coordinates": [33.0, 55.44, 70.22, 48.53, 11.15]}, {"formula_id": "formula_134", "formula_text": "E c\u2208C c = E \uf8ee \uf8f0 w\u2208S,\u03b8\u227a D w \u2202 \u2202\u03b8 log p(w|DEPS w ) Q w + c\u2208C,\u03b8\u227a D c \u2202 \u2202\u03b8 c(DEPS c ) \uf8f9 \uf8fb (136)", "formula_coordinates": [33.0, 137.27, 346.84, 404.17, 34.31]}, {"formula_id": "formula_135", "formula_text": "\u2202 \u2202\u03b8 E T t=1 L t = E T t=1 \u2202 \u2202\u03b8 log p(\u03b8 t |\u03b8) Q \u03b8t (137", "formula_coordinates": [33.0, 200.36, 459.25, 685.23, 30.2]}, {"formula_id": "formula_136", "formula_text": ")", "formula_coordinates": [33.0, 885.59, 469.98, 91.43, 8.64]}, {"formula_id": "formula_137", "formula_text": "log p(\u03b8 t | \u03b8) = log N (\u03b8 t | \u03b8, \u03c3 2 ) = log 1 \u221a 2\u03c0\u03c3 \u2212 1 2\u03c3 2 (\u03b8 t \u2212 \u03b8) 2 (138) Then, \u2202 \u2202\u03b8 log p(\u03b8 t | \u03b8) = \u2212 1 2\u03c3 2 \u2022 2(\u03b8 t \u2212 \u03b8) \u2022 (\u22121) (139) = 1 \u03c3 2 (\u03b8 t \u2212 \u03b8) (140) = 1 \u03c3 2 (\u03b8 + t \u2212 \u03b8) (141) = 1 \u03c3 2 t (142", "formula_coordinates": [33.0, 55.13, 550.01, 486.31, 166.99]}, {"formula_id": "formula_138", "formula_text": ")", "formula_coordinates": [33.0, 537.12, 701.74, 4.32, 8.64]}, {"formula_id": "formula_139", "formula_text": "\u2202 \u2202\u03b8 E T t=1 L t = E T t=1 1 \u03c3 2 tQ\u03b8t (143) = 1 \u03c3 2 E [ 1 (L 1 + L 2 + \u2022 \u2022 \u2022 + L T ) + 2 (L 2 + L 3 + \u2022 \u2022 \u2022 + L T ) + \u2022 \u2022 \u2022 + T L T ] (144) = 1 \u03c3 2 E[ 1 L 1 + ( 1 + 2 )L 2 + ( 1 + 2 + 3 )L 3 + \u2022 \u2022 \u2022 + ( 1 + \u2022 \u2022 \u2022 + T )L T ] (145) = 1 \u03c3 2 E T t=1 t \u03c4 =1 \u03c4 L t (146)", "formula_coordinates": [34.0, 114.16, 91.72, 427.28, 112.99]}, {"formula_id": "formula_140", "formula_text": "s t = f (s t\u22121 , x t , \u03b8)(147", "formula_coordinates": [34.0, 258.63, 418.14, 278.5, 9.68]}], "doi": ""}