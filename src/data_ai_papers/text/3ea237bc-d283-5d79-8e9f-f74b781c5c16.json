{"title": "Robust Average-Reward Markov Decision Processes", "authors": "Yue Wang; Alvaro Velasquez; George Atia; Ashley Prater-Bennette; Shaofeng Zou", "pub_date": "2023-03-01", "abstract": "In robust Markov decision processes (MDPs), the uncertainty in the transition kernel is addressed by finding a policy that optimizes the worst-case performance over an uncertainty set of MDPs. While much of the literature has focused on discounted MDPs, robust average-reward MDPs remain largely unexplored. In this paper, we focus on robust average-reward MDPs, where the goal is to find a policy that optimizes the worst-case average reward over an uncertainty set. We first take an approach that approximates average-reward MDPs using discounted MDPs. We prove that the robust discounted value function converges to the robust average-reward as the discount factor \u03b3 goes to 1, and moreover, when \u03b3 is large, any optimal policy of the robust discounted MDP is also an optimal policy of the robust average-reward. We further design a robust dynamic programming approach, and theoretically characterize its convergence to the optimum. Then, we investigate robust average-reward MDPs directly without using discounted MDPs as an intermediate step. We derive the robust Bellman equation for robust average-reward MDPs, prove that the optimal policy can be derived from its solution, and further design a robust relative value iteration algorithm that provably find its solution, or equivalently, the optimal robust policy.", "sections": [{"heading": "Introduction", "text": "A Markov decision process (MDP) is an effective mathematical tool for sequential decision-making in stochastic environments (Derman 1970;Puterman 1994). Solving an MDP problem entails finding an optimal policy that maximizes a cumulative reward according to a given criterion. However, in practice there could exist a mismatch between the assumed MDP model and the underlying environment due to various factors, such as non-stationarity of the environment, modeling error, exogenous perturbation, partial observability, and adversarial attacks. The ensuing model mismatch could result in solution policies with poor performance.\nThis challenge spurred noteworthy efforts on developing and analyzing a framework of robust MDPs e.g., (Bagnell, Ng, and Schneider 2001;Nilim and El Ghaoui 2004;Iyengar 2005). Rather than adopting a fixed MDP model, in the robust MDP setting, one seeks to optimize the worst-case performance over an uncertainty set of possible MDP models. The", "publication_ref": ["b8", "b31", "b4", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Challenges and Contributions", "text": "In this paper, we derive characterizations of robust averagereward MDPs with general uncertainty sets, and develop model-based approaches with provable theoretical guarantee. Our approach is fundamentally different from previous work on robust discounted MDPs, robust and non-robust averagereward MDPs. In particular, the key challenges and the main contributions are summarized below.\n\u2022 We characterize the limiting behavior of robust discounted value function as the discount factor \u03b3 \u2192 1.\nFor the standard non-robust setting and for a specific transition kernel, the discounted non-robust value function converges to the average-reward non-robust value function as \u03b3 \u2192 1 (Puterman 1994). However, in the robust setting, we need to consider the worst-case limiting behavior under all possible transition kernels in the uncertainty set. Hence, the previous point-wise convergence result (Puterman 1994) cannot be directly applied. In (Tewari and Bartlett 2007), a finite interval uncertainty set is studied, where due to its special structure, the number of possible worst-case transition kernels of robust discounted MDPs is finite, and hence the order of min (over transition kernel) and lim \u03b3\u21921 can be exchanged, and therefore, the robust discounted value function converges to the robust average-reward value function. This result, however, does not hold for general uncertainty sets investigated in this paper. We first prove the uniform convergence of discounted non-robust value function to average-reward w.r.t. the transition kernels and policies.\nBased on this uniform convergence, we show the convergence of the robust discounted value function to the robust average-reward. This uniform convergence result is the first in the literature and is of key importance to motivate our algorithm design and to guarantee convergence to the optimal robust policy in the average-reward setting. \u2022 We design algorithms for robust policy evaluation and optimal control based on the limit method. Based on the uniform convergence, we then use robust discounted MDPs to approximate robust average-reward MDPs. We show that when \u03b3 is large, any optimal policy of the robust discounted MDP is also an optimal policy of the robust average-reward, and hence solves the robust optimal control problem in the average reward setting. This result is similar to the Blackwell optimality (Blackwell 1962;Hordijk and Yushkevich 2002) for the non-robust setting, however, our proof is fundamentally different. Technically, the proof in (Blackwell 1962;Hordijk and Yushkevich 2002) is based on the fact that the difference between the discounted value functions of two policies is a rational function of the discount factor, which has a finite number of zeros. However, in the robust setting with a general uncertainty set, the difference is no longer a rational function due to the min over the transition kernel. We construct a novel proof based on the limiting behavior of robust discounted MDPs, and show that the (optimal) robust discounted value function converges to the (optimal) robust average-reward as \u03b3 \u2192 1. Motivated by these insights, we then design our algorithms by applying a sequence of robust discounted Bellman operators while increasing the discount factor at a certain rate. We prove that our method can (i) evaluate the robust average-reward for a given policy and; (ii) find the optimal robust value function and, in turn, the optimal robust policy for general uncertainty sets. \u2022 We design a robust relative value iteration method without using the discounted MDPs as an intermediate step. We further pursue a direct approach that solves the robust average-reward MDPs without using the limit method, i.e., without using discounted MDPs as an intermediate step. We derive a robust Bellman equation for robust average-reward MDPs, and show that the pair of robust relative value function and robust average-reward is a solution to the robust Bellman equation under the average-reward setting. We further prove that if we can find any solution to the robust Bellman equation, then the optimal policy can be derived by a greedy approach. The problem hence can be equivalently solved by solving the robust Bellman equation. We then design a robust value iteration method which provably converges to the solution of the robust Bellman equation, i.e., solve the optimal policy for the robust average-reward MDP problem.", "publication_ref": ["b31", "b31", "b43", "b6", "b14", "b6", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Robust discounted MDPs. Model-based methods for robust discounted MDPs were studied in (Iyengar 2005;Nilim and El Ghaoui 2004;Bagnell, Ng, and Schneider 2001;Satia and Lave Jr 1973;Wiesemann, Kuhn, and Rustem 2013;Lim and Autef 2019;Xu and Mannor 2010;Yu and Xu 2015;Lim, Xu, and Mannor 2013;Tamar, Mannor, and Xu 2014), where the uncertainty set is assumed to be known, and the problem can be solved using robust dynamic programming. Later, the studies were generalized to the model-free setting where stochas-tic samples from the centroid MDP of the uncertainty set are available in an online fashion (Roy, Xu, and Pokutta 2017;Badrinath and Kalathil 2021;Zou 2021, 2022;Tessler, Efroni, and Mannor 2019) and an offline fashion (Zhou et al. 2021;Yang, Zhang, and Zhang 2021;Panaganti and Kalathil 2021;Goyal and Grand-Clement 2018;Kaufman and Schaefer 2013;Wiesemann 2018, 2021;Si et al. 2020). There are also empirical studies on robust RL, e.g., (Vinitsky et al. 2020;Pinto et al. 2017;Abdullah et al. 2019;Hou et al. 2020;Rajeswaran et al. 2017;Huang et al. 2017;Kos and Song 2017;Lin et al. 2017;Pattanaik et al. 2018;Mandlekar et al. 2017). For discounted MDPs, the robust Bellman operator is a contraction, based on which robust dynamic programming and value-based methods can be designed. In this paper, we focus on robust average-reward MDPs. However, the robust Bellman operator for averagereward MDPs is not a contraction, and its fixed point may not be unique. Moreover, the average-reward setting depends on the limiting behavior of the underlying stochastic process, which is thus more intricate.\nRobust average-reward MDPs. Studies on robust averagereward MDPs are quite limited in the literature. Robust average-reward MDPs under a specific finite interval uncertainty set was studied in (Tewari and Bartlett 2007), where the authors showed the existence of a Blackwell optimal policy, i.e., there exists some \u03b4 \u2208 [0, 1), such that the optimal robust policy exists and remains unchanged for any discount factor \u03b3 \u2208 [\u03b4, 1). However, this result depends on the structure of the uncertainty set. For general uncertainty sets, the existence of a Blackwell optimal policy may not be guaranteed. More recently, (Lim, Xu, and Mannor 2013) designed a model-free algorithm for a specific 1 -norm uncertainty set and characterized its regret bound. However, their method also relies on the structure of the 1 -norm uncertainty set, and may not be generalizable to other types of uncertainty sets. In this paper, our results can be applied to various types of uncertainty sets, and thus is more general.", "publication_ref": ["b19", "b4", "b37", "b48", "b24", "b49", "b51", "b25", "b41", "b34", "b3", "b42", "b54", "b50", "b28", "b11", "b20", "b38", "b44", "b30", "b0", "b15", "b33", "b17", "b22", "b26", "b29", "b27", "b43", "b25"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries and Problem Model", "text": "In this section, we introduce some preliminaries on discounted MDPs, average-reward MDPs, and robust MDPs.\nDiscounted MDPs. A discounted MDP (S, A, P, r, \u03b3) is specified by: a state space S, an action space A, a transition kernel P = {p a s \u2208 \u2206(S), a \u2208 A, s \u2208 S} 1 , where p a s is the distribution of the next state over S upon taking action a in state s (with p a s,s denoting the probability of transitioning to s ), a reward function r : S \u00d7 A \u2192 [0, 1], and a discount factor \u03b3 \u2208 [0, 1). At each time step t, the agent at state s t takes an action a t , the environment then transitions to the next state s t+1 according to p at st , and produces a reward signal r(s t , a t ) \u2208 [0, 1] to the agent. In this paper, we also write r t = r(s t , a t ) for convenience.\nA stationary policy \u03c0 : S \u2192 \u2206(A) is a distribution over A for any given state s, and the agent takes action a at state s with probability \u03c0(a|s). The discounted value function of a stationary policy \u03c0 starting from s \u2208 S is defined as the 1 \u2206(S): the (|S| \u2212 1)-dimensional probability simplex on S.\nexpected discounted cumulative reward by following policy \u03c0:\nV \u03c0 P,\u03b3 (s) E \u03c0,P [ \u221e t=0 \u03b3 t r t |S 0 = s].\nAverage-Reward MDPs. Different from discounted MDPs, average-reward MDPs do not discount the reward over time, and consider the behavior of the underlying Markov process under the steady-state distribution. More specifically, under a specific transition kernel P, the average-reward of a policy \u03c0 starting from s \u2208 S is defined as\ng \u03c0 P (s) lim n\u2192\u221e E \u03c0,P 1 n n\u22121 t=0 r t |S 0 = s ,(1)\nwhich we also refer to in this paper as the average-reward value function for convenience. The average-reward value function can also be equivalently written as follows:\ng \u03c0 P = lim n\u2192\u221e 1 n n\u22121 t=0 (P \u03c0 ) t r \u03c0 P \u03c0\n* r \u03c0 , where (P \u03c0 ) s,s a \u03c0(a|s)p a s,s and r \u03c0 (s) a \u03c0(a|s)r(s, a) are the transition matrix and reward function induced by \u03c0, and P \u03c0 * lim n\u2192\u221e 1 n n\u22121 t=0 (P \u03c0 ) t is the limit matrix of P \u03c0 .\nIn the average-reward setting, we also define the following relative value function\nV \u03c0 P (s) E \u03c0,P \u221e t=0 (r t \u2212 g \u03c0 P )|S 0 = s ,(2)\nwhich is the cumulative difference over time between the reward and the average value g \u03c0 P . It has been shown that (Puterman 1994)\n: V \u03c0 P = H \u03c0 P r \u03c0 , where H \u03c0 P (I \u2212 P \u03c0 + P \u03c0 * ) \u22121 (I \u2212 P \u03c0 * )\nis defined as the deviation matrix of P \u03c0 . The relationship between the average-reward and the relative value functions can be characterized by the following Bellman equation (Puterman 1994):\nV \u03c0 P (s) = E \u03c0 r(s, A) \u2212 g \u03c0 P (s) + s \u2208S p A s,s V \u03c0 P (s ) . (3)\nRobust discounted and average-reward MDPs. For robust MDPs, the transition kernel is not fixed but belongs to some uncertainty set P. After the agent takes an action, the environment transits to the next state according to an arbitrary transition kernel P \u2208 P. In this paper, we focus on the (s, a)rectangular uncertainty set (Nilim and El Ghaoui 2004; Iyengar 2005), i.e., P = s,a P a s , where P a s \u2286 \u2206(S). We note that there are also studies on relaxing the (s, a)-rectangular uncertainty set to s-rectangular uncertainty set, which is not the focus of this paper.\nUnder the robust setting, we consider the worst-case performance over the uncertainty set of MDPs. More specifically, the robust discounted value function of a policy \u03c0 for a discounted MDP is defined as V \u03c0 P,\u03b3 (s) min\n\u03ba\u2208 t\u22650 P E \u03c0,\u03ba \u221e t=0 \u03b3 t r t |S 0 = s , (4\n)\nwhere \u03ba = (P 0 , P 1 ...) \u2208 t\u22650 P.\nIn this paper, we focus on the following worst-case averagereward for a policy \u03c0:\ng \u03c0 P (s) min \u03ba\u2208 t\u22650 P lim n\u2192\u221e E \u03c0,\u03ba 1 n n\u22121 t=0 r t |S 0 = s , (5)\nto which, for convenience, we refer as the robust averagereward value function.\nFor robust discounted MDPs, it has been shown that the robust discounted value function is the unique fixed-point of the robust discounted Bellman operator (Nilim and El Ghaoui 2004;Iyengar 2005;Puterman 1994):\nT \u03c0 V (s) a\u2208A \u03c0(a|s) r(s, a) + \u03b3\u03c3 P a s (V ) ,(6)\nwhere \u03c3 P a s (V ) min p\u2208P a s p V is the support function of V on P a s . Based on the contraction of T \u03c0 , robust dynamic programming approaches, e.g., robust value iteration, can be designed (Nilim and El Ghaoui 2004;Iyengar 2005) (see Appendix for a review of these methods). However, there is no such contraction result for robust average-reward MDPs. In this paper, our goal is to find a policy that optimizes the robust average-reward value function:\nmax \u03c0\u2208\u03a0 g \u03c0 P (s), for any s \u2208 S,(7)\nwhere \u03a0 is the set of all stationary policies, and we denote by g * P (s) max \u03c0 g \u03c0 P (s) the optimal robust average-reward.", "publication_ref": ["b31", "b31", "b19", "b31", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Limit Approach for Robust Average-Reward MDPs", "text": "We first take a limit approach to solve the problem of robust average-reward MDPs in eq. (7). It is known that under the non-robust setting, for any fixed \u03c0 and P, the discounted value function converges to the average-reward value function as the discount factor \u03b3 approaches 1 (Puterman 1994), i.e.,\nlim \u03b3\u21921 (1 \u2212 \u03b3)V \u03c0 P,\u03b3 = g \u03c0 P .(8)\nWe take a similar idea, and show that the same result holds in the robust case: lim \u03b3\u21921 (1 \u2212 \u03b3)V \u03c0 P,\u03b3 = g \u03c0 P under a mild assumption. Based on this result, we further design algorithms (Algorithms 1 and 2) that apply a sequence of robust discounted Bellman operators while increasing the discount factor at a certain rate. We then theoretically prove that our algorithms converge to the optimal solutions.\nIn the following, we first show that the convergence lim \u03b3\u21921 (1 \u2212 \u03b3)V \u03c0 P,\u03b3 = g \u03c0 P is uniform on the set \u03a0 \u00d7 P. In studies of average-reward MDPs, it is usually the case that a certain class of MDPs are considered, e.g., unichain and communicating (Wei et al. 2020;Zhang and Ross 2021;Chen, Jain, and Luo 2022;Wan, Naik, and Sutton 2021). In this paper, we focus on the unichain setting to highlight the major technical novelty to achieve robustness. Assumption 1. For any s \u2208 S, a \u2208 A, the uncertainty set P a s is a compact subset of \u2206(S). And for any \u03c0 \u2208 \u03a0, P \u2208 P, the induced MDP is a unichain.\nThe first part of Assumption 1 amounts to assuming that the uncertainty set is closed. We remark that many standard uncertainty sets satisfy this assumption, e.g., those defined by -contamination (Huber 1965), finite interval (Tewari and Bartlett 2007), total-variation (Rahimian, Bayraksan, and De-Mello 2022) and KL-divergence (Hu and Hong 2013). The unichain assumption is also widely used in studies of average-reward MDPs, e.g., (Puterman 1994;Wan, Naik, and Sutton 2021;Zhang and Ross 2021;Lan 2020;Zhang, Zhang, and Maguluri 2021). Also it is worth noting that under the unichain assumption, the robust average-reward is identical for every starting state, i.e., g \u03c0 P (s 1 ) = g \u03c0 P (s 2 ), \u2200s 1 , s 2 \u2208 S (Bertsekas 2011). Remark 1. The results in this section actually only require the uniform boundedness of H \u03c0 P , \u2200\u03c0 \u2208 \u03a0, P \u2208 P (Lemma 2 in Appendix). Assumption 1 is one sufficient condition.\nIn (Puterman 1994), the convergence lim \u03b3\u21921 (1 \u2212 \u03b3)V \u03c0 P,\u03b3 = g \u03c0 P for a fixed policy \u03c0 and a fixed transition kernel P (non-robust setting) is point-wise. However, such point-wise convergence does not provide any convergence guarantee on the robust discounted value function, as the robust value function measures the worst-case performance over the uncertainty set and the order of lim and min may not be exchanged in general. In the following theorem, we prove the uniform convergence of the discounted value function under the foregoing assumption. Theorem 1 (Uniform convergence). Under Assumption 1, the discounted value function converges uniformly to the average-reward value function on \u03a0 \u00d7 P as \u03b3 \u2192 1, i.e., lim \u03b3\u21921\n(1 \u2212 \u03b3)V \u03c0 P,\u03b3 = g \u03c0 P , uniformly.\nWith uniform convergence in Theorem 1, the order of the limit \u03b3 \u2192 1 and min P can be interchanged, then the following convergence of the robust discounted value function can be established. Theorem 2. The robust discounted value function in eq. (4) converges to the robust average-reward uniformly on \u03a0:\nlim \u03b3\u21921 (1 \u2212 \u03b3)V \u03c0 P,\u03b3 = g \u03c0 P uniformly.(10)\nWe note that a similar convergence result is shown in (Tewari and Bartlett 2007), but only for a special uncertainty set of finite interval. Our Theorem 2 holds for general compact uncertainty sets. Moreover, it is worth highlighting that our proof technique is fundamentally different from the one in (Tewari and Bartlett 2007). Specifically, under the finite interval uncertainty set, the worst-case transition kernels are from a finite set, i.e., V \u03c0 P,\u03b3 = min P\u2208M V \u03c0 P,\u03b3 for a finite set M \u2286 P. This hence implies the interchangeability of lim and min. However, for general uncertainty sets, the number of worst-case transition kernels may not be finite. We demonstrate the interchangeability via our uniform convergence result in Theorem 1.\nThe previous two convergence results play a fundamental role in limit method for robust average-reward MDPs, and are of key importance to motivate the design of the following two algorithms, the basic idea of which is to apply a sequence of robust discounted Bellman operators on an arbitrary initialization while increasing the discount factor at a certain rate.\nWe first consider the robust policy evaluation problem, which aims to estimate the robust average-reward g \u03c0 P for a fixed policy \u03c0. This problem for robust discounted MDPs is well studied in the literature, however, results for robust average-reward MDPs are quite limited except for the one in (Tewari and Bartlett 2007) for a specific finite interval uncertainty set. We present the a robust value iteration (robust VI) algorithm for evaluating the robust average-reward with general uncertainty sets in Algorithm 1.\nAlgorithm 1: Robust VI: Policy Evaluation Input: \u03c0, V 0 (s) = 0, \u2200s, T 1: for t = 0, 1, ..., T \u2212 1 do 2: \u03b3 t \u2190 t+1 t+2 3:\nfor all s \u2208 S do\n4: V t+1 (s) \u2190 E \u03c0 [(1 \u2212 \u03b3 t )r(s, A) + \u03b3 t \u03c3 P A s (V t )]\n5:\nend for 6: end for 7: return V T At each time step t, the discount factor \u03b3 t is set to t+1 t+2 , which converges to 1 as t \u2192 \u221e. Subsequently, a robust Bellman operator w.r.t discount factor \u03b3 t is applied on the current estimate V t of the robust discounted value function (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t . As the discount factor approaches 1, the estimated robust discounted value function converges to the robust average-reward g \u03c0 P by Theorem 2. The following result shows that the output of Algorithm 1 converges to the robust average-reward. Theorem 3. Algorithm 1 converges to robust average reward, i.e., lim T \u2192\u221e V T = g \u03c0 P . Besides the robust policy evaluation problem, it is also of great practical importance to find an optimal policy that maximizes the worst-case average-reward, i.e., to solve eq. ( 7). Based on a similar idea as the one of Algorithm 1, we extend our limit approach to solve the robust optimal control problem in Algorithm 2.\nAlgorithm 2: Robust VI: Optimal Control Input: V 0 (s) = 0, \u2200s, T 1: for t = 0, 1, ..., T \u2212 1 do 2: \u03b3 t \u2190 t+1 t+2 3: for all s \u2208 S do 4: V t+1 (s) \u2190 max a\u2208A (1 \u2212 \u03b3 t )r(s, a) + \u03b3 t \u03c3 P a s (V t ) 5:\nend for 6: end for 7: for s \u2208 S do\n8: \u03c0 T (s) \u2190 arg max a\u2208A (1 \u2212 \u03b3 t )r(s, a) + \u03b3 t \u03c3 P a\ns (V T ) 9: end for 10: return V T , \u03c0 T Similar to Algorithm 1, at each time step, the discount factor \u03b3 t is set to be closer to 1, and a one-step robust discounted Bellman operator (for optimal control) w.r.t. \u03b3 t is applied to the current estimate V t . The following theorem establishes that V T in Algorithm 2 converges to the optimal robust value function, hence can find the optimal robust policy.\nTheorem 4. The output V T in Algorithm 2 converges to the optimal robust average-reward g * P : V T \u2192 g * P as T \u2192 \u221e.\nAs discussed in (Blackwell 1962;Hordijk and Yushkevich 2002), the average-reward criterion is insensitive and under selective since it is only interested in the performance under the steady-state distribution. For example, two policies providing rewards: 100 + 0 + 0 + \u2022 \u2022 \u2022 and 0 + 0 + 0 + \u2022 \u2022 \u2022 are equally good/bad. Towards this issue, for the non-robust setting, a more sensitive term of optimality was introduced by Blackwell (Blackwell 1962). More specifically, a policy is said to be Blackwell optimal if it optimizes the discounted value function for all discount factor \u03b3 \u2208 (\u03b4, 1) for some \u03b4 \u2208 (0, 1). Together with eq. ( 8), the optimal policy obtained by taking \u03b3 \u2192 1 is optimal not only for the average-reward criterion, but also for the discounted criterion with large \u03b3. Intuitively, it is optimal under the average-reward setting, and is sensitive to early rewards.\nFollowing a similar idea, we justify that the obtained policy from Algorithm 2 is not only optimal in the robust averagereward setting, but also sensitive to early rewards.\nDenote by \u03a0 * D the set of all the deterministic optimal policies for robust average-reward (proved to exist in Lemma 7), i.e. \u03a0 * D = {\u03c0 \u2208 \u03a0 D : g \u03c0 P = g * P } .\nTheorem 5 (Blackwell optimality). There exists 0 < \u03b4 < 1, such that for any \u03b3 > \u03b4, the deterministic optimal robust policy for robust discounted value function V * P,\u03b3 belongs to \u03a0 * D . Moreover, when \u03a0 * D is a singleton, there exists a unique Blackwell optimal policy. This result implies that using the limit method in this section to find the optimal robust policy for average-reward MDPs has an additional advantage that the policy it finds not only optimizes the average reward in steady state, but also is sensitive to early rewards.\nIt is worth highlighting the distinction of our results from the technique used in the proof of Blackwell optimality (Blackwell 1962). In the non-robust setting, the existence of a stationary Blackwell optimal policy is proved via contradiction, where a difference function of two policies \u03c0 and \u03bd: f \u03c0,\u03bd (\u03b3) V \u03c0 P,\u03b3 \u2212 V \u00b5 P,\u03b3 is used in the proof. It was shown by contradiction that f has infinitely many zeros, which however contradicts with the fact that f is a rational function of \u03b3 with a finite number of zeros. A similar technique was also used in (Tewari and Bartlett 2007) for the finite interval uncertainty set. Specifically, in (Tewari and Bartlett 2007), it was shown that the worst-case transition kernels for any \u03c0, \u03b3 are from a finite set M, hence\nf \u03c0,\u03bd (\u03b3) min P\u2208M V \u03c0 P,\u03b3 \u2212 min P\u2208M V \u00b5 P,\u03b3\ncan also be shown to be a rational function with a finite number of zeroes. For a general uncertainty set P, the difference function f \u03c0,\u03bd (\u03b3), however, may not be rational. This makes the method in (Blackwell 1962;Tewari and Bartlett 2007) inapplicable to our problem.", "publication_ref": ["b53", "b7", "b44", "b18", "b43", "b32", "b16", "b31", "b44", "b53", "b23", "b52", "b31", "b43", "b43", "b43", "b6", "b14", "b6", "b6", "b43", "b43", "b6", "b43"], "figure_ref": [], "table_ref": []}, {"heading": "Direct Approach for Robust Average-Reward MDPs", "text": "The limit approach in Section is based on the uniform convergence of the discounted value function, and uses discounted MDPs to approximate average-reward MDPs. In this section, we develop a direct approach to solving the robust averagereward MDPs that does not adopt discounted MDPs as intermediate steps.\nFor average-reward MDPs, the relative value iteration (RVI) approach (Puterman 1994) is commonly used since it is numerically stable and has convergence guarantee. In the following, we generalize the RVI algorithm to the robust setting, and design the robust RVI algorithm in Algorithm 3.\nWe first generalize the relative value function in eq. ( 2) to the robust relative value function. The robust relative value function measures the difference between the worst-case cumulative reward and the worst-case average-reward for a policy \u03c0. Definition 1. The robust relative value function is defined as\nV \u03c0 P (s) min \u03ba\u2208 t\u22650 P E \u03ba,\u03c0 \u221e t=0 (r t \u2212 g \u03c0 P )|S 0 = s , (11)\nwhere g \u03c0 P is the worst-case average-reward defined in eq. (5). The following theorem presents a robust Bellman equation for robust average-reward MDPs. Theorem 6. For any s and \u03c0, (V \u03c0 P , g \u03c0 P ) is a solution to the following robust Bellman equation:\nV (s) + g = a \u03c0(a|s) r(s, a) + \u03c3 P a s (V ) .(12)\nIt can be seen that the robust Bellman equation for averagereward MDPs has a similar structure to the one for discounted MDPs in eq. ( 6) except for a discount factor. This actually reveals a fundamental difference between the robust Bellman operator of the discounted MDPs and the average-reward ones. For a discounted MDP, its robust Bellman operator is a contraction with constant \u03b3 (Nilim and El Ghaoui 2004; Iyengar 2005), and hence the fixed point is unique. Based on this, the robust value function can be found by recursively applying the robust Bellman operator (see Appendix ). In sharp contrast, in the average-reward setting, the robust Bellman is not necessarily a contraction, and the fixed point may not be unique. Therefore, repeatedly applying the robust Bellman operator in the average-reward setting may not even converge, which underscores that the two problem settings are fundamentally different.\nWe first derive the following equivalent optimality condition for robust average-reward MDPs. Theorem 7. For any (g, V ) that is a solution to\nmax a r(s, a) \u2212 g + \u03c3 P a s (V ) \u2212 V (s) = 0, \u2200s,(13)\ng = g * P . If we further set \u03c0 * (s) = arg max a r(s, a) + \u03c3 P a s (V )(14)\nfor any s \u2208 S, then \u03c0 * is an optimal robust policy.\nTheorem 7 suggests that as long as we find a solution (g, V ) to eq. ( 13), which though may not be unique, then g is the optimal robust average-reward g * P , and the greedy policy \u03c0 * is the optimal policy to our robust average-reward MDP problem in eq. ( 7).\nIn the following, we generalize the RVI approach to the robust setting, and design a robust RVI algorithm in Algorithm 3. We will further show that the output of this algorithm converges to a solution to eq. ( 13), and further the optimal policy could be obtained by eq. ( 14). Here 1 de-Algorithm 3: Robust RVI Input: V 0 , and arbitrary s * \u2208 S\n1: w 0 \u2190 V 0 \u2212 V 0 (s * )1 2: while sp(w t \u2212 w t+1 ) \u2265 do 3:\nfor all s \u2208 S do 4:\nV t+1 (s) \u2190 max a (r(s, a) + \u03c3 P a s (w t ))\n5:\nw t+1 (s) \u2190 V t+1 (s) \u2212 V t+1 (s * ) 6:\nend for 7: end while 8: return w t , V t notes the all-ones vector, and sp denotes the span semi-norm: sp(w) = max s w(s)\u2212min s w(s). Different from Algorithm 2, in Algorithm 3, we do not need to apply the robust discounted Bellman operator. The method directly solves the robust optimal control problem for average-reward robust MDPs.\nTo study the convergence of the robust RVI algorithm, we first make an additional assumption as follows. Assumption 2. There exists a positive integer J such that for any P = {p a s \u2208 \u2206(S)} \u2208 P and any stationary deterministic policy \u03c0, there exists \u03ba > 0 and a state s \u2208 S, such that ((P \u03c0 ) J ) x,s \u2265 \u03ba, \u2200x \u2208 S.\nThis assumption is shown to be equivalent to assuming unichain and aperiodic (Bertsekas 2011). It can be also replaced using some weaker ones, e.g., Proposition 4.3.2 of (Bertsekas 2011), or be removed by designing a variant of RVI, e.g., Proposition 4.3.4 of (Bertsekas 2011). In the following theorem, we show that our Algorithm 3 converges to a solution of eq. ( 13), hence according to Theorem 7 if we set \u03c0 according to ( 14), then \u03c0 is the optimal robust policy. Theorem 8. (w t , V t ) converges to a solution (w, V ) to eq. (13) as \u2192 0.\nRemark 2. In this section, we mainly present the robust RVI algorithm for the robust optimal control problem, and its convergence and optimality guarantee. A robust RVI algorithm for robust policy evaluation can be similarly designed by replacing the max in line 4, Algorithm 3 with an expectation w.r.t. \u03c0. The convergence results in Theorem 8 can also be similarly derived.", "publication_ref": ["b31", "b5", "b5", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Examples and Numerical Results", "text": "In this section, we study several commonly used uncertainty set models, including contamination model, Kullback-Lerbler (KL) divergence and total-variation defined model. As can be observed from Algorithms 1 to 3, for different uncertainty sets, the only difference lies in how the support function \u03c3 P a s (V ) is calculated. In the sequel, we discuss how to efficiently calculate the support function for various uncertainty sets.\nWe numerically compare our robust (relative) value iteration methods v.s. non-robust (relative) value iteration method on different uncertainty sets. Our experiments are based on the Garnet problem G(20, 40) (Archibald, McKinnon, and Thomas 1995). More specifically, there are 20 states and 30 actions; the nominal transition kernel P = {p a s \u2208 \u2206(S)} is randomly generated according to the uniform distribution, and the reward functions r(s, a) \u223c N(0, \u03c3 s,a ), where \u03c3 s,a \u223c Uniform[0, 1]. In our experiments, the uncertainty sets are designed to be centered at the nominal transition kernel. We run different algorithms, i.e., (robust) value iteration and (robust) relative value iteration, and obtain the greedy policies at each time step. Then, we use robust averagereward policy evaluation (Algorithm 1) to evaluate the robust average-reward of these policies. We plot the robust averagereward against the number of iterations. Contamination model. For any (s, a) the uncertainty set P a s is defined as P a s = {q : q = (1 \u2212 R)p a s + Rp , p \u2208 \u2206(S)}, where p a s is the nominal transition kernel. It can be viewed as an adversarial model, where at each time-step, the environment transits according to the nominal transition kernel p with probability 1 \u2212 R, and according to an arbitrary kernel p with probability R. Note that \u03c3\nP a s (V ) = (1 \u2212 R)(p a s ) V + R min s V (s).\nOur experimental results under the contamination model are shown in Figure 1.  Total variation. The total variation distance is another commonly used distance metric to measure the difference between two distributions. For two distributions p and q, it is defined as D T V (p, q) = 1 2 p \u2212 q 1 . Consider an uncertainty set defined via total variation:\nP a s = {q : D T V (q||p a s ) \u2264 R}.\nThen, its support function can be efficiently solved as follows (Iyengar 2005):\n\u03c3 P a s (V ) = p V \u2212 R min \u00b5\u22650 {max s (V (s) \u2212 \u00b5(s)) \u2212 min s (V (s) \u2212 \u00b5(s))} .\nOur experimental results under the total variation model are shown in Figure 2. Kullback-Lerbler (KL) divergence. The Kullback-Leibler divergence is widely used to measure the distance between two probability distributions. For distributions p, q, it is defined as D KL (q||p) = s q(s) log q(s) p(s) . Consider an uncertainty set defined via KL divergence: P a s = {q : D KL (q||p a s ) \u2264 R}. Then, its support function can be efficiently solved using the duality result in (Hu and Hong   It can be seen that our robust methods can obtain policies that achieve higher worst-case reward. Also, both our limitbased robust value iteration and our direct method of robust relative value iteration converge to the optimal robust policies, which validates our theoretical results.", "publication_ref": ["b1", "b19"], "figure_ref": ["fig_1", "fig_3"], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper, we investigated the problem of robust MDPs under the average-reward setting. We established uniform convergence of the discounted value function to averagereward, which further implies the uniform convergence of the robust discounted value function to robust average-reward. Based on this insight, we designed a robust dynamic programming approach using the robust discounted MDPs as an approximation (the limit method). We theoretically proved their convergence and optimality and proved a robust version of the Blackwell optimality (Blackwell 1962). We then designed a direct approach for robust average-reward MDPs, where we derived the robust Bellman equation for robust average-reward MDPs. We further designed a robust RVI method, which was proven to converge to the optimal robust solution. Technically, our proof techniques are fundamentally different from existing studies on average-reward robust MDPs, e.g., those in (Blackwell 1962;Tewari and Bartlett 2007).", "publication_ref": ["b6", "b6", "b43"], "figure_ref": [], "table_ref": []}, {"heading": "Review of Robust Discounted MDPs", "text": "In this section, we provide a brief review on the existing methods and results for robust discounted MDPs.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Robust Policy Evaluation", "text": "We first consider the robust policy evaluation problem, where we aim to estimate the robust value function V \u03c0 P,\u03b3 for any policy \u03c0. It has been shown that the robust Bellman operator T \u03c0 is a \u03b3-contraction, and the robust value function V \u03c0 P,\u03b3 is its unique fixed-point. Hence by recursively applying the robust Bellman operator, we can find the robust discounted value function (Nilim and El Ghaoui 2004;Iyengar 2005). for all s \u2208 S do\n3: V t+1 (s) \u2190 E \u03c0 [r(s, A) + \u03b3\u03c3 P A s (V t )] 4:\nend for 5: end for 6: return V T", "publication_ref": ["b19"], "figure_ref": [], "table_ref": []}, {"heading": "Robust Optimal Control", "text": "Another important problem in robust MDP is to find the optimal policy which maximizes the robust discounted value function:\n\u03c0 * = arg max \u03c0 V \u03c0 P,\u03b3 .(15)\nA robust value iteration approach is developed in (Nilim and El Ghaoui 2004; Iyengar 2005) as follows.\nAlgorithm 5: Optimal Control for robust discounted MDPs\nInput: V 0 , T 1: for t = 0, 1, ..., T \u2212 1 do 2:\nfor all s \u2208 S do 3:\nV t+1 (s) \u2190 max a r(s, a) + \u03b3\u03c3 P a s (V t ) 4:\nend for 5: end for 6: \u03c0 * (s) \u2190 arg max a r(s, a) + \u03b3\u03c3 P a s (V T ) , \u2200s 7: return \u03c0 *", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Equivalence between Time-Varying and Stationary Models", "text": "We first provide an equivalence result between time-varying and stationary transition kernel models under stationary policies, which is an analog result to the one for robust discounted MDPs (Iyengar 2005;Nilim and El Ghaoui 2004). This result will be used in our following proofs.\nRecall the definitions of robust discounted value function and worst-case average reward in eqs. ( 4) and ( 5), the worst-case is taken w.r.t. \u03ba = (P 0 , P 1 ...) \u2208 t\u22650 P, therefore, the transition kernel at each time step could be different. This model is referred to as time-varying transition kernel model (as in (Iyengar 2005;Nilim and El Ghaoui 2004)). Another commonly used setting is that the transition kernels at different time step are the same, which is referred to as the stationary model (Iyengar 2005;Nilim and El Ghaoui 2004). In this paper, we use the following notations to distinguish the two models. By E P [\u2022], we denote the expectation when the transition kernels at all time steps are the same, P, i.e., the stationary model. We also denote by g \u03c0 P (s) lim n\u2192\u221e E P,\u03c0 1 n n\u22121 t=0 r t S 0 = s and V \u03c0 P,\u03b3 (s) E P,\u03c0 \u221e t=0 \u03b3 t r t S 0 = s being the expected average-reward and expected discounted value function under the stationary model P. By E \u03ba [\u2022], we denote the expectation when the transition kernel at time t is P t , i.e., the time-varying model.\nFor the discounted setting, it has been shown in (Nilim and El Ghaoui 2004) that for a stationary policy \u03c0, any \u03b3 \u2208 [0, 1), and any s \u2208 S,\nV \u03c0 P,\u03b3 (s) = min \u03ba\u2208 t\u22650 P E \u03c0,\u03ba \u221e t=0 \u03b3 t r t |S 0 = s = min P\u2208P E \u03c0,P \u221e t=0 \u03b3 t r t |S 0 = s .(16)\nIn the following theorem, we prove an analog of eq. ( 16) for robust-average reward MDPs that if we consider stationary policies, then the robust average-reward problem with the time-varying model can be equivalently solved by a stationary model. Specifically, we define the worst-case average reward for the stationary transition kernel model as follows:\nmin P\u2208P lim n\u2192\u221e E \u03c0,P 1 n n\u22121 t=0 r t S 0 = s . (17\n)\nRecall the worst-case average reward for the time-varying model in eq. ( 5). We will show that for any stationary policy, eq. ( 5) can be equivalently solved by solving eq. (17).\nTheorem 9. Consider an arbitrary stationary policy \u03c0. Then, the worst-case average-reward under the time-varying model is the same as the one under the stationary model:\ng \u03c0 P (s) min \u03ba\u2208 t\u22650 P lim n\u2192\u221e E \u03ba,\u03c0 1 n n\u22121 t=0 r t |S 0 = s = min P\u2208P lim n\u2192\u221e E P,\u03c0 1 n n\u22121 t=0 r t S 0 = s . (18\n)\nSimilar result also holds for the robust relative value function:\nV \u03c0 P (s) min \u03ba\u2208 t\u22650 P E \u03ba,\u03c0 \u221e t=0 (r t \u2212 g \u03c0 P )|S 0 = s = min P\u2208P E P,\u03c0 \u221e t=0 (r t \u2212 g \u03c0 P )|S 0 = s .(19)\nProof. From the robust Bellman equation in Theorem 6 2 , we have that\nV \u03c0 P (s) + g \u03c0 P = a \u03c0(a|s) r(s, a) + \u03c3 P a s (V \u03c0 P ) . (20\n)\nDenote by arg min p\u2208P a s (p) V \u03c0 P p a s 3 , and denote by P \u03c0 {p a s : s \u2208 S, a \u2208 A}. It then follows that\nV \u03c0 P (s) = a \u03c0(a|s) r(s, a) \u2212 g \u03c0 P + \u03c3 P a s (V \u03c0 P ) = a \u03c0(a|s)(r(s, a) \u2212 g \u03c0 P ) + a \u03c0(a|s)E P \u03c0 [V \u03c0 P (S 1 )|S 0 = s, A 0 = a] = a \u03c0(a|s)(r(s, a) \u2212 g \u03c0 P ) + E P \u03c0 ,\u03c0 [V \u03c0 P (S 1 )|S 0 = s] = a \u03c0(a|s)(r(s, a) \u2212 g \u03c0 P ) + E P \u03c0 ,\u03c0 a \u03c0(a|S 1 )(r(S 1 , a) \u2212 g \u03c0 P )|S 0 = s + E P \u03c0 ,\u03c0 a \u03c0(a|S 1 )\u03c3 P a S 1 (V \u03c0 P )|S 0 = s = a \u03c0(a|s)(r(s, a) \u2212 g \u03c0 P ) + E P \u03c0 ,\u03c0 [r 1 \u2212 g \u03c0 P |S 0 = s] + E P \u03c0 ,\u03c0 \u03c3 P A 1 S 1 (V \u03c0 P )|S 0 = s = a \u03c0(a|s)(r(s, a) \u2212 g \u03c0 P ) + E P \u03c0 ,\u03c0 r 1 \u2212 g \u03c0 P S 0 = s + E P \u03c0 ,\u03c0 (p A1 S1 ) V \u03c0 P |S 0 = s = E P \u03c0 ,\u03c0 r 0 \u2212 g \u03c0 P + r 1 \u2212 g \u03c0 P |S 0 = s + E P \u03c0 ,\u03c0 [V \u03c0 P (S 2 )|S 0 = s] ...... = E P \u03c0 ,\u03c0 \u221e t=0 (r t \u2212 g \u03c0 P )|s . (21\n)\nBy the definition, the following always hold:\nmin \u03ba\u2208 t\u22650 P E \u03ba,\u03c0 \u221e t=0 (r t \u2212 g \u03c0 P )|S 0 = s \u2264 min P\u2208P E P,\u03c0 \u221e t=0 (r t \u2212 g \u03c0 P )|S 0 = s .(22)\nThis hence implies that a stationary transition kernel sequence \u03ba = (P \u03c0 , P \u03c0 , ...) is one of the worst-case transition kernels for V \u03c0 P . Therefore, eq. ( 19) can be proved.\nConsider the transition kernel P \u03c0 . We denote its non-robust average-reward and the non-robust relative value function by g \u03c0 P \u03c0\nand V \u03c0 P \u03c0 . By the non-robust Bellman equation (Sutton and Barto 2018), we have that\nV \u03c0 P \u03c0 (s) = a \u03c0(a|s)(r(s, a) \u2212 g \u03c0 P \u03c0 ) + E P \u03c0 ,\u03c0 [V \u03c0 P \u03c0 (S 1 )|s].(23)\nOn the other hand, the robust Bellman equation shows that\nV \u03c0 P (s) = V \u03c0 P \u03c0 (s) = a \u03c0(a|s)(r(s, a) \u2212 g \u03c0 P ) + E P \u03c0 ,\u03c0 [V \u03c0 P \u03c0 (S 1 )|s].(24)\nThese two equations hence implies that g \u03c0 P = g \u03c0 P \u03c0 , and hence the stationary kernel (P \u03c0 , P \u03c0 , ...) is also a worst-case kernel of robust average-reward in the time-varying setting. This proves eq. (18).", "publication_ref": ["b19", "b19", "b19", "b40"], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Theorem 1", "text": "In the proof, unless otherwise specified, we denote by v the l \u221e norm of a vector v, and for a matrix A, we denote by A its matrix norm induced by l \u221e norm, i.e.,\nA = sup x\u2208R d Ax \u221e x \u221e . Lemma 1. [Theorem 8.2.3 in (Puterman 1994)] For any P, \u03b3, \u03c0, V \u03c0 P,\u03b3 = 1 1 \u2212 \u03b3 g \u03c0 P + h \u03c0 P + f \u03c0 P (\u03b3),(25)\nwhere h \u03c0 P = H \u03c0 P r \u03c0 , and Following Proposition 8.4.6 in (Puterman 1994), we can show the following lemma. Lemma 2. H \u03c0 P is continuous on \u03a0 \u00d7 P. If \u03a0 and P are compact, H \u03c0 P is uniformly bounded on \u03a0 \u00d7 P, i.e., there exists a constant h, such that H \u03c0 P \u2264 h for any \u03c0, P. For simplicity, denote by\nf \u03c0 P (\u03b3) = 1 \u03b3 \u221e n=1 (\u22121) n 1\u2212\u03b3 \u03b3 n (H \u03c0 P ) n+1 r \u03c0 .\nS \u03c0 \u221e (P, \u03b3) 1 \u03b3 \u221e n=1 (\u22121) n 1 \u2212 \u03b3 \u03b3 n (H \u03c0 P ) n+1 r \u03c0 , S \u03c0 N (P, \u03b3) 1 \u03b3 N n=1 (\u22121) n 1 \u2212 \u03b3 \u03b3 n (H \u03c0 P ) n+1 r \u03c0 .(26)\nClearly S \u03c0 \u221e (P, \u03b3) = f \u03c0 P (\u03b3) and lim N \u2192\u221e S \u03c0 N (P, \u03b3) = S \u03c0 \u221e (P, \u03b3) for any specific \u03c0, P. Lemma 3. There exists \u03b4 \u2208 (0, 1), such that\nlim N \u2192\u221e S \u03c0 N (P, \u03b3) = S \u03c0 \u221e (P, \u03b3) (27\n)\nuniformly on \u03a0 \u00d7 P \u00d7 [\u03b4, 1]. Proof. Note that H \u03c0 P \u2264 h, hence there exists \u03b4, s.t. 1 \u2212 \u03b4 \u03b4 h \u2264 k < 1 (28)\nfor some constant k. Then for any \u03b3 \u2265 \u03b4,\n1 \u2212 \u03b3 \u03b3 h \u2264 1 \u2212 \u03b4 \u03b4 h \u2264 k. (29\n)\nMoreover, note that\n1 \u03b3 (\u22121) n 1 \u2212 \u03b3 \u03b3 n (H \u03c0 P ) n+1 r \u2264 1 \u03b3 1 \u2212 \u03b3 \u03b3 n h n+1 \u2264 hk n \u03b4 M n ,(30)\nwhich is because\nA + B \u2264 A + B for induced l \u221e norm, Ax \u2264 A x and r \u03c0 \u221e \u2264 1. Note that \u221e n=1 M n = h \u03b4 k 1 \u2212 k ,(31)\nhence by Weierstrass M -test (Rudin 2022), S \u03c0 N (P, \u03b3) uniformly converges to S \u03c0 \u221e (P, \u03b3) on \u03a0 \u00d7 P \u00d7 [\u03b4, 1]. Lemma 4. There exists a uniform constant L, such that\nS \u03c0 N (P, \u03b3 1 ) \u2212 S \u03c0 N (P, \u03b3 2 ) \u2264 L|\u03b3 1 \u2212 \u03b3 2 |, (32\n)\nfor any N , \u03c0, P, \u03b3 1 , \u03b3 2 \u2208 [\u03b4, 1].\nProof. We first show that \u03b3S \u03c0 N (P, \u03b3)\n= N n=1 (\u22121) n 1\u2212\u03b3 \u03b3 n (H \u03c0 P ) n+1 r \u03c0 T \u03c0 N (P, \u03b3) is uniformly Lipschitz w.r.t. the l \u221e norm, i.e., T \u03c0 N (P, \u03b3 1 ) \u2212 T \u03c0 N (P, \u03b3 2 ) \u2264 l|\u03b3 1 \u2212 \u03b3 2 |,(33)\nfor any N , \u03c0, P, \u03b3 1 , \u03b3 2 \u2208 [\u03b4, 1] and some constant l.\nClearly, it can be shown by verifying \u2207T \u03c0 N (P, \u03b3) is uniformly bounded for any \u03c0, N, P or \u03b3. First, it can be shown that\n\u2207T \u03c0 N (P, \u03b3) = N n=1 (\u22121) n n 1 \u2212 \u03b3 \u03b3 n\u22121 \u22121 \u03b3 2 (H \u03c0 P ) n+1 r \u03c0 ,(34)\nand moreover\n\u2207T \u03c0 N (P, \u03b3) \u2264 N n=1 n 1 \u2212 \u03b3 \u03b3 n\u22121 1 \u03b3 2 h n+1 l N (\u03b3). (35\n) Note that h 1 \u2212 \u03b3 \u03b3 l N (\u03b3) = N n=1 n 1 \u2212 \u03b3 \u03b3 n 1 \u03b3 2 h n+2 ,(36)\nthen, we can show that\n1 \u2212 h 1 \u2212 \u03b3 \u03b3 l N (\u03b3) = N n=1 n 1 \u2212 \u03b3 \u03b3 n\u22121 1 \u03b3 2 h n+1 \u2212 N n=1 n 1 \u2212 \u03b3 \u03b3 n 1 \u03b3 2 h n+2 = 1 \u03b3 2 h 2 \u2212 N 1 \u2212 \u03b3 \u03b3 N 1 \u03b3 2 h N +2 + N n=2 1 \u2212 \u03b3 \u03b3 n\u22121 1 \u03b3 2 h n+1 \u2264 1 \u03b3 2 h 2 + h 2 \u03b3 2 1 \u2212 \u03b3 \u03b3 h 1 1 \u2212 1\u2212\u03b3 \u03b3 h = h 2 \u03b3 2 + h 2 \u03b3 2 1 \u2212 \u03b3 \u03b3 h 1 1 \u2212 1\u2212\u03b3 \u03b3 h . (37\n)\nHence, we have that\n\u2207T \u03c0 N (P, \u03b3) \u2264 l N (\u03b3) \u2264 1 1 \u2212 h 1\u2212\u03b3 \u03b3 h 2 \u03b3 2 + h 2 \u03b3 2 1 \u2212 \u03b3 \u03b3 h 1 1 \u2212 1\u2212\u03b3 \u03b3 h \u2264 1 1 \u2212 k h 2 \u03b4 2 + h 2 \u03b4 2 k 1 \u2212 k ,(38)\nwhich implies a uniform bound on \u2207T \u03c0 N (P, \u03b3) . Now, we have that\n|S \u03c0 N (P, \u03b3 1 ) \u2212 S \u03c0 N (P, \u03b3 2 )| \u2264 |\u03b3 2 \u2212 \u03b3 1 | \u03b3 1 \u03b3 2 T \u03c0 N (P, \u03b3 1 ) + T \u03c0 N (P, \u03b3 1 ) \u2212 T \u03c0 N (P, \u03b3 2 ) \u03b3 2 . (39\n)\nTo show T \u03c0 N (P, \u03b3) is uniformly bounded, we have that\nT \u03c0 N (P, \u03b3) \u2264 N n=1 1 \u2212 \u03b3 \u03b3 n (H \u03c0 P ) n+1 r \u2264 N n=1 1 \u2212 \u03b3 \u03b3 n h n+1 \u2264 N n=1 k n h \u2264 h k 1 \u2212 k . (40\n)\nThen, it follows that\nS \u03c0 N (P, \u03b3 1 ) \u2212 S \u03c0 N (P, \u03b3 2 ) = \u03b3 2 \u2212 \u03b3 1 \u03b3 1 \u03b3 2 T \u03c0 N (P, \u03b3 1 ) + T \u03c0 N (P, \u03b3 1 ) \u2212 T \u03c0 N (P, \u03b3 2 ) \u03b3 2 \u2264 1 \u03b4 2 h k 1 \u2212 k + 1 \u03b4 1 1 \u2212 k h 2 \u03b4 2 + h 2 \u03b4 2 k 1 \u2212 k |\u03b3 1 \u2212 \u03b3 2 | L|\u03b3 1 \u2212 \u03b3 2 |,(41)\nwhere\nL = 1 \u03b4 2 h k 1\u2212k + 1 \u03b4 1 1\u2212k h 2 \u03b4 2 + h 2 \u03b4 2 k 1\u2212k\nis a universal constant that does not depend on N, P, \u03c0 or \u03b3.\nLemma 5. S \u03c0 \u221e (P, \u03b3) uniformly converges as \u03b3 \u2192 1 on \u03a0 \u00d7 P. Also, S \u03c0 \u221e (P, \u03b3) is L-Lipschitz for any \u03b3 > \u03b4: for any \u03c0, P and any \u03b3 1 , \u03b3 2 \u2208 (\u03b4, 1].\nS \u03c0 \u221e (P, \u03b3 1 ) \u2212 S \u03c0 \u221e (P, \u03b3 2 ) \u2264 L|\u03b3 1 \u2212 \u03b3 2 |.(42)\nProof. From Lemma 3, for any , there exists N , such that for any n \u2265 N , \u03c0, P, \u03b3 > \u03b4,\nS \u03c0 \u221e (P, \u03b3) \u2212 S \u03c0 n (P, \u03b3) < .(43)\nThus for any \u03b3 1 , \u03b3 2 \u2208 (\u03b4, 1],\nS \u03c0 \u221e (P, \u03b3 1 ) \u2212 S \u03c0 \u221e (P, \u03b3 2 ) \u2264 S \u03c0 \u221e (P, \u03b3 1 ) \u2212 S \u03c0 n (P, \u03b3 1 ) + S \u03c0 n (P, \u03b3 1 ) \u2212 S \u03c0 n (P, \u03b3 2 ) + S \u03c0 n (P, \u03b3 2 ) \u2212 S \u03c0 \u221e (P, \u03b3 2 ) \u2264 2 + S \u03c0 n (P, \u03b3 1 ) \u2212 S \u03c0 n (P, \u03b3 2 ) \u2264 2 + L|\u03b3 1 \u2212 \u03b3 2 |,(44)\nwhere the last step is from Lemma 4. Thus, for any , there exists \u03c9 = max {\u03b4, 1 \u2212 }, such that for any \u03b3 1 , \u03b3 2 > \u03c9,\nS \u03c0 \u221e (P, \u03b3 1 ) \u2212 S \u03c0 \u221e (P, \u03b3 2 ) < (2 + L) ,(45)\nand hence by Cauchy's criterion we conclude that S \u03c0 \u221e (P, \u03b3) converges uniformly on \u03a0 \u00d7 P. On the other hand, since eq. (44) holds for any , it implies that\nS \u03c0 \u221e (P, \u03b3 1 ) \u2212 S \u03c0 \u221e (P, \u03b3 2 ) \u2264 L|\u03b3 1 \u2212 \u03b3 2 |,(46)\nwhich completes the proof.\nWe now prove Theorem 1. For any P, \u03c0, we have that\nV \u03c0 P,\u03b3 = 1 1 \u2212 \u03b3 g \u03c0 P + h \u03c0 P + f \u03c0 P (\u03b3).(47)\nIt then follows that\n(1 \u2212 \u03b3)V \u03c0 P,\u03b3 = g \u03c0 P + (1 \u2212 \u03b3)h \u03c0 P + (1 \u2212 \u03b3)f \u03c0 P (\u03b3). (48\n)\nClearly (1 \u2212 \u03b3)h \u03c0 P \u2192 0 uniformly on \u03a0 \u00d7 P because h \u03c0 P = H \u03c0 P r \u03c0 \u2264 h is uniformly bounded. Then, (1 \u2212 \u03b3 1 )f \u03c0 P (\u03b3 1 ) \u2212 (1 \u2212 \u03b3 2 )f \u03c0 P (\u03b3 2 ) \u2264 (1 \u2212 \u03b3 1 )f \u03c0 P (\u03b3 1 ) \u2212 (1 \u2212 \u03b3 1 )f \u03c0 P (\u03b3 2 ) + (1 \u2212 \u03b3 1 )f \u03c0 P (\u03b3 2 ) \u2212 (1 \u2212 \u03b3 2 )f \u03c0 P (\u03b3 2 ) \u2264 (1 \u2212 \u03b3 1 )L|\u03b3 1 \u2212 \u03b3 2 | + f \u03c0 P (\u03b3 2 ) |\u03b3 1 \u2212 \u03b3 2 |.(49)\nFor any \u03c0, P, \u03b3 > \u03b4,\nf \u03c0 P (\u03b3) = 1 \u03b3 \u221e n=1 (\u22121) n 1 \u2212 \u03b3 \u03b3 n (H \u03c0 P ) n+1 r \u03c0 \u2264 1 \u03b3 \u221e n=1 1 \u2212 \u03b3 \u03b3 n h n+1 \u2264 h \u03b4 1 \u2212 \u03b3 \u03b3 h 1 1 \u2212 1\u2212\u03b3 \u03b3 h \u2264 h \u03b4 k 1 \u2212 k c f .(50)\nHence, (1 \u2212 \u03b3)f \u03c0 P (\u03b3) \u2192 0 uniformly on \u03a0 \u00d7 P due to the fact that f \u03c0 P (\u03b3) is uniformly bounded for any \u03c0, \u03b3 > \u03b4, P. Then we have that lim \u03b3\u21921 (1 \u2212 \u03b3)V \u03c0 P,\u03b3 = g \u03c0 P uniformly on P \u00d7 \u03a0. This completes the proof of Theorem 1.", "publication_ref": ["b35"], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Theorem 2", "text": "We first show a lemma which allows us to interchange the order of lim and max. Lemma 6. If a function f (x, y) converges uniformly to F (x) on X as y \u2192 y 0 , then\nmax x lim y\u2192y0 f (x, y) = lim y\u2192y0 max x f (x, y).(51)\nProof. For each f (x, y), denote by arg max x f (x, y) = x y , and hence f (x y , y) \u2265 f (x, y) for any x, y. Also denote by arg max x F (x) = x . Now because f (x, y) uniformly converges to F (x), then for any , there exists \u03b4 , such that \u2200|y \u2212 y\n0 | < \u03b4 , |f (x, y) \u2212 F (x)| \u2264 (52) for any x. Now consider |f (x y , y) \u2212 F (x )| for |y \u2212 y 0 | < \u03b4 . If f (x y , y) \u2212 F (x ) > 0, then |f (x y , y) \u2212 F (x )| = f (x y , y) \u2212 F (x ) = f (x y , y) \u2212 F (x y ) + F (x y ) \u2212 F (x ) \u2264 ;(53)\nOn the other hand if f (x y , y) \u2212 F (x ) < 0, then |f (x y , y) \u2212 F (x )| = F (x ) \u2212 f (x y , y) = F (x ) \u2212 f (x , y) + f (x , y) \u2212 f (x y , y) \u2264 .(54)\nHence, we showed that for any , there exists \u03b4 , such that \u2200|y \u2212 y\n0 | < \u03b4 , |f (x y , y) \u2212 F (x )| = | max x f (x, y) \u2212 max x F (x)| \u2264 ,(55)\nand hence\nlim y\u2192y0 max x f (x, y) = max x F (x) = max x lim y\u2192y0 f (x, y),(56)\nand this completes the proof.\nThen, we show that the robust discounted value function converges uniformly to the robust average-reward as the discounted factor approaches 1.\nTheorem 10 (Restatement of Theorem 2). The robust discounted value function converges uniformly to the robust average-reward on \u03a0:\nlim \u03b3\u21921 (1 \u2212 \u03b3)V \u03c0 P,\u03b3 = g \u03c0 P .(57)\nProof. Due to Theorem 9, for any stationary policy \u03c0, g \u03c0 P (s) = min P\u2208P g \u03c0 P (s) under the stationary model. Hence from the uniform convergence in Theorem 1, we first show the following:\ng \u03c0 P = min P\u2208P g \u03c0 P = min P\u2208P lim \u03b3\u21921 (1 \u2212 \u03b3)V \u03c0 P,\u03b3 (a) = lim \u03b3\u21921 min P\u2208P (1 \u2212 \u03b3)V \u03c0 P,\u03b3 = lim \u03b3\u21921 (1 \u2212 \u03b3)V \u03c0 P,\u03b3 ,(58)\nwhere (a) is because Lemma 6. Moreover, note that lim \u03b3\u21921 (1 \u2212 \u03b3)V \u03c0 P,\u03b3 = g \u03c0 P uniformly on \u03a0 \u00d7 P, hence the convergence in ( 58) is also uniform on \u03a0. Thus, we complete the proof.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Theorem 3", "text": "Theorem 11 (Restatement of Theorem 3). V T generated by Algorithm 1 converges to the robust average-reward g \u03c0 P as T \u2192 \u221e. Proof. From discounted robust Bellman equation (Nilim and El Ghaoui 2004), it can be shown that\n(1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t = (1 \u2212 \u03b3 t ) a \u03c0(a|s)(r(s, a) + \u03b3 t \u03c3 P a s (V \u03c0 P,\u03b3t )).(59)\nThen we can show that for any s \u2208 S,\n|V t+1 (s) \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 (s)| = |V t+1 (s) \u2212 (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t (s) + (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t (s) \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 (s)|(60)\n\u2264 |(1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t (s) \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 (s)| + |V t+1 (s) \u2212 (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t (s)| = |(1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t (s) \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 (s)| + a \u03c0(a|s) (1 \u2212 \u03b3 t )r(s, a) + \u03b3 t \u03c3 P a s (V t ) \u2212 ((1 \u2212 \u03b3 t )r(s, a) + \u03b3 t \u03c3 P a s ((1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t )) = |(1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t (s) \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 (s)| + a \u03c0(a|s) \u03b3 t \u03c3 P a s (V t ) \u2212 \u03b3 t \u03c3 P a s ((1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t ) = |(1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t (s) \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 (s)| + \u03b3 t a \u03c0(a|s) \u03c3 P a s (V t ) \u2212 \u03c3 P a s ((1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t ) .(61)\nIf we denote by\n\u2206 t V t \u2212 (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t \u221e , then \u2206 t+1 \u2264 (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 \u221e + \u03b3 t max s a \u03c0(a|s) \u03c3 P a s (V t ) \u2212 \u03c3 P a s ((1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t ) .(62)\nIt can be easily verified that \u03c3 P a s (V ) is a 1-Lipschitz function, thus the second term in (62) can be further bounded as\na \u03c0(a|s) \u03c3 P a s (V t ) \u2212 \u03c3 P a s ((1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t ) \u2264 a \u03c0(a|s) V t \u2212 (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t \u221e = V t \u2212 (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t \u221e ,(63)\nand hence\n\u2206 t+1 \u2264 (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 \u221e + \u03b3 t \u2206 t .(64)\nRecall that\n(1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t = (1 \u2212 \u03b3 t ) min P V \u03c0 P,\u03b3t . (65) Let s * t arg max s |(1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t (s) \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 (s)|. Then it follows that (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 \u221e = |(1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t (s * t ) \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 (s * t )|.(66)\nNote that from (Nilim and El Ghaoui 2004;Iyengar 2005), for any stationary policy \u03c0, there exists a stationary model P such\nthat V \u03c0 P,\u03b3 (s) = E P,\u03c0 \u221e t=0 \u03b3 t r t |S 0 = s V \u03c0 P,\u03b3\n. Hence in the following, for each \u03b3 t , we denote the worst-case transition kernel of V \u03c0 P,\u03b3t by\nP t . If (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t (s * t ) \u2265 (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 (s * t ), then |(1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t (s * t ) \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 (s * t )| = min P (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t (s * t ) \u2212 min P (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 (s * t ) = (1 \u2212 \u03b3 t )V \u03c0 Pt,\u03b3t (s * t ) \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 Pt+1,\u03b3t+1 (s * t ) = (1 \u2212 \u03b3 t )V \u03c0 Pt,\u03b3t (s * t ) \u2212 (1 \u2212 \u03b3 t )V \u03c0 Pt+1,\u03b3t (s * t ) + (1 \u2212 \u03b3 t )V \u03c0 Pt+1,\u03b3t (s * t ) \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 Pt+1,\u03b3t+1 (s * t ) (a) \u2264 (1 \u2212 \u03b3 t )V \u03c0 Pt+1,\u03b3t (s * t ) \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 Pt+1,\u03b3t+1 (s * t ) \u2264 (1 \u2212 \u03b3 t )V \u03c0 Pt+1,\u03b3t \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 Pt+1,\u03b3t+1 \u221e ,(67)\nwhere\n(a) is due to (1 \u2212 \u03b3 t )V \u03c0 Pt,\u03b3t (s * t ) = min P (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t (s * t ) \u2264 (1 \u2212 \u03b3 t )V \u03c0 Pt+1,\u03b3t (s * t ). Now, according to Lemma 1, (1 \u2212 \u03b3 t )V \u03c0 Pt+1,\u03b3t = g \u03c0 Pt+1 + (1 \u2212 \u03b3 t )h \u03c0 Pt+1 + (1 \u2212 \u03b3 t )f \u03c0 Pt+1 (\u03b3 t ),(68)\n(1 \u2212 \u03b3 t+1 )V \u03c0 Pt+1,\u03b3t+1 = g \u03c0 Pt+1 + (1 \u2212 \u03b3 t+1 )h \u03c0 Pt+1 + (1 \u2212 \u03b3 t+1 )f \u03c0 Pt+1 (\u03b3 t+1 ).(69)\nHence, for any \u03b3 t > \u03b4, eq. ( 67) can be further bounded as\n(1 \u2212 \u03b3 t )V \u03c0 Pt+1,\u03b3t \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 Pt+1,\u03b3t+1 \u221e = (\u03b3 t+1 \u2212 \u03b3 t )h \u03c0 Pt+1 + (1 \u2212 \u03b3 t )f \u03c0 Pt+1 (\u03b3 t ) \u2212 (1 \u2212 \u03b3 t+1 )f \u03c0 Pt+1 (\u03b3 t+1 ) \u221e \u2264 (\u03b3 t+1 \u2212 \u03b3 t ) h \u03c0 Pt+1 \u221e + f \u03c0 Pt+1 (\u03b3 t ) \u2212 f \u03c0 Pt+1 (\u03b3 t+1 ) \u221e + \u03b3 t+1 f \u03c0 Pt+1 (\u03b3 t+1 ) \u2212 \u03b3 t f \u03c0 Pt+1 (\u03b3 t ) \u221e (a) \u2264 h(\u03b3 t+1 \u2212 \u03b3 t ) + L(\u03b3 t+1 \u2212 \u03b3 t ) + \u03b3 t+1 f \u03c0 Pt+1 (\u03b3 t+1 ) \u2212 \u03b3 t f \u03c0 Pt+1 (\u03b3 t ) \u221e \u2264 h(\u03b3 t+1 \u2212 \u03b3 t ) + L(\u03b3 t+1 \u2212 \u03b3 t ) + \u03b3 t+1 f \u03c0 Pt+1 (\u03b3 t+1 ) \u2212 \u03b3 t+1 f \u03c0 Pt+1 (\u03b3 t ) \u221e + \u03b3 t+1 f \u03c0 Pt+1 (\u03b3 t ) \u2212 \u03b3 t f \u03c0 Pt+1 (\u03b3 t ) \u221e \u2264 h(\u03b3 t+1 \u2212 \u03b3 t ) + L(\u03b3 t+1 \u2212 \u03b3 t ) + \u03b3 t+1 f \u03c0 Pt+1 (\u03b3 t+1 ) \u2212 f \u03c0 Pt+1 (\u03b3 t ) \u221e + f \u03c0 Pt+1 (\u03b3 t ) \u221e (\u03b3 t+1 \u2212 \u03b3 t ) (b) \u2264 (h + L + \u03b3 t+1 L + sup \u03c0,P,\u03b3 f \u03c0 P (\u03b3) \u221e )(\u03b3 t+1 \u2212 \u03b3 t ) \u2264 K(\u03b3 t+1 \u2212 \u03b3 t ),(70)\nwhere (a) is from Lemma 5 for any \u03b3 t > \u03b4, c f is defined in (50) and K h + 2L + c f is a uniform constant; And (b) is from Lemma 5. Similarly, the inequality also holds for the case when\n(1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t (s * t ) \u2264 (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 (s * t )\n. Thus we have that for any t such that \u03b3 t > \u03b4,\n\u2206 t+1 \u2264 K(\u03b3 t+1 \u2212 \u03b3 t ) + \u03b3 t \u2206 t , (71\n)\nwhere K is a uniform constant. Following Lemma 8 from (Tewari and Bartlett 2007), we have that \u2206 t \u2192 0. Note that\nV t \u2212 g \u03c0 P \u221e \u2264 V t \u2212 (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t \u221e + (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t \u2212 g \u03c0 P \u221e = \u2206 t + (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t \u2212 g \u03c0 P \u221e . (72\n)\nTogether with Theorem 2, we further have that\nlim t\u2192\u221e V t \u2212 g \u03c0 P \u221e = 0,(73)\nwhich completes the proof.", "publication_ref": ["b19", "b43"], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Theorem 4", "text": "Note that the optimal robust average-reward is defined as\ng * P (s) max \u03c0 g \u03c0 P (s).(74)\nWe further define V * P,\u03b3 (s) max \u03c0 V \u03c0 P,\u03b3 (s).\nTheorem 12 (Restatement of Theorem 4). V T generated by Algorithm 2 converges to the optimal robust average-reward g * P as T \u2192 \u221e.\nProof. Firstly, from the uniform convergence in Theorem 2, it can be shown that\nlim t\u2192\u221e (1 \u2212 \u03b3 t )V * P,\u03b3t = g * P . (76\n)\nWe then show that for any s \u2208 S,\n|V t+1 (s) \u2212 (1 \u2212 \u03b3 t+1 )V * P,\u03b3t+1 (s)| \u2264 |V t+1 (s) \u2212 (1 \u2212 \u03b3 t )V * P,\u03b3t (s)| + |(1 \u2212 \u03b3 t )V * P,\u03b3t (s) \u2212 (1 \u2212 \u03b3 t+1 )V * P,\u03b3t+1 (s)| (a) = |(1 \u2212 \u03b3 t )V * P,\u03b3t (s) \u2212 (1 \u2212 \u03b3 t+1 )V * P,\u03b3t+1 (s)| + max a (1 \u2212 \u03b3 t )r(s, a) + \u03b3 t \u03c3 P a s (V t ) \u2212 max a ((1 \u2212 \u03b3 t )r(s, a) + \u03b3 t \u03c3 P a s ((1 \u2212 \u03b3 t )V * P,\u03b3t )) \u2264 |(1 \u2212 \u03b3 t )V * P,\u03b3t (s) \u2212 (1 \u2212 \u03b3 t+1 )V * P,\u03b3t+1 (s)| + max a (1 \u2212 \u03b3 t )r(s, a) + \u03b3 t \u03c3 P a s (V t ) \u2212 ((1 \u2212 \u03b3 t )r(s, a) + \u03b3 t \u03c3 P a s ((1 \u2212 \u03b3 t )V * P,\u03b3t )) ,(77)\nwhere (a) is because the optimal robust Bellman equation, and the last inequality is from the fact that\n| max x f (x)\u2212max x g(x)| \u2264 max x |f (x) \u2212 g(x)|.\nHence eq. ( 77) can be further bounded as\n|V t+1 (s) \u2212 (1 \u2212 \u03b3 t+1 )V * P,\u03b3t+1 (s)| \u2264 |(1 \u2212 \u03b3 t )V * P,\u03b3t (s) \u2212 (1 \u2212 \u03b3 t+1 )V * P,\u03b3t+1 (s)| + \u03b3 t max a \u03c3 P a s (V t ) \u2212 \u03c3 P a s ((1 \u2212 \u03b3 t )V * P,\u03b3t ) . (78\n)\nIf we denote by \u2206 t V t \u2212 (1 \u2212 \u03b3 t )V * P,\u03b3t \u221e , then\n\u2206 t+1 \u2264 (1 \u2212 \u03b3 t )V * P,\u03b3t \u2212 (1 \u2212 \u03b3 t+1 )V * P,\u03b3t+1 \u221e + \u03b3 t max s.a \u03c3 P a s (V t ) \u2212 \u03c3 P a s ((1 \u2212 \u03b3 t )V * P,\u03b3t ) . (79\n)\nSince the support function \u03c3 P a s (V ) is 1-Lipschitz, then it can be shown that for any s, a,\n\u03c3 P a s (V t ) \u2212 \u03c3 P a s ((1 \u2212 \u03b3 t )V * P,\u03b3t ) \u2264 V t \u2212 (1 \u2212 \u03b3 t )V * P,\u03b3t \u221e . (80\n)\nHence \u2206 t+1 \u2264 (1 \u2212 \u03b3 t )V * P,\u03b3t \u2212 (1 \u2212 \u03b3 t+1 )V * P,\u03b3t+1 \u221e + \u03b3 t \u2206 t .(81)\nSimilar to (70) in Theorem 3, we can show that\n(1 \u2212 \u03b3 t )V * P,\u03b3t \u2212 (1 \u2212 \u03b3 t+1 )V * P,\u03b3t+1 \u221e \u2264 K|\u03b3 t \u2212 \u03b3 t+1 |,(82)\nand similar to Lemma 8 from (Tewari and Bartlett 2007), lim\nt\u2192\u221e \u2206 t = 0. (83) Moreover, note that V t \u2212 g * P \u221e \u2264 V t \u2212 (1 \u2212 \u03b3 t )V * P,\u03b3t \u221e + (1 \u2212 \u03b3 t )V * P,\u03b3t \u2212 g * P \u221e = \u2206 t + (1 \u2212 \u03b3 t )V * P,\u03b3t \u2212 g * P \u221e ,(84)\nwhich together with eq. (76) implies that V t \u2212 g * P \u221e \u2192 0, (85) and hence it completes the proof.\nLemma 7. There exists a deterministic optimal policy, i.e., \u2203\u03c0 * \u2208 \u03a0 D , s.t. g \u03c0 * P = g * P = max \u03c0\u2208\u03a0 g \u03c0 P .\nProof of Lemma 7\nLemma 8. (Restatement of Lemma 7). There exists a deterministic optimal policy, i.e., \u2203\u03c0 * \u2208 \u03a0 D , s.t. g \u03c0 * P = g * P = max \u03c0\u2208\u03a0 g \u03c0 P . Proof. Assume that there is no deterministic optimal robust policy, i.e., there exists a strictly random policy \u03c0 r \u2208 \u03a0, such that for any deterministic policy \u03c0 \u2208 \u03a0 D , g \u03c0r P > g \u03c0 P .\n(86) According to theorem 2, we have that lim\n\u03b3\u21921 (1 \u2212 \u03b3)V \u03c0r P,\u03b3 = g \u03c0r P ,(87)\nlim \u03b3\u21921 (1 \u2212 \u03b3)V \u03c0 P,\u03b3 = g \u03c0 P , \u2200\u03c0 \u2208 \u03a0 D . (88\n)\nSince there are only finite number of deterministic policies, there exists \u03b4 < 1, such that for any \u03b3 > \u03b4,\nV \u03c0r P,\u03b3 > V \u03c0 P,\u03b3 , \u2200\u03c0 \u2208 \u03a0 D . (89\n)\nThis implies that for \u03b3 > \u03b4, the random policy \u03c0 r is better than all the deterministic policies, i.e., V \u03c0r P,\u03b3 > max\n\u03c0\u2208\u03a0 D V \u03c0 P,\u03b3 .(90)\nHowever, Theorem 3.1 of (Iyengar 2005) implies that there exists deterministic optimal robust policy, i.e., max\n\u03c0\u2208\u03a0 D V \u03c0 P,\u03b3 = max \u03c0\u2208\u03a0 V \u03c0 P,\u03b3 \u2265 V \u03c0r P,\u03b3 ,(91)\nwhich contradicts to (90). Hence it implies that there exists a deterministic optimal robust policy, and completes the proof.", "publication_ref": ["b43", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Theorem 5", "text": "Theorem 13 (Restatement of Theorem 5). There exists 0 < \u03b4 < 1, such that for any \u03b3 > \u03b4, a deterministic optimal robust policy for robust discounted value function V * P,\u03b3 is also an optimal policy for robust average-reward, i.e., V \u03c0 * P,\u03b3 = V * P,\u03b3 .\n(92) Moreover, when arg max \u03c0\u2208\u03a0 D g \u03c0 P is a singleton, there exists a unique Blackwell optimal policy. Proof. According to Lemma 7, there exists \u03c0 * \u2208 \u03a0 D such that g * P = g \u03c0 * P .\n(93) Assume the robust average-reward of all deterministic policies are sorted in a descending order:\ng * P = g \u03c0 * 1 P = g \u03c0 * 2 P = ... = g \u03c0 * m P > g \u03c01 P \u2265 ... \u2265 g \u03c0n P (94)\nfor all \u03c0 * i , \u03c0 i \u2208 \u03a0 D , and we define \u03a0 * = {\u03c0 * i : i = 1, ..., m}. Denote by d = g\n\u03c0 * i P \u2212 g \u03c01 P .\nFrom Theorem 2, we know that for any \u03c0 \u2208 \u03a0 D , lim\n\u03b3\u21921 (1 \u2212 \u03b3)V \u03c0 P,\u03b3 = g \u03c0 P .(95)\nBecause the set \u03a0 D is finite, for any < d 2 , there exists \u03b4 < 1, such that for any \u03b3 > \u03b4 , \u03c0 * i and \u03c0 j ,\n|(1 \u2212 \u03b3)V \u03c0 * i P,\u03b3 \u2212 g * P | < ,(96)\n|(1 \u2212 \u03b3)V \u03c0j P,\u03b3 \u2212 g \u03c0j P | < .(97)\nIt hence implies that\n(1 \u2212 \u03b3)V \u03c0 * i P,\u03b3 \u2265 (d \u2212 2 ) + (1 \u2212 \u03b3)V \u03c0j P,\u03b3 > (1 \u2212 \u03b3)V \u03c0j P,\u03b3 ,(98)\nand V\n\u03c0 * i P,\u03b3 > V \u03c0j P,\u03b3 .(99)\nNote that from Theorem 3.1 in (Iyengar 2005), i.e., max \u03c0\u2208\u03a0 D V \u03c0 P,\u03b3 = V * P,\u03b3 , we have that for any \u03b3, there exists a deterministic policy \u03c0 \u2208 \u03a0 D , such that V * P,\u03b3 = V \u03c0 P,\u03b3 . Together with (99), it implies that all the possible optimal robust polices of V \u03c0 P,\u03b3 belong to {\u03c0 * 1 , ...\u03c0 * m }, i.e., the set \u03a0 * . Hence, there exists \u03c0 * j \u2208 \u03a0 * , such that V\n\u03c0 * j P,\u03b3 = max \u03c0\u2208\u03a0 D V \u03c0 P,\u03b3 = V * P,\u03b3 .(100)\nFor the second part, when the optimal robust policy of robust average-reward is unique, i.e., \u03a0 * = {\u03c0 * }. Then from the results above, there exists \u03b4 , such that for any \u03b3 > \u03b4 , V \u03c0 * P,\u03b3 > V \u03c0 P,\u03b3 for any \u03c0 * = \u03c0 \u2208 \u03a0 D , and hence \u03c0 * is the optimal policy for discounted robust MDPs, which is the unique Blackwell optimal policy.", "publication_ref": ["b19"], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Results for Direct Approach", "text": "Recall that V \u03c0 P (s) min\n\u03ba\u2208 t\u22650 P E \u03ba,\u03c0 \u221e t=0 (r t \u2212 g \u03c0 P ) S 0 = s ,(101)\nwhere\ng \u03c0 P = min \u03ba\u2208 t\u22650 P lim n\u2192\u221e E \u03ba,\u03c0 1 n n\u22121 t=0 r t |S 0 = s . (102\n)\nWe first show that the robust relative function is always finite. Lemma 9. For any \u03c0, V \u03c0 P is finite.\nProof. According to Theorem 9, V \u03c0 P = min P\u2208P V \u03c0 P = min P\u2208P E P,\u03c0 \u221e t=0 (r t \u2212 g \u03c0 P ) . Note that V \u03c0 P can be rewritten as\nV \u03c0 P = min P\u2208P E P,\u03c0 \u221e t=0 (r t \u2212 g \u03c0 P ) = min P\u2208P E P,\u03c0 lim n\u2192\u221e n t=0 (r t \u2212 g \u03c0 P ) = min P\u2208P E P,\u03c0 lim n\u2192\u221e n t=0 (r t \u2212 g \u03c0 P + g \u03c0 P \u2212 g \u03c0 P ) = min P\u2208P E P,\u03c0 lim n\u2192\u221e (R n \u2212 ng \u03c0 P + ng \u03c0 P \u2212 ng \u03c0 P ) ,(103)\nwhere R n = n t=0 r t . Note that for any P \u2208 P and n, ng \u03c0\nP \u2265 ng \u03c0 P , hence lim n\u2192\u221e (R n \u2212 ng \u03c0 P + ng \u03c0 P \u2212 ng \u03c0 P ) \u2265 lim n\u2192\u221e (R n \u2212 ng \u03c0 P ),(104)\nand thus the lower bound of V \u03c0 P can be derived as follows,\nV \u03c0 P \u2265 min P\u2208P E P,\u03c0 \u221e t=0 (r t \u2212 g \u03c0 P ) = min P\u2208P V \u03c0 P = min P\u2208P H \u03c0 P r \u03c0 .(105)\nwhich is finite due to the fact that H \u03c0 P is continuous on the compact set P. From Theorem 9, we denote the stationary worst-case transition kernel of g \u03c0 P by P g . Then the upper bound of V \u03c0 P can be bounded by noting that\nV \u03c0 P = min P\u2208P E P,\u03c0 \u221e t=0 (r t \u2212 g \u03c0 Pg ) \u2264 E Pg,\u03c0 \u221e t=0 (r t \u2212 g \u03c0 Pg ) = V \u03c0 Pg ,(106)\nwhich is also finite and P g denotes the worst-case transition kernel of g \u03c0 P . Hence we show that V \u03c0 P is finite for any \u03c0 and hence complete the proof.\nAfter showing that the robust relative value function is well-defined, we show the following robust Bellman equation for average-reward robust MDPs. Theorem 14 (Restatement of Theorem 6). For any s and \u03c0, (V \u03c0 P , g \u03c0 P ) is a solution to the following robust Bellman equation:\nV (s) + g = a \u03c0(a|s) r(s, a) + \u03c3 P a s (V ) .(107)\nProof. From the definition,\nV \u03c0 P (s) = min \u03ba\u2208 t\u22650 P E \u03ba,\u03c0 \u221e t=0 (r t \u2212 g \u03c0 P ) S 0 = s ,(108)\nhence V \u03c0 P (s) = min \u03ba\u2208 t\u22650 P E \u03ba,\u03c0 \u221e t=0 (r t \u2212 g \u03c0 P ) S 0 = s = min \u03ba\u2208 t\u22650 P E \u03ba,\u03c0 (r 0 \u2212 g \u03c0 P ) + \u221e t=1 (r t \u2212 g \u03c0 P ) S 0 = s = min \u03ba\u2208 t\u22650 P a \u03c0(a|s)r(s, a) \u2212 g \u03c0 P + E \u03ba,\u03c0 \u221e t=1 (r t \u2212 g \u03c0 P ) S 0 = s = a \u03c0(a|s) (r(s, a) \u2212 g \u03c0 P ) + min \u03ba\u2208 t\u22650 P \uf8f1 \uf8f2 \uf8f3 a,s \u03c0(a|s)P a s,s E \u03ba,\u03c0 \u221e t=1 (r t \u2212 g \u03c0 P )|S 1 = s \uf8fc \uf8fd \uf8fe = a \u03c0(a|s) (r(s, a) \u2212 g \u03c0 P ) + min P0\u2208P min \u03ba=(P1,...)\u2208 t\u22651 P \uf8f1 \uf8f2 \uf8f3 a,s \u03c0(a|s)(P 0 ) a s,s E \u03ba,\u03c0 \u221e t=1 (r t \u2212 g \u03c0 P )|S 1 = s \uf8fc \uf8fd \uf8fe = a \u03c0(a|s) (r(s, a) \u2212 g \u03c0 P ) + min P0\u2208P \uf8f1 \uf8f2 \uf8f3 a,s \u03c0(a|s)(P 0 ) a s,s min \u03ba=(P1,...)\u2208 t\u22651 P E \u03ba,\u03c0 \u221e t=1 (r t \u2212 g \u03c0 P )|S 1 = s \uf8fc \uf8fd \uf8fe = a \u03c0(a|s) (r(s, a) \u2212 g \u03c0 P ) + a \u03c0(a|s) s min p a s,s \u2208P a s p a s,s V \u03c0 P (s ) = a \u03c0(a|s) (r(s, a) \u2212 g \u03c0 P ) + a \u03c0(a|s)\u03c3 P a s (V \u03c0 P ) = a \u03c0(a|s) r(s, a) \u2212 g \u03c0 P + \u03c3 P a s (V \u03c0 P ) . (109\n)\nThis hence completes the proof.\nTheorem 15. [Restatement of Theorem 7, Part 1] For any (g, V ) that is a solution to max a r(s, a) \u2212 g + \u03c3 P a s (V ) \u2212 V (s) = 0, \u2200s, then g = g * P . Proof. In this proof, for two vectors v, w \u2208 R n , v \u2265 w denotes that v(s) \u2265 w(s) entry-wise.\nLet\nB(g, V )(s) max a r(s, a) \u2212 g + \u03c3 P a s (V ) \u2212 V (s) . Since (g, V\n) is a solution to (13), hence for any a \u2208 A and any s \u2208 S, r(s, a)\n\u2212 g + \u03c3 P a s (V ) \u2212 V (s) \u2264 0,(110)\nfrom which it follows that for any policy \u03c0,\ng(s) \u2265 r \u03c0 (s) + a \u03c0(a|s)\u03c3 P a s (V ) \u2212 V (s) r \u03c0 (s) + a \u03c0(a|s)(p a s ) V \u2212 V (s),(111)\nwhere r \u03c0 (s) a \u03c0(a|s)r(s, a), p a s arg min p\u2208P a s p V , and P V = {p a s : s \u2208 S, a \u2208 A}. We also denotes the state transition matrix induced by \u03c0 and P V by P \u03c0 V . Using these notations, and rewrite eq. (111), we have that g1 \u2265 r \u03c0 + (P \u03c0 V \u2212 I)V.\nSince the inequality in eq. ( 112) holds entry-wise, all entries of P \u03c0 V are positive, then by multiplying both sides of eq. (112) by P \u03c0 V , we have that g1 = gP \u03c0 V 1 \u2265 P \u03c0\nV r \u03c0 + P \u03c0 V (P \u03c0 V \u2212 I)V.\nMultiplying the both sides of eq. ( 113) by P \u03c0 V , and repeatedly doing that, we have that g1 \u2265 (P \u03c0 V ) 2 r \u03c0 + (P \u03c0 V ) 2 (P \u03c0 V \u2212 I)V,\n. . . . . .\ng1 \u2265 (P \u03c0 V ) n\u22121 r \u03c0 + (P \u03c0 V ) n\u22121 (P \u03c0 V \u2212 I)V.\nSumming up these inequalities from eq. (112) to eq. ( 116), we have that ng1 \u2265 (I + P \u03c0 V + ... + (P \u03c0 V ) n\u22121 )r \u03c0 + (I + P \u03c0 V + ... + (P \u03c0 V ) n\u22121 )(P \u03c0 V \u2212 I)V,\nand from which, it follows that g1 \u2265 1 n (I + P \u03c0 V + ... + (P \u03c0 V ) n\u22121 )r \u03c0 + 1 n (I + P \u03c0 V + ... + (P \u03c0 V ) n\u22121 )(P \u03c0 V \u2212 I)V = 1 n (I + P \u03c0 V + ... + (P \u03c0 V ) n\u22121 )r \u03c0 + 1 n ((P \u03c0 V ) n \u2212 I)V.\nIt can be easily verified that lim n\u2192\u221e 1 n ((P \u03c0 V ) n \u2212 I)V = 0, and hence it implies that g1 \u2265 lim n\u2192\u221e 1 n (I + P \u03c0 V + ... + (P \u03c0 V ) n\u22121 )r \u03c0 = lim\nn\u2192\u221e 1 n E P \u03c0 V ,\u03c0 n t=0 r t = g \u03c0 P \u03c0 V 1 \u2265 g \u03c0 P 1.(119)\nSince eq. ( 119) holds for any policy \u03c0, it follows that g \u2265 g * P . On the other hand, since B(g, V ) = 0, there exists a policy \u03c4 such that g1 = r \u03c4 + (P \u03c4 V \u2212 I)V,\nwhere r \u03c4 , P \u03c4 V are similarly defined as for \u03c0. From Theorem 9, there exists a stationary transition kernel P \u03c4 ave such that g \u03c4 P = g \u03c4 P \u03c4\nave . We denote the state transition matrix induced by \u03c4 and P \u03c4 ave by P \u03c4 . Then because P \u03c4 V is the worst-case transition of V , it follows that\nP \u03c4 V V \u2264 P \u03c4 V.(121)\nThus g1 \u2264 r \u03c4 + (P \u03c4 \u2212 I)V.\nSimilarly, we have that g1 \u2264 (P \u03c4 ) j\u22121 r \u03c4 + (P \u03c4 ) j\u22121 (P \u03c4 \u2212 I)V,\nfor j = 2, ..., n. Summing these inequalities together we have that ng1 \u2264 (I + P \u03c4 + ... + (P \u03c4 ) n\u22121 )r \u03c4 + (I + P \u03c4 + ... + (P \u03c4 ) n\u22121 )(P \u03c4 ) n\u22121 (P \u03c4 \u2212 I)V = (I + P \u03c4 + ... + (P \u03c4 ) n\u22121 )r \u03c4 + ((P \u03c4 ) n \u2212 I)V.\nHence\ng1 \u2264 lim n\u2192\u221e 1 n E P \u03c4 ave ,\u03c4 n t=0 r t = g \u03c4 P \u03c4 ave 1 = g \u03c4 P 1 \u2264 g * P 1.(125)\nThus g = g * P , and this concludes the proof. Theorem 16 (Restatement of Theorem 7, Part 2). For any (g, V ) that is a solution to \nfor any s \u2208 S, then \u03c0 * is an optimal robust policy.\nProof. Note that for any stationary policy \u03c0, we denote by \u03c3 P \u03c0 (V ) ( a \u03c0(a|s 1 )\u03c3 P a s 1 (V ), ..., a \u03c0(a|s |S| )\u03c3 P a s |S| (V )) being a vector in R |S| . Then eq. ( 14) is equivalent to r \u03c0 * + \u03c3 P \u03c0 * (V ) = max \u03c0 {r \u03c0 + \u03c3 P \u03c0 (V )} .\nHence,\nr \u03c0 * \u2212 g + \u03c3 P \u03c0 * (V ) \u2212 V = max \u03c0 {r \u03c0 \u2212 g + \u03c3 P \u03c0 (V ) \u2212 V } .(129)\nSince (g, V ) is a solution to (13), it follows that r \u03c0 * \u2212 g + \u03c3 P \u03c0 * (V ) \u2212 V = 0.\nAccording to the robust Bellman equation eq. ( 12), (g \u03c0 * P , V \u03c0 * P ) is a solution to eq. ( 130). Thus from Theorem 15, g \u03c0 * P = g * P , and hence \u03c0 * is an optimal robust policy.\nTheorem 17 (Restatement of Theorem 8). (w T , V t ) in Algorithm 3 converges to a solution of eq. (13).\nProof. We first denote the update operator as \nmin{p i , q i } Then n i=1 p i x i \u2212 n i=1 q i x i = n i=1 (p i \u2212 b i )x i \u2212 n i=1 (q i \u2212 b i )x i \u2264 n i=1 (p i \u2212 b i ) max{x i } \u2212 n i=1 (q i \u2212 b i ) min{x i } = n i=1 (p i \u2212 b i )sp(x) + n i=1 (p i \u2212 b i ) \u2212 n i=1 (q i \u2212 b i ) min{x i } = 1 \u2212 n i=1 b i sp(x).(136)\nThus we showed that\nsp(Lv \u2212 Lu) \u2264 1 \u2212 n i=1 b i sp(v \u2212 u).(137)\nNow from Assumption 2, and following Theorem 8.5.3 from (Puterman 1994), it can be shown that there exists 1 > \u03bb > 0, such that for any a, u, v,\nn i=1 b i \u2265 \u03bb.(138)\nFurther, following Theorem 8.5.2 in (Puterman 1994), it can be shown that L is a J-step contraction operator for some integer J, i.e.,\nsp(L J v \u2212 L J u) \u2264 (1 \u2212 \u03bb)sp(v \u2212 u).(139)\nThen, it can be shown that the relative value iteration converges to a solution of the optimal equation similar to the relative value iteration for non-robust MDPs under the average-reward criterion (Theorem 8.5.7 in (Puterman 1994), Section 1.6.4 in (Sigaud and Buffet 2013)), and hence (w t , V t ) converges to a solution to eq. ( 13) as \u2192 0.", "publication_ref": ["b31", "b31", "b31", "b39"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgment", "text": "This work was supported by the National Science Foundation under Grants CCF-2106560, CCF-2007783, CCF-2106339  and CCF-1552497.   ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Wasserstein robust reinforcement learning", "journal": "", "year": "2019", "authors": "M A Abdullah; H Ren; H B Ammar; V Milenkovic; R Luo; M Zhang; J Wang"}, {"ref_id": "b1", "title": "On the generation of Markov decision processes", "journal": "Journal of the Operational Research Society", "year": "1995", "authors": "T Archibald; K Mckinnon; L Thomas"}, {"ref_id": "b2", "title": "Steady-State Planning in Expected Reward Multichain MDPs", "journal": "Journal of Artificial Intelligence Research", "year": "2021", "authors": "G K Atia; A Beckus; I Alkhouri; A Velasquez"}, {"ref_id": "b3", "title": "Robust Reinforcement Learning using Least Squares Policy Iteration with Provable Performance Guarantees", "journal": "PMLR", "year": "2021", "authors": "K P Badrinath; D Kalathil"}, {"ref_id": "b4", "title": "Solving uncertain Markov decision processes", "journal": "", "year": "2001", "authors": "J A Bagnell; A Y Ng; J G Schneider"}, {"ref_id": "b5", "title": "Dynamic Programming and Optimal Control 3rd edition, volume II", "journal": "Athena Scientific", "year": "2011", "authors": "D P Bertsekas"}, {"ref_id": "b6", "title": "Discrete dynamic programming. The Annals of Mathematical Statistics", "journal": "", "year": "1962", "authors": "D Blackwell"}, {"ref_id": "b7", "title": "Learning Infinite-Horizon Average-Reward Markov Decision Processes with Constraints", "journal": "", "year": "2022", "authors": "L Chen; R Jain; H Luo"}, {"ref_id": "b8", "title": "Finite state Markovian decision processes", "journal": "Academic Press, Inc", "year": "1970", "authors": "C Derman"}, {"ref_id": "b9", "title": "Twice regularized MDPs and the equivalence between robustness and regularization", "journal": "", "year": "2021", "authors": "E Derman; M Geist; S Mannor"}, {"ref_id": "b10", "title": "Maximum entropy RL (provably) solves some robust RL problems", "journal": "", "year": "2021", "authors": "B Eysenbach; S Levine"}, {"ref_id": "b11", "title": "Robust Markov decision process: Beyond rectangularity", "journal": "", "year": "2018", "authors": "V Goyal; J Grand-Clement"}, {"ref_id": "b12", "title": "Fast Bellman updates for robust MDPs", "journal": "PMLR", "year": "2018", "authors": "C P Ho; M Petrik; W Wiesemann"}, {"ref_id": "b13", "title": "Partial policy iteration for L1-robust Markov decision processes", "journal": "Journal of Machine Learning Research", "year": "2021", "authors": "C P Ho; M Petrik; W Wiesemann"}, {"ref_id": "b14", "title": "Blackwell optimality", "journal": "Springer", "year": "2002", "authors": "A Hordijk; A A Yushkevich"}, {"ref_id": "b15", "title": "Robust Reinforcement Learning with Wasserstein Constraint", "journal": "", "year": "2020", "authors": "L Hou; L Pang; X Hong; Y Lan; Z Ma; D Yin"}, {"ref_id": "b16", "title": "Kullback-Leibler divergence constrained distributionally robust optimization. Available at Optimization Online", "journal": "", "year": "2013", "authors": "Z Hu; L J Hong"}, {"ref_id": "b17", "title": "Adversarial attacks on neural network policies", "journal": "", "year": "2017", "authors": "S Huang; N Papernot; I Goodfellow; Y Duan; P Abbeel"}, {"ref_id": "b18", "title": "A Robust Version of the Probability Ratio Test", "journal": "Ann. Math. Statist", "year": "1965", "authors": "P J Huber"}, {"ref_id": "b19", "title": "Mathematics of Operations Research", "journal": "", "year": "2005", "authors": "G N Iyengar"}, {"ref_id": "b20", "title": "Robust modified policy iteration", "journal": "INFORMS Journal on Computing", "year": "2013", "authors": "D L Kaufman; A J Schaefer"}, {"ref_id": "b21", "title": "Reinforcement Learning in Robotics: A Survey", "journal": "The International Journal of Robotics Research", "year": "2013", "authors": "J Kober; J A Bagnell; J Peters"}, {"ref_id": "b22", "title": "Delving into adversarial attacks on deep policies", "journal": "", "year": "2017", "authors": "J Kos; D Song"}, {"ref_id": "b23", "title": "First-order and Stochastic Optimization Methods for Machine Learning", "journal": "Springer Nature", "year": "2020", "authors": "G Lan"}, {"ref_id": "b24", "title": "Kernel-based reinforcement learning in robust Markov decision processes", "journal": "PMLR", "year": "2019", "authors": "S H Lim; A Autef"}, {"ref_id": "b25", "title": "Reinforcement learning in robust Markov decision processes", "journal": "", "year": "2013", "authors": "S H Lim; H Xu; S Mannor"}, {"ref_id": "b26", "title": "Tactics of adversarial attack on deep reinforcement learning agents", "journal": "", "year": "2017", "authors": "Y.-C Lin; Z.-W Hong; Y.-H Liao; M.-L Shih; M.-Y Liu; M Sun"}, {"ref_id": "b27", "title": "Adversarially robust policy learning: Active construction of physically-plausible perturbations", "journal": "", "year": "2004", "authors": "A Mandlekar; Y Zhu; A Garg; L Fei-Fei; S Savarese; L El Ghaoui"}, {"ref_id": "b28", "title": "Sample Complexity of Robust Reinforcement Learning with a Generative Model", "journal": "", "year": "2021", "authors": "K Panaganti; D Kalathil"}, {"ref_id": "b29", "title": "Robust Deep Reinforcement Learning with Adversarial Attacks", "journal": "", "year": "2018", "authors": "A Pattanaik; Z Tang; S Liu; G Bommannan; G Chowdhary"}, {"ref_id": "b30", "title": "Robust adversarial reinforcement learning", "journal": "PMLR", "year": "2017", "authors": "L Pinto; J Davidson; R Sukthankar; A Gupta"}, {"ref_id": "b31", "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "journal": "", "year": "1994", "authors": "M L Puterman"}, {"ref_id": "b32", "title": "Effective scenarios in multistage distributionally robust optimization with a focus on total variation distance", "journal": "SIAM Journal on Optimization", "year": "2022", "authors": "H Rahimian; G Bayraksan; T H De-Mello"}, {"ref_id": "b33", "title": "Epopt: Learning robust neural network policies using model ensembles", "journal": "", "year": "2017", "authors": "A Rajeswaran; S Ghotra; B Ravindran; S Levine"}, {"ref_id": "b34", "title": "Reinforcement learning under model mismatch", "journal": "", "year": "2017", "authors": "A Roy; H Xu; S Pokutta"}, {"ref_id": "b35", "title": "Functional Analysis. McGraw-Hill Science &Engineering &Math", "journal": "", "year": "2022", "authors": "W Rudin"}, {"ref_id": "b36", "title": "Robust Constrained-MDPs: Soft-Constrained Robust Policy Optimization under Model Uncertainty", "journal": "", "year": "2020", "authors": "R H Russel; M Benosman; J Van Baar"}, {"ref_id": "b37", "title": "Markovian decision processes with uncertain transition probabilities", "journal": "Operations Research", "year": "1973", "authors": "J K Satia; R E Lave"}, {"ref_id": "b38", "title": "Distributionally robust policy evaluation and learning in offline contextual bandits", "journal": "PMLR", "year": "2020", "authors": "N Si; F Zhang; Z Zhou; J Blanchet"}, {"ref_id": "b39", "title": "Markov decision processes in artificial intelligence", "journal": "John Wiley & Sons", "year": "2013", "authors": "O Sigaud; O Buffet"}, {"ref_id": "b40", "title": "Reinforcement Learning: An Introduction", "journal": "The MIT Press", "year": "2018", "authors": "R S Sutton; A G Barto"}, {"ref_id": "b41", "title": "Scaling up robust MDPs using function approximation", "journal": "PMLR", "year": "2014", "authors": "A Tamar; S Mannor; H Xu"}, {"ref_id": "b42", "title": "Action robust reinforcement learning and applications in continuous control", "journal": "PMLR", "year": "2019", "authors": "C Tessler; Y Efroni; S Mannor"}, {"ref_id": "b43", "title": "Bounded parameter Markov decision processes with average reward criterion", "journal": "Springer", "year": "2007", "authors": "A Tewari; P L Bartlett"}, {"ref_id": "b44", "title": "Learning and planning in average-reward markov decision processes", "journal": "PMLR", "year": "2020", "authors": "E Vinitsky; Y Du; K Parvate; K Jang; P Abbeel; A Bayen; Y Wan; A Naik; R S Sutton"}, {"ref_id": "b45", "title": "Online Robust Reinforcement Learning with Model Uncertainty", "journal": "", "year": "2021", "authors": "Y Wang; S Zou"}, {"ref_id": "b46", "title": "Policy Gradient Method For Robust Reinforcement Learning", "journal": "PMLR", "year": "2022", "authors": "Y Wang; S Zou"}, {"ref_id": "b47", "title": "", "journal": "", "year": "", "authors": "C.-Y Wei; M J Jahromi; H Luo; H Sharma; R Jain"}, {"ref_id": "b48", "title": "Model-free reinforcement learning in infinite-horizon average-reward markov decision processes", "journal": "", "year": "2013", "authors": "W Pmlr. Wiesemann; D Kuhn; B Rustem"}, {"ref_id": "b49", "title": "Distributionally Robust Markov Decision Processes", "journal": "", "year": "2010", "authors": "H Xu; S Mannor"}, {"ref_id": "b50", "title": "Towards Theoretical Understandings of Robust Markov Decision Processes", "journal": "", "year": "2021", "authors": "W Yang; L Zhang; Z Zhang"}, {"ref_id": "b51", "title": "Distributionally robust counterpart in Markov decision processes", "journal": "IEEE Transactions on Automatic Control", "year": "2015", "authors": "P Yu; H Xu"}, {"ref_id": "b52", "title": "Finite Sample Analysis of Average-Reward TD Learning and Q-Learning", "journal": "", "year": "2021", "authors": "S Zhang; Z Zhang; S T Maguluri"}, {"ref_id": "b53", "title": "On-policy deep reinforcement learning for the average-reward criterion", "journal": "PMLR", "year": "2021", "authors": "Y Zhang; K W Ross"}, {"ref_id": "b54", "title": "Finite-Sample Regret Bound for Distributionally Robust Offline Tabular Reinforcement Learning", "journal": "PMLR", "year": "2021", "authors": "Z Zhou; Q Bai; Z Zhou; L Qiu; J Blanchet; P Glynn"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Comparison on contamination model with R = 0.4.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 2 :2Figure 2: Comparison on total variation model with R = 0.6.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 3 :3Figure 3: Comparison on KL-divergence model with R = 0.8.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Algorithm 4 :4Policy evaluation for robust discounted MDPs Input: \u03c0, V 0 , T 1: for t = 0, 1, ..., T \u2212 1 do 2:", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "a) \u2212 g + \u03c3 P a s (V ) \u2212 V (s) = 0, \u2200s,(126)if we set \u03c0 * (s) = arg max a r(s, a) + \u03c3 P a s (V )", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Lv(s) max a (r(s, a) + \u03c3 P a s (v)). (131) Now, consider sp(Lv \u2212 Lu). Denote by\u015b arg max s (Lv(s) \u2212 Lu(s)) ands arg min s (Lv(s) \u2212 Lu(s)). Also denote by a v arg max a (r(\u015b, a) + \u03c3 P \u00e1 s (v)) and a u arg max a (r(\u015b,a) + \u03c3 P \u00e1 s (u)) Then Lv(\u015b) \u2212 Lu(\u015b) = max a (r(\u015b, a) + \u03c3 P \u00e1 s (v)) \u2212 max a (r(\u015b, a) + \u03c3 P \u00e1 s (u)) r(\u015b, a v ) + \u03c3 P av s (v) \u2212 (r(\u015b, a u ) + \u03c3 P a\u00fa s (u)) \u2264 r(\u015b, a v ) + \u03c3 P av s (v) \u2212 (r(\u015b, a v ) + \u03c3 P av s (u)) = \u03c3 P av s (v) \u2212 \u03c3 Pav s p\u2208P av s p v and p av,\u00fa s = arg min p\u2208P av s p u. Thus eq. (132) can be further bounded as Lv(\u015b) \u2212 Lu(\u015b) v \u2212 u (x 1 , x 2 , ..., x n ), p av,\u00fa s = (p 1 , ..., p n ) and p au,v s = (q 1 , ..., q n ). Further denote by b i", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "V \u03c0 P,\u03b3 (s) E \u03c0,P [ \u221e t=0 \u03b3 t r t |S 0 = s].", "formula_coordinates": [3.0, 330.8, 66.48, 146.85, 14.11]}, {"formula_id": "formula_1", "formula_text": "g \u03c0 P (s) lim n\u2192\u221e E \u03c0,P 1 n n\u22121 t=0 r t |S 0 = s ,(1)", "formula_coordinates": [3.0, 360.13, 161.59, 197.87, 30.2]}, {"formula_id": "formula_2", "formula_text": "g \u03c0 P = lim n\u2192\u221e 1 n n\u22121 t=0 (P \u03c0 ) t r \u03c0 P \u03c0", "formula_coordinates": [3.0, 319.5, 232.96, 227.24, 27.15]}, {"formula_id": "formula_3", "formula_text": "V \u03c0 P (s) E \u03c0,P \u221e t=0 (r t \u2212 g \u03c0 P )|S 0 = s ,(2)", "formula_coordinates": [3.0, 360.07, 324.52, 197.93, 30.2]}, {"formula_id": "formula_4", "formula_text": ": V \u03c0 P = H \u03c0 P r \u03c0 , where H \u03c0 P (I \u2212 P \u03c0 + P \u03c0 * ) \u22121 (I \u2212 P \u03c0 * )", "formula_coordinates": [3.0, 319.5, 385.95, 240.43, 23.15]}, {"formula_id": "formula_5", "formula_text": "V \u03c0 P (s) = E \u03c0 r(s, A) \u2212 g \u03c0 P (s) + s \u2208S p A s,s V \u03c0 P (s ) . (3)", "formula_coordinates": [3.0, 330.06, 455.85, 227.94, 22.27]}, {"formula_id": "formula_6", "formula_text": "\u03ba\u2208 t\u22650 P E \u03c0,\u03ba \u221e t=0 \u03b3 t r t |S 0 = s , (4", "formula_coordinates": [3.0, 390.59, 655.38, 163.54, 30.2]}, {"formula_id": "formula_7", "formula_text": ")", "formula_coordinates": [3.0, 554.13, 666.11, 3.87, 8.64]}, {"formula_id": "formula_8", "formula_text": "g \u03c0 P (s) min \u03ba\u2208 t\u22650 P lim n\u2192\u221e E \u03c0,\u03ba 1 n n\u22121 t=0 r t |S 0 = s , (5)", "formula_coordinates": [4.0, 66.67, 86.45, 225.83, 30.2]}, {"formula_id": "formula_9", "formula_text": "T \u03c0 V (s) a\u2208A \u03c0(a|s) r(s, a) + \u03b3\u03c3 P a s (V ) ,(6)", "formula_coordinates": [4.0, 81.56, 201.25, 210.94, 20.23]}, {"formula_id": "formula_10", "formula_text": "max \u03c0\u2208\u03a0 g \u03c0 P (s), for any s \u2208 S,(7)", "formula_coordinates": [4.0, 120.12, 325.86, 172.38, 16.66]}, {"formula_id": "formula_11", "formula_text": "lim \u03b3\u21921 (1 \u2212 \u03b3)V \u03c0 P,\u03b3 = g \u03c0 P .(8)", "formula_coordinates": [4.0, 127.38, 476.9, 165.12, 16.38]}, {"formula_id": "formula_13", "formula_text": "lim \u03b3\u21921 (1 \u2212 \u03b3)V \u03c0 P,\u03b3 = g \u03c0 P uniformly.(10)", "formula_coordinates": [4.0, 372.05, 504.16, 185.95, 16.38]}, {"formula_id": "formula_14", "formula_text": "Algorithm 1: Robust VI: Policy Evaluation Input: \u03c0, V 0 (s) = 0, \u2200s, T 1: for t = 0, 1, ..., T \u2212 1 do 2: \u03b3 t \u2190 t+1 t+2 3:", "formula_coordinates": [5.0, 53.64, 205.1, 171.27, 58.6]}, {"formula_id": "formula_15", "formula_text": "4: V t+1 (s) \u2190 E \u03c0 [(1 \u2212 \u03b3 t )r(s, A) + \u03b3 t \u03c3 P A s (V t )]", "formula_coordinates": [5.0, 58.98, 265.92, 214.84, 11.73]}, {"formula_id": "formula_16", "formula_text": "Algorithm 2: Robust VI: Optimal Control Input: V 0 (s) = 0, \u2200s, T 1: for t = 0, 1, ..., T \u2212 1 do 2: \u03b3 t \u2190 t+1 t+2 3: for all s \u2208 S do 4: V t+1 (s) \u2190 max a\u2208A (1 \u2212 \u03b3 t )r(s, a) + \u03b3 t \u03c3 P a s (V t ) 5:", "formula_coordinates": [5.0, 53.64, 520.03, 225.1, 85.51]}, {"formula_id": "formula_17", "formula_text": "8: \u03c0 T (s) \u2190 arg max a\u2208A (1 \u2212 \u03b3 t )r(s, a) + \u03b3 t \u03c3 P a", "formula_coordinates": [5.0, 58.98, 629.67, 216.63, 10.04]}, {"formula_id": "formula_18", "formula_text": "f \u03c0,\u03bd (\u03b3) min P\u2208M V \u03c0 P,\u03b3 \u2212 min P\u2208M V \u00b5 P,\u03b3", "formula_coordinates": [5.0, 394.14, 636.5, 162.93, 13.91]}, {"formula_id": "formula_19", "formula_text": "V \u03c0 P (s) min \u03ba\u2208 t\u22650 P E \u03ba,\u03c0 \u221e t=0 (r t \u2212 g \u03c0 P )|S 0 = s , (11)", "formula_coordinates": [6.0, 66.33, 279.5, 226.17, 30.2]}, {"formula_id": "formula_20", "formula_text": "V (s) + g = a \u03c0(a|s) r(s, a) + \u03c3 P a s (V ) .(12)", "formula_coordinates": [6.0, 75.06, 387.92, 217.44, 19.61]}, {"formula_id": "formula_21", "formula_text": "max a r(s, a) \u2212 g + \u03c3 P a s (V ) \u2212 V (s) = 0, \u2200s,(13)", "formula_coordinates": [6.0, 70.75, 633.88, 221.75, 14.13]}, {"formula_id": "formula_22", "formula_text": "g = g * P . If we further set \u03c0 * (s) = arg max a r(s, a) + \u03c3 P a s (V )(14)", "formula_coordinates": [6.0, 54.0, 653.52, 238.5, 34.45]}, {"formula_id": "formula_23", "formula_text": "1: w 0 \u2190 V 0 \u2212 V 0 (s * )1 2: while sp(w t \u2212 w t+1 ) \u2265 do 3:", "formula_coordinates": [6.0, 324.48, 204.94, 128.46, 32.23]}, {"formula_id": "formula_24", "formula_text": "w t+1 (s) \u2190 V t+1 (s) \u2212 V t+1 (s * ) 6:", "formula_coordinates": [6.0, 324.48, 248.77, 160.93, 21.27]}, {"formula_id": "formula_25", "formula_text": "P a s (V ) = (1 \u2212 R)(p a s ) V + R min s V (s).", "formula_coordinates": [7.0, 52.83, 355.76, 239.66, 23.56]}, {"formula_id": "formula_26", "formula_text": "P a s = {q : D T V (q||p a s ) \u2264 R}.", "formula_coordinates": [7.0, 54.0, 554.94, 238.5, 23.15]}, {"formula_id": "formula_27", "formula_text": "\u03c3 P a s (V ) = p V \u2212 R min \u00b5\u22650 {max s (V (s) \u2212 \u00b5(s)) \u2212 min s (V (s) \u2212 \u00b5(s))} .", "formula_coordinates": [7.0, 54.0, 578.43, 238.5, 20.61]}, {"formula_id": "formula_28", "formula_text": "3: V t+1 (s) \u2190 E \u03c0 [r(s, A) + \u03b3\u03c3 P A s (V t )] 4:", "formula_coordinates": [10.0, 58.98, 219.85, 178.32, 19.7]}, {"formula_id": "formula_29", "formula_text": "\u03c0 * = arg max \u03c0 V \u03c0 P,\u03b3 .(15)", "formula_coordinates": [10.0, 265.27, 316.54, 292.73, 16.21]}, {"formula_id": "formula_30", "formula_text": "Input: V 0 , T 1: for t = 0, 1, ..., T \u2212 1 do 2:", "formula_coordinates": [10.0, 54.0, 380.41, 117.7, 32.72]}, {"formula_id": "formula_31", "formula_text": "V \u03c0 P,\u03b3 (s) = min \u03ba\u2208 t\u22650 P E \u03c0,\u03ba \u221e t=0 \u03b3 t r t |S 0 = s = min P\u2208P E \u03c0,P \u221e t=0 \u03b3 t r t |S 0 = s .(16)", "formula_coordinates": [10.0, 215.43, 677.13, 175.32, 30.2]}, {"formula_id": "formula_32", "formula_text": "min P\u2208P lim n\u2192\u221e E \u03c0,P 1 n n\u22121 t=0 r t S 0 = s . (17", "formula_coordinates": [11.0, 233.87, 133.2, 319.98, 30.2]}, {"formula_id": "formula_33", "formula_text": ")", "formula_coordinates": [11.0, 553.85, 143.93, 4.15, 8.64]}, {"formula_id": "formula_34", "formula_text": "g \u03c0 P (s) min \u03ba\u2208 t\u22650 P lim n\u2192\u221e E \u03ba,\u03c0 1 n n\u22121 t=0 r t |S 0 = s = min P\u2208P lim n\u2192\u221e E P,\u03c0 1 n n\u22121 t=0 r t S 0 = s . (18", "formula_coordinates": [11.0, 207.44, 227.34, 346.41, 64.76]}, {"formula_id": "formula_35", "formula_text": ")", "formula_coordinates": [11.0, 553.85, 272.63, 4.15, 8.64]}, {"formula_id": "formula_36", "formula_text": "V \u03c0 P (s) min \u03ba\u2208 t\u22650 P E \u03ba,\u03c0 \u221e t=0 (r t \u2212 g \u03c0 P )|S 0 = s = min P\u2208P E P,\u03c0 \u221e t=0 (r t \u2212 g \u03c0 P )|S 0 = s .(19)", "formula_coordinates": [11.0, 208.77, 317.02, 349.23, 63.27]}, {"formula_id": "formula_37", "formula_text": "V \u03c0 P (s) + g \u03c0 P = a \u03c0(a|s) r(s, a) + \u03c3 P a s (V \u03c0 P ) . (20", "formula_coordinates": [11.0, 207.81, 409.6, 346.04, 21.69]}, {"formula_id": "formula_38", "formula_text": ")", "formula_coordinates": [11.0, 553.85, 411.99, 4.15, 8.64]}, {"formula_id": "formula_39", "formula_text": "V \u03c0 P (s) = a \u03c0(a|s) r(s, a) \u2212 g \u03c0 P + \u03c3 P a s (V \u03c0 P ) = a \u03c0(a|s)(r(s, a) \u2212 g \u03c0 P ) + a \u03c0(a|s)E P \u03c0 [V \u03c0 P (S 1 )|S 0 = s, A 0 = a] = a \u03c0(a|s)(r(s, a) \u2212 g \u03c0 P ) + E P \u03c0 ,\u03c0 [V \u03c0 P (S 1 )|S 0 = s] = a \u03c0(a|s)(r(s, a) \u2212 g \u03c0 P ) + E P \u03c0 ,\u03c0 a \u03c0(a|S 1 )(r(S 1 , a) \u2212 g \u03c0 P )|S 0 = s + E P \u03c0 ,\u03c0 a \u03c0(a|S 1 )\u03c3 P a S 1 (V \u03c0 P )|S 0 = s = a \u03c0(a|s)(r(s, a) \u2212 g \u03c0 P ) + E P \u03c0 ,\u03c0 [r 1 \u2212 g \u03c0 P |S 0 = s] + E P \u03c0 ,\u03c0 \u03c3 P A 1 S 1 (V \u03c0 P )|S 0 = s = a \u03c0(a|s)(r(s, a) \u2212 g \u03c0 P ) + E P \u03c0 ,\u03c0 r 1 \u2212 g \u03c0 P S 0 = s + E P \u03c0 ,\u03c0 (p A1 S1 ) V \u03c0 P |S 0 = s = E P \u03c0 ,\u03c0 r 0 \u2212 g \u03c0 P + r 1 \u2212 g \u03c0 P |S 0 = s + E P \u03c0 ,\u03c0 [V \u03c0 P (S 2 )|S 0 = s] ...... = E P \u03c0 ,\u03c0 \u221e t=0 (r t \u2212 g \u03c0 P )|s . (21", "formula_coordinates": [11.0, 55.62, 462.39, 495.49, 209.27]}, {"formula_id": "formula_40", "formula_text": ")", "formula_coordinates": [12.0, 553.85, 63.97, 4.15, 8.64]}, {"formula_id": "formula_41", "formula_text": "min \u03ba\u2208 t\u22650 P E \u03ba,\u03c0 \u221e t=0 (r t \u2212 g \u03c0 P )|S 0 = s \u2264 min P\u2208P E P,\u03c0 \u221e t=0 (r t \u2212 g \u03c0 P )|S 0 = s .(22)", "formula_coordinates": [12.0, 153.26, 105.92, 404.74, 30.2]}, {"formula_id": "formula_42", "formula_text": "V \u03c0 P \u03c0 (s) = a \u03c0(a|s)(r(s, a) \u2212 g \u03c0 P \u03c0 ) + E P \u03c0 ,\u03c0 [V \u03c0 P \u03c0 (S 1 )|s].(23)", "formula_coordinates": [12.0, 190.11, 195.21, 367.89, 21.69]}, {"formula_id": "formula_43", "formula_text": "V \u03c0 P (s) = V \u03c0 P \u03c0 (s) = a \u03c0(a|s)(r(s, a) \u2212 g \u03c0 P ) + E P \u03c0 ,\u03c0 [V \u03c0 P \u03c0 (S 1 )|s].(24)", "formula_coordinates": [12.0, 172.5, 241.73, 385.5, 21.69]}, {"formula_id": "formula_44", "formula_text": "A = sup x\u2208R d Ax \u221e x \u221e . Lemma 1. [Theorem 8.2.3 in (Puterman 1994)] For any P, \u03b3, \u03c0, V \u03c0 P,\u03b3 = 1 1 \u2212 \u03b3 g \u03c0 P + h \u03c0 P + f \u03c0 P (\u03b3),(25)", "formula_coordinates": [12.0, 54.0, 329.41, 504.0, 57.73]}, {"formula_id": "formula_45", "formula_text": "f \u03c0 P (\u03b3) = 1 \u03b3 \u221e n=1 (\u22121) n 1\u2212\u03b3 \u03b3 n (H \u03c0 P ) n+1 r \u03c0 .", "formula_coordinates": [12.0, 151.72, 392.74, 186.69, 18.54]}, {"formula_id": "formula_46", "formula_text": "S \u03c0 \u221e (P, \u03b3) 1 \u03b3 \u221e n=1 (\u22121) n 1 \u2212 \u03b3 \u03b3 n (H \u03c0 P ) n+1 r \u03c0 , S \u03c0 N (P, \u03b3) 1 \u03b3 N n=1 (\u22121) n 1 \u2212 \u03b3 \u03b3 n (H \u03c0 P ) n+1 r \u03c0 .(26)", "formula_coordinates": [12.0, 205.07, 473.54, 352.93, 65.03]}, {"formula_id": "formula_47", "formula_text": "lim N \u2192\u221e S \u03c0 N (P, \u03b3) = S \u03c0 \u221e (P, \u03b3) (27", "formula_coordinates": [12.0, 248.7, 576.16, 305.15, 16.65]}, {"formula_id": "formula_48", "formula_text": ")", "formula_coordinates": [12.0, 553.85, 578.55, 4.15, 8.64]}, {"formula_id": "formula_49", "formula_text": "uniformly on \u03a0 \u00d7 P \u00d7 [\u03b4, 1]. Proof. Note that H \u03c0 P \u2264 h, hence there exists \u03b4, s.t. 1 \u2212 \u03b4 \u03b4 h \u2264 k < 1 (28)", "formula_coordinates": [12.0, 54.0, 600.28, 504.0, 57.46]}, {"formula_id": "formula_50", "formula_text": "1 \u2212 \u03b3 \u03b3 h \u2264 1 \u2212 \u03b4 \u03b4 h \u2264 k. (29", "formula_coordinates": [12.0, 259.23, 679.68, 294.62, 22.31]}, {"formula_id": "formula_51", "formula_text": ")", "formula_coordinates": [12.0, 553.85, 686.74, 4.15, 8.64]}, {"formula_id": "formula_52", "formula_text": "1 \u03b3 (\u22121) n 1 \u2212 \u03b3 \u03b3 n (H \u03c0 P ) n+1 r \u2264 1 \u03b3 1 \u2212 \u03b3 \u03b3 n h n+1 \u2264 hk n \u03b4 M n ,(30)", "formula_coordinates": [13.0, 167.2, 71.58, 390.8, 25.51]}, {"formula_id": "formula_53", "formula_text": "A + B \u2264 A + B for induced l \u221e norm, Ax \u2264 A x and r \u03c0 \u221e \u2264 1. Note that \u221e n=1 M n = h \u03b4 k 1 \u2212 k ,(31)", "formula_coordinates": [13.0, 63.96, 106.4, 494.04, 55.24]}, {"formula_id": "formula_54", "formula_text": "S \u03c0 N (P, \u03b3 1 ) \u2212 S \u03c0 N (P, \u03b3 2 ) \u2264 L|\u03b3 1 \u2212 \u03b3 2 |, (32", "formula_coordinates": [13.0, 228.18, 203.66, 325.67, 12.69]}, {"formula_id": "formula_55", "formula_text": ")", "formula_coordinates": [13.0, 553.85, 206.05, 4.15, 8.64]}, {"formula_id": "formula_56", "formula_text": "for any N , \u03c0, P, \u03b3 1 , \u03b3 2 \u2208 [\u03b4, 1].", "formula_coordinates": [13.0, 54.0, 223.53, 124.42, 9.76]}, {"formula_id": "formula_57", "formula_text": "= N n=1 (\u22121) n 1\u2212\u03b3 \u03b3 n (H \u03c0 P ) n+1 r \u03c0 T \u03c0 N (P, \u03b3) is uniformly Lipschitz w.r.t. the l \u221e norm, i.e., T \u03c0 N (P, \u03b3 1 ) \u2212 T \u03c0 N (P, \u03b3 2 ) \u2264 l|\u03b3 1 \u2212 \u03b3 2 |,(33)", "formula_coordinates": [13.0, 54.0, 239.14, 504.0, 49.7]}, {"formula_id": "formula_58", "formula_text": "\u2207T \u03c0 N (P, \u03b3) = N n=1 (\u22121) n n 1 \u2212 \u03b3 \u03b3 n\u22121 \u22121 \u03b3 2 (H \u03c0 P ) n+1 r \u03c0 ,(34)", "formula_coordinates": [13.0, 190.74, 335.23, 367.26, 30.2]}, {"formula_id": "formula_59", "formula_text": "\u2207T \u03c0 N (P, \u03b3) \u2264 N n=1 n 1 \u2212 \u03b3 \u03b3 n\u22121 1 \u03b3 2 h n+1 l N (\u03b3). (35", "formula_coordinates": [13.0, 198.7, 388.82, 355.15, 30.2]}, {"formula_id": "formula_60", "formula_text": ") Note that h 1 \u2212 \u03b3 \u03b3 l N (\u03b3) = N n=1 n 1 \u2212 \u03b3 \u03b3 n 1 \u03b3 2 h n+2 ,(36)", "formula_coordinates": [13.0, 63.96, 399.55, 494.04, 73.06]}, {"formula_id": "formula_61", "formula_text": "1 \u2212 h 1 \u2212 \u03b3 \u03b3 l N (\u03b3) = N n=1 n 1 \u2212 \u03b3 \u03b3 n\u22121 1 \u03b3 2 h n+1 \u2212 N n=1 n 1 \u2212 \u03b3 \u03b3 n 1 \u03b3 2 h n+2 = 1 \u03b3 2 h 2 \u2212 N 1 \u2212 \u03b3 \u03b3 N 1 \u03b3 2 h N +2 + N n=2 1 \u2212 \u03b3 \u03b3 n\u22121 1 \u03b3 2 h n+1 \u2264 1 \u03b3 2 h 2 + h 2 \u03b3 2 1 \u2212 \u03b3 \u03b3 h 1 1 \u2212 1\u2212\u03b3 \u03b3 h = h 2 \u03b3 2 + h 2 \u03b3 2 1 \u2212 \u03b3 \u03b3 h 1 1 \u2212 1\u2212\u03b3 \u03b3 h . (37", "formula_coordinates": [13.0, 176.78, 497.23, 377.07, 155.6]}, {"formula_id": "formula_62", "formula_text": ")", "formula_coordinates": [13.0, 553.85, 633.71, 4.15, 8.64]}, {"formula_id": "formula_63", "formula_text": "\u2207T \u03c0 N (P, \u03b3) \u2264 l N (\u03b3) \u2264 1 1 \u2212 h 1\u2212\u03b3 \u03b3 h 2 \u03b3 2 + h 2 \u03b3 2 1 \u2212 \u03b3 \u03b3 h 1 1 \u2212 1\u2212\u03b3 \u03b3 h \u2264 1 1 \u2212 k h 2 \u03b4 2 + h 2 \u03b4 2 k 1 \u2212 k ,(38)", "formula_coordinates": [13.0, 172.36, 679.23, 263.18, 27.75]}, {"formula_id": "formula_64", "formula_text": "|S \u03c0 N (P, \u03b3 1 ) \u2212 S \u03c0 N (P, \u03b3 2 )| \u2264 |\u03b3 2 \u2212 \u03b3 1 | \u03b3 1 \u03b3 2 T \u03c0 N (P, \u03b3 1 ) + T \u03c0 N (P, \u03b3 1 ) \u2212 T \u03c0 N (P, \u03b3 2 ) \u03b3 2 . (39", "formula_coordinates": [14.0, 192.64, 112.97, 361.21, 40.04]}, {"formula_id": "formula_65", "formula_text": ")", "formula_coordinates": [14.0, 553.85, 136.85, 4.15, 8.64]}, {"formula_id": "formula_66", "formula_text": "T \u03c0 N (P, \u03b3) \u2264 N n=1 1 \u2212 \u03b3 \u03b3 n (H \u03c0 P ) n+1 r \u2264 N n=1 1 \u2212 \u03b3 \u03b3 n h n+1 \u2264 N n=1 k n h \u2264 h k 1 \u2212 k . (40", "formula_coordinates": [14.0, 221.9, 178.46, 331.95, 125.92]}, {"formula_id": "formula_67", "formula_text": ")", "formula_coordinates": [14.0, 553.85, 289.13, 4.15, 8.64]}, {"formula_id": "formula_68", "formula_text": "S \u03c0 N (P, \u03b3 1 ) \u2212 S \u03c0 N (P, \u03b3 2 ) = \u03b3 2 \u2212 \u03b3 1 \u03b3 1 \u03b3 2 T \u03c0 N (P, \u03b3 1 ) + T \u03c0 N (P, \u03b3 1 ) \u2212 T \u03c0 N (P, \u03b3 2 ) \u03b3 2 \u2264 1 \u03b4 2 h k 1 \u2212 k + 1 \u03b4 1 1 \u2212 k h 2 \u03b4 2 + h 2 \u03b4 2 k 1 \u2212 k |\u03b3 1 \u2212 \u03b3 2 | L|\u03b3 1 \u2212 \u03b3 2 |,(41)", "formula_coordinates": [14.0, 192.58, 326.26, 365.42, 84.32]}, {"formula_id": "formula_69", "formula_text": "L = 1 \u03b4 2 h k 1\u2212k + 1 \u03b4 1 1\u2212k h 2 \u03b4 2 + h 2 \u03b4 2 k 1\u2212k", "formula_coordinates": [14.0, 80.47, 419.74, 151.73, 15.1]}, {"formula_id": "formula_70", "formula_text": "S \u03c0 \u221e (P, \u03b3 1 ) \u2212 S \u03c0 \u221e (P, \u03b3 2 ) \u2264 L|\u03b3 1 \u2212 \u03b3 2 |.(42)", "formula_coordinates": [14.0, 227.3, 471.55, 330.7, 12.69]}, {"formula_id": "formula_71", "formula_text": "S \u03c0 \u221e (P, \u03b3) \u2212 S \u03c0 n (P, \u03b3) < .(43)", "formula_coordinates": [14.0, 252.29, 506.62, 305.71, 12.69]}, {"formula_id": "formula_72", "formula_text": "S \u03c0 \u221e (P, \u03b3 1 ) \u2212 S \u03c0 \u221e (P, \u03b3 2 ) \u2264 S \u03c0 \u221e (P, \u03b3 1 ) \u2212 S \u03c0 n (P, \u03b3 1 ) + S \u03c0 n (P, \u03b3 1 ) \u2212 S \u03c0 n (P, \u03b3 2 ) + S \u03c0 n (P, \u03b3 2 ) \u2212 S \u03c0 \u221e (P, \u03b3 2 ) \u2264 2 + S \u03c0 n (P, \u03b3 1 ) \u2212 S \u03c0 n (P, \u03b3 2 ) \u2264 2 + L|\u03b3 1 \u2212 \u03b3 2 |,(44)", "formula_coordinates": [14.0, 133.5, 541.69, 424.5, 53.57]}, {"formula_id": "formula_73", "formula_text": "S \u03c0 \u221e (P, \u03b3 1 ) \u2212 S \u03c0 \u221e (P, \u03b3 2 ) < (2 + L) ,(45)", "formula_coordinates": [14.0, 231.3, 629.56, 326.7, 12.69]}, {"formula_id": "formula_74", "formula_text": "S \u03c0 \u221e (P, \u03b3 1 ) \u2212 S \u03c0 \u221e (P, \u03b3 2 ) \u2264 L|\u03b3 1 \u2212 \u03b3 2 |,(46)", "formula_coordinates": [14.0, 227.3, 675.59, 330.71, 12.69]}, {"formula_id": "formula_75", "formula_text": "V \u03c0 P,\u03b3 = 1 1 \u2212 \u03b3 g \u03c0 P + h \u03c0 P + f \u03c0 P (\u03b3).(47)", "formula_coordinates": [15.0, 240.92, 73.65, 317.08, 22.31]}, {"formula_id": "formula_76", "formula_text": "(1 \u2212 \u03b3)V \u03c0 P,\u03b3 = g \u03c0 P + (1 \u2212 \u03b3)h \u03c0 P + (1 \u2212 \u03b3)f \u03c0 P (\u03b3). (48", "formula_coordinates": [15.0, 207.62, 119.64, 346.23, 12.69]}, {"formula_id": "formula_77", "formula_text": ")", "formula_coordinates": [15.0, 553.85, 122.03, 4.15, 8.64]}, {"formula_id": "formula_78", "formula_text": "Clearly (1 \u2212 \u03b3)h \u03c0 P \u2192 0 uniformly on \u03a0 \u00d7 P because h \u03c0 P = H \u03c0 P r \u03c0 \u2264 h is uniformly bounded. Then, (1 \u2212 \u03b3 1 )f \u03c0 P (\u03b3 1 ) \u2212 (1 \u2212 \u03b3 2 )f \u03c0 P (\u03b3 2 ) \u2264 (1 \u2212 \u03b3 1 )f \u03c0 P (\u03b3 1 ) \u2212 (1 \u2212 \u03b3 1 )f \u03c0 P (\u03b3 2 ) + (1 \u2212 \u03b3 1 )f \u03c0 P (\u03b3 2 ) \u2212 (1 \u2212 \u03b3 2 )f \u03c0 P (\u03b3 2 ) \u2264 (1 \u2212 \u03b3 1 )L|\u03b3 1 \u2212 \u03b3 2 | + f \u03c0 P (\u03b3 2 ) |\u03b3 1 \u2212 \u03b3 2 |.(49)", "formula_coordinates": [15.0, 54.0, 139.34, 504.0, 57.97]}, {"formula_id": "formula_79", "formula_text": "f \u03c0 P (\u03b3) = 1 \u03b3 \u221e n=1 (\u22121) n 1 \u2212 \u03b3 \u03b3 n (H \u03c0 P ) n+1 r \u03c0 \u2264 1 \u03b3 \u221e n=1 1 \u2212 \u03b3 \u03b3 n h n+1 \u2264 h \u03b4 1 \u2212 \u03b3 \u03b3 h 1 1 \u2212 1\u2212\u03b3 \u03b3 h \u2264 h \u03b4 k 1 \u2212 k c f .(50)", "formula_coordinates": [15.0, 207.91, 220.68, 350.09, 134.47]}, {"formula_id": "formula_80", "formula_text": "max x lim y\u2192y0 f (x, y) = lim y\u2192y0 max x f (x, y).(51)", "formula_coordinates": [15.0, 226.89, 443.97, 331.11, 14.13]}, {"formula_id": "formula_81", "formula_text": "0 | < \u03b4 , |f (x, y) \u2212 F (x)| \u2264 (52) for any x. Now consider |f (x y , y) \u2212 F (x )| for |y \u2212 y 0 | < \u03b4 . If f (x y , y) \u2212 F (x ) > 0, then |f (x y , y) \u2212 F (x )| = f (x y , y) \u2212 F (x ) = f (x y , y) \u2212 F (x y ) + F (x y ) \u2212 F (x ) \u2264 ;(53)", "formula_coordinates": [15.0, 54.0, 478.64, 505.24, 63.29]}, {"formula_id": "formula_82", "formula_text": "On the other hand if f (x y , y) \u2212 F (x ) < 0, then |f (x y , y) \u2212 F (x )| = F (x ) \u2212 f (x y , y) = F (x ) \u2212 f (x , y) + f (x , y) \u2212 f (x y , y) \u2264 .(54)", "formula_coordinates": [15.0, 54.0, 550.16, 504.0, 27.53]}, {"formula_id": "formula_83", "formula_text": "0 | < \u03b4 , |f (x y , y) \u2212 F (x )| = | max x f (x, y) \u2212 max x F (x)| \u2264 ,(55)", "formula_coordinates": [15.0, 196.48, 585.91, 361.52, 32.01]}, {"formula_id": "formula_84", "formula_text": "lim y\u2192y0 max x f (x, y) = max x F (x) = max x lim y\u2192y0 f (x, y),(56)", "formula_coordinates": [15.0, 199.53, 643.31, 358.47, 14.13]}, {"formula_id": "formula_85", "formula_text": "lim \u03b3\u21921 (1 \u2212 \u03b3)V \u03c0 P,\u03b3 = g \u03c0 P .(57)", "formula_coordinates": [16.0, 259.5, 84.02, 298.5, 16.38]}, {"formula_id": "formula_86", "formula_text": "g \u03c0 P = min P\u2208P g \u03c0 P = min P\u2208P lim \u03b3\u21921 (1 \u2212 \u03b3)V \u03c0 P,\u03b3 (a) = lim \u03b3\u21921 min P\u2208P (1 \u2212 \u03b3)V \u03c0 P,\u03b3 = lim \u03b3\u21921 (1 \u2212 \u03b3)V \u03c0 P,\u03b3 ,(58)", "formula_coordinates": [16.0, 250.8, 136.98, 307.2, 81.51]}, {"formula_id": "formula_87", "formula_text": "(1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t = (1 \u2212 \u03b3 t ) a \u03c0(a|s)(r(s, a) + \u03b3 t \u03c3 P a s (V \u03c0 P,\u03b3t )).(59)", "formula_coordinates": [16.0, 180.8, 313.87, 377.2, 21.69]}, {"formula_id": "formula_88", "formula_text": "|V t+1 (s) \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 (s)| = |V t+1 (s) \u2212 (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t (s) + (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t (s) \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 (s)|(60)", "formula_coordinates": [16.0, 103.55, 359.32, 454.45, 29.03]}, {"formula_id": "formula_89", "formula_text": "\u2264 |(1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t (s) \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 (s)| + |V t+1 (s) \u2212 (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t (s)| = |(1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t (s) \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 (s)| + a \u03c0(a|s) (1 \u2212 \u03b3 t )r(s, a) + \u03b3 t \u03c3 P a s (V t ) \u2212 ((1 \u2212 \u03b3 t )r(s, a) + \u03b3 t \u03c3 P a s ((1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t )) = |(1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t (s) \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 (s)| + a \u03c0(a|s) \u03b3 t \u03c3 P a s (V t ) \u2212 \u03b3 t \u03c3 P a s ((1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t ) = |(1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t (s) \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 (s)| + \u03b3 t a \u03c0(a|s) \u03c3 P a s (V t ) \u2212 \u03c3 P a s ((1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t ) .(61)", "formula_coordinates": [16.0, 106.31, 391.21, 451.69, 122.12]}, {"formula_id": "formula_90", "formula_text": "\u2206 t V t \u2212 (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t \u221e , then \u2206 t+1 \u2264 (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 \u221e + \u03b3 t max s a \u03c0(a|s) \u03c3 P a s (V t ) \u2212 \u03c3 P a s ((1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t ) .(62)", "formula_coordinates": [16.0, 90.46, 521.75, 467.54, 47.89]}, {"formula_id": "formula_91", "formula_text": "a \u03c0(a|s) \u03c3 P a s (V t ) \u2212 \u03c3 P a s ((1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t ) \u2264 a \u03c0(a|s) V t \u2212 (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t \u221e = V t \u2212 (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t \u221e ,(63)", "formula_coordinates": [16.0, 220.99, 601.86, 337.01, 64.09]}, {"formula_id": "formula_92", "formula_text": "\u2206 t+1 \u2264 (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 \u221e + \u03b3 t \u2206 t .(64)", "formula_coordinates": [16.0, 190.74, 688.83, 367.26, 13.08]}, {"formula_id": "formula_93", "formula_text": "(1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t = (1 \u2212 \u03b3 t ) min P V \u03c0 P,\u03b3t . (65) Let s * t arg max s |(1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t (s) \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 (s)|. Then it follows that (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 \u221e = |(1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t (s * t ) \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 (s * t )|.(66)", "formula_coordinates": [17.0, 54.0, 71.29, 504.0, 54.57]}, {"formula_id": "formula_94", "formula_text": "that V \u03c0 P,\u03b3 (s) = E P,\u03c0 \u221e t=0 \u03b3 t r t |S 0 = s V \u03c0 P,\u03b3", "formula_coordinates": [17.0, 54.0, 146.93, 203.09, 14.4]}, {"formula_id": "formula_95", "formula_text": "P t . If (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t (s * t ) \u2265 (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 (s * t ), then |(1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t (s * t ) \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 (s * t )| = min P (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t (s * t ) \u2212 min P (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 (s * t ) = (1 \u2212 \u03b3 t )V \u03c0 Pt,\u03b3t (s * t ) \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 Pt+1,\u03b3t+1 (s * t ) = (1 \u2212 \u03b3 t )V \u03c0 Pt,\u03b3t (s * t ) \u2212 (1 \u2212 \u03b3 t )V \u03c0 Pt+1,\u03b3t (s * t ) + (1 \u2212 \u03b3 t )V \u03c0 Pt+1,\u03b3t (s * t ) \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 Pt+1,\u03b3t+1 (s * t ) (a) \u2264 (1 \u2212 \u03b3 t )V \u03c0 Pt+1,\u03b3t (s * t ) \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 Pt+1,\u03b3t+1 (s * t ) \u2264 (1 \u2212 \u03b3 t )V \u03c0 Pt+1,\u03b3t \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 Pt+1,\u03b3t+1 \u221e ,(67)", "formula_coordinates": [17.0, 63.96, 167.17, 494.04, 132.73]}, {"formula_id": "formula_96", "formula_text": "(a) is due to (1 \u2212 \u03b3 t )V \u03c0 Pt,\u03b3t (s * t ) = min P (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t (s * t ) \u2264 (1 \u2212 \u03b3 t )V \u03c0 Pt+1,\u03b3t (s * t ). Now, according to Lemma 1, (1 \u2212 \u03b3 t )V \u03c0 Pt+1,\u03b3t = g \u03c0 Pt+1 + (1 \u2212 \u03b3 t )h \u03c0 Pt+1 + (1 \u2212 \u03b3 t )f \u03c0 Pt+1 (\u03b3 t ),(68)", "formula_coordinates": [17.0, 63.96, 305.71, 494.04, 40.8]}, {"formula_id": "formula_97", "formula_text": "(1 \u2212 \u03b3 t+1 )V \u03c0 Pt+1,\u03b3t+1 = g \u03c0 Pt+1 + (1 \u2212 \u03b3 t+1 )h \u03c0 Pt+1 + (1 \u2212 \u03b3 t+1 )f \u03c0 Pt+1 (\u03b3 t+1 ).(69)", "formula_coordinates": [17.0, 153.19, 349.76, 404.81, 12.69]}, {"formula_id": "formula_98", "formula_text": "(1 \u2212 \u03b3 t )V \u03c0 Pt+1,\u03b3t \u2212 (1 \u2212 \u03b3 t+1 )V \u03c0 Pt+1,\u03b3t+1 \u221e = (\u03b3 t+1 \u2212 \u03b3 t )h \u03c0 Pt+1 + (1 \u2212 \u03b3 t )f \u03c0 Pt+1 (\u03b3 t ) \u2212 (1 \u2212 \u03b3 t+1 )f \u03c0 Pt+1 (\u03b3 t+1 ) \u221e \u2264 (\u03b3 t+1 \u2212 \u03b3 t ) h \u03c0 Pt+1 \u221e + f \u03c0 Pt+1 (\u03b3 t ) \u2212 f \u03c0 Pt+1 (\u03b3 t+1 ) \u221e + \u03b3 t+1 f \u03c0 Pt+1 (\u03b3 t+1 ) \u2212 \u03b3 t f \u03c0 Pt+1 (\u03b3 t ) \u221e (a) \u2264 h(\u03b3 t+1 \u2212 \u03b3 t ) + L(\u03b3 t+1 \u2212 \u03b3 t ) + \u03b3 t+1 f \u03c0 Pt+1 (\u03b3 t+1 ) \u2212 \u03b3 t f \u03c0 Pt+1 (\u03b3 t ) \u221e \u2264 h(\u03b3 t+1 \u2212 \u03b3 t ) + L(\u03b3 t+1 \u2212 \u03b3 t ) + \u03b3 t+1 f \u03c0 Pt+1 (\u03b3 t+1 ) \u2212 \u03b3 t+1 f \u03c0 Pt+1 (\u03b3 t ) \u221e + \u03b3 t+1 f \u03c0 Pt+1 (\u03b3 t ) \u2212 \u03b3 t f \u03c0 Pt+1 (\u03b3 t ) \u221e \u2264 h(\u03b3 t+1 \u2212 \u03b3 t ) + L(\u03b3 t+1 \u2212 \u03b3 t ) + \u03b3 t+1 f \u03c0 Pt+1 (\u03b3 t+1 ) \u2212 f \u03c0 Pt+1 (\u03b3 t ) \u221e + f \u03c0 Pt+1 (\u03b3 t ) \u221e (\u03b3 t+1 \u2212 \u03b3 t ) (b) \u2264 (h + L + \u03b3 t+1 L + sup \u03c0,P,\u03b3 f \u03c0 P (\u03b3) \u221e )(\u03b3 t+1 \u2212 \u03b3 t ) \u2264 K(\u03b3 t+1 \u2212 \u03b3 t ),(70)", "formula_coordinates": [17.0, 85.49, 383.41, 472.51, 145.33]}, {"formula_id": "formula_99", "formula_text": "(1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t (s * t ) \u2264 (1 \u2212 \u03b3 t+1 )V \u03c0 P,\u03b3t+1 (s * t )", "formula_coordinates": [17.0, 274.81, 558.1, 175.91, 13.0]}, {"formula_id": "formula_100", "formula_text": "\u2206 t+1 \u2264 K(\u03b3 t+1 \u2212 \u03b3 t ) + \u03b3 t \u2206 t , (71", "formula_coordinates": [17.0, 242.44, 588.49, 311.41, 9.65]}, {"formula_id": "formula_101", "formula_text": ")", "formula_coordinates": [17.0, 553.85, 588.81, 4.15, 8.64]}, {"formula_id": "formula_102", "formula_text": "V t \u2212 g \u03c0 P \u221e \u2264 V t \u2212 (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t \u221e + (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t \u2212 g \u03c0 P \u221e = \u2206 t + (1 \u2212 \u03b3 t )V \u03c0 P,\u03b3t \u2212 g \u03c0 P \u221e . (72", "formula_coordinates": [17.0, 108.38, 629.79, 445.47, 13.08]}, {"formula_id": "formula_103", "formula_text": ")", "formula_coordinates": [17.0, 553.85, 632.18, 4.15, 8.64]}, {"formula_id": "formula_104", "formula_text": "lim t\u2192\u221e V t \u2212 g \u03c0 P \u221e = 0,(73)", "formula_coordinates": [17.0, 259.82, 662.19, 298.18, 16.21]}, {"formula_id": "formula_105", "formula_text": "g * P (s) max \u03c0 g \u03c0 P (s).(74)", "formula_coordinates": [18.0, 264.57, 84.0, 293.43, 16.21]}, {"formula_id": "formula_107", "formula_text": "lim t\u2192\u221e (1 \u2212 \u03b3 t )V * P,\u03b3t = g * P . (76", "formula_coordinates": [18.0, 255.4, 188.54, 298.45, 16.21]}, {"formula_id": "formula_108", "formula_text": ")", "formula_coordinates": [18.0, 553.85, 190.93, 4.15, 8.64]}, {"formula_id": "formula_109", "formula_text": "|V t+1 (s) \u2212 (1 \u2212 \u03b3 t+1 )V * P,\u03b3t+1 (s)| \u2264 |V t+1 (s) \u2212 (1 \u2212 \u03b3 t )V * P,\u03b3t (s)| + |(1 \u2212 \u03b3 t )V * P,\u03b3t (s) \u2212 (1 \u2212 \u03b3 t+1 )V * P,\u03b3t+1 (s)| (a) = |(1 \u2212 \u03b3 t )V * P,\u03b3t (s) \u2212 (1 \u2212 \u03b3 t+1 )V * P,\u03b3t+1 (s)| + max a (1 \u2212 \u03b3 t )r(s, a) + \u03b3 t \u03c3 P a s (V t ) \u2212 max a ((1 \u2212 \u03b3 t )r(s, a) + \u03b3 t \u03c3 P a s ((1 \u2212 \u03b3 t )V * P,\u03b3t )) \u2264 |(1 \u2212 \u03b3 t )V * P,\u03b3t (s) \u2212 (1 \u2212 \u03b3 t+1 )V * P,\u03b3t+1 (s)| + max a (1 \u2212 \u03b3 t )r(s, a) + \u03b3 t \u03c3 P a s (V t ) \u2212 ((1 \u2212 \u03b3 t )r(s, a) + \u03b3 t \u03c3 P a s ((1 \u2212 \u03b3 t )V * P,\u03b3t )) ,(77)", "formula_coordinates": [18.0, 108.38, 221.25, 449.62, 119.72]}, {"formula_id": "formula_110", "formula_text": "| max x f (x)\u2212max x g(x)| \u2264 max x |f (x) \u2212 g(x)|.", "formula_coordinates": [18.0, 54.0, 348.17, 504.0, 20.61]}, {"formula_id": "formula_111", "formula_text": "|V t+1 (s) \u2212 (1 \u2212 \u03b3 t+1 )V * P,\u03b3t+1 (s)| \u2264 |(1 \u2212 \u03b3 t )V * P,\u03b3t (s) \u2212 (1 \u2212 \u03b3 t+1 )V * P,\u03b3t+1 (s)| + \u03b3 t max a \u03c3 P a s (V t ) \u2212 \u03c3 P a s ((1 \u2212 \u03b3 t )V * P,\u03b3t ) . (78", "formula_coordinates": [18.0, 125.0, 382.36, 428.85, 38.93]}, {"formula_id": "formula_112", "formula_text": ")", "formula_coordinates": [18.0, 553.85, 407.48, 4.15, 8.64]}, {"formula_id": "formula_113", "formula_text": "\u2206 t+1 \u2264 (1 \u2212 \u03b3 t )V * P,\u03b3t \u2212 (1 \u2212 \u03b3 t+1 )V * P,\u03b3t+1 \u221e + \u03b3 t max s.a \u03c3 P a s (V t ) \u2212 \u03c3 P a s ((1 \u2212 \u03b3 t )V * P,\u03b3t ) . (79", "formula_coordinates": [18.0, 120.04, 451.14, 433.82, 16.21]}, {"formula_id": "formula_114", "formula_text": ")", "formula_coordinates": [18.0, 553.85, 453.54, 4.15, 8.64]}, {"formula_id": "formula_115", "formula_text": "\u03c3 P a s (V t ) \u2212 \u03c3 P a s ((1 \u2212 \u03b3 t )V * P,\u03b3t ) \u2264 V t \u2212 (1 \u2212 \u03b3 t )V * P,\u03b3t \u221e . (80", "formula_coordinates": [18.0, 187.98, 494.79, 365.87, 13.33]}, {"formula_id": "formula_116", "formula_text": ")", "formula_coordinates": [18.0, 553.85, 497.19, 4.15, 8.64]}, {"formula_id": "formula_117", "formula_text": "Hence \u2206 t+1 \u2264 (1 \u2212 \u03b3 t )V * P,\u03b3t \u2212 (1 \u2212 \u03b3 t+1 )V * P,\u03b3t+1 \u221e + \u03b3 t \u2206 t .(81)", "formula_coordinates": [18.0, 54.0, 517.53, 504.0, 25.04]}, {"formula_id": "formula_118", "formula_text": "(1 \u2212 \u03b3 t )V * P,\u03b3t \u2212 (1 \u2212 \u03b3 t+1 )V * P,\u03b3t+1 \u221e \u2264 K|\u03b3 t \u2212 \u03b3 t+1 |,(82)", "formula_coordinates": [18.0, 195.85, 559.27, 362.15, 13.08]}, {"formula_id": "formula_119", "formula_text": "t\u2192\u221e \u2206 t = 0. (83) Moreover, note that V t \u2212 g * P \u221e \u2264 V t \u2212 (1 \u2212 \u03b3 t )V * P,\u03b3t \u221e + (1 \u2212 \u03b3 t )V * P,\u03b3t \u2212 g * P \u221e = \u2206 t + (1 \u2212 \u03b3 t )V * P,\u03b3t \u2212 g * P \u221e ,(84)", "formula_coordinates": [18.0, 54.0, 591.13, 504.0, 43.48]}, {"formula_id": "formula_120", "formula_text": "\u03b3\u21921 (1 \u2212 \u03b3)V \u03c0r P,\u03b3 = g \u03c0r P ,(87)", "formula_coordinates": [19.0, 237.66, 138.0, 320.34, 16.7]}, {"formula_id": "formula_121", "formula_text": "lim \u03b3\u21921 (1 \u2212 \u03b3)V \u03c0 P,\u03b3 = g \u03c0 P , \u2200\u03c0 \u2208 \u03a0 D . (88", "formula_coordinates": [19.0, 237.66, 158.48, 316.19, 16.38]}, {"formula_id": "formula_122", "formula_text": ")", "formula_coordinates": [19.0, 553.85, 160.87, 4.15, 8.64]}, {"formula_id": "formula_123", "formula_text": "V \u03c0r P,\u03b3 > V \u03c0 P,\u03b3 , \u2200\u03c0 \u2208 \u03a0 D . (89", "formula_coordinates": [19.0, 257.21, 191.36, 296.64, 14.0]}, {"formula_id": "formula_124", "formula_text": ")", "formula_coordinates": [19.0, 553.85, 194.07, 4.15, 8.64]}, {"formula_id": "formula_125", "formula_text": "\u03c0\u2208\u03a0 D V \u03c0 P,\u03b3 .(90)", "formula_coordinates": [19.0, 298.93, 220.79, 259.07, 17.27]}, {"formula_id": "formula_126", "formula_text": "\u03c0\u2208\u03a0 D V \u03c0 P,\u03b3 = max \u03c0\u2208\u03a0 V \u03c0 P,\u03b3 \u2265 V \u03c0r P,\u03b3 ,(91)", "formula_coordinates": [19.0, 240.52, 253.43, 317.48, 17.59]}, {"formula_id": "formula_127", "formula_text": "g * P = g \u03c0 * 1 P = g \u03c0 * 2 P = ... = g \u03c0 * m P > g \u03c01 P \u2265 ... \u2265 g \u03c0n P (94)", "formula_coordinates": [19.0, 207.99, 412.6, 350.01, 16.64]}, {"formula_id": "formula_128", "formula_text": "\u03c0 * i P \u2212 g \u03c01 P .", "formula_coordinates": [19.0, 371.25, 430.54, 38.75, 16.64]}, {"formula_id": "formula_129", "formula_text": "\u03b3\u21921 (1 \u2212 \u03b3)V \u03c0 P,\u03b3 = g \u03c0 P .(95)", "formula_coordinates": [19.0, 259.5, 459.81, 298.5, 16.38]}, {"formula_id": "formula_130", "formula_text": "|(1 \u2212 \u03b3)V \u03c0 * i P,\u03b3 \u2212 g * P | < ,(96)", "formula_coordinates": [19.0, 258.44, 496.12, 299.56, 16.64]}, {"formula_id": "formula_131", "formula_text": "|(1 \u2212 \u03b3)V \u03c0j P,\u03b3 \u2212 g \u03c0j P | < .(97)", "formula_coordinates": [19.0, 255.33, 514.6, 302.67, 14.97]}, {"formula_id": "formula_132", "formula_text": "(1 \u2212 \u03b3)V \u03c0 * i P,\u03b3 \u2265 (d \u2212 2 ) + (1 \u2212 \u03b3)V \u03c0j P,\u03b3 > (1 \u2212 \u03b3)V \u03c0j P,\u03b3 ,(98)", "formula_coordinates": [19.0, 193.87, 544.81, 364.13, 16.64]}, {"formula_id": "formula_133", "formula_text": "\u03c0 * i P,\u03b3 > V \u03c0j P,\u03b3 .(99)", "formula_coordinates": [19.0, 284.87, 573.76, 273.13, 16.64]}, {"formula_id": "formula_134", "formula_text": "\u03c0 * j P,\u03b3 = max \u03c0\u2208\u03a0 D V \u03c0 P,\u03b3 = V * P,\u03b3 .(100)", "formula_coordinates": [19.0, 256.17, 633.87, 301.84, 21.21]}, {"formula_id": "formula_135", "formula_text": "\u03ba\u2208 t\u22650 P E \u03ba,\u03c0 \u221e t=0 (r t \u2212 g \u03c0 P ) S 0 = s ,(101)", "formula_coordinates": [20.0, 246.24, 83.46, 311.76, 30.2]}, {"formula_id": "formula_136", "formula_text": "g \u03c0 P = min \u03ba\u2208 t\u22650 P lim n\u2192\u221e E \u03ba,\u03c0 1 n n\u22121 t=0 r t |S 0 = s . (102", "formula_coordinates": [20.0, 211.44, 133.91, 342.24, 30.2]}, {"formula_id": "formula_137", "formula_text": ")", "formula_coordinates": [20.0, 553.68, 144.64, 4.32, 8.64]}, {"formula_id": "formula_138", "formula_text": "V \u03c0 P = min P\u2208P E P,\u03c0 \u221e t=0 (r t \u2212 g \u03c0 P ) = min P\u2208P E P,\u03c0 lim n\u2192\u221e n t=0 (r t \u2212 g \u03c0 P ) = min P\u2208P E P,\u03c0 lim n\u2192\u221e n t=0 (r t \u2212 g \u03c0 P + g \u03c0 P \u2212 g \u03c0 P ) = min P\u2208P E P,\u03c0 lim n\u2192\u221e (R n \u2212 ng \u03c0 P + ng \u03c0 P \u2212 ng \u03c0 P ) ,(103)", "formula_coordinates": [20.0, 202.5, 231.74, 355.5, 122.33]}, {"formula_id": "formula_139", "formula_text": "P \u2265 ng \u03c0 P , hence lim n\u2192\u221e (R n \u2212 ng \u03c0 P + ng \u03c0 P \u2212 ng \u03c0 P ) \u2265 lim n\u2192\u221e (R n \u2212 ng \u03c0 P ),(104)", "formula_coordinates": [20.0, 198.9, 361.76, 359.1, 32.18]}, {"formula_id": "formula_140", "formula_text": "V \u03c0 P \u2265 min P\u2208P E P,\u03c0 \u221e t=0 (r t \u2212 g \u03c0 P ) = min P\u2208P V \u03c0 P = min P\u2208P H \u03c0 P r \u03c0 .(105)", "formula_coordinates": [20.0, 241.43, 415.53, 316.57, 69.16]}, {"formula_id": "formula_141", "formula_text": "V \u03c0 P = min P\u2208P E P,\u03c0 \u221e t=0 (r t \u2212 g \u03c0 Pg ) \u2264 E Pg,\u03c0 \u221e t=0 (r t \u2212 g \u03c0 Pg ) = V \u03c0 Pg ,(106)", "formula_coordinates": [20.0, 239.61, 528.19, 318.39, 78.39]}, {"formula_id": "formula_142", "formula_text": "V (s) + g = a \u03c0(a|s) r(s, a) + \u03c3 P a s (V ) .(107)", "formula_coordinates": [20.0, 216.11, 687.72, 341.89, 19.61]}, {"formula_id": "formula_143", "formula_text": "V \u03c0 P (s) = min \u03ba\u2208 t\u22650 P E \u03ba,\u03c0 \u221e t=0 (r t \u2212 g \u03c0 P ) S 0 = s ,(108)", "formula_coordinates": [21.0, 207.11, 72.01, 350.89, 30.2]}, {"formula_id": "formula_144", "formula_text": "hence V \u03c0 P (s) = min \u03ba\u2208 t\u22650 P E \u03ba,\u03c0 \u221e t=0 (r t \u2212 g \u03c0 P ) S 0 = s = min \u03ba\u2208 t\u22650 P E \u03ba,\u03c0 (r 0 \u2212 g \u03c0 P ) + \u221e t=1 (r t \u2212 g \u03c0 P ) S 0 = s = min \u03ba\u2208 t\u22650 P a \u03c0(a|s)r(s, a) \u2212 g \u03c0 P + E \u03ba,\u03c0 \u221e t=1 (r t \u2212 g \u03c0 P ) S 0 = s = a \u03c0(a|s) (r(s, a) \u2212 g \u03c0 P ) + min \u03ba\u2208 t\u22650 P \uf8f1 \uf8f2 \uf8f3 a,s \u03c0(a|s)P a s,s E \u03ba,\u03c0 \u221e t=1 (r t \u2212 g \u03c0 P )|S 1 = s \uf8fc \uf8fd \uf8fe = a \u03c0(a|s) (r(s, a) \u2212 g \u03c0 P ) + min P0\u2208P min \u03ba=(P1,...)\u2208 t\u22651 P \uf8f1 \uf8f2 \uf8f3 a,s \u03c0(a|s)(P 0 ) a s,s E \u03ba,\u03c0 \u221e t=1 (r t \u2212 g \u03c0 P )|S 1 = s \uf8fc \uf8fd \uf8fe = a \u03c0(a|s) (r(s, a) \u2212 g \u03c0 P ) + min P0\u2208P \uf8f1 \uf8f2 \uf8f3 a,s \u03c0(a|s)(P 0 ) a s,s min \u03ba=(P1,...)\u2208 t\u22651 P E \u03ba,\u03c0 \u221e t=1 (r t \u2212 g \u03c0 P )|S 1 = s \uf8fc \uf8fd \uf8fe = a \u03c0(a|s) (r(s, a) \u2212 g \u03c0 P ) + a \u03c0(a|s) s min p a s,s \u2208P a s p a s,s V \u03c0 P (s ) = a \u03c0(a|s) (r(s, a) \u2212 g \u03c0 P ) + a \u03c0(a|s)\u03c3 P a s (V \u03c0 P ) = a \u03c0(a|s) r(s, a) \u2212 g \u03c0 P + \u03c3 P a s (V \u03c0 P ) . (109", "formula_coordinates": [21.0, 54.0, 108.9, 499.68, 310.61]}, {"formula_id": "formula_145", "formula_text": ")", "formula_coordinates": [21.0, 553.68, 400.21, 4.32, 8.64]}, {"formula_id": "formula_146", "formula_text": "B(g, V )(s) max a r(s, a) \u2212 g + \u03c3 P a s (V ) \u2212 V (s) . Since (g, V", "formula_coordinates": [21.0, 80.16, 485.08, 266.87, 11.26]}, {"formula_id": "formula_147", "formula_text": "\u2212 g + \u03c3 P a s (V ) \u2212 V (s) \u2264 0,(110)", "formula_coordinates": [21.0, 264.12, 512.77, 293.88, 11.26]}, {"formula_id": "formula_148", "formula_text": "g(s) \u2265 r \u03c0 (s) + a \u03c0(a|s)\u03c3 P a s (V ) \u2212 V (s) r \u03c0 (s) + a \u03c0(a|s)(p a s ) V \u2212 V (s),(111)", "formula_coordinates": [21.0, 140.77, 546.82, 417.23, 21.69]}, {"formula_id": "formula_156", "formula_text": "n\u2192\u221e 1 n E P \u03c0 V ,\u03c0 n t=0 r t = g \u03c0 P \u03c0 V 1 \u2265 g \u03c0 P 1.(119)", "formula_coordinates": [22.0, 233.47, 243.75, 324.53, 62.19]}, {"formula_id": "formula_158", "formula_text": "P \u03c4 V V \u2264 P \u03c4 V.(121)", "formula_coordinates": [22.0, 278.47, 399.67, 279.53, 12.69]}, {"formula_id": "formula_162", "formula_text": "g1 \u2264 lim n\u2192\u221e 1 n E P \u03c4 ave ,\u03c4 n t=0 r t = g \u03c4 P \u03c4 ave 1 = g \u03c4 P 1 \u2264 g * P 1.(125)", "formula_coordinates": [22.0, 197.48, 558.04, 360.52, 30.2]}, {"formula_id": "formula_165", "formula_text": "r \u03c0 * \u2212 g + \u03c3 P \u03c0 * (V ) \u2212 V = max \u03c0 {r \u03c0 \u2212 g + \u03c3 P \u03c0 (V ) \u2212 V } .(129)", "formula_coordinates": [23.0, 185.92, 132.53, 372.08, 14.13]}, {"formula_id": "formula_167", "formula_text": "min{p i , q i } Then n i=1 p i x i \u2212 n i=1 q i x i = n i=1 (p i \u2212 b i )x i \u2212 n i=1 (q i \u2212 b i )x i \u2264 n i=1 (p i \u2212 b i ) max{x i } \u2212 n i=1 (q i \u2212 b i ) min{x i } = n i=1 (p i \u2212 b i )sp(x) + n i=1 (p i \u2212 b i ) \u2212 n i=1 (q i \u2212 b i ) min{x i } = 1 \u2212 n i=1 b i sp(x).(136)", "formula_coordinates": [23.0, 176.17, 560.01, 381.83, 147.33]}, {"formula_id": "formula_168", "formula_text": "sp(Lv \u2212 Lu) \u2264 1 \u2212 n i=1 b i sp(v \u2212 u).(137)", "formula_coordinates": [24.0, 222.96, 105.23, 335.04, 30.32]}, {"formula_id": "formula_169", "formula_text": "n i=1 b i \u2265 \u03bb.(138)", "formula_coordinates": [24.0, 283.99, 170.41, 274.01, 30.32]}, {"formula_id": "formula_170", "formula_text": "sp(L J v \u2212 L J u) \u2264 (1 \u2212 \u03bb)sp(v \u2212 u).(139)", "formula_coordinates": [24.0, 229.77, 235.17, 328.23, 11.03]}], "doi": ""}