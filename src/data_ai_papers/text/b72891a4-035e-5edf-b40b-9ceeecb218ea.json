{"title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity", "authors": "Yao Lu; Max Bartolo; Alastair Moore; Sebastian Riedel; Pontus Stenetorp", "pub_date": "", "abstract": "When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, finetuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are \"fantastic\" and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true fewshot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13% relative improvement for GPTfamily models across eleven different established text classification tasks.", "sections": [{"heading": "Introduction", "text": "Large pretrained language models (PLMs, Devlin et al., 2019;Peters et al., 2018;Raffel et al., 2020;Liu et al., 2019;Yang et al., 2019;Radford et al., 2019) have shown remarkable performance when conditioned with an appropriate textual context (Petroni et al., 2019(Petroni et al., , 2020Jiang et al., 2020;Shin et al., 2020;Davison et al., 2019). For example, when conditioned on a long document and a \"TL;DR:\" token, they can generate a summary of said document, and when provided a partial question (\"The theory of relativity was developed by __\"), they can generate the correct answer. Perhaps most strikingly, when primed with a context consisting of very few training examples, they produce  text classification results that can match those of fully supervised models. This type of few shot setting, is commonly referred to as \"In-context Learning\" (Brown et al., 2020).\nA core component of in-context learning is the text-based prompt that serves as the context. Composing a prompt requires: (i) text linearisation using a template; and (ii) training sample concatenation (See Table 1 for an example). It has been established that the structure of the template has a large impact on performance (Shin et al., 2020;Gao et al., 2020;Schick and Sch\u00fctze, 2020;Jiang et al., 2020). However, to the best of our knowledge, no work has studied the effect of the sample ordering on In-context Learning performance.\nPerhaps counter-intuitively, we find that the right sample order can make as much of a difference as Example training set (the greatest musicians, 1) (redundant concept, 0) linearization Review: the greatest musicians. Sentiment: positive Review: redundant concept. Sentiment: negative concatenation Review: the greatest musicians. Sentiment: positive. Review: redundant concept. Sentiment: negative OR Review: redundant concept. Sentiment: negative. Review: the greatest musicians. Sentiment: positive the right template. As can be seen in Figure 1, some permutations have comparable performance (over 85% accuracy) to supervised training for sentiment classification, while others perform close to random (around 50%). This order sensitivity is universal across models, and although increasing the model size somewhat addresses it, the problem is still present for some text classification tasks (Subj in Figure 1) for models with billions of parameters.\nIn our analysis, we find no common denominator between performant sample orders and that they are not transferable across different model sizes and tasks. In a fully-supervised setting, we could rely on a development set to select among sample orders. However, this is not desirable in a few-shot setting where the size of the development set is very limited, even unavailable (Perez et al., 2021) . Instead, we use the generative nature of language models to construct an unlabelled artificial development set and refer to it as a probing set. As the probing set is unlabelled, we use the predicted label distribution statistics and propose entropy-based metrics to measure the quality of candidate prompts.Experimental results show that we can achieve on average 13% relative improvement across eleven different established text classification tasks across all different sizes (four orders of magnitude) of PLMs.\nTo summarise, our contributions are as follows:\n1. We study order sensitivity for In-context Learning, which we show is crucial for the success of pretrained language models for fewshot learning.\n2. We propose a simple, generation-based probing method to identify performant prompts without requiring additional data.\n3. Our probing method is universally applicable and effective across different sizes of pretrained language models and for different types of datasets -achieving on average a 13% relative improvement over a wide range of tasks.", "publication_ref": ["b4", "b15", "b19", "b10", "b25", "b18", "b17", "b16", "b7", "b21", "b2", "b21", "b5", "b20", "b7", "b14"], "figure_ref": ["fig_1", "fig_1"], "table_ref": ["tab_0"]}, {"heading": "Order Sensitivity and Prompt Design", "text": "In this section, we study the relationship between permutation performance and various factors. For the ease of visualisation, we use a fixed random subset of four samples with a balanced label distribution from the SST-2 dataset and consider all 24 possible sample order permutations. This setup is illustrated in Figure 2. We also test five randomlyselected sets of examples and summarised variance statistics in the experiment section (Section 5).\nAlthough beneficial, increasing model size does not guarantee low variance We evaluate the order permutations for four different sizes of GPT-2 (0.1B-1.5B) 1 and GPT-3 (2.7B-175B). As we can observe in Figure 1, models can obtain remarkable few-shot performance. We see that the GPT2-XL (1.5B) model can even surpass 90% accuracy given just four samples. This result is comparable to those of supervised models trained on more than 60,000 samples. However, the performance variation of different permutations remain a big issue, especially for \"smaller\" models. 2 The same model can exhibit nearly perfect behaviour given one sample order, but then fall back to be on par with a random baseline for another. While increasing the model size (by a few order of magnitudes) can sometimes alleviate the issue, it still cannot resolve it entirely (especially if we consider tasks other than SST-2). In contrast, different initialisations of supervised fine-tuning approaches typically result in less than 1% standard deviation for their test set performance (Gao et al., 2020). Adding training samples does not significantly reduce variance To further explore the order sensitivity of few-shot prompts, we increase the number of training samples and then sample a subset of at most 24 different orderings. 3 We use the GPT2 family models for this experiment. In Figure 3, we can observe that increasing the number of training samples leads to increases in performance. However, a high level of variance remains, even with a large number of samples and can even increase. Based on this, we draw the conclusion that order sensitivity is likely to be a fundamental issue of In-context Learning regardless of the number of training samples.\nPerformant prompts are not transferable across models We find that a specific permutation's performance may drop from 88.7% to 51.6% by changing the underlying model from GPT2-XL (1.5B) to GPT2-Large (0.8B). This suggests that a particular permutation working well for one model does not imply that it will provide good results for another model. To validate this hypothesis, we use all possible order permutations of the four samples as prompts -24 in total. We then perform prediction conditioned on each of these prompts for different models and calculate the pairwise Spearman's rank correlation coefficient between the scores. These results are shown in Figure 4.\nIf there is a common pattern for performant prompts, we should then be able to observe high correlation across models. However, the behaviour of permutations is seemingly random even across  24 0.20 -0.04 1.00 -0.27 -0.03 0.01 -0.14   0.23 0.08 1.00 -0.04 0.10 0.19 -0.12 -0.35   0.09 1.00 0.08 0.20 -0.11 -0.26 0.01 -0.23   1.00 0.09 0.23 -0.24 0.07 -0.10 -0.24 -0 different sizes of the same model. For example, the 175B and 2.7B model only has a correlation of 0.05, this means a good permutation for the 2.7B model is in no way guaranteed that it will also yield good performance for the 175B model.\nPerformant label orderings are not consistent across models In addition to training example ordering, we also explore label ordering for training prompts. We use all patterns of the abovementioned full permutations -six different label patterns. 4 We then compute the pairwise Spearman correlation across different models as described in the previous paragraph. As shown in Figure 5, the behaviour of label orderings is once again seemingly random across different sizes of the same model. It is thus not possible to identify a label ordering that is performant across different models.\nDegenerate behaviour of bad prompts We perform error analysis across performant and nonperformant prompts and observe that the majority of failing prompts suffer from highly unbalanced predicted label distributions (Figure 6, left). An intuitive way to address this would be by calibrating the output distribution, along the lines of Zhao et al. (2021). However, we find that although calibration leads to much higher performance, the variance remains high (Figure 6, right).", "publication_ref": ["b5"], "figure_ref": ["fig_2", "fig_1", "fig_3", "fig_5", "fig_6", "fig_6"], "table_ref": ["tab_0"]}, {"heading": "Methodology", "text": "The previous section demonstrates that prompt order can have a substantial effect on performance, with some orderings of the same prompts for the same model providing random performance, and other \"better\" orderings providing performance competitive with supervised approaches. This suggests that there could be various ways of selecting prompt orders to achieve better performance, but the challenge is to do so automatically and without the need for additional labels (e.g., a development set). Hence, in this section, we explore the question of: \"How can we automatically generate a 'probing set' to find performant prompt orderings\"? We approach this by: (i) for a randomly-selected set of training samples, we use every possible ordering permutation of this set as candidates; (ii) constructing a probing set by querying the language model using all candidate prompts as context; and (iii) use this probing set to identify the best ordering by ranking them using a probing metric.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sampling from the Language Model to Construct a Probing Set", "text": "We propose a simple methodology to automatically construct a \"probing set\", by directly sam-pling from the language model itself. This approach makes it possible to generate probing sets automatically, without access to any additional data. Concretely, given a set of training samples S = {(x i , y i )}, i = 1, \u2022 \u2022 \u2022 , n, where x i and y i denote the sentence and label of the i th training sample. We then define a transformation T , mapping each sample into natural language space, such that t i = T (x i , y i ). t i is therefore a text sequence of the i th training sample using the template defined by T . In this work, we use a simple transformation function T such that T (x i , y i ) = input:x i type:y i . This transforms each sample into a standard format sentence, which linearises each element in the set into natural language space defined as\nS = {t i }, i = 1, \u2022 \u2022 \u2022 , n.\nWe then define a full permutation function group of n training samples,\nF = {f m }, m = 1, \u2022 \u2022 \u2022 , n!,\nwhere each function f m takes S as input and outputs c m : the concatenation of a unique permutation. In our case, sampling four training samples at random gives up to 24 possible ordering permutations of the transformed samples.\nFor each prompt candidate c m , we then sample from the language model to obtain the probing sequence g m \u223c P (\u2022|c m ; \u03b8), where \u03b8 denotes the parameters of the pretrained language model. We stop decoding from the language model upon generating the special end-of-sentence token defined by a template, or reach the generation length limit. Our probing set construction method is illustrated in Figure 7, where the objective is to generate a probing set that shares a similar distribution to the training samples.\nWe run this sampling process for all possible prompt ordering permutations and extract probing samples from them (T \u22121 (g)). Then gather extracted samples together to form the probing set D = T \u22121 (g 1 )\u2295...\u2295T \u22121 (g n! ). Although the probing set contains predicted label for each sentence, there is no guarantee on the validity of these labels. Therefore, we discard them from the probing set as we are only interested in sampling probes from the language model corresponding to the input distribution.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Probing Metrics", "text": "Once we have constructed a probing set for a given set of samples, we can now use that probing set to identify the best possible prompt ordering for that particular sample set. Here, we explore two Figure 7: Our probing set construction method, showing the various possible ordering permutations of the randomly selected training samples, the resulting generation for each permutation, and the concatenation of each into a probing set. Note that we discard the generated labels, as there is no guarantee that these generated labels are correct.\nmethods for selecting the best ordering: Global Entropy (GlobalE), and Local Entropy (LocalE).\nGlobal Entropy (GlobalE) The motivation behind GlobalE is to identify prompts of specific sample orderings that avoid the issue of extremely unbalanced predictions (as we have previously established it as key problem for non-performant prompts). We compute the predicted label\u0177 i for data point (x i , y i ) under context c m as follows:\ny i,m = argmax v\u2208V P (v|c m \u2295 T (x i ); \u03b8)(1)\nFor each label v \u2208 V (where V denotes the target label set), we compute the label probability over the probing set as:\np v m = i 1 {\u0177 i,m =v} |D| (2)\nWe then use the predicted category label entropy as the GlobalE score for c m as follows:\nGlobalE m = v\u2208V \u2212p v m log p v m (3) Local Entropy (LocalE)\nThe motivation behind LocalE is that if a model is overly confident for all probing inputs, then it is likely that the model is not behaving as desired. At the very least, it is poorly calibrated, which could also be an indication of a poor capability to appropriately differentiate between classes. Similar to the GlobalE computation, we calculate the prediction probability of a data point (x i , y i ) over the target labels v \u2208 V under context c m , as follows:\np v i,m = P (x i ,y i )\u223cD (v|c m \u2295 T (x i ); \u03b8), v \u2208 V (4)\nWe then calculate the average prediction entropy per data point as the LocalE score:\nLocalE m = i v\u2208V \u2212p v i,m log p v i,m |D| (5)\nAs we now have a way to score each prompt ordering, based on its effect against the probing set, we can rank each prompt ordering by performance as measured by GlobalE or LocalE respectively.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup", "text": "We use four different sizes of GPT-2 (Radford et al., 2019) (with 0.1B, 0.3B, 0.8B, and 1.5B parameteers) and two sizes of GPT-3 (Brown et al., 2020) (with 2.7B, and 175B parameters). Due to limited context window size (up to 1024 word-pieces for the GPT-2 series of models), we use a 4-shot setting for all datasets except AGNews and DBPedia. Our experiments are based on the open-source checkpoints of GPT-2 models and access to the OpenAI GPT-3 API. 5 For probing set generation, we restrict the maximum generation length to 128. We also use sampling with a temperature, t, of 2, and we also make use of block n-gram repetitions (Paulus et al., 2018) to encourage diverse generation. We use 24 different permutations for each set of randomly selected training samples and use 5 different sets (except for GPT-3 with 175B parameters, where we only do two sets with 12 different permutation due to the high monetary cost) for each experiment, giving a total of 120 runs. We report the mean and standard deviation of the corresponding evaluation metric over 5 different sets.\nFor performant prompt selection, we rank candidate prompts using the LocalE and GlobalE prob-  ing metrics over the automatically generated probing set. We then select top k samples ranked by highest entropy values, where k = 4 in our experiments, of the available 24 permutations as performant prompts. Finally, we use these performant prompts to evaluate performance on various datasets and demonstrate both better performance and reduced variance. We also provide results for a majority baseline, which always predicts the majority label in the dataset, as a lower-bound of performance. We also provide an oracle to show the upper-bound of performance by selecting the top four performant orderings based on prompt performance on the validation set.", "publication_ref": ["b13"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation Datasets", "text": "Similar to previous work (Gao et al., 2020;Zhao et al., 2021), we use eleven text classification datasets ranging from sentiment classification to textual entailment. Further details of the datasets are provided in the Appendix. For evaluation, we sub-sample 256 samples of the validation sets for all datasets to control for the GPT-3 inference costs as it requires the usage of a monetary paid-for API.", "publication_ref": ["b5", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "We report experimental results in Table 2 and observe consistent improvements for both LocalE and GlobalE across all tasks.\nEntropy-based probing is effective for performant prompt selection regardless of model size We find that GlobalE achieves, on average, a 13% relative improvement across the eleven different sentence classification tasks in comparison to prompts that do not make use of probing. LocalE provides results slightly inferior to GlobalE, with an average 9.6% relative improvement over the baseline model. Our selected performant prompts also demonstrate considerably lower variance than using all candidate prompts.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Ranking using Entropy-based probing is robust", "text": "In Figure 8, we visualise the average performance when varying K for the top K prompt selection. K = 24 corresponds to using all sampled prompt orders, which is equivalent to the baseline model performance in Table 2. We can observe that the slope of curves are negative for all datasets, suggesting that our method can rank performant prompts effectively. Though K = 1 can provide good performance for most cases, in our experiments, we use K = 4 as preliminary experiments indicated that it yielded stable performance across datasets.  4) for the SST-2 dataset. Experimental results in Table 3 indicate that Entropy-based probing is valid for different templates. We also observe that the randomness across different templates is similar to Section 2. These findings suggest that Entropy-based probing is not sensitive to specific templates, as it consistently provides improvements for all cases.\nPerformant permutation selection is a safe option for In-context Learning We find that for models that suffer from high prompt variance, our prompt selection process can show large improvements -up to 30% relative improvement. Furthermore, for tasks with low initial prompt performance variance, our method does not negatively impact performance. Our prompt selection provides marginal improvement at worse and on average a 13% relative improvement in the most cases.\nSentence-pair tasks remain challenging for smaller-sized models even with performant permutation selection For the CB and RTE datasets,   the performance of GPT-2 models is not significantly different from that of a random baseline. Despite this, we find that our method for identifying performant prompts can still provide minimal performance gains, although these are still within the levels of a random guess or majority vote. One reason for this could be that, for these particular sizes of models on these tasks, no good prompt exists. As such, optimising the prompt is not particularly effective in this setting. This is further supported by the observation that prompt selection can considerably improve performance on both CB and RTE at larger model sizes (particularly so for the GPT-3 175B parameter model). In fact, we find that prompt selection using GlobalE improves performance by 4.9% for GPT-3 175B on CB. This indicates that our method is widely applicable to all model sizes, and across all tasks, as long as they already possess some existing classification ability that can be improved through prompt design.\nEntropy-based probing outperforms using subsets of the training data for tuning If one was not to rely on generation, an alternative approach to prompt selection could be to split the (limited) training data to form a validation set. To compare GPT-2 0.1B GPT-2 0.3B GPT-2 0.8B GPT-2 1.5B  against this approach, we split the 4-shot training samples (same setting as in Table 2) in half. We then select the top four performing prompts using validation set performance. As can be seen in Table 5, this approach consistently outperforms the baseline. However, both Entropy-based probing methods consistently provides better performance across all model sizes.", "publication_ref": [], "figure_ref": ["fig_7"], "table_ref": ["tab_2", "tab_5", "tab_4", "tab_2"]}, {"heading": "Related Work", "text": "Unified Interface Design for NLP Most previous work focuses on shared-parameters models, pretrain on some tasks, then fine-tune for different tasks, e.g. ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), etc. Eventually, leading to multiple task-specific models. There has for some time been attempts to design a unified interface for NLP tasks (Kumar et al., 2016;Raffel et al., 2020).In parallel with these works, GPT-2 (Radford et al., 2019) shows that appending trigger tokens (e.g. \"TL;DR\") at the end of language model input can cause language models to behave like summarisation models. The zero-shot capability of language models shows the potential to unify NLP tasks into a language modelling framework where fine-tuning is not necessary to achieve good performance. Furthermore, GPT-3 (Brown et al., 2020) shows that task-agnostic, few-shot performance can be improved by scaling up language models. It can sometimes even become competitive with prior state-of-the-art fine-tuning approaches. We show that our probing method is better than relying on held out examples (Figure 5) and thus enables true few-shot learning.", "publication_ref": ["b15", "b8", "b19", "b18"], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "Conclusion", "text": "We have shown that few-shot prompts suffer from order sensitivity, in that for the same prompt the order in which samples are provided can make the difference between state-of-the-art and random performance. In our analysis of the problem, we established that it is present across tasks, model sizes, prompt templates, samples, and number of training samples. To alleviate this problem, we introduced a novel probing method that exploits the generative nature of language models to construct an artificial development set. We were able to identity performant permutations using entropy-based statistics over this set, leading to an on average 13% improvement across eleven text classification tasks.   ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "not sure where to even begin the only real film on our watch lists no one will care because it is just one story SST-5 not a bad documentary, but the story feels tacked on. one that i have never liked and was always too long to understand and not enjoyable in parts. This movie is the opposite of what it pretentious title implies. DBPedia Gweno Mott's book: Gweno is a New Yorker cartoonist published by Little, Brown, 1995/2002/2013. L. Ego Equestrians is North America's first dedicated equine show in Las Vegas. Graphed is a graph visualization package from the GraphViz project. MR a solid first film for the debut helmer. A good deal more of the material in his previous films can be found here but this film does not come across [...] it is so effective and engaging It feels more real And at some point, maybe it was about [...]", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "CR", "text": "It works just the same, i just prefer my iPhone 6. the battery last so long for me it feels like ive already had my phone a year. works great with both phones MPQA this is really going nowhere why does it look so angry?? Excellent book and will get a good reputation Subj this will become apparent as it gets older. how about something more subtle to show this girl's love? a perfect summary of an episode where the entire series is one massive meta romp, with [...] Premise: In the early 1940s, the United States and the Soviet Union were at war with Germany. Hypothesis: Germany was at war with the United States and Russia.\nPremise: Water is a precious commodity. Hypothesis: Water is not a precious commodity.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "CB", "text": "Premise: In the back corner of Melissa's classroom her father walked through the door and walked across the front. [...] Hypothesis: his curiosity was directed towards some, something other than Melissa Premise: Maggie took Gloria out for a drive to the nearby city limits of Fort Myers on Tuesday Hypothesis: he couldn't bear looking down his nose at all the other houses Premise: There was one in Dallas. When it came out in New Jersey. And there were, [...] Hypothesis: I would never see that movie ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "", "journal": "", "year": "", "authors": "Benjamin Tom B Brown; Nick Mann; Melanie Ryder; Jared Subbiah; Prafulla Kaplan; Arvind Dhariwal; Pranav Neelakantan; Girish Shyam;  Sastry"}, {"ref_id": "b1", "title": "The pascal recognising textual entailment challenge", "journal": "Springer", "year": "2005", "authors": "Oren Ido Dagan; Bernardo Glickman;  Magnini"}, {"ref_id": "b2", "title": "Commonsense knowledge mining from pretrained models", "journal": "", "year": "2019", "authors": "Joe Davison; Joshua Feldman; Alexander M Rush"}, {"ref_id": "b3", "title": "The commitmentbank: Investigating projection in naturally occurring discourse", "journal": "", "year": "2019", "authors": "Marie-Catherine De Marneffe; Mandy Simons; Judith Tonhauser"}, {"ref_id": "b4", "title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b5", "title": "Making pre-trained language models better few-shot learners", "journal": "", "year": "2020", "authors": "Tianyu Gao; Adam Fisch; Danqi Chen"}, {"ref_id": "b6", "title": "Mining and summarizing customer reviews", "journal": "", "year": "2004", "authors": "Minqing Hu; Bing Liu"}, {"ref_id": "b7", "title": "How can we know what language models know?", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020-06", "authors": "Zhengbao Jiang; F Frank;  Xu"}, {"ref_id": "b8", "title": "Ask me anything: Dynamic memory networks for natural language processing", "journal": "PMLR", "year": "2016", "authors": "Ankit Kumar; Ozan Irsoy; Peter Ondruska; Mohit Iyyer; James Bradbury; Ishaan Gulrajani; Victor Zhong; Romain Paulus; Richard Socher"}, {"ref_id": "b9", "title": "What makes good in-context examples for gpt-3? arXiv preprint", "journal": "", "year": "2021", "authors": "Jiachang Liu; Dinghan Shen; Yizhe Zhang; Bill Dolan; Lawrence Carin; Weizhu Chen"}, {"ref_id": "b10", "title": "Roberta: A robustly optimized bert pretraining approach", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b11", "title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "journal": "", "year": "2004", "authors": "Bo Pang; Lillian Lee"}, {"ref_id": "b12", "title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "journal": "", "year": "2005", "authors": "Bo Pang; Lillian Lee"}, {"ref_id": "b13", "title": "A deep reinforced model for abstractive summarization", "journal": "", "year": "2018", "authors": "Romain Paulus; Caiming Xiong; Richard Socher"}, {"ref_id": "b14", "title": "True few-shot learning with language models", "journal": "", "year": "2021", "authors": "Ethan Perez; Douwe Kiela; Kyunghyun Cho"}, {"ref_id": "b15", "title": "Deep contextualized word representations", "journal": "Long Papers", "year": "2018", "authors": "Matthew Peters; Mark Neumann; Mohit Iyyer; Matt Gardner; Christopher Clark; Kenton Lee; Luke Zettlemoyer"}, {"ref_id": "b16", "title": "How context affects language models' factual predictions", "journal": "", "year": "2020", "authors": "Fabio Petroni; Patrick Lewis; Aleksandra Piktus; Tim Rockt\u00e4schel; Yuxiang Wu; Alexander H Miller; Sebastian Riedel"}, {"ref_id": "b17", "title": "Association for Computational Linguistics", "journal": "", "year": "2019", "authors": "Fabio Petroni; Tim Rockt\u00e4schel; Sebastian Riedel; Patrick Lewis; Anton Bakhtin; Yuxiang Wu; Alexander Miller"}, {"ref_id": "b18", "title": "Language models are unsupervised multitask learners", "journal": "", "year": "2019", "authors": "A Radford; Jeffrey Wu; R Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"ref_id": "b19", "title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "Journal of Machine Learning Research", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b20", "title": "It's not just size that matters: Small language models are also few-shot learners", "journal": "", "year": "2020", "authors": "Timo Schick; Hinrich Sch\u00fctze"}, {"ref_id": "b21", "title": "Autoprompt: Eliciting knowledge from language models with automatically generated prompts", "journal": "", "year": "2020", "authors": "Taylor Shin; Yasaman Razeghi; I V Robert L Logan; Eric Wallace; Sameer Singh"}, {"ref_id": "b22", "title": "Recursive deep models for semantic compositionality over a sentiment treebank", "journal": "", "year": "2013", "authors": "Richard Socher; Alex Perelygin; Jean Wu; Jason Chuang; D Christopher;  Manning; Y Andrew; Christopher Ng;  Potts"}, {"ref_id": "b23", "title": "Building a question answering test collection", "journal": "", "year": "2000", "authors": "M Ellen; Dawn M Voorhees;  Tice"}, {"ref_id": "b24", "title": "Annotating expressions of opinions and emotions in language. Language resources and evaluation", "journal": "", "year": "2005", "authors": "Janyce Wiebe; Theresa Wilson; Claire Cardie"}, {"ref_id": "b25", "title": "Xlnet: Generalized autoregressive pretraining for language understanding", "journal": "", "year": "2019", "authors": "Zhilin Yang; Zihang Dai; Yiming Yang; Jaime Carbonell; Ruslan Salakhutdinov; Quoc V Le"}, {"ref_id": "b26", "title": "Character-level convolutional networks for text classification", "journal": "", "year": "2015", "authors": "Xiang Zhang; Junbo Zhao; Yann Lecun"}, {"ref_id": "b27", "title": "Calibrate before use: Improving few-shot performance of language models", "journal": "", "year": "2021", "authors": "Z Tony; Eric Zhao; Shi Wallace; Dan Feng; Sameer Klein;  Singh"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure1: Four-shot performance for 24 different sample orders across different sizes of GPT-family models (GPT-2 and GPT-3) for the SST-2 and Subj datasets.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: Training sample permutations for the Incontext Learning setting. The concatenation of training samples as well as test data transforms the classification task into a sequence generation task.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: Order sensitivity using different numbers of training samples.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 :5Figure 4: Training sample permutation performance correlation across different models.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 6 :6Figure 6: Left: Predicted SST-2 label distribution under different prompts. Right: 2-shot calibrated performance (Zhao et al., 2021) of all possible permutations on GPT2-XL (1.5B).", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 8 :8Figure 8: Average performance of different Top K permutation selection on GPT2-Large (0.8B)", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Procedures for prompt construction.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "49.2 4.7 50.8 11.9 49.7 2.7 50.1 1.0", "figure_data": "SST-2SST-5 DBPediaMRCRMPQASubjTREC AGNewsRTECBMajority50.923.19.450.050.050.050.018.825.052.751.8Finetuning (Full)95.058.799.390.889.487.897.097.494.780.990.5GPT-2 0.1B 52.1 0.7 LocalE 58.9 7.8 29.0 4.9 44.9 9.7 58.6 7.6 58.4 6.4 68.9 7.1 65.2 3.9 34.4 3.4 53.3 4.9 66.0 6.3 65.0 3.4 72.5 6.0 52.9 1.3 48.0 3.961.0 5.953.0 3.3 49.9 1.6GlobalE63.8 5.8 35.8 2.056.1 4.366.4 5.864.8 2.773.5 4.553.0 1.3 46.1 3.762.1 5.753.0 3.0 50.3 1.6Oracle73.5 1.7 38.2 4.060.5 4.274.3 4.970.8 4.481.3 2.555.2 1.7 58.1 4.370.3 2.856.8 2.0 52.1 1.3GPT-2 0.3B61.0 13.2 25.9 5.951.7 7.054.2 7.856.7 9.454.5 8.854.4 7.9 52.6 4.9 47.7 10.6 48.8 2.6 50.2 5.3LocalE75.3 4.6 31.0 3.447.1 3.765.2 6.670.9 6.367.6 7.266.7 9.3 53.0 3.951.2 7.351.8 1.0 47.1 4.2GlobalE78.7 5.2 31.7 5.258.3 5.467.0 5.970.7 6.768.3 6.9 65.8 10.1 53.3 4.659.6 7.251.1 1.9 50.3 3.7Oracle85.5 4.3 40.5 6.365.2 7.674.7 6.180.4 5.477.3 2.379.4 2.4 63.3 2.968.4 8.053.9 1.3 62.5 7.4GPT-2 0.8B74.5 10.3 34.7 8.2 55.0 12.5 64.6 13.1 70.9 12.7 65.5 8.756.4 9.1 56.5 2.7 62.2 11.6 53.2 2.0 38.8 8.5LocalE81.1 5.5 40.3 4.756.7 7.582.6 4.285.4 3.873.6 4.870.4 4.2 56.2 1.762.7 8.153.3 1.6 38.4 5.2GlobalE84.8 4.1 46.9 1.167.7 3.684.3 2.986.7 2.575.8 3.168.6 6.5 57.2 2.370.7 3.653.5 1.5 41.2 4.5Oracle88.9 1.8 48.4 0.772.3 3.387.5 1.189.9 0.980.3 4.976.6 4.1 62.1 1.578.1 1.357.3 1.0 53.2 5.3"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Our main results on subset of the validation set. To fit the data within the GPT-2 model context window size, we use 1-shot for DBPedia, 2-shot for AGNews, 4-shot for other datasets. All the baseline results are calculated based on 5 different random seeds over 24 train context permutations. LocalE and GlobalE results are calculated based on the top 4 context permutations using our proposed approach. For the GPT-3 175B, we only use 2 seeds with 12 different permutations due to a limited computation budget.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": ": Prompt selection performance of different tem-plates on SST-2ID TemplateLabel Mapping1Review: {Sentence} Sentiment: {Label}positive/negative2Input: {Sentence} Prediction: {Label}positive/negative3Review: {Sentence} Sentiment: {Label}good/bad4{Sentence} It was {Label} good/bad"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Different Templates for SST-2", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Comparing our method with splitting the training set into train and development for SST-2.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Gao et al. (2020) uses an external language model to generate templates, andShin et al. (2020) uses gradient-guided search to find templates that maximise performance.Jiang et al. (2020) uses a mining-based method to create multiple diverse templates automatically. While our study is fundamentally different from theirs in that we do not make use of a standard-size training set, we do come to the opposite conclusion. All previous work on prompt design focuses on the textual quality of the prompt and, to the best of our knowledge, none has studied order sensitivity in detail.True Few-shot Learning Perez et al. (2021) evaluated few-shot capability of LMs when a heldout validation set is not available. Experimental result suggested that previous work overestimate the few-shot ability of LMs in this (true few-shot learning) setting. Our work instead use the generative nature of language models to construct a probing set without relying on held-out examples.", "figure_data": "struct templates, Order Sensitivity of Prompt Design Gao et al.(2020) demonstrated that finetuning-based ap-proaches are not as order sensitive as In-contextLearning. Making use of a standard-size trainingset, Liu et al. (2021) used nearest neighbour searchto retrieve the most relevant training samples fora specific test sample. They were successful inretrieving relevant samples and concluded that af-ter retrieving them the order in which they areprovided in the prompt has little to no effect onperformance.Prompt Design for PLMs The core challengeof prompt design is to convert training data (if itexists) into a text sequence. Most work on promptdesign focuses on how to make prompts more com-patible with language models. Petroni et al. (2019)uses human effort to design natural language sen-tences and then perform token prediction given theinput context. However, hand-crafted templatesrequire significant human effort and is likely to endup with sub-optimal performance. Recent work hasexplored automatic template construction: Schickand Sch\u00fctze (2020) uses cloze-style tasks to con-"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Examples of transformation notations.", "figure_data": "Dataset# of Classes Avg. Len. BalancedSST-2 (Socher et al., 2013)212.4YesSST-5 (Socher et al., 2013)523.1NoMR (Pang and Lee, 2005)225.7YesCR (Hu and Liu, 2004)222.1YesMPQA (Wiebe et al., 2005)23.9YesSubj (Pang and Lee, 2004)228.9YesTREC (Voorhees and Tice, 2000)611.6NoAGNews (Zhang et al., 2015)453.8YesDBPedia (Zhang et al., 2015)1465.5YesCB (De Marneffe et al., 2019)369.7/8.4NoRTE (Dagan et al., 2005)255.3/11.9Yes"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Statistics of evaluation datasets, average length is calculated based on GPT-2 sentence-piece length. For sentence-pair tasks, we report each sentence's average length separately.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "S = {t i }, i = 1, \u2022 \u2022 \u2022 , n.", "formula_coordinates": [4.0, 306.14, 277.6, 107.22, 10.64]}, {"formula_id": "formula_1", "formula_text": "F = {f m }, m = 1, \u2022 \u2022 \u2022 , n!,", "formula_coordinates": [4.0, 405.24, 306.18, 120.54, 10.63]}, {"formula_id": "formula_2", "formula_text": "y i,m = argmax v\u2208V P (v|c m \u2295 T (x i ); \u03b8)(1)", "formula_coordinates": [5.0, 100.36, 403.02, 188.78, 18.38]}, {"formula_id": "formula_3", "formula_text": "p v m = i 1 {\u0177 i,m =v} |D| (2)", "formula_coordinates": [5.0, 135.08, 483.2, 154.05, 28.32]}, {"formula_id": "formula_4", "formula_text": "GlobalE m = v\u2208V \u2212p v m log p v m (3) Local Entropy (LocalE)", "formula_coordinates": [5.0, 70.86, 562.19, 218.28, 62.36]}, {"formula_id": "formula_5", "formula_text": "p v i,m = P (x i ,y i )\u223cD (v|c m \u2295 T (x i ); \u03b8), v \u2208 V (4)", "formula_coordinates": [5.0, 77.61, 759.9, 211.53, 17.78]}, {"formula_id": "formula_6", "formula_text": "LocalE m = i v\u2208V \u2212p v i,m log p v i,m |D| (5)", "formula_coordinates": [5.0, 325.18, 305.03, 199.24, 27.86]}], "doi": "10.18653/v1/D19-1250"}