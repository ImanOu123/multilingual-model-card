{"title": "Word representations: A simple and general method for semi-supervised learning", "authors": "Joseph Turian; Lev Ratinov; Yoshua Bengio", "pub_date": "", "abstract": "If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston ( 2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here:", "sections": [{"heading": "Introduction", "text": "By using unlabelled data to reduce data sparsity in the labeled training data, semi-supervised approaches improve generalization accuracy. Semi-supervised models such as Ando and Zhang (2005), Suzuki and Isozaki (2008), and Suzuki et al. (2009) achieve state-of-the-art accuracy. However, these approaches dictate a particular choice of model and training regime. It can be tricky and time-consuming to adapt an existing supervised NLP system to use these semi-supervised techniques. It is preferable to use a simple and general method to adapt existing supervised NLP systems to be semi-supervised.\nOne approach that is becoming popular is to use unsupervised methods to induce word features-or to download word features that have already been induced-plug these word features into an existing system, and observe a significant increase in accuracy. But which word features are good for what tasks? Should we prefer certain word features? Can we combine them?\nA word representation is a mathematical object associated with each word, often a vector. Each dimension's value corresponds to a feature and might even have a semantic or grammatical interpretation, so we call it a word feature. Conventionally, supervised lexicalized NLP approaches take a word and convert it to a symbolic ID, which is then transformed into a feature vector using a one-hot representation: The feature vector has the same length as the size of the vocabulary, and only one dimension is on. However, the one-hot representation of a word suffers from data sparsity: Namely, for words that are rare in the labeled training data, their corresponding model parameters will be poorly estimated. Moreover, at test time, the model cannot handle words that do not appear in the labeled training data. These limitations of one-hot word representations have prompted researchers to investigate unsupervised methods for inducing word representations over large unlabeled corpora. Word features can be hand-designed, but our goal is to learn them.\nOne common approach to inducing unsupervised word representation is to use clustering, perhaps hierarchical. This technique was used by a variety of researchers (Miller et al., 2004;Liang, 2005;Koo et al., 2008;Huang & Yates, 2009). This leads to a one-hot representation over a smaller vocabulary size. Neural language models (Bengio et al., 2001;Schwenk & Gauvain, 2002;Mnih & Hinton, 2007;Collobert & Weston, 2008), on the other hand, induce dense real-valued low-dimensional word embeddings using unsupervised approaches. (See Bengio (2008) for a more complete list of references on neural language models.)\nUnsupervised word representations have been used in previous NLP work, and have demonstrated improvements in generalization accuracy on a variety of tasks. But different word representations have never been systematically compared in a controlled way. In this work, we compare different techniques for inducing word representations, evaluating them on the tasks of named entity recognition (NER) and chunking.\nWe retract former negative results published in Turian et al. (2009) about Collobert and Weston (2008) embeddings, given training improvements that we describe in Section 7.1.", "publication_ref": ["b0", "b42", "b43", "b28", "b22", "b18", "b16", "b2", "b39", "b29", "b9", "b1", "b44", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Distributional representations", "text": "Distributional word representations are based upon a cooccurrence matrix F of size W\u00d7C, where W is the vocabulary size, each row F w is the initial representation of word w, and each column F c is some context. Sahlgren (2006) and Turney and Pantel (2010) describe a handful of possible design decisions in contructing F, including choice of context types (left window? right window? size of window?) and type of frequency count (raw? binary? tf-idf?). F w has dimensionality W, which can be too large to use F w as features for word w in a supervised model. One can map F to matrix f of size W \u00d7 d, where d C, using some function g, where f = g(F). f w represents word w as a vector with d dimensions. The choice of g is another design decision, although perhaps not as important as the statistics used to initially construct F.\nThe self-organizing semantic map (Ritter & Kohonen, 1989) is a distributional technique that maps words to two dimensions, such that syntactically and semantically related words are nearby (Honkela et al., 1995;Honkela, 1997).\nLSA (Dumais et al., 1988;Landauer et al., 1998), LSI, and LDA (Blei et al., 2003) induce distributional representations over F in which each column is a document context. In most of the other approaches discussed, the columns represent word contexts. In LSA, g computes the SVD of F.\nHyperspace Analogue to Language (HAL) is another early distributional approach (Lund et al., 1995;Lund & Burgess, 1996) to inducing word representations. They compute F over a corpus of 160 million word tokens with a vocabulary size W of 70K word types. There are 2\u2022W types of context (columns): The first or second W are counted if the word c occurs within a window of 10 to the left or right of the word w, respectively. f is chosen by taking the 200 columns (out of 140K in F) with the highest variances. ICA is another technique to transform F into f . (V\u00e4yrynen & Honkela, 2004;V\u00e4yrynen & Honkela, 2005;V\u00e4yrynen et al., 2007). ICA is expensive, and the largest vocabulary size used in these works was only 10K. As far as we know, ICA methods have not been used when the size of the vocab W is 100K or more.\nExplicitly storing cooccurrence matrix F can be memory-intensive, and transforming F to f can be time-consuming. It is preferable that F never be computed explicitly, and that f be constructed incrementally.\u0158eh\u016f\u0159ek and Sojka ( 2010) describe an incremental approach to inducing LSA and LDA topic models over 270 millions word tokens with a vocabulary of 315K word types. This is similar in magnitude to our experiments.\nAnother incremental approach to constructing f is using a random projection: Linear mapping g is multiplying F by a random matrix chosen a priori. This random indexing method is motivated by the Johnson-Lindenstrauss lemma, which states that for certain choices of random matrix, if d is sufficiently large, then the original distances between words in F will be preserved in f (Sahlgren, 2005). Kaski (1998) uses this technique to produce 100-dimensional representations of documents. Sahlgren (2001) was the first author to use random indexing using narrow context. Sahlgren (2006) does a battery of experiments exploring different design decisions involved in constructing F, prior to using random indexing. However, like all the works cited above, Sahlgren (2006) only uses distributional representation to improve existing systems for one-shot classification tasks, such as IR, WSD, semantic knowledge tests, and text categorization. It is not well-understood what settings are appropriate to induce distributional word representations for structured prediction tasks (like parsing and MT) and sequence labeling tasks (like chunking and NER). Previous research has achieved repeated successes on these tasks using clustering representations (Section 3) and distributed representations (Section 4), so we focus on these representations in our work.", "publication_ref": ["b37", "b45", "b34", "b15", "b14", "b11", "b20", "b6", "b26", "b25", "b48", "b47", "b50", "b36", "b17", "b35", "b37", "b37"], "figure_ref": [], "table_ref": []}, {"heading": "Clustering-based word representations", "text": "Another type of word representation is to induce a clustering over words. Clustering methods and distributional methods can overlap. For example, Pereira et al. (1993) begin with a cooccurrence matrix and transform this matrix into a clustering.", "publication_ref": ["b32"], "figure_ref": [], "table_ref": []}, {"heading": "Brown clustering", "text": "The Brown algorithm is a hierarchical clustering algorithm which clusters words to maximize the mutual information of bigrams (Brown et al., 1992). So it is a class-based bigram language model. It runs in time O(V\u2022K 2 ), where V is the size of the vocabulary and K is the number of clusters.\nThe hierarchical nature of the clustering means that we can choose the word class at several levels in the hierarchy, which can compensate for poor clusters of a small number of words. One downside of Brown clustering is that it is based solely on bigram statistics, and does not consider word usage in a wider context.\nBrown clusters have been used successfully in a variety of NLP applications: NER (Miller et al., 2004;Liang, 2005;, PCFG parsing (Candito & Crabb\u00e9, 2009), dependency parsing (Koo et al., 2008;Suzuki et al., 2009), and semantic dependency parsing (Zhao et al., 2009). Martin et al. (1998) presents algorithms for inducing hierarchical clusterings based upon word bigram and trigram statistics. Ushioda (1996) presents an extension to the Brown clustering algorithm, and learn hierarchical clusterings of words as well as phrases, which they apply to POS tagging.", "publication_ref": ["b7", "b28", "b22", "b8", "b18", "b43", "b53", "b27", "b46"], "figure_ref": [], "table_ref": []}, {"heading": "Other work on cluster-based word representations", "text": "Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. HMMs can be used to induce a soft clustering, specifically a multinomial distribution over possible clusters (hidden states). Li and McCallum (2005) use an HMM-LDA model to improve POS tagging and Chinese Word Segmentation. Huang and Yates (2009) induce a fully-connected HMM, which emits a multinomial distribution over possible vocabulary words. They perform hard clustering using the Viterbi algorithm. (Alternately, they could keep the soft clustering, with the representation for a particular word token being the posterior probability distribution over the states.) However, the CRF chunker in Huang and Yates (2009), which uses their HMM word clusters as extra features, achieves F1 lower than a baseline CRF chunker (Sha & Pereira, 2003). Goldberg et al. (2009) use an HMM to assign POS tags to words, which in turns improves the accuracy of the PCFG-based Hebrew parser. Deschacht and Moens (2009) use a latent-variable language model to improve semantic role labeling.", "publication_ref": ["b21", "b16", "b16", "b40", "b13", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Distributed representations", "text": "Another approach to word representation is to learn a distributed representation. (Not to be confused with distributional representations.) A distributed representation is dense, lowdimensional, and real-valued. Distributed word representations are called word embeddings. Each dimension of the embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic properties. A distributed representation is compact, in the sense that it can represent an exponential number of clusters in the number of dimensions.\nWord embeddings are typically induced using neural language models, which use neural networks as the underlying predictive model (Bengio, 2008). Historically, training and testing of neural language models has been slow, scaling as the size of the vocabulary for each model computation (Bengio et al., 2001;. However, many approaches have been proposed in recent years to eliminate that linear dependency on vocabulary size (Morin & Bengio, 2005;Collobert & Weston, 2008;Mnih & Hinton, 2009) and allow scaling to very large training corpora.", "publication_ref": ["b1", "b2", "b31", "b9", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "Collobert and Weston (2008) embeddings", "text": "Collobert and Weston ( 2008) presented a neural language model that could be trained over billions of words, because the gradient of the loss was computed stochastically over a small sample of possible outputs, in a spirit similar to Bengio and S\u00e9n\u00e9cal (2003). This neural model of Collobert and Weston (2008) was refined and presented in greater depth in Bengio et al. (2009). The model is discriminative and nonprobabilistic.\nFor each training update, we read an n-gram x = (w 1 , . . . , w n ) from the corpus. The model concatenates the learned embeddings of the n words, giving e(w 1 ) \u2295 . . . \u2295 e(w n ), where e is the lookup table and \u2295 is concatenation. We also create a corrupted or noise n-gram x = (w 1 , . . . , w n\u2212q ,w n ), wherew n w n is chosen uniformly from the vocabulary. 1 For convenience, we write e(x) to mean e(w 1 ) \u2295 . . . \u2295 e(w n ). We predict a score s(x) for x by passing e(x) through a single hidden layer neural network. The training criterion is that n-grams that are present in the training corpus like x must have a score at least some margin higher than corrupted n-grams lik\u1ebd x. Specifically: L(x) = max(0, 1 \u2212 s(x) + s(x)). We minimize this loss stochastically over the n-grams in the corpus, doing gradient descent simultaneously over the neural network parameters and the embedding lookup table.\nWe implemented the approach of Collobert and Weston (2008), with the following differences: \u2022 We did not achieve as low log-ranks on the English Wikipedia as the authors reported in Bengio et al. (2009), despite initially attempting to have identical experimental conditions.\n\u2022 We corrupt the last word of each n-gram.\n\u2022 We had a separate learning rate for the embeddings and for the neural network weights. We found that the embeddings should have a learning rate generally 1000-32000 times higher than the neural network weights. Otherwise, the unsupervised training criterion drops slowly.\n\u2022 Although their sampling technique makes training fast, testing is still expensive when the size of the vocabulary is large. Instead of cross-validating using the log-rank over the validation data as they do, we instead used the moving average of the training loss on training examples before the weight update.", "publication_ref": ["b5", "b9", "b4", "b9", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "HLBL embeddings", "text": "The log-bilinear model (Mnih & Hinton, 2007) is a probabilistic and linear neural model. Given an n-gram, the model concatenates the embeddings of the n \u2212 1 first words, and learns a linear model to predict the embedding of the last word. The similarity between the predicted embedding and the current actual embedding is transformed into a probability by exponentiating and then normalizing. Mnih and Hinton (2009) speed up model evaluation during training and testing by using a hierarchy to exponentially filter down the number of computations that are performed. This hierarchical evaluation technique was first proposed by Morin and Bengio (2005). The model, combined with this optimization, is called the hierarchical log-bilinear (HLBL) model. n-gram is corrupted. In Bengio et al. (2009), the last word in the n-gram is corrupted.", "publication_ref": ["b29", "b30", "b31", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Supervised evaluation tasks", "text": "We evaluate the hypothesis that one can take an existing, near state-of-the-art, supervised NLP system, and improve its accuracy by including word representations as word features. This technique for turning a supervised approach into a semi-supervised one is general and task-agnostic.\nHowever, we wish to find out if certain word representations are preferable for certain tasks. Lin and Wu (2009) finds that the representations that are good for NER are poor for search query classification, and vice-versa. We apply clustering and distributed representations to NER and chunking, which allows us to compare our semi-supervised models to those of Ando and Zhang (2005) and Suzuki and Isozaki (2008).", "publication_ref": ["b24", "b0", "b42"], "figure_ref": [], "table_ref": []}, {"heading": "Chunking", "text": "Chunking is a syntactic sequence labeling task. We follow the conditions in the CoNLL-2000 shared task (Sang & Buchholz, 2000).\nThe linear CRF chunker of Sha and Pereira ( 2003) is a standard near-state-of-the-art baseline chunker. In fact, many off-the-shelf CRF implementations now replicate Sha and Pereira (2003), including their choice of feature set:\n\u2022 CRF++ by Taku Kudo (http://crfpp.\nsourceforge.net/) \u2022 crfsgd by L\u00e9on Bottou (http://leon.\nbottou.org/projects/sgd) \u2022 CRFsuite by by Naoaki Okazaki (http:// www.chokkan.org/software/crfsuite/)\nWe use CRFsuite because it makes it simple to modify the feature generation code, so one can easily add new features.\nWe use SGD optimization, and enable negative state features and negative transition features. (\"feature.possible transitions=1, feature.possible states=1\")\nTable 1 shows the features in the baseline chunker. As you can see, the Brown and embedding features are unigram features, and do not participate in conjunctions like the word features and tag features do. Koo et al. (2008) sees further accuracy improvements on dependency parsing when using word representations in compound features.\nThe data comes from the Penn Treebank, and is newswire from the Wall Street Journal in 1989. Of the 8936 training sentences, we used 1000 randomly sampled sentences (23615 words) for development. We trained models on the 7936\n\u2022 Word features: w i for i in {\u22122, \u22121, 0, +1, +2}, w i \u2227 w i+1 for i in {\u22121, 0}. \u2022 Tag features: w i for i in {\u22122, \u22121, 0, +1, +2}, t i \u2227 t i+1 for i in {\u22122, \u22121, 0, +1}. t i \u2227 t i+1 \u2227 t i+2 for i in {\u22122, \u22121, 0}. \u2022 Embedding features [if applicable]: e i [d] for i in {\u22122, \u22121, 0, +1, +2}\n, where d ranges over the dimensions of the embedding e i . \u2022 Brown features [if applicable]: substr(b i , 0, p)\nfor i in {\u22122, \u22121, 0, +1, +2}, where substr takes the p-length prefix of the Brown cluster b i . training partition sentences, and evaluated their F1 on the development set. After choosing hyperparameters to maximize the dev F1, we would retrain the model using these hyperparameters on the full 8936 sentence training set, and evaluate on test. One hyperparameter was l2-regularization sigma, which for most models was optimal at 2 or 3.2. The word embeddings also required a scaling hyperparameter, as described in Section 7.2.", "publication_ref": ["b38", "b40", "b18"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Named entity recognition", "text": "NER is typically treated as a sequence prediction problem. Following Ratinov and Roth (2009), we use the regularized averaged perceptron model. Ratinov and Roth (2009) describe different sequence encoding like BILOU and BIO, and show that the BILOU encoding outperforms BIO, and the greedy inference performs competitively to Viterbi while being significantly faster. Accordingly, we use greedy inference and BILOU text chunk representation. We use the publicly available implementation from Ratinov and Roth (2009) (see the end of this paper for the URL). In our baseline experiments, we remove gazetteers and non-local features (Krishnan & Manning, 2006). However, we also run experiments that include these features, to understand if the information they provide mostly overlaps with that of the word representations.\nAfter each epoch over the training set, we measured the accuracy of the model on the development set. Training was stopped after the accuracy on the development set did not improve for 10 epochs, generally about 50-80 epochs total. The epoch that performed best on the development set was chosen as the final model.\nWe use the following baseline set of features from Zhang and Johnson (2003):\n\u2022 Previous two predictions y i\u22121 and y i\u22122 \u2022 Current word x i \u2022 x i word type information: all-capitalized, is-capitalized, all-digits, alphanumeric, etc.\n\u2022 Prefixes and suffixes of x i , if the word contains hyphens, then the tokens between the hyphens\n\u2022 Tokens in the window c = (x i\u22122 , x i\u22121 , x i , x i+1 , x i+2 )\n\u2022 Capitalization pattern in the window c \u2022 Conjunction of c and y i\u22121 . Word representation features, if present, are used the same way as in Table 1.\nWhen using the lexical features, we normalize dates and numbers. For example, 1980 becomes *DDDD* and 212-325-4751 becomes *DDD*-*DDD*-*DDDD*. This allows a degree of abstraction to years, phone numbers, etc. This delexicalization is performed separately from using the word representation. That is, if we have induced an embedding for 12/3/2008 , we will use the embedding of 12/3/2008 , and *DD*/*D*/*DDDD* in the baseline features listed above.\nUnlike in our chunking experiments, after we chose the best model on the development set, we used that model on the test set too. (In chunking, after finding the best hyperparameters on the development set, we would combine the dev and training set and training a model over this combined set, and then evaluate on test.)\nThe standard evaluation benchmark for NER is the CoNLL03 shared task dataset drawn from the Reuters newswire. These postprocessing steps will adversely affect all NER models across-the-board, nonetheless allowing us to compare different models in a controlled manner.", "publication_ref": ["b33", "b33", "b33", "b19", "b52"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Unlabled Data", "text": "Unlabeled data is used for inducing the word representations. We used the RCV1 corpus, which contains one year of Reuters English newswire, from August 1996 to August 1997, about 63 millions words in 3.3 million sentences. We left case intact in the corpus. By comparison, Collobert and Weston (2008) downcases words and delexicalizes numbers.\nWe use a preprocessing technique proposed by Liang, (2005, p. 51), which was later used by Koo et al. (2008): Remove all sentences that are less than 90% lowercase a-z. We assume that whitespace is not counted, although this is not specified in Liang's thesis. We call this preprocessing step cleaning.\nIn Turian et al. (2009), we found that all word representations performed better on the supervised task when they were induced on the clean unlabeled data, both embeddings and Brown clusters. This is the case even though the cleaning process was very aggressive, and discarded more than half of the sentences. According to the evidence and arguments presented in Bengio et al. (2009), the non-convex optimization process for Collobert and Weston (2008) embeddings might be adversely affected by noise and the statistical sparsity issues regarding rare words, especially at the beginning of training. For this reason, we hypothesize that learning representations over the most frequent words first and gradually increasing the vocabulary-a curriculum training strategy (Elman, 1993;Bengio et al., 2009;Spitkovsky et al., 2010)-would provide better results than cleaning.\nAfter cleaning, there are 37 million words (58% of the original) in 1.3 million sentences (41% of the original). The cleaned RCV1 corpus has 269K word types. This is the vocabulary size, i.e. how many word representations were induced. Note that cleaning is applied only to the unlabeled data, not to the labeled data used in the supervised tasks.\nRCV1 is a superset of the CoNLL03 corpus. For this reason, NER results that use RCV1 word representations are a form of transductive learning.", "publication_ref": ["b9", "b18", "b44", "b4", "b9", "b12", "b4", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments and Results", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Details of inducing word representations", "text": "The Brown clusters took roughly 3 days to induce, when we induced 1000 clusters, the baseline in prior work (Koo et al., 2008;. We also induced 100, 320, and 3200 Brown clusters, for comparison. (Because Brown clustering scales quadratically in the number of clusters, inducing 10000 clusters would have been prohibitive.) Because Brown clusters are hierarchical, we can use cluster supersets as features. We used clusters at path depth 4, 6, 10, and 20 . These are the prefixes used in Table 1.\nThe Collobert and Weston (2008) (C&W) embeddings were induced over the course of a few weeks, and trained for about 50 epochs. One of the difficulties in inducing these embeddings is that there is no stopping criterion defined, and that the quality of the embeddings can keep improving as training continues. Collobert (p.c.) simply leaves one computer training his embeddings indefinitely. We induced embeddings with 25, 50, 100, or 200 dimensions over 5-gram windows. In comparison to Turian et al. (2009), we use improved C&W embeddings in this work: \u2022 They were trained for 50 epochs, not just 20 epochs.\n\u2022 We initialized all embedding dimensions uniformly in the range [-0.01, +0.01], not [-1,+1]. For rare words, which are typically updated only 143 times per epoch 2 , and given that our embedding learning rate was typically 1e-6 or 1e-7, this means that rare word embeddings will be concentrated around zero, instead of spread out randomly.\nThe HLBL embeddings were trained for 100 epochs (7 days). 3 Unlike our Collobert and Weston ( 2008) embeddings, we did not extensively tune the learning rates for HLBL. We used a learning rate of 1e-3 for both model parameters and embedding parameters. We induced embeddings with 100 dimensions over 5-gram windows, and embeddings with 50 dimensions over 5-gram windows. Embeddings were induced over one pass approach using a random tree, not two passes with an updated tree and embeddings re-estimation.", "publication_ref": ["b18", "b9", "b44"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Scaling of Word Embeddings", "text": "Like many NLP systems, the baseline system contains only binary features. The word embeddings, however, are real numbers that are not necessarily in a bounded range. If the range of the word embeddings is too large, they will exert more influence than the binary features.\nWe generally found that embeddings had zero mean. We can scale the embeddings by a hyperparameter, to control their standard deviation. Assume that the embeddings are represented by a matrix E:\nE \u2190 \u03c3 \u2022 E/stddev(E)(1)\n\u03c3 is a scaling constant that sets the new standard deviation after scaling the embeddings.\n(a)  Figure 1 shows the effect of scaling factor \u03c3 on both supervised tasks. We were surprised to find that on both tasks, across Collobert and Weston (2008) and HLBL embeddings of various dimensionality, that all curves had similar shapes and optima. This is one contributions of our work. In Turian et al. (2009), we were not able to prescribe a default value for scaling the embeddings. However, these curves demonstrate that a reasonable choice of scale factor is such that the embeddings have a standard deviation of 0.1.", "publication_ref": ["b9", "b44"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Capacity of Word Representations", "text": "(a)  There are capacity controls for the word representations: number of Brown clusters, and number of dimensions of the word embeddings. Figure 2 shows the effect on the validation F1 as we vary the capacity of the word representations.\nIn general, it appears that more Brown clusters are better. We would like to induce 10000 Brown clusters, however this would take several months.\nIn Turian et al. (2009), we hypothesized on the basis of solely the HLBL NER curve that higher-dimensional word embeddings would give higher accuracy. Figure 2 shows that this hypothesis is not true. For NER, the C&W curve is almost flat, and we were suprised to find the even 25-dimensional C&W word embeddings work so well. For chunking, 50-dimensional embeddings had the highest validation F1 for both C&W and HLBL. These curves indicates that the optimal capacity of the word embeddings is task-specific.   ", "publication_ref": ["b44"], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "Final results", "text": "Table 2 shows the final chunking results and Table 3 shows the final NER F1 results. We compare to the state-of-the-art methods of Ando and Zhang (2005), Suzuki and Isozaki (2008), and-for NER- Lin and Wu (2009). Tables 2 and 3 show that accuracy can be increased further by combining the features from different types of word representations. But, if only one word representation is to be used, Brown clusters have the highest accuracy. Given the improvements to the C&W embeddings since Turian et al. (2009), C&W embeddings outperform the HLBL embeddings. On chunking, there is only a minute difference between Brown clusters and the embeddings. Com-  bining representations leads to small increases in the test F1. In comparison to chunking, combining different word representations on NER seems gives larger improvements on the test F1.\nOn NER, Brown clusters are superior to the word embeddings. Since much of the NER F1 is derived from decisions made over rare words, we suspected that Brown clustering has a superior representation for rare words. Brown makes a single hard clustering decision, whereas the embedding for a rare word is close to its initial value since it hasn't received many training updates (see Footnote 2). Figure 3 shows the total number of per-token errors incurred on the test set, depending upon the frequency of the word token in the unlabeled data. For NER, Figure 3 (b) shows that most errors occur on rare words, and that Brown clusters do indeed incur fewer errors for rare words. This supports our hypothesis that, for rare words, Brown clustering produces better representations than word embeddings that haven't received sufficient training updates. For chunking, Brown clusters and C&W embeddings incur almost identical numbers of errors, and errors are concentrated around the more common words. We hypothesize that non-rare words have good representations, regardless of the choice of word representation technique. For tasks like chunking in which a syntactic decision relies upon looking at several token simultaneously, compound features that use the word representations might increase accuracy more (Koo et al., 2008).\nUsing word representations in NER brought larger gains on the out-of-domain data than on the in-domain data. We were surprised by this result, because the OOD data was not even used during the unsupervised word representation induction, as was the in-domain data. We are curious to investigate this phenomenon further. Ando and Zhang (2005) present a semisupervised learning algorithm called alternating structure optimization (ASO). They find a lowdimensional projection of the input features that gives good linear classifiers over auxiliary tasks. These auxiliary tasks are sometimes specific to the supervised task, and sometimes general language modeling tasks like \"predict the missing word\". Suzuki and Isozaki (2008) present a semisupervised extension of CRFs. (In Suzuki et al. (2009), they extend their semi-supervised approach to more general conditional models.) One of the advantages of the semi-supervised learning approach that we use is that it is simpler and more general than that of Ando and Zhang (2005) and Suzuki and Isozaki (2008). Their methods dictate a particular choice of model and training regime and could not, for instance, be used with an NLP system based upon an SVM classifier. Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce. Since they can scale to millions of phrases, and they train over 800B unlabeled words, they achieve state-of-the-art accuracy on NER using their phrase clusters. This suggests that extending word representations to phrase representations is worth further investigation.", "publication_ref": ["b0", "b42", "b24", "b44", "b18", "b0", "b42", "b43", "b0", "b42", "b24"], "figure_ref": ["fig_3", "fig_3"], "table_ref": ["tab_5", "tab_6", "tab_5"]}, {"heading": "Conclusions", "text": "Word features can be learned in advance in an unsupervised, task-inspecific, and model-agnostic manner. These word features, once learned, are easily disseminated with other researchers, and easily integrated into existing supervised NLP systems. The disadvantage, however, is that accuracy might not be as high as a semi-supervised method that includes task-specific information and that jointly learns the supervised and unsupervised tasks (Ando & Zhang, 2005;Suzuki & Isozaki, 2008;Suzuki et al., 2009). Unsupervised word representations have been used in previous NLP work, and have demonstrated improvements in generalization accuracy on a variety of tasks. Ours is the first work to systematically compare different word representations in a controlled way. We found that Brown clusters and word embeddings both can improve the accuracy of a near-state-of-the-art supervised NLP system. We also found that combining different word representations can improve accuracy further. Error analysis indicates that Brown clustering induces better representations for rare words than C&W embeddings that have not received many training updates.\nAnother contribution of our work is a default method for setting the scaling parameter for word embeddings. With this contribution, word embeddings can now be used off-the-shelf as word features, with no tuning.\nFuture work should explore methods for inducing phrase representations, as well as techniques for increasing in accuracy by using word representations in compound features.", "publication_ref": ["b0", "b42", "b43"], "figure_ref": [], "table_ref": []}, {"heading": "Replicating our experiments", "text": "You can visit http://metaoptimize.com/ projects/wordreprs/ to find: The word representations we induced, which you can download and use in your experiments; The code for inducing the word representations, which you can use to induce word representations on your own data; The NER and chunking system, with code for replicating our experiments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "Thank you to Magnus Sahlgren, Bob Carpenter, Percy Liang, Alexander Yates, and the anonymous reviewers for useful discussion. Thank you to Andriy Mnih for inducing his embeddings on RCV1 for us. Joseph Turian and Yoshua Bengio acknowledge the following agencies for research funding and computing support: NSERC, RQCHP, CIFAR. Lev Ratinov was supported by the Air Force Research Laboratory (AFRL) under prime contract no. FA8750-09-C-0181. Any opinions, findings, and conclusion or recommendations expressed in this material are those of the author and do not necessarily reflect the view of the Air Force Research Laboratory (AFRL).", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "A highperformance semi-supervised learning method for text chunking", "journal": "ACL", "year": "2005", "authors": "R Ando; T Zhang"}, {"ref_id": "b1", "title": "Neural net language models", "journal": "", "year": "2008", "authors": "Y Bengio"}, {"ref_id": "b2", "title": "A neural probabilistic language model. NIPS", "journal": "", "year": "2001", "authors": "Y Bengio; R Ducharme; P Vincent"}, {"ref_id": "b3", "title": "A neural probabilistic language model", "journal": "Journal of Machine Learning Research", "year": "2003", "authors": "Y Bengio; R Ducharme; P Vincent; C Jauvin"}, {"ref_id": "b4", "title": "Curriculum learning. ICML", "journal": "", "year": "2009", "authors": "Y Bengio; J Louradour; R Collobert; J Weston"}, {"ref_id": "b5", "title": "Quick training of probabilistic neural nets by importance sampling", "journal": "AISTATS", "year": "2003", "authors": "Y Bengio; J.-S S\u00e9n\u00e9cal"}, {"ref_id": "b6", "title": "Latent dirichlet allocation", "journal": "Journal of Machine Learning Research", "year": "2003", "authors": "D M Blei; A Y Ng; M I Jordan"}, {"ref_id": "b7", "title": "Class-based n-gram models of natural language", "journal": "Computational Linguistics", "year": "1992", "authors": "P F Brown; P V Desouza; R L Mercer; V J D Pietra; J C Lai"}, {"ref_id": "b8", "title": "Improving generative statistical parsing with semi-supervised word clustering", "journal": "IWPT", "year": "2009", "authors": "M Candito; B Crabb\u00e9"}, {"ref_id": "b9", "title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "journal": "", "year": "2008", "authors": "R Collobert; J Weston"}, {"ref_id": "b10", "title": "Semisupervised semantic role labeling using the Latent Words Language Model", "journal": "EMNLP", "year": "2009", "authors": "K Deschacht; M.-F Moens"}, {"ref_id": "b11", "title": "Using latent semantic analysis to improve access to textual information", "journal": "ACM", "year": "1988", "authors": "S T Dumais; G W Furnas; T K Landauer; S Deerwester; R Harshman"}, {"ref_id": "b12", "title": "Learning and development in neural networks: The importance of starting small", "journal": "Cognition", "year": "1993", "authors": "J L Elman"}, {"ref_id": "b13", "title": "Enhancing unlexicalized parsing performance using a wide coverage lexicon, fuzzy tag-set mapping, and EM-HMM-based lexical probabilities", "journal": "", "year": "2009", "authors": "Y Goldberg; R Tsarfaty; M Adler; M Elhadad"}, {"ref_id": "b14", "title": "Self-organizing maps of words for natural language processing applications", "journal": "", "year": "1997", "authors": "T Honkela"}, {"ref_id": "b15", "title": "Contextual relations of words in grimm tales, analyzed by self-organizing map", "journal": "", "year": "1995", "authors": "T Honkela; V Pulkki; T Kohonen"}, {"ref_id": "b16", "title": "Distributional representations for handling sparsity in supervised sequence labeling", "journal": "ACL", "year": "2009", "authors": "F Huang; A Yates"}, {"ref_id": "b17", "title": "Dimensionality reduction by random mapping: Fast similarity computation for clustering. IJCNN", "journal": "", "year": "1998", "authors": "S Kaski"}, {"ref_id": "b18", "title": "Simple semi-supervised dependency parsing", "journal": "ACL", "year": "2008", "authors": "T Koo; X Carreras; M Collins"}, {"ref_id": "b19", "title": "An effective two-stage model for exploiting nonlocal dependencies in named entity recognition", "journal": "COLING-ACL", "year": "2006", "authors": "V Krishnan; C D Manning"}, {"ref_id": "b20", "title": "An introduction to latent semantic analysis", "journal": "", "year": "1998", "authors": "T K Landauer; P W Foltz; D Laham"}, {"ref_id": "b21", "title": "Semi-supervised sequence modeling with syntactic topic models", "journal": "AAAI", "year": "2005", "authors": "W Li; A Mccallum"}, {"ref_id": "b22", "title": "", "journal": "", "year": "2005", "authors": "P Liang"}, {"ref_id": "b23", "title": "Master's thesis, Massachusetts Institute of Technology", "journal": "", "year": "", "authors": ""}, {"ref_id": "b24", "title": "Phrase clustering for discriminative learning", "journal": "ACL-IJCNLP", "year": "2009", "authors": "D Lin; X Wu"}, {"ref_id": "b25", "title": "Producing highdimensional semantic spaces from lexical co-occurrence", "journal": "Behavior Research Methods, Instrumentation, and Computers", "year": "1996", "authors": "K Lund; C Burgess"}, {"ref_id": "b26", "title": "Semantic and associative priming in highdimensional semantic space", "journal": "Cognitive Science Proceedings", "year": "1995", "authors": "K Lund; C Burgess; R A Atchley"}, {"ref_id": "b27", "title": "Algorithms for bigram and trigram word clustering", "journal": "Speech Communication", "year": "1998", "authors": "S Martin; J Liermann; H Ney"}, {"ref_id": "b28", "title": "Name tagging with word clusters and discriminative training", "journal": "HLT-NAACL", "year": "2004", "authors": "S Miller; J Guinness; A Zamanian"}, {"ref_id": "b29", "title": "", "journal": "", "year": "2007", "authors": "A Mnih; G E Hinton"}, {"ref_id": "b30", "title": "A scalable hierarchical distributed language model. NIPS", "journal": "", "year": "2009", "authors": "A Mnih; G E Hinton"}, {"ref_id": "b31", "title": "Hierarchical probabilistic neural network language model", "journal": "AISTATS", "year": "2005", "authors": "F Morin; Y Bengio"}, {"ref_id": "b32", "title": "Distributional clustering of english words. ACL", "journal": "", "year": "1993", "authors": "F Pereira; N Tishby; L Lee"}, {"ref_id": "b33", "title": "Design challenges and misconceptions in named entity recognition", "journal": "CoNLL", "year": "2009", "authors": "L Ratinov; D Roth"}, {"ref_id": "b34", "title": "Self-organizing semantic maps", "journal": "Biological Cybernetics", "year": "1989", "authors": "H Ritter; T Kohonen"}, {"ref_id": "b35", "title": "Vector-based semantic analysis: Representing word meanings based on random labels", "journal": "ESSLLI", "year": "2001", "authors": "M Sahlgren"}, {"ref_id": "b36", "title": "An introduction to random indexing", "journal": "", "year": "2005", "authors": "M Sahlgren"}, {"ref_id": "b37", "title": "The word-space model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces. Doctoral dissertation", "journal": "", "year": "2006", "authors": "M Sahlgren"}, {"ref_id": "b38", "title": "Introduction to the CoNLL-2000 shared task: Chunking. CoNLL", "journal": "", "year": "2000", "authors": "E T Sang; S Buchholz"}, {"ref_id": "b39", "title": "Connectionist language modeling for large vocabulary continuous speech recognition", "journal": "", "year": "2002", "authors": "H Schwenk; J.-L Gauvain"}, {"ref_id": "b40", "title": "Shallow parsing with conditional random fields. HLT-NAACL", "journal": "", "year": "2003", "authors": "F Sha; F C N Pereira"}, {"ref_id": "b41", "title": "From baby steps to leapfrog: How \"less is more", "journal": "", "year": "2010", "authors": "V Spitkovsky; H Alshawi; D Jurafsky"}, {"ref_id": "b42", "title": "Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data", "journal": "", "year": "2008", "authors": "J Suzuki; H Isozaki"}, {"ref_id": "b43", "title": "An empirical study of semi-supervised structured conditional models for dependency parsing", "journal": "EMNLP", "year": "2009", "authors": "J Suzuki; H Isozaki; X Carreras; M Collins"}, {"ref_id": "b44", "title": "A preliminary evaluation of word representations for named-entity recognition", "journal": "", "year": "2009", "authors": "J Turian; L Ratinov; Y Bengio; D Roth"}, {"ref_id": "b45", "title": "From frequency to meaning: Vector space models of semantics", "journal": "Journal of Artificial Intelligence Research", "year": "2010", "authors": "P D Turney; P Pantel"}, {"ref_id": "b46", "title": "Hierarchical clustering of words. COLING", "journal": "", "year": "1996", "authors": "A Ushioda"}, {"ref_id": "b47", "title": "Comparison of independent component analysis and singular value decomposition in word context analysis", "journal": "", "year": "2005", "authors": "J V\u00e4yrynen; T Honkela"}, {"ref_id": "b48", "title": "Word category maps based on emergent features created by ICA", "journal": "", "year": "2004", "authors": "J J V\u00e4yrynen; T Honkela"}, {"ref_id": "b49", "title": "", "journal": "Finnish Artificial Intelligence Society", "year": "", "authors": ""}, {"ref_id": "b50", "title": "Towards explicit semantic features using independent component analysis. Proceedings of the Workshop Semantic Content Acquisition and Representation (SCAR)", "journal": "", "year": "2007", "authors": "J J V\u00e4yrynen; T Honkela; L Lindqvist"}, {"ref_id": "b51", "title": "Software framework for topic modelling with large corpora", "journal": "LREC", "year": "2010", "authors": "R Reh\u016f\u0159ek; P Sojka"}, {"ref_id": "b52", "title": "A robust risk minimization based named entity recognition system", "journal": "CoNLL", "year": "2003", "authors": "T Zhang; D Johnson"}, {"ref_id": "b53", "title": "Multilingual dependency learning: a huge feature engineering method to semantic dependency parsing", "journal": "CoNLL", "year": "2009", "authors": "H Zhao; W Chen; C Kit; G Zhou"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Effect as we vary the scaling factor \u03c3 (Equation 1) on the validation set F1. We experiment with Collobert and Weston (2008) and HLBL embeddings of various dimensionality. (a) Chunking results. (b) NER results.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Effect as we vary the capacity of the word representations on the validation set F1. (a) Chunking results. (b) NER results.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: For word tokens that have different frequency in the unlabeled data, what is the total number of per-token errors incurred on the test set? (a) Chunking results. (b) NER results.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Features templates used in the CRF chunker.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Final chunking F1 results. In the last section, we show how many unlabeled words were used.", "figure_data": "SystemDev Test MUC7Baseline90.03 84.39 67.48Baseline+Nonlocal91.91 86.52 71.80HLBL 100-dim92.00 88.13 75.25Gazetteers92.09 87.36 77.76C&W 50-dim92.27 87.93 75.74Brown, 1000 clusters92.32 88.52 78.84C&W 200-dim92.46 87.96 75.51C&W+HLBL92.52 88.56 78.64Brown+HLBL92.56 88.93 77.85Brown+C&W92.79 89.31 80.13HLBL+Gaz92.91 89.35 79.29C&W+Gaz92.98 88.88 81.44Brown+Gaz93.25 89.41 82.71Lin and Wu (2009), 3.4B-88.44-Ando and Zhang (2005), 27M93.15 89.31-Suzuki and Isozaki (2008), 37M93.66 89.36-Suzuki and Isozaki (2008), 1B94.48 89.92-All (Brown+C&W+HLBL+Gaz), 37M 93.17 90.04 82.50All+Nonlocal, 37M93.95 90.36 84.15Lin and Wu (2009), 700B-90.90-"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "", "figure_data": ": Final NER F1 results, showing the cumulativeeffect of adding word representations, non-local features, andgazetteers to the baseline. To speed up training, in combinedexperiments (C&W plus another word representation),we used the 50-dimensional C&W embeddings, not the200-dimensional ones. In the last section, we show howmany unlabeled words were used."}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u2022 Word features: w i for i in {\u22122, \u22121, 0, +1, +2}, w i \u2227 w i+1 for i in {\u22121, 0}. \u2022 Tag features: w i for i in {\u22122, \u22121, 0, +1, +2}, t i \u2227 t i+1 for i in {\u22122, \u22121, 0, +1}. t i \u2227 t i+1 \u2227 t i+2 for i in {\u22122, \u22121, 0}. \u2022 Embedding features [if applicable]: e i [d] for i in {\u22122, \u22121, 0, +1, +2}", "formula_coordinates": [5.0, 72.0, 62.1, 218.27, 92.2]}, {"formula_id": "formula_1", "formula_text": "\u2022 Tokens in the window c = (x i\u22122 , x i\u22121 , x i , x i+1 , x i+2 )", "formula_coordinates": [5.0, 307.28, 160.43, 218.27, 25.01]}, {"formula_id": "formula_2", "formula_text": "E \u2190 \u03c3 \u2022 E/stddev(E)(1)", "formula_coordinates": [7.0, 133.79, 272.94, 156.47, 10.56]}], "doi": ""}