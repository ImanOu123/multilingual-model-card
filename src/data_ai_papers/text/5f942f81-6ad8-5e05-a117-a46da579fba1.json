{"title": "Active Evaluation: Efficient NLG Evaluation with Few Pairwise Comparisons", "authors": "Akash Kumar Mohankumar; Mitesh M Khapra", "pub_date": "", "abstract": "Recent studies have shown the advantages of evaluating NLG systems using pairwise comparisons as opposed to direct assessment. Given k systems, a naive approach for identifying the top-ranked system would be to uniformly obtain pairwise comparisons from all k 2 pairs of systems. However, this can be very expensive as the number of human annotations required would grow quadratically with k. In this work, we introduce Active Evaluation, a framework to efficiently identify the top-ranked system by actively choosing system pairs for comparison using dueling bandit algorithms. We perform extensive experiments with 13 dueling bandits algorithms on 13 NLG evaluation datasets spanning 5 tasks and show that the number of human annotations can be reduced by 80%. To further reduce the number of human annotations, we propose model-based dueling bandit algorithms which combine automatic evaluation metrics with human evaluations. Specifically, we eliminate sub-optimal systems even before the human annotation process and perform human evaluations only on test examples where the automatic metric is highly uncertain. This reduces the number of human annotations required further by 89%. In effect, we show that identifying the top-ranked system requires only a few hundred human annotations, which grow linearly with k. Lastly, we provide practical recommendations and best practices to identify the top-ranked system efficiently. Our code has been made publicly available at https: //github.com/akashkm99/duelnlg", "sections": [{"heading": "Introduction", "text": "In the last few years, the field of NLG has made rapid progress with the advent of large-scale models trained on massive amounts of data (Vaswani et al., 2017;Xue et al., 2020;Liu et al., 2020;Brown et al., 2020). However, evaluation of NLG systems continues to be a challenge. On the one hand, we have automatic evaluation metrics which are easy to compute but unreliable. In particular, many studies have shown that they do not correlate well with human judgments (Novikova et al., 2017;Elliott and Keller, 2014;Sai et al., 2019Sai et al., , 2020a. On the other hand, we have human evaluations, which are relatively more reliable but tedious, expensive, and time-consuming. Further, recent studies have highlighted some limitations of human evaluations that involve direct assessment on an absolute scale, e.g., Likert scale. Specifically, human evaluations using direct assessment have been shown to suffer from annotator bias, high variance and sequence effects where the annotation of one item is influenced by preceding items (Kulikov et al., 2019;Sudoh et al., 2021;Liang et al., 2020;See et al., 2019;Mathur et al., 2017).\nIn this work, we focus on reducing the cost and time required for human evaluations while not compromising on reliability. We take motivation from studies which show that selecting the better of two options is much easier for human annotators than providing an absolute score, which requires annotators to maintain a consistent standard across samples (Kendall, 1948;Simpson and Gurevych, 2018). In particular, recent works show that ranking NLG systems using pairwise comparisons is a more reliable alternative than using direct assessment (See et al., 2019;Li et al., 2019;Sedoc et al., 2019;Dhingra et al., 2019). While this is promising, a naive approach for identifying the top-ranked system from a set of k systems using uniform exploration is prohibitively expensive. Specifically, uniform exploration obtains an equal number of annotations for all the k 2 system pairs; as a result, the required human annotations grows as O(k 2 ).\nTo reduce the number of pairwise annotations, we introduce Active Evaluation, a framework to efficiently identify the top-ranked NLG system. Our Active Evaluation framework consists of a learner that selects a pair of systems to compare at each time step. The learner, then, receives a feedback signal indicating the (human) preference between the selected systems on one input context, randomly sampled from the test dataset. The learner's objective is to reliably compute the topranked system with as few human annotations as possible. We adopt algorithms from the stochastic dueling bandits literature (Bengs et al., 2021) to decide which pair of NLG systems to compare at each time step. To check if existing dueling bandits algorithms can indeed provide reliable top-rank estimates with minimal annotations, we evaluate 13 such algorithms on 13 NLG evaluation datasets spanning five tasks viz., machine translation, summarization, data-to-text generation, paraphrase generation, and grammatical error correction. We show that the best performing dueling bandit algorithm can reduce the number of human annotations by 80% when compared to uniform exploration.\nTo further reduce human annotations, we leverage automatic evaluation metrics in our Active Evaluation framework. We utilize existing automatic metrics such as BLEU (Papineni et al., 2002), BertScore (Zhang et al., 2020), etc for pairwise evaluations by converting the direct evaluation scores into preference probabilities using pairwise probability models. We also develop trained pairwise metrics that directly predict the comparison outcome given pairs of generated texts and context or reference as input. To incorporate such evaluation metrics in our Active Evaluation framework, we propose three model-based dueling bandits algorithms, viz., (i) Random Mixing: human annotations and evaluation metric predictions are randomly mixed, (ii) Uncertainty-aware selection: human annotations are obtained only when the predictions from the evaluation metric is highly uncertain, (iii) UCB Elimination: poorly performing NLG systems are eliminated using an Upper Confidence Bound (UCB) on the evaluation metric scores. Through our experiments, we show that the number of human annotations can be further reduced by 89% on average (this reduction is over and above the 80% reduction that we got earlier). In effect, we show that given k systems, we can find the top-ranked NLG system efficiently with just a few hundred comparisons that vary as O(k). Lastly, we provide practical recommendations to efficiently identify the top-ranked NLG system based on our empirical study on various design choices and hyperparameters.", "publication_ref": ["b57", "b26", "b7", "b34", "b11", "b41", "b42", "b22", "b54", "b24", "b49", "b28", "b20", "b51", "b49", "b23", "b47", "b9", "b2", "b36", "b64"], "figure_ref": [], "table_ref": []}, {"heading": "Active Evaluation Framework", "text": "We introduce the problem and our Active Evaluation setup in section 2.1. Later in section 2.2, we describe the different approaches to decide which pairs of NLG systems to compare at each time step. Finally, in section 2.3, we formalize the notion of top-ranked system.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Problem Formulation and Setup", "text": "We consider the problem of finding the top-ranked NLG system from a given set of k systems, denoted by S = {1, 2, . . . , k}. Our Active Evaluation framework consist of a learner which at each time step t, chooses a pair of systems s\n(t) 1 , s (t)\n2 \u2208 S for comparison. Then, we ask human annotators to compare the outputs of the chosen systems on a randomly sampled input context and provide the comparison outcome as feedback to the learner. Specifically, we first sample an input context X (t) from the test dataset and obtain the generated texts\nY (t) 1 , Y (t) 2 from the chosen systems s (t) 1 , s (t)\n2 . We then display the generated texts Y\n(t) 1 , Y (t) 2\nalong with the context X (t) to human annotators and obtain a comparison outcome w (t) = 1, 0, or 0.5 denoting whether Y (t) 1 is of better, worse, or equal (tie) quality as Y (t) 2 . Note that the feedback w (t) indicates the preference on only one input sample and not the entire test dataset. The overall framework is depicted in figure 1. The learner's objective is to find the top-ranked system with as few pairwise comparisons as possible.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Choosing System Pairs for Comparison", "text": "The learner should decide the pair of systems (s\n(t) 1 , s (t)\n2 ) to compare at each time step t. The naive approach is to uniformly explore all the k 2 system pairs. Specifically, the probability of selecting a pair (i, j), i \u0338 = j at time t is given by\nP unif orm ((s (t) 1 , s (t) 2 ) = (i, j)) = 1 k 2\nHowever, as we show in our experiments, the number of human annotations required to find the topranked system by this approach is very expensive and grows quadratically with the number of systems since we equally explore all k 2 pairs. To reduce the number of annotations, we use dueling bandit algorithms to actively choose pairs of systems to compare based on the history of previous observations. We provide an overview of 13 dueling bandits algorithms proposed in the literature in appendix B. We refer the readers to (Bengs et al., 2021) for a complete survey.", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}, {"heading": "Identifying the top-ranked system", "text": "We now formalize the notion of the top-ranked system. Let p ij denote the preference probability of system i over system j i.e. the probability that a generated text from system i is preferred over system j in the test dataset. We say that a system i \"beats\" system j if p ij > 1 2 . In other words, system i beats system j if the probability of winning in a pairwise comparison is larger for i than it is for j. We define the top-ranked system i * as the one that beats all other systems, i.e. p i * j > 1 2 , \u2200j \u2208 S \u2212 i * .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Pairwise Probability Models", "text": "Our Active Evaluation framework, which we described in the previous section, completely relied on human annotators to compare pairs of generated texts (Y 1 , Y 2 ) to provide the preference feedback w. We can further reduce the number of required human annotations by estimating the human preference feedback using automatic evaluation metrics. However, most existing evaluation metrics are designed for direct assessment and not directly suitable for pairwise evaluations. In this section, we de-scribe three pairwise probability models to convert direct evaluation scores into pairwise preference probabilities. Let f (Y ) denote the score provided by a direct assessment metric f to a generated text Y (The dependence of f on the reference/context is omitted for brevity). The pairwise preference probabilityp(Y 1 \u227b Y 2 ) between any two hypotheses Y 1 and Y 2 can be modeled in 3 different ways:\n\u2022 Linear:\np(Y 1 \u227b Y 2 ) = 1 2 + (f (Y 1 ) \u2212 f (Y 2 ))\n\u2022 Bradley-Terry-Luce (BTL) (Bradley and Terry, 1952;Luce, 1979):\np(Y 1 \u227b Y 2 ) = f (Y 1 ) f (Y 1 ) + f (Y 2 )\n\u2022 BTL-logistic::\nAs detailed in appendix C.2, we appropriately preprocess the scores f (Y ) to ensure that preference probability lies between 0 and 1. We can now predict the comparison outcome w by thresholding the preference probability at two thresholds \u03c4 1 and \u03c4 2 (\u2265 \u03c4 1 ) to incorporate ties i.e.:\nw = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1, ifp(Y 1 \u227b Y 2 ) > \u03c4 2 0, ifp(Y 1 \u227b Y 2 ) < \u03c4 1 0.5, Otherwise\nWe choose \u03c4 1 and \u03c4 2 using grid search on the validation set. Refer appendix C.2 for more details.", "publication_ref": ["b6", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "Model-based Dueling Bandits", "text": "In the previous section, we discussed pairwise probability models to obtain the estimated preference probabilityp(Y 1 \u227b Y 2 ) and the comparison outcome\u0175 using scores assigned by direct assessment metrics. We now propose three model-based dueling bandit algorithms wherein we combine such predictions from evaluation metrics with human annotations in the Active Evaluation framework.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Random Mixing", "text": "Here, we randomly provide either the real (human) or the evaluation metric predicted feedback to the learner. Specifically, at any time t, we use the predicted comparison outcome\u0175 (t) as the feedback with probability p m and use human annotations w (t) as feedback with probability 1 \u2212 p m . The hyperparameter p m controls the ratio of estimated and real feedback given to the learner. As with other hyperparameters, we tune p m on the validation set.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Uncertainty-aware Selection", "text": "In this algorithm, we estimate uncertainty in the evaluation metric predictions and decide to ask for human annotations only when the evaluation metric is highly uncertain. We specifically focus on trainable neural evaluation metrics such as Bleurt (Sellam et al., 2020) where we estimate the prediction uncertainty using recent advances in Bayesian deep learning. Letp(Y 1 \u227b Y 2 |\u03b8) denote the preference probability modelled by a neural evaluation metric with parameters \u03b8. Given a training dataset D tr , Bayesian inference involves computing the posterior distribution p(\u03b8|D tr ) and marginalization over the parameters \u03b8:\np(Y 1 \u227b Y 2 |D tr ) = \u03b8p (Y 1 \u227b Y 2 |\u03b8)p(\u03b8|D tr )d\u03b8\nHowever, computing the true posterior and averaging over all possible parameters is intractable in practice. Hence, several approximations have been proposed in variational inference such as finding a surrogate distribution q \u03d5 (\u03b8) for the true posterior. Gal and Ghahramani (2016) have shown that we can use the Dropout distribution (Srivastava et al., 2014) as the approximate posterior q \u03d5 (\u03b8). Specifically, we can perform approximate Bayesian inference by applying Dropout during test time. Hence, the posterior can now be approximated with Montecarlo samples as follows:\np(Y 1 \u227b Y 2 |D tr ) \u2248 1 L L l=1p (Y 1 \u227b Y 2 |\u03b8 l )\nwhere {\u03b8 l } L l=1 are L samples from the Dropout distribution q \u03d5 (\u03b8) (i.e. we apply Dropout L times independently during testing). We now discuss two different Bayesian uncertainty measures: BALD: The Bayesian Active Learning by Disagreement (BALD) (Houlsby et al., 2011) is defined as the mutual information between the model predictions and the model posterior. Let p l = p(Y 1 \u227b Y 2 |\u03b8 l ), where \u03b8 l \u223c q \u03d5 (\u03b8), be the evaluation metric prediction using the l th sample \u03b8 l from the Dropout distribution. Also, letp = 1 L L l=1 p l be the mean prediction. As shown in (Gal et al., 2017), we can approximate the BALD measure using samples from the Dropout distribution as:\nI = H(p) \u2212 1 L L l=1 H(p l )\nwhere H is the binary cross entropy function. The BALD uncertainty score is essentially the difference in entropy of the mean predictionp and the average entropy of the individual predictions {p l } L l=1 . Hence, the BALD uncertainty score is high when the metric's mean prediction is uncertain (high entropy) but the individual predictions are highly confident (low entropy), i.e., when the metric produces disagreeing predictions with high confidence.", "publication_ref": ["b50", "b15", "b52", "b18", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "STD:", "text": "We also adopt the standard deviation of the preference probability taken over the posterior distribution as a measure of uncertainty:\n\u03c3 = Var \u03b8\u223cp(\u03b8|D tr ) (p(Y 1 \u227b Y 2 |\u03b8))\nSimilar to BALD, we can approximate the above measure using the empirical standard deviation of samples drawn from the dropout distribution.\nOur proposed algorithm asks for human annotations only if the uncertainty measure (BALD or STD) is above a particular threshold.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "UCB Elimination", "text": "The key idea here is to eliminate a set of \"poorly performing\" NLG systems using the automatic metric and perform human evaluations with the remaining set of systems. To eliminate sub-optimal systems, we first need to quantify a performance measure for the systems. We use the Copeland score (Zoghi et al., 2015) which is defined as the normalized total number of pairwise wins for a system:\nC i = 1 k\u22121 j\u0338 =i 1(p ij > 1 2 ).\nCopeland score is the highest for the top-ranked system with a value of 1 and it is less than 1 for all other systems. To estimate the Copeland score, we first predict the pairwise preference probability between any two systems i and j as follows:\np ij = 1 N Y 1 ,Y 2 \u2208D ijp (Y 1 \u227b Y 2 |\u03b8)\nwhere D ij is the test dataset consisting of generated texts from systems i and j, N is the total number of test examples, \u03b8 is the learned model parameters. We can now estimate the Copeland score\u0108 i using the estimated preferencep ij and eliminate all systems with Copeland scores below a threshold. However, a major problem with this approach is that evaluation metrics are often inaccurate and we could wrongly eliminate the true top-ranked system without performing any human evaluations. For example, consider the example where i * is the top-ranked system with p i * j > 0.51 , \u2200j \u2208 S \u2212 i.\nIf several of the predicted probabilitiesp i * j are less than 0.5, our top-ranked system i * will receive a low estimated Copeland score and will be incorrectly eliminated. To overcome this problem, we define an Upper Confidence Bound (UCB) on the preference probability using uncertainty estimates that we described in 4.2. Specifically, the upper confidence bound\u00fb ij is given by\u00fb ij =p ij + \u03b1\u03c3 ij where \u03b1 is a hyperparameter that controls the size of the confidence region and\u03c3 2 ij is the estimated variance given by:\n\u03c3 2 ij = 1 N 2 Y 1 ,Y 2 \u2208D ij Var \u03b8\u223cq \u03d5 (\u03b8)p (Y 1 \u227b Y 2 |\u03b8)\nwhere q \u03d5 (\u03b8) is the Dropout distribution. Using the upper confidence estimates\u00fb ij , we now define the optimistic Copeland score for a system i a\u015d\nC u i = 1 K\u22121 j\u0338 =i 1(\u00fb ij > 1 2 ).\nHere, we consider a system i to beat another system j (\u00fb ij > 0.5) if either the estimated preference is high (p ij is high) or if there is an high uncertainty in the estimation (\u03c3 ij is high). In UCB Elimination, we eliminate a system only if the optimistic Copeland score is below a threshold.", "publication_ref": ["b66"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup", "text": "In this section, we describe the (i) NLG tasks and datasets used in our experiments, (ii) automatic evaluation metrics used in our model-based algorithms, and (iii) annotation complexity measure used for comparing dueling bandit algorithms.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Tasks & Datasets", "text": "We use a total of 13 datasets spanning 5 tasks in our experiments which are summarized in table 1. Machine Translation (MT): We use 7 human evaluation datasets collected from the WMT news translation tasks (Bojar et al., 2015(Bojar et al., , 2016 viz. fin\u2192eng, rus\u2192eng, deu\u2192eng language pairs in WMT 2015 and tur\u2192eng, ron\u2192eng, cze\u2192eng, deu\u2192eng language pairs in WMT 2016. Grammatical Error Correction (GEC): We utilize two human evaluation datasets collected by (Napoles et al., 2019) where the source texts are from (i) student essays (FCE), and (ii) formal articles in Wikipedia (Wiki). We also use another GEC dataset collected by (Napoles et al., 2015a) from the CoNLL-2014Shared Task (Ng et al., 2014 , 2020). The task here is to generate natural language utterance from dialogue acts. Paraphrase Generation: We use human evaluations of model generated English paraphrases released with the ParaBank dataset (Hu et al., 2019). Summarization: We make use of the human evaluations (Stiennon et al., 2020) of GPT3-like transformers on the TL;DR dataset (V\u00f6lske et al., 2017). We provide further details including preprocessing steps and downloadable links in appendix A.1.", "publication_ref": ["b5", "b4", "b30", "b31", "b19", "b58"], "figure_ref": [], "table_ref": []}, {"heading": "Automatic NLG Evaluation Metrics", "text": "We can predict the comparison outcome w using two approaches. First, we can use pairwise probability models with existing direct assessment metrics as discussed in section 3. Alternatively, we can train evaluation metrics to directly predict the comparison outcome given pairs of generated texts and context/reference as input. We discuss both these approaches below: Direct Assessment Metrics: We experiment with a total of 10 direct assessment metrics viz. chrF (Popovic, 2015), BLEU-4 (Papineni et al., 2002), ROUGE-L (Lin, 2004), Embedding Average (Wieting et al., 2016), Vector Extrema (Forgues et al., 2014), Greedy Matching (Rus and Lintean, 2012), Laser (Artetxe and Schwenk, 2019), BertScore (Zhang et al., 2020), MoverScore (Zhao et al., 2019) and Bleurt (Sellam et al., 2020). We mention the implementation details in appendix A.2. Pairwise Evaluation Metrics: We finetune the pretrained Electra-base transformer model (Clark et al., 2020) to directly predict the comparison outcome w. We curate task-specific human evaluation datasets consisting of tuples of the form (context/reference, hypothesis 1, hypothesis 2, label) for finetuning. Due to space constraints, we mention Table 2: Annotation complexity of the top 7 best performing dueling bandit algorithms along with the uniform exploration algorithm on 13 datasets spanning 5 NLG tasks details on the datasets and finetuning in appendix A.3 and A.4. For the summarization task alone, we couldn't find any pairwise human judgment dataset sufficient for finetuning the Electra model.", "publication_ref": ["b38", "b36", "b25", "b59", "b14", "b64", "b65", "b50", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Annotation Complexity Measure", "text": "To evaluate the performance of dueling bandit algorithms, we define annotation complexity as the minimum number of human annotations needed by an algorithm to identify the top-ranked NLG system with high confidence. Let i * be the actual top-ranked system, and\u00ee * (n) denote the estimated winner by the algorithm after n human annotations, then annotation complexity is defined as:\nmin n \u2032 : \u2200n \u2265 n \u2032 , P (\u00ee * (n) = i * ) > 1 \u2212 \u03b4 acc\nwhere \u03b4 acc is the allowable failure probability i.e. the learner can make a mistake with at most \u03b4 acc probability. To compute the annotation complexity, we run each dueling bandit algorithm with 200 different random seeds and find the minimum number of human annotations after which the algorithm correctly returns the top-ranked NLG system in at least 190/200 runs (we set \u03b4 acc = 0.05).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results & Discussion", "text": "We discuss the performance of dueling bandits algorithms in 6.1, automatic metrics in 6.2 and our proposed model-based algorithms in 6.3. Lastly in 6.4, we analyze the variation of annotation complexity with the number of NLG system.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Analysis of Dueling Bandit Algorithms", "text": "We report the annotation complexity of the top 7 dueling bandit algorithms along with uniform exploration on 13 datasets in table 2. We observe that the annotation complexity of uniform exploration is consistently high across all 13 datasets. In particular, the required human annotations become prohibitively expensive when the number of NLG  (Zoghi et al., 2014b), RCS (Zoghi et al., 2014a), RMED (Komiyama et al., 2015) are able to effectively identify the top-ranked system with much fewer annotations. In particular, RMED performs the best with a reduction of 80.01% in human annotations compared to uniform exploration. We also examine an alternative approach to assess the performance of dueling bandit algorithms. Here, we fix the number of human annotations (fixed annotation budget) and compute the accuracy in predicting the top-ranked system. As we show in figure 2, RMED achieves the highest top-rank prediction accuracy for any given number of human annotations. We provide the complete results in appendix F.2.", "publication_ref": ["b21"], "figure_ref": [], "table_ref": []}, {"heading": "Performance of Evaluation Metrics", "text": "Before we utilize automatic evaluation metrics using our proposed model-based algorithms, we analyze the effectiveness of these metrics for pairwise NLG evaluations. In table 3, we report the sentencelevel accuracy in predicting the comparison outcome w using direct assessment metrics with the Linear probability model (as discussed in section 3) along with our trained Electra metric. Across the tasks, we observe that metrics that utilize con-   textualized word embeddings, such as BertScore, perform much better than n-gram and static word embedding-based metrics. In MT, we observe that Bleurt, specifically finetuned on WMT human judgment data, performs the best. In Data-to-Text and Paraphrase generation, our trained Electra metric finetuned on task-specific data significantly outperforms the existing metrics. Interestingly, on the summarization task, all the existing metrics perform much worse than random predictions i.e. they do not add any useful value in evaluation. Hence, we exclude the TLDR dataset from our analysis on model-based algorithms. Finally, as we show in appendix F.3, we observed that the performance is largely similar across all the three probability models: Linear, BTL, and BTL-logistic.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Analysis of Model-based Algorithms", "text": "We use our proposed model-based algorithms and incorporate the two best-performing evaluation We compare the annotation complexity of various model-based algorithms in table 4. We observe that the Random Mixing algorithm with Bleurt and Electra reduces annotation complexity by 70.43% and 73.15%, respectively, when compared to the standard (model-free) RMED algorithm (row 1). Our Uncertainty-aware selection algorithm with the BALD measure further reduces the annotation complexity by around 37% (compared with Random Mixing). We notice that our UCB Elimination algorithm also provides significant improvements over standard RMED. Since UCB Elimination is complementary to Uncertainty-aware selection, we apply both these algorithms together and observe the lowest annotation complexity with a reduction of 89.54% using Electra and 84.00% using Bleurt over standard RMED. Lastly, in figure 3, we analyze the effect of using other evaluation metrics such as BLEU, BertScore, etc., in Random Mixing. Interestingly, we notice that using metrics such as BLEU, which have low accuracy values, results in a higher annotation complexity than standard (model-free) RMED in some datasets. That is, we may even require a greater number of human annotations to over-compensate for the inaccurate predictions from metrics like BLEU. However, with Laser, MoverScore, and BertScore, we observe significant reductions in annotation complexity. Please refer appendix F.4 for further results.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Effect of Number of NLG systems", "text": "We analyze how annotation complexity varies with the number of NLG systems. Specifically, we chose a subset of k systems out of the total 28 systems in the ParaBank dataset and computed the annotation complexity among these k systems. As shown in figure 4, the annotation complexity of uniform ex-  ", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Practical Recommendations", "text": "We summarize the key insights from this study and provide practical recommendations on efficiently identifying the top-ranked NLG system.\n1. Use RMED dueling bandit algorithm to actively choose system pairs for comparison. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Several works (Bojar et al., 2014(Bojar et al., , 2015Sakaguchi et al., 2014Sakaguchi et al., , 2016 in Machine translation and Grammatical Error Correction adopt the TrueSkill algorithm (Herbrich et al., 2006), originally used for ranking Xbox gamers, to efficiently rank NLG systems from pairwise annotations. A recent work (Sakaguchi and Durme, 2018) proposes an online algorithm to rank NLG systems when we receive pairwise preference feedback in the form of a continuous scalar with bounded support. The key difference in our work is that we focus on the problem of identifying the top-rank system instead of ranking all the systems. Experimental study of dueling bandit algorithms have been limited to synthetic simulations in a few works (Yue and Joachims, 2011;Urvoy et al., 2013). Most others (Zoghi et al., 2014b,a;Komiyama et al., 2015;Zoghi et al., 2015;Wu and Liu, 2016) focus on information retrieval applications that involve evaluating search retrieval algorithms (Radlinski et al., 2008). To the best of our knowledge, ours is the first work to extensively study the effectiveness of dueling bandit algorithms for NLG evaluation.", "publication_ref": ["b3", "b5", "b46", "b45", "b17", "b44", "b63", "b56", "b21", "b66", "b60", "b39"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion & Future work", "text": "In this work, we focused on the problem of identifying the top-ranked NLG system with few pairwise annotations. We formulated this problem in an Active Evaluation framework and showed that dueling bandit algorithms can reduce the number of human annotations by 80%. We then proposed modelbased algorithms to combine automatic metrics with human evaluations and showed that human annotations can be reduced further by 89%; thereby requiring only a few hundred human annotations to identify the top-ranked system. In future work, we would like to extend our analysis to the general problem of finding the top-k ranked systems.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Discussion on Ethics & Broader Impact", "text": "Evaluating Natural Language Generation (NLG) models accurately and reliably with few human annotations is an important aspect of NLG research and its real-world applications. Our work shows that we can significantly reduce the number of human annotations required to find the top-ranked NLG system with high confidence. We envision that our work will benefit a wide range of applications such as translation systems, grammatical checkers, etc., where practitioners can find the best NLG model among a set of candidates more accurately and with fewer human annotations. Despite these improvements, there are still several challenges towards reliable NLG evaluation. For example, our model-based approaches, which use automatic metrics, may be subject to biases and other undesirable mistakes, depending on the metric and how they are trained in practice. Our approach may be used to evaluate models that generate fake news, toxic content, or other harmful applications, even though it is not specifically designed for such cases. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Further Details on Experiments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1 Tasks & Datasets", "text": "In table 5, we report the dataset statistics along with links to download the original datasets. We now discuss the preprocessing steps: Machine Translation: In WMT 2015 and 2016 tasks, human annotators were asked to rank five system outputs (translated sentences) relative to each other. As recommended by the organizers (Bojar et al., 2014), we convert each of these rankings into 5 2 pairwise comparisons of systems. Grammatical Error Correction: The Grammarly evaluation datasets follow the RankME (Novikova et al., 2018) annotation style where annotators were shown 8 outputs side by side for each input and were asked to provide a numerical score to each of them. We discarded one of the outputs out of the 8, which was human crafted, and used the remaining 7 model-generated outputs. We then convert these 7 scores into 7 2 pairwise comparisons of systems. Human evaluations of the CoNLL-2014 Shared Task followed the same process as WMT 2015. Hence, we follow the same preprocessing steps as WMT. Data-to-Text Generation: The E2E NLG Challenge also follows the RankME annotation format. We follow the same preprocessing steps as the Grammarly datasets. Out of the total 21 systems, we held out 5 systems to train the Electra model and use the remaining 16 systems. Paraphrase Generation: For ParaBank, we follow the same preprocessing steps as the Grammarly datasets. Out of the total 35 systems, we held out of 7 systems and only used the remaining 28 systems. Summarization: We select 11 systems that have human annotations between each pair of them. These systems are GPT3-like models with varying model sizes (3B, 6B, 12B) and training strategies. We do not perform any additional preprocessing here.", "publication_ref": ["b3", "b35"], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Direct Assessment Metrics: Implementation Details", "text": "We use the nlg-eval library 1 for the implementation of BLEU-4, ROUGE-L, Embedding Average, Vector Extrema, and Greedy Matching. For chrF, Laser and BertScore, we use the implementations from the VizSeq library 2 . We use the official implementation released by the original authors for Mover-  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.3 Finetuning Datasets", "text": "Here, we describe the task-specific datasets used for finetuning the Electra model (pairwise evaluation metric described in section 5.2). For MT, we used human evaluations of WMT 2013 and 2014, consisting of a total of 650k examples. For GEC, we curated a training dataset of 180k pairs of texts and human preference using data released by (Napoles et al., 2015b) and the development set released by (Napoles et al., 2019). We utilize 11k examples from 5 held-out systems in the E2E NLG Challenge (apart from the 16 systems used for evaluations) for Data-to-Text generation. Lastly, we use a dataset of 180k examples from 7 held-out systems in the ParaBank dataset for paraphrase generation. We use 90% \u2212 10% split for splitting the dataset into train and validation sets. Note that these datasets do not have any overlap with the datasets used for evaluating dueling bandit algorithms.", "publication_ref": ["b32", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "A.4 Finetuning Details", "text": "We use the pretrained Electra-base model (Clark et al., 2020) with 110M parameters (12 layers and 12 attention heads) as our base model. We finetune the model using ADAM optimizer with \u03b2 1 = 0.9 and \u03b2 2 = 0.99. We use a linear learning rate decay with a maximum learning rate of 1e-5 and warm-up for 10% of training. We use a batch size of 128 and finetune for four epochs. We finetune all the models on Google Cloud TPU v3-8. To estimate prediction, we apply Dropout to the Electra model during test time as described in 4.2.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "B Summary of Dueling Bandit Algorithms", "text": "We now provide an overview of various dueling bandit algorithms in the literature. We first introduce a few additional notations and terminologies in B.1. Later in B.2, we describe the various structural assumptions made by different dueling bandit algorithms. Finally, in B.3, we summarize 13 dueling bandit algorithms that we analyze in this work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.1 Notations and Terminologies", "text": "Let \u2206 ij = p ij \u2212 1 2 where p ij is the preference probability of system i over j, as defined in section 2.3. We call a system as the Copeland winner if it beats more number of systems than any other system. Mathematically, a Copeland winner i * is defined as i * = arg max i k j=1 1(\u2206 ij > 0). A special case of the Copeland winner is the Condorcet winner, which is the system that beats all other systems. In all our NLG tasks and datasets, we observed that this special case holds true i.e. there exists a system that beats all other k \u2212 1 systems, and we define it as the top-ranked system. Nevertheless, we mention these two definitions to distinguish algorithms that work for the general Copeland winner, even if the Condorcet winner does not exist.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.2 Assumptions", "text": "All the dueling bandit algorithms that we analyze in this work assume a stochastic feedback setup in which the feedback is generated according to an underlying (unknown) stationary probabilistic process. Specifically, in our Active Evaluation framework, this is equivalent to assuming that the annotator preference is stationary over time and is given by some fixed distribution p a (w|Y\n(t) 1 , Y (t)\n2 ). Further, many dueling bandit algorithms make various assumptions on the true pairwise preferences and exploit these assumptions to derive theoretical guarantees (Bengs et al., 2021). In table 6, we describe the various commonly used assumptions by dueling bandit algorithms. For example, the stochastic triangle inequality assumption (STI), described in row 4 of table 6, assumes that the true preference probabilities between systems obey the triangle inequality. We note here that one cannot verify the validity of these assumptions apriori since we do not have access to the true preferences.", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}, {"heading": "B.3 Algorithms", "text": "In table 7, we describe the various dueling bandit algorithms along with the assumptions (used to provide theoretical guarantees) and the target winner. We summarize these algorithms below: IF: Interleaved Filtering (IF) (Yue et al., 2012) algorithm consists of a sequential elimination strategy where a currently selected system s i is compared against the rest of the active systems (not yet eliminated). If the system s j beats a system s i with high confidence, then s i is eliminated, and s j is compared against all other active systems. Similarly, if the system s i beats s j with high confidence, then s j is eliminated, and s i is continued to be compared against the remaining active systems. Under the assumptions of TO, SST, and STI, the authors provide theoretical guarantees for the expected regret achieved by IF.\nBTM: Beat The Mean (BTM) (Yue and Joachims, 2011), similar to IF, is an elimination-based algorithm that selects the system s i with the fewest comparisons and compares it with a randomly chosen system from the set of active systems. Based on the comparison outcome, a score and confidence interval are assigned to the system s i . BTM eliminates a system as soon as there is another system with a significantly higher score. Knockout, Seq Elim, Single Elim: Knockout (Falahatgar et al., 2017b), Sequential Elimination (Falahatgar et al., 2017a), Single Elimination (Mohajer et al., 2017 are all algorithms that proceed in a knockout tournament fashion where the systems are randomly paired, and the winner in each duel will play the next round (losers are knocked out) until the overall winner is determined. During a duel, the algorithm repeatedly compares the two systems to reliably determine the winner. The key difference between the three algorithms is the assumptions they use and how they determine the number of comparisons required to identify the winning system in a duel with high probability. Plackett Luce: Plackett Luce Condorcet winner identification algorithm (Sz\u00f6r\u00e9nyi et al., 2015) assumes that the true rank distribution follows the Placket-Luce model (Plackett, 1975). The algorithm is based on a budgeted version of QuickSort. The authors show that it achieves a worst-time annotation complexity of the order k log k under the Placket-Luce assumption. RUCB: Relative Upper Confidence Bound (RUCB) (Zoghi et al., 2014b) is an adaptation of the wellknown UCB algorithm (Auer et al., 2002) to the dueling bandit setup. Similar to UCB, RUCB selects the first system s\n(1) t based on \"optimistic\" estimates of the pairwise preference probabilities i.e. based on an upper confidence bound of preference probabilities. The second system s\n(2) t is chosen to be the one that is most likely to beat s\n(1) t . RCS: Relative Confidence Sampling (RCS) (Zoghi et al., 2014a) follows a Bayesian approach by maintaining a posterior distribution over the preference probabilities. At each time step t, the algorithm samples preference probabilities from the posterior and simulates a round-robin tournament among the systems to determine the Condorcet winner. The estimated Condorcet winner is chosen as the first system s\n(1) t and second system s\n(2)\nt is chosen such that it has the best chance of beating s\n(1) t . RMED: Relative Minimum Empirical Divergence1 (RMED) algorithm (Komiyama et al., 2015) maintains an empirical estimate of the \"likelihood\" that a system is the Condorcet winner. It then uses this estimate to sample the first system s \u2203 a total order \u227b over S:\ni \u227b j \u21d0\u21d2 \u2206 ij > 0 Strong stochastic transitivity (SST) \u2206 ij > 0, \u2206 jk > 0 =\u21d2 \u2206 ik \u2265 max(\u2206 ij , \u2206 jk ) Relaxed stochastic transitivity (RST) \u2203\u03b3 \u2265 1: \u2206 ij > 0, \u2206 jk > 0 =\u21d2 \u03b3\u2206 ik \u2265 max(\u2206 ij , \u2206 jk ) Stochastic triangle inequality (STI) \u2206 ij > 0, \u2206 jk > 0 =\u21d2 \u2206 ik \u2264 \u2206 ij + \u2206 jk Condorcet winner (CW) \u2203i * : \u2206 i * ,j > 0, \u2200j \u2208 S \u2212 i *", "publication_ref": ["b62", "b63", "b13", "b12", "b55", "b37", "b1", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "PL model", "text": "The underlying rank distribution follows the Plackett-Luce (PL) model (Plackett, 1975;Luce, 1979) (Yue et al., 2012) TO+SST+STI Condorcet BTM (Yue and Joachims, 2011) TO+RST+STI Condorcet Seq-Elim. (Falahatgar et al., 2017a) SST Condorcet Plackett Luce (Sz\u00f6r\u00e9nyi et al., 2015) PL model Condorcet Knockout (Falahatgar et al., 2017b) SST+STI Condorcet Single Elim. (Mohajer et al., 2017) TO Condorcet RUCB (Zoghi et al., 2014b) CW Condorcet RCS (Zoghi et al., 2014a) CW Condorcet RMED (Komiyama et al., 2015) CW Condorcet SAVAGE (Urvoy et al., 2013) -Copeland CCB (Zoghi et al., 2015) -Copeland DTS (Wu and Liu, 2016) -Copeland DTS++ (Wu and Liu, 2016) -Copeland\nTable 7: Summary of dueling bandits algorithms in the literature along with their theoretical assumptions and the target winner of the learner 2013) is a generic algorithm that can be adopted for various ranking problems such as Copeland winner identification. SAVAGE (Copeland) algorithm, at each time step, randomly samples a pair of systems from the set of active system pairs (not yet eliminated) and updates the preference estimates. A system pairs (s i , s j ) is eliminated if either (i) the result of comparison between s i and s j is already known with high probability, or (ii) there exists some system s k where the estimated Copeland score of s k is significantly higher than s i or s j .\nCCB: Copeland Confidence Bound (CCB) (Zoghi et al., 2015) is similar to the RUCB algorithm but is designed to identify the Copeland Winner (a generalization of the Condorcet winner). The CCB algorithm maintains optimistic preference estimates and uses them to choose the first system s\n(1)\nt and then selects the second system s\n(2) t that is likely to discredit the hypothesis that s\n(1) t is indeed the Copeland winner. The algorithm successively removes all other systems that are highly unlikely to be a Copeland winner.", "publication_ref": ["b37", "b27", "b62", "b63", "b12", "b55", "b13", "b29", "b21", "b56", "b66", "b60", "b60", "b66"], "figure_ref": [], "table_ref": []}, {"heading": "DTS, DTS++:", "text": "The Double Thompson Sampling (DTS) algorithm (Wu and Liu, 2016) maintains a posterior distribution over the pairwise preference matrix, and selects the system pairs s\n(1) t , s\n(2) t based on two independent samples from the posterior distribution. The algorithm updates the posterior distributions based on the comparison outcome and eliminates systems that are unlikely to be the Copeland winner. DTS++ is an improvement proposed by the authors, which differs from DTS in the way the algorithm breaks ties. Both have the same theoretical guarantees, but DTS++ has been empirically shown to achieve better performance (in terms of regret minimization).", "publication_ref": ["b60"], "figure_ref": [], "table_ref": []}, {"heading": "C Hyperparameters Details", "text": "We discuss the details of the hyperparameters and the tuning procedure used for dueling bandit algorithm in C.1, pairwise probability models in C.2 and our model-based algorithm in C.3. In all three cases, we use the validation split of the finetuning datasets described in A.3 as our validation dataset. For example, the validation split of the finetuning datasets for MT consists of 10% of the WMT 2013 and 2014 datasets. We use this dataset to tune the hyperparameters for WMT 2015 and 2016 datasets.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1 Dueling Bandit Algorithms", "text": "For all algorithms other than Knockout and Single Elimination, we use the hyperparameters recommended by the original authors for all the datasets. For example, in the RMED algorithm, described in algorithm 1 of (Komiyama et al., 2015), we use f (K) = 0.3K 1.01 as suggested by the authors. For the RCS algorithm, described in algorithm 1 of (Zoghi et al., 2014a), we use \u03b1 (exploratory constant) = 0.501. For RUCB (algorithm 1 of (Zoghi et al., 2014b)), we use \u03b1 = 0.51. Similarly, for all algorithms other than Knockout and Single Elimination, we use the recommended hyperparameters mentioned in the original paper. For knockout and Single Elimination, we found that the performance was very sensitive to the hyperparameters. For these two algorithms, we manually tuned the hyperparameters on the validation set. In Knockout, algorithm 3 of (Falahatgar et al., 2017b), we use \u03f5 = 0.2, \u03b4 = 0.05, \u03b3 = 1.0 for WMT'16 ron-eng and TLDR OpenAI datasets. We use \u03f5 = 0.2, \u03b4 = 0.05, \u03b3 = 0.6 for ParaBank and Grammarly-Wiki datasets and \u03f5 = 0.2, \u03b4 = 0.09, \u03b3 = 0.6 for all other datasets. In Single Elimination, we use m (number of pairwise comparisons per duel) = 1000 for WMT'16 ron-eng, E2E NLG, Grammarly-FCE, m = 1500 for CoNLL'14 shared task and m = 500 for all other datasets.", "publication_ref": ["b21", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "C.2 Pairwise Probability Models", "text": "Letf (Y ) be the unnormalized score given an automatic evaluation metric for an hypothesis Y . We preprocess the scoref (Y ) to obtain f (Y ) to ensure that the pairwise probability scores is always a valid i.e. lies between 0 and 1. To preprocess the scores, we use the validation dataset consisting of tuples of the form {Y\n(i) 1 , Y (i) 2 , w (i) } N i=1\nwhere\nY (i) 1 , Y (i) 2\nrepresent the ith generated texts and w (i) is the corresponding comparison outcome provided by human annotators.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Linear: Let", "text": "\u2206 i = |f (Y (i) 1 ) \u2212f (Y (i)\n2 )| and \u2206 = max i \u2206 i . We divide the unormalizedf (Y ) scores by 2\u2206 i.e.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "f (Y ) =f (Y ) 2\u2206", "text": ".\nBTL: Let f m i = max{f (Y (i) 1 ),f (Y (i)\n2 )}, f m = max i f m i . We now subtract the scores by f m to ensure that the scores are non-negative i.e. f (Y ) =f (Y ) \u2212 f m BTL-Logistic: BTL-Logistic model always provides a score between 0 and 1. However, we found that dividing the scores by a temperature co-efficient \u03b3 can provide better results i.e.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "f (Y ) =f (Y ) \u03b3", "text": "We tune \u03b3 using grid search between 0.005 and 1 on the validation set to minimize the crossentropy loss between the preference probabilitie\u015d p(Y 1 \u227b Y 2 ) and the human labels w.\nThresholds: As described in section 3, we threshold the preference probabilitiesp(Y 1 \u227b Y 2 ) at two thresholds \u03c4 1 and \u03c4 2 to obtain the predicted comparison outcome\u0175. We perform a grid search by varying \u03c4 1 from 0.4 to 0.5 and \u03c4 2 from 0.5 to 0.6 with a step size of 0.001. We choose the optimal thresholds that maximize the prediction accuracy on the validation dataset.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.3 Model-based Algorithms", "text": "We manually tune the hyperparameters in our model-based algorithms on the validation dataset.\nFor clarity, we first describe the hyperparameters in the different model-based algorithms. In Random Mixing, we need to choose the mixing probability p m hyperparameter. In Uncertainty-aware Selection (BALD), we need to choose a threshold value \u03c4 BALD for the BALD score at which we decide to ask for human annotations. For UCB elimination, we should choose a threshold \u03c4 cop for optimistic Copeland scores and the \u03b1 hyperparameter, which controls the size of the confidence region. In table 8 and 9, we report the tuned hyperparameter values when using Electra and Bleurt (with the Linear probability model) as the evaluation model. Another hyperparameter is the number of Monte-Carlo samples L to obtain from the Dropout distribution as discussed in section 4.2. We set L = 20, i.e. we independently apply dropout 20 times for each test predictions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D Effect of Hyperparameters in Model-based Algorithms", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.1 Sensitivity to Hyperparameters", "text": "We study how hyperparameters in our proposed model-based algorithms affect annotation complexity. Recall that in Random Mixing, the mixing probability p m controls the ratio of real and model generated feedback given to the learner. In Uncertaintyaware Selection (BALD), we obtain human annotations when the BALD score is above a threshold \u03c4 BALD . Here, as well \u03c4 BALD implicitly controls the fraction of real and predicted feedback.   ity decreases, i.e., with a greater amount of feedback received from Bleurt, the number of required human annotations is lower. However, as shown in figure 6, we observe the opposite trend when we use metrics such as BLEU, which are highly inaccurate. In these cases, we require a greater number of human annotations to compensate for the highly erroneous feedback received from the evaluation metric. Therefore, the optimal mixing probability p m in such cases is close to 0 i.e. equivalent to the model-free case. For moderately accurate metrics such as Laser, we observed the optimal p m was close to 0.4 to 0.6. The key insight from these observations is that the higher the accuracy of the metric, the higher amount of feedback can be obtained from the metric to identify the top-ranked system. In figure 7, we analyze how the annotation complexity of UCB Elimination with Bleurt varies with the optimistic Copeland threshold \u03c4 cop hyperparameter. We fixed \u03b1 hyperparameter to 0.6. We observed that UCB Elimination is much more robust to \u03c4 cop and a general value of \u03c4 cop = 0.8 worked well across all datasets and metrics.", "publication_ref": [], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "D.2 Best Practices in Choosing Hyperparameters", "text": "The optimal approach to choose hyperparameters is usually to tune them on a validation set. But, at  times, it may not be possible either because of computational reasons or because a human-annotated validation dataset may not be available. In such cases, we provide a few heuristics based on our previous analysis to choose hyperparameters in our model-based algorithms:\n1. Choose the mixing probability p m in Random Mixing proportionately with the accuracy of the metric. For example, we observed that for metrics with sentence-level prediction accuracy greater than 70%, p m = 0.8 tend to work well. For accuracy between 65% to 70%, p m in the range of 0.5-0.7 worked well.\n2. Once we choose a value of p m , we can find an appropriate BALD threshold \u03c4 BALD where 100\u00d7p m % of BALD scores are above \u03c4 BALD and 100\u00d7(1\u2212p m )% of BALD score are below \u03c4 BALD . Choosing the BALD threshold this way ensures that we can directly control the desired amount of model-predicted feedback given to the learner. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E Robustness to Delayed Feedback", "text": "In some instances, human annotations are obtained from multiple crowdsourced annotators in parallel to reduce the time taken for annotations. In such cases, the learner is required to choose the system pairs (s\n(t) 1 , s (t)\n2 ) to give to some annotator i even before we obtain the result w (t\u22121) of the previous comparison from some other annotator j. In other words, the learner may experience a delay d > 0 in feedback where at time t, the learner may only have access to the comparison history up to time t \u2212 d \u2212 1. As shown in figure 8, we observe that the top-performing dueling bandit algorithms tend to be robust to delays in feedback. We notice that the variation in the annotation complexity of RMED and RCS as measured by standard deviation is only 64.49 and 62.86, respectively.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F Additional Results", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F.1 Effect of number of NLG systems", "text": "In figure 10, we compare the variations in annotation complexity of Random Mixing (with Electra metric) using uniform exploration and dueling bandit algorithms. Similar to the model-free case discussed in section 6.4, the annotation complexity of uniform exploration grows as O(k 2 ) but the annotation complexity only varies as O(k) for RMED, We report the annotation complexity of all 13 dueling bandit algorithms on 13 evaluation datasets in table 10. In figure 11, we show the top-rank prediction accuracy as a function of the number of human annotations for various dueling bandit algorithms on all the datasets, other than WMT 16 tur-eng, which is separately depicted in figure 2.", "publication_ref": [], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "F.3 Performance of Evaluation Metrics", "text": "In table 11, we report the sentence-level accuracy in predicting the comparison outcome for 10 direct assessment metrics using three probability models along with the trained pairwise metric (Electra). We observe that there is little variation in performance across the three probability models. To further illustrate this, we plot the accuracy on the WMT datasets in figure 9 and observe that the performance is largely similar across Linear, BTL, and BTL-logistic models.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F.4 Model-based Algorithms", "text": "In figure 12, we show the top-rank prediction accuracy as a function of the number of human annotations for various model-based algorithms using the Electra metric with RMED. We observe that Random Mixing and Uncertainty-aware Selection (BALD) algorithms have significantly higher prediction accuracy than model-free RMED for any given number of human annotations. Further, when we use UCB Elimination with Uncertainty-aware Selection, we observe the highest top-rank prediction accuracy for any given number of annotations. .  ", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Acknowledgments", "text": "We thank the Department of Computer Science and Engineering, IIT Madras, and the Robert Bosch Center for Data Science and Artificial Intelligence, IIT Madras (RBC-DSAI), for providing us resources required to carry out this research. We also wish to thank Google for providing access to TPUs through the TFRC program. We thank the anonymous reviewers for their constructive feedback in enhancing the work.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond", "journal": "Trans. Assoc. Comput. Linguistics", "year": "2019", "authors": ""}, {"ref_id": "b1", "title": "Finite-time analysis of the multiarmed bandit problem", "journal": "Mach. Learn", "year": "2002", "authors": "Peter Auer; Nicol\u00f2 Cesa-Bianchi; Paul Fischer"}, {"ref_id": "b2", "title": "Preference-based online learning with dueling bandits: A survey", "journal": "J. Mach. Learn. Res", "year": "2021", "authors": "Viktor Bengs; R\u00f3bert Busa-Fekete; Adil El Mesaoudi-Paul; Eyke H\u00fcllermeier"}, {"ref_id": "b3", "title": "Findings of the 2014 workshop on statistical machine translation", "journal": "", "year": "2014-06-26", "authors": "Ondrej Bojar; Christian Buck; Christian Federmann; Barry Haddow; Philipp Koehn; Johannes Leveling; Christof Monz; Pavel Pecina; Matt Post; Herve Saint-Amand; Radu Soricut; Lucia Specia; Ales Tamchyna"}, {"ref_id": "b4", "title": "Findings of the 2016 conference on machine translation", "journal": "", "year": "2016-08-11", "authors": "Ondrej Bojar; Rajen Chatterjee; Christian Federmann; Yvette Graham; Barry Haddow; Matthias Huck; Antonio Jimeno-Yepes; Philipp Koehn; Varvara Logacheva; Christof Monz; Matteo Negri; Aur\u00e9lie N\u00e9v\u00e9ol; Mariana L Neves; Martin Popel; Matt Post; Raphael Rubino; Carolina Scarton; Lucia Specia; Marco Turchi; Karin M Verspoor; Marcos Zampieri"}, {"ref_id": "b5", "title": "Findings of the 2015 workshop on statistical machine translation", "journal": "", "year": "2015-09-18", "authors": "Ondrej Bojar; Rajen Chatterjee; Christian Federmann; Barry Haddow; Matthias Huck; Chris Hokamp; Philipp Koehn; Varvara Logacheva; Christof Monz; Matteo Negri; Matt Post; Carolina Scarton; Lucia Specia; Marco Turchi"}, {"ref_id": "b6", "title": "Rank analysis of incomplete block designs: I. the method of paired comparisons", "journal": "Biometrika", "year": "1952", "authors": "R Bradley; M E Terry"}, {"ref_id": "b7", "title": "Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners", "journal": "", "year": "2020-12-06", "authors": "Tom B Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell; Sandhini Agarwal; Ariel Herbert-Voss; Gretchen Krueger; Tom Henighan; Rewon Child; Aditya Ramesh; Daniel M Ziegler; Jeffrey Wu; Clemens Winter; Christopher Hesse; Mark Chen; Eric Sigler; Mateusz Litwin"}, {"ref_id": "b8", "title": "ELECTRA: pretraining text encoders as discriminators rather than generators", "journal": "", "year": "2020-04-26", "authors": "Kevin Clark; Minh-Thang Luong; Quoc V Le; Christopher D Manning"}, {"ref_id": "b9", "title": "Handling divergent reference texts when evaluating table-to-text generation", "journal": "Long Papers", "year": "2019-07-28", "authors": "Bhuwan Dhingra; Manaal Faruqui; Ankur P Parikh; Ming-Wei Chang; Dipanjan Das; William W Cohen"}, {"ref_id": "b10", "title": "Evaluating the state-of-the-art of end-to-end natural language generation: The E2E NLG challenge", "journal": "Comput. Speech Lang", "year": "2020", "authors": "Ondrej Dusek; Jekaterina Novikova; Verena Rieser"}, {"ref_id": "b11", "title": "Comparing automatic evaluation measures for image description", "journal": "Short Papers", "year": "2014-06-22", "authors": "Desmond Elliott; Frank Keller"}, {"ref_id": "b12", "title": "Maxing and ranking with few assumptions", "journal": "", "year": "2017-12-04", "authors": "Moein Falahatgar; Yi Hao; Alon Orlitsky; Venkatadheeraj Pichapati; Vaishakh Ravindrakumar"}, {"ref_id": "b13", "title": "Maximum selection and ranking under noisy comparisons", "journal": "PMLR", "year": "2017-08-11", "authors": "Moein Falahatgar; Alon Orlitsky; Venkatadheeraj Pichapati; Ananda Theertha Suresh"}, {"ref_id": "b14", "title": "Bootstrapping dialog systems with word embeddings", "journal": "", "year": "2014", "authors": "Gabriel Forgues; Joelle Pineau; Jean-Marie Larchev\u00eaque; R\u00e9al Tremblay"}, {"ref_id": "b15", "title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning", "journal": "", "year": "2016-06-19", "authors": "Yarin Gal; Zoubin Ghahramani"}, {"ref_id": "b16", "title": "Deep bayesian active learning with image data", "journal": "PMLR", "year": "2017-08-11", "authors": "Yarin Gal; Riashat Islam; Zoubin Ghahramani"}, {"ref_id": "b17", "title": "Trueskill tm : A bayesian skill rating system", "journal": "MIT Press", "year": "2006-12-04", "authors": "Ralf Herbrich; Tom Minka; Thore Graepel"}, {"ref_id": "b18", "title": "Bayesian active learning for classification and preference learning", "journal": "", "year": "2011", "authors": "Neil Houlsby; Ferenc Huszar; Zoubin Ghahramani; M\u00e1t\u00e9 Lengyel"}, {"ref_id": "b19", "title": "PARABANK: monolingual bitext generation and sentential paraphrasing via lexically-constrained neural machine translation", "journal": "AAAI Press", "year": "2019-01-27", "authors": "J Edward Hu; Rachel Rudinger; Matt Post; Benjamin Van Durme"}, {"ref_id": "b20", "title": "Rank correlation methods", "journal": "", "year": "1948", "authors": "M Kendall"}, {"ref_id": "b21", "title": "Regret lower bound and optimal algorithm in dueling bandit problem", "journal": "", "year": "2015-07-03", "authors": "Junpei Komiyama; Junya Honda; Hisashi Kashima; Hiroshi Nakagawa"}, {"ref_id": "b22", "title": "Importance of search and evaluation strategies in neural dialogue modeling", "journal": "Association for Computational Linguistics", "year": "2019-10-29", "authors": "Ilia Kulikov; Alexander H Miller; Kyunghyun Cho; Jason Weston"}, {"ref_id": "b23", "title": "ACUTE-EVAL: improved dialogue evaluation with optimized questions and multi-turn comparisons", "journal": "CoRR", "year": "2019", "authors": "Margaret Li; Jason Weston; Stephen Roller"}, {"ref_id": "b24", "title": "Beyond user self-reported likert scale ratings: A comparison model for automatic dialog evaluation", "journal": "ArXiv", "year": "2005", "authors": "Weixin Liang; J Zou; Zhou Yu"}, {"ref_id": "b25", "title": "ROUGE: A package for automatic evaluation of summaries", "journal": "Association for Computational Linguistics", "year": "2004", "authors": "Chin-Yew Lin"}, {"ref_id": "b26", "title": "Multilingual denoising pretraining for neural machine translation", "journal": "Trans. Assoc. Comput. Linguistics", "year": "2020", "authors": "Yinhan Liu; Jiatao Gu; Naman Goyal; Xian Li; Sergey Edunov; Marjan Ghazvininejad; Mike Lewis; Luke Zettlemoyer"}, {"ref_id": "b27", "title": "Individual choice behavior: A theoretical analysis", "journal": "", "year": "1979", "authors": "R Luce"}, {"ref_id": "b28", "title": "Sequence effects in crowdsourced annotations", "journal": "Association for Computational Linguistics", "year": "2017-09-09", "authors": "Nitika Mathur; Timothy Baldwin; Trevor Cohn"}, {"ref_id": "b29", "title": "Active learning for top-k rank aggregation from noisy comparisons", "journal": "PMLR", "year": "2017-08-11", "authors": "Soheil Mohajer; Changho Suh; Adel M Elmahdy"}, {"ref_id": "b30", "title": "Enabling robust grammatical error correction in new domains: Data sets, metrics, and analyses", "journal": "Transactions of the Association for Computational Linguistics", "year": "2019", "authors": "Courtney Napoles; Maria Nadejde; J Tetreault"}, {"ref_id": "b31", "title": "Ground truth for grammaticality correction metrics", "journal": "", "year": "2015", "authors": "Courtney Napoles; Keisuke Sakaguchi; Matt Post; J Tetreault"}, {"ref_id": "b32", "title": "Ground truth for grammaticality correction metrics", "journal": "", "year": "2015", "authors": "Courtney Napoles; Keisuke Sakaguchi; Matt Post; J Tetreault"}, {"ref_id": "b33", "title": "The conll-2014 shared task on grammatical error correction", "journal": "ACL", "year": "2014-06-26", "authors": " Hwee Tou Ng; Mei Siew; Ted Wu; Christian Briscoe; Raymond Hendy Hadiwinoto; Christopher Susanto;  Bryant"}, {"ref_id": "b34", "title": "Why we need new evaluation metrics for NLG", "journal": "Association for Computational Linguistics", "year": "2017-09-09", "authors": "Jekaterina Novikova; Ondrej Dusek; Amanda Cercas Curry; Verena Rieser"}, {"ref_id": "b35", "title": "Rankme: Reliable human ratings for natural language generation", "journal": "Association for Computational Linguistics", "year": "2018-06-01", "authors": "Jekaterina Novikova; Ondrej Dusek; Verena Rieser"}, {"ref_id": "b36", "title": "BLEU: a method for automatic evaluation of machine translation", "journal": "Association for Computational Linguistics", "year": "2002", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"ref_id": "b37", "title": "The analysis of permutations", "journal": "Journal of The Royal Statistical Society Series C-applied Statistics", "year": "1975", "authors": "R Plackett"}, {"ref_id": "b38", "title": "chrf: character n-gram f-score for automatic MT evaluation", "journal": "", "year": "2015-09-18", "authors": "Maja Popovic"}, {"ref_id": "b39", "title": "How does clickthrough data reflect retrieval quality?", "journal": "ACM", "year": "2008-10-26", "authors": "Filip Radlinski; Madhu Kurup; Thorsten Joachims "}, {"ref_id": "b40", "title": "A comparison of greedy and optimal assessment of natural language student input using word-to-word similarity metrics", "journal": "", "year": "2012-06-07", "authors": "Vasile Rus; C Mihai;  Lintean"}, {"ref_id": "b41", "title": "Re-evaluating ADEM: A deeper look at scoring dialogue responses", "journal": "AAAI Press", "year": "2019-01-27", "authors": "B Ananya; Mithun Das Sai; Mitesh M Gupta; Mukundhan Khapra;  Srinivasan"}, {"ref_id": "b42", "title": "Improving dialog evaluation with a multi-reference adversarial dataset and large scale pretraining", "journal": "Trans. Assoc. Comput. Linguistics", "year": "2020", "authors": "B Ananya; Akash Sai; Siddhartha Kumar Mohankumar; Mitesh M Arora;  Khapra"}, {"ref_id": "b43", "title": "A survey of evaluation metrics used for NLG systems", "journal": "", "year": "2008", "authors": "B Ananya; Akash Sai; Mitesh M Kumar Mohankumar;  Khapra"}, {"ref_id": "b44", "title": "Efficient online scalar annotation with bounded support", "journal": "Association for Computational Linguistics", "year": "2018-07-15", "authors": "Keisuke Sakaguchi; Benjamin Van Durme"}, {"ref_id": "b45", "title": "Reassessing the goals of grammatical error correction: Fluency instead of grammaticality", "journal": "Trans. Assoc. Comput. Linguistics", "year": "2016", "authors": "Keisuke Sakaguchi; Courtney Napoles; Matt Post; Joel R Tetreault"}, {"ref_id": "b46", "title": "Efficient elicitation of annotations for human evaluation of machine translation", "journal": "", "year": "2014-06-26", "authors": "Keisuke Sakaguchi; Matt Post; Benjamin Van Durme"}, {"ref_id": "b47", "title": "", "journal": "", "year": "2019", "authors": "Jo\u00e3o Sedoc; Daphne Ippolito; Arun Kirubarajan; Jai Thirani; Lyle Ungar; Chris Callison-Burch"}, {"ref_id": "b48", "title": "Chateval: A tool for chatbot evaluation", "journal": "", "year": "2019", "authors": ""}, {"ref_id": "b49", "title": "What makes a good conversation? how controllable attributes affect human judgments", "journal": "Association for Computational Linguistics", "year": "2019-06-02", "authors": "Abigail See; Stephen Roller; Douwe Kiela; Jason Weston"}, {"ref_id": "b50", "title": "BLEURT: learning robust metrics for text generation", "journal": "Association for Computational Linguistics", "year": "2020-07-05", "authors": "Thibault Sellam; Dipanjan Das; Ankur P Parikh"}, {"ref_id": "b51", "title": "Finding convincing arguments using scalable bayesian preference learning", "journal": "Trans. Assoc. Comput. Linguistics", "year": "2018", "authors": "D Edwin; Iryna Simpson;  Gurevych"}, {"ref_id": "b52", "title": "Dropout: a simple way to prevent neural networks from overfitting", "journal": "J. Mach. Learn. Res", "year": "2014", "authors": "Nitish Srivastava; Geoffrey E Hinton; Alex Krizhevsky; Ilya Sutskever; Ruslan Salakhutdinov"}, {"ref_id": "b53", "title": "Learning to summarize from human feedback", "journal": "", "year": "1325", "authors": "L Nisan Stiennon; Jeff Ouyang; D Wu; Ryan J Ziegler; Chelsea Lowe; A Voss; Dario Radford; Paul Amodei;  Christiano"}, {"ref_id": "b54", "title": "Is this translation error critical?: Classification-based human and automatic machine translation evaluation focusing on critical errors", "journal": "", "year": "2021", "authors": "Katsuhito Sudoh; Kosuke Takahashi; Satoshi Nakamura"}, {"ref_id": "b55", "title": "Online rank elicitation for plackett-luce: A dueling bandits approach", "journal": "", "year": "2015-12-07", "authors": "Bal\u00e1zs Sz\u00f6r\u00e9nyi; R\u00f3bert Busa-Fekete; Adil Paul; Eyke H\u00fcllermeier"}, {"ref_id": "b56", "title": "Generic exploration and k-armed voting bandits", "journal": "", "year": "2013", "authors": "F Tanguy Urvoy; R Cl\u00e9rot; Sami F\u00e9raud;  Naamane"}, {"ref_id": "b57", "title": "Attention is all you need", "journal": "", "year": "2017-12-04", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Lukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b58", "title": "Tl;dr: Mining reddit to learn automatic summarization", "journal": "Association for Computational Linguistics", "year": "2017-09-07", "authors": "Michael V\u00f6lske; Martin Potthast; Shahbaz Syed; Benno Stein"}, {"ref_id": "b59", "title": "Towards universal paraphrastic sentence embeddings", "journal": "", "year": "2016-05-02", "authors": "John Wieting; Mohit Bansal; Kevin Gimpel; Karen Livescu"}, {"ref_id": "b60", "title": "Double thompson sampling for dueling bandits", "journal": "", "year": "2016-12-05", "authors": "Huasen Wu; Xin Liu"}, {"ref_id": "b61", "title": "", "journal": "", "year": "", "authors": "Linting Xue; Noah Constant; Adam Roberts; Mihir Kale; Rami Al-Rfou; Aditya Siddhant; Aditya Barua"}, {"ref_id": "b62", "title": "The k-armed dueling bandits problem", "journal": "J. Comput. Syst. Sci", "year": "2012", "authors": "Yisong Yue; Josef Broder; Robert Kleinberg; Thorsten Joachims"}, {"ref_id": "b63", "title": "Beat the mean bandit", "journal": "Omnipress", "year": "2011-06-28", "authors": "Yisong Yue; Thorsten Joachims"}, {"ref_id": "b64", "title": "BERTScore: evaluating text generation with BERT", "journal": "", "year": "2020-04-26", "authors": "Tianyi Zhang; Varsha Kishore; Felix Wu; Kilian Q Weinberger; Yoav Artzi"}, {"ref_id": "b65", "title": "Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance", "journal": "Association for Computational Linguistics", "year": "2019-11-03", "authors": "Wei Zhao; Maxime Peyrard; Fei Liu; Yang Gao; Christian M Meyer; Steffen Eger"}, {"ref_id": "b66", "title": "Copeland dueling bandits", "journal": "", "year": "2015-12-07", "authors": "Masrour Zoghi; Zohar S Karnin; Shimon Whiteson; Maarten De Rijke"}, {"ref_id": "b67", "title": "Grammarly CoNLL '14 Task E2E NLG Para-Bank TL; DR tur-eng ron-eng cze-eng deu-eng fin-eng rus-eng deu-eng FCE Wiki Uniform", "journal": "", "year": "19479", "authors": ""}, {"ref_id": "b68", "title": "", "journal": "", "year": "10824", "authors": " Seq-Elim"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Our Active Evaluation framework consisting of a learner that chooses a pair of systems to compare at each time step. The learner receives feedback from either human annotators or the automatic metric.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Annotation complexity of Random Mixing with RMED using various automatic evaluation metrics", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Annotation complexity of (model-free) uniform exploration and dueling bandit algorithms v/s the number of NLG systems on the ParaBank dataset", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "selects the second system s (2) t that is most likely to beat s (1) t . SAVAGE: Sensitivity Analysis of VAriables for Generic Exploration (SAVAGE) (Urvoy et al.,", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure 5: Variation in annotation complexity with Mixing probability in Random Mixing with Bleurt on the left and with BALD threshold in Uncertainty-aware Selection (BALD) with Bleurt on the right", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 7 :7Figure 7: Annotation complexity of UCB Elimination with Bleurt v/s the Copland threshold for \u03b1 = 0.6", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 10 :10Figure 10: Annotation complexity of Random Mixing using the Electra metric with uniform exploration and dueling bandit algorithms as function of number of NLG systems on the ParaBank dataset", "figure_data": ""}, {"figure_label": "1112", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 11 :Figure 12 :1112Figure 11: Top-rank prediction accuracy as a function of the number of human annotations for (model-free) Uniform exploration and RUCB, RCS, and RMED dueling bandit algorithms on 12 NLG datasets", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Description of tasks and datasets with the number of NLG systems and pairwise human annotations et al.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Top-rank prediction accuracy v/s number of human annotations used on WMT 16 tur-eng dataset systems is high, e.g. E2E NLG (16 systems) and ParaBank (28 systems) datasets. On the other hand, dueling bandit algorithms such as RUCB", "figure_data": "1.00Prediction Accuracy0.25 0.50 0.75Uniform RUCB RCS RMED0.0001000200030004000Number of Human AnnotationsFigure 2:"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": ": Sentence-level accuracy of direct assessmentmetrics with linear probability model and our trainedElectra metric in predicting the comparison outcomeEvaluation MetricNoneEmbed. Avg.MoverScoreBleurtBleuLaserBertScoreElectra20001000Annotation Complexity0 0 2500 5000WMT16 tur-eng WMT15 fin-eng WMT15 rus-eng Grammarly-FCE WMT16 cze-eng WMT16 deu-eng Grammarly-Wiki E2E NLG20000100000WMT16 ron-eng WMT15 deu-eng CoNLL'14 TaskParaBankDataset"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Annotation complexity of model-based algorithms when used with RMED and Bleurt/Electra metric.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Masrour Zoghi, Shimon Whiteson, Maarten de Rijke, and R\u00e9mi Munos. 2014a. Relative confidence sampling for efficient on-line ranker evaluation. In Seventh ACM International Conference on Web Search and Data Mining, WSDM 2014, New York, NY, USA, February 24-28, 2014, pages 73-82. ACM.", "figure_data": "Masrour Zoghi, Shimon Whiteson, R\u00e9mi Munos, andMaarten de Rijke. 2014b. Relative upper confidencebound for the k-armed dueling bandit problem. InProceedings of the 31th International Conference onMachine Learning, ICML 2014, Beijing, China, 21-26 June 2014, volume 32 of JMLR Workshop andConference Proceedings, pages 10-18. JMLR.org."}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Description of tasks and datasets with the number of NLG systems, number of pairwise human annotations, label distribution and the downloadable links to the datasets before preprocessing", "figure_data": "Score and Bleurt. Among these metrics, Bleurtis the only trainable metric. We use the publiclyreleased Bleurt-base checkpoint trained on WMTdirect judgments data. As described in section 4.2,we apply Dropout to the Bleurt model during testtime to estimate prediction uncertainty."}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Various assumptions made by dueling bandit algorithms in the literature", "figure_data": "AlgorithmAssumptionsTargetIF"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Tuned Hyperparameters of Model-based algorithms when used with the Electra Metric", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "In figure 5, we show the effect of p m in Random Mixing with Bleurt and \u03c4 BALD in Uncertainty-aware Selection with Bleurt. We observe that with increases in both the hyperparameters, the annotation complex-", "figure_data": "DatasetRand. Mix.Uncertainty (BALD)UCB-Elim.p m\u03c4 BALD\u03b1\u03c4 copWMT (all 7 datasets)0.80.0050.50.8Grammarly (FCE & Wiki)0.80.00050.50.8CoNLL'140.010.0000510.7E2E NLG0.70.00250.50.8ParaBank0.40.00050.50.8"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "Tuned Hyperparameters of Model-based algorithms when used with the Bleurt Metric", "figure_data": "Random MixingUncertainty-aware SelectionAnnotation Complexity0 2000 4000 60000 4000 6000 2000WMT16 tur\u2192eng WMT16 ron\u2192eng WMT16 cze\u2192eng WMT16 deu\u2192eng WMT15 fin\u2192eng WMT15 rus\u2192eng WMT15 deu\u2192eng0.00.20.40.60.80.0000.0020.0040.0060.008Mixing Probability pmBALD Threshold \u03c4BALD"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "Sentence-level accuracy of direct assessment metrics with linear, BTL, and BTL-logistic probability models and our trained Electra metric in predicting the comparison outcome", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "(t) 1 , s (t)", "formula_coordinates": [2.0, 473.42, 275.78, 29.75, 15.86]}, {"formula_id": "formula_1", "formula_text": "Y (t) 1 , Y (t) 2 from the chosen systems s (t) 1 , s (t)", "formula_coordinates": [2.0, 306.14, 372.16, 195.35, 15.87]}, {"formula_id": "formula_2", "formula_text": "(t) 1 , Y (t) 2", "formula_coordinates": [2.0, 459.73, 388.01, 35.82, 15.87]}, {"formula_id": "formula_3", "formula_text": "(t) 1 , s (t)", "formula_coordinates": [2.0, 314.22, 568.3, 29.75, 15.87]}, {"formula_id": "formula_4", "formula_text": "P unif orm ((s (t) 1 , s (t) 2 ) = (i, j)) = 1 k 2", "formula_coordinates": [2.0, 334.45, 631.71, 155.88, 29.32]}, {"formula_id": "formula_5", "formula_text": "p(Y 1 \u227b Y 2 ) = 1 2 + (f (Y 1 ) \u2212 f (Y 2 ))", "formula_coordinates": [3.0, 346.03, 202.61, 160.33, 24.43]}, {"formula_id": "formula_6", "formula_text": "p(Y 1 \u227b Y 2 ) = f (Y 1 ) f (Y 1 ) + f (Y 2 )", "formula_coordinates": [3.0, 359.66, 267.77, 131.86, 25.5]}, {"formula_id": "formula_7", "formula_text": "w = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1, ifp(Y 1 \u227b Y 2 ) > \u03c4 2 0, ifp(Y 1 \u227b Y 2 ) < \u03c4 1 0.5, Otherwise", "formula_coordinates": [3.0, 342.53, 409.3, 143.82, 45.21]}, {"formula_id": "formula_8", "formula_text": "p(Y 1 \u227b Y 2 |D tr ) = \u03b8p (Y 1 \u227b Y 2 |\u03b8)p(\u03b8|D tr )d\u03b8", "formula_coordinates": [4.0, 77.86, 281.68, 203.97, 21.39]}, {"formula_id": "formula_9", "formula_text": "p(Y 1 \u227b Y 2 |D tr ) \u2248 1 L L l=1p (Y 1 \u227b Y 2 |\u03b8 l )", "formula_coordinates": [4.0, 92.15, 486.03, 175.7, 33.98]}, {"formula_id": "formula_10", "formula_text": "I = H(p) \u2212 1 L L l=1 H(p l )", "formula_coordinates": [4.0, 125.31, 737.51, 109.37, 33.98]}, {"formula_id": "formula_11", "formula_text": "\u03c3 = Var \u03b8\u223cp(\u03b8|D tr ) (p(Y 1 \u227b Y 2 |\u03b8))", "formula_coordinates": [4.0, 336.65, 255.14, 157.27, 11.22]}, {"formula_id": "formula_12", "formula_text": "C i = 1 k\u22121 j\u0338 =i 1(p ij > 1 2 ).", "formula_coordinates": [4.0, 306.14, 494.22, 131.23, 15.17]}, {"formula_id": "formula_13", "formula_text": "p ij = 1 N Y 1 ,Y 2 \u2208D ijp (Y 1 \u227b Y 2 |\u03b8)", "formula_coordinates": [4.0, 344.47, 585.15, 141.62, 30.47]}, {"formula_id": "formula_14", "formula_text": "\u03c3 2 ij = 1 N 2 Y 1 ,Y 2 \u2208D ij Var \u03b8\u223cq \u03d5 (\u03b8)p (Y 1 \u227b Y 2 |\u03b8)", "formula_coordinates": [5.0, 83.49, 243.58, 193.0, 30.47]}, {"formula_id": "formula_15", "formula_text": "C u i = 1 K\u22121 j\u0338 =i 1(\u00fb ij > 1 2 ).", "formula_coordinates": [5.0, 70.86, 324.91, 133.06, 15.17]}, {"formula_id": "formula_16", "formula_text": "min n \u2032 : \u2200n \u2265 n \u2032 , P (\u00ee * (n) = i * ) > 1 \u2212 \u03b4 acc", "formula_coordinates": [6.0, 83.06, 424.4, 193.37, 13.13]}, {"formula_id": "formula_17", "formula_text": "(t) 1 , Y (t)", "formula_coordinates": [15.0, 223.78, 186.34, 35.82, 15.87]}, {"formula_id": "formula_18", "formula_text": "i \u227b j \u21d0\u21d2 \u2206 ij > 0 Strong stochastic transitivity (SST) \u2206 ij > 0, \u2206 jk > 0 =\u21d2 \u2206 ik \u2265 max(\u2206 ij , \u2206 jk ) Relaxed stochastic transitivity (RST) \u2203\u03b3 \u2265 1: \u2206 ij > 0, \u2206 jk > 0 =\u21d2 \u03b3\u2206 ik \u2265 max(\u2206 ij , \u2206 jk ) Stochastic triangle inequality (STI) \u2206 ij > 0, \u2206 jk > 0 =\u21d2 \u2206 ik \u2264 \u2206 ij + \u2206 jk Condorcet winner (CW) \u2203i * : \u2206 i * ,j > 0, \u2200j \u2208 S \u2212 i *", "formula_coordinates": [16.0, 77.47, 94.17, 190.6, 81.3]}, {"formula_id": "formula_19", "formula_text": "(i) 1 , Y (i) 2 , w (i) } N i=1", "formula_coordinates": [17.0, 180.0, 236.67, 78.05, 15.86]}, {"formula_id": "formula_20", "formula_text": "Y (i) 1 , Y (i) 2", "formula_coordinates": [17.0, 70.86, 252.52, 44.53, 15.86]}, {"formula_id": "formula_21", "formula_text": "\u2206 i = |f (Y (i) 1 ) \u2212f (Y (i)", "formula_coordinates": [17.0, 148.44, 307.22, 112.83, 15.87]}, {"formula_id": "formula_22", "formula_text": "BTL: Let f m i = max{f (Y (i) 1 ),f (Y (i)", "formula_coordinates": [17.0, 81.77, 407.72, 166.12, 15.87]}, {"formula_id": "formula_23", "formula_text": "(t) 1 , s (t)", "formula_coordinates": [19.0, 105.21, 463.94, 29.75, 15.87]}], "doi": "10.1023/A:1013689704352"}