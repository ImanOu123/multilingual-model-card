{"title": "Event Recognition in Videos by Learning from Heterogeneous Web Sources", "authors": "Lin Chen; Lixin Duan; Dong Xu", "pub_date": "", "abstract": "In this work, we propose to leverage a large number of loosely labeled web videos (e.g., from YouTube) and web images (e.g., from Google/Bing image search) for visual event recognition in consumer videos without requiring any labeled consumer videos. We formulate this task as a new multi-domain adaptation problem with heterogeneous sources, in which the samples from different source domains can be represented by different types of features with different dimensions (e.g., the SIFT features from web images and space-time (ST) features from web videos) while the target domain samples have all types of features. To effectively cope with the heterogeneous sources where some source domains are more relevant to the target domain, we propose a new method called Multi-domain Adaptation with Heterogeneous Sources (MDA-HS) to learn an optimal target classifier, in which we simultaneously seek the optimal weights for different source domains with different types of features as well as infer the labels of unlabeled target domain data based on multiple types of features. We solve our optimization problem by using the cutting-plane algorithm based on group-based multiple kernel learning. Comprehensive experiments on two datasets demonstrate the effectiveness of MDA-HS for event recognition in consumer videos.", "sections": [{"heading": "Introduction", "text": "There is an increasing interest in developing new event recognition techniques for searching and indexing the explosively growing consumer videos. However, visual event recognition in consumer videos is a challenging task because of cluttered backgrounds, complex camera motion and large intra-class variations.\nIn [4], Chang et al. developed a multi-modal system to fuse visual and audio features for consumer video classification. For action recognition in YouTube videos, different strategies were exploited in [24] to effectively integrate motion and static features, and a multi-instance learning approach was proposed in [16] to fuse different features.\nTo improve the recognition accuracies for YouTube videos, Wang et al. [30] proposed a new descriptor by describing each video with dense trajectories. In these works, a sufficient labeled training videos are required to learn robust classifiers for event recognition.\nHowever, collecting of labeled training videos based on human annotation is time-consuming and expensive. Observing that keyword (or tag) based search can be readily used to collect a large number of relevant web images or web videos without human annotation [9], researchers recently proposed new approaches that exploit the rich and massive web data for the event recognition task. With the aid of YouTube videos, Duan et al. [7] proposed a domain adaptation approach by reducing the data distribution mismatch between web and consumer videos. In [9], Duan et al. developed a multi-domain adaptation scheme by leveraging web images from different sources. In [17], classifiers are learnt for action recognition by using incrementally collected web images. However, temporal information was not used in [9,17], so both works [9,17] cannot distinguish events like \"sitting down\" and \"standing up\" [7].\nIn this work, we propose to leverage a large number of freely available web videos (e.g., from YouTube) and web images (e.g., from Google/Bing image search) for event recognition in consumer videos (see Fig. 1), where there are no labeled consumer videos. We propose to additionally use web images for event recognition, which is based on two motivations [7]: 1) there are more web images available with loose labels than web videos; 2) the learnt classifiers using relevant web images are also useful because web images are generally associated with more accurate tags than web videos. Motivated by [7,9], we formulate this task as a new multi-domain adaptation problem with heterogeneous sources, in which samples from different source domains can be represented by different types of features with different dimensions (e.g., SIFT features from web images and Space-time (ST) features from web videos 1 ) while the target domain samples have all types of features.\nObserving that some source domains are more relevant to the target domain, in Section 3, we propose a new method called Multi-domain Adaptation with Heterogeneous Sources (MDA-HS) to effectively cope with heterogeneous sources. Specifically, we seek the optimal weights for different source domains with different types of features and also infer the labels of unlabeled target domain data based on all types of features. For each source domain, we propose to learn an adapted classifier based on the pre-learnt source classifier with data distribution mismatch, for which we minimize the distance between the two classifiers in terms of their weight vectors. We introduce a new regularizer by summing the weighted distances from all the source domains and combine all the weighted adapted classifiers as a new target classifier. We also propose a new \u03c1-SVM based objective function by using the new regularizer and target classifier for domain adaptation. We develop an iterative optimization method by using the cutting plane method and solving a group-based multiple kernel learning (MKL) problem. In Section 4, we conduct comprehensive experiments using two benchmark consumer video datasets as the target domain, and the results demonstrate that our method MDA-HS outperforms the existing multi-domain adaptation methods for event recognition.", "publication_ref": ["b3", "b23", "b15", "b29", "b8", "b6", "b8", "b16", "b8", "b16", "b8", "b16", "b6", "b6", "b6", "b8", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Recently, domain adaptation has attracted increasing attention in computer vision because of its successful applications in object recognition [12,13,14,20,26], event recognition [7,9] and video concept detection [8]. Most existing approaches focus on the setting with a single source domain. For example, a few SVM based methods [2,7,32] were recently developed. Saenko et al. [26] and Kulis et al. [20] proposed to learn the feature transformations for domain adaptation. Gopalan et al. [14] and Gong et al. [13] proposed new domain adaptation methods by interpolating new subspaces to bridge the two domains.\nMulti-domain adaptation methods [9,15,10,5,27,28] were also proposed when training data come from multiple source domains. Duan et al. [9] proposed a domain selection method to select the most relevant source domains. Based on the discovered latent source domains, Hoffman et al. [15] extended [20] for multi-domain adaptation by learning multiple transformations. In [5], Chattopadhyay et al. proposed a two-step approach to first learn the weight for each source domain and then learn the target classifier by using the learnt source domain weights. However, most existing multi-domain adaptation methods assume all the training samples from the source and target domains be treated as two source domains, in which different features are extracted from the same set of samples. are represented by the same type of feature. Therefore, they cannot effectively handle our setting with labeled singleview data from heterogeneous source domains and unlabeled multi-view data from the target domain. These existing methods can only fuse the decisions from multiple models (with each model learnt by using one type of feature) in a late-fusion fashion [27,10,5,28] or concatenate multiple features into one lengthy vector as the feature representation for target domain data in an early fusion fashion [10,5,28,9]. In contrast, our work, MDA-HS, can simultaneously learn the optimal target classifier and seek the optimal weights for different source domains with different types of features.\nOur work is also different from heterogeneous domain adaption (HDA) methods [20,11], in which the samples from the source and the target domains are represented by different types of features. In contrary, in our work the samples from each pair of source and target domains share the same type of feature because we assume the target domain samples are represented by all types of features. In the existing HDA methods [20,11], labeled target domain samples must be provided. In contrast, we do not require any labeled target domain samples.\nOur work is also different from existing multi-view learning approaches including multi-view based domain adaptation methods [33,6]. These works [33,6] generally assume all the data in the source and target domains have multiple types of features, which is different from our setting (see Fig. 2). Moreover, these works cannot be used to learn the weights for different source domains, which is the key challenging issue in multi-domain adaptation.", "publication_ref": ["b11", "b12", "b13", "b19", "b25", "b6", "b8", "b7", "b1", "b6", "b31", "b25", "b19", "b13", "b12", "b8", "b14", "b9", "b4", "b26", "b27", "b8", "b14", "b19", "b4", "b26", "b9", "b4", "b27", "b9", "b4", "b27", "b8", "b19", "b10", "b19", "b10", "b32", "b5", "b32", "b5"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Consumer Video Recognition using Heterogeneous Data Sources", "text": "In this paper, our task is to recognize visual events in consumer videos by leveraging a large number of loosely labeled web images and web videos, where there are no labeled consumer videos available. Specifically, we represent each web image by using 2D visual features (e.g., SIFT features [25]) and each web video by using 3D visual features (e.g., space-time features [21]). And every consumer video is described based on both 2D and 3D features. As different domains (i.e., the consumer video domain and the web domain) have different data distributions and the samples from different source domains (i.e., web image domain and web video domain) are in different feature spaces, we aim to cope with the unsupervised domain adaptation problem with heterogeneous sources in this work.\nFollowing the terminology of domain adaptation, we refer to the consumer video domain as the target domain, as well as consider the web image and video domains as the heterogeneous source domains. Note that the target data have multiple views of features, while the data from each source domain has only one view of feature. Our goal is to learn a robust target classifier by using the loosely labeled single-view data from the heterogeneous source domains and the unlabeled multi-view data from the target domain. In this work, we assume that we have S heterogeneous source domains and focus on the binary classification problem. For each class, we are given a set of labeled single-view data {(x s i , y s i )| ns i=1 } from the s-th source domain, where n s is the total number of samples from the s-th source domain and each sample x s is drawn from a fixed but unknown data distribution P s , y s \u2208 {\u22121, 1} is the label of x s , and s = 1, . . . , S. And we are also provided with a set of unlabeled multi-view data {z i | nT i=1 } from the target domain, where n T is the total number of target domain samples and each sample z has S views (i.e., z = z [1] , ..., z [S] ) and the s-th view z [s] (drawn from P\n[s]\nT ) is the same view as x s , namely, z [s] and x s share the same type of feature with the same dimension (see Fig. 2 for the feature correspondences). Note that for our domain adaptation setting with heterogeneous sources, we have\nP i = P j , P [i] T = P [j] T and P i = P [i]\nT (\u2200i, j = 1, ..., S and i = j). The goal of our work is to simultaneously cope with multiple data distribution mismatches between each pair of web domain and consumer domain and assign higher weights to the most relevant source domains.\nIn the remainder of this paper, we denote the transpose of a vector or matrix by using the superscript . Also, we define 0 n and 1 n as n \u00d7 1 vectors of all zeros and all ones, respectively. Let us denote as the element-wise product between two vectors or two matrices. Moreover, a \u2264 b represents a i \u2264 b i , \u2200i. I n is defined as a n \u00d7 n identity ", "publication_ref": ["b24", "b20", "b0"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Proposed Formulation", "text": "Motivated by multiple kernel learning (MKL) [31], we propose to learn the following target classifier f T for the prediction of any test sample z from the target domain, which fuses the decisions from multiple views of data:\nf T (z) = S s=1 d s w s \u03c6 s (z [s] ),(1)\nwhere w s is the weight vector for the s-th view of target data; \u03c6 s is the feature mapping function for the target data z [s] of the s-th view; and d s \u2265 0 is a combination coefficient.\nRecall that there are no labeled data in the target domain. The existing domain adaptation methods [27,13] are not specifically designed for our setting with multiple heterogeneous domains, so these methods generally cannot work well. Recently, Aytar and Zisserman [1] investigated the single source domain adaptation problem and tried to utilize the pre-learnt source classifier u \u03c6(x) to learn the target classifier by regularizing u and the weight vector w T of the target classifier, i.e., w T \u2212 \u03b3u 2 2 , where the parameter \u03b3 controls the amount of knowledge transferred from the source domain to the target domain. Inspired by their method [1], we make use of a set of pre-learnt source classifiers f s (x s ) = u s \u03c6 s (x s )'s trained by using the training data from each individual source domain and propose a new regularizer as follows for multiple heterogeneous source domains by linearly combining the distances between the target classifier and pre-learnt source classifiers in terms of their weight vectors from all views:\nS s=1 d s w s \u2212 \u03b3 s u s 2 2 , (2\n)\nwhich is to be minimized in our proposed optimization problem. It is worth noting that we also use the same d s in (1) as the weight in the above regularizer. The explanation is as follows. If the target model in the s-th view is closer to the source model, d s will be larger. In this case, we also expect the classifier from the s-th view will have a bigger contribution on the final prediction in the target classifier (1).\nNote that d = [d 1 , . . . , d S ]\nis usually constrained based on either L1 or L2 norm [31]. In this work, we assume that d 2 2 = 1. In order to learn the target classifier in (1) as well as simultaneously infer the labels y T i of unlabeled training data from the target domain, we introduce our new regularizer in (2) and our target classifier in (1) into a \u03c1-SVM based objective function as follows:\nmin d\u2208D,yT min ws,\u03b3s, \u03c1,\u03be s i ,\u03be T i 1 2 S s=1 d s w s \u2212 \u03b3 s u s 2 2 + \u03b8\u03b3 2 s \u2212 \u03c1 + 1 2 C S S s=1 ns i=1 \u03be s2 i +C T n T i=1 \u03be T 2 i , (3) s.t. y T i \u2208 {\u00b11}, y T i S s=1 d s w s \u03c6 s (z [s] i ) \u2265 \u03c1\u2212\u03be T i ,(4) y s i d s w s \u03c6 s (x s i ) \u2265 \u03c1 \u2212 \u03be s i , s = 1, ..., S, (5)\nwhere \u03b8, C S , C T > 0 are regularization parameters, D =\nd | d 2 2 = 1, d \u2265 0 S is the domain of d, y T = [y T 1 , ..., y T nT ]\nis the label vector of the target training samples, and \u03be s i , \u03be T i are slack variables of the training samples in s-th source domain and the target domain, respectively. In the above formulation, we penalize the L2 norm of \u03b3 s 's in (3) to avoid overfitting. Note that we enforce the target model of the s-th view to have good classification performance on the corresponding labeled source data. We argue that such supervision is very important for our multi-domain adaptation problem. The reason is two-fold: 1) There is a certain amount of overlap between the s-th source domain and the target domain when using the s-th view of features, so it is very possible that a well trained model using the labeled source domain data would not perform poorly on the target domain; 2) we do not have any labeled data in the target domain, so the performance of our model will become much worse without having the constraints in (5) (see our experimental results in Section 4). It is also worth mentioning that this problem is a mixed integer programming (MIP) problem. T , we then define \u03a6 [s] over all the samples by setting the columns not related to the samples from the s-th source domain and the target domain as zeros, namely:", "publication_ref": ["b30", "b26", "b12", "b0", "b1", "b0", "b0", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "A Dual Perspective", "text": "\u03a6 [s] = O hs\u00d7N (1,s\u22121) , \u03a6 s , O hs\u00d7N (s+1,S) , \u03a6 [s] T . (6\n)\nBased on f s and f\n[s]\nT , we can similarly define f [s] as  6) and ( 7) become an empty matrix or an empty vector.\nf [s] = 0 N (1,s\u22121) , f s , 0 N (s+1,S) , f [s] T . (7\nWe solve (3) by first taking the dual form of the inner optimization problem with respect to the primal variables \u03c1, w s , \u03b3 s , \u03be s i and \u03be T i , where s = 1, . . . , S. Specifically, by introducing the Lagrange multipliers \u03b1 i 's for the inequality constraints in (4) and ( 5), we then have the Lagrangian L of inner optimization problem in (3). By setting the derivatives of the Lagrangian to zeros with respect to the primal variables \u03c1, w s 's, \u03b3 s 's, \u03be s i 's and \u03be T i 's, and substituting the resultant equalities back into L, we can rewrite the optimization problem in (3) as follows by replacing the inner problem with its dual form:\nmin d\u2208D, y\u2208Y max \u03b1\u2208A \u2212 1 2 \u03b1 S s=1 d sK [s]\nyy +\u0128 \u03b1, (8) where \u03b1 = [\u03b1 1 , . . . , \u03b1 n ] is a vector of dual variables with n = S s=1 n s + n T (note its first N (1, S) elements are for the constraints in (5) of all source domain samples and its last n T elements are for the constraints in (4) of the target domain samples); A = {\u03b1|\u03b1 1 n = 1, \u03b1 \u2265 0 n } is the domain of \u03b1; y is the label vector of all training samples, which takes the values from the feasible set of\nY = {y|y = [y 1 , . . . , y S , y T ] , y T \u2208 {\u22121, 1} nT } (note y s = [y s 1 , . . . ,", "publication_ref": ["b2", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "y s", "text": "ns ] is the given label vector for the s-th source domain data), and y is also represented as y = [y 1 , . . . , y n ] with its elements in the same order;\nI = diag 1 n1 /C S , . . . , 1 nS /C S , 1 nT /C T\nis a diagonal matrix;K [s] is the transformed kernel matrix defined over all the samples but only based on the s-th source domain data and the s-th view of target data, namely:\nK [s] = K [s] + 1 \u03b8 f [s] f [s] ,(9)\nwhere K [s] = \u03a6 [s] \u03a6 [s] with \u03a6 [s] and f [s] as defined in ( 6) and ( 7), respectively. Convex relaxation. Note that we need to determine the label vector y T for the unlabeled target domain data by solving the MIP problem in (8), which is NP hard. We thus relax (8) to be a convex optimization problem which is the lower bound of ( 8) as shown in the following Proposition 1:\nProposition 1 The objective value of the mixed integer programming (MIP) problem in ( 8) is lower bounded by the optimal value of the following group-based multiple kernel learning (MKL) problem:\nmin D max \u03b1\u2208A \u2212 1 2 \u03b1 \u239b \u239d S s=1 o: y o \u2208Y D so Q so +\u0128 \u239e \u23a0 \u03b1, (10\n)\ns.t. D 2,1 = 1, D so \u2265 0 \u2200s , \u2200o,\nwhere Q so is a base label-kernel defined as Q so =K [s] (y o y o ) with y o being the o-th feasible labeling candidate for y, D = [D so ] \u2208 R S\u00d7|Y| is the kernel coefficient matrix (note |Y| is the size of Y), and\nD 2,1 = |Y| o=1 S s=1 D 2 so is the mixed L 2,1 norm.\nProof According to the theoretical results in [23], the objective value of ( 8) is lower bounded by the optimal value of the following optimization problem:\nmin d,\u03bc max \u03b1\u2208A \u2212 1 2 \u03b1 \u239b \u239d S s=1 o:y o \u2208Y d s \u03bc oK [s] y o y o +\u0128 \u239e \u23a0 \u03b1, (11) s.t. d 2 2 = 1, d \u2265 0, \u03bc 1 = 1, \u03bc \u2265 0, (12\n)\nwhere \u03bc = [\u03bc 1 , ..., \u03bc |Y| ] . By setting D so = d s \u03bc o , we have D 2,1 = 1. And ( 11) is converted into the convex optimization problem in (10), and its optimal objective value is the lower bound of the objective value of (11).", "publication_ref": ["b7", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Detailed Algorithm", "text": "As the size of Y increases exponentially with the number of unlabeled target data, making the optimization of the objective function in (10) still computationally expensive when there exist a large number of unlabeled target data. Fortunately, we can employ the cutting-plane method to iteratively select a small number of most violated labeling candidates (i.e., y o 's) which are good enough to approximate the optimal solution [19]. The details are listed in Algorithm 1. Find the most violated labeling candidate y o+1 by solving (13) 5:\nSet Y o+1 = Y o \u222a {y o+1 } 6: o \u2190 o + 1 7:\nuntil The objective of (10) converges [s] is decomposed by using eigenvalue decomposition. Motivated by [23,22], we develop an efficient algorithm to solve the integer programming problem in ( 13) by relaxing the L 2 norm into L \u221e norm:\nwhereK [s] = U [s] U\nmax y o \u2208Y U [s] \u03b1 y o \u221e = max j=1,...,n max y o \u2208Y n i=1 \u03b1 i y o i U [s] ij , (14\n)\nwhere U\n[s] ij is the element in the i-th row and j-th column of U [s] . The integer programming problem in ( 14) can be efficiently solved by simply sorting the coefficients \u03b1 i y o i 's. Note we only need to infer the labels of unlabeled target domain data (i.e., y T \u2208 {\u22121, 1} n T ), because the source label vectors y s 's are already given. Solving \u03b1 and D. After finding y o , we fix Y = Y o and solve the remaining group-based MKL problem in (10) by alternatively updating \u03b1 and D. Specifically, when we fix D, (10) reduces to a standard SVM and \u03b1 is updated by efficiently solving the standard SVM with existing softwares such as LIBSVM [3]. When \u03b1 is fixed, after re-formulating (10) in its primal form as well as dropping the irrelevant terms, D can be updated by solving the following optimization problem 2 :\nmin D\u2208M 1 2 S s=1 |Y| o=1 v so 2 2 D so , (15\n)\nwhere\nM = D D 2,1 = 1, D so \u2265 0 \u2200s, o is the do- main of D, v so 2 = D so \u221a \u03b1 Q so \u03b1, \u2200s, o.\nWith simple derivation, (15) has the following close-form solution:\nD so = v so 2/3 2 S l=1 v lo 4/3 2 1/4 |Y| o=1 S l=1 v lo 4/3 2 3/4 . (16\n)\nWe list the algorithm to solve the group-based MKL problem in (10) in Algorithm 2. Update D \u03c4 +1 by using (16) 5:\n\u03c4 \u2190 \u03c4 + 1 6: until The objective of (10) converges with fixed Y Target classifier. After finding the optimal \u03b1, D and y o 's, the target classifier in (1) can be rewritten as\nf T (z) = S s=1 \u03b2 s \u03a6 [s] \u03c6 s (z [s] ) + 1 \u03b8 f [s] f s (z [s] ) ,\nwhere\n\u03b2 s = \u03b1 |Y| o=1 D so y o .", "publication_ref": ["b18", "b12", "b22", "b21", "b9", "b2", "b14", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We compare our work with the baseline method SVM, the existing single source domain adaptation algorithms Geodesic Flow Kernel (GFK) 3 [13] and Domain Adaptive SVM (DASVM) [2], as well as the existing multidomain adaptation methods Domain Adaptation Machine (DAM) [10], Conditional Probability based Multi-source Domain Adaptation (CPMDA) [5], Maximal Margin Target Label Learning (MMTLL) [28] and Domain Selection Machine (DSM) [9].\nWe also report the results of two simplified versions (referred to as MDA-HS sim1 and MDA-HS sim2) of our method MDA-HS in order to validate our new regularizer in (2) and the constraint in (5). In MDA-HS sim1, we set the parameter \u03b8 = \u221e. In this case, we have \u03b3 s = 0 in (3) and our regularizer in (2) becomes S s=1 d s w s 2 2 , so the pre-learnt source classifiers will not be employed when calculating the kernel (See Eq. ( 9)). In order to demonstrate it is beneficial to employ the source domain data in MDA-HS, we compare our work with MDA-HS sim2 which excludes the constraints in (5).", "publication_ref": ["b2", "b12", "b1", "b9", "b4", "b27", "b8", "b1", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Datasets and Features", "text": "We evaluate all the methods on two benchmark consumer video datasets (i.e., Kodak [7] and CCV [18]), which are also used in [9]. To construct the heterogenous sources, we use the YouTube dataset in [7] and additionally collect two datasets by using Google/Bing image search. Note the labels of source domain data are noisy because we do not spend additional efforts to annotate the YouTube dataset and the two web image datasets. The detailed information of the five datasets is introduced below. Google/Bing image dataset: We download the top ranked 200 images for each event class by using related keywords as queries (e.g., we use \"wedding ceremony\", \"wedding reception\" and \"wedding dance\" for the event class \"wedding\") and we enforce the returned images to be photo with full color by using the advanced options provided by Google and Bing image search. We do not download the corrupted images or the images with invalid URLs. Finally, we have collected 1049 images and 1134 images from Google and Bing respectively. YouTube dataset: The YouTube dataset was used as the source domain data in [7], which consists of 906 videos from six event classes (i.e., \"birthday\", \"picnic\", \"parade\", \"show\", \"sports\" and \"wedding\"). The videos were collected by using keyword based search from YouTube. According to the study in [7], at least 20% of the videos in this dataset are with incorrect labels. Kodak dataset: The Kodak dataset was used in [7,9], which contains 195 consumer videos from six event classes (i.e., \"birthday\", \"picnic\", \"parade\", \"show\", \"sports\" and \"wedding\"). CCV dataset: The CCV dataset [18] collected by Columbia University was also used in [9]. It consists of a training set of 4,659 videos and a test set of 4,658 videos from 20 semantic categories. Following [9], we only use the videos from the event related categories and we also merge \"wedding ceremony\", \"wedding reception\" and \"wedding dance\" as \"wedding\", \"non-music performance\" and \"music performance\" as \"show\" 4 , and \"baseball\", \"basketball\", \"biking\", \"ice skating\", \"skiing\", \"soccer\", \"swimming\" as \"sports\". Finally, there are 2440 videos from five event classes 5 (i.e., \"birthday\", \"parade\", \"show\", \"sports\" and \"wedding\"). Features: We extract the 128-dim SIFT features using Difference of Gaussians (DoG) interest point detector [25] for each image in the Google and Bing datasets. Then, each image is represented by a 4000-dim token frequency (TF) feature using the bag-of-word (BoW) representation, in which the codebook is constructed by using k-means to cluster all the SIFT features from the images. For each video in the Kodak and CCV datasets, we sample one keyframe per two seconds and then extract the SIFT features from each keyframe. Then, each keyframe is represented by a 4000dim TF feature based on the BoW representation by using the same codebook. Finally, we average the TF features over all the keyframes within each video as the final feature representation for the videos in the Kodak and CCV datasets when using the SIFT features.\nFor each video in the Kodak, YouTube and CCV datasets, we extract three types of space-time (ST) features (i.e., 96-dim Histogram of Oriented Gradient, 108-dim Histogram of Optical Flow and 192-dim Motion Boundary Histogram) by using the source codes provided in [30], in which we set the trajectory length as 50, the sampling stride as 16, and all the other parameters as their default values. We also use the BoW representation for each type of ST features, in which the codebook is constructed by using k-means to cluster the ST features from all videos in the YouTube dataset into 2000 clusters. Finally, each video is represented as a 6000-dim feature by concatenating the 2000-dim TF feature from each type of ST feature.", "publication_ref": ["b6", "b17", "b8", "b6", "b6", "b6", "b6", "b8", "b17", "b8", "b8", "b4", "b24", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setups", "text": "In our experiments, the Google and Bing image datasets and the YouTube dataset are used as S = 3 heterogeneous source domains, and Kodak/CCV dataset is used the target domain.\nNote we do not have any labeled consumer videos in the target domain. We refer to the baseline SVM as SVM A in which S independent SVM classifiers (i.e., f s 's) are trained based on the training data from each individual source domain and further used to predict the test data from the target domain using the same feature. And the final prediction of each test sample is obtained by averaging the predictions from all the classifiers. We also employ the same late fusion strategy for the single source domain adaptation methods GFK and DASVM. The traditional multiple source domain adaptation methods CPMDA, DAM, DSM and MMTLL can not directly deal with our setting with single-view source data and multi-view target data, so we use S pre-learnt source classifiers and also averagely fuse the kernels from all views as the kernel for the target domain data.\nWe evaluate all the methods by training one-vsrest SVMs with the Gaussian kernel k s (\nx i , x j ) = \u03c6 s (x i ) \u03c6 s (x j ) = exp \u2212 1 \u03bd x i \u2212 x j 2 2\n, in which we set the bandwidth parameter \u03bd as the mean of the square distances between all pairs of training samples when using s-th view of features. We also set the regularization parameters C S and C T as 1 and 10 respectively, since the target domain data is more important than the source domain data. We fix \u03b8 = 0.1 for our method MDA-HS and its simplified cases MDA-HS sim1 and MDA-HS sim2. As in [7,9], we use the non-interpolated average precision (AP) for performance evaluation and report the mean AP (MAP) over all the event classes.", "publication_ref": ["b6", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Table 1 shows the MAPs of all methods. We have the following observations from these results: there is no consistent winner between SVM A and DASVM on both datasets. These results show that it is a quite challenging task to conduct domain adaptation from heterogeneous source domains. The existing domain adaptation methods cannot always achieve good performances in this application because these methods cannot effectively cope with the heterogeneous sources. Moreover, the promising performance from SVM A indicates that the data distributions of the s-th source domain and the target domain when using the s-the view of features overlap to some extent.\n2) DSM is beter than SVM A. An explanation is that DSM can select the more relevant source domains.\n3) MDA-HS and MDA-HS sim1 are both much better than MDA-HS sim2, which demonstrates it is beneficial to train the target weight vector w s 's from multiple views of features by using the labeled source domain samples. Again, the explanation is there is a certain amount of overlap between each source domain and the target domain when using the same view of features. Moreover, MDA-HS is also better than MDA-HS sim1, which demonstrates the effectiveness of our new regularizer by leveraging the pre-learnt source classifiers. 4) Our method MDA-HS achieves the best performance among all methods on both datasets, which clearly demonstrates the effectiveness of our MDA-HS for event recognition in consumer videos by utilizing our new regularizer and the new target classifier. Analysis on the learnt source domain weights. We report the learnt weights of source domains in our MDA-HS and also report the per-event AP results of three SVMs with each trained by using the training data from one single source domain (i.e., YouTube, Bing or Google). If the AP of one SVM is higher, the corresponding source domain is expected to be more relevant to the target domain when using the same view of features, namely, we have larger source domain weight. As in our method MDA-HS, we relax our original optimization problem in (8) to be a group-based MKL problem in (10), in this experiment we therefore an- alyze D in (10) instead of d in (8). Specifically, we report the three coefficients of the column in D with the largest L 2 norm, and similar results can be observed for other columns. In Fig. 3, we take the event \"sports\" as an example to show the per-event AP results of three SVMs as well as the three source domain weights (i.e., the three learnt coefficients) on both datasets. From these results, we observe that MDA-HS can assign the highest weight to the most relevant source domain for which the per-event AP is also the highest. The results clearly show the effectiveness of the groupbased MKL technique in MDA-HS for combining multiple heterogeneous source domains.", "publication_ref": ["b7", "b9", "b7"], "figure_ref": ["fig_5"], "table_ref": ["tab_1"]}, {"heading": "Conclusion", "text": "We have proposed a new framework for visual event recognition in consumer videos by leveraging a large number of freely available web videos (e.g., from YouTube) and web images (e.g., from Google/Bing image search). This task is formulated as a new multi-domain adaptation problem with heterogeneous sources. By introducing a new target classifier and a new regularizer based on the weights of heterogeneous source domains, our method called Multi-domain Adaptation with Heterogeneous Sources (MDA-HS) can simultaneously seek the optimal weights for different source domains with different types of features, infer the labels of unlabeled target domain data based on multiple types of features, and learn the optimal target classifier. Comprehensive experiments by using two benchmark consumer video datasets, Kodak and CCV, demonstrate the effectiveness of our method MDA-HS for event recognition without requiring any labeled consumer videos.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgement. This research is partially supported by Multi-plAtform Game Innovation Centre (MAGIC) in Nanyang Technological University. MAGIC is funded by the Interactive Digital Media Programme Office (IDMPO) hosted by the Media Development Authority of Singapore.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Tabula rasa: Model transfer for object category detection", "journal": "", "year": "2011", "authors": "Y Aytar; A Zisserman"}, {"ref_id": "b1", "title": "Domain adaptation problems: A dasvm classification technique and a circular validation strategy. T-PAMI", "journal": "", "year": "2010", "authors": "L Bruzzone; M Marconcini"}, {"ref_id": "b2", "title": "LIBSVM: A library for support vector machines", "journal": "ACM Transactions on Intelligent Systems and Technology", "year": "2011", "authors": "C.-C Chang; C.-J Lin"}, {"ref_id": "b3", "title": "Large-scale multimodal semantic concept detection for consumer video", "journal": "", "year": "2007", "authors": "S.-F Chang; D Ellis; W Jiang; K Lee; A Yanagawa; E C Loui; J Luo"}, {"ref_id": "b4", "title": "Multisource domain adaptation and its application to early detection of fatigue", "journal": "", "year": "2007", "authors": "R Chattopadhyay; J Ye; S Panchanathan; W Fan; I Davidson"}, {"ref_id": "b5", "title": "Co-training for domain adaptation", "journal": "", "year": "2011", "authors": "M Chen; K Q Weinberger; J C Blitzer"}, {"ref_id": "b6", "title": "Visual event recognition in videos by learning from web data", "journal": "T-PAMI", "year": "2012", "authors": "L Duan; I W Tsang; D Xu; J Luo"}, {"ref_id": "b7", "title": "Domain transfer multiple kernel learning", "journal": "T-PAMI", "year": "2012", "authors": "L Duan; I W Tsang; D Xu; S J Maybank"}, {"ref_id": "b8", "title": "Exploiting web images for event recognition in consumer videos: A multiple source domain adaptation approach", "journal": "", "year": "2012", "authors": "L Duan; D Xu; S.-F Chang"}, {"ref_id": "b9", "title": "Domain adaptation from multiple sources: A domain-dependent regularization approach", "journal": "T-NNLS", "year": "2012", "authors": "L Duan; D Xu; I W Tsang"}, {"ref_id": "b10", "title": "Learning with augmented features for heterogeneous domain adaptation", "journal": "", "year": "2012", "authors": "L Duan; D Xu; I W Tsang"}, {"ref_id": "b11", "title": "Connecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation", "journal": "", "year": "2013", "authors": "B Gong; K Grauman; F Sha"}, {"ref_id": "b12", "title": "Geodesic flow kernel for unsupervised domain adaptation", "journal": "", "year": "2012", "authors": "B Gong; Y Shi; F Sha; K Grauman"}, {"ref_id": "b13", "title": "Domain adaptation for object recognition: An unsupervised approach", "journal": "", "year": "2011", "authors": "R Gopalan; R Li; R Chellappa"}, {"ref_id": "b14", "title": "Discovering latent domains for multisource domain adaptation", "journal": "", "year": "2012", "authors": "J Hoffman; K Saeko; B Kulis; T Darrell"}, {"ref_id": "b15", "title": "Object, scene and actions: Combining multiple features for human action recognition", "journal": "", "year": "2010", "authors": "N Ikizler-Cinbis; S Sclaroff"}, {"ref_id": "b16", "title": "Web-based classifiers for human action recognition", "journal": "T-MM", "year": "2012", "authors": "N Ikizler-Cinbis; S Sclaroff"}, {"ref_id": "b17", "title": "Consumer video understanding: A benchmark database and an evaluation of human and machine performance", "journal": "", "year": "2011", "authors": "Y.-G Jiang; G Ye; S.-F Chang; D Ellis; A C Loui"}, {"ref_id": "b18", "title": "The cutting plane method for solving convex programs", "journal": "SIAM Journal on Applied Mathematics", "year": "1960", "authors": "J E Kelley"}, {"ref_id": "b19", "title": "What you saw is not what you get: Domain adaptation using asymmetric kernel transforms", "journal": "", "year": "2011", "authors": "B Kulis; K Saenko; T Darrell"}, {"ref_id": "b20", "title": "Space-time interest points", "journal": "", "year": "2003", "authors": "I Laptev; T Lindeberg"}, {"ref_id": "b21", "title": "Text-based image retrieval using progressive multi-instance learning", "journal": "", "year": "2011", "authors": "W Li; L Duan; D Xu; I W Tsang"}, {"ref_id": "b22", "title": "Tighter and convex maximum margin clustering", "journal": "", "year": "2009", "authors": "Y.-F Li; I W Tsang; J T ; .-Y Kwok; Z.-H Zhou"}, {"ref_id": "b23", "title": "Recognizing realistic actions from videos \"in the wild", "journal": "", "year": "2009", "authors": "J Liu; J Luo; M Shah"}, {"ref_id": "b24", "title": "Disctinctive image features from scale-invariance keypoint", "journal": "IJCV", "year": "2004", "authors": "D G Lowe"}, {"ref_id": "b25", "title": "Adapting visual category models to new domains", "journal": "", "year": "2010", "authors": "K Saenko; B Kulis; M Fritz; T Darrell"}, {"ref_id": "b26", "title": "An empirical analysis of domain adaptation algorithms for genomic sequence analysis", "journal": "", "year": "2009", "authors": "G Schweikert; C Widmer; B Sch\u00f6lkopf; G R\u00e4tsch"}, {"ref_id": "b27", "title": "Healing sample selection bias by source classifier selection", "journal": "", "year": "2011", "authors": "C.-W Seah; I W Tsang; Y.-S Ong"}, {"ref_id": "b28", "title": "composite kernel learning", "journal": "", "year": "2010", "authors": "M Szafranski; Y Grandvalet; A Rakotomamonjy"}, {"ref_id": "b29", "title": "Action recognition by dense trajectories", "journal": "", "year": "2011", "authors": "H Wang; A Kl\u00e4ser; C Schmid; C.-L Liu"}, {"ref_id": "b30", "title": "Soft margin multiple kernel learning", "journal": "T-NNLS", "year": "2013", "authors": "X Xu; I W Tsang; D Xu"}, {"ref_id": "b31", "title": "Cross-domain video concept detection using adaptive svms", "journal": "", "year": "2007", "authors": "J Yang; R Yan; A Hauptmann"}, {"ref_id": "b32", "title": "Multi-view transfer learning with a large margin approach", "journal": "", "year": "2011", "authors": "D Zhang; J He; Y Liu; L Si; R D Lawrence"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 .2Figure 2. Illustration of our multi-domain adaptation setting with heterogeneous sources, which consists of the single view source data and multi-view target data.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Before solving the optimization problem in (3), let us define \u03a6 s = [\u03c6 s (x s 1 ), . . . , \u03c6 s (x s ns )], \u03a6 [s] T = [\u03c6 s (z [s] 1 ), . . . , \u03c6 s (z [s] n T )] as the nonlinear mapping function for the data in the s-th source domain and the target domain in the s-th view respectively, and denote f s = [f s (x s 1 ), . . . , f s (x s ns )] and f [s] T = [f s (z [s] 1 ), . . . , f s (z [s] n T )] as the decision values after using the pre-learnt source classifiers f s (x), s = 1, . . . , S. Moreover, let us denote h s as the dimension of \u03c6 s (x s ) and N (p, q) = q s=p n s as the total number of samples from the p-th source domain to the q-th source domain (q \u2265 p). Base on \u03a6 s and \u03a6 [s]", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": ")Note when s = 1 (resp. s = S), O hs\u00d7N (1,s\u22121) and 0 N (1,s\u22121) (resp. O hs\u00d7N (s+1,S) and 0 N (s+1,S) ) in (", "figure_data": ""}, {"figure_label": "21313", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "2 , ( 13 ) 1 repeat 3 :21313Finding the most violated y o . At each iteration in Algorithm 1, when D and \u03b1 are fixed after Step 3, we find the most violated y o by solving the following optimization problem for each s: max y o \u2208Y \u03b1 K [s] y o y o \u03b1 = max y o \u2208Y U [s] \u03b1 y o 2 Algorithm Cutting-plane algorithm for MDA-HS 1: Initialize y 1 based on the outputs from the source classifiers and set o = 1, Y o = {y 1 } 2: Update \u03b1 and D in the group-based MKL problem (10) with Y = Y o by using Algorithm 2 4:", "figure_data": ""}, {"figure_label": "23", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Algorithm 2 : repeat 3 :23The algorithm of group-based MKL 1: Initialize D 1 by using uniform values such that D 1 2,1 = 1 and set \u03c4 = 1 2With fixed Y, update \u03b1 by solving the standard SVM problem with D \u03c4 in (10) 4:", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 3 .3Figure3. Illustration of three learnt weights of source domains for the event \"sports\". We show the learnt weights by using MDA-HS and the per-event APs of three SVMs with each trained using the training data from one source domain.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "", "figure_data": "d\u011e\u01c6\u019a\u01b5\u0102\u016f \u018b\u01b5\u011e\u018c\u01c7\u0357 \u035e\u0190\u0189\u017d\u018c\u019a\u0190\u035f'\u017d\u017d\u0150\u016f\u011e \u015d\u0175\u0102\u0150\u011e\u0190\u015d\u0176\u0150 \u015d\u0175\u0102\u0150\u011e\u0190z\u017d\u01b5d\u01b5\u010f\u011e |\u015d\u011a\u011e\u017d\u0190Z\u011e\u016f\u011e|\u0102\u0176\u019a \u015d\u0175\u0102\u0150\u011e\u0190Z\u011e\u016f\u011e|\u0102\u0176\u019a \u015d\u0175\u0102\u0150\u011e\u0190Z\u011e\u016f\u011e|\u0102\u0176\u019a |\u015d\u011a\u011e\u017d\u0190\u0359\u0359\u0359/\u018c\u018c\u011e\u016f\u011e|\u0102\u0176\u019a \u015d\u0175\u0102\u0150\u011e\u0190/\u018c\u018c\u011e\u016f\u011e|\u0102\u0176\u019a \u015d\u0175\u0102\u0150\u011e\u0190/\u018c\u018c\u011e\u016f\u011e|\u0102\u0176\u019a |\u015d\u011a\u011e\u017d\u0190\u0359\u0359\u0359\u03ee \u0128\u011e\u0102\u019a\u01b5\u018c\u011e\u0190\u03ee \u0128\u011e\u0102\u019a\u01b5\u018c\u011e\u0190\u03ef \u0128\u011e\u0102\u019a\u01b5\u018c\u011e\u0190 \u0128\u018c\u017d\u0175 |\u015d\u011a\u011e\u017d\u0190\u03ee \u0128\u011e\u0102\u019a\u01b5\u018c\u011e\u0190\u03ee \u0128\u011e\u0102\u019a\u01b5\u018c\u011e\u0190\u03ef \u0128\u011e\u0102\u019a\u01b5\u018c\u011e\u0190K\u01b5\u019a\u0189\u01b5\u019a\u0128\u018c\u017d\u0175 \u016c\u011e\u01c7\u0128\u018c\u0102\u0175\u011e\u0190\u0128\u018c\u017d\u0175 \u016c\u011e\u01c7\u0128\u018c\u0102\u0175\u011e\u0190\u0128\u018c\u017d\u0175 |\u015d\u011a\u011e\u017d\u0190d\u0102\u018c\u0150\u011e\u019a \u0110\u016f\u0102\u0190\u0190\u015d\u0128\u015d\u011e\u018c\u0359 h\u0176\u016f\u0102\u010f\u011e\u016f\u011e\u011a \u0110\u017d\u0176\u0190\u01b5\u0175\u011e\u018c |\u015d\u011a\u011e\u017d\u0190 \u015d\u0176 \u019a\u015a\u011e s\u015d\u011a\u011e\u017d |\u011e\u0176\u019a Z\u011e\u0110\u017d\u0150\u0176\u015d\u019a\u015d\u017d\u0176Figure 1. Overview of our proposed framework for visual even-t recognition in consumer videos. The source domain data con-tain both web images from Google/Bing and web videos from Y-ouTube. The target domain contains unlabeled consumer videos."}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "MAP (%) of all methods on the Kodak and CCV datasets.", "figure_data": "MethodKodakCCVSVM A44.8040.84CPMDA [5]25.7230.89DASVM [2]43.4941.55DAM [10]44.2138.56DSM [9]46.2143.44GFK [13]26.7634.38MMTLL [28]44.0935.98MDA-HS sim231.2230.74MDA-HS sim144.6842.38MDA-HS49.6144.521) SVM A outperforms the existing domain adaptationmethods GFK, CPMDA, DAM and MMTLL. Moreover,"}], "formulas": [{"formula_id": "formula_0", "formula_text": "[s]", "formula_coordinates": [3.0, 68.04, 528.93, 8.27, 6.97]}, {"formula_id": "formula_1", "formula_text": "P i = P j , P [i] T = P [j] T and P i = P [i]", "formula_coordinates": [3.0, 81.39, 578.35, 134.28, 15.08]}, {"formula_id": "formula_2", "formula_text": "f T (z) = S s=1 d s w s \u03c6 s (z [s] ),(1)", "formula_coordinates": [3.0, 367.09, 385.58, 183.56, 30.78]}, {"formula_id": "formula_3", "formula_text": "S s=1 d s w s \u2212 \u03b3 s u s 2 2 , (2", "formula_coordinates": [3.0, 377.46, 679.95, 169.31, 30.78]}, {"formula_id": "formula_4", "formula_text": ")", "formula_coordinates": [3.0, 546.77, 690.28, 3.87, 9.38]}, {"formula_id": "formula_5", "formula_text": "Note that d = [d 1 , . . . , d S ]", "formula_coordinates": [4.0, 72.25, 167.37, 107.38, 10.72]}, {"formula_id": "formula_6", "formula_text": "min d\u2208D,yT min ws,\u03b3s, \u03c1,\u03be s i ,\u03be T i 1 2 S s=1 d s w s \u2212 \u03b3 s u s 2 2 + \u03b8\u03b3 2 s \u2212 \u03c1 + 1 2 C S S s=1 ns i=1 \u03be s2 i +C T n T i=1 \u03be T 2 i , (3) s.t. y T i \u2208 {\u00b11}, y T i S s=1 d s w s \u03c6 s (z [s] i ) \u2265 \u03c1\u2212\u03be T i ,(4) y s i d s w s \u03c6 s (x s i ) \u2265 \u03c1 \u2212 \u03be s i , s = 1, ..., S, (5)", "formula_coordinates": [4.0, 69.9, 259.87, 226.64, 119.88]}, {"formula_id": "formula_7", "formula_text": "d | d 2 2 = 1, d \u2265 0 S is the domain of d, y T = [y T 1 , ..., y T nT ]", "formula_coordinates": [4.0, 60.3, 399.94, 236.22, 25.01]}, {"formula_id": "formula_8", "formula_text": "\u03a6 [s] = O hs\u00d7N (1,s\u22121) , \u03a6 s , O hs\u00d7N (s+1,S) , \u03a6 [s] T . (6", "formula_coordinates": [4.0, 322.21, 213.3, 224.1, 15.07]}, {"formula_id": "formula_9", "formula_text": ")", "formula_coordinates": [4.0, 546.31, 216.63, 3.87, 9.38]}, {"formula_id": "formula_10", "formula_text": "f [s] = 0 N (1,s\u22121) , f s , 0 N (s+1,S) , f [s] T . (7", "formula_coordinates": [4.0, 341.74, 263.88, 204.58, 15.07]}, {"formula_id": "formula_11", "formula_text": "min d\u2208D, y\u2208Y max \u03b1\u2208A \u2212 1 2 \u03b1 S s=1 d sK [s]", "formula_coordinates": [4.0, 316.89, 463.87, 146.29, 30.77]}, {"formula_id": "formula_12", "formula_text": "Y = {y|y = [y 1 , . . . , y S , y T ] , y T \u2208 {\u22121, 1} nT } (note y s = [y s 1 , . . . ,", "formula_coordinates": [4.0, 313.95, 586.97, 236.23, 24.74]}, {"formula_id": "formula_13", "formula_text": "I = diag 1 n1 /C S , . . . , 1 nS /C S , 1 nT /C T", "formula_coordinates": [4.0, 313.95, 638.87, 178.54, 11.67]}, {"formula_id": "formula_14", "formula_text": "K [s] = K [s] + 1 \u03b8 f [s] f [s] ,(9)", "formula_coordinates": [4.0, 372.07, 692.73, 178.12, 23.53]}, {"formula_id": "formula_15", "formula_text": "min D max \u03b1\u2208A \u2212 1 2 \u03b1 \u239b \u239d S s=1 o: y o \u2208Y D so Q so +\u0128 \u239e \u23a0 \u03b1, (10", "formula_coordinates": [5.0, 80.0, 205.71, 212.41, 41.14]}, {"formula_id": "formula_16", "formula_text": ")", "formula_coordinates": [5.0, 292.41, 226.15, 4.15, 9.38]}, {"formula_id": "formula_17", "formula_text": "s.t. D 2,1 = 1, D so \u2265 0 \u2200s , \u2200o,", "formula_coordinates": [5.0, 98.03, 252.64, 153.38, 10.7]}, {"formula_id": "formula_18", "formula_text": "D 2,1 = |Y| o=1 S s=1 D 2 so is the mixed L 2,1 norm.", "formula_coordinates": [5.0, 70.84, 311.58, 225.7, 29.26]}, {"formula_id": "formula_19", "formula_text": "min d,\u03bc max \u03b1\u2208A \u2212 1 2 \u03b1 \u239b \u239d S s=1 o:y o \u2208Y d s \u03bc oK [s] y o y o +\u0128 \u239e \u23a0 \u03b1, (11) s.t. d 2 2 = 1, d \u2265 0, \u03bc 1 = 1, \u03bc \u2265 0, (12", "formula_coordinates": [5.0, 65.59, 383.57, 230.96, 59.74]}, {"formula_id": "formula_20", "formula_text": ")", "formula_coordinates": [5.0, 292.41, 432.03, 4.15, 9.38]}, {"formula_id": "formula_21", "formula_text": "Set Y o+1 = Y o \u222a {y o+1 } 6: o \u2190 o + 1 7:", "formula_coordinates": [5.0, 320.32, 172.41, 125.3, 34.33]}, {"formula_id": "formula_22", "formula_text": "whereK [s] = U [s] U", "formula_coordinates": [5.0, 314.56, 234.07, 83.91, 11.33]}, {"formula_id": "formula_23", "formula_text": "max y o \u2208Y U [s] \u03b1 y o \u221e = max j=1,...,n max y o \u2208Y n i=1 \u03b1 i y o i U [s] ij , (14", "formula_coordinates": [5.0, 315.43, 287.86, 231.2, 30.9]}, {"formula_id": "formula_24", "formula_text": ")", "formula_coordinates": [5.0, 546.63, 298.18, 4.15, 9.38]}, {"formula_id": "formula_25", "formula_text": "min D\u2208M 1 2 S s=1 |Y| o=1 v so 2 2 D so , (15", "formula_coordinates": [5.0, 374.44, 515.06, 172.2, 31.91]}, {"formula_id": "formula_26", "formula_text": ")", "formula_coordinates": [5.0, 546.64, 526.52, 4.15, 9.38]}, {"formula_id": "formula_27", "formula_text": "M = D D 2,1 = 1, D so \u2265 0 \u2200s, o is the do- main of D, v so 2 = D so \u221a \u03b1 Q so \u03b1, \u2200s, o.", "formula_coordinates": [5.0, 314.56, 558.4, 236.23, 22.66]}, {"formula_id": "formula_28", "formula_text": "D so = v so 2/3 2 S l=1 v lo 4/3 2 1/4 |Y| o=1 S l=1 v lo 4/3 2 3/4 . (16", "formula_coordinates": [5.0, 346.24, 598.84, 200.4, 42.44]}, {"formula_id": "formula_29", "formula_text": ")", "formula_coordinates": [5.0, 546.64, 616.34, 4.15, 9.38]}, {"formula_id": "formula_30", "formula_text": "f T (z) = S s=1 \u03b2 s \u03a6 [s] \u03c6 s (z [s] ) + 1 \u03b8 f [s] f s (z [s] ) ,", "formula_coordinates": [6.0, 68.08, 233.54, 200.74, 30.78]}, {"formula_id": "formula_31", "formula_text": "\u03b2 s = \u03b1 |Y| o=1 D so y o .", "formula_coordinates": [6.0, 87.12, 273.49, 112.02, 14.97]}, {"formula_id": "formula_32", "formula_text": "x i , x j ) = \u03c6 s (x i ) \u03c6 s (x j ) = exp \u2212 1 \u03bd x i \u2212 x j 2 2", "formula_coordinates": [7.0, 60.32, 518.78, 236.22, 24.6]}], "doi": "10.1109/CVPR.2013.344"}