{"title": "Knowledge Transfer in Incremental Learning for Multilingual Neural Machine Translation", "authors": "Kaiyu Huang; Peng Li; Jin Ma; Ting Yao; Yang Liu", "pub_date": "", "abstract": "In the real-world scenario, a longstanding goal of multilingual neural machine translation (MNMT) is that a single model can incrementally adapt to new language pairs without accessing previous training data. In this scenario, previous studies concentrate on overcoming catastrophic forgetting while lacking encouragement to learn new knowledge from incremental language pairs, especially when the incremental language is not related to the set of original languages. To better acquire new knowledge, we propose a knowledge transfer method that can efficiently adapt original MNMT models to diverse incremental language pairs. The method flexibly introduces the knowledge from an external model into original models, which encourages the models to learn new language pairs, completing the procedure of knowledge transfer. Moreover, all original parameters are frozen to ensure that translation qualities on original language pairs are not degraded. Experimental results show that our method can learn new knowledge from diverse language pairs incrementally meanwhile maintaining performance on original language pairs, outperforming various strong baselines in incremental learning for MNMT. 1   ", "sections": [{"heading": "Introduction", "text": "Multilingual neural machine translation (MNMT) aims at handling multiple translation directions in a single model and achieves great success in recent years (Wenzek et al., 2021;Cheng et al., 2022). However, the powerful MNMT models need to be retrained from scratch  using a mixture of original and incremental training data when new language pairs arrive (Tang et al., 2020). Considering that the original training data of MNMT models is often large-scale (Fan et al., 2021;Costa-juss\u00e0 et al., 2022), and thus the method that utilizes original data to train incrementally is time-consuming and cumbersome (Ebrahimi and Kann, 2021). Therefore, a practical scenario is that these models can continually support new language pairs while preserving the previously learned knowledge without accessing previous training data, which belongs to incremental learning, and has drawn much attention recently (Dabre et al., 2020;Gu et al., 2022;Zhao et al., 2022). In this scenario, existing studies attempt to overcome the issue of catastrophic forgetting (French, 1993) on original language pairs, such as replaybased methods (Garcia et al., 2021; and regularization-based methods (Huang et al., 2022;Zhao et al., 2022). However, the methods primarily focus on balancing performance between old and new translation directions and use only incremental data to acquire new knowledge, restricting the development of new language pairs (Escolano et al., 2021;Ke and Liu, 2022). As shown in Table 1, prior incremental learning methods cannot achieve comparable performance on new translation directions, compared with training a bilingual translation model from scratch. Therefore, is it possible to leverage external knowledge without increasing the amount of incremental training data to facilitate the learning procedure of new language adaptation?\nFortunately, the development of Transformerbased language models unveils that Feed-Forward Networks (FFN) might be a core component that stores the knowledge (Geva et al., 2021;Dai et al., 2022;Geva et al., 2022;V\u00e1zquez et al., 2022). In these efforts, external knowledge is injected into FFN layers to enhance the performance of pre-trained language models. More importantly, it opens the door to leveraging the knowledge from neural models in adapting MNMT models to incremental language pairs. In this work, considering the knowledge in neural networks, we propose a knowledge transfer (KT) method that can efficiently adapt MNMT models to diverse incremental language pairs. First, we convert incremental training data into continuous representation by additional parameters, forming pluggable modules. Then the pluggable modules can be flexibly introduced in the embedding layer and FFN layers of original MNMT models, respectively. The two stages are regarded as a process of knowledge transfer and equip original models with knowledge of unseen languages before adaptation, alleviating the representation gap between original models and introduced parameters. And the knowledge transfer method can further provide better optimization than training from scratch for the introduced parameters. Moreover, except for the pluggable modules, all the parameters of the original model are frozen. Therefore, our architecture can also retain previously learned knowledge from the original translation model to completely maintain the translation qualities on original language pairs. To sum up, our contributions are as follows:\n\u2022 We propose a knowledge transfer method with pluggable modules to acquire more knowledge of new languages, which achieves competitive translation qualities on incremental language pairs.\n\u2022 Our architecture can efficiently adapt to diverse language pairs in incremental learning and naturally retain the performance on origi-nal language pairs when the original training data is not available.\n\u2022 Experiments show that our method can learn knowledge from the other large-scale translation models for adapting original models with different sizes to new language pairs.", "publication_ref": ["b11", "b4", "b39", "b11", "b9", "b6", "b19", "b46", "b14", "b15", "b22", "b46", "b10", "b25", "b17", "b7", "b16", "b43"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Related Work", "text": "Replay-Based Methods. The first branch of works utilizes previous training data or create pseudo data that is essentially a replay on old tasks (de Masson D'Autume et al., 2019;Kanwatchara et al., 2021). Specifically, previous data sometimes cannot be accessed due to data protection and security (Feyisetan et al., 2020;Qu et al., 2021). In this scenario, Sun et al. (2019) replay pseudo samples of previous tasks, which can avoid forgetting previously learned knowledge.\nHowever, the pseudo data with noise significantly hurts the performance for both old and new tasks, and the data generation procedure requires additional time costs, restricting the efficiency of incremental learning Garcia et al., 2021). In contrast to these methods, our approach does not require extra data and is more flexible in the real-world scenario.\nRegularization-Based Methods. The second branch of works introduces additional penalty terms to the learning objective on the parameters, alleviating the issue of catastrophic forgetting (Kirkpatrick et al., 2017;Thompson et al., 2019;Castellucci et al., 2021;Gu et al., 2022).\nIn particular, Shao and Feng (2022) propose a complementary online knowledge distillation method that utilizes previous models (teachers) to supplement current model (student) training. In contrast to these methods, our architecture can naturally avoid forgetting previously learned knowledge and retain the performance of old tasks. It allows our method to focus solely on learning new knowledge better instead of preserving old knowledge.\nParameter-Isolation Based Methods. The third branch of works introduces extra parameters to support new tasks and freeze all original parameters to completely retain the performance on previous tasks Madotto et al., 2021;Zhu et al., 2021). However, the methods only utilize incremental training data to optimize the additional parameters which are randomly initialized (Escolano et al., 2021;He et al., 2021),  hindering old models from learning knowledge of incremental languages (Dabre et al., 2020;Ke and Liu, 2022). Chalkidis et al. (2021) combine pseudo data with the prompt-tuning method to alleviate this issue for multilingual tasks. Our method attempts to exploit the potentiality of incremental training data to acquire new knowledge via knowledge transfer while not leveraging extra data, compared with existing methods.\nEN", "publication_ref": ["b8", "b24", "b13", "b35", "b38", "b15", "b27", "b40", "b2", "b19", "b36", "b31", "b47", "b10", "b20", "b6", "b25"], "figure_ref": [], "table_ref": []}, {"heading": "Method", "text": "In this work, we aim to completely maintain the performance of previous translation tasks without original training data. As shown in Figure 1, we introduce additional components for new language pairs and adopt a strategy that does not disturb the parameters of the original model. We hope to minimize the impact on the original model during the incremental learning process. As a result, we exclusively concentrate on how to handle the situation of learning new language pairs. Furthermore, the additional components are transferred from parameters in another pre-trained translation model, in a similar way to a pluggable module, rather than randomly initialized, as shown in Figure 2. It can also reduce the cost of learning new language pairs during the training stage, enhancing the practicability and efficiency of incremental learning methods in the real-world scenario.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Task Definition", "text": "An ideal requirement is that original MNMT models can be continually updated to support new language pairs while retaining translation qualities on original language pairs without accessing previous training data, as shown in Figure 1.\nFormally, an MNMT model is trained on initially selecting a set of available parallel data D = {D 1 , ..., D i , ..., D N } which covers N languages, and D i represents the original parallel training corpus on the i-th language pair. The training objective of the initial MNMT model is to maximize the log-likelihood L:\nL D (\u03b8) = D i \u2208D (x,y)\u2208D i log p(y|x; \u03b8) (1)\nwhere \u03b8 represents the trainable parameters of MNMT models. The source sentence is denoted as x, while the target sentence is denoted as y. In order to specify the source and target languages, two language tokens are added at the beginning of each source and target sentence, respectively. Incremental learning is updating the original MNMT model on an updated set of parallel data D (U ) = {D 1 , ..., D N , ..., D M } which covers M languages, and N > M . A dilemma, though, is that the original training data D is often unavailable due to data security. Thus, we can only utilize the new data D \u2032 = {D N +1 , ..., D j , ..., D M } to incrementally train the original MNMT model, and D j represents the incremental parallel training corpus on the j-th language pair. The optimization objective in incremental learning is given by:\nL D \u2032 (\u03b8) = D j \u2208D \u2032 (x,y)\u2208D j log p(y|x; \u03b8) (2)\nAs a result, the number of language pairs that the MNMT model support increases from N to M .", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Knowledge Transfer via Pluggable Modules", "text": "Based Figure 2: Illustration of our architecture. We extend two parts of space in the embedding layer and the FFN layers, respectively. These spaces are injected with pluggable modules that are pre-trained by another translation model. During the training stage, the parameters of the original model are frozen and the pluggable modules are trainable. set of original languages, a certain proportion of the out-of-vocabulary (OOV) tokens with unclear semantics will occur due to different character sets between the original and incremental languages, which hinders performance on new language pairs . However, the external model is not troubled by this situation and covers sufficient tokens for the incremental language pairs. Therefore, we expand an extra space in the embedding layer and concatenate the embeddings of non-overlap tokens between the original model and the external model, bridging the representation gap through vocabulary adaptation.\nFeed-Forward Adaptation. On the other hand, FFN layers can be seen as key-value memories, which has previously been investigated by (Sukhbaatar et al., 2019). Each FFN layer consists of two linear networks with a non-linearity activation function and is given by:\nFFN(H) = f (H \u2022 K \u22a4 ) \u2022 V (3)\nwhere K, V \u2208 R d ffn \u00d7d model are parameter matrices, d ffn is the dimension of the FFN, H is the input hidden state of FFN layers and the dimension is d model , f represents the non-linearity activation function, e.g., GELU and ReLU.\nThe first linear network is regarded as the keys and activates a set of intermediate neurons. The second linear network integrates the corresponding value matrices by weighted sum, using the activated neurons as weights. The FFN layers store knowledge in the key-value memories manner. Thus, we leverage the continuous representation in the FFN layers of the external model that stores useful language knowledge and transfer the knowledge into the FFN layers of the original model, forming a type of pluggable module. We combine the output of the original FFN layers and the injected pluggable modules to share linguistic knowledge, alleviating the representation gap through Feed-Forward layers adaptation. The fusion FFN output H (f) is given by:\nH (f) = FFN original (H) + FFN external (H) (4)", "publication_ref": ["b37"], "figure_ref": [], "table_ref": []}, {"heading": "Training and Inference", "text": "During the training stage, previously learned knowledge can be naturally preserved with a frozen training strategy, which can avoid the issue of catastrophic forgetting. The training procedure of our method is divided into two stages, as shown in Figure 2. \nL D \u2032 (\u03b8) = D j \u2208D \u2032 (x,y)\u2208D j log p(y|x;\u03b8) (5)\nwhere\u03b8 represents the trainable parameters of the external neural models. We only retain the parameters in the embedding layer (\u03b8 e ) and FFN layers (\u03b8 f ) of the external model as the pluggable modules for the next training stage.\nStage 2: Pluggable Module Tuning. Directly transferring the additional parameters limits the MNMT model capacity, especially for the language pairs with sufficient data. Therefore, we further train the pluggable modules in the second stage:\nL D \u2032 (\u03b8 e ,\u03b8 f ) = D j \u2208D \u2032 (x,y)\u2208D j log p(y|x;\u03b8 e ,\u03b8 f )(6\n) where\u03b8 e and\u03b8 f represent the trainable parameters of pluggable modules in the embedding layer and FFN layers, respectively.\nInference. For the inference stage, the original translation directions follow the original model without any pluggable modules while the incremental translation directions require the concatenation of the original model and pluggable modules.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments 4.1 Datasets", "text": "To ensure the reliability of the experiments, the original MNMT model is implemented on a multilingual machine translation dataset 2 (WMT-7) that covers seven languages (Farhad et al., 2021). And we provide four incremental languages considered for incremental adaptation 3 . An extensive description and comprehensive information regarding the datasets for all languages can be found in Appendix A. All training data are sourced from the WMT (Workshop on Machine Translation) and FLoRes datasets, ensuring reliable quality.\nLanguage Choice. As contrasted to previous studies for incrementally adapting translation models to new languages, we further provide a comprehensive language setting. Previous works often investigate the situation of the related languages which are similar language families and scripts to the original languages. In our setting, the incremental languages have distinct scripts and belong to several language families, which leads to a serious language representation gap. Please refer to Appendix A.2 for more details of language consideration in our setting.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Implementation Details", "text": "Baselines. We implement a vanilla Transformer for original languages as the initial model which is trained on multiple parallel data jointly (Johnson et al., 2017). And we compare the proposed method with different architectures for adapting the original model to new language pairs. All methods utilize the preprocessing script of a shared BPE model with 32k tokens based on the Sentencepiece library 4 . The baselines can be listed as follows:\nFrom-scratch (Johnson et al., 2017): A vanilla Transformer is trained from scratch on the incremental languages with the multilingual training strategy. Note that the models do not support the original translation directions.\nAdapter : We follow previous adapter architectures and introduce extra parameters in each FFN layer of the original MNMT model. All original parameters are frozen and only the adapters are trainable.\nExtension (Lakew et al., 2018): On the basis of the adapter architecture, we extend the original vocabulary (V P ) for new languages adaptation. Initially, a supplementary vocabulary (V Q ) is created using the standard Byte-Pair Encoding (BPE) procedure from the incremental training data. Subsequently, V P and V Q are combined to form a unified vocabulary V, which is defined as V = V P \u222a V Q . The embeddings of the original models are expanded to match the size of the complete vocabulary (V), and the additional embeddings are initialized using a Gaussian distribution.\nSerial/Parallel (Zhu et al., 2021): We follow Zhu et al. (2021) to introduce adapters in the serial or parallel connection manner. Our pluggable modules in the FFN layers can also be converted into a serial manner.\nTraining Setup. We implement all models based on the open-source toolkit fairseq 5 (Ott et al., 2019). For a fair comparison, we employ the same configuration of Transformer-Big (Vaswani et al., 2017)    Evaluation. We report the detokenized casesensitive BLEU of models by the SacreBLEU evaluation script (Post, 2018) 6 . We show the training time of each method in terms of kiloseconds and use the beam search decoding algorithm with a beam size of 5 and a length penalty of 1.0.", "publication_ref": ["b23", "b23", "b47", "b47", "b32", "b42"], "figure_ref": [], "table_ref": []}, {"heading": "Main Results", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Adding A Single Language", "text": "As shown in  The Adapter methods are more vulnerable to adapting original models to some incremental language pairs, e.g., 0.12 BLEU scores on en\u2192bn and 16.94 BLEU scores on bn\u2192en. Because the methods with an unaltered vocabulary result in the sentence being broken up into semantically meaningless OOV tokens. Although the Extension methods can alleviate the issue of OOV tokens and fragmentary semantics by rebuilding embedding layers, the  extended parameters are still hard to optimize. The knowledge transfer method can further guide additional parameters to achieve greater improvements on these language pairs. Considering the different introduced manners of pluggable modules, based on the baselines, parallel modules tend to be weaker than serials. It demonstrates that parallel architecture is more difficult to learn new knowledge from limited training data. And the results show that the knowledge transfer method mitigates this issue and explores the potential of parallel architectures, achieving obvious improvement on all eight translation directions, even outperforming the model training from scratch.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Adding Multiple Languages Simultaneously", "text": "As shown in Table 3, we examine the translation qualities in incremental learning when eight new language pairs arrive simultaneously. The results show that our proposed method can also achieve better performance compared with the baselines. Notably, in the low resource scenario (ro and uk), our method of adding multiple languages obtains better performance compared with adding a single language.\nBesides, adding multiple languages simultaneously in incremental training makes more training samples available and it facilitates the optimization of challenging pluggable modules in a parallel manner. In this setting, the parallel pluggable modules of all methods demonstrate better performance than the serial. Moreover, the situation of incremental language pairs that are difficult to learn is still alive with the Adapter. It even shows more severe degeneration on the other incremental languages (17.24 BLEU on en\u2192de of adding a sin-gle language while 6.56 BLEU of adding multiple languages simultaneously). However, our method does not significantly been disturbed by different conditions in incremental learning, which exhibits good stability, as shown in Table 2 and Table 3.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_6", "tab_5"]}, {"heading": "Degeneration in Incremental Learning.", "text": "As shown in Table 4, to demonstrate the reliability and effectiveness, we investigate the degeneration on the original translation directions, compared with various outstanding continual learning methods. The results demonstrate that our method achieves competitive performance on the incremental translation directions and even outperforms the fine-tuning strategy (up to +0.57/+0.40 for ro\u2192en and en\u2192ro respectively). Please refer to Appendix 4.6 and B.2 for more details of the original models and all baselines.\nBesides, the results also show that prior replaybased and regularization-based methods still suffer from pronounced degeneration on the original translation directions without the original data. Although no degradation has occurred using Prompt and Prefix, they are vulnerable to learning new knowledge from updated training samples incrementally. More importantly, considering the reliability of the comparison, we have only selected the translation directions between Romanian and English. Because previous methods cannot obtain comparable results when the incremental languages are not related to the set of original languages.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_8"]}, {"heading": "Results on Pre-trained Models", "text": "As shown in Table 5, we leverage pre-trained M2M-100 models (Fan et al., 2021)    edge transfer methods. Knowledge distillation (KD) (Hinton et al., 2015) is a widely used technique to transfer knowledge between models. The results show that only utilizing KD cannot achieve comparable performance for incremental language adaptation. However, KD can be arbitrarily integrated into our method (KT) and further facilitate the procedure of knowledge transfer. The combination of KD and KT achieves better translation qualities than only using one alone based on all model settings. Besides, both KD and KT are better at learning knowledge from the large pre-trained models. It proves that the large pre-trained model contains more useful knowledge. And we find that the size between models also determines the performance on incremental translation directions. The small M2M-100 model (0.4B) is beneficial for the same size original model (0.4B) but is insufficient to support the large original model (1.2B). In contrast, the large M2M-100 model (1.2B) plays a positive role in both small and large original models by knowledge transfer. However, the small original model (0.4B) limits learning sufficient knowledge from the large M2M-100 model according to the comparison between No.6 and No.12, as shown in Table 5.", "publication_ref": ["b11", "b21"], "figure_ref": [], "table_ref": ["tab_10", "tab_10"]}, {"heading": "Ablation Studies", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Effects on Transfer Areas", "text": "As shown in Table 6, we further investigate the effectiveness of our method in different transfer areas. The results demonstrate that our method can help each pluggable module to be better optimized separately and achieves better performance when both two pluggable modules are injected through knowledge transfer for all incremental languages. Specifically, the method improves translation qualities related to Romanian and Ukrainian when it affects the pluggable module in the embedding layer. On the contrary, it is more effective to transfer the knowledge for the pluggable modules in the FFN layers on translation directions related to German and Bengali, according to the comparison between 2 and 3. A possible reason is that the resource of different language pairs influences the efficiency of knowledge transfer.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_12"]}, {"heading": "Effects on Pluggable Modules", "text": "Previous parameter-isolation based methods propose various components to introduce additional parameters in the hidden layers (He et al., 2021). As shown in Table 7, inspired by them, we modify the usage of the pluggable modules in the hidden layers and our method is stable on the four translation directions. In particular, we also inject the pluggable modules in the Self-Attention layer. However, the special modification of pluggable modules does not demonstrate effective performance in incremental learning for MNMT.", "publication_ref": ["b20"], "figure_ref": [], "table_ref": ["tab_13"]}, {"heading": "Results on Original Language Pairs", "text": "To demonstrate the validity and reliability of our method, we build two powerful MNMT models as the original models. As shown in Table 8, the original models achieve state-of-the-art performance on all original translation directions, compared with the other powerful MNMT models.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_15"]}, {"heading": "More Comparisons", "text": "Due to space limitation, we provide a more detailed analysis of our method in Appendix C, including the training cost of the incremental learning, the  visualization of sentence representations on all language pairs, and the case study on new language pairs, demonstrating the effectiveness of the knowledge transfer method in incremental learning for new language adaptation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "In this work, we propose a knowledge transfer method in incremental learning for MNMT, which leverages the knowledge from neural models. It can encourage original models to learn new knowledge from updated training data while naturally mitigating the issue of degradation on previous translation directions. Moreover, it is more efficient to utilize the knowledge transfer scheme than introducing randomly initialized parameters in incremental learning. Experimental results demonstrate that the proposed method outperforms several strong baselines in the comprehensive language consideration.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "In this work, we attempt to extend an existing MNMT model to support new language pairs with an acceptable expense. In addition to the advantages, our method has the following limitations:\n(1) Additional introduced parameters. We utilize the parameter-isolation based method to support new language pairs. The total parameters of the MNMT model have been increased by pluggable modules to achieve better performance than prior studies. In the future, we will compress the number of parameters to the same size of original models meanwhile preserve the performance on all translation directions.\n(2) The gap between our scenario and the realworld scenario. Our proposed method is a whitebox service in incremental learning. Thus, we train a powerful MNMT model as the original model instead of directly utilizing existing models from the Internet. And we only consider eight incremental language pairs due to the limitation of computation resources. We try our best to simulate the realworld scenario and we will apply our proposed method for large-scale pre-trained MNMT models (e.g., NLLB 54.5B and M2M 12B) to validate the effectiveness in industrial scenarios.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Dataset Details", "text": "We utilize six language pairs to train the original MNMT model that covers 12 translation directions and 7 languages (WMT-7). All the original training data comes from the recent WMT general translation track. And we conduct eight incremental language pairs in incremental learning from the WMT news translation track and FLoRes. All data follow the license that can be freely used for research purposes (Farhad et al., 2021). The license of FLoRes dataset is CC-BY-SA 4.0. In addition, we follow Fan et al. (2021) to clean the training sample. We introduce the characteristics of different languages to analyze the linguistic diversity, as shown in Table 9. All language pairs are Englishcentric and the statistics of training data are shown in Table 10.", "publication_ref": ["b11"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "A.1 Data Statistics", "text": "As the general setting, all language pairs are divided into three categories in terms of the amount of parallel data, including high resource (>10M), medium resource (1M~10M), and low resource (100k~1M). Specifically, the original language pairs are, High resource: Japanese and Polish; Medium resource: Icelandic and Pashto; Low resource: Hausa and Tamil. And the incremental language pairs are, Medium resource: German and Bengali; Low resource: Ukrainian and Romanian. Note that incremental training data is often a nonhigh resource in the real-world scenario.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Language Consideration", "text": "In this work, we explore a more complex and comprehensive scenario for MNMT in incremental learning, taking into account the diversity of incremental languages. These incremental languages differ from the original languages in terms of their scripts and belong to different language families, which leads to a serious vocabulary and linguistic gap. Inspired by , if the incremental language has a distinct script with the set of original languages, a certain proportion of OOV tokens with unclear semantics will occur between the original and incremental languages and hinder the performance on new language pairs. Moreover, it is important to note that a language family refers to a group of languages that share a common ancestry, known as the proto-language 7 . This concept highlights the historical connections among languages and their evolution over time. Additionally, differences in grammar and word order can be observed across distinct language families 8 . These linguistic variations further contribute to the existing gap between incremental languages, making their translation more challenging.\nIn our setting, the 4 incremental languages include: Bengali, which is not related to any of the original 7 languages, and has a distinct script; Ukrainian, which is related to the original language Polish with the language family Slavic, but has a distinct script with Cyrillic; Romanian, is Romance language that is not related to all the original languages, but has a share script with Latin characters; German, which is similar to the original languages in the language families and scripts. The statistics and details of datasets for original and incremental languages are shown in Table 9.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Model Details", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.1 Training Setup", "text": "We implement Transformer translation models in all our experiments. In particular, the small original model (0.4B) consists of 6 stacked encoder layers, 6 stacked decoder layers, and 16 multiattention heads, followed by the configuration of Transformer-Big (Vaswani et al., 2017). The dimensions of d model and d ffn are 1024 and 4096 respectively. The large original model (1.2B) consists of 24 stacked encoder layers, 24 stacked decoder layers, and 16 multi-attention heads, followed by the configuration of M2M-100 (Fan et al., 2021  dimensions of d model and d ffn are 1024 and 8192 respectively. We use Adam (Kingma and Ba, 2014) and a half-precision training scheme to optimize the parameters of all MNMT models. In addition, we reset the optimizer and learning scheduler in incremental learning and use the temperature-based sampling scheme (Arivazhagan et al., 2019) with a temperature of T = 5 to balance the training data between diverse language pairs. We adopt the early stop (patience is 10) strategy in incremental learning and the batch size is 4096\u00d74 in all training procedures. To eliminate the randomness of the result, we report the mean BLEU scores of the models that are trained in five seeds. All incremental models are trained on 2 NVIDIA A100 GPUs.", "publication_ref": ["b42", "b11", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "B.2 Continual Learning Baselines", "text": "We compare our method with various representative baselines in continual learning. The baselines are as follows:\n\u2022 Replay (Sun et al., 2019): creating pseudo data for the original language pairs and training new language pairs jointly with the pseudo data and incremental training data.\n\u2022 EWC (Kirkpatrick et al., 2017): computing the importance of the parameters with Fisher matrix and employing an additional penalty into the loss function to preserve original knowledge.\n\u2022 Self-KD (Castellucci et al., 2021): utilizing the original models as the teacher model to distill old knowledge.\n\u2022 LFR (Gu et al., 2022): constraining the parameters of original models with low forget-  ting risk regions. We choose the LRF-CM for adapting new language pairs.\n\u2022 Prompt (Chalkidis et al., 2021): prepending prompts to the input embedding in the first layer.\n\u2022 Prefix (Li and Liang, 2021): prepending prefixes to the keys and values of the attention at every layer.", "publication_ref": ["b38", "b27", "b2", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "C More Comparisons", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1 Training Cost", "text": "To further illustrate the efficiency of our method, we investigate the training time compared with the stronger baselines, as shown in Figure 3. The results show that the knowledge transfer method can reduce the training time of incremental learning, which is more efficient and practical than the other methods.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "C.2 Visualization of Multilingual Representations", "text": "As shown in Figure 4, we visualize the sentence representations on xx-to-English translation directions to investigate the representation gap between languages. Due to comparability in one representation space, we need multi-source sentences that represent the same meaning in different languages. We use \"FLoRes\" and reduce the 1024-dim representations to 2-dim with t-SNE (Van der Maaten and Hinton, 2008) for visualization.\nAs Figure 4 shows, the sentence representations using our method are drawn closer than the standard Adapter method (one of the baselines). It demonstrates that our method can well adapt to the new language. Moreover, previous studies have shown that if sentences with similar semantics are closer together in the representation space, it can usually improve the translation performance of zero-shot translation. Experimental results in two translation directions show that our method can achieve better performance for zero-shot translation, which is consistent with our visualization.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.3 Case Study", "text": "We present several translation examples to provide a comprehensive understanding of the knowledge transfer method, as shown in Table 12. The examples demonstrate that our method can effectively adapt original models to new languages especially when the incremental language is not related to the set of original languages. In particular, due to the vocabulary gap, the Adapter method is vulnerable to learning incremental languages that have a distinct script with Latin. Although the Extension alleviates this issue by expanding the embedding layer, the additional parameters are not fully optimized to suffer from the off-target problem for MNMT.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "D Potential Risks of Our Method", "text": "Since our proposed method can increase the unlimited number of translation directions, it is possible for some malicious users to use the MNMT model to provide translation services for politically sensitive languages. For instance, a malicious user may utilize our model to generate hateful or offensive sentences in some politically sensitive languages.\nSource en\u2192bn S1: Widespread looting reportedly continued overnight, as law enforcement officers were not present on Bishkek's streets. S2: Bishkek was described as sinking into a state of \"anarchy\" by one observer, as gangs of people roamed the streets and plundered stores of consumer goods. S3: Several Bishkek residents blamed protesters from the south for the lawlessness.    12: Examples of several baselines and our method on en-to-bn and en-to-uk translation directions for each label based on FLoRes testset. We only highlight some words and fragments to show the representative difference between various methods.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Acknowledgements", "text": "This work is supported by the National Key R&D Program of China (2022ZD0160502) and the National Natural Science Foundation of China (No.  61925601, 62276152, 62236011). We sincerely thank the reviewers for their insightful comments and suggestions to improve the quality of the paper.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "We provide the limitations of our work in the section 'Limitation'.\nA2. Did you discuss any potential risks of your work?\nWe provide the potential risks of our work in Appendix D.\nA3. Do the abstract and introduction summarize the paper's main claims?\nThe paper's main claims are summarized in the section 'Abstract' and the section 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A4. Have you used AI writing assistants when working on this paper?", "text": "Left blank.\nB Did you use or create scientific artifacts?\nWe provide the dataset and open toolkit in the section 4.1, 4.2, and Appendix A.\nB1. Did you cite the creators of artifacts you used?\nWe cite the dataset and open toolkit in the section 4.1, 4.2, and Appendix A.\nB2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nWe discuss the license of dataset in Appendix A.\nB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\nWe discuss the existing artifact was consistent with their intended use n the section 4.2 and Appendix A.\nB4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?\nWe check the details of dataset in Appendix A.\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\nWe provide the details of domains and languages in Appendix A.\nB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\nWe carefully provide the statistics of all data in Appendix A.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C Did you run computational experiments?", "text": "We provide the computational experiments in Appendix B and C.\nC1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?\nWe provide all implementation details and training setup in section 4, Appendix B.1 and Appendix B.2. The section 4.4 also contains the number of parameters in the models used. We report the training cost in Appendix C.1.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Massively multilingual neural machine translation in the wild: Findings and challenges", "journal": "", "year": "2019", "authors": "Naveen Arivazhagan; Ankur Bapna; Orhan Firat; Dmitry Lepikhin; Melvin Johnson; Maxim Krikun; Mia Xu Chen; Yuan Cao; George Foster; Colin Cherry"}, {"ref_id": "b1", "title": "Simple, scalable adaptation for neural machine translation", "journal": "", "year": "2019", "authors": "Ankur Bapna; Orhan Firat"}, {"ref_id": "b2", "title": "Learning to solve nlp tasks in an incremental number of languages", "journal": "Short Papers", "year": "2021", "authors": "Giuseppe Castellucci; Simone Filice; Danilo Croce; Roberto Basili"}, {"ref_id": "b3", "title": "Multieurlex-a multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer", "journal": "", "year": "", "authors": ""}, {"ref_id": "b4", "title": "Multilingual mix: Example interpolation improves multilingual neural machine translation", "journal": "Long Papers", "year": "2022", "authors": "Yong Cheng; Ankur Bapna; Orhan Firat; Yuan Cao; Pidong Wang; Wolfgang Macherey"}, {"ref_id": "b5", "title": "", "journal": "", "year": "", "authors": "James Marta R Costa-Juss\u00e0; Onur Cross; Maha \u00c7elebi; Kenneth Elbayad; Kevin Heafield; Elahe Heffernan; Janice Kalbassi; Daniel Lam; Jean Licht;  Maillard"}, {"ref_id": "b6", "title": "A survey of multilingual neural machine translation", "journal": "ACM Computing Surveys (CSUR)", "year": "2020", "authors": "Raj Dabre; Chenhui Chu; Anoop Kunchukuttan"}, {"ref_id": "b7", "title": "Neural knowledge bank for pretrained transformers", "journal": "", "year": "2022", "authors": "Damai Dai; Wenbin Jiang; Qingxiu Dong; Yajuan Lyu; Qiaoqiao She; Zhifang Sui"}, {"ref_id": "b8", "title": "Episodic memory in lifelong language learning", "journal": "", "year": "2019", "authors": "Sebastian Cyprien De Masson D'autume; Lingpeng Ruder; Dani Kong;  Yogatama"}, {"ref_id": "b9", "title": "How to adapt your pretrained multilingual model to 1600 languages", "journal": "Long Papers", "year": "2021", "authors": "Abteen Ebrahimi; Katharina Kann"}, {"ref_id": "b10", "title": "From bilingual to multilingual neural-based machine translation by incremental training", "journal": "Journal of the Association for Information Science and Technology", "year": "2021", "authors": "Carlos Escolano; Marta R Costa-Juss\u00e0;  Fonollosa"}, {"ref_id": "b11", "title": "Beyond english-centric multilingual machine translation", "journal": "J. Mach. Learn. Res", "year": "2021", "authors": "Angela Fan; Shruti Bhosale; Holger Schwenk; Zhiyi Ma; Ahmed El-Kishky; Siddharth Goyal; Mandeep Baines; Onur Celebi; Guillaume Wenzek; Vishrav Chaudhary"}, {"ref_id": "b12", "title": "Proceedings of the Sixth Conference on Machine Translation", "journal": "", "year": "", "authors": "Akhbardeh Farhad; Arkhangorodsky Arkady; Biesialska Magdalena; Bojar Ond\u0159ej; Chatterjee Rajen; Chaudhary Vishrav; Marta R Costa-Jussa; Espa\u00f1a-Bonet Cristina; Fan Angela; Federmann Christian"}, {"ref_id": "b13", "title": "Privacy-and utility-preserving textual analysis via calibrated multivariate perturbations", "journal": "", "year": "2020", "authors": "Oluwaseyi Feyisetan; Borja Balle; Thomas Drake; Tom Diethe"}, {"ref_id": "b14", "title": "Catastrophic interference in connectionist networks: Can it be predicted, can it be prevented?", "journal": "", "year": "1993", "authors": "Robert French"}, {"ref_id": "b15", "title": "Towards continual learning for multilingual machine translation via vocabulary substitution", "journal": "", "year": "2021", "authors": "Xavier Garcia; Noah Constant; Ankur Parikh; Orhan Firat"}, {"ref_id": "b16", "title": "Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space", "journal": "", "year": "2022", "authors": "Mor Geva; Avi Caciularu; Kevin Ro Wang; Yoav Goldberg"}, {"ref_id": "b17", "title": "Transformer feed-forward layers are key-value memories", "journal": "", "year": "2021", "authors": "Mor Geva; Roei Schuster; Jonathan Berant; Omer Levy"}, {"ref_id": "b18", "title": "The flores-101 evaluation benchmark for low-resource and multilingual machine translation", "journal": "Transactions of the Association for Computational Linguistics", "year": "2022", "authors": "Naman Goyal; Cynthia Gao; Vishrav Chaudhary; Peng-Jen Chen; Guillaume Wenzek; Da Ju; Sanjana Krishnan; Marc'aurelio Ranzato; Francisco Guzman; Angela Fan"}, {"ref_id": "b19", "title": "Continual learning of neural machine translation within low forgetting risk regions", "journal": "", "year": "2022", "authors": "Shuhao Gu; Bojie Hu; Yang Feng"}, {"ref_id": "b20", "title": "Towards a unified view of parameter-efficient transfer learning", "journal": "", "year": "2021", "authors": "Junxian He; Chunting Zhou; Xuezhe Ma; Taylor Berg-Kirkpatrick; Graham Neubig"}, {"ref_id": "b21", "title": "Distilling the knowledge in a neural network", "journal": "", "year": "2015", "authors": "Geoffrey Hinton; Oriol Vinyals; Jeff Dean"}, {"ref_id": "b22", "title": "Omniknight: Multilingual neural machine translation with language-specific selfdistillation", "journal": "", "year": "2022", "authors": "Yichong Huang; Xiaocheng Feng; Xinwei Geng; Bing Qin"}, {"ref_id": "b23", "title": "Google's multilingual neural machine translation system: Enabling zero-shot translation", "journal": "Transactions of the Association for Computational Linguistics", "year": "2017", "authors": "Melvin Johnson; Mike Schuster; V Quoc; Maxim Le; Yonghui Krikun; Zhifeng Wu; Nikhil Chen; Fernanda Thorat; Martin Vi\u00e9gas; Greg Wattenberg;  Corrado"}, {"ref_id": "b24", "title": "Rational lamol: A rationalebased lifelong learning framework", "journal": "Long Papers", "year": "2021", "authors": "Kasidis Kanwatchara; Thanapapas Horsuwan; Piyawat Lertvittayakumjorn"}, {"ref_id": "b25", "title": "Continual learning of natural language processing tasks: A survey", "journal": "", "year": "2022", "authors": "Zixuan Ke; Bing Liu"}, {"ref_id": "b26", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b27", "title": "Overcoming catastrophic forgetting in neural networks", "journal": "", "year": "2017", "authors": "James Kirkpatrick; Razvan Pascanu; Neil Rabinowitz; Joel Veness; Guillaume Desjardins; Andrei A Rusu; Kieran Milan; John Quan; Tiago Ramalho; Agnieszka Grabska-Barwinska"}, {"ref_id": "b28", "title": "Transfer learning in multilingual neural machine translation with dynamic vocabulary", "journal": "", "year": "2018", "authors": "M Surafel; Aliia Lakew; Matteo Erofeeva; Marcello Negri; Marco Federico;  Turchi"}, {"ref_id": "b29", "title": "Prefix-tuning: Optimizing continuous prompts for generation", "journal": "Long Papers", "year": "2021", "authors": "Lisa Xiang; Percy Li;  Liang"}, {"ref_id": "b30", "title": "Continual mixed-language pre-training for extremely low-resource neural machine translation", "journal": "", "year": "2021", "authors": "Zihan Liu; Pascale Genta Indra Winata;  Fung"}, {"ref_id": "b31", "title": "Continual learning in task-oriented dialogue systems", "journal": "", "year": "2021", "authors": "Andrea Madotto; Zhaojiang Lin; Zhenpeng Zhou; Seungwhan Moon; A Paul; Bing Crook; Zhou Liu; Eunjoon Yu; Pascale Cho; Zhiguang Fung;  Wang"}, {"ref_id": "b32", "title": "fairseq: A fast, extensible toolkit for sequence modeling", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Myle Ott; Sergey Edunov; Alexei Baevski; Angela Fan; Sam Gross; Nathan Ng; David Grangier; Michael Auli"}, {"ref_id": "b33", "title": "Dictionary-based data augmentation for cross-domain neural machine translation", "journal": "", "year": "2020", "authors": "Wei Peng; Chongxuan Huang; Tianhao Li; Yun Chen; Qun Liu"}, {"ref_id": "b34", "title": "A call for clarity in reporting BLEU scores", "journal": "", "year": "2018", "authors": "Matt Post"}, {"ref_id": "b35", "title": "Natural language understanding with privacy-preserving bert", "journal": "Association for Computing Machinery", "year": "2021", "authors": "Chen Qu; Weize Kong; Liu Yang; Mingyang Zhang; Michael Bendersky; Marc Najork"}, {"ref_id": "b36", "title": "Overcoming catastrophic forgetting beyond continual learning: Balanced training for neural machine translation", "journal": "Long Papers", "year": "2022", "authors": "Chenze Shao; Yang Feng"}, {"ref_id": "b37", "title": "Augmenting self-attention with persistent memory", "journal": "", "year": "2019", "authors": "Sainbayar Sukhbaatar; Edouard Grave; Guillaume Lample; Herve Jegou; Armand Joulin"}, {"ref_id": "b38", "title": "Lamol: Language modeling for lifelong language learning", "journal": "", "year": "2019", "authors": "Fan-Keng Sun; Cheng-Hao Ho; Hung-Yi Lee"}, {"ref_id": "b39", "title": "Multilingual translation with extensible multilingual pretraining and finetuning", "journal": "", "year": "2020", "authors": "Yuqing Tang; Chau Tran; Xian Li; Peng-Jen Chen; Naman Goyal; Vishrav Chaudhary; Jiatao Gu; Angela Fan"}, {"ref_id": "b40", "title": "Overcoming catastrophic forgetting during domain adaptation of neural machine translation", "journal": "", "year": "2019", "authors": "Brian Thompson; Jeremy Gwinnup; Huda Khayrallah; Kevin Duh; Philipp Koehn"}, {"ref_id": "b41", "title": "Visualizing data using t-sne", "journal": "Journal of machine learning research", "year": "2008", "authors": "Laurens Van Der Maaten; Geoffrey Hinton"}, {"ref_id": "b42", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b43", "title": "A closer look at parameter contributions when training neural language and translation models", "journal": "", "year": "2022", "authors": "Ra\u00fal V\u00e1zquez; Hande Celikkanat; Vinit Ravishankar; Mathias Creutz; J\u00f6rg Tiedemann"}, {"ref_id": "b44", "title": "2021. Findings of the wmt 2021 shared task on large-scale multilingual machine translation", "journal": "", "year": "", "authors": "Guillaume Wenzek; Vishrav Chaudhary; Angela Fan; Sahir Gomez; Naman Goyal; Somya Jain; Douwe Kiela; Tristan Thrush; Francisco Guzm\u00e1n"}, {"ref_id": "b45", "title": "How robust is neural machine translation to language imbalance in multilingual tokenizer training? arXiv preprint", "journal": "", "year": "2022", "authors": "Shiyue Zhang; Vishrav Chaudhary; Naman Goyal; James Cross; Guillaume Wenzek; Mohit Bansal; Francisco Guzman"}, {"ref_id": "b46", "title": "Life-long learning for multilingual neural machine translation with knowledge distillation", "journal": "", "year": "2022", "authors": "Yang Zhao; Junnan Zhu; Lu Xiang; Jiajun Zhang; Yu Zhou; Feifei Zhai; Chengqing Zong"}, {"ref_id": "b47", "title": "Counter-interference adapter for multilingual machine translation", "journal": "", "year": "2021", "authors": "Yaoming Zhu; Jiangtao Feng; Chengqi Zhao; Mingxuan Wang; Lei Li"}, {"ref_id": "b48", "title": "Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? We provide the experimental setup with hyper-parameters and configuration in Appendix B", "journal": "", "year": "", "authors": ""}, {"ref_id": "b49", "title": "error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? We report the statistics about our results in Appendix B", "journal": "", "year": "", "authors": ""}, {"ref_id": "b50", "title": "for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)? We used existing packages of scripts and toolkit and we report these details", "journal": "", "year": "", "authors": ""}, {"ref_id": "b51", "title": "crowdworkers) or research with human participants? Left blank", "journal": "", "year": "", "authors": ""}, {"ref_id": "b52", "title": "Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators", "journal": "", "year": "", "authors": " D1"}, {"ref_id": "b53", "title": "crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic", "journal": "", "year": "", "authors": ""}, {"ref_id": "b54", "title": "Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?", "journal": "", "year": "", "authors": " D3"}, {"ref_id": "b55", "title": "Was the data collection protocol approved (or determined exempt) by an ethics review board? No response", "journal": "", "year": "", "authors": " D4"}, {"ref_id": "b56", "title": "Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? No response", "journal": "", "year": "", "authors": " D5"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Illustration of incremental learning for MNMT. The green parts are trainable for incremental adaptation. The parameters of the original model are frozen at the stage of incremental learning.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Stage 1 :1External Model Training. To convert incremental training data into continuous representation by additional parameters. We first leverage the incremental training data to train an external Transformer-based neural network.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: The training time of various methods in incremental learning for MNMT.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Angel (2006), explains the Continuum approach as a method being used to help organizations reach a higher level of performance. S2: It has been known for a long time that different types of brain damage, traumas, lesions, and tumours affect behaviour and cause changes in some mental functions.S3: The Sundarbans has been declared a UNESCO World Heritage Site. The part of the forest within Indian territory is called Sundarbans National Park. Reference S1: \"\u0410\u043d\u0433\u0435\u043b\" (2006) \u043f\u043e\u044f\u0441\u043d\u044e\u0454 \u043f\u0456\u0434\u0445\u0456\u0434 \u041a\u043e\u043d\u0442\u0438\u043d\u0443\u0443\u043c\u0430 \u044f\u043a \u043c\u0435\u0442\u043e\u0434, \u0449\u043e \u0432\u0438\u043a\u043e\u0440\u0438\u0441\u0442\u043e\u0432\u0443\u0454\u0442\u044c\u0441\u044f \u0434\u043b\u044f \u0434\u043e\u043f\u043e\u043c\u043e\u0433\u0438 \u043e\u0440\u0433\u0430\u043d\u0456\u0437\u0430\u0446\u0456\u044f\u043c \u0443 \u0434\u043e\u0441\u044f\u0433\u043d\u0435\u043d\u043d\u0456 \u0432\u0438\u0449\u043e\u0433\u043e \u0440\u0456\u0432\u043d\u044f \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0456. S2: \"\u0411\u0443\u043b\u043e \u0434\u0430\u0432\u043d\u043e \u0432\u0456\u0434\u043e\u043c\u043e, \u0449\u043e \u0440\u0456\u0437\u043d\u0456 \u0442\u0438\u043f\u0438 \u043f\u043e\u0448\u043a\u043e\u0434\u0436\u0435\u043d\u044c \u043c\u043e\u0437\u043a\u0443, \u0442\u0440\u0430\u0432\u043c\u0438, \u0443\u0440\u0430\u0436\u0435\u043d\u043d\u044f \u0442\u0430 \u043f\u0443\u0445\u043b\u0438\u043d\u0438 \u0432\u043f\u043b\u0438\u0432\u0430\u044e\u0442\u044c \u043d\u0430 \u043f\u043e\u0432\u0435\u0434\u0456\u043d\u043a\u0443 \u0456 \u043f\u0440\u0438\u0437\u0432\u043e\u0434\u044f\u0442\u044c \u0434\u043e \u0437\u043c\u0456\u043d \u0443 \u0434\u0435\u044f\u043a\u0438\u0445 \u043f\u0441\u0438\u0445\u0456\u0447\u043d\u0438\u0445 \u0444\u0443\u043d\u043a\u0446\u0456\u044f\u0445. S3: \u0421\u0443\u043d\u0434\u0430\u0440\u0431\u0430\u043d \u0431\u0443\u0432 \u043e\u0433\u043e\u043b\u043e\u0448\u0435\u043d\u0438\u0439 \u043e\u0431'\u0454\u043a\u0442\u043e\u043c \u0421\u0432\u0456\u0442\u043e\u0432\u043e\u0457 \u0441\u043f\u0430\u0434\u0449\u0438\u043d\u0438 \u042e\u041d\u0415\u0421\u041a\u041e. \u0427\u0430\u0441\u0442\u0438\u043d\u0430 \u043b\u0456\u0441\u0443 \u0432 \u043c\u0435\u0436\u0430\u0445 \u0456\u043d\u0434\u0456\u0439\u0441\u044c\u043a\u043e\u0457 \u0442\u0435\u0440\u0438\u0442\u043e\u0440\u0456\u0457 \u043d\u0430\u0437\u0438\u0432\u0430\u0454\u0442\u044c\u0441\u044f \u041d\u0430\u0446\u0456\u043e\u043d\u0430\u043b\u044c\u043d\u0438\u043c \u043f\u0430\u0440\u043a\u043e\u043c \u0421\u0443\u043d\u0434\u0430\u0440\u0431\u0430\u043d. Adapter (fail) S1: Angel (2006), explic\u0103 abordarea Continuum ca metod\u0103 utilizat\u0103 pentru a ajuta organiza\u0163iile la un nivel mai mare de performan\u0163\u0103. S2: \u0412\u0456\u0434\u043e\u043c\u043e \u0434\u0430\u0432\u043d\u043e\u0433\u043e \u0447\u0430\u0441\u0443 \u0432\u0456\u0434\u043e\u043c\u0438\u043b\u043e\u0441\u044f, \u0449\u043e \u0440\u0456\u0437\u043d\u0456 \u0442\u0438\u043f\u0438 \u043c\u043e\u0437\u043e\u043a\u0443 \u043f\u043e\u0448\u043a\u043e\u0434\u0443, \u0442\u0440\u0430\u0432\u043c\u0438, \u0440\u0435\u0437\u0456\u043e\u043d\u0456\u0432 \u0456 \u0442\u0443\u043c\u043e\u0440\u0456\u0432 \u0432\u043f\u043b\u0438\u0432\u0430\u044e\u0442\u044c \u043f\u043e\u0432\u0435\u0434\u0456\u043d\u043a\u0443 \u0456 \u0432\u0438\u043a\u043b\u0438\u043a\u0430\u044e\u0442\u044c \u0437\u043c\u0456\u043d\u0438 \u0432 \u0434\u0435\u044f\u043a\u0438\u0445 \u043f\u0441\u0438\u0445\u0456\u0447\u043d\u0438\u0445 \u0444\u0443\u043d\u043a\u0446\u0456\u044f\u0445. S3: Die Sundarbans wurde eine UNESCO Weltkulturbebe erkl\u00e4rt, die Teil des Waldes innerhalb indischen Territoriums wird Sundarbans National Park genannt. Extension S1: Angel (2006), \u043f\u043e\u044f\u0441\u043d\u044e\u0454 \u043f\u0456\u0434\u0445\u0456\u0434 Continuum \u044f\u043a \u043c\u0435\u0442\u043e\u0434, \u044f\u043a\u0438\u0439 \u0432\u0438\u043a\u043e\u0440\u0438\u0441\u0442\u043e\u0432\u0443\u0454\u0442\u044c\u0441\u044f, \u0449\u043e\u0431 \u0434\u043e\u043f\u043e\u043c\u043e\u0433\u0442\u0438 \u043e\u0440\u0433\u0430\u043d\u0456\u0437\u0430\u0446\u0456\u044f\u043c \u0434\u043e\u0441\u044f\u0433\u0442\u0438 \u0431\u0456\u043b\u044c\u0448 \u0432\u0438\u0441\u043e\u043a\u043e\u0433\u043e \u0440\u0456\u0432\u043d\u044f \u0432\u0438\u0434\u0430\u0442\u043d\u043e\u0441\u0442\u0456. S2: \u0417 \u0434\u0430\u0432\u043d\u043e \u0432\u0456\u0434\u043e\u043c\u043e, \u0449\u043e \u0440\u0456\u0437\u043d\u0456 \u0442\u0438\u043f\u0438 \u043f\u043e\u0448\u043a\u043e\u0434\u0436\u0435\u043d\u043d\u044f \u043c\u043e\u0437\u043a\u0443, \u0442\u0440\u0430\u0432\u043c\u0438, \u043f\u0435\u0440\u0456\u0437\u0430\u0446\u0456\u044f\u043c\u0438 \u0456 \u0442\u0443\u043c\u0430\u0440\u0438 \u0432\u043f\u043b\u0438\u0432\u0430\u044e\u0442\u044c \u043d\u0430 \u043f\u043e\u0432\u0435\u0434\u0456\u043d\u043a\u0443 \u0456 \u0432\u0438\u043a\u043b\u0438\u043a\u0430\u044e\u0442\u044c \u0437\u043c\u0456\u043d\u0438 \u0432 \u0434\u0435\u044f\u043a\u0438\u0445 \u043f\u0441\u0438\u0445\u0456\u0447\u043d\u0438\u0445 \u0444\u0443\u043d\u043a\u0446\u0456\u044f\u0445. S3: \u0421\u0443\u043d\u0434\u0430\u0440\u0431\u0430\u043d\u0441\u044c\u043a\u0456 \u0431\u0443\u043b\u0438 \u043e\u0433\u043e\u043b\u043e\u0448\u0435\u043d\u0456 UNESCO \u0421\u0432\u0456\u0442\u043d\u044c\u043e\u0457 \u0421\u043f\u0430\u0434\u0449\u0438\u043d\u0438, \u0447\u0430\u0441\u0442\u0438\u043d\u0430 \u043b\u0456\u0441\u0443 \u0432 \u043c\u0435\u0436\u0430\u0445 \u0406\u043d\u0434\u0456\u0457 \u043d\u0430\u0437\u0438\u0432\u0430\u0454\u0442\u044c\u0441\u044f \u0421\u0430\u043d\u0434\u0430\u0440\u0431\u0430\u043d\u0441\u044c\u043a\u043e\u044e \u041d\u0430\u0446\u0456\u043e\u043d\u0430\u043b\u044c\u043d\u0438\u043c \u043f\u0430\u0440\u043a\u043e\u043c. Ours S1: \u0410\u043d\u0433\u0435\u043b (2006), \u043f\u043e\u044f\u0441\u043d\u044e\u0454 \u043f\u0456\u0434\u0445\u0456\u0434 \u041a\u043e\u043d\u0442\u0438\u043d\u0443\u0443\u043c\u0430 \u044f\u043a \u043c\u0435\u0442\u043e\u0434, \u044f\u043a\u0438\u0439 \u0432\u0438\u043a\u043e\u0440\u0438\u0441\u0442\u043e\u0432\u0443\u0454\u0442\u044c\u0441\u044f, \u0449\u043e\u0431 \u0434\u043e\u043f\u043e\u043c\u043e\u0433\u0442\u0438 \u043e\u0440\u0433\u0430\u043d\u0456\u0437\u0430\u0446\u0456\u044f\u043c \u0434\u043e\u0441\u044f\u0433\u0442\u0438 \u0431\u0456\u043b\u044c\u0448 \u0432\u0438\u0441\u043e\u043a\u043e\u0433\u043e \u0440\u0456\u0432\u043d\u044f \u043f\u0440\u043e\u0434\u0443\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0456. S2: \u0414\u0430\u0432\u043d\u043e \u0432\u0456\u0434\u043e\u043c\u043e, \u0449\u043e \u0440\u0456\u0437\u043d\u0456 \u0442\u0438\u043f\u0438 \u043f\u043e\u0448\u043a\u043e\u0434\u0436\u0435\u043d\u044c \u043c\u043e\u0437\u043a\u0443, \u0442\u0440\u0430\u0432\u043c, \u0443\u0440\u0430\u0436\u0435\u043d\u043d\u044f \u0442\u0430 \u043f\u0443\u0445\u043b\u0438\u043d\u0438 \u0432\u043f\u043b\u0438\u0432\u0430\u044e\u0442\u044c \u043d\u0430 \u043f\u043e\u0432\u0435\u0434\u0456\u043d\u043a\u0443 \u0456 \u0432\u0438\u043a\u043b\u0438\u043a\u0430\u044e\u0442\u044c \u0437\u043c\u0456\u043d\u0438 \u0432 \u0434\u0435\u044f\u043a\u0438\u0445 \u043f\u0441\u0438\u0445\u0456\u0447\u043d\u0438\u0445 \u0444\u0443\u043d\u043a\u0446\u0456\u044f\u0445. S3: \u0421\u0443\u043d\u0434\u0430\u0440\u0431\u0430\u043d \u0431\u0443\u0432 \u043e\u0433\u043e\u043b\u043e\u0448\u0435\u043d\u0438\u0439 \u043e\u0431'\u0454\u043a\u0442\u043e\u043c \u0421\u0432\u0456\u0442\u043e\u0432\u043e\u0457 \u0441\u043f\u0430\u0434\u0449\u0438\u043d\u0438 \u042e\u041d\u0415\u0421\u041a\u041e. \u0427\u0430\u0441\u0442\u0438\u043d\u0430 \u043b\u0456\u0441\u0443 \u043d\u0430 \u0442\u0435\u0440\u0438\u0442\u043e\u0440\u0456\u0457 \u0406\u043d\u0434\u0456\u0457 \u043d\u0430\u0437\u0438\u0432\u0430\u0454\u0442\u044c\u0441\u044f \u041d\u0430\u0446\u0456\u043e\u043d\u0430\u043b\u044c\u043d\u0438\u0439 \u043f\u0430\u0440\u043a\u043e\u043c \u0421\u0443\u043d\u0434\u0430\u0440\u0431\u0430\u043d.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "The BLEU scores of incremental learning methods on new translation directions. The original model is mBART25 which does not support the languages of Ukrainian (uk) and Bengali (bn).", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "RO, . . . , UK }", "figure_data": "Data Model TaskJA EN PL + + EN HA + \u2026 Original Languages L 1 = {E N, JA, PL , . . . , H A} Original L 2 = {BN, Original Translation Directions Passage of Time Incremental Learning EN JA EN PL EN HA \u2026 Incremental Languages + EN BN EN UK EN RO \u2026 Original Translation Directions Incremental Translation Directions trainable frozen \u274c unavailable Original Pluggable MNMT Model MNMT Model Modules"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": "FFN AdaptationDecoder\u2716 NPluggable FFNDecoder\u2716 NFusionFeed-Forward NetworkPluggable ModuleFeed-Forward NetworkCross-AttentionCross-AttentionSelf-AttentionSelf-AttentionPluggable EmbeddingEmbeddingPluggable ModuleEmbedding[Target id]+ [ Target Sentence ][Target id]+ [Encoder\u2716 NPluggable FFNEncoder\u2716 NFusionVocabulary AdaptationFeed-Forward NetworkPluggable ModuleFeed-Forward NetworkPluggable EmbeddingSelf-AttentionSelf-AttentionPluggable Embedding+EmbeddingPluggable ModuleEmbeddingOverlap Tokens[Source id]+ [ Source Sentence ][Source id]+ [ Source Sentence ]on the original MNMT model, we open upadditional spaces for adapting to new languagepairs, as shown in Figure 2. However, the addi-tional spaces with initialized parameters are weakin their abilities to capture the shared linguisticfeatures among different languages. Because thelinguistic distribution of the original model is fit-ted previously, which leads to a representation gapbetween the original model and introduced spaces.Thus, we not only leverage new training data butalso exploit the potentiality of these data by intro-ducing two types of pluggable modules to bridgethe representation gap.Vocabulary Adaptation. On the one hand, if the new language has a distinct script with the"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Results in BLEU of adding a single language pair merely for MNMT in incremental learning. The highest score on each translation direction is highlighted in bold. The second highest score on each translation direction is highlighted in underline.", "figure_data": "MethodModulesWMT16 ro\u2192en en\u2192ro de\u2192en en\u2192de bn\u2192en en\u2192bn uk\u2192en en\u2192uk WMT14 FLoRes FLoResFrom-scratch Adapter Extension KT (Ours)-Serial Parallel Serial Parallel Serial Parallel35.42 31.02 35.38 32.17 36.28 36.34 36.8825.90 15.11 14.58 19.88 23.17 25.53 26.4831.33 24.34 29.57 26.90 30.45 30.96 31.6525.34 7.86 6.56 15.94 20.92 24.52 25.6530.40 10.04 24.95 24.40 28.78 29.82 30.1014.66 0.54 0.34 9.88 11.62 12.95 15.1032.71 23.19 31.52 27.56 31.94 32.95 33.8625.05 9.91 9.40 14.33 19.59 24.31 25.42"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_7", "figure_caption": ", we investigate the translation qualities when a new language pair arrive. The results demonstrate that our proposed method (KT) outperforms several baselines in terms of average BLEU scores for all incremental translation directions. Specifically, KT achieves an average BLEU score of 27.14 for xx\u2192en and 21.32 for en\u2192xx translations. In particular, based on KT, the plug-", "figure_data": "smooth:exp | version:2.2.0.13a |"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Results on the original (WMT-7) and incremental language pairs with different continual learning methods. \" \u2020\" indicates that we further extend the embedding layers based on each method. The highest score is highlighted in bold.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Results of knowledge transfer for the incremental language pairs. The external models are two sizes of M2M-100. The highest score of each original model is highlighted in bold.", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Results on different transfer areas for the incremental language pairs. The highest score is highlighted in bold. \"\u2713\" indicates that the pluggable module is injected through knowledge transfer in this area. \"\u2717\" indicates that the pluggable module is randomly initialized with the Gaussian distribution.", "figure_data": "Methodro\u2192en en\u2192ro bn\u2192en en\u2192bnOurs34.4425.6230.8115.71+Self-Attention +Gate-Fusion +Dropout34.14 33.39 33.9825.35 24.52 25.4230.37 29.85 30.0115.28 14.54 14.13"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "ModelSize en\u2192ha en\u2192is en\u2192ja en\u2192pl en\u2192ps en\u2192ta AVG.", "figure_data": "Ours Ours M2M-100 0.4B 0.4B 1.2B M2M-100 1.2B12.41 13.42 2.75 6.1421.30 22.62 13.12 18.6014.48 16.12 9.51 11.6726.43 27.91 22.55 28.084.93 5.41 2.92 4.7911.26 15.14 12.03 16.25 1.67 8.75 1.82 11.85ModelSize ha\u2192en is\u2192en ja\u2192en pl\u2192en ps\u2192en ta\u2192en AVG.Ours Ours M2M-100 0.4B 0.4B 1.2B M2M-100 1.2B12.88 13.41 4.78 9.2431.19 32.31 22.44 29.3319.14 19.27 10.59 13.4330.20 31.91 25.75 28.8711.21 12.29 7.65 10.9116.54 20.19 17.62 21.14 2.82 12.34 2.70 15.75"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "Results of English-to-xx and xx-to-English with different MNMT models on the original language pairs (WMT-7).", "figure_data": ""}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "The statistics of train, dev, and test data for the original languages (WMT-7) and the incremental languages. The top half part represents the set of the original languages. The second half represents the set of the incremental languages.", "figure_data": ""}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_19", "figure_caption": "The BLEU scores on zero-shot direction.", "figure_data": ""}, {"figure_label": "S1", "figure_type": "table", "figure_id": "tab_20", "figure_caption": "\u09b0\u09be\u09a4\u099c\u09c1 \u09c7\u09dc \u09ac \u09be\u09aa\u0995 \u09b2\u09c1 \u09a8 \u099a\u09b2\u09c7\u099b \u0995\u09be\u09b0\u09a3 \u09bf\u09ac\u09b6\u09c7\u0995\u0995'\u09b8 \u099f \u098f \u0986\u0987\u09a8 \u09c7\u09df\u09be\u0997\u0995\u09be\u09b0\u09c0 \u0995\u09ae \u0995\u09a4 \u09be\u09b0\u09be \u0989\u09aa\u09bf \u09a4 \u09bf\u099b\u09c7\u09b2\u09a8 \u09a8\u09be\u0964 S2: \"\u098f\u0995\u099c\u09a8 \u09aa\u09af \u09c7\u09ac \u09c7\u0995\u09b0 \u09be\u09b0\u09be \u09bf\u09ac\u09b6\u09c7\u0995\u0995 \u09ac\u09b2\u09c7\u09a4 \u09ac\u09be\u099d\u09be\u09c7\u09a8\u09be \u09b9\u09c7\u09df\u09bf\u099b\u09b2 \"\"\u0985\u09b0\u09be\u099c\u0995\u09a4\u09be\"\", \u0995\u09be\u09b0\u09a8 \u098f\u0995\u09a6\u09b2 \u09b2\u09be\u0995 \u09b0\u09be \u09be\u09df \u0998 \u09c7\u09b0 \u09ac\u09dc\u09be\u09a4 \u098f\u09ac\u0982 \u09ad\u09be\u0997 \u09aa\u09c7\u09a8 \u09b0 \u09bf\u099c\u09bf\u09a8\u09b8\u09aa \u09b2\u09c1 \u09a0 \u0995\u09b0\u09a4\u0964\" S3: \u09bf\u09ac\u09b6\u09c7\u0995\u09c7\u0995\u09b0 \u09ac\u09b6 \u09bf\u0995\u099b \u09ac\u09be\u09bf\u09b8 \u09be \u0986\u0987\u09a8\u09b6\u09c3 \u09b2\u09be\u09b0 \u0998\u09be\u099f\u09bf\u09a4\u09b0 \u099c\u09a8 \u09a6\u09bf \u09c7\u09a8\u09b0 \u09bf\u09a4\u09ac\u09be\u09a6\u0995\u09be\u09b0\u09c0\u09c7\u09a6\u09b0 \u09a6\u09be\u09b7 \u09bf\u09a6\u09c7\u09df\u09c7\u099b\u09a8\u0964 \u09ac\u09be\u0982\u09b2\u09be \u09ac\u09be\u0982\u09b2\u09be \u09ac\u09be\u0982\u09b2\u09be \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be \u09ac\u09be\u0982\u09b2\u09be \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be \u09ac\u09be\u0982\u09b2\u09be \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u09b2\u09be S2: \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac S3: \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac\u09be\u0982\u09b2\u09be, \u09ac Extension S1: \u09bf\u09ac\u09b6\u09c7\u0995\u09c7\u0995\u09b0 \u09b0\u09be \u09be\u09df \u0986\u0987\u09a8\u09b6\u09c3 \u09b2\u09be \u09b0 \u09be\u0995\u09be\u09b0\u09c0 \u09ac\u09be\u09bf\u09b9\u09a8\u09c0\u09b0 \u0995\u09ae \u0995\u09a4 \u09be\u09b0\u09be \u0989\u09aa\u09bf \u09a4 \u09bf\u099b\u09c7\u09b2\u09a8 \u09a8\u09be \u09ac\u09c7\u09b2\u0987 \u09b0\u09be\u09a4\u09be\u09b0\u09be\u09bf\u09a4 \u09b2\u09c1 \u099f\u09aa\u09be\u099f \u099a\u09be\u09b2\u09be\u09c7\u09a8\u09be \u0985\u09ac \u09be\u09b9\u09a4 \u09bf\u099b\u09b2 \u09ac\u09c7\u09b2 \u099c\u09be\u09a8\u09be \u09bf\u0997\u09c7\u09df\u09c7\u099b \u0964 S2: \u09bf\u09ac\u09b6\u09c7\u0995 \u098f\u0995\u099c\u09a8 \u09aa\u09af \u09c7\u09ac \u09c7\u0995\u09b0 \u09be\u09b0\u09be '\u09b0\u09be\u099c\u0995\u09c0\u09df' \u0985\u09ac \u09be\u09df \u09a1 \u09c7\u09ac \u09af\u09be\u0993\u09df\u09be \u09ac\u09c7\u09b2 \u09ac\u09a3 \u09a8\u09be \u0995\u09b0\u09be \u09b9\u09c7\u09df\u09bf\u099b\u09b2, \u0995\u09be\u09b0\u09a3 \u099c\u09a8\u09a4\u09be\u09b0 \u09a6\u09b2 \u09b0\u09be \u09be\u09df \u0998\u09be\u09b0\u09be\u09c7\u09ab\u09b0\u09be \u0995\u09c7\u09b0 \u098f\u09ac\u0982 \u09ad\u09be \u09be \u09aa\u09c7\u09a3 \u09b0 \u09a6\u09be\u0995\u09be\u09a8 \u09b2\u09c1 \u099f \u0995\u09c7\u09b0 \u09bf\u09a6\u09c7\u09df\u09bf\u099b\u09b2 \u0964 S3: \u09ac\u09b6 \u0995\u09c7\u09df\u0995\u099c\u09a8 \u09bf\u09ac\u09b6\u09c7\u0995\u09c7\u0995\u09b0 \u09ac\u09be\u09bf\u09b8 \u09be \u09ac\u0986\u0987\u09bf\u09a8 \u0986\u099a\u09b0\u09c7\u09a3\u09b0 \u099c\u09a8 \u09a6\u09bf \u09a3 \u09a5\u09c7\u0995 \u09bf\u09a4\u09ac\u09be\u09a6\u0995\u09be\u09b0\u09c0\u09c7\u09a6\u09b0 \u09a6\u09be\u09b7\u09be\u09c7\u09b0\u09be\u09aa \u0995\u09c7\u09b0\u09c7\u099b\u09a8 \u0964 Ours S1: \u09bf\u09ac\u09b6\u09c7\u0995\u09c7\u0995\u09b0 \u09b0\u09be \u09be\u09df \u0986\u0987\u09a8 \u09c7\u09df\u09be\u0997\u0995\u09be\u09b0\u09c0 \u0995\u09ae \u0995\u09a4 \u09be\u09b0\u09be \u0989\u09aa\u09bf \u09a4 \u09a8\u09be \u09a5\u09be\u0995\u09be\u09df \u09b0\u09be\u09a4\u09ad\u09b0 \u09ac \u09be\u09aa\u0995 \u09b2\u09c1 \u099f\u09aa\u09be\u099f \u0985\u09ac \u09be\u09b9\u09a4 \u09bf\u099b\u09b2 \u09ac\u09c7\u09b2 \u099c\u09be\u09a8\u09be \u0997\u09c7\u099b \u0964 S2: \u09bf\u09ac\u09b6\u09c7\u0995\u0995\u09c7\u0995 \u098f\u0995\u099c\u09a8 \u09aa\u09af \u09c7\u09ac \u09c7\u0995\u09b0 \u09be\u09b0\u09be '\u09b0\u09be\u00e5\u099c\u09a4T' \u0985\u09ac \u09be\u09b0 \u09ae\u09c7\u09a7 \u09a1 \u09c7\u09ac \u09af\u09be\u0993\u09df\u09be\u09b0 \u09ac\u09a3 \u09a8\u09be \u09a6\u0993\u09df\u09be \u09b9\u09c7\u09df\u09bf\u099b\u09b2, \u0995\u09be\u09b0\u09a3 \u098f\u0995\u09a6\u09b2 \u09b2\u09be\u0995 \u09b0\u09be \u09be\u09df \u0998\u09be\u09b0\u09be\u09c7\u09ab\u09b0\u09be \u0995\u09c7\u09b0 \u098f\u09ac\u0982 \u09ad\u09be\u0997 \u09aa\u09c7\u09a3 \u09b0 \u09a6\u09be\u0995\u09be\u09a8 \u09b2\u09c1 \u099f \u0995\u09c7\u09b0 \u09bf\u09a8\u09c7\u09df \u09af\u09be\u09df \u0964 S3: \u09bf\u09ac\u09b6\u09c7\u0995\u09c7\u0995\u09b0 \u09ac\u09b6 \u0995\u09c7\u09df\u0995\u099c\u09a8 \u09ac\u09be\u09bf\u09b8 \u09be \u09a6\u09bf \u09a3 \u09bf\u09a6\u0995 \u09a5\u09c7\u0995 \u0986\u09b8\u09be \u09bf\u09a4\u09ac\u09be\u09a6\u0995\u09be\u09b0\u09c0\u09c7\u09a6\u09b0\u09c7\u0995 \u098f\u0987 \u0985\u09a7\u09c7\u09ae \u09b0 \u099c\u09a8 \u09a6\u09be\u09b7\u09be\u09c7\u09b0\u09be\u09aa \u0995\u09c7\u09b0\u09c7\u099b \u0964", "figure_data": "S1:Adapter(fail)"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_21", "figure_caption": "", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "EN", "formula_coordinates": [3.0, 108.78, 140.47, 6.46, 4.97]}, {"formula_id": "formula_1", "formula_text": "L D (\u03b8) = D i \u2208D (x,y)\u2208D i log p(y|x; \u03b8) (1)", "formula_coordinates": [3.0, 335.09, 179.14, 190.06, 30.96]}, {"formula_id": "formula_2", "formula_text": "L D \u2032 (\u03b8) = D j \u2208D \u2032 (x,y)\u2208D j log p(y|x; \u03b8) (2)", "formula_coordinates": [3.0, 331.66, 458.94, 193.49, 31.1]}, {"formula_id": "formula_3", "formula_text": "FFN(H) = f (H \u2022 K \u22a4 ) \u2022 V (3)", "formula_coordinates": [4.0, 119.11, 667.95, 170.76, 21.19]}, {"formula_id": "formula_4", "formula_text": "H (f) = FFN original (H) + FFN external (H) (4)", "formula_coordinates": [4.0, 315.42, 604.75, 209.74, 14.25]}, {"formula_id": "formula_5", "formula_text": "L D \u2032 (\u03b8) = D j \u2208D \u2032 (x,y)\u2208D j log p(y|x;\u03b8) (5)", "formula_coordinates": [5.0, 96.38, 109.76, 193.49, 31.1]}, {"formula_id": "formula_6", "formula_text": "L D \u2032 (\u03b8 e ,\u03b8 f ) = D j \u2208D \u2032 (x,y)\u2208D j log p(y|x;\u03b8 e ,\u03b8 f )(6", "formula_coordinates": [5.0, 76.84, 318.38, 208.79, 37.18]}], "doi": "10.18653/v1/N19-4009"}