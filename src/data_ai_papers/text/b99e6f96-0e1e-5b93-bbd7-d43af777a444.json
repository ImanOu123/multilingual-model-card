{"title": "Dynamic 3D Scene Analysis from a Moving Vehicle", "authors": "Bastian Leibe; Nico Cornelis; Kurt Cornelis; Luc Van Gool; Eth Zurich", "pub_date": "", "abstract": "In this paper, we present a system that integrates fully automatic scene geometry estimation, 2D object detection, 3D localization, trajectory estimation, and tracking for dynamic scene interpretation from a moving vehicle. Our sole input are two video streams from a calibrated stereo rig on top of a car. From these streams, we estimate Structurefrom-Motion (SfM) and scene geometry in real-time. In parallel, we perform multi-view/multi-category object recognition to detect cars and pedestrians in both camera images. Using the SfM self-localization, 2D object detections are converted to 3D observations, which are accumulated in a world coordinate frame. A subsequent tracking module analyzes the resulting 3D observations to find physically plausible spacetime trajectories. Finally, a global optimization criterion takes object-object interactions into account to arrive at accurate 3D localization and trajectory estimates for both cars and pedestrians. We demonstrate the performance of our integrated system on challenging real-world data showing car passages through crowded city areas.", "sections": [{"heading": "Introduction", "text": "The task we address in this paper is dynamic scene analysis from a moving, camera-equipped vehicle. At any point in time, we want to detect other traffic participants in the environment (cars, bicyclists, and pedestrians), localize them in 3D, estimate their past trajectories, and predict their future motion (as shown in Fig. 1). Such a capability has obvious applications in driver assistance systems, but it also serves as a testbed for many interesting research challenges.\nScene analysis of this sort requires multi-viewpoint, multi-category object detection. Since we cannot control the vehicle's path, nor the environment it passes through, the detectors need to be robust to a large range of lighting variations, noise, clutter, and partial occlusion. For 3D localization, an accurate estimate of the scene geometry is necessary. The ability to integrate such measurements over time additionally requires continuous self-localization and recalibration. In order to finally make predictions about future states, powerful tracking is needed that can cope with a changing background. On the other hand, each object will typically persist in the vehicle's field of view only for a few seconds. It is thus not as important to uniquely track a person's identity as in classic surveillance scenarios.\nIn this paper, we present a system which addresses those challenges by integrating recognition, reconstruction, and tracking in a collaborative ensemble. Namely, SfM yields scene geometry for each image pair, which greatly helps the other modules. Recognition picks out objects of interest and separates them from the dynamically changing background. Tracking adds a temporal context to individual object detections and provides them with a history supporting their presence in the current video frame. Detected object trajectories, finally, are extrapolated to future frames and are constantly reevaluated in the light of new evidence.\nThe paper contains the following contributions. 1) We present an integrated system for dynamic scene analysis on a mobile platform. We demonstrate how its individual components can benefit from each other's continuous input and how the transferred knowledge can be used to improve scene analysis. 2) In particular, we present a multiview/multi-category object detection module that can reli-ably detect cars and pedestrians in crowded real-world traffic scenes. We show how knowledge about the scene geometry can be used in such a system both to improve recognition performance and to fuse the outputs of multiple detectors. 3) We demonstrate how the resulting 2D detections can be integrated over time to arrive at accurate 3D localization and orientation estimates of static objects. 4) In order to deal with moving objects, we propose a novel tracking approach which formulates the tracking problem as spacetime trajectory analysis followed by hypothesis selection. This approach is capable of tracking a large and variable number of objects through complex outdoor scenes with a moving camera. In addition, it can model physical objectobject interactions to arrive at a globally optimal scene interpretation. 5) Finally, we demonstrate the performance of our integrated system on two challenging video sequences of car passages through crowded city centers showing accurate 3D localization and trajectory estimation results for cars, bicyclists, and pedestrians.\nThe paper is structured as follows. The following sections discuss related work and give a general overview of our system. After that, Sections 2, 3, and 4 describe our scene geometry estimation, object detection, and tracking approaches in detail. Section 5 presents experimental results. A final discussion concludes our work.\nRelated Work. Scene analysis with a moving camera is a notoriously difficult task because of the combined effects of egomotion, blur, and rapidly changing lighting conditions [3,6]. In addition, the introduction of a moving camera invalidates many simplifying techniques we have grown fond of, such as background subtraction and a constant ground plane assumption. Such techniques have been routinely used in surveillance and tracking applications from static cameras (e.g. [2,12]), but they are no longer applicable here. While object tracking under such conditions has been demonstrated in clean highway situations [3], reliable performance in urban areas is still an open challenge [7].\nIn order to allow tracking with a moving camera, several approaches have started to explore the possibilities of combining tracking with detection [1,8,21,23]. At the same time, object detection itself has made tremendous progress over the last few years [5,15,18,22,23], to an extent that state-of-the-art detectors are becoming applicable in complex outdoor scenes. [10] have shown that geometric scene context can greatly help recognition and have proposed a method to estimate it from a single image. More recently, [13] have combined recognition and SfM, however only for the purpose of localizing static objects.\nIn our approach, we integrate geometry estimation and tracking-by-detection in a combined system that searches for the best scene interpretation by global optimization. [2] also perform global trajectory optimization to track up to six mutually occluding individuals by modelling their posi-tions on a discrete occupancy grid. However, their approach requires static cameras, and optimization is performed only for one individual at a time. In contrast, our approach models object positions continuously while moving through a 3D world and allows to find a combined optimal solution. System Overview. Our input data are two video streams from a calibrated stereo rig mounted on top of a vehicle. From this data, an SfM module computes a camera pose and ground plane estimate for each image. This information is fed to an object detection module, which processes both camera images to detect cars and pedestrians in the vehicle's field of view. The necessary reliability of the detection module is achieved by integrating multiple local cues, fusing the output of several single-view detectors, and making use of the continuously updated ground plane estimate. Using the estimated camera pose, 2D detections are then converted to 3D observations and passed to the subsequent tracking module. This module analyzes the incoming 3D observations to find plausible spacetime trajectories and selects the best explanation for each frame pair by a global optimization criterion.", "publication_ref": ["b3", "b6", "b1", "b12", "b3", "b7", "b0", "b8", "b21", "b23", "b5", "b15", "b18", "b22", "b23", "b10", "b13", "b1"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Real-Time Scene Geometry Estimation", "text": "Real-Time Structure-from-Motion (SfM). Our SfM module is based on the approach by [4], which is highly optimized and runs at 26-30 fps. It takes the green channel of each camera as input and extracts image feature points by finding local maxima of a simple feature measure based on average intensities of four subregions. The extracted features are matched between consecutive images and then fed into a classic SfM pipeline [9], which reconstructs feature tracks and refines 3D point locations by triangulation. Bundle adjustment is running in parallel with the main SfM algorithm to refine camera poses and 3D feature locations for previous frames and thus reduce drift.\nOnline Ground Plane Estimation. For each image pair, SfM delivers an updated camera calibration. In addition, we obtain an online ground plane estimate by computing local normals on a set of trapezoidal road strips between the reconstructed wheel contact points of adjacent frames and averaging those local measurements over a larger window. In order to do this reliably, it is necessary to find a good compromise for the window size this estimate is based on. We experimentally found a window size of 3m, roughly corresponding to the ground patch beneath the vehicle, to be optimal for a variety of different cases. Note that this procedure automatically adjusts for the driving speed. A lower driving speed leads to more accurate reconstruction, so that the smaller strip sizes are sufficient. Conversely, higher speed (or lower frame rate) reduces reconstruction quality, but this is compensated for by the larger strip size between frames.\nFigure 2 highlights the importance of this continuous reestimation step if later stages are to trust its results. In this example, the camera vehicle hits a speedbump, causing a massive jolt in camera perspective. The top row of Fig. 2 shows the resulting detections when the ground plane estimate from the previous frame is simply taken over. As can be seen, this results in several false positives at improbable locations and scales. The bottom image displays the detections when the reestimated ground plane is used instead.\nHere, the negative effect is considerably lessened.", "publication_ref": ["b4", "b9"], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "Object Detection", "text": "The recognition system is based on a battery of singleview, single-category ISM detectors [15]. This approach lets local features, extracted around interest regions, vote for the object center in a 3-dimensional Hough space, followed by a top-down segmentation and verification step. For our application, we use the robust multi-cue extension from [14], which integrates local Shape Context descriptors [19] computed at Harris-Laplace, Hessian-Laplace, and DoG interest regions [17,19]. The main contribution of this section is how to fuse those different detectors and how to integrate scene geometry into the recognition system.\nOur system uses a set of 5 single-view detectors for the different car orientations and one additional pedestrian detector (Fig. 2). We do not differentiate between pedestrians and bicyclists here, as they are often indistinguishable from a distance and our detector responds well to both categories. We start by running all detectors in parallel on both camera images and collect their hypotheses (without the final verification step). For each such hypothesis h, we compute two per-pixel probability maps p(p = figure|h) and p(p = ground|h), as described in [15]. Integration of Scene Geometry Constraints. Given the camera calibration and ground plane estimate from SfM, we can associate each image-plane hypothesis h with a 3D location by projecting a ray from the camera center through the base point of its detection bounding box. If the ray intersects the ground plane, we can estimate the object's realworld size by projecting a second ray through the bounding box top point and intersecting it with a vertical plane through its 3D base. Using this information, we can express the likelihood for a real-world object H given image I entirely by the image-plane hypotheses h according to the following marginalization:\np(H|I)= h p(H|h)p(h|I) \u223c h p(h|H)p(H)p(h|I) (1)\nThe following paragraphs describe each of those three factors in detail and explain how they are used in our recognition system. 2D Recognition Score. The last term in eq. (1) is the likelihood of hypothesis h given the image. Using the top-down segmentation of h, we express this likelihood in terms of the pixels h occupies:\np(h|I) = p\u2208I p(h|p) \u2248 p\u2208Seg(h) p(p = figure|h)p(h), (2)\nwhere Seg(h) denotes the segmentation area of h, i.e. the pixels for which p(p = figure|h) > p(p = ground |h).\nGround Plane Constraints. The middle term p(H) expresses a 3D prior for finding an object at location H, which we split into separate priors for the object size and distance given its category.\np(H) = p(H size |H categ )p(H dist |H categ )p(H categ ) (3)\nIn our application, we assume a uniform distance prior and model the size prior by a Gaussian (similar to [10]). This effective coupling between object distance and size through a ground plane assumption has several beneficial effects. First, it significantly reduces the search volume during voting to a corridor in Hough space (Fig. 3(left)). In addition, the Gaussian size prior serves to \"pull\" object hypotheses towards the correct locations, thus improving also recognition quality.\nMulti-Detector Integration. The third factor in eq. (1), finally, is a 2D/3D transfer function p(h|H), which relates the image-plane hypothesis h to the 3D object hypothesis H. This term is of particular interest in combination with the sum over all h, since it allows to effectively fuse the results of the different single-view detectors by clustering the inferred world states. The intuition behind this step is that two image-plane detections are consistent if they correspond to the same 3D object (Fig. 3(right)). Thus, we can disambiguate between overlapping responses from different detectors on the basis of the world state they would infer, which is done in the following global optimization step.", "publication_ref": ["b15", "b14", "b19", "b17", "b19", "b15", "b10"], "figure_ref": ["fig_1", "fig_2", "fig_2"], "table_ref": []}, {"heading": "Multi-Category Hypothesis Selection.", "text": "In order to obtain the final interpretation for the current image pair, we search for the combination of hypotheses that together best explain the observed evidence. In [15], this is done by adopting an MDL formulation and expressing the savings [16] of a particular hypothesis h as\nS h = K 0 S data \u2212 K 1 S model \u2212 K 2 S error , (4\n)\nwhere S data corresponds to the number N of data points or pixels that are explained this way; S model denotes the model cost, usually a constant; and S error describes a cost for the error that is made by this representation. More generally, it can be shown that if the error term is chosen as the sum over all data points x assigned to a hypothesis h of the probabilities that the point assignment is wrong\nS error = x\u2208h (1 \u2212 p(x|h)),(5)\nthen the savings reduce to the merit term\nS h = \u2212\u03ba 1 + x\u2208h ((1 \u2212 \u03ba 2 ) + \u03ba 2 p(x|h)) , (6\n)\nwhich is effectively just the sum over the data assignment likelihoods, together with a regularization term to compensate for unequal sampling. When hypotheses overlap, they compete for data points, resulting in interaction costs. As shown in [16], the optimal hypothesis selection can then be formulated as a Quadratic Boolean Optimization Problem\nmax m m T Qm = max m m T \u23a1 \u23a2 \u23a3 q 11 \u2022 \u2022 \u2022 q 1M . . . . . . . . . q M1 \u2022 \u2022 \u2022 q MM \u23a4 \u23a5 \u23a6m (7)\nwith an indicator vector m = {m 1 , . . . , m M }, where m i = 1 if h i is selected and 0 otherwise; and an interaction matrix Q. Here, we pursue a similar approach. In contrast to [15], however, we perform the hypothesis selection not over image-plane hypotheses h i , but over their corresponding world hypotheses H i . Combining eqs. (1) and ( 6), we obtain the following merit terms where A \u03c3,v (h k ) acts as a normalization factor expressing the expected area of a 2D hypothesis at its detected scale and aspect. Two 3D hypotheses H i and H j interact if their supporting image-plane hypotheses h ki and h kj compete for the same pixels. In this case, we assume that the hypothesis H * \u2208 {H i , H j } that is farther away from the camera is occluded and subtract its support in the overlapping image area. The interaction cost then becomes\nq ii = S Hi = \u2212\u03ba 1 + k p(h k |H i )p(H i )f (h k ), (8) f (h k ) = 1 A \u03c3,v (h k ) p\u2208Seg(h) ((1\u2212\u03ba 2 ) + \u03ba 2 p(p = fig.|h k ))(9) t (a) (b) (c)\nq ij = \u2212 1 2 k * p(h k * |H * )p(H * )f (h k * ).(10)\nAs a result of this procedure, we obtain a set of world hypotheses {H i }, together with their supporting segmentations in the image. At the same time, the hypothesis selection procedure naturally integrates the contributions from the different single-view, single-category detectors. We perform the optimization separately for the two camera images and pass the resulting detections to the following temporal integration stage.", "publication_ref": ["b15", "b16", "b16", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Temporal Integration and Tracking", "text": "In order to present our tracking approach, we introduce the concept of event cones. The event cone of an observation H i,t = {x i,t , v i,t is the spacetime volume it can physically influence from its current position given its maximal velocity and turn rate. Figure 4 shows an illustration for several cases of this concept. If an object is static at time t and its orientation is unknown, all motion directions are equally probable, and the affected spacetime volume is a simple double cone reaching both forwards and backwards in time (Fig. 4(a)). If the object moves holonomically, i.e. without external constraints linking its speed and turn rate, the event cone becomes tilted in the motion direction (Fig. 4(b)). An example for this case would be a pedestrian at low speeds. In the case of nonholonomic motion, as in a car which can only move along its main axis and only turn while moving, the event cones get additionally deformed according to those (often nonlinear) constraints (Fig. 4(c)).\nWe thus search for plausible trajectories through the spacetime observation volume by linking up event cones. Starting from an observation H i,t , we follow its event cone up and down the timeline and collect all observations that fall inside its volume in the adjoining time steps. Since we do not know the starting velocity v i,t yet, we begin with the case in Fig. 4(a). In all subsequent time steps, however, we It is important to point out that an individual event cone is not more powerful in its descriptive abilities than a bidirectional Extended Kalman Filter, since it is based on essentially the same equations. However, our approach goes beyond Kalman Filters in several important respects. First of all, we are no longer bound by a Markovian assumption. When reestimating the object state, we can take several previous time steps into account. In our approach, we aggregate the information from all previous time steps, weighted with a temporal discount \u03bb. In addition, we are not restricted to tracking a single hypothesis. Instead, we start independent trajectory searches from all available observations (at all time steps) and collect the corresponding hypotheses. The final scene interpretation is then obtained by a global optimization criterion which selects the combination of trajectory hypotheses that best explains the observed data under the constraints that each observation may only belong to a single object and no two objects may occupy the same physical space at the same time. The following sections explain those steps in more detail.\nColor Model. For each observation, we compute an objectspecific color model a i , using the top-down segmentations provided by the previous stage. Figure 5 shows an example of this input. For each detection H i,t , we build an 8 \u00d7 8 \u00d7 8 RGB color histogram over the segmentation area, weighted by the per-pixel confidence k p(p = fig.|h k )p(h k |H i,t ) in this segmentation. Similar to [20], we compare color models by their Bhattacharyya coefficient\np(a i |A) \u223c q a i (q)A(q)(11)\nDynamic Model. Given a partially grown trajectory H t0:t , we first select the subset of observations which fall inside its event cone. Using the simple motion model\u1e61\nx = v cos \u03b8 y = v sin \u03b8 \u03b8 = K c and\u1e8b = v cos \u03b8 y = v sin \u03b8 \u03b8 = K c v (12)\nfor holonomic and nonholonomic motion on the ground plane, respectively, we compute predicted positions\nx p t+1 = x t + v\u0394t cos \u03b8 y p t+1 = y t + v\u0394t sin \u03b8 \u03b8 p t+1 = \u03b8 t + K c \u0394tand\nx p t+1 = x t + v\u0394t cos \u03b8 y p t+1 = y t + v\u0394t sin \u03b8 \u03b8 p t+1 = \u03b8 t + K c v\u0394t (13\n)\nand approximate the positional uncertainty by an oriented Gaussian to arrive at the dynamic model D\nD : p x t+1 y t+1 \u223c N x p t+1 y p t+1 , R T \u03c3 2 mov 0 0 \u03c3 2 turn R p(\u03b8 t+1 ) \u223c N (\u03b8 p t+1 , \u03c3 2 steer )(14)\nwhere R is the rotation matrix, K c the path curvature, and the nonholonomic constraint is approximated by adapting the rotational uncertainty \u03c3 turn as a function of v.\nSpacetime Trajectory Search. Each candidate observation H i,t+1 is then evaluated under the covariance of D and compared to the trajectory's appearance model A (its mean color histogram), yielding\np(H i,t+1 |H t0:t ) = p(H i,t+1 |A t )p(H i,t+1 |D t ). (15\n)\nAfter this, the trajectory is updated by the weighted mean of its predicted position and the supporting observations:\nx t+1 = 1 Z p(H t+1 |H t0:t )x p t+1 + i p(H i,t+1 |H t0:t )x i . (16\n)\nwith p(H t+1 |H t0:t ) = e \u2212\u03bb and normalization factor Z. Velocity, rotation, and appearance model are updated in the same fashion. Static cars are treated as a special case, since their orientation cannot be inferred from their motion direction and our appearance-based detectors provide a too coarse orientation estimate for our goal to estimate a precise 3D bounding box. Instead, we accumulate detections over a longer time frame. Using the observation that the main localization uncertainty from our detectors occurs both along the car's main axis and along our vehicle's viewing direction, we then estimate the car orientation as the weighted mean between our detectors' orientation estimate and the cluster shape of all inlier observations projected onto the ground plane.", "publication_ref": ["b20"], "figure_ref": ["fig_3", "fig_3", "fig_3", "fig_3", "fig_3", "fig_4"], "table_ref": []}, {"heading": "Global Trajectory Selection.", "text": "We express the support S of a trajectory H t0:t reaching from time t 0 to t by the evidence collected from the images I t0:t during that time span:\nS(H t0:t |I t0:t ) = i p(H t0:t |H i,ti )p(H i,ti |I ti )(17)\n= p(H t0:\nt ) i p(H i,ti |H t0:t ) p(H i,ti ) p(H i,ti |I ti ) \u223c p(H t0:t ) i p(H i,ti |H t0:t )p(H i,ti |I ti )\nwhere p(H i,ti ) = j p(H i,ti |H j ) is a normalization factor that can be omitted, since we are only interested in relative scores. Further, we define\np(H i,ti |H t0:t ) = p(H ti |H t0:t )p(H i,ti |H ti )(18)\n= e \u2212\u03bb(t\u2212ti) p(H i,ti |A ti )p(H i,ti |D ti )\nthat is, we express the likelihood of an observation H i,ti belonging to trajectory H t0:t = (A, D) t0:t by evaluating it under the trajectory's appearance and dynamic model at that time, weighted with a temporal discount.\nIn order to find the combination of trajectory hypotheses that together best explain the observed evidence, we again solve a Quadratic Boolean Optimization Problem max e m m T Q m with the additional constraint that no two objects may occupy the same space at the same time. With a similar derivation as in Section 3, we arrive at\nq ii = \u2212 \u03ba 1 + H k,t k \u2208Hi ((1\u2212 \u03ba 2 ) + \u03ba 2 g k,i )(19)\nq ij = \u2212 1 2 H k,t k \u2208Hi\u2229Hj ((1\u2212 \u03ba 2 ) + \u03ba 2 g k, * + \u03ba 3 O ij ) (20) g k,i = p(H k,t k |H i )p(H k,t k |I t k ).\nwhere again H * \u2208 {H i , H j } denotes the weaker of the two hypotheses and the additional penalty term O ij measures the physical overlap between the spacetime trajectory volumes of H i and H j given average object dimensions. The hypothesis selection procedure always searches for the best explanation of the current world state given all evidence available up to now. It is not guaranteed that this explanation is consistent with the one we got for the previous frame. However, as soon as it is selected, it explains the whole past, as if it had always existed. We can thus follow a trajectory back in time to determine where a pedestrian came from when he first stepped into view, even though no hypothesis was selected for him back then. Fig. 6 visualizes the estimated spacetime trajectories for such a case. Efficiency Considerations. The main computational cost in this stage comes from three factors: the cost to find trajectories, to build the quadratic interaction matrix Q, and to solve the final optimization problem. However, the first two steps can reuse information from previous time steps. Thus, instead of building up trajectories from scratch at each time step t, we merely check for each of the existing hypotheses H t0:t\u22121 if it can be extended by the new observations. In addition, we start new trajectory searches down the time line from each new observation H i,t . Similarly, most entries of the previous interaction matrix Q t\u22121 can be reused and just need to be weighted with the temporal discount e \u2212\u03bb .\nThe cost of the optimization problem depends on the connectedness of the matrix Q, i.e. on the number of nonzero interactions between hypotheses. For static cars and for the 2D case in Section 3, this number is typically very low, since only few hypotheses overlap. For pedestrian trajectories, the number of interactions may however grow quite large. In this paper, we therefore just compute a greedy approximation for both optimization problems. However, a range of efficient relaxation techniques have become available in recent years which can be used to compute more exact solutions (e.g. [11]).", "publication_ref": ["b11"], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "Experimental Results", "text": "Data Sets. In the following, we evaluate our integrated approach on two challenging video sequences. The first test sequence consists of 1175 image pairs recorded at 25fps and a resolution of 360\u00d7288 pixels over a distance of about 500m. It contains a total of 77 (sufficiently visible) static cars parked on both sides of the street, 4 moving cars, but almost no pedestrians at sufficiently high resolutions. The main difficulties for object detection here lie in the relatively low resolution, strong partial occlusion between parked cars, frequently encountered motion blur, and extreme contrast changes between brightly lit areas and dark shadows. Only the car detectors are used for this sequence.\nThe second sequence consists of 290 image pairs captured over the course of about 400m at the very sparse frame rate of 3fps and a resolution of 384\u00d7288 pixels. This very challenging sequence shows a vehicle passage through a crowded city center, with parked cars and bicycles on both street sides, numerous pedestrians and bicyclists travelling on the side walks and crossing the street, and several speed bumps. Apart from the difficulties mentioned above, this sequence poses the additional challenge of detecting and separating many mutually occluding pedestrians at very low resolutions while simultaneously limiting the number of false positives on background clutter. In addition, temporal integration is further complicated by the low frame rate.\nIn the following sections, we present experimental results for object detection and tracking performance on both sequences. However, it would clearly be unrealistic to expect perfect detection and tracking results under such difficult conditions, which may make the quantitative results hard to interpret. We therefore provide the result videos at http://www.vision.ethz.ch/bleibe/cvpr07. Object Detection Performance. Figure 7(left) displays example detection results of our system on difficult images from the two test sequences. All images have been processed at their original resolution by SfM and bilinearly interpolated to twice their initial size for object detection. For a quantitative evaluation we annotated one video stream for each sequence and marked all objects that were within 50m distance and visible by at least 30-50%. It is important  to note that this includes many cases with partial visibility. Fig 7(right) shows the resulting detection performance with and without ground plane constraints. As can be seen from the plots, both recall and precision are greatly improved by the inclusion of scene geometry, up to an operating point of 0.34 fp/frame for cars and 1.65 fp/frame for pedestrians.\nTracking Performance. Figure 8 shows online tracking results of our system (using only detections from previous frames) for both sequences. As can be seen, our system manages to localize and track other traffic participants despite significant egomotion and dynamic scene changes. The 3D localization and orientation estimates typically converge at a distance of 15-30m and lead to accurate 3D bounding boxes for cars and pedestrians. A major challenge for sequence #2 is to filter out false positives from incorrect detections. At 3fps, this is not always possible. However, false positives typically get only low confidence ratings and quickly fade out again as they fail to get continuous support.", "publication_ref": [], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper, we have presented an integrated system for dynamic 3D scene analysis from a moving platform. We have proposed a novel method to fuse the output of multiple single-view object detectors and to integrate continuously reestimated scene geometry constraints. In order to aggregate detections over time, we have further proposed a novel tracking approach that can localize and track a variable number of objects with a moving camera and that arrives at a consistent scene interpretation by global optimiza-tion. The resulting system obtains an accurate analysis of dynamic scenes, even at very low frame rates.\nOne of the key points we want to make here is convergence. The different fields of Computer Vision have advanced tremendously in recent years. While all modalities considered in this paper, SfM, object detection, and tracking, are far from being solved yet individually, all three have become sufficiently mature to be useful in combination with the others. As we have demonstrated here, the individual tasks can benefit considerably by the integration and the close collaboration with the other modalities. and novel capabilities can emerge as a consequence. Many more such cross-links can be exploited. For example, stereo depth estimates can directly be used to extract foci of attention for object detection [6]. Similarly, results from tracking could be used to guide feature extraction and speed up recognition considerably. It is reasonable to expect that those additions will increase system performance, and we will investigate them in future work.", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgments: This work is supported, in parts, by EU projects DIRAC (IST-027787) and HERMES (IST-027110). We also wish to acknowledge the support of Toyota Motor Corporation/Toyota Motor Europe, KU Leuven Research Fund's GOA project MARVEL, and TeleAtlas for providing additional survey videos to test on.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Ensemble tracking", "journal": "", "year": "2005", "authors": "S Avidan"}, {"ref_id": "b1", "title": "Robust people tracking with global trajectory optimization", "journal": "", "year": "2006", "authors": "J Berclaz; F Fleuret; P Fua"}, {"ref_id": "b2", "title": "Figure 8. 3D localization and tracking results of our system. The bottom row shows a bird's eye view reconstruction of the third scene", "journal": "", "year": "", "authors": ""}, {"ref_id": "b3", "title": "Real-time multiple vehicle tracking from a moving vehicle", "journal": "MVA", "year": "2000", "authors": "M Betke; E Haritaoglu; L Davis"}, {"ref_id": "b4", "title": "Fast compact city modeling for navigation pre-visualization", "journal": "", "year": "2006", "authors": "N Cornelis; K Cornelis; L Van Gool"}, {"ref_id": "b5", "title": "Histograms of oriented gradients for human detection", "journal": "", "year": "2005", "authors": "N Dalal; B Triggs"}, {"ref_id": "b6", "title": "Real-time object detection for smart vehicles", "journal": "", "year": "1999", "authors": "D Gavrila; V Philomin"}, {"ref_id": "b7", "title": "A bayesian framework for multi-cue 3d object tracking", "journal": "", "year": "2004", "authors": "J Giebel; D Gavrila; C Schn\u00f6rr"}, {"ref_id": "b8", "title": "On-line boosting and vision", "journal": "", "year": "2006", "authors": "H Grabner; H Bischof"}, {"ref_id": "b9", "title": "Multiple view geometry in computer vision", "journal": "Cambridge University Press", "year": "2000", "authors": "R Hartley; A Zisserman"}, {"ref_id": "b10", "title": "Putting objects into perspective", "journal": "", "year": "2006", "authors": "D Hoiem; A Efros; M Hebert"}, {"ref_id": "b11", "title": "Multiclass image labeling with semidefinite programming", "journal": "", "year": "2006", "authors": "J Keuchel"}, {"ref_id": "b12", "title": "Model-based object tracking in monocular image sequences of road traffic scenes", "journal": "IJCV", "year": "1993", "authors": "D Koller; K Daniilidis; H.-H Nagel"}, {"ref_id": "b13", "title": "Integrating recognition and reconstruction for cognitive traffic scene analysis from a moving vehicle", "journal": "", "year": "2006", "authors": "B Leibe; N Cornelis; K Cornelis; L V Gool"}, {"ref_id": "b14", "title": "Segmentation based multi-cue integration for object detection", "journal": "", "year": "2006", "authors": "B Leibe; K Mikolajczyk; B Schiele"}, {"ref_id": "b15", "title": "Pedestrian detection in crowded scenes", "journal": "", "year": "2005", "authors": "B Leibe; E Seemann; B Schiele"}, {"ref_id": "b16", "title": "Segmentation of range images as the search for geometric parametric models", "journal": "IJCV", "year": "1995", "authors": "A Leonardis; A Gupta; R Bajcsy"}, {"ref_id": "b17", "title": "Distinctive image features from scale-invariant keypoints", "journal": "IJCV", "year": "2004", "authors": "D Lowe"}, {"ref_id": "b18", "title": "Multiple object class detection with a generative model", "journal": "", "year": "2006", "authors": "K Mikolajczyk; B Leibe; B Schiele"}, {"ref_id": "b19", "title": "A performance evaluation of local descriptors", "journal": "Trans. PAMI", "year": "2005", "authors": "K Mikolajczyk; C Schmid"}, {"ref_id": "b20", "title": "An adaptive color-based particle filter", "journal": "Image and Vision Computing", "year": "2003", "authors": "K Nummiaro; E Koller-Meier; L Van Gool"}, {"ref_id": "b21", "title": "A boosted particle filter: Multitarget detection and tracking", "journal": "", "year": "2004", "authors": "K Okuma; A Taleghani; N Freitas; J Little; D Lowe"}, {"ref_id": "b22", "title": "Detecting pedestrians using patterns of motion and appearance", "journal": "", "year": "2003", "authors": "P Viola; M Jones; D Snow"}, {"ref_id": "b23", "title": "Tracking of multiple, partially occluded humans based on static body part detections", "journal": "", "year": "2006", "authors": "B Wu; R Nevatia"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. Online 3D localization and trajectory estimation results of our system obtained from inside a moving vehicle. The different bounding box intensities correspond to our system's confidence level in its estimates.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 .2Figure 2. (top and right) Illustration for the importance of a continuous reestimation of scene geometry. The images show the effect on object detection when the vehicle hits a speedbump (top) if using an unchanged ground plane estimate; (bottom) if using the online reestimate. (bottom left) Training viewpoints used for cars and pedestrians.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 .3Figure 3. Benefits of scene geometry for object detection. (left) A ground plane significantly reduces the search volume for Hough voting. A Gaussian size prior additionally \"pulls\" object hypotheses towards the right locations. (right) The responses of multiple detectors are combined if they refer to the same scene object.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 .4Figure 4. Visualization of example event cones for (a) a static object with unknown orientation; (b) a holonomically moving object; (c) a non-holonomically moving object.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 .5Figure 5. Detections and corresponding top-down segmentations used to learn the object-specific color model. can reestimate the object state from the new evidence and adapt the growing trajectory accordingly.It is important to point out that an individual event cone is not more powerful in its descriptive abilities than a bidirectional Extended Kalman Filter, since it is based on essentially the same equations. However, our approach goes beyond Kalman Filters in several important respects. First of all, we are no longer bound by a Markovian assumption. When reestimating the object state, we can take several previous time steps into account. In our approach, we aggregate the information from all previous time steps, weighted with a temporal discount \u03bb. In addition, we are not restricted to tracking a single hypothesis. Instead, we start independent trajectory searches from all available observations (at all time steps) and collect the corresponding hypotheses. The final scene interpretation is then obtained by a global optimization criterion which selects the combination of trajectory hypotheses that best explains the observed data under the constraints that each observation may only belong to a single object and no two objects may occupy the same physical space at the same time. The following sections explain those steps in more detail.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 6 .6Figure 6. Visualization of the estimated spacetime trajectories for cars and pedestrians from the scene in Fig. 1. Blue dots show pedestrian observations; red dots correspond to car observations.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Sequence # 2 (2290 frames, 758 annotated cars, 942 pedestrians) Recall #false positives/image cars, no ground plane cars, with ground plane pedestrians, no ground plane pedestrians, with ground plane", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 7 .7Figure 7. (left) Example car and pedestrian detections of our system on difficult images from the two test sequences. (right) Quantitative comparison of the detection performance with and without scene geometry constraints (the crosses mark the operating point for tracking).", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "p(H|I)= h p(H|h)p(h|I) \u223c h p(h|H)p(H)p(h|I) (1)", "formula_coordinates": [3.0, 314.18, 376.28, 230.97, 20.89]}, {"formula_id": "formula_1", "formula_text": "p(h|I) = p\u2208I p(h|p) \u2248 p\u2208Seg(h) p(p = figure|h)p(h), (2)", "formula_coordinates": [3.0, 318.86, 490.52, 226.29, 21.57]}, {"formula_id": "formula_2", "formula_text": "p(H) = p(H size |H categ )p(H dist |H categ )p(H categ ) (3)", "formula_coordinates": [3.0, 318.97, 593.96, 226.18, 10.33]}, {"formula_id": "formula_3", "formula_text": "S h = K 0 S data \u2212 K 1 S model \u2212 K 2 S error , (4", "formula_coordinates": [4.0, 85.09, 296.72, 197.46, 10.65]}, {"formula_id": "formula_4", "formula_text": ")", "formula_coordinates": [4.0, 282.56, 297.39, 3.87, 8.97]}, {"formula_id": "formula_5", "formula_text": "S error = x\u2208h (1 \u2212 p(x|h)),(5)", "formula_coordinates": [4.0, 114.26, 400.04, 172.18, 20.89]}, {"formula_id": "formula_6", "formula_text": "S h = \u2212\u03ba 1 + x\u2208h ((1 \u2212 \u03ba 2 ) + \u03ba 2 p(x|h)) , (6", "formula_coordinates": [4.0, 84.49, 440.96, 198.06, 20.89]}, {"formula_id": "formula_7", "formula_text": ")", "formula_coordinates": [4.0, 282.56, 441.63, 3.87, 8.97]}, {"formula_id": "formula_8", "formula_text": "max m m T Qm = max m m T \u23a1 \u23a2 \u23a3 q 11 \u2022 \u2022 \u2022 q 1M . . . . . . . . . q M1 \u2022 \u2022 \u2022 q MM \u23a4 \u23a5 \u23a6m (7)", "formula_coordinates": [4.0, 61.82, 531.8, 224.62, 48.49]}, {"formula_id": "formula_9", "formula_text": "q ii = S Hi = \u2212\u03ba 1 + k p(h k |H i )p(H i )f (h k ), (8) f (h k ) = 1 A \u03c3,v (h k ) p\u2208Seg(h) ((1\u2212\u03ba 2 ) + \u03ba 2 p(p = fig.|h k ))(9) t (a) (b) (c)", "formula_coordinates": [4.0, 56.06, 96.68, 444.7, 622.41]}, {"formula_id": "formula_10", "formula_text": "q ij = \u2212 1 2 k * p(h k * |H * )p(H * )f (h k * ).(10)", "formula_coordinates": [4.0, 348.97, 278.24, 196.12, 27.61]}, {"formula_id": "formula_11", "formula_text": "p(a i |A) \u223c q a i (q)A(q)(11)", "formula_coordinates": [5.0, 111.74, 556.4, 174.63, 20.29]}, {"formula_id": "formula_12", "formula_text": "x = v cos \u03b8 y = v sin \u03b8 \u03b8 = K c and\u1e8b = v cos \u03b8 y = v sin \u03b8 \u03b8 = K c v (12)", "formula_coordinates": [5.0, 100.33, 616.4, 186.04, 35.17]}, {"formula_id": "formula_13", "formula_text": "x p t+1 = x t + v\u0394t cos \u03b8 y p t+1 = y t + v\u0394t sin \u03b8 \u03b8 p t+1 = \u03b8 t + K c \u0394tand", "formula_coordinates": [5.0, 46.81, 678.52, 112.07, 38.09]}, {"formula_id": "formula_14", "formula_text": "x p t+1 = x t + v\u0394t cos \u03b8 y p t+1 = y t + v\u0394t sin \u03b8 \u03b8 p t+1 = \u03b8 t + K c v\u0394t (13", "formula_coordinates": [5.0, 163.81, 678.52, 118.41, 38.09]}, {"formula_id": "formula_15", "formula_text": ")", "formula_coordinates": [5.0, 282.22, 693.87, 4.15, 8.97]}, {"formula_id": "formula_16", "formula_text": "D : p x t+1 y t+1 \u223c N x p t+1 y p t+1 , R T \u03c3 2 mov 0 0 \u03c3 2 turn R p(\u03b8 t+1 ) \u223c N (\u03b8 p t+1 , \u03c3 2 steer )(14)", "formula_coordinates": [5.0, 308.89, 102.76, 236.44, 39.05]}, {"formula_id": "formula_17", "formula_text": "p(H i,t+1 |H t0:t ) = p(H i,t+1 |A t )p(H i,t+1 |D t ). (15", "formula_coordinates": [5.0, 324.98, 240.08, 215.97, 10.33]}, {"formula_id": "formula_18", "formula_text": ")", "formula_coordinates": [5.0, 540.94, 240.75, 4.15, 8.97]}, {"formula_id": "formula_19", "formula_text": "x t+1 = 1 Z p(H t+1 |H t0:t )x p t+1 + i p(H i,t+1 |H t0:t )x i . (16", "formula_coordinates": [5.0, 314.54, 290.24, 226.41, 27.37]}, {"formula_id": "formula_20", "formula_text": ")", "formula_coordinates": [5.0, 540.94, 297.63, 4.15, 8.97]}, {"formula_id": "formula_21", "formula_text": "S(H t0:t |I t0:t ) = i p(H t0:t |H i,ti )p(H i,ti |I ti )(17)", "formula_coordinates": [5.0, 311.78, 540.08, 233.32, 20.65]}, {"formula_id": "formula_22", "formula_text": "t ) i p(H i,ti |H t0:t ) p(H i,ti ) p(H i,ti |I ti ) \u223c p(H t0:t ) i p(H i,ti |H t0:t )p(H i,ti |I ti )", "formula_coordinates": [5.0, 372.14, 560.36, 170.08, 51.61]}, {"formula_id": "formula_23", "formula_text": "p(H i,ti |H t0:t ) = p(H ti |H t0:t )p(H i,ti |H ti )(18)", "formula_coordinates": [5.0, 320.78, 656.36, 224.32, 10.33]}, {"formula_id": "formula_24", "formula_text": "= e \u2212\u03bb(t\u2212ti) p(H i,ti |A ti )p(H i,ti |D ti )", "formula_coordinates": [5.0, 382.46, 670.6, 150.76, 12.29]}, {"formula_id": "formula_25", "formula_text": "q ii = \u2212 \u03ba 1 + H k,t k \u2208Hi ((1\u2212 \u03ba 2 ) + \u03ba 2 g k,i )(19)", "formula_coordinates": [6.0, 78.61, 323.24, 207.76, 23.6]}, {"formula_id": "formula_26", "formula_text": "q ij = \u2212 1 2 H k,t k \u2208Hi\u2229Hj ((1\u2212 \u03ba 2 ) + \u03ba 2 g k, * + \u03ba 3 O ij ) (20) g k,i = p(H k,t k |H i )p(H k,t k |I t k ).", "formula_coordinates": [6.0, 74.3, 345.44, 212.08, 45.08]}], "doi": ""}