{"title": "Dissecting Transformer Length Extrapolation via The Lens of Receptive Field Analysis", "authors": "Ta-Chung Chi; Ting-Han Fan; Alexander I Rudnicky; Peter J Ramadge", "pub_date": "", "abstract": "Length extrapolation permits training a transformer language model on short sequences that preserves perplexities when tested on substantially longer sequences. A relative positional embedding design, ALiBi, has had the widest usage to date. We dissect ALiBi via the lens of receptive field analysis empowered by a novel cumulative normalized gradient tool. The concept of receptive field further allows us to modify the vanilla Sinusoidal positional embedding to create Sandwich, the first parameter-free relative positional embedding design that truly length information uses longer than the training sequence. Sandwich shares with KERPLE and T5 the same logarithmic decaying temporal bias pattern with learnable relative positional embeddings; these elucidate future extrapolatable positional embedding design.", "sections": [{"heading": "Introduction", "text": "The length of input sequences is an important hyperparameter choice for pretraining a transformer language model. A vanilla transformer language model has a quadratic training cost w.r.t L tr , the training sequence length. As the value of L tr increases, cost becomes impractical. However, we can use the model for substantially longer evaluation sequence lengths L ex L tr as gradients no longer need to be recorded. The discrepancy between L tr and L ex motivates the task of length extrapolation (Press et al., 2022): Can a transformer language model maintain equally good, if not better, perplexities when longer sequences are used in the testing stage?\nSeveral extrapolatable transformer language models have been proposed including ALiBi (Press et al., 2022) and KERPLE (Chi et al., 2022), of which the relative positional embedding design is hypothesized to be critical to success. Empirically, they extrapolate to L ex L tr much better than other absolute and relative positional embeddings  \n\u2212 1 2 ' \u00d7 * \" ) \" * # ) \" * # ) # * $ ) \" * $ ) # * $ ) $ 0 1 0 2 1 0 3 0 1 2 * % ) \" * % ) # * % ) $ * % ) %\n* \" ) \" * # ) \" * # ) # * $ ) \" * $ ) # * $ ) $ 0 0 0 \u221e 0 0 \u221e 0 0 \u221e * % ) \" * % ) # * % ) $ * % ) %\nFigure 2: Windowed Attention. This is the same design as Longformer (Beltagy et al., 2020). We limit the context window size to w = 2 in this example. Left = self-attention matrix, right = temporal biases matrix.\nincluding Sinusoidal (Vaswani et al., 2017), Rotary (Su et al., 2021), and T5 (Raffel et al., 2020), resulting in the adoption of ALiBi for the recently released Bloom (Scao et al., 2022) model. Despite the significant empirical success of ALiBi, there is still a lack of fundamental understanding of why it works. 1 Figure 1 shows the implementation of ALiBi. We hereinafter refer to the coefficient 1 2 h as slope. Intuitively, ALiBi encourages a token to focus on neighbors based on its temporal biases matrix. When two tokens are distant, ALiBi becomes highly similar to windowed attention, shown in Figure 2. Experiments in \u00a74 will further establish the connection between the two.\nWindowed attention allows the easy derivation of a theoretical (maximum) receptive field: wR for an R layer transformer model with windowed attention size w. A windowed attention model can extrapolate if L tr > wR because 1) wR is fully covered by L tr during the training stage, and 2) it simply ignores the additional L ex \u2212 wR tokens during the testing stage. Surprisingly, a model can still extrapolate when L tr < wR which we show in \u00a74. This calls for the need for empirical receptive field measurement and motivates our model-agnostic cumulative normalized gradient tool. The tool we develop can be applied back on ALiBi to show that L tr covers most of its empirical receptive field.\nOur analysis tool also provides critical context for explaining the length extrapolation failure (Press et al., 2022;Chi et al., 2022) of Sinusoidal (Vaswani et al., 2017) and Rotary (Su et al., 2021) by showing their violation of the empirical receptive field coverage principle. Sinusoidal can be fixed by dropping the intermediate terms and keeping only the decay-with-distance biases; this leads to the creation of Sandwich, the first parameter-free relative positional embedding that uses information beyond L tr . Sandwich shares a similar temporal bias pattern with trainable positional embeddings such as KERPLE (Chi et al., 2022) and T5 (Raffel et al., 2020), and they jointly suggest the future design of extrapolatable transformer positional embeddings.", "publication_ref": ["b17", "b17", "b7", "b5", "b22", "b21", "b18", "b17", "b7", "b22", "b21", "b7", "b18"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Related Work", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Length Extrapolation", "text": "In the context of language modeling, we expect token-level perplexities to remain at least the same, if not lower (i.e. better), when L ex L tr sequences are provided. Recurrent neural networks (Mikolov et al., 2010;Mikolov and Zweig, 2012;Zaremba et al., 2014) can easily perform length extrapolation. But this is not an easy task for transformer language models, among which only those equipped with special relative positional embeddings (Press et al., 2022;Chi et al., 2022) are length extrapolatable.", "publication_ref": ["b13", "b14", "b26", "b17", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Positional Embeddings", "text": "It is widely believed that the design of positional embeddings is the key to successful length extrapolation of transformer language models (Press et al., 2022;Chi et al., 2022). We can roughly catego-rize existing positional embeddings into absolute (APE) (Vaswani et al., 2017) and relative (RPE) (Su et al., 2021;Raffel et al., 2020;Press et al., 2022;Chi et al., 2022) variants. APE often assigns one positional embedding per token and combines them directly with input embeddings. In contrast, RPE adds temporal bias terms to the self-attention matrix to encode the relative distance between token pairs. For example, the right triangular matrix in Figure 1 shows the set of temporal bias terms. It is challenging for APE to extrapolate well without any further fine-tuning since either the beyond L positional embeddings do not exist, or the model needs to process unseen positional embeddings (e.g. unseen sinusoidal embeddings). (Press et al., 2022;Chi et al., 2022). In contrast, RPE usually performs better length extrapolation since it is easier to construct the additional temporal bias terms.", "publication_ref": ["b17", "b7", "b22", "b21", "b18", "b17", "b7", "b17", "b7"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Windowed and Sparse Attention", "text": "We will see later that ALiBi can be viewed as imposing a windowed attention mask on the selfattention matrix, similar to previous transformer models with sparse attention (Beltagy et al., 2020;Zaheer et al., 2020;Gupta and Berant, 2020). Interpreting ALiBi from the perspective of windowed attention allows us to easily calculate the theoretical receptive field of a model.", "publication_ref": ["b5", "b25", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Receptive Field", "text": "A model's receptive field is defined as the size of the input region that contributes the most to model outputs. It is often measured in the context of convolution neural networks (Luo et al., 2016;Dai et al., 2017;Araujo et al., 2019;Raghu et al., 2021;Dosovitskiy et al., 2021) and their dilated variants (Oord et al., 2016;Yu and Koltun, 2016;Chang et al., 2017;Beltagy et al., 2020) with the ultimate goal of receptive field size maximization. Even though we focus on transformer language models, we borrow the idea to show that the empirical receptive field coverage of a model is crucial to its length extrapolation performance. vectors in R d H at each layer:\nq m = W q e m , k m = W k e m , v m = W v e m , where W q , W k , W v \u2208 R d H\n\u00d7d are learnable matrices. The resulting vectors are processed by the self-attention module for pre-Softmax logits:\nl mn = q m , k n , if m \u2265 n \u2212 inf, otherwise\nfollowed by the scaled softmax normalization:\na m,n = exp(l m,n / d/H) L i=1 exp(l m,i / d/H)(1)\nTo be precise, the matrices (W\n(h) q , W(\nh) k , W (h) v ), vectors (q (h) m , k (h) m , v (h) m , o (h) m\n), and scalars (l\n(h) mn , a (h)\nmn ) are associated with a head number h. For notation simplicity, we only show the dependency on h when we need it. For example, the output vector o\n(h) m at position m for head h is: o (h) m = L n=1 a (h) m,n v (h) n\nAll the H output vectors are concatenated, denoted by \u2295, and transformed by\nW o \u2208 R d\u00d7d to obtain o m \u2208 R d : o m = W o (o (1) m \u2295 o (2) m \u2295 \u2022 \u2022 \u2022 \u2295 o (H) m )\nA layer normalization (Ba et al., 2016) on o m , i.e. LayerNorm(o m ), gives the input embedding to the next layer. After R layers of propagation, the last o m is transformed by V \u2208 R v\u00d7d and normalized by Softmax to get the distribution p \u2208 R v over vocabulary size v:\np = Softmax(V o m )(2)\nWe set R = 12, H = 12, d = 768, and L tr = 512 for all experiments reported in this paper.", "publication_ref": ["b12", "b8", "b3", "b9", "b15", "b24", "b6", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "ALiBi", "text": "ALiBi modifies l m,n to be:\nl mn = q m , k n \u2212 1 2 h (m \u2212 n), if m \u2265 n \u2212 inf, otherwise (3) The range of h is n \u2022 8\nH , where n = {1 . . . H}.\nThe quick brown fox jumps over the lazy dog This formulation ensures the same 5 tokens are always evaluated with different numbers of previous tokens.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Windowed Attention", "text": "If the windowed attention has a size w, then:\nl mn = q m , k n , if n + w > m \u2265 n \u2212 inf, otherwise", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation of Length Extrapolation", "text": "We prepare N = 1000 text segments of length L ex > L tr from the evaluation dataset. For each segment, we alter the number of previous tokens ranging from 1 to L ex \u2212 1 of the last token and only calculate its perplexity:\nPPL = exp 1 N N i=1 \u2212 log p i ,\nwhere p i is the predicted probability from Eq. (2) of the last (L ex -th) token in the i-th segment. This ensures that the same set of tokens is always used for perplexity calculation and only their number of previous tokens is varied, see Figure 3. 2", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "ALiBi and Windowed Attention", "text": "Here, we alter the slope ( 1 2 h ) of ALiBi to check if the length extrapolation property persists and reveal the connection between ALiBi and windowed attention. We present three experiments on two datasets, ArXiv and OpenWebText2 (Appendix A), to ensure that the observations are consistent across different text domains, shown in Table 1 and 4. L ex Shift all h by \u2206 Same h for all heads Windowed Attention with Size w \u2206:-3 0 2 4 6 8 h:0 2 4 6 8 w:40 80 100 120 160 320 512 5.76 5.57 5.50 5.63 5.70 5.70 9.45 6.65 5.85 5.60 5.70 8.27 7.28 7.04 6.77 6.41 6.04 1024 7.15 5.64 5.31 5.81 55.4 55.4 9.20 7.01 8.66 25.4 55.4 8.27 7.29 7.02 8.90 67.4 178  2048 7.15 5.94 5.89 6.92 94.4 94.4 9.21 7.08 8.66 31.7 94.4 8.27 7.29 7.03 8.90 67.5 202  4096 7.15 5.95 5.92 6.94 96.0 96.0 9.21 7.08 8.66 31.8 96.0 8.27 7.29 7.02 8.90 67.5 202  8192 7.15 5.95 5.92 6.94 96.0 96.0 9.21 7.08 8.66 31.8 96.0 8.27 7.29 7.02 8.90 67.5 202   Table 1: The three experiments on the Arxiv dataset.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2", "tab_0"]}, {"heading": "Slope Shift (Shift all h by \u2206)", "text": "We first investigated whether slope diversity (each attention head has one slope) is the key to length extrapolation. We shift h by a fixed amount \u2206 and find that the model, unfortunately, fails to extrapolate beyond a certain quantity. This implies that diversity itself might not be the deciding factor, but that the actual slope value is more important.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Slope Equalization (Same h for all heads)", "text": "To identify the slope magnitude that enables length extrapolation, we set all slopes to be the same instead of the original geometric sequence. We then steadily increase the slope value from 0 to 8 and find that only large slopes ( 1 2 h ), or equivalently small h, allow a model to extrapolate well. Large slopes implicitly enforce a narrow windowed bias on the self-attention matrix such that distant tokens cannot interact with each other.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Windowed Attention (Size w)", "text": "We make the implicit window effect explicit as shown by Eq. (3), which is also adopted by Longformer (Beltagy et al., 2020). We define the windowed attention size to be w. The model underperforms at small w and diverges on long L ex at large w. The same trend holds in the first two experiments when h is too small or large.", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}, {"heading": "Other Observations", "text": "First, ALiBi does not in fact extrapolate since its perplexities all increase instead of staying the same when L ex > L tr . In contrast, windowed attention models are extrapolatable up to w = 100. Second, we can clearly see that once L ex passes a certain threshold, the perplexity either remains the same or explodes. This suggests that the model is either ignoring tokens beyond a certain length (same) 3 or not using it properly (explosion). In the next", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Receptive Field Measurement", "text": "Following the definition of windowed attention size w, an R layer transformer has a theoretical receptive field (TRF) of wR, which is the maximum number of tokens that contribute to the prediction of the next token. In practice, a neural model often uses a subset of TRF, named empirical receptive field (ERF). While previous work (Luo et al., 2016;Dai et al., 2017;Araujo et al., 2019;Raghu et al., 2021;Dosovitskiy et al., 2021;Beltagy et al., 2020) aims to increase ERF to match TRF, we show that decreasing ERF could serve as one feasible approach to enable successful length extrapolation.\nConsider the case where TRF \u2264 L tr : This model can extrapolate easily because its TRF is fully covered and trained. Concretely, if we set R = 12, L tr = 512 in Table 1 and 4, we know that as long as w < 42.6 = 512/12, TRF will be fully covered by L tr . Surprisingly, the model is still able to extrapolate up to w = 100, leading to a TRF of 100 * 12 = 1200 512. This can be explained by the ERF and TRF discrepancy discussed above; this calls for the need to quantify ERF.", "publication_ref": ["b12", "b8", "b3", "b9", "b5"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Quantifying Empirical Receptive Field", "text": "We first calculate the normalized gradient (Luo et al., 2016) of each input token w.r.t the prediction of the next token:\ns m = g m 2 Lex n=1 g n 2 ,\nwhere g m is the gradient vector of the input embedding e m . We then calculate the cumulative sum as: Figure 4 demonstrates how we derive the model's ERF when it is predicting the 2048-th token. For models with w \u2208 [40, 80, 100], the most recent L ex = L tr = 512 (1536-th to 2047-th) covers more than 99% of the total (1.0) normalized gradient, so their ERF is smaller than 512. In contrast, models with w \u2208 [120, 160, 320] have ERF = 768, 1024, and 1536 tokens, respectively. Since L tr = 512 does not fully cover their ERFs, they fail to extrapolate well.\nc m = Lex n=m s n , 0 \u2264 c m \u2264 1,\nWe next focus on the more complex Figure 5, in which neither of the configurations reaches 0.99 within the most recent L tr = 512 tokens. Generally, this explains why the perplexity often bumps up when L ex goes from 512 to 1024: Models cannot perfectly process more tokens than they were trained on. If we take a closer look, the \u2206 = \u22123 model has the strongest windowing effect and the smallest ERF=768 tokens, therefore its perplexity plateaus the soonest at L ex = 1024 in Table 1. The remaining models all need ERF=2048 tokens to reach c m = 0.99, which explains why their perplexities become stable only after L ex = 2048 (Table 1). For \u2206 \u2208 [6, 8] models specifically, the difference between L tr and ERF is too large to be handled, resulting in exploded perplexities.", "publication_ref": ["b12"], "figure_ref": [], "table_ref": []}, {"heading": "Fixing Failed Cases", "text": "We fix the failed cases in Table 1 section 1 (varying \u2206) and section 3 (varying w) by increasing L tr to cover their ERFs. We increase L tr to 1024 for windowed attention with w = 160; For shifted ALiBi with \u2206 = 6, we need L tr = 2048 tokens.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Analyses of Sinusoidal and Rotary", "text": "Sinusoidal (Vaswani et al., 2017) constructs the positional embedding at position m and \u2200i \u2208 [1, d/2] as:\np m,2i = sin m 10000 2i/d , p m,2i+1 = cos m 10000 2i/d\n(5)\nThey will be added with the input embeddings {e m } L m=1 followed by the query and key transformations as shown in Eq. (4). Unlike addition, Rotary (Su et al., 2021)  (\nW q (e m + p m )) (W k (e n + p n )) = (4) e m W q W k e n semantic info. + e m W q W k p n + p m W q W k e n + p m W q W k p n mixture of semantic and positional info. \u2248 e m W q W k e n semantic info. + p m p n positional info.\ntokens because neither of their formulations guarantees a L tr -bounded receptive field. Figure 7 tells additional stories: To predict the last token (2048th), Sinusoidal focuses on the 512-th token when L tr = 512 and the 128-th token when L tr = 128 as indicated by the sudden jump on their normalized gradient plots. This is because the model has only seen at most L tr positional embeddings and overfitted on them, which provides explicit evidence to the Sinusoidal, or APE in general, overfitting hypothesis made by the author of ALiBi 4 . It also explains why RPE is a better choice for length extrapolatable transformers: They cannot overfit on the positional embeddings.\n6 A New RPE for Length Extrapolation", "publication_ref": ["b22", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Introduction to Sandwich", "text": "We fix the overfitting issue of Sinusoidal by transforming it into a new RPE, Sandwich, shown in Eq. (4). Specifically, we drop the cross terms and keep only the inner product of two positional embeddings 5 at m and n. Now p m p n with m, n \u2208 4 https://twitter.com/OfirPress/status/ 1435690039925567489 5 We set pm,n to 2d as doing so gives better empirical performance; it only needs to be computed once before training.\n[1, L] become the temporal bias terms of Sandwich:\np m p n =d /2 i=1 sin m 10000 2i/d sin n 10000 2i/d + cos m 10000 2i/d cos n 10000 2i/d =d /2 i=1 cos m \u2212 n 10000 2i/d\nA similar observation was previously made in a context different from length extrapolation (Yan et al., 2019).\nThe largest value of p m p n happens at the point where m \u2212 n = 0, which gives the maximum value ofd/2. To align L tr with the ERF of Sandwich, we need to further check that p m p n demonstrates a similar windowed attention effect as ALiBi. This can be done by subtracting all p m p n byd/2 and further dividing them by a set of predefined compression ratios. for the sake of simplicity, we set the compression ratios to be the same as ALiBi's h = n \u2022 8 H with n \u2208 {1 . . . H}:\np m p n \u2212d/2 h (6)\nEq. ( 6) is added after the scaled softmax is done in Eq. (1). Figures 8 and 9 show a visualization of Sandwich when h = 8. Sandwich indeed has the same decay-with-distance pattern as ALiBi. 6 Note that we deliberately decouple thisd from d in Eq. (5) since we treatd as a hyperparameter that controls the shape of Sandwich. A largerd leads to a stronger windowed attention effect as shown in Figure 10. We setd = 128 in this work for all the experiments. We also experiment with smaller and largerd and only find worse performance. Finally, readers can find the reference Python implementation in Appendix E.", "publication_ref": ["b23"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Experiments and Discussion", "text": "To verify the performance of Sandwich, we train a transformer language model following previous 6 Fun fact: We imagine different compression ratios as the ways we eat sandwiches: For a huge sandwich, we have to squeeze it more to fit in our mouths! work (Press et al., 2022;Chi et al., 2022). Table 3 presents the results; the left part contains all models without learnable parameters, and the right part contains models with learnable parameters. These numbers should not be compared across sections.\nIn general, models on the right achieve lower perplexities across the three datasets. This is expected as they can adapt to individual datasets more easily thanks to the additional learnable parameters. However, there is no free lunch: They often consume more GPU memory and run much slower. For example, T5 is 10% slower than Sandwich during the training stage. Note that Sandwich can also be equipped with learnable parameters such as learnable compression ratios h; this is left to future work. We now shift our focus to the left section. When L ex = L tr = 512, Sandwich is comparable to other models except that Rotary performs a bit better on OpenWebText2. Once we increase L ex , Sandwich begins to reveal its advantages: On ArXiv and GitHub, it is consistently better than all the baselines but only marginally worse than ALiBi when L ex \u2265 4096 on OpenWebText2.\nIt is worth mentioning that Sandwich is the first parameter-free RPE that truly makes use of distant token information beyond L tr = 512. To see this, notice that lower (better) perplexities occur at L ex > L tr = 512. The gradient analysis tool in \u00a75.1 further corroborates this in Figure 11, which reveals a receptive field pattern distinct from that of ALiBi and windowed attention. Even though Sandwich allocates about 60% of the total cumulative gradient on the most recent L tr = 512 tokens, distant tokens beyond L tr still contribute substantially Table 3: Perplexity Comparison on the OpenWebText2, GitHub, and ArXiv datasets. All models are trained for 50k steps with a training length of 512 and five random seeds. The models in the left section have parameterfree positional embeddings. In contrast, both KERPLE and T5 are equipped with learnable parameters. A fair comparison should only be made within the same section. x \u2020 means sandwich is statistically significantly better than x. x * means sandwich is statistically significantly worse than x. The test used is paired two-sided t-test with \u03b1 = 0.05. More details about the datasets and hyperparameters are provided in Appendix C and D.\nto the model prediction.\nWhy do ALiBi and windowed attention need to have their ERFs covered by L tr while Sandwich does not? To answer this question, we revisit Figure 9 and approximate (least-squared) the original temporal bias pattern using a log curve, which gives a snug fit 7 : y = \u22120.825 \u2022 log (1 + |m \u2212 n|) \u2212 0.8. Table 3 shows its language modeling performance under the \"smoothed\" column. Pictorially, the log curve decays relatively fast when two tokens are nearby and plateaus when the distance between them increases. In other words, tokens that are far away from the last one (m = 8192) share similar temporal biases, possibly leading to beneficial averaging and denoising effects. Note that the averaging effect does not come out of thin air during the extrapolation stage: The almost linear segment ranging from 1536 to 1792 suggests that Sandwich was trained to perform averaging within L tr ; it just needs to average over more historical tokens when it extrapolates to longer L ex . In contrast, ALiBi's linear bias lacks the middle ground to learn the averaging behavior: It either decays so fast that distant tokens are masked out or so slow that the ERF becomes much greater than L tr . The averaging hypothesis also explains why Sandwich, KERPLE, and T5's perplexities go up in Table 3 instead of continuing to decrease after some L ex (4096 on ArXiv for example): While averaging and denoising improve performance, doing so over too many historical tokens (very large L ex ) will reintroduce noises.", "publication_ref": ["b17", "b7"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Connection to KERPLE and T5", "text": "KERPLE (Chi et al., 2022) has the formulation of c \u2212 r 1 \u2022 log (1 + r 2 |m \u2212 n|). The \u22120.8 in our fitted log curve term can be absorbed by c, as Softmax is shift-invariant, and if we set r 1 = 0.825 and r 2 = 1, Sandwich becomes a special case of KERPLE. T5 (Raffel et al., 2020) adopts the logbinning strategy that assigns distinct bins to nearby tokens whereas distant tokens all share the same bin. In spirit, T5 treats distant tokens similarly to Sandwich. Figure 11  share a similar empirical receptive field pattern.", "publication_ref": ["b7", "b18"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper, we first establish the connection between ALiBi and windowed attention through their constructions and language modeling performance. We then develop a cumulative normalized gradient tool to measure the empirical receptive field. It shows that length extrapolation of ALiBi and windowed attention is possible when the training sequence length covers the empirical receptive field. It also reveals the models' limitation of not utilizing information beyond the training sequence length. Fortunately, this is overcome by our new relative positional embedding, Sandwich, which is simplified from the earliest proposed Sinusoidal positional embedding. Finally, Sandwich demonstrates a log-decaying temporal bias pattern similar to that previously seen in the design of KERPLE and T5, and such pattern is likely to be the secret to successful length extrapolation. Together these findings supports more effective design of future extrapolatable transformer language models.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "Although Sandwich, KERPLE, and T5 use information beyond training sequence length, their receptive fields still highly favor the most recent tokens. While this recency bias is beneficial to the modeling of human-written text, it is problematic in other scenarios.\nLet us consider the task of parity prediction: A model needs to predict whether a bit string has an even or odd number of ones. For example, the parity of [1, 1, 0, 1] is odd (or 1) and the parity of [1, 0, 1, 0] is even (or 0). Unlike human-written text, every single bit is equally important. Transformer language models with current RPEs still struggle on this simple task (Anil et al., 2022). Its difficulty can be explained by the recency bias effect that we described. Devising a new positional embedding or transformer model architecture that solves this problem is a promising direction for future work.", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}, {"heading": "Ethics Statement", "text": "Our work advances the understanding of positional embeddings adopted in almost all transformer models. In addition, our proposed new positional embedding significantly reduces energy consumption and training cost thanks to its length extrapolation property. Finally, our work lays the groundwork for developing future transformers that are greener and more cost-efficient enabled by improved length extrapolation. Inappropriate usage of our technique might have negative societal impacts. These include the ethical challenges of improper text generation and privacy issues inherent in the data collection process. These implications apply to any natural language processing research and are not unique to this specific work. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Results on OpenWebText2", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Efficient Inference", "text": "Although ALiBi might not be using token information further than L tr , it has the nice property of efficient inference (Press, 2022). Tables 1 and 4 show that ALiBi perplexities stay constant when L ex \u2265 2048. This suggests a cache window siz\u0113 w = 2048 for inference. The generation of the first w tokens remains the same, and we can still cache all q m , k m , and v m vectors for m \u2208 [1,2048]. When it comes to generating thew + 1-th token, we simply discard the first cached q 1 , k 1 , and v 1 and use the rest ofw \u2212 1 tokens along with the newly added token to perform self-attention. If we want to generate a length L ex text snippet, the complexity is O(w \u00d7 L ex ) instead of O(L 2 ex ). This complexity is also better than that of an APE model, which is O(w 2 \u00d7L ex ) since an APE model needs to completely re-encode the previousw vectors when generating new tokens following the firstw ones.\nWe implement the process discussed above to verify that ALiBi indeed allows for efficient inference. The results, along with ones for Sandwich, are presented in Table 5. Both ALiBi and Sandwich permit efficient inference by settingw = 2048. It is worth pointing out that the performance of Sandwich at L ex = 4096 becomes a bit worse compared to that in Table 3. This is more evidence that Sandwich is using longer than L tr token information.", "publication_ref": ["b17"], "figure_ref": [], "table_ref": ["tab_2", "tab_5"]}, {"heading": "C Scientific Artifacts", "text": "We use the gpt-neox library (Andonian et al., 2021) under Apache-2.0 license and the datasets (Gao et al., 2020) released by the authors of gpt-neox. The codebase and datasets (Table 6) are publicly released for research purposes. The steps taken to protect the privacy and anonymization are discussed in Gao et al. (2020) section 6 and 7. Finally, Gao et al. (2020) section 5 also discusses the distribution and statistics of the datasets used in this work.", "publication_ref": ["b10", "b10", "b10"], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "D Implementation Details", "text": "The configurations and hyperparameters are outlined in Table 7. The pretraining takes 5 hours on a single NVIDIA A-100 GPU. We do not tune any hyperparameters and just use the default ones.     B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? appendix C B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? appendix C B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? appendix C B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. appendix C C Did you run computational experiments? section 6 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? section 6 and appendix D The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\nC2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? section 6 and appendix D C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? section 6 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)? Not applicable. Left blank. D Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank. D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? No response.\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)? No response.\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? No response.\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No response.\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? No response.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Acknowledgment", "text": "The authors acknowledge the support from Boeing (2019-STU-PA-259), Amazon (CC ADV 00474341 2021 TR), NSF MRI Award 1919452, and Princeton Research Computing.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "ETC: Encoding long and structured inputs in transformers", "journal": "", "year": "2020", "authors": "Joshua Ainslie; Santiago Ontanon; Chris Alberti; Vaclav Cvicek; Zachary Fisher; Philip Pham; Anirudh Ravula; Sumit Sanghai; Qifan Wang; Li Yang"}, {"ref_id": "b1", "title": "Tri Songz, Wang Phil, and Samuel Weinbach. 2021. GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch", "journal": "", "year": "", "authors": "Alex Andonian; Quentin Anthony; Stella Biderman; Sid Black; Preetham Gali; Leo Gao; Eric Hallahan; Josh Levy-Kramer; Connor Leahy; Lucas Nestler; Kip Parker; Michael Pieler; Shivanshu Purohit"}, {"ref_id": "b2", "title": "Exploring length generalization in large language models", "journal": "", "year": "2022", "authors": "Cem Anil; Yuhuai Wu; Anders Johan Andreassen; Aitor Lewkowycz; Vedant Misra; Vinay Venkatesh Ramasesh; Ambrose Slone; Guy Gur-Ari; Ethan Dyer; Behnam Neyshabur"}, {"ref_id": "b3", "title": "Computing receptive fields of convolutional neural networks", "journal": "", "year": "2019", "authors": "Andre Araujo; Wade Norris; Jack Sim"}, {"ref_id": "b4", "title": "", "journal": "", "year": "", "authors": "Jimmy Lei Ba; Jamie Ryan Kiros; Geoffrey E Hin"}, {"ref_id": "b5", "title": "Longformer: The long-document transformer", "journal": "", "year": "2020", "authors": "Iz Beltagy; Matthew E Peters; Arman Cohan"}, {"ref_id": "b6", "title": "Dilated recurrent neural networks", "journal": "Curran Associates, Inc", "year": "2017", "authors": "Shiyu Chang; Yang Zhang; Wei Han; Mo Yu; Xiaoxiao Guo; Wei Tan; Xiaodong Cui; Michael Witbrock; A Mark; Thomas S Hasegawa-Johnson;  Huang"}, {"ref_id": "b7", "title": "Kerple: Kernelized relative positional embedding for length extrapolation", "journal": "", "year": "2022", "authors": "Ta-Chung Chi; Ting-Han Fan; J Peter; Alexander I Ramadge;  Rudnicky"}, {"ref_id": "b8", "title": "Deformable convolutional networks", "journal": "", "year": "2017", "authors": "Jifeng Dai; Haozhi Qi; Yuwen Xiong; Yi Li; Guodong Zhang; Han Hu; Yichen Wei"}, {"ref_id": "b9", "title": "An image is worth 16x16 words: Transformers for image recognition at scale", "journal": "", "year": "2021", "authors": "Alexey Dosovitskiy; Lucas Beyer; Alexander Kolesnikov; Dirk Weissenborn; Xiaohua Zhai; Thomas Unterthiner; Mostafa Dehghani; Matthias Minderer; Georg Heigold; Sylvain Gelly; Jakob Uszkoreit; Neil Houlsby"}, {"ref_id": "b10", "title": "The Pile: An 800gb dataset of diverse text for language modeling", "journal": "", "year": "2020", "authors": "Leo Gao; Stella Biderman; Sid Black; Laurence Golding; Travis Hoppe; Charles Foster; Jason Phang; Horace He; Anish Thite; Noa Nabeshima; Shawn Presser; Connor Leahy"}, {"ref_id": "b11", "title": "GMAT: global memory augmentation for transformers", "journal": "", "year": "2006", "authors": "Ankit Gupta; Jonathan Berant"}, {"ref_id": "b12", "title": "Understanding the effective receptive field in deep convolutional neural networks", "journal": "Curran Associates Inc", "year": "2016", "authors": "Wenjie Luo; Yujia Li; Raquel Urtasun; Richard Zemel"}, {"ref_id": "b13", "title": "Recurrent neural network based language model", "journal": "Makuhari", "year": "2010-01", "authors": "Tomas Mikolov; Martin Karafi\u00e1t; Lukas Burget"}, {"ref_id": "b14", "title": "Context dependent recurrent neural network language model", "journal": "", "year": "2012", "authors": "Tomas Mikolov; Geoffrey Zweig"}, {"ref_id": "b15", "title": "Wavenet: A generative model for raw audio", "journal": "", "year": "2016", "authors": "Aaron Van Den Oord; Sander Dieleman; Heiga Zen; Karen Simonyan; Oriol Vinyals; Alex Graves; Nal Kalchbrenner; Andrew Senior; Koray Kavukcuoglu"}, {"ref_id": "b16", "title": "Ofir Press. 2022. The use case for relative position embeddings", "journal": "", "year": "", "authors": ""}, {"ref_id": "b17", "title": "Train short, test long: Attention with linear biases enables input length extrapolation", "journal": "", "year": "2022", "authors": "Ofir Press; Noah Smith; Mike Lewis"}, {"ref_id": "b18", "title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "Journal of Machine Learning Research", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b19", "title": "Chiyuan Zhang, and Alexey Dosovitskiy. 2021. Do vision transformers see like convolutional neural networks?", "journal": "", "year": "", "authors": "Maithra Raghu; Thomas Unterthiner; Simon Kornblith"}, {"ref_id": "b20", "title": "Matthias Gall\u00e9, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model", "journal": "", "year": "", "authors": "Angela Teven Le Scao; Christopher Fan; Ellie Akiki; Suzana Pavlick; Daniel Ili\u0107; Roman Hesslow; Alexandra Sasha Castagn\u00e9; Fran\u00e7ois Luccioni;  Yvon"}, {"ref_id": "b21", "title": "Roformer: Enhanced transformer with rotary position embedding", "journal": "", "year": "2021", "authors": "Jianlin Su; Yu Lu; Shengfeng Pan; Bo Wen; Yunfeng Liu"}, {"ref_id": "b22", "title": "Attention is all you need. Advances in neural information processing systems", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b23", "title": "Tener: adapting transformer encoder for named entity recognition", "journal": "", "year": "2019", "authors": "Hang Yan; Bocao Deng; Xiaonan Li; Xipeng Qiu"}, {"ref_id": "b24", "title": "Multi-scale context aggregation by dilated convolutions", "journal": "", "year": "2016-05-02", "authors": "Fisher Yu; Vladlen Koltun"}, {"ref_id": "b25", "title": "Big bird: Transformers for longer sequences. CoRR, abs", "journal": "", "year": "2007", "authors": "Manzil Zaheer; Guru Guruganesh; Avinava Dubey; Joshua Ainslie; Chris Alberti; Santiago Onta\u00f1\u00f3n; Philip Pham; Anirudh Ravula; Qifan Wang; Li Yang; Amr Ahmed"}, {"ref_id": "b26", "title": "Recurrent neural network regularization", "journal": "", "year": "2014", "authors": "Wojciech Zaremba; Ilya Sutskever; Oriol Vinyals"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: ALiBi. For a transformer language model with H attention heads, the range of h is n \u2022 8 H , where n = {1 . . . H}. Left = self-attention matrix, right = temporal biases matrix.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "\u2212", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: We always evaluate the perplexities of the 5 tokens numbered from 1 to 5. The upper brackets represent L ex = 5. The lower brackets represent L ex = 3. This formulation ensures the same 5 tokens are always evaluated with different numbers of previous tokens.", "figure_data": ""}, {"figure_label": "45", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :Figure 5 :45Figure 4: Cumulative normalized gradient on ArXiv when predicting the next (2048-th) token.", "figure_data": ""}, {"figure_label": "67", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 6 :Figure 7 :67Figure 6: Cumulative normalized gradient of Rotary on ArXiv when predicting the last (2048-th) token with L tr = 512.", "figure_data": ""}, {"figure_label": "8910", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 8 :Figure 9 :Figure 10 :8910Figure 8: The visualization of Eq. (6) when the compression ratio h = 8 andd = 128.", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 11 :11Figure 11: Cumulative normalized gradient of Sandwich, Smoothed Sandwich, KERPLE, and T5 on ArXiv when predicting the last (2048-th) token with L tr = 512.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "shows that both are now able to maintain stable perplexities.", "figure_data": "L exWindowed Attention w = 160 Arxiv OpenWebText2 Arxiv OpenWebText2 Shift all h by \u2206 = 62048 4.415.26.219.94096 6.219.86.219.98192 6.219.96.219.9"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "includes the three experiments conducted in \u00a74 on OpenWebText2. Their corresponding receptive field plots are shown in Figure 12 and 13.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "19.0 19.5 20.0 20.5 20.5 32.7 22.2 19.7 19.7 20.5 25.3  23.7 23.1 24.0 22.9 21.9 1024 21.6 19.3 19.6 24.8 232 232 32.8 23.2 24.9 146 232 25.3 23.7 23.2 137 234 353 2048 21.6 19.7 20.5 29.3 299 299 32.8 23.2 24.9 165 299 25.3 23.7 23.2 137 236 408 4096 21.6 19.7 20.5 29.4 299 299 32.9 23.2 24.", "figure_data": "L ex\u2206:-3 0Shift all h by \u2206 2 468Same h for all heads h:0 2 4 68Windowed Attention Size w w:40 80 100 120 160 320512 18.6 9 165 29925.3 23.7 23.2 137 236 4088192 21.6 19.7 20.5 29.4 299 299 32.9 23.2 24.9 165 29925.3 23.7 23.2 137 236 408"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "The three experiments on the OpenWebText2 dataset.", "figure_data": "1.01.0 0.940.80.80.0 0.2 0.4 0.60500 100 40 80 120 160 3201024153620480.2 0.4 0.60-3 0 2 4 6 8500102415362048Figure 12: Cumulative normalized gradient on OpenWebText2 when predicting the last (2048-th) token. Windowed Attention Size w =Figure 13: Cumulative normalized gradient on OpenWebText2 when predicting the last (2048-th) token. Shift all h by \u2206 =L exOpenWebText2 Sandwich ALiBi Sandwich ALiBi Sandwich ALiBi Arxiv GitHub409623.923.55.315.592.793.01819224.123.55.355.592.813.0116384 24.123.55.355.592.813.01"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Efficient Inference withw = 2048.", "figure_data": "OpenWebText2GitHubArXivRaw Size66.77 GB95.16 GB 56.21 GBTypeInternetCodingAcademic"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Dataset Overview. Raw Size is the size before any up-or down-sampling.", "figure_data": "# LayersHidden Size # Attention Heads Train Seq. Len.# Trainable Params.126412512162MOptimizerBatch SizeTrain StepsPrecision# Trainable Params. for RPEsAdam (lr 6e-4)3250,000bfloat160"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "A1. Did you describe the limitations of your work?", "figure_data": ": 162M Model Configurations.13534"}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u2212 1 2 ' \u00d7 * \" ) \" * # ) \" * # ) # * $ ) \" * $ ) # * $ ) $ 0 1 0 2 1 0 3 0 1 2 * % ) \" * % ) # * % ) $ * % ) %", "formula_coordinates": [1.0, 310.43, 220.29, 202.87, 78.27]}, {"formula_id": "formula_1", "formula_text": "* \" ) \" * # ) \" * # ) # * $ ) \" * $ ) # * $ ) $ 0 0 0 \u221e 0 0 \u221e 0 0 \u221e * % ) \" * % ) # * % ) $ * % ) %", "formula_coordinates": [1.0, 310.43, 384.36, 202.87, 78.27]}, {"formula_id": "formula_2", "formula_text": "q m = W q e m , k m = W k e m , v m = W v e m , where W q , W k , W v \u2208 R d H", "formula_coordinates": [3.0, 70.46, 100.03, 210.72, 47.53]}, {"formula_id": "formula_3", "formula_text": "l mn = q m , k n , if m \u2265 n \u2212 inf, otherwise", "formula_coordinates": [3.0, 111.63, 181.07, 135.53, 36.81]}, {"formula_id": "formula_4", "formula_text": "a m,n = exp(l m,n / d/H) L i=1 exp(l m,i / d/H)(1)", "formula_coordinates": [3.0, 107.59, 250.37, 181.54, 30.26]}, {"formula_id": "formula_5", "formula_text": "(h) q , W(", "formula_coordinates": [3.0, 207.81, 291.84, 36.81, 15.14]}, {"formula_id": "formula_6", "formula_text": "h) k , W (h) v ), vectors (q (h) m , k (h) m , v (h) m , o (h) m", "formula_coordinates": [3.0, 70.58, 291.84, 219.91, 31.39]}, {"formula_id": "formula_7", "formula_text": "(h) mn , a (h)", "formula_coordinates": [3.0, 70.86, 308.09, 219.64, 29.3]}, {"formula_id": "formula_8", "formula_text": "(h) m at position m for head h is: o (h) m = L n=1 a (h) m,n v (h) n", "formula_coordinates": [3.0, 106.79, 365.12, 133.59, 62.32]}, {"formula_id": "formula_9", "formula_text": "W o \u2208 R d\u00d7d to obtain o m \u2208 R d : o m = W o (o (1) m \u2295 o (2) m \u2295 \u2022 \u2022 \u2022 \u2295 o (H) m )", "formula_coordinates": [3.0, 70.86, 450.57, 218.28, 59.76]}, {"formula_id": "formula_10", "formula_text": "p = Softmax(V o m )(2)", "formula_coordinates": [3.0, 134.44, 608.84, 154.69, 13.71]}, {"formula_id": "formula_11", "formula_text": "l mn = q m , k n \u2212 1 2 h (m \u2212 n), if m \u2265 n \u2212 inf, otherwise (3) The range of h is n \u2022 8", "formula_coordinates": [3.0, 70.52, 717.58, 218.62, 63.93]}, {"formula_id": "formula_12", "formula_text": "l mn = q m , k n , if n + w > m \u2265 n \u2212 inf, otherwise", "formula_coordinates": [3.0, 326.83, 327.01, 175.72, 36.81]}, {"formula_id": "formula_13", "formula_text": "PPL = exp 1 N N i=1 \u2212 log p i ,", "formula_coordinates": [3.0, 344.9, 471.1, 140.78, 34.7]}, {"formula_id": "formula_14", "formula_text": "s m = g m 2 Lex n=1 g n 2 ,", "formula_coordinates": [4.0, 370.24, 635.19, 90.09, 30.06]}, {"formula_id": "formula_15", "formula_text": "c m = Lex n=m s n , 0 \u2264 c m \u2264 1,", "formula_coordinates": [4.0, 350.01, 710.02, 130.54, 34.18]}, {"formula_id": "formula_16", "formula_text": "p m,2i = sin m 10000 2i/d , p m,2i+1 = cos m 10000 2i/d", "formula_coordinates": [5.0, 351.72, 582.31, 127.14, 50.71]}, {"formula_id": "formula_17", "formula_text": "W q (e m + p m )) (W k (e n + p n )) = (4) e m W q W k e n semantic info. + e m W q W k p n + p m W q W k e n + p m W q W k p n mixture of semantic and positional info. \u2248 e m W q W k e n semantic info. + p m p n positional info.", "formula_coordinates": [6.0, 80.45, 322.56, 443.97, 48.82]}, {"formula_id": "formula_18", "formula_text": "p m p n =d /2 i=1 sin m 10000 2i/d sin n 10000 2i/d + cos m 10000 2i/d cos n 10000 2i/d =d /2 i=1 cos m \u2212 n 10000 2i/d", "formula_coordinates": [6.0, 303.31, 425.34, 225.28, 106.48]}, {"formula_id": "formula_19", "formula_text": "p m p n \u2212d/2 h (6)", "formula_coordinates": [6.0, 386.95, 719.75, 137.47, 26.05]}], "doi": "10.18653/v1/2020.emnlp-main.19"}