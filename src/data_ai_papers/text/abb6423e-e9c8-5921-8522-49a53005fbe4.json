{"title": "Improving sparse word similarity models with asymmetric measures", "authors": "Jean Mark Gawron", "pub_date": "", "abstract": "We show that asymmetric models based on Tversky (1977) improve correlations with human similarity judgments and nearest neighbor discovery for both frequent and middle-rank words. In accord with Tversky's discovery that asymmetric similarity judgments arise when comparing sparse and rich representations, improvement on our two tasks can be traced to heavily weighting the feature bias toward the rarer word when comparing high-and midfrequency words.", "sections": [{"heading": "Introduction", "text": "A key assumption of most models of similarity is that a similarity relation is symmetric. This assumption is foundational for some conceptions, such as the idea of a similarity space, in which similarity is the inverse of distance; and it is deeply embedded into many of the algorithms that build on a similarity relation among objects, such as clustering algorithms. The symmetry assumption is not, however, universal, and it is not essential to all applications of similarity, especially when it comes to modeling human similarity judgments. Citing a number of empirical studies, Tversky (1977) calls symmetry directly into question, and proposes two general models that abandon symmetry. The one most directly related to a large body of word similarity work that followed is what he calls the ratio model, which defines sim(a, b) as:\nf (A \u2229 B) f (A \u2229 B) + \u03b1f (A\\B) + \u03b2f (B\\A)(1)\nHere A and B represent feature sets for the objects a and b respectively; the term in the numerator is a function of the set of shared features, a measure of similarity, and the last two terms in the denominator measure dissimilarity: \u03b1 and \u03b2 are real-number weights; when \u03b1 = \u03b2, symmetry is abandoned.\nTo motivate such a measure, Tversky presents experimental data with asymmetric similarity results, including similarity comparisons of countries, line drawings of faces, and letters. Tversky shows that many similarity judgment tasks have an inherent asymmetry; but he also argues, following Rosch (1975), that certain kinds of stimuli are more naturally used as foci or standards than others. Goldstone (in press) summarizes the results succinctly: \"Asymmetrical similarity occurs when an object with many features is judged as less similar to a sparser object than vice versa; for example, North Korea is judged to be more like China than China is [like] North Korea.\" Thus, one source of asymmetry is the comparison of sparse and dense representations.\nThe relevance of such considerations to word similarity becomes clear when we consider that for many applications, word similarity measures need to be well-defined when comparing very frequent words with infrequent words. To make this concrete, let us consider a word representation in the word-as-vector paradigm (Lee, 1997;Lin, 1998), using a dependency-based model. Suppose we want to measure the semantic similarity of boat, rank 682 among the nouns in the BNC corpus studied below, which has 1057 nonzero dependency features based on 50 million words of data, with dinghy, rank 6200, which has only 113 nonzero features. At the level of the vector representations we are using, these are events of very different dimensionality; that is, there are ten times as many features in the representation of boat as there are in the representation of dinghy. If in Tversky/Rosch terms, the more frequent word is also a more likely focus, then this is exactly the kind of situation in which asymmetric similarity judgments will arise. Below we show that an asymmetric measure, using \u03b1 and \u03b2 biased in favor of the less frequent word, greatly improves the performance of a dependency-based vector model in capturing human similarity judgments.\nBefore presenting these results, it will be helpful to slightly reformulate and slightly generalize Tversky's ratio model. The reformulation will allow us to directly draw the connection between the ratio model and a set of similarity measures that have played key roles in the similarity literature. First, since Tversky has primarily additive f in mind, we can reformulate f(A \u2229 B) as follows\nf(A \u2229 B) = f \u2208A\u2229B wght(f )(2)\nNext, since we are interested in generalizing from sets of features, to real-valued vectors of features, w 1 , w 2 , we define\n\u03c3 SI (w 1 , w 2 ) = f \u2208w 1 \u2229w 2 SI(w 1 [f ], w 2 [f ]).\n(3) Here SI is some numerical operation on realnumber feature values (SI stands for shared information). If the operation is MIN and w 1 [f ] and w 2 [f ] both contain the feature weights for f , then\nf \u2208A\u2229B wght(f )= \u03c3 MIN (w 1 , w 2 ) = f \u2208w 1 \u2229w 2 MIN(w 1 [f ], w 2 [f ]),\nso with SI set to MIN, Equation (3) includes Equation (2) as a special case. Similarly, \u03c3(w 1 , w 1 ) represents the summed feature weights of w 1 , and therefore,\nf (w 1 \\ w 2 ) = \u03c3(w 1 , w 1 ) \u2212 \u03c3(w 1 , w 2 )\nIn this generalized form, then, (1) becomes\n\u03c3(w 1 ,w 2 ) \u03c3(w 1 ,w 2 )+\u03b1[\u03c3(w 1 ,w 1 )\u2212\u03c3(w 1 ,w 2 )]+\u03b2[\u03c3(w 2 ,w 2 )\u2212\u03c3(w 1 ,w 2 )] = \u03c3(w 1 ,w 2 ) \u03b1\u03c3(w 1 ,w 1 )+\u03b2\u03c3(w 2 ,w 2 )+\u03c3(w 1 ,w 2 )\u2212(\u03b1+\u03b2)\u03c3(w 1 ,w 2 ) (4) Thus, if \u03b1 + \u03b2 = 1, Tversky's ratio model be- comes simply: sim(w 1 , w 2 ) = \u03c3(w 1 ,w 2 ) \u03b1\u03c3(w 1 ,w 1 )+(1\u2212\u03b1)\u03c3(w 2 ,w 2 ) (5)\nThe computational advantage of this reformulation is that the core similarity operation \u03c3(w 1 , w 2 ) is done on what is generally only a small number of shared features, and the \u03c3(w i , w i ) calculations (which we will call self-similarities), can be computed in advance. Note that sim(w 1 , w 2 ) is symmetric if and only if \u03b1 = 0.5. When \u03b1 > 0.5, sim(w 1 , w2) is biased in favor of w 1 as the referent; When \u03b1 < 0.5, sim(w 1 , w2) is biased in favor of w 2 .\nConsider four similarity functions that have played important roles in the literature on similarity:\nDICE PROD(w 1 , w 2 ) = 2 * w 1 \u2022w 2 w 1 2 + w 2 2 DICE \u2020 (w 1 , w 2 ) = 2 * f \u2208w 1 \u2229w 2 min(w 1 [f ], w 2 [f ]) w 1 [f ]+ w 2 [f ] LIN(w 1 , w 2 ) = f \u2208w 1 \u2229w 2 w 1 [f ]+ w 2 [f ] w 1 [f ]+ w 2 [f ]\nCOS(w 1 , w 2 ) = DICE PROD applied to unit vectors (6) The function DICE PROD is not well known in the word similarity literature, but in the data mining literature it is often just called Dice coefficient, because it generalized the set comparison function of Dice (1945). Observe that cosine is a special case of DICE PROD. DICE \u2020 was introduced in Curran ( 2004) and was the most successful function in his evaluation. Since LIN was introduced in Lin (1998); several different functions have born that name. The version used here is the one used in Curran (2004).\nThe three distinct functions in Equation 6have a similar form. In fact, all can be defined in terms of \u03c3 functions differing only in their SI operation.\nLet \u03c3 SI be a shared feature sum for operation SI, as defined in Equation (3). We define the Tverskynormalized version of \u03c3 SI , written T SI , as: 1\nT SI (w 1 , w 2 ) = 2 \u2022 \u03c3 SI (w 1 , w 2 ) \u03c3 SI (w 1 , w 1 ) + \u03c3 SI (w 2 , w 2 ) (7)\nNote that T SI is just the special case of Tversky's ratio model ( 5) in which \u03b1 = 0.5 and the similarity measure is symmetric. We define three SI operations \u03c3 PROD 2 , \u03c3 MIN , and \u03c3 AVG as follows:\nSI \u03c3 SI (w 1 , w 2 ) PROD f \u2208w 1 \u2229w 2 w 1 [f ] * w 2 [f ] AVG f \u2208w 1 \u2229w 2 w 1 [f ]+w 2 [f ] 2 MIN f \u2208w 1 \u2229w 2 MIN(w 1 [f ], w 2 [f ])\n1 Paralleling ( 7) is Jaccard-family normalization:\n\u03c3JACC(w1, w2) = \u03c3(w1, w2) \u03c3(w1, w1) + \u03c3(w2, w2) \u2212 \u03c3(w1, w2)\nIt is easy to generalize the result from van Rijsbergen (1979) for the original set-specific versions of Dice and Jaccard, and show that all of the Tversky family functions discussed above are monotonic in Jaccard.\n2 \u03c3PROD, of course, is dot product.\nThis yields the three similarity functions cited above:\nDICE PROD(w 1 , w 2 ) = T PROD (w 1 , w 2 ) DICE \u2020 (w 1 , w 2 ) = T MIN (w 1 , w 2 ) LIN(w 1 , w 2 ) = T AVG (w 1 , w 2 ) (8)\nThus, all three of these functions are special cases of symmetric ratio models. Below, we investigate asymmetric versions of all three, which we write as T \u03b1,SI (w 1 , w 2 ), defined as:\n\u03c3 SI (w 1 , w 2 ) \u03b1 \u2022 \u03c3 SI (w 1 , w 1 ) + (1 \u2212 \u03b1) \u2022 \u03c3 SI (w 2 , w 2 ) (9\n)\nFollowing Lee (1997), who investigates a different family of asymmetric similarity functions, we will refer to these as \u03b1-skewed measures. We also will look at a rank-biased family of measures: R \u03b1,SI (w 1 , w 2 ) = T \u03b1,SI (w h , w l ) where w l = arg min w\u2208{w 1 ,w 2 } Rank(w) w h = arg max w\u2208{w 1 ,w 2 } Rank(w) (10) Here, T \u03b1,SI (w h , w l ) is as defined in ( 9), and the \u03b1weighted word is always the less frequent word. For example, consider comparing the 100-feature vector for dinghy to the 1000 feature vector for boat: if \u03b1 is high, we give more weight to the proportion of dinghy's features that are shared than we give to the proportion of boat's features that are shared.\nIn the following sections we present data showing that the performance of a dependency-based similarity system in capturing human similarity judgments can be greatly improved with rankbias and \u03b1-skewing. We will investigate the three asymmetric functions defined above. 3 We argue that the advantages of rank bias are tied to improved similarity estimation when comparing vectors of very different dimensionality. We then turn to the problem of finding a word's nearest semantic neighbors. The nearest neighbor problem is a rather a natural ground in which to try out ideas on asymmetry, since the nearest neighbor relation is itself not symmetrical. We show that \u03b1-skewing can be used to improve the quality of nearest neighbors found for both high-and midfrequency words.\n3 Interestingly, Equation ( 9) does not yield an asymmetric version of cosine. Plugging unit vectors into the \u03b1-skewed version of DICE PROD still leaves us with a symmetric function (COS), whatever the value of \u03b1.", "publication_ref": ["b18", "b16", "b10", "b12", "b4", "b2", "b19", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Systems", "text": "1. We parsed the BNC with the Malt Dependency parser (Nivre, 2003) and the Stanford parser (Klein and Manning, 2003), creating two dependency DBs, using basically the design in Lin (1998), with features weighted by PMI (Church and Hanks, 1990).\n2. For each of the 3 rank-biased similarity systems (R \u03b1,SI ) and cosine, we computed correlations with human judgments for the pairs in 2 standard wordsets: the combined Miller-Charles/Rubenstein-Goodenough word sets (Miller and Charles, 1991;Rubenstein and Goodenough, 1965) and the Wordsim 353 word set (Finkelstein et al., 2002), as well as to a subset of the Wordsim set restricted to reflect semantic similarity judgments, which we will refer to as Wordsim 201.\n3. For each of 3 \u03b1-skewed similarity systems (T \u03b1,SI ) and cosine, we found the nearest neighbor from among BNC nouns (of any rank) for the 10,000 most frequent BNC nouns using the the dependency DB created in step 2.\n4. To evaluate of the quality of the nearest neighbors pairs found in Step 4, we scored them using the Wordnet-based Personalized Pagerank system described in Agirre (2009) (UKB), a non distributional WordNet based measure, and the best system in Table 1.", "publication_ref": ["b15", "b9", "b1", "b14", "b17", "b5", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Human correlations", "text": "Table 1 presents the Spearman's correlation with human judgments for Cosine, UKB, and our 3 \u03b1-skewed models using Malt-parser based vectors applied to the combined Miller-Charles/Rubenstein-Goodenough word sets, the Wordsim 353 word set, and the Wordsim 202 word set. The first of each of the column pairs is a symmetric system, and the second a rank-biased variant, based on Equation (10). In all cases, the biased system improves on the performance of its symmetric counterpart; in the case of DICE \u2020 and DICE PROD, that improvement is enough for the biased system to outperform cosine, the best of the symmetric distributionally based systems. The value .97 was chosen for \u03b1 because it produced the best \u03b1-system on the MC/RG corpus. That value MC/RG Wdsm201 Wdsm353 \u03b1 = .5 \u03b1 = .97 \u03b1 = .5 \u03b1 = .97 \u03b1 = .5 \u03b1 = .  is probably probably an overtrained optimum. The point is that \u03b1-skewing always helps: For all three systems, the improvement shown in raising \u03b1 from .5 to whatever the optimum is is monotonic. This is shown in Figure 1. Table 2 shows very similar results using the Stanford parser, demonstrating the pattern is not limited to a single parsing model. In Table 3, we list the pairs whose reranking on the MC/RG dataset contributed most to the improvement of the \u03b1 = .9 system over the default \u03b1 = .5 system. In the last column an approximation of the amount of correlation improvement provided by that pair (\u03b4): 4 Note the 3 of the 5 items contributing the most improvement this system were pairs with a large difference in rank. Choosing \u03b1 = .9, weights recall toward the rarer word. We conjecture that the reason this helps is Tversky's principle: It is natural to use the sparser  ", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": ["tab_2"]}, {"heading": "Nearest neighbors", "text": "Figure 2 gives the results of our nearest neighbor study on the BNC for the case of DICE PROD. The graphs for the other two \u03b1-skewed systems are nearly identical, and are not shown due to space limitations. The target word, the word whose nearest neighbor is being found, always receives the weight 1 \u2212 \u03b1. The x-axis shows target word rank; the y-axis shows the average UKB similarity scores assigned to nearest neighbors every 50 ranks. All the systems show degraded nearest neighbor quality as target words grow rare, but at lower ranks, the \u03b1 = .04 nearest neighbor system fares considerably better than the symmetric \u03b1 = .50 system; the line across the bottom tracks the score of a system with randomly generated nearest neighbors. The symmetric DICE PROD system is as an excellent nearest neighbor system at high ranks but drops below the \u03b1 = .04 system at around rank 3500. We see that the \u03b1 = .8 system is even better than the symmetric system at high ranks, but degrades much more quickly.\nWe explain these results on the basis of the principle developed for the human correlation data: To reflect natural judgments of similarity for comparisons of representations of differing sparseness, \u03b1 should be tipped toward the sparser representation.\nThus, \u03b1 = .80 works best for high rank target words, because most nearest neighbor candi-MC/RG Wdsm201 Wdsm353 \u03b1 = .5 opt opt \u03b1 \u03b1 = .5 opt opt \u03b1 \u03b1 = .  dates are less frequent, and \u03b1 = .8 tips the balance toward the nontarget words. On the other hand, when the target word is a low ranking word, a high \u03b1 weight means it never receives the highest weight, and this is disastrous, since most good candidates are higher ranking. Conversely, \u03b1 = .04 works better.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Previous work", "text": "The debt owed to Tversky (1977) has been made clear in the introduction. Less clear is the debt owed to Jimenez et al. (2012), which also proposes an asymmetric similarity framework based on Tversky's insights. Jimenez et al. showed the continued relevance of Tversky's work.\nMotivated by the problem of measuring how well the distribution of one word w 1 captures the distribution of another w 2 , Weeds and Weir (2005) also explore asymmetric models, expressing similarity calculations as weighted combinations of several variants of what they call precision and recall. Some of their models are also Tverskyan ratio models. To see this, we divide (9) everywhere by \u03c3(w 1 , w 2 ):\nT SI (w 1 , w 2 ) = 1 \u03b1\u2022\u03c3(w 1 ,w 1 ) \u03c3(w 1 ,w 2 ) + (1\u2212\u03b1)\u2022\u03c3(w 2 ,w 2 ) \u03c3(w 1 ,w 2 )\nIf the SI is MIN, then the two terms in the denominator are the inverses of what W&W call difference-weighted precision and recall:\nPREC(w 1 , w 2 ) = \u03c3MIN (w 1 ,w 2 ) \u03c3MIN (w 1 ,w 1 ) REC(w 1 , w 2 ) = \u03c3MIN (w 1 ,w 2 ) \u03c3MIN (w 2 ,w 2 ) , So for T MIN , ( 9) can be rewritten:\n1 \u03b1 PREC(w 1 ,w 2 ) + 1\u2212\u03b1 REC(w 1 ,w 2 )\nThat is, T MIN is a weighted harmonic mean of precision and recall, the so-called weighted Fmeasure (Manning and Sch\u00fctze, 1999). W&W's additive precision/recall models appear not to be Tversky models, since they compute separate sums for precision and recall from the f \u2208 w 1 \u2229 w 2 , one using w 1 [f ], and one using w 2 [f ].\nLong before Weed and Weir, Lee (1999) proposed an asymmetric similarity measure as well. Like Weeds and Weir, her perspective was to calculate the effectiveness of using one distribution as a proxy for the other, a fundamentally asymmetric problem. For distributions q and r, Lee's \u03b1-skew divergence takes the KL-divergence of a mixture of q and r from q, using the \u03b1 parameter to define the proportions in the mixture.", "publication_ref": ["b18", "b8", "b20", "b13", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We have shown that Tversky's asymmetric ratio models can improve performance in capturing human judgments and produce better nearest neighbors. To validate these very preliminary results, we need to explore applications compatible with asymmetry, such as the TOEFL-like synonym discovery task in Freitag et al. (2005), and the PP-attachment task in Dagan et al. (1999).", "publication_ref": ["b6", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "This work reported here was supported by NSF CDI grant # 1028177.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "A study on similarity and relatedness using distributional and wordnetbased approaches", "journal": "", "year": "2009", "authors": "E Agirre; E Alfonseca; K Hall; J Kravalova; M Pasca; A Soroa"}, {"ref_id": "b1", "title": "Word association norms, mutual information, and lexicography. Computational linguistics", "journal": "", "year": "1990", "authors": "K W Church; P Hanks"}, {"ref_id": "b2", "title": "From Distributional to Semantic Similarity", "journal": "", "year": "2004", "authors": "J R Curran"}, {"ref_id": "b3", "title": "Similaritybased models of word cooccurrence probabilities", "journal": "", "year": "1999", "authors": "I Dagan; L Lee; F C N Pereira"}, {"ref_id": "b4", "title": "Measures of the amount of ecologic association between species", "journal": "Ecology", "year": "1945", "authors": "L R Dice"}, {"ref_id": "b5", "title": "Placing search in context: The concept revisited", "journal": "ACM Transactions on Information Systems", "year": "2002", "authors": "L Finkelstein; E Gabrilovich; Yossi Matias; Ehud Rivlin; Zach Solan; Gadi Wolfman; Eytan Rup"}, {"ref_id": "b6", "title": "New experiments in distributional representations of synonymy", "journal": "", "year": "2005", "authors": "D Freitag; M Blume; J Byrnes; E Chow; S Kapadia; R Rohwer; Z Wang"}, {"ref_id": "b7", "title": "MIT Encylcopedia of Cognitive Sciences", "journal": "MIT Press", "year": "", "authors": "R L Goldstone"}, {"ref_id": "b8", "title": "Soft cardinality: A parameterized similarity function for text comparison", "journal": "Association for Computational Linguistics", "year": "2012", "authors": "S Jimenez; C Becerra; A Gelbukh"}, {"ref_id": "b9", "title": "Fast exact inference with a factored model for natural language parsing", "journal": "MIT Press", "year": "2002", "authors": "D Klein; Christopher D Manning"}, {"ref_id": "b10", "title": "Similarity-based approaches to natural language processing", "journal": "", "year": "1997", "authors": "L Lee"}, {"ref_id": "b11", "title": "Measures of distributional similarity", "journal": "Association for Computational Linguistics", "year": "1999", "authors": "L Lee"}, {"ref_id": "b12", "title": "Automatic retrieval and clustering of similar words", "journal": "Association for Computational Linguistics", "year": "1998", "authors": "D Lin"}, {"ref_id": "b13", "title": "Foundations of statistical natural language processing", "journal": "MIT Press", "year": "1999", "authors": "C D Manning; H Sch\u00fctze"}, {"ref_id": "b14", "title": "Contextual correlates of semantic similarity. Language and Cognitive Processes", "journal": "", "year": "1991", "authors": "G A Miller; W G Charles"}, {"ref_id": "b15", "title": "An efficient algorithm for projective dependency parsing", "journal": "", "year": "2003", "authors": "J Nivre"}, {"ref_id": "b16", "title": "Family resemblances: Studies in the internal structure of categories", "journal": "Cognitive psychology", "year": "1975", "authors": "E Rosch; C B Mervis"}, {"ref_id": "b17", "title": "Contextual correlates of synonymy", "journal": "Communications of the ACM", "year": "1965", "authors": "H Rubenstein; J B Goodenough"}, {"ref_id": "b18", "title": "Features of similarity", "journal": "Psychological Review", "year": "1977", "authors": "A Tversky"}, {"ref_id": "b19", "title": "Information retrieval", "journal": "Butterworth-Heinemann", "year": "1979", "authors": "C J Van Rijsbergen"}, {"ref_id": "b20", "title": "Co-occurrence retrieval: A flexible framework for lexical distributional similarity", "journal": "Computational Linguistics", "year": "2005", "authors": "J Weeds; D Weir"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Scores monotonically increase with \u03b1", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: UKB evaluation scores for nearest neighbor pairs across word ranks, sampled every 50 ranks.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ": Pairs contributing the biggest improve-ment, MC/RG word setrepresentation as the focus in the comparison."}], "formulas": [{"formula_id": "formula_0", "formula_text": "f (A \u2229 B) f (A \u2229 B) + \u03b1f (A\\B) + \u03b2f (B\\A)(1)", "formula_coordinates": [1.0, 104.3, 688.1, 186.03, 25.79]}, {"formula_id": "formula_1", "formula_text": "f(A \u2229 B) = f \u2208A\u2229B wght(f )(2)", "formula_coordinates": [2.0, 121.34, 238.82, 168.99, 23.49]}, {"formula_id": "formula_2", "formula_text": "\u03c3 SI (w 1 , w 2 ) = f \u2208w 1 \u2229w 2 SI(w 1 [f ], w 2 [f ]).", "formula_coordinates": [2.0, 85.22, 321.74, 192.14, 14.0]}, {"formula_id": "formula_3", "formula_text": "f \u2208A\u2229B wght(f )= \u03c3 MIN (w 1 , w 2 ) = f \u2208w 1 \u2229w 2 MIN(w 1 [f ], w 2 [f ]),", "formula_coordinates": [2.0, 72.14, 413.66, 213.26, 27.56]}, {"formula_id": "formula_4", "formula_text": "f (w 1 \\ w 2 ) = \u03c3(w 1 , w 1 ) \u2212 \u03c3(w 1 , w 2 )", "formula_coordinates": [2.0, 98.66, 512.54, 165.27, 11.85]}, {"formula_id": "formula_5", "formula_text": "\u03c3(w 1 ,w 2 ) \u03c3(w 1 ,w 2 )+\u03b1[\u03c3(w 1 ,w 1 )\u2212\u03c3(w 1 ,w 2 )]+\u03b2[\u03c3(w 2 ,w 2 )\u2212\u03c3(w 1 ,w 2 )] = \u03c3(w 1 ,w 2 ) \u03b1\u03c3(w 1 ,w 1 )+\u03b2\u03c3(w 2 ,w 2 )+\u03c3(w 1 ,w 2 )\u2212(\u03b1+\u03b2)\u03c3(w 1 ,w 2 ) (4) Thus, if \u03b1 + \u03b2 = 1, Tversky's ratio model be- comes simply: sim(w 1 , w 2 ) = \u03c3(w 1 ,w 2 ) \u03b1\u03c3(w 1 ,w 1 )+(1\u2212\u03b1)\u03c3(w 2 ,w 2 ) (5)", "formula_coordinates": [2.0, 72.14, 555.55, 220.15, 115.39]}, {"formula_id": "formula_6", "formula_text": "DICE PROD(w 1 , w 2 ) = 2 * w 1 \u2022w 2 w 1 2 + w 2 2 DICE \u2020 (w 1 , w 2 ) = 2 * f \u2208w 1 \u2229w 2 min(w 1 [f ], w 2 [f ]) w 1 [f ]+ w 2 [f ] LIN(w 1 , w 2 ) = f \u2208w 1 \u2229w 2 w 1 [f ]+ w 2 [f ] w 1 [f ]+ w 2 [f ]", "formula_coordinates": [2.0, 307.7, 152.11, 217.73, 61.47]}, {"formula_id": "formula_7", "formula_text": "T SI (w 1 , w 2 ) = 2 \u2022 \u03c3 SI (w 1 , w 2 ) \u03c3 SI (w 1 , w 1 ) + \u03c3 SI (w 2 , w 2 ) (7)", "formula_coordinates": [2.0, 317.42, 494.42, 208.23, 26.73]}, {"formula_id": "formula_8", "formula_text": "SI \u03c3 SI (w 1 , w 2 ) PROD f \u2208w 1 \u2229w 2 w 1 [f ] * w 2 [f ] AVG f \u2208w 1 \u2229w 2 w 1 [f ]+w 2 [f ] 2 MIN f \u2208w 1 \u2229w 2 MIN(w 1 [f ], w 2 [f ])", "formula_coordinates": [2.0, 326.66, 603.26, 178.71, 58.4]}, {"formula_id": "formula_9", "formula_text": "DICE PROD(w 1 , w 2 ) = T PROD (w 1 , w 2 ) DICE \u2020 (w 1 , w 2 ) = T MIN (w 1 , w 2 ) LIN(w 1 , w 2 ) = T AVG (w 1 , w 2 ) (8)", "formula_coordinates": [3.0, 94.1, 98.3, 196.23, 53.37]}, {"formula_id": "formula_10", "formula_text": "\u03c3 SI (w 1 , w 2 ) \u03b1 \u2022 \u03c3 SI (w 1 , w 1 ) + (1 \u2212 \u03b1) \u2022 \u03c3 SI (w 2 , w 2 ) (9", "formula_coordinates": [3.0, 88.1, 223.82, 198.02, 26.73]}, {"formula_id": "formula_11", "formula_text": ")", "formula_coordinates": [3.0, 286.12, 231.26, 4.21, 10.91]}, {"formula_id": "formula_12", "formula_text": "T SI (w 1 , w 2 ) = 1 \u03b1\u2022\u03c3(w 1 ,w 1 ) \u03c3(w 1 ,w 2 ) + (1\u2212\u03b1)\u2022\u03c3(w 2 ,w 2 ) \u03c3(w 1 ,w 2 )", "formula_coordinates": [5.0, 86.66, 736.82, 187.01, 32.23]}, {"formula_id": "formula_13", "formula_text": "1 \u03b1 PREC(w 1 ,w 2 ) + 1\u2212\u03b1 REC(w 1 ,w 2 )", "formula_coordinates": [5.0, 363.98, 280.94, 105.41, 30.2]}], "doi": ""}