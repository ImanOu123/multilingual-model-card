{"title": "LOCOST: State-Space Models for Long Document Abstractive Summarization", "authors": "Florian Le Bronnec; Song Duong; Mathieu Ravaut; Alexandre Allauzen; Nancy F Chen; Vincent Guigue; Alberto Lumbreras; Laure Soulier; Patrick Gallinari", "pub_date": "", "abstract": "State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies. We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs. With a computational complexity of O(L log L), this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document abstractive summarization tasks. The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference. Additionally, LOCOST effectively handles inputs exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.", "sections": [{"heading": "Introduction", "text": "Nowadays the design of efficient models for long texts remains an open challenge despite the recent progress achieved in natural language processing (NLP). The introduction of transformer architectures (Vaswani et al., 2017) indeed came as a major bump in performance and scalability for text generation. However the quadratic complexity in the input length still restricts the application of large pretrained models to long texts. For instance, BERT (Devlin et al., 2019) and BART (Lewis et al., 2020) are limited to a context size of 512 and 1024 tokens respectively, which amounts to 2-3 paragraphs of standard text.\nTo mitigate this issue, a straightforward approach is to leverage sparse-attention patterns (Child et al., 2019) to better cope with long texts. As key examples, Guo et al. (2022) and Zaheer et al. (2020) extended the context capacity of encoderdecoder models (Raffel et al., 2020; and showed drastic increases in the performance on long text summarization, motivating the quest to incorporate longer contexts. However, in practice, even the best sparse-transformers need heavy computational resources to handle sequences of length larger than 8K tokens (see Figure 4).\nDeep state-space models (SSMs) (Gu et al., 2022b) have been proposed for sequence processing, with complexity O(L log L), initially for computer vision and audio and more recently for text. Their recurrent architectures are designed for capturing long-range dependencies (Gu et al., 2020). Up to now, their applications have been restrained to either unconditional autoregressive generation, i.e., with a decoder-only (Fu et al., 2023; ; or sequence classification, i.e., with an encoder-only (Gu et al., 2022b,a;Nguyen et al., 2022). Tackling conditional text generation with SSMs as required e.g. for summarization remains yet unexplored.\nIn this paper, we propose LOCOST an encoder-decoder architecture to explore the performance of SSMs for conditional text generation tasks, through the lens of abstractive summarization. We demonstrate that SSMs can be competitive with transformer-based models while drastically reducing their memory requirements. We opt for a lightweight architecture design, comparable to the average base transformers (roughly 250M parameters) in order to process extremely long sequences on standard compute resources. Our experimentations with extremely long sequences yield stateof-the-art results on the challenging BookSum-Book. With an increase of up to 2 points in average ROUGE score compared to sparse attention baselines, our model is able to process entire books, without truncation, and on a single GPU. Our contributions are threefold:\n\u2022 We propose a new encoder-decoder architecture based on state-space models. By bypassing the self-attention mechanism used in transformers, the model enjoys a complexity of O(L log L) instead of O(L 2 ) as in traditional transformers.\n\u2022 Compared with the best-performing sparse transformers of the same size, the model achieves 93-96% of the best performance on various long document abstractive summarization while being up to 50% more memory-efficient during training and up to 87% at inference time, see Figure 1.\n\u2022 The model is able to process entire input sequences of up to 600K tokens, a length far out of reach for sparse transformers. This allows the model to achieve a new state-of-the-art on a challenging full-book summarization task.\nTo the best of our knowledge, this is the first encoder-decoder that performs competitively with sparse transformers with no attention in the encoder. Furthermore, this work represents the first successful attempt at processing extremely long texts e.g. entire books without any truncation, all in a single pass. The proposed model opens new perspectives for addressing long texts with lesser resources. *", "publication_ref": ["b34", "b8", "b22", "b4", "b16", "b38", "b29", "b15", "b13", "b10", "b26"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Related Work", "text": "In this section, we first review memory-efficient transformers and existing alternatives to the attention mechanism. Then, we discuss recent literature on state-space models.\nMemory efficiency for transformers. Reducing the memory consumption of transformers is an active research field. Optimization at the hardware level  helped to improve the scaling of the attention computation on recent GPUs. A line of work considers retrieving-augmented transformers, like (Borgeaud et al., 2022;Wang et al., 2023), that use additional modules to enhance the language modeling backbone. While crucial in developing memory-efficient architectures, we consider these last two topics as being orthogonal to our work that focuses on the models' architecture.\nProfuse literature focuses on tailoring the models' architecture for long inputs. Since the computational complexity of attention comes from the computation of the self-attention matrix, a straightforward way to reduce its cost is to approximate it using sparse-attention patterns. These patterns typically incorporate a combination of local attention and a set of carefully selected tokens. For instance, in addition to global tokens, BigBird (Zaheer et al., 2020) considers random tokens, while LSG (Condevaux and Harispe, 2023) considers sparse tokens through various strategy of sparsification. LongT5 (Guo et al., 2022) chunks the sequence into blocks and averages their representations, which gives a number of global tokens equal to the number of blocks. An overview of the complexity of various sparse-transformers can be found in Table 1.\nIn contrast, we propose an alternative, computationally efficient architecture, without the need of costly self-attention blocks nor sparse-attention patterns.\nAttention-free transformers. Some variants of transformers already avoid the standard attention mechanism. For example Katharopoulos et al. (2020); Hua et al. (2022) approximate the softmax similarity in the attention by a more efficient computation. More recently, mixing architectures were introduced in . They are the main component of the FNet (Lee- Thorp et al., 2022) model, an encoder that replaces self-attention with a Discrete Fourier Transform (DFT). FNet has a complexity of O(L log L) and is an encoder-only model, thus restricted to classification and regression tasks.\nOur proposed model also bypasses attention in the encoder, reaching the same computational complexity as encoders such as FNet, while being a much more versatile model, specifically designed for conditional text generation.", "publication_ref": ["b36", "b38", "b16", "b19", "b17", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Encoder architecture Complexity per layer", "text": "Transformer (full) O(L 2 ) LED O(Lw) BigBird O(Lw + L(g + r)) LSG O(Lw + L(g + s)) LongT5 (TGlobal) O(Lw + L \u230aL/c\u230b) LOCOST O(L log(L))\nTable 1: Computational complexity per encoder layer as a function of the input length L, the local window size w (typically set to 256 tokens), the number of global tokens g, random tokens r, sparse tokens s and the chunk size c. LOCOST has a much lower complexity than other sparse-attention baselines.\nState-space models (SSMs). Deep learning implementations of SSMs consist of emerging architectures, first presented in (Gu et al., 2020). These architectures are particularly appealing for processing long sequences due to their reduced complexity compared to transformers, and their stronger theoretical guarantees compared to RNNs (Gu et al., 2022b), more details in Section 3. In practical applications, SSMs have found success in both classification and unconditional autoregressive generation for language modeling. Gu et al. (2022b) proposed a classification model that significantly improved the Long-Range Arena benchmark (Tay et al., 2021), which includes classification tasks involving images, synthetic sequences, and texts.\nOther studies have applied SSMs to video classification (Nguyen et al., 2022) and text classification (Wang et al., 2022). Regarding language modeling, many researchers have leveraged the natural causal formulation of SSMs, employing a decoder-only architecture for tasks like audio generation  and, more recently, autoregressive language modeling (Fu et al., 2023).\nIn this work, we tackle the more challenging task of conditional text generation and study the performance of SSMs, used as an encoder-decoder architecture, on long document abstractive summarization. With our proposed architecture, we demonstrate the abilities of our model to process input sequences of up to 600K tokens, while being competitive to sparse-transformers on long document abstractive summarization.", "publication_ref": ["b13", "b15", "b15", "b32", "b26", "b35", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Background", "text": "For contextualization, we leverage state-space models instead of self-attention. Throughout the paper, L denotes the sequence length, H the embedding dimension and N the dimension of the state-space hidden state (to be introduced in Section 3). Before delving into our model in Section 4, we describe below the main components of the state-space architecture and elaborate on their potential for long sequence processing.\nState-space models. For unidimensional inputs u = (u 0 , ..., u L\u22121 ) \u2208 R L , deep SSMs (Gu et al., 2022b) are based on the recurrent equation:\nx j+1 = Ax j + bu j+1 , y j+1 = c \u22a4 x j+1 + du j+1 ,(1)\nwhere x j is the SSM hidden state and y j the output of the SSM. The state matrix A \u2208 R N \u00d7N carries and transforms the hidden state through the iterations along with b \u2208 R N , c \u2208 R N , and d \u2208 R which are learned parameters.\nState-space convolution. By unrolling the recurrence above, the output sequence y \u2208 R L can be expressed as: y j = j l=0 c \u22a4 A j\u2212l bu l + du j , \u2200l \u2208 {1, ..., L}. Let * denote the causal convolution operator (details about this operator are in Appendix A). Then, we can define a convolution kernel \u03ba \u2208 R L that depends on A, b, c. A SSM layer is therefore parametrized by A, b, c, d through \u03ba and its output is defined by y as in the following equation:\n\uf8f1 \uf8f2 \uf8f3 y = \u03ba * u + du, \u03ba = c \u22a4 b, c \u22a4 Ab, . . . , c \u22a4 A L\u22121 b .\n(2)\nFor multidimensional u \u2208 R L\u00d7H , we simply compute H convolutions with one kernel \u03ba h for each dimension.\nSSMs efficiency. Due to the linear timedependency between hidden states, as shown in Equation ( 1), we can compute the whole output y directly as a convolution, without iteration over the time dimension, as opposed to RNNs. A naive implementation of (2) would incur a quadratic complexity in the input length L, matching the complexity of transformers and thus be prohibitive for long sequences. However, thanks to the FFT, this computation can be performed in O(L log L) (see Appendix A for more details).", "publication_ref": ["b15"], "figure_ref": [], "table_ref": []}, {"heading": "Model", "text": "In this section, we present the LOCOST model. We first introduce the bidirectional deep state-space model, then show how to use it to enable global contextualization of the tokens. Then, we present the architecture of the LOCOST layer with an efficient contextualization that can be used as a dropin replacement for the self-attention mechanism in transformers.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Capturing local and global contexts", "text": "Intuition. In deep SSMs, information from previous tokens flows up to the current token through the hidden states x. The convolution view provides another angle: each output y j is a weighted sum of the previous tokens u 0 , . . . , u j , whose weights are given by \u03ba.\nBidirectional contextualization. To aggregate information from both directions, we consider bidirectional convolutions. A first kernel, \u2190 \u2212 \u03ba performs the regular causal convolution \u2190 \u2212 \u03ba * u. A second kernel \u2212 \u2192 \u03ba is used to compute the cross-correlation with u. The results of these two operations are summed out (similar to bi-recurrent encoder). The overall operation is described by the following equation:\ny j = l\u2264j \u2190 \u2212 \u03ba j\u2212l \u2299 u l + l\u2265j \u2212 \u2192 \u03ba l\u2212j \u2299 u l + d \u2299 u j = BiSSM(U ) j .(3)\nIn this equation, U \u2208 R L\u00d7H is the embedding matrix of the input text: (u 0 , . . . , u L\u22121 ). The kernels \u2212 \u2192 \u03ba , \u2190 \u2212 \u03ba are computed as in Equation ( 2), with their respective parameters (\n\u2212 \u2192 A, \u2212 \u2192 c , \u2212 \u2192 b ) and ( \u2190 \u2212 A, \u2190 \u2212 c , \u2190 \u2212 b )\n. The element-wise product is denoted by \u2299 and we consider multidimensional inputs, with one kernel per dimension.\nThe output y j is now contextualized as a weighted sum of previous u \u2264j and subsequent u \u2265j inputs. For scalar inputs, more insights on how far in the future or in the past a scalar input u l contributes to the scalar output y j are given by the spectral radii \u03c1( \u2212 \u2192 A) and \u03c1( \u2190 \u2212 A). Indeed the sensitivity of an output y j with respect to an input u l is bounded by the following quantity:\n\u2202y j \u2202u l \u2264 \u03c1( \u2190 \u2212 A) j\u2212l | \u2190 \u2212 c \u22a4 \u2190 \u2212 b | if l < j, \u03c1( \u2212 \u2192 A) l\u2212j | \u2212 \u2192 c \u22a4 \u2212 \u2192 b | if l > j.\nFor multidimensional inputs, using a state-space kernel for each dimension enables a fine-grained adjustment of the spectral radii independently for each of them. A small value corresponds to modeling local contexts, while a large value captures global ones. Some of the corresponding kernel weights of this convolution can be visualized on Figure 3. A more complete visualization can be found in Appendix C.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Architecture", "text": "Encoder. Our encoder consists of a stack of LO-COST layers, illustrated in Figure 2a. It is computed as follows:\n\u2022 Embedding matrix U \u2208 R L\u00d7H is first projected onto Q, V \u2208 C L\u00d7H .\n\u2022 V is contextualized through a BiSSM.\n\u2022 A pointwise multiplication Q \u2299 BiSSM(V ) acts as a first gate before passing the output through a feedforward layer.\n\u2022 This feedforward layer employs a second gating mechanism (see Figure 2b). For this component, we use gated GeLU that has shown to be efficient by Shazeer (2020).\nThe architecture of the LOCOST layer (Figure 2a) resembles that of a transformer layer except that the self-attention mechanism is replaced by a gated bidirectional state-space model. We follow Gu et al. (2022a) for the parametrization and initialization of the state-space models (more details in Appendix E).\nDecoder. Since our focus is on long input summarization, the generation output length is very short compared to the input. For decoding, we follow the practice of other efficient architectures (Zaheer et al., 2020;Beltagy et al., 2020;Guo et al., 2022) and use a vanilla transformer decoder equipped with dense self-and cross-attention. A full description of hyperparameters of the model is provided in Appendix B.\nComplexity. The LOCOST layer takes O(H 2 L+ HN L + HL log L) time and O(HN L) space to compute. We refer to Appendix D for more details.", "publication_ref": ["b14", "b38", "b0", "b16"], "figure_ref": ["fig_1", "fig_1", "fig_1"], "table_ref": []}, {"heading": "Experiments", "text": "To validate our experiments, we focus on the long document abstractive summarization task as it represents a typical conditional generation problem with long input requirements.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental setup", "text": "Approach. We evaluate LOCOST following a classical pre-training then fine-tuning approach.\nFor fine-tuning, we used the official train, validation and test splits of each dataset. We train all models until convergence and select the best model based on the validation Mean ROUGE (mean of ROUGE-1/2/LSum) for test evaluation.\nMetrics. We evaluate LOCOST both with reference-based and reference-free metrics. For reference-based summarization evaluation, we use the traditional n-gram overlap summarization metrics ROUGE-1/2/Lsum (Lin, 2004). We average them into a single score to compare with other baselines. We also report BERTScore (BS) , a model-based metric. For referencefree evaluation, we report the BLANC (BL) score (Vasilyev et al., 2020), a metric that has been shown to correlate well with human evaluations. We also assess the throughput (samples per second) and the memory usage (MiB of GPU RAM) of LOCOST compared with other state-of-the-art sparse transformers.\nInference. In all of our experiments, we intentionally favored simplicity and opted for greedy decoding.", "publication_ref": ["b23", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "Pre-training", "text": "Pre-training objective. To pre-train the model, we leverage the gap-sentences generation (GSG) unsupervised pre-training objective, which was introduced by PEGASUS  and is well-suited for sequence-to-sequence generation. Unlike BART (Lewis et al., 2020) or T5 (Raffel et al., 2020) pre-training objectives, GSG endows the model with zero-shot summarization capabilities. GSG was successfully applied by subsequent generation models such as LongT5 (Guo et al., 2022) and PEGASUS-X (Phang et al., 2022). Namely, a document D is split into its M sentences: D = {s 1 , . . . , s M }. Given a ratio \u03b1, GSG then identifies K = \u230a\u03b1M \u230b sentences from D that maximize the ROUGE-1 (noted R-1) with the rest of the document:\nU = arg top-K j R-1 i\u0338 =j {s i }, s j (4)\nThe resulting subset U \u2286 {1, . . . , M } splits the document into a pseudo summary\u0176 = {s i } i\u2208U and a pseudo-sourceD = {s i } i / \u2208U , which are used for pre-training with the standard cross-entropy loss.\nPre-training data. We pre-train the model exclusively on the C4 dataset (Raffel et al., 2020), in BF16 for 1M steps, using an input sequence length of 4,096 and an output sequence length of 910.\nPre-training optimization. The learning rate scheduler we use is identical to T5, employing an inverse square root function, with the warm-up steps set to 10,000. We set the GSG-ratio \u03b1 = 0.2 and do not employ dropout during this phase. We follow closely the same pre-training as LongT5 (Guo et al., 2022).", "publication_ref": ["b22", "b29", "b16", "b28", "b29", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Fine-tuning", "text": "Fine-tuning datasets. We evaluate LOCOST on a series of long-input abstractive summarization tasks. A table of statistics for all the datasets can be found in Appendix F.\n\u2022 arXiv (Cohan et al., 2018) Articles extracted from arXiv using the core body document as the input sequence and the abstract as the target sequence.\n\u2022 PubMed (Cohan et al., 2018) Similar to arXiv, but articles come from PubMed, a medical database.\n\u2022 GovReport (Huang et al., 2021) A longdocument summarization dataset of US government reports with their executive summaries.\n\u2022 SummScreenFD (Chen et al., 2022) A longdocument summarization dataset of TV series transcripts of entire episodes with human-written recaps of the episodes.\n\u2022 BookSum (-Chapter & -Book) (Kryscinski et al., 2022) A collection of chapters from various books with a summary for each of them. We also consider the book-level version where the model has to summarize entire books.\nFine-tuning optimization. We fine-tune in BF16 using a constant learning rate of 5 \u00d7 10 \u22124 and a dropout rate of 0.1 for all datasets. We experiment with lengths ranging from 4,096 to 32,768 for the input and 512 for the output, except for GovReport and BookSum-Book where we use 1024.\nBaselines. We consider both competitive sparse transformers, including LED (Beltagy et al., 2020), BigBird (Zaheer et al., 2020), LongT5 (Guo et al., 2022) and LSG (Condevaux and Harispe, 2023), as well as dense encoder-decoders like BART (Lewis et al., 2020), T5 (Raffel et al., 2020) and PEGASUS . For a fair comparison, we only compare to sparse transformers architectures of equivalent size (roughly 250M parameters).", "publication_ref": ["b5", "b5", "b20", "b0", "b38", "b16", "b6", "b22", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Long-input summarization.  4.\nQualitative evaluation: GPT-3.5 preference.\nSince our input texts are very long, performing a full human-based evaluation would be very costly and time consuming. Instead, we perform a mock human evaluation using GPT-3.5 * . This practice has been used and has shown success in summary evaluation (Shen et al., 2023;Gilardi et al., 2023;Chiang and Lee, 2023). We ask the model to rate the generated summary on four dimensions: relevance, consistency, fluency, and coherence. More * We use gpt-3.5-turbo-16k model for evaluation.  % denotes the relative performance on the Mean ROUGE score w.r.t. LongT5, the best performing sparse-transformer at the given size, which is indicated as 100%. BS stands for BERTScore and BL for BLANC.  details are given in Appendix I. We perform evaluation on 500 samples randomly taken from PubMed. The results are shown in Table 5. LOCOST produces summaries at a competitive level with respect to LongT5 (93-97%).\nGovReport SummScreenFD Model L R-1 R-2 R-L % R-1 R-2 R-L %", "publication_ref": ["b11", "b3"], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "Extrapolating to longer sequences", "text": "Because the lengths of the inputs considered during training are often limited due to complexity issues, a desirable property for a model would be to extrapolate at inference time to sequences much longer than the ones used during training.\nWe train LOCOST on a maximum input length of 4,096 and evaluate it on the test set of arXiv with a maximum input length of 8,192 tokens. As shown in Table 6, this experiment confirms that LOCOST is indeed able to extrapolate to longer sequences than those employed in training. Note that LongT5 leverages relative positional encodings, enabling extrapolation capability. However, as previously mentioned, this comes at the expense of an increased complexity compared to LOCOST. In the next section, we push this idea further by considering extra-long sequences.    than the ones seen during training. Due to the reduced memory usage at both train and inference time, we conduct in this section an analysis of its performances when facing extremely long texts e.g. summarizing entire books. We consider the book-level setting of BookSum. We train multiple instances of LOCOST for 100 epochs on truncated books with a context length ranging from 1K to 32K and select the best model on Mean ROUGE on the validation set. We evaluate these models on the test set with untruncated books, and report the results in Figure 5. We found that increasing the input length during training leads to an overall increase in the test Mean ROUGE scores as more contexts are being considered for optimization. Once more, this confirms the generalization capability of LOCOST on extra-long sequence lengths.", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": ["tab_8"]}, {"heading": "Extra", "text": "Results on full-book summarization. Based on the observations above, we put our best model LOCOST-32K to the test and compare it with LongT5 and current state-of-the-art models on BookSum-Book. For LongT5, we fine-tune the available checkpoint on the maximum possible in-* For a fair comparison with already existing results, we used ROUGE-L instead of ROUGE-Lsum on BookSum-Book.   put length during training (16K) and report its performance on the longest possible input length at inference time (32K). For the other models, the results come from the original papers, in which the models initially produce individual summaries for each paragraph of the book and then rank them according to the model's level of confidence. Results are shown in Table 7. Despite being the model with the least number of parameters, LOCOST achieves state-of-the-art Mean ROUGE compared to LongT5 and even large variants of BART, T5 and PEGASUS. LOCOST is also the only model capable of processing the full documents without truncation and handle sequence lengths of up to 600K tokens. This reveals that effectively processing full contexts without truncation can lead to strong performance enhancement.\nOur paper explores a new encoder-decoder architecture dedicated to handle long input texts. By replacing the self-attention block by SSMs, we design a low complexity and lightweight model able to process long sequences up to 600K tokens at inference time on a single GPU. Our model achieves competitive results on summarization datasets. Moreover, surpassing the limits of existing sparse transformer alternatives, new state-of-the-art results are obtained on the BookSum-Book dataset. To the best of our knowledge, LOCOST is the first model able to process entire books without truncation, all in a single pass. These results offer exciting possibilities for abstractive text-processing tasks requiring extra-long sequences.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_10"]}, {"heading": "Limitations", "text": "Though we investigated lightweight models for computational reasons, scaling the architecture to a larger size could be studied. We focused on long document abstractive summarization, we leave for future work the study of SSMs on other long inputs abstractive tasks. Although replacing self-attention with state-space encoders drastically reduces the computational complexity, the use of dense crossattention in the decoder still limits the output sequence length in terms of computation during training.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ethics Statement", "text": "We performed pre-training on a subset of the C4 dataset, which has been identified to include inappropriate content like hate speech and explicit material, as noted in the studies conducted by Luccioni and Viviano (2021) and also exhibits negative biases towards certain ethnicities (Dodge et al., 2021). It is important to investigate potential solutions for mitigating these problems through more meticulous preprocessing in order to prevent the emergence of such undesirable attributes in future research. Nevertheless, it is worth mentioning that despite these concerns, the C4 dataset serves as a benchmark within the community, and the reported results solely focus on the quality of the summaries, thereby avoiding any unethical implications. In this paper, we consider a relatively small size for LOCOST. We believe our work could be reproducible with limited resources. We tracked the GPU power consumption during pre-training.\nThe average power usage was 190W per GPU. We trained for 140 hours on 16 GPUs. Given the local CO 2 intensity of 58 gCO 2 /kWh * , we can estimate that approximately 25kg of CO 2 have been emitted during the pre-training, to be compared with the average emissions of 4.6t of CO 2 par capita in 2019 * .", "publication_ref": ["b9"], "figure_ref": [], "table_ref": []}, {"heading": "A Convolution", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1 Causal convolution", "text": "In this section indices of sequence are represented by bracketed numbers. The causal convolution between sequences u, \u03ba \u2208 R L denoted as * presented in section 3 is defined as:\n(\u03ba * u)[j] = j l=0 \u03ba[j \u2212 l]u[l].(5)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Convolution and DFT", "text": "We are going to detail the link between convolution and the Discrete Fourier Transform. For that purpose, we need another tool, the circular convolution.\nCircular convolution. Let's define\u03ba the periodized version of \u03ba as: \u2200j \u2208 N,\u03ba[j] = \u03ba[j mod L]. For index 0 \u2264 j \u2264 L \u2212 1, the discrete circular convolution between u and \u03ba is defined as:\n(\u03ba \u229b u)[j] = L\u22121 l=0\u03ba [j \u2212 l]u[l].(6)\nConvolution theorem. The convolution theorem states that (the derivation consists only in permuting the symbols):\n\u03ba \u229b u = F \u22121 (\u03ba \u2299\u00fb) ,(7)\nwhere. designates the DFT of a sequence and \u2299 designates the element-wise multiplication.\nCausal convolution with DFT. To compute \u03ba * u with a DFT, a trick is to pad \u03ba and u with L zeros before taking their DFT. Indeed, if we replace \u03ba and u with their padded versions (hence vectors of R 2L ) in eq. ( 6) we see immediately that it coincides with the causal convolution (5). This means that using the Fast Fourier Transform (FFT) algorithm, the causal convolution can be computed in O(L log L).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Hyperparameters", "text": "The set of hyperparameters used are presented in Table 8.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_12"]}, {"heading": "C Visualisation of learned kernels", "text": "A more complete visualization of the learned kernels can be found in Figure 3 and 7.   ", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "E State-space models implementation details", "text": "Parametrization. We chose to follow the parametrization exposed in (Gu et al., 2022a).\n\u2022 The multi-dimensional state-tensor * A \u2208\nC H\u00d7N \u00d7N is made of H diagonal matrices A h = diag(\u03bb h ) \u2208 C N \u00d7N . \u2022 For 0 \u2264 h \u2264 H and 0 \u2264 n \u2264 N , \u03bb \u2208 R H\u00d7N is \u03bb h,n = exp \u2206 h \u03bb Re h,n + i\u2206 h \u03bb Im h,n .\n\u2022 \u2206 \u2208 R H is a time-scaling parameter.\n\u2022 We use N = 256. Most work chose either N = 64 or N = 256 (Gu et al., 2022a;Fu et al., 2023). Since increasing N from 64 to 256 did only incur a negligible increase in memory consumption, we chose the latter, with the rationale that it should give more expressive power to \u03ba.\nInitialization. As reported in (Gu et al., 2022a) (see their Table 3), SSMs with special initialization are tailored for long inputs processing. This has * Using parameters in C gives better expressive power to the convolution, see Gu et al. (2022a) for theoretical and empirical justifications. been experimentally confirmed in (Zuo et al., 2022), where they use non-trainable state-space layers to provide long-range contextualization in addition to local attention.\n\u2022 \u03bb Re h,n is initialized to \u2212 1 2 and \u03bb Im h,n to \u03c0n.\n\u2022 \u2206 h is initialized randomly following U([0, 1]).\n\u2022 b, c \u2208 C N \u00d7H are initialized randomly following N (0, 1) * .", "publication_ref": ["b14", "b14", "b10", "b14", "b14", "b41"], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "F Dataset details", "text": "Statistics. The statistics of the datasets can be found in Table 9.\nLicense. C4: ODC-BY, arXiv/PubMed: unknown, BookSum: BSD-3-Clause, GovReport: unknown, SummScreenFD: unknown.\nUsage. All datasets were solely used for research purposes. Note that they are all in english and we refer to the original publications for more details.  ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_14"]}, {"heading": "H Sample outputs", "text": "Here is a sample summary (gold human abstract + generated summary) from PubMed. Ground Truth: \"Background : the purpose of this study was to perform the creating opportunities for parent empowerment ( cope ) program for iranian mothers and evaluate its effectiveness on stress, anxiety, and participation of mothers who have premature infants hospitalized in neonatal intensive care units ( nicus).materials and methods : a randomized clinical trial was conducted with 90 mothers of premature infants hospitalized in the educational neonatal nicus of state hospitals affiliated to isfahan university of medical sciences. For measuring the variables, state -trait anxiety inventory, the parental stressor scale : neonatal intensive care, and the index of parental participation / hospitalized infant were used. Intervention group received two phases of cope program. This program consisted of information and behavioral activities about the characteristics of premature infants. Sessions interval was from 2 to 4 days. Stress and anxiety were measured three times ( before each phase and 2 -4 days after the second phase ). Mothers participation was evaluated 2 -4 days after the second phase. The t -test, 2, mann -whitney u test, and repeated measurement test were used for data analysis. Results : mothers in the intervention group reported significantly less anxiety and less stress in the nicu after performing each phase of the cope program ( p 0.001 ), whereas at this time, the level of stress in the comparison group increased. Also, cope mothers participated in their infants' care rather than the mothers in the comparison group ( p 0.001). Conclusion : cope program was effective for iranian mothers. This study shows that irrespective of any culture, giving early and comprehensible educational -behavioral information may have positive effect on maternal psychological condition and maternal infant interaction.\"\nLOCOST: \"Background : the premature birth of an infant and the following neonatal intensive care cause psychological distress and can have a traumatizing effect on parents. The neonatal intensive care unit ( nicu ) environment has the potential to exacerbate stress for parents. The purpose of this study was to create opportunities for parent empowerment ( cope ) program in this survey.materials and methods : this is a quasi -experimental study.\nThe study population consisted of 90 mothers with infants hospitalized in the nicu of isfahan, iran. The cope program included information about the behavior of infants, parenting role, and parenting stressor scales. Data were collected through demographic and infant variables questionnaire, state anxiety inventory, and parental stressor scale at the first contact point. Then, the cope program was given to the mothers. Data were analyzed by spss version 16 using descriptive statistics.results:the study resulted in the Absence of any significant difference in the state anxiety inventory and the parental stressor scale results. Moreover, the mean of maternal state anxiety inventory in the comparison group was significantly higher than the intervention group ( p 0.001 ). The parental stressor scale results showed significant difference between the two groups ( p 0.001 ). The state anxiety inventory showed significant difference between the two groups ( p 0.001 ). The parental stressor scale results showed significant difference between the two groups ( p 0.001). Conclusions : the cope program was feasible and the mother -infant positive interactions were high. Therefore, this study can be done by providing the parents with strategies that support them.\"\nI GPT-3.5 evaluation\nAs a proxy for subjective assessment, we make use of gpt-3.5-turbo-16k as a mock human evaluator. We ask the model to rate the generated summary on four dimensions: relevance, consistency, fluency, and coherence with the following prompt:\n\"Imagine you are a human annotator now.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "This work has been partly funded through project ACDC ANR-21-CE23-0007 and ANR-23-PEIA-0008, PEPR IA, project \"Principes th\u00e9oriques et algorithmiques de l'apprentissage frugal (SHARP)\". This project was provided with computing AI and storage resources by GENCI at IDRIS thanks to the grants 20XX-AD011014060, 20XX-AD011014022 and 20XX-A0151014638 on the supercomputer Jean Zay's V100/A100 partition. This work has also been partly funded through the Singapore International Pre-Graduate Award (SIPGA).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "You will evaluate the quality of summaries written for an article. Please follow these steps: Carefully read the article, and be aware of the information it contains. Read the proposed summary. Rate the summary on four dimensions: relevance, consistency, fluency, and coherence. You should rate on a scale from 1 (worst) to 5 (best). Definitions are as follows:\nRelevance: The rating measures how well the summary captures the key points of the article. Consider whether all and only the important aspects are contained in the summary.\nConsistency: The rating measures whether the facts in the summary are consistent with the facts in the original article. Consider whether the summary does reproduce all facts accurately and does not make up untrue information.\nFluency: This rating measures the quality of individual sentences, whether they are well-written and grammatically correct. Consider the quality of individual sentences.\nCoherence: The rating measures the quality of all sentences collectively, to fit together and sound natural. The article and the summary are given below:\nArticle: {insert article} Summary: {insert summary}.\nRate the summary in the following format:\nRelevance: Consistency: Fluency: Coherence:\"", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Longformer: The long-document transformer", "journal": "", "year": "2020", "authors": "Iz Beltagy; Matthew E Peters; Arman Cohan"}, {"ref_id": "b1", "title": "Erich Elsen, and Laurent Sifre. 2022. Improving language models by retrieving from trillions of tokens", "journal": "PMLR", "year": "", "authors": "Sebastian Borgeaud; Arthur Mensch; Jordan Hoffmann; Trevor Cai; Eliza Rutherford; Katie Millican; George Bm Van Den Driessche; Jean-Baptiste Lespiau; Bogdan Damoc; Aidan Clark; Diego De Las; Aurelia Casas; Jacob Guy; Roman Menick; Tom Ring; Saffron Hennigan; Loren Huang; Chris Maggiore; Albin Jones; Andy Cassirer; Michela Brock; Geoffrey Paganini; Oriol Irving; Simon Vinyals; Karen Osindero; Jack Simonyan;  Rae"}, {"ref_id": "b2", "title": "SummScreen: A dataset for abstractive screenplay summarization", "journal": "Long Papers", "year": "2022", "authors": "Mingda Chen; Zewei Chu; Sam Wiseman; Kevin Gimpel"}, {"ref_id": "b3", "title": "Can large language models be an alternative to human evaluations?", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Han Cheng; Hung-Yi Chiang;  Lee"}, {"ref_id": "b4", "title": "Generating long sequences with sparse transformers. CoRR, abs", "journal": "", "year": "1904", "authors": "Rewon Child; Scott Gray; Alec Radford; Ilya Sutskever"}, {"ref_id": "b5", "title": "A discourse-aware attention model for abstractive summarization of long documents", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Arman Cohan; Franck Dernoncourt; Soon Doo; Trung Kim; Seokhwan Bui; Walter Kim; Nazli Chang;  Goharian"}, {"ref_id": "b6", "title": "LSG Attention: Extrapolation of pretrained Transformers to long sequences", "journal": "", "year": "2023", "authors": "Charles Condevaux; S\u00e9bastien Harispe"}, {"ref_id": "b7", "title": "FlashAttention: Fast and memory-efficient exact attention with IO-awareness", "journal": "", "year": "2022", "authors": "Tri Dao; Daniel Y Fu; Stefano Ermon; Atri Rudra; Christopher R\u00e9"}, {"ref_id": "b8", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b9", "title": "Documenting large webtext corpora: A case study on the colossal clean crawled corpus", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Jesse Dodge; Maarten Sap; Ana Marasovi\u0107; William Agnew; Gabriel Ilharco; Dirk Groeneveld; Margaret Mitchell; Matt Gardner"}, {"ref_id": "b10", "title": "Hungry hungry hippos: Towards language modeling with state space models", "journal": "", "year": "2023", "authors": "Y Daniel; Tri Fu; Khaled Dao;  Kamal Saab; W Armin; Atri Thomas; Christopher Rudra;  Re"}, {"ref_id": "b11", "title": "ChatGPT outperforms crowd workers for text-annotation tasks", "journal": "Proceedings of the National Academy of Sciences", "year": "2023", "authors": "Fabrizio Gilardi; Meysam Alizadeh; Ma\u00ebl Kubli"}, {"ref_id": "b12", "title": "It's raw! Audio generation with state-space models", "journal": "PMLR", "year": "2022", "authors": "Karan Goel; Albert Gu; Chris Donahue; Christopher Re"}, {"ref_id": "b13", "title": "Hippo: Recurrent memory with optimal polynomial projections", "journal": "Curran Associates, Inc", "year": "2020", "authors": "Albert Gu; Tri Dao; Stefano Ermon; Atri Rudra; Christopher R\u00e9"}, {"ref_id": "b14", "title": "On the parameterization and initialization of diagonal state space models", "journal": "", "year": "2022", "authors": "Albert Gu; Karan Goel; Ankit Gupta; Christopher R\u00e9"}, {"ref_id": "b15", "title": "Efficiently modeling long sequences with structured state spaces", "journal": "", "year": "2022", "authors": "Albert Gu; Karan Goel; Christopher Re"}, {"ref_id": "b16", "title": "LongT5: Efficient text-to-text transformer for long sequences", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Mandy Guo; Joshua Ainslie; David Uthus; Santiago Ontanon; Jianmo Ni; Yun-Hsuan Sung; Yinfei Yang"}, {"ref_id": "b17", "title": "Transformer quality in linear time", "journal": "PMLR", "year": "2022", "authors": "Weizhe Hua; Zihang Dai; Hanxiao Liu; Quoc Le"}, {"ref_id": "b18", "title": "Efficient attentions for long document summarization", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Luyang Huang; Shuyang Cao; Nikolaus Parulian; Ji Heng; Lu Wang"}, {"ref_id": "b19", "title": "Transformers are rnns: Fast autoregressive transformers with linear attention", "journal": "", "year": "2020", "authors": "A Katharopoulos; A Vyas; N Pappas; F Fleuret"}, {"ref_id": "b20", "title": "BOOKSUM: A collection of datasets for long-form narrative summarization", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Wojciech Kryscinski; Nazneen Rajani; Divyansh Agarwal; Caiming Xiong; Dragomir Radev"}, {"ref_id": "b21", "title": "FNet: Mixing tokens with Fourier transforms", "journal": "", "year": "2022", "authors": "James Lee-Thorp; Joshua Ainslie; Ilya Eckstein; Santiago Ontanon"}, {"ref_id": "b22", "title": "BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension", "journal": "", "year": "2020", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal; Marjan Ghazvininejad; Abdelrahman Mohamed; Omer Levy; Veselin Stoyanov; Luke Zettlemoyer"}, {"ref_id": "b23", "title": "ROUGE: A package for automatic evaluation of summaries", "journal": "", "year": "2004", "authors": "Chin-Yew Lin"}, {"ref_id": "b24", "title": "Pay attention to MLPs", "journal": "", "year": "2021", "authors": "Hanxiao Liu; Zihang Dai; David So; Quoc V Le"}, {"ref_id": "b25", "title": "What's in the box? an analysis of undesirable content in the Common Crawl corpus", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Alexandra Luccioni; Joseph Viviano"}, {"ref_id": "b26", "title": "S4ND: Modeling images and videos as multidimensional signals with state spaces", "journal": "", "year": "2022", "authors": "Eric Nguyen; Karan Goel; Albert Gu; Gordon Downs; Preey Shah; Tri Dao; Stephen Baccus; Christopher R\u00e9"}, {"ref_id": "b27", "title": "Pytorch: An imperative style, high-performance deep learning library", "journal": "Curran Associates, Inc", "year": "2019", "authors": "Adam Paszke; Sam Gross; Francisco Massa; Adam Lerer; James Bradbury; Gregory Chanan; Trevor Killeen; Zeming Lin; Natalia Gimelshein; Luca Antiga; Alban Desmaison; Andreas Kopf; Edward Yang; Zachary Devito; Martin Raison; Alykhan Tejani; Sasank Chilamkurthy; Benoit Steiner; Lu Fang; Junjie Bai; Soumith Chintala"}, {"ref_id": "b28", "title": "Investigating efficiently extending transformers for long input summarization", "journal": "", "year": "2022", "authors": "Jason Phang; Yao Zhao; Peter J Liu"}, {"ref_id": "b29", "title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "Journal of Machine Learning Research", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b30", "title": "SCROLLS: Standardized CompaRison over long language sequences", "journal": "", "year": "2022", "authors": "Uri Shaham; Elad Segal; Maor Ivgi; Avia Efrat; Ori Yoran; Adi Haviv; Ankit Gupta; Wenhan Xiong; Mor Geva; Jonathan Berant; Omer Levy"}, {"ref_id": "b31", "title": "Yang You, and Lidong Bing. 2023. Are Large Language Models Good Evaluators for Abstractive Summarization? arXiv e-prints", "journal": "", "year": "", "authors": "Chenhui Shen; Liying Cheng"}, {"ref_id": "b32", "title": "Long range arena : A benchmark for efficient transformers", "journal": "", "year": "2021", "authors": "Yi Tay; Mostafa Dehghani; Samira Abnar; Yikang Shen; Dara Bahri; Philip Pham; Jinfeng Rao; Liu Yang; Sebastian Ruder; Donald Metzler"}, {"ref_id": "b33", "title": "Fill in the BLANC: Human-free quality estimation of document summaries", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Oleg Vasilyev; Vedant Dharnidharka; John Bohannon"}, {"ref_id": "b34", "title": "Attention is all you need", "journal": "Curran Associates, Inc", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Illia Kaiser;  Polosukhin"}, {"ref_id": "b35", "title": "", "journal": "", "year": "2022", "authors": "Junxiong Wang; Jing Nathan Yan; Albert Gu; Alexander M Rush"}, {"ref_id": "b36", "title": "Augmenting language models with long-term memory", "journal": "", "year": "2023", "authors": "Weizhi Wang; Li Dong; Hao Cheng; Xiaodong Liu; Xifeng Yan; Jianfeng Gao; Furu Wei"}, {"ref_id": "b37", "title": "Transformers: State-of-the-art natural language processing", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; R\u00e9mi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger; Quentin Drame; Alexander M Lhoest;  Rush"}, {"ref_id": "b38", "title": "Big bird: Transformers for longer sequences", "journal": "", "year": "2020", "authors": "Manzil Zaheer; Guru Guruganesh; Joshua Kumar Avinava Dubey; Chris Ainslie; Santiago Alberti; Philip Ontanon; Anirudh Pham; Qifan Ravula; Li Wang; Amr Yang;  Ahmed"}, {"ref_id": "b39", "title": "PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization", "journal": "PMLR", "year": "2020", "authors": "Jingqing Zhang; Yao Zhao; Mohammad Saleh; Peter Liu"}, {"ref_id": "b40", "title": "Bertscore: Evaluating text generation with bert", "journal": "", "year": "2020", "authors": "Tianyi Zhang; * ; Varsha Kishore; * ; Felix Wu; * ; Kilian Q Weinberger; Yoav Artzi"}, {"ref_id": "b41", "title": "Efficient long sequence modeling via state space augmented transformer", "journal": "", "year": "2022", "authors": "Simiao Zuo; Xiaodong Liu; Jian Jiao; Denis Charles; Eren Manavoglu; Tuo Zhao; Jianfeng Gao"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Mean ROUGE score with inference memory usage on long-document summarization with input length 16K (left: SummScreenFD dataset, right: Gov-Report dataset). The size of the circles represents the training memory usage. LOCOST demonstrates competitive performances compared to state-of-the-art sparse transformers of the same size, while being significantly more memory-efficient at both training and inference.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: The embedded sequence is contextualized via a gated bidirectional SSM before passing through a gated feedforward net.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Visualization of the kernels corresponding to the first dimension for several layers of the pre-trained model. Bins show the average decay of the forward and backward kernels. This illustrates the different scales of each kernel. Layers 1 and 10 capture short and extrashort range contextualizations, while Layers 4 and 7 model extra-long and long contexts, respectively.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure5: LOCOST trained on increasing sequence lengths evaluated on BookSum-Book dataset without truncation, with texts reaching up to 600K tokens.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "DComputational complexity of a LOCOST layer Projection onto Q and V takes O(LH 2 ) time and O(LH) space. Computing the SSM kernel \u03ba = c \u22a4 b, c \u22a4 Ab, . . . , c \u22a4 A L\u22121 b takes O(LHN ) time and space. Finally, calculating H convolutions in parallel with DFT takes O(LH log L) time.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 6 :6Figure 6: Complete visualization of the kernel of the first dimension of the model through all the 12 layers, includes visualization from Figure 3.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "", "figure_data": "and 3"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Results on arXiv, PubMed and BookSum-Chapter with a input length of 4K, 4K and 8K tokens respectively.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Results on the test set of SCROLLS for GovReport and SummScreenFD. L denotes the considered input length. 11 2 12 2 13 2 14 2 15 2 16 2 17 2 18 2 19", "figure_data": "70000LOCOST40000LOCOST60000LongT5 T5\u221253%35000LongT5 T5Train memory (MiB)20000 30000 40000 50000LEDInference memory (MiB)10000 15000 20000 25000 30000LED\u221287%10000500002 102 112 12 Input length 2 132 142 152 10 2 Input lengthFigure 4: Memory consumption during a typical training (forward + backward) (left) and inference iteration (onlyforward) (right). Batch size = 1. Ending cross means out-of-memory or architectural limitations after this point."}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Throughput comparison for different models at 4K and 16K input length.", "figure_data": "ModelRel Cons Flu CohLongT5base 4.64.73.73.7LOCOST4.34.43.63.5"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "GPT3.5 evaluation on PubMed with 4K input length using gpt-3.5-turbo-16k. Rel stands for relevance, Cons for factual consistency, Flu for fluency and Coh for coherence.", "figure_data": "arXiv-4K arXiv-8KModelLMean-RMean-RGain (%)LongT5base 4K34.835.52.0LOCOST4K33.534.32.4"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Extrapolating to longer sequences experiments.", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "", "figure_data": ": Results on BookSum-Book. While being thesmallest model, LOCOST achieves state-of-the-art onMean ROUGE when summarizing entire books."}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "LOCOST hyperparameters.    ", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Visualization of the kernel (in absolute value) of size 768 \u00d7 2048 for each of the 12 layers. We clearly show that each layer has kernels of different scales that will model different context ranges.Software. Our code is based on Pytorch(Paszke et al., 2019), Huggingface(Wolf et al., 2020) and  H3 (Fu et al., 2023). LongT5, LED models and weights are released under the Apache 2.0 license. The license for the LSG model and weights is unknown.", "figure_data": "#Examples per splitInput LengthDatasetTrain ValidationTestAverageMedianMax90 tharXiv203,0376,436 6,44010,720.188,519 378,82520,170PubMed119,9246,633 6,6584,747.973,883 452,9158,883GovReport17,45797297310,576.068,840 240,73418,834SummScreenFD3,6733383379,589.369,04426,44715,171BookSum-Chapter9,6001,484 1,4315986.474311 204,56711,804BookSum-Book3144546 143,562.75 104,381 667,817 305,749G Implementation detailsEvaluation. For ROUGE score compu-tations, we used the implementation fromhttps://github.com/google-research/google-research/tree/master/rouge,releasedunderApache2.0license.BERTScore was computed using the pack-age https://pypi.org/project/bert-score/and is released under a MIT license. BLANC usinghttps://pypi.org/project/blanc/, releasedunder a MIT license."}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Statistics for the summarization datasets. Input length is computed using a SentencePiece tokenizer.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "Transformer (full) O(L 2 ) LED O(Lw) BigBird O(Lw + L(g + r)) LSG O(Lw + L(g + s)) LongT5 (TGlobal) O(Lw + L \u230aL/c\u230b) LOCOST O(L log(L))", "formula_coordinates": [3.0, 97.65, 81.4, 160.91, 60.04]}, {"formula_id": "formula_1", "formula_text": "x j+1 = Ax j + bu j+1 , y j+1 = c \u22a4 x j+1 + du j+1 ,(1)", "formula_coordinates": [3.0, 361.32, 185.03, 163.82, 29.45]}, {"formula_id": "formula_2", "formula_text": "\uf8f1 \uf8f2 \uf8f3 y = \u03ba * u + du, \u03ba = c \u22a4 b, c \u22a4 Ab, . . . , c \u22a4 A L\u22121 b .", "formula_coordinates": [3.0, 329.86, 443.2, 169.64, 36.44]}, {"formula_id": "formula_3", "formula_text": "y j = l\u2264j \u2190 \u2212 \u03ba j\u2212l \u2299 u l + l\u2265j \u2212 \u2192 \u03ba l\u2212j \u2299 u l + d \u2299 u j = BiSSM(U ) j .(3)", "formula_coordinates": [4.0, 307.4, 358.77, 217.74, 45.95]}, {"formula_id": "formula_4", "formula_text": "\u2212 \u2192 A, \u2212 \u2192 c , \u2212 \u2192 b ) and ( \u2190 \u2212 A, \u2190 \u2212 c , \u2190 \u2212 b )", "formula_coordinates": [4.0, 405.41, 448.84, 117.45, 21.65]}, {"formula_id": "formula_5", "formula_text": "\u2202y j \u2202u l \u2264 \u03c1( \u2190 \u2212 A) j\u2212l | \u2190 \u2212 c \u22a4 \u2190 \u2212 b | if l < j, \u03c1( \u2212 \u2192 A) l\u2212j | \u2212 \u2192 c \u22a4 \u2212 \u2192 b | if l > j.", "formula_coordinates": [4.0, 331.13, 622.68, 171.92, 42.96]}, {"formula_id": "formula_6", "formula_text": "\u2022 Embedding matrix U \u2208 R L\u00d7H is first projected onto Q, V \u2208 C L\u00d7H .", "formula_coordinates": [5.0, 70.87, 179.22, 218.26, 33.42]}, {"formula_id": "formula_7", "formula_text": "U = arg top-K j R-1 i\u0338 =j {s i }, s j (4)", "formula_coordinates": [5.0, 341.77, 669.23, 183.37, 23.35]}, {"formula_id": "formula_8", "formula_text": "GovReport SummScreenFD Model L R-1 R-2 R-L % R-1 R-2 R-L %", "formula_coordinates": [7.0, 107.24, 199.85, 282.22, 18.09]}, {"formula_id": "formula_9", "formula_text": "(\u03ba * u)[j] = j l=0 \u03ba[j \u2212 l]u[l].(5)", "formula_coordinates": [13.0, 115.07, 177.65, 174.8, 34.56]}, {"formula_id": "formula_10", "formula_text": "(\u03ba \u229b u)[j] = L\u22121 l=0\u03ba [j \u2212 l]u[l].(6)", "formula_coordinates": [13.0, 113.14, 373.02, 176.73, 33.98]}, {"formula_id": "formula_11", "formula_text": "\u03ba \u229b u = F \u22121 (\u03ba \u2299\u00fb) ,(7)", "formula_coordinates": [13.0, 126.99, 467.38, 162.88, 20.42]}, {"formula_id": "formula_12", "formula_text": "C H\u00d7N \u00d7N is made of H diagonal matrices A h = diag(\u03bb h ) \u2208 C N \u00d7N . \u2022 For 0 \u2264 h \u2264 H and 0 \u2264 n \u2264 N , \u03bb \u2208 R H\u00d7N is \u03bb h,n = exp \u2206 h \u03bb Re h,n + i\u2206 h \u03bb Im h,n .", "formula_coordinates": [13.0, 306.14, 504.27, 218.27, 65.6]}], "doi": "10.18653/v1/2022.acl-long.589"}