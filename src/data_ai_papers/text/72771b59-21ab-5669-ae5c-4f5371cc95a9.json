{"title": "Probabilistic Tracking with Exemplars in a Metric Space", "authors": "Kentaro Toyama; Andrew Blake", "pub_date": "", "abstract": "A new, exemplar-based, probabilistic paradigm for visual tracking is presented. Probabilistic mechanisms are attractive because they handle fusion of information, especially temporal fusion, in a principled manner. Exemplars are selected representatives of raw training data, used here to represent probabilistic mixture distributions of object configurations. Their use avoids tedious hand-construction of object models, and problems with changes of topology. Using exemplars in place of a parameterized model poses several challenges, addressed here with what we call the \"Metric Mixture\" (M 2 ) approach, which has a number of attractions. Principally, it provides alternatives to standard learning algorithms by allowing the use of metrics that are not embedded in a vector space. Secondly, it uses a noise model that is learned from training data. Lastly, it eliminates any need for an assumption of probabilistic pixelwise independence. Experiments demonstrate the effectiveness of the M 2 model in two domains: tracking walking people using \"chamfer\" distances on binary edge images, and tracking mouth movements by means of a shuffle distance.", "sections": [{"heading": "Introduction", "text": "There is, of course, a substantial literature on tracking, driven either by image features (Amini et al., 1988;Kass et al., 1987) or by raw intensity (Bascle and Deriche, 1995;Black and Jepson, 1996;Hager and Toyama, 1996), or both (Cootes et al., 1998). Tracking can be formulated in a probabilistic framework in both the feature-driven (Terzopoulos and Szeliski, 1992) and intensity-driven (Storvik, 1994) settings. The probabilistic formulation has the attraction that uncertainty is handled in a systematic fashion, allowing principled handling of sensor fusion and temporal fusion. Many such tracking algorithms, however, demand that complex models be defined and trained for each object class * http://research.microsoft.com/vision. to be tracked-a process that is often laborious and difficult to automate fully.\nOur aim, therefore, is to develop a paradigm which retains the probabilistic setting while avoiding the use of explicit models to describe target objects. The use of exemplars, for example, the contour exemplars in Fig. 1, offers an alternative that can tackle this problem (Brand, 1999;Efros and Leung, 1999;Freeman and Pasztor, 1999;Frey and Jojic, 2000;Gavrila and Philomin, 1999). Exemplar-based models can be constructed very directly from training sets, without the need to set up complex intermediate representations, such as parameterized contour models or 3D articulated models.\nExisting tracking algorithms that use exemplarbased models have certain limitations. Single-frame exemplar-based tracking (Gavrila and Philomin, 1999), though effective, is limited by its inability to incorporate temporal constraints, resulting in jerky recovered motion and reduced power to recover from occlusion. Full temporal tracking can be obtained via Kalman filtering or particle filtering, for which a probabilistic framework is needed. Frey and Jojic (2000) have demonstrated elegantly how exemplars can be embedded in learned probabilistic models by treating them as centers in probabilistic mixtures. Their motionsequence analysis is, in principle, fully automated, requiring only the structural form of a generative image-sequence model to be specified in advance. However, the approach has serious drawbacks:\n\u2022 inference is done with online expectationmaximization (EM), which is computation intensive and limited, for practical purposes, to low resolution images; \u2022 images have to be represented simply as arrays of pixels, ruling out nonlinear transformations that can help with invariance to scene conditions, including the conversion of images to edge maps that proves so powerful with non-probabilistic exemplars (Gavrila and Philomin, 1999); \u2022 finally, image noise is treated as white despite known, strong statistical correlations between pixels (Field, 1987).\nThe problem, therefore, is to combine exemplars in a metric space (Gavrila and Philomin, 1999) with a probabilistic treatment (Frey and Jojic, 2000), retaining the best features of each approach. Unfortunately, this combination is not straightforward. The very techniques which make probabilistic treatment possible (i.e., modeling with Gaussians, PCA, k-means, EM, etc.), are not applicable given that exemplar-based models need have no vector-space structure. (There is no clear sense in which two of the outline contours in Fig. 1 can be added together.) We propose the Metric Mixture (M 2 ) model, described below, to solve this problem. Figure 1 shows the approach applied to tracking a walking person. One note on terminology: the theory and algorithms could be presented as for true metrics. A function \u03c1 is a metric when ( 1 (a, c). The M 2 theory, however, can also apply to certain functions without axioms (3) and (4). We will refer to these latter functions as \"distance functions.\"\n) \u03c1(a, b) \u2265 0, \u2200a, b, (2) \u03c1(a, b) = 0 iff a = b, (3) \u03c1(a, b) = \u03c1(b, a), and (4) \u03c1(a, b) + \u03c1(b, c) \u2265 \u03c1", "publication_ref": ["b0", "b16", "b2", "b3", "b13", "b5", "b23", "b22", "b6", "b8", "b9", "b10", "b10", "b9", "b10", "b7", "b10", "b9"], "figure_ref": ["fig_0", "fig_0", "fig_0"], "table_ref": []}, {"heading": "Pattern-Theoretic Tracking", "text": "Test image sequences Z = {z 1 , . . . , z T } are to be analyzed in terms of a probabilistic model learned from a training image sequence Z * = {z * 1 , . . . , z * T * }. Images may be preprocessed for ease of analysis, for example by filtering to produce an intensity image with certain features (e.g., ridges) enhanced, or nonlinearly filtered to produce a sparse binary image with feature pixels marked. A given image z is to be approximated, in a standard pattern theoretic manner (Mumford, 1996), as an ideal image or object x \u2208 X that has been subjected to a geometrical transformation T \u03b1 from a continuous set \u03b1 \u2208 A, i.e.: z \u2248 T \u03b1 x.\n(1)", "publication_ref": ["b18"], "figure_ref": [], "table_ref": []}, {"heading": "Transformations and Exemplars", "text": "The partition of the underlying image space into the transformation set A and class X of normalized images could take a variety of forms. For example, in analysis of face images, A could be a shape space, modeling geometrical distortions, and X could be a space of textures, in the manner of Cootes et al. (1998) and Vetter and Poggio (1996). Alternatively A could be a space of planar similarity transformations, leaving X to absorb both distortions and texture/shading distributions. In any case, A is to be defined analytically in advance, leaving X to be inferred from the training sequence Z * . A feature of this work is that the class X of normalized images is not assumed to be amenable to straightforward analytical description; instead X is defined in terms of a set {x k , k = 1, . . . , K } of exemplars, together with a distance function \u03c1, in the spirit of Gavrila (Gavrila and Philomin, 1999). For example, the face of a particular individual, might be represented by a set of exemplarsx k consisting of normalized (registered), frontal views of that face, wearing a variety of expressions, and in a variety of poses and lighting conditions. Crucially, exemplars will be interpreted probabilistically, so that the uncertainty inherent in the approximation (1) is accounted for explicitly. The interpretation of an image z is then as a state vector X = (\u03b1, k).", "publication_ref": ["b5", "b24", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Learning", "text": "Aspects of the probabilistic model that must be learned from Z * include:\n1. The set of exemplars {x k , k = 1, . . . , K }. 2. Component distributions, centered on each of the T \u03b1xk , for some \u03b1 for observations given state X = (\u03b1, k). The details of this density, and the algorithm for learning it, constitute a new approach to the vexed question of how to model image observations probabilistically without tripping over the issue of statistical independence. 3. A predictor in the form of a conditional density p(X t | X t\u22121 ) to represent the (typically strong) prior dependency between states at successive time steps.\nThese elements (together with a prior p(X 1 )) form a structured prior distribution for a randomly sampled image sequence z 1 , . . . , z T , which can be tested for plausibility by random simulation (see Fig. 3, for example).\nThe prior model then forms a basis for interpretation of image sequences via the posterior\np(X 1 , X 2 , . . . | z 1 , z 2 , . . . ; )\nwhere is the set of learned parameters of the probabilistic model, including the exemplar set, the noise parameters, and the dynamic model.", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Probabilistic Modeling of Images and Observations", "text": "The probabilistic dependency structure for the M 2 model is depicted in Fig. 2 and is similar to Frey and Jojic (2000). However, the similarity of dependency structure belies crucial innovations in representation and probability distributions which are explained below.", "publication_ref": ["b9"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Objects", "text": "An object in the class X is taken to be an image that has been preprocessed to enhance certain features, resulting in a preprocessed image x. The M 2 approach is general enough to apply to a variety of such imageswe will consider two: unprocessed raw images, and sparse binary images with true-valued pixels marking a set of feature curves. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Patches.", "text": "In the case of real-valued output from preprocessing, z is an image subregion, or patch, visible as an intensity function I z (r). As mentioned earlier, it is undesirable to have to assume a known parameterization of the intensity function on that patch.\nFor now, we make the conservative assumption that some linear parameterization, with parameters y \u2208 R d , of a priori unknown form and dimension d, exists, so that:\nI z (r) = d i=1 I i (r)y i (2\n)\nwhere I 1 (r), . . . , I d (r) are independent image basis functions and y = (y 1 , . . . , y d ). Given the linearity assumption, all that will need be inferred about the nature of the patch basis is its dimensionality d. There is no requirement to know or infer the form of the I i . A suitable distance function \u03c1 is needed for patches. For robustness we will use a \"shuffle distance\" (Kutulakos, 2000), which is an L 2 norm applied after first associating each pixel in one image with the most similar pixel in a neighborhood around the corresponding pixel in the other image. (We show later why we chose this distance over others.)", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}, {"heading": "Curves.", "text": "The situation for binary images is similar to that for patches, except that a different distance function is needed, and the interpretation of the linear parameterization is a little different, too. Now z is visible as a curve r z (s), with curve-parameter s, and linearly dependent on y \u2208 R d :\nr z (s) = d i=1 r i (s)y i , (3\n)\nwhere r 1 (s), . . . , r d (s) are now independent curve basis functions such as parametric B-splines (Bartels et al., 1987). In this case, the distance measure \u03c1 used is a (non-symmetric) chamfer distance (Gavrila and Philomin, 1999). The chamfer distance is defined to be\n\u03c1(z, z) = min s (s) ds g(|r z (s ) \u2212 rz(s)|),(4)\nwhere g(\u2022) is the profile of the chamfer. A particularly interesting case is the quadratic chamfer, in which g(u) = u 2 , or a truncated form g(u) = min(u 2 , g 0 ). In that case, chamfer distance ( 4) is known to approximate a curve-normal-weighted L2 distance between the two curves, in the limit that they are similar. (Note that chamfer distance is related to Hausdorff distance which has been used successfully in tracking (Huttenlocher et al., 1993); the difference is that the integral in (4) becomes a max operator in the Hausdorff distance.) A great attraction of the chamfer distance is that it can be computed directly from the (binary) images z andz, as \u03c1(z, z) = ds \u03b3 (z, rz(s))\n(5) using a chamfer image\n\u03b3 (z, r) = min s g(|r z (s ) \u2212 r|)\nconstructed directly from z. This allows \u03c1(z, z) to be evaluated repeatedly for a given z and variousz directly from (5) which, being simply a curve integral (approximated), is numerically very efficient.", "publication_ref": ["b1", "b10", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Geometric Transformations", "text": "Geometric transformations \u03b1 \u2208 A are applied to exemplars to give transformed mixture centers:\nz = T \u03b1x .\nFor example, in the case of Euclidean similarity, \u03b1 = (u, \u03b8, s) and vectors transform as\nT \u03b1 r = u + R(\u03b8 ) s r,\nin which (u, \u03b8, s) are offset, rotation angle and scaling factor respectively. Where the observations are curves, this induces a transformation\nr z (s) = T \u03b1 r x (s)\nand in the case of patches, the induced transformation is I z (T \u03b1 r) = I x (r).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Metric Mixture (M 2 ) Model", "text": "The observation likelihood function, at the heart of the M 2 approach, can now be specified. Note that the observation is deemed to be the finite dimensional vector y, rather than the infinite dimensional image or curve z.\nThe full image/curve is accessed only as a \"machine\" for computing an observation density. Note also, We exploit the fact that we only need to know enough about p(y | X ) to evaluate it. There is no call to sample from it. Hence no constructive form for the observer need be given, and we can avoid controversies about pixelwise independence.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Exemplars as Mixture Centers.", "text": "The object class is defined in terms of a set X = {x k , k = 1, . . . , K } of untransformed exemplars, to be inferred from the training set Z. A transformed exemplarz serves as a center in a mixture component:\np(y |z) \u221d 1 Z exp \u2212 \u03bb\u03c1(z, z) (6)\n-a \"metric exponential\" distribution-whose normalization constant or \"partition function\" is Z .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Metric-Based Mixture Kernels.", "text": "For tracking of the full state, both motion and shape, the hypothesis is X = (\u03b1, k). The mixture model above leads to an observation likelihood\np(y | X ) \u2261 p(y | \u03b1, k) \u221d 1 Z exp \u2212 \u03bb\u03c1(T \u03b1xk , z). (7)\nIf only motion is to be tracked, the hypothesis is simply \u03b1 so the observation likelihood becomes\np(y | \u03b1) \u221d k \u03c0 k 1 Z exp \u2212 \u03bb\u03c1(T \u03b1xk , z),\na mixture with component priors \u03c0 k . For this interpretation to make sense, it is necessary to \"tie\" the dimension d k of the y-space associated with each component to be a constant d k = d. Henceforth, we deal with the joint \u03b1, k space as in ( 7) so that tying the d k will not be necessary.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Partition Function.", "text": "In order to learn the value of an exponential parameter \u03bb from training data, it is necessary to know something about the partition function Z . This is difficult in general, but straightforward in the case that \u03c1 is a (truncated) quadratic chamfer function because that gives an approximately Gaussian distribution. Similarly, an L 2 norm on patches leads to a Gaussian mixture distribution, as does the shufflemetric used in experiments reported here. 1 In that case, the exponential constant \u03bb in the observation likelihood is interpreted as \u03bb = 1 2\u03c3 2 , where \u03c3 is an distance constant, and the partition function is Z \u221d \u03c3 d . From this, it can be shown (see appendix) that the chamfer distance \u03c1 |z \u2261 \u03c1(z, z) is a \u03c3 2 \u03c7 2 d random variable (i.e., \u03c1/\u03c3 2 has a \u03c7 2 d distribution) which is in fact also a distribution. This allows the parameters \u03c3, d of the observation likelihood (7) to be learned from training data, as set out below.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Learning Algorithms", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Learning Mixture Kernel Centers", "text": "Following the probabilistic interpretation of exemplars as kernel centersx k in (6), we exploit the temporal continuity of the training sequence Z * to choose initial mixture centers, and proceed to cluster iteratively.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "1.", "text": "The training set is assumed to be approximately aligned from the outset (this is easily achieved provided the foreground in the training sequence is reasonably easy to separate, for example by background subtraction/filtering). To improve the initial alignment, first a datum, z * 0 , is chosen from the entire training sequence Z * according to a minimax rule:\nz * 0 \u2190 arg min z\u2208Z * max z \u2208Z * \u2212{z} \u03c1(z, z ).\nThen,\n\u03b1 * t = arg min \u03b1 \u03c1 T \u22121 \u03b1 z * t , z * 0 and x * t = T \u22121 \u03b1 * t z * t ,\nminimizing by direct descent. 2. To initialize centers, a subsequence of the x * t is chosen to form the initialx k , selected in such a way as to be evenly spaced in chamfer distance. Thus th\u1ebd x k are chosen so that \u03c1(x k+1 ,x k ) \u2248 \u03c1 c , for some appropriate choice of \u03c1 c that gives approximately the required number K of exemplars. 3. For the remainder of the aligned training data x * t , t = 1 . . . T * , find the cluster that minimizes the distance from x * t to the cluster center:\nk t (x * t ) = arg min k \u03c1(x k , x * t ). (8\n)\nLabel the set of all elements in cluster k as C k = {x * t : k t (x * t ) = k} and let N k = |C k |.\n4. For each cluster k, find the new representative, which is the element in that cluster that minimizes the maximum distance to all other elements in that cluster:\nx k \u2190 arg min x\u2208C k max x \u2208C k \u2212{x} \u03c1(x, x ). (9\n)\n5. Repeat Steps 3 and 4 for a fixed number of iterations or until convergence and save the final exemplarsx k . 6. Set mixture weights:\n\u03c0 k \u221d N k .\nSteps 3 and 4 implement a k-medoids algorithm (Ripley, 1996). This is analogous to the iterative computation of cluster centers in the k-means algorithm, but is applicable in a non-metric space where it is impossible to compute cluster means. In place of a mean, an existing member of the training set is chosen by a minimax rule, since that is equivalent to the mean in the limit that the training set is dense and is defined over a vector space with a Euclidean distance.", "publication_ref": ["b21"], "figure_ref": [], "table_ref": []}, {"heading": "Learning the M 2 Kernel Parameters", "text": "To learn observation likelihood parameters \u03c3, d, we obtain a validation set Z v . (This could simply be the training set Z less the (unaligned) exemplars {z k }.) For each z v from Z v , the corresponding aligning transformation \u03b1 v and mixture centerx v is estimated by minimizing, by direct descent, the distance:\nmin \u03b1\u2208A,x\u2208X \u03c1(T \u03b1x , z v ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Now, following Section 3.3, we treat the distances", "text": "\u03c1 v (z v ) = \u03c1 T \u03b1 vx v , z v , z v \u2208 Z v as \u03c3 2 \u03c7 2 d distributed.\nAn approximate but simple approach to parameter estimation is via the sample moments\n\u03c1 k = 1 N k z v \u2208C k \u03c1 v (z v ) and\u03c1 2 k = 1 N k z v \u2208C k \u03c1 2 v (z v ),\nwhich, from the form of the mean and variance of the \u03c7 2 statistic, in terms of \u03c1, \u03c3 , gives the following estimates for d k and \u03c3 k :\nd k = 2\u03c1 2 k \u03c1 2 k \u2212\u03c1 2 k and \u03c3 k = \u03c1 k /d. (10\n)\nIntuitively, d k is estimated here in terms of the histogram of \u03c1-values. A histogram whose mass is concentrated at low \u03c1-values gives a lower d estimate.\nAlternatively, the full maximum likelihood (MLE) solution, complete with integer constraint on d, yields \u03c3 values in terms of d, exactly as above, and integer d \u2265 1 as the value maximizing the likelihood\nL(d) = \u2212log (d/2) + (d/2)(log(d/2) \u22121 \u2212 log(\u03c1 a /\u03c1 g )) (11\n)\n(dropping the k-subscripts for simplicity), where\u03c1 a ,\u03c1 g are respectively the arithmetic and geometric means of the \u03c1-samples, and (\u2022) is the well-known, transcendental -function. Such a d can always be found since L(d) is asymptotically a decreasing function of d.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Notes", "text": "1. If\u03c1 a /\u03c1 g > 4/e the solution for d is the trivial d = 1.\n2. The estimation procedures are equivalent to fitting a -distribution to the \u03c1-values to determine parameters d k . The moments estimator fits an unconstrained -distribution, so the integer constraint on d is not applied.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The MLE applies the integer constraint to d.", "text": "However, in practice the MLE turns out to be less robust than a moments estimator, in cases when the observed \u03c1 statistic does not follow the assumed distribution closely.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Learning Dynamics", "text": "In line with recent developments in probabilistic tracking (Blake and Isard, 1998), sequences of estimated X t from a training set are treated as if they were fixed timeseries data, and used to learn two components (assumed independent) of p(X t | X t\u22121 ):\n1. a Markov matrix M for p(k t | k t\u22121 ), learned by histogramming transitions; 2. a first order auto-regressive process (ARP) for p(\u03b1 t | \u03b1 t\u22121 ), with coefficients calculated using the Yule-Walker algorithm (Gelb, 1974).", "publication_ref": ["b4", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "In order to demonstrate the necessity for, and applicability of, the M 2 model, we performed tracking experiments in two separate domains. In the first, we tracked walking people using contour edges. Here, background clutter and simulated occlusion threaten to distract tracking without a reasonable dynamic model and a good likelihood function.\nIn the second, we track a person's mouth based on raw pixel values. Unlike the pedestrian-tracking domain, images are cropped such that only the mouth, and no background, is visible. While distraction is not a problem, the complex articulations of the mouth make tracking difficult (even state-ofthe-art face-tracking algorithms (Cootes et al., 1998;Neven, 2000) have difficulty tracking lip and tongue articulation).   Dimension appears to be consistently slightly overestimated (cluster size N = 1000). This may be due to the approximation inherent in using the chamfer distance.", "publication_ref": ["b5", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Tracking Human Motion", "text": "(naturally, we took advantage of the fixed background only for the purposes of generating exemplars-not for tracking). Examples of a few exemplars are shown in Fig. 3. Dynamics were learned as described in Section 4.3 on 5 sequences of the same walking person, each about 100 frames long. Figure 3 overlays several frames from a sequence generated at random from the learned model. The full sequence is available as generatd.mpg. 2", "publication_ref": [], "figure_ref": ["fig_3", "fig_3"], "table_ref": []}, {"heading": "Validity of the M 2 Model.", "text": "A practical test of the M 2 methodology is whether consistent d values can be estimated from Eq. (10). We tested this for chamfer distance by conducting experiments on synthetically generated polygons with d vertices, with the results shown in Fig. 4.\nFigure 5 shows values of dimension d for the pedestrian contour exemplars. Note that dimensionality increases with cluster size up to a point, but it eventually converges to d \u2248 10. We read the fact that d does not simply increase unboundedly with cluster size, as an indication that d reflects an intrinsic local dimensionality.", "publication_ref": [], "figure_ref": ["fig_4", "fig_5"], "table_ref": []}, {"heading": "Practical Tracking.", "text": "We can now compute observation likelihoods as in Eq. ( 7) and track using the following Bayesian framework. A classical forward algorithm (Rabiner, 1989) would give p t (X t ) \u2261 p(X t | z 1 , . . . , z t ) as:\np t (X t ) = k t\u22121 \u03b1 t\u22121 p(y t | X t ) p(X t | X t\u22121 ) p t\u22121 (X t\u22121 ),\nwhere p(y t | X t ) is computed according to Eq. (7). Exact inference is infeasible given that \u03b1 is real-valued, so the integral is performed using a form of particle filter (Gordon et al., 1993;Isard and Blake, 1996). To display results, we calculateX t = arg max p t (X t ).\nFigure 1 shows cropped, sample images of tracking on a sequence that was not in the training sequence. Tracking in this case is straightforward and accurate. Figure 6 shows the same exemplar set (trained on Figure 6. Cropped, sample frames from a tracked test sequence. The same learned model is used as Fig. 1, but now the test sequence contains a new individual walking. The motion is captured nonetheless, though the match is not quite so close-note the arms in the final frame (see also video walk3.mpg).\nFigure 7. Cropped, sample frames from a tracked test sequence. The same learned model and test sequence is used as in Fig. 6, but now the test sequence is periodically blanked out, to test robustness to occlusion (see also video walk3occ.mpg).\none person) used to track a different person entirely. Although the swing of this subject's arms is not captured by the existing exemplars, the gait is nevertheless accurately tracked. Finally, we ran an experiment to verify tracking robustness against occlusion and other visual disturbances. In Fig. 7, we simulated occlusions by rendering black two adjacent frames out of every ten frames in the test sequence, and so tracking was forced to rely on the prior in these frames.\nThe sequence was accurately tracked in the nonoccluded frames, bridged by reasonable state estimates in the black frames-something that would be impossible without incorporation of a dynamic model.\nExperiments with a more complex and agile set of movements are shown in Fig. 8. In this case it is necessary to use a greater number of exemplars (K = 300). Note that the experiments here show unsupervised learning-parameter estimation and tracking-on a single sequence. ", "publication_ref": ["b20", "b12", "b15"], "figure_ref": ["fig_0", "fig_0", "fig_6"], "table_ref": []}, {"heading": "Mouth Tracking", "text": "The mouth tracking sequences consisted of closely cropped images of a single subject's mouth while the person was speaking and making faces. The training sequence consisted of 210 frames captured at 30 Hz. We tested on a longer test sequence of 570 frames (of which 270 are shown in the video files described below). Dynamics were learned as in Section 4.3, with K = 30 exemplar clusters. Tracking was performed as in Section 5.1, but with no \u03b1 transformations, since the images were largely registered. On this training set, the shuffle distance d values exhibited greater variance (the extremes running from 1.2 to 13.8), but the majority of clusters showed a dimensionality of d = 4 \u00b1 1, indicating again that the dimension constant d in the M 2 model is learned consistently (see Fig. 9 for a histogram showing the distribution of estimated dimensions).\nThe results for this experiment can be seen in video format (see also, Fig. 10): ml2.mpg shows the result of tracking based on the L 2 distance (Euclidean distance between vectors formed by concatenating the raw pixel values of an image), and mshuffle.mpg shows tracking using the shuffle distance.\nIn the videos, the left-hand image shows the test image, and the right-hand image shows the a posteri- ori best-match exemplar from the training sequence. Both functions do well with the initial two-thirds of the test sequence, while the subject is speaking. As soon as the subject begins to make faces and stick out his tongue, the L 2 -based likelihood fails, whereas tracking based on the shuffle distance continues largely successfully.\nFigure 10. A sampling of frames from the mouth sequence. Top row, the test sequence; middle row, tracking using the L 2 distance; bottom row, tracking using the shuffle distance. The shuffle distance produces matches that are perceptually more similar to the test sequence. Figure 11 shows a comparison of maximumlikelihood matches, on one of the difficult test imagesa tongue sticking out to the left-for a variety of distance functions. Most of the functions prefer an exemplar without the tongue. This may be because of the high contrast between pixels projected dimly by the inside of the mouth and those projected brightly by lip and tongue; even a small difference in tongue configuration can result in a large difference in L 2 , and other, distances. On the other hand, the flow-based distance and the shuffle distance (really an inexpensive version of the flow-based distance) return exemplars that are perceptually similar. These functions come closer to approximating perceptual distances by their relative invariance to local warping of images. These observations were what originally led to our experiments with different distance functions, and they justify the need for the ability to handle metrics that are not embedded in a vector space.", "publication_ref": [], "figure_ref": ["fig_7", "fig_0", "fig_0", "fig_0"], "table_ref": []}, {"heading": "Conclusion", "text": "The Metric Mixture approach combines the advantages of exemplar-based models (Gavrila and Philomin, 1999) with a probabilistic framework (Frey and Jojic, 2000) into a single probabilistic exemplar-based paradigm. The power of the M 2 technique comes from its generality: both object models and noise models can be learned automatically, and metrics can be chosen without significant restrictions on the structure of the metric space (a drawback of Markov random field models of image-pixel dependencies, for example).\nWe intend to explore several avenues in future work:\n\u2022 One problem with exemplar sets is that they can grow exponentially with object complexity. Tree structures appear to be an effective way to deal with this problem (Gavrila and Philomin, 1999;Wei and Levoy, 2000), and we would like to find effective ways of using them in a probabilistic setting. Note however, that the use of a dynamical model for prediction greatly reduces the effective size (perplexity) of the exemplar set, so the lack of tree structure has not been a serious limiting factor yet. \u2022 We propose to continue testing on sequences with more intense background clutter, and with more varied transformations \u03b1, to explore the limits of the exemplar approach. where O(y) is a linear term in the parameter vector y. Matrix H i, j is a nonsingular, symmetric, metric matrix (Blake and Isard, 1998) which can be diagonalized as H = UDU , in which U is orthogonal and D is diagonal. Now, from (6), and using the normalization properties of Gaussians,\np(z |z) = ( \u221a 2\u03c0\u03c3 ) \u2212d |H| \u22121/2 exp \u2212 1 2\u03c3 2 (\u03c1 |z),\nwhere 1/(2\u03c3 2 ) = \u03bb as before. Therefore y is a normal random variable: y = Bw where w \u223c N (0, I d ) and\nB = \u03c3 H \u22121/2 = \u03c3 U D \u22121/2 U .\nFinally, \u03c1 |z = w B H \u22121 Bw = \u03c3 2 w w so (\u03c1 |z) is a \u03c3 2 \u03c7 2 d random variable, as claimed.", "publication_ref": ["b10", "b9", "b10", "b25", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We thank P. Anandan, Neil Lawrence, and Chris Williams for stimulating discussions. John MacCormick kindly provided video data; Rick Szeliski, code for performing flow-based matching.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Notes", "text": "1. The shuffle-metric can be thought of as using an image-sized array s of hidden variables augmenting the state vector X , before applying a classical L 2 norm. 2. All movie files mentioned in this paper are available at http://research.microsoft.com/vision/papers.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Using dynamic programming for minimizing the energy of active contours in the presence of hard constraints", "journal": "", "year": "1988", "authors": "A Amini; S Tehrani; T Weymouth"}, {"ref_id": "b1", "title": "An Introduction to Splines for use in Computer Graphics and Geometric Modeling", "journal": "Morgan Kaufmann", "year": "1987", "authors": "R Bartels; J Beatty; B Barsky"}, {"ref_id": "b2", "title": "Region tracking through image sequences", "journal": "", "year": "1995-06", "authors": "B Bascle; R Deriche"}, {"ref_id": "b3", "title": "Eigentracking: Robust matching and tracking of articulated objects using a view-based representation", "journal": "", "year": "1996", "authors": "M Black; A Jepson"}, {"ref_id": "b4", "title": "Active Contours. Springer: Berlin. Brand, M. 1999. Shadow puppetry", "journal": "", "year": "1998", "authors": "A Blake; M Isard"}, {"ref_id": "b5", "title": "Active appearance models", "journal": "", "year": "1998", "authors": "T Cootes; G Edwards; C Taylor"}, {"ref_id": "b6", "title": "Texture synthesis by non-parametric sampling", "journal": "", "year": "1999", "authors": "A Efros; T Leung"}, {"ref_id": "b7", "title": "Relations between the statistics of natural images and the response properties of cortical cells", "journal": "J. Optical Soc. of America A", "year": "1987", "authors": "D Field"}, {"ref_id": "b8", "title": "Learning to estimate scenes from images", "journal": "MIT Press", "year": "1999", "authors": "W Freeman; E Pasztor"}, {"ref_id": "b9", "title": "Learning graphical models of images, videos and their spatial transformations", "journal": "", "year": "2000", "authors": "B Frey; N Jojic"}, {"ref_id": "b10", "title": "Real-time object detection for smart vehicles", "journal": "", "year": "1999", "authors": "D Gavrila; V Philomin"}, {"ref_id": "b11", "title": "Applied Optimal Estimation", "journal": "MIT Press", "year": "1974", "authors": "A Gelb"}, {"ref_id": "b12", "title": "Novel approach to nonlinear/non-Gaussian Bayesian state estimation", "journal": "IEE Proc. F", "year": "1993", "authors": "N Gordon; D Salmond; A Smith"}, {"ref_id": "b13", "title": "XVision: Combining image warping and geometric constraints for fast tracking", "journal": "", "year": "1996", "authors": "G Hager; K Toyama"}, {"ref_id": "b14", "title": "Tracking nonrigid objects in complex scenes", "journal": "", "year": "1993", "authors": "D Huttenlocher; J Noh; W Rucklidge"}, {"ref_id": "b15", "title": "Visual tracking by stochastic propagation of conditional density", "journal": "", "year": "1996-04", "authors": "M Isard; A Blake"}, {"ref_id": "b16", "title": "Snakes: Active contour models", "journal": "", "year": "1987", "authors": "M Kass; A Witkin; D Terzopoulos"}, {"ref_id": "b17", "title": "Approximate N -view stereo", "journal": "", "year": "2000", "authors": "K Kutulakos"}, {"ref_id": "b18", "title": "Pattern theory: A unifying perspective", "journal": "Cambridge University Press", "year": "1996", "authors": "D Mumford"}, {"ref_id": "b19", "title": "Eyematic interfaces", "journal": "", "year": "2000", "authors": "H Neven"}, {"ref_id": "b20", "title": "A tutorial on hidden Markov models and selected applications in speech recognition", "journal": "", "year": "1989", "authors": "L R Rabiner"}, {"ref_id": "b21", "title": "Pattern Recognition and Neural Networks", "journal": "Cambridge University Press", "year": "1996", "authors": "B Ripley"}, {"ref_id": "b22", "title": "A Bayesian approach to dynamic contours through stochastic sampling and simulated annealing", "journal": "IEEE Trans. Patt. Anal. Mach. Intel", "year": "1994", "authors": "G Storvik"}, {"ref_id": "b23", "title": "Tracking with Kalman snakes", "journal": "", "year": "1992", "authors": "D Terzopoulos; R Szeliski"}, {"ref_id": "b24", "title": "Image synthesis from a single example image", "journal": "", "year": "1996-04", "authors": "T Vetter; T Poggio"}, {"ref_id": "b25", "title": "Fast texture synthesis using treestructured vector quantization", "journal": "ACM", "year": "2000", "authors": "L.-Y Wei; M Levoy"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. Cropped, sample frames from a tracked test sequence. The overlays represent the maximum a posteriori exemplars. Exemplars and dynamics were learned from an independent training sequence of the same individual walking along a similar path (see also the video walk1.mpg, viewable at http://research.microsoft.com/vision/papers).", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 .2Figure 2. Probabilistic graphical structure for the M 2 model: The observation z t at time t is an image drawn from a mixture with centers {T \u03b1xk , k = 1, . . . , K }, where {x k , k = 1, . . . , K } are exemplars; T \u03b1 is a geometrical transformation, indexed by the (realvalued) parameter \u03b1.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Forthe person tracking experiments, training and test sequences show various people walking from right to left in front of a stationary camera. The background in all of the training sequences is fixed, allowing us to use simple background subtraction and edge-detection routines to automatically generate the exemplars", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 .3Figure 3. A sequence generated at random from a model based on learned dynamics and exemplars. Edges shown represent the contours of successive model exemplars.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 4 .4Figure 4. Convergence of d-estimate with cluster size, for synthesized polygons. (a) Estimated dimension converges to around d = 24 as cluster size increases to 1000 (true dimension d = 22). (b) Estimated dimensionality (solid) closely follows ground truth dimensionality (dashed).Dimension appears to be consistently slightly overestimated (cluster size N = 1000). This may be due to the approximation inherent in using the chamfer distance.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 .5Figure 5. Estimated dimension d for image contour exemplars, using quadratic chamfer distance, appears to behave consistently: it converges to d = 10 as data set size increases.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 8 .8Figure 8. Results on learning only, with a more varied set of motions, requiring a larger number of exemplars. The sequence is taken from the movie, Center Stage (see also video ballet0.mpg).", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 9 .9Figure 9. A histogram of estimated dimensionality for clusters learned for the mouth-tracking sequence.", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 11 .11Figure 11. Best match, based on various distance functions: (a) test image, (b) L 2 distance, (c) L 2 after blurring, (d) histogram matching, (e) L 2 distance after projecting to PCA subspace with 20 bases, (f) L 2 after projection to PCA subspace with 80 bases, (g) L 2 after image warp based on optic flow, (h) shuffle distance as described in text.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "from (4) with quadratic g(u) = u 2 ,\u03c1 |z \u2261 \u03c1(z, z) = r z (s) \u2212 rz(s) 2 . From (3), \u03c1 |z = y H \u22121 y + O(y)", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": ") \u03c1(a, b) \u2265 0, \u2200a, b, (2) \u03c1(a, b) = 0 iff a = b, (3) \u03c1(a, b) = \u03c1(b, a), and (4) \u03c1(a, b) + \u03c1(b, c) \u2265 \u03c1", "formula_coordinates": [2.0, 309.37, 494.32, 215.19, 33.87]}, {"formula_id": "formula_1", "formula_text": "p(X 1 , X 2 , . . . | z 1 , z 2 , . . . ; )", "formula_coordinates": [3.0, 358.41, 224.77, 118.26, 10.62]}, {"formula_id": "formula_2", "formula_text": "I z (r) = d i=1 I i (r)y i (2", "formula_coordinates": [4.0, 140.66, 243.54, 140.93, 28.75]}, {"formula_id": "formula_3", "formula_text": ")", "formula_coordinates": [4.0, 281.58, 253.15, 3.87, 8.92]}, {"formula_id": "formula_4", "formula_text": "r z (s) = d i=1 r i (s)y i , (3", "formula_coordinates": [4.0, 137.96, 521.14, 143.62, 28.75]}, {"formula_id": "formula_5", "formula_text": ")", "formula_coordinates": [4.0, 281.58, 530.74, 3.87, 8.92]}, {"formula_id": "formula_6", "formula_text": "\u03c1(z, z) = min s (s) ds g(|r z (s ) \u2212 rz(s)|),(4)", "formula_coordinates": [4.0, 98.81, 646.07, 186.64, 15.78]}, {"formula_id": "formula_7", "formula_text": "\u03b3 (z, r) = min s g(|r z (s ) \u2212 r|)", "formula_coordinates": [4.0, 358.9, 300.82, 116.12, 15.57]}, {"formula_id": "formula_8", "formula_text": "z = T \u03b1x .", "formula_coordinates": [4.0, 399.0, 452.99, 36.12, 10.71]}, {"formula_id": "formula_9", "formula_text": "T \u03b1 r = u + R(\u03b8 ) s r,", "formula_coordinates": [4.0, 376.54, 512.64, 81.08, 10.71]}, {"formula_id": "formula_10", "formula_text": "r z (s) = T \u03b1 r x (s)", "formula_coordinates": [4.0, 384.87, 584.26, 64.18, 10.71]}, {"formula_id": "formula_11", "formula_text": "p(y |z) \u221d 1 Z exp \u2212 \u03bb\u03c1(z, z) (6)", "formula_coordinates": [5.0, 121.92, 295.26, 163.54, 22.97]}, {"formula_id": "formula_12", "formula_text": "p(y | X ) \u2261 p(y | \u03b1, k) \u221d 1 Z exp \u2212 \u03bb\u03c1(T \u03b1xk , z). (7)", "formula_coordinates": [5.0, 75.39, 423.35, 210.06, 22.97]}, {"formula_id": "formula_13", "formula_text": "p(y | \u03b1) \u221d k \u03c0 k 1 Z exp \u2212 \u03bb\u03c1(T \u03b1xk , z),", "formula_coordinates": [5.0, 99.32, 490.65, 158.23, 26.11]}, {"formula_id": "formula_14", "formula_text": "z * 0 \u2190 arg min z\u2208Z * max z \u2208Z * \u2212{z} \u03c1(z, z ).", "formula_coordinates": [5.0, 361.56, 462.93, 123.44, 17.62]}, {"formula_id": "formula_15", "formula_text": "\u03b1 * t = arg min \u03b1 \u03c1 T \u22121 \u03b1 z * t , z * 0 and x * t = T \u22121 \u03b1 * t z * t ,", "formula_coordinates": [5.0, 326.23, 518.15, 193.92, 17.2]}, {"formula_id": "formula_16", "formula_text": "k t (x * t ) = arg min k \u03c1(x k , x * t ). (8", "formula_coordinates": [5.0, 368.4, 679.41, 152.28, 17.44]}, {"formula_id": "formula_17", "formula_text": ")", "formula_coordinates": [5.0, 520.69, 682.2, 3.87, 8.92]}, {"formula_id": "formula_18", "formula_text": "x k \u2190 arg min x\u2208C k max x \u2208C k \u2212{x} \u03c1(x, x ). (9", "formula_coordinates": [6.0, 121.56, 164.07, 160.02, 39.95]}, {"formula_id": "formula_19", "formula_text": ")", "formula_coordinates": [6.0, 281.58, 188.71, 3.87, 8.92]}, {"formula_id": "formula_20", "formula_text": "\u03c0 k \u221d N k .", "formula_coordinates": [6.0, 167.68, 240.05, 37.1, 10.62]}, {"formula_id": "formula_21", "formula_text": "min \u03b1\u2208A,x\u2208X \u03c1(T \u03b1x , z v ).", "formula_coordinates": [6.0, 138.77, 499.5, 78.18, 16.03]}, {"formula_id": "formula_22", "formula_text": "\u03c1 v (z v ) = \u03c1 T \u03b1 vx v , z v , z v \u2208 Z v as \u03c3 2 \u03c7 2 d distributed.", "formula_coordinates": [6.0, 70.26, 552.4, 174.56, 36.61]}, {"formula_id": "formula_23", "formula_text": "\u03c1 k = 1 N k z v \u2208C k \u03c1 v (z v ) and\u03c1 2 k = 1 N k z v \u2208C k \u03c1 2 v (z v ),", "formula_coordinates": [6.0, 75.64, 621.26, 204.44, 26.85]}, {"formula_id": "formula_24", "formula_text": "d k = 2\u03c1 2 k \u03c1 2 k \u2212\u03c1 2 k and \u03c3 k = \u03c1 k /d. (10", "formula_coordinates": [6.0, 94.71, 707.32, 186.59, 27.25]}, {"formula_id": "formula_25", "formula_text": ")", "formula_coordinates": [6.0, 281.31, 716.06, 4.15, 8.92]}, {"formula_id": "formula_26", "formula_text": "L(d) = \u2212log (d/2) + (d/2)(log(d/2) \u22121 \u2212 log(\u03c1 a /\u03c1 g )) (11", "formula_coordinates": [6.0, 337.59, 222.1, 182.82, 26.56]}, {"formula_id": "formula_27", "formula_text": ")", "formula_coordinates": [6.0, 520.41, 238.96, 4.15, 8.92]}, {"formula_id": "formula_28", "formula_text": "p t (X t ) = k t\u22121 \u03b1 t\u22121 p(y t | X t ) p(X t | X t\u22121 ) p t\u22121 (X t\u22121 ),", "formula_coordinates": [7.0, 311.2, 476.61, 212.67, 20.62]}, {"formula_id": "formula_29", "formula_text": "p(z |z) = ( \u221a 2\u03c0\u03c3 ) \u2212d |H| \u22121/2 exp \u2212 1 2\u03c3 2 (\u03c1 |z),", "formula_coordinates": [11.0, 85.03, 241.91, 186.8, 25.73]}, {"formula_id": "formula_30", "formula_text": "B = \u03c3 H \u22121/2 = \u03c3 U D \u22121/2 U .", "formula_coordinates": [11.0, 100.78, 320.03, 121.27, 11.84]}], "doi": ""}