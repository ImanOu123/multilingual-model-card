{"title": "The Manifold Tangent Classifier", "authors": "Salah Rifai; Yann N Dauphin; Pascal Vincent; Yoshua Bengio; Xavier Muller", "pub_date": "", "abstract": "We combine three important ideas present in previous work for building classifiers: the semi-supervised hypothesis (the input distribution contains information about the classifier), the unsupervised manifold hypothesis (data density concentrates near low-dimensional manifolds), and the manifold hypothesis for classification (different classes correspond to disjoint manifolds separated by low density). We exploit a novel algorithm for capturing manifold structure (high-order contractive auto-encoders) and we show how it builds a topological atlas of charts, each chart being characterized by the principal singular vectors of the Jacobian of a representation mapping. This representation learning algorithm can be stacked to yield a deep architecture, and we combine it with a domain knowledge-free version of the TangentProp algorithm to encourage the classifier to be insensitive to local directions changes along the manifold. Record-breaking classification results are obtained.", "sections": [{"heading": "Introduction", "text": "Much of machine learning research can be viewed as an exploration of ways to compensate for scarce prior knowledge about how to solve a specific task by extracting (usually implicit) knowledge from vast amounts of data. This is especially true of the search for generic learning algorithms that are to perform well on a wide range of domains for which they were not specifically tailored. While such an outlook precludes using much domain-specific knowledge in designing the algorithms, it can however be beneficial to leverage what might be called \"generic\" prior hypotheses, that appear likely to hold for a wide range of problems. The approach studied in the present work exploits three such prior hypotheses:\n1. The semi-supervised learning hypothesis, according to which learning aspects of the input distribution p(x) can improve models of the conditional distribution of the supervised target p(y|x), i.e., p(x) and p(y|x) share something (Lasserre et al., 2006). This hypothesis underlies not only the strict semi-supervised setting where one has many more unlabeled examples at his disposal than labeled ones, but also the successful unsupervised pretraining approach for learning deep architectures, which has been shown to significantly improve supervised performance even without using additional unlabeled examples (Hinton et al., 2006;Bengio, 2009;Erhan et al., 2010).\nThe recently proposed Contractive Auto-Encoder (CAE) algorithm (Rifai et al., 2011a), based on the idea of encouraging the learned representation to be robust to small variations of the input, was shown to be very effective for unsupervised feature learning. Its successful application in the pre-training of deep neural networks is yet another illustration of what can be gained by adopting hypothesis 1. In addition, Rifai et al. (2011a) propose, and show empirical evidence for, the hypothesis that the trade-off between reconstruction error and the pressure to be insensitive to variations in input space has an interesting consequence: It yields a mostly contractive mapping that, locally around each training point, remains substantially sensitive only to a few input directions (with different directions of sensitivity for different training points). This is taken as evidence that the algorithm indirectly exploits hypothesis 2 and models a lower-dimensional manifold. Most of the directions to which the representation is substantially sensitive are thought to be directions tangent to the datasupporting manifold (those that locally define its tangent space).\nThe present work follows through on this interpretation, and investigates whether it is possible to use this information, that is presumably captured about manifold structure, to further improve classification performance by leveraging hypothesis 3. To that end, we extract a set of basis vectors for the local tangent space at each training point from the Contractive Auto-Encoder's learned parameters. This is obtained with a Singular Value Decomposition (SVD) of the Jacobian of the encoder that maps each input to its learned representation. Based on hypothesis 3, we then adopt the \"generic prior\" that class labels are likely to be insensitive to most directions within these local tangent spaces (ex: small translations, rotations or scalings usually do not change an image's class). Supervised classification algorithms that have been devised to efficiently exploit tangent directions given as domain-specific prior-knowledge (Simard et al., 1992(Simard et al., , 1993, can readily be used instead with our learned tangent spaces. In particular, we will show record-breaking improvements by using TangentProp for fine tuning CAE-pre-trained deep neural networks. To the best of our knowledge this is the first time that the implicit relationship between an unsupervised learned mapping and the tangent space of a manifold is rendered explicit and successfully exploited for the training of a classifier. This showcases a unified approach that simultaneously leverages all three \"generic\" prior hypotheses considered. Our experiments (see Section 6) show that this approach sets new records for domain-knowledge-free performance on several real-world classification problems. Remarkably, in some cases it even outperformed methods that use weak or strong domain-specific prior knowledge (e.g. convolutional networks and tangent distance based on a-priori known transformations). Naturally, this approach is even more likely to be beneficial for datasets where no prior knowledge is readily available.", "publication_ref": ["b11", "b9", "b0", "b7", "b16", "b16", "b21", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Contractive auto-encoders (CAE)", "text": "We consider the problem of the unsupervised learning of a non-linear feature extractor from a dataset D = {x 1 , . . . , x n }. Examples x i \u2208 IR d are i.i.d. samples from an unknown distribution p(x).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Traditional auto-encoders", "text": "The auto-encoder framework is one of the oldest and simplest techniques for the unsupervised learning of non-linear feature extractors. It learns an encoder function h, that maps an input x \u2208 IR d to a hidden representation h(x) \u2208 IR d h , jointly with a decoder function g, that maps h back to the input space as r = g(h(x)) the reconstruction of x. The encoder and decoder's parameters \u03b8 are learned by stochastic gradient descent to minimize the average reconstruction error L(x, g(h(x))) for the examples of the training set. The objective being minimized is:\nJ AE (\u03b8) = x\u2208D L(x, g(h(x))).(1)\nWe will will use the most common forms of encoder, decoder, and reconstruction error: Loss function: Either the squared error: L(x, r) = x \u2212 r 2 or Bernoulli cross-entropy:\nEncoder: h(x) = s(W x + b h ),\nL(x, r) = \u2212 d i=1 x i log(r i ) + (1 \u2212 x i ) log(1 \u2212 r i ).\nThe set of parameters of such an auto-encoder is \u03b8 = {W, b h , b r }.\nHistorically, auto-encoders were primarily viewed as a technique for dimensionality reduction, where a narrow bottleneck (i.e. d h < d) was in effect acting as a capacity control mechanism. By contrast, recent successes (Bengio et al., 2007;Ranzato et al., 2007a;Kavukcuoglu et al., 2009;Vincent et al., 2010;Rifai et al., 2011a) tend to rely on rich, oftentimes over-complete representations (d h > d), so that more sophisticated forms of regularization are required to pressure the auto-encoder to extract relevant features and avoid trivial solutions. Several successful techniques aim at sparse representations (Ranzato et al., 2007a;Kavukcuoglu et al., 2009;Goodfellow et al., 2009). Alternatively, denoising auto-encoders (Vincent et al., 2010) change the objective from mere reconstruction to that of denoising.", "publication_ref": ["b3", "b14", "b10", "b26", "b16", "b14", "b10", "b8", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "First order and higher order contractive auto-encoders", "text": "More recently, Rifai et al. (2011a) introduced the Contractive Auto-Encoder (CAE), that encourages robustness of representation h(x) to small variations of a training input x, by penalizing its sensitivity to that input, measured as the Frobenius norm of the encoder's Jacobian J(x) = \u2202h \u2202x (x). The regularized objective minimized by the CAE is the following:\nJ CAE (\u03b8) = x\u2208D L(x, g(h(x))) + \u03bb J(x) 2 , (2\n)\nwhere \u03bb is a non-negative regularization hyper-parameter that controls how strongly the norm of the Jacobian is penalized. Note that, with the traditional sigmoid encoder form given above, one can easily obtain the Jacobian of the encoder. Its j th row is obtained form the j th row of W as:\nJ(x) j = \u2202h j (x) \u2202x = h j (x)(1 \u2212 h j (x))W j .(3)\nComputing the extra penalty term (and its contribution to the gradient) is similar to computing the reconstruction error term (and its contribution to the gradient), thus relatively cheap.\nIt is also possible to penalize higher order derivatives (Hessian) by using a simple stochastic technique that eschews computing them explicitly, which would be prohibitive. It suffices to penalize differences between the Jacobian at x and the Jacobian at nearby pointsx = x + (stochastic corruptions of x). This yields the CAE+H (Rifai et al., 2011b) variant with the following optimization objective:\nJ CAE+H (\u03b8) = x\u2208D L(x, g(h(x))) + \u03bb ||J(x)|| 2 + \u03b3E \u223cN (0,\u03c3 2 I) ||J(x) \u2212 J(x + )|| 2 , (4\n)\nwhere \u03b3 is an additional regularization hyper-parameters that controls how strongly we penalize local variations of the Jacobian, i.e. higher order derivatives. The expectation E is over Gaussian noise variable . In practice stochastic samples thereof are used for each stochastic gradient update.\nThe CAE+H is the variant used for our experiments.\n3 Characterizing the tangent bundle captured by a CAE Rifai et al. (2011a) reason that, while the regularization term encourages insensitivity of h(x) in all input space directions, this pressure is counterbalanced by the need for accurate reconstruction, thus resulting in h(x) being substantially sensitive only to the few input directions required to distinguish close by training points. The geometric interpretation is that these directions span the local tangent space of the underlying manifold that supports the data. The tangent bundle of a smooth manifold is the manifold along with the set of tangent planes taken at all points on it. Each such tangent plane can be equipped with a local Euclidean coordinate system or chart. In topology, an atlas is a collection of such charts (like the locally Euclidean map in each page of a geographic atlas). Even though the set of charts may form a non-Euclidean manifold (e.g., a sphere), each chart is Euclidean.", "publication_ref": ["b16", "b17", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Conditions for the feature mapping to define an atlas on a manifold", "text": "In order to obtain a proper atlas of charts, h must be a diffeomorphism. and x j by only looking at h(x i ) and h(x j )). Since h(x) = s(W x + b h ) and s is invertible, using the definition of injectivity we get (by composing h(\nx i ) = h(x j ) with s \u22121 ) \u2200i, j h(x i ) = h(x j ) \u21d0\u21d2 W \u2206 ij = 0 where \u2206 ij = x i \u2212 x j .\nIn order to preserve the injectivity of h, W has to form a basis spanned by its rows W k , where\n\u2200 i, j \u2203 \u03b1 \u2208 IR d h , \u2206 ij = d h k \u03b1 k W k .\nWith this condition satisfied, mapping h is injective in the subspace spanned by the variations in the training set. If we limit the domain of h to h(X ) \u2282 (0, 1)\nd h comprising values obtainable by h applied to some set X , then we obtain surjectivity by definition, hence bijectivity of h between the training set D and h(D). ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Obtaining an atlas from the learned feature mapping", "text": "Now that we have necessary conditions for local invertibility of h(x) for x \u2208 D, let us consider how to define the local chart around x from the nature of h. Because h must be sensitive to changes from an example x i to one of its neighbors x j , but insensitive to other changes (because of the CAE penalty), we expect that this will be reflected in the spectrum of the Jacobian matrix J(x) = \u2202h(x) \u2202x at each training point x. In the ideal case where J(x) has rank k, h(x + v) differs from h(x) only if v is in the span of the singular vectors of J(x) with non-zero singular value. In practice, J(x) has many tiny singular values. Hence, we define a local chart around x using the Singular Value Decomposition of J T (x) = U (x)S(x)V T (x) (where U (x) and V (x) are orthogonal and S(x) is diagonal). The tangent plane H x at x is given by the span of the set of principal singular vectors B x :\nB x = {U \u2022k (x)|S kk (x) > } and H x = {x + v|v \u2208 span(B x )},\nwhere U \u2022k (x) is the k-th column of U (x), and span({z k }) = {x|x = k w k z k , w k \u2208 IR}. We can thus define an atlas A captured by h, based on the local linear approximation around each example:\nA = {(M x , \u03c6 x )|x \u2208 D, \u03c6 x (x) = B x (x \u2212 x)}.(5)\nNote that this way of obtaining an atlas can also be applied to subsequent layers of a deep network.\nIt is thus possible to use a greedy layer-wise strategy to initialize a network with CAEs (Rifai et al., 2011a) and obtain an atlas that corresponds to the nonlinear features computed at any layer.", "publication_ref": ["b16"], "figure_ref": [], "table_ref": []}, {"heading": "Exploiting the learned tangent directions for classification", "text": "Using the previously defined charts for every point of the training set, we propose to use this additional information provided by unsupervised learning to improve the performance of the supervised task. In this we adopt the manifold hypothesis for classification mentioned in the introduction.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "CAE-based tangent distance", "text": "One way of achieving this is to use a nearest neighbor classifier with a similarity criterion defined as the shortest distance between two hyperplanes (Simard et al., 1993). The tangents extracted on each points will allow us to shrink the distances between two samples when they can approximate each other by a linear combination of their local tangents. Following Simard et al. (1993), we define the tangent distance between two points x and y as the distance between the two hyperplanes H x , H y \u2282 IR d spanned respectively by B x and B y . Using the usual definition of distance between two spaces, d(H x , H y ) = inf{ z\u2212w 2 |/ (z, w) \u2208 H x \u00d7H y }, we obtain the solution for this convex problem by solving a system of linear equations (Simard et al., 1993). This procedure corresponds to allowing the considered points x and y to move along the directions spanned by their associated local charts. Their distance is then evaluated on the new coordinates where the distance is minimal. We can then use a nearest neighbor classifier based on this distance.", "publication_ref": ["b22", "b22", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "CAE-based tangent propagation", "text": "Nearest neighbor techniques are often impractical for large scale datasets because their computational requirements scale linearly with n for each test case. By contrast, once trained, neural networks yield fast responses for test cases. We can also leverage the extracted local charts when training a neural network. Following the tangent propagation approach of Simard et al. (1992), but exploiting our learned tangents, we encourage the output o of a neural network classifier to be insensitive to variations in the directions of the local chart of x by adding the following penalty to its supervised objective function:\n\u2126(x) = u\u2208Bx \u2202o \u2202x (x) u 2 (6)\nContribution of this term to the gradients of network parameters can be computed in O(N w ), where N w is the number of neural network weights.", "publication_ref": ["b21"], "figure_ref": [], "table_ref": []}, {"heading": "The Manifold Tangent Classifier (MTC)", "text": "Putting it all together, here is the high level summary of how we build and train a deep network:\n1. Train (unsupervised) a stack of K CAE+H layers (Eq. 4). Each is trained in turn on the representation learned by the previous layer.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "2.", "text": "For each x i \u2208 D compute the Jacobian of the last layer representation J (K) (x i ) = \u2202h (K) \u2202x (x i ) and its SVD 1 . Store the leading d M singular vectors in set B xi . 3. On top of the K pre-trained layers, stack an output layer of size the number of classes. Finetune the whole network for supervised classification 2 with an added tangent propagation penalty (Eq. 6), using for each x i , tangent directions B xi .\nWe call this deep learning algorithm the Manifold Tangent Classifier (MTC). Alternatively, instead of step 3, one can use the tangent vectors in B xi in a tangent distance nearest neighbors classifier.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related prior work", "text": "Many Non-Linear Manifold Learning algorithms (Roweis and Saul, 2000;Tenenbaum et al., 2000) have been proposed which can automatically discover the main directions of variation around each training point, i.e., the tangent bundle. Most of these algorithms are non-parametric and local, i.e., explicitly parametrizing the tangent plane around each training point (with a separate set of parameters for each, or derived mostly from the set of training examples in every neighborhood), as most explicitly seen in Manifold Parzen Windows (Vincent and Bengio, 2003) and manifold Charting (Brand, 2003). See Bengio and Monperrus (2005) for a critique of local non-parametric manifold algorithms: they might require a number of training examples which grows exponentially with manifold dimension and curvature (more crooks and valleys in the manifold will require more examples). One attempt to generalize the manifold shape non-locally (Bengio et al., 2006) is based on explicitly predicting the tangent plane associated to any given point x, as a parametrized function of x. Note that these algorithms all explicitly exploit training set neighborhoods (see Figure 2), i.e. they use pairs or tuples of points, with the goal to explicitly model the tangent space, while it is 1 J (K) is the product of the Jacobians of each encoder (see Eq. 3) in the stack. It suffices to compute its leading dM SVD vectors and singular values. This is achieved in O(dM \u00d7 d \u00d7 d h ) per training example. For comparison, the cost of a forward propagation through a single MLP layer is O(d \u00d7 d h ) per example.\n2 A sigmoid output layer is preferred because computing its Jacobian is straightforward and efficient (Eq. 3). The supervised cost used is the cross entropy. Training is by stochastic gradient descent. modeled implicitly by the CAE's objective function (that is not based on pairs of points). More recently, the Local Coordinate Coding (LCC) algorithm (Yu et al., 2009) and its Local Tangent LCC variant (Yu and Zhang, 2010) were proposed to build a a local chart around each training example (with a local low-dimensional coordinate system around it) and use it to define a representation for each input x: the responsibility of each local chart/anchor in explaining input x and the coordinate of x in each local chart. That representation is then fed to a classifier and yield better generalization than x itself. The tangent distance (Simard et al., 1993) and TangentProp (Simard et al., 1992) algorithms were initially designed to exploit prior domain-knowledge of directions of invariance (ex: knowledge that the class of an image should be invariant to small translations rotations or scalings in the image plane). However any algorithm able to output a chart for a training point might potentially be used, as we do here, to provide directions to a Tangent distance or TangentProp (Simard et al., 1992) based classifier. Our approach is nevertheless unique as the CAE's unsupervised feature learning capabilities are used simultaneously to provide a good initialization of deep network layers and a coherent non-local predictor of tangent spaces. TangentProp is itself closely related to the Double Backpropagation algorithm (Drucker and LeCun, 1992), in which one instead adds a penalty that is the sum of squared derivatives of the prediction error (with respect to the network input). Whereas TangentProp attempts to make the output insensitive to selected directions of change, the double backpropagation penalty term attempts to make the error at a training example invariant to changes in all directions. Since one is also trying to minimize the error at the training example, this amounts to making that minimization more robust, i.e., extend it to the neighborhood of the training examples.\nAlso related is the Semi-Supervised Embedding algorithm (Weston et al., 2008). In addition to minimizing a supervised prediction error, it encourages each layer of representation of a deep architecture to be invariant when the training example is changed from x to a near neighbor of x in the training set. This algorithm works implicitly under the hypothesis that the variable y to predict from x is invariant to the local directions of change present between nearest neighbors. This is consistent with the manifold hypothesis for classification (hypothesis 3 mentioned in the introduction). Instead of removing variability along the local directions of variation, the Contractive Auto-Encoder (Rifai et al., 2011a) initially finds a representation which is most sensitive to them, as we explained in section 2.", "publication_ref": ["b18", "b23", "b25", "b4", "b1", "b2", "b29", "b28", "b22", "b21", "b21", "b6", "b27", "b16"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Experiments", "text": "We conducted experiments to evaluate our approach and the quality of the manifold tangents learned by the CAE, using a range of datasets from different domains:\nMNIST is a dataset of 28 \u00d7 28 images of handwritten digits. The learning task is to predict the digit contained in the images. Reuters Corpus Volume I is a popular benchmark for document classification. It consists of 800,000 real-world news wire stories made available by Reuters. We used the 2000 most frequent words calculated on the whole dataset to create a bag-of-words vector representation. We used the LYRL2004 split to separate between a train and test set. CIFAR-10 is a dataset of 70,000 32 \u00d7 32 RGB real-world images. It contains images of real-world objects (i.e. cars, animals) with all the variations present in natural images (i.e. backgrounds). Forest Cover Type is a large-scale database of cartographic variables for the prediction of forest cover types made available by the US Forest Service.\nWe investigate whether leveraging the CAE learned tangents leads to better classification performance on these problems, using the following methodology: Optimal hyper-parameters for (a stack of) CAEs are selected by cross-validation on a disjoint validation set extracted from the training set. The quality of the feature extractor and tangents captured by the CAEs is evaluated by initializing an neural network (MLP) with the same parameters and fine-tuning it by backpropagation on the supervised classification task. The optimal strength of the supervised TangentProp penalty and number of tangents d M is also cross-validated.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Figure 1 shows a visualization of the tangents learned by the CAE. On MNIST, the tangents mostly correspond to small geometrical transformations like translations and rotations. On CIFAR-10, the  model also learns sensible tangents, which seem to correspond to changes in the parts of objects. The tangents on RCV1-v2 correspond to the addition or removal of similar words and removal of irrelevant words. We also note that extracting the tangents of the model is a way to visualize what the model has learned about the structure of the manifold. Interestingly, we see that hypothesis 3 holds for these datasets because most tangents do not change the class of the example. We use KNN using tangent distance to evaluate the quality of the learned tangents more objectively. Table 1 shows that using the tangents extracted from a CAE always lead to better performance than a traditional KNN.\nAs described in section 4.2, the tangents extracted by the CAE can be used for fine-tuning the multilayer perceptron using tangent propagation, yielding our Manifold Tangent Classifier (MTC). As it is a semi-supervised approach, we evaluate its effectiveness with a varying amount of labeled examples on MNIST. Following Weston et al. (2008), the unsupervised feature extractor is trained on the full training set and the supervised classifier is trained on a restricted labeled set.   (Weston et al., 2008;Ranzato et al., 2007b;Salakhutdinov and Hinton, 2007  Table 3 shows our results on the full MNIST dataset with some results taken from (LeCun et al., 1999;Hinton et al., 2006). The CAE in this figure is a two-layer deep network with 2000 units per layer pretrained with the CAE+H objective. The MTC uses the same stack of CAEs trained with tangent propagation using 15 tangents. The prior state of the art for the permutation invariant version of the task was set by the Deep Boltzmann Machines (Salakhutdinov and Hinton, 2009) at 0.95%. Using our approach, we reach 0.81% error on the test set. Remarkably, the MTC also outperforms the basic Convolutional Neural Network (CNN) even though the CNN exploits prior knowledge about vision using convolution and pooling to enhance the results. ", "publication_ref": ["b27", "b27", "b15", "b19", "b12", "b9", "b20"], "figure_ref": ["fig_2"], "table_ref": ["tab_1", "tab_5"]}, {"heading": "Conclusion", "text": "In this work, we have shown a new way to characterize a manifold by extracting a local chart at each data point based on the unsupervised feature mapping built with a deep learning approach. The developed Manifold Tangent Classifier successfully leverages three common \"generic prior hypotheses\" in a unified manner. It learns a meaningful representation that captures the structure of the manifold, and can leverage this knowledge to reach superior classification performance. On datasets from different domains, it successfully achieves state of the art performance.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgments The authors would like to acknowledge the support of the following agencies for research funding and computing support: NSERC, FQRNT, Calcul Qu\u00e9bec and CIFAR.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Learning deep architectures for AI. Foundations and Trends in Machine Learning", "journal": "Now Publishers", "year": "2009", "authors": "Y Bengio"}, {"ref_id": "b1", "title": "Non-local manifold tangent learning", "journal": "MIT Press", "year": "2005", "authors": "Y Bengio; M Monperrus"}, {"ref_id": "b2", "title": "Non-local manifold parzen windows", "journal": "MIT Press", "year": "2006", "authors": "Y Bengio; H Larochelle; P Vincent"}, {"ref_id": "b3", "title": "Greedy layer-wise training of deep networks", "journal": "", "year": "2007", "authors": "Y Bengio; P Lamblin; D Popovici; H Larochelle"}, {"ref_id": "b4", "title": "Charting a manifold", "journal": "MIT Press", "year": "2003", "authors": "M Brand"}, {"ref_id": "b5", "title": "Algorithms for manifold learning", "journal": "", "year": "2005", "authors": "L Cayton"}, {"ref_id": "b6", "title": "Improving generalisation performance using double back-propagation", "journal": "IEEE Transactions on Neural Networks", "year": "1992", "authors": "H Drucker; Y Lecun"}, {"ref_id": "b7", "title": "Why does unsupervised pre-training help deep learning?", "journal": "JMLR", "year": "2010", "authors": "D Erhan; Y Bengio; A Courville; P.-A Manzagol; P Vincent; S Bengio"}, {"ref_id": "b8", "title": "Measuring invariances in deep networks", "journal": "", "year": "2009", "authors": "I Goodfellow; Q Le; A Saxe; A Ng"}, {"ref_id": "b9", "title": "A fast learning algorithm for deep belief nets", "journal": "Neural Computation", "year": "2006", "authors": "G E Hinton; S Osindero; Y Teh"}, {"ref_id": "b10", "title": "Learning invariant features through topographic filter maps", "journal": "IEEE", "year": "2009", "authors": "K Kavukcuoglu; M Ranzato; R Fergus; Y Lecun"}, {"ref_id": "b11", "title": "Principled hybrids of generative and discriminative models", "journal": "IEEE Computer Society", "year": "2006", "authors": "J A Lasserre; C M Bishop; T P Minka"}, {"ref_id": "b12", "title": "Object recognition with gradient-based learning", "journal": "Springer", "year": "1999", "authors": "Y Lecun; P Haffner; L Bottou; Y Bengio"}, {"ref_id": "b13", "title": "Sample complexity of testing the manifold hypothesis", "journal": "", "year": "2010", "authors": "H Narayanan; S Mitter"}, {"ref_id": "b14", "title": "Efficient learning of sparse representations with an energy-based model", "journal": "", "year": "2007", "authors": "M Ranzato; C Poultney; S Chopra; Y Lecun"}, {"ref_id": "b15", "title": "Unsupervised learning of invariant feature hierarchies with applications to object recognition", "journal": "IEEE Press", "year": "2007", "authors": "M Ranzato; F Huang; Y Boureau; Y Lecun"}, {"ref_id": "b16", "title": "Contracting auto-encoders: Explicit invariance during feature extraction", "journal": "", "year": "2011", "authors": "S Rifai; P Vincent; X Muller; X Glorot; Y Bengio"}, {"ref_id": "b17", "title": "Higher order contractive auto-encoder", "journal": "", "year": "2011", "authors": "S Rifai; G Mesnil; P Vincent; X Muller; Y Bengio; Y Dauphin; X Glorot"}, {"ref_id": "b18", "title": "Nonlinear dimensionality reduction by locally linear embedding", "journal": "Science", "year": "2000", "authors": "S Roweis; L K Saul"}, {"ref_id": "b19", "title": "Learning a nonlinear embedding by preserving class neighbourhood structure", "journal": "Omnipress", "year": "2007", "authors": "R Salakhutdinov; G E Hinton"}, {"ref_id": "b20", "title": "Deep Boltzmann machines", "journal": "", "year": "2009", "authors": "R Salakhutdinov; G E Hinton"}, {"ref_id": "b21", "title": "Tangent prop -A formalism for specifying selected invariances in an adaptive network", "journal": "Morgan Kaufmann", "year": "1992", "authors": "P Simard; B Victorri; Y Lecun; J Denker"}, {"ref_id": "b22", "title": "Efficient pattern recognition using a new transformation distance", "journal": "Morgan Kaufmann", "year": "1993", "authors": "P Y Simard; Y Lecun; J Denker"}, {"ref_id": "b23", "title": "A global geometric framework for nonlinear dimensionality reduction", "journal": "Science", "year": "2000", "authors": "J Tenenbaum; V De Silva; J C Langford"}, {"ref_id": "b24", "title": "Application of distributed svm architectures in classifying forest data cover types", "journal": "Computers and Electronics in Agriculture", "year": "2008", "authors": "M Trebar; N Steele"}, {"ref_id": "b25", "title": "Manifold parzen windows", "journal": "MIT Press", "year": "2003", "authors": "P Vincent; Y Bengio"}, {"ref_id": "b26", "title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "journal": "JMLR", "year": "2010", "authors": "P Vincent; H Larochelle; I Lajoie; Y Bengio; P.-A Manzagol"}, {"ref_id": "b27", "title": "Deep learning via semi-supervised embedding", "journal": "", "year": "2008", "authors": "J Weston; F Ratle; R Collobert"}, {"ref_id": "b28", "title": "Improved local coordinate coding using local tangents", "journal": "", "year": "2010", "authors": "K Yu; T Zhang"}, {"ref_id": "b29", "title": "Nonlinear learning using local coordinate coding", "journal": "", "year": "2009", "authors": "K Yu; T Zhang; Y Gong"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "where s is the element-wise logistic sigmoid s(z) = 1 1+e \u2212z . Parameters are a d h \u00d7 d weight matrix W and bias vector b h \u2208 IR d h . Decoder: r = g(h(x)) = s 2 (W T h(x) + b r ). Parameters are W T (tied weights, shared with the encoder) and bias vector b r \u2208 IR d . Activation function s 2 is either a logistic sigmoid (s 2 = s) or the identity (linear decoder).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Let M x be an open ball on the manifold M around training example x. By smoothness of the manifold M and of mapping h, we obtain bijectivity locally around the training examples (on the manifold) as well, i.e., between \u222a x\u2208D M x and h(\u222a x\u2208D M x ).", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 1 :1Figure1: Visualisation of the tangents learned by the CAE for MNIST, CIFAR-10 and RCV1 (top to bottom). The left-most column is the example and the following columns are its tangents. On RCV1, we show the tangents of a document with the topic \"Trading & Markets\" (MCAT) with the negative terms in red(-) and the positive terms in green(+).", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 2 :2Figure 2: Tangents extracted by local PCA on CIFAR-10. This shows the limitation of approaches that rely on training set neighborhoods.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "It must be smooth (C \u221e ) and invertible on open Euclidean balls on the manifold M around the training points. Smoothness is guaranteed because of our choice of parametrization (affine + sigmoid). Injectivity (different values of h(x) correspond to different values of x) on the training examples is encouraged by minimizing reconstruction error (otherwise we cannot distinguish training examples x i", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Classification accuracy on several datasets using KNN variants measured on 10,000 test examples with 1,000 training examples. The KNN is trained on the raw input vector using the Euclidean distance while the K-layer+KNN is computed on the representation learned by a K-layer CAE. The KNN+Tangents uses at every sample the local charts extracted from the 1-layer CAE to compute tangent distance.", "figure_data": "KNN KNN+Tangents 1-Layer CAE+KNN 2-Layer CAE+KNNMNIST86.988.790.5591.15CIFAR-1025.426.525.1-COVERTYPE 70.270.9869.5467.45"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "shows our results for a single hidden layer MLP initialized with CAE+H pretraining (noted CAE for brevity) and for the same classifier fine-tuned with tangent propagation (i.e. the manifold tangent classifier of section 4.3, noted MTC). The methods that do not leverage the semi-supervised learning hypothesis (Support Vector Machines, traditional Neural Networks and Convolutional Neural Networks) give very poor performance when the amount of labeled data is low. In some cases, the methods that can learn from unlabeled data can reduce the classification error by half. The CAE gives better results than other approaches across almost the whole range considered. It shows that the features extracted", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Semi-supervised classification error on the MNIST test set with 100, 600, 1000 and 3000 labeled training examples. We compare our method with results from", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "from the rich unlabeled data distribution give a good inductive prior for the classification task. Note that the MTC consistently outperforms the CAE on this benchmark.", "figure_data": ").NNSVM CNN TSVM DBN-rNCA EmbedNN CAE MTC10025.81 23.44 22.98 16.81-16.8613.47 12.0360011.44 8.857.686.168.75.976.35.131000 10.77.776.455.38-5.734.773.643000 6.044.213.353.453.33.593.222.57"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Classification error on the MNIST test set with the full training set.", "figure_data": "K-NNNNSVMDBNCAEDBMCNNMTC3.09% 1.60% 1.40% 1.17% 1.04% 0.95% 0.95% 0.81%"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Classification error on the Forest CoverType dataset.We also trained a 4 layer MTC on the Forest CoverType dataset. FollowingTrebar and Steele (2008), we use the data split DS2-581 which contains over 500,000 training examples. The MTC yields the best performance for the classification task beating the previous state of the art held by the distributed SVM (mixture of several non-linear SVMs).", "figure_data": "SVM Distributed SVM MTC4.11%3.46%3.13%"}], "formulas": [{"formula_id": "formula_0", "formula_text": "J AE (\u03b8) = x\u2208D L(x, g(h(x))).(1)", "formula_coordinates": [2.0, 245.05, 624.19, 258.95, 20.06]}, {"formula_id": "formula_1", "formula_text": "Encoder: h(x) = s(W x + b h ),", "formula_coordinates": [2.0, 108.0, 670.57, 137.28, 9.72]}, {"formula_id": "formula_2", "formula_text": "L(x, r) = \u2212 d i=1 x i log(r i ) + (1 \u2212 x i ) log(1 \u2212 r i ).", "formula_coordinates": [3.0, 143.87, 85.02, 360.14, 24.48]}, {"formula_id": "formula_3", "formula_text": "J CAE (\u03b8) = x\u2208D L(x, g(h(x))) + \u03bb J(x) 2 , (2", "formula_coordinates": [3.0, 216.04, 322.67, 284.09, 22.13]}, {"formula_id": "formula_4", "formula_text": ")", "formula_coordinates": [3.0, 500.13, 325.06, 3.87, 8.64]}, {"formula_id": "formula_5", "formula_text": "J(x) j = \u2202h j (x) \u2202x = h j (x)(1 \u2212 h j (x))W j .(3)", "formula_coordinates": [3.0, 220.83, 394.79, 283.17, 22.31]}, {"formula_id": "formula_6", "formula_text": "J CAE+H (\u03b8) = x\u2208D L(x, g(h(x))) + \u03bb ||J(x)|| 2 + \u03b3E \u223cN (0,\u03c3 2 I) ||J(x) \u2212 J(x + )|| 2 , (4", "formula_coordinates": [3.0, 120.82, 515.69, 379.31, 23.03]}, {"formula_id": "formula_7", "formula_text": ")", "formula_coordinates": [3.0, 500.13, 518.98, 3.87, 8.64]}, {"formula_id": "formula_8", "formula_text": "x i ) = h(x j ) with s \u22121 ) \u2200i, j h(x i ) = h(x j ) \u21d0\u21d2 W \u2206 ij = 0 where \u2206 ij = x i \u2212 x j .", "formula_coordinates": [4.0, 108.0, 170.72, 272.37, 46.62]}, {"formula_id": "formula_9", "formula_text": "\u2200 i, j \u2203 \u03b1 \u2208 IR d h , \u2206 ij = d h k \u03b1 k W k .", "formula_coordinates": [4.0, 191.12, 218.42, 154.54, 14.11]}, {"formula_id": "formula_10", "formula_text": "B x = {U \u2022k (x)|S kk (x) > } and H x = {x + v|v \u2208 span(B x )},", "formula_coordinates": [4.0, 173.08, 442.99, 265.84, 9.65]}, {"formula_id": "formula_11", "formula_text": "A = {(M x , \u03c6 x )|x \u2208 D, \u03c6 x (x) = B x (x \u2212 x)}.(5)", "formula_coordinates": [4.0, 212.17, 489.35, 291.83, 9.65]}, {"formula_id": "formula_12", "formula_text": "\u2126(x) = u\u2208Bx \u2202o \u2202x (x) u 2 (6)", "formula_coordinates": [5.0, 251.66, 245.94, 252.34, 30.0]}], "doi": ""}