{"title": "Learning to Teach in Cooperative Multiagent Reinforcement Learning", "authors": "Shayegan Omidshafiei; Dong-Ki Kim; Miao Liu; Gerald Tesauro; Matthew Riemer; Christopher Amato; Murray Campbell", "pub_date": "2018-08-31", "abstract": "Collective human knowledge has clearly benefited from the fact that innovations by individuals are taught to others through communication. Similar to human social groups, agents in distributed learning systems would likely benefit from communication to share knowledge and teach skills. The problem of teaching to improve agent learning has been investigated by prior works, but these approaches make assumptions that prevent application of teaching to general multiagent problems, or require domain expertise for problems they can apply to. This learning to teach problem has inherent complexities related to measuring long-term impacts of teaching that compound the standard multiagent coordination challenges. In contrast to existing works, this paper presents the first general framework and algorithm for intelligent agents to learn to teach in a multiagent environment. Our algorithm, Learning to Coordinate and Teach Reinforcement (LeCTR), addresses peer-to-peer teaching in cooperative multiagent reinforcement learning. Each agent in our approach learns both when and what to advise, then uses the received advice to improve local learning. Importantly, these roles are not fixed; these agents learn to assume the role of student and/or teacher at the appropriate moments, requesting and providing advice in order to improve teamwide performance and learning. Empirical comparisons against state-of-the-art teaching methods show that our teaching agents not only learn significantly faster, but also learn to coordinate in tasks where existing methods fail.", "sections": [{"heading": "Introduction", "text": "In social settings, innovations by individuals are taught to others in the population through communication channels (Rogers 2010), which not only improves final performance, but also the effectiveness of the entire learning process (i.e., rate of learning). There exist analogous settings where learning agents interact and adapt behaviors while interacting in a shared environment (e.g., autonomous cars and assistive robots). While any given agent may not be an expert during learning, it may have local knowledge that teammates may be unaware of. Similar to human social groups, these learning agents would likely benefit from communication to share knowledge and teach skills, thereby improving the effectiveness of system-wide learning. It is also desirable for agents in such systems to learn to teach one another, rather than rely on Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nhand-crafted teaching heuristics created by domain experts. The benefit of learned peer-to-peer teaching is that it can accelerate learning even without relying on the existence of \"all-knowing\" teachers. Despite these potential advantages, no algorithms exist for learning to teach in multiagent systems.\nThis paper targets the learning to teach problem in the context of cooperative Multiagent Reinforcement Learning (MARL). Cooperative MARL is a standard framework for settings where agents learn to coordinate in a shared environment. Recent works in cooperative MARL have shown final task performance can be improved by introducing inter-agent communication mechanisms (Sukhbaatar, Fergus, and others 2016;Foerster et al. 2016;Lowe et al. 2017). Agents in these works, however, merely communicate to coordinate in the given task, not to improve overall learning by teaching one another. By contrast, this paper targets a new multiagent paradigm in which agents learn to teach by communicating action advice, thereby improving final performance and accelerating teamwide learning.\nThe learning to teach in MARL problem has unique inherent complexities that compound the delayed reward, credit assignment, and partial observability issues found in general multiagent problems (Oliehoek and Amato 2016). As such, there are several key issues that must be addressed. First, agents must learn when to teach, what to teach, and how to learn from what is being taught. Second, despite coordinating in a shared environment, agents may be independent/decentralized learners with privacy constraints (e.g., robots from distinct corporations that cannot share full policies), and so must learn how to teach under these constraints. A third issue is that agents must estimate the impact of each piece of advice on their teammate's learning progress. Delays in the accumulation of knowledge make this credit assignment problem difficult, even in supervised/unsupervised learning (Graves et al. 2017). Nonstationarities due to agent interactions and the temporally-extended nature of MARL compound these difficulties in our setting. These issues are unique to our learning to teach setting and remain largely unaddressed in the literature, despite being of practical importance for future decision-making systems. One of the main reasons for the lack of progress addressing these inherent challenges is the significant increase in the computational complexity of this new teaching/learning paradigm compared to multiagent problems that have previously been considered.\nOur paper targets the problem of learning to teach in a multiagent team, which has not been considered before. Each agent in our approach learns both when and what to advise, then uses the received advice to improve local learning. Importantly, these roles are not fixed (see Fig. 1); these agents learn to assume the role of student and/or teacher at appropriate moments, requesting and providing advice to improve teamwide performance and learning. In contrast to prior works, our algorithm supports teaching of heterogeneous teammates and applies to settings where advice exchange incurs a communication cost. Comparisons conducted against state-of-the-art teaching methods show that our teaching agents not only learn significantly faster, but also learn to coordinate in tasks where existing methods fail.", "publication_ref": ["b21", "b10", "b17", "b20", "b12"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Background: Cooperative MARL", "text": "Our work targets cooperative MARL, where agents execute actions that jointly affect the environment, then receive feedback via local observations and a shared reward. This setting is formalized as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP), defined as I, S, A, T , R, \u2126, O, \u03b3 (Oliehoek and Amato 2016); I is the set of n agents, S is the state space, A = \u00d7 i A i is the joint action space, and \u2126 = \u00d7 i \u2126 i is the joint observation space. 1 Joint action a = a 1 , . . . , a n causes state s \u2208 S to transition to s \u2208 S with probability P (s\n|s, a) = T (s, a, s ). At each timestep t, joint observation o = o 1 , . . . , o n is observed with probability P (o|s , a) = O(o, s , a). Given its obser- vation history, h i t = (o i 1 , . . . , o i t ), agent i executes actions dictated by its policy a i = \u03c0 i (h i t ).\nThe joint policy is denoted by \u03c0 = \u03c0 1 , . . . , \u03c0 n and parameterized by \u03b8. It may sometimes be desirable to use a recurrent policy representation (e.g., recurrent neural network) to compute an internal state h t that compresses the observation history, or to explicitly compute a belief state (probability distribution over states); with abuse of notation, we use h t to refer to all such variations of internal states/observation histories. At each timestep, the team receives reward r t = R(s t , a t ), with the objective being to maximize value, V (s; \u03b8) = E[ t \u03b3 t r t |s 0 = s], given discount factor \u03b3 \u2208 [0, 1). Let action-value Q i (o i , a i ; h i ) denote agent i's expected value for executing action a i given a new local observation o i and internal state h i , and using its policy thereafter. We denote by Q(o i ; h i ) the vector of action-values (for all actions) given new observation o i .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Teaching in Cooperative MARL", "text": "This work explores multiagent teaching in a setting where no agent is necessarily an all-knowing expert. This section provides a high-level overview of the motivating scenario. Consider the cooperative MARL setting in Fig. 1, where agents i and j learn a joint task (i.e., a Dec-POMDP). In each learning iteration, these agents interact with the environment and collect data used by their respective learning algorithms, L i and L j , to update their policy parameters, \u03b8 i and \u03b8 j . This is the standard cooperative MARL problem, which we hereafter refer to as P Task : the task-level learning problem. For  Each agent learns to execute the task using task-level policy \u03c0, to request advice using learned student policy \u03c0 S , and to respond with action advice using learned teacher policy \u03c0 T . Each agent can assume a student and/or teacher role at any time. In this example, agent i uses its student policy to request help, agent j advises action a j , which the student executes instead of its originally-intended action a i . By learning to transform the local knowledge captured in task-level policies into action advice, the agents can help one another learn.\nexample, task-level policy \u03c0 i is the policy agent i learns and uses to execute actions in the task. Thus, task-level policies summarize each agent's learned behavioral knowledge.\nDuring task-level learning, it is unlikely for any agent to be an expert. However, each agent may have unique experiences, skill sets, or local knowledge of how to learn effectively in the task. Throughout the learning process, it would likely be useful for agents to advise one another using this local knowledge, in order to improve final performance and accelerate teamwide learning. Moreover, it would be desirable for agents to learn when and what to advise, rather than rely on hand-crafted and domain-specific advising heuristics. Finally, following advising, agents should ideally have learned effective task-level policies that no longer rely on teammate advice at every timestep. We refer to this new problem, which involves agents learning to advise one another to improve joint task-level learning, as P Advise : the advising-level problem.\nThe advising mechanism used in this paper is action advising, where agents suggest actions to one another. By learning to appropriately transform local knowledge (i.e., task-level policies) into action advice, teachers can affect students' experiences and their resulting task-level policy updates. Action advising makes few assumptions, in that learners need only use task-level algorithms L i , L j that support off-policy exploration (enabling execution of action advice for policy updates), and that they receive advisinglevel observations summarizing teammates' learning progress (enabling learning of when/what to advise). Action advising has a good empirical track record (Torrey and Taylor 2013;Taylor et al. 2014;Fachantidis, Taylor, and Vlahavas 2017;da Silva, Glatt, and Costa 2017). However, existing frameworks have key limitations: the majority are designed for single-agent RL and do not consider multiagent learning; their teachers always advise optimal actions to students, mak-ing decisions about when (not what) to teach; they also use heuristics for advising, rather than training teachers by measuring student learning progress. By contrast, agents in our paper learn to interchangeably assume the role of a student (advice requester) and/or teacher (advice responder), denoted S and T , respectively. Each agent learns task-level policy \u03c0 used to actually perform the task, student policy \u03c0 S used to request advice during task-level learning, and teacher policy \u03c0 T used to advise a teammate during task-level learning. 2 Before detailing the algorithm, let us first illustrate the multiagent interactions in this action advising scenario. Consider again Fig. 1, where agents are learning to execute a task (i.e., solving P Task ) while advising one another. While each agent in our framework can assume a student and/or teacher role at any time, Fig. 1 visualizes the case where agent i is the student and agent j the teacher. At a given task-level learning timestep, agent i's task-level policy \u03c0 i outputs an action ('original action a i ' in Fig. 1). However, as the agents are still learning to solve P Task , agent i may prefer to execute an action that maximizes local learning. Thus, agent i uses its student policy \u03c0 i S to decide whether to ask teammate j for advice. If this advice request is made, teammate j checks its teacher policy \u03c0 j T and task-level policy \u03c0 j to decide whether to respond with action advice. Given a response, agent i then executes the advised action ( a j in Fig. 1) as opposed to its originally-intended action (a i in Fig. 1). This results in a local experience that agent i uses to update its task-level policy. A reciprocal process occurs when the agents' roles are reversed. The benefit of advising is that agents can learn to use local knowledge to improve teamwide learning.\nSimilar to recent works that model the multiagent learning process (Hadfield-Menell et al. 2016;Foerster et al. 2018), we focus on the pairwise (two agent) case, targeting the issues of when/what to advise, then detail extensions to n agents. Even in the pairwise case, there exist issues unique to our learning to teach paradigm. First, note that the objectives of P Task and P Advise are distinct. Task-level problem, P Task , has a standard MARL objective of agents learning to coordinate to maximize final performance in the task. Learning to advise ( P Advise ), however, is a higher-level problem, where agents learn to influence teammates' task-level learning by advising them. However, P Task and P Advise are also coupled, as advising influences the task-level policies learned. Agents in our problem must learn to advise despite the nonstationarities due to changing task-level policies, which are also a function of algorithms L i , L j and policy parameterizations \u03b8 i , \u03b8 j .\nLearning to teach is also distinct from prior works that involve agents learning to communicate (Sukhbaatar, Fergus, and others 2016;Foerster et al. 2016;Lowe et al. 2017). These works focus on agents communicating in order to coordinate in a task. By contrast, our problem focuses on agents learning how advising affects the underlying task-level learning process, then using this knowledge to accelerate learning even when agents are non-experts. Thus, the objectives of communication-based multiagent papers are disparate from ours, and the two approaches may even be combined.\n2 Tilde accents (e.g., \u03c0) denote advising-level properties.  In Phase II, advising policies are trained using rewards correlated to task-level learning (see Table 1). Task-level, student, and teacher policy colors above follows convention of Fig. 1.", "publication_ref": ["b27", "b26", "b7", "b6", "b13", "b11", "b10", "b17"], "figure_ref": ["fig_0", "fig_0", "fig_0", "fig_0", "fig_0", "fig_0", "fig_0"], "table_ref": []}, {"heading": "LeCTR: Algorithm for Learning to Coordinate and Teach Reinforcement", "text": "This section introduces our learning to teach approach, details how issues specific to our problem setting are resolved, and summarizes overall training protocol. Pseudocode is presented in the supplementary material due to limited space.\nOverview Our algorithm, Learning to Coordinate and Teach Reinforcement (LeCTR), solves advising-level problem P Advise . The objective is to learn advising policies that augment agents' task-level algorithms L i , L j to accelerate solving of P Task . Our approach involves 2 phases (see Fig. 2):\n\u2022 Phase I: agents learn P Task from scratch using blackbox learning algorithms L i , L j and latest advising policies. \u2022 Phase II: advising policies are updated using advising-level rewards correlated to teammates' task-level learning.\nNo restrictions are placed on agents' task-level algorithms (i.e., they can be heterogeneous). Iteration of Phases I and II enables training of increasingly capable advising policies.\nAdvising Policy Inputs & Outputs LeCTR learns student policies \u03c0 i S , \u03c0 j S and teacher policies \u03c0 i T , \u03c0 j T for agents i and j, constituting a jointly-initiated advising approach that learns when to request advice and when/what to advise. It is often infeasible to learn high-level policies that directly map task-level policy parameters \u03b8 i , \u03b8 j (i.e., local knowledge) to advising decisions: the agents may be independent/decentralized learners and the cost of communicating task-level policy parameters may be high; sharing policy parameters may be undesirable due to privacy concerns; and learning advising policies over the task-level policy parameter space may be infeasible (e.g., if the latter policies involve millions of parameters). Instead, each LeCTR agent learns advising policies over advising-level observations o. As detailed below, these observations are selected to provide information about agents' task-level state and knowledge in a more compact manner than full policy parameters \u03b8 i , \u03b8 j .\nEach LeCTR agent can be a student, teacher, or both simultaneously (i.e., request advice for its own state, while advising a teammate in a different state). For clarity, we detail advising protocols when agents i and j are student and teacher, respectively (see Fig. 1). LeCTR uses distinct advising-level observations for student and teacher policies. Student policy \u03c0 i S for agent i decides when to request advice using advising-level observation o i S = o i , Q i (o i ; h i ) , where o i and Q i (o i ; h i ) are the agent's task-level observation and action-value vectors, respectively. Through o i S , agent i observes a measure of its local task-level observation and policy state. Thus, agent i's student-perspective action is a i S = \u03c0 i S ( o i S ) \u2208 {request advice, do not request advice}. Similarly, agent j's teacher policy \u03c0 j T uses advisinglevel observation o j T = o i , Q i (o i ; h i ), Q j (o i ; h i ) to decide when/what to advise. o j T provides teacher agent j with a measure of student i's task-level state/knowledge (via o i and Q i (o i ; h i )) and of its own task-level knowledge given the student's context (via Q j (o i ; h i )). Using \u03c0 j T , teacher j decides what to advise: either an action from student i's action space, A i , or a special no-advice action a \u2205 . Thus, the teacherperspective action for agent j is a j T = \u03c0 j T ( o j T ) \u2208 A i \u222a { a \u2205 }. Given no advice, student i executes originally-intended action a i . However, given advice a j T , student i executes action \u03b2 i ( a j T ), where \u03b2 i (\u2022) is a local behavioral policy not known by j. The assumption of local behavioral policies increases the generality of LeCTR, as students may locally transform advised actions before execution.\nFollowing advice execution, agents collect task-level experiences and update their respective task-level policies. A key feature is that LeCTR agents learn what to advise by training \u03c0 i T , \u03c0 j T , rather than always advising actions they would have taken in students' states. These agents may learn to advise exploratory actions or even decline to advise if they estimate that such advice will improve teammate learning.\nRewarding Advising Policies Recall in Phase II of LeCTR, advising policies are trained to maximize advisinglevel rewards that should, ideally, reflect the objective of accelerating task-level learning. Without loss of generality, we focus again on the case where agents i and j assume student and teacher roles, respectively, to detail these rewards. Since student policy \u03c0 i S and teacher policy \u03c0 j T must coordinate to help student i learn, they receive identical advisinglevel rewards, r i S = r j T . The remaining issue is to identify advising-level rewards that reflect learning progress. Remark 1. Earning task-level rewards by executing advised actions may not imply actual learning. Thus, rewarding advising-level policies with the task-level reward, r, received after advice execution can lead to poor advising policies.\nWe evaluate many choices of advising-level rewards, which are summarized and described in Table 1. The unifying intuition is that each reward type corresponds to a different measure of the advised agent's task-level learning, which occurs after executing an advised action. Readers are referred to the supplementary material for more details.\nNote that at any time, task-level action a i executed by agent i may either be selected by its local task-level policy, or by a teammate j via advising. In Phase II, pair \u03c0 i S , \u03c0 j T is rewarded only if advising occurs (with zero advising reward otherwise). Analogous advising-level rewards apply for the reverse student-teacher pairing j-i, where r j S = r i T . During Phase II, we train all advising-level policies using a joint advising-level reward r = r i T + r j T to induce cooperation. Advising-level rewards are only used during advising-level training, and are computed using either information already available to agents or only require exchange of scalar values (rather than full policy parameters). It is sometimes desirable to consider advising under communication constraints, which can be done by deducting a communication cost c from these advising-level rewards for each piece of advice exchanged.\nTraining Protocol Recall LeCTR's two phases are iterated to enable training of increasingly capable advising policies. In Phase I, task-level learning is conducted using agents' blackbox learning algorithms and latest advising policies. At the task-level, agents may be independent learners with distinct algorithms. Advising policies are executed in a decentralized fashion, but their training in Phase II is centralized. Our advising policies are trained using the multiagent actor-critic approach of Lowe et al. (2017). Let joint advising-level observations, advising-level actions, and advising-level policies (i.e., 'actors') be, respectively, denoted by o = o i S , o j S , o i T , o j T , a = a i S , a j S , a i T , a j T , and \u03c0 = \u03c0 i S , \u03c0 j S , \u03c0 i T , \u03c0 j T , with \u03b8 parameterizing \u03c0. To induce \u03c0 to learn to teach both agents i and j, we use a centralized action-value function (i.e., 'critic') with advising-level reward\nr = r i T + r j T . Critic Q( o, a; \u03b8) is trained by minimizing loss, L( \u03b8)=E o, a, r, o \u223c M [( r+\u03b3 Q( o , a ; \u03b8)\u2212 Q( o, a; \u03b8)) 2 ] a = \u03c0( o ) ,(1)\nwhere a = \u03c0( o ) are next advising actions computed using the advising policies, and M denotes advising-level replay buffer (Mnih et al. 2015). The policy gradient theorem (Sutton et al. 2000) is invoked on objective J( \u03b8) = E[ T t=k \u03b3 t\u2212k r t ] to update advising policies using gradients,\n\u2207 \u03b8 J( \u03b8) = E o, a\u223c M \u2207 \u03b8 log \u03c0( a| o) Q( o, a; \u03b8) (2) = E o, a\u223c M \u03b1\u2208{i,j} \u03c1\u2208{S,T } \u2207 \u03b8 \u03b1 \u03c1 log \u03c0 \u03b1 \u03c1 ( a \u03b1 \u03c1 | o \u03b1 \u03c1 )\u2207 a \u03b1 \u03c1 Q( o, a; \u03b8) ,\nwhere \u03c0 \u03b1 \u03c1 is agent \u03b1's policy in role \u03c1. During training, the advising feedback nonstationarities mentioned earlier are handled as follows: in Phase I, tasklevel policies are trained online (i.e., no replay memory is used so impact of advice on task-level policies is immediately observed by agents); in Phase II, centralized advising-level learning reduces nonstationarities due to teammate learning, and reservoir sampling is used to further reduce advising reward nonstationarities (see supplementary material for details). Our overall approach stabilizes advising-level learning.\nAdvising n Agents In the n agent case, students must also decide how to fuse advice from multiple teachers. This is a complex problem requiring full investigation in future work; feasible ideas include using majority voting for advice fusion (as in da Silva, Glatt, and Costa (2017)), or asking a specific agent for advice by learning a 'teacher score' modulated based on teacher knowledge/previous teaching experiences.\nTable 1: Summary of rewards used to train advising policies. Rewards shown are for the case where agent i is student and agent j teacher (i.e., flip the indices for the reverse case). Each reward corresponds to a different measure of task-level learning after the student executes an action advice and uses it to update its task-level policy. Refer to the supplementary material for more details.", "publication_ref": ["b17", "b18", "b24"], "figure_ref": ["fig_1", "fig_0"], "table_ref": []}, {"heading": "Advising Reward Name", "text": "Description Reward Value r j T = r i S JVG: Joint Value Gain Task-level value V (s; \u03b8) improvement after learning V (s; \u03b8 t+1 ) \u2212 V (s; \u03b8 t ) QTR: Q-Teaching Reward Teacher's estimate of best vs. intended student action\nmax a Q T (o i , a;h i ) \u2212 Q T (o i , a i ;h i ) LG: Loss Gain Student's task-level loss L(\u03b8 i ) reduction L(\u03b8 i t ) \u2212 L(\u03b8 i t+1 )\nLGG: Loss Gradient Gain Student's task-level policy gradient magnitude  \n||\u2207 \u03b8 i L(\u03b8 i )|| 2 2 TDG: TD Gain Student's temporal difference (TD) error \u03b4 i reduction |\u03b4 i t | \u2212 |\u03b4 i t+1 | VEG: Value Estimation Gain Student's value estimateV (\u03b8 i ) gain above threshold \u03c4 1(V (\u03b8 i ) > \u03c4 ) Agent j a 1 a 2 Agent i a 1 0 1 a 2 0.1 0(", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation", "text": "We conduct empirical evaluations on a sequence of increasingly challenging domains involving two agents. In the 'Repeated' game domain, agents coordinate to maximize the payoffs in Fig. 3a over 5 timesteps. In 'Hallway' (see Fig. 4a), agents only observe their own positions and receive +1 reward if they reach opposite goal states; task-level actions are 'move left/right', states are agents' joint grid positions. The higher-dimensional 'Room' game (see Fig. 5a) has the same state/observation/reward structure, but 4 actions ('move up/right/down/left'). Recall student-perspective advising-level actions are to 'ask' or 'not ask' for advice.\nTeacher-perspective actions are to advise an action from the teammate's task-level action space, or to decline advising. For the Repeated, Hallway, and Room games, respectively, each iteration of LeCTR Phase I consists of 50, 100, and 150 task-level learning iterations. Our task-level agents are independent Q-learners with tabular policies for the Repeated game and tile-coded policies (Sutton and Barto 1998) for the other games. Advising policies are neural networks with internal rectified linear unit activations. Refer to the supplementary material for hyperparameters. The advising-level learning nature of our problem makes these domains challenging, despite their visual simplicity; their complexity is comparable to domains tested in recent MARL works that learn over multiagent learning processes (Foerster et al. 2018), which also consider two agent repeated/gridworld games.\nCounterexample demonstrating Remark 1 Fig. 3b (a) Hallway domain overview.  shows results given a poor choice of advising-level reward, r T = r, in the Repeated game. The left plot (in green) shows task-level return received due to both local policy actions and advised actions, which increases as teachers learn. However, in the right plot (blue) we evaluate how well task-level policies perform by themselves, after they have been trained using the final advising-level policies. The poor performance of the resulting task-level policies indicates that advising policies learned to maximize their own rewards r T = r by always advising optimal actions to students, thereby disregarding whether task-level policies actually learn. No exploratory actions are advised, causing poor task-level performance after advising. This counterexample demonstrates that advisinglevel rewards that reflect student learning progress, rather than task-level reward r, are critical for useful advising.\nComparisons to existing teaching approaches Table 2 shows extensive comparisons of existing heuristics-based teaching approaches, no teaching (independent Q-learning), and LeCTR with all advising rewards introduced in Table 1. We use the VEG advising-level reward in the final version of our LeCTR algorithm, but show all advising reward results for completeness. We report both final task-level performance after teaching,V , and also area under the task-level learning curve (AUC) as a measure of rate of learning; higher values are better for both. Single-agent approaches requiring an expert teacher are extended to the MARL setting by using teammates' policies (pre-trained to expert level) as each agent's teacher. In the Repeated game, LeCTR attains best performance in terms of final value and rate of learning (AUC).\nExisting approaches always advise the teaching agent's optimal action to its teammate, resulting in suboptimal returns.\nIn the Hallway and Room games, approaches that tend to over-advise (e.g., Ask Uncertain, Early Advising, and Early Correcting) perform poorly. AdHocVisit and AdHocTD fare better, as their probabilistic nature permits agents to take exploratory actions and sometimes learn optimal policies. Importance Advising and Correct Important heuristics lead agents to suboptimal (distant) goals in Hallway and Room, yet attain positive value due to domain symmetries.\nLeCTR outperforms all approaches when using the VEG advising-level reward (Table 2). While the JVG advisinglevel reward seems an intuitive measure of learning progress due to directly measuring task-level performance, its high variance in situations where the task-level value is sensitive to policy initialization sometimes destabilizes training. JVG is also expensive to compute, requiring game rollouts after each advice exchange. LG and TDG perform poorly due to the high variance of task-level losses used to compute them. We hypothesize that VEG performs best as its thresholded binary advising-level reward filters the underlying noisy task-level losses for teachers. A similar result is reported in recent work on teaching of supervised learners, where threshold-based advising-level rewards have good empirical performance (Fan et al. 2018)  Teaching for transfer learning Learning to teach can also be applied to multiagent transfer learning. We first pretrain task-level policies in the Hallway/Room tasks (denote these T 1 ), flip agents' initial positions, then train agents to use teammates' T 1 task-level policies to accelerate learning in flipped task T 2 . Results for Hallway and Room are shown in Figs. 6a and 6b, respectively, where advising accelerates rate of learning using prior task knowledge. Next, we test transferability of advising policies themselves (i.e., use advising policies trained for one task to accelerate learning in a brand new, but related, task). We fix (no longer train) advising policies from the above transfer learning test. We then consider 2 variants of Room: one with the domain (including initial agent positions) flipped vertically (T 3 ), and one flipped vertically and horizontally (T 4 ). We evaluate the fixed advising policies (trained to transfer from T 1 \u2192 T 2 ) on transfer from T 3 \u2192 T 4 . Learning without advising on T 4 yields AUC 24 \u00b1 26, while using the fixed advising policy for transfer T 3 \u2192 T 4 attains AUC 68 \u00b1 17. Thus, learning is accelerated even when using pre-trained advising policies. While transfer learning typically involves more significant differences in tasks, these preliminary results motivate future Figure 7: Hallway game, impact of communication cost c on advising policy behaviors. First and second rows show probabilities of action advice, P ( a i ) and P ( a j ), for agents i and j, respectively, as their advising policies are trained using LeCTR.\n.\nwork on applications of advising for MARL transfer learning.\nAdvising heterogeneous teammates We consider heterogeneous variants of the Room game where one agent, j, uses rotated versions of its teammate i's action space; e.g., for rotation 90 \u2022 , agent i's action indices correspond to (up/right/down/left), while j's to (left/up/right/down). Comparisons of LeCTR and the best-performing existing methods are shown in Fig. 5b for all rotations. Prior approaches (Importance Advising and Correct Important) work well for homogeneous actions (0 \u2022 rotation). However, they attain 0 AUC for heterogeneous cases, as agents always advise action indices corresponding to their local action spaces, leading teammates to no-reward regions. AdHocVisit works reasonably well for all rotations, by sometimes permitting agents to explore. LeCTR attains highest AUC for all rotations.\nEffect of communication cost on advice exchange We evaluate impact of communication cost on advising by deducting cost c from advising rewards for each piece of advice exchanged. Fig. 7 shows a comparison of action advice probabilities for communication costs c = 0 and c = 0.5 in the Hallway game. With no cost (c = 0 in Fig. 7a), agents learn to advise each other opposite actions ( a left and a right , respectively) in addition to exploratory actions. As LeCTR's VEG advising-level rewards are binary (0 or 1), two-way advising nullifies positive advising-level rewards, penalizing excessive advising. Thus, when c = 0.5 (Fig. 7b), advising becomes unidirectional: one agent advises opposite exploratory actions of its own, while its teammate tends not to advise.", "publication_ref": ["b23", "b11", "b8"], "figure_ref": ["fig_3", "fig_4", "fig_5", "fig_3", "fig_7", "fig_5"], "table_ref": ["tab_2", "tab_2"]}, {"heading": "Related Work", "text": "Effective diffusion of knowledge has been studied in many fields, including inverse reinforcement learning (Ng and Russell 2000), apprenticeship learning (Abbeel and Ng 2004), and learning from demonstration (Argall et al. 2009), wherein students discern and emulate key demonstrated behaviors. Works on curriculum learning (Bengio et al. 2009) are also related, particularly automated curriculum learning (Graves et al. 2017). Though Graves et al. focus on single student supervised/unsupervised learning, they highlight interesting measures of learning progress also used here. Several works meta-learn active learning policies for supervised learning (Bachman, Sordoni, and Trischler 2017;Fang, Li, and Cohn 2017;Pang, Dong, and Hospedales 2018;Fan et al. 2018). Our work also uses advising-level metalearning, but in the regime of MARL, where agents must learn to advise teammates without destabilizing coordination.\nIn action advising, a student executes actions suggested by a teacher, who is typically an expert always advising the optimal action (Torrey and Taylor 2013). These works typically use state importance value I(s,\u00e2) = max a Q(s, a)\u2212Q(s,\u00e2) to decide when to advise, estimating the performance difference between the student's best action versus intended/worstcase action\u00e2. In student-initiated approaches such as Ask Uncertain (Clouse 1996) and Ask Important (Amir et al. 2016), the student decides when to request advice using heuristics based on I(s,\u00e2). In teacher-initiated approaches such as Importance Advising (Torrey and Taylor 2013), Early Correcting (Amir et al. 2016), and Correct Important (Torrey and Taylor 2013), the teacher decides when to advise by comparing student policy \u03c0 S to expert policy \u03c0 T . Q-Teaching (Fachantidis, Taylor, and Vlahavas 2017) learns when to advise by rewarding the teacher I(s,\u00e2) when it advises. See the supplementary material for details of these approaches.\nWhile most works on information transfer target singleagent settings, several exist for MARL. These include imitation learning of expert demonstrations (Le et al. 2017), cooperative inverse reinforcement learning with a human and robot (Hadfield-Menell et al. 2016), and transfer to parallel learners in tasks with similar value functions (Taylor et al. 2013). To our knowledge, AdHocVisit and AdHocTD (da Silva, Glatt, and Costa 2017) are the only action advising methods that do not assume expert teachers; teaching agents always advise the action they would have locally taken in the student's state, using state visit counts as a heuristic to decide when to exchange advise. Wang et al. (2018) uses da Silva, Glatt, and Costa's teaching algorithm with minor changes.", "publication_ref": ["b19", "b0", "b2", "b4", "b12", "b3", "b9", "b8", "b27", "b5", "b1", "b1", "b27", "b7", "b16", "b13", "b25", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "Contribution", "text": "This work introduced a new paradigm for learning to teach in cooperative MARL settings. Our algorithm, LeCTR, uses agents' task-level learning progress as advising policy feedback, training advisors that improve the rate of learning without harming final performance. Unlike prior works (Torrey and Taylor 2013;Taylor et al. 2014;Zimmer, Viappiani, and Weng 2014), our approach avoids hand-crafted advising policies and does not assume expert teachers. Due to the many complexities involved, we focused on the pairwise problem, targeting the issues of when and what to teach. A natural avenue for future work is to investigate the n-agent setting, extending the ideas presented here where appropriate.", "publication_ref": ["b27", "b26", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Supplementary Material", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Details of Advising-level Rewards", "text": "Recall in Phase II of LeCTR, advising policies are trained to maximize advising-level rewards that should, ideally, reflect the objective of accelerating task-level learning. Selection of an appropriate advising-level reward is, itself, non-obvious. Due to this, we considered a variety of advising-level rewards, each corresponding to a different measure of task-level learning after the student executes an action advice and uses it to update its task-level policy. Advising-level rewards r j T below are detailed for the case where agent i is student and agent j teacher (i.e., flip the indices for the reverse case). Recall that the shared reward used to jointly train all advising policies is r = r i T + r j T . \u2022 Joint Value Gain (JVG): Let \u03b8 t and \u03b8 t+1 , respectively, denote agents' joint task-level policy parameters before and after learning from an experience resulting from action advise. The JVG advising-level reward measures improvement in task-level value due to advising, such that,\nr j T = V (s; \u03b8 t+1 ) \u2212 V (s; \u03b8 t ).\n(3) This is, perhaps, the most intuitive choice of advising-level reward, as it directly measures the gain in task-level performance due to advising. However, the JVG reward has high variance in situations where the task-level value is sensitive to policy initialization, which sometimes destabilizes training. Moreover, the JVG reward requires a full evaluation of task-level performance after each advising step, which can be expensive due to the game rollouts required. \u2022 Q-Teaching Reward (QTR): The QTR advising-level reward extends Q-Teaching (Fachantidis, Taylor, and Vlahavas 2017) to MARL by using\nr j T = I T (o i , a i ; h i ) = max a Q T (o i , a;h i ) \u2212 Q T (o i , a i ;h i ),(4)\neach time advising occurs. The motivating intuition for QTR is that teacher j should have higher probability of advising when they estimate that the student's intended action, a i , can be outperformed by a different action (the arg max action).\n\u2022 TD Gain (TDG): For temporal difference (TD) learners, the TDG advising-level reward measures improvement of student i's task-level TD error due to advising,\nr j T = |\u03b4 i t | \u2212 |\u03b4 i t+1 |,(5)\nwhere \u03b4 i t is i's TD error at timestep t. For example, if agents are independent Q-learners at the task-level, then,\n\u03b4 = r + max a Q(o , a ; \u03b8 i , h i ) \u2212 Q(o, a; \u03b8 i , h i ). (6)\nThe motivating intuition for the TDG advising-level reward is that actions that are anticipated to reduce student i's tasklevel TD error should be advised by the teacher j.\n\u2022 Loss Gain (LG): The LG advising-level reward applies to many loss-based algorithms, measuring improvement of the task-level loss function used by student learner i, r j T = L(\u03b8 i t ) \u2212 L(\u03b8 i t+1 ).\nFor example, if agents are independent Q-learners using parameterized task-level policies at the task-level, then\nL(\u03b8 i ) = [r + \u03b3 max a Q(o , a ; \u03b8 i , h i ) \u2212 Q(o, a; \u03b8 i , h i )] 2 .\n(8) The motivating intuition for the LG reward is similar to the TDG, in that teachers should advise actions they anticipate to decrease student's task-level loss function.\n\u2022 Loss Gradient Gain (LGG): The LGG advising-level reward is an extension of the gradient prediction gain (Graves et al. 2017), which measures the magnitude of student parameter updates due to teaching,\nr j T = ||\u2207 \u03b8 i L(\u03b8 i )|| 2 2 . (9\n)\nThe intuition here is that larger task-level parameter updates may be correlated to learning progress. \u2022 Value Estimation Gain (VEG): VEG rewards teachers when student's local value function estimates exceed a threshold \u03c4 ,\nr j T = 1(V (\u03b8 i ) > \u03c4 ),(10)\nusingV (\u03b8 i ) = max a i Q(o i , a i ; \u03b8 i , h i ) and indicator function 1(\u2022). The motivation here is that the student's value function approximation is correlated to its estimated performance as a function of its local experiences. A convenient means of choosing \u03c4 is to set it as a fraction of the value estimated when no teaching occurs.", "publication_ref": ["b7", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Details of Heuristics-based Advising Approaches", "text": "Existing works on action advising typically use the state importance value I \u03c1 (s,\u00e2) = max a Q \u03c1 (s, a) \u2212 Q \u03c1 (s,\u00e2) to decide when to advise, where \u03c1 = S for student-initiated advising, \u03c1 = T for teacher-initiated advising, Q \u03c1 is the corresponding action-value function, and\u00e2 is the student's intended action if known (or the worst-case action otherwise). I \u03c1 (s,\u00e2) estimates the performance difference of best versus intended student action in state s. The following is a summary of prior advising approaches:\n\u2022 The Ask Important heuristic (Amir et al. 2016) requests advice whenever I S (s,\u00e2) \u2265 k, where k is a threshold parameter. \u2022 Ask Uncertain requests when I S (s,\u00e2) < k (Clouse 1996), where k is a threshold parameter. \u2022 Early Advising advises until advice budget depletion.\n\u2022 Importance Advising advises when I T (s, a) \u2265 k (Torrey and Taylor 2013), where k is a threshold parameter. \u2022 Early Correcting advises when \u03c0 S (s) = \u03c0 T (s) (Amir et al. 2016). \u2022 Correct Important advises when I T (s) \u2265 k and \u03c0 S (s) = \u03c0 T (s) (Torrey and Taylor 2013), where k is a threshold parameter. \u2022 Q-Teaching (Fachantidis, Taylor, and Vlahavas 2017) learns when to advise by rewarding the teacher I T (s,\u00e2) when advising occurs. Constrained by a finite advice budget, Q-Teaching has advising performance similar to Importance Advising, with the advantage of not requiring a tuned threshold k.\nPairwise combinations of student-and teacher-initiated approaches can be used to constitute a jointly-initiated approach (Amir et al. 2016), such as ours. As shown in our experiments, application of single-agent teaching approaches yields poor performance in MARL games.\nOptimal Action Advising Note that in the majority of prior approaches, the above heuristics are used to decide when to advise. To address the question of what to advise, these works typically assume that teachers have expert-level knowledge and always advise optimal action to students.\nOptimal action advising has a strong empirical track record in single-agent teaching approaches (Torrey and Taylor 2013;Zimmer, Viappiani, and Weng 2014;Amir et al. 2016). In such settings, the assumed homogeneity of the teacher and student's optimal policies indeed leads optimal action advice to improve student learning (i.e., when the expert teacher's optimal policy is equivalent to student's optimal policy). In the context of multiagent learning, however, this advising strategy has primarily been applied to games where behavioral homogeneity does not substantially degrade team performance (da Silva, Glatt, and Costa 2017). However, there exist scenarios where multiple agents learn best by exhibiting behavioral diversity (e.g., by exploring distinct regions of the state-action space), or where agents have heterogeneous capabilities/action/observation spaces altogether (e.g., coordination of 2-armed and 3-armed robots, robots with different sensors, etc.). Use of optimal action advising in cooperative multiagent tasks can lead to suboptimal joint return, particularly when the optimal policies for agents are heterogeneous. We show this empirically in several of our experiments.\nIn contrast to earlier optimal advising approaches, our LeCTR algorithm applies to the above settings in addition to the standard homogeneous case; this is due to LeCTR's ability to learn a policy over not only when to advise, but also what to advise. As shown in our experiments, while existing probabilistic advising strategies (e.g., AdHocTD and AdHocVisit) attain reasonable performance in heterogeneous action settings, they do so passively by permitting students to sometimes explore their local action spaces. By contrast, LeCTR agents attain even better performance by actively learning what to advise within teammates' action spaces; this constitutes a unique strength of our approach.", "publication_ref": ["b1", "b5", "b1", "b27", "b7", "b1", "b27", "b29", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Architecture, Training Details, and Hyperparameters", "text": "At the teaching level, our advising-level critic is parameterized by a 3-layer multilayer perceptron (MLP), consisting of internal rectified linear unit (ReLU) activations, linear output, and 32 hidden units per layer. Our advising-level actors (advice request/response policies) use a similar parameterization, with the softmax function applied to outputs for discrete advising-level action probabilities. Recurrent neural networks may also be used in settings where use of advisinglevel observation histories yields better performance, though we did not find this necessary in our domains. As in Lowe et al. (2017), we use the Gumbel-Softmax estimator (Jang, Gu, and Poole 2016) to compute gradients for the teaching policies over discrete advising-level actions (readers are referred to their paper for additional details).\nPolicy training is conducted with the Adam optimization algorithm (Kingma and Ba 2014), using a learning rate of 1e\u22123. We use \u03b3 = 0.95 at the task-level and \u03b3 = 0.99 at the advising-level level to induce long-horizon teaching policies. Similar to Graves et al. (2017), we use reservoir sampling to adaptively rescale advising-level rewards with time-varying and non-normalized magnitudes (all except VEG) to the interval [\u22121, 1]. Refer to Graves et al. for details on how this is conducted.", "publication_ref": ["b17", "b14", "b15", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Procedures", "text": "In Table 2,V is computed by running each algorithm until convergence of task-level policies \u03c0 = \u03c0 i , \u03c0 j , and computing the mean value obtained by the final joint policy \u03c0. The area under the learning curve (AUC) is computed by intermittently evaluating the resulting task-level policies \u03c0 throughout learning; while teacher advice is used during learning, the AUC is computed by evaluating the resulting \u03c0 after advising (i.e., in absence of teacher advice actions, such that AUC measures actual student learning progress). All results and uncertainties are reported using at least 10 independent runs, with most results using over 20 independent runs. In Table 2, best results in bold are computed using a Student's t-test with significance level \u03b1 = 0.05.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2", "tab_2"]}, {"heading": "Notation", "text": "The following summarizes the notation used throughout the paper. In general: superscripts i denote properties for an agent i (e.g., a i ); bold notation denotes joint properties for the team (e.g., a = a i , a j ); tilde accents denote properties at the advising-level (e.g., a i ); and bold characters with tilde accent denote joint advising-level properties (e.g., a = a i , a j ).  for agents \u03b1 \u2208 {i, j} do 3:\nLet \u2212\u03b1 denote \u03b1's teammate. Initialize task-level policy parameters \u03b8 3:\nfor Phase I episode e = 1 to E do 4:\no \u2190 initial task-level observation 5:\nfor task-level timestep t = 1 to t end do 6:\no \u2190 GETADVISEOBS(o, \u03b8)\n7:\nfor agents \u03b1 \u2208 {i, j} do 8:\nExchange advice a \u03b1 via advising policies 9:\nif No advising occurred then 10:\nSelect action a \u03b1 via local policy \u03c0 \u03b1 11:\nend if 12:\nend for 13:\na \u2190 a i , a j , a \u2190 a i , a j 14:\nr, o \u2190 Execute action a in task 15:\n\u03b8 i \u2190 L i (\u03b8 i , o i , a i , r, o i ) 16:\n\u03b8 j \u2190 L j (\u03b8 j , o j , a j , r, o j ) ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "Research funded by IBM (as part of the MIT-IBM Watson AI Lab initiative) and a Kwanjeong Educational Foundation Fellowship. The authors thank Dr. Kasra Khosoussi for fruitful discussions early in the paper development process.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Apprenticeship learning via inverse reinforcement learning", "journal": "ACM", "year": "2004", "authors": "P Abbeel; A Y Ng"}, {"ref_id": "b1", "title": "Interactive teaching strategies for agent training", "journal": "International Joint Conferences on Artificial Intelligence", "year": "2016", "authors": "O Amir; E Kamar; A Kolobov; B J Grosz"}, {"ref_id": "b2", "title": "A survey of robot learning from demonstration", "journal": "Robotics and autonomous systems", "year": "2009", "authors": "B D Argall; S Chernova; M Veloso; B Browning"}, {"ref_id": "b3", "title": "Learning algorithms for active learning", "journal": "", "year": "2017", "authors": "P Bachman; A Sordoni; A Trischler"}, {"ref_id": "b4", "title": "Curriculum learning", "journal": "ACM", "year": "2009", "authors": "Y Bengio; J Louradour; R Collobert; J Weston"}, {"ref_id": "b5", "title": "On integrating apprentice learning and reinforcement learning", "journal": "", "year": "1996", "authors": "J A Clouse"}, {"ref_id": "b6", "title": "Simultaneously learning and advising in multiagent reinforcement learning", "journal": "", "year": "2017", "authors": "F L Da Silva; R Glatt; A H R Costa"}, {"ref_id": "b7", "title": "Learning to teach reinforcement learning agents", "journal": "Machine Learning and Knowledge Extraction", "year": "2017", "authors": "A Fachantidis; M E Taylor; I Vlahavas"}, {"ref_id": "b8", "title": "Learning to teach", "journal": "", "year": "2018", "authors": "Y Fan; F Tian; T Qin; X.-Y Li; T.-Y Liu"}, {"ref_id": "b9", "title": "Learning how to active learn: A deep reinforcement learning approach", "journal": "", "year": "2017", "authors": "M Fang; Y Li; T Cohn"}, {"ref_id": "b10", "title": "Learning to communicate with deep multi-agent reinforcement learning", "journal": "", "year": "2016", "authors": "J Foerster; I A Assael; N De Freitas; S Whiteson"}, {"ref_id": "b11", "title": "Learning with opponentlearning awareness", "journal": "", "year": "2018", "authors": "J N Foerster; R Y Chen; M Al-Shedivat; S Whiteson; P Abbeel; I Mordatch"}, {"ref_id": "b12", "title": "Automated curriculum learning for neural networks", "journal": "", "year": "2017", "authors": "A Graves; M G Bellemare; J Menick; R Munos; K Kavukcuoglu"}, {"ref_id": "b13", "title": "Cooperative inverse reinforcement learning", "journal": "", "year": "2016", "authors": "D Hadfield-Menell; S J Russell; P Abbeel; A Dragan"}, {"ref_id": "b14", "title": "Categorical reparameterization with gumbel-softmax", "journal": "", "year": "2016", "authors": "E Jang; S Gu; B Poole"}, {"ref_id": "b15", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "D P Kingma; J Ba"}, {"ref_id": "b16", "title": "Coordinated multi-agent imitation learning", "journal": "", "year": "1995", "authors": "H M Le; Y Yue; P Carr; P Lucey"}, {"ref_id": "b17", "title": "Multi-agent actor-critic for mixed cooperativecompetitive environments", "journal": "", "year": "2017", "authors": "R Lowe; Y Wu; A Tamar; J Harb; O P Abbeel; I Mordatch"}, {"ref_id": "b18", "title": "Human-level control through deep reinforcement learning", "journal": "Nature", "year": "2015", "authors": "V Mnih; K Kavukcuoglu; D Silver; A A Rusu; J Veness; M G Bellemare; A Graves; M Riedmiller; A K Fidjeland; G Ostrovski"}, {"ref_id": "b19", "title": "Algorithms for inverse reinforcement learning", "journal": "Morgan Kaufmann Publishers Inc", "year": "2000", "authors": "A Y Ng; S J Russell"}, {"ref_id": "b20", "title": "A concise introduction to decentralized POMDPs", "journal": "", "year": "2016", "authors": "F A Oliehoek; C ; K Amato; M Dong; T Hospedales"}, {"ref_id": "b21", "title": "Diffusion of innovations", "journal": "Simon and Schuster", "year": "2010", "authors": "E M Rogers"}, {"ref_id": "b22", "title": "Learning multiagent communication with backpropagation", "journal": "", "year": "2016", "authors": "S Sukhbaatar; R Fergus"}, {"ref_id": "b23", "title": "Reinforcement learning: An introduction", "journal": "MIT press Cambridge", "year": "1998", "authors": "R S Sutton; A G Barto"}, {"ref_id": "b24", "title": "Policy gradient methods for reinforcement learning with function approximation", "journal": "", "year": "2000", "authors": "R S Sutton; D A Mcallester; S P Singh; Y Mansour"}, {"ref_id": "b25", "title": "Transfer learning in multi-agent systems through parallel transfer", "journal": "Omnipress", "year": "2013", "authors": "A Taylor; I Dusparic; E Galv\u00e1n-L\u00f3pez; S Clarke; V Cahill"}, {"ref_id": "b26", "title": "Reinforcement learning agents providing advice in complex video games", "journal": "Connection Science", "year": "2014", "authors": "M E Taylor; N Carboni; A Fachantidis; I Vlahavas; L Torrey"}, {"ref_id": "b27", "title": "Teaching on a budget: Agents advising agents in reinforcement learning", "journal": "", "year": "2013", "authors": "L Torrey; M Taylor"}, {"ref_id": "b28", "title": "Efficient convention emergence through decoupled reinforcement social learning with teacher-student mechanism", "journal": "", "year": "2018", "authors": "Y Wang; W Lu; J Hao; J Wei; H.-F Leung"}, {"ref_id": "b29", "title": "Teacher-student framework: a reinforcement learning approach", "journal": "", "year": "2014", "authors": "M Zimmer; P Viappiani; P Weng"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Overview of teaching via action advising in MARL.Each agent learns to execute the task using task-level policy \u03c0, to request advice using learned student policy \u03c0 S , and to respond with action advice using learned teacher policy \u03c0 T . Each agent can assume a student and/or teacher role at any time. In this example, agent i uses its student policy to request help, agent j advises action a j , which the student executes instead of its originally-intended action a i . By learning to transform the local knowledge captured in task-level policies into action advice, the agents can help one another learn.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure2: LeCTR consists of two iterated phases: task-level learning (Phase I), and advising-level learning (Phase II). In Phase II, advising policies are trained using rewards correlated to task-level learning (see Table1). Task-level, student, and teacher policy colors above follows convention of Fig.1.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "a) Repeated game payoffs. Each agent has 2 actions (a1, a2). Counterexample showing poor advising reward choice rT = r. LeCTR Phase I and II iterations are shown as background bands.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: Repeated game. (b)shows a counterexample where using r T = r yields poor advising, as teachers learn to advise actions that maximize reward (left half, green), but do not actually improve student task-level learning (right half, blue).", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 4 :4Figure 4: Hallway game. (a) Agents receive +1 reward by navigating to opposite states in 17-grid hallway. (b) LeCTR accelerates learning & teaching compared to no-teaching.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 :5Figure 5: Room game. (a) Agents receive +1 reward by navigating to opposite goals in 17\u00d75 grid. (b) LeCTR outperforms prior approaches when agents are heterogeneous.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Fig. 4b shows improvement of LeCTR's advising policies due to training, measured by the number of task-level", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 6 :6Figure 6: LeCTR accelerates multiagent transfer learning.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "No communication cost, c = 0, agents advise opposite actions. With c = 0.5, one agent leads & advises actions opposite its own.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "\u03c1TrAgent's advising-level role, where \u03c1 = S for student role, \u03c1 = T for teacher role\u03c0 i S Agent i's advice request policy \u03c0 i T Agent i's advice response policy \u03c0 Joint advising policy \u03c0 = \u03c0 i S Agent i's advice response a i T \u2208 A j \u222a { a \u2205 } aJoint advising action a = a i S Advising-level reward r = r i T + r j T \u03b3 Advising-level discount factor M Advising-level experience replay memoryLeCTR Algorithm Algorithm 1 Get advising-level observations 1: function GETADVISEOBS(o, \u03b8) 2:", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "\u03b1 , Q \u03b1 (o \u03b1 ; h \u03b1 ) 5: o \u03b1 T = o \u2212\u03b1 , Q \u2212\u03b1 (o \u2212\u03b1 ; h \u2212\u03b1 ), Q \u03b1 (o \u2212\u03b1 ; h \u2212\u03b1 )Phase II episode e = 1 to E do 2:", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "level critic by minimizing loss,L( \u03b8) = E o, a, r, o \u223c M [( r+\u03b3 Q( o , a ; \u03b8)\u2212 Q( o, a; \u03b8)) 2 ] a = \u03c0( o )23:for agents \u03b1 \u2208 {i, j} do 24:for roles \u03c1 \u2208 {S, T } do25:Update advising policy parameters \u03b8 \u03b1 \u03c1 via,\u2207 \u03b8 J( \u03b8) = = E o, a\u223c M", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "V and Area under the Curve (AUC) for teaching algorithms. Best results in bold (computed via a t-test with p < 0.05). Independent Q-learning correspond to the no-teaching case. \u2020 Final version LeCTR uses the VEG advising-level reward.", "figure_data": "AlgorithmRepeated GameHallway GameRoom Gam\u0113VAUCVAUCVAUCIndependent Q-learning (No Teaching)2.75\u00b12.12 272\u00b1210 0.56\u00b10.35 36\u00b1240.42\u00b10.33 22\u00b125Ask Important (Amir et al. 2016)1.74\u00b11.89 178\u00b1181 0.53\u00b10.36 39\u00b1270.00\u00b10.000\u00b10Ask Uncertain (Clouse 1996)1.74\u00b11.89 170\u00b1184 0.00\u00b10.000\u00b100.00\u00b10.000\u00b10Early Advising (Torrey and Taylor 2013)0.45\u00b10.00 45\u00b110.00\u00b10.000\u00b100.00\u00b10.000\u00b10Import. Advising (Torrey and Taylor 2013)0.45\u00b10.00 45\u00b110.67\u00b10.07 39\u00b1170.57\u00b10.03 48\u00b18Early Correcting (Amir et al. 2016)0.45\u00b10.00 45\u00b110.00\u00b10.000\u00b100.00\u00b10.000\u00b10Correct Important (Torrey and Taylor 2013)0.45\u00b10.00 45\u00b110.67\u00b10.07 39\u00b1160.56\u00b10.00 51\u00b17AdHocVisit (da Silva, Glatt, and Costa 2017) 2.49\u00b12.04 244\u00b1199 0.57\u00b10.34 38\u00b1240.43\u00b10.33 22\u00b126AdHocTD (da Silva, Glatt, and Costa 2017)1.88\u00b11.94 184\u00b1189 0.49\u00b10.37 26\u00b1240.39\u00b10.33 26\u00b129LeCTR (with JVG)4.16\u00b11.17 405\u00b1114 0.25\u00b10.37 21\u00b1310.11\u00b10.276\u00b121LeCTR (with QTR)4.52\u00b10.00 443\u00b130.21\u00b10.35 12\u00b1220.20\u00b10.32 11\u00b122LeCTR (with TDG)3.36\u00b11.92 340\u00b1138 0.19\u00b10.34 15\u00b1250.26\u00b10.34 25\u00b132LeCTR (with LG)3.88\u00b11.51 375\u00b1132 0.13\u00b10.29 13\u00b1220.37\u00b10.34 30\u00b132LeCTR (with LGG)4.41\u00b10.69 430\u00b1530.22\u00b10.35 27\u00b1290.56\u00b10.27 56\u00b123LeCTR  \u20204.52\u00b10.00 443\u00b130.77\u00b10.00 71\u00b130.68\u00b10.07 79\u00b116"}], "formulas": [{"formula_id": "formula_0", "formula_text": "|s, a) = T (s, a, s ). At each timestep t, joint observation o = o 1 , . . . , o n is observed with probability P (o|s , a) = O(o, s , a). Given its obser- vation history, h i t = (o i 1 , . . . , o i t ), agent i executes actions dictated by its policy a i = \u03c0 i (h i t ).", "formula_coordinates": [2.0, 53.64, 332.95, 240.52, 55.12]}, {"formula_id": "formula_1", "formula_text": "r = r i T + r j T . Critic Q( o, a; \u03b8) is trained by minimizing loss, L( \u03b8)=E o, a, r, o \u223c M [( r+\u03b3 Q( o , a ; \u03b8)\u2212 Q( o, a; \u03b8)) 2 ] a = \u03c0( o ) ,(1)", "formula_coordinates": [4.0, 317.84, 348.94, 247.95, 46.96]}, {"formula_id": "formula_2", "formula_text": "\u2207 \u03b8 J( \u03b8) = E o, a\u223c M \u2207 \u03b8 log \u03c0( a| o) Q( o, a; \u03b8) (2) = E o, a\u223c M \u03b1\u2208{i,j} \u03c1\u2208{S,T } \u2207 \u03b8 \u03b1 \u03c1 log \u03c0 \u03b1 \u03c1 ( a \u03b1 \u03c1 | o \u03b1 \u03c1 )\u2207 a \u03b1 \u03c1 Q( o, a; \u03b8) ,", "formula_coordinates": [4.0, 319.5, 465.12, 240.48, 48.55]}, {"formula_id": "formula_3", "formula_text": "max a Q T (o i , a;h i ) \u2212 Q T (o i , a i ;h i ) LG: Loss Gain Student's task-level loss L(\u03b8 i ) reduction L(\u03b8 i t ) \u2212 L(\u03b8 i t+1 )", "formula_coordinates": [5.0, 59.98, 128.92, 496.44, 23.72]}, {"formula_id": "formula_4", "formula_text": "||\u2207 \u03b8 i L(\u03b8 i )|| 2 2 TDG: TD Gain Student's temporal difference (TD) error \u03b4 i reduction |\u03b4 i t | \u2212 |\u03b4 i t+1 | VEG: Value Estimation Gain Student's value estimateV (\u03b8 i ) gain above threshold \u03c4 1(V (\u03b8 i ) > \u03c4 ) Agent j a 1 a 2 Agent i a 1 0 1 a 2 0.1 0(", "formula_coordinates": [5.0, 53.7, 151.98, 460.88, 142.54]}, {"formula_id": "formula_5", "formula_text": "r j T = V (s; \u03b8 t+1 ) \u2212 V (s; \u03b8 t ).", "formula_coordinates": [9.0, 119.71, 282.41, 117.04, 13.83]}, {"formula_id": "formula_6", "formula_text": "r j T = I T (o i , a i ; h i ) = max a Q T (o i , a;h i ) \u2212 Q T (o i , a i ;h i ),(4)", "formula_coordinates": [9.0, 63.96, 428.89, 228.54, 26.43]}, {"formula_id": "formula_7", "formula_text": "r j T = |\u03b4 i t | \u2212 |\u03b4 i t+1 |,(5)", "formula_coordinates": [9.0, 140.13, 552.32, 152.38, 13.83]}, {"formula_id": "formula_8", "formula_text": "\u03b4 = r + max a Q(o , a ; \u03b8 i , h i ) \u2212 Q(o, a; \u03b8 i , h i ). (6)", "formula_coordinates": [9.0, 77.06, 597.23, 215.44, 16.65]}, {"formula_id": "formula_10", "formula_text": "L(\u03b8 i ) = [r + \u03b3 max a Q(o , a ; \u03b8 i , h i ) \u2212 Q(o, a; \u03b8 i , h i )] 2 .", "formula_coordinates": [9.0, 330.64, 81.68, 226.19, 16.65]}, {"formula_id": "formula_11", "formula_text": "r j T = ||\u2207 \u03b8 i L(\u03b8 i )|| 2 2 . (9", "formula_coordinates": [9.0, 403.12, 193.64, 151.01, 13.83]}, {"formula_id": "formula_12", "formula_text": ")", "formula_coordinates": [9.0, 554.13, 196.7, 3.87, 8.64]}, {"formula_id": "formula_13", "formula_text": "r j T = 1(V (\u03b8 i ) > \u03c4 ),(10)", "formula_coordinates": [9.0, 402.0, 267.59, 156.0, 13.83]}], "doi": ""}