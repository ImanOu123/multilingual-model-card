{"title": "No-Regret Learning Dynamics for Extensive-Form Correlated Equilibrium", "authors": "Andrea Celli; Alberto Marchesi; Gabriele Farina; Nicola Gatti", "pub_date": "2022-09-02", "abstract": "The existence of simple, uncoupled no-regret dynamics that converge to correlated equilibria in normal-form games is a celebrated result in the theory of multi-agent systems. Specifically, it has been known for more than 20 years that when all players seek to minimize their internal regret in a repeated normal-form game, the empirical frequency of play converges to a normal-form correlated equilibrium. Extensive-form (that is, tree-form) games generalize normal-form games by modeling both sequential and simultaneous moves, as well as private information. Because of the sequential nature and presence of partial information in the game, extensive-form correlation has significantly different properties than the normalform counterpart, many of which are still open research directions. Extensive-form correlated equilibrium (EFCE) has been proposed as the natural extensive-form counterpart to normal-form correlated equilibrium. However, it was currently unknown whether EFCE emerges as the result of uncoupled agent dynamics. In this paper, we give the first uncoupled no-regret dynamics that converge to the set of EFCEs in n-player general-sum extensive-form games with perfect recall. First, we introduce a notion of trigger regret in extensive-form games, which extends that of internal regret in normal-form games. When each player has low trigger regret, the empirical frequency of play is close to an EFCE. Then, we give an efficient no-trigger-regret algorithm. Our algorithm decomposes trigger regret into local subproblems at each decision point for the player, and constructs a global strategy of the player from the local solutions at each decision point.", "sections": [{"heading": "Introduction", "text": "The Nash equilibrium (NE) [37] is the most common notion of rationality in game theory, and its computation in two-player, zero-sum games has been the flagship computational challenge in the area at the interplay between computer science and game theory (see, e.g., the landmark results in heads-up no-limit poker by Brown and Sandholm [5] and Morav\u010d\u00edk et al. [34]). The assumption underpinning NE is that the interaction among players is fully decentralized. Therefore, an NE is a distribution on the uncorrelated strategy space (i.e., a product of independent distributions, one per player). A competing notion of rationality is the correlated equilibrium (CE) proposed by Aumann [2]. A correlated strategy is a general distribution over joint action profiles and it is customarily modeled via a trusted external mediator that draws an action profile from this distribution, and privately recommends to each player her component. A correlated strategy is a CE if no player has an incentive to choose an action different from the mediator's recommendation, because, assuming that all other players also obey, the suggested strategy is the best in expectation.\nMany real-world strategic interactions involve more than two players with arbitrary (i.e., general-sum) utilities. In these settings, the notion of NE presents some weaknesses which render the CE a natural solution concept: (i) computing an NE is an intractable problem, being PPAD-complete even in two-player games [9,11]; (ii) the NE is prone to equilibrium selection issues; and (iii) the social welfare that can be attained via an NE may be significantly lower than what can be achieved via a CE [32,43]. Moreover, in normal-form games, the notion of CE arises from simple learning dynamics in senses that NE does not [28,8].\nThe notion of extensive-form correlated equilibrium (EFCE) by von Stengel and Forges [50] is a natural extension of the CE to the case of sequential strategic interactions. In an EFCE, the mediator draws, before the beginning of the sequential interaction, a recommended action for each of the possible decision points (i.e., information sets) that players may encounter in the game, but she does not immediately reveal recommendations to each player. Instead, the mediator incrementally reveals relevant individual moves as players reach new information sets. At any decision point, the acting player is free to defect from the recommended action, but doing so comes at the cost of future recommendations, which are no longer issued if the player deviates.\nOriginal contributions We focus on general-sum extensive-form games with an arbitrary number of players (including the chance player). In this setting, the problem of computing a feasible EFCE can be solved in polynomial time in the size of the game tree [30] via a variation of the Ellipsoid Against Hope algorithm [38,31]. However, in practice, this approach cannot scale beyond toy problems. Therefore, the following question remains open: is it possible to devise simple dynamics leading to a feasible EFCE? In this paper, we show that the answer is positive. To do so, we define an EFCE via the notion of trigger agent [26,15]. Then, we define the notion of trigger regret, i.e., a notion of internal regret suitable for extensive-form games. We provide an algorithm, which we call ICFR, that minimizes trigger agent regrets via the decomposition of these regrets locally at each information set. In order to do so, ICFR instantiates an internal regret minimizer and multiple external regret minimizers for each information set. We show that it is possible to orchestrate the learning procedure so that, for each information set, employing one regret minimizer per round does not compromise the overall convergence of the algorithm. The empirical frequency of play generated by ICFR converges to an EFCE almost surely in the limit. These results generalize the seminal work by Hart and Mas-Colell [28] to the sequential case via a simple and natural framework.\nConcurrent and Subsequent Work (Updated 2022) An updated and improved journal version of this paper is available on arXiv at https://arxiv.org/abs/2104.01520 [23]. In the journal version, we completely revised the way in which our result is presented, by casting it into the framework of phi-regret minimization [27,48,26]. This is a powerful improvement over the previous way of presenting our work. It helps to better connect the work to prior results that are also based on the phi-regret minimization framework. Notably, we also strengthened our results by providing highprobability convergence bounds for EFCE that hold at finite time, besides almost-sure convergence results in the limit. The conference version only included an almost-sure convergence guarantee.\nIn this section we mention related work that surfaced after the publication of the present paper, and summarize some recent trends related to phi-regret dynamics in games.\nFirst, we acknowledge the thesis work by Zhang [52] on computing certain refinements of EFCE via polynomial-time uncoupled learning dynamics, though their procedure could require up to exponential memory. In a later chapter of the thesis, the author notes that some of the dynamics introduced in the thesis can be modified to guarantee polynomial memory usage when convergence to the set of (unrefined) EFCE is sought. That work was conducted independently and concurrently with ours.\nIn a subsequent paper, Morrill et al. [35] extend some of our results by conducting a study of different forms of correlation in extensive-form games, defining a taxonomy of solution concepts that includes in particular EFCE. Each of their solution concepts is attained by a particular set of no-regret learning dynamics, which is obtained by instantiating the phi-regret minimization framework with a suitablydefined deviation function. Specifically, they identify a general class of deviations-called behavioral deviations-that induce equilibria that can be found through uncoupled no-regret learning dynamics.\nBehavioral deviations are defined as those specifying an action transformation independently at each information set of the game. As the authors note, the deviation functions involved in the definition of EFCE do not fall under that category. A particular class of behavioral deviation functions-called causal partial sequence deviations-induces solution concepts that are (subsets of) EFCEs. So, their result begets an alternative set of no-regret learning dynamics that converge to EFCE, based on a different set of deviation functions than those we use in this article.\nA somewhat recent trend in the learning in game literature has seen the introduction of the concept of optimistic learning dynamics which can guarantee convergence to the set of equilibria faster than the O(1/ \u221a T ) rate attainable in the fully adversarial setting. That line of work was pioneered by Daskalakis et al. [13], and has since been extended along several lines [40,41,49,10,14,12,39], incorporating partial or noisy information feedback [25,51,29], and more recently, general Markov games [16,53]. In the case of EFCE dynamics, the work of Anagnostides et al. [1] establishes O(T 1/4 ) trigger regret bounds through optimistic hedge, building on [10], they showed multiplicative stability of the fixed points associated with EFCE. Finally, we acknowledge some very recent papers that have developed dynamics converging to EFCE under bandit feedback [3,45].", "publication_ref": ["b37", "b5", "b34", "b2", "b9", "b11", "b32", "b43", "b28", "b8", "b50", "b30", "b38", "b31", "b26", "b15", "b28", "b23", "b27", "b48", "b26", "b52", "b35", "b13", "b40", "b41", "b49", "b10", "b14", "b12", "b39", "b25", "b51", "b29", "b16", "b53", "b0", "b10", "b3", "b45"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "In this section, we provide some groundings on sequential games and regret minimization (see the books by Shoham and Leyton-Brown [44] and Cesa-Bianchi and Lugosi [8], for additional details).", "publication_ref": ["b44", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Extensive-form games", "text": "We focus on extensive-form games (EFGs) with imperfect information. We denote the set of players as P \u222a {c}, where c is a chance player that selects actions according to fixed known probability distributions, representing exogenous stochasticity. An EFG is usually defined by means of a game tree, where H is the set of nodes of the tree, and a node h \u2208 H is identified by the ordered sequence of actions from the root to the node. Z \u2286 H is the set of terminal nodes, which are the leaves of the tree. For every h \u2208 H \\ Z, we let P (h) \u2208 P \u222a {c} be the unique player who acts at h and A(h) be the set of actions she has available. For each player i \u2208 P, we let u i : Z \u2192 R be her payoff function. Moreover, we denote by p c : Z \u2192 (0, 1) the function assigning each terminal node z \u2208 Z to the product of probabilities of chance moves encountered on the path from the root of the game tree to z. Imperfect information is encoded by using information sets (infosets). Given i \u2208 P, a player i's infoset I groups nodes belonging to player i that are indistinguishable for her, i.e., A(h) = A(k) for any pair of nodes h, k \u2208 I. I i denotes the set of all player i's infosets. Moreover, we let A(I) be the set of actions available at infoset I \u2208 I i . As customary, we assume that the game has perfect recall, i.e., the infosets are such that no player forgets information once acquired. In EFGs with perfect recall, the infosets I i of each player i \u2208 P are partially ordered. We write I J whenever infoset I \u2208 I i precedes J \u2208 I i according to such ordering, i.e., formally, there exists a path in the game tree connecting a node h \u2208 I to some node k \u2208 J. For the ease of notation, given I \u2208 I i , we let C (I) be the set of player i's infosets that follow infoset I (this included), defined as C (I) := {J \u2208 I i | I J}. Moreover, given I \u2208 I i and a \u2208 A(I), we let C(I, a) \u2286 I i be the set of player i's infosets that immediately follow I by playing action a, i.e., those reachable from at least one node h \u2208 I by following a path that includes a and does not pass through another infoset of i.\nNormal-form plans and strategies A normal-form plan for player i \u2208 P is a tuple \u03c0 i \u2208 \u03a0 i := \u00d7 I\u2208Ii A(I) which specifies an action for each player i's infoset, where \u03c0 i (I) represents the action selected by \u03c0 i at infoset I \u2208 I i . We denote with \u03c0 \u2208 \u03a0 := \u00d7 i\u2208P \u03a0 i a joint normal-form plan, defining a plan \u03c0 i \u2208 \u03a0 i for each player i \u2208 P. Moreover, a tuple defining normal-form plans for the opponents of player i \u2208 P is denoted as \u03c0 \u2212i \u2208 \u03a0 \u2212i := \u00d7 j =i\u2208P \u03a0 j . A normal-form strategy \u00b5 i \u2208 \u2206 \u03a0i is a probability distribution over \u03a0 i , where \u00b5 i [\u03c0 i ] denotes the probability of selecting a plan \u03c0 i \u2208 \u03a0 i according to \u00b5 i . Moreover, \u00b5 \u2208 \u2206 \u03a0 is a joint probability distribution defined over \u03a0, with \u00b5[\u03c0] being the probability that the players end up playing the plans prescribed by \u03c0 \u2208 \u03a0.\nSequences For any player i \u2208 P, given an infoset I \u2208 I i and an action a \u2208 A(I), we denote with \u03c3 = (I, a) the sequence of player i's actions reaching infoset I and terminating with a. Notice that, in EFGs with perfect recall, such sequence is uniquely determined, as paths that reach nodes belonging to the same infoset identify the same sequence of player i's actions. We let \u03a3 i := {(I, a) | I \u2208 I i , a \u2208 A(I)} \u222a {\u2205 i } be the set of player i's sequences, where \u2205 i is the empty sequence of player i (representing the case in which she never plays). Additionally, given an infoset I \u2208 I i , we let \u03c3(I) \u2208 \u03a3 i be the sequence of player i's actions that identify infoset I.\nSubsets of (joint) normal-form plans We now define a few useful subsets of \u03a0 i . The reader is encouraged to refer to Figure 1 for a simple example. For every player i \u2208 P and infoset I \u2208 I i , we let \u03a0 i (I) \u2286 \u03a0 i be the set of player i's normal-form plans that prescribe to play so as to reach infoset I whenever possible (depending on the opponents' actions up to that point) and any action whenever reaching I is not possible anymore. Moreover, for every sequence \u03c3 = (I, a) \u2208 \u03a3 i , we let \u03a0 i (\u03c3) \u2286 \u03a0 i (I) \u2286 \u03a0 i be the set of player i's plans that reach infoset I and recommend action a at I. Similarly, given a terminal node z \u2208 Z, we denote with \u03a0 i (z) \u2286 \u03a0 i the set of normal-form plans by which player i plays so as to reach z, while \u03a0(z) := \u00d7 i\u2208P \u03a0 i (z) and \u03a0 \u2212i (z) := \u00d7 j =i\u2208P \u03a0 j (z).  Additional notation For every i \u2208 P and I \u2208 I i , we let Z(I) \u2286 Z be the set of terminal nodes that are reachable from infoset I \u2208 I i of player i. Moreover, Z(I, a) \u2286 Z(I) \u2286 Z is the set of terminal nodes reachable by playing action a \u2208 A(I) at infoset I, whereas Z c (I, a) := Z(I) \\ Z(I, a) is the set of terminal nodes which are reachable by playing an action different from a at I. For any player i \u2208 P, normal-form plan \u03c0 i \u2208 \u03a0 i , infoset I \u2208 I i , and terminal node z \u2208 Z, we define \u03c1 \u03c0i I\u2192z as a function equal to 1 if z is reachable from I when player i plays according to \u03c0 i , and 0 otherwise. Finally, we define a notion of reach such that, for each normal-form plan \u03c0 = (\u03c0 i , \u03c0 \u2212i ) \u2208 \u03a0, infoset I \u2208 I i , and terminal node z \u2208 Z, we have \u03c1\n\u03a0 1 (A) = {\u03c0 1 , . . . , \u03c0 16 } \u03a0 1 (B) = {\u03c0 1 , . . . , \u03c0 8 } \u03a0 1 (C) = {\u03c0 1 , . . . , \u03c0 8 } \u03a0 1 (D) = {\u03c0 9 , . . . , \u03c0 16 } \u03a0 1 ((C, f )) = {\u03c0 3 , \u03c0 4 , \u03c0 7 , \u03c0 8 } \u03a0 1 ((D, g)) = {\u03c0 9 , \u03c0 11 , \u03c0 13 , \u03c0 15 } \u03a0 1 (z) = {\u03c0 1 , \u03c0 2 , \u03c0 3 , \u03c0 4 }\n(\u03c0i,\u03c0\u2212i) I\u2192z := \u03c1 \u03c0i I\u2192z \u2022 1[\u03c0 \u2212i \u2208 \u03a0 \u2212i (z)].", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "External and internal regret minimization", "text": "In the regret minimization framework [54], each player i \u2208 P plays repeatedly against the others by making a series of decisions from a set X i . A regret minimizer for player i \u2208 P is a device that, at each iteration t = 1, . . . , T , supports two operations: (i) RECOMMEND, which provides the next decision x t+1 i \u2208 X i on the basis of the past history of play and the observed utilities up to iteration t; and (ii) OBSERVE, which receives a utility function u t i : X i \u2192 R that is used to evaluate decision x t i . A regret minimizer is evaluated in terms of its cumulative regret. Two types of regret minimizers are commonly studied, depending on the adopted notion of regret, either external or internal regret.\nExternal regret An external-regret minimizer R EXT for player i \u2208 P is a device minimizing the cumulative external regret of player i up to iteration T , which is defined as:\nR T i := max xi\u2208Xi T t=1 u t i (x i ) \u2212 T t=1 u t i (x t i ).(1)\nR T i represents how much player i would have gained by always taking the best decision in hindsight, given the history of utilities observed up to iteration T .\nInternal regret An internal-regret minimizer R INT for player i \u2208 P is a device minimizing the cumulative internal regret of player i up to iteration T , which is defined as:\nmax xi,xi\u2208Xi R T i,(xi,xi) := max xi,xi\u2208Xi T t=1 1[x i = x t i ] u t i (x i ) \u2212 u t i (x i ) .(2)\nIntuitively, player i has small internal regret if, for each pair of decisions (x i ,x i ), she does not regret of not having playedx i each time she selected x i . The notion of internal regret is strictly stronger than the notion of external regret: any algorithm with small internal regret also has small external regret, but the converse does not hold (see Stoltz and Lugosi [47] for an example).\nRegret minimizers show an interesting connection with games when the decision sets X i are the sets of normal-form plans \u03a0 i and the observed utilities u t i are obtained by playing the game according to the selected plans \u03c0 t i . Letting \u03c0 t := (\u03c0 t i ) i\u2208P be the joint normal-form plan resulting at each iteration t = 1, . . . , T , we denote with {\u03c0 t } T t=1 the overall sequence of plays made by the players. Then, the empirical frequency of play\u03bc T \u2208 \u2206 \u03a0 generated by {\u03c0 t } T t=1 is such that for every \u03c0 \u2208 \u03a0:\n\u00b5 T (\u03c0) := |{1 \u2264 t \u2264 T | \u03c0 t = \u03c0}| T .(3)\nIf all the players play according to some external-regret minimizers, then\u03bc T approaches the set of (normal-form) coarse correlated equilibria, even in EFGs (see Cesa-Bianchi and Lugosi [8] and Celli et al. [7] for further details). Moreover, Foster and Vohra [24] and Hart and Mas-Colell [28] established that the empirical frequency of play generated by any no-internal-regret algorithm (see Cesa-Bianchi and Lugosi [8] and Blum and Mansour [4] for some examples) converges to the set of correlated equilibria in repeated games with simultaneous moves (i.e., normal-form games).", "publication_ref": ["b54", "b8", "b7", "b24", "b28", "b8", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Extensive-form correlated equilibria", "text": "The definition of EFCE requires the following notion of trigger agent, which, intuitively, is associated to each player and each of her sequences of action recommendations.\nDefinition 1 (Trigger agent for EFCE). Given a player i \u2208 P, a sequence \u03c3 = (I, a) \u2208 \u03a3 i , and a probability distribution\u03bc i \u2208 \u2206 \u03a0i(I) , an (\u03c3,\u03bc i )-trigger agent for player i is an agent that takes on the role of player i and commits to following all recommendations unless she reaches I and gets recommended to play a. If this happens, the player stops committing to the recommendations and plays according to a plan sampled from\u03bc i until the game ends.\nIt follows that joint probability distribution \u00b5 \u2208 \u2206 \u03a0 is an EFCE if, for every i \u2208 P, player i's expected utility when following the recommendations is at least as large as the expected utility that any (\u03c3,\u03bc i )-trigger agent for player i can achieve (assuming the opponents' do not deviate).\nFor any \u00b5 \u2208 \u2206 \u03a0 , sequence \u03c3 = (I, a) \u2208 \u03a3 i , and (\u03c3,\u03bc i )-trigger agent, we define the probability of the game ending in a terminal node z \u2208 Z(I) as:\np \u03c3 \u00b5,\u03bci (z) := \uf8eb \uf8ec \uf8ec \uf8ed \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i(z) \u00b5(\u03c0 i , \u03c0 \u2212i ) \uf8f6 \uf8f7 \uf8f7 \uf8f8 \uf8eb \uf8ed \u03c0i\u2208\u03a0i(z)\u03bc i (\u03c0 i ) \uf8f6 \uf8f8 p c (z),(4)\nwhich accounts for the fact that the agent follows recommendations until she receives the recommendation of playing a at I, and, thus, she 'gets triggered' and plays according to\u03c0 i sampled from\u03bc i from I onwards. Moreover, the probability of reaching a terminal node z \u2208 Z(I, a) when following the recommendations is defined as follows:\nq \u00b5 (z) := \uf8eb \uf8ed \u03c0\u2208\u03a0(z) \u00b5(\u03c0) \uf8f6 \uf8f8 p c (z).(5)\nThe definition of EFCE reads as follows (see Appendix A or the work by Farina et al. [20] for details):\nDefinition 2 (Extensive-form correlated equilibrium). An EFCE of an EFG is a joint probability distribution \u00b5 \u2208 \u2206 \u03a0 such that, for every i \u2208 P and (\u03c3,\u03bc i )-trigger agent for player i, with \u03c3 = (I, a) \u2208 \u03a3 i , it holds:\nz\u2208Z(I,a) q \u00b5 (z)u i (z) \u2265 z\u2208Z(I) p \u03c3 \u00b5,\u03bci (z)u i (z).(6)\nA joint probability distribution \u00b5 \u2208 \u2206 \u03a0 is said to be an -EFCE when the maximum deviation \u03b4(\u00b5) under \u00b5 is such that:\n\u03b4(\u00b5) := max i\u2208P max \u03c3=(I,a)\u2208\u03a3i \uf8f1 \uf8f2 \uf8f3 max \u00b5i\u2208\u2206 \u03a0 i (I) \uf8f1 \uf8f2 \uf8f3 z\u2208Z(I) p \u03c3 \u00b5,\u03bci (z)u i (z) \uf8fc \uf8fd \uf8fe \u2212 z\u2208Z(I,a) q \u00b5 (z)u i (z) \uf8fc \uf8fd \uf8fe \u2264 . (7)", "publication_ref": ["b20"], "figure_ref": [], "table_ref": []}, {"heading": "Trigger regret and relationships with EFCE", "text": "In this section, we introduce the notion of trigger regret. Intuitively, it measures the regret that each trigger agent has for not having played the best-in-hindsight strategy. As we will show, when each trigger agent has low trigger regret, then the empirical frequency of play is close to being an EFCE.\nGiven a sequence {\u03c0 t } T t=1 , the vector of immediate utilities u t i observed by player i \u2208 P after any iteration t = 1, . . . , T is defined as follows. For every infoset I \u2208 I i and action a \u2208 A(I) we have:\nu t i [I, a] := z\u2208Z(I,a)\\ J\u2208C(I,a) Z(J) 1[\u03c0 t \u2212i \u2208 \u03a0 \u2212i (z)] p c (z)u i (z),\nwhich represents the utility experienced by player i if the game ends after playing action a at infoset I, without going through other player i's infosets and assuming that the other players play as prescribed by the plans \u03c0 t \u2212i \u2208 \u03a0 \u2212i at iteration t. Notice that the summation is over the terminal nodes immediately reachable from I by playing a and the payoff of each terminal node is multiplied by the probability of reaching it given chance probabilities.\nFor i \u2208 P, the following recursive formula defines player i's utility attainable at infoset I \u2208 I i when a normal-form plan \u03c0 i \u2208 \u03a0 i is selected:\nV t I (\u03c0 i ) := u t i [I, \u03c0 i (I)] + J\u2208C(I,\u03c0i(I)) V t J (\u03c0 i ).(8)\nDefinition 3 (Trigger regret). For every player i \u2208 P and sequence \u03c3 = (I, a) \u2208 \u03a3 i , we let R T \u03c3 be the trigger regret for sequence \u03c3, which we define as follows:\nR T \u03c3 := max \u03c0i\u2208\u03a0i(I) T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)] V t I (\u03c0 i ) \u2212 V t I (\u03c0 t i ) .\nThe trigger regret for \u03c3 = (I, a) represents the regret experienced by the trigger agent that gets triggered on sequence \u03c3, i.e., when infoset I is reached and action a is recommended. Notice that R T \u03c3 only accounts for those iterations in which \u03c0 t i \u2208 \u03a0 i (\u03c3), i.e., intuitively, when the actions prescribed by the normal-form plan \u03c0 t i trigger the agent associated to sequence \u03c3. The following theorem shows that minimizing the trigger regrets for each player i \u2208 P and sequence \u03c3 \u2208 \u03a3 i allows to approach the set of EFCEs.\nTheorem 1. At all times T , the empirical frequency of play\u03bc T (Equation 3) is an -EFCE, where\n:= max i\u2208P max \u03c3\u2208\u03a3i R T \u03c3 T . Corollary 1. If lim sup T \u2192\u221e max i\u2208P max \u03c3\u2208\u03a3i R T \u03c3 T \u2264 0, then lim sup T \u2192\u221e \u03b4(\u03bc T ) \u2264 0\n, that is, for any > 0, eventually the empirical frequency of play\u03bc T becomes an -EFCE.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Laminar regret decomposition for trigger regret", "text": "In order to design an algorithm minimizing trigger regrets, we first develop a new regret decomposition that extends the laminar regret decomposition framework introduced by Farina et al. [19]. Our decomposition exploits the structure of the EFG to show that trigger regrets can be minimized by minimizing other suitably defined regret terms which are local at each infoset.\nFirst, for each player i \u2208 P, sequence \u03c3 = (J, a) \u2208 \u03a3 i , and infoset I \u2208 C (J) (i.e., any infoset following from J, this included), we define the notion of subtree regret as follows:\nR T \u03c3,I := max \u03c0i\u2208\u03a0i(I) T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)] V t I (\u03c0 i ) \u2212 V t I (\u03c0 t i ) .\nEach term R T \u03c3,I represents the regret at infoset I experienced by the trigger agent that gets triggered on sequence \u03c3 = (J, a). Differently from the trigger regret R T \u03c3 , which is defined only for the infoset J of \u03c3, the subtree regrets R T \u03c3,I are defined for all the infosets I \u2208 I i such that J I. Remark 1. Given player i \u2208 P, it is immediate to see that, if R T \u03c3,I = o(T ) for each \u03c3 = (J, a) \u2208 \u03a3 i and I \u2208 C (J), then R T \u03c3 = o(T ) for every \u03c3 \u2208 \u03a3 i . Therefore, we can safely focus on the problem of minimizing subtree regrets, as this will automatically guarantee convergence to an EFCE.\nNext, we need to introduce, for every player i \u2208 P and infoset I \u2208 I i , the following parameterized utility function defined at each iteration t = 1, . . . , T :\nu t I : A(I) a \u2192 u t i [I, a] + J\u2208C(I,a) V t J (\u03c0 t i ),(9)\nwhich represents the utility that player i gets, at iteration t, by playing action a at I and following the actions prescribed by \u03c0 t i at the subsequent infosets. Then, for each sequence \u03c3 = (J, a ) \u2208 \u03a3 i , infoset I \u2208 C (J), and action a \u2208 A(I), the laminar subtree regret of action a is defined as:\nR T \u03c3,I,a := T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)] \u00fb t I (a) \u2212\u00fb t I (\u03c0 t i (I)) ,(10)\nwhile, for \u03c3 = (J, a ) \u2208 \u03a3 i and I \u2208 C (J), the laminar subtree regret is:\nR T \u03c3,I := max a\u2208A(I)R T \u03c3,I,a .(11)\nThe following two lemmas show that the subtree regrets can be minimized by minimizing the laminar subtree regrets at all the infosets of the game.\nLemma 1. The subtree regret for each player i \u2208 P, sequence \u03c3 = (J, a ) \u2208 \u03a3 i , and infoset I \u2208 C (J) can be decomposed as:\nR T \u03c3,I = max a\u2208A(I) \uf8f1 \uf8f2 \uf8f3R T \u03c3,I,a + I \u2208C(I,a) R T \u03c3,I \uf8fc \uf8fd \uf8fe .\nThe lemma is proved by recursively applying the definitions of R T \u03c3,I and V t I (\u03c0 i ), and by exploiting Equation (9). Then, Lemma 1 is used to show the following.\nLemma 2. For every player i \u2208 P, sequence \u03c3 = (J, a ) \u2208 \u03a3 i , and infoset I \u2208 C (J), it holds:\nR T \u03c3,I \u2264 max \u03c0i\u2208\u03a0i(I) I \u2208C (I) 1[\u03c0 i \u2208 \u03a0 i (I )]R T \u03c3,I .(12)", "publication_ref": ["b19", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Internal counterfactual regret minimization", "text": "We propose the internal counterfactual regret minimization algorithm (ICFR) as a way to minimize the laminar subtree regrets described in the previous section. At each iteration t, ICFR builds a normal-form plan \u03c0 t i in a top-down fashion by sampling an action locally at each infoset, following a simple rule: if the current infoset can be reached through \u03c0 t i , then an action is sampled according to an internal-regret minimizer; otherwise, an external-regret minimizer is employed. for I \u2208 Ii do 18:\nR INT I .OBSERVE(1[\u03c0 t i \u2208 \u03a0i(I)] \u2022\u00fb t I ) 19: for \u03c3 \u2208 \u03a3 c i (I) do 20: R EXT \u03c3,I .OBSERVE(1[\u03c0 t i \u2208 \u03a0i(\u03c3)] \u2022\u00fb t I )\nIn order to minimize the laminar subtree regrets, ICFR needs to instantiate different regret minimizers for each infoset. For every infoset I \u2208 I i , the algorithm instantiates an internal-regret minimizer R INT I employing an arbitrary no-internal-regret algorithm. Moreover, let \u03a3 c i (I) \u2286 \u03a3 i be the set of sequences of player i that do not allow to reach I and whose last action is played at an infoset preceding I. Formally,\n\u03a3 c i (I) := {(J, a) \u2208 \u03a3 i | J I, a / \u2208 \u03c3(I)}.\nICFR instantiates an additional external-regret minimizer R EXT \u03c3,I for each sequence \u03c3 \u2208 \u03a3 c i (I).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The internal-regret minimizer R INT", "text": "I is responsible for the minimization of the laminar subtree regret\u015d R T \u03c3,I associated to trigger sequences \u03c3 = (I, a) \u2208 \u03a3 i for each a \u2208 A(I). Instead, the external-regret minimizers R EXT \u03c3,I are responsible for the laminar subtree regrets of sequences \u03c3 \u2208 \u03a3 c i (I). Algorithm 1 provides a description of the procedures adopted by ICFR. At iteration t and for each I \u2208 I i , an action is sampled as follows: if the (possibly partial) normal-form plan \u03c0 t i sampled up to this point allows I to be reached (i.e., it is still possible that \u03c0 t i \u2208 \u03a0 i (I)), then an action is selected according to the internal-regret minimizer R INT I (Line 12). Otherwise, if I cannot be reached through the (possibly partial) plan \u03c0 t i , then we let \u03c3 t I be the unique sequence in \u03a3 c i (I) whose actions are prescribed by \u03c0 t i (Line 14). In this case, the player follows the strategy recommended by the external-regret minimizer R EXT \u03c3 t I ,I (Line 15). In the update procedure, the regret minimizers are fed with the vectors\u00fb t I , which, with an abuse of notation, denote the vectors whose components are defined by the values of the corresponding parameterized utility functions\u00fb t I in Equation (9). In particular, for each I \u2208 I i , the internal-regret minimizer R INT I observes the utility vector\u00fb t I only if the sampled plan \u03c0 t i allows to reach infoset I, while each external-regret minimizer R EXT \u03c3,I is updated only if \u03c0 t i prescribes all the actions in the corresponding sequence \u03c3 (Line 18 and Line 20, respectively).\nThe crucial insight is that for each infoset I \u2208 I i , no matter the action selected at I, only one of the regret minimizers will receive a non-zero utility. Consequently, only one of the regret minimizers can cumulate regret at time t, and that is the regret whose recommendation we follow. Therefore, it is possible to show that the empirical frequency of play\u03bc T obtained via ICFR converges almost surely to an EFCE. We start with the following auxiliary result. Example We provide a simple example illustrating the key ideas of ICFR. Figure 2-Left describes an EFG with two infosets I, J of the same player (player i).\nEven in such a simple setting ICFR has to ensure that six laminar subtree regrets are properly minimized (see Figure 2-Right). To simplify the notation, throughout the example we writeR T a,I in place ofR T (I,a),J (the remaining regrets are treated analogously). ICFR instantiates one internal-regret minimizer for each infoset of player i. We denote them by R INT I and R INT J , respectively. Then, we observe that \u03a3 c i (J) = {(I, b)}, because b is the only action of player i satisfying the following conditions: (i) it departs from an infoset which is on the path from the root node to J and (ii) if player i selected b at infoset I, she would no longer be able to reach J. Therefore, ICFR instantiates the external-regret minimizer R EXT b,J .\nSuppose to be at iteration t of ICFR. The sampling procedure starts from infoset I. Being the root of the EFG, I is always reached by player i. Therefore, an action is selected following the recommendation of the internal-regret minimizer R INT I . During the update procedure, R INT I is provided with the utility resulting from the normal-form plan \u03c0 t i obtained from the sampling procedure. Intuitively, this ensures thatR T a,I andR T b,I are small. Now, there are two possibilities: Case \u03c0 t i (I) = a. The partial plan \u03c0 t i allows J to be reached. Therefore, at J, an action is chosen according to the strategy recommended by R INT J . Then, in the update procedure, the internal-regret minimizer R INT J is provided with the observed utility, while the external-regret minimizer is not updated. This ensures thatR T c,J andR T d,J are managed properly. By Equation 11, the choice at t does not impactR T b,J since \u03c0 t i / \u2208 \u03a0 i (I, b), whileR T a,J is affected by the choice at J because a \u2208 \u03c3(J). The internal-regret minimizer R INT J guarantees thatR T c,J = o(T ) andR T d,J = o(T ). Then, by using Lemma 3, we have thatR T a,J = o(T ) holds as well. Case \u03c0 t i (I) = b. We have that \u03c3 t J = (I, b). An action at J is sampled according to the external-regret minimizer R EXT b,J , which is then provided with the observed utility (the internal-regret minimizer R INT J is not updated). This ensures that the increase inR T b,J is small. The other regret terms are not impacted by the choice at t.", "publication_ref": ["b9"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental evaluation", "text": "We evaluate the convergence of ICFR on the standard benchmark games for the computation of correlated equilibria. We use parametric instances from four different multi-player games: Kuhn poker [33], Leduc poker [46], Goofspiel [42], and Battleship [20]. Instances of the Kuhn, Leduc, and Goofspiel games are parametric in the number of players p and in the number of card ranks r. To increase the readability, we denote by Kp.r the Kuhn poker instance with p players and r ranks (the other instances are treated analogously). Our Battleship instance (denoted by BS) has a grid of size 2 \u00d7 2 and maximum number of rounds per player equal to 3. A detailed description of the games is provided in Appendix C.1. We use Regret matching [28] for external-regret minimizers, and the no-internal-regret algorithm by Blum and Mansour [4] for internal-regret minimizers. All experiments are run on a 64-core machine with 512 GB of RAM.\nConvergence of ICFR Figure 3-Center displays the maximum deviation \u03b4(\u03bc T ) as a function of the number of rounds T . According to Equation (7), the strategy\u03bc T is guaranteed to be a \u03b4(\u03bc T )-EFCE. We set a maximum number of 10 4 iterations and, for each instance, we provide the average and the standard deviation computed over 50 different seeds. First, we notice that ICFR attains roughly an empirical convergence rate of O(1/T ). The performance over the Battleship instance suggests that equilibria with large support size are significantly more challenging to be computed. Second, we remark that, unlike recent algorithms for computing EFCEs by Farina et al. [20,21], ICFR can be applied to games with more than two players including chance. Moreover, since EFCE \u2286 EFCCE \u2286 NFCCE, ICFR also provides a flexible way to compute -EFCCEs and -NFCCEs. In the former case, the only known algorithm can only handle games with two players and no chance [22]. In the latter case, the recent algorithms by Celli et al. [7] are significantly outperformed. For example, previous algorithms cannot reach a 0.1-NFCCE in less than 24h on a Leduc instance with 1200 total infosets and a one-bet maximum per bidding round. ICFR reaches = 0.1 in around 9h on an arguably more complex Leduc instance (i.e., more than 9k total infosets and a two-bet maximum per round). Further details on the computation of EFCCEs and NFCCEs are provided in Appendix C.2, together with the plots of the decoupled EFCE deviations of each player.\nSocial Welfare Figure 3-Right provides a visual depiction of the quality of the solutions attained by ICFR in terms of their social welfare. The figure displays the payoffs obtained for 100 different seeds in a two-player Goofspiel instance without chance (i.e., the prize deck is sorted).", "publication_ref": ["b33", "b46", "b42", "b20", "b28", "b4", "b7", "b20", "b21", "b22", "b7"], "figure_ref": ["fig_3", "fig_3"], "table_ref": []}, {"heading": "Broader Impact", "text": "Correlated equilibria provide an appropriate solution concept for coordination problems in which agents have arbitrary utilities, and may work towards different objectives. The study of uncoupled dynamics converging to correlated equilibria in problems with sequential actions and hidden information lays new theoretical foundations for multi-agent reinforcement learning problems. Most of the work in the multi-agent reinforcement learning community either studies fully competitive settings, where agents play selfishly to reach a Nash equilibrium, or fully cooperative scenarios in which agents have the exact same goals. Our work could enable techniques that are in-between these two extremes: agents have arbitrary objectives, but coordinate their actions towards an equilibrium with some desired properties.\nAs we argued in the paper, the social welfare that can be attained via a Nash equilibrium (that is, by playing selfishly) may be significantly lower than what can be achieved via a correlated equilibrium. We provided some empirical evidences that ICFR computes equilibria which attain a social welfare 'not too far' from the optimal one. This could have an arguably positive societal impact when applied to real economic problems. However, further research in this direction is required to prevent 'winner-takes-all' scenarios in problems with an unbalanced reward structure where equilibria with high social welfare may just award players with the largest utilities at the expense of the others. This could provide a way to reach fair equilibria both in theory and in practice.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Extensive-form correlated equilibrium", "text": "In the context of EFGs, the two most widely adopted notions of correlated equilibrium are the normal-form correlated equilibrium (NFCE) [2] and the extensive-form correlated equilibrium (EFCE) [50]. In the former, the mediator draws and recommends a complete normal-form plan to each player before the game starts. Then, each player decides whether to follow the recommended plan or deviate to an arbitrary strategy she desires. In an EFCE, the mediator draws a normal-form plan for each player before the beginning of the game, but she does not immediately reveal it to each player. Instead, the mediator incrementally reveals individual moves as players reach new infosets. At any infoset, the acting player is free to deviate from the recommended action, but doing so comes at the cost of future recommendations, which are no longer issued if the player deviates.\nIn an EFCE, players know less about the normal-form plans that were sampled by the mediator than in an NFCE, where the whole normal-form plan is immediately revealed. Therefore, by exploiting an EFCE, the mediator can more easily incentivize players to follow strategies that may hurt them, as long as players are indifferent as to whether or not to follow the recommendations. This is beneficial when the mediator wants to maximize, e.g., the social-welfare of the game.\nA coarse correlated equilibrium enforces protection against deviations which are independent of the recommended move. Normal-form coarse correlated equilibria (NFCCEs) [36,6] and extensive-form coarse correlated equilibria (EFCCEs) [22] are the coarse equivalent of NFCE and EFCE, respectively. For arbitrary EFGs with perfect recall, the following inclusion of the set of equilibria holds: NFCE \u2286 EFCE \u2286 EFCCE \u2286 NFCCE [50,22]. Appendix A.1 provides a suitable formal definition of the set of EFCEs via the notion of trigger agent (originally introduced by Gordon et al. [26] and Dud\u00edk and Gordon [15]). Finally, Appendix A.2 summarizes existing approaches for computing EFCEs.", "publication_ref": ["b2", "b50", "b36", "b6", "b22", "b50", "b22", "b26", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "A.1 Formal definition of the set of EFCEs", "text": "The definition requires the following notion of trigger agent, which, intuitively, is associated to each player and each of her sequences of action recommendations. Definition 4 (Trigger agent for EFCE). Given a player i \u2208 P, a sequence \u03c3 = (I, a) \u2208 \u03a3 i , and a probability distribution\u03bc i \u2208 \u2206 \u03a0i(I) , an (\u03c3,\u03bc i )-trigger agent for player i is an agent that takes on the role of player i and commits to following all recommendations unless she reaches I and gets recommended to play a. If this happens, the player stops committing to the recommendations and plays according to a plan sampled from\u03bc i until the game ends.\nIt follows that joint probability distribution \u00b5 \u2208 \u2206 \u03a0 is an EFCE if, for every i \u2208 P, player i's expected utility when following the recommendations is at least as large as the expected utility that any (\u03c3,\u03bc i )-trigger agent for player i can achieve (assuming the opponents' do not deviate).\nGiven \u03c3 = (I, a) \u2208 \u03a3 i , in order to express the expected utility of a (\u03c3,\u03bc i )-trigger agent, it is convenient to define the probability of the game ending in each terminal node z \u2208 Z. Three cases are possible. In the first one, z \u2208 Z(I, a). The probability of reaching z given the joint probability distribution \u00b5 \u2208 \u2206 \u03a0 and a (\u03c3,\u03bc i )-trigger agent is defined as:\np \u03c3 \u00b5,\u03bci (z) := \uf8eb \uf8ec \uf8ec \uf8ed \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i(z) \u00b5(\u03c0 i , \u03c0 \u2212i ) \uf8f6 \uf8f7 \uf8f7 \uf8f8 \uf8eb \uf8ed \u03c0i\u2208\u03a0i(z)\u03bc i (\u03c0 i ) \uf8f6 \uf8f8 p c (z),(13)\nwhich accounts for the fact that the agent follows recommendations until she receives the recommendation of playing a at I, and, thus, she 'gets triggered' and plays according to\u03c0 i sampled from\u03bc i from I onwards. The second case is z \u2208 Z c (I, a), which is reached with probability:\ny \u03c3 \u00b5,\u03bci (z) := \uf8eb \uf8ec \uf8ec \uf8ed \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i(z) \u00b5(\u03c0 i , \u03c0 \u2212i ) \uf8f6 \uf8f7 \uf8f7 \uf8f8 \uf8eb \uf8ed \u03c0i\u2208\u03a0i(z)\u03bc i (\u03c0 i ) \uf8f6 \uf8f8 p c (z) + \uf8eb \uf8ed \u03c0\u2208\u03a0(z) \u00b5(\u03c0) \uf8f6 \uf8f8 p c (z),(14)\nwhere the first term accounts for the event that z is reached when the agent 'gets triggered', while the second term is the probability of reaching z while not being triggered (notice that the two events are independent). Finally, the third case is when z \u2208 Z \\ Z(I) and the infoset I is never reached. Then, the probability of reaching z is defined as:\nq \u00b5 (z) := \uf8eb \uf8ed \u03c0\u2208\u03a0(z) \u00b5(\u03c0) \uf8f6 \uf8f8 p c (z).(15)\nBy exploiting the above definitions, the definition of EFCE reads as follows.\nDefinition 5 (Extensive-form correlated equilibrium). An EFCE of an EFG is a probability distribution \u00b5 \u2208 \u2206 \u03a0 such that, for every i \u2208 P and (\u03c3,\u03bc i )-trigger agent for player i, with \u03c3 = (I, a) \u2208 \u03a3 i , it holds:\nz\u2208Z \uf8eb \uf8ed \u03c0\u2208\u03a0(z) \u00b5(\u03c0) \uf8f6 \uf8f8 p c (z)u i (z) \u2265 z\u2208Z(I,a) p \u03c3 \u00b5,\u03bci (z)u i (z) + z\u2208Z c (I,a) y \u03c3 \u00b5,\u03bci (z)u i (z) + z\u2208Z\\Z(I) q \u00b5 (z)u i (z).(16)\nNoticing that the left-hand side of Equation ( 16) is equal to z\u2208Z q \u00b5 (z)u i (z) and that y \u03c3 \u00b5,\u03bci (z) = p \u03c3 \u00b5,\u03bci (z) + q \u00b5 (z), we can rewrite Equation ( 16) as follows:\nz\u2208Z(I,a) q \u00b5 (z)u i (z) \u2265 z\u2208Z(I) p \u03c3 \u00b5,\u03bci (z)u i (z).(17)\nA probability distribution \u00b5 \u2208 \u2206 \u03a0 is said to be an -EFCE if, for every i \u2208 P and (\u03c3,\u03bc i )-trigger agent for player i, with \u03c3 = (I, a) \u2208 \u03a3 i , it holds:\nz\u2208Z(I,a) q \u00b5 (z)u i (z) \u2265 z\u2208Z(I) p \u03c3 \u00b5,\u03bci (z)u i (z) \u2212 .(18)\nA.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Computation of EFCEs", "text": "The problem of computing an optimal EFCE in extensive-form games with more than two players and/or chance moves is known to be NP-hard [50]. However, Huang and von Stengel [30] show that the problem of finding one EFCE can be solved in polynomial time via a variation of the Ellipsoid Against Hope algorithm [38,31]. This holds for arbitrary EFGs with multiple players and/or chance moves. Unfortunately, that algorithm is mainly a theoretical tool, and it is known to have limited scalability beyond toy problems. Dud\u00edk and Gordon [15] provide an alternative sampling-based algorithm to compute EFCEs. However, their algorithm is centralized and based on MCMC sampling which may limit its practical appeal. Our framework is arguably simpler and based on the classical counterfactual regret minimization algorithm [55,19]. Moreover, our framework is fully decentralized since each player, at every decision point, plays so as to minimize her internal/external regret.\nIf we restrict our attention to two-player perfect-recall games without chance moves, than the problem of determining an optimal EFCE can be characterized through a succint linear program with polynomial size in the game description [50]. In this setting, Farina et al. [20] show that the problem of computing an EFCE can be formulated as the solution to a bilinear saddle-point problem, which they solve via a subgradient descent method. Moreover, Farina et al. [21] design a regret minimization algorithm suitable for this specific scenario. In a recent paper, Farina and Sandholm [17] showed that that an optimal EFCE, EFCCE and NFCCE can be computed in polynomial time in the game size in two-player general-sum games that satisfy a condition known as triangle-freeness. The triangle-freeness condition holds, for example, when all chance moves are public, that is, both players observe all chance moves.", "publication_ref": ["b50", "b30", "b38", "b31", "b15", "b55", "b19", "b50", "b20", "b21", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "B Omitted proofs B.1 Proofs for Section 4", "text": "The following auxiliary result is exploited in the proof of Theorem 1.\nLemma 4. For every iteration t = 1, . . . , T , player i \u2208 P, plan\u03c0 i \u2208 \u03a0 i , joint plan \u03c0 t = (\u03c0 t i , \u03c0 t \u2212i ) \u2208 \u03a0, and infoset I \u2208 I i , the following holds:\nV t I (\u03c0 i ) \u2212 V t I (\u03c0 t i ) = z\u2208Z(I) \u03c1 (\u03c0i,\u03c0 t \u2212i ) I\u2192z \u2212 \u03c1 \u03c0 t I\u2192z p c (z)u i (z).\nProof. Given an arbitrary infoset I \u2208 I i , the set of terminal nodes immediately reachable from I through action a \u2208 A(I) is defined as Z I (I, a) := Z(I, a) \\ J\u2208C(I,a)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Z(J).", "text": "By expanding V t I (\u03c0 i ) according to its definition (Equation ( 8)) and by substituting the definition of immediate utility vector u t i we obtain that\nV t I (\u03c0 i ) = u t i [I,\u03c0 i (I)] + J\u2208C(I,\u03c0i(I)) V t J (\u03c0 i ) = z\u2208Z I (I,\u03c0i(I)) 1[\u03c0 t \u2212i \u2208 \u03a0 \u2212i (z)] p c (z)u i (z) + J\u2208C(I,\u03c0i(I)) V t J (\u03c0 i ) = z\u2208Z(I) \u03c1\u03c0 i I\u2192z 1[\u03c0 t \u2212i \u2208 \u03a0 \u2212i (z)] p c (z)u i (z),\nwhere the last expression is obtained by expanding recursively the terms V t J (\u03c0 i ). By definition, \u03c1\n(\u03c0i,\u03c0 t \u2212i ) I\u2192z = \u03c1\u03c0 i I\u2192z \u2022 1[\u03c0 t \u2212i \u2208 \u03a0 \u2212i (z)]\n. Therefore, we can write\nV t I (\u03c0 i ) = z\u2208Z(I) \u03c1 (\u03c0i,\u03c0 t \u2212i ) I\u2192z p c (z)u i (z). Analogously, by expanding V t I (\u03c0 t i )\n, we obtain that V t I (\u03c0 t i ) = z\u2208Z(I) \u03c1 \u03c0 t I\u2192z p c (z)u i (z). This concludes the proof.\nTheorem 1. At all times T , the empirical frequency of play\u03bc T (Equation 3) is an -EFCE, where\n:= max i\u2208P max \u03c3\u2208\u03a3i R T \u03c3 T .\nProof. Fix any player i \u2208 P and any sequence \u03c3 = (I, a) \u2208 \u03a3 i for her. From Lemma 4, the regret R T \u03c3 is\nR T \u03c3 = max \u03c0i\u2208\u03a0i(I) T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)] \uf8eb \uf8ed z\u2208Z(I) \u03c1 (\u03c0i,\u03c0 t \u2212i ) I\u2192z \u2212 \u03c1 \u03c0 t I\u2192z p c (z)u i (z) \uf8f6 \uf8f8 = max \u03c0i\u2208\u03a0i(I) T t=1 \u03c0\u2208\u03a0 1[\u03c0 = \u03c0 t ] \uf8eb \uf8ed 1[\u03c0 i \u2208 \u03a0 i (\u03c3)] \uf8eb \uf8ed z\u2208Z(I) \u03c1 (\u03c0i,\u03c0\u2212i) I\u2192z \u2212 \u03c1 \u03c0 I\u2192z p c (z)u i (z) \uf8f6 \uf8f8 \uf8f6 \uf8f8 = max \u03c0i\u2208\u03a0i(I) \u03c0\u2208\u03a0 1[\u03c0 i \u2208 \u03a0 i (\u03c3)] \uf8eb \uf8ed T t=1 1[\u03c0 = \u03c0 t ] \uf8eb \uf8ed z\u2208Z(I) \u03c1 (\u03c0i,\u03c0\u2212i) I\u2192z \u2212 \u03c1 \u03c0 I\u2192z p c (z)u i (z) \uf8f6 \uf8f8 \uf8f6 \uf8f8 .\nBy using the definition of empirical frequency of play, we can write\nT t=1 1[\u03c0 = \u03c0 t ] = T\u03bc T (\u03c0). Hence, R T \u03c3 = T max \u03c0i\u2208\u03a0i(I) \u03c0\u2208\u03a0 1[\u03c0 i \u2208 \u03a0 i (\u03c3)] \uf8eb \uf8ed\u03bc T (\u03c0) \uf8eb \uf8ed z\u2208Z(I) \u03c1 (\u03c0i,\u03c0\u2212i) I\u2192z \u2212 \u03c1 \u03c0 I\u2192z p c (z)u i (z) \uf8f6 \uf8f8 \uf8f6 \uf8f8 = T max \u03c0i\u2208\u03a0i(I) \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i\u03bc T (\u03c0) \uf8eb \uf8ed z\u2208Z(I) \u03c1 (\u03c0i,\u03c0\u2212i) I\u2192z \u2212 \u03c1 \u03c0 I\u2192z p c (z)u i (z) \uf8f6 \uf8f8 = T max \u03c0i\u2208\u03a0i(I) z\u2208Z(I) \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i\u03bc T (\u03c0) \u03c1 (\u03c0i,\u03c0\u2212i) I\u2192z \u2212 \u03c1 \u03c0 I\u2192z p c (z)u i (z).\nUsing the definition of the \u03c1 I\u2192z symbols, that is,\n\u03c1 (\u03c0i,\u03c0\u2212i) I\u2192z = \u03c1\u03c0 i I\u2192z \u2022 1[\u03c0 \u2212i \u2208 \u03a0 \u2212i (z)], \u03c1 \u03c0 I\u2192z = \u03c1 \u03c0i I\u2192z \u2022 1[\u03c0 \u2212i \u2208 \u03a0 \u2212i (z)], we further obtain R T \u03c3 = T max \u03c0i\u2208\u03a0i(I) z\u2208Z(I) \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i\u03bc T (\u03c0) \u03c1\u03c0 i I\u2192z \u2212 \u03c1 \u03c0i I\u2192z 1[\u03c0 \u2212i \u2208 \u03a0 \u2212i (z)]p c (z)u i (z) = T max \u03c0i\u2208\u03a0i(I) z\u2208Z(I) \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i(z)\u03bc T (\u03c0) \u03c1\u03c0 i I\u2192z \u2212 \u03c1 \u03c0i I\u2192z p c (z)u i (z) = T \uf8eb \uf8ec \uf8ec \uf8ed max \u03c0i\u2208\u03a0i(I) z\u2208Z(I) \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i(z)\u03bc T (\u03c0)\u03c1\u03c0 i I\u2192z p c (z)u i (z) B \uf8f6 \uf8f7 \uf8f7 \uf8f8 \u2212 T \uf8eb \uf8ec \uf8ec \uf8ed z\u2208Z(I) \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i(z)\u03bc T (\u03c0)\u03c1 \u03c0i I\u2192z p c (z)u i (z) C \uf8f6 \uf8f7 \uf8f7 \uf8f8 .\nWe now analyze B and C separately.\nB By convexity, we have:\nB = max \u00b5i\u2208\u2206 \u03a0 i (I) \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 \u03c0i\u2208\u03a0i(I)\u03bc i (\u03c0 i ) \uf8eb \uf8ec \uf8ec \uf8ed z\u2208Z(I) \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i(z)\u03bc T (\u03c0)\u03c1\u03c0 i I\u2192z p c (z)u i (z) \uf8f6 \uf8f7 \uf8f7 \uf8f8 \uf8fc \uf8f4 \uf8f4 \uf8fd \uf8f4 \uf8f4 \uf8fe = max \u00b5i\u2208\u2206 \u03a0 i (I) \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 z\u2208Z(I) \uf8eb \uf8ed \u03c0i\u2208\u03a0i(I)\u03bc i (\u03c0 i )\u03c1\u03c0 i I\u2192z \uf8f6 \uf8f8 \uf8eb \uf8ec \uf8ec \uf8ed \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i(z)\u03bc T (\u03c0)p c (z)u i (z) \uf8f6 \uf8f7 \uf8f7 \uf8f8 \uf8fc \uf8f4 \uf8f4 \uf8fd \uf8f4 \uf8f4 \uf8fe . Since\u03c0 i \u2208 \u03a0 i (I) and z \u2208 Z(I), \u03c1\u03c0 i I\u2192z = 1[\u03c0 i \u2208 \u03a0 i (z)]. So, B = max \u00b5i\u2208\u2206 \u03a0 i (I) \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 z\u2208Z(I) \uf8eb \uf8ed \u03c0i\u2208\u03a0i(I)\u03bc i (\u03c0 i ) 1[\u03c0 i \u2208 \u03a0 i (z)] \uf8f6 \uf8f8 \uf8eb \uf8ec \uf8ec \uf8ed \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i(z)\u03bc T (\u03c0)p c (z)u i (z) \uf8f6 \uf8f7 \uf8f7 \uf8f8 \uf8fc \uf8f4 \uf8f4 \uf8fd \uf8f4 \uf8f4 \uf8fe = max \u00b5i\u2208\u2206 \u03a0 i (I) \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 z\u2208Z(I) \uf8eb \uf8ed \u03c0i\u2208\u03a0i(z)\u03bc i (\u03c0 i ) \uf8f6 \uf8f8 \uf8eb \uf8ec \uf8ec \uf8ed \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i(z)\u03bc T (\u03c0) \uf8f6 \uf8f7 \uf8f7 \uf8f8 p c (z)u i (z) \uf8fc \uf8f4 \uf8f4 \uf8fd \uf8f4 \uf8f4 \uf8fe = max \u00b5i\u2208\u2206 \u03a0 i (I) \uf8f1 \uf8f2 \uf8f3 z\u2208Z(I) p \u03c3 \u00b5 T ,\u03bci (z) u i (z) \uf8fc \uf8fd \uf8fe . (19\n) C Since \u03c0 i \u2208 \u03a0 i (\u03c3) \u2286 \u03a0 i (I) and z \u2208 Z(I), \u03c1 \u03c0i I\u2192z = 1[z \u2208 Z(\u03c3)] \u2022 1[\u03c0 i \u2208 \u03a0 i (z)]. Therefore, C = z\u2208Z(I) \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i(z)\u03bc T (\u03c0) 1[z \u2208 Z(\u03c3)]1[\u03c0 i \u2208 \u03a0 i (z)] p c (z)u i (z) = z\u2208Z(I) \uf8eb \uf8ec \uf8ec \uf8ed 1[z \u2208 Z(\u03c3)] \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i(z)\u03bc T (\u03c0) 1[\u03c0 i \u2208 \u03a0 i (z)] p c (z)u i (z) \uf8f6 \uf8f7 \uf8f7 \uf8f8 = z\u2208Z(\u03c3) \uf8eb \uf8ec \uf8ec \uf8ed \u03c0i\u2208\u03a0i(z) \u03c0\u2212i\u2208\u03a0\u2212i(z)\u03bc T (\u03c0) \uf8f6 \uf8f7 \uf8f7 \uf8f8 p c (z)u i (z) = z\u2208Z(\u03c3) q\u03bcT (z) u i (z).(20)\nSubstituting the expressions in (19) and (20) into the expression for R T \u03c3 , we obtain\nR T \u03c3 T = max \u00b5i\u2208\u2206 \u03a0 i (I) \uf8f1 \uf8f2 \uf8f3 z\u2208Z(I) p \u03c3 \u00b5 T ,\u03bci (z) u i (z) \uf8fc \uf8fd \uf8fe \u2212 z\u2208Z(\u03c3) q\u03bcT (z) u i (z).(21)\nFinally, using the hypothesis, we can write\n= max i\u2208P max \u03c3\u2208\u03a3i R T \u03c3 T = max i\u2208P max \u03c3\u2208\u03a3i \uf8f1 \uf8f2 \uf8f3 max \u00b5i\u2208\u2206 \u03a0 i (I) \uf8f1 \uf8f2 \uf8f3 z\u2208Z(I) p \u03c3 \u00b5 T ,\u03bci (z) u i (z) \uf8fc \uf8fd \uf8fe \u2212 z\u2208Z(\u03c3) q\u03bcT (z) u i (z) \uf8fc \uf8fd \uf8fe = \u03b4(\u03bc T ).\nThis concludes the proof. Proof. By Equation ( 21) we obtain:\n0 \u2265 lim sup T \u2192\u221e R T \u03c3 T = lim sup T \u2192\u221e \uf8eb \uf8ed max \u00b5i\u2208\u2206 \u03a0 i (I) \uf8f1 \uf8f2 \uf8f3 z\u2208Z(I) p \u03c3 \u00b5 T ,\u03bci (z) u i (z) \uf8fc \uf8fd \uf8fe \u2212 z\u2208Z(\u03c3) q\u03bcT (z) u i (z) \uf8f6 \uf8f8 \u2265 max \u00b5i\u2208\u2206 \u03a0 i (I) \uf8f1 \uf8f2 \uf8f3 lim sup T \u2192\u221e z\u2208Z(I) p \u03c3 \u00b5 T ,\u03bci (z) u i (z) \uf8fc \uf8fd \uf8fe \u2212 lim sup T \u2192\u221e z\u2208Z(\u03c3) q\u03bcT (z) u i (z),\nwhere the last inequality follows from swapping the order of lim sup and max. By definition of lim sup, for any > 0, eventually \u03b4(\u03bc T ) < 0 (more precisely: for any > 0, there must be a \u03c4 = \u03c4 ( ) such that \u03b4(\u03bc T ) < for all T \u2265 \u03c4 ), which means that eventually the empirical frequency of play\u03bc T becomes an -EFCE. Proof. By using the recursive definitions of R T \u03c3,I and V t I (\u03c0 i ), we get:\nR T \u03c3,I = max \u03c0i\u2208\u03a0i(I) T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)] V t I (\u03c0 i ) \u2212 V t I (\u03c0 t i ) = max \u03c0i\u2208\u03a0i(I) T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)]V t I (\u03c0 i ) \u2212 T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)]V t I (\u03c0 t i ) = max \u03c0i\u2208\u03a0i(I) \uf8f1 \uf8f2 \uf8f3 T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)] \uf8eb \uf8ed u t i [I,\u03c0 i (I)] + I \u2208C(I,\u03c0i(I)) V t I (\u03c0 i ) \uf8f6 \uf8f8 \uf8fc \uf8fd \uf8fe \u2212 T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)]V t I (\u03c0 t i ) = max a\u2208A(I) \uf8f1 \uf8f2 \uf8f3 T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)]u t i [I, a] + I \u2208C(I,a) max \u03c0i\u2208\u03a0i(I ) T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)]V t I (\u03c0 i ) \uf8fc \uf8fd \uf8fe \u2212 T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)]V t I (\u03c0 t i ) = max a\u2208A(I) \uf8f1 \uf8f2 \uf8f3 T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)]u t i [I, a] + I \u2208C(I,a) R T \u03c3,I + T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)]V t I (\u03c0 t i ) \uf8fc \uf8fd \uf8fe \u2212 T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)]V t I (\u03c0 t i ),\nwhere the last step is by definition of subtree regret. By rewriting the above expression according to Equation ( 9) we get the result.\nLemma 2. For every player i \u2208 P, sequence \u03c3 = (J, a ) \u2208 \u03a3 i , and infoset I \u2208 C (J), it holds:\nR T \u03c3,I \u2264 max \u03c0i\u2208\u03a0i(I) I \u2208C (I) 1[\u03c0 i \u2208 \u03a0 i (I )]R T \u03c3,I .(12)\nProof. Consider an arbitrary sequence \u03c3 = (J, a ) \u2208 \u03a3 i and infoset I \u2208 C (J). By Lemma 1 we have: By starting from I and applying the above equation inductively, we obtain the result.\nR T \u03c3,I = max a\u2208A(I) \uf8f1 \uf8f2 \uf8f3 T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)] \u00fb t I (a) \u2212\u00fb t I (\u03c0 t i (I)) + I \u2208C(I,a) R T \u03c3,I \uf8fc \uf8fd \uf8fe = max a\u2208A(I) \uf8f1 \uf8f2 \uf8f3 T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)]\u00fb t I (a) + I \u2208C(I,a) R T \u03c3,I \uf8fc \uf8fd \uf8fe \u2212 T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)]\u00fb t I (\u03c0 t i (I)) \u2264 max a\u2208A(I) T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)]\u00fb t I (a) + max a\u2208A(I) \uf8f1 \uf8f2 \uf8f3 I \u2208C(I,a) R T \u03c3,I \uf8fc \uf8fd \uf8fe \u2212 T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)]", "publication_ref": ["b19", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "B.3 Proofs for Section 6", "text": "Lemma 5. For any I \u2208 I i and t = 1, . . . , T , if it is the case that \u03c0 t i / \u2208 \u03a0 i (I), then the sequence \u03c3 t I defined by SAMPLEINTERNAL exists and is unique.\nProof. It is enough to proceed from infoset I towards the root of the tree. Eventually, the procedure reaches an infoset I \u2208 I i such that \u03c0 t i \u2208 \u03a0 i (I ). Then, \u03c3 t I is identified by the pair (I , \u03c0 t i (I )). \nt i \u2208 \u03a0 i (I, a)] \u00fb t J (\u00e2) \u2212\u00fb t J (\u03c0 t i (J)) \u2265 max a\u2208A(J) \uf8f1 \uf8f2 \uf8f3 T t=1 a\u2208A(I) 1[\u03c0 t i \u2208 \u03a0 i (I, a)] \u00fb t J (\u00e2) \u2212\u00fb t J (\u03c0 t i (J)) \uf8fc \uf8fd \uf8fe = max a\u2208A(J) T t=1 1[\u03c0 t i \u2208 \u03a0 i (I)] \u00fb t J (\u00e2) \u2212\u00fb t J (\u03c0 t i (J)) = max a\u2208A(J) T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3(I))] \u00fb t J (\u00e2) \u2212\u00fb t J (\u03c0 t i (J))\n=R T \u03c3(I),J .\nThis concludes the proof.\nTheorem 2. When all the players play according to ICFR,\u03bc T converges almost surely to an EFCE.\nProof. By Theorem 1, in order to converge to an EFCE, it is enough to minimize the trigger regrets R T \u03c3 for each player i \u2208 P and sequence \u03c3 = (I, a) \u2208 \u03a3 i . This can be done by minimizing the subtree regrets R T \u03c3,I via the minimization of laminar subtree regretsR T \u03c3,I for each sequence \u03c3 = (J, a) \u2208 \u03a3 i and infoset I \u2208 C (J) (Lemma 2). For any infoset I \u2208 I i , the laminar subtree regretsR T \u03c3,I are partitioned in three groups on the basis of the trigger sequence \u03c3:\n\u2022 Group 1: \u03c3 = (I, a) \u2208 \u03a3 i . Laminar subtree regrets belonging to this group are updated at rounds t such that \u03c0 t i \u2208 \u03a0 i (I), otherwise they remain unchanged. Therefore, they are only updated when the strategy at I is recommended by the internal-regret minimizer R INT I , which guaranteesR T \u03c3,I = o(T ) [8].\n\u2022 Group 2: \u03c3 = (J, a) \u2208 \u03a3 i is such that J I, J = I, and a is not on the path from J to I (i.e., for any \u03c0 i \u2208 \u03a0 i , \u03c0 i (J) = a implies \u03c0 i / \u2208 \u03a0 i (I)). The sequence \u03c3 t I is defined as a sequence compatible with \u03c0 t i and belonging to \u03a3 c i (I). By Lemma 5, for each I \u2208 I i and t = 1, . . . , T , \u03c3 t I exists and is unique. Then, at most one laminar subtree regret term of Group 2 is updated at each round t, otherwise they are left unchanged. Whenever one of these regrets is affected by the choice at t, the action at I is selected according to the external-regret minimizer R EXT \u03c3 t I ,I . This ensures that each laminar subtree regret belonging to this group is o(T ) by the known properties of no-external-regret algorithms [8].\n\u2022 Group 3: \u03c3 = (J, a) \u2208 \u03a3 i is such that J I, J = I, and a is on the path from J to I (notice that for each J I, J = I one such a is unique because player i has perfect recall). Let I \u2208 I i be such that J I I and I \u2208 C(J, a). Notice that, given I, J, and \u03c3, one such I is unique because of the perfect recall assumption. By Lemma 3, we know that ifR T \u03c3 ,I = o(T ) for all \u03c3 = (I , a ) \u2208 \u03a3 i , then it must be the case thatR T \u03c3,I = o(T ) (notice that \u03c3 = (J, a) is the same as \u03c3(I )). By applying the lemma recursively, until all \u03c3 belong to either Group 1 or 2, we can guarantee thatR T \u03c3,I = o(T ).\nThis concludes the proof.", "publication_ref": ["b8", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "C Experimental Evaluation", "text": "Appendix C.1 provides a detailed description of the benchmark games used in our experiments. Finally, Appendix C.2 shows additional experimental results for ICFR.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1 Benchmark games", "text": "The size (in terms on number of infosets and sequences) of the parametric instances we use as benchmark is described in Figure 4. In the following, we provide a detailed explanation of the rules of the games.\nas defined in the outer maximization in Equation (7). Intuitively, for each player, this represents the maximum utility any trigger agent for that player could gain by deviating from the point in which it gets triggered onwards.\nWe do not only consider deviations as prescribed by EFCE, but we also show results for other solutions concepts involving correlation in EFGs, namely EFCCEs and NFCCEs (see Appendix A for their informal description, while for their formal definitions the reader can refer to Farina et al. [22]). As for EFCCEs, the players' incentives to deviate are defined in a way similar to EFCE, using the definition of trigger agent suitable for EFCCEs (see [22]). Instead, for NFCCEs, each player's incentive to deviate corresponds to the utility she/he could gain by playing the best normal-form plan given\u03bc T . We recall that the following relation holds: EFCE \u2286 EFCCE \u2286 NFCCE.\nIn Figures 5 -11, we report players' incentives to deviate obtained with ICFR for EFCE (Left), EFCCE (Center), and NFCCE (Right). As the plots show, the convergence rate is similar for the three cases, electing ICFR as an appealing algorithm also for EFCCEs and NFCCEs. This is the first example of algorithm computing -EFCCEs efficiently in general-sum EFGs with more than two players and chance. Celli et al. [7] propose some algorithms to compute -NFCCEs in general-sum EFGs with an arbitrary number of players (including chance). Our algorithm outperforms those of Celli et al. [7], since the latter cannot reach a 0.1-NFCCE in less than 24h on a Leduc instance with 1200 total infosets and a one-bet maximum per bidding round. Instead, ICFR reaches = 0.1 in around 9h on an arguably more complex Leduc instance (i.e., more than 9k total infosets and a two-bet maximum per round).      ", "publication_ref": ["b7", "b22", "b22", "b7", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments and Disclosure of Funding", "text": "This work is based on work supported by the Italian MIUR PRIN 2017 Project ALGADIMAR \"Algorithms, Games, and Digital Market\", the National Science Foundation under grants IIS-1718457, IIS-1617590, IIS-1901403, and CCF-1733556, and the ARO under awards W911NF-17-1-0082 and W911NF2010081. Gabriele Farina is supported by a Facebook fellowship.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Kuhn poker The two-player version of the game was originally proposed by [33], while the three-player variation is due to [18]. In a three-player Kuhn poker game with rank r, there are r possible cards. Each player initially pays one chip to the pot, and she/he is dealt a single private card. The first player may check or bet (i.e., put an additional chip in the pot). Then, the second player can check or bet after a first player's check, or fold/call the first player's bet. If no bet was previously made, the third player can either check or bet. Otherwise, she/he has to fold or call. After a bet of the second player (resp., third player), the first player (resp., the first and the second players) still has to decide whether to fold or to call the bet. At the showdown, the player with the highest card who has not folded wins all the chips in the pot.\nGoofspiel This game was originally introduced by [42]. Goofspiel is essentially a bidding game where each player has a hand of cards numbered from 1 to r (i.e., the rank of the game). A third stack of r cards is shuffled and singled out as prizes. Each turn, a prize card is revealed, and each player privately chooses one of her/his cards to bid, with the highest card winning the current prize. In case of a tie, the prize card is discarded. After r turns, all the prizes have been dealt out and the payoff of each player is computed as follows: each prize card's value is equal to its face value and the players' scores are computed as the sum of the values of the prize cards they have won. We remark that due to the tie-breaking rule that we employ, even two-player instances of the game are general-sum. All the Goofspiel instances have limited information, i.e., actions of the other players are observed only at the end of the game. This makes the game strategically more challenging, as players have less information regarding previous opponents' actions.\nLeduc We use a three-player version of the classical Leduc hold'em poker introduced by Southey et al. [46]. In a Leduc game instance with r ranks the deck consists of three suits with r cards each. As the game starts players pay one chip to the pot. There are two betting rounds. In the first one a single private card is dealt to each player while in the second round a single board card is revealed. The maximum number of raise per round is set to two, with raise amounts of 2 and 4 in the first and second round, respectively.\nBattleship is a parametric version of the classic board game, where two competing fleets take turns at shooting at each other. For a detailed explanation of the Battleship game see the work by [20] that introduced it. Our instance has loss multiplier equal to 2, and one ship of length 2 and value 1 for each player", "publication_ref": ["b33", "b18", "b42", "b46", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "C.2 Additional results", "text": "We provide detailed results on the convergence of ICFR in terms of players' incentives to deviate from the obtained empirical frequency of play\u03bc T . For EFCEs, these incentives correspond to the maximum deviation of each player,", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Faster no-regret learning dynamics for extensive-form correlated and coarse correlated equilibria", "journal": "", "year": "2022", "authors": "Ioannis Anagnostides; Gabriele Farina; Christian Kroer; Andrea Celli; Tuomas Sandholm"}, {"ref_id": "b1", "title": "", "journal": "ACM", "year": "", "authors": ""}, {"ref_id": "b2", "title": "Subjectivity and correlation in randomized strategies", "journal": "Journal of mathematical Economics", "year": "1974", "authors": " Robert J Aumann"}, {"ref_id": "b3", "title": "Efficient \u03a6-regret minimization in extensive-form games via online mirror descent. CoRR, abs/2205.15294", "journal": "", "year": "", "authors": "Yu Bai; Chi Jin; Song Mei; Ziang Song; Tiancheng Yu"}, {"ref_id": "b4", "title": "From external to internal regret", "journal": "Journal of Machine Learning Research", "year": "2007-06", "authors": "Avrim Blum; Yishay Mansour"}, {"ref_id": "b5", "title": "Superhuman AI for heads-up no-limit poker: Libratus beats top professionals", "journal": "Science", "year": "2017", "authors": "Noam Brown; Tuomas Sandholm"}, {"ref_id": "b6", "title": "Computing optimal ex ante correlated equilibria in two-player sequential games", "journal": "", "year": "2019", "authors": "Andrea Celli; Stefano Coniglio; Nicola Gatti"}, {"ref_id": "b7", "title": "Learning to correlate in multi-player general-sum sequential games", "journal": "", "year": "2019", "authors": "Andrea Celli; Alberto Marchesi; Tommaso Bianchi; Nicola Gatti"}, {"ref_id": "b8", "title": "Prediction, learning, and games", "journal": "Cambridge university press", "year": "2006", "authors": "Nicolo Cesa; - Bianchi; G\u00e1bor Lugosi"}, {"ref_id": "b9", "title": "Settling the complexity of two-player nash equilibrium", "journal": "IEEE", "year": "2006", "authors": "Xi Chen; Xiaotie Deng"}, {"ref_id": "b10", "title": "Hedging in games: Faster convergence of external and swap regrets", "journal": "", "year": "", "authors": "Xi Chen; Binghui Peng"}, {"ref_id": "b11", "title": "The complexity of computing a Nash equilibrium", "journal": "SIAM Journal on Computing", "year": "2009", "authors": "C Daskalakis; P W Goldberg; C H Papadimitriou"}, {"ref_id": "b12", "title": "Fast rates for nonparametric online learning: from realizability to learning in games", "journal": "ACM", "year": "2022", "authors": "Constantinos Daskalakis; Noah Golowich"}, {"ref_id": "b13", "title": "Near-optimal no-regret algorithms for zero-sum games", "journal": "", "year": "2011", "authors": "Constantinos Daskalakis; Alan Deckelbaum; Anthony Kim"}, {"ref_id": "b14", "title": "Near-optimal no-regret learning in general games", "journal": "", "year": "2021", "authors": "Constantinos Daskalakis; Maxwell Fishelson; Noah Golowich"}, {"ref_id": "b15", "title": "A sampling-based approach to computing equilibria in succinct extensive-form games", "journal": "", "year": "2009", "authors": "Miroslav Dud\u00edk; Geoffrey J Gordon"}, {"ref_id": "b16", "title": "Regret minimization and convergence to equilibria in general-sum markov games. CoRR, abs/2207.14211", "journal": "", "year": "", "authors": "Liad Erez; Tal Lancewicki; Uri Sherman; Tomer Koren; Yishay Mansour"}, {"ref_id": "b17", "title": "Polynomial-time computation of optimal correlated equilibria in two-player extensive-form games with public chance moves and beyond", "journal": "", "year": "", "authors": "Gabriele Farina; Tuomas Sandholm"}, {"ref_id": "b18", "title": "Ex ante coordination and collusion in zero-sum multi-player extensive-form games", "journal": "", "year": "2018", "authors": "Gabriele Farina; Andrea Celli; Nicola Gatti; Tuomas Sandholm"}, {"ref_id": "b19", "title": "Online convex optimization for sequential decision processes and extensive-form games", "journal": "", "year": "1917", "authors": "Gabriele Farina; Christian Kroer; Tuomas Sandholm"}, {"ref_id": "b20", "title": "Correlation in extensiveform games: Saddle-point formulation and benchmarks", "journal": "", "year": "2019", "authors": "Gabriele Farina; Chun Kai Ling; Fei Fang; Tuomas Sandholm"}, {"ref_id": "b21", "title": "Efficient regret minimization algorithm for extensive-form correlated equilibrium", "journal": "", "year": "2019", "authors": "Gabriele Farina; Chun Kai Ling; Fei Fang; Tuomas Sandholm"}, {"ref_id": "b22", "title": "Coarse correlation in extensiveform games", "journal": "", "year": "", "authors": "Gabriele Farina; Tommaso Bianchi; Tuomas Sandholm"}, {"ref_id": "b23", "title": "Simple uncoupled no-regret learning dynamics for extensive-form correlated equilibrium", "journal": "", "year": "2021", "authors": "Gabriele Farina; Andrea Celli; Alberto Marchesi; Nicola Gatti"}, {"ref_id": "b24", "title": "Calibrated learning and correlated equilibrium", "journal": "Games and Economic Behavior", "year": "1997", "authors": "P Dean;  Foster; V Rakesh;  Vohra"}, {"ref_id": "b25", "title": "Learning in games: Robustness of fast convergence", "journal": "", "year": "2016", "authors": "Dylan J Foster; Zhiyuan Li; Thodoris Lykouris; Karthik Sridharan; \u00c9va Tardos"}, {"ref_id": "b26", "title": "No-regret learning in convex games", "journal": "", "year": "2008", "authors": "J Geoffrey; Amy Gordon; Casey Greenwald;  Marks"}, {"ref_id": "b27", "title": "A general class of no-regret learning algorithms and gametheoretic equilibria", "journal": "Springer", "year": "2003", "authors": "Amy Greenwald; Amir Jafari"}, {"ref_id": "b28", "title": "A simple adaptive procedure leading to correlated equilibrium", "journal": "Econometrica", "year": "2000", "authors": "Sergiu Hart; Andreu Mas-Colell"}, {"ref_id": "b29", "title": "Noregret learning in games with noisy feedback: Faster rates and adaptivity via learning rate separation. CoRR, abs/2206.06015", "journal": "", "year": "", "authors": "Yu-Guan Hsieh; Kimon Antonakopoulos; Volkan Cevher; Panayotis Mertikopoulos"}, {"ref_id": "b30", "title": "Computing an extensive-form correlated equilibrium in polynomial time", "journal": "Springer", "year": "2008", "authors": "Wan Huang;  Bernhard Von Stengel"}, {"ref_id": "b31", "title": "Polynomial-time computation of exact correlated equilibrium in compact games", "journal": "Games and Economic Behavior", "year": "2015", "authors": "Albert Xin Jiang; Kevin Leyton-Brown"}, {"ref_id": "b32", "title": "Worst-case equilibria", "journal": "Springer", "year": "1999", "authors": "Elias Koutsoupias; Christos Papadimitriou"}, {"ref_id": "b33", "title": "A simplified two-person poker", "journal": "", "year": "1950", "authors": " Harold W Kuhn"}, {"ref_id": "b34", "title": "Deepstack: Expert-level artificial intelligence in heads-up no-limit poker", "journal": "Science", "year": "2017", "authors": "Matej Morav\u010d\u00edk; Martin Schmid; Neil Burch; Viliam Lis\u1ef3; Dustin Morrill; Nolan Bard; Trevor Davis; Kevin Waugh; Michael Johanson; Michael Bowling"}, {"ref_id": "b35", "title": "Efficient deviation types and learning for hindsight rationality in extensive-form games", "journal": "PMLR", "year": "2021-07", "authors": "Dustin Morrill; D' Ryan; Marc Orazio;  Lanctot; R James; Michael Wright; Amy R Bowling;  Greenwald"}, {"ref_id": "b36", "title": "Strategically zero-sum games: the class of games whose completely mixed equilibria cannot be improved upon", "journal": "International Journal of Game Theory", "year": "1978", "authors": "H Moulin; J-P Vial"}, {"ref_id": "b37", "title": "Equilibrium points in n-person games", "journal": "", "year": "1950", "authors": "F John;  Nash"}, {"ref_id": "b38", "title": "Computing correlated equilibria in multiplayer games", "journal": "Journal of the ACM (JACM)", "year": "2008", "authors": "H Christos; Tim Papadimitriou;  Roughgarden"}, {"ref_id": "b39", "title": "Optimal no-regret learning in general games: Bounded regret with unbounded step-sizes via clairvoyant mwu", "journal": "", "year": "2021", "authors": "Georgios Piliouras; Ryann Sim; Stratis Skoulakis"}, {"ref_id": "b40", "title": "Online learning with predictable sequences", "journal": "", "year": "2013", "authors": "Alexander Rakhlin; Karthik Sridharan"}, {"ref_id": "b41", "title": "Optimization, learning, and games with predictable sequences", "journal": "", "year": "2013", "authors": "Alexander Rakhlin; Karthik Sridharan"}, {"ref_id": "b42", "title": "Goofspiel-the game of pure strategy", "journal": "Journal of Applied Probability", "year": "1971", "authors": "M Sheldon;  Ross"}, {"ref_id": "b43", "title": "How bad is selfish routing", "journal": "Journal of the ACM (JACM)", "year": "2002", "authors": "Tim Roughgarden; \u00c9va Tardos"}, {"ref_id": "b44", "title": "Multiagent systems: Algorithmic, game-theoretic, and logical foundations", "journal": "Cambridge University Press", "year": "2008", "authors": "Yoav Shoham; Kevin Leyton-Brown"}, {"ref_id": "b45", "title": "Sample-efficient learning of correlated equilibria in extensive-form games. CoRR, abs/2205.07223", "journal": "", "year": "", "authors": "Ziang Song; Song Mei; Yu Bai"}, {"ref_id": "b46", "title": "Bayes' bluff: Opponent modelling in poker", "journal": "", "year": "2005", "authors": "Finnegan Southey; Michael H Bowling; Bryce Larson; Carmelo Piccione; Neil Burch; Darse Billings; D. Chris Rayner"}, {"ref_id": "b47", "title": "Internal regret in on-line portfolio selection", "journal": "", "year": "2005", "authors": "Gilles Stoltz; G\u00e1bor Lugosi"}, {"ref_id": "b48", "title": "Learning correlated equilibria in games with compact sets of strategies", "journal": "Games and Economic Behavior", "year": "2007", "authors": "Gilles Stoltz; G\u00e1bor Lugosi"}, {"ref_id": "b49", "title": "Fast convergence of regularized learning in games", "journal": "", "year": "2015", "authors": "Vasilis Syrgkanis; Alekh Agarwal; Haipeng Luo; Robert E Schapire"}, {"ref_id": "b50", "title": "Extensive-form correlated equilibrium: Definition and computational complexity", "journal": "Mathematics of Operations Research", "year": "2008", "authors": "Fran\u00e7oise Bernhard Von Stengel;  Forges"}, {"ref_id": "b51", "title": "More adaptive algorithms for adversarial bandits", "journal": "PMLR", "year": "2018", "authors": "Chen- ; Yu Wei; Haipeng Luo"}, {"ref_id": "b52", "title": "A simple adaptive procedure converging to forgiving correlated equilibria", "journal": "", "year": "2022", "authors": "Hugh Zhang"}, {"ref_id": "b53", "title": "Policy optimization for markov games: Unified framework and faster convergence. CoRR, abs/2206.02640", "journal": "", "year": "", "authors": "Runyu Zhang; Qinghua Liu; Huan Wang; Caiming Xiong; Na Li; Yu Bai"}, {"ref_id": "b54", "title": "Online convex programming and generalized infinitesimal gradient ascent", "journal": "", "year": "2003", "authors": "Martin Zinkevich"}, {"ref_id": "b55", "title": "Regret minimization in games with incomplete information", "journal": "", "year": "2008", "authors": "Martin Zinkevich; Michael Johanson; Michael Bowling; Carmelo Piccione"}], "figures": [{"figure_label": "8", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "\u03c0 1 a c e g \u03c0 2 a c e h \u03c0 3 a c f g \u03c0 4 a c f h \u03c0 5 a d e g \u03c0 6 a d e h \u03c0 7 a d f g \u03c0 88a d f h A B C D \u03c0 9 b c e g \u03c0 10 b c e h \u03c0 11 b c f g \u03c0 12 b c f h \u03c0 13 b d e g \u03c0 14 b d e h \u03c0 15 b d f g \u03c0 16 b d f h", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: (Left) Sample game tree. Black round nodes belong to Player 1, white round nodes belong to Player 2, and white square nodes are leaves. Rounded, gray lines denote information sets. (Center) Set \u03a01 of normal-form plans for Player 1. Each plan identifies an action at each information set. (Right) Examples of certain subsets of \u03a01 defined in this subsection.", "figure_data": ""}, {"figure_label": "322", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Lemma 3 .Theorem 2 .Figure 2 :322Figure 2: (Left) EFG with two infosets I and J of player i. (Right) The laminar subtree regrets.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: (Left) Dimension of the game instances in terms of number of players and infosets/sequences for each player. (Center) Convergence of ICFR. (Right) Social welfare attained at different -EFCEs computed via ICFR (black dots corresponds to different seeds).", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Corollary 1 .1If lim sup then lim sup T \u2192\u221e \u03b4(\u03bc T ) \u2264 0, that is, for any > 0, eventually the empirical frequency of play\u03bc T becomes an -EFCE.", "figure_data": ""}, {"figure_label": "251", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "B. 2 5 Lemma 1 .251Proof for Section The subtree regret for each player i \u2208 P, sequence \u03c3 = (J, a ) \u2208 \u03a3 i , and infoset I \u2208 C (J) can be decomposed as:", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Lemma 3 .3For any I, J\u2208 I i : I J, ifR T \u03c3,J = o(T ) for all \u03c3 = (I, a) \u2208 \u03a3 i thenR T \u03c3(I),J = o(T ).Proof. By hypothesis and since the action space A(I) is finite we have that", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 5 :5Figure 5: Players' incentives to deviate with ICFR in the Battleship game.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 6 :6Figure 6: Players' incentives to deviate with ICFR in two-player Goofspiel with 3 ranks.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 7 :7Figure 7: Players' incentives to deviate with ICFR in two-player Goofspiel with 4 ranks.", "figure_data": ""}, {"figure_label": "89", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 8 :Figure 9 :89Figure 8: Players' incentives to deviate with ICFR in three-player Goofspiel with 3 ranks.", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Figure 10 :10Figure 10: Players' incentives to deviate with ICFR in three-player Kuhn Poker with 4 ranks.", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Figure 11 :11Figure 11: Players' incentives to deviate with ICFR in three-player Leduc Poker with 3 ranks.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Algorithm 1 ICFR (for Player i)", "figure_data": "1: function ICFR(i) 2: Initialize the regret minimizers 3: t \u2190 1 4: while t < T do 5: \u03c0 t i \u2190 SAMPLEINTERNAL 6: Observe u t i (i.e., u t i [I, a] for each pair (I, a)) 7: UPDATEINTERNAL(\u03c0 t i , u t i ) t \u2190 t + 1 8: 9: function SAMPLEINTERNAL 10: for I \u2208 Ii in a top-down order do 11: if \u03c0 t i \u2208 \u03a0i(I) then 12: \u03c0 t i (I) \u2190 R INT I .RECOMMEND() 13: else 14: \u03c3 t I \u2190 \u03a3 c i (I) \u2229 {(J, \u03c0 t i (J)) | J I} 15: \u03c0 t i (I) \u2190 R EXT \u03c3 t I ,I .RECOMMEND()16: function UPDATEINTERNAL(\u03c0 t i , u t i ) 17:"}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u03a0 1 (A) = {\u03c0 1 , . . . , \u03c0 16 } \u03a0 1 (B) = {\u03c0 1 , . . . , \u03c0 8 } \u03a0 1 (C) = {\u03c0 1 , . . . , \u03c0 8 } \u03a0 1 (D) = {\u03c0 9 , . . . , \u03c0 16 } \u03a0 1 ((C, f )) = {\u03c0 3 , \u03c0 4 , \u03c0 7 , \u03c0 8 } \u03a0 1 ((D, g)) = {\u03c0 9 , \u03c0 11 , \u03c0 13 , \u03c0 15 } \u03a0 1 (z) = {\u03c0 1 , \u03c0 2 , \u03c0 3 , \u03c0 4 }", "formula_coordinates": [4.0, 372.75, 266.68, 129.48, 86.01]}, {"formula_id": "formula_1", "formula_text": "(\u03c0i,\u03c0\u2212i) I\u2192z := \u03c1 \u03c0i I\u2192z \u2022 1[\u03c0 \u2212i \u2208 \u03a0 \u2212i (z)].", "formula_coordinates": [4.0, 283.44, 504.69, 145.8, 14.3]}, {"formula_id": "formula_2", "formula_text": "R T i := max xi\u2208Xi T t=1 u t i (x i ) \u2212 T t=1 u t i (x t i ).(1)", "formula_coordinates": [4.0, 222.78, 666.22, 281.22, 30.2]}, {"formula_id": "formula_3", "formula_text": "max xi,xi\u2208Xi R T i,(xi,xi) := max xi,xi\u2208Xi T t=1 1[x i = x t i ] u t i (x i ) \u2212 u t i (x i ) .(2)", "formula_coordinates": [5.0, 171.31, 104.6, 332.69, 30.2]}, {"formula_id": "formula_4", "formula_text": "\u00b5 T (\u03c0) := |{1 \u2264 t \u2264 T | \u03c0 t = \u03c0}| T .(3)", "formula_coordinates": [5.0, 235.19, 254.91, 268.81, 23.89]}, {"formula_id": "formula_5", "formula_text": "p \u03c3 \u00b5,\u03bci (z) := \uf8eb \uf8ec \uf8ec \uf8ed \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i(z) \u00b5(\u03c0 i , \u03c0 \u2212i ) \uf8f6 \uf8f7 \uf8f7 \uf8f8 \uf8eb \uf8ed \u03c0i\u2208\u03a0i(z)\u03bc i (\u03c0 i ) \uf8f6 \uf8f8 p c (z),(4)", "formula_coordinates": [5.0, 176.28, 551.57, 327.72, 48.11]}, {"formula_id": "formula_6", "formula_text": "q \u00b5 (z) := \uf8eb \uf8ed \u03c0\u2208\u03a0(z) \u00b5(\u03c0) \uf8f6 \uf8f8 p c (z).(5)", "formula_coordinates": [5.0, 241.16, 657.89, 262.84, 34.15]}, {"formula_id": "formula_7", "formula_text": "z\u2208Z(I,a) q \u00b5 (z)u i (z) \u2265 z\u2208Z(I) p \u03c3 \u00b5,\u03bci (z)u i (z).(6)", "formula_coordinates": [6.0, 217.38, 109.95, 286.62, 22.93]}, {"formula_id": "formula_8", "formula_text": "\u03b4(\u00b5) := max i\u2208P max \u03c3=(I,a)\u2208\u03a3i \uf8f1 \uf8f2 \uf8f3 max \u00b5i\u2208\u2206 \u03a0 i (I) \uf8f1 \uf8f2 \uf8f3 z\u2208Z(I) p \u03c3 \u00b5,\u03bci (z)u i (z) \uf8fc \uf8fd \uf8fe \u2212 z\u2208Z(I,a) q \u00b5 (z)u i (z) \uf8fc \uf8fd \uf8fe \u2264 . (7)", "formula_coordinates": [6.0, 117.96, 174.99, 386.04, 34.55]}, {"formula_id": "formula_9", "formula_text": "u t i [I, a] := z\u2208Z(I,a)\\ J\u2208C(I,a) Z(J) 1[\u03c0 t \u2212i \u2208 \u03a0 \u2212i (z)] p c (z)u i (z),", "formula_coordinates": [6.0, 179.0, 321.87, 253.99, 24.45]}, {"formula_id": "formula_10", "formula_text": "V t I (\u03c0 i ) := u t i [I, \u03c0 i (I)] + J\u2208C(I,\u03c0i(I)) V t J (\u03c0 i ).(8)", "formula_coordinates": [6.0, 215.66, 446.55, 288.34, 22.93]}, {"formula_id": "formula_11", "formula_text": "R T \u03c3 := max \u03c0i\u2208\u03a0i(I) T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)] V t I (\u03c0 i ) \u2212 V t I (\u03c0 t i ) .", "formula_coordinates": [6.0, 187.27, 511.15, 237.47, 30.2]}, {"formula_id": "formula_12", "formula_text": ":= max i\u2208P max \u03c3\u2208\u03a3i R T \u03c3 T . Corollary 1. If lim sup T \u2192\u221e max i\u2208P max \u03c3\u2208\u03a3i R T \u03c3 T \u2264 0, then lim sup T \u2192\u221e \u03b4(\u03bc T ) \u2264 0", "formula_coordinates": [6.0, 108.0, 649.22, 272.33, 61.75]}, {"formula_id": "formula_13", "formula_text": "R T \u03c3,I := max \u03c0i\u2208\u03a0i(I) T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)] V t I (\u03c0 i ) \u2212 V t I (\u03c0 t i ) .", "formula_coordinates": [7.0, 184.76, 172.81, 242.49, 30.2]}, {"formula_id": "formula_14", "formula_text": "u t I : A(I) a \u2192 u t i [I, a] + J\u2208C(I,a) V t J (\u03c0 t i ),(9)", "formula_coordinates": [7.0, 216.49, 319.81, 287.51, 22.93]}, {"formula_id": "formula_15", "formula_text": "R T \u03c3,I,a := T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)] \u00fb t I (a) \u2212\u00fb t I (\u03c0 t i (I)) ,(10)", "formula_coordinates": [7.0, 202.37, 386.39, 301.63, 30.2]}, {"formula_id": "formula_16", "formula_text": "R T \u03c3,I := max a\u2208A(I)R T \u03c3,I,a .(11)", "formula_coordinates": [7.0, 260.82, 436.6, 243.18, 17.45]}, {"formula_id": "formula_17", "formula_text": "R T \u03c3,I = max a\u2208A(I) \uf8f1 \uf8f2 \uf8f3R T \u03c3,I,a + I \u2208C(I,a) R T \u03c3,I \uf8fc \uf8fd \uf8fe .", "formula_coordinates": [7.0, 219.21, 515.47, 173.58, 34.54]}, {"formula_id": "formula_18", "formula_text": "R T \u03c3,I \u2264 max \u03c0i\u2208\u03a0i(I) I \u2208C (I) 1[\u03c0 i \u2208 \u03a0 i (I )]R T \u03c3,I .(12)", "formula_coordinates": [7.0, 213.66, 604.14, 290.34, 22.93]}, {"formula_id": "formula_19", "formula_text": "R INT I .OBSERVE(1[\u03c0 t i \u2208 \u03a0i(I)] \u2022\u00fb t I ) 19: for \u03c3 \u2208 \u03a3 c i (I) do 20: R EXT \u03c3,I .OBSERVE(1[\u03c0 t i \u2208 \u03a0i(\u03c3)] \u2022\u00fb t I )", "formula_coordinates": [8.0, 320.32, 258.77, 172.08, 27.99]}, {"formula_id": "formula_20", "formula_text": "\u03a3 c i (I) := {(J, a) \u2208 \u03a3 i | J I, a / \u2208 \u03c3(I)}.", "formula_coordinates": [8.0, 120.13, 177.97, 171.85, 12.69]}, {"formula_id": "formula_21", "formula_text": "p \u03c3 \u00b5,\u03bci (z) := \uf8eb \uf8ec \uf8ec \uf8ed \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i(z) \u00b5(\u03c0 i , \u03c0 \u2212i ) \uf8f6 \uf8f7 \uf8f7 \uf8f8 \uf8eb \uf8ed \u03c0i\u2208\u03a0i(z)\u03bc i (\u03c0 i ) \uf8f6 \uf8f8 p c (z),(13)", "formula_coordinates": [14.0, 176.28, 546.5, 356.06, 48.11]}, {"formula_id": "formula_22", "formula_text": "y \u03c3 \u00b5,\u03bci (z) := \uf8eb \uf8ec \uf8ec \uf8ed \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i(z) \u00b5(\u03c0 i , \u03c0 \u2212i ) \uf8f6 \uf8f7 \uf8f7 \uf8f8 \uf8eb \uf8ed \u03c0i\u2208\u03a0i(z)\u03bc i (\u03c0 i ) \uf8f6 \uf8f8 p c (z) + \uf8eb \uf8ed \u03c0\u2208\u03a0(z) \u00b5(\u03c0) \uf8f6 \uf8f8 p c (z),(14)", "formula_coordinates": [14.0, 125.8, 644.22, 406.54, 48.11]}, {"formula_id": "formula_23", "formula_text": "q \u00b5 (z) := \uf8eb \uf8ed \u03c0\u2208\u03a0(z) \u00b5(\u03c0) \uf8f6 \uf8f8 p c (z).(15)", "formula_coordinates": [15.0, 241.16, 94.13, 291.18, 34.15]}, {"formula_id": "formula_24", "formula_text": "z\u2208Z \uf8eb \uf8ed \u03c0\u2208\u03a0(z) \u00b5(\u03c0) \uf8f6 \uf8f8 p c (z)u i (z) \u2265 z\u2208Z(I,a) p \u03c3 \u00b5,\u03bci (z)u i (z) + z\u2208Z c (I,a) y \u03c3 \u00b5,\u03bci (z)u i (z) + z\u2208Z\\Z(I) q \u00b5 (z)u i (z).(16)", "formula_coordinates": [15.0, 96.45, 184.01, 435.9, 34.15]}, {"formula_id": "formula_25", "formula_text": "z\u2208Z(I,a) q \u00b5 (z)u i (z) \u2265 z\u2208Z(I) p \u03c3 \u00b5,\u03bci (z)u i (z).(17)", "formula_coordinates": [15.0, 217.38, 264.46, 314.97, 22.93]}, {"formula_id": "formula_26", "formula_text": "z\u2208Z(I,a) q \u00b5 (z)u i (z) \u2265 z\u2208Z(I) p \u03c3 \u00b5,\u03bci (z)u i (z) \u2212 .(18)", "formula_coordinates": [15.0, 209.27, 332.68, 323.08, 22.93]}, {"formula_id": "formula_27", "formula_text": "V t I (\u03c0 i ) \u2212 V t I (\u03c0 t i ) = z\u2208Z(I) \u03c1 (\u03c0i,\u03c0 t \u2212i ) I\u2192z \u2212 \u03c1 \u03c0 t I\u2192z p c (z)u i (z).", "formula_coordinates": [15.0, 186.42, 697.21, 239.16, 26.39]}, {"formula_id": "formula_28", "formula_text": "V t I (\u03c0 i ) = u t i [I,\u03c0 i (I)] + J\u2208C(I,\u03c0i(I)) V t J (\u03c0 i ) = z\u2208Z I (I,\u03c0i(I)) 1[\u03c0 t \u2212i \u2208 \u03a0 \u2212i (z)] p c (z)u i (z) + J\u2208C(I,\u03c0i(I)) V t J (\u03c0 i ) = z\u2208Z(I) \u03c1\u03c0 i I\u2192z 1[\u03c0 t \u2212i \u2208 \u03a0 \u2212i (z)] p c (z)u i (z),", "formula_coordinates": [16.0, 158.98, 160.19, 294.04, 81.83]}, {"formula_id": "formula_29", "formula_text": "(\u03c0i,\u03c0 t \u2212i ) I\u2192z = \u03c1\u03c0 i I\u2192z \u2022 1[\u03c0 t \u2212i \u2208 \u03a0 \u2212i (z)]", "formula_coordinates": [16.0, 79.65, 249.06, 452.69, 33.19]}, {"formula_id": "formula_30", "formula_text": "V t I (\u03c0 i ) = z\u2208Z(I) \u03c1 (\u03c0i,\u03c0 t \u2212i ) I\u2192z p c (z)u i (z). Analogously, by expanding V t I (\u03c0 t i )", "formula_coordinates": [16.0, 79.65, 265.3, 453.04, 32.17]}, {"formula_id": "formula_31", "formula_text": ":= max i\u2208P max \u03c3\u2208\u03a3i R T \u03c3 T .", "formula_coordinates": [16.0, 272.82, 322.3, 73.16, 23.89]}, {"formula_id": "formula_32", "formula_text": "R T \u03c3 = max \u03c0i\u2208\u03a0i(I) T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)] \uf8eb \uf8ed z\u2208Z(I) \u03c1 (\u03c0i,\u03c0 t \u2212i ) I\u2192z \u2212 \u03c1 \u03c0 t I\u2192z p c (z)u i (z) \uf8f6 \uf8f8 = max \u03c0i\u2208\u03a0i(I) T t=1 \u03c0\u2208\u03a0 1[\u03c0 = \u03c0 t ] \uf8eb \uf8ed 1[\u03c0 i \u2208 \u03a0 i (\u03c3)] \uf8eb \uf8ed z\u2208Z(I) \u03c1 (\u03c0i,\u03c0\u2212i) I\u2192z \u2212 \u03c1 \u03c0 I\u2192z p c (z)u i (z) \uf8f6 \uf8f8 \uf8f6 \uf8f8 = max \u03c0i\u2208\u03a0i(I) \u03c0\u2208\u03a0 1[\u03c0 i \u2208 \u03a0 i (\u03c3)] \uf8eb \uf8ed T t=1 1[\u03c0 = \u03c0 t ] \uf8eb \uf8ed z\u2208Z(I) \u03c1 (\u03c0i,\u03c0\u2212i) I\u2192z \u2212 \u03c1 \u03c0 I\u2192z p c (z)u i (z) \uf8f6 \uf8f8 \uf8f6 \uf8f8 .", "formula_coordinates": [16.0, 105.82, 375.95, 400.36, 113.85]}, {"formula_id": "formula_33", "formula_text": "T t=1 1[\u03c0 = \u03c0 t ] = T\u03bc T (\u03c0). Hence, R T \u03c3 = T max \u03c0i\u2208\u03a0i(I) \u03c0\u2208\u03a0 1[\u03c0 i \u2208 \u03a0 i (\u03c3)] \uf8eb \uf8ed\u03bc T (\u03c0) \uf8eb \uf8ed z\u2208Z(I) \u03c1 (\u03c0i,\u03c0\u2212i) I\u2192z \u2212 \u03c1 \u03c0 I\u2192z p c (z)u i (z) \uf8f6 \uf8f8 \uf8f6 \uf8f8 = T max \u03c0i\u2208\u03a0i(I) \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i\u03bc T (\u03c0) \uf8eb \uf8ed z\u2208Z(I) \u03c1 (\u03c0i,\u03c0\u2212i) I\u2192z \u2212 \u03c1 \u03c0 I\u2192z p c (z)u i (z) \uf8f6 \uf8f8 = T max \u03c0i\u2208\u03a0i(I) z\u2208Z(I) \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i\u03bc T (\u03c0) \u03c1 (\u03c0i,\u03c0\u2212i) I\u2192z \u2212 \u03c1 \u03c0 I\u2192z p c (z)u i (z).", "formula_coordinates": [16.0, 125.89, 497.52, 372.38, 138.44]}, {"formula_id": "formula_34", "formula_text": "\u03c1 (\u03c0i,\u03c0\u2212i) I\u2192z = \u03c1\u03c0 i I\u2192z \u2022 1[\u03c0 \u2212i \u2208 \u03a0 \u2212i (z)], \u03c1 \u03c0 I\u2192z = \u03c1 \u03c0i I\u2192z \u2022 1[\u03c0 \u2212i \u2208 \u03a0 \u2212i (z)], we further obtain R T \u03c3 = T max \u03c0i\u2208\u03a0i(I) z\u2208Z(I) \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i\u03bc T (\u03c0) \u03c1\u03c0 i I\u2192z \u2212 \u03c1 \u03c0i I\u2192z 1[\u03c0 \u2212i \u2208 \u03a0 \u2212i (z)]p c (z)u i (z) = T max \u03c0i\u2208\u03a0i(I) z\u2208Z(I) \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i(z)\u03bc T (\u03c0) \u03c1\u03c0 i I\u2192z \u2212 \u03c1 \u03c0i I\u2192z p c (z)u i (z) = T \uf8eb \uf8ec \uf8ec \uf8ed max \u03c0i\u2208\u03a0i(I) z\u2208Z(I) \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i(z)\u03bc T (\u03c0)\u03c1\u03c0 i I\u2192z p c (z)u i (z) B \uf8f6 \uf8f7 \uf8f7 \uf8f8 \u2212 T \uf8eb \uf8ec \uf8ec \uf8ed z\u2208Z(I) \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i(z)\u03bc T (\u03c0)\u03c1 \u03c0i I\u2192z p c (z)u i (z) C \uf8f6 \uf8f7 \uf8f7 \uf8f8 .", "formula_coordinates": [16.0, 79.3, 658.79, 379.87, 64.73]}, {"formula_id": "formula_35", "formula_text": "B = max \u00b5i\u2208\u2206 \u03a0 i (I) \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 \u03c0i\u2208\u03a0i(I)\u03bc i (\u03c0 i ) \uf8eb \uf8ec \uf8ec \uf8ed z\u2208Z(I) \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i(z)\u03bc T (\u03c0)\u03c1\u03c0 i I\u2192z p c (z)u i (z) \uf8f6 \uf8f7 \uf8f7 \uf8f8 \uf8fc \uf8f4 \uf8f4 \uf8fd \uf8f4 \uf8f4 \uf8fe = max \u00b5i\u2208\u2206 \u03a0 i (I) \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 z\u2208Z(I) \uf8eb \uf8ed \u03c0i\u2208\u03a0i(I)\u03bc i (\u03c0 i )\u03c1\u03c0 i I\u2192z \uf8f6 \uf8f8 \uf8eb \uf8ec \uf8ec \uf8ed \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i(z)\u03bc T (\u03c0)p c (z)u i (z) \uf8f6 \uf8f7 \uf8f7 \uf8f8 \uf8fc \uf8f4 \uf8f4 \uf8fd \uf8f4 \uf8f4 \uf8fe . Since\u03c0 i \u2208 \u03a0 i (I) and z \u2208 Z(I), \u03c1\u03c0 i I\u2192z = 1[\u03c0 i \u2208 \u03a0 i (z)]. So, B = max \u00b5i\u2208\u2206 \u03a0 i (I) \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 z\u2208Z(I) \uf8eb \uf8ed \u03c0i\u2208\u03a0i(I)\u03bc i (\u03c0 i ) 1[\u03c0 i \u2208 \u03a0 i (z)] \uf8f6 \uf8f8 \uf8eb \uf8ec \uf8ec \uf8ed \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i(z)\u03bc T (\u03c0)p c (z)u i (z) \uf8f6 \uf8f7 \uf8f7 \uf8f8 \uf8fc \uf8f4 \uf8f4 \uf8fd \uf8f4 \uf8f4 \uf8fe = max \u00b5i\u2208\u2206 \u03a0 i (I) \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 z\u2208Z(I) \uf8eb \uf8ed \u03c0i\u2208\u03a0i(z)\u03bc i (\u03c0 i ) \uf8f6 \uf8f8 \uf8eb \uf8ec \uf8ec \uf8ed \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i(z)\u03bc T (\u03c0) \uf8f6 \uf8f7 \uf8f7 \uf8f8 p c (z)u i (z) \uf8fc \uf8f4 \uf8f4 \uf8fd \uf8f4 \uf8f4 \uf8fe = max \u00b5i\u2208\u2206 \u03a0 i (I) \uf8f1 \uf8f2 \uf8f3 z\u2208Z(I) p \u03c3 \u00b5 T ,\u03bci (z) u i (z) \uf8fc \uf8fd \uf8fe . (19", "formula_coordinates": [17.0, 102.33, 217.52, 425.87, 264.12]}, {"formula_id": "formula_36", "formula_text": ") C Since \u03c0 i \u2208 \u03a0 i (\u03c3) \u2286 \u03a0 i (I) and z \u2208 Z(I), \u03c1 \u03c0i I\u2192z = 1[z \u2208 Z(\u03c3)] \u2022 1[\u03c0 i \u2208 \u03a0 i (z)]. Therefore, C = z\u2208Z(I) \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i(z)\u03bc T (\u03c0) 1[z \u2208 Z(\u03c3)]1[\u03c0 i \u2208 \u03a0 i (z)] p c (z)u i (z) = z\u2208Z(I) \uf8eb \uf8ec \uf8ec \uf8ed 1[z \u2208 Z(\u03c3)] \u03c0i\u2208\u03a0i(\u03c3) \u03c0\u2212i\u2208\u03a0\u2212i(z)\u03bc T (\u03c0) 1[\u03c0 i \u2208 \u03a0 i (z)] p c (z)u i (z) \uf8f6 \uf8f7 \uf8f7 \uf8f8 = z\u2208Z(\u03c3) \uf8eb \uf8ec \uf8ec \uf8ed \u03c0i\u2208\u03a0i(z) \u03c0\u2212i\u2208\u03a0\u2212i(z)\u03bc T (\u03c0) \uf8f6 \uf8f7 \uf8f7 \uf8f8 p c (z)u i (z) = z\u2208Z(\u03c3) q\u03bcT (z) u i (z).(20)", "formula_coordinates": [17.0, 90.08, 458.71, 442.27, 211.09]}, {"formula_id": "formula_37", "formula_text": "R T \u03c3 T = max \u00b5i\u2208\u2206 \u03a0 i (I) \uf8f1 \uf8f2 \uf8f3 z\u2208Z(I) p \u03c3 \u00b5 T ,\u03bci (z) u i (z) \uf8fc \uf8fd \uf8fe \u2212 z\u2208Z(\u03c3) q\u03bcT (z) u i (z).(21)", "formula_coordinates": [17.0, 173.26, 688.72, 359.09, 34.54]}, {"formula_id": "formula_38", "formula_text": "= max i\u2208P max \u03c3\u2208\u03a3i R T \u03c3 T = max i\u2208P max \u03c3\u2208\u03a3i \uf8f1 \uf8f2 \uf8f3 max \u00b5i\u2208\u2206 \u03a0 i (I) \uf8f1 \uf8f2 \uf8f3 z\u2208Z(I) p \u03c3 \u00b5 T ,\u03bci (z) u i (z) \uf8fc \uf8fd \uf8fe \u2212 z\u2208Z(\u03c3) q\u03bcT (z) u i (z) \uf8fc \uf8fd \uf8fe = \u03b4(\u03bc T ).", "formula_coordinates": [18.0, 157.89, 92.77, 303.03, 78.74]}, {"formula_id": "formula_39", "formula_text": "0 \u2265 lim sup T \u2192\u221e R T \u03c3 T = lim sup T \u2192\u221e \uf8eb \uf8ed max \u00b5i\u2208\u2206 \u03a0 i (I) \uf8f1 \uf8f2 \uf8f3 z\u2208Z(I) p \u03c3 \u00b5 T ,\u03bci (z) u i (z) \uf8fc \uf8fd \uf8fe \u2212 z\u2208Z(\u03c3) q\u03bcT (z) u i (z) \uf8f6 \uf8f8 \u2265 max \u00b5i\u2208\u2206 \u03a0 i (I) \uf8f1 \uf8f2 \uf8f3 lim sup T \u2192\u221e z\u2208Z(I) p \u03c3 \u00b5 T ,\u03bci (z) u i (z) \uf8fc \uf8fd \uf8fe \u2212 lim sup T \u2192\u221e z\u2208Z(\u03c3) q\u03bcT (z) u i (z),", "formula_coordinates": [18.0, 145.57, 270.12, 320.87, 102.46]}, {"formula_id": "formula_40", "formula_text": "R T \u03c3,I = max \u03c0i\u2208\u03a0i(I) T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)] V t I (\u03c0 i ) \u2212 V t I (\u03c0 t i ) = max \u03c0i\u2208\u03a0i(I) T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)]V t I (\u03c0 i ) \u2212 T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)]V t I (\u03c0 t i ) = max \u03c0i\u2208\u03a0i(I) \uf8f1 \uf8f2 \uf8f3 T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)] \uf8eb \uf8ed u t i [I,\u03c0 i (I)] + I \u2208C(I,\u03c0i(I)) V t I (\u03c0 i ) \uf8f6 \uf8f8 \uf8fc \uf8fd \uf8fe \u2212 T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)]V t I (\u03c0 t i ) = max a\u2208A(I) \uf8f1 \uf8f2 \uf8f3 T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)]u t i [I, a] + I \u2208C(I,a) max \u03c0i\u2208\u03a0i(I ) T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)]V t I (\u03c0 i ) \uf8fc \uf8fd \uf8fe \u2212 T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)]V t I (\u03c0 t i ) = max a\u2208A(I) \uf8f1 \uf8f2 \uf8f3 T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)]u t i [I, a] + I \u2208C(I,a) R T \u03c3,I + T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)]V t I (\u03c0 t i ) \uf8fc \uf8fd \uf8fe \u2212 T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)]V t I (\u03c0 t i ),", "formula_coordinates": [18.0, 114.9, 545.77, 382.2, 177.49]}, {"formula_id": "formula_41", "formula_text": "R T \u03c3,I \u2264 max \u03c0i\u2208\u03a0i(I) I \u2208C (I) 1[\u03c0 i \u2208 \u03a0 i (I )]R T \u03c3,I .(12)", "formula_coordinates": [19.0, 213.66, 228.3, 318.69, 22.93]}, {"formula_id": "formula_42", "formula_text": "R T \u03c3,I = max a\u2208A(I) \uf8f1 \uf8f2 \uf8f3 T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)] \u00fb t I (a) \u2212\u00fb t I (\u03c0 t i (I)) + I \u2208C(I,a) R T \u03c3,I \uf8fc \uf8fd \uf8fe = max a\u2208A(I) \uf8f1 \uf8f2 \uf8f3 T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)]\u00fb t I (a) + I \u2208C(I,a) R T \u03c3,I \uf8fc \uf8fd \uf8fe \u2212 T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)]\u00fb t I (\u03c0 t i (I)) \u2264 max a\u2208A(I) T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)]\u00fb t I (a) + max a\u2208A(I) \uf8f1 \uf8f2 \uf8f3 I \u2208C(I,a) R T \u03c3,I \uf8fc \uf8fd \uf8fe \u2212 T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3)]", "formula_coordinates": [19.0, 100.44, 295.18, 371.9, 114.25]}, {"formula_id": "formula_43", "formula_text": "t i \u2208 \u03a0 i (I, a)] \u00fb t J (\u00e2) \u2212\u00fb t J (\u03c0 t i (J)) \u2265 max a\u2208A(J) \uf8f1 \uf8f2 \uf8f3 T t=1 a\u2208A(I) 1[\u03c0 t i \u2208 \u03a0 i (I, a)] \u00fb t J (\u00e2) \u2212\u00fb t J (\u03c0 t i (J)) \uf8fc \uf8fd \uf8fe = max a\u2208A(J) T t=1 1[\u03c0 t i \u2208 \u03a0 i (I)] \u00fb t J (\u00e2) \u2212\u00fb t J (\u03c0 t i (J)) = max a\u2208A(J) T t=1 1[\u03c0 t i \u2208 \u03a0 i (\u03c3(I))] \u00fb t J (\u00e2) \u2212\u00fb t J (\u03c0 t i (J))", "formula_coordinates": [19.0, 210.44, 661.14, 254.01, 62.12]}], "doi": "10.1145/3490486.3538288"}