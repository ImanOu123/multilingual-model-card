{"title": "Efficiently Sampling Functions from Gaussian Process Posteriors", "authors": "James T Wilson; Viacheslav Borovitskiy; Alexander Terenin; Peter Mostowsky; Marc Peter Deisenroth", "pub_date": "", "abstract": "Gaussian processes are the gold standard for many real-world modeling problems, especially in cases where a model's success hinges upon its ability to faithfully represent predictive uncertainty. These problems typically exist as parts of larger frameworks, wherein quantities of interest are ultimately defined by integrating over posterior distributions. These quantities are frequently intractable, motivating the use of Monte Carlo methods. Despite substantial progress in scaling up Gaussian processes to large training sets, methods for accurately generating draws from their posterior distributions still scale cubically in the number of test locations. We identify a decomposition of Gaussian processes that naturally lends itself to scalable sampling by separating out the prior from the data. Building off of this factorization, we propose an easy-to-use and general-purpose approach for fast posterior sampling, which seamlessly pairs with sparse approximations to afford scalability both during training and at test time. In a series of experiments designed to test competing sampling schemes' statistical properties and practical ramifications, we demonstrate how decoupled sample paths accurately represent Gaussian process posteriors at a fraction of the usual cost.", "sections": [{"heading": "Introduction", "text": "Gaussian processes (GPs) are a powerful framework for reasoning about unknown functions f given partial knowledge of their behaviors. In decision-making scenarios, wellcalibrated predictive uncertainty is crucial for balancing important tradeoffs, such as exploration versus exploita-Proceedings of the 37 th International Conference on Machine Learning, Online, PMLR 119, 2020. Copyright 2020 by the author(s). tion and long-term versus short-term rewards. Bayesian methods naturally strike this balance (Ghavamzadeh et al., 2015;Shahriari et al., 2015). While many quantities of interest defined with respect to Bayesian posteriors cannot be computed analytically (such as expectations of nonlinear functionals), they may be readily estimated via Monte Carlo methods. Depending on this sample-based estimator's relative cost and statistical behavior, its performance may vary from state-of-the-art to method-of-last-resort.\nUnlike methods for scalable training (Hensman et al., 2013;Wang et al., 2019), techniques for efficiently sampling from GP posteriors have received relatively little attention in the machine learning literature. On the one hand, na\u00efve approaches to sampling are statistically well-behaved, but scale poorly owing to their need to solve for increasingly large linear systems at test time. On the other hand, fast approximation strategies using Fourier features (Rahimi and Recht, 2008) avoid costly matrix operations, but are prone to misrepresenting predictive posteriors (Wang et al., 2018;Mutny and Krause, 2018;Calandriello et al., 2019). Investigating their respective behaviors, we find that many of these strategies are complementary, with one often excelling where others falter. Motivated by this comparison of strengths and weaknesses, we leverage a lesser known decomposition of GP posteriors that allows us to incorporate the best of both worlds.\nOur approach centers on the observation that we may implicitly condition Gaussian random variables by combining them with an explicit corrective term. Translating this intuition to GPs, we may decompose the posterior as the sum of a prior and an update. By doing so, we are able to separately represent each of these terms using a basis well-suited for sampling. This notion of \"conditioning by kriging\" was first presented by Matheron in the early 1970s, with various applications to geostatistics (Journel and Huijbregts, 1978;de Fouquet, 1994;Chiles and Delfiner, 2009). The concept was later rediscovered in astrophysics (Hoffman and Ribak, 1991;Van de Weygaert and Bertschinger, 1996), where it has been used to help simulate the universe as we know it.\nWe unite these ideas with techniques from the growing literature on approximate GPs to obtain an easy-to-use and general-purpose approach for accurately sampling from GP posteriors in linear time.\narXiv:2002.09309v4 [stat.ML] 16 Aug 2020", "publication_ref": ["b11", "b39", "b14", "b47", "b30", "b48", "b25", "b2", "b20", "b4", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Review of Gaussian processes", "text": "As notation, let f : X \u2192 R be an unknown function with domain X \u2286 R d whose behavior is indicated by a training set consisting of n Gaussian observations y i = f (x i ) + \u03b5 i subject to measurement noise \u03b5 i \u223c N (0, \u03c3 2 ).\nA Gaussian process is a random function f : X \u2192 R such that, for any finite set of locations X * \u2286 X , the random vector f * = f (X * ) follows a Gaussian distribution. In particular, if f \u223c GP(\u00b5, k), then f * \u223c N (\u00b5 * , K * , * ) is multivariate normal with covariance K * , * = k(X * , X * ) specified by a kernel k. Henceforth, we assume a zeromean prior \u00b5(\u2022) = 0 and continuous, stationary covariance function k(x, x ) = k(x \u2212 x ).\nGiven n observations y, the GP posterior at X * is defined as f * | y \u223c N (m * |n , K * , * |n ), where we have defined m * |n = K * ,n (K n,n + \u03c3 2 I) \u22121 y K * , * |n = K * , * \u2212 K * ,n (K n,n + \u03c3 2 I) \u22121 K n, * .\n(1)\nWhen using (1) to help guide reinforcement learning agents (Kuss and Rasmussen, 2004), black-box optimizers (Snoek et al., 2012), and other complex algorithms, we often rely on samples to estimate quantities of interest. The standard way of generating these samples is via a location-scale transform of Gaussian random variables \u03b6 \u223c N (0, I), namely\nf * | y = m * |n + K 1/2 * , * |n \u03b6,(2)\nwhere (\u2022) 1/2 denotes a matrix square root, such as a Cholesky factor. Since this scheme is exact up to numerical error, we take it to be the gold standard against which the sample quality of alternatives will be judged. Unfortunately, this sampling strategy is also one of the least scalable, since the cost of computing K 1/2 * , * |n is already O( * 3 ). The first column of Figure 1 visualizes sampling from a GP posterior given varying amounts of training data n. Since matrices on the right-hand-side of (1) grow as training sets increases in size, this method of sampling can be seen to accumulate little to no error as n increases. However, this growth requires us to invert increasingly large matrices both during training and at test time, causing standard GP inference and sampling methods to scale poorly in n.", "publication_ref": ["b23", "b42"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Function-space approximations to GPs", "text": "The preceding interpretation of GPs, as distributions over functions with Gaussian marginals, is commonly known as the function-space view (Rasmussen and Williams, 2006). From this perspective, a natural way of approximating GPs is to represent f in terms of its behavior u = f (Z) at a carefully chosen set of inducing locations Z = {z 1 , ..., z m }. In line with this function-space intuition of reasoning about f via a small set of locations, this family of approximations is commonly referred to as sparse Gaussian processes.\nRather than directly conditioning on observations y, sparse GPs begin by defining inducing distributions q(u) that explain for the data. Over the years, distinct iterations of sparse GPs have proposed different inducing paradigms (Snelson and Ghahramani, 2006;Titsias, 2009;Hensman et al., 2017). In this work, we remain agnostic regarding the choice of q(u) and simply assume access to draws u \u223c q(u).\nGiven q(u), we approximate posterior distributions as\np(f * | y) \u2248 R m p(f * | u)q(u) du.\n(3)\nIf u \u223c N (\u00b5 u , \u03a3 u ), we compute this integral analytically to obtain a Gaussian distribution with mean and covariance\nm * |m = K * ,m K \u22121 m,m \u00b5 m K * , * |m = K * , * +K * ,m K \u22121 m,m (\u03a3 u \u2212K m,m )K \u22121 m,m K m, * .(4)\nBy virtue of explaining for n observations using m inducing variables, sparse GPs can be trained with O(\u00f1m 2 ) time complexity, where the choice of batch size 1 \u2264\u00f1 \u2264 n depends on the particular algorithm. Since high-quality approximations can be constructed using m n (Burt et al., 2019), sparse GPs drastically improve upon their exact counterparts' O(n 3 ) scaling.\nWhile posterior moments (4) may be computed at reduced cost, this benefit does not carry over when sampling. The standard procedure for sampling from sparse GPs is the same as in (2) and incurs O( * 3 ) cost. 1 When used to drive Monte-Carlo-based algorithms, sparse GPs can therefore be fast during training but slow during deployment. The middle column of Figure 1 depicts samples from a sparse GP posterior with m = 8 inducing locations.", "publication_ref": ["b31", "b41", "b45", "b15", "b1"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Weight-space approximations to GPs", "text": "In the function-space view of GPs, we reason about f in terms of the values it may assume at locations x \u2208 X . We now turn to the weight-space view, where we will reason about f as a weighted sum of basis functions. Per the kernel trick (Sch\u00f6lkopf and Smola, 2001), k can be viewed as the inner product in a reproducing kernel Hilbert space (RKHS) H equipped with a feature map \u03d5 : X \u2192 H. If H is separable, we may approximate this inner product as\nk(x, x ) = \u03d5(x), \u03d5(x ) H \u2248 \u03c6(x) \u03c6(x),(5)\nwhere \u03c6 : X \u2192 R is a finite-dimensional feature map (Rasmussen and Williams, 2006). For stationary covariance functions, Bochner's theorem implies that a suitable -dimensional feature map can be constructed via a set of random Fourier features (RFF) (Rahimi and Recht, 2008).\nIn this case, we have \u03c6 i (x) = 2 / cos(\u03b8 i x + \u03c4 i ), where \u03b8 i are sampled proportional to the kernel's spectral density and \u03c4 j \u223c U (0, 2\u03c0). By defining the Bayesian linear model\nf (\u2022) = i=1 w i \u03c6 i (\u2022) w i \u223c N (0, 1),(6)\nwe obtain an -dimensional GP approximation. As in previous sections, f is now a random function with Gaussian marginals. However, this stochasticity is now entirely controlled by the distribution of weights w.\nFor Gaussian likelihoods, the posterior weight distribution w | y \u223c N (\u00b5 w|n , \u03a3 w|n ) is Gaussian with moments\n\u00b5 w|n = (\u03a6 \u03a6 + \u03c3 2 I) \u22121 \u03a6 y \u03a3 w|n = (\u03a6 \u03a6 + \u03c3 2 I) \u22121 \u03c3 2 ,(7)\nwhere \u03a6 = \u03c6(X) is an n \u00d7 feature matrix. In both cases, we may solve for the right-hand side at O(min{ , n} 3 ) cost by applying the Woodbury matrix identity.\nApproximating the posterior f | y as weighted sums of basis functions in (6) is particularly advantageous for purposes of sampling. As before, we may generate draws from (7) by first computing \u03a3 1/2 w|n at O( 3 ) cost. 2 Unlike before, we now sample weight vectors rather than function values and each draw now defines an actual function evaluable at arbitrary locations x \u2208 X . These methods have recently attracted attention in Bayesian optimization (Hern\u00e1ndez-Lobato et al., 2014;Shahriari et al., 2015), where the ability to finetune test locations X * by differentiating through samples is particularly valuable .\nUnfortunately, these efficiency gains are counterbalanced by loss in expressivity. GP approximations equipped with covariance functions arising from finite-dimensional feature maps are well-known to exhibit undesirable pathologies at test time, see Rasmussen and Quinonero-Candela (2005). In the case of Fourier-feature-based approximations, this tendency manifests as variance starvation, whereby their extrapolatory predictions become increasingly ill-behaved as n increases (Wang et al., 2018;Mutny and Krause, 2018;Calandriello et al., 2019). Intuitively, this occurs because the Fourier basis is only efficient at representing stationary GPs. The posterior, however, is generally nonstationary. This tendency is evident in the right column of Figure 1: samples from the posterior clearly deteriorate in quality as we transition from low to high-data regimes.\nMotivation. Prior to presenting our primary contributions, we briefly pause to restate key trends discussed above and shown in Figure 1. Sampling from sparse GPs accommodates large amounts of training data n = |X|, but scales poorly with the number of test locations * = |X * |. Conversely, sampling from Fourier-feature-based weight-space approximations scales gracefully with * , but results in high approximation error as n increases. Function-and weightspace approaches to sampling from GP posteriors therefore exhibit opposing strengths and weaknesses.\nHence, the question: can we obtain the best of both worlds?", "publication_ref": ["b36", "b31", "b30", "b17", "b39", "b32", "b48", "b25", "b2"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "Sampling with Matheron's rule", "text": "Our approach to designing an improved sampling scheme, which doubles as a rough outline for this section, is as follows: (i) analyze the shortcomings of existing methods, (ii) identify a decomposition of GPs that isolates these issues, (iii) represent the different terms using bases that address their corresponding issues. We begin by reviewing Matheron's rule for Gaussian random variables (Journel and Huijbregts, 1978;Chiles and Delfiner, 2009;Doucet, 2010), which is central to our analysis.\nTheorem 1 (Matheron's Rule). Let a and b be jointly Gaussian random variables. Then the random variable a conditional on b = \u03b2 is equal in distribution to\n(a | b = \u03b2) d = a + Cov(a, b) Cov(b, b) \u22121 (\u03b2 \u2212 b). (8)\nProof. Follows immediately by computing the mean and covariance of both sides.\nIntuitively, Matheron's rule tells us that conditional random variable a | b can be broken up into a term representing the prior p(a, b) and a term that communicates the error in the prior upon observing that b = \u03b2. Hence, we may sample a | b by drawing a and b together from the prior and, subsequently, updating a to account for residuals \u03b2 \u2212 b as in ( 8). The corresponding statement for GPs is as follows.\nCorollary 2. For a Gaussian process f \u223c GP(0, k) with marginal f m = f (Z), the process conditioned on f m = u admits, in distribution, the representation\n(f | u)(\u2022) posterior d = f (\u2022) prior + k(\u2022, Z)K \u22121 m,m (u \u2212 f m ) update .(9)\nProof. By Theorem 1, the corollary holds for arbitrary finitedimensional marginals, so the claim follows.\nUnlike ( 1) and ( 4), Corollary 2 defines a pathwise update: rather than conditioning the prior as a distribution, we update the prior as realized in terms of sample paths. As we will soon see, this ability to go from prior to posterior (function) draws without needing to compute posterior covariance matrices (and their square-roots) will be the key to unlocking fast and accurate sampling from GP posteriors.\nWe are not the first to have realized this fact. This approach to simulating Gaussian conditionals is implicit in Matheron's pioneering work in the field of geostatistics, where it was subsequently popularized by Journel and Huijbregts (1978). Decades later, ( 9) was rediscovered in the context of N -body simulations by Hoffman and Ribak (1991). We combine these ideas with modern machine learning methods (such as sparse GPs and random Fourier features) to create a more efficient approach to sampling.", "publication_ref": ["b20", "b4", "b9", "b20", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Pathwise updates in weight-and function-spaces", "text": "Rewriting the standard formulae for sparse and exact GP posteriors, respectively, as pathwise updates in accordance with Theorem 1, we obtain\nf * | u d = f * + K * ,m K \u22121 m,m (u \u2212 f m )(10)\nf * | y d = f * + K * ,n (K n,n + \u03c3 2 I) \u22121 (y \u2212 f \u2212 \u03b5). (11\n)\nWhen sampling from sparse GPs in ( 10), we draw f * and f m together from the prior, and independently generate target values u \u223c q(u). When sampling from exact GPs in ( 11), we again begin by jointly drawing f * and f from the prior. Here however, we no longer need to generate targets u = y. Instead, we combine f with noise variates \u03b5 \u223c N (0, \u03c3 2 I) such that f + \u03b5 constitutes a draw from the prior distribution of y.\nTurning to the weight-space setting, the analogous pathwise update given an initial weight vector w \u223c N (0, I) is then\nw | y d = w + \u03a6 (\u03a6\u03a6 + \u03c3 2 I) \u22121 (y \u2212 \u03a6w \u2212 \u03b5). (12\n)\nAt first glance, it appears that sampling via Theorem 1 does not improve over standard methods. Whereas ( 12) is of modest practical interest (it allows us to sample at O(min{ , n} 3 ) cost without resorting to an eigendecomposition), ( 10) and ( 11) are actually more expensive than their standard counterparts.\nAt the same time, however, Theorem 1 allows us to view GP posteriors from a different perspective. In particular, separating the effect of the prior from that of the data allows us to better diagnose the different sampling scheme's shortcomings. For function-space approaches, we see that O( * 3 ) time complexity is specific to the prior, since the update is linear in * . For weight-space methods, we see that erratic extrapolations stem from difficulty representing the data (i.e., the update), since stationary priors are well-behaved under the Fourier basis. Equipped with a better understanding of why these methods fail, we now demonstrate how to address these issues.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Pathwise updates with decoupled bases", "text": "So far, we have implicitly assumed a unified view of GP posteriors: when sampling in weight-space and in functionspace, we sought to generate draws from conditional distributions over weight vectors and function values, respectively. Several recent works (Cheng and Boots, 2017;Salimbeni et al., 2018;Shi et al., 2020) have introduced decompositions that separately represent different aspects of GPs via different bases, such as RKHS subspaces and their orthogonal complements. There, the authors exploit the different bases' properties to better approximate the overarching process. We will do the same, but our goal will be to efficiently sample from the accompanying posteriors. Corollary 2 is a pathwise update for Gaussian random variables that doubles as a decomposition of the posterior. To further build on this distinction, we restate this using a weight-space approximation to the prior\n(f | u)(\u2022) sparse posterior d \u2248 i=1 w i \u03c6 i (\u2022) weight-space prior + m j=1 v j k(\u2022, z j ), function-space update (13\n)\nwhere we have defined v = K \u22121 m,m (u \u2212 \u03a6w). The equivalent expression for exact GPs with Gaussian observations is obtained by adding noise \u03b5 \u223c N (0, \u03c3 2 I) to \u03a6w and replacing Z, u, and K \u22121 m,m with X, y, and (K n,n + \u03c3 2 I) \u22121 . Figure 2 acts as a visual guide for decoupled sampling, showing the progression from prior (6) to posterior (13).\nStepping through this example: (i) we draw a function f from an approximate prior, (ii) we construct an update function to account for the residuals u \u2212 f (Z) produced by an independent sample u \u223c q(u), (iii) we add these functions together to obtain a function drawn from an approximate posterior (13) that we may freely evaluate anywhere in X .\nIn (13), we obtain an efficient approximator by separately discretizing the prior using Fourier basis functions \u03c6 i (\u2022) and the update using canonical basis functions k(\u2022, z j ). While other decompositions exist (see Appendix A), this particular decoupling directly capitalizes upon each basis' strengths: the Fourier basis is well-suited for representing the prior (Rahimi and Recht, 2008) and the canonical basis is wellsuited for representing the data (Burt et al., 2019).\nBy combining these bases as in ( 13), we therefore inherit the best of both worlds. As in weight-space methods, we may efficiently approximate draws from the prior using an -dimensional Bayesian linear model f (\u2022) = \u03c6(\u2022) w, where weights w are standard normal (owing to the assumed stationarity of kernel k). 3 As in function-space methods, we may faithfully represent the data since basis functions k(\u2022, z j ) are in one-to-one correspondence with inducing locations z j \u2208 Z. This retention of statistical propriety is evident on the right-hand side of Figure 2: despite using half as many basis functions as the weight-space method (see Figure 1), decoupled sampling's statistical properties mirror those of the gold standard.\nExpanding upon these properties, we note the following intuitive behaviors. The update function's role of \"correcting\" for residuals u \u2212 f (Z) subsumes that of representing the posterior mean: replacing the prior draw f with the prior mean E[f ] reduces (13) to the standard expression for the conditional expectation E[f | u]. Since this task is performed in the canonical basis, the expected value of decoupled sample paths is guaranteed to coincide with that of (sparse) GP's posterior. As a result, decoupled sampling becomes increasingly well-behaved as the number of training (inducing) locations grows and uncertainty decreases. Conversely, we are guaranteed to revert to the prior as we move away from the data, assuming local basis functions k(\u2022, z) (see the center column of Figure 2). Decoupled sampling complements these desiderata with function draws' inherent strengths. The immediate implication here is that decoupled sampling scales linearly with respect to the number of test locations X * . A more subtle point is that these functions are pathwise differentiable with respect to x-an affordance with significant consequences when seeking to understand Gaussian processes' extrema. While these insights tell us about decoupled sampling's qualitative behavior, they do not allow us to make quantitative statements about its purported benefits. To this end, the following section provides a means of objectively comparing different sampling schemes' statistical properties.", "publication_ref": ["b3", "b35", "b40", "b30", "b1"], "figure_ref": ["fig_1", "fig_1", "fig_0", "fig_1"], "table_ref": []}, {"heading": "Error bounds", "text": "Due to its use of an approximate prior, decoupled samplingintroduces an additional source of error at test time. Anecdotal evidence (see Figure 2) suggests that this sampling error is often small in comparison to the error introduced by inducing point approximations. Here, we study decoupled sampling's analytic properties to clarify how quality of the approximate prior impacts that of decoupled function draws. We present the results of this analysis below, and reserve proofs and derivations of associated constants for Appendix B. As a convenient shorthand, we refer to the particular decoupled sparse GP approximation introduced in (13) as DSGP.\nGaussian processes are often compared via a suitable notion of similarity on the space of probability distributions (Gibbs and Su, 2002). We focus on the 2-Wasserstein distance between GPs (Mallasto and Feragen, 2017). By their Kantorovich dual formulations (Peyr\u00e9 and Cuturi, 2019), Wasserstein distances upper bound the error for integrating (Lipschitz) continuous functions with respect to approximating distributions, making them a natural performance metric in Monte Carlo settings. Moreover, 2-Wasserstein distances between exact GPs and finite-dimensional approximations thereof are finite and thus facilitate meaningful performance comparisons. For DSGP, we may bound these as follows. Proposition 3. Assume that X \u2286 R d is compact and that stationary kernel k is sufficiently regular for f \u223c GP(0, k) to be almost surely continuous. Let f | y be the posterior of f , f (s) that of a sparse GP, and f (d) that of a corresponding DSGP defined via an approximate prior f (w) . Then we have\nW 2,L 2 (X ) f (d) , f | y \u2264 W 2,L 2 (X ) f (s) , f | y error in the (sparse) posterior + C 1 W 2,C(X ) f (w) , f error in the prior ,(14)\nwhere W 2,L 2 (X ) and W 2,C(X ) are the 2-Wasserstein distances over L 2 (X ) and the space of continuous functions C(X ) equipped with the supremum norm, respectively.\nProof. Appendix B.\nThis bound tells us that the error exhibited by DSGP sample paths cleanly separates into independent terms associated with the sparse GP and the approximate prior. In particular, the way in which error in the prior carries over to the posterior is controlled by the inducing locations Z, which C 1 depends on, but not by the inducing distribution q(u).\nWe continue this analysis by studying DSGP's moments. Since a DSGP's mean is guaranteed to coincide with that of a sparse GP, we focus on the error it introduces into the posterior covariance. When using RFF to approximate the prior, this error will depend on the -dimensional basis \u03c6 given by parameters \u03c4 \u223c U (0, 2\u03c0) and \u03b8 \u223c s(\u03b8), where s(\u2022) denotes the (normalized) spectral density of k. We therefore bound the expectation of this error. d) respectively denote the covariance functions of processes f | y, f (w) , f (s) , f (d) . Denoting the supremum norm over continuous functions by \u2022 C(X 2 ) , it follows that\nProposition 4. Continuing from Proposition 3, let k (f |y) , k (w) , k (s) , k(\nE \u03c6 k (d) \u2212 k (f |y) C(X 2 ) \u2264 k (s) \u2212 k (f |y) C(X 2 ) + C 2 C 3 \u221a ,(15)\nwhere the constants C 2 and C 3 are given by Sutherland and Schneider (2015) and in Appendix B, respectively.\nProof. Appendix B.\nFigure 4: Median performances and interquartile ranges of parallel Thompson Sampling (TS) and popular baselines when optimizing d-dimensional functions drawn from GP priors. Function-space TS delivers competitive performance for d = 2, but is held back by its inability to efficiently utilize gradient information to combat the curse of dimensionality. RFF-based TS avoids this issue but requires b m basis functions to perform well. TS with decoupling sampling matches or outperforms competing approaches in all observed cases. See Appendix C.2 for additional results and runtime distributions.\nMuch like DSGPs themselves, the error in the posterior covariance separates into terms associated with the covariance of the sparse GP k (s) and approximate prior k (w) . This latter source of error represents discrepancies introduced by using RFF to approximate the prior and decays at a dimension-free rate as the number of basis functions increases. Intuitively, this behavior reflects RFF's nature as a Monte Carlo estimator of the true covariance. In practice, the number of training points n typically grows faster than the dimensionality d. Hence, purely RFF-based GP posteriors struggle to capitalize upon this property due to variance starvation. Since DSGP does not exhibit this pathology, it fully benefits from this dimension-free rate of convergence.", "publication_ref": ["b12", "b24", "b27", "b43"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Experiments", "text": "We investigate decoupled sampling's behavior in a series of sample tests accompanied by two practical applications, Thompson sampling and dynamical system simulation. Each of these experiments highlights different properties of decoupled sample paths: uncertainty calibration, reliability and differentiability, and computational savings. 4\nUncertainty calibration with the 2-Wasserstein distance.\nTo better understand how the bounds presented in Section 3.3 manifest in the real world, we put the various sampling schemes through numerical experiments that empirically estimated the 2-Wasserstein distance bounded by ( 14). These tests allow us to see how this distance is affected by factors, such as the number of training points, whose effects are difficult to directly analyze. In each trial, we measured the distance between the true posterior and empirical distributions of samples generated using the various strategies introduced in the paper. To eliminate confounding variables, experiments were run using exact GPs with known hyperparameters (see Appendix C for details).\nOur investigation focuses on each method's behavior as the number of inducing locations m (equivalently, the number of training points n) increases relative to the number of basis functions employed. For fair comparison, the total number of basis functions b = m + utilized by weight-space and decoupled samplers was held equal, where denotes an initial allocation. For decoupled sampling, specifies the number of Fourier features used to approximate the prior.\nFigure 3 shows that weight-space sampling tends to deteriorate as m increases relative to b. Variance starvation causes sample paths' extrapolatory behavior to increasingly misrepresent the posterior. This issue is exacerbated as dimensionality d rises, since we can expect the (randomly chosen) test locations X * to lie further and further away from the data.\nIn contrast, decoupled sampling retains its performance, and may even improve. This reflects the fact that the data is represented in an efficient basis that grows alongside it. For sparse GPs with m \u2265 n (which includes exact GPs as a special case), we may always represent the data exactly: usually, however, m n inducing locations (i.e., kernel basis functions) suffice (Burt et al., 2019). Since we expect posteriors to contract as training sets expand, the functions drawn from these posteriors behave increasingly similar to their means. Since decoupled sample paths are guaranteed to exhibit the correct means, their statistical properties may improve. This process occurs more slowly in higherdimensional cases. However, since away from data these function draws revert to the approximate prior, they exhibit constant error when extrapolating-the approximation error of said prior.\nThompson Sampling with reliable, differentiable draws. Thompson Sampling (TS) is a classic strategy for decisionmaking in the face of uncertainty, whereby a choice x \u2208 X is selected according to its estimated probability of being Figure 5: Sparse GP-based simulation of a FitzHugh-Nagumo model neuron subject to evolution noise \u03b5 t \u223c N (0, 10 \u22122 I) and current injection I(t) \u2208 R. Left: True drift function f given a fixed current I(t) = 0.5. Middle: Medians and interquartile ranges of 1000 voltage traces generated in response to a sinusoidal control signal (dashed black) using iterative (orange) and decoupled (blue) sampling are compared with those of ground truth simulations (gray). Upper right: Runtime comparison of iterative and decoupled sampling: the former scales cubically, while the latter runs in linear time. Lower right: 2-Wasserstein distances between state distributions at times t are approximated using the Sinkhorn algorithm (Cuturi, 2013). The noise-floor (gray) was established using additional ground truth simulations.\noptimal (Thompson, 1933). When used as a vehicle for GP-based optimization, TS evaluates a pathwise minimizer\nx n+1 \u2208 arg min x\u2208X (f | y)(x)(16)\nof a function drawn f | y from the posterior. Upon finding this minimizer, x n+1 is evaluated to obtain y n+1 , the pair (x n+1 , y n+1 ) is added to the training set, and the process repeats. In practice, this algorithm is (embarrassingly) parallelized by independently drawing \u03ba > 1 functions and evaluating a minimizer of each one (Hern\u00e1ndez-Lobato et al., 2017;Kandasamy et al., 2018).\nWe compare the performance of parallel TS equipped with the various sampling schemes discussed in Section 3, along with two common baselines. To help eliminate confounding variables, experiments were run using functions drawn from known GP priors with fixed measurement noise y i \u223c N (f i , 10 \u22123 ). Across trials, we varied both the dimensionality d of search spaces X = [0, 1] d and the number of initial basis functions . We set \u03ba = d, but this choice was not found to greatly influence results. The total number of basis functions allocated to weight-space and decoupled samplers was again matched, so that b = m + .\nFigure 4 shows that different methods of sampling from GP posteriors dramatically influence achieved performance. While all methods suffered from the curse of dimensionality, TS in function-space deteriorates most aggressively, owing to its inability to efficiently exploit gradient information and to the prohibitive cost for generating large sample vectors f * | y. Weight-space TS resolves both of these issues and, therefore, performs competitively-so long as b m, in which case it accurately approximates the posterior. On the other hand, TS in weight-space collapses due to variance starvation as m increases relative to b, often performing worse than simpler alternatives. Decoupled sampling avoids these shortcomings. As function draws, decoupled sample paths (f | y)(X * ) boast linear time complexity O( * ) and can be minimized by pathwise differentiating with respect to X * . Moreover, because the canonical basis is able to efficiently represent the data, these sample paths retain their statistical properties even when b is comparable to n or, in the case of sparse GPs, when b n.\nSimulating dynamical systems in linear time. Modelbased simulators are commonly used in cases where realworld data collection proves impractical or impossible. For example, GP surrogates are a key component of state-ofthe-art methods for solving the types of continuous control problems seen in robotics (Deisenroth et al., 2015;Kamthe and Deisenroth, 2018). Without loss of generality, we assume that our goal is to model a time-invariant system whose dynamics are governed by a stochastic differential equation, discretized according to the the Euler-Maruyama integrator\n\u2206s t = s t+1 \u2212 s t = f (s t , c t )\u2206t + \u221a \u2206t\u03a3\u03b5 t ,(17)\nwhere s t denotes the state at time t, c t \u2208 U \u2286 R c a control input, and \u03b5 t \u223c N (0, I) a standard normal random vector.\nHaving trained a (sparse) GP to represent possible drift functions f , we simulate the system's evolution over time by unrolling: given a state-control pair (s t , c t ), we sample a transition \u2206s t according to the GP posterior and step as in ( 17). Since the resulting trajectory s 1:t is determined online, standard approaches to sampling require us to itera-tively condition on the preceding sample f t when drawing f t+1 | f 1:t . Use of caching and rank-1 downdates help limit associated costs however, the resulting algorithm's time complexity still scales cubically in the number of steps t (see Appendix C.3). By virtue of drawing functions, decoupled sampling avoids this machinery and allows us to simulate trajectories in linear time O(t).\nTo better understand the practical ramifications of unrolling with decoupled samples, we used a sparse GP to simulate the dynamics of a well-known model of a biological neuron (FitzHugh, 1961;Nagumo et al., 1962). Results are shown in Figure 5. For both sampling schemes, simulated trajectories accurately characterizes the ways in which the system may respond to a given control signal. Their respective costs, however, vary dramatically: simulations that required 10 hours using the iterative approach, owing to cubic costs, ran in 20 seconds using decoupled sampling while achieving similar accuracy.", "publication_ref": ["b1", "b5", "b44", "b16", "b22", "b7", "b21", "b10", "b26"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Conclusion", "text": "Decomposing Gaussian processes is a general strategy for constructing efficient approximation schemes. We have focused on a particular case, where a posterior is seen as the sum of a prior and an update, and shown how this decoupling can be exploited to efficiently draw functions from said posterior. Even within this choice of decomposition however, optimal treatment of these components will ultimately depend upon the nature of the task at hand. For example, when working with structured covariance matrices, it is sometimes possible to efficiently generate draws from the prior without introducing approximation error (Dietrich and Newsam, 1997;Wilson and Nickisch, 2015). These alternatives can then be combined with ideas discussed in previous sections to achieve the desired balance of speed versus accuracy.\nOwing to the generality of our assumptions and simplicity of our proposals, decoupled sampling can be used as a plugin extension to existing sample-based algorithms driven by (sparse) GPs. Separately representing the prior and the data with bases better suited for sampling allows us to obtain the \"best of both worlds\" by bringing together previous methods' strengths. The result of this union, decoupled sampling, draws functions from GPs that may be evaluated in linear time without fear of misrepresenting their posteriors.", "publication_ref": ["b8", "b49"], "figure_ref": [], "table_ref": []}, {"heading": "A. Alternative decompositions", "text": "As mentioned in the Section 3.2, the proposed representation of the GP posteriors-as the sum of a weight-space prior and a function-space update-is one of many possible choices. Here, we briefly reflect on two such alternatives.\nTo begin with, we may directly represent sparse GP posteriors in weight-space via a Bayesian linear model f (\u2022) = \u03c6(\u2022) w.\nTo this end, we may rewrite (12) for a given draw u \u223c q(u) as\nw | u d = w + \u03a6 (\u03a6\u03a6 ) \u22121 (u \u2212 \u03a6w),(18)\nwhere \u03a6 = \u03c6(Z) now denotes an m \u00d7 feature matrix. Prima facie, this appears to resolve many of the problems discussed earlier in the text: inducing distribution q(u) relays information about y and the Bayesian linear model needs only explain for the function's behavior at m n locations. In practice, ( 18) does more harm than good however, since f must now exactly pass through u due to a lack of measurement noise \u03c3 2 .\nAlternatively, we may think to employ an orthogonal decomposition f (\u2022) = f (\u2022) + f \u22a5 (\u2022) (Salimbeni et al., 2018;Shi et al., 2020). Here, we interpret \"orthogonality\" in the statistical sense of independent random variables (Rodgers et al., 1984). For Gaussian random variables, this distinction amounts to satisfying the definition Cov(f , f \u22a5 ) = 0. In the case of sparse GPs, f is typically represented in terms of canonical basis functions k(\u2022, Z) such that (f | u)(\u2022) denotes the posterior mean function given q(u). Consequently, f \u22a5 denotes the process residuals (f\n\u22a5 | u)(\u2022) = (f | u)(\u2022)\u2212(f | u)(\u2022)\n. By construction however, f \u22a5 is independent of f and, hence, of particular values u. Moreover, since (\nf | u)(Z) = (f | u)(Z) = u, it follows that f \u22a5 (Z) = (f \u22a5 | u)(Z) = 0.\nGenerating draws from this type of decomposition is made difficult by orthogonal component f \u22a5 | u, whose covariance can readily be shown as\nCov(f \u22a5 , f \u22a5 ) = k(\u2022, \u2022) \u2212 k(\u2022, Z)K \u22121 m,m k(Z, \u2022). (19\n)\nSampling schemes based on random Fourier feature approximations of f \u22a5 are nearly identical to (18): all that has changed is that the Bayesian linear model must now pass exactly through zero, rather than u, at each of the m inducing locations. This approach to sampling therefore inherits the issues outlined above.", "publication_ref": ["b35", "b40", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "B. Error analysis", "text": "Definition 5 (Preliminaries). Consider a Gaussian process f defined on R d and restricted to a compact subset X \u2286 R d . Let y \u2208 R n . Assume a Gaussian likelihood y i \u223c N (f (x i ), \u03c3 2 ), with \u03c3 2 \u2265 0. Let f (w) be a weight-space prior approximation. Let f | y be the true posterior, let f (s) be an inducing point approximate posterior, and let f (d) be the decoupled posterior approximation. Let k, k (w) , k (f |y) , k (s) , k (d) be their respective kernels.\nProposition 6. We have that\nW 2,L 2 (X ) f (d) , f | y \u2264 W 2,L 2 (X ) f (s) , f | y + C 1 W 2,L \u221e (X ) f (w) , f(20)\nwhere\nC 1 = 2 diam(X ) d 1 + k 2 C(X 2 ) K \u22121 mm 2 L( \u221e ; 1 ) , W 2,L 2 (X )\nand W 2,C(X ) are the 2-Wasserstein distances over L 2 (X ) and the space of continuous functions C(X ) equipped with the supremum norm, respectively, and \u2022 L( \u221e ; 1 ) is the corresponding operator norm of a matrix.\nProof. By the triangle inequality, we have\nW 2,L 2 (X ) f (d) , f | y \u2264 W 2,L 2 (X ) f (d) , f (s) + W 2,L 2 (X ) f (s) , f | y . (21\n)\nWe proceed bound the first term pathwise. For arbitrary x \u2208 M , write\nf (d) (x) \u2212 f (s) (x) 2 \u2264 2 f (w) (x) \u2212 f (x) 2 + K xm K \u22121 mm (f (w) (z) \u2212 f (z)) 2 (22) \u2264 2 f (w) \u2212 f 2 L \u221e (X ) + K xm K \u22121 mm 2 1 f (w) (z) \u2212 f (z) 2 \u221e (23) \u2264 2 f (w) \u2212 f 2 L \u221e (X ) + K xm 2 \u221e K \u22121 mm 2 L( \u221e ; 1 ) f (w) \u2212 f 2 L \u221e (X )(24)\n\u2264 2 1 + k 2 C(X 2 ) K \u22121 mm 2 L( \u221e ; 1 ) f (w) \u2212 f 2 L \u221e (X )(25)\n= 2 1 + k 2 C(X 2 ) K \u22121 mm 2 L( \u221e ; 1 ) f (w) \u2212 f 2 C(X )(26)\nwhere in ( 22) we have used Matheron's rule, in ( 23) we have used H\u00f6lder's inequality with p = 1, q = \u221e, in ( 24) we have used the definition of an operator norm, and in ( 26) we have used that given sample paths are continuous so \u2022 L \u221e (X ) can be replaced with \u2022 C(X ) . We now lift this to a bound on the Wasserstein distance by integrating both sides. With \u03b3 \u2208 C denoting couplings between GP(0, k) and GP(0, k (w) ), write\nW 2 2,L 2 (X ) (f (d) , f (s) ) \u2264 inf \u03b3\u2208C f (d) \u2212 f (s) 2 L 2 (X ) d\u03b3 (27) \u2264 C|X | inf \u03b3\u2208C f (w) \u2212 f 2 C(X ) d\u03b3 (28) = C diam(X ) d W 2 2,C(X ) (f (w) , f ) (29\n)\nwhere C is the constant above. Finally, note that f is sample-continuous, and C(X ) is a separable metric space, so W 2,C(X ) is a proper metric. The claim follows.\nProposition 7. Assume k is stationary continuous covariance defined on R d \u00d7 R d , X \u2286 R d is compact. We have that\nE \u03c9\u223c\u03c1 \u03c5\u223cU k (d) \u2212 k (f |y) C(X 2 ) \u2264 k (s) \u2212 k (f |y) C(X 2 ) + C 2 C 3 \u221a (30)\nwhere \u2022 C(X 2 ) is the supremum norm over continuous functions, C 2 is the constant given by Sutherland and Schneider (2015), which depends only on the Lipschitz constant of k, the rate of decay of the spectral density \u03c1, the dimension d, and the diameter of the domain X , and\nC 3 = m 1 + K \u22121 m,m C(X 2 ) k C(X 2 ) 2 .\nProof. By the triangle inequality, we have\nE \u03c9\u223c\u03c1 \u03c5\u223cU k (d) \u2212 k f |y C(X 2 ) \u2264 E \u03c9\u223c\u03c1 \u03c5\u223cU k (d) \u2212 k (s) C(X 2 ) + k (s) \u2212 k f |y C(X 2 ) (31\n)\nwhere we have used that the latter term does not depend on \u03c9. We proceed to bound the inner portion of the first term. Define the bounded linear operator M k : C(X \u00d7 X ) \u2192 C(X \u00d7 X ) by the expression\n(M k c)(x, x ) = c(x, x ) \u2212 C x,m K \u22121 m,m K m,x \u2212 K x,m K \u22121 m,m C m,x + K x,m K \u22121 m,m C m,m K \u22121 m,m K m,x .(32)\nLet \u03a3 = Cov(u). By explicit calculation, we have\nk (d) (x, x ) = (M k k (w) )(x, x ) + K x,m K \u22121 m,m \u03a3K \u22121 m,m K m,x(33)\nand we also have\nk (s) (x, x ) = k (f |y) (x, x ) + K x,m K \u22121 m,m \u03a3K \u22121 m,m K m,x(34)\nhence\nk (d) \u2212 k (s) C(X 2 ) = M k k (w) \u2212 k (f |y) C(X 2 ) = M k k (w) \u2212 M k k C(X 2 ) \u2264 M k L(C;C) k (w) \u2212 k C(X 2 ) . (35\n)\nWe proceed to bound the operator norm\nM k L(C;C) . Write M k c C(X 2 ) \u2264 c C(X 2 ) + C \u2022,m K \u22121 m,m K m,\u2022 C(X 2 ) + K \u2022,m K \u22121 m,m C m,\u2022 C(X 2 ) (36) + K \u2022,m K \u22121 m,m C m,m K \u22121 m,m K m,\u2022 C(X 2 ) .(37)\nNow, note that\nC \u2022,m K \u22121 m,m K m,\u2022 C(X 2 ) = sup x,x \u2208X C x,m K \u22121 m,m K m,x(38)\n\u2264 sup x,x \u2208X C x,m \u221e K \u22121 m,m L( \u221e ; 1 ) K m,x \u221e (39) \u2264 c C(X 2 ) K \u22121 m,m L( \u221e ; 1 ) k C(X 2 )(40)\nby H\u00f6lder's inequality with p = 1 and q = \u221e, and then by the definition of the operator norm\n\u2022 L( \u221e ; 1 ) . Similarly K \u2022,m K \u22121 m,m C m,m K \u22121 m,m K m,\u2022 C(X 2 ) \u2264 m c C(X 2 ) K \u22121 m,m 2 L( \u221e ; 1 ) k 2 C(X 2 ) (41) hence M k c C(X 2 ) \u2264 c C(X 2 ) + 2 c C(X 2 ) K \u22121 m,m L( \u221e ; 1 ) k C(X 2 ) + m c C(X 2 ) K \u22121 m,m 2 L( \u221e ; 1 ) k 2 C(X 2 ) (42) \u2264 c C(X 2 ) m 1 + K \u22121 m,m L( \u221e ; 1 ) k C(X 2 ) 2(43)\nand therefore\nM k L(C;C) = sup c =0 M k c C(X 2 ) c C(X 2 ) \u2264 m 1 + K \u22121 m,m L( \u221e ; 1 ) k C(X 2 ) 2 .(44)\nNote that this term is independent of \u03c9, and hence constant with respect to the expectation. Finally, Sutherland and Schneider (2015) have shown that there exists a constant C 2 such that.\nE \u03c9\u223c\u03c1 \u03c5\u223cU k (w) \u2212 k C(X 2 ) \u2264 C 2 \u221a . (45\n)\nPutting together all the inequalities gives the result.", "publication_ref": ["b43", "b43"], "figure_ref": [], "table_ref": []}, {"heading": "C. Additional experiments", "text": "This appendix provides additional details regarding experiments discussed in Section 4. All experiments (and figures) were run using zero-mean GP priors with Mat\u00e9rn-5 /2 kernels. For dynamical systems experiments, hyperparameters were learned (MLE type-2). In all other cases, hyperparameters were assumed to be known and specified as: lengthscales l = d /100, measurement noise variance \u03c3 2 = 10 \u22123 , and kernel amplitude \u03b1 = 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1. 2-Wasserstein sample tests", "text": "In each trial, a set of training locations X \u223c U [0, 1] n\u00d7d was randomly generated and corresponding observations y \u223c N (0, K n,n + \u03c3 2 I) were subsequently drawn from the prior. Similarly, test sets X * \u223c U [0, 1] * \u00d7d were sampled uniformly at random. For each sampling schemes, 100, 000 draws f * | y were then used to form an unbiased estimate (m * |n , K * , * |n ) to the true posterior moments (m * |n , K * , * |n ). Given both sets of moments, 2-Wasserstein distances were computed as\nW 2, 2 * N (m * |n , K * , * |n ), N (m * |n , K * , * |n ) 2 = m * |n \u2212 m * |n 2 + tr K * , * |n + K * , * |n \u2212 2 K 1 /2 * , * |n K * , * |n K 1 /2 * , * |n 1 /2 ,(46)\nwhere K\n1 /2 * , * |n denotes the symmetric matrix square root, and W 2, 2 * denotes the 2-Wasserstein distance between probability measures over * -dimensional vectors equipped with Euclidean distance. As an additional baseline, we compared decoupled sampling with a LanczOs Variance Estimates (LOVE) based alternative (Pleiss et al., 2018). The LOVE approach to sampling from GP posteriors exploits structured covariance matrices in conjunction with fast (approximate) solvers to achieve linear time complexity with respect to number of test locations. For example, when inducing locations Z are defined to be a regularly spaced grid, the prior covariance K m,m = k(Z, Z) can be expressed as the Kronecker product of Toeplitz matrices-a property that can be used to dramatically expedite much of the related linear algebra (Zimmerman, 1989;Saat\u00e7i, 2012;Wilson and Nickisch, 2015).\nHere, we are interested in comparing the performance of sampling schemes themselves and not that of approximate GPs. As before, we will therefore sample from exact GPs with known hyperparameters. As an additional caveat however, we now define training locations as regularly spaced grids, such that LOVE may represent the data exactly. Similarly, we allow LOVE to utilize n conjugate gradient iterations during precomputation.\nResults of these experiments are show in Figure 6. LOVE's performance improves significantly as m = n increases but still lags behind that of decoupled sampling for matching m. Several points are immediately worth addressing here. First, kernel interpolation methods such as LOVE offer improved scaling w.r.t. m when compared to na\u00efve inducing point methods (even when additional structure is imposed on Z). LOVE can therefore utilize many more inducing locations than traditional sparse GPs in exchange for the imposed structural constraints. Assessing the relative merits of these inducing paradigms is beyond the scope of this work. Second, during sample generation, LOVE exhibits O(m + * ) time complexity, compared to decoupled sampling's O(m \u00d7 * ). Third, LOVE samples function values f * at locations X * whereas decoupled sampling generates function draws (f | u)(\u2022), the implications of which were previously explored in Section 4. Fourth and finally, the techniques and ideas espoused by these frameworks are complementary: just as we may approximate the prior via a collection of Fourier features, we may approximate the update via, e.g., kernel interpolation.", "publication_ref": ["b28", "b51", "b34", "b49"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "C.2. Thompson sampling", "text": "As baselines, we compared against Random Search (Bergstra and Bengio, 2012) and Dividing Rectangles (Jones et al., 1993), the latter of which was run in strictly sequential fashion (i.e., \u03ba = 1). Minimization tasks were drawn from a known GP prior (see above) and their global minimums were estimated by running gradient descent from a large number of starting locations (for purposes of measuring regret). Here, we discuss algorithmic differences between variants of TS.\nFor function-space TS, batches were constructed as follows.\n1. Construct a mesh X * consisting of |X * | = 10 6 random points. For simplicity, a new mesh X * was generated at each TS iteration and shared between batch elements, but steps (2-5) we Figure 7: Results for parallel Thompson sampling, shown as quartiles over 32 independent runs with matched seeds. run independently. Weight-space and decoupled TS employed a similar procedure, with minor differences stemming from use of function draws.\n1. Construct a mesh X * consisting of |X * | = 250, 000 random points.", "publication_ref": ["b0", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Generate a function draw (f | y)(\u2022).", "text": "3. Define starting locations X s \u2286 X * corresponding to the s = 32 smallest elements of (f | y)(X * ).\n4. Run multi-start gradient-based optimization: we employed an off-the-shelf version of L-BFGS-B.\n5. Select x i \u2208 arg min 1\u2264i\u2264s (f | y)(X * ) as the i-th batch element, where X s denotes the optimized locations.\nAs before, steps (2-5) we run independently. Optimization performance and runtimes are shown below.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.3. Dynamical systems", "text": "We investigated decoupled sampling's impact on (sequential) Monte Carlo methods' runtimes by using a sparse GP to simulate a simple dynamical system, the FitzHugh-Nagumo model neuron (FitzHugh, 1961;Nagumo et al., 1962) with diffusion coefficient \u03a3 = 0.01 \u2022 I. Training and simulation were both performed using a step size \u2206t = 0.25.\nDuring training, independent sparse GPs with m = 32 shared inducing locations were fit to 3-dimensional inputs x t = [s t , c t ], where s \u2208 [0, 1] 2 denotes the (normalized) state vector at time t and c \u2208 [0, 1] the coinciding (normalized) control input, with targets defined as the i-th element of the Euler-Maruyama transition vectors specified by (17). Owing to the Figure 8: Empirical distributions of per trial runtimes for parallel TS with different sampling strategies; subplots are 1-to-1 with those in Figure 7.", "publication_ref": ["b10", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "The authors would like to express their gratitude to Prof. Mikhail Lifshits, without whom our collaboration would never have started. This research was partially supported by \"Native towns\", a social investment program of PJSC \"Gazprom Neft\" and by the Ministry of Science and Higher ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "need to separate out signal from noise, the training set consisted of 10, 000 uniform random training points and training was performed using stochastic gradient descent.\nAt test time, a baseline was constructed by iteratively drawing drift vectors f t+1 | f 1:t . At each iteration, the current input x t is added to the set of inducing locations Z t+1 = Z t \u222a {x t } and the i-th inducing distribution is augmented to incorporate the sampled drift as\nwhere v = k t (x t , Z t )k t (x t , x t ) \u22121 /2 is defined in terms of the posterior covariance given the m + t preceding inducing locations. When the inducing covariance is parameterized by its Cholesky factor, \u03a3\n1 /2 t+1 can be directly computed via a rank-1 downdate (Gill et al., 1974;Seeger, 2004). Since only the m-th leading principal submatrix of \u03a3 1 /2 t+1 needs to be modified (the remaining terms are all zero because f t is directly observed), this downdate incurs O(m 2 ) time complexity per iteration. In similar fashion, the prior covariance and its Cholesky factor may be maintained online. Here, however, as well as when computing posterior marginals, the matrices are no longer sparse, resulting in O((m + t) 2 ) cost per step. Overall, the iterative approach to unrolling scales cubically in the number of steps.", "publication_ref": ["b13", "b37"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Random search for hyperparameter optimization", "journal": "Journal of Machine Learning Research", "year": "2012", "authors": "J Bergstra; Y Bengio"}, {"ref_id": "b1", "title": "Rates of convergence for sparse variational Gaussian process regression", "journal": "", "year": "2019", "authors": "D R Burt; C E Rasmussen; M V D Wilk"}, {"ref_id": "b2", "title": "Gaussian process optimization with adaptive sketching: scalable and no regret", "journal": "", "year": "2019", "authors": "D Calandriello; L Carratino; A Lazaric; M Valko; L Rosasco"}, {"ref_id": "b3", "title": "Variational inference for Gaussian process models with linear complexity", "journal": "", "year": "2017", "authors": "C.-A Cheng; B Boots"}, {"ref_id": "b4", "title": "Geostatistics: Modeling Spatial Uncertainty", "journal": "Wiley", "year": "2009", "authors": "J.-P Chiles; P Delfiner"}, {"ref_id": "b5", "title": "Sinkhorn distances: lightspeed computation of optimal transport", "journal": "", "year": "2013", "authors": "M Cuturi"}, {"ref_id": "b6", "title": "Reminders on the conditioning Kriging", "journal": "Springer", "year": "1994", "authors": "C D Fouquet"}, {"ref_id": "b7", "title": "Gaussian processes for data-efficient learning in robotics and control", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2015", "authors": "M P Deisenroth; D Fox; C E Rasmussen"}, {"ref_id": "b8", "title": "Fast and exact simulation of stationary Gaussian processes through circulant embedding of the covariance matrix", "journal": "SIAM Journal of Scientific Computing", "year": "1997", "authors": "C R Dietrich; G N Newsam"}, {"ref_id": "b9", "title": "A note on efficient conditional simulation of Gaussian distributions", "journal": "", "year": "2010", "authors": "A Doucet"}, {"ref_id": "b10", "title": "Impulses and physiological states in theoretical models of nerve membrane", "journal": "Biophysical Journal", "year": "1961", "authors": "R Fitzhugh"}, {"ref_id": "b11", "title": "Bayesian reinforcement learning: a survey. Foundations and Trends in Machine Learning", "journal": "", "year": "2015", "authors": "M Ghavamzadeh; S Mannor; J Pineau; A Tamar"}, {"ref_id": "b12", "title": "On choosing and bounding probability metrics", "journal": "International Statistical Review", "year": "2002", "authors": "A L Gibbs; F E Su"}, {"ref_id": "b13", "title": "Methods for modifying matrix factorizations", "journal": "", "year": "1974", "authors": "P E Gill; G H Golub; W Murray; M A Saunders"}, {"ref_id": "b14", "title": "Gaussian processes for big data", "journal": "", "year": "2013", "authors": "J Hensman; N Fusi; N D Lawrence"}, {"ref_id": "b15", "title": "Variational Fourier features for Gaussian processes", "journal": "Journal of Machine Learning Research", "year": "2017", "authors": "J Hensman; N Durrande; A Solin"}, {"ref_id": "b16", "title": "Parallel and distributed Thompson sampling for large-scale accelerated exploration of chemical space", "journal": "", "year": "2017", "authors": "J M Hern\u00e1ndez-Lobato; J Requeima; E O Pyzer-Knapp; A Aspuru-Guzik"}, {"ref_id": "b17", "title": "Predictive entropy search for efficient global optimization of black-box functions", "journal": "", "year": "2014", "authors": "J M Hern\u00e1ndez-Lobato; M W Hoffman; Z Ghahramani"}, {"ref_id": "b18", "title": "Constrained realizations of Gaussian fields: a simple algorithm", "journal": "The Astrophysical Journal", "year": "1991", "authors": "Y Hoffman; E Ribak"}, {"ref_id": "b19", "title": "Lipschitzian optimization without the Lipschitz constant", "journal": "", "year": "1993", "authors": "D R Jones; C D Perttunen; B E Stuckman"}, {"ref_id": "b20", "title": "Mining geostatistics", "journal": "Academic Press London", "year": "1978", "authors": "A G Journel; C J Huijbregts"}, {"ref_id": "b21", "title": "Data-efficient reinforcement learning with probabilistic model predictive control", "journal": "", "year": "2018", "authors": "S Kamthe; M P Deisenroth"}, {"ref_id": "b22", "title": "Parallelised Bayesian optimisation via Thompson Sampling", "journal": "", "year": "2018", "authors": "K Kandasamy; A Krishnamurthy; J Schneider; B P\u00f3czos"}, {"ref_id": "b23", "title": "Gaussian processes in reinforcement learning", "journal": "", "year": "2004", "authors": "M Kuss; C E Rasmussen"}, {"ref_id": "b24", "title": "Learning from uncertain curves: The 2-Wasserstein metric for Gaussian processes", "journal": "", "year": "2017", "authors": "A Mallasto; A Feragen"}, {"ref_id": "b25", "title": "Efficient high dimensional Bayesian optimization with additivity and quadrature Fourier features", "journal": "", "year": "2018", "authors": "M Mutny; A Krause"}, {"ref_id": "b26", "title": "An active pulse transmission line simulating nerve axon", "journal": "", "year": "1962", "authors": "J Nagumo; S Arimoto; S Yoshizawa"}, {"ref_id": "b27", "title": "Computational Optimal Transport: With Applications to Data Science. Foundations and Trends in Machine Learning", "journal": "", "year": "2019", "authors": "G Peyr\u00e9; M Cuturi"}, {"ref_id": "b28", "title": "Constant-time predictive distributions for Gaussian processes", "journal": "", "year": "2018", "authors": "G Pleiss; J R Gardner; K Q Weinberger; A G Wilson"}, {"ref_id": "b29", "title": "A unifying view of sparse approximate Gaussian process regression", "journal": "Journal of Machine Learning Research", "year": "2005", "authors": "J Qui\u00f1onero-Candela; C E Rasmussen"}, {"ref_id": "b30", "title": "Random features for large-scale kernel machines", "journal": "", "year": "2008", "authors": "A Rahimi; B Recht"}, {"ref_id": "b31", "title": "Gaussian Processes for Machine Learning", "journal": "MIT Press", "year": "2006", "authors": "C E Rasmussen; C K I Williams"}, {"ref_id": "b32", "title": "Healing the relevance vector machine through augmentation", "journal": "", "year": "2005", "authors": "C E Rasmussen; J Quinonero-Candela"}, {"ref_id": "b33", "title": "Linearly independent, orthogonal, and uncorrelated variables", "journal": "The American Statistician", "year": "1984", "authors": "J L Rodgers; W A Nicewander; L Toothaker"}, {"ref_id": "b34", "title": "Scalable inference for structured Gaussian process models", "journal": "", "year": "2012", "authors": "Y Saat\u00e7i"}, {"ref_id": "b35", "title": "Orthogonally decoupled variational Gaussian processes", "journal": "", "year": "2018", "authors": "H Salimbeni; C.-A Cheng; B Boots; M P Deisenroth"}, {"ref_id": "b36", "title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond", "journal": "MIT Press", "year": "2001", "authors": "B Sch\u00f6lkopf; A J Smola"}, {"ref_id": "b37", "title": "Low rank updates for the Cholesky decomposition", "journal": "", "year": "2004", "authors": "M Seeger"}, {"ref_id": "b38", "title": "Bayesian inference and optimal design for the sparse linear model", "journal": "Journal of Machine Learning Research", "year": "2008", "authors": "M W Seeger"}, {"ref_id": "b39", "title": "Taking the human out of the loop: a review of Bayesian optimization", "journal": "Proceedings of the IEEE", "year": "2015", "authors": "B Shahriari; K Swersky; Z Wang; R P Adams; N. De Freitas"}, {"ref_id": "b40", "title": "Sparse orthogonal variational inference for Gaussian processes", "journal": "In Artificial Intelligence and Statistics", "year": "2020", "authors": "J Shi; M K Titsias; A Mnih"}, {"ref_id": "b41", "title": "Sparse Gaussian processes using pseudo-inputs", "journal": "", "year": "2006", "authors": "E Snelson; Z Ghahramani"}, {"ref_id": "b42", "title": "Practical Bayesian optimization of machine learning algorithms", "journal": "", "year": "2012", "authors": "J Snoek; H Larochelle; R P Adams"}, {"ref_id": "b43", "title": "On the error of random Fourier features", "journal": "", "year": "2015", "authors": "D J Sutherland; J Schneider"}, {"ref_id": "b44", "title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples", "journal": "Biometrika", "year": "1933", "authors": "W R Thompson"}, {"ref_id": "b45", "title": "Variational learning of inducing variables in sparse Gaussian processes", "journal": "", "year": "2009", "authors": "M K Titsias"}, {"ref_id": "b46", "title": "Peak and gravity constraints in Gaussian primordial density fields: an application of the Hoffman-Ribak method", "journal": "Monthly Notices of the Royal Astronomical Society", "year": "1996", "authors": "R V D Weygaert; E Bertschinger"}, {"ref_id": "b47", "title": "Exact Gaussian processes on a million data points", "journal": "", "year": "2019", "authors": "K Wang; G Pleiss; J Gardner; S Tyree; K Q Weinberger; A G Wilson"}, {"ref_id": "b48", "title": "Batched large-scale Bayesian optimization in high-dimensional spaces", "journal": "", "year": "2018", "authors": "Z Wang; C Gehring; P Kohli; S Jegelka"}, {"ref_id": "b49", "title": "Kernel interpolation for scalable structured Gaussian processes", "journal": "", "year": "2015", "authors": "A Wilson; H Nickisch"}, {"ref_id": "b50", "title": "Maximizing acquisition functions for Bayesian optimization", "journal": "", "year": "2018", "authors": "J T Wilson; F Hutter; M P Deisenroth"}, {"ref_id": "b51", "title": "Computationally exploitable structure of covariance matrices and generalized convariance matrices in spatial models", "journal": "Journal of Statistical Computation and Simulation", "year": "1989", "authors": "D L Zimmerman"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Comparison of GP posteriors and sample paths given n = 4 (top) and n = 1000 (bottom) observations at shaded locations. Error values shown in bottom right-hand corner of each figure denote 2-Wasserstein distances (see Section 4) between empirical (closed-form) posteriors and the true posterior (dashed black). Left: Mean and two standard deviations of exact GP posterior (green) along with samples at * = 1024 test locations. Middle: Sparse GP with inducing variables u at m = 8 locations z \u2208 X denoted by '\u2022'. Right: Random Fourier feature-based GP with = 2000 basis functions; for n = 1000, variance starvation has started to set in and predictions away from the data show visible signs of deterioration.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure2: Visual overview of decoupled sampling with a weight-space prior (orange) and a function-space update (purple); this example continues from Figure1. Left: 1000 Fourier basis functions \u03c6 i (x) = cos(\u03b8 i x + \u03c4 i ) are used to construct a function draw f (\u2022) = \u03c6(\u2022) w from an approximate prior (orange), resulting in residuals (dashed black) at each of m = 8 inducing locations z j \u2208 Z (black circles). Middle: a conditional sample path f | u (blue) is formed by adding an update (purple) consisting of canonical basis functions \u03c8 j (\u2022) = k(\u2022, z j ) to f . Right: the empirical distribution of sample paths f | u is compared with that of the sparse GP posterior (dashed black). 2-Wasserstein errors of empirical (closed-form) posteriors were measured against the exact GP's moments.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure3: Empirical estimates of 2-Wasserstein distances between true posteriors and empirical distributions of 100, 000 samples at 1024 test locations X * given varying amounts of training data, shown as medians and interquartile ranges (shaded regions) measured over 64 independent trials. Weight-space (orange) and decoupled (blue) sampling utilized a total of b = m + basis functions. Results using \u2208 {1024, 4096, 16384} initial bases correspond with {light, medium, dark} tones and { , , } markers. See Appendix C.1 for extended results and comparison with LOVE(Pleiss et al., 2018).", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 6 :6Figure6: Medians and interquartile ranges of empirically estimated 2-Wasserstein distances measured over 32 independent trials consisting of 100,000 samples. LOVE (green) improves as the regularly spaced grids of training locations fill the space. Weight-space (orange) and decoupled (blue) sampling utilized a total of b = m + basis functions. Results using \u2208 {1024, 4096, 16384} initial bases correspond with {light, medium, dark} tones and { , , } markers.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "2.Draw a vector of independent values f * | y \u223c N (m * |n , K * , * |n I), where is the element-wise product. 3. Define an active set X s \u2286 X * corresponding to the s = 2048 smallest elements of f * | y. 4. Jointly sample f s | y \u223c N (m s|n , K s,s|n ). 5. Select x i \u2208 arg min 1\u2264i\u2264s f s | y as the i-th batch element.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "f * | y = m * |n + K 1/2 * , * |n \u03b6,(2)", "formula_coordinates": [2.0, 117.81, 402.27, 171.63, 14.68]}, {"formula_id": "formula_1", "formula_text": "p(f * | y) \u2248 R m p(f * | u)q(u) du.", "formula_coordinates": [2.0, 351.16, 216.21, 146.55, 17.65]}, {"formula_id": "formula_2", "formula_text": "m * |m = K * ,m K \u22121 m,m \u00b5 m K * , * |m = K * , * +K * ,m K \u22121 m,m (\u03a3 u \u2212K m,m )K \u22121 m,m K m, * .(4)", "formula_coordinates": [2.0, 311.87, 272.27, 229.57, 29.11]}, {"formula_id": "formula_3", "formula_text": "k(x, x ) = \u03d5(x), \u03d5(x ) H \u2248 \u03c6(x) \u03c6(x),(5)", "formula_coordinates": [2.0, 336.22, 633.96, 205.22, 11.18]}, {"formula_id": "formula_4", "formula_text": "f (\u2022) = i=1 w i \u03c6 i (\u2022) w i \u223c N (0, 1),(6)", "formula_coordinates": [3.0, 88.67, 409.57, 200.77, 19.61]}, {"formula_id": "formula_5", "formula_text": "\u00b5 w|n = (\u03a6 \u03a6 + \u03c3 2 I) \u22121 \u03a6 y \u03a3 w|n = (\u03a6 \u03a6 + \u03c3 2 I) \u22121 \u03c3 2 ,(7)", "formula_coordinates": [3.0, 110.2, 524.49, 179.24, 29.15]}, {"formula_id": "formula_6", "formula_text": "(a | b = \u03b2) d = a + Cov(a, b) Cov(b, b) \u22121 (\u03b2 \u2212 b). (8)", "formula_coordinates": [4.0, 62.9, 246.06, 226.54, 12.56]}, {"formula_id": "formula_7", "formula_text": "(f | u)(\u2022) posterior d = f (\u2022) prior + k(\u2022, Z)K \u22121 m,m (u \u2212 f m ) update .(9)", "formula_coordinates": [4.0, 82.93, 444.33, 206.51, 26.37]}, {"formula_id": "formula_8", "formula_text": "f * | u d = f * + K * ,m K \u22121 m,m (u \u2212 f m )(10)", "formula_coordinates": [4.0, 313.11, 129.4, 228.33, 14.23]}, {"formula_id": "formula_9", "formula_text": "f * | y d = f * + K * ,n (K n,n + \u03c3 2 I) \u22121 (y \u2212 f \u2212 \u03b5). (11", "formula_coordinates": [4.0, 313.65, 148.69, 223.65, 14.2]}, {"formula_id": "formula_10", "formula_text": ")", "formula_coordinates": [4.0, 537.29, 152.61, 4.15, 8.64]}, {"formula_id": "formula_11", "formula_text": "w | y d = w + \u03a6 (\u03a6\u03a6 + \u03c3 2 I) \u22121 (y \u2212 \u03a6w \u2212 \u03b5). (12", "formula_coordinates": [4.0, 314.09, 301.85, 223.21, 12.56]}, {"formula_id": "formula_12", "formula_text": ")", "formula_coordinates": [4.0, 537.29, 305.78, 4.15, 8.64]}, {"formula_id": "formula_13", "formula_text": "(f | u)(\u2022) sparse posterior d \u2248 i=1 w i \u03c6 i (\u2022) weight-space prior + m j=1 v j k(\u2022, z j ), function-space update (13", "formula_coordinates": [5.0, 73.24, 360.41, 212.05, 43.61]}, {"formula_id": "formula_14", "formula_text": ")", "formula_coordinates": [5.0, 285.29, 371.14, 4.15, 8.64]}, {"formula_id": "formula_15", "formula_text": "W 2,L 2 (X ) f (d) , f | y \u2264 W 2,L 2 (X ) f (s) , f | y error in the (sparse) posterior + C 1 W 2,C(X ) f (w) , f error in the prior ,(14)", "formula_coordinates": [6.0, 316.32, 274.3, 225.12, 39.5]}, {"formula_id": "formula_16", "formula_text": "Proposition 4. Continuing from Proposition 3, let k (f |y) , k (w) , k (s) , k(", "formula_coordinates": [6.0, 307.44, 563.74, 235.74, 22.27]}, {"formula_id": "formula_17", "formula_text": "E \u03c6 k (d) \u2212 k (f |y) C(X 2 ) \u2264 k (s) \u2212 k (f |y) C(X 2 ) + C 2 C 3 \u221a ,(15)", "formula_coordinates": [6.0, 344.9, 624.51, 196.54, 37.48]}, {"formula_id": "formula_18", "formula_text": "x n+1 \u2208 arg min x\u2208X (f | y)(x)(16)", "formula_coordinates": [8.0, 117.9, 360.28, 171.54, 16.55]}, {"formula_id": "formula_19", "formula_text": "\u2206s t = s t+1 \u2212 s t = f (s t , c t )\u2206t + \u221a \u2206t\u03a3\u03b5 t ,(17)", "formula_coordinates": [8.0, 324.55, 590.69, 216.89, 18.59]}, {"formula_id": "formula_20", "formula_text": "w | u d = w + \u03a6 (\u03a6\u03a6 ) \u22121 (u \u2212 \u03a6w),(18)", "formula_coordinates": [12.0, 218.09, 159.81, 323.35, 12.56]}, {"formula_id": "formula_21", "formula_text": "\u22a5 | u)(\u2022) = (f | u)(\u2022)\u2212(f | u)(\u2022)", "formula_coordinates": [12.0, 332.93, 293.97, 139.79, 9.68]}, {"formula_id": "formula_22", "formula_text": "f | u)(Z) = (f | u)(Z) = u, it follows that f \u22a5 (Z) = (f \u22a5 | u)(Z) = 0.", "formula_coordinates": [12.0, 55.44, 305.92, 486.0, 21.64]}, {"formula_id": "formula_23", "formula_text": "Cov(f \u22a5 , f \u22a5 ) = k(\u2022, \u2022) \u2212 k(\u2022, Z)K \u22121 m,m k(Z, \u2022). (19", "formula_coordinates": [12.0, 205.95, 368.25, 331.35, 12.69]}, {"formula_id": "formula_24", "formula_text": ")", "formula_coordinates": [12.0, 537.29, 370.64, 4.15, 8.64]}, {"formula_id": "formula_25", "formula_text": "W 2,L 2 (X ) f (d) , f | y \u2264 W 2,L 2 (X ) f (s) , f | y + C 1 W 2,L \u221e (X ) f (w) , f(20)", "formula_coordinates": [12.0, 145.65, 555.42, 395.8, 12.03]}, {"formula_id": "formula_26", "formula_text": "C 1 = 2 diam(X ) d 1 + k 2 C(X 2 ) K \u22121 mm 2 L( \u221e ; 1 ) , W 2,L 2 (X )", "formula_coordinates": [12.0, 83.07, 593.01, 268.26, 16.39]}, {"formula_id": "formula_27", "formula_text": "W 2,L 2 (X ) f (d) , f | y \u2264 W 2,L 2 (X ) f (d) , f (s) + W 2,L 2 (X ) f (s) , f | y . (21", "formula_coordinates": [12.0, 148.24, 704.69, 389.05, 12.03]}, {"formula_id": "formula_28", "formula_text": ")", "formula_coordinates": [12.0, 537.29, 707.08, 4.15, 8.64]}, {"formula_id": "formula_29", "formula_text": "f (d) (x) \u2212 f (s) (x) 2 \u2264 2 f (w) (x) \u2212 f (x) 2 + K xm K \u22121 mm (f (w) (z) \u2212 f (z)) 2 (22) \u2264 2 f (w) \u2212 f 2 L \u221e (X ) + K xm K \u22121 mm 2 1 f (w) (z) \u2212 f (z) 2 \u221e (23) \u2264 2 f (w) \u2212 f 2 L \u221e (X ) + K xm 2 \u221e K \u22121 mm 2 L( \u221e ; 1 ) f (w) \u2212 f 2 L \u221e (X )(24)", "formula_coordinates": [13.0, 108.92, 89.6, 432.53, 77.87]}, {"formula_id": "formula_30", "formula_text": "\u2264 2 1 + k 2 C(X 2 ) K \u22121 mm 2 L( \u221e ; 1 ) f (w) \u2212 f 2 L \u221e (X )(25)", "formula_coordinates": [13.0, 191.79, 172.33, 349.65, 22.08]}, {"formula_id": "formula_31", "formula_text": "= 2 1 + k 2 C(X 2 ) K \u22121 mm 2 L( \u221e ; 1 ) f (w) \u2212 f 2 C(X )(26)", "formula_coordinates": [13.0, 191.79, 198.52, 349.65, 22.08]}, {"formula_id": "formula_32", "formula_text": "W 2 2,L 2 (X ) (f (d) , f (s) ) \u2264 inf \u03b3\u2208C f (d) \u2212 f (s) 2 L 2 (X ) d\u03b3 (27) \u2264 C|X | inf \u03b3\u2208C f (w) \u2212 f 2 C(X ) d\u03b3 (28) = C diam(X ) d W 2 2,C(X ) (f (w) , f ) (29", "formula_coordinates": [13.0, 184.67, 289.28, 356.77, 66.25]}, {"formula_id": "formula_33", "formula_text": ")", "formula_coordinates": [13.0, 537.29, 344.99, 4.15, 8.64]}, {"formula_id": "formula_34", "formula_text": "E \u03c9\u223c\u03c1 \u03c5\u223cU k (d) \u2212 k (f |y) C(X 2 ) \u2264 k (s) \u2212 k (f |y) C(X 2 ) + C 2 C 3 \u221a (30)", "formula_coordinates": [13.0, 177.18, 414.72, 364.26, 26.79]}, {"formula_id": "formula_35", "formula_text": "C 3 = m 1 + K \u22121 m,m C(X 2 ) k C(X 2 ) 2 .", "formula_coordinates": [13.0, 196.28, 473.1, 166.76, 19.43]}, {"formula_id": "formula_36", "formula_text": "E \u03c9\u223c\u03c1 \u03c5\u223cU k (d) \u2212 k f |y C(X 2 ) \u2264 E \u03c9\u223c\u03c1 \u03c5\u223cU k (d) \u2212 k (s) C(X 2 ) + k (s) \u2212 k f |y C(X 2 ) (31", "formula_coordinates": [13.0, 148.09, 528.45, 389.21, 22.13]}, {"formula_id": "formula_37", "formula_text": ")", "formula_coordinates": [13.0, 537.29, 530.84, 4.15, 8.64]}, {"formula_id": "formula_38", "formula_text": "(M k c)(x, x ) = c(x, x ) \u2212 C x,m K \u22121 m,m K m,x \u2212 K x,m K \u22121 m,m C m,x + K x,m K \u22121 m,m C m,m K \u22121 m,m K m,x .(32)", "formula_coordinates": [13.0, 92.08, 590.52, 449.36, 12.69]}, {"formula_id": "formula_39", "formula_text": "k (d) (x, x ) = (M k k (w) )(x, x ) + K x,m K \u22121 m,m \u03a3K \u22121 m,m K m,x(33)", "formula_coordinates": [13.0, 176.02, 631.91, 365.42, 12.69]}, {"formula_id": "formula_40", "formula_text": "k (s) (x, x ) = k (f |y) (x, x ) + K x,m K \u22121 m,m \u03a3K \u22121 m,m K m,x(34)", "formula_coordinates": [13.0, 184.34, 664.56, 357.1, 12.69]}, {"formula_id": "formula_41", "formula_text": "k (d) \u2212 k (s) C(X 2 ) = M k k (w) \u2212 k (f |y) C(X 2 ) = M k k (w) \u2212 M k k C(X 2 ) \u2264 M k L(C;C) k (w) \u2212 k C(X 2 ) . (35", "formula_coordinates": [13.0, 71.62, 702.45, 465.67, 17.2]}, {"formula_id": "formula_42", "formula_text": ")", "formula_coordinates": [13.0, 537.29, 704.84, 4.15, 8.64]}, {"formula_id": "formula_43", "formula_text": "M k L(C;C) . Write M k c C(X 2 ) \u2264 c C(X 2 ) + C \u2022,m K \u22121 m,m K m,\u2022 C(X 2 ) + K \u2022,m K \u22121 m,m C m,\u2022 C(X 2 ) (36) + K \u2022,m K \u22121 m,m C m,m K \u22121 m,m K m,\u2022 C(X 2 ) .(37)", "formula_coordinates": [14.0, 137.04, 70.22, 404.4, 54.71]}, {"formula_id": "formula_44", "formula_text": "C \u2022,m K \u22121 m,m K m,\u2022 C(X 2 ) = sup x,x \u2208X C x,m K \u22121 m,m K m,x(38)", "formula_coordinates": [14.0, 148.14, 152.7, 393.31, 18.59]}, {"formula_id": "formula_45", "formula_text": "\u2264 sup x,x \u2208X C x,m \u221e K \u22121 m,m L( \u221e ; 1 ) K m,x \u221e (39) \u2264 c C(X 2 ) K \u22121 m,m L( \u221e ; 1 ) k C(X 2 )(40)", "formula_coordinates": [14.0, 247.82, 178.85, 293.62, 37.85]}, {"formula_id": "formula_46", "formula_text": "\u2022 L( \u221e ; 1 ) . Similarly K \u2022,m K \u22121 m,m C m,m K \u22121 m,m K m,\u2022 C(X 2 ) \u2264 m c C(X 2 ) K \u22121 m,m 2 L( \u221e ; 1 ) k 2 C(X 2 ) (41) hence M k c C(X 2 ) \u2264 c C(X 2 ) + 2 c C(X 2 ) K \u22121 m,m L( \u221e ; 1 ) k C(X 2 ) + m c C(X 2 ) K \u22121 m,m 2 L( \u221e ; 1 ) k 2 C(X 2 ) (42) \u2264 c C(X 2 ) m 1 + K \u22121 m,m L( \u221e ; 1 ) k C(X 2 ) 2(43)", "formula_coordinates": [14.0, 55.44, 227.01, 486.0, 103.9]}, {"formula_id": "formula_47", "formula_text": "M k L(C;C) = sup c =0 M k c C(X 2 ) c C(X 2 ) \u2264 m 1 + K \u22121 m,m L( \u221e ; 1 ) k C(X 2 ) 2 .(44)", "formula_coordinates": [14.0, 153.28, 352.53, 388.16, 26.6]}, {"formula_id": "formula_48", "formula_text": "E \u03c9\u223c\u03c1 \u03c5\u223cU k (w) \u2212 k C(X 2 ) \u2264 C 2 \u221a . (45", "formula_coordinates": [14.0, 239.23, 415.23, 298.07, 26.79]}, {"formula_id": "formula_49", "formula_text": ")", "formula_coordinates": [14.0, 537.29, 422.29, 4.15, 8.64]}, {"formula_id": "formula_50", "formula_text": "W 2, 2 * N (m * |n , K * , * |n ), N (m * |n , K * , * |n ) 2 = m * |n \u2212 m * |n 2 + tr K * , * |n + K * , * |n \u2212 2 K 1 /2 * , * |n K * , * |n K 1 /2 * , * |n 1 /2 ,(46)", "formula_coordinates": [14.0, 127.45, 635.32, 413.99, 43.41]}], "doi": ""}