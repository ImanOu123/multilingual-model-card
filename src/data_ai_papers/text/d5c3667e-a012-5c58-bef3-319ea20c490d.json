{"title": "Self-supervised Multi-view Stereo via Effective Co-Segmentation and Data-Augmentation", "authors": "Hongbin Xu; Zhipeng Zhou; Yu Qiao; Wenxiong Kang; Qiuxia Wu", "pub_date": "2021-04-12", "abstract": "Recent studies have witnessed that self-supervised methods based on view synthesis obtain clear progress on multiview stereo (MVS). However, existing methods rely on the assumption that the corresponding points among different views share the same color, which may not always be true in practice. This may lead to unreliable self-supervised signal and harm the final reconstruction performance. To address the issue, we propose a framework integrated with more reliable supervision guided by semantic co-segmentation and dataaugmentation. Specially, we excavate mutual semantic from multi-view images to guide the semantic consistency. And we devise effective data-augmentation mechanism which ensures the transformation robustness by treating the prediction of regular samples as pseudo ground truth to regularize the prediction of augmented samples. Experimental results on DTU dataset show that our proposed methods achieve the state-ofthe-art performance among unsupervised methods, and even compete on par with supervised methods. Furthermore, extensive experiments on Tanks&Temples dataset demonstrate the effective generalization ability of the proposed method.", "sections": [{"heading": "Introduction", "text": "Multi-view stereo (MVS) aims at recovering 3D scenes from multi-view images and calibrated cameras, which is an important problem and widely studied in computer vision community (Seitz et al. 2006). Recent success of deep learning has triggered the interest of extending MVS pipelines to endto-end neural networks. The learning-based methods (Yao et al. 2018(Yao et al. , 2019 adopt CNNs to estimate the feature maps and build a cost volume upon the reference camera frustum to predict a per-view depth map for reconstruction. With the help of large-scale 3D ground truth, they outperform traditional geometry-based approaches and dominate the leaderboard. Whereas the learning-driven approaches strongly depend on the availability of 3D ground truth data for training, which is not easy to acquire (Zhong, Li, and Dai 2018). Thus it drives the community to focus on unsupervised/selfsupervised MVS approaches. Recently, there has been a surge in the number of selfsupervised MVS methods that transform the depth estimation problem to an image reconstruction problem (Khot et al. 2019;Dai et al. 2019;Huang et al. 2020). The predicted depth map and the input image are used to reconstruct the image on another view, thus the self-supervision loss is built to estimate the difference between the reconstructed and realistic image on that view. However, as summarized in Figure 1, despite the impressive efforts in previous unsupervised methods, there still exists a clear gap between supervised and unsupervised results. In this paper, we suggest to rethink the task of self-supervision itself to improve the accuracy in MVS.\nPrevious self-supervised MVS methods largely rely on the same color constancy hypothesis, assuming the corresponding points among different views have the same color. However, as Figure 2 shows, in realistic scenarios, various factors may disturb the color distribution, such as light conditions, reflections, noise, etc. Consequently, the ideal self-supervision loss is susceptible to be confused by these common disturbances in color, leading to ambiguous supervision in challenging scenarios, namely color constancy ambiguity. To address the issues, we aim to incorporate the following extra priors of correspondence with the prior of color constancy in self-supervision loss: (1) The prior of semantic correspondence can provide abstract matching For the prior of semantic consistency, most of the previous methods rely on the manually annotated semantic labels (Yang et al. 2018;Dovesi et al. 2019) restricted in fixed scenarios like autonomous driving with specified semantic classes. Whereas in the concern of MVS, on the one hand the semantic annotations are relatively expensive, on the other hand the huge variation in scenarios makes the semantic categories unfixed for segmentation which requires specified classes. Differently, we adopt non-negative matrix factorization (NMF) (Ding, He, and Simon 2005) to excavate the common semantic clusters among multiview images dynamically for unsupervised co-segmentation (Collins, Achanta, and Susstrunk 2018). Then the semantic consistency is maximized among the re-projected multiview semantic maps.\nFor the prior of data augmentation consistency, heavy data augmentation seldom appears in previous self-supervised MVS methods (Khot et al. 2019;Dai et al. 2019;Huang et al. 2020), because the natural color fluctuation in data augmentation will lead to the color constancy ambiguity in selfsupervision. To preserve the reliability of self-supervision, we attach an additional data-augmentation branch with various transformations to the regular training branch. The output of regular training branch is taken as pseudo ground truth to supervise the output of augmented training branch.\nIn summary, our contributions are:\n(1) We propose a unified unsupervised MVS pipeline called Joint Data-Augmentation and Co-Segmentation framework(JDACS) where extra priors of semantic consistency and data augmentation consistency can provide reliable guidance to overcome the color constancy ambiguity.\n(2) We propose a novel self-supervision signal based on semantic consistency, which can excavate mutual semantic correspondences from multi-view images at unfixed scenarios in a totally unsupervised manner.\n(3) We propose a novel way to incorporate heavy data augmentation into unsupervised MVS, which can provide regularization towards color fluctuation.\n(4) The experimental results show that our proposed method can lead to a leap of performance among unsupervised methods and compete on par with some top supervised methods.", "publication_ref": ["b26", "b33", "b34", "b36", "b21", "b10", "b17", "b31", "b12", "b11", "b9", "b21", "b10", "b17"], "figure_ref": ["fig_0", "fig_1"], "table_ref": []}, {"heading": "Related Work", "text": "Supervised MVS: Recent advances in deep learning have interested a series of learnable systems for solving MVS problems (Huang et al. 2018;Ji et al. 2017). MVSNet (Yao et al. 2018) is an end-to-end MVS pipeline that builds a cost volume upon the reference camera frustum and learns the 3D regularization with CNNs. Many variants based on MVS-Net have been proposed for improving the performance (Yao et al. 2019;Luo et al. 2019). Concurrently, along with the fervor for expanding the MVS framework to a coarse-tofine manner, (Chen et al. 2019;Yu and Gao 2020;Yang et al. 2020;Cheng et al. 2020;Gu et al. 2019;Xu and Tao 2020) separate the single MVS pipeline into multiple stages, achieving impressive performances. Unsupervised MVS: Under the assumption of photometric consistency (Godard, Mac Aodha, and Brostow 2017), unsupervised learning has been developed in multi-view systems. (Khot et al. 2019) inherit the self-supervision signal based on view synthesis and dynamically aggregates informative clues from nearby views. (Dai et al. 2019) predict the depth maps for all views simultaneously and filter the occluded regions. (Huang et al. 2020) further endow the depthnormal consistency into the MVS pipeline for improvement. Whereas all these methods share the assumption of color constancy, suffering from ambiguous supervision in challenging scenarios. Segmentation Guided Algorithms: By assigning each pixel in the image to a specific class, semantic segmentation (Long, Shelhamer, and Darrell 2015) can provide an abstract representation. Several methods incorporate the scene parsing information with other tasks. SegStereo (Yang et al. 2018) enables joint learning for segmentation and disparity esitimation simultaneously and (Cheng et al. 2017) utilize semantic clues to guide the training of optical flow estimation. These methods rely on annotated labels for segmentation in specific scenes like autonomous driving, whereas we differently concentrate on excavating semantics from dynamic scenarios. Co-segmentation methods aim at predicting foreground pixels of objects given an image collection (Joulin, Bach, and Ponce 2012). We apply unsupervised co- segmentation (Casser et al. 2019) on the multi-view pairs to exploit the common semantics.", "publication_ref": ["b18", "b19", "b33", "b34", "b24", "b5", "b35", "b32", "b8", "b16", "b30", "b15", "b21", "b10", "b17", "b23", "b31", "b7", "b20", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Method", "text": "In this section, we present Joint Data-Augmentation and Co-Segmentation framework(JDACS). To improve the reliability towards color constancy ambiguity, we incorporate extra priors of semantic consistency and data-augmentation consistency with a basic structure of deep MVS pipeline (Yao et al. 2018) in JDACS. As Figure 3 shows, the architecture of JDACS consists of Depth Estimation branch, Co-Segmentation branch and Data-Augmentation branch.", "publication_ref": ["b33"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Depth Estimation Branch", "text": "As an unsupervised method, our proposed framework can be combined with arbitrary MVS networks. Here, we adopt MVSNet (Yao et al. 2018) as a representative backbone. The network firstly extracts features using a CNN from N input images. Then a variance-based cost volume is constructed via differentiable homography warping and a 3D U-Net is used to regularize the 3D cost volume. Finally, the depth map is inferred for every reference image. A sketch of the pipeline is shown in Figure 3. Photometric Consistency: The key idea of photometric consistency (Barnes et al. 2009) is to minimize the difference between synthesized image and original image on the same view. Denote that the 1-st view is the reference view and the remaining N \u2212 1 views as source views indexed by i(2 \u2264 i \u2264 N ). For a particular pair of images (I 1 , I i ) with associated intrinsic and extrinsic parameters (K, T ). We can calculate the corresponding position p j in source view based on its coordinate p j in reference view.\np j = KT (D(p j )K \u22121 p j )(1)\nwhere j(1 \u2264 j \u2264 HW ) is the index of pixels and D represents the predicted depth map.\nThe warped image I i can then be obtained by using the differentiable bilinear sampling from I i .\nI i (p j ) = I i (p j )(2)\nAlong with the warping, a binary validity mask M i is generated simultaneously, indicating valid pixels in the novel view because some pixels may be projected to the external area of images. In a MVS system, we can warp all N \u2212 1 source views to the reference view to calculate the loss.\nL P C = N i=2 ||(I i \u2212 I 1 ) M i || 2 + ||(\u2207I i \u2212 \u2207I 1 ) M i || 2 ||M i || 1\n(3) where \u2207 denotes the gradient operator and is dot product.", "publication_ref": ["b33", "b2"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Co-Segmentation Branch", "text": "In previous methods (Yang et al. 2018;Casser et al. 2019), handcrafted semantic annotations are usually utilized to provide extra supervision to improve the performance. However, due to the huge variation of scenarios and the expensive cost for manual annotations in MVS, we differently choose to mine the implicit common segments from multi-view images via unsupervised co-segmentation. Co-segmentation aims at localizing the foreground pixels of the common objects given an image collection. It has been proven that nonnegative matrix factorization (NMF) has an inherent clustering property in (Ding, He, and Simon 2005). Following a classical co-segmentation pipeline (Collins, Achanta, and Susstrunk 2018), NMF applied to the activations of a pretrained CNN layer can be exploited to find semantic correspondences across images. Non-negative Matrix Factorization: Non-negative matrix factorization(NMF) is a group of algorithms in multivariate analysis and linear algebra where a matrix A is factorized into two matrices P and Q. All the three matrices are with the property that having no negative elements. As (Ding, He, and Simon 2005) shows, NMF has an inherent clustering property that it automatically clusters the columns of matrix A = (a 1 , ..., a n ). More specifically, if we impose an orthonormal constraint on Q(QQ T = I), then the approximation of A by A P Q achieved by minimizing the following error function is equivalent to the optimization of K-means clustering.\n||A \u2212 P Q|| F , P \u2265 0, Q \u2265 0 (4) where the subscript F means the Frobenius Norm.\nClustering on CNN Activations: ReLU is a common component for many modern CNNs, due to its desirable gradient properties. The CNN feature maps activated by ReLU result in non-negative activations, which naturally fit for the target of NMF. As shown in Figure 3, we apply a pretrained VGG network (Simonyan and Zisserman 2014) for feature extraction. Denote that the extracted feature map is of dimension (H, W, C) on each of the N views. Then the multi-view feature maps are concatenated and reshaped to a (N HW, C) matrix A. By utilizing multiplicative update rule in (Ding, He, and Simon 2005) to solve NMF, A is factorized into a (N HW, K) matrix P and (K, C) matrix Q, where K is the NMF factors representing the number of semantic clusters. For a comprehensive understanding, we provide a brief interpretation of the results P , Q and the clustering effect of NMF in Figure 4.\nThe Q matrix: Due to the orthonormal constraints of NMF(QQ T = I) (Ding, He, and Simon 2005), each row of the (K, C) matrix Q can be viewed as a cluster centroid of C dimensions, which corresponds to a coherent object among views.\nThe P matrix: The rows of the (N HW, K) matrix P correspond to the spatial positions of all pixels from N views. In general, the matrix factorization A \u2248 P Q enforces the product between each row of P and each column of Q to best approximate the C dimensional feature of each pixel in A.\nAs shown in Figure 4, K = 3 semantic objects are clustered in Q from the feature embeddings of all pixels in A, thus P contains the similarity between each pixel and each of the K = 3 clustered semantic objects. Consequently, P can further be reshaped into N heat maps of dimension (H, W, K) and fed into a softmax layer to construct the co-segmentation maps S. Semantic Consistency Loss: With the co-segmentation maps S extracted from matrix P , we can design a selfsupervision constraint based on semantic consistency. The key idea is to expand the photometric consistency across multiple views (Barnes et al. 2009) to the segmentation maps. Similar to the photometric consistency discussed in Section 3, we can calculate the corresponding position p j in source views with the pixel p j in reference view according to Equation 1, given the predicted depth value D(p j ) and the j-th pixel in the image. Then the warped segmentation map S i from the i-th source view can be reconstructed by bilinear sampling.\nS i (p j ) = S i (p j )(5)\nFinally, the semantic-consistency objective L SC is measured by calculating the per-pixel cross-entropy loss between the warped segmentation map S i and the ground truth labels converted from reference segmentation map S 1 .\nL SC = \u2212 N i=2 [ 1 ||M i || 1 HW j=1 f (S 1,j ) log(S i,j )M i,j ] (6)\nwhere f (S 1,j ) = onehot(arg max(S 1,j )) and M i is a binary mask indicating valid pixels from the i-th view to reference view.", "publication_ref": ["b31", "b4", "b11", "b9", "b11", "b11", "b11", "b2"], "figure_ref": ["fig_2", "fig_3", "fig_3"], "table_ref": []}, {"heading": "Data-Augmentation Branch", "text": "Some recent works (Xie et al. 2019;Chen et al. 2020) in contrastive learning demonstrate the benefits of data augmentation in self-supervised learning. The intuition is that data augmentation brings challenging samples which bust the reliability of unsupervised loss and hence provides robustness towards variations.\nBriefly, a random vector \u03b8 is defined to parameterize an arbitrary augmentation \u03c4 \u03b8 : I \u2192\u012a \u03c4 \u03b8 on image I. However, data augmentation has seldom been applied in selfsupervised methods (Khot et al. 2019;Dai et al. 2019;Huang et al. 2020), because natural color fluctuation in augmented images may disturb the color constancy constraint of self-supervision. Hence, we enforce the unsupervised data augmentation consistency by contrasting the output of original data and augmented samples as a regularization, instead of optimizing the original objective of view synthesis. Data Augmentation Consistency Loss: Specifically, as shown in Figure 3, the prediction of a regular forward pass for original images I in Depth Estimation branch is denoted as D. Accordingly, the prediction of augmented im-ages\u012a \u03c4 \u03b8 is denote asD \u03c4 \u03b8 . In a contrastive manner, the dataaugmentation consistency is ensured by minimizing the difference between D andD \u03c4 \u03b8 :\nL DA = 1 ||M \u03c4 \u03b8 || 1 ||(D \u2212D \u03c4 \u03b8 ) M \u03c4 \u03b8 || 2 (7)\nwhere M \u03c4 \u03b8 represents the unoccluded mask under transformation \u03c4 \u03b8 . Due to the epipolar constraints among different views, the integrated augmentation methods in our framework should not change the spatial location of pixels. We will show some augmentation methods used in our method as follows: Cross-view Masking: To simulate the occlusion hallucination among the multi-view situations, we randomly generate a binary crop mask 1 \u2212 M \u03c4 \u03b8 1 to block out some regions on reference view. Then the occlusion mask is projected to other views to mask out the corresponding area in images.\nFollowing the assumption that the remaining regions M \u03c4 \u03b8 1 should be immune to the transformation, we can contrast the validity regions between the results of original and augmented samples. Gamma Correction: Gamma correction is a nonlinear operation used to adjust the illuminance of images. To simulate various illuminations, we integrate random gamma correction \u03c4 \u03b82 parameterized by \u03b8 2 to challenge the unsupervised loss.  Color Jitter and Blur: Many transformations can attach color fluctuation to images, such as random color jitter, random blur, random noise. The color fluctuation makes the unsupervised loss in MVS unreliable, because the photometric loss requires the color constancy among views. In contrast, these transformations denoted as \u03c4 \u03b83 can create challenging scenes and regularize the robustness towards color fluctuation in self-supervision.\nThe overall transformation \u03c4 \u03b8 can be represented as a combination of the aforementioned augmentations: \u03c4 \u03b8 = \u03c4 \u03b83 \u2022 \u03c4 \u03b82 \u2022 \u03c4 \u03b81 , where \u2022 represents function composition.", "publication_ref": ["b29", "b6", "b21", "b10", "b17"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Overall Architecture and Loss", "text": "As shown in Figure 3, the overall framework has three components: Depth Estimation branch, Co-Segmentation branch and Data-Augmentation branch. In our paper, we aim to handle the color constancy ambiguity problem in selfsupervised MVS, as discussed in Section 1. Apart from the basic self-supervision signal based on photometric consistency L P C (Equation 1), we add two extra self-supervision signals of semantic consistency L SC and data-augmentation consistency L DA to the framework. In addition to the aforementioned loss, some common regularization terms suggested by (Mahjourian, Wicke, and Angelova 2018;Khot et al. 2019) for depth estimation are applied, such as structured similarity L SSIM and depth smoothness L Smooth .\nThe final objective can be constructed as follows:\nL = \u03bb 1 L P C + \u03bb 2 L SC + \u03bb 3 L DA +\u03bb 4 L SSIM + \u03bb 5 L Smooth (8)\nwhere the weights are empirically set as: \u03bb 1 = 0.8, \u03bb 2 = 0.1, \u03bb 3 = 0.1, \u03bb 4 = 0.2, \u03bb 5 = 0.0067.", "publication_ref": ["b25", "b21"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Experiments", "text": "In this section, we conduct comprehensive experiments to evaluate the proposed JDACS framework. First, we intro-   (Chen et al. 2019).   ", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}, {"heading": "Ground Truth", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Benchmark Results on DTU", "text": "Comparison with SOTA: The official metrics of the DTU dataset (Aanaes et al. 2016) are: Accuracy, Completeness and Overall. These metrics are used to compare our proposed methods with other methods. The comparison includes traditional methods such as Furu (Furukawa and Ponce 2009), Tola (Tola, Strecha, and Fua 2012), Camp (Campbell et al. 2008), Gipuma (Galliani, Lasinger, and Schindler 2015). For the supervised methods, single stage networks such as Surfacenet (Ji et al. 2017), MVSNet (Yao et al. 2018)   The quantitative results are shown in Table 1. From Table 1, we can conclude that our proposed method outperforms previous unsupervised methods in all official metrics. Furthermore, our proposed method can reconstruct better point cloud than traditional methods and some supervised methods in the metric of Overall. The supervised methods tend to have better performance in the metric of Accuracy, while  unsupervised methods usually achieve better performance in the metric of Completeness. The qualitative comparisons in Figure 5 demonstrate that our proposed method is comparable with some of the SOTA supervised methods. Supervised vs Self-Supervised: From Table 1, we can find that there still exists a clear gap of performance between SOTA supervised methods and previous unsupervised methods. To provide a fair comparison without extra components, we compare our proposed self-supervision framework with supervised methods in the same network settings. The only difference is that our model is trained without any ground truth depth maps. The comparison is provided in Table 2.\nThe supervised baselines are borrowed from previous papers(MVSNet from (Chen et al. 2019), CVP-MVSNet from (Yang et al. 2020)). The results in Table 2 demonstrate that our proposed framework can compete on par with the supervised opponents in the same network settings.", "publication_ref": ["b13", "b28", "b3", "b14", "b19", "b33", "b5", "b32"], "figure_ref": [], "table_ref": ["tab_1", "tab_1", "tab_1", "tab_3", "tab_3"]}, {"heading": "Ablation Studies", "text": "Effect of Different Prior Components: To evaluate the effect of our proposed prior of semantic consistency and data augmentation consistency, we train the networks with different combinations of these self-supervised signals. The quantitative results with different components in our proposed JDACS framework are summarized in Table 3 and Table 4.\nThe model settings of JDACS in Table 3 and JDACS-MS in Table 4 is the same as the ones in Table 2. The qualitative visualization of the results of different components in JDACS-MS is provided in Figure 6. The experimental results demonstrate that endowing these extra priors into the self-supervision training can promote the performance in MVS. For example, as illustrated in Table 3, the Overall error metric decreases from 0.6777mm to 0.5953mm by including the prior of semantic consistency, from 0.6777mm to 0.5898mm with the help of involving data augmentation based branch. Effect of Semantic Cluster Numbers: Different from manual semantic annotations in supervised learning, the semantic concepts excavated in an unsupervised manner are ambiguous. The number of semantic clusters K is a significant hyper-parameter for determining the categories of common semantic concepts among different views. Hence we conduct experiments about the effect of different semantic cluster numbers K and the results are reported in Table 5. Furthermore, a brief visualization of these semantic clusters is provided in Figure 7. From the visualization and the table, we can conclude that when the semantic clusters are more than 4, the localization of the semantic parts becomes less accurate than the ones with less than 4 clusters. As a result, we select K = 4 clusters as a default setting in our proposed method.  ", "publication_ref": [], "figure_ref": ["fig_4", "fig_6"], "table_ref": ["tab_4", "tab_4", "tab_5", "tab_3", "tab_4", "tab_6"]}, {"heading": "Generalization", "text": "In this section, we compare our proposed JDACS with previous unsupervised methods on Tanks&Temples dataset. Due to the requirement of more than 20G memories in GPU using the original post-processing tool provided by (Yao et al. 2018), instead, we use an open simplified version 2 which can be deployed on a GPU with 11G memories like RTX 2080Ti.\nWe follow the same hyper-parameter settings as MVS 2 (Dai et al. 2019). The quantitative comparison with previous unsupervised methods is provided in Table 6 and the visualization of the reconstructed dense point clouds is shown in Figure 8. Our proposed JDACS has better performance by the mean score of 8 scenes than previous unsupervised methods, which is the best unsupervised MVS method until September 9, 2020.", "publication_ref": ["b33", "b10"], "figure_ref": [], "table_ref": ["tab_8"]}, {"heading": "Conclusion", "text": "In this paper, we have proposed a novel unsupervised learning based MVS framework, JDACS, aiming at alleviating the gap between supervision and self-supervision caused by the coarse hypothesis of color constancy. On the one hand, our proposed method can enforce cross-view dataaugmentation consistency into self-supervision with challenging variations. On the other hand, we can excavate the implicit common semantic clusters among different views and enforce the cross-view semantic consistency to provide a semantic-level correspondence metric. Experimental results on multiple benchmarks demonstrate the effectiveness of our proposed self-supervised framework.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Supplementary Details for Implementation", "text": "Implementation of NMF NMF plays an important role in the Co-Segmentation branch of our JDACS framework. We use the multiplicative update rule to calculate the solution of NMF iteratively, as shown in Algorithm 1.\nAlgorithm 1 Multiplicative Update Rule Based NMF Set the number of segments as K; Set the number of maximum iterations as ite max and the tolerance constant as tol; Initialize non-negative matrices P and Q such that\nP \u2265 0, Q \u2265 0; for each iterative step v, 1 \u2264 v \u2264 ite max do Q v+1 [i,j] \u2190 Q v [i,j]\n((P v ) t A)\n[i,j]\n((P v ) t P v Q v ) [i,j] P v+1 [i,j] \u2190 P v [i,j] A(Q v+1 ) t [i,j] (P v Q v+1 (Q v+1 ) t ) [i,j] if A \u2212 P v+1 Q v+1\nF \u2264 tol then P = P v+1 , Q = Q v+1 , stop the iterative process end if end for Implemtation of JDACS-MS As mentioned in the main paper, if a multi-stage MVS-Net is applied in the Depth Estimation branch of JDACS, the framework is denoted as JDACS-MS. The predicted depth maps on all stages are utilized to calculate the self-supervision loss, as shown in Figure 9. In default, we adopt CVP-MVSNet as the backbone network.\nSimilar to the loss function of JDACS (Equation 8 in the main paper), the final objective of JDACS-MS can be constructed as follows:\nL JDACS\u2212M S = 5 s=1 (\u03bb 1 L s P C + \u03bb 2 L s SC + \u03bb 3 L s DA +\u03bb 4 L SSIM + \u03bb 5 L Smooth )(9)\nwhere s represent each stage of the multi-stage MVSNet which is separated into 5 stages in default. The weights are empirically set as: \u03bb 1 = 0.8, \u03bb 2 = 0.1, \u03bb 3 = 0.1, \u03bb 4 = 0.2, \u03bb 5 = 0.0067.   Then the Photometric consistency loss and Co-segmentation consistency loss are calculated, and the gradient is calculated during back-propagation. After updating the gradients, the cached memories are cleared. In the second step, the forward-propagation in the Data-augmentation branch is conducted and the weights are updated during the backpropagation.\nHow to Adjust the Weights for Self-supervised Loss? In practice, the convergence is sensitive to the attribution of weights for each term in self-supervision loss. If inappropriate weights are applied, it is likely to result in trivial solution in self-supervision. Hence, it is important to balance the weights. For the photometric consistency loss term, the weight is assigned following an open implementation 3 . For the co-segmentation consistency loss term, the weight is set according to the scale of the loss, which can be selected from 0.01 to 0.1. For the data-augmentation consistency loss term, it is mentioned that the data-augmentation consistency is actually a strong regularization to the self-supervised framework. In the starting phase of the training process, it may corrupt the convergence of self-supervision. Hence, we set the weight of data-augmentation consistency loss to 0.01 as an initial value, and increase it by 2 times after each 2 epochs, acting as a warming up process.", "publication_ref": [], "figure_ref": ["fig_9"], "table_ref": []}, {"heading": "Visualization", "text": "In addition to the qualitative comparison in 3D reconstruction shown in Figure 6 of the main paper, we further provide more comparisons in Figure 10. Furthermore, we present the visualization of the reconstruction results on DTU dataset (Figure 11) and Tanks&Temples dataset (Figure 12). Please refer to Table 1 and Table 6 in the main paper for quantative results on the datasets.  ", "publication_ref": [], "figure_ref": ["fig_4", "fig_0", "fig_0", "fig_0"], "table_ref": ["tab_1", "tab_8"]}, {"heading": "Limitation and Discussion", "text": "Restriction of Coarse-grained Semantic Feature As shown in Table 5 and Figure 8 in the main paper, the cosegmentation results can only provide coarse-grained semantic feature with no more than 4 semantic clusters. The reason is that the semantic centroids are clustered from the feature space of a pretrained VGG specialized for classification task, where only the coarse-grained semantics are enough to construct distinguishable clues. However, in intuition, fine-grained semantics can provide more effective priors of correspondence for self-supervision. In the future, more accurate and refined semantic features are required for further improving the performance of self-supervision.\nRestriction of Texture-less Region Although our proposed method can handle challenging cases with huge variation in color, it still fails to generalize to texture-less regions. The convergence of all self-supervision reconstruction loss is only effective on colorful regions. Because any pixels in texture-less regions share the same color intensity, leading to the fact that self-supervision loss is fixed to 0 and becomes meaningless. However, the texture-less regions often appear in realistic scenarios, where self-supervision may be confused and fail to generalize. Exploration of handling texture-less regions may provide a potential direction in the future. ", "publication_ref": [], "figure_ref": ["fig_8"], "table_ref": ["tab_6"]}, {"heading": "", "text": "This work was supported in part by the Shanghai Committee of Science and Technology, China (Grant No. 20DZ1100800), in part by the National Natural Science Foundation of China under Grant (61876176, U1713208), and in part by the Shenzhen Basic Research Program (CXB201104220032A), Guangzhou Research Program (201803010066). This work was done during his internship at Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "", "journal": "", "year": "", "authors": "H Aanaes; R R Jensen; G Vogiatzis; E Tola; A Dahl"}, {"ref_id": "b1", "title": "Large-scale data for multiple-view stereopsis", "journal": "International Journal of Computer Vision", "year": "", "authors": ""}, {"ref_id": "b2", "title": "PatchMatch: A randomized correspondence algorithm for structural image editing", "journal": "ACM Trans. Graph", "year": "2009", "authors": "C Barnes; E Shechtman; A Finkelstein; D B Goldman"}, {"ref_id": "b3", "title": "Using multiple hypotheses to improve depth-maps for multiview stereo", "journal": "Springer", "year": "2008", "authors": "N D Campbell; G Vogiatzis; C Hern\u00e1ndez; R Cipolla"}, {"ref_id": "b4", "title": "Unsupervised monocular depth and ego-motion learning with structure and semantics", "journal": "", "year": "2019", "authors": "V Casser; S Pirk; R Mahjourian; A Angelova"}, {"ref_id": "b5", "title": "Point-based multiview stereo network", "journal": "", "year": "2019", "authors": "R Chen; S Han; J Xu; H Su"}, {"ref_id": "b6", "title": "A simple framework for contrastive learning of visual representations", "journal": "", "year": "2020", "authors": "T Chen; S Kornblith; M Norouzi; G Hinton"}, {"ref_id": "b7", "title": "Segflow: Joint learning for video object segmentation and optical flow", "journal": "", "year": "2017", "authors": "J Cheng; Y.-H Tsai; S Wang; M.-H Yang"}, {"ref_id": "b8", "title": "Deep stereo using adaptive thin volume representation with uncertainty awareness", "journal": "", "year": "2020", "authors": "S Cheng; Z Xu; S Zhu; Z Li; L E Li; R Ramamoorthi; H Su"}, {"ref_id": "b9", "title": "Deep feature factorization for concept discovery", "journal": "", "year": "2018", "authors": "E Collins; R Achanta; S Susstrunk"}, {"ref_id": "b10", "title": "Mvs2: Deep unsupervised multi-view stereo with multi-view symmetry", "journal": "IEEE", "year": "2019", "authors": "Y Dai; Z Zhu; Z Rao; B Li"}, {"ref_id": "b11", "title": "On the equivalence of nonnegative matrix factorization and spectral clustering", "journal": "SIAM", "year": "2005", "authors": "C Ding; X He; H D Simon"}, {"ref_id": "b12", "title": "Real-time semantic stereo matching", "journal": "", "year": "2019", "authors": "P L Dovesi; M Poggi; L Andraghetti; M Mart\u00ed; H Kjellstr\u00f6m; A Pieropan; S Mattoccia"}, {"ref_id": "b13", "title": "Accurate, dense, and robust multiview stereopsis", "journal": "", "year": "2009", "authors": "Y Furukawa; J Ponce"}, {"ref_id": "b14", "title": "Massively parallel multiview stereopsis by surface normal diffusion", "journal": "", "year": "2015", "authors": "S Galliani; K Lasinger; K Schindler"}, {"ref_id": "b15", "title": "Unsupervised monocular depth estimation with left-right consistency", "journal": "", "year": "2017", "authors": "C Godard; O Mac Aodha; G J Brostow"}, {"ref_id": "b16", "title": "Cascade Cost Volume for High-Resolution Multi-View Stereo and Stereo Matching", "journal": "", "year": "2019", "authors": "X Gu; Z Fan; S Zhu; Z Dai; F Tan; P Tan"}, {"ref_id": "b17", "title": "M\u02c63VS-Net: Unsupervised Multi-metric Multi-view Stereo Network", "journal": "", "year": "2020", "authors": "B Huang; C Huang; Y He; J Liu; X Liu"}, {"ref_id": "b18", "title": "Deepmvs: Learning multi-view stereopsis", "journal": "", "year": "2018", "authors": "P.-H Huang; K Matzen; J Kopf; N Ahuja; J.-B Huang"}, {"ref_id": "b19", "title": "Surfacenet: An end-to-end 3d neural network for multiview stereopsis", "journal": "", "year": "2017", "authors": "M Ji; J Gall; H Zheng; Y Liu; L Fang"}, {"ref_id": "b20", "title": "Multi-class cosegmentation", "journal": "IEEE", "year": "2012", "authors": "A Joulin; F Bach; J Ponce"}, {"ref_id": "b21", "title": "Learning unsupervised multi-view stereopsis via robust photometric consistency", "journal": "", "year": "2019", "authors": "T Khot; S Agrawal; S Tulsiani; C Mertz; S Lucey; M Hebert"}, {"ref_id": "b22", "title": "Tanks and temples: Benchmarking large-scale scene reconstruction", "journal": "ACM Transactions on Graphics (ToG)", "year": "2017", "authors": "A Knapitsch; J Park; Q.-Y Zhou; V Koltun"}, {"ref_id": "b23", "title": "Fully convolutional networks for semantic segmentation", "journal": "", "year": "2015", "authors": "J Long; E Shelhamer; T Darrell"}, {"ref_id": "b24", "title": "P-mvsnet: Learning patch-wise matching confidence aggregation for multiview stereo", "journal": "", "year": "2019", "authors": "K Luo; T Guan; L Ju; H Huang; Y Luo"}, {"ref_id": "b25", "title": "Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints", "journal": "", "year": "2018", "authors": "R Mahjourian; M Wicke; A Angelova"}, {"ref_id": "b26", "title": "A comparison and evaluation of multi-view stereo reconstruction algorithms", "journal": "IEEE", "year": "2006", "authors": "S M Seitz; B Curless; J Diebel; D Scharstein; R Szeliski"}, {"ref_id": "b27", "title": "Very deep convolutional networks for large-scale image recognition", "journal": "", "year": "2014", "authors": "K Simonyan; A Zisserman"}, {"ref_id": "b28", "title": "Efficient large-scale multiview stereo for ultra high-resolution image sets", "journal": "Machine Vision and Applications", "year": "2012", "authors": "E Tola; C Strecha; P Fua"}, {"ref_id": "b29", "title": "Unsupervised data augmentation for consistency training", "journal": "", "year": "2019", "authors": "Q Xie; Z Dai; E Hovy; M.-T Luong; Q V Le"}, {"ref_id": "b30", "title": "Learning Inverse Depth Regression for Multi-View Stereo with Correlation Cost Volume", "journal": "", "year": "2020", "authors": "Q Xu; W Tao"}, {"ref_id": "b31", "title": "Segstereo: Exploiting semantic information for disparity estimation", "journal": "", "year": "2018", "authors": "G Yang; H Zhao; J Shi; Z Deng; J Jia"}, {"ref_id": "b32", "title": "Cost Volume Pyramid Based Depth Inference for Multi-View Stereo", "journal": "", "year": "2020", "authors": "J Yang; W Mao; J M Alvarez; M Liu"}, {"ref_id": "b33", "title": "Mvsnet: Depth inference for unstructured multi-view stereo", "journal": "", "year": "2018", "authors": "Y Yao; Z Luo; S Li; T Fang; L Quan"}, {"ref_id": "b34", "title": "Recurrent mvsnet for high-resolution multi-view stereo depth inference", "journal": "", "year": "2019", "authors": "Y Yao; Z Luo; S Li; T Shen; T Fang; L Quan"}, {"ref_id": "b35", "title": "Fast-mvsnet: Sparse-to-dense multi-view stereo with learned propagation and gauss-newton refinement", "journal": "", "year": "2020", "authors": "Z Yu; S Gao"}, {"ref_id": "b36", "title": "Open-world stereo video matching with deep rnn", "journal": "", "year": "2018", "authors": "Y Zhong; H Li; Y Dai"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Comparison between SOTA supervised and unsupervised MVS methods.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Illustration of the color constancy ambiguity problem in self-supervised MVS.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Illustration of our Joint Data-Augmentation and Co-Segmentation (JDACS) MVS framework.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Brief illustration of the clustering effect of NMF.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 6 :6Figure 6: Qualitative results JDACS on scan12 of the DTU dataset. Top row: Overview of generated point clouds with different combinations of self-supervision components. Bottom row: zoomed local areas. L P C : Photometric-Consistency Loss; L SC : Semantic-Consistency Loss; L DA : Data-Augmentation-Consistency Loss.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": ", P-MVSNet(Luo et al. 2019), R-MVSNet(Yao et al. 2019), and multi-stage networks such as Point-MVSNet(Chen et al. 2019), Fast-MVSNet (Yu and Gao    ", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 7 :7Figure 7: Visualization of the co-segmentation results with different number of segmentation parts K.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 8 :8Figure 8: Visualization of the generated 3D point clouds without any finetuning on Tanks&Temples dataset.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 9 :9Figure 9: Brief illustration of JDACS-MS.", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 10 :10Figure 10: More qualitative comparison in 3D reconstruction between our JDACS and SOTA supervised method (CVP-MVSNet) on DTU dataset. From left to right: ground truth, results of CVP-MVSNet, our results.", "figure_data": ""}, {"figure_label": "1112", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 11 :Figure 12 :1112Figure 11: Point cloud reconstruction results on the DTU test set.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Quantitative results on DTU evaluation benchmark. Geo. represents traditional geometric methods. Sup. represents supervised methods. UnSup. represents unsupervised methods.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "In the DTU benchmark, Accuracy is measured as the distance from the result to the ground truth, encapsulating the quality of reconstruction; Completeness is measured as the distance from the ground truth to the result, encapsulating how much of the surface is captured; Overall is a the average of Accuracy and Completeness, acting as a compositive error metric. In the Tanks&Temples bench-", "figure_data": "Scan 114 Scan 77 Scan 9 Figure 5: Qualitative comparison in 3D reconstruction be-Ground Truth CVP-MVSNet JDACS(Ours) tween our JDACS and SOTA supervised method(CVP-MVSNet) on DTU dataset. From left to right: ground truth, results of supervised CVP-MVSNet, our results. Implementation Details Backbone: In default, the most concise MVSNet (Yao et al. 2018) is applied as backbone in our JDACS framework. We denote the framework as JDACS-MS if a multi-stage MVS-Net like CVP-MVSNet (Yang et al. 2020) is selected as backbone. Training and Testing: During the training phase, we only use the training set of DTU without any ground truth depth maps. Our proposed JDACS 1 is implemented in Pytorch and trained on 4 NVIDIA RTX 2080Ti GPUs. In default, the hyper-parameters during training and testing phase follow the same setting of Unsup MVS (Khot et al. 2019). With a pattern of data-parallel, the batch size is set to 1 per GPU for JDACS and 4 per GPU for JDACS-MS, which consume no more than 10G memories in each GPU. We use Adam opti-mizer with a learning rate of 0.001 which decreases by 0.5 times for every two epochs. JDACS is trained for 10 epochs as MVSNet (Yao et al. 2018) and JDACS-MS is trained for 27 epochs as CVP-MVSNet(Yang et al. 2020). Depth Map Size Acc. Comp. Overall 288 \u00d7 216 0.456 0.646 0.551 288 \u00d7 216 0.571 0.515 0.543 1600 \u00d7 1152 0.296 0.406 0.351 Error Metrics: Method Supervised Input Size MVSNet 1152 \u00d7 864 JDACS \u00d7 1152 \u00d7 864 CVP-MVSNet 1600 \u00d7 1152 JDACS-MS \u00d7 1600 \u00d7 1152 1600 \u00d7 1152 0.398 0.318 0.358"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Comparison between the backbone networks with same settings trained by supervision and our JDACS self-supervision framework. Due to the GPU memory limitation, we decrease the resolution of MVSNet to 1152 \u00d7 864 as", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Ablation Study of different components in our JDACS self-supervision network.", "figure_data": "LP C LSC LDAAcc.Comp. Overall0.4645 0.4092 0.43690.4433 0.3892 0.41630.4330 0.3373 0.38510.3977 0.3177 0.3577"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Ablation Study of different components in our JDACS-MS self-supervision network. mark, F-score in each scene is calculated following the official evaluation process.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Ablation Study of different numbers of semantic clusters K.", "figure_data": "ImagesK = 2K = 4K = 6K = 8Ref ViewSrc Views"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Quantitative comparison with previous unsupervised methods without finetuning on Tanks&Temples dataset.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Data-augmentation Consistency Various transformations are adopted for generating challenging samples, such as occluding mask, Gaussian noise, blur, random jitter in brightness, color and constrast. In JDACS with a backbone of single stage MVSNet, the input is the original multi-view images and differently randomized transformation is applied to each view. In JDACS-MS with a backbone of multiple stage MVSNet, the input is an image pyramid of multiple scales and different transformation is added to different level of the image pyramid on each view.", "figure_data": "How to Avoid Overflow in GPU Memory? The pro-posed framework possesses three parallel branches whichmay lead to an overflow in GPU memory during training.It may be impracticable to train the model on a GPU with11G memory, such as GTX 1080 Ti or RTX2080 Ti. Hence,we conduct a simple trick to avoid the memory overflowby trading the GPU memory with time. In the training pro-cess, each step comprised of these three parallel branchesand loss functions is separated into two sequential train-ing step. For example, we can propagate the Depth Estima-tion branch and Co-segmentation branch in the first step,and save the estimated depth map as pseudo depth label."}], "formulas": [{"formula_id": "formula_0", "formula_text": "p j = KT (D(p j )K \u22121 p j )(1)", "formula_coordinates": [3.0, 122.66, 621.48, 169.84, 12.69]}, {"formula_id": "formula_1", "formula_text": "I i (p j ) = I i (p j )(2)", "formula_coordinates": [3.0, 141.82, 695.2, 150.68, 10.62]}, {"formula_id": "formula_2", "formula_text": "L P C = N i=2 ||(I i \u2212 I 1 ) M i || 2 + ||(\u2207I i \u2212 \u2207I 1 ) M i || 2 ||M i || 1", "formula_coordinates": [3.0, 321.26, 313.68, 233.29, 30.32]}, {"formula_id": "formula_3", "formula_text": "S i (p j ) = S i (p j )(5)", "formula_coordinates": [4.0, 140.2, 695.2, 152.3, 10.62]}, {"formula_id": "formula_4", "formula_text": "L SC = \u2212 N i=2 [ 1 ||M i || 1 HW j=1 f (S 1,j ) log(S i,j )M i,j ] (6)", "formula_coordinates": [4.0, 333.75, 106.44, 224.25, 30.32]}, {"formula_id": "formula_5", "formula_text": "L DA = 1 ||M \u03c4 \u03b8 || 1 ||(D \u2212D \u03c4 \u03b8 ) M \u03c4 \u03b8 || 2 (7)", "formula_coordinates": [4.0, 350.09, 457.6, 207.91, 23.9]}, {"formula_id": "formula_6", "formula_text": "L = \u03bb 1 L P C + \u03bb 2 L SC + \u03bb 3 L DA +\u03bb 4 L SSIM + \u03bb 5 L Smooth (8)", "formula_coordinates": [5.0, 106.43, 608.95, 186.07, 23.6]}, {"formula_id": "formula_7", "formula_text": "P \u2265 0, Q \u2265 0; for each iterative step v, 1 \u2264 v \u2264 ite max do Q v+1 [i,j] \u2190 Q v [i,j]", "formula_coordinates": [10.0, 63.96, 199.97, 228.54, 51.42]}, {"formula_id": "formula_8", "formula_text": "((P v ) t P v Q v ) [i,j] P v+1 [i,j] \u2190 P v [i,j] A(Q v+1 ) t [i,j] (P v Q v+1 (Q v+1 ) t ) [i,j] if A \u2212 P v+1 Q v+1", "formula_coordinates": [10.0, 73.93, 245.31, 137.58, 50.53]}, {"formula_id": "formula_9", "formula_text": "L JDACS\u2212M S = 5 s=1 (\u03bb 1 L s P C + \u03bb 2 L s SC + \u03bb 3 L s DA +\u03bb 4 L SSIM + \u03bb 5 L Smooth )(9)", "formula_coordinates": [10.0, 65.26, 473.29, 227.24, 44.35]}], "doi": ""}