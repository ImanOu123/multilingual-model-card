{"title": "Compositional Generalization without Trees using Multiset Tagging and Latent Permutations", "authors": "Matthias Lindemann; Alexander Koller; Ivan Titov", "pub_date": "", "abstract": "Seq2seq models have been shown to struggle with compositional generalization in semantic parsing, i.e. generalizing to unseen compositions of phenomena that the model handles correctly in isolation. We phrase semantic parsing as a two-step process: we first tag each input token with a multiset of output tokens. Then we arrange the tokens into an output sequence using a new way of parameterizing and predicting permutations. We formulate predicting a permutation as solving a regularized linear program and we backpropagate through the solver. In contrast to prior work, our approach does not place a priori restrictions on possible permutations, making it very expressive. Our model outperforms pretrained seq2seq models and prior work on realistic semantic parsing tasks that require generalization to longer examples. We also outperform non-tree-based models on structural generalization on the COGS benchmark. For the first time, we show that a model without an inductive bias provided by trees achieves high accuracy on generalization to deeper recursion depth. 1   ", "sections": [{"heading": "Introduction", "text": "Sequence-to-sequence models have been very successfully applied to many structural tasks in NLP such as semantic parsing. However, they have also been shown to struggle with compositional generalization (Lake and Baroni, 2018;Finegan-Dollak et al., 2018;Kim and Linzen, 2020;Hupkes et al., 2020), i.e. the model fails on examples that contain unseen compositions or deeper recursion of phenomena that it handles correctly in isolation. For example, a model which correctly parses 'Mary knew that Jim slept' should also be able to parse sentences with deeper recursion than it has seen during training such as 'Paul said that Mary knew that Jim slept'. This sort of generalization is easy for humans but challenging for neural models.\nFigure 1: We model seq2seq tasks by first predicting a multiset (dashed boxes) for every input token, and then permuting the tokens to put them into order. We realize the permutation using the red edges: if there is an edge from token i to j, then i is the predecessor of j in the output. Every token is visited exactly once.\nIn order for a model to generalize compositionally in semantic parsing, it has to identify reusable 'fragments' and be able to systematically combine them in novel ways. One way to make a model sensitive to fragments is to make it rely on a tree that makes the compositional structure explicit. However, this complicates the training because these trees need to be obtained or induced. This is computationally demanding or at least requires structural preprocessing informed by domain knowledge.\nIn this paper, we propose a two-step sequencebased approach with a structural inductive bias that does not rely on trees: the 'fragments' are multisets of output tokens that we predict for every input token in a first step. A second step then arranges the tokens we predicted in the previous step into a single sequence using a permutation model. In contrast to prior work Lindemann et al., 2023), there are no hard constraints on the permutations that our model can predict. We show that this enables structural generalization on tasks that go beyond the class of synchronous context-free languages.\nWe overcome two key technical challenges in this work: Firstly, we do not have supervision for the correspondence between input tokens and output tokens. Therefore, we induce the correspondence during training. Secondly, predicting permutations without restrictions is computationally challenging. For this, we develop a differentiable GPU-friendly algorithm.\nOn realistic semantic parsing tasks our approach outperforms previous work on generalization to longer examples than seen at training. We also outperform all other non-tree models on the structural generalization tasks in semantic parsing on COGS (Kim and Linzen, 2020). For the first time, we also show that a model without an inductive bias towards trees can obtain high accuracy on generalization to deeper recursion on COGS.\nTo summarize, our main contributions are:\n\u2022 a flexible seq2seq model that performs well on structural generalization in semantic parsing without assuming that input and output are related to each other via a tree structure.\n\u2022 a differentiable algorithm to parameterize and predict permutations without a priori restrictions on what permutations are possible.", "publication_ref": ["b29", "b15", "b23", "b21", "b31", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Overview and Motivation", "text": "Our approach consists of two stages. In the first stage (multiset tagging), we predict a multiset z i of tokens for every input token x i from the contextualized representation of x i . This is motivated by the observation that input tokens often systematically correspond to a fragment of the output (like slept corresponding to sleep and agent and a variable in Fig. 1). Importantly, we expect this systematic relationship to be largely invariant to phrases being used in new contexts or deeper recursion. We refer to the elements of the multisets as multiset tokens.\nIn the second stage (permutation), we order the multiset tokens to arrive at a sequential output. Conceptually, we do this by going from left to right over the output and determining which multiset token to put in every position. Consider the example in Fig. 1. For the first output position, we simply select a multiset token (* in the example). All subsequent tokens are put into position by 'jumping' from the token that was last placed into the output to a new multiset token. In Fig. 1, we jump from * to girl (shown by the outgoing red edge from *). This indicates that girl is the successor of * in the output and hence the second output token. From girl we jump to one of the x 1 tokens to determine what the third output token is and so on. Since we predict a permutation, we must visit every multiset token exactly once in this process.\nThe jumps are inspired by reordering in phrasebased machine translation (Koehn et al., 2007) and methods from dependency parsing, where directly modeling bi-lexical relationships on hidden states has proven successful (Kiperwasser and Goldberg, 2016). Note also that any permutation can be represented with jumps. In contrast, prior work Lindemann et al., 2023) has put strong restrictions on the possible permutations that can be predicted. Our approach is more flexible and empirically it also scales better to longer inputs, which opens up new applications and datasets.\nSetup.\nWe assume we are given a dataset D = {(x 1 , y 1 ), . . .} of input utterances x and target sequences y. If we had gold alignments, it would be straightforward to train our model. Since we do not have this supervision, we have to discover during training which tokens of the output y belong into which multiset z i . We describe the model and the training objective of the multiset tagging in Section 3. After the model is trained, we can annotate our training set with the most likely z, and then train the permutation model.\nFor predicting a permutation, we associate a score with each possible jump and search for the highest-scoring sequence of jumps. We ensure that the jumps correspond to a permutation by means of constraints, which results in a combinatorial optimization problem. The flexibility of our model and its parametrization come with the challenge that this problem is NP-hard. We approximate it with a regularized linear program which also ensures differentiability. Our permutation model and its training are described in Section 4. In Section 5, we discuss how to solve the regularized linear program and how to backpropagate through the solution.", "publication_ref": ["b26", "b25", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "Learning Multisets", "text": "For the multiset tagging, we need to train a model that predicts the multisets z 1 , . . . , z n of tokens by conditioning on the input. We represent a multiset z i as an integer-valued vector that contains for every vocabulary item v the multiplicity of v in z i , i.e. z i,v = k means that input token i contributes k occurrences of output type v. If v is not present in multiset z i , then z i,v = 0. For example, in Fig. 1, z 3,sleep = 1 and z 2,x 1 = 2. As discussed in Sec-tion 2, we do not have supervision for z 1 , . . . , z n and treat them as latent variables. To allow for efficient exact training, we assume that all z i,v are independent of each other conditioned on the input:\nP (z | x) = i,v P (z i,v | x) (1)\nwhere v ranges over the entire vocabulary.\nParametrization. We parameterize P (z i,v | x) as follows. We first pass the input x through a pretrained RoBERTa encoder (Liu et al., 2019), where ENCODER(x) is the output of the final layer. We then add RoBERTa's static word embeddings from the first, non-contextualized, layer to that:\nh i = ENCODER(x) i + EMBED(x i ) (2)\nWe then pass h i through a feedforward network obtainingh i = FF(h i ) and define a distribution over the multiplicity of v in the multiset z i :\nP (z i,v = k|x i ) = exp h T i w v,k + b v,k l exp h T i w v,l + b v,l(3)\nwhere the weights w and biases b are specific to v and the multiplicity k. In contrast to standard sequence-to-sequence models, this softmax is not normalized over the vocabulary but over the multiplicity, and we have distinct distributions for every vocabulary item v. Despite the independence assumptions in Eq. (1), the model can still be strong because h i takes the entire input x into account.\nTraining. The probability of generating the overall multiset m as the union of all z i is the probability that for every vocabulary item v, the total number of occurrences of v across all input positions i sums to m v :\nP (m|x) = v P (z 1,v + . . . + z n,v = m v |x)\nThis can be computed recursively:\nP (z 1,v + . . . + z n,v = m v | x) = k P (z 1,v = k|x)P (z 2,v + . . . z n,v = m v \u2212 k|x)\nLet m(y) be the multiset of tokens in the gold sequence y. We train our model with gradient ascent to maximize the marginal log-likelihood of m(y):\n(x,y)\u2208D log P (m(y) | x)(4)\nLike Lindemann et al. (2023), we found it helpful to initialize the training of our model with high-confidence alignments from an IBM-1 model (Brown et al., 1993) (see Appendix C.3 for details). Preparation for permutation. The scoring function of the permutation model expects a sequence as input. There is no a priori obvious order for the elements within the individual multisets z i . We handle this by imposing a canonical order ORDER(z i ) on the elements of z i by sorting the multiset tokens by their id in the vocabulary. They are then concatenated to form the input z \u2032 = ORDER(z 1 ) . . . ORDER(z n ) to the permutation model.", "publication_ref": ["b33", "b31", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Relaxed Permutations", "text": "After predicting a multiset for every input token and arranging the elements within each multiset to form a sequence z \u2032 , we predict a permutation of z \u2032 .\nWe represent a permutation as a matrix V that contains exactly one 1 in every row and column and zeros otherwise. We write V i,j = 1 if position i in z \u2032 is mapped to position j in the output y. Let P be the set of all permutation matrices. Now we formalize the parametrization of permutations as discussed in Section 2. We associate a score predicted by our neural network with each permutation V and search for the permutation with the highest score. The score of a permutation decomposes into a sum of scores for binary 'features' of V. We use two types of features.\nThe first type of feature is active if the permutation V maps input position i to output position j (i.e. V ij = 1). We associate this feature with the score s i \u2192j and use these scores only to model what the first and the last token in the output should be. That is, s i \u21921 models the preference to map position i in the input to the first position in the output, and analogously s i \u2192n models the preference to put i into the last position in the output. For all output positions j that are neither the first nor the last position, we simply set s i \u2192j = 0.\nThe second type of feature models the jumps we introduced in Section 2. We introduce a feature that is 1 iff V contains a jump from k to i, and associate this with a score s k\u21b7i . In order for there to be a jump from k to i, the permutation V must map input i to some output position j (V ij = 1) and it must also map input position k to output position j \u2212 1 (V k,j\u22121 = 1). Hence, the product V k,j\u22121 V ij is 1 iff there is a jump from k to i at output position j. Based on this, the sum j V k,j\u22121 V ij equals 1 if there is any output position j at which there is a jump from k to i and 0 otherwise. This constitutes the second type of feature.\nMultiplying the features with their respective scores, we want to find the highest-scoring permutation under the following overall scoring function:\narg max V\u2208P i,j V ij s i \u2192j + i,k s k\u21b7i \uf8eb \uf8ed j V k,j\u22121 V ij \uf8f6 \uf8f8 (5)\nLet V * (s) be the solution to Eq. (5) as a function of the scores. Unfortunately, V * (s) does not have sensible derivatives because P is discrete. This makes V * (s) unsuitable as a neural network layer. In addition, Eq. ( 5) is NP-hard (see Appendix A.1).\nWe now formulate an optimization problem that approximates Eq. (5) and which has useful derivatives. Firstly, we relax the permutation matrix V to a bistochastic matrix U, i.e. U has non-negative elements and every row and every column each sum to 1. Secondly, note that Eq. (5) contains quadratic terms. As we will discuss in the next section, our solver assumes a linear objective, so we replace V k,j\u22121 V ij with an auxiliary variable W ijk . The variable W ijk is designed to take the value 1 if and only if U i,j = 1 and U k,j\u22121 = 1. We achieve this by coupling W and U using constraints. Then, the optimization problem becomes:\narg max U,W i,j U ij s i \u2192j + i,j,k W ijk s k\u21b7i (6) subject to i U ij = 1 \u2200j (6a) j U ij = 1 \u2200i (6b) k W ijk = U ij \u2200j > 1, i (6c) i W ijk = U k(j\u22121) \u2200j > 1, k (6d) U, W \u2265 0 (6e)\nFinally, in combination with the linear objective, the argmax operation still causes the solution U * (s) of Eq. ( 6) as a function of s to have no useful derivatives. This is because an infinitesimal change in s has no effect on the solution U * (s) for almost all s.\nTo address this, we add an entropy regularization term \u03c4 (H(U) + H(W)) to the objective Eq. ( 6), where H(U) = \u2212 ij U ij (log U ij \u22121), and \u03c4 > 0 determines the strength of the regularization. The entropy regularization 'smooths' the solution U * (s) in an analogous way to softmax being a smoothed version of argmax. The parameter \u03c4 behaves analogously to the softmax temperature: the smaller \u03c4 , the sharper U * (s) will be. We discuss how to solve the regularized linear program in Section 5.\nPredicting permutations. At test time, we want to find the highest scoring permutation, i.e. we want to solve Eq. (5). We approximate this by using U * (s) instead, the solution to the entropy regularized version of Eq. (6). Despite using a low temperature \u03c4 , there is no guarantee that U * (s) can be trivially rounded to a permutation matrix. Therefore, we solve the linear assignment problem with U * (s) as scores using the Hungarian Algorithm (Kuhn, 1955). The linear assignment problem asks for the permutation matrix V that maximizes ij V ij U * (s) ij .", "publication_ref": ["b27"], "figure_ref": [], "table_ref": []}, {"heading": "Parametrization", "text": "We now describe how we parameterize the scores s to permute the tokens into the right order. We first encode the original input utterance x like in Eq. (2) to obtain a hidden representation h i for input token x i . Let a be the function that maps a(i) \u2192 j if the token in position i in z \u2032 came from the multiset that was generated by token x j . For example, in Fig. 1, a(6) = 3 since sleep was predicted from input token slept. We then define the hidden representation h \u2032 i as the concatenation of h a(i) and an embedding of z \u2032 i :\nh \u2032 i = h a(i) ; EMBED(z \u2032 i )(7)\nWe parameterize the scores for starting the output with token i as\ns i \u21921 = w T start FF start (h \u2032 i )\nand analogously for ending it with token i:\ns i \u2192n = w T end FF end (h \u2032 i )\nWe set s i \u2192j = 0 for all other i, j.\nWe parameterize the jump scores s k\u21b7i using Geometric Attention (Csord\u00e1s et al., 2022) \nfrom h \u2032 k to h \u2032 i .\nIntuitively, Geometric Attention favours selecting the 'matching' element h \u2032 i that is closest to h \u2032 k in terms of distance |i \u2212 k| in the string. We refer to Csord\u00e1s et al. (2022) for details. ", "publication_ref": ["b8", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Learning Permutations", "text": "We now turn to training the permutation model. At training time, we have access to the gold output y and a sequence z \u2032 from the output of the multiset tagging (see the end of Section 3). We note that whenever y (or z \u2032 ) contains one vocabulary item at least twice, there are multiple permutations that can be applied to z \u2032 to yield y. Many of these permutations will give the right result for the wrong reasons and the permutation that is desirable for generalization is latent. For example, consider Fig. 2. The token agent is followed by the entity who performs the action, whereas theme is followed by the one affected by the action. The permutation indicated by dashed arrows generalizes poorly to a sentence like Emma knew that James slid since slid will introduce a theme role rather than an agent role (as the sliding is happening to James). Thus, this permutation would then lead to the incorrect output know theme Emma ... slide agent James, in which Emma is affected by the knowing event and James is the one who slides something.\nIn order to train the permutation model in this setting, we use a method that is similar to EM for structured prediction. 2 During training, the model output U * (s) and W * (s) often represents a soft permutation that does not permute z \u2032 into y. Our goal is to push the model output into the space of (soft) permutations that lead to y. More formally, let Q \u2208 Q(y, z \u2032 ) be the set of bistochastic matrices such that Q i,j = 0 iff z \u2032 i \u0338 = y j . That is, any permutation included in Q(y, z \u2032 ) leads to the gold output y when applied to z \u2032 . First, we project the current prediction U * (s) and W * (s) into Q(y, z \u2032 ) using the KL divergence 2 It can also be derived as a lower bound to the marginal loglikelihood log P \u03b8 (y|x, z \u2032 ) (see Appendix A.4), and be seen as a form of posterior regularization (Ganchev et al., 2010) as a measure of distance (E-step):\nU,\u0174 = arg min U\u2208Q(y,z \u2032 ),W KL(U || U * (s))+ (8) KL(W || W * (s))\nsubject to Eq. (6a) to Eq. (6e).\nSimilar to the M-step of EM, we then treat\u00db and\u0174 as soft gold labels and train our model to minimize the KL divergence between labels and model:\nKL(\u00db || U * (s)) + KL(\u0174 || W * (s))\nEq. ( 8) can be solved in the same way as the entropy regularized version of Eq. ( 6) because expanding the definition of KL-divergence leads to a regularized linear program with a similar feasible region (see Appendix A.3 for details).", "publication_ref": ["b16"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Inference for Relaxed Permutations", "text": "Now we describe how to solve the entropy regularized form of Eq. (6) and how to backpropagate through it. This section may be skipped on the first reading as it is not required to understand the experiments; we note that the resulting algorithm (Algorithm 1) is conceptually relatively simple. Before describing our method, we explain the general principle.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Bregman's Method", "text": "Bregman's method (Bregman, 1967) is a method for constrained convex optimization. In particular, it can be used to solve problems of the form\nx * = arg max x\u2208C 0 \u2229C 1 ...\u2229Cn,x\u22650 s T x + \u03c4 H(x) regularizer (9\n)\nwhere C 0 , . . . , C n are linear equality constraints,\nH(x) = \u2212 i x i (log x i \u2212 1\n) is a form of entropy regularization, and \u03c4 determines the strength of the regularization. Note that our parameterization of permutations (Eq. ( 6)) has this form. Bregman's method is a simple iterative process. We start with the scores s and then cyclically iterate over the constraints and project the current estimate x i onto the chosen constraint until convergence:\nx 0 = exp s \u03c4 x i+1 = arg min x\u2208C i mod (n\u22121) KL(x || x i ) (10\n)\nwhere KL( ) for all C 0 , . . . , C n in closed-form. We discuss how to do this for Eq. (6) in the next section.\nx | y) = i x i log x i y i \u2212 x i + y i is the generalized KL divergence.\nAs an example, consider a problem of the form Eq. ( 9) with a single linear constraint C \u2206 = {x | i x i = 1}. In this case, Bregman's method coincides with the softmax function. This is because the KL projection\nx * = arg min x\u2208C \u2206 KL(x || y) for y > 0 has the closed-form solution x * i = y i i y i .\nIf we have a closed-form expression for a KL projection (such as normalizing a vector), we can use automatic differentiation to backpropagate through it. To backpropagate through the entire solver, we apply automatic differentiation to the composition of all projection steps.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Bregman's Method for Eq. (6)", "text": "In order to apply Bregmans' method to solve the entropy regularized version of Eq. (6), we need to decompose the constraints into sets which we can efficiently project onto. We choose the following three sets here: (i), containing Eq. (6a) and Eq. (6c), and (ii), containing Eq. (6a) and Eq. (6d), and finally (iii), containing only Eq. (6b). We now need to establish what the KL projections are for our chosen sets. For (iii), the projection is simple: Proposition 1. (Benamou et al. (2015), Prop. 1) For A, m > 0, the KL projection\narg min U KL(U || A) subject to j U ij = m i is given by U * ij = m i A ij j \u2032 A ij \u2032 .\nLet us now turn to (i) and (ii). The constraints Eq. (6d) and Eq. (6c) are structurally essentially the same, meaning that we can project onto (ii) in basically the same manner as onto (i). We project onto (i), with the following proposition: Proposition 2. For A, B > 0, the KL projection\narg min U, W KL(U || A) + KL(W || B) subject to i U ij = 1 \u2200j k W ijk = U ij \u2200j, i(11)\nis given by:\nU * ij = T ij i \u2032 T i \u2032 j W * ijk = U * ij B ijk k \u2032 B ijk \u2032 where T ij = A ij \u2022 k B ijk .\nThe proof can be found in Appendix A.2. We present the overall algorithm in Algorithm 1, and note that it is easy to implement for GPUs. In practice, we implement all projections in log space for numerical stability.", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Doubling Task", "text": "Our permutation model is very expressive and is not limited to synchronous context-free languages. This is in contrast to the formalisms that other approaches rely on Lindemann et al., 2023). To evaluate if our model can structurally generalize beyond the synchronous contextfree languages in practice, we consider the function F = {(w, ww) | w \u2208 \u03a3 * }. This function is related to processing challenging natural language phenomena such as reduplication and cross-serial dependencies. We compare our model with an LSTM-based seq2seq model with attention and a Transformer in the style of Csord\u00e1s et al. (2021) that uses a relative positional encoding. Since the input is a sequence of symbols rather than English, we replace RoBERTa with a bidirectional LSTM and use randomly initialized embeddings. The models are trained on inputs of lengths 5 to 10 and evaluated on longer examples. The results can be found in Fig. 3. All models get perfect or close to perfect accuracy on inputs of length 11 but accuracy quickly deteriorates for the LSTM and the Transformer. In contrast, our model extrapolates very well to longer sequences.", "publication_ref": ["b31"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "COGS", "text": "COGS is a synthetic benchmark for compositional generalization introduced by Kim and Linzen  (2020). Models are tested for 21 different cases of generalization, 18 of which focus on using a lexical item in new contexts (Lex). There are 1000 instances per generalization case. Seq2seq models struggle in particular with the structural generalization tasks (Yao and Koller, 2022), and we focus on those: (i) generalization to deeper PP recursion than seen during training (\"Emma saw a hedgehog on a chair in the garden beside the tree...\"), (ii) deeper CP recursion (\"Olivia mentioned that James saw that Emma admired that the dog slept\"), and (iii) PPs modifying subjects when PPs modified only objects in the training data (OS).\nWe follow previous work and use a lexicon (Akyurek and Andreas, 2021) to map some input tokens to output tokens (see Appendix C.2 for details). We also use this mechanism to handle the variable symbols in COGS.\nWe report the means and standard deviations for 10 random seeds in Table 1. Our approach obtains high accuracy on CP and PP recursion but exact match accuracy is low for OS. This is in part because our model sometimes predicts semantic representations for OS that are equivalent to the gold standard but use a different order for the conjuncts. Therefore, we report accuracy that accounts for this in Table 2. In both tables, we also report the impact of using a simple copy mechanism instead of the more complex lexicon induction mechanism (-Lex). Our model outperforms all other non-treebased models by a considerable margin.\nStructural generalization without trees. All previous methods that obtain high accuracy on recursion generalization on COGS use trees. Some approaches directly predict a tree over the input (Liu et al., 2021;Wei\u00dfenhorn et al., 2022), while others use derivations from a grammar for data augmentation (Qiu et al., 2022) or decompose the input along a task-specific parse tree (Drozdov et al., 2022). Our results show that trees are not as important for compositional generalization as their success in the literature may suggest, and that weaker   Drozdov et al. (2022). LexLSTM is the model by Akyurek and Andreas (2021) and Dangle is the model by Zheng and Lapata (2021). We report their results for T5.  structural assumptions already reap some of the benefits.", "publication_ref": ["b44", "b0", "b32", "b41", "b37", "b13", "b13", "b0", "b45"], "figure_ref": [], "table_ref": ["tab_2", "tab_4"]}, {"heading": "Model", "text": "Logical forms with variables. COGS uses logical forms with variables, which were removed in conversion to variable-free formats for evaluation of some approaches (Zheng and Lapata, 2021;Qiu et al., 2022;Drozdov et al., 2022). Recently, Wu et al. (2023) have argued for keeping the variable symbols because they are important for some semantic distinctions; we keep the variables.", "publication_ref": ["b45", "b37", "b13", "b43"], "figure_ref": [], "table_ref": []}, {"heading": "ATIS", "text": "While COGS is a good benchmark for compositional generalization, the data is synthetic and does not contain many phenomena that are frequent in semantic parsing on real data, such as paraphrases that map to the same logical form. ATIS (Dahl et al., 1994) is a realistic English semantic parsing dataset with executable logical forms. We follow the setup of our previous work (Lindemann et al., 2023)   resentation (Guo et al., 2020). Apart from the usual iid split, we evaluate on a length split, where a model is trained on examples with few conjuncts and has to generalize to longer logical forms with more conjuncts. For a fair comparison with previous work, we do not use the lexicon/copying. We also evaluate a version of our model without RoBERTa that uses a bidirectional LSTM and GloVe embeddings instead. This mirrors the model of L'23.\nTable 3 shows mean accuracy and standard deviations over 5 runs. Our model is competitive with non-pretrained models in-distribution, and outperforms all other models on the length generalization. The high standard deviation on the length split stems from an outlier run with 18% accuracythe second worst-performing run achieved an accuracy of 44%. Even without pretraining, our model performs very well. In particular, without grammarbased decoding our model performs on par or outperforms L'23 with grammar-based decoding.\nThe runtime of the model in L'23 is dominated by the permutation model and it takes up to 12 hours to train on ATIS. Training the model presented here only takes around 2 hours for both stages.\nPerformance breakdown. In order for our approach to be accurate, both the multiset tagging model and the permutation model have to be accurate. Table 4 explores which model acts as the bottleneck in terms of accuracy on ATIS and COGS. The answer depends on the dataset: for the synthetic COGS dataset, predicting the multisets correctly is easy except for OS, and the model struggles more with getting the permutation right. In contrast, for ATIS, the vast majority of errors can be attributed to the first stage.  ", "publication_ref": ["b9", "b31", "b18"], "figure_ref": [], "table_ref": ["tab_6", "tab_7"]}, {"heading": "Okapi", "text": "Finally, we consider the recent Okapi (Hosseini et al., 2021) semantic parsing dataset, in which an English utterance from one of three domains (Calendar, Document, Email) has to be mapped to an API request. We again follow the setup of L'23 and evaluate on their length split, where a model has to generalize to longer logical forms. In contrast to all other datasets we consider, Okapi is quite noisy because it was collected with crowd workers. This presents a realistic additional challenge on top of the challenge of structural generalization.\nThe results of 5 runs can be found in Table 5. Our model outperforms both BART (Lewis et al., 2020) and the model of L'23. In the comparison without pretraining, our model also consistently achieves higher accuracy than the comparable model of L'23 without grammar-based decoding.", "publication_ref": ["b20", "b30"], "figure_ref": [], "table_ref": ["tab_8"]}, {"heading": "Related Work", "text": "Predicting permutations. Mena et al. (2018) and Lyu and Titov (2018) use variational autoencoders based on the Sinkhorn algorithm to learn latent permutations. The Sinkhorn algorithm (Sinkhorn, 1964) is also an instance of Bregman's method and solves the entropy regularized version of Eq. (6) without the W-term. This parameterization is con-siderably weaker than ours since it cannot capture our notion of 'jumps'.  compute soft permutations as an expected value by marginalizing over the permutations representable by ITGs (Wu, 1997). This approach is exact but excludes some permutations. In particular, it excludes permutations needed for COGS. 3 In addition, the algorithm they describe takes a lot of resources as it is both O(n 5 ) in memory and compute. Devatine et al. (2022) investigate sentence reordering methods. They use bigram scores, which results in a similar computational problem to ours. However, they deal with it by restricting what permutations are possible to enable tractable dynamic programs. Eisner and Tromble (2006) propose local search methods for decoding permutations for machine translation.\nOutside of NLP, Kushinsky et al. (2019) have applied Bregman's method to the quadratic assignment problem, which Eq. ( 5) is a special case of. Since they solve a more general problem, using their approach for Eq. (6) would require O(n 4 ) rather than O(n 3 ) variables in the linear program.\nCompositional generalization. Much research on compositional generalization has focused on lexical generalization with notable success (Andreas, 2020;Akyurek and Andreas, 2021;Conklin et al., 2021;Csord\u00e1s et al., 2021). Structural generalization remains more challenging for seq2seq models (Yao and Koller, 2022). Zheng and Lapata (2022) modify the transformer architecture and re-encode the input and partially generated output for every decoding step to disentangle the information in the representations. Structure has also been introduced in models by means of grammars: Qiu et al. (2022) heuristically induce a quasi-synchronous grammar (QCFG, Smith and Eisner (2006)) and use it for data augmentation for a seq2seq model. Kim (2021) introduces neural QCFGs which perform well on compositional generalization tasks but are very compute-intensive. Other works directly parse into trees or graphs inspired by methods from syntactic parsing (Liu et al., 2021;Herzig and Berant, 2021;Wei\u00dfenhorn et al., 2022;Jambor and Bahdanau, 2022;Petit and Corro, 2023).\nSeveral approaches, including ours, have decoupled the presence or absence of output tokens from their order:  train a model end-to-to-end to permute the input (as discussed above) and then monotonically translate it into an output sequence. Lindemann et al. (2023) also present an end-to-end differentiable model that first applies a 'fertility step' which predicts for every word how many copies to make of its representation, and then uses the permutation method of  to reorder the representation before translating them. Cazzaro et al. (2023) first translate the input monotonically and feed it into a second model. They use alignments from an external aligner to train the first model. The second model is a tagger or a pretrained seq2seq model and predicts the output as a permutation of its input. We compare against such a baseline for permutations in Appendix B, finding that it does not work as well as ours in the compositional generalization setups we consider.", "publication_ref": ["b35", "b34", "b38", "b42", "b11", "b14", "b28", "b1", "b0", "b6", "b44", "b46", "b37", "b39", "b24", "b32", "b19", "b41", "b22", "b36", "b31", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper, we have presented a flexible new seq2seq model for semantic parsing. Our approach consists of two steps: We first tag each input token with a multiset of output tokens. Then we arrange those tokens into a sequence using a permutation model. We introduce a new method to predict and learn permutations based on a regularized linear program that does not restrict what permutations can be learned. The model we present has a strong ability to generalize compositionally on synthetic and natural semantic parsing datasets. Our results also show that trees are not necessarily required to generalize well to deeper recursion than seen at training time.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "The conditional independence assumptions are a limitation for the applicability of our multiset tagging model. For example, the independence assumptions are too strong to apply it to natural language generation tasks such as summarization. From a technical point of view, the independence assumptions are important to be able to induce the latent assignment of output tokens to multisets efficiently. Future work may design multiset tagging methods that make fewer independence assumptions.\nWhile our method for predicting permutations is comparatively fast and only has a memory requirement of O(n 3 ), inference on long sequences, e.g. with more than 100 tokens, remains somewhat slow. In future work, we plan to investigate other approximate inference techniques like local search and dual decomposition.\nRegarding the importance of trees for compositional generalization, our model has no explicit structural inductive bias towards trees. However, we do not exclude that the pretrained RoBERTa model that we use as a component implicitly captures trees or tree-like structures to a certain degree.\nKL(x || z) + KL(Y * (x) || W) = KL(x || z) + i,j x i W i,j j \u2032 W i,j \u2032 (log x i\u00a8\u1e84 i,j j \u2032 W i,j \u2032\u1e84 i,j \u2212 1) = KL(x || z) + i x i $ $ $ $ $ j W i,j $ $ $ $ $ j \u2032 W i,j \u2032 (log x i j \u2032 W i,j \u2032 \u2212 1) = i x i (log( x i z i ) \u2212 1 + log x i j \u2032 W i,j \u2032 \u2212 1) = i x i (2 log x i \u2212 log z i \u2212 log \uf8eb \uf8ed j \u2032 W i,j \u2032 \uf8f6 \uf8f8 \u2212 2) = 2 i x i (log x i \u2212 1 \u2212 1 2 (log z i + log \uf8ee \uf8f0 j \u2032 W i,j \u2032 \uf8f9 \uf8fb ) log q i ) = 2 \u2022 KL(x || q)\nFigure 4: Rewriting the objective function of Eq. ( 14). We note that the generalized KL divergence KL(\nx | y) = i x i log xi y i \u2212 x i + y i simplifies to KL(x | y) = i x i (log xi y i \u2212 1)\nbecause we want to find the argmin wrt to x.\nin Prop. 2 for any value of j do not interact with constraints for other values of j, we can assume w.l.o.g. that j takes a single value only and drop it in the notation. We want to solve:\narg min x, Y KL(x || z) + KL(Y || W) subject to i x i = 1 k Y ik = x i \u2200i(13)\nWe can find Y * = arg min Y KL(Y || W) subject to j Y i,j = x i based on Prop. 1:\nY * i,j = x i W i,j j W i,j\n. That is, we can express Y * as a function of x (which we write Y * (x)), and therefore our overall problem is now a problem in one variable (x):\narg min x KL(x || z) + KL(Y * (x) || W) subject to i x i = 1 (14)\nWe can now rewrite the objective function as\narg min x KL(x || q) subject to i x i = 1 (15\n)\nwhere q i = z i \u2022 j \u2032 W i,j \u2032 .\nThis step is justified in detail in Fig. 4. The rewritten optimization problem has the right form to apply Prop. 1 a second time. We obtain:\nx * i = q i i \u2032 q i \u2032 By plugging this into Y * (x)\n, we obtain the solution to the overall optimization problem.\nA.3 Reduction of Eq. (8) to Eq. (6)\nIn this section, we show how Eq. (8) can be reduced to a problem of the entropy regularized version of Eq. (6). This is useful because it means we can use Algorithm 1 to solve Eq. (8).\nFirst, we show that computing a KL projection is equivalent to solving an entropy-regularized linear program. Let C be the feasible region of the linear constraints.\narg max x\u2208C s T x \u2212 \u03c4 i x i (log x i \u2212 1) = arg min x\u2208C \u03c4 i x i (log x i \u2212 s i \u03c4 \u2212 1) = arg min x\u2208C KL(x || exp( s \u03c4 ))\nDue to this, Eq. ( 8) is equivalent to a linear program that has the same feasible region as Eq. ( 6) except for the additional constraint U \u2208 Q(y, z \u2032 ). Note that U \u2208 Q(y, z \u2032 ) essentially rules out certain correspondences. Therefore we can approximately enforce U \u2208 Q(y, z \u2032 ) by masking U * (s) such that any forbidden correspondence receives a very low score.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.4 Derivation of loss function as ELBO", "text": "We now show how the training procedure we use to train our permutation model can be derived from a form of evidence lower bound (ELBO).\nIdeally, our permutation model would be a distribution P \u03b8 (R|x, z \u2032 ) over permutation matrices R and we would maximize the marginal likelihood, i.e. marginalizing over all permutations:\nP (y|x, z \u2032 ) = R\u2208P P \u03b8 (R|x, z \u2032 )P (y|z \u2032 , R) (16)\nwhere P (y|z \u2032 , R) = j i R ij \u2022 1(y j = z i ) with 1 being the indicator function. P (y|z \u2032 , R) returns 1 iff applying the permutation R to z \u2032 results in y. Unfortunately, computing Eq. ( 16) exactly is intractable in general due to the sum over permutation matrices. We instead use techniques from variational inference and consider the following evidence lower bound (ELBO):\nlog P (y|x, z \u2032 ) \u2265 max Q E R\u223cQ(R|x,z \u2032 ,y) log P (y|z \u2032 , R) \u2212 KL(Q(R|x, z \u2032 , y) || P \u03b8 (R|x, z \u2032 ))(17)\nwhere Q(R|x, z \u2032 , y) is an approximate variational posterior. We now relax the restriction that P (R|x, z \u2032 ) places non-zero mass only on permutation matrices and use the following definition of P \u03b8 (R|x, z \u2032 ):\nP \u03b8 (R ij = 1|x, z \u2032 ) = U * (s) ij\nwhere U * (s) is the solution to Eq. (6) with added entropy regularization. It turns out, in our case, we can easily construct a variational posterior Q that has zero reconstruction loss (the first term on the right side in Eq. ( 17)): we can choose any Q(R|x, z \u2032 , y) \u2208 Q(y, z \u2032 ) where Q(y, z \u2032 ) is the set of bistochastic matrices such that Q(R|x, z \u2032 , y) i,j = 0 iff z \u2032 i \u0338 = y j . To see that this gives zero reconstruction error, consider position j in the output: The probability mass is distributed across precisely those positions i in z \u2032 where the  Table 6: Performance of our permutation model in comparison to using BART for predicting the permutation.\nWe ignore the order of conjuncts for evaluation.\nright kind of token lives. In other words, any alignment with non-zero probability will reconstruct the output token at position j. Therefore we can use the following lower bound to the log-likelihood:\nlog P (y|x, z \u2032 ) \u2265 (18) \u2212 min Q\u2208Q(y,z) KL(Q(R|x, z \u2032 , y) || P \u03b8 (R|x, z \u2032 ))\nDuring training, we need to compute the gradient of Eq. (18). By Danskin's theorem (Danskin, 1967), this is:\n\u2212\u2207 \u03b8 KL(Q * || P \u03b8 (R|x, z \u2032 ))(19)\nwhere Q * \u2208 Q(y, z \u2032 ) is the minimizer of Eq. (18). Note that Q * can equivalently be characterised a\u015d U (Eq. ( 8)).\nIn practice, we also add \u2212KL(\u0174|W * (s)) to our objective in Eq. (18) to speed up convergence; this does not change the fact that we use a lower bound.", "publication_ref": ["b10"], "figure_ref": [], "table_ref": []}, {"heading": "B Additional results and analysis", "text": "Okapi. In Fig. 5 we show the accuracy of our model on the document domain in comparison with previous work by number of conjuncts in the logical form.\nPermutation baseline. A simpler approach for predicting a permutation of the output z \u2032 from the multiset tagging is to use a seq2seq model. In order to compare our approach to such a baseline, we concatenate the original input x with a separator token and z \u2032 . We then feed this as input to a BART-base model which is trained to predict the output sequence y. At inference time, we use beam search and enforce the output to be a permutation of the input. As detailed in Table 6, this approach works well in-distribution and it also shows a small improvement over finetuning BART directly on the length split of ATIS. However, it does not perform as well as our approach. On COGS, our model outperforms the permutation baseline by an even bigger margin. Unseen variable symbols could be a challenge for BART on COGS which might explain part of the gap in performance.\nThis approach towards predicting permutations is similar to that of Cazzaro et al. (2023) except that they do not constrain the beam search to permutations. We found that not constraining the output to be a permutation worked worse in the compositional generalization setups.", "publication_ref": ["b5"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "C Further model details C.1 Parametrization of permutation model", "text": "We do not share parameters between the multiset tagging model and the permutation model.\nTokens that appear more than once in the same multiset have the same representation h \u2032 i in Eq. (7). In order to distinguish them, we concatenate another embedding to h \u2032 i : if the token z \u2032 i is the k-th instance of its type in its multiset, we concatenate an embedding for k to h \u2032 i . For example, in Fig. 1, z \u2032 5 = 'x 1 ' and it is the second instance of 'x 1 ' in its multiset, so we use the embedding for 2.\nWe found it helpful to make the temperature \u03c4 of the scores for Algorithm 1 dependent on the number of elements in the permutation, setting \u03c4 = (log n) \u22121 , so that longer sequences have slightly sharper distributions.\nSince the permutation model is designed to model exactly permutations, during training, z and y must have the same elements. This is not guaranteed because z is the prediction of the multiset model which may not have perfect accuracy on the training data. For simplicity, we disregard instances where z and y do not have the same elements. In practice, this leads to a very small loss in training data for the permutation model.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.2 Lexicon mechanism", "text": "The lexicon L is a lookup table that deterministically maps an input token x i to an output token L(x i ), and we modify the distribution for multiset tagging as follows:\nP \u2032 (z i,v = k|x) = P (z i,L = k|x i ) if v = L(x i ) P (z i,v = k|x)else\nwhere P (z i,v = k|x) is as defined in Eq. (3) and L is a special lexicon symbol in the vocabulary. P (z i,L |x i ) is a distribution over the multiplicity of L(x i ), independent of the identity of L(x i ). We use the 'simple' lexicon induction method by Akyurek and Andreas (2021). Unless otherwise specified during learning, L(x i ) = x i like in a copy mechanism.\nHandling of variables in COGS. For the COGS dataset, a model has to predict variable symbols. The variables are numbered (0-based) by the input token that introduced it (e.g. in Fig. 1, slept, the third token, introduces a variable symbol x 2 ). In order to robustly predict variable symbols for sentences with unseen length, we use a similar mechanism as the lexicon look up table: we introduce another special symbol in the vocabulary, Var. If Var is predicted with a multiplicity of k at i-th input token, it adds the token x i\u22121 to its multiset k times.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "C.3 Initialization of Multiset Tagging model", "text": "If there are l alignments with a posterior probability of at least \u03c7 that an input token i produces token v, we add the term \u03bb log P (z i,v \u2265 l | x) to Eq. (4). \u03bb is the hyperparameter determining the strength. This additional loss is only used during the first g epochs.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D Datasets and Preprocessing", "text": "We show basic statistics about the data we use in Table 7. Except for the doubling task, all our datasets are in English. COGS uses a small fragment of English generated by a grammar, see Kim and Linzen (2020) for details.\nDoubling task. For the doubling task, we use an alphabet of size |\u03a3| = 11. To generate inputs with a specific range of lengths, we first draw a length from the range uniformly at random. The symbols in the input are also drawn uniformly at random and then concatenated into a sequence.   Drozdov et al. (2022) we do not apply structural preprocessing to the original COGS meaning representation and keep the variable symbols: all our preprocessing is local and aimed at reducing the length of the logical form (to keep runtimes low). We delete any token in {\",\",\"_\",\"(\",\")\",\"x\",\".\",\";\",\"AND\"} as these do not contribute to the semantics and can be reconstructed easily in post-processing. The tokens {\"agent\", \"theme\", \"recipient\", \"ccomp\", \"xcomp\", \"nmod\", \"in\", \"on\", \"beside\"} are always preceded by a \".\" and we merge \".\" and any of those tokens into a single token. Example: * cookie ( x _ 3 ) ; * table ( x _ 6 ) ; lend . agent ( x _ 1 , Dylan ) AND lend . theme ( x _ 1 , x _ 3 ) AND lend . recipient ( x _ 1 , x _ 9 ) AND cookie . nmod . beside ( x _ 3 , x _ 6 ) AND girl ( x _ 9 ) Becomes * cookie 3 * table 6 lend .agent 1 Dylan lend .theme 1 3 lend .recipient 1 9 cookie .nmod .beside 3 6 girl 9   The chosen hyperparameters along with the search space are provided in the github repository.", "publication_ref": ["b23", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "G Number of parameters, computing infrastructure and runtime", "text": "We show the number of parameters in the models we train in Table 9.\nAll experiments were run on GeForce GTX 1080 Ti or GeForce GTX 2080 Ti with 12GB RAM and Intel Xeon Silver or Xeon E5 CPUs.\nThe runtime of one run contains the time for training, evaluation on the devset after each epoch and running the model on the test set. We show runtimes of the model we train in Table 8. Since we evaluate on 5 random seeds (10 for COGS due to high variance of results), our experiments overall took around 64 hours of compute time on our computing infrastructure.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_14", "tab_13"]}, {"heading": "Acknowledgements", "text": "We thank Bailin Wang and Jonas Groschwitz for technical discussions; we thank Hao Zheng for discussions and for providing system outputs for further analysis. We also say thank you to Christine Sch\u00e4fer and Agostina Calabrese for their comments on this paper.\nML is supported by the UKRI Centre for Doctoral Training in Natural Language Processing, funded by the UKRI (grant EP/S022481/1), the University of Edinburgh, School of Informatics and School of Philosophy, Psychology & Language Sciences, and a grant from Huawei Technologies. IT is supported by the Dutch National Science Foundation (NWO Vici VI.C.212.053). ATIS. We follow the pre-procressing by Lindemann et al. (2023) and use the variable-free FunQL representation as annotated by Guo et al. (2020). We use spacy 3.0.5 (model en_core_web_sm) to tokenize the input.\nOkapi. Again, we follow the preprocessing of Lindemann et al. (2023). We use spacy 3.0.5 (model en_core_web_sm) to tokenize both the input utterances and the output logical forms.", "publication_ref": ["b31", "b18", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "E Details on evaluation metrics", "text": "We provide code for all evaluation metrics in our repository.\nDoubling. We use exact match accuracy on the string.\nCOGS. For COGS we use exact match accuracy on the sequence in one evaluation setup. The other evaluation setup disregards the order of conjuncts: we first remove the 'preamble' (which contains all the definite descriptions) from the conjunctions. We count a prediction as correct if the set of definite descriptions in the preamble matches the set of definite descriptions in the gold logical form and the set of clauses in the prediction match the set of clauses in the gold logical form.\nATIS. We allow for different order of conjuncts between system output and gold parse in computing accuracy. We do this by sorting conjuncts before comparing two trees node by node. This is the same evaluation metric as used by Lindemann et al. (2023).\nOkapi. We follow Hosseini et al. (2021); Lindemann et al. (2023) and disregard the order of the parameters for computing accuracy. We use a case-insensitive string comparison.", "publication_ref": ["b31", "b20", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "A Math Details", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1 NP-hardness", "text": "We show that Eq. (5) can be used to decide the Hamiltonian Path problem. Let G = (V, E) be a graph with nodes V = {1, 2, . . . , n}. A Hamiltonian path P = v 1 , v 2 , . . . , v n is a path in G (i.e. (v i , v i+1 ) \u2208 E for all i) such that each node of G appears exactly once. Deciding if a graph has a Hamiltonian path is NP-complete.\nReduction of Hamiltonian path to Eq. (5). Note that a necessary but not sufficient condition for P to be a Hamiltonian path is that P is a permutation of V . This will be ensured by the constraints on the solution in Eq. (5).\nWe construct a score function\nand let s i \u2192j = 0 for all i, j. If we find the solution of Eq. (5) for the score function Eq. (12), we obtain a permutation P of V , which may or may not be a path in G. In a path of n nodes, there are n \u2212 1 edges that are crossed. If the score of the solution is n \u2212 1, then all node pairs (v i , v i+1 ) that are adjacent in P must have had a score of 1, indicating an edge (v i , v i+1 ) \u2208 E. Therefore, P must be a Hamiltonian path. If the score of the solution is less than n \u2212 1, then there is no permutation of V that is also a path, and hence G has no Hamiltonian path.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Proof of Proposition 2", "text": "We now prove Prop. 2 using a very similar technique as Kushinsky et al. (2019). As the constraints", "publication_ref": ["b28"], "figure_ref": [], "table_ref": []}, {"heading": "F Hyperparameters", "text": "We use the same hyperparameters for all splits of a dataset. For our model, we only tune the hyperparameters of the multiset tagging model; the permutation model is fixed, and we use the same configuration for all tasks where we use RoBERTa.\nFor model ablations where we use an LSTM instead of RoBERTa, we use the same hyperparameters for Okapi and ATIS, and a smaller model for the doubling task. These configurations were determined by hand without tuning. For BART, we use the same hyperparameter as Lindemann et al. (2023). We follow the random hyperparameter search procedure of Lindemann et al. (2023) for the multiset tagging models and the LSTM/transformer we train from scratch: we sample 20 configurations and evaluate them on the development set. We run the two best-performing configurations again with a different random seed and pick the one with the highest accuracy (comparing the union of the predicted multisets with the gold multiset). We then train and evaluate our model with entirely different random seeds. The data we create programmatically is likely too simple to be protected by copyright.\nB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? 6 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Not applicable. Left blank.\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? 6, Appendix D B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. 6, Appendix D C Did you run computational experiments? 6 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? Appendix H\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance. Appendix D D Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank. D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? No response.\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)? No response.\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? No response.\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No response.\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? No response.", "publication_ref": ["b31", "b31"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Lexicon learning for few shot sequence modeling", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Ekin Akyurek; Jacob Andreas"}, {"ref_id": "b1", "title": "Good-enough compositional data augmentation", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Jacob Andreas"}, {"ref_id": "b2", "title": "Iterative bregman projections for regularized transportation problems", "journal": "SIAM Journal on Scientific Computing", "year": "2015", "authors": "Jean-David Benamou; Guillaume Carlier; Marco Cuturi; Luca Nenna; Gabriel Peyr\u00e9"}, {"ref_id": "b3", "title": "The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming", "journal": "", "year": "1967", "authors": " Lev M Bregman"}, {"ref_id": "b4", "title": "The mathematics of statistical machine translation: Parameter estimation", "journal": "Computational Linguistics", "year": "1993", "authors": "F Peter; Stephen A Della Brown; Vincent J Pietra; Robert L Della Pietra;  Mercer"}, {"ref_id": "b5", "title": "Translate first reorder later: Leveraging monotonicity in semantic parsing", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Francesco Cazzaro; Davide Locatelli; Ariadna Quattoni; Xavier Carreras"}, {"ref_id": "b6", "title": "Meta-learning to compositionally generalize", "journal": "Long Papers", "year": "2021", "authors": "Henry Conklin; Bailin Wang; Kenny Smith; Ivan Titov"}, {"ref_id": "b7", "title": "2021. The devil is in the detail: Simple tricks improve systematic generalization of transformers", "journal": "Association for Computational Linguistics", "year": "", "authors": "R\u00f3bert Csord\u00e1s; Kazuki Irie; Juergen Schmidhuber"}, {"ref_id": "b8", "title": "The neural data router: Adaptive control flow in transformers improves systematic generalization", "journal": "", "year": "2022", "authors": "R\u00f3bert Csord\u00e1s; Kazuki Irie; J\u00fcrgen Schmidhuber"}, {"ref_id": "b9", "title": "Expanding the scope of the ATIS task: The ATIS-3 corpus", "journal": "", "year": "1994-03-08", "authors": "Deborah A Dahl; Madeleine Bates; Michael Brown; William Fisher; Kate Hunicke-Smith; David Pallett; Christine Pao; Alexander Rudnicky; Elizabeth Shriberg"}, {"ref_id": "b10", "title": "The Theory of Max-Min and its Application to Weapons Allocation Problems", "journal": "", "year": "1967", "authors": "John M Danskin"}, {"ref_id": "b11", "title": "R\u00e9-ordonnancement via programmation dynamique pour l'adaptation cross-lingue d'un analyseur en d\u00e9pendances (sentence reordering via dynamic programming for cross-lingual dependency parsing )", "journal": "", "year": "2022", "authors": "Nicolas Devatine; Caio Corro; Fran\u00e7ois Yvon"}, {"ref_id": "b12", "title": "Actes de la 29e Conf\u00e9rence sur le Traitement Automatique des Langues Naturelles", "journal": "", "year": "", "authors": ""}, {"ref_id": "b13", "title": "Compositional semantic parsing with large language models", "journal": "", "year": "2022", "authors": "Andrew Drozdov; Nathanael Sch\u00e4rli; Ekin Aky\u00fcrek; Nathan Scales; Xinying Song; Xinyun Chen; Olivier Bousquet; Denny Zhou"}, {"ref_id": "b14", "title": "Local search with very large-scale neighborhoods for optimal permutations in machine translation", "journal": "", "year": "2006", "authors": "Jason Eisner;  Roy W Tromble"}, {"ref_id": "b15", "title": "Improving textto-SQL evaluation methodology", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Catherine Finegan-Dollak; Jonathan K Kummerfeld; Li Zhang; Karthik Ramanathan; Sesh Sadasivam; Rui Zhang; Dragomir Radev"}, {"ref_id": "b16", "title": "Posterior regularization for structured latent variable models", "journal": "The Journal of Machine Learning Research", "year": "2010", "authors": "Kuzman Ganchev; Joao Gra\u00e7a; Jennifer Gillenwater; Ben Taskar"}, {"ref_id": "b17", "title": "AMR dependency parsing with a typed semantic algebra", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Jonas Groschwitz; Matthias Lindemann; Meaghan Fowlie; Mark Johnson; Alexander Koller"}, {"ref_id": "b18", "title": "Benchmarking meaning representations in neural semantic parsing", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Jiaqi Guo; Qian Liu; Jian-Guang Lou; Zhenwen Li; Xueqing Liu; Tao Xie; Ting Liu"}, {"ref_id": "b19", "title": "Spanbased semantic parsing for compositional generalization", "journal": "Long Papers", "year": "2021", "authors": "Jonathan Herzig; Jonathan Berant"}, {"ref_id": "b20", "title": "Compositional generalization for natural language interfaces to web apis", "journal": "", "year": "2021", "authors": "Saghar Hosseini; Ahmed Hassan Awadallah; Yu Su"}, {"ref_id": "b21", "title": "Compositionality decomposed: how do neural networks generalise", "journal": "Journal of Artificial Intelligence Research", "year": "2020", "authors": "Dieuwke Hupkes; Verna Dankers; Mathijs Mul; Elia Bruni"}, {"ref_id": "b22", "title": "LAGr: Label aligned graphs for better systematic generalization in semantic parsing", "journal": "Long Papers", "year": "2022", "authors": "Dora Jambor; Dzmitry Bahdanau"}, {"ref_id": "b23", "title": "COGS: A compositional generalization challenge based on semantic interpretation", "journal": "", "year": "2020", "authors": "Najoung Kim; Tal Linzen"}, {"ref_id": "b24", "title": "Sequence-to-sequence learning with latent neural grammars", "journal": "Curran Associates, Inc", "year": "2021", "authors": "Yoon Kim"}, {"ref_id": "b25", "title": "Simple and accurate dependency parsing using bidirectional LSTM feature representations", "journal": "Transactions of the Association for Computational Linguistics", "year": "2016", "authors": "Eliyahu Kiperwasser; Yoav Goldberg"}, {"ref_id": "b26", "title": "Moses: Open source toolkit for statistical machine translation", "journal": "Association for Computational Linguistics", "year": "2007", "authors": "Philipp Koehn; Hieu Hoang; Alexandra Birch; Chris Callison-Burch; Marcello Federico; Nicola Bertoldi; Brooke Cowan; Wade Shen; Christine Moran; Richard Zens; Chris Dyer; Ond\u0159ej Bojar; Alexandra Constantin; Evan Herbst"}, {"ref_id": "b27", "title": "The hungarian method for the assignment problem", "journal": "", "year": "1955", "authors": " Harold W Kuhn"}, {"ref_id": "b28", "title": "Sinkhorn algorithm for lifted assignment problems", "journal": "SIAM Journal on Imaging Sciences", "year": "2019", "authors": "Yam Kushinsky; Haggai Maron; Nadav Dym; Yaron Lipman"}, {"ref_id": "b29", "title": "Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks", "journal": "PMLR", "year": "2018", "authors": "Brenden Lake; Marco Baroni"}, {"ref_id": "b30", "title": "BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension", "journal": "", "year": "2020", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal; Marjan Ghazvininejad; Abdelrahman Mohamed; Omer Levy; Veselin Stoyanov; Luke Zettlemoyer"}, {"ref_id": "b31", "title": "Compositional generalisation with structured reordering and fertility layers", "journal": "", "year": "2023", "authors": "Matthias Lindemann; Alexander Koller; Ivan Titov"}, {"ref_id": "b32", "title": "Learning algebraic recombination for compositional generalization", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Chenyao Liu; Shengnan An; Zeqi Lin; Qian Liu; Bei Chen; Jian-Guang Lou; Lijie Wen; Nanning Zheng; Dongmei Zhang"}, {"ref_id": "b33", "title": "Roberta: A robustly optimized bert pretraining approach. ArXiv, abs", "journal": "", "year": "1907", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b34", "title": "AMR parsing as graph prediction with latent alignment", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Chunchuan Lyu; Ivan Titov"}, {"ref_id": "b35", "title": "Learning latent permutations with gumbel-sinkhorn networks", "journal": "", "year": "2018", "authors": "Gonzalo Mena; David Belanger; Scott Linderman; Jasper Snoek"}, {"ref_id": "b36", "title": "On graph-based reentrancy-free semantic parsing", "journal": "Transactions of the Association for Computational Linguistics", "year": "2023", "authors": "Alban Petit; Caio Corro"}, {"ref_id": "b37", "title": "Improving compositional generalization with latent structure and data augmentation", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Linlu Qiu; Peter Shaw; Panupong Pasupat; Pawel Nowak; Tal Linzen; Fei Sha; Kristina Toutanova"}, {"ref_id": "b38", "title": "A relationship between arbitrary positive matrices and doubly stochastic matrices", "journal": "The Annals of Mathematical Statistics", "year": "1964", "authors": "Richard Sinkhorn"}, {"ref_id": "b39", "title": "Quasisynchronous grammars: Alignment by soft projection of syntactic dependencies", "journal": "", "year": "2006", "authors": "David Smith; Jason Eisner"}, {"ref_id": "b40", "title": "Structured reordering for modeling latent alignments in sequence transduction", "journal": "", "year": "2021", "authors": "Bailin Wang; Mirella Lapata; Ivan Titov"}, {"ref_id": "b41", "title": "Compositional generalization with a broadcoverage semantic parser", "journal": "", "year": "2022", "authors": "Pia Wei\u00dfenhorn; Lucia Donatelli; Alexander Koller"}, {"ref_id": "b42", "title": "Stochastic inversion transduction grammars and bilingual parsing of parallel corpora", "journal": "Computational Linguistics", "year": "1997", "authors": "Dekai Wu"}, {"ref_id": "b43", "title": "Recogs: How incidental details of a logical form overshadow an evaluation of semantic interpretation", "journal": "", "year": "2023", "authors": "Zhengxuan Wu; D Christopher; Christopher Manning;  Potts"}, {"ref_id": "b44", "title": "Structural generalization is hard for sequence-to-sequence models", "journal": "", "year": "2022", "authors": "Yuekun Yao; Alexander Koller"}, {"ref_id": "b45", "title": "Compositional generalization via semantic tagging", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Hao Zheng; Mirella Lapata"}, {"ref_id": "b46", "title": "Disentangled sequence to sequence learning for compositional generalization", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Hao Zheng; Mirella Lapata"}, {"ref_id": "b47", "title": "Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? 6, Appendix G, code submission", "journal": "", "year": "", "authors": ""}, {"ref_id": "b48", "title": "error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? 6, Appendix B C4", "journal": "", "year": "", "authors": ""}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Two possible permutations (represented with red arrows) of z \u2032 that yield the same result y. Only the permutation marked with solid arrows corresponds to the right linguistic relation that will generalize well.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Accuracy by input length for doubling task.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure 5: Accuracy on the document domain of Okapi by number of conjuncts in the gold logical form.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "We call Algorithm 1 Bregman's method for Eq. (6) with entropic regularization function BREGMAN(s, \u03c4 ) U ij = exp(\u03c4 \u22121 s i \u2192j ) W ijk = exp(\u03c4 \u22121 s k\u21b7i ) while within budget and not converged do U, W = KL project(U, W; 6a,6c) with Prop. 2 U, W = KL project(U, W; 6a,6d) with Prop. 2 KL(x | x i ) a KL projection. In order to apply Bregman's method, we need to be able to compute the KL projection arg min x\u2208C KL(x | x i", "figure_data": "U = KL project (U, 6b) with Prop. 1 end while return U, W end functionarg min x\u2208C"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Exact match accuracy on COGS by generalization type. \u22c4 refers to models using pretrained transformers, and \u2663 refers to models implicitly or explicitly using trees. Q'22 isQiu et al. (2022), D'22 is  ", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Accuracy on ATIS. \u2020 indicates grammar-based decoding. L'23 is the model of Lindemann et al. (2023).", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Performance breakdown of the first and second stage. 'Freq' refers to accuracy measured on the predicted multiset; it measures performance of the first stage. 'Seq' measures the accuracy of both stages. 'Seq/Freq' is the percentage of correct predictions given that the multiset is predicted correctly.", "figure_data": "FreqSeqSeq/FreqOS50\u00b1349\u00b11311\u00b115COGSCP97\u00b1579\u00b11182\u00b113PP99\u00b1085\u00b11485\u00b115ATISiid Length 42.2\u00b113.6 41.4\u00b113.5 97.8\u00b10.8 77.6\u00b11.4 76.7\u00b11.7 98.7\u00b10.5ModelCalendarDocEmailBART-base \u22c4 L'23 \u2663 L'23  \u2020 \u2663 Ours \u22c4 Ours36.7\u00b13.0 57.2\u00b119.9 36.1\u00b15.6 43.9\u00b13.8 0.6\u00b10.3 20.5\u00b19.8 69.5\u00b113.9 42.4\u00b15.7 55.6\u00b12.7 74.3\u00b13.5 57.8\u00b15.5 60.6\u00b14.8 65.6\u00b12.8 41.4\u00b14.9 47.6\u00b14.5"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Accuracy on length splits by domain on Okapi.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Examples of lengths 5 -10 are used as training, examples of", "figure_data": "DatasetSplit/Version TrainDevTestDoubling4,0005001,000COGS24,155 3,000 21,000ATISiid length4,465 4,017497 942448 331Calendar1,1452001061OkapiDocument2,328412514Email2,343200991"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Number of examples per dataset/split. length 11 are used as development data (e.g. for hyperparameter selection), and examples of length 11 -20 are used as test data.", "figure_data": "D.1 PreprocessingCOGS. Unlike Zheng and Lapata (2022); Qiu et al. (2022);"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Average runtime for train/evaluate on dev and test in minutes. For the doubling task, we note that our model has converged usually after 1 4 of the time in the table.", "figure_data": "DatasetModelFirst stage Second stageLSTM2.462DoublingTransformer10.424Ours0.2730.463COGSOurs125.091127.16ATISOurs Ours/LSTM 3.345 125.506127.119 1.816Okapi/CalendarOurs Ours/LSTM 1.493 124.927127.03 1.876"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Number of parameters in millions in the models we train. This includes the 124.646m params of RoBERTa when we finetune it.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "P (z | x) = i,v P (z i,v | x) (1)", "formula_coordinates": [3.0, 123.38, 136.42, 166.49, 25.55]}, {"formula_id": "formula_1", "formula_text": "h i = ENCODER(x) i + EMBED(x i ) (2)", "formula_coordinates": [3.0, 104.9, 274.64, 184.97, 14.19]}, {"formula_id": "formula_2", "formula_text": "P (z i,v = k|x i ) = exp h T i w v,k + b v,k l exp h T i w v,l + b v,l(3)", "formula_coordinates": [3.0, 80.35, 349.92, 209.52, 38.91]}, {"formula_id": "formula_3", "formula_text": "P (m|x) = v P (z 1,v + . . . + z n,v = m v |x)", "formula_coordinates": [3.0, 84.05, 584.2, 191.89, 25.01]}, {"formula_id": "formula_4", "formula_text": "P (z 1,v + . . . + z n,v = m v | x) = k P (z 1,v = k|x)P (z 2,v + . . . z n,v = m v \u2212 k|x)", "formula_coordinates": [3.0, 71.67, 639.03, 216.64, 45.32]}, {"formula_id": "formula_5", "formula_text": "(x,y)\u2208D log P (m(y) | x)(4)", "formula_coordinates": [3.0, 129.17, 743.69, 160.69, 26.26]}, {"formula_id": "formula_6", "formula_text": "arg max V\u2208P i,j V ij s i \u2192j + i,k s k\u21b7i \uf8eb \uf8ed j V k,j\u22121 V ij \uf8f6 \uf8f8 (5)", "formula_coordinates": [4.0, 98.88, 179.71, 190.98, 67.9]}, {"formula_id": "formula_7", "formula_text": "arg max U,W i,j U ij s i \u2192j + i,j,k W ijk s k\u21b7i (6) subject to i U ij = 1 \u2200j (6a) j U ij = 1 \u2200i (6b) k W ijk = U ij \u2200j > 1, i (6c) i W ijk = U k(j\u22121) \u2200j > 1, k (6d) U, W \u2265 0 (6e)", "formula_coordinates": [4.0, 78.97, 517.57, 210.89, 170.7]}, {"formula_id": "formula_8", "formula_text": "h \u2032 i = h a(i) ; EMBED(z \u2032 i )(7)", "formula_coordinates": [4.0, 360.96, 554.06, 164.19, 15.4]}, {"formula_id": "formula_9", "formula_text": "s i \u21921 = w T start FF start (h \u2032 i )", "formula_coordinates": [4.0, 365.17, 612.18, 100.23, 19.96]}, {"formula_id": "formula_10", "formula_text": "s i \u2192n = w T end FF end (h \u2032 i )", "formula_coordinates": [4.0, 366.93, 656.74, 96.71, 19.96]}, {"formula_id": "formula_11", "formula_text": "from h \u2032 k to h \u2032 i .", "formula_coordinates": [4.0, 306.14, 706.76, 218.28, 28.54]}, {"formula_id": "formula_12", "formula_text": "U,\u0174 = arg min U\u2208Q(y,z \u2032 ),W KL(U || U * (s))+ (8) KL(W || W * (s))", "formula_coordinates": [5.0, 333.74, 93.51, 191.41, 47.66]}, {"formula_id": "formula_13", "formula_text": "KL(\u00db || U * (s)) + KL(\u0174 || W * (s))", "formula_coordinates": [5.0, 336.95, 219.01, 156.66, 21.19]}, {"formula_id": "formula_14", "formula_text": "x * = arg max x\u2208C 0 \u2229C 1 ...\u2229Cn,x\u22650 s T x + \u03c4 H(x) regularizer (9", "formula_coordinates": [5.0, 331.79, 523.51, 189.12, 29.85]}, {"formula_id": "formula_15", "formula_text": ")", "formula_coordinates": [5.0, 520.91, 524.14, 4.24, 13.15]}, {"formula_id": "formula_16", "formula_text": "H(x) = \u2212 i x i (log x i \u2212 1", "formula_coordinates": [5.0, 306.14, 574.33, 122.26, 20.95]}, {"formula_id": "formula_17", "formula_text": "x 0 = exp s \u03c4 x i+1 = arg min x\u2208C i mod (n\u22121) KL(x || x i ) (10", "formula_coordinates": [5.0, 346.75, 686.5, 173.86, 51.9]}, {"formula_id": "formula_18", "formula_text": ")", "formula_coordinates": [5.0, 520.61, 708.24, 4.54, 13.15]}, {"formula_id": "formula_19", "formula_text": "x | y) = i x i log x i y i \u2212 x i + y i is the generalized KL divergence.", "formula_coordinates": [5.0, 306.14, 745.24, 217.78, 28.86]}, {"formula_id": "formula_20", "formula_text": "x * = arg min x\u2208C \u2206 KL(x || y) for y > 0 has the closed-form solution x * i = y i i y i .", "formula_coordinates": [6.0, 70.86, 380.22, 220.15, 31.91]}, {"formula_id": "formula_21", "formula_text": "arg min U KL(U || A) subject to j U ij = m i is given by U * ij = m i A ij j \u2032 A ij \u2032 .", "formula_coordinates": [6.0, 70.86, 669.37, 218.28, 37.84]}, {"formula_id": "formula_22", "formula_text": "arg min U, W KL(U || A) + KL(W || B) subject to i U ij = 1 \u2200j k W ijk = U ij \u2200j, i(11)", "formula_coordinates": [6.0, 345.84, 91.2, 179.31, 85.55]}, {"formula_id": "formula_23", "formula_text": "U * ij = T ij i \u2032 T i \u2032 j W * ijk = U * ij B ijk k \u2032 B ijk \u2032 where T ij = A ij \u2022 k B ijk .", "formula_coordinates": [6.0, 305.75, 198.69, 156.27, 87.12]}, {"formula_id": "formula_24", "formula_text": "KL(x || z) + KL(Y * (x) || W) = KL(x || z) + i,j x i W i,j j \u2032 W i,j \u2032 (log x i\u00a8\u1e84 i,j j \u2032 W i,j \u2032\u1e84 i,j \u2212 1) = KL(x || z) + i x i $ $ $ $ $ j W i,j $ $ $ $ $ j \u2032 W i,j \u2032 (log x i j \u2032 W i,j \u2032 \u2212 1) = i x i (log( x i z i ) \u2212 1 + log x i j \u2032 W i,j \u2032 \u2212 1) = i x i (2 log x i \u2212 log z i \u2212 log \uf8eb \uf8ed j \u2032 W i,j \u2032 \uf8f6 \uf8f8 \u2212 2) = 2 i x i (log x i \u2212 1 \u2212 1 2 (log z i + log \uf8ee \uf8f0 j \u2032 W i,j \u2032 \uf8f9 \uf8fb ) log q i ) = 2 \u2022 KL(x || q)", "formula_coordinates": [13.0, 178.32, 84.64, 238.63, 250.86]}, {"formula_id": "formula_25", "formula_text": "x | y) = i x i log xi y i \u2212 x i + y i simplifies to KL(x | y) = i x i (log xi y i \u2212 1)", "formula_coordinates": [13.0, 81.37, 348.82, 443.04, 31.09]}, {"formula_id": "formula_26", "formula_text": "arg min x, Y KL(x || z) + KL(Y || W) subject to i x i = 1 k Y ik = x i \u2200i(13)", "formula_coordinates": [13.0, 86.88, 459.96, 202.98, 85.55]}, {"formula_id": "formula_27", "formula_text": "Y * i,j = x i W i,j j W i,j", "formula_coordinates": [13.0, 222.81, 567.96, 63.45, 21.3]}, {"formula_id": "formula_28", "formula_text": "arg min x KL(x || z) + KL(Y * (x) || W) subject to i x i = 1 (14)", "formula_coordinates": [13.0, 77.54, 639.84, 212.32, 51.36]}, {"formula_id": "formula_29", "formula_text": "arg min x KL(x || q) subject to i x i = 1 (15", "formula_coordinates": [13.0, 128.14, 726.3, 157.18, 51.12]}, {"formula_id": "formula_30", "formula_text": ")", "formula_coordinates": [13.0, 285.32, 744.55, 4.54, 13.15]}, {"formula_id": "formula_31", "formula_text": "where q i = z i \u2022 j \u2032 W i,j \u2032 .", "formula_coordinates": [13.0, 305.75, 397.59, 126.99, 20.95]}, {"formula_id": "formula_32", "formula_text": "x * i = q i i \u2032 q i \u2032 By plugging this into Y * (x)", "formula_coordinates": [13.0, 306.14, 460.01, 136.28, 52.31]}, {"formula_id": "formula_33", "formula_text": "arg max x\u2208C s T x \u2212 \u03c4 i x i (log x i \u2212 1) = arg min x\u2208C \u03c4 i x i (log x i \u2212 s i \u03c4 \u2212 1) = arg min x\u2208C KL(x || exp( s \u03c4 ))", "formula_coordinates": [13.0, 336.15, 670.76, 160.09, 83.87]}, {"formula_id": "formula_34", "formula_text": "P (y|x, z \u2032 ) = R\u2208P P \u03b8 (R|x, z \u2032 )P (y|z \u2032 , R) (16)", "formula_coordinates": [14.0, 85.12, 311.52, 204.74, 26.74]}, {"formula_id": "formula_35", "formula_text": "log P (y|x, z \u2032 ) \u2265 max Q E R\u223cQ(R|x,z \u2032 ,y) log P (y|z \u2032 , R) \u2212 KL(Q(R|x, z \u2032 , y) || P \u03b8 (R|x, z \u2032 ))(17)", "formula_coordinates": [14.0, 70.86, 463.78, 224.02, 50.71]}, {"formula_id": "formula_36", "formula_text": "P \u03b8 (R ij = 1|x, z \u2032 ) = U * (s) ij", "formula_coordinates": [14.0, 117.84, 601.49, 123.35, 14.48]}, {"formula_id": "formula_37", "formula_text": "log P (y|x, z \u2032 ) \u2265 (18) \u2212 min Q\u2208Q(y,z) KL(Q(R|x, z \u2032 , y) || P \u03b8 (R|x, z \u2032 ))", "formula_coordinates": [14.0, 321.77, 465.74, 203.39, 37.73]}, {"formula_id": "formula_38", "formula_text": "\u2212\u2207 \u03b8 KL(Q * || P \u03b8 (R|x, z \u2032 ))(19)", "formula_coordinates": [14.0, 355.69, 559.02, 169.46, 21.19]}, {"formula_id": "formula_39", "formula_text": "P \u2032 (z i,v = k|x) = P (z i,L = k|x i ) if v = L(x i ) P (z i,v = k|x)else", "formula_coordinates": [15.0, 306.14, 156.85, 223.69, 29.97]}], "doi": "10.18653/v1/2021.acl-long.382"}