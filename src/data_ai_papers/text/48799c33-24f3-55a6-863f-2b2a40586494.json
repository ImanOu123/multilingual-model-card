{"title": "On the (Non-)existence of Convex, Calibrated Surrogate Losses for Ranking", "authors": "Cl\u00e9ment Calauz\u00e8nes; Nicolas Usunier; Patrick Gallinari", "pub_date": "", "abstract": "We study surrogate losses for learning to rank, in a framework where the rankings are induced by scores and the task is to learn the scoring function. We focus on the calibration of surrogate losses with respect to a ranking evaluation metric, where the calibration is equivalent to the guarantee that near-optimal values of the surrogate risk imply near-optimal values of the risk defined by the evaluation metric. We prove that if a surrogate loss is a convex function of the scores, then it is not calibrated with respect to two evaluation metrics widely used for search engine evaluation, namely the Average Precision and the Expected Reciprocal Rank. We also show that such convex surrogate losses cannot be calibrated with respect to the Pairwise Disagreement, an evaluation metric used when learning from pairwise preferences. Our results cast lights on the intrinsic difficulty of some ranking problems, as well as on the limitations of learning-to-rank algorithms based on the minimization of a convex surrogate risk.", "sections": [{"heading": "Introduction", "text": "A surrogate loss is a loss function used as a substitute for the true quality measure during training in order to ease the optimization of the empirical risk. The hinge loss or the exponential loss, which are used in Support Vector Machines or AdaBoost as convex upper bounds of the classification error, are well-known examples of surrogate losses for binary classification. In this paper, we study surrogate losses for learning to rank, in a context where a set of items should be ranked given an input query and where the ranking is obtained by sorting the items according to predicted numerical scores. This work is motivated by the intensive research that has recently been carried out on machine learning approaches to improve the quality of search engine results, and more specifically on the design of surrogate losses that lead to high quality rankings (see [16] for a review).\nConsidering algorithms for learning to rank on the axis of scalability, there are first algorithms that are designed for small-scale datasets only and that directly solve the NP-hard problem [5] without using any surrogate loss; after them come algorithms that use a surrogate loss chosen as a non-convex but continuous and (almost everywhere) differentiable approximation of the evaluation metric [3,21,10], and finally algorithms that use a convex surrogate loss. Most algorithms for learning to rank fall into the latter category, including the reference algorithms RankBoost [12] and Ranking SVMs [14,4] or the regression approach of [8], because convex surrogate losses lead to optimization problems that can be solved efficiently while non-convex approaches may require intensive computations to find a good local optimum. The disadvantage of convex surrogate losses is that they cannot closely approximate the evaluation metrics on the whole prediction space. However, as more examples are available and smaller values of the surrogate risk are achieved, the only region of interest becomes that of near-optimal predictions. It is thus possible that the minimization of the surrogate risk provably leads to optimal predictions according to the risk defined by the evaluation measure. In that case, the surrogate loss is said to be calibrated with respect to the evaluation metric.\nThe calibration of surrogate losses has been extensively studied for various classification settings [1,26,27,18,19] and for AUC optimization [7,15]. For each of these tasks, many usual convex losses are calibrated with respect to the natural evaluation metric. In the context of learning to rank for search engines, several families of convex losses are calibrated with respect to the Discounted Cumulative Gain (DCG) and its variants [8,2,17]. However, other metrics than the DCG are often used as reference for the evaluation of ranked results, such as the Average Precision (AP), used in past TREC competitions [22], the Expected Reciprocal Rank (ERR), used the Yahoo! Learning to Rank Challenge [6], or the Pairwise Disagreement (PD), used when learning from pairwise preferences. And despite the multiplicity of convex losses that have been proposed for ranking, none of them was proved to be calibrated with respect to any of these three metrics. This lead us to the question of whether convex losses can be calibrated with respect to the AP, the ERR, or the PD.\nOur main contribution is a definitive and negative answer to that question. We prove that if a surrogate loss is convex, then it cannot be calibrated with respect to any of the AP, the ERR or the PD. Thus, if one of these metrics should be optimized, the price to pay for the computational advantage of convex losses is an inconsistent learning procedure, which may converge to non-optimal predictions as the number of examples increases.\nOur result generalizes previous works on non-calibration. First, Duchi et al. [11] showed that many convex losses based on pairwise comparisons, such as those of RankBoost [12] or Ranking SVMs [14,4], are not calibrated with respect to the PD. Secondly, Buffoni et al. [2] showed that specific convex losses, called order-preserving, are not calibrated with respect to the AP or the ERR, even though these losses are calibrated with respect to (any variant of) the DCG. Our result is stronger than those because we do not make any assumption on the exact structure of the loss; our approach as a whole is also more general because it directly applies to the three evaluation metrics (AP, ERR and PD). Finally, Duchi et al. conjectured that no convex loss can be calibrated with the PD in general [11,Section 2.1] because it would provide a polynomial algorithm to solve an NP-hard problem. Our approach thus leads to a direct proof of this conjecture.\nIn the next section, we describe our framework for learning to rank. We then present in Section 3 the general framework of calibration of [20], and give a new characterization of calibration for the evaluation metrics we consider (Theorem 2), and the implications of the convexity of a surrogate loss. Our main result is proved in Section 4. Section 5 concludes the paper, and Section 6 is a technical part containing the full proof of Theorem 2.\nNotation Let V, W be two sets. A set-valued function g from V to W maps all v \u2208 V to a subset of W (set-valued functions appear in the paper as the result of arg min operations). Given a subset V of V, the image of V by g, denoted by g(V ), is the union of the images by g of all members of V , i.e. g(V ) = v\u2208V g(v). If n is a positive integer, [n] is the set {1, ..., n}, and S n is the set of permutations of [n]. Boldface characters are used for vectors of R n . If x \u2208 R n , the i-th component of x is denoted by x i (normal font and subscript). The cardinal of a finite set V is denoted by |V|.", "publication_ref": ["b15", "b4", "b2", "b20", "b9", "b11", "b13", "b3", "b7", "b0", "b25", "b26", "b17", "b18", "b6", "b14", "b7", "b1", "b16", "b21", "b5", "b10", "b11", "b13", "b3", "b1", "b10", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Ranking Framework", "text": "We describe in this section the formal framework of ranking we consider. We first present the prediction problem we address, and then define the two main objects of our study: evaluation metrics for ranking and surrogate losses. We end the section with an outline of our technical contributions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Framework and Definitions", "text": "We consider a framework similar to label ranking [9] or subset ranking [8]. Let X be a measurable space (the instance space). An instance x \u2208 X represents a query and its associated n items to rank, for an integer n \u2265 3. The items are indexed from 1 to n, and the goal is to order the set of item indexes [n] = {1, ..., n} given x. The ordering (or ranking) is predicted by a scoring function, which is a measurable function f : X \u2192 R n . For any input instance x, the scoring function f predicts a vector of n relevance scores (one score for each item) and the ranking is predicted by sorting the item indexes by decreasing scores. We use permutations over [n] to represent rankings, with the following conventions. First, given a permutation \u03c3 in S n , k in [n] is the rank of the item \u03c3(k); second, low ranks are better, so that \u03c3(1) is the top-ranked item. \ny \u2208 Y = {0, ..., Y } n , Y \u2208 N, Y \u2265 1 Discounted Cumulative Gain (higher values mean better performances) n k=1 2 y \u03c3(k) \u22121 log(1+k)\nExpected Reciprocal Rank (higher values mean better performances)\nn k=1 R k k k\u22121 q=1 (1 \u2212 R q ), R k = 2 y \u03c3(k) -1 2 Y y \u2208 Y = {0, 1}n\nAverage Precision (higher values mean better performances)\n1 |{i:yi=1}| i:yi=1 \u03c3 -1 (i) k=1 y \u03c3(k) \u03c3 -1 (i) y \u2208 Y = all DAGs over [n]\nPairwise Disagreement (lower values mean better performances) i\u2192j\u2208y\nI \u03c3 -1 (i) > \u03c3 -1 (j)\nThe quality of a ranking is measured by a ranking evaluation metric, relatively to a feedback. The feedback space, denoted by Y, is a finite set, and an evaluation metric is a function r : Y \u00d7 S n \u2192 R.\nWe use the convention that lower values of r are preferable, and thus when we discuss existing metrics for which higher values are better (e.g. the DCG, the AP or the ERR), we implicitly consider their opposite. Table 1 gives the formula and feedback spaces of the evaluation metrics that we discuss in the paper. The first three metrics -the DCG, the ERR and the AP -are commonly used for search engine evaluation. The feedback they consider is a vector of relevance judgments (one judgment per item). The last measure we consider is the PD, which is widely used when learning from pairwise preferences. For the feedback space of the PD, we follow [11] and take Y as the set of all directed acyclic graph (DAG) over [n]. For a DAG y \u2208 Y, there is an edge from item i to j (denoted i \u2192 j \u2208 y) when i is preferred to j, or, equivalently when i should have better rank than j.\nIn general, using a sorting algorithm, any ranking evaluation metric r induces a quality measure on vectors of scores instead of rankings, considering that the sorting algorithm break ties randomly. Thus, using the following set-valued function from R n to S n , called arg sort, which gives the set of rankings induced by a vector of scores:\n\u2200s = (s 1 , ..., s n ) \u2208 R n , arg sort(s) = \u03c3 \u2208 S n |\u2200k \u2208 [n \u2212 1] , s \u03c3(k) \u2265 s \u03c3(k+1) ,\nthe evaluation metric on vectors of scores induced by r is defined by:\n\u2200y \u2208 Y, \u2200s \u2208 R n , r (y, s) = \u03c3\u2208arg sort(s) r (y, \u03c3) | arg sort(s)| .\nFor a fixed, but unknown, probability measure D on X \u00d7 Y, the objective of a learning algorithm is to find a scoring function f with low ranking risk R (D, f ) = X \u00d7Y r (y, f (x))dD(x, y) using a training set of (instance, feedback) pairs (e.g. drawn i.i.d. according to D).\nThe optimization of the empirical ranking risk is usually intractable because the ranking loss is discontinuous. To address this issue, algorithms optimize the empirical risk associated to a surrogate loss instead. Throughout the paper, we assume that this loss is bounded below, so that all the infima we take are well-defined. Without loss of generality, we assume that the surrogate loss has nonnegative values, and we define a surrogate loss as a measurable function : Y \u00d7 R n \u2192 R + . The surrogate risk of a scoring function f is then defined by L(D, f ) = X \u00d7Y (y, f (x))dD(x, y).", "publication_ref": ["b8", "b7", "b10"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Outline of the Analysis", "text": "Any learning algorithm that performs empirical or structural risk minimization on the surrogate risk can, at most, be expected to reach low values of the surrogate risk. The question we address in this paper is whether such an algorithm provably solves the real learning task, which is to achieve low values of the ranking risk. More formally, the criterion under study is whether the following implication holds for every sequence of scoring functions (f k ) k\u22650 and every data distribution D:\nL(D, f k ) \u2212\u2192 k\u2192\u221e inf f L(D, f ) \u21d2 R (D, f k ) \u2212\u2192 k\u2192\u221e inf f R (D, f )(1)\nwhere the infima are taken over all scoring functions. In particular, we show that if a surrogate loss is convex in the sense that (y, .) is convex for every y \u2208 Y, and if the evaluation metric is the AP, the ERR or the PD, then there are distributions and sequences of scoring functions for which (1) does not hold. In other words, we show that learning-to-rank algorithms that define their objective through a convex surrogate loss cannot provably optimize any of these evaluation metrics.\nIn order to perform a general analysis for all the three evaluation metrics, we consider Assumption (A) below, which formalizes the common property of these metrics that is relevant to our study. Intuitively, it means that for any given item, there is a feedback for which the performance only depends on the rank of this item, with a strict improvement of performances when one improves the rank of the item:\n(A) \u2203\u03b2 1 < \u03b2 2 < ... < \u03b2 n such that \u2200i \u2208 [n] , \u2203y \u2208 Y : \u2200\u03c3 \u2208 S n , r(y, \u03c3) = \u03b2 \u03c3 -1 (i) .\nNote that in the assumption, the values of \u03b2 k (i.e. the performance when item i is predicted at rank k) are the same for all items. This is not a strong requirement because the metrics we consider do not depend on how we index the elements. The DCG, the AP and the ERR satisfy (A): for each i, we take the vector of relevance with a 1 for item i and 0 for all other items so that the values of the metrics only depends on the rank of i (which should be ranked first). The PD satisfies Assumption (A) as well: for each i, take y as the DAG containing the edges i \u2192 j, \u2200j \u2208 [n] \\ {i} and only those edges. For this feedback, i is preferred to all other items (and no preference is specified regarding the other items) and thus the quality of a ranking only depends on the rank of i.\nOur analysis is organized as follows. In the next section, we introduce the notion of a calibrated surrogate loss defined by Steinwart [20], which is a criterion equivalent to (1). We then obtain a new condition that is equivalent to calibration when (A) holds, and finally we restrict our attention to evaluation metrics satisfying (A) and to convex surrogate losses. In that context, using our new condition for calibration, we show that evaluation metrics with a calibrated surrogate loss necessarily satisfy a specific property. Then, in Section 4, we prove that the AP, the ERR and the PD do not satisfy this property. Since Assumption (A) holds for these three metrics, this latter result implies that they do not have any convex and calibrated surrogate loss. Equivalently, it implies that (1) does not hold in general for these metrics if the surrogate loss is convex.", "publication_ref": ["b19", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "A New Characterization of Calibration", "text": "We present in this section the notion of calibration as studied in [20], which is the basis of our work. Then, we provide a characterization of calibration more specific to the evaluation metrics we consider, that relates more closely calibrated surrogate losses and evaluation metrics. This more specific characterization of calibration is the starting point of the analysis of convex and calibrated surrogate losses carried out in the last subsection and that allows us to state the results of Section 4.", "publication_ref": ["b19"], "figure_ref": [], "table_ref": []}, {"heading": "The Framework of Calibration", "text": "Applying the general results of [20] to our setting, the criterion defined by (1) can be studied by restricting our attention to the contributions of a single instance to the surrogate and ranking risk. These contributions are called the inner surrogate risk and the inner ranking risk respectively. Denoting the set of probability distributions over Y by P = p : Y \u2192 [0, 1]| y\u2208Y p(y) = 1 , the inner risks are respectively defined for all p \u2208 P and all s \u2208 R n by: More precisely, [20,Theorem 2.8] shows that (1) holds for any distribution D and any sequence of scoring functions if and only if the surrogate loss is r-calibrated according to the definition below.\nL (p, s) =\nSimilarly to (1), the calibration is an implication of two limits, but it involves the inner risks L and R instead of the risks L and R . For convenience in the rest of the work, we write the implication between the two limits of L and R as an inclusion of the sets of near-optimal vectors of scores. For any \u03b5 > 0 and \u03b4 > 0, the latter sets are respectively denoted by M (p, \u03b4) = {s \u2208 R n |L (p, s) \u2212 L (p) < \u03b4} and M r (p, \u03b5) = {s \u2208 R n |R (p, s) \u2212 R (p) < \u03b5} , so that the definition of an r-calibrated loss is the following: Definition 1. [20, Definition 2.7] The surrogate loss is r-calibrated if \u2200p \u2208 P, \u2200\u03b5 > 0, \u2203\u03b4 > 0 : M (p, \u03b4) \u2286 M r (p, \u03b5) .", "publication_ref": ["b19", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Calibration through Optimal Rankings", "text": "Definition 1 is the starting point of our analysis, and our goal is to show that if the evaluation metric is the AP, the ERR or the PD, then no convex surrogate loss can satisfy it. The goal of this subsection is to give a stronger characterization of r-calibrated surrogate losses when Assumption (A) holds.\nThe starting point of this characterization is to rewrite Definition 1 in terms of rankings induced by the sets of near-optimal scores, from which we can deduce that is r-calibrated if and only if 1 : \u2200p \u2208 P, \u2200\u03b5 > 0, \u2203\u03b4 > 0 : arg sort(M (p, \u03b4)) \u2286 arg sort(M r (p, \u03b5)) .\nIn contrast to this characterization of calibration, our result (Theorem 2 below), which is specific to metrics that satisfy (A), replaces the inclusion (which can be strict in general) of sets of ranking by an equality when \u03b5 tends to 0. More specifically, we define the set of optimal rankings for the inner ranking risk with the following set-valued function from P to S n : \u2200p \u2208 P, A r (p) = arg min \u03c3\u2208Sn R (p, \u03c3) , so that when Assumption (A) holds, the set of optimal rankings is equal to a set of rankings induced by near-optimal scores of the inner surrogate risk: Theorem 2. If Assumption (A) holds, then is r-calibrated if and only if\n\u2200p \u2208 P, \u2203\u03b4 > 0 s.t. arg sort(M (p, \u03b4)) = A r (p) .\nThe proof of Theorem 2 is deferred to Section 6 at the end of the paper. This theorem enables us to relate the surrogate loss and the evaluation metric so that the convexity of induces some constraints on r that are not satisfied by all evaluation metrics.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The implication of Convexity on Sets of Optimal Rankings", "text": "If (y, .) is convex for all y \u2208 P, then the inner risk L(p, .) is also convex for every distribution p \u2208 P. This implies that M (p, \u03b4) is a convex subset of R n . Thus, if is r-calibrated, then Theorem 2 implies that A r (p) = arg sort(M (p, \u03b4)) is a set of rankings induced by a convex set of R n .\nThe following theorem presents a condition that the set A r (p) must satisfy if it is generated by a convex set of scores: if there exists at least one pair of items (i, j) which are inverted in two rankings of A r (p), then i and j are \"indifferent\" in A r (p): Theorem 3. Assume that for all y \u2208 Y, the function s \u2192 (y, s) is convex. If Assumption (A) holds and is r-calibrated, then r satisfies:\n\u2200p \u2208 P, \u2200i, j \u2208 [n] , \u2200\u03c3, \u03c3 \u2208 A r (p), \u03c3 -1 (i) < \u03c3 -1 (j) and \u03c3 -1 (i) > \u03c3 -1 (j) \u21d2 \u2203s \u2208 R n : s i = s j and arg sort(s) \u2286 A r (p) .(2)\nProof of Theorem 3. Assume that the conditions of the theorem are satisfied. From now on, we fix some p \u2208 P and two i and j in [n]. Take \u03c3 and \u03c3 in A r (p) and assume that \u03c3 -1 (i) < \u03c3 -1 (j) and \u03c3 -1 (i) > \u03c3 -1 (j). Since Assumption (A) holds, there is a \u03b4 > 0 such that A r (p) = arg sort M (p, \u03b4) by Theorem 2. Thus, there are two score vectors u and v in M (p, \u03b4) such that u i \u2265 u j (u induces the ranking \u03c3) and v i \u2264 v j (v induces the ranking \u03c3 ).\nMoreover, since is convex, the function L (p, .) is convex for every p \u2208 P, and thus M (p, \u03b4) is convex. Consequently, for all t \u2208 [0, 1], the vector \u03b3(t) = (1 \u2212 t)u + tv belongs to M (p, \u03b4). We define g :\nt \u2192 \u03b3 i (t) \u2212 \u03b3 j (t) for t \u2208 [0, 1].\nThen, g is continuous, with g(0) = u i \u2212 u j \u2265 0 and g(1) = v i \u2212 v j \u2264 0. By the intermediate value theorem, there is t 0 \u2208 [0, 1] such that g(t 0 ) = 0. The consequence is that the score vector s, defined by s = \u03b3(t 0 ), satisfies s \u2208 M (p, \u03b4) and s i = s j .\nTable 2: Examples for Corollary 4. There are three elements to rank. i j k represents the permutation that ranks item i first, j second and k last. For the ERR and the AP, we consider binary relevance judgments. p 110 denotes a Dirac distribution at the feedback vector y = [1, 1, 0]. p 001 is defined similarly. For the Pairwise Disagreement, p 1 2 3 is the Dirac distribution at the DAG containing the edges 1 \u2192 2, 2 \u2192 3 and 1 \u2192 3, i.e. the DAG corresponding to 1 2 3. The Dirac distribution at the DAG containing only the edge 3 \u2192 1 is denoted by p 3 1 . In all cases,p(\u03b1) is a mixture between two Dirac distributions. The sets A r (p(\u03b1)) are obtained by direct calculations. The set A r (p(\u03b1)) is the same for all \u03b1s in the range given in the third column.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "DISTRIBUTIONp(\u03b1) METRIC RANGE OF \u03b1", "text": "A r (p(\u03b1))\n(1 \u2212 \u03b1)p 110 + \u03b1p 001 ERR \u03b1 \u2208 1 3 , 1 2 {(1 3 2), (2 3 1)} AP \u03b1 = 5 13 {(1 2 3), (3 1 2), (2 1 3), (3 2 1)} (1 \u2212 \u03b1)p 1 2 3 + \u03b1p 3 1 PD \u03b1 \u2208 2 3 , 1 {(2 3 1), (3 1 2)}\nThe contrapositive of Theorem 3 is our technical tool to prove the nonexistence of convex and calibrated losses. Indeed, for a given evaluation metric r, if we are able to exhibit a distribution p \u2208 P such that (2) is not satisfied, this evaluation metric cannot have a surrogate loss both convex and calibrated. In the next subsection, we apply this argument to the AP, the ERR and the PD. Remark 1. It has been proved by several authors that there exist convex surrogate losses that are DCG-calibrated [8,2,17]. Thus, the DCG satisfies (2). It can be seen by observing that the optimal rankings for the DCG are exactly those generated by sorting the items according to the vector of score s * (p) defined by s * i (p) = y\u2208Y p(y)2 yi , i.e. A r (p) = arg sort(s * (p)).", "publication_ref": ["b7", "b1", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Nonexistence Results", "text": "We now present the main result of the nonexistence of convex, calibrated surrogate losses: Corollary 4. No convex surrogate loss is calibrated with respect to the AP, the ERR or the PD.\nProof. We consider the case where there are three elements to rank, and we use the examples and the notations of Table 2. Since all three metrics satisfy (A), Theorem 3 implies that if r (taken as either the AP, the ERR or the PD) has a calibrated, convex surrogate loss, then, for any distributio\u00f1 p(\u03b1), we have: if item i is preferred to j according to a ranking in A r (p(\u03b1)), and j is preferred to i according to another ranking in A r (p(\u03b1)), then one of the two assertions below must hold:\n(a) (i j k), (j i k) \u2286 A r (p(\u03b1)) , (b) (k i j), (k j i) \u2286 A r (p(\u03b1))\nbecause there exists s \u2208 R 3 such that arg sort(s) \u2286 A r (p(\u03b1)) for which either s i = s j \u2264 s k or s i = s j \u2265 s k . Now, let us consider the case of the ERR. Taking an arbitrary \u03b1 \u2208 1 3 , 1 2 , we see on the last column of Table 2 that A r (p(\u03b1)) contains two rankings: one of them ranks item 1 before item 2, and the other one ranks 2 before 1. If the ERR had a convex calibrated surrogate loss, then either (a) or (b) should hold. However, we see that neither (a) nor (b) holds. Thus their is no convex, ERR-calibrated surrogate loss. For the AP, a similar argument with items 1 and 3 leads to the conclusion. For the PD, taking any two items leads to the result.\nA first consequence of Corollary 4 is that for ranking problems evaluated in terms of AP, ERR or PD, surrogate losses defined as convex upper bounds on an evaluation metric as discussed in [24], as well as convex surrogate losses proposed in the structured output framework such as SVM map [25] are not calibrated with respect to the evaluation metric they are designed for. The convex surrogate losses used by most participants of the recent Yahoo! Learning to Rank Challenge [6] are also not calibrated with respect to the ERR, the official evaluation metric of the challenge. The fact that the minimization of a non-calibrated surrogate risk leads to suboptimal prediction functions on some data distributions suggests that convex losses are not a definitive solution to learning to rank. Significant improvements in performances may then be obtained by switching to other approaches than the optimization of a convex risk.", "publication_ref": ["b23", "b24", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We proved that convex surrogate losses cannot be calibrated with three major ranking evaluation metrics. The result cast light on the intrinsic limitations of all algorithms based on (empirical) convex risk minimization for ranking, even though most existing algorithms for learning to rank follow this approach. A possible direction for future work is to study whether the calibration of convex losses can be obtained under low noise conditions. Such studies was carried out for the PD [11], and calibrated, convex surrogate losses were found for special cases of practical interest. Nonetheless, in order to obtain algorithms that do not rely on low noise assumptions, our results suggest to explore whether alternatives to convex surrogate approaches can lead to improvements in terms of performances. A first possibility is to turn to non-convex losses for ranking as in [10,3], and to study the calibration of such losses. Another alternative is to use another surrogate approach than scoring, such as directly learning pairwise preferences [13], even though the reconstruction of an optimal ranking, given the pairwise predictions, that is optimal for evaluation metrics such as the AP, the ERR or the PD is still mostly an open issue.", "publication_ref": ["b10", "b9", "b2", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Theorem 2", "text": "We remind the statement of Theorem 2: if r satisfies (A), then is r-calibrated if and only if for all p \u2208 P, there exists \u03b4 > 0 such that A r (p) = arg sort(M (p, \u03b4) . We prove the result using the following set-valued function which defines the set of optimal rankings for the inner surrogate risk:\nA (p) = arg min \u03c3\u2208Sn L (p, \u03c3) where L (p, \u03c3) = inf L (p, s) | s \u2208 R n s.t. \u03c3 \u2208 arg sort(s) .\nThen, Theorem 2 is a direct implication of the two following claims that we prove in this section:\n(a) the assertion \u2200p \u2208 P, \u2203\u03b4 > 0, arg sort(M (p, \u03b4) = A (p) is true in general;\n(b) if Assumption (A) holds, then is r-calibrated if and only if \u2200p \u2208 P, A (p) = A r (p).\nThe proof of these two claims is based on three lemmas that we present before the final proof. The first lemma, which does not need any assumption on the evaluation metric, both proves equality (a) and provides a general characterization of calibration in terms of optimal rankings. The second lemma concerns the surrogate loss; it states that a slight perturbation in p does not affect \"too much\" A (p). The third lemma concerns evaluation metrics and gives a simple consequence of Assumption (A). The final proof of Theorem 2 connects all these pieces together to prove (b). (ii) Fix p \u2208 P and take \u03b4 0 = min \u03c3 \u2208A (p) L (p, \u03c3) \u2212 L (p) > 0, with the convention min \u2205 = +\u221e. The choice of \u03b4 0 guarantees that \u2200s \u2208 R n , L (p, s) \u2212 L (p) < \u03b4 0 \u21d2 arg sort(s) \u2286 A (p), which is equivalent to arg sort M (p, \u03b4 0 ) \u2286 A (p). The reverse inclusion is given by the first point.\n(iii) Since r can only take a finite set of values, we can prove that is r-calibrated if and only if: \u2200p \u2208 P, \u2203\u03b4 > 0 : \u2200s \u2208 R n , L (p, s) \u2212 L (p) < \u03b4 \u21d2 R (p, s) = R (p). Moreover, we have R (p, s) = R (p) \u21d4 arg sort(s) \u2286 A r (p) since R (p, s) is the mean of R (p, \u03c3) for \u03c3 \u2208 arg sort(s). Thus, is r-calibrated if and only if for every p \u2208 P, there exists \u03b4 > 0 such that arg sort M (p, \u03b4) \u2286 A r (p). This characterization and the first two points give the result.\nWe now present a more technical result on A , which shows the set of optimal rankings cannot dramatically change under a slight perturbation in the distribution over the feedback space. From now on, for any p \u2208 P and any \u03b7 > 0, we denote by B(p, \u03b7) the open ball of P (with respect to . 1 ) of radius \u03b7 centered at p, i.e. B(p, \u03b7) = {p \u2208 P| p \u2212 p 1 < \u03b7}. Lemma 6. \u2200p \u2208 P, \u2203\u03b7 > 0 such that A (B(p, \u03b7)) = A (p).\nProof. Note that A (p) \u2286 A (B(p, \u03b7)) since p \u2208 B(p, \u03b7). We now prove A (B(p, \u03b7)) \u2286 A (p); the main argument is that L (., \u03c3) is continuous for every \u03c3 because Y is finite [23,Theorem 2]. Indeed, let us fix p \u2208 P and define \u03b5 = 1 2 min \u03c3 \u2208A (p) L (p, \u03c3 ) \u2212 L (p) . For each \u03c3 \u2208 S n , since L (., \u03c3) is continuous, there exists \u03b7 \u03c3 > 0 such that \u2200p \u2208 B(p, \u03b7 \u03c3 ), | L (p , \u03c3) \u2212 L (p, \u03c3)| < \u03b5.\nLet \u03b7 = min \u03c3\u2208Sn \u03b7 \u03c3 , and let p be an arbitrary member of B(p, \u03b7). By the definition of \u03b5, we have:\n\u2200\u03c3 \u2208 A (p) , L (p , \u03c3 ) = L (p , \u03c3 ) \u2212 L (p, \u03c3 ) + L (p, \u03c3 ) \u2212 L (p) + L (p) > \u2212\u03b5 + 2\u03b5 + L (p) .\nThus, \u2200\u03c3 \u2208 A (p) , L (p , \u03c3 ) > L (p) + \u03b5. Additionally, the definition of \u03b7 gives \u2200\u03c3 \u2208 A (p) , L (p , \u03c3) < L (p) + \u03b5. Thus, we have min \u03c3 \u2208A (p) L (p , \u03c3 ) > min \u03c3\u2208A (p) L (p , \u03c3). This proves that a ranking that is not optimal for L (p, .) cannot be optimal for L (p , .). Thus A (p ) \u2286 A (p) from which we conclude A (B(p, \u03b7)) \u2286 A (p). Now that we have studied the properties of A , we analyze in more depth the evaluation metrics. We prove the following consequence of Assumption (A): for each possible ranking there is a distribution over the feedback space for which this ranking is the unique optimal ranking. Lemma 7. If Assumption (A) holds, then \u2200\u03c3 \u2208 S n , \u2203p \u03c3 \u2208 P such that A r (p \u03c3 ) = {\u03c3}.\nProof. Assume (A) holds, and, for each item k, let us denote by y k the feedback corresponding to item k in Assumption (A). Now, let us take some \u03c3 \u2208 S n and define p \u03c3 as p \u03c3 (y k ) = \u03b1 \u03c3 -1 (k) with \u03b1 1 > ... > \u03b1 n > 0 and n k=1 \u03b1 k = 1. Then, for any \u03c3 \u2208 S n , we have the equality R (p \u03c3 , \u03c3 ) = n k=1 \u03b1 \u03c3 -1 (k) r y k , \u03c3 = n k=1 \u03b1 \u03c3 -1 (k) \u03b2 \u03c3 -1 (k) . Since the \u03b1s are non-negative, and since there are ties neither the \u03b1s nor in the \u03b2s, the rearrangement inequality implies that the minimum value of R (p \u03c3 , \u03c3 ) is obtained for the single permutation \u03c3 for which the \u03b2 \u03c3 -1 (k) are in reverse order relatively to the \u03b1 \u03c3 -1 (k) (i.e. smaller values \u03b2 \u03c3 -1 (k) should be associated to greater values of \u03b1 \u03c3 -1 (k) ). Since the \u03b1 k s are decreasing with k and the \u03b2 k s are increasing, the minimum value of \u03c3 \u2192 R (p \u03c3 , \u03c3 ) = n k=1 \u03b1 \u03c3 -1 (k) \u03b2 \u03c3 -1 (k) is obtained if and only if \u03c3 -1 = \u03c3 -1 (i.e. \u03c3 = \u03c3).\nProof of Theorem 2. We remind to the reader that by the second point of Lemma 5, for any p \u2208 P, there is \u03b4 > 0 such that A (p) = arg sort(M (p, \u03b4) . What remains to show is that if Assumption (A) holds, then is r-calibrated if and only if \u2200p \u2208 P, A (p) = A r (p).\n(\"if\" direction) If \u2200p \u2208 P, A (p) = A r (p) then is r-calibrated by Lemma 5.\n(\"only if\" direction) Assume that (A) holds and that is r-calibrated. Let p \u2208 P. By Point (iii) of Lemma 5, we know that A (p) \u2286 A r (p). We now prove the reverse inclusion A (p ) \u2286 A r (p ).\nBy Lemma 6, there exists some \u03b7 > 0 such that A (B(p, \u03b7)) = A (p). Let \u03c3 \u2208 A r (p). The idea is to use Lemma 7 to find some p \u2208 B(p, \u03b7) such that A (p ) = {\u03c3} which would prove \u03c3 \u2208 A (p) and thus the result. The rest of the proof consists in building p . Using Lemma 7, let p \u03c3 \u2208 P such that A r (p \u03c3 ) = {\u03c3}. Now, let p = (1\u2212 \u03b7 4 )p+ \u03b7 4 p \u03c3 . Then, we have p \u2212 p 1 = \u03b7 4 p \u2212 p \u03c3 1 \u2264 \u03b7/2 and thus p \u2208 B(p, \u03b7). Moreover, A r (p ) = {\u03c3} since \u03c3 is optimal for both p and p \u03c3 , and any other permutation is suboptimal for p \u03c3 . We also have A (p ) = {\u03c3} because A has non-empty values and calibration implies that A (p ) \u2286 A r (p ) by Lemma 5.", "publication_ref": ["b22"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "This work was partially funded by the French DGA. The authors thank M. R. Amini, D. Buffoni, S. Cl\u00e9men\u00e7on, L. Denoyer and G. Wisniewski for their comments and suggestions.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Convexity, classification, and risk bounds", "journal": "J. of the American Stat. Assoc", "year": "2006", "authors": "P L Bartlett; M I Jordan; J D Mcauliffe"}, {"ref_id": "b1", "title": "Learning scoring functions with order-preserving losses and standardized supervision", "journal": "", "year": "2011", "authors": "D Buffoni; C Calauz\u00e8nes; P Gallinari; N Usunier"}, {"ref_id": "b2", "title": "Learning to rank with nonsmooth cost functions", "journal": "", "year": "2007", "authors": "C J Burges; R Ragno; Q V Le"}, {"ref_id": "b3", "title": "Adapting ranking svm to document retrieval", "journal": "", "year": "2006", "authors": "Y Cao; J Xu; T.-Y Liu; H Li; Y Huang; H.-W Hon"}, {"ref_id": "b4", "title": "How to reverse-engineer quality rankings", "journal": "Mach. Learn", "year": "2012-09", "authors": "A Chang; C Rudin; M Cavaretta; R Thomas; G Chou"}, {"ref_id": "b5", "title": "Yahoo! learning to rank challenge overview", "journal": "J. of Mach. Learn. Res", "year": "2011", "authors": "O Chapelle; Y Chang"}, {"ref_id": "b6", "title": "Ranking and scoring using empirical risk minimization", "journal": "", "year": "2005", "authors": "S Cl\u00e9men\u00e7on; G Lugosi; N Vayatis"}, {"ref_id": "b7", "title": "Statistical analysis of bayes optimal subset ranking", "journal": "IEEE Trans. Info. Theory", "year": "2008", "authors": "D Cossock; T Zhang"}, {"ref_id": "b8", "title": "Log-linear models for label ranking", "journal": "", "year": "2003", "authors": "O Dekel; C D Manning; Y Singer"}, {"ref_id": "b9", "title": "Tighter bounds for structured estimation", "journal": "", "year": "2008", "authors": "C B Do; Q Le; C H Teo; O Chapelle; A Smola"}, {"ref_id": "b10", "title": "On the consistency of ranking algorithms", "journal": "", "year": "2010", "authors": "J Duchi; L W Mackey; M I Jordan"}, {"ref_id": "b11", "title": "An efficient boosting algorithm for combining preferences", "journal": "J. of Mach. Learn. Res", "year": "2003", "authors": "Y Freund; R Iyer; R E Schapire; Y Singer"}, {"ref_id": "b12", "title": "Label ranking by learning pairwise preferences", "journal": "Artificial Intelligence", "year": "2008-11", "authors": "E Hullermeier; J Furnkranz; W Cheng; K Brinker"}, {"ref_id": "b13", "title": "Optimizing search engines using clickthrough data", "journal": "", "year": "2002", "authors": "T Joachims"}, {"ref_id": "b14", "title": "Bipartite ranking through minimization of univariate loss", "journal": "", "year": "2011", "authors": "W Kotlowski; K Dembczynski; E Huellermeier"}, {"ref_id": "b15", "title": "Learning to rank for information retrieval", "journal": "Foundations and Trends in Information Retrieval", "year": "2009-03", "authors": "T.-Y Liu"}, {"ref_id": "b16", "title": "On ndcg consistency of listwise ranking methods", "journal": "J. of Mach. Learn. Res. -Proc. Track", "year": "2011", "authors": "P D Ravikumar; A Tewari; E Yang"}, {"ref_id": "b17", "title": "Surrogate Regret Bounds for Proper Losses", "journal": "", "year": "2009", "authors": "M D Reid; R C Williamson"}, {"ref_id": "b18", "title": "Surrogate losses and regret bounds for cost-sensitive classification with example-dependent costs", "journal": "", "year": "2011", "authors": "C Scott"}, {"ref_id": "b19", "title": "How to compare different loss functions and their risks", "journal": "Constructive Approximation", "year": "2007", "authors": "I Steinwart"}, {"ref_id": "b20", "title": "Softrank: optimizing non-smooth rank metrics", "journal": "", "year": "2008", "authors": "M Taylor; J Guiver; S Robertson; T Minka"}, {"ref_id": "b21", "title": "TREC: experiment and evaluation in information retrieval. Digital libraries and electronic publishing", "journal": "MIT Press", "year": "2005", "authors": "E Voorhees; D Harman; N I Standards; T "}, {"ref_id": "b22", "title": "Continuity of the bayes risk", "journal": "The Annals of Math. Stat", "year": "1970", "authors": "R A Wijsman"}, {"ref_id": "b23", "title": "Directly optimizing evaluation measures in learning to rank", "journal": "", "year": "2008", "authors": "J Xu; T.-Y Liu; M Lu; H Li; W.-Y Ma"}, {"ref_id": "b24", "title": "A support vector method for optimizing average precision", "journal": "", "year": "2007", "authors": "Y Yue; T Finley; F Radlinski; T Joachims"}, {"ref_id": "b25", "title": "Statistical analysis of some multi-category large margin classification methods", "journal": "J. of Mach. Learn. Res", "year": "2004", "authors": "T Zhang"}, {"ref_id": "b26", "title": "Statistical behavior and consistency of classification methods based on convex risk minimization. The Annals of Stat", "journal": "", "year": "2004", "authors": "T Zhang"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "y\u2208Y p(y) (y, s) and R (p, s) = \u03c3\u2208arg sort(s) R (p, \u03c3) | arg sort(s)| , where \u2200\u03c3 \u2208 S n , R (p, \u03c3) = y\u2208Y p(y)r (y, \u03c3) . Their optimal values are denoted by L (p) = inf s\u2208R n L (p, s) and R (p) = R (p) = min \u03c3\u2208Sn R (p, \u03c3).", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Lemma 5 .5The following claims are true:(i) \u2200p \u2208 P, \u2200\u03b4 > 0, A (p) \u2286 arg sort M (p, \u03b4) . (ii) \u2200p \u2208 P, \u2203\u03b4 0 > 0 : A (p) = arg sort M (p, \u03b4 0 ) . (iii) is r-calibrated if and only if: \u2200p \u2208 P, A (p) \u2286 A r (p).Proof. (i) Fix p \u2208 P and \u03b4 > 0. Let \u03c3 \u2208 A (p). By the definition of L, there is an s \u2208 R n such that \u03c3 \u2208 arg sort(s) and L (p, s) \u2212 L (p, \u03c3) < \u03b4. Since L (p, \u03c3) = min \u03c3 \u2208Sn L (p, \u03c3 ) = L (p), we have L (p, s) \u2212 L (p) < \u03b4 . This proves s \u2208 M (p, \u03b4) and thus \u03c3 \u2208 arg sort M (p, \u03b4) .", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Formulas of r(y, \u03c3) for some common ranking evaluation metrics", "figure_data": "TYPE OF FEEDBACKMETRICFORMULA"}], "formulas": [{"formula_id": "formula_0", "formula_text": "y \u2208 Y = {0, ..., Y } n , Y \u2208 N, Y \u2265 1 Discounted Cumulative Gain (higher values mean better performances) n k=1 2 y \u03c3(k) \u22121 log(1+k)", "formula_coordinates": [3.0, 126.58, 128.82, 332.3, 38.98]}, {"formula_id": "formula_1", "formula_text": "n k=1 R k k k\u22121 q=1 (1 \u2212 R q ), R k = 2 y \u03c3(k) -1 2 Y y \u2208 Y = {0, 1}n", "formula_coordinates": [3.0, 135.43, 154.11, 367.95, 44.69]}, {"formula_id": "formula_2", "formula_text": "1 |{i:yi=1}| i:yi=1 \u03c3 -1 (i) k=1 y \u03c3(k) \u03c3 -1 (i) y \u2208 Y = all DAGs over [n]", "formula_coordinates": [3.0, 113.98, 180.3, 372.64, 40.45]}, {"formula_id": "formula_3", "formula_text": "I \u03c3 -1 (i) > \u03c3 -1 (j)", "formula_coordinates": [3.0, 409.12, 210.44, 72.3, 10.09]}, {"formula_id": "formula_4", "formula_text": "\u2200s = (s 1 , ..., s n ) \u2208 R n , arg sort(s) = \u03c3 \u2208 S n |\u2200k \u2208 [n \u2212 1] , s \u03c3(k) \u2265 s \u03c3(k+1) ,", "formula_coordinates": [3.0, 137.44, 416.97, 337.13, 12.03]}, {"formula_id": "formula_5", "formula_text": "\u2200y \u2208 Y, \u2200s \u2208 R n , r (y, s) = \u03c3\u2208arg sort(s) r (y, \u03c3) | arg sort(s)| .", "formula_coordinates": [3.0, 186.96, 452.72, 238.07, 27.27]}, {"formula_id": "formula_6", "formula_text": "L(D, f k ) \u2212\u2192 k\u2192\u221e inf f L(D, f ) \u21d2 R (D, f k ) \u2212\u2192 k\u2192\u221e inf f R (D, f )(1)", "formula_coordinates": [3.0, 178.4, 690.73, 325.61, 14.69]}, {"formula_id": "formula_7", "formula_text": "(A) \u2203\u03b2 1 < \u03b2 2 < ... < \u03b2 n such that \u2200i \u2208 [n] , \u2203y \u2208 Y : \u2200\u03c3 \u2208 S n , r(y, \u03c3) = \u03b2 \u03c3 -1 (i) .", "formula_coordinates": [4.0, 125.06, 189.06, 338.38, 10.03]}, {"formula_id": "formula_8", "formula_text": "L (p, s) =", "formula_coordinates": [4.0, 245.3, 603.86, 40.67, 8.77]}, {"formula_id": "formula_9", "formula_text": "\u2200p \u2208 P, \u2203\u03b4 > 0 s.t. arg sort(M (p, \u03b4)) = A r (p) .", "formula_coordinates": [5.0, 204.87, 384.44, 204.76, 9.65]}, {"formula_id": "formula_10", "formula_text": "\u2200p \u2208 P, \u2200i, j \u2208 [n] , \u2200\u03c3, \u03c3 \u2208 A r (p), \u03c3 -1 (i) < \u03c3 -1 (j) and \u03c3 -1 (i) > \u03c3 -1 (j) \u21d2 \u2203s \u2208 R n : s i = s j and arg sort(s) \u2286 A r (p) .(2)", "formula_coordinates": [5.0, 115.81, 558.0, 388.19, 25.89]}, {"formula_id": "formula_11", "formula_text": "t \u2192 \u03b3 i (t) \u2212 \u03b3 j (t) for t \u2208 [0, 1].", "formula_coordinates": [5.0, 150.69, 682.57, 132.23, 9.65]}, {"formula_id": "formula_12", "formula_text": "(1 \u2212 \u03b1)p 110 + \u03b1p 001 ERR \u03b1 \u2208 1 3 , 1 2 {(1 3 2), (2 3 1)} AP \u03b1 = 5 13 {(1 2 3), (3 1 2), (2 1 3), (3 2 1)} (1 \u2212 \u03b1)p 1 2 3 + \u03b1p 3 1 PD \u03b1 \u2208 2 3 , 1 {(2 3 1), (3 1 2)}", "formula_coordinates": [6.0, 130.22, 201.04, 351.56, 58.31]}, {"formula_id": "formula_13", "formula_text": "(a) (i j k), (j i k) \u2286 A r (p(\u03b1)) , (b) (k i j), (k j i) \u2286 A r (p(\u03b1))", "formula_coordinates": [6.0, 111.93, 511.52, 388.13, 9.65]}, {"formula_id": "formula_14", "formula_text": "A (p) = arg min \u03c3\u2208Sn L (p, \u03c3) where L (p, \u03c3) = inf L (p, s) | s \u2208 R n s.t. \u03c3 \u2208 arg sort(s) .", "formula_coordinates": [7.0, 127.05, 337.43, 357.91, 18.65]}, {"formula_id": "formula_15", "formula_text": "\u2200\u03c3 \u2208 A (p) , L (p , \u03c3 ) = L (p , \u03c3 ) \u2212 L (p, \u03c3 ) + L (p, \u03c3 ) \u2212 L (p) + L (p) > \u2212\u03b5 + 2\u03b5 + L (p) .", "formula_coordinates": [8.0, 110.82, 238.97, 390.36, 8.74]}], "doi": ""}