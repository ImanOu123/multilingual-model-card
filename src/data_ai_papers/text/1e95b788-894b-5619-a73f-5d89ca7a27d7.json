{"title": "Learning to Rank using Gradient Descent", "authors": "Chris Burges; Erin Renshaw; Nicole Hamilton; Greg Hullender", "pub_date": "", "abstract": "We investigate using gradient descent methods for learning ranking functions; we propose a simple probabilistic cost function, and we introduce RankNet, an implementation of these ideas using a neural network to model the underlying ranking function. We present test results on toy data and on data from a commercial internet search engine.", "sections": [{"heading": "Introduction", "text": "Any system that presents results to a user, ordered by a utility function that the user cares about, is performing a ranking function. A common example is the ranking of search results, for example from the Web or from an intranet; this is the task we will consider in this paper. For this problem, the data consists of a set of queries, and for each query, a set of returned documents. In the training phase, some query/document pairs are labeled for relevance (\"excellent match\", \"good match\", etc.). Only those documents returned for a given query are to be ranked against each other. Thus, rather than consisting of a single set of objects to be ranked amongst each other, the data is instead partitioned by query. In this paper we propose a new approach to this problem. Our approach follows (Herbrich et al., 2000) in that we train on pairs of examples to learn a ranking function that maps to the reals (having the model evaluate on pairs would be prohibitively slow for many applications). However (Herbrich et al., 2000) cast the ranking problem as an ordinal regression problem; rank boundaries play a critical role during training, as they do for several other algorithms (Crammer & Singer, 2002;Harrington, 2003). For our application, given that item A appears higher than item B in the output list, the user concludes that the system ranks A higher than, or equal to, B; no mapping to particular rank values, and no rank boundaries, are needed; to cast this as an ordinal regression problem is to solve an unnecessarily hard problem, and our approach avoids this extra step. We also propose a natural probabilistic cost function on pairs of examples. Such an approach is not specific to the underlying learning algorithm; we chose to explore these ideas using neural networks, since they are flexible (e.g. two layer neural nets can approximate any bounded continuous function (Mitchell, 1997)), and since they are often faster in test phase than competing kernel methods (and test speed is critical for this application); however our cost function could equally well be applied to a variety of machine learning algorithms. For the neural net case, we show that backpropagation (LeCun et al., 1998) is easily extended to handle ordered pairs; we call the resulting algorithm, together with the probabilistic cost function we describe below, RankNet. We present results on toy data and on data gathered from a commercial internet search engine. For the latter, the data takes the form of 17,004 queries, and for each query, up to 1000 returned documents, namely the top docu-ments returned by another, simple ranker. Thus each query generates up to 1000 feature vectors.\nNotation: we denote the number of relevance levels (or ranks) by N , the training sample size by m, and the dimension of the data by d.", "publication_ref": ["b10", "b10", "b5", "b8", "b15", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Previous Work", "text": "RankProp (Caruana et al., 1996) is also a neural net ranking model. RankProp alternates between two phases: an MSE regression on the current target values, and an adjustment of the target values themselves to reflect the current ranking given by the net. The end result is a mapping of the data to a large number of targets which reflect the desired ranking, which performs better than just regressing to the original, scaled rank values. Rankprop has the advantage that it is trained on individual patterns rather than pairs; however it is not known under what conditions it converges, and it does not give a probabilistic model. (Herbrich et al., 2000) cast the problem of learning to rank as ordinal regression, that is, learning the mapping of an input vector to a member of an ordered set of numerical ranks. They model ranks as intervals on the real line, and consider loss functions that depend on pairs of examples and their target ranks. The positions of the rank boundaries play a critical role in the final ranking function. (Crammer & Singer, 2002) cast the problem in similar form and propose a ranker based on the perceptron ('PRank'), which maps a feature vector x \u2208 R d to the reals with a learned w \u2208 R d such that the output of the mapping function is just w \u2022 x. PRank also learns the values of N increasing thresholds 1 b r = 1, \u2022 \u2022 \u2022 , N and declares the rank of x to be min r {w \u2022 x \u2212 b r < 0}. PRank learns using one example at a time, which is held as an advantage over pair-based methods (e.g. (Freund et al., 2003)), since the latter must learn using O(m 2 ) pairs rather than m examples. However this is not the case in our application; the number of pairs is much smaller than m 2 , since documents are only compared to other documents retrieved for the same query, and since many feature vectors have the same assigned rank. We find that for our task the memory usage is strongly dominated by the feature vectors themselves. Although the linear version is an online algorithm 2 , PRank has been compared to batch ranking algorithms, and a quadratic kernel version was found to outperform all such algorithms described in (Herbrich et al., 2000). (Harrington, 2003) has proposed a simple but very effective extension of PRank, which approximates finding the Bayes point by averaging over PRank models. Therefore in this paper we will compare RankNet with PRank, kernel PRank, large margin PRank, and RankProp. (Dekel et al., 2004) provide a very general framework for ranking using directed graphs, where an arc from A to B means that A is to be ranked higher than B (which here and below we write as A B). This approach can represent arbitrary ranking functions, in particular, ones that are inconsistent -for example A B, B C, C A. We adopt this more general view, and note that for ranking algorithms that train on pairs, all such sets of relations can be captured by specifying a set of training pairs, which amounts to specifying the arcs in the graph. In addition, we introduce a probabilistic model, so that each training pair {A, B} has associated posterior P (A B). This is an important feature of our approach, since ranking algorithms often model preferences, and the ascription of preferences is a much more subjective process than the ascription of, say, classes. (Target probabilities could be measured, for example, by measuring multiple human preferences for each pair.) Finally, we use cost functions that are functions of the difference of the system's outputs for each member of a pair of examples, which encapsulates the observation that for any given pair, an arbitrary offset can be added to the outputs without changing the final ranking; again, the goal is to avoid unnecessary learning.\nRankBoost (Freund et al., 2003) is another ranking algorithm that is trained on pairs, and which is closer in spirit to our work since it attempts to solve the preference learning problem directly, rather than solving an ordinal regression problem. In (Freund et al., 2003), results are given using decision stumps as the weak learners. The cost is a function of the margin over reweighted examples. Since boosting can be viewed as gradient descent (Mason et al., 2000), the question naturally arises as to how combining RankBoost with our pair-wise differentiable cost function would compare. Due to space constraints we will describe this work elsewhere.", "publication_ref": ["b4", "b10", "b5", "b7", "b10", "b8", "b6", "b7", "b7", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "A Probabilistic Ranking Cost Function", "text": "We consider models where the learning algorithm is given a set of pairs of samples [A, B] in R d , together with target probabilitiesP AB that sample A is to be ranked higher than sample B. This is a general formulation: the pairs of ranks need not be complete (in that taken together, they need not specify a complete ranking of the training data), or even consistent. We consider models f : R d \u2192 R such that the rank order of a set of test samples is specified by the real values that f takes, specifically, f (x 1 ) > f (x 2 ) is taken to mean that the model asserts that x 1 x 2 .\nDenote the modeled posterior P (x i x j ) by P ij , i, j = 1, . . . , m, and letP ij be the desired target values for those posteriors. Define o i \u2261 f (x i ) and o ij \u2261 f (x i ) \u2212 f (x j ). We will use the cross entropy cost function\nC ij \u2261 C(o ij ) = \u2212P ij log P ij \u2212 (1 \u2212P ij ) log (1 \u2212 P ij ) (1)\nwhere the map from outputs to probabilities are modeled using a logistic function (Baum & Wilczek, 1988)\nP ij \u2261 e oij 1 + e oij\n(2)\nC ij then becomes\nC ij = \u2212P ij o ij + log(1 + e oij )(3)\nNote that C ij asymptotes to a linear function; for problems with noisy labels this is likely to be more robust than a quadratic cost. Also, whenP ij = 1 2 (when no information is available as to the relative rank of the two patterns), C ij becomes symmetric, with its minimum at the origin. This gives us a principled way of training on patterns that are desired to have the same rank; we will explore this below. We plot C ij as a function of o ij in the left hand panel of Figure 1, for the three valuesP = {0, 0.5, 1}.", "publication_ref": ["b0"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Combining Probabilities", "text": "The above model puts consistency requirements on th\u0113 P ij , in that we require that there exist 'ideal' outputs o i of the model such that\nP ij \u2261 e\u014d ij 1 + e\u014d ij (4)\nwhere\u014d ij \u2261\u014d i \u2212\u014d j . This consistency requirement arises because if it is not met, then there will exist no set of outputs of the model that give the desired pairwise probabilities. The consistency condition leads to constraints on possible choices of theP 's. For example, givenP ij andP jk , Eq. (4) gives\nP ik =P ijPjk 1 + 2P ijPjk \u2212P ij \u2212P jk (5)\nThis is plotted in the right hand panel of Figure 1, for the caseP ij =P jk = P . We draw attention to some appealing properties of the combined probabil-ityP ik . First,P ik = P at the three points P = 0, P = 0.5 and P = 1, and only at those points. For example, if we specify that P (A B) = 0.5 and that P (B C) = 0.5, then it follows that P (A C) = 0.5; complete uncertainty propagates. Complete certainty (P = 0 or P = 1) propagates similarly. Finally confidence, or lack of confidence, builds as expected: for 0 < P < 0.5, thenP ik < P , and for 0.5 < P < 1.0, thenP ik > P (for example, if P (A B) = 0.6, and P (B C) = 0.6, then P (A C) > 0.6). These considerations raise the following question: given the consistency requirements, how much freedom is there to choose the pairwise probabilities? We have the following 3\nTheorem: Given a sample set x i , i = 1, . . . , m and any permutation Q of the consecutive integers {1, 2, . . . , m}, suppose that an arbitrary target posterior 0 \u2264P kj \u2264 1 is specified for every adjacent pair k = Q(i), j = Q(i + 1), i = 1, . . . , m \u2212 1. Denote the set of suchP 's, for a given choice of Q, a set of 'adjacency posteriors'. Then specifying any set of adjacency posteriors is necessary and sufficient to uniquely identify a target posterior 0 \u2264P ij \u2264 1 for every pair of samples x i , x j .\nProof: Sufficiency: suppose we are given a set of adjacency posteriors. Without loss of generality we can relabel the samples such that the adjacency posteriors may be writtenP i,i+1 , i = 1, . . . , m \u2212 1. From Eq. (4),\u014d is just the log odds:\no ij = logP ij 1 \u2212P ij (6)\nFrom its definition as a difference, any\u014d jk , j \u2264 k, can be computed as k\u22121 m=j\u014d m,m+1 . Eq. ( 4) then shows that the resulting probabilities indeed lie in [0, 1]. Uniqueness can be seen as follows: for any i, j, P ij can be computed in multiple ways, in that given a set of previously computed posteriorsP im1 ,P m1m2 , \u2022 \u2022 \u2022 ,P mnj , thenP ij can be computed by first computing the corresponding\u014d kl 's, adding them, and then using (4). However since\u014d kl =\u014d k \u2212\u014d l , the intermediate terms cancel, leaving just\u014d ij , and the resultingP ij is unique. Necessity: if a target posterior is specified for every pair of samples, then by definition for any Q, the adjacency posteriors are specified, since the adjacency posteriors are a subset of the set of all pairwise posteriors. Although the above gives a straightforward method for computingP ij given an arbitrary set of adjacency posteriors, it is instructive to compute theP ij for the special case when all adjacency posteriors are equal to some value P . Then\u014d i,i+1 = log(P/(1\u2212P )), and\u014d i,i+n\no i -o j C ij -5 -4 -3 -2 -1 0 1 2 3 4 5\n=\u014d i,i+1 +\u014d i+1,i+2 + \u2022 \u2022 \u2022 +\u014d i+n\u22121,i+n = n\u014d i,i+1 gives P i,i+n = \u2206 n /(1 + \u2206 n ),\nwhere \u2206 is the odds ratio \u2206 = P/(1\u2212P ). The expected strengthening (or weakening) of confidence in the ordering of a given pair, as their difference in ranks increases, is then captured by:\nLemma: Let n > 0. Then if P > 1 2 , then P i,i+n \u2265 P with equality when n = 1, and P i,i+n increases strictly monotonically with n. If P < 1 2 , then P i,i+n \u2264 P with equality when n = 1, and P i,i+n decreases strictly monotonically with n. If P = 1 2 , then P i,i+n = 1 2 for all n.\nProof: Assume that n > 0. Since P i,i+n = 1/(1 + ( 1\u2212P P ) n ), then for P > 1 2 , 1\u2212P P < 1 and the denominator decreases strictly monotonically with n; and for P < 1 2 , 1\u2212P P > 1 and the denominator increases strictly monotonically with n; and for P = 1 2 , P i,i+n = 1 2 by substitution. Finally if n = 1, then P i,i+n = P by construction.\nWe end this section with the following observation. In (Hastie & Tibshirani, 1998) and (Bradley & Terry, 1952), the authors consider models of the following form: for some fixed set of events A 1 , . . . , A k , pairwise probabilities P (A i |A i or A j ) are given, and it is assumed that there is a set of probabilitiesP i such that P (A i |A i or A j ) =P i /(P i +P j ). In our model, one might modelP i as N exp(o i ), where N is an overall normalization. However the assumption of the exis-tence of such underlying probabilities is overly restrictive for our needs. For example, there exists no un-derlyingP i which reproduce even a simple 'certain' ranking P (A B) = P (B C) = P (A C) = 1.", "publication_ref": ["b9", "b1"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "RankNet: Learning to Rank with Neural Nets", "text": "The above cost function is quite general; here we explore using it in neural network models, as motivated above. It is useful first to remind the reader of the back-prop equations for a two layer net with q output nodes (LeCun et al., 1998). For the ith training sample, denote the outputs of net by o i , the targets by t i , let the transfer function of each node in the jth layer of nodes be g j , and let the cost function be \no i = g 3 \uf8eb \uf8ed j w 32 ij g 2 k w 21 jk x k + b 2 j + b 3 i \uf8f6 \uf8f8 \u2261 g 3 i (7\n) where for the weights w and offsets b, the upper indices index the node layer, and the lower indices index the nodes within each corresponding layer. Taking derivatives of f with respect to the parameters gives\n\u2202f \u2202b 3 i = \u2202f \u2202o i g 3 i \u2261 \u2206 3 i (8) \u2202f \u2202w 32 in = \u2206 3 i g 2 n (9) \u2202f \u2202b 2 m = g 2 m i \u2206 3 i w 32 im \u2261 \u2206 2 m (10) \u2202f \u2202w 21 mn = x n \u2206 2 m (11)\nwhere x n is the nth component of the input.\nTurning now to a net with a single output, the above is generalized to the ranking problem as follows. The cost function becomes a function of the difference of the outputs of two consecutive training samples:\nf (o 2 \u2212 o 1 ).\nHere it is assumed that the first pattern is known to rank higher than, or equal to, the second (so that, in the first case, f is chosen to be monotonic increasing). Note that f can include parameters encoding the weight assigned to a given pair. A forward prop is performed for the first sample; each node's activation and gradient value are stored; a forward prop is then performed for the second sample, and the activations and gradients are again stored. The gradient of the cost is then \u2202f \u2202\u03b1 = \u2202o2 \u2202\u03b1 \u2212 \u2202o1 \u2202\u03b1 f . We use the same notation as before but add a subscript, 1 or 2, denoting which pattern is the argument of the given function, and we drop the index on the last layer. Thus, denot-\ning f \u2261 f (o 2 \u2212 o 1 ), we have \u2202f \u2202b 3 = f (g 3 2 \u2212 g 3 1 ) \u2261 \u2206 3 2 \u2212 \u2206 3 1 (12\n)\n\u2202f \u2202w 32 m = \u2206 3 2 g 2 2m \u2212 \u2206 3 1 g 2 1m (13\n)\n\u2202f \u2202b 2 m = \u2206 3 2 w 32 m g 2 2m \u2212 \u2206 3 1 w 32 m g 2 1m (14\n)\n\u2202f \u2202w 21 mn = \u2206 2 2m g 1 2n \u2212 \u2206 2 1m g 1 1n(15)\nNote that the terms always take the form of the difference of a term depending on x 1 and a term depending on x 2 , 'coupled' by an overall multiplicative factor of f , which depends on both 4 . A sum over weights does not appear because we are considering a two layer net with one output, but for more layers the sum appears as above; thus training RankNet is accomplished by a straightforward modification of back-prop.", "publication_ref": ["b13"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments on Artificial Data", "text": "In this section we report results for RankNet only, in order to validate and explore the approach.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Data, and Validation Tests", "text": "We created artificial data in d = 50 dimensions by constructing vectors with components chosen randomly in the interval [\u22121, 1]. We constructed two target ranking functions. For the first, we used a two layer neural net with 50 inputs, 10 hidden units and one output unit, and with weights chosen randomly and uniformly from [\u22121, 1]. Labels were then computed by passing the data through the net and binning the outputs into one of 6 bins (giving 6 relevance levels). For the second, for each input vector x, we computed the mean of three terms, where each term was scaled to have zero mean and unit variance over the data. The first term was the dot product of x with a fixed random vector.\nFor the second term we computed a random quadratic polynomial by taking consecutive integers 1 through d, randomly permuting them to form a permutation index Q(i), and computing i x i x Q(i) . The third term was computed similarly, but using two random permutations to form a random cubic polynomial of the coefficients. The two ranking functions were then used to create 1,000 files with 50 feature vectors each. Thus for the search engine task, each file corresponds to 50 documents returned for a single query. Up to 800 files were then used for training, and 100 for validation, 100 for test.\nWe checked that a net with the same architecture as that used to create the net ranking function (i.e. two layers, ten hidden units), but with first layer weights initialized to zero and second layer initialized randomly in [\u22120.1, 0.1], could learn 1000 train vectors (which gave 20,382 pairs; for a given query with n i documents with label i = 1, . . . , L, the number of pairs is L j=2 (n j j\u22121 i=1 n i )) with zero error. In all our RankNet experiments, the initial learning rate was set to 0.001, and was halved if the average error in an epoch was greater than that of the previous epoch; also, hard target probabilities (1 or 0) were used throughout, except for the experiments in Section 5.2. The number of pairwise errors, and the averaged cost function, were found to decrease approximately monotonically on the training set. The net that gave minimum validation error (9.61%) was saved and used to test on the test set, which gave 10.01% error rate.\nTable 1 shows the test error corresponding to minimal validation error for variously sized training sets, for the two tasks, and for a linear net and a two layer net with five hidden units (recall that the random net used to generate the data has ten hidden units). We used validation and test sets of size 5000 feature vectors. The training ran for 100 epochs or until the error on the training set fell to zero. Although the two layer net gives improved performance for the random network data, it does not for the polynomial data; as expected, a random polynomial is a much harder function to learn. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Allowing Ties", "text": "Table 2 compares results, for the polynomial ranking function, of training on ties, assigning P = 1 for nonties and P = 0.5 for ties, using a two layer net with 10 hidden units. The number of training pairs are shown in parentheses. The Table shows the pairwise test error for the network chosen by highest accuracy on the validation set over 100 training epochs. We conclude that for this kind of data at least, training on ties makes little difference. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Experiments on Real Data", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Data and Error Metric", "text": "We report results on data used by an internet search engine. The data for a given query is constructed from that query and from a precomputed index. Querydependent features are extracted from the query combined with four different sources: the anchor text, the URL, the document title and the body of the text. Some additional query-independent features are also used. In all, we use 569 features, many of which are counts. As a preprocessing step we replace the counts by their logs, both to reduce the range, and to allow the net to more easily learn multiplicative relationships. The data comprises 17,004 queries for the English / US market, each with up to 1000 returned documents. We shuffled the data and used 2/3 (11,336 queries) for training and 1/6 each (2,834 queries) for validation and testing. For each query, one or more of the returned documents had a manually generated rating, from 1 (meaning 'poor match') to 5 (meaning 'excellent match'). Unlabeled documents were given rating 0. Ranking accuracy was computed using a normalized discounted cumulative gain measure (NDCG) (Jarvelin & Kekalainen, 2000). We chose to compute the NDCG at rank 15, a little beyond the set of documents initially viewed by most users. For a given query q i , the results are sorted by decreasing score output by the algorithm, and the NDCG is then computed as\nN i \u2261 N i 15 j=1\n(2 r(j) \u2212 1)/ log(1 + j)\nwhere r(j) is the rating of the j'th document, and where the normalization constant N i is chosen so that a perfect ordering gets NDCG score 1. For those queries with fewer than 15 returned documents, the NDCG was computed for all the returned documents.\nNote that unlabeled documents does not contribute to the sum directly, but will still reduce the NDCG by displacing labeled documents; also note that N i = 1 is an unlikely event, even for a perfect ranker, since some unlabeled documents may in fact be highly relevant. The labels were originally collected for evaluation and comparison of top ranked documents, so the 'poor' rating sometimes applied to documents that were still in fact quite relevant. To circumvent this problem, we also trained on randomly chosen unlabeled documents as extra examples of low relevance documents. We chose as many of these as would still fit in memory (2% of the unlabeled training data). This resulted in our training on 384,314 query/document feature vectors, and on 3,464,289 pairs.", "publication_ref": ["b11"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "We trained six systems: for PRank, a linear and quadratic kernel (Crammer & Singer, 2002) and the Online Aggregate PRank -Bayes Point Machine (OAP-BPM), or large margin (Harrington, 2003) versions; a single layer net trained with RankProp; and for RankNet, a linear net and a two layer net with 10 hidden units. All tests were performed using a 3GHz machine, and each process was limited to about 1GB memory. For the kernel PRank model, training was found to be prohibitively slow, with just one epoch taking over 12 hours. Rather than learning with the quadratic kernel and then applying a reduced set method (Burges, 1996), we simply added a further step of preprocessing, taking the features, and The model that gave the best results were kept, and then used to test on the 2,834 query test set. For large margin PRank, the validation set was also used to choose between three values of the Bernoulli mean, \u03c4 = {0.3, 0.5, 0.7} (Harrington, 2003), and to choose the number of perceptrons averaged over; the best validation results were found for \u03c4 = 0.3 and 100 perceptrons.  were tested on, and the number of documents used in the validation and test phases are much larger than could be used for training (cf. Table 3). Note also that the fraction of labeled documents in the test set is only approximately 1%, so the low NDCG scores are likely to be due in part to relevant but unlabeled documents being given high rank. Although the difference in NDCG for the linear and two layer nets is not statistically significant at the 5% standard error level, a Wilcoxon rank test shows that the null hypothesis (that the medians are the same) can be rejected at the 16% level. Table 5 shows the results of testing on the training set; comparing Tables 4 and 5 shows that the linear net is functioning at capacity, but that the two layer net may still benefit from more training data. In ", "publication_ref": ["b5", "b8", "b3", "b8"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Discussion", "text": "Can these ideas be extended to the kernel learning framework? The starting point is the choice of a suitable cost function and function space (Sch\u00f6lkopf & Smola, 2002). We can again obtain a probabilistic model by writing the objective function as\nF = m i,j=1 C(P ij ,P ij ) + \u03bb f 2 H (17\n)\nwhere the second (regularization) term is the L 2 norm of f in the reproducing kernel Hilbert space H. F differs from the usual setup in that minimizing the first term results in outputs that model posterior probabilities of rank order; it shares the usual setup in the second term. Note that the representer theorem (Kimeldorf & Wahba, 1971;Sch\u00f6lkopf & Smola, 2002) applies to this case also: any solution f * that minimizes (17) can be written in the form\nf * (x) = m i=1 \u03b1 i k(x, x i ) (18)\nsince in the first term on the right of Eq. 17, the modeled function f appears only through its evaluations on training points. One could again certainly minimize Eq. 17 using gradient descent; however depending on the kernel, the objective function may not be convex. As our work here shows, kernel methods, for large amounts of very noisy training data, must be used with care if the resulting algorithm is to be wieldy.", "publication_ref": ["b17", "b12", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusions", "text": "We have proposed a probabilistic cost for training systems to learn ranking functions using pairs of training examples. The approach can be used for any differentiable function; we explored using a neural network formulation, RankNet. RankNet is simple to train and gives excellent performance on a real world ranking problem with large amounts of data. Comparing the linear RankNet with other linear systems clearly demonstrates the benefit of using our pair-based cost function together with gradient descent; the two layer net gives further improvement. For future work it will be interesting to investigate extending the approach to using other machine learning methods for the ranking function; however evaluation speed and simplicity is a critical constraint for such systems.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We thank John Platt and Leon Bottou for useful discussions, and Leon Wong and Robert Ragno for their support of this project.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Supervised learning of probability distributions by neural networks", "journal": "", "year": "1988", "authors": "E Baum; F Wilczek"}, {"ref_id": "b1", "title": "The Rank Analysis of Incomplete Block Designs 1: The Method of Paired Comparisons", "journal": "Biometrika", "year": "1952", "authors": "R Bradley; M Terry"}, {"ref_id": "b2", "title": "Signature Verification Using a \"Siamese\" Time Delay Neural Network", "journal": "World Scientific", "year": "1993", "authors": "J Bromley; J W Bentz; L Bottou; I Guyon; Y Le-Cun; C Moore; E Sackinger; R Shah"}, {"ref_id": "b3", "title": "Simplified support vector decision rules", "journal": "", "year": "1996", "authors": "C Burges"}, {"ref_id": "b4", "title": "Using the future to \"sort out\" the present: Rankprop and multitask learning for medical risk evaluation", "journal": "", "year": "1996", "authors": "R Caruana; S Baluja; T Mitchell"}, {"ref_id": "b5", "title": "Pranking with ranking", "journal": "NIPS", "year": "2002", "authors": "K Crammer; Y Singer"}, {"ref_id": "b6", "title": "Loglinear models for label-ranking", "journal": "NIPS", "year": "2004", "authors": "O Dekel; C Manning; Y Singer"}, {"ref_id": "b7", "title": "An efficient boosting algorithm for combining preferences", "journal": "Journal of Machine Learning Research", "year": "2003", "authors": "Y Freund; R Iyer; R Schapire; Y Singer"}, {"ref_id": "b8", "title": "Online ranking/collaborative filtering using the Perceptron algorithm", "journal": "", "year": "2003", "authors": "E Harrington"}, {"ref_id": "b9", "title": "Classification by pairwise coupling", "journal": "NIPS", "year": "1998", "authors": "T Hastie; R Tibshirani"}, {"ref_id": "b10", "title": "Large margin rank boundaries for ordinal regression", "journal": "MIT Press", "year": "2000", "authors": "R Herbrich; T Graepel; K Obermayer"}, {"ref_id": "b11", "title": "IR evaluation methods for retrieving highly relevant documents", "journal": "", "year": "2000", "authors": "K Jarvelin; J Kekalainen"}, {"ref_id": "b12", "title": "Some results on Tchebycheffian Spline Functions", "journal": "J. Mathematical Analysis and Applications", "year": "1971", "authors": "G S Kimeldorf; G Wahba"}, {"ref_id": "b13", "title": "Efficient backprop. Neural Networks: Tricks of the Trade", "journal": "Springer", "year": "1998", "authors": "Y Lecun; L Bottou; G B Orr; K.-R M\u00fcller"}, {"ref_id": "b14", "title": "Boosting algorithms as gradient descent", "journal": "", "year": "2000", "authors": "L Mason; J Baxter; P Bartlett; M Frean"}, {"ref_id": "b15", "title": "Machine learning", "journal": "McGraw-Hill", "year": "1997", "authors": "T M Mitchell"}, {"ref_id": "b16", "title": "Probabilistic approaches for multiclass classification with neural networks", "journal": "", "year": "1991", "authors": "P Refregier; F Vallet"}, {"ref_id": "b17", "title": "Learning with kernels", "journal": "MIT Press", "year": "2002", "authors": "B Sch\u00f6lkopf; A Smola"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Appearing in Proceedings of the 22 nd International Conference on Machine Learning, Bonn, Germany, 2005. Copyright 2005 by the author(s)/owner(s).", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 .1Figure 1. Left: the cost function, for three values of the target probability. Right: combining probabilities", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "qi=1 f (o i , t i ). If \u03b1 k are the parameters of the model, then a gradient descent step amounts to \u03b4\u03b1 k = \u2212\u03b7 k \u2202f \u2202\u03b1 k , where the \u03b7 k are positive learning rates. The net embodies the function", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Test Pairwise % Correct for Random Network (Net) and Random Polynomial (Poly) Ranking Functions.", "figure_data": "Train Size1005002500 12500Net, Linear82.39 88.86 89.91 90.06Net, 2 Layer 82.29 88.80 96.94 97.67Poly, Linear59.63 66.68 68.30 69.00Poly, 2 Layer 59.54 66.97 68.56 69.27"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "The effect of training on ties for the polynomial ranking function.", "figure_data": "Train Size No TiesAll Ties1000.595 (2060)0.596 (2450)5000.670 (10282)0.669 (12250)10000.681 (20452)0.682 (24500)50000.690 (101858) 0.688 (122500)"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Sample sizes used for the experiments.", "figure_data": "Number of Queries Number of DocumentsTrain11,336384,314Valid2,8342,726,714Test2,8342,715,175every quadratic combination, as a new feature set. Al-though this resulted in feature vectors in a space ofvery high (162,734) dimension, it gave a far less com-plex system than the quadratic kernel. For each test,each algorithm was trained for 100 epochs (or for asmany epochs as required so that the training error didnot change for ten subsequent epochs), and after eachepoch it was tested on the 2,834 query validation set."}, {"figure_label": "45", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Results on the test set. Confidence intervals are the standard error at 95%. Results of testing on the 11,336 query training set.", "figure_data": "Mean NDCG ValidationTestQuad PRank0.3790.327\u00b10.011Linear PRank0.4100.412\u00b10.010OAP-BPM0.4550.454\u00b10.011RankProp0.4590.460\u00b10.011One layer net0.4790.477\u00b10.010Two layer net0.4890.488\u00b10.010Mean NDCG Training SetOne layer net 0.479\u00b10.005Two layer net 0.500\u00b10.005Table 3 collects statistics on the data used; the NDCGresults at rank 15 are shown, with 95% confidence in-tervals 5 , in Table 4. Note that testing was done inbatch mode (one query file tested on all models at atime), and so all returned documents for a given query"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Training times.", "figure_data": "ModelTrain TimeLinear PRank0hr 11 minRankProp0hr 23 minOne layer RankNet1hr 7minTwo layer RankNet 5hr 51minOAP-BPM10hr 23minQuad PRank39hr 52min"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "we show the wall clock time for training 100 epochs for each method. The quadratic PRank is slow largely because the quadratic features had to be computed on the fly. No algorithmic speedup techniques (LeCun et al., 1998) were implemented for the neural net training; the optimal net was found at epoch 20 for the linear net and epoch 22 for the two-layer net.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "C ij \u2261 C(o ij ) = \u2212P ij log P ij \u2212 (1 \u2212P ij ) log (1 \u2212 P ij ) (1)", "formula_coordinates": [3.0, 60.49, 203.76, 228.93, 22.16]}, {"formula_id": "formula_1", "formula_text": "P ij \u2261 e oij 1 + e oij", "formula_coordinates": [3.0, 141.6, 257.78, 59.2, 24.66]}, {"formula_id": "formula_2", "formula_text": "C ij = \u2212P ij o ij + log(1 + e oij )(3)", "formula_coordinates": [3.0, 109.53, 309.41, 179.88, 12.01]}, {"formula_id": "formula_3", "formula_text": "P ij \u2261 e\u014d ij 1 + e\u014d ij (4)", "formula_coordinates": [3.0, 141.6, 526.25, 147.82, 23.53]}, {"formula_id": "formula_4", "formula_text": "P ik =P ijPjk 1 + 2P ijPjk \u2212P ij \u2212P jk (5)", "formula_coordinates": [3.0, 105.89, 642.2, 183.53, 20.22]}, {"formula_id": "formula_5", "formula_text": "o ij = logP ij 1 \u2212P ij (6)", "formula_coordinates": [3.0, 388.13, 447.59, 153.29, 20.22]}, {"formula_id": "formula_6", "formula_text": "o i -o j C ij -5 -4 -3 -2 -1 0 1 2 3 4 5", "formula_coordinates": [4.0, 55.37, 130.72, 215.66, 114.6]}, {"formula_id": "formula_7", "formula_text": "=\u014d i,i+1 +\u014d i+1,i+2 + \u2022 \u2022 \u2022 +\u014d i+n\u22121,i+n = n\u014d i,i+1 gives P i,i+n = \u2206 n /(1 + \u2206 n ),", "formula_coordinates": [4.0, 55.44, 356.39, 233.96, 22.6]}, {"formula_id": "formula_8", "formula_text": "o i = g 3 \uf8eb \uf8ed j w 32 ij g 2 k w 21 jk x k + b 2 j + b 3 i \uf8f6 \uf8f8 \u2261 g 3 i (7", "formula_coordinates": [4.0, 316.34, 554.41, 220.84, 46.92]}, {"formula_id": "formula_9", "formula_text": "\u2202f \u2202b 3 i = \u2202f \u2202o i g 3 i \u2261 \u2206 3 i (8) \u2202f \u2202w 32 in = \u2206 3 i g 2 n (9) \u2202f \u2202b 2 m = g 2 m i \u2206 3 i w 32 im \u2261 \u2206 2 m (10) \u2202f \u2202w 21 mn = x n \u2206 2 m (11)", "formula_coordinates": [4.0, 372.61, 668.97, 168.81, 52.5]}, {"formula_id": "formula_10", "formula_text": "f (o 2 \u2212 o 1 ).", "formula_coordinates": [5.0, 55.44, 209.15, 48.1, 10.95]}, {"formula_id": "formula_11", "formula_text": "ing f \u2261 f (o 2 \u2212 o 1 ), we have \u2202f \u2202b 3 = f (g 3 2 \u2212 g 3 1 ) \u2261 \u2206 3 2 \u2212 \u2206 3 1 (12", "formula_coordinates": [5.0, 55.44, 364.56, 229.54, 53.2]}, {"formula_id": "formula_12", "formula_text": ")", "formula_coordinates": [5.0, 284.98, 400.98, 4.42, 9.96]}, {"formula_id": "formula_13", "formula_text": "\u2202f \u2202w 32 m = \u2206 3 2 g 2 2m \u2212 \u2206 3 1 g 2 1m (13", "formula_coordinates": [5.0, 97.94, 418.71, 187.05, 24.92]}, {"formula_id": "formula_14", "formula_text": ")", "formula_coordinates": [5.0, 284.98, 425.46, 4.42, 9.96]}, {"formula_id": "formula_15", "formula_text": "\u2202f \u2202b 2 m = \u2206 3 2 w 32 m g 2 2m \u2212 \u2206 3 1 w 32 m g 2 1m (14", "formula_coordinates": [5.0, 101.94, 445.65, 183.04, 24.93]}, {"formula_id": "formula_16", "formula_text": ")", "formula_coordinates": [5.0, 284.98, 452.39, 4.42, 9.96]}, {"formula_id": "formula_17", "formula_text": "\u2202f \u2202w 21 mn = \u2206 2 2m g 1 2n \u2212 \u2206 2 1m g 1 1n(15)", "formula_coordinates": [5.0, 94.16, 472.59, 195.24, 24.93]}, {"formula_id": "formula_18", "formula_text": "N i \u2261 N i 15 j=1", "formula_coordinates": [6.0, 351.71, 242.04, 51.83, 30.86]}, {"formula_id": "formula_20", "formula_text": "F = m i,j=1 C(P ij ,P ij ) + \u03bb f 2 H (17", "formula_coordinates": [7.0, 359.56, 579.36, 177.42, 30.6]}, {"formula_id": "formula_21", "formula_text": ")", "formula_coordinates": [7.0, 536.98, 589.32, 4.42, 9.96]}, {"formula_id": "formula_22", "formula_text": "f * (x) = m i=1 \u03b1 i k(x, x i ) (18)", "formula_coordinates": [8.0, 124.76, 87.54, 164.64, 30.61]}], "doi": ""}