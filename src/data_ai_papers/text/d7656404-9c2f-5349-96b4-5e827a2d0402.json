{"title": "Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning", "authors": "Lean Wang; Lei Li; Damai Dai; Deli Chen; Hao Zhou; Fandong Meng; Jie Zhou; Xu Sun", "pub_date": "", "abstract": "In-context learning (ICL) emerges as a promising capability of large language models (LLMs) by providing them with demonstration examples to perform diverse tasks. However, the underlying mechanism of how LLMs learn from the provided context remains under-explored. In this paper, we investigate the working mechanism of ICL through an information flow lens. Our findings reveal that label words in the demonstration examples function as anchors: (1) semantic information aggregates into label word representations during the shallow computation layers' processing; (2) the consolidated information in label words serves as a reference for LLMs' final predictions. Based on these insights, we introduce an anchor re-weighting method to improve ICL performance, a demonstration compression technique to expedite inference, and an analysis framework for diagnosing ICL errors in GPT2-XL. The promising applications of our findings again validate the uncovered ICL working mechanism and pave the way for future studies. 1   ", "sections": [{"heading": "Introduction", "text": "In-context Learning (ICL) has emerged as a powerful capability alongside the development of scaledup large language models (LLMs) (Brown et al., 2020). By instructing LLMs using few-shot demonstration examples, ICL enables them to perform a wide range of tasks, such as text classification (Min et al., 2022a) and mathematical reasoning . Since ICL does not require updates to millions or trillions of model parameters and relies on human-understandable natural language instructions (Dong et al., 2023), it has become a promising approach for harnessing the full potentiality of LLMs. Despite its significance, the inner working mechanism of ICL remains an open question, garnering considerable interest from research Figure 1: Visualization of the information flow in a GPT model performing ICL. The line depth reflects the significance of the information flow from the right word to the left. The flows involving label words are highlighted. Label words gather information from demonstrations in shallow layers, which is then extracted in deep layers for final prediction.\ncommunities (Xie et al., 2022;Dai et al., 2022;Aky\u00fcrek et al., 2022;Li et al., 2023b).\nIn this paper, we find that the label words serve as anchors that aggregate and distribute information in ICL. We first visualize the attention interactive pattern between tokens with a GPT model (Brown et al., 2020) on sentiment analysis (Figure 1). Initial observations suggest that label words aggregate information in shallow layers and distribute it in deep layers. 2 To draw a clearer picture of this phenomenon, we design two metrics based on saliency Figure 2: Illustration of our hypothesis. In shallow layers, label words gather information from demonstrations to form semantic representations for deeper processing, while deep layers extract and utilize this information from label words to formulate the final prediction.\nscores to portray the information flow in ICL and further propose the following hypothesis:\nInformation Flow with Labels as Anchors H 1 : In shallow layers, label words gather the information of demonstrations to form semantic representations for deeper layers. H 2 : In deep layers, the model extracts the information from label words to form the final prediction.\nTwo experiments are designed to validate the hypothesis using GPT2-XL (Radford et al., 2019) and GPT-J (Wang and Komatsuzaki, 2021) across several text classification benchmarks. (1) By blocking the information aggregation path to label words in certain layers, we find that such isolation in shallow layers significantly impairs model performance. This indicates that label words collect useful information during forward propagation in shallow layers. (2) We investigate the relationship between the attention distributions on the label words of the target position and the model's final prediction. Our results illustrate a strong positive correlation, where a candidate label's probability increases with more attention weight on its corresponding label token. In summary, these experimental findings suggest that our hypothesis holds well with large language models on real-world datasets.\nDrawing on insights from the information flow perspective, we explore three approaches to enhance ICL's effectiveness, efficiency, and interpretability. (1) An anchor re-weighting method is introduced, which employs a learnable vector to adjust the significance of different label words in demonstrations, leading to a 16.7% average accuracy boost compared to standard ICL baselines. (2) For quicker ICL inference, inputs are compressed into pre-calculated anchor representations since model predictions primarily rely on label word activations. Testing shows a 1.8 \u00d7 speedup in inference with only a minimal performance trade-off. (3) An error analysis of ICL on GPT2-XL demonstrates that the label confusion matrix aligns closely with the distance distribution of anchor key vectors, implying that errors might result from similar anchor representations. These promising applications further validate our hypothesis and shed light on future ICL studies for better transparency of LLMs.", "publication_ref": ["b1", "b13", "b4", "b23", "b3", "b0", "b1", "b15", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Label Words are Anchors", "text": "This section confirms the intuitive findings using two saliency score-based metrics as discussed in \u00a7 2.1. The quantitative results lead to a proposed hypothesis for the ICL working mechanism: H 1 : In shallow layers, label words aggregate information from demonstration examples to form semantic representations for later computations. H 2 : In deep layers, the model makes predictions by extracting information from label words. The validation for these hypotheses is presented in \u00a7 2.2 and \u00a7 2.3, respectively.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Hypothesis Motivated by Saliency Scores", "text": "This section aims to discover the inherent patterns in the attention interaction between tokens for a GPT model. The saliency technique (Simonyan et al., 2013), a common interpretation tool, is employed for highlighting critical token interactions. Following common practice, we use the Taylor expansion (Michel et al., 2019) to calculate the saliency score for each element of the attention matrix:\nI l = h A \u22a4 h,l \u2202L(x) \u2202A h,l .(1)\nHere, A h,l is the value of the attention matrix of the h-th attention head in the l-th layer, x is the input, and L(x) is the loss function of the task, e.g., the cross-entropy objective for a classification problem. We average all attention heads to obtain the saliency matrix I l for the l-th layer. I l (i, j) represents the significance of the information flow from the j-th word to the i-th word for ICL. By observing I l , we can get an intuitive impression that as the layer goes deeper, demonstration label words will become more dominant for the prediction, as depicted in Figure 1.\nTo draw a clearer picture of this phenomenon, we propose three quantitative metrics based on I l . Our focus lies in three components: (i) the label words, such as \"Negative\" and \"Positive\" in Figure 2, denoted as p 1 , ..., p C , where C represents the total number of label words; 3 (ii) the target position, where the model generates prediction labels (i.e., the final token in the input), which we denote as q; and (iii) the text part, i.e., the tokens before label words in the demonstration.\nThe definitions of the three quantitative metrics follow below. S wp , the mean significance of information flow from the text part to label words:\nSwp = (i,j)\u2208Cwp I l (i, j) |Cwp| , Cwp = {(p k , j) : k \u2208 [1, C], j < p k }.(2)\nS pq , the mean significance of information flow from label words to the target position:\nSpq = (i,j)\u2208Cpq I l (i, j) |Cpq| , Cpq = {(q, p k ) : k \u2208 [1, C]}.(3)\nS ww , the mean significance of the information flow amongst all words, excluding influences represented by S wp and S pq :\nSww = (i,j)\u2208Cww I l (i, j) |Cww| , Cww ={(i, j) : j < i} \u2212 Cwp \u2212 Cpq.(4)\nS wp , S pq , and S ww help assess different information flows in the model. S wp indicates the intensity of information aggregation onto label words. A high S pq demonstrates a strong information extraction from label words for final decision-making.\nS ww assesses average information flow among words, serving as a benchmark to gauge the intensity of the patterns identified by S wp and S pq .\nExperimental Settings We choose GPT2-XL from the GPT series (Radford et al., 2019) as our primary model for investigation, due to its moderate model size (of 1.5B parameters) that is suitable for our hardware resource and its decent ICL performance (Dai et al., 2022). For datasets, we use Stanford Sentiment Treebank Binary (SST-2) (Socher et al., 2013) for sentiment analysis, Text REtrieval Conference Question Classification (TREC) (Li and Roth, 2002;Hovy et al., 2001) for question type classification, AG's news topic classification dataset (AGNews) (Zhang et al., 2015) for topic classification, and EmoContext (EmoC) (Chatterjee et al., 2019) for emotion classification. Templates for constructing demonstrations are provided in Appendix A. 1000 examples are sampled from the test set for evaluation, with one demonstration per class sampled from the training set. Experiments with more demonstrations yield similar outcomes (refer to Appendix F.1 for details). Results reflect averages from five random seeds.\nResults and Analysis Figure 3 reveals that: (1) in shallow layers, S pq , the significance of the information flow from label words to targeted positions, is low, while S wp , the information flow from the text part to label words is high; (2) in deep layers, S pq , the importance of information flow from label words to the targeted position becomes the dominant one. Notably, S pq and S wp usually surpass S ww , suggesting that interactions involving label words outweigh others.\nProposed Hypothesis Based on this, we propose the hypothesis that label words function as anchors in the ICL information flow. In shallow layers, label words gather information from demonstration examples to form semantic representations for deeper layers, while in deep layers, the model extracts the information from label words to form the final prediction. Figure 2 gives an illustration for our hypothesis.", "publication_ref": ["b16", "b12", "b15", "b3", "b17", "b9", "b5", "b26", "b2"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Shallow Layers: Information Aggregation", "text": "In this part, we validate our hypothesis' first component. We assume that the information aggregation in ICL relies on the information flow from the text part to label tokens, which is facilitated by the transformer's attention mechanism. By manipulating  the attention layer in the model to block this flow and examining the model behavior change, we validate the existence of the information aggregation process and its contribution to the final prediction.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Settings", "text": "We retain the same test sample size of 1000 inputs as \u00a7 2.1. We use the same demonstration for a single random seed. To further validate our findings on larger models, we incorporate GPT-J (6B) (Wang and Komatsuzaki, 2021) in experiments, which exceeds GPT2-XL in model size and capacity.\nImplementation Details To block the information flow to label words, we isolate label words by manipulating the attention matrix A. Specifically, we set A l (p, i)(i < p) to 0 in the attention matrix A l of the l-th layer, where p represents label words and i represents preceding words. Consequently, in the l-th layer, label words cannot access Figure 4: The impact of isolating label words versus randomly isolating non-label words within the first or last 5 layers. Isolating label words within the first 5 layers exerts the most substantial impact, highlighting the importance of shallow-layer information aggregation via label words.\ninformation from the prior demonstration text.\nMetrics We use the following metrics to assess the impact of blocking information flow from the text part to label tokens: (1) Label Loyalty: measures the consistency of output labels with and without isolation.\n(2) Word Loyalty: employs the Jaccard similarity to compare the top-5 predicted words with and without isolation, capturing more subtle model output alterations (See Appendix C for details). Low loyalty indicates a profound impact of isolation on model predictions.\nResults and Analysis Figure 4 illustrates a notable influence on the model's behavior when label words are isolated within the first 5 layers. Yet, this influence becomes inconsequential within the last 5 layers, or when random non-label words are used. This observation underlines the fundamental importance of shallow-layer information aggregation via label words in ICL. It also emphasizes the superiority of label words over non-label words. Further tests with variable numbers of layers reaffirm these findings (Appendix D). Moreover, similar results were obtained when testing ICL with semantically unrelated labels (refer to Appendix F.2).", "publication_ref": ["b20"], "figure_ref": [], "table_ref": []}, {"heading": "Deep Layers: Information Extraction", "text": "We proceed to validate the latter part of our hypothesis that the model extracts information from label words to form the final prediction. We denote the sum of the attention matrices in the l-th layer as A l . 4 In deeper layers, we find a strong correlation between the attention distributions on the label words of the target position, represented as (A l (q, p 1 ), ..., A l (q, p C )), and the model's final prediction, affirming our hypothesis. The experimental setup mirrors that discussed in \u00a7 2.2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We utilize the AUC-ROC score to quantify the correlation between A l (q, p i ) and model prediction, which we denote as AUCROC l for the l-th layer.\nWe prefer the AUC-ROC metric due to two primary reasons: (1) A l (q, p i ) might differ from the probability of the model outputting label i by a constant factor. As Kobayashi et al. (2020) points out, attention should be multiplied by the norm of the key vector to yield 'more interpretable attention'. The AUC-ROC metric can implicitly account for these factors, thus allowing us to uncover the correlation more effectively.\n(2) The proportion of different labels output by the model may be unbalanced. Using the AUC-ROC metric can help mitigate this issue, reducing disturbances caused by class imbalance.\nConsidering the residual mechanism of transformers, we can view each layer's hidden state as the cumulative effect of all prior layer calculations. To quantify the accumulated contribution of the first l layers to model prediction, we introduce R l :\nR l = l i=1 (AUCROCi \u2212 0.5) N i=1 (AUCROCi \u2212 0.5) .\n(\n)5\nThis measure tracks the positive contribution above a baseline AUC-ROC threshold of 0.5. The value of R l signifies the proportional contribution of the first l layers to the model prediction.", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "Results and Analysis", "text": "Figures 5a and 5b delineate correlation metrics for GPT2-XL and GPT-J, averaged across four datasets. The AUCROC l for deep layers approaches 0.8, illustrating a strong correlation between the attention distributions on label words of the target position and the model's final prediction. Moreover, shallow layers show negligible cumulative contributions (R l ), with a significant increase in middle and deep layers. These results signify the crucial role of deep layers for final prediction, validating that the model extracts information from label words in deep layers to form the final prediction. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Discussion of Our Hypothesis", "text": "In \u00a7 2.2, we have affirmed that the model's shallow layers assemble information from demonstrations via label words to form semantic representations. In \u00a7 2.3, we verify that the aforementioned aggregated information on label words is then extracted to form the final prediction in the deep layers. Recognizing the crucial function of label words in this process, we have introduced the term \"Anchors\" to denote them. Given the considerable role these \"anchors\" fulfill, we find it intuitive to design ICL improvements based on them, as elaborated in \u00a7 3.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Applications of Our Anchor-Based Understanding", "text": "With insights from the validated hypothesis, we propose strategies to boost ICL's accuracy and inference speed. We propose an anchor re-weighting method in \u00a7 3.1 to adjust the demonstrations' contributions and improve accuracy. In \u00a7 3.2, we explore a context compression technique that reduces original demonstrations to anchor hidden states to speed up ICL inference. Besides, in \u00a7 3.3, we utilize anchor distances to perform an analysis to understand the errors ICL made in real-world scenarios. These approaches corroborate our hypothesis, pointing to potential paths for future ICL enhancements.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Anchor Re-weighting", "text": "Based on our analysis in \u00a7 2, we draw parallels between ICL and logistic regression and propose an approach to improve ICL's accuracy by reweighting label anchors.\n3.1.1 Method \u00a7 2.3 illustrates a strong correlation between the model's output category and the attention distribution (A (q, p 1 ) , . . . , A (q, p C )) on label words p 1 , ..., p C of the target position q in deep layers.\nWe can view the attention module as a classifier f ,\nPr f (Y = i|X = x) \u2248A(q, pi) = exp(qqk T p i / \u221a d) N j=1 exp(qqk T j / \u221a d) .(6)\nBy setting q q / \u221a d =x and k p i \u2212 k p C = \u03b2 i , we deduce:\nlog Pr f (Y = i|X = x) Pr f (Y = C|X = x) = \u03b2 T ix .(7)\nThis approximates a logistic regression model where:\nlog Pr f (Y = i|X = x) Pr f (Y = C|X = x) = \u03b2 i 0 + \u03b2 T i x.(8)\nIn this equation, \u03b2 i 0 and \u03b2 T i are parameters that can be learned, while x is the input feature.\nInspired by the similarity between ICL and logistic regression, we've incorporated a learnable \u03b2 i 0 into Eq. ( 7), which is equivalent to adjusting the attention weights A(q, p i ):\nA(q, p i ) = exp(\u03b2 i 0 )A(q, p i ) (9)\nEach \u03b2 i 0 is a learnable parameter, set uniquely for different attention heads and layers. Refer to Appendix G for more details.\nTo train the re-weighting vector \u03b2 = \u03b2 i 0 , we utilize an auxiliary training set (X train , Y train ).\nHere, we perform ICL with normal demonstrations and optimize \u03b2 with respect to the classification loss L on (X train , Y train ):\n\u03b2 \u22c6 = arg min \u03b2 L(X train , Y train ). (10\n)\nThis approach can be metaphorically described as \"re-weighting the anchors,\" leading us to term it as Anchor Re-weighting. It can also be viewed as a modification of the demonstration contributions since demonstration information has been incorporated into the anchors as suggested by our prior analysis in \u00a7 2.2. Additionally, it can be interpreted as a unique adapter variant, introducing minimal parameters while preserving most of the original model. However, it is specifically designed based on our anchor hypothesis and requires fewer parameters than traditional adapters.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We choose one sample per class as normal demonstrations and choose four extra samples per class to form the auxiliary training set (X train , Y train ). The setup follows \u00a7 2.2, with results averaged over five random seeds. Owing to computational constraints, we employ GPT2-XL for evaluation, excluding GPT-J. The parameters \u03b2 i 0 are trained using gradient descent. More details can be found in Appendix H.\nWe compare Anchoring Re-weighting with two baselines: (1) Vanilla ICL with the same demonstration (1-shot per class) (2) Vanilla ICL, where the auxiliary training set of \u03b2 is included as demonstrations (5-shot per class) for a fair comparison.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "As Table 1 shows, the proposed anchor reweighting significantly enhances ICL performance, particularly on the SST-2 and EmoC datasets. Besides, adding more demonstrations for vanilla ICL may not bring a stable accuracy boost due to the potential noise introduced, as discussed in Zhao et al. (2021). Different from vanilla ICL which utilizes the extra examples to form a demonstration, we train a re-weighting vector \u03b2 to modulate label anchor contributions. This shortens the input context and thus brings (almost) no extra cost to the inference speed. The consistent improvements of our method suggest that the re-weighting mechanism could be a better alternative to utilize demonstration examples. Furthermore, it reiterates the crucial role that anchors play in ICL.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Anchor-Only Context Compression", "text": "We further explore a context compression technique that reduces the full demonstration to anchor hidden states for accelerating ICL inference.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Method", "text": "In \u00a7 2.3, we find that the model output heavily relies on the label words, which collect information  The effect after adding parameter \u03b2 i 0 . For AGNews, due to the length limit, we only use three demonstrations per class. Our Anchor Re-weighting method achieves the best performance overall tasks. from the demonstrations. Given the auto-regressive nature of GPT-like models, where hidden states of tokens depend solely on preceding ones, label words' information aggregation process is independent of subsequent words. This allows for the calculation and caching of the label word hidden states H = {{h i l } C i=1 } N l=1 (h i l is the l-th layer's hidden state of the i-th label word in the demonstration). By concatenating h 1 l , ..., h C l at the front in each layer during inference, instead of using the full demonstration, we can speed up inference.\nIn our preliminary experiments, concatenating hidden states of label words alone was inadequate for completing the ICL task. 5 This might be due to the critical role of formatting information in helping the model to determine the output space at the target position, 6 as highlighted in Min et al. (2022b). As a solution, we amalgamate the hidden states of both the formatting and the label words, a method we've termed Hidden anchor .", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We follow the same experimental settings as \u00a7 2.2. We compare our Hidden anchor input compression method with two equally efficient baselines. Text anchor : This method concatenates the formatting and label text with the input, as opposed to concatenating the hidden states at each layer. Hidden random : This approach concatenates the hidden states of formatting and randomly selected nonlabel words (equal in number to Hidden anchor ). Hidden random-top : To establish a stronger baseline, we randomly select 20 sets of non-label words in Hidden random and report the one with the highest label loyalty.\nThe Text anchor method is included to demonstrate that the effectiveness of Hidden anchor is attributed to the aggregation of information in label  words, rather than the mere text of label words.\nIf we find that Hidden anchor surpasses Text anchor in performance, it solidifies the notion that the aggregated information within label words carries significant importance. The Hidden random method is introduced to illustrate that anchor hidden states encapsulate most of the demonstration information among all hidden states. We assess all compression methods using the label loyalty and word loyalty introduced in \u00a7 2.2, in addition to classification accuracy.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "We can see from Table 2 that the proposed compression method Hidden anchor achieves the best results among all three compression methods on all metrics and for both models. For example, with the GPT-J model, the compression method with anchor states only leads to a 1.5 accuracy drop compared to the uncompressed situation, indicating that the compression introduces negligible information loss. Further, we estimate the efficiency improvements over the original ICL. As shown in Table 3, the speed-up ratio ranges from 1.1\u00d7 to 2.9\u00d7, as the efficiency gain is influenced by the length of the demonstrations. We refer readers to Appendix I for Model SST-2 TREC AGNews EmoC  a more elaborated analysis of the speed-up ratios.\nBesides, we observe that the acceleration effect is more pronounced in the GPT-J model compared to GPT2-XL, demonstrating its great potential to apply to larger language models.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3", "tab_5"]}, {"heading": "Anchor Distances for Error Diagnosis", "text": "Lastly, we perform an error analysis for ICL by examining the distances between the key vectors in the attention module that correspond to the label words.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Method", "text": "Our previous analysis in \u00a7 2.3 shows a strong correlation between the model output and A(q, p i ), which is determined by q q k T p i as per Eq. 7. Should the key vectors k for label words p i and p k be similar, A(q, p i ) and A(q, p k ) will also likely be similar, leading to potential label confusion. Furthermore, considering the distribution of query vectors q q , we employ a PCA-like method to extract the components of the key vectors along the directions with significant variations in q q , denoted a\u015d k (see Appendix J for details). We anticipate that the distances between theseks can correspond to the category confusion of the model, thus revealing one possible origin of ICL errors. Here, we normalize the distances to a scale of 0-1, with 0 indicating the highest degree of category confusion:\nConfusion pred ij = \u2225k p i \u2212k p j \u2225 max s\u0338 =t \u2225k ps \u2212k p t \u2225 ,(11)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We utilize the GPT2-XL model and TREC dataset, as the model displays varying confusion levels between categories on this dataset. We use all 500 samples of the TREC test set and use 1 demonstration per class for convenience of analysis.\nWe calculate the actual model confusion score, Confusion ij , between category i and category k using the AUC-ROC metric (detailed in Appendix K). We then compare the predicted confusion score, Confusion pred ij , and the actual confusion score, Confusion ij , via heatmaps.  We set undefined diagonals to 1 for better visualization.\nThe heatmaps display similarity in confusing category pairs, particularly in lighter-colored blocks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Figure 6 shows that the proposed approximation metric, Confusion pred ij , can identify the most confusing case (Description-Entity) and performs reasonably well for highly confusing categories (Entity-Abbreviation, Description-Abbreviation). This high correlation indicates that ICL makes errors in categories with similar label anchors. Overall, this result demonstrates that our anchor-based analysis framework could serve as an interpretation tool for better understanding ICL's errors.", "publication_ref": [], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "Related Work", "text": "The existing literature on in-context learning analysis can be broadly divided into two streams, each focusing on different aspects. The first stream explores the influencing factors of ICL based on input perturbation, such as the order (Min et al., 2022b), the formatting (Yoo et al., 2022;, and the selection of the demonstration (Liu et al., 2022). Designing proper demonstration construc-tion strategies (Ye et al., 2023;Li et al., 2023a) and calibration techniques (Zhao et al., 2021;Min et al., 2022a) could bring clear boosts to the ICL performance. The second stream investigates the inner working mechanism of ICL through different conceptual lenses, such as making an analogy of ICL to gradient descent (von Oswald et al., 2022;Dai et al., 2022) and viewing the process of ICL as a Bayesian inference (Xie et al., 2022).\nIn this paper, we provide a novel perspective by examining the information flow in language models to gain an understanding of ICL. Our approach offers new insights and demonstrates the potential for leveraging this understanding to improve the effectiveness, efficiency, and interpretability of ICL.", "publication_ref": ["b14", "b25", "b11", "b24", "b8", "b13", "b19", "b3", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper, we propose a hypothesis that label words serve as anchors in in-context learning for aggregating and distributing the task-relevant information flow. Experimental results with attention manipulation and analysis of predictions correlation consolidate the hypothesis holds well in GPT2-XL and GPT-J models. Inspired by the new understanding perspective, we propose three practical applications. First, an anchor re-weighting method is proposed to improve ICL accuracy. Second, we explore a demonstration compression technique to accelerate ICL inference. Lastly, we showcase an analysis framework to diagnose ICL errors on a real-world dataset. These promising applications again verify the hypothesis and open up new directions for future investigations on ICL.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "Our study, while providing valuable insights into in-context learning (ICL), has several limitations. Firstly, our research scope was limited to classification tasks and did not delve into the realm of generative tasks. Additionally, our hypothesis was only examined within conventional ICL paradigms, leaving other ICL paradigms such as the chain of thought prompting (CoT)  unexplored. Secondly, due to hardware constraints, we mainly investigated models up to a scale of 6 billion parameters. Further research that replicates our study using larger-scale models would be beneficial in corroborating our findings and refining the hypotheses set forth in our investigation. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix A Experimental Settings", "text": "For models, we use GPT2-XL (1.5B) (Radford et al., 2019) and GPT-J (6B) (Wang and Komatsuzaki, 2021) in this paper.\nFor datasets, we use a sentiment analysis task, Stanford Sentiment Treebank Binary (SST-2) (Socher et al., 2013), a question type classification task, Text REtrieval Conference Question Classification (TREC) (Li and Roth, 2002;Hovy et al., 2001), a topic classification task, AG's news topic classification dataset (AGNews) (Zhang et al., 2015), and an emotion classification task, Emo-Context (EmoC) (Chatterjee et al., 2019). The ICL templates of these tasks are shown in Table 4.  is prominent, while S pq (the information flow from label words to targeted positions) is less significant. However, in deeper layers, S pq dominates. Importantly, S wp and S pq generally exceed S ww , indicating that interactions involving label words are predominant.", "publication_ref": ["b15", "b20", "b17", "b9", "b5", "b26", "b2"], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "C Reason for Using Word Loyalty Besides Label Loyalty", "text": "Label loyalty alone may not capture changes in the probability distribution of non-label words or the relative ratio of the probability of the label words within the entire vocabulary. Word loyalty helps address this limitation, which is shown in Table 5.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "D Isolating Different Numbers of Layers", "text": "We study the impact of the numbers of isolated layers, as shown in Figures 8a and 8b. It can be found that isolating shallow layers cause a significant impact, isolating deep layers has a negligible impact on the model, even when the number of isolation layers increases. This further illustrates Isolation Layer Output Label V 5 (sorted by probability)\nFirst 5 layers World \"\\n\", \" The\", \" Google\",\"<|endoftext|>\", \" A\" No isolation World \" World\", \" Technology\", \" Politics\", \" Israel\", \" Human\"  the important role of information aggregation via label words in the shallow layers.", "publication_ref": [], "figure_ref": ["fig_9"], "table_ref": []}, {"heading": "E Details for the Calculation of AUCROC l", "text": "Suppose the positions of the label words in the input x are p 1 , ..., p C (without loss of generality, we suppose p i corresponds to the ith class), the targeted position is q, the sum of the attention ma-trices of all attention heads at the l layer is A l . We postulate that there's a strong correlation between the attention distributions on the label words of the target position (A l (q, p 1 ), ..., A l (q, p C )) and the model's final prediction. We use the AUC-ROC score to quantify this correlation. We regard (A l (q, p 1 ), ..., A l (q, p C )) as a classifier's prediction for the model output label (that is, A l (q, p i ) is equivalent to the probability of model outputting label i), and compute the AUC-ROC value of this prediction relative to the actual model output. We denote this as AUCROC l . For the case with more demonstrations (Appendix F.1), we simply sum up all A l (q, p) of the same class.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F Additional Experimental Results", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F.1 Results with More Demonstrations", "text": "We implement our experimental analysis utilizing two demonstrations per class, resulting in a total of 4, 12, 8, and 8 demonstrations respectively for SST-2, TREC, AGNews, and EmoC. Our findings, as depicted in Figure 9, Figure 10, and Figure 11, exhibit a high degree of similarity to the results obtained from experiments that employ one demonstration per class.", "publication_ref": [], "figure_ref": ["fig_11", "fig_13"], "table_ref": []}, {"heading": "F.2 Results for In-Context Learning with semantically-unrelated labels", "text": "The applicability of our analytical conclusions to ICL variants, such as the semantically unrelated label ICL (Wei et al., 2023), is an intriguing subject.\nGiven that both GPT2-XL and GPT-J-6B perform at levels akin to random guessing in this ICL setting, we chose LLaMA-33B (Touvron et al., 2023) and SST-2 for our experiment. We substituted labels with 'A'/'B', and adhered to a similar experimental setup as in sections \u00a7 2.2 and \u00a7 2.3. However, we applied eight shots per class to facilitate the model in achieving an accuracy of 83.0% on SST-2. The outcomes align with those derived in \u00a7 2.2 and \u00a7 2.3. Figure 12 shows the more pronounced impact of isolating labels in the shallow layers compared to their isolation in the deep layers or the isolation of non-label tokens. Figure 13   firmed that the model leverages information from anchors in the deeper layers to perform classification.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "G Implementation of Anchor", "text": "Re-weighting\nIn order to implement anchor re-weighting, specific adjustments are made in the model's computational process. After calculating the attention matrix A h l of the hth head in the lth layer, we multiply each A h l (q, p i ) by exp(\u03b2 i 0,lh ) before proceeding with further computations. This means that for each attention head, we introduce the following modifications:  Figure 12: The impact of isolating label words versus randomly isolating non-label words within the first or last 5 layers. Isolating label words within the first 5 layers exerts a more pronounced effect, highlighting the importance of shallow-layer information aggregation via label words.\nAttention h l (Q, K, V ) =\u00c2 h l V, A h l = softmax QK T \u221a d , A h l (k, j) = exp(\u03b2 i 0,lh )A h l (k, j), if k = q, j = pi A h l (k, j), otherwise .(12", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "H Training Settings of Anchor Re-weighting", "text": "For each random seed, we fix the demonstration and sample 1000 test samples from the test datasets as described in \u00a7 2.2. The optimization of parame- ter vector \u03b2 is carried out using gradient descent, specifically with the Adam optimizer (Kingma and Ba, 2015). The learning rate is set at 0.01, with \u03b2 1 = 0.9 and \u03b2 2 = 0.999. Due to memory constraints, we use a batch size of 1. This optimization process is repeated for 10 epochs. Owing to limitations in computational resources, we restrict our evaluation to the GPT2-XL model and exclude the GPT-J model from our assessment.  From Table 6, we observe a correlation between the acceleration ratios and the ratio of the total demonstration length (L demo ) to the length of the text predicted (L x ). It suggests that a greater ratio of total length to predicted text length may yield a higher acceleration ratio.\nIn addition, the table illustrates that datasets with longer demonstration lengths tend to exhibit higher acceleration ratios. For instance, the AGNews dataset, which has the longest L demo , presents the highest acceleration ratio among the datasets analyzed. These findings could indicate an increased efficiency of the Hidden anchor method in contexts involving longer demonstration lengths.", "publication_ref": ["b6"], "figure_ref": [], "table_ref": ["tab_9"]}, {"heading": "J Calculation ofk", "text": "For the sampled sequence x 1 , ..., x T to be predicted, we denote the query vectors of the target positions as q 1 , ..., q T . We then compute the matrix Q = (q 1 \u2212 q, ..., q T \u2212 q) by subtracting the mean vector, q, from each query vector. Subsequently, we determine the M directions, v 1 , ..., v M , that correspond to the M largest variation directions for the centralized query vectorsq 1 , ...,q T . The i th direction, v i , is chosen to maximize the variance of the projection of the centralized query vectors onto it, while also being orthogonal to the previously chosen directions, v 1 , ..., v i\u22121 . This process can be formalized as follows:\nv 1 = arg max \u2225v\u2225=1 Var v \u22a4Q , v 2 = arg max \u2225v\u2225=1,v\u22a5v 1 Var v \u22a4Q , ... v M = arg max \u2225v\u2225=1,v\u22a5v 1 ,...,v\u22a5v M \u22121 Var v \u22a4Q .(13)\nWe define \u03c3 i as the square root of the variance of the projection ofQ onto the i th direction, i.e.,", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Var v \u22a4", "text": "iQ . To derive featuresks, we project the key vector k onto the directions v 1 , ..., v M and scale the projections by the corresponding standard deviations \u03c3 1 , ..., \u03c3 M . Each feature,k i , is thus calculated as \u03c3 i v T i k.\nWe further examine the influence of M on the prediction confusion matrix, Confusionij pred , as depicted in Figure 14. Given the similarity in outcomes for various M , we settle on a value of M = 10 for computation of Confusionij pred .", "publication_ref": [], "figure_ref": ["fig_15"], "table_ref": []}, {"heading": "K Calculation of Confusion ij", "text": "To gauge the true degree of confusion between categories i and k for a given model, we suggest utilizing the Confusion ij metric:\nFirst, we procure all test samples x t bearing true labels i or k. We then obtain the probabilities p t i and p t j yielded by the model for categories i and k, respectively, on these samples. These probabilities are normalized to a total of 1. Essentially, we derive a classifier f that delivers the probabilities p t i and p t j for the categories i and k respectively, on the test samples x t . By calculating the Area Under the Receiver Operating Characteristic Curve (AUC-ROC) value of this classifier f , we get the degree of confusion between category i and k, termed as Confusion ij .\nThe computed Confusionij is a value that never exceeds 1. The closer Confusionij approximates 1, the less pronounced the confusion, and vice versa.\nWe use the above metric instead of directly analyzing the output labels of the model because previous work has indicated the issue of insufficient output probability calibration in ICL (Zhao et al., 2021), which is greatly affected by factors such as sample ordering and model preferences for specific label words. By leveraging our defined degree of confusion, Confusion ij , we can implicitly alleviate the disturbances arising from insufficient probability calibration on the output labels. This allows for a more accurate representation of the model's degree of confusion for different categories, mitigating the impact of randomness.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "L Reproducibility", "text": "In the supplementary material, we have provided codes that allow for the faithful replication of our experiments and subsequent result analysis. To ensure consistency and reproducibility across different devices, we have fixed the five random seeds to the values of 42, 43, 44, 45, and 46. We invite readers to delve into the code for additional implementation details that may arouse their interest. \n9854", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgement", "text": "We thank all reviewers for their thoughtful and insightful suggestions. This work is supported in part by a Tencent Research Grant and National Natural Science Foundation of China (No. 62176002). Xu Sun is the corresponding author.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "What learning algorithm is in-context learning?", "journal": "", "year": "2022", "authors": "Ekin Aky\u00fcrek; Dale Schuurmans; Jacob Andreas; Tengyu Ma; Denny Zhou"}, {"ref_id": "b1", "title": "Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners", "journal": "", "year": "2020-12-06", "authors": "Tom B Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell; Sandhini Agarwal; Ariel Herbert-Voss; Gretchen Krueger; Tom Henighan; Rewon Child; Aditya Ramesh; Daniel M Ziegler; Jeffrey Wu; Clemens Winter; Christopher Hesse; Mark Chen; Eric Sigler; Mateusz Litwin"}, {"ref_id": "b2", "title": "SemEval-2019 task 3: EmoContext contextual emotion detection in text", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Ankush Chatterjee; Kedhar Nath Narahari; Meghana Joshi; Puneet Agrawal"}, {"ref_id": "b3", "title": "Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers", "journal": "", "year": "2022", "authors": "Damai Dai; Yutao Sun; Li Dong; Yaru Hao; Zhifang Sui; Furu Wei"}, {"ref_id": "b4", "title": "A survey for in-context learning", "journal": "", "year": "2023", "authors": "Qingxiu Dong; Lei Li; Damai Dai; Ce Zheng; Zhiyong Wu; Baobao Chang; Xu Sun; Jingjing Xu; Zhifang Sui"}, {"ref_id": "b5", "title": "Ulf Hermjakob, Chin-Yew Lin, and Deepak Ravichandran", "journal": "", "year": "2001", "authors": "Eduard Hovy; Laurie Gerber"}, {"ref_id": "b6", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2015-05-07", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b7", "title": "Attention is not only a weight: Analyzing transformers with vector norms", "journal": "", "year": "2020", "authors": "Goro Kobayashi; Tatsuki Kuribayashi; Sho Yokoi; Kentaro Inui"}, {"ref_id": "b8", "title": "Unified demonstration retriever for incontext learning", "journal": "", "year": "2023", "authors": "Xiaonan Li; Kai Lv; Hang Yan; Tianyang Lin; Wei Zhu; Yuan Ni; Guotong Xie; Xiaoling Wang; Xipeng Qiu"}, {"ref_id": "b9", "title": "Learning question classifiers", "journal": "", "year": "2002", "authors": "Xin Li; Dan Roth"}, {"ref_id": "b10", "title": "Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. 2023b. Transformers as algorithms: Generalization and stability in in-context learning", "journal": "", "year": "", "authors": "Yingcong Li"}, {"ref_id": "b11", "title": "What makes good in-context examples for GPT-3?", "journal": "", "year": "2022", "authors": "Jiachang Liu; Dinghan Shen; Yizhe Zhang; Bill Dolan; Lawrence Carin; Weizhu Chen"}, {"ref_id": "b12", "title": "Are sixteen heads really better than one?", "journal": "", "year": "2019-12-08", "authors": "Paul Michel; Omer Levy; Graham Neubig"}, {"ref_id": "b13", "title": "Noisy channel language model prompting for few-shot text classification", "journal": "Long Papers", "year": "2022", "authors": "Sewon Min; Mike Lewis; Hannaneh Hajishirzi; Luke Zettlemoyer"}, {"ref_id": "b14", "title": "Rethinking the role of demonstrations: What makes in-context learning work?", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Sewon Min; Xinxi Lyu; Ari Holtzman; Mikel Artetxe; Mike Lewis; Hannaneh Hajishirzi; Luke Zettlemoyer"}, {"ref_id": "b15", "title": "Language models are unsupervised multitask learners", "journal": "OpenAI blog", "year": "2019", "authors": "Alec Radford; Jeffrey Wu; Rewon Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"ref_id": "b16", "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "journal": "CoRR", "year": "2013", "authors": "Karen Simonyan; Andrea Vedaldi; Andrew Zisserman"}, {"ref_id": "b17", "title": "Recursive deep models for semantic compositionality over a sentiment treebank", "journal": "Association for Computational Linguistics", "year": "2013", "authors": "Richard Socher; Alex Perelygin; Jean Wu; Jason Chuang; Christopher D Manning; Andrew Ng; Christopher Potts"}, {"ref_id": "b18", "title": "Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models", "journal": "ArXiv", "year": "", "authors": "Hugo Touvron; Thibaut Lavril; Gautier Izacard; Xavier Martinet; Marie-Anne Lachaux; Timoth\u00e9e Lacroix; Naman Baptiste Rozi\u00e8re; Eric Goyal; Faisal Hambro; Aurelien Azhar; Armand Rodriguez;  Joulin"}, {"ref_id": "b19", "title": "Transformers learn in-context by gradient descent", "journal": "ArXiv preprint", "year": "2022", "authors": "Eyvind Johannes Von Oswald; E Niklasson; Jo\u00e3o Randazzo; Alexander Sacramento; Andrey Mordvintsev; Max Zhmoginov;  Vladymyrov"}, {"ref_id": "b20", "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model", "journal": "", "year": "2021", "authors": "Ben Wang; Aran Komatsuzaki"}, {"ref_id": "b21", "title": "Chain of thought prompting elicits reasoning in large language models", "journal": "", "year": "2022", "authors": "Jason Wei; Xuezhi Wang; Dale Schuurmans; Maarten Bosma; Ed Huai Hsin Chi; F Xia; Quoc Le; Denny Zhou"}, {"ref_id": "b22", "title": "", "journal": "", "year": "", "authors": "Jerry W Wei; Jason Wei; Yi Tay; Dustin Tran; Albert Webson; Yifeng Lu; Xinyun Chen; Hanxiao Liu; Da Huang; Denny Zhou"}, {"ref_id": "b23", "title": "An explanation of in-context learning as implicit bayesian inference", "journal": "", "year": "2022-04-25", "authors": "Sang Michael Xie; Aditi Raghunathan; Percy Liang; Tengyu Ma"}, {"ref_id": "b24", "title": "Compositional exemplars for in-context learning", "journal": "", "year": "2023", "authors": "Jiacheng Ye; Zhiyong Wu; Jiangtao Feng; Tao Yu; Lingpeng Kong"}, {"ref_id": "b25", "title": "Ground-truth labels matter: A deeper look into input-label demonstrations", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Junyeob Kang Min Yoo; Hyuhng Joon Kim; Hyunsoo Kim; Hwiyeol Cho; Sang-Woo Jo; Sang-Goo Lee; Taeuk Lee;  Kim"}, {"ref_id": "b26", "title": "Character-level convolutional networks for text classification", "journal": "", "year": "2015-12-07", "authors": "Xiang Zhang; Junbo Jake Zhao; Yann Lecun"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Review: I dislike \u2026 Sentiment: Negative Review: A good \u2026 Sentiment: Positive Review: \u2026 Sentiment: dislike \u2026 Sentiment: Negative Review: A good \u2026 Sentiment: Positive Review: \u2026 Sentiment:", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Relative sizes of S wp , S pq , and S ww in different layers on SST-2 and AGNews. Results of other datasets can be found in Apendix B. Initially, S wp occupies a significant proportion, but it gradually decays over layers, while S pq becomes the dominant one.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure5: AUCROC l and R l of each layer in GPT models. The result is averaged over SST-2, TREC, AGNews, and Emoc. AUCROC l reaches 0.8 in deep layers, and R l increases mainly in the middle and later layers.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 6 :6Figure6: Predicted and real confusion matrix on TREC. We set undefined diagonals to 1 for better visualization. The heatmaps display similarity in confusing category pairs, particularly in lighter-colored blocks.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "BFigure7illustrates the relative sizes of S wp , S pq , and S ww on TREC and EmoC, mirroring results on SST-2 and AGNews. In shallow layers, S wp (the information flow from the text part to label words)", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 7 :7Figure7: Relative size of S wp , S pq , and S ww on TREC and EmoC, which is similar to that on SST-2 and AG-News.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 8 :8Figure8: The chart demonstrates variations in label loyalty and word loyalty, dependent on whether label or non-label words are isolated in various layers. 'First' refers to the first several layers, while 'Last' to the last ones. Deep-colored lines represent label word isolation, whereas light colors denote non-label words. Remarkably, isolating label words in the shallow layers significantly influences the outcome, regardless of whether this is compared to isolation in deep layers or to nonlabel word isolation.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Results on the EmoC dataset", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 9 :9Figure 9: Relative sizes of S wp , S pq , and S ww when more demonstrations are employed.", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Figure 11 :11Figure 11: AUCROC l and R l of each layer in GPT models when more demonstrations are employed.", "figure_data": ""}, {"figure_label": "13", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "Figure 13 :13Figure13: AUCROC l and R l of each layer of LLaMA-33B on SST-2. Still, deep layers display higher relevance to model prediction, reinforcing the idea that the model extracts information from deep-layer anchors for classification.", "figure_data": ""}, {"figure_label": "14", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "Figure 14 :14Figure 14: Predicted confusion matrices under M =5, 10, 20, 50, 100, 200.    ", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Method SST-2 TREC AGNews EmoC Average Vanilla In-Context Learning ( 1-shot per class ) 61.28 57.56 73.32 15.44 51.90 Vanilla In-Context Learning ( 5-shot per class ) 64.75 60.40", "figure_data": "52.529.8046.87Anchor Re-weighting (1-shot per class)90.07 60.9281.9441.6468.64"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Acceleration ratios of the Hidden anchor method.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Demonstration templates and label words. Here <S1> represents the demonstration, <S> represents the input to be predicted, and <L> represents the label word corresponding to the demonstration. To save space, we only show one demonstration for each task.", "figure_data": "TaskTemplateLabel WordsSST-2Review: <S1> Sentiment: <L> Review: <S> Sentiment:Positive, NegativeTRECQuestion: <S1> Answer Type: <L> Question: <S> Answer Type:Abbreviation, Entity Description, Person Location, NumberAGNews Article: <S1> Answer: <L> Article: <S> Answer:World, Sports Business, TechnologyEmoCDialogue: <S1> Emotion: <L> Dialogue: <S> Emotion:Others, Happy Sad, AngryZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improv-ing few-shot performance of language models. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Vir-tual Event, volume 139 of Proceedings of Machine Learning Research, pages 12697-12706. PMLR."}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Results on a test sample with the label \"World\" from AGNews.", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Acceleration ratios, L demo and L x .", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "I l = h A \u22a4 h,l \u2202L(x) \u2202A h,l .(1)", "formula_coordinates": [2.0, 371.66, 752.41, 153.36, 24.61]}, {"formula_id": "formula_1", "formula_text": "Swp = (i,j)\u2208Cwp I l (i, j) |Cwp| , Cwp = {(p k , j) : k \u2208 [1, C], j < p k }.(2)", "formula_coordinates": [3.0, 109.93, 433.68, 179.81, 42.87]}, {"formula_id": "formula_2", "formula_text": "Spq = (i,j)\u2208Cpq I l (i, j) |Cpq| , Cpq = {(q, p k ) : k \u2208 [1, C]}.(3)", "formula_coordinates": [3.0, 125.72, 516.05, 164.02, 42.87]}, {"formula_id": "formula_3", "formula_text": "Sww = (i,j)\u2208Cww I l (i, j) |Cww| , Cww ={(i, j) : j < i} \u2212 Cwp \u2212 Cpq.(4)", "formula_coordinates": [3.0, 110.41, 612.94, 179.33, 42.4]}, {"formula_id": "formula_4", "formula_text": "R l = l i=1 (AUCROCi \u2212 0.5) N i=1 (AUCROCi \u2212 0.5) .", "formula_coordinates": [5.0, 120.43, 471.12, 119.12, 31.07]}, {"formula_id": "formula_5", "formula_text": ")5", "formula_coordinates": [5.0, 282.77, 477.89, 6.97, 10.81]}, {"formula_id": "formula_6", "formula_text": "Pr f (Y = i|X = x) \u2248A(q, pi) = exp(qqk T p i / \u221a d) N j=1 exp(qqk T j / \u221a d) .(6)", "formula_coordinates": [6.0, 131.99, 291.79, 157.74, 54.74]}, {"formula_id": "formula_7", "formula_text": "log Pr f (Y = i|X = x) Pr f (Y = C|X = x) = \u03b2 T ix .(7)", "formula_coordinates": [6.0, 117.95, 390.1, 171.79, 22.73]}, {"formula_id": "formula_8", "formula_text": "log Pr f (Y = i|X = x) Pr f (Y = C|X = x) = \u03b2 i 0 + \u03b2 T i x.(8)", "formula_coordinates": [6.0, 107.65, 453.65, 182.09, 22.73]}, {"formula_id": "formula_9", "formula_text": "A(q, p i ) = exp(\u03b2 i 0 )A(q, p i ) (9)", "formula_coordinates": [6.0, 119.05, 577.39, 170.82, 15.17]}, {"formula_id": "formula_10", "formula_text": "\u03b2 \u22c6 = arg min \u03b2 L(X train , Y train ). (10", "formula_coordinates": [6.0, 105.25, 722.71, 180.07, 20.96]}, {"formula_id": "formula_11", "formula_text": ")", "formula_coordinates": [6.0, 285.32, 723.11, 4.54, 13.15]}, {"formula_id": "formula_12", "formula_text": "Confusion pred ij = \u2225k p i \u2212k p j \u2225 max s\u0338 =t \u2225k ps \u2212k p t \u2225 ,(11)", "formula_coordinates": [8.0, 84.88, 566.86, 204.98, 36.64]}, {"formula_id": "formula_13", "formula_text": "Attention h l (Q, K, V ) =\u00c2 h l V, A h l = softmax QK T \u221a d , A h l (k, j) = exp(\u03b2 i 0,lh )A h l (k, j), if k = q, j = pi A h l (k, j), otherwise .(12", "formula_coordinates": [13.0, 315.88, 695.1, 205.41, 78.46]}, {"formula_id": "formula_14", "formula_text": "v 1 = arg max \u2225v\u2225=1 Var v \u22a4Q , v 2 = arg max \u2225v\u2225=1,v\u22a5v 1 Var v \u22a4Q , ... v M = arg max \u2225v\u2225=1,v\u22a5v 1 ,...,v\u22a5v M \u22121 Var v \u22a4Q .(13)", "formula_coordinates": [15.0, 77.26, 283.99, 212.6, 104.84]}], "doi": "10.18653/v1/S19-2005"}