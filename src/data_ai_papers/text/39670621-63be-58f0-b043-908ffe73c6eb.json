{"title": "Pegasos: Primal Estimated sub-GrAdient SOlver for SVM", "authors": "Shai Shalev-Shwartz; Yoram Singer; Nathan Srebro; Andrew Cotter", "pub_date": "", "abstract": "We describe and analyze a simple and effective stochastic sub-gradient descent algorithm for solving the optimization problem cast by Support Vector Machines (SVM). We prove that the number of iterations required to obtain a solution of accuracy is\u00d5(1/ ), where each iteration operates on a single training example. In contrast, previous analyses of stochastic gradient descent methods for SVMs require \u2126(1/ 2 ) iterations. As in previously devised SVM solvers, the number of iterations also scales linearly with 1/\u03bb, where \u03bb is the regularization parameter of SVM. For a linear kernel, the total run-time of our method is\u00d5(d/(\u03bb )), where d is a bound on the number of non-zero features in each example. Since the run-time does not depend directly on the size of the training set, the resulting algorithm is especially suited for learning from large datasets. Our approach also extends to non-linear kernels while working solely on the primal objective function, though in this case the runtime does depend linearly on the training set size. Our algorithm is particularly well suited for large text classification problems, where we demonstrate an order-of-magnitude speedup over previous SVM learning methods.", "sections": [{"heading": "Introduction", "text": "Support Vector Machines (SVMs) are effective and popular classification learning tool [36,12]. The task of learning a support vector machine is typically cast as a constrained quadratic programming problem. However, in its native form, it is in fact an unconstrained empirical loss minimization with a penalty term for the norm of the classifier that is being learned. Formally, given a training set S = {(x i , y i )} m i=1 , where x i \u2208 \u00ca n and y i \u2208 {+1, \u22121}, we would like to find the minimizer of the problem \nwhere (w; (x, y)) = max{0, 1 \u2212 y w, x } ,\nand u, v denotes the standard inner product between the vectors u and v. We denote the objective function of Eq. (1) by f (w). We say that an optimization method finds anaccurate solution\u0175 if f (\u0175) \u2264 min w f (w) + . The standard SVM problem also includes an unregularized bias term. We omit the bias throughout the coming sections and revisit the incorporation of a bias term in Sec. 6. We describe and analyze in this paper a simple stochastic sub-gradient descent algorithm, which we call Pegasos, for solving Eq. (1). At each iteration, a single training example is chosen at random and used to estimate a sub-gradient of the objective, and a step with pre-determined step-size is taken in the opposite direction. We show that with high probability over the choice of the random examples, our algorithm finds an -accurate solution using only\u00d5(1/(\u03bb )) iterations, while each iteration involves a single inner product between w and x. Put differently, the overall runtime required to obtain an accurate solution is\u00d5(n/(\u03bb )), where n is the dimensionality of w and x. Moreover, this runtime can be reduced to\u00d5(d/(\u03bb )) where d is the number of non-zero features in each example x. Pegasos can also be used with non-linear kernels, as we describe in Sec. 4. We would like to emphasize that a solution is found in probability solely due to the randomization steps employed by the algorithm and not due to the data set. The data set is not assumed to be random, and the analysis holds for any data set S. Furthermore, the runtime does not depend on the number of training examples and thus our algorithm is especially suited for large datasets.\nBefore indulging into the detailed description and analysis of Pegasos, we would like to draw connections to and put our work in context of some of the more recent work on SVM. For a more comprehensive and up-to-date overview of relevant work see the references in the papers cited below as well as the web site dedicated to kernel methods at http://www.kernel-machines.org . Due to the centrality of the SVM optimization problem, quite a few methods were devised and analyzed. The different approaches can be roughly divided into the following categories.\nInterior Point (IP) methods: IP methods (see for instance [7] and the references therein) cast the SVM learning task as a quadratic optimization problem subject to linear constraints. The constraints are replaced with a barrier function. The result is a sequence of unconstrained problems which can be optimized very efficiently using Newton or Quasi-Newton methods. The advantage of IP methods is that the dependence on the accuracy is double logarithmic, namely, log(log(1/ )). Alas, IP methods typically require run time which is cubic in the number of examples m. Moreover, the memory requirements of IP methods are O(m 2 ) which renders a direct use of IP methods very difficult when the training set consists of many examples. It should be noted that there have been several attempts to reduce the complexity based on additional assumptions (see e.g. [15]). However, the dependence on m remains super linear. In addition, while the focus of the paper is the optimization problem cast by SVM, one needs to bear in mind that the optimization problem is a proxy method for obtaining good classification error on unseen examples. Achieving a very high accuracy in the optimization process is usually unnecessary and does not translate to a significant increase in the generalization accuracy. The time spent by IP methods for finding a single accurate solution may, for instance, be better utilized for trying different regularization values.\nDecomposition methods: To overcome the quadratic memory requirement of IP methods, decomposition methods such as SMO [29] and SVM-Light [20] tackle the dual representation of the SVM optimization problem, and employ an active set of constraints thus working on a subset of dual variables. In the extreme case, called row-action methods [8], the active set consists of a single constraint. While algorithms in this family are fairly simple to implement and entertain general asymptotic convergence properties [8], the time complexity of most of the algorithms in this family is typically super linear in the training set size m. Moreover, since decomposition methods find a feasible dual solution and their goal is to maximize the dual objective function, they often result in a rather slow convergence rate to the optimum of the primal objective function. (See also the discussion in [19].) Primal optimization: Most existing approaches, including the methods discussed above, focus on the dual of Eq. (1), especially when used in conjunction with non-linear kernels. However, even when non-linear kernels are used, the Representer theorem [23] allows us to re-parametrize w as w = \u03b1 i y i x i and cast the primal objective Eq. (1) as an unconstrained optimization problem with the variables \u03b1 1 , . . . , \u03b1 m (see Sec. 4). Tackling the primal objective directly was studied, for example, by Chapelle [10], who considered using smooth loss functions instead of the hinge loss, in which case the optimization problem becomes a smooth unconstrained optimization problem. Chapelle then suggested using various optimization approaches such as conjugate gradient descent and Newton's method. We take a similar approach here, however we cope with the non-differentiability of the hingeloss directly by using sub-gradients instead of gradients. Another important distinction is that Chapelle views the optimization problem as a function of the variables \u03b1 i . In contrast, though Pegasos maintains the same set of variables, the optimization process is performed with respect to w, see Sec. 4 for details.", "publication_ref": ["b35", "b11", "b5", "b3", "b6", "b14", "b28", "b19", "b7", "b7", "b18", "b22", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Stochastic gradient descent:", "text": "The Pegasos algorithm is an application of a stochastic subgradient method (see for example [25,34]). In the context of machine learning problems, the efficiency of the stochastic gradient approach has been studied in [26,1,3,27,6,5]. In particular, it has been claimed and experimentally observed that, \"Stochastic algorithms yield the best generalization performance despite being the worst optimization algorithms\". This claim has recently received formal treatment in [4,32].\nTwo concrete algorithms that are closely related to the Pegasos algorithm and are also variants of stochastic sub-gradient methods are the NORMA algorithm [24] and a stochastic gradient algorithm due to Zhang [37]. The main difference between Pegasos and these variants is in the procedure for setting the step size. We elaborate on this issue in Sec. 7. The convergence rate given in [24] implies that the number of iterations required to achieve -accurate solution is O(1/(\u03bb ) 2 ). This bound is inferior to the corresponding bound of Pegasos. The analysis in [37] for the case of regularized loss shows that the squared Euclidean distance to the optimal solution converges to zero but the rate of convergence depends on the step size parameter. As we show in Sec. 7, tuning this parameter is crucial to the success of the method. In contrast, Pegasos is virtually parameter free. Another related recent work is Nesterov's general primal-dual subgradient method for the minimization of non-smooth functions [28]. Intuitively, the ideas presented in [28] can be combined with the stochastic regime of Pegasos. We leave this direction and other potential extensions of Pegasos for future research.\nOnline methods: Online learning methods are very closely related to stochastic gradient methods, as they operate on only a single example at each iteration. Moreover, many online learning rules, including the Perceptron rule, can be seen as implementing a stochastic gradient step. Many such methods, including the Perceptron and the Passive Aggressive method [11] also have strong connections to the \"margin\" or norm of the predictor, though they do not directly minimize the SVM objective. Nevertheless, online learning algorithms were proposed as fast alternatives to SVMs (e.g. [16]). Such algorithms can be used to obtain a predictor with low generalization error using an online-to-batch conversion scheme [9]. However, the conversion schemes do not necessarily yield an -accurate solutions to the original SVM problem and their performance is typically inferior to direct batch optimizers. As noted above, Pegasos shares the simplicity and speed of online learning algorithms, yet it is guaranteed to converge to the optimal SVM solution.\nCutting Planes Approach: Recently, Joachims [21] proposed SVM-Perf, which uses a cutting planes method to find a solution with accuracy in time O(md/(\u03bb 2 )). This bound was later improved by Smola et al [33] to O(md/(\u03bb )). The complexity guarantee for Pegasos avoids the dependence on the data set size m. In addition, while SVM-Perf yields very significant improvements over decomposition methods for large data sets, our experiments (see Sec. 7) indicate that Pegasos is substantially faster than SVM-Perf.", "publication_ref": ["b24", "b33", "b25", "b0", "b2", "b26", "b5", "b4", "b3", "b31", "b23", "b36", "b23", "b36", "b27", "b27", "b10", "b15", "b8", "b20", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "The Pegasos Algorithm", "text": "As mentioned above, Pegasos performs stochastic gradient descent on the primal objective Eq. (1) with a carefully chosen stepsize. We describe in this section the core of the Pegasos procedure in detail and provide pseudo-code. We also present a few variants of the basic algorithm and discuss few implementation issues.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Basic Pegasos Algorithms", "text": "On each iteration Pegasos operates as follow. Initially, we set w 1 to the zero vector. On iteration t of the algorithm, we first choose a random training example (x i t , y i t ) by picking an index i t \u2208 {1, . . . , m} uniformly at random. We then replace the objective in Eq. ( 1) with an approximation based on the training example (x i t , y i t ), yielding:\nf (w; i t ) = \u03bb 2 w 2 + (w; (x i t , y i t )) .(3)\nWe consider the sub-gradient of the above approximate objective, given by:\n\u2207 t = \u03bb w t \u2212 \u00bd[y i t w t , x i t < 1] y i t x i t ,(4)\nwhere \u00bd[y w, x < 1] is the indicator function which takes a value of one if its argument is true (w yields non-zero loss on the example (x, y)), and zero otherwise. We then update \nw t+1 \u2190 min 1, 1/ \u221a \u03bb w t+1 w t+1 ] OUTPUT: w T +1\nFig. 1 The Pegasos Algorithm. w t+1 \u2190 w t \u2212 \u03b7 t \u2207 t using a step size of \u03b7 t = 1/(\u03bbt). Note that this update can be written as:\nw t+1 \u2190 (1 \u2212 1 t )w t + \u03b7 t \u00bd[y i t w t , x i t < 1] y i t x i t .(5)\nAfter a predetermined number T of iterations, we output the last iterate w T +1 . The pseudocode of Pegasos is given in Fig. 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Incorporating a Projection Step", "text": "The above description of Pegasos is a verbatim application of the stochastic gradient-descent method. A potential variation is the gradient-projection approach where we limit the set of admissible solutions to the ball of radius 1/ \u221a \u03bb. To enforce this property, we project w t after each iteration onto this sphere by performing the update:\nw t+1 \u2190 min 1, 1/ \u221a \u03bb w t+1 w t+1 .(6)\nOur initial presentation and implementation of Pegasos [31] included a projection step, while here we include it as an optional step. However, the newly revised analysis presented in this paper does not require such a projection and establishes almost the same guarantees for the basic (without projection) Pegasos algorithm. We did not notice major differences between the projected and unprojected variants in our experiments (see Sec. 7).", "publication_ref": ["b30"], "figure_ref": [], "table_ref": []}, {"heading": "Mini-Batch Iterations", "text": "In our analysis, we actually consider a more general algorithm that utilizes k examples at each iteration, where 1 \u2264 k \u2264 m is a parameter that needs to be provided to the algorithm. That is, at each iteration, we choose a subset A t \u2282 [m] = {1, . . . , m}, |A t | = k, of k examples uniformly at random among all such subsets. When k = m each iteration handles the original objective function. This case is often referred to as batch or deterministic iterations. To underscore the difference between the fully deterministic case and the stochastic case, we refer to the subsamples in the latter case as mini-batches and call the process mini-batch iterates. We thus consider the approximate objective function:\nf (w; A t ) = \u03bb 2 w 2 + 1 k i\u2208A t (w; (x i , y i )) .(7)\nINPUT: S, \u03bb, T , k\nINITIALIZE: Set w 1 = 0 FOR t = 1, 2, . . . , T Choose At \u2286 [m]\n, where |At| = k, uniformly at random Set\nA + t = {i \u2208 At : y i wt, x i < 1} Set \u03b7t = 1 \u03bbt Set w t+1 \u2190 (1 \u2212 \u03b7t \u03bb)wt + \u03b7t k i\u2208A + t y i x i [Optional: w t+1 \u2190 min 1, 1/ \u221a \u03bb w t+1 w t+1 ] OUTPUT: w T +1\nFig. 2 The Mini-Batch Pegasos Algorithm.\nNote that we overloaded our original definition of f and that the original objective can be denoted as f (w) = f (w; [m]). As before, we consider the sub-gradient of the approximate objective given by:\n\u2207 t = \u03bb w t \u2212 1 k i\u2208A t \u00bd[y i w t , x i < 1] y i x i .(8)\nWe update w t+1 \u2190 w t \u2212 \u03b7 t \u2207 t using the same predetermined step size \u03b7 t = 1/(\u03bbt).\nPseudo-code of this more general algorithm is given in Fig. 2. As before, we include an optional projection step.\nWhen k = m we choose A t = S on each round t and we obtain the deterministic subgradient descent method. In the other extreme case, when k = 1, we recover the stochastic sub-gradient algorithm of Figure 1.\nIn the above description we refer to A t as chosen uniformly at random among the subsets of [m] of size k, i.e. chosen without repetitions. Our analysis still holds when A t is a multiset chosen i.i.d. with repetitions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sparse Feature Vectors", "text": "We conclude this section with a short discussion of implementation details when the instances are sparse, namely, when each instance has very few non-zero elements. In this case, we can represent w as a pair (v, a) where v \u2208 \u00ca n is a vector and a is a scalar. The vector w is defined as w = a v. We do not require the vector v to be normalized and hence we over-represent w. However, using this representation, it is easily verified that the total number of operations required for performing one iteration of the basic Pegasos algorithm (with k = 1) is O(d), where d is the number of non-zero elements in x.\nWhen projection steps are included, we represent w as a triplet (v, a, \u03bd) with the following variables \u03bd = w = a v . Storing the norm of w allows us to perform the projection step using a constant number of operations involving only a and \u03bd. After w is updated, the stored norm \u03bd needs to be updated, which can again be done in time O(d) as before.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Analysis", "text": "In this section we analyze the convergence properties of Pegasos. Although our main interest is in the special case where k = 1 given in Figure 1, we actually analyze here the more general mini-batch variant of Figure 2. Throughout this section we denote w = argmin w f (w) .\nRecall that on each iteration of the algorithm, we focus on an instantaneous objective function f (w; A t ). We start by bounding the average instantaneous objective of the algorithm relatively to the average instantaneous objective of the optimal solution. We first need the following lemma which is based on a result from [17], though we provide the proof here for completeness. The lemma relies on the notion of strongly convex functions (see for example [30]). A function f is called \u03bb-strongly convex if f (w) \u2212 \u03bb 2 w 2 is a convex function. Lemma 1 Let f 1 , . . . , f T be a sequence of \u03bb-strongly convex functions. Let B be a closed convex set and define \u03a0 B (w) = arg min w \u2208B w \u2212 w . Let w 1 , . . . , w T +1 be a sequence of vectors such that w 1 \u2208 B and for t \u2265 1, w t+1 = \u03a0 B (w t \u2212 \u03b7 t \u2207 t ), where \u2207 t belongs to the sub-gradient set of f t at w t and \u03b7 t = 1/(\u03bbt). Assume that for all t, \u2207 t \u2264 G. Then, for all u \u2208 B we have\n1 T T t=1 f t (w t ) \u2264 1 T T t=1 f t (u) + G 2 (1 + ln(T )) 2 \u03bb T .\nProof Since f t is strongly convex and \u2207 t is in the sub-gradient set of f t at w t we have that (see [30])\nw t \u2212 u, \u2207 t \u2265 f t (w t ) \u2212 f t (u) + \u03bb 2 w t \u2212 u 2 . (10\n)\nNext, we show that\nw t \u2212 u, \u2207 t \u2264 w t \u2212 u 2 \u2212 w t+1 \u2212 u 2 2 \u03b7 t + \u03b7 t 2 G 2 . (11\n)\nLet w t denote w t \u2212 \u03b7 t \u2207 t . Since w t+1 is the projection of w t onto B, and u \u2208 B we have that w t \u2212 u 2 \u2265 w t+1 \u2212 u 2 . Therefore,\nw t \u2212 u 2 \u2212 w t+1 \u2212 u 2 \u2265 w t \u2212 u 2 \u2212 w t \u2212 u 2 = 2\u03b7 t w t \u2212 u, \u2207 t \u2212 \u03b7 2 t \u2207 t 2 .\nRearranging the above and using the assumption \u2207 t \u2264 G yields Eq. (11). Comparing Eq. (10) and Eq. (11) and summing over t we obtain\nT t=1 (f t (w t ) \u2212 f t (u)) \u2264 T t=1 w t \u2212 u 2 \u2212 w t+1 \u2212 u 2 2 \u03b7 t + \u03bb 2 w t \u2212 u 2 + G 2 2 T t=1 \u03b7 t .\nNext, we use the definition \u03b7 t = 1/(\u03bb t) and note that the first sum on the right-hand side of the above equation collapses to \u2212\u03bb (T + 1) w T +1 \u2212 u 2 . Thus,\nT t=1 (f t (w t ) \u2212 f t (u)) \u2264 \u2212\u03bb (T + 1) w T +1 \u2212 u 2 + G 2 2 \u03bb T t=1 1 t \u2264 G 2 2 \u03bb (1 + ln(T )) .\nBased on the above lemma, we are now ready to bound the average instantaneous objective of Pegasos.", "publication_ref": ["b16", "b29", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Theorem 1", "text": "Assume that for all (x, y) \u2208 S the norm of x is at most R. Let w be as defined in Eq. ( 9) and let c = ( \u221a \u03bb + R) 2 whenever we perform the projection step and c = 4 R 2 whenever we do not perform the projection step. Then, for\nT \u2265 3, 1 T T t=1 f (w t ; A t ) \u2264 1 T T t=1 f (w ; A t ) + c(1 + ln(T )) 2\u03bbT .\nProof To simplify our notation we use the shorthand f t (w) = f (w; A t ). The update of the algorithm can be rewritten as w t+1 = \u03a0 B (w t \u2212 \u03b7 t \u2207 t ), where \u2207 t is defined in Eq. ( 8) and B is the Euclidean ball of radius 1/ \u221a \u03bb if we perform a projection step and otherwise B = \u00ca n . Thus, to prove the theorem it suffices to show that the conditions stated in Lemma 1 hold. Since f t is a sum of a \u03bb-strongly convex function ( \u03bb 2 w 2 ) and a convex function (the average hinge-loss over A t ), it is clearly \u03bb-strongly convex. Next, we derive a bound on \u2207 t . If we perform a projection step then using the fact that w t \u2264 1/ \u221a \u03bb and that\nx \u2264 R combined with the triangle inequality we obtain \u2207 t \u2264 \u221a \u03bb + R. If we do not perform a projection step then we can first rewrite the update step as\nw t+1 = 1 \u2212 1 t w t \u2212 1 t \u03bb v t ,(12)\nwhere v t = 1 |A t | i\u2208A t \u00bd[y i w t , x t < 1] y i x i .\nTherefore, the initial weight of each v i is 1 \u03bb i and then on rounds j = i + 1, . . . , t it will be multiplied by 1 \u2212 1 j = j\u22121 j . Thus, the overall weight of\nv i in w t+1 is 1 \u03bb i t j=i+1 j\u22121 j = 1 \u03bb t ,\nwhich implies that we can rewrite w t+1 as\nw t+1 = 1 \u03bb t t i=1 v i .(13)\nFrom the above we immediately get that w t+1 \u2264 R/\u03bb and therefore \u2207 t \u2264 2R. Finally, we need to prove that w \u2208 B. If we do not perform projections then we have\nw \u2208 \u00ca n = B.\nOtherwise, we need to show that w \u2264 1/ \u221a \u03bb. To do so, we examine the dual form of the SVM problem and use the strong duality theorem. In its more traditional form, the SVM learning problem was described as the following constrained optimization problem,\n1 2 w 2 + C m i=1 \u03be i s.t. \u2200i \u2208 [m] : \u03be i \u2265 0, \u03be i \u2265 1 \u2212 y i w, x i .(14)\nSetting C = 1/(\u03bbm) this problem becomes equivalent to our formulation given in Eq. (1) and Eq. (2). The dual problem of Eq. ( 14) is,\nm i=1 \u03b1 i \u2212 1 2 m i=1 \u03b1 i y i x i 2 s.t. \u2200i \u2208 [m] : 0 \u2264 \u03b1 i \u2264 C .(15)\nLet us denote the optimal primal and dual solutions by (w , \u03be ) and \u03b1 , respectively. The primal solution can be written in terms of its dual counterpart as w = m i=1 \u03b1 i y i x i . At the optimum value \u03b1 , Eq. ( 15) can be rewritten as,\n\u03b1 1 \u2212 1 2 w 2 .\nMoreover, from strong duality we know that the primal objective value is equal to the dual objective value at the optimum, thus\n1 2 w 2 + C \u03be 1 = \u03b1 1 \u2212 1 2 w 2 .\nnote that \u03b1 \u221e \u2264 C = 1 \u03bbm . Therefore, \u03b1 1 \u2264 1/\u03bb and we get that\n1 2 w 2 \u2264 1 2 w 2 + C \u03be 1 = \u03b1 1 \u2212 1 2 w 2 \u2264 1 \u03bb \u2212 1 2 w 2 .\nRearranging the terms yields w \u2264 1/ \u221a \u03bb. The bound in the theorem now follows from Lemma 1.\nWe now turn to obtaining a bound on the overall objective f (w t ) evaluated at a single predictor w t . The convexity of f implies that:\nf 1 T T t=1 w t \u2264 1 T T t=1 f (w t ) .(16)\nUsing the above inequality and Thm. 1, we immediately obtain Corollary 1 which provides a convergence analysis for the deterministic case when k = m where f (w, A t ) = f (w).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Corollary 1 Assume that the conditions stated in Thm. 1 and that", "text": "A t = S for all t. Let w = 1 T T t=1 w t . Then, f (w) \u2264 f (w ) + c(1 + ln(T )) 2 \u03bb T .\nWhen A t \u2282 S, Corollary 1 no longer holds. However, Kakade and Tewari [22] have shown that a similar bound holds with high probability as long as A t is sampled from S. [22]) Assume that the conditions stated in Thm. 1 hold and that for all t, each element in A t is sampled uniformly at random from S (with or without repetitions). Assume also that R \u2265 1 and \u03bb \u2264 1/4. Then, with a probability of at least 1 \u2212 \u03b4 we have 1", "publication_ref": ["b21", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Lemma 2 (Corollary 7 in", "text": "T T t=1 f (w t ) \u2212 f (w ) \u2264 21 c ln(T /\u03b4) \u03bb T .\nCombining the above with Eq. ( 16) we immediately obtain the following corrolary.\nCorollary 2 Assume that the conditions stated in Lemma 2 hold and letw = 1 T T t=1 w t . Then, with probability of at least 1 \u2212 \u03b4 we have\nf (w) \u2264 f (w ) + 21 c ln(T /\u03b4) \u03bb T .\nThe previous corollaries hold for the average hypothesisw. In practice, the final hypothesis, w T +1 , often provides better results. We next bridge this gap by providing a similar convergence rate for a different mechanism of choosing the output vector. To do so, we first show that at least half of the hypotheses are good.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Lemma 3", "text": "Assume that the conditions stated in Lemma 2 hold. Then, if t is selected at random from [T ], we have with a probability of at least 1 2 that f (w t ) \u2264 f (w ) + 42 c ln(T /\u03b4) \u03bb T .\nProof Define a random variable Z = f (w t ) \u2212 f (w ) where the randomness is over the choice of the index t. From the definition of w as the minimizer of f (w) we clearly have that Z is a non-negative random variable. Thus, from Markov inequality \u00c8[Z \u2265\n2 [Z]] \u2264 1 2 .\nThe claim now follows by combining the fact that [Z] = 1 T T t=1 f (w t ) \u2212 f (w ) with the bound given in Lemma 2.\nBased on the above lemma we conclude that if we terminate the procedure at a random iteration, in at least half of the cases the last hypothesis is an accurate solution. Therefore, we can simply try a random stopping time and evaluate the error of the last hypothesis 1 . The above lemma tells us that on average after two attempts we are likely to find a good solution.", "publication_ref": ["b0", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Using Mercer kernels", "text": "One of the main benefits of SVMs is that they can be used with kernels rather then with direct access to the feature vectors x. The crux of this property stems from the Representer Theorem [23], which implies that the optimal solution of Eq. (1) can be expressed as a linear combination of the training instances. It is therefore possible to train and use a SVM without direct access to the training instances, and instead only access their inner products as specified through a kernel operator. That is, instead of considering predictors which are linear functions of the training instances x themselves, we consider predictors which are linear functions of some implicit mapping \u03c6(x) of the instances. Training then involves solving the minimization problem:\nmin w \u03bb 2 w 2 + 1 m (x,y)\u2208S (w; (\u03c6(x), y)) ,(17)\nwhere (w; (\u03c6(x), y)) = max{0, 1 \u2212 y w, \u03c6(x) } .\nHowever, the mapping \u03c6(\u2022) is never specified explicitly but rather through a kernel operator K(x, x ) = \u03c6(x), \u03c6(x ) yielding the inner products after the mapping \u03c6(\u2022). One possible and rather common approach for solving the optimization problem 17 is to switch to the dual problem, which can be written in terms of inner products of vectors \u03c6(\u2022). Therefore, the dual problem can be solely expressed using kernel operators. However, solving the dual problem is not necessary. Following [16,24,10], the approach we take here is to directly minimize the primal problem while still using kernels.\nINPUT: S, \u03bb, T\nINITIALIZE: Set \u03b1 1 = 0 FOR t = 1, 2, . . . , T Choose it \u2208 {0, . . . , |S|} uniformly at random. For all j = it, set \u03b1 t+1 [j] = \u03b1t[j] If y it 1 \u03bbt j \u03b1t[j]y it K(x it , x j ) < 1, then: Set \u03b1 t+1 [it] = \u03b1t[it] + 1 Else: Set \u03b1 t+1 [it] = \u03b1t[it]\nOUTPUT: \u03b1 T +1 Fig. 3 The Kernelized Pegasos Algorithm.\nWe now show that the Pegasos algorithm can be implemented using only kernel evaluations, without direct access to the feature vectors \u03c6(x) or explicit access to the weight vector w. For simplicity, we focus on adapting the basic Pegasos algorithm given in Fig. 1 without the optional projection step. As we have shown in the proof of Thm. 1 (in particular, Eq. ( 13)), for all t we can rewrite w t+1 as\nw t+1 = 1 \u03bb t t i=1 \u00bd[y i t w t , \u03c6(x i t ) < 1] y i t \u03c6(x i t ) .\nFor each t, let \u03b1 t+1 \u2208 \u00ca m be the vector such that \u03b1 t+1 [j] counts how many times example j has been selected so far and we had a non-zero loss on it, namely,\n\u03b1 t+1 [j] = |{t \u2264 t : i t = j \u2227 y j w t , \u03c6(x j ) < 1}| .\nInstead of keeping in memory the weight vector w t+1 , we will represent w t+1 , using \u03b1 t+1 according to\nw t+1 = 1 \u03bb t m j=1 \u03b1 t+1 [j] y j \u03c6(x j ) .\nIt is now easy to implement the Pegasos algorithm by maintaining the vector \u03b1. The pseudocode of this kernelized implementation of Pegasos is given in Fig. 3. Note that only one element of \u03b1 is changed at each iteration. It is also important to emphasize that although the feature mapping \u03c6(\u2022) was used in the above mathematical derivations, the pseudo-code of the algorithm itself makes use only of kernel evaluations and obviously does not refer to the implicit mapping \u03c6(\u2022).\nSince the iterates w t remain as before (just their representation changes), the guarantees on the accuracy after a number of iterations are still valid. We are thus guaranteed to find an -accurate solution after\u00d5(1/(\u03bb )) iterations. However, checking for non-zero loss at iteration t might now require as many as min(t, m) kernel evaluations, bringing the overall runtime to\u00d5(m/(\u03bb )). Therefore, although the number of iterations required does not depend on the number of training examples, the runtime does.\nIt is worthwhile pointing out that even though the solution is represented in terms of the variables \u03b1, we are still calculating the sub-gradient with respect to the weight vector w. A different approach, that was taken, e.g., by Chapelle [10], is to rewrite the primal problem as a function of \u03b1 and then taking gradients with respect to \u03b1. Concretely, the Representer theorem guarantees that the optimal solution of Eq. ( 17) is spanned by the training instances, i.e. it is of the form, w = m i=1 \u03b1[i]\u03c6(x i ). In optimizing Eq. ( 17) we can therefore focus only on predictors of this form, parametrized through \u03b1 \u2208 \u00ca m . The training objective can then be written in terms of the \u03b1 variables and kernel evaluations:\nmin \u03b1 \u03bb 2 m i,j=1 \u03b1[i]\u03b1[j]K(x i , x j ) + 1 m m i=1 max{0, 1 \u2212 y i m j=1 \u03b1[j]K(x i , x j )} .(19)\nNow, one can use stochastic gradient updates for solving Eq. (19), where gradients should be taken w.r.t. \u03b1. We emphasize again that our approach is different as we compute subgradients w.r.t. w. Setting the step direction according to the sub-gradient w.r.t w has two important advantages. First, only at most one new non-zero \u03b1[i] is introduced at each iteration, as opposed to a sub-gradient step w.r.t. \u03b1 which will involve all m coefficients. More importantly, the objective given in Eq. ( 19) is not necessarily strongly-convex w.r.t. \u03b1, even though it is strongly convex w.r.t. w. Thus, a gradient descent approach using gradients w.r.t. \u03b1 might require \u2126(1/ 2 ) iterations to achieve accuracy . Interestingly, Chapelle also proposes preconditioning the gradients w.r.t. \u03b1 by the kernel matrix, which effectively amounts to taking gradients w.r.t. w, as we do here. Unsurprisingly given the above discussion, Chapelle observes much better results with this preconditioning.", "publication_ref": ["b22", "b15", "b23", "b9", "b9", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Other prediction problems and loss functions", "text": "So far, we focused on the SVM formulation for binary classification using the hinge-loss. In this section we show how Pegasos can seamlessly be adapted to other prediction problems in which we use other loss functions.\nThe basic observation is that the only place in which we use the fact that (w; (x, y)) is the hinge-loss (Eq. (2)) is when we calculated a sub-gradient of (w; (x, y)) with respect to w. The assumptions we made are that is convex and that the norm of the sub-gradient is at most R. The generality of these assumptions implies that we can apply Pegasos with any loss function which satisfies these requirements.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Examples", "text": "Example 1 (Binary classification with the log-loss) Instead of the hinge-loss, other loss functions can also be used with binary labels y \u2208 {+1, \u22121}. A popular choice is the logloss defined as: (w, (x, y)) = log(1 + exp(\u2212y w, x )). It is easy to verify that the log loss is a convex function whose gradient w.r.t. w satisfies \u2207 \u2264 x .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Example 2 (Regression with the -insensitive loss)", "text": "We now turn to regression problems over the reals, that is y \u2208 \u00ca. The standard Support Vector Regression formulation uses the loss function defined as (w; (x, y)) = max{0, | w, x \u2212 y| \u2212 }. This loss is also convex with a sub-gradient bounded by x .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Example 3 (Cost-sensitive multiclass categorization)", "text": "In multi-class categorization problems, the goal is to predict a label y \u2208 Y where Y is a finite discrete set of classes. A possible loss function is the so-called cost-sensitive loss defined as:\n(w; (x, y)) = max y \u2208Y \u03b4(y , y) \u2212 w, \u03c6(x, y) + w, \u03c6(x, y ) ,(20)\nwhere \u03b4(y , y) is the cost of predicting y instead of y and \u03c6(x, y) is a mapping from inputlabel pair (x, y) into a vector space. See for example [11]. The multiclass loss is again a convex loss function whose sub-gradient is bounded by 2 max y \u03c6(x, y ) .", "publication_ref": ["b10"], "figure_ref": [], "table_ref": []}, {"heading": "Example 4 (Multiclass categorization with the log-loss)", "text": "Given the same setting of the above multiclass example, we can also generalize the log loss to handle multiclass problems. Omitting the cost term, the multiclass loss amounts to:\n(w; (x, y)) = log \uf8eb \uf8ed 1 + r =y e w,\u03c6(x,r) \u2212 w,\u03c6(x,y) \uf8f6 \uf8f8 ,(21)\nwhere \u03c6(x, y) is defined above. The log-loss version of the multiclass loss is convex as well with a bounded sub-gradient whose value is at most, 2 max y \u03c6(x, y ) .\nExample 5 (Sequence prediction) Sequence prediction is similar to cost-sensitive multiclass categorization, but the set of targets, Y, can be very large. For example, in phoneme recognition tasks, X is the set of all speech utterances and Y is the set of all phoneme sequences. Therefore, |Y| is exponential in the length of the sequence. Nonetheless, if the functions \u03c6 and \u03b4 adheres to a specific structure then we can still calculate sub-gradients efficiently and therefore solve the resulting optimization problem efficiently using Pegasos.\nTo recap the examples provided above we give a table of the sub-gradients of some popular loss functions. To remind the reader, given a convex function f (w), a sub-gradient of f at w 0 is a vector v which satisfies:\n\u2200w, f (w) \u2212 f (w 0 ) \u2265 v, w \u2212 w 0 .\nThe following two properties of sub-gradients are used for calculating the sub-gradients in the table below.\n1. If f (w) is differentiable at w 0 , then the gradient of f at w 0 is the unique sub-gradient of f at w 0 . 2. If f (w) = max i f i (w) for r differentiable functions f 1 , . . . , f r , and j = arg max i f i (w 0 ), then the gradient of f j at w 0 is a sub-gradient of f at w 0 .\nBased on the above two properties, we now show explicitly how to calculate a subgradient for several loss functions. In the following table, we use the notation z = w t , x i .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Loss function Subgradient", "text": "(z, y i ) = max{0, 1 \u2212 y i z} vt = \u2212y i x i if y i z < 1 0 otherwise (z, y i ) = log(1 + e \u2212y i z ) vt = \u2212 y i 1+e y i z x i (z, y i ) = max{0, |y i \u2212 z| \u2212 } vt = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 x i if z \u2212 y i > \u2212x i if y i \u2212 z > 0 otherwise (z, y i ) = max y\u2208Y \u03b4(y, y i ) \u2212 zy i + zy vt = \u03c6(x i ,\u0177) \u2212 \u03c6(x i , y i ) where\u0177 = arg max y \u03b4(y, yi) \u2212 zy i + zy (z, y i ) = log \uf8eb \uf8ed 1 + r =y i e zr \u2212zy i \uf8f6 \uf8f8 vt = r pr\u03c6(x i , r) \u2212 \u03c6(x i , y i )\nwhere pr = e zr / j e z j", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Incorporating a bias term", "text": "In many applications, the weight vector w is augmented with a bias term which is a scalar, typically denoted as b. The prediction for an instance x becomes w, x + b and the loss is accordingly defined as,\n((w, b); (x, y)) = max{0, 1 \u2212 y( w, x + b)} . (22\n)\nThe bias term often plays a crucial role when the distribution of the labels is uneven as is typically the case in text processing applications where the negative examples vastly outnumber the positive ones. We now review several approaches for learning the bias term while underscoring the advantages and disadvantages of each approach. The first approach is rather well known and its roots go back to early work on pattern recognition [14]. This approach simply amounts to adding one more feature to each instance x thus increasing the dimension to n + 1. The artificially added feature always takes the same value. We assume without loss of generality that the value of the constant feature is 1. Once the constant feature is added the rest of the algorithm remains intact, thus the bias term is not explicitly introduced. The analysis can be repeated verbatim and we therefore obtain the same convergence rate for this modification. Note however that by equating the n + 1 component of w with b, the norm-penalty counterpart of f becomes w 2 + b 2 . The disadvantage of this approach is thus that we solve a relatively different optimization problem. On the other hand, an obvious advantage of this approach is that it requires no modifications to the algorithm itself rather than a modest increase in the dimension and it can thus be used without any restriction on A t .\nAn alternate approach incorporates b explicitly by defining the loss as given in Eq. ( 22) while not penalizing for b. Formally, the task is to find an approximate solution to the following problem:\nmin w,b \u03bb 2 w 2 + 1 m (x,y)\u2208S [1 \u2212 y( w, x + b)] + .(23)\nNote that all the sub-gradients calculations with respect to w remain intact. The sub-gradient with respect to b is also simple to compute. This approach is also very simple to implement and can be used with any choice of A t , in particular, sets consisting of a single instance. The caveat of this approach is that the function f ceases to be strongly convex due to the incorporation of b. Precisely, the objective function f becomes piece-wise linear in the direction of b and is thus no longer strongly convex. Therefore, the analysis presented in the previous section no longer holds. An alternative proof technique yields a slower convergence rate of O(1/ \u221a T ). A third method entertains the advantages of the two methods above at the price of a more complex algorithm that is applicable only for large batch sizes (large values of k), but not for the basic Pegasos algorithm (with k = 1). The main idea is to rewrite the optimization problem given in Eq. ( 23 \n(x,y)\u2208S [1 \u2212 y( w, x + b)] + .(24)\nBased on the above, we redefine f (w; A t ) to be \u03bb 2 w 2 + g(w; A t ). On each iteration of the algorithm, we find a sub-gradient of f (w; A t ) and subtract it (multiplied by \u03b7 t ) from w t . The problem however is how to find a sub-gradient of g(w; A t ), as g(w; A t ) is defined through a minimization problem over b. This essentially amounts to solving the minimization problem in Eq. (24). The latter problem is a generalized weighted median problem that can be solved efficiently in time O(k). The above adaptation indeed work for the case k = m where we have A t = S and we obtain the same rate of convergence as in the no-bias case. However, when A t = S we cannot apply the analysis from the previous section to our case since the expectation of f (w; A t ) over the choice of A t is no longer equal to f (w; S). When A t is large enough, it might be possible to use more involved measure concentration tools to show that the expectation of f (w; A t ) is close enough to f (w; S) so as to still obtain fast convergence properties.\nA final possibility is to search over the bias term b in an external loop, optimizing the weight vector w using Pegasos for different possible values of b. That is, consider the objective:\nJ (b; S) = min w 1 m (x,y)\u2208S [1 \u2212 y( w, x + b)] + .(25)\nFor a fixed b, the minimization problem in Eq. ( 25) is very similar to SVM training without a bias term, and can be optimized using Pegasos. The objective J (b; S) is convex in the single scalar variable b, and so J (b; S) can be optimized to within accuracy by binary search using O(log 1/ ) evaluations of J (b; S), i.e. O(log 1/ ) applications of the Pegasos algorithm. Since this modification introduced only an additional logarithmic factor, the overall runtime for training an SVM with a bias term remains\u00d5(d/(\u03bb )). Although incorporating a regularized or unregularized bias term might be better in practice, the latter \"outer loop\" approach is the only method that we are aware of which guarantees an overall runtime of O(d/(\u03bb )).", "publication_ref": ["b13", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In this section we present experimental results that demonstrate the merits of our algorithm. We start by demonstrating the practicality of Pegasos for solving large scale linear problems, especially when the feature vectors are sparse. In particular, we compare its runtime on three large datasets to the runtimes of the state-of-the-art solver SVM-Perf [21], a cutting plane algorithm designed specifically for use with sparse feature vectors, as well as of two more conventional SVM solvers: LASVM [2] and SVM-Light [20]. We next demonstrate that Pegasos can also be a reasonable approach for large scale problems involving non-linear kernels by comparing it to LASVM and SVM-Light on four large data sets using Gaussian kernels. We then investigate the effect of various parameters and implementation choices on Pegasos: we demonstrate the runtime dependence on the regularization parameter \u03bb; we explore the empirical behavior of the mini-batch variant and the dependence on the mini-batch size k; and we compare the effect of sampling training examples both with and without replacement. Finally, we compare Pegasos to two previously proposed methods that are based on stochastic gradient descent: Norma [24] by Kivinen, Smola, Williamson and to the method by Zhang [37].\nWe also include in our experiments a comparison with stochastic Dual Coordinate Ascent (DCA). Following Pegasos's initial presentation [31], stochastic DCA was suggested as an alternative optimization method for SVMs [18]. DCA shares numerous similarities with Pegasos. Like Pegasos, at each iteration only a single (random) training example (y i , x i ) is considered, and if y i w, x i < 1, an update of the form w \u2190 w + \u03b7y i x i is performed. However, the DCA step size \u03b7 is not predetermined, but rather chosen so as to maximize the dual objective. DCA's convergence properties and the differences between DCA and Pegasos behavior are not yet well understood. For informational purposes, we include a comparison to DCA in our empirical evaluations.\nOur implementation of Pegasos is based on the algorithm from Fig. 1, outputting the last weight vector rather than the average weight vector, as we found that in practice it performs better. We did not incorporate a bias term in any of our experiments. We found that including an unregularized bias term does not significantly change the predictive performance for any of the data sets used. Furthermore, most methods we compare to, including [21,24,37,18], do not incorporate a bias term either. Nonetheless, there are clearly learning problems where the incorporation of the bias term could be beneficial.\nWe used our own implementation of Pegasos, as well as stochastic DCA, and both were instrumented to periodically output the weight vector w or, in the kernel case, the vector of coefficients \u03b1. The source code for SVM-Perf, LASVM and SVM-Light were downloaded from their respective authors' web pages, and were similarly modified. These modifications allowed us to generate traces of each algorithm's progress over time, which were then used to generate all plots and tables. Whenever a runtime is reported, the time spent inside the instrumentation code, as well as the time spent loading the data file, is not included. All implementations are in C/C++, and all experiments were performed on a single core of a load-free machine with an Intel Core i7 920 CPU and 12G of RAM. Fig. 4 contains traces of the primal suboptimality, and testing classification error, achieved by Pegasos, stochastic DCA, SVM-Perf, and LASVM. The latter of these is not an algorithm specialized for linear SVMs, and therefore should not be expected to perform as well as the others. Neither Pegasos nor stochastic DCA have a natural stopping criterion. Hence, in order to uniformly summarize the performance of the various algorithms, we found the first time at which the primal suboptimality was less than some predetermined termination threshold . We chose this threshold for each dataset such that a primal suboptimality less than guarantees a classification error on test data which is at most 1.1 times the test data classification error at the optimum. (For instance, if full optimization of SVM yields a test classification error of 1%, then we chose such that a -accurate optimization would guarantee test classification error of at most 1.1%.) The time taken to satisfy the termination criterion, on each dataset, for each algorithm, along with classification errors on test data achieved at termination, are reported in Table 1.   1 Training runtime and test error achieved (in parentheses) using various optimization methods on linear SVM problems. The suboptimality thresholds used for termination are = 0.0275, 0.00589 and 0.0449 on the astro-ph, CCAT and cov1 datasets (respectively). The testing classification errors at the optima of the SVM objectives are 3.36%, 6.03% and 22.6%.\nBased both on the plots of Fig. 4, and on Table 1, we can see that, SVM-Perf is a very fast method on it own. Indeed, SVM-Perf was shown in [21] to achieve a speedup over SVM-Light of several orders of magnitude on most datasets. Nonetheless, Pegasos and stochastic DCA achieve a significant improvement in run-time over SVM-Perf. It is interesting to note that the performance of Pegasos does not depend on the number of examples but rather on the value of \u03bb. Indeed, the runtime of Pegasos for the Covertype dataset is longer than its runtime for CCAT, although the latter dataset is larger. This issue is explored further in Sec. 7.3 given in the sequel.", "publication_ref": ["b20", "b1", "b19", "b23", "b36", "b30", "b17", "b20", "b23", "b36", "b17", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments with Gaussian kernels", "text": "Pegasos is particularly well suited for optimization of linear SVMs, in which case the runtime does not depend on the data set size. However, as we show in the sequel, the kernelized Pegasos variant described in section 4 gives good performance on a range of kernel SVM problems, provided that these problems have sufficient regularization. Although Pegasos does not outperform state-of-the-art methods in our experiments, it should be noted that Pegasos is a very simple method to implement, requiring only a few lines of code.\nThe experiments in this section were performed on four datasets downloaded from L\u00e9on Bottou's LASVM web page 2 . The USPS and MNIST datasets were used for the task of classifying the digit 8 versus the rest of the classes. In the following table, \u03b3 is the parameter controlling the width of the Gaussian kernel K (x, y) = e \u2212\u03b3 x\u2212y 2 2 , and \u03bb is the Pegasos regularization parameter. The parameters for the Reuters dataset are taken from [2], while those for the Adult dataset are from [29]. The parameters for the USPS and MNIST datasets are based on those in [2], but we increased the regularization parameters by a factor of 1000. This change resulted in no difference in the testing set classification error at the optimum on the USPS dataset, and increased it from 0.46% to 0.57% on MNIST. We discuss the performance of Pegasos with smaller values of the regularization parameter \u03bb in the next section. As in the linear experiments, we chose a primal suboptimality threshold for each dataset which guarantees a testing classification error within 10% of that at the optimum. The runtime required to achieve these targets, along with the test classification errors, are reported in Table 2.\nAs in the linear case, Pegasos (and stochastic DCA) achieve a reasonably low value of the primal objective very quickly, much faster than SVM-Light. However, on the USPS and MNIST datasets, very high optimization accuracy is required in order to achieve nearoptimal predictive performance, and such accuracy is much harder to achieve using the stochastic methods. Note that the test error on these data sets is very small (roughly 0.5%).\nFurthermore, when using kernels, LASVM essentially dominates Pegasos and stochastic DCA, even when relatively low accuracy is required. On all four datasets, LASVM appears to enjoy the best properties of the other algorithms: it both makes significant progress during early iterations, and converges rapidly in later iterations. Nevertheless, the very simple method Pegasos still often yields very good predictive performance, with a competitive runtime.\nIt can also be interesting to compare the different methods also in terms of the number of kernel evaluations performed. All of the implementations use the same sparse representation for vectors, so the amount of time which it takes to perform a single kernel evaluation should, for each dataset, be roughly the same across all four algorithms. However, various other factors, such overhead resulting from the complexity of the implementation, or caching of portions of the kernel matrix, do affect the number of kernel evaluations which are performed in a given unit of time. Nevertheless, the discrepancy between the relative performance in terms of runtime and the relative performance in terms of number of kernel evaluations is fairly minor. To summarize this discrepancy, we calculated for each method and each data set the kernel evaluation throughput: the number of kernel evaluations performed per second of execution in the above runs. For each data set, we then normalized these throughputs by dividing each method's throughput by the Pegasos throughput, thus obtaining a relative measure indicating whether some methods are using much more, or much fewer, kernel evaluations, relative to their runtime. The resulting relative kernel evaluation throughputs are summarized in Table 3. It is unsurprising that Pegasos and stochastic DCA, as the simplest algorithms, tend to have performance most dominated by kernel evaluations. If we were to compare the algorithms in terms of the number of kernel evaluations, rather than elapsed time, then LASVMs performance would generally improve slightly relative to the others. But in any case, the change to the relative performance would not be dramatic.", "publication_ref": ["b1", "b1", "b28", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Effect of the regularization parameter \u03bb", "text": "We return now to the influence of the values of the regularization parameter \u03bb on the runtime of Pegasos and stochastic DCA. Recall that in the previous section, we choose to use a much larger value of \u03bb in our experiments with the MNIST and USPS datasets. Fig. 6 shows the suboptimalities of the primal objective achieved by Pegasos and stochastic DCA after   certain fixed numbers of iterations, on the USPS dataset, as a function of the regularization parameter \u03bb. As predicted by the formal analysis, the primal suboptimality after a fixed number of Pegasos iterations is inversely proportional to \u03bb. Hence, the runtime to achieve a predetermined suboptimality threshold would increase in proportion to \u03bb. Very small values of \u03bb (small amounts of regularization) result in rather long runtimes. This phenomenon has been observed before and there have been rather successful attempts to improve Pegasos when \u03bb is small (see for example [13]). It is interesting to note that stochastic DCA does not seem to suffer from this problem. Although Pegasos and stochastic DCA have comparable runtimes for moderate values of \u03bb, stochastic DCA is much better behaved when \u03bb is very small (i.e. when the problem is barely infused with regularization).", "publication_ref": ["b12"], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Experiments with the Mini-Batch Variant", "text": "In this section, we explore the influence of the mini-batch size, k, of the mini-batch variant of Fig. 2. Increasing k does not reduce our theoretical guarantees on the number of iterations T that are required to attain a primal suboptimality goal. Since the runtime of each iteration scales linearly with k, the convergence analysis suggests that increasing k would only cause a linear increase in the overall runtime kT required to achieve a predetermined accuracy goal. We show that in practice, for moderate sizes of k, a roughly linear (in k) improvement in the number of required iterations T can be achieved, leaving the overall runtime kT almost fixed. For a serial implementation of Pegasos, this result would be uninteresting. However, using large samples for computing the subgradients can be useful in a parallel implementation, where the O(k) work of each iteration could be done in parallel, thus reducing the overall required elapsed time. Fig. 7 includes two plots which illustrate the impact of k on the performance of Pegasos. The first plot shows that, on the astro-ph dataset, for sufficiently small values of k, the primal suboptimality achieved after T iterations is roughly proportional to the product kT . This property holds in the region for which the curves are roughly horizontal, which in this experiment, corresponds to mini-batch sizes of up to a few hundred training points. Note also that the three curves on the left hand side plot of Fig. 7 start increasing at different values of k. It appears that, when k is overly large, there is initially indeed a loss of performance. However, as the number of iterations increases, the slower behavior due to the mini-batch size is alleviated.\nThe second plot further underscores this phenomenon. We can see that, for three values of k, all significantly greater than 100, the experiments with the largest mini-batch size made the least progress while performing the same amount of computation. However, as the number of iterations grows, the suboptimalities become similar. The end result is that the overall runtime does not seem to be strongly dependent on the mini-batch size. We do not yet have a good quantitative theoretical understanding of the mini-batch results observed here.", "publication_ref": [], "figure_ref": ["fig_5", "fig_5"], "table_ref": []}, {"heading": "Comparison of sampling procedures", "text": "The analysis of Pegasos requires sampling with replacement at each iteration. Based on private communication with L\u00e9on Bottou we experimented with sampling without replacement. Specifically, we chose a random permutation over the training examples and performed updates in accordance to the selected order. Once we traversed all the permuted examples, we chose a new permutation and iteratively repeated the process. We also experimented with a further simplified approach in which a single random permutation is drawn and then used repeatedly. Fig. 8 indicates that, on the astro-ph dataset, the sampling without replacement procedures outperform significantly the uniform i.i.d. sampling procedure. Further, it seems that choosing a new permutation every epoch, rather than keeping the permutation intact, provides some slight further improvement. We would like to note though that while the last experiment underscores the potential for additional improvement in the convergence rate, the rest of the experiments reported in the paper were conducted in accordance with the formal analysis using uniform sampling with replacements.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Comparison to other stochastic gradient methods", "text": "In our last set of experiments, we compared Pegasos to Norma [24] and to a variant of stochastic gradient descent due to Zhang [37]. Both methods share similarities with Pegasos when k = 1, and differ in their schemes for setting the learning rate \u03b7 t . Thm. 4 from [24], suggests to set \u03b7 t = p/(\u03bb \u221a t), where p \u2208 (0, 1). Based on the bound given in the theorem, the optimal choice of p is 0.5(2 + 0.5T \u22121/2 ) 1/2 , which for t \u2265 100 is in the range [0.7, 0.716]. Plugging the optimal value of p into Thm. 4 in [24] yields the bound O(1/(\u03bb \u221a T )). We therefore conjectured that Pegasos would converge significantly faster than Norma. On the left hand side of Fig. 7.6 we compare Pegasos (with the optional projection step) to Norma on the Astro-Physics dataset. We divided the dataset to a training set with 29,882 examples and a test set with 32,487 examples. We report the final objective value and the average hinge-loss on the test set. As in [21], we set \u03bb = 2 \u2022 10 \u22124 . It is clear from the figure that Pegasos outperforms Norma. Moreover, Norma fails to converge even after 10 6 iterations. The poor performance of Norma can be attributed to the fact that the value of \u03bb here is rather small. We now turn to comparing Pegasos to the algorithm of Zhang [37] which simply sets \u03b7 t = \u03b7, where \u03b7 is a (fixed) small number. A major disadvantage of this approach is that finding an adequate value for \u03b7 is a difficult task on its own. Based on the analysis given in [37] we started by setting \u03b7 to be 10 \u22125 . Surprisingly, this value turned out to be a poor choice and the optimal choice of \u03b7 was substantially larger. On the right hand side of Fig. 7.6 we illustrate the convergence of stochastic gradient descent with \u03b7 t set to be a fixed value from the set {0.001, 0.01, 0.1, 1, 10}. It is apparent that for some choices of \u03b7 the method converges at about the same rate of Pegasos while for other choices of \u03b7 the method fails to converge. For large datasets, the time required for evaluating the objective is often much longer than the time required for training a model. Therefore, searching for \u03b7 is significantly more expensive than running the algorithm a single time. The apparent advantage of Pegasos is due to the fact that we do not need to search for a good value for \u03b7 but rather have a predefined schedule for \u03b7 t .", "publication_ref": ["b23", "b36", "b23", "b23", "b28", "b20", "b36", "b36"], "figure_ref": ["fig_5", "fig_5"], "table_ref": []}, {"heading": "Conclusions", "text": "We described and analyzed a simple and effective algorithm for approximately minimizing the objective function of SVM. We derived fast rate of convergence results and experimented with the algorithm. Our empirical results indicate that for linear kernels, Pegasos achieves state-of-the-art results, despite of, or possibly due to, its simplicity. When used with more complex kernels, Pegasos may still be a simple competitive alternative to more complicated methods, especially when fairly lax optimization can be tolerated.\nRecently, Bottou and Bousquet [4] proposed to analyse optimization algorithms from the perspective of the underlying machine learning task. In a subsequent paper [32], we analyze Pegasos and other SVM training methods from a machine learning perspective, and showed that Pegasos is more efficient than other methods when measuring the runtime required to guarantee good predictive performance (test error).", "publication_ref": ["b3", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We would like to thank L\u00e9on Bottou for useful discussions and suggestions, and Thorsten Joachims and L\u00e9on Bottou for help with the experiments. We would also like to thank the anonymous reviewers for their helpful comments. Part of this work was done while SS and NS were visiting IBM research labs, Haifa, Israel. This work was partially supported by grant I-773-8.6/2003 from the German Israeli Foundation (GIF) and by the Israel Science Foundation under grant number 522/04.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Natural gradient works efficiently in learning", "journal": "Neural Computation", "year": "1998", "authors": "S Amari"}, {"ref_id": "b1", "title": "Fast kernel classifiers with online and active learning", "journal": "Journal of Machine Learning Research", "year": "2005", "authors": "A Bordes; S Ertekin; J Weston; L Bottou"}, {"ref_id": "b2", "title": "Online algorithms and stochastic approximations", "journal": "Cambridge University Press", "year": "1998", "authors": "L Bottou"}, {"ref_id": "b3", "title": "The tradeoffs of large scale learning", "journal": "Advances in Neural Information Processing Systems", "year": "2008", "authors": "L Bottou; O Bousquet"}, {"ref_id": "b4", "title": "Large scale online learning", "journal": "MIT Press", "year": "2004", "authors": "L Bottou; Y Lecun"}, {"ref_id": "b5", "title": "Stochastic approximations and efficient learning", "journal": "The MIT Press", "year": "2002", "authors": "L Bottou; N Murata"}, {"ref_id": "b6", "title": "Convex Optimization", "journal": "Cambridge University Press", "year": "2004", "authors": "S Boyd; L Vandenberghe"}, {"ref_id": "b7", "title": "Parallel Optimization: Theory, Algorithms, and Applications", "journal": "Oxford University Press", "year": "1997", "authors": "Y Censor; S Zenios"}, {"ref_id": "b8", "title": "On the generalization ability of on-line learning algorithms", "journal": "IEEE Transactions on Information Theory", "year": "2004", "authors": "N Cesa-Bianchi; A Conconi; C Gentile"}, {"ref_id": "b9", "title": "Training a support vector machine in the primal", "journal": "Neural Computation", "year": "2007", "authors": "O Chapelle"}, {"ref_id": "b10", "title": "Online passive aggressive algorithms", "journal": "Journal of Machine Learning Research", "year": "2006", "authors": "K Crammer; O Dekel; J Keshet; S Shalev-Shwartz; Y Singer"}, {"ref_id": "b11", "title": "An Introduction to Support Vector Machines", "journal": "Cambridge University Press", "year": "2000", "authors": "N Cristianini; J Shawe-Taylor"}, {"ref_id": "b12", "title": "Proximal regularization for online and batch learning", "journal": "", "year": "2009", "authors": "C Do; Q Le; C Foo"}, {"ref_id": "b13", "title": "Pattern Classification and Scene Analysis", "journal": "Wiley", "year": "1973", "authors": "R O Duda; P E Hart"}, {"ref_id": "b14", "title": "Efficient SVM training using low-rank kernel representations", "journal": "J. of Mach. Learning Res", "year": "2001", "authors": "S Fine; K Scheinberg"}, {"ref_id": "b15", "title": "Large margin classification using the perceptron algorithm", "journal": "Machine Learning", "year": "1999", "authors": "Y Freund; R E Schapire"}, {"ref_id": "b16", "title": "Logarithmic regret algorithms for online convex optimization", "journal": "", "year": "2006", "authors": "E Hazan; A Kalai; S Kale; A Agarwal"}, {"ref_id": "b17", "title": "A dual coordinate descent method for largescale linear SVM", "journal": "ICML", "year": "2008", "authors": "C Hsieh; K Chang; C Lin; S Keerthi; S Sundararajan"}, {"ref_id": "b18", "title": "Qp algorithms with guaranteed accuracy and run time for support vector machines", "journal": "Journal of Machine Learning Research", "year": "2006", "authors": "D Hush; P Kelly; C Scovel; I Steinwart"}, {"ref_id": "b19", "title": "Making large-scale support vector machine learning practical", "journal": "MIT Press", "year": "1998", "authors": "T Joachims"}, {"ref_id": "b20", "title": "Training linear SVMs in linear time", "journal": "", "year": "2006", "authors": "T Joachims"}, {"ref_id": "b21", "title": "On the generalization ability of online strongly convex programming algorithms", "journal": "Advances in Neural Information Processing Systems", "year": "2009", "authors": "S Kakade; A Tewari"}, {"ref_id": "b22", "title": "Some results on tchebycheffian spline functions", "journal": "J. Math. Anal. Applic", "year": "1971", "authors": "G Kimeldorf; G Wahba"}, {"ref_id": "b23", "title": "Online learning with kernels", "journal": "IEEE Transactions on Signal Processing", "year": "2002", "authors": "J Kivinen; A J Smola; R C Williamson"}, {"ref_id": "b24", "title": "Stochastic approximation algorithms and applications", "journal": "Springer-Verlag", "year": "1997", "authors": "H Kushner; G Yin"}, {"ref_id": "b25", "title": "A statistical study of on-line learning", "journal": "Cambridge University Press", "year": "1998", "authors": "N Murata"}, {"ref_id": "b26", "title": "Statistical analysis of learning dynamics", "journal": "Signal Processing", "year": "1999", "authors": "N Murata; S Amari"}, {"ref_id": "b27", "title": "Primal-dual subgradient methods for convex problems", "journal": "", "year": "2005", "authors": "Y Nesterov"}, {"ref_id": "b28", "title": "Fast training of Support Vector Machines using sequential minimal optimization", "journal": "MIT Press", "year": "1998", "authors": "J C Platt"}, {"ref_id": "b29", "title": "Convex Analysis", "journal": "Princeton University Press", "year": "1970", "authors": "R Rockafellar"}, {"ref_id": "b30", "title": "Pegasos: Primal Estimated sub-GrAdient SOlver for SVM", "journal": "", "year": "2007", "authors": "S Shalev-Shwartz; Y Singer; N Srebro"}, {"ref_id": "b31", "title": "Proceedings of the 25th International Conference on Machine Learning", "journal": "", "year": "2008", "authors": "S Shalev-Shwartz; N Srebro"}, {"ref_id": "b32", "title": "Bundle methods for machine learning", "journal": "Advances in Neural Information Processing Systems", "year": "2007", "authors": "A Smola; S Vishwanathan; Q Le"}, {"ref_id": "b33", "title": "Introduction to Stochastic Search and Optimization", "journal": "Wiley", "year": "2003", "authors": "J C Spall"}, {"ref_id": "b34", "title": "Fast rates for regularized objectives", "journal": "Advances in Neural Information Processing Systems", "year": "2009", "authors": "K Sridharan; N Srebro; S Shalev-Shwartz"}, {"ref_id": "b35", "title": "Statistical Learning Theory", "journal": "Wiley", "year": "1998", "authors": "V N Vapnik"}, {"ref_id": "b36", "title": "Solving large scale linear prediction problems using stochastic gradient descent algorithms", "journal": "", "year": "2004", "authors": "T Zhang"}], "figures": [{"figure_label": "22", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": ") as min w \u03bb 2 w 222+ g(w; S) where g(w; S) = min b 1 m", "figure_data": ""}, {"figure_label": "14", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "7. 1 Fig. 414Fig.4Comparison of linear SVM optimizers. Primal suboptimality (top row) and testing classification error (bottom row), for one run each of Pegasos, stochastic DCA, SVM-Perf, and LASVM, on the astro-ph (left), CCAT (center) and cov1 (right) datasets. In all plots the horizontal axis measures runtime in seconds.", "figure_data": ""}, {"figure_label": "55", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Fig. 5 Fig. 555Fig. 5 Comparison of kernel SVM optimizers. Primal suboptimality (top row), primal suboptimality in log scale (middle row) and testing classification error (bottom row), for one run each of Pegasos, stochastic DCA, SVM-Light, and LASVM, on the Reuters (left column), Adult (center column) and USPS (right column) datasets. Plots of traces generated on the MNIST dataset (not shown) appear broadly similar to those for the USPS dataset. The horizontal axis is runtime in seconds.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Fig. 66Fig.6 Demonstration of dependence of Pegasos' performance on regularization, on the USPS dataset. This plot shows (on a log-log scale) the primal suboptimalities of Pegasos and stochastic DCA after certain fixed numbers of iterations, for various values of \u03bb.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Fig. 77Fig.7The effect of the mini-batch size on the runtime of Pegasos for the astro-ph dataset. The first plot shows the primal suboptimality achieved for certain fixed values of overall runtime kT , for various values of the mini-batch size k. The second plot shows the primal suboptimality achieved for certain fixed values of k, for various values of kT . Very similar results were achieved for the CCAT dataset.", "figure_data": ""}, {"figure_label": "89", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Fig. 8 Fig. 989Fig.8The effect of different sampling methods on the performance of Pegasos for the astro-ph dataset. The curves show the primal suboptimality achieved by uniform i.i.d. sampling, sampling from a fixed permutation, and sampling from a different permutation for every epoch.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Choose it \u2208 {1, . . . , |S|} uniformly at random. Set \u03b7t = 1 \u03bbt If y it wt, x it < 1, then: Set w t+1 \u2190 (1 \u2212 \u03b7t\u03bb)wt + \u03b7ty it x it Else (if y it wt, x it \u2265 1): Set w t+1 \u2190 (1 \u2212 \u03b7t\u03bb)wt", "figure_data": "INPUT: S, \u03bb, TINITIALIZE: Set w 1 = 0FOR t = 1, 2, . . . , T[ Optional:"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ""}, {"figure_label": "23", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Training runtime and test error achieved (in parentheses) using various optimization methods on linear SVM problems. = 0.00719, 0.0445, 0.000381 and 0.00144 on the Reuters, Adult, USPS and MNIST datasets (respectively). The testing classification errors at the optima of the SVM objectives are 2.88%, 14.9%, 0.457% and 0.57%. Relative kernel evaluation throughputs: the number of kernel evaluations per second of runtime divided by Pegasos's number of kernel evaluations per second of runtime on the same dataset.", "figure_data": "DatasetPegasosSDCASVM-LightLASVMReuters11.031.140.88Adult10.900.940.60USPS10.970.690.81MNIST10.940.990.61"}], "formulas": [{"formula_id": "formula_2", "formula_text": "f (w; i t ) = \u03bb 2 w 2 + (w; (x i t , y i t )) .(3)", "formula_coordinates": [4.0, 164.64, 513.02, 252.99, 22.06]}, {"formula_id": "formula_3", "formula_text": "\u2207 t = \u03bb w t \u2212 \u00bd[y i t w t , x i t < 1] y i t x i t ,(4)", "formula_coordinates": [4.0, 161.76, 561.34, 255.87, 16.81]}, {"formula_id": "formula_4", "formula_text": "w t+1 \u2190 min 1, 1/ \u221a \u03bb w t+1 w t+1 ] OUTPUT: w T +1", "formula_coordinates": [5.0, 157.2, 122.88, 179.53, 28.18]}, {"formula_id": "formula_5", "formula_text": "w t+1 \u2190 (1 \u2212 1 t )w t + \u03b7 t \u00bd[y i t w t , x i t < 1] y i t x i t .(5)", "formula_coordinates": [5.0, 139.2, 217.46, 278.43, 22.29]}, {"formula_id": "formula_6", "formula_text": "w t+1 \u2190 min 1, 1/ \u221a \u03bb w t+1 w t+1 .(6)", "formula_coordinates": [5.0, 177.0, 354.11, 240.63, 24.24]}, {"formula_id": "formula_7", "formula_text": "f (w; A t ) = \u03bb 2 w 2 + 1 k i\u2208A t (w; (x i , y i )) .(7)", "formula_coordinates": [5.0, 151.44, 579.14, 266.19, 26.55]}, {"formula_id": "formula_8", "formula_text": "INITIALIZE: Set w 1 = 0 FOR t = 1, 2, . . . , T Choose At \u2286 [m]", "formula_coordinates": [6.0, 146.4, 52.5, 81.03, 26.45]}, {"formula_id": "formula_9", "formula_text": "A + t = {i \u2208 At : y i wt, x i < 1} Set \u03b7t = 1 \u03bbt Set w t+1 \u2190 (1 \u2212 \u03b7t \u03bb)wt + \u03b7t k i\u2208A + t y i x i [Optional: w t+1 \u2190 min 1, 1/ \u221a \u03bb w t+1 w t+1 ] OUTPUT: w T +1", "formula_coordinates": [6.0, 146.4, 79.08, 179.66, 57.34]}, {"formula_id": "formula_10", "formula_text": "\u2207 t = \u03bb w t \u2212 1 k i\u2208A t \u00bd[y i w t , x i < 1] y i x i .(8)", "formula_coordinates": [6.0, 156.12, 226.7, 261.51, 26.67]}, {"formula_id": "formula_12", "formula_text": "1 T T t=1 f t (w t ) \u2264 1 T T t=1 f t (u) + G 2 (1 + ln(T )) 2 \u03bb T .", "formula_coordinates": [7.0, 147.84, 221.15, 195.18, 29.54]}, {"formula_id": "formula_13", "formula_text": "w t \u2212 u, \u2207 t \u2265 f t (w t ) \u2212 f t (u) + \u03bb 2 w t \u2212 u 2 . (10", "formula_coordinates": [7.0, 147.12, 277.91, 266.48, 18.48]}, {"formula_id": "formula_14", "formula_text": ")", "formula_coordinates": [7.0, 413.6, 280.86, 3.91, 8.52]}, {"formula_id": "formula_15", "formula_text": "w t \u2212 u, \u2207 t \u2264 w t \u2212 u 2 \u2212 w t+1 \u2212 u 2 2 \u03b7 t + \u03b7 t 2 G 2 . (11", "formula_coordinates": [7.0, 136.32, 311.39, 277.28, 24.27]}, {"formula_id": "formula_16", "formula_text": ")", "formula_coordinates": [7.0, 413.6, 319.98, 3.91, 8.52]}, {"formula_id": "formula_17", "formula_text": "w t \u2212 u 2 \u2212 w t+1 \u2212 u 2 \u2265 w t \u2212 u 2 \u2212 w t \u2212 u 2 = 2\u03b7 t w t \u2212 u, \u2207 t \u2212 \u03b7 2 t \u2207 t 2 .", "formula_coordinates": [7.0, 126.0, 370.55, 242.58, 33.0]}, {"formula_id": "formula_18", "formula_text": "T t=1 (f t (w t ) \u2212 f t (u)) \u2264 T t=1 w t \u2212 u 2 \u2212 w t+1 \u2212 u 2 2 \u03b7 t + \u03bb 2 w t \u2212 u 2 + G 2 2 T t=1 \u03b7 t .", "formula_coordinates": [7.0, 104.04, 434.75, 281.7, 62.79]}, {"formula_id": "formula_19", "formula_text": "T t=1 (f t (w t ) \u2212 f t (u)) \u2264 \u2212\u03bb (T + 1) w T +1 \u2212 u 2 + G 2 2 \u03bb T t=1 1 t \u2264 G 2 2 \u03bb (1 + ln(T )) .", "formula_coordinates": [7.0, 118.92, 533.99, 250.74, 56.28]}, {"formula_id": "formula_20", "formula_text": "T \u2265 3, 1 T T t=1 f (w t ; A t ) \u2264 1 T T t=1 f (w ; A t ) + c(1 + ln(T )) 2\u03bbT .", "formula_coordinates": [8.0, 134.64, 95.42, 221.58, 49.07]}, {"formula_id": "formula_21", "formula_text": "w t+1 = 1 \u2212 1 t w t \u2212 1 t \u03bb v t ,(12)", "formula_coordinates": [8.0, 181.56, 264.59, 235.95, 17.88]}, {"formula_id": "formula_22", "formula_text": "where v t = 1 |A t | i\u2208A t \u00bd[y i w t , x t < 1] y i x i .", "formula_coordinates": [8.0, 72.0, 286.07, 191.72, 18.33]}, {"formula_id": "formula_23", "formula_text": "v i in w t+1 is 1 \u03bb i t j=i+1 j\u22121 j = 1 \u03bb t ,", "formula_coordinates": [8.0, 138.72, 314.34, 150.78, 48.2]}, {"formula_id": "formula_24", "formula_text": "w t+1 = 1 \u03bb t t i=1 v i .(13)", "formula_coordinates": [8.0, 204.24, 391.67, 213.27, 29.67]}, {"formula_id": "formula_25", "formula_text": "w \u2208 \u00ca n = B.", "formula_coordinates": [8.0, 72.0, 451.67, 59.49, 18.12]}, {"formula_id": "formula_26", "formula_text": "1 2 w 2 + C m i=1 \u03be i s.t. \u2200i \u2208 [m] : \u03be i \u2265 0, \u03be i \u2265 1 \u2212 y i w, x i .(14)", "formula_coordinates": [8.0, 117.6, 505.91, 299.91, 29.67]}, {"formula_id": "formula_27", "formula_text": "m i=1 \u03b1 i \u2212 1 2 m i=1 \u03b1 i y i x i 2 s.t. \u2200i \u2208 [m] : 0 \u2264 \u03b1 i \u2264 C .(15)", "formula_coordinates": [8.0, 133.2, 574.79, 284.31, 31.59]}, {"formula_id": "formula_28", "formula_text": "\u03b1 1 \u2212 1 2 w 2 .", "formula_coordinates": [9.0, 210.96, 83.78, 72.78, 22.29]}, {"formula_id": "formula_29", "formula_text": "1 2 w 2 + C \u03be 1 = \u03b1 1 \u2212 1 2 w 2 .", "formula_coordinates": [9.0, 160.92, 143.06, 169.14, 22.29]}, {"formula_id": "formula_30", "formula_text": "1 2 w 2 \u2264 1 2 w 2 + C \u03be 1 = \u03b1 1 \u2212 1 2 w 2 \u2264 1 \u03bb \u2212 1 2 w 2 .", "formula_coordinates": [9.0, 103.44, 193.82, 284.1, 22.29]}, {"formula_id": "formula_31", "formula_text": "f 1 T T t=1 w t \u2264 1 T T t=1 f (w t ) .(16)", "formula_coordinates": [9.0, 175.56, 289.55, 241.95, 29.42]}, {"formula_id": "formula_32", "formula_text": "A t = S for all t. Let w = 1 T T t=1 w t . Then, f (w) \u2264 f (w ) + c(1 + ln(T )) 2 \u03bb T .", "formula_coordinates": [9.0, 72.0, 358.46, 660.66, 54.45]}, {"formula_id": "formula_33", "formula_text": "T T t=1 f (w t ) \u2212 f (w ) \u2264 21 c ln(T /\u03b4) \u03bb T .", "formula_coordinates": [9.0, 165.72, 494.63, 159.54, 29.55]}, {"formula_id": "formula_34", "formula_text": "f (w) \u2264 f (w ) + 21 c ln(T /\u03b4) \u03bb T .", "formula_coordinates": [9.0, 179.4, 581.06, 130.86, 22.29]}, {"formula_id": "formula_35", "formula_text": "2 [Z]] \u2264 1 2 .", "formula_coordinates": [10.0, 72.0, 190.91, 53.6, 18.0]}, {"formula_id": "formula_36", "formula_text": "min w \u03bb 2 w 2 + 1 m (x,y)\u2208S (w; (\u03c6(x), y)) ,(17)", "formula_coordinates": [10.0, 159.12, 428.42, 258.39, 26.52]}, {"formula_id": "formula_38", "formula_text": "INITIALIZE: Set \u03b1 1 = 0 FOR t = 1, 2, . . . , T Choose it \u2208 {0, . . . , |S|} uniformly at random. For all j = it, set \u03b1 t+1 [j] = \u03b1t[j] If y it 1 \u03bbt j \u03b1t[j]y it K(x it , x j ) < 1, then: Set \u03b1 t+1 [it] = \u03b1t[it] + 1 Else: Set \u03b1 t+1 [it] = \u03b1t[it]", "formula_coordinates": [11.0, 161.76, 58.14, 170.35, 94.47]}, {"formula_id": "formula_39", "formula_text": "w t+1 = 1 \u03bb t t i=1 \u00bd[y i t w t , \u03c6(x i t ) < 1] y i t \u03c6(x i t ) .", "formula_coordinates": [11.0, 144.12, 277.91, 201.54, 29.67]}, {"formula_id": "formula_40", "formula_text": "\u03b1 t+1 [j] = |{t \u2264 t : i t = j \u2227 y j w t , \u03c6(x j ) < 1}| .", "formula_coordinates": [11.0, 137.28, 346.46, 215.1, 16.17]}, {"formula_id": "formula_41", "formula_text": "w t+1 = 1 \u03bb t m j=1 \u03b1 t+1 [j] y j \u03c6(x j ) .", "formula_coordinates": [11.0, 176.64, 386.15, 136.38, 29.67]}, {"formula_id": "formula_42", "formula_text": "min \u03b1 \u03bb 2 m i,j=1 \u03b1[i]\u03b1[j]K(x i , x j ) + 1 m m i=1 max{0, 1 \u2212 y i m j=1 \u03b1[j]K(x i , x j )} .(19)", "formula_coordinates": [12.0, 86.64, 92.03, 330.87, 29.66]}, {"formula_id": "formula_43", "formula_text": "(w; (x, y)) = max y \u2208Y \u03b4(y , y) \u2212 w, \u03c6(x, y) + w, \u03c6(x, y ) ,(20)", "formula_coordinates": [12.0, 126.36, 590.3, 291.15, 19.38]}, {"formula_id": "formula_44", "formula_text": "(w; (x, y)) = log \uf8eb \uf8ed 1 + r =y e w,\u03c6(x,r) \u2212 w,\u03c6(x,y) \uf8f6 \uf8f8 ,(21)", "formula_coordinates": [13.0, 137.76, 164.81, 279.75, 32.49]}, {"formula_id": "formula_45", "formula_text": "\u2200w, f (w) \u2212 f (w 0 ) \u2265 v, w \u2212 w 0 .", "formula_coordinates": [13.0, 168.6, 436.34, 152.46, 16.17]}, {"formula_id": "formula_46", "formula_text": "(z, y i ) = max{0, 1 \u2212 y i z} vt = \u2212y i x i if y i z < 1 0 otherwise (z, y i ) = log(1 + e \u2212y i z ) vt = \u2212 y i 1+e y i z x i (z, y i ) = max{0, |y i \u2212 z| \u2212 } vt = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 x i if z \u2212 y i > \u2212x i if y i \u2212 z > 0 otherwise (z, y i ) = max y\u2208Y \u03b4(y, y i ) \u2212 zy i + zy vt = \u03c6(x i ,\u0177) \u2212 \u03c6(x i , y i ) where\u0177 = arg max y \u03b4(y, yi) \u2212 zy i + zy (z, y i ) = log \uf8eb \uf8ed 1 + r =y i e zr \u2212zy i \uf8f6 \uf8f8 vt = r pr\u03c6(x i , r) \u2212 \u03c6(x i , y i )", "formula_coordinates": [14.0, 115.68, 76.26, 259.36, 217.36]}, {"formula_id": "formula_47", "formula_text": "((w, b); (x, y)) = max{0, 1 \u2212 y( w, x + b)} . (22", "formula_coordinates": [14.0, 154.8, 400.46, 258.8, 16.17]}, {"formula_id": "formula_48", "formula_text": ")", "formula_coordinates": [14.0, 413.6, 401.1, 3.91, 8.52]}, {"formula_id": "formula_49", "formula_text": "min w,b \u03bb 2 w 2 + 1 m (x,y)\u2208S [1 \u2212 y( w, x + b)] + .(23)", "formula_coordinates": [15.0, 146.76, 86.54, 270.75, 26.63]}, {"formula_id": "formula_50", "formula_text": "(x,y)\u2208S [1 \u2212 y( w, x + b)] + .(24)", "formula_coordinates": [15.0, 225.96, 276.02, 191.55, 16.17]}, {"formula_id": "formula_51", "formula_text": "J (b; S) = min w 1 m (x,y)\u2208S [1 \u2212 y( w, x + b)] + .(25)", "formula_coordinates": [15.0, 143.64, 478.19, 273.87, 17.88]}], "doi": "10.1162/neco.2007.19.5.1155"}