{"title": "Nearly tight sample complexity bounds for learning mixtures of Gaussians via sample compression schemes *", "authors": "Hassan Ashtiani; Shai Ben-David; Nicholas J A Harvey; Christopher Liaw; Abbas Mehrabian; Yaniv Plan", "pub_date": "", "abstract": "We prove that \u0398(kd 2 /\u03b5 2 ) samples are necessary and sufficient for learning a mixture of k Gaussians in R d , up to error \u03b5 in total variation distance. This improves both the known upper bounds and lower bounds for this problem. For mixtures of axis-aligned Gaussians, we show that O(kd/\u03b5 2 ) samples suffice, matching a known lower bound. The upper bound is based on a novel technique for distribution learning based on a notion of sample compression. Any class of distributions that allows such a sample compression scheme can also be learned with few samples. Moreover, if a class of distributions has such a compression scheme, then so do the classes of products and mixtures of those distributions. The core of our main result is showing that the class of Gaussians in R d has a small-sized sample compression.", "sections": [{"heading": "Introduction", "text": "Estimating distributions from observed data is a fundamental task in statistics that has been studied for over a century. This task frequently arises in applied machine learning and it is common to assume that the distribution can be modeled using a mixture of Gaussians. Popular software packages have implemented heuristics, such as the EM algorithm, for learning a mixture of Gaussians. The theoretical machine learning community also has a rich literature on distribution learning; the recent survey [9] considers learning structured distributions, and the survey [13] focuses on mixtures of Gaussians.\nThis paper develops a general technique for distribution learning, then employs this technique in the important setting of mixtures of Gaussians. The theoretical model we adopt is density estimation:\ngiven i.i.d. samples from an unknown target distribution, find a distribution that is close to the target distribution in total variation (TV) distance. Our focus is on sample complexity bounds: using as few samples as possible to obtain a good estimate of the target distribution. For background on this model see, e.g., [7,Chapter 5] and [9].\nOur new technique for proving upper bounds on the sample complexity involves a form of sample compression. If it is possible to \"encode\" members of a class of distributions using a carefully chosen subset of the samples, then this yields an upper bound on the sample complexity of distribution learning for that class. In particular, by constructing compression schemes for mixtures of axisaligned Gaussians and general Gaussians, we obtain new upper bounds on the sample complexity of learning with respect to these classes, which we prove to be optimal up to logarithmic factors.", "publication_ref": ["b8", "b13", "b6", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Main results", "text": "In this section, all learning results refer to the problem of producing a distribution within total variation distance \u03b5 from the target distribution. Our first main result is an upper bound for learning mixtures of multivariate Gaussians. This bound is tight up to logarithmic factors. Theorem 1.1 The class of k-mixtures of d-dimensional Gaussians can be learned using O(kd 2 /\u03b5 2 ) samples.\nWe emphasize that the O(\u2022) notation hides a factor polylog(kd/\u03b5), but has no dependence whatsoever on the condition number or scaling of the distribution. Previously, the best known upper bounds on the sample complexity of this problem were O(kd 2 /\u03b5 4 ), due to [3], and O(k 4 d 4 /\u03b5 2 ), based on a VC-dimension bound that we discuss below. For the case of a single Gaussian (i.e., k = 1), a sample complexity bound of O(d 2 /\u03b5 2 ) is well known, again using a VC-dimension bound discussed below.\nOur second main result is a minimax lower bound matching Theorem 1.1 up to logarithmic factors. Theorem 1.2 Any method for learning the class of k-mixtures of d-dimensional Gaussians has sample complexity \u2126(kd 2 /\u03b5 2 log 3 (1/\u03b5)) = \u2126(kd 2 /\u03b5 2 ).\nHere and below \u2126 (and O) allow for poly-logarithmic factors. Previously, the best known lower bound on the sample complexity was \u2126(kd/\u03b5 2 ) [20]. Even for a single Gaussian (i.e., k = 1), an \u2126(d 2 /\u03b5 2 ) lower bound was not known prior to this work.\nOur third main result is an upper bound for learning mixtures of axis-aligned Gaussians, i.e., Gaussians with diagonal covariance matrices. This bound is tight up to logarithmic factors. Theorem 1.3 The class of k-mixtures of axis-aligned d-dimensional Gaussians can be learned using O(kd/\u03b5 2 ) samples.\nA matching lower bound of \u2126(kd/\u03b5 2 ) was proved in [20]. Previously, the best known upper bounds were O(kd/\u03b5 4 ), due to [3], and O((k\n4 d 2 + k 3 d 3 )/\u03b5 2 )\n, based on a VC-dimension bound that we discuss below.\nComputational efficiency. Although our approach for proving sample complexity upper bounds is algorithmic, our focus is not on computational efficiency. The resulting algorithms have nearly optimal sample complexities, but their running times are exponential in the dimension d and the number of mixture components k. More precisely, the running time is 2 kd 2 polylog(d,k,1/\u03b5) for mixtures of general Gaussians, and 2 kd polylog(d,k,1/\u03b5) for mixtures of axis-aligned Gaussians. The existence of a polynomial time algorithm for density estimation is unknown even for the class of mixtures of axis-aligned Gaussians, see [10,Question 1.1].\nEven for the case of a single Gaussian, the published proofs of the O(d 2 /\u03b5 2 ) bound (of which we are aware) are not algorithmically efficient. Using ideas from our proof of Theorem 1.1, in the full version we show that an algorithmically efficient proof for single Gaussians can be obtained by computing the empirical mean and a careful modification of the sample covariance matrix of O(d 2 /\u03b5 2 ) samples.", "publication_ref": ["b2", "b20", "b20", "b2", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Related work", "text": "Distribution learning is a vast topic and many approaches have been considered in the literature; here we only review approaches that are most relevant to our problem.\nFor parametric families of distributions, a common approach is to use the samples to estimate the parameters of the distribution, possibly in a maximum likelihood sense, or possibly aiming to approximate the true parameters. For the specific case of mixtures of Gaussians, there is a substantial theoretical literature on algorithms that approximate the mixing weights, means and covariances; see [13] for a recent survey of this literature. The strictness of this objective cuts both ways. On the one hand, a successful learner uncovers substantial structure of the target distribution. On the other hand, this objective is clearly impossible when the means and covariances are extremely close. Thus, algorithms for parameter estimation of mixtures necessarily require some \"separability\" assumptions on the target parameters.\nDensity estimation has a long history in the statistics literature, where the focus is on the sample complexity question; see [6,7,19] for general background. It was first studied in the computational learning theory community under the name PAC learning of distributions by [14], whose focus is on the computational complexity of the learning problem.\nFor density estimation there are various possible measures of distance between distributions, the most popular ones being the TV distance and the Kullback-Leibler (KL) divergence. Here we focus on the TV distance since it has several appealing properties, such as being a metric and having a natural probabilistic interpretation. In contrast, KL divergence is not even symmetric and can be unbounded even for intuitively close distributions. For a detailed discussion on why TV is a natural choice, see [7,Chapter 5].\nA popular method for distribution learning in practice is kernel density estimation (see, e.g., [7,Chapter 9]). The few rigorously proven sample complexity bounds for this method require either smoothness assumptions (e.g., [7, Theorem 9.5]) or boundedness assumptions (e.g., [12, Theorem 2.2]) on the class of densities. The class of Gaussians is not universally Lipschitz or universally bounded, so those results do not apply to the problems we consider.\nAnother elementary method for density estimation is using histogram estimators (see, e.g., [7,Section 10.3]). Straightforward calculations show that histogram estimators for mixtures of Gaussians result in a sample complexity that is exponential in the dimension. The same is true for estimators based on piecewise polynomials.\nThe minimum distance estimate [7, Section 6.8] is another approach for deriving sample complexity upper bounds for distribution learning. This approach is based on uniform convergence theory. In particular, an upper bound for any class of distributions can be achieved by bounding the VCdimension of an associated set system, called the Yatracos class (see [7, page 58] for the definition). For example, [11] used this approach to bound the sample complexity of learning high-dimensional log-concave distributions. For the class of single Gaussians in d dimensions, this approach leads to the optimal sample complexity upper bound of O(d 2 /\u03b5 2 ). However, for mixtures of Gaussians and axis-aligned Gaussians in R d , the best known VC-dimension bounds [1,Theorem 8.14] \nresult in loose upper bounds of O(k 4 d 4 /\u03b5 2 ) and O((k 4 d 2 + k 3 d 3 )/\u03b5 2 ), respectively.\nAnother approach is to first approximate the mixture class using a more manageable class such as piecewise polynomials, and then study the associated Yatracos class, see, e.g., [5]. However, piecewise polynomials do a poor job in approximating d-dimensional Gaussians, resulting in an exponential dependence on d.\nFor density estimation of mixtures of Gaussians, the current best sample complexity upper bounds (in terms of k and d) are O(kd 2 /\u03b5 4 ) for general Gaussians and O(kd/\u03b5 4 ) for axis-aligned Gaussians, both due to [3]. For the general Gaussian case, their method takes an i.i.d. sample of size O(kd 2 /\u03b5 2 ) and partitions this sample in every possible way into k subsets. Based on those partitions, k O(kd 2 /\u03b5 2 ) \"candidate distributions\" are generated. The problem is then reduced to learning with respect to this finite class of candidates. Their sample complexity has a suboptimal factor of 1/\u03b5 4 , of which 1/\u03b5 2 arises in their approach for choosing the best candidate, and another factor 1/\u03b5 2 is due to the exponent in the number of candidates.\nOur approach via compression schemes also ultimately reduces the problem to learning with respect to finite classes. However, our compression technique leads to a more refined bound. In the case of mixtures of Gaussians, one factor of 1/\u03b5 2 is again incurred due to learning with respect to finite classes. The key is that the number of compressed samples has no additional factor of 1/\u03b5 2 , so the overall sample complexity bound has only an O(1/\u03b5 2 ) dependence on \u03b5.\nAs for lower bounds on the sample complexity, much fewer results are known for learning mixtures of Gaussians. The only lower bound of which we are aware is due to [20], which shows a bound of \u2126(kd/\u03b5 2 ) for learning mixtures of spherical Gaussians (and hence for general Gaussians as well). This bound is tight for the axis-aligned case, as we show in Theorem 1.3, but loose in the general case, as we show in Theorem 1.2.", "publication_ref": ["b13", "b5", "b6", "b19", "b14", "b6", "b6", "b6", "b10", "b0", "b4", "b2", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Comparison to parameter estimation", "text": "In this section we observe that neither our upper bound (Theorem 1.1) nor our lower bound (Theorem 1.2) can directly follow from bounds on parameter estimation for Gaussian distributions. Recall that our sample complexity upper bound in Theorem 1.1 has no dependence on the condition number of the distribution. We now show that, if a learning algorithm with entrywise approximation guarantee is used to learn the distribution in KL divergence or TV distance, then the approximation parameter must depend on the condition number. Let \u03ba(\u03a3) be the condition number of the covariance matrix \u03a3, i.e., the ratio of the maximum and minimum eigenvalues; refer to Section 2 for other relevant definitions.\nProposition 1.4 Set \u03b5 = 2 \u03ba(\u03a3)+1\n. There exist two covariance matrices \u03a3 and\u03a3 that are good entrywise approximations:\n|\u03a3 i,j \u2212\u03a3 i,j | \u2264 \u03b5 and\u03a3 i,j \u2208 [1, 1 + 2\u03b5] \u2022 \u03a3 i,j \u2200i, j,\nbut the corresponding distributions are as far as they can get, i.e.,\nKL N (0, \u03a3) N (0,\u03a3) = \u221e and TV N (0, \u03a3), N (0,\u03a3) = 1.\nThus, given a black-box algorithm that provides an entrywise \u03b5-approximation to the true covariance matrix \u03a3, if \u03b5 \u2265 2 \u03ba(\u03a3)+1 , it might output\u03a3, which does not approximate \u03a3 in KL divergence or total variation distance.\nOne might imagine that lower bounds on the sample complexity of parameter estimation readily imply lower bounds on distribution learning. The following proposition shows this is not the case. Proposition 1.5 For any \u03b5 \u2208 (0, 1/2] there exist two covariance matrices \u03a3 and\u03a3 such that\nTV N (0, \u03a3), N (0,\u03a3) \u2264 \u03b5, but there exist i, j such that, for any c \u2265 1,\u03a3 i,j \u2208 [1/c, c] \u2022 \u03a3 i,j .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Our techniques", "text": "We introduce a method for learning distributions via a novel form of compression. Given a class of distributions, suppose there is a method for \"compressing\" information about the true distribution using a mix of samples from that distribution and some additional bits. Further, suppose there exists a fixed (deterministic) decoder for the class, such that given the samples and additional bits, it approximately recovers the original distribution. In this case, if the size of the compressed set and the number of bits is guaranteed to be small, we show that the sample complexity of learning that class is small as well.\nMore precisely, we say a class of distributions admits (\u03c4, t, m) compression if there exists a decoder function such that upon generating m i.i.d. samples from any distribution in the class, we are guaranteed, with reasonable probability, to have a subset of size at most \u03c4 of that sample, and a sequence of at most t bits, on which the decoder outputs an approximation to the original distribution. Note that \u03c4, t, and m can be functions of \u03b5, the accuracy parameter.\nWe prove that compression implies learning. In particular, if a class admits (\u03c4, t, m) compression, then the sample complexity of learning with respect to this class is bounded by O(m + (\u03c4 + t)/\u03b5 2 ) (Theorem 3.5).\nAn attractive property of compression is that it enjoys two closure properties. Specifically, if a base class admits compression, then the class of mixtures of that base class, as well as the class of products of the base class, are compressible (Lemmas 3.6 and 3.7).\nConsequently, it suffices to provide a compression scheme for the class of single Gaussian distributions in order to obtain a compression scheme for the class of mixtures of Gaussians (and therefore, to be able to bound their sample complexity). We prove that the class of d-dimensional Gaussian distributions admits ( O(d), O(d 2 ), O(d)) compression (Lemma 4.1). The high level idea is that by generating O(d) samples from a Gaussian, one can get a rough sketch of the geometry of the Gaussian. In particular, the points drawn from a Gaussian concentrate around an ellipsoid centered at the mean and whose principal axes are the eigenvectors of the covariance matrix. Using ideas from convex geometry and random matrix theory, we show one can in fact encode the center of the ellipsoid and the principal axes using a linear combination of these samples. Then we discretize the coefficients and obtain an approximate encoding.\nThe above results together imply tight (up to logarithmic factors) upper bounds of O(kd 2 /\u03b5 2 ) for mixtures of k Gaussians, and O(kd/\u03b5 2 ) for mixtures of k axis-aligned Gaussians over R d . The compression framework we introduce is quite flexible, and can be used to prove sample complexity upper bounds for other distribution classes as well. This is left for future work.\nIn this paper we assume the target belongs to some known class of distributions (this is called the realizable setting in the learning theory literature). In the full version of this paper [2] we relax this requirement and give similar sample complexity bounds for the setting where the target is close (in TV distance) to some distribution in the class (known as agnostic learning).\nLower bound. For proving our lower bound for mixtures of Gaussians, we first prove a lower bound of \u2126(d 2 /\u03b5 2 ) for learning a single Gaussian. Although the approach is quite intuitive, the details are intricate and much care is required to make a formal proof. The main step is to construct a large family (of size 2 \u2126(d 2 ) ) of covariance matrices such that the associated Gaussian distributions are well-separated in terms of their TV distance while simultaneously ensuring that their relative KL divergences are small. Once this is established, we can then apply a generalized version of Fano's inequality to complete the proof.\nTo construct this family of covariance matrices, we sample 2 \u2126(d 2 ) matrices from the following probabilistic process: start with an identity covariance matrix; then choose a random subspace of dimension d/9 and slightly increase the eigenvalues corresponding to this eigenspace. It is easy to bound the KL divergences between the constructed Gaussians. To lower bound the total variation, we show that for every pair of these distributions, there is some subspace for which a vector drawn from one Gaussian will have slightly larger projection than a vector drawn from the other Gaussian. Quantifying this gap will then give us the desired lower bound on the total variation distance.\nPaper outline. We set up our formal framework and notation in Section 2. In Section 3, we define compression schemes for distributions, prove their closure properties, and show their connection with density estimation. Theorem 1.1 and Theorem 1.3 are proved in Section 4. The proof of Theorem 1.2 as well as all other omitted proofs can be found in the full version [2].", "publication_ref": ["b1", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "A distribution learning method or density estimation method is an algorithm that takes as input a sequence of i.i.d. samples generated from a distribution g, and outputs (a description of) a distribution g as an estimation for g. We work with continuous distributions in this paper, and so we identify a probability distribution by its probability density function. Let f 1 and f 2 be two probability distributions defined over R d and let L be the set of Lebesgue measurable subsets of R d . Their total variation (TV) distance is defined by\nTV (f 1 , f 2 ) := sup B\u2208L B f 1 (x) \u2212 f 2 (x) dx = 1 2 f 1 \u2212 f 2 1 ,\nwhere\nf 1 := R d |f (x)|dx is the L 1 norm of f . The Kullback-Leibler (KL) divergence between f 1 and f 2 is defined by KL (f 1 f 2 ) := R d f 1 (x) log f 1 (x) f 2 (x) dx.\nIn the following definitions, F is a class of probability distributions, and g is a distribution (not necessarily in F).\nDefinition 2.1 (\u03b5-approximation) A distribution\u011d is an \u03b5-approximation for g if \u011d \u2212 g 1 \u2264 \u03b5.\nDefinition 2.2 (PAC-learning distributions) A distribution learning method is called a (realizable) PAC-learner for F with sample complexity m F (\u03b5, \u03b4) if, for all distributions g \u2208 F and all \u03b5, \u03b4 \u2208 (0, 1), given \u03b5, \u03b4, and an i.i.d. sample of size m F (\u03b5, \u03b4) from g, with probability at least 1 \u2212 \u03b4 (over the samples) the method outputs an \u03b5-approximation of g.\nLet \u2206 n := { (w 1 , . . . , w n ) : w i \u2265 0, w i = 1 } denote the n-dimensional simplex. Definition 2.3 (k-mix(F))\nLet F be a class of probability distributions. Then the class of k-mixtures of F, written k-mix(F), is defined as\nk-mix(F) := k i=1 w i f i : (w 1 , . . . , w k ) \u2208 \u2206 k , f 1 , . . . , f k \u2208 F .\nLet d denote the dimension. A Gaussian distribution with mean \u00b5 \u2208 R d and covariance matrix \u03a3 \u2208 R d\u00d7d is denoted by N (\u00b5, \u03a3). If \u03a3 is a diagonal matrix, then N (\u00b5, \u03a3) is called an axis-aligned Gaussian. For a distribution g, we write X \u223c g to mean X is a random variable with distribution g, and we write S \u223c g m to mean that S is an i.i.d. sample of size m generated from g.\nWe will use v or v 2 to denote the Euclidean norm of a vector v, A or A 2 to denote the operator norm of a matrix A, and A F := Tr(A T A) to denote the Frobenius norm of a matrix A.\nFor x \u2208 R, we will write (x) + := max{0, x}. All logarithms are in the natural base.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Compression schemes and their connection with learning", "text": "Let F be a class of distributions over a domain Z.\nDefinition 3.1 (distribution decoder) A distribution decoder for F is a deterministic function J :\n\u221e n=0 Z n \u00d7 \u221e n=0{0\n, 1} n \u2192 F, which takes a finite sequence of elements of Z and a finite sequence of bits, and outputs a member of F. Definition 3.2 (distribution compression schemes) Let \u03c4, t, m : (0, 1) \u2192 Z \u22650 be functions. We say F admits (\u03c4, t, m) compression if there exists a decoder J for F such that for any distribution g \u2208 F, the following holds:\nFor any \u03b5 \u2208 (0, 1), if a sample S is drawn from g m(\u03b5) , then with probability at least 2/3, there exists a sequence L of at most \u03c4 (\u03b5) elements of S, and a sequence B of at most t(\u03b5) bits, such that J (L, B) \u2212 g 1 \u2264 \u03b5.\nNote that S and L are sequences rather than sets; in particular, they can contain repetitions. Also note that in this definition, m(\u03b5) is a lower bound on the number of samples needed, whereas \u03c4 (\u03b5), t(\u03b5) are upper bounds on the size of compression and the number of bits.\nEssentially, the definition asserts that with reasonable probability, there is a (short) sequence consisting of elements S and some (small number of) additional bits, from which g can be approximately reconstructed. We say that the distribution g is \"encoded\" with L and B, and in general we would like to have a compression scheme of a small size.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Remark 3.3", "text": "In the definition above we required the probability of existence of L and B to be at least 2/3, but one can boost this probability to 1 \u2212 \u03b4 by generating a sample of size m(\u03b5) log(1/\u03b4).\nNext we show that if a class of distributions can be compressed, then it can be learned; thus we build the connection between compression and learning. We will need the following useful result about PAC-learning of finite classes of distributions, which immediately follows from [7,Theorem 6.3]  Theorem 3.4 ( [7]) There exists a deterministic algorithm that, given candidate distributions f 1 , . . . , f M , a parameter \u03b5 > 0, and log(3M 2 /\u03b4)/2\u03b5 2 i.i.d. samples from an unknown distribution g, outputs an index j \u2208 [M ] such that\nf j \u2212 g 1 \u2264 3 min i\u2208[M ] f i \u2212 g 1 + 4\u03b5, with probability at least 1 \u2212 \u03b4/3.\nThe proof of the following theorem appears in the full version [2]. Theorem 3.5 (compressibility implies learnability) Suppose F admits (\u03c4, t, m) compression. Let \u03c4 (\u03b5) := \u03c4 (\u03b5/6) + t(\u03b5/6). Then F can be learned using\nO m \u03b5 6 log 1 \u03b4 + \u03c4 (\u03b5) log(m( \u03b5 6 ) log(1/\u03b4)) + log(1/\u03b4) \u03b5 2 = O m \u03b5 6 + \u03c4 (\u03b5) \u03b5 2 samples.\nWe next prove two closure properties of compression schemes. First, Lemma 3.6 below states that if a class F of distributions can be compressed, then the class of distributions that are formed by taking products of members of F can also be compressed. If p 1 , . . . , p d are distributions over domains Z 1 , . . . , Z d , then\nd i=1 p i denotes the standard product distribution over d i=1 Z i . For a class F of distributions, define F d := d i=1 p i : p 1 , . . . , p d \u2208 F . Lemma 3.6 (compressing product distributions) If F admits (\u03c4 (\u03b5), t(\u03b5), m(\u03b5)) compression, then F d admits (d\u03c4 (\u03b5/d), dt(\u03b5/d), m(\u03b5/d) log(3d)) compression.\nOur next lemma states that if a class F of distributions can be compressed, then the class of distributions that are formed by taking mixtures of members of F can also be compressed. ", "publication_ref": ["b6", "b6", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Upper bound: learning mixtures of Gaussians by compression schemes", "text": "In this section we prove an upper bound of O(kd 2 /\u03b5 2 ) for the sample complexity of learning mixtures of k Gaussians in d dimensions, and an upper bound of O(kd/\u03b5 2 ) for the sample complexity of learning mixtures of k axis-aligned Gaussians. The heart of the proof is to show that Gaussians have compression schemes in any dimension. Remark 4.2 In the special case d = 1, there also exists a (2, 0, O(1/\u03b5)) (i.e. constant size) compression scheme: if we draw C/\u03b5 samples from N (\u00b5, \u03c3 2 ), for a sufficiently large constant C, with probability at least 2/3 there exist two points in the sample such that one of them is within distance \u03c3\u03b5/2 of \u00b5 \u2212 \u03c3 and the other one is within distance \u03c3\u03b5/2 of \u00b5 + \u03c3. Given these two points, the decoder can estimate \u00b5 and \u03c3 up to additive precision \u03b5\u03c3/2, which results in an \u03b5-approximation of N (\u00b5, \u03c3 2 ) in total variation distance. Remarkably, this compression scheme has constant size, as the value of \u03c4 + t is independent of \u03b5 (unlike Lemma 4.1). This scheme can be used instead of Lemma 4.1 in the proof of Theorem 1.3, although it would not improve the sample complexity bound asymptotically. the sample complexity of learning a class (of concepts, functions, or distributions) is typically proportional to (some notion of) intrinsic dimension of the class divided by \u03b5 2 , where \u03b5 is the error tolerance. For the case of agnostic binary classification, the intrinsic dimension is captured by the VC-dimension of the concept class (see [21,4]). For the case of distribution learning with respect to 'natural' parametric classes, we expect this dimension to be equal to the number of parameters. This is indeed true for the class of Gaussians (which have d 2 parameters) and axis-aligned Gaussians (which have d parameters), and we showed in this paper that it holds for their mixtures as well (which have kd 2 and kd parameters, respectively).\nIn binary classification, the combinatorial notion of Littlestone-Warmuth compression has been shown to be sufficient [15] and necessary [18] for learning. In this work, we showed that the new but related notion of distribution compression is sufficient for distribution learning. Whether the existence of compression schemes is necessary for learning an arbitrary class of distributions remains an intriguing open problem.\nIt is worth mentioning that while it may first seem that the VC-dimension of the Yatracos set associated with a class of distributions can characterize its sample complexity, it is not hard to come up with examples where this VC-dimension is infinite while the class can be learned with finite samples. Covering numbers do not characterize the sample complexity either: for instance the class of Gaussians does not have a finite covering number in the TV metric, nevertheless it is learnable with finitely many samples.\nA concept related to compression is that of core-sets. In a sense, core-sets can be viewed as a special case of compression, where the decoder is required to be the empirical error minimizer. See [17] for using core-sets in maximum likelihood estimation.", "publication_ref": ["b21", "b3", "b15", "b18", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We thank Yaoliang Yu for pointing out a mistake in an earlier version of this paper, and Luc Devroye for fruitful discussions. Abbas Mehrabian was supported by a CRM-ISM postdoctoral fellowship and an IVADO-Apog\u00e9e-CFREF postdoctoral fellowship. Nicholas Harvey was supported by an NSERC Discovery Grant. Christopher Liaw was supported by an NSERC graduate award. Yaniv Plan was supported by NSERC grant 22R23068.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Addendum", "text": "The lower bound of Theorem 1.2 was recently improved in a subsequent work [8] from \u2126(kd 2 /\u03b5 2 log 3 (1/\u03b5)) to \u2126(kd 2 /\u03b5 2 log(1/\u03b5)) using a different construction.", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Let N (\u00b5, \u03a3) denote the target distribution, which we are to encode.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Remark 4.3", "text": "The case of rank-deficient \u03a3 can easily be reduced to the case of full-rank \u03a3. If the rank of \u03a3 is r < d, then any X \u223c N (\u00b5, \u03a3) lies in some affine subspace S of dimension r. With high probability, the first d samples from N (\u00b5, \u03a3) uniquely identify S. We encode S using these samples, and for the rest of the process we work in this affine subspace. Hence, we may assume \u03a3 has full rank d.\nTo prove Lemma 4.1, we will need the following result from the random matrix theory literature [cf.  \nNote that the lemma can be improved to require only m \u2265 Cd samples [see 16, Corollary 4.1], but this would not improve our final bound.\nThe remainder of the proof amounts to showing that with only a small number of additional bits, we can approximate the mean and each eigenvector of the covariance matrix as a linear combination of a subset of the drawn samples.\nNote that both \u03a3 and \u03a8 are positive definite, and that \u03a3 = \u03a8 2 . Moreover, it is easy to see that\nLemma 4.5 Let C > 0 be a sufficiently large constant. Given m = 2Cd(1 + log d) samples S from N (\u00b5, \u03a3), with probability at least 2/3, one can encode vectors v 1 , . . . , v d , \u00b5 \u2208 R d satisfying\nand\n)) bits and the points in S.\nLemma 4.1 now follows immediately from the following lemma\nwhere the v i are orthogonal and \u03a3 is full rank, and that \u03a8 \u22121 ( \u00b5 \u2212 \u00b5) \u2264 \u03b6, and that", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Discussion", "text": "A central open problem in distribution learning and density estimation is characterizing the sample complexity of learning a distribution class. An insight from supervised learning theory is that", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Neural network learning: theoretical foundations", "journal": "Cambridge University Press", "year": "1999", "authors": "Martin Anthony; Peter Bartlett"}, {"ref_id": "b1", "title": "Near-optimal sample complexity bounds for robust learning of Gaussians mixtures via compression schemes", "journal": "", "year": "", "authors": "Hassan Ashtiani; Shai Ben-David; Nicholas J A Harvey; Christopher Liaw; Abbas Mehrabian; Yaniv Plan"}, {"ref_id": "b2", "title": "Sample-efficient learning of mixtures", "journal": "AAAI Publications", "year": "2018", "authors": "Hassan Ashtiani; Shai Ben-David; Abbas Mehrabian"}, {"ref_id": "b3", "title": "Learnability and the Vapnik-Chervonenkis dimension", "journal": "J. ACM", "year": "1989-10", "authors": "Anselm Blumer; Andrzej Ehrenfeucht; David Haussler; Manfred K Warmuth"}, {"ref_id": "b4", "title": "Efficient density estimation via piecewise polynomial approximation", "journal": "ACM", "year": "2014", "authors": "Ilias Siu-On Chan; Rocco A Diakonikolas; Xiaorui Servedio;  Sun"}, {"ref_id": "b5", "title": "A course in density estimation", "journal": "Progress in Probability and Statistics", "year": "1987", "authors": "Luc Devroye"}, {"ref_id": "b6", "title": "Combinatorial methods in density estimation", "journal": "Springer-Verlag", "year": "2001", "authors": "Luc Devroye; G\u00e1bor Lugosi"}, {"ref_id": "b7", "title": "The minimax learning rate of normal and Ising undirected graphical models", "journal": "", "year": "2018", "authors": "Luc Devroye; Abbas Mehrabian; Tommy Reddad"}, {"ref_id": "b8", "title": "Learning Structured Distributions", "journal": "Chapman and Hall/CRC", "year": "2016", "authors": "Ilias Diakonikolas"}, {"ref_id": "b9", "title": "Statistical query lower bounds for robust estimation of high-dimensional Gaussians and Gaussian mixtures", "journal": "", "year": "2017-10", "authors": "Ilias Diakonikolas; Daniel M Kane; Alistair Stewart"}, {"ref_id": "b10", "title": "Learning multivariate log-concave distributions", "journal": "", "year": "2017", "authors": "Ilias Diakonikolas; Daniel M Kane; Alistair Stewart"}, {"ref_id": "b11", "title": "Estimation of analytic functions", "journal": "", "year": "1999", "authors": "Ildar Ibragimov"}, {"ref_id": "b12", "title": "", "journal": "Inst. Math. Statist", "year": "2001", "authors": ""}, {"ref_id": "b13", "title": "Disentangling Gaussians", "journal": "Communications of the ACM", "year": "2012", "authors": "Adam Kalai; Ankur Moitra; Gregory Valiant"}, {"ref_id": "b14", "title": "On the learnability of discrete distributions", "journal": "ACM", "year": "1994", "authors": "Michael Kearns; Yishay Mansour; Dana Ron; Ronitt Rubinfeld; Robert E Schapire; Linda Sellie"}, {"ref_id": "b15", "title": "Relating data compression and learnability", "journal": "", "year": "1986", "authors": "Nick Littlestone; Manfred Warmuth"}, {"ref_id": "b16", "title": "Smallest singular value of random matrices and geometry of random polytopes", "journal": "Adv. Math", "year": "2005", "authors": "Alexander E Litvak; Alain Pajor; Mark Rudelson; Nicole Tomczak-Jaegermann"}, {"ref_id": "b17", "title": "Training Gaussian mixture models at scale via coresets", "journal": "Journal of Machine Learning Research", "year": "2018", "authors": "Mario Lucic; Matthew Faulkner; Andreas Krause; Dan Feldman"}, {"ref_id": "b18", "title": "Sample compression schemes for VC classes", "journal": "Journal of the ACM (JACM)", "year": "2016", "authors": "Shay Moran; Amir Yehudayoff"}, {"ref_id": "b19", "title": "Density estimation for statistics and data analysis. Monographs on Statistics and Applied Probability", "journal": "Chapman & Hall", "year": "1986", "authors": "W Bernard;  Silverman"}, {"ref_id": "b20", "title": "Near-optimal-sample estimators for spherical gaussian mixtures", "journal": "Curran Associates, Inc", "year": "2014", "authors": "Alon Ananda Theertha Suresh; Jayadev Orlitsky; Ashkan Acharya;  Jafarpour"}, {"ref_id": "b21", "title": "On the uniform convergence of relative frequencies of events to their probabilities", "journal": "", "year": "1971", "authors": "N Vladimir; Alexey Vapnik;  Ya;  Chervonenkis"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "and a standard Chernoff bound. It states that a finite class of size M can be learned using O(log(M/\u03b4)/\u03b5 2 ) samples. Denote by [M ] the set {1, 2, ..., M }. Throughout the paper, a/bc always means a/(bc).", "figure_data": ""}, {"figure_label": "37", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Lemma 3 . 7 (37compressing mixtures) If F admits (\u03c4 (\u03b5), t(\u03b5), m(\u03b5)) compression, then k-mix(F) admits (k\u03c4 (\u03b5/3), kt(\u03b5/3) + k log 2 (4k/\u03b5)), 48m(\u03b5/3)k log(6k)/\u03b5) compression.", "figure_data": ""}, {"figure_label": "41", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Lemma 4 . 141For any positive integer d, the class of d-dimensional Gaussians admits an O(d log(2d)), O(d 2 log(2d) log(d/\u03b5)), O(d log(2d)) compression scheme.", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Proof of Theorem 1 . 1 .11Combining Lemma 4.1 and Lemma 3.7 implies that the class of k-mixtures of d-dimensional Gaussians admits an O(kd log(2d)), O(kd 2 log(2d) log(d/\u03b5) + k log(k/\u03b5)), O(dk log k log(2d)/\u03b5)", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "4 d 2 + k 3 d 3 )/\u03b5 2 )", "formula_coordinates": [2.0, 261.43, 545.05, 67.9, 10.31]}, {"formula_id": "formula_1", "formula_text": "result in loose upper bounds of O(k 4 d 4 /\u03b5 2 ) and O((k 4 d 2 + k 3 d 3 )/\u03b5 2 ), respectively.", "formula_coordinates": [3.0, 108.0, 533.0, 395.99, 19.55]}, {"formula_id": "formula_2", "formula_text": "Proposition 1.4 Set \u03b5 = 2 \u03ba(\u03a3)+1", "formula_coordinates": [4.0, 108.0, 319.13, 134.79, 13.47]}, {"formula_id": "formula_3", "formula_text": "|\u03a3 i,j \u2212\u03a3 i,j | \u2264 \u03b5 and\u03a3 i,j \u2208 [1, 1 + 2\u03b5] \u2022 \u03a3 i,j \u2200i, j,", "formula_coordinates": [4.0, 176.97, 350.8, 258.05, 9.65]}, {"formula_id": "formula_4", "formula_text": "KL N (0, \u03a3) N (0,\u03a3) = \u221e and TV N (0, \u03a3), N (0,\u03a3) = 1.", "formula_coordinates": [4.0, 152.22, 385.65, 307.56, 8.74]}, {"formula_id": "formula_5", "formula_text": "TV N (0, \u03a3), N (0,\u03a3) \u2264 \u03b5, but there exist i, j such that, for any c \u2265 1,\u03a3 i,j \u2208 [1/c, c] \u2022 \u03a3 i,j .", "formula_coordinates": [4.0, 107.64, 499.76, 380.65, 9.65]}, {"formula_id": "formula_6", "formula_text": "TV (f 1 , f 2 ) := sup B\u2208L B f 1 (x) \u2212 f 2 (x) dx = 1 2 f 1 \u2212 f 2 1 ,", "formula_coordinates": [5.0, 179.2, 673.64, 253.6, 23.97]}, {"formula_id": "formula_7", "formula_text": "f 1 := R d |f (x)|dx is the L 1 norm of f . The Kullback-Leibler (KL) divergence between f 1 and f 2 is defined by KL (f 1 f 2 ) := R d f 1 (x) log f 1 (x) f 2 (x) dx.", "formula_coordinates": [6.0, 108.0, 74.81, 396.3, 46.24]}, {"formula_id": "formula_8", "formula_text": "Definition 2.1 (\u03b5-approximation) A distribution\u011d is an \u03b5-approximation for g if \u011d \u2212 g 1 \u2264 \u03b5.", "formula_coordinates": [6.0, 108.0, 155.64, 384.13, 9.72]}, {"formula_id": "formula_9", "formula_text": "Let \u2206 n := { (w 1 , . . . , w n ) : w i \u2265 0, w i = 1 } denote the n-dimensional simplex. Definition 2.3 (k-mix(F))", "formula_coordinates": [6.0, 108.0, 229.81, 341.23, 30.09]}, {"formula_id": "formula_10", "formula_text": "k-mix(F) := k i=1 w i f i : (w 1 , . . . , w k ) \u2208 \u2206 k , f 1 , . . . , f k \u2208 F .", "formula_coordinates": [6.0, 166.19, 279.86, 279.62, 14.11]}, {"formula_id": "formula_11", "formula_text": "\u221e n=0 Z n \u00d7 \u221e n=0{0", "formula_coordinates": [6.0, 116.3, 463.09, 73.97, 14.11]}, {"formula_id": "formula_12", "formula_text": "f j \u2212 g 1 \u2264 3 min i\u2208[M ] f i \u2212 g 1 + 4\u03b5, with probability at least 1 \u2212 \u03b4/3.", "formula_coordinates": [7.0, 108.0, 158.05, 271.95, 33.81]}, {"formula_id": "formula_13", "formula_text": "O m \u03b5 6 log 1 \u03b4 + \u03c4 (\u03b5) log(m( \u03b5 6 ) log(1/\u03b4)) + log(1/\u03b4) \u03b5 2 = O m \u03b5 6 + \u03c4 (\u03b5) \u03b5 2 samples.", "formula_coordinates": [7.0, 109.92, 251.84, 392.15, 24.77]}, {"formula_id": "formula_14", "formula_text": "d i=1 p i denotes the standard product distribution over d i=1 Z i . For a class F of distributions, define F d := d i=1 p i : p 1 , . . . , p d \u2208 F . Lemma 3.6 (compressing product distributions) If F admits (\u03c4 (\u03b5), t(\u03b5), m(\u03b5)) compression, then F d admits (d\u03c4 (\u03b5/d), dt(\u03b5/d), m(\u03b5/d) log(3d)) compression.", "formula_coordinates": [7.0, 108.0, 322.57, 397.75, 64.5]}], "doi": "10.1145/76359.76371"}