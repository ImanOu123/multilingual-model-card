{"title": "WikiBio: a Semantic Resource for the Intersectional Analysis of Biographical Events", "authors": "Marco Antonio Stranisci; Rossana Damiano; Enrico Mensa; Viviana Patti; Daniele Paolo Radicioni; Tommaso Caselli", "pub_date": "", "abstract": "Biographical event detection is a relevant task for the exploration and comparison of the ways in which people's lives are told and represented. In this sense, it may support several applications in digital humanities and in works aimed at exploring bias about minoritized groups. Despite that, there are no corpora and models specifically designed for this task. In this paper we fill this gap by presenting a new corpus annotated for biographical event detection. The corpus, which includes 20 Wikipedia biographies, was compared with five existing corpora to train a model for the biographical event detection task. The model was able to detect all mentions of the target-entity in a biography with an F-score of 0.808 and the entity-related events with an F-score of 0.859. Finally, the model was used for performing an analysis of biases about women and non-Western people in Wikipedia biographies.", "sections": [{"heading": "Introduction", "text": "Detecting biographical events from unstructured data is a relevant task to explore and compare bias in representations of individuals. In recent years, the interest in this topic has been favored by studies about social biases on allegedly objective public archives such as Wikipedia. Sun and Peng (2021) developed a resource for investigating gender bias on Wikipedia biographies showing that personal life events tend to be more frequent in female career sections than in those of men. Lucy et al. (2022) developed BERT-based contextualized embeddings for exploring representations of women on Wikipedia and Reddit.\nThe detection of biographical events has been addressed with complementary approaches by different research communities. Projects in Digital Humanities have focused mostly on representational aspects, delivering ontologies and knowledge graphs for the collection and study of biographical events (Tuominen et al., 2018;Fokkens et al., 2017;Plum et al., 2019;Krieger, 2014). When it comes to NLP, the focus has been mainly on developing models for the detection and classification of events Gottschalk and Demidova, 2018). Few are the works that directly target biographies and focus on identifying biographical events with varied approaches (supervised and unsupervised) across different datasets (e.g., Wikipedia vs. newspaper articles), making their comparison impossible (Bamman and Smith, 2014;Russo et al., 2015;Menini et al., 2017). Although not directly targeting biographies, some works focused on the identification of entity-related sequences of events (Chambers and Jurafsky, 2008) and entity-based storylines (Chambers and Jurafsky, 2009;Minard et al., 2015;.\nDespite the above mentioned variety of approaches to biographical event detection, there are pending and urgent issues to be addressed, which limit a full development of the research area. In particular, we have identified three critical issues: i) the lack of a benchmark annotated corpus for evaluating biographical event detection; ii) the lack of models specifically designed for detecting and extracting biographical events; and finally iii) the lack of a systematic study of the potential representation bias of minority groups, non-Western people, and younger generations in biography repositories publicly available, such as Wikipedia (D'ignazio and Klein, 2020). Contributions Our work addresses these issues by presenting a novel benchmark corpus, a BERTbased model for biographical event detection, and an analysis of 48, 789 Wikipedia biographies of writers born since 1808. Our results show that existing data sets annotated for event detection may be easily re-used to detect biographical events achieving good results in terms of F-measure. The analysis of the 48, 789 biographies from Wikipedia extends the findings from previous work indicating that representational biases are present in an allegedly objective source such as Wikipedia along intersectional axes (Crenshaw, 2017), namely ethnicity and gender.\nThe rest of the paper is organized as follows. In Section 2, we present WikiBio, a novel manually annotated corpus of biographical events. Section 3 presents the experiments in event detection and coreference resolution of the target entities of a biographies. Section 4 is devoted to the analysis of the biases in Wikipedia biographies. Conclusions and future work end the paper in Section 5.\nCode and WikiBio corpus are available at the following url: https://github.com/ marcostranisci/WikiBio/.", "publication_ref": ["b33", "b18", "b34", "b11", "b25", "b17", "b13", "b1", "b29", "b20", "b5", "b6", "b23", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "The WikiBio Corpus", "text": "WikiBio is a corpus annotated for biographical event detection, composed of 20 Wikipedia biographies. The corpus includes all the events which are associated with the entity target of the biography.\nIn this section, we present our annotation scheme, discuss the agreement scores and present some cases of disagreement. Lastly, we present the results of our annotation effort, and compare them with existing corpora annotated for event detection and coreference resolution.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Annotation Tasks", "text": "Since the biographical event detection task consists in annotating all events related to the person who is the subject of a biography, annotation guidelines focus on two separate subtasks: (i) the identification of all the mentions of the target entity and the resolution of its coreference chains; and (ii) the identification and linking of all the events that involve the target entity.\nEntity annotation. The entity annotation subtask requires the identification of all mentions of a specific Named Entity (NE) (Grishman and Sundheim, 1996) of type Person, which is the target of the biography and all its coreferences (Deemter and Kibble, 2000) within the Wikipedia biography. For the modeling of this subtask, we used the GUM corpus (Zeldes, 2017), introducing different guidelines about the following aspects: i) only the mentions of the entity-target of the biography must be annotated; ii) mentions of the target entity must be selected only when they have a role in the event (Example 1, where the possessives \"his\" is not annotated); and iii) indirect mentions of the target entity must be annotated only if they are related to biographical events (Examples 2 and 3).\n1. Kenule Saro-Wiwa was born in Bori [...] His father's hometown was the village of Bane, Ogoniland.\n2. He married Wendy Bruce, whom he had known since they were teenagers.\n3. In 1985, the Biafran Civil War novel Sozaboy was published.\nEvent Annotation. Although there is an intuitive understanding of how to identify event descriptions in natural language texts, there is quite a large variability in their realizations (Pustejovsky et al., 2003b). Araki et al. (2018) point out that some linguistic categories, e.g., nouns, fits on an event continuum. This makes the identification of event mentions a non trivial task. Our event annotation task mainly relies on TimeML (Pustejovsky et al., 2003a) and RED (O'Gorman et al., 2016), where 'event' is \"a cover term for situations that happen or occur.\" (Pustejovsky et al., 2003a) Events are annotated at a single token level with no restrictions on the parts of speech that realize the event. Following Bonial and Palmer (2016), we introduced a special tag (LINK) for marking a limited set of light and copular verbs, as illustrated in Example 4. The adoption of LINK is aimed at increasing the compatibility of the annotated corpus with OntoNotes, the resource with the highest number of annotated events.  Corpus Annotation and IAA. The annotation task was performed by three expert annotators (two men and one woman -all authors of the paper), near-native speakers of British English, having a long experience in annotating data for the specific task (event and entity detection). One annotator (A0) was in charge of preparing the data by discarding all non-relevant sentences to speed-up the annotation process. This resulted in a final set of 1, 691 sentences containing at least one mention of a target entity. The entity and event annotations were conducted as follows: A0 annotated the entire relevant sentences, while a subset of 400 sentences was annotated by A1 and A2, who respectively labeled 200 sentences each. We report pair-wise Inter-Annotator Agreement (IAA) using Cohen's kappa in Table 1. In general, there is a fair agreement across all the annotation layers. At the same time, we observe a peculiar behavior across the annotators: there is a higher agreement between A0 and A2 for the event and entity layers when compared to A0 and A1, but the opposite occurs with the relations layers (LINK and CONT_MOD). For the events, the higher disagreement is due to nominal events, often misinterpreted as not bearing an eventive meaning. For instance, the noun \"trip\" in example 6 was not annotated by A1. For the entities, we observed that disagreement is due to two reasons. The first is the consequence of a disagreement in the event annotations. Whenever annotators disagree on the identification of an event, they also disagree on the annotation of the related entity mention, as in the case of the pronoun 'his' in example 6. Another reason of disagreement regards indirect mentions. Annotators often disagree on annotation spans, as in \"Biafran Civil War novel Sozaboy was published\" where A1 selected 'SozaBoy', while A2 'novel Sozaboy'. When it comes to LINK, problems are mainly due to the identification of light verbs. Despite the decision of considering only a close set of copular and light verbs to be marked as LINK (Cfr Bonial and Palmer ( 2016)), annotators used this label for other verbs, such as 'begin' or 'hold'. 7. Walker began to take up reading and writing.", "publication_ref": ["b14", "b9", "b37", "b27", "b0", "b26", "b24", "b26", "b3"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "WikiBio: Overview and Comparison with Other Resources", "text": "The WikiBio corpus is composed of 20 biographies of African, and African-American writers extracted from Wikipedia for a total amount of 2, 720 sentences. Among them, only 1, 691 sentences include at least one event related to the entity target of the biography. More specifically, there are 3, 290 annotated events, 2, 985 mentions of a target entity, 343 LINK tags, and 75 CONT_MOD links.\nCorpora size and genres We compare WikiBio against five relevant existing corpora that, in principle, could be used to train models for biographical event detection: GUM (Zeldes, 2017), Litbank (Sims et al., 2019), Newsreader , OntoNotes (Hovy et al., 2006), and Time-Bank (Pustejovsky et al., 2003b). For each corpus, we took into account the number of relevant annotations and the types of texts. As it can be observed in Table 2, corpora vary in size and genres.\nOntoNotes is the biggest one and includes 159, 938 events, and 22, 234 entity mentions. The smaller is NewsReader, with only 594 annotated events. TimeBank and LitBank are similar in scope, since they both include about 7.5K events, while GUM includes 9, 762 entity mentions.  biographies such as news, literary works, and transcription of TV news. To get a high-level picture of the potential similarities and differences in terms of probability distributions, we calculated the Jensen-Shannon Divergence (Men\u00e9ndez et al., 1997). Such metric may be useful for identifying which corpora are most similar to WikiBio. The results show that WikiBio converges more with GUM (0.43), OntoNotes (0.48) and LitBank (0.49) rather than with TimeBank (0.51) and Newsreader (0.54). Such differences have driven the selection of data for the training set described in Section 3.2.", "publication_ref": ["b37", "b31", "b15", "b27", "b19"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Annotations of entities, events, and coreference", "text": "The distribution of the target entity within biographies in the WikiBio corpus has been compared with two annotated corpora for coreference resolution and named entity recognition: OntoNotes (Hovy et al., 2006) and GUM (Zeldes, 2017). Since such corpora were developed for identifying the coreferences of all NEs in a document, we modified annotations to keep only the most frequent NEs of type 'person' in each document. The rationale was making these resources comparable with WikiBio, which includes only the coreferences to a single entity, namely the subject of each biography.\nAfter doing that, we computed the ratio between the number of tokens that mention the target entity and the total number of tokens, and the ratio between the number of sentences where the target entity is mentioned against the total number of sentences. While this operation did not impact on GUM, in which 174 out of 175 documents contain mentions of people, it had an important impact on OntoNotes, in which 1, 094 documents (40%) do not mention entities of the type Person. Tokens mentioning the target entity are 5% on OntoNotes, 8.7% on GUM and 4% on WikiBio. Such differences can be explained by the average length of documents in these corpora, which is of 388 tokens in OntoNotes, 978 in GUM, and 3, 754 in WikiBio. As a matter of fact, if the percentage of sentences mentioning the target-entity is considered instead of the total number of tokens, WikiBio shows an higher ratio (61.7%) of sentences mentioning the target entity, than OntoNotes (20.8%) and GUM (42.6%).\nThe three most frequently occurring lemmas in the WikiBio corpus seem to be strongly related to the considered domain: 'write' represents 3.2% of the total, 'publish' 2.9%, and 'work' 1.8%. 'Return' (1.3%) appears to have a more general scope, since it highlights a movement of the target entity from a place to another. The comparison with other corpora annotated for event detection shows differences concerning the most frequent events. The top three in OntoNotes (Bonial et al., 2010) are three light verbs: 'be', 'have', and 'do'. This may be intrinsically linked to its annotation scheme which considers all verbs as candidates for being events, including semantically empty ones (Section 2.1). NewsReader  and TimeBank (Pustejovsky et al., 2003b) include two verbs expressing reporting actions among the top five, thus revealing that they are corpora of annotated news. Litbank (Sims et al., 2019), which is a corpus of 100 annotated novels, includes in its top-ranked events two visual perception verbs and two verbs of movement, which may reveal the centrality of characters in this documents. The event 'say' is top-ranked in all the five corpora.", "publication_ref": ["b15", "b37", "b2", "b27", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "Detecting Biographical Events", "text": "In this section we describe a series of experiments for the detection of biographical events. Experiments involve the use of the existing annotated corpora for two tasks: entity mentions detection  (Section 3.1) and event detection (Section 3.2). In both cases we used a 66 million parameters Distil-Bert model (Sanh et al., 2019). In this setting the WikiBio corpus is both used as part of the training set and as a benchmark for testing how well existing annotated corpora may be used for the task. For such experiments a NVIDIA RTX 3030 ti was used.\nThe average length of each fine-tuning session was 40 minutes.", "publication_ref": ["b30"], "figure_ref": [], "table_ref": []}, {"heading": "Entity Detection", "text": "For this task we adapted the annotations in OntoNotes (Hovy et al., 2006) and GUM (Zeldes, 2017) keeping only mentions of the most frequent entities of type 'person'. As a result we obtained 870 documents from OntoNotes, 174 from GUM. The WikiBio corpus was split into three subsets: five documents for the development, 10 for the test, and five for the training. Given the imbalance between the existing resources and WikiBio, we always trained the model with a fixed number of 100 documents, in order to reduce the overfitting of the model over the other datasets.\nExperiments consist in training a DistilBert model for identifying all the tokens mentioning the target entity of a given model and were performed on six different training sets. Since the focus of our work is to develop a model for detecting biographical events, WikiBio was used as development set for better monitoring its degree of compatibility with existing corpora. Following the approach by Joshi et al. (2020), we split each document into sequences of 128 tokens, and for each document we created one batch of variable length containing all the sequences. Table 4 shows the results of these experiments. As it can be observed, including the WikiBio corpus in the training set did not result in an increase of the performance of the model. This may be due to the low number of WikiBio documents in the training.The highest performance was obtained in two experiments: one using a training set only composed of documents from OntoNotes, which obtained a F-score of 0.808, and one with a miscellaneous of 50 OntoNotes and 50 GUM documents, that obtained 0.792. To understand if the difference between the two experiments is significant, we performed a One-Way ANOVA test over the train, development, and test F-scores obtained in both experiments. The test returned a p-value of 0.44, which confirms a significant difference between the two results", "publication_ref": ["b15", "b37", "b16"], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Event Detection", "text": "Event Detection experiments were guided by the comparison between WikiBio and the resources for event detection described in Section 2.2. Since OntoNotes was annotated according to the Prop-Bank guidelines (Bonial et al., 2010), which only consider verbs as candidates for such annotation, we partly modified its annotations before running the experiments. We first adapted the OntoNotes semantic annotation by replacing light and copular verbs (Bonial and Palmer, 2016) with nominal (Meyers et al., 2004) and adjectival events. Then we ran a battery of experiments by fine-tuning a DistilBert-based model using each dataset for training, and a series of miscellaneous of the most similar corpora to WikiBio according to the Jensen-Shannon Divergence metric (Table 3). Since we were concerned with both assessing the effectiveness of WikiBio for training purposes and testing how far biographic events can be extracted, we designed our training and testing data as follows. WikiBio was employed in different learning phases: in devising the training set (i.e., existing resources were employed either alone or mixed with Wik-iBio); additionally, the development set was always built by starting from WikiBio sentences. Finally, we always tested on WikiBio data.\nAs for the entity-detection experiments, the 1, 691 sentences containing events annotated in the WikiBio corpus were split into three sets of equal size that were used for training ( 564  between OntoNotes and other corpora, we sampled a number of sentences for training which did not exceeded 5, 073, namely three times the number of sentences annotated in our corpus. Such length was fixed also for miscellaneous training sets. Experiments were organized in two sessions. In the first session we fine-tuned a DistilBert model for five epochs, using as training set the five corpora presented in Section 2.2 individually as well as three combinations of them: i) misc_01, a miscellaneous of sentences extracted on equal size from all corpora; ii) misc_02, in which sentences from NewsReader, the most different corpus with Wik-iBio (Table 3), were removed; iii) misc_03, a combination of sentences from OntoNotes and Litbank, namely the two most similar corpora with Wik-iBio. The model was fine-tuned on these training sets both with and without a subset of the WikiBio corpus for a total of 16 different training sets. In addition, we also fine-tuned and tested WikiBio alone. We then continued the fine-tuning only for the models which obtained the best F-scores.\nObserving Table 5, it emerges that, differently from entity-detection experiments, including a subset of WikiBio in the training set, even if in a small percentage, always improves the results of the classifier. This especially happens for Litbank (+0.191 F-Score), and TimeBank (+0.031 F-Score).\nWhen looking at results of finetuning for single corpora, it emerges that the model trained on the modified version of OntoNotes and TimeBank obtains the best scores. Such results are interesting for two reasons. They confirm the intuition that OntoNotes annotations may be easily modified to account for nominal and adjectival events. They also confirm the high compatibility of WikiBio and TimeBank guidelines (Sect. 2.1). Even if the latter is more divergent from WikiBio than other corpora, it seems to be compatible with it. As expected for its limited size and high divergence with WikiBio, the training set based on NewReader sentences ob-tains the worst results, with an F-Score below 0.5.\nResults of miscellaneus training sets are interesting as well: they generally result in models with better performance, and they seem to work better on the basis of their divergence with WikiBio. Trained on misc_01, a combination of all corpora, the model scores 0.827, which is below the result obtained with the modified version of OntoNotes. If Newsreader is removed, the model obtains 0.831, and 0.832 if also TimeBank is removed. It is also worth mentioning the delta between the F-score on the training and the test sets, which is \u22120.054 for misc_01, \u22120.029 for misc_02, and \u22120.013 for misc_03.\nAfter the first fine-tuning step, we performed a One-Way ANOVA for testing the significance of differences between experiments. Analyzed in such a way, the four best-ranked models never showed a p-value below 0.5, which means that there are no significant differences between them. Thereby, we kept them for the second fine-tuning step that consists on training the model for 15 epochs on these datasets. Absolute results (Table 5) show that the model trained on Timebank obtained the best F-Score. However, as for the entity detection experiments, we considered the deltas between the training and test F-scores to select the best model for our analysis. All models acquired by employing a miscellaneous training set obtained a lower delta between training and test, and scored a similar F-Score.", "publication_ref": ["b2", "b3", "b21"], "figure_ref": [], "table_ref": ["tab_5", "tab_5", "tab_9", "tab_9"]}, {"heading": "An Intersectional Analysis of Wikipedia Biographies", "text": "In this section we provide an analysis of writers' biographies on Wikipedia adopting intersectionality as a theoretical framework and the model described in Section 3 as a tool for detecting biographical events.   2017) has been developed in the context of gender and black studies to account inequalities that cannot be explained without a joint analysis of socio-demographic factors. For instance, African American women workers suffer higher discrimination than their male counterpart, as Crenshaw (1989) observed in her seminal work. Therefore, the injection of different socio-demographic features for the analysis of discriminations may unfold hidden forms of inequities about certain segments of population. We adopt this framework to analyse how the representations of non-Western women writers on Wikipedia differs from those of Western Women, Transnational Men, and Western Men.\nFor this analysis, we gathered 48, 486 Wikipedia biographies of writers born since 1808. We define as Transnational all the writers born outside Western countries and people who belong to ethnic minorities (Boter et al., 2020;Stranisci et al., 2022). Western men's biographies are 28, 036, Western women's 12, 413, Transnational men's 5, 471, and Transnational women's 2, 470. Information about occupation, gender, year of birth, ethnic group, and country of birth was obtained from Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014), which has been used for filtering and classifying biographies.\nFor each biography, we first identified all the mentions of the corresponding target entity (Section 3.1). We then removed the sentences that do not contain a mention of the entity. This reduced the number of sentences to be annotated for event detection from 1, 486, 320 to 1, 163, 475 (\u221221.8%). As a final step, we annotated events (Section 3.2) in the filtered sentences.\nTable 6 shows the distribution of biographical events about men, women, Western, and Transnational people. The vast majority of events are about Western men (62.2%), while at the opposite side of the spectrum there are Transnational women writers, whose representation is below 5%. Ethnicity is a cause of underrepresentation more than gender: events about Transnational men are only 11.2% of the total, while those about Western women 21.4%. The average number of events per-author shows a richness in the description of Transnational Women (50.92 events) against Western ones (43.73 events).\nThe analysis of event types presents a similar distribution. 27, 885 event types -intended as the number of unique tokens that occur in each distribution -are detected in Western men's biographies (44.9 per biography), while only 9, 254 in Transnational women's biographies (40.4 per biography). However, the overlap of event types between these two categories is very large (92.6%) The same comparison, conducted on the other groups, reveals a higher number of group-specific event types: 87.8% of event types about Transnational Men are shared with Western Men, and the rate is lower for Western Women (84.1%).\nA comparative analysis of most distinctive events per category of people provides additional insight about the representation of women and Transnational writers in Wikipedia biographies. In order to do so, we first computed the average frequency of each event in all biographies of the four groups of writers in Table 6. We then compared these distributions with the Shifterator library (Gallagher et al., 2021), which allows computing and plotting pairwise comparisons between different distribution of texts with different metrics. Coherently with the analysis performed in previous sections, we chose the Jensen-Shannon Divergence metric, and analyzed the distribution of events about Transnational Women against Transnational Men, Western Men, and Western Women. Table 7 shows the most diverging events between Transnational and Western writers, while Table 8 shows the 20 events about Transnational women that diverge most with other distributions: Transnational men, Western men, and Western women. Events are ordered on the basis of how much they are specific to the distribution of Transnational women. In Appendix A graphs with comparisons between distributions can be consulted.\nA first insight from a general overview of distinctive events about Transnational Women writers is that they seem to never die. Events like 'death' or 'died' are never distinctive for them but always for the group against which they are compared. This may be explained by the average year of birth of Transnational Women writers with a biography on Wikipedia, which is 1951, while for Western men is 1936, 1943 for Transnational men, and 1944 for Western woman.\nThe analysis of the most salient biographical  events between Transnational women and Transnational men shows how intersectionality helps to identify gender biases. When Transnationals are considered as a single group (Table 7) against the Western counterparts, the majority of the biographical events are related to career (award, conferred) or to social commitment (activist, migrated, exile). When the comparison is made within the Transnational group (Table 8), the gender bias demonstrated by Sun and Peng (2021) and Bamman and Smith (2014) clearly emerges. In fact, 'married', 'marriage', and 'divorce' are associated to Transnational women. In addition, there is a lack of career-related events about them, while this is not the case for men (actor, chairman, politician). The comparison between Transnational women and Western men still shows a gender bias, but less prominent. Among the most salient events, only 'mother' highlights a potential bias, while events on Transnational women career ('win', 'won', 'award', 'selected'), education ('degree', 'education', 'schooling') and social commitment ('activist') are present. Finally, the comparison between Transnational and Western women offers three additional insights. First, the only event about private life which is salient for one of the two groups is 'married'. This indicates that private life events of women -in general -are always presented in relation to their conjugal status. Second, careers and social commitments are particularly present for Transnational women. Finally, the framing of the concept \"relocation\" is expressed using different event triggers: the more neutral 'move' is used for Western women, while the more marked, negatively connotated term 'migrate' is associated with Transnational women.\nSummarizing, Transnational Women are underrepresented on Wikipedia with respect to other groups, both in terms of number of biographies and events. The analysis of their most distinctive biographical events shows that the already-known tendency of mentioning private life events about women in Wikipedia biographies (Sun and Peng, 2021;Bamman and Smith, 2014) can be refined when coupled to ethnic origins. Indeed, the extent of the presence of gender biases is more salient when comparing the biographical entries within the same broad \"ethnic\" group, while is becomes obfuscated across groups, making other bias (i.e., racial) more prominent.", "publication_ref": ["b7", "b4", "b32", "b12", "b33", "b1", "b33", "b1"], "figure_ref": [], "table_ref": ["tab_1", "tab_11", "tab_11", "tab_13", "tab_15", "tab_13", "tab_15"]}, {"heading": "Conclusion and Future Work", "text": "In this paper we presented a novel set of computational resources for deepening the analysis of biographical events and improving their automatic detection. We found that existing annotated corpora may be successfully reused to train models that obtain good performances. The model for entity detection, trained on OntoNotes, obtained a Fscore of 0.808, while the model for event detection, trained on TimeBank and Wikibio, scored 0.859. We have applied these newly developed resources to perform an analysis of biases in Transnational women writers on Wikipedia adopting intersectionality as a framework to interpret our results. In particular, we have identified that the representation of women and non-Western people on Wikipedia is problematic and biased. Using different axes of analysis -as suggested by intersectionality -it becomes easier to better identify these biases. For instance, gender bias against Transnational women are more marked when comparing their biographies against those of Transnational men rather than Western ones. On the other hand, potential racial biases emerge when comparing Transnational women to Western women. Using an intersectional framework would benefit the understanding and countering of biases of women and non-Western people on Wikipedia.\nFuture work will improve the model for biographical event detection, and to extend the analysis on a wider set of biographical entries from different sources.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations and Ethical Issues", "text": "This work presents some limitations that will be addressed in future work. In particular, i) even if  the model for biographical event detection obtained good results, more sophisticated approaches may be devised to increase its effectiveness (e.g., best performing LMs, multi-task settings); ii) the intersectional analysis was performed on a specific sample of people, and thus limited to writers. Taking into account people with other occupations may lead to different results; finally, iii) only Wikipedia biographies were considered: biographies from other sources may differ in style and thus pose novel challenges to the biographical event detection task.\nThe research involved the collection of documents from Wikipedia, which are released under the Creative Commons Attribution-ShareAlike 3.0 license. The annotation of the experiment was not crowdsourced. All the three annotators are member of the research team who carried out the research as well as authors of the present paper. They are all affiliated with the University of Turin with whom they have a contract regulated by the Italian laws. Their annotation activity is part of their effort related to the development of the present work, which was economically recognized within their contracts with the University of Turin. A data statement for the research can be accessed at the following url: https: //github.com/marcostranisci/ WikiBio/blob/master/README.md   ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Comparison Between Transnational", "text": "Women and Men through the JS Divergence Metric\nIn this Section you can observe a comparative analysis of the divergence between events about Transnational women against Transnational men (Figure 1), Western men (Figure 2), and Western women (Figure 3). All divergences were computed and plotted with Shifterator (Gallagher et al., 2021). . Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? 3\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Interoperable annotation of events and event relations across domains", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Jun Araki; Lamana Mulaffer; Arun Pandian; Yukari Yamakawa; Kemal Oflazer; Teruko Mitamura"}, {"ref_id": "b1", "title": "Unsupervised discovery of biographical structure from text", "journal": "Transactions of the Association for Computational Linguistics", "year": "2014", "authors": "David Bamman; A Noah;  Smith"}, {"ref_id": "b2", "title": "Propbank annotation guidelines", "journal": "", "year": "2010", "authors": "Claire Bonial; Olga Babko-Malaya; D Jinho; Jena Choi; Martha Hwang;  Palmer"}, {"ref_id": "b3", "title": "Comprehensive and consistent PropBank light verb annotation", "journal": "", "year": "2016", "authors": "Claire Bonial; Martha Palmer"}, {"ref_id": "b4", "title": "Unhinging the National Framework: Perspectives on Transnational Life Writing", "journal": "Sidestone Press", "year": "2020", "authors": "Babs Boter; Marleen Rensen; Giles Scott-Smith"}, {"ref_id": "b5", "title": "Unsupervised learning of narrative event chains", "journal": "", "year": "2008", "authors": "Nathanael Chambers; Dan Jurafsky"}, {"ref_id": "b6", "title": "Unsupervised learning of narrative schemas and their participants", "journal": "", "year": "2009", "authors": "Nathanael Chambers; Dan Jurafsky"}, {"ref_id": "b7", "title": "Demarginalizing the intersection of race and sex: A black feminist critique of antidiscrimination doctrine, feminist theory and antiracist politics", "journal": "", "year": "1989", "authors": "Kimberl\u00e9 Crenshaw"}, {"ref_id": "b8", "title": "On intersectionality: Essential writings", "journal": "The New Press", "year": "2017", "authors": "W Kimberl\u00e9;  Crenshaw"}, {"ref_id": "b9", "title": "On coreferring: Coreference in muc and related annotation schemes", "journal": "Computational linguistics", "year": "2000", "authors": "Rodger Kees Van Deemter;  Kibble"}, {"ref_id": "b10", "title": "2020. Data feminism", "journal": "MIT press", "year": "", "authors": "D Catherine; Lauren F Klein"}, {"ref_id": "b11", "title": "BiographyNet: Extracting Relations Between People and Events", "journal": "New Academic Press", "year": "2017-12-26", "authors": "Antske Fokkens; Serge Ter Braake; Niels Ockeloen; Piek Vossen; Susan Leg\u00eane; Guus Schreiber;  Victor De;  Boer"}, {"ref_id": "b12", "title": "Generalized word shift graphs: a method for visualizing and explaining pairwise comparisons between texts", "journal": "EPJ Data Science", "year": "2021", "authors": "J Ryan;  Gallagher; R Morgan; Lewis Frank; Aaron J Mitchell; Andrew J Schwartz;  Reagan; M Christopher; Peter Sheridan Danforth;  Dodds"}, {"ref_id": "b13", "title": "Eventkg: a multilingual event-centric temporal knowledge graph", "journal": "Springer", "year": "2018", "authors": "Simon Gottschalk; Elena Demidova"}, {"ref_id": "b14", "title": "Message understanding conference-6: A brief history", "journal": "", "year": "1996", "authors": "Ralph Grishman; M Beth;  Sundheim"}, {"ref_id": "b15", "title": "Ontonotes: the 90% solution", "journal": "Short Papers", "year": "2006", "authors": "Eduard Hovy; Mitch Marcus; Martha Palmer; Lance Ramshaw; Ralph Weischedel"}, {"ref_id": "b16", "title": "Spanbert: Improving pre-training by representing and predicting spans", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020", "authors": "Mandar Joshi; Danqi Chen; Yinhan Liu; S Daniel; Luke Weld; Omer Zettlemoyer;  Levy"}, {"ref_id": "b17", "title": "A detailed comparison of seven approaches for the annotation of timedependent factual knowledge in RDF and OWL", "journal": "", "year": "2014", "authors": "Hans-Ulrich Krieger"}, {"ref_id": "b18", "title": "Discovering differences in the representation of people using contextualized semantic axes", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Li Lucy; Divya Tadimeti; David Bamman"}, {"ref_id": "b19", "title": "The jensen-shannon divergence", "journal": "Journal of the Franklin Institute", "year": "1997", "authors": " Ml Men\u00e9ndez; L Pardo; M C Pardo;  Pardo"}, {"ref_id": "b20", "title": "Ramble on: Tracing movements of popular historical figures", "journal": "", "year": "2017", "authors": "Stefano Menini; Rachele Sprugnoli; Giovanni Moretti; Enrico Bignotti; Sara Tonelli; Bruno Lepri"}, {"ref_id": "b21", "title": "The NomBank project: An interim report", "journal": "Association for Computational Linguistics", "year": "2004", "authors": "Adam Meyers; Ruth Reeves; Catherine Macleod; Rachel Szekely; Veronika Zielinska; Brian Young; Ralph Grishman"}, {"ref_id": "b22", "title": "Meantime, the newsreader multilingual event and time corpus", "journal": "", "year": "2016", "authors": "Anne-Lyse Minard; Manuela Speranza; Ruben Urizar; Begona Altuna; Marieke Van Erp; Anneleen Schoen; Chantal Van Son"}, {"ref_id": "b23", "title": "Semeval-2015 task 4: Timeline: Cross-document event ordering. In 9th international workshop on semantic evaluation", "journal": "", "year": "2015", "authors": "Anne-Lyse Myriam Minard; Manuela Speranza; Eneko Agirre; Itziar Aldabe; Bernardo Marieke Van Erp; German Magnini; Ruben Rigau;  Urizar"}, {"ref_id": "b24", "title": "Richer event description: Integrating event coreference with temporal, causal and bridging annotation", "journal": "", "year": "2016", "authors": "Kristin Tim O'gorman; Martha Wright-Bettner;  Palmer"}, {"ref_id": "b25", "title": "Large-scale data harvesting for biographical data", "journal": "", "year": "2019-09-05", "authors": "Alistair Plum; Marcos Zampieri; Constantin Orasan; Eveline Wandl-Vogt; Ruslan Mitkov"}, {"ref_id": "b26", "title": "Timeml: Robust specification of event and temporal expressions in text", "journal": "", "year": "2003", "authors": "James Pustejovsky; M Jos\u00e9; Robert Castano; Roser Ingria;  Sauri; J Robert; Andrea Gaizauskas; Graham Setzer;  Katz;  Dragomir R Radev"}, {"ref_id": "b27", "title": "The TimeBank corpus", "journal": "", "year": "2003", "authors": "James Pustejovsky; Patrick Hanks; Roser Sauri; Andrew See; Robert Gaizauskas; Andrea Setzer; Dragomir Radev; Beth Sundheim; David Day; Lisa Ferro"}, {"ref_id": "b28", "title": "Building event-centric knowledge graphs from news", "journal": "Journal of Web Semantics", "year": "2016", "authors": "Marco Rospocher; Piek Marieke Van Erp; Antske Vossen; Itziar Fokkens; German Aldabe; Aitor Rigau; Thomas Soroa; Tessel Ploeger;  Bogaard"}, {"ref_id": "b29", "title": "Extracting and visualising biographical events from wikipedia", "journal": "", "year": "2015", "authors": "Irene Russo; Tommaso Caselli; Monica Monachini"}, {"ref_id": "b30", "title": "Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter", "journal": "", "year": "2019", "authors": "Victor Sanh; Lysandre Debut; Julien Chaumond; Thomas Wolf"}, {"ref_id": "b31", "title": "Literary event detection", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Matthew Sims; Jong Ho Park; David Bamman"}, {"ref_id": "b32", "title": "The URW-KG: a resource for tackling the underrepresentation of non-western writers", "journal": "", "year": "2022", "authors": "Marco Antonio Stranisci; Giuseppe Spillo; Cataldo Musto; Viviana Patti; Rossana Damiano"}, {"ref_id": "b33", "title": "Men are elected, women are married: Events gender bias on wikipedia", "journal": "Short Papers", "year": "2021", "authors": "Jiao Sun; Nanyun Peng"}, {"ref_id": "b34", "title": "Bio CRM: A data model for representing biographical data for prosopographical research", "journal": "", "year": "2018", "authors": "Eero Jouni Antero Tuominen; Petri Antero Hyv\u00f6nen;  Leskinen"}, {"ref_id": "b35", "title": "Newsreader: Using knowledge resources in a cross-lingual reading machine to generate more knowledge from massive streams of news. Knowledge-Based Systems", "journal": "", "year": "2016", "authors": "Piek Vossen; Rodrigo Agerri; Itziar Aldabe; Agata Cybulska; Antske Marieke Van Erp; Egoitz Fokkens; Anne-Lyse Laparra; Alessio Minard; German Palmero Aprosio; Marco Rigau; Roxane Rospocher;  Segers"}, {"ref_id": "b36", "title": "Wikidata: a free collaborative knowledgebase", "journal": "Communications of the ACM", "year": "2014", "authors": "Denny Vrande\u010di\u0107; Markus Kr\u00f6tzsch"}, {"ref_id": "b37", "title": "The GUM corpus: Creating multilayer resources in the classroom. Language Resources and Evaluation", "journal": "", "year": "2017", "authors": "Amir Zeldes"}, {"ref_id": "b38", "title": "including hyperparameter search and best-found hyperparameter values? The focus of the experiments was to test the impact of different training set over the same vanilla version of a small LM like DistilBert. So we didn't provide information about that C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments", "journal": "", "year": "", "authors": ""}, {"ref_id": "b39", "title": "If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used", "journal": "", "year": "", "authors": " Nltk;  Spacy;  Rouge"}, {"ref_id": "b40", "title": "crowdworkers) or research with human participants? Section 2.1 and Section", "journal": "Limitations and Ethical Issues", "year": "", "authors": ""}, {"ref_id": "b41", "title": "Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators", "journal": "", "year": "", "authors": " D1"}, {"ref_id": "b42", "title": "crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic", "journal": "", "year": "", "authors": ""}, {"ref_id": "b43", "title": "Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?", "journal": "", "year": "", "authors": " D3"}, {"ref_id": "b44", "title": "Was the data collection protocol approved (or determined exempt) by an ethics review board? Not applicable", "journal": "", "year": "", "authors": " D4"}, {"ref_id": "b45", "title": "Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data", "journal": "", "year": "", "authors": " D5"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "6. When Ng\u0169g\u0129 returned to America at the end of his month trip[...]    ", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: The comparison of events between Transnational Women biographies and Transnational Men biographies.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: The comparison of events between Transnational Women biographies and Western men biographies.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: The comparison of events between Transnational Women biographies and Transnational Women.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ": Inter-Annotator Agreement (Cohen's Kappa).events is annotated by linking the contextual modal-ity marker and the target event, as illustrated inExample 5:5. Feeling alienated, he decided to quit college, but was stopped [...]<CONT_MOD source='decided' target = quit'value='INTENTION' / ><CONT_MOD source='stopped' target = 'quit'value='NOT_HAPPENED' / >"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "A list of five existing resources that have been employed in the biographical event detection task.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "The similarity between corpora for event annotation computed with the Jensen-Shannon Divergence.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Results of entity detection experiments.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "The number of events and event types broken down by gender and ethnicity of writers.", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Comparison of biographical events betweenTransnational and Western writers.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Transnational Women Transnational Men defeated, daughter, actress, married, lost, appeared, marriage, deafeating, won, began, activist, loosing, divorced, raised, attended, win, featured, seeded, mother, grew actor, son, chairman, lyricist, served,politician, critic, father,  joined, death, accused,  known, poet, scholar,  elected, imprisoned,  president, established,  exile    ", "figure_data": "Transnational Women Western Men activist, degree, won, wrote, enlisted, service,actress, received, born,actor, claimed, father,daughter, award, educa-assigned,drafted,tion, defeated, recipient,directed,developed,defeating, win, selected,deathmother, writer, school-ing, completed, poet,lostTransnational Women Western Women defeated, defeating, wrote, appeared, mar-lost, activist, education,ried, author, published,loosing, schooling, de-starred, death, lives,gree, poet, completed,moved, died, sold, illus-win, seeded, injury,trator, illustrated, nom-award, match, reach,inated, reviewer, write,migrated, participated,lived, developed, spentprofessor, loss"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "Comparison of biographical events about Transnational Women vs. Transnational Men, Western Men, and Western Women. The tokens in the Table cells were obtained by maximizing the JSD divergence. We used the Shifterator software library (see Appendix A for details).", "figure_data": ""}], "formulas": [], "doi": "10.18653/v1/P19-1353"}