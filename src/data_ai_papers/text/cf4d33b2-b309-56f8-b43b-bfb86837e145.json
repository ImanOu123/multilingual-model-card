{"title": "Inferring Rankings Using Constrained Sensing", "authors": "Srikanth Jagabathula; Devavrat Shah", "pub_date": "2011-06-19", "abstract": "We consider the problem of recovering a function over the space of permutations (or, the symmetric group) over n elements from given partial information; the partial information we consider is related to the group theoretic Fourier Transform of the function. This problem naturally arises in several settings such as ranked elections, multi-object tracking, ranking systems, and recommendation systems. Inspired by the work of Donoho and Stark in the context of discrete-time functions, we focus on non-negative functions with a sparse support (support size \u226a domain size). Our recovery method is based on finding the sparsest solution (through \u21130 optimization) that is consistent with the available information. As the main result, we derive sufficient conditions for functions that can be recovered exactly from partial information through \u21130 optimization. Under a natural random model for the generation of functions, we quantify the recoverability conditions by deriving bounds on the sparsity (support size) for which the function satisfies the sufficient conditions with a high probability as n \u2192 \u221e. \u21130 optimization is computationally hard. Therefore, the popular compressive sensing literature considers solving the convex relaxation, \u21131 optimization, to find the sparsest solution. However, we show that \u21131 optimization fails to recover a function (even with constant sparsity) generated using the random model with a high probability as n \u2192 \u221e. In order to overcome this problem, we propose a novel iterative algorithm for the recovery of functions that satisfy the sufficient conditions. Finally, using an Information Theoretic framework, we study necessary conditions for exact recovery to be possible.", "sections": [{"heading": "I. INTRODUCTION", "text": "F UNCTIONS over permutations serve as rich tools for modeling uncertainty in several important practical applications; they correspond to a general model class, where each model has a factorial number of parameters. However, in many practical applications, only partial information is available about the underlying functions; this is because either the problem setting naturally makes only partial information available, or memory constraints allow only partial information to be maintained as opposed to the entire function -which requires storing a factorial number of parameters in general. In either case, the following important question arises: which \"types\" of functions can be recovered from access to only partial information? Intuitively, one expects a characterization that relates the \"complexity\" of the functions that can be recovered to the \"amount\" of partial information one has access to. One of the main goals of this paper is to formalize this statement. More specifically, this paper considers the problem of exact recovery of a function over the space of permutations given only partial information. When the function is a probability distribution, the partial information we consider can be thought of as lower-order marginals; more generally, the types of partial information we consider are related to the group theoretic Fourier Transform of the function, which provides a general way yo represent varying \"amounts\" of partial information. In this context, our goal is to (a) characterize a class of functions that can be recovered exactly from the given partial information, and (b) design a procedure for their recovery. We restrict ourselves to nonnegative functions, which span many of the useful practical applications. Due to the generality of the setting we consider, a thorough understanding of this problem impacts a wideranging set of applications. Before we present the precise problem formulation and give an overview of our approach, we provide below a few motivating applications that can be modeled effectively using functions over permutations.\nA popular application where functions over permutations naturally arise is the problem of rank aggregation. This problem arises in various contexts. The classical setting is that of ranked election, which has been studied in the area of Social Choice Theory for the past several decades. In the ranked election problem, the goal is to determine a \"socially preferred\" ranking of n candidates contesting an election using the individual preference lists (permutations of candidates) of the voters. Since the \"socially preferred\" outcome should be independent of the identities of voters, the available information can be summarized as a function over permutations that maps each permutation \u03c3 to the fraction of voters that have the preference list \u03c3. While described in the context of elections, the ranked election setting is more general and also applies to aggregating through polls the population preferences on global issues, movies, movie stars, etc. Similarly, rank aggregation has also been studied in the context of aggregating webpage rankings [2], where one has to aggregate rankings over a large number of webpages. Bulk of the work done on the ranked election problem deals with the question of aggregation given access to the entire function over permutations that summarizes population preferences. In many practical settings, however, determining the function itself is non-trivial -even for reasonable small values of n. Like in the setting of polling, one typically can gather only partial information about population preferences. Therefore, our ability to recover functions over permutations from available partial information impacts our ability to aggregate rankings. Interestingly, in the context of ranked election, Diaconis [3] showed through spectral analysis that a partial set of Fourier coefficients of the function possesses \"rich\" information about the underlying function. This hints to the possibility that, in relevant applications, limited partial information can still capture a lot of structure of the underlying function.\nAnother important problem, which has received a lot of attention recently, is the Identity Management Problem or the Multi-object tracking problem. This problem is motivated by applications in air traffic control and sensor networks, where the goal is to track the identities of n objects from noisy measurements of identities and positions. Specifically, consider an area with sensors deployed that can identify the unique signature and the position associated with each object when it passes close to it. Let the objects be labeled 1, 2, . . . , n and let x(t) = (x 1 (t), x 2 (t), . . . , x n (t)) denote the vector of positions of the n objects at time t. Whenever a sensor registers the signature of an object the vector x(t) is updated. A problem, however, arises when two objects, say i, j, pass close to a sensor simultaneously. Because the sensors are inexpensive, they tend to confuse the signatures of the two objects; thus, after the two objects pass, the sensor has information about the positions of the objects, but it only has beliefs about which position belongs to which object. This problem is typically modeled as a probability distribution over permutations, where, given a position vector x(t), a permutation \u03c3 of 1, 2, . . . , n describes the assignment of the positions to objects. Because the measurements are noisy, to each position vector x(t), we assign, not a single permutation, but a distribution over permutations. Since we now have a distribution over permutations, the factorial blow-up makes it challenging to maintain it. Thus, it is often approximated using a partial set of Fourier coefficients. Recent work by [4], [5] deals with updating the distribution with new observations in the Fourier domain. In order to obtain the final beliefs one has to recover the distribution over permutations from a partial set of Fourier coefficients.\nFinally, consider the task of coming up with rankings for teams in a sports league, for example, the \"Formula-one\" car racing or American football, given the outcomes of various games. In this context, one approach is to model the final ranking of the teams using, not just one permutation, but a distribution over permutations. A similar approach has been taken in ranking players in online games (cf. Microsoft's TrueSkill solution [6]), where the authors, instead of maintaining scores, maintain a distribution over scores for each player. In this context, clearly, we can gather only partial information and the goal is to fit a model to this partial information. Similar questions arise in recommendation systems in cases where rankings, instead of ratings, are available or are preferred.\nIn summary, all the examples discussed above relate to inferring a function over permutations using partial information. To fix ideas, let S n denote the permutation group of order n and f : S n \u2192 R + denote a non-negative function defined over the permutations. We assume we have access to partial information about f (\u2022) that, as discussed subsequently, corresponds to a subset of coefficients of the group theoretic Fourier Transform of f (\u2022). We note here that a partial set of Fourier coefficients not only provides a rigorous way to compress the high-dimensional function f (\u2022) (as used in [4], [5]), but also have natural interpretations, which makes it easy to gather in practice. Under this setup, our goal is to characterize the functions f that can be recovered. The problem of exact recovery of functions from a partial information has been widely studied in the context of discrete-time functions; however, the existing approaches dont naturally extend to our setup. One of the classical approaches for recovery is to find the function with the minimum \"energy\" consistent with the given partial information. This approach was extended to functions over permutations in [7], where the authors obtain lower bounds on the energy contained in subsets of Fourier Transform coefficients to obtain better \u2113 2 guarantees when using the function the minimum \"energy.\" This approach, however, does not naturally extend to the case of exact recovery. In another approach, which recently gained immense popularity, the function is assumed to have a sparse support and conditions are derived for the size of the support for which exact recovery is possible. This work was pioneered by Donoho; in [1], Donoho and Stark use generalized uncertainty principles to recover a discrete-time function with sparse support from a limited set of Fourier coefficients. Inspired by this, we restrict our attention to functions with a sparse support.\nAssuming that the function is sparse, our approach to performing exact recovery is to find the function with the sparsest support that is consistent with the given partial information, henceforth referred to as \u2113 0 optimization. This approach is often justified by the philosophy of Occam's razor. We derive sufficient conditions in terms of sparsity (support size) for functions that can be recovered through \u2113 0 optimization. Furthermore, finding a function with the sparsest support through \u2113 0 minimization is in general computationally hard. This problem is typically overcome by considering the convex relaxation of the \u2113 0 optimization problem. However, as we show in Theorem III.2, such a convex relaxation does not yield exact recovery in our case. Thus, we propose a simple iterative algorithm called the 'sparsest-fit' algorithm and prove that the algorithm performs exact recovery of functions that satisfy the sufficient conditions.\nIt is worth noting that our work has important connections to the work done in the recently popular area of compressive sensing. Broadly speaking, this work derives sufficient conditions under which the sparsest function that is consistent with the given information can be found by solving the corresponding \u2113 1 relaxation problem. However, as discussed below in the section on relevant work, the sufficient conditions derived in this work do not apply to our setting. Therefore, our work may be viewed as presenting an alternate set of conditions under which the \u2113 0 optimization problem can be solved efficiently.", "publication_ref": ["b1", "b2", "b3", "b4", "b5", "b3", "b4", "b6", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "A. Related Work", "text": "Fitting sparse models to observed data has been a classical approach used in statistics for model recovery and is inspired by the philosophy of Occam's Razor. Motivated by this, sufficient conditions based on sparsity for learnability have been of great interest over years in the context of communication, signal processing and statistics, cf. [8], [9]. In recent years, this approach has become of particular interest due to exciting developments and wide ranging applications including:\n\u2022 In signal processing (see [10], [11], [12], [13], [14])\nwhere the goal is to estimate a 'signal' by means of minimal number of measurements. This is referred to as compressive sensing. \u2022 In coding theory through the design of low-density parity check codes [15], [16], [17] or in the design Reed Solomon codes [18] where the aim is to design a coding scheme with maximal communication rate. \u2022 In the context of streaming algorithms through the design of 'sketches' (see [19], [20], [21], [22], [23]) for the purpose of maintaining a minimal 'memory state' for the streaming algorithm's operation. In all of the above work, the basic question (see [24]) pertains to the design of an m \u00d7 n \"measurement\" matrix A so that x can be recovered efficiently from measurements y = Ax (or its noisy version) using the \"fewest\" possible number measurements m. The setup of interest is when x is sparse and when m < n or m \u226a n. The type of interesting results (such as those cited above) pertain to characterization of the sparsity K of x that can be recovered for a given number of measurements m. The usual tension is between the ability to recover x with large k using a sensing matrix A with minimal m.\nThe sparsest recovery approach of this paper is similar (in flavor) to the above stated work; in fact, as is shown subsequently, the partial information we consider can be written as a linear transform of the function f (\u2022). However, the methods or approaches of the prior work do not apply. Specifically, the work considers finding the sparsest function consistent with the given partial information by solving the corresponding \u2113 1 relaxation problem. The work derives a necessary and sufficient condition, called the Restricted Nullspace Property, on the structure of the matrix A that guarantees that the solutions to the \u2113 0 and \u2113 1 relaxation problems are the same (see [11], [21]). However, such sufficient conditions trivially fail in our setup (see [25]). Therefore, our work provides an alternate set of conditions that guarantee efficient recovery of the sparsest function.", "publication_ref": ["b7", "b8", "b9", "b10", "b11", "b12", "b13", "b14", "b15", "b16", "b17", "b18", "b19", "b20", "b21", "b22", "b23", "b10", "b20", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "B. Our Contributions", "text": "Recovery of a function over permutations from only partial information is clearly a hard problem both from a theoretical and computational standpoint. We make several contributions in this paper to advance our understanding of the problem in both these respects. As the main result, we obtain sufficient conditions -in terms of sparsity -for functions that can be recovered exactly from partial information. Specifically, our result establishes a relation between the \"complexity\" (as measured in sparsity) of the function that can be recovered and the \"amount\" of partial information available.\nOur recovery scheme consists of finding the sparsest solution consistent with the given partial information through \u2113 0 optimization. We derive sufficient conditions under which a function can be recovered through \u2113 0 optimization. First, we state the sufficient conditions for recovery through \u2113 0 optimization in terms of the structural properties of the functions. To understand the strength of the sufficient conditions, we propose a random generative model for functions with a given support size; we then obtain bounds on the size of the support for which a function generated according to the random generative model satisfies the sufficient conditions with a high probability. To our surprise, it is indeed possible to recover, with high probability, functions with seemingly large sparsity for given partial information (see precise statement of Theorems III.3-III.6 for details).\nFinding the sparsest solution through \u2113 0 optimization is computationally hard. This problem is typically overcome by considering the \u2113 1 convex relaxation of the \u2113 0 optimization problem. However, as we show in Example II-C.1, \u2113 1 relaxation does not always result in exact recovery, even when the the sparsity of the underlying function is only 4. In fact, a necessary and sufficient condition for \u2113 1 relaxation to yield the sparsest solution x that satisfies the constraints y = Ax is the so called Restricted Nullspace Condition (RNC) on the measurement matrix A; interestingly, the more popular Restricted Isoperimetric Property (RIP) on the measurement matrix A is a sufficient condition. However, as shown below, the types of partial information we consider can be written as a linear transform of f (\u2022). Therefore, Example II-C.1 shows that in our setting, the measurement matrix does not satisfy RNC. It is natural to wonder if Example II-C.1 is anomalous. We show that this is indeed not the case. Specifically, we show in Theorem III.2 that, with a high probability, \u2113 1 relaxation fails to recover a function generated according to the random generative model.\nSince convex relaxations fail in recovery, we exploit the structural property of permutations to design a simple iterative algorithm called the 'sparsest-fit' algorithm to perform recovery. We prove that the algorithm recovers a function from a partial set of its Fourier coefficients as long as the function satisfies the sufficient conditions.\nWe also study the limitation of any recovery algorithm to recover a function exactly from a given form of partial information. Through an application of classical information theoretic Fano's inequality, we obtain a bound on the sparsity beyond which recovery is not asymptotically reliable; a recovery scheme is called asymptotically reliable if the probability of error asymptotically goes to 0.\nIn summary, we obtain an intuitive characterization of the \"complexity\" (as measured in sparsity) of the functions that can be recovered from the given partial information. We show how \u2113 1 relaxation fails in recovery in this setting. Hence, the sufficient conditions we derive correspond to an alternate set of conditions that guarantee efficient recovery of the sparsest function.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C. Organization", "text": "Section II introduces the model, useful notations and the precise formulation of the problem. In Section III, we provide the statements of our results. Section IV describes our iterative algorithm that can recover f fromf (\u03bb) when certain conditions (see Condition 1) are satisfied. Sections V to XI provide detailed proofs. Conclusions are presented Section XII.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "II. PROBLEM STATEMENT", "text": "In this section, we introduce the necessary notations, definitions and provide the formal problem statement.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Notations", "text": "Let n be the number of elements and S n be set of all possible n! permutations or rankings of these of n elements. Our interest is in learning non-negative valued functions f defined on S n , i.e. f : S n \u2192 R + , where R + = {x \u2208 R : x \u2265 0}. The support of f is defined as\nsupp (f ) = {\u03c3 \u2208 S n : f (\u03c3) = 0} .\nThe cardinality of support, | supp (f ) | will be called the sparsity of f and will be denoted by K. We will also call it the \u2113 0 norm of f , denoted by |f | 0 .\nIn this paper, we wish to learn f from a partial set of Fourier coefficients. To define the Fourier transform of a function over the permutation group, we need some notations. To this end, consider a partition of n, i.e. an ordered tuple \u03bb = (\u03bb 1 , \u03bb 2 , . . . , \u03bb r ), such that \u03bb 1 \u2265 \u03bb 2 \u2265 . . . \u2265 \u03bb r \u2265 1, and n = \u03bb 1 + \u03bb 2 + . . . + \u03bb r . For example, \u03bb = (n \u2212 1, 1) is a partition of n. Now consider a partition of the n elements, {1, . . . , n}, as per the \u03bb partition, i.e. divide n elements into r bins with ith bin having \u03bb i elements. It is easy to see that n elements can be divided as per the \u03bb partition in D \u03bb distinct ways, with\nD \u03bb = n! r i=1 \u03bb i ! .\nLet the distinct partitions be denoted by t i ,\n1 \u2264 i \u2264 D \u03bb 1 . For example, for \u03bb = (n \u2212 1, 1) there are D \u03bb = n!/(n \u2212 1)! = n distinct ways given by t i \u2261 {1, . . . , i \u2212 1, i + 1, . . . , n}{i}, 1 \u2264 i \u2264 n.\nGiven a permutation \u03c3 \u2208 S n , its action on t i is defined through its action on the n elements of t i , resulting in a \u03bb partition with the n elements permuted. In the above example with \u03bb = (n \u2212 1, 1), \u03c3 acts on t i to give t \u03c3(i) , i.e. \u03c3 : t i \u2192 t \u03c3(i) , where t i \u2261 {1, . . . , i \u2212 1, i + 1, . . . , n}{i} and t \u03c3(i) \u2261 {1, . . . , \u03c3(i) \u2212 1, \u03c3(i) + 1, . . . , n}{\u03c3(i)}. Now, for a given partition \u03bb and a permutation \u03c3 \u2208 S n , define a\n0/1 valued D \u03bb \u00d7 D \u03bb matrix M \u03bb (\u03c3) as M \u03bb ij (\u03c3) = 1, if \u03c3(t j ) = t i 0, otherwise. for all 1 \u2264 i, j \u2264 D \u03bb\nThis matrix M \u03bb (\u03c3) corresponds to a degree D \u03bb representation of the permutation group.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B. Partial Information as a Fourier Coefficient", "text": "The partial information we consider in this paper is the Fourier transform coefficient of f at the representation M \u03bb , for each \u03bb. The motivation for considering Fourier coefficients at representations M \u03bb is two fold: first, they provide a rigorous way to compress the high-dimensional function f (\u2022) (as used in [4], [5]), and second, as we shall see, Fourier coefficients at representations M \u03bb have natural interpretations, which makes it easy to gather in practice. In addition, each representation M \u03bb contains a subset of the lower-order irreducible representations; thus, for each \u03bb, M \u03bb conveniently captures the information contained in a subset of the lower-order Fourier coefficients up to \u03bb. We now define the Fourier coefficient of f at the representation M \u03bb , which we call \u03bb-partial information.\nDefinition II.1 (\u03bb-Partial Information). Given a function f : S n \u2192 R + and partition \u03bb. The Fourier Transform coefficient at representation M \u03bb , which we call the \u03bb-partial information, is denoted byf (\u03bb) and is defined a\u015d\nf (\u03bb) = \u03c3\u2208Sn f (\u03c3)M \u03bb (\u03c3).\nRecall the example of \u03bb = (n\u22121, 1) with f as a probability distribution on S n . Then,f (\u03bb) is an n \u00d7 n matrix with the (i, j)th entry being the probability of element j mapped to element i under f . That is,f (\u03bb) corresponds to the first order marginal of f in this case.", "publication_ref": ["b3", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "C. Problem Formulation", "text": "We wish to recover a function f based on its partial informationf (\u03bb) based on partition \u03bb. As noted earlier, the classical approach based on Occam's razor suggests recovering the function as a solution of the following \u2113 0 optimization problem:\nminimize g 0 over g : S n \u2192 R + subject to\u011d(\u03bb) =f (\u03bb).(1)\nWe note that the question of recovering f fromf (\u03bb) is very similar to the question studied in the context of compressed sensing, i.e. recover x from y = Ax. To see this, with an abuse of notation imaginef (\u03bb) as the D 2 \u03bb dimensional vector and f as n! dimensional vector. Then,f (\u03bb) = Af where each column of A corresponds to M \u03bb (\u03c3) for certain permutation \u03c3.\nThe key difference from the compressed sensing literature is that A is given in our setup rather than being a design choice.\nQuestion One. As the first question of interest, we wish to identify precise conditions under which \u2113 0 optimization problem (1) recovers the original function f as its unique solution.\nUnlike the popular literature (cf. compressed sensing), such conditions can not be based on sparsity only. This is well explained by the following (counter-)example. In addition, the example also shows that linear independence of the support of f does not guarantee uniqueness of the solution to the \u2113 0 optimization problem.\nExample II-C.1. For any n \u2265 4, consider the four permutations \u03c3 1 = (1, 2), \u03c3 2 = (3, 4), \u03c3 3 = (1, 2)(3, 4) and \u03c3 4 = id, where id is the identity permutation. In addition, consider the partition \u03bb = (n \u2212 1, 1). Then, it is easy to see that\nM \u03bb (\u03c3 1 ) + M \u03bb (\u03c3 2 ) = M \u03bb (\u03c3 3 ) + M \u03bb (\u03c3 4 ).\nWe now consider three cases where a bound on sparsity is not sufficient to guarantee the existence of a unique solution to (1).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "1)", "text": "This example shows that a sparsity bound (even 4) on f is not sufficient to guarantee that f will indeed be the sparsest solution. Specifically, suppose that f (\u03c3 i ) = p i , where p i \u2208 R + for 1 \u2264 i \u2264 4, and f (\u03c3) = 0 for all other \u03c3 \u2208 S n . Without loss of generality, let p 1 \u2264 p 2 . Then,\nf (\u03bb) =p 1 M \u03bb (\u03c3 1 ) + p 2 M \u03bb (\u03c3 2 ) + p 3 M \u03bb (\u03c3 3 ) + p 4 M \u03bb (\u03c3 4 ) =(p 2 \u2212 p 1 )M \u03bb (\u03c3 2 ) + (p 3 + p 1 )M \u03bb (\u03c3 3 ) + (p 4 + p 1 )M \u03bb (\u03c3 4 ). Thus, function g with g(\u03c3 2 ) = p 2 \u2212 p 1 , g(\u03c3 3 ) = p 3 + p 1 , g(\u03c3 4 ) = p 4 + p 1 and g(\u03c3) = 0 for all other \u03c3 \u2208 S n is such that\u011d(\u03bb) =f (\u03bb) but g 0 = 3 < 4 = f 0 .\nThat is, f can not be recovered as the solution of \u2113 0 optimization problem (1) even when support of f is only 4.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "2)", "text": "This example shows that although f might be a sparsest solution, it may not be unique. In particular, suppose that f (\u03c3 1 ) = f (\u03c3 2 ) = p and f (\u03c3\n) = 0 for all other \u03c3 \u2208 S n . Then,f (\u03bb) = pM \u03bb (\u03c3 1 ) + pM \u03bb (\u03c3 2 ) = pM \u03bb (\u03c3 3 ) + pM \u03bb (\u03c3 4\n). Thus, (1) does not have a unique solution. 3) Finally, this example shows that even though the support of f corresponds to a linearly independent set of columns, the sparsest solution may not be unique. Now suppose that f (\u03c3 i ) = p i , where p i \u2208 R + for 1 \u2264 i \u2264 3, and f (\u03c3) = 0 for all other \u03c3 \u2208 S n . Without loss of generality, let p 1 \u2264 p 2 . Then,\nf (\u03bb) =p 1 M \u03bb (\u03c3 1 ) + p 2 M \u03bb (\u03c3 2 ) + p 3 M \u03bb (\u03c3 3 ) =(p 2 \u2212 p 1 )M \u03bb (\u03c3 2 ) + (p 3 + p 1 )M \u03bb (\u03c3 3 ) + p 1 M \u03bb (\u03c3 4 ).\nHere, note that M \u03bb (\u03c3 1 ), M \u03bb (\u03c3 2 ), M \u03bb (\u03c3 3 ) is linearly independent, yet the solution to (1) is not unique.\nQuestion Two. The resolution of the first question will provide a way to recover f by means of solving the \u2113 0 optimization problem in (1). However, in general, it is computationally a hard problem. Therefore, we wish to obtain a simple and possibly iterative algorithm to recover f (and hence for solving (1)).\nQuestion Three. Once we identify the conditions for exact recovery of f , the next natural question to ask is \"how restrictive are the conditions we imposed on f for exact recovery?\" In other words, as mentioned above, we know that the sufficient conditions don't translate to a simple sparsity bound on functions, however, can we find a sparsity bound such that \"most,\" if not all, functions that satisfy the sparsity bound can be recovered? We make the notion of \"most\" functions precise by proposing a natural random generative model for functions with a given sparsity. Then, for given a partition \u03bb, we want to obtain K(\u03bb) so that if K < K(\u03bb) then recovery of f generated according to the generative model fromf (\u03bb) is possible with high probability.\nThis question is essentially an inquiry into whether the situation demonstrated by Example II-C.1 is contrived or not. In other words, it is an inquiry into whether such examples happen with vanishingly low probability for a randomly chosen function. To this end, we describe a natural random function generation model. Definition II.2 (Random Model). Given K \u2208 Z + and an interval C = [a, b], 0 < a < b, a random function f with sparsity K and values in C is generated as follows: choose K permutations from S n independently and uniformly at random 2 , say \u03c3 1 , . . . , \u03c3 K ; select K values from C uniformly at random, say p 1 , . . . , p K ; then function f is defined as\nf (\u03c3) = p i if \u03c3 = \u03c3 i , 1 \u2264 i \u2264 K 0 otherwise.\nWe will denote this model as R(K, C ).\nQuestion Four. Can we characterize a limitation on the ability of any algorithm to recover f fromf (\u03bb) ?", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "III. MAIN RESULTS", "text": "As the main result of this paper, we provide answers to the four questions stated in Section II-C. We start with recalling some notations. Let \u03bb = (\u03bb 1 , . . . , \u03bb r ) be the given partition of n. We wish to recover function f : S n \u2192 R + from available informationf (\u03bb). Let the sparsity of f be K,\nsupp (f ) = {\u03c3 1 , . . . , \u03c3 K }, and f (\u03c3 k ) = p k , 1 \u2264 k \u2264 K.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Answers One & Two.", "text": "To answer the first two questions, we need to find sufficiency conditions for recovering f through \u2113 0 optimization (1) and a simple algorithm to recover the function. For that, we first try to gain a qualitative understanding of the conditions that f must satisfy. Note that a necessary condition for \u2113 0 optimization to recover f is that (1) must have a unique solution; otherwise, without any additional information, we wouldn't know which of the multiple solutions is the true solution. Clearly, sincef (\u03bb) = \u03c3\u2208Sn f (\u03c3)M \u03bb (\u03c3), (1) will have a unique solution only if M \u03bb (\u03c3) \u03c3\u2208supp(f ) is linearly independent. However, this linear independence condition is, in general, not sufficient to guarantee a unique solution; in particular,\neven if M \u03bb (\u03c3) \u03c3\u2208supp(f ) is linearly indepen- dent, there could exist M \u03bb (\u03c3 \u2032 ) \u03c3 \u2032 \u2208H such thatf (\u03bb) = \u03c3 \u2032 \u2208H M \u03bb (\u03c3 \u2032 ) and |H| \u2264 K, where K := |supp (f )|;\nExample II-C.1 illustrates such a scenario. Thus, a sufficient condition for f to be the unique sparsest solution of (1) is that not only is M \u03bb (\u03c3) \u03c3\u2208supp(f ) linearly independent, but M \u03bb (\u03c3), M \u03bb (\u03c3 \u2032 ) \u03c3\u2208supp(f ),\u03c3 \u2032 \u2208H is linearly independent for all H \u2282 S n such that |H| \u2264 K; in other words, not only we want M \u03bb (\u03c3) for \u03c3 \u2208 supp (f ) to be linearly independent, but we want them to be linearly independent even after the addition of at most K permutations to the support of f . Note that this condition is similar to the Restricted Isometry Property (RIP) introduced in [10], which roughly translates to the property that \u2113 0 optimization recovers x of sparsity K from y = Ax provided every subset of 2K columns of A is linearly independent. Motivated by this, we impose the following conditions on f . Condition 1 (Sufficiency Conditions). Let f satisfy the following:\n\u2022 Unique Witness: for any \u03c3 \u2208 supp (f ), there exists\n1 \u2264 i \u03c3 , j \u03c3 \u2264 D \u03bb such that M \u03bb i\u03c3j\u03c3 (\u03c3) = 1, but M \u03bb i\u03c3 j\u03c3 (\u03c3 \u2032 ) = 0, for all \u03c3 \u2032 ( = \u03c3) \u2208 supp (f ) .\n\u2022 Linear Independence: for any collection of integers c 1 , . . . , c K taking values in {\u2212K, . . . , K},\nK k=1 c k p k = 0, unless c 1 = . . . = c K = 0.\nThe above discussion motivates the \"unique witness\" condition; indeed, M \u03bb (\u03c3) for \u03c3 satisfying the \"unique witness\" condition are linearly independent because every permutation has a unique witness and no non-zero linear combination of M \u03bb (\u03c3) can yield zero. On the other hand, as shown in the proof of Theorem III.1, the linear independence condition is required for the uniqueness of the sparsest solution.\nNow we state a formal result that establishes Condition 1 as sufficient for recovery of f as the unique solution of \u2113 0 optimization problem. Further, it allows for a simple, iterative recovery algorithm. Thus, Theorem III.1 provides answers to questions One and Two of Section II-C.\nTheorem III.1. Under Condition 1, the function f is the unique solution of the \u2113 0 optimization problem (1). Further, a simple, iterative algorithm called the sparsest-fit algorithm, described in Section IV, recovers f . Linear Programs Don't Work. Theorem III.1 states that under Condition 1, the \u2113 0 optimization recovers f and the sparsest-fit algorithm is a simple iterative algorithm to recover it. In the context of compressive sensing literature (cf. [11], [13], [14], [21]), it has been shown that convex relaxation of \u2113 0 optimization, such as the Linear Programing relaxation, have the same solution in similar scenarios. Therefore, it is natural to wonder whether such a relaxation would work in our case. To this end, consider the following Linear Programing relaxation of (1) stated as the following \u2113 1 minimization problem:\nminimize g 1 over g : S n \u2192 R + subject to\u011d(\u03bb) =f (\u03bb).(2)\nExample II-C.1 provides a scenario where \u2113 1 relaxation fails in recovery. In fact, we can prove a stronger result. The following result establishes that -with a high probabilitya function generated randomly as per Definition II.2 cannot be recovered by solving the linear program (2) because there exists a function g such that\u011d(\u03bb) =f (\u03bb) and g 1 = f 1 .\nTheorem III.2. Consider a function f randomly generated as per Definition II.2 with sparsity K \u2265 2. Then, as longs as \u03bb is not the partition (1, 1, . . . , 1) (n times), with probability 1 \u2212 o(1), there exists a function g distinct from f such that g(\u03bb) =f (\u03bb) and g 1 = f 1 .\nAnswer Three. Next, we turn to the third question. Specifically, we study the conditions for high probability recoverability of a random function f in terms of its sparsity. That is, we wish to identify the high probability recoverability threshold K(\u03bb). In what follows, we spell out the result starting with few specific cases so as to better explain the dependency of K(\u03bb) on D \u03bb . Case 1: \u03bb = (n \u2212 1, 1). Here D \u03bb = n andf (\u03bb) provides the first order marginal information. As stated next, for this case the achievable recoverability threshold K(\u03bb) scales 3 as n log n.", "publication_ref": ["b9", "b0", "b10", "b12", "b13", "b20", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Theorem III.3. A randomly generated f as per Definition II.2 can be recovered by the sparsest-fit algorithm with probability", "text": "1 \u2212 o(1) as long as K \u2264 (1 \u2212 \u03b5)n log n for any fixed \u03b5 > 0. Case 2: \u03bb = (n \u2212 m, m) with 1 < m = O(1). Here D \u03bb = \u0398(n m ) andf (\u03bb)\nprovides the mth order marginal information. As stated next, for this case we find that K(\u03bb) scales at least as n m log n. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Theorem III.4. A randomly generated f as per Definition II.2 can be recovered fromf (\u03bb) by the sparsest-fit algorithm for", "text": "\u03bb = (n \u2212 m, m), m = O(1), with probability 1 \u2212 o(1) as long as K \u2264 (1\u2212\u03b5) m! n m log n for\n\u03bb = (\u03bb 1 , . . . , \u03bb r ) with \u03bb 1 = n \u2212 n 2 9 \u2212\u03b4 for any \u03b4 > 0, with probability 1 \u2212 o(1) as long as K \u2264 (1 \u2212 \u03b5)D \u03bb log log D \u03bb for any fixed \u03b5 > 0.\nCase 4: Any \u03bb = (\u03bb 1 , . . . , \u03bb r ). The results stated thus far suggest that the threshold is essentially D \u03bb , ignoring the logarithm term. For general \u03bb, we establish a bound on K(\u03bb) as stated in Theorem III.6 below. Before stating the result, we introduce some notation. For given \u03bb, define \u03b1 = (\u03b1 1 , . . . , \u03b1 r ) with\n\u03b1 i = \u03bb i /n, 1 \u2264 i \u2264 r. Let H(\u03b1) = \u2212 r i=1 \u03b1 i log \u03b1 i , and H \u2032 (\u03b1) = \u2212 r i=2 \u03b1 i log \u03b1 i .\nTheorem III.6. Given \u03bb = (\u03bb 1 , . . . , \u03bb r ), a randomly generated f as per Definition II.2 can be recovered fromf (\u03bb) by the sparsest-fit algorithm with probability 1 \u2212 o(1) as long as\nK \u2264 C D \u03b3(\u03b1) \u03bb ,(3)\nwhere\n\u03b3(\u03b1) = M M + 1 1 \u2212 C \u2032 H(\u03b1) \u2212 H \u2032 (\u03b1) H(\u03b1) , with M = 1 1 \u2212 \u03b1 1 and 0 < C, C \u2032 < \u221e are constants.\nAt a first glance, the above result seems very different from the crisp formulas of Theorems III.3-III.5. Therefore, let us consider a few special cases. First, observe that as \u03b1 1 \u2191 1, M/(M + 1) \u2192 1. Further, as stated in Lemma III.1, H \u2032 (\u03b1)/H(\u03b1) \u2192 1. Thus, we find that the bound on sparsity essentially scales as D \u03bb . Note that the cases 1, 2 and 3 fall squarely under this scenario since\n\u03b1 1 = \u03bb 1 /n = 1 \u2212 o(1).\nThus, this general result contains the results of Theorems III.3-III.5 (ignoring the logarithm terms).\nNext, consider the other extreme of \u03b1 1 \u2193 0. Then, M \u2192 1 and again by Lemma III.1, H \u2032 (\u03b1)/H(\u03b1) \u2192 1. Therefore, the bound on sparsity scales as \u221a D \u03bb . This ought to be the case because for \u03bb = (1, . . . , 1) we have \u03b1 1 = 1/n \u2192 1, D \u03bb = n!, and unique witness property holds only up to o(\n\u221a D \u03bb ) = o( \u221a n!\n) due to the standard Birthday paradox. In summary, Theorem III.6 appears reasonably tight for the general form of partial information \u03bb. We now state the Lemma III.1 used above (proof in Appendix A).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Lemma III.1. Consider any", "text": "\u03b1 = (\u03b1 1 , . . . , \u03b1 r ) with 1 \u2265 \u03b1 1 \u2265 \u2022 \u2022 \u2022 \u2265 \u03b1 r \u2265 0 and r i=1 \u03b1 r = 1. Then, lim \u03b11\u21911 H \u2032 (\u03b1) H(\u03b1) = 1, lim \u03b11\u21930 H \u2032 (\u03b1) H(\u03b1) = 1.\nAnswer Four. Finally, we wish to understand the fundamental limitation on the ability to recover f fromf (\u03bb) by any algorithm. To obtain a meaningful bound (cf. Example II-C.1), we shall examine this question under an appropriate information theoretic setup.\nTo this end, as in random model R(K, C ), consider a function f generated with given K and \u03bb. For technical reasons (or limitations), we will assume that the values p i s are chosen from a discrete set. Specifically, let each p i be chosen from integers {1, . . . , T } instead of compact set C . We will denote this random model by R(K, T ).\nConsider any algorithm that attempts to recover f from f (\u03bb) under R(K, T ). Let h be the estimation of the algorithm. Define probability of error of the algorithm as\np err = Pr (h = f ) .\nWe state the following result.\nTheorem III.7. With respect to random model R(K, T ), the probability of error is uniformly bounded away from 0 for all n large enough and any \u03bb, if\nK \u2265 3D 2 \u03bb n log n log D 2 \u03bb n log n \u2228 T ,(4)\nwhere for any two numbers x and y, x\u2228y denotes max {x, y}.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "IV. SPARSEST-FIT ALGORITHM", "text": "As mentioned above, finding the sparsest distribution that is consistent with the given partial information is in general a computationally hard problem. In this section, we propose an efficient algorithm to fit the sparsest distribution to the given partial informationf (\u03bb), for any partition \u03bb of n. The algorithm we propose determines the sparsest distribution exactly as long as the underlying distribution belongs to the general family of distributions that satisfy the 'unique witness' and 'linear independence' conditions; we call this the 'sparsestfit' algorithm. In this case, it follows from Theorem III.1 that the 'sparsest-fit' algorithm indeed recovers the underlying distribution f (\u2022) exactly from partial informationf (\u03bb). When the conditions are not satisfied, the algorithm produces a certificate to that effect and aborts.\nUsing the degree D \u03bb representation of the permutations, the algorithm processes the elements of the partial information matrixf (\u03bb) sequentially and incrementally builds the permutations in the support. We describe the sparsest-fit algorithm as a general procedure to recover a set of non-negative values given sums of these values over a collection of subsets, which for brevity we call subset sums. In this sense, it can be thought of as a linear equation solver customized for a special class of systems of linear equations.\nNext we describe the algorithm in detail and prove the relavant theorems.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Sparsest-fit algorithm", "text": "We now describe the sparsest-fit algorithm that was also referred to in Theorems III.1, III.3-III.6 to recover function f fromf (\u03bb) under Condition 1.\nSetup. The formal description of the algorithm is given in Fig. 1. The algorithm is described there as a generic procedure to recover a set of non-negative values given a collection of their subset sums. As explained in Fig. 1, the inputs to the algorithm are L positive numbers q 1 , . . . , q L sorted in ascending order q 1 \u2264 q 2 \u2264 \u2022 \u2022 \u2022 \u2264 q L . As stated in assumptions C1-C3 in Fig. 1, the algorithm assumes that the L numbers are different subset sums of K distinct positive numbers p 1 , . . . , p K i.e., q \u2113 = T \u2113 p k for some T \u2113 \u2282 {1, 2, . . . , K}, and the values and subsets satisfy the conditions:\nfor each 1 \u2264 k \u2264 K, p k = q \u2113 for some 1 \u2264 \u2113 \u2264 L and T p k = T \u2032 p k for T = T \u2032 .\nGiven this setup, the sparsest-fit algorithm recovers the values p k and subset membership sets\nA k := {\u2113 : k \u2208 T \u2113 } for 1 \u2264 k \u2264 K using q \u2113 , but without any knowledge of K or subsets T \u2113 , 1 \u2264 \u2113 \u2264 L.\nBefore we describe the algorithm, note that in order to use the sparsest-fit algorithm to recover f (\u2022) we give the non-zero elements of the partial information matrixf (\u03bb) as inputs q \u2113 . In this case, L equals the number of non-zero entries off (\u03bb), p k = f (\u03c3 k ), and the sets A k correspond to M \u03bb (\u03c3 k ). Here, assumption C1 of the algorithm is trivially satisfied. As we argue in Section V, assumptions C2, C3 are implied by the 'unique witness' and 'linear independence' conditions.\nDescription. The formal description is given below in the Fig. 1. The algorithm processes elements q 1 , q 2 , . . . , q L sequentially and builds membership sets incrementally. It maintains the number of non-empty membership sets at the end of each iteration \u2113 as k(\u2113). Partial membership sets are maintained as sets A k , which at the end of iteration \u2113 equals {1 \u2264 k \u2264 k(\u2113) : k \u2208 T \u2113 \u2032 for some \u2113 \u2032 \u2264 \u2113}. The values found are maintained as p 1 , p 2 , . . . , p k(\u2113) . The value of k(0) is initialized to zero and the sets A k are initialized to be empty.\nIn each iteration \u2113, the algorithm checks if the value q \u2113 can be written as a subset sum of values p 1 , p 2 , . . . , p k(\u2113\u22121) for some subset T . If q \u2113 can be expressed as k\u2208T p k for some T \u2282 {1, 2, . . . , k(\u2113 \u2212 1)}, then the algorithm adds \u2113 to sets A k for k \u2208 T and updates k(\u2113) as k(\u2113) = k(\u2113 \u2212 1) before ending the iteration. In case there exists no such subset T , the algorithm updates k(\u2113) as k(\u2113 \u2212 1) + 1, makes the set A k(\u2113) non-empty by adding \u2113 to it, and sets p k(\u2113) to q \u2113 . At the end the algorithm outputs\n(p k , A k ) for 1 \u2264 k \u2264 k(L). Input: Positive values {q 1 , q 2 , . . . , q L } sorted in ascending order i.e., q 1 \u2264 q 2 \u2264 . . . \u2264 q L . Assumptions: \u2203 positive values {p 1 , p 2 , . . . , p K } such that: C1. For each 1 \u2264 \u2113 \u2264 L, q \u2113 = k\u2208T \u2113 p k , for some T \u2113 \u2286 {1, 2, . . . , K} C2. For each 1 \u2264 k \u2264 K, there exists a q \u2113 such that q \u2113 = p K . C3. k\u2208T p k = k \u2032 \u2208T \u2032 p k \u2032 , for all T, T \u2032 \u2286 {1, 2, . . . , J} and T \u2229 T \u2032 = \u2205. Output: {p 1 , p 2 , . . . , p K }, \u2200 1 \u2264 k \u2264 K set A k s.t.\nA k = {\u2113 : q \u2113 = j\u2208T p j and index k belongs to set T }. We now argue that under assumptions C1-C3 stated in Fig. 1, the algorithm finds (p k , A k ) for 1 \u2264 k \u2264 K accurately. Note that by Assumption C2, there exists at least one q \u2113 such that it is equal to p k , for each 1 \u2264 k \u2264 K. Assumption C3 guarantees that the condition in the if statement is not satisfied whenever q \u2113 = p k(\u2113) . Therefore, the algorithm correctly assigns values to each of the p k s. Note that the condition in the if statement being true implies that q \u2113 is a subset sum of some subset T \u2282 p 1 , p 2 , . . . , p k(\u2113\u22121) . Assumption C3 ensures that if such a combination exists then it is unique. Thus, when the condition is satisfied, index \u2113 belongs only to the sets A k such that k \u2208 T . When the condition in the if statement is false, then from Assumptions C2 and C3 it follows that \u2113 is contained only in A k(\u2113) . From this discussion we conclude that the sparsest-fit algorithm correctly assigns all the indices to each of the A k s. Thus, the algorithm recovers p k , A k for 1 \u2264 k \u2264 K under Assumptions C1, C2 and C3. We summarize it in the following Lemma.", "publication_ref": [], "figure_ref": ["fig_1", "fig_1", "fig_1", "fig_1", "fig_1"], "table_ref": []}, {"heading": "Algorithm:", "text": "initialization: p 0 = 0, k(0) = 0, A k = \u2205 for all possible k. for \u2113 = 1 to L if q \u2113 = k\u2208T p k for some T \u2286 {0, 1, . . . , k(\u2113 \u2212 1)} k(\u2113) = k(\u2113 \u2212 1) A k = A k \u222a {\u2113} \u2200 k \u2208 T else k(\u2113) = k(\u2113 \u2212 1) + 1 p k(\u2113) = q \u2113 A k(\u2113) = A k(\u2113) \u222a {\u2113} end if end for Output K = k(L) and (p k , A k ), 1 \u2264 k \u2264 K.\nLemma IV.1. The sparsest-fit algorithm recovers p k , A k for 1 \u2264 k \u2264 K under Assumptions C1, C2 and C3.\nComplexity of the algorithm. Initially, we sort at most D 2 \u03bb elements. This has a complexity of O(D 2 \u03bb log D \u03bb ). Further, note that the for loop in the algorithm iterates for at most D 2 \u03bb times. In each iteration, we are solving a subset-sum problem. Since there are at most K elements, the worst-case complexity of subset-sum in each iteration is O(2 K ). Thus, the worst-case complexity of the algorithm is O(D 2 \u03bb log D \u03bb + D 2 \u03bb 2 K ). However, using the standard balls and bins argument, we can prove that for K = O(D \u03bb log D \u03bb ), with a high probability, there are at most O(log D \u03bb ) elements in each subset-sum problem. Thus, the complexity would then be O exp(log 2 D \u03bb ) with a high probability.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "V. PROOF OF THEOREM III.1", "text": "The proof of Theorem III.1 requires us to establish two claims : under Condition 1, (i) the sparsest-fit algorithm finds f and (ii) the \u2113 0 optimization (1) has f as it's unique solution. We establish these two claims in that order.\nThe sparsest-fit algorithm works. As noted in Section IV, the sparsest-fit algorithm can be used to recover f from f (\u03bb). As per Lemma IV.1, the correctness of the sparsestfit algorithm follows under Assumptions C1, C2 and C3. The Assumption C1 is trivially satisfied in the context of recovering f fromf (\u03bb) as discussed in Section IV. Next, we show that Condition 1 implies C2 and C3. Note that the unique witness of Condition 1 implies C2 while C3 is a direct implication of linear independence of Condition 1. Therefore, we have established that the sparsest-fit algorithm recovers f fromf (\u03bb) under Condition 1.\nUnique Solution of \u2113 0 Optimization. To arrive at a contradiction, assume that there exists a function g :\nS n \u2192 R + such that\u011d(\u03bb) =f (\u03bb) and L \u25b3 = g \u21130 \u2264 f \u21130 = K. Let supp (f ) = {\u03c3 k \u2208 S n : 1 \u2264 k \u2264 K}, f (\u03c3 k ) = p k , 1 \u2264 k \u2264 K, supp (g) = {\u03c1 \u2113 \u2208 S n : 1 \u2264 \u2113 \u2264 L}, g(\u03c1 \u2113 ) = q \u2113 , 1 \u2264 \u2113 \u2264 L.\nBy hypothesis of Theorem III.1, f satisfies Condition 1. Therefore, entries of matrixf (\u03bb) contains the values p 1 , p 2 , . . . , p K . Also, by our assumptionf (\u03bb) =\u011d(\u03bb). Now, by definition, each entry of the matrix\u011d(\u03bb) is a summation of a subset of L numbers, q \u2113 , 1 \u2264 \u2113 \u2264 L. Therefore, it follows that for each\nk, 1 \u2264 k \u2264 K, we have p k = j\u2208T k q j ,\nfor some T k \u2286 {1, 2, . . . , L} .\nEquivalently,\np = Aq,(5)\nwhere p = [p k ] 1\u2264k\u2264K , q = [q \u2113 ] 1\u2264\u2113\u2264L A \u2208 {0, 1} K\u00d7L . Now consider the matrixf (\u03bb). As noted before, each of its entries is a summation of a subset of numbers p k , 1 \u2264 k \u2264 K. Further, each p k , 1 \u2264 k \u2264 K contributes to exactly D \u03bb distinct entries off (\u03bb). Therefore, it follows that the summation of all entries off (\u03bb\n) is D \u03bb (p 1 + \u2022 \u2022 \u2022 + p K ). That is, ijf (\u03bb) ij = D \u03bb K k=1 p k . Similarly, ij\u011d (\u03bb) ij = D \u03bb L \u2113=1 q \u2113 . Butf (\u03bb) =\u011d(\u03bb). Therefore, p \u2022 1 = q \u2022 1,(6)\nwhere 1 is vector of all 1s of appropriate dimension (we have abused the notation 1 here): in LHS, it is of dimension K, in RHS it is of dimension L. Also, from (5) we have\np \u2022 1 = Aq \u2022 1 = L \u2113=1 c \u2113 q \u2113 ,(7)\nfor some c j \u2208 Z + . From ( 6) and ( 7), it follows that\nj q j = j c j q j .(8)\nNow, there are two options: (1) either all the c \u2113 s are > 0, or (2) some of them are equal to zero. In the case (1), when c \u2113 > 0 for all 1 \u2264 \u2113 \u2264 L, it follows that c \u2113 = 1 for each 1 \u2264 \u2113 \u2264 L; or else, RHS of (8) will be strictly larger than LHS since q \u2113 > 0 for all 1 \u2264 \u2113 \u2264 L by definition. Therefore, the matrix A in ( 5) must contain exactly one non-zero entry, i.e. 1, in each column. Since p k > 0 for all 1 \u2264 k \u2264 K, it follows that there must be at least K non-zero entries in A. Finally, since L \u2264 K, it follows that we must have L = K. In summary, it must be that A is a K \u00d7K matrix with each row and column having exactly one 1, and rest of the entries 0. That is, A is a permutation matrix. That is, p k , 1 \u2264 k \u2264 K is permutation of q 1 , . . . , q L with L = K. By relabeling the q \u2113 s, if required, without loss of generality, we assume that p k = q k , for 1 \u2264 k \u2264 K. Since\u011d(\u03bb) =f (\u03bb) and p k = q k for 1 \u2264 k \u2264 K, it follows that g also satisfies Condition 1. Therefore, the sparsest-fit algorithm accurately recovers g from\u011d(\u03bb). Since the input to the algorithm is only\u011d(\u03bb) and\u011d(\u03bb) =f (\u03bb), it follows that g = f and we have reached contradiction to our assumption that f is not the unique solution of optimization problem (1). Now consider the remaining case (2) and suppose that c \u2113 = 0 for some \u2113. Then, it follows that some of the columns in the A matrix are zeros. Removing those columns of A we can write p =\u00c3q, where\u00c3 is formed from A by removing the zero columns and q is formed from q by removing q \u2113 s such that c \u2113 = 0. LetL be the size ofq. Since at least one column was removed, L < L \u2264 K. The conditionL < K implies that the vector p lies in a lower dimensional space. Further,\u00c3 is a 0, 1 valued matrix. Therefore, it follows that p violates the linear independence property of Condition 1 resulting in a contradiction. This completes the proof of Theorem III.1.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "VI. PROOF OF THEOREM III.2", "text": "We prove this theorem by showing that when two permutations, say \u03c3 1 , \u03c3 2 , are chosen uniformly at random, with a high probability, the sum of their representation matrices M \u03bb (\u03c3 1 ) + M \u03bb (\u03c3 2 ) can be decomposed in at least two ways. For that, note that a permutation can be represented using cycle notation, e.g. for n = 4, the permutation 1 \u2192 2, 2 \u2192 1, 3 \u2192 4, 4 \u2192 3 can be represented as a composition of two cycles (12)(34). We call two cycles distinct if they have no elements in common, e.g. the cycles (12) and (34) are distinct. Given two permutations \u03c3 1 and \u03c3 2 , let \u03c3 1,2 = \u03c3 1 \u03c3 2 be their composition. Now consider two permutations \u03c3 1 and \u03c3 2 such that they have distinct cycles. For example, \u03c3 1 = (1, 2) and \u03c3 2 = (3, 4) are permutations with distinct cycles. Then \u03c3 1,2 = \u03c3 1 \u03c3 2 = (12)(34). We first prove the theorem for \u03bb = (n \u2212 1, 1) and then extend it to a general \u03bb; thus, we fix the partition \u03bb = (n \u2212 1, 1). Then, we have:\nM \u03bb (\u03c3 1 ) + M \u03bb (\u03c3 2 ) = M \u03bb (\u03c3 1,2 ) + M \u03bb (id)(9)\nwhere \u03c3 1 and \u03c3 2 have distinct cycles and id is the identity permutation. Now, assuming that p 1 \u2264 p 2 , consider the following:\np 1 M \u03bb (\u03c3 1 ) + p 2 M \u03bb (\u03c3 2 ) = p 1 M \u03bb (\u03c3 1,2 ) + p 1 M \u03bb (id) + (p 2 \u2212 p 1 )M \u03bb (\u03c3 2 ). Thus, givenf (\u03bb) = p 1 M \u03bb (\u03c3 1 ) + p 2 M \u03bb (\u03c3 2 )\n, it can be decomposed in two distinct ways with both having the same \u2113 1 norm. Of course, the same analysis can be carried out when f has a sparsity K. Thus, we conclude that whenever f has two permutations with distinct cycles in its support, the \u2113 1 minimization solution is not unique. Therefore, to establish claim of Theorem III.2, it is sufficient to prove that when we choose two permutations uniformly at random, they have distinct cycles with a high probability.\nTo this end, let E denote the event that two permutations chosen uniformly at random have distinct cycles. Since permutations are chosen uniformly at random, Pr (E ) can be computed by fixing one of the permutations to be id. Then, Pr (E ) is the probability that a permutation chosen at random has more than one cycle.\nLet us evaluate Pr (E c ). For that, consider a permutation having exactly one cycle with the cycle containing l elements.\nThe number of such permutations will be n l (l \u2212 1)!. This is because we can choose the l elements that form the cycle in n l ways and the l numbers can be arranged in the cycle in (l \u2212 1)! ways. Therefore,\nPr(E c ) = 1 n! n l=1 n l (l \u2212 1)! = n r=1 1 l(n \u2212 l)! (10)\nNow, without loss of generality let's assume that n is even. Then,\nn/2 l=1 1 l(n \u2212 l)! \u2264 n/2 l=1 1 n 2 ! = 1 n 2 \u2212 1 ! (11)\nThe other half of the sum becomes\nn l=n/2 1 l(n \u2212 l)! \u2264 n/2 k=0 1 n 2 k! \u2264 2 n \u221e k=0 1 k! \u2264 O(1) n (12\n)\nPutting everything together, we have\nPr(E ) \u2265 1 \u2212 Pr(E c ) \u2265 1 \u2212 1 n 2 \u2212 1 ! + O(1) n \u2192 1 as n \u2192 \u221e.\nThus, Theorem III.2 is true for \u03bb = (n \u2212 1, 1).\nIn order to extend the proof to a general \u03bb, we observe that the standard cycle notation for a permutation we discussed above can be extended to \u03bb partitions for a general \u03bb. Specifically, for any given \u03bb, observe that a permutation can be imagined as a perfect matching in a D \u03bb \u00d7 D \u03bb bipartite graph, which we call the \u03bb-bipartite graph and denote it by G \u03bb = (V \u03bb 1 \u00d7 V \u03bb 2 , E \u03bb ); here V \u03bb 1 and V \u03bb 2 respectively denote the left and right vertex sets with |V \u03bb 1 | = |V \u03bb 2 | = D \u03bb with a node for every \u03bb partition of n. Let t 1 , t 2 , . . . , t D \u03bb denote the D \u03bb \u03bb-partitions of n; then, the nodes in V \u03bb 1 and V \u03bb 2 can be labeled by t 1 , t 2 , . . . , t D \u03bb . Since every perfect matching in a bipartite graph can be decomposed into its corresponding distinct cycles (the cycles can be obtained by superposing the bipartite graph corresponding to identity permutation with the \u03bb-bipartite graph of the permutation), every permutation can be written as a combination of distinct cycles in its \u03bb-bipartite graph. The special case of this for \u03bb = (n\u22121, 1) is the standard cycle notation we discussed above; for brevity, we call the \u03bbbipartite graph for \u03bb = (n \u2212 1, 1) the standard bipartite graph.\nIn order to prove the theorem for a general \u03bb, using an argument similar to above, it can be shown that it is sufficient to prove that a randomly chosen permutation contains at least two distinct cycles in its \u03bb-bipartite graph with a high probability. For that, it is sufficient to prove that a permutation with at least two distinct cycles in its standard bipartite graph has at least two distinct cycles in its \u03bb-bipartite graph for any general \u03bb. The theorem then follows from the result we established above that a randomly chosen permutation has at least two distinct cycles in its standard bipartite graph with a high probability.\nTo that end, consider a permutation, \u03c3, with at least two distinct cycles in the standard bipartite graph. Let A := (a 1 , a 2 , . . . , a \u21131 ) and B := (b 1 , b 2 , . . . , b \u21132 ) denote the first two cycles in the standard bipartite graph; clearly, \u2113 1 \u2113 2 \u2265 2 and at least one of \u2113 1 , \u2113 2 is \u2264 n/2. Without loss of generality we assume that \u2113 2 \u2264 n/2. Let \u03bb = (\u03bb 1 , \u03bb 2 , . . . , \u03bb r ). Since \u03bb 1 \u2265 \u03bb 2 \u2265 . . . \u2265 \u03bb r , we have \u03bb r \u2264 n/2. First, we consider the case when \u03bb r < n/2. Now consider the \u03bb-partition, t 1 , of n constructed as follows: a 1 placed in the rth partition, a 2 in the first partition, all the elements of the second cycle b 1 , b 2 , . . . , b \u21132 arbitrarily in the first r \u2212 1 partitions and the rest placed arbitrarily. Note that such a construction is possible by the assumption on \u03bb r . Let t \u2032 1 denote \u03c3(t 1 ); then, t \u2032 1 = t 1 because t 1 does not contain a 2 in the rth partition while t \u2032 1 contains \u03c3(a 1 ) = a 2 in the rth partition. Thus, the partition t 1 belongs to a cycle that has a length of at least 2 partitions. Thus, we have found one cycle, which we denote by C 1 . Now consider a second partition t 2 constructed as follows: b 1 placed in the rth partition, b 2 in the first and the rest placed arbitrarily. Again, note that \u03c3(t 2 ) = t 2 . Thus, t 2 belongs to a cycle of length at least 2, which we denote by C 2 . Now we have found two cycles C 1 , C 2 , and we are left with proving that they are distinct. In order to establish the cycles are distinct, note that none of the partitions in cycle C 1 can be t 2 . This is true because, by construction, t 2 contains b 1 in the rth partition while none of the partitions in C 1 can contain any elements from the cycle B in the rth partition. This finishes the proof for all \u03bb such that \u03bb r < n/2.\nWe now consider the case when \u03bb r = n/2. Since \u03bb 1 \u2265 \u03bb r , it follows that r = 2 and \u03bb = (n/2, n/2). For \u2113 2 < n/2, it is still feasible to construct t 1 and t 2 , and the theorem follows from the arguments above. Now we consider the case when \u2113 1 = \u2113 2 = n/2; let \u2113 := \u2113 1 = \u2113 2 . Note that now it is infeasible to construct t 1 as described above. Therefore, we consider t 1 = {a 1 , b 2 , . . . , b \u2113 } {b 1 , a 2 , . . . , a \u2113 } and t 2 = {b 1 , a 2 , . . . , a \u2113 } {a 1 , b 2 , . . . , b \u2113 }. Clearly, t 1 = t 2 , \u03c3(t 1 ) = t 1 and \u03c3(t 2 ) = t 2 . Thus, t 1 and t 2 belong to two cycles, C 1 and C 2 , each with length at least 2. It is easy to see that these cycles are also distinct because every \u03bb\u2212partition in the cycle C 1 will have only one element from cycle A in the first partition and, hence, C 1 cannot contain the \u03bb\u2212partition t 2 . This completes the proof of the theorem.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "VII. PROOF OF THEOREM III.3 :", "text": "\u03bb = (n \u2212 1, 1)\nOur interest is in recovering a random function f from partial informationf (\u03bb). To this end, let\nK = f 0 , supp (f ) = {\u03c3 k \u2208 S n : 1 \u2264 k \u2264 K}, and f (\u03c3 k ) = p k , 1 \u2264 k \u2264 K.\nHere \u03c3 k and p k are randomly chosen as per the random model R(K, C ) described in Section II. For \u03bb = (n \u2212 1, 1), D \u03bb = n; thenf (\u03bb) is an n \u00d7 n matrix with its (i, j)th entry bein\u011d\nf (\u03bb) ij = k:\u03c3 k (j)=i p k , for 1 \u2264 i, j \u2264 n.\nTo establish Theorem III.3, we prove that as long as K \u2264 C 1 n log n with C 1 = 1\u2212\u03b5, f can be recovered by the sparsestfit algorithm with probability 1 \u2212 o(1) for any fixed \u03b5 > 0. Specifically, we show that for K \u2264 C 1 n log n, Condition 1 is satisfied with probability 1 \u2212 o(1), which in turn implies that the sparsest-fit algorithm recovers f as per Theorem III.1. Note that the \"linear independence\" property of Condition 1 is satisfied with probability 1 under R(K, C ) as p k are chosen from a distribution with continuous support. Therefore, we are left with establishing \"unique witness\" property.\nTo this end, let 4\u03b4 = \u03b5 so that C 1 \u2264 1 \u2212 4\u03b4. Let E k be the event that \u03c3 k satisfies the unique witness property, 1 \u2264 k \u2264 K. Under R(K, C ), since K permutations are chosen from S n independently and uniformly at random, it follows that Pr(E k ) is the same for all k. Therefore, by union bound, it is sufficient to establish that K Pr(E c 1 ) = o(1). Since we are interested in K = O(n log n), it is sufficient to establish Pr(E c 1 ) = O(1/n 2 ). Finally, once again due the symmetry, it is sufficient to evaluate Pr(E 1 ) assuming \u03c3 1 = id, i.e. \u03c3 1 (i) = i for all 1 \u2264 i \u2264 n. Define\nF j = {\u03c3 k (j) = j, for 2 \u2264 k \u2264 K}, for 1 \u2264 j \u2264 n.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "It then follows that", "text": "Pr(E 1 ) = Pr \u222a n j=1 F j . Therefore, for any L \u2264 n, we have\nPr(E c 1 ) = Pr \u2229 n j=1 F c j \u2264 Pr \u2229 L j=1 F c j = Pr (F c 1 ) \uf8ee \uf8f0 L j=2 Pr F c j \u2229 j\u22121 \u2113=1 F c \u2113 \uf8f9 \uf8fb . (13\n)\nNext we show that for the selection of L = n 1\u2212\u03b4 , the RHS of ( 13) is bounded above by exp(\u2212n \u03b4 ) = O(1/n 2 ). That will complete the proof of achievability.\nFor that, we start by bounding Pr(F c 1 ):\nPr (F c 1 ) = 1 \u2212 Pr (F 1 ) = 1 \u2212 1 \u2212 1 n K\u22121 .(14)\nThe last equality follows because all permutations are chosen uniformly at random. For j \u2265 2, we now evaluate Pr F c j \u2229 j\u22121 \u2113=1 F c \u2113 . Given \u2229 j\u22121 \u2113=1 F c \u2113 , for any k, 2 \u2264 k \u2264 K, \u03c3 k (j) will take a value from n\u2212j+1 values, possibly including j, uniformly at random. Thus, we obtain the following bound:\nPr F c j \u2229 j\u22121 \u2113=1 F c \u2113 \u2264 1 \u2212 1 \u2212 1 n \u2212 j + 1 K\u22121 .(15)\nFrom ( 13)- (15), we obtain that\nPr(E c 1 ) \u2264 L j=1 1 \u2212 1 \u2212 1 n \u2212 j + 1 K\u22121 \u2264 1 \u2212 1 \u2212 1 n \u2212 L K L \u2264 1 \u2212 1 \u2212 1 n \u2212 L C1n log n L ,(16)\nwhere we have used K \u2264 C 1 n log n in the last inequality.\nSince\nL = n 1\u2212\u03b4 , n \u2212 L = n(1 \u2212 o(1)). Using the standard fact 1 \u2212 x = e \u2212x (1 + O(x 2 )) for small x \u2208 [0, 1), we have 1 \u2212 1 n \u2212 L = exp \u2212 1 n \u2212 L 1 + O 1 n 2 . (17\n)\nFinally, observe that\n1 + O 1 n 2 C1n log n = \u0398(1).\nTherefore, from ( 16) and ( 17), it follows that\nPr(E c 1 ) \u2264 1 \u2212 \u0398 exp \u2212 C 1 log n 1 \u2212 n \u2212\u03b4 L \u2264 [1 \u2212 \u0398 (exp (\u2212(C 1 + \u03b4) log n))] L = 1 \u2212 \u0398 1 n C1+\u03b4 L \u2264 exp \u2212\u0398 L n C1+\u03b4 = exp \u2212\u2126(n 2\u03b4 ) ,(18)\nwhere we have used the fact that 1 18), it follows that Pr(E 1 ) = O(1/n 2 ). This completes the proof of achievability.\n\u2212 x \u2264 e \u2212x for x \u2208 [0, 1] and L = n 1\u2212\u03b4 , C 1 \u2264 1 \u2212 4\u03b4. From (\nVIII. PROOF OF THEOREM III.4 :\n\u03bb = (n \u2212 m, m)\nOur interest is in recovering the random function f from partial informationf (\u03bb). As in proof of Theorem III.3, we use the notation\nK = f 0 , supp (f ) = {\u03c3 k \u2208 S n : 1 \u2264 k \u2264 K}, and f (\u03c3 k ) = p k , 1 \u2264 k \u2264 K.\nHere \u03c3 k and p k are randomly chosen as per the random model\nR(K, C ) described in Section II. For \u03bb = (n \u2212 m, m), D \u03bb = n! (n\u2212m)!m! \u223c n m andf (\u03bb) is an D \u03bb \u00d7 D \u03bb matrix.\nTo establish Theorem III.4, we shall prove that as long as K \u2264 C 1 n m log n with 0 < C 1 < 1 m! a constant, f can be recovered by the sparsest-fit algorithm with probability 1 \u2212 o(1). We shall do so by verifying that the Condition 1 holds with probability 1\u2212o(1), so that the sparsest-fit algorithm will recover f as per Theorem III.1. As noted earlier, the \"linear independence\" of Condition 1 is satisfied with probability 1 under R(K, C ). Therefore, we are left with establishing the \"unique witness\" property.\nTo this end, for the purpose of bounding, without loss of generality, let us assume that K = (1\u22122\u03b4) m! n m log n for some \u03b4 > 0. Set L = n 1\u2212\u03b4 . Following arguments similar to those in the proof of Theorem III.3, it will be sufficient to establish that\nPr(E c 1 = O(1/n 2m )\n; where E 1 is the event that permutation \u03c3 1 = id satisfies the unique witness property.\nTo this end, recall thatf (\u03bb) is a D \u03bb \u00d7 D \u03bb matrix. Each row (and column) of this matrix corresponds to a distinct \u03bb partition of n : t i , 1 \u2264 i \u2264 D \u03bb . Without loss of generality, let us order the D \u03bb \u03bb partitions of n so that the ith partition, t i , is defined as follows: t 1 = {1, . . . , n\u2212 m}{n\u2212 m+ 1, . . . , n}, and for 2 \u2264 i \u2264 L,\nt i = {1, . . . , n \u2212 im, n \u2212 (i \u2212 1)m + 1, . . . , n} {n \u2212 im + 1, . . . , n \u2212 (i \u2212 1)m}.\nNote that since \u03c3 1 = id, we have \u03c3\n1 (t i ) = t i for all 1 \u2264 i \u2264 D \u03bb . Define F j = {\u03c3 k (t j ) = t j , for 2 \u2264 k \u2264 K}, for 1 \u2264 j \u2264 D \u03bb .\nThen it follows that\nPr(E 1 ) = Pr \u222a D \u03bb j=1 F j .\nTherefore,\nPr(E c 1 ) = Pr \u2229 D \u03bb j=1 F c j \u2264 Pr \u2229 L j=1 F c j = Pr (F c 1 ) \uf8ee \uf8f0 L j=2 Pr F c j \u2229 j\u22121 \u2113=1 F c \u2113 \uf8f9 \uf8fb . (19\n)\nFirst, we bound Pr(F c 1 ). Each permutation \u03c3 k , k = 1, maps t 1 = {1, . . . , n \u2212 m}{n \u2212 m + 1, . . ., n} to {\u03c3 k (1), . . . \u03c3 k (n \u2212 m)}{\u03c3 k (n \u2212 m + 1), . . . , \u03c3 k (n)}. Therefore, \u03c3 k (t 1 ) = t 1 iff \u03c3 k maps set of elements {n \u2212 m + 1, . . . , n} to the same set of elements. Therefore,\nPr (\u03c3 k (t 1 ) = t 1 ) = 1 n m = m! m\u22121 \u2113=0 (n \u2212 \u2113) . \u2264 m! (n \u2212 Lm) m .(20)\nTherefore, it follows that\nPr (F c 1 ) = 1 \u2212 Pr (F 1 ) = 1 \u2212 Pr (\u03c3 k (t 1 ) = t 1 , 2 \u2264 k \u2264 K) = 1 \u2212 K k=2 (1 \u2212 Pr (\u03c3 k (t 1 ) = t 1 )) \u2264 1 \u2212 1 \u2212 m! (n \u2212 Lm) m K . (21\n)\nNext we evaluate Pr F c j\n\u2229 j\u22121 \u2113=1 F c \u2113 for 2 \u2264 j \u2264 L.\nGiven \u2229 j\u22121 \u2113=1 F c \u2113 , we have (at least partial) information about the action of \u03c3 k , 2 \u2264 k \u2264 K over elements {n \u2212 (j \u2212 1)m + 1, . . . , n}. Conditional on this, we are interested in the action of \u03c3 k on t j , i.e. {n \u2212 jm + 1, . . . , n \u2212 jm + m}. Specifically, we want to (upper) bound the probability that these elements are mapped to themselves. Given \u2229 j\u22121 \u2113=1 F c \u2113 , each \u03c3 k will map {n \u2212 jm + 1, . . . , n \u2212 jm + m} to one of the n\u2212(j\u22121)m m possibilities with equal probability. Further, {n \u2212 jm + 1, . . . , n \u2212 jm + m} is not a possibility. Therefore, for the purpose of upper bound, we obtain that\nPr F c j \u2229 j\u22121 \u2113=1 F c \u2113 \u2264 1 \u2212 1 \u2212 1 n\u2212(j\u22121)m m K\u22121 \u2264 1 \u2212 1 \u2212 m! (n \u2212 Lm) m K .(22)\nFrom ( 19)- (22), we obtain that\nPr(E c 1 ) \u2264 1 \u2212 1 \u2212 m! (n \u2212 Lm) m K L . (23) Now Lm = o(n) and hence n \u2212 Lm = n(1 \u2212 o(1)). Using 1 \u2212 x = e \u2212x (1 + O(x 2 )) for small x \u2208 [0, 1), we have 1 \u2212 m! (n \u2212 Lm) m = exp \u2212 m! (n \u2212 Lm) m 1 + O 1 n 2m .(24)\nFinally, observe that since K = O(n m log n),\n1 + O 1 n 2m K = \u0398(1).\nThus, from ( 23) and (24), it follows that\nPr(E c 1 ) \u2264 1 \u2212 \u0398 exp \u2212 Km! n m (1 \u2212 Lm/n) m L \u2264 1 \u2212 \u0398 exp \u2212 (1 \u2212 2\u03b4) log n (1 \u2212 n \u2212\u03b4 m) m L \u2264 [1 \u2212 \u0398 (exp (\u2212(1 \u2212 3\u03b4/2) log n))] L \u2264 1 \u2212 \u0398 1 n 1\u22123\u03b4/2 L \u2264 exp \u2212\u2126(Ln \u22121+3\u03b4/2 ) \u2264 exp \u2212\u2126(n \u03b4/2 ) = O 1 n 2m .(25)\nIn above, we have used the fact that 1\u2212x \u2264 e \u2212x for x \u2208 [0, 1] and choice of L = n 1\u2212\u03b4 . This completes the proof of Theorem III.4.\nIX. PROOF OF THEOREM III.5: \u03bb 1 = n \u2212 n \u2212\u03b4 for some \u03b4 > 0. For such \u03bb = (\u03bb 1 , . . . , \u03bb r ),\nD \u03bb = n! r i=1 \u03bb i ! \u2264 n! \u03bb 1 ! \u2264 n n\u2212\u03bb1 = n \u00b5 . (26\n)\nOur interest is in the case when K \u2264 (1 \u2212 \u03b5)D \u03bb log log D \u03bb for any \u03b5 > 0. For this, the structure of arguments will be similar to those used in Theorems III.3 and III.4. Specifically, it will be sufficient to establish that Pr(\nE c 1 ) = O(1/D 2 \u03bb )\n, where E 1 is the event that permutation \u03c3 1 = id satisfies the unique witness property.\nTo this end, we order the rows (and corresponding columns) of the D \u03bb \u00d7 D \u03bb matrixf (\u03bb) in a specific manner. Specifically, we are interested in the L = 3n partition t 2 corresponds to the one in which the first part contains the \u03bb 1 elements {1, . . . , n \u2212 2\u00b5, n \u2212 \u00b5 + 1, . . . , n}, while the other r \u2212 1 parts contain {n \u2212 2\u00b5 + 1, . . . , n \u2212 \u00b5} in that order. More generally, for 3 \u2264 \u2113 \u2264 L, t \u2113 contains {1, . . . , n \u2212 \u2113\u00b5, n \u2212 (\u2113 \u2212 1)\u00b5 + 1, . . . , n} in the first partition and remaining elements {n \u2212 \u2113\u00b5 + 1, . . . , n \u2212 (\u2113 \u2212 1)\u00b5} in the rest of the r \u2212 1 parts in that order. By our choice of L, L\u00b5 = o(n) and, hence, the above is well defined. Next, we bound Pr(E c 1 ) using these L rows. Now \u03c3 1 = id and hence \u03c3 1 (t i ) = t i for all 1 \u2264 i \u2264 D \u03bb . Define\nF j = {\u03c3 k (t j ) = t j , for 2 \u2264 k \u2264 K}, for 1 \u2264 j \u2264 D \u03bb .\nThen it follows that\nPr(E 1 ) = Pr \u222a D \u03bb j=1 F j .\nTherefore,\nPr(E c 1 ) = Pr \u2229 D \u03bb j=1 F c j \u2264 Pr \u2229 L j=1 F c j = Pr (F c 1 ) \uf8ee \uf8f0 L j=2 Pr F c j \u2229 j\u22121 \u2113=1 F c \u2113 \uf8f9 \uf8fb . (27)\nFirst, we bound Pr(F c 1 ). Each permutation \u03c3 k , 1 \u2264 k \u2264 K maps t 1 to one of the D \u03bb possible other \u03bb partitions with equal probability. Therefore, it follows that\nPr (\u03c3 k (t 1 ) = t 1 ) = 1 D \u03bb . (28\n)\nThus,\nPr (F c 1 ) = 1 \u2212 Pr (F 1 ) = 1 \u2212 Pr (\u03c3 k (t 1 ) = t 1 , 2 \u2264 k \u2264 K) = 1 \u2212 K k=2 (1 \u2212 Pr (\u03c3 k (t 1 ) = t 1 )) = 1 \u2212 1 \u2212 1 D \u03bb K . (29\n)\nNext we evaluate Pr F c j \u2229 j\u22121 \u2113=1 F c \u2113 for 2 \u2264 j \u2264 L. Given \u2229 j\u22121 \u2113=1 F c \u2113 ,\nwe have (at least partial) information about the action of \u03c3 k , 2 \u2264 k \u2264 K over elements {n\u2212(j\u22121)\u00b5+1, . . . , n}. Conditional on this, we are interested in the action of \u03c3 k on t j . Given the partial information, each of the \u03c3 k will map t j to one of at least D \u03bb(j) different options with equal probability for \u03bb(j) = (\u03bb 1 \u2212 (j \u2212 1)\u00b5, \u03bb 2 , . . . , \u03bb r ) -this is because the elements 1, . . . , \u03bb 1 \u2212 (j \u2212 1)\u00b5 in the first part and all elements in the remaining r \u2212 1 parts are mapped completely randomly conditional on \u2229 j\u22121 \u2113=1 F c \u2113 . Therefore, it follows that\nPr F c j \u2229 j\u22121 \u2113=1 F c \u2113 \u2264 1 \u2212 1 \u2212 1 D \u03bb(j) K . (30)\nFrom ( 27)-(30), we obtain that\nPr(E c 1 ) \u2264 L j=1 1 \u2212 1 \u2212 1 D \u03bb(j) K \u2264 1 \u2212 1 \u2212 1 D \u03bb(L) K L .(31)\nIn above we have used the fact that\nD \u03bb = D \u03bb(1) \u2265 . . . \u2265 D \u03bb(L) . Consider D \u03bb(j) D \u03bb(j+1) = (n \u2212 (j \u2212 1)\u00b5)! (\u03bb 1 \u2212 j\u00b5)! (n \u2212 j\u00b5)! (\u03bb 1 \u2212 (j \u2212 1)\u00b5)! = \u00b5\u22121 \u2113=0 (n \u2212 (j \u2212 1)\u00b5 \u2212 \u2113) (\u03bb 1 \u2212 (j \u2212 1)\u00b5 \u2212 \u2113) = n \u03bb 1 \u00b5 \u00b5\u22121 \u2113=0 1 \u2212 (j\u22121)\u00b5\u2212\u2113 n 1 \u2212 (j\u22121)\u00b5\u2212\u2113 \u03bb1 (32)\nTherefore, it follows that\nD \u03bb(1) D \u03bb(L) = n \u03bb 1 (L\u22121)\u00b5 (L\u22121)\u00b5 \u2113=0 1 \u2212 \u2113 n 1 \u2212 \u2113 \u03bb1 . (33\n)\nUsing 1 + x \u2264 e x for any x \u2208 (\u22121, 1), 1 \u2212 x \u2265 e \u22122x for\nx \u2208 (0, 1/2) and L\u00b5 = o(n), we have that for any \u2113,\n0 \u2264 \u2113 \u2264 (L \u2212 1)\u00b5 1 \u2212 \u2113 n 1 \u2212 \u2113 \u03bb1 = 1 \u2212 \u2113 n + \u2113 \u03bb1 \u2212 \u2113 2 n\u03bb1 1 \u2212 \u2113 2 \u03bb 2 1 \u2264 exp \u2212 \u2113 2 \u2212 \u2113\u00b5 n\u03bb 1 + 2\u2113 2 \u03bb 2 1 \u2264 exp \u2113\u00b5 n\u03bb 1 + 2\u2113 2 \u03bb 2 1 .(34)\nTherefore, we obtain\nD \u03bb(1) D \u03bb(L) \u2264 n \u03bb 1 L\u00b5 exp \u0398 L 2 \u00b5 3 n\u03bb 1 + 2L 3 \u00b5 3 \u03bb 2 1 .(35) Now n \u03bb 1 L\u00b5 = 1 + \u00b5 \u03bb 1 L\u00b5 \u2264 exp L\u00b5 2 \u03bb 1 . (36\n)\nIt can be checked that for given choice of L, \u00b5, we have\nL\u00b5 2 = o(\u03bb 1 ), L 3 \u00b5 3 = o(\u03bb 2 1\n) and L 2 \u00b5 3 = o(n\u03bb 1 ). Therefore, in summary we have that\nD \u03bb(1) D \u03bb(L) = 1 + o(1). (37\n)\nUsing similar approximations to evaluate the bound on RHS of (31) along with (26) yields,\nPr(E c 1 ) \u2264 exp \u2212L exp \u2212 K D \u03bb(L) = exp (\u2212L exp (\u2212(1 \u2212 \u03b5) log log D \u03bb (1 + o(1)))) \u2264 exp (\u2212L exp (\u2212 log log D \u03bb )) = exp \u2212 L log D \u03bb = exp \u2212 3n 4 9 \u22122\u03b4 log 3 n log D \u03bb \u2264 exp (\u22122 log D \u03bb ) = 1 D 2 \u03bb .(38)\nThis completes the proof of Theorem III.5.", "publication_ref": ["b14", "b21", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "X. PROOF OF THEOREM III.6: GENERAL \u03bb", "text": "We shall establish the bound on sparsity up to which recovery of f is possible fromf (\u03bb) using the sparsest-fit algorithm for general \u03bb.\nLet \u03bb = (\u03bb 1 , . . . , \u03bb r ), r \u2265 2 with \u03bb 1 \u2265 \u2022 \u2022 \u2022 \u2265 \u03bb r \u2265 1. As before, let K = f 0 , supp (f ) = {\u03c3 k \u2208 S n : 1 \u2264 k \u2264 K}, and f (\u03c3 k ) = p k , 1 \u2264 k \u2264 K.\nHere \u03c3 k and p k are randomly chosen as per the random model R(K, C ) described in Section II. And, we are given partial informationf (\u03bb) which is D \u03bb \u00d7 D \u03bb matrix with\nD \u03bb = n! r i=1 \u03bb i ! .\nFinally, recall definition \u03b1 = (\u03b1 i ) 1\u2264i\u2264r with\n\u03b1 i = \u03bb i /n, 1 \u2264 i \u2264 r, H(\u03b1) = \u2212 r i=1 \u03b1 i log \u03b1 i , and H \u2032 (\u03b1) = \u2212 r i=2 \u03b1 i log \u03b1 i .\nAs usual, to establish that the sparsest-fit algorithm recovers f fromf (\u03bb), we will need to establish \"unique witness\" property as \"linear independence\" is satisfied due to choice of p k s as per random model R(K, C ).\nFor the ease of exposition, we will need an additional notation of \u03bb-bipartite graph: it is a complete bipartite graph\nG \u03bb = (V \u03bb 1 \u00d7V \u03bb 2 , E \u03bb ) with vertices V \u03bb 1 , V \u03bb 2\nhaving a node each for a distinct \u03bb partition of n and thus |V \u03bb\n1 | = |V \u03bb 2 | = D \u03bb .\nAction of a permutation \u03c3 \u2208 S n , represented by a 0/1 valued D \u03bb \u00d7 D \u03bb matrix, is equivalent to a perfect matching in G \u03bb . In this notation, a permutation \u03c3 has \"unique witness\" with respect to a collection of permutations, if and only if there is an edge in the matching corresponding to \u03c3 that is not present in any other permutation's matching.\nLet E L denote the event that L \u2265 2 permutations chosen uniformly at random satisfy the \"unique witness\" property. To establish Theorem III.6, we wish to show that Pr(E c K ) = o(1) as long as K \u2264 K * 1 (\u03bb) where K * 1 (\u03bb) is defined as per (3). To do so, we shall study Pr(E c L+1 |E L ) for L \u2265 1. Now consider the bipartite graph, G \u03bb L , which is subgraph of G \u03bb , formed by the superimposition of the perfect matchings corresponding to the L random permutations, \u03c3 i , 1 \u2264 i \u2264 L. Now, the probability of E c L+1 given that E L has happened is equal to the probability that a new permutation, generated uniformly at random, has its perfect matching so that all its edges end up overlapping with those of G \u03bb L . Therefore, in order to evaluate this probability we count the number such permutations.\nFor the ease of exposition, we will first count the number of such permutations for the cases when \u03bb = (n \u2212 1, 1) followed by \u03bb = (n \u2212 2, 2). Later, we shall extend the analysis to a general \u03bb. As mentioned before, for \u03bb = (n \u2212 1, 1), the corresponding G \u03bb is a complete graph with n nodes on left and right. With a bit of abuse of notation, the left and right vertices be labeled 1, 2, . . . , n. Now each permutation, say \u03c3 \u2208 S n , corresponds to a perfect matching in G \u03bb with an edge from left i to right j if and only if \u03c3(i) = j. Now, consider G \u03bb L , the superimposition of all the perfect matching of the given L permutations. We want to count (or obtain an upper bound on) the number of permutations that will have corresponding perfect matching so that all of its edges overlap with edges of G \u03bb L . Now each permutation maps a vertex on left to a vertex on right. In the graph G \u03bb L , each vertex i on the left has degree of at most L. Therefore, if we wish a choose a permutation so that all of its perfect matching's edges overlap with those of G L \u03bb , it has at most L choices for each vertex on left. There are n vertices in total on left. Therefore, total number of choices are bounded above by L n . From this, we conclude that for\n\u03bb = (n \u2212 1, 1), Pr(E c L+1 |E L ) \u2264 L n n! .\nIn a similar manner, when \u03bb = (n \u2212 2, 2), the complete bipartite graph G \u03bb has D \u03bb = n 2 nodes on the left and right; each permutation corresponds to a perfect matching in this graph. We label each vertex, on left and right, in G \u03bb by unordered pairs {i, j}, for 1 \u2264 i < j \u2264 n. Again, we wish to bound given Pr(E c L+1 |E L ). For this, let G \u03bb L , a subgraph of G \u03bb , be obtained by the union of edges that belong to the perfect matchings of given L permutations. We would like to count the number possible permutations that will have corresponding matching with edges overlapping with those of G \u03bb L . For this, we consider the \u230an/2\u230b pairs {1, 2} , {3, 4} , . . . , {2\u230an/2\u230b \u2212 1, 2\u230an/2\u230b}. Now if n is even then they end up covering all n elements. If not, we consider the last, nth element, {n} as an additional set. Now using a similar argument as before, we conclude that there are at most L \u230an/2\u230b ways of mapping each of these \u230an/2\u230b pairs such that all of these edges overlap with the edges of G \u03bb L . Note that this mapping fixes what each of these \u2308n/2\u2309 unordered pairs get mapped to. Given this mapping, there are 2! ways of fixing the order in each unordered pair. For example, if an unordered pair {i, j} maps to unordered pair {k, l} there there are 2! = 2 options: : i \u2192 k, j \u2192 l or i \u2192 l, j \u2192 k. Thus, once we fix the mapping of each of the \u2308n/2\u2309 disjoint unordered pairs, there can be at most (2!) \u2308n/2\u2309 permutations with the given mapping of unordered pairs. Finally, note that once the mapping of these \u230an/2\u230b pairs is decided, if n is even that there is no element that is left to be mapped. For n odd, since mapping of the n \u2212 1 elements is decided, so is that of {n}. Therefore, in summary in both even n or odd n case, there are at most L \u230an/2\u230b (2!) \u2308n/2\u2309 permutations that have all of the edge of corresponding perfect matching in G \u03bb overlapping with the edges of G \u03bb L . Therefore, As before, for the purpose of upper bounding the number of permutations that have corresponding perfect matchings in G \u03bb overlapping with edges of G \u03bb L , each of the first M partitions can be mapped in L different ways; in total at most L M ways. For each of these mappings, we have options at the most\nPr(E c L+1 |E L ) \u2264 L \u230an/2\u230b (2!) \u230an/2\u230b n! .\n(\u03bb 2 !\u03bb 3 ! . . . \u03bb r !) M .\nGiven the mapping of the first M partitions, the mapping of the N elements of the M +1st partition is determined (without ordering). Therefore, the additional choice is at most N !. In summary, the total number of permutations can be at most\nL M r i=2 \u03bb i ! M N !.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Using this bound, we obtain", "text": "Pr E c L+1 |E L \u2264 1 n! L M r i=2 \u03bb i ! M N !.(39)\nLet,\nx L \u25b3 = 1 n! L M r i=2 \u03bb i ! M N !.\nNote that E k+1 \u2282 E k for k \u2265 1. Therefore, it follows that\nPr (E K ) = Pr (E K \u2229 E K\u22121 ) = Pr (E K |E K\u22121 ) Pr (E K\u22121 ) . (40\n)\nRecursive application of argument behind (40) and fact that Pr(E 1 ) = 1, we have\nPr (E K ) = Pr (E 1 ) K\u22121 L=1 Pr (E L+1 |E L ) = K\u22121 L=1 1 \u2212 Pr E c L+1 |E L = K\u22121 L=1 (1 \u2212 x L ) \u2265 1 \u2212 K\u22121 L=1 x L . (41\n)\nUsing (39), it follows that x k+1 \u2265 x k for k \u2265 1. Therefore,\nK L=2 x L \u2264 Kx K \u2264 1 n! K M+1 r i=2 \u03bb i ! M N ! = 1 n! K M+1 n! \u03bb 1 !D \u03bb M N ! = K M+1 D M \u03bb n! \u03bb 1 ! M N ! n! = K M+1 D M \u03bb n! \u03bb 1 !(n \u2212 \u03bb 1 )! M N !((n \u2212 \u03bb 1 )!) M n! . (42\n)\nSince n = N + M (n \u2212 \u03bb 1 ), we have a binomial and a multinomial coefficient in RHS of (42). We simplify this expression by obtaining an approximation for a multinomial coefficient through Stirling's approximation. Thus, we can write\nM log n! \u03bb 1 !(n \u2212 \u03bb 1 )! = M n\u03b1 1 log 1 \u03b1 1 + M n(1 \u2212 \u03b1 1 ) log 1 1 \u2212 \u03b1 1 (43) + 0.5 log 1 n M \u03b1 M 1 (1 \u2212 \u03b1 1 ) M \u2212 O(M )\nwhere \u03b1 1 = \u03bb 1 /n. Similarly, we can write Observe that |X | = (n!) K . Therefore, using Stirling's approximation, it follows that\nlog|X | = (1 + o(1))Kn log n.(62)\nNow Y =f (\u03bb) is a D \u03bb \u00d7 D \u03bb matrix. Let Y = [Y ij ] with Y ij , 1 \u2264 i, j \u2264 D \u03bb , taking values in {1, . . . , KT }; it is easy to see that H(Y ij ) \u2264 log KT . Therefore, it follows that\nH(Y ) \u2264 D \u03bb i,j=1\nH(Y ij ) \u2264 D 2 \u03bb log KT = D 2 \u03bb (log K + log T ) . (63) For small enough constant \u03b4 > 0, it is easy to see that the condition of (61) will follow if K satisfies the following two inequalities:\nD 2 \u03bb log K Kn log n \u2264 1 3 (1 + \u03b4) \u21d0 K log K \u2265 3(1 \u2212 \u03b4/2)D 2 \u03bb n log n ,(64)\nD 2 \u03bb log T Kn log n \u2264 1 3 (1 + \u03b4) \u21d0 K \u2265 3(1 \u2212 \u03b4/2)D 2 \u03bb log T n log n .(65)\nIn order to obtain a bound on K from (64), consider the following: for large numbers x, y, let y = (c+\u03b5)x log x, for some constants c, \u03b5 > 0. Then, log y = log x + log log x + log(c + \u03b5) which is (1 + o(1)) log x. Therefore,\ny log y = c + \u03b5 1 + o(1)\nx \u2265 cx,\nfor x \u2192 \u221e and constants c, \u03b5 > 0. Also, observe that y/ log y is a non-decreasing function; hence, it follows that for y \u2265 (c+\u03b5)x log x, y/ log y \u2265 cx for large x. Now take x = D 2 \u03bb n log n , c = 3, \u03b5 = 1 and y = K. Note that D \u03bb \u2265 n for all \u03bb of interest; therefore, x \u2192 \u221e as n \u2192 \u221e. Hence, (64) is satisfied for the choice of\nK \u2265 4D 2 \u03bb n log n log D 2 \u03bb n log n .(67)\nFrom ( 61), ( 64), (65), and (67) it follows that the probability of error of any algorithm is at least \u03b4 > 0 for n large enough and any \u03bb if\nK \u2265 4D 2 \u03bb n log n log D 2 \u03bb n log n \u2228 T .(68)\nThis completes the proof of Theorem III.7.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "XII. CONCLUSION", "text": "In summary, we considered the problem of exactly recovering a non-negative function over the space of permutations from a given partial set of Fourier coefficients. This problem is motivated by the wide ranging applications it has across several disciplines. This problem has been widely studied in the context of discrete-time functions in the recently popular compressive sensing literature. However, unlike our setup, where we want to perform exact recovery from a given set of Fourier coefficients, the work in the existing literature pertains to the choice of a limited set of Fourier coefficients that can be used to perform exact recovery.\nInspired by the work of Donoho and Stark [1] in the context of discrete-time functions, we focused on the recovery of non-negative functions with a sparse support (support size \u226a domain size). Our recovery scheme consisted of finding the function with the sparsest support, consistent with the given information, through \u2113 0 optimization. As we showed through some counter-examples, this procedure, however, will not recover the exact solution in all the cases. Thus, we identified sufficient conditions under which a function can be recovered through \u2113 0 optimization. For each kind of partial information, we then quantified the sufficient conditions in terms of the \"complexity\" of the functions that can be recovered. Since the sparsity (support size) of a function is a natural measure of its complexity, we quantified the sufficient conditions in terms of the sparsity of the function. In particular, we proposed a natural random generative model for the functions of a given sparsity. Then, we derived bounds on sparsity for which a function generated according to the random model satisfies the sufficient conditions with a high probability as n \u2192 \u221e. Specifically, we showed that, for partial information corresponding to partition \u03bb, the sparsity bound essentially scales as D M/(M+1) \u03bb\n. For \u03bb 1 /n \u2192 1, this bound essentially becomes D \u03bb and for \u03bb 1 /n \u2192 0, the bound essentially becomes D 1/2 \u03bb . Even though we found sufficient conditions for the recoverability of functions by finding the sparsest solution, \u2113 0 optimization is in general computationally hard to carry out. This problem is typically overcome by considering its convex relaxation, the \u2113 1 optimization problem. However, we showed that \u2113 1 optimization fails to recover a function generated by the random model with a high probability. Thus, we proposed a novel iterative algorithm to perform \u2113 0 optimization for functions that satisfy the sufficient conditions, and extended it to the general case when the underlying distribution may not satisfy the sufficient conditions and the observations maybe noisy.\nWe studied the limitation of any recovery algorithm by means of information theoretic tools. While the bounds we obtained are useful in general, due to technical limitations, they do not apply to the random model we considered. Closing this gap and understanding recovery conditions in the presence of noise are natural next steps.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Since \u03b4 < 1, \u03b4n log \u03b4 \u2264 0 and log(\u03b4/\u03b1 M 1 ) \u2264 \u2212M log \u03b1 1 . Thus, we can write\nIt now follows from (42), ( 45) and (46) that\nTherefore, for Pr(E K ) = 1 \u2212 o(1), a sufficient condition is\nfor some c > 0. We now claim that log n = O(M n\u03b1 1 log(1/\u03b1 1 )). The claim is clearly true for \u03b1 1 \u2192 \u03b8 for some 0 < \u03b8 < 1. Now suppose \u03b1 1 \u2192 1. Then,\nfor n large enough and \u03b1 1 \u2192 0. This establishes the claim. Since log n = O(M n\u03b1 1 log(1/\u03b1 1 )), it now follows that (48) is implied by\nIn order to see why the claim is true, note that Stirling's approximation suggests,\nTherefore,", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Now consider,", "text": "Thus, the first term in the RHS of (51) is non-negative for any \u03bb i \u2265 1. In addition, for every \u03bb i , either \u03bb i \u2212log \u03bb i \u2192 \u221e or log(n/\u03bb i ) \u2192 \u221e as n \u2192 \u221e. Therefore, the term on the RHS of (51) is asymptotically non-negative. Hence,\nThus, it now follows from (50) that ( 49) is implied by\nThat is, we have \"unique witness\" property satisfied as long as\nwhere\nand C \u2032 is some constant. This completes the proof of Theorem III.6.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "XI. PROOF OF THEOREM III.7: LIMITATION ON RECOVERY", "text": "In order to make a statement about the inability of any algorithm to recover f usingf (\u03bb), we rely on the formalism of classical information theory. In particular, we establish a bound on the sparsity of f beyond which recovery is not asymptotically reliable (precise definition of asymptotic reliability is provided later).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Information theory preliminaries", "text": "Here we recall some necessary Information Theory preliminaries. Further details can be found in the book by Cover and Thomas [26].\nConsider a discrete random variable X that is uniformly distributed over a finite set X . Let X be transmitted over a noisy channel to a receiver; suppose the receiver receives a random variable Y , which takes values in a finite set Y . Essentially, such \"transmission over noisy channel\" setup describes any two random variables X, Y defined through a joint probability distribution over a common probability space. Now letX = g(Y ) be an estimation of the transmitted information that the receiver produces based on the observation Y using some function g : Y \u2192 X . Define probability of error as p err = Pr(X =X). Since X is uniformly distributed over X , it follows that\nRecovery of X is called asymptotically reliable if p err \u2192 0 as |X | \u2192 \u221e. Therefore, in order to show that recovery is not asymptotically reliable, it is sufficient to prove that p err is bounded away from 0 as |X | \u2192 \u221e. In order to obtain a lower bound on p err , we use Fano's inequality:\nUsing (56), we can write\nwhere we used H(Y |X) \u2265 0 for a discrete 4 valued random variable. The inequality (a) follows from the data processing inequality: if we have Markov chain X \u2192 Y \u2192X, then I(X;X) \u2264 I(X; Y ). Since H(X) = log|X |, from (57) we obtain\nTherefore, to establish that probability of error is bounded away from zero, it is sufficient to show that\nfor any fixed constant \u03b4 > 0.", "publication_ref": ["b25"], "figure_ref": [], "table_ref": []}, {"heading": "B. Proof of theorem III.7.", "text": "Our goal is to show that when K is large enough (in particular, as claimed in the statement of Theorem III.7), the probability of error of any recovery algorithm is uniformly bounded away from 0. For that, we first fix a recovery algorithm, and then utilize the above setup to show that recovery is not asymptotically reliable when K is large. Specifically, we use (59), for which we need to identify random variables X and Y .\nTo this end, for a given K and T , let f be generated as per the random model R(K, T ). Let random variable X represent the support of function f i.e., X takes values in X = S K n . Given \u03bb, letf (\u03bb) be the partial information that the recovery algorithm uses to recover f . Let random variable Y represent f (\u03bb), the D \u03bb \u00d7 D \u03bb matrix. Let h = h(Y ) denote the estimate of f , and g = g(Y ) = supp h denote the estimate of the support of f produced by the given recovery algorithm. Then,\nTherefore, in order to uniformly lower bound the probability of error of the recovery algorithm, it is sufficient to lower bound its probability of making an error in recovering the support of f . Therefore, we focus on\nIt follows from the discussion in Section XI-A that in order to show that p err is uniformly bounded away from 0, it is sufficient to show that for some constant \u03b4 > 0", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "APPENDIX PROOF OF AUXILIARY LEMMA", "text": "Here we present the proof of Lemma III.1. For this, first consider the limit \u03b1 1 \u2191 1. Specifically, let \u03b1 1 = 1 \u2212 \u03b5, for a very small positive \u03b5. Then,\nIn order to obtain a lower bound, we minimize H \u2032 (\u03b1)/H(\u03b1) over \u03b1 \u2265 0. It follows from (69) that, for a given \u03b1 1 = 1 \u2212 \u03b5, H \u2032 (\u03b1)/H(\u03b1) is minimized for the choice of \u03b1 i , i \u2265 2 that minimizes H \u2032 (\u03b1). Thus, we maximize r to \u03b1 i \u2265 0 and\nHere we are maximizing a convex function over a convex set. Therefore, maximization is achieved on the boundary of the convex set. That is, the maximum is \u03b5 log \u03b5; consequently, the minimum value of H \u2032 (\u03b1) = \u03b5 log(1/\u03b5). Therefore, it follows that for\nTo prove a similar claim for \u03b1 1 \u2193 0, let \u03b1 1 = \u03b5 for a small, positive \u03b5. Then, it follows that r = \u2126(1/\u03b5) since r i=1 \u03b1 i = 1 and \u03b1 1 \u2265 \u03b1 i for all i, 2 \u2264 i \u2264 r. Using a convex maximization based argument similar to the one we used above, it can be checked that H \u2032 (\u03b1) = \u2126(log(1/\u03b5)). Therefore, it follows that ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Uncertainty principles and signal recovery", "journal": "SIAM Journal on Applied Mathematics", "year": "1989", "authors": "D Donoho; P Stark"}, {"ref_id": "b1", "title": "Rank aggregation methods for the web", "journal": "WWW", "year": "2001", "authors": "C Dwork; R Kumar; M Naor; D Sivakumar"}, {"ref_id": "b2", "title": "Group Representations in Probability and Statistics", "journal": "", "year": "1988", "authors": "P Diaconis"}, {"ref_id": "b3", "title": "Efficient inference for distributions on permutations", "journal": "", "year": "2008", "authors": "J Huang; C Guestrin; L Guibas"}, {"ref_id": "b4", "title": "Multi-object tracking with representations of the symmetric group", "journal": "", "year": "2007", "authors": "R Kondor; A Howard; T Jebara"}, {"ref_id": "b5", "title": "Trueskill through time: Revisiting the history of chess", "journal": "Advances in Neural Information Processing Systems", "year": "2007", "authors": "P Dangauthier; R Herbrich; T Minka; T Graepel"}, {"ref_id": "b6", "title": "Nonlinear approximation theory on compact groups", "journal": "Journal of Fourier Analysis and Applications", "year": "2001", "authors": "K Kueh; T Olson; D Rockmore; K Tan"}, {"ref_id": "b7", "title": "Communication in the presence of noise", "journal": "Proceedings of the IRE", "year": "1949", "authors": "C Shannon"}, {"ref_id": "b8", "title": "Certain topics in telegraph transmission theory", "journal": "", "year": "2002", "authors": "H Nyquist"}, {"ref_id": "b9", "title": "Decoding by linear programming", "journal": "IEEE Transactions on", "year": "2005-12", "authors": "E Candes; T Tao"}, {"ref_id": "b10", "title": "Stable signal recovery from incomplete and inaccurate measurements", "journal": "Communications on Pure and Applied Mathematics", "year": "2006", "authors": "E Candes; J Romberg; T Tao"}, {"ref_id": "b11", "title": "Quantitative robust uncertainty principles and optimally sparse decompositions", "journal": "Foundations of Computational Mathematics", "year": "2006", "authors": "E Candes; J Romberg"}, {"ref_id": "b12", "title": "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information", "journal": "IEEE Transactions on Information Theory", "year": "2006", "authors": "E Candes; J Romberg; T Tao"}, {"ref_id": "b13", "title": "Compressed sensing", "journal": "IEEE Transactions on Information Theory", "year": "2006", "authors": "D Donoho"}, {"ref_id": "b14", "title": "Low-density parity-check codes", "journal": "IRE Transactions on", "year": "1962", "authors": "R Gallager"}, {"ref_id": "b15", "title": "Expander codes", "journal": "IEEE Transactions on Information Theory", "year": "1996", "authors": "M Sipser; D A Spielman"}, {"ref_id": "b16", "title": "Improved low-density parity-check codes using irregular graphs", "journal": "IEEE Transactions on Information Theory", "year": "2001", "authors": "M Luby; M Mitzenmacher; M Shokrollahi; D Spielman"}, {"ref_id": "b17", "title": "Polynomial codes over certain finite fields", "journal": "Journal of the Society for Industrial and Applied Mathematics", "year": "1960", "authors": "I Reed; G Solomon"}, {"ref_id": "b18", "title": "Just relax: Convex programming methods for identifying sparse signals in noise", "journal": "IEEE transactions on information theory", "year": "2006", "authors": "J Tropp"}, {"ref_id": "b19", "title": "Greed is good: Algorithmic results for sparse approximation", "journal": "IEEE Transactions on Information Theory", "year": "2004", "authors": ""}, {"ref_id": "b20", "title": "Combining geometry and combinatorics: A unified approach to sparse signal recovery", "journal": "", "year": "2008", "authors": "R Berinde; A Gilbert; P Indyk; H Karloff; M Strauss"}, {"ref_id": "b21", "title": "Combinatorial algorithms for compressed sensing", "journal": "", "year": "2006", "authors": "G Cormode; S Muthukrishnan"}, {"ref_id": "b22", "title": "One sketch for all: fast algorithms for compressed sensing", "journal": "ACM", "year": "2007", "authors": "A C Gilbert; M J Strauss; J A Tropp; R Vershynin"}, {"ref_id": "b23", "title": "Data Streams: Algorithms and Applications, ser. Foundations and Trends in Theoretical Computer Science", "journal": "Now Publishers", "year": "2005", "authors": "S Muthukrishnan"}, {"ref_id": "b24", "title": "Inferring rankings under constrained sensing", "journal": "", "year": "2008", "authors": "S Jagabathula; D Shah"}, {"ref_id": "b25", "title": "Elements of Information Theory 2nd Edition, ser", "journal": "Wiley-Interscience", "year": "2006-07", "authors": "T M Cover; J A Thomas"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "any fixed \u03b5 > 0. In general, for any \u03bb with \u03bb 1 = n \u2212 m and m = O(1), arguments of Theorem III.4 can be adapted to show that K(\u03bb) scales as n m log n. Theorems III.3 and III.4 suggest that the recoverability threshold scales D \u03bb log D \u03bb for \u03bb = (\u03bb 1 , . . . , \u03bb r ) with \u03bb 1 = n \u2212 m for m = O(1). Next, we consider the case of more general \u03bb.Case 3: \u03bb = (\u03bb 1 , . . . , \u03bb r ) with \u03bb 1 = n \u2212 O n 2 9 \u2212\u03b4 for any \u03b4 > 0. As stated next, for this case, the recoverability threshold K(\u03bb) scales at least as D \u03bb log log D \u03bb . Theorem III.5. A randomly generated f as per Definition II.2 can be recovered fromf (\u03bb) by the sparsest-fit algorithm for", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Fig. 1 .1Fig. 1. Sparsest-fit algorithm", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "2 92\u2212\u03b4 , \u03b4 > 0 So far we have obtained the sharp result that algorithm the sparsest-fit algorithm recovers f up to sparsity essentially 1 m! n m log n for \u03bb with \u03bb 1 = n\u2212m where m = O(1). Now we investigate this further when m scales with n, i.e. m = \u03c9(1). Let \u03bb 1 = n \u2212 \u00b5 with \u00b5 \u2264 n 2 9", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Now consider the case of general \u03bb = (\u03bb 1 , \u03bb 2 , . . . , \u03bb r ). Let M = \u230an/(n \u2212 \u03bb 1 )\u230b and N = n \u2212 M (n \u2212 \u03bb 1 ). Clearly, 0 \u2264 N < n\u2212\u03bb 1 . Now we partition the set {1, 2, . . . , n} into M +1 partitions covering all elements: {1, . . . , n \u2212 \u03bb 1 }, . . . , {(n \u2212 \u03bb 1 )(M \u22121)+1, . . . , (n\u2212\u03bb 1 )M } and {(n\u2212\u03bb 1 )M +1, . . . , n}.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "For that, first consider a general multinomial coefficient m!/(k 1 !k 2 ! . . . k l !) with m = i k i . Then, using the Stirling's approximation log n! = n log n \u2212 n + 0.5 log n + O(1), for any n, we obtain log m! k 1 !k 2 ! . . . k l ! = m log m \u2212 m + 0.5 log m + O(1)\u2212 l i=1 (k i log k i \u2212 k i + 0.5 log k i + O(1)) 2 . . . k l \u2212 O(l)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "\u03b4(1 \u2212 \u03b1 1 ) M \u2212 O(M )where \u03b4 = N/n. It now follows from (43) and (44) thatM log n! \u03bb 1 !(n \u2212 \u03bb 1 )! \u2212 log n! N !((n \u2212 \u03bb 1 )!) M = \u2212 M n\u03b1 1 log \u03b1 1 + \u03b4n log \u03b4 (45)", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "supp (f ) = {\u03c3 \u2208 S n : f (\u03c3) = 0} .", "formula_coordinates": [4.0, 105.24, 139.19, 138.48, 17.26]}, {"formula_id": "formula_1", "formula_text": "D \u03bb = n! r i=1 \u03bb i ! .", "formula_coordinates": [4.0, 140.4, 322.89, 68.28, 25.9]}, {"formula_id": "formula_2", "formula_text": "1 \u2264 i \u2264 D \u03bb 1 . For example, for \u03bb = (n \u2212 1, 1) there are D \u03bb = n!/(n \u2212 1)! = n distinct ways given by t i \u2261 {1, . . . , i \u2212 1, i + 1, . . . , n}{i}, 1 \u2264 i \u2264 n.", "formula_coordinates": [4.0, 48.96, 353.8, 251.12, 60.65]}, {"formula_id": "formula_3", "formula_text": "0/1 valued D \u03bb \u00d7 D \u03bb matrix M \u03bb (\u03c3) as M \u03bb ij (\u03c3) = 1, if \u03c3(t j ) = t i 0, otherwise. for all 1 \u2264 i, j \u2264 D \u03bb", "formula_coordinates": [4.0, 56.88, 515.21, 229.0, 45.64]}, {"formula_id": "formula_4", "formula_text": "f (\u03bb) = \u03c3\u2208Sn f (\u03c3)M \u03bb (\u03c3).", "formula_coordinates": [4.0, 384.12, 178.85, 106.68, 22.93]}, {"formula_id": "formula_5", "formula_text": "minimize g 0 over g : S n \u2192 R + subject to\u011d(\u03bb) =f (\u03bb).(1)", "formula_coordinates": [4.0, 341.28, 373.07, 221.84, 25.18]}, {"formula_id": "formula_6", "formula_text": "M \u03bb (\u03c3 1 ) + M \u03bb (\u03c3 2 ) = M \u03bb (\u03c3 3 ) + M \u03bb (\u03c3 4 ).", "formula_coordinates": [4.0, 349.56, 692.81, 175.92, 12.49]}, {"formula_id": "formula_7", "formula_text": "f (\u03bb) =p 1 M \u03bb (\u03c3 1 ) + p 2 M \u03bb (\u03c3 2 ) + p 3 M \u03bb (\u03c3 3 ) + p 4 M \u03bb (\u03c3 4 ) =(p 2 \u2212 p 1 )M \u03bb (\u03c3 2 ) + (p 3 + p 1 )M \u03bb (\u03c3 3 ) + (p 4 + p 1 )M \u03bb (\u03c3 4 ). Thus, function g with g(\u03c3 2 ) = p 2 \u2212 p 1 , g(\u03c3 3 ) = p 3 + p 1 , g(\u03c3 4 ) = p 4 + p 1 and g(\u03c3) = 0 for all other \u03c3 \u2208 S n is such that\u011d(\u03bb) =f (\u03bb) but g 0 = 3 < 4 = f 0 .", "formula_coordinates": [5.0, 73.2, 140.13, 226.77, 104.13]}, {"formula_id": "formula_8", "formula_text": ") = 0 for all other \u03c3 \u2208 S n . Then,f (\u03bb) = pM \u03bb (\u03c3 1 ) + pM \u03bb (\u03c3 2 ) = pM \u03bb (\u03c3 3 ) + pM \u03bb (\u03c3 4", "formula_coordinates": [5.0, 73.2, 305.03, 226.86, 34.87]}, {"formula_id": "formula_9", "formula_text": "f (\u03bb) =p 1 M \u03bb (\u03c3 1 ) + p 2 M \u03bb (\u03c3 2 ) + p 3 M \u03bb (\u03c3 3 ) =(p 2 \u2212 p 1 )M \u03bb (\u03c3 2 ) + (p 3 + p 1 )M \u03bb (\u03c3 3 ) + p 1 M \u03bb (\u03c3 4 ).", "formula_coordinates": [5.0, 74.76, 423.81, 223.68, 48.12]}, {"formula_id": "formula_10", "formula_text": "f (\u03c3) = p i if \u03c3 = \u03c3 i , 1 \u2264 i \u2264 K 0 otherwise.", "formula_coordinates": [5.0, 359.28, 213.83, 154.5, 24.58]}, {"formula_id": "formula_11", "formula_text": "supp (f ) = {\u03c3 1 , . . . , \u03c3 K }, and f (\u03c3 k ) = p k , 1 \u2264 k \u2264 K.", "formula_coordinates": [5.0, 315.6, 389.03, 243.84, 17.26]}, {"formula_id": "formula_12", "formula_text": "even if M \u03bb (\u03c3) \u03c3\u2208supp(f ) is linearly indepen- dent, there could exist M \u03bb (\u03c3 \u2032 ) \u03c3 \u2032 \u2208H such thatf (\u03bb) = \u03c3 \u2032 \u2208H M \u03bb (\u03c3 \u2032 ) and |H| \u2264 K, where K := |supp (f )|;", "formula_coordinates": [5.0, 312.0, 557.81, 251.24, 46.71]}, {"formula_id": "formula_13", "formula_text": "1 \u2264 i \u03c3 , j \u03c3 \u2264 D \u03bb such that M \u03bb i\u03c3j\u03c3 (\u03c3) = 1, but M \u03bb i\u03c3 j\u03c3 (\u03c3 \u2032 ) = 0, for all \u03c3 \u2032 ( = \u03c3) \u2208 supp (f ) .", "formula_coordinates": [6.0, 69.0, 140.03, 231.09, 41.14]}, {"formula_id": "formula_14", "formula_text": "K k=1 c k p k = 0, unless c 1 = . . . = c K = 0.", "formula_coordinates": [6.0, 69.0, 185.33, 231.06, 25.33]}, {"formula_id": "formula_15", "formula_text": "minimize g 1 over g : S n \u2192 R + subject to\u011d(\u03bb) =f (\u03bb).(2)", "formula_coordinates": [6.0, 78.24, 574.43, 221.84, 25.18]}, {"formula_id": "formula_16", "formula_text": "1 \u2212 o(1) as long as K \u2264 (1 \u2212 \u03b5)n log n for any fixed \u03b5 > 0. Case 2: \u03bb = (n \u2212 m, m) with 1 < m = O(1). Here D \u03bb = \u0398(n m ) andf (\u03bb)", "formula_coordinates": [6.0, 312.0, 225.83, 250.98, 47.26]}, {"formula_id": "formula_17", "formula_text": "\u03bb = (n \u2212 m, m), m = O(1), with probability 1 \u2212 o(1) as long as K \u2264 (1\u2212\u03b5) m! n m log n for", "formula_coordinates": [6.0, 312.0, 328.55, 251.1, 29.74]}, {"formula_id": "formula_18", "formula_text": "\u03bb = (\u03bb 1 , . . . , \u03bb r ) with \u03bb 1 = n \u2212 n 2 9 \u2212\u03b4 for any \u03b4 > 0, with probability 1 \u2212 o(1) as long as K \u2264 (1 \u2212 \u03b5)D \u03bb log log D \u03bb for any fixed \u03b5 > 0.", "formula_coordinates": [6.0, 312.0, 504.47, 251.08, 36.1]}, {"formula_id": "formula_19", "formula_text": "\u03b1 i = \u03bb i /n, 1 \u2264 i \u2264 r. Let H(\u03b1) = \u2212 r i=1 \u03b1 i log \u03b1 i , and H \u2032 (\u03b1) = \u2212 r i=2 \u03b1 i log \u03b1 i .", "formula_coordinates": [6.0, 317.76, 615.11, 239.4, 46.39]}, {"formula_id": "formula_20", "formula_text": "K \u2264 C D \u03b3(\u03b1) \u03bb ,(3)", "formula_coordinates": [6.0, 400.44, 706.85, 162.68, 19.95]}, {"formula_id": "formula_21", "formula_text": "\u03b3(\u03b1) = M M + 1 1 \u2212 C \u2032 H(\u03b1) \u2212 H \u2032 (\u03b1) H(\u03b1) , with M = 1 1 \u2212 \u03b1 1 and 0 < C, C \u2032 < \u221e are constants.", "formula_coordinates": [7.0, 48.96, 69.89, 211.32, 74.79]}, {"formula_id": "formula_22", "formula_text": "\u03b1 1 = \u03bb 1 /n = 1 \u2212 o(1).", "formula_coordinates": [7.0, 195.72, 216.95, 104.25, 17.26]}, {"formula_id": "formula_23", "formula_text": "\u221a D \u03bb ) = o( \u221a n!", "formula_coordinates": [7.0, 48.96, 292.67, 251.1, 29.86]}, {"formula_id": "formula_24", "formula_text": "\u03b1 = (\u03b1 1 , . . . , \u03b1 r ) with 1 \u2265 \u03b1 1 \u2265 \u2022 \u2022 \u2022 \u2265 \u03b1 r \u2265 0 and r i=1 \u03b1 r = 1. Then, lim \u03b11\u21911 H \u2032 (\u03b1) H(\u03b1) = 1, lim \u03b11\u21930 H \u2032 (\u03b1) H(\u03b1) = 1.", "formula_coordinates": [7.0, 48.96, 365.75, 251.1, 81.22]}, {"formula_id": "formula_25", "formula_text": "p err = Pr (h = f ) .", "formula_coordinates": [7.0, 136.44, 631.77, 76.08, 10.43]}, {"formula_id": "formula_26", "formula_text": "K \u2265 3D 2 \u03bb n log n log D 2 \u03bb n log n \u2228 T ,(4)", "formula_coordinates": [7.0, 93.48, 705.77, 206.6, 25.11]}, {"formula_id": "formula_27", "formula_text": "for each 1 \u2264 k \u2264 K, p k = q \u2113 for some 1 \u2264 \u2113 \u2264 L and T p k = T \u2032 p k for T = T \u2032 .", "formula_coordinates": [7.0, 312.0, 552.23, 251.13, 34.06]}, {"formula_id": "formula_28", "formula_text": "A k := {\u2113 : k \u2208 T \u2113 } for 1 \u2264 k \u2264 K using q \u2113 , but without any knowledge of K or subsets T \u2113 , 1 \u2264 \u2113 \u2264 L.", "formula_coordinates": [7.0, 312.0, 588.11, 251.12, 41.14]}, {"formula_id": "formula_29", "formula_text": "(p k , A k ) for 1 \u2264 k \u2264 k(L). Input: Positive values {q 1 , q 2 , . . . , q L } sorted in ascending order i.e., q 1 \u2264 q 2 \u2264 . . . \u2264 q L . Assumptions: \u2203 positive values {p 1 , p 2 , . . . , p K } such that: C1. For each 1 \u2264 \u2113 \u2264 L, q \u2113 = k\u2208T \u2113 p k , for some T \u2113 \u2286 {1, 2, . . . , K} C2. For each 1 \u2264 k \u2264 K, there exists a q \u2113 such that q \u2113 = p K . C3. k\u2208T p k = k \u2032 \u2208T \u2032 p k \u2032 , for all T, T \u2032 \u2286 {1, 2, . . . , J} and T \u2229 T \u2032 = \u2205. Output: {p 1 , p 2 , . . . , p K }, \u2200 1 \u2264 k \u2264 K set A k s.t.", "formula_coordinates": [8.0, 55.44, 236.87, 243.66, 188.86]}, {"formula_id": "formula_30", "formula_text": "initialization: p 0 = 0, k(0) = 0, A k = \u2205 for all possible k. for \u2113 = 1 to L if q \u2113 = k\u2208T p k for some T \u2286 {0, 1, . . . , k(\u2113 \u2212 1)} k(\u2113) = k(\u2113 \u2212 1) A k = A k \u222a {\u2113} \u2200 k \u2208 T else k(\u2113) = k(\u2113 \u2212 1) + 1 p k(\u2113) = q \u2113 A k(\u2113) = A k(\u2113) \u222a {\u2113} end if end for Output K = k(L) and (p k , A k ), 1 \u2264 k \u2264 K.", "formula_coordinates": [8.0, 55.44, 491.87, 243.54, 148.78]}, {"formula_id": "formula_31", "formula_text": "S n \u2192 R + such that\u011d(\u03bb) =f (\u03bb) and L \u25b3 = g \u21130 \u2264 f \u21130 = K. Let supp (f ) = {\u03c3 k \u2208 S n : 1 \u2264 k \u2264 K}, f (\u03c3 k ) = p k , 1 \u2264 k \u2264 K, supp (g) = {\u03c1 \u2113 \u2208 S n : 1 \u2264 \u2113 \u2264 L}, g(\u03c1 \u2113 ) = q \u2113 , 1 \u2264 \u2113 \u2264 L.", "formula_coordinates": [8.0, 312.0, 611.75, 255.0, 75.58]}, {"formula_id": "formula_32", "formula_text": "k, 1 \u2264 k \u2264 K, we have p k = j\u2208T k q j ,", "formula_coordinates": [9.0, 48.96, 57.35, 95.86, 40.75]}, {"formula_id": "formula_33", "formula_text": "p = Aq,(5)", "formula_coordinates": [9.0, 150.6, 123.81, 149.48, 9.96]}, {"formula_id": "formula_34", "formula_text": ") is D \u03bb (p 1 + \u2022 \u2022 \u2022 + p K ). That is, ijf (\u03bb) ij = D \u03bb K k=1 p k . Similarly, ij\u011d (\u03bb) ij = D \u03bb L \u2113=1 q \u2113 . Butf (\u03bb) =\u011d(\u03bb). Therefore, p \u2022 1 = q \u2022 1,(6)", "formula_coordinates": [9.0, 48.96, 202.67, 251.12, 142.65]}, {"formula_id": "formula_35", "formula_text": "p \u2022 1 = Aq \u2022 1 = L \u2113=1 c \u2113 q \u2113 ,(7)", "formula_coordinates": [9.0, 134.04, 389.75, 166.04, 44.59]}, {"formula_id": "formula_36", "formula_text": "j q j = j c j q j .(8)", "formula_coordinates": [9.0, 136.44, 460.05, 163.64, 20.98]}, {"formula_id": "formula_37", "formula_text": "M \u03bb (\u03c3 1 ) + M \u03bb (\u03c3 2 ) = M \u03bb (\u03c3 1,2 ) + M \u03bb (id)(9)", "formula_coordinates": [9.0, 348.72, 435.65, 214.4, 12.49]}, {"formula_id": "formula_38", "formula_text": "p 1 M \u03bb (\u03c3 1 ) + p 2 M \u03bb (\u03c3 2 ) = p 1 M \u03bb (\u03c3 1,2 ) + p 1 M \u03bb (id) + (p 2 \u2212 p 1 )M \u03bb (\u03c3 2 ). Thus, givenf (\u03bb) = p 1 M \u03bb (\u03c3 1 ) + p 2 M \u03bb (\u03c3 2 )", "formula_coordinates": [9.0, 312.0, 497.81, 228.0, 48.49]}, {"formula_id": "formula_39", "formula_text": "Pr(E c ) = 1 n! n l=1 n l (l \u2212 1)! = n r=1 1 l(n \u2212 l)! (10)", "formula_coordinates": [10.0, 71.88, 97.37, 228.32, 34.11]}, {"formula_id": "formula_40", "formula_text": "n/2 l=1 1 l(n \u2212 l)! \u2264 n/2 l=1 1 n 2 ! = 1 n 2 \u2212 1 ! (11)", "formula_coordinates": [10.0, 97.8, 156.77, 202.4, 35.56]}, {"formula_id": "formula_41", "formula_text": "n l=n/2 1 l(n \u2212 l)! \u2264 n/2 k=0 1 n 2 k! \u2264 2 n \u221e k=0 1 k! \u2264 O(1) n (12", "formula_coordinates": [10.0, 69.72, 210.53, 226.29, 34.95]}, {"formula_id": "formula_42", "formula_text": ")", "formula_coordinates": [10.0, 296.01, 222.28, 4.19, 8.97]}, {"formula_id": "formula_43", "formula_text": "Pr(E ) \u2265 1 \u2212 Pr(E c ) \u2265 1 \u2212 1 n 2 \u2212 1 ! + O(1) n \u2192 1 as n \u2192 \u221e.", "formula_coordinates": [10.0, 71.28, 270.69, 197.3, 47.64]}, {"formula_id": "formula_44", "formula_text": "\u03bb = (n \u2212 1, 1)", "formula_coordinates": [10.0, 478.92, 513.83, 59.55, 17.26]}, {"formula_id": "formula_45", "formula_text": "K = f 0 , supp (f ) = {\u03c3 k \u2208 S n : 1 \u2264 k \u2264 K}, and f (\u03c3 k ) = p k , 1 \u2264 k \u2264 K.", "formula_coordinates": [10.0, 334.2, 560.27, 206.76, 32.14]}, {"formula_id": "formula_46", "formula_text": "f (\u03bb) ij = k:\u03c3 k (j)=i p k , for 1 \u2264 i, j \u2264 n.", "formula_coordinates": [10.0, 340.2, 638.15, 194.64, 21.79]}, {"formula_id": "formula_47", "formula_text": "F j = {\u03c3 k (j) = j, for 2 \u2264 k \u2264 K}, for 1 \u2264 j \u2264 n.", "formula_coordinates": [11.0, 57.48, 218.99, 234.0, 17.26]}, {"formula_id": "formula_48", "formula_text": "Pr(E c 1 ) = Pr \u2229 n j=1 F c j \u2264 Pr \u2229 L j=1 F c j = Pr (F c 1 ) \uf8ee \uf8f0 L j=2 Pr F c j \u2229 j\u22121 \u2113=1 F c \u2113 \uf8f9 \uf8fb . (13", "formula_coordinates": [11.0, 69.36, 289.97, 226.65, 65.77]}, {"formula_id": "formula_49", "formula_text": ")", "formula_coordinates": [11.0, 296.01, 335.44, 4.19, 8.97]}, {"formula_id": "formula_50", "formula_text": "Pr (F c 1 ) = 1 \u2212 Pr (F 1 ) = 1 \u2212 1 \u2212 1 n K\u22121 .(14)", "formula_coordinates": [11.0, 104.4, 417.05, 195.8, 41.07]}, {"formula_id": "formula_51", "formula_text": "Pr F c j \u2229 j\u22121 \u2113=1 F c \u2113 \u2264 1 \u2212 1 \u2212 1 n \u2212 j + 1 K\u22121 .(15)", "formula_coordinates": [11.0, 62.16, 530.45, 238.04, 33.63]}, {"formula_id": "formula_52", "formula_text": "Pr(E c 1 ) \u2264 L j=1 1 \u2212 1 \u2212 1 n \u2212 j + 1 K\u22121 \u2264 1 \u2212 1 \u2212 1 n \u2212 L K L \u2264 1 \u2212 1 \u2212 1 n \u2212 L C1n log n L ,(16)", "formula_coordinates": [11.0, 75.48, 578.69, 224.72, 106.11]}, {"formula_id": "formula_53", "formula_text": "L = n 1\u2212\u03b4 , n \u2212 L = n(1 \u2212 o(1)). Using the standard fact 1 \u2212 x = e \u2212x (1 + O(x 2 )) for small x \u2208 [0, 1), we have 1 \u2212 1 n \u2212 L = exp \u2212 1 n \u2212 L 1 + O 1 n 2 . (17", "formula_coordinates": [11.0, 48.96, 697.13, 251.22, 60.27]}, {"formula_id": "formula_54", "formula_text": ")", "formula_coordinates": [11.0, 296.01, 734.2, 4.19, 8.97]}, {"formula_id": "formula_55", "formula_text": "1 + O 1 n 2 C1n log n = \u0398(1).", "formula_coordinates": [11.0, 375.6, 73.13, 131.16, 26.56]}, {"formula_id": "formula_56", "formula_text": "Pr(E c 1 ) \u2264 1 \u2212 \u0398 exp \u2212 C 1 log n 1 \u2212 n \u2212\u03b4 L \u2264 [1 \u2212 \u0398 (exp (\u2212(C 1 + \u03b4) log n))] L = 1 \u2212 \u0398 1 n C1+\u03b4 L \u2264 exp \u2212\u0398 L n C1+\u03b4 = exp \u2212\u2126(n 2\u03b4 ) ,(18)", "formula_coordinates": [11.0, 340.32, 120.53, 222.92, 119.55]}, {"formula_id": "formula_57", "formula_text": "\u2212 x \u2264 e \u2212x for x \u2208 [0, 1] and L = n 1\u2212\u03b4 , C 1 \u2264 1 \u2212 4\u03b4. From (", "formula_coordinates": [11.0, 312.0, 239.93, 251.16, 30.39]}, {"formula_id": "formula_58", "formula_text": "\u03bb = (n \u2212 m, m)", "formula_coordinates": [11.0, 477.12, 288.83, 66.87, 17.26]}, {"formula_id": "formula_59", "formula_text": "K = f 0 , supp (f ) = {\u03c3 k \u2208 S n : 1 \u2264 k \u2264 K}, and f (\u03c3 k ) = p k , 1 \u2264 k \u2264 K.", "formula_coordinates": [11.0, 334.2, 345.95, 206.76, 32.26]}, {"formula_id": "formula_60", "formula_text": "R(K, C ) described in Section II. For \u03bb = (n \u2212 m, m), D \u03bb = n! (n\u2212m)!m! \u223c n m andf (\u03bb) is an D \u03bb \u00d7 D \u03bb matrix.", "formula_coordinates": [11.0, 312.0, 391.07, 250.98, 29.38]}, {"formula_id": "formula_61", "formula_text": "Pr(E c 1 = O(1/n 2m )", "formula_coordinates": [11.0, 312.0, 569.45, 83.22, 13.09]}, {"formula_id": "formula_62", "formula_text": "t i = {1, . . . , n \u2212 im, n \u2212 (i \u2212 1)m + 1, . . . , n} {n \u2212 im + 1, . . . , n \u2212 (i \u2212 1)m}.", "formula_coordinates": [11.0, 321.96, 675.47, 231.24, 32.14]}, {"formula_id": "formula_63", "formula_text": "1 (t i ) = t i for all 1 \u2264 i \u2264 D \u03bb . Define F j = {\u03c3 k (t j ) = t j , for 2 \u2264 k \u2264 K}, for 1 \u2264 j \u2264 D \u03bb .", "formula_coordinates": [11.0, 312.0, 708.59, 250.98, 47.5]}, {"formula_id": "formula_64", "formula_text": "Pr(E 1 ) = Pr \u222a D \u03bb j=1 F j .", "formula_coordinates": [12.0, 122.28, 77.57, 104.4, 19.23]}, {"formula_id": "formula_65", "formula_text": "Pr(E c 1 ) = Pr \u2229 D \u03bb j=1 F c j \u2264 Pr \u2229 L j=1 F c j = Pr (F c 1 ) \uf8ee \uf8f0 L j=2 Pr F c j \u2229 j\u22121 \u2113=1 F c \u2113 \uf8f9 \uf8fb . (19", "formula_coordinates": [12.0, 69.36, 124.13, 226.65, 68.89]}, {"formula_id": "formula_66", "formula_text": ")", "formula_coordinates": [12.0, 296.01, 172.72, 4.19, 8.97]}, {"formula_id": "formula_67", "formula_text": "Pr (\u03c3 k (t 1 ) = t 1 ) = 1 n m = m! m\u22121 \u2113=0 (n \u2212 \u2113) . \u2264 m! (n \u2212 Lm) m .(20)", "formula_coordinates": [12.0, 96.36, 271.05, 203.84, 86.16]}, {"formula_id": "formula_68", "formula_text": "Pr (F c 1 ) = 1 \u2212 Pr (F 1 ) = 1 \u2212 Pr (\u03c3 k (t 1 ) = t 1 , 2 \u2264 k \u2264 K) = 1 \u2212 K k=2 (1 \u2212 Pr (\u03c3 k (t 1 ) = t 1 )) \u2264 1 \u2212 1 \u2212 m! (n \u2212 Lm) m K . (21", "formula_coordinates": [12.0, 73.92, 379.97, 222.09, 97.11]}, {"formula_id": "formula_69", "formula_text": ")", "formula_coordinates": [12.0, 296.01, 453.88, 4.19, 8.97]}, {"formula_id": "formula_70", "formula_text": "\u2229 j\u22121 \u2113=1 F c \u2113 for 2 \u2264 j \u2264 L.", "formula_coordinates": [12.0, 171.24, 481.85, 128.85, 19.59]}, {"formula_id": "formula_71", "formula_text": "Pr F c j \u2229 j\u22121 \u2113=1 F c \u2113 \u2264 1 \u2212 1 \u2212 1 n\u2212(j\u22121)m m K\u22121 \u2264 1 \u2212 1 \u2212 m! (n \u2212 Lm) m K .(22)", "formula_coordinates": [12.0, 59.28, 627.41, 240.92, 68.79]}, {"formula_id": "formula_72", "formula_text": "Pr(E c 1 ) \u2264 1 \u2212 1 \u2212 m! (n \u2212 Lm) m K L . (23) Now Lm = o(n) and hence n \u2212 Lm = n(1 \u2212 o(1)). Using 1 \u2212 x = e \u2212x (1 + O(x 2 )) for small x \u2208 [0, 1), we have 1 \u2212 m! (n \u2212 Lm) m = exp \u2212 m! (n \u2212 Lm) m 1 + O 1 n 2m .(24)", "formula_coordinates": [12.0, 81.48, 57.35, 481.76, 697.17]}, {"formula_id": "formula_73", "formula_text": "1 + O 1 n 2m K = \u0398(1).", "formula_coordinates": [12.0, 384.72, 159.65, 112.92, 26.56]}, {"formula_id": "formula_74", "formula_text": "Pr(E c 1 ) \u2264 1 \u2212 \u0398 exp \u2212 Km! n m (1 \u2212 Lm/n) m L \u2264 1 \u2212 \u0398 exp \u2212 (1 \u2212 2\u03b4) log n (1 \u2212 n \u2212\u03b4 m) m L \u2264 [1 \u2212 \u0398 (exp (\u2212(1 \u2212 3\u03b4/2) log n))] L \u2264 1 \u2212 \u0398 1 n 1\u22123\u03b4/2 L \u2264 exp \u2212\u2126(Ln \u22121+3\u03b4/2 ) \u2264 exp \u2212\u2126(n \u03b4/2 ) = O 1 n 2m .(25)", "formula_coordinates": [12.0, 322.8, 207.65, 240.44, 168.88]}, {"formula_id": "formula_75", "formula_text": "D \u03bb = n! r i=1 \u03bb i ! \u2264 n! \u03bb 1 ! \u2264 n n\u2212\u03bb1 = n \u00b5 . (26", "formula_coordinates": [12.0, 386.64, 523.53, 172.41, 70.56]}, {"formula_id": "formula_76", "formula_text": ")", "formula_coordinates": [12.0, 559.05, 577.72, 4.19, 8.97]}, {"formula_id": "formula_77", "formula_text": "E c 1 ) = O(1/D 2 \u03bb )", "formula_coordinates": [12.0, 444.21, 630.05, 66.56, 13.45]}, {"formula_id": "formula_78", "formula_text": "F j = {\u03c3 k (t j ) = t j , for 2 \u2264 k \u2264 K}, for 1 \u2264 j \u2264 D \u03bb .", "formula_coordinates": [13.0, 50.4, 202.31, 248.16, 17.26]}, {"formula_id": "formula_79", "formula_text": "Pr(E 1 ) = Pr \u222a D \u03bb j=1 F j .", "formula_coordinates": [13.0, 122.28, 248.57, 104.4, 19.23]}, {"formula_id": "formula_80", "formula_text": "Pr(E c 1 ) = Pr \u2229 D \u03bb j=1 F c j \u2264 Pr \u2229 L j=1 F c j = Pr (F c 1 ) \uf8ee \uf8f0 L j=2 Pr F c j \u2229 j\u22121 \u2113=1 F c \u2113 \uf8f9 \uf8fb . (27)", "formula_coordinates": [13.0, 69.36, 298.97, 230.84, 68.89]}, {"formula_id": "formula_81", "formula_text": "Pr (\u03c3 k (t 1 ) = t 1 ) = 1 D \u03bb . (28", "formula_coordinates": [13.0, 118.32, 426.57, 177.69, 24.22]}, {"formula_id": "formula_82", "formula_text": ")", "formula_coordinates": [13.0, 296.01, 433.96, 4.19, 8.97]}, {"formula_id": "formula_83", "formula_text": "Pr (F c 1 ) = 1 \u2212 Pr (F 1 ) = 1 \u2212 Pr (\u03c3 k (t 1 ) = t 1 , 2 \u2264 k \u2264 K) = 1 \u2212 K k=2 (1 \u2212 Pr (\u03c3 k (t 1 ) = t 1 )) = 1 \u2212 1 \u2212 1 D \u03bb K . (29", "formula_coordinates": [13.0, 73.92, 482.93, 222.09, 90.73]}, {"formula_id": "formula_84", "formula_text": ")", "formula_coordinates": [13.0, 296.01, 556.84, 4.19, 8.97]}, {"formula_id": "formula_85", "formula_text": "Next we evaluate Pr F c j \u2229 j\u22121 \u2113=1 F c \u2113 for 2 \u2264 j \u2264 L. Given \u2229 j\u22121 \u2113=1 F c \u2113 ,", "formula_coordinates": [13.0, 48.96, 586.73, 251.22, 35.43]}, {"formula_id": "formula_86", "formula_text": "Pr F c j \u2229 j\u22121 \u2113=1 F c \u2113 \u2264 1 \u2212 1 \u2212 1 D \u03bb(j) K . (30)", "formula_coordinates": [13.0, 75.6, 722.93, 224.6, 27.61]}, {"formula_id": "formula_87", "formula_text": "Pr(E c 1 ) \u2264 L j=1 1 \u2212 1 \u2212 1 D \u03bb(j) K \u2264 1 \u2212 1 \u2212 1 D \u03bb(L) K L .(31)", "formula_coordinates": [13.0, 354.0, 79.13, 209.24, 64.93]}, {"formula_id": "formula_88", "formula_text": "D \u03bb = D \u03bb(1) \u2265 . . . \u2265 D \u03bb(L) . Consider D \u03bb(j) D \u03bb(j+1) = (n \u2212 (j \u2212 1)\u00b5)! (\u03bb 1 \u2212 j\u00b5)! (n \u2212 j\u00b5)! (\u03bb 1 \u2212 (j \u2212 1)\u00b5)! = \u00b5\u22121 \u2113=0 (n \u2212 (j \u2212 1)\u00b5 \u2212 \u2113) (\u03bb 1 \u2212 (j \u2212 1)\u00b5 \u2212 \u2113) = n \u03bb 1 \u00b5 \u00b5\u22121 \u2113=0 1 \u2212 (j\u22121)\u00b5\u2212\u2113 n 1 \u2212 (j\u22121)\u00b5\u2212\u2113 \u03bb1 (32)", "formula_coordinates": [13.0, 312.0, 180.71, 251.24, 142.17]}, {"formula_id": "formula_89", "formula_text": "D \u03bb(1) D \u03bb(L) = n \u03bb 1 (L\u22121)\u00b5 (L\u22121)\u00b5 \u2113=0 1 \u2212 \u2113 n 1 \u2212 \u2113 \u03bb1 . (33", "formula_coordinates": [13.0, 352.08, 350.45, 206.97, 35.67]}, {"formula_id": "formula_90", "formula_text": ")", "formula_coordinates": [13.0, 559.05, 362.2, 4.19, 8.97]}, {"formula_id": "formula_91", "formula_text": "Using 1 + x \u2264 e x for any x \u2208 (\u22121, 1), 1 \u2212 x \u2265 e \u22122x for", "formula_coordinates": [13.0, 312.0, 392.93, 251.12, 18.39]}, {"formula_id": "formula_92", "formula_text": "0 \u2264 \u2113 \u2264 (L \u2212 1)\u00b5 1 \u2212 \u2113 n 1 \u2212 \u2113 \u03bb1 = 1 \u2212 \u2113 n + \u2113 \u03bb1 \u2212 \u2113 2 n\u03bb1 1 \u2212 \u2113 2 \u03bb 2 1 \u2264 exp \u2212 \u2113 2 \u2212 \u2113\u00b5 n\u03bb 1 + 2\u2113 2 \u03bb 2 1 \u2264 exp \u2113\u00b5 n\u03bb 1 + 2\u2113 2 \u03bb 2 1 .(34)", "formula_coordinates": [13.0, 312.0, 406.07, 251.24, 122.59]}, {"formula_id": "formula_93", "formula_text": "D \u03bb(1) D \u03bb(L) \u2264 n \u03bb 1 L\u00b5 exp \u0398 L 2 \u00b5 3 n\u03bb 1 + 2L 3 \u00b5 3 \u03bb 2 1 .(35) Now n \u03bb 1 L\u00b5 = 1 + \u00b5 \u03bb 1 L\u00b5 \u2264 exp L\u00b5 2 \u03bb 1 . (36", "formula_coordinates": [13.0, 312.0, 558.17, 251.24, 111.97]}, {"formula_id": "formula_94", "formula_text": ")", "formula_coordinates": [13.0, 559.05, 653.32, 4.19, 8.97]}, {"formula_id": "formula_95", "formula_text": "L\u00b5 2 = o(\u03bb 1 ), L 3 \u00b5 3 = o(\u03bb 2 1", "formula_coordinates": [13.0, 312.0, 691.25, 113.17, 12.97]}, {"formula_id": "formula_96", "formula_text": "D \u03bb(1) D \u03bb(L) = 1 + o(1). (37", "formula_coordinates": [13.0, 392.4, 725.25, 166.65, 25.29]}, {"formula_id": "formula_97", "formula_text": ")", "formula_coordinates": [13.0, 559.05, 733.36, 4.19, 8.97]}, {"formula_id": "formula_98", "formula_text": "Pr(E c 1 ) \u2264 exp \u2212L exp \u2212 K D \u03bb(L) = exp (\u2212L exp (\u2212(1 \u2212 \u03b5) log log D \u03bb (1 + o(1)))) \u2264 exp (\u2212L exp (\u2212 log log D \u03bb )) = exp \u2212 L log D \u03bb = exp \u2212 3n 4 9 \u22122\u03b4 log 3 n log D \u03bb \u2264 exp (\u22122 log D \u03bb ) = 1 D 2 \u03bb .(38)", "formula_coordinates": [14.0, 51.0, 86.61, 249.2, 152.97]}, {"formula_id": "formula_99", "formula_text": "Let \u03bb = (\u03bb 1 , . . . , \u03bb r ), r \u2265 2 with \u03bb 1 \u2265 \u2022 \u2022 \u2022 \u2265 \u03bb r \u2265 1. As before, let K = f 0 , supp (f ) = {\u03c3 k \u2208 S n : 1 \u2264 k \u2264 K}, and f (\u03c3 k ) = p k , 1 \u2264 k \u2264 K.", "formula_coordinates": [14.0, 48.96, 310.19, 251.1, 63.33]}, {"formula_id": "formula_100", "formula_text": "D \u03bb = n! r i=1 \u03bb i ! .", "formula_coordinates": [14.0, 140.4, 416.37, 68.28, 25.77]}, {"formula_id": "formula_101", "formula_text": "\u03b1 i = \u03bb i /n, 1 \u2264 i \u2264 r, H(\u03b1) = \u2212 r i=1 \u03b1 i log \u03b1 i , and H \u2032 (\u03b1) = \u2212 r i=2 \u03b1 i log \u03b1 i .", "formula_coordinates": [14.0, 48.96, 447.47, 251.1, 58.28]}, {"formula_id": "formula_102", "formula_text": "G \u03bb = (V \u03bb 1 \u00d7V \u03bb 2 , E \u03bb ) with vertices V \u03bb 1 , V \u03bb 2", "formula_coordinates": [14.0, 48.96, 582.05, 172.12, 18.39]}, {"formula_id": "formula_103", "formula_text": "1 | = |V \u03bb 2 | = D \u03bb .", "formula_coordinates": [14.0, 224.16, 594.05, 75.81, 18.39]}, {"formula_id": "formula_104", "formula_text": "\u03bb = (n \u2212 1, 1), Pr(E c L+1 |E L ) \u2264 L n n! .", "formula_coordinates": [14.0, 312.0, 380.03, 168.0, 33.34]}, {"formula_id": "formula_105", "formula_text": "Pr(E c L+1 |E L ) \u2264 L \u230an/2\u230b (2!) \u230an/2\u230b n! .", "formula_coordinates": [15.0, 106.2, 114.89, 136.68, 25.11]}, {"formula_id": "formula_106", "formula_text": "(\u03bb 2 !\u03bb 3 ! . . . \u03bb r !) M .", "formula_coordinates": [15.0, 137.76, 278.21, 73.44, 12.61]}, {"formula_id": "formula_107", "formula_text": "L M r i=2 \u03bb i ! M N !.", "formula_coordinates": [15.0, 132.12, 359.45, 84.72, 33.61]}, {"formula_id": "formula_108", "formula_text": "Pr E c L+1 |E L \u2264 1 n! L M r i=2 \u03bb i ! M N !.(39)", "formula_coordinates": [15.0, 84.84, 423.17, 215.36, 33.61]}, {"formula_id": "formula_109", "formula_text": "x L \u25b3 = 1 n! L M r i=2 \u03bb i ! M N !.", "formula_coordinates": [15.0, 114.12, 479.21, 120.72, 33.73]}, {"formula_id": "formula_110", "formula_text": "Pr (E K ) = Pr (E K \u2229 E K\u22121 ) = Pr (E K |E K\u22121 ) Pr (E K\u22121 ) . (40", "formula_coordinates": [15.0, 90.24, 545.75, 205.77, 32.26]}, {"formula_id": "formula_111", "formula_text": ")", "formula_coordinates": [15.0, 296.01, 561.64, 4.19, 8.97]}, {"formula_id": "formula_112", "formula_text": "Pr (E K ) = Pr (E 1 ) K\u22121 L=1 Pr (E L+1 |E L ) = K\u22121 L=1 1 \u2212 Pr E c L+1 |E L = K\u22121 L=1 (1 \u2212 x L ) \u2265 1 \u2212 K\u22121 L=1 x L . (41", "formula_coordinates": [15.0, 90.48, 617.57, 205.53, 133.69]}, {"formula_id": "formula_113", "formula_text": ")", "formula_coordinates": [15.0, 296.01, 730.84, 4.19, 8.97]}, {"formula_id": "formula_114", "formula_text": "K L=2 x L \u2264 Kx K \u2264 1 n! K M+1 r i=2 \u03bb i ! M N ! = 1 n! K M+1 n! \u03bb 1 !D \u03bb M N ! = K M+1 D M \u03bb n! \u03bb 1 ! M N ! n! = K M+1 D M \u03bb n! \u03bb 1 !(n \u2212 \u03bb 1 )! M N !((n \u2212 \u03bb 1 )!) M n! . (42", "formula_coordinates": [15.0, 317.88, 77.45, 241.17, 161.19]}, {"formula_id": "formula_115", "formula_text": ")", "formula_coordinates": [15.0, 559.05, 215.56, 4.19, 8.97]}, {"formula_id": "formula_116", "formula_text": "M log n! \u03bb 1 !(n \u2212 \u03bb 1 )! = M n\u03b1 1 log 1 \u03b1 1 + M n(1 \u2212 \u03b1 1 ) log 1 1 \u2212 \u03b1 1 (43) + 0.5 log 1 n M \u03b1 M 1 (1 \u2212 \u03b1 1 ) M \u2212 O(M )", "formula_coordinates": [15.0, 341.16, 477.45, 222.08, 80.76]}, {"formula_id": "formula_117", "formula_text": "log|X | = (1 + o(1))Kn log n.(62)", "formula_coordinates": [17.0, 368.04, 87.83, 195.2, 17.26]}, {"formula_id": "formula_118", "formula_text": "H(Y ) \u2264 D \u03bb i,j=1", "formula_coordinates": [17.0, 335.64, 148.01, 71.41, 31.45]}, {"formula_id": "formula_119", "formula_text": "D 2 \u03bb log K Kn log n \u2264 1 3 (1 + \u03b4) \u21d0 K log K \u2265 3(1 \u2212 \u03b4/2)D 2 \u03bb n log n ,(64)", "formula_coordinates": [17.0, 316.44, 242.09, 251.96, 25.11]}, {"formula_id": "formula_120", "formula_text": "D 2 \u03bb log T Kn log n \u2264 1 3 (1 + \u03b4) \u21d0 K \u2265 3(1 \u2212 \u03b4/2)D 2 \u03bb log T n log n .(65)", "formula_coordinates": [17.0, 313.2, 268.61, 255.2, 25.24]}, {"formula_id": "formula_121", "formula_text": "y log y = c + \u03b5 1 + o(1)", "formula_coordinates": [17.0, 375.84, 349.65, 84.51, 23.52]}, {"formula_id": "formula_123", "formula_text": "K \u2265 4D 2 \u03bb n log n log D 2 \u03bb n log n .(67)", "formula_coordinates": [17.0, 370.92, 454.49, 192.32, 25.11]}, {"formula_id": "formula_124", "formula_text": "K \u2265 4D 2 \u03bb n log n log D 2 \u03bb n log n \u2228 T .(68)", "formula_coordinates": [17.0, 356.52, 524.81, 206.72, 25.11]}], "doi": ""}