{"title": "MINEDOJO: Building Open-Ended Embodied Agents with Internet-Scale Knowledge", "authors": "Linxi Fan; Guanzhi Wang; Yunfan Jiang; Ajay Mandlekar; Yuncong Yang; Haoyi Zhu; Andrew Tang; De-An Huang; Yuke Zhu; Anima Anandkumar;  Nvidia;  Caltech;  Stanford;  Columbia;  Sjtu; U T Austin", "pub_date": "2022-11-22", "abstract": "Autonomous agents have made great strides in specialist domains like Atari games and Go. However, they typically learn tabula rasa in isolated environments with limited and manually conceived objectives, thus failing to generalize across a wide spectrum of tasks and capabilities. Inspired by how humans continually learn and adapt in the open world, we advocate a trinity of ingredients for building generalist agents: 1) an environment that supports a multitude of tasks and goals, 2) a large-scale database of multimodal knowledge, and 3) a flexible and scalable agent architecture. We introduce MINEDOJO, a new framework built on the popular Minecraft game that features a simulation suite with thousands of diverse open-ended tasks and an internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and forum discussions. Using MINEDOJO's data, we propose a novel agent learning algorithm that leverages large pre-trained video-language models as a learned reward function. Our agent is able to solve a variety of openended tasks specified in free-form language without any manually designed dense shaping reward. We open-source the simulation suite, knowledge bases, algorithm implementation, and pretrained models (https://minedojo.org) to promote research towards the goal of generally capable embodied agents. 36th Conference on Neural Information Processing Systems (NeurIPS 2022) Track on Datasets and Benchmarks.", "sections": [{"heading": "Introduction", "text": "Developing autonomous embodied agents that can attain human-level performance across a wide spectrum of tasks has been a long-standing goal for AI research. There has been impressive progress towards this goal, most notably in games [80,85,126] and robotics [68,99,146,134,107]. These embodied agents are typically trained tabula rasa in isolated worlds with limited complexity and diversity. Although highly performant, they are specialist models that do not generalize beyond a narrow set of tasks. In contrast, humans inhabit an infinitely rich reality, continuously learn from and adapt to a wide variety of open-ended tasks, and are able to leverage large amount of prior knowledge from their own experiences as well as others.\nWe argue that three main pillars are necessary for generalist embodied agents to emerge. First, the environment in which the agent acts needs to enable an unlimited variety of open-ended goals [116,71,120,117]. Natural evolution is able to nurture an ever-expanding tree of diverse life forms thanks to the infinitely varied ecological settings that the Earth supports [117,129]. This process has not stagnated for billions of years. In contrast, today's agent training algorithms cease to make new progress after convergence in narrow environments [80,146]. Second, a large-scale database of prior knowledge is necessary to facilitate learning in open-ended settings. Just as humans frequently learn from the internet, agents should also be able to harvest practical knowledge encoded in large amounts of video demos [42,77], multimedia tutorials [79], and forum discussions [127,65,54]. In a Figure 1: MINEDOJO is a novel framework for developing open-ended, generally capable agents that can learn and adapt continually to new goals. MINEDOJO features a benchmarking suite with thousands of diverse open-ended tasks specified in natural language prompts, and also provides an internet-scale, multimodal knowledge base of YouTube videos, Wiki pages, and Reddit posts. The database captures the collective experience and wisdom of millions of Minecraft gamers for an AI agent to learn from. Best viewed zoomed in. complex world, it would be extremely inefficient for an agent to learn everything from scratch through trial and error. Third, the agent's architecture needs to be flexible enough to pursue any task in openended environments, and scalable enough to convert large-scale knowledge sources into actionable insights [19,96]. This motivates the design of an agent that has a unified observation/action space, conditions on natural language task prompts, and adopts the Transformer pre-training paradigm [27,91,15] to internalize knowledge effectively.\nIn light of these three pillars, we introduce MINEDOJO, a new framework to help the community develop open-ended, generally-capable agents. It is built on the popular Minecraft game, where a player explores a procedurally generated 3D world with diverse types of terrains to roam, materials to mine, tools to craft, structures to build, and wonders to discover. Unlike most other games [80,85,126], Minecraft defines no specific reward to maximize and no fixed storyline to follow, making it well suited for developing open-ended environments for embodied AI research. We make the following three major contributions:\n1. Simulation platform with thousands of diverse open-ended tasks. MINEDOJO provides convenient APIs on top of Minecraft that standardize task specification, world settings, and agent's observation/action spaces. We introduce a benchmark suite that consists of thousands of natural language-prompted tasks, making it two orders of magnitude larger than prior Minecraft benchmarks like the MineRL Challenge [48,62]. The suite includes long-horizon, open-ended tasks that cannot be easily evaluated through automated procedures, such as \"build an epic modern house with two floors and a swimming pool\". Inspired by the Inception score [98] and FID score [55] that are commonly used to assess AI-generated image quality, we introduce a novel agent evaluation protocol using a large video-language model pre-trained on Minecraft YouTube videos. This complements human scoring [104] that is precise but more expensive. Our learned evaluation metric has good agreement with human judgment in a subset of the full task suite considered in the experiments.\n2. Internet-scale multimodal Minecraft knowledge base. Minecraft has more than 100 million active players [131], who have collectively generated an enormous wealth of data. They record tutorial videos, stream live play sessions, compile recipes, and discuss tips and tricks on forums. MINEDOJO features a massive collection of 730K+ YouTube videos with time-aligned transcripts, 6K+ free-form Wiki pages, and 340K+ Reddit posts with multimedia contents (Fig. 3). We hope that this enormous knowledge base can help the agent acquire diverse skills, develop complex strategies, discover interesting objectives, and learn actionable representations automatically. 3. Novel algorithm for embodied agents with large-scale pre-training. We develop a new learning algorithm for embodied agents that makes use of the internet-scale domain knowledge we have collected from the web. Using the massive volume of YouTube videos from MINEDOJO, we train a video-text contrastive model in the spirit of CLIP [92], which associates natural language subtitles with their time-aligned video segments. We demonstrate that this learned correlation score can be used effectively as an open-vocabulary, massively multi-task reward function for RL training.\nOur agent solves the majority of 12 tasks in our experiment using the learned reward model (Fig. 2). It achieves competitive performance to agents trained with meticulously engineered dense-shaping rewards, and in some cases outperforms them, with up to 73% improvement in success rates. For open-ended tasks that do not have a simple success criterion, our agents also perform well without any special modifications.\nIn summary, this paper proposes an open-ended task suite, internet-scale domain knowledge, and agent learning with recent advances on large pre-trained models [13]. We have open-sourced MINEDOJO's simulator, knowledge bases, algorithm implementations, pretrained model checkpoints, and task curation tools at https://minedojo.org/. We hope that MINEDOJO will serve as an effective starter framework for the community to develop new algorithms and advance towards generally capable embodied agents.  [48] and makes the following upgrades: 1) We provide unified observation and action spaces across all tasks, facilitating the development of multi-task and continually learning agents that can constantly adapt to new scenarios and novel tasks. This deviates from the MineRL Challenge design that tailors observation and action spaces to individual tasks; 2) Our simulation unlocks all three types of worlds in Minecraft, including the Overworld, the Nether, and the End, which substantially expands the possible task space, while MineRL only supports the Overworld natively; and 3) We provide convenient APIs to configure initial conditions and world settings to standardize our tasks.\nWith this MINEDOJO simulator, we define thousands of benchmarking tasks, which are divided into two categories: 1) Programmatic tasks that can be automatically assessed based on the ground-truth simulator states; and 2) Creative tasks that do not have well-defined or easily-automated success criteria, which motivates our novel evaluation protocol using a learned model (Sec. 4). To scale up the number of Creative tasks, we mine ideas from YouTube tutorials and use OpenAI's GPT-3 [15] service to generate substantially more task definitions. Compared to Creative tasks, Programmatic tasks are simpler to get started, but tend to have restricted scope, limited language variations, and less open-endedness in general.", "publication_ref": ["b79", "b84", "b125", "b67", "b98", "b145", "b133", "b106", "b115", "b70", "b119", "b116", "b116", "b128", "b79", "b145", "b41", "b76", "b78", "b126", "b64", "b53", "b18", "b95", "b26", "b90", "b14", "b79", "b84", "b125", "b47", "b61", "b97", "b54", "b103", "b130", "b91", "b12", "b47", "b14"], "figure_ref": ["fig_7", "fig_0"], "table_ref": []}, {"heading": "Task Suite I: Programmatic Tasks", "text": "We formalize each programmatic task as a 5-tuple: T = (G, G, I, f S , f R ). G is an English description of the task goal, such as \"find material and craft a gold pickaxe\". G is a natural language guidance that provides helpful hints, recipes, or advice to the agent. We leverage OpenAI's GPT-3-davinci API to automatically generate detailed guidance for a subset of the tasks. For the example goal \"bring a pig into Nether\", GPT-3 returns: 1) Find a pig in the overworld; 2) Right-click on the pig with a lead; 3) Right-click on the Nether Portal with the lead and pig selected; 4) The pig will be pulled through the portal! I is the initial conditions of the agent and the world, such as the initial inventory, spawn terrain, and weather. f S : s t \u2192 {0, 1} is the success criterion, a deterministic function that maps the current world state s t to a Boolean success label. f R : s t \u2192 R is an optional dense reward function. We only provide f R for a small subset of the tasks in MINEDOJO due to the high costs of meticulously crafting dense rewards. For our current agent implementation (Sec. 4.1), we do not use detailed guidance. Inspired by concurrent works SayCan [3] and Socratic Models [143], one potential idea is to feed each step in the guidance to our learned reward model sequentially so that it becomes a stagewise reward function for a complex multi-stage task.\nMINEDOJO provides 4 categories of programmatic tasks with 1,581 template-generated natural language goals to evaluate the agent's different capabilities systematically and comprehensively:\n1. Survival: surviving for a designated number of days.", "publication_ref": ["b2", "b142"], "figure_ref": [], "table_ref": []}, {"heading": "2.", "text": "Harvest: finding, obtaining, cultivating, or manufacturing hundreds of materials and objects.\n3. Tech Tree: crafting and using a hierarchy of tools.\n4. Combat: fighting various monsters and creatures that require fast reflex and martial skills.\nEach task template has a number of variations based on the terrain, initial inventory, quantity, etc., which form a flexible spectrum of difficulty. In comparison, the NeurIPS MineRL Diamond challenge [48] is a subset of our programmatic task suite, defined by the task goal \"obtain 1 diamond\" in MINEDOJO.", "publication_ref": ["b47"], "figure_ref": [], "table_ref": []}, {"heading": "Task Suite II: Creative Tasks", "text": "We define each creative task as a 3-tuple, T = (G, G, I), which differs from programmatic tasks due to the lack of straightforward success criteria. Inspired by model-based metrics like the Inception score [98] and FID score [55] for image generation, we design a novel task evaluation metric based on a pre-trained contrastive video-language model (Sec. 4.1). In the experiments, we find that the learned metric exhibits a high level of agreement with human evaluations (see Table 2).\nWe brainstorm and author 216 Creative tasks, such as \"build a haunted house with zombie inside\" and \"race by riding a pig\". Nonetheless, such a manual approach is not scalable. Therefore, we develop two systematic approaches to extend the total number of task definitions to 1,560. This makes our Creative tasks 3 orders of magnitude larger than Minecraft BASALT challenge [104], which has 4 Creative tasks.\nApproach 1. Task Mining from YouTube Tutorial Videos. We identify our YouTube dataset as a rich source of tasks, as many human players demonstrate and narrate creative missions in the tutorial playlists. To collect high-quality tasks and accompanying videos, we design a 3-stage pipeline that makes it easy to find and annotate interesting tasks (see Sec. C.2 for details). Through this pipeline, we extract 1,042 task ideas from the common wisdom of a huge number of veteran Minecraft gamers, such as \"make an automated mining machine\" and \"grow cactus up to the sky\".\nApproach 2. Task Creation by GPT-3. We leverage GPT-3's few-shot capability to generate new task ideas by seeding it with the tasks we manually author or mine from YouTube. The prompt template is: Here are some example creative tasks in Minecraft: {a few examples}. Let's brainstorm more detailed while reasonable creative tasks in Minecraft.\nGPT-3 contributes 302 creative tasks after de-duplication, and demonstrates a surprisingly proficient understanding of Minecraft terminology.", "publication_ref": ["b97", "b54", "b103"], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Collection of Starter Tasks", "text": "We curate a set of 64 core tasks for future researchers to get started more easily. If their agent works well on these tasks, they can more confidently scale to the full benchmark.\n\u2022 32 programmatic tasks: 16 \"standard\" and 16 \"difficult\", spanning all 4 categories (survival, harvesting, combat, and tech tree). We rely on our Minecraft knowledge to decide the difficulty level. \"Standard\" tasks require fewer steps and lower resource dependencies to complete. \u2022 32 creative tasks: 16 \"standard\" and 16 \"difficult\". Similarly, tasks labeled with \"standard\" are typically short-horizon tasks.\nWe recommend that researchers run 100 evaluation episodes for each task and report the percentage success rate. The programmatic tasks have ground-truth success, while the creative tasks need our novel evaluation protocol (Sec. 5).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Internet-scale Knowledge Base", "text": "Two commonly used approaches [112,126,85,36] to train embodied agents include training agents from scratch using RL with well-tuned reward functions for each task, or using a large amount of human-demonstrations to bootstrap agent learning. However, crafting well-tuned reward functions is challenging or infeasible for our task suite (Sec. 2.2), and employing expert gamers to provide large amounts of demonstration data would also be costly and infeasible [126].\nInstead, we turn to the open web as an ever-growing, virtually unlimited source of learning material for embodied agents. The internet provides a vast amount of domain knowledge about Minecraft, which we harvest by extensive web scraping and filtering. We collect 33 years worth of YouTube videos, 6K+ Wiki pages, and millions of Reddit comment threads. Instead of hiring a handful of human demonstrators, we capture the collective wisdom of millions of Minecraft gamers around the world. Furthermore, language is a key and pervasive component of our database that takes the form of YouTube transcripts, textual descriptions in Wiki, and Reddit discussions. Language facilitates open-vocabulary understanding, provides grounding for image and video modalities, and unlocks the power of large language models [27,109,15] for embodied agents. To ensure socially responsible model development, we take special measures to filter out low-quality and toxic contents [13,51] from our databases, detailed in the Appendix (Sec. D). YouTube Videos and Transcripts. Minecraft is among the most streamed games on YouTube [41].\nHuman players have demonstrated a stunning range of creative activities and sophisticated missions that take hours to complete (examples in Fig. 3). We collect 730K+ narrated Minecraft videos, which add up to 33 years of duration and 2.2B words in English transcripts. In comparison, HowTo100M [77] is a large-scale human instructional video dataset that includes 15 years of experience in total -about half of our volume. The time-aligned transcripts enable the agent to ground free-form natural language in video pixels and learn the semantics of diverse activities without laborious human labeling.\nWe operationalize this insight in our pre-trained video-language model (Sec. 4.1).\nMinecraft Wiki. The Wiki pages cover almost every aspect of the game mechanics, and supply a rich source of unstructured knowledge in multimodal tables, recipes, illustrations, and step-by-step tutorials. We use Selenium [103] to scrape 6,735 pages that interleave text, images, tables, and diagrams. The pages are highly unstructured and do not share any common schema, as the Wiki is meant for human consumption rather than AI training. To preserve the layout information, we additionally save the screenshots of entire pages and extract 2.2M bounding boxes of the visual elements (visualization in Fig.\nA.4 and A.5). We do not use Wiki data in our current experiments. Since the Wiki contains detailed recipes for all crafted objects, they could be provided as input or training data for hierarchical planning methods and policy sketches [8]. Another promising future direction is to apply document understanding models such as LayoutLM [138,137] and DocFormer [9] to learn actionable knowledge from these unstructured Wiki data.\nReddit. We scrape 340K+ posts along with 6.6M comments under the \"r/Minecraft\" subreddit. These posts ask questions on how to solve certain tasks, showcase cool architectures and achievements in image/video snippets, and discuss general tips and tricks for players of all expertise levels. We do not use Reddit data for training in Sec. 5, but a potential idea is to finetune large language models [27,91] on our Reddit corpus to generate instructions and execution plans that are better grounded in the Minecraft domain. Concurrent works [3,56,143] have explored similar ideas and showed excellent results on robot learning, which is encouraging for more future research in MINEDOJO. Here we take an initial step towards this goal by developing a proof of concept that demonstrates how a single language-prompted agent can be trained in MINEDOJO to complete several complex Minecraft tasks. To this end, we propose a novel agent learning algorithm that takes advantage of the massive YouTube data offered by MINEDOJO. We note that this is only one of the numerous possible  In this paper, we consider a multi-task reinforcement learning (RL) setting, where an agent is tasked with completing a collection of MINEDOJO tasks specified by language instructions (Sec. 2). Solving these tasks often requires the agent to interact with the Minecraft world in a prolonged fashion. Agents developed in popular RL benchmarks [119,146] often rely on meticulously crafted dense and task-specific reward functions to guide random explorations. However, these rewards are hard or even infeasible to define for our diverse and open-ended tasks in MINEDOJO. To address this challenge, our key insight is to learn a dense, language-conditioned reward function from in-the-wild YouTube videos and their transcripts. Therefore, we introduce MINECLIP, a contrastive video-language model that learns to correlate video snippets and natural language descriptions (Fig. 4) ", "publication_ref": ["b111", "b125", "b84", "b35", "b125", "b26", "b108", "b14", "b12", "b50", "b40", "b76", "b102", "b7", "b137", "b136", "b8", "b26", "b90", "b2", "b55", "b142", "b118", "b145"], "figure_ref": ["fig_7", "fig_1"], "table_ref": []}, {"heading": "Pre-Training MINECLIP on Large-scale Videos", "text": "Formally, the learned reward function can be defined as \u03a6 R : (G, V ) \u2192 R that maps a language goal G and a video snippet V to a scalar reward. An ideal \u03a6 R should return a high reward if the behavior depicted in the video faithfully follows the language description, and a low reward otherwise. This can be achieved by optimizing the InfoNCE objective [125,52,20], which learns to correlate positive video and text pairs [118,6,78,4,136].\nSimilar to the image-text CLIP model [92], MINECLIP is composed of a separate text encoder \u03c6 G that embeds a language goal and a video encoder \u03c6 V that embeds a moving window of 16 consecutive frames with 160 \u00d7 256 resolution (Fig. 4). Our neural architecture has a similar design as CLIP4Clip [75], where \u03c6 G reuses OpenAI CLIP's pretrained text encoder, and \u03c6 V is factorized into a frame-wise image encoder \u03c6 I and a temporal aggregator \u03c6 a that summarizes the sequence of 16 image features into a single video embedding. Unlike CLIP4Clip, we insert two extra layers of residual CLIP Adapter [38] after the aggregator \u03c6 a to produce a better video feature, and finetune only the last two layers of the pretrained \u03c6 I and \u03c6 G . ", "publication_ref": ["b124", "b51", "b19", "b117", "b5", "b77", "b3", "b135", "b91", "b74", "b37"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "RL with MINECLIP Reward", "text": "We train a language-conditioned policy network that takes as input raw pixels and predicts discrete control. The policy is trained with PPO [102] on the MINECLIP rewards. In each episode, the agent is prompted with a language goal and takes a sequence of actions to fulfill this goal. When calculating the MINECLIP rewards, we concatenate the agent's latest 16 egocentric RGB frames in a temporal window to form a video snippet. MINECLIP handles all task prompts zero-shot without any further finetuning. In our experiments (Sec. 5), we show that MINECLIP provides effective dense rewards out of the box, despite the domain shift between in-the-wild YouTube frames and simulator frames. Besides regular video data augmentation, we do not employ any special domain adaptation methods during pre-training. Our finding is consistent with CLIP's strong zero-shot performances on robustness benchmarks in object recognition [92].\nCompared to hard-coded reward functions in popular benchmarks [146,119,34], the MINECLIP model has 150M parameters and is thus much more expensive to query. We make several design choices to greatly accelerate RL training with MINECLIP in the loop:\n1. The language goal G is fixed for a specific task, so the text features \u03c6 G can be precomputed to avoid invoking the text encoder repeatedly.\n2. Our agent's RGB encoder reuses the pre-trained weights of \u03c6 I from MINECLIP. We do not finetune \u03c6 I during RL training, which saves computation and endows the agent with good visual representations from the beginning.\n3. MINECLIP's video encoder \u03c6 V is factorized into an image encoder \u03c6 I and a light-weight aggregator \u03c6 a . This design choice enables efficient image feature caching. Consider two overlapping video sequences of 8 frames, V[0:8] and V [1:9]. We can cache the image features of the 7 overlapping frames V [1] to V [7] to maximize compute savings. If \u03c6 V is a monolithic model like S3D [135] in VideoCLIP [136], then the video features from every sliding window must be recomputed, which would incur a much higher cost per time step.\n4. We leverage Self-Imitation Learning [84] to store the trajectories with high MINECLIP reward values in a buffer, and alternate between PPO and self-imitation gradient steps. It further improves sample efficiency as shown in the Appendix ( ", "publication_ref": ["b101", "b91", "b145", "b118", "b33", "b0", "b6", "b134", "b135", "b83"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We evaluate our agent-learning approach (Section 4) on 8 Programmatic tasks and 4 Creative tasks from the MINEDOJO benchmarking suite. We select these 12 tasks due to the diversity of skills required to solve them (e.g., harvesting, combat, building, navigation) and domain-specific entities (e.g., animals, resources, monsters, terrains, and structures). We split the tasks into 3 groups and train one multi-task agent for each group: Animal-Zoo (4 Programmatic tasks on hunting or harvesting resource from animals), Mob-Combat (Programmatic, fight 4 types of hostile monsters), and Creative (4 tasks).\nIn the experiments, we empirically check the quality of MINECLIP against manually written reward functions, and quantify how different variants of our learned model affect the RL performance. Table 1 presents our main results, and Fig. 2 visualizes our learned agent behavior in 4 of the considered tasks.\nPolicy networks of all methods share the same architecture and are trained by PPO + Self-Imitation (Sec. 4.2, training details in the Appendix, Sec. F). We compare the following methods:\n\u2022 Ours (Attn): our agent trained with the MINECLIP[attn] reward model. For Programmatic tasks, we also add the final success condition as a binary reward. For Creative tasks, MINECLIP is the only source of reward.\n\u2022 Ours (Avg): the average-pooling variant of our method.\n\u2022 Manual Reward: hand-engineered dense reward using ground-truth simulator states.\n\u2022 Sparse-only: the final binary success as a single sparse reward. Note that neither sparse-only nor manual reward is available for Creative tasks.\n\u2022 CLIP OpenAI : pre-trained OpenAI CLIP model that has not been finetuned on any MINEDOJO videos.\nMINECLIP is competitive with manual reward. For Programmatic tasks (first 8 rows), RL agents guided by MINECLIP achieve competitive performance as those trained by manual reward.\nIn three of the tasks, they even outperform the hand-engineered reward functions, which rely on privileged simulator states unavailable to MINECLIP. For a more statistically sound analysis, we conduct the Paired Student's t-test to compare the mean success rate of each task (pairing column 3 \"Ours (Attn)\" and column 5 \"Manual Reward\" in Table 1). The test yields p-value 0.3991 0.05, which indicates that the difference between our method and manual reward is not considered statistically significant, and therefore they are comparable with each other. Because all tasks require nontrivial exploration, our approach also dominates the Sparse-only baseline. Note that the original OpenAI CLIP model fails to achieve any success. We hypothesize that the creatures in Minecraft look dramatically different from their real-world counterparts, which causes CLIP to produce misleading signals worse than no shaping reward at all. It implies the importance of finetuning on MINEDOJO's YouTube data.\nMINECLIP provides automated evaluation. For Creative tasks (last 4 rows), there are no programmatic success criteria available. We convert MINECLIP into a binary success classifier by thresholding the reward value it outputs for an episode. To test the quality of MINECLIP as an automatic evaluation metric, we ask human judges to curate a dataset of 100 successful and 100 failed trajectories for each task. We then run both MINECLIP variants and CLIP OpenAI on the dataset and report the binary F1 score of their judgement against human ground-truth in Table 2. The results demonstrate that both MINECLIP[attn] and MINECLIP[avg] attain a very high degree of agreement with human evaluation results on this subset of the Creative task suite that we investigate. CLIP OpenAI baseline also achieves nontrivial agreement on Find Ocean and Dig Hole tasks, likely because real-world oceans and holes have similar texture. We use the attn variant as an automated success criterion to score the 4 Creative task results in Table 1. Our proposed method consistently learns better than CLIP OpenAI -guided agents. It shows that MINECLIP is an effective approach to solving open-ended tasks when no straightforward reward signal is available. We provide further analysis beyond these 4 tasks in the Appendix (Sec. G.4). MINECLIP shows good zero-shot generalization to significant visual distribution shift. We evaluate the learned policy without finetuning on a combination of unseen weather, lighting conditions, and terrains -27 scenarios in total. For the baseline, we train agents with the original CLIP OpenAI image encoder (not trained on our YouTube videos) by imitation learning. The robustness against visual shift can be quantitatively measured by the relative performance degradation on novel test scenarios for each task. Table 3 shows that while all methods incur performance drops, agents with MINECLIP encoder is more robust to visual changes than the baseline across all tasks. Pre-training on diverse in-the-wild YouTube videos is important to boosting zero-shot visual generalization capability, a finding consistent with literature [92,82].\nLearning a Single Agent for All 12 Tasks We have also trained a single agent for all 12 tasks. To reduce the computational burden without loss of generality, the agent is trained by self-imitating from successful trajectories generated from the self-imitation learning pipeline (Section F.3). We summarize the result in Table 4. Similar to our main experiments, all numbers represent percentage success rates averaged over 3 training seeds, each tested for 200 episodes. Compared to the original agents, the new 12-multitask agent sees a performance boost in 6 tasks, degradation in 4 tasks, and roughly the same success rates in 2 tasks. This result suggests that there are both positive and negative task transfers happening. To improve the multi-task performance, more advanced algorithms [141,133] can be employed to mitigate the negative transfer effects.\nWe also perform Paired Student's t-test to statistically compare the performance of the 12-multitask agent and those separately trained on each task group. We obtain a p-value of 0.3720 0.05, which suggests that the difference between the two training settings is not statistically significant.\nGeneralize to Novel Tasks To test the ability to generalize to new open-vocabulary commands, we evaluate the agent on two novel tasks: \"harvest spider string\" and \"hunt pig\". Table 5 shows that the agent struggles in the zero-shot setting because it has not interacted with pigs or spider strings during training, and thus does not know how to interact with them effectively. However, the performance improves substantially by finetuning with the MINECLIP reward. Here the baseline methods are trained from scratch using RL with the MINECLIP encoders and reward. Therefore, the only difference is whether the policy has been pre-trained on the 12 tasks or not. Given the same environment sampling budget (only around 5% of total samples), it significantly outperforms baselines. It suggests that the multitask agent has learned transferable knowledge on hunting and resource collection, which enables it to quickly adapt to novel tasks.", "publication_ref": ["b91", "b81", "b140", "b132"], "figure_ref": ["fig_0"], "table_ref": ["tab_2", "tab_2", "tab_4", "tab_2", "tab_5"]}, {"heading": "Related work", "text": "Open-ended Environments for Decision-making Agents. There are many environments developed with the goal of open-ended agent learning. Prior works include maze-style worlds [121,129,61], purely text-based game [69], grid worlds [21,16], browser/GUI-based environments [108,124], and indoor simulators for robotics [1,107,114,34,110,99,89]. Minecraft offers an exciting alternative for open-ended agent learning. It is a 3D visual world with procedurally generated landscapes and extremely flexible game mechanics that support an enormous variety of activities. Prior methods in open-ended agent learning [30,57,130,63,26] do not make use of external knowledge, but our approach leverages internet-scale database to learn open-vocabulary reward models, thanks to Minecraft's abundance of gameplay data online.\nMinecraft for AI Research. The Malmo platform [60] is the first comprehensive release of a Gym-style agent API [14] for Minecraft. Based on Malmo, MineRL [48] provides a codebase and human play trajectories for the annual Diamond Challenge at NeurIPS [47,49,62]. MINEDOJO's simulator builds upon the pioneering work of MineRL, but greatly expands the API and benchmarking task suite. Other Minecraft benchmarks exist with different focuses. For example, CraftAssist [44] and IGLU [66] study agents with interactive dialogues. BASALT [104] applies human evaluation to 4 open-ended tasks. EvoCraft [45] is designed for structure building, and Crafter [50] optimizes for fast experimentation. Unlike prior works, MINEDOJO's core mission is to facilitate the development of generally capable embodied agents using internet-scale knowledge. We include a feature comparison table of different Minecraft platforms for AI research in Table A\n.1.\nInternet-scale Multimodal Knowledge Bases. Big dataset such as Common Crawl [24], the Pile [37], LAION [100], YouTube-8M [2] and HowTo100M [77] have been fueling the success of large pretrained language models [27,91,15] and multimodal models [118,6,78,145,7,4,136]. While generally useful for learning representations, these datasets are not specifically targeted at embodied agents. To provide agent-centric training data, RoboNet [25] collects video frames from 7 robot platforms, and Ego4D [43] recruits volunteers to record egocentric videos of household activities. In comparison, MINEDOJO's knowledge base is constructed without human curation efforts, much larger in volume, more diverse in data modalities, and comprehensively covers all aspects of the Minecraft environment.\nEmbodied Agents with Large-scale Pre-training. Inspired by the success in NLP, embodied agent research [29,11,94,23] has seen a surge in adoption of the large-scale pre-training paradigm. The recent advances can be roughly divided into 4 categories. 1) Novel agent architecture: Decision Transformer [19,58,144] applies the powerful self-attention models to sequential decision making. GATO [95] and Unified-IO [74] learn a single model to solve various decision-making tasks with different control interfaces. VIMA [59] unifies a wide range of robot manipulation tasks with multimodal prompting. 2) Pre-training for better representations: R3M [82] trains a general-purpose visual encoder for robot perception on Ego4D videos [43]. CLIPort [111] leverages the pre-trained CLIP model [92] to enable free-form language instructions for robot manipulation.\n3) Pre-training for better policies: AlphaStar [126] achieves champion-level performance on StarCraft by imitating from numerous human demos. SayCan [3] leverages large language models (LMs) to ground value functions in the physical world. [72] and [96] directly reuse pre-trained LMs as policy backbone. VPT [10] is a concurrent work that learns an inverse dynamics model from human contractors to pseudo-label YouTube videos for behavior cloning. VPT is complementary to our approach, and can be finetuned to solve language-conditioned open-ended tasks with our learned reward model. 4) Data-driven reward functions: Concept2Robot [105] and DVD [18] learn a binary classifier to score behaviors from in-the-wild videos [42]. LOReL [81] crowd-sources humans labels to train language-conditioned reward function for offline RL. AVID [113] and XIRL [142] extract reward signals via cycle consistency. MINEDOJO's task benchmark and internet knowledge base are generally useful for developing new algorithms in all the above categories. In Sec. 4, we also propose an open-vocabulary, multi-task reward model using MINEDOJO YouTube videos.", "publication_ref": ["b120", "b128", "b60", "b68", "b20", "b15", "b107", "b123", "b0", "b106", "b113", "b33", "b109", "b98", "b88", "b29", "b56", "b129", "b62", "b25", "b59", "b13", "b47", "b46", "b48", "b61", "b43", "b65", "b103", "b44", "b49", "b23", "b36", "b99", "b1", "b76", "b26", "b90", "b14", "b117", "b5", "b77", "b144", "b6", "b3", "b135", "b24", "b42", "b28", "b10", "b93", "b22", "b18", "b57", "b143", "b94", "b73", "b58", "b81", "b42", "b110", "b91", "b125", "b2", "b71", "b95", "b9", "b104", "b17", "b41", "b80", "b112", "b141"], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Conclusion", "text": "In this work, we introduce the MINEDOJO framework for developing generally capable embodied agents. MINEDOJO features a benchmarking suite of thousands of Programmatic and Creative tasks, and an internet-scale multimodal knowledge base of videos, wiki, and forum discussions. As an example of the novel research possibilities enabled by MINEDOJO, we propose MINECLIP as an effective language-conditioned reward function trained with in-the-wild YouTube videos. MINECLIP achieves strong performance empirically and agrees well with human evaluation results, making it a good automatic metric for Creative tasks. We look forward to seeing how MINEDOJO empowers the community to make progress on the important challenge of open-ended agent learning. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgement", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Minecraft Framework Comparison", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B MINEDOJO Simulator", "text": "We design unified observation and action spaces across all tasks to facilitate the development of multi-tasking and continually learning agents that can adapt to novel tasks and scenarios. The codebase is open sourced at github.com/MineDojo/MineDojo.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.1 Observation Space", "text": "Our observation space contains multiple modalities. The agent perceives the world mainly through the RGB screen. To provide the same information as human players receive, we also supplement the agent with observations about its inventory, location, health, surrounding blocks, etc. The full observation space is shown below. We refer readers to see our code documentation for technical details such as data type for each observable item.\nWe also support a LIDAR sensor that returns the groundtruth type of the blocks that the agent sees, however this is considered privileged information and does not go into the benchmarking specification. However, it is still useful for hand engineering the dense reward function, which we use in our experiments (Sec. 5). Amounts and directions of LIDAR rays can be arbitrarily configured at the cost of a lower simulation throughput. Harvest. This task group tests the agent's ability to collect useful resources such as minerals (iron, diamond, obsidian), food (beef, pumpkin, carrots, milk), and other useful items (wool, oak wood, coal). We construct these tasks by enumerating the Cartesian product between target items to collect, initial inventory, and world conditions (terrain, weather, lighting, etc.) so that they cover a spectrum of difficulty. For instance, if the task is to harvest wool, then it is relatively easy if the agent has a shear in its initial inventory with a sheep nearby, but more difficult if the agent has to craft the shear from raw material and explore extensively to find a sheep. We filter out combinations that are impossible (such as farming certain plants in the desert) from the Cartesian product.\nTech Tree. Minecraft includes several levels of tools and armors with different properties and difficulties to unlock. To progress to a higher level of tools and armors, the agent needs to develop systematic and compositional skills to navigate the technology tree (e.g. wood \u2192 stone \u2192 iron \u2192 diamond). In this task group, the agent is asked to craft and use a hierarchy of tools starting from a less advanced level. For example, some task asks the agent to craft a wooden sword from bare hand. Another task may ask the agent to craft a gold helmet. An agent that can successfully complete these tasks should have the ability to transfer similar exploration strategies to different tech levels.\nCombat. We test agent's reflex and martial skills to fight against various monsters and creatures. Similar to how we develop the Harvest task group, we generate these tasks by enumerating the Cartesian product between the target entity to combat with, initial inventory, and world conditions to cover a spectrum of difficulty.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.2 Creative Tasks", "text": "We construct Creative tasks using three approaches: 1) manual brainstorming, 2) mining from YouTube tutorial videos, and 3) generate by querying GPT-3 API. We elaborate the second and third approaches below.\nTask Mining from YouTube Tutorial Videos. Our YouTube dataset serves the dual purpose of a rich task source, as many human players demonstrate and narrate creative missions in the tutorial playlists. To collect high-quality tasks and accompanying videos, we design a 3-stage pipeline that makes it easy to find and annotate interesting tasks.\nStage 1: We search for YouTube playlists with the key phrases, \"Minecraft Tutorial\" and \"Minecraft Guide\". Then we apply heuristic rules (see Sec. D.1) to filter out low-quality videos;\nStage 2: We only show the title of the video to a human annotator through a command-line interface, who makes a binary decision to accept or reject it as a potential task. This step is typically very fast, taking a few seconds on average; Stage 3: For the accepted tasks in stage 2, we design a labeling UI using Label Studio [70] that displays the full video and YouTube description. A human annotator can choose to reject the video, adjust the timestamps, select the title, or refine the description to be the task goal (Fig. A.2). Through this pipeline, we extract 1,042 task ideas from the common wisdom of a huge number of veteran Minecraft gamers. Some examples are \"make an automated mining machine\" and \"grow cactus up to the sky\".", "publication_ref": ["b69"], "figure_ref": [], "table_ref": []}, {"heading": "C.3 GPT-3 Guidance", "text": "We leverage OpenAI's GPT-3-davinci API to automatically generate detailed guidance for a subset of the tasks. Inspired by [67], we adopt the following template to prompt GPT-3: How to {task goal} in Minecraft? Let's think step by step. Here are some examples:\nThe guidance for the task \"find material and craft a gold pickaxe\" is 1) Find a place with a lot of trees; 2) Cut down the trees and gather the wood; 3) Find a place with a lot of stone; 4) Mine the stone and gather the cobblestone; 5) Find a place with a lot of iron; 6) Mine the iron and gather the iron ingots; 7) Find a place with a lot of gold; 8) Mine the gold and gather the gold ingots; 9) Craft a gold pickaxe.\nThe guidance for the task \"sail on boat with a sheep\" is 1) Find a boat; 2) Place the sheep in the boat; 3) Right-click on the boat with an empty hand to get in; 4) Use the WASD keys to move the boat. The sheep should stay in the boat.", "publication_ref": ["b66"], "figure_ref": [], "table_ref": []}, {"heading": "C.4 Playthrough: Defeat the Ender Dragon", "text": "Our benchmarking suite includes a special task called \"Playthrough\". The agent is initialized barehanded in a freshly created world and aims to defeat the Ender dragon, which is considered the final boss of Minecraft. This task holds a unique position in our benchmark because killing the dragon means \"beating the game\" in the traditional sense of the phrase, and is considered the most significant achievement for a new player. This boss is optional and plenty of people choose to skip it without affecting their open-ended game experience. \"Playthrough\" is technically a programmatic task, because we can check the simulator state for the defeat of the Ender dragon. However, we decide to create its own category due to the uniqueness as well as the sheer difficulty of the task. The mission requires lots of preparation, exploration, agility, and trial-and-error, which may take a regular human player many days to complete. It would be extremely long horizon (hundreds of thousands of steps) and difficult for an agent to tackle. We consider this one of the moonshot goals in MINEDOJO.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D Internet-Scale Database", "text": "We upload our databases to zenodo.org, which is an open repository platform operated by CERN. The data DOIs, URLs, and licenses are listed below. In this section, we describe our database properties and data collection process in details. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.1 YouTube Videos and Transcripts", "text": "Minecraft is among the most streamed games on YouTube [41]. Human players have demonstrated a stunning range of creative activities and sophisticated missions that take hours to complete. We collect 33 years worth of video and 2.2B words in the accompanying English transcripts. The distribution of video duration is shown in Fig. A.3. The time-aligned transcripts enable the agent to ground free-form natural language in video pixels and learn the semantics of diverse activities without laborious human labeling.\nWe use the official YouTube Data API [140] to collect our database, following the procedure below: a) Search for channels that contain Minecraft videos using a list of keywords, e.g., \"Minecraft\", \"Minecraft Guide\", \"Minecraft Walkthrough\", \"Minecraft Beginner\". We do not directly search for videos at this step because there is a limit of total results returned by the API;\nb) Search for all the video IDs uploaded by each channel that we obtain at the previous step. There are many false positives at this step because some channels (like gaming news channel) may cover a range of topics other than Minecraft; c) To remove the false positives, we rely on the video category chosen by the user when the video was uploaded and filter out all the videos that do not belong to the Minecraft category; d) To curate a language-grounded dataset, we favor videos that have English transcripts, which can be manually uploaded by the user, automatically transcribed from audio, or automatically translated from another language by the YouTube engine. For each video, we filter it out if 1) the view count is less than 100; or 2) the aspect ratio is less 1; or 3) the duration is less than 1 minute long; or 4) marked as age-restricted.\ne) To further clean the dataset and remove potentially harmful contents, we employ the Detoxify [51] tool to process each video title and description. Detoxify is trained on Wikipedia comments to predict multiple types of toxicity like threats, obscenity, insults, and identitybased hate speech. We delete a video if the toxicity probability in any category is above 0.5.\nWe release all the video IDs along with metadata such as video titles, view counts, like counts, duration, and FPS. In line with prior practice [64], we do not release the actual MP4 files and transcripts due to legal concerns.", "publication_ref": ["b40", "b139", "b50", "b63"], "figure_ref": [], "table_ref": []}, {"heading": "D.2 Minecraft Wiki", "text": "The Wiki pages cover almost every aspect of the game mechanics, and supply a rich source of unstructured knowledge in multimodal tables, recipes, illustrations, and step-by-step tutorials (example screenshots in ). We use Selenium [103] to scrape 6,735 pages that interleave text, images, tables, and diagrams. We elaborate the details of each web element scraped by Selenium: a) Screenshot. Using Selenium's built-in function, we take a full screenshot of the rendered Wiki page in order to preserve the human-readable visual formatting. We also record the bounding boxes of each salient web element on the page. b) Text. We hand-select several HTML tags that likely contain meaningful text data, such as p, h1, h2, ul, dl. c) Images and Animations. We download the raw source file of each image element (JPG, PNG, GIF, etc.), as well as the corresponding caption if available. There are also animation effects enabled by JavaScript on the Wiki. We save all image frames in the animation. d) Sprites. Sprite elements are micro-sized image icons that are typically embedded in text to create multimodal tutorials and explanations. We save all the sprites and locate their bounding boxes within the text too. We store the header cells separately as they carry the semantic meaning of each column. A table can be easily reconstructed with the stored text strings and bounding boxes.", "publication_ref": ["b102"], "figure_ref": [], "table_ref": []}, {"heading": "D.3 Reddit", "text": "There are more than 1M subreddits (i.e., Reddit topics) where people can discuss a wide range of themes and subjects. Prior works use Reddit data for conversational response selection [5,139,54] and abstractive summarization [127,65]  We release all post IDs and their corresponding metadata. We also provide a Python function based on PRAW for researchers to download the post contents after obtaining a license key for the official Reddit API.", "publication_ref": ["b4", "b138", "b53", "b126", "b64"], "figure_ref": [], "table_ref": []}, {"heading": "E MINECLIP Algorithm Details", "text": "We implement all our neural networks in PyTorch v1.11 [86]. Training MINECLIP uses the PyTorch-Lightning framework [32], pre-trained models hosted on HuggingFace [132], and the x-transformers library for Transformer variants [128]. ", "publication_ref": ["b85", "b31", "b131", "b127"], "figure_ref": [], "table_ref": []}, {"heading": "E.1 Video-Text Pair Extraction", "text": "Similar to VideoCLIP [136], we sample 640K pairs of 16-second video snippets and time-aligned English transcripts by the following procedure:\n1) Collect a list of keywords corresponding to the supported entities, blocks, and items in Minecraft;\n2) Perform string matching over our YouTube video transcripts to obtain 640K text segments; ", "publication_ref": ["b135"], "figure_ref": [], "table_ref": []}, {"heading": "E.2 Architecture", "text": "MINECLIP architecture is composed of three parts:\nFrame-wise image encoder \u03c6 I We use the ViT-B/16 architecture [28] to compute a 512-D embedding for each RGB frame. We initialize the weights from OpenAI CLIP's public checkpoint [92] and only finetune the last two layers during training. The input resolution is 160 \u00d7 256, which is different from CLIP's default 224 \u00d7 224 resolution. We adapt the positional embeddings via bicubic interpolation, which does not introduce any new learnable parameters.\nTemporal aggregator \u03c6 a Given a sequence of frame-wise RGB features, a temporal aggregator network summarizes the sequence into one video embedding. After the aggregator, we insert two extra layers of residual CLIP Adapter [38]. The residual weight is initialized such that it is very close to an identity function at the beginning of training. We consider two variants of \u03c6 a :\n1. Average pooling (MINECLIP[avg]): a simple, parameter-free operator. It is fast to execute but loses the temporal information, because average pooling is permutation-invariant.", "publication_ref": ["b27", "b91", "b37"], "figure_ref": [], "table_ref": []}, {"heading": "Self-Attention (MINECLIP[attn]", "text": "): a 2-layer transformer encoder with 512 embedding size, 8 attention heads, and Gated Linear Unit variant with Swish activation [106,22]. The transformer sequence encoder is relatively slower, but captures more temporal information and achieves better performance in our experiments (Table 1).\nText encoder \u03c6 G We use a 12-layer 512-wide GPT model with 8 attention heads [90,91]. The input string is converted to lower-case byte pair encoding with a 49,152 vocabulary size, and capped at 77 tokens. We exactly follow the text encoder settings in CLIP and initialize the weights from their public checkpoint. Only the last two layers of \u03c6 G is finetuned during training. ", "publication_ref": ["b105", "b21", "b89", "b90"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "E.3 Training", "text": "We train MINECLIP on the 640K video-text pairs for 2 epochs. We sample 16 RGB frames from each video uniformly, and apply temporally-consistent random resized crop [17,33] as data augmentation. We use Cosine learning rate annealing with 500 gradient steps of warming up [73]. We apply a lower learning rate (\u00d70.5) on the pre-trained weights and layer-wise learning rate decay for better finetuning [53]. Training is performed on 1 node of 8\u00d7 V100 GPUs with FP16 mixed precision [76] via the PyTorch native amp module. All hyperparameters are listed in Table A\n.2.", "publication_ref": ["b16", "b32", "b72", "b52", "b75"], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "F Policy Learning Details", "text": "In this section, we elaborate how a trained MINECLIP can be adapted as a reward function with two different formulations. We then discuss the algorithm for policy learning. Finally, we demonstrate how we combine self imitation learning and on-policy learning to further improve sample efficiency.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F.1 Adapt MINECLIP as Reward Function", "text": "We investigate two ways to convert MINECLIP output to scalar reward, dubbed DIRECT and DELTA. The ablation results for Animal-Zoo task group are presented in Table A\n.3.\nDirect. For a task T with the goal description G, MINECLIP outputs the probability P G that the observation video semantically corresponds to G, against a set of negative goal descriptions G \u2212 . Note that we omit timestep subscript for simplicity. As an example, for the task \"shear sheep\", G is \"shear a sheep\" and G \u2212 may include negative prompts like \"milk a cow\", \"hunt a sheep\", \"hunt a cow\", etc. To compute the DIRECT reward, we further process the raw probability using the formula r = max(P G \u2212 1 N T , 0) where N T is the number of prompts passed to MINECLIP. 1 N T is the baseline probability of randomly guessing which text string corresponds to the video. We threshold r at zero to avoid highly uncertain probability estimates below the random baseline. We call the variant without the post-processing DIRECT-Naive: r = P G as the reward signal for every time step.\nDelta. The DIRECT formulation yields strong performance when the task is concerned with moving creatures, e.g. farm animals and monsters that run around constantly. However, we discover that DIRECT is suboptimal if the task deals with static objects, e.g., \"find a nether portal\". Simply using the  We alternate between the PPO phase and the SI phase. A pseudocode of our interleaved training procedure is given in Algorithm 1. We use a prioritized strategy to sample trajectories from the buffer D SI . Specifically, we assign equal probability to all successful trajectories. Unsuccessful trajectories can still be sampled but with lower probabilities proportional to their episodic returns. \nIn", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "G Experiment Details", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G.1 Task Details", "text": "We experiment with three task groups with four tasks per group. We train one multi-task agent for each group. In this section, we describe each task goals, initial setup, and the manual dense-shaping reward function.\nAnimal Zoo: 4 Programmatic tasks on hunting or harvesting resource from animals. We spawn various animal types (pig, sheep, and cow) in the same environment to serve as distractors. It is considered a failure if the agent does not take action on the correct animal specified by the prompt.\n\u2022 Milk Cow: find and approach a cow, then obtain milk from it with an empty bucket. The prompt is milk a cow. We initialize the agent with an empty bucket to collect milk. We also spawn sheep, cow, and pig nearby the agent. The manual dense reward shaping is a navigation reward based on geodesic distance obtained from privileged LIDAR. The combined reward passed to PPO can be formulated as r t = \u03bb nav max(d min,t\u22121 \u2212 d min,t , 0) + \u03bb success 1(milk collected), where \u03bb nav = 10 and \u03bb success = 200. d min,t = min(d min , d t )\nwhere d min denotes the minimal distance to the cow that the agent has achieved so far in the episode history.\n\u2022 Hunt Cow: find and approach a cow, then hunt with a sword. The cow will run away so the agent needs to chase after it. The prompt is hunt a cow. We initialize the agent with a diamond sword. The manual dense reward shaping consists of two parts, a valid attack reward and a navigation reward based on geodesic distance obtained from privileged LIDAR.\nMathematically, the reward is r t = \u03bb attack 1(valid attack) + \u03bb nav max(d min,t\u22121 \u2212 d min,t , 0) + \u03bb success 1(cow hunted), where \u03bb attack = 5, \u03bb nav = 1, and \u03bb success = 200. We additionally reset d min every time the agent hits the cow to encourage the chasing behavior.\n\u2022 Shear Sheep: find and approach a sheep, then collect wool from the sheep with a shear. The prompt is shear a sheep. We initialize the agent with a shear. The manual dense reward shaping is a navigation reward based on geodesic distance obtained from the privileged LIDAR sensor, similar to \"Milk Cow\".\n\u2022 Hunt Sheep: find and approach a sheep, then hunt with a sword. The sheep will run away so the agent needs to chase after it. An episode will terminate once any entity is hunted. The prompt is hunt a sheep. We initialize the agent with a diamond sword. The manual dense Our action space is a trimmed version of the full action space. It consists of movement control, camera control, \"use\" action, and \"attack\" action, which add up to 89 discrete choices. Concretely, it includes 81 actions for discrete camera control (9 \u00d7 9 resulted from the Cartesian product between yaw and pitch, each ranges from \u221260 degree to 60 degree with a discrete interval of 15 degree). It also includes 6 movement actions (forward, forward + jump, jump, back, move left, and move right) and 2 functional actions of \"use\" and \"attack\". Note that the \"no-op\" action is merged into the 81 camera actions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G.3 RL Training", "text": "All hyperparameters used in our RL experiment are listed in Table A. 4. We visualize the learned behaviors of 4 tasks in Figure 2. Demos of more tasks can be found on our website https: //minedojo.org.\nAction smoothing. Due to the stochastic nature of PPO, we observe a lot of action jittering in the agent's behavior during training. This leads to two negative effects that degrade the learning performance: 1) exploration difficulty due to inconsistent action sequence. For example, the agent may be required to take multiple consecutive attack actions in order to complete certain tasks; and 2) rapidly switching different movement and camera motions result in videos that are highly non-smooth and disorienting. This causes a domain gap from the training data of MINECLIP, which are typically smooth human gameplay videos. Therefore, the reward signal quality deteriorates significantly.\nTo remedy the issue, we impose an action smoothing loss to be jointly optimized with the PPO objective (Eq. 2) during training. Concretely, consider a sliding window W with window size |W| that contains |W| consecutive action distributions W = {\u03c0 t\u2212|W|+1 , \u03c0 t\u2212|W|+2 , . . . , \u03c0 t }, the action smoothing loss is defined as\nL smooth = 1 |W| |W|\u22121 i=1 KL(\u03c0 t \u03c0 t\u2212|W|+i ),(3)\nwhere KL(\u2022) denotes Kullback-Leibler divergence.\nMulti-stage training for multi-task RL. Due to hardware limitations, we are not able to run a large number of parallel simulators for all tasks in a task group. Therefore, we adopt a multi-stage strategy to split the tasks and train them sequentially with a single policy network. For the task groups Animal-Zoo and Creative, we split the four tasks into two stages of two parallel training tasks each. We carry over the self-imitation buffers when switching to the next stage. We also follow the recommended practice in [83] and reset the policy head at the beginning of stage 2 to encourage exploration and reduce overfitting. We adopt a similar replay buffer balancing strategy as [46] to prevent any task from dominating the training.", "publication_ref": ["b3", "b82", "b45"], "figure_ref": ["fig_0"], "table_ref": ["tab_7"]}, {"heading": "G.4 Evaluation", "text": "In this section, we elaborate on our human and automatic evaluation procedure for Creative tasks. We first ask the human annotators to manually label 100 successful and 100 failure trajectories. This produces a combined dataset of 200 trajectories with groundtruth binary labels to evaluate the learned reward functions. On this dataset, we run MINECLIP to produce step-wise rewards and compute a score that averages over each trajectory. We then apply K-means clustering with K = 2 to all scores and determine a decision boundary \u03b4 from the mean of the two centroids. A trajectory with a score greater than \u03b4 is classified as successful, and vice versa for failure. In this way, we essentially convert MINECLIP to a binary classifier. The quality of MINECLIP can be measured by the F1 score of its binary classification output against the human labels. We demonstrate that MINECLIP has high agreements with humans (Table 2), and thus qualifies as an effective automatic evaluation metric for Creative tasks in the absence of human judges.\nTo further investigate MINECLIP's evaluation on more complex Creative tasks, we annotate 50 YouTube video segments each for 5 more tasks that are much more semantically complex: \"build a farm\", \" build a fence\", \"build a house\", \"ride a minecart\", and \"build a swimming pool\". We then run MINECLIP evaluation on these videos against a negative set. As shown in Table A.5, though not perfect, MINECLIP generally has a positive agreement with human judgment. We note that the current MINECLIP is a proof-of-concept step in leveraging internet data for automated evaluation, and further scaling on more training data and parameters may lead to more improvements. Meanwhile, human judgment remains a useful and important alternative [93,97].", "publication_ref": ["b92", "b96"], "figure_ref": [], "table_ref": ["tab_4", "tab_7"]}, {"heading": "H Limitations and Potential Societal Impact", "text": "Unlike human demonstrations [126] or offline RL datasets [35], our YouTube dataset contains only the video screen observations but not the actual control actions. This allows us to scale up the dataset tremendously, but at the same time poses a challenge to imitation learning algorithms that require observation-action pairs to learn. Our proposed algorithm, MINECLIP, side-steps this problem by learning a reward model, but we believe that directly inferring the human expert policy from YouTube is another important direction complementary to our approach. There are promising techniques that can potentially overcome this limitation, such as the Learning-from-Observation (LfO) family of algorithms [123,122,115,31].\nOur database is scraped from the internet, which inevitably contains offensive YouTube videos or toxic Reddit posts. While we have made our best effort to filter out these harmful contents (Sec. D.1), there can still be undesirable biases and toxicity that elude our automatic filters. Furthermore, we advocate the use of large pre-trained language models in our main paper, and MINECLIP is finetuned from the pre-trained weights of OpenAI CLIP [92]. These foundation models are known to contain harmful stereotypes and generate hateful commentary [15,13,40]. We ask the researchers who will use our code and database to exercise their best judgment during new model development to avoid any negative social impact.", "publication_ref": ["b125", "b34", "b122", "b121", "b114", "b30", "b91", "b14", "b12", "b39"], "figure_ref": [], "table_ref": []}, {"heading": "I Datasheet", "text": "We present a Datasheet [39] for documentation and responsible usage of our internet knowledge databases.", "publication_ref": ["b38"], "figure_ref": [], "table_ref": []}, {"heading": "I.1 Motivation", "text": "For what purpose was the dataset created? We create this internet-scale multimodal knowledge base to facilitate research towards open-ended, generally capable embodied agents.\nWho created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)? This knowledge base was created by Linxi Fan (Nvidia), Guanzhi Wang (Caltech), Yunfan Jiang (Stanford), Ajay Mandlekar (Nvidia), Yuncong Yang (Columbia), Haoyi Zhu (SJTU), Andrew Tang (Columbia), De-An Huang (Nvidia), Yuke Zhu (Nvidia and UT Austin), and Anima Anandkumar (Nvidia and Caltech).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "I.2 Distribution", "text": "Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? Yes, the dataset is publicly available on the internet.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "We are extremely grateful to Anssi Kanervisto, Shikun Liu, Zhiding Yu, Chaowei Xiao, Weili Nie, Jean Kossaifi, Jonathan Raiman, Neel Kant, Saad Godil, Jaakko Haapasalo, Bryan Catanzaro, John Spitzer, Zhiyuan \"Jerry\" Lin, Yingqi Zheng, Chen Tessler, Dieter Fox, Oli Wright, Jeff Clune, Jack Parker-Holder, and many other colleagues and friends for their helpful feedback and insightful discussions. We also thank the anonymous reviewers for offering us highly constructive advice and kind encouragement during the review and rebuttal period. NVIDIA provides the necessary computing resource and infrastructure for this project. Guanzhi Wang is supported by the Kortschak fellowship in Computing and Mathematical Sciences at Caltech.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Modality Shape Description", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "RGB (3, H, W)", "text": "Ego-centric RGB frames. Equipment (6,) Names, quantities, variants, and durabilities of equipped items. Inventory (36,) Names, quantities, variants, and durabilities of inventory items. Voxel (3,3,3) Names, variants, and properties of 3 \u00d7 3 \u00d7 3 surrounding blocks. Life statistics (1,) Agent's health, oxygen, food saturation, etc. GPS (3,) GPS location of the agent. Compass (2,) Yaw and pitch of the agent. Nearby tools (2,) Indicate if crafting table and furnace are nearby the agent. Damage source (1,) Information about the damage on the agent. Lidar (Num rays,) Ground-truth lidar observation.", "publication_ref": ["b2", "b2", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "B.2 Action Space", "text": "We design a compound action space. At each step the agent chooses one movement action (forward, backward, camera actions, etc.) and one optional functional action as listed in the table below. Some functional actions such as craft take one argument, while others like attack does not take any argument. This compound action space can be modelled in an autoregressive manner [126]. We refer readers to our code documentation for example usages of our action space.", "publication_ref": ["b125"], "figure_ref": [], "table_ref": []}, {"heading": "Name Description Argument", "text": "no_op Do nothing. \u2205 use Use the item held in the main hand.\n\u2205 drop Drop the item held in the main hand.\n\u2205 attack Attack with barehand or tool held in the main hand. \u2205 craft Execute a crafting recipe to obtain new items.\nIndex of recipe equip Equip an inventory item.\nSlot index of the item place Place an inventory item on the ground.\nSlot index of the item destroy Destroy an inventory item.\nSlot index of the item", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.3 Customizing the Environment", "text": "Environments in MINECLIP simulator can be easily and flexibly customized. Through our simulator API, users can control terrain, weather, day-night condition (different lighting), the spawn rate and range of specified entities and materials, etc. We support a wide range of terrains, such as desert, jungle, taiga, and iced plain, and special in-game structures, such as ocean monument, desert temple, and End city. Please visit our website for video demonstrations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C MINEDOJO Task Suite", "text": "In this section, we explain how we collect the Programmatic (Sec. 2.1) and Creative tasks (Sec. 2.2).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1 Programmatic Tasks", "text": "Programmatic tasks are constructed by filling manually written templates for four categories of tasks, namely \"Survival\", \"Harvest\", \"Tech Tree\", and \"Combat\". The task specifications are included in our codebase. Please refer to Survival. This task group tests the ability to stay alive in the game. It is nontrivial to survive in Minecraft, because the agent grows hungry as time passes and the health bar drops gradually. Hostile mobs like zombie and skeleton spawn at night, which are very dangerous if the agent does not have the appropriate armor to protect itself or weapons to fight back. We provide two tasks with different levels of difficulty for Survival. One is to start from scratch without any assistance. The other is to start with initial weapons and food. raw probability from MINECLIP as reward can cause the learned agent to stare at the object of interest but fail to move closer and interact. Therefore, we propose to use an alternative formulation, DELTA, to remedy this issue. Concretely, the reward value at timestep t becomes r t = P G,t \u2212P G,t\u22121 . We empirically validate that this formulation provides better shaped reward for the task group with static entities.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F.2 Policy Network Architecture", "text": "Our policy architecture consists of three parts: an input feature encoder, a policy head, and a value function. To handle multimodal observations (Sec. B.1), the feature extractor contains several modality-specific components:\n\u2022 RGB frame: we use the frozen frame-wise image encoder \u03c6 I in MINECLIP to optimize for compute efficiency and provide the agent with good visual representations from the beginning (Sec. 4.2). \u2022 Task goal: \u03c6 G computes the text embedding of the natural language task goal.\n\u2022 Yaw and Pitch: compute sin(\u2022) and cos(\u2022) features respectively, then pass through an MLP.\n\u2022 GPS: normalize and featurize via MLP.\n\u2022 Voxel: to process the 3 \u00d7 3 \u00d7 3 surrounding voxels, we embed discrete block names to dense vectors, flatten them, and pass through an MLP. \u2022 Past action: our agent is conditioned on its immediate past action, which is embedded and featurized by MLP.\nFeatures from all modalities are concatenated, passed through another fusion MLP, and finally fed into the policy head and value function head. We use an MLP to model the policy head that maps from the input feature vectors to the action probability distribution. We use another MLP to estimate the value function, conditioned on the same input features.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F.3 RL Training", "text": "PPO. We use the popular PPO algorithm [102] (Proximal Policy Optimization) as our RL training backbone. PPO is an on-policy method that optimizes for a surrogate objective while ensuring that the deviation from the previous policy is relatively small. PPO updates the policy network by\nwhere L(s, a, \u03b8 old , \u03b8) = min \u03c0 \u03b8 (a|s) \u03c0 \u03b8old (a|s)\nA is an estimator of the advantage function (GAE [101] in our case) and is a hyperparameter that controls the deviation between the new policy and the old one.\nSelf Imitation Learning. We apply self-imitation learning [84] (SI) to further improve sample efficiency because computing the reward with MINECLIP in the loop makes the training more expensive. Self-imitation learning is essentially supervised learning on a buffer D SI of good trajectories generated by the agent's past self. In our case, the trajectories are generated by the behavior policy during PPO rollouts, and only added to D SI if it is a successful trial or if the episodic return exceeds a certain threshold. Self imitation optimizes \u03c0 \u03b8 for the objective J SI = E s,a\u223cD SI log \u03c0 \u03b8 (a|s) with respect to \u03b8. reward shaping consists of two parts, a valid attack reward and a navigation reward based on geodesic distance obtained from the privileged LIDAR sensor, similar to \"Hunt Cow\".\nMob Combat: fight 4 different types of hostile monsters: Spider, Zombie, Zombie Pigman (a creature in the Nether world), and Enderman (a creature in the End world). The prompt template is \"Combat {monster}\". For all tasks within this group, we initialize the agent with a diamond sword, a shield, and a full suite of diamond armors. The agent is spawned in the Nether for Zombie Pigman task, and in the End for Enderman. The manual dense-shaping reward can be expressed as r t = \u03bb attack 1(valid attack) + \u03bb success 1({monster} hunted) where \u03bb attack = 5 and \u03bb success = 200.\nCreative: 4 tasks that do not have manual dense reward shaping or code-defined success criterion.\n\u2022 Find Nether Portal: find and move close to a Nether Portal, then enter the Nether world through the portal. The prompt is find a nether portal.\n\u2022 Find Ocean: find and move close to an ocean. The prompt is find an ocean.\n\u2022 Dig Hole: dig holes in the ground. The prompt is dig a hole. We initialize the agent with an iron shovel.\n\u2022 Lay Carpet: lay down carpets to cover the wooden floor inside a house. The prompt is put carpets on the floor. We initialize the agent with a number of carpets in its inventory.\nNote that we categorize \"Find Nether Portal\" and \"Find Ocean\" as Creative tasks even though they seem similar to object navigation [12]. While finding terrains and other structures is semantically well defined, it is not easy to define a function to evaluate success automatically because the simulator does not have the exact location information of these structures given a randomly generated world.\nIn principle, we can make a sweep by querying each chunk of voxels in the world to recognize the terrains, but that would be prohibitively expensive. Therefore, we opt to use MineCLIP as the reward signal and treat these tasks as Creative.", "publication_ref": ["b101", "b100", "b83", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "G.2 Observation and Action Space", "text": "We use a subset of the full observation and action space listed in Sec. B.1 and B.2, because the tasks in our current experiments do not involve actions like crafting or inventory management. Our observation space consists of RGB frame, compass, GPS, and Voxels.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Imitating interactive intelligence", "journal": "", "year": "2020", "authors": "Josh Abramson; Arun Ahuja; Iain Barr; Arthur Brussee; Federico Carnevale; Mary Cassin; Rachita Chhaparia; Stephen Clark; Bogdan Damoc; Andrew Dudzik; Petko Georgiev; Aurelia Guy; Tim Harley; Felix Hill"}, {"ref_id": "b1", "title": "Youtube-8m: A large-scale video classification benchmark", "journal": "", "year": "2016", "authors": "Sami Abu-El-Haija; Nisarg Kothari; Joonseok Lee; Paul Natsev; George Toderici; Balakrishnan Varadarajan; Sudheendra Vijayanarasimhan"}, {"ref_id": "b2", "title": "Do as i can", "journal": "", "year": "", "authors": "Michael Ahn; Anthony Brohan; Noah Brown; Yevgen Chebotar; Omar Cortes; Byron David; Chelsea Finn; Keerthana Gopalakrishnan; Karol Hausman; Alex Herzog; Daniel Ho; Jasmine Hsu; Julian Ibarz; Brian Ichter; Alex Irpan; Eric Jang; Rosario Jauregui Ruano; Kyle Jeffrey; Sally Jesmonth; J Nikhil; Ryan Joshi; Dmitry Julian; Yuheng Kalashnikov; Kuang-Huei Kuang; Sergey Lee; Yao Levine; Linda Lu; Carolina Luu; Peter Parada; Jornell Pastor; Kanishka Quiambao; Jarek Rao; Diego Rettinghouse; Pierre Reyes;  Sermanet"}, {"ref_id": "b3", "title": "Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text", "journal": "", "year": "2021", "authors": "Hassan Akbari; Liangzhe Yuan; Rui Qian; Wei-Hong Chuang; Shih-Fu Chang; Yin Cui; Boqing Gong"}, {"ref_id": "b4", "title": "Conversational contextual cues: The case of personalization and history for response ranking", "journal": "", "year": "2016", "authors": "Rami Al-Rfou; Marc Pickett; Javier Snaider; Brian Yun Hsuan Sung; Ray Strope;  Kurzweil"}, {"ref_id": "b5", "title": "Selfsupervised multimodal versatile networks", "journal": "", "year": "2020", "authors": "Adri\u00e0 Jean-Baptiste Alayrac; Rosalia Recasens; Relja Schneider; Jason Arandjelovic; Jeffrey De Ramapuram; Lucas Fauw; Sander Smaira; Andrew Dieleman;  Zisserman"}, {"ref_id": "b6", "title": "Noise estimation using density estimation for self-supervised multimodal learning", "journal": "AAAI Press", "year": "2021", "authors": "Elad Amrani; Rami Ben-Ari; Daniel Rotman; Alex M Bronstein"}, {"ref_id": "b7", "title": "Modular multitask reinforcement learning with policy sketches", "journal": "", "year": "2017-06-11", "authors": "Jacob Andreas; Dan Klein; Sergey Levine"}, {"ref_id": "b8", "title": "Docformer: End-to-end transformer for document understanding", "journal": "", "year": "2021", "authors": "Srikar Appalaraju; Bhavan Jasani; Yusheng Bhargava Urala Kota; R Xie;  Manmatha"}, {"ref_id": "b9", "title": "Video pretraining (vpt): Learning to act by watching unlabeled online videos", "journal": "", "year": "2022", "authors": "Bowen Baker; Ilge Akkaya; Peter Zhokhov; Joost Huizinga; Jie Tang; Adrien Ecoffet; Brandon Houghton; Raul Sampedro; Jeff Clune"}, {"ref_id": "b10", "title": "Rearrangement: A challenge for embodied ai", "journal": "", "year": "2020", "authors": "Dhruv Batra; Angel X Chang; Sonia Chernova; Andrew J Davison; Jia Deng; Vladlen Koltun; Sergey Levine; Jitendra Malik; Igor Mordatch; Roozbeh Mottaghi; Manolis Savva; Hao Su"}, {"ref_id": "b11", "title": "Objectnav revisited: On evaluation of embodied agents navigating to objects", "journal": "", "year": "2020", "authors": "Dhruv Batra; Aaron Gokaslan; Aniruddha Kembhavi; Oleksandr Maksymets; Roozbeh Mottaghi; Manolis Savva; Alexander Toshev; Erik Wijmans"}, {"ref_id": "b12", "title": "On the opportunities and risks of foundation models", "journal": "", "year": "2021", "authors": "Rishi Bommasani; Drew A Hudson; Ehsan Adeli; Russ Altman; Simran Arora; Michael S Sydney Von Arx; Jeannette Bernstein; Antoine Bohg; Emma Bosselut; Erik Brunskill; Shyamal Brynjolfsson; Dallas Buch; Rodrigo Card; Niladri Castellon; Annie Chatterji; Kathleen Chen; Jared Quincy Creel; Dora Davis; Chris Demszky; Moussa Donahue; Esin Doumbouya; Stefano Durmus; John Ermon; Kawin Etchemendy; Li Ethayarajh; Chelsea Fei-Fei; Trevor Finn; Lauren Gale; Karan Gillespie; Noah Goel; Shelby Goodman; Neel Grossman; Tatsunori Guha; Peter Hashimoto; John Henderson; Daniel E Hewitt; Jenny Ho; Kyle Hong; Jing Hsu; Thomas Huang; Saahil Icard; Dan Jain; Pratyusha Jurafsky; Siddharth Kalluri; Geoff Karamcheti; Fereshte Keeling; Omar Khani; Pang Wei Khattab; Mark Koh; Ranjay Krass; Rohith Krishna; Ananya Kuditipudi; Faisal Kumar; Mina Ladhak; Tony Lee; Jure Lee; Isabelle Leskovec;  Levent; Lisa Xiang; Xuechen Li; Tengyu Li; Ali Ma; Christopher D Malik; Suvir Manning; Eric Mirchandani; Zanele Mitchell; Suraj Munyikwa; Avanika Nair; Deepak Narayan; Alex Narayanan ; Krishnan Srinivasan; Rohan Tamkin; Armin W Taori; Florian Thomas; Rose E Tram\u00e8r; William Wang; Bohan Wang; Jiajun Wu; Yuhuai Wu; Sang Michael Wu; Michihiro Xie; Jiaxuan Yasunaga; Matei You; Michael Zaharia; Tianyi Zhang; Xikun Zhang; Yuhui Zhang; Lucia Zhang; Kaitlyn Zheng; Percy Zhou;  Liang"}, {"ref_id": "b13", "title": "", "journal": "", "year": "2016", "authors": "Greg Brockman; Vicki Cheung; Ludwig Pettersson; Jonas Schneider; John Schulman; Jie Tang; Wojciech Zaremba"}, {"ref_id": "b14", "title": "Language models are few-shot learners", "journal": "", "year": "2020", "authors": "Tom Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared D Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell"}, {"ref_id": "b15", "title": "Towards grounded-language learning beyond memorization", "journal": "", "year": "2020", "authors": "Tianshi Cao; Jingkang Wang; Yining Zhang; Sivabalan Manivasagam;  Babyai++"}, {"ref_id": "b16", "title": "Quo vadis, action recognition? A new model and the kinetics dataset", "journal": "IEEE Computer Society", "year": "2017-07-21", "authors": "Jo\u00e3o Carreira; Andrew Zisserman"}, {"ref_id": "b17", "title": "Learning generalizable robotic reward functions from in-the-wild human videos", "journal": "", "year": "2021", "authors": "Annie S Chen; Suraj Nair; Chelsea Finn"}, {"ref_id": "b18", "title": "Decision transformer: Reinforcement learning via sequence modeling", "journal": "", "year": "2021-12-06", "authors": "Lili Chen; Kevin Lu; Aravind Rajeswaran; Kimin Lee; Aditya Grover; Michael Laskin; Pieter Abbeel; Aravind Srinivas; Igor Mordatch"}, {"ref_id": "b19", "title": "A simple framework for contrastive learning of visual representations", "journal": "PMLR", "year": "2020-07", "authors": "Ting Chen; Simon Kornblith; Mohammad Norouzi; Geoffrey E Hinton"}, {"ref_id": "b20", "title": "Babyai: A platform to study the sample efficiency of grounded language learning", "journal": "", "year": "2019", "authors": "Maxime Chevalier-Boisvert; Dzmitry Bahdanau; Salem Lahlou; Lucas Willems; Chitwan Saharia; Thien Huu Nguyen; Yoshua Bengio"}, {"ref_id": "b21", "title": "", "journal": "", "year": "", "authors": "Aakanksha Chowdhery; Sharan Narang; Jacob Devlin; Maarten Bosma; Gaurav Mishra; Adam Roberts; Paul Barham;  Hyung Won; Charles Chung; Sebastian Sutton; Parker Gehrmann; Kensen Schuh; Sasha Shi; Joshua Tsvyashchenko; Abhishek Maynez; Parker Rao; Yi Barnes; Noam Tay; Vinodkumar Shazeer; Emily Prabhakaran; Nan Reif; Ben Du; Reiner Hutchinson; James Pope; Jacob Bradbury; Michael Austin; Guy Isard; Pengcheng Gur-Ari; Toju Yin; Anselm Duke; Sanjay Levskaya; Sunipa Ghemawat; Henryk Dev; Xavier Michalewski; Vedant Garcia; Kevin Misra; Liam Robinson; Denny Fedus; Daphne Zhou; David Ippolito; Hyeontaek Luan;  Lim"}, {"ref_id": "b22", "title": "A review of physics simulators for robotic applications", "journal": "IEEE Access", "year": "2021", "authors": "Jack Collins; Shelvin Chand; Anthony Vanderkop; David Howard"}, {"ref_id": "b23", "title": "", "journal": "Common Crawl. Common crawl", "year": "2012", "authors": ""}, {"ref_id": "b24", "title": "Robonet: Large-scale multi-robot learning", "journal": "PMLR", "year": "2019-10-30", "authors": "Sudeep Dasari; Frederik Ebert; Stephen Tian; Suraj Nair; Bernadette Bucher; Karl Schmeckpeper; Siddharth Singh; Sergey Levine; Chelsea Finn"}, {"ref_id": "b25", "title": "Emergent complexity and zero-shot transfer via unsupervised environment design", "journal": "", "year": "2020", "authors": "Michael Dennis; Natasha Jaques; Eugene Vinitsky; Alexandre M Bayen; Stuart Russell; Andrew Critch; Sergey Levine"}, {"ref_id": "b26", "title": "Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2018", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova Bert"}, {"ref_id": "b27", "title": "An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint", "journal": "", "year": "2020", "authors": "Alexey Dosovitskiy; Lucas Beyer; Alexander Kolesnikov; Dirk Weissenborn; Xiaohua Zhai; Thomas Unterthiner; Mostafa Dehghani; Matthias Minderer; Georg Heigold; Sylvain Gelly; Jakob Uszkoreit; Neil Houlsby"}, {"ref_id": "b28", "title": "A survey of embodied AI: from simulators to research tasks", "journal": "IEEE Trans. Emerg. Top. Comput. Intell", "year": "2022", "authors": "Jiafei Duan; Samson Yu; Hui Li Tan; Hongyuan Zhu; Cheston Tan"}, {"ref_id": "b29", "title": "Go-explore: a new approach for hard-exploration problems", "journal": "", "year": "2019", "authors": "Adrien Ecoffet; Joost Huizinga; Joel Lehman; Kenneth O Stanley; Jeff Clune"}, {"ref_id": "b30", "title": "Imitating latent policies from observation", "journal": "", "year": "2018", "authors": "Ashley D Edwards; Himanshu Sahni; Yannick Schroecker; Charles L Isbell"}, {"ref_id": "b31", "title": "", "journal": "", "year": "", "authors": ""}, {"ref_id": "b32", "title": "Rubiksnet: Learnable 3d-shift for efficient video action recognition", "journal": "", "year": "", "authors": "* Linxi Fan; Shyamal Buch; * ; Guanzhi Wang; Ryan Cao; Yuke Zhu; Juan Carlos Niebles; Li Fei-Fei"}, {"ref_id": "b33", "title": "Secant: Self-expert cloning for zero-shot generalization of visual policies", "journal": "", "year": "2021", "authors": "Linxi Fan; Guanzhi Wang; De-An Huang; Zhiding Yu; Li Fei-Fei; Yuke Zhu; Anima Anandkumar"}, {"ref_id": "b34", "title": "D4rl: Datasets for deep data-driven reinforcement learning", "journal": "", "year": "2020", "authors": "Justin Fu; Aviral Kumar; Ofir Nachum; George Tucker; Sergey Levine"}, {"ref_id": "b35", "title": "Superhuman performance in gran turismo sport using deep reinforcement learning", "journal": "IEEE Robotics Autom. Lett", "year": "2021", "authors": "Florian Fuchs; Yunlong Song; Elia Kaufmann; Davide Scaramuzza; Peter D\u00fcrr"}, {"ref_id": "b36", "title": "The pile: An 800gb dataset of diverse text for language modeling", "journal": "", "year": "2020", "authors": "Leo Gao; Stella Biderman; Sid Black; Laurence Golding; Travis Hoppe; Charles Foster; Jason Phang; Horace He; Anish Thite; Noa Nabeshima"}, {"ref_id": "b37", "title": "Clip-adapter: Better vision-language models with feature adapters", "journal": "", "year": "2021", "authors": "Peng Gao; Shijie Geng; Renrui Zhang; Teli Ma; Rongyao Fang; Yongfeng Zhang; Hongsheng Li; Yu Qiao"}, {"ref_id": "b38", "title": "Datasheets for datasets", "journal": "Commun. ACM", "year": "2021", "authors": "Timnit Gebru; Jamie Morgenstern; Briana Vecchione; Jennifer Wortman Vaughan; Hanna M Wallach; Hal Daum\u00e9; Iii ; Kate Crawford"}, {"ref_id": "b39", "title": "Realtoxicityprompts: Evaluating neural toxic degeneration in language models", "journal": "", "year": "2020", "authors": "Suchin Samuel Gehman; Maarten Gururangan; Yejin Sap; Noah A Choi;  Smith"}, {"ref_id": "b40", "title": "Minecraft, the most-watched game on youtube, passes 1 trillion views", "journal": "", "year": "2021-12", "authors": "Jordan Gerblick"}, {"ref_id": "b41", "title": "The\" something something\" video database for learning and evaluating visual common sense", "journal": "", "year": "2017", "authors": "Raghav Goyal; Samira Ebrahimi Kahou; Vincent Michalski; Joanna Materzynska; Susanne Westphal; Heuna Kim; Valentin Haenel; Ingo Fruend; Peter Yianilos; Moritz Mueller-Freitag"}, {"ref_id": "b42", "title": "", "journal": "", "year": "", "authors": "Kristen Grauman; Andrew Westbury; Eugene Byrne; Zachary Chavis; Antonino Furnari; Rohit Girdhar; Jackson Hamburger; Hao Jiang; Miao Liu; Xingyu Liu; Miguel Martin; Tushar Nagarajan; Ilija Radosavovic; Santhosh Kumar Ramakrishnan; Fiona Ryan; Jayant Sharma; Michael Wray; Mengmeng Xu; Eric Zhongcong Xu; Chen Zhao; Siddhant Bansal; Dhruv Batra; Vincent Cartillier; Sean Crane; Tien Do; Morrie Doulaty; Akshay Erapalli; Christoph Feichtenhofer; Adriano Fragomeni; Qichen Fu; Abrham Gebreselasie; Cristina Gonzalez; James Hillis; Xuhua Huang; Yifei Huang; Wenqi Jia; Weslie Khoo; Jachym Kolar; Satwik Kottur; Anurag Kumar; Federico Landini; Chao Li; Yanghao Li; Zhenqiang Li; Karttikeya Mangalam; Raghava Modhugu; Jonathan Munro; Tullie Murrell; Takumi Nishiyasu; Will Price; Paola Ruiz Puentes; Merey Ramazanova; Leda Sari; Kiran Somasundaram; Audrey Southerland; Yusuke Sugano; Ruijie Tao; Minh Vo; Yuchen Wang; Xindi Wu; Takuma Yagi; Ziwei Zhao; Yunyi Zhu; ; James; M Rehg; Yoichi Sato; Jianbo Shi; Mike Zheng Shou; Antonio Torralba; Lorenzo Torresani; Mingfei Yan; Jitendra Malik"}, {"ref_id": "b43", "title": "Craftassist: A framework for dialogueenabled interactive agents", "journal": "", "year": "2019", "authors": "Jonathan Gray; Kavya Srinet; Yacine Jernite; Haonan Yu; Zhuoyuan Chen; Demi Guo; Siddharth Goyal; C Lawrence Zitnick; Arthur Szlam"}, {"ref_id": "b44", "title": "EvoCraft: A New Challenge for Open-Endedness", "journal": "Springer International Publishing", "year": "", "authors": "Djordje Grbic; Rasmus Berg Palm; Elias Najarro; Claire Glanois; Sebastian Risi"}, {"ref_id": "b45", "title": "Learning universal controllers with transformers", "journal": "", "year": "2022", "authors": "Agrim Gupta; Linxi Fan; Surya Ganguli; Li Fei-Fei;  Metamorph"}, {"ref_id": "b46", "title": "The minerl 2019 competition on sample efficient reinforcement learning using human priors", "journal": "", "year": "2019", "authors": "H William; Cayden Guss; Katja Codel; Brandon Hofmann; Noboru Houghton; Stephanie Kuno; Sharada Milani; Diego Perez Mohanty; Ruslan Liebana; Nicholay Salakhutdinov; Manuela Topin; Phillip Veloso;  Wang"}, {"ref_id": "b47", "title": "Minerl: A large-scale dataset of minecraft demonstrations", "journal": "", "year": "2019", "authors": "H William; Brandon Guss; Nicholay Houghton; Phillip Topin; Cayden Wang; Manuela Codel; Ruslan Veloso;  Salakhutdinov"}, {"ref_id": "b48", "title": "Shinya Shiroshita, Nicholay Topin, Avinash Ummadisingu, and Oriol Vinyals. The minerl 2020 competition on sample efficient reinforcement learning using human priors", "journal": "", "year": "2021", "authors": "H William; Mario Ynocente Guss; Sam Castro; Brandon Devlin;  Houghton; Crissman Noboru Sean Kuno; Stephanie Loomis; Sharada Milani; Keisuke Mohanty; Ruslan Nakata; John Salakhutdinov;  Schulman"}, {"ref_id": "b49", "title": "Benchmarking the spectrum of agent capabilities", "journal": "", "year": "2021", "authors": "Danijar Hafner"}, {"ref_id": "b50", "title": "", "journal": "", "year": "", "authors": "Laura Hanu;  Unitary Team;  Detoxify;  Github"}, {"ref_id": "b51", "title": "Momentum contrast for unsupervised visual representation learning", "journal": "", "year": "2020", "authors": "Kaiming He; Haoqi Fan; Yuxin Wu; Saining Xie; Ross B Girshick"}, {"ref_id": "b52", "title": "Masked autoencoders are scalable vision learners", "journal": "", "year": "2021", "authors": "Kaiming He; Xinlei Chen; Saining Xie; Yanghao Li; Piotr Doll\u00e1r; Ross Girshick"}, {"ref_id": "b53", "title": "A repository of conversational datasets", "journal": "", "year": "2019", "authors": "Matthew Henderson; Pawe\u0142 Budzianowski; I\u00f1igo Casanueva; Sam Coope; Daniela Gerz; Girish Kumar; Nikola Mrk\u0161i\u0107; Georgios Spithourakis; Pei-Hao Su; Ivan Vuli\u0107; Tsung-Hsien Wen"}, {"ref_id": "b54", "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium", "journal": "", "year": "2017-12-04", "authors": "Martin Heusel; Hubert Ramsauer; Thomas Unterthiner; Bernhard Nessler; Sepp Hochreiter"}, {"ref_id": "b55", "title": "Inner monologue: Embodied reasoning through planning with language models", "journal": "", "year": "2022", "authors": "Wenlong Huang; Fei Xia; Ted Xiao; Harris Chan; Jacky Liang; Pete Florence; Andy Zeng; Jonathan Tompson; Igor Mordatch; Yevgen Chebotar; Pierre Sermanet; Noah Brown; Tomas Jackson; Linda Luu; Sergey Levine; Karol Hausman; Brian Ichter"}, {"ref_id": "b56", "title": "Evolving multimodal robot behavior via many stepping stones with the combinatorial multiobjective evolutionary algorithm", "journal": "Evolutionary computation", "year": "2022", "authors": "Joost Huizinga; Jeff Clune"}, {"ref_id": "b57", "title": "Offline reinforcement learning as one big sequence modeling problem", "journal": "", "year": "2021-12-06", "authors": "Michael Janner; Qiyang Li; Sergey Levine"}, {"ref_id": "b58", "title": "General robot manipulation with multimodal prompts", "journal": "", "year": "2022", "authors": "Yunfan Jiang; Agrim Gupta; Zichen Zhang; Guanzhi Wang; Yongqiang Dou; Yanjun Chen; Li Fei-Fei; Anima Anandkumar; Yuke Zhu; Linxi Fan;  Vima"}, {"ref_id": "b59", "title": "The malmo platform for artificial intelligence experimentation", "journal": "IJCAI", "year": "2016", "authors": "Matthew Johnson; Katja Hofmann; Tim Hutton; David Bignell"}, {"ref_id": "b60", "title": "Obstacle tower: A generalization challenge in vision, control, and planning", "journal": "", "year": "2019", "authors": "Arthur Juliani; Ahmed Khalifa; Vincent-Pierre Berges; Jonathan Harper; Ervin Teng; Hunter Henry; Adam Crespi; Julian Togelius; Danny Lange"}, {"ref_id": "b61", "title": "Fran\u00e7ois Fleuret, Alexander Nikulin, Yury Belousov, Oleg Svidchenko, and Aleksei Shpilman. Minerl diamond 2021 competition: Overview, results, and lessons learned", "journal": "", "year": "2022", "authors": "Anssi Kanervisto; Stephanie Milani; Karolis Ramanauskas; Nicholay Topin; Zichuan Lin; Junyou Li; Jianing Shi; Deheng Ye; Qiang Fu; Wei Yang; Weijun Hong; Zhongyue Huang; Haicheng Chen; Guangjun Zeng; Yue Lin; Vincent Micheli; Eloi Alonso"}, {"ref_id": "b62", "title": "Multi-task curriculum learning in a complex, visual, hard-exploration domain: Minecraft. arXiv preprint", "journal": "", "year": "2021", "authors": "Ingmar Kanitscheider; Joost Huizinga; David Farhi; William Hebgen Guss; Brandon Houghton; Raul Sampedro; Peter Zhokhov; Bowen Baker; Adrien Ecoffet; Jie Tang; Oleg Klimov; Jeff Clune"}, {"ref_id": "b63", "title": "The kinetics human action video dataset", "journal": "", "year": "2017", "authors": "Will Kay; Joao Carreira; Karen Simonyan; Brian Zhang; Chloe Hillier; Sudheendra Vijayanarasimhan; Fabio Viola; Tim Green; Trevor Back; Paul Natsev; Mustafa Suleyman; Andrew Zisserman"}, {"ref_id": "b64", "title": "Abstractive summarization of reddit posts with multi-level memory networks", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Byeongchang Kim; Hyunwoo Kim; Gunhee Kim"}, {"ref_id": "b65", "title": "Michel Galley, and Ahmed Awadallah. Neurips 2021 competition iglu: Interactive grounded language understanding in a collaborative environment", "journal": "", "year": "2021", "authors": "Julia Kiseleva; Ziming Li; Mohammad Aliannejadi; Shrestha Mohanty; Maartje Ter Hoeve; Mikhail Burtsev; Alexey Skrynnik; Artem Zholus; Aleksandr Panov; Kavya Srinet; Arthur Szlam; Yuxuan Sun; Katja Hofmann"}, {"ref_id": "b66", "title": "Large language models are zero-shot reasoners", "journal": "", "year": "2022", "authors": "Takeshi Kojima; Shane Shixiang; Machel Gu; Yutaka Reid; Yusuke Matsuo;  Iwasawa"}, {"ref_id": "b67", "title": "Ai2-thor: An interactive 3d environment for visual ai", "journal": "", "year": "2017", "authors": "Eric Kolve; Roozbeh Mottaghi; Winson Han; Eli Vanderbilt; Luca Weihs; Alvaro Herrasti; Daniel Gordon; Yuke Zhu; Abhinav Gupta; Ali Farhadi"}, {"ref_id": "b68", "title": "The nethack learning environment", "journal": "Curran Associates, Inc", "year": "2020", "authors": "Heinrich K\u00fcttler; Nantas Nardelli; Alexander Miller; Roberta Raileanu; Marco Selvatici; Edward Grefenstette; Tim Rockt\u00e4schel"}, {"ref_id": "b69", "title": "", "journal": "Label Studio. Label studio", "year": "", "authors": ""}, {"ref_id": "b70", "title": "Pfeiffer-a distributed open-ended evolutionary system", "journal": "Citeseer", "year": "2005", "authors": " Wb Langdon"}, {"ref_id": "b71", "title": "Pre-trained language models for interactive decision-making", "journal": "", "year": "2022", "authors": "Shuang Li; Xavier Puig; Chris Paxton; Yilun Du; Clinton Wang; Linxi Fan; Tao Chen; De-An Huang"}, {"ref_id": "b72", "title": "SGDR: stochastic gradient descent with warm restarts", "journal": "", "year": "2017-04-24", "authors": "Ilya Loshchilov; Frank Hutter"}, {"ref_id": "b73", "title": "Unified-io: A unified model for vision, language, and multi-modal tasks", "journal": "", "year": "2022", "authors": "Jiasen Lu; Christopher Clark; Rowan Zellers; Roozbeh Mottaghi; Aniruddha Kembhavi"}, {"ref_id": "b74", "title": "Clip4clip: An empirical study of clip for end to end video clip retrieval", "journal": "", "year": "2021", "authors": "Huaishao Luo; Lei Ji; Ming Zhong; Yang Chen; Wen Lei; Nan Duan; Tianrui Li"}, {"ref_id": "b75", "title": "Mixed precision training", "journal": "", "year": "2018-04-30", "authors": "Paulius Micikevicius; Sharan Narang; Jonah Alben; Gregory F Diamos; Erich Elsen; David Garc\u00eda; Boris Ginsburg; Michael Houston; Oleksii Kuchaiev; Ganesh Venkatesh; Hao Wu"}, {"ref_id": "b76", "title": "Howto100m: Learning a text-video embedding by watching hundred million narrated video clips", "journal": "", "year": "2019", "authors": "Antoine Miech; Dimitri Zhukov; Jean-Baptiste Alayrac; Makarand Tapaswi; Ivan Laptev; Josef Sivic"}, {"ref_id": "b77", "title": "End-to-end learning of visual representations from uncurated instructional videos", "journal": "", "year": "2020", "authors": "Antoine Miech; Jean-Baptiste Alayrac; Lucas Smaira; Ivan Laptev; Josef Sivic; Andrew Zisserman"}, {"ref_id": "b78", "title": "", "journal": "", "year": "2016", "authors": "Minecraft Wiki;  Minecraft"}, {"ref_id": "b79", "title": "Playing atari with deep reinforcement learning", "journal": "", "year": "2013", "authors": "Volodymyr Mnih; Koray Kavukcuoglu; David Silver; Alex Graves; Ioannis Antonoglou; Daan Wierstra; Martin Riedmiller"}, {"ref_id": "b80", "title": "Learning language-conditioned robot behavior from offline data and crowd-sourced annotation", "journal": "", "year": "2021-11-11", "authors": "Suraj Nair; Eric Mitchell; Kevin Chen; Brian Ichter; Silvio Savarese; Chelsea Finn"}, {"ref_id": "b81", "title": "R3m: A universal visual representation for robot manipulation", "journal": "", "year": "2022", "authors": "Suraj Nair; Aravind Rajeswaran; Vikash Kumar; Chelsea Finn; Abhinav Gupta"}, {"ref_id": "b82", "title": "The primacy bias in deep reinforcement learning", "journal": "", "year": "2022", "authors": "Evgenii Nikishin; Max Schwarzer; D' Pierluca; Pierre-Luc Oro; Aaron Bacon;  Courville"}, {"ref_id": "b83", "title": "Self-imitation learning", "journal": "PMLR", "year": "2018", "authors": "Junhyuk Oh; Yijie Guo; Satinder Singh; Honglak Lee"}, {"ref_id": "b84", "title": "Ilya Sutskever, Jie Tang, Filip Wolski, and Susan Zhang. Dota 2 with large scale deep reinforcement learning", "journal": "", "year": "2019", "authors": ": Openai; Christopher Berner; Greg Brockman; Brooke Chan; Vicki Cheung; Przemys\u0142aw D\u0119biak; Christy Dennison; David Farhi; Quirin Fischer; Shariq Hashme; Chris Hesse; Rafal J\u00f3zefowicz; Scott Gray; Catherine Olsson; Jakub Pachocki; Michael Petrov; P D O Henrique; Jonathan Pinto; Tim Raiman; Jeremy Salimans; Jonas Schlatter; Szymon Schneider;  Sidor"}, {"ref_id": "b85", "title": "Pytorch: An imperative style, high-performance deep learning library", "journal": "", "year": "2019", "authors": "Adam Paszke; Sam Gross; Francisco Massa; Adam Lerer; James Bradbury; Gregory Chanan; Trevor Killeen; Zeming Lin; Natalia Gimelshein; Luca Antiga; Alban Desmaison; Andreas K\u00f6pf; Edward Yang; Zach Devito; Martin Raison; Alykhan Tejani; Sasank Chilamkurthy; Benoit Steiner; Lu Fang; Junjie Bai; Soumith Chintala"}, {"ref_id": "b86", "title": "The multi-agent reinforcement learning in malm\u00d6 (marl\u00d6) competition", "journal": "", "year": "2019", "authors": "Diego Perez-Liebana; Katja Hofmann; Noburu Sharada Prasanna Mohanty; Andre Kuno; Sam Kramer; Raluca D Devlin; Daniel Gaina;  Ionita"}, {"ref_id": "b87", "title": "Praw: The python reddit api wrapper", "journal": "", "year": "2010", "authors": ""}, {"ref_id": "b88", "title": "Virtualhome: Simulating household activities via programs", "journal": "", "year": "2018-06-18", "authors": "Xavier Puig; Kevin Ra; Marko Boben; Jiaman Li; Tingwu Wang; Sanja Fidler; Antonio Torralba"}, {"ref_id": "b89", "title": "Improving language understanding by generative pre-training", "journal": "OpenAI", "year": "2018", "authors": "Alec Radford; Karthik Narasimhan; Tim Salimans; Ilya Sutskever"}, {"ref_id": "b90", "title": "Language models are unsupervised multitask learners", "journal": "OpenAI blog", "year": "2019", "authors": "Alec Radford; Jeffrey Wu; Rewon Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"ref_id": "b91", "title": "Learning transferable visual models from natural language supervision", "journal": "PMLR", "year": "2021", "authors": "Alec Radford; Jong Wook Kim; Chris Hallacy; Aditya Ramesh; Gabriel Goh; Sandhini Agarwal; Girish Sastry; Amanda Askell; Pamela Mishkin; Jack Clark"}, {"ref_id": "b92", "title": "Hierarchical text-conditional image generation with clip latents", "journal": "", "year": "2022", "authors": "Aditya Ramesh; Prafulla Dhariwal; Alex Nichol; Casey Chu; Mark Chen"}, {"ref_id": "b93", "title": "Recent advances in robot learning from demonstration. Annual review of control, robotics, and autonomous systems", "journal": "", "year": "2020", "authors": "Harish Ravichandar; S Athanasios; Sonia Polydoros; Aude Chernova;  Billard"}, {"ref_id": "b94", "title": "Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent", "journal": "", "year": "2022", "authors": "Scott Reed; Konrad Zolna; Emilio Parisotto; Sergio Gomez Colmenarejo; Alexander Novikov; Gabriel Barth-Maron; Mai Gimenez; Yury Sulsky; Jackie Kay; Jost Tobias Springenberg"}, {"ref_id": "b95", "title": "Can wikipedia help offline reinforcement learning? arXiv preprint", "journal": "", "year": "2022", "authors": "Machel Reid; Yutaro Yamada; Shixiang Shane Gu"}, {"ref_id": "b96", "title": "Photorealistic textto-image diffusion models with deep language understanding", "journal": "", "year": "2022", "authors": "Chitwan Saharia; William Chan; Saurabh Saxena; Lala Li; Jay Whang; Emily Denton; Seyed Kamyar Seyed;  Ghasemipour; S Sara Burcu Karagol Ayan; Rapha Gontijo Mahdavi; Tim Lopes; Jonathan Salimans;  Ho; J David; Mohammad Fleet;  Norouzi"}, {"ref_id": "b97", "title": "Improved techniques for training gans", "journal": "", "year": "2016-12-05", "authors": "Tim Salimans; Ian J Goodfellow; Wojciech Zaremba; Vicki Cheung; Alec Radford; Xi Chen"}, {"ref_id": "b98", "title": "Habitat: A platform for embodied ai research", "journal": "", "year": "2019-10", "authors": "Manolis Savva; Abhishek Kadian; Oleksandr Maksymets; Yili Zhao; Erik Wijmans; Bhavana Jain; Julian Straub; Jia Liu; Vladlen Koltun; Jitendra Malik; Devi Parikh; Dhruv Batra"}, {"ref_id": "b99", "title": "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs", "journal": "", "year": "2021", "authors": "Christoph Schuhmann; Richard Vencu; Romain Beaumont; Robert Kaczmarczyk; Clayton Mullis; Aarush Katta; Theo Coombes; Jenia Jitsev; Aran Komatsuzaki"}, {"ref_id": "b100", "title": "Highdimensional continuous control using generalized advantage estimation", "journal": "", "year": "2016", "authors": "John Schulman; Philipp Moritz; Sergey Levine; Michael I Jordan; Pieter Abbeel"}, {"ref_id": "b101", "title": "Proximal policy optimization algorithms", "journal": "", "year": "2017", "authors": "John Schulman; Filip Wolski; Prafulla Dhariwal; Alec Radford; Oleg Klimov"}, {"ref_id": "b102", "title": "", "journal": "", "year": "2011", "authors": "Selenium Webdriver;  Selenium"}, {"ref_id": "b103", "title": "The minerl basalt competition on learning from human feedback", "journal": "", "year": "2021", "authors": "Rohin Shah; Cody Wild; Steven H Wang; Neel Alex; Brandon Houghton; William Guss; Sharada Mohanty; Anssi Kanervisto; Stephanie Milani; Nicholay Topin; Pieter Abbeel; Stuart Russell; Anca Dragan"}, {"ref_id": "b104", "title": "Concept2robot: Learning manipulation concepts from instructions and human demonstrations. The International", "journal": "Journal of Robotics Research", "year": "2021", "authors": "Lin Shao; Toki Migimatsu; Qiang Zhang; Karen Yang; Jeannette Bohg"}, {"ref_id": "b105", "title": "Glu variants improve transformer", "journal": "", "year": "2020", "authors": "Noam Shazeer"}, {"ref_id": "b106", "title": "igibson 1.0: a simulation environment for interactive tasks in large realistic scenes", "journal": "", "year": "2020", "authors": "Bokui Shen; Fei Xia; Chengshu Li; Roberto Mart\u00edn-Mart\u00edn; Linxi Fan; Guanzhi Wang; Claudia P\u00e9rez-D'arpino; Shyamal Buch; Sanjana Srivastava; Lyne P Tchapmi; Micael E Tchapmi; Kent Vainio; Josiah Wong; Li Fei-Fei; Silvio Savarese"}, {"ref_id": "b107", "title": "World of bits: an open-domain platform for web-based agents", "journal": "", "year": "", "authors": "Tim Tianlin; Andrej Shi;  Karpathy; Jim Linxi; Jonathan Fan; Percy Hernandez;  Liang"}, {"ref_id": "b108", "title": "Megatron-lm: Training multi-billion parameter language models using model parallelism", "journal": "", "year": "2019", "authors": "Mohammad Shoeybi; Mostofa Patwary; Raul Puri; Patrick Legresley; Jared Casper; Bryan Catanzaro"}, {"ref_id": "b109", "title": "ALFRED: A benchmark for interpreting grounded instructions for everyday tasks", "journal": "", "year": "2020", "authors": "Mohit Shridhar; Jesse Thomason; Daniel Gordon; Yonatan Bisk; Winson Han; Roozbeh Mottaghi; Luke Zettlemoyer; Dieter Fox"}, {"ref_id": "b110", "title": "Cliport: What and where pathways for robotic manipulation", "journal": "", "year": "2021", "authors": "Mohit Shridhar; Lucas Manuelli; Dieter Fox"}, {"ref_id": "b111", "title": "Mastering chess and shogi by self-play with a general reinforcement learning algorithm", "journal": "", "year": "2017", "authors": "David Silver; Thomas Hubert; Julian Schrittwieser; Ioannis Antonoglou; Matthew Lai; Arthur Guez; Marc Lanctot; Laurent Sifre; Dharshan Kumaran; Thore Graepel; Timothy Lillicrap; Karen Simonyan; Demis Hassabis"}, {"ref_id": "b112", "title": "Avid: Learning multi-stage tasks via pixel-level translation of human videos", "journal": "", "year": "2019", "authors": "Laura Smith; Nikita Dhawan; Marvin Zhang; Pieter Abbeel; Sergey Levine"}, {"ref_id": "b113", "title": "BEHAVIOR: benchmark for everyday household activities in virtual, interactive, and ecological environments", "journal": "", "year": "2021-11-11", "authors": "Sanjana Srivastava; Chengshu Li; Michael Lingelbach; Roberto Mart\u00edn-Mart\u00edn; Fei Xia; Kent Elliott Vainio; Zheng Lian; Cem Gokmen; C Karen Buch; Silvio Liu; Hyowon Savarese; Jiajun Gweon; Li Wu;  Fei-Fei"}, {"ref_id": "b114", "title": "Third-person imitation learning", "journal": "", "year": "2017", "authors": "Bradly C Stadie; Pieter Abbeel; Ilya Sutskever"}, {"ref_id": "b115", "title": "Open-ended artificial evolution", "journal": "International Journal of Computational Intelligence and Applications", "year": "2003", "authors": "K Russell;  Standish"}, {"ref_id": "b116", "title": "Open-endedness: The last grand challenge you've never heard of. O'Reilly Online", "journal": "", "year": "2017", "authors": "O Kenneth; Joel Stanley; Lisa Lehman;  Soros"}, {"ref_id": "b117", "title": "Learning video representations using contrastive bidirectional transformer", "journal": "", "year": "2019", "authors": "Chen Sun; Fabien Baradel; Kevin Murphy; Cordelia Schmid"}, {"ref_id": "b118", "title": "", "journal": "", "year": "2018", "authors": "Yuval Tassa; Yotam Doron; Alistair Muldal; Tom Erez; Yazhe Li; Diego De Las; David Casas; Abbas Budden; Josh Abdolmaleki; Andrew Merel; Timothy Lefrancq; Martin Lillicrap;  Riedmiller"}, {"ref_id": "b119", "title": "Open-ended evolution: Perspectives from the oee workshop in york", "journal": "Artificial life", "year": "2016", "authors": "Tim Taylor; Mark Bedau; Alastair Channon; David Ackley; Wolfgang Banzhaf; Guillaume Beslon; Emily Dolson; Tom Froese; Simon Hickinbotham; Takashi Ikegami"}, {"ref_id": "b120", "title": "Steph Hughes-Fitt, Valentin Dalibard, and Wojciech Marian Czarnecki. Open-ended learning leads to generally capable agents", "journal": "", "year": "2021", "authors": "Adam Stooke; Anuj Mahajan; Catarina Barros; Charlie Deck; Jakob Bauer; Jakub Sygnowski; Maja Trebacz; Max Jaderberg; Michael Mathieu; Nat Mcaleese; Nathalie Bradley-Schmieg; Nathaniel Wong"}, {"ref_id": "b121", "title": "Behavioral cloning from observation", "journal": "", "year": "2018", "authors": "Faraz Torabi; Garrett Warnell; Peter Stone"}, {"ref_id": "b122", "title": "Recent advances in imitation learning from observation", "journal": "", "year": "2019", "authors": "Faraz Torabi; Garrett Warnell; Peter Stone"}, {"ref_id": "b123", "title": "Shibl Mourad, and Doina Precup. Androidenv: A reinforcement learning platform for android", "journal": "", "year": "2021", "authors": "Daniel Toyama; Philippe Hamel; Anita Gergely; Gheorghe Comanici; Amelia Glaese; Zafarali Ahmed; Tyler Jackson"}, {"ref_id": "b124", "title": "Representation learning with contrastive predictive coding", "journal": "", "year": "2018", "authors": "Aaron Van Den Oord; Yazhe Li; Oriol Vinyals"}, {"ref_id": "b125", "title": "Mastering the real-time strategy game starcraft ii. DeepMind blog", "journal": "", "year": "2019", "authors": "Oriol Vinyals; Igor Babuschkin; Junyoung Chung; Michael Mathieu; Max Jaderberg; Wojciech M Czarnecki; Andrew Dudzik; Aja Huang; Petko Georgiev; Richard Powell"}, {"ref_id": "b126", "title": "Mining Reddit to learn automatic summarization", "journal": "Association for Computational Linguistics", "year": "2017-09", "authors": "Michael V\u00f8lske; Martin Potthast; Shahbaz Syed; Benno Stein;  Tl;Dr"}, {"ref_id": "b127", "title": "", "journal": "", "year": "2022", "authors": "Phil Wang"}, {"ref_id": "b128", "title": "Paired open-ended trailblazer (poet): Endlessly generating increasingly complex and diverse learning environments and their solutions", "journal": "", "year": "2019", "authors": "Rui Wang; Joel Lehman; Jeff Clune; Kenneth O Stanley"}, {"ref_id": "b129", "title": "Enhanced poet: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions", "journal": "", "year": "2020", "authors": "Rui Wang; Joel Lehman; Aditya Rawal; Jiale Zhi; Yulun Li; Jeff Clune; Kenneth O Stanley"}, {"ref_id": "b130", "title": "Wikipedia contributors. Minecraft -Wikipedia, the free encyclopedia", "journal": "", "year": "2022", "authors": ""}, {"ref_id": "b131", "title": "Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface's transformers: State-of-the-art natural language processing", "journal": "", "year": "2019", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; R\u00e9mi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao;  Gugger"}, {"ref_id": "b132", "title": "Understanding and improving information transfer in multi-task learning", "journal": "", "year": "2020", "authors": "Sen Wu; R Hongyang; Christopher Zhang;  R\u00e9"}, {"ref_id": "b133", "title": "A benchmark for interactive navigation in cluttered environments", "journal": "", "year": "2019", "authors": "Fei Xia; William B Shen; Chengshu Li; Priya Kasimbeg; Micael Tchapmi; Alexander Toshev; Li Fei-Fei; Roberto Mart\u00edn-Mart\u00edn; Silvio Savarese"}, {"ref_id": "b134", "title": "Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification", "journal": "", "year": "2018", "authors": "Saining Xie; Chen Sun; Jonathan Huang; Zhuowen Tu; Kevin Murphy"}, {"ref_id": "b135", "title": "Videoclip: Contrastive pre-training for zero-shot video-text understanding", "journal": "", "year": "2021", "authors": "Hu Xu; Gargi Ghosh; Po-Yao Huang; Dmytro Okhonko; Armen Aghajanyan; Florian Metze; Luke Zettlemoyer; Christoph Feichtenhofer"}, {"ref_id": "b136", "title": "Multimodal pre-training for visually-rich document understanding", "journal": "", "year": "2020", "authors": "Yang Xu; Yiheng Xu; Tengchao Lv; Lei Cui; Furu Wei; Guoxin Wang; Yijuan Lu; Dinei Florencio; Cha Zhang; Wanxiang Che; Min Zhang; Lidong Zhou"}, {"ref_id": "b137", "title": "Layoutlm: Pre-training of text and layout for document image understanding", "journal": "", "year": "2019", "authors": "Yiheng Xu; Minghao Li; Lei Cui; Shaohan Huang; Furu Wei; Ming Zhou"}, {"ref_id": "b138", "title": "Learning semantic textual similarity from conversations", "journal": "Association for Computational Linguistics", "year": "2018-07", "authors": "Yinfei Yang; Steve Yuan; Daniel Cer; Sheng-Yi Kong; Noah Constant; Petr Pilar; Heming Ge; Yun-Hsuan Sung; Brian Strope; Ray Kurzweil"}, {"ref_id": "b139", "title": "", "journal": "", "year": "2012", "authors": "Youtube Data; Api "}, {"ref_id": "b140", "title": "Gradient surgery for multi-task learning", "journal": "", "year": "2020", "authors": "Tianhe Yu; Saurabh Kumar; Abhishek Gupta; Sergey Levine; Karol Hausman; Chelsea Finn"}, {"ref_id": "b141", "title": "Xirl: Cross-embodiment inverse reinforcement learning", "journal": "", "year": "2021", "authors": "Kevin Zakka; Andy Zeng; Pete Florence; Jonathan Tompson; Jeannette Bohg; Debidatta Dwibedi"}, {"ref_id": "b142", "title": "Socratic models: Composing zero-shot multimodal reasoning with language", "journal": "", "year": "2022", "authors": "Andy Zeng; Adrian Wong; Stefan Welker; Krzysztof Choromanski; Federico Tombari; Aveek Purohit; Michael Ryoo; Vikas Sindhwani; Johnny Lee; Vincent Vanhoucke; Pete Florence"}, {"ref_id": "b143", "title": "", "journal": "", "year": "2022", "authors": "Qinqing Zheng; Amy Zhang; Aditya Grover"}, {"ref_id": "b144", "title": "Actbert: Learning global-local video-text representations. arXiv preprint arXiv", "journal": "", "year": "2011", "authors": "Linchao Zhu; Yi Yang"}, {"ref_id": "b145", "title": "robosuite: A modular simulation framework and benchmark for robot learning", "journal": "", "year": "2020", "authors": "Yuke Zhu; Josiah Wong; Ajay Mandlekar; Roberto Mart\u00edn-Mart\u00edn"}, {"ref_id": "b146", "title": "All datasets can be downloaded from https://zenodo.org/. Please refer to this table of URL, DOI, and licensing: Database DOI License YouTube", "journal": "", "year": "", "authors": ""}, {"ref_id": "b147", "title": "Have any third parties imposed IP-based or other restrictions on the data associated with the instances?", "journal": "", "year": "", "authors": ""}, {"ref_id": "b148", "title": "Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? No. dataset? The authors will be supporting, hosting, and maintaining the dataset", "journal": "", "year": "", "authors": ""}, {"ref_id": "b149", "title": "Please contact Linxi Fan (linxif@nvidia.com), Guanzhi Wang (guanzhi@caltech.edu), and Yunfan Jiang", "journal": "", "year": "", "authors": ""}, {"ref_id": "b150", "title": "Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? Yes. New updates will be posted on", "journal": "", "year": "", "authors": ""}, {"ref_id": "b151", "title": "Will older versions of the dataset continue to be supported/hosted/maintained? Yes, old versions will be permanently accessible on zenodo", "journal": "", "year": "", "authors": ""}, {"ref_id": "b152", "title": "If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? Yes", "journal": "", "year": "", "authors": ""}, {"ref_id": "b153", "title": "Composition What do the instances that comprise the dataset represent? For YouTube videos, our data is in JSON format with video URLs and metadata. We do not provide the raw MP4 files for legal concerns. For Wiki, we provide the text, images, tables, and diagrams embedded on the web pages. For Reddit, our data is in JSON format with post IDs and metadata, similar to YouTube. Users can reconstruct the Reddit dataset by running our script after obtaining an official", "journal": "", "year": "", "authors": ""}, {"ref_id": "b154", "title": "How many instances are there in total (of each type, if appropriate)? There are more than 730K YouTube videos with 2.2B words of transcripts, 6,735 Wiki pages with 2.2M bounding boxes of visual elements, and more than 340K Reddit posts with 6", "journal": "", "year": "", "authors": ""}, {"ref_id": "b155", "title": "Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? We provide all instances in our Zenodo data repositories. Is there a label or target associated with each instance?", "journal": "", "year": "", "authors": ""}, {"ref_id": "b156", "title": "Are relationships between individual instances made explicit (e.g., users' movie ratings, social network links)? We provide metadata for each YouTube video link and", "journal": "", "year": "", "authors": ""}, {"ref_id": "b157", "title": "Are there recommended data splits (e.g., training, development/validation, testing)? No. The entire database is intended for pre-training", "journal": "", "year": "", "authors": ""}, {"ref_id": "b158", "title": "Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)? We follow prior works [64] and only release the video URLs of YouTube videos due to legal concerns. Researchers need to acquire the MP4 and transcript files separately. Similarly, we only release the post IDs for the Minecraft Reddit database", "journal": "", "year": "", "authors": ""}, {"ref_id": "b159", "title": "Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? We have made our best efforts to detoxify the contents via an automated procedure", "journal": "", "year": "", "authors": ""}, {"ref_id": "b160", "title": "Collection Process The collection procedure, preprocessing, and cleaning are explained in details in Sec. D. Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)? All data collection, curation", "journal": "", "year": "", "authors": ""}, {"ref_id": "b161", "title": "Over what timeframe was the data collected? The data was collected between", "journal": "", "year": "2021-12", "authors": ""}, {"ref_id": "b162", "title": "Uses Has the dataset been used for any tasks already? Yes, we have used the MINEDOJO YouTube database for agent pre-training. Please refer to Sec. 5 and Sec. G for algorithmic and training details", "journal": "", "year": "", "authors": ""}, {"ref_id": "b163", "title": "What (other) tasks could the dataset be used for? Our knowledge base is primarily intended to facilitate research in open-ended, generally capable embodied agents. However, it can also be broadly applicable to research in video understanding, document understanding, language modeling, multimodal learning", "journal": "", "year": "", "authors": ""}, {"ref_id": "b164", "title": "Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses", "journal": "", "year": "", "authors": ""}, {"ref_id": "b165", "title": "Are there tasks for which the dataset should not be used? We strongly oppose any research that intentionally generates harmful or toxic contents using our YouTube, Wiki, and Reddit data", "journal": "", "year": "", "authors": ""}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Visualization of our agent's learned behaviors on four selected tasks. Leftmost texts are the task prompts used in training. Best viewed on a color display.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 4 :4Figure 4: Algorithm design. MINECLIP is a contrastive video-language model pre-trained on MINEDOJO's massive Youtube database. It computes the correlation between an open-vocabulary language goal string and a 16-frame video snippet. The correlation score can be used as a learned dense reward function to train a strong multi-task RL agent.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "4Agent Learning with Large-scale Pre-training One of the grand challenges of embodied AI is to build a single agent that can complete a wide range of open-world tasks. The MINEDOJO framework aims to facilitate new techniques towards this goal by providing an open-ended task suite (Sec. 2) and large-scale internet knowledge base (Sec. 3).", "figure_data": ""}, {"figure_label": "64537", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Milk Cow 64 . 5 \u00b1 3764537Lay Carpet 97.6 \u00b1 1.9 98.8 \u00b1 1.0 N/A N/A 0.0 \u00b1 0.0 ways to use MINEDOJO's internet database -the Wiki and Reddit corpus also hold great potential to drive new algorithm discoveries for the community in future works.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Fig. A.8).", "figure_data": ""}, {"figure_label": "2346781011121", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "1 s ur vival_sword_food : 2 category : survival 3 prompt : survive as long as possible given a sword and some food 4 5 6 category : harvest 7 prompt : harvest wool from a sheep with shears and a sheep nearby 8 9 10 category : tech -tree 11 prompt : find material and craft a wooden sword 12 13Figure A. 1 :2346781011121Figure A.1: Example specifications.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure A. 2 :2Figure A.2: Labeling UI to mine tasks from YouTube. A human annotator can choose to reject the video (Invalid), adjust the timestamps, select the title, or edit and expand the original description to be the new task goal.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure A. 3 :3Figure A.3: Distribution of YouTube video duration. The histogram is trimmed by the 85th percentile to hide much longer videos that can run for many hours.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "10.5281/zenodo.6641142 Creative Commons Attribution 4.0 International (CC BY 4.0) Wiki 10.5281/zenodo.6640448 Creative Commons Attribution Non Commercial Share Alike 3.0 Unported Reddit 10.5281/zenodo.6641114 Creative Commons Attribution 4.0 International (CC BY 4.0)", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure A. 4 :4Figure A.4: Wiki dataset examples. Closewise order: Villager trade table, mineral ingredient descriptions, monster gallery, and terrain explanation.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Fig. A.4 and Fig. A.5", "figure_data": ""}, {"figure_label": "56", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure A. 5 :Figure A. 6 :56Figure A.5: More Wiki database examples with bounding boxes (annotated in red). Left: wood block introduction; right: first day tutorial.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": ". The r/Minecraft subreddit contains free-form discussions of game strategies and images/videos showcases of Minecraft builds and creations (examples in Fig. A.7). The distribution of post types is shown in Fig. A.6. To scrape the Reddit contents, we use PRAW [88], a Python wrapper on top of the official Reddit API. Our procedure is as follows: a) Obtain the ID and metadata (e.g. post title, number of comments, content, score) of every post in the \"r/Minecraft\" subreddit since it was created. For quality control, we only consider posts with scores (upvotes) \u2265 5 and not marked as NSFW. b) Determine each post's type. There are 4 native post types -text, image/video, link, and poll. We group text and poll posts together as text posts, and store their body text. For image/video and link posts, we store the source file URLs on external media hosting sites like Imgur and Gfycat. Based on the URL of each link post, we classify it as an image post, a video post or a general link post.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Figure A. 7 :7Figure A.7: Examples of posts and comment threads from the Reddit database.", "figure_data": ""}, {"figure_label": "345", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "3 ) 4 ) 5 )345For each matched transcript segment, randomly grow it to 16 \u223c 77 tokens (limited by CLIP's context length); Randomly sample a timestamp within the start and end time of the matched transcript as the center for the video clip; Randomly grow the video clip from the center timestamp to 8 \u223c 16 seconds.", "figure_data": ""}, {"figure_label": "679111213141", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "forall D SI,T do 6 if \u03c4 T is successful then 7 D 9 D 11 Increase counter accordingly; 12 Update \u03c0 \u03b8 following Equation 2; 13 Fit 14 if 1 (679111213141SI,T \u2190 D SI,T \u222a \u03c4 T 8 else if \u03c4 T 's episode return \u2265 \u00b5 return (D SI,T ) + \u2206 \u00d7 \u03c3 return (D SI,T ) then SI,T \u2190 D SI,T \u222a \u03c4 T 10 end V F (\u2022) by regression on mean-squared error; counter mod \u03c9 = 0) then 15 Determine the number of trajectories to sample from each buffer # sample = min({|D SI,T |, \u2200T \u2208 training tasks}); 16 Sample # sample trajectories from each buffer in a prioritized manner to construct D SI ; 17 Update \u03c0 \u03b8 on D SI with supervised objective; 18 end", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_16", "figure_caption": "Figure A. 8 :8Figure A.8: Adding the self imitation technique [84] significantly improves the performance of RL training in MINEDOJO.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_17", "figure_caption": "Fig. A.8, we demonstrate that adding self-imitation dramatically improves the stability, performance, and sample efficiency of RL training in MINEDOJO.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Our novel MINECLIP reward model is able to achieve competitive performance with manually written dense reward function for Programmatic tasks, and significantly outperforms the CLIP OpenAI method across all Creative tasks. Entries represent percentage success rates averaged over 3 seeds, each tested for 200 episodes. Success conditions are precise in Programmatic tasks, but estimated by MineCLIP for Creative tasks.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": ". MINECLIP is multi-task by design, as it is trained on open-vocabulary and diverse English transcripts.During RL training, MINECLIP provides a high-quality reward signal without any domain adaptation techniques, despite the domain gap between noisy YouTube videos and clean simulator-rendered frames. MINECLIP eliminates the need to manually engineer reward functions for each and every MINEDOJO task. For Creative tasks that lack a simple success criterion (Sec. 2.2), MINECLIP also serves the dual purpose of an automatic evaluation metric that agrees well with human judgement on a subset of tasks we investigate (Sec. 4.2, Table2). Because the learned reward model incurs a non-trivial computational overhead, we introduce several techniques to significantly improve RL training efficiency, making MINECLIP a practical module for open-ended agent learning in Minecraft (Sec. 4.2).", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "MINECLIP agrees well with the ground-truth human judgment on the Creative tasks we consider. Numbers are F1 scores between MINECLIP's binary classification of tasks success and human labels (scaled to the percentage for better readability).From the MINEDOJO YouTube database, we follow the procedure in VideoCLIP[136] to sample 640K pairs of 16-second video snippets and time-aligned English transcripts, after applying a keyword filter. We train two MINECLIP variants with different types of aggregator \u03c6 a : (1) MINECLIP[avg] does simple average pooling, which is fast but agnostic to the temporal ordering; (2) MINECLIP[attn] encodes the sequence by two transformer layers, which is relatively slower but captures more temporal information, and thus produces a better reward signal in general. Details of data preprocessing, architecture, and hyperparameters are listed in the Appendix (Sec. E).", "figure_data": "Tasks Find Nether Portal Find Ocean Dig Hole Lay CarpetOurs (Attn)98.7100.099.497.4Ours (Avg)100.0100.0100.098.4CLIP OpenAI48.798.480.654.1"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "MINECLIP agents have stronger zero-shot visual generalization ability to unseen terrains, weathers, and lighting. Numbers outside parentheses are percentage success rates averaged over 3 seeds (each tested for 200 episodes), while those inside parentheses are relative performance changes.Tasks Ours (Attn), train Ours (Attn), unseen test CLIP OpenAI , train CLIP OpenAI , unseen test", "figure_data": "Milk Cow64.5 \u00b1 37.164.8 \u00b1 31.3(+ 0.8%)90.0 \u00b1 0.429.2 \u00b1 3.7 (\u221267.6%)Hunt Cow83.5 \u00b1 7.155.9 \u00b1 7.2 (\u221232.9%)72.7 \u00b1 3.516.7 \u00b1 1.6 (\u221277.0%)Combat Spider80.5 \u00b1 13.062.1 \u00b1 29.7(\u221222.9%)79.5 \u00b1 2.554.2 \u00b1 9.6 (\u221231.8%)Combat Zombie47.3 \u00b1 10.639.9 \u00b1 25.3(\u221215.4%)50.2 \u00b1 7.530.8 \u00b1 14.4(\u221238.6%)"}, {"figure_label": "45", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "We train a single multi-task agent for all 12 tasks. All numbers represent percentage success rates averaged over 3 seeds, each tested for 200 episodes.", "figure_data": "GroupTasks Single Agent on All TasksOriginalPerformance ChangeMilk Cow91.5 \u00b1 0.764.5 \u00b1 37.1\u2191Hunt Cow46.8 \u00b1 3.783.5 \u00b1 7.1\u2193Shear Sheep73.5 \u00b1 0.812.1 \u00b1 9.1\u2191Hunt Sheep27.0 \u00b1 1.08.1 \u00b1 4.1\u2191Combat Spider72.1 \u00b1 1.380.5 \u00b1 13.0\u2193Combat Zombie27.1 \u00b1 2.747.3 \u00b1 10.6\u2193Combat Pigman6.5 \u00b1 1.21.6 \u00b1 2.3\u2191Combat Enderman0.0 \u00b1 0.00.0 \u00b1 0.0=Find Nether Portal99.1 \u00b1 0.437.4 \u00b1 40.8\u2191Find Ocean95.1 \u00b1 1.533.4 \u00b1 45.6\u2191Dig Hole85.8 \u00b1 1.291.6 \u00b1 5.9\u2193Lay Carpet96.5 \u00b1 0.897.6 \u00b1 1.9="}, {"figure_label": "A", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "1: Comparison table of different Minecraft platforms for AI research.", "figure_data": "EnvironmentSimulatorTask SuiteKnowledge BaseRealNumberLanguage-FeaturesMinecraftof TasksgroundedFeaturesData ScaleMINEDOJOUnified observation and action3, 000+Automatically scraped from740K YouTube videos;space;the Internet;7K Wiki pages;unlocks all three types ofmultimodal data (videos, im-350K Reddit postsworld (the Overworld, theages, texts, tables and dia-Nether, and the End)grams)MineRLBuilt on top of Malmo;11Annotated state-action pairs of60M frames of recorded hu-v0.4 [48]actively maintainedhuman demonstrationsman player dataMineRLMouse and keyboard control5Labeled contractor data;2K hours of contractor data;v1.0unlabeled videos scraped from270K hours of unlabeled(VPT) [10]the InternetvideosMarL\u00d6 [87]Cooperative and competitive14multiagent tasks;parameterizable environmentsMalmo [60]First comprehensive release ofN/Aa Gym-style agent API forMinecraftCraftAssist [44] Bot assistant;N/AInteractive dialogues;800K dialogue-action dictio-dialogue interactionscrowd-sourced house buildingnary pairs;dataset2.6K houses with atomicbuilding actionsIGLU [66]Interactive dialogues with hu-157mans;aimed at building structuresdescribed by natural languageEvoCraft [45] Aimed at generating creativeN/Aartifacts;allows for direction manipula-tion of blocksCrafter [50]2D clone of Minecraft;22Human experts dataset100 episodesfast experimentation"}, {"figure_label": "A", "figure_type": "table", "figure_id": "tab_8", "figure_caption": ".2: Training hyperparameters for MINECLIP.", "figure_data": "Hyperparameter ValueLR schedule Cosine with warmup [73]Warmup steps 500Peak LR 1.5e-4Final LR 1e-5Weight decay 0.2Layerwise LR decay 0.65Pre-trained layers LR multiplier 0.5\u00d7Batch size per GPU 64Parallel GPUs 8Video resolution 160 \u00d7 256Number of frames 16Image encoder ViT-B/16 [28]"}, {"figure_label": "A", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "3: Ablation on different MINECLIP reward formulations.", "figure_data": ""}, {"figure_label": "A", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "4: Hyperparameters in RL experiments. \"{state} MLP\" refers to MLPs to process observations of compass, GPS, and voxel blocks. \"Embed Dim\" denotes the same dimension size used to embed all discrete observations into dense vectors.", "figure_data": ""}, {"figure_label": "A", "figure_type": "table", "figure_id": "tab_11", "figure_caption": ".5: MINECLIP's evaluation on more complex Creative tasks. Numbers represent F1 scores between MINECLIP's evaluation on tasks success and human labels. Scaled to percentage for better readability. Build a Farm Build a Fence Build a House Ride a Minecart Build a Swimming Pool", "figure_data": "Tasks Ours (Attn)78.791.463.795.985.0Ours (Avg)73.483.137.496.994.7CLIP OpenAI62.524.552.970.071.7"}], "formulas": [{"formula_id": "formula_0", "formula_text": ".1.", "formula_coordinates": [11.0, 361.33, 368.56, 12.87, 8.64]}, {"formula_id": "formula_1", "formula_text": ".2.", "formula_coordinates": [33.0, 414.87, 426.62, 12.87, 8.64]}, {"formula_id": "formula_2", "formula_text": ".3.", "formula_coordinates": [33.0, 396.16, 556.27, 12.87, 8.64]}, {"formula_id": "formula_3", "formula_text": "L smooth = 1 |W| |W|\u22121 i=1 KL(\u03c0 t \u03c0 t\u2212|W|+i ),(3)", "formula_coordinates": [37.0, 223.63, 361.82, 280.37, 31.57]}], "doi": "10.1109/CVPR.2017.502"}