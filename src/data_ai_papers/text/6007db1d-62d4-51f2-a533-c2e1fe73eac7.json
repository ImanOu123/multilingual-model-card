{"title": "Cross-lingual Word Clusters for Direct Transfer of Linguistic Structure", "authors": "Oscar T\u00e4ckstr\u00f6m; Ryan Mcdonald; Jakob Uszkoreit", "pub_date": "", "abstract": "It has been established that incorporating word cluster features derived from large unlabeled corpora can significantly improve prediction of linguistic structure. While previous work has focused primarily on English, we extend these results to other languages along two dimensions. First, we show that these results hold true for a number of languages across families. Second, and more interestingly, we provide an algorithm for inducing cross-lingual clusters and we show that features derived from these clusters significantly improve the accuracy of cross-lingual structure prediction. Specifically, we show that by augmenting direct-transfer systems with cross-lingual cluster features, the relative error of delexicalized dependency parsers, trained on English treebanks and transferred to foreign languages, can be reduced by up to 13%. When applying the same method to direct transfer of named-entity recognizers, we observe relative improvements of up to 26%.", "sections": [{"heading": "Introduction", "text": "The ability to predict the linguistic structure of sentences or documents is central to the field of natural language processing (NLP). Structures such as named-entity tag sequences (Bikel et al., 1999) or sentiment relations (Pang and Lee, 2008) are inherently useful in data mining, information retrieval and other user-facing technologies. More fundamental structures such as part-of-speech tag sequences (Ratnaparkhi, 1996) or syntactic parse trees (Collins, 1997;K\u00fcbler et al., 2009), on the other hand, comprise the core linguistic analysis for many important downstream tasks such as machine translation (Chiang, 2005;Collins et al., 2005). Currently, supervised data-driven methods dominate the literature on linguistic structure prediction (Smith, 2011). Regrettably, the majority of studies on these methods have focused on evaluations specific to English, since it is the language with the most annotated resources. Notable exceptions include the CoNLL shared tasks (Tjong Kim Sang, 2002;Tjong Kim Sang and De Meulder, 2003;Buchholz and Marsi, 2006;Nivre et al., 2007) and subsequent studies on this data, as well as a number of focused studies on one or two specific languages, as discussed by Bender (2011).\nWhile annotated resources for parsing and several other tasks are available in a number of languages, we cannot expect to have access to labeled resources for all tasks in all languages. This fact has given rise to a large body of research on unsupervised (Klein and Manning, 2004), semi-supervised (Koo et al., 2008) and transfer (Hwa et al., 2005) systems for prediction of linguistic structure. These methods all attempt to benefit from the plethora of unlabeled monolingual and/or cross-lingual data that has become available in the digital age. Unsupervised methods are appealing in that they are often inherently language independent. This is borne out by the many recent studies on unsupervised parsing that include evaluations covering a number of languages (Cohen and Smith, 2009;Gillenwater et al., 2010;Naseem et al., 2010;Spitkovsky et al., 2011). However, the performance for most languages is still well below that of supervised systems and recent work has established that the performance is also below simple methods of linguistic transfer .\nIn this study we focus on semi-supervised and linguistic-transfer methods for multilingual structure prediction. In particular, we pursue two lines of research around the use of word cluster features in discriminative models for structure prediction:\n1. Monolingual word cluster features induced from large corpora of text for semi-supervised learning (SSL) of linguistic structure. Previous studies on this approach have typically focused only on a small set of languages and tasks (Freitag, 2004;Miller et al., 2004;Koo et al., 2008;Turian et al., 2010;Faruqui and Pad\u00f3, 2010;Haffari et al., 2011;Tratz and Hovy, 2011). Here we show that this method is robust across 13 languages for dependency parsing and 4 languages for named-entity recognition (NER). This is the first study with such a broad view on this subject, in terms of language diversity.\n2. Cross-lingual word cluster features for transferring linguistic structure from English to other languages. We develop an algorithm that generates cross-lingual word clusters; that is clusters of words that are consistent across languages. This is achieved by means of a probabilistic model over large amounts of monolingual data in two languages, coupled with parallel data through which cross-lingual word-cluster constraints are enforced. We show that by augmenting the delexicalized direct transfer system of  with cross-lingual cluster features, we are able to reduce its error by up to 13% relative. Further, we show that by applying the same method to direct-transfer NER, we achieve a relative error reduction of 26%.\nBy incorporating cross-lingual cluster features in a linguistic transfer system, we are for the first time combining SSL and cross-lingual transfer.", "publication_ref": ["b4", "b38", "b40", "b14", "b28", "b9", "b13", "b41", "b45", "b44", "b7", "b35", "b2", "b26", "b27", "b25", "b11", "b22", "b34", "b43", "b21", "b32", "b27", "b47", "b20", "b23", "b46"], "figure_ref": [], "table_ref": []}, {"heading": "Monolingual Word Cluster Features", "text": "Word cluster features have been shown to be useful in various tasks in natural language processing, including syntactic dependency parsing (Koo et al., 2008;Haffari et al., 2011;Tratz and Hovy, 2011), syntactic chunking (Turian et al., 2010), and NER (Freitag, 2004;Miller et al., 2004;Turian et al., 2010;Faruqui and Pad\u00f3, 2010). Intuitively, the reason for the effectiveness of cluster features lie in their ability to aggregate local distributional information from large unlabeled corpora, which aid in conquering data sparsity in supervised training regimes as well as in mitigating cross-domain generalization issues.\nIn line with much previous work on word clusters for tasks such as dependency parsing and NER, for which local syntactic and semantic constraints are of importance, we induce word clusters by means of a probabilistic class-based language model (Brown et al., 1992;Clark, 2003). However, rather than the more commonly used model of Brown et al. (1992), we use the predictive class bigram model introduced by Uszkoreit and Brants (2008). The two models are very similar, but whereas the former takes classto-class transitions into account, the latter directly models word-to-class transitions. By ignoring classto-class transitions, an approximate maximum likelihood clustering can be found efficiently with the distributed exchange algorithm (Uszkoreit and Brants, 2008). This is a useful property, as we later develop an algorithm for inducing cross-lingual word clusters that calls this monolingual algorithm as a subroutine. More formally, let C : V \u2192 1, . . . , K be a (hard) clustering function that maps each word type from the vocabulary, V, to one of K cluster identities. With the model of Uszkoreit and Brants (2008), the likelihood of a sequence of word tokens, w = w i m i=1 , with w i \u2208 V \u222a {S}, where S is a designated start-ofsegment symbol, factors as\nL(w; C) = m i=1 p(w i |C(w i ))p(C(w i )|w i\u22121 ) . (1)\nCompare this to the model of Brown et al. (1992):\nL (w; C) = m i=1 p(w i |C(w i ))p(C(w i )|C(w i\u22121 )) .\nWhile the use of class-to-class transitions can lead to more compact models, which is often useful for conquering data sparsity, when clustering large data sets we can get reliable statistics directly on the wordto-class transitions (Uszkoreit and Brants, 2008).\nIn addition to the clustering model that we make use of in this study, a number of additional word clustering and embedding variants have been proposed. For example, Turian et al. (2010) assessed the effectiveness of the word embedding techniques of Collobert and Weston (2008) and Mnih and Hinton (2007)   embedding method based on canonical correlation analysis that provides state-of-the art results for wordbased SSL for English NER. As an alternative to clustering words, Lin and Wu (2009) proposed a phrase clustering approach that obtained the state-of-the-art result for English NER.", "publication_ref": ["b27", "b23", "b46", "b47", "b21", "b32", "b47", "b20", "b6", "b10", "b6", "b48", "b48", "b48", "b6", "b48", "b47", "b15", "b33", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Monolingual Cluster Experiments", "text": "Before moving on to the multilingual setting, we conduct a set of monolingual experiments where we evaluate the use of the monolingual word clusters just described as features for dependency parsing and NER. In the parsing experiments, we study the following thirteen languages: 1 Danish (DA), German (DE), Greek (EL), English (EN), Spanish (ES), French (FR), Italian (IT), Korean (KO), Dutch (NL), Portugese (PT), Russian (RU), Swedish (SV) and Chinese (ZH) -representing the Chinese, Germanic, Hellenic, Romance, Slavic, Altaic and Korean genera. In the NER experiments, we study three Germanic languages: German (DE), English (EN) and Dutch (NL); and one Romance language: Spanish (ES). Details of the labeled and unlabeled data sets used are given in Appendix A. For all experiments we fixed the number of clusters to 256 as this performed well on held-out data. Furthermore, we only clustered the 1 million most frequent word types in each language for both efficiency and sparsity reasons. For\nWord & bias w\u22121,0,1, w\u22121:0, w0:1, w\u22121:1, b Pre-/suffix w :1,:2,:3,:4,:5 \u22121,0,1 , w \u22125:,\u22124:,\u22123:,\u22122:,\u22121: First letter is capitalized. \u2192/f -Transition from previous to current label conjoined with feature f . w :j : j-character prefix of w. w \u2212j: : j-character suffix of w. f i : Feature f at relative position i. f i,j : Union of features at positions i and j. f i:j : Conjoined feature sequence between relative positions i and j (inclusive). b: Bias.\n\u22121,0,1 Orthography Hyp \u22121,0,1 , Cap \u22121,0,1 , Cap \u22121:0 , Cap 0:1 , Cap \u22121:1 PoS p\u22121,0,1, p\u22121:0, p0:1, p\u22121:1, p\u22122:1, p\u22121:2 Cluster c\u22121,0,1, c\u22121:0, c0:1, c\u22121:1, c\u22122:1, c\u22121:2 Transition \u2192/p\u22121,0,1,\u2192/c\u22121,0,1,\u2192/Cap \u22121,0,1 ,\u2192/b\nlanguages in which our unlabeled data did not have at least 1 million types, we considered all types.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Cluster Augmented Feature Models", "text": "All of the parsing experiments reported in this study are based on the transition-based dependency parsing paradigm (Nivre, 2008). For all languages and settings, we use an arc-eager decoding strategy, with a beam of eight hypotheses, and perform ten epochs of the averaged structured perceptron algorithm (Zhang and Clark, 2008). We extend the state-of-the-art feature model recently introduced by Zhang and Nivre (2011) by adding an additional word cluster based feature template for each word based template. Additionally, we add templates where one or more partof-speech feature is replaced with the corresponding cluster feature. The resulting set of additional feature templates are shown in Table 1. The expanded feature model includes all of the feature templates defined by Zhang and Nivre (2011), which we also use as the baseline model, whereas Table 1 only shows our new templates due to space limitations. For all NER experiments, we use a sequential firstorder conditional random field (CRF) with a unit variance Normal prior, trained with L-BFGS until -convergence ( = 0.0001, typically obtained after less than 400 iterations). The feature model used for the NER tagger is shown in Table 2. These are similar to the features used by Turian et al. (2010), with the main difference that we do not use any long range features and that we add templates that conjoin adjacent clusters and adjacent tags as well as templates that conjoin label transitions with tags, clusters and capitalization features.   ", "publication_ref": ["b36", "b51", "b52", "b47"], "figure_ref": [], "table_ref": ["tab_0", "tab_0", "tab_1"]}, {"heading": "Results", "text": "The results of the parsing experiments, measured with labeled accuracy score (LAS) on all sentence lengths, excluding punctuation, are shown in Table 3.\nThe baselines are all comparable to the state-of-theart. On average, the addition of word cluster features yields a 6% relative reduction in error and upwards of 15% (for RU). All languages improve except FR, which sees neither an increase nor a decrease in LAS.\nWe observe an average absolute increase in LAS of approximately 1%, which is inline with previous observations (Koo et al., 2008). It is perhaps not surprising that RU sees a large gain as it is a highly inflected language, making observations of lexical features far more sparse. Some languages, e.g., FR, NL, and ZH see much smaller gains. One likely culprit is a divergence between the tokenization schemes used in the treebank and in our unlabeled data, which for Indo-European languages is closely related to the Penn Treebank tokenization. For example, the NL treebank contains many multi-word tokens that are typically broken apart by our automatic tokenizer. The NER results, in terms of F 1 measure, are listed in Table 4. Introducing word cluster features for NER reduces relative errors on the test set by 21% (39% on the development set) on average. Broken down per language, reductions on the test set vary from substantial for NL (30%) and EN (26%), down to more modest for DE (17%) and ES (12%). The addition of cluster features most markedly improve recognition of the PER category, with an average error reduction on the test set of 44%, while the reductions for ORG (19%), LOC (17%) and MISC (10%) are more modest, but still significant. Although our results are below the best reported results for EN and DE (Lin and Wu, 2009;Faruqui and Pad\u00f3, 2010), the relative improvements of adding word clusters are inline with previous results on NER for EN (Miller et al., 2004;Turian et al., 2010), who report error reductions of approximately 25% from adding word cluster features. Slightly higher reductions where achieved for DE by Faruqui and Pad\u00f3 (2010), who report a reduction of 22%. Note that we did not tune hyper-parameters of the supervised learning methods and of the clustering method, such as the number of clusters (Turian et al., 2010;Faruqui and Pad\u00f3, 2010), and that we did not apply any heuristic for data cleaning such as that used by Turian et al. (2010).", "publication_ref": ["b27", "b29", "b20", "b32", "b47", "b20", "b47", "b20", "b47"], "figure_ref": [], "table_ref": ["tab_3", "tab_4"]}, {"heading": "Cross-lingual Word Cluster Features", "text": "All results of the previous section rely on the availability of large quantities of language specific annotations for each task. Cross-lingual transfer methods and unsupervised methods have recently been shown to hold promise as a way to at least partially sidestep the demand for labeled data. Unsupervised methods attempt to infer linguistic structure without using any annotated data (Klein and Manning, 2004) or possibly by using a set of linguistically motivated rules (Naseem et al., 2010) or a linguistically informed model structure (Berg-Kirkpatrick and Klein, 2010). The aim of transfer methods is instead to use knowledge induced from labeled resources in one or more source languages to construct systems for target languages in which no or few such resources are available (Hwa et al., 2005). Currently, the performance of even the most simple direct transfer systems far exceeds that of unsupervised systems (Cohen et al., 2011;S\u00f8gaard, 2011). ", "publication_ref": ["b26", "b34", "b3", "b25", "b12", "b42"], "figure_ref": [], "table_ref": []}, {"heading": "Direct Transfer of Discriminative Models", "text": "Our starting point is the delexicalized direct transfer method proposed by  based on work by Zeman and Resnik (2008). This method was shown to outperform a number of state-of-the-art unsupervised and transfer-based baselines. The method is simple; for a given training set, the learner ignores all lexical identities and only observes features over other characteristics, e.g., part-of-speech tags, orthographic features, direction of syntactic attachment, etc. The learner builds a model from an annotated source language data set, after which the model is used to directly make target language predictions.\nThere are three basic assumptions that drive this approach. First, that high-level tasks, such as syntactic parsing, can be learned reliably using coarse-grained statistics, such as part-of-speech tags, in place of fine-grained statistics such as lexical word identities. Second, that the parameters of features over coarsegrained statistics are in some sense language inde-pendent, e.g., a feature that indicates that adjectives modify their closest noun is useful in all languages. Third, that these coarse-grained statistics are robustly available across languages. The approach proposed by  relies on these three assumptions. Specifically, by replacing fine-grained language specific part-of-speech tags with universal part-of-speech tags, generated with the method described by , a universal parser is achieved that can be applied to any language for which universal part-of-speech tags are available.\nBelow, we extend this approach to universal parsing by adding cross-lingual word cluster features. A cross-lingual word clustering is a clustering of words in two languages, in which the clusters correspond to some meaningful cross-lingual property. For example, prepositions from both languages should be in the same cluster, proper names from both languages in another cluster and so on. By adding features defined over these clusters, we can, to some degree, re-lexicalize the delexicalized models, while maintaining the \"universality\" of the features. This approach is outlined in Figure 1. Assuming that we have an algorithm for generating cross-lingual word clusters (see Section 4.2), we can augment the delexicalized parsing algorithm to use these word cluster features at training and testing time.\nIn order to further motivate the proposed approach, consider the accuracy of the supervised English parser. A parser with lexical, part-of-speech and cluster features achieves 90.7% LAS (see Table 3). If we remove all lexical and cluster features, the same parser achieves 83.1%. However, if we add back just the cluster features, the accuracy jumps back up to 89.5%, which is only 1.2% below the full system. Thus, if we can accurately learn cross-lingual clusters, there is hope of regaining some of the accuracy lost due to the delexicalization process.", "publication_ref": ["b50"], "figure_ref": ["fig_1"], "table_ref": ["tab_3"]}, {"heading": "Inducing Cross-lingual Word Clusters", "text": "Our first method for inducing cross-lingual clusters has two stages. First, it clusters a source language (S) as in the monolingual case, and then projects these clusters to a target language (T), using word alignments. Given two aligned word sequences w S = w S i m S i=1 and w T = w T i m T j=1 , let A T |S be a set of scored alignments from the source language to the target language, where (w T j , w S a j , s j,a j ) \u2208 A T |S is an alignment from the a j th source word to the jth target word, with score s j,a j \u2265 \u03b4. 2 We use the shorthand j \u2208 A T |S to denote those target words w T j that are aligned to some source word w S a j . Provided a clustering C S , we assign the target word t \u2208 V T to the cluster with which it is most often aligned:\nC T (t) = argmax k j\u2208A T |S s.t. w T j =t s j,a j C S (w S a j ) = k , (2)\nwhere [\u2022] is the indicator function. We refer to the cross-lingual clusters induced in this way as PRO-JECTED CLUSTERS.\nThis simple projection approach has two potential drawbacks. First, it only provides a clustering of those target language words that occur in the word aligned data, which is typically smaller than our monolingual data sets. Second, the mapped clustering may not necessarily correspond to an acceptable target language clustering in terms of monolingual likelihood. In order to tackle these issues, we propose the following more complex model. First, to find clusterings that are good according to both the source and target language, and to make use of more unlabeled data, we model word sequences in each language by the monolingual language model with likelihood function defined by equation (1). Denote these likelihood functions respectively by L S (w S ; C S ) and L T (w T ; C T ), where we have overloaded notation so that the word sequences denoted by w S and w T include much more plentiful non-aligned data when taken as an argument of the monolingual likelihood functions. Second, we couple the clusterings defined by these individual models, by introducing additional factors based on word alignments, as proposed by Och (1999):\nL T |S (w T ; A T |S , C T , C S ) = j\u2208A T |S p(w T j |C T (w T j ))p(C T (w T j )|C S (w S a j )) .\nand the symmetric L S|T (w S ; A S|T , C S , C T ). Note that the simple projection defined by equation (2) correspond to a hard assignment variant of this probabilistic formulation when the source clustering is fixed. Combining all four factors results in the joint monolingual and cross-lingual objective function\nL S,T (w S , w T ; A T |S , A S|T , C S , C T ) = L S (. . .) \u2022 L T (. . .) \u2022 L T |S (. . .) \u2022 L S|T (. . .) . (3\n)\nThe intuition of this approach is that the clusterings C S and C T are forced to jointly explain the source and target data, treating the word alignments as a form of soft constraints. We approximately optimize (3) with the alternating procedure in Algorithm 1, in which we iteratively maximize L S and L T , keeping the other factors fixed. In this way we can generate cross-lingual clusterings using all the monolingual data while forcing the clusterings to obey the word alignment constraints. We refer to the clusters induced with this method as X-LINGUAL CLUSTERS.\nIn practice we found that each unconstrained monolingual run of the exchange algorithm (lines Algorithm 1 Cross-lingual clustering.\nRandomly initialize source/target clusterings C S and C T . for i = 1 . . . N do 1. FindC S \u2248 argmax C S L S (w S ; C S ). ( \u2020) 2. ProjectC S to C T using equation ( 2).\n-keep cluster of non-projected words in C T fixed. 3. FindC T \u2248 argmax C T L T (w T ; C T ). ( \u2020) 4. ProjectC T to C S using equation (2).\n-keep cluster of non-projected words in C S fixed. end for \u2020 Optimized via the exchange algorithm keeping the cluster of projected words fixed and only clustering additional words not in the projection.\n1 and 3) moves the clustering too far from those that obey the word alignment constraints, which causes the procedure to fail to converge. However, we found that fixing the clustering of the words that are assigned clusters in the projection stages (lines 2 and 4) and only clustering the remaining words works well in practice. Furthermore, we found that iterating the procedure has little effect on performance and set N = 1 for all subsequent experiments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Cross-lingual Experiments", "text": "In our first set of experiments on using cross-lingual cluster features, we evaluate direct transfer of our EN parser, trained on Stanford style dependencies (De Marneffe et al., 2006), to the the ten non-EN Indo-European languages listed in Section 3. We exclude KO and ZH as initial experiments proved direct transfer a poor technique when transferring parsers between such diverse languages. We study the impact of using cross-lingual cluster features by comparing the strong delexicalized baseline model of , which only has features derived from universal part-of-speech tags, projected from English with the method of , to the same model when adding features derived from cross-lingual clusters. In both cases the feature models are the same as those used in Section 3.1, except that they are delexicalized by removing all lexical word-identity features. We evaluate both the PRO-JECTED CLUSTERS and the X-LINGUAL CLUSTERS.\nFor these experiments we train the perceptron for only five epochs in order to prevent over-fitting, which is an acute problem due to the divergence between the training and testing data sets in this setting. Furthermore, in accordance to standard practices we only evaluate unlabeled attachment score (UAS) due to the fact that each treebank uses a different -possibly non-overlapping -label set.\nIn our second set of experiments, we evaluate direct transfer of a NER system trained on EN to DE, ES and NL. We use the same feature models as in the monolingual case, with the exception that we use universal part-of-speech tags for all languages and we remove the capitalization feature when transferring from EN to DE. Capitalization is both a prevalent and highly predictive feature of named-entities in EN, while in DE, capitalization is even more prevalent, but has very low predictive power. Interestingly, while delexicalization has shown to be important for direct transfer of dependency-parsers , we noticed in preliminary experiments that it substantially degrades performance for NER. We hypothesize that this is because word features are predictive of common proper names and that these are often translated directly across languages, at least in the case of newswire text. As for the transfer parser, when training the source NER model, we regularize the model more heavily by setting \u03c3 = 0.1.\nAppendix A contains the details of the training, testing, unlabeled and parallel/aligned data sets.", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Table 5 lists the results of the transfer experiments for dependency parsing. The baseline results are comparable to those in  and thus also significantly outperform the results of recent unsupervised approaches (Berg-Kirkpatrick and Klein, 2010;Naseem et al., 2010). Importantly, crosslingual cluster features are helpful across the board and give a relative error reduction ranging from 3% for DA to 13% for PT, with an average reduction of 6%, in terms of unlabeled attachment score (UAS). This shows the utility of cross-lingual cluster features for syntactic transfer. However, X-LINGUAL CLUSTERS provides roughly the same performance as PROJECTED CLUSTERS suggesting that even simple methods of cross-lingual clustering are sufficient for direct transfer dependency parsing.\nWe would like to stress that these results are likely to be under-estimating the parsers' actual ability to predict Stanford-style dependencies in the target languages. This is because the target language annotations that we use for evaluation differ from the  Stanford dependency annotation. Some of these differences are warranted in that certain target language phenomena are better captured by the native annotation. However, differences such as choice of lexical versus functional head are more arbitrary.\nTo highlight this point we run two additional experiments. First, we had linguists, who were also fluent speakers of German, re-annotate the DE test set so that unlabeled arcs are consistent with Stanfordstyle dependencies. Using this data, NO CLUSTERS obtains 60.0% UAS, PROJECTED CLUSTERS 63.6% and X-LINGUAL CLUSTERS 64.4%. When compared to the scores on the original data set (48.9%, 50.3% and 50.7%, respectively) we can see that not only is the baseline system doing much better, but that the improvements from cross-lingual clustering are much more pronounced. Next, we investigated the accuracy of subject and object dependencies, as these are often annotated in similar ways across treebanks, typically modifying the main verb of the sentence. The bottom half of Table 5 gives the scores when restricted to such dependencies in the gold data. We measure the percentage of modifiers in subject and object dependencies that modify the correct word. Indeed, here we see the difference in performance become clearer, with the cross-lingual cluster model reducing errors by 14% relative to the non-cross-lingual model and upwards of 22% relative for IT.\nWe now turn to the results of the transfer experiments for NER, listed in Table 6. While the performance of the transfer systems is very poor when no word clusters are used, adding cross-lingual word clusters give substantial improvements across all languages. The simple PROJECTED CLUSTERS work well, but the X-LINGUAL CLUSTERS provide even larger improvements. On average the latter reduce  errors on the test set by 22% in terms of F 1 and up to 26% for ES. We also measure how well the direct transfer NER systems are able to detect entity boundaries (ignoring the entity categories). Here, on average, the best clusters provide a 24% relative error reduction on the test set (75.8 vs. 68.1 F 1 ). To our knowledge there are no comparable results on transfer learning of NER systems. Based on the results of this first attempt at this scenario, we believe that transfer learning by multilingual word clusters could be developed into a practical way to construct NER systems for resource poor languages.", "publication_ref": ["b3", "b34"], "figure_ref": [], "table_ref": ["tab_6", "tab_6", "tab_8"]}, {"heading": "Conclusion", "text": "In the first part of this study, we showed that word clusters induced from a simple class-based language model can be used to significantly improve on stateof-the-art supervised dependency parsing and NER for a wide range of languages and even across language families. Although the improvements vary between languages, the addition of word cluster features never has a negative impact on performance. This result has important practical consequences as it allows practitioners to simply plug in word cluster features into their current feature models. Given previous work on word clusters for various linguistic structure prediction tasks, these results are not too surprising. However, to our knowledge this is the first study to apply the same type of word cluster features across languages and tasks.\nIn the second part, we provided two simple methods for inducing cross-lingual word clusters. The first method works by projecting word clusters, induced from monolingual data, from a source language to a target language directly via word alignments. The second method, on the other hand, makes use of monolingual data in both the source and the target language, together with word alignments that act as constraints on the joint clustering. We then showed that by using these cross-lingual word clusters, we can significantly improve on direct transfer of discriminative models for both parsing and NER. As in the monolingual case, both types of cross-lingual word cluster features yield improvements across the board, with the more complex method providing a significantly larger improvement for NER. Although the performance of transfer systems is still substantially below that of supervised systems, this research provides one step towards bridging this gap. Further, we believe that it opens up an avenue for future work on multilingual clustering methods, cross-lingual feature projection and domain adaptation for direct transfer of linguistic structure.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We thank John DeNero for help with creating the word alignments; Reut Tsarfaty and Joakim Nivre for rewarding discussions on evaluation; Slav Petrov and Kuzman Ganchev for discussions on cross-lingual clustering; and the anonymous reviewers, along with Joakim Nivre, for valuable comments that helped improve the paper. The first author is grateful for the financial support of the Swedish National Graduate School of Language Technology (GSLT).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Data Sets", "text": "In the parsing experiments, we use the following data sets. For DA, DE, EL, ES, IT, NL, PT and SV, we use the predefined training and evaluation data sets from the CoNLL 2006/2007 data sets (Buchholz and Marsi, 2006;Nivre et al., 2007). For EN we use sections 02-21, 22, and 23 of the Penn WSJ Treebank (Marcus et al., 1993) for training, development and evaluation. For FR we used the French Treebank (Abeill\u00e9 and Barrier, 2004) with splits defined in Candito et al. (2010). For KO we use the Sejong Korean Treebank (Han et al., 2002) randomly splitting the data into 80% training, 10% development and 10% evaluation. For RU we use the SynTagRus Treebank (Boguslavsky et al., 2000;Apresjan et al., 2006) randomly splitting the data into 80% training, 10% development and 10% evaluation. For ZH we use the Penn Chinese Treebank v6 (Xue et al., 2005) using the proposed data splits from the documentation. Both EN and ZH were converted to dependencies using v1.6.8 of the Stanford Converter (De Marneffe et al., 2006). FR was converted using the procedure defined in Candito et al. (2010). RU and KO are native dependency treebanks. For the CoNLL data sets we use the part-of-speech tags provided with the data. For all other data sets, we train a part-of-speech tagger on the training data in order to tag the development and evaluation data.\nFor the NER experiments we use the training, development and evaluation data sets from the CoNLL 2002/2003 shared tasks (Tjong Kim Sang, 2002;Tjong Kim Sang and De Meulder, 2003) for all four languages (DE, EN, ES and NL). The data set for each language consists of newswire text annotated with four entity categories: Location (LOC), Miscellaneous (MISC), Organization (ORG) and Person (PER). We use the part-of-speech tags supplied with the data, except for ES where we instead use universal part-of-speech tags .\nUnlabeled data for training the monolingual cluster models was extracted from one year of newswire articles from multiple sources from a news aggregation website. This consists of 0.8 billion (DA) to 121.6 billion (EN) tokens per language. All word alignments for the cross-lingual clusterings were produced with the dual decomposition aligner described by DeNero and Macherey (2011) using 10.5 million (DA) to 12.1 million (FR) sentences of aligned web data.", "publication_ref": ["b7", "b35", "b30", "b0", "b8", "b24", "b5", "b1", "b49", "b17", "b8", "b45", "b44"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Enriching a french treebank", "journal": "", "year": "2004", "authors": "Anne Abeill\u00e9; Nicolas Barrier"}, {"ref_id": "b1", "title": "A syntactically and semantically tagged corpus of russian: State of the art and prospects", "journal": "", "year": "2006", "authors": "Juri Apresjan; Igor Boguslavsky; Boris Iomdin; Leonid Iomdin; Andrei Sannikov; Victor Sizov"}, {"ref_id": "b2", "title": "On achieving and evaluating language-independence in NLP", "journal": "Linguistic Issues in Language Technology", "year": "2011", "authors": "Emily M Bender"}, {"ref_id": "b3", "title": "Phylogenetic grammar induction", "journal": "", "year": "2010", "authors": "Taylor Berg; - Kirkpatrick; Dan Klein"}, {"ref_id": "b4", "title": "An algorithm that learns what's in a name", "journal": "", "year": "1999", "authors": "M Daniel; Richard Bikel; Ralph M Schwartz;  Weischedel"}, {"ref_id": "b5", "title": "Dependency treebank for Russian: Concept, tools, types of information", "journal": "", "year": "2000", "authors": "Igor Boguslavsky; Svetlana Grigorieva; Nikolai Grigoriev; Leonid Kreidlin; Nadezhda Frid"}, {"ref_id": "b6", "title": "Classbased n-gram models of natural language", "journal": "Computational Linguistics", "year": "1992", "authors": "F Peter; Peter V Brown; Robert L Desouza; Vincent J Mercer; Jenifer C Della Pietra;  Lai"}, {"ref_id": "b7", "title": "CoNLL-X shared task on multilingual dependency parsing", "journal": "", "year": "2006", "authors": "Sabine Buchholz; Erwin Marsi"}, {"ref_id": "b8", "title": "Statistical french dependency parsing: treebank conversion and first results", "journal": "", "year": "2010", "authors": "Marie Candito; Beno\u00eet Crabb\u00e9; Pascal Denis"}, {"ref_id": "b9", "title": "A hierarchical phrase-based model for statistical machine translation", "journal": "", "year": "2005", "authors": "David Chiang"}, {"ref_id": "b10", "title": "Combining distributional and morphological information for part of speech induction", "journal": "", "year": "2003", "authors": "Alexander Clark"}, {"ref_id": "b11", "title": "Shared logistic normal distributions for soft parameter tying in unsupervised grammar induction", "journal": "", "year": "2009", "authors": "B Shay; Noah A Cohen;  Smith"}, {"ref_id": "b12", "title": "Unsupervised structure prediction with non-parallel multilingual guidance", "journal": "", "year": "2011", "authors": "B Shay; Dipanjan Cohen; Noah A Das;  Smith"}, {"ref_id": "b13", "title": "Clause restructuring for statistical machine translation", "journal": "", "year": "2005", "authors": "Michael Collins; Philipp Koehn; Ivona Ku\u010derov\u00e1"}, {"ref_id": "b14", "title": "Three generative, lexicalised models for statistical parsing", "journal": "", "year": "1997", "authors": "Michael Collins"}, {"ref_id": "b15", "title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "journal": "", "year": "2008", "authors": "Ronan Collobert; Jason Weston"}, {"ref_id": "b16", "title": "Unsupervised partof-speech tagging with bilingual graph-based projections", "journal": "", "year": "2011", "authors": "Dipanjan Das; Slav Petrov"}, {"ref_id": "b17", "title": "Generating typed dependency parses from phrase structure parses", "journal": "", "year": "2006", "authors": "Marie-Catherine De Marneffe; Bill Maccartney; Chris D Manning"}, {"ref_id": "b18", "title": "Model-based aligner combination using dual decomposition", "journal": "", "year": "2011", "authors": "John Denero; Klaus Macherey"}, {"ref_id": "b19", "title": "Multi-view learning of word embeddings via cca", "journal": "", "year": "2011", "authors": "Paramveer Dhillon; Dean Foster; Lyle Dean"}, {"ref_id": "b20", "title": "Training and evaluating a german named entity recognizer with semantic generalization", "journal": "", "year": "2010", "authors": "Manaal Faruqui; Sebastian Pad\u00f3"}, {"ref_id": "b21", "title": "Trained named entity recognition using distributional clusters", "journal": "", "year": "2004", "authors": "Dayne Freitag"}, {"ref_id": "b22", "title": "Sparsity in dependency grammar induction", "journal": "", "year": "2010", "authors": "Jennifer Gillenwater; Kuzman Ganchev; Jo\u00e3o Gra\u00e7a; Fernando Pereira; Ben Taskar"}, {"ref_id": "b23", "title": "An ensemble model that combines syntactic and semantic clustering for discriminative dependency parsing", "journal": "", "year": "2011", "authors": "Gholamreza Haffari; Marzieh Razavi; Anoop Sarkar"}, {"ref_id": "b24", "title": "Development and evaluation of a korean treebank and its application to nlp", "journal": "", "year": "2002", "authors": "Chung-Hye Han; Na-Rare Han; Eon-Suk Ko; Martha Palmer"}, {"ref_id": "b25", "title": "Bootstrapping parsers via syntactic projection across parallel texts", "journal": "Natural Language Engineering", "year": "2005", "authors": "Rebecca Hwa; Philip Resnik; Amy Weinberg; Clara Cabezas; Okan Kolak"}, {"ref_id": "b26", "title": "Corpus-based induction of syntactic structure: models of dependency and constituency", "journal": "", "year": "2004", "authors": "Dan Klein; Chris D Manning"}, {"ref_id": "b27", "title": "Simple semi-supervised dependency parsing", "journal": "", "year": "2008", "authors": "Terry Koo; Xavier Carreras; Michael Collins"}, {"ref_id": "b28", "title": "Dependency parsing", "journal": "Morgan & Claypool Publishers", "year": "2009", "authors": "Sandra K\u00fcbler; Ryan Mcdonald; Joakim Nivre"}, {"ref_id": "b29", "title": "Phrase clustering for discriminative learning", "journal": "", "year": "2009", "authors": "Dekang Lin; Xiaoyun Wu"}, {"ref_id": "b30", "title": "Building a large annotated corpus of English: the Penn treebank", "journal": "Computational Linguistics", "year": "1993", "authors": "Mitchell P Marcus; Mary Ann Marcinkiewicz; Beatrice Santorini"}, {"ref_id": "b31", "title": "Multi-source transfer of delexicalized dependency parsers", "journal": "", "year": "2011", "authors": "Ryan Mcdonald; Slav Petrov; Keith Hall"}, {"ref_id": "b32", "title": "Name tagging with word clusters and discriminative training", "journal": "", "year": "2004", "authors": "Scott Miller; Jethran Guinness; Alex Zamanian"}, {"ref_id": "b33", "title": "Three new graphical models for statistical language modelling", "journal": "", "year": "2007", "authors": "Andriy Mnih; Geoffrey Hinton"}, {"ref_id": "b34", "title": "Using universal linguistic knowledge to guide grammar induction", "journal": "", "year": "2010", "authors": "Tahira Naseem; Harr Chen; Regina Barzilay; Mark Johnson"}, {"ref_id": "b35", "title": "The CoNLL 2007 shared task on dependency parsing", "journal": "", "year": "2007", "authors": "Joakim Nivre; Johan Hall; Sandra K\u00fcbler; Ryan Mcdonald; Jens Nilsson; Sebastian Riedel; Deniz Yuret"}, {"ref_id": "b36", "title": "Algorithms for deterministic incremental dependency parsing", "journal": "Computational Linguistics", "year": "2008", "authors": "Joakim Nivre"}, {"ref_id": "b37", "title": "An efficient method for determining bilingual word classes", "journal": "", "year": "1999", "authors": "Franz Josef Och"}, {"ref_id": "b38", "title": "Opinion mining and sentiment analysis", "journal": "Now Publishers Inc", "year": "2008", "authors": "Bo Pang; Lillian Lee"}, {"ref_id": "b39", "title": "A universal part-of-speech tagset", "journal": "", "year": "2011", "authors": "Slav Petrov; Dipanjan Das; Ryan Mcdonald"}, {"ref_id": "b40", "title": "A maximum entropy model for part-of-speech tagging", "journal": "", "year": "1996", "authors": "Adwait Ratnaparkhi"}, {"ref_id": "b41", "title": "Linguistic Structure Prediction", "journal": "Morgan & Claypool Publishers", "year": "2011", "authors": "Noah A Smith"}, {"ref_id": "b42", "title": "Data point selection for crosslanguage adaptation of dependency parsers", "journal": "", "year": "2011", "authors": "Anders S\u00f8gaard"}, {"ref_id": "b43", "title": "Lateen EM: Unsupervised training with multiple objectives, applied to dependency grammar induction", "journal": "", "year": "2011", "authors": "I Valentin; Hiyan Spitkovsky; Daniel Alshawi;  Jurafsky"}, {"ref_id": "b44", "title": "Introduction to the conll-2003 shared task: Languageindependent named entity recognition", "journal": "", "year": "2003", "authors": "Erik F Tjong; Kim Sang; Fien De Meulder"}, {"ref_id": "b45", "title": "Introduction to the conll-2002 shared task: Language-independent named entity recognition", "journal": "", "year": "2002", "authors": "Erik F ; Tjong Kim Sang"}, {"ref_id": "b46", "title": "A fast, effective, non-projective, semantically-enriched parser", "journal": "", "year": "2011", "authors": "Stephen Tratz; Eduard Hovy"}, {"ref_id": "b47", "title": "Word representations: A simple and general method for semi-supervised learning", "journal": "", "year": "2010", "authors": "Joseph Turian; Lev-Arie Ratinov; Yoshua Bengio"}, {"ref_id": "b48", "title": "Distributed word clustering for large scale class-based language modeling in machine translation", "journal": "", "year": "2008", "authors": "Jakob Uszkoreit; Thorsten Brants"}, {"ref_id": "b49", "title": "The penn chinese treebank: Phrase structure annotation of a large corpus", "journal": "Natural Language Engineering", "year": "2005", "authors": "Naiwen Xue; Fei Xia; Fu-Dong Chiou; Marta Palmer"}, {"ref_id": "b50", "title": "Cross-language parser adaptation between related languages", "journal": "", "year": "2008", "authors": "Daniel Zeman; Philip Resnik"}, {"ref_id": "b51", "title": "A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing", "journal": "", "year": "2008", "authors": "Yue Zhang; Stephen Clark"}, {"ref_id": "b52", "title": "Transition-based dependency parsing with rich non-local features", "journal": "", "year": "2011", "authors": "Yue Zhang; Joakim Nivre"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "along with the word clustering technique of Brown et al. (1992) for syntactic chunking and NER. Recently, Dhillon et al. (2011) proposed a word Single words S0c{p}, N0c{p}, Valency S0cv l , S0cvr, N0cS0v l Unigrams S 0h c, S 0l c, S0rc, N 0l c Third-order S 0h2 c, S 0l2 c, S0r2c, N 0l2 c Label set S0cS 0l l, S0cS0rl, N0cN 0l l, N0cN0rl", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Cross-lingual word cluster features for parsing. Top-left: Cross-lingual (EN-ES) word clustering model. Top-right: Samples of some of the induced cross-lingual word clusters. Bottom-left: Delexicalized cluster-augmented source (EN) treebank for training transfer parser. Bottom-right: Parsing of target (ES) sentence using the transfer parser.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Additional cluster-based parser features. S i and N i : the i th tokens in the stack and buffer. Gx{y} expands to Gxy and Gx.", "figure_data": "p: the part-of-speech tag, c: the cluster. v: the valence of the left (l) orright (r) set of children. l: the label of the token underconsideration. d: distance between the words on the top ofthe stack and buffer. S ih , S ir and S il : the head, right-mostmodifier and left-most modifier of the token at the top ofthe stack."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "88.9 76.1 90.3 82.8 85.7 81.4 82.0 77.2 86.9 83.5 84.7 74.9 83.0 CLUSTERS 85.8 89.5 77.3 90.7 83.6 85.7 82.2 83.6 77.8 87.6 86.0 86.5 75.5 84.0", "figure_data": "DADEELENESFRITKONLPTRUSVZHAVGNO CLUSTERS84.3"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Supervised parsing results measured with labeled attachment score (LAS) on the test set. All results are statistically significant at p < 0.05, except FR and NL.", "figure_data": "DEENESNLAVGNO CLUSTERS65.4 89.2 75.0 75.776.3CLUSTERS74.8 91.8 81.1 84.283.0\u2191 DEVELOPMENT SET \u2193 TEST SETNO CLUSTERS69.1 83.5 78.9 79.677.8CLUSTERS74.4 87.8 82.0 85.782.5"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Supervised NER results measured with F 1 -score on the CoNLL 2002/2003 development and test sets.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Direct transfer dependency parsing from English. Results measured by unlabeled attachment score (UAS). ONLY SUBJECT/OBJECT RELATIONS -UAS measured only over words marked as subject/object in the evaluation data.", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Direct transfer NER results (from English) measured with average F 1 -score on the CoNLL 2002/2003 development and test sets.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "L(w; C) = m i=1 p(w i |C(w i ))p(C(w i )|w i\u22121 ) . (1)", "formula_coordinates": [2.0, 324.75, 411.23, 215.25, 33.71]}, {"formula_id": "formula_1", "formula_text": "L (w; C) = m i=1 p(w i |C(w i ))p(C(w i )|C(w i\u22121 )) .", "formula_coordinates": [2.0, 322.28, 478.83, 208.65, 33.71]}, {"formula_id": "formula_2", "formula_text": "\u22121,0,1 Orthography Hyp \u22121,0,1 , Cap \u22121,0,1 , Cap \u22121:0 , Cap 0:1 , Cap \u22121:1 PoS p\u22121,0,1, p\u22121:0, p0:1, p\u22121:1, p\u22122:1, p\u22121:2 Cluster c\u22121,0,1, c\u22121:0, c0:1, c\u22121:1, c\u22122:1, c\u22121:2 Transition \u2192/p\u22121,0,1,\u2192/c\u22121,0,1,\u2192/Cap \u22121,0,1 ,\u2192/b", "formula_coordinates": [3.0, 319.18, 82.84, 215.31, 60.27]}, {"formula_id": "formula_3", "formula_text": "C T (t) = argmax k j\u2208A T |S s.t. w T j =t s j,a j C S (w S a j ) = k , (2)", "formula_coordinates": [6.0, 84.21, 540.69, 214.59, 36.99]}, {"formula_id": "formula_4", "formula_text": "L T |S (w T ; A T |S , C T , C S ) = j\u2208A T |S p(w T j |C T (w T j ))p(C T (w T j )|C S (w S a j )) .", "formula_coordinates": [6.0, 329.27, 342.59, 194.65, 51.26]}, {"formula_id": "formula_5", "formula_text": "L S,T (w S , w T ; A T |S , A S|T , C S , C T ) = L S (. . .) \u2022 L T (. . .) \u2022 L T |S (. . .) \u2022 L S|T (. . .) . (3", "formula_coordinates": [6.0, 327.82, 493.12, 207.94, 35.79]}, {"formula_id": "formula_6", "formula_text": ")", "formula_coordinates": [6.0, 535.76, 519.46, 4.24, 9.46]}], "doi": ""}