{"title": "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation", "authors": "Nataniel Ruiz; Yuanzhen Li; Varun Jampani; Yael Pritch; Michael Rubinstein; Kfir Aberman; Google Research", "pub_date": "2023-03-15", "abstract": "Figure 1. With just a few images (typically 3-5) of a subject (left), DreamBooth-our AI-powered photo booth-can generate a myriad of images of the subject in different contexts (right), using the guidance of a text prompt. The results exhibit natural interactions with the environment, as well as novel articulations and variation in lighting conditions, all while maintaining high fidelity to the key visual features of the subject.", "sections": [{"heading": "Introduction", "text": "Can you imagine your own dog traveling around the world, or your favorite bag displayed in the most exclusive showroom in Paris? What about your parrot being the main character of an illustrated storybook? Rendering such imaginary scenes is a challenging task that requires synthesizing instances of specific subjects (e.g., objects, animals) in new contexts such that they naturally and seamlessly blend into the scene.\nRecently developed large text-to-image models have shown unprecedented capabilities, by enabling high-quality and diverse synthesis of images based on a text prompt written in natural language [54,61]. One of the main advantages of such models is the strong semantic prior learned from a large collection of image-caption pairs. Such a prior learns, for instance, to bind the word \"dog\" with various instances of dogs that can appear in different poses and contexts in an image. While the synthesis capabilities of these models are unprecedented, they lack the ability to mimic the appearance of subjects in a given reference set, and synthesize novel renditions of the same subjects in different contexts. The main reason is that the expressiveness of their output domain is limited; even the most detailed textual description of an object may yield instances with different appearances.\nFurthermore, even models whose text embedding lies in a shared language-vision space [52] cannot accurately reconstruct the appearance of given subjects but only create variations of the image content (Figure 2).\nIn this work, we present a new approach for \"personalization\" of text-to-image diffusion models (adapting them to user-specific image generation needs). Our goal is to expand the language-vision dictionary of the model such that it binds new words with specific subjects the user wants to generate. Once the new dictionary is embedded in the model, it can use these words to synthesize novel photorealistic images of the subject, contextualized in different scenes, while preserving their key identifying features. The effect is akin to a \"magic photo booth\"-once a few images of the subject are taken, the booth generates photos of the subject in different conditions and scenes, as guided by simple and intuitive text prompts (Figure 1).\nMore formally, given a few images of a subject (\u223c3-5), our objective is to implant the subject into the output domain of the model such that it can be synthesized with a unique identifier. To that end, we propose a technique to represent a given subject with rare token identifiers and fine-tune a pre-trained, diffusion-based text-to-image framework.\nWe fine-tune the text-to-image model with the input images and text prompts containing a unique identifier followed by the class name of the subject (e.g., \"A [V] dog\"). The latter enables the model to use its prior knowledge on the subject class while the class-specific instance is bound with the unique identifier. In order to prevent language drift [34,40] that causes the model to associate the class name (e.g., \"dog\") with the specific instance, we propose an autogenous, class-specific prior preservation loss, which leverages the semantic prior on the class that is embedded in the model, and encourages it to generate diverse instances of the same class as our subject.\nWe apply our approach to a myriad of text-based image generation applications including recontextualization of subjects, modification of their properties, original art renditions, and more, paving the way to a new stream of previously unassailable tasks. We highlight the contribution of each component in our method via ablation studies, and compare with alternative baselines and related work. We also conduct a user study to evaluate subject and prompt fidelity in our synthesized images, compared to alternative approaches.\nTo the best of our knowledge, ours is the first technique that tackles this new challenging problem of subject-driven generation, allowing users, from just a few casually captured images of a subject, synthesize novel renditions of the subject in different contexts while maintaining its distinctive features.\nTo evaluate this new task, we also construct a new dataset Figure 2. Subject-driven generation. Given a particular clock (left), it is hard to generate it while maintaining high fidelity to its key visual features (second and third columns showing DALL-E2 [54] image-guided generation and Imagen [61] text-guided generation; text prompt used for Imagen: \"retro style yellow alarm clock with a white clock face and a yellow number three on the right part of the clock face in the jungle\"). Our approach (right) can synthesize the clock with high fidelity and in new contexts (text prompt: \"a [V] clock in the jungle\").\nthat contains various subjects captured in different contexts, and propose a new evaluation protocol that measures the subject fidelity and prompt fidelity of the generated results. We make our dataset and evaluation protocol publicly available on the project webpage.", "publication_ref": ["b52", "b59", "b50", "b32", "b38", "b52", "b59"], "figure_ref": [], "table_ref": []}, {"heading": "Related work", "text": "Image Composition. Image composition techniques [13,38,70] aim to clone a given subject into a new background such that the subject melds into the scene. To consider composition in novel poses, one may apply 3D reconstruction techniques [6,8,41,49,68] which usually works on rigid objects and require a larger number of views. Some drawbacks include scene integration (lighting, shadows, contact) and the inability to generate novel scenes. In contrast, our approach enable generation of subjects in novel poses and new contexts.\nText-to-Image Editing and Synthesis. Text-driven image manipulation has recently achieved significant progress using GANs [9,22,[28][29][30] combined with image-text representations such as CLIP [52], yielding realistic manipulations using text [2,7,21,43,48,71]. These methods work well on structured scenarios (e.g. human face editing) and can struggle over diverse datasets where subjects are varied. Crowson et al. [14] use VQ-GAN [18] and train over more diverse data to alleviate this concern. Other works [4,31] exploit the recent diffusion models [25,25,45,58,60,[62][63][64][65][66], which achieve state-of-the-art generation quality over highly diverse datasets, often surpassing GANs [15]. While most works that require only text are limited to global editing [14,33], Bar-Tal et al. [5] proposed a text-based localized editing technique without using masks, showing impressive results. While most of these editing approaches allow modification of global properties or local editing of a given image, none enables generating novel renditions of a given subject in new contexts.\nThere also exists work on text-to-image synthesis [14,16,19,24,27,35,36,50,51,55,58,67,74]. Recent large text-to-image models such as Imagen [61], DALL-E2 [54], Parti [72], CogView2 [17] and Stable Diffusion [58] demonstrated unprecedented semantic generation. These models do not provide fine-grained control over a generated image and use text guidance only. Specifically, it is challenging or impossible to preserve the identity of a subject consistently across synthesized images.\nControllable Generative Models. There are various approaches to control generative models, where some of them might prove to be viable directions for subject-driven prompt-guided image synthesis. Liu et al. [39] propose a diffusion-based technique allowing for image variations guided by reference image or text. To overcome subject modification, several works [3,44] assume a user-provided mask to restrict the modified area. Inversion [12,15,54] can be used to preserve a subject while modifying context. Prompt-to-prompt [23] allows for local and global editing without an input mask. These methods fall short of identitypreserving novel sample generation of a subject.\nIn the context of GANs, Pivotal Tuning [57] allows for real image editing by finetuning the model with an inverted latent code anchor, and Nitzan et al. [46] extended this work to GAN finetuning on faces to train a personalized prior, which requires around 100 images and are limited to the face domain. Casanova et al. [11] propose an instance conditioned GAN that can generate variations of an instance, although it can struggle with unique subjects and does not preserve all subject details.\nFinally, the concurrent work of Gal et al. [20] proposes a method to represent visual concepts, like an object or a style, through new tokens in the embedding space of a frozen text-to-image model, resulting in small personalized token embeddings. While this method is limited by the expressiveness of the frozen diffusion model, our fine-tuning approach enables us to embed the subject within the model's output domain, resulting in the generation of novel images of the subject which preserve its key visual features.", "publication_ref": ["b11", "b36", "b69", "b4", "b6", "b39", "b47", "b67", "b7", "b20", "b26", "b27", "b28", "b50", "b0", "b5", "b19", "b41", "b46", "b70", "b12", "b16", "b2", "b29", "b23", "b23", "b43", "b56", "b58", "b61", "b62", "b63", "b64", "b65", "b13", "b12", "b31", "b3", "b12", "b14", "b17", "b22", "b25", "b33", "b34", "b48", "b49", "b53", "b56", "b66", "b73", "b59", "b52", "b71", "b15", "b56", "b37", "b1", "b42", "b10", "b13", "b52", "b21", "b55", "b44", "b9", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Method", "text": "Given only a few (typically 3-5) casually captured images of a specific subject, without any textual description, our objective is to generate new images of the subject with high detail fidelity and with variations guided by text prompts. Example variations include changing the subject location, changing subject properties such as color or shape, modifying the subject's pose, viewpoint, and other semantic modifications. We do not impose any restrictions on input image capture settings and the subject image can have varying contexts. We next provide some background on textto-image diffusion models (Sec. 3.1), then present our finetuning technique to bind a unique identifier with a subject described in a few images (Sec. 3.2), and finally propose a class-specific prior-preservation loss that enables us to overcome language drift in our fine-tuned model (Sec. 3.3).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Text-to-Image Diffusion Models", "text": "Diffusion models are probabilistic generative models that are trained to learn a data distribution by the gradual denoising of a variable sampled from a Gaussian distribution. Specifically, we are interested in a pre-trained text-toimage diffusion modelx \u03b8 that, given an initial noise map \u223c N (0, I) and a conditioning vector c = \u0393(P) generated using a text encoder \u0393 and a text prompt P, generates an image x gen =x \u03b8 ( , c). They are trained using a squared error loss to denoise a variably-noised image or latent code z t := \u03b1 t x + \u03c3 t as follows:\nE x,c, ,t w t x \u03b8 (\u03b1 t x + \u03c3 t , c) \u2212 x 2 2 (1)\nwhere x is the ground-truth image, c is a conditioning vector (e.g., obtained from a text prompt), and \u03b1 t , \u03c3 t , w t are terms that control the noise schedule and sample quality, and are functions of the diffusion process time t \u223c U([0, 1]). A more detailed description is given in the supplementary material.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Personalization of Text-to-Image Models", "text": "Our first task is to implant the subject instance into the output domain of the model such that we can query the model for varied novel images of the subject. One natural idea is to fine-tune the model using the few-shot dataset of the subject. Careful care had to be taken when finetuning generative models such as GANs in a few-shot scenario as it can cause overfitting and mode-collapse -as well as not capturing the target distribution sufficiently well. There has been research on techniques to avoid these pitfalls [37,42,47,56,69], although, in contrast to our work, this line of work primarily seeks to generate images that resemble the target distribution but has no requirement of subject preservation. With regards to these pitfalls, we observe the peculiar finding that, given a careful fine-tuning setup Figure 3. Fine-tuning. Given \u223c 3\u22125 images of a subject we finetune a text-to-image diffusion model with the input images paired with a text prompt containing a unique identifier and the name of the class the subject belongs to (e.g., \"A [V] dog\"), in parallel, we apply a class-specific prior preservation loss, which leverages the semantic prior that the model has on the class and encourages it to generate diverse instances belong to the subject's class using the class name in a text prompt (e.g., \"A dog\"). using the diffusion loss from Eq 1, large text-to-image diffusion models seem to excel at integrating new information into their domain without forgetting the prior or overfitting to a small set of training images.\nDesigning Prompts for Few-Shot Personalization Our goal is to \"implant\" a new (unique identifier, subject) pair into the diffusion model's \"dictionary\" . In order to bypass the overhead of writing detailed image descriptions for a given image set we opt for a simpler approach and label all input images of the subject \"a [identifier] [class noun]\", where [identifier] is a unique identifier linked to the subject and [class noun] is a coarse class descriptor of the subject (e.g. cat, dog, watch, etc.). The class descriptor can be provided by the user or obtained using a classifier. We use a class descriptor in the sentence in order to tether the prior of the class to our unique subject and find that using a wrong class descriptor, or no class descriptor increases training time and language drift while decreasing performance. In essence, we seek to leverage the model's prior of the specific class and entangle it with the embedding of our subject's unique identifier so we can leverage the visual prior to generate new poses and articulations of the subject in different contexts.", "publication_ref": ["b35", "b40", "b45", "b54", "b68"], "figure_ref": [], "table_ref": []}, {"heading": "Rare-token Identifiers", "text": "We generally find existing English words (e.g. \"unique\", \"special\") suboptimal since the model has to learn to disentangle them from their original meaning and to re-entangle them to reference our subject. This motivates the need for an identifier that has a weak prior in both the language model and the diffusion model. A hazardous way of doing this is to select random characters in the English language and concatenate them to generate a rare identifier (e.g. \"xxy5syt00\"). In reality, the tokenizer might tokenize each letter separately, and the prior for the diffusion model is strong for these letters. We often find that these tokens incur the similar weaknesses as using common English words. Our approach is to find rare tokens in the vocabulary, and then invert these tokens into text space, in order to minimize the probability of the identifier having a strong prior. We perform a rare-token lookup in the vocabulary and obtain a sequence of rare token identifiers f (V), where f is a tokenizer; a function that maps character sequences to tokens andV is the decoded text stemming from the tokens f (V). The sequence can be of variable length k, and find that relatively short sequences of k = {1, ..., 3} work well. Then, by inverting the vocabulary using the detokenizer on f (V) we obtain a sequence of characters that define our unique identifierV. For Imagen, we find that using uniform random sampling of tokens that correspond to 3 or fewer Unicode characters (without spaces) and using tokens in the T5-XXL tokenizer range of {5000, ..., 10000} works well.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Class-specific Prior Preservation Loss", "text": "In our experience, the best results for maximum subject fidelity are achieved by fine-tuning all layers of the model. This includes fine-tuning layers that are conditioned on the text embeddings, which gives rise to the problem of language drift. Language drift has been an observed problem in language models [34,40], where a model that is pre-trained on a large text corpus and later fine-tuned for a specific task progressively loses syntactic and semantic knowledge of the language. To the best of our knowledge, we are the first to find a similar phenomenon affecting diffusion models, where to model slowly forgets how to generate subjects of the same class as the target subject.\nAnother problem is the possibility of reduced output diversity. Text-to-image diffusion models naturally posses high amounts of output diversity. When fine-tuning on a small set of images we would like to be able to generate the subject in novel viewpoints, poses and articulations. Yet, there is a risk of reducing the amount of variability in the output poses and views of the subject (e.g. snapping to the few-shot views). We observe that this is often the case, especially when the model is trained for too long.\nTo mitigate the two aforementioned issues, we propose an autogenous class-specific prior preservation loss that encourages diversity and counters language drift. In essence, our method is to supervise the model with its own generated samples, in order for it to retain the prior once the few-shot fine-tuning begins. This allows it to generate diverse images of the class prior, as well as retain knowledge about the class prior that it can use in conjunction with knowledge about the subject instance. Specifically, we generate data x pr =x(z t1 , c pr ) by using the ancestral sampler on the frozen pre-trained diffusion model with random initial noise z t1 \u223c N (0, I) and conditioning vector c pr := \u0393(f (\"a [class noun]\")). The loss becomes:\nE x,c, , ,t [w t x \u03b8 (\u03b1 t x + \u03c3 t , c) \u2212 x 2 2 + \u03bbw t x \u03b8 (\u03b1 t x pr + \u03c3 t , c pr ) \u2212 x pr 2 2 ], (2)\nwhere the second term is the prior-preservation term that supervises the model with its own generated images, and \u03bb controls for the relative weight of this term. Figure 3 illustrates the model fine-tuning with the class-generated samples and prior-preservation loss. Despite being simple, we find this prior-preservation loss is effective in encouraging output diversity and in overcoming language-drift. We also find that we can train the model for more iterations without risking overfitting. We find that \u223c 1000 iterations with \u03bb = 1 and learning rate 10 \u22125 for Imagen [61] and 5 \u00d7 10 \u22126 for Stable Diffusion [59], and with a subject dataset size of 3-5 images is enough to achieve good results. During this process, \u223c 1000 \"a [class noun]\" samples are generated -but less can be used. The training process takes about 5 minutes on one TPUv4 for Imagen, and 5 minutes on a NVIDIA A100 for Stable Diffusion.", "publication_ref": ["b32", "b38", "b59", "b57"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In this section, we show experiments and applications. Our method enables a large expanse of text-guided semantic modifications of our subject instances, including recontextualization, modification of subject properties such as material and species, art rendition, and viewpoint modification. Importantly, across all of these modifications, we are able to preserve the unique visual features that give the subject its identity and essence. If the task is recontextualization, then the subject features are unmodified, but appearance (e.g., pose) may change. If the task is a stronger semantic modification, such as crossing between our subject and another species/object, then the key features of the subject are preserved after modification. In this section, we reference the subject's unique identifier using [V]. We include specific Imagen and Stable Diffusion implementation details in the supp. material.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dataset and Evaluation", "text": "Dataset We collected a dataset of 30 subjects, including unique objects and pets such as backpacks, stuffed animals, dogs, cats, sunglasses, cartoons, etc. We separate each subject into two categories: objects and live subjects/pets. 21 of the 30 subjects are objects, and 9 are live subjects/pets.   We provide one sample image for each of the subjects in Figure 5. Images for this dataset were collected by the authors or sourced from Unsplash [1]. We also collected 25 prompts: 20 recontextualization prompts and 5 property modification prompts for objects; 10 recontextualization, 10 accessorization, and 5 property modification prompts for live subjects/pets. The full list of prompts can be found in the supplementary material.\nFor the evaluation suite we generate four images per subject and per prompt, totaling 3,000 images. This allows us to robustly measure performances and generalization capabilities of a method. We make our dataset and evaluation protocol publicly available on the project webpage for future use in evaluating subject-driven generation.\nEvaluation Metrics One important aspect to evaluate is subject fidelity: the preservation of subject details in generated images. For this, we compute two metrics: CLIP-I and DINO [10]. CLIP-I is the average pairwise cosine similarity between CLIP [52] embeddings of generated and real im-ages. Although this metric has been used in other work [20], it is not constructed to distinguish between different subjects that could have highly similar text descriptions (e.g. two different yellow clocks). Our proposed DINO metric is the average pairwise cosine similarity between the ViT-S/16 DINO embeddings of generated and real images. This is our preferred metric, since, by construction and in contrast to supervised networks, DINO is not trained to ignore differences between subjects of the same class. Instead, the self-supervised training objective encourages distinction of unique features of a subject or image. The second important aspect to evaluate is prompt fidelity, measured as the average cosine similarity between prompt and image CLIP embeddings. We denote this as CLIP-T. Figure 6. Encouraging diversity with prior-preservation loss. Naive fine-tuning can result in overfitting to input image context and subject appearance (e.g. pose). PPL acts as a regularizer that alleviates overfitting and encourages diversity, allowing for more pose variability and appearance diversity.", "publication_ref": ["b8", "b50", "b18"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Comparisons", "text": "We compare our results with Textual Inversion, the recent concurrent work of Gal et al. [20], using the hyperparameters provided in their work. We find that this work is the only comparable work in the literature that is subjectdriven, text-guided and generates novel images. We generate images for DreamBooth using Imagen, DreamBooth using Stable Diffusion and Textual Inversion using Stable Diffusion. We compute DINO and CLIP-I subject fidelity metrics and the CLIP-T prompt fidelity metric. In Table 1 we show sizeable gaps in both subject and prompt fidelity metrics for DreamBooth over Textual Inversion. We find that DreamBooth (Imagen) achieves higher scores for both subject and prompt fidelity than DreamBooth (Stable Diffusion), approaching the upper-bound of subject fidelity for real images. We believe that this is due to the larger expressive power and higher output quality of Imagen.\nFurther, we compare Textual Inversion (Stable Diffusion) and DreamBooth (Stable Diffusion) by conducting a user study. For subject fidelity, we asked 72 users to answer questionnaires of 25 comparative questions (3 users per questionnaire), totaling 1800 answers. Samples are randomly selected from a large pool. Each question shows the set of real images for a subject, and one generated image of that subject by each method (with a random prompt). Users are asked to answer the question: \"Which of the two images best reproduces the identity (e.g. item type and details) of the reference item?\", and we include a \"Cannot Determine / Both Equally\" option. Similarly for prompt fidelity, we  ask \"Which of the two images is best described by the reference text?\". We average results using majority voting and present them in Table 2. We find an overwhelming preference for DreamBooth for both subject fidelity and prompt fidelity. This shines a light on results in Table 1, where DINO differences of around 0.1 and CLIP-T differences of 0.05 are significant in terms of user preference. Finally, we show qualitative comparisons in Figure 4. We observe that DreamBooth better preserves subject identity, and is more faithful to prompts. We show samples of the user study in the supp. material.", "publication_ref": ["b18"], "figure_ref": ["fig_0"], "table_ref": ["tab_1"]}, {"heading": "Ablation Studies", "text": "Prior Preservation Loss Ablation We fine-tune Imagen on 15 subjects from our dataset, with and without our proposed prior preservation loss (PPL). The prior preservation loss seeks to combat language drift and preserve the prior. We compute a prior preservation metric (PRES) by computing the average pairwise DINO embeddings between generated images of random subjects of the prior class and real images of our specific subject. The higher this metric, the more similar random subjects of the class are to our specific subject, indicating collapse of the prior. We report results in Table 3 and observe that PPL substantially counteracts language drift and helps retain the ability to generate diverse images of the prior class. Additionally, we compute a diversity metric (DIV) using the average LPIPS [73] cosine similarity between generated images of same subject with same prompt. We observe that our model trained with PPL achieves higher diversity (with slightly diminished subject fidelity), which can also be observed qualitatively in Figure 6, where our model trained with PPL overfits less to the environment of the reference images and can generate the dog in more diverse poses and articulations.", "publication_ref": ["b72"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Class-Prior Ablation", "text": "We finetune Imagen on a subset of our dataset subjects (5 subjects) with no class noun, a randomly sampled incorrect class noun, and the correct class noun. With the correct class noun for our subject, we are able to faithfully fit to the subject, take advantage of the Figure 7. Recontextualization. We generate images of the subjects in different environments, with high preservation of subject details and realistic scene-subject interactions. We show the prompts below each image.\nclass prior, allowing us to generate our subject in various contexts. When an incorrect class noun (e.g. \"can\" for a backpack) is used, we run into contention between our subject and and the class prior -sometimes obtaining cylindrical backpacks, or otherwise misshapen subjects. If we train with no class noun, the model does not leverage the class prior, has difficulty learning the subject and converging, and can generate erroneous samples. Subject fidelity results are shown in Table 4, with substantially higher subject fidelity for our proposed approach.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Applications", "text": "Recontextualization We can generate novel images for a specific subject in different contexts (Figure 7) with descriptive prompts (\"a [V] [class noun] [context description]\"). Importantly, we are able to generate the subject in new poses and articulations, with previously unseen scene structure and realistic integration of the subject in the scene (e.g. contact, shadows, reflections).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Art Renditions Given a prompt \"a painting of a [V] [class noun] in the style of [famous painter]\" or \"a statue of a [V]", "text": "[class noun] in the style of [famous sculptor]\" we are able to generate artistic renditions of our subject. Unlike style transfer, where the source structure is preserved and only the style is transferred, we are able to generate meaningful, novel variations depending on the artistic style, while preserving subject identity. E.g, as shown in Figure 8, \"Michelangelo\", we generated a pose that is novel and not seen in the input images. Novel View Synthesis We are able to render the subject under novel viewpoints. In Figure 8, we generate new im- ages of the input cat (with consistent complex fur patterns) under new viewpoints. We highlight that the model has not seen this specific cat from behind, below, or above -yet it is able to extrapolate knowledge from the class prior to generate these novel views given only 4 frontal images of the subject.\nProperty Modification We are able to modify subject properties. For example, we show crosses between a specific Chow Chow dog and different animal species in the bottom row of Figure 8. We prompt the model with sentences of the following structure: \"a cross of a [V] dog and a [target species]\". In particular, we can see in this example that the identity of the dog is well preserved even when the species changes -the face of the dog has certain unique features that are well preserved and melded with the target species. Other property modifications are possible, such as material modification (e.g. \"a transparent [V] teapot\" in Figure 7). Some are harder than others and depend on the prior of the base generation model.", "publication_ref": [], "figure_ref": ["fig_2", "fig_2", "fig_2"], "table_ref": []}, {"heading": "Limitations", "text": "We illustrate some failure models of our method in Figure 9. The first is related to not being able to accurately generate the prompted context. Possible reasons are a weak prior for these contexts, or difficulty in generating both the subject and specified concept together due to low probability of co-occurrence in the training set. The second is context-appearance entanglement, where the appearance of the subject changes due to the prompted context, exemplified in Figure 9 with color changes of the backpack. Third, we also observe overfitting to the real images that happen when the prompt is similar to the original setting in which the subject was seen.\nOther limitations are that some subjects are easier to learn than others (e.g. dogs and cats). Occasionally, with subjects that are rarer, the model is unable to support as many subject variations. Finally, there is also variability in the fidelity of the subject and some generated images might contain hallucinated subject features, depending on the strength of the model prior, and the complexity of the semantic modification.", "publication_ref": [], "figure_ref": ["fig_3", "fig_3"], "table_ref": []}, {"heading": "Conclusions", "text": "We presented an approach for synthesizing novel renditions of a subject using a few images of the subject and the guidance of a text prompt. Our key idea is to embed a given subject instance in the output domain of a text-to-image diffusion model by binding the subject to a unique identifier. Remarkably -this fine-tuning process can work given only 3-5 subject images, making the technique particularly accessible. We demonstrated a variety of applications with animals and objects in generated photorealistic scenes, in most cases indistinguishable from real images.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgement", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Supplementary Material Background", "text": "Text-to-Image Diffusion Models Diffusion models are probabilistic generative models that are trained to learn a data distribution by the gradual denoising of a variable sampled from a Gaussian distribution. Specifically, this corresponds to learning the reverse process of a fixed-length Markovian forward process. In simple terms, a conditional diffusion modelx \u03b8 is trained using a squared error loss to denoise a variably-noised image z t := \u03b1 t x+\u03c3 t as follows:\nE x,c, ,t w t x \u03b8 (\u03b1 t x + \u03c3 t , c) \u2212 x 2 2 (3)\nwhere x is the ground-truth image, c is a conditioning vector (e.g., obtained from a text prompt), \u223c N (0, I) is a noise term and \u03b1 t , \u03c3 t , w t are terms that control the noise schedule and sample quality, and are functions of the diffusion process time t \u223c U([0, 1]). At inference time, the diffusion model is sampled by iteratively denoising z t1 \u223c N (0, I) using either the deterministic DDIM [64] or the stochastic ancestral sampler [25]. Intermediate points z t1 , . . . , z t T , where 1 = t 1 > \u2022 \u2022 \u2022 > t T = 0, are generated, with decreasing noise levels. These points,x t 0 :=x \u03b8 (z t , c), are functions of the x-predictions.\nRecent state-of-the-art text-to-image diffusion models use cascaded diffusion models in order to generate highresolution images from text [54,61]. Specifically, [61] uses a base text-to-image model with 64x64 output resolution, and two text-conditional super-resolution (SR) models 64 \u00d7 64 \u2192 256 \u00d7 256 and 256 \u00d7 256 \u2192 1024 \u00d7 1024. Ramesh et al. [54] use a similar configuration, with unconditional SR models. A key component of high-quality sample generations from [61] is the use of noise conditioning augmentation [26] for the two SR modules. This consists in corrupting the intermediate image using noise with specific strength, and then conditioning the SR model on the level of corruption. Saharia et al. [61] select Gaussian noise as the form of augmentation.\nOther recent state-of-the-art text-to-image diffusion models, such as Stable Diffusion [59], use a single diffusion model to generate high-resolution images. Specifically, the forward and backward diffusion processes occur in a lowerdimensional latent space and an encoder-decoder architecture is trained on a large image dataset to translate images into latent codes. At inference time, a random noise latent code goes through the backward diffusion process and the pre-trained decoder is used to generate the final image. Our method can be naturally applied to this scenario, where the U-Net (and possibly the text encoder) are trained, and the decoder is fixed.", "publication_ref": ["b63", "b23", "b52", "b59", "b59", "b52", "b59", "b24", "b59", "b57"], "figure_ref": [], "table_ref": []}, {"heading": "Vocabulary Encoding", "text": "The details of text-conditioning in text-to-image diffusion models are of high importance for visual quality and semantic fidelity. Ramesh et al. [54] use CLIP text embeddings that are translated into image embeddings using a learned prior, while Saharia et al. [61] use a pre-trained T5-XXL language model [53]. In our work, we use the latter. Language models like T5-XXL generate embeddings of a tokenized text prompt, and vocabulary encoding is an important pre-processing step for prompt embedding. In order to transform a text prompt P into a conditioning embedding c, the text is first tokenized using a tokenizer f using a learned vocabulary. Following [61], we use the SentencePiece tokenizer [32]. After tokenizing a prompt P using tokenizer f we obtain a fixed-length vector f (P). The language model \u0393 is conditioned on this token identifier vector to produce an embedding c := \u0393(f (P)). Finally, the text-to-image diffusion model is directly conditioned on c.", "publication_ref": ["b52", "b59", "b51", "b59", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "Dataset", "text": "Our dataset includes 30 subjects. We separate each subject into two categories: objects and live subjects/pets. 21 of the 30 subjects are objects, and 9 are live subjects/pets. We provide one sample image for each of the subjects in Figure 5. Images for this dataset were collected by the authors or sourced from Unsplash [1].\nWe also collected 25 prompts: 20 recontextualization prompts and 5 property modification prompts for objects. 10 recontextualization, 10 accessorization, and 5 property modification prompts for live subjects/pets. Prompts are shown in Figure 10 For the evaluation suite we generate four images per subject and per prompt, totaling 3,000 images. This allows us to robustly measure performances and generalization capabilities of a method. We make our dataset and evaluation protocol publicly available on the project webpage for future use in evaluating subject-driven generation.", "publication_ref": [], "figure_ref": ["fig_1", "fig_4"], "table_ref": []}, {"heading": "Subject Fidelity Metrics", "text": "In the main paper we comment on the superiority of our proposed DINO metric in terms of subject fidelity. We hypothesize that this is because DINO is, in essence, trained in a self-supervised manner to distinguish different images from each other modulo data augmentations. This is in contrast to the CLIP-I metric, where CLIP is trained with text-image pairs and encodes more descriptive information about images -but not necessarily fine details that are not present in the text annotations. We give an example in Figure 11, where the first column contains a reference real image, the second column a different real image, the third column a DreamBooth generated image and the last column an image generated using Textual Inversion. We compare the 2nd, 3rd and 4th image to the real reference image using the CLIP-I and DINO metrics. We observe that the 2nd real image obtains both the highest CLIP-I and DINO scores. The DreamBooth sample looks much more similar to the reference sample than the Textual Inversion sample, yet the CLIP-I score for the Textual Inversion sample is much higher than the DreamBooth sample. However, we can see that the DINO similarity is higher for the Dream-Booth sample -which more closely tracks human evaluation of subject fidelity. In order to quantitatively test this, we compute correlations between DINO/CLIP-I scores and normalized human preference scores. DINO has a Pearson correlation coeff. of 0.32 with human preference (vs. 0.27 for the CLIP-I metric used in [20]), with a very low p-value of 9.44 \u00d7 10 \u221230 .", "publication_ref": ["b18"], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "User Study", "text": "Below we include the full instructions used for our user study. For subject fidelity:\n\u2022 Read the task carefully, inspect the reference items and then inspect the generated items.\n\u2022 Select which of the two generated items (A or B) reproduces the identity (e.g. item type and details) of the reference item.\n\u2022 The subject might be wearing accessories (e.g. hats, outfits). These should not affect your answer. Do not take them into account.\n\u2022 If you're not sure, select Cannot Determine / Both Equally.\nFor text fidelity:\n\u2022 Read the task carefully, inspect the reference text and then inspect the generated items.\n\u2022 Select which of the two generated items (A or B) is best described by the reference text.\n\u2022 If you're not sure, select Cannot Determine / Both Equally.\nFor each study we asked 72 users to answer questionnaires of 25 comparative questions (3 users per questionnaire), totaling 1800 answers -with 600 image pairs evaluated.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Additional Applications and Examples", "text": "Additional Samples We provide a large amount of additional random samples in an annex HTML file. We compare real images, to DreamBooth generated images using Imagen and Stable Diffusion as well as images generated using Textual Inversion on Stable Diffusion.\nRecontextualization We show additional high-quality examples of recontextualization in Figure 12.\nArt Renditions We show additional examples of original artistic renditions of a personalized model in Figure 13.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Expression Manipulation", "text": "Our method allows for new image generation of the subject with modified expressions that are not seen in the original set of subject images. We show examples in Figure 14. The range of expressiveness is high, ranging from negative to positive valence emotions and different levels of arousal. In all examples, the uniqueness of the subject dog is preserved -specifically, the asymmetric white streak on its face remains in all generated images. Novel View Synthesis We show more viewpoints for novel view synthesis in Figure 15, along with prompts used to generate the samples.\nAccessorization An interesting capability stemming from the strong compositional prior of the generation model is the ability to accessorize subjects. In Figure 16 we show examples of accessorization of a Chow Chow dog. We prompt the model with a sentence of the form: \"a [V] [class noun] wearing [accessory]\". In this manner, we are able to fit different accessories onto this dog -with aesthetically pleasing results. Note that the identity of the dog is preserved in all frames, and subject-accessory contact and articulation are realistic.\nProperty Modification We are able to modify subject instance properties. For example we can include a color adjective in the prompt sentence \"a [color adjective] [V] [class noun]\". In that way, we can generate novel instances of our subject with different colors. The generated scene can be very similar to the original scene, or it can be changed given a descriptive prompt. We show color changes of a car in the first row of Figure 17. We select similar viewpoints for effect, but we can generate different viewpoints of the car with different colors in different scenarios. This is a simple example of property modification, but more semantically complex property modifications can be achieved using our method. For example, we show crosses between a specific Chow Chow dog and different animal species in the bottom row of Figure 17. We prompt the model with sentences of the following structure: \"a cross of a [V] dog and a [target species]\". In particular, we can see in this example that the identity of the dog is well preserved even when the species changes -the face of the dog has certain individual properties that are well preserved and melded with the target species. Other property modifications are possible, such as material modification (e.g. a dog made out of stone). Some are harder than others and depend on the prior of the base generation model.", "publication_ref": [], "figure_ref": ["fig_0", "fig_1"], "table_ref": []}, {"heading": "Comic Book Generation", "text": "In addition to photorealistic images, our method is able to capture the appearance of drawn media and more. In Figure 18 we present, to the best of our knowledge, the first instance of a full comic with a persistent character generated by a generative model. Each comic frame was generated using a descriptive prompt (e.g \"a [V] cartoon grabbing a fork and a knife saying \"time to eat\"\").", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Additional Experiments Prior Preservation Loss", "text": "Here we show qualitative examples of how our prior preservation loss (PPL) conserves variability in the prior and show sample results in Figure 19. We verify that a vanilla model is able to generate a large variety of dogs, while a naively fine-tuned model on the subject dog exhibits language drift and generates our subject dog given the prompt \"a dog\". Our proposed loss preserves the variability of the prior and the model is able to generate new instances of our dog given a prompt of the style \"a [V] dog\" but also varied instances of dogs given a \"a dog\" prompt.", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Effect of Training Images", "text": "Here we run an experiment on the effects of the number of input images for model personalization. Specifically, we train models for two subjects, 5 models per subject with input images ranging from 1 to 5. We generate 4 images for 10 different recontextualization prompts for each subject. We present qualitative results in Figure 20. We can observe that for some subjects that are more common, and lie more strongly in the distribution of the diffusion model, such as the selected Corgi dog, we are able to accurately  capture the appearance using only two images -and sometimes only one, given careful hyperparameter choice. For objects that are more rare, such as the selected backpack, we need more samples to accurately preserve the subject and to recontextualize it to diverse settings. Our quantitative results support these conclusions -we present the DINO subject fidelity metric in Table 5 and the CLIP-T prompt fidelity metric in Table 6. For both subjects we see that the optimal amount of input images for subject and prompt is 4. This number can vary depending on the subject and we settle on 3-5 images for model personalization.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Personalized Instance-Specific Super-Resolution and Low-level Noise Augmentation for Imagen", "text": "While the text-to-image diffusion model controls for most visual semantics, the super-resolution (SR) models are essential to achieve photorealistic content and to preserve subject instance details. We find that if SR networks are used without fine-tuning, the generated output can contain artifacts since the SR models might not be familiar with certain details or textures of the subject instance, or the subject instance might have hallucinated incorrect features, or missing details. Figure 21 (bottom row) shows some sample output images with no fine-tuning of SR models, where the model hallucinates some high-frequency details. We find that fine-tuning the 64 \u00d7 64 \u2192 256 \u00d7 256 SR model is essential for most subjects, and fine-tuning the 256 \u00d7 256 \u2192 1024 \u00d7 1024 model can benefit some subject instances with high levels of fine-grained detail.\nWe find results to be suboptimal if the training recipes and test parameters of Saharia et al. [61] are used to finetune the SR models with the given few shots of a subject instance. Specifically, we find that maintaining the original level of noise augmentation used to train the SR networks leads to the blurring of high-frequency patterns of the subject and of the environment. See Figure 21 (middle row) for sample generations. In order to faithfully reproduce the subject instance, we reduce the level of noise augmentation from 10 \u22123 to 10 \u22125 during fine-tuning of the 256 \u00d7 256 SR model. With this small modification, We are able to recover fine-grained details of the subject instance. We show how using lower noise to train the super-resolution models improves fidelity. Specifically, we show in Figure 21 that if the super-resolution models are not fine-tuned, we observe hallucination of high-frequency patterns on the subject which hurts identity preservation. Further, if we use the groundtruth noise augmentation level used for training the Imagen 256 \u00d7 256 model (10 \u22123 ), we obtain blurred and non-crisp details. If the noise used to train the SR model is reduced to 10 \u22125 , then we conserve a large amount of detail without pattern hallucination or blurring.", "publication_ref": ["b59"], "figure_ref": [], "table_ref": []}, {"heading": "Comparisons", "text": "We include additional qualitative comparisons with Gal et al. [20] in Figure 22. For this comparison, we train our model on the training images of two objects appear in the teaser of their work (headless sculpture and cat toy) kindly provided by Gal et al. [20], and apply the prompts suggested in their paper. For prompts where they present several generated images, we handpicked their best sample (with the highest image quality and morphological similarity to the subject). We find that our work can generate the same semantic variations of these unique objects, with a high emphasis on preserving the subject identity, as can be seen, for instance, by the detailed patterns of the cat sculpture that are preserved.\nNext, we show comparisons of recontextualization of a subject clock, with distinctive features using our method and prompt engineering using vanilla Imagen [61] and the public API of DALL-E 2 [54]. After multiple iterations using both models, we settle for the base prompt \"retro style yellow alarm clock with a white clock face and a yellow number three on the lower right part of the clock face\" to describe all of the important features of the subject clock example. We find that while DALL-E 2 and vanilla Imagen are able to generate retro-style yellow alarm clocks, they struggle to represent a number 3 on the clock face, distinct from the clock face numbers. In general, we find that it is very hard to control fine-grained details of subject appearance, even with exhaustive prompt engineering. Also, we find that context can bleed into the appearance of our subject instance. We show the results in Figure 23, and can observe that our method conserves fine-grained details of the subject instance such as the shape, the clock face font, and the large yellow number three on the clock face, among others.", "publication_ref": ["b18", "b18", "b59", "b52"], "figure_ref": [], "table_ref": []}, {"heading": "Societal Impact", "text": "This project aims to provide users with an effective tool for synthesizing personal subjects (animals, objects) in different contexts. While general text-to-image models might be biased towards specific attributes when synthesizing im-ages from text, our approach enables the user to get a better reconstruction of their desirable subjects. On contrary, malicious parties might try to use such images to mislead viewers. This is a common issue, existing in other generative models approaches or content manipulation techniques. Future research in generative modeling, and specifically of personalized generative priors, must continue investigating and revalidating these concerns. Figure 12. Additional recontextualization samples of a backpack, vase, and teapot subject instances. We are able to generate images of the subject instance in different environments, with high preservation of subject details and realistic interaction between the scene and the subject. We display the conditioning prompts below each image. Figure 13. Additional artistic renderings of a dog instance in the style of famous painters. We remark that many of the generated poses, e.g., the Michelangelo renditions, were not seen in the training set. We also note that some renditions seem to have novel compositions and faithfully imitate the style of the painter. Figure 14. Expression manipulation of a dog instance. Our technique can synthesize various expressions that do not appear in the input images, demonstrating the extrapolation power of the model. Note the unique asymmetric white streak on the subject dog's face. Figure 15. Text-guided view synthesis. Our technique can synthesize images with specified viewpoints for a subject cat (left to right: top, bottom, side, and back views). Note that the generated poses are different from the input poses, and the background changes in a realistic manner given a pose change. We also highlight the preservation of complex fur patterns on the subject cat's forehead. Figure 16. Outfitting a dog with accessories. The identity of the subject is preserved and many different outfits or accessories can be applied to the dog given a prompt of type \"a [V] dog wearing a police/chef/witch outfit\". We observe a realistic interaction between the subject dog and the outfits or accessories, as well as a large variety of possible options. Figure 17. Modification of subject properties while preserving their key features. We show color modifications in the first row (using prompts \"a [color] [V] car\"), and crosses between a specific dog and different animals in the second row (using prompts \"a cross of a [V] dog and a [target species]\"). We highlight the fact that our method preserves unique visual features that give the subject its identity or essence, while performing the required property modification. Figure 18. Generated comic. We present, to the best of our knowledge, the first comic comic with a persistent character generated by prompting a generative model. Figure 19. Preservation of class semantic priors with prior-preservation loss. Fine-tuning using images of our subject without priorpreservation loss results in language drift and the model loses the capability of generating other members of our subject's class. Using a prior-preservation loss term allows our model to avoid this and to preserve the subject class' prior.\nFigure 20. Impact of number of input images. We observe that given only one input image, we are close to capture the identity of some subjects (e.g. Corgi dog). More images are usually needed -two images are sufficient to reconstruct the Corgi dog in this example whereas at least 3 are needed for a more rare item such as the backpack. Figure 21. Ablations with fine-tuning the super-resolution (SR) models. Using the normal level of noise augmentation of [61] to train the SR models results in blurred high-frequency patterns, while no fine-tuning results in hallucinated high-frequency patterns. Using low-level noise augmentation for SR models improves sample quality and subject fidelity. Image credit (input images): Unsplash. Figure 22. Comparisons with Gal et al. [20] using the subjects, images, and prompts from their work. Our approach is able to generate semantically correct variations of unique objects, exhibiting a higher degree of preservation of subject features. Input images provided by Gal et al. [20]. Figure 23. Comparison with DALL-E 2 and Imagen with detailed prompt engineering. After several trial-and-error iterations, the base prompt used to generate DALL-E 2 and Imagen results was \"retro style yellow alarm clock with a white clock face and a yellow number three on the right part of the clock face\", which is highly descriptive of the subject clock. In general, it is hard to control fine-grained details of subject appearance using prompts, even with large amounts of prompt engineering. Also, we can observe how context cues in the prompt can bleed into subject appearance (e.g. with a blue number 3 on the clock face when the context is \"on top of blue fabric\"). Image credit (input images): Unsplash.", "publication_ref": ["b18", "b59", "b18", "b18"], "figure_ref": ["fig_0", "fig_1", "fig_2", "fig_3"], "table_ref": []}, {"heading": "", "text": "We thank Rinon Gal, Adi Zicher, Ron Mokady, Bill Freeman, Dilip Krishnan, Huiwen Chang and Daniel Cohen-Or for their valuable inputs that helped improve this work, and to Mohammad Norouzi, Chitwan Saharia and William Chan for providing us with their support and the pretrained Imagen models. Finally, a special thanks to David Salesin for his feedback, advice and for his support for the project.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Clip2stylegan: Unsupervised extraction of stylegan edit directions", "journal": "", "year": "2021", "authors": "Rameen Abdal; Peihao Zhu; John Femiani; J Niloy; Peter Mitra;  Wonka"}, {"ref_id": "b1", "title": "", "journal": "", "year": "2022", "authors": "Omri Avrahami; Ohad Fried; Dani Lischinski"}, {"ref_id": "b2", "title": "Blended diffusion for text-driven editing of natural images", "journal": "", "year": "2022", "authors": "Omri Avrahami; Dani Lischinski; Ohad Fried"}, {"ref_id": "b3", "title": "Yoni Kasten, and Tali Dekel. Text2live: Text-driven layered image and video editing", "journal": "", "year": "2022", "authors": "Omer Bar-Tal; Dolev Ofri-Amar; Rafail Fridman"}, {"ref_id": "b4", "title": "Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields", "journal": "", "year": "2021-10", "authors": "Jonathan T Barron; Ben Mildenhall; Matthew Tancik; Peter Hedman; Ricardo Martin-Brualla; Pratul P Srinivasan"}, {"ref_id": "b5", "title": "", "journal": "", "year": "2021", "authors": "David Bau; Alex Andonian; Audrey Cui; Yeonhwan Park; Ali Jahanian; Aude Oliva; Antonio Torralba"}, {"ref_id": "b6", "title": "Samurai: Shape and material from unconstrained real-world arbitrary image collections", "journal": "", "year": "2022", "authors": "Mark Boss; Andreas Engelhardt; Abhishek Kar; Yuanzhen Li; Deqing Sun; Jonathan T Barron; Hendrik Lensch; Varun Jampani"}, {"ref_id": "b7", "title": "Large scale gan training for high fidelity natural image synthesis", "journal": "", "year": "2018", "authors": "Andrew Brock; Jeff Donahue; Karen Simonyan"}, {"ref_id": "b8", "title": "Emerging properties in self-supervised vision transformers", "journal": "", "year": "2021", "authors": "Mathilde Caron; Hugo Touvron; Ishan Misra; Herv\u00e9 J\u00e9gou; Julien Mairal; Piotr Bojanowski; Armand Joulin"}, {"ref_id": "b9", "title": "Instanceconditioned gan", "journal": "", "year": "2021", "authors": "Arantxa Casanova; Marlene Careil; Jakob Verbeek; Michal Drozdzal; Adriana Romero Soriano"}, {"ref_id": "b10", "title": "Ilvr: Conditioning method for denoising diffusion probabilistic models", "journal": "", "year": "2021", "authors": "Jooyoung Choi; Sungwon Kim; Yonghyun Jeong; Youngjune Gwon; Sungroh Yoon"}, {"ref_id": "b11", "title": "Dovenet: Deep image harmonization via domain verification", "journal": "", "year": "2020", "authors": "Wenyan Cong; Jianfu Zhang; Li Niu; Liu Liu; Zhixin Ling; Weiyuan Li; Liqing Zhang"}, {"ref_id": "b12", "title": "Vqgan-clip: Open domain image generation and editing with natural language guidance", "journal": "", "year": "2022", "authors": "Katherine Crowson; Stella Biderman; Daniel Kornis; Dashiell Stander; Eric Hallahan; Louis Castricato; Edward Raff"}, {"ref_id": "b13", "title": "Diffusion models beat gans on image synthesis", "journal": "", "year": "2021", "authors": "Prafulla Dhariwal; Alexander Nichol"}, {"ref_id": "b14", "title": "Mastering text-to-image generation via transformers", "journal": "Advances in Neural Information Processing Systems", "year": "2021", "authors": "Ming Ding; Zhuoyi Yang; Wenyi Hong; Wendi Zheng; Chang Zhou; Da Yin; Junyang Lin; Xu Zou; Zhou Shao; Hongxia Yang"}, {"ref_id": "b15", "title": "Cogview2: Faster and better text-to-image generation via hierarchical transformers", "journal": "", "year": "2022", "authors": "Ming Ding; Wendi Zheng; Wenyi Hong; Jie Tang"}, {"ref_id": "b16", "title": "Taming transformers for high-resolution image synthesis", "journal": "", "year": "2021", "authors": "Patrick Esser; Robin Rombach; Bjorn Ommer"}, {"ref_id": "b17", "title": "Make-a-scene: Scenebased text-to-image generation with human priors", "journal": "", "year": "2022", "authors": "Oran Gafni; Adam Polyak; Oron Ashual; Shelly Sheynin; Devi Parikh; Yaniv Taigman"}, {"ref_id": "b18", "title": "Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-toimage generation using textual inversion", "journal": "", "year": "2022", "authors": "Rinon Gal; Yuval Alaluf; Yuval Atzmon"}, {"ref_id": "b19", "title": "Stylegan-nada: Clip-guided domain adaptation of image generators", "journal": "", "year": "2021", "authors": "Rinon Gal; Or Patashnik; Haggai Maron; Gal Chechik; Daniel Cohen-Or"}, {"ref_id": "b20", "title": "Generative adversarial nets", "journal": "", "year": "2014", "authors": "Ian Goodfellow; Jean Pouget-Abadie; Mehdi Mirza; Bing Xu; David Warde-Farley; Sherjil Ozair; Aaron Courville; Yoshua Bengio"}, {"ref_id": "b21", "title": "Prompt-to-prompt image editing with cross attention control", "journal": "", "year": "2022", "authors": "Amir Hertz; Ron Mokady; Jay Tenenbaum; Kfir Aberman; Yael Pritch; Daniel Cohen-Or"}, {"ref_id": "b22", "title": "Semantic object accuracy for generative text-to-image synthesis", "journal": "", "year": "2020", "authors": "Tobias Hinz; Stefan Heinrich; Stefan Wermter"}, {"ref_id": "b23", "title": "Denoising diffusion probabilistic models", "journal": "", "year": "2020", "authors": "Jonathan Ho; Ajay Jain; Pieter Abbeel"}, {"ref_id": "b24", "title": "Cascaded diffusion models for high fidelity image generation", "journal": "J. Mach. Learn. Res", "year": "2022", "authors": "Jonathan Ho; Chitwan Saharia; William Chan; J David; Mohammad Fleet; Tim Norouzi;  Salimans"}, {"ref_id": "b25", "title": "Zero-shot text-guided object generation with dream fields", "journal": "", "year": "2022", "authors": "Ajay Jain; Ben Mildenhall; Jonathan T Barron; Pieter Abbeel; Ben Poole"}, {"ref_id": "b26", "title": "Alias-free generative adversarial networks", "journal": "", "year": "2021", "authors": "Tero Karras; Miika Aittala; Samuli Laine; Erik H\u00e4rk\u00f6nen; Janne Hellsten; Jaakko Lehtinen; Timo Aila"}, {"ref_id": "b27", "title": "A style-based generator architecture for generative adversarial networks", "journal": "", "year": "2019", "authors": "Tero Karras; Samuli Laine; Timo Aila"}, {"ref_id": "b28", "title": "Analyzing and improving the image quality of stylegan", "journal": "", "year": "2020", "authors": "Tero Karras; Samuli Laine; Miika Aittala; Janne Hellsten; Jaakko Lehtinen; Timo Aila"}, {"ref_id": "b29", "title": "Diffusionclip: Text-guided diffusion models for robust image manipulation", "journal": "", "year": "2022", "authors": "Gwanghyun Kim; Taesung Kwon; Jong Chul Ye"}, {"ref_id": "b30", "title": "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing", "journal": "", "year": "2018", "authors": "Taku Kudo; John Richardson"}, {"ref_id": "b31", "title": "Clipstyler: Image style transfer with a single text condition", "journal": "", "year": "2021", "authors": "Gihyun Kwon; Jong Chul Ye"}, {"ref_id": "b32", "title": "Countering language drift via visual grounding", "journal": "", "year": "2019", "authors": "Jason Lee; Kyunghyun Cho; Douwe Kiela"}, {"ref_id": "b33", "title": "Controllable text-to-image generation", "journal": "Advances in Neural Information Processing Systems", "year": "2019", "authors": "Bowen Li; Xiaojuan Qi; Thomas Lukasiewicz; Philip Torr"}, {"ref_id": "b34", "title": "Object-driven text-to-image synthesis via adversarial training", "journal": "", "year": "2019", "authors": "Wenbo Li; Pengchuan Zhang; Lei Zhang; Qiuyuan Huang; Xiaodong He; Siwei Lyu; Jianfeng Gao"}, {"ref_id": "b35", "title": "Few-shot image generation with elastic weight consolidation", "journal": "", "year": "2020", "authors": "Yijun Li; Richard Zhang; Jingwan Lu; Eli Shechtman"}, {"ref_id": "b36", "title": "St-gan: Spatial transformer generative adversarial networks for image compositing", "journal": "", "year": "2018", "authors": "Chen-Hsuan Lin; Ersin Yumer; Oliver Wang; Eli Shechtman; Simon Lucey"}, {"ref_id": "b37", "title": "More control for free! image synthesis with semantic diffusion guidance", "journal": "", "year": "2021", "authors": "Xihui Liu; Dong Huk Park; Samaneh Azadi; Gong Zhang; Arman Chopikyan; Yuxiao Hu; Humphrey Shi; Anna Rohrbach; Trevor Darrell"}, {"ref_id": "b38", "title": "Countering language drift with seeded iterated learning", "journal": "PMLR", "year": "2020", "authors": "Yuchen Lu; Soumye Singhal; Florian Strub; Aaron Courville; Olivier Pietquin"}, {"ref_id": "b39", "title": "Nerf: Representing scenes as neural radiance fields for view synthesis", "journal": "Springer", "year": "2020", "authors": "Ben Mildenhall; P Pratul; Matthew Srinivasan; Jonathan T Tancik; Ravi Barron; Ren Ramamoorthi;  Ng"}, {"ref_id": "b40", "title": "Freeze the discriminator: a simple baseline for fine-tuning gans", "journal": "", "year": "2020", "authors": "Sangwoo Mo; Minsu Cho; Jinwoo Shin"}, {"ref_id": "b41", "title": "Self-distilled stylegan: Towards generation from internet photos", "journal": "", "year": "2022", "authors": "Ron Mokady; Omer Tov; Michal Yarom; Oran Lang; Inbar Mosseri; Tali Dekel; Daniel Cohen-Or; Michal Irani"}, {"ref_id": "b42", "title": "Glide: Towards photorealistic image generation and editing with text-guided diffusion models", "journal": "", "year": "2021", "authors": "Alex Nichol; Prafulla Dhariwal; Aditya Ramesh; Pranav Shyam; Pamela Mishkin; Bob Mcgrew; Ilya Sutskever; Mark Chen"}, {"ref_id": "b43", "title": "Improved denoising diffusion probabilistic models", "journal": "PMLR", "year": "2021", "authors": "Alexander Quinn; Nichol ; Prafulla Dhariwal"}, {"ref_id": "b44", "title": "Mystyle: A personalized generative prior", "journal": "", "year": "2022", "authors": "Yotam Nitzan; Kfir Aberman; Qiurui He; Orly Liba; Michal Yarom; Yossi Gandelsman; Inbar Mosseri; Yael Pritch; Daniel Cohen-Or"}, {"ref_id": "b45", "title": "Few-shot image generation via cross-domain correspondence", "journal": "", "year": "2021", "authors": "Utkarsh Ojha; Yijun Li; Jingwan Lu; Alexei A Efros; Yong Jae Lee; Eli Shechtman; Richard Zhang"}, {"ref_id": "b46", "title": "Styleclip: Text-driven manipulation of stylegan imagery", "journal": "", "year": "2021", "authors": "Or Patashnik; Zongze Wu; Eli Shechtman; Daniel Cohen-Or; Dani Lischinski"}, {"ref_id": "b47", "title": "Dreamfusion: Text-to-3d using 2d diffusion", "journal": "", "year": "2022", "authors": "Ben Poole; Ajay Jain; Jonathan T Barron; Ben Mildenhall"}, {"ref_id": "b48", "title": "Learn, imagine and create: Text-to-image generation from prior knowledge", "journal": "", "year": "2019", "authors": "Tingting Qiao; Jing Zhang; Duanqing Xu; Dacheng Tao"}, {"ref_id": "b49", "title": "Mirrorgan: Learning text-to-image generation by redescription", "journal": "", "year": "2019", "authors": "Tingting Qiao; Jing Zhang; Duanqing Xu; Dacheng Tao"}, {"ref_id": "b50", "title": "Learning transferable visual models from natural language supervision", "journal": "", "year": "2021", "authors": "Alec Radford; Jong Wook Kim; Chris Hallacy; Aditya Ramesh; Gabriel Goh; Sandhini Agarwal; Girish Sastry; Amanda Askell; Pamela Mishkin; Jack Clark"}, {"ref_id": "b51", "title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "J. Mach. Learn. Res", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; J Peter;  Liu"}, {"ref_id": "b52", "title": "Hierarchical text-conditional image generation with clip latents", "journal": "", "year": "2022", "authors": "Aditya Ramesh; Prafulla Dhariwal; Alex Nichol; Casey Chu; Mark Chen"}, {"ref_id": "b53", "title": "Zero-shot text-to-image generation", "journal": "PMLR", "year": "2021", "authors": "Aditya Ramesh; Mikhail Pavlov; Gabriel Goh; Scott Gray; Chelsea Voss; Alec Radford; Mark Chen; Ilya Sutskever"}, {"ref_id": "b54", "title": "Few-shot adaptation of generative adversarial networks. ArXiv, abs", "journal": "", "year": "2010", "authors": "Esther Robb; Wen-Sheng Chu; Abhishek Kumar; Jia-Bin Huang"}, {"ref_id": "b55", "title": "Pivotal tuning for latent-based editing of real images", "journal": "ACM Transactions on Graphics", "year": "", "authors": "Daniel Roich; Ron Mokady; H Amit; Daniel Bermano;  Cohen-Or"}, {"ref_id": "b56", "title": "High-resolution image synthesis with latent diffusion models", "journal": "", "year": "2021", "authors": "Robin Rombach; Andreas Blattmann; Dominik Lorenz; Patrick Esser; Bj\u00f6rn Ommer"}, {"ref_id": "b57", "title": "High-resolution image synthesis with latent diffusion models", "journal": "", "year": "2022", "authors": "Robin Rombach; Andreas Blattmann; Dominik Lorenz; Patrick Esser; Bj\u00f6rn Ommer"}, {"ref_id": "b58", "title": "Palette: Image-to-image diffusion models", "journal": "", "year": "2022", "authors": "Chitwan Saharia; William Chan; Huiwen Chang; Chris Lee; Jonathan Ho; Tim Salimans; David Fleet; Mohammad Norouzi"}, {"ref_id": "b59", "title": "", "journal": "", "year": "", "authors": "Chitwan Saharia; William Chan; Saurabh Saxena; Lala Li; Jay Whang; Emily Denton"}, {"ref_id": "b60", "title": "Photorealistic text-to-image diffusion models with deep language understanding", "journal": "", "year": "2022", "authors": "Burcu Ghasemipour; Sara Karagol Ayan; Rapha Mahdavi; Tim Gontijo-Lopes; Jonathan Salimans;  Ho; J David; Mohammad Fleet;  Norouzi"}, {"ref_id": "b61", "title": "Image superresolution via iterative refinement", "journal": "", "year": "2021", "authors": "Chitwan Saharia; Jonathan Ho; William Chan; Tim Salimans; J David; Mohammad Fleet;  Norouzi"}, {"ref_id": "b62", "title": "Deep unsupervised learning using nonequilibrium thermodynamics", "journal": "PMLR", "year": "2015", "authors": "Jascha Sohl-Dickstein; Eric Weiss; Niru Maheswaranathan; Surya Ganguli"}, {"ref_id": "b63", "title": "Denoising diffusion implicit models", "journal": "", "year": "2020", "authors": "Jiaming Song; Chenlin Meng; Stefano Ermon"}, {"ref_id": "b64", "title": "Generative modeling by estimating gradients of the data distribution", "journal": "", "year": "2019", "authors": "Yang Song; Stefano Ermon"}, {"ref_id": "b65", "title": "Improved techniques for training score-based generative models", "journal": "", "year": "2020", "authors": "Yang Song; Stefano Ermon"}, {"ref_id": "b66", "title": "Deep fusion generative adversarial networks for text-to-image synthesis", "journal": "", "year": "2020", "authors": "Ming Tao; Hao Tang; Songsong Wu; Nicu Sebe; Xiao-Yuan Jing; Fei Wu; Bingkun Bao;  Df-Gan"}, {"ref_id": "b67", "title": "Ref-NeRF: Structured view-dependent appearance for neural radiance fields. CVPR", "journal": "", "year": "2022", "authors": "Dor Verbin; Peter Hedman; Ben Mildenhall; Todd Zickler; Jonathan T Barron; Pratul P Srinivasan"}, {"ref_id": "b68", "title": "Minegan: Effective knowledge transfer from gans to target domains with few images", "journal": "", "year": "2020-06", "authors": "Yaxing Wang; Abel Gonzalez-Garcia; David Berga; Luis Herranz; Fahad Shahbaz Khan; Joost Van De Weijer"}, {"ref_id": "b69", "title": "Gp-gan: Towards realistic high-resolution image blending", "journal": "", "year": "2019", "authors": "Huikai Wu; Shuai Zheng; Junge Zhang; Kaiqi Huang"}, {"ref_id": "b70", "title": "Tedigan: Text-guided diverse face image generation and manipulation", "journal": "", "year": "2021", "authors": "Weihao Xia; Yujiu Yang; Jing-Hao Xue; Baoyuan Wu"}, {"ref_id": "b71", "title": "Scaling autoregressive models for content-rich text-to-image generation", "journal": "", "year": "2022", "authors": "Jiahui Yu; Yuanzhong Xu; Jing Yu Koh; Thang Luong; Gunjan Baid; Zirui Wang; Vijay Vasudevan; Alexander Ku; Yinfei Yang;  Burcu Karagol Ayan"}, {"ref_id": "b72", "title": "The unreasonable effectiveness of deep features as a perceptual metric", "journal": "", "year": "2018", "authors": "Richard Zhang; Phillip Isola; Alexei A Efros; Eli Shechtman; Oliver Wang"}, {"ref_id": "b73", "title": "Photographic text-to-image synthesis with a hierarchically-nested adversarial network", "journal": "", "year": "2018", "authors": "Zizhao Zhang; Yuanpu Xie; Lin Yang"}], "figures": [{"figure_label": "4", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 4 .4Figure 4. Comparisons with Textual Inversion [20] Given 4 input images (top row), we compare: DreamBooth Imagen (2nd row), DreamBooth Stable Diffusion (3rd row), Textual Inversion (bottom row). Output images were created with the following prompts (left to right): \"a [V] vase in the snow\", \"a [V] vase on the beach\", \"a [V] vase in the jungle\", \"a [V] vase with the Eiffel Tower in the background\". DreamBooth is stronger in both subject and prompt fidelity.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 5 .5Figure 5. Dataset. Example images for each subject in our proposed dataset.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 8 .8Figure 8. Novel view synthesis, art renditions, and property modifications. We are able to generate novel and meaningful images while faithfully preserving subject identity and essence. More applications and examples in the supplementary material.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 9 .9Figure 9. Failure modes. Given a rare prompted context the model might fail at generating the correct environment (a). It is possible for context and subject appearance to become entangled (b). Finally, it is possible for the model to overfit and generate images similar to the training set, especially if prompts reflect the original environment of the training set (c).", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 10 .10Figure 10. Prompts. Evaluation prompts for both objects and live subjects.", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 11 .11Figure 11. CLIP-I vs. DINO Metrics. The DreamBooth CLIP-I similarity to the reference image is lower than that of the Textual Inversion sample, even though the DreamBooth subject looks more similar to the reference subject. The DINO metric more closely tracks human evaluation of subject fidelity here.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Subject fidelity and prompt fidelity user preference.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Prior preservation loss (PPL) ablation displaying a prior preservation (PRES) metric, diversity metric (DIV) and subject and prompt fidelity metrics.", "figure_data": "MethodDINO \u2191 CLIP-I \u2191Correct Class0.7440.853No Class0.3030.607Wrong Class0.4540.728"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Effect of the number of input images on subject fidelity (DINO).", "figure_data": "Method12345Backpack 0.798 0.851 0.871 0.876 0.864Dog0.646 0.683 0.734 0.740 0.730Table 6. Effect of the number of input images on prompt fidelity(CLIP-T)."}], "formulas": [{"formula_id": "formula_0", "formula_text": "E x,c, ,t w t x \u03b8 (\u03b1 t x + \u03c3 t , c) \u2212 x 2 2 (1)", "formula_coordinates": [3.0, 351.29, 426.76, 193.82, 12.69]}, {"formula_id": "formula_1", "formula_text": "E x,c, , ,t [w t x \u03b8 (\u03b1 t x + \u03c3 t , c) \u2212 x 2 2 + \u03bbw t x \u03b8 (\u03b1 t x pr + \u03c3 t , c pr ) \u2212 x pr 2 2 ], (2)", "formula_coordinates": [5.0, 60.08, 179.96, 226.29, 28.87]}, {"formula_id": "formula_2", "formula_text": "E x,c, ,t w t x \u03b8 (\u03b1 t x + \u03c3 t , c) \u2212 x 2 2 (3)", "formula_coordinates": [13.0, 92.54, 216.26, 193.82, 12.69]}], "doi": ""}