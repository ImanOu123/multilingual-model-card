{"title": "", "authors": "Jake Topping; Francesco Di Giovanni; Benjamin P Chamberlain; Xiaowen Dong; Michael M Bronstein", "pub_date": "2022-11-12", "abstract": "Most graph neural networks (GNNs) use the message passing paradigm, in which node features are propagated on the input graph. Recent works pointed to the distortion of information flowing from distant nodes as a factor limiting the efficiency of message passing for tasks relying on long-distance interactions. This phenomenon, referred to as 'over-squashing', has been heuristically attributed to graph bottlenecks where the number of k-hop neighbors grows rapidly with k. We provide a precise description of the over-squashing phenomenon in GNNs and analyze how it arises from bottlenecks in the graph. For this purpose, we introduce a new edge-based combinatorial curvature and prove that negatively curved edges are responsible for the over-squashing issue. We also propose and experimentally test a curvature-based graph rewiring method to alleviate the over-squashing.", "sections": [{"heading": "INTRODUCTION", "text": "Figure 1: Top: evolution of curvature on a surface may reduce the bottleneck. Bottom: this paper shows how the same may be done on graphs to improve GNN performance. Blue/red shows negative/positive curvature.\nIn the past few years, deep learning on graphs and in particular graph neural networks (GNNs) (Sperduti, 1994;Goller & Kuchler, 1996;Sperduti & Starita, 1997;Frasconi et al., 1998;Gori et al., 2005;Scarselli et al., 2008;Bruna et al., 2014;Defferrard et al., 2016;Kipf & Welling, 2017;Gilmer et al., 2017) have become very popular in the machine learning community due to their ability to deal with broad classes of systems of relations and interactions.\nThe vast majority of GNNs follow the message passing paradigm (Gilmer et al., 2017), using learnable non-linear functions to diffuse information on the graph. Multiple popular GNN architectures such as GCN (Kipf & Welling, 2017) and GAT (Veli\u010dkovi\u0107 et al., 2018) can be posed as particular flavors of this scheme and considered instances of a more general framework of geometric deep learning (Bronstein et al., 2021). Some of the drawbacks of the message passing paradigm have now been identified and formalized, including the limits of expressive power (Xu et al., 2019;Morris et al., 2019;Maron et al., 2019) and the problem of over-smoothing (NT & Maehara, 2019;Oono & Suzuki, 2020). On the other hand, much less is known about the phenomenon of over-squashing, consisting in the distortion of messages being propagated from distant nodes. Alon & Yahav (2021) proposed rewiring the graph as a way of reducing the bottleneck, defined as those topological properties in the graph leading to over-squashing. This approach is in line with multiple other results e.g. using connectivity diffusion (Klicpera et al., 2019) as a preprocessing step to facilitate graph learning. Yet, the exact understanding Message passing neural networks (MPNNs). Assume that the graph G is equipped with node features X \u2208 R n\u00d7p0 where x i \u2208 R p0 is the feature vector at node i = 1, . . . , n = |V |. We denote by h ( ) i \u2208 R p the representation of node i at layer \u2265 0, with h (0) i = x i . Given a family of message functions \u03c8 : R p \u00d7 R p \u2192 R p and update functions \u03c6 : R p \u00d7 R p \u2192 R p +1 , we can write the ( + 1)-st layer output of a generic MPNN as follows (Gilmer et al., 2017):\nh ( +1) i = \u03c6 \uf8eb \uf8ed h ( ) i , n j=1\u00c2 ij \u03c8 (h ( ) i , h ( ) j ) \uf8f6 \uf8f8 .\n(1)\nHere we have used the augmented normalized adjacency matrix to propagate messages from each node to its neighbors, which simply leads to a degree normalization of the message functions \u03c8 . To avoid heavy notations the node features and representations are assumed to be scalar from now on; these assumptions simplify the discussion and the vector case leads to analogous results.", "publication_ref": ["b49", "b16", "b50", "b13", "b17", "b46", "b5", "b11", "b23", "b15", "b15", "b53", "b4", "b57", "b30", "b27", "b36", "b39", "b0", "b24", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "THE OVER-SQUASHING PROBLEM", "text": "Multiple recent papers observed that MPNNs tend to perform poorly in situations when the learned task requires long-range dependencies and at the same time the structure of the graph results in exponentially many long-range neighboring nodes. We say that a graph learning problem has long-range dependencies when the output of a MPNN depends on representations of distant nodes interacting with each other. If long-range dependencies are present, messages coming from nonadjacent nodes need to be propagated across the network without being too distorted. In many cases however (e.g. in 'small-world' graphs such as social networks), the size of the receptive field B r (i) grows exponentially with r. If this occurs, representations of exponentially many neighboring nodes need to be compressed into fixed-size vectors to propagate messages to node i, causing a phenomenon referred to as over-squashing of information (Alon & Yahav, 2021). In line with Alon & Yahav (2021), we refer to those structural properties of the graph that lead to over-squashing as a bottleneck 1 .\nSensitivity analysis. The hidden feature h\n( ) i = h ( )\ni (x 1 , . . . , x n ) computed by an MPNN with layers as in equation 1 is a differentiable function of the input node features {x 1 , . . . , x n } as long as the update and message functions \u03c6 and \u03c8 are differentiable. The over-squashing of information can then be understood in terms of one node representation h ( ) i failing to be affected by some input feature x s of node s at distance r from node i. Hence, we propose the Jacobian \u2202h (r) i /\u2202x s as an explicit and formal way of assessing the over-squashing effect 2 . Lemma 1. Assume an MPNN as in equation 1. Let i, s \u2208 V with s \u2208 S r+1 (i). If |\u2207\u03c6 | \u2264 \u03b1 and |\u2207\u03c8 | \u2264 \u03b2 for 0 \u2264 \u2264 r, then \u2202h\n(r+1) i \u2202x s \u2264 (\u03b1\u03b2) r+1 (\u00c2 r+1 ) is .(2)\nLemma 1 states that if \u03c6 and \u03c8 have bounded derivatives, then the propagation of messages is controlled by a suitable power of\u00c2. For example, if d G (i, s) = r + 1 and the sub-graph induced on B r+1 (i) is a binary tree, then (\u00c2 r+1 ) is = 2 \u22121 3 \u2212r , which gives an exponential decay of the node dependence on input features at distance r, as also heuristically argued by Alon & Yahav (2021).\nThe sensitivity analysis in Lemma 1 relates the over-squashing -as measured by the Jacobian of the node representations -to the graph topology via powers of the augmented normalized adjacency matrix. In the next section we explore this connection further by analyzing which local properties of the graph structure affect the right hand side in equation 2, hence causing the bottleneck. We will address this problem by introducing a new combinatorial notion of edge-based curvature and showing that negatively curved edges are those responsible for the over-squashing phenomenon.", "publication_ref": ["b0", "b0", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "GRAPH CURVATURE AND BOTTLENECK", "text": "A natural object in Riemannian geometry is the Ricci curvature, a bilinear form determining the geodesic dispersion, i.e. whether geodesics starting at nearby points with 'same' velocity remain parallel (Euclidean space), converge (spherical space), or diverge (hyperbolic space). To motivate the introduction of a Ricci curvature for graphs, we focus on these three cases. Consider two nodes i \u223c j and two edges starting at i and j respectively. In a discrete spherical geometry (Figure 2a), the edges would meet at k to form a triangle (complete graph). In a discrete Euclidean geometry (Figure 2b), the edges would stay parallel and form a 4-cycle based at i \u223c j (orthogonal grid). Finally, in a discrete hyperbolic geometry (Figure 2c), the mutual distance of the edge endpoints would have grown compared to that of i and j (tree). Therefore, a Ricci curvature for graphs should provide us with more sophisticated tools than the degree to analyze the neighborhood of an edge. Curvatures on graphs. The main examples of edge-based curvature are the Forman curvature F (i, j) (Forman, 2003) and the Ollivier curvature \u03ba(i, j) in Ollivier (2007;2009) (see Appendix). While F (i, j) is given in terms of combinatorial quantities (Sreejith et al., 2016), results are scarce and the definition is biased towards negative curvature. The theory on \u03ba(i, j) instead is richer (Lin et al., 2011;M\u00fcnch, 2019) but its formulation makes it hard to control local quantities.\nBalanced Forman curvature. We propose a new curvature to address the shortcomings of the existing candidates. We use the following definitions to describe the neighborhood of an edge i \u223c j and we refer to the Appendix for a more complete discussion:\n(i) \u2206 (i, j) := S 1 (i) \u2229 S 1 (j) are the triangles based at i \u223c j. (ii) i (i, j) := {k \u2208 S 1 (i) \\ S 1 (j), k = j : \u2203w \u2208 (S 1 (k) \u2229 S 1 (j)) \\ S 1 (i)}\nare the neighbors of i forming a 4-cycle based at the edge i \u223c j without diagonals inside. (iii) \u03b3 max (i, j) is the maximal number of 4\u2212cycles based at i \u223c j traversing a common node (see Definition 4).\nIn line with the discussion about geodesic dispersion, one expects \u2206 to be related to positive curvature (complete graph), i to zero curvature (grid), and the remaining outgoing edges to negative curvature (tree). Our new curvature formulation reflects such an intuition and recovers the expected results in the classical cases. In the example in Figure 3 we have 0 (0, 1) = {2, 3} while 1 (0, 1) = {5}, both without 4,6 because of the triangle 1-6-0. The degeneracy factor \u03b3 max (0, 1) = 2, as there exist two 4-cycles passing through node 5.\nDefinition 1 (Balanced Forman curvature). For any edge i \u223c j in a simple, unweighted graph G, we let Ric(i, j) be zero if min{d i , d j } = 1 and otherwise\nRic(i, j) := 2 d i + 2 d j \u2212 2 + 2 | \u2206 (i, j)| max{d i , d j } + | \u2206 (i, j)| min{d i , d j } + (\u03b3 max ) \u22121 max{d i , d j } (| i | + | j |), (3)\nwhere the last term is set to be zero\nif | i | (and hence | j |) is zero. In particular Ric(i, j) > \u22122.\nThe curvature is negative when i \u223c j behaves as a bridge between S 1 (i) and S 1 (j), while it is positive when S 1 (i) and S 1 (j) stay connected after removing i \u223c j. We refer to Ric as Balanced Forman curvature. We can relate the Balanced Forman curvature to the Jacobian of hidden features, while also extending many results valid for the Ollivier curvature \u03ba(i, j) thanks to our next theorem.\nTheorem 2. Given an unweighted graph G, for any edge i \u223c j we have \u03ba(i, j) \u2265 Ric(i, j).", "publication_ref": ["b12", "b37", "b38", "b51", "b25", "b31"], "figure_ref": ["fig_0", "fig_0", "fig_0"], "table_ref": []}, {"heading": "Graph G RicG", "text": "Cycles  Appendix).We also note that the computational complexity for \u03ba scales as O(|E|d 3 max ), while for our Ric we have O(|E|d 2 max ), with d max the maximal degree. From Theorem 2 and Paeng (2012), we find:\nC3 3 2 C4 1 C n\u22655 0 Complete Kn n n\u22121 Grid Gn 0 Tree Tr 4 r+1 \u2212 2\nCorollary 3. If Ric(i, j) \u2265 k > 0 for any edge i \u223c j, then diam(G) \u2264 2 k .\nTherefore, controlling the curvature everywhere grants an upper bound on how 'long' the long-range dependencies can be. Moreover, we can also adapt the volume growth results in Paeng (2012) showing that positive curvature everywhere prevents a too fast expansion of the r-hop for r sufficiently large.\nCurvature and over-squashing. Thanks to our new combinatorial curvature and the sensitivity analysis in Lemma 1, we are able to relate local curvature properties to the Jacobian of the node representations. This leads to one of the main results of this paper: edges with high negative curvature are those causing the graph bottleneck and thus leading to the over-squashing phenomenon:\nTheorem 4. Consider a MPNN as in equation 1. Let i \u223c j with d i \u2264 d j and assume that:\n(i) |\u2207\u03c6 | \u2264 \u03b1 and |\u2207\u03c8 | \u2264 \u03b2 for each 0 \u2264 \u2264 L \u2212 1, with L \u2265 2 the depth of the MPNN.\n(ii) There exists \u03b4 s.t. 0 < \u03b4 < (max{d i , d j }) \u2212 1 2 , \u03b4 < \u03b3 \u22121 max , and Ric(i, j) \u2264 \u22122 + \u03b4.\nThen there exists Q j \u2282 S 2 (i) satisfying |Q j | > \u03b4 \u22121 and for 0 \u2264 0 \u2264 L \u2212 2 we have\n1 |Q j | k\u2208Qj \u2202h ( 0 +2) k \u2202h ( 0) i < (\u03b1\u03b2) 2 \u03b4 1 4 .(4)\nCondition (i) is always satisfied and allows us to control the message passing functions. The requirement (ii) instead means that the curvature of (i, j) is negative enough when compared to the degrees of i and j (recall that Ric(i, j) > \u22122). The further condition on \u03b3 max is to avoid pathological cases where we have a large number of degenerate 4-cycles passing through the same three nodes.\nTo understand the conclusions in Theorem 4, let us fix 0 = 0. The equation 4 shows that negatively curved edges are the ones causing bottlenecks, interpreted as how the graph topology prevents a representation h\n(2)\nk to be affected by non-adjacent features h\n(0) i = x i .\nTheorem 4 implies that if we have a negatively curved edge as in (ii), then there exist a large number of nodes k such that GNNson average -struggle to propagate messages from i to k in two layers despite these nodes k being at distance 2 from i. In this case the over-squashing occurs as measured by the Jacobian in equation 4 and hence the propagation of information suffers. If the task at hand has long-range dependencies, then the over-squashing caused by the negatively curved edges may compromise the performance.\nBottleneck via Cheeger constant. We now relate the previous discussion about bottlenecks and curvature to spectral properties of the graph. In particular, since the spectral gap of a graph can be interpreted as a topological obstruction to the graph being partitioned into two communities, we argue below that this quantity is related to the graph bottleneck and should hence be controllable by the curvature. We start with an intuitive explanation: suppose we are given a graph G with two communities separated by few edges. In this case, we see that the graph can be easily disconnected. This property is encoded in the classical notion of the Cheeger constant (Chung & Graham, 1997) \nh G := min S\u2282V h S , h S := |\u2202S| min{vol(S), vol(V \\ S)}(5)\nwhere \u2202S = {(i, j) : i \u2208 S, j \u2208 V \\ S} and vol(S) = i\u2208S d i . The main result about the Cheeger constant is the Cheeger inequality (Cheeger, 2015;Chung & Graham, 1997):\n2h G \u2265 \u03bb 1 \u2265 h 2 G 2 (6)\nwhere \u03bb 1 is the first non-zero eigenvalue of the normalized graph Laplacian, often referred to as the spectral gap. A graph with two tightly connected communities (S and V \\ S :=S) and few inter-community edges has a small Cheeger constant h G . For nodes in different communities to interact with each other, all messages need to go through the same few bridges hence leading to the over-squashing of information (a similar intuition was explored in Alon & Yahav (2021)). Therefore, h G can be interpreted as a rough measure of graph 'bottleneckedness', in the sense that the smaller its value, the more likely the over-squashing is to occur across inter-community edges. Since Theorem 4 implies that negatively curved edges induce the bottleneck, we expect a relationship between h G and the curvature of the graph. The next proposition follows from Theorem 2 and Lin et al. (2011):\nProposition 5. If Ric(i, j) \u2265 k > 0 for all i \u223c j, then \u03bb 1 /2 \u2265 h G \u2265 k 2 .\nTherefore, a positive lower bound on the curvature gives us a control on h G and hence on the spectral gap of the graph. In the next section, we show that diffusion-based graph-rewiring methods might fail to significantly alter h G and hence correct the graph bottleneck potentially induced by inter-community edges. This will lead us to propose an alternative curvature-based graph rewiring.", "publication_ref": ["b40", "b9", "b7", "b9", "b0", "b25"], "figure_ref": [], "table_ref": []}, {"heading": "CURVATURE-BASED REWIRING METHODS", "text": "The traditional paradigm of message passing graph neural networks assumes that messages are propagated on the input graph (Gilmer et al., 2017). More recently, there is a trend to decouple the input graph from the graph used for information propagation. This can take the form of graph subsampling or resampling to deal with scalability (Hamilton et al., 2017) or topological noise (Zhang et al., 2019), using larger motif-based (Monti et al., 2018) or multi-hop filters (Rossi et al., 2020), or changing the graph either as a preprocessing step (Klicpera et al., 2019;Alon & Yahav, 2021) or adaptively for the downstream task (Wang et al., 2019;Kazi et al., 2020). Such methods are often generically referred to as graph rewiring.\nIn the context of this paper, we assume that graph rewiring attempts to produce a new graph G = (V, E ) with a different edge structure that reduces the bottleneck and hence potentially alleviates the over-squashing of information. We propose a method that leverages the graph curvature to guide the rewiring steps in a surgical way by modifying the negatively-curved edges, so to decrease the bottleneck without significantly compromising the statistical properties of the input graph. We also rigorously show that a random-walk based rewiring method might generally fail to obtain an edge set E with a significant improvement in its bottleneckedness as measured by the Cheeger constant.\nCurvature-based graph rewiring. Since according to Theorem 4 negatively curved edges induce a bottleneck and are hence responsible for over-squashing, a curvature-based rewiring method should attempt to alleviate a graph's strongly-negatively curved edges. To this end we implement a simple rewiring method called Stochastic Discrete Ricci Flow (SDRF), described in Algorithm 1.\nAlgorithm 1: Stochastic Discrete Ricci Flow (SDRF) Input: graph G, temperature \u03c4 > 0, max number of iterations, optional Ric upper-bound C + Repeat 1)\nFor edge i \u223c j with minimal Ricci curvature Ric(i, j): Calculate vector x where x kl = Ric kl (i, j) \u2212 Ric(i, j), the improvement to Ric(i, j) from adding edge k \u223c l where k \u2208 B 1 (i), l \u2208 B 1 (j); Sample index k, l with probability softmax(\u03c4 x) kl and add edge k \u223c l to G. 2) Remove edge i \u223c j with maximal Ricci curvature Ric(i, j) if Ric(i, j) > C + . Until convergence, or max iterations reached; At each iteration this preprocessing step adds an edge to 'support' the graph's most negatively curved edge, and then removes the most positively curved edge. The requirement on the added edge k \u223c l that k \u2208 B 1 (i) and l \u2208 B 1 (j) ensures that we're adding either an extra 3-or 4-cycle around the negative edge i \u223c j so that this is a local modification. The graph edit distance between the original and preprocessed graph is bounded above by 2 \u00d7 the max number of iterations. The temperature \u03c4 determines how stochastic the edge addition is, with \u03c4 = \u221e being fully deterministic (the best edge is always added). At each step we remove the edge with most positive curvature to balance the distributions of curvature and node degrees. We use Balanced Forman curvature as in equation 3 for Ric(i, j). C + can be chosen to stop the method skewing the curvature distribution negative, including C + = \u221e to not remove any edges. The method is inspired by the continuous (backwards) Ricci flow with the aim of homogenizing edge curvatures. This is different from more direct extensions of Ricci flow on graphs where it becomes increasingly expensive to propagate messages across negatively curved edges (as in other applications such as Ni et al. (2019)). An example alongside its continuous analogue can be seen in Figure 1.\nCan random-walk based rewiring address bottlenecks? A good way of understanding the effectiveness of SDRF in reducing the graph bottleneck is through comparison with random-walk based rewiring strategies. Recall that, as argued in Section 3, the Cheeger constant h G of a graph constitutes a rough measure of its bottleneckedness as induced by the inter-community edges (a small h G is indicative of a bottleneck). Suppose we are given a graph G with a small h G and wish to rewire it into a graph G with a significantly improved Cheeger constant in order to reduce the inter-community bottleneck. A random-walk based rewiring method such as DIGL (Klicpera et al., 2019) acts by smoothing out the graph adjacency and hence tends to promote connections among nodes at short diffusion distance (Coifman & Lafon, 2006). Accordingly, such a rewiring method might fail to correct structural features like the bottleneck, which is instead more prominent for nodes that are at long diffusion distance. 3 To emphasize this point, we consider a classic example: given \u03b1 \u2208 (0, 1), the Personalized Page Rank (PPR) matrix is defined by (Brin & Page, 1998) as\nR \u03b1 := \u221e k=0 \u03b8 P P R k (D \u22121 A) k = \u03b1 \u221e k=0 (1 \u2212 \u03b1)(D \u22121 A) k .\nAssume that we rewire the graph using R \u03b1 as in Klicpera et al. (2019) with the PPR kernel, meaning that we replace the given adjacency A with R \u03b1 . Since R \u03b1 is stochastic, the new Cheeger constant of the rewired graph can be computed as\nh S,\u03b1 = |\u2202S| \u03b1 vol \u03b1 (S) \u2261 1 |S| i\u2208S j\u2208S (R \u03b1 ) ij .\nBy applying (Chung, 2007, Lemma 5), we show that we cannot improve the Cheeger constant (and hence the bottleneck) arbitrarily well (in contrast to a curvature-based approach). We refer to Proposition 17 and Remark 18 in Appendix E for results that are more tailored to the actual strategy adopted in Klicpera et al. (2019) where we also take into account the effect of the sparsification. The property that the new Cheeger constant is directly controlled by the old one stems from the fact that a random-walk approach like in Klicpera et al. ( 2019) is meant to act more relevantly on intra-community edges rather than inter-community edges because it prioritizes short diffusion distance nodes. This is also why this method performs well on high-homophily datasets, as discussed below. In particular, for a fixed \u03b1 \u2208 (0, 1), the bound in Theorem 6 can be very small. As a specific example, consider two complete graphs K n joined by one bridge. Then h G = (n(n \u2212 1) + 1) \u22121 , which means that the bound on the right hand side is O(n \u22122 ).\nTheorem 6 implies that a diffusion approach such as DIGL might fail to yield a new edge set E with a sufficiently improved bottleneck. By contrast, from Theorem 4 and Proposition 5, we deduce that a curvature-based rewiring method such as SDRF properly addresses the edges that cause the bottleneck.\nGraph structure preservation. Although a graph-rewiring approach aims at providing a new edge set E potentially more beneficial for the given learning task, it is still desirable to control how far E is from E. In this regard, we note that a curvature-based rewiring is surgical in nature and hence more likely to preserve the structure of the input graph better than a random-walk based approach. Consider, for example, that we are given \u03c1 > 0 and wish to rewire the graph such that the new edge set E is within graph-edit distance \u03c1 from the original E. Theorem 4 tells us how to do the rewiring under such constraints in order to best address the over-squashing: the topological modifications need to be localized around the most negatively-curved edges. We can do this with SDRF, with the maximum number of iterations set to \u03c1/2. Secondly, we also point out that Ric(i, j) < \u22122 + \u03b4 implies that min{d i , d j } > 2/\u03b4. Therefore, if we mostly modify the edge set at those nodes i, j joined by an edge with large negative curvature, then we are perturbing nodes with high degree where such a change is relatively insignificant, and thus overall statistical properties of the rewired graph such as degree distribution are likely to be better preserved. Moreover, graph convolutional networks tend to be more stable to perturbations of high degree nodes (Z\u00fcgner et al., 2020;Kenlay et al., 2021), making curvature-based rewiring more suitable for the downstream learning tasks with popular GNN architectures.\nHomophily and bottleneck. As a final remark, note that the graph rewiring techniques considered in this paper (both DIGL and SDRF) are based purely on the topological structure of the graph and completely agnostic to the node features and to whether the dataset is homophilic (adjacent nodes have same labels) or heterophilic. Nonetheless, the different nature of these rewiring methods allows us to draw a few broad conclusions about their suitability in each of these settings. A random-walk  approach such as DIGL tends to improve the connectivity among nodes that are at short diffusion distance; since for a high-homophily dataset these nodes often share the same label, a rewiring method like DIGL is likely to act as graph denoising and yield improved performance. On the other hand, for datasets with low homophily, nodes at short diffusion distance are more likely to belong to different label classes, meaning that a diffusion-based rewiring might inject noise and hence compromise performance as also noted in Klicpera et al. (2019). Conversely, on a low-homophily dataset, a curvature-based approach as SDRF modifies the edge set mainly around the most negatively curved edges, meaning that it decreases the bottleneck without significantly increasing the connectivity among nodes with different labels. In fact, long-range dependencies are often more relevant in low-homophily settings, where nodes sharing the same labels are in general not neighbors. This observation is largely confirmed by experimental results reported in the next section.", "publication_ref": ["b15", "b19", "b58", "b29", "b43", "b24", "b0", "b54", "b21", "b35", "b24", "b10", "b3", "b24", "b24", "b59", "b22", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENTAL RESULTS", "text": "Experiment setup. To demonstrate the theoretical results in this paper we ran a suite of semisupervised node classification tasks comparing our curvature-based rewiring method SDRF to DIGL from Klicpera et al. (2019) (GDC with the PPR kernel) and the +FA method from Alon & Yahav (2021), where the last layer of the GNN is made fully connected. We evaluate the methods on nine datasets: Cornell, Texas and Wisconsin from the WebKB dataset 4 ; Chameleon and Squirrel (Rozemberczki et al., 2021) along with Actor (Tang et al., 2009); and Cora (McCallum et al., 2000), Citeseer (Sen et al., 2008) and Pubmed (Namata et al., 2012). Statistics for these datasets can be found in Appendix F.1. Our base model is a GCN (Kipf & Welling, 2017). Following Shchur et al. (2018) and Klicpera et al. (2019) we optimized hyperparameters for all dataset-preprocessing combinations separately by random search over 100 data splits. Results are reported as average accuracies on a test set used once with 95% confidence intervals calculated by bootstrapping. We compared the performance on graphs with no preprocessing, making the graph undirected, +FA, DIGL, SDRF, and the given combinations. For DIGL + Undirected we symmetrized the diffusion matrix as in Klicpera et al. (2019), and for SDRF + Undirected we made the graph undirected before applying SDRF. For more details on the experiments and datasets see Appendix F, and for the hyperparameters used for each model and preprocessing see Appendix F.4.", "publication_ref": ["b24", "b0", "b52", "b28", "b47", "b32", "b48", "b24", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Node classification results.", "text": "Table 2 shows the results of the experiments. As well as reporting results we give a measure of homophily H(G) proposed by Pei et al. (2019) (restated in Appendix F, equation 21), by which we can see our experiment set is diverse with respect to homophily. We see that SDRF improves upon the baseline in all cases, and that the largest improvements are seen on the low-homophily datasets. We also see that SDRF matches or outperforms DIGL and +FA on most datasets, supporting our argument that curvature-based rewiring is a viable candidate for improving GNN performance.\nGraph topology change. Furthermore, SDRF preserves the graph topology to a far greater extent than DIGL due to its surgical nature. Table 3 shows the number of edges added / removed by the two preprocessings on each dataset as a percentage of the original number of edges. We see that for optimal performance DIGL makes the graph much denser, which significantly affects the node degrees and may negatively impact the time and space complexity of the downstream GNN, which typically are O(E). In comparison, SDRF adds and removes a similar number of edges and approximately preserves the degree distribution. The effect on the full degree distribution for three  ", "publication_ref": ["b41"], "figure_ref": [], "table_ref": ["tab_2", "tab_4"]}, {"heading": "CONCLUSION", "text": "In this paper, we studied the graph bottleneck and the over-squashing phenomena limiting the performance of message passing graph neural networks from a geometric perspective. We started with a Jacobian approach to determine how the over-squashing phenomenon is dictated by the graph topology as in equation 2. We then investigated further how the topology induces the bottleneck and hence causes over-squashing. We introduced a new notion of edge-based Ricci curvature called Balanced Forman curvature, relating it to the classical Ollivier curvature (Theorem 2). We then proved in Theorem 4 that negatively-curved edges are responsible for over-squashing, calling for a possibility of curvature-based rewiring of the graph in order to improve its bottleneckedness. We show one such possibility (Algorithm 1), inspired by the classical Ricci flow and comment on the advantages of surgical method such as this. We show both theoretically and experimentally that the proposed method can be advantageous compared to a diffusion-based rewiring approach, opening the door for curvature-based rewiring methods for improving GNN performance going forward.\nLimitations and future directions. Our paper establishes a geometric perspective on the graph bottleneck and over-squashing, providing new tools to study and cope with these phenomena. One limitation of our work is that the theoretical results presented here do not currently extend to multigraphs. In addition, the current methodology is agnostic to information beyond the graph topology, such as node features. In future works, we will develop a notion of the curvature and the corresponding rewiring method that can take into account such information.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "APPENDIX", "text": "The Appendix is structured as follows:\n(i) In Appendix A we prove Lemma 1 and a side-result about the role of self-loops. We also summarize how to extend the analysis in Lemma 1 to message passing models with sum aggregations (not average), meaning those architectures where we do not normalize the adjacency matrix.\n(ii) In Appendix B we introduce and describe different quantities we use to characterize the neighbourhood of a given edge i \u223c j. These objects are all essential to studying the new notion of balanced Forman curvature. The focus is on how we can distinguish 4-cycles in a computationally tractable way without losing too much accuracy.\n(iii) In Appendix C we provide a brief literature review about existing curvature candidates. In particular, we report the definitions of (modified) Ollivier curvature and Forman curvature.\n(iv) In Appendix D we prove the statements in Section 3, i.e. Theorem 2, Corollary 3, Theorem 4 and Proposition 5. We also comment on the role of the assumptions and compare the bound in Theorem 2 with the existing literature. Finally, we relate the classical notion of betweenness centrality to the over-squashing effect and the negatively curved edges in a graph.\n(v) In Appendix E we prove the results in Section 4, namely Theorem 6 and an analogous result.\n(vi) In Appendix F we describe more fully the experiments from Section 5, including a full analysis on degree distribution (Appendix F.2) and the hyperparameters used in our experiments (Appendix F.4).\n(vii) In Appendix G we comment on hardware specifications.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A PROOFS OF RESULTS IN SECTION 2", "text": "Lemma 1. Assume an MPNN as in equation 1. Let i, s \u2208 V with s \u2208 S r+1 (i). If |\u2207\u03c6 | \u2264 \u03b1 and\n|\u2207\u03c8 | \u2264 \u03b2 for 0 \u2264 \u2264 r, then \u2202h (r+1) i \u2202x s \u2264 (\u03b1\u03b2) r+1 (\u00c2 r+1 ) is .(2)\nProof. Let i \u2208 V and s \u2208 S r+1 (i). We recall that to ease the notations we assume that node features and hidden representations are scalar. The proof in the more general higher-dimensional case follows without any modification. We compute\n\u2202h (r+1) i \u2202x s = \u2202 1 \u03c6 r (. . .)\u2202 xs h (r) i + \u2202 2 \u03c6 r (. . .) n jr=1\u00e2 ijr \u2202 1 \u03c8 r (h (r) i , h (r) jr )\u2202 xs h (r) i + \u2202 2 \u03c8 r (h (r) i , h (r) jr )\u2202 xs h (r) jr .\nWe can iterate the computation above and see that the right hand side can be expanded as\n\u2202h (r+1) i \u2202x s = jr,...,j0 kr\u2208{i,jr} \u2022 \u2022 \u2022 k1\u2208{i,jr,...,j1}\u00e2 ijr\u00e2krjr\u22121 . . .\u00e2 k1j0 Z ijrkrjr\u22121...k1j0 (X)\u2202 xs h (0) j0 ,\nfor some functions Z ijr...k1j0 of the input features obtained as products of r + 1 partial derivatives of the maps \u03c6 and r + 1 partial derivatives of the maps \u03c8 . Since H (0) = X, we have\n\u2202 xs h (0) j0 = \u03b4 j0s\nmeaning that the previous sum becomes \u2202h (r+1) i \u2202x s = jr,...,j1 kr\u2208{i,jr}", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "\u2022 \u2022 \u2022", "text": "k1\u2208{i,jr,...,j1}\u00e2 ijr\u00e2krjr\u22121 . . .\u00e2 k1s Z ijrkrjr\u22121...k1s (X).\nSince d G (i, s) = r + 1, the only non-vanishing terms in the sum above are the minimal walks from i to s. In fact, if there existed a different choice of coefficients yielding a non-zero term then we would find a walk joining i to s of length lesser than r + 1, which is in contradiction with the definition of geodesic distance. Since Z i...s (X) is a product of r + 1-partial derivatives of the aggregation and update maps and by assumption their gradients are bounded by \u03b1 and \u03b2 respectively, we conclude that \u2202h\n(r+1) i \u2202x s \u2264 (\u03b1\u03b2) r+1 jr,...,j1\u00e2 ijr\u00e2jrjr\u22121 . . .\u00e2 j1s = \u03b1 r+1 (\u00c2 r+1 ) is\nwhich completes the proof of the Lemma.\nAs a byproduct of this analysis, we can also provide a rigorous motivation for the role of self-loops in GNNs (see Appendix for details):\nCorollary 7. If h ( +1) i = j\u223ci \u03c8 (h ( ) j ), then h ( +1) i\nonly depends on nodes that can be reached via walks of length exactly + 1. By adding self-loops, the GNN also takes into account nodes that can be reached via walks of length r \u2264 + 1.\nProof. If h ( +1) i = j\u223ci \u03c8 (h ( ) j ) for each \u2208 [0, L \u2212 1]\n, then we can argue as in the proof of Lemma 1 and find \u2202h\n( +1) i \u2202x s = j ,...,j1 a ij . . . a j1s \u03c8 (h ( ) j ) \u2022 \u2022 \u2022 \u03c8 0 (x s ).\nThe combinatorial coefficient a ij . . . a j1s is non-zero iff there exists a walk from i to s of length exactly +1, since we are not taking into account the contribution coming from self-loops. Conversely, if each term a ij was replaced by\u00e2 ij then we would find that\u00e2 ij . . .\u00e2 j1s is non-zero iff there exists a walk from i to s of length at most r + 1, since the diagonal entries are now positive.\nRemark 8. As a specific instance of Corollary 7, we note that if we do not include self-loops in the adjacency matrix, then the output of a 2-layer simplified graph neural network at node i is independent of the features of neighbours k that do not form a triangle with i. Once again here the dependence is precisely measured via the Jacobian of the hidden features with respect to the input features.\nRemark 9. We note that the role of self-loops has also implicitly been noted in Xu et al. (2018) where the analysis of the Jacobian of node representations on the graph augmented with self-loops has been related to lazy random-walks.", "publication_ref": ["b56"], "figure_ref": [], "table_ref": []}, {"heading": "GNNs with different aggregations", "text": "We note that similar conclusions extend to message passing architectures where the aggregations are sums and not averages meaning that we take the augmented adjacency without normalizing by the degree matrices. Consistently with Lemma 1, we restrict to the setting where features and node representations at each layer are scalars to make the discussion simpler. In line with the Xu et al. (2018) we consider a GNN-model of the form h\n( +1) i = ReLU \uf8eb \uf8ed j\u2208\u00d1i h ( ) j w \uf8f6 \uf8f8 .\nNote that the augmented neighbourhood\u00d1 i is defined as N i \u222a {i}. Differently from the setting of Theorem 1 in Xu et al. (2018), the aggregation here is not an average but a simple sum. Let us now take nodes i and s such that s \u2208 S r+1 (i) as in the statement of Lemma 1. In this case, instead of simply considering the quantity |\u2202h \nJ r+1 (i, s) := \u2202h (r+1) i \u2202xs k \u2202h (r+1) i \u2202x k\nThis of course represents now a relative importance of feature x s on the representation of node i at layer r + 1. If -similarly to Theorem 1 in Xu et al. (2018) -we assume that all paths in the computational graph of the model are activated with the same probability, then we obtain that on average\nJ r+1 (i, s) =\u00c3 r+1 is k\u00c3 r+1 ik \u2264\u00c3 r+1 is Vol(B r+1 (i)) ,\nwhere\u00c3 = A + I and vol(S) = j\u2208S d j . In particular, we again find that if we have a tree structure, then the right hand side decays exponentially as 2 \u2212(r+1) .", "publication_ref": ["b56", "b56", "b56"], "figure_ref": [], "table_ref": []}, {"heading": "B PRELIMINARY ANALYSIS OF AN EDGE-NEIGHBORHOOD", "text": "Given an edge i \u223c j, we introduce the sets below:\n(i) \u2206 (i, j) := S 1 (i) \u2229 S 1 (j)\n, the number of triangles based at the edge i \u223c j.\n(ii) i (i, j) := {k \u2208 S 1 (i) \\ S 1 (j), k = j : \u2203w \u2208 (S 1 (k) \u2229 S 1 (j)) \\ S 1 (i)}, the number of nodes k \u2208 S 1 (i) forming a 4-cycle based at i \u223c j without diagonals inside. (iii) Q i (j) := S 1 (i) \\ ({j} \u222a \u2206 (i, j) \u222a i (i, j))\n, simply the complement of the neighbours of i with respect to the sets introduced in (i) and (ii) once we also exclude j.\nIn the following we simply write \u2206 , i and Q i when the edge i \u223c j is clear from the context.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "4-cycle contributions.", "text": "In general the sets i and j may differ. This may occur when there exists a node k belonging to i that admits multiple solutions w as in the definition of i . This feature needs to be taken into account when comparing Ollivier's Ricci curvature to the new notion we present below. We first introduce the following class to ease the notations. Definition 2. For any simple, undirected graph G = (V, E), if U \u2282 V , then we set\nD(U ) := {\u03d5 : U \u2192 V, |U | = |\u03d5(U )|, (z, \u03d5(z)) \u2208 E, \u2200z \u2208 U } .\nWe note that any \u03d5 \u2208 D(U ) is injective.\nWe may now define a quantity which measures the maximal number of 1-1 pairings that can be performed from i to j . Definition 3. For any edge i \u223c j we let\nm (i, j) := max |U | : U \u2282 i , \u2203\u03d5 : U \u2192 j , \u03d5 \u2208 D(U ) .\nWe often simply write m . While the quantity m plays a role in the derivation of the Ollivier curvature of i \u223c j it is not computationally-friendly, as to determine m we need to identify and distinguish all possible 4-cycles based at i \u223c j and then choose a maximal pairing map. Accordingly, we consider a looser term which is easier to compute: Definition 4. For any pair of adjacent nodes i \u223c j we define\n\u03b3 max (i, j) := max max k\u2208 i {(A k \u2022 (A j \u2212 A i A j )) \u2212 1}, max w\u2208 j {(A w \u2022 (A i \u2212 A j A i )) \u2212 1} ,\nwhere A s denotes the s-th row of the adjacency matrix. We usually simply write \u03b3 max .\nRemark 10. We note that given k \u2208 S 1 (i) \\ S 1 (j) the term (A k \u2022 (A j \u2212 A i A j )) \u2212 1 yields the number of nodes w forming a 4-cycle of the form i \u223c k \u223c w \u223c j \u223c i with no diagonals inside. The value \u03b3 max measures the maximal degeneracy of edges forming 4-cycles, meaning that it is equal to 1 iff for each k \u2208 i there exists a unique node w \u2208 j such that i \u223c k \u223c w \u223c j \u223c i is a 4-cycle.\nWe now end the discussion about 4-cycle contributions by proving the following inequality, which allows us to avoid to compute directly the term m up to giving up some accuracy. Lemma 11. For any edge i \u223c j we have\n| m | \u2265 max{| i |, | j |} \u03b3 max .\nProof. The proof is based on a combinatorial argument. Let i = {k 1 , . . . , k r } and let m = {k 1 , . . . , k }, with < r. By definition there exists \u03d5 : {k 1 , . . . , k } \u2192 {w 1 , . . . , w }, with k i \u223c w i and w i \u2208 j , for 1 \u2264 i \u2264 . Given k \u2208 i \\ {k 1 , . . . , k }, then there are no w \u2208 j \\ {w 1 , . . . , w } such that k \u223c w, otherwise we could extend \u03d5 by setting k \u2192 \u03d5(k) := w and we would then get | m | = + 1. Accordingly, we have\ns=1 (A ws \u2022 (A i \u2212 A j A i )) \u2212 1) \u2265 | i |, which implies \u03b3 max | m | \u2261 \u03b3 max \u2265 s=1 (A ws \u2022 (A i \u2212 A j A i )) \u2212 1) \u2265 | i |.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C EXISTING CURVATURE CANDIDATES", "text": "Ollivier Ricci curvature For i \u2208 V and \u03b1 \u2208 [0, 1) we define a probability measure on B 1 (i) by:\n\u00b5 \u03b1 i : j \u2192 \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 \u03b1, j = i 1\u2212\u03b1 di , j \u2208 S 1 (i), 0, otherwise.\nBefore we introduce the Ollivier curvature, we recall that the transportation distance between two finitely supported probability measures as above can be computed as\nW 1 (\u00b5 \u03b1 i , \u00b5 \u03b1 j ) := inf M k\u2208S1(i) w\u2208S1(j) M kw d G (k, w),\nwhere d G (\u2022, \u2022) is the geodesic distance on the graph and the infimum is taken over all matrices M satisfying the marginal constraints:\nk\u2208S1(i) M kw = \u00b5 \u03b1 j (w), w\u2208S1(j) M kw = \u00b5 \u03b1 i (k).\nWe are now ready to define the Ollivier Ricci curvature: the formulation below is due to Lin et al. (2011). Definition 5. Given i \u223c j we define the \u03b1-Ollivier curvature by\n\u03ba \u03b1 (i, j) := 1 \u2212 W 1 (\u00b5 \u03b1 i , \u00b5 \u03b1 j ).(7)\nSince \u03ba \u03b1 (1 \u2212 \u03b1) \u22121 is increasing and bounded the quantity below is well-defined:\n\u03ba(i, j) := lim \u03b1\u21921 1 \u2212 W 1 (\u00b5 \u03b1 i , \u00b5 \u03b1 j ) 1 \u2212 \u03b1 .(8)\nForman Ricci curvature In the following we report a formula for the augmented Forman Ricci curvature on unweighted graphs Samal et al. (2018). We also note that Forman curvature on graphs has also been studied in Sreejith et al. (2016); Weber et al. (2018). Definition 6. For any edge i \u223c j the augmented Forman curvature is given by\nF (i, j) := 4 \u2212 d i \u2212 d j + 3| \u2206 |.\nWe note that such formulation of curvature does not distinguish contributions coming from 4-cycles.\nIn fact, for the orthogonal grid with degree d \u2265 4, Forman Ricci curvature is equal to 2(2 \u2212 d) < 0. This does not reflect that the r-hop neighbourhood for such a graph grows polynomially in r.\nWe conclude this appendix by reporting a lower bound for the Ollivier Ricci curvature derived in Jost & Liu (2014). We recall that \u03ba \u03b1 , with \u03b1 \u2208 [0, 1) was defined in equation 7.\nTheorem 12 (Jost & Liu (2014)). For any edge i \u223c j, with d i \u2264 d j , the following bound is satisfied:\n\u03ba 0 (i, j) \u2265 \u03a6(i, j) := \u2212 1 \u2212 1 d i \u2212 1 d j \u2212 | \u2206 | d j + \u2212 1 \u2212 1 d i \u2212 1 d j \u2212 | \u2206 | d i + + | \u2206 | d j .", "publication_ref": ["b25", "b45", "b51", "b55", "b20", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "D PROOFS OF RESULTS IN SECTION 3", "text": "We first recall our definition of Balanced Forman: Definition 7. For any edge i \u223c j we let Ric(i, j) be zero if min{d i , d j } = 1, otherwise\nRic(i, j) := 2 d i + 2 d j \u2212 2 + 2 | \u2206 | max{d i , d j } + | \u2206 | min{d i , d j } + (\u03b3 max ) \u22121 max{d i , d j } (| i | + | j |)(9)\nwhere the last term is set to be zero if | i | (and hence | j |) is zero.\nWe also extend the previous definition to the weighted case. In this setting we let G = (V, E, \u03c9) be a simple, locally finite, undirected graph with normalized weights. We first report the formula for the augmented Forman in the weighted case Samal et al. (2018):\nF (i, j) = \u03c9(i) + \u03c9(j) + k\u2208S1(i)\u2229S1(j) \u03c9 2 ij \u03c9 \u2206 \u2212 k\u2208S1(i)\\S1(j) \u03c9(i) \u03c9 ij \u03c9 ik \u2212 k\u2208S1(j)\\S1(i) \u03c9(j) \u03c9 ij \u03c9 jk ,\nwhere \u03c9 \u2206 is taken to be the Heron formula for the area of a triangle while \u03c9(\u2022) denotes some weighting scheme for the nodes as well. We propose a similar definition for the weighted case, which reduces to the one discussed above in the combinatorial setting. We recall that W is the weighted adjacency matrix while A is the combinatorial one. Moreover, we write A i j = (A j \u2212 A i ) + and similarly for W i j . Definition 8. For any pair of adjacent nodes i, j \u2208 V we define Ric(i, j) to be 0 if min{|d i |, |d j |} = 1, otherwise we set\nRic(i, j) := 1 d i \uf8eb \uf8ed 1 \u2212 k\u2208Qi \u03c9 ij \u03c9 ik \uf8f6 \uf8f8 + 1 d j \uf8eb \uf8ed 1 \u2212 k\u2208Qj \u03c9 ij \u03c9 jk \uf8f6 \uf8f8 + 1 max{d i , d j } k\u2208S1(i)\u2229S1(j) \u03c9 2 ij \u03c9 \u2206 + k\u2208 i \u03c9 ij \u03c9 \u03c9 ij (\u03b3 max ) \u22121 max{d i , d j } \u2212 \u221a \u03c9 ij \u03bd d i + k\u2208 j \u03c9 ij \u03c9 \u03c9 ij (\u03b3 max ) \u22121 max{d i , d j } \u2212 \u221a \u03c9 ij \u03bd d j ,\nThen there exists Q j \u2282 S 2 (i) satisfying |Q j | > \u03b4 \u22121 and for 0 \u2264 0 \u2264 L \u2212 2 we have\n1 |Q j | k\u2208Qj \u2202h ( 0 +2) k \u2202h ( 0) i < (\u03b1\u03b2) 2 \u03b4 1 4 .\n(4)\nProof. As usual we let d i := d and d j := d + s, for some s \u2265 0. We first observe that from the requirement \u03b4 2 (d + s) \u2264 1 in (ii), we derive Ric(i, j) \u2264 \u22122 + \u03b4 iff\n4 + 2 s d + 3| \u2206 | + s d | \u2206 | + \u03b3 \u22121 max (| i | + | j |) \u2264 \u03b4(d + s).\nTherefore we have \u03b4|\n\u2206 | 3 + s d \u2264 \u03b4 2 (d + s), meaning that \u03b4| \u2206 | \u2264 1. (10\n)\nFrom now on we let Q j denote again the complement S 1 (j) \\ (S 1 (i) \u222a j \u222a {i}). Without loss of generality we set 0 = 0 and hence h (0) i = x i ; the very same proof applies to any other choice of 0 . Given k \u2208 Q j , since k \u2208 S 2 (i), we can apply Lemma 1 and derive\n\u2202h (2) k \u2202x i \u2264 (\u03b1\u03b2) 2 (\u00c2) 2 ik .(11)\nWe may expand the power of the augmented normalized adjacency matrix as\n(\u00c2) 2 ik = 1 (d k + 1)(d i + 1) w\u2208S1(k)\u2229S1(i) 1 d w + 1 .\nIf we introduce the setQ j = {k \u2208 Q j : \u03c3 ik > 1}, we can then write\nk\u2208Qj (\u00c2) 2 ik = k\u2208Qj 1 (d k + 1)(d i + 1) w\u2208S1(k)\u2229S1(i) 1 d w + 1 = = 1 \u221a d i + 1 \uf8eb \uf8ed k\u2208Qj 1 \u221a d k + 1 1 d j + 1 + k\u2208Qj 1 \u221a d k + 1 w\u2208S1(k)\u2229S1(i)\u2229S1(j) 1 d w + 1 \uf8f6 \uf8f8 (12\n)\nwhere in the last equality we have again used the fact that k \u2208Q j iff there is w \u2208 S 1 (k)\u2229S 1 (i)\u2229S 1 (j).\nTo avoid heavy notations, we introduce V k := S 1 (k) \u2229 S 1 (i) \u2229 S 1 (j). Let us first focus on the first sum in equation 12. We have\n1 \u221a d i + 1 k\u2208Qj 1 \u221a d k + 1 1 d j + 1 \u2264 1 \u221a d i + 1 |Q j | 1 d j + 1 \u2264 1 \u221a d i + 1 \u2264 1. (13\n)\nWe now consider the second sum in equation 12. We assume | \u2206 | \u2265 1, otherwiseQ j = \u2205. We let\n\u2126 := w \u2208 \u2206 : d w < 1 C |Q j | | \u2206 | + 2 C\nfor some C > 0 to be chosen below. Then, the second sum in equation 12 can be split as\nk\u2208Qj 1 \u221a d k + 1 \uf8eb \uf8ed w\u2208V k \u2229\u2126 1 d w + 1 + w\u2208V k \\\u2126 1 d w + 1 \uf8f6 \uf8f8 . (14\n)\nSince any w \u2208 V k has degree at least three, we can bound the first term in equation 14 as\nk\u2208Qj 1 \u221a d k + 1 w\u2208V k \u2229\u2126 1 d w + 1 \u2264 k\u2208Qj 1 \u221a d k + 1 |V k \u2229 \u2126| 4 .\nWe now observe that\nk\u2208Qj |V k \u2229 \u2126| \u221a d k + 1 \u2264 k\u2208Qj |V k \u2229 \u2126| = (k, w) \u2208 E : k \u2208Q j , w \u2208 V k \u2229 \u2126 \u2264 max w\u2208\u2126 d w |\u2126|. Since d w \u2264 (1/C)|Q j |/| \u2206 | + 2/C\nfor any w \u2208 \u2126 we see that the first term in equation 14 can be bounded by\nk\u2208Qj 1 \u221a d k + 1 w\u2208V k \u2229\u2126 1 d w + 1 \u2264 k\u2208Qj 1 \u221a d k + 1 |V k \u2229 \u2126| 4 \u2264 1 C |Q j | | \u2206 | + 2 C |\u2126| 4 \u2264 1 C |Q j | | \u2206 | + 2 C | \u2206 | 4 . (15\n)\nby definition of \u2126. We now bound the second term in equation 14 as\nk\u2208Qj 1 \u221a d k + 1 w\u2208V k \\\u2126 1 d w + 1 \u2264 k\u2208Qj 1 \u221a d k + 1 C| \u2206 | |Q j | |V k \\ \u2126|\nwhere we have used that\nd \u22121 w \u2264 C| \u2206 |/|Q j | if w \u2208 V k \\ \u2126. Since |V k \\ \u2126| \u221a d k + 1 \u2264 |V k | |S 1 (k)| \u2264 |V k | |V k | \u2264 |V k | \u2264 | \u2206 | we see that k\u2208Qj 1 \u221a d k + 1 C| \u2206 | |Q j | |V k \\ \u2126| \u2264 C| \u2206 | |Q j | | \u2206 ||Q j | = C| \u2206 | 3 2 . (16\n)\nWe are now ready to complete the proof of the theorem. According to equation 11 it suffices to show that 1\n|Q j | k\u2208Qj (\u00c2 2 ) ik \u2264 \u03b4 1/4 .\nFrom equation 12 and equation 13 we derive that the left hand side of the equation above is bounded by\n1 |Q j | k\u2208Qj (\u00c2 2 ) ik \u2264 1 |Q j | + 1 |Q j | \uf8eb \uf8ed k\u2208Qj 1 \u221a d k + 1 w\u2208V k 1 d w + 1 \uf8f6 \uf8f8 \u2264 \u03b4 + 1 |Q j | \uf8eb \uf8ed k\u2208Qj 1 \u221a d k + 1 w\u2208V k 1 d w + 1 \uf8f6 \uf8f8\nwhere in the last inequality we have used Lemma 14 to bound |Q j | \u22121 by \u03b4. In particular we note that if \u2206 = \u2205 thenQ j = \u2205, and the bound would be simply controlled by \u03b4 as claimed. When | \u2206 | > 0, we can use equation 15 and equation 16 to estimate the second term from above by\n1 |Q j | \uf8eb \uf8ed k\u2208Qj 1 \u221a d k + 1 w\u2208V k 1 d w + 1 \uf8f6 \uf8f8 \u2264 1 |Q j | 1 C |Q j | | \u2206 | + 2 C | \u2206 | 4 + 1 |Q j | C| \u2206 | 3 2 \u2264 1 4 1 C + 2| \u2206 | C|Q j | + C| \u2206 | |Q j | | \u2206 |.\nBy applying Lemma 14 we get 1 4\n1 C + 2| \u2206 | C|Q j | + C| \u2206 | |Q j | | \u2206 | \u2264 1 4 1 C + 2 \u03b4 C + C\u03b4 | \u2206 |.\nPublished as a conference paper at ICLR 2022\nWe now choose C = \u03b4 \u22121/4 , so that the previous quantity can be bounded by 1 4\n1 C + 2 \u03b4 C + C\u03b4 | \u2206 | \u2264 1 4 \u03b4 1 4 + 2\u03b4 5 4 + \u03b4 1 4 \u03b4| \u2206 | \u2264 1 4 \u03b4 1 4 + 2\u03b4 5 4 + \u03b4 1 4\nwhere in the last inequality we have used equation 10. Therefore, we have shown that\n1 |Q j | k\u2208Qj (\u00c2 2 ) ik \u2264 \u03b4 + 1 4 \u03b4 1 4 + 2\u03b4 5 4 + \u03b4 1 4 \u2264 3\u03b4 1 4\nwhere we have used that \u03b4 < 1. This completes the proof (once we absorb the extra factor 3 in the constant \u03b1\u03b2 in equation 11).\nRemark 15. The requirement \u03b4 max{d i , d j } < 1 can be replaced by a more general bound \u03b4 max{d i , d j } < r. The argument above extends to this case up to renaming the constant \u03b1\u03b2 in the statement so to include an extra factor r.\nWe note that the condition \u03b4 max{d i , d j } < r would be stronger than the one appearing in (ii) of Theorem 4. In this regard, we recall that for a d-tree the curvature satisfies Ric(i, j) = \u22122 + 4 d . We can also prove Proposition 5:\nProposition 5. If Ric(i, j) \u2265 k > 0 for all i \u223c j, then \u03bb 1 /2 \u2265 h G \u2265 k 2 .\nProof. This follows as an immediate Corollary of Theorem 2 and (Lin et al., 2011, Theorem 4.2).\nBetweenness centrality to measure bottleneck. In equation 2 we have derived how the topology of the graph affects the dependence of the hidden node representation h (r+1) i on the input feature x s , for nodes i, s at distance r + 1. We note that in this case\u00c2 r+1 is is exactly measuring the number of minimal paths from i to s. If the receptive field B r+1 (i) is a binary tree, then we have seen that the entry of the power matrix decays exponentially. The reason for such decay stems from the existence of exponentially many nodes in the receptive field combined with the lack of multiple minimal paths (shortcuts). When such conditions hold, most of the minimal paths go through the same nodes, which is exactly what happens for the tree where each node is in the minimal paths between different branches. Since the frequency in which a node appears in the minimal path of distinct pairs of nodes is measured by the betweenness centrality Freeman (1977), we propose a topological characterization of the 'bottleneckedness' of a graph as follows: Definition 9 (bottleneck). The bottleneck-value of G is b G := 1 n n i=1 b(i), where b(i) denotes the betweenness centrality on node i.", "publication_ref": ["b45"], "figure_ref": [], "table_ref": []}, {"heading": "From a standard combinatorial argument it follows that if", "text": "G is connected, then b G = 1 n i,j (d G (i, j) \u2212 1) .(17)\nWe note that b G = 0 iff G is the complete graph K n . Therefore, b G determines how far the given topology is from K n , with the latter representing the limit case of a fully connected layer Alon & Yahav (2021) where no bottleneck may occur as any pair of nodes would be neighbours. This further supports our intuition that the betweenness centrality is a good topological candidate for providing a global measurement of bottleneckedness in the graph.\nIt also follows from equation 17 that any update to the graph topology consisting of edge additions would decrease b G and thus reduce the bottleneck. The quantity b G is global in nature though and hence lacks robustness. As a pedagogical example, consider a barbell G(m, 2r + 1), with m the size of the two cliques joined by a path of length 2r + 1 and focus on the edge i \u223c j in the middle of such path. Nodes i and j are central to the graph, in the sense that most minimal paths go through them and indeed their betweenness centrality is b(i) = b(j) = (m + r) 2 + (m + r). If now we add a single edge joining the two cliques K m , the values b(i) and b(j) decrease dramatically by \u2126(m 2 + r).\nSince the operation is non-local, we see that the representation h\n( )\ni of any MPNN is unaffected by the edge addition for any \u2208 (0, r), and similarly for j. Eventually, if we keep adding edges, the receptive fields B s (i) will be affected for small values of s as well: the drawback of such approach is that the resulting adjacency may be significantly different. Conversely, the curvature provides a more precise, local and hence robust way of controlling the bottleneck and hence the over-squashing problem. Nonetheless, we relate the betweenness centrality to the Jacobian of the hidden features.\nTheorem 16. Given i \u223c j, let \u2126\nj := S 1 (i) \u2229 S 1 (j) \u222a {j}. If Ric(i, j) \u2264 \u22122 + \u03b4, for 0 < \u03b4 < (1 + \u03b3 max ) \u22121 , then 1 |\u2126 j | k\u2208\u2126j b(k) \u2265 \u03b4 \u22121 .\nProof. We rewrite the quantity in the statement as\n1 | \u2206 | + 1 \uf8eb \uf8ed k\u2208 \u2206 b(k) + b(j) \uf8f6 \uf8f8 .\nBy definition, given a node k \u2208 V , the betweenness centrality of k is given by\nb(k) := s,t\u2208V :s =k,t =k \u03c3 st (k) \u03c3 st\nwhere \u03c3 st is the number of minimal paths between s and t while \u03c3 st (k) is the number of minimal paths from s to t passing through k. For convenience, we introduce the setQ j \u2282 Q j defined b\u0177\nQ j := {w \u2208 Q j : \u03c3 iw > 1}.\nEquivalently,Q j consists of those nodes in S 1 (j) \\ S 1 (i) which form a 4-cycle based at i \u223c j with a diagonal inside. Indeed, if w \u2208 Q j and \u03c3 iw > 1, then there exists more than one minimal path between i and w, in addition to the one passing through node j. For any such path there exists k \u2208 S 1 (i) \u2229 S 1 (w). Since w \u2208 Q j and Q j \u2229 j = \u2205, we derive that k \u2208 S 1 (j) as well. We then get\nk\u2208 \u2206 b(k) = k\u2208 \u2206 s,t\u2208V :s =k,t =k \u03c3 st (k) \u03c3 st \u2265 k\u2208 \u2206 w\u2208Qj \u03c3 iw (k) \u03c3 iw = w\u2208Qj 1 \u03c3 iw k\u2208 \u2206 \u03c3 iw (k).\nBy summing \u03c3 iw (k) for all k \u2208 \u2206 we obtain all the 2-long minimal paths between i and w with the exception of the one passing through j:\nk\u2208 \u2206 b(k) \u2265 w\u2208Qj 1 \u03c3 iw (\u03c3 iw \u2212 1).(18)\nOn the other hand we also have b(j) = s,t\u2208V :s =j,t =j \u03c3 st (j)\n\u03c3 st \u2265 z\u2208Qj \u03c3 iz (j) \u03c3 iz = z\u2208Qj 1 \u03c3 iz . (19\n)\nBy combining equation 18 and equation 19 we finally get\n1 | \u2206 | + 1 \uf8eb \uf8ed k\u2208 \u2206 b(k) + b(j) \uf8f6 \uf8f8 \u2265 1 | \u2206 | + 1 \uf8eb \uf8ed w\u2208Qj 1 \u03c3 iw (\u03c3 iw \u2212 1) + z\u2208Qj 1 \u03c3 iz \uf8f6 \uf8f8 = 1 | \u2206 | + 1 \uf8eb \uf8ed |Q j | + z\u2208Qj \\Qj 1 \u03c3 iz \uf8f6 \uf8f8 = |Q j | | \u2206 | + 1 ,\nwhere in the last equality we have used that by definition \u03c3 iz = 1 for all z \u2208 Q j \\Q j . By Lemma 14 the last quantity is larger than \u03b4 \u22121 .", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "F EXPERIMENTS", "text": "Our experiments in this paper are semi-supervised node classification (semi-supervised in that the graph structure provides some unlabelled information) on nine common graph learning datasets. Cornell, Texas and Wisconsin are small heterophilic datasets based on webpage networks from the WebKB dataset. Chameleon and Squirrel (Rozemberczki et al., 2021) are medium heterophilic datasets based on Wikipedia networks, along with Actor, the actor-only induced subgraph of the filmdirector-actor-writer network (Tang et al., 2009). Cora (McCallum et al., 2000), Citeseer (Sen et al., 2008) and Pubmed (Namata et al., 2012) are medium homophilic datasets based on citation networks. As in Klicpera et al. (2019), for all experiments we consider the largest connected component of the graph.\nWhen splitting the data into train/validation/test sets, we first separate the data into a development set and the test set. This is done once to ensure the test set is not used for any training or hyperparameter fitting before the final evaluation. For each of the 100 random splits the development set is divided randomly into a train set and a validation set, where we train models on the train set and evaluate on the validation set. We fit hyperparameters by random search, maximising the mean accuracy across the validation sets. The accuracy then reported in Table 2 is the mean accuracy on the test set from models trained on the train sets with the chosen hyperparameters, along with a 95% confidence interval calculated by bootstrapping the test set accuracies with 1000 samples. For Cora, Citeseer and Pubmed the development set contains 1500 nodes with the rest kept for the test set, and for each random split the train set is chosen to contain 20 nodes of each class while the rest form the validation set. As this is the same method as Klicpera et al. (2019) and we use the same random seeds, we are using the same test set and expect to have comparable results. For the remaining datasets we perform a 60/20/20 split, with 20% of nodes set aside as the test set and then for each random split the remaining 80% is split into 60% training, 20% validation. For datasets with disconnected graphs, the statistics shown here are for the largest connected component.  In the captions we see the Wasserstein distance W 1 between the original and preprocessed graphs.", "publication_ref": ["b52", "b28", "b47", "b32", "b24", "b24"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "", "text": "Acknowledgements. This research was supported in part by the EPSRC CDT in Modern Statistics and Statistical Machine Learning (EP/S023151/1) and the ERC Consolidator Grant No. 724228 (LEMAN).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "with \u03c9 \u2206 := 1 3 (\u03c9 2 ij + \u03c9 2 ik + \u03c9 2 jk ), z \u2208 S 1 (i) \u2229 S 1 (j), and, for a given k \u2208 i ,\nImportant convention. Without losing generality, in the following we always assume that 1 \u2264 d i \u2264 d j . In particular, we write d i .\n= d and d j = d + s, for some s \u2265 0, omitting to specify that both d and s are of course depending on i and j. Moreover, from now on we only focus on the unweighted case.\nWe can now prove our main comparison theorem.\nTheorem 2. Given an unweighted graph G, for any edge i \u223c j we have \u03ba(i, j) \u2265 Ric(i, j).\nProof. We stick to the aforementioned convention:\nThe strategy of the proof amounts to finding a transportation plan providing an upper bound for W 1 (\u00b5 \u03b1 i , \u00b5 \u03b1 j ) and hence a lower bound for the curvature \u03ba(i, j). In particular, we consider plans moving the mass \u00b5 \u03b1 i from B 1 (i) to B 1 (j).\nIf d = 1, then the optimal transport plan consists of moving the mass \u03b1 from i to j and the remaining mass 1 \u2212 \u03b1 on j to S 1 (j). This yields a unit Wasserstein distance between \u00b5 \u03b1 i and \u00b5 \u03b1 j and hence zero Ollivier curvature \u03ba(i, j), which coincides with the value of balanced Forman Ric(i, j).\nAssume now that d \u2265 2. A (possibly non-optimal) transport plan from \u00b5 \u03b1 i to \u00b5 \u03b1 j is given by:\nto its unique image in j under a bijection \u03d5 as per definition of m .\n(ii) The remaining mass on each node k \u2208 m will need to travel by at most distance 3 to S 1 (j).\n(iii) The extra-mass (1 \u2212 \u03b1)(1/d \u2212 1/(d + s)) on each common neighbour k \u2208 \u2206 will need to travel by at most distance 2 to S 1 (j).\n(iv) Move the mass (1 \u2212 \u03b1)/d from j to S 1 (j).\n(v) Move the mass \u03b1 from i to j. This leaves left-over mass (1 \u2212 \u03b1)/(d + s) at i from the distribution \u00b5 \u03b1 j . This mass can be compensated from mass in S 1 (i) which is at distance one.\n(vi) Finally, we move the mass (1 \u2212 \u03b1)/d of any untouched node in S 1 (i) to S 1 (j) along a path of length lesser or equal than three. Note that the remaining mass is equal to\nwhere the last terms comes from (v).\nIf we sum all the contributions we find\nTherefore we have\nBy using Lemma 11, we can bound the right hand side as\nwhich completes the proof.\nRemark 13. By inspection Ric(i, j) \u2265 \u03a6(i, j), with \u03a6(i, j) as in Theorem 12. We have three cases:\ntakes into account the positive contribution of 4-cycles as well.\nFrom the previous inequalities we derive\nIn this case we have\nProof. This follows immediately from Theorem 2 and Corollary 1 in Paeng (2012).\nTo address the proof of Theorem 4, we first need the Lemma below. Lemma 14. Given i \u223c j, with\nProof. According to our convention we let d i = d and d j = d + s, for s \u2265 0. We also recall that Q j = S 1 (j)\\( \u2206 \u222a j \u222a{i}). If we multiply equation 3 by d j = d+s, we see that Ric\nBy\nTherefore, we conclude\nTheorem 4. Consider a MPNN as in equation 1. Let i \u223c j with d i \u2264 d j and assume that: (ii) There exists \u03b4 s.t. 0 < \u03b4 < (max{d i , d j }) \u2212 1 2 , \u03b4 < \u03b3 \u22121 max , and Ric(i, j) \u2264 \u22122 + \u03b4.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E PROOFS OF RESULTS IN SECTION 4", "text": "Theorem 6. Let S \u2282 V with vol(S) \u2264 vol(G)/2. Then h S,\u03b1 \u2264 1\u2212\u03b1 \u03b1 davg(S) dmin(S) h S , where d avg (S) and d min (S) are the average and minimum degree on S, respectively.\nProof. Given a signal f : V \u2192 R on the vertex set and U \u2282 V , analogously to Chung (2007), we introduce the notation f (U ) := i\u2208U f (i).\nLet us rewrite the new Cheeger constant h S,\u03b1 as follows:\nwith \u03c7 S the characteristic function of the subset S, i.e. \u03c7 S (i) = 1 iff i \u2208 S. Since the graph G is connected, we can bound h S,\u03b1 from above as\n.\nIt was proven in (Chung, 2007, Lemma 5) that\nBy applying equation equation 20 to the bound for the Cheeger constant h S,\u03b1 we finally see that\nWe also report an equivalent result, again relying on (Chung, 2007, Lemma 5). Proposition 17. Let S \u2282 V with vol(S) \u2264 vol(G)/2. For any k \u2208 N, there exists S k,\u03b1 \u2282 S with vol\nProof. Let k \u2208 N. By modifying slightly the argument in (Chung, 2007, Lemma 5), we derive that\nTherefore, we obtain\nWe then conclude that the complement of S k,\u03b1 has volume greater or equal than vol(S)(1 \u2212 (2k) \u22121 ), which completes the proof.\nRemark 18. The previous proposition shows that after sparsifying the personalized page rank operator R \u03b1 as suggested in Klicpera et al. (2019) by setting entries below some threshold equal to zero, there will still be only few edges connecting different communities, once again highlighting that random-walk based methods are generally not suited to tackle the graph bottleneck.", "publication_ref": ["b8", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "F.3 VISUALIZING CURVATURE AND SENSITIVITY TO FEATURES", "text": "Figure 6: Rewiring of the Cornell graph. Left-to-right: original graph, DIGL, and SDRF rewiring.\nEdges are colored by curvature; nodes are colored by the maximum absolute entry of a trained 2-layer GCN's Jacobian between the GCN's prediction for that node and the features of the nodes 2 hops away in the original graph. SDRF homogenizes curvature and so lifts the upper bound on the Jacobian from Theorem 4. DIGL also does to an extent, though at the expense of preserving graph topology.\nF.4 HYPERPARAMETERS       ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "On the bottleneck of graph neural networks and its practical implications", "journal": "", "year": "2021", "authors": "Uri Alon; Eran Yahav"}, {"ref_id": "b1", "title": "The logical expressiveness of graph neural networks", "journal": "In ICLR", "year": "2019", "authors": "Pablo Barcel\u00f3; V Egor; Mikael Kostylev; Jorge Monet; Juan P\u00e9rez; Juan Pablo Reutter;  Silva"}, {"ref_id": "b2", "title": "Network geometry", "journal": "Nature Reviews Physics", "year": "2021", "authors": "Marian Boguna; Ivan Bonamassa; Manlio De Domenico; Shlomo Havlin; Dmitri Krioukov; M \u00c1ngeles Serrano"}, {"ref_id": "b3", "title": "The anatomy of a large-scale hypertextual web search engine. Computer networks and ISDN systems", "journal": "", "year": "1998", "authors": "Sergey Brin; Lawrence Page"}, {"ref_id": "b4", "title": "Geometric deep learning: Grids, groups, graphs, geodesics, and gauges", "journal": "", "year": "2021", "authors": "Joan Michael M Bronstein; Taco Bruna; Petar Cohen;  Veli\u010dkovi\u0107"}, {"ref_id": "b5", "title": "Spectral networks and locally connected networks on graphs", "journal": "", "year": "2014", "authors": "Joan Bruna; Wojciech Zaremba; Arthur Szlam; Yann Lecun"}, {"ref_id": "b6", "title": "Hyperbolic graph convolutional neural networks", "journal": "", "year": "2019", "authors": "Ines Chami; Rex Ying; Christopher R\u00e9; Jure Leskovec"}, {"ref_id": "b7", "title": "A lower bound for the smallest eigenvalue of the laplacian", "journal": "Princeton University Press", "year": "2015", "authors": "Jeff Cheeger"}, {"ref_id": "b8", "title": "Four proofs for the cheeger inequality and graph partition algorithms", "journal": "Citeseer", "year": "2007", "authors": "Fan Chung"}, {"ref_id": "b9", "title": "Spectral graph theory. Number 92", "journal": "American Mathematical Soc", "year": "1997", "authors": "R K Fan; Fan Chung Chung;  Graham"}, {"ref_id": "b10", "title": "Diffusion maps", "journal": "Applied and computational harmonic analysis", "year": "2006", "authors": "St\u00e9phane Ronald R Coifman;  Lafon"}, {"ref_id": "b11", "title": "Convolutional neural networks on graphs with fast localized spectral filtering", "journal": "Curran Associates, Inc", "year": "2016", "authors": "Micha\u00ebl Defferrard; Xavier Bresson; Pierre Vandergheynst"}, {"ref_id": "b12", "title": "Discrete and computational geometry", "journal": "", "year": "2003", "authors": "Robin Forman"}, {"ref_id": "b13", "title": "A general framework for adaptive processing of data structures", "journal": "IEEE Trans. Neural Networks", "year": "1998", "authors": "Paolo Frasconi; Marco Gori; Alessandro Sperduti"}, {"ref_id": "b14", "title": "A set of measures of centrality based on betweenness", "journal": "Sociometry", "year": "1977", "authors": "C Linton;  Freeman"}, {"ref_id": "b15", "title": "Neural message passing for quantum chemistry", "journal": "PMLR", "year": "2017", "authors": "Justin Gilmer; S Samuel;  Schoenholz; F Patrick; Oriol Riley; George E Vinyals;  Dahl"}, {"ref_id": "b16", "title": "Learning task-dependent distributed representations by backpropagation through structure", "journal": "", "year": "1996", "authors": "Christoph Goller; Andreas Kuchler"}, {"ref_id": "b17", "title": "A new model for learning in graph domains", "journal": "IEEE", "year": "2005", "authors": "Marco Gori; Gabriele Monfardini; Franco Scarselli"}, {"ref_id": "b18", "title": "The ricci flow on surfaces", "journal": "Amer. Math. Soc", "year": "1986", "authors": "Richard Hamilton"}, {"ref_id": "b19", "title": "Inductive representation learning on large graphs", "journal": "", "year": "2017", "authors": "Rex William L Hamilton; Jure Ying;  Leskovec"}, {"ref_id": "b20", "title": "Ollivier's ricci curvature, local clustering and curvature-dimension inequalities on graphs", "journal": "Discrete & Computational Geometry", "year": "2014", "authors": "J\u00fcrgen Jost; Shiping Liu"}, {"ref_id": "b21", "title": "Differentiable graph module (dgm) graph convolutional networks", "journal": "", "year": "2020", "authors": "Anees Kazi; Luca Cosmo; Nassir Navab; Michael Bronstein"}, {"ref_id": "b22", "title": "Interpretable stability bounds for spectral graph filters", "journal": "", "year": "2021", "authors": "Henry Kenlay; Dorina Thanou; Xiaowen Dong"}, {"ref_id": "b23", "title": "Semi-Supervised Classification with Graph Convolutional Networks", "journal": "", "year": "2017", "authors": "N Thomas; Max Kipf;  Welling"}, {"ref_id": "b24", "title": "Diffusion improves graph learning", "journal": "", "year": "2019", "authors": "Johannes Klicpera; Stefan Wei\u00dfenberger; Stephan G\u00fcnnemann"}, {"ref_id": "b25", "title": "Ricci curvature of graphs", "journal": "Tohoku Mathematical Journal, Second Series", "year": "2011", "authors": "Yong Lin; Linyuan Lu; Shing-Tung Yau"}, {"ref_id": "b26", "title": "Hyperbolic graph neural networks", "journal": "Curran Associates, Inc", "year": "2019", "authors": "Qi Liu; Maximilian Nickel; Douwe Kiela"}, {"ref_id": "b27", "title": "Provably powerful graph networks", "journal": "", "year": "2019", "authors": "Heli Haggai Maron; Hadar Ben-Hamu; Yaron Serviansky;  Lipman"}, {"ref_id": "b28", "title": "Automating the construction of internet portals with machine learning", "journal": "Information Retrieval", "year": "2000", "authors": "Andrew Kachites Mccallum; Kamal Nigam; Jason Rennie; Kristie Seymore"}, {"ref_id": "b29", "title": "Motifnet: A motif-based graph convolutional network for directed graphs", "journal": "", "year": "2018", "authors": "F Monti; K Otness; M M Bronstein"}, {"ref_id": "b30", "title": "Weisfeiler and leman go neural: Higher-order graph neural networks", "journal": "AAAI Press", "year": "2019", "authors": "Christopher Morris; Martin Ritzert; Matthias Fey; L William; Jan Eric Hamilton; Gaurav Lenssen; Martin Rattan;  Grohe"}, {"ref_id": "b31", "title": "Non-negative ollivier curvature on graphs, reverse poincar\\'e inequality, buser inequality, liouville property, harnack inequality and eigenvalue estimates", "journal": "", "year": "2019", "authors": "Florentin M\u00fcnch"}, {"ref_id": "b32", "title": "Query-driven active surveying for collective classification", "journal": "", "year": "2012", "authors": "Galileo Namata; Ben London; Lise Getoor; Bert Huang; Umd Edu"}, {"ref_id": "b33", "title": "Network alignment by discrete ollivier-ricci flow", "journal": "", "year": "", "authors": "Yu-Yao Chien-Chun Ni; Jie Lin; Xianfeng Gao;  Gu"}, {"ref_id": "b34", "title": "", "journal": "", "year": "2018", "authors": " Springer"}, {"ref_id": "b35", "title": "", "journal": "Community detection on networks with ricci flow. Scientific reports", "year": "2019", "authors": "Yu-Yao Chien-Chun Ni; Feng Lin; Jie Luo;  Gao"}, {"ref_id": "b36", "title": "Revisiting graph neural networks: All we have is low-pass filters", "journal": "", "year": "2019", "authors": "N T Hoang; Takanori Maehara"}, {"ref_id": "b37", "title": "Ricci curvature of metric spaces", "journal": "Comptes Rendus Mathematique", "year": "2007", "authors": "Yann Ollivier"}, {"ref_id": "b38", "title": "Ricci curvature of markov chains on metric spaces", "journal": "Journal of Functional Analysis", "year": "2009", "authors": "Yann Ollivier"}, {"ref_id": "b39", "title": "Graph neural networks exponentially lose expressive power for node classification", "journal": "", "year": "2020", "authors": "Kenta Oono; Taiji Suzuki"}, {"ref_id": "b40", "title": "Volume and diameter of a graph and ollivier's ricci curvature", "journal": "European Journal of Combinatorics", "year": "2012", "authors": " Seong-Hun;  Paeng"}, {"ref_id": "b41", "title": "Geom-gcn: Geometric graph convolutional networks", "journal": "", "year": "2019", "authors": "Hongbin Pei; Bingzhe Wei; Kevin Chen-Chuan; Yu Chang; Bo Lei;  Yang"}, {"ref_id": "b42", "title": "Finite extinction time for the solutions to the ricci flow on certain three-manifolds. arXiv preprint math/0307245", "journal": "", "year": "2003", "authors": "Grisha Perelman"}, {"ref_id": "b43", "title": "Sign: Scalable inception graph neural networks. CoRR, abs", "journal": "", "year": "2004", "authors": "Emanuele Rossi; Fabrizio Frasca; Ben Chamberlain; Davide Eynard; Michael M Bronstein; Federico Monti"}, {"ref_id": "b44", "title": "Multi-scale attributed node embedding", "journal": "Journal of Complex Networks", "year": "2021", "authors": "Carl Benedek Rozemberczki; Rik Allen;  Sarkar"}, {"ref_id": "b45", "title": "Comparative analysis of two discretizations of ricci curvature for complex networks", "journal": "Scientific reports", "year": "2018", "authors": "Areejit Samal; Jiao Sreejith; Shiping Gu; Emil Liu; J\u00fcrgen Saucan;  Jost"}, {"ref_id": "b46", "title": "The graph neural network model", "journal": "IEEE transactions on neural networks", "year": "2008", "authors": "Franco Scarselli; Marco Gori; Ah Chung Tsoi; Markus Hagenbuchner; Gabriele Monfardini"}, {"ref_id": "b47", "title": "Collective classification in network data", "journal": "AI magazine", "year": "2008", "authors": "Prithviraj Sen; Galileo Namata; Mustafa Bilgic; Lise Getoor; Brian Galligher; Tina Eliassi-Rad"}, {"ref_id": "b48", "title": "Pitfalls of graph neural network evaluation", "journal": "", "year": "2018", "authors": "Oleksandr Shchur; Maximilian Mumme; Aleksandar Bojchevski; Stephan G\u00fcnnemann"}, {"ref_id": "b49", "title": "Encoding labeled graphs by labeling RAAM", "journal": "", "year": "1994", "authors": "Alessandro Sperduti"}, {"ref_id": "b50", "title": "Supervised neural networks for the classification of structures", "journal": "IEEE Trans. Neural Networks", "year": "1997", "authors": "Alessandro Sperduti; Antonina Starita"}, {"ref_id": "b51", "title": "Forman curvature for complex networks", "journal": "Journal of Statistical Mechanics: Theory and Experiment", "year": "2016-06", "authors": "R P Sreejith; Karthikeyan Mohanraj; J\u00fcrgen Jost; Emil Saucan; Areejit Samal"}, {"ref_id": "b52", "title": "Social influence analysis in large-scale networks", "journal": "Association for Computing Machinery", "year": "2009", "authors": "Jie Tang; Jimeng Sun; Chi Wang; Zi Yang"}, {"ref_id": "b53", "title": "Graph attention networks", "journal": "", "year": "2018", "authors": "Petar Veli\u010dkovi\u0107; Guillem Cucurull; Arantxa Casanova; Adriana Romero; Pietro Li\u00f2; Yoshua Bengio"}, {"ref_id": "b54", "title": "Dynamic graph CNN for learning on point clouds", "journal": "ACM Trans. Graphics", "year": "2019", "authors": "Yue Wang; Yongbin Sun; Ziwei Liu; E Sanjay;  Sarma; Justin M Michael M Bronstein;  Solomon"}, {"ref_id": "b55", "title": "Coarse geometry of evolving networks", "journal": "Journal of complex networks", "year": "2018", "authors": "Melanie Weber; Emil Saucan; J\u00fcrgen Jost"}, {"ref_id": "b56", "title": "Representation learning on graphs with jumping knowledge networks", "journal": "PMLR", "year": "2018", "authors": "Keyulu Xu; Chengtao Li; Yonglong Tian; Tomohiro Sonobe; Ken-Ichi Kawarabayashi; Stefanie Jegelka"}, {"ref_id": "b57", "title": "How powerful are graph neural networks?", "journal": "", "year": "2019", "authors": "Keyulu Xu; Weihua Hu; Jure Leskovec; Stefanie Jegelka"}, {"ref_id": "b58", "title": "Bayesian graph convolutional neural networks for semi-supervised classification", "journal": "", "year": "2019", "authors": "Y Zhang; S Pal; M Coates; D \u00dcstebay"}, {"ref_id": "b59", "title": "Adversarial attacks on graph neural networks: Perturbations and their patterns", "journal": "ACM Transactions on Knowledge Discovery from Data (TKDD)", "year": "2020", "authors": "Daniel Z\u00fcgner; Oliver Borchert; Amir Akbarnejad; Stephan G\u00fcnnemann"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Different regimes of curvatures on graphs analogous to spherical (a), planar (b), and hyperbolic (c) geometries in the continuous setting.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3: 4-cycle contribution.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Theorem 6 .6Let S \u2282 V with vol(S) \u2264 vol(G)/2. Then h S,\u03b1 \u2264 1\u2212\u03b1 \u03b1 davg(S) dmin(S) h S , where d avg (S) and d min (S) are the average and minimum degree on S, respectively.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "/\u2202x s |, we normalize the Jacobian entries -obtaining what is referred to as influence score in Xu et al. (2018):", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "The homophily index H(G) proposed byPei et al. (2019) is defined as H(G) = 1 |V | v\u2208V Number of v's neighbors who have the same label as v Number of v's neighbors .", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 :5Figure5: Comparing the degree distribution of the original graphs to the preprocessed version. The xaxis is node degree in log 2 scale, and the plots are a kernel density estimate of the degree distribution. In the captions we see the Wasserstein distance W 1 between the original and preprocessed graphs.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Examples of the Balanced Forman curvature.", "figure_data": "Theorem 2 generalizes Jost & Liu (2014, Theorem 3) (see"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "None 52.69 \u00b1 0.21 61.19 \u00b1 0.49 54.60 \u00b1 0.86 41.33 \u00b1 0.18 30.32 \u00b1 0.99 23.84 \u00b1 0.43 81.89 \u00b1 0.79 72.31 \u00b1 0.17 78.16 \u00b1 0.23 Undirected 53.20 \u00b1 0.53 63.38 \u00b1 0.87 51.37 \u00b1 1.15 42.02 \u00b1 0.30 35.53 \u00b1 0.78 21.45 \u00b1 0.47 ---+FA 58.29 \u00b1 0.49 64.82 \u00b1 0.29 55.48 \u00b1 0.62 42.67 \u00b1 0.17 36.86 \u00b1 0.44 24.14 \u00b1 0.43 81.65 \u00b1 0.18 70.47 \u00b1 0.18 79.48 \u00b1 0.12 DIGL (PPR) 58.26 \u00b1 0.50 62.03 \u00b1 0.43 49.53 \u00b1 0.27 42.02 \u00b1 0.13 33.22 \u00b1 0.14 24.77 \u00b1 0.32 83.21 \u00b1 0.27 73.29 \u00b1 0.17 78.84 \u00b1 0.08 DIGL + Undirected 59.54 \u00b1 0.64 63.54 \u00b1 0.38 52.23 \u00b1 0.54 42.68 \u00b1 0.12 32.48 \u00b1 0.23 25.45 \u00b1 0.30 ---SDRF 54.60 \u00b1 0.39 64.46 \u00b1 0.38 55.51 \u00b1 0.27 42.73 \u00b1 0.15 37.05 \u00b1 0.17 28.42 \u00b1 0.75 82.76 \u00b1 0.23 72.58 \u00b1 0.20 79.10 \u00b1 0.11 SDRF + Undirected 57.54 \u00b1 0.34 70.35 \u00b1 0.60 61.55 \u00b1 0.86 44.46 \u00b1 0.17 37.67 \u00b1 0.23 28.35 \u00b1 0.06", "figure_data": "CornellTexasWisconsinChameleonSquirrelActorCoraCiteseerPubmedH(G)0.110.060.160.250.220.240.830.710.79---"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Experimental results on common node classification benchmarks. Top two in bold.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Comparing the degree distribution of the original graphs to the preprocessed version. The xaxis is node degree in log 2 scale, and the plots are a kernel density estimate of the degree distribution.In the captions we see the Wasserstein distance W 1 between the original and preprocessed graphs.", "figure_data": "Original DIGL SDRFOriginal DIGL SDRFOriginal DIGL SDRF01248163264 1280 1 2 4 8 16 32 64 128 256 512 1024204840960 1 2 4 8 16 32 64 128 256 512 1024 2048(a) Wisconsin:(b) Actor:(c) Pubmed:W1(Original, DIGL) = 11.83W1(Original, DIGL) = 831.88W1(Original, DIGL) = 247.01W1(Original, SDRF) = 0.28W1(Original, SDRF) = 0.28W1(Original, SDRF) = 0.03Figure 4: DIGLSDRFCornell351.1% / 0.0%7.8% / 33.3%Texas483.3% / 0.0%2.4% / 10.4%Wisconsin300.6% / 0.0%1.4% / 7.5%Chameleon 336.1% / 11.8%5.6% / 5.6%Squirrel73.2% / 66.4%4.2% / 4.2%Actor8331.3% / 0.0%1.9% / 3.0%Cora3038.0% / 0.5%1.0% / 1.0%Citeseer2568.3% / 0.0%1.1% / 1.1%Pubmed2747.1% / 0.1%0.2% / 0.2%"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "% edges added / removed by method.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "h ( +1) i = \u03c6 \uf8eb \uf8ed h ( ) i , n j=1\u00c2 ij \u03c8 (h ( ) i , h ( ) j ) \uf8f6 \uf8f8 .", "formula_coordinates": [2.0, 217.34, 674.36, 177.33, 33.53]}, {"formula_id": "formula_1", "formula_text": "( ) i = h ( )", "formula_coordinates": [3.0, 286.24, 272.52, 39.06, 14.07]}, {"formula_id": "formula_2", "formula_text": "(r+1) i \u2202x s \u2264 (\u03b1\u03b2) r+1 (\u00c2 r+1 ) is .(2)", "formula_coordinates": [3.0, 255.1, 371.9, 248.9, 26.36]}, {"formula_id": "formula_3", "formula_text": "(i) \u2206 (i, j) := S 1 (i) \u2229 S 1 (j) are the triangles based at i \u223c j. (ii) i (i, j) := {k \u2208 S 1 (i) \\ S 1 (j), k = j : \u2203w \u2208 (S 1 (k) \u2229 S 1 (j)) \\ S 1 (i)}", "formula_coordinates": [4.0, 126.71, 274.08, 307.33, 28.2]}, {"formula_id": "formula_4", "formula_text": "Ric(i, j) := 2 d i + 2 d j \u2212 2 + 2 | \u2206 (i, j)| max{d i , d j } + | \u2206 (i, j)| min{d i , d j } + (\u03b3 max ) \u22121 max{d i , d j } (| i | + | j |), (3)", "formula_coordinates": [4.0, 122.03, 462.37, 381.97, 24.8]}, {"formula_id": "formula_5", "formula_text": "if | i | (and hence | j |) is zero. In particular Ric(i, j) > \u22122.", "formula_coordinates": [4.0, 250.8, 493.39, 242.8, 11.7]}, {"formula_id": "formula_6", "formula_text": "C3 3 2 C4 1 C n\u22655 0 Complete Kn n n\u22121 Grid Gn 0 Tree Tr 4 r+1 \u2212 2", "formula_coordinates": [4.0, 391.18, 619.42, 100.57, 73.22]}, {"formula_id": "formula_7", "formula_text": "Corollary 3. If Ric(i, j) \u2265 k > 0 for any edge i \u223c j, then diam(G) \u2264 2 k .", "formula_coordinates": [4.0, 108.0, 646.65, 267.23, 22.62]}, {"formula_id": "formula_8", "formula_text": "1 |Q j | k\u2208Qj \u2202h ( 0 +2) k \u2202h ( 0) i < (\u03b1\u03b2) 2 \u03b4 1 4 .(4)", "formula_coordinates": [5.0, 237.36, 214.34, 266.64, 30.27]}, {"formula_id": "formula_9", "formula_text": "(0) i = x i .", "formula_coordinates": [5.0, 349.49, 326.91, 35.5, 14.07]}, {"formula_id": "formula_10", "formula_text": "h G := min S\u2282V h S , h S := |\u2202S| min{vol(S), vol(V \\ S)}(5)", "formula_coordinates": [5.0, 202.03, 490.13, 301.97, 22.53]}, {"formula_id": "formula_11", "formula_text": "2h G \u2265 \u03bb 1 \u2265 h 2 G 2 (6)", "formula_coordinates": [5.0, 271.45, 543.49, 232.55, 23.89]}, {"formula_id": "formula_12", "formula_text": "Proposition 5. If Ric(i, j) \u2265 k > 0 for all i \u223c j, then \u03bb 1 /2 \u2265 h G \u2265 k 2 .", "formula_coordinates": [5.0, 108.0, 670.8, 287.95, 13.47]}, {"formula_id": "formula_13", "formula_text": "Algorithm 1: Stochastic Discrete Ricci Flow (SDRF) Input: graph G, temperature \u03c4 > 0, max number of iterations, optional Ric upper-bound C + Repeat 1)", "formula_coordinates": [6.0, 107.64, 348.82, 371.41, 47.1]}, {"formula_id": "formula_14", "formula_text": "R \u03b1 := \u221e k=0 \u03b8 P P R k (D \u22121 A) k = \u03b1 \u221e k=0 (1 \u2212 \u03b1)(D \u22121 A) k .", "formula_coordinates": [7.0, 190.85, 130.31, 230.29, 30.55]}, {"formula_id": "formula_15", "formula_text": "h S,\u03b1 = |\u2202S| \u03b1 vol \u03b1 (S) \u2261 1 |S| i\u2208S j\u2208S (R \u03b1 ) ij .", "formula_coordinates": [7.0, 227.14, 200.5, 157.71, 27.8]}, {"formula_id": "formula_16", "formula_text": "|\u2207\u03c8 | \u2264 \u03b2 for 0 \u2264 \u2264 r, then \u2202h (r+1) i \u2202x s \u2264 (\u03b1\u03b2) r+1 (\u00c2 r+1 ) is .(2)", "formula_coordinates": [14.0, 108.0, 460.88, 396.0, 42.51]}, {"formula_id": "formula_17", "formula_text": "\u2202h (r+1) i \u2202x s = \u2202 1 \u03c6 r (. . .)\u2202 xs h (r) i + \u2202 2 \u03c6 r (. . .) n jr=1\u00e2 ijr \u2202 1 \u03c8 r (h (r) i , h (r) jr )\u2202 xs h (r) i + \u2202 2 \u03c8 r (h (r) i , h (r) jr )\u2202 xs h (r) jr .", "formula_coordinates": [14.0, 134.52, 561.35, 344.16, 58.55]}, {"formula_id": "formula_18", "formula_text": "\u2202h (r+1) i \u2202x s = jr,...,j0 kr\u2208{i,jr} \u2022 \u2022 \u2022 k1\u2208{i,jr,...,j1}\u00e2 ijr\u00e2krjr\u22121 . . .\u00e2 k1j0 Z ijrkrjr\u22121...k1j0 (X)\u2202 xs h (0) j0 ,", "formula_coordinates": [14.0, 119.02, 648.15, 375.17, 30.4]}, {"formula_id": "formula_19", "formula_text": "\u2202 xs h (0) j0 = \u03b4 j0s", "formula_coordinates": [14.0, 276.13, 719.92, 59.23, 14.07]}, {"formula_id": "formula_20", "formula_text": "(r+1) i \u2202x s \u2264 (\u03b1\u03b2) r+1 jr,...,j1\u00e2 ijr\u00e2jrjr\u22121 . . .\u00e2 j1s = \u03b1 r+1 (\u00c2 r+1 ) is", "formula_coordinates": [15.0, 185.37, 210.2, 253.53, 29.78]}, {"formula_id": "formula_21", "formula_text": "Corollary 7. If h ( +1) i = j\u223ci \u03c8 (h ( ) j ), then h ( +1) i", "formula_coordinates": [15.0, 108.0, 301.05, 216.32, 14.28]}, {"formula_id": "formula_22", "formula_text": "Proof. If h ( +1) i = j\u223ci \u03c8 (h ( ) j ) for each \u2208 [0, L \u2212 1]", "formula_coordinates": [15.0, 108.0, 354.28, 238.75, 14.28]}, {"formula_id": "formula_23", "formula_text": "( +1) i \u2202x s = j ,...,j1 a ij . . . a j1s \u03c8 (h ( ) j ) \u2022 \u2022 \u2022 \u03c8 0 (x s ).", "formula_coordinates": [15.0, 214.11, 379.55, 192.94, 29.79]}, {"formula_id": "formula_24", "formula_text": "( +1) i = ReLU \uf8eb \uf8ed j\u2208\u00d1i h ( ) j w \uf8f6 \uf8f8 .", "formula_coordinates": [15.0, 244.63, 639.43, 128.48, 35.37]}, {"formula_id": "formula_25", "formula_text": "J r+1 (i, s) := \u2202h (r+1) i \u2202xs k \u2202h (r+1) i \u2202x k", "formula_coordinates": [16.0, 249.32, 104.56, 107.14, 44.55]}, {"formula_id": "formula_26", "formula_text": "J r+1 (i, s) =\u00c3 r+1 is k\u00c3 r+1 ik \u2264\u00c3 r+1 is Vol(B r+1 (i)) ,", "formula_coordinates": [16.0, 221.33, 205.72, 169.34, 28.2]}, {"formula_id": "formula_27", "formula_text": "(i) \u2206 (i, j) := S 1 (i) \u2229 S 1 (j)", "formula_coordinates": [16.0, 129.48, 330.16, 116.23, 9.65]}, {"formula_id": "formula_28", "formula_text": "(ii) i (i, j) := {k \u2208 S 1 (i) \\ S 1 (j), k = j : \u2203w \u2208 (S 1 (k) \u2229 S 1 (j)) \\ S 1 (i)}, the number of nodes k \u2208 S 1 (i) forming a 4-cycle based at i \u223c j without diagonals inside. (iii) Q i (j) := S 1 (i) \\ ({j} \u222a \u2206 (i, j) \u222a i (i, j))", "formula_coordinates": [16.0, 123.94, 346.85, 380.06, 40.46]}, {"formula_id": "formula_29", "formula_text": "D(U ) := {\u03d5 : U \u2192 V, |U | = |\u03d5(U )|, (z, \u03d5(z)) \u2208 E, \u2200z \u2208 U } .", "formula_coordinates": [16.0, 174.19, 505.3, 263.61, 8.74]}, {"formula_id": "formula_30", "formula_text": "m (i, j) := max |U | : U \u2282 i , \u2203\u03d5 : U \u2192 j , \u03d5 \u2208 D(U ) .", "formula_coordinates": [16.0, 182.98, 588.08, 249.92, 11.48]}, {"formula_id": "formula_31", "formula_text": "\u03b3 max (i, j) := max max k\u2208 i {(A k \u2022 (A j \u2212 A i A j )) \u2212 1}, max w\u2208 j {(A w \u2022 (A i \u2212 A j A i )) \u2212 1} ,", "formula_coordinates": [16.0, 115.98, 695.15, 380.05, 16.51]}, {"formula_id": "formula_32", "formula_text": "| m | \u2265 max{| i |, | j |} \u03b3 max .", "formula_coordinates": [17.0, 256.11, 183.42, 99.78, 26.46]}, {"formula_id": "formula_33", "formula_text": "s=1 (A ws \u2022 (A i \u2212 A j A i )) \u2212 1) \u2265 | i |, which implies \u03b3 max | m | \u2261 \u03b3 max \u2265 s=1 (A ws \u2022 (A i \u2212 A j A i )) \u2212 1) \u2265 | i |.", "formula_coordinates": [17.0, 107.64, 298.66, 323.85, 78.2]}, {"formula_id": "formula_34", "formula_text": "\u00b5 \u03b1 i : j \u2192 \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 \u03b1, j = i 1\u2212\u03b1 di , j \u2208 S 1 (i), 0, otherwise.", "formula_coordinates": [17.0, 242.47, 454.86, 125.86, 39.5]}, {"formula_id": "formula_35", "formula_text": "W 1 (\u00b5 \u03b1 i , \u00b5 \u03b1 j ) := inf M k\u2208S1(i) w\u2208S1(j) M kw d G (k, w),", "formula_coordinates": [17.0, 205.78, 535.06, 200.44, 22.6]}, {"formula_id": "formula_36", "formula_text": "k\u2208S1(i) M kw = \u00b5 \u03b1 j (w), w\u2208S1(j) M kw = \u00b5 \u03b1 i (k).", "formula_coordinates": [17.0, 208.1, 597.71, 195.8, 22.6]}, {"formula_id": "formula_37", "formula_text": "\u03ba \u03b1 (i, j) := 1 \u2212 W 1 (\u00b5 \u03b1 i , \u00b5 \u03b1 j ).(7)", "formula_coordinates": [17.0, 247.45, 672.08, 256.55, 12.69]}, {"formula_id": "formula_38", "formula_text": "\u03ba(i, j) := lim \u03b1\u21921 1 \u2212 W 1 (\u00b5 \u03b1 i , \u00b5 \u03b1 j ) 1 \u2212 \u03b1 .(8)", "formula_coordinates": [17.0, 239.72, 710.97, 264.28, 24.98]}, {"formula_id": "formula_39", "formula_text": "F (i, j) := 4 \u2212 d i \u2212 d j + 3| \u2206 |.", "formula_coordinates": [18.0, 242.14, 137.62, 127.72, 9.65]}, {"formula_id": "formula_40", "formula_text": "\u03ba 0 (i, j) \u2265 \u03a6(i, j) := \u2212 1 \u2212 1 d i \u2212 1 d j \u2212 | \u2206 | d j + \u2212 1 \u2212 1 d i \u2212 1 d j \u2212 | \u2206 | d i + + | \u2206 | d j .", "formula_coordinates": [18.0, 130.36, 241.06, 351.28, 25.08]}, {"formula_id": "formula_41", "formula_text": "Ric(i, j) := 2 d i + 2 d j \u2212 2 + 2 | \u2206 | max{d i , d j } + | \u2206 | min{d i , d j } + (\u03b3 max ) \u22121 max{d i , d j } (| i | + | j |)(9)", "formula_coordinates": [18.0, 123.41, 336.0, 380.59, 24.8]}, {"formula_id": "formula_42", "formula_text": "F (i, j) = \u03c9(i) + \u03c9(j) + k\u2208S1(i)\u2229S1(j) \u03c9 2 ij \u03c9 \u2206 \u2212 k\u2208S1(i)\\S1(j) \u03c9(i) \u03c9 ij \u03c9 ik \u2212 k\u2208S1(j)\\S1(i) \u03c9(j) \u03c9 ij \u03c9 jk ,", "formula_coordinates": [18.0, 181.28, 427.11, 249.44, 63.67]}, {"formula_id": "formula_43", "formula_text": "Ric(i, j) := 1 d i \uf8eb \uf8ed 1 \u2212 k\u2208Qi \u03c9 ij \u03c9 ik \uf8f6 \uf8f8 + 1 d j \uf8eb \uf8ed 1 \u2212 k\u2208Qj \u03c9 ij \u03c9 jk \uf8f6 \uf8f8 + 1 max{d i , d j } k\u2208S1(i)\u2229S1(j) \u03c9 2 ij \u03c9 \u2206 + k\u2208 i \u03c9 ij \u03c9 \u03c9 ij (\u03b3 max ) \u22121 max{d i , d j } \u2212 \u221a \u03c9 ij \u03bd d i + k\u2208 j \u03c9 ij \u03c9 \u03c9 ij (\u03b3 max ) \u22121 max{d i , d j } \u2212 \u221a \u03c9 ij \u03bd d j ,", "formula_coordinates": [18.0, 179.38, 586.53, 253.24, 140.65]}, {"formula_id": "formula_44", "formula_text": "1 |Q j | k\u2208Qj \u2202h ( 0 +2) k \u2202h ( 0) i < (\u03b1\u03b2) 2 \u03b4 1 4 .", "formula_coordinates": [21.0, 237.36, 100.23, 138.47, 30.27]}, {"formula_id": "formula_45", "formula_text": "4 + 2 s d + 3| \u2206 | + s d | \u2206 | + \u03b3 \u22121 max (| i | + | j |) \u2264 \u03b4(d + s).", "formula_coordinates": [21.0, 189.9, 169.73, 232.2, 22.31]}, {"formula_id": "formula_46", "formula_text": "\u2206 | 3 + s d \u2264 \u03b4 2 (d + s), meaning that \u03b4| \u2206 | \u2264 1. (10", "formula_coordinates": [21.0, 108.0, 203.65, 391.85, 45.81]}, {"formula_id": "formula_47", "formula_text": ")", "formula_coordinates": [21.0, 499.85, 240.13, 4.15, 8.64]}, {"formula_id": "formula_48", "formula_text": "\u2202h (2) k \u2202x i \u2264 (\u03b1\u03b2) 2 (\u00c2) 2 ik .(11)", "formula_coordinates": [21.0, 263.8, 300.32, 240.2, 26.51]}, {"formula_id": "formula_49", "formula_text": "(\u00c2) 2 ik = 1 (d k + 1)(d i + 1) w\u2208S1(k)\u2229S1(i) 1 d w + 1 .", "formula_coordinates": [21.0, 201.91, 353.7, 208.18, 27.27]}, {"formula_id": "formula_50", "formula_text": "k\u2208Qj (\u00c2) 2 ik = k\u2208Qj 1 (d k + 1)(d i + 1) w\u2208S1(k)\u2229S1(i) 1 d w + 1 = = 1 \u221a d i + 1 \uf8eb \uf8ed k\u2208Qj 1 \u221a d k + 1 1 d j + 1 + k\u2208Qj 1 \u221a d k + 1 w\u2208S1(k)\u2229S1(i)\u2229S1(j) 1 d w + 1 \uf8f6 \uf8f8 (12", "formula_coordinates": [21.0, 128.82, 409.82, 371.03, 68.16]}, {"formula_id": "formula_51", "formula_text": ")", "formula_coordinates": [21.0, 499.85, 456.4, 4.15, 8.64]}, {"formula_id": "formula_52", "formula_text": "1 \u221a d i + 1 k\u2208Qj 1 \u221a d k + 1 1 d j + 1 \u2264 1 \u221a d i + 1 |Q j | 1 d j + 1 \u2264 1 \u221a d i + 1 \u2264 1. (13", "formula_coordinates": [21.0, 160.62, 528.28, 339.23, 26.88]}, {"formula_id": "formula_53", "formula_text": ")", "formula_coordinates": [21.0, 499.85, 535.34, 4.15, 8.64]}, {"formula_id": "formula_54", "formula_text": "\u2126 := w \u2208 \u2206 : d w < 1 C |Q j | | \u2206 | + 2 C", "formula_coordinates": [21.0, 226.52, 588.51, 149.02, 23.23]}, {"formula_id": "formula_55", "formula_text": "k\u2208Qj 1 \u221a d k + 1 \uf8eb \uf8ed w\u2208V k \u2229\u2126 1 d w + 1 + w\u2208V k \\\u2126 1 d w + 1 \uf8f6 \uf8f8 . (14", "formula_coordinates": [21.0, 195.66, 639.04, 304.19, 35.52]}, {"formula_id": "formula_56", "formula_text": ")", "formula_coordinates": [21.0, 499.85, 652.98, 4.15, 8.64]}, {"formula_id": "formula_57", "formula_text": "k\u2208Qj 1 \u221a d k + 1 w\u2208V k \u2229\u2126 1 d w + 1 \u2264 k\u2208Qj 1 \u221a d k + 1 |V k \u2229 \u2126| 4 .", "formula_coordinates": [21.0, 179.3, 704.59, 253.4, 28.64]}, {"formula_id": "formula_58", "formula_text": "k\u2208Qj |V k \u2229 \u2126| \u221a d k + 1 \u2264 k\u2208Qj |V k \u2229 \u2126| = (k, w) \u2208 E : k \u2208Q j , w \u2208 V k \u2229 \u2126 \u2264 max w\u2208\u2126 d w |\u2126|. Since d w \u2264 (1/C)|Q j |/| \u2206 | + 2/C", "formula_coordinates": [22.0, 108.0, 101.37, 386.31, 51.08]}, {"formula_id": "formula_59", "formula_text": "k\u2208Qj 1 \u221a d k + 1 w\u2208V k \u2229\u2126 1 d w + 1 \u2264 k\u2208Qj 1 \u221a d k + 1 |V k \u2229 \u2126| 4 \u2264 1 C |Q j | | \u2206 | + 2 C |\u2126| 4 \u2264 1 C |Q j | | \u2206 | + 2 C | \u2206 | 4 . (15", "formula_coordinates": [22.0, 141.64, 171.92, 358.21, 61.35]}, {"formula_id": "formula_60", "formula_text": ")", "formula_coordinates": [22.0, 499.85, 217.11, 4.15, 8.64]}, {"formula_id": "formula_61", "formula_text": "k\u2208Qj 1 \u221a d k + 1 w\u2208V k \\\u2126 1 d w + 1 \u2264 k\u2208Qj 1 \u221a d k + 1 C| \u2206 | |Q j | |V k \\ \u2126|", "formula_coordinates": [22.0, 177.07, 262.76, 257.86, 28.64]}, {"formula_id": "formula_62", "formula_text": "d \u22121 w \u2264 C| \u2206 |/|Q j | if w \u2208 V k \\ \u2126. Since |V k \\ \u2126| \u221a d k + 1 \u2264 |V k | |S 1 (k)| \u2264 |V k | |V k | \u2264 |V k | \u2264 | \u2206 | we see that k\u2208Qj 1 \u221a d k + 1 C| \u2206 | |Q j | |V k \\ \u2126| \u2264 C| \u2206 | |Q j | | \u2206 ||Q j | = C| \u2206 | 3 2 . (16", "formula_coordinates": [22.0, 107.64, 302.61, 392.21, 91.8]}, {"formula_id": "formula_63", "formula_text": ")", "formula_coordinates": [22.0, 499.85, 372.83, 4.15, 8.64]}, {"formula_id": "formula_64", "formula_text": "|Q j | k\u2208Qj (\u00c2 2 ) ik \u2264 \u03b4 1/4 .", "formula_coordinates": [22.0, 256.09, 425.81, 101.02, 22.21]}, {"formula_id": "formula_65", "formula_text": "1 |Q j | k\u2208Qj (\u00c2 2 ) ik \u2264 1 |Q j | + 1 |Q j | \uf8eb \uf8ed k\u2208Qj 1 \u221a d k + 1 w\u2208V k 1 d w + 1 \uf8f6 \uf8f8 \u2264 \u03b4 + 1 |Q j | \uf8eb \uf8ed k\u2208Qj 1 \u221a d k + 1 w\u2208V k 1 d w + 1 \uf8f6 \uf8f8", "formula_coordinates": [22.0, 173.37, 483.88, 266.45, 76.64]}, {"formula_id": "formula_66", "formula_text": "1 |Q j | \uf8eb \uf8ed k\u2208Qj 1 \u221a d k + 1 w\u2208V k 1 d w + 1 \uf8f6 \uf8f8 \u2264 1 |Q j | 1 C |Q j | | \u2206 | + 2 C | \u2206 | 4 + 1 |Q j | C| \u2206 | 3 2 \u2264 1 4 1 C + 2| \u2206 | C|Q j | + C| \u2206 | |Q j | | \u2206 |.", "formula_coordinates": [22.0, 128.25, 613.71, 356.69, 68.23]}, {"formula_id": "formula_67", "formula_text": "1 C + 2| \u2206 | C|Q j | + C| \u2206 | |Q j | | \u2206 | \u2264 1 4 1 C + 2 \u03b4 C + C\u03b4 | \u2206 |.", "formula_coordinates": [22.0, 189.67, 711.61, 250.23, 23.22]}, {"formula_id": "formula_68", "formula_text": "1 C + 2 \u03b4 C + C\u03b4 | \u2206 | \u2264 1 4 \u03b4 1 4 + 2\u03b4 5 4 + \u03b4 1 4 \u03b4| \u2206 | \u2264 1 4 \u03b4 1 4 + 2\u03b4 5 4 + \u03b4 1 4", "formula_coordinates": [23.0, 154.88, 98.9, 318.11, 22.31]}, {"formula_id": "formula_69", "formula_text": "1 |Q j | k\u2208Qj (\u00c2 2 ) ik \u2264 \u03b4 + 1 4 \u03b4 1 4 + 2\u03b4 5 4 + \u03b4 1 4 \u2264 3\u03b4 1 4", "formula_coordinates": [23.0, 201.21, 138.48, 209.08, 26.88]}, {"formula_id": "formula_70", "formula_text": "Proposition 5. If Ric(i, j) \u2265 k > 0 for all i \u223c j, then \u03bb 1 /2 \u2265 h G \u2265 k 2 .", "formula_coordinates": [23.0, 108.0, 279.28, 287.95, 13.47]}, {"formula_id": "formula_71", "formula_text": "G is connected, then b G = 1 n i,j (d G (i, j) \u2212 1) .(17)", "formula_coordinates": [23.0, 250.08, 483.46, 253.92, 39.25]}, {"formula_id": "formula_72", "formula_text": "( )", "formula_coordinates": [23.0, 368.34, 665.13, 9.56, 6.12]}, {"formula_id": "formula_73", "formula_text": "j := S 1 (i) \u2229 S 1 (j) \u222a {j}. If Ric(i, j) \u2264 \u22122 + \u03b4, for 0 < \u03b4 < (1 + \u03b3 max ) \u22121 , then 1 |\u2126 j | k\u2208\u2126j b(k) \u2265 \u03b4 \u22121 .", "formula_coordinates": [24.0, 106.83, 85.02, 397.17, 52.77]}, {"formula_id": "formula_74", "formula_text": "1 | \u2206 | + 1 \uf8eb \uf8ed k\u2208 \u2206 b(k) + b(j) \uf8f6 \uf8f8 .", "formula_coordinates": [24.0, 243.92, 181.65, 125.35, 34.37]}, {"formula_id": "formula_75", "formula_text": "b(k) := s,t\u2208V :s =k,t =k \u03c3 st (k) \u03c3 st", "formula_coordinates": [24.0, 247.4, 248.58, 116.0, 26.88]}, {"formula_id": "formula_76", "formula_text": "Q j := {w \u2208 Q j : \u03c3 iw > 1}.", "formula_coordinates": [24.0, 247.32, 323.65, 117.35, 9.65]}, {"formula_id": "formula_77", "formula_text": "k\u2208 \u2206 b(k) = k\u2208 \u2206 s,t\u2208V :s =k,t =k \u03c3 st (k) \u03c3 st \u2265 k\u2208 \u2206 w\u2208Qj \u03c3 iw (k) \u03c3 iw = w\u2208Qj 1 \u03c3 iw k\u2208 \u2206 \u03c3 iw (k).", "formula_coordinates": [24.0, 130.99, 402.55, 350.02, 28.64]}, {"formula_id": "formula_78", "formula_text": "k\u2208 \u2206 b(k) \u2265 w\u2208Qj 1 \u03c3 iw (\u03c3 iw \u2212 1).(18)", "formula_coordinates": [24.0, 239.63, 474.32, 264.37, 28.64]}, {"formula_id": "formula_79", "formula_text": "\u03c3 st \u2265 z\u2208Qj \u03c3 iz (j) \u03c3 iz = z\u2208Qj 1 \u03c3 iz . (19", "formula_coordinates": [24.0, 283.61, 533.41, 216.24, 26.8]}, {"formula_id": "formula_80", "formula_text": ")", "formula_coordinates": [24.0, 499.85, 540.47, 4.15, 8.64]}, {"formula_id": "formula_81", "formula_text": "1 | \u2206 | + 1 \uf8eb \uf8ed k\u2208 \u2206 b(k) + b(j) \uf8f6 \uf8f8 \u2265 1 | \u2206 | + 1 \uf8eb \uf8ed w\u2208Qj 1 \u03c3 iw (\u03c3 iw \u2212 1) + z\u2208Qj 1 \u03c3 iz \uf8f6 \uf8f8 = 1 | \u2206 | + 1 \uf8eb \uf8ed |Q j | + z\u2208Qj \\Qj 1 \u03c3 iz \uf8f6 \uf8f8 = |Q j | | \u2206 | + 1 ,", "formula_coordinates": [24.0, 147.46, 592.04, 318.28, 106.12]}], "doi": "10.1088/1742-5468/2016/06/063206"}