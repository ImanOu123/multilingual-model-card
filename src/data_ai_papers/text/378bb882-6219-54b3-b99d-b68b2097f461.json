{"title": "Efficient Computation of Robust Low-Rank Matrix Approximations in the Presence of Missing Data using the L 1 Norm", "authors": "Anders Eriksson; Anton Van Den Hengel", "pub_date": "", "abstract": "The calculation of a low-rank approximation of a matrix is a fundamental operation in many computer vision applications. The workhorse of this class of problems has long been the Singular Value Decomposition. However, in the presence of missing data and outliers this method is not applicable, and unfortunately, this is often the case in practice. In this paper we present a method for calculating the low-rank factorization of a matrix which minimizes the L 1 norm in the presence of missing data. Our approach represents a generalization the Wiberg algorithm, one of the more convincing methods for factorization under the L 2 norm. By utilizing the differentiability of linear programs, we can extend the underlying ideas behind this approach to include this class of L 1 problems as well. We show that the proposed algorithm can be efficiently implemented using existing optimization software. We also provide preliminary experiments on synthetic as well as real world data with very convincing results.", "sections": [{"heading": "Introduction", "text": "This paper paper deals with low-rank matrix approximations in the presence of missing data. That is, the following optimization problem\nmin U,V ||\u0174 (Y \u2212 U V ) ||,(1)\nwhere Y \u2208 R m\u00d7n is a matrix containing some measurements, and the unknowns are U \u2208 R m\u00d7r and V \u2208 R r\u00d7n .We let\u0175 ij represent an element of the matrix\u0174 \u2208 R m\u00d7n such that\u0175 ij is 1 if y ij is known, and 0 otherwise.\nHere || \u2022 || can in general be any matrix norm, but in this work we consider the 1-norm,\n||A|| 1 = i,j |a ij |,(2)\nin particular.\nThe calculation of a low-rank factorization of a matrix is a fundamental operation in many computer vision applications. It has been used in a wide range of problems including structure-from-motion [18], polyhedral object modeling from range images [17], layer extraction [12], recognition [19] and shape from varying illumination [11].\nIn the case where all of the elements of Y are known the singular value decomposition may be used to calculate the best approximation as measured by the L 2 norm. It is often the case in practice, however, that some of the elements of Y are unknown. It is also common that the noise in the elements of Y is such that the L 2 norm is not the most appropriate. The L 1 norm is often used to reduce sensitivity to the presence of outliers in the data. Unfortunately, it turns out that introducing missing data and using the L 1 norm instead, makes the problem (1) significantly more difficult. It is firstly a non-smooth problem, so many of the standard optimization tools available will not be applicable. Secondly, it is a non-convex problem so certificates of global optimality are in general hard to provide. And finally it can also be very computationally demanding task to solve, when applied to a real world applications the number of unknowns are typically very large.\nIn this paper we will present a method that efficiently computes a low rank approximation of a matrix in the presence of missing data, under the L 1 norm, by effectively addressing the issues of non-smoothness and computational requirements. Our proposed method should be viewed as a generalization of, one of the more successful algorithms for the L 2 case, the Wiberg method [20].", "publication_ref": ["b17", "b16", "b11", "b18", "b10", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Notation", "text": "All of the variables used in this paper are either clearly defined or should otherwise be obvious from the context in which they appear. Additionally, I n denotes a n \u00d7 n identity matrix, and \u2297 are the Hadamard and Kronecker products respectively. Upper case roman letters denote matrices and lower case ones, vectors and scalars. We also use the convention that v = vec(V ), a notation that will be used interchangeably throughout the remainder of this paper.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Previous Work", "text": "The subject of matrix approximation has been extensively studied, especially using L 2 type norms. A number of different names for the process are used in the literature, such as principal component analysis, subspace learning and matrix or subspace factorization. In this paper we describe the problem in terms of the search for a pair of matrices specifying a low rank approximation of a measurement matrix, but the approach is equally applicable to any of these equivalent problems.\nFor an outstanding survey of many of the existing methods for least L 2 -norm factorization see the work of [5]. This paper also contains a direct quantitative comparison of a number of key methods. Unfortunately, their comparison did not include the Wiberg algorithm [20]. This method, which was first proposed more than 30 years ago, has been largely misunderstood or neglected by computer vision researchers. An issue which was addressed in the excellent work of [16], effectively reintroducing the Wiberg algorithm to the vision community. It was also shown there, that on many problems the Wiberg method outperforms many of the existing, more recent methods.\nThe subject of robust matrix factorization has not received as much attention within computer vision as it has in other areas (see [1,15] for example). This is beginning to be addressed, however. A very good starting point towards a study of more robust methods, however, is the work of [9]. One of the first methods suggested was Iteratively Re-weighted Least Squares which minimizes a weighted L 2 norm. The method is unfortunately very sensitive to initialization (see [13] for more detail).\nBlack and Jepson in [4] describe a method by which it is possible to robustly recover the coefficients of a linear combination of known basis vectors that best reconstructs a particular input measurement. This might be seen as a robust method for the recovery of V given U in our context. De la Torre and Black in [9], present a robust approach to Principal Component Analysis which is capable of recovering both the basis vectors and coefficients, which is based on the Huber distance.\nThe L 1 norm was suggested by Croux and Filzmoser in [7] as a method for addressing the sensitivity of the L 2 norm to outliers in the data. The approach they proposed was based on a weighting scheme which applies only at the level of rows and columns of the measurement matrix. This means that if an element of the measurement matrix is to be identified as an outlier then its entire row or column must also be so identified.\nKe and Kanade in [13] present a factorization method based on the L 1 norm which does not suffer from the limitations of the Croux and Filzmoser approach and which is achieved through alternating convex programs. This approach is based on the observation that, under the L 1 -norm, for a fixed U , the problem (1) can be written as a linear problem in V , and vice versa. A succession of improved matrix approximations can then be obtained by solving a sequence of such linear programs, alternately fixing U and V . It was also shown here, that one can also solve the Huber-norm, an approximation of the L 1 -norm, in a similar fashion, with the difference that each subproblem now is a quadratic problem. Both these formulations do result in convex subproblems, for which efficient solvers exist, however this does not guarantee that global optimality is obtained for the original problem in the L 1 -norm.\nThe excellent work by [6] also needs mentioning. Here they apply Branch and Bound and convex under estimators to the general problem of bilinear problem, which includes (1), both under L 1 and L 2 norms. This approach are provably globally optimal, but is in general very time consuming and in practice only useful for small scale problems.", "publication_ref": ["b4", "b19", "b15", "b0", "b14", "b8", "b12", "b3", "b8", "b6", "b12", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "The Wiberg Algorithm", "text": "As previously mentioned the Wiberg algorithm is a numerical method developed for the task of low-rank matrix approximation with missing data using the L 2 -norm. We will in this section give a brief description of the underlying ideas behind this method in an attempt to motivate some of the steps taken in the derivation of our generalized version to come.\nThe Wiberg algorithm is initially based on the observation that, for a fixed U in, (1)the L 2 -norm becomes a linear, least-squares minimization problem in V ,\nmin v ||W y \u2212 W (I n \u2297 U )v|| 2 2 ,(3)\nW = diag(\u0175), with a closed-form solution given by (4).\nv * (U ) = G(U ) T G(U ) \u22121 G(U )W y,(4)\nwhere G(U ) = (I n \u2297 U ). Similar statements can also be made for fixed values of V ,\nmin u ||W y \u2212 W (V T \u2297 I m )u|| 2 2 ,(5)\nu * (V ) = F (V ) T F (V ) \u22121 F (V )W y,(6)\nand\nF (V ) = (V T \u2297 I m ).\nHere it should be mentioned that using (4) and ( 6), alternatively fixing U while updating V , and vice versa, was one of the earliest algorithms for finding matrix approximations in the presence of missing data. This process is also known as the alternated least squares (ALS) approach. The disadvantage, however, is that it has in practice been shown to converge very slowly (see [5], for example). The alternated LP and QP approaches of [13] were motivated by this method.\nContinuing with the Wiberg approach, by substituting (4) into equation ( 5) we get\nmin U ||W y \u2212 W U V * (U )|| 2 2 = ||W y \u2212 \u03c6(U )|| 2 2 ,(7)\na non-linear least squares problem in U . It is the application of the Gauss-Newton method [3] to the above problem that results in the Wiberg algorithm. The difference between the Wiberg algorithm and ALS may thus be interpreted as the fact that the former effectively computes Gauss-Newton updates while the latter carries out exact cyclic coordinate minimization.\nAs such, the Wiberg sequence of iterates U k are generated by approximating \u03c6 by its first order Taylor expansion at U k and solving the resulting subproblem\nmin \u03b4 = ||W y \u2212 \u2202\u03c6(U k ) \u2202U \u03b4|| 2 2 .(8)\nIf we let J k denote the Jacobian \u2202\u03c6(U k )/\u2202U and we can write the solution to (8) as\n\u03b4 * k = J T k J k \u22121 J T k W y,(9)\nthe well known normal equation. The next iterate U k+1 is then given by\nU k+1 = V k + \u03b4 * k .", "publication_ref": ["b3", "b4", "b12", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Linear Programming and Differentiation", "text": "Before moving on we first need to show some additional properties of linear programming. This section deals with the sensitivity of the solution to a linear program with respect to changes in the coefficients of its constraints.\nLets consider a linear program in standard, or canonical form:\nmin x\u2208R n c T x (10) s.t. Ax = b (11) x \u2265 0 (12\n)\nwith c \u2208 R n , A \u2208 R m\u00d7n and b \u2208 R m . It can furthermore be assumed, without loss of generality, that A has full rank. This class of problem has been studied extensively for over a century. As a result there exist a variety of algorithms for efficiently solving (10)- (12). Perhaps the most known well algorithm is the simplex method of [8]. It is the approach taken in that algorithm that we will follow in this section.\nFirst, from [14] (pp. 19-21), we have the following definition and theorem. Definition 3.1. Given the set of m linear equations in n unknowns, Ax = b, let B be be any nonsingular m \u00d7 m submatrix made up of columns of A. Then, if all n \u2212 m components of x not associated with columns of B are set equal to zero, the solution to the resulting set of equations is said to be a basic solution to (10)-( 12) with respect to the basis B. The components associated to with columns of B are called basic variables. Theorem 3.1. Fundamental theorem of Linear Programming Given a linear program in canonical form such as (10)-( 12), then if the problem is feasible there always exists an optimal basic solution.\nGiven that a minimizer x * of the linear program ( 10)-( 12) may been obtained using some optimization algorithm, we are interested in the sensitivity of the minimizer with respect to the coefficients of the constraints. That is, we wish to compute the partial derivatives \u2202x * /\u2202A and \u2202x * /\u2202b. The following theorem is based on the approach of Freund in [10].\nTheorem 3.2. Let B be a unique optimal basis for (10)-( 12) with minimizer x * partitioned as\nx * = x * B x * N .\nWhere x * B is the optimal basic solution and x * N = 0 the optimal non-basic solution.\nReordering the columns of A if necessary, there is a partition of A such that A = [B N ], and N are the columns of A associated with the non-basic variables x * N . Then x * is differentiable at A, b with the partial derivatives given by\n\u2202x * B \u2202B = \u2212(x * B ) T \u2297 B \u22121 (13\n)\n\u2202x * B \u2202N = 0 (14\n)\n\u2202x * B \u2202b = B \u22121 (15\n)\n\u2202x * N \u2202A = \u2202x * N \u2202b = 0.(16)\nProof. Given the set of optimal basic variables, the linear program 10 can be written\nmin x B \u2208R m c T B x B (17\n)\ns.t. Bx B = b (18) x B \u2265 0 (19) x N = 0 (20\n)\nwhere c B \u2208 R m contains the elements of c associated with the basic variables only. Since B is of full rank and we know it is an optimal and feasible basis for (10)-( 12) it follows that the solution to Bx B = b is both feasible (x B \u2265 0) and optimal. The above statements represent the foundation of the simplex method. Now, since by assumption, the basis B is unique we have that\nx * B = B \u22121 b (21) x * N = 0 (22)\nand as B is a smooth bijection from R m onto itself, then x * is differentiable with respect to the coefficients in A and b. Differentiation of ( 13) becomes\n\u2202x * B \u2202B = \u2202 \u2202B B \u22121 b (23) = b T \u2297 I m \u2202B \u22121 \u2202B (24) = \u2212 b T \u2297 I m B \u2212T \u2297 B \u22121 (25) = \u2212(x * B ) T \u2297 B \u22121 .(26)\nEquations ( 14)-( 16) follow trivially from the differentiation of ( 21) and ( 22).\nIn conclusion, by combining the results of theorem 3.1 we can write the derivatives as\n\u2202x * \u2202A = \u2212(x * B ) T \u2297 B \u22121 0 m\u00d7(n\u2212m)m 0 (n\u2212m)\u00d7m 2 0 (n\u2212m)\u00d7(n\u2212m)m (27\n)\n\u2202x * \u2202b = B \u22121 0 (n\u2212m)\u00d7m .(28)", "publication_ref": ["b11", "b7", "b13", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "The L 1 -Wiberg Algorithm", "text": "In this section we will present the derivation of a generalization of the Wiberg algorithm to the L 1 -norm. We will follow a similar approach to the derivation of the standard Wiberg algorithm. That is, by rewriting the problem as a function of U only, then linearizing it, solving the resulting subproblem and updating the current iterate using the minimizer of said subproblem.\nOur starting point for the derivation our generalization of the Wiberg algorithm is again the minimization problem\nmin U,V ||\u0174 (Y \u2212 U V ) || 1 . (29\n)\nFollowing the approach of section 2.1 we first note that for fixed U and V it is possible to rewrite the optimization problem (29) as\nv * (U ) = arg min v ||W y \u2212 W (I n \u2297 U )v|| 1 ,(30)\nu * (V ) = arg min u ||W y \u2212 W (V T \u2297 I m )u|| 1 ,(31)\nboth linear problem in V and U respectively. Substituting (30) into equation ( 31) we obtain\nmin U f (U ) = ||W y \u2212 W U V * (U )|| 1 = = ||W y \u2212 \u03c6 1 (U )|| 1 .(32)\nUnfortunately, this is not a least squares minimization problem so the Gauss-Newton algorithm is not applicable. Nor does v * (U ) have an easily differentiable, closed-form solution, but the results of the previous section allow us to continue in a similar fashion.\nIt can be shown that, by letting v = v + \u2212 v \u2212 , an equivalent formulation of (30) is\nmin v + ,v \u2212 ,t,s [ 0 0 1 T 0 ] v + v \u2212 t s (33) s.t. \u2212G(U ) G(U ) \u2212I G(U ) \u2212G(U ) \u2212I I A(U ) v + v \u2212 t s = \u2212W y W y b (34) v + , v \u2212 , t, s \u2265 0 (35) v + , v \u2212 \u2208 R rn , t \u2208 R mn , s \u2208 R 2mn . (36)\nNote that (33)-(36) here is a linear program in canonical form, allowing us to apply theorem 3.1 directly. Let V * (U ) denote the optimal basic solution of (33)-(36). Assuming that the prerequisites of theorem 3.1 hold, then V * (U ) is differentiable and we can compute the Jacobian of the nonlinear function \u03c6 1 (U ) = W U V * (U ). Using (27) and applying the chain-rule, we obtain\n\u2202G \u2202U = (I nr \u2297 W ) (T r,n \u2297 I m ) (vec(I n ) \u2297 I mr )(37) \u2202A \u2202U = \u2212 \u2202G \u2202U \u2202G \u2202U 0 \u2202G \u2202U \u2212 \u2202G \u2202U 0 0 (38) \u2202v \u2202U = Q (v * B ) T \u2297 B \u22121 Q T \u2297 I 2mn \u2202A \u2202U (39)\nHere T m,n denotes the mn \u00d7 mn matrix for which T m,n vec(A) = vec(A T ), and Q \u2208 R mn\u00d7m is obtained by removing the columns corresponding to the non-basic variables of x * from the identity matrix I mn . Combining the above expressions we arrive at\nJ(U ) = \u2202 \u2202U (W U V * (U )) = W G(U ) \u2202v \u2202U + (v * B ) T \u2297 W (I n \u2297 T r,n \u2297 I m ) (vec(I n ) \u2297 I mr ) (40)\nThe Gauss-Newton method, in the least squares case, works by linearizing the non-linear part and solving the resulting subproblem. By equation (40) the same can be done for \u03c6 1 (U ). Linearizing W y \u2212 W U V * (U ) by its first order Taylor expansion results in the following approximation of (32\n) around U k f (\u03b4) = ||W y \u2212 J(U k )(\u03b4 \u2212 u k )|| 1 .\n(41)\nMinimizing ( 41)\nmin \u03b4 ||W y \u2212 J(U )(\u03b4 \u2212 u)|| 1 (42\n)\nis again a linear problem, but now in \u03b4.\nmin \u03b4,t [ 0 1 T ] [ \u03b4 t ] (43) s.t. \u2212J(U )\u2212I J(U )\u2212I [ \u03b4 t ] = \u2212(W y\u2212W vec(U V * )) W y\u2212W vec(U V * ) (44) ||\u03b4|| 1 \u2264 \u00b5 (45) \u03b4 \u2208 R mr , t \u2208 R mn . (46\n)\nLet \u03b4 * k be the minimizer of ( 43)-( 46), with U = U k , then the update rule for our proposed method is simply\nU k+1 = U k + \u03b4 * k . (47\n)\nNote that in (44) we have added the constraint ||\u03b4|| 1 \u2264 \u00b5. This is done as a trust region strategy to limit the step sizes that can be taken at each iteration so to ensure a nonincreasing sequence of iterates. See algorithm 1 below for details on how the step length \u00b5 is handled.\nWe are now ready to present our complete L 1 -Wiberg method in Algorithm 1.\nProper initialization is a crucial issue for any iterative algorithm and can greatly affect its performance. Obtaining this initialization is highly problem dependent, for certain applications good initial estimates of the solution are readily available and for others finding a sufficiently good U 0 might be considerably more demanding. In this work we either initialized our algorithm randomly or through the rankr truncation of the singular value decomposition of\u0174 \u2297 Y .\nFinally, a comment on the convergence of the proposed algorithm. We know that, owing to the use of trust region setting, it can be shown that algorithm 1 will produce a sequence of iterates {U 0 , ..., U k } with non-increasing function values, f (U 0 ) \u2265 ... \u2265 f (U k ) \u2265 0}. We currently have no proof, however, that the assumptions of theorem 3.1 always hold, which is a requirement for the differentiability of V * (U ). Unfortunately this non-smoothness prevents the application of the standard tools for proving convergence to a local minima. In our considerable experimentation, however, we have never observed an instance in which the algorithm does not converge at a local minima.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In this section we present a number of experiments carried out to evaluate our proposed method. These include real and synthetic tests.\nWe have evaluated the performance of the L 1 -Wiberg algorithm method against that of two of the state of the art methods presented by Ke and Kanade in [13], (alternated LP and alternated QP).\nAll algorithms were implemented in Matlab. Linear and quadratic optimization subproblems were solved using linprog and quadprog respectively.\nAlgorithm 1: L 1 -Wiberg Algorithm input : U 0 , 1 > \u03b7 2 > \u03b7 1 > 0 and c > 1 k = 0 ; 1 repeat 2\nCompute the Jacobian of \u03c6 1 = J(U k ) using 3 (37)-(40) ; Solve the subproblem (43)-( 46) to obtain \u03b4 * k ;\n4 \nLet gain = f (U k ) \u2212 f (U k + \u03b4 * ) f (U k ) \u2212f (U k + \u03b4 * ) ; 5 if gain \u2265 then 6 U k+1 = U k + \u03b4 * ;\n16\nRemarks.\n\u2022 Lines 9-14 are a standard implementation for dealing with \u00b5, the size of the trust region in the subproblem (43)-( 46).see for instance [2] for further details.\n\u2022 Typical parameter values used were \u03b7 1 = 1 4 , \u03b7 2 = 3 4 , = 10 \u22123 and c = 2. \u2022 In the current implementation we use a simple termination criteria. Iteration is stopped when the reduction in function value f (U k ) \u2212 f (U k+1 ) is deemed sufficiently small (\u2264 10 \u22126 ).", "publication_ref": ["b12", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Synthetic Data", "text": "The aim of the experiments in this section was to empirically obtain a better understanding of the following properties of each of the tested algorithm, resulting error, rate of convergence, execution time and the computational requirements.\nFor the synthetic tests a set of randomly created measurement matrices were generated. The elements of the measurement matrix Y were drawn from a uniform distribution between [\u22121, 1]. Then 20% of the elements were chosen at random and designated as missing by setting the corresponding entry in the matrix\u0174 to 0. In addition, to simulate outliers, uniformly distributed noise over [\u22125, 5] were added to 10% of the elements in Y .\nSince the alternated QP method of Ke and Kanade relies on quadratic programing and as such does not scale as well as linear programs we deliberately kept the synthetic problems relatively small, with m = 7, n = 12 and r = 3.\nFigure 1 shows a histogram of the error produced by each algorithm on 100 synthetic matrices, created as described above. It can be seen in this figure that our proposed method clearly outperforms the other two. But what should also be noted here is the poor performance of the alternated linear program approach. Even though it can easily be shown that this algorithm will produce a non-increasing sequence of iterates, there is no guarantee that it will converge to a local minima. This is what we believe is actually occurring in these tests. The alternated linear program converges to a point that is not a local minima, typically after only a small number of iterations. Owing to its poor performance we have excluded this method from the remainder of experiments.\nNext we examine the convergence rate of the algorithms. A typical instance of the error convergence from from both the AQP and L 1 -Wiberg algorithms, applied to one of the synthetic problems, can be seen in figure 2. These figures are not intended to show the quality of the final solution, but rather how it quickly it is obtained by the competing methods.\nFigure 3 depicts the performance of the algorithms in 100 synthetic tests and is again intended to show convergence rate rather than the final error. Note the independent scaling of each set of results and the fact that the Y-axis is on a logarithmic scale. Again it is obvious that the L 1 Wiberg algorithm significantly outperforms the alternated quadratic programming approach. It can be seen that the latter method has a tendency to flatline, that is to converge very slowly after an initial period of more rapid progress. This is a behavior that has also been observed for alternated approaches in the L 2 instance, see [5].\nTable 1 summarizes the same set of synthetic tests. What should be noted here is the low average error produced by our method, the long execution time of the alternated quadratic program approach and the poor results obtained by alternated linear program method.\nThe results of these experiments, although confined to smaller scale problems, do indeed demonstrate the promise of our suggested algorithm.", "publication_ref": ["b4"], "figure_ref": ["fig_1"], "table_ref": ["tab_0"]}, {"heading": "Structure from Motion", "text": "Next we present an experiment on a real world application, namely structure from motion.\nWe use the well known dinosaur sequence, available from http://www.robots.ox.ac.uk/\u02dcvgg/, contain-Algorithm Alt. LP [13]  ing projections of 319 points tracked over 36 views. Now, finding the full 3D-reconstruction of this scene can be posed as a low-rank matrix approximation task. In addition, as we are considering robust approximation in this work, we also included outliers to the problem by adding uniformly distributed noise [\u221250, 50] to 10% of the tracked points. We applied our proposed method to this problem, initializing it using truncated singular value decomposition as described in the previous section. For comparison we also include the result from running the standard Wiberg algorithm. Attempts to evaluate the AQP method on this on the same data were abandoned when the execution time exceeded several of hours.\nThe residual for the visible points of the two different matrix approximations is given in figure 4. The L 2 norm ap- proximation seems to be strongly affected by the presence of outliers in the data. The error in the matrix approximation appears to be evenly distributed among all the elements of the residual. In the L 1 case this does not seem to occur. Instead the reconstruction error is concentrated to a few elements of the residual. The root mean square error of the inliers only, as well as execution times are given in table 2 The resulting reconstructed scene can be seen in figure 5.", "publication_ref": ["b12"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper we have studied the problem of low-rank matrix approximation in the presence of missing data. We have proposed a method for solving this task under the robust L 1 norm which can be interpreted as a generalization of the standard Wiberg algorithm. We have also shown through a number of experiments, on both synthetic and real world data, that the L 1 Wiberg method proposed is both practical and efficient and performs very well in comparison to other existing methods.\nFurther work in the area will include an investigation into the convergence properties of the algorithm and particularly the search for a mathematical proof that the algorithm indeed converges to a local minima. Jointly, the issue of non-differentiable points of \u03c6 1 should also be further investigated.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "Alt. QP [13] 2. Results from the dinosaur sequence with 10% outliers.\nFigure 5. Images from the dinosaur sequence, and the resulting reconstruction using our proposed method.", "publication_ref": ["b12"], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "This research was supported under Australian Research Council's Discovery Projects funding scheme (project DP0988439).", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Neural networks and principal component analysis: learning from examples without local minima", "journal": "Neural Netw", "year": "1989", "authors": "P Baldi; K Hornik"}, {"ref_id": "b1", "title": "Nonlinear Programming, Theory and Algorithms", "journal": "Wiley", "year": "1993", "authors": "M Bazaraa; H Sherali; C Shetty"}, {"ref_id": "b2", "title": "Numerical Methods for Least Squares Problems. SIAM", "journal": "", "year": "1995", "authors": "A Bjorck"}, {"ref_id": "b3", "title": "Eigentracking: Robust matching and tracking of articulated objects using a view-based representation", "journal": "In International Journal of Computer Vision", "year": "1998", "authors": "M J Black; A D Jepson"}, {"ref_id": "b4", "title": "Damped newton algorithms for matrix factorization with missing data", "journal": "", "year": "2005", "authors": "A M Buchanan; A W Fitzgibbon"}, {"ref_id": "b5", "title": "Globally optimal bilinear programming for computer vision applications", "journal": "", "year": "2008", "authors": "M K Chandraker; D J Kriegman"}, {"ref_id": "b6", "title": "Robust factorization of a data matrix", "journal": "", "year": "1998", "authors": "C Croux; P Filzmoser"}, {"ref_id": "b7", "title": "Linear Programming and Extensions", "journal": "Princeton University Press", "year": "1998", "authors": "G Dantzig"}, {"ref_id": "b8", "title": "A framework for robust subspace learning", "journal": "Int. J. Comput. Vision", "year": "2003", "authors": "F De La Torre; M J Black"}, {"ref_id": "b9", "title": "The sensitivity of a linear program solution to changes in matrix coefficients", "journal": "", "year": "1984", "authors": "R M Freund"}, {"ref_id": "b10", "title": "Photometric stereo under a light source with arbitrary motion", "journal": "Journal of the Optical Society of America A", "year": "", "authors": "H Hayakawa"}, {"ref_id": "b11", "title": "A subspace approach to layer extraction. Computer Vision and Pattern Recognition", "journal": "IEEE Computer Society Conference on", "year": "2001", "authors": "Q Ke; T Kanade"}, {"ref_id": "b12", "title": "Robust L1 norm factorization in the presence of outliers and missing data by alternative convex programming", "journal": "", "year": "2005", "authors": "Q Ke; T Kanade"}, {"ref_id": "b13", "title": "Linear and Nonlinear Programming", "journal": "Springer", "year": "2008", "authors": "D G Luenberger; Y Ye"}, {"ref_id": "b14", "title": "A simplified neuron model as a principal component analyzer", "journal": "Journal of Mathematical Biology", "year": "1982", "authors": "E Oja"}, {"ref_id": "b15", "title": "On the Wiberg algorithm for matrix factorization in the presence of missing components", "journal": "Int. J. Comput. Vision", "year": "2007", "authors": "T Okatani; K Deguchi"}, {"ref_id": "b16", "title": "Principal component analysis with missing data and its application to polyhedral object modeling", "journal": "", "year": "2001", "authors": "H Shum; K Ikeuchi; R Reddy"}, {"ref_id": "b17", "title": "Shape and motion from image streams under orthography: a factorization method", "journal": "Int. J. Comput. Vision", "year": "1992", "authors": "C Tomasi; T Kanade"}, {"ref_id": "b18", "title": "Eigenfaces for recognition", "journal": "Journal of Cognitive Neuroscience", "year": "1991", "authors": "M Turk; A Pentland"}, {"ref_id": "b19", "title": "Computation of principal components when data are missing", "journal": "", "year": "1976", "authors": "T Wiberg"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 .1Figure 1. A histogram representing the frequency of different magnitudes of error in the estimate generated by each of the methods. [Frequency vs. Error]", "figure_data": ""}, {"figure_label": "23", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 .Figure 3 .23Figure 2. Plots showing the norm of the residual at each iteration of two randomly generated tests for both the L1 Wiberg and alternated quadratic programming algorithms. [Residual norm vs. Iteration]", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 .4Figure 4. Resulting residuals using the standard Wiberg algorithm (top), and our proposed L1-Wiberg algorithm (bottom).", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Alt. QP[13] Wiberg L 1 (Alg.1) The alternated QP algorithm was terminated after 200 iterations and 400 solved quadratic programs. The averaged results from running 100 synthetic experiments.", "figure_data": "Error (L 1 )4.602.291.01Execution Time (sec)0.1693.571.51# Iterations4.72177.64  *21.77# LP/QP solved9.44355.28  *24.13Time per LP/QP0.0160.264  *0.061"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Wiberg L 2 [20] Wiberg L 1 (Alg. 1)", "figure_data": "RMS Error of Inliers-2.0290.862Execution Time>4 hrs3 min 2 sec17 min 44 secTable"}], "formulas": [{"formula_id": "formula_0", "formula_text": "min U,V ||\u0174 (Y \u2212 U V ) ||,(1)", "formula_coordinates": [1.0, 108.82, 583.33, 177.54, 14.58]}, {"formula_id": "formula_1", "formula_text": "||A|| 1 = i,j |a ij |,(2)", "formula_coordinates": [1.0, 121.81, 695.07, 164.55, 19.91]}, {"formula_id": "formula_2", "formula_text": "min v ||W y \u2212 W (I n \u2297 U )v|| 2 2 ,(3)", "formula_coordinates": [2.0, 368.63, 482.22, 176.48, 16.21]}, {"formula_id": "formula_3", "formula_text": "v * (U ) = G(U ) T G(U ) \u22121 G(U )W y,(4)", "formula_coordinates": [2.0, 339.19, 529.22, 205.92, 13.33]}, {"formula_id": "formula_4", "formula_text": "min u ||W y \u2212 W (V T \u2297 I m )u|| 2 2 ,(5)", "formula_coordinates": [2.0, 366.21, 586.84, 178.9, 16.21]}, {"formula_id": "formula_5", "formula_text": "u * (V ) = F (V ) T F (V ) \u22121 F (V )W y,(6)", "formula_coordinates": [2.0, 338.72, 606.41, 206.39, 13.33]}, {"formula_id": "formula_6", "formula_text": "F (V ) = (V T \u2297 I m ).", "formula_coordinates": [2.0, 325.74, 630.89, 85.51, 11.23]}, {"formula_id": "formula_7", "formula_text": "min U ||W y \u2212 W U V * (U )|| 2 2 = ||W y \u2212 \u03c6(U )|| 2 2 ,(7)", "formula_coordinates": [3.0, 63.95, 144.49, 222.41, 16.66]}, {"formula_id": "formula_8", "formula_text": "min \u03b4 = ||W y \u2212 \u2202\u03c6(U k ) \u2202U \u03b4|| 2 2 .(8)", "formula_coordinates": [3.0, 101.69, 302.72, 184.67, 22.31]}, {"formula_id": "formula_9", "formula_text": "\u03b4 * k = J T k J k \u22121 J T k W y,(9)", "formula_coordinates": [3.0, 108.82, 368.84, 177.54, 14.99]}, {"formula_id": "formula_10", "formula_text": "U k+1 = V k + \u03b4 * k .", "formula_coordinates": [3.0, 106.44, 406.62, 69.84, 12.55]}, {"formula_id": "formula_11", "formula_text": "min x\u2208R n c T x (10) s.t. Ax = b (11) x \u2265 0 (12", "formula_coordinates": [3.0, 133.84, 537.04, 152.52, 44.37]}, {"formula_id": "formula_12", "formula_text": ")", "formula_coordinates": [3.0, 282.21, 572.77, 4.15, 8.64]}, {"formula_id": "formula_13", "formula_text": "x * = x * B x * N .", "formula_coordinates": [3.0, 463.6, 343.3, 51.4, 18.18]}, {"formula_id": "formula_14", "formula_text": "\u2202x * B \u2202B = \u2212(x * B ) T \u2297 B \u22121 (13", "formula_coordinates": [3.0, 389.06, 438.7, 151.9, 23.89]}, {"formula_id": "formula_15", "formula_text": ")", "formula_coordinates": [3.0, 540.96, 447.34, 4.15, 8.64]}, {"formula_id": "formula_16", "formula_text": "\u2202x * B \u2202N = 0 (14", "formula_coordinates": [3.0, 389.06, 463.18, 151.9, 23.89]}, {"formula_id": "formula_17", "formula_text": ")", "formula_coordinates": [3.0, 540.96, 471.81, 4.15, 8.64]}, {"formula_id": "formula_18", "formula_text": "\u2202x * B \u2202b = B \u22121 (15", "formula_coordinates": [3.0, 389.06, 487.66, 151.9, 23.89]}, {"formula_id": "formula_19", "formula_text": ")", "formula_coordinates": [3.0, 540.96, 496.29, 4.15, 8.64]}, {"formula_id": "formula_20", "formula_text": "\u2202x * N \u2202A = \u2202x * N \u2202b = 0.(16)", "formula_coordinates": [3.0, 353.59, 512.14, 191.52, 23.89]}, {"formula_id": "formula_21", "formula_text": "min x B \u2208R m c T B x B (17", "formula_coordinates": [3.0, 385.05, 573.57, 155.91, 16.81]}, {"formula_id": "formula_22", "formula_text": ")", "formula_coordinates": [3.0, 540.96, 575.46, 4.15, 8.64]}, {"formula_id": "formula_23", "formula_text": "s.t. Bx B = b (18) x B \u2265 0 (19) x N = 0 (20", "formula_coordinates": [3.0, 400.86, 594.51, 144.25, 39.54]}, {"formula_id": "formula_24", "formula_text": ")", "formula_coordinates": [3.0, 540.96, 624.72, 4.15, 8.64]}, {"formula_id": "formula_25", "formula_text": "x * B = B \u22121 b (21) x * N = 0 (22)", "formula_coordinates": [4.0, 133.84, 106.96, 152.52, 27.64]}, {"formula_id": "formula_26", "formula_text": "\u2202x * B \u2202B = \u2202 \u2202B B \u22121 b (23) = b T \u2297 I m \u2202B \u22121 \u2202B (24) = \u2212 b T \u2297 I m B \u2212T \u2297 B \u22121 (25) = \u2212(x * B ) T \u2297 B \u22121 .(26)", "formula_coordinates": [4.0, 85.24, 187.88, 201.12, 71.17]}, {"formula_id": "formula_27", "formula_text": "\u2202x * \u2202A = \u2212(x * B ) T \u2297 B \u22121 0 m\u00d7(n\u2212m)m 0 (n\u2212m)\u00d7m 2 0 (n\u2212m)\u00d7(n\u2212m)m (27", "formula_coordinates": [4.0, 62.44, 331.14, 219.77, 24.15]}, {"formula_id": "formula_28", "formula_text": ")", "formula_coordinates": [4.0, 282.21, 339.77, 4.15, 8.64]}, {"formula_id": "formula_29", "formula_text": "\u2202x * \u2202b = B \u22121 0 (n\u2212m)\u00d7m .(28)", "formula_coordinates": [4.0, 62.44, 359.04, 223.92, 24.15]}, {"formula_id": "formula_30", "formula_text": "min U,V ||\u0174 (Y \u2212 U V ) || 1 . (29", "formula_coordinates": [4.0, 106.59, 534.5, 175.63, 14.58]}, {"formula_id": "formula_31", "formula_text": ")", "formula_coordinates": [4.0, 282.21, 534.82, 4.15, 8.64]}, {"formula_id": "formula_32", "formula_text": "v * (U ) = arg min v ||W y \u2212 W (I n \u2297 U )v|| 1 ,(30)", "formula_coordinates": [4.0, 75.11, 604.79, 211.25, 16.21]}, {"formula_id": "formula_33", "formula_text": "u * (V ) = arg min u ||W y \u2212 W (V T \u2297 I m )u|| 1 ,(31)", "formula_coordinates": [4.0, 67.05, 624.63, 219.32, 16.21]}, {"formula_id": "formula_34", "formula_text": "min U f (U ) = ||W y \u2212 W U V * (U )|| 1 = = ||W y \u2212 \u03c6 1 (U )|| 1 .(32)", "formula_coordinates": [4.0, 79.31, 683.24, 207.05, 30.6]}, {"formula_id": "formula_35", "formula_text": "min v + ,v \u2212 ,t,s [ 0 0 1 T 0 ] v + v \u2212 t s (33) s.t. \u2212G(U ) G(U ) \u2212I G(U ) \u2212G(U ) \u2212I I A(U ) v + v \u2212 t s = \u2212W y W y b (34) v + , v \u2212 , t, s \u2265 0 (35) v + , v \u2212 \u2208 R rn , t \u2208 R mn , s \u2208 R 2mn . (36)", "formula_coordinates": [4.0, 320.84, 167.94, 224.27, 98.33]}, {"formula_id": "formula_36", "formula_text": "\u2202G \u2202U = (I nr \u2297 W ) (T r,n \u2297 I m ) (vec(I n ) \u2297 I mr )(37) \u2202A \u2202U = \u2212 \u2202G \u2202U \u2202G \u2202U 0 \u2202G \u2202U \u2212 \u2202G \u2202U 0 0 (38) \u2202v \u2202U = Q (v * B ) T \u2297 B \u22121 Q T \u2297 I 2mn \u2202A \u2202U (39)", "formula_coordinates": [4.0, 323.59, 372.77, 221.53, 71.27]}, {"formula_id": "formula_37", "formula_text": "J(U ) = \u2202 \u2202U (W U V * (U )) = W G(U ) \u2202v \u2202U + (v * B ) T \u2297 W (I n \u2297 T r,n \u2297 I m ) (vec(I n ) \u2297 I mr ) (40)", "formula_coordinates": [4.0, 322.04, 523.04, 223.07, 37.06]}, {"formula_id": "formula_38", "formula_text": ") around U k f (\u03b4) = ||W y \u2212 J(U k )(\u03b4 \u2212 u k )|| 1 .", "formula_coordinates": [4.0, 321.31, 632.75, 165.5, 32.63]}, {"formula_id": "formula_39", "formula_text": "min \u03b4 ||W y \u2212 J(U )(\u03b4 \u2212 u)|| 1 (42", "formula_coordinates": [4.0, 360.57, 701.68, 180.39, 14.66]}, {"formula_id": "formula_40", "formula_text": ")", "formula_coordinates": [4.0, 540.96, 702.0, 4.15, 8.64]}, {"formula_id": "formula_41", "formula_text": "min \u03b4,t [ 0 1 T ] [ \u03b4 t ] (43) s.t. \u2212J(U )\u2212I J(U )\u2212I [ \u03b4 t ] = \u2212(W y\u2212W vec(U V * )) W y\u2212W vec(U V * ) (44) ||\u03b4|| 1 \u2264 \u00b5 (45) \u03b4 \u2208 R mr , t \u2208 R mn . (46", "formula_coordinates": [5.0, 70.08, 97.11, 216.28, 67.49]}, {"formula_id": "formula_42", "formula_text": ")", "formula_coordinates": [5.0, 282.21, 155.63, 4.15, 8.64]}, {"formula_id": "formula_43", "formula_text": "U k+1 = U k + \u03b4 * k . (47", "formula_coordinates": [5.0, 122.72, 211.14, 159.49, 12.69]}, {"formula_id": "formula_44", "formula_text": ")", "formula_coordinates": [5.0, 282.21, 213.53, 4.15, 8.64]}, {"formula_id": "formula_45", "formula_text": "Algorithm 1: L 1 -Wiberg Algorithm input : U 0 , 1 > \u03b7 2 > \u03b7 1 > 0 and c > 1 k = 0 ; 1 repeat 2", "formula_coordinates": [5.0, 314.84, 77.77, 178.77, 51.15]}, {"formula_id": "formula_46", "formula_text": "Let gain = f (U k ) \u2212 f (U k + \u03b4 * ) f (U k ) \u2212f (U k + \u03b4 * ) ; 5 if gain \u2265 then 6 U k+1 = U k + \u03b4 * ;", "formula_coordinates": [5.0, 322.31, 166.38, 228.06, 49.96]}], "doi": ""}