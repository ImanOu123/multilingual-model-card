{"title": "Looking Around the Corner using Transient Imaging", "authors": "Ahmed Kirmani; Tyler Hutchison; James Davis; Ramesh Raskar", "pub_date": "", "abstract": "We show that multi-path analysis using images from a timeof-flight (ToF) camera provides a tantalizing opportunity to infer about 3D geometry of not only visible but hidden parts of a scene. We provide a novel framework for reconstructing scene geometry from a single viewpoint using a camera that captures a 3D time-image I(x, y, t) for each pixel. We propose a framework that uses the time-image and transient reasoning to expose scene properties that may be beyond the reach of traditional computer vision. We corroborate our theory with free space hardware experiments using a femtosecond laser and an ultrafast photo detector array. The ability to compute the geometry of hidden elements, unobservable by both the camera and illumination source, will create a range of new computer vision opportunities.", "sections": [{"heading": "Introduction", "text": "Camera-based 2D intensity images, I(x, y), have long been used to observe and interpret scenes. New sensors and algorithms for scene understanding will clearly benefit many application areas such as robotics, industrial applications, user interfaces and surveillance. This paper introduces a novel framework called Transient Light Transport which allows new properties of scenes to be observed and interpreted. In a traditional camera, the light incident at a pixel is integrated along angular, temporal and wavelength dimensions during the exposure time to record a single intensity value. Distinct scenes may result in identical projections (images) and, hence, identical pixel values. Thus, it is challenging to estimate scene properties which are not directly observable. Steady-state light transport assumes an equilibrium in global illumination. In a room-sized environment, a microsecond exposure (integration) time is long enough for a light impulse to fully traverse all the possible multi-paths introduced due to inter-reflections between scene elements and reach steady state. Traditional video cameras sample light very slowly compared to the time scale at which the transient properties of light come into play. In our transient light transport framework, light takes a finite amount of time to travel from one scene point to the other. Recent advances in ultra-high speed imaging have made it possible to sample light as it travels 0.3 millimeter in 1 picosecond. The dynamics of transient light transport in response to a single ray impulse illumination can be extremely complex, even for a simple scene. Unlike a traditional 2D pixel, which measures the total number of photons, transient light transport measures photon arrival rate as a function of time. In Section 3, we propose a transient imaging camera model which samples incident light continuously. In Section 4, we propose the theoretical framework called inverse transient light transport for estimating the geometry of scenes that may contain elements occluded from both the camera and illumination. Section 5 describes our hardware prototype comprising a femtosec- axes. With higher dimensional light capture, we expand the horizons of scene understanding. Our paper uses LIDAR-like imaging hardware, but, in contrast, we exploit the multi-path information which is rejected in both LIDAR and OCT.\nond laser and a directionally sensitive, picosecond accurate photo sensor array. Using our prototype, we demonstrate all the key functionalities required in a transient imaging camera: geometry, photometry, multi-bounce observations and free-space functioning. Finally, in Section 6, we experimentally demonstrate how our proposed imaging model enables novel scene understanding which allows us to look around the corner without any device in the line of sight (Figure 1).", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Related work", "text": "Global light transport: Light often follows a complex path between the emitter and sensor. A description of steady-state light transport in a scene is referred to as the rendering equation [1]. Extensions have been described to include time in light transport [2]. In [3], Raskar and Davis proposed inverse analysis using a 5D time-light transport matrix to recover geometric and photometric scene parameters. In addition, Smith et. al. [4] proposed a modification of the rendering equation via a transient rendering framework. Accurate measurement of physical scene properties is called inverse-rendering [5]. Complex models have been developed for reconstructing specular [6], transparent [7], Lambertian [8] scenes and joint lighting and reflectance [9]. Recent work in image-based modeling and computational photography has also shown several methods for capturing steady-state light transport [10]. Our work is highly influenced by the following pioneering efforts in steady-state global light transport analysis. Nayar et. al. decomposed an image into its direct and indirect components under the assumption that the scene has no high-frequency components [11]. Seitz et. al. [12] have decomposed images into multi-bounce components under the assumption that the scene is Lambertian. Although the dual photography approach [10] can see an object hidden from a camera, it requires a projector in the object's line of sight. Our method exploits transient, rather than steady-state transport, to estimate more challenging scene properties.\nLIDAR and Time-gated imaging: LIght Detection And Ranging systems modulate light, typically on the order of nanoseconds, and measure the phase of the reflected signal to determine depth [13]. Flash LIDAR systems use a 2D imager to provide fast measurement of full depth maps [14,15]. Importantly, a number of companies (Canesta, MESA, 3DV, PMD) are pushing this technology towards consumer price points. The quality of phase estimation can be improved by simulating the expected shape of the reflected signal or estimating the effect of ambient light [16]. Separately detecting multiple peaks in the sensor response can allow two surfaces, such as a forest canopy and a ground plane, to be detected, and waveform analysis can detect surface discontinuities [17]. Time-gated imaging captures I(x, y, t \u03b4 ) by integrating the reflected pulse of light over extremely short windows. Multiple captures at incremental time windows, t \u03b4 , allow the time image I(x, y, t) to be captured at up to 100 picosecond accuracy. Nanosecond windows are used for imaging tanks at the range of kilometers and picosecond gating allows imaging in turbid water.\nFemtosecond Imaging: Optical coherence tomography (OCT) [18], an interferometric technique, and two-photon microscopy [19], using fluorescence, allow high-quality, micrometer-resolution 3D imaging of biological tissue. Both these methods are based on pulsed femtosecond illumination. Experiments in this paper are the first attempt at free-space use of femto-laser illumination in contrast to their established use in optical fibers or millimeter-scale biological samples. Streak cameras are ultrafast photonic recorders which deposit photons across a spatial dimension, rather than integrating them in a single pixel. Picosecond streak cameras have been available for decades [20]. Modern research systems can function in the attosecond range [21]. All the existing methods based on time sampling of light make no use of global light transport reasoning to infer scene characteristics. They instead image in a single direction time-gated window to improve SNR and reject multi-path scattering. This paper shows that complex global reasoning about scene content is possible given a measured multi-path time profile.", "publication_ref": ["b0", "b1", "b2", "b3", "b4", "b5", "b6", "b7", "b8", "b9", "b10", "b11", "b9", "b12", "b13", "b14", "b15", "b16", "b17", "b18", "b19", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Transient Light Transport", "text": "The theory of light transport describes the interaction of light with a scene. Incident illumination causes the first bounce. This direct bounce is followed by a complex pattern of inter-reflections whose dynamics are governed by the scene geometry and material properties of the scene elements. At any time instant we only observe a projection of the complete set of scene light rays. This projection only comprises the rays that are directed towards the camera. We consider a scene S composed of M small planar facets p 1 , . . . p M with 3D positions\nz i \u2208 R 3 . Let Z = [z 1 , . . . , z M ].\nDefine relative visibility between patches, v ij = v ji = 0 or 1 depending on whether or not patch p i is occluded from p j . Let D = [d ij ] be the Euclidean distance matrix containing pairwise distances. For analytical convenience, we consider the camera (observer) and illumination (source) as a single patch denoted by p 0 . It is straightforward to extend the following analysis to include multiple sources and the camera at an arbitrary position in the scene. We assume that the scene is static and material properties are constant over sub-nanosecond imaging intervals. Transient Imaging Camera: Our model comprises a generalized sensor and a pulsed illumination source. Each sensor pixel observes a unique patch in the scene. It also continuously time samples the incoming irradiance, creating a 3D time image, I(x i , y i , t). The pixel at sensor position (x i , y i ) observes the patch p i over time. The pulsed illumination source generates arbitrarily short duration and directional impulse rays. The direction of an impulse ray aimed at patch p i is specified by (\u03b8 i , \u03c6 i ). The sensor and illumination are synchronized for precise measurement of Time Difference Of Arrival (TDOA). Space Time Impulse Response of the scene S denoted by STIR(S) is a collection of time images, each captured with an impulse ray illuminating a single scene patch p j . This is a 5D function: STIR(x i , y i , \u03b8 j , \u03c6 j , t). We measure STIR using the following (Figure 3):\n1 For each patch p j : j = 1, . . . , M. 1a Illuminate p j with an impulse ray (\u03b8 j , \u03c6 j ). 1b Record time image {I(x i , y i , t\n) : i = 1 . . . M; t = 0 . . . T } = STIR(x i , y i , \u03b8 j , \u03c6 j , t).\nIn still scenes with static material properties, light transport is a Linear and Time Invariant (LTI) process. Hence, the STIR of the scene characterizes its appearance under any general illumination from a given camera view. In this paper, we use the STIR for developing novel algorithms and models for scene understanding, particularly for seeing hidden scene patches.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Inverse Geometry", "text": "Unlike traditional time-of-flight imaging, our goal is to compute the direct distances, d 0i , using the first bounce, and the pairwise distances, d ij . Instead of using intrinsic camera calibration, we exploit second and higher order bounces to estimate scene geometry. First, we use the onset information contained in the STIR to estimate pairwise distances.\nThen we compute a robust isometric embedding to determine patch coordinates. We develop our formulation for a scene with the following strict assumptions:\n1 Each patch is visible from all the other patches (v ij = 1\u2200i, j). If not, then we image locally with a set of patches for which this is true. 2 The reflectance of each patch p i has a non-zero diffuse component. This assumption ensures that we are able to estimate direct distances d 0i . In Section 4.3, we discuss the extension our framework to scenes consisting of patches hidden from the camera and illumination.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Distances from STIR", "text": "Define O 1 = {O 1 i |i = 1, .\n. . , M} as the set of first onsets: the collection of all time instants, O 1 i , when the pixel observing patch p i receives the first non-zero response while the source illuminates the same patch p i (Figure 5). O 1 i is the time taken by the impulse ray originating at p 0 directed towards p i to arrive back at p 0 after the first bounce; this corresponds to the direct path p 0 \u2192 p i \u2192 p 0 . Similarly, we define O 2 = {O 2 ij |i, j = 1, . . . , M; j = i} as the set of second onsets: the collection of times when the transient imaging camera receives the first non-zero response from a patch p i while illuminating a different patch p j (Figure 5). This corresponds to the multi-path\np 0 \u2192 p j \u2192 p i \u2192 p 0 . O 2 ij = O 2 ji .\nIt is straightforward to label the onsets in O 1 and O 2 because they correspond to the first non-zero responses in STIR time images.\nIn order to compute D using O 1 and O 2 , we construct the forward distance transform, T 2 , of size (M (M + 1)/2 \u00d7 M (M + 1)/2) which models the sum of appropriate combinations of path lengths contained in the distance vector d = vec(D) and relates it to the vector of observed onsets O. Then we solve the linear system T 2 d = O to obtain distance estimatesd. As an example, consider a scene with 3 patches (M = 3) as shown in Figure 3. The linear system for this scene is constructed as:\n\u23a1 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3 2 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 2 0 0 0 0 1 1 1 0 0 0 0 0 2 \u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6 \u23a1 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3 d 01 d 12 d 13 d 02 d 23 d 03 \u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6 = c \u23a1 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3 O 1 1 O 2 12 O 2 13 O 2 1 O 2 23 O 3 1 \u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6\nFor any M , matrix T 2 is full rank and well-conditioned. Due to synchronization errors, device delays and response times the observed onsets have measurement uncertainties which introduce errors in distance estimates. We use the redundancy in second onset values\n(O 2 ij = O 2 ji\n) to obtain multiple estimates,d, and reduce error by averaging them. A detailed noise analysis can be found in the supporting technical report 8.", "publication_ref": [], "figure_ref": ["fig_4", "fig_4", "fig_2"], "table_ref": []}, {"heading": "Structure from Pairwise Distances", "text": "The problem of estimating scene structure, Z, from pairwise distance estimates, D, is equivalent to finding an isometric embedding\u1e90 \u2282 R M \u00d73 \u2192 R 3 (Algorithm 1, [22]). For computational convenience we take p 0 to be the origin (z 0 = (0, 0, 0)). A computer simulation that recovers the scene structure from noisy distance estimates using the isometric embedding algorithm is shown in Figure 4. We used the estimated coordinates,\u1e90, iteratively to recompute robust distance estimates. The use of convex optimization to compute optimal embeddings in the presence of distance uncertainties is explained in [22].\nAlgorithm 1 ISOEMBED [D] 1. Compute h ij = 1 2 d 2 0i + d 2 0j \u2212 d 2 ij . Construct Gram matrix H M \u00d7M = [h ij ] 2. Compute the SVD of H = U \u03a3V T 3. Pick 3 largest eigenvalue-vectors \u03a3 3\u00d73 3 , U M \u00d73 3 , V 3\u00d7M 3 4. Compute embedding Z e = (\u03a3 3 ) 1/2 V 3\n5. Rotate and translate to align\u1e90 = RZ e + T", "publication_ref": ["b21", "b21"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Scenes with Occluders", "text": "We now consider a scene that contains a set of patches (say H) hidden from both the camera and the source. Hidden surface estimation is viewed as two sub-problems: (1) Labeling third onsets and (2) Inferring distances to hidden patches from integrated path lengths. To estimate the structure of the hidden patches, we make the following strong assumptions: 1 The number of hidden patches is known or assumed.\n2 All third bounces arrive before fourth and higher order bounces. 3 No two or more distinct third bounces arrive at the same time in the same time profile STIR(x i , y i , \u03b8 j , \u03c6 j , t = 0...T ). The second assumption is true for scenes that have no interreflection amongst hidden patches. The third assumption is generally valid because we measure the STIR one patch at a time. If a patch, p i , is hidden from p 0 , then the first and second onsets involving p i cannot be observed, i.e the vector of distances d H = [d ij ] : p i \u2208 H, j = 0, . . . , M cannot be estimated using just O 1 and O 2 . Hence, we need to consider the set of third onsets, O 3  134 and the onsets arrive in different time profiles of the STIR(S). We sort the remaining onsets based on their arrival times and label them based on the a priori assumption of the proximity of hidden patches to visible patches. In this example, w.l.o.g we assume that p 2 is closer to p 1 than p 3 . Hence, the onset O 3 121 arrives earlier than O 3 131 (see onset arrival profile in Figure 5). This labeling procedure can be generalized for multiple hidden patches:\n= {O 3 ijk : i, j, k = 1, . . . , M; i = j; j = k},\n1 Estimate the distances to all the visible scene patches (Section 4.1) and use the arrival times to label all third bounce onsets corresponding to visible geometry. 2 Fix an arbitrary ordering of hidden patches based on their proximity to some visible patch. 3 Use arrival times to identify the third onset pairs corresponding to same path length (O 3 ijk = O 3 kji ). Label them with the ordering of step 2. 4 Sort the remaining onsets according to their arrival times and use step 2 ordering to label them. We construct the distance operator, T 3 , that relates third bounces arrival times involving hidden patches, O H , and the distances to the hidden patches, d H . We solve the resulting linear system T 3 d H = O H and obtain the complete distance set, D. We then estimate the structure, Z, as discussed in Section 4.2. An example of reconstructing hidden 3D geometry is shown in Figure 6. \n\u23a1 \u23a2 \u23a2 \u23a3 2 0 0 0 1 1 0 0 0 0 2 0 0 0 1 1 \u23a4 \u23a5 \u23a5 \u23a6 \u23a1 \u23a2 \u23a2 \u23a3 d 21 d 24 d 31 d 34 \u23a4 \u23a5 \u23a5 \u23a6 = c \u23a1 \u23a2 \u23a2 \u23a3 O 3 121 \u2212 O 1 1 O 2 124 \u2212 (O 1 1 + O 1 4 )/2 O 3 131 \u2212 O 1 3 O 2 134 \u2212 (O 1 1 + O 1 4 )/2 \u23a4 \u23a5 \u23a5 \u23a6", "publication_ref": ["b2", "b2"], "figure_ref": ["fig_4", "fig_5"], "table_ref": []}, {"heading": "Hardware Prototype", "text": "We corroborate the framework developed in Section 4 with experiments conducted using a prototype transient imaging camera. Our experiments demonstrate feasibility but not a full-fledged imaging apparatus. In particular, we intend this prototype (Figure 1) to show that it is feasible to reason about multi-bounce global transport using the STIR.\nWe used a commercially-available reverse-biased silicon photo sensor (Thorlabs FDS02, $72). This sensor has an active area of 250 microns in diameter and a condensing lens to gather more light. Photo-currents were digitized by a 5 GHz oscilloscope. Our least count was 50 ps (1.5cm light travel). Our ray impulse source was a modelocked Ti-Sapphire laser with a center wavelength of 810 nm, that emitted 50 femtosecond long pulses at a repetition rate of 93.68 MHz. The spatial bandwidth of these pulses greatly exceeds the response bandwidth of the sensor. Average laser power was 420 milliwatts, corresponding to a peak power of greater than 85 kW.\nWe need to sample the incident light with picosecond resolution and be highly sensitive to a low photon arrival rate. Our depth resolution is limited by the response time of the detector and digitizer (250 ps, 7.5cm light travel). The high peak power of our laser was critical for registering SNR above the dark current of our photo sensor. Also, our STIR acquisition times are in nanoseconds, which allows us to take a large number of exposures and time average them to reduce Gaussian noise. In absence of a 2D photo sensor array, we emulated directionality by raster scanning the scene with a steerable laser and sensor. We conducted four proof-of-concept experiments (Figure 7) in flatland (2D) to demonstrate the following key properties of a transient imaging camera: free space functioning, linearity, multi-path light collection, inverse square intensity falloff and time invariance. We achieved synchronization by triggering our pulses based on a reference photo sensor. A small part of the laser pulse was deflected into a reference photo sensor using a semi-reflective glass patch and all pulse arrivals (onsets) were measured as TDOA w.r.t the reference pulse.", "publication_ref": [], "figure_ref": ["fig_0", "fig_6"], "table_ref": []}, {"heading": "Applications and Experiments", "text": "We use the transient imaging prototype and the algorithmic framework developed in Section 4 to estimate geometry for objects that do not reflect any light to camera due to specularity or occlusion.\nMissing direct reflection: Consider the example shown in Figure 8(top) comprising a mirror and a diffuser. In traditional cameras it is difficult to estimate the distance to a specular surface because there is no direct reflection received at the camera. Using transient imaging analysis, we can estimate the distances to specular surfaces by observing indirect bounces. If we aim the laser, L, towards a mirror (in a known direction) it will strike an unknown point on M . The reflected light will then illuminate points on the diffuser. Separately, the position of the diffuser, x, is estimated via stereo triangulation (using the known angle of the laser beam) or ToF (Section 4.1). When the laser illuminates M , the total path length sensed at a pixel observing D is (z + y + x). Since x is known, the point M is obtained using conic multilateration. Note that, in dual photography [10], we create the dual image, i.e. the projector view, but that does not allow 3D estimation. We conducted 3 raster scans and assumed z 1 = z 2 = z 3 = z. The path lengths z i +x i +y i , i = 1, 2, 3 were estimated using TDOA. We incurred a position error of 1.1662 cm and a maximum distance error of 7.14% in reconstruction by multilateration.\nLooking Around the Corner: We show an example of multi-path analysis in a scene that contains patches which are not visible to either the camera or the illumination source. Consider the ray diagram shown in Figure 8(bottom). Only light rays that have first bounced off the diffuser reach the hidden patches P 1 , P 2 , P 3 . Light that is reflected from the hidden patches (second bounce) can only reach the camera once it is reflected off the diffuser again (third bounce). The position and depth of the points on the diffuser are estimated using first bounce onsets. We then raster scan across the diffuser length and measure the time difference of arrival (TDOA) between the first and third bounce onsets.\nIn this experiment we imaged a hidden 1 \u2212 0 \u2212 1 barcode using the first and third bounces off of a single diffuser. We used 2 sensors, S 1 and S 2 , and a femtosecond laser source, L, neither of which had the barcode in their line of sight. The patches P 1 and P 3 were ground mirrors and P 2 was free space. The mirrors were aligned to maximize the SNR required for registering a third bounce. The maximum separation between P 1 and P 3 was limited to 5 cm because of SNR considerations. The first bounce, LD 1 S 1 , was recorded by S 1 , and the two third bounces from the hidden patches, LD 1 P 1 D 4 S 2 and LD 1 P 3 D 3 S 2 , arrived at S 2 within 200 ps of each other. Our current sensor was not fast enough and could only record the sum of the two third bounces. The two bounces can be recorded more accurately with a faster picosecond detector or separated using deconvolution using S 2 's impulse response. As a proof of concept, we computed a high quality estimate by blocking P 1 and P 3 , one at a time. The reconstruction results are shown in Figure 8(b). We incurred a maximum error of 0.9574 cm in coordinate reconstruction.", "publication_ref": ["b9"], "figure_ref": ["fig_7", "fig_7", "fig_7"], "table_ref": []}, {"heading": "Future Work", "text": "To our knowledge, neither the transient light transport theory nor the presented imaging experiments have ever been conceived in literature. Our work, as it stands, has several limitations which make it challenging to generalize the transient imaging method to more complex, general scenes. However, with sophisticated modeling and advanced instrumentation, it is possible to alleviate these restrictions. Experimental: Isolating onsets in practice is inherently noisy, as onsets do not arrive at discrete instances; rather, they arrive as a continuous time profile. Though we have assumed a discrete patch model, future research should include continuous surface models and utilize tools in differential geometry to model the transport in general scenes. Additionally, the ray impulses are low pass filtered by the sensor response. All these reasons cause a broad temporal blur, rather than a sharp distinct onset. A noise model for the transient imaging camera can be found in supporting material 8. While working with focusing optics, spatial blur causes some scene patches to correspond to the same camera pixel. We alleviate this by working within the camera's depth-of-field. Every bounce off a diffuse surface creates considerable light loss and, thus, impacts the SNR. Another challenge is to collect strong multi-path signals (requiring single photon sensitivity) with ultra-fast time sampling. Commercial solutions, such as Intensified CCD cameras, allow image acquisition at very low light levels and at relatively high gating speeds (200ps or lower). The illumination source must be powerful enough to overcome ambient light. Also, our current method will not work for scenes which have arbitrarily placed highly specular objects, though reasoning may be improved with the use of appropriate priors.\nTheoretical: Novel noise models for the transient imaging camera are required to account for uncertainties due to light-matter interplay. If two or more scene patches are occluded from each other (v ij = 0, i, j = 0), our theoretical model fails. We circumvent this problem by using our transient imaging framework locally, with a subset of scene patches that satisfy our assumptions. The number of STIR measurements grow polynomially with number of patches, but the onset labeling complexity is exponential in the number of bounce orders used for inversion. Our framework will benefit from optimization-based onset labeling algorithms to account for time arrival uncertainties. We made a set of strong a priori assumptions for hidden surface estimation. Statistical regularization schemes, along with novel scene geometry priors, will allow us to extend transient reasoning to complex scenes where hidden surfaces may involve local scattering.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "The goal of this paper is to explore the opportunities in multi-path analysis of light transport. We developed the theoretical basis for analysis and demonstrated potential methods for recovering scene properties for a range of simple scenarios. Emerging trends in femtosecond accurate emitters, detectors and nonlinear optics may support single-shot time-image cameras. Upcoming low-cost solid state lasers will also support ultra-short operation. The key contribution here is exploration of a new area of algorithms for solving hard problems in computer vision based on time-image analysis. This, in turn, has tremendous applications in other areas such as scatter-free reconstruction in medical imaging, better layout understanding for fire and rescue personnel (Figure 1), tracking beyond line of sight in surveillance and car collision avoidance at blind corners, and robot path planning with extended observable structure. New research will adapt current work in structure from motion, segmentation, recognition and tracking to a novel time-image analysis that resolves shapes in challenging conditions. ", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for critical suggestions. We are very thankful to Prof. Neil Gershenfeld (CBA), Prof. Joseph Paradiso, Dr. Franco Wong, Dr. Manu Prakash (MIT) for making available their opto-electronic apparatus that made our experiments possible. We would also like to thank MIT EECS Professors George Verghese, Franz Kaertner, Rajeev Ram for invaluable discussions. MIT undergraduate students: George Hansel, Kesavan Yogeswaran, assisted in carrying out initial experiments. We thank Biyeun Buczyk (MIT) for greatly improving the figures and Paula Aguilera (MIT) for creating the video presentation. We finally thank Gavin Miller, Adam Smith and James Skorupski for several initial discussions.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "The rendering equation", "journal": "", "year": "", "authors": "J Kajiya"}, {"ref_id": "b1", "title": "Transfer equations in global illumination", "journal": "", "year": "", "authors": "J Arvo"}, {"ref_id": "b2", "title": "5d time-light transport matrix: What can we reason about scene properties", "journal": "", "year": "", "authors": "R Raskar;  Davis"}, {"ref_id": "b3", "title": "Transient rendering", "journal": "", "year": "2002-08", "authors": "A Smith; J Skorupski;  Davis"}, {"ref_id": "b4", "title": "A survey of inverse rendering problems", "journal": "", "year": "", "authors": "G Patow;  Pueyo"}, {"ref_id": "b5", "title": "A theory of refractive and specular 3d shape by light-path triangulation", "journal": "", "year": "", "authors": "N Kiriakos;  Steger"}, {"ref_id": "b6", "title": "Reconstructing the surface of inhomogeneous transparent scenes by scatter trace photography", "journal": "", "year": "", "authors": "N Morris;  Kutulakos"}, {"ref_id": "b7", "title": "Shape from interreflections", "journal": "", "year": "", "authors": "K S K Nayar; T Ikeuchi;  Kanade"}, {"ref_id": "b8", "title": "A signal-processing framework for inverse rendering", "journal": "", "year": "", "authors": "R Ramamoorthi;  Hanrahan"}, {"ref_id": "b9", "title": "Dual photography", "journal": "", "year": "", "authors": "P Sen; G Chen; S Garg; M Marschner; M Horowitz; H Levoy;  Lensch"}, {"ref_id": "b10", "title": "Fast separation of direct and global components of a scene using high frequency illumination", "journal": "", "year": "", "authors": "G S K Nayar; M Krishnan; R Grossberg;  Raskar"}, {"ref_id": "b11", "title": "A theory of inverse light transport", "journal": "", "year": "", "authors": "Y S M Seitz; K N Matsushita;  Kutulakos"}, {"ref_id": "b12", "title": "Laser radar", "journal": "", "year": "", "authors": " G W Kamerman"}, {"ref_id": "b13", "title": "3d imaging in the studio (and elsewhere", "journal": "", "year": "", "authors": "G G J Iddan;  Yahav"}, {"ref_id": "b14", "title": "Solid-state time-of-flight range camera", "journal": "Quan. Elec., IEEE Jour", "year": "", "authors": "R Lange; P Seitz"}, {"ref_id": "b15", "title": "Computing depth under ambient illumination using multi-shuttered light", "journal": "", "year": "", "authors": "H G Banos; J Davis"}, {"ref_id": "b16", "title": "Toward laser pulse waveform analysis for scene interpretation", "journal": "", "year": "", "authors": "N Vandapel; J R Amidi;  Miller"}, {"ref_id": "b17", "title": "Optical coherence tomography (oct): A review", "journal": "", "year": "", "authors": "J M Schmitt"}, {"ref_id": "b18", "title": "Two-photon laser scanning fluorescence microscopy", "journal": "Science", "year": "", "authors": "W Denk; J Strickler; Webb W "}, {"ref_id": "b19", "title": "Picosecond streak camera fluorometry -a review", "journal": "IEEE Jour", "year": "", "authors": "A Campillo;  Shapiro"}, {"ref_id": "b20", "title": "Attosecond streak camera", "journal": "Physical Review Letters", "year": "", "authors": "J Itatani; G L Qu\u00e9r\u00e9; M Y Yudin; F Ivanov; P B Krausz;  Corkum"}, {"ref_id": "b21", "title": "Convex optimization & euclidean distance geometry", "journal": "Meboo Publishing USA", "year": "", "authors": "J Dattorro"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. Can you look around the corner into a room with no imaging device in the line of sight? This paper shows that by emitting short pulses (1-2), and analyzing multi-bounce reflection from the door (4-1), we can infer hidden geometry even if the intermediate bounces (3) are not visible. The transient imaging camera prototype consists of (a) Femtosecond laser illumination (b) Picosecond-accurate detectors and (c) an Ultrafast sampling oscilloscope. We measure the Space Time Impulse Response (STIR) of the scene (d) containing a hidden 1-0-1 barcode and reconstruct the hidden surface. Please refer to supplementary video 8 for more results.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 .2PHOTODIODE ARRAY", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 .3Figure 3. Measuring the STIR of a scene with 3 patches using the transient imaging camera. We successively illuminate a single patch and record a 3D time image. Collection of such time images creates a 5D STIR.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 .4Figure 4. (a) Estimating distances in an all-visible scene comprising of 3 rectangles which are discretized as 49 patches. Note that reflectance is not relevant. (b) Original geometry shows the surface normals in green. (c) We used noisy 1 st and 2 nd time onsets (Gaussian noise \u223c N (\u03bc, \u03c3 2 ), \u03bc = device resolution = 250ps and \u03c3 = 0.1) to estimate the distances using the T 2 operator (inset shows enlarged view). (d) This is followed by isometric embedding and surface fitting. The reconstruction errors are plotted. Color bar shows %-error in reconstructed coordinate values.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 .5Figure 5. A scene with M = 4 patches. Patches p 2 and p 3 are hidden.The blue (first) and green (second) onsets are a result of directly observing visible patches p 1 and p 4 . The pattern of arrival of third onsets depends on the relative distance of the hidden patches p 2 and p 3 from the visible patches. The onsets that correspond to light traversing the same Euclidean distance are readily identified. Once the onsets are labeled, they are used to obtain distances that involve hidden patches.d 02 , d 03 and d 23 can be computed using multilateration. Now, we apply our labeling algorithm to identify third onsets. The onsets, O 3 141 and O 3 414 , are readily labeled using TDOA, since we know the distances to patch p 1 and p 4 . The onsets O 3 121 , O 3 131 , O 3 424 , O 3 434 , O 3 124 , O 3 134 , O 3 421 , O 3 431 are disambiguated using the facts that O 3 421 = O 3 124 , O 3 431 = O 3134 and the onsets arrive in different time profiles of the STIR(S). We sort the remaining onsets based on their arrival times and label them based on the a priori assumption of the proximity of hidden patches to visible patches. In this example, w.l.o.g we assume that p 2 is closer to p 1 than p 3 . Hence, the onset O3 121 arrives earlier than O 3 131 (see onset arrival profile in Figure5). This labeling procedure can be generalized for multiple hidden patches:1 Estimate the distances to all the visible scene patches (Section 4.1) and use the arrival times to label all third bounce onsets corresponding to visible geometry. 2 Fix an arbitrary ordering of hidden patches based on their proximity to some visible patch. 3 Use arrival times to identify the third onset pairs corresponding to same path length (O 3 ijk = O 3 kji ). Label them with the ordering of step 2. 4 Sort the remaining onsets according to their arrival times and use step 2 ordering to label them. We construct the distance operator, T 3 , that relates third bounces arrival times involving hidden patches, O H , and the distances to the hidden patches, d H . We solve the resulting linear system T 3 d H = O H and obtain the complete distance set, D. We then estimate the structure, Z, as discussed in Section 4.2. An example of reconstructing hidden 3D geometry is shown in Figure6. \u23a1", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 6 .6Figure 6. (a) Estimating distances in scenes with hidden patches. Unknown to the estimation algorithm, the hidden patches are on a plane (shown in black). (b) Original patch geometry. We use 1 st , 2 nd and 3 rd bounce onsets, our labeling algorithm and the T 3 operator (c) to estimate hidden geometry. (d) The isometric embedding error plot verifies negligible reconstruction error and near co-planarity of patches. Onset noise and color bar schemes are same as Figure 4.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 7 .7Figure 7. Design and Verification of a Transient Imaging Camera. (a) The ray impulses are recorded after being attenuated by a varying neutral density filter. The peak pulse intensity decreases linearly with the attenuation. (b) The intensity of the first bounce from a diffuser obeys the inverse square fall-off pattern. (c) We are able to record pulse intensities that are discernible from the noise floor even after the ray impulse has been reflected by three (2 diffuse and 1 specular) patches. The time shifts are linearly proportional to the multi-path length.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "FirstFigure 8 .8Figure 8. Missing direct reflection (Top): (a) A photo of the setup. (b) Ray diagram describing the light pulse path in 3 raster scans. (c) Plot showing multilateration using the 3 raster scans data: original and reconstructed scene geometries. (d) Oscilloscope data plot showing the TDOA between the 2 nd bounce and the reference signal. Looking around the corner (Bottom): (a) A photo of the setup showing 101 hidden barcode. The sensors and the laser are completely shielded from the barcode. (b) Ray diagram tracing the paths of 1 st and 3 rd bounces in the 2 raster scans. (c) Plot showing the scene geometry reconstruction, 1 st bounce and the two separately recorded 3 rd bounces. Note the very small delay (\u2264 200 ps) between two 3 rd bounce arrivals. (d) Oscilloscope data plot showing the 1 st bounce and the time delayed sum of two 3 rd bounces for both raster scans. Please zoom in the PDF version for details.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "z i \u2208 R 3 . Let Z = [z 1 , . . . , z M ].", "formula_coordinates": [3.0, 50.11, 194.46, 236.25, 23.92]}, {"formula_id": "formula_1", "formula_text": ") : i = 1 . . . M; t = 0 . . . T } = STIR(x i , y i , \u03b8 j , \u03c6 j , t).", "formula_coordinates": [3.0, 70.04, 548.76, 216.33, 22.54]}, {"formula_id": "formula_2", "formula_text": "Define O 1 = {O 1 i |i = 1, .", "formula_coordinates": [3.0, 308.86, 511.56, 106.17, 13.18]}, {"formula_id": "formula_3", "formula_text": "p 0 \u2192 p j \u2192 p i \u2192 p 0 . O 2 ij = O 2 ji .", "formula_coordinates": [3.0, 308.86, 643.13, 236.25, 23.64]}, {"formula_id": "formula_4", "formula_text": "\u23a1 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3 2 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 2 0 0 0 0 1 1 1 0 0 0 0 0 2 \u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6 \u23a1 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3 d 01 d 12 d 13 d 02 d 23 d 03 \u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6 = c \u23a1 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a2 \u23a3 O 1 1 O 2 12 O 2 13 O 2 1 O 2 23 O 3 1 \u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6", "formula_coordinates": [4.0, 58.27, 158.96, 200.02, 79.51]}, {"formula_id": "formula_5", "formula_text": "(O 2 ij = O 2 ji", "formula_coordinates": [4.0, 194.24, 294.0, 48.63, 13.18]}, {"formula_id": "formula_6", "formula_text": "Algorithm 1 ISOEMBED [D] 1. Compute h ij = 1 2 d 2 0i + d 2 0j \u2212 d 2 ij . Construct Gram matrix H M \u00d7M = [h ij ] 2. Compute the SVD of H = U \u03a3V T 3. Pick 3 largest eigenvalue-vectors \u03a3 3\u00d73 3 , U M \u00d73 3 , V 3\u00d7M 3 4. Compute embedding Z e = (\u03a3 3 ) 1/2 V 3", "formula_coordinates": [4.0, 50.11, 512.72, 235.0, 75.93]}, {"formula_id": "formula_7", "formula_text": "= {O 3 ijk : i, j, k = 1, . . . , M; i = j; j = k},", "formula_coordinates": [4.0, 308.86, 525.79, 236.25, 23.17]}, {"formula_id": "formula_8", "formula_text": "\u23a1 \u23a2 \u23a2 \u23a3 2 0 0 0 1 1 0 0 0 0 2 0 0 0 1 1 \u23a4 \u23a5 \u23a5 \u23a6 \u23a1 \u23a2 \u23a2 \u23a3 d 21 d 24 d 31 d 34 \u23a4 \u23a5 \u23a5 \u23a6 = c \u23a1 \u23a2 \u23a2 \u23a3 O 3 121 \u2212 O 1 1 O 2 124 \u2212 (O 1 1 + O 1 4 )/2 O 3 131 \u2212 O 1 3 O 2 134 \u2212 (O 1 1 + O 1 4 )/2 \u23a4 \u23a5 \u23a5 \u23a6", "formula_coordinates": [5.0, 50.11, 661.09, 240.86, 55.52]}], "doi": ""}