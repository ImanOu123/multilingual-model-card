{"title": "Random Features for Large-Scale Kernel Machines", "authors": "Ali Rahimi; Ben Recht", "pub_date": "", "abstract": "To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. Our randomized features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user specified shift-invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classification and regression tasks linear machine learning algorithms that use these features outperform state-of-the-art large-scale kernel machines.", "sections": [{"heading": "Introduction", "text": "Kernel machines such as the Support Vector Machine are attractive because they can approximate any function or decision boundary arbitrarily well with enough training data. Unfortunately, methods that operate on the kernel matrix (Gram matrix) of the data scale poorly with the size of the training dataset. For example, a dataset with half a million training examples might take days to train on modern workstations. On the other hand, specialized algorithms for linear Support Vector Machines and regularized regression run much more quickly when the dimensionality of the data is small because they operate on the covariance matrix rather than the kernel matrix of the training data [1,2]. We propose a way to combine the advantages of the linear and nonlinear approaches. Inspired by randomized algorithms for approximating kernel matrices (e.g., [3,4]), we efficiently convert the training and evaluation of any kernel machine into the corresponding operations of a linear machine by mapping data into a relatively low-dimensional randomized feature space. Our experiments show that random features combined with very simple linear learning techniques compete favorably with state-of-the-art kernel-based classification and regression algorithms. Random features significantly reduce the computation needed for training, and obtain similar or better testing error.\nThe kernel trick is a simple way to generate features for algorithms that depend only on the inner product between pairs of input points. It relies on the observation that any positive definite function k(x, y) with x, y \u2208 R d defines an inner product and a lifting \u03c6 so that the inner product between lifted datapoints can be quickly computed as \u03c6(x), \u03c6(y) = k(x, y). The cost of this convenience is that algorithms access the data only through evaluations of k(x, y), or through the kernel matrix consisting of k applied to all pairs of datapoints. As a result, large training sets incur large computational and storage costs.\nInstead of relying on the implicit lifting provided by the kernel trick, we propose explicitly mapping the data to a low-dimensional Euclidean inner product space using a randomized feature map z : R d \u2192 R D so that the inner product between a pair of transformed points approximates their kernel evaluation: k(x, y) = \u03c6(x), \u03c6(y) \u2248 z(x) z(y).\n(1) Unlike the kernel's lifting \u03c6, z is low-dimensional. Thus, we can simply transform the input with z, and then apply fast linear learning methods to approximate the answer of the corresponding nonlinear kernel machine. In what follows, we show how to construct feature spaces that uniformly approximate popular shift-invariant kernels k(x \u2212 y) to within with only\nD = O(d \u22122 log 1 2 )\ndimensions, and empirically show that excellent regression and classification performance can be obtained for even smaller D.\nIn addition to giving us access to extremely fast learning algorithms, these randomized feature maps also provide a way to quickly evaluate the machine. With the kernel trick, evaluating the machine at a test point x requires computing f (x) = N i=1 c i k(x i , x), which requires O(N d) operations to compute and requires retaining much of the dataset unless the machine is very sparse. This is often unacceptable for large datasets. On the other hand, after learning a hyperplane w, a linear machine can be evaluated by simply computing f (x) = w z(x), which, with the randomized feature maps presented here, requires only O(D + d) operations and storage.\nWe demonstrate two randomized feature maps for approximating shift invariant kernels. Our first randomized map, presented in Section 3, consists of sinusoids randomly drawn from the Fourier transform of the kernel function we seek to approximate. Because this map is smooth, it is wellsuited for interpolation tasks. Our second randomized map, presented in Section 4, partitions the input space using randomly shifted grids at randomly chosen resolutions. This mapping is not smooth, but leverages the proximity between input points, and is well-suited for approximating kernels that depend on the L 1 distance between datapoints. Our experiments in Section 5 demonstrate that combining these randomized maps with simple linear learning algorithms competes favorably with state-of-the-art training algorithms in a variety of regression and classification scenarios.", "publication_ref": ["b0", "b1", "b2", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "The most popular methods for large-scale kernel machines are decomposition methods for solving Support Vector Machines (SVM). These methods iteratively update a subset of the kernel machine's coefficients using coordinate ascent until KKT conditions are satisfied to within a tolerance [5,6]. While such approaches are versatile workhorses, they do not always scale to datasets with more than hundreds of thousands of datapoints for non-linear problems. To extend learning with kernel machines to these scales, several approximation schemes have been proposed for speeding up operations involving the kernel matrix.\nThe evaluation of the kernel function can be sped up using linear random projections [3]. Throwing away individual entries [3] or entire rows [4,7,8] of the kernel matrix lowers the storage and computational cost of operating on the kernel matrix. These approximations either preserve the separability of the data [4], or produce good low-rank or sparse approximations of the true kernel matrix [3,7]. Fast multipole and multigrid methods have also been proposed for this purpose, but, while they appear to be effective on small and low-dimensional problems, to our knowledge, their effectiveness has not been demonstrated on large datasets. Further, the quality of the Hermite or Taylor approximation that these methods rely on degrades exponentially with the dimensionality of the dataset [9]. Fast nearest neighbor lookup with KD-Trees has been used to approximate multiplication with the kernel matrix, and in turn, a variety of other operations [10].\nWe compare our work to the Core Vector Machine (CVM), a state-of-the-art technique that takes an altogether different approach than those thus far discussed [12]. CVM transforms a classification problem into a support vector data-description problem, and solves this using a fast minimumenclosing ball algorithm that randomly samples the training data.\nUnlike previous work, instead of approximating the kernel matrix, our work approximates the kernel function directly. The feature map we present in Section 4 is reminiscent of KD-trees in that it partitions the input space using multi-resolution axis-aligned grids similar to those developed in [11] for embedding linear assignment problems.", "publication_ref": ["b4", "b5", "b2", "b2", "b3", "b6", "b7", "b3", "b2", "b6", "b8", "b9", "b11", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Random Fourier Features", "text": "Our first set of random features consists of random Fourier bases cos(\u03c9 x + b) where \u03c9 \u2208 R d and b \u2208 R are random variables. These mappings project data points on a randomly chosen line, and then pass the resulting scalar through a sinusoidal function (see Figure 1 and Algorithm 1). Drawing the direction of these lines from an appropriate distribution guarantees that the product of two transformed points will approximate a desired shift-invariant kernel.\nR D R 2 \u03c9 x Kernel Name k(\u2206) p(\u03c9) Gaussian e \u2212 \u2206 2 2 2 (2\u03c0) \u2212 D 2 e \u2212 \u03c9 2 2 2 Laplacian e \u2212 \u2206 1 d 1 \u03c0(1+\u03c9 2 d ) Cauchy d 2 1+\u2206 2 d e \u2212 \u2206 1\nFigure 1: Random Fourier Features. Each component of the feature map z(x) projects x onto a random direction \u03c9 drawn from the Fourier transform p(\u03c9) of k(\u2206), and wraps this line onto the unit circle in R 2 . After transforming two points x and y in this way, their inner product is an unbiased estimator of k(x, y). The mapping z(x) = cos(\u03c9 x + b) additionally rotates this circle by a random amount b and projects the points onto the interval [0, 1]. The table lists some popular shift-invariant kernels and their Fourier transforms. To deal with non-isotropic kernels, we can first whiten the data and apply one of these kernels\nThe following classical theorem from harmonic analysis provides the key insight behind this transformation: Theorem 1 (Bochner [13]). A continuous kernel k(x, y) = k(x \u2212 y) on R d is positive definite if and only if k(\u03b4) is the Fourier transform of a non-negative measure.\nIf a shift-invariant kernel k(\u03b4) is properly scaled, Bochner's theorem guarantees that its Fourier transform p(\u03c9) is a proper probability distribution. Defining \u03b6 \u03c9 (x) = e j\u03c9 x , we have\nk(x \u2212 y) = R d p(\u03c9)e j\u03c9 (x\u2212y) d\u03c9 = E \u03c9 [\u03b6 \u03c9 (x)\u03b6 \u03c9 (y) * ],(2)\nso \u03b6 \u03c9 (x)\u03b6 \u03c9 (y) * is an unbiased estimate of k(x, y) when \u03c9 is drawn from p.\nSince both the probability distribution p(\u03c9) and the kernel k(\u2206) are real, the integral (2) converges when the complex exponentials are replaced with cosines. Therefore, we may obtain a real-valued mapping that satisfies the condition E[z \u03c9 (x)z \u03c9 (y)] = k(x, y) by setting z \u03c9 (x) = \u221a 2 cos(\u03c9 x + b), where \u03c9 is drawn from p(\u03c9) and b is drawn uniformly from [0, 2\u03c0]. That z \u03c9 (x)z \u03c9 (y) has expected value k(x, y) is a consequence of the sum of angles formula.\nWe can lower the variance of the estimate of the kernel by concatenating D randomly chosen z \u03c9 into one D-dimensional vector z and normalizing each component by\n\u221a D. The inner product z(x) z(y) = 1 D D j=1 z \u03c9j (x)z \u03c9j (y)\nis a sample average of z \u03c9 and is therefore a lower variance approximation to the expectation (2). Since z \u03c9 is bounded between + \u221a 2 and \u2212 \u221a 2 for a fixed pair of points x and y, Hoeffding's inequality guarantees exponentially fast convergence in D between z(x) z(y) and k(x, y):\nPr [|z(x) z(y) \u2212 k(x, y)| \u2265 ] \u2264 2 exp(\u2212D 2 /4\n). Building on this observation, a much stronger assertion can be proven for every pair of points in the input space simultaneously: Claim 1 (Uniform convergence of Fourier features). Let M be a compact subset of R d with diameter diam(M). Then, for the mapping z defined in Algorithm 1, we have Pr sup\nx,y\u2208M |z(x) z(y) \u2212 k(y, x)| \u2265 \u2264 2 8 \u03c3 p diam(M) 2 exp \u2212 D 2 4(d + 2) ,\nwhere The proof of this assertion first guarantees that z(x) z(y) is close to k(x \u2212 y) for the centers of an -net over M \u00d7 M. This result is then extended to the entire space using the fact that the feature map is smooth with high probability. See the Appendix for details.\n\u03c3 2 p \u2261 E p [\u03c9 \u03c9]\nBy a standard Fourier identity, the scalar \u03c3 2 p is equal to the trace of the Hessian of k at 0. It quantifies the curvature of the kernel at the origin. For the spherical Gaussian kernel, k(x, y) = exp \u2212\u03b3 x \u2212 y 2 , we have \u03c3 2 p = 2d\u03b3.\nAlgorithm 1 Random Fourier Features.\nRequire: A positive definite shift-invariant kernel k(x, y) = k(x \u2212 y). Ensure: A randomized feature map z(x) :\nR d \u2192 R D so that z(x) z(y) \u2248 k(x \u2212 y).\nCompute the Fourier transform p of the kernel k: p\n(\u03c9) = 1 2\u03c0 e \u2212j\u03c9 \u03b4 k(\u03b4) d\u2206. Draw D iid samples \u03c9 1 , \u2022 \u2022 \u2022 , \u03c9 D \u2208 R d from p and D iid samples b 1 , . . . , b D \u2208 R from the uniform distribution on [0, 2\u03c0]. Let z(x) \u2261 2 D [ cos(\u03c9 1 x+b1) \u2022\u2022\u2022 cos(\u03c9 D x+b D ) ] .", "publication_ref": ["b12"], "figure_ref": [], "table_ref": []}, {"heading": "Random Binning Features", "text": "Our second random map partitions the input space using randomly shifted grids at randomly chosen resolutions and assigns to an input point a binary bit string that corresponds to the bins in which it falls (see Figure 2 and Algorithm 2). Because it uses rectilinear grids, this mapping is well-suited for kernels that depend only on the L 1 distance between pairs of points. The grids are constructed so that the probability that two points x and y are assigned to the same bin is proportional to k(x, y). The inner product between a pair of transformed points is proportional to the number of times the two points are binned together, and is therefore an unbiased estimate of k(x, y).\n10000000 01000000 00100000 00010000 00001000 00000100 00000010 00000001 We first describe a randomized mapping to approximate the \"hat\" kernel k hat (x, y; \u03b4) = max 0, 1 \u2212 |x\u2212y| \u03b4 on a segment of R, then show how to construct mappings for more general separable multi-dimensional kernels. Partition the real number line with a grid of pitch \u03b4, and shift this grid randomly by an amount u drawn uniformly at random from [0, \u03b4]. This grid partitions the real number line into intervals [u + n\u03b4, u + (n + 1)\u03b4] for all integers n. The probability that two points x and y fall in the same bin in this grid is max 0, 1 \u2212 |x\u2212y| \u03b4 [11]. In other words, if we number the bins in the grid so that a point x falls in binx = x\u2212u \u03b4 and y falls in bin\u0177 = y\u2212u \u03b4 , then Pr u [x =\u0177|\u03b4] = k hat (x, y; \u03b4). If we encodex as a binary indicator vector z(x) over the bins, z(x) z(y) = 1 if x and y fall in the same bin and zero otherwise, so\n\u2248 + + + \u2022 \u2022 \u2022 = k(x i , x j ) z 1 (x i ) z 1 (x j ) z 2 (x i ) z 2 (x j ) z 3 (x i ) z 3 (x j ) z(x i ) z(x j )\nPr u [z(x) z(y) = 1|\u03b4] = E u [z(x) z(y)|\u03b4] = k hat (x, y; \u03b4).\nTherefore z is a random map for k hat . Now consider shift-invariant kernels that can be written as convex combinations of hat kernels on a compact subset of R \u00d7 R: k(x, y) = \u221e 0 k hat (x, y; \u03b4)p(\u03b4) d\u03b4. If the pitch \u03b4 of the grid is sampled from p, z again gives a random map for\nk because E \u03b4,u [z(x) z(y)] = E \u03b4 [E u [z(x) z(y)|\u03b4]] = E \u03b4 [k hat (x, y; \u03b4)] = k(x, y).\nThat is, if the pitch \u03b4 of the grid is sampled from p, and the shift u is drawn uniformly from [0, \u03b4] the probability that x and y are binned together is k(x, y). Lemma 1 in the appendix shows that p can be easily recovered from k by setting p(\u03b4) = \u03b4k(\u03b4). For example, in the case of the Laplacian kernel, k Laplacian (x, y) = exp(\u2212|x \u2212 y|), p(\u03b4) is the Gamma distribution \u03b4 exp(\u2212\u03b4). For the Gaussian kernel, k(\u03b4) is not convex, sok is not everywhere positive and \u03b4k(\u03b4) is not a probability distribution, so this procedure does not yield a random map for the Gaussian.\nRandom maps for separable multivariate shift-invariant kernels of the form k(x \u2212 y) = d m=1 k m (|x m \u2212y m |) (such as the multivariate Laplacian kernel) can be constructed in a similar way if each k m can be written as a convex combination of hat kernels. We apply the above binning process over each dimension of R d independently. The probability that x m and y m are binned together in dimension m is k m (|x m \u2212 y m |). Since the binning process is independent across dimensions, the probability that x and y are binned together in every dimension is\nd m=1 k m (|x m \u2212y m |) = k(x\u2212y).\nIn this multivariate case, z(x) encodes the integer vector [x 1 ,\u2022\u2022\u2022 ,x d ] corresponding to each bin of the d-dimensional grid as a binary indicator vector. In practice, to prevent overflows when computing z(x) when d is large, our implementation eliminates unoccupied bins from the representation. Since there are never more bins than training points, this ensures no overflow is possible.\nWe can again reduce the variance of the estimator z(x) z(y) by concatenating P random binning functions z into an larger list of features z and scaling by 1/P . The inner product z(x) z(y) = 1 P P p=1 z p (x) z p (y) is the average of P independent z(x) z(y) and has therefore lower variance. Since z is binary, Hoeffding's inequality guarantees that for a fixed pair of points x and y, z(x) z(y) converges exponentially quickly to k(x, y) as P increases. Again, a much stronger claim is that this convergence holds simultaneously for all points: Claim 2. Let M be a compact subset of R d with diameter diam(M). Let \u03b1 = E[1/\u03b4] and let L k denote the Lipschitz constant of k with respect to the L 1 norm. With z as above, we have Pr sup\nx,y\u2208M |z(x) z(y) \u2212 k(x, y)| \u2264 \u2265 1 \u2212 36dP \u03b1 diam(M) exp \uf8eb \uf8ed \u2212 P 2 8 + ln L k d + 1 \uf8f6 \uf8f8 ,\nThe proof is analogous to that of Claim 1. Since the function z is piecewise constant over M, M \u00d7 M can be broken up into a few small rectangular cells so that throughout each cell, k(x, y) does not change much and z(x) and z(y) do not change at all. With high probability, at the centers of these cells z(x) z(y) is close to k(x, y), which guarantees that k(x, y) and z(x) z(y) are close throughout M. (See Appendix).\nNote that \u03b1 = \u221e 0 1 \u03b4 p(\u03b4) d\u03b4 = \u221e 0k(\n\u03b4) d\u03b4 is 1, and L k = 1 for the Laplacian kernel.", "publication_ref": ["b10"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Algorithm 2 Random Binning Features.", "text": "Require:\nA point x \u2208 R d . A kernel function k(x, y) = d m=1 k m (|x m \u2212 y m |), so that p m (\u2206) \u2261 \u2206k m (\u2206) is a probability distribution on \u2206 \u2265 0. Ensure: A randomized feature map z(x) so that z(x) z(y) \u2248 k(x \u2212 y).\nfor p = 1 . . . P do Draw grid parameters \u03b4, u \u2208 R d with the pitch \u03b4 m \u223c p m , and shift u m from the uniform distribution on [0, \u03b4 m ].\nLet z return the coordinate of the bin containing x as a binary indicator vector z p (x) \u2261 hash(\nx 1 \u2212u 1 \u03b4 1 , \u2022 \u2022 \u2022 , x d \u2212u d \u03b4 d ). end for z(x) \u2261 1 P [ z1(x)\u2022\u2022\u2022z P (x) ] .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In the first set of experiments, summarized in Table 1, we show that least squares regression on our random features is a fast way to approximate the training of supervised kernel machines. We restrict our attention to the CVM because it was shown in [12] to be both faster and more accurate than other known approaches, including, in most cases, random sampling of datapoints [4]. In this set of experiments, we trained regressors and classifiers by solving the least squares problem min w Z w \u2212 y 2 2 + \u03bb w 2 2 , where y denotes the vector of desired outputs and Z denotes the matrix of random features. To evaluate the resulting machine on a datapoint x, we can simply compute w z(x).  Vector Machine, and various state-of-the-art exact methods reported in the literature. For classification tasks, the percent of testing points incorrectly predicted is reported. For regression tasks, the RMS error normalized by the norm of the ground truth is reported.\nWe base our comparison on the five standard large-scale datasets evaluated in [12], excluding the synthetic datasets. We replicated the results in the literature pertaining to the CVM, SVM light , and libSVM using binaries provided by the respective authors. 1 Despite the simplicity of our approach, least-squares with random features is faster than, and provides competitive accuracy with, alternative methods. It also produces very compact functions because only w and a set of O(D) random vectors or a hash-table of partitions need to be retained. Random Fourier features perform better on the tasks that largely rely on interpolation. On the other hand, random binning features perform better on memorization tasks (as measured by the number of support vectors needed by the SVM), because they explicitly preserve locality in the input space. This difference is most dramatic in the Forest dataset. Since the decision boundary for this problem is not smooth and requires tens of thousands of support vectors, the Fourier features perform quite poorly on this dataset. ", "publication_ref": ["b11", "b3", "b11", "b0"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Conclusion", "text": "We have presented randomized features whose inner products uniformly approximate many popular kernels, and demonstrated that these features are a powerful and economical tool for large-scale supervised learning. We showed empirically that providing these features as input to a standard linear learning algorithm produces results competitive with state-of-the-art kernel machines in accuracy, training time, and evaluation time.\nIt is worth noting any mixture of these features (such as combining partitioning with Fourier features or sampling frequencies from mixture models) can be readily computed and applied to learning problems.\nAn exciting direction for future research is to empirically and analytically evaluate random features on other learning tasks. While we have focused on regression and classification, our features can be applied to accelerate many other kernel methods, including semi-supervised and unsupervised learning algorithms. In all of these cases, a significant computational speed-up can be achieved by first computing random features and then applying the associated linear technique.  Proof of Lemma 1. We want p so that\nk(\u2206) = \u221e 0 p(\u03b4) max(0, 1 \u2212 \u2206/\u03b4) d\u03b4 (3) = \u2206 0 p(\u03b4) \u2022 0 d\u03b4 + \u221e \u2206 p(\u03b4)(1 \u2212 \u2206/\u03b4) d\u03b4 = \u221e \u2206 p(\u03b4) d\u03b4 \u2212 \u2206 \u221e \u2206 p(\u03b4)/\u03b4 d\u03b4.(4)\nTo solve for p, differentiate twice w.r. M \u2206 is compact and has diameter at most twice diam(M), so we can find an -net that covers M \u2206 using at most T = (4 diam M/r) d balls of radius r [15]. Let {\u2206 i } T i=1 denote the centers of these balls, and let L f denote the Lipschitz constant of f . We have |f (\u2206)| < for all \u2206 \u2208 M \u2206 if |f (\u2206 i )| < /2 and L f < 2r for all i. We bound the probability of these two events.\nSince f is differentiable, L f = \u2207f (\u2206 * ) , where \u2206 * = arg max \u2206\u2208M\u2206 \u2207f (\u2206) . By linearity of expectation, E[\u2207s(\n\u2206)] = \u2207k(\u2206), so E[L 2 f ] = E \u2207s(\u2206 * ) \u2212 \u2207k(\u2206 * ) 2 = E \u2207s(\u2206 * ) 2 \u2212 E \u2207k(\u2206 * ) 2 \u2264 E \u2207s(\u2206 * ) 2 \u2264 E p \u03c9 2 = \u03c3 2 p . By Markov's inequality, Pr L 2 f \u2265 t \u2264 E[L 2 f ]/t, so Pr L f \u2265 2r \u2264 2r\u03c3 p 2 .\n(5)\nThe union bound followed by Hoeffding's inequality applied to the anchors in the -net gives Pr\n\u222a T i=1 |f (\u2206 i )| \u2265 /2 \u2264 2T exp \u2212D 2 /8 .(6)\nCombining ( 5) and ( 6) gives a bound in terms of the free variable r:\nPr sup \u2206\u2208M\u2206 |f (\u2206)| \u2264 \u2265 1 \u2212 2 4 diam(M) r d exp \u2212D 2 /8 \u2212 2r\u03c3 p 2 .(7)\nThis has the form 1 \u2212 \u03ba 1 r \u2212d \u2212 k 2 r 2 . Setting r = \u03ba1 We further subdivide these cells into smaller rectangles of some small width r to ensure that the kernel k varies very little over each of these cells. This results in at most ", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Training linear SVMs in linear time", "journal": "", "year": "2006", "authors": "T Joachims"}, {"ref_id": "b1", "title": "Interior-point methods for massive Support Vector Machines", "journal": "SIAM Journal of Optimization", "year": "2003", "authors": "M C Ferris; T S Munson"}, {"ref_id": "b2", "title": "Sampling techniques for kernel methods", "journal": "", "year": "2001", "authors": "D Achlioptas; F Mcsherry; B Schoelkopf"}, {"ref_id": "b3", "title": "Random projection, margins, kernels, and feature-selection", "journal": "LNCS", "year": "2006", "authors": "A Blum"}, {"ref_id": "b4", "title": "Using sparseness and analytic QP to speed training of Support Vector Machines", "journal": "", "year": "1999", "authors": "J Platt"}, {"ref_id": "b5", "title": "LIBSVM: a library for support vector machines", "journal": "", "year": "2001", "authors": "C.-C Chang; C.-J Lin"}, {"ref_id": "b6", "title": "Fast monte-carlo algorithms for finding low-rank approximations", "journal": "", "year": "1998", "authors": "A Frieze; R Kannan; S Vempala"}, {"ref_id": "b7", "title": "On the nystrom method for approximating a Gram matrix for improved kernel-based learning", "journal": "", "year": "2005", "authors": "P Drineas; M W Mahoney"}, {"ref_id": "b8", "title": "Efficient kernel machines using the improved fast gauss transform", "journal": "", "year": "2004", "authors": "C Yang; R Duraiswami; L Davis"}, {"ref_id": "b9", "title": "Fast gaussian process regression using KD-Trees", "journal": "", "year": "2005", "authors": "Y Shen; A Y Ng; M Seeger"}, {"ref_id": "b10", "title": "Fast image retrieval via embeddings", "journal": "", "year": "2003", "authors": "P Indyk; N Thaper"}, {"ref_id": "b11", "title": "Core Vector Machines: Fast SVM training on very large data sets", "journal": "Journal of Machine Learning Research", "year": "2005", "authors": "I W Tsang; J T Kwok; P.-M Cheung"}, {"ref_id": "b12", "title": "Fourier Analysis on Groups", "journal": "Wiley Classics Library. Wiley-Interscience", "year": "1994", "authors": "W Rudin"}, {"ref_id": "b13", "title": "Comments on the 'Core Vector Machines: Fast SVM training on very large data sets'", "journal": "Journal of Machine Learning Research", "year": "2007-02", "authors": "G Loosli; S Canu"}, {"ref_id": "b14", "title": "A Proofs Lemma 1. Suppose a function k(\u2206) : R \u2192 R is twice differentiable and has the form", "journal": "Bull. Amer. Soc", "year": "2001", "authors": "F Cucker; S Smale"}, {"ref_id": "b15", "title": "\u2212 \u2206 \u03b4 ) d\u03b4. Then p(\u03b4) = \u03b4k(\u03b4)", "journal": "", "year": "", "authors": ""}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "is the second moment of the Fourier transform of k. Further, sup x,y\u2208M |z(x) z(y) \u2212 k(y, x)| \u2264 with any constant probability when D = \u2126 d 2 log \u03c3p diam(M) .", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Random Binning Features. (left) The algorithm repeatedly partitions the input space using a randomly shifted grid at a randomly chosen resolution and assigns to each point x the bit string z(x) associated with the bin to which it is assigned. (right) The binary adjacency matrix that describes this partitioning has z(xi) z(xj) in its ijth entry and is an unbiased estimate of kernel matrix.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 (3Figure 3(left) illustrates the benefit of training classifiers on larger datasets. When training data are easy to obtain, this provides an easy way to improve the accuracy of classifiers without additional modeling effort. Figure 3(middle) and (right) show that good performance can be obtained even from a modest number of features.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 3 :3Figure 3: Accuracy on testing dataset continues to improve as the training set grows. On the Forest dataset, using random binning, doubling the dataset size reduces testing error by up to 40% (left). Error decays quickly as P grows (middle). Training time grows slowly as P grows (right).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "t. to \u2206 to find thatk(\u2206) = \u2212 \u221e \u2206 p(\u03b4)/\u03b4 d\u03b4 andk(\u2206) = p(\u2206)/\u2206. Proof of Claim 1. Define s(x, y) \u2261 z(x) z(y), and f (x, y) \u2261 s(x, y) \u2212 k(y, x), and recall that |f (x, y)| \u2264 2 and E[f (x, y)] = 0 by construction. Since f , and s are shift invariant, as their arguments we use \u2206 \u2261 x \u2212 y \u2208 M \u2206 for notational simplicity.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "\u03c3p diam(M) \u2265 1 and diam(M) \u2265 1, proves the first part of the claim. To prove the second part of the claim, pick any probability for the RHS and solve for D. Proof of Claim 2. M can be covered by rectangles over each of which z is constant. Let \u03b4 pm be the pitch of the pth grid along the mth dimension. Each grid has at most diam(M) \u03b4pm bins, and P overlapping grids produce at most N m = mth dimension. The expected value of the right hand side is P + P diam(M)\u03b1. By Markov's inequality and the union bound, Pr \u2200 d m=1 N m \u2264 t(P + P diam(M)\u03b1) \u2265 1 \u2212 d/t. That is, with probability 1 \u2212 d/t, along every dimension, we have at most t(P + P diam(M)\u03b1) one-dimensional cells. Denote by d mi the width of the ith cell along the mth dimension and observe that Nm i=1 d mi \u2264 diam(M).", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "8 .8dimensional cells over each dimension. Plugging in the upper bound for N m , setting t \u2265 1 \u03b1P and assuming \u03b1 diam(M) \u2265 1, with probability 1 \u2212 d/t, M can be covered withT \u2264 3tP \u03b1 diam(M) r d rectangles of side r centered at {x i } T i=1 . The condition |z(x, y) \u2212 k(x, y)| \u2264 on M \u00d7 M holds if |z(x i , y i ) \u2212 k(x i , y i )| \u2264 \u2212 L k r d and z(x)is constant throughout each rectangle. With r d = 2L k , the union bound followed by Hoeffding's inequality givesPr [\u222a ij |z(x i , y j ) \u2212 k(x i , y j )| \u2265 /2] \u2264 2T 2 exp \u2212P 2 /8 (8)Combining this with the probability that z(x) is constant in each cell gives a bound in terms of t:Pr sup x,y\u2208M\u00d7M |z(x, y) \u2212 k(x, y)| \u2264 \u22651 \u2212 d t \u2212 2(3tP \u03b1 diam(M)) d 2L k exp \u2212 P 2This has the form 1 \u2212 \u03ba 1 t \u22121 \u2212 \u03ba 2 t d . To prove the claim, set t = \u03ba12\u03ba2 1 d+1 , which results in an upper bound of 1 \u2212 3\u03ba 1 \u03ba 1 d+1 2 .", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Comparison of testing error and training time between ridge regression with random features, Core", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "D = O(d \u22122 log 1 2 )", "formula_coordinates": [1.0, 417.71, 721.18, 86.29, 11.1]}, {"formula_id": "formula_1", "formula_text": "R D R 2 \u03c9 x Kernel Name k(\u2206) p(\u03c9) Gaussian e \u2212 \u2206 2 2 2 (2\u03c0) \u2212 D 2 e \u2212 \u03c9 2 2 2 Laplacian e \u2212 \u2206 1 d 1 \u03c0(1+\u03c9 2 d ) Cauchy d 2 1+\u2206 2 d e \u2212 \u2206 1", "formula_coordinates": [3.0, 160.24, 101.45, 331.34, 68.43]}, {"formula_id": "formula_2", "formula_text": "k(x \u2212 y) = R d p(\u03c9)e j\u03c9 (x\u2212y) d\u03c9 = E \u03c9 [\u03b6 \u03c9 (x)\u03b6 \u03c9 (y) * ],(2)", "formula_coordinates": [3.0, 191.15, 367.79, 312.85, 19.31]}, {"formula_id": "formula_3", "formula_text": "\u221a D. The inner product z(x) z(y) = 1 D D j=1 z \u03c9j (x)z \u03c9j (y)", "formula_coordinates": [3.0, 108.0, 477.33, 396.0, 32.96]}, {"formula_id": "formula_4", "formula_text": "Pr [|z(x) z(y) \u2212 k(x, y)| \u2265 ] \u2264 2 exp(\u2212D 2 /4", "formula_coordinates": [3.0, 108.0, 549.34, 197.57, 10.31]}, {"formula_id": "formula_5", "formula_text": "x,y\u2208M |z(x) z(y) \u2212 k(y, x)| \u2265 \u2264 2 8 \u03c3 p diam(M) 2 exp \u2212 D 2 4(d + 2) ,", "formula_coordinates": [3.0, 154.53, 604.86, 320.55, 26.46]}, {"formula_id": "formula_6", "formula_text": "\u03c3 2 p \u2261 E p [\u03c9 \u03c9]", "formula_coordinates": [3.0, 134.47, 640.08, 57.21, 12.19]}, {"formula_id": "formula_7", "formula_text": "R d \u2192 R D so that z(x) z(y) \u2248 k(x \u2212 y).", "formula_coordinates": [4.0, 282.76, 157.14, 169.71, 10.53]}, {"formula_id": "formula_8", "formula_text": "(\u03c9) = 1 2\u03c0 e \u2212j\u03c9 \u03b4 k(\u03b4) d\u2206. Draw D iid samples \u03c9 1 , \u2022 \u2022 \u2022 , \u03c9 D \u2208 R d from p and D iid samples b 1 , . . . , b D \u2208 R from the uniform distribution on [0, 2\u03c0]. Let z(x) \u2261 2 D [ cos(\u03c9 1 x+b1) \u2022\u2022\u2022 cos(\u03c9 D x+b D ) ] .", "formula_coordinates": [4.0, 117.96, 169.71, 386.04, 53.11]}, {"formula_id": "formula_9", "formula_text": "\u2248 + + + \u2022 \u2022 \u2022 = k(x i , x j ) z 1 (x i ) z 1 (x j ) z 2 (x i ) z 2 (x j ) z 3 (x i ) z 3 (x j ) z(x i ) z(x j )", "formula_coordinates": [4.0, 222.35, 384.56, 275.15, 42.37]}, {"formula_id": "formula_10", "formula_text": "Pr u [z(x) z(y) = 1|\u03b4] = E u [z(x) z(y)|\u03b4] = k hat (x, y; \u03b4).", "formula_coordinates": [4.0, 108.0, 613.25, 230.76, 9.65]}, {"formula_id": "formula_11", "formula_text": "k because E \u03b4,u [z(x) z(y)] = E \u03b4 [E u [z(x) z(y)|\u03b4]] = E \u03b4 [k hat (x, y; \u03b4)] = k(x, y).", "formula_coordinates": [4.0, 108.0, 653.17, 396.0, 20.61]}, {"formula_id": "formula_12", "formula_text": "d m=1 k m (|x m \u2212y m |) = k(x\u2212y).", "formula_coordinates": [5.0, 375.36, 141.61, 128.64, 14.11]}, {"formula_id": "formula_13", "formula_text": "x,y\u2208M |z(x) z(y) \u2212 k(x, y)| \u2264 \u2265 1 \u2212 36dP \u03b1 diam(M) exp \uf8eb \uf8ed \u2212 P 2 8 + ln L k d + 1 \uf8f6 \uf8f8 ,", "formula_coordinates": [5.0, 136.65, 317.47, 356.3, 30.39]}, {"formula_id": "formula_14", "formula_text": "Note that \u03b1 = \u221e 0 1 \u03b4 p(\u03b4) d\u03b4 = \u221e 0k(", "formula_coordinates": [5.0, 108.0, 426.88, 152.13, 15.22]}, {"formula_id": "formula_15", "formula_text": "A point x \u2208 R d . A kernel function k(x, y) = d m=1 k m (|x m \u2212 y m |), so that p m (\u2206) \u2261 \u2206k m (\u2206) is a probability distribution on \u2206 \u2265 0. Ensure: A randomized feature map z(x) so that z(x) z(y) \u2248 k(x \u2212 y).", "formula_coordinates": [5.0, 108.0, 469.58, 396.0, 36.15]}, {"formula_id": "formula_16", "formula_text": "x 1 \u2212u 1 \u03b4 1 , \u2022 \u2022 \u2022 , x d \u2212u d \u03b4 d ). end for z(x) \u2261 1 P [ z1(x)\u2022\u2022\u2022z P (x) ] .", "formula_coordinates": [5.0, 117.96, 551.32, 133.53, 39.52]}, {"formula_id": "formula_17", "formula_text": "k(\u2206) = \u221e 0 p(\u03b4) max(0, 1 \u2212 \u2206/\u03b4) d\u03b4 (3) = \u2206 0 p(\u03b4) \u2022 0 d\u03b4 + \u221e \u2206 p(\u03b4)(1 \u2212 \u2206/\u03b4) d\u03b4 = \u221e \u2206 p(\u03b4) d\u03b4 \u2212 \u2206 \u221e \u2206 p(\u03b4)/\u03b4 d\u03b4.(4)", "formula_coordinates": [7.0, 125.31, 649.74, 378.69, 55.22]}, {"formula_id": "formula_18", "formula_text": "\u2206)] = \u2207k(\u2206), so E[L 2 f ] = E \u2207s(\u2206 * ) \u2212 \u2207k(\u2206 * ) 2 = E \u2207s(\u2206 * ) 2 \u2212 E \u2207k(\u2206 * ) 2 \u2264 E \u2207s(\u2206 * ) 2 \u2264 E p \u03c9 2 = \u03c3 2 p . By Markov's inequality, Pr L 2 f \u2265 t \u2264 E[L 2 f ]/t, so Pr L f \u2265 2r \u2264 2r\u03c3 p 2 .", "formula_coordinates": [8.0, 108.0, 184.07, 396.0, 73.6]}, {"formula_id": "formula_19", "formula_text": "\u222a T i=1 |f (\u2206 i )| \u2265 /2 \u2264 2T exp \u2212D 2 /8 .(6)", "formula_coordinates": [8.0, 225.53, 280.47, 278.47, 12.69]}, {"formula_id": "formula_20", "formula_text": "Pr sup \u2206\u2208M\u2206 |f (\u2206)| \u2264 \u2265 1 \u2212 2 4 diam(M) r d exp \u2212D 2 /8 \u2212 2r\u03c3 p 2 .(7)", "formula_coordinates": [8.0, 142.64, 314.7, 361.36, 26.46]}], "doi": ""}