{"title": "FNet: Mixing Tokens with Fourier Transforms", "authors": "James Lee-Thorp; Joshua Ainslie; Ilya Eckstein; Santiago Onta\u00f1\u00f3n", "pub_date": "", "abstract": "We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that \"mix\" input tokens. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths, our FNet model is significantly faster: when compared to the \"efficient Transformers\" on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts. 1   ", "sections": [{"heading": "Introduction", "text": "The Transformer architecture (Vaswani et al., 2017) has achieved rapid and widespread dominance in NLP. At its heart is a attention mechanism -an inductive bias that connects each token in the input through a relevance weighted basis of every other token. Many papers have prodded and probed the Transformer, and in particular the attention sublayers, in an effort to better understand the architecture; see, for example, Tenney et al. (2019); Vig and Belinkov (2019); Clark et al. (2019); Voita et al. (2019). Although potentially limited in their effectiveness (Hewitt and Liang, 2019), these probes generally back the intuition that, by allowing higher order units to form out of compositions of the input, Transformer models can flexibly capture diverse syntactic and semantic relationships.\nIn this work, we investigate whether simpler token mixing mechanisms can wholly replace the relatively complex self-attention layers in Transformer encoder architectures. We first replace the attention sublayer with two parameterized matrix multiplications -one mixing the sequence dimension and one mixing the hidden dimension. Seeing promising results in this simple linear mixing scheme, we further investigate the efficacy of faster, structured linear transformations. Surprisingly, we find that the Fourier Transform, despite having no parameters at all, achieves nearly the same performance as dense linear mixing and scales very efficiently to long inputs, especially on GPUs (owing to the O(N log N ) Fast Fourier Transform (FFT) algorithm). We call the resulting model FNet.\nWhile Fourier Transforms have previously been used to approximate or speed up computations in Convolutional Neural Networks (El-Bakry and Zhao, 2004;Mathieu et al., 2014;Highlander and Rodriguez, 2015;Pratt et al., 2017;Chitsaz et al., 2020;Goldberg et al., 2020), Recurrent Neural Networks (Koplon and Sontag, 1997;Zhang and Chan, 2000;Zhang et al., 2018), Transformers (Choromanski et al., 2020;Tamkin et al., 2020), and MLP layers more generally (Cheng et al., 2015;Moczulski et al., 2016;Sindhwani et al., 2015), we believe our work is the first to wholly replace particular neural network sublayers with a Fourier Transform. This approach of viewing the Fourier Transform as a first class mixing mechanism is reminiscent of the MLP-Mixer (Tolstikhin et al., 2021) for vision, which replaces attention with MLPs; although in contrast to MLP-Mixer, FNet has no learnable parameters that mix along the spatial dimension.\nGiven the favorable asymptotic complexity of the FFT, our work also connects with the literature on \"long sequence\" or \"efficient\" Transformers, which aim to make the attention mechanism scale better via sparsity patterns (Child et al., 2019;Qiu et al., 2020;Parmar et al., 2018;Beltagy et al., 2020;Zaheer et al., 2020;Tay et al., 2020b,a;Kitaev et al., 2020;Vyas et al., 2020; or via linearization of the attention matrix (Katharopoulos et al., 2020;Choromanski et al., 2021;Peng et al., 2021). As we will show in our experiments, while some of those works achieve O(N ) scaling of attention, this complexity often hides large constants, which make them less scalable in practice than FNet.\nThe contributions of our paper are:\n\u2022 We show that simple linear transformations, including even (parameter-free) Fourier Transforms, along with standard MLPs in feedforward layers, are competent at modeling diverse relationships in text. That such a simple linear transformation works at all is surprising, and suggests that, for at least some NLP problems, attention may not be the principal component driving the performance of Transformers.\n\u2022 We introduce a new model, FNet, that uses the Fourier Transform as a mixing mechanism. FNet offers an excellent compromise between speed, memory footprint, and accuracy, achieving 92% and 97%, respectively, of the accuracy of BERT-Base and BERT-Large (Devlin et al., 2019) on the GLUE benchmark , while training 80% faster on GPUs and 70% faster on TPUs.\n\u2022 We find that FNet hybrid models containing only two self-attention sublayers achieve 97 \u2212 99% of their BERT counterparts' accuracy on GLUE, while still running 40 \u2212 70% faster. This indicates that, while attention can improve accuracy, it may not be necessary to use in every layer.\n\u2022 We demonstrate FNet scales very well to long inputs and offers a better compromise between speed and accuracy than the efficient Transformers evaluated on the Long-Range Arena (LRA) benchmark (Tay et al., 2021a). Specifically, FNet achieves accuracy comparable to the most accurate efficient Transformer architectures but is significantly faster at both training and inference than all of the evaluated Transformer architectures across all sequence lengths on GPUs. On TPUs, FNet is faster for relatively shorter sequence lengths; for longer sequences, the only efficient Transformers that are faster than FNet on TPUs are less accurate on the LRA benchmark. Based on this, we argue that rather than seeking more efficient approximations of the attention, there may be more value in seeking out completely new mixing mechanisms.\n2 Related work", "publication_ref": ["b60", "b57", "b61", "b9", "b62", "b19", "b14", "b33", "b20", "b40", "b6", "b16", "b26", "b69", "b68", "b7", "b50", "b4", "b36", "b49", "b58", "b5", "b41", "b38", "b2", "b67", "b25", "b63", "b63", "b8", "b39", "b12", "b53"], "figure_ref": [], "table_ref": []}, {"heading": "Fourier Transforms in neural networks", "text": "Fourier analysis features heavily in studies of the universal approximation properties of neural networks; see, for example, (Cybenko, 1989;Barron, 1993). In terms of practical applications, discrete Fourier Transforms (DFT), and in particular the Fast Fourier Transform (FFT), have been used to tackle signal processing problems such as fitting neural networks to FFTs of electrocardiogram signals (Minami et al., 1999;Gothwal et al., 2011;Mironovova and B\u00edla, 2015) and vibration signals (Zhang et al., 2013), or to evolve solutions of Partial Differential Equations . Because ordinary multiplication in the frequency domain corresponds to a convolution in the time domain, FFTs have been deployed in Convolutional Neural Networks to speed up computations, in Recurrent Neural Networks to speed up training and reduce exploding and vanishing gradients, and generally to approximate dense, linear layers to reduce computational complexity; see references cited in Section 1. DFTs have also been used indirectly in several Transformer works. The Performer (Choromanski et al., 2020) linearizes the Transformer selfattention mechanism by leveraging random Fourier features to approximate a Gaussian representation of the softmax kernel. In our work, rather than approximating attention, we replace attention with the Fourier Transform, which acts as an alternate hidden representation mixing mechanism. Tamkin et al. (2020) use spectral filters to generate hierarchical features, showing that the filtered embeddings perform well in different tasks (word-level, sentence-level or document-level), depending on which frequency scales are filtered. In contrast to FNet, they separate Fourier frequencies, rather than using the transform to combine features.", "publication_ref": ["b11", "b1", "b34", "b17", "b35", "b70", "b7", "b50"], "figure_ref": [], "table_ref": []}, {"heading": "Modeling semantic relations via attention", "text": "Attention models have achieved state of the art results across virtually all NLP tasks and even some image tasks (Dosovitskiy et al., 2021). This success is generally attributed to the flexibility and capacity of attention. Although some works (Ramsauer et al., 2021) have endeavoured to gain a deeper understanding of attention, the pervasive intuition is that the success of attention models derives from the token-dependent attention patterns in different layers; see, for example, (Tenney et al., 2019). However, it is natural to ask: Do we really need the flexibility, and associated cost, of attention? Tay et al. (2020a) empirically investigated the importance of the dot product operation in the attention mechanism in their Synthesizer model (related to our \"Linear\" baseline below). They find that learnt token-dependent attention weights are highly expressive, but not necessarily crucial for realizing accurate NLP models. You et al. (2020) replace attention weights in the Transformer encoder and decoder with unparameterized Gaussian distributions, showing minimal performance degradation provided they retain learnable cross-attention weights. Similarly, Raganato et al. (2020) find little to no accuracy degradation when replacing all but one of the attention heads of each attention layer in the encoder with fixed, non-learnable positional patterns. Finally, Tolstikhin et al. (2021) present MLP-Mixer, where attention is replaced by MLPs, with limited performance degradation in image classification tasks.", "publication_ref": ["b13", "b44", "b57", "b51", "b66", "b43", "b58"], "figure_ref": [], "table_ref": []}, {"heading": "Efficient and long sequence models", "text": "The standard attention mechanism (Vaswani et al., 2017) has a quadratic time and memory bottleneck with respect to sequence length. This limits its applicability in tasks involving long range dependencies. Most efforts to improve attention efficiency are based on sparsifying the attention matrix. Tay et al. (2020c) survey many of the recent efficient attention works; see also citations in Section 1. Several \"efficient Transformers\" achieve O(N \u221a N ) or even O(N ) theoretical complexity. However, the constants hidden by this notation can be large. For example, in models such as Longformer (Beltagy et al., 2020), ETC , and Big-Bird (Zaheer et al., 2020), attention is O(N ) as a function of the input length, but quadratic in the number of \"global tokens\"; the latter must be sufficiently large to ensure good performance.\nThe Long-Range Arena benchmark (Tay et al., 2021a) attempts to compare many of the efficient Transformers in a series of tasks requiring long range dependencies, finding that the Performer (Choromanski et al., 2021), Linear Transformer (Katharopoulos et al., 2020), Linformer , and Image Transformer (Local Attention) (Parmar et al., 2018) were the fastest on TPUs and had the lowest peak memory usages per device. 2 Instead, in this paper we completely replace self-attention with a different mixing, namely the Fourier Transform, which offers: (1) performance, (2) reduced model size (no learnable parameters), and (3) simplicity.\nFinally, we note that, in an effort to investigate different token mixing mechanisms, we compare a vanilla BERT model (Devlin et al., 2019) with a vanilla FNet, ignoring more recent Transformer optimizations, which we consider orthogonal to this work; see, for example, (Narang et al., 2021;Kim and Hassan, 2020;Shleifer and Rush, 2020).", "publication_ref": ["b60", "b55", "b2", "b67", "b53", "b8", "b63", "b38", "b12", "b37", "b24", "b48"], "figure_ref": [], "table_ref": []}, {"heading": "Model", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Discrete Fourier Transform", "text": "The Fourier Transform decomposes a function into its constituent frequencies. Given a sequence {x n } with n \u2208 [0, N \u2212 1], the discrete Fourier Transform (DFT) is defined by the formula:\nX k = N \u22121 n=0 x n e \u2212 2\u03c0i N nk , 0 \u2264 k \u2264 N \u2212 1. (1)\nFor each k, the DFT generates a new representation X k as a sum of all of the original input tokens x n , with so-called \"twiddle factors\". There are two primary approaches to computing the DFT: the Fast Fourier Transform (FFT) and matrix multiplication.\nThe standard FFT algorithm is the Cooley-Tukey algorithm (Cooley and Tukey, 1965;Frigo and Johnson, 2005), which recursively re-expresses the DFT of a sequence of length N = N 1 N 2 in terms of N 1 smaller DFTs of sizes N 2 to reduce the computation time to O(N log N ). An alternative approach is to simply apply the DFT matrix to the input sequence. The DFT matrix, W , is a Vandermonde matrix for the roots of unity up to a normalization factor: where n, k = 0, . . . , N \u2212 1. This matrix multiplication is an O(N 2 ) operation, which has higher asymptotic complexity than the FFT, but turns out to be faster for relatively shorter sequences on TPUs.\nW nk = e \u2212 2\u03c0i N nk / \u221a N ,(2)", "publication_ref": ["b10", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "FNet architecture", "text": "FNet is an attention-free Transformer architecture, wherein each layer consists of a Fourier mixing sublayer followed by a feed-forward sublayer. The architecture is shown in Figure 1. Essentially, we replace the self-attention sublayer of each Transformer encoder layer with a Fourier sublayer, which applies a 2D DFT to its (sequence length, hidden dimension) embedding input -one 1D DFT along the sequence dimension, F seq , and one 1D DFT along the hidden dimension, F h : 3\ny = F seq (F h (x)) .(3)\nAs indicated by Equation (3), we only keep the real part of the result; hence, we do not need to modify the (nonlinear) feed-forward sublayers or output layers to handle complex numbers. We found that FNet obtained the best results when the real part of the total transformation was only extracted at the end of the Fourier sublayer; that is, after applying both F seq and F h . We also experimented with the Hadamard, Hartley and Discrete Cosine Transforms. Of these three, the Hartley Transform was the strongest alternative, obtaining comparable accuracy to Equation (3); see Appendix A.3 for details.\nThe simplest interpretation for the Fourier Transform is as a particularly effective mechanism for mixing tokens, which provides the feed-forward sublayers sufficient access to all tokens. Because of the duality of the Fourier Transform, we can also view each alternating encoder block as applying alternating Fourier and inverse Fourier Transforms, transforming the input back and forth between the \"time\" and frequency domain. Because multiplying by the feed-forward sublayer coefficients in the frequency domain is equivalent to convolving (with a related set of coefficients) in the time domain, FNet can be thought of as alternating between multiplications and convolutions. 4 We use the same embedding layers as in Devlin et al. (2019); namely, we combine the word embeddings, absolute position embeddings of the tokens and type embeddings of the sentences. Because of the positional information encoded by the Fourier Transform in Equation ( 1) (see n, k indices), FNet performs just as well without position embeddings. Nevertheless, we include the position embeddings to allow for a cleaner comparison with BERT.", "publication_ref": ["b12"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Implementation", "text": "Empirically, we found that on GPUs: the FFT is faster than matrix multiplications for all sequence lengths we consider (512 \u2212 8192 tokens), whereas on TPUs: for relatively shorter sequences (\u2264 4096 tokens), it is faster to cache the DFT matrix and then compute the DFT through matrix multiplications than using the FFT; for longer sequences, the FFT is faster. As a result, our GPU FNet implementation always uses the FFT, while our TPU implementation computes the 2D DFT using matrix multiplications for sequences up to lengths of 4096 and the FFT for longer lengths. Presumably the GPU vs TPU difference is primarily a result of two factors: (1) TPUs are even more highly optimized for matrix multiplications than GPUs, and (2) GPUs offer a more efficient FFT implementation than TPUs. We suspect that FNet will only become more performant on TPUs as the TPU implementation of the FFT improves. Our model uses JAX and, in particular, the Flax framework 5 . Core \nd h + 4nd 2 h 112M 339M Linear n 2 d h + nd 2 h 94M 269M FNet (mat) n 2 d h + nd 2 h 83M 238M FNet (FFT) nd h log(n)+ 83M 238M nd h log(d h ) Random n 2 d h + nd 2 h 83M 238M FF-only 0 83M 238M\nmodel code is given in Appendix A.7 and the full source core is available online. 6\n4 Results", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Transfer learning", "text": "We compare FNet and Transformer architectures in a common transfer learning setting. For a fuller picture, we compare multiple models (see Table 1 for parameter counts in \"Base\" configuration):\n\u2022 BERT-Base: a Transformer encoder model.\n\u2022 FNet encoder: we replace every self-attention sublayer with a Fourier sublayer.\n\u2022 Linear encoder: we replace each self-attention sublayer with a two learnable, dense, linear sublayers, one applied to the hidden dimension and one to the sequence dimension.\n\u2022 Random encoder: we replace each selfattention sublayer with a two constant random matrices, one applied to the hidden dimension and one applied to the sequence dimension.\n\u2022 Feed Forward-only (FF-only) encoder: we completely remove the self-attention sublayer; so that this model has no token mixing.\nDespite its simplicity, the Linear baseline turns out to be surprisingly accurate and fast. Our Linear model is similar to the MLP-Mixer (Tolstikhin et al., 2021) (for vision) and also the Random Synthesizer (Tay et al., 2020a), but simplifies the latter model further by removing the multiple heads and softmax projections, resulting in just two matrix multiplications in the mixing sublayer.\nIt is reasonable to expect that the Linear encoder, which uses densely parameterized mixing layers, will learn more flexibly than FNet, which uses parameter-free mixing layers. As we will show, although the Linear-Base model outperforms FNet-Base slightly on GLUE (0.3 points), it has several efficiency drawbacks relative to FNet: it has a much larger memory footprint (see Table 4b), it is slower to train on regular 512 sequence lengths (see Table 3), and scales significantly worse on long sequence lengths (see Tables 4b-4c). 7 We also found that Linear-Large was more difficult to train due to gradient blow up (see \"Large\" scores in Table 2).\nWe adopt the same fixed \"Base\" and \"Large\" model and training configurations as for the original BERT (Devlin et al., 2019), except that we pretrain on the much larger C4 dataset (Raffel et al., 2020) and use a 32000 SentencePiece vocabulary model (Kudo and Richardson, 2018) (see Appendix A.1 for full pre-training details). For fine-tuning on the GLUE benchmark , we found that different BERT runs with the same base learning rate could yield slightly different results. Consequently, for the Base (Large) models, we performed 3 (6) trials, respectively, for each base learning rate and reported the best result across all experiments. This reflects our observation that BERT-Large was less stable than BERT-Base, as noted in Devlin et al. (2019).\nWe report the results for the best base learning rate (no early stopping) on the GLUE Validation split in Table 2. 8 For Base models, results mirror the pre-training metrics (see Appendix A.1): BERT performs best. FNet and the Linear model both underperform BERT by 7.5 \u2212 8%. Referring to Table 3, we see that although less accurate, FNet trains significantly faster than BERT -80% faster on GPUs and 70% faster on TPUs -and performs 63% of BERT's FLOPS. Measured in isolation, the Fourier sublayers perform forward and backward passes an order of magnitude faster than the self-attention sublayers (see Appendix A.4), but FNet's overall training speed is impeded by the feed-forward sublayers that all models share.  Returning to Table 2: the FF-only model severely underperforms all other models: as expected, token mixing is critical to the expressivity of the model. For example, 50% accuracy scores on the binary classification tasks (QNLI, SST-2, RTE), indicate that the model fails to learn the tasks. The weak accuracy of the Random model suggests that not just any mixing will do; rather, a structured mixing is required. We also include metrics from a hybrid FNet attention model. In the hybrid model, we replace the final two Fourier sublayers of FNet with self-attention sublayers -other configurations are possible, but we generally found that replacing the final layers worked best; see Appendix A.5. With the addition of just two self-attention sublayers, the hybrid FNet models achieve 97% and 99% of their respective BERT counterpart's accuracies with only limited speed degradations (see Table 3). Interestingly, the gap between BERT and FNet shrinks to just 3% for Large models; this is likely due to FNet-Large being more stable during training than BERT-Large. 9 The Linear-Large model severely underperforms its Base counterpart on GLUE benchmark due to training instabilities. We generally found that the Linear model and BERT were less stable than the models with no parameters in their mixing sublayers, namely the FNet, Random and FF-only models.\nThe speed vs MLM accuracy curve for GPU (8 V100 chips) pre-training is shown in Figure 2 (see Appendix A.2 for TPU results). Both TPU and GPU models are trained for 1 million steps as in  6 in Appendix A.1. We found that the smaller model architectures benefited from larger learning rates, so we select the best result using 10 \u22123 and 10 \u22124 for all models. 10 The GPU (Figure 2), and TPU (Figure 3 in Appendix A.2) results display the same trends. For larger, slower models, BERT and FNet-Hybrid define the Pareto speed-accuracy efficiency frontier. For smaller, faster models, FNet and the Linear model define the efficiency frontier.", "publication_ref": ["b51", "b12", "b42", "b27", "b12"], "figure_ref": ["fig_1", "fig_1", "fig_2"], "table_ref": ["tab_0", "tab_3", "tab_2", "tab_3", "tab_1", "tab_1", "tab_2", "tab_1", "tab_2", "tab_5"]}, {"heading": "Long-Range Arena (LRA) benchmark", "text": "Of the efficient Transformers evaluated on LRA benchmark by Tay et al. (2021a), their results suggest that (1) the vanilla Transformer is (by a small margin) the second most accurate model, and\n(2) the Performer (Choromanski et al., 2021) is the fastest model. We benchmark FNet's accuracy against both of these models using Tay et al. (2021a)'s codebase and running on the same hardware (4 \u00d7 4 TPU v3 chips); the results are shown in Table 4a. 11 To ensure a fair comparison, we also report the results of our own experiments for the 10 We have opted to compare FNet with Transformer models as the latter are the most commonly used models in NLP transfer learning settings. It would also be interesting to compare FNet with convolutional-based models, although, to our knowledge, such models have only recently found limited success in pre-training NLP setups (Tay et al., 2021b); and even there, the authors did not consider the small model regime. 11 The \"Linear\" model in Table 4 is the baseline model introduced in Section 4.1.\nvanilla Transformer (see Appendix A.6 for details).\nTable 4a suggests that, in aggregate, the (vanilla) Transformer and FNet obtain comparable results. Given that the Transformer is the second most accurate model evaluated by Tay et al. (2021a) and that the relative differences in the average accuracy scores within Table 4a are small, our results suggest that FNet is competitive with the most accurate of the efficient Transformers on LRA.\nTurning to efficiency, in Table 4b, we provide training speed and memory usage statistics from our experiments on GPUs (8 V100 chips); see Appendix A.2 for results on TPUs. We perform a sweep over sequence lengths {512, 1024, 2048, 4096, 8192}. On GPUs, FNet is much faster than all other models across all sequence lengths, due to the highly efficient FFT implementation on GPUs. Table 4b also indicates that FNet has a lighter memory footprint (this holds for both GPUs and TPUs; see extended results in Appendix A.2). This is partly because FNet has no learnable parameters in its mixing sublayer, but also due to the FFT's efficiency, especially at longer sequence lengths. Lastly, Table 4c shows that training speed gains generally carry over to inference gains (see Appendix A.2 for detailed TPU results).", "publication_ref": ["b53", "b8", "b53"], "figure_ref": [], "table_ref": ["tab_3", "tab_3", "tab_3", "tab_3", "tab_3", "tab_3", "tab_3"]}, {"heading": "Conclusions", "text": "In this work, we studied simplified token mixing modules for Transformer-like encoder architectures, making several contributions. First, we showed that simple, linear mixing transformations, along with the nonlinearities in feed-forward layers, can competently model diverse semantic relationships in text. Second, we introduced FNet, a Transformer-like model wherein the self-attention sublayer is replaced by an unparameterized Fourier Transform. FNets achieve 92 and 97% of their respective BERT-Base and BERT-Large counterparts' accuracy on the GLUE benchmark, but train 70 \u2212 80% faster on GPUs/TPUs. Third, because of its favorable scaling properties, FNet is very competitive with the \"efficient Transformers\" evaluated on the Long-Range Arena benchmark, matching the accuracy of the most accurate models while being much faster and lighter on memory.\nOur work highlights the potential of linear units as a drop-in replacement for the attention mechanism in text classification tasks. We found the Fourier Transform to be a particularly efficient and effective mixing mechanism, due to the speed of the FFT. However, we only performed a cursory survey of other linear transformations (see also Appendix A.3), and additional fast alternatives are worth exploring.\nGiven the speed and accuracy advantages of smaller FNet models relative to Transformers, we suspect that FNet will be effective as a lightweight, distilled student model deployed in resourceconstrained settings such as production services or on edge devices. The need for such lightweight serving models is only forecast to grow given the interest in giant models (Raffel et al., 2020;Brown et al., 2020;Lepikhin et al., 2021). A natural avenue to explore in this regard is knowledge distillation of small FNet models from larger Transformer teacher models, following, for example, Sanh et al. (2019); Jiao et al. (2020); Turc et al. (2019).\nAnother aspect of interest and worthy of further study is hybrid FNet-attention models. We found that adding only a few self-attention sublayers to FNet offers a simple way to trade speed for accuracy. Specifically, replacing the final two Fourier sublayers with self-attention provided 97 \u2212 99% of BERT's accuracy with limited speed penalties.\nThroughout this work we have restricted our focus to encoders. FNet decoders can be designed by \"causally\" masking the Vandermonde matrix, but a lower level implementation is required to introduce causal masking to FFTs. How to adapt Fourier mixing for encoder-decoder cross-attention is an open question as evidence suggests that crossattention may be crucial to performance (You et al., 2020). We have focused on tasks which do not require generation so we leave FNet decoders and encoder-decoder setups to future work; although we do remark that the FNet encoder could be used as a drop in replacement in a Transformer as other works have successfully demonstrated; see, for example, (Zaheer et al., 2020;Guo et al., 2021). ", "publication_ref": ["b42", "b29", "b46", "b22", "b59", "b66", "b67", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "A Appendices", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1 Pre-training details", "text": "We adopt the same fixed \"Base\" and \"Large\" model and learning configurations as for the original BERT (Devlin et al., 2019). We train on the much larger C4 dataset (Raffel et al., 2020) and use a 32000 SentencePiece vocabulary model (Kudo and Richardson, 2018) trained on a 100 million sentence subset of C4. Our TPU experiments use a batch size of 256 as in Devlin et al. (2019) and are each run on 4 \u00d7 4 TPU v3 chips. Our GPU experiments use a smaller batch size of 64 and are run on 8 V100 chips. Because the training configuration is lifted from Devlin et al. (2019), it may be slightly biased towards the BERT attention model.\nTable 5 summarizes the pre-training metrics for the different models; the pre-training speeds are shown in Table 3 in the main text. Although they have weaker accuracy metrics, the Linear model and FNet train nearly 80% faster than BERT on GPUs, and 70% faster on TPUs (see Table 3). We also find that the three models with no learnable parameters in their mixing layer, namely FNet, the Random model and the FF-only model, are the most stable during training.\nBERT's higher accuracy on the MLM pretraining task is not simply a result of having more parameters than the other models. Indeed, Table 5 shows that BERT-Base is actually more accurate than FNet-Large, which contains more than twice as many parameters. BERT is presumably more expressive because the mixing (attention) weights are both task specific and token dependent, determined  Turc et al. (2019), for all models, we fix the feed-forward size to 4d h and the number of self-attention heads to d h /64. Smaller architectures have a similar number of parameters across all models because the majority of parameters are in the embedding layers. Each FNet-Hybrid (\"FNet-H\") model contains 2 self-attention sublayers. We exclude FNet-Hybrid models with only 2 total layers.", "publication_ref": ["b12", "b42", "b27", "b12", "b12", "b59"], "figure_ref": [], "table_ref": ["tab_4", "tab_2", "tab_2", "tab_4"]}, {"heading": "Dimensions", "text": "Parameters  Tay et al. (2020a). FNet's mixing weights, on the other hand, are neither task specific nor token dependent.\nFinally, Table 6 shows the model sizes that were used to construct Figure 2 (main text) and Figure 3 (Appendix A.2).", "publication_ref": ["b51"], "figure_ref": ["fig_1", "fig_2"], "table_ref": ["tab_5"]}, {"heading": "A.2 TPU results", "text": "In this section, we report FNet efficiency results for TPUs; the main text focuses on GPUs. Figure 3 shows the speed vs MLM pre-training accuracy curve when training on TPU (4 \u00d7 4 v3 chips). As on GPUs, FNet and the Linear model define the Pareto efficiency frontier for smaller, faster models, while BERT defines the frontier for larger, slower models.\nTable 7 shows Long Range Arena Text classification efficiency results on TPUs (4 \u00d7 4 v3 chips). The Linear model and FNet train faster than all the efficient Transformers for sequence lengths \u2264 2048 and 512, respectively. For longer sequences, FNet is slower than the Performer and, based on results in Tay et al. (2021a), likely also slower than the other efficient Transformers that linearize attention, namely Local Attention (Parmar et al., 2018), Linformer  and Linear Transformer (Katharopoulos et al., 2020). However, it is worth noting that Table 4a suggests that FNet is more accurate than all of the aforementioned models. Moreover, we expect that the GPU speed gains will  ", "publication_ref": ["b53", "b38", "b63"], "figure_ref": ["fig_2"], "table_ref": ["tab_7", "tab_3"]}, {"heading": "A.3 Additional configurations that we experimented with", "text": "We experimented with a number of additional ideas to improve FNet. Fourier Transform algorithm. On GPUs, the FFT was the fastest algorithm for computing the DFT across all sequence lengths that we experimented with (512 \u2212 8192). On TPUs, it is faster to compute the DFT directly using matrix multiplications for relatively shorter sequence lengths (up to lengths of 4096; see Table 7). This efficiency boundary between matrix multiplication and FFT on TPUs will change depending on the XLA precision for the matrix multiplications. We found that, although (slower) HIGHEST XLA precision was required to very accurately reproduce FFT in computing the DFT, (faster) DEFAULT XLA precision was sufficient to facilitate accurate model convergence.\nModifying the Fourier Transform computation. To keep the entire FNet architecture simple, the Fourier sublayer accepts real input and returns real output. The standard Fourier sublayer in FNet simply extracts the real part after computing the 2D DFT. We found that FNet was less accurate and less stable during training if only the real part of the DFT was used throughout the computation. Simply extracting the absolute value (instead of the real part) also led to a significantly less accurate model. Because the feed-forward sublayer mixes the hidden dimension, we experimented with applying a 1D DFT along the token dimension only in the Fourier sublayer (i.e. no hidden dimension mixing in the Fourier sublayer). This yielded some training speed gains but hurt accuracy. The 1D (token mixing only) DFT model still significantly outperformed the (no token mixing) FF-only model, indicating that token mixing is most important mechanism in the Fourier sublayer.\nOther transforms. We experimented with three natural alternatives to the Fourier Transform:\n\u2022 Discrete Cosine Transform (DCT). The DCT is closely related to the DFT but transforms real input to real output. However, we found that the DCT model underperformed FNet (\u223c 4% accuracy degradation).\n\u2022 Hadamard Transform 12 . Although the Hadamard Transform was slightly faster than the DFT, it yielded less accurate results (\u223c 2% accuracy degradation).\n\u2022 Hartley Transform. The Hartley Transform, which transforms real input to real output, can be described in terms of the Fourier Transform: H = {F} \u2212 {F }. We found that the Hartley Transform matched the Fourier Transform on GLUE (76.7 vs. 76.7).\nIntroducing learnable parameters to the Fourier sublayer. Our attempts to introduce learnable parameters into the Fourier sublayer were either detrimental or inconsequential, and generally slightly slowed the model. For the (sequence length, hidden dimension) input in each Fourier sublayer, we tried two approaches to introduce learnable parameters: (1) element wise multiplication with a (sequence length, hidden dimension) matrix, and (2) regular matrix multiplication with (sequence length, sequence length) and (hidden dimension, hidden dimension) matrices. We experimented with these approaches in various configurations: preceding and/or following the DFT, and also in combination with inverse DFT (e.g. transform to frequency domain, apply element wise multiplication, transform back to time domain), but most setups degraded accuracy and reduced training stability, while a few did not change accuracy but lead to small speed decreases. In a slightly different set of experiments and in an effort to provide more flexibility to the model, we added (complex) learnable weights to the 2D DFT matrix. This model was stable but did not yield any accuracy gains, suggesting that the DFT is locally optimal in some sense.\nFNet block modifications. The standard FNet encoder block structure follows that of the Transformer: a Fourier sublayer followed by a feedforward sublayer, with residual connections and layer norms after each sublayer; see Figure 1. We tried several modifications to this structure, based on the intuition of moving in and out of the frequency domain between multiplications. For example, the sandwiching of Fourier, feed-forward, Fourier (or inverse Fourier) sublayers and only applying the residual connections and layer norms to the final result, yields a structure that more closely  Although the Fourier mixing sublayer itself performs forward and backward passes significantly faster than the self-attention sublayer, FNet is overall 70-80% faster than BERT because the overall training and inference speeds are bottle-necked by the feed-forward sublayers that all models share.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": ["tab_7"]}, {"heading": "A.5 FNet-Hybrid ablations", "text": "Table 9 shows the effects of varying the number of attention sublayers and the attention layout in the FNet-Hybrid model. For the \"BOTTOM\" layout, all attention sublayers are placed in the first few encoder layers, where they replace the Fourier mixing sublayers. For the \"TOP\" layout, attention sublayers are placed in the final encoder layers; for the \"MIDDLE\" layout they are placed in the middle layers; and for the \"MIXED\" layout, they are distributed through the model. From the Table 9, we can make two observations: (1) more attention improves accuracy at the cost of speed, and ultimately with diminishing returns;\n(2) placing attention layers at the top of the model gives the best accuracy results. Given our focus on speed, we chose to focus FNet-Hybrid experiments in the main text of the paper on the 2 attention layer, \"TOP\" configuration variant.\nA.6 A note on Long-Range Arena hyperparameter settings\nConcerning the Long-Range Arena setup, several hyperparameters are not described in Tay et al. (2021a) and there a few mismatches between the configurations described in the paper and the code repository. Where possible, we prioritize configurations described in the paper with only two exceptions. Firstly, for the CIFAR10 (Image) task, we perform a sweep of the number of layers in the range [1,2,3,4]. We found that 1 layer worked best for all models; Tay et al. (2021a) suggest 3 layers yielded the best results. Secondly, for the Pathfinder task, we found that a base learning rate of 0.001 (as given in the code repository) yielded better results for all models than the 0.01 value indicated in Tay et al. (2021a). We also perform a very small sweep over the embedding dimension and batch size, which are not listed in Tay et al. (2021a).\nWe also remark that the accuracy comparisons between our runs and those from Tay et al. (2021a) should be performed with the caveat that we found that results for certain tasks -Text and Retrieval in particular -can vary quite a bit between runs, especially for the Transformer; we report the best results.", "publication_ref": ["b53", "b53", "b53", "b53", "b53"], "figure_ref": [], "table_ref": ["tab_10", "tab_10"]}, {"heading": "A.7 FNet code", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "1 i m p o r t f l a x . l i n e n as nn 2 i m p o r t j a x 3 i m p o r t j a x . numpy as j n p 4 5 6 class F o u r i e r T r a n s f o r m L a y e r ( nn . Module ) : 7 @nn. compact 8 d e f _ _ c a l l _ _ ( s e l f , x ) : 9 r e t u r n j a x . vmap ( j n p . f f t . f f t n ) ( x ) . r e a l 10 11 12 class FeedForwardLayer ( nn . ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Etc: Encoding long and structured inputs in transformers", "journal": "", "year": "2020", "authors": "Joshua Ainslie; Santiago Ontanon; Chris Alberti; Vaclav Cvicek; Zachary Fisher; Philip Pham; Anirudh Ravula; Sumit Sanghai; Qifan Wang; Li Yang"}, {"ref_id": "b1", "title": "Universal approximation bounds for superpositions of a sigmoidal function", "journal": "IEEE Transactions on Information theory", "year": "1993", "authors": " Andrew R Barron"}, {"ref_id": "b2", "title": "Longformer: The long-document transformer", "journal": "", "year": "2020", "authors": "Iz Beltagy; E Matthew; Arman Peters;  Cohan"}, {"ref_id": "b3", "title": "Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners", "journal": "Curran Associates, Inc", "year": "", "authors": "Tom Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared D Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell; Sandhini Agarwal; Ariel Herbert-Voss; Gretchen Krueger; Tom Henighan; Rewon Child; Aditya Ramesh; Daniel Ziegler; Jeffrey Wu; Clemens Winter; Chris Hesse; Mark Chen; Eric Sigler; Mateusz Litwin"}, {"ref_id": "b4", "title": "An exploration of parameter redundancy in deep networks with circulant projections", "journal": "", "year": "2015", "authors": "Yu Cheng; Felix X Yu; Rogerio S Feris; Sanjiv Kumar; Alok Choudhary; Shi-Fu Chang"}, {"ref_id": "b5", "title": "Generating long sequences with sparse transformers", "journal": "", "year": "2019", "authors": "Rewon Child; Scott Gray; Alec Radford; Ilya Sutskever"}, {"ref_id": "b6", "title": "Acceleration of convolutional neural network using fft-based split convolutions", "journal": "", "year": "2020", "authors": "Kamran Chitsaz; Mohsen Hajabdollahi; Nader Karimi; Shadrokh Samavi; Shahram Shirani"}, {"ref_id": "b7", "title": "Masked language modeling for proteins via linearly scalable long-context transformers", "journal": "", "year": "2020", "authors": "Krzysztof Choromanski; Valerii Likhosherstov; David Dohan; Xingyou Song; Jared Davis; Tamas Sarlos; David Belanger; Lucy Colwell; Adrian Weller"}, {"ref_id": "b8", "title": "Rethinking attention with performers", "journal": "", "year": "2021-05-03", "authors": "Valerii Krzysztof Marcin Choromanski; David Likhosherstov; Xingyou Dohan; Andreea Song; Tam\u00e1s Gane; Peter Sarl\u00f3s; Jared Quincy Hawkins; Afroz Davis; Lukasz Mohiuddin; David Benjamin Kaiser; Lucy J Belanger; Adrian Colwell;  Weller"}, {"ref_id": "b9", "title": "What does BERT look at? an analysis of BERT's attention", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Kevin Clark; Urvashi Khandelwal; Omer Levy; Christopher D Manning"}, {"ref_id": "b10", "title": "An algorithm for the machine calculation of complex fourier series", "journal": "Mathematics of computation", "year": "1965", "authors": "W James; John W Cooley;  Tukey"}, {"ref_id": "b11", "title": "Approximation by superpositions of a sigmoidal function", "journal": "", "year": "1989", "authors": "George Cybenko"}, {"ref_id": "b12", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Long and Short Papers", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b13", "title": "An image is worth 16x16 words: Transformers for image recognition at scale", "journal": "", "year": "2021-05-03", "authors": "Alexey Dosovitskiy; Lucas Beyer; Alexander Kolesnikov; Dirk Weissenborn; Xiaohua Zhai; Thomas Unterthiner; Mostafa Dehghani; Matthias Minderer; Georg Heigold; Sylvain Gelly; Jakob Uszkoreit; Neil Houlsby"}, {"ref_id": "b14", "title": "Fast object/face detection using neural networks and fast fourier transform", "journal": "International Journal of Signal Processing", "year": "2004", "authors": "M Hazem; Qiangfu El-Bakry;  Zhao"}, {"ref_id": "b15", "title": "The design and implementation of fftw3", "journal": "Proceedings of the IEEE", "year": "2005", "authors": "Matteo Frigo; G Steven;  Johnson"}, {"ref_id": "b16", "title": "Rethinking fun: Frequencydomain utilization networks", "journal": "", "year": "2020", "authors": "Kfir Goldberg; Stav Shapiro; Elad Richardson; Shai Avidan"}, {"ref_id": "b17", "title": "Cardiac arrhythmias detection in an ecg beat signal using fast fourier transform and artificial neural network", "journal": "Journal of Biomedical Science and Engineering", "year": "2011", "authors": "Himanshu Gothwal; Silky Kedawat; Rajesh Kumar"}, {"ref_id": "b18", "title": "Longt5: Efficient text-to-text transformer for long sequences", "journal": "", "year": "2021", "authors": "Mandy Guo; Joshua Ainslie; David Uthus; Santiago Ontanon; Jianmo Ni; Yun-Hsuan Sung; Yinfei Yang"}, {"ref_id": "b19", "title": "Designing and interpreting probes with control tasks", "journal": "", "year": "2019", "authors": "John Hewitt; Percy Liang"}, {"ref_id": "b20", "title": "Very efficient training of convolutional neural networks using fast fourier transform and overlap-and-add", "journal": "BMVA Press", "year": "2015-09-07", "authors": "Tyler Highlander; Andres Rodriguez"}, {"ref_id": "b21", "title": "Data movement is all you need: A case study of transformer networks", "journal": "", "year": "2020", "authors": "Andrei Ivanov; Nikoli Dryden; Tal Ben-Nun; Shigang Li; Torsten Hoefler"}, {"ref_id": "b22", "title": "TinyBERT: Distilling BERT for natural language understanding", "journal": "", "year": "2020", "authors": "Xiaoqi Jiao; Yichun Yin; Lifeng Shang; Xin Jiang; Xiao Chen; Linlin Li; Fang Wang; Qun Liu"}, {"ref_id": "b23", "title": "Nikolaos Pappas, and Fran\u00e7ois Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention", "journal": "PMLR", "year": "", "authors": "Angelos Katharopoulos; Apoorv Vyas"}, {"ref_id": "b24", "title": "FastFormers: Highly efficient transformer models for natural language understanding", "journal": "", "year": "2020", "authors": "Jin Young; Hany Kim;  Hassan"}, {"ref_id": "b25", "title": "Reformer: The efficient transformer", "journal": "", "year": "2020-04-26", "authors": "Nikita Kitaev; Lukasz Kaiser; Anselm Levskaya"}, {"ref_id": "b26", "title": "Using fourier-neural recurrent networks to fit sequential input/output data", "journal": "Neurocomputing", "year": "1997", "authors": "Ren\u00e9e Koplon; Eduardo D Sontag"}, {"ref_id": "b27", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Taku Kudo; John Richardson"}, {"ref_id": "b28", "title": "On the equivalence between one-dimensional discrete walsh-hadamard and multidimensional discrete fourier transforms", "journal": "IEEE Computer Architecture Letters", "year": "1979", "authors": "O Henry;  Kunz"}, {"ref_id": "b29", "title": "Gshard: Scaling giant models with conditional computation and automatic sharding", "journal": "", "year": "2021-05-03", "authors": "Dmitry Lepikhin; Hyoukjoong Lee; Yuanzhong Xu; Dehao Chen; Orhan Firat; Yanping Huang; Maxim Krikun; Noam Shazeer; Zhifeng Chen"}, {"ref_id": "b30", "title": "Fourier neural operator for parametric partial differential equations", "journal": "", "year": "2021-05-03", "authors": "Zongyi Li; Nikola Borislavov Kovachki; Kamyar Azizzadenesheli; Burigede Liu; Kaushik Bhattacharya; Andrew M Stuart; Anima Anandkumar"}, {"ref_id": "b31", "title": "Fft-based deep learning deployment in embedded systems", "journal": "IEEE", "year": "2018", "authors": "Sheng Lin; Ning Liu; Mahdi Nazemi; Hongjia Li; Caiwen Ding; Yanzhi Wang; Massoud Pedram"}, {"ref_id": "b32", "title": "Generating wikipedia by summarizing long sequences", "journal": "", "year": "2018-04-30", "authors": "J Peter; Mohammad Liu; Etienne Saleh; Ben Pot; Ryan Goodrich; Lukasz Sepassi; Noam Kaiser;  Shazeer"}, {"ref_id": "b33", "title": "Fast training of convolutional networks through ffts: International conference on learning representations (iclr2014), cbls", "journal": "", "year": "2014-04", "authors": "Michael Mathieu; Mikael Henaff; Yann Lecun"}, {"ref_id": "b34", "title": "Real-time discrimination of ventricular tachyarrhythmia with fourier-transform neural network", "journal": "IEEE transactions on Biomedical Engineering", "year": "1999", "authors": "Hiroshi Kei-Ichiro Minami; Takeshi Nakajima;  Toyoshima"}, {"ref_id": "b35", "title": "Fast fourier transform for feature extraction and neural network for classification of electrocardiogram signals", "journal": "IEEE", "year": "2015", "authors": "Martina Mironovova; Jir\u00ed B\u00edla"}, {"ref_id": "b36", "title": "ACDC: A structured efficient linear layer", "journal": "", "year": "2016-05-02", "authors": "Marcin Moczulski; Misha Denil; Jeremy Appleyard; Nando De Freitas"}, {"ref_id": "b37", "title": "Do transformer modifications transfer across implementations and applications?", "journal": "", "year": "2021", "authors": "Sharan Narang;  Hyung Won; Yi Chung; Liam Tay; Thibault Fedus; Michael Fevry; Karishma Matena; Noah Malkan; Noam Fiedel; Zhenzhong Shazeer; Yanqi Lan; Wei Zhou; Nan Li;  Ding"}, {"ref_id": "b38", "title": "Image transformer", "journal": "PMLR", "year": "2018", "authors": "Niki Parmar; Ashish Vaswani; Jakob Uszkoreit; Lukasz Kaiser; Noam Shazeer; Alexander Ku; Dustin Tran"}, {"ref_id": "b39", "title": "Random feature attention", "journal": "", "year": "2021-05-03", "authors": "Hao Peng; Nikolaos Pappas; Dani Yogatama; Roy Schwartz; Noah A Smith; Lingpeng Kong"}, {"ref_id": "b40", "title": "Fcnn: Fourier convolutional neural networks", "journal": "Springer", "year": "2017", "authors": "Harry Pratt; Bryan Williams; Frans Coenen; Yalin Zheng"}, {"ref_id": "b41", "title": "Blockwise selfattention for long document understanding", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Jiezhong Qiu; Hao Ma; Omer Levy; Sinong Wen-Tau Yih; Jie Wang;  Tang"}, {"ref_id": "b42", "title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "Journal of Machine Learning Research", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b43", "title": "Fixed encoder self-attention patterns in transformer-based machine translation", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Alessandro Raganato; Yves Scherrer; J\u00f6rg Tiedemann"}, {"ref_id": "b44", "title": "Hopfield networks is all you need", "journal": "", "year": "2021-05-03", "authors": "Hubert Ramsauer; Bernhard Sch\u00e4fl; Johannes Lehner; Philipp Seidl; Michael Widrich; Lukas Gruber; Markus Holzleitner; Thomas Adler; David P Kreil; Michael K Kopp; G\u00fcnter Klambauer; Johannes Brandstetter; Sepp Hochreiter"}, {"ref_id": "b45", "title": "Efficient content-based sparse attention with routing transformers", "journal": "", "year": "2021", "authors": "Aurko Roy; Mohammad Saffar; Ashish Vaswani; David Grangier"}, {"ref_id": "b46", "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter", "journal": "", "year": "2019", "authors": "Victor Sanh; Lysandre Debut; Julien Chaumond; Thomas Wolf"}, {"ref_id": "b47", "title": "Fast transformer decoding: One write-head is all you need", "journal": "", "year": "2019", "authors": "Noam Shazeer"}, {"ref_id": "b48", "title": "Pretrained summarization distillation", "journal": "", "year": "2020", "authors": "Sam Shleifer; Alexander M Rush"}, {"ref_id": "b49", "title": "Structured transforms for small-footprint deep learning", "journal": "", "year": "2015", "authors": "Vikas Sindhwani; N Tara; Sanjiv Sainath;  Kumar"}, {"ref_id": "b50", "title": "Language through a prism: A spectral approach for multiscale language representations", "journal": "Curran Associates, Inc", "year": "2020", "authors": "Alex Tamkin; Dan Jurafsky; Noah Goodman"}, {"ref_id": "b51", "title": "Synthesizer: Rethinking self-attention in transformer models", "journal": "", "year": "2020", "authors": "Yi Tay; Dara Bahri; Donald Metzler; Da-Cheng Juan; Zhe Zhao; Che Zheng"}, {"ref_id": "b52", "title": "Sparse sinkhorn attention", "journal": "PMLR", "year": "2020", "authors": "Yi Tay; Dara Bahri; Liu Yang; Donald Metzler; Da-Cheng Juan"}, {"ref_id": "b53", "title": "", "journal": "", "year": "2021", "authors": "Yi Tay; Mostafa Dehghani; Samira Abnar; Yikang Shen; Dara Bahri; Philip Pham; Jinfeng Rao; Liu Yang; Sebastian Ruder; Donald Metzler"}, {"ref_id": "b54", "title": "Long range arena : A benchmark for efficient transformers", "journal": "", "year": "2021", "authors": ""}, {"ref_id": "b55", "title": "Efficient transformers: A survey", "journal": "", "year": "2020", "authors": "Yi Tay; Mostafa Dehghani; Dara Bahri; Donald Metzler"}, {"ref_id": "b56", "title": "Zhen Qin, and Donald Metzler. 2021b. Are pretrained convolutions better than pretrained transformers?", "journal": "", "year": "", "authors": "Yi Tay; Mostafa Dehghani; Jai Prakash Gupta; Vamsi Aribandi; Dara Bahri"}, {"ref_id": "b57", "title": "BERT rediscovers the classical NLP pipeline", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Ian Tenney; Dipanjan Das; Ellie Pavlick"}, {"ref_id": "b58", "title": "Mlp-mixer: An all-mlp architecture for vision", "journal": "", "year": "2021", "authors": "Neil Ilya O Tolstikhin; Alexander Houlsby; Lucas Kolesnikov; Xiaohua Beyer; Thomas Zhai; Jessica Unterthiner; Andreas Yung; Daniel Steiner; Jakob Keysers;  Uszkoreit"}, {"ref_id": "b59", "title": "Well-read students learn better: On the importance of pre-training compact models", "journal": "", "year": "2019", "authors": "Iulia Turc; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b60", "title": "Attention is all you need. Advances in neural information processing systems", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b61", "title": "Analyzing the structure of attention in a transformer language model", "journal": "", "year": "2019", "authors": "Jesse Vig; Yonatan Belinkov"}, {"ref_id": "b62", "title": "Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Elena Voita; David Talbot; Fedor Moiseev; Rico Sennrich; Ivan Titov"}, {"ref_id": "b63", "title": "Fast transformers with clustered attention", "journal": "", "year": "2020", "authors": "Apoorv Vyas; Angelos Katharopoulos; Fran\u00e7ois Fleuret"}, {"ref_id": "b64", "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Alex Wang; Amanpreet Singh; Julian Michael; Felix Hill; Omer Levy; Samuel Bowman"}, {"ref_id": "b65", "title": "Linformer: Selfattention with linear complexity", "journal": "", "year": "2020", "authors": "Sinong Wang; Belinda Li; Madian Khabsa; Han Fang; Hao Ma"}, {"ref_id": "b66", "title": "Hard-coded Gaussian attention for neural machine translation", "journal": "", "year": "2020", "authors": "Weiqiu You; Simeng Sun; Mohit Iyyer"}, {"ref_id": "b67", "title": "Big bird: Transformers for longer sequences", "journal": "Advances in Neural Information Processing Systems", "year": "2020", "authors": "Manzil Zaheer; Guru Guruganesh; Joshua Kumar Avinava Dubey; Chris Ainslie; Santiago Alberti; Philip Ontanon; Anirudh Pham; Qifan Ravula; Li Wang;  Yang"}, {"ref_id": "b68", "title": "Learning long term dependencies via fourier recurrent units", "journal": "PMLR", "year": "2018", "authors": "Jiong Zhang; Yibo Lin; Zhao Song; Inderjit Dhillon"}, {"ref_id": "b69", "title": "Forenet: fourier recurrent networks for time series prediction", "journal": "", "year": "2000", "authors": "Y Zhang; Lai-Wan Chan"}, {"ref_id": "b70", "title": "Fault diagnosis and prognosis using wavelet packet decomposition, fourier transform and artificial neural network", "journal": "Journal of Intelligent Manufacturing", "year": "2013", "authors": "Zhenyou Zhang; Yi Wang; Kesheng Wang"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: FNet architecture with N encoder blocks.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Speed-accuracy trade-offs for GPU pre-training. The dashed line shows the Pareto efficiency frontier, indicating the best trade-offs. For smaller models (faster training speeds; left-hand side of figure), the FNet (yellow squares) and Linear (red triangles) models define the frontier, while for larger models (slower training speeds; righthand side of figure), BERT (blue circles) and FNet-Hybrid (green stars) define the frontier.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Speed-accuracy trade-offs for TPU pre-training. The dashed line shows the Pareto efficiency frontier.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Number of mixing layer operations (forward pass) and learnable parameters, excluding any task specific output projection layers. n is the sequence length and d h is the model hidden dimension. The mixing layer operations are given on a per layer basis.", "figure_data": "Mixing layer ops Model paramsModel(per layer)Base LargeBERT2n 2"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "GLUE Validation results on TPUs, after finetuning on respective tasks. We report the mean of accuracy and F1 scores for QQP and MRPC, Spearman correlations for STS-B and accuracy scores for all other tasks. The MNLI metrics are reported by the match/mismatch splits. Average scores exclude any failure cases. After controlling for batch size and training steps, the GPU metrics (not shown) are similar.", "figure_data": "ModelMNLI QQP QNLI SST-2 CoLA STS-B MRPC RTE Avg.BERT-Base Linear-Base84/81 74/7587 8491 8093 9473 6789 6783 8369 6983.3 77.0FNet-Base Random-Base72/73 51/5083 7080 6195 7669 6779 476 7363 5776.7 56.6FF-only-Base34/3531524867FAIL735449.3FNet-Hybrid-Base BERT-Large Linear-Large78/79 88/88 35/3685 88 8488 92 8094 95 7976 71 6786 88 2479 86 7360 66 6080.6 84.7 59.8FNet-Large FNet-Hybrid-Large 79/80 78/7685 8785 8994 9278 8184 8888 8669 7081.9 83.6"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": "Pre-trainingInferenceGFLOPSModelGPUTPUGPUTPU/exampleBERT-Base305213823298Linear-Base199 (1.5x) 149 (1.4x) 52 (1.6x)20 (1.6x)71 (73%)FNet-Base169 (1.8x) 128 (1.7x) 46 (1.8x)23 (1.4x)62 (63%)Random-Base182 (1.7x) 130 (1.6x) 52 (1.6x)22 (1.4x)71 (73%)FF-only-Base FNet-Hybrid-Base 198 (1.5x) 149 (1.4x) 51 (1.6x) 162 (1.9x) 118 (1.8x) 43 (1.9x)16 (2.0x) 24 (1.3x)59 (60%) 68 (69%)BERT-LargeOOM503263111337Linear-Large592397 (1.3x) 170 (1.5x) 108 (1.0x) 247 (73%)FNet-Large FNet-Hybrid-Large511 541275 (1.8x) 149 (1.8x) 82 (1.4x) 217 (64%) 294 (1.7x) 157 (1.7x) 84 (1.3x) 227 (67%)"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Accuracy, inference speed and memory usage results on the Long-Range Arena (LRA) benchmark. GPU training for sequence lengths up to 8192. Only the fastest efficient Transformer, namely Performer, from Tay et al. (2021a) is shown. Left: training speeds (in steps per second; larger is better), with speed-up multipliers relative to the Transformer given in parentheses. Right: peak memory usage (in GB; smaller is better).", "figure_data": "ModelListOps Text Retrieval Image Pathfinder Path-X Avg.Transformer (ours) Linear (ours) FNet (ours)36.06 33.75 35.3361.54 53.35 65.1159.67 58.95 59.6141.51 41.04 38.6780.38 83.69 77.80OOM 55.83 FAIL 54.16 FAIL 55.30Transformer (*)36.3764.2757.4642.4471.40FAIL 54.39Local Attention (*)15.8252.9853.3941.4666.63FAIL 46.06Sparse Trans. (*) Longformer (*)17.07 35.6363.58 62.8559.59 56.8944.24 42.2271.71 69.71FAIL 51.24 FAIL 53.46Linformer (*)35.7053.9452.2738.5676.34FAIL 51.36Reformer (*) Sinkhorn Trans. (*)37.27 33.6756.10 61.2053.40 53.8338.07 41.2368.50 67.45FAIL 50.67 FAIL 51.39Synthesizer (*)36.9961.6864.6741.6169.45FAIL 52.88BigBird (*) Linear Trans. (*) Performer (*)36.05 16.13 18.0164.02 65.90 65.4059.29 53.09 53.8240.83 42.34 42.7774.87 75.30 77.05FAIL 55.01 FAIL 50.55 FAIL 51.41Training Speed (steps/s)Peak Memory Usage (GB)Seq. length512102420484096 8192 512 1024 2048 4096 8192Transformer21104OOM OOM 1.6 4.0 12.2 OOM OOMLinear34 (1.6x) 19 (1.8x) 9 (2.0x)4OOM 0.9 1.6 2.86.9 OOMFNet (FFT) 43 (2.0x) 24 (2.3x) 14 (3.2x) Performer 28 (1.3x) 15 (1.5x) 9 (1.9x)7 44 20.8 1.3 2.2 1.1 1.9 3.13.9 5.57.4 10.4(b) Seq. length5121024204840968192 16384Transformer122876244OOM OOMLinear9 (1.4x) 14 (2.0x) 30 (2.6x) 72 (3.4x)208OOMFNet (FFT) Performer8 (1.5x) 12 (2.3x) 23 (3.4x) 43 (5.7x) 11 (1.2x) 17 (1.6x) 32 (2.4x) 60 (4.0x)83 116164 238"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Loss and accuracy pre-training metrics on TPUs. The GPU metrics are very similar. \"B\" denotes Base, \"L\" is Large and \"H\" is Hybrid.", "figure_data": "LossAccuracyModelTotal MLM NSP MLM NSPBERT-B Linear-B1.76 1.48 0.28 0.68 0.86 2.12 1.78 0.35 0.62 0.83FNet-B2.45 2.06 0.40 0.58 0.80Random-B 5.02 4.48 0.55 0.26 0.70FF-only-B 7.54 6.85 0.69 0.13 0.50FNet-H-B 2.13 1.79 0.34 0.63 0.84BERT-L Linear-L1.49 1.23 0.25 0.72 0.88 1.91 1.60 0.31 0.65 0.85FNet-L2.11 1.75 0.36 0.63 0.82FNet-H-L 1.89 1.58 0.31 0.67 0.85"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Pre-training model sizes (ignoring output projection layers). As in", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "TPU training speeds (in steps per second; larger is better), inference speeds (in milliseconds per batch; smaller is better) and peak memory usage during training (in GB; smaller is better) on the Long-Range Arena Text classification task. Speed up multipliers relative to the Transformer are given in parentheses.", "figure_data": "Seq. length512102420484096819216386Training Speed (steps/s)Transformer8.05.61.7OOMOOMOOMLinear FNet (mat) FNet (FFT)9.4 (1.2x) 9.5 (1.2x) 8.6 (1.1x)9.1 (1.6x) 9.1 (1.6x) 6.0 (1.1x)7.6 (4.5x) 6.1 (3.6x) 3.2 (1.9x)3.9 3.0 1.61.4 0.8 0.8OOM 0.2 0.3Performer9.2 (1.2x)8.4 (1.5x)6.9 (4.1x)4.22.21.1Inference Speed (ms/batch)Transformer7.013.239.4129.9490.2OOMLinear FNet (mat)5.6 (1.2x) 6.0 (1.2x)6.5 (2.0x) 7.7 (1.7x) 15.4 (2.6x) 40.7 (3.2x) 137.0 (3.6x) 454.5 9.6 (4.1x) 20.4 (6.4x) 54.6 (9.0x) OOMFNet (FFT) 10.8 (0.7x) 16.8 (0.8x) 29.9 (1.3x) 58.8 (2.2x) 113.6 (4.3x) 263.2Performer6.1 (1.2x)7.2 (1.8x) 10.1 (3.9x) 17.5 (7.4x) 31.8 (15.4x) 61.0Peak Memory Usage (GB)Transformer1.12.15.89.1OOMOOMLinear0.91.11.94.914.8OOMFNet (mat) FNet (FFT) Performer0.8 0.8 1.00.9 0.9 1.31.3 1.3 1.82.2 2.0 3.04.8 3.5 5.111.9 6.3 9.6"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Training (forward and backward passes; left) and inference (forward pass; left) speeds for only the mixing sublayers -all other model sublayers are removed. Both speeds are measured in milliseconds per batch (smaller is better), with batch sizes of 64 (GPU) and 256 (TPU). All batch examples have the sequence length fixed at 512. FNet uses the FFT for GPUs and matrix multiplications for TPUs. Speed up multipliers relative to self-attention are given in parentheses.", "figure_data": "Training speed (ms/batch) Inference speed (ms/batch)GPUTPUGPUTPUSelf-attention (Base)136764316Linear (Base) FNet (Base)36 (3.7x) 11 (12.2x)12 (6.1x) 8 (9.9x)15 (2.8x) 11 (4.0x)4 (3.9x) 8 (2.1x)Self-attention (Large)40421212843Linear (Large) FNet (Large)103 (3.9x) 18 (22.2x)35 (6.1x) 22 (9.7x)36 (3.6x) 18 (7.3x)10 (4.5x) 22 (2.0x)mimics convolutions. However, these setups de-graded accuracy and lead to a more unstable modelduring training. Adding extra feed-forward sub-layers to this layering, or swapping out the feed-forward sublayers for simpler dense sublayers, didnot help either.A.4 Mixing layer speeds"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "summarizes the inference and training speeds for the different mixing layers. For each of the Base and Large configurations, we have removed all other sublayers and transformations and then calculated the speed per batch of input examples. The FNet training speeds are particularly fast because no parameters are updated. The Linear model has faster inference than FNet on TPUs because it is performing real matrix multiplications, whereas FNet performs complex matrix multiplications; see Equation (2).", "figure_data": ""}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "GPU pre-training accuracy and speed ablations for FNet-Hybrid models in the Base configuration. Batch size is 64. Metrics are recorded after 100k steps, which we have generally found to be a good indicator of final relative performance. See text for a description of the layouts.", "figure_data": "AttentionAccuracySpeedLayers Layout MLM NSP (ms/batch)2 2BOTTOM 0.497 0.733 MIDDLE 0.499 0.686193 1962MIXED 0.509 0.7271942 0 2TOP TOP TOP0.526 0.738 0.486 0.679 0.526 0.738193 173 1934TOP0.539 0.7402146TOP0.546 0.746235"}], "formulas": [{"formula_id": "formula_0", "formula_text": "X k = N \u22121 n=0 x n e \u2212 2\u03c0i N nk , 0 \u2264 k \u2264 N \u2212 1. (1)", "formula_coordinates": [3.0, 316.88, 465.52, 207.54, 34.79]}, {"formula_id": "formula_1", "formula_text": "W nk = e \u2212 2\u03c0i N nk / \u221a N ,(2)", "formula_coordinates": [3.0, 358.94, 710.15, 165.48, 22.0]}, {"formula_id": "formula_2", "formula_text": "y = F seq (F h (x)) .(3)", "formula_coordinates": [4.0, 129.42, 573.52, 159.71, 20.55]}, {"formula_id": "formula_3", "formula_text": "d h + 4nd 2 h 112M 339M Linear n 2 d h + nd 2 h 94M 269M FNet (mat) n 2 d h + nd 2 h 83M 238M FNet (FFT) nd h log(n)+ 83M 238M nd h log(d h ) Random n 2 d h + nd 2 h 83M 238M FF-only 0 83M 238M", "formula_coordinates": [5.0, 80.08, 170.02, 197.52, 94.3]}], "doi": "10.1109/ICCV.2015.327"}