{"title": "Multi-Label Prediction via Compressed Sensing", "authors": "Daniel Hsu; Sham M Kakade; John Langford; Tong Zhang", "pub_date": "2009-06-02", "abstract": "We consider multi-label prediction problems with large output spaces under the assumption of output sparsity -that the target (label) vectors have small support. We develop a general theory for a variant of the popular error correcting output code scheme, using ideas from compressed sensing for exploiting this sparsity. The method can be regarded as a simple reduction from multi-label regression problems to binary regression problems. We show that the number of subproblems need only be logarithmic in the total number of possible labels, making this approach radically more efficient than others. We also state and prove robustness guarantees for this method in the form of regret transform bounds (in general), and also provide a more detailed analysis for the linear prediction setting.", "sections": [{"heading": "Introduction", "text": "Suppose we have a large database of images, and we want to learn to predict who or what is in any given one. A standard approach to this task is to collect a sample of these images x along with corresponding labels y = (y 1 , . . . , y d ) \u2208 {0, 1} d , where y i = 1 if and only if person or object i is depicted in image x, and then feed the labeled sample to a multi-label learning algorithm. Here, d is the total number of entities depicted in the entire database. When d is very large (e.g. 10 3 , 10 4 ), the simple one-against-all approach of learning a single predictor for each entity can become prohibitively expensive, both at training and testing time.\nOur motivation for the present work comes from the observation that although the output (label) space may be very high dimensional, the actual labels are often sparse. That is, in each image, only a small number of entities may be present and there may only be a small amount of ambiguity in who or what they are. In this work, we consider how this sparsity in the output space, or output sparsity, eases the burden of large-scale multi-label learning.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Exploiting output sparsity.", "text": "A subtle but critical point that distinguishes output sparsity from more common notions of sparsity (say, in feature or weight vectors) is that we are interested in the sparsity of E[y|x] rather than y. In general, E[y|x] may be sparse while the actual outcome y may not (e.g. if there is much unbiased noise); and, vice versa, y may be sparse with probability one but E[y|x] may have large support (e.g. if there is little distinction between several labels).\nConventional linear algebra suggests that we must predict d parameters in order to find the value of the d-dimensional vector E[y|x] for each x. A crucial observation -central to the area of compressed sensing [1] -is that methods exist to recover E[y|x] from just O(k log d) measurements when E[y|x] is k-sparse. This is the basis of our approach.\nOur contributions. We show how to apply algorithms for compressed sensing to the output coding approach [2]. At a high level, the output coding approach creates a collection of subproblems of the form \"Is the label in this subset or its complement?\", solves these problems, and then uses their solution to predict the final label.\nThe role of compressed sensing in our application is distinct from its more conventional uses in data compression. Although we do employ a sensing matrix to compress training data, we ultimately are not interested in recovering data explicitly compressed this way. Rather, we learn to predict compressed label vectors, and then use sparse reconstruction algorithms to recover uncompressed labels from these predictions. Thus we are interested in reconstruction accuracy of predictions, averaged over the data distribution.\nThe main contributions of this work are:\n1. A formal application of compressed sensing to prediction problems with output sparsity.\n2. An efficient output coding method, in which the number of required predictions is only logarithmic in the number of labels d, making it applicable to very large-scale problems.\n3. Robustness guarantees, in the form of regret transform bounds (in general) and a further detailed analysis for the linear prediction setting.\nPrior work. The ubiquity of multi-label prediction problems in domains ranging from multiple object recognition in computer vision to automatic keyword tagging for content databases has spurred the development of numerous general methods for the task. Perhaps the most straightforward approach is the well-known one-against-all reduction [3], but this can be too expensive when the number of possible labels is large (especially if applied to the power set of the label space [4]). When structure can be imposed on the label space (e.g. class hierarchy), efficient learning and prediction methods are often possible [5,6,7,8,9]. Here, we focus on a different type of structure, namely output sparsity, which is not addressed in previous work. Moreover, our method is general enough to take advantage of structured notions of sparsity (e.g. group sparsity) when available [10]. Recently, heuristics have been proposed for discovering structure in large output spaces that empirically offer some degree of efficiency [11].\nAs previously mentioned, our work is most closely related to the class of output coding method for multi-class prediction, which was first introduced and shown to be useful experimentally in [2]. Relative to this work, we expand the scope of the approach to multi-label prediction and provide bounds on regret and error which guide the design of codes. The loss based decoding approach [12] suggests decoding so as to minimize loss. However, it does not provide significant guidance in the choice of encoding method, or the feedback between encoding and decoding which we analyze here.\nThe output coding approach is inconsistent when classifiers are used and the underlying problems being encoded are noisy. This is proved and analyzed in [13], where it is also shown that using a Hadamard code creates a robust consistent predictor when reduced to binary regression. Compared to this method, our approach achieves the same robustness guarantees up to a constant factor, but requires training and evaluating exponentially (in d) fewer predictors.\nOur algorithms rely on several methods from compressed sensing, which we detail where used.", "publication_ref": ["b0", "b1", "b3", "b4", "b5", "b6", "b7", "b8", "b9", "b10", "b1", "b11", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "Let X be an arbitrary input space and Y \u2282 R d be a d-dimensional output (label) space. We assume the data source is defined by a fixed but unknown distribution over X \u00d7 Y. Our goal is to learn a predictor F :\nX \u2192 Y with low expected \u2113 2 2 -error E x F (x) \u2212 E[y|x] 2 2\n(the sum of mean-squared-errors over all labels) using a set of n training data {(x i , y i )} n i=1 . We focus on the regime in which the output space is very high-dimensional (d very large), but for any given x \u2208 X , the expected value E[y|x] of the corresponding label y \u2208 Y has only a few non-zero entries. A vector is k-sparse if it has at most k non-zero entries.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Learning and Prediction", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Learning to Predict Compressed Labels", "text": "Let A : R d \u2192 R m be a linear compression function, where m \u2264 d (but hopefully m \u226a d). We use A to compress (i.e. reduce the dimension of) the labels Y, and learn a predictor H : X \u2192 A(Y) of these compressed labels. Since A is linear, we simply represent A \u2208 R m\u00d7d as a matrix.\nSpecifically, given a sample {(x i , y i )} n i=1 , we form a compressed sample {(x i , Ay i )} n i=1 and then learn a predictor H of E[Ay|x] with the objective of minimizing the\n\u2113 2 2 -error E x H(x) \u2212 E[Ay|x] 2 2 .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Predicting Sparse Labels", "text": "To obtain a predictor F of E[y|x], we compose the predictor H of E[Ay|x] (learned using the compressed sample) with a reconstruction algorithm R : R m \u2192 R d . The algorithm R maps predictions of compressed labels h \u2208 R m to predictions of labels y \u2208 Y in the original output space. These algorithms typically aim to find a sparse vector y such that Ay closely approximates h.\nRecent developments in the area of compressed sensing have produced a spate of reconstruction algorithms with strong performance guarantees when the compression function A satisfies certain properties. We abstract out the relevant aspects of these guarantees in the following definition.\nDefinition. An algorithm R is a valid reconstruction algorithm for a family of compression functions (A k \u2282 m\u22651 R m\u00d7d : k \u2208 N) and sparsity error sperr : N \u00d7 R d \u2192 R, if there exists a function f : N \u2192 N and constants C 1 , C 2 \u2208 R such that: on input k \u2208 N, A \u2208 A k with m rows, and h \u2208 R m , the algorithm R(k, A, h) returns an f (k)-sparse vector y satisfying )), as well as the \"measurement noise\" (the prediction error H(x) \u2212 E[Ay|x] 2 ). This is a subtle condition and precludes certain reconstruction algorithm (e.g. Basis Pursuit [14]) that require the user to supply a bound on the measurement noise. However, the condition is needed in our application, as such bounds on the prediction error (for each x) are not generally known beforehand.\ny \u2212 y 2 2 \u2264 C 1 \u2022 h \u2212 Ay 2 2 + C 2 \u2022 sperr(k, y) for all y \u2208 R d .\nWe make a few additional remarks on the definition.\n1. The minimum number of rows of matrices A \u2208 A k may in general depend on k (as well as the ambient dimension d). In the next section, we show how to construct such A with close to the optimal number of rows.\n2. The sparsity error sperr(k, y) should measure how poorly y \u2208 R d is approximated by a k-sparse vector.", "publication_ref": ["b13"], "figure_ref": [], "table_ref": []}, {"heading": "3.", "text": "A reasonable output sparsity f (k) for sparsity level k should not be much more than k, e.g.\nf (k) = O(k).\nConcrete examples of valid reconstruction algorithms (along with the associated A k , sperr, etc.) are given in the next section.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 1 Training algorithm parameters sparsity level k, compression function", "text": "A \u2208 A k with m rows, regression learning algo- rithm L input training data S \u2282 X \u00d7 R d for i = 1, . . . , m do h i \u2190 L({(x, (Ay) i ) : (x, y) \u2208 S}) end for output regressors H = [h 1 , . . . , h m ]", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 2 Prediction algorithm parameters sparsity level k, compression function", "text": "A \u2208 A k with m rows, valid reconstruction algo- \nrithm R for A k input regressors H = [h 1 , . . . , h m ], test point x \u2208 X output y = R(k, A, [h 1 (x), . . . , h m (x)])", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithms", "text": "Our prescribed recipe is summarized in Algorithms 1 and 2. We give some examples of compression functions and reconstruction algorithms in the following subsections.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Compression Functions", "text": "Several valid reconstruction algorithms are known for compression matrices that satisfy a restricted isometry property.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definition. A matrix", "text": "A \u2208 R m\u00d7d satisfies the (k, \u03b4)-restricted isometry property ((k, \u03b4)-RIP), \u03b4 \u2208 (0, 1), if (1 \u2212 \u03b4) x 2 2 \u2264 Ax 2 2 \u2264 (1 + \u03b4) x 2 2 for all k-sparse x \u2208 R d .\nWhile some explicit constructions of (k, \u03b4)-RIP matrices are known (e.g. [15]), the best guarantees are obtained when the matrix is chosen randomly from an appropriate distribution, such as one of the following [16,17]. The hidden constants in the big-O notation depend inversely on \u03b4 and the probability of success.\nA striking feature of these constructions is the very mild dependence of m on the ambient dimension d. This translates to a significant savings in the number of learning problems one has to solve after employing our reduction.\nSome reconstruction algorithms require a stronger guarantee of bounded coherence \u00b5(A \n) \u2264 O(1/k), where \u00b5(A) defined as \u00b5(A) = max 1\u2264i<j\u2264d |(A \u22a4 A) i,j |/ |(A \u22a4 A) i,i ||(A \u22a4 A) j,\nH = [h 1 , . . . , h m ], test point x \u2208 X h \u2190 [h 1 (x), . . . , h m (x)] \u22a4 (predict compressed label vector) y \u2190 0, J \u2190 \u2205, r \u2190 h for i = 1, .\n. . , 2k do j * \u2190 arg max j |r \u22a4 a j |/ a j 2 (column of A most correlated with residual r) J \u2190 J \u222a {j * } (add j * to set of selected columns) y J \u2190 (A J ) \u2020 h, y J c \u2190 0 (least-squares restricted to columns in J) r \u2190 h \u2212 A y (update residual) end for output y ", "publication_ref": ["b14", "b15", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Reconstruction Algorithms", "text": "In this section, we give some examples of valid reconstruction algorithms. Each of these algorithm is valid with respect to the sparsity error given by\nsperr(k, y) = y \u2212 y (1:k) 2 2 + 1 k y \u2212 y (1:k) 2 1\nwhere y (1:k) is the best k-sparse approximation of y (i.e. the vector with just the k largest (in magnitude) coefficients of y).\nThe following theorem relates reconstruction quality to approximate sparse regression, giving a sufficient condition for any algorithm to be valid for RIP matrices.\nTheorem 1. Let A k = {(k + f (k), \u03b4)\n-RIP matrices} for some function f : N \u2192 N, and let A \u2208 A k have m rows. If for any h \u2208 R m , a reconstruction algorithm R returns an f (k)-sparse solution y = R(k, A, h) satisfying\nA y \u2212 h 2 2 \u2264 inf y\u2208R d C Ay (1:k) \u2212 h 2 2 ,\nthen it is a valid reconstruction algorithm for A k and sperr given above, with output sparsity f and regret factors\nC 1 = 2(1 + \u221a C) 2 /(1 \u2212 \u03b4) and C 2 = 4(1 + (1 + \u221a C)/(1 \u2212 \u03b4)) 2 .\nProofs are deferred to Section 6.\nIterative and greedy algorithms. Orthogonal Matching Pursuit (OMP) [18], FoBa [19], and CoSaMP [20] are examples of iterative or greedy reconstruction algorithms. OMP is a greedy forward selection method that repeatedly selects a new column of A to use in fitting h (see Algorithm 3). FoBa is similar, except it also incorporates backward steps to un-select columns that are later discovered to be unnecessary. CoSaMP is also similar to OMP, but instead selects larger sets of columns in each iteration.\nFoBa and CoSaMP are valid reconstruction algorithms for RIP matrices ((8k, 0.1)-RIP and (4k, 0.1)-RIP, respectively) and have linear output sparsity (8k and 2k). These guarantees are apparent from the cited references. For OMP, we give the following guarantee.\nTheorem 2. If \u00b5(A) \u2264 0.1/k, then after f (k) = 2k steps of OMP, the algorithm returns y satisfying\nA y \u2212 h 2 2 \u2264 23 Ay (1:k) \u2212 h 2 2 \u2200y \u2208 R d .\nThis theorem, combined with Theorem 1, implies that OMP is valid for matrices A with \u00b5(A) \u2264 0.1/k and has output sparsity f (k) = 2k. \u2113 1 algorithms. Basis Pursuit (BP) [14] and its variants are based on finding the minimum \u2113 1 -norm solution to a linear system. While the basic form of BP is ill-suited for our application (it requires the user to supply the amount of measurement error Ay \u2212 h 2 ), its more advanced path-following or multi-stage variants may be valid [21].", "publication_ref": ["b17", "b18", "b19", "b13", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Analysis", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "General Robustness Guarantees", "text": "We now state our main regret transform bound, which follows immediately from the definition of a valid reconstruction algorithm and linearity of expectation. Theorem 3 (Regret Transform). Let R be a valid reconstruction algorithm for {A k : k \u2208 N} and sperr :\nN\u00d7R d \u2192 R.\nThen there exists some constants C 1 and C 2 such that the following holds. Pick any k \u2208 N, A \u2208 A k with m rows, and H : X \u2192 R m . Let F : X \u2192 R d be the composition of R(k, A, \u2022) and H, i.e. F (x) = R(k, A, H(x)). Then\nE x F (x) \u2212 E[y|x] 2 2 \u2264 C 1 \u2022 E x H(x) \u2212 E[Ay|x] 2 2 + C 2 \u2022 sperr(k, E[y|x]).\nThe simplicity of this theorem is a consequence of the careful composition of the learned predictors with the reconstruction algorithm meeting the formal specifications described above.\nIn order compare this regret bound with the bounds afforded by Sensitive Error Correcting Output Codes (SECOC) [13], we need to relate E x H(x) \u2212 E[Ay|x] 2 2 to the average scaled mean-squared-error over all induced regression problems; the error is scaled by the maximum difference L i = max y\u2208Y (Ay) i \u2212 min y (Ay) i between induced labels:\nr = 1 m m i=1 E x H(x) i \u2212 E[(Ay) i |x] L i 2 .\nIn k-sparse multi-label problems, we have Y = {y \u2208 {0, 1} d : y 0 \u2264 k}. In these terms, SECOC can be tuned to yield  \nE x F (x) \u2212 E[y|x]\nC 1 \u2022 E x H(x) \u2212 E[Ay|x] 2 2 \u2264 4C 1 \u2022 k 2 \u2022r,", "publication_ref": ["b12"], "figure_ref": [], "table_ref": []}, {"heading": "Linear Prediction", "text": "A danger of using generic reductions is that one might create a problem instance that is even harder to solve than the original problem. This is an oft cited issue with using output codes for multi-class problems. In the case of linear prediction, however, the danger is mitigated, as we now show. Suppose, for instance, there is a perfect linear predictor of E[y|x], i.e. E[y|x] = B \u22a4 x for some B \u2208 R p\u00d7d (here X = R p ). Then it is easy to see that H = BA \u22a4 is a perfect linear predictor of E[Ay|x]:\nH \u22a4 x = AB \u22a4 x = AE[y|x] = E[Ay|x].\nThe following theorem generalizes this observation to imperfect linear predictors for certain well-behaved A.\nTheorem 4. Suppose X \u2282 R p . Let B \u2208 R p\u00d7d be a linear function with\nE x B \u22a4 x \u2212 E[y|x] 2 2 = \u01eb.\nLet A \u2208 R m\u00d7d have entries drawn i.i.d. from N (0, 1/m), and let H = BA \u22a4 . Then with high probability (over the choice of A),\nE x H \u22a4 x \u2212 AE[y|x] 2 2 \u2264 1 + O(1/ \u221a m) \u01eb.\nRemark 5. Similar guarantees can be proven for the Bernoulli-based matrices. Note that d does not appear in the bound, which is in contrast to the expected spectral norm of A:\nroughly 1 + O( d/m).\nTheorem 4 implies that the errors of any linear predictor are not magnified much by the compression function. So a good linear predictor for the original problem implies an almost-as-good linear predictor for the induced problem.\nUsing this theorem together with known results about linear prediction [22], it is straightforward to derive sample complexity bounds for achieving a given error relative to that of the best linear predictor in some class. The bound will depend polynomially in k but only logarithmically in d. This is cosmetically similar to learning bounds for feature-efficient algorithms (e.g. [23,22]) which are concerned with sparsity in the weight vector, rather than in the output.", "publication_ref": ["b21", "b22", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Proofs", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Theorem 1", "text": "Let \n\u2113 = k + f (k), y \u2208 R d ,\ny \u2212 y 2 2 \u2264 C 1 \u2022 Ay \u2212 h 2 2 + C 2 \u2022 ( \u2206 2 2 + k \u22121 \u2206 2 1 )\nwhere \u2206 = y \u2212 y (1:k) . Using the triangle inequality, the (\u2113, \u03b4)-RIP of A \u2208 A k , and the hypothesis that\nA y \u2212 h 2 2 \u2264 C Ay (1:k) \u2212 h 2 2 , we have y \u2212 y 2 \u2264 y \u2212 y (1:k) 2 + \u2206 2 \u2264 (1 \u2212 \u03b4) \u22121/2 A y \u2212 Ay (1:k) 2 + \u2206 2 \u2264 (1 \u2212 \u03b4) \u22121/2 A y \u2212 h 2 + h \u2212 Ay (1:k) 2 + \u2206 2 \u2264 (1 \u2212 \u03b4) \u22121/2 (1 + \u221a C) Ay (1:k) \u2212 h 2 + \u2206 2 \u2264 (1 \u2212 \u03b4) \u22121/2 (1 + \u221a C) ( Ay \u2212 h 2 + A\u2206 2 ) + \u2206 2 .(1)\nWe need to relate A\u2206 2 to \u2206 2 and \u2206 1 . Write \u2206 = i\u22650 y Ji , where J i = {k + i\u2113 + 1, . . . , k + (i + 1)\u2113} and y J \u2208 R d is the vector whose jth component is y j if j \u2208 J and is 0 otherwise. Note that each y Ji is \u2113-sparse, y Ji+1 1 \u2264 y Ji 1 , and y Ji+1 \u221e \u2264 \u2113 \u22121 y Ji 1 . By H\u00f6lder's inequality,\ny Ji+1 2 \u2264 ( y Ji+1 \u221e y Ji+1 1 ) 1/2 \u2264 (\u2113 \u22121 y Ji 2 1 ) 1/2 = \u2113 \u22121/2 y Ji 1 ,and\nso i\u22650 y Ji 2 \u2264 y J0 2 + i\u22650 y Ji+1 2 \u2264 y J0 2 + \u2113 \u22121/2 i\u22650 y Ji 1 \u2264 \u2206 2 + \u2113 \u22121/2 \u2206 1 .\nBy the triangle inequality and the (\u2113, \u03b4)-RIP of A, we have\nA\u2206 2 \u2264 i\u22650 Ay Ji 2 \u2264 i\u22650 (1 + \u03b4) 1/2 y Ji 2 \u2264 (1 + \u03b4) 1/2 ( \u2206 2 + \u2113 \u22121/2 \u2206 1 ).\nCombining this final inequality with (1) gives\ny \u2212 y 2 \u2264 C 0 \u2022 Ay \u2212 h 2 + (1 + C 0 (1 + \u03b4) 1/2 ) \u2022 ( \u2206 2 + \u2113 \u22121/2 \u2206 1 )\nwhere\nC 0 = (1 \u2212 \u03b4) \u22121/2 (1 + \u221a C\n). Now squaring both sides and simplifying using the fact (x + y) 2 \u2264 2x 2 + 2y 2 concludes the proof.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Theorem 2", "text": "We first begin with two simple lemmas. Lemma 6. Suppose OMP is run for k iterations starting with y (0) = 0, and produces intermediate solutions y (1) , y (2) , . . . , y (k) . Then there exists some\n0 \u2264 i < k such that if j i is the column selected in step i, then (a \u22a4 ji (h \u2212 Ay (i) )) 2 \u2264 h 2 2 /k.\nProof. Let r (i) = h \u2212 Ay (i) . Suppose column j i is added to J in step i. Let y (i+1) = y (i) + \u03b1 i e ji , where \u03b1 i = a \u22a4 ji r (i) and e ji is the j i th elementary vector. Then\nr (i) 2 2 \u2212 r (i+1) 2 2 \u2265 r (i) 2 2 \u2212 h \u2212 A y (i+1) 2 2 = r (i) 2 2 \u2212 h \u2212 A(y (i) + \u03b1 i e ji ) 2 2 = r (i) 2 2 \u2212 r (i) \u2212 \u03b1 i a ji 2 2 = 2\u03b1 i a \u22a4 ji r (i) \u2212 \u03b1 2 i a ji 2 2 = (a \u22a4 ji r (i) ) 2 .\nMoreover,\nk\u22121 i=0 r (i) 2 2 \u2212 r (i+1) 2 2 = r (0) 2 2 \u2212 r (k) 2 2 \u2264 h 2 2 , so there is some i \u2208 {0, 1, . . . , k \u2212 1} such that (a \u22a4 ji r (i) ) 2 \u2264 r (i) 2 2 \u2212 r (i+1) 2 2 \u2264 h 2 2 /k. Lemma 7. If y \u2208 R d is k-sparse and \u00b5(A) \u2264 \u03b4/(k \u2212 1), then Ay 2 2 \u2265 (1 \u2212 \u03b4) y 2 2 .\nThis result also appears in Appendix A1 of [24]. We reproduce the proof here.\nProof. Expanding Ay 2 2 , we have\nAy 2 2 = k i=1 a i 2 y 2 i + i =j y i y j (a \u22a4 i a j ) \u2265 y 2 2 \u2212 i =j y i y j (a \u22a4 i a j ) ,\nso we need to show this latter summation is at most \u03b4 y 2 2 . Indeed,\ni =j y i y j (a \u22a4 i a j ) \u2264 i =j |y i y j ||a \u22a4 i a j | (triangle inequality) \u2264 \u00b5(A) i =j |y i y j | (definition of coherence) = \u00b5(A) \uf8eb \uf8ed k i=1 k j=1 |y i ||y j | \u2212 k i=1 y 2 i \uf8f6 \uf8f8 = \u00b5(A) \uf8eb \uf8ed k i=1 |y i | 2 \u2212 y 2 2 \uf8f6 \uf8f8 \u2264 \u00b5(A)(k y 2 2 \u2212 y 2 2 ) (Cauchy-Schwarz) = \u00b5(A)(k \u2212 1) y 2 2 \u2264 \u03b4 y 2 2\n(assumption on \u00b5(A))\nwhich concludes the proof.\nWe are now ready to prove Theorem 2. Without loss of generality, we assume that the columns of A = [a 1 | . . . |a d ] are normalized (so a j 2 = 1) and that the support of y is (some subset of) {1, . . . , k} (so y is k-sparse).\nIn addition to the vector y returned by OMP and the vector y we want to compare to, we consider two other solution vectors:\n\u2022 y \u2032 : a (2k \u2212 1)-sparse solution obtained by running up to k \u2212 1 iterations of OMP starting from y. Lemma 6 implies that there exists such a vector y \u2032 with the following property: if j * is the column OMP would select when the current solution is y \u2032 , then\n(a \u22a4 j * (h \u2212 Ay \u2032 )) 2 \u2264 h \u2212 Ay 2 2 /k.(2)\nSince y \u2032 is obtained by starting with y, it can only have smaller squared-error than y. Without loss of generality, let the support of y \u2032 be (some subset of) {1, . . . , 2k}.\n\u2022 y \u2032 : the actual solution produced by OMP (starting from 0) just before OMP chooses a column j \u2208 supp(y \u2032 ).\nNote that if OMP never chooses a column j \u2208 supp(y \u2032 ) within 2k steps, then\nA y \u2212 h 2 2 \u2264 A y \u2032 \u2212 h 2 2 \u2264 Ay \u2212 h 2\n2 and the theorem is proven. Therefore we assume that this event does occurs and so y \u2032 is defined. Since y \u2032 precedes the final solution y returned by OMP, it can only have larger squared-error than y.\nWe will bound h \u2212 A y 2 as follows:\nh \u2212 A y 2 \u2264 h \u2212 A y \u2032 2 (since y \u2032 precedes y) \u2264 h \u2212 Ay \u2032 2 + A( y \u2032 \u2212 y \u2032 ) 2 (triangle inequality) \u2264 h \u2212 Ay 2 + A( y \u2032 \u2212 y \u2032 ) 2 .\n(since y precedes y \u2032 )\nWe thus need to bound A( y \u2032 \u2212 y \u2032 ) 2 in terms of h \u2212 Ay 2 .\nLet r = h \u2212 A y \u2032 and r = h \u2212 Ay \u2032 . Then\nA( y \u2032 \u2212 y \u2032 ) 2 2 = (A y \u2032 \u2212 Ay \u2032 ) \u22a4 A( y \u2032 \u2212 y \u2032 ) = (h \u2212 Ay \u2032 ) \u22a4 A( y \u2032 \u2212 y \u2032 ) \u2212 (h \u2212 A y \u2032 ) \u22a4 A( y \u2032 \u2212 y \u2032 ) \u2264 h \u2212 Ay \u2032 2 A( y \u2032 \u2212 y \u2032 ) 2 + |(h \u2212 A y \u2032 ) \u22a4 A( y \u2032 \u2212 y \u2032 )| (Cauchy-Schwarz) = r 2 A( y \u2032 \u2212 y \u2032 ) 2 + | r \u22a4 A( y \u2032 \u2212 y \u2032 )|. Using the fact x \u2264 b \u221a x + c \u21d2 x \u2264 (4/3)(b 2 + c)\n(which in turn follows from the quadratic formula and the fact 2xy \u2264 x 2 + y 2 ), the above inequality implies\n3 4 A( y \u2032 \u2212 y \u2032 ) 2 2 \u2264 r 2 2 + | r \u22a4 A( y \u2032 \u2212 y \u2032 )|.(3)\nWe now work on bounding the second term on the righthand side. Let j > 2k be the column chosen by OMP when the current solution is y \u2032 . Then we have\n|a \u22a4 j r| \u2265 |a \u22a4 \u2113 r| \u2200\u2113 \u2264 2k.(4)\nAlso, since y \u2032 \u2212 y \u2032 has support {1, . . . , 2k}, we have that\nA( y \u2032 \u2212 y \u2032 ) = A {1:2k} ( y \u2032 \u2212 y \u2032 )(5)\nwhere A {1:2k} is the same as A except with zeros in all but the first 2k columns. Then,\n| r \u22a4 A( y \u2032 \u2212 y \u2032 )| = | r \u22a4 A {1:2k} ( y \u2032 \u2212 y \u2032 )| (Equation (5)) \u2264 r \u22a4 A {1:2k} \u221e y \u2032 \u2212 y \u2032 1 (H\u00f6lder's inequality) \u2264 |a \u22a4 j r| y \u2032 \u2212 y \u2032 1 (Inequality (4)) \u2264 |a \u22a4 j r| + |a \u22a4 j A( y \u2032 \u2212 y \u2032 )| y \u2032 \u2212 y \u2032 1 (triangle inequality) \u2264 |a \u22a4 j r| + a \u22a4 j A {1:2k} \u221e y \u2032 \u2212 y \u2032 1 y \u2032 \u2212 y \u2032 1\n(Equation ( 5) and H\u00f6lder)\n\u2264 |a \u22a4 j r| y \u2032 \u2212 y \u2032 1 + \u00b5(A) y \u2032 \u2212 y \u2032 2 1 (definition of coherence) \u2264 5k 2 (a \u22a4 j r) 2 + 1 10k y \u2032 \u2212 y \u2032 2 1 + \u00b5(A) y \u2032 \u2212 y \u2032 2 1 (since xy \u2264 (x 2 + y 2 )/2) \u2264 5k 2 (a \u22a4 j r) 2 + 1 5k y \u2032 \u2212 y \u2032 2 1 (since \u00b5(A) \u2264 0.1/k) \u2264 5k 2 (a \u22a4 j r) 2 + 1 5k (2k y \u2032 \u2212 y \u2032 2 2 ) (Cauchy-Schwarz) \u2264 5k 2 (a \u22a4 j r) 2 + 1 2 A( y \u2032 \u2212 y \u2032 ) 2 2 . (Lemma 7\n)\nContinuing from Inequality (3), we have\nA( y \u2032 \u2212 y \u2032 ) 2 2 \u2264 4 r 2 2 + 10k(a \u22a4 j r) 2 .\nSince (a \u22a4 j r) 2 \u2264 (a \u22a4 j * r) 2 , where j * \u2264 2k is the column that OMP would select when the current solution is y \u2032 , and since (a\n\u22a4 j * r) 2 \u2264 h \u2212 Ay 2 2 /k (by Inequality (2)\n), we have that\nA( y \u2032 \u2212 y \u2032 ) 2 2 \u2264 4 r 2 2 + 10 h \u2212 Ay 2 2 \u2264 14 h \u2212 Ay 2 2 .\nTherefore,\nh \u2212 A y \u2032 2 \u2264 (1 + \u221a 14) h \u2212 Ay 2 .\nSquaring both sides gives the conclusion.", "publication_ref": ["b0", "b1", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Theorem 4", "text": "We use the following Chernoff bound for sums of \u03c7 2 random variables, a proof of which can be found in the Appendix A of [25].\nLemma 8. Fix any \u03bb 1 \u2265 . . . \u2265 \u03bb D > 0, and let X 1 , . . . , X D be i.i.d. \u03c7 2 random variables with one degree of freedom. Then Pr[\nD i=1 \u03bb i X i > (1 + \u03b3) D i=1 \u03bb i ] \u2264 exp(\u2212(D\u03b3 2 /24) \u2022 (\u03bb/\u03bb 1 )) for any 0 < \u03b3 < 1, where \u03bb = (\u03bb 1 + . . . + \u03bb D )/D. Write A = (1/ \u221a m)[\u03b8 1 | \u2022 \u2022 \u2022 |\u03b8 m ] \u22a4 , where each \u03b8 i is an independent d-dimensional Gaussian random vector N (0, I d ). Define v x = B \u22a4 x \u2212 E[y|x] so \u01eb = E x v x 2 2\n, and assume without loss of generality that v x has full d-dimensional support. Using this definition and linearity of expectation, we have\nE x Av x 2 2 = 1 m E x m i=1 (\u03b8 \u22a4 i v x ) 2 = 1 m m i=1 \u03b8 \u22a4 i (E x v x v \u22a4 x )\u03b8 i .\nOur goal is to show that this quantity is (1 + O(1/ \u221a m))\u01eb with high probability. Since N (0, I d ) is rotationally invariant and\nE x v x v \u22a4\nx is symmetric and positive definite, we may assume\nE x v x v \u22a4\nx is diagonal and has eigenvalues \u03bb 1 \u2265 . . . \u2265 \u03bb d > 0. Then, the above expression simplifies to\n1 m m i=1 \u03b8 \u22a4 i (E x v x v \u22a4 x )\u03b8 i = 1 m m i=1 d j=1 \u03bb j \u03b8 2 ij .\nEach \u03b8 2 ij is a \u03c7 2 random variable with one degree of freedom, so E\u03b8 2 ij = 1. Thus, the expected value of the above quantity is\nd j=1 trace(E x v x v \u22a4 x ) = E x trace(v x v \u22a4 x ) = E x v x 2 2\n. Now applying Lemma 8, with D = md variables and \u03bb = (\u03bb 1 +. . .+\u03bb d )/d, we have Pr[(1/m) i,j \u03bb j \u03b8 2 ij > (1+t)\u01eb] \u2264 exp(\u2212(mdt 2 /24)(\u03bb/\u03bb 1 )) \u2264 exp(\u2212mt 2 /24) (using the fact \u03bb 1 \u2264 d\u03bb). This bound is \u03b4 when t = (24/m) ln(1/\u03b4).", "publication_ref": ["b24"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Validation", "text": "We conducted an empirical assessment of our proposed reduction on two labeled data sets with large label spaces. These experiments demonstrate the feasibility of our method -a sanity check that the reduction does in fact preserve learnability -and compare different compression and reconstruction options.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data", "text": "Image data. 1 The first data set was collected by the ESP Game [26], an online game in which players ultimately provide word tags for a diverse set of web images.\nThe set contains nearly 68000 images, with about 22000 unique labels. We retained just the 1000 most frequent labels: the least frequent of these occurs 39 times in the data, and the most frequent occurs about 12000 times. Each image contains about four labels on average. We used half of the data for training and half for testing.\nWe represented each image as a bag-of-features vector in a manner similar to [27]. Specifically, we identified 1024 representative SURF features points [28] from 10 \u00d7 10 gray-scale patches chosen randomly from the training images; this partitions the space of image patches (represented with SURF features) into Voronoi cells. We then built a histogram for each image, counting the number of patches that fall in each cell.\nText data. 2 The second data set was collected by Tsoumakas et al. [11] from del.icio.us, a social bookmarking service in which users assign descriptive textual tags to web pages.\nThe set contains about 16000 labeled web page and 983 unique labels. The least frequent label occurs 21 times and the most frequent occurs almost 6500 times. Each web page is assigned 19 labels on average. Again, we used half the data for training and half for testing.\nEach web page is represented as a boolean bag-of-words vector, with the vocabulary chosen using a combination of frequency thresholding and \u03c7 2 feature ranking. See [11] for details.\nEach binary label vector (in both data sets) indicates the labels of the corresponding data point.", "publication_ref": ["b0", "b25", "b26", "b27", "b1", "b10", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Output Sparsity", "text": "We first performed a bit of exploratory data analysis to get a sense of how sparse the target in our data is. We computed the least-squares linear regressor B \u2208 R p\u00d7d on the training data (without any output coding) and predicted the label probabilities p(x) = B \u22a4 x on the test data (clipping values to the range [0, 1]). Using p(x) as a surrogate for the actual target E[y|x], we examined the relative \u2113 2 2 error of p and its best k-sparse approximation \u01eb(k, p(\nx)) = d i=k+1 p (i) (x) 2 / p(x) 2 2 , where p (1) (x) \u2265 . . . \u2265 p (d) (x)\n. Examining E x \u01eb(k, p(x)) as a function of k, we saw that in both the image and text data, the fall-off with k is eventually super-polynomial, but we are interested in the behavior for small k where it appears polynomial k \u2212r for some r. Around k = 10, we estimated an exponent of 0.50 for the image data and 0.55 for the text data. This is somewhat below the standard of what is considered sparse (e.g. vectors with small \u2113 1 -norm show k \u22121 decay). Thus, we expect the reconstruction algorithms will have to contend with the sparsity error of the target.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Procedure", "text": "We used least-squares linear regression as our base learning algorithm, with no regularization on the image data and with \u2113 2 -regularization with the text data (\u03bb = 0.01) for numerical stability. We did not attempt any parameter tuning.\nThe compression functions we used were generated by selecting m random rows of the 1024\u00d71024 Hadamard matrix, for m \u2208 {100, 200, 300, 400}. We also experimented with Gaussian matrices; these yielded similar but uniformly worse results.\nWe tested the greedy and iterative reconstruction algorithms described earlier (OMP, FoBa, and CoSaMP) as well as a path-following version of Lasso based on LARS [21]. Each algorithm was used to recover a k-sparse label vector y k from the predicted compressed label H(x), for k = 1, . . . , 10. We measured the \u2113 2 2 distance y k \u2212 y 2 2 of the prediction to the true test label y. In addition, we measured the precision of the predicted support at various values of k using the 10-sparse label prediction. That is, we ordered the coefficients of each 10-sparse label prediction y 10 by magnitude, and measured the precision of predicting the first k coordinates | supp( y 10\n(1:k) ) \u2229 supp(y)|/k. Actually, for k \u2265 6, we used y 2k instead of y 10 .\nWe used correlation decoding (CD) as a baseline method, as it is a standard decoding method for ECOC approaches. CD predicts using the top k coordinates in A \u22a4 H(x), ordered by magnitude. For mean-squared-error comparisons, we used the least-squares approximation of H(x) using these k columns of A. Note that CD is not a valid reconstruction algorithm when m < d.", "publication_ref": ["b20"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "As expected, the performance of the reduction, using any reconstruction algorithm, improves as the number of induced subproblems m is increased (see Figures 3 and 4; at m = 300, 400, the precision-at-k is nearly the same as one-againstall, i.e. m = 1024). When m is small and A \u2208 A K , the reconstruction algorithm cannot reliably choose k \u2265 K coordinates, so its performance may degrade after this point by over-fitting. But when the compression function A is in A K for a sufficiently large K, then the squared-error decreases as the output sparsity k increases up to K. Note the fact that precision-at-k decreases as k increases is expected, as fewer data will have at least k correct labels.\nAll of the reconstruction algorithms at least match or out-performed the baseline on the mean-squared-error criterion, except when m = 100. When A has few rows, (1) A \u2208 A K only for very small K, and (2) many of its columns will have significant correlation. In this case, when choosing k > K columns, it is better to choose correlated columns to avoid over-fitting. Both OMP and FoBa explicitly avoid this and thus do not fare well; but CoSaMP, Lasso, and CD do allow selecting correlated columns and thus perform better in this regime.\nThe results for precision-at-k are similar to that of mean-squared-error, except that choosing correlated columns does not necessarily help in the small m regime. This is because the extra correlated columns need not correspond to accurate label coordinates.\nIn summary, the experiments demonstrate the feasibility and robustness of our reduction method for two natural multilabel prediction tasks. They show that predictions of relatively few compressed labels are sufficient to recover an accurate sparse label vector, and as our theory suggests, the robustness of the reconstruction algorithms is a key factor in their success. ", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Compressed sensing", "journal": "IEEE Trans. Info. Theory", "year": "2006", "authors": "David Donoho"}, {"ref_id": "b1", "title": "Solving multiclass learning problems via error-correcting output codes", "journal": "Journal of Artificial Intelligence Research", "year": "1995", "authors": "T Dietterich; G Bakiri"}, {"ref_id": "b2", "title": "In defense of one-vs-all classification", "journal": "Journal of Machine Learning Research", "year": "2004", "authors": "R Rifkin; A Klautau"}, {"ref_id": "b3", "title": "Learning multi-label scene classification", "journal": "Pattern Recognition", "year": "2004", "authors": "M Boutell; J Luo; X Shen; C Brown"}, {"ref_id": "b4", "title": "Knowledge discovery in multi-label phenotype data", "journal": "", "year": "2001", "authors": "A Clare; R D King"}, {"ref_id": "b5", "title": "Max-margin markov networks", "journal": "", "year": "2003", "authors": "B Taskar; C Guestrin; D Koller"}, {"ref_id": "b6", "title": "Incremental algorithms for hierarchical classification", "journal": "Journal of Machine Learning Research", "year": "2006", "authors": "N Cesa-Bianchi; C Gentile; L Zaniboni"}, {"ref_id": "b7", "title": "Support vector machine learning for interdependent and structured output spaces", "journal": "", "year": "2004", "authors": "I Tsochantaridis; T Hofmann; T Joachims; Y Altun"}, {"ref_id": "b8", "title": "Kernel-based learning of hierarchical multilabel classification models", "journal": "Journal of Machine Learning Research", "year": "2006", "authors": "J Rousu; C Saunders; S Szedmak; J Shawe-Taylor"}, {"ref_id": "b9", "title": "Learning with structured sparsity", "journal": "", "year": "2009", "authors": "J Huang; T Zhang; D Metaxax"}, {"ref_id": "b10", "title": "Effective and efficient multilabel classification in domains with large number of labels", "journal": "", "year": "2008", "authors": "G Tsoumakas; I Katakis; I Vlahavas"}, {"ref_id": "b11", "title": "Reducing multiclass to binary: A unifying approach for margin classifiers", "journal": "Journal of Machine Learning Research", "year": "2000", "authors": "Erin Allwein; Robert Schapire; Yoram Singer"}, {"ref_id": "b12", "title": "Sensitive error correcting output codes", "journal": "", "year": "2005", "authors": "J Langford; A Beygelzimer"}, {"ref_id": "b13", "title": "Stable signal recovery from incomplete and inaccurate measurements", "journal": "Comm. Pure Appl. Math", "year": "2006", "authors": "Emmanuel Cand\u00e8s; Justin Romberg; Terrence Tao"}, {"ref_id": "b14", "title": "Deterministic constructions of compressed sensing matrices", "journal": "J. of Complexity", "year": "2007", "authors": "R Devore"}, {"ref_id": "b15", "title": "Uniform uncertainty principle for Bernoulli and subgaussian ensembles", "journal": "Constructive Approximation", "year": "2008", "authors": "Alain Shahar Mendelson; Nicole Pajor;  Tomczak-Jaegermann"}, {"ref_id": "b16", "title": "Sparse reconstruction by convex relaxation: Fourier and Gaussian measurements", "journal": "", "year": "2006", "authors": "M Rudelson; R Vershynin"}, {"ref_id": "b17", "title": "Matching pursuits with time-frequency dictionaries", "journal": "IEEE Transactions on Signal Processing", "year": "1993", "authors": "S Mallat; Z Zhang"}, {"ref_id": "b18", "title": "Adaptive forward-backward greedy algorithm for sparse learning with linear models", "journal": "", "year": "2008", "authors": "Tong Zhang"}, {"ref_id": "b19", "title": "CoSaMP: Iterative signal recovery from incomplete and inaccurate samples. Applied and Computational Harmonic Analysis", "journal": "", "year": "2007", "authors": "D Needell; J A Tropp"}, {"ref_id": "b20", "title": "Least angle regression", "journal": "Annals of Statistics", "year": "2004", "authors": "Trevor Bradley Efron; Iain Hastie; Robert Johnstone;  Tibshirani"}, {"ref_id": "b21", "title": "On the complexity of linear prediction: Risk bounds, margin bounds, and regularization", "journal": "", "year": "2008", "authors": "M Sham; Karthik Kakade; Ambuj Sridharan;  Tewari"}, {"ref_id": "b22", "title": "Feature selection, l1 vs. l2 regularization, and rotational invariance", "journal": "", "year": "2004", "authors": "Andrew Ng"}, {"ref_id": "b23", "title": "Stable recovery of sparse overcomplete representations in the presence of noise", "journal": "IEEE Trans. Info. Theory", "year": "2006", "authors": "David Donoho; Michael Elad; Vladimir Temlyakov"}, {"ref_id": "b24", "title": "Learning Probability Distributions", "journal": "", "year": "2000", "authors": "Sanjoy Dasgupta"}, {"ref_id": "b25", "title": "Labeling images with a computer game", "journal": "", "year": "2004", "authors": "Laura Luis Von Ahn;  Dabbish"}, {"ref_id": "b26", "title": "Learning object representations for visual object class recognition", "journal": "", "year": "2007", "authors": "Marcin Marsza\u0142ek; Cordelia Schmid; Hedi Harzallah; Joost Van De Weijer"}, {"ref_id": "b27", "title": "SURF: Speeded up robust features", "journal": "Computer Vision and Image Understanding", "year": "2008", "authors": "Herbert Bay; Andreas Ess; Tinne Tuytelaars; Luc Van Gool"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "The function f is the output sparsity of R and the constants C 1 and C 2 are the regret factors. Informally, if the predicted compressed label H(x) is close to E[Ay|x] = AE[y|x], then the sparse vector y returned by the reconstruction algorithm should be close to E[y|x]; this latter distance y \u2212 E[y|x] 2 2 should degrade gracefully in terms of the accuracy of H(x) and the sparsity of E[y|x]. Moreover, the algorithm should be agnostic about the sparsity of E[y|x] (and thus the sparsity error sperr(k, E[y|x]", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Training and prediction algorithms.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "\u2022\u2022All entries i.i.d. Gaussian N (0, 1/m), with m = O(k log(d/k)). All entries i.i.d. Bernoulli B(1/2) over {\u00b11/ \u221a m}, with m = O(k log(d/k)). \u2022 m randomly chosen rows of the d \u00d7 d Hadamard matrix over {\u00b11/ \u221a m}, with m = O(k log 5 d).", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Algorithm 33j | It is easy to check that the Gaussian, Bernoulli, and Hadamard-based random matrices given above have coherence bounded by O( (log d)/m) with high probability. Thus, one can take m = O(k 2 log d) to guarantee 1/k coherence. This is a factor k worse than what was needed for (k, \u03b4)-RIP, but the dependence on d is still small. Prediction algorithm with R = OMP parameters sparsity level k, compression function A = [a 1 | . . . |a d ] \u2208 A k with m rows, input regressors", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 2 :2Figure 2: Prediction algorithm specialized with Orthogonal Matching Pursuit.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "2 2 \u226424k 2 \u2022r for general k. For now, ignore the sparsity error. For simplicity, let A \u2208 R m\u00d7d with entries chosen i.i.d. from the Bernoulli B(1/2) distribution over {\u00b11/ \u221a m}, where m = O(k log d). Then for any k-sparse y, we have Ay \u221e \u2264 k/ \u221a m, and thus L i \u2264 2k/ \u221a m for each i. This gives the bound", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "which is within a constant factor of the guarantee afforded by SECOC. Note that our reduction induces exponentially (in d) fewer subproblems than SECOC. Now we consider the sparsity error. In the extreme case m = d, E[y|x] is allowed to be fully dense (k = d) and sperr(k, E[y|x]) = 0. When m = O(k log d) < d, we potentially incur an extra penalty in sperr(k, E[y|x]), which relates how far E[y|x] is from being k-sparse. For example, suppose E[y|x] has small \u2113 p norm for 0 \u2264 p < 2. Then even if E[y|x] has full support, the penalty will decrease polynomially in k \u2248 m/ log d.", "figure_data": ""}, {"figure_label": "34", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 3 :Figure 4 :34Figure 3: Mean-squared-error versus output sparsity k, m \u2208 {100, 200}. Top: image data. Bottom: text data. In each plot: the top set of lines corresponds to m = 100, and the bottom set to m = 200.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "X \u2192 Y with low expected \u2113 2 2 -error E x F (x) \u2212 E[y|x] 2 2", "formula_coordinates": [2.0, 72.0, 580.43, 468.11, 29.26]}, {"formula_id": "formula_1", "formula_text": "\u2113 2 2 -error E x H(x) \u2212 E[Ay|x] 2 2 .", "formula_coordinates": [3.0, 266.04, 181.85, 130.41, 18.39]}, {"formula_id": "formula_2", "formula_text": "y \u2212 y 2 2 \u2264 C 1 \u2022 h \u2212 Ay 2 2 + C 2 \u2022 sperr(k, y) for all y \u2208 R d .", "formula_coordinates": [3.0, 72.0, 389.69, 331.59, 36.87]}, {"formula_id": "formula_3", "formula_text": "f (k) = O(k).", "formula_coordinates": [3.0, 462.36, 598.77, 55.89, 9.96]}, {"formula_id": "formula_4", "formula_text": "A \u2208 A k with m rows, regression learning algo- rithm L input training data S \u2282 X \u00d7 R d for i = 1, . . . , m do h i \u2190 L({(x, (Ay) i ) : (x, y) \u2208 S}) end for output regressors H = [h 1 , . . . , h m ]", "formula_coordinates": [4.0, 78.0, 119.75, 210.68, 82.51]}, {"formula_id": "formula_5", "formula_text": "rithm R for A k input regressors H = [h 1 , . . . , h m ], test point x \u2208 X output y = R(k, A, [h 1 (x), . . . , h m (x)])", "formula_coordinates": [4.0, 300.48, 131.63, 210.65, 46.75]}, {"formula_id": "formula_6", "formula_text": "A \u2208 R m\u00d7d satisfies the (k, \u03b4)-restricted isometry property ((k, \u03b4)-RIP), \u03b4 \u2208 (0, 1), if (1 \u2212 \u03b4) x 2 2 \u2264 Ax 2 2 \u2264 (1 + \u03b4) x 2 2 for all k-sparse x \u2208 R d .", "formula_coordinates": [4.0, 72.0, 368.21, 467.95, 30.39]}, {"formula_id": "formula_7", "formula_text": ") \u2264 O(1/k), where \u00b5(A) defined as \u00b5(A) = max 1\u2264i<j\u2264d |(A \u22a4 A) i,j |/ |(A \u22a4 A) i,i ||(A \u22a4 A) j,", "formula_coordinates": [4.0, 72.0, 554.63, 467.91, 43.06]}, {"formula_id": "formula_8", "formula_text": "H = [h 1 , . . . , h m ], test point x \u2208 X h \u2190 [h 1 (x), . . . , h m (x)] \u22a4 (predict compressed label vector) y \u2190 0, J \u2190 \u2205, r \u2190 h for i = 1, .", "formula_coordinates": [5.0, 81.96, 111.95, 251.6, 46.06]}, {"formula_id": "formula_9", "formula_text": "sperr(k, y) = y \u2212 y (1:k) 2 2 + 1 k y \u2212 y (1:k) 2 1", "formula_coordinates": [5.0, 210.24, 322.24, 191.05, 23.09]}, {"formula_id": "formula_10", "formula_text": "Theorem 1. Let A k = {(k + f (k), \u03b4)", "formula_coordinates": [5.0, 72.0, 405.83, 154.63, 17.26]}, {"formula_id": "formula_11", "formula_text": "A y \u2212 h 2 2 \u2264 inf y\u2208R d C Ay (1:k) \u2212 h 2 2 ,", "formula_coordinates": [5.0, 235.32, 437.21, 146.41, 18.87]}, {"formula_id": "formula_12", "formula_text": "C 1 = 2(1 + \u221a C) 2 /(1 \u2212 \u03b4) and C 2 = 4(1 + (1 + \u221a C)/(1 \u2212 \u03b4)) 2 .", "formula_coordinates": [5.0, 72.0, 468.95, 267.45, 25.66]}, {"formula_id": "formula_13", "formula_text": "A y \u2212 h 2 2 \u2264 23 Ay (1:k) \u2212 h 2 2 \u2200y \u2208 R d .", "formula_coordinates": [5.0, 222.12, 641.69, 172.81, 18.88]}, {"formula_id": "formula_14", "formula_text": "N\u00d7R d \u2192 R.", "formula_coordinates": [6.0, 486.96, 217.13, 53.01, 18.51]}, {"formula_id": "formula_15", "formula_text": "E x F (x) \u2212 E[y|x] 2 2 \u2264 C 1 \u2022 E x H(x) \u2212 E[Ay|x] 2 2 + C 2 \u2022 sperr(k, E[y|x]).", "formula_coordinates": [6.0, 140.88, 258.65, 330.13, 18.87]}, {"formula_id": "formula_16", "formula_text": "r = 1 m m i=1 E x H(x) i \u2212 E[(Ay) i |x] L i 2 .", "formula_coordinates": [6.0, 219.24, 354.53, 173.53, 31.21]}, {"formula_id": "formula_17", "formula_text": "E x F (x) \u2212 E[y|x]", "formula_coordinates": [6.0, 94.44, 404.87, 75.85, 17.26]}, {"formula_id": "formula_18", "formula_text": "C 1 \u2022 E x H(x) \u2212 E[Ay|x] 2 2 \u2264 4C 1 \u2022 k 2 \u2022r,", "formula_coordinates": [6.0, 216.12, 463.13, 179.77, 18.87]}, {"formula_id": "formula_19", "formula_text": "H \u22a4 x = AB \u22a4 x = AE[y|x] = E[Ay|x].", "formula_coordinates": [6.0, 219.12, 665.09, 173.65, 11.92]}, {"formula_id": "formula_20", "formula_text": "E x B \u22a4 x \u2212 E[y|x] 2 2 = \u01eb.", "formula_coordinates": [7.0, 249.72, 93.53, 112.57, 21.39]}, {"formula_id": "formula_21", "formula_text": "E x H \u22a4 x \u2212 AE[y|x] 2 2 \u2264 1 + O(1/ \u221a m) \u01eb.", "formula_coordinates": [7.0, 211.56, 136.67, 188.89, 24.82]}, {"formula_id": "formula_22", "formula_text": "roughly 1 + O( d/m).", "formula_coordinates": [7.0, 326.52, 173.97, 97.17, 9.96]}, {"formula_id": "formula_23", "formula_text": "\u2113 = k + f (k), y \u2208 R d ,", "formula_coordinates": [7.0, 87.72, 360.41, 91.05, 18.39]}, {"formula_id": "formula_24", "formula_text": "y \u2212 y 2 2 \u2264 C 1 \u2022 Ay \u2212 h 2 2 + C 2 \u2022 ( \u2206 2 2 + k \u22121 \u2206 2 1 )", "formula_coordinates": [7.0, 192.72, 381.17, 231.64, 18.88]}, {"formula_id": "formula_25", "formula_text": "A y \u2212 h 2 2 \u2264 C Ay (1:k) \u2212 h 2 2 , we have y \u2212 y 2 \u2264 y \u2212 y (1:k) 2 + \u2206 2 \u2264 (1 \u2212 \u03b4) \u22121/2 A y \u2212 Ay (1:k) 2 + \u2206 2 \u2264 (1 \u2212 \u03b4) \u22121/2 A y \u2212 h 2 + h \u2212 Ay (1:k) 2 + \u2206 2 \u2264 (1 \u2212 \u03b4) \u22121/2 (1 + \u221a C) Ay (1:k) \u2212 h 2 + \u2206 2 \u2264 (1 \u2212 \u03b4) \u22121/2 (1 + \u221a C) ( Ay \u2212 h 2 + A\u2206 2 ) + \u2206 2 .(1)", "formula_coordinates": [7.0, 72.0, 402.89, 468.08, 119.91]}, {"formula_id": "formula_26", "formula_text": "y Ji+1 2 \u2264 ( y Ji+1 \u221e y Ji+1 1 ) 1/2 \u2264 (\u2113 \u22121 y Ji 2 1 ) 1/2 = \u2113 \u22121/2 y Ji 1 ,and", "formula_coordinates": [7.0, 72.0, 573.77, 385.09, 32.83]}, {"formula_id": "formula_27", "formula_text": "so i\u22650 y Ji 2 \u2264 y J0 2 + i\u22650 y Ji+1 2 \u2264 y J0 2 + \u2113 \u22121/2 i\u22650 y Ji 1 \u2264 \u2206 2 + \u2113 \u22121/2 \u2206 1 .", "formula_coordinates": [7.0, 88.92, 597.64, 408.13, 41.67]}, {"formula_id": "formula_28", "formula_text": "A\u2206 2 \u2264 i\u22650 Ay Ji 2 \u2264 i\u22650 (1 + \u03b4) 1/2 y Ji 2 \u2264 (1 + \u03b4) 1/2 ( \u2206 2 + \u2113 \u22121/2 \u2206 1 ).", "formula_coordinates": [7.0, 134.4, 671.57, 348.13, 22.93]}, {"formula_id": "formula_29", "formula_text": "y \u2212 y 2 \u2264 C 0 \u2022 Ay \u2212 h 2 + (1 + C 0 (1 + \u03b4) 1/2 ) \u2022 ( \u2206 2 + \u2113 \u22121/2 \u2206 1 )", "formula_coordinates": [8.0, 155.76, 93.41, 305.56, 18.87]}, {"formula_id": "formula_30", "formula_text": "C 0 = (1 \u2212 \u03b4) \u22121/2 (1 + \u221a C", "formula_coordinates": [8.0, 99.12, 107.51, 109.47, 25.66]}, {"formula_id": "formula_31", "formula_text": "0 \u2264 i < k such that if j i is the column selected in step i, then (a \u22a4 ji (h \u2212 Ay (i) )) 2 \u2264 h 2 2 /k.", "formula_coordinates": [8.0, 164.4, 213.05, 365.37, 18.39]}, {"formula_id": "formula_32", "formula_text": "r (i) 2 2 \u2212 r (i+1) 2 2 \u2265 r (i) 2 2 \u2212 h \u2212 A y (i+1) 2 2 = r (i) 2 2 \u2212 h \u2212 A(y (i) + \u03b1 i e ji ) 2 2 = r (i) 2 2 \u2212 r (i) \u2212 \u03b1 i a ji 2 2 = 2\u03b1 i a \u22a4 ji r (i) \u2212 \u03b1 2 i a ji 2 2 = (a \u22a4 ji r (i) ) 2 .", "formula_coordinates": [8.0, 128.16, 273.17, 360.73, 35.91]}, {"formula_id": "formula_33", "formula_text": "k\u22121 i=0 r (i) 2 2 \u2212 r (i+1) 2 2 = r (0) 2 2 \u2212 r (k) 2 2 \u2264 h 2 2 , so there is some i \u2208 {0, 1, . . . , k \u2212 1} such that (a \u22a4 ji r (i) ) 2 \u2264 r (i) 2 2 \u2212 r (i+1) 2 2 \u2264 h 2 2 /k. Lemma 7. If y \u2208 R d is k-sparse and \u00b5(A) \u2264 \u03b4/(k \u2212 1), then Ay 2 2 \u2265 (1 \u2212 \u03b4) y 2 2 .", "formula_coordinates": [8.0, 72.0, 312.77, 468.01, 51.99]}, {"formula_id": "formula_34", "formula_text": "Ay 2 2 = k i=1 a i 2 y 2 i + i =j y i y j (a \u22a4 i a j ) \u2265 y 2 2 \u2212 i =j y i y j (a \u22a4 i a j ) ,", "formula_coordinates": [8.0, 164.76, 416.57, 287.53, 31.45]}, {"formula_id": "formula_35", "formula_text": "i =j y i y j (a \u22a4 i a j ) \u2264 i =j |y i y j ||a \u22a4 i a j | (triangle inequality) \u2264 \u00b5(A) i =j |y i y j | (definition of coherence) = \u00b5(A) \uf8eb \uf8ed k i=1 k j=1 |y i ||y j | \u2212 k i=1 y 2 i \uf8f6 \uf8f8 = \u00b5(A) \uf8eb \uf8ed k i=1 |y i | 2 \u2212 y 2 2 \uf8f6 \uf8f8 \u2264 \u00b5(A)(k y 2 2 \u2212 y 2 2 ) (Cauchy-Schwarz) = \u00b5(A)(k \u2212 1) y 2 2 \u2264 \u03b4 y 2 2", "formula_coordinates": [8.0, 125.64, 491.09, 365.24, 187.83]}, {"formula_id": "formula_36", "formula_text": "(a \u22a4 j * (h \u2212 Ay \u2032 )) 2 \u2264 h \u2212 Ay 2 2 /k.(2)", "formula_coordinates": [9.0, 241.92, 186.17, 298.16, 18.87]}, {"formula_id": "formula_37", "formula_text": "A y \u2212 h 2 2 \u2264 A y \u2032 \u2212 h 2 2 \u2264 Ay \u2212 h 2", "formula_coordinates": [9.0, 102.0, 252.41, 437.95, 30.39]}, {"formula_id": "formula_38", "formula_text": "h \u2212 A y 2 \u2264 h \u2212 A y \u2032 2 (since y \u2032 precedes y) \u2264 h \u2212 Ay \u2032 2 + A( y \u2032 \u2212 y \u2032 ) 2 (triangle inequality) \u2264 h \u2212 Ay 2 + A( y \u2032 \u2212 y \u2032 ) 2 .", "formula_coordinates": [9.0, 146.76, 323.69, 323.6, 48.75]}, {"formula_id": "formula_39", "formula_text": "A( y \u2032 \u2212 y \u2032 ) 2 2 = (A y \u2032 \u2212 Ay \u2032 ) \u22a4 A( y \u2032 \u2212 y \u2032 ) = (h \u2212 Ay \u2032 ) \u22a4 A( y \u2032 \u2212 y \u2032 ) \u2212 (h \u2212 A y \u2032 ) \u22a4 A( y \u2032 \u2212 y \u2032 ) \u2264 h \u2212 Ay \u2032 2 A( y \u2032 \u2212 y \u2032 ) 2 + |(h \u2212 A y \u2032 ) \u22a4 A( y \u2032 \u2212 y \u2032 )| (Cauchy-Schwarz) = r 2 A( y \u2032 \u2212 y \u2032 ) 2 + | r \u22a4 A( y \u2032 \u2212 y \u2032 )|. Using the fact x \u2264 b \u221a x + c \u21d2 x \u2264 (4/3)(b 2 + c)", "formula_coordinates": [9.0, 72.0, 415.37, 432.56, 90.39]}, {"formula_id": "formula_40", "formula_text": "3 4 A( y \u2032 \u2212 y \u2032 ) 2 2 \u2264 r 2 2 + | r \u22a4 A( y \u2032 \u2212 y \u2032 )|.(3)", "formula_coordinates": [9.0, 213.36, 520.84, 326.72, 23.09]}, {"formula_id": "formula_41", "formula_text": "|a \u22a4 j r| \u2265 |a \u22a4 \u2113 r| \u2200\u2113 \u2264 2k.(4)", "formula_coordinates": [9.0, 246.72, 583.61, 293.36, 18.99]}, {"formula_id": "formula_42", "formula_text": "A( y \u2032 \u2212 y \u2032 ) = A {1:2k} ( y \u2032 \u2212 y \u2032 )(5)", "formula_coordinates": [9.0, 237.0, 627.53, 303.08, 18.87]}, {"formula_id": "formula_43", "formula_text": "| r \u22a4 A( y \u2032 \u2212 y \u2032 )| = | r \u22a4 A {1:2k} ( y \u2032 \u2212 y \u2032 )| (Equation (5)) \u2264 r \u22a4 A {1:2k} \u221e y \u2032 \u2212 y \u2032 1 (H\u00f6lder's inequality) \u2264 |a \u22a4 j r| y \u2032 \u2212 y \u2032 1 (Inequality (4)) \u2264 |a \u22a4 j r| + |a \u22a4 j A( y \u2032 \u2212 y \u2032 )| y \u2032 \u2212 y \u2032 1 (triangle inequality) \u2264 |a \u22a4 j r| + a \u22a4 j A {1:2k} \u221e y \u2032 \u2212 y \u2032 1 y \u2032 \u2212 y \u2032 1", "formula_coordinates": [10.0, 104.88, 95.09, 382.04, 85.47]}, {"formula_id": "formula_44", "formula_text": "\u2264 |a \u22a4 j r| y \u2032 \u2212 y \u2032 1 + \u00b5(A) y \u2032 \u2212 y \u2032 2 1 (definition of coherence) \u2264 5k 2 (a \u22a4 j r) 2 + 1 10k y \u2032 \u2212 y \u2032 2 1 + \u00b5(A) y \u2032 \u2212 y \u2032 2 1 (since xy \u2264 (x 2 + y 2 )/2) \u2264 5k 2 (a \u22a4 j r) 2 + 1 5k y \u2032 \u2212 y \u2032 2 1 (since \u00b5(A) \u2264 0.1/k) \u2264 5k 2 (a \u22a4 j r) 2 + 1 5k (2k y \u2032 \u2212 y \u2032 2 2 ) (Cauchy-Schwarz) \u2264 5k 2 (a \u22a4 j r) 2 + 1 2 A( y \u2032 \u2212 y \u2032 ) 2 2 . (Lemma 7", "formula_coordinates": [10.0, 168.12, 178.37, 338.84, 113.8]}, {"formula_id": "formula_45", "formula_text": "A( y \u2032 \u2212 y \u2032 ) 2 2 \u2264 4 r 2 2 + 10k(a \u22a4 j r) 2 .", "formula_coordinates": [10.0, 226.44, 317.81, 164.17, 18.99]}, {"formula_id": "formula_46", "formula_text": "\u22a4 j * r) 2 \u2264 h \u2212 Ay 2 2 /k (by Inequality (2)", "formula_coordinates": [10.0, 104.04, 353.93, 161.98, 18.52]}, {"formula_id": "formula_47", "formula_text": "A( y \u2032 \u2212 y \u2032 ) 2 2 \u2264 4 r 2 2 + 10 h \u2212 Ay 2 2 \u2264 14 h \u2212 Ay 2 2 .", "formula_coordinates": [10.0, 222.6, 376.01, 171.49, 33.87]}, {"formula_id": "formula_48", "formula_text": "h \u2212 A y \u2032 2 \u2264 (1 + \u221a 14) h \u2212 Ay 2 .", "formula_coordinates": [10.0, 227.76, 425.75, 161.53, 26.02]}, {"formula_id": "formula_49", "formula_text": "D i=1 \u03bb i X i > (1 + \u03b3) D i=1 \u03bb i ] \u2264 exp(\u2212(D\u03b3 2 /24) \u2022 (\u03bb/\u03bb 1 )) for any 0 < \u03b3 < 1, where \u03bb = (\u03bb 1 + . . . + \u03bb D )/D. Write A = (1/ \u221a m)[\u03b8 1 | \u2022 \u2022 \u2022 |\u03b8 m ] \u22a4 , where each \u03b8 i is an independent d-dimensional Gaussian random vector N (0, I d ). Define v x = B \u22a4 x \u2212 E[y|x] so \u01eb = E x v x 2 2", "formula_coordinates": [10.0, 72.0, 551.21, 467.98, 65.31]}, {"formula_id": "formula_50", "formula_text": "E x Av x 2 2 = 1 m E x m i=1 (\u03b8 \u22a4 i v x ) 2 = 1 m m i=1 \u03b8 \u22a4 i (E x v x v \u22a4 x )\u03b8 i .", "formula_coordinates": [10.0, 183.48, 629.81, 245.05, 31.21]}, {"formula_id": "formula_51", "formula_text": "E x v x v \u22a4", "formula_coordinates": [10.0, 127.68, 681.29, 32.87, 12.01]}, {"formula_id": "formula_52", "formula_text": "E x v x v \u22a4", "formula_coordinates": [10.0, 375.24, 681.29, 32.99, 12.01]}, {"formula_id": "formula_53", "formula_text": "1 m m i=1 \u03b8 \u22a4 i (E x v x v \u22a4 x )\u03b8 i = 1 m m i=1 d j=1 \u03bb j \u03b8 2 ij .", "formula_coordinates": [11.0, 218.52, 95.69, 176.17, 31.21]}, {"formula_id": "formula_54", "formula_text": "d j=1 trace(E x v x v \u22a4 x ) = E x trace(v x v \u22a4 x ) = E x v x 2 2", "formula_coordinates": [11.0, 127.32, 151.13, 207.25, 14.89]}, {"formula_id": "formula_55", "formula_text": "x)) = d i=k+1 p (i) (x) 2 / p(x) 2 2 , where p (1) (x) \u2265 . . . \u2265 p (d) (x)", "formula_coordinates": [12.0, 82.56, 74.49, 457.39, 30.36]}], "doi": ""}