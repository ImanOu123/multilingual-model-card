{"title": "Relative Attributes", "authors": "Devi Parikh; Kristen Grauman", "pub_date": "", "abstract": "Human-nameable visual \"attributes\" can benefit various recognition tasks. However, existing techniques restrict these properties to categorical labels (for example, a person is 'smiling' or not, a scene is 'dry' or not), and thus fail to capture more general semantic relationships. We propose to model relative attributes. Given training data stating how object/scene categories relate according to different attributes, we learn a ranking function per attribute. The learned ranking functions predict the relative strength of each property in novel images. We then build a generative model over the joint space of attribute ranking outputs, and propose a novel form of zero-shot learning in which the supervisor relates the unseen object category to previously seen objects via attributes (for example, 'bears are furrier than giraffes'). We further show how the proposed relative attributes enable richer textual descriptions for new images, which in practice are more precise for human interpretation. We demonstrate the approach on datasets of faces and natural scenes, and show its clear advantages over traditional binary attribute prediction for these new tasks.", "sections": [{"heading": "Introduction", "text": "While traditional visual recognition approaches map low-level image features directly to object category labels, recent work proposes models using visual attributes [1][2][3][4][5][6][7][8]. Attributes are properties observable in images that have human-designated names (e.g., 'striped', 'four-legged'), and they are valuable as a new semantic cue in various problems. For example, researchers have shown their impact for strengthening facial verification [5], object recognition [6,8,16], generating descriptions of unfamiliar objects [1], and to facilitate \"zero-shot\" transfer learning [2], where one trains a classifier for an unseen object simply by specifying which attributes it has.\nProblem: Most existing work focuses wholly on attributes as binary predicates indicating the presence (or absence) of a certain property in an image [1][2][3][4][5][6][7][8]16]. This may suffice for part-based attributes (e.g., 'has a head') and some (f) Manmade Figure 1. Binary attributes are an artificially restrictive way to describe images. While it is clear that (a) is smiling, and (c) is not, the more informative and intuitive description for (b) is via relative attributes: he is smiling more than (a) but less than (c). Similarly, scene (e) is less natural than (d), but more so than (f). Our main idea is to model relative attributes via learned ranking functions, and then demonstrate their impact on novel forms of zero-shot learning and generating image descriptions. binary properties (e.g., 'spotted'). However, for a large variety of attributes, not only is this binary setting restrictive, but it is also unnatural. For instance, it is not clear if in Figure 1(b) Hugh Laurie is smiling or not; different people are likely to respond inconsistently in providing the presence or absence of the 'smiling' attribute for this image, or of the 'natural' attribute for Figure 1(e).\nIndeed, we observe that relative visual properties are a semantically rich way by which humans describe and compare objects in the world. They are necessary, for instance, to refine an identifying description (\"the 'rounder' pillow\"; \"the same except 'bluer\"'), or to situate with respect to reference objects (\"'brighter' than a candle; 'dimmer' than a flashlight\"). Furthermore, they have potential to enhance active and interactive learning-for instance, offering a better guide for a visual search (\"find me similar shoes, but 'shinier'.\" or \"refine the retrieved images of downtown Chicago to those taken on 'sunnier' days\").\nProposal: In this work, we propose to model relative attributes. As opposed to predicting the presence of an attribute, a relative attribute indicates the strength of an attribute in an image with respect to other images. For exam-ple, in Figure 1, while it is difficult to assign a meaningful value to the binary attribute 'smiling', we could all agree on the relative attribute, i.e. Hugh Laurie is smiling less than Scarlett Johansson, but more than Jared Leto. In addition to being more natural, relative attributes would offer a richer mode of communication, thus allowing access to more detailed human supervision (and so potentially higher recognition accuracy), as well as the ability to generate more informative descriptions of novel images.\nHow can we learn relative properties? Whereas traditional supervised classification is appropriate to learn attributes that are intrinsically binary, it falls short when we want to represent visual properties that are nameable but not categorical. Our goal is instead to estimate the degree of that attribute's presence-which, importantly, differs from the probability of a binary classifier's prediction. To this end, we devise an approach that learns a ranking function for each attribute, given relative similarity constraints on pairs of examples (or more generally a partial ordering on some examples). The learned ranking function can estimate a real-valued rank 1 for images indicating the relative strength of the attribute presence in them. Then, we introduce novel forms of zero-shot learning and description that exploit the relative attribute predictions.\nThe proposed ranking approach accounts for a subtle but important difference between relative attributes and conceivable alternatives based on regression or multi-way classification. While such alternatives could also allow for a richer vocabulary, during training they could suffer from similar inconsistencies as binary attributes. For example, it is more difficult to define and perhaps more importantly, agree on, \"With what strength is he smiling?\" than \"Is he smiling more than she is?\". Thus, we expect the relative mode of supervision our approach permits to be more natural and consistent for human labelers.\nContributions: Our main contribution is the idea to learn relative visual attributes, which to our knowledge has not been explored in any prior work. Our other contribution is to devise and demonstrate two new tasks well-served by relative attributes: (1) zero-shot learning from relative comparisons, and (2) image description in reference to example images or categories. We demonstrate the approach for both tasks using the Outdoor Scenes dataset [11] and a subset of the Public Figure Face Database [12]. We find that relative attributes yield significantly better zero-shot learning accuracy when compared to their binary counterparts. In addition, we conduct human subject studies to evaluate the informativeness of the automatically generated image descriptions, and find that relative attributes are clearly more powerful than existing binary attributes in uniquely identifying an image. 1 Throughout this paper we refer to rank as a real-valued score.", "publication_ref": ["b0", "b1", "b2", "b3", "b4", "b5", "b6", "b7", "b4", "b5", "b7", "b15", "b0", "b1", "b0", "b1", "b2", "b3", "b4", "b5", "b6", "b7", "b15", "b10", "b11", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "We review related work on visual attributes, other uses of relative cues, and methods for learning comparisons.\nBinary attributes: Learning attribute categories allows prediction of color or texture types [13], and can also provide a mid-level cue for object or face recognition [2,5,8]. Beyond object recognition, the semantics intrinsic to attributes enable zero-shot transfer [2,6,14], or description and part localization [1,7,15]. Rather than manually define attribute vocabularies, some work aims to discover attribute-related concepts on the Web [3,4], extract them from existing knowledge sources [6,16] or discover them interactively [9]. In contrast to our approach, all such methods restrict the attributes to be categorical (and in fact, binary).\nRelative information: Relative information has been explored in vision in a variety of ways. Recent work on largescale recognition exploits WordNet-based information to specify a semantic-distance sensitive classifier [18], or to make do with few labels by sharing training images among semantically similar classes [17]. Stemming from a related motivation of limited labeled data, Wang et al. [19] make use of explicit similarity-based supervision such as \"A serval is like a leopard\" or \"A zebra is similar to the crosswalk in texture\" to share training instances for categories with limited or no training instances. Unlike our approach, that method learns a model for each object category, and does not model attributes. In contrast, our attribute models are category-independent and transferrable, enabling relative descriptions between all classes. Moreover, whereas that technique captures similarity among object categories, ours models a general ordering of the images sorted by the strength of their attributes, as well as a joint space over multiple such relative attributes. Kumar et al. [12] explore comparative facial attributes such as \"Lips like Barack Obama\" for face verification. These attributes, although comparative, are also modeled as binary classifiers and are similarity-based as opposed to an ordering. Gupta et al. [20] and Siddiquie et al. [21] use prepositions and adjectives to relate objects to each other for more effective contextual modeling and active learning, respectively. In contrast, our work involves relative modeling of attribute strengths for a richer vocabulary that enhances supervision and description of images.\nLearning to rank: Learning to rank has received extensive attention in the machine learning literature [22][23][24], for information retrieval in general and image retrieval in particular [25,26]. Given a query image, user preferences (often captured via click-data) are incorporated to learn a ranking function with the goal of retrieving more relevant images in the top search results. Learned distance metrics (e.g., [27,28]) can induce a ranking on images; however, this ranking is also specific to a query image, and typically intended for nearest-neighbor-based classifiers. Our work learns a ranking function on images based on constraints specifying the relative strength of attributes, and the resulting function is not relative to any other image in the dataset. Thus, unlike query-centric retrieval tasks, we can characterize individual images by the strength of the attributes present, which we show is valuable for new recognition and description applications.", "publication_ref": ["b12", "b1", "b4", "b7", "b1", "b5", "b13", "b0", "b6", "b14", "b2", "b3", "b5", "b15", "b8", "b17", "b16", "b18", "b11", "b19", "b20", "b21", "b22", "b23", "b24", "b25", "b26", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "Approach", "text": "We first present our approach for learning relative attributes (Section 3.1), and then explain how we can use relative attributes for enhanced zero-shot learning (Section 3.2) and image description generation (Section 3.3).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Learning Relative Attributes", "text": "We are given a set of training images I = {i} represented in R n by feature-vectors {x i } and a set of M attributes A = {a m }. In addition, for each attribute a m , we are given a set of ordered pairs of images O m = {(i, j)} and a set of un-ordered pairs S m = {(i, j)} such that (i, j) \u2208 O m =\u21d2 i j, i.e. image i has a stronger presence of attribute a m than j, and (i, j) \u2208 S m =\u21d2 i \u223c j, i.e. i and j have similar relative strengths of a m . We note that O m and S m can be deduced from any partial ordering of the images I in the training data with respect to strength of a m . Either O m or S m , but not both, can be empty.\nOur goal is to learn M ranking functions\nr m (x i ) = w T m x i ,(1)\nfor m = 1, . . . , M , such that the maximum number of the following constraints is satisfied:\n\u2200(i, j) \u2208 O m : w T m x i > w T m x j (2) \u2200(i, j) \u2208 S m : w T m x i = w T m x j .(3)\nWhile this is an NP hard problem [22], it is possible to approximate the solution with the introduction of nonnegative slack variables, similar to SVM classification. We directly adapt the formulation proposed in [22], which was originally applied to web page ranking, except we use a quadratic loss function together with similarity constraints, leading to the following optimization problem:\nminimize 1 2 ||w T m || 2 2 + C \u03be 2 ij + \u03b3 2 ij (4) s.t. w T m x i \u2265 w T m x j + 1 \u2212 \u03be ij ; \u2200(i, j) \u2208 O m (5) |w T m x i \u2212 w T m x j | \u2264 \u03b3 ij ; \u2200(i, j) \u2208 S m (6) \u03be ij \u2265 0; \u03b3 ij \u2265 0.(7)\n!\"#$$%&'() *#(+%,) ! \"# -#,.) *#(+%,)\n! $# /) 0) 1)\n2)\n3)", "publication_ref": ["b21", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "4)", "text": "Figure 2. Distinction between learning a wide-margin ranking function (right) that enforces the desired ordering on training points (1-6) , and a wide-margin binary classifier (left) that only separates the two classes (+ and \u2212), and does not necessarily preserve a desired ordering on the points.\nRearranging the constraints reveals that the above formulation, without the similarity constraints in Eqn. 6, is quite similar to the SVM classification problem, but on pairwise difference vectors:\nminimize 1 2 ||w T m || 2 2 + C \u03be 2 ij + \u03b3 2 ij (8) s.t. w T m (x i \u2212 x j ) \u2265 1 \u2212 \u03be ij ; \u2200(i, j) \u2208 O m (9) |w T m (x i \u2212 x j )| \u2264 \u03b3 ij ; \u2200(i, j) \u2208 S m (10) \u03be ij \u2265 0; \u03b3 ij \u2265 0, (11\n)\nwhere C is the trade-off constant between maximizing the margin and satisfying the pairwise relative constraints. We solve the above primal problem using Newton's method [29]. While we use a linear ranking function in our experiments, the above formulation can be easily extended to kernels. We note that this learning-to-rank formulation learns a function that explicitly enforces a desired ordering on the training images; the margin is the distance between the closest two projections within all desired (training) rankings. In contrast, if one were to train a binary classifier, only the margin between the nearest binary-labeled examples is enforced; ordering among examples beyond those defining the margin is arbitrary. See Figure 2. Our experiments confirm this distinction does indeed matter in practice, as our learnt ranking function is more effective at capturing the relative strengths of the attributes than the score of a binary classifier (i.e., the magnitude of the SVM decision function).\nIn addition, training with comparisons (image i is similar to j in terms of attribute a m , or i exhibits a m less than j) is well-suited to the task at hand. Attribute strengths are arguably more natural to express in relative terms, as opposed to requiring absolute judgments in isolation (i.e., i represents a m with degree 10).\nWe apply our learnt relative attributes in two new settings: (1) zero-shot learning with relative relationships and (2) generating image descriptions. We now introduce our approach to incorporate relative attributes for each of these applications in turn.", "publication_ref": ["b28"], "figure_ref": [], "table_ref": []}, {"heading": "Zero-Shot Learning From Relationships", "text": "Consider N categories of interest. For example, each category may be an object class, or a type of scene. During training, S of these categories are 'seen' categories for which training images are provided, while the remaining U = N \u2212 S categories are 'unseen', for which no training images are provided.\nThe S categories are described via relative attributes with respect to each other, be it pairwise relationships or partial orders. For example, \"bears are furrier than giraffes but less furry than rabbits\", \"lions are larger than dogs, as large as tigers, but less large than elephants\", etc. We note that all pairs of categories need not be related in the supervision, and different subsets of categories can be related for the different attributes.\nThe U unseen categories, on the other hand, are described relative to one or two seen categories for a subset of the attributes, i.e., unseen class c\n(u) j can be described as c (s) i c (u) j c (s) k for attribute a m , or c (s) i c (u) j , or c (u) j c (s) k , where c (s) i and c (s)\nk are seen categories. We note the simple and flexible supervision required for the categories, especially the unseen ones: for any attribute (not necessarily all), the user can select any seen category depicting a stronger and/or weaker presence of the attribute to which to relate the unseen category. While alternative list-based learning to rank techniques are available [23], we choose the pairwise learning technique as described in Section 3.1 to ensure this ease of supervision.\nDuring testing, a novel image is to be classified into any of the N categories. Our zero-shot learning setting is more general than the model proposed by Lampert et al. [2], in that the supervisor may not only associate attributes with categories, but also express how the categories relate along any number of the attributes. We expect this richer representation to allow better divisions between both the unseen and seen categories, as we demonstrate in the experiments.\nWe propagate the category relationships provided during training to the corresponding images, i.e., for seen classes c\n(s) i and c (s) j , c (s) i c (s) j =\u21d2 i j; \u2200i \u2208 c (s) i , \u2200j \u2208 c (s) j\nfor attribute a m . 2 We then learn all M relative attributes as described in Section 3.1. Predicting the real-valued rank of all images in the training dataset I allows us to transform x i \u2208 R n \u2192x i \u2208 R M , such that each image i is now represented as an M -dimensional vectorx i indicating its rank score for all M attributes.\nWe now build a generative model for each of the S seen categories in R M . We use a Gaussian distribution, and estimate the mean \u00b5 i , so we have c\n(s) i \u223c N (\u00b5 (s) i , \u03a3 (s) i ), for i = 1, . . . , S.\nThe parameters of the generative model corresponding to each of the U unseen categories are selected under the guidance of the input relative descriptions. In particular, given an unseen category c (u) , we employ the following: to be the variance of the same.\n1. If c (u) j is described as c (s) i c (u) j c (s) k ,\nIn the first three cases, we simply set \u03a3\n(u) j = 1 S S i=1 \u03a3 (s) i .\nGiven a test image i, we computex i \u2208 R M indicating the relative attribute ranking-scores for the image. It is then assigned to the seen or unseen category that assigns it the highest likelihood:\nc * = argmax j\u2208{1,...,N } P (x i | \u00b5 j , \u03a3 j ) . (12\n)\nFrom a Bayesian perspective, our approach to setting the parameters of the unseen categories' generative models can be considered to be priors transferred from the knowledge of the models for the seen categories. Under reasonable priors, the choice of mean and covariances correspond to the minimum mean-squared error and maximum likelihood estimates. Related formulations of transfer through parameter sharing have been studied by Fei-Fei et al. [30] and Stark et al. [31] for learning shape-based object models with few training images, though no prior models consider transferring knowledge based on relative comparisons, as we do here.\nWe note that if one or more images from the unseen categories were subsequently to become available, our estimated parameters could easily be updated in light of the additional evidence. Furthermore, our general approach could potentially support more specific supervision about the relative relationships, should it be available (e.g., bears (unseen) are significantly more furry than cows (seen)).", "publication_ref": ["b22", "b1", "b1", "b29", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "Describing Images in Relative Terms", "text": "The second application of relative attributes that we propose is that of describing novel images. The goal is to be able to relate any new example to other images according to different properties-whether its class happens to be familiar or not. This basic functionality would allow, for instance, the meaningful search example applications given in the introduction. (See recent work in [32] for other forms of image description based on object-action-scene tags.)\nDuring training, we are given a set of training images I = {i}, each represented by a feature-vector x i \u2208 R n , a list A = {a m } of M attributes along with O m = {(i, j)} s.t. i j and S m = {(i, j)} s.t. i \u223c j in relative strength of a m . 3 . We learn M ranking-functions as described in Section 3.1, and evaluate them on all training images in I.\nGiven a novel image j to be described, we evaluate all learnt ranking functions r m (x j ). For each attribute a m , we identify two reference images i and k from I that will be used to describe j via relative attributes. In principle, with a good ranking function any reference images could be informative. In our implementation, we adhere to the following guidelines. To avoid generating an overly precise description, we wish to select i and k such that they are not very similar to j in terms of attribute strength. However, to avoid trivial descriptions, they must not be too far from j, either.\nHence, we pick i and k such that i j and j k in strength of a m , and 1 8 th of the images in I lie between i and j, as well as between j and k. In the case of boundary conditions where no such i or k exist, i is chosen to be the image in I with the least strength of a m , and k is set to the image in I with the highest strength of a m . The image j can then be described in terms of all or a subset of the M attributes, relative to any identified pairs (i, k). Figure 7 shows an example description generated by our approach, as well as an illustration of selected pairs (i, k).\nWhile more elaborate analysis of the dataset distribution-and even psychophysics knowledge of the sensitivity of humans to change in different attributescould make the selection of reference images more effective, we employ this straightforward technique as a proof-of-concept and leave such analysis for future work.", "publication_ref": ["b31", "b2"], "figure_ref": ["fig_9"], "table_ref": []}, {"heading": "Experiments", "text": "We evaluate our approach on two datasets: (1) Outdoor Scene Recognition (OSR) Dataset [11] containing 2688 images from 8 categories. We use the 512-dimensional gist [11] descriptor as our image features.  1. Binary and relative attribute assignments used in our experiments. Note that none of the relative orderings violate the binary memberships. The OSR dataset includes images from the following categories: coast (C), forest (F), highway (H), inside-city (I), mountain (M), open-country (O), street (S) and tall-building (T). The 8 attributes shown above are listed in [11] as the properties subjects used to organize the images.The PubFig dataset includes images of: Alex Rodriguez (A), Clive Owen (C), Hugh Laurie (H), Jared Leto (J), Miley Cyrus (M), Scarlett Johansson (S), Viggo Mortensen (V) and Zac Efron (Z). The 11 attributes shown above are a subset of the attributes provided with the dataset. They were chosen for their simplicity, sufficient variation among the 8 categories, and to avoid redundancy (e.g. using Young instead of Old, Middle Aged, Youth, Child). each). We use a concatenation of the gist descriptor and a 45-dimensional Lab color histogram as our image features.\nT\u227aI\u223cS\u227aH\u227aC\u223cO\u223cM\u223cF open 0 0 0 1 1 1 1 0 T\u223cF\u227aI\u223cS\u227aM\u227aH\u223cC\u223cO perspective 1 1 1 1 0 0 0 0 O\u227aC\u227aM\u223cF\u227aH\u227aI\u227aS\u227aT large-objects 1 1 1 0 0 0 0 0 F\u227aO\u223cM\u227aI\u223cS\u227aH\u223cC\u227aT diagonal-plane 1 1 1 1 0 0 0 0 F\u227aO\u223cM\u227aC\u227aI\u223cS\u227aH\u227aT close-depth 1 1 1 1 0 0 0 1 C\u227aM\u227aO\u227aT\u223cI\u223cS\u223cH\u223cF PubFig ACHJ MS V Z Masculine-looking 1 1 1 1 0 0 1 1 S\u227aM\u227aZ\u227aV\u227aJ\u227aA\u227aH\u227aC White 0 1 1 1 1 1 1 1 A\u227aC\u227aH\u227aZ\u227aJ\u227aS\u227aM\u227aV Young 0 0 0 0 1 1 0 1 V\u227aH\u227aC\u227aJ\u227aA\u227aS\u227aZ\u227aM Smiling 1 1 1 0 1 1 0 1 J\u227aV\u227aH\u227aA\u223cC\u227aS\u223cZ\u227aM Chubby 1 0 0 0 0 0 0 0 V\u227aJ\u227aH\u227aC\u227aZ\u227aM\u227aS\u227aA Visible-forehead 1 1 1 0 1 1 1 0 J\u227aZ\u227aM\u227aS\u227aA\u223cC\u223cH\u223cV Bushy-eyebrows 0 1 0 1 0 0 0 0 M\u227aS\u227aZ\u227aV\u227aH\u227aA\u227aC\u227aJ Narrow-eyes 0 1 1 0 0 0 1 1 M\u227aJ\u227aS\u227aA\u227aH\u227aC\u227aV\u227aZ Pointy-nose 0 0 1 0 0 0 0 1 A\u227aC\u227aJ\u223cM\u223cV\u227aS\u227aZ\u227aH Big-lips 1 0 0 0 1 1 0 0 H\u227aJ\u227aV\u227aZ\u227aC\u227aM\u227aA\u227aS Round-face 1 0 0 0 1 1 0 0 H\u227aV\u227aJ\u227aC\u227aZ\u227aA\u227aS\u227aM Table\nTable 1 provides more details about the datasets, and shows the binary memberships and relative orderings of categories by attributes. These were collected using the judgements of a colleague unfamiliar with the details of this work. We see the limitation of binary attributes in distinguishing between some categories, while the same set of attributes used relatively tease them apart. Although we have a full ordering, in our experiments we sample random pairs of categories as supervision (as noted below). Recall that different pairs of categories can be related for different attributes. Note that we collect the binary supervision only to train baseline approaches; our approach uses only the relative supervision.\nAs a sanity check, we first demonstrate the superiority of our learnt ranks to capture relative orderings, as compared to an approach that treats the score of binary classifiers as a rank (Section 4.1). Then we evaluate the use of relative attributes for the two new tasks (Sections 4.2 and 4.3).", "publication_ref": ["b10", "b10", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Learned Ranking vs. Classifier Scores", "text": "We train a binary linear SVM h m by transferring the binary supervision listed in Table 1 to the training images for each attribute. For an image-pair (i, j) in a held-out test set (2648 images for OSR, 560 for PubFig), we evaluate the learnt classifier, and if h m (x i ) > h m (x j ) we predict i j, else i \u227a j for a m . For comparison, we learn a linear ranking function r m for each attribute using the relative constraints in Table 1, and compare r m (x i ) to r m (x j ) on the same test pairs. Both methods' predictions are then compared to the ground-truth relative ordering.\nThe learnt ranking function's accuracy is 89% and 82% on the OSR and PubFig datasets, respectively, as compared to 80% and 67% if using the binary classifier scores, confirming the advantage of a ranking function to effectively capture relative information.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Zero-Shot Learning Results", "text": "We compare our zero-shot approach to two baselines: Baselines: Our first baseline is the Direct Attribute Prediction (DAP) model of Lampert et al. [2], which uses binary attribute descriptions for all categories. We train linear SVMs by transferring the binary supervision in Table 1 to training images from the seen categories. A test image x is assigned to a category using\nc * = argmax c\u2208{1,...,N } M m=1 P (a m = b c m | x),(13)\nwhere\nP (a m = b c m | x)\nis computed by transforming the binary classifier score via a sigmoid function, and b c m is the ground-truth binary bit taken by attribute a m for class c as seen in Table 1. If a m is not used to describe an unseen category, P (a m = b c m | x) is uniform (0.5). We call our second baseline \"score-based relative attributes\" (SRA). It follows the same approach as in Section 3.2, except that it replaces rank values with the binary classifier output score. It is a stronger baseline than DAP, as it has the same benefits of the generative modeling of seen classes and relative descriptions of unseen classes as our approach. It is not limited by the binary description of the categories, which may be deprived as seen in Table 1.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "Set up:", "text": "We compare all methods in several different scenarios. Unless specified, we use 2 unseen and 6 seen categories. To train the ranking functions, we use 4 categorypairs among seen categories, and unseen categories are described relative to the two closest seen categories for each attribute (one stronger, one weaker). We use 30 training images per class, and the rest for testing, and report mean perclass accuracy over 10 random train/test and seen/unseen splits.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proportion of unseen categories:", "text": "We first study zeroshot learning accuracy as the proportion of unseen categories increases. Figure 3 shows the results.\nFirst, we see even when all 8 categories are seen (0 unseen), our approach significantly outperforms both baselines. This validates the power of relative attributes for the classical recognition task. Also, SRA's gains over DAP with  Further, as we would expect, accuracy for all three approaches decreases with more unseen categories. However, our method remains better than the baselines for most of the spectrum, until only 3 seen categories remain, at which point it performs similarly to SRA. This is expected, since beyond that with only 2 seen categories, the relative and binary supervision becomes equivalent. Both still compare favorably to DAP due to the benefit of relative description.\nIn general, we can expect that with even more total categories, the description power of relative attributes will also increase, as unseen categories would have more categories to be related to (even with a fixed number of attributes). A binary description, on the other hand, can only lose discriminative power as more categories are added.", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Amount of supervision:", "text": "We next study the impact of varying the amount of supervision.\nFigure 4 shows the results as we increase the number of pairs of seen categories used to generate relative constraints, where for each attribute we randomly select the category pairs to be related from all 6 2 possibilities. Our performance is quite robust to the number and choice of pairs; as few as two pairs suffice. 4 When using only one pair, our method receives significantly less supervision than the two baselines, for which all six categories are labeled (hence their flat curves). In spite of this, our approach performs favorably on OSR, though suffers compared to SRA on PubFig.   Figure 5 shows the results as we decrease the number of attributes used to describe the unseen category during training. Note that the number of attributes used to describe the seen categories during training remains the same (see item 4 in Sec. 3.2). The accuracy of all methods degrades; however, the approaches using relative attributes (SRA and ours) decay gracefully, whereas DAP suffers more dramatically. This illustrates how each attribute conveys stronger distinguishing power when used relatively. This is a key result. This scenario exemplifies the high level of flexibility in supervision of unseen categories that our approach enables, which is crucial for practical applications.", "publication_ref": ["b5", "b3"], "figure_ref": ["fig_5", "fig_6"], "table_ref": []}, {"heading": "Quality of supervision:", "text": "What happens if the relationships described for an unseen class are 'looser'? That is, what if the annotator relates it to seen classes whose attribute strengths are more distant, e.g., says \"Miley is younger than Vitto\" rather than \"Miley is younger than Scarlett\" (a person closer in age)? Ideally, the supervisor would have freedom to specify any reference categories; that is the most natural form of description, and does not require the supervisor to know the exhaustive list of seen categories. Thus, we next evaluate performance as we increase the number of relative ranks away (looseness) from the seen categories used to describe the unseen category. 5 Figure 6 shows the results. We see our approach is very robust to the looseness of the constraints. We attribute this to the power of relative attributes to jointly carve out regions in the space of attribute strengths corresponding to the unseen category. This makes the distance of the reference categories less relevant, as long as the relationships are correctly indicated.", "publication_ref": ["b4"], "figure_ref": ["fig_8"], "table_ref": []}, {"heading": "Describing Images Results", "text": "Next we demonstrate our approach to generate relative descriptions of novel images. To quantify their effectiveness, we perform a human subject study that pits the binary attribute baseline against our relative approach. Our method reports the properties predicted relative to reference images (see Sec. 3.3), while the baseline reports the predicted presence/absence of attributes only. The human subject must guess which image led to the auto-generated descriptions. To our knowledge, these are the first results to quantify how well algorithm-generated attribute descriptions can communicate to humans.\nWe recruited 18 subjects, only some familiar with vision. We randomly selected 20 PubFig and 10 OSR images. For each of the 30 test cases, we present the subject a description using three randomly selected attributes plus a multiple-choice set of three images, one of which is correct. The subject is asked to rank their guesses for which fits the description best. See Figure 8(a). To avoid bias, we divided the subjects into two groups; each group saw either the binary or the relative attributes, but not both. Further, we display reference images for either group's task, to help subjects understand the attribute meanings.\nFigure 8(b) shows the results. Subjects are significantly more likely to identify the correct image using our method's description, i.e., 48% vs. 34% in the first choice. This reinforces our claim that relative attributes can better capture the \"concept\" of the image, and suggests their real promise for improved guided search or interactive learning.\nWe note that we augmented the baseline's binary descriptions with prototype images (showing stark contrast of attribute presence), even though, unlike our approach, they are not an intrinsic part of the generated description. We suspect that subjects would perform even worse with purely textual binary descriptions. Thus, the human study is, if anything, generous to the baseline.\nOur approach can be used to generate purely textual descriptions as well, where an image is described relative to other categories instead of images. Figure 8 (c) shows examples. Here our method selects the categories to compare to such that at least 50% of the images in the category have an attribute strength larger than (less than) that computed for the image to be described. Echoing our quantitative results, we can qualitatively see that the relative descriptions are more precise and informative than the binary ones. More results can be found on the authors' websites.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We introduced relative attributes, which allow for a richer language of supervision and description than the commonly used categorical (binary) attributes. We presented two novel applications: zero-shot learning based on relationships and describing images relative to other images or categories. Through extensive experiments as well as a human subject study, we clearly demonstrated the advantages of our idea. Future work includes exploring more novel applications of relative attributes, such as guided search or interactive learning, and automatic discovery of relative attributes.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements:", "text": "We thank the subjects of our human studies for their time. This research is supported in part by NSF IIS-1065390, ONR ATL N00014-11-1-0105, and the Luce Foundation.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Describing Objects by their Attributes", "journal": "CVPR", "year": "2009", "authors": "A Farhadi; I Endres; D Hoiem; D A Forsyth"}, {"ref_id": "b1", "title": "Learning To Detect Unseen Object Classes by Between-Class Attribute Transfer", "journal": "CVPR", "year": "2009", "authors": "C H Lampert; H Nickisch; S Harmeling"}, {"ref_id": "b2", "title": "What Helps Where And Why? Semantic Relatedness for Knowledge Transfer", "journal": "CVPR", "year": "2010", "authors": "M Rohrbach; M Stark; G Szarvas; I Gurevych; B Schiele"}, {"ref_id": "b3", "title": "Automatic Attribute Discovery and Characterization from Noisy Web Data", "journal": "ECCV", "year": "2010", "authors": "T L Berg; A C Berg; J Shih"}, {"ref_id": "b4", "title": "Attribute and Simile Classifiers for Face Verification", "journal": "", "year": "2009", "authors": "N Kumar; A Berg; P Belhumeur; S K Nayar"}, {"ref_id": "b5", "title": "Learning Models for Object Recognition from Natural Language Descriptions", "journal": "BMVC", "year": "2009", "authors": "J Wang; K Markert; M Everingham"}, {"ref_id": "b6", "title": "Joint Learning of Visual Attributes", "journal": "", "year": "2009", "authors": "G Wang; D Forsyth"}, {"ref_id": "b7", "title": "A Discriminative Latent Model of Object Classes and Attributes", "journal": "ECCV", "year": "2010", "authors": "Y Wang; G Mori"}, {"ref_id": "b8", "title": "Interactively Building a Discriminative Vocabulary of Nameable Attributes. CVPR", "journal": "", "year": "2011", "authors": "D Parikh; K Grauman"}, {"ref_id": "b9", "title": "Zero-Shot Learning with Semantic Output Codes. NIPS", "journal": "", "year": "2009", "authors": "M Palatucci; D Pomerleau; G Hinton; T Mitchell"}, {"ref_id": "b10", "title": "Modeling the Shape of the Scene: a Holistic Representation of the Spatial Envelope", "journal": "", "year": "2001", "authors": "A Oliva; A Torralba"}, {"ref_id": "b11", "title": "Attribute and Simile Classifiers for Face Verification", "journal": "", "year": "2009", "authors": "N Kumar; A C Berg; P N Belhumeur; S K Nayar"}, {"ref_id": "b12", "title": "Learning Visual Attributes. NIPS", "journal": "", "year": "2007", "authors": "V Ferrari; A Zisserman"}, {"ref_id": "b13", "title": "Attribute Learning in Large-scale Datasets. Workshop on Parts and Attributes, ECCV", "journal": "", "year": "2010", "authors": "O Russakovsky; L Fei-Fei"}, {"ref_id": "b14", "title": "Attribute-centric Recognition for Crosscategory Generalization", "journal": "CVPR", "year": "2010", "authors": "A Farhadi; I Endres; D Hoiem"}, {"ref_id": "b15", "title": "Visual Recognition with Humans in the Loop. ECCV", "journal": "", "year": "2010", "authors": "S Branson; C Wah; B Babenko; F Schroff; P Welinder; P Perona; P ; S Belongie"}, {"ref_id": "b16", "title": "Semantic Label Sharing for Learning with Many Categories", "journal": "ECCV", "year": "2010", "authors": "R Fergus; H Bernal; Y Weiss; A Torralba"}, {"ref_id": "b17", "title": "What Does Classifying More Than 10,000 Image Categories Tell Us?", "journal": "ECCV", "year": "2010", "authors": "J Deng; A C Berg; K Li; L Fei-Fei"}, {"ref_id": "b18", "title": "Comparative Object Similarity for Improved Recognition with Few or No Examples", "journal": "CVPR", "year": "2010", "authors": "G Wang; D Forsyth; D Hoiem"}, {"ref_id": "b19", "title": "Beyond Nouns: Exploiting Prepositions and Comparative Adjectives for Learning Visual Classifiers. ECCV", "journal": "", "year": "2008", "authors": "A Gupta; L S Davis"}, {"ref_id": "b20", "title": "Beyond Active Noun Tagging: Modeling Contextual Interactions for Multi-Class Active Learning", "journal": "CVPR", "year": "2010", "authors": "B Siddiquie; A Gupta"}, {"ref_id": "b21", "title": "Optimizing Search Engines using Clickthrough Data", "journal": "KDD", "year": "2002", "authors": "T Joachims"}, {"ref_id": "b22", "title": "Learning to Rank: From Pairwise Approach to Listwise Approach. ICML", "journal": "", "year": "2007", "authors": "Z Cao; T Qin; T Liu; M Tsai; H Li"}, {"ref_id": "b23", "title": "Learning to Rank for Information Retrieval. Foundations and Trends in Information Retrieval", "journal": "", "year": "2009", "authors": "T Liu"}, {"ref_id": "b24", "title": "Learning to Re-Rank: Query-Dependent Image Re-Ranking Using Click Data", "journal": "WWW", "year": "2011", "authors": "V Jain; M Varma"}, {"ref_id": "b25", "title": "Multiple-Instance Ranking: Learning to Rank Images for Image Retrieval", "journal": "CVPR", "year": "2008", "authors": "Y Hu; M Li; N Yu"}, {"ref_id": "b26", "title": "Distance Metric Learning, with Application to Clustering with Side-Information", "journal": "", "year": "2002", "authors": "E P Xing; A Y Ng; M I Jordan; S Russell"}, {"ref_id": "b27", "title": "Learning Globally-Consistent Local Distance Functions for Shape-based Image retrieval and Classification. ICCV", "journal": "", "year": "2007", "authors": "A Frome; Y Singer; F Sha; J Malik"}, {"ref_id": "b28", "title": "Training a Support Vector Machine in the Primal. Neural Computation", "journal": "", "year": "2007", "authors": "O Chapelle"}, {"ref_id": "b29", "title": "A Bayesian Approach to Unsupervised One-Shot Learning of Object Categories ICCV", "journal": "", "year": "2003", "authors": "L Fei-Fei; R Fergus; P Perona"}, {"ref_id": "b30", "title": "A Shape-Based Object Class Model for Knowledge Transfer", "journal": "ICCV", "year": "2009", "authors": "M Stark; M Goesele; B Schiele"}, {"ref_id": "b31", "title": "Every Picture Tells a Story: Generating Sentences from Images. ECCV", "journal": "", "year": "2010", "authors": "A Farhadi; M Hejrati; M Sadeghi; P Young; C Rashtchian; J Hockenmaier; D Forsyth"}, {"ref_id": "b32", "title": "Rank Aggregation Methods for the Web", "journal": "WWW", "year": "2001", "authors": "C Dwork; R Kumar; M Naor; D Sivakumar"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "\u2208R M and M \u00d7 M covariance matrix \u03a3 (s) i from the ranking-scores of the training im-ages from class c (s)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "kare seen categories, then we set the m-th component of the mean \u00b5 im \u2212 d m , where d m is the average distance between the sorted mean ranking-scores \u00b5 (s) im 's of seen classes for attribute a m . It is reasonable to expect the unseen class to be as far from the specified seen class as other seen classes tend to be from each other. 3. Similarly, if c (u) j is described as c (u) j c (s) k , we set \u00b5 (u) jm to \u00b5 (s) im + d m . 4. If a m is not used to describe c (u) j , we set \u00b5 (u) jm to be the mean across all training image ranks for a m and the m-th diagonal entry of \u03a3 (u) j", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "(2) A subset of the Public Figure Face Database (PubFig)[12] containing 800 images from 8 random identities (100 images", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 3 .3Figure 3. Zero-shot learning performance as the proportion of unseen categories increases. Total number of classes N remains constant at 8.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 4 .4Figure 4. Zero-shot learning performance as more pairs of seen categories are related (i.e. labeled) during training. 0 unseen categories demonstrate the benefit of the generative modeling of the categories in SRA.Further, as we would expect, accuracy for all three approaches decreases with more unseen categories. However, our method remains better than the baselines for most of the spectrum, until only 3 seen categories remain, at which point it performs similarly to SRA. This is expected, since beyond that with only 2 seen categories, the relative and binary supervision becomes equivalent. Both still compare favorably to DAP due to the benefit of relative description.In general, we can expect that with even more total categories, the description power of relative attributes will also increase, as unseen categories would have more categories to be related to (even with a fixed number of attributes). A binary description, on the other hand, can only lose discriminative power as more categories are added.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 5 .5Figure 5. Zero-shot learning performance as fewer attributes are used to describe the unseen categories.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 6 .6Figure 6. Zero-shot learning performance as the unseen categories are described via looser relationships.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 7 .7Figure 7. Part of example description generated for left image by binary attribute baseline (middle) and our method (right). See text for details.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "r m (x i ) = w T m x i ,(1)", "formula_coordinates": [3.0, 131.48, 440.21, 154.88, 12.69]}, {"formula_id": "formula_1", "formula_text": "\u2200(i, j) \u2208 O m : w T m x i > w T m x j (2) \u2200(i, j) \u2208 S m : w T m x i = w T m x j .(3)", "formula_coordinates": [3.0, 102.5, 496.54, 183.86, 29.14]}, {"formula_id": "formula_2", "formula_text": "minimize 1 2 ||w T m || 2 2 + C \u03be 2 ij + \u03b3 2 ij (4) s.t. w T m x i \u2265 w T m x j + 1 \u2212 \u03be ij ; \u2200(i, j) \u2208 O m (5) |w T m x i \u2212 w T m x j | \u2264 \u03b3 ij ; \u2200(i, j) \u2208 S m (6) \u03be ij \u2265 0; \u03b3 ij \u2265 0.(7)", "formula_coordinates": [3.0, 55.11, 640.15, 231.25, 70.11]}, {"formula_id": "formula_3", "formula_text": "! $# /) 0) 1)", "formula_coordinates": [3.0, 469.01, 82.01, 54.81, 88.53]}, {"formula_id": "formula_4", "formula_text": "minimize 1 2 ||w T m || 2 2 + C \u03be 2 ij + \u03b3 2 ij (8) s.t. w T m (x i \u2212 x j ) \u2265 1 \u2212 \u03be ij ; \u2200(i, j) \u2208 O m (9) |w T m (x i \u2212 x j )| \u2264 \u03b3 ij ; \u2200(i, j) \u2208 S m (10) \u03be ij \u2265 0; \u03b3 ij \u2265 0, (11", "formula_coordinates": [3.0, 318.56, 285.6, 226.55, 70.11]}, {"formula_id": "formula_5", "formula_text": ")", "formula_coordinates": [3.0, 540.96, 346.39, 4.15, 8.64]}, {"formula_id": "formula_6", "formula_text": "(u) j can be described as c (s) i c (u) j c (s) k for attribute a m , or c (s) i c (u) j , or c (u) j c (s) k , where c (s) i and c (s)", "formula_coordinates": [4.0, 50.11, 284.46, 236.25, 45.37]}, {"formula_id": "formula_7", "formula_text": "(s) i and c (s) j , c (s) i c (s) j =\u21d2 i j; \u2200i \u2208 c (s) i , \u2200j \u2208 c (s) j", "formula_coordinates": [4.0, 54.42, 545.35, 231.44, 14.07]}, {"formula_id": "formula_8", "formula_text": "(s) i \u223c N (\u00b5 (s) i , \u03a3 (s) i ), for i = 1, . . . , S.", "formula_coordinates": [4.0, 308.86, 72.47, 236.25, 23.83]}, {"formula_id": "formula_9", "formula_text": "1. If c (u) j is described as c (s) i c (u) j c (s) k ,", "formula_coordinates": [4.0, 316.33, 156.5, 183.15, 14.3]}, {"formula_id": "formula_10", "formula_text": "(u) j = 1 S S i=1 \u03a3 (s) i .", "formula_coordinates": [4.0, 465.2, 371.76, 79.79, 14.73]}, {"formula_id": "formula_11", "formula_text": "c * = argmax j\u2208{1,...,N } P (x i | \u00b5 j , \u03a3 j ) . (12", "formula_coordinates": [4.0, 360.2, 449.9, 180.76, 19.06]}, {"formula_id": "formula_12", "formula_text": ")", "formula_coordinates": [4.0, 540.96, 452.29, 4.15, 8.64]}, {"formula_id": "formula_13", "formula_text": "T\u227aI\u223cS\u227aH\u227aC\u223cO\u223cM\u223cF open 0 0 0 1 1 1 1 0 T\u223cF\u227aI\u223cS\u227aM\u227aH\u223cC\u223cO perspective 1 1 1 1 0 0 0 0 O\u227aC\u227aM\u223cF\u227aH\u227aI\u227aS\u227aT large-objects 1 1 1 0 0 0 0 0 F\u227aO\u223cM\u227aI\u223cS\u227aH\u223cC\u227aT diagonal-plane 1 1 1 1 0 0 0 0 F\u227aO\u223cM\u227aC\u227aI\u223cS\u227aH\u227aT close-depth 1 1 1 1 0 0 0 1 C\u227aM\u227aO\u227aT\u223cI\u223cS\u223cH\u223cF PubFig ACHJ MS V Z Masculine-looking 1 1 1 1 0 0 1 1 S\u227aM\u227aZ\u227aV\u227aJ\u227aA\u227aH\u227aC White 0 1 1 1 1 1 1 1 A\u227aC\u227aH\u227aZ\u227aJ\u227aS\u227aM\u227aV Young 0 0 0 0 1 1 0 1 V\u227aH\u227aC\u227aJ\u227aA\u227aS\u227aZ\u227aM Smiling 1 1 1 0 1 1 0 1 J\u227aV\u227aH\u227aA\u223cC\u227aS\u223cZ\u227aM Chubby 1 0 0 0 0 0 0 0 V\u227aJ\u227aH\u227aC\u227aZ\u227aM\u227aS\u227aA Visible-forehead 1 1 1 0 1 1 1 0 J\u227aZ\u227aM\u227aS\u227aA\u223cC\u223cH\u223cV Bushy-eyebrows 0 1 0 1 0 0 0 0 M\u227aS\u227aZ\u227aV\u227aH\u227aA\u227aC\u227aJ Narrow-eyes 0 1 1 0 0 0 1 1 M\u227aJ\u227aS\u227aA\u227aH\u227aC\u227aV\u227aZ Pointy-nose 0 0 1 0 0 0 0 1 A\u227aC\u227aJ\u223cM\u223cV\u227aS\u227aZ\u227aH Big-lips 1 0 0 0 1 1 0 0 H\u227aJ\u227aV\u227aZ\u227aC\u227aM\u227aA\u227aS Round-face 1 0 0 0 1 1 0 0 H\u227aV\u227aJ\u227aC\u227aZ\u227aA\u227aS\u227aM Table", "formula_coordinates": [5.0, 308.86, 93.56, 227.96, 178.27]}, {"formula_id": "formula_14", "formula_text": "c * = argmax c\u2208{1,...,N } M m=1 P (a m = b c m | x),(13)", "formula_coordinates": [6.0, 89.99, 320.09, 196.37, 30.2]}, {"formula_id": "formula_15", "formula_text": "P (a m = b c m | x)", "formula_coordinates": [6.0, 77.62, 360.34, 73.37, 12.2]}], "doi": ""}