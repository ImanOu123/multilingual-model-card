{"title": "Hilbert Space Embeddings of Hidden Markov Models", "authors": "Le Song; Sajid Siddiqi; Geoffrey Gordon; Alex Smola", "pub_date": "", "abstract": "Hidden Markov Models (HMMs) are important tools for modeling sequence data. However, they are restricted to discrete latent states, and are largely restricted to Gaussian and discrete observations. And, learning algorithms for HMMs have predominantly relied on local search heuristics, with the exception of spectral methods such as those described below. We propose a nonparametric HMM that extends traditional HMMs to structured and non-Gaussian continuous distributions. Furthermore, we derive a localminimum-free kernel spectral algorithm for learning these HMMs. We apply our method to robot vision data, slot car inertial sensor data and audio event classification data, and show that in these applications, embedded HMMs exceed the previous state-of-the-art performance.", "sections": [{"heading": "Introduction", "text": "Hidden Markov Models (HMMs) have successfully modeled sequence data in a wide range of applications including speech recognition, analysis of genomic sequences, and analysis of time series. HMMs are latent variable models of dynamical systems: they assume a latent state which evolves according to Markovian dynamics, as well as observations which depend only on the hidden state at a particular time (Figure 1).\nDespite their simplicity and wide applicability, HMMs are limited in two major respects: first, they are usually restricted to discrete or Gaussian observations, and second, the latent state variable is usually restricted to have only moderate cardinality. For non-Gaussian continuous observations, and for structured observations with large cardinalities, standard inference algorithms for HMMs run into trouble: we need huge numbers of latent states to capture such observation distributions accurately, and marginalizing out these states during inference can be very computationally intensive. Furthermore, standard HMM learning algorithms are not able to fit the required transition and observation distributions accurately: local search heuristics, such as the EM algorithm (Rabiner, 1989), lead to bad local optima, and standard approaches to regularization result in under-or overfitting.\nRecently, (Hsu et al., 2009) proposed a spectral algorithm for learning HMMs with discrete observations and hidden states. At its core, the algorithm performs a singular value decomposition of a matrix of joint probabilities of past and future observations, and then uses the result, along with additional matrices of joint probabilities, to recover parameters which allow tracking or filtering. The algorithm employs an observable representation of a HMM, and avoids explicitly recovering the HMM transition and observation matrices. This implicit representation enables the algorithm to find a consistent estimate of the distribution of observation sequences, without resorting to local search.\nUnfortunately, this spectral algorithm is only formulated for HMMs with discrete observations. In contrast, many sources of sequential data are continous or structured; the spectral algorithm does not apply to such data without discretization and flattening. So, the goal of the current paper is to provide a new kernelbased representation and kernelized spectral learning algorithm for HMMs; this new representation and algorithm will allow us to learn HMMs in any domain where we can define a kernel. Furthermore, our algorithm is free of local minima and admits finite-sample generalization guarantees.\nIn particular, we will represent HMMs using a recent concept called Hilbert space embedding (Smola et al., 2007;Sriperumbudur et al., 2008). The essence of Hilbert space embedding is to represent probability measures (in our case, corresponding to distributions over observations and latent states in a HMM) as points in Hilbert spaces. We can then perform inference in the HMM by updating these points, entirely in their Hilbert spaces, using covariance operators (Baker, 1973;Fukumizu et al., 2007) and conditional embedding operators (Song et al., 2009). By making use of the Hilbert space's metric structure, our method works naturally with continous and structured random variables, without the need for discretization.\nIn addition to generalizing HMMs to arbitary domains where kernels are defined, our learning algorithm contributes to the theory of Hilbert space embeddings with hidden variables. Previously, (Song et al., 2009) derived a kernel algorithm for HMMs; however, they only provided results for fully observable models, where the training data includes labels for the true latent states. By contrast, our algorithm only requires access to an (unlabeled) sequence of observations.\nWe provide experimental results comparing embedded HMMs learned by our spectral algorithm to several other well-known approaches to learning models of time series data. The results demonstrate that our novel algorithm exceeds the previous state-of-the-art performance, often beating the next best algorithm by a substantial margin.", "publication_ref": ["b6", "b4", "b13", "b17", "b0", "b3", "b15", "b15"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Preliminaries", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Notation and HMMs", "text": "In this paper, we follow the convention that uppercase letters denote random variables (e.g. X t , H t ) and lowercase letters their instantiations (e.g. x t , h t ). We will use P to denote probability distribution in the dis-crete cases and density in the continuous cases. For matrices and vectors, we will use notation u = (u i ) i and C = (C ij ) ij to list their entries. Following (Hsu et al., 2009), we abbreviate a sequence (x 1 , . . . , x t ) by x 1:t , and its reverse (x t , . . . , x 1 ) by x t:1 . When we use a sequence as a subscript, we mean the product of quantities indexed by the sequence elements (e.g.\nA xt:1 = A xt . . . A x1 ).\nWe use 1 m to denote an m \u00d7 1 column of ones.\nA discrete HMM defines a probability distribution over sequences of hidden states, H t \u2208 {1, . . . , N }, and observations, X t \u2208 {1, . . . , M }. We assume N M , and let T \u2208 R N \u00d7N be the state transition probability matrix with T ij = P(H t+1 = i|H t = j), O \u2208 R M \u00d7N be the observation probability matrix with O ij = P(X t = i|H t = j), and \u03c0 \u2208 R N be the stationary state distribution with \u03c0 i = P(H t = i). The conditional independence properties of the HMM imply that T , O and \u03c0 fully characterize the probability distribution of any sequence of states and observations (Figure 1). (Jaeger, 2000) demonstrated that discrete HMMs can be formulated in terms of 'observation operators' A xt . Each A xt is a matrix of size N \u00d7 N with its ij-th entry defined as P(H t+1 = i|H t = j)P(X t = x t |H t = j), or in matrix notation,", "publication_ref": ["b4", "b5"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Observable representation for HMMs", "text": "A xt = T diag(O xt,1 , . . . , O xt,m ).\n(1)\nThen the probability of a sequence of observations, x 1:t , can be written as matrix operations,\nP(x 1:t ) = 1 N A xt . . . A x1 \u03c0 = 1 N A xt:1 \u03c0,(2)\nEssentially, each A xt incorporates information about one-step observation likelihoods and one-step hidden state transitions. The sequence of matrix multiplications in equation ( 2) effectively implements the marginalization steps for the sequence of hidden variables, H t+1:1 . Likewise, the predictive distribution for one-step future X t+1 given a history of observations can be written as a sequence of matrix multiplications,\n(P(X t+1 = i|x 1:t )) M i=1 \u221d OA xt:1 \u03c0 (3)\nThe drawback of the representations in (2) and ( 3) is that they requires the exact knowledge of the transition matrix T and observation matrix O, and neither quantity is available during training (since the latent states are usually not observable).\nA key observation concerning Equations ( 2) and ( 3) is that if we are only interested in the final quantity 1 N A xt:1 \u03c0 and OA xt:1 \u03c0, we may not need to recover the A xt s exactly. Instead, it will suffice to recover them up to some invertible transformation. More specifically, suppose that matrix S is invertible, we can define a set of new quantities, b 1 := S\u03c0, b \u221e := OS \u22121 , B x := SA x S \u22121 (4) and equivalently compute OA xt:1 \u03c0 by cancelling out all S during matrix multiplications, resulting in\nOA xt:1 \u03c0 = OS \u22121 SA xt S \u22121 . . . SA x1 S \u22121 (S\u03c0) = b \u221e B xt:1 b 1 (5)\nThe natural question is how to choose S such that b 1 , b \u221e and B x can be computed based purely on observation sequences, x 1:t . (Hsu et al., 2009) show that S = U O works, where U is the top N left singular vectors of the following joint probability matrix (assuming stationarity of the distribution):\nC 2,1 := (P(X t+1 = i, X t = j)) M i,j=1 .(6)\nFurthermore, b 1 , b \u221e and B x can also be computed based on observable quantities (assuming stationarity of the distribution),\nu 1 := (P(X t = i)) M i=1 ,(7)\nC 3,x,1 := (P(X t+2 = i, X t+1 = x, X t = j)) M i,j=1(8)\nwhich are the marginal probability vector of sequence singletons, and one slice of the joint probability matrix of sequence triples (i.e. a slice indexed by x from a 3-dimensional matrix). Finally, (Hsu et al., 2009\n) derived b 1 = U u, b \u221e = C 2,1 (U C 2,1 ) \u2020 (9\n)\nB x = (U C 3,x,1 )(U C 2,1 ) \u2020 .(10)", "publication_ref": ["b4", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "A spectral algorithm for learning HMMs", "text": "The spectral algorithm for learning HMMs proceeds by first estimating u 1 , C 2,1 and C 3,x,1 . Given a dataset of m i.i.d.\ntriples (x l 1 , x l 2 , x l 3 ) m l=1 from a HMM (su- perscripts index training examples), we estimat\u00ea u 1 = 1 m l=1:m \u03d5(x l 1 )(11)\nC 2,1 = 1 m l=1:m \u03d5(x 2 )\u03d5(x 1 ) (12) C 3,x,1 = 1 m l=1:m I[x l 2 = x]\u03d5(x l 3 )\u03d5(x l 1 )(13)\nwhere the delta function (or delta kernel) is defined as I[x l 2 = x] = 1 if x l 2 = x and 0 otherwise; and we have used 1-of-M representation for discrete variables. In this representation, \u03d5(x = i) is a vector of length M with all entries equal to zero except 1 at i-th position. For instance, if x = 2, then \u03d5(x) = (0, 1, 0, . . . , 0) . Furthermore, we note that\u0108 3,x,1 is not a single but a collection of matrices each indexed by an x. Effectively, the delta function I[x l 2 = x] partition the observation triples according to x, and each\u0108 3,x,1 only gets a fraction of the data for the estimation.\nNext, a 'thin' SVD is computed for\u0108 2,1 . Let its top N left singular vectors be\u00db , then the observable representation for the HMM (b 1 ,b \u221e andB x ) can be estimated by replacing the population quantities with their corresponding finite sample counterparts.\nA key feature of the algorithm is that it does not explicitly estimate the transition and observation models; instead it estimates a set of observable quantities that differ by an invertible transformation. The core part of the algorithm is a SVD which is local minimum free. Furthermore, (Hsu et al., 2009) also prove that under suitable conditions this spectral algorithm for HMMs efficiently estimates both the marginal and predictive distributions.", "publication_ref": ["b4"], "figure_ref": [], "table_ref": []}, {"heading": "Hilbert Space Embeddings of HMMs", "text": "The spectral algorithm for HMMs derived by (Hsu et al., 2009) is only formulated for discrete random variables. Based on their formulation, it is not clear how one can apply this algorithm to general cases with continuous and structured variables. For instance, a difficulty lies in estimating\u0108 3,x,1 . As we mentioned earlier, to estimate each\u0108 3,x,1 , we need to partition the observation triples according to x, and each\u0108 3,x,1 only gets a fraction of the data for the estimation. For continous observations, x can take infinite number of possibile values, which makes the partition estimator impractical. Alternatively, one can perform a Parzen window density estimation for continuous variables. However, further approximations are needed in order to make Parzen window compatible with this spectral algorithm (Siddiqi et al., 2009). In the following, we will derive a new presentation and a kernel spectral algorithm for HMMs using a recent concept called Hilbert space embeddings of distributions (Smola et al., 2007;Sriperumbudur et al., 2008). The essence of our method is to represent distributions as points in Hilbert spaces, and update these points entirely in the Hilbert spaces using operators (Song et al., 2009). This new approach avoids the need for partitioning the data making it applicable to any domain where kernels can be defined.", "publication_ref": ["b4", "b12", "b13", "b17", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Hilbert space embeddings", "text": "Let F be a reproducing kernel Hilbert space (RKHS) associated with kernel k(x, x ) := \u03d5(x), \u03d5(x ) F . Then for all functions f \u2208 F and x \u2208 X we have the reproducing property: f, \u03d5(x) F = f (x), i.e. the evaluation of function f at x can be written as an inner product. Examples of kernels include the Gaussian RBF kernel k(x, x ) = exp(\u2212s x \u2212 x 2 ), however kernel functions have also been defined on strings, graphs, and other structured objects (Sch\u00f6lkopf et al., 2004).\nLet P be the set of probability distributions on X , and X the random variable with distribution P \u2208 P. Following (Smola et al., 2007), we define the mapping of P \u2208 P to RKHS F,\n\u00b5 X := E X\u223cP [\u03d5(X)],(14)\nas the Hilbert space embedding of P or simply mean map. For all f \u2208 F, E X\u223cP [f (X)] = f, \u00b5 X F by the reproducing property. A characteristic RKHS is one for which the mean map is injective: that is, each distribution has a unique embedding (Sriperumbudur et al., 2008). This property holds for many commonly used kernels. For X = R d , this includes the Gaussian, Laplace, and B-spline kernels.\nAs a special case of the mean map, the marginal probability vector of a discrete variable X is a Hilbert space embedding, i.e. (P(\nX = i)) M i=1 = \u00b5 X .\nHere the kernel is the delta function k(x, x ) = I[x = x ], and the feature map is the 1-of-M representation for discrete variables (see section 2.3).\nGiven m i.i.d. observations x l m\nl=1 , an estimate of the mean map is straightforward:\n\u03bc X := 1 m m l=1 \u03d5(x l ) = 1 m \u03a51 m , where \u03a5 := (\u03d5(x 1 ), . . . , \u03d5(x m\n)) is a conceptual arrangement of feature maps into columns. Furthermore, this estimate computes an approximation within an error of O p (m \u22121/2 ) (Smola et al., 2007):\nTheorem 1 Assume k(x, x ) bounded. With probabil- ity 1 \u2212 \u03b4, \u03bc X \u2212 \u00b5 X F = O p (m \u22121/2 (log(1/\u03b4)) 1/2 ).", "publication_ref": ["b10", "b13", "b17", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Covariance operators", "text": "The covariance operator is a generalization of the covariance matrix. Given a joint distribution P(X, Y ) over two variables X on X and Y on Y 1 , the uncentered covariance operator C XY is (Baker, 1973) \nC XY := E XY [\u03d5(X) \u2297 \u03c6(Y )],(15)\nwhere \u2297 denotes tensor product. Alternatively, C XY can simply be viewed as an embedding of joint distribution P(X, Y ) using joint feature map \u03c8(x, y) := \u03d5(x) \u2297 \u03c6(y) (in tensor product RKHS G \u2297 F). For discrete variables X and Y with delta kernels on both domains, the covariance operator will coincide with the joint probability table, i.e. (P(\nX = i, Y = j) M i,j=1 = C XY (also see section 2.3).\nGiven m pairs of i.i.d. observations (x l , y l ) m l=1 , we denote by \u03a5 = \u03d5(x 1 ), . . . , \u03d5(x m ) and \u03a6 = \u03c6(y 1 ), . . . , \u03c6(y m ) . Conceptually, the covariance operator C XY can then be estimated as\u0108 XY = 1 m \u03a5\u03a6 . This estimate also computes an approximation within an error of O p (m \u22121/2 ) (as a corollary of Theorem 1):\nCorollary 2 Assume k(x, x ) and l(x, x ) bounded. With probability 1 \u2212 \u03b4, \u0108 XY \u2212 C XY F \u2297G = O p (m \u22121/2 (log(1/\u03b4)) 1/2 ).", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "Conditional embedding operators", "text": "By analogy with the embedding of marginal distributions, the conditional density P(Y |x) can also be represented as an RKHS element, \u00b5\nY |x := E Y |x [\u03c6(Y )].\nWe emphasize that \u00b5 Y |x now traces out a family of embeddings in G, with each element corresponding to a particular value of x. These conditional embeddings can be defined via a conditional embedding operator C Y |X : F \u2192 G (Song et al., 2009),\n\u00b5 Y |x = C Y |X \u03d5(x) := C Y X C \u22121 XX \u03d5(x). (16\n)\nFor discrete variables with delta kernels, conditional embedding operators correspond exactly to conditional probability tables (CPT), i.e. (P(Y = i|X = j)) M i,j=1 = C Y |X , and each individual conditional embedding corresponds to one column of the CPT, i.e. (P\n(Y = i|X = x)) M i=1 = \u00b5 Y |x . Given m i.i.d. pairs (x l , y l ) m l=1 from P(X, Y ), the conditional embedding operator can be estimated a\u015d C Y |X = \u03a6\u03a5 m ( \u03a5\u03a5 m + \u03bbI) \u22121 = \u03a6(K + \u03bbmI) \u22121 \u03a5 (17)\nwhere we have defined the kernel matrix K := \u03a5 \u03a5 with (i, j)th entry k(x i , x j ). Note that there is a regularizing parameter \u03bb, to avoid overfitting (in much the same way as is done for kernel canonical correlation analysis: see Fukumizu et al., 2007). Furthermore, we have (Song et al., 2009) Theorem 3 Assume k(x, x ) and l(y, y ) bounded, and \u03d5(x) in the range of\nC XX . With probability 1 \u2212 \u03b4, \u03bc Y |x \u2212 \u00b5 Y |x G = O p (\u03bb 1/2 + (\u03bbm) \u22121/2 (log(1/\u03b4)) 1/2 ).", "publication_ref": ["b15", "b3", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Hilbert space observable representation", "text": "We will focus on the embedding \u00b5 Xt+1|x1:t for the predictive density P(X t+1 |x 1:t ) of a HMM. Analogue to the discrete case, we first express \u00b5 Xt+1|x1:t as a set of Hilbert space 'observable operators' A x . Specifically, let the kernels on the observations and hidden states be k(x, x ) = \u03d5(x), \u03d5(x ) F and l(h, h ) = \u03c6(h), \u03c6(h ) G respectively. For rich RKHSs, we define a linear operator\nA x : G \u2192 G such that A x \u03c6(h t ) = P(X t = x|h t ) E Ht+1|ht [\u03c6(H t+1 )]. (18\n)\nThen, by applying variable elimination, we have\n\u00b5 Xt+1|x1:t = E Ht+1|x1:t E Xt+1|Ht+1 [\u03d5(X t+1 )] = C Xt+1|Ht+1 E Ht+1|x1:t [\u03c6(H t+1 )] = C Xt+1|Ht+1 A xt E Ht|x1:t\u22121 [\u03c6(H t )] = C Xt+1|Ht+1 t \u03c4 =1 A x\u03c4 \u00b5 H1 . (19\n)\nwhere we used the following recursive relation\nE Ht+1|x1:t [\u03c6(H t+1 )] = E Ht|x1:t\u22121 P(X t = x t |H t ) E Ht+1|Ht [\u03c6(H t+1 )] = A xt E Ht|x1:t\u22121 [\u03c6(H t )] .(20)\nIf we let T := C Xt|Ht , O := C Xt+1|Ht+1 and \u03c0 := \u00b5 H1 , we obtain a form \u00b5 Xt+1|x1:t = OA xt:1 \u03c0 analogous to the discrete case (Equation ( 3)). The key difference is that Hilbert space representations are applicable to general domains with kernels defined.\nSimilar to the discrete case, the operators A x cannot be directly estimated from the data since the hidden states are not provided. Therefore we derive a representation for \u00b5 Xt+1|x1:t based only on observable quantities (assuming stationarity of the distribution):\n\u00b5 1 := E Xt [\u03d5(X t )] = \u00b5 Xt (21) C 2,1 := E Xt+1Xt [\u03d5(X t+1 ) \u2297 \u03d5(X t )] = C Xt+1Xt (22) C 3,x,1 := E Xt+2(Xt+1=x)Xt [\u03d5(X t+2 ) \u2297 \u03d5(X t )] = P(X t+1 = x)C 3,1|2 \u03d5(x). (23\n)\nwhere we have defined C 3,1|2 := C Xt+2Xt|Xt+1 . First, we examine the relation between these observable quantities and the unobserved O, T and \u03c0:\n\u00b5 1 = E Ht E Xt|Ht [\u03d5(X t )] = C Xt|Ht E Ht [\u03c6(H t )] = O\u03c0 (24) C 2,1 = E Ht E Xt+1Ht+1|Ht [\u03d5(X t+1 )] \u2297 E Xt|Ht [\u03d5(X t )] =C Xt+1|Ht+1 C Ht+1|Ht C HtHt C Xt|Ht = OT C HtHt O (25) C 3,x,1 = E Ht OA x T \u03c6(H t ) \u2297 E Xt|Ht [\u03d5(X t )] = OA x T C HtHt O(26)\nIn ( 26), we plugged in the following expansion\nE Xt+2Ht+2Ht+1(Xt+1=x)|Ht [\u03d5(X t+2 )] = E Ht+1|Ht P(x|H t+1 )E Ht+2|Ht+1 E Xt+2|Ht+2 [\u03d5(X t+2 )] = OA x T \u03c6(H t ) (27)\nSecond, analogous to the discrete case, we perform a 'thin' SVD of the covariance operator C 2,1 , and take its top N left singular vectors U, such that the operator U O is invertible. Some simple algebraic manipulations establish the relation between observable and unobservable quantities\n\u03b2 1 := U \u00b5 1 = (U O)\u03c0 (28\n)\n\u03b2 \u221e := C 2,1 (U C 2,1 ) \u2020 = O(U O) \u22121 (29\n)\nB x := (U C 3,x,1 )(U C 2,1 ) \u2020 = (UO)A x (UO) \u22121 . (30\n)\nWith \u03b2 1 , \u03b2 \u221e and B xt:1 , \u00b5 Xt+1|x1:t can be expressed as the multiplication of observable quantities\n\u00b5 Xt+1|x1:t = \u03b2 \u221e B xt:1 \u03b2 1 (31)\nIn practice, C 3,x,1 (in equation ( 26)) is difficult to estimate, since it requires partitioning the training samples according to X t+1 = x. Intead, we use C 3,1|2 \u03d5(x) which does not require such partitioning, and is only a fixed multiplicative scalar P(x) away from C 3,x,1 . As opposed to B x in (30), if we defin\u0113\nB x := (U (C 3,1|2 \u03d5(x)))(U C 2,1 ) \u2020 (32\n)\nwe have \u00b5 Xt+1|x1:t \u221d \u03b2 \u221eBxt:1 \u03b2 1 .\nWe may want to predict i steps into future, i.e. obtain embeddings \u00b5 Xt+i|xt:1 instead of \u00b5 Xt+1|xt:1 . This can be achieved by defining an i-step covariance operator C i+1,1 := E Xt+iXt [\u03d5(X t+i ) \u2297 \u03d5(X t )] and replacing C 2,1 in \u03b2 \u221e (equation ( 29)) by C i+1,1 . We then obtain the embedding \u00b5 Xt+i|xt:1 \u221d \u03b2 i \u221eBxt:1 \u03b2 1 where we used\n\u03b2 i \u221e to denote C i+1,1 (U C 2,1 ) \u2020 .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Kernel spectral algorithm for HMMs", "text": "Given a sample of m i.i.d.\ntriplets (x l 1 , x l 2 , x l 3 ) m l=1\nfrom a HMM, the kernel spectral algorithm for HMMs proceeds by first performing a 'thin' SVD of the sample covariance\u0108 2,1 . Specifically, we denote feature matrices \u03a5 = (\u03d5(x 1 1 ), . . . , \u03d5(x m 1 )) and \u03a6 = (\u03d5(x 1 2 ), . . . , \u03d5(x m 2 )), and estimate\u0108 2,1 = 1 m \u03a6\u03a5 . Then the left singular vector v = \u03a6\u03b1 (\u03b1 \u2208 R m ) can be estimated in a similar way to kernel Principal Component Analysis (Sch\u00f6lkopf et al., 1998):\n\u03a6\u03a5 \u03a5\u03a6 v = \u03c9v \u21d4 \u03a6KL\u03b1 = \u03c9\u03a6\u03b1 \u21d4 LKL\u03b1 = \u03c9L\u03b1, (\u03b1 \u2208 R m , \u03c9 \u2208 R)(33)\nwhere K = \u03a5 \u03a5 and L = \u03a6 \u03a6 are the kernel matrices, and \u03b1 is the generalized eigenvector. After normalization, we have\nv = 1 \u221a \u03b1 L\u03b1\n\u03a6\u03b1. Then the U operator in equation ( 28), ( 29) and ( 30) is the column concatenation of the N top left singular vectors, i.e.\u00db = (v 1 , . . . , v N ). If we let A := (\u03b1 1 , . . . , \u03b1 N ) \u2208 R m\u00d7N be the column concatenation of the N top \u03b1 i , and\nD := diag (\u03b1 1 L\u03b1 1 ) \u22121/2 , . . . , (\u03b1 N L\u03b1 N ) \u22121/2 \u2208 R N \u00d7N , we can concisely express\u00db = \u03a6AD.\nNext we estimate\u03bc 1 = 1 m \u03a51 m , and according to (28\n) \u03b2 1 = 1 m D A \u03a6 \u03a51 m . Similarly, according to (29) \u03b2 \u221e = 1 m \u03a6\u03a5 D A \u03a6 1 m \u03a6\u03a5 \u2020 = \u03a6KLAD\u2126 \u22121 ,\nwhere we have defined \u2126 := diag (\u03c9 1 , . . . , \u03c9 N ), and used the relation LKLA = LA\u2126 and\nA LA = D \u22122 . Last denote \u03a8 = \u03d5(x 1 3 ), . . . , \u03d5(x m 3 ) , then\u0108 3,1|2 (\u2022) = \u03a8 diag (L + \u03bbI) \u22121 \u03a6 (\u2022) KLAD\u2126 \u22121 in (32).\nThe kernel spectral algorithm for HMMs can be summarized in Algorithm 1. Note that in the algorithm, we assume that the marginal probability P(x \u03c4 ) (\u03c4 = 1 . . . t) is provided to the algorithm. In practice, this Algorithm 1 Kernel Spectral Algorithm for HMMs\nIn: m i.i.d. triples (x l 1 , x l 2 , x l 3 ) m l=1\n, a sequence x 1:t . Out:\u03bc Xt+1|xt:1 1: Denote feature matrices \u03a5 = (\u03d5(x 1 1 ), . . . , \u03d5(x m 1 )), \u03a6 = (\u03d5(x 1 2 ) . . . \u03d5(x m 2 )) and \u03a8 = (\u03d5(x 1 3 ) . . . \u03d5(x m 3 )). 2: Compute kernel matrices K = \u03a5 \u03a5, L = \u03a6 \u03a6, G = \u03a6 \u03a5 and F = \u03a6 \u03a8. 3: Compute top N generalized eigenvectors \u03b1 i using LKL\u03b1 i = \u03c9 i L\u03b1 i (\u03c9 i \u2208 R and \u03b1 i \u2208 R m ). 4: Denote A = (\u03b1 1 , . . . , \u03b1 N ), \u2126 = diag(\u03c9 1 , . . . , \u03c9 N )\nand\nD = diag (\u03b1 1 L\u03b1 1 ) \u22121/2 , . . . , (\u03b1 N L\u03b1 N ) \u22121/2 . 5:\u03b2 1 = 1 m D A G1 m 6:\u03b2 \u221e = \u03a6Q where Q = KLAD\u2126 \u22121 7:B x\u03c4 = P(x\u03c4 ) m D A F diag (L + \u03bbI) \u22121 \u03a6 \u03d5(x \u03c4 ) Q, for \u03c4 = 1, . . . , t. 8:\u03bc Xt+1|xt:1 =\u03b2 \u221eBxt:1\u03b21\nquantity is never explicitly estimated. Therefore, the algorithm returns\u03b2 \u221eBxt:1\u03b21 which is just a constant scaling away from \u00b5 Xt+1|xt:1 (noteB x := B x /P(x)).", "publication_ref": ["b9"], "figure_ref": [], "table_ref": []}, {"heading": "Sample complexity", "text": "In this section, we analyze the sample complexity of our kernel spectral algorithm for HMMs. In particular, we want to investigate how the difference between the estimated embedding\u03bc Xt+1|x1:t and its population counterpart scales with respect to the number m of training samples and the length t of the sequence x 1:t in the conditioning. We use Hilbert space distances as our error measure and obtain the following result (the proof follows the template of (Hsu et al., 2009), and it can be found in the appendix):\nTheorem 4 Assume \u03d5(x) F \u2264 1, \u03c6(h) G \u2264 1, max x A x 2 \u2264 1. Then \u00b5 Xt+1|x1:t \u2212\u03bc Xt+1|x1:t F = O p (t(\u03bb 1/2 + (\u03bbm) \u22121/2 (log(1/\u03b4)) 1/2 )) with probability 1 \u2212 \u03b4.\nThis result tells us that as we obtain more training data we can better estimate the conditional embedding. However, we expect that Theorem 4 can be further improved. Currently it suggests that given a sequence of length t, in order to obtain an unbiased estimator of \u00b5 Xt+1|x1:t , we need to decrease \u03bb with a schedule of O p (m \u22121/2 ) and obtain an overall convergence rate of O p (tm \u22121/4 ). Second, the assumption, max x A x 2 \u2264 1, imposes smoothness constrants on the likelihood function P(x|H t ) for the theorem to hold. Finally, the current bound depends on the length t of the conditioning sequence. (Hsu et al., 2009) provide a result that is independent of t using the KLdivergence as the error measure. For Hilbert space embeddings, it remains an open question as to how to estimate the KL-divergence and obtain a bound independent of t.\nx i + j i +1 x i x i \u22121 i \u2212k . . . x i +1 x i x \u22121 x i + j x i x i \u22121 i \u2212k . . . x i x i \u22121 \u22121 C 3,x,1 C 2,1 C future,x,past C future,past i ... x ... x ... x ...", "publication_ref": ["b4", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Predicting future observations", "text": "We have shown how to maintain the Hilbert space embeddings \u00b5 Xt+1|x1:t for the predictive distribution P(X t+1 |x 1:t ). The goal here is to determine the most probable future observations based on \u00b5 Xt+1|x1:t . We note that in general we cannot directly obtain the probability of the future observation based on the embedding presentation of the distribution. To obtain a density estimator based on \u00b5 Xt+1|x1:t , one can in principle fit an exponential family modelP(X t+1 |x 1:t ) \u221d exp( \u03b8, \u03d5(X t+1 ) that matches \u00b5 Xt+1|x1:t with E Xt+1\u223cP [\u03d5(X t+1 )]. In this case, an infinite dimensional parameter \u03b8 needs to be estimated from finite amount of data. Many problems, such as a practical consistent estimator, remain unsolved however (Fukumizu, 2009). However, for a Gaussian RBF kernel defined over a compact subset of a real vector space, the embedding \u00b5 Xt+1|x1:t can be viewed as a nonparametric density estimator after proper normalization. In particular, let f be a constant function in the RKHS such that f, \u03d5(X t+1 ) F = 1, then the normalization constant Z can be estimated as\u1e90 = f,\u03bc Xt+1|x1:t F . Sinc\u00ea \u00b5 Xt+1|x1:t is represented as\nm l=1 \u03b3 i \u03d5(x l\n3 ),\u1e90 is simply m l=1 \u03b3 i . We can then find the maximum a posteri (MAP) future observation b\u0177\nx t+1 = argmax xt+1 \u03bc Xt+1|x1:t , \u03d5(x t+1 ) F /\u1e90 (34)\nSince \u03d5(x) F = 1 for a Gaussian RBF kernel, a geometric interpretation of the above MAP estimate is to find a delta distribution \u03b4 xt+1 such that its embedding \u03d5(x t+1 ) is closest to\u03bc Xt+1|x1:t , i.e.x t+1 = argmin xt+1 \u03d5(x t+1 ) \u2212\u03bc Xt+1|x1:t F . The optimization in (34) may be a hard problem in general, and is analogous to the argmax operation in structured SVMs (Tsochantaridis et al., 2005). In some cases, however, it is possible to define the feature map \u03d5(x) in such a way that an efficient algorithm for solving the optimization can be obtained, e.g. Cortes et al. (2005). In practice, we can also decodex ", "publication_ref": ["b2", "b18", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Learning with sequences of observations", "text": "In the learning algorithm formulated above, each variable X t corresponds to a single observation x t from a data sequence. In this case, the operator C 2,1 only captures the dependence between a single past observation and a single future observation (similarly for C 3,x,1 ).\nIn system identification theory, this corresponds to assuming 1-step observability (Van Overschee & De Moor, 1996) which is unduly restrictive for many partially observable real-world dynamical systems of interest. More complex sufficient statistics of past and future may need to be modeled, such as the block Hankel matrix formulations for subspace methods (Van Overschee & De Moor, 1996), to identify linear systems that are not 1-step observable. To overcome this limitation one can consider sequences of observations in the past and future and estimate operators C f uture,past and C f uture,x,past accordingly (Figure 2). As long as past and future sequences never overlap, these matrices have rank equal to that of the dynamics model and the theoretical properties of the learning algorithm continue to hold (see (Siddiqi et al., 2009) for details).", "publication_ref": ["b19", "b19", "b12"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Experimental Results", "text": "We designed 3 sets of experiments to evaluate the effectiveness of learning embedded HMMs for difficult realworld filtering and prediction tasks. In each case we compare the learned embedded HMM to several alternative time series models including (I) linear dynamical systems (LDS) learned by Subspace Identification (Subspace ID) (Van Overschee & De Moor, 1996) with stability constraints (Siddiqi et al., 2008), (II) discrete HMMs learned by EM, and (III) the Reduced-rank HMM (RR-HMM) learned by spectral methods (Siddiqi et al., 2009). In these experiments we demonstrate that the kernel spectral learning algorithm for embedded HMMs achieves the state-of-the-art performance.", "publication_ref": ["b19", "b11", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Robot Vision Data", "text": "In this experiment, a video of 2000 frames was collected at 6 Hz from a Point Grey Bumblebee2 stereo camera mounted on a Botrics Obot d100 mobile robot platform circling a stationary obstacle (under imperfect human control) (Figure 3(A)) and 1500 frames were used as training data for each model. Each frame from the training data was reduced to 100 dimensions via SVD on single observations. The goal of this experiment was to learn a model of the noisy video, and, after filtering, to predict future image observations.\nWe trained a 50-dimensional 2 embedded HMM using Algorithm 1 with sequences of 20 consecutive observations (Section 3.8). Gaussian RBF kernels are used and the bandwidth parameter is set with the median of squared distance between training points (median trick). The regularization parameter \u03bb is set of 10 \u22124 . For comparison, a 50-dimensional RR-HMM with Parzen windows is also learned with sequences of 20 observations (Siddiqi et al., 2009); a 50-dimensional LDS is learned using Subspace ID with Hankel matrices of 20 time steps; and finally a 50-state discrete HMM and axis-aligned Gaussian observation models is learned using EM algorithm run until convergence.\nFor each model, we performed filtering 3 for different extents t 1 = 100, 101, . . . , 250, then predicted an image which was a further t 2 steps in the future, for t 2 = 1, 2..., 100. The squared error of this prediction in pixel space was recorded, and averaged over all the different filtering extents t 1 to obtain means which are plotted in Figure 3(B). As baselines, we also plot the error obtained by using the mean of filtered data as a predictor (Mean), and the error obtained by using the last filtered observation (Last).\nAny of the more complex algorithms perform better than the baselines (though as expected, the 'Last' predictor is a good one-step predictor), indicating that this is a nontrivial prediction problem. The embedded HMM learned by the kernel spectral algorithm yields significantly lower prediction error compared to each of the alternatives (including the recently published RR-HMM) consistently for the duration of the prediction horizon (100 timesteps, i.e. 16 seconds).", "publication_ref": ["b12"], "figure_ref": ["fig_2", "fig_2"], "table_ref": []}, {"heading": "Slot Car Inertial Measurement Data", "text": "In a second experiment, the setup consisted of a track and a miniature car (1:32 scale model) guided by a slot cut into the track. Figure 4 this data while the slot car circled the track controlled by a constant policy. The goal of this experiment was to learn a model of the noisy IMU data, and, after filtering, to predict future IMU readings.\nWe trained a 20-dimensional embedded HMM using Algorithm 1 with sequences of 150 consecutive observations (Section 3.8). The bandwidth parameter of the Gaussian RBF kernels is set with 'median trick'. The regularization parameter \u03bb is set of 10 \u22124 . For comparison, a 20-dimensional RR-HMM with Parzen windows is learned also with sequences of 150 observations; a 20-dimensional LDS is learned using Subspace ID with Hankel matrices of 150 time steps; and finally, a 20-state discrete HMM (with 400 level of discretization for observations) is learned using EM algorithm run until convergence.\nFor each model, we performed filtering for different extents t 1 = 100, 101, . . . , 250, then predicted an image which was a further t 2 steps in the future, for t 2 = 1, 2..., 100. The squared error of this prediction in the IMU's measurement space was recorded, and averaged over all the different filtering extents t 1 to obtain means which are plotted in Figure 4(B). Again the embedded HMM learned by the kernel spectral algorithm yields lower prediction error compared to each of the alternatives consistently for the duration of the prediction horizon.", "publication_ref": [], "figure_ref": ["fig_3", "fig_3"], "table_ref": []}, {"heading": "Audio Event Classification", "text": "Our final experiment concerns an audio classification task. The data, recently presented in (Ramos et al., 2010), consisted of sequences of 13-dimensional Mel-Frequency Cepstral Coefficients (MFCC) obtained from short clips of raw audio data recorded using a portable sensor device. Six classes of labeled audio clips were present in the data, one being Human Speech. For this experiment we grouped the latter five classes into a single class of Non-human sounds to formulate a binary Human vs. Non-human classification task. Since the original data had a disproportionately large amount of Human Speech samples, this grouping resulted in a more balanced dataset with 40 minutes 11 seconds of Human and 28 minutes 43 seconds of Non-human audio data. To reduce noise and training time we averaged the data every 100 timesteps (corresponding to 1 second) and downsampled.\nFor each of the two classes, we trained embedded HMMs with 10, 20, . . . , 50 latent dimensions using spectral learning and Gaussian RBF kernels with bandwidth set with the 'median trick'. The regularization parameter \u03bb is set at 10 \u22121 . For efficiency we used random features for approximating the kernel (Rahimi & Recht, 2008). For comparison, regular HMMs with axis-aligned Gaussian observation models, LDSs and RR-HMMs were trained using multi-restart EM (to avoid local minima), stable Subspace ID and the spectral algorithm of (Siddiqi et al., 2009) respectively, also with 10, . . . , 50 latent dimensions or states.\nFor RR-HMMs, regular HMMs and LDSs, the classconditional data sequence likelihood is the scoring function for classification. For embedded HMMs, the scoring function for a test sequence x 1:t is the log of the product of the compatibility scores for each observation, i.e. For each model size, we performed 50 random 2:1 partitions of data from each class and used the resulting datasets for training and testing respectively. The mean accuracy and 95% confidence intervals over these 50 randomizations are reported in Figure 5. The graph indicates that embedded HMMs have higher accuracy and lower variance than other standard alternatives at every model size. Though other learning algorithms for HMMs and LDSs exist, our experiment shows this to be a non-trivial sequence classification problem where embedded HMMs significantly outperform commonly used sequential models trained using typical learning and model selection methods.", "publication_ref": ["b8", "b7", "b12"], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Conclusion", "text": "We proposed a Hilbert space embedding of HMMs that extends traditional HMMs to structured and non-Gaussian continuous observation distributions. The essence of this new approach is to represent distributions as elements in Hilbert spaces, and update these elements entirely in the Hilbert spaces using operators. This allows us to derive a local-minimum-free kernel spectral algorithm for learning the embedded HMMs, which exceeds previous state-of-the-art in real world challenging problems. We believe that this new way of combining kernel methods and graphical models can potentially solve many other difficult problems in graphical models and advance kernel methods to more structured territory.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix: Proofs for Sample Complexity", "text": "Let k(x, x ) = \u03d5(x), \u03d5(x ) F and l(h, h ) = \u03c6(h), \u03c6(h ) G be the kernels for the observation variables X t and hidden states H t respectively. We assume that the feature map is bounded, i.e. \u03d5(x) F \u2264 1 and \u03c6(h) G \u2264 1. In this case, the norms of the conditional embeddings of H t are bounded by 1, i.e. by convexity\n\u00b5 Ht|xt\u22121:1 G = E Ht|xt\u22121:1 [\u03c6(H t )] G \u2264 E Ht|xt\u22121:1 [ \u03c6(H t ) G ] \u2264 1 (35)\nWe will use \u2022 2 to denote the spectral norm of an operator. We assume that the spectral norms of O and T are finite, i.e. O 2 \u2264 C O and T 2 \u2264 C T . Let\u00db be the column concatenation of the top N left singular vectors of\u0108 2,1 . We defin\u1ebd\n\u03b2 1 :=\u00db \u00b5 1 (36\n)\n\u03b2 \u221e := C 2,1 (\u00db C 2,1 ) \u2020 (37) B x := P(x) \u00db (C 3,1|2 \u03d5(x)) \u00db C 2,1 \u2020(38)\nWe further define\n\u03b4 1 := (\u00db O) \u22121 (\u03b2 1 \u2212\u03b2 1 ) G (39\n)\n\u03b4 \u221e := \u03b2 \u221e \u2212\u03b2 \u221e 2 (40) \u2206 := max xt (\u00db O) \u22121 (B xt \u2212B xt )(\u00db O) 2 (41) \u03b3 := max xt A xt 2 (we assume \u03b3 \u2264 1). (42\n)\nLemma 5 (\u00db O) \u22121 (B xt:1\u03b21 \u2212B xt:1\u03b21 ) G \u2264 (1 + \u2206) t \u03b4 1 + (1 + \u2206) t \u2212 1 Proof By induction on t. (\u00db O) \u22121 (\u03b2 1 \u2212\u03b2 1 ) G = \u03b4 1 = (1 + \u2206) 0 \u03b4 1 + (1 + \u2206) 0 \u2212 1 when t = 0.\nFor the induction step define\u03b2 t :=B xt\u22121:1\u03b21 and\u03b2 t :=B\nxt\u22121:1\u03b21 . Assume for t > 1:\n(\u00db O) \u22121 (\u03b2 t \u2212\u03b2 t ) G \u2264(1 + \u2206) t\u22121 \u03b4 1 + (1 + \u2206) t\u22121 \u2212 1 (43)\nThen, we have\n(\u00db O) \u22121 (B xt:1\u03b21 \u2212B xt:1\u03b21 ) G \u2264 (\u00db O) \u22121 (B xt \u2212B xt )(\u00db O) 2 (\u00db O) \u22121\u03b2 t G (44) + (\u00db O) \u22121 (B xt \u2212B xt )(\u00db O) 2 (\u00db O) \u22121 (\u03b2 t \u2212\u03b2 t ) G (45) + (\u00db O) \u22121B xt (\u00db O) 2 (\u00db O) \u22121 (\u03b2 t \u2212\u03b2 t ) G(46)\nFor ( 44), we have For ( 46), we have\n(\u00db O) \u22121 (B xt \u2212B xt )(\u00db O) 2 (\u00db O) \u22121\u03b2 t G \u2264\u2206 \u00b5 Ht|xt\u22121:1 G \u2264 \u2206 (47\n(\u00db O) \u22121B xt (\u00db O) 2 (\u00db O) \u22121 (\u03b2 t \u2212\u03b2 t ) G \u2264 A xt 2 (\u00db O) \u22121 (\u03b2 t \u2212\u03b2 t ) G \u2264(1 + \u2206) t\u22121 \u03b4 1 + (1 + \u2206) t\u22121 \u2212 1 (49)\nSumming the above three bounds, we have For ( 51), we have\n(\u03b2 \u221e \u2212\u03b2 \u221e )(\u00db O)(\u00db O) \u22121B xt:1\u03b21 F \u2264 (\u03b2 \u221e \u2212\u03b2 \u221e )(\u00db O) 2 (\u00db O) \u22121B xt:1\u03b21 G \u2264 \u03b2 \u221e \u2212\u03b2 \u221e 2 O 2 \u00b5 Ht+1|xt:1 G \u2264C O \u03b2 \u221e \u2212\u03b2 \u221e 2 \u2264 C O \u03b4 \u221e(54)\nFor ( 52), we have\n(\u03b2 \u221e \u2212\u03b2 \u221e )(\u00db O)(\u00db O) \u22121 (B xt:1\u03b21 \u2212B xt:1\u03b21 ) F \u2264 (\u03b2 \u221e \u2212\u03b2 \u221e )(\u00db O) 2 (\u00db O) \u22121 (B xt:1\u03b21 \u2212B xt:1\u03b21 ) G \u2264 \u03b2 \u221e \u2212\u03b2 \u221e 2 O 2 (1 + \u2206) t \u03b4 1 + (1 + \u2206) t \u2212 \u03b3 \u2264C O \u03b4 \u221e (1 + \u2206) t \u03b4 1 + (1 + \u2206) t \u2212 1 (55)\nFor (53), we have\n\u03b2 \u221e (\u00db O)(\u00db O) \u22121 (B xt:1\u03b21 \u2212B xt:1\u03b21 ) F \u2264 \u03b2 \u221e (\u00db O) 2 (\u00db O) \u22121 (B xt:1\u03b21 \u2212B xt:1\u03b21 ) G \u2264 OT 2 (1 + \u2206) t \u03b4 1 + (1 + \u2206) t \u2212 1 \u2264C O C T (1 + \u2206) t \u03b4 1 + (1 + \u2206) t \u2212 1(56)\nDefine:\n1 := \u00b5 1 \u2212\u03bc 1 F (57)\n2 := C 2,1 \u2212\u0108 2,1 F \u2297F(58)\n3 := max x \u00b5 3,1|x \u2212\u03bc 3,1|x F \u2297F (59)\nLemma 7 Suppose 2 \u2264 \u03b5\u03c3 N (C 2,1 ) for some \u03b5 \u2264 1/2. Let \u03b5 0 = 2 2 /((1 \u2212 \u03b5)\u03c3 N (C 2,1 )) 2 . Then:\n1. \u03b5 0 < 1, 2. \u03c3 n (\u00db \u0108 2,1 ) \u2265 (1 \u2212 \u03b5)\u03c3 N (C 2,1 ), 3. \u03c3 N (\u00db C 2,1 ) \u2265 \u221a 1 \u2212 \u03b5 0 \u03c3 N (C 2,1 ), 4. \u03c3 N (\u00db O) \u2265 \u221a 1 \u2212 \u03b5 0 \u03c3 N (O).\nProof This lemma can be proved by an extension of Lemma 9 in (Hsu et al., 2009). Lemma 8\n\u03b4 1 := (\u00db O) \u22121 (\u03b2 1 \u2212\u03b2 1 ) G \u2264 1 \u03c3 N (\u00db O) Proof (\u00db O) \u22121 (\u03b2 1 \u2212\u03b2 1 ) G = (\u00db O) \u22121 (\u00db \u00b5 1 \u2212\u00db \u03bc 1 ) G \u2264 (\u00db O) \u22121\u00db 2 \u00b5 1 \u2212\u03bc 1 F (60) \u2264 1 \u03c3 n (\u00db O)(61)\nLemma 9\n\u03b4 \u221e := \u03b2 \u221e \u2212 \u03b2 \u221e 2 \u2264 1+ \u221a 5 2 2 min{\u03c3 N (\u01082,1),\u03c3 N (\u00db C2,1)} 2 + 2 \u03c3 N (\u00db C2,1) Proof \u03b2 \u221e \u2212 \u03b2 \u221e 2 = C 2,1 (\u00db C 2,1 ) \u2020 \u2212\u0108 2,1 (\u00db \u0108 2,1 ) \u2020 2 \u2264 \u0108 2,1 ((\u00db C 2,1 ) \u2020 \u2212 (\u00db \u0108 2,1 ) \u2020 ) 2 + (C 2,1 \u2212\u0108 2,1 )(\u00db C 2,1 ) \u2020 2 \u2264 \u0108 2,1 2 ((\u00db C 2,1 ) \u2020 \u2212 (\u00db \u0108 2,1 ) \u2020 ) 2 + C 2,1 \u2212\u0108 2,1 2 (\u00db C 2,1 ) \u2020 2 \u2264 1 + \u221a 5 2 2 min{\u03c3 N (\u0108 2,1 ), \u03c3 N (\u00db C 2,1 )} 2 + 2 \u03c3 N (\u00db C 2,1 ) (62\n)\nLemma 10 \u2206 := max xt (\u00db O) \u22121 (B xt \u2212B xt )(\u00db O) 2 \u2264 1 \u03c3 N (\u00db O) 2 min{\u03c3 N (\u01082,1),\u03c3 N (\u00db C2,1)} 2 + 3 \u03c3 N (\u00db C2,1) Proof max xt (\u00db O) \u22121 (B xt \u2212B xt )(\u00db O) 2 \u2264 max xt 1 \u03c3 N (\u00db O) \u03bc 3,1|xt ((\u00db C 2,1 ) \u2020 \u2212 (\u00db \u0108 2,1 ) \u2020 ) 2 + (\u00b5 3,1|xt \u2212\u03bc 3,1|xt )(\u00db C 2,1 ) \u2020 2 \u2264 1 \u03c3 N (\u00db O) 2 min{\u03c3 N (\u0108 2,1 ), \u03c3 N (\u00db C 2,1 )} 2 + 3 \u03c3 N (\u00db C 2,1 )(63)\nBased on the properties of Hilbert space embeddings explained in the main text, we have\nLemma 11 With probability 1 \u2212 \u03b4,\n1 = \u00b5 1 \u2212\u03bc 1 F (64\n)\n= O p (m \u22121/2 (log(1/\u03b4)) 1/2 ) (65)\n2 = C 2,1 \u2212\u0108 2,1 F \u2297F(66)\n= O p (m \u22121/2 (log(1/\u03b4)) 1/2 ) (67)\n3 = max xt \u00b5 3,1|xt \u2212\u03bc 3,1|xt F \u2297F (68\n)\n= O p (\u03bb 1/2 + (\u03bbm) \u22121/2 (log(1/\u03b4)) 1/2 ). ( 69)\nPlugging these results, and we finally have Theorem 12 Assume k(x, x ) and l(h, h ) bounded, and max x A x 2 \u2264 1, then with probability 1 \u2212 \u03b4, \u00b5 Xt+1|xt:1 \u2212\u03bc Xt+1|xt:1 F = O p (\u03bb 1/2 + (\u03bbm) \u22121/2 (log(1/\u03b4)) 1/2 ).", "publication_ref": ["b4"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Joint measures and cross-covariance operators", "journal": "Transactions of the American Mathematical Society", "year": "1973", "authors": "C Baker"}, {"ref_id": "b1", "title": "A general regression techinque for learning transductions", "journal": "", "year": "2005", "authors": "C Cortes; M Mohri; J Weston"}, {"ref_id": "b2", "title": "Exponential manifold by reproducing kernel hilbert spaces", "journal": "Cambridge University Press", "year": "2009", "authors": "K Fukumizu"}, {"ref_id": "b3", "title": "Statistical consistency of kernel canonical correlation analysis", "journal": "J. Mach. Learn. Res", "year": "2007", "authors": "K Fukumizu; F Bach; A Gretton"}, {"ref_id": "b4", "title": "A spectral algorithm for learning hidden markov models", "journal": "", "year": "2009", "authors": "D Hsu; S Kakade; T Zhang"}, {"ref_id": "b5", "title": "Observable operator models for discrete stochastic time series", "journal": "Neural Computation", "year": "2000", "authors": "H Jaeger"}, {"ref_id": "b6", "title": "A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the", "journal": "IEEE", "year": "1989", "authors": "L R Rabiner"}, {"ref_id": "b7", "title": "Random features for largescale kernel machines", "journal": "MIT Press", "year": "2008", "authors": "A Rahimi; B Recht"}, {"ref_id": "b8", "title": "Automatic state discovery for unstructured audio scene classification", "journal": "", "year": "2010", "authors": "J Ramos; S Siddiqi; A Dubrawski; G Gordon; A Sharma"}, {"ref_id": "b9", "title": "Nonlinear component analysis as a kernel eigenvalue problem", "journal": "Neural Comput", "year": "1998", "authors": "B Sch\u00f6lkopf; A J Smola; K.-R M\u00fcller"}, {"ref_id": "b10", "title": "", "journal": "Kernel Methods in Computational Biology", "year": "2004", "authors": "B Sch\u00f6lkopf; K Tsuda; J.-P Vert"}, {"ref_id": "b11", "title": "A constraint generation approach to learning stable linear dynamical systems", "journal": "", "year": "2008", "authors": "S Siddiqi; B Boots; G Gordon"}, {"ref_id": "b12", "title": "Reduced-rank hidden markov models", "journal": "", "year": "2009", "authors": "S Siddiqi; B Boots; G Gordon"}, {"ref_id": "b13", "title": "", "journal": "", "year": "2007", "authors": "A Smola; A Gretton; L Song; B Sch\u00f6lkopf"}, {"ref_id": "b14", "title": "A Hilbert space embedding for distributions", "journal": "Springer", "year": "", "authors": ""}, {"ref_id": "b15", "title": "", "journal": "", "year": "2009", "authors": "L Song; J Huang; A Smola; K Fukumizu"}, {"ref_id": "b16", "title": "Hilbert space embeddings of conditional distributions", "journal": "", "year": "", "authors": ""}, {"ref_id": "b17", "title": "Injective Hilbert space embeddings of probability measures", "journal": "", "year": "2008", "authors": "B Sriperumbudur; A Gretton; K Fukumizu; G Lanckriet; B Sch\u00f6lkopf"}, {"ref_id": "b18", "title": "Large margin methods for structured and interdependent output variables", "journal": "J. Mach. Learn. Res", "year": "2005", "authors": "I Tsochantaridis; T Joachims; T Hofmann; Y Altun"}, {"ref_id": "b19", "title": "Subspace identification for linear systems: Theory, implementation, applications", "journal": "Kluwer", "year": "1996", "authors": "P Van Overschee; B De Moor"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. Graphical model for Hidden Markov Models.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 .2Figure 2. Operators C f uture,past and C f uture,x,past capture the dependence of sequences of k past and j future observations instead of single past and future observations.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 .3Figure 3. Robot vision data. (A) Sample images from the robot's camera. The figure below depicts the hallway environment with a central obstacle (black) and the path that the robot took through the environment (the red counterclockwise ellipse). (B) Squared error for prediction with different estimated models and baselines.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 .4Figure 4. Slot car inertial measurement data. (A) The slot car platform and the IMU (top) and the racetrack (bottom). (B) Squared error for prediction with different estimated models and baselines.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 .5Figure5. Accuracies and 95% confidence intervals for Human vs. Non-human audio event classification, comparing embedded HMMs to other common sequential models at different latent state space sizes.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "t\u03c4=1 log \u03d5(x \u03c4 ),\u03bc X\u03c4 |x1:\u03c4\u22121 F .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": ") \u22121 (B xt \u2212B xt )(\u00db O) 2 (\u00db O) \u22121 (\u03b2 t \u2212\u03b2 t ) G \u2264\u2206((1 + \u2206) t\u22121 \u03b4 1 + (1 + \u2206) t\u22121 \u2212 1)(48)", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "( 1 +F\u2264FF1\u2206) t \u03b4 1 + (1 + \u2206) t \u2212 (1 + \u2206) + \u2206 =(1 + \u2206) t \u03b4 1 + (1 + \u2206) t \u2212 1 (50) Lemma 6 \u00b5 Xt+1|xt:1 \u2212\u03bc Xt+1|xt:1 F \u2264 C O \u03b4 \u221e + (C O C T + C O \u03b4 \u221e ) ((1 + \u2206) t \u03b4 1 + (1 + \u2206) t \u2212 1)Proof Using triangle inequality\u00b5 Xt+1|xt:1 \u2212\u03bc Xt+1|xt:1 F = \u03b2 \u221eBxt:1\u03b21 \u2212\u03b2 \u221eBxt:1\u03b21 (\u03b2 \u221e \u2212\u03b2 \u221e )(\u00db O)(\u00db O) \u221e \u2212\u03b2 \u221e )(\u00db O)(\u00db O) \u22121 (B xt:1\u03b21 \u2212B xt:1\u03b21 ) (\u00db O)(\u00db O) \u22121 (B xt:1\u03b21 \u2212B xt:1\u03b21 )", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "A xt:1 = A xt . . . A x1 ).", "formula_coordinates": [2.0, 330.93, 153.91, 92.12, 9.65]}, {"formula_id": "formula_1", "formula_text": "A xt = T diag(O xt,1 , . . . , O xt,m ).", "formula_coordinates": [2.0, 356.13, 399.07, 136.62, 9.65]}, {"formula_id": "formula_2", "formula_text": "P(x 1:t ) = 1 N A xt . . . A x1 \u03c0 = 1 N A xt:1 \u03c0,(2)", "formula_coordinates": [2.0, 341.67, 453.25, 199.77, 10.65]}, {"formula_id": "formula_3", "formula_text": "(P(X t+1 = i|x 1:t )) M i=1 \u221d OA xt:1 \u03c0 (3)", "formula_coordinates": [2.0, 354.37, 570.59, 187.07, 14.11]}, {"formula_id": "formula_4", "formula_text": "OA xt:1 \u03c0 = OS \u22121 SA xt S \u22121 . . . SA x1 S \u22121 (S\u03c0) = b \u221e B xt:1 b 1 (5)", "formula_coordinates": [3.0, 62.41, 139.91, 227.03, 26.67]}, {"formula_id": "formula_5", "formula_text": "C 2,1 := (P(X t+1 = i, X t = j)) M i,j=1 .(6)", "formula_coordinates": [3.0, 96.35, 266.76, 193.09, 14.11]}, {"formula_id": "formula_6", "formula_text": "u 1 := (P(X t = i)) M i=1 ,(7)", "formula_coordinates": [3.0, 76.52, 325.77, 212.92, 14.11]}, {"formula_id": "formula_7", "formula_text": "C 3,x,1 := (P(X t+2 = i, X t+1 = x, X t = j)) M i,j=1(8)", "formula_coordinates": [3.0, 61.88, 343.11, 227.56, 14.11]}, {"formula_id": "formula_8", "formula_text": ") derived b 1 = U u, b \u221e = C 2,1 (U C 2,1 ) \u2020 (9", "formula_coordinates": [3.0, 55.44, 401.68, 234.0, 35.67]}, {"formula_id": "formula_9", "formula_text": ")", "formula_coordinates": [3.0, 285.2, 427.69, 4.24, 8.74]}, {"formula_id": "formula_10", "formula_text": "B x = (U C 3,x,1 )(U C 2,1 ) \u2020 .(10)", "formula_coordinates": [3.0, 99.64, 442.15, 189.81, 11.72]}, {"formula_id": "formula_11", "formula_text": "triples (x l 1 , x l 2 , x l 3 ) m l=1 from a HMM (su- perscripts index training examples), we estimat\u00ea u 1 = 1 m l=1:m \u03d5(x l 1 )(11)", "formula_coordinates": [3.0, 55.44, 493.66, 408.12, 46.11]}, {"formula_id": "formula_12", "formula_text": "C 2,1 = 1 m l=1:m \u03d5(x 2 )\u03d5(x 1 ) (12) C 3,x,1 = 1 m l=1:m I[x l 2 = x]\u03d5(x l 3 )\u03d5(x l 1 )(13)", "formula_coordinates": [3.0, 72.65, 544.18, 390.92, 36.44]}, {"formula_id": "formula_13", "formula_text": "\u00b5 X := E X\u223cP [\u03d5(X)],(14)", "formula_coordinates": [4.0, 129.47, 120.73, 159.97, 9.65]}, {"formula_id": "formula_14", "formula_text": "X = i)) M i=1 = \u00b5 X .", "formula_coordinates": [4.0, 143.92, 264.99, 79.49, 12.32]}, {"formula_id": "formula_15", "formula_text": "Given m i.i.d. observations x l m", "formula_coordinates": [4.0, 55.44, 316.4, 145.84, 12.7]}, {"formula_id": "formula_16", "formula_text": "\u03bc X := 1 m m l=1 \u03d5(x l ) = 1 m \u03a51 m , where \u03a5 := (\u03d5(x 1 ), . . . , \u03d5(x m", "formula_coordinates": [4.0, 56.64, 330.84, 232.8, 26.52]}, {"formula_id": "formula_17", "formula_text": "Theorem 1 Assume k(x, x ) bounded. With probabil- ity 1 \u2212 \u03b4, \u03bc X \u2212 \u00b5 X F = O p (m \u22121/2 (log(1/\u03b4)) 1/2 ).", "formula_coordinates": [4.0, 55.44, 397.2, 234.0, 23.13]}, {"formula_id": "formula_18", "formula_text": "C XY := E XY [\u03d5(X) \u2297 \u03c6(Y )],(15)", "formula_coordinates": [4.0, 111.77, 498.32, 177.67, 9.65]}, {"formula_id": "formula_19", "formula_text": "X = i, Y = j) M i,j=1 = C XY (also see section 2.3).", "formula_coordinates": [4.0, 55.44, 588.79, 234.0, 23.18]}, {"formula_id": "formula_20", "formula_text": "Corollary 2 Assume k(x, x ) and l(x, x ) bounded. With probability 1 \u2212 \u03b4, \u0108 XY \u2212 C XY F \u2297G = O p (m \u22121/2 (log(1/\u03b4)) 1/2 ).", "formula_coordinates": [4.0, 307.44, 70.19, 234.0, 33.59]}, {"formula_id": "formula_21", "formula_text": "Y |x := E Y |x [\u03c6(Y )].", "formula_coordinates": [4.0, 456.7, 153.84, 84.75, 9.95]}, {"formula_id": "formula_22", "formula_text": "\u00b5 Y |x = C Y |X \u03d5(x) := C Y X C \u22121 XX \u03d5(x). (16", "formula_coordinates": [4.0, 348.19, 227.39, 188.82, 13.31]}, {"formula_id": "formula_23", "formula_text": ")", "formula_coordinates": [4.0, 537.01, 229.62, 4.43, 8.74]}, {"formula_id": "formula_24", "formula_text": "(Y = i|X = x)) M i=1 = \u00b5 Y |x . Given m i.i.d. pairs (x l , y l ) m l=1 from P(X, Y ), the conditional embedding operator can be estimated a\u015d C Y |X = \u03a6\u03a5 m ( \u03a5\u03a5 m + \u03bbI) \u22121 = \u03a6(K + \u03bbmI) \u22121 \u03a5 (17)", "formula_coordinates": [4.0, 305.57, 308.15, 435.53, 60.67]}, {"formula_id": "formula_25", "formula_text": "C XX . With probability 1 \u2212 \u03b4, \u03bc Y |x \u2212 \u00b5 Y |x G = O p (\u03bb 1/2 + (\u03bbm) \u22121/2 (log(1/\u03b4)) 1/2 ).", "formula_coordinates": [4.0, 313.76, 465.99, 227.68, 24.15]}, {"formula_id": "formula_26", "formula_text": "A x : G \u2192 G such that A x \u03c6(h t ) = P(X t = x|h t ) E Ht+1|ht [\u03c6(H t+1 )]. (18", "formula_coordinates": [4.0, 328.28, 597.43, 208.73, 27.45]}, {"formula_id": "formula_27", "formula_text": ")", "formula_coordinates": [4.0, 537.01, 614.93, 4.43, 8.74]}, {"formula_id": "formula_28", "formula_text": "\u00b5 Xt+1|x1:t = E Ht+1|x1:t E Xt+1|Ht+1 [\u03d5(X t+1 )] = C Xt+1|Ht+1 E Ht+1|x1:t [\u03c6(H t+1 )] = C Xt+1|Ht+1 A xt E Ht|x1:t\u22121 [\u03c6(H t )] = C Xt+1|Ht+1 t \u03c4 =1 A x\u03c4 \u00b5 H1 . (19", "formula_coordinates": [4.0, 326.21, 649.92, 210.81, 65.99]}, {"formula_id": "formula_29", "formula_text": ")", "formula_coordinates": [4.0, 537.01, 701.78, 4.43, 8.74]}, {"formula_id": "formula_30", "formula_text": "E Ht+1|x1:t [\u03c6(H t+1 )] = E Ht|x1:t\u22121 P(X t = x t |H t ) E Ht+1|Ht [\u03c6(H t+1 )] = A xt E Ht|x1:t\u22121 [\u03c6(H t )] .(20)", "formula_coordinates": [5.0, 67.5, 85.93, 221.94, 42.01]}, {"formula_id": "formula_31", "formula_text": "\u00b5 1 := E Xt [\u03d5(X t )] = \u00b5 Xt (21) C 2,1 := E Xt+1Xt [\u03d5(X t+1 ) \u2297 \u03d5(X t )] = C Xt+1Xt (22) C 3,x,1 := E Xt+2(Xt+1=x)Xt [\u03d5(X t+2 ) \u2297 \u03d5(X t )] = P(X t+1 = x)C 3,1|2 \u03d5(x). (23", "formula_coordinates": [5.0, 62.89, 268.65, 226.55, 55.82]}, {"formula_id": "formula_32", "formula_text": ")", "formula_coordinates": [5.0, 285.01, 314.51, 4.43, 8.74]}, {"formula_id": "formula_33", "formula_text": "\u00b5 1 = E Ht E Xt|Ht [\u03d5(X t )] = C Xt|Ht E Ht [\u03c6(H t )] = O\u03c0 (24) C 2,1 = E Ht E Xt+1Ht+1|Ht [\u03d5(X t+1 )] \u2297 E Xt|Ht [\u03d5(X t )] =C Xt+1|Ht+1 C Ht+1|Ht C HtHt C Xt|Ht = OT C HtHt O (25) C 3,x,1 = E Ht OA x T \u03c6(H t ) \u2297 E Xt|Ht [\u03d5(X t )] = OA x T C HtHt O(26)", "formula_coordinates": [5.0, 55.44, 375.51, 234.0, 107.17]}, {"formula_id": "formula_34", "formula_text": "E Xt+2Ht+2Ht+1(Xt+1=x)|Ht [\u03d5(X t+2 )] = E Ht+1|Ht P(x|H t+1 )E Ht+2|Ht+1 E Xt+2|Ht+2 [\u03d5(X t+2 )] = OA x T \u03c6(H t ) (27)", "formula_coordinates": [5.0, 55.44, 510.11, 234.0, 41.71]}, {"formula_id": "formula_35", "formula_text": "\u03b2 1 := U \u00b5 1 = (U O)\u03c0 (28", "formula_coordinates": [5.0, 66.19, 641.87, 218.83, 9.65]}, {"formula_id": "formula_36", "formula_text": ")", "formula_coordinates": [5.0, 285.01, 641.87, 4.43, 8.74]}, {"formula_id": "formula_37", "formula_text": "\u03b2 \u221e := C 2,1 (U C 2,1 ) \u2020 = O(U O) \u22121 (29", "formula_coordinates": [5.0, 62.21, 656.32, 222.8, 11.72]}, {"formula_id": "formula_38", "formula_text": ")", "formula_coordinates": [5.0, 285.01, 658.4, 4.43, 8.74]}, {"formula_id": "formula_39", "formula_text": "B x := (U C 3,x,1 )(U C 2,1 ) \u2020 = (UO)A x (UO) \u22121 . (30", "formula_coordinates": [5.0, 64.73, 672.85, 220.28, 11.72]}, {"formula_id": "formula_40", "formula_text": ")", "formula_coordinates": [5.0, 285.01, 674.92, 4.43, 8.74]}, {"formula_id": "formula_41", "formula_text": "\u00b5 Xt+1|x1:t = \u03b2 \u221e B xt:1 \u03b2 1 (31)", "formula_coordinates": [5.0, 375.36, 70.22, 166.08, 9.96]}, {"formula_id": "formula_42", "formula_text": "B x := (U (C 3,1|2 \u03d5(x)))(U C 2,1 ) \u2020 (32", "formula_coordinates": [5.0, 353.79, 165.5, 183.23, 12.03]}, {"formula_id": "formula_43", "formula_text": ")", "formula_coordinates": [5.0, 537.01, 167.58, 4.43, 8.74]}, {"formula_id": "formula_44", "formula_text": "\u03b2 i \u221e to denote C i+1,1 (U C 2,1 ) \u2020 .", "formula_coordinates": [5.0, 307.44, 265.34, 233.5, 24.31]}, {"formula_id": "formula_45", "formula_text": "triplets (x l 1 , x l 2 , x l 3 ) m l=1", "formula_coordinates": [5.0, 433.72, 305.64, 107.22, 16.11]}, {"formula_id": "formula_46", "formula_text": "\u03a6\u03a5 \u03a5\u03a6 v = \u03c9v \u21d4 \u03a6KL\u03b1 = \u03c9\u03a6\u03b1 \u21d4 LKL\u03b1 = \u03c9L\u03b1, (\u03b1 \u2208 R m , \u03c9 \u2208 R)(33)", "formula_coordinates": [5.0, 335.41, 421.2, 206.04, 24.24]}, {"formula_id": "formula_47", "formula_text": "v = 1 \u221a \u03b1 L\u03b1", "formula_coordinates": [5.0, 396.01, 480.88, 47.93, 15.08]}, {"formula_id": "formula_48", "formula_text": "D := diag (\u03b1 1 L\u03b1 1 ) \u22121/2 , . . . , (\u03b1 N L\u03b1 N ) \u22121/2 \u2208 R N \u00d7N , we can concisely express\u00db = \u03a6AD.", "formula_coordinates": [5.0, 307.44, 532.52, 234.0, 34.61]}, {"formula_id": "formula_49", "formula_text": ") \u03b2 1 = 1 m D A \u03a6 \u03a51 m . Similarly, according to (29) \u03b2 \u221e = 1 m \u03a6\u03a5 D A \u03a6 1 m \u03a6\u03a5 \u2020 = \u03a6KLAD\u2126 \u22121 ,", "formula_coordinates": [5.0, 307.44, 576.33, 426.18, 40.85]}, {"formula_id": "formula_50", "formula_text": "A LA = D \u22122 . Last denote \u03a8 = \u03d5(x 1 3 ), . . . , \u03d5(x m 3 ) , then\u0108 3,1|2 (\u2022) = \u03a8 diag (L + \u03bbI) \u22121 \u03a6 (\u2022) KLAD\u2126 \u22121 in (32).", "formula_coordinates": [5.0, 307.44, 627.92, 234.0, 35.27]}, {"formula_id": "formula_51", "formula_text": "In: m i.i.d. triples (x l 1 , x l 2 , x l 3 ) m l=1", "formula_coordinates": [6.0, 55.44, 81.33, 155.55, 16.11]}, {"formula_id": "formula_52", "formula_text": "D = diag (\u03b1 1 L\u03b1 1 ) \u22121/2 , . . . , (\u03b1 N L\u03b1 N ) \u22121/2 . 5:\u03b2 1 = 1 m D A G1 m 6:\u03b2 \u221e = \u03a6Q where Q = KLAD\u2126 \u22121 7:B x\u03c4 = P(x\u03c4 ) m D A F diag (L + \u03bbI) \u22121 \u03a6 \u03d5(x \u03c4 ) Q, for \u03c4 = 1, . . . , t. 8:\u03bc Xt+1|xt:1 =\u03b2 \u221eBxt:1\u03b21", "formula_coordinates": [6.0, 60.23, 193.92, 230.96, 76.45]}, {"formula_id": "formula_53", "formula_text": "Theorem 4 Assume \u03d5(x) F \u2264 1, \u03c6(h) G \u2264 1, max x A x 2 \u2264 1. Then \u00b5 Xt+1|x1:t \u2212\u03bc Xt+1|x1:t F = O p (t(\u03bb 1/2 + (\u03bbm) \u22121/2 (log(1/\u03b4)) 1/2 )) with probability 1 \u2212 \u03b4.", "formula_coordinates": [6.0, 55.44, 482.65, 234.01, 47.82]}, {"formula_id": "formula_54", "formula_text": "x i + j i +1 x i x i \u22121 i \u2212k . . . x i +1 x i x \u22121 x i + j x i x i \u22121 i \u2212k . . . x i x i \u22121 \u22121 C 3,x,1 C 2,1 C future,x,past C future,past i ... x ... x ... x ...", "formula_coordinates": [6.0, 313.33, 62.75, 220.78, 54.84]}, {"formula_id": "formula_55", "formula_text": "m l=1 \u03b3 i \u03d5(x l", "formula_coordinates": [6.0, 437.65, 489.09, 41.89, 14.11]}, {"formula_id": "formula_56", "formula_text": "x t+1 = argmax xt+1 \u03bc Xt+1|x1:t , \u03d5(x t+1 ) F /\u1e90 (34)", "formula_coordinates": [6.0, 319.4, 539.41, 222.04, 12.28]}, {"formula_id": "formula_57", "formula_text": "\u00b5 Ht|xt\u22121:1 G = E Ht|xt\u22121:1 [\u03c6(H t )] G \u2264 E Ht|xt\u22121:1 [ \u03c6(H t ) G ] \u2264 1 (35)", "formula_coordinates": [9.0, 87.8, 319.44, 201.63, 28.3]}, {"formula_id": "formula_58", "formula_text": "\u03b2 1 :=\u00db \u00b5 1 (36", "formula_coordinates": [9.0, 80.07, 439.43, 333.58, 9.65]}, {"formula_id": "formula_59", "formula_text": ")", "formula_coordinates": [9.0, 413.65, 439.43, 47.31, 8.74]}, {"formula_id": "formula_60", "formula_text": "\u03b2 \u221e := C 2,1 (\u00db C 2,1 ) \u2020 (37) B x := P(x) \u00db (C 3,1|2 \u03d5(x)) \u00db C 2,1 \u2020(38)", "formula_coordinates": [9.0, 76.1, 454.37, 381.44, 33.44]}, {"formula_id": "formula_61", "formula_text": "\u03b4 1 := (\u00db O) \u22121 (\u03b2 1 \u2212\u03b2 1 ) G (39", "formula_coordinates": [9.0, 75.14, 530.43, 209.87, 17.2]}, {"formula_id": "formula_62", "formula_text": ")", "formula_coordinates": [9.0, 285.01, 532.5, 4.43, 8.74]}, {"formula_id": "formula_63", "formula_text": "\u03b4 \u221e := \u03b2 \u221e \u2212\u03b2 \u221e 2 (40) \u2206 := max xt (\u00db O) \u22121 (B xt \u2212B xt )(\u00db O) 2 (41) \u03b3 := max xt A xt 2 (we assume \u03b3 \u2264 1). (42", "formula_coordinates": [9.0, 71.17, 552.97, 218.27, 59.5]}, {"formula_id": "formula_64", "formula_text": ")", "formula_coordinates": [9.0, 285.01, 598.34, 4.43, 8.74]}, {"formula_id": "formula_65", "formula_text": "Lemma 5 (\u00db O) \u22121 (B xt:1\u03b21 \u2212B xt:1\u03b21 ) G \u2264 (1 + \u2206) t \u03b4 1 + (1 + \u2206) t \u2212 1 Proof By induction on t. (\u00db O) \u22121 (\u03b2 1 \u2212\u03b2 1 ) G = \u03b4 1 = (1 + \u2206) 0 \u03b4 1 + (1 + \u2206) 0 \u2212 1 when t = 0.", "formula_coordinates": [9.0, 55.44, 631.29, 234.0, 73.58]}, {"formula_id": "formula_66", "formula_text": "(\u00db O) \u22121 (\u03b2 t \u2212\u03b2 t ) G \u2264(1 + \u2206) t\u22121 \u03b4 1 + (1 + \u2206) t\u22121 \u2212 1 (43)", "formula_coordinates": [9.0, 354.54, 99.59, 186.9, 31.97]}, {"formula_id": "formula_67", "formula_text": "(\u00db O) \u22121 (B xt:1\u03b21 \u2212B xt:1\u03b21 ) G \u2264 (\u00db O) \u22121 (B xt \u2212B xt )(\u00db O) 2 (\u00db O) \u22121\u03b2 t G (44) + (\u00db O) \u22121 (B xt \u2212B xt )(\u00db O) 2 (\u00db O) \u22121 (\u03b2 t \u2212\u03b2 t ) G (45) + (\u00db O) \u22121B xt (\u00db O) 2 (\u00db O) \u22121 (\u03b2 t \u2212\u03b2 t ) G(46)", "formula_coordinates": [9.0, 307.44, 177.51, 241.46, 123.01]}, {"formula_id": "formula_68", "formula_text": "(\u00db O) \u22121 (B xt \u2212B xt )(\u00db O) 2 (\u00db O) \u22121\u03b2 t G \u2264\u2206 \u00b5 Ht|xt\u22121:1 G \u2264 \u2206 (47", "formula_coordinates": [9.0, 317.8, 348.04, 219.21, 34.46]}, {"formula_id": "formula_69", "formula_text": "(\u00db O) \u22121B xt (\u00db O) 2 (\u00db O) \u22121 (\u03b2 t \u2212\u03b2 t ) G \u2264 A xt 2 (\u00db O) \u22121 (\u03b2 t \u2212\u03b2 t ) G \u2264(1 + \u2206) t\u22121 \u03b4 1 + (1 + \u2206) t\u22121 \u2212 1 (49)", "formula_coordinates": [9.0, 320.66, 506.17, 220.78, 55.06]}, {"formula_id": "formula_70", "formula_text": "(\u03b2 \u221e \u2212\u03b2 \u221e )(\u00db O)(\u00db O) \u22121B xt:1\u03b21 F \u2264 (\u03b2 \u221e \u2212\u03b2 \u221e )(\u00db O) 2 (\u00db O) \u22121B xt:1\u03b21 G \u2264 \u03b2 \u221e \u2212\u03b2 \u221e 2 O 2 \u00b5 Ht+1|xt:1 G \u2264C O \u03b2 \u221e \u2212\u03b2 \u221e 2 \u2264 C O \u03b4 \u221e(54)", "formula_coordinates": [10.0, 76.28, 238.55, 213.16, 85.13]}, {"formula_id": "formula_71", "formula_text": "(\u03b2 \u221e \u2212\u03b2 \u221e )(\u00db O)(\u00db O) \u22121 (B xt:1\u03b21 \u2212B xt:1\u03b21 ) F \u2264 (\u03b2 \u221e \u2212\u03b2 \u221e )(\u00db O) 2 (\u00db O) \u22121 (B xt:1\u03b21 \u2212B xt:1\u03b21 ) G \u2264 \u03b2 \u221e \u2212\u03b2 \u221e 2 O 2 (1 + \u2206) t \u03b4 1 + (1 + \u2206) t \u2212 \u03b3 \u2264C O \u03b4 \u221e (1 + \u2206) t \u03b4 1 + (1 + \u2206) t \u2212 1 (55)", "formula_coordinates": [10.0, 55.44, 355.73, 241.72, 76.66]}, {"formula_id": "formula_72", "formula_text": "\u03b2 \u221e (\u00db O)(\u00db O) \u22121 (B xt:1\u03b21 \u2212B xt:1\u03b21 ) F \u2264 \u03b2 \u221e (\u00db O) 2 (\u00db O) \u22121 (B xt:1\u03b21 \u2212B xt:1\u03b21 ) G \u2264 OT 2 (1 + \u2206) t \u03b4 1 + (1 + \u2206) t \u2212 1 \u2264C O C T (1 + \u2206) t \u03b4 1 + (1 + \u2206) t \u2212 1(56)", "formula_coordinates": [10.0, 68.13, 465.88, 221.31, 70.84]}, {"formula_id": "formula_73", "formula_text": "2 := C 2,1 \u2212\u0108 2,1 F \u2297F(58)", "formula_coordinates": [10.0, 109.01, 624.43, 180.43, 15.13]}, {"formula_id": "formula_74", "formula_text": "1. \u03b5 0 < 1, 2. \u03c3 n (\u00db \u0108 2,1 ) \u2265 (1 \u2212 \u03b5)\u03c3 N (C 2,1 ), 3. \u03c3 N (\u00db C 2,1 ) \u2265 \u221a 1 \u2212 \u03b5 0 \u03c3 N (C 2,1 ), 4. \u03c3 N (\u00db O) \u2265 \u221a 1 \u2212 \u03b5 0 \u03c3 N (O).", "formula_coordinates": [10.0, 62.24, 67.7, 404.09, 650.21]}, {"formula_id": "formula_75", "formula_text": "\u03b4 1 := (\u00db O) \u22121 (\u03b2 1 \u2212\u03b2 1 ) G \u2264 1 \u03c3 N (\u00db O) Proof (\u00db O) \u22121 (\u03b2 1 \u2212\u03b2 1 ) G = (\u00db O) \u22121 (\u00db \u00b5 1 \u2212\u00db \u03bc 1 ) G \u2264 (\u00db O) \u22121\u00db 2 \u00b5 1 \u2212\u03bc 1 F (60) \u2264 1 \u03c3 n (\u00db O)(61)", "formula_coordinates": [10.0, 307.44, 200.32, 234.0, 139.47]}, {"formula_id": "formula_76", "formula_text": "\u03b4 \u221e := \u03b2 \u221e \u2212 \u03b2 \u221e 2 \u2264 1+ \u221a 5 2 2 min{\u03c3 N (\u01082,1),\u03c3 N (\u00db C2,1)} 2 + 2 \u03c3 N (\u00db C2,1) Proof \u03b2 \u221e \u2212 \u03b2 \u221e 2 = C 2,1 (\u00db C 2,1 ) \u2020 \u2212\u0108 2,1 (\u00db \u0108 2,1 ) \u2020 2 \u2264 \u0108 2,1 ((\u00db C 2,1 ) \u2020 \u2212 (\u00db \u0108 2,1 ) \u2020 ) 2 + (C 2,1 \u2212\u0108 2,1 )(\u00db C 2,1 ) \u2020 2 \u2264 \u0108 2,1 2 ((\u00db C 2,1 ) \u2020 \u2212 (\u00db \u0108 2,1 ) \u2020 ) 2 + C 2,1 \u2212\u0108 2,1 2 (\u00db C 2,1 ) \u2020 2 \u2264 1 + \u221a 5 2 2 min{\u03c3 N (\u0108 2,1 ), \u03c3 N (\u00db C 2,1 )} 2 + 2 \u03c3 N (\u00db C 2,1 ) (62", "formula_coordinates": [10.0, 307.44, 391.21, 234.0, 239.86]}, {"formula_id": "formula_77", "formula_text": ")", "formula_coordinates": [10.0, 537.01, 622.33, 4.43, 8.74]}, {"formula_id": "formula_78", "formula_text": "Lemma 10 \u2206 := max xt (\u00db O) \u22121 (B xt \u2212B xt )(\u00db O) 2 \u2264 1 \u03c3 N (\u00db O) 2 min{\u03c3 N (\u01082,1),\u03c3 N (\u00db C2,1)} 2 + 3 \u03c3 N (\u00db C2,1) Proof max xt (\u00db O) \u22121 (B xt \u2212B xt )(\u00db O) 2 \u2264 max xt 1 \u03c3 N (\u00db O) \u03bc 3,1|xt ((\u00db C 2,1 ) \u2020 \u2212 (\u00db \u0108 2,1 ) \u2020 ) 2 + (\u00b5 3,1|xt \u2212\u03bc 3,1|xt )(\u00db C 2,1 ) \u2020 2 \u2264 1 \u03c3 N (\u00db O) 2 min{\u03c3 N (\u0108 2,1 ), \u03c3 N (\u00db C 2,1 )} 2 + 3 \u03c3 N (\u00db C 2,1 )(63)", "formula_coordinates": [10.0, 307.44, 685.56, 256.16, 34.49]}, {"formula_id": "formula_79", "formula_text": "1 = \u00b5 1 \u2212\u03bc 1 F (64", "formula_coordinates": [11.0, 81.76, 307.6, 203.25, 11.15]}, {"formula_id": "formula_80", "formula_text": ")", "formula_coordinates": [11.0, 285.01, 307.6, 4.43, 8.74]}, {"formula_id": "formula_81", "formula_text": "2 = C 2,1 \u2212\u0108 2,1 F \u2297F(66)", "formula_coordinates": [11.0, 81.76, 343.54, 207.68, 15.13]}, {"formula_id": "formula_82", "formula_text": "3 = max xt \u00b5 3,1|xt \u2212\u03bc 3,1|xt F \u2297F (68", "formula_coordinates": [11.0, 81.76, 380.71, 203.25, 14.13]}, {"formula_id": "formula_83", "formula_text": ")", "formula_coordinates": [11.0, 285.01, 380.71, 4.43, 8.74]}], "doi": ""}