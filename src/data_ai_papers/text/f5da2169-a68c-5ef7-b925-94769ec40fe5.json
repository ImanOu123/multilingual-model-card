{"title": "Feature-based Learning for Diverse and Privacy-Preserving Counterfactual Explanations", "authors": "Vy Vo; Trung Le; Van Nguyen; He Zhao; Edwin V Bonilla Csiro's Data61;  Australia; Gholamreza Haffari; Dinh Phung; Edwin V Bonilla", "pub_date": "2023-06-01", "abstract": "Interpretable machine learning seeks to understand the reasoning process of complex black-box systems that are long notorious for lack of explainability. One flourishing approach is through counterfactual explanations, which provide suggestions on what a user can do to alter an outcome. Not only must a counterfactual example counter the original prediction from the black-box classifier but it should also satisfy various constraints for practical applications. Diversity is one of the critical constraints that however remains less discussed. While diverse counterfactuals are ideal, it is computationally challenging to simultaneously address some other constraints. Furthermore, there is a growing privacy concern over the released counterfactual data. To this end, we propose a feature-based learning framework that effectively handles the counterfactual constraints and contributes itself to the limited pool of private explanation models. We demonstrate the flexibility and effectiveness of our method in generating diverse counterfactuals of actionability and plausibility. Our counterfactual engine is more efficient than counterparts of the same capacity while yielding the lowest re-identification risks.\u2022 Computing methodologies \u2192 Machine learning; \u2022 Security and privacy \u2192 Privacy protections.", "sections": [{"heading": "INTRODUCTION", "text": "The eminence of deep neural networks in recent years has proliferated the use of machine learning in various real-world applications. Such models provide remarkable predictive performance yet often at a cost of transparency and interpretability. This has sparked controversy over whether to rely on algorithmic predictions for high-stakes decision making such as graduate admission [1,54], job recruitment [3], credit assessment [26] or criminal justice [15,36]. Progress in interpretable machine learning offers interesting solutions to explaining the underlying behavior of black-box models [37,41,53]. One useful approach is through counterfactual examples 1 , which sheds light on what modifications to be made to an individual's profile that can counter an unfavorable decision outcome from a black-box classifier. Such explanations explore what-if scenarios that suggest possible recourses for future improvement. Counterfactual explainability indeed has important social implications at both personal and organizational levels. For instance, feedback like 'getting 1 more referral' or 'being fluent in at least 2 languages' would help unsuccessful candidates better prepare for future job applications. By advocating for transparency in decision making, organizations can improve their attractiveness to top talents while inspecting possible prejudice implicitly introduced in historical data and consequentially embedded in the classifiers producing biased decisions.\nThe ultimate goal of this line of research is to provide realistic guidelines as to what actions an individual can take to achieve a desired outcome. Desiderata of counterfactual explanations have been extensively discussed in previous literature [18,21,51,52].\nTo be of practical use, a counterfactual explanation should at least satisfy the following characteristics: \u2022 Validity: By definition, a counterfactual example must change the original black-box outcome to a desired one. \u2022 Sparsity: Counterfactuals should be close to the original example where a minimal number of features are modified. \u2022 Actionability: Counterfactual explanations should only suggest actionable or feasible changes. In particular, changes should be made on mutable features e.g., Work Experience or SAT scores, while leaving immutable features unchanged e.g., Gender or Ethnicity. \u2022 Diversity: Diverse explanations are preferable to capture different preferences from the same user so that they can freely explore multiple options to select the best fit. \u2022 Plausibility 2 : Plausible or realistic counterfactuals are to obey the input domain and constraints within/among features. For example, Age cannot decrease or be above 200. \u2022 Scalability: Inference should be done simultaneously and efficiently for multiple input examples. Among these desiderata, diversity emerges as a non-trivial property to address. Given an instance, a diverse counterfactual engine returns a set of different counterfactual profiles that should all lead to the desired outcome. Ensuring that the entire explanation set satisfies validity while dealing with constraints given by actionability and plausibility poses a computational challenge. Scalability becomes another important consideration mainly due to the fact that most of the existing approaches process counterfactuals separately for each input data point. Furthermore, strongly enforcing sparsity results in a smaller subset of features that can be changed. This hence can compromise diversity since we expect counterfactual states to differ from one to another substantially. On the other hand, there has been a growing concern over the privacy risks of model explanation [32,46,48]. A\u00efvodji et al. [2] points out that diverse counterfactual explanations make the system more vulnerable as the released examples reveal the model decision boundaries and could disclose sensitive information such as health conditions or financial data. A linkage attack is one such malicious attempt, which refers to the action of recovering the identity (i.e., re-identifying) of an anonymized record in the published dataset using background knowledge. It is often done by linking records to an external dataset of the population based on the combination of several attributes [12,28,44]. Netflix $1M Machine Learning Contest is a notorious data breach, in which the company disclosed a dataset of 100 million subscribers with their movie ratings and preferences. Narayanan and Shmatikov [34] revealed a successful attack of 68% that was easily achieved by cross-referencing the users' dates and precise ratings of 2 movies with a non-anonymous dataset published by IMDb (Internet Movie Database).\nDespite an overwhelming number of counterfactual explanation approaches, only a few works tackle diverse counterfactual generation [5,22,33,40,43]. However, the trade-offs between diversity and the aforementioned constraints, including privacy protection, have not been well studied in previous papers (See Table 1 for comparison). Filling this gap, our work proposes a novel learningbased framework that effectively addresses all the above desiderata 2 The terms plausibility and feasibility are often used interchangeably.\nwhile mitigating the re-identification risk. From a methodological perspective, our method diverges markedly from existing approaches in the following ways:\nFirstly, we reformulate the combinatorial search task into a stochastic optimization problem to be solved via gradient descent. Unlike most previous methods that perform optimization per-input basis, we employ amortized inference to generate diverse counterfactual explanations efficiently. Amortization has been previously adopted wherein a counterfactual generative distribution is modelled via Markov Decision Processes [52] or Variational Autoencoders [9,30,38]. On one hand, none of these amortized methods addresses diversity. On the other, we here take a different approach: we construct a learnable generation module that directly models the conditional distributions of individual features such that they form a valid counterfactual distribution when combined.\nAnother point of difference of ours lies in the usage of Bernoulli sampling to ensure sparsity. In prior works, standard metrics such as L1 or L2 are often used to penalize the distance between the counterfactual and original data point. Verma et al. [51] criticizes this approach as unnatural, especially for categorical features. Avoiding the use of distance measures, we optimize along a feature selection module to output the likelihood of the feature being mutated. This module can be adapted to any user-defined constraints about the mutability of features.\nFinally, we go beyond existing approaches by tackling the constraint of privacy preservation exposed to diverse explanations. The key strategy is to discretize continuous features and operate the counterfactual generation engine in the categorical feature space. Discretization is closely related to the generalization technique used in privacy-preserving data mining (PPDM) [12,31]. It is also treated as a subroutine to analyze the composition of differential privacy algorithms [14,17]. The idea is that it is by nature easier to uniquely identify a profile based on continuous features, so discretization is expected to increase the quantities of profiles linked back to a certain group of attributes. Another defense effort against linkage attack can be found in [16]. The paper proposes an algorithm named CF-K that heuristically searches for an equivalence class for each counterfactual instance such that every record is indistinguishable from at least \u2212 1 others. CF-K is viewed as an add-on that theoretically can be implemented on top of any counterfactual generative system. However, in practice, this strategy is extremely expensive since it requires repetitively querying the model explainer for a possibly larger number of counterfactuals than requested. Though sharing the same motivation, we here contribute a counterfactual explanation model with a built-in privacy preservation functionality.\nOur contributions can be summarized as follows:\n\u2022 We introduce Learning to Counter (L2C) -a stochastic featurebased approach for learning counterfactual explanations that address the counterfactual desirable properties in a single endto-end differentiable framework. \u2022 Through extensive experiments on real-world datasets, L2C is shown to balance the counterfactual trade-offs more effectively than the existing methods and achieve diverse explanations with the lowest re-identifiability risk. To the best of our knowledge, L2C is the first amortized engine that supports diverse counterfactual generations with privacy-preservation capability.", "publication_ref": ["b0", "b53", "b2", "b25", "b14", "b35", "b36", "b40", "b52", "b0", "b17", "b20", "b50", "b51", "b31", "b45", "b47", "b1", "b11", "b27", "b43", "b33", "b4", "b21", "b32", "b39", "b42", "b1", "b51", "b8", "b29", "b37", "b50", "b11", "b30", "b13", "b16", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "RELATED WORKS", "text": "Recent years have seen an explosion in the literature on counterfactual explainability, from works that initially focused on one or two characteristics or families of models to those that can deal with multiple constraints and various model types. There have been many attempts to summarize major themes of research and discuss open challenges in great depth. We therefore refer readers to [18,21,51] for excellent surveys of methods in this area. We here focus on reviewing algorithms that can support diverse (or at least multiple) local counterfactual generations.\nDealing with the combinatorial nature of the task, earlier works commonly adopt mixed integer programming [43], genetic algorithms [45], or SMT solvers [20]. Another recent popular approach is gradient-based optimization [5,33], which involves iteratively perturbing the input data point according to an objective function that incorporates desired constraints. The whole idea of diversity is to explore different combinations of features and feature values that can counter the original prediction while accommodating various user needs. To support diversity, Russell [43] in particular enforces hard constraints on the current generations to be different from the previous ones. Such a constraint will however be removed whenever the solver cannot be satisfied. Meanwhile, Mothilal et al. [33] and Bui et al. [5] add another loss term for diversity using Determinantal Point Processes [25], whereas the other works only demonstrate the capacity to generate multiple counterfactuals via empirical results. All of the aforementioned algorithms are computationally expensive in that input data points are handled singly and individual runs are additionally required to produce several counterfactuals. Reducing computational costs, Redelmeier et al. [40] attempts to model the conditional likelihood of mutable features given the immutable features using the training data. They then adopt Monte Carlo sampling to generate counterfactuals from this distribution and filter out samples that do not meet counterfactual constraints. Given such a generative distribution, sampling of counterfactuals can therefore be done straightforwardly. Amortized optimization is another strategy to improve inference speed [9,30,38,52].\nIn response to the privacy warning about model explanations [32,46,48], several defense strategies have been introduced to alleviate the risks. With strong theoretical guarantees, differential privacy [11] stands out as the promising solution to preventing member inference attack and model stealing attack [2,32,47]. With regards to linkage attacks, CF-K [16] is the only work we are aware of that tackles linkage attack in counterfactual explanations.", "publication_ref": ["b17", "b20", "b50", "b42", "b44", "b19", "b4", "b32", "b42", "b32", "b4", "b24", "b39", "b8", "b29", "b37", "b51", "b31", "b45", "b47", "b10", "b1", "b31", "b46", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "STOCHASTIC FEATURE-BASED COUNTERFACTUAL LEARNING 3.1 Problem setup", "text": "Let X denote the input space where = [ ] =1 is an input vector with features of both continuous and categorical types. As discussed previously, we discretize the continuous features into equalsized buckets, which gives us an input of categorical features wherein each feature has levels. We apply one-hot encoding on each feature and flatten them into a single input vector \u2208 {0, 1} where = =1 . Concretely, feature is now represented by the vector \u2208 O where the set of one-hot vectors O is defined as {0, 1} : =1 = 1. Let be the black-box classifying function and = ( ) be the decision outcome on the input . A valid counterfactual example associated with is one that alters the original outcome into a desired outcome \u2032 \u2260 with \u2032 = ( ). Let denote the corresponding one-hot representation of .\nActionability indicates that some features can be mutable (i.e., changeable), while others should be kept immutable (i.e., unchangeable). Without loss of generality, let us impose an ordering on the set of features such that the first features are mutable features (i.e., the ones that can be modified) and denote K := {1, ..., } \u2282 {1, ..., }. For each mutable feature (i.e., or the one-hot vector with \u2208 K), we aim to learn a local feature-based perturbation distribution ( | ) where \u2208 O , while leaving the immutable features unchanged.\nIt is worth noting that our method functions equally well on heterogeneous data where only categorical features are one-hot encoded while continuous features are retained at their original values. However, we believe that performing data discretization (or generalization in terms of PPDM) initially and deploying the classifiers in the discrete feature space would provide better privacy protection. For the purpose of comparing our prototype with existing approaches, we follow the standard practice of explaining classification models trained on the mixed dataset of continuous and categorical features. To make it compatible with the discretization subroutine of our framework, we represent the prediction on a transformed (fully categorical) input vector with the prediction on the input where the categorical values associated with mutable continuous features are substituted with the middle point of the corresponding intervals. We refer to this mechanism as one-hot decoding, which will be detailed shortly.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Methodology", "text": "We now detail how L2C works and addresses each counterfactual constraint. The explanation pipeline of L2C is depicted in Figure 1.\nFor each mutable feature with \u2208 K, we learn a local featurebased perturbation distribution ( | ) (i.e.,\u02dc\u2208 O ), which is a categorical distribution Cat( | ) with category probability = 1 , 2 , ..., . We form a counterfactual example by concatenating \u223c Cat( | ) for the mutable features and for the immutable features. To achieve validity, we learn the local feature-based perturbation distribution by maximizing the chance that the counterfactual examples counter the original outcome on . Additionally, learning local feature-based perturbation distributions over the mutable features allows us to conduct a global counterfactual distribution ( | ) over the counterfactual examples defined above. Sampling from this distribution naturally leads to multiple counterfactual generations efficiently, and we also expect that individual samples together can form diverse combinations of features, thereby promoting diversity within the generative examples.\nTable 1: Desiderata comparison of related counterfactual explanation methods. * Privacy refers to whether the output data is protected against linkage attack or re-identification risk. L2C satisfies all of these critical constraints.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Method", "text": "Sparsity Actionability Diversity Plausibility Scalability Privacy * L2C (Ours) DICE [33] COPA [5] MCCE [40] Coherent CF [43] MACE [20] MOC [8] CERTIFAI [45] Feasible-VAE [30] FastAR [52] CRUDS [9] C-CHVAE [38] CF-K [16] One-hot Encoder We first discretize the continuous features of input and one-hot encode all features into representations . For every feature , the generator G learns a local perturbation distribution Cat( | ) so that together they form a distribution of diverse counterfactual representations . Simultaneously, the selector S learns to output the distribution Multi-Bernoulli( | ) capturing the probability of each feature being modified. Every feature sample pair ( , ) is passed through an operation in the blue box, which decides whether to accept the change being made to the feature given by . The output is then decoded into the representations compatible with the black-box system. G and S are jointly trained via back-propagation according to Eq. (4). Intuitively, G aims to construct a \"bridge\" across the decision boundary travelling from the input to a local space of counterfactuals.\nAs previously discussed, too much of diversity can compromise sparsity. Dealing with this constraint, for each mutable feature , we propose to learn a local feature-based selection distribution that generates a random binary variable \u223c Bernoulli( | ) wherein we replace by \u223c Cat( | ) if = 1 and leave\u02dc= if = 0. Therefore, the formula to update is\n= (1 \u2212 ) +\n. The benefit of having = [ ] \u2208K is thus to control sparsity by adding one more channel to decide if we should modify a mutable feature . Appendix D presents an ablation study showing that without the selection distribution, the perturbation distribution alone can generate diverse counterfactuals yet requires changing plenty of mutable features. Meanwhile, optimizing the selection distribution jointly helps harmonize the trade-off between diversity and sparsity.", "publication_ref": ["b32", "b4", "b39", "b42", "b19", "b7", "b44", "b29", "b51", "b8", "b37", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Optimization Objective", "text": "In this section, we explain how to design the building blocks of our framework L2C. As shown in Figure 1, our framework consists of two modules: a counterfactual generator G and a feature selector S. The counterfactual generator G is used to model the feature-based perturbation distribution, while feature selector S is employed to model the feature-based selection distribution.\nSpecifically, given a one-hot vector representation of a data example , we feed to G to form G( ) = [G ( )] \u2208K . We then apply the softmax activation function to G ( ) to define the featurebased local distribution (i.e., Cat( | ))) for as\n( ) = exp G ( ) =1 exp G ( ) , \u2200 = 1, ..., .(1)\nThe module S takes to form S( ) = [S ( )] \u2208K . We then apply the Sigmoid function to S ( ) to define the feature-based selection distribution (i.e., Bernoulli( | )) for as\n( ) = 1 1 + exp \u2212 S ( ) .\nTo encourage sparsity by reducing the number of mutable features chosen to be modified, we regularize S through L1-norm \u2225 ( ) \u2225 1 with ( ) = [ ( )] \u2208K .\nTo summarize, given a one-hot vector representation of a data example , we use G to work out the local feature-based perturbation distribution Cat( ( )) for every \u2208 K. We then sample \u223c Cat( ( )) for every \u2208 K. Subsequently, we use S to work out the local feature-based selection distribution Bernoulli( ( )) for every \u2208 K. We then sample \u223c Bernoulli( | ) and update = (1 \u2212 ) + for every \u2208 K. Finally, we concatenate for \u2208 K and for \u2209 K to form the counterfactual example . G and S are parameterized with neural networks over total parameters . For to be a valid and sparse counterfactual associated with a desired outcome \u2032 , we propose the following criterion\nmin E CE( ( ), \u2032 ) + E \u2225 ( )\u2225 1 ,(2)\nwhere is the black-box function, CE is the cross-entropy loss, \u2225 \u2022 \u2225 1 is L1-norm, is a loss weight.\nOne-hot decoding. Recall that formed by concatenating many one-hot vectors is an incompatible representation to the classifier , which in fact requires both continuous and one-hot features. We make a design choice of reconstructing the continuous features by taking the middle point of the range corresponding to the selected level. Specifically, the input to the one-hot decoder is = [ ] =1 . If the feature originally is a categorical feature, we set = . Otherwise, we set =\n+ (2 \u22121) ( \u2212 ) 2\n, which is the middle point of the -th interval [ + ( \u2212 1)( \u2212 )/ , + ( \u2212 )/ ] where [ , ] is the original value range of the feature and corresponds to the level \u2208 {1, ..., } (i.e., = 1 and = 0 if \u2260 ). Note that one-hot decoding is only applied when a continuous feature is indicated by S to be mutable ( = 1). Otherwise, we revert to the original continuous value. Formally, we rewrite\n= (1 \u2212 ) + \u2211\ufe01 =1 + (2 \u2212 1)( \u2212 ) 2 . (3\n)\nTo assure model differentiability for training, the one-hot vector is relaxed into its continuous representation by using Gumbel-Softmax reparametrization trick, which is detailed in Section 3.4.\nThe final optimization objective is now given as\nmin E CE( ( ), \u2032 ) + E \u2225 ( )\u2225 1 .(4)\nPlausibility. A counterfactual generative engine needs to ensure explanations are realistic for real-world applications. There are two common types of plausibility constraints: Unary and Binary monotonicity constraints. The former deals with individual features (e.g., Age cannot decrease) while the latter is concerned with the correlation of a pair of features (e.g., increasing in Education level increases Age). To handle binary constraints on heterogeneous data is not as straightforward as unary constraints. We thus delay the discussion on binary constraints until Section 5.4 and focus on unary constraints in the main analysis.\nDealing with such a constraint, one can simply eliminate any counterfactuals violating the constraints during inference time. This however creates additional computational overhead and may compromise some desiderata. There are only few attempts addressing feature constraints in counterfactuals, notably Mahajan et al. [30], Verma et al. [52]: Verma et al. [51] incorporate hard conditions in a Markov Decision process where a feature gets updated only if the corresponding action does not violate the constraints. Meanwhile, Mahajan et al. [30] includes a hinge loss into the loss function for unary features, while specifically learning a separate linear model for every feature pair subject to a binary constraint. For L2C, the learnable local distributions can be used for this purpose conveniently. Our proposed strategy is to impose rule-based unary constraints on related features in the optimization process. Technically, for every feature to be perturbed with a non-decreasing (non-increasing) constraint, we penalize the probabilities corresponding to lower (higher) levels towards zero by multiplying them with a positive infinitesimal quantity. Concretely, given a mutable feature under monotonic constraints, let \u2208 {1, ..., } denote the current state -the level corresponding to (i.e., = 1 and = 0 if \u2260 ). Let us denote the restricted set of levels as C = {1, ..., \u22121} if the feature is non-decreasing and C = { + 1, ..., } if it is nonincreasing. The perturbation distribution Cat( | ) for given in Eq. (1) now becomes\n( ) \u221d 1 C ( ) \u00d7 exp G ( ) =1 exp G ( ) , \u2200 = 1, ..., ,(5)\nwhere 1 C (.) is the indicator function such that 1 C ( ) = 1 if \u2208 C and 1 C ( ) = 0 otherwise, meaning that the probabilities at the other levels are untouched. We here explicitly force the model to generate more samples at the higher (lower) levels while maintaining differentiability of the objective function. We choose = \u221210 in our experiments, but any positive value arbitrarily close to zero would suffice.", "publication_ref": ["b29", "b51", "b50", "b29"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Reparameterization for Continuous Optimization", "text": "Our L2C involves multiple sampling rounds back and forth to optimize the networks. To make the process continuous and differentiable for training, we adopt the reparameterization tricks [19,29]:\n1) Sampling \u223c Cat( | ). : To obtain differentiable counterfactual samples, we adopt the classic temperature-dependent Gumbel-Softmax trick [19,29]. Given the categorical variable z with category probability 1 , 2 , ...,\n. The relaxed representation is sampled from the Categorical Concrete distribution as \u223c Cat-Concrete(log 1 , ..., log ) by\n= exp (log ( ) + )/ =1 exp (log ( ) + )/ .\nwith temperature , random noises independently drawn from Gumbel distribution = \u2212 log(\u2212 log ), \u223c Uniform(0, 1). As discussed, we apply this mechanism consistently to the one-hot representations of all features. The continuous relaxation of Eq. (3) can be gained by simply using the one-hot relaxation .\n2) Sampling \u223c Bernoulli( | ). : We again apply the Gumbel-Softmax trick to relax Bernoulli variables of 2 categories. With temperature , random noises 0 and 1 \u223c = \u2212 log(\u2212 log ), \u223c Uniform(0, 1), the continuous representation is sampled from Binary Concrete distribution as \u223c Bin-Concrete( , 1 \u2212 ) by\n= exp{ log ( ) + 1 / } exp{ log(1 \u2212 ( )) + 0 }/ } + exp{ log ( ) + 1 / )} .", "publication_ref": ["b18", "b28", "b18", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENTAL SETUP", "text": "We experiment with 4 popular real-word datasets: German Credit [10], Adult Income [24] Graduate Admission [1] and Student Performance [7]. For each dataset, we select a fixed subset of immutable features based on our domain knowledge and suggestions from [52]. We reserve the privacy analysis for German Credit and Adult Income datasets, which contain personal financial information and various attributes through which data subjects can be re-identified [16]. While implementing the black-box classifiers and the baseline methods, we standardize numerical features to unit variance and one-hot encode categorical features. Note again that, for our method only, we discretize numerical features into equal-sized buckets and decode the numerical features back to their original representations whenever necessary to consult the black-box model. Appendix A describes our tasks and model design in greater detail. Our code repository can be accessed at https://github.com/isVy08/L2C/. Performance metrics. Following the past works Mothilal et al. [33], Redelmeier et al. [40], Verma et al. [52], Table 2 outlines the commonly used metrics for quantitatively assessing the desirability of counterfactual explanations. As for diversity, a widely adopted measure is the pairwise distance between counterfactual examples, with distance defined separately for numerical and categorical features [33,40]. Though this approach is meaningful for interpreting categorical features, we however find it quite obscure for numerical features. This motivates us to discretize numerical features again when computing Diversity, which captures how often a feature gets altered as well as how much the change is -specifically via how often it switches to a different categorical level. The computation of Diversity only considers valid counterfactuals, so if valid counterfactuals are none, Diversity is set to zero. It is worth noting that there fundamentally exists a trade-off between sparsity and diversity. To quantify how well a method can balance these two properties, we suggest taking Harmonic mean of Diversity and Sparsity, motivated by the development of F1-score in measuring Precision against Recall. For metrics used in privacy analysis, refer to Section 5.2.\nEvaluation setup. We consider a general setting of binary classification where a counterfactual outcome \u2032 is opposite to the original outcome , whether has label 1 or 0. From each method, we generate a set of 100 counterfactual explanations. During generation, most methods, including ours, require multiple iterations of searching for the optimal set of counterfactuals based on the optimization constraints. To assure a fair comparison on efficiency, a global maximum time budget of 5 minutes is imposed to search for a set of 100 counterfactuals per input sample. We compare our method top-performing baselines that support diverse counterfactual explanations: DICE [33], MCCE [40] and COPA [5]. DICE offers several search strategies: Random, KD tree, or Genetic algorithm. DICE-KDTree was consistently reported to fail across datasets [52], so we exclude it from our evaluation. We do not consider MACE [20] since it is extremely expensive on large datasets [52] and often fails to converge in our experiments, nor MOC [8] due to the lack of Python implementation.", "publication_ref": ["b9", "b23", "b0", "b6", "b51", "b15", "b32", "b39", "b51", "b32", "b39", "b32", "b39", "b4", "b51", "b19", "b51", "b7"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "RESULTS AND DISCUSSION 5.1 Counterfactual Explanation Desiderata", "text": "We first study whether an algorithmic recourse approach generates a set of diverse counterfactuals without sacrificing the other desiderata. Note that COPA has only been shown to work effectively on linear classifiers. Table 3  Under the same time budget, our method L2C succeeds in generating 100% valid counterfactuals with full coverage. Together with DICE, L2C first satisfies the most important criterion of a counterfactual explanation and resolves the trade-off against validity. Recall that we have specified a fixed set of immutable features for each dataset, based on which we can work out the minimum sparsity threshold a counterfactual explanation should adhere to (i.e., % immutable features). Actionability can then be assessed by comparing Sparsity with this level to determine if a method satisfies the mutability of features. An adequate explanation must achieve at least this level of sparsity. MCCE evidently fails to fulfill this constraint on Adult Income and Student Performance datasets.\nOur reported results here are obtained under no other conditions than the constraints related to feature immutability and monotonicity described in Table 7. Nevertheless, we would like to highlight the flexibility of our framework in controlling the quality of counterfactual generations during inference. Users can freely specify any sparsity threshold or additional conditions of interest to filer out unsatisfactory examples without re-training or re-optimization as in methods like DICE. Specifically, DICE employs gradient search directly on each query according to a selected set of weighting hyperparameters for each term in the objective function. To get a less sparse or more diverse example than the current generation, one needs to activate a new search routine.\nToo many constraints or too much sparsity clearly affects the diversity level of the counterfactual set. Maintaining a high Harmonic mean scores while satisfying almost all feature constraints demonstrates that L2C can effectively manage these trade-offs. The fact that L2C converges to valid counterfactuals with minimal violation in such a short inference time can be attributed to the practice of injecting hard constraints during optimization and global training does enhance the effect. It certainly helps circumvent the burden of heuristically eradicating violated samples. Notice also that our quantitative results align with the descriptions in Table 1. None of the diverse counterfactual explanation approaches address plausibility thoroughly whereas those reported to support the feasibility of features do not guarantee diversity. Appendix C provides empirical evidence for this claim, in which we compare L2C with popular amortized algorithmic recourse approaches and demonstrate our consistent superiority in generating diverse explanations efficiently without violating the required constraints (See Table 8).", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2", "tab_6", "tab_8"]}, {"heading": "Re-identification Risk Analysis", "text": "Preliminaries. We start by reviewing the fundamental concepts related to a public dataset:\n\u2022 Identifiers: Attributes that uniquely identify an individual. Identifiers can be a person's full name, government tax number or driver's license number. \u2022 Quasi-identifiers: Attributes that themselves do not uniquely identify a person, but when combined are sufficiently correlated to at least one individual record. For example, the combination of gender, birth dates and ZIP codes can re-identify 87% of American residents [50]. \u2022 Sensitive attributes: Attribute that are protected against unauthorized access. Sensitive data is confidential and if leaked could harm personal safety or emotional well-being. Examples are salary, medical conditions, salary, criminal histories, or phone numbers. \u2022 Equivalence class: An equivalence class is a group of records with identical quasi-identifiers. Every public dataset must first be anonymized by removing identifiers. However, the data may still be vulnerable to re-identification attacks due to the potential existence of quasi-identifiers. To quantify the level at which a dataset is susceptible to re-identification risk, the following 3 metrics are commonly used: \u2022 k-Anonymity [44]: A dataset satisfies -anonymity if for each record in the dataset, the quasi-identifiers are indistinguishable from at least \u2212 1 other people also in the dataset.\n\u2022 l-Diversity [28]: A dataset has -diversity if, for every equivalence class, there are at least distinct values for each sensitive attribute. \u2022 k-Map [12]: Given an auxiliary dataset used for re-identification (e.g., US Census or IMDb dataset in the Netflix example), so-called the 'attack' dataset, a dataset satisfies -map if every equivalence class is mapped to at least records in the 'attack' dataset.\nEvaluation metrics. Suppose a company releases an API that permits users to query a set of counterfactual examples. We now analyze the level of privacy leakage associated with the output data, by quantifying the percentage of successful attacks w.r.t the aforementioned metrics. Respectively, we measure (1) 1-Anonymity: % equivalence classes with only = 1 member, (2) 1-Diversity: % equivalence classes with = 1 value for a sensitive attribute, (3) 1-Map: % counterfactual examples exactly matched with any single record in an 'attack' dataset. Notice that given a -anonymized dataset, the existence of a one-to-one mapping with the 'attack' dataset means the released dataset fails k-Map.\nExperiments. By definition, violations w.r.t k-Anonymity and l-Diversity are computed against the output set of examples, while k-Map requires an external dataset. For Adult Income, we choose the validation set as the 'attack' set. For German Credit, there exists multiple versions of this dataset across the literature. The one used in our main analysis is adopted from [52], which has been subject to pre-processing. We use another version published by Penn State University 3 for re-identification. We assume these hold-out sets belong to some larger datasets of population availably accessed by the public. Quasi-identifiers and sensitive attributes are given in Appendix A. Following Goethals et al. [16], we consider the black-box predicted label as part of the quasi-identifiers.\nLet's first look into the attacks on the raw output explanations presented in Table 4. We here assume that the attacker's goal is to collect as many examples as possible without caring about which one is valid. If the attacker has no information about how the data is discretized, it is much less likely to find exact matches in the 'attack', thereby reducing the re-identifiability of L2C data. Now we assume the attacker gets access to both the API and our discretization mechanism. They therefore could correspondingly discretize the data in the 'attack' set and retrieve the matches. Our analysis assumes this worse scenario, meaning that for L2C only, we compute 1-Map against the discretized data. The entries N/A in Table 4 are due to the fact that no counterfactual example in the data of DICE-Genetic or COPA has matches, so their robustness remains unverifiable in our experiment. We must highlight that no match does not necessarily translate to zero privacy risk. We also note that such a case is different from L2C, whose result is nearly 0.00%. L2C in fact still returns matches for some records wherein we achieve k-Map of 2\u22123 specifically. Overall, L2C yields the lowest re-identifiability risk. Another interesting observation on German Credit is that although DICE performs well on k-Anonymity, the number of attacks on l-Diversity is dramatically high. This sheds light on the limitation of k-Anonymity discussed in [28] about Homogeneity attacks and Background knowledge attacks. Basically, attacking a k-anonymized dataset with a high can still reveal some Privacy under CF-K. We here investigate the effectiveness of the idea behind CF-K proposed in [16]. CF-K searches for an equivalence class for each counterfactual example and suggests only publishing profiles of at least -sized equivalence class. Since the authors do not publish their codes, plus it is hugely time-consuming to run on models like DICE, we extend the above experiment and examine the effect when every output counterfactual set is 2-anonymized. Specifically, for every set of 100 generations, we remove records not belonging to any equivalence classes. Given now that the data is now 2-anonymized, we evaluate attacks against l-Diversity and k-Map. We also measure % of valid counterfactuals left in the set, assuming that a user requests for 100 per instance. The more valid examples lost from a model explainer imply that it will be more costly to search for sufficient equivalence classes for every instance. Figure 2 depicts that in most cases, -anonymization enhances the protection of the sensitive attributes but does greatly compromise the validity of the output explanations.\nThe purpose of -anonymization is to ensure when attacked, an individual remains indistinguishable from at least \u22121 others. However, notice that -anonymization does not prevent k-Map against which the number of successful attacks is still high for DICE and MCCE. The threat is thus no less severe when the attacker is interested in the re-identifiability of both datasets. If the released data contains the sensitive information that is missing from the 'attack' dataset, and given the fact that 1-diversity remains well above zero for all methods, the attacker could easily infer private information of every linked record. To this end, generalizing the data as done in L2C is proved to be useful to prevent such an inference attack. It is also observed that combining CF-K with L2C in particular significantly improves the anonymity of our counterfactual data. We therefore believe that the integration of L2C with other privacy techniques in the cybersecurity area would yield a more effective safeguard.", "publication_ref": ["b49", "b43", "b27", "b11", "b51", "b15", "b27", "b15"], "figure_ref": [], "table_ref": ["tab_3", "tab_3"]}, {"heading": "Discretization", "text": "Discretization is an important pre-processing step in data analysis in which the problem of optimal discretization with a minimum number of splits is proved to be NP-Hard [6,35]. We here adopt the unsupervised Equal-frequency discretizer, which splits a continuous attribute into buckets with the same number of instances 4 . More concretely in this experiment, features are quantized using Python function qcut 5 , which requires specifying the maximum number of buckets/levels and later adjusts it depending on the input data distribution. We set the maximum buckets to be 4 such that every bucket averagely has 25% of total observations. Table 13 reports \nL2C DICE-R MCCE DICE-G COPA 0% 25% 50% 75% 100% 1-Diversity (1) German Credit + CF-K Raw L2C DICE-R MCCE DICE-G 0% 25% 50% 75% 100% Adult Income L2C DICE-R MCCE DICE-G COPA 0% 25% 50% 75% 100% 1-Diversity (2) L2C DICE-R MCCE DICE-G 0% 25% 50% 75% 100% L2C DICE-R MCCE DICE-G COPA 0% 25% 50% 75% 100% Validity Loss L2C DICE-R MCCE DICE-G 0% 20% 40% 60% L2C DICE-R MCCE -10% 0% 10% 20% 1-Map L2C DICE-R MCCE -10% 0% 10% 20%\nFigure 2: Privacy risk comparison between the raw output data and the data subject to 2-anonymization under the strategy of CF-K. For all metrics, lower is better. l-Diversity is evaluated on 2 most sensitive attributes.\nhow the numerical features of each dataset are discretized. We argue that having very few buckets is likely to cause under-fitting since there are very few useful combinatorial patterns that can counter the original label. Whereas we need diversity for effective learning, too many buckets are undesirable since it can hurt generalization due to some following reasons : (1) each bucket would contain too little data and the chosen middle value may not represent the bucket well, and (2) the model has more combinations of features to explore, thus can converge to sub-optimal combinations that cannot generalize well on unseen test points. In this regard, we decide to split data into equal-sized buckets in the hope of balancing the trade-off.\nVarious discretization methods exist [39]. Table 5 analyzes the performance of our method L2C under 3 other discretization strategies. The first one is Minimal entropy partitioning (MDP) [13]. It is an old-school supervised approach and one of the most widely used. MDP determines the binary discretization for a value range by selecting the cut point that minimizes the class entropy. The algorithm can be applied recursively on sub-partitions induced by the initial cut point and the paper proposes using Minimum Description Length Principle 6 [42] as a stopping condition. Another strategy is to apply Domain knowledge where feature values can be grouped based on common demographic or social characteristics. For example, Age could be translated into different age groups (e.g., Teenagers, Young Adults, etc.), or TOEFL scores are divided into proficiency levels. While the aforementioned methods are univariate, we also implement a multivariate approach using Decision tree. Following the motivation of MDP, we run CART [4] to search for the splits that minimize class information entropy. Note that we here consider the predicted labels from the black-box models as the target variable. The goal is to each sure the prediction on each combination of features is stable as possible. To avoid fine-grained intervals, we set the minimum number of samples for a split to be 30. Our experiment shows that we are still able to achieve 100% of Validity and Coverage in roughly the same amount of time. We therefore only present the remaining metrics in Table 5, which here demonstrates a comparable quality of explanations among different discretization strategies. Given that L2C performance is relatively insensitive to the choice of discretizers, we therefore suggest using Equal-frequency for which no labels or external knowledge is required. ", "publication_ref": ["b5", "b34", "b3", "b38", "b12", "b41", "b3"], "figure_ref": [], "table_ref": ["tab_2", "tab_4", "tab_4"]}, {"heading": "Feature Correlational Constraints", "text": "While the treatment of unary constraints is straightforward for heterogeneous datasets, we argue that this is not the case for binary constraints. For example, suggesting that a person get a Master's degree at precisely the age of 34 is unrealistically rigid. This issue indeed stems from the presence of continuous features. A direct solution is to allow for more flexible suggestions through discretization (e.g., suggesting an age range from 30 \u2212 40 instead of an exact value at 34). This indeed aligns with the generative mechanism of our L2C, which sets us apart from existing works. However, discretization is currently treated as a subroutine of internal processing, meaning that in the output examples, the values for the continuous features are still returned in the numerical format for the sake of consistency. Therefore, the best strategy would be to have the machine learning classifiers trained in the discretized feature space accordingly. Practitioners could then ignore the one-hot decoding stage and deploying L2C for this purpose would be effortless. In Appendix B, we demonstrate how L2C effectively addresses binary constraints in this scenario with success rates of 91.54% and 100.00% respectively on German Credit and Adult Income.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSION AND FUTURE WORK", "text": "In this paper, we study the challenges facing algorithmic recourse approaches in generating diverse counterfactual explanations: how diversity can be tackled without compromising the other desiderata of an explanation while preserving privacy against linkage attacks. We analyze how existing engines fail to resolve the tradeoffs among counterfactual constraints and fill the research gap with our novel framework L2C. Here we target a broad class of differentiable machine learning classifiers. To fit non-differentiable models in our framework, one could use policy gradient [49] or attempt to approximate such models as decision trees or random forests with a differentiable version [27,55]. L2C is currently proposed to deal with re-identification risks of the released examples. Defense against model stealing and membership inference attacks however remains exigent. Integrating differential privacy in the framework of L2C is one interesting research avenue, which we leave for future works to explore.", "publication_ref": ["b48", "b26", "b54"], "figure_ref": [], "table_ref": []}, {"heading": "A EXPERIMENTAL DETAILS A.1 Dataset statistics", "text": "\u2022 German Credit [10]: This dataset includes information of customers taking credit at a bank. The task is to classify a customer as a good (label 1) or bad (label 0) credit risk. \u2022 Adult Income [24]: The dataset was extracted from the US 1994 census data on adult incomes. The task is to classify if an individual's income exceeds $50,000 per year (label 1) or not (label 0). \u2022 Graduate Admission [1]: The set contains data of Indian students' applications to a Master's program. The original target variable is an ordinal variable on the scale of [0\u22121] indicating the chance of a student being admitted, where 1 indicates the highest chance. We set a threshold of 0.7 and re-categorize students as either \"having a higher chance\" (\u2265 0.70-label 1) or \"having a lower chance\" (< 0.70-label 0). The binary classification task is to determine if a student profile has a higher chance of being successful at their application. \u2022 Student Performance [7]: This dataset records the performance of students at two schools Gabriel Pereira and Mousinho da Silveira. The task is to predict if a student achieves a final score above average (label 1) or not (label 0). The train and test splits contain data of students from these two schools separately [5]. Table 7 summarizes the experimental settings for every dataset. Regarding the underlying black-box models, we experiment with Logistic Regression for the linear classifier and Neural Network for the non-linear classifier. We train linear classifiers on German Credit and Student Performance, and non-linear classifiers on Graduate Admission (with 3 layers and 40-dimensional hidden units) and Adult Income (with 3 layers and 30-dimensional hidden units). For each task, we further sample 20% random observations of the training sets as validation sets and train 5 black-box models with the same architecture but with different initializations. Since we have specified a fixed subset of immutable features, we therefore can derive the minimum sparsity level an explanation method should obey as Min Sparsity = No. immutable features Total no. features", "publication_ref": ["b9", "b23", "b0", "b6", "b4"], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "A.2 Model Design", "text": "We parameterize G and S with neural networks of 3 and 2 layers respectively. Each layer consists of a dense layer and a ReLU activation, except the last layer of S takes Sigmoid activation to produce a probability vector. The final layer of G is another dense layer that outputs a logit vector of the same dimension as the input, representing the counterfactual distribution. We set the sparsity loss coefficient = 1 \u2212 4 and use the same architecture for all tasks. We train our model with Adam optimizer for 200 epochs, at = 0.2 and a learning rate of 1 \u2212 4.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B FEATURE CONSTRAINTS OF DISCRETIZED BLACK-BOX MODELS", "text": "We here demonstrate how binary constraints can be effectively addressed within L2C framework for realistic explanations. We use the same model architecture and experimental setup, except that the data now is initially discretized and the black-box classifiers are trained on the discretized feature space. Not only would this help produce more realistic suggestions but it would also enhance privacy protection. We reuse the Equal-frequency strategy for consistency, yet note that practitioners are highly recommended to consider generalizing the data in such a way that satisfies the privacy metrics outlined in Section 5.2. We then run L2C models on German Credit and Adult Income datasets with feature correlations specified in Table 7: respectively the constraints Present residence \u2192 Age and Education level \u2192 Age. To enforce the increasing constraint on the child variable (i.e. Age), we further track the perturbation of the parent variable (i.e. Present residence, Education level) and apply the Eq. (5) to update the distribution of samples where the parent variable is indicated (by generator G) to increase. Table 6 reports the quality of explanations produced under both types of constraints, where Binary measures the proportion of examples C meeting the binary constraints. Here we again substantiate the effectiveness of L2C in balancing counterfactual constraints.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_6", "tab_5"]}, {"heading": "C AMORTIZED BASELINES", "text": "Table 8 compares L2C with popular amortized approaches: Feasible-VAE [30], CRUDS [9] and FastAR [52] across desiderata. We now provide the experimental setup for the amortized baseline models. Whereas the non-amortized algorithms are run directly on the testing sets, for amortized methods, we train the base generative models on the training sets and use the testing sets only for evaluation. We tune the base generative models under various different hyper-parameter settings via grid search and report the best results. We determine the best settings via two metrics: Coverage and Diversity. When there is a trade-off, Coverage is chosen to be the deciding criterion. Specifically, for Feasible-VAE [30], we tune the hidden dimensions of the VAE encoder within {10, 30, 50, 70, 90} and regularization term on Validity within {42, 62, 82, 102, 122}. For CRUDS [9], the base model is a Conditional Subspace Variational Auto-encoder [23]. In the original paper, the network only has 1 hidden layer of 64 nodes, which we find to be of low capacity. We thus experiment with 2 layers and different hidden dimensions within {16, 32, 64}. For FastAR [52], the hyper-parameters include manifold distance and entropy loss coefficient. Across datasets, Verma et al. [52] shows the best Coverage under = 0.1. We thus set = 0.1 as well in our experiments, while focusing on tuning the latter hyper-parameter within {0.01, 0.05, 0.1, 0.5, 1.0}. For the remaining hyper-parameters, we adopt the best values reported by the authors. A pre-trained FastAR model can only interpret one decision outcome chosen as the desired one (often the positive label). We must therefore train separate FastAR models on the positive and negative subsets and combine the results. We further find that although it is straightforward to obtain multiple generations in a single model, FastAR algorithm is optimized for one optimal counterfactual state for a given input. Thus in the hope of achieving better diversity, we train 100 different model initializations and accordingly collect a set of 100 explanations for evaluation. The inference time accumulates as a result, which is the reason why our reported results on time efficiency for FastAR are different from what are reported in the authors' paper.", "publication_ref": ["b29", "b8", "b51", "b29", "b8", "b22", "b51", "b51"], "figure_ref": [], "table_ref": ["tab_8"]}, {"heading": "D THE ROLE OF FEATURE SELECTOR", "text": "We now validate the importance of learning the local feature-based selection distribution via the feature selector S. We first remove S from L2C framework and replace the probability vector ( ) with a binary mask vector \u2208 [0, 1] where = 1 if \u2208 K (i.e., a mutable feature) and = 0 otherwise. We thus use in substitution of to update the counterfactual representations as previously done. We only optimize the generator G to learn the feature-based perturbation distribution, and the training objective Eq. (4) excludes the regularization term for sparsity accordingly. Figure 3 investigates the performance of L2C under this alternative setup, in comparison with the proposed method that jointly optimizes S and G. L2C still achieves 100% of Validity and Coverage, so we only report the relevant metrics.\nOne drawback of omitting the Selector component is that we lose the flexibility in tailoring the quality of counterfactual generations to potential user preferences. It is seen that the Selector introduces significant sparsity to gain an effective balance for the trade-off against diversity. Furthermore, these results support our claim about the role of the generator G in that the perturbation distribution alone can yield impressively diverse explanations with sparsity remaining under the required maximum level. This is a benefit of learning an entire feature distribution in that sometimes an output sample falls into the original input value i.e., = while combining adequately with other features.   ", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "E QUALITATIVE EXAMPLES", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F ADDITIONAL ANALYSIS ON DISCRETIZERS", "text": "In the main paper, we have proven that the quality of L2C explanations is relatively insensitive to the choice of discretizers. This means that we can achieve interpretability without compromising the desiderata of a counterfactual explanation. Here we conduct an additional privacy analysis for similar purpose. In Table 9, we report our privacy analysis results on German dataset across different discretization strategies. We reuse the settings reported in the paper. It is observed that there is a same pattern with the desiderata analysis: regardless of the choice of discretizers, the performance of L2C compared to the baseline methods remains relatively stable. For L2C, it is also possible to leverage different discretization methods on different features. In practice, this strategy is highly recommended based on user demand and/or human knowledge. In Tables 10 and 11 , we report the performance of L2C on Adult dataset under Mixed discretization strategy. Specifically, we randomly assign one of the four mentioned strategies (Equal Frequency/MDP/CART/Domain Knowledge) to every continuous feature. The results are averaged over 5 different random initializations. It is seen that the performance of L2C is well above the baselines where the privacy level remains rather stable again.  ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_9", "tab_10"]}, {"heading": "G DEGREE OF FEATURE MUTABILITY", "text": "Following the setting from previous works, we currently limit our discussion to binary states of mutability (i.e., whether a feature gets changed or not). However, in practice, actionability should further deal with the difficulty level associated with mutable features, also among their categorical levels. Although this extension remains beyond existing works and ours, it can be seen that L2C can be flexibly modified to handle these scenarios. We here sketch some ideas for future works to explore. Based on human experts, one can define a (constant) cost vector \u2208 [0, 1] that reflects the (relative) level of difficulty among mutable features (where is the number of mutable features and \n.\nAnother cost vector can also be defined for each categorical level of a feature. One could reuse our mechanism to handle Unary constraints in Equation (5) to explicitly force the model to generate more samples at the low-cost levels (See Lines 494 -520). Let denote the cost of category of feature , the perturbation distribution can be re-defined as\n( ) \u221d (1 \u2212 ) \u00d7 exp G ( ) =1 exp G ( ) , \u2200 = 1, ..., ,", "publication_ref": ["b4"], "figure_ref": [], "table_ref": []}, {"heading": "H L2C UNDER HIGH DIMENSIONALITY", "text": "Aware of the fact that high-dimensionality and sparsity can affect any systems' performance, we conduct an additional analysis to investigate the performance of L2C on a high-dimensional textual dataset. We choose the use case of Email Spam Detection 7 where 7 https://archive.ics.uci.edu/ml/datasets/spambase the feature set is the vocabulary of the entire corpus and each word is considered a binary feature (indicating whether a word appears in an email or not). A possible scenario is an attacker somehow gains access to the detector and would like to build a model explainer that can tell which word should be removed in order to fool the detector. This dataset contains up to 2, 136 features (after flattening), thus being extremely sparse. To make it comparable, we reuse the same model architecture and training settings in the other datasets. Table 12 reports the Validity and Inference Time (per input when generating a set of 100 samples) of L2C explanations on this dataset, compared with the other datasets of smaller scale.\nWe observe that for some extremely sparse data points, it is indeed difficult to find enough valid counterfactuals within the time budget of 5 minutes, leading to a total decrease in validity and an increase in the average inference time, compared to the smaller datasets. However, even for such a challenging dataset, L2C still works on a large number of inputs to achieve an overall accuracy of 87%. If we compare L2C with DiCE -the closest baseline in performance (also the most popular), we posit that L2C is more scalable since DiCE iteratively perturbs every single feature, hence would be catastrophically expensive when dealing with text data. Furthermore, we argue that if the dataset contains more of the categorical features, it poses the same serious issue to all counterfactual generation systems. Sparsity and curse of dimensionality are thus long-standing problems in this research area, which would result in a dedicated paper altogether. To address it may require more careful investigation into the model design and hyper-parameter tuning, which we leave open for future works. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "ACKNOWLEDGEMENT", "text": "Dinh Phung and Trung Le gratefully acknowledge the support by the US Airforce FA2386-21-1-4049 grant and the Australian Research Council ARC DP230101176 project. This does not imply endorsement by the funding agency of the research findings or conclusions. Any errors or misinterpretations in this paper are the sole responsibility of the authors.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": " ", "text": "( 12 , 18 ]\n \n187 ( 18 , 24 ] 224 ( 24 , 72 ] 230\nCredit Amount  ( 16 , 17 ] 179 ( 17 , 22 ] 181\nSchool absences ( -0 , 4 ] 466 ( 4 , 32 ] 183\nFirst period grade ( -0 , 10 ] 252 ( 10 , 13 ] 245 ( 13 , 19 ] 152\nSecond period grade ( -0 , 10 ] 228 ( 10 , 13 ] 269 ( 13 , 19 ] ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "A comparison of regression models for prediction of graduate admissions", "journal": "IEEE", "year": "2019", "authors": "Asfia Mohan S Acharya; Aneeta S Armaan;  Antony"}, {"ref_id": "b1", "title": "Alexandre Bolot, and S\u00e9bastien Gambs. 2020. Model extraction from counterfactual explanations", "journal": "", "year": "2020", "authors": "Ulrich A\u00efvodji"}, {"ref_id": "b2", "title": "Hiring by algorithm: predicting and preventing disparate impact", "journal": "Available at SSRN", "year": "2016", "authors": "Ifeoma Ajunwa; Sorelle Friedler; Carlos E Scheidegger; Suresh Venkatasubramanian"}, {"ref_id": "b3", "title": "Classification and regression trees", "journal": "", "year": "2017", "authors": "Leo Breiman; H Jerome; Richard A Friedman; Charles J Olshen;  Stone"}, {"ref_id": "b4", "title": "Counterfactual Plans under Distributional Ambiguity", "journal": "", "year": "2022", "authors": "Ngoc Bui; Duy Nguyen; Viet Anh Nguyen"}, {"ref_id": "b5", "title": "On finding optimal discretizations for two attributes", "journal": "Springer", "year": "1998", "authors": "S Bogdan; Sinh Hoa Chlebus;  Nguyen"}, {"ref_id": "b6", "title": "Using data mining to predict secondary school student performance", "journal": "", "year": "2008", "authors": "Paulo Cortez; Alice Maria Gon\u00e7alves Silva"}, {"ref_id": "b7", "title": "Multiobjective counterfactual explanations", "journal": "Springer", "year": "2020", "authors": "Susanne Dandl; Christoph Molnar; Martin Binder; Bernd Bischl"}, {"ref_id": "b8", "title": "Cruds: Counterfactual recourse using disentangled subspaces", "journal": "", "year": "2020", "authors": "Michael Downs; Jonathan L Chu; Yaniv Yacoby; Finale Doshi-Velez; Weiwei Pan"}, {"ref_id": "b9", "title": "UCI Machine Learning Repository", "journal": "", "year": "2017", "authors": "Dheeru Dua; Casey Graff"}, {"ref_id": "b10", "title": "Calibrating noise to sensitivity in private data analysis", "journal": "Springer", "year": "2006", "authors": "Cynthia Dwork; Frank Mcsherry; Kobbi Nissim; Adam Smith"}, {"ref_id": "b11", "title": "Protecting privacy using kanonymity", "journal": "Journal of the American Medical Informatics Association", "year": "2008", "authors": "Khaled El Emam; Fida Kamal Dankar"}, {"ref_id": "b12", "title": "Multi-interval discretization of continuousvalued attributes for classification learning", "journal": "", "year": "1993", "authors": "Usama Fayyad; Keki Irani"}, {"ref_id": "b13", "title": "Faster Privacy Accounting via Evolving Discretization", "journal": "", "year": "2022", "authors": "Badih Ghazi; Pritish Kamath; Ravi Kumar; Pasin Manurangsi"}, {"ref_id": "b14", "title": "Legal technology: Criminal justice algorithms: Ai in the courtroom", "journal": "The", "year": "2018", "authors": "Richard Gifford"}, {"ref_id": "b15", "title": "The privacy issue of counterfactual explanations: explanation linkage attacks", "journal": "", "year": "2022", "authors": "Sofie Goethals; Kenneth S\u00f6rensen; David Martens"}, {"ref_id": "b16", "title": "Numerical composition of differential privacy", "journal": "Advances in Neural Information Processing Systems", "year": "2021", "authors": "Sivakanth Gopi; Yin Tat Lee; Lukas Wutschitz"}, {"ref_id": "b17", "title": "Counterfactual explanations and how to find them: literature review and benchmarking", "journal": "Data Mining and Knowledge Discovery", "year": "2022", "authors": "Riccardo Guidotti"}, {"ref_id": "b18", "title": "Categorical reparameterization with gumbel-softmax", "journal": "", "year": "2016", "authors": "Eric Jang; Shixiang Gu; Ben Poole"}, {"ref_id": "b19", "title": "Modelagnostic counterfactual explanations for consequential decisions", "journal": "", "year": "2020", "authors": " Amir-Hossein; Gilles Karimi; Borja Barthe; Isabel Balle;  Valera"}, {"ref_id": "b20", "title": "A survey of algorithmic recourse: definitions, formulations, solutions, and prospects", "journal": "", "year": "2020", "authors": " Amir-Hossein; Gilles Karimi; Bernhard Barthe; Isabel Sch\u00f6lkopf;  Valera"}, {"ref_id": "b21", "title": "Algorithmic recourse: from counterfactual explanations to interventions", "journal": "", "year": "2021", "authors": "Bernhard Amir-Hossein Karimi; Isabel Sch\u00f6lkopf;  Valera"}, {"ref_id": "b22", "title": "Learning latent subspaces in variational autoencoders", "journal": "", "year": "2018", "authors": "Jack Klys; Jake Snell; Richard Zemel"}, {"ref_id": "b23", "title": "Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid", "journal": "", "year": "1996", "authors": "Ron Kohavi"}, {"ref_id": "b24", "title": "Determinantal point processes for machine learning", "journal": "Foundations and Trends\u00ae in Machine Learning", "year": "2012", "authors": "Alex Kulesza; Ben Taskar"}, {"ref_id": "b25", "title": "Benchmarking state-of-the-art classification algorithms for credit scoring: An update of research", "journal": "European Journal of Operational Research", "year": "2015", "authors": "Stefan Lessmann; Bart Baesens;  Hsin-Vonn; Lyn C Seow;  Thomas"}, {"ref_id": "b26", "title": "FO-CUS: Flexible optimizable counterfactual explanations for tree ensembles", "journal": "", "year": "2022", "authors": "Ana Lucic; Harrie Oosterhuis; Hinda Haned; Maarten De Rijke"}, {"ref_id": "b27", "title": "l-diversity: Privacy beyond k-anonymity", "journal": "ACM Transactions on Knowledge Discovery from Data (TKDD)", "year": "2007", "authors": "Ashwin Machanavajjhala; Daniel Kifer; Johannes Gehrke; Muthuramakrishnan Venkitasubramaniam"}, {"ref_id": "b28", "title": "The concrete distribution: A continuous relaxation of discrete random variables", "journal": "", "year": "2016", "authors": "Andriy Chris J Maddison; Yee Whye Mnih;  Teh"}, {"ref_id": "b29", "title": "Preserving causal constraints in counterfactual explanations for machine learning classifiers", "journal": "", "year": "2019", "authors": "Divyat Mahajan; Chenhao Tan; Amit Sharma"}, {"ref_id": "b30", "title": "Privacy-preserving data mining: methods, metrics, and applications", "journal": "IEEE Access", "year": "2017", "authors": "Ricardo Mendes; P Jo\u00e3o;  Vilela"}, {"ref_id": "b31", "title": "Model reconstruction from model explanations", "journal": "", "year": "2019", "authors": "Smitha Milli; Ludwig Schmidt; D Anca; Moritz Dragan;  Hardt"}, {"ref_id": "b32", "title": "Explaining machine learning classifiers through diverse counterfactual explanations", "journal": "", "year": "2020", "authors": "Amit Ramaravind K Mothilal; Chenhao Sharma;  Tan"}, {"ref_id": "b33", "title": "How to break anonymity of the netflix prize dataset", "journal": "", "year": "2006", "authors": "Arvind Narayanan; Vitaly Shmatikov"}, {"ref_id": "b34", "title": "Quantization of real value attributesrough set and boolean reasoning approach", "journal": "Citeseer", "year": "1995-09-28", "authors": "H Son; Andrzej Nguyen;  Skowron"}, {"ref_id": "b35", "title": "Information-theoretic source code vulnerability highlighting", "journal": "IEEE", "year": "2021", "authors": "Trung Van Nguyen; Olivier De Le; Paul Vel; John Montague; Dinh Grundy;  Phung"}, {"ref_id": "b36", "title": "Seyit Camtepe, Paul Quirk, and Dinh Phung. 2022. An Information-Theoretic and Contrastive Learning-based Approach for Identifying Code Statements Causing Software Vulnerability", "journal": "", "year": "2022", "authors": "Trung Van Nguyen; Chakkrit Le; John Tantithamthavorn; Hung Grundy;  Nguyen"}, {"ref_id": "b37", "title": "Learning modelagnostic counterfactual explanations for tabular data", "journal": "", "year": "2020", "authors": "Martin Pawelczyk; Klaus Broelemann; Gjergji Kasneci"}, {"ref_id": "b38", "title": "Data discretization: taxonomy and big data challenge", "journal": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery", "year": "2016", "authors": "Sergio Ram\u00edrez-Gallego; Salvador Garc\u00eda; H\u00e9ctor Mouri\u00f1o-Tal\u00edn; David Mart\u00ednez-Rego; Ver\u00f3nica Bol\u00f3n-Canedo; Amparo Alonso-Betanzos; Jos\u00e9 Manuel Ben\u00edtez; Francisco Herrera"}, {"ref_id": "b39", "title": "MCCE: Monte Carlo sampling of realistic counterfactual explanations", "journal": "", "year": "2021", "authors": "Annabelle Redelmeier; Martin Jullum; Kjersti Aas; Anders L\u00f8land"}, {"ref_id": "b40", "title": "Explaining the predictions of any classifier", "journal": "", "year": "2016", "authors": "Sameer Marco Tulio Ribeiro; Carlos Singh;  Guestrin"}, {"ref_id": "b41", "title": "Modeling by shortest data description", "journal": "Automatica", "year": "1978", "authors": "Jorma Rissanen"}, {"ref_id": "b42", "title": "Efficient search for diverse coherent explanations", "journal": "", "year": "2019", "authors": "Chris Russell"}, {"ref_id": "b43", "title": "Protecting privacy when disclosing information: k-anonymity and its enforcement through generalization and suppression", "journal": "", "year": "1998", "authors": "Pierangela Samarati; Latanya Sweeney"}, {"ref_id": "b44", "title": "Certifai: A common framework to provide explanations and analyse the fairness and robustness of black-box models", "journal": "", "year": "2020", "authors": "Shubham Sharma; Jette Henderson; Joydeep Ghosh"}, {"ref_id": "b45", "title": "On the privacy risks of model explanations", "journal": "", "year": "2021", "authors": "Reza Shokri; Martin Strobel; Yair Zick"}, {"ref_id": "b46", "title": "Membership inference attacks against machine learning models", "journal": "IEEE", "year": "2017", "authors": "Reza Shokri; Marco Stronati; Congzheng Song; Vitaly Shmatikov"}, {"ref_id": "b47", "title": "Counterfactual explanations of machine learning predictions: opportunities and challenges for AI safety", "journal": "SafeAI@ AAAI", "year": "2019", "authors": "Kacper Sokol; A Peter;  Flach"}, {"ref_id": "b48", "title": "Policy gradient methods for reinforcement learning with function approximation", "journal": "Advances in neural information processing systems", "year": "1999", "authors": "S Richard; David Sutton; Satinder Mcallester; Yishay Singh;  Mansour"}, {"ref_id": "b49", "title": "Simple demographics often identify people uniquely", "journal": "Health", "year": "2000", "authors": "Latanya Sweeney"}, {"ref_id": "b50", "title": "Counterfactual explanations for machine learning: A review", "journal": "", "year": "2020", "authors": "Sahil Verma; John Dickerson; Keegan Hines"}, {"ref_id": "b51", "title": "Amortized Generation of Sequential Algorithmic Recourses for Black-Box Models", "journal": "", "year": "2022", "authors": "Sahil Verma; Keegan Hines; John P Dickerson"}, {"ref_id": "b52", "title": "Seyit Camtepe, and Dinh Phung. 2023. An Additive Instance-Wise Approach to Multi-class Model Interpretation", "journal": "", "year": "", "authors": "Vy Vo; Trung Van Nguyen;  Le; Reza Quan Hung Tran;  Haf"}, {"ref_id": "b53", "title": "Grade: Machine learning support for graduate admissions", "journal": "Ai Magazine", "year": "2014", "authors": "Austin Waters; Risto Miikkulainen"}, {"ref_id": "b54", "title": "Deep neural decision trees", "journal": "ICML Workshop on Human Interpretability in Machine Learning", "year": "2018", "authors": "Yongxin Yang; Irene Garcia Morillo; Timothy M Hospedales"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure1: For illustration purposes only, all features are assumed mutable in the figure. We first discretize the continuous features of input and one-hot encode all features into representations . For every feature , the generator G learns a local perturbation distribution Cat( | ) so that together they form a distribution of diverse counterfactual representations . Simultaneously, the selector S learns to output the distribution Multi-Bernoulli( | ) capturing the probability of each feature being modified. Every feature sample pair ( , ) is passed through an operation in the blue box, which decides whether to accept the change being made to the feature given by . The output is then decoded into the representations compatible with the black-box system. G and S are jointly trained via back-propagation according to Eq. (4). Intuitively, G aims to construct a \"bridge\" across the decision boundary travelling from the input to a local space of counterfactuals.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Analysis of L2C performance when the selector S is removed.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "=1 = 1 )1. Recall in Section 3.3 that the feature-based selection distribution (i.e., Bernoulli( | )) for every feature is given as( ) = 1 1 + exp \u2212 S ( ) .We want the features with high-cost values to be sampled less likely, meaning to have a low probability of being changed. To lower such a probability, while facilitating differentiable training, for every feature , one can re-define the feature-based selection distribution as (1 \u2212 ) \u00d7 ( ), or( ) = (1 \u2212 ) \u00d7 1 1 + exp \u2212 S ( )", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Description of quantitative evaluation metrics. C denotes a set of counterfactual examples generated by an algorithmic recourse approach for a given input instance.", "figure_data": "DesiderataMetricDescriptionValidityValidityProportion of samples in C can counter the orig-inal black-box decision outcome.CoverageCoverage = 100% if there exists at least 1 validcounterfactual in C.Sparsity/SparsityProportion of features kept unchanged, aver-Actionabilityaged over the number of samples in C.DiversityDiversityHamming distance of a pair of counterfactualsamples across all features where numerical fea-tures are discretized. The metric is averagedover all pairs of samples in C.Sparsity -DiversityHarmonicF-measure of Diversity and Sparsity =Balancemean2 \u2022 Diversity \u2022 Sparsity/(Diversity +Sparsity).PlausibilityUnaryProportion of examples C meeting the unarymonotonic constraints, averaged over the num-ber of features subject to constraints."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "reports the average results over 5 model initializations. Appendix E provides several illustrative examples for qualitative assessment.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Desirability of counterfactual explanation methods. \u2193 Lower is better. \u2191 Higher is better. Bold / Underline indicates the best / second-best performance for each dataset. Time records total inference time in seconds.", "figure_data": "MethodSparsity (%)\u2191 Diversity (%)\u2191 Harmonic Mean (%)\u2191 Validity (%)\u2191 Coverage (%)\u2191 Unary (%)\u2191 Time(s)\u2193German Credit (Logistic Regression) -Min Sparsity: 20.00%L2C (Ours)61.3537.3146.39100.00100.0099.0618DICE-Random88.2315.2926.06100.00100.0090.811,150DICE-Genetic43.4537.5640.2962.8790.2456.6617,615COPA57.8818.8828.4744.0044.0084.3117,583MCCE28.7633.4030.9148.74100.0058.762Adult Income (Neural Network) -Min Sparsity: 30.77%L2C (Ours)45.7028.1134.80100.00100.0097.62444DICE-Random89.269.0516.44100.00100.0087.1512,332DICE-Genetic41.4826.2732.1492.64100.0072.70505,174MCCE24.934.587.7430.6374.7645.7998Graduate Admission (Neural Network) -Min Sparsity: 14.29%L2C (Ours)42.2337.9039.94100.00100.00100.004DICE-Random66.2530.9342.15100.00100.0085.30412DICE-Genetic23.0547.5431.0492.91100.0066.696,171MCCE17.3922.9819.5143.7984.6079.111Student Performance (Logistic Regression) -Min Sparsity: 38.57%L2C (Ours)55.3229.5438.51100.00100.00100.006DICE-Random87.6013.6423.60100.00100.0098.992,518DICE-Genetic39.2039.8838.5484.83100.0060.773,406COPA50.4525.2833.6867.2667.2695.3218,774MCCE25.9724.9725.4660.9893.1067.701private information of the data subjects because profiles in the sameequivalence class are similar (very few distinct values in sensitiveattributes), or the adversary has some background knowledge thatany help narrow down possible values."}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Successful attacks on counterfactual explanation methods. Bold / Underline indicates the lowest / secondlowest privacy risk for each dataset. 1-Diversity is evaluated on 2 most sensitive attributes as shown in the columns 2 & 3.", "figure_data": "Method1-Anoy.\u21931-Diversity\u21931-Map\u2193German CreditL2C (Ours)62.15%67.09% 71.60% 0.21%DICE-Random55.15%82.75% 89.96% 23.67%MCCE62.83%65.40% 76.95% 26.64%DICE-Genetic15.36%90.19% 95.80%N/ACOPA87.61%89.41% 89.24%N/AAdult IncomeL2C (Ours)5.07%14.90% 39.58% 0.00%DICE-Random72.21%91.28% 93.07%3.31%MCCE15.19%66.42% 72.36%1.77%DICE-Genetic15.11%65.32% 87.54%N/A"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Desirability of L2C counterfactual explanations under various discretization strategies.", "figure_data": "StrategySparsityDiversityHarmonicUnary (%)\u2191(%)\u2191(%)\u2191Mean (%)\u2191German Credit -Min Sparsity: 20.00%Equal Freq.  *61.3537.3146.3999.06MDP61.5835.0044.63100.00CART60.9239.8548.18100.00Domain Know. 61.7337.5646.70100.00Graduate Admission -Min Sparsity: 14.29%Equal Freq.  *42.2337.9039.94100.00MDP58.2041.3441.56100.00CART41.8041.3441.56100.00Domain Know. 42.1742.3042.22100.00Student Performance -Min Sparsity: 38.57%Equal Freq.  *55.3229.5438.51100.00MDP55.5728.5037.67100.00CART55.0727.7936.94100.00Domain Know. 55.4731.5740.23100.00"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Desirability of counterfactual examples generated from L2C for explaining a discretized black-box classifiers.", "figure_data": "HarmonicValidityUnaryBinaryMean (%)\u2191(%)\u2191(%)\u2191(%)\u2191German Credit48.56%100.00%98.38%91.54%Adult Income23.89%100.00%100.00%100.00%"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Dataset statistics. * Features subject to non-decreasing constraints.", "figure_data": "DatasetGermanAdultGraduateStudentCreditIncomeAdmissionPerformanceTrain/Dev/Test640/160/20028942/7235/9045320/80/100339/84/226No. features2013714Foreign workerRaceUniversityMother's edu.Immutable featuresNo. liable people Personal statusSex Native countryratingFather's edu. Family edu. supportPurposeMarital statusFirst period gradeAgeAgeResearchAgeFeature constraints  *Present employment Present residenceEducation levelexperienceDurationFeature correlationsIncreasing Age increasesIncreasing Education levelPresent residenceincreases AgeAge, JobAgeForeign workerSexQuasi-identifiersPersonal status Present employmentRace RelationshipPresent residenceMarital statusProperty, HousingSensitive featuresCredit amount Savings accountCapital gain Capital lossBlack-box modelLogistic RegressionNeural NetworkNeural NetworkLogistic RegressionTest accuracy67.00%85.53%90.60%94.69%Min Sparsity20.00%30.77%14.29%38.57%"}, {"figure_label": "14", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "17 illustrate some examples of our generated counterfactuals for each dataset. For illustration purposes only, we report the discretized values for numerical features where the edge values of each numerical interval are rounded to the nearest whole number. Immutable features are italicized.", "figure_data": ""}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Desirability of amortized counterfactual explanation methods. \u2193 Lower is better. \u2191 Higher is better. Bold / Underline indicates the best / second-best performance for each dataset. Time records total inference time in seconds.", "figure_data": "MethodSparsity (%)\u2191 Diversity (%)\u2191 Harmonic Mean (%)\u2191 Validity (%)\u2191 Coverage (%)\u2191 Unary (%)\u2191 Time(s)\u2193German Credit (Logistic Regression) -Min Sparsity: 20.00%L2C (Ours)61.3537.3146.39100.00100.0099.0618FastAR95.930.681.3395.7995.7999.8710,605F-VAE45.931.593.06100.00100.0074.4136CRUDS29.7214.2119.1860.0060.0071.3142,920Graduate Admission (Neural Network) -Min Sparsity: 14.29%L2C (Ours)42.2337.9039.94100.00100.00100.004FastAR27.971.322.3587.4187.4185.285,405F-VAE7.711.481.84100.00100.0044.6016CRUDS11.3117.3813.4060.2876.0049.0021,460Student Performance (Logistic Regression) -Min Sparsity: 38.57%L2C (Ours)55.3229.5438.51100.00100.00100.006FastAR81.861.442.7797.7197.7199.7616,370F-VAE27.377.6811.92100.00100.0069.6836CRUDS24.0633.6428.5562.12100.00100.0039,57160%German CreditW/o Selector Proposed method50%40%30%20%10%0%SparsityDiversity Harmonic MeanGraduate Admission40%30%20%10%0%SparsityDiversity Harmonic MeanStudent Performance50%40%30%20%10%0%SparsityDiversity Harmonic Mean"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Privacy of L2C counterfactual explanations under various discretization strategies on German dataset.", "figure_data": "Strategy1-Anoy. \u2193 1-Diversity \u21931-Map \u2193Equal Freq.  *62.15%67.09% 71.60% 0.21%MDP59.03%66.52% 68.40% 0.10%CART65.88%69.24% 74.25% 0.18%Domain Know. 60.85%65.84% 70.07% 0.23%"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Desirability of L2C counterfactual explanations under mixed discretization on Adult dataset. * Proposed method. Strategy Sparsity (%)\u2191 Diversity (%)\u2191 Harmonic Mean (%)\u2191 Validity (%)\u2191 Coverage (%)\u2191 Unary (%)\u2191 Time (s)\u2193 Equal Freq.", "figure_data": "45.7028.1134.80100.00100.0097.62444Mixed43.6238.8341.00100.00100.00100.00378"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Privacy of L2C counterfactual explanations under mixed discretization on Adult dataset. * Proposed method.", "figure_data": "Strategy1-Anoy. \u2193 1-Diversity \u2193 1-Map \u2193Equal Freq.  *5.07%14.90% 39.58%0.00%Mixed5.53%15.39% 38.43%0.00%"}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Validity and Time Efficiency of L2C w.r.t feature size.", "figure_data": "DatasetNo. Features Validity Time per inputGraduate Admission34100%40 msStudent Performance48100%30 msGerman Credit78100%90 msAdult Income109100%50 msSpam Detection2,13687%13 s"}, {"figure_label": "16", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Counterfactual examples from Graduate Admission dataset.", "figure_data": "Graduate AdmissionOriginal inputCounterfactuals(Low chance)(High chance)GRE Score290 -312 290 -312 312 -323 290 -312 290 -312 290 -312TOEFL Score92 -104 104 -110 104 -110 110 -120 110 -12092 -104"}], "formulas": [{"formula_id": "formula_0", "formula_text": "= (1 \u2212 ) +", "formula_coordinates": [4.0, 145.45, 617.03, 49.09, 7.93]}, {"formula_id": "formula_1", "formula_text": "( ) = exp G ( ) =1 exp G ( ) , \u2200 = 1, ..., .(1)", "formula_coordinates": [5.0, 108.78, 96.03, 185.81, 25.94]}, {"formula_id": "formula_2", "formula_text": "( ) = 1 1 + exp \u2212 S ( ) .", "formula_coordinates": [5.0, 131.72, 158.21, 92.54, 21.19]}, {"formula_id": "formula_3", "formula_text": "min E CE( ( ), \u2032 ) + E \u2225 ( )\u2225 1 ,(2)", "formula_coordinates": [5.0, 95.29, 342.39, 199.29, 11.88]}, {"formula_id": "formula_4", "formula_text": "+ (2 \u22121) ( \u2212 ) 2", "formula_coordinates": [5.0, 144.55, 464.64, 54.07, 14.42]}, {"formula_id": "formula_5", "formula_text": "= (1 \u2212 ) + \u2211\ufe01 =1 + (2 \u2212 1)( \u2212 ) 2 . (3", "formula_coordinates": [5.0, 92.74, 550.51, 198.67, 24.24]}, {"formula_id": "formula_6", "formula_text": ")", "formula_coordinates": [5.0, 291.41, 557.06, 3.17, 7.94]}, {"formula_id": "formula_7", "formula_text": "min E CE( ( ), \u2032 ) + E \u2225 ( )\u2225 1 .(4)", "formula_coordinates": [5.0, 94.32, 628.18, 200.26, 11.88]}, {"formula_id": "formula_8", "formula_text": "( ) \u221d 1 C ( ) \u00d7 exp G ( ) =1 exp G ( ) , \u2200 = 1, ..., ,(5)", "formula_coordinates": [5.0, 355.83, 417.67, 202.91, 25.95]}, {"formula_id": "formula_9", "formula_text": "= exp (log ( ) + )/ =1 exp (log ( ) + )/ .", "formula_coordinates": [5.0, 380.51, 668.95, 127.66, 25.94]}, {"formula_id": "formula_10", "formula_text": "= exp{ log ( ) + 1 / } exp{ log(1 \u2212 ( )) + 0 }/ } + exp{ log ( ) + 1 / )} .", "formula_coordinates": [6.0, 62.4, 205.65, 231.45, 22.96]}, {"formula_id": "formula_11", "formula_text": "L2C DICE-R MCCE DICE-G COPA 0% 25% 50% 75% 100% 1-Diversity (1) German Credit + CF-K Raw L2C DICE-R MCCE DICE-G 0% 25% 50% 75% 100% Adult Income L2C DICE-R MCCE DICE-G COPA 0% 25% 50% 75% 100% 1-Diversity (2) L2C DICE-R MCCE DICE-G 0% 25% 50% 75% 100% L2C DICE-R MCCE DICE-G COPA 0% 25% 50% 75% 100% Validity Loss L2C DICE-R MCCE DICE-G 0% 20% 40% 60% L2C DICE-R MCCE -10% 0% 10% 20% 1-Map L2C DICE-R MCCE -10% 0% 10% 20%", "formula_coordinates": [9.0, 77.32, 289.52, 187.02, 208.74]}, {"formula_id": "formula_12", "formula_text": "( ) \u221d (1 \u2212 ) \u00d7 exp G ( ) =1 exp G ( ) , \u2200 = 1, ..., ,", "formula_coordinates": [15.0, 88.06, 596.93, 182.96, 25.94]}], "doi": "10.1145/3580305.3599343"}