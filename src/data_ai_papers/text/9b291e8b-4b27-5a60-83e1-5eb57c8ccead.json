{"title": "Document Summarization Based on Data Reconstruction", "authors": "Zhanying He; Chun Chen; Jiajun Bu; Can Wang; Lijun Zhang; Deng Cai; Xiaofei He", "pub_date": "", "abstract": "Document summarization is of great value to many real world applications, such as snippets generation for search results and news headlines generation. Traditionally, document summarization is implemented by extracting sentences that cover the main topics of a document with a minimum redundancy. In this paper, we take a different perspective from data reconstruction and propose a novel framework named Document Summarization based on Data Reconstruction (DSDR). Specifically, our approach generates a summary which consist of those sentences that can best reconstruct the original document. To model the relationship among sentences, we introduce two objective functions: (1) linear reconstruction, which approximates the document by linear combinations of the selected sentences; (2) nonnegative linear reconstruction, which allows only additive, not subtractive, linear combinations. In this framework, the reconstruction error becomes a natural criterion for measuring the quality of the summary. For each objective function, we develop an efficient algorithm to solve the corresponding optimization problem. Extensive experiments on summarization benchmark data sets DUC 2006 and DUC 2007 demonstrate the effectiveness of our proposed approach.", "sections": [{"heading": "Introduction", "text": "With the explosive growth of the Internet, people are overwhelmed by a large number of accessible documents. Summarization can represent the document with a short piece of text covering the main topics, and help users sift through the Internet, catch the most relevant document, and filter out redundant information. So document summarization has become one of the most important research topics in the natural language processing and information retrieval communities.\nIn recent years, automatic summarization has been applied broadly in varied domains. For example, search engines can provide users with snippets as the previews of the document contents (Turpin et al. 2007;Huang, Liu, and Chen 2008;Cai et al. 2004;He et al. 2007). News sites usually describe hot news topics in concise headlines to facilitate browsing. Both the snippets and headlines are specific forms of document summary in practical applications.\nMost of the existing generic summarization approaches use a ranking model to select sentences from a candidate set (Brin and Page 1998;Kleinberg 1999;Wan and Yang 2007). These methods suffer from a severe problem that top ranked sentences usually share much redundant information. Although there are some methods (Conroy and O'leary 2001;Park et al. 2007;Shen et al. 2007) trying to reduce the redundancy, selecting sentences which have both good coverage and minimum redundancy is a non-trivial task.\nIn this paper, we propose a novel summarization method from the perspective of data reconstruction. As far as we know, our approach is the first to treat the document summarization as a data reconstruction problem. We argue that a good summary should consist of those sentences that can best reconstruct the original document. Therefore, the reconstruction error becomes a natural criterion for measuring the quality of summary. We propose a novel framework called Document Summarization based on Data Reconstruction (DSDR) which finds the summary sentences by minimizing the reconstruction error. DSDR firstly learns a reconstruction function for each candidate sentence of an input document and then obtains the error formula by that function. Finally it obtains an optimal summary by minimizing the reconstruction error. From the geometric interpretation, DSDR tends to select sentences that span the intrinsic subspace of candidate sentence space so that it is able to cover the core information of the document.\nTo model the relationship among sentences, we discuss two kinds of reconstruction. The first one is linear reconstruction, which approximates the document by linear combinations of the selected sentences. Optimizing the corresponding objective function is achieved through a greedy method which extracts sentences sequentially. The second one is non-negative linear reconstruction, which allows only additive, not subtractive, combinations among the selected sentences. Previous studies have shown that there is psychological and physiological evidence for parts-based representation in the human brain (Palmer 1977;Wachsmuth, Oram, and Perrett 1994;Cai et al. 2011). Naturally, a document summary should consist of the parts of sentences. With the nonnegative constraints, our method leads to parts-based reconstruction so that no redundant information needs to be subtracted from the combination. We formulate the nonnegative linear reconstruction as a convex optimization problem and design a multiplicative updating algorithm which guarantees converging monotonically to a global minima.\nExtensive experiments on summarization benchmark data sets DUC 2006 and DUC 2007 demonstrate the effectiveness of our proposed approach.", "publication_ref": ["b28", "b13", "b2", "b10", "b0", "b14", "b30", "b7", "b22", "b25", "b21", "b29", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "Related work", "text": "Recently, lots of extractive document summarization methods have been proposed. Most of them involve assigning salient scores to sentences of the original document and composing the result summary of the top sentences with the highest scores. The computation rules of salient scores can be categorized into three groups (Hu, Sun, and Lim 2008): feature based measurements, lexical chain based measurements and graph based measurements. In (Wang et al. 2008), the semantic relations of terms in the same semantic role are discovered by using the WordNet (Miller 1995). A tree pattern expression for extracting information from syntactically parsed text is proposed in (Choi 2011). Algorithms like PageRank (Brin and Page 1998) and HITS (Kleinberg 1999) are used in the sentence score propagation based on the graph constructed based on the similarity between sentences. Wan and Yang (2007) show that graph based measurements can also improve the single-document summarization by integrating multiple documents of the same topic.\nMost of these scoring-based methods have to incorporate with the adjustment of word weights which is one of the most important factors that influence the summarization performance (Nenkova, Vanderwende, and McKeown 2006). So much work has been studied on how to extract sentences without saliency scores. Inspired by the latent semantic indexing (LSA), the singular value decomposition (SVD) is used to select highly ranked sentences for generic document summarization (Gong and Liu 2001). Harabagiu and Lacatusu (2005) analyze five different topic representations and propose a novel topic representation based on topic themes. Wang et al. (2008) use the symmetric non-negative matrix factorization (SNMF) to cluster sentences into groups and select sentences from each group for summarization.", "publication_ref": ["b12", "b31", "b18", "b6", "b0", "b14", "b30", "b20", "b7", "b9", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "The Proposed Framework", "text": "Most of the existing summarization methods aim to obtain the summary which covers the core information of the document. In this paper, we study the summarization from a data reconstruction perspective. We believe that a good summary should contain those sentences that can be used to reconstruct the document as well as possible, namely, minimizing the reconstruction error.\nIn this section, we describe the details of our proposed framework Document Summarization based on Data Reconstruction (DSDR) which minimizes the reconstruction error for summarization. The algorithm procedure of DSDR is as follows:\n\u2022 After stemming and stop-word elimination, we decompose the document into individual sentences and create a weighted term-frequency vector for every sentence. All the sentences form the candidate set.\n\u2022 For any sentence in the document, DSDR selects the related sentences from the candidate set to reconstruct the given sentence by learning a reconstruction function for the sentence. \u2022 For the entire document (or, a set of documents), DSDR aims to find an optimal set of representative sentences to approximate the entire document (or, the set of documents), by minimizing the reconstruction error.\nWe denote the candidate sentence set as\nV = [v 1 , v 2 , . . . , v n ]\nT where v i \u2208 R d is a weighted termfrequency vector for sentence i. Here notice that, we use V to represent both the matrix and the candidate set {v i }. Suppose there are totally d terms and n sentences in the document, we will have a matrix V in the size of n \u00d7 d. We denote the summary sentence set as X = [x 1 , x 2 , . . . , x m ] T with m < n and X \u2282 V .\nGiven a sentence v i \u2208 V , DSDR attempts to represent it with a reconstruction function f i (X) given the selected sentence set X. Denoting the parameters of f i as a i , we obtain the reconstruction error of v i as:\nL(v i , f i (X; a i )) = v i \u2212 f i (X; a i ) 2 , (1\n)\nwhere \u2022 is the L 2 -norm. By minimizing the sum of reconstruction errors over all the sentences in the document, DSDR picks the optimal set of representative sentences. The objective function of DSDR can be formally defined as:\nmin X,ai n i=1 v i \u2212 f i (X; a i ) 2 .\n(2)\nIn the following, we will discuss two types of the reconstruction function f i (X; a i ), namely, linear reconstruction and nonnegative linear reconstruction.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Linear Reconstruction", "text": "First we define the reconstruction functions f i (X) as a linear function:\nf i (X; a i ) = m j=1 x j a ij , X = [x 1 , x 2 , . . . , x m ] T . (3)\nThen a candidate sentence v i can be approximately represented as:\nv i \u2248 m j=1 x j a ij , 1 \u2264 i \u2264 n.\nNow, the reconstruction error of the document can be obtained as:\nn i=1 v i \u2212 X T a i 2\nThe solution from minimizing the above equation often exhibits high variance and results in high generalization error especially when the dimension of sentence vectors is smaller than the number of sentences. The variance can be reduced by shrinking the coefficients a i , if we impose a penalty on its size. Inspired by ridge regression (Hoerl and Kennard 1970), we penalize the coefficients of linear reconstruction error in DSDR as follows: min\nX,A n i=1 v i \u2212 X T a i 2 + \u03bb a i 2 s.t. X \u2282 V, |X| = m A = [a 1 , a 2 , . . . , a n ] T \u2208 R n\u00d7m .(4)\nThe set {x i } includes the selected representative sentences from the original candidate sentence set V and will be used as the document summary finally. \u03bb is the regularization parameter controlling the amount of shrinkage. The optimization problem in Eq. (4) faces two combinatorial challenges: (1) Evaluating the best reconstruction error of one candidate sentence v i , we would find the optimal X with size of m out of exponentially many options. (2) The optimal set for v i is usually not optimal for v j . So to reconstruct all the candidate sentences, we would have to search over an exponential number of possible sets to determine the unique optimal X. Actually, a similar problem that selects m < n basic vectors from n candidates to approximate a single vector in the least squares criterion has been proved to be NP hard (Natarajan 1995).\nThe optimization problem in Eq. ( 4) is equivalent to the following problem (Yu, Bi, and Tresp 2006):\nmin X J = Tr[V (X T X + \u03bbI) \u22121 V T ] s.t. X \u2282 V, |X| = m (5)\nwhere V is the candidate sentence set, X is the selected sentence set, I is the identity matrix, and Tr[\u2022] is the matrix trace calculation. Please see (Yu, Bi, and Tresp 2006) for the detailed derivation from Eq. (4) to Eq. (5).\nFor the optimization problem (5), we use a greedy algorithm to find the approximate solution. Given the previously selected sentence set X 1 , DSDR selects the next new sentence x i \u2208 V as follows:\nmin xi J(x i ) = Tr[V (X T X + \u03bbI) \u22121 V T ] s.t. X = X 1 \u222a x i , x i \u2208 V.(6)\nDenoting P = X T 1 X 1 + \u03bbI, Eq. (6) can be rewritten as:\nJ(x i ) = Tr[V (X T X + \u03bbI) \u22121 V T ] = Tr[V (P + x i x T i ) \u22121 V T ] = Tr V P \u22121 V T \u2212 V P \u22121 x i x T i P \u22121 V T 1 + x T i P \u22121 x i ,(7)\nwhere the Woodbury matrix identity (Riedel 1992) is applied in the second step. Fixing the candidate sentence set V and the selected sentence set X 1 , Tr[V P \u22121 V T ] is a constant, so the objective function is the same as maximizing the second part in the trace:\nmax xi Tr V P \u22121 x i x T i P \u22121 V T 1 + x T i P \u22121 x i = V P \u22121 x i 2 1 + x T i P \u22121 x i .(8)\nTo simplify the computation, we introduce a matrix B = V P \u22121 V T . Then the index of the new sentence x i can be obtained by:\ni = arg max i B * i 2 1 + B ii ,(9)\nAlgorithm 1 DSDR with linear reconstruction Input:\n\u2022 The candidate data set:\nV = [v 1 , v 2 , . . . , v n ] T\n\u2022 The number of sentences to be selected: m \u2022 The trade off parameter: \u03bb Output:\n\u2022 The set of m summary sentences:\nX = [x 1 , x 2 , . . . , x m ] T \u2286 V 1: initialize X \u2190 \u2205; 2: B 0 \u2190 V V T /\u03bb; 3: for t = 1 to m do 4: for i = 1 to n do 5: score(x i ) \u2190 B t\u22121 * i 2 /(1 + B t\u22121\nii )\n6: end for 7:\nx i \u2190 arg max xi score(x i ) 8: X \u2190 X \u222a x i 9: B t \u2190 B t\u22121 \u2212 B t\u22121 * i [B t\u22121 * i ] T /(1 + B t\u22121\nii ) 10: end for 11: return X;\nwhere i is the index of the new sentence x i in the candidate sentence set V , B * i and B ii are the ith column and diagonal entry of matrix B.\nOnce we find the new sentence x i , we add it into X 1 and update the matrix B as follows:\nB t = V P \u22121 t V T = V (P t\u22121 + x i x i T ) \u22121 V T = B t\u22121 \u2212 V P \u22121 t\u22121 x i x i T P \u22121 t\u22121 V T 1 + x i T P \u22121 t\u22121 x i = B t\u22121 \u2212 B t\u22121 * i [B t\u22121 * i ] T 1 + B t\u22121 ii . (10\n)\nwhere the matrix B t\u22121 denotes the matrix B at the step t\u22121.\nInitially the previously selected sentence set X 1 is empty. So the matrix P is initialized as:\nP 0 = \u03bbI. (11\n)\nThen the initialization of the matrix B can be written as:\nB 0 = V P \u22121 0 V T = 1 \u03bb V V T . (12\n)\nWe describe our sequential method for linear reconstruction in Algorithm 1. Given a document with n sentences, Algorithm 1 generates a summary with m sentences with the complexity as follows:\n\u2022 O(n 2 d) to calculate the initialization B 0 according to\nStep (2).\n\u2022 O(n 2 m) for the Step (3) to Step (10).\n-O(n) to calculate score(x i ) in Step ( 5)\n-O(n 2 ) to update the matrix B in Step (9).\nThe overall cost for Algorithm 1 is O(n 2 (d + m)).", "publication_ref": ["b11", "b19", "b33", "b33", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Nonnegative Linear Reconstruction", "text": "The linear reconstruction optimization problem Eq. (4) in the previous section might come up with a ij 's with negative values, which means redundant information needs to be removed from the summary sentence set X. To minimize the redundant information, in this section, we use the nonnegative linear reconstruction which adds nonnegative constraints on the coefficients. Nonnegative constraints on data representation has received considerable attention due to its psychological and physiological interpretation of naturally occurring data whose representation may be parts-based in the human brain (Palmer 1977;Wachsmuth, Oram, and Perrett 1994;Cai et al. 2011). Our nonnegative linear reconstruction method leads to parts-based reconstruction because it allows only additive, not subtractive, combinations of the sentences.\nFor the sake of efficient optimization, following (Yu et al. 2008;Cai and He 2012),we formulate the objective function of nonnegative DSDR as follows:\nmin ai,\u03b2 J = n i=1 v i \u2212 V T a i 2 + n j=1 a 2 ij \u03b2j + \u03b3 \u03b2 1 s.t. \u03b2 j \u2265 0, a ij \u2265 0 and a i \u2208 R n ,(13)\nwhere \u03b2 = [\u03b2 1 , . . . , \u03b2 n ] T is an auxiliary variable to control the candidate sentences selection. Similar to LASSO (Tibshirani 1996), the L 1 norm of \u03b2 will enforce some elements to be zeros. If \u03b2 j = 0, then all a 1j , . . . , a nj must be 0 which means the j-th candidate sentence is not selected. The new formulation in Eq. ( 13) is a convex problem and can guarantee a global optimal solution.\nBy fixing a i 's and setting the derivative of J with respect to \u03b2 to be zero, we can obtain the minimum solution of \u03b2:\n\u03b2 j = n i=1 a 2 ij \u03b3 . (14\n)\nOnce the \u03b2 is obtained, the minimization under the nonnegative constraints can be solved using the Lagrange method. Let \u03b1 ij be the Lagrange multiplier for constraint a ij \u2265 0 and A = [a ij ], the Lagrange L is:\nL = J + Tr[\u03b1A T ], \u03b1 = [\u03b1 ij ].\nThe derivative of L with respect to A is:\n\u2202L \u2202A = \u22122V V T + 2AV V T + 2Adiag(\u03b2) \u22121 + \u03b1.\nSetting the above derivative to be zero, \u03b1 can be represented as:\n\u03b1 = 2V V T + 2AV V T \u2212 2Adiag(\u03b2) \u22121 ,\nwhere diag(\u03b2) is a matrix with diagonal entries of \u03b2 1 , . . . , \u03b2 n . Using the Kuhn-Tucker condition \u03b1 ij a ij = 0, we get:\n(V V T ) ij a ij \u2212 (AV V T ) ij a ij \u2212 (Adiag(\u03b2)) ij a ij = 0.\nThis leads to the following updating formula:\na ij \u2190 a ij (V V T ) ij [AV V T + Adiag(\u03b2)] ij . (15\n)\nAlgorithm 2 DSDR with nonnegative linear reconstruction Input:\n\u2022 The candidate sentence set:\nV = [v 1 , v 2 , . . . , v n ] T\n\u2022 The trade off parameter: \u03b3 > 0 Output:\n\u2022 The set of the summary sentences: X \u2286 V Procedure:\n1: initialize a ij , \u03b2 j ; 2: initialize X \u2190 \u2205; 3: repeat 4:\n\u03b2 j = n i=1 a 2 ij \u03b3 ; 5: repeat 6: a ij \u2190 aij (V V T )ij [AV V T +Adiag(\u03b2)]ij ; 7:\nuntil converge; 8: until converge; 9: X \u2190 {v i |v i \u2282 V, \u03b2 j = 0}; 10: return X;\nThe Eq. ( 14) and Eq. ( 15) are iteratively performed until convergence. For the convergence of this updating formula, we have the following Theorem 1. Theorem 1. Under the iterative updating rule as Eq. ( 15), the objective function J is non-increasing with fixed \u03b2, and that the convergence of the iteration is guaranteed.\nProof. To prove Theorem 1, we introduce an auxiliary function as:\nG(u, a i ) = n j=1 (P a i ) j a ij u 2 j \u2212 2(V V T ) ij u j ,(16)\nwhere P = V V T + diag(\u03b2), and u = [u 1 , . . . , u n ] T is a positive vector. G(u, a i ) can also be identified as the sum of singular-variable functions: Sha et al. (2007) have proved that if a ij updates as:\nG(u, a i ) = n j=1 G j (u j ). (17\n) Let F (a i ) = a T i P a i \u2212 2(V V T ) i * a i ,\na ij \u2190 arg min uj G j (u j ),(18)\nG(u, a i ) converges monotonically to the global minimum of F (a i ).\nTaking the derivation of G j (u j ) with respect to u j and setting it to be zero, we obtain the updating formulation as:\na ij \u2190 a ij (V V T ) ij [AV V T + Adiag(\u03b2)] ij ,(19)\nwhich agrees with Eq. (15). We can rewrite the objective function J as:\nJ = n i=1 F (a i ) + Tr[V V T ] + \u03b3 \u03b2 1 . (20\n)\nFixing \u03b2, we can obtain the minimizer of J by minimizing each F (a i ) separately. Since the objective function J is the sum of all the individual terms F (a i ) plus a term independent of a i , we have shown that J is non-increasing with fixed \u03b2 under the updating rule as Eq. ( 15).\nAlgorithm 2 describes the DSDR with nonnegative linear reconstruction. Suppose the maximum number of iterations for Step (4) and Step (6) are t 1 and t 2 respectively, the total computational cost for Algorithm 2 is O(t 1 (n + t 2 (n 3 ))).", "publication_ref": ["b21", "b29", "b3", "b33", "b1", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In this study, we use the standard summarization benchmark data sets DUC 2006 and DUC 2007 for the evaluation. DUC 2006 and DUC 2007 contain 50 and 45 document sets respectively, with 25 news articles in each set. The sentences in each article have been separated by NIST 1 . And every sentence is either used in its entirety or not at all for constructing a summary. The length of a result summary is limited by 250 tokens (whitespace delimited).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation Metric", "text": "We use the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) toolkit (Lin 2004) which has been widely adopted by DUC for automatic summarization evaluation. ROUGE measures summary quality by counting overlapping units such as the n-gram, word sequences and word pairs between the peer summary (produced by algorithms) and the model summary (produced by humans). We choose two automatic evaluation methods ROUGE-N and ROUGE-L in our experiment. Formally, ROUGE-N is an n-gram recall between a candidate summary and a set of reference summaries and ROUGE-L uses the longest common subsequence (LCS) matric. ROUGE-N is computed as follows:\nROU GE \u2212 N = S\u2208Ref gramn \u2208S Count match (gramn) S\u2208Ref gramn \u2208S Count(gramn)\nwhere n stands for the length of the n-gram, Ref is the set of reference summaries. Count match (gram n ) is the maximum number of n-grams co-occurring in a candidate summary and a set of reference summaries, and Count(gram n ) is the number of n-grams in the reference summaries. ROUGE toolkit reports separate scores for 1, 2, 3 and 4-gram, and also for the longest common subsequence. Among these different scores, the unigram-based ROUGE score (ROUGE-1) has been shown to agree with human judgment most (Lin and Hovy 2003). Due to limited space, more information can be referred to the toolkit package.", "publication_ref": ["b17", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Compared Methods", "text": "We compare our DSDR with several state-of-the-art summarization approaches described briefly as follows:\n\u2022 Random: selects sentences randomly for each document set.   (Wan and Yang 2008): considers the topic clusters as hubs and the sentences as authorities, then ranks the sentences with the authorities scores. Finally, the highest ranked sentences are chosen to constitute the summary. \u2022 SNMF (Wang et al. 2008): uses symmetric non-negative matrix factorization(SNMF) to cluster sentences into groups and select sentences from each group for summarization. It is important to note that our algorithm is unsupervised. Thus we do not compare with any supervised methods (Toutanova et al. 2007;Haghighi and Vanderwende 2009;Celikyilmaz and Hakkani-Tur 2010;Lin and Bilmes 2011).", "publication_ref": ["b30", "b31", "b27", "b8", "b4", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Results", "text": "Overall Performance Comparison ROUGE can generate three types of scores: recall, precision and F-measure. We get similar experimental results using the three types with DSDR taking the lead. In this study, we use F-measure to compare different approaches. The F-measure of four ROUGE metrics are shown in our experimental results: ROUGE-1, ROUGE-2, ROUGE-3 and ROUGE-L.     5.6 * 10 \u221212 3.4 * 10 \u221210 1.9 * 10 \u22129 DSDR-non 2.5 * 10 \u221217 8.0 * 10 \u221213 1.4 * 10 \u221214 7.9 * 10 \u221215 1.1 * 10 \u221214 \"DSDR-non\" denote DSDR with the linear reconstruction and DSDR with the nonnegative reconstruction respectively. As shown by the highest ROUGE scores in bold type from the two tables, it is obvious that DSDR takes the lead followed by ClusterHITS. ClusterHITS considers topics as hubs and sentences as authorities where hubs and authorities can interact with each other. So that the correlations between topics and sentences can improve the quality of summary. Besides, selecting sentences randomly is a little better than just selecting the leading sentences. Among all the seven summarization algorithms, LSA and SNMF show the poorest performance on both data sets. Directly applying SVD on the terms by sentences matrix, summarization by LSA chooses those sentences with the largest indexes along the orthogonal latent semantic directions. Although SNMF relaxes the orthogonality, it relies on the sentence pairwise similarity. Whereas, our DSDR selects sentences which span the intrinsic subspace of the candidate sentence space. Such sentences are contributive to reconstruct the original document, and so are contributive to improve the summary quality. Under the DSDR framework, the sequential method of linear reconstruction is suboptimal, so DSDR-non outperforms DSDR-lin.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluations on Different Document Sets", "text": "In Figure 1, we illustrate the ROUGE-1 scores for each document set from DUC 2006 and DUC 2007 respectively. In each panel, the vertical axis describes the scores of the DSDR approach and the horizontal axis contains the best scores of other methods. The black stars indicate that DSDR gets the best scores on the corresponding document sets while the red circles suggest the best scores are obtained from other methods. It can be obviously observed that both the proposed reconstruction methods are better than others, since the number of black stars are much more than that of red circles in each panel.\nTo check whether the difference between DSDR and other approaches is significant, we perform the paired t-test between the ROUGE scores of DSDR and that of other approaches on both data sets. Table 3 and Table 4 show the associated p-values on DUC 2006 and DUC 2007 data sets respectively. The test at the 99% confidence interval demonstrates that our proposed framework can obtain very encouraging and promising results compared to the others.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": ["tab_3", "tab_4"]}, {"heading": "Conclusion", "text": "In this paper, we propose a novel summarization framework called Document Summarization based on Data Reconstruction (DSDR) which selects the most representative sentences that can best reconstruct the entire document. We introduce two types of reconstruction (linear and nonnegative) and develop efficient optimization methods for them. The linear reconstruction problem is solved using a greedy strategy and the nonnegative reconstruction problem is solved using a multiplicative updating. The experimental results show that out DSDR (with both reconstruction types) can outperform other state-of-the-art summarization approaches. DSDR with linear reconstruction is more efficient while DSDR with nonnegative reconstruction has better performance (by generating less redundant sentences). It would be of great interests to develop more efficient solution for DSDR with nonnegative reconstruction.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "The anatomy of a large-scale hypertextual Web search engine. Computer networks and ISDN systems", "journal": "", "year": "1998", "authors": "S Brin; L Page"}, {"ref_id": "b1", "title": "Manifold adaptive experimental design for text categorization", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": "2012", "authors": "D Cai; X He"}, {"ref_id": "b2", "title": "Organizing WWW images based on the analysis of page layout and web link structure", "journal": "", "year": "2004", "authors": "D Cai; X He; W.-Y Ma; J.-R Wen; H Zhang"}, {"ref_id": "b3", "title": "Graph regularized non-negative matrix factorization for data representation", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2011", "authors": "D Cai; X He; J Han; T S Huang"}, {"ref_id": "b4", "title": "A hybrid hierarchical model for multi-document summarization", "journal": "", "year": "2010", "authors": "A Celikyilmaz; D Hakkani-Tur"}, {"ref_id": "b5", "title": "Annual Meeting of the Association for Computational Linguistics", "journal": "", "year": "", "authors": ""}, {"ref_id": "b6", "title": "Tree pattern expression for extracting information from syntactically parsed text corpora. Data Mining and Knowledge Discovery", "journal": "", "year": "2011", "authors": "Y Choi"}, {"ref_id": "b7", "title": "Generic text summarization using relevance measure and latent semantic analysis", "journal": "ACM", "year": "2001", "authors": "J Conroy; O 'leary; D Acm; Y Gong; X Liu"}, {"ref_id": "b8", "title": "Exploring content models for multi-document summarization", "journal": "", "year": "2009", "authors": "A Haghighi; L Vanderwende"}, {"ref_id": "b9", "title": "Topic themes for multidocument summarization", "journal": "ACM", "year": "2005", "authors": "S Harabagiu; F Lacatusu"}, {"ref_id": "b10", "title": "Clustering and searching www images using link and page layout analysis", "journal": "ACM Transactions on Multimedia Computing, Communications and Applications", "year": "2007", "authors": "X He; D Cai; J.-R Wen; W.-Y Ma; H.-J Zhang"}, {"ref_id": "b11", "title": "Ridge regression: Biased estimation for nonorthogonal problems", "journal": "Technometrics", "year": "1970", "authors": "A Hoerl; R Kennard"}, {"ref_id": "b12", "title": "Comments-oriented document summarization: understanding documents with readers' feedback", "journal": "ACM", "year": "2008", "authors": "M Hu; A Sun; E Lim"}, {"ref_id": "b13", "title": "Query biased snippet generation in xml search", "journal": "", "year": "2008", "authors": "Y Huang; Z Liu; Y Chen"}, {"ref_id": "b14", "title": "Authoritative sources in a hyperlinked environment", "journal": "Journal of the ACM (JACM)", "year": "1999", "authors": "J Kleinberg"}, {"ref_id": "b15", "title": "A class of submodular functions for document summarization", "journal": "", "year": "2011-06", "authors": "H Lin; J Bilmes"}, {"ref_id": "b16", "title": "Automatic evaluation of summaries using n-gram co-occurrence statistics", "journal": "", "year": "2003", "authors": "C Lin; E Hovy"}, {"ref_id": "b17", "title": "Rouge: A package for automatic evaluation of summaries", "journal": "", "year": "2004", "authors": "C Lin"}, {"ref_id": "b18", "title": "Wordnet: a lexical database for english", "journal": "Communications of the ACM", "year": "1995", "authors": "G Miller"}, {"ref_id": "b19", "title": "Sparse approximate solutions to linear systems", "journal": "SIAM journal on computing", "year": "1995", "authors": "B Natarajan"}, {"ref_id": "b20", "title": "A compositional context sensitive multi-document summarizer: exploring the factors that influence summarization", "journal": "ACM", "year": "2006", "authors": "A Nenkova; L Vanderwende; K Mckeown"}, {"ref_id": "b21", "title": "Hierarchical structure in perceptual representation", "journal": "Cognitive Psychology", "year": "1977", "authors": "S Palmer"}, {"ref_id": "b22", "title": "Multi-document Summarization Based on Cluster Using Non-negative Matrix Factorization. SOFSEM: Theory and Practice of Computer Science", "journal": "", "year": "2007", "authors": "S Park; J Lee; D Kim; C Ahn"}, {"ref_id": "b23", "title": "A sherman-morrison-woodbury identity for rank augmenting matrices with application to centering", "journal": "SIAM Journal on Matrix Analysis and Applications", "year": "1992", "authors": "K Riedel"}, {"ref_id": "b24", "title": "Multiplicative updates for nonnegative quadratic programming", "journal": "Neural Computation", "year": "2007", "authors": "F Sha; Y Lin; L Saul; D Lee"}, {"ref_id": "b25", "title": "Document summarization using conditional random fields", "journal": "", "year": "2007", "authors": "D Shen; J Sun; H Li; Q Yang; Z Chen"}, {"ref_id": "b26", "title": "Regression shrinkage and selection via the lasso", "journal": "Journal of the Royal Statistical Society. Series B", "year": "1996", "authors": "R Tibshirani"}, {"ref_id": "b27", "title": "The pythy summarization system: Microsoft research at duc", "journal": "", "year": "2007", "authors": "K Toutanova; C Brockett; M Gamon; J Jagarlamudi; H Suzuki; L Vanderwende"}, {"ref_id": "b28", "title": "Fast generation of result snippets in web search", "journal": "", "year": "2007", "authors": "A Turpin; Y Tsegay; D Hawking; H E Williams"}, {"ref_id": "b29", "title": "Recognition of objects and their component parts: responses of single units in the temporal cortex of the macaque", "journal": "Cerebral Cortex", "year": "1994", "authors": "E Wachsmuth; M Oram; D Perrett"}, {"ref_id": "b30", "title": "CollabSum: exploiting multiple document clustering for collaborative single document summarizations", "journal": "ACM", "year": "2007", "authors": "X Wan; Yang ; J ; X ; Yang ; J "}, {"ref_id": "b31", "title": "Multi-document summarization via sentence-level semantic analysis and symmetric matrix factorization", "journal": "", "year": "2008", "authors": "D Wang; T Li; S Zhu; C Ding"}, {"ref_id": "b32", "title": "Using leading text for news summaries: Evaluation results and implications for commercial summarization applications", "journal": "", "year": "1998", "authors": "M Wasson"}, {"ref_id": "b33", "title": "Non-greedy active learning for text categorization using convex ansductive experimental design", "journal": "ACM", "year": "2006", "authors": "K Yu; S Zhu; W Xu; Y Gong;  Acm; K Yu; J Bi; V Tresp"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: The scores of all algorithms on each document set of DUC 2006 and DUC 2007, the black stars denote our proposed methods are best while the red circles denote otherwise. \"DSDR-lin\" and \"DSDR-non\" denote DSDR with the linear reconstruction and DSDR with the nonnegative reconstruction respectively.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Average F-measure performance on DUC 2006. \"DSDR-lin\" and \"DSDR-non\" denote DSDR with the linear reconstruction and DSDR with the nonnegative reconstruction respectively.", "figure_data": "AlgorithmRouge-1 Rouge-2 Rouge-3 Rouge-LRandom0.285070.042910.010230.25926Lead0.274490.047210.011810.23225LSA0.257820.037070.008670.23264ClusterHITS 0.287520.051670.012820.25715SNMF0.254530.038150.008150.22530DSDR-lin0.309410.054270.013000.27576DSDR-non0.331680.060470.014820.29850"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ": Average F-measure performance on DUC 2007.\"DSDR-lin\" and \"DSDR-non\" denote DSDR with the linearreconstruction and DSDR with the nonnegative reconstruc-tion respectively.AlgorithmRouge-1 Rouge-2 Rouge-3 Rouge-LRandom0.320280.054320.013100.29127Lead0.314460.061510.018300.26575LSA0.259470.036410.008540.22751ClusterHITS 0.328730.066250.019270.29578SNMF0.286510.042320.008900.25502DSDR-lin0.360550.071630.021240.32369DSDR-non0.395730.074390.019650.35335\u2022 Lead (Wasson 1998): for each document set, orders thedocuments chronologically and takes the leading sen-tences one by one.\u2022 LSA (Gong and Liu 2001): applies the singular value de-composition (SVD) on the terms by sentences matrix toselect highest ranked sentences.\u2022 ClusterHITS"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Table 2 show the ROUGE evaluation results on DUC 2006 and DUC 2007 data sets respectively. \"DSDR-lin\" and", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "The associated p-values of the paired t-test on DUC 2006.", "figure_data": "RandomLeadLSAClusterHITS SNMFDSDR-lin4.6  *  10 \u221214 7.1  *  10\u221269.2  *  10 \u221214 4.0  *  10 \u221299.3  *  10 \u22128DSDR-non 2.6  *  10 \u221225 6.7  *  10 \u221217 2.3  *  10 \u221230 6.0  *  10 \u2212231.8  *  10 \u221225"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "The associated p-values of the paired t-test on DUC 2007.", "figure_data": "RandomLeadLSAClusterHITS SNMFDSDR-lin5.2  *  10 \u221214 1.7  *  10 \u22128"}], "formulas": [{"formula_id": "formula_0", "formula_text": "V = [v 1 , v 2 , . . . , v n ]", "formula_coordinates": [2.0, 319.5, 153.3, 238.5, 20.61]}, {"formula_id": "formula_1", "formula_text": "L(v i , f i (X; a i )) = v i \u2212 f i (X; a i ) 2 , (1", "formula_coordinates": [2.0, 360.29, 290.08, 193.84, 11.72]}, {"formula_id": "formula_2", "formula_text": ")", "formula_coordinates": [2.0, 554.13, 292.47, 3.87, 8.64]}, {"formula_id": "formula_3", "formula_text": "min X,ai n i=1 v i \u2212 f i (X; a i ) 2 .", "formula_coordinates": [2.0, 383.08, 370.71, 111.35, 30.32]}, {"formula_id": "formula_4", "formula_text": "f i (X; a i ) = m j=1 x j a ij , X = [x 1 , x 2 , . . . , x m ] T . (3)", "formula_coordinates": [2.0, 331.61, 491.92, 226.39, 30.32]}, {"formula_id": "formula_5", "formula_text": "v i \u2248 m j=1 x j a ij , 1 \u2264 i \u2264 n.", "formula_coordinates": [2.0, 379.08, 549.93, 119.34, 30.32]}, {"formula_id": "formula_6", "formula_text": "n i=1 v i \u2212 X T a i 2", "formula_coordinates": [2.0, 401.38, 605.09, 74.98, 30.32]}, {"formula_id": "formula_7", "formula_text": "X,A n i=1 v i \u2212 X T a i 2 + \u03bb a i 2 s.t. X \u2282 V, |X| = m A = [a 1 , a 2 , . . . , a n ] T \u2208 R n\u00d7m .(4)", "formula_coordinates": [3.0, 94.44, 79.4, 198.06, 40.74]}, {"formula_id": "formula_8", "formula_text": "min X J = Tr[V (X T X + \u03bbI) \u22121 V T ] s.t. X \u2282 V, |X| = m (5)", "formula_coordinates": [3.0, 98.05, 317.01, 194.45, 25.4]}, {"formula_id": "formula_9", "formula_text": "min xi J(x i ) = Tr[V (X T X + \u03bbI) \u22121 V T ] s.t. X = X 1 \u222a x i , x i \u2208 V.(6)", "formula_coordinates": [3.0, 89.49, 440.24, 203.01, 26.87]}, {"formula_id": "formula_10", "formula_text": "J(x i ) = Tr[V (X T X + \u03bbI) \u22121 V T ] = Tr[V (P + x i x T i ) \u22121 V T ] = Tr V P \u22121 V T \u2212 V P \u22121 x i x T i P \u22121 V T 1 + x T i P \u22121 x i ,(7)", "formula_coordinates": [3.0, 68.78, 489.86, 223.72, 58.65]}, {"formula_id": "formula_11", "formula_text": "max xi Tr V P \u22121 x i x T i P \u22121 V T 1 + x T i P \u22121 x i = V P \u22121 x i 2 1 + x T i P \u22121 x i .(8)", "formula_coordinates": [3.0, 67.11, 620.32, 225.39, 26.35]}, {"formula_id": "formula_12", "formula_text": "i = arg max i B * i 2 1 + B ii ,(9)", "formula_coordinates": [3.0, 128.96, 682.65, 163.55, 24.8]}, {"formula_id": "formula_13", "formula_text": "V = [v 1 , v 2 , . . . , v n ] T", "formula_coordinates": [3.0, 437.98, 79.34, 90.62, 11.23]}, {"formula_id": "formula_14", "formula_text": "X = [x 1 , x 2 , . . . , x m ] T \u2286 V 1: initialize X \u2190 \u2205; 2: B 0 \u2190 V V T /\u03bb; 3: for t = 1 to m do 4: for i = 1 to n do 5: score(x i ) \u2190 B t\u22121 * i 2 /(1 + B t\u22121", "formula_coordinates": [3.0, 324.48, 124.75, 233.52, 76.69]}, {"formula_id": "formula_15", "formula_text": "x i \u2190 arg max xi score(x i ) 8: X \u2190 X \u222a x i 9: B t \u2190 B t\u22121 \u2212 B t\u22121 * i [B t\u22121 * i ] T /(1 + B t\u22121", "formula_coordinates": [3.0, 324.48, 212.4, 185.53, 39.63]}, {"formula_id": "formula_16", "formula_text": "B t = V P \u22121 t V T = V (P t\u22121 + x i x i T ) \u22121 V T = B t\u22121 \u2212 V P \u22121 t\u22121 x i x i T P \u22121 t\u22121 V T 1 + x i T P \u22121 t\u22121 x i = B t\u22121 \u2212 B t\u22121 * i [B t\u22121 * i ] T 1 + B t\u22121 ii . (10", "formula_coordinates": [3.0, 364.68, 363.11, 189.17, 91.0]}, {"formula_id": "formula_17", "formula_text": ")", "formula_coordinates": [3.0, 553.85, 435.83, 4.15, 8.64]}, {"formula_id": "formula_18", "formula_text": "P 0 = \u03bbI. (11", "formula_coordinates": [3.0, 419.81, 505.68, 134.05, 9.65]}, {"formula_id": "formula_19", "formula_text": ")", "formula_coordinates": [3.0, 553.85, 506.0, 4.15, 8.64]}, {"formula_id": "formula_20", "formula_text": "B 0 = V P \u22121 0 V T = 1 \u03bb V V T . (12", "formula_coordinates": [3.0, 382.15, 541.18, 171.7, 22.31]}, {"formula_id": "formula_21", "formula_text": ")", "formula_coordinates": [3.0, 553.85, 548.24, 4.15, 8.64]}, {"formula_id": "formula_22", "formula_text": "min ai,\u03b2 J = n i=1 v i \u2212 V T a i 2 + n j=1 a 2 ij \u03b2j + \u03b3 \u03b2 1 s.t. \u03b2 j \u2265 0, a ij \u2265 0 and a i \u2208 R n ,(13)", "formula_coordinates": [4.0, 66.98, 270.34, 225.52, 46.25]}, {"formula_id": "formula_23", "formula_text": "\u03b2 j = n i=1 a 2 ij \u03b3 . (14", "formula_coordinates": [4.0, 130.32, 423.83, 158.03, 36.64]}, {"formula_id": "formula_24", "formula_text": ")", "formula_coordinates": [4.0, 288.35, 445.22, 4.15, 8.64]}, {"formula_id": "formula_25", "formula_text": "L = J + Tr[\u03b1A T ], \u03b1 = [\u03b1 ij ].", "formula_coordinates": [4.0, 98.06, 515.72, 130.46, 11.72]}, {"formula_id": "formula_26", "formula_text": "\u2202L \u2202A = \u22122V V T + 2AV V T + 2Adiag(\u03b2) \u22121 + \u03b1.", "formula_coordinates": [4.0, 64.49, 547.39, 198.8, 22.31]}, {"formula_id": "formula_27", "formula_text": "\u03b1 = 2V V T + 2AV V T \u2212 2Adiag(\u03b2) \u22121 ,", "formula_coordinates": [4.0, 81.11, 597.16, 164.36, 10.81]}, {"formula_id": "formula_28", "formula_text": "(V V T ) ij a ij \u2212 (AV V T ) ij a ij \u2212 (Adiag(\u03b2)) ij a ij = 0.", "formula_coordinates": [4.0, 54.07, 650.63, 218.44, 11.72]}, {"formula_id": "formula_29", "formula_text": "a ij \u2190 a ij (V V T ) ij [AV V T + Adiag(\u03b2)] ij . (15", "formula_coordinates": [4.0, 99.66, 682.18, 188.69, 24.8]}, {"formula_id": "formula_30", "formula_text": ")", "formula_coordinates": [4.0, 288.35, 690.81, 4.15, 8.64]}, {"formula_id": "formula_31", "formula_text": "V = [v 1 , v 2 , . . . , v n ] T", "formula_coordinates": [4.0, 455.68, 79.47, 90.62, 11.23]}, {"formula_id": "formula_32", "formula_text": "\u03b2 j = n i=1 a 2 ij \u03b3 ; 5: repeat 6: a ij \u2190 aij (V V T )ij [AV V T +Adiag(\u03b2)]ij ; 7:", "formula_coordinates": [4.0, 324.48, 170.69, 137.97, 64.25]}, {"formula_id": "formula_33", "formula_text": "G(u, a i ) = n j=1 (P a i ) j a ij u 2 j \u2212 2(V V T ) ij u j ,(16)", "formula_coordinates": [4.0, 334.14, 395.59, 223.86, 30.32]}, {"formula_id": "formula_34", "formula_text": "G(u, a i ) = n j=1 G j (u j ). (17", "formula_coordinates": [4.0, 380.27, 473.72, 173.58, 30.32]}, {"formula_id": "formula_35", "formula_text": ") Let F (a i ) = a T i P a i \u2212 2(V V T ) i * a i ,", "formula_coordinates": [4.0, 319.5, 484.45, 238.5, 40.48]}, {"formula_id": "formula_36", "formula_text": "a ij \u2190 arg min uj G j (u j ),(18)", "formula_coordinates": [4.0, 381.71, 542.65, 176.29, 16.07]}, {"formula_id": "formula_37", "formula_text": "a ij \u2190 a ij (V V T ) ij [AV V T + Adiag(\u03b2)] ij ,(19)", "formula_coordinates": [4.0, 365.16, 617.95, 192.84, 24.8]}, {"formula_id": "formula_38", "formula_text": "J = n i=1 F (a i ) + Tr[V V T ] + \u03b3 \u03b2 1 . (20", "formula_coordinates": [4.0, 352.89, 677.01, 200.96, 30.32]}, {"formula_id": "formula_39", "formula_text": ")", "formula_coordinates": [4.0, 553.85, 687.74, 4.15, 8.64]}, {"formula_id": "formula_40", "formula_text": "ROU GE \u2212 N = S\u2208Ref gramn \u2208S Count match (gramn) S\u2208Ref gramn \u2208S Count(gramn)", "formula_coordinates": [5.0, 60.39, 449.63, 204.61, 26.95]}], "doi": ""}