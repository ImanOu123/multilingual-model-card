{"title": "All in One: Multi-Task Prompting for Graph Neural Networks", "authors": "Xiangguo Sun; Hong Cheng; Jia Li; Bo Liu; Jihong Guan", "pub_date": "2023-12-17", "abstract": "Recently, \"pre-training and fine-tuning\" has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a \"negative transfer\" to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pretrained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In this way, the prompting idea from NLP can be seamlessly introduced to the graph area. Then, to further narrow the gap between various graph tasks and state-of-the-art pre-training strategies, we further study the task space of various graph applications and reformulate downstream problems to the graph-level task. Afterward, we introduce meta-learning to efficiently learn a better initialization for the multi-task prompt of graphs so that our prompting framework can be more reliable and general for different tasks. We conduct extensive experiments, results from which demonstrate the superiority of our method.\u2022 Networks \u2192 Online social networks; \u2022 Computing methodologies \u2192 Knowledge representation and reasoning.", "sections": [{"heading": "INTRODUCTION", "text": "Graph neural networks (GNNs) have been widely applied to various applications such as social computing [5,28] , anomaly detection [30,31] , and network analysis [4]. Beyond exploring various exquisite GNN structures, recent years have witnessed a new research trend on how to train a graph model for dedicated problems.\nTraditional supervised learning methods on graphs heavily rely on graph labels, which are not always sufficient in the real world. Another shortcoming is the over-fitting problem when the testing data is out-of-distribution [24]. To solve these challenges, many studies turn to \"pre-training and fine-tuning\" [13], which means pre-training a graph model with easily accessible data, and then transferring the graph knowledge to a new domain or task via tuning the last layer of the pre-trained model. Although much progress has been achieved on pre-training strategies [9], there still exists a huge gap between these pretexts and multiple downstream tasks. For example, a typical pretext for the pre-training graph is binary edge prediction. Usually, this pre-training strategy makes connected nodes closer in a latent representation space. However, many downstream tasks are not limited to edge-level tasks but also include node-level tasks (e.g., node multi-class classification) or graph-level tasks (e.g., graph classification). If we transfer the above pre-trained model to multi-class node classification, it may require us to carefully search the results in higher dimensional parameter space for the additional classes of node labels. This tuning may even break down (a.k.a negative transfer [33]) when connected nodes have different labels. Tuning this pre-trained model to graphlevel tasks is neither efficient because we have to pay huge efforts to learn an appropriate function mapping node embedding to the whole graph representation. A promising solution to the above problems is to extend \"pretraining and fine-tuning\" to \"pre-training, prompting, and finetuning\". Prompt learning is a very attractive idea derived from natural language processing (NLP) and has shown notable effectiveness in generalizing pre-trained language models to a wide range of language applications [20]. Specifically, a language prompt refers to a piece of text appended to the rear of an input text. For example, a sentiment task like \"KDD2023 will witness many high-quality papers. I feel so [MASK]\" can be easily transferred to a word prediction task via a preset prompt (\"I feel so [MASK]\"). It is highly expected that the language model may predict \"[MASK]\" as \"excited\" rather than \"upset\" without further optimizing parameters for the new sentiment task because this model has already been pre-trained via the pretext of masked words prediction and contains some useful knowledge to answer this question. By this means, some downstream objectives can be naturally aligned with the pre-training target. Inspired by the success of the language prompt, we hope to introduce the same idea to graphs. As shown in Figure 1, prompt tuning in the graph domain is to seek some light-weighted prompt, keep the pre-training model frozen, and use the prompt to reformulate downstream tasks in line with the pre-training task. In this way, the pre-trained model can be easily applied to downstream applications with highly efficient fine-tuning or even without any fine-tuning. This is particularly useful when the downstream task is a few-shot setting.\nHowever, designing the graph prompt is more intractable than language prompts. First, classic language prompts are usually some preset phrases or learnable vectors attached at the end of input texts. As shown in Figure 2, we only need to consider the content for the language prompt, whereas the graph prompt not only requires the prompt \"content\" but also needs to know how to organize these prompt tokens and how to insert the prompt into the original graph, both of which are undefined problems.\nSecond, there is a huge difficulty in reconciling downstream problems to the pre-training task. In the NLP area, we usually pretrain a language model via masked prediction and then transfer it to various applications like question answering [22], sentiment classification [17]. The underlying support [21] is that these language tasks usually share a large overlapping task sub-space, making a masked language task easily transferred to other applications. However, how much does the same observation exist (if truly exists) in graph learning? It is crucial but difficult to decide on an appropriate pre-training task and reformulate downstream tasks to improve``K DD2023 will witness many high-quality papers. I feel so [excited] '' prompt answer input input prompt tasker (answer) insert the prompt to the input graph inserting pattern: prompt token: token structure: the capability of model generalization. Currently, we only find very few works [27] studying the graph prompt issue. However, it can only deal with a single-type task (e.g., node classification) using a specific pretext (e.g., edge prediction), which is far from addressing the multi-task setting with different-level tasks. Last but not least, learning a reliable prompt usually needs huge manpower and is more sensitive to prompt initialization in the multi-task setting [18]. Although there are some works [14,38] in the NLP area trying to initialize the prompt via hand-crafted content or some discrete features, these methods are task-bounded, which is not sufficient when we confront a new task. This problem may be even worse in our multi-task graph area since graph features vary a lot in different domains and tasks.\nPresented work. To further fill the gap between graph pretraining and downstream tasks, we introduce the prompt method from NLP to graphs under the multi-task background. Specifically, to address the first challenge, we propose to unify the format of the language prompt and graph prompt in one way so that we can smoothly transfer the prompt idea from NLP to graphs, then we design the graph prompt from prompt tokens, token structures, and prompt inserting patterns. To address the second challenge, we first study the task subspace in graphs and then propose to reformulate node-level and edge-level tasks to graph-level tasks by induced graphs from original graphs. To address the third challenge, we introduce the meta-learning technique over multiple tasks to learn better prompts. We carefully evaluate our method with other approaches and the experimental results extensively demonstrate our advantages.\nContributions:\n\u2022 We unify the prompt format in the language area and graph area, and further propose an effective graph prompt for multitask settings (section 3.3). \u2022 We propose an effective way to reformulate node-level and edge-level tasks to graph-level tasks, which can further match many pre-training pretexts (section 3.2). \u2022 We introduce the meta-learning technique to our graph prompting study so that we can learn a reliable prompt for improving the multi-task performance (section 3.4). \u2022 We carefully analyze why our method works (section 3.5) and confirm the effectiveness of our method via extensive experiments (section 4).", "publication_ref": ["b4", "b27", "b29", "b30", "b3", "b23", "b12", "b8", "b32", "b19", "b21", "b16", "b20", "b26", "b17", "b13", "b37"], "figure_ref": ["fig_0", "fig_1"], "table_ref": []}, {"heading": "BACKGROUND", "text": "Graph Neural Networks. Graph neural networks (GNNs) have presented powerful expressiveness in many graph-based applications [10,12,15,29] . The nature of most GNNs is to capture the underlying message-passing patterns for graph representation. To this end, there are many effective neural network structures proposed such as graph attention network (GAT) [32], graph convolution network (GCN) [34], Graph Transformer [25]. Recent works also consider how to make graph learning more adaptive when data annotation is insufficient or how to transfer the model to a new domain, which triggered many graph pre-training studies instead of traditional supervised learning.\nGraph Pre-training. Graph pre-training [13] aims to learn some general knowledge for the graph model with easily accessible information to reduce the annotation costs of new tasks. Some effective pre-training strategies include node-level comparison like GCA [40], edge-level pretext like edge prediction [13], and graph-level contrastive learning such as GraphCL [36] and SimGRACE [35].\nIn particular, GraphCL minimizes the distance between a pair of graph-level representations for the same graph with different augmentations whereas SimGRACE tries to perturb the graph model parameter spaces and narrow down the gap between different perturbations for the same graph. These graph-level strategies perform more effectively in graph knowledge learning [11] and are the default strategies of this paper. Prompt Learning & Motivations. Intuitively, the above graphlevel pre-training strategies have some intrinsic similarities with the language-masked prediction task: aligning two graph views generated by node/edge/feature mask or other perturbations is very similar to predicting some vacant \"blanks\" on graphs. That inspires us to further consider: why can't we use a similar format prompt for graphs to improve the generalization of graph neural networks? Instead of fine-tuning a pre-trained model with an adaptive task head, prompt learning aims to reformulate input data to fit the pretext [7]. Many effective prompt methods are firstly proposed in the NLP area, including some hand-crafted prompts like GPT-3 [3], discrete prompts like [7,26], and trainable prompts in the continuous spaces like [16,19]. Despite significant results achieved, prompt-based methods have been rarely introduced in graph domains yet. We only find very few works like GPPT [27], trying to design prompts for graphs. Unfortunately, most of them are very limited and are far from sufficient to meet the multi-task demands.  3a). However, things are a little complicated in the graph domain since graph-related tasks are far from similar. As shown in Figure 3b, it is far-fetched to treat the edge-level task and the node-level task as the same one because node-level operations and edge-level operations are far more different [27]. This gap limits the performance of pre-training models and might even cause negative transfer [13].\nThe same problem also happens in our \"pre-training, prompting, and fine-tuning\" framework since we aim to learn a graph prompt for multiple tasks, which means we need to further narrow down the gap between these tasks by reformulating different graph tasks in a more general form.", "publication_ref": ["b9", "b11", "b14", "b28", "b31", "b33", "b24", "b12", "b39", "b12", "b35", "b34", "b10", "b6", "b2", "b6", "b25", "b15", "b18", "b26", "b26", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Why", "text": "Reformulate to the Graph Level. With the above motivation, we revisit the potential task space on the graph and find their hierarchical relation as shown in Figure 3b. Intuitively, many node-level operations such as \"changing node features\", \"delete/add a node\", or edge-level operations such as \"add/delete an edge\", can be treated as some basic operations at the graph level. For example, \"delete a subgraph\" can be treated as \"delete nodes and edges\". Compared with node-level and edge-level tasks, graph-level tasks are more general and contain the largest overlapping task sub-spaces for knowledge transfer, which has been adopted as the mainstream task in many graph pre-training models [11,35,36]. This observation further inspires us to reformulate downstream tasks to look like the graph-level task and then leverage our prompting model to match graph-level pre-training strategies.", "publication_ref": ["b10", "b34", "b35"], "figure_ref": [], "table_ref": []}, {"heading": "How to", "text": "Reformulate Downstream Tasks. Specifically, we reformulate node-level and edge-level tasks to graph-level tasks by building induced graphs for nodes and edges, respectively. As shown in Figure 4a, an induced graph for a target node means its local area in the network within distance, which is also known as its -ego network. This subgraph preserves the node's local structure by neighboring node connections and its semantic context by neighboring node features, which is the main scope of most graph neural encoders. When we treat the target node's label as this induced graph label, we can easily translate the node classification problem into graph classification; Similarly, we present an induced graph for a pair of nodes in Figure 4b. Here, the pair of nodes can be treated as a positive edge if there is an edge connecting them, or a negative edge if not. This subgraph can be easily built by extending this node pair to their distance neighbors. We can reformulate the edgelevel task by assigning the graph label with the edge label of the target node pair. Note that for unweighted graphs, the distance is equal to -hop length; for weighted graphs, the distance refers to the shortest path distance, where the induced graph can be easily found by many efficient algorithms [1,39].  (3) inserting pattern, which presents how to add the prompt to the input data. In the NLP area, the prompt is usually added in the front or the back end of the input sentences by default. However, in the graph area, there are no explicit positions like a sentence to joint graph prompt, making the graph prompting more difficult.", "publication_ref": ["b0", "b38"], "figure_ref": ["fig_3", "fig_3"], "table_ref": []}, {"heading": "Prompt Tokens. Let a graph instance be", "text": "G = (V, E) where V = { 1 , 2 , \u2022 \u2022 \u2022 , }\nis the node set containing nodes; each node has a feature vector denoted by x \u2208 R 1\u00d7 for node ; E = {( , )| , \u2208 V} is the edge set where each edge connects a pair of nodes in V. With the previous discussion, we here present our prompt graph as G = (P, S) where\nP = { 1 , 2 , \u2022 \u2022 \u2022 , | P | }\ndenotes the set of prompt tokens and |P | is the number of tokens. Each token \u2208 P can be represented by a token vector p \u2208 R 1\u00d7 with the same size of node features in the input graph; Note that in practice, we usually have |P | \u226a and |P | \u226a \u210e where \u210e is the size of the hidden layer in the pre-trained graph model. With these token vectors, the input graph can be reformulated by adding the -th token to graph node (e.g.,x = x + p ). Then, we replace the input features with the prompted features and send them to the pre-trained model for further processing.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Token Structures", "text": ". S = {( , )| ,\n\u2208 P} is the token structure denoted by pair-wise relations among tokens. Unlike the NLP prompt, the token structure in the prompt graph is usually implicit. To solve this problem, we propose three methods to design the prompt token structures: (1) the first way is to learn tunable parameters:\nA = | P | \u22121 \u222a =1 = +1 { }\nwhere is a tunable parameter indicating how possible the token and the token should be connected; (2) the second way is to use the dot product of each prompt token pair and prune them according to the dot value. In this case, ( , ) \u2208 S iff (p \u2022p ) < where (\u2022) is a sigmoid function and is a pre-defined threshold;\n(3) the third way is to treat the tokens as independent and then we have S = \u2205.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Inserting Patterns.", "text": "Let be the inserting function that indicates how to add the prompt graph G to the input graph G, then the manipulated graph can be denoted as G = (G, G ). We can define the inserting pattern as the dot product between prompt tokens and input graph nodes, and then use a tailored connection likex = x\n+ | P | =1\np where is a weighted value to prune unnecessary connections:\n= (p \u2022 x ), if (p \u2022 x ) > 0, otherwise(1)\nAs an alternative and special case, we can also use a more simplified way to getx = x\n+ | P | =1 p .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Multi-task Prompting via Meta Learning", "text": "3.4.1 Constructing Meta Prompting Tasks. Let be the -th task with supporting data D and query data D ; Specifically, for the graph classification task, D and D contain labeled graphs; for the node classification task, we generate an induced graph for each node as mentioned in section 3.2.3, align the graph label with the target node label, and treat this graph as a member in D or D ; for the edge classification task, we first generate edge induced graphs for training and testing and the edge label is up to its two endpoints.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Applying", "text": "Meta-learning to Graph Prompting. Let be prompt parameters, * be the fixed parameters of the pre-trained graph backbone, and be the tasker's parameters. We use , | * to denote the pipeline with prompt graph ( ), pre-trained model ( * , fixed), and downstream tasker ( ). Let L D ( ) be the task loss with pipline on data D. Then for each task , the corresponding parameters can be updated as follows:\n= \u22121 \u2212 \u2207 \u22121 L D \u22121 , \u22121 | * = \u22121 \u2212 \u2207 \u22121 L D \u22121 , \u22121 | * (2)\nwhere the initialization is set as: 0 = , and 0 = . The goal of this section is to learn effective initialization settings ( , ) for meta prompting tasks, which can be achieved by minimizing the meta loss on various tasks:\n* , * = arg min , \u2211\ufe01 \u2208 T L D , | * (3)\nwhere T is the task set. According to the chain rule, we use the second-order gradient to update (or ) based on query data:\n\u2190 \u2212 \u2022 = \u2212 \u2022 \u2211\ufe01 \u2208 T \u2207 L D , | * = \u2212 \u2022 \u2211\ufe01 \u2208 T \u2207 L D , | * \u2022 \u2207 ( ) = \u2212 \u2022 \u2211\ufe01 \u2208 T \u2207 L D , | * \u2022 I\u2212 H L D , | *(4)\nwhere H (L) is the Hessian matrix with (H (L)) = 2 L/ ; and can be updated in the same way.\nKindly note that in the prompt learning area, the task head is also known as the answering function, which connects the prompt to the answers for downstream tasks to be reformulated. The answering function can be both tunable or hand-craft templates. In section 3.5, we also propose a very simple but effective hand-crafted prompt answering template without any tunable task head.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Overall Learning Process.", "text": "To improve the learning stability, we organize these tasks as multi-task episodes where each episode contains batch tasks including node classification (\" \" for short), edge classification (\"\u2113\" for short), and graph classification (\" \" for short). Let E = (T E , L E , S E , Q E ) be a multi-task episode. We de-\nfine task batch T E = {T ( ) E , T ( ) E , T (\u2113 ) E } where each subset T (\u22b3) E = { \u22b31 , \u2022 \u2022 \u2022 , \u22b3 \u22b3 }; loss function sets L E = {L ( ) , L ( ) , L (\u2113 ) }, sup- porting data S E = {S ( ) E , S ( ) E , S (\u2113 ) E } where each subset S (\u22b3) E = {D \u22b31 , \u2022 \u2022 \u2022 , D \u22b3 \u22b3 }, and query data Q E = {Q ( ) E , Q ( ) E , Q (\u2113 ) E } where S (\u22b3) E = {D \u22b31 , \u2022 \u2022 \u2022 , D \u22b3 \u22b3 }.\nThen the multi-task prompting is presented in Algorithm 1. We treat each node/edge/graph class as a binary classification task so that they can share the same task head. Note that our method can also deal with other tasks beyond classification with only a few adaptations (see Appendix A).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Why It Works?", "text": "3.5.1 Connection to Existing Work. A prior study on graph prompt is proposed by [27], namely GPPT. They use edge prediction as a pre-training pretext and reformulate node classification to the pretext by designing labeled tokens added to the original graph. The compound graph will be sent to the pre-trained model again to predict the link connecting each node to the label tokens. Their work somehow is a special case of our method when our prompt \n4 for \u22b3 \u2208 T E , \u22b3 = , , \u2113 do 5 \u22b3 , \u22b3 \u2190 , 6 \u22b3 \u2190 \u22b3 \u2212 \u2207 \u22b3 L (\u22b3) D \u22b3 \u22b3 , \u22b3 | * 7 \u22b3 \u2190 \u22b3 \u2212 \u2207 \u22b3 L (\u22b3) D \u22b3 \u22b3 , \u22b3 | * 8 end // outer meta update 9\nUpdate , by Equation ( 4) on\nQ E = {D \u22b3 | \u22b3 \u2208 T E , \u22b3 = , , \u2113 } 10 end 11 return * , * | *\ngraph only contains isolated tokens, each of which corresponds to a node category. However, there are at least three notable differences:\n(1) GPPT is not flexible to manipulate original graphs; (2) GPPT is only applicable for node classification; and (3) GPPT only supports edge prediction task as the pretext but is not compatible with more advanced graph-level pre-training strategies such as GraphCL [36], UGRAPHEMB [2], SimGRACE [35] etc. We further discuss these issues w.r.t. flexibility, efficiency, and compatibility as below.\n3.5.2 Flexibility. The nature of prompting is to manipulate the input data to match the pretext. Therefore, the flexibility of data operations is the bottleneck of prompting performance. Let be any graph-level transformation such as \"changing node features\", \"adding or removing edges/subgraphs\" etc., and * be the frozen pre-trained graph model. For any graph G with adjacency matrix A and node feature matrix X, Fang et al. [6] have proved that we can always learn an appropriate prompt token * making the following equation stand:\n* A, X + * = * ( (A, X)) +\nThis means we can learn an appropriate token applied to the original graph to imitate any graph manipulation. Here denotes the error bound between the manipulated graph and the prompting graph w.r.t. their representations from the pre-trained graph model. This error bound is related to some non-linear layers of the model (unchangeable) and the quality of the learned prompt (changeable), which is promising to be further narrowed down by a more advanced prompt scheme. In this paper, we extend the standalone token to a prompt graph that has multiple prompt tokens with learnable inner structures. Unlike the indiscriminate inserting in Equation (5) (\"X + * \" means the prompt token should be added to every node of the original graph), the inserting pattern of our proposed prompt graph is highly customized. Let (G, G ) denote the inserting pattern defined in section 3.3; G is the original graph, and G is the prompt graph, then we can learn an optimal prompt graph G * to extend Equation (5) as follows: * (G, G * ) = * (g(A, X)) + * (6)\nBy efficient tuning, the new error bound * can be further reduced. In section 4.6, we empirically demonstrate that * can be significantly smaller than via efficient training. That means our method supports more flexible transformations on graphs to match various pre-training strategies.", "publication_ref": ["b26", "b35", "b1", "b34", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Efficiency.", "text": "Assume an input graph has nodes and edges and the prompt graph has tokens with edges. Let the graph model contain layers and the maximum dimension of one layer be . The parameter complexity of the prompt graph is only ( ). In contrast, some typical graph models (e.g., GAT [32]) usually contain\n( 2 +\n) parameters to generate node embedding and additional ( ) parameters to obtain the whole graph representation ( is the multi-head number). The parameters may be even larger in other graph neural networks (e.g., graph transformer [37]). In our prompt learning framework, we only need to tune the prompt with the pre-trained graph model frozen, making the training process converge faster than traditional transfer tuning.\nFor the time complexity, a typical graph model (e.g., GCN [34]) usually needs ( 2 + + ) time to generate node embedding via message passing and then obtain the whole graph representation (e.g., ( ) for summation pooling). By inserting the prompt into the original graph, the total time is ( ( + ) 2 + ( + ) +( + ) ). Compared with the original time, the additional time cost is only\n( 2 + + )\nwhere \u226a , \u226a , \u226a . Besides the efficient parameter and time cost, our work is also memory friendly. Taking node classification as an example, the memory cost of a graph model largely includes parameters, graph features, and graph structure information. As previously discussed, our method only needs to cache the prompt parameters, which are far smaller than the original graph model. For the graph features and structures, traditional methods usually need to feed the whole graph into a graph model, which needs huge memory to cache these contents. However, we only need to feed an induced graph to the model for each node, the size of which is usually far smaller than the original graph. Notice that in many real-world applications, we are often interested in only a few parts of the total nodes, which means our method can stop timely if there is no more node to be predicted and we do not need to propagate messages on the whole graph either. This is particularly helpful for large-scale data.\n3.5.4 Compatibility. Unlike GPPT, which can only use binary edge prediction as a pretext, and is only applicable for node classification as downstream tasks, our framework can support node-level, edge-level, and graph-level downstream tasks, and adopt various graph-level pretexts with only a few steps of tuning. Besides, when transferring the model to different tasks, traditional approaches usually need to additionally tune a task head. In contrast, our method focuses on the input data manipulation and it relies less on the downstream tasks. This means we have a larger tolerance for the task head. For example, in section 4.3, we study the transferability from other domains or tasks but we only adapt our prompt, leaving the source task head unchanged. We can even select some specific pretext and customize the details of our prompt without any tuned task head. Here we present a case that does not need to tune a task head and we evaluate its feasibility in section 4.4.\nPrompt without Task Head Tuning: Pretext: GraphCL [36], a graph contrastive learning task that tries to maximize the agreement between a pair of views from the same graph. Downstream Tasks: node/edge/graph classification. Prompt Answer: node classification. Assume there are categories for the nodes. We design the prompt graph with sub-graphs (a.k.a sub-prompts) where each sub-graph has tokens. Each sub-graph corresponds to one node category. Then we can generate graph views for all input graphs. We classify the target node with label \u2113 (\u2113 = 1, 2, \u2022 \u2022 \u2022 , ) if the \u2113-th graph view is closest to the induced graph. It is similar to edge/graph classification. Interestingly, by shrinking the prompt graph as isolate tokens aligned with node classes and replacing the induced graphs with the whole graph, our prompt format can degenerate to GPPT, which means we can also leverage edge-level pretext for node classification. Since this format is exactly the same as GPPT, we will not discuss it anymore. Instead, we directly compare GPPT on node classification with our method.", "publication_ref": ["b31", "b36", "b33", "b35"], "figure_ref": [], "table_ref": []}, {"heading": "EVALUATION", "text": "In this section, we extensively evaluate our method with other approaches on node-level, edge-level, and graph-level tasks of graphs. In particular, we wish to answer the following research questions: Q1: How effective is our method under the few-shot learning background for multiple graph tasks? Q2: How adaptable is our method when transferred to other domains or tasks? Q3: How do the main components of our method impact the performance? Q4: How efficient is our model compared with traditional approaches? Q5: How powerful is our method when we manipulate graphs?", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Settings", "text": "4.1.1 Datasets. : We compare our methods with other approaches on five public datasets including Cora [34], CiteSeer [34], Reddit [8], Amazon [23], and Pubmed [34]. Detailed statistics are presented in Table 1 where the last column refers to the number of node classes. To conduct edge-level and graph-level tasks, we sample edges and subgraphs from the original data where the label of an edge is decided by its two endpoints and the subgraph label follows the majority of the subgraph nodes. For example, if nodes have 3 different classes, say 1 , 2 , 3 , then edge-level tasks contain at least 6 categories ( 1 , 2 , 3 , 1 2 , 1 3 , 2 3 ). We also evaluate additional graph classification and link prediction on more specialized datasets where the graph label and the link label are inborn and not related to any node (see Appendix A).", "publication_ref": ["b33", "b33", "b7", "b22", "b33"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Approaches. Compared approaches are from three categories:", "text": "(1) Supervised methods: these methods directly train a GNN model on a specific task and then directly infer the result. We here take three famous GNN models including GAT [32], GCN [34], and Graph Transformer [25] (short as GT). These GNN models are also included as the backbones of our prompt methods. (2) Pre-training with fine-tuning: These methods first pre-train a GNN model in a self-supervised way such as GraphCL [36] and SimGRACE [35], then the pre-trained model will be fine-tuned for a new downstream task.\n(3) Prompt methods: With a pre-trained model frozen and a learnable prompt graph, our prompt method aims to change the input graph and reformulate the downstream task to fit the pre-training strategies.", "publication_ref": ["b31", "b33", "b24", "b35", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "Implementations.", "text": "We set the number of graph neural layers as 2 with a hidden dimension of 100. To study the transferability across different graph data, we use SVD (Singular Value Decomposition) to reduce the initial features to 100 dimensions. The token number of our prompt graph is set as 10. We also discuss the impact of token numbers in section 4.4 where we change the token number from 1 to 20. We use the Adam optimizer for all approaches. The learning rate is set as 0.001 for most datasets. In the meta-learning stage, we split all the node-level, edge-level, and graph-level tasks randomly in 1:1 for meta-training and meta-testing. Reported results are averaged on all tested tasks. More implementation details are shown in Appendix A, in which we also analyze the performance on more datasets and more kinds of tasks such as regression, link prediction, and so on.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Multi-Task Performance with Few-shot Learning Settings (RQ1)", "text": "We compared our prompt-based methods with other mainstream training schemes on node-level, edge-level, and graph-level tasks under the few-shot setting. We repeat the evaluation 5 times and report the average results in Table 2, Table 12 (Appendix A), and Table 13 (Appendix A). From the results, we can observe that most supervised methods are very hard to achieve better performance compared with pre-train methods and prompt methods. This is because the empirical annotations required by supervised frameworks in the few-shot setting are very limited, leading to poor performance. In contrast, pre-training approaches contain more prior knowledge, making the graph model rely less on data labels. However, to achieve better results on a specific task, we usually need to carefully select an appropriate pre-training approach and carefully tune the model to match the target task, but this huge effort is not ensured to be applicable to other tasks. The gap between pre-training strategies and downstream tasks is still very large, making the graph model very hard to transfer knowledge on multi-task settings (we further discuss the transferability in section 4.3.) Compared with pre-training approaches, our solutions further improve the compatibility of graph models. The reported improvements range from 1.10% to 8.81% on node-level tasks, 1.28% to 12.26% on edge-level tasks, and 0.14% to 10.77% on graph-level tasks. In particular, we also compared our node-level performance with the previously mentioned node-level prompt model GPPT in Table 2. Kindly note that our experiment settings are totally different from GPPT. In GPPT, they study the few-shot problem by masking 30% or 50% data labels. However, in our paper, we propose a more challenging problem: how does the model perform if we further reduce the label data? So in our experiment, each class only has 100 labeled samples. This different setting makes our labeled ratio approximately only 25% on Cora, 18% on CiteSeer, 1.7% on Reddit, 7.3% on Amazon, and 1.5% on Pubmed, which are far less than the reported GPPT (50% labeled).", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2", "tab_1", "tab_1", "tab_2"]}, {"heading": "Transferability Analysis (RQ2)", "text": "To evaluate the transferability, we compared our method with the hard transfer method and the fine-tuning method. Here the hard transfer method means we seek the source task model which has the same task head as the target task and then we directly conduct the model inference on the new task. The fine-tune method means we load the source task model and then tune the task head for the new task. We evaluate the transferability from two perspectives: (1) how effectively is the model transferred to different tasks within the same domain? and (2) how effectively is the model transferred to different domains?", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Transferability to Different Level Tasks.", "text": "Here we pre-train the graph neural network on Amazon, then conduct the model on two source tasks (graph level and node level), and further evaluate the performance on the target task (edge level). For simplicity, both source tasks and the target task are built as binary classifications with 1 : 1 positive and negative samples (we randomly select a class as the positive label and sample negatives from the rest). We report the results in Table 3, from which we have two observations: First, our prompt method significantly outperforms the other approaches and the prediction results make sense. In contrast, the problem of the hard transfer method is that the source model sometimes can not well decide on the target tasks because the target classes may be far away from the source classes. This may even cause negative transfer results (results that are lower than random guess). In most cases, the fine-tuning method can output meaningful results with a few steps of tuning but it can still encounter a negative transfer problem. Second, the graph-level task has better adaptability than the node-level task for the edge-level target, which is in line with our previous intuition presented in Figure 3 (section 3.2).", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Transferability to Different Domains.", "text": "We also conduct the model on Amazon and PubMed as source domains, then load the model states from these source domains and report the performance on the target domain (Cora). Since different datasets have various input feature dimensions, we here use SVD to unify input features from all domains as 100 dimensions. Results are shown in Table 4, from which we can find that the good transferability of our prompt also exists when we deal with different domains.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Ablation Study (RQ3)", "text": "In this section, we compare our complete framework with four variants: \"w/o meta\" is the prompt method without meta-learning    any across links between prompt tokens and the input graphs. We report the performance in Figure 5, from which we can find the meta-learning and token structure all contribute significantly to the final results. In particular, the inserting pattern between a prompt graph and the input graph plays a very crucial role in the final performance. As previously discussed, the purpose of the promptbased method is to relieve the difficulty of traditional \"pre-train, fine-tuning\" by filling the gap between the pre-training model and the task head. This means the prompt graph is proposed to further improve the fine-tuning performance. This is particularly important when we transfer the model across different tasks/domains, which proposes harder demand for the task head. As suggested in Figure 5, even when we totally remove the tunable task head, the \"w/o h\" variant can still perform very competitively, which suggests the powerful capability of bridging upstream and downstream tasks.", "publication_ref": [], "figure_ref": ["fig_5", "fig_5"], "table_ref": []}, {"heading": "Efficiency Analysis (RQ4)", "text": "Figure 6 presents the impact of increasing token number on the model performance, from which we can find that most tasks can reach satisfactory performance with very limited tokens, making the complexity of the prompt graph very small. The limited token numbers make our tunable parameter space far smaller than traditional methods, which can be seen in Table 5. This means our method can be efficiently trained with a few steps of tuning. As shown in Figure 7, the prompt-based method converges faster than traditional pre-train and supervised methods, which further suggests the efficiency advantages of our method.  ", "publication_ref": [], "figure_ref": ["fig_6", "fig_8"], "table_ref": ["tab_5"]}, {"heading": "Flexibility on Graph Transformation (RQ5)", "text": "As discussed in section 3.5.2, the flexibility of data transformation is the bottleneck of prompt-based methods. Here we manipulate several graphs by dropping nodes, dropping edges, and masking features, then we calculate the error bound mentioned in Equation 5 and 6. We compare the original error with the naive prompt mentioned in Equation 5, and our prompt graph with 3, 5, and 10 tokens. As shown in Table 6, our designed prompt graph significantly reduces the error between the original graph and the manipulated graph. This means our method is more powerful to stimulate various graph transformations and can further support significant improvement for downstream tasks. This capability can also be observed in the graph visualization from two approaches. As shown in Figure 8, the graph representations from a pre-trained model present lower resolution to node classes compared with our prompted graph.", "publication_ref": [], "figure_ref": ["fig_10"], "table_ref": ["tab_6"]}, {"heading": "CONCLUSION", "text": "In this paper, we study the multi-task problem of graph prompts with few-shot settings. We propose a novel method to reformulate different-level tasks to unified ones and further design an effective prompt graph with a meta-learning technique. We extensively evaluate the performance of our method. Experiments demonstrate the effectiveness of our framework.         ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A APPENDIX", "text": "In this section, we supplement more experiments to evaluate the effectiveness of our framework further. The source code is publicly available at https://anonymous.4open.science/r/mpg Additional Datasets Besides the datasets mentioned in the main experiments of our paper, we here supplement more datasets in Table 7 to further evaluate the effectiveness of our framework. Specifically, ENZYMES and ProteinsFull are two molecule/protein datasets that are used in our additional graph-level classification tasks. Movielens and QM9 are used to evaluate the performance of our method on edge-level and graph-level regression, respectively.\nIn particular, Movielens contains user's rating scores to the movies, each edge in which has a score value ranging from 0 to 5. QM9 is a molecule graph dataset where each graph has 19 regression targets, which are treated as graph-level multi-output regression. Person-alityCafe and Facebook datasets are used to test the performance of link prediction, both of which are social networks where edges denote the following/quoting relations.\nMulti-label v.s. Multi-class Classification In the main experiments, we treat the classification task as a multi-label problem. Here we present the experimental results under a multi-class setting. As reported in Table 8, our prompt-based method still outperforms the rest methods.\nAdditional Graph-level Classification Here, we evaluate the graph-level classification performance where the graph label is not impacted by nodes' attributes. As shown in Table 9, our method is more effective in the multi-class graph classification, especially in the few-shot setting.\nEdge/Graph-level Regression Beyond classification tasks, our method can also support to improve graph models on regression tasks. Here, we evaluate the regression performance of both graphlevel (QM9) and edge-level (MovieLens) datasets by MAE (mean absolute error) and MSE (mean squared error). We only feed 100shot edge induced graphs for the model and the results are shown in Table 10, from which we can observe that our prompt-based methods outperform traditional approaches.\nLink Prediction Beyond edge classification, link prediction is also a widely studied problem in the graph learning area. Here, the edges are split into three parts: (1) 80% of the edges are for message passing only. (2) 10% of the rest edges as the supervision training set. and (3) the rest edges as the testing set. For each edge in the training set and the testing set, we treat these edges as positive samples and sample non-adjacent nodes as negative samples. We generate the edge-induced graph for these node pairs according to the first part edges. The graph label is assigned as positive if the node pairs have a positive edge and vice versa. To further evaluate our method's potential in the extremely limited setting, we only sample 100 positive edges from the training set to train our model. In the testing stage, each positive edge has 100 negative edges. We evaluate the performance by MRR (mean reciprocal rank), and Hit Ratio@ 1, 5, 10. Results from Table 11 demonstrate that the performance of our prompt-based method still keeps the best in most cases. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_9", "tab_10", "tab_1", "tab_1"]}], "references": [{"ref_id": "b0", "title": "Efficient top-k shortest-path distance queries on large networks by pruned landmark labeling", "journal": "", "year": "2015", "authors": "Takuya Akiba; Takanori Hayashi; Nozomi Nori; Yoichi Iwata; Yuichi Yoshida"}, {"ref_id": "b1", "title": "Unsupervised inductive graph-level representation learning via graph-graph proximity", "journal": "", "year": "1988", "authors": "Yunsheng Bai; Hao Ding; Yang Qiao; Agustin Marinovic; Ken Gu; Ting Chen; Yizhou Sun; Wei Wang"}, {"ref_id": "b2", "title": "Language models are few-shot learners", "journal": "Advances in neural information processing systems", "year": "2020", "authors": "Tom Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared D Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell"}, {"ref_id": "b3", "title": "Multi-level graph convolutional networks for crossplatform anchor link prediction", "journal": "", "year": "2020", "authors": "Hongxu Chen; Hongzhi Yin; Xiangguo Sun; Tong Chen; Bogdan Gabrys; Katarzyna Musial"}, {"ref_id": "b4", "title": "BrainNet: Epileptic Wave Detection from SEEG with Hierarchical Graph Diffusion Learning", "journal": "", "year": "2022", "authors": "Junru Chen; Yang Yang; Tao Yu; Yingying Fan; Xiaolong Mo; Carl Yang"}, {"ref_id": "b5", "title": "Prompt Tuning for Graph Neural Networks", "journal": "", "year": "2022", "authors": "Taoran Fang; Yunchao Zhang; Yang Yang; Chunping Wang"}, {"ref_id": "b6", "title": "Making Pre-trained Language Models Better Few-shot Learners", "journal": "", "year": "2021", "authors": "Tianyu Gao; Adam Fisch; Danqi Chen"}, {"ref_id": "b7", "title": "Inductive representation learning on large graphs. Advances in neural information processing systems", "journal": "", "year": "2017", "authors": "Will Hamilton; Zhitao Ying; Jure Leskovec"}, {"ref_id": "b8", "title": "A Multi-Strategy based Pre-Training Method for Cold-Start Recommendation", "journal": "ACM Transactions on Information Systems", "year": "2022", "authors": "Bowen Hao; Hongzhi Yin; Jing Zhang; Cuiping Li; Hong Chen"}, {"ref_id": "b9", "title": "GraphMAE: Self-Supervised Masked Graph Autoencoders", "journal": "", "year": "2022", "authors": "Zhenyu Hou; Xiao Liu; Yukuo Cen; Yuxiao Dong; Hongxia Yang; Chunjie Wang; Jie Tang"}, {"ref_id": "b10", "title": "Strategies For Pre-training Graph Neural Networks", "journal": "", "year": "2020", "authors": "W Hu; J Liu; M Gomes; P Zitnik; V Liang; J Pande;  Leskovec"}, {"ref_id": "b11", "title": "Wiener Graph Deconvolutional Network Improves Graph Self-Supervised Learning", "journal": "", "year": "2023", "authors": "Cheng Jiashun; Li Man; Li Jia; Fugee Tsung"}, {"ref_id": "b12", "title": "Self-supervised learning on graphs: Deep insights and new direction", "journal": "", "year": "2020", "authors": "Wei Jin; Tyler Derr; Haochen Liu; Yiqi Wang; Suhang Wang; Zitao Liu; Jiliang Tang"}, {"ref_id": "b13", "title": "The Power of Scale for Parameter-Efficient Prompt Tuning", "journal": "", "year": "2021", "authors": "Brian Lester; Rami Al-Rfou; Noah Constant"}, {"ref_id": "b14", "title": "Predicting path failure in time-evolving graphs", "journal": "", "year": "2019", "authors": "Jia Li; Zhichao Han; Hong Cheng; Jiao Su; Pengyun Wang; Jianfeng Zhang; Lujia Pan"}, {"ref_id": "b15", "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation", "journal": "", "year": "2021", "authors": "Lisa Xiang; Percy Li;  Liang"}, {"ref_id": "b16", "title": "Vision-Language Pre-Training for Multimodal Aspect-Based Sentiment Analysis", "journal": "", "year": "2022", "authors": "Yan Ling; Jianfei Yu; Rui Xia"}, {"ref_id": "b17", "title": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing", "journal": "", "year": "2021", "authors": "Pengfei Liu; Weizhe Yuan; Jinlan Fu; Zhengbao Jiang; Hiroaki Hayashi; Graham Neubig"}, {"ref_id": "b18", "title": "P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks", "journal": "Short Papers", "year": "2022", "authors": "Xiao Liu; Kaixuan Ji; Yicheng Fu; Weng Tam; Zhengxiao Du; Zhilin Yang; Jie Tang"}, {"ref_id": "b19", "title": "Recent advances in natural language processing via large pre-trained language models: A survey", "journal": "", "year": "2021", "authors": "Hayley Bonan Min; Elior Ross;  Sulem;  Amir Pouran Ben; Thien Veyseh; Oscar Huu Nguyen; Eneko Sainz; Ilana Agirre; Dan Heinz;  Roth"}, {"ref_id": "b20", "title": "Exploring low-dimensional intrinsic task subspace via prompt tuning", "journal": "", "year": "2021", "authors": "Yujia Qin; Xiaozhi Wang; Yusheng Su; Yankai Lin; Ning Ding; Zhiyuan Liu; Juanzi Li; Lei Hou; Peng Li; Maosong Sun"}, {"ref_id": "b21", "title": "Getting closer to AI complete question answering: A set of prerequisite real tasks", "journal": "", "year": "2020", "authors": "Anna Rogers; Olga Kovaleva; Matthew Downey; Anna Rumshisky"}, {"ref_id": "b22", "title": "Pitfalls of graph neural network evaluation", "journal": "", "year": "2018", "authors": "Oleksandr Shchur; Maximilian Mumme; Aleksandar Bojchevski; Stephan G\u00fcnnemann"}, {"ref_id": "b23", "title": "Towards out-of-distribution generalization: A survey", "journal": "", "year": "2021", "authors": "Zheyan Shen; Jiashuo Liu; Yue He; Xingxuan Zhang; Renzhe Xu; Han Yu; Peng Cui"}, {"ref_id": "b24", "title": "Masked label prediction: Unified message passing model for semisupervised classification", "journal": "", "year": "2020", "authors": "Yunsheng Shi; Zhengjie Huang; Shikun Feng; Hui Zhong; Wenjin Wang; Yu Sun"}, {"ref_id": "b25", "title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts", "journal": "EMNLP", "year": "2020", "authors": "Taylor Shin; Yasaman Razeghi; Robert L Logan; I V ; Eric Wallace; Sameer Singh"}, {"ref_id": "b26", "title": "GPPT: Graph pre-training and prompt tuning to generalize graph neural networks", "journal": "", "year": "2022", "authors": "Mingchen Sun; Kaixiong Zhou; Xin He; Ying Wang; Xin Wang"}, {"ref_id": "b27", "title": "Self-supervised Hypergraph Representation Learning for Sociological Analysis", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": "2023", "authors": "Xiangguo Sun; Hong Cheng; Bo Liu; Jia Li; Hongyang Chen; Guandong Xu; Hongzhi Yin"}, {"ref_id": "b28", "title": "Multi-level hyperedge distillation for social linking prediction on sparsely observed networks", "journal": "", "year": "2021", "authors": "Xiangguo Sun; Hongzhi Yin; Bo Liu; Hongxu Chen; Qing Meng; Wang Han; Jiuxin Cao"}, {"ref_id": "b29", "title": "Structure Learning Via Meta-Hyperedge for Dynamic Rumor Detection", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": "2022", "authors": "Xiangguo Sun; Hongzhi Yin; Bo Liu; Qing Meng; Jiuxin Cao; Alexander Zhou; Hongxu Chen"}, {"ref_id": "b30", "title": "Rethinking Graph Neural Networks for Anomaly Detection", "journal": "", "year": "2022", "authors": "Jianheng Tang; Jiajin Li; Ziqi Gao; Jia Li"}, {"ref_id": "b31", "title": "Graph Attention Networks", "journal": "", "year": "2018", "authors": "Petar Veli\u010dkovi\u0107; Guillem Cucurull; Arantxa Casanova; Adriana Romero; Pietro Li\u00f2; Yoshua Bengio"}, {"ref_id": "b32", "title": "Afec: Active forgetting of negative transfer in continual learning", "journal": "Advances in Neural Information Processing Systems", "year": "2021-06", "authors": "Liyuan Wang; Mingtian Zhang; Zhongfan Jia; Qian Li; Chenglong Bao; Kaisheng Ma"}, {"ref_id": "b33", "title": "Semi-supervised classification with graph convolutional networks", "journal": "", "year": "2016", "authors": "Max Welling; N Thomas;  Kipf"}, {"ref_id": "b34", "title": "SimGRACE: A Simple Framework for Graph Contrastive Learning without Data Augmentation", "journal": "", "year": "2022", "authors": "Jun Xia; Lirong Wu; Jintao Chen; Bozhen Hu; Stan Z Li"}, {"ref_id": "b35", "title": "Graph contrastive learning with augmentations", "journal": "Advances in Neural Information Processing Systems", "year": "2020", "authors": "Yuning You; Tianlong Chen; Yongduo Sui; Ting Chen; Zhangyang Wang; Yang Shen"}, {"ref_id": "b36", "title": "Graph transformer networks", "journal": "Advances in neural information processing systems", "year": "2019", "authors": "Seongjun Yun; Minbyul Jeong; Raehyun Kim; Jaewoo Kang; Hyunwoo J Kim"}, {"ref_id": "b37", "title": "Factual Probing Is [MASK]: Learning vs. Learning to Recall", "journal": "", "year": "2021", "authors": "Zexuan Zhong; Dan Friedman; Danqi Chen"}, {"ref_id": "b38", "title": "Efficient single-source shortest path and distance queries on large graphs", "journal": "", "year": "2013", "authors": "Andy Diwen Zhu; Xiaokui Xiao; Sibo Wang; Wenqing Lin"}, {"ref_id": "b39", "title": "Graph contrastive learning with adaptive augmentation", "journal": "", "year": "2021", "authors": "Yanqiao Zhu; Yichen Xu; Feng Yu; Qiang Liu; Shu Wu; Liang Wang"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Fine-tuning, Pre-training, and Prompting.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Our graph prompt inspired by the language prompt.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "(a) Induced graphs for nodes (b) Induced graphs for edges", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Induced graphs for nodes and edges", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Algorithm 1 :1Overall Learning Process Input: Overall pipeline , | * with prompt parameter , pre-trained model with frozen parameter * , and task head parameterized by ; Multi-task episodes E = {E 1 , \u2022 \u2022 \u2022 , E }; Output: Optimal pipeline * , * | * 1 Initialize and 2 while not done do // inner adaptation 3 Sample E \u2208 E where E = (T E , L E , S E , Q E )", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 :5Figure 5: Effectiveness of main components", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 6 :6Figure 6: Impact of token numbers", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 7 :7Figure 7: Training losses with epochs. Mean values and 65% confidence intervals by 5 repeats with different seeds.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 8 :8Figure 8: Visualization of graph representations.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Statistics of datasets.", "figure_data": "Dataset #Nodes#Edges#Features #LabelsCora2,7085,4291,4337CiteSeer3,3279,1043,7036Reddit232,965 23,213,83860241Amazon 13,752491,72276710Pubmed 19,71788,6485003"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Node-level performance (%) with 100-shot setting. IMP (%): the average improvement of prompt over the rest. 73.21 82.97 83.00 83.20 89.33 55.64 62.03 65.38 79.00 73.42 97.81 75.00 77.56 79.72 GCN 77.55 77.45 83.71 88.00 81.79 94.79 54.38 52.47 56.82 95.36 93.99 96.23 53.64 66.67 69.89 GT 74.25 75.21 82.04 86.33 85.62 90.13 61.50 61.38 65.56 85.50 86.01 93.01 51.50 67.34 71.91 76.78 81.96 87.64 88.40 89.93 57.37 66.42 67.43 78.67 72.26 95.65 76.03 77.05 80.02 GraphCL+GCN 78.75 79.13 84.90 87.49 89.36 90.25 55.00 65.52 74.65 96.00 95.92 98.33 69.37 70.00 74.74 GraphCL+GT 73.80 74.12 82.77 88.50 88.92 91.25 63.50 66.06 68.04 94.39 93.62 96.97 75.00 78.45 75.05 SimGRACE+GAT 76.85 77.48 83.37 90.50 91.00 91.56 56.59 65.47 67.77 84.50 84.73 89.69 72.50 68.21 81.", "figure_data": "Training schemesMethodsCora Acc F1 AUC Acc F1 AUC Acc F1 AUC Acc F1 AUC Acc F1 AUC CiteSeer Reddit Amazon Pubmedsupervised 74.45 pre-train GAT + GraphCL+GAT 76.05 97 fine-tune SimGRACE+GCN 77.20 76.39 83.13 83.50 84.21 93.22 58.00 55.81 56.93 95.00 94.50 98.03 77.50 75.71 87.53SimGRACE+GT77.40 78.11 82.95 87.50 87.05 91.85 66.00 69.95 70.03 79.00 73.42 97.58 70.50 73.30 74.22GraphCL+GAT76.50 77.26 82.99 88.00 90.52 91.82 57.84 67.02 75.33 80.01 75.62 97.96 77.50 78.26 83.02GraphCL+GCN79.20 79.62 85.29 88.50 91.59 91.43 56.00 68.57 78.82 96.50 96.37 98.70 72.50 72.64 79.57promptGraphCL+GT SimGRACE+GAT75.00 76.00 83.36 91.00 91.00 93.29 65.50 66.08 68.86 95.50 95.43 97.56 76.50 79.11 76.00 76.95 78.51 83.55 93.00 93.14 92.44 57.63 66.64 69.43 95.50 95.43 97.56 73.00 74.04 81.89SimGRACE+GCN77.85 76.57 83.79 90.00 89.47 94.87 59.50 55.97 59.46 95.00 95.24 98.42 78.00 78.22 87.66SimGRACE+GT78.75 79.53 85.03 91.00 91.26 95.62 69.50 71.43 70.75 86.00 83.72 98.24 73.00 73.79 76.64IMP (%)1.47 1.94 1.10 3.81 5.25 2.05 3.97 5.04 6.98 4.49 5.84 2.24 8.81 4.55 4.62Reported Acc of GPPT (Label Ratio 50%) 77.16 --65.81 --92.13 --86.80 --72.23 --appr. Label Ratio of our 100-shot setting\u223c 25%\u223c 18%\u223c 1.7%\u223c 7.3%\u223c 1.5%"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Transferability (%) on Amazon from different level tasks spaces. Source tasks: graph-level tasks and node-level tasks. Target task: edge-level tasks.", "figure_data": "Source task Methods Accuracy F1-score AUC scorehard51.5065.9640.34graph levelfine-tune 62.5070.5953.91prompt70.5071.2274.02hard40.5011.8529.48node levelfine-tune 46.0054.2437.26prompt59.5068.7355.90"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": "fullw/o metaw/o hw/o token structurew/o inserting100.0090.0080.0070.0060.00AccF1AUCAccF1AUCAccF1AUCnode-leveledge-levelgraph-levelTransferability (%) from different domains. Sourcedomains: Amazon and PubMed. Target domain: CoraSource DomainsAmazonPubMedTaskshard fine-tune prompt hard fine-tune promptnode levelAcc F1 AUC 17.56 26.9 13.1164.14 77.59 88.7965.07 80.23 92.5955.62 66.33 82.3457.93 70.00 83.3462.07 76.60 88.46edge levelAcc 17.00 F1 10.51 AUC 4.2677.00 81.58 94.2782.00 84.62 96.1910.00 2.17 6.1590.50 89.73 93.8996.50 91.80 94.70graph levelAcc 46.00 F1 62.76 AUC 54.2387.50 89.11 86.3388.00 88.12 94.9950.00 10.00 90.8591.00 93.90 91.4795.50 95.60 98.47step; \"w/o h\" is our method without task head tuning, which ispreviously introduced in section 3.5.4; \"w/o token structure\" isthe prompt where all the tokens are treated as isolated withoutany inner connection; and \"w/o inserting\" is the prompt without"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Tunable parameters comparison. RED (%): average reduction of the prompt method to others.", "figure_data": "MethodsCoraCiteSeer Reddit Amazon Pubmed RED (%)GAT\u223c 155K \u223c 382K\u223c 75K\u223c 88K\u223c 61K95.4\u2193GCN\u223c 154K \u223c 381K\u223c 75K\u223c 88K\u223c 61K95.4\u2193GT\u223c 615K \u223c 1.52M \u223c 286K \u223c 349K\u223c 241K98.8\u2193prompt\u223c 7K\u223c 19K\u223c 3K\u223c 4K\u223c 3K-node-level Accnode-level F1node-level AUCedge-level Accedge-level F1edge-level AUC100.0090.0080.0015101520"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Error bound discussed by section 3.5.2 RED (%): average reduction of each method to the original error.", "figure_data": "Prompt SolutionsToken NumberDrop NodesDrop EdgesMask FeaturesRED (%)Original Error (without prompt)00.9917 2.63306.8209-Naive Prompt (Equation 5)10.8710 0.52412.083566.70\u2193Our Prompt Graph30.0875 0.23370.654290.66\u2193(with token, structure,50.0685 0.15130.437293.71\u2193and inserting patterns)100.0859 0.11440.260095.59\u2193"}, {"figure_label": "910", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Additional graph-level classification. Graph/edge-level regression with few-shot settings.", "figure_data": "MethodsProteinsFull (100 shots) ENZYMES (50 shots) Acc (%) Macro F1 (%) Acc (%) Macro F1 (%)Supervised66.6465.0331.3330.25Pre-train + Fine-tune66.5066.4334.6733.94Prompt70.5070.1735.0034.92Prompt w/o h68.5068.5036.6734.05TasksGraph RegressionEdge RegressionDatasetsQM9 (100 shots) MovieLens (100 shots)MethodsMAEMSEMAEMSESupervised0.30060.13270.22850.0895Pre-train + Fine-tune 0.15390.03510.21710.0774Prompt0.13840.02950.19490.0620Prompt w/o h0.14240.03410.21200.0744"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Statistics of Additional Datasets", "figure_data": "Dataset#Nodes #Edges #Features #Labels #GraphsENZYMES19,58074,564216600ProteinsFull43,471162,0883221,113Movielens10,352100,836100-1QM92,333,625 4,823,49816-129,433PersonalityCafe 100,340 3,788,03210001Facebook4,03988,2341,28301"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Multi-class node classification (100-shots)", "figure_data": "MethodsCora Acc (%) Macro F1 (%) Acc (%) Macro F1 (%) CiteSeerSupervised74.1173.2677.3377.64Pre-train and Fine-tune77.9777.6379.6779.83Prompt80.1279.7580.5080.65Prompt w/o h78.5578.1880.0080.05Reported Acc of GPPT (Label Ratio 50%)77.16-65.81-"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Evaluation on link prediction (100-shot settings)", "figure_data": "DatasetsPersonalityCafeFacebookMethodsMRR Hit@1 Hit@5 Hit@10 MRR Hit@1 Hit@5 Hit@10Supervised0.180.040.240.560.130.060.170.35Pre-train + Fine-tune0.130.050.120.340.100.020.160.33Prompt0.200.070.320.600.190.100.230.39Prompt w/o h 0.200.060.300.500.150.090.150.33Label Ratio\u223c 0.003% (training) \u223c 80%(message passing)\u223c 0.1% (training) \u223c 80%(message passing)"}], "formulas": [{"formula_id": "formula_0", "formula_text": "G = (V, E) where V = { 1 , 2 , \u2022 \u2022 \u2022 , }", "formula_coordinates": [4.0, 53.8, 645.68, 240.25, 20.34]}, {"formula_id": "formula_1", "formula_text": "P = { 1 , 2 , \u2022 \u2022 \u2022 , | P | }", "formula_coordinates": [4.0, 210.38, 701.01, 83.44, 9.43]}, {"formula_id": "formula_2", "formula_text": ". S = {( , )| ,", "formula_coordinates": [4.0, 402.95, 193.42, 70.6, 8.46]}, {"formula_id": "formula_3", "formula_text": "A = | P | \u22121 \u222a =1 = +1 { }", "formula_coordinates": [4.0, 409.51, 261.4, 57.04, 24.29]}, {"formula_id": "formula_4", "formula_text": "+ | P | =1", "formula_coordinates": [4.0, 361.13, 429.21, 26.03, 14.99]}, {"formula_id": "formula_5", "formula_text": "= (p \u2022 x ), if (p \u2022 x ) > 0, otherwise(1)", "formula_coordinates": [4.0, 378.87, 461.32, 179.87, 19.39]}, {"formula_id": "formula_6", "formula_text": "+ | P | =1 p .", "formula_coordinates": [4.0, 385.97, 498.85, 39.24, 14.99]}, {"formula_id": "formula_7", "formula_text": "= \u22121 \u2212 \u2207 \u22121 L D \u22121 , \u22121 | * = \u22121 \u2212 \u2207 \u22121 L D \u22121 , \u22121 | * (2)", "formula_coordinates": [5.0, 111.13, 102.92, 183.46, 33.09]}, {"formula_id": "formula_8", "formula_text": "* , * = arg min , \u2211\ufe01 \u2208 T L D , | * (3)", "formula_coordinates": [5.0, 110.94, 190.13, 183.64, 21.76]}, {"formula_id": "formula_9", "formula_text": "\u2190 \u2212 \u2022 = \u2212 \u2022 \u2211\ufe01 \u2208 T \u2207 L D , | * = \u2212 \u2022 \u2211\ufe01 \u2208 T \u2207 L D , | * \u2022 \u2207 ( ) = \u2212 \u2022 \u2211\ufe01 \u2208 T \u2207 L D , | * \u2022 I\u2212 H L D , | *(4)", "formula_coordinates": [5.0, 68.25, 245.69, 226.34, 90.62]}, {"formula_id": "formula_10", "formula_text": "fine task batch T E = {T ( ) E , T ( ) E , T (\u2113 ) E } where each subset T (\u22b3) E = { \u22b31 , \u2022 \u2022 \u2022 , \u22b3 \u22b3 }; loss function sets L E = {L ( ) , L ( ) , L (\u2113 ) }, sup- porting data S E = {S ( ) E , S ( ) E , S (\u2113 ) E } where each subset S (\u22b3) E = {D \u22b31 , \u2022 \u2022 \u2022 , D \u22b3 \u22b3 }, and query data Q E = {Q ( ) E , Q ( ) E , Q (\u2113 ) E } where S (\u22b3) E = {D \u22b31 , \u2022 \u2022 \u2022 , D \u22b3 \u22b3 }.", "formula_coordinates": [5.0, 53.8, 491.79, 241.77, 74.49]}, {"formula_id": "formula_11", "formula_text": "4 for \u22b3 \u2208 T E , \u22b3 = , , \u2113 do 5 \u22b3 , \u22b3 \u2190 , 6 \u22b3 \u2190 \u22b3 \u2212 \u2207 \u22b3 L (\u22b3) D \u22b3 \u22b3 , \u22b3 | * 7 \u22b3 \u2190 \u22b3 \u2212 \u2207 \u22b3 L (\u22b3) D \u22b3 \u22b3 , \u22b3 | * 8 end // outer meta update 9", "formula_coordinates": [5.0, 320.76, 204.98, 182.45, 90.33]}, {"formula_id": "formula_12", "formula_text": "Q E = {D \u22b3 | \u22b3 \u2208 T E , \u22b3 = , , \u2113 } 10 end 11 return * , * | *", "formula_coordinates": [5.0, 317.29, 299.9, 151.98, 33.18]}, {"formula_id": "formula_14", "formula_text": "( 2 +", "formula_coordinates": [6.0, 60.55, 265.13, 30.07, 9.37]}, {"formula_id": "formula_15", "formula_text": "( 2 + + )", "formula_coordinates": [6.0, 60.55, 407.59, 61.8, 9.37]}], "doi": "10.1145/3580305.3599256"}