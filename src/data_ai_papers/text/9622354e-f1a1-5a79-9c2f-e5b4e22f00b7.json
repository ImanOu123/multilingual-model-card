{"title": "Coreference Resolution in a Modular, Entity-Centered Model", "authors": "Aria Haghighi; Dan Klein", "pub_date": "", "abstract": "Coreference resolution is governed by syntactic, semantic, and discourse constraints. We present a generative, model-based approach in which each of these factors is modularly encapsulated and learned in a primarily unsupervised manner. Our semantic representation first hypothesizes an underlying set of latent entity types, which generate specific entities that in turn render individual mentions. By sharing lexical statistics at the level of abstract entity types, our model is able to substantially reduce semantic compatibility errors, resulting in the best results to date on the complete end-to-end coreference task.", "sections": [{"heading": "Introduction", "text": "Coreference systems exploit a variety of information sources, ranging from syntactic and discourse constraints, which are highly configurational, to semantic constraints, which are highly contingent on lexical meaning and world knowledge. Perhaps because configurational features are inherently easier to learn from small data sets, past work has often emphasized them over semantic knowledge.\nOf course, all state-of-the-art coreference systems have needed to capture semantic compatibility to some degree. As an example of nominal headword compatibility, a \"president\" can be a \"leader\" but cannot be not an \"increase.\" Past systems have often computed the compatibility of specific headword pairs, extracted either from lexical resources (Ng, 2007;Bengston and Roth, 2008;Rahman and Ng, 2009), web statistics (Yang et al., 2005), or surface syntactic patterns (Haghighi and Klein, 2009). While the pairwise approach has high precision, it is neither realistic nor scalable to explicitly enumerate all pairs of compatible word pairs. A more compact approach has been to rely on named-entity recognition (NER) systems to give coarse-grained entity types for each mention (Soon et al., 1999;Ng and Cardie, 2002). Unfortunately, current systems use small inventories of types and so provide little constraint. In general, coreference errors in state-of-theart systems are frequently due to poor models of semantic compatibility (Haghighi and Klein, 2009).\nIn this work, we take a primarily unsupervised approach to coreference resolution, broadly similar to Haghighi and Klein (2007), which addresses this issue. Our generative model exploits a large inventory of distributional entity types, including standard NER types like PERSON and ORG, as well as more refined types like WEAPON and VEHICLE. For each type, distributions over typical heads, modifiers, and governors are learned from large amounts of unlabeled data, capturing type-level semantic information (e.g. \"spokesman\" is a likely head for a PER-SON). Each entity inherits from a type but captures entity-level semantic information (e.g. \"giant\" may be a likely head for the Microsoft entity but not all ORGs). Separately from the type-entity semantic module, a log-linear discourse model captures configurational effects. Finally, a mention model assembles each textual mention by selecting semantically appropriate words from the entities and types.\nDespite being almost entirely unsupervised, our model yields the best reported end-to-end results on a range of standard coreference data sets.", "publication_ref": ["b18", "b1", "b21", "b25", "b12", "b22", "b16", "b12", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Key Abstractions", "text": "The key abstractions of our model are illustrated in Figure 1 and described here.\nMentions: A mention is an observed textual reference to a latent real-world entity. Mentions are as-  sociated with nodes in a parse tree and are typically realized as NPs. There are three basic forms of mentions: proper (denoted NAM), nominal (NOM), and pronominal (PRO). We will often describe proper and nominal mentions together as referring mentions.\nWe represent each mention M as a collection of key-value pairs. The keys are called properties and the values are words. For example, the left mention in Figure 1(a) has a proper head property, denoted NAM-HEAD, with value \"Obama.\" The set of properties we consider, denoted R, includes several varieties of heads, modifiers, and governors (see Section 5.2 for details). Not every mention has a value for every property.\nEntities: An entity is a specific individual or object in the world. Entities are always latent in text. Where a mention has a single word for each property, an entity has a list of signature words. Formally, entities are mappings from properties r \u2208 R to lists L r of \"canonical\" words which that entity uses for that property. For instance in Figure 1(b), the list of nominal heads for the Barack Obama entity includes \"president.\"\nTypes: Coreference systems often make a mention / entity distinction. We extend this hierarchy to include types, which represent classes of entities (PERSON, ORGANIZATION, and so on). Types allow the sharing of properties across entities and mediate the generation of entities in our model (Section 3.1). See Figure 1(c) for a concrete example.\nWe represent each type \u03c4 as a mapping between properties r and pairs of multinomials (\u03b8 r , f r ). Together, these distributions control the lists L r for entities of that type. \u03b8 r is a unigram distribution of words that are semantically licensed for property r. f r is a \"fertility\" distribution over the integers that characterizes entity list lengths. For example, for the type PERSON, \u03b8 r for proper heads is quite flat (there are many last names) but f r is peaked at 1 (people have a single last name).", "publication_ref": [], "figure_ref": ["fig_1", "fig_1", "fig_1", "fig_1"], "table_ref": []}, {"heading": "Generative Model", "text": "We now describe our generative model. At the parameter level, we have one parameter group for the types \u03c4 = (\u03c6, \u03c4 1 , . . . , \u03c4 t ), where \u03c6 is a multinomial prior over a fixed number t of types and the {\u03c4 i } are the parameters for each individual type, described in greater detail below. A second group comprises loglinear parameters \u03c0 over discourse choices, also described below. Together, these two groups are drawn according to P (\u03c4 |\u03bb)P (\u03c0|\u03c3 2 ), where \u03bb and \u03c3 2 are a small number of scalar hyper-parameters described in Section 4.\nConditioned on the parameters (\u03c4 , \u03c0), a document is generated as follows: A semantic module generates a sequence E of entities. E is in principle infinite, though during inference only a finite number are ever instantiated. A discourse module generates a vector Z which assigns an entity index Z i to each mention position i. Finally, a mention generation module independently renders the sequence of mentions (M) from their underlying entities. The syntactic position and structure of mentions are treated as observed, including the mention forms (pronominal, etc.). We use X to refer to this ungenenerated information. Our model decomposes as follows:\nP (E, Z, M|\u03c4 , \u03c0, X) = P (E|\u03c4 ) [Semantic, Section 3.1] P (Z|\u03c0, X) [Discourse, Section 3.2] P (M|Z, E, \u03c4 ) [Mention, Section 3.3]\nWe detail each of these components in subsequent sections. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "R E", "text": "Figure 2: Depiction of the entity generation process (Section 3.1). Each entity draws a type (T ) from \u03c6, and, for each property r \u2208 R, forms a word list (L r ) by choosing a length from T 's f r distribution and then independently drawing that many words from T 's \u03b8 r distribution. Example values are shown for the person type and the nominal head property (NOM-HEAD).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Semantic Module", "text": "The semantic module is responsible for generating a sequence of entities. Each entity E is generated independently and consists of a type indicator T , as well as a collection {L r } r\u2208R of word lists for each property. These elements are generated as follows:\nEntity Generation Draw entity type T \u223c \u03c6 For each mention property r \u2208 R, Fetch {(f r , \u03b8 r )} for \u03c4 T Draw word list length |L r | \u223c f r Draw |L r | words from w \u223c \u03b8 r\nSee Figure 2 for an illustration of this process. Each word list L r is generated by first drawing a list length from f r and then independently populating that list from the property's word distribution \u03b8 r . 1 Past work has employed broadly similar distributional models for unsupervised NER of proper men-tions (Collins and Singer, 1999;Elsner et al., 2009). However, to our knowledge, this is the first work to incorporate such a model into an entity reference process.", "publication_ref": ["b4", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Discourse Module", "text": "The discourse module is responsible for choosing an entity to evoke at each of the n mention positions. Formally, this module generates an entity assignment vector Z = (Z 1 , . . . , Z n ), where Z i indicates the entity index for the ith mention position. Most linguistic inquiry characterizes NP anaphora by the pairwise relations that hold between a mention and its antecedent (Hobbs, 1979;Kehler et al., 2008). Our discourse module utilizes this pairwise perspective to define each Z i in terms of an intermediate \"antecedent\" variable A i . A i either points to a previous antecedent mention position (A i < i) and \"steals\" its entity assignment or begins a new entity (A i = i). The choice of A i is parametrized by affinities s \u03c0 (i, j; X) between mention positions i and j. Formally, this process is described as:\nEntity Assignment\nFor each mention position, i = 1, . . . , n, Draw antecedent position A i \u2208 {1, . . . , i}:\nP (A i = j|X) \u221d s \u03c0 (i, j; X) Z i = Z A i , if A i < i K + 1, otherwise\nHere, K denotes the number of entities allocated in the first i-1 mention positions. This process is an instance of the sequential distance-dependent Chinese Restaurant Process (DD-CRP) of Blei and Frazier (2009). During inference, we variously exploit both the A and Z representations (Section 4). For nominal and pronoun mentions, there are several well-studied anaphora cues, including centering (Grosz et al., 1995), nearness (Hobbs, 1978), and deterministic constraints, which have all been utilized in prior coreference work (Soon et al., 1999;Ng and Cardie, 2002). In order to combine these cues, we take a log-linear, feature-based approach and parametrize s \u03c0 (i, j; X) = exp{\u03c0 f X (i, j)}, where f X (i, j) is a feature vector over mention positions i and j, and \u03c0 is a parameter vector; the features may freely condition on X. We utilize the following features between a mention and an an-tecedent: tree distance, sentence distance, and the syntactic positions (subject, object, and oblique) of the mention and antecedent. Features for starting a new entity include: a definiteness feature (extracted from the mention's determiner), the top CFG rule of the mention parse node, its syntactic role, and a bias feature. These features are conjoined with the mention form (nominal or pronoun). Additionally, we restrict pronoun antecedents to the current and last two sentences, and the current and last three sentences for nominals. Additionally, we disallow nominals from having direct pronoun antecedents.\nIn addition to the above, if a mention is in a deterministic coreference configuration, as defined in Haghighi and Klein (2009), we force it to take the required antecedent. In general, antecedent affinities learn to prefer close antecedents in prominent syntactic positions. We also learn that new entity nominals are typically indefinite or have SBAR complements (captured by the CFG feature).\nIn contrast to nominals and pronouns, the choice of entity for a proper mention is governed more by entity frequency than antecedent distance. We capture this by setting s \u03c0 (i, j; X) in the proper case to 1 for past positions and to a fixed \u03b1 otherwise. 2", "publication_ref": ["b14", "b15", "b2", "b9", "b13", "b22", "b16", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Mention Module", "text": "Once the semantic module has generated entities and the discourse model selects entity assignments, each mention M i generates word values for a set of observed properties R i :\nMention Generation For each mention M i , i = 1, . . . , n Fetch (T, {L r } r\u2208R ) from E Z i Fetch {(f r , \u03b8 r )} r\u2208R from \u03c4 T For r \u2208 R i : w \u223c (1 \u2212 \u03b1 r )UNIFORM(L r ) + (\u03b1 r )\u03b8 r\nFor each property r, there is a hyper-parameter \u03b1 r which interpolates between selecting a word from the entity list L r and drawing from the underlying type property distribution \u03b8 r . Intuitively, a small value of \u03b1 r indicates that an entity prefers to re-use \u03c4 1", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Person Organization", "text": "[software] \u03c4 \u03c4\nNN- NOD ORG [Microsoft] [company, firm] NOM- HEAD NAM- HEAD T \u03c4 2 E 1 E 2 Z 1 Z 2 Z 3 M 1 M 2 M 3 [Steve,\nE, \u03c4 E, \u03c4 E, \u03c4 E, M E, M E 2 E 1 E 1 M M\nFigure 3: Depiction of the discourse module (Section 3.2); each random variable is annotated with an example value. For each mention position, an entity assignment (Z i ) is made. Conditioned on entities (E Zi ), mentions (M i ) are rendered (Section 3.3). The Y symbol denotes that a random variable is the parent of all Y random variables. a small number of words for property r. This is typically the case for proper and nominal heads as well as modifiers. At the other extreme, setting \u03b1 r to 1 indicates the property isn't particular to the entity itself, but rather only on its type. We set \u03b1 r to 1 for pronoun heads as well as for the governor of the head properties.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Learning and Inference", "text": "Our learning procedure involves finding parameters and assignments which are likely under our model's posterior distribution P (E, Z, \u03c4 , \u03c0|M, X). The model is modularized in such a way that running EM on all variables simultaneously would be very difficult. Therefore, we adopt a variational approach which optimizes various subgroups of the variables in a round-robin fashion, holding approximations to the others fixed. We first describe the variable groups, then the updates which optimize them in turn.\nDecomposition: We decompose the entity vari-ables E into types, T, one for each entity, and word lists, L, one for each entity and property. We decompose the mentions M into referring mentions (propers and nominals), M r , and pronominal mentions, M p (with sizes n r and n p respectively). The entity assignments Z are similarly divided into Z r and Z p components. For pronouns, rather than use Z p , we instead work with the corresponding antecedent variables, denoted A p , and marginalize over antecedents to obtain Z p . With these variable groups, we would like to approximation our model posterior P (T, L, Z r , A p , \u03c4 , \u03c0|M, X) using a simple factored representation. Our variational approximation takes the following form:\nQ(T, L, Z r , A p , \u03c4 , \u03c0) = \u03b4 r (Z r , L) n k=1 q k (T k ) np i=1 r i (A p i ) \u03b4 s (\u03c4 )\u03b4 d (\u03c0)\nWe use a mean field approach to update each of the RHS factors in turn to minimize the KL-divergence between the current variational posterior and the true model posterior. The \u03b4 r , \u03b4 s , and \u03b4 d factors place point estimates on a single value, just as in hard EM. Updating these factors involves finding the value which maximizes the model (expected) loglikelihood under the other factors. For instance, the \u03b4 s factor is a point estimate of the type parameters, and is updated with:\n3 \u03b4 s (\u03c4 ) \u2190 argmax \u03c4 E Q \u2212\u03b4s ln P (E, Z, M, \u03c4 , \u03c0) (1)\nwhere Q \u2212\u03b4s denotes all factors of the variational approximation except for the factor being updated. The r i (pronoun antecedents) and q k (type indicator) factors maintain a soft approximation and so are slightly more complex. For example, the r i factor update takes the standard mean field form:\nr i (A p i ) \u221d exp{E Q \u2212r i ln P (E, Z, M, \u03c4 , \u03c0)} (2)\nWe briefly describe the update for each additional factor, omitting details for space.\nUpdating type parameters \u03b4 s (\u03c4 ): The type parameters \u03c4 consist of several multinomial distributions which can be updated by normalizing expected counts as in the EM algorithm. The prior P (\u03c4 |\u03bb) consists of several finite Dirichlet draws for each multinomial, which are incorporated as pseudocounts. 4 Given the entity type variational posteriors {q k (\u2022)}, as well as the point estimates of the L and Z r elements, we obtain expected counts from each entity's attribute word lists and referring mention usages.\nUpdating discourse parameters \u03b4 d (\u03c0): The learned parameters for the discourse module rely on pairwise antecedent counts for assignments to nominal and pronominal mentions. 5 Given these expected counts, which can be easily obtained from other factors, the update reduces to a weighted maximum entropy problem, which we optimize using LBFGS. The prior P (\u03c0|\u03c3 2 ) is a zero-centered normal distribution with shared diagonal variance \u03c3 2 , which is incorporated via L2 regularization during optimization.\nUpdating referring assignments and word lists \u03b4 r (Z r , L): The word lists are usually concatenations of the words used in nominal and proper mentions and so are updated together with the assignments for those mentions. Updating the \u03b4 r (Z r , L) factor involves finding the referring mention entity assignments, Z r , and property word lists L for instantiated entities which maximize E Q \u2212\u03b4r ln P (T, L, Z r , A p , M, \u03c4 , \u03c0). We actually only need to optimize over Z r , since for any Z r , we can compute the optimal set of property word lists L. Essentially, for each entity we can compute the L r which optimizes the probability of the referring mentions assigned to the entity (indicated by Z r ). In practice, the optimal L r is just the set of property words in the assigned mentions. Of course enumerating and scoring all Z r hypotheses is intractable, so we instead utilize a left-to-right sequential beam search. Each partial hypothesis is an assignment to a prefix of mention positions and is scored as though it were a complete hypothesis. Hypotheses are extended via adding a new mention to an existing entity or creating a new one. For our experiments, we limited the number of hypotheses on the beam to the top fifty and did not notice an improvement in model score from increasing beam size.\nUpdating pronominal antecedents r i (A p i ) and entity types q k (T k ): These updates are straightforward instantiations of the mean-field update (2).\nTo produce our final coreference partitions, we assign each referring mention to the entity given by the \u03b4 r factor and each pronoun to the most likely entity given by the r i .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Factor Staging", "text": "In order to facilitate learning, some factors are initially set to fixed heuristic values and only learned in later iterations. Initially, the assignment factors \u03b4 r and {r i } are fixed. For \u03b4 r , we use a deterministic entity assignment Z r , similar to the Haghighi and Klein ( 2009)'s SYN-CONSTR setting: each referring mention is coreferent with any past mention with the same head or in a deterministic syntactic configuration (appositives or predicative nominatives constructions). 6 The {r i } factors are heuristically set to place most of their mass on the closest antecedent by tree distance. During training, we proceed in stages, each consisting of 5 iterations:\nStage Learned Fixed B 3 All 1 \u03b4 s , \u03b4 d , {q k } {r i },\u03b4 r 74.6 2 \u03b4 s , \u03b4 d , {q k }, \u03b4 r {r i } 76.3 3 \u03b4 s , \u03b4 d , {q k }, \u03b4 r , {r i } - 78.0\nWe evaluate our system at the end of stage using the B 3 All metric on the A05CU development set (see Section 5 for details).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We considered the challenging end-to-end system mention setting, where in addition to predicting mention partitions, a system must identify the mentions themselves and their boundaries automatically. Our system deterministically extracts mention boundaries from parse trees (Section 5.2). We utilized no coreference annotation during training, but did use minimal prototype information to prime the learning of entity types (Section 5.3).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Datasets", "text": "For evaluation, we used standard coreference data sets derived from the ACE corpora:\n\u2022 A04CU: Train/dev/test split of the newswire portion of the ACE 2004 training set 7 utilized in Culotta et al. (2007), Bengston and Roth (2008) and Stoyanov et al. (2009). Consists of 90/68/38 documents respectively. \u2022 A05ST: Train/test split of the newswire portion of the ACE 2005 training set utilized in Stoyanov et al. (2009). Consists of 57/24 documents respectively. \u2022 A05RA: Train/test split of the ACE 2005 training set utilized in Rahman and Ng (2009). Consists of 482/117 documents respectively.\nFor all experiments, we evaluated on the dev and test sets above. To train, we included the text of all documents above, though of course not looking at either their mention boundaries or reference annotations in any way. We also trained on the following much larger unlabeled datasets utilized in Haghighi and Klein ( 2009): \u2022 BLLIP: 5k articles of newswire parsed with the Charniak (2000) parser. \u2022 WIKI: 8k abstracts of English Wikipedia articles parsed by the Berkeley parser (Petrov et al., 2006). Articles were selected to have subjects amongst the frequent proper nouns in the evaluation datasets.", "publication_ref": ["b6", "b1", "b23", "b23", "b21", "b3", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Mention Detection and Properties", "text": "Mention boundaries were automatically detected as follows: For each noun or pronoun (determined by parser POS tag), we associated a mention with the maximal NP projection of that head or that word itself if no NP can be found. This procedure recovers over 90% of annotated mentions on the A05CU dev set, but also extracts many unannotated \"spurious\" mentions (for instance events, times, dates, or abstract nouns) which are not deemed to be of interest by the ACE annotation conventions. Mention properties were obtained from parse trees using the the Stanford typed dependency extractor (de Marneffe et al., 2006). The mention properties we considered are the mention head (annotated with mention type), the typed modifiers of the head, and the governor of the head (conjoined with   Haghighi and Klein (2009) and current work are fully supervised. The current work outperforms all other systems, supervised or unsupervised. For comparison purposes, the B 3 N one variant used on A05RA is calculated slightly differently than other B 3 N one results; see Rahman and Ng (2009). the mention's syntactic position). We discard determiners, but make use of them in the discourse component (Section 3.2) for NP definiteness.", "publication_ref": ["b7", "b12", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Prototyping Entity Types", "text": "While it is possible to learn type distributions in a completely unsupervised fashion, we found it useful to prime the system with a handful of important types. Rather than relying on fully supervised data, we took the approach of Haghighi and Klein (2006).\nFor each type of interest, we provided a (possiblyempty) prototype list of proper and nominal head words, as well as a list of allowed pronouns. For instance, for the PERSON type we might provide:\nNAM Bush, Gore, Hussein NOM president, minister, official PRO he, his, she, him, her, you, ... The prototypes were used as follows: Any entity with a prototype on any proper or nominal head word attribute list (Section 3.1) was constrained to have the specified type; i.e. the q k factor (Section 4) places probability one on that single type. Similarly to Haghighi and Klein (2007) and Elsner et al. (2009), we biased these types' pronoun distributions to the allowed set of pronouns.\nIn general, the choice of entity types to prime with prototypes is a domain-specific question. For experiments here, we utilized the types which are annotated in the ACE coreference data: person (PERS), organization (ORG), geo-political entity (GPE), weapon (WEA), vehicle (VEH), location (LOC), and facility (FAC). Since the person type in ACE conflates individual persons with groups of people (e.g., soldier vs. soldiers), we added the group (GROUP) type and generated a prototype specification.\nWe obtained our prototype list by extracting at most four common proper and nominal head words from the newswire portions of the 2004 and 2005 ACE training sets (A04CU and A05ST); we chose prototype words to be minimally ambiguous with respect to type. 8 When there are not at least three proper heads for a type (WEA for instance), we did not provide any proper prototypes and instead strongly biased the type fertility parameters to generate empty NAM-HEAD lists.\nBecause only certain semantic types were annotated under the arbitrary ACE guidelines, there are many mentions which do not fall into those limited categories. We therefore prototype (refinements of) the ACE types and then add an equal number of unconstrained \"other\" types which are automatically induced. A nice consequence of this approach is that we can simply run our model on all mentions, discarding at evaluation time any which are of nonprototyped types.", "publication_ref": ["b10", "b11", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation", "text": "We evaluated on multiple coreference resolution metrics, as no single one is clearly superior, partic-ularly in dealing with the system mention setting. We utilized MUC (Vilain et al., 1995), B 3 All (Stoyanov et al., 2009), B 3 N one (Stoyanov et al., 2009), and Pairwise F1. The B 3 All and B 3 N one are B 3 variants (Bagga and Baldwin, 1998) that differ in their treatment of spurious mentions. For Pairwise F1, precision measures how often pairs of predicted coreferent mentions are in the same annotated entity. We eliminated any mention pair from this calculation where both mentions were spurious. 9", "publication_ref": ["b23", "b23", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Table 1 shows our results. We compared to two state-of-the-art supervised coreference systems. The Stoyanov et al. (2009) numbers represent their THRESHOLD ESTIMATION setting and the Rahman and Ng ( 2009) numbers represent their highestperforming cluster ranking model. We also compared to the strong deterministic system of Haghighi and Klein (2009). 10 Across all data sets, our model, despite being largely unsupervised, consistently outperforms these systems, which are the best previously reported results on end-to-end coreference resolution (i.e. including mention detection). Performance on the A05RA dataset is generally lower because it includes articles from blogs and web forums where parser quality is significantly degraded.\nWhile Bengston and Roth (2008) do not report on the full system mention task, they do report on the more optimistic setting where mention detection is performed but non-gold mentions are removed for evaluation using an oracle. On this more lenient setting, they report 78.4 B 3 on the A04CU test set. Our model yields 80.3.", "publication_ref": ["b23", "b12", "b1"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Analysis", "text": "We now discuss errors and improvements made by our system. One frequent source of error is the merging of mentions with explicitly contrasting modifiers, such as new president and old president. While it is not unusual for a single entity to admit multiple modifiers, the particular modifiers new and old are incompatible in a way that new and popular are not. Our model does not represent the negative covariance between these modifiers.\nWe compared our output to the deterministic system of Haghighi and Klein (2009). Many improvements arise from correctly identifying mentions which are semantically compatible but which do not explicitly appear in an appositive or predicatenominative configuration in the data. For example, analyst and it cannot corefer in our system because it is not a likely pronoun for the type PERSON.\nWhile the focus of our model is coreference resolution, we can also isolate and evaluate the type component of our model as an NER system. We test this component by presenting our learned model with boundary-annotated non-pronominal entities from the A05ST dev set and querying their predicted type variable T . Doing so yields 83.2 entity classification accuracy under the mapping between our prototyped types and the coarse ACE types. Note that this task is substantially more difficult than the unsupervised NER in Elsner et al. (2009) because the inventory of named entities is larger (7 vs. 3) and because we predict types over nominal mentions that are more difficult to judge from surface forms. In this task, the plurality of errors are confusions between the GPE (geo-political entity) and ORG entity types, which have very similar distributions.", "publication_ref": ["b12", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "Our model is able to acquire and exploit knowledge at either the level of individual entities (\"Obama\" is a \"president\") and entity types (\"company\" can refer to a corporation). As a result, it leverages semantic constraints more effectively than systems operating at either level alone. In conjunction with reasonable, but simple, factors capturing discourse and syntactic configurational preferences, our entity-centric semantic model lowers coreference error rate substantially, particularly on semantically disambiguated references, giving a sizable improvement over the state-of-the-art. 11 ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgements: This project is funded in part by the Office of Naval Research under MURI Grant No. N000140911081.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Algorithms for scoring coreference chains", "journal": "", "year": "1998", "authors": "A Bagga;  Baldwin"}, {"ref_id": "b1", "title": "Understanding the Value of Features for Corefernce Resolution", "journal": "", "year": "2008", "authors": "Eric Bengston; Dan Roth"}, {"ref_id": "b2", "title": "Distance Dependent Chinese Restaurant Processes", "journal": "", "year": "2009", "authors": "David Blei; Peter I Frazier"}, {"ref_id": "b3", "title": "Maximum Entropy Inspired Parser", "journal": "", "year": "2000", "authors": "Eugene Charniak"}, {"ref_id": "b4", "title": "Unsupervised Models for Named Entity Classification", "journal": "", "year": "1999", "authors": "Michael Collins; Yoram Singer"}, {"ref_id": "b5", "title": "Head-Driven Statistical Models for Natural Language Parsing", "journal": "", "year": "1999", "authors": "Mike Collins"}, {"ref_id": "b6", "title": "First-order Probabilistic Models for Coreference Resolution", "journal": "", "year": "2007", "authors": "A Culotta;  Wick; A Hall;  Mccallum"}, {"ref_id": "b7", "title": "Generating Typed Dependency Parses from Phrase Structure Parses", "journal": "", "year": "2006", "authors": "M C De Marneffe; B Maccartney; C D Manning"}, {"ref_id": "b8", "title": "Structured generative models for unsupervised named-entity clustering", "journal": "", "year": "2009", "authors": "M Elsner; M Charniak;  Johnson"}, {"ref_id": "b9", "title": "Centering: A Framework for Modeling the Local Coherence of Discourse", "journal": "Computational Linguistics", "year": "1995", "authors": "Barbara J Grosz; Aravind K Joshi; Scott Weinstein"}, {"ref_id": "b10", "title": "Prototype-Driven Learning for Sequence Models", "journal": "", "year": "2006", "authors": "Aria Haghighi; Dan Klein"}, {"ref_id": "b11", "title": "Unsupervised Coreference Resolution in a Nonparametric Bayesian Model", "journal": "", "year": "2007", "authors": "Aria Haghighi; Dan Klein"}, {"ref_id": "b12", "title": "Simple Coreference Resolution with Rich Syntactic and Semantic Features", "journal": "", "year": "2009", "authors": "Aria Haghighi; Dan Klein"}, {"ref_id": "b13", "title": "Resolving Pronoun References. Lingua", "journal": "", "year": "1978", "authors": "J R Hobbs"}, {"ref_id": "b14", "title": "", "journal": "Coherence and Coreference. Cognitive Science", "year": "1979", "authors": "J R Hobbs"}, {"ref_id": "b15", "title": "", "journal": "Coherence and Coreference Revisited", "year": "2008", "authors": "Andrew Kehler; Laura Kertz; Hannah Rohde; Jeffrey Elman"}, {"ref_id": "b16", "title": "Improving Machine Learning Approaches to Coreference Resolution", "journal": "", "year": "2002", "authors": "Vincent Ng; Claire Cardie"}, {"ref_id": "b17", "title": "Machine Learning for Coreference Resolution: From Local Classification to Global Ranking", "journal": "", "year": "2005", "authors": "Vincent Ng"}, {"ref_id": "b18", "title": "Shallow semantics for coreference resolution", "journal": "", "year": "2007", "authors": "Vincent Ng"}, {"ref_id": "b19", "title": "Learning Accurate, Compact, and Interpretable Tree Annotation", "journal": "", "year": "2006-07", "authors": "Slav Petrov; Leon Barrett; Romain Thibaux; Dan Klein"}, {"ref_id": "b20", "title": "Combinatorial Stochastic Processes", "journal": "", "year": "2002", "authors": "J Pitman"}, {"ref_id": "b21", "title": "Supervised models for coreference resolution", "journal": "", "year": "2009", "authors": "A Rahman;  Ng"}, {"ref_id": "b22", "title": "A Machine Learning Approach to Coreference Resolution of Noun Phrases", "journal": "", "year": "1999", "authors": "W H Soon; H T Ng; D C Y Lim"}, {"ref_id": "b23", "title": "Conundrums in Noun Phrase Coreference Resolution: Making Sense of the State-of-the-art", "journal": "", "year": "2009", "authors": "V Stoyanov; C Gilbert; E Cardie;  Riloff"}, {"ref_id": "b24", "title": "Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme", "journal": "", "year": "", "authors": "Marc Vilain; John Burger; John Aberdeen"}, {"ref_id": "b25", "title": "Improving pronoun resolution using statistics-based semantic compatibility information", "journal": "", "year": "2005", "authors": "X Yang; C L Su;  Tan"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: The key abstractions of our model (Section 2). (a) Mentions map properties (r) to words (w r ). (b) Entities map properties (r) to word lists (L r ). (c) Types map properties (r) to distributions over property words (\u03b8 r ) and the fertilities of those distributions (f r ). For (b) and (c), we only illustrate a subset of the properties.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Experimental results with system mentions. All systems except", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "P (E, Z, M|\u03c4 , \u03c0, X) = P (E|\u03c4 ) [Semantic, Section 3.1] P (Z|\u03c0, X) [Discourse, Section 3.2] P (M|Z, E, \u03c4 ) [Mention, Section 3.3]", "formula_coordinates": [2.0, 339.24, 614.24, 174.73, 60.52]}, {"formula_id": "formula_1", "formula_text": "Entity Generation Draw entity type T \u223c \u03c6 For each mention property r \u2208 R, Fetch {(f r , \u03b8 r )} for \u03c4 T Draw word list length |L r | \u223c f r Draw |L r | words from w \u223c \u03b8 r", "formula_coordinates": [3.0, 83.75, 493.58, 161.46, 98.86]}, {"formula_id": "formula_2", "formula_text": "P (A i = j|X) \u221d s \u03c0 (i, j; X) Z i = Z A i , if A i < i K + 1, otherwise", "formula_coordinates": [3.0, 324.95, 411.34, 142.95, 48.07]}, {"formula_id": "formula_3", "formula_text": "Mention Generation For each mention M i , i = 1, . . . , n Fetch (T, {L r } r\u2208R ) from E Z i Fetch {(f r , \u03b8 r )} r\u2208R from \u03c4 T For r \u2208 R i : w \u223c (1 \u2212 \u03b1 r )UNIFORM(L r ) + (\u03b1 r )\u03b8 r", "formula_coordinates": [4.0, 78.76, 492.01, 192.65, 100.01]}, {"formula_id": "formula_4", "formula_text": "NN- NOD ORG [Microsoft] [company, firm] NOM- HEAD NAM- HEAD T \u03c4 2 E 1 E 2 Z 1 Z 2 Z 3 M 1 M 2 M 3 [Steve,", "formula_coordinates": [4.0, 341.07, 75.95, 167.66, 177.8]}, {"formula_id": "formula_5", "formula_text": "E, \u03c4 E, \u03c4 E, \u03c4 E, M E, M E 2 E 1 E 1 M M", "formula_coordinates": [4.0, 339.61, 100.91, 175.11, 128.91]}, {"formula_id": "formula_6", "formula_text": "Q(T, L, Z r , A p , \u03c4 , \u03c0) = \u03b4 r (Z r , L) n k=1 q k (T k ) np i=1 r i (A p i ) \u03b4 s (\u03c4 )\u03b4 d (\u03c0)", "formula_coordinates": [5.0, 89.26, 267.4, 192.28, 52.24]}, {"formula_id": "formula_7", "formula_text": "3 \u03b4 s (\u03c4 ) \u2190 argmax \u03c4 E Q \u2212\u03b4s ln P (E, Z, M, \u03c4 , \u03c0) (1)", "formula_coordinates": [5.0, 79.38, 448.58, 219.42, 36.23]}, {"formula_id": "formula_8", "formula_text": "r i (A p i ) \u221d exp{E Q \u2212r i ln P (E, Z, M, \u03c4 , \u03c0)} (2)", "formula_coordinates": [5.0, 83.49, 572.31, 215.31, 21.22]}, {"formula_id": "formula_9", "formula_text": "Stage Learned Fixed B 3 All 1 \u03b4 s , \u03b4 d , {q k } {r i },\u03b4 r 74.6 2 \u03b4 s , \u03b4 d , {q k }, \u03b4 r {r i } 76.3 3 \u03b4 s , \u03b4 d , {q k }, \u03b4 r , {r i } - 78.0", "formula_coordinates": [6.0, 81.4, 369.71, 207.79, 60.92]}], "doi": ""}