{"title": "Controlling Fairness and Bias in Dynamic Learning-to-Rank", "authors": "Marco Morik; Ashudeep Singh; Thorsten Joachims", "pub_date": "2020-05-29", "abstract": "Rankings are the primary interface through which many online platforms match users to items (e.g. news, products, music, video). In these two-sided markets, not only the users draw utility from the rankings, but the rankings also determine the utility (e.g. exposure, revenue) for the item providers (e.g. publishers, sellers, artists, studios). It has already been noted that myopically optimizing utility to the users -as done by virtually all learning-to-rank algorithms -can be unfair to the item providers. We, therefore, present a learning-to-rank approach for explicitly enforcing meritbased fairness guarantees to groups of items (e.g. articles by the same publisher, tracks by the same artist). In particular, we propose a learning algorithm that ensures notions of amortized group fairness, while simultaneously learning the ranking function from implicit feedback data. The algorithm takes the form of a controller that integrates unbiased estimators for both fairness and utility, dynamically adapting both as more data becomes available. In addition to its rigorous theoretical foundation and convergence guarantees, we find empirically that the algorithm is highly practical and robust.\u2022 Information systems \u2192 Learning to rank.", "sections": [{"heading": "INTRODUCTION", "text": "We consider the problem of dynamic Learning-to-Rank (LTR), where the ranking function dynamically adapts based on the feedback that users provide. Such dynamic LTR problems are ubiquitous in online systems -news-feed rankings that adapt to the number of \"likes\" an article receives, online stores that adapt to the number of positive reviews for a product, or movie-recommendation systems that adapt to who has watched a movie. In all of these systems, learning and prediction are dynamically intertwined, where past feedback influences future rankings in a specific form of online learning with partial-information feedback [18].\nWhile dynamic LTR systems are in widespread use and unquestionably useful, there are at least two issues that require careful design considerations. First, the ranking system induces a bias through the rankings it presents. In particular, items ranked highly are more likely to collect additional feedback, which in turn can influence future rankings and promote misleading rich-get-richer dynamics [3,32,33,40]. Second, the ranking system is the arbiter of how much exposure each item receives, where exposure directly influences opinion (e.g. ideological orientation of presented news articles) or economic gain (e.g. revenue from product sales or streaming) for the provider of the item. This raises fairness considerations about how exposure should be allocated based on the merit of the items [14,42]. We will show in the following that naive dynamic LTR methods that are oblivious to these issues can lead to economic disparity, unfairness, and polarization.\nIn this paper, we present the first dynamic LTR algorithm -called FairCo -that overcomes rich-get-richer dynamics while enforcing a configurable allocation-of-exposure scheme. Unlike existing fair LTR algorithms [14,42,43,48], FairCo explicitly addresses the dynamic nature of the learning problem, where the system is unbiased and fair even though the relevance and the merit of items are still being learned. At the core of our approach lies a merit-based exposure-allocation criterion that is amortized over the learning process [14,42]. We view the enforcement of this merit-based exposure criterion as a control problem and derive a P-controller that optimizes both the fairness of exposure as well as the quality of the rankings. A crucial component of the controller is the ability to estimate merit (i.e. relevance) accurately, even though the feedback is only revealed incrementally as the system operates, and the feedback is biased by the rankings shown in the process [32]. To this effect, FairCo includes a new unbiased cardinal relevance estimator -as opposed to existing ordinal methods [4,33] -, which can be used both as an unbiased merit estimator for fairness and as a ranking criterion.\nIn addition to the theoretical justification of FairCo, we provide empirical results on both synthetic news-feed data and real-world movie recommendation data. We find that FairCo is effective at enforcing fairness while providing good ranking performance. Furthermore, FairCo is efficient, robust, and easy to implement.", "publication_ref": ["b17", "b2", "b31", "b32", "b39", "b13", "b41", "b13", "b41", "b42", "b47", "b13", "b41", "b31", "b3", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "MOTIVATION", "text": "Consider the following illustrative example of a dynamic LTR problem. An online news-aggregation platform wants to present a ranking of the top news articles on its front page. Through some external mechanism, it identifies a set D = {d 1 , ..., d 20 } of 20 articles at the beginning of each day, but it is left with the learning problem of how to rank these 20 articles on its front page. As users start coming to the platform, the platform uses the following naive algorithm to learn the ranking. Executing this algorithm at the beginning of a day, the platform starts by presenting the 20 articles in random order for the first user. It may then observe that the user reads the article in position 3 and increments the counter C(d) for this article. For the next user, this article now gets ranked first and the counters are updated based on what the second user reads. This cycle continues for each subsequent user. Unfortunately, this naive algorithm has at least two deficiencies that make it suboptimal or unsuitable for many ranking applications.\nThe first deficiency lies in the choice of C(d) as an estimate of average relevance for each article -namely the fraction of users that want to read the article. Unfortunately, even with infinite amounts of user feedback, the counters C(d) are not consistent estimators of average relevance [32,33,40]. In particular, items that happened to get more reads in early iterations get ranked highly, where more users find them and thus have the opportunity to provide more positive feedback for them. This perpetuates a rich-get-richer dynamic, where the feedback count C(d) recorded for each article does not reflect how many users actually wanted to read the article.\nThe second deficiency of the naive algorithm lies in the ranking policy itself, creating a source of unfairness even if the true average relevance of each article was accurately known [7,14,42]. Consider the following omniscient variant of the naive algorithm that ranks the articles by their true average relevance (i.e. the true fraction of users who want to read each article). How can this ranking be unfair? Let us assume that we have two groups of articles, G right and G left , with 10 items each (i.e. articles from politically rightand left-leaning sources). 51% of the users (right-leaning) want to read the articles in group G right , but not the articles in group G left .\nIn reverse, the remaining 49% of the users (left-leaning) like only the articles in G left . Ranking articles solely by their true average relevance puts items from G right into positions 1-10 and the items from G left in positions 11-20. This means the platform gives the articles in G left vastly less exposure than those in G right . We argue that this can be considered unfair since the two groups receive disproportionately different outcomes despite having similar merit (i.e. relevance). Here, a 2% difference in average relevance leads to a much larger difference in exposure between the groups.\nWe argue that these two deficiencies -namely bias and unfairness -are not just undesirable in themselves, but that they have undesirable consequences. For example, biased estimates lead to poor ranking quality, and unfairness is likely to alienate the leftleaning users in our example, driving them off the platform and encouraging polarization.\nFurthermore, note that these two deficiencies are not specific to the news example, but that the naive algorithm leads to analogous problems in many other domains. For example, consider a ranking system for job applicants, where rich-get-richer dynamics and exposure allocation may perpetuate and even amplify existing unfairness (e.g. disparity between male and female applicants). Similarly, consider an online marketplace where products of different sellers (i.e. groups) are ranked. Here rich-get-richer dynamics and unfair exposure allocation can encourage monopolies and drive some sellers out of the market.\nThese examples illustrate the following two desiderata that a less naive dynamic LTR algorithm should fulfill. Unbiasedness: The algorithm should not be biased or subject to rich-get-richer dynamics. Fairness: The algorithm should enforce a fair allocation of exposure based on merit (e.g. relevance).\nWith these two desiderata in mind, this paper develops alternatives to the Naive algorithm. In particular, after introducing the dynamic learning-to-rank setting in Section 4, Section 5 formalizes an amortized notion of merit-based fairness, accounting for the fact that merit itself is unknown at the beginning of the learning process and is only learned throughout. Section 6 then addresses the bias problem, providing estimators that eliminate the presentation bias for both global and personalized ranking policies. Finally, Section 7 proposes a control-based algorithm that is designed to optimize ranking quality while dynamically enforcing fairness.", "publication_ref": ["b31", "b32", "b39", "b6", "b13", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "RELATED WORK", "text": "Ranking algorithms are widely recognized for their potential for societal impact [8], as they form the core of many online systems, including search engines, recommendation systems, news feeds, and online voting. Controlling rich-get-richer phenomena in recommendations and rankings has been studied from the perspective of both optimizing utility through exploration as well as ensuring fairness of such systems [2,41,49]. There are several adverse consequences of naive ranking systems [20], such as political polarization [11], misinformation [46], unfair allocation of exposure [43], and biased judgment [8] through phenomena such as the Matthew effect [3,24]. Viewing such ranking problems as two-sided markets of users and items that each derive utility from the ranking system brings a novel perspective to tackling such problems [1,42]. In this work, we take inspiration from these works to develop methods for mitigating bias and unfairness in a dynamic setting.\nMachine learning methods underlie most ranking algorithms. There has been a growing concern around the question of how machine learning algorithms can be unfair, especially given their numerous real-world applications [10]. There have been several definitions proposed for fairness in the binary classification setting [9], as well as recently in the domains of rankings in recommendations and information retrieval [13,14,17,42]. The definitions of fairness in ranking span from ones purely based on the composition of the top-k [17], to relevance-based definitions such as fairness of exposure [42], and amortized attention equity [14]. We will discuss these definitions in greater detail in Section 5. Our work also relates to the recent interest in studying the impact of fairness when learning algorithms are applied in dynamic settings [22,36,44].\nIn information retrieval, there has been a long-standing interest in learning to rank from biased click data. As already argued above, the bias in logged click data occurs because the feedback is incomplete and biased by the presentation. Numerous approaches based on preferences (e.g. [26,31]), click models (e.g. [19]), and randomized interventions (e.g. [37]) exist. Most recently, a new approach for de-biasing feedback data using techniques from causal inference and missing data analysis was proposed to provably eliminate selection biases [6,33]. We follow this approach in this paper, extend it to the dynamic ranking setting, and propose a new unbiased regression objective in Section 6.\nLearning in our dynamic ranking setting is related to the conventional learning-to-rank algorithms such as LambdaRank, Lamb-daMART, RankNet, Softrank etc. [16,45]. However, to implement fairness constraints based on merit, we need to explicitly estimate relevance to the user as a measure of merit while the scores estimated by these methods don't necessarily have a meaning. Our setting is also closely related to online learning to rank for top-k ranking where feedback is observed only on the top-k items, and hence exploration interventions are necessary to ensure convergence [27,35,38,50]. These algorithms are designed with respect to a click-model assumption [50] or learning in the presence of document features [35]. A key difference in our method is that we do not consider exploration through explicit interventions, but merely exploit user-driven exploration. However, explicit exploration could also be incorporated into our algorithms to improve the convergence rate of our methods.", "publication_ref": ["b7", "b1", "b40", "b48", "b19", "b10", "b45", "b42", "b7", "b2", "b23", "b0", "b41", "b9", "b8", "b12", "b13", "b16", "b41", "b16", "b41", "b13", "b21", "b35", "b43", "b25", "b30", "b18", "b36", "b5", "b32", "b15", "b44", "b26", "b34", "b37", "b49", "b49", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "DYNAMIC LEARNING-TO-RANK", "text": "We begin by formally defining the dynamic LTR problem. Given is a set of items D that needs to be ranked in response to incoming requests. At each time step t, a request x t , r t \u223c P(x, r)\narrives i.i.d. at the ranking system. Each request consists of a feature vector describing the user's information need x t (e.g. query, user profile), and the user's vector of true relevance ratings r t for all items in the collection D. Only the feature vector x t is visible to the system, while the true relevance ratings r t are hidden. Based on the information in x t , a ranking policy \u03c0 t (x) produces a ranking \u03c3 t that is presented to the user. Note that the policy may ignore the information in x t , if we want to learn a single global ranking like in the introductory news example. After presenting the ranking \u03c3 t , the system receives a feedback vector c t from the user with a non-negative value c t (d) for every d \u2208 D. In the simplest case, it is 1 for click and 0 for no click, and we will use the word \"click\" as a placeholder throughout this paper for simplicity. But the feedback may take many other forms and does not have to be binary. For example, in a video streaming service, the feedback may be the percentage the user watched of each video.\nAfter the feedback c t was received, the dynamic LTR algorithm A now updates the ranking policy and produces the policy \u03c0 t +1 that is used in the next time step.\n\u03c0 t +1 \u2190\u2212 A((x 1 , \u03c3 1 , c 1 ), ..., (x t , \u03c3 t , c t ))\nAn instance of such a dynamic LTR algorithm is the Naive algorithm already outlined in Section 2. It merely computes c t to produce a new ranking policy for t + 1 (here a global ranking independent of x).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Partial and Biased Feedback", "text": "A key challenge of dynamic LTR lies in the fact that the feedback c t provides meaningful feedback only for the items that the user examined. Following a large body of work on click models [19], we model this as a censoring process. Specifically, for a binary vector e t indicating which items were examined by the user, we model the relationship between c t and r t as follows.\nc t (d) = r t (d) if e t (d) = 1 0 otherwise (2)\nComing back to the running example of news ranking, r t contains the full information about which articles the user is interested in reading, while c t reveals this information only for the articles d examined by the user (i.e. e t (d) = 1). Analogously, in the job placement application r t indicates for all candidates d whether they are qualified to receive an interview call, but c t reveals this information only for those candidates examined by the employer.\nA second challenge lies in the fact that the examination vector e t cannot be observed. This implies that a feedback value of c t (d) = 0 is ambiguous -it may either indicate lack of examination (i.e. e t (d) = 0) or negative feedback (i.e. r t (d) = 0). This would not be problematic if e t was uniformly random, but which items get examined is strongly biased by the ranking \u03c3 t presented to the user in the current iteration. Specifically, users are more likely to look at an item high in the ranking than at one that is lower down [32]. We model this position bias as a probability distribution on the examination vector e t \u223c P(e |\u03c3 t , x t , r t ).\n(\n)3\nMost click models can be brought into this form [19]. For the simplicity of this paper, we merely use the Position-Based Model (PBM) [21]. It assumes that the marginal probability of examination p t (d) for each item d depends only on the rank rank(d |\u03c3 ) of d in the presented ranking \u03c3 . Despite its simplicity, it was found that the PBM can capture the main effect of position bias accurately enough to be reliable in practice [5,33,47].", "publication_ref": ["b18", "b31", "b18", "b20", "b4", "b32", "b46"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluating Ranking Performance", "text": "We measure the quality of a ranking policy \u03c0 by its utility to the users. Virtually all ranking metrics used in information retrieval define the utility U (\u03c3 | r) of a ranking \u03c3 as a function of the relevances of the individual items r. In our case, these item-based relevances r represent which articles the user likes to read, or which candidates are qualified for an interview. A commonly used utility measure is the DCG [30]\nU DCG (\u03c3 | r) = d \u2208\u03c3 r(d) log 2 (1 + rank(d |\u03c3 ))\n, or the NDCG when normalized by the DCG of the optimal ranking. Over a distribution of requests P(x, r), a ranking policy \u03c0 (x) is evaluated by its expected utility\nU (\u03c0 ) = \u222b U (\u03c0 (x)| r) d P(x, r).(4)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Optimizing Ranking Performance", "text": "The user-facing goal of dynamic LTR is to converge to the policy \u03c0 * = argmax \u03c0 U (\u03c0 ) that maximizes utility. Even if we solve the problem of estimating U (\u03c0 ) despite our lack of knowledge of e, this maximization problem could be computationally challenging, since the space of ranking policies is exponential even when learning just a single global ranking. Fortunately, it is easy to show [39] that sorting-based policies \u03c0 (x) \u2261 argsort\nd \u2208 D R(d |x) ,(5)\nwhere \nR(d |x) = \u222b r(d) d P(r |x),(6)\n\u03c3 = argsort d \u2208 D R(d)(7)\nIn Section 6, we will use techniques from causal inference and missing-data analysis to design unbiased and consistent estimators for R(d |x) and R(d) that only require access to the observed feedback c t .", "publication_ref": ["b38"], "figure_ref": [], "table_ref": []}, {"heading": "FAIRNESS IN DYNAMIC LTR", "text": "While sorting by R(d |x) (or R(d) for global rankings) may provide optimal utility to the user, the introductory example has already illustrated that this ranking can be unfair. There is a growing body of literature to address this unfairness in ranking, and we now extend merit-based fairness [14,42] to the dynamic LTR setting.\nThe key scarce resource that a ranking policy allocates among the items is exposure. Based on the model introduced in the previous section, we define the exposure of an item d as the marginal probability of examination p t (d) = P(e t (d) = 1|\u03c3 t , x t , r t ). It is the probability that the user will see d and thus have the opportunity to read that article, buy that product, or interview that candidate. We discuss in Section 6 how to estimate p t (d). Taking a group-based approach to fairness, we aggregate exposure by groups\nG = {G 1 , . . . , G m }. Exp t (G i ) = 1 |G i | d \u2208G i p t (d).(8)\nThese groups can be legally protected groups (e.g. gender, race), reflect some other structure (e.g. items sold by a particular seller), or simply put each item in its own group (i.e. individual fairness).\nIn order to formulate fairness criteria that relate exposure to merit, we define the merit of an item as its expected average relevance R(d) and again aggregate over groups.\nMerit(G i ) = 1 |G i | d \u2208G i R(d)(9)\nIn Section 6, we will discuss how to get unbiased estimates of Merit(G i ) using the biased feedback data c t .\nWith these definitions in hand, we can address the types of disparities identified in Section 2. Specifically, we extend the Disparity of Treatment criterion of [42] to the dynamic ranking problem, using an amortized notion of fairness as in [14]. In particular, for any two groups G i and G j the disparity\nD E \u03c4 (G i , G j ) = 1 \u03c4 \u03c4 t =1 Exp t (G i ) Merit(G i ) \u2212 1 \u03c4 \u03c4 t =1 Exp t (G j ) Merit(G j )(10)\nmeasures in how far amortized exposure over \u03c4 time steps was fulfilled. This exposure-based fairness disparity expresses in how far, averaged over all time steps, each group of items got exposure proportional to its relevance. The further the disparity is from zero, the greater is the violation of fairness. Note that other allocation strategies beyond proportionality could be implemented as well by using alternate definitions of disparity [42]. Exposure can also be allocated based on other fairness criteria, for example, a Disparity of Impact that a specific exposure allocation implies [42]. If we consider the feedback c t (e.g. clicks, purchases, votes) as a measure of impact\nImp t (G i ) = 1 |G i | d \u2208G i c t (d),(11)\nthen keeping the following disparity close to zero controls how exposure is allocated to make impact proportional to relevance.\nD I \u03c4 (G i , G j ) = 1 \u03c4 \u03c4 t =1 Imp t (G i ) Merit(G i ) \u2212 1 \u03c4 \u03c4 t =1 Imp t (G j ) Merit(G j )(12)\nWe refer to this as the impact-based fairness disparity. In Section 7 we will derive a controller that drives such exposure and impact disparities to zero.", "publication_ref": ["b13", "b41", "b41", "b13", "b41", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "UNBIASED ESTIMATORS", "text": "To be able to implement the ranking policies in Equation ( 5) and the fairness disparities in Equations ( 10) and ( 12), we need accurate estimates of the position bias p t , the expected conditional relevances R(d |x), and the expected average relevances R(d). We consider these estimation problems in the following.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Estimating the Position Bias", "text": "Learning a model for p t is not part of our dynamic LTR problem, as the position-bias model is merely an input to our dynamic LTR algorithms. Fortunately, several techniques for estimating positionbias models already exist in the literature [5,23,33,47], and we are agnostic to which of these is used. In the simplest case, the examination probabilities p t (d) only depend on the rank of the item in \u03c3 , analogous to a Position-Based Click Model [21] with a fixed probability for each rank. It was shown in [5,33,47] how these position-based probabilities can be estimated from explicit and implicit swap interventions. Furthermore, it was shown in [23] how the contextual features x about the users and query can be incorporated in a neural-network based propensity model, allowing it to capture that certain users may explore further down the ranking for some queries. Once any of these propensity models are learned, they can be applied to predict p t for any new query x t and ranking \u03c3 t .", "publication_ref": ["b4", "b22", "b32", "b46", "b20", "b4", "b32", "b46", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Estimating Conditional Relevances", "text": "The key challenge in estimating R(d |x) from Equation ( 6) lies in our inability to directly observe the true relevances r t . Instead, the only data we have is the partial and biased feedback c t . To overcome this problem, we take an approach inspired by [33] and extend it to the dynamic ranking setting. The key idea is to correct for the selection bias with which relevance labels are observed in c t using techniques from survey sampling and causal inference [28,29]. However, unlike the ordinal estimators proposed in [33], we need cardinal relevance estimates since our fairness disparities are cardinal in nature. We, therefore, propose the following cardinal relevance estimator.\nThe key idea behind this estimator lies in a training objective that only uses c t , but that in expectation is equivalent to a least-squares objective that has access to r t . To start the derivation, let's consider how we would estimate R(d |x), if we had access to the relevance labels (r 1 , ..., r \u03c4 ) of the previous \u03c4 time steps. A straightforward solution would be to solve the following least-squares objective for a given regression modelR w (d |x t ) (e.g. a neural network), where w are the parameters of the model.\nL r (w) = \u03c4 t =1 d r t (d) \u2212R w (d |x t ) 2 (13)\nThe minimum w * of this objective is the least-squares regression estimator of R(d |x t ). Since the (r 1 , ..., r \u03c4 ) are not available, we define an asymptotically equivalent objective that merely uses the biased feedback (c 1 , ..., c \u03c4 ). The new objective corrects for the position bias using Inverse Propensity Score (IPS) weighting [28,29], where the position bias (p 1 , ..., p \u03c4 ) takes the role of the missingness model.\nL c (w) = \u03c4 t =1 dR w (d |x t ) 2 + c t (d) p t (d) (c t (d) \u2212 2R w (d |x t ))(14)\nWe denote the regression estimator defined by the minimum of this objective asR Reg (d |x t ). The regression objective in ( 14) is unbiased, meaning that its expectation is equal to the regression objective L r (w) that uses the unobserved true relevances (r 1 , ..., r \u03c4 ).\nE e L c (w)\n= \u03c4 t =1 d e t (d ) R w (d |x t ) 2 + c t (d) p t (d) (c t (d)\u22122R w (d |x t )) P(e t (d)|\u03c3 t , x t ) = \u03c4 t =1 dR w (d |x t ) 2 + 1 p t (d) r t (d)(r t (d) \u2212 2R w (d |x t )) p t (d) = \u03c4 t =1 dR w (d |x t ) 2 + r t (d) 2 \u2212 2 r t (d)R w (d |x t ) = \u03c4 t =1 d r t (d) \u2212R w (d |x t ) 2 = L r (w)\nLine 2 formulates the expectation in terms of the marginal exposure probabilities P(e t (d)|\u03c3 t , x t ), which decomposes the expectation as the objective is additive in d. Note that P(e t (d) = 1|\u03c3 t , x t ) is therefore equal to p t (d) under our exposure model. Line 3 substitutes c t (d) = e t (d) r t (d) and simplifies the expression, since e t (d) r t (d) = 0 whenever the user is not exposed to an item. Note that the propensities p t (\u03c3 ) for the exposed items now cancel, as long as they are bounded away from zero -meaning that all items have some probability of being found by the user. In case users do not naturally explore low enough in the ranking, active interventions can be used to stochastically promote items in order to ensure non-zero examination propensities (e.g. [27]). Note that unbiasedness holds for any sequence of (x 1 , r 1 , \u03c3 1 )..., (x T , r T , \u03c3 T ), no matter how complex the dependencies between the rankings \u03c3 t are.\nBeyond this proof of unbiasedness, it is possible to use standard concentration inequalities to show that L c (w) converges to L r (w) as the size \u03c4 of the training sequence increases. Thus, under standard conditions on the capacity for uniform convergence, it is possible to show convergence of the minimizer of L c (w) to the least-squares regressor as the size \u03c4 of the training sequence increases. We will use this regression objective to learn neuralnetwork rankers in Section 8.2.", "publication_ref": ["b32", "b27", "b28", "b32", "b27", "b28", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "Estimating Average Relevances", "text": "The conditional relevances R(d |x) are used in the ranking policies from Equation (5). But when defining merit in Equation ( 9) for the fairness disparities, the average relevance R(d) is needed. Furthermore, R(d) serves as the ranking criterion for global rankings in Equation (7). While we could marginalize R(d |x) over P(x) to derive R(d), we argue that the following is a more direct way to get an unbiased estimate.R\nIPS (d) = 1 \u03c4 \u03c4 t =1 c t (d) p t (d) . (15\n)\nThe following shows that this estimator is unbiased as long as the propensities are bounded away from zero.\nE e R IPS (d) = 1 \u03c4 \u03c4 t =1 e t (d ) e t (d) r t (d) p t (d) P(e t (d)|\u03c3 t , x t ) = 1 \u03c4 \u03c4 t =1 r t (d) p t (d) p t (d) = 1 \u03c4 \u03c4 t =1 r t (d) = R(d)\nIn the following experiments, we will use this estimator whenever a direct estimate of R(d) is needed for the fairness disparities or as a global ranking criterion.", "publication_ref": ["b4", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "DYNAMICALLY CONTROLLING FAIRNESS", "text": "Given the formalization of the dynamic LTR problem, our definition of fairness, and our derivation of estimators for all relevant parameters, we are now in the position to tackle the problem of ranking while enforcing the fairness conditions. We view this as a control problem since we need to be robust to the uncertainty in the estimatesR(d |x) andR(d) at the beginning of the learning process. Specifically, we propose a controller that is able to make up for the initial uncertainty as these estimates converge during the learning process. Following our pairwise definitions of amortized fairness from Section 5, we quantify by how much fairness between all classes is violated using the following overall disparity metric.\nD \u03c4 = 2 m(m \u2212 1) m i=0 m j=i+1 D \u03c4 (G i , G j )(16)\nThis metric can be instantiated with the disparity D E \u03c4 (G i , G j ) from Equation (10) for exposure-based fairness, or D I \u03c4 (G i , G j ) from Equation (12) for impact-based fairness. Since optimal fairness is achieved for D \u03c4 = 0, we seek to minimize D \u03c4 .\nTo this end, we now derive a method we call FairCo, which takes the form of a Proportional Controller (a.k.a. P-Controller) [12]. A P-controller is a widely used control-loop mechanism that applies feedback through a correction term that is proportional to the error. In our application, the error corresponds to the violation of our amortized fairness disparity from Equations ( 10) and (12). Specifically, for any set of disjoint groups G = {G 1 , . . . , G m }, the error term of the controller for any item d is defined as\n\u2200G \u2208 G \u2200d \u2208 G : err \u03c4 (d) = (\u03c4 \u2212 1) \u2022 max G i D \u03c4 \u22121 (G i , G) .\nThe error term err \u03c4 (G) is zero for the group that already has the maximum exposure/impact w.r.t. its merit. For items in the other groups, the error term grows with increasing disparity.\nNote that the disparityD \u03c4 \u22121 (G i , G) in the error term uses the es-timatedMerit(G) from Equation (15), which converges to Merit(G) as the sample size \u03c4 increases. To avoid division by zero,Merit(G) can be set to some minimum constant.\nWe are now in a position to state the FairCo ranking policy as FairCo:\n\u03c3 \u03c4 = argsort d \u2208D R (d |x) + \u03bb err \u03c4 (d) .(17)\nWhen the exposure-based disparityD E \u03c4 \u22121 (G i , G) is used in the error term, we refer to this policy as FairCo(Exp). If the impact-based disparityD I \u03c4 \u22121 (G i , G) is used, we refer to it as FairCo(Imp). Like the policies in Section 4.3, FairCo is a sort-based policy. However, the sorting criterion is a combination of relevanceR(d |x) and an error term representing the fairness violation. The idea behind FairCo is that the error term pushes the items from the underexposed groups upwards in the ranking. The parameter \u03bb can be chosen to be any positive constant. While any choice of \u03bb leads to asymptotic convergence as shown by the theorem below for exposure fairness, a suitable choice of \u03bb can have influence on the finite-sample behavior of FairCo: a higher \u03bb can lead to an oscillating behavior, while a smaller \u03bb makes the convergence smoother but slower. We explore the role of \u03bb in the experiments, but find that keeping it fixed at \u03bb = 0.01 works well across all of our experiments. Another key quality of FairCo is that it is agnostic to the choice of error metric, and we conjecture that it can easily be adapted to other types of fairness disparities. Furthermore, it is easy to implement and it is very efficient, making it well suited for practical applications.\nTo illustrate the theoretical properties of FairCo, we now analyze its convergence for the case of exposure-based fairness. To disentangle the convergence of the estimator forMerit(G) from the convergence of FairCo, consider a time point \u03c4 0 whereMerit(G) is already close to Merit(G) for all G \u2208 G. We can thus focus on the question whether FairCo can drive D E \u03c4 to zero starting from any unfairness that may have persisted at time \u03c4 0 . To make this problem well-posed, we need to assume that exposure is not available in overabundance, otherwise it may be unavoidable to give some groups more exposure than they deserve even if they are put at the bottom of the ranking. A sufficient condition for excluding this case is to only consider problems for which the following is true: for all pairs of groups G i , G j , if G i is ranked entirely above G j at any time point t, then\nExp t (G i ) Merit(G i ) \u2265 Exp t (G j ) Merit(G j ) .(18)\nIntuitively, the condition states that ranking G i ahead of G j reduces the disparity if G i has been underexposed in the past. We can now state the following theorem. The proof of the theorem is included in Appendix B. Note that this theorem holds for any time point \u03c4 0 , even if the estimated merits change substantially up to \u03c4 0 . So, once the estimated merits have converged to the true merits, FairCo(Exp) will ensure that the amortized disparity D E \u03c4 converges to zero as well.", "publication_ref": ["b11", "b11", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "EMPIRICAL EVALUATION", "text": "In addition to the theoretical justification of our approach, we also conducted an empirical evaluation 1 . We first present experiments on a semi-synthetic news dataset to investigate different aspects of the proposed methods under controlled conditions. After that we evaluate the methods on real-world movie preference data for external validity.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "Robustness Analysis on News Data", "text": "To be able to evaluate the methods in a variety of specifically designed test settings, we created the following simulation environment from articles in the Ad Fontes Media Bias dataset 2 . It simulates a dynamic ranking problem on a set of news articles belonging to two groups G left and G right (e.g. left-leaning and right-leaning news articles).\nIn each trial, we sample a set of 30 news articles D. For each article, the dataset contains a polarity value \u03c1 d that we rescale to the interval between -1 and 1, while the user polarities are simulated. Each user has a polarity that is drawn from a mixture of two normal\ndistributions clipped to [\u22121, 1] \u03c1 u t \u223c clip [\u22121,1] p ne\u0434 N (\u22120.5, 0.2) + (1 \u2212 p ne\u0434 )N (0.5, 0.2) (19)\nwhere p ne\u0434 is the probability of the user to be left-leaning (mean=\u22120.5).\nWe use p ne\u0434 = 0.5 unless specified. In addition, each user has an openness parameter o u t \u223c U(0.05, 0.55), indicating on the breadth of interest outside their polarity. Based on the polarities of the user u t and the item d, the true relevance is drawn from the Bernoulli\ndistribution r t (d) \u223c Bernoulli p = exp \u2212(\u03c1 u t \u2212 \u03c1 d ) 2 2(o u t ) 2\n.\nAs the model of user behavior, we use the Position-based click model (PBM [19]), where the marginal probability that user u t examines an article only depends only on its position. We choose an exposure drop-off analogous to the gain function in DCG as\np t (d) = 1 log 2 (rank(d |\u03c3 t ) + 1) . (20\n)\nThe remainder of the simulation follows the dynamic ranking setup. At each time step t a user u t arrives to the system, the algorithm presents an unpersonalized ranking \u03c3 t , and the user provides feedback c t according to p t and r t . The algorithm only observes c t and not r t .\nTo investigate group-fairness, we group the items according to their polarity, where items with a polarity \u03c1 d \u2208 [\u22121, 0) belong to the left-leaning group G left and items with a polarity \u03c1 d \u2208 [0, 1] belong to the right-leaning group G right .\nWe measure ranking quality by the average cumulative NDCG  16). 1 The implementation is available at https://github.com/MarcoMorik/Dynamic-Fairness.  8.1.1 Can FairCo reduce unfairness while maintaining good ranking quality? This is the key question in evaluating FairCo, and Figure 1 shows how NDCG and Unfairness converge for Naive, D-ULTR(Glob), and FairCo(Imp). The plots show that Naive achieves the lowest NDCG and that its unfairness remains high as the number of user interactions increases. D-ULTR(Glob) achieve the best NDCG, as predicted by the theory, but its unfairness is only marginally better than that of Naive. Only FairCo manages to substantially reduce unfairness, and this comes only at a small decrease in NDCG compared to D-ULTR(Glob).\nThe following questions will provide further insight into these results, evaluating the components of the FairCo and exploring its robustness.  The first component of FairCo we evaluate is the unbiased IPS estimatorR IPS (d) from Equation (15). Figure 1 shows the absolute difference between the estimated global relevance and true global relevance forR IPS (d) and the estimator used in the Naive. While the error for Naive stagnates at around 0.25, the estimation error ofR IPS (d) approaches zero as the number of users increases. This verifies that IPS eliminates the effect of position bias and learns accurate estimates of the true expected relevance for each news article so that we can use them for the fairness and ranking criteria. 3 Does FairCo overcome the rich-get-richer dynamic? The illustrating example in Section 2 argues that naively ranking items is highly sensitive to the initial conditions (e.g. which items get the first clicks), leading to a rich-get-richer dynamic. We now test whether FairCo overcomes this problem. In particular, we adversarially modify the user distribution so that the first x users are right-leaning (p ne\u0434 = 0), followed by x left-leaning users (p ne\u0434 = 1), before we continue with a balanced user distribution (p ne\u0434 = 0.5). Figure 3 shows the unfairness after 3000 user interactions. As expected, Naive is the most sensitive to the head-start that the rightleaning articles are getting. D-ULTR(Glob) fares better and its unfairness remains constant (but high) independent of the initial user distribution since the unbiased estimatorR IPS (d) corrects for the presentation bias so that the estimates still converge to the true relevance. FairCo inherits this robustness to initial conditions since it uses the sameR IPS (d) estimator, and its active control for unfairness makes it the only method that achieves low unfairness across the whole range.  8.1.4 How effective is the FairCo compared to a more expensive Linear-Programming Baseline? As a baseline, we adapt the linear programming method from [42] to the dynamic LTR setting to minimize the amortized fairness disparities that we consider in this work. The method uses the current relevance and disparity estimates to solve a linear programming problem whose solution is a stochastic ranking policy that satisfies the fairness constraints in expectation at each \u03c4 . The details of this method are described in Appendix A. Figure 4 shows NDCG and Impact Unfairness after 3000 users averaged over 15 trials for both LinProg and FairCo for different values of their hyperparameter \u03bb. For \u03bb = 0, both methods reduce to D-ULTR(Glob) and we can see that their solutions are unfair. As \u03bb increases, both methods start enforcing fairness at the expense of NDCG. In these and other experiments, we found no evidence that the LinProg baseline is superior to FairCo. However, LinProg is substantially more expensive to compute, which makes FairCo preferable in practice.  8.1.5 Is FairCo effective for different group sizes? In this experiment, we vary asymmetry of the polarity within the set of 30 news articles, ranging from G left = 1 to G left = 15 news articles. For each group size, we run 20 trials for 3000 users each. Figure 5 shows that regardless of the group ratio, FairCo reduces unfairness for the whole range while maintaining NDCG. This is in contrast to Naive and D-ULTR(Glob), which suffer from high unfairness.  8.1.6 Is FairCo effective for different user distributions? Finally, to examine the robustness to varying user distributions, we control the polarity distribution of the users by varying p ne\u0434 in Equation (19). We run 20 trials each on 3000 users. In Figure 6, observe that Naive and D-ULTR(Glob) suffer from high unfairness when there is a large imbalance between the minority and the majority group, while FairCo is able to control the unfairness in all settings.", "publication_ref": ["b1", "b18", "b0", "b14", "b41", "b18"], "figure_ref": ["fig_0", "fig_0", "fig_8", "fig_10", "fig_12", "fig_14"], "table_ref": []}, {"heading": "Evaluation on Real-World Preference Data", "text": "To evaluate our method on a real-world preference data, we adopt the ML-20M dataset [25]. We select the five production companies with the most movies in the dataset -MGM, Warner Bros, Paramount, 20th Century Fox, Columbia. These production companies form the groups for which we aim to ensure fairness of exposure. To exclude movies with only a few ratings and have a diverse user population, from the set of 300 most rated movies by these production companies, we select 100 movies with the highest standard deviation in the rating across users. For the users, we select 10 4 users who have rated the most number of the chosen movies. This leaves us with a partially filled ratings matrix with 10 4 users and 100 movies. To avoid missing data for the ease of evaluation, we use an off-the-shelf matrix factorization algorithm 3 to fill in the missing entries. We then normalize the ratings to [0, 1] by apply a Sigmoid function centered at rating b = 3 with slope a = 10. These serve as relevance probabilities where higher star ratings correspond to a higher likelihood of positive feedback. Finally, for each trial we obtain a binary relevance matrix by drawing a Bernoulli sample for each user and movie pair with these probabilities. We use the user embeddings from the matrix factorization model as the user features x t .\nIn the following experiments we use FairCo to learn a sequence of ranking policies \u03c0 t (x) that are personalized based on x. The goal is to maximize NDCG while providing fairness of exposure to the production companies. User interactions are simulated analogously to the previous experiments. At each time step t, we sample a user x t and the ranking algorithm presents a ranking of the 100 movies. The user follows the position-based model from Equation (20) and reveal c t accordingly.\nFor the conditional relevance modelR Reg (d |x) used by FairCo and D-ULTR, we use a one hidden-layer neural network that consists of D = 50 input nodes fully connected to 64 nodes in the hidden layer with ReLU activation, which is connected to 100 output nodes with Sigmoid to output the predicted probability of relevance of each movie. Since training this network with less than 100 observations is unreliable, we use the global ranker D-ULTR(Glob) for the first 100 users. We then train the network at \u03c4 = 100 users, and then update the network after every 10 users on all previously collected feedback i.e. c 1 , ..., c \u03c4 using the unbiased regression objective, L c (w), from Eq. ( 14) with the Adam optimizer [34].  We first evaluate whether training a personalized model using the de-biasedR Reg (d |x) regression estimator improves ranking performance over a non-personalized model. Figure 7 shows that ranking byR Reg (d |x) (i.e. D-ULTR) provides substantially higher NDCG than the unbiased global ranking D-ULTR(Glob) and the Naive ranking.\nTo get an upper bound on the performance of the personalization models, we also train a Skyline model using the (in practice unobserved) true relevances r t with the least-squares objective from Eq. (13). Even though the unbiased regression estimatorR Reg (d |x) only has access to the partial feedback c t , it tracks the performance of Skyline. As predicted by the theory, they appear to converge asymptotically. 3 Surprise library (http://surpriselib.com/) for SVD with biased=False and D=50    8 shows that FairCo(Exp) can effectively control Exposure Unfairness, unlike the other methods that do not actively consider fairness. Similarly, Figure 9 shows that FairCo(Imp) is effective at controlling Impact Unfairness. As expected, the improvement in fairness comes at a reduction in NDCG, but this reduction is small.   10 evaluates how an algorithm that optimizes Exposure Fairness performs in terms of Impact Fairness and vice versa. The plots show that the two criteria achieve different goals and that they are substantially different. In fact, optimizing for fairness in impact can even increase the unfairness in exposure, illustrating that the choice of criterion needs to be grounded in the requirements of the application.", "publication_ref": ["b24", "b2", "b19", "b33", "b2"], "figure_ref": ["fig_16", "fig_18", "fig_19", "fig_0"], "table_ref": []}, {"heading": "CONCLUSIONS", "text": "We identify how biased feedback and uncontrolled exposure allocation can lead to unfairness and undesirable behavior in dynamic LTR. To address this problem, we propose FairCo, which is able to adaptively enforce amortized merit-based fairness constraints even though their underlying relevances are still being learned. The algorithm is robust to presentation bias and thus does not exhibit rich-get-richer dynamics. Finally, FairCo is easy to implement and computationally efficient, which makes it well suited for practical applications.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A LINEAR PROGRAMMING BASELINE", "text": "Here, we present a version of the fairness constraint defined in Singh and Joachims [42] that explicitly computes an optimal ranking to present in each time step \u03c4 by solving a linear program (LP). In particular, we formulate an LP that explicitly maximizes the estimated DCG of the ranking \u03c3 \u03c4 while minimizing the estimated cumulative fairness disparity D I \u03c4 formulated in Equation ( 11). This is used as a baseline to compare the P-Controller with.\nTo avoid an expensive search over the exponentially-sized space of rankings as in [14], we exploit an alternative representation [42] as a doubly-stochastic matrix P that is sufficient for representing \u03c3 t . In this matrix, the entry P y, j denotes the probability of placing item y at position j. Both DCG as well as Imp \u03c4 (G i ) are linear functions of P, which means that the optimum can be computed as the following linear program.\nP * = argmax P, \u03be \u22650 yR (y|x) n j=1 P y, j log(1 + j) Utility \u2212\u03bb i, j \u03be i j s.t. \u2200y, j : n i=1 P y,i = 1, y \u2032 P y \u2032 , j = 1, 0 \u2264 P y, j \u2264 1 \u2200 G i , G j : \u00ce mp \u03c4 (G i |P \u03c4 ) Merit(G i ) \u2212\u00ce mp \u03c4 (G j |P \u03c4 ) Merit(G j ) + D \u03c4 \u22121 (G i , G j ) \u2264 \u03be i j(21)\nThe parameter \u03bb controls trade-off between DCG of \u03c3 t and fairness. We explore this parameter empirically in Section 8.1. It remains to define\u00cemp \u03c4 (G j |P \u03c4 ). Assuming the PBM click model with q(j) denoting the examination propensity of item d at position j, the estimated probability of a click isR(d)\u2022q(j). So we can estimate the impact on the items in group G for the rankings defined by P a\u015d\nImp \u03c4 (G |P) = 1 |G | d \u2208GR (d |x) n j=1\nP y, j q(j)\nWe use the scipy.optimize.linprog LP solver to solve for the optimal P * , and then use a Birkhoff von-Neumann decomposition [15,42] to sample a deterministic ranking \u03c3 \u03c4 to present to the user. This ranking is guaranteed to achieve the DCG and fairness optimized by P * in expectation.\nNote that the number of variables in the LP is O(n 2 + |G| 2 ), and even a polynomial-time LP solver incurs substantial computation cost when working with a large number of items in a practical dynamic ranking application.", "publication_ref": ["b41", "b13", "b41", "b14", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "B CONVERGENCE OF FAIRCO-CONTROLLER", "text": "In this section we will prove the convergence theorem of FairCo for exposure fairness. We conjecture that analogous proofs apply to other fairness criteria as well. To prove the main theorem, we will first set up the following lemmas.\nLemma B.1. Under the conditions of the main theorem, for any value of \u03bb and any \u03c4 > \u03c4 0 : if D E \u03c4 \u22121 (G i , G j ) > 1 (\u03c4 \u22121)\u03bb , then \u03c4 D E \u03c4 (G i , G j ) \u2264 (\u03c4 \u2212 1)D E \u03c4 \u22121 (G i , G j ).\nProof. From the definition of D E \u03c4 in Eq. ( 10) we know that for \u03c4 > \u03c4 0 ,\n\u03c4 D E \u03c4 (G i , G j ) = (\u03c4 \u2212 1)D E \u03c4 \u22121 (G i , G j ) + Exp \u03c4 (G i ) Merit(G i ) \u2212 Exp \u03c4 (G j )\nMerit(G j ) .\nSince D E \u03c4 \u22121 (G i , G j ) > 1 (\u03c4 \u22121)\u03bb , we know that for all items in G j it holds that err \u03c4 (d) > 1 \u03bb . Hence, FairCo adds a correction term \u03bberr \u03c4 (d) to theR(d) of all d \u2208 G j that is greater than \u03bb 1 \u03bb = 1. Since 0 \u2264R(d) \u2264 1, the ranking is dominated by the correction term \u03bberr \u03c4 (d). This means that all d \u2208 G j are ranked above all d \u2208 G i . Under the feasibility condition from Eq.( 18), this implies that\nExp \u03c4 (G i ) Mer it (G i ) \u2264 Exp \u03c4 (G j ) Mer it (G j ) and thus \u03c4 D E \u03c4 (G i , G j ) \u2264 (\u03c4 \u2212 1)D E \u03c4 \u22121 (G i , G j ). \u25a1 Lemma B.2.\nUnder the conditions of the main theorem, for any value of \u03bb > 0 there exists \u2206 \u2265 0 such that for any G i , G j and \u03c4 > \u03c4 0 : if\nD E \u03c4 \u22121 (G i , G j ) \u2264 1 (\u03c4 \u22121)\u03bb , then \u03c4 D E \u03c4 (G i , G j ) \u2264 1 \u03bb + \u2206.\nProof. Using the definition the definition of D E \u03c4 in Eq. (10), we know that\n\u03c4 D E \u03c4 (G i , G j ) = (\u03c4 \u2212 1)D E \u03c4 \u22121 (G i , G j ) + Exp \u03c4 (G i ) Merit(G i ) \u2212 Exp \u03c4 (G j ) Merit(G j ) \u2264 1 \u03bb + Exp \u03c4 (G i ) Merit(G i ) \u2212 Exp \u03c4 (G j ) Merit(G j ) \u2264 1 \u03bb + \u2206 where \u2206 = max \u03c3 max G,G \u2032 G G \u2032 Exp \u03c3 (G) Mer it (G) \u2212 Exp \u03c3 (G \u2032 ) Mer it (G \u2032 )\n. Note that \u2206 is a constant independent of \u03c4 and refers to the ranking \u03c3 for which two groups G, G \u2032 have the maximum exposure difference (e.g. one is placed at the top of the ranking, and the other is placed at the bottom). \u25a1 Using these two lemmas, we conclude the following theorem:\nTheorem B.3. For any set of disjoint groups G = {G 1 , . . . , G m } with any fixed target meritsMerit(G i ) > 0 that fulfill (18), any relevance modelR(d |x) \u2208 [0, 1], any exposure model p t (d) with 0 \u2264 p t (d) \u2264 p max , and any value \u03bb > 0, running FairCo(Exp) from time \u03c4 0 will always ensure that the overall disparity D E \u03c4 with respect to the target merits converges to zero at a rate of O 1 \u03c4 , no matter how unfair the exposures 1 \u03c4 0 \u03c4 0 t =1 Exp t (G j ) up to \u03c4 0 have been.\nProof. To prove that D E \u03c4 converges to zero at a rate of O 1 \u03c4 , we will show that for all \u03c4 \u2265 \u03c4 0 , the following holds:\nD E \u03c4 \u2264 1 \u03c4 2 m(m \u2212 1) m i=1 m j=i+1 max \u03c4 0 D E \u03c4 0 (G i , G j ) , 1 \u03bb + \u2206\nThe two terms in the max provide an upper bound on the disparity at time \u03c4 for any G i and G j . To show this, we prove by induction that \u03c4 D E \u03c4 (G i , G j ) \u2264 max \u03c4 0 D E \u03c4 0 (G i , G j ) , 1 \u03bb + \u2206 for all \u03c4 \u2265 \u03c4 0 . At the start of the induction at \u03c4 = \u03c4 0 , the max directly upper bounds \u03c4 0 D E \u03c4 0 (G i , G j ). In the induction step from \u03c4 \u2212 1 to \u03c4 , if\n(\u03c4 \u2212 1)D E \u03c4 \u22121 (G i , G j ) > 1 \u03bb , then Lemma B.1 implies that \u03c4 D E \u03c4 (G i , G j ) \u2264 (\u03c4 \u2212 1)D E \u03c4 \u22121 (G i , G j ) \u2264 max \u03c4 0 D E \u03c4 0 (G i , G j ) , 1 \u03bb + \u2206 . If (\u03c4 \u22121)D E \u03c4 \u22121 (G i , G j ) \u2264 1 \u03bb , then Lemma B.2 implies that \u03c4 D E \u03c4 (G i , G j ) \u2264 1 \u03bb + \u2206 \u2264 max \u03c4 0 D E \u03c4 0 (G i , G j ) , 1\n\u03bb + \u2206 as well. This completes the induction, and we conclude that\nD E \u03c4 (G i , G j ) \u2264 1 \u03c4 max \u03c4 0 |D E \u03c4 0 (G i , G j )|, 1 \u03bb + \u2206 .\nPutting everything together, we get\nD E \u03c4 = 2 m(m \u2212 1) m i=0 m j=i+1 D E \u03c4 (G i , G j ) \u2264 2 m(m \u2212 1) m i=0 m j=i+1 1 \u03c4 max \u03c4 0 |D E \u03c4 0 (G i , G j )|, 1 \u03bb + \u2206 \u2264 1 \u03c4 2 m(m \u2212 1) m i=0 m j=i+1 max \u03c4 0 D E \u03c4 0 (G i , G j ) , 1 \u03bb + \u2206 (since \u03bb, \u2206, \u03c4 > 0) \u25a1", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}, {"heading": "ACKNOWLEDGMENTS", "text": "This research was supported in part by NSF Awards IIS-1901168 and a gift from Workday. All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Beyond Personalization: Research Directions in Multistakeholder Recommendation", "journal": "", "year": "2019", "authors": "Himan Abdollahpouri; Gediminas Adomavicius; Robin Burke; Ido Guy; Dietmar Jannach; Toshihiro Kamishima; Jan Krasnodebski; Luiz Pizzato"}, {"ref_id": "b1", "title": "Controlling popularity bias in learning-to-rank recommendation", "journal": "", "year": "2017", "authors": "Himan Abdollahpouri; Robin Burke; Bamshad Mobasher"}, {"ref_id": "b2", "title": "Power-law distribution of the world wide web", "journal": "Science", "year": "2000", "authors": "A Lada;  Adamic; A Bernardo;  Huberman"}, {"ref_id": "b3", "title": "A General Framework for Counterfactual Learning-to-Rank", "journal": "", "year": "2019", "authors": "A Agarwal; K Takatsu; I Zaitsev; T Joachims"}, {"ref_id": "b4", "title": "Estimating Position Bias Without Intrusive Interventions", "journal": "", "year": "2019", "authors": "A Agarwal; I Zaitsev; Xuanhui Wang; Cheng Li; M Najork; T Joachims"}, {"ref_id": "b5", "title": "Unbiased learning to rank with unbiased propensity estimation", "journal": "", "year": "2018", "authors": "Qingyao Ai; Keping Bi; Cheng Luo; Jiafeng Guo; W Bruce Croft"}, {"ref_id": "b6", "title": "TREC Fair Ranking Track", "journal": "", "year": "2019", "authors": "Asia Michael Ekstrand Sebastian Kohlmeier; Fernando Biega;  Diaz"}, {"ref_id": "b7", "title": "Bias on the Web", "journal": "Commun. ACM", "year": "2018", "authors": "Ricardo Baeza-Yates"}, {"ref_id": "b8", "title": "", "journal": "Fairness and Machine Learning", "year": "2018", "authors": "Solon Barocas; Moritz Hardt; Arvind Narayanan"}, {"ref_id": "b9", "title": "Big data's disparate impact", "journal": "Calif. L. Rev", "year": "2016", "authors": "Solon Barocas; D Andrew;  Selbst"}, {"ref_id": "b10", "title": "Automating the news: How personalized news recommender system design choices impact news reception", "journal": "Communication Research", "year": "2014", "authors": "A Michael;  Beam"}, {"ref_id": "b11", "title": "Process control: modeling, design, and simulation", "journal": "Prentice Hall Professional", "year": "2003", "authors": " B Wayne Bequette"}, {"ref_id": "b12", "title": "Fairness in Recommendation Ranking through Pairwise Comparisons", "journal": "", "year": "2019", "authors": "Alex Beutel; Jilin Chen; Tulsee Doshi; Hai Qian; Li Wei; Yi Wu; Lukasz Heldt; Zhe Zhao; Lichan Hong; Ed H Chi; Cristos Goodrow"}, {"ref_id": "b13", "title": "Equity of Attention: Amortizing Individual Fairness in Rankings", "journal": "", "year": "2018", "authors": "J Asia; Krishna P Biega; Gerhard Gummadi;  Weikum"}, {"ref_id": "b14", "title": "Lattice theory", "journal": "American Mathematical Soc", "year": "1940", "authors": "Garrett Birkhoff"}, {"ref_id": "b15", "title": "From Ranknet to Lambdarank to Lambdamart: An overview", "journal": "Learning", "year": "2010", "authors": "J C Christopher;  Burges"}, {"ref_id": "b16", "title": "", "journal": "", "year": "2017", "authors": "Damian L Elisa Celis; Nisheeth K Straszak;  Vishnoi"}, {"ref_id": "b17", "title": "Prediction, learning, and games", "journal": "Cambridge University Press", "year": "2006", "authors": "Nicol\u00f2 Cesa; - Bianchi; G\u00e1bor Lugosi"}, {"ref_id": "b18", "title": "Click models for web search", "journal": "Synthesis Lectures on Information Concepts, Retrieval, and Services", "year": "2015", "authors": "Aleksandr Chuklin; Ilya Markov; Maarten De Rijke"}, {"ref_id": "b19", "title": "How algorithmic popularity bias hinders or promotes quality", "journal": "Scientific reports", "year": "2018", "authors": "Giovanni Luca Ciampaglia; Azadeh Nematzadeh; Filippo Menczer; Alessandro Flammini"}, {"ref_id": "b20", "title": "An experimental comparison of click position-bias models", "journal": "", "year": "2008", "authors": "Nick Craswell; Onno Zoeter; Michael Taylor; Bill Ramsey"}, {"ref_id": "b21", "title": "Runaway Feedback Loops in Predictive Policing", "journal": "", "year": "2018", "authors": "Danielle Ensign; A Sorelle; Scott Friedler; Carlos Neville; Suresh Scheidegger;  Venkatasubramanian"}, {"ref_id": "b22", "title": "Intervention Harvesting for Context-Dependent Examination-Bias Estimation", "journal": "", "year": "2019", "authors": "A Zhichong Fang; T Agarwal;  Joachims"}, {"ref_id": "b23", "title": "The few-getricher: a surprising consequence of popularity-based rankings", "journal": "", "year": "2019", "authors": "Fabrizio Germano; Vicen\u00e7 G\u00f3mez; Ga\u00ebl Le Mens"}, {"ref_id": "b24", "title": "The movielens datasets: History and context", "journal": "ACM TIIS", "year": "2015", "authors": "Maxwell Harper; Joseph A Konstan"}, {"ref_id": "b25", "title": "Large Margin Ranking Boundaries for Ordinal Regression", "journal": "", "year": "2000", "authors": "Graepel Herbrich; Obermayer "}, {"ref_id": "b26", "title": "Balancing exploration and exploitation in listwise and pairwise online learning to rank for information retrieval", "journal": "Information Retrieval", "year": "2013", "authors": "Katja Hofmann; Shimon Whiteson; Maarten De Rijke"}, {"ref_id": "b27", "title": "A generalization of sampling without replacement from a finite universe", "journal": "Journal of the American statistical Association", "year": "1952", "authors": "G Daniel; Donovan J Horvitz;  Thompson"}, {"ref_id": "b28", "title": "Causal inference in statistics, social, and biomedical sciences", "journal": "Cambridge University Press", "year": "2015", "authors": "W Guido; Donald B Imbens;  Rubin"}, {"ref_id": "b29", "title": "Cumulated gain-based evaluation of IR techniques", "journal": "TOIS", "year": "2002", "authors": "Kalervo J\u00e4rvelin; Jaana Kek\u00e4l\u00e4inen"}, {"ref_id": "b30", "title": "Optimizing Search Engines Using Clickthrough Data", "journal": "", "year": "2002", "authors": "T Joachims"}, {"ref_id": "b31", "title": "Evaluating the Accuracy of Implicit Feedback from Clicks and Query Reformulations in Web Search", "journal": "ACM TOIS", "year": "2007", "authors": "T Joachims; L Granka; Bing Pan; H Hembrooke; F Radlinski; G Gay"}, {"ref_id": "b32", "title": "Unbiased Learning-to-Rank with Biased Feedback", "journal": "", "year": "2017", "authors": "T Joachims; A Swaminathan; T Schnabel"}, {"ref_id": "b33", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b34", "title": "Online Learning to Rank with Features", "journal": "", "year": "2018", "authors": "Shuai Li; Tor Lattimore; Csaba Szepesv\u00e1ri"}, {"ref_id": "b35", "title": "Delayed Impact of Fair Machine Learning", "journal": "", "year": "2018", "authors": "Lydia Liu; Sarah Dean; Esther Rolf; Max Simchowitz; Moritz Hardt"}, {"ref_id": "b36", "title": "Minimally Invasive Randomization for Collecting Unbiased Preferences from Clickthrough Logs", "journal": "", "year": "2006", "authors": "F Radlinski; T Joachims"}, {"ref_id": "b37", "title": "Learning Diverse Rankings with Multi-Armed Bandits", "journal": "", "year": "2008", "authors": "F Radlinski; R Kleinberg; T Joachims"}, {"ref_id": "b38", "title": "The probability ranking principle in IR", "journal": "Journal of documentation", "year": "1977", "authors": " Stephen E Robertson"}, {"ref_id": "b39", "title": "Experimental study of inequality and unpredictability in an artificial cultural market", "journal": "Science", "year": "2006", "authors": "M J Salganik; P Sheridan Dodds; D J Watts"}, {"ref_id": "b40", "title": "Recommendations as Treatments: Debiasing Learning and Evaluation", "journal": "", "year": "2016", "authors": "Tobias Schnabel; Adith Swaminathan; Ashudeep Singh; Navin Chandak; Thorsten Joachims"}, {"ref_id": "b41", "title": "Fairness of Exposure in Rankings", "journal": "", "year": "2018", "authors": "Ashudeep Singh; Thorsten Joachims"}, {"ref_id": "b42", "title": "Policy Learning for Fairness in Ranking", "journal": "", "year": "2019", "authors": "Ashudeep Singh; Thorsten Joachims"}, {"ref_id": "b43", "title": "Consequential ranking algorithms and long-term welfare", "journal": "", "year": "2019", "authors": "Vicen\u00e7 Behzad Tabibian; Abir G\u00f3mez; Bernhard De; Manuel Gomez Sch\u00f6lkopf;  Rodriguez"}, {"ref_id": "b44", "title": "Softrank: optimizing non-smooth rank metrics", "journal": "", "year": "2008", "authors": "Michael Taylor; John Guiver; Stephen Robertson; Tom Minka"}, {"ref_id": "b45", "title": "The spread of true and false news online", "journal": "Science", "year": "2018", "authors": "Soroush Vosoughi; Deb Roy; Sinan Aral"}, {"ref_id": "b46", "title": "Position bias estimation for unbiased learning to rank in personal search", "journal": "ACM", "year": "2018", "authors": "Xuanhui Wang; Nadav Golbandi; Michael Bendersky; Donald Metzler; Marc Najork"}, {"ref_id": "b47", "title": "Fair Learning-to-Rank from Implicit Feedback", "journal": "", "year": "2019", "authors": "Himank Yadav; Zhengxiao Du; Thorsten Joachims"}, {"ref_id": "b48", "title": "Challenging the long tail recommendation", "journal": "", "year": "2012", "authors": "Bin Hongzhi Yin; Jing Cui; Junjie Li; Chen Yao;  Chen"}, {"ref_id": "b49", "title": "Online learning to rank in stochastic click models", "journal": "", "year": "2017", "authors": "Masrour Zoghi; Tomas Tunys; Mohammad Ghavamzadeh; Branislav Kveton; Csaba Szepesvari; Zheng Wen"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Algorithm 1 :1Naive Dynamic LTR Algorithm Initialize counters C(d) = 0 for each d \u2208 D; foreach user do present ranking \u03c3 = argsort D [C(d)] (random tiebreak); increment C(d) for the articles read by the user.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "are optimal for virtually all U (\u03c3 | r) commonly used in IR (e.g. DCG). So, the problem lies in estimating the expected relevance R(d |x) of each item d conditioned on x. When learning a single global ranking, this further simplifies to estimating the expected average relevance R(d) = \u222b r(d)d P(r, x) for each item d. The global ranking can then be derived via", "figure_data": ""}, {"figure_label": "71", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Theorem 7 . 1 .71For any set of disjoint groups G = {G 1 , . . . , G m } with any fixed target meritsMerit(G i ) > 0 that fulfill(18), any relevance modelR(d |x) \u2208 [0, 1], any exposure model p t (d) with 0 \u2264 p t (d) \u2264 p max , and any value \u03bb > 0, running FairCo(Exp) from time \u03c4 0 will always ensure that the overall disparity D E \u03c4 with respect to the target merits converges to zero at a rate of O 1 \u03c4 , no matter how unfair the exposures 1 \u03c4 0 \u03c4 0 t =1 Exp t (G j ) up to \u03c4 0 have been.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "t=1 U DCG (\u03c3 t | r t ) over all the users up to time \u03c4 . We measure Exposure Unfairness via DE \u03c4 and Impact Unfairness via D I \u03c4 as defined in Equation (", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "2 https://www.adfontesmedia.com/interactive-media-bias-chart/ In all news experiments, we learn a global ranking and compare the following methods. Naive: Rank by the sum of the observed feedback c t . D-ULTR(Glob): Dynamic LTR by sorting via the unbiased esti-matesR IPS (d) from Eq.(15). FairCo(Imp): Fairness controller from Eq. (17) for impact fairness.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 1 :1Figure 1: Convergence of NDCG (left) and Unfairness (right) as the number of users increases. (100 trials)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": ") R(d)| Naive R IPS (d)", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 2 :2Figure 2: Error of relevance estimators as the number of users increases (|D| = 30, 10 trials)", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 3 :3Figure 3: The effect of a block of right-leaning users on the Unfairness of Impact. (50 trials, 3000 users)", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 4 :4Figure 4: Comparing the LP Baseline and the P-Controller in terms of NDCG (left) and Unfairness (right) for different values of \u03bb. (15 trials, 3000 users)", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Figure 5 :5Figure 5: NDCG (left) and Unfairness (right) for varying proportion of G left(20 trials, 3000 users)    ", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "Figure 6 :6Figure 6: NDCG (left) and Unfairness (right) with varying user distributions. (20 trials, 3000 users)", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_16", "figure_caption": "Figure 7 :7Figure 7: Comparing the NDCG of personalized and nonpersonalized rankings on the Movie data. (10 trials) 8.2.1 Does personalization via unbiased objective improve NDCG?.We first evaluate whether training a personalized model using the de-biasedR Reg (d |x) regression estimator improves ranking performance over a non-personalized model. Figure7shows that ranking byR Reg (d |x) (i.e. D-ULTR) provides substantially higher NDCG than the unbiased global ranking D-ULTR(Glob) and the Naive ranking.To get an upper bound on the performance of the personalization models, we also train a Skyline model using the (in practice unobserved) true relevances r t with the least-squares objective from Eq.(13). Even though the unbiased regression estimatorR Reg (d |x) only has access to the partial feedback c t , it tracks the performance of Skyline. As predicted by the theory, they appear to converge asymptotically.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_18", "figure_caption": "Figure 8 :8Figure 8: NDCG (left) and Exposure Unfairness (right) on the Movie data as the number of user interactions increases. (10 trials)", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_19", "figure_caption": "Figure 9 :9Figure 9: NDCG (left) and Impact Unfairness (right) on the Movie data as the number of user interactions increases. (10 trials)", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_21", "figure_caption": "Figure 10 :10Figure 10: Unfairness of Exposure (left) and Unfairness of Impact (right) for the personalized controller optimized for either Exposure or Impact. (10 trials) 8.2.3 How different are exposure and impact fairness? Figure10evaluates how an algorithm that optimizes Exposure Fairness performs in terms of Impact Fairness and vice versa. The plots show that the two criteria achieve different goals and that they are substantially different. In fact, optimizing for fairness in impact can even increase the unfairness in exposure, illustrating that the choice of criterion needs to be grounded in the requirements of the application.", "figure_data": ""}], "formulas": [{"formula_id": "formula_1", "formula_text": "\u03c0 t +1 \u2190\u2212 A((x 1 , \u03c3 1 , c 1 ), ..., (x t , \u03c3 t , c t ))", "formula_coordinates": [3.0, 366.84, 226.94, 142.38, 9.89]}, {"formula_id": "formula_2", "formula_text": "c t (d) = r t (d) if e t (d) = 1 0 otherwise (2)", "formula_coordinates": [3.0, 386.98, 390.66, 171.22, 17.7]}, {"formula_id": "formula_3", "formula_text": ")3", "formula_coordinates": [3.0, 551.86, 618.06, 6.34, 4.09]}, {"formula_id": "formula_4", "formula_text": "U DCG (\u03c3 | r) = d \u2208\u03c3 r(d) log 2 (1 + rank(d |\u03c3 ))", "formula_coordinates": [4.0, 100.29, 180.99, 143.39, 24.83]}, {"formula_id": "formula_5", "formula_text": "U (\u03c0 ) = \u222b U (\u03c0 (x)| r) d P(x, r).(4)", "formula_coordinates": [4.0, 110.39, 245.91, 183.65, 14.94]}, {"formula_id": "formula_6", "formula_text": "d \u2208 D R(d |x) ,(5)", "formula_coordinates": [4.0, 151.81, 371.83, 142.24, 17.19]}, {"formula_id": "formula_7", "formula_text": "R(d |x) = \u222b r(d) d P(r |x),(6)", "formula_coordinates": [4.0, 117.02, 405.31, 177.03, 14.94]}, {"formula_id": "formula_8", "formula_text": "\u03c3 = argsort d \u2208 D R(d)(7)", "formula_coordinates": [4.0, 129.68, 501.31, 164.37, 17.19]}, {"formula_id": "formula_9", "formula_text": "G = {G 1 , . . . , G m }. Exp t (G i ) = 1 |G i | d \u2208G i p t (d).(8)", "formula_coordinates": [4.0, 318.18, 108.84, 240.02, 41.95]}, {"formula_id": "formula_10", "formula_text": "Merit(G i ) = 1 |G i | d \u2208G i R(d)(9)", "formula_coordinates": [4.0, 377.9, 231.91, 180.3, 23.71]}, {"formula_id": "formula_11", "formula_text": "D E \u03c4 (G i , G j ) = 1 \u03c4 \u03c4 t =1 Exp t (G i ) Merit(G i ) \u2212 1 \u03c4 \u03c4 t =1 Exp t (G j ) Merit(G j )(10)", "formula_coordinates": [4.0, 333.3, 345.71, 224.91, 25.37]}, {"formula_id": "formula_12", "formula_text": "Imp t (G i ) = 1 |G i | d \u2208G i c t (d),(11)", "formula_coordinates": [4.0, 379.63, 506.76, 178.57, 23.71]}, {"formula_id": "formula_13", "formula_text": "D I \u03c4 (G i , G j ) = 1 \u03c4 \u03c4 t =1 Imp t (G i ) Merit(G i ) \u2212 1 \u03c4 \u03c4 t =1 Imp t (G j ) Merit(G j )(12)", "formula_coordinates": [4.0, 333.44, 565.11, 224.77, 25.37]}, {"formula_id": "formula_14", "formula_text": "L r (w) = \u03c4 t =1 d r t (d) \u2212R w (d |x t ) 2 (13)", "formula_coordinates": [5.0, 100.4, 518.61, 193.65, 29.32]}, {"formula_id": "formula_15", "formula_text": "L c (w) = \u03c4 t =1 dR w (d |x t ) 2 + c t (d) p t (d) (c t (d) \u2212 2R w (d |x t ))(14)", "formula_coordinates": [5.0, 62.51, 639.68, 231.54, 29.32]}, {"formula_id": "formula_16", "formula_text": "= \u03c4 t =1 d e t (d ) R w (d |x t ) 2 + c t (d) p t (d) (c t (d)\u22122R w (d |x t )) P(e t (d)|\u03c3 t , x t ) = \u03c4 t =1 dR w (d |x t ) 2 + 1 p t (d) r t (d)(r t (d) \u2212 2R w (d |x t )) p t (d) = \u03c4 t =1 dR w (d |x t ) 2 + r t (d) 2 \u2212 2 r t (d)R w (d |x t ) = \u03c4 t =1 d r t (d) \u2212R w (d |x t ) 2 = L r (w)", "formula_coordinates": [5.0, 329.78, 135.94, 235.07, 135.8]}, {"formula_id": "formula_17", "formula_text": "IPS (d) = 1 \u03c4 \u03c4 t =1 c t (d) p t (d) . (15", "formula_coordinates": [5.0, 395.89, 683.39, 158.89, 28.78]}, {"formula_id": "formula_18", "formula_text": ")", "formula_coordinates": [5.0, 554.78, 695.74, 3.42, 4.09]}, {"formula_id": "formula_19", "formula_text": "E e R IPS (d) = 1 \u03c4 \u03c4 t =1 e t (d ) e t (d) r t (d) p t (d) P(e t (d)|\u03c3 t , x t ) = 1 \u03c4 \u03c4 t =1 r t (d) p t (d) p t (d) = 1 \u03c4 \u03c4 t =1 r t (d) = R(d)", "formula_coordinates": [6.0, 80.23, 114.45, 187.39, 103.1]}, {"formula_id": "formula_20", "formula_text": "D \u03c4 = 2 m(m \u2212 1) m i=0 m j=i+1 D \u03c4 (G i , G j )(16)", "formula_coordinates": [6.0, 105.95, 424.06, 188.09, 28.67]}, {"formula_id": "formula_21", "formula_text": "\u2200G \u2208 G \u2200d \u2208 G : err \u03c4 (d) = (\u03c4 \u2212 1) \u2022 max G i D \u03c4 \u22121 (G i , G) .", "formula_coordinates": [6.0, 72.06, 609.95, 203.58, 15.45]}, {"formula_id": "formula_22", "formula_text": "\u03c3 \u03c4 = argsort d \u2208D R (d |x) + \u03bb err \u03c4 (d) .(17)", "formula_coordinates": [6.0, 385.51, 106.97, 172.7, 17.19]}, {"formula_id": "formula_23", "formula_text": "Exp t (G i ) Merit(G i ) \u2265 Exp t (G j ) Merit(G j ) .(18)", "formula_coordinates": [6.0, 393.04, 513.69, 165.16, 24.18]}, {"formula_id": "formula_24", "formula_text": "distributions clipped to [\u22121, 1] \u03c1 u t \u223c clip [\u22121,1] p ne\u0434 N (\u22120.5, 0.2) + (1 \u2212 p ne\u0434 )N (0.5, 0.2) (19)", "formula_coordinates": [7.0, 53.8, 336.05, 240.25, 25.35]}, {"formula_id": "formula_25", "formula_text": "distribution r t (d) \u223c Bernoulli p = exp \u2212(\u03c1 u t \u2212 \u03c1 d ) 2 2(o u t ) 2", "formula_coordinates": [7.0, 53.8, 424.35, 188.54, 33.28]}, {"formula_id": "formula_26", "formula_text": "p t (d) = 1 log 2 (rank(d |\u03c3 t ) + 1) . (20", "formula_coordinates": [7.0, 120.21, 514.41, 170.41, 19.35]}, {"formula_id": "formula_27", "formula_text": ")", "formula_coordinates": [7.0, 290.62, 520.48, 3.42, 4.09]}, {"formula_id": "formula_28", "formula_text": "P * = argmax P, \u03be \u22650 yR (y|x) n j=1 P y, j log(1 + j) Utility \u2212\u03bb i, j \u03be i j s.t. \u2200y, j : n i=1 P y,i = 1, y \u2032 P y \u2032 , j = 1, 0 \u2264 P y, j \u2264 1 \u2200 G i , G j : \u00ce mp \u03c4 (G i |P \u03c4 ) Merit(G i ) \u2212\u00ce mp \u03c4 (G j |P \u03c4 ) Merit(G j ) + D \u03c4 \u22121 (G i , G j ) \u2264 \u03be i j(21)", "formula_coordinates": [11.0, 54.19, 255.54, 239.85, 118.96]}, {"formula_id": "formula_29", "formula_text": "Imp \u03c4 (G |P) = 1 |G | d \u2208GR (d |x) n j=1", "formula_coordinates": [11.0, 358.93, 157.46, 121.3, 29.32]}, {"formula_id": "formula_30", "formula_text": "\u03c4 D E \u03c4 (G i , G j ) = (\u03c4 \u2212 1)D E \u03c4 \u22121 (G i , G j ) + Exp \u03c4 (G i ) Merit(G i ) \u2212 Exp \u03c4 (G j )", "formula_coordinates": [12.0, 54.89, 221.31, 244.23, 24.18]}, {"formula_id": "formula_31", "formula_text": "Exp \u03c4 (G i ) Mer it (G i ) \u2264 Exp \u03c4 (G j ) Mer it (G j ) and thus \u03c4 D E \u03c4 (G i , G j ) \u2264 (\u03c4 \u2212 1)D E \u03c4 \u22121 (G i , G j ). \u25a1 Lemma B.2.", "formula_coordinates": [12.0, 53.59, 326.06, 240.45, 45.18]}, {"formula_id": "formula_32", "formula_text": "D E \u03c4 \u22121 (G i , G j ) \u2264 1 (\u03c4 \u22121)\u03bb , then \u03c4 D E \u03c4 (G i , G j ) \u2264 1 \u03bb + \u2206.", "formula_coordinates": [12.0, 61.24, 385.26, 184.43, 13.99]}, {"formula_id": "formula_33", "formula_text": "\u03c4 D E \u03c4 (G i , G j ) = (\u03c4 \u2212 1)D E \u03c4 \u22121 (G i , G j ) + Exp \u03c4 (G i ) Merit(G i ) \u2212 Exp \u03c4 (G j ) Merit(G j ) \u2264 1 \u03bb + Exp \u03c4 (G i ) Merit(G i ) \u2212 Exp \u03c4 (G j ) Merit(G j ) \u2264 1 \u03bb + \u2206 where \u2206 = max \u03c3 max G,G \u2032 G G \u2032 Exp \u03c3 (G) Mer it (G) \u2212 Exp \u03c3 (G \u2032 ) Mer it (G \u2032 )", "formula_coordinates": [12.0, 53.47, 432.19, 247.67, 103.4]}, {"formula_id": "formula_34", "formula_text": "D E \u03c4 \u2264 1 \u03c4 2 m(m \u2212 1) m i=1 m j=i+1 max \u03c4 0 D E \u03c4 0 (G i , G j ) , 1 \u03bb + \u2206", "formula_coordinates": [12.0, 334.17, 226.07, 203.44, 28.67]}, {"formula_id": "formula_35", "formula_text": "(\u03c4 \u2212 1)D E \u03c4 \u22121 (G i , G j ) > 1 \u03bb , then Lemma B.1 implies that \u03c4 D E \u03c4 (G i , G j ) \u2264 (\u03c4 \u2212 1)D E \u03c4 \u22121 (G i , G j ) \u2264 max \u03c4 0 D E \u03c4 0 (G i , G j ) , 1 \u03bb + \u2206 . If (\u03c4 \u22121)D E \u03c4 \u22121 (G i , G j ) \u2264 1 \u03bb , then Lemma B.2 implies that \u03c4 D E \u03c4 (G i , G j ) \u2264 1 \u03bb + \u2206 \u2264 max \u03c4 0 D E \u03c4 0 (G i , G j ) , 1", "formula_coordinates": [12.0, 317.75, 309.25, 250.35, 68.97]}, {"formula_id": "formula_36", "formula_text": "D E \u03c4 (G i , G j ) \u2264 1 \u03c4 max \u03c4 0 |D E \u03c4 0 (G i , G j )|, 1 \u03bb + \u2206 .", "formula_coordinates": [12.0, 353.58, 394.97, 168.76, 18.4]}, {"formula_id": "formula_37", "formula_text": "D E \u03c4 = 2 m(m \u2212 1) m i=0 m j=i+1 D E \u03c4 (G i , G j ) \u2264 2 m(m \u2212 1) m i=0 m j=i+1 1 \u03c4 max \u03c4 0 |D E \u03c4 0 (G i , G j )|, 1 \u03bb + \u2206 \u2264 1 \u03c4 2 m(m \u2212 1) m i=0 m j=i+1 max \u03c4 0 D E \u03c4 0 (G i , G j ) , 1 \u03bb + \u2206 (since \u03bb, \u2206, \u03c4 > 0) \u25a1", "formula_coordinates": [12.0, 332.4, 439.1, 225.8, 115.51]}], "doi": "10.1145/3397271.3401100"}