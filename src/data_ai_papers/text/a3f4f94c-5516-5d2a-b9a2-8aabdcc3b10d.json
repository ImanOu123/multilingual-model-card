{"title": "A Balanced Data Approach for Evaluating Cross-Lingual Transfer: Mapping the Linguistic Blood Bank", "authors": "Dan Malkin; Tomasz Limisiewicz; Gabriel Stanovsky", "pub_date": "", "abstract": "We show that the choice of pretraining languages affects downstream cross-lingual transfer for BERT-based models. We inspect zeroshot performance in balanced data conditions to mitigate data size confounds, classifying pretraining languages that improve downstream performance as donors, and languages that are improved in zero-shot performance as recipients. We develop a method of quadratic time complexity in the number of languages to estimate these relations, instead of an exponential exhaustive computation of all possible combinations. We find that our method is effective on a diverse set of languages spanning different linguistic features and two downstream tasks. Our findings can inform developers of largescale multilingual language models in choosing better pretraining configurations. 1 * Work done while visiting the Hebrew University.", "sections": [{"heading": "Introduction", "text": "Pretrained language models are setting state-of-theart results by leveraging raw texts during pretraining (PLMs; Peters et al., 2018;Devlin et al., 2019, inter alia). Interestingly, when pretraining on multilingual corpora, PLMs seem to exhibit zero-shot cross-lingual abilities, achieving non-trivial performance on downstream examples in languages seen only during pretraining. For example, in the bottom of Figure 1, a named entity recognition model finetuned on Russian is capable of predicting correctly name entity tags for texts in English, seen only during pretraining (Pires et al., 2019;Conneau et al., 2020b;K et al., 2020;Conneau et al., 2020a;Lazar et al., 2021;Turc et al., 2021).\nPrevious analyses examined how several factors contribute to this emerging behavior. For example, parameter sharing and model depth are important in certain configurations (K et al., 2020;Conneau et al., 2020b), as well as typological similarities Figure 1: We build a complete, directed graph over a diverse set of 22 languages. Weighted edges show the improvement of bilingual LM over monolingual performance (bold edges represent larger weights). Languages which consistently improve performance are termed \"donors\" and marked in red, while languages which benefit most are termed \"recipients\" (marked in blue). We show that our observations hold in several configurations on two downstream tasks. between languages (Pires et al., 2019), and the choice of specific finetune languages (Turc et al., 2021).\nIn this work, we focus on an important factor that we find missing in prior work, namely the effect that pretraining languages have on downstream zero-shot performance. In particular, we ask three major research questions: (1) Does the choice of pretraining languages affect downstream crosslingual transfer, and if so, to what extent? (2) Is English the optimal pretraining language, when controlling for confounding factors such as data size and domain? And finally, (3) Can we choose pretraining languages to improve downstream zeroshot performance?\nIn addressing these research questions, we aim to decouple the language from its corresponding dataset. To the best of our knowledge, prior work has conflated pretrain corpus size and its domain with other examined factors, thus skewing results towards over-represented languages, such as English or German (Joshi et al., 2020). 2 To achieve this, we first construct a linguistically-balanced pretraining corpus based on Wikipedia, composed of a diverse set of 22 languages. We carefully control for the amount of data and domain distribution in each of the languages (Section 3).\nNext, since the number of pretraining configurations grows exponentially with the number of languages n represented in the dataset, it is infeasible to exhaustively test all possible configurations, much less extend it for more languages. 3 In Section 4 we propose a novel pretraining-based approach that is quadratic in the number of languages. This is achieved by training all n 2 combinations of bilingual masked language models over our corpus, thus yielding a complete directed graph (Figure 1), where an edge l 1 \u2192 l 2 estimates how much a language l 1 contributes to zero-shot performance in language l 2 , based only on language modeling performance.\nIn Section 5, we use the graph to identify languages which generally contribute as pretraining languages (termed \"donors\"), and languages which often benefit from training with other languages (termed \"recipients\"). Further, we use the graph to make observations regarding the effect of typological features on bilingual language modeling, and make available an interactive graph explorer.\nFinally, our evaluations on two multilingual downstream tasks (part of speech tagging and named entity recognition) lead to three main conclusions (Section 6): (1) the choice of pretraining languages indeed leads to differences in zero-shot performance; (2) controlling for the amount of data allotted for each language during pretraining ques-tions the primacy of English as the main pretraining language; and (3) our hypotheses regarding donors and recipient language hold in both downstream tasks, and against two additional control groups.\n2 Metrics for Pretraining-Aware Cross-Lingual Transfer\nIn this section, we extend existing metrics for zeroshot cross-lingual transfer to account for pretraining languages. Intuitively, our metrics for a model M and a given downstream task take into account three factors: (1) P , the set of languages seen during pretraining, (2) s \u2208 P , the source language used for finetuning, and (3) t \u2208 P , the target language, seen during inference.\nFormally, we adapt the formulation of Hu et al. (2020) to define a pretraining-aware bilingual zeroshot transfer score Z as: 4\nZ P (s \u2192 t) := \u03b5(M P,s , t)(1)\nWhere M P,l is a model pretrained on the set of languages P and finetuned on downstream task instances in the language l \u2208 P , and \u03b5(M, l) is an evaluation of model M on instances in language l in terms of the downstream metric, e.g., word-label accuracy for part of speech tagging.\nFollowing, we extend the definition of zero-shot transfer score to a set of downstream test languages D \u2286 P to measure P 's aggregated effect on zeroshot performance, by averaging over all bilingual transfer combinations in D:\nZ P (D) = 1 |D| 2 \u2212 |D| \u2022 l 1 ,l 2 \u2208D l 1 \u0338 =l 2 Z P (l 1 \u2192 l 2 ) (2)\nIn the following sections, we will use these metrics to evaluate how different choices for pretraining languages influence downstream performance.", "publication_ref": ["b15", "b16", "b3", "b9", "b2", "b10", "b18", "b9", "b3", "b16", "b18", "b8", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Data Selection", "text": "We collect a pretraining dataset to test the effect of pretraining languages on cross-lingual transfer.\nFirst, we choose a set of 22 languages from 9 language families, as listed in Table 1. These represent a wide variety of scripts, as well as typological and morphological features. We note that our approach can be readily extended to other languages beyond those selected in this study.\nSecond, we aim to balance the amount of data and control for its domain across languages, to mitigate possible confounders in our evaluations. Below we outline design choices we make toward this goal.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data Balancing", "text": "To achieve a balanced dataset across our languages, we sample consecutive sentences from every language's Wikipedia dump from November 2021, such that each language is represented by 10 million characters. 5 This amount was chosen to align all languages to the lower-resource ones (e.g., Piedmontese or Irish) which comprise approximately of 10mb. We choose to sample texts from Wikipedia as it consists of roughly similar encyclopedic domain across languages, and is widely used for training PLMs (Devlin et al., 2019).\nCan we balance the amount of information across languages? We note that a possible confound in our study is that languages may encode different amounts of information in texts of similar character count. This may happen due to differences in the underlying texts or in inherent language properties. 6 To estimate the amount of information in each of our 10 7 character partitions, we tokenize each language partition l with the same word-piece tokenizer, and look at the ratio between the total number of tokens in l and the number of unique tokens in l, finding a good correlation across all our languages (r = 0.73), which may indicate that our dataset is indeed balanced in terms of information. Our intuition is that an imbalanced amount of information would lead the tokenizer to \"invest\" more tokens in some of the languages while neglecting the less informative ones.\nIs our sample representative of the full Wikipedia corpus in each language? Another concern may be that our sampled corpus per language is not indicative of the full corpus for that language, which may be much larger (see Table 1). To test this, we create three discrete length distributions. Two length distributions for sentences (in 5 Wikipedia dump was obtained and cleaned using wikiextractor (Attardi, 2015). 6 For example logographic or abjad writing systems may be more condensed than other scripts (Perfetti and Liu, 2005).", "publication_ref": ["b4", "b0", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Language", "text": "Code Table 1: The size of the full Wikipedia dump for the languages in our study (in millions of characters) versus our fixed sized sampling of it. This exemplifies both the linguistic diversity as well as the variance in data sizes in the original Wikipedia corpus, often used for pretraining PLMs. In contrast, we create a balanced pretraining dataset by sampling 10M characters from all languages such that they conform to the smallest language portion in our set (Piedmontese).\nterms of words and tokens), and word length distribution in terms of characters. We then compare those three distributions between our sample and the full data using Earth Movers Distance. All means and standard deviations score below 0.001, indicating that indeed all samples are similarly distributed to their respective full corpus in terms of these metrics.\nFigure 2: Bilingual finetune scores between language pairs in our balanced corpus. Coordinate (i, j) represents F(l i \u2192 l j ), i.e., the performance in MRR[%] (which correlates with perplexity) of an LM pretrained on a bilingual corpus over languages (l i , l j ) and tested intrinsically on l j . The last column (marked Don.) sums over each line, i.e., index i in the column represents how much language i donated to all other languages. Similarly, the j'th index in the last row (marked Recp.) sums over column j and represents how much language l j improved in all configurations.\nwe use the graph to formulate a set of downstream cross-lingual hypotheses regarding how different languages will affect zero-shot performance, and validate these hypotheses on two downstream tasks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup", "text": "For all evaluations discussed below, we train a BERT model (Devlin et al., 2019) with 4 layers and 4 attention heads, an MLM task head, and an embedding dimension of 512. 7 We train a single wordpiece tokenizer (Wu et al., 2016) on our entire dataset. 8 We train the models with a batch size of 8 samples, with sentences truncated to 128 tokens. Each language model was trained up to 4 epochs. This was determined by examining the training loss on 6 diverse languages in our set and observing that they converge around 4 epochs. A subset 7 We use the implementation provided by Hugging Face: https://huggingface.co/ bert-base-uncased.\n8 To allow future exploration, we also tokenize over 22 additional languages (listed in the Appendix) which are sampled in the same manner but are not included in this study.\nof 6 languages was trained on 4 additional seeds to verify the stability of the results, as seen in Table 5 and Table 6 in the Appendix. Masks were applied with default settings, generating 15% mask tokens and 10% random tokens for each input sequence (Devlin et al., 2019). We used a single GPU core (nvidia tesla M60, gtx 980, and RTX 2080Ti). Training time varied between 80 -120 minutes.", "publication_ref": ["b4", "b20", "b4"], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "Building a Pretraining Language Graph", "text": "Intuitively, we measure MLM performance when pretraining on a pair of languages (l 1 , l 2 ) as a proxy to the extent of how l 1 and l 2 contribute to one another in zero-shot cross-lingual transfer.\nThis methodology relies on two assumptions. First, we assume that the cross-lingual zero-shot performance as defined in Equation 2 is monotonic, i.e., that adding pretraining languages will improve the average downstream performance. This is defined formally as:\nP \u2032 \u2286 P \u21d2 Z P \u2032 (D) \u2264 Z P (D)(3)\nFollowing this assumption will allow us to extend our bilingual observations to a pretraining language set P of arbitrary size.\nSecond, we assume that MLM performance correlates with downstream task performance, which is often the assumption made when training PLMs to minimize perplexity (Peters et al., 2018;Devlin et al., 2019).\nBilingual MLM finetune score. Formally, for every language pair s, t \u2208 P , we compute the following finetune score, F:\nF(s \u2192 t) := \u03b5(M s,t , t) \u2212 \u03b5(M t , t) \u03b5(M t , t)(4)\nWhere M s,t is a model pretrained on s, t, and \u03b5 is an intrinsic evaluation metric for MLM. 9 I.e., F(s, t) estimates how much the target language t \"gains\" in the MLM task from additional pretraining on the source language s compared to monolingual pretraining on t.\nFigure 2 depicts a weighted adjacency matrix where coordinate (i, j) corresponds to F(l i \u2192 l j ). As shown in Figure 1, the same information can be conveyed in a complete directed weighted graph, where each node represents a language, and edges (l 1 , l 2 ) are weighted by F(l 1 \u2192 l 2 ).", "publication_ref": ["b15", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Language-Level donation and recipience.", "text": "Next, for each language l \u2208 P we compute a Donation score, D, as an aggregate over all of its finetune scores as a source language (i.e., how much it contributed to other languages), and similarly a recipience score, R, by aggregating over all its finetune scores as a target language, to measure how much l is contributed to by other languages. Formally:\nD(l) := t\u2208P t\u0338 =l F(l \u2192 t) (5) R(l) := s\u2208P s\u0338 =l F(s \u2192 l)(6)\nWe depict both donation and recipience scores as aggregate row and column vectors in Figure 2.\nThus, based on the two assumptions above, our hypothesis is that the downstream cross-lingual transfer will be proportional to the sum of recipience scores for all pretraining languages. Formally: 9 We specifically use mean reciprocal rank (MRR), which correlates with perplexity. Our languages on a \"donor\" versus \"recipient\" axes. A positive coordinate on the \"donor\" score (X axis) represents a language that on average improved other languages' performance in bilingual pretraining, while a negative score indicates a language which hurts other languages on average. Inversely, a positive score on the Y axis represents languages whose performance was improved by bilingual pretraining, while negative scores represent languages whose performance was hurt by it. The II quadrant represents O type languages (donating but not receiving), languages on the IV 's quadrant are AB+ type languages (receiving but not donating)\nZ P (D) \u221d l\u2208D R(l)(7)\nMoreover, higher donation scores for languages in the pretrain set will result in higher scores in the downstream task. Formally:\nl\u2208P D(l) \u2264 l\u2208P \u2032 D(l) \u21d2 Z P (D) \u2264 Z P \u2032 (D) (8)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Pretraining Graph Analysis", "text": "We present several key observations based on the bilingual pretraining graph described in the previous section and summarized by the adjacency matrix in Figure 2, as well as an interactive exploration interface. In the following sections, we use these observations in our downstream evaluations.\nSome language combinations are detrimental.\nNegative finetune scores are present in some of the target languages, e.g., between Korean (ko) and Arabic (ar), which means that initializing a language model for Arabic with weights learned for Korean hurts MLM performance on Arabic, compared to an Arabic monolingual baseline. I.e., in these language configurations, initializing the model with another language model's weights leads to worse performance than random initialization.\n\u03b5(M l1 , l 1 ) F (l 1 \u2192 l 2 ) \u03c1 = 0.21 \u03b5(M l2 , l 2 ) \u03c1 = -0.48\nBilingual MLM relations are not symmetric.\nIn fact, we observe a moderate negative correlation between F(l 1 \u2192 l 2 ) and F(l 2 \u2192 l 1 ), as shown in Figure 3. For example, for German and Finnish we get 0.51 = F(f i \u2192 de) > F(de \u2192 f i) = \u22120.24. I.e., Finnish initialization improves German MLM, while the inverse is detrimental for Finnish.\nMonolingual performance correlates with donation score. Perhaps expectedly, relatively worseperforming models benefit most from the bilingual transfer, while better-performing monolingual models tend to be better donors, although to a lesser extent (Figure 4). 10\nDifferent script leads to larger variance in bilingual finetuning. However, language family does not affect it. We find that fine-tuning between languages with different scripts is a high-risk, highreward scenario. The highest transfer scores occur in this setting, but the proportion of negative scores is also higher. A shared script is a safe setting with a high proportion of neutral or positive donations (Figure 5a). In contrast with recent findings (Pires et al., 2019), we did not observe a 10 Correlations are statistically significant (p < 0.05 based on Student's T-test). 11 We motivate our choice of bins in Appendix. statistically significant influence for the language family (Figure 5b).\nFinetuning as transfusion: mapping the linguistic blood-bank. The non-symmetric nature of the scores gives rise to a coarse-grained ontology loosely reminiscent of human blood types, depicted in Figure 3. Languages which on average donate but do not receive (D(l) > 0 and R(l) < 0) are denoted O type languages, while the inverse (receiving but not donating) are denoted as AB+ type.", "publication_ref": ["b16"], "figure_ref": ["fig_1", "fig_2", "fig_2"], "table_ref": []}, {"heading": "Interactive Exploration", "text": "To allow further exploration of our bilingual pretraining graph, we develop a publicly available web-based interactive exploration interface. 12 We enable exploration of interactions between different linguistic features, based on The World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013), allowing users to filter and focus on specific traits and analyze how they affect bilingual pretraining.", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}, {"heading": "Downstream Zero-Shot Performance", "text": "In this section, we validate our method for estimating the effect of pretraining language combinations on downstream performance. Towards that end, in Section 6.1, we construct several pretraining configurations, based on pretraining observations. Then, in Section 6.2 we describe the multilingual datasets we use for two downstream tasks. Finally, our results are presented in Section 6.3, showing the influence of pretraining configuration on downstream performance.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Choosing Pretraining Sets", "text": "We use the donation scores to identify pretraining languages projected to lead to better downstream zero-shot performance, and the recipience score to find downstream languages which will perform better languages as source (finetune) languages.\nOur setup is summarized in Table 2.\nDonating languages. We define three sets of languages for pretraining, using the donation score while keeping the sets linguistically diverse: (1) Most Donating: Japanese, Telugu, Finnish, and Russian;\n(2) Least Donating: Nepali, Burmese, Armenian, and English. We also include Englishs as it is a popular source language; and (3) Random:A randomly selected set of 4 languages: Hebrew, Irish, French and Swedish.\nRecipient languages. To validate that lower recipience scores indeed indicate that languages are less likely to improve via cross lingual transfer, we added 6 languages to all configurations described above: 3 Most Recipient languages (R h ): Hindi, German, and Hungarian, and 3 Least Recipient languages (R l ): Arabic, Greek, and Tamil. Finally, we add a fourth control configuration which was pretrained only on C := R h \u222a R l .\nHypotheses. We hypothesize that the more donating pretraining sets will improve cross-lingual transfer in downstream tasks, and that more recipient languages will have better cross-lingual performance compared to least recipient languages. These can be formally articulated using Equations 9 and 10:\n\u2200P : Z P (R h ) > Z P (R l )(9)\nZ M ostDon. (C) > Z Random (C) > Z LeastDon. (C)(10)", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Tasks", "text": "We evaluated all pretraining configurations detailed in Table 2 on two of XTREME's tasks: part of speech tagging (POS) and named entity recognition (NER). Both of which commonly appear in NLP pipelines such as CoreNLP (Manning et al., 2014) and spaCy (Honnibal and Montani, 2017). We aim to balance the data in both tasks across different finetune languages, so as not to skew results towards higher-resource languages.\nFor part-of-speech tagging, XTREME borrows from universal dependencies (Nivre et al., 2020). Since XTREME is imbalanced across languages, we truncated the data to 1000 sentences to fit the lower-resource languages, e.g., XTREME annotates POS in 909 sentences in Hungarian. For NER, we applied a similar procedure, where XTREME's data was taken from the Wikiann (panx) dataset (Rahimi et al., 2019) which we truncated to 5000 sentences (the data size available for Hindi NER in XTREME).\nExperimental setup. We use the code and default hyperparameter default values provided by XTREME to train the downstream tasks (Hu et al., 2020), adapted for multilingual training.", "publication_ref": ["b12", "b6", "b13", "b17", "b7"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Results", "text": "Several key observations can be made based on the results for both POS tagging and NER across all training configurations, which are presented in Tables 3 and 4   Table 4: Recipience results for named entity recognition (NER) and part of speech tagging (POS) as mean and standard deviation over five random seeds. We report results across different training configurations for two groups of downstream recipient languages. In accordance with our pretraining results, the Most Recipient set does better than the Least recipient set across both tasks in zero-shot and monolingual performance.\ncalculated zero-shot transfer scores on C, using Z P (C) defined by Equation 2. Monolingual results under each pretrain set P were calculated by the average F 1 performance of each language in C:\n1 |C| \u2022 l\u2208C \u03b5(M P,l , l)(11)\nWhere \u03b5(M P,l , l) denotes the F 1 score of a model pretrained on P , finetuned on l and evaluated on l.\nPretraining configuration affects downstream cross-lingual transfer. In both tasks, we observe a variance in results when changing the pretraining configuration, despite all of them having similar amounts of data. This may imply that previous work has omitted an important interfering factor.\nRecipience score correlates with downstream cross-lingual performance. We evaluated zeroshot transfer for each language set R \u2208 {R l , R h } as the average zero-shot transfer scores over all pretraining configurations. Multilingual pretraining can improve monolingual performance. As seen in Table 3, the Most Donating pretraining configuration achieved a monolingual score which is slightly higher than the control group, while the Least Donating configuration underperforms all other sets. This suggests that multilingual pretraining datasets can benefit monolingual downstream results compared to more data in a single language.\nEnglish might not be an optimal pretraining language. Corresponding with our previous results, if donation score is indicative of a language's contribution in pretraining, English's relative low donation score might indicate that it is not the best language to pretrain upon. English was also part of the Least Donating pretraining configuration which scored lower than Most Donating as seen in Table 3. Further research can ascertain this finding.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3", "tab_4", "tab_3", "tab_3"]}, {"heading": "Limitations and Future Work", "text": "As with other works on cross-lingual transfer, our results are influenced by many hyperparameters. Below we explicitly define our design choices and how they can be explored in future work. First, data scarcity in low-resource languages restricted us to small data amounts. Although our experiments showed a non-trivial signal for pretraining and downstream tasks, future work may apply our framework to larger data sizes.\nSecond, for efficiency's sake, we trained relatively small models to enable us to train a large number of language configurations, while ensuring convergence in 6 languages. Furthermore, we did not do any hyper-parameter tuning and used only values reported in previous work, and use only the BERT architecture. Future work may revisit any of these design choices to shed more light on their effect.\nThird, similarly to other works, our data was scraped from Wikipedia, and we did not account for language contamination across supposedly monolingual corpora (e.g., due to code switching). Such contamination may confound with cross-lingual transfer, as was recently shown by Blevins and Zettlemoyer (2022).\nFinally, our downstream analysis focused on POS tagging and NER since they were available for many languages and are common in many NLP pipelines. Further experimentation can test if our results hold for more NLP tasks.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "To the best of our knowledge, this is the first work to control for the amount of data allocated for each language during pretraining and finetuning while evaluating on many languages.\nPerhaps most related to our work, Turc et al. (2021) challenge the primacy of English as a source language for cross-lingual transfer in various downstream tasks. Their work shows that German and Russian are often more effective sources. In all of their experiments, they use mBERT's imbalanced pretraining corpus. Blevins and Zettlemoyer (2022) complement this hypothesis by showing that English pretraining data actually contains a significant amount of non-English text, which correlates with the model's transfer capabilities.\nWu and Dredze (2020) evaluate how mBERT performs on a wide set of languages, focusing on the quality of representation for low-resource languages in various downstream tasks by defining a scale from low to high resource. They show that mBERT underperforms non BERT monolingual baselines for low resource languages while performing well for high resource ones.\nWhile Pires et al. (2019); Limisiewicz and Mare\u010dek (2021) show that typology plays a significant role for mBERT's multilingual performance, this is not replicated in our balanced evaluation, and has lesser impact in Wu et al. (2022) as well.\nFinally, Conneau et al. (2020a) introduce the transfer-interference trade-off where low resource languages benefit from multilingual training, up to a point where the overall performance on monolingual and cross-lingual benchmarks degrades.", "publication_ref": ["b18", "b1", "b16", "b11", "b21", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusions", "text": "We explored the effect of pretraining language selection on downstream zero-shot transfer.\nWe first choose a diverse pretraining set of 22 languages, and curate a pretraining corpus which is balanced across these languages.\nSecond, we devise an estimation technique, quadratic in the number of languages, projecting which pretraining languages will serve better in cross-lingual transfer and which specific downstream languages will do best in that setting.\nFinally, we test our hypothesis on two downstream multilignual tasks, and show that the choice of pretraining languages indeed leads to varying downstream cross-lingual results, and that our method is a good estimation for downstream performance. Taken together, our results suggest that pretraining language selection should be a factor in estimating cross-lingual transfer, and that current practices which focus on high-resource languages may be sub-optimal. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Appendix", "text": "Full list of tokenized languages The full list of Wikipedia language codes for languages used in our tokenizer training is:\n\u2022 pms, ga, ne, cy, fi, hy, my, hi, te, ta, ko, el, hu, he, zh, ar, sv, ja, fr, de, ru, en -languages that are also evaluated and trained. Elaborated in Table 1.\n\u2022 af, am, ca, cs, da, es, id, is, it, mg, nl, pl, sk, sw, th, tr, ur, vi, yi -Additional languages corresponding to Afrikaans, Amharic, Catalan, Czech, Danish, Spanish, Indonesian, Icelandic, Italian, Malagasy, Dutch, Polish, Slovak, Swahili, Thai, Turkish, Urdu, Vietnamese, and Yiddish.\nTransfer Distribution In the histogram of crosslingual transfers (Figure 7), we observe that the distribution has multiple local maximums (modes).\nWe distinguish four main level of cross-lingual transfer described in Section 4.2 (F(l i \u2192 l j )):\n\u2022 negative transfer F(l i \u2192 l j ) < \u221210\n\u2022 neutral transfer \u221210 \u2264 F(l i \u2192 l j ) < 10\n\u2022 positive transfer 10 \u2264 F(l i \u2192 l j ) < 55\n\u2022 very positive transfer 55 \u2264 F(l i \u2192 l j )\nThe choice of division borders was done in order to separate distinct modes of the distribution and to obtain interpretable bins (e.g. neutral transfer centered around zero).      3 where O type languages are marked in red and AB+ type languages languages are marked in blue. Monolingual performance explains some of the pretraining contribution, namely recipient languages appear near the low end of the spectrum while donors appear towards the end.", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Acknowledgements", "text": "We would like to thank Roy Schwartz for his helpful comments and suggestions and the anonymous reviewers for their valuable feedback. This work was supported in part by a research gift from the Allen Institute for AI. Tomasz Limisiewicz's visit to the Hebrew University has been supported by grant 338521 of the Charles University Grant Agency and the Mobility Fund of Charles University.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "", "journal": "", "year": "2015", "authors": "Giusepppe Attardi"}, {"ref_id": "b1", "title": "Language contamination explains the cross-lingual capabilities of english pretrained models", "journal": "", "year": "2022", "authors": "Terra Blevins; Luke Zettlemoyer"}, {"ref_id": "b2", "title": "Unsupervised cross-lingual representation learning at scale", "journal": "", "year": "2020", "authors": "Alexis Conneau; Kartikay Khandelwal; Naman Goyal; Vishrav Chaudhary; Guillaume Wenzek; Francisco Guzm\u00e1n; Edouard Grave; Myle Ott; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b3", "title": "Emerging cross-lingual structure in pretrained language models", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Alexis Conneau; Shijie Wu; Haoran Li; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b4", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b5", "title": "", "journal": "", "year": "2013", "authors": "Matthew S Dryer; Martin Haspelmath"}, {"ref_id": "b6", "title": "spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing", "journal": "", "year": "2017", "authors": "Matthew Honnibal; Ines Montani"}, {"ref_id": "b7", "title": "XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation", "journal": "PMLR", "year": "2020-07", "authors": "Junjie Hu; Sebastian Ruder; Aditya Siddhant; Graham Neubig; Orhan Firat; Melvin Johnson"}, {"ref_id": "b8", "title": "The state and fate of linguistic diversity and inclusion in the NLP world", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Pratik Joshi; Sebastin Santy; Amar Budhiraja; Kalika Bali; Monojit Choudhury"}, {"ref_id": "b9", "title": "Cross-lingual ability of multilingual BERT: an empirical study", "journal": "", "year": "2020-04-26", "authors": "K Karthikeyan; Zihan Wang; Stephen Mayhew; Dan Roth"}, {"ref_id": "b10", "title": "Filling the gaps in Ancient Akkadian texts: A masked language modelling approach", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Koren Lazar; Benny Saret; Asaf Yehudai; Wayne Horowitz; Nathan Wasserman; Gabriel Stanovsky"}, {"ref_id": "b11", "title": "Examining cross-lingual contextual embeddings with orthogonal structural probes", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Tomasz Limisiewicz; David Mare\u010dek"}, {"ref_id": "b12", "title": "The Stanford CoreNLP natural language processing toolkit", "journal": "", "year": "2014", "authors": "Christopher Manning; Mihai Surdeanu; John Bauer; Jenny Finkel; Steven Bethard; David Mcclosky"}, {"ref_id": "b13", "title": "Universal Dependencies v2: An evergrowing multilingual treebank collection", "journal": "", "year": "2020", "authors": "Joakim Nivre; Marie-Catherine De Marneffe; Filip Ginter; Jan Haji\u010d; Christopher D Manning; Sampo Pyysalo; Sebastian Schuster; Francis Tyers; Daniel Zeman"}, {"ref_id": "b14", "title": "Orthography to phonology and meaning: Comparisons across and within writing systems", "journal": "Reading and Writing", "year": "2005", "authors": "A Charles; Ying Perfetti;  Liu"}, {"ref_id": "b15", "title": "Deep contextualized word representations", "journal": "Long Papers", "year": "2018", "authors": "Matthew E Peters; Mark Neumann; Mohit Iyyer; Matt Gardner; Christopher Clark; Kenton Lee; Luke Zettlemoyer"}, {"ref_id": "b16", "title": "How multilingual is multilingual BERT?", "journal": "", "year": "2019", "authors": "Telmo Pires; Eva Schlinger; Dan Garrette"}, {"ref_id": "b17", "title": "Massively multilingual transfer for NER", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Afshin Rahimi; Yuan Li; Trevor Cohn"}, {"ref_id": "b18", "title": "Revisiting the primacy of english in zero-shot cross-lingual transfer", "journal": "", "year": "2021", "authors": "Iulia Turc; Kenton Lee; Jacob Eisenstein; Ming-Wei Chang; Kristina Toutanova"}, {"ref_id": "b19", "title": "Are all languages created equal in multilingual BERT?", "journal": "", "year": "2020", "authors": "Shijie Wu; Mark Dredze"}, {"ref_id": "b20", "title": "Google's neural machine translation system: Bridging the gap between human and machine translation", "journal": "", "year": "2016", "authors": "Yonghui Wu; Mike Schuster; Zhifeng Chen; V Quoc; Mohammad Le; Wolfgang Norouzi; Maxim Macherey; Yuan Krikun; Qin Cao; Klaus Gao;  Macherey"}, {"ref_id": "b21", "title": "Oolong: Investigating what makes crosslingual transfer hard with controlled studies", "journal": "ArXiv preprint", "year": "2022", "authors": "Zhengxuan Wu; Isabel Papadimitriou; Alex Tamkin"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure3: Our languages on a \"donor\" versus \"recipient\" axes. A positive coordinate on the \"donor\" score (X axis) represents a language that on average improved other languages' performance in bilingual pretraining, while a negative score indicates a language which hurts other languages on average. Inversely, a positive score on the Y axis represents languages whose performance was improved by bilingual pretraining, while negative scores represent languages whose performance was hurt by it. The II quadrant represents O type languages (donating but not receiving), languages on the IV 's quadrant are AB+ type languages (receiving but not donating)", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 4 :4Figure4: Scatter-plot. Y-axis represents cross-lingual transfer F(l 1 \u2192 l 2 ) for a each possible pair of languages, while the x-axis represents the monolingual MRR score for a source language (left) and the target language (right).", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 5 :5Figure 5: We divide language pairs into four bins by bilingual finetune score (F(l 1 \u2192 l 2 )). 11 The figures present the percentage of pairs assigned to each bin for samples of language pairs: (a) written in the same or different script; (b) belonging to the same or different language family. Sharing the language family has no significant effect on the transfer score (p > 0.05), while the effect of sharing scripts is significant (p < 0.05) (p-values based on Pearson's \u03c7 2 test).", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 6 :6Figure 6: Our visualization tool, based on Streamlit (https://streamlit.io)", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 7 :7Figure 7: Histogram of cross-lingual transfers F(l i \u2192 l j ). Horizontal lines (at \u221210, 10, and 55) are the borders between four transfer levels.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": ". For each configuration P in Most Donating, Least Donating, Random, Control we", "figure_data": "Base Pretrain Set (Donors)Shared Pretrain Set Most Recipient (Rh) Least Recipient (Rl)Total DataSummaryMost Donating Least Donating {ne, my, hy, en} {ja, te, fi, ru} Random {he, ga, fr, sv} Control {}+{hi, de, hu} {hi, de, hu} {hi, de, hu} {hi, de, hu}+{ar, el, ta} {ar, el, ta} {ar, el, ta} {ar, el, ta}10 8 characters Most donating pretraining set. 10 8 characters Least donating pretrain set. 10 8 characters Random donating pretrain set. 10 8 characters No additional donating languages."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Four pretraining language configurations. Each consists of donating languages (first column) and recipient languages (second column). The control group has the same amount of data, equally distributed among its languages.", "figure_data": "NER [%F1] Avg. Monolingual Avg. Zeroshot Avg. Monolingual Avg. Zeroshot POS [%F1]Most Donating Random Least Donating49.3\u00b1.4 49.2\u00b1.3 48.8\u00b1.215.6\u00b1.4 15.6\u00b1.1 14.8\u00b1.361.4\u00b1.1 61.3\u00b1.1 60.9\u00b1.228.1\u00b1.3 26.9\u00b1.3 26.9\u00b1.6Control49.0\u00b1.215.6\u00b1.261.9\u00b1.127.4\u00b1.3"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Donation results for named entity recognition (NER) and part of speech tagging (POS) as mean and standard deviation over five random seeds. For each pretraining language group (Most Donating, Random, Least Donating, and Control), we report corresponding average monolingual and zero shot performance. Most Donating consistently outperforms Least Donating in both tasks, and in both monolingual and zeroshot performance. Most Donating is on par with Control in monolingual performance in NER, despite having less in-domain data.", "figure_data": "NER [%F1] Avg. Monolingual Avg. Zeroshot Avg. Monolingual Avg. Zeroshot POS [%F1]Most Recipient (R h ) Least Recipient (R l )50.3\u00b1.6 47.9\u00b1.418.4\u00b1.6 12.4\u00b1.464.1\u00b1.3 58.6\u00b1.428.7\u00b1.7 26.0\u00b1.7"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": "reveals that theMost Recipient set outperforms the Least Recipient"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Averaged MRR scores for five seeds. Bilingual training was done with five seeds over a group of six diverse languages to verify the results are stable. The table shows mean results. The column indicates the source languages, the row indicates the target languages.", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "standard deviations for MRR scores over five seeds. Bilingual training was done with five seeds over a group of six diverse languages to verify the results are stable. The table shows the standard deviation of the results. The column indicates the source languages, the row indicates the target languages.", "figure_data": "my 23.3ne 24.2de 24.7hi 25.0en 25.9hu 27.2hy 28.3ar 32.1he 33.4ru 34.3zh 36.3ta 36.4ko 36.5ga 36.7ja 37.6fi 38.0cy 39.0te 40.0el 41.0fr 41.4pms 58.9"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Monolingual results (MRR scores) for all 22 languages in our study, ordered from low to high. Colors coding follows Figure", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "Z P (s \u2192 t) := \u03b5(M P,s , t)(1)", "formula_coordinates": [2.0, 358.51, 349.77, 165.91, 20.96]}, {"formula_id": "formula_1", "formula_text": "Z P (D) = 1 |D| 2 \u2212 |D| \u2022 l 1 ,l 2 \u2208D l 1 \u0338 =l 2 Z P (l 1 \u2192 l 2 ) (2)", "formula_coordinates": [2.0, 312.33, 542.75, 212.09, 44.69]}, {"formula_id": "formula_2", "formula_text": "P \u2032 \u2286 P \u21d2 Z P \u2032 (D) \u2264 Z P (D)(3)", "formula_coordinates": [4.0, 342.16, 760.33, 182.26, 21.19]}, {"formula_id": "formula_3", "formula_text": "F(s \u2192 t) := \u03b5(M s,t , t) \u2212 \u03b5(M t , t) \u03b5(M t , t)(4)", "formula_coordinates": [5.0, 100.8, 238.69, 188.33, 27.79]}, {"formula_id": "formula_4", "formula_text": "D(l) := t\u2208P t\u0338 =l F(l \u2192 t) (5) R(l) := s\u2208P s\u0338 =l F(s \u2192 l)(6)", "formula_coordinates": [5.0, 130.16, 582.28, 158.97, 70.91]}, {"formula_id": "formula_5", "formula_text": "Z P (D) \u221d l\u2208D R(l)(7)", "formula_coordinates": [5.0, 372.24, 489.36, 152.19, 25.43]}, {"formula_id": "formula_6", "formula_text": "l\u2208P D(l) \u2264 l\u2208P \u2032 D(l) \u21d2 Z P (D) \u2264 Z P \u2032 (D) (8)", "formula_coordinates": [5.0, 313.13, 573.42, 211.29, 26.89]}, {"formula_id": "formula_7", "formula_text": "\u03b5(M l1 , l 1 ) F (l 1 \u2192 l 2 ) \u03c1 = 0.21 \u03b5(M l2 , l 2 ) \u03c1 = -0.48", "formula_coordinates": [6.0, 79.49, 96.6, 424.93, 147.45]}, {"formula_id": "formula_8", "formula_text": "\u2200P : Z P (R h ) > Z P (R l )(9)", "formula_coordinates": [7.0, 360.21, 243.79, 164.22, 20.55]}, {"formula_id": "formula_9", "formula_text": "Z M ostDon. (C) > Z Random (C) > Z LeastDon. (C)(10)", "formula_coordinates": [7.0, 307.16, 291.77, 217.26, 25.07]}, {"formula_id": "formula_10", "formula_text": "1 |C| \u2022 l\u2208C \u03b5(M P,l , l)(11)", "formula_coordinates": [8.0, 137.58, 530.46, 151.56, 33.52]}], "doi": "10.18653/v1/2020.acl-main.747"}