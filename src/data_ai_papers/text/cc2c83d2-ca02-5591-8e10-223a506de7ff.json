{"title": "KinyaBERT: a Morphology-aware Kinyarwanda Language Model", "authors": "Antoine Nzeyimana; Andre Niyongabo Rubungo", "pub_date": "", "abstract": "Pre-trained language models such as BERT have been successful at tackling many natural language processing tasks. However, the unsupervised sub-word tokenization methods commonly used in these models (e.g., byte-pair encoding -BPE) are sub-optimal at handling morphologically rich languages. Even given a morphological analyzer, naive sequencing of morphemes into a standard BERT architecture is inefficient at capturing morphological compositionality and expressing word-relative syntactic regularities. We address these challenges by proposing a simple yet effective twotier BERT architecture that leverages a morphological analyzer and explicitly represents morphological compositionality. Despite the success of BERT, most of its evaluations have been conducted on high-resource languages, obscuring its applicability on low-resource languages. We evaluate our proposed method on the low-resource morphologically rich Kinyarwanda language, naming the proposed model architecture KinyaBERT. A robust set of experimental results reveal that KinyaBERT outperforms solid baselines by 2% in F1 score on a named entity recognition task and by 4.3% in average score of a machine-translated GLUE benchmark. KinyaBERT fine-tuning has better convergence and achieves more robust results on multiple tasks even in the presence of translation noise. 1   ", "sections": [{"heading": "Introduction", "text": "Recent advances in natural language processing (NLP) through deep learning have been largely enabled by vector representations (or embeddings) learned through language model pre-training (Bengio et al., 2003;Mikolov et al., 2013;Pennington et al., 2014;Bojanowski et al., 2017;Peters et al., 2018;Devlin et al., 2019). Language models such as BERT (Devlin et al., 2019) are pre-trained on large text corpora and then fine-tuned on downstream tasks, resulting in better performance on many NLP tasks. Despite attempts to make multilingual BERT models (Conneau et al., 2020), research has shown that models pre-trained on high quality monolingual corpora outperform multilingual models pre-trained on large Internet data (Scheible et al., 2020;Virtanen et al., 2019). This has motivated many researchers to pretrain BERT models on individual languages rather than adopting the \"language-agnostic\" multilingual models. This work is partly motivated by the same findings, but also proposes an adaptation of the BERT architecture to address representational challenges that are specific to morphologically rich languages such as Kinyarwanda.\nIn order to handle rare words and reduce the vocabulary size, BERT-like models use statistical sub-word tokenization algorithms such as byte pair encoding (BPE) (Sennrich et al., 2016). While these techniques have been widely used in language modeling and machine translation, they are not optimal for morphologically rich languages (Klein and Tsarfaty, 2020). In fact, sub-word tokenization methods that are solely based on surface forms, including BPE and character-based models, cannot capture all morphological details. This is due to morphological alternations (Muhirwe, 2007) and non-concatenative morphology (McCarthy, 1981) that are often exhibited by morphologically rich languages. For example, as shown in Table 1, a BPE model trained on 390 million tokens of Kinyarwanda text cannot extract the true sub-word lexical units (i.e. morphemes) for the given words. This work addresses the above problem by proposing a language model architecture that explicitly represents most of the input words with morphological parses produced by a morphological analyzer. In this architecture BPE is only used to handle words which cannot be directly decomposed by the morphological analyzer such as misspellings,", "publication_ref": ["b5", "b39", "b49", "b6", "b50", "b14", "b14", "b12", "b55", "b61", "b56", "b27", "b41", "b37"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Word", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Morphemes", "text": "Monolingual BPE Multilingual BPE twagezeyo 'we arrived there' tu . a . ger . ye . yo twag . ezeyo _twa . ge . ze . yo ndabyizeye 'I hope so' n . ra . bi . izer . ye ndaby . izeye _ ndab . yiz . eye umwarimu 'teacher' u . mu . arimu umwarimu _um . wari . mu proper names and foreign language words. Given the output of a morphological analyzer, a second challenge is in how to incorporate the produced morphemes into the model. One naive approach is to feed the produced morphemes to a standard transformer encoder as a single monolithic sequence. This approach is used by Mohseni and Tebbifakhr (2019). One problem with this method is that mixing sub-word information and sentencelevel tokens in a single sequence does not encourage the model to learn the actual morphological compositionality and express word-relative syntactic regularities. We address these issues by proposing a simple yet effective two-tier transformer encoder architecture. The first tier encodes morphological information, which is then transferred to the second tier to encode sentence level information. We call this new model architecture KinyaBERT because it uses BERT's masked language model objective for pre-training and is evaluated on the morphologically rich Kinyarwanda language.\nThis work also represents progress in low resource NLP. Advances in human language technology are most often evaluated on the main languages spoken by major economic powers such as English, Chinese and European languages. This has exacerbated the language technology divide between the highly resourced languages and the underrepresented languages. It also hinders progress in NLP research because new techniques are mostly evaluated on the mainstream languages and some NLP advances become less informed of the diversity of the linguistic phenomena (Bender, 2019). Specifically, this work provides the following research contributions:\n\u2022 A simple yet effective two-tier BERT architecture for representing morphologically rich languages.\n\u2022 New evaluation datasets for Kinyarwanda language including a machine-translated subset of the GLUE benchmark (Wang et al., 2019) and a news categorization dataset.\n\u2022 Experimental results which set a benchmark for future studies on Kinyarwanda language understanding, and on using machinetranslated versions of the GLUE benchmark.\n\u2022 Code and datasets are made publicly available for reproducibility 1 .", "publication_ref": ["b40", "b4", "b62"], "figure_ref": [], "table_ref": []}, {"heading": "Morphology-aware Language Model", "text": "Our modeling objective is to be able to express morphological compositionality in a Transformerbased (Vaswani et al., 2017) language model. For morphologically rich languages such as Kinyarwanda, a set of morphemes (typically a stem and a set of functional affixes) combine to produce a word with a given surface form. This requires an alternative to the ubiquitous BPE tokenization, through which exact sub-word lexical units (i.e. morphemes) are used. For this purpose, we use a morphological analyzer which takes a sentence as input and, for every word, produces a stem, zero or more affixes and assigns a part of speech (POS) tag to each word. This section describes how this morphological information is obtained and then integrated in a two-tier transformer architecture (Figure 1) to learn morphology-aware input representations.", "publication_ref": ["b60"], "figure_ref": [], "table_ref": []}, {"heading": "Morphological Analysis and Part-of-Speech Tagging", "text": "Kinyarwanda, the national language of Rwanda, is one of the major Bantu languages (Nurse and Philippson, 2006) spoken in central and eastern Africa. Kinyarwanda has 16 noun classes. Modifiers (demonstratives, possessives, adjectives, numerals) carry a class marking morpheme that agrees with the main noun class. The verbal morphology (Nzeyimana, 2020) also includes subject and object markers that agree with the class of the subject or object. This agreement therefore enables users of the language to approximately disambiguate referred entities based on their classes. We leverage this syntactic agreement property in designing our unsupervised POS tagger. Figure 1: KinyaBERT model architecture: Encoding of the sentence 'John twarahamusanze biradutangaza' (We were surprised to find John there). The morphological analyzer produces morphemes for each word and assigns a POS tag to it. The two-tier transformer model then generates contextualized embeddings (blue vectors at the top). The red colored embeddings correspond to the POS tags, yellow is for the stem embeddings, green is for the variable length affixes while the purple embeddings correspond to the affix set.\nOur morphological analyzer for Kinyarwanda was built following finite-state two-level morphology principles (Koskenniemi, 1983;Karttunen, 2000, 2003). For every inflectable word type, we maintain a morphotactics model using a directed acyclic graph (DAG) that represents the regular sequencing of morphemes. We effectively model all inflectable word types in Kinyarwanda which include verbals, nouns, adjectives, possessive and demonstrative pronouns, numerals and quantifiers. The morphological analyzer also includes many hand-crafted rules for handling morphographemics and other linguistic regularities of the Kinyarwanda language. The morphological analyzer was independently developed and calibrated by native speakers as a closed source solution before the current work on language modeling. Similar to Nzeyimana (2020), we use a classifier trained on a stemming dataset to disambiguate between competing outputs of the morphological analyzer. Furthermore, we improve the disambiguation quality by leveraging a POS tagger at the phrase level so that the syntactic context can be taken into consideration.\nWe devise an unsupervised part of speech tagging algorithm which we explain here. Let x = (x 1 , x 2 , x 3 , ...x n ) be a sequence of tokens (e.g. words) to be tagged with a corresponding sequence of tags y = (y 1 , y 2 , y 3 , ...y n ). A sample of actual POS tags used for Kinyarwanda is given in Table 12 the Appendix. Using Bayes' rule, the optimal tag sequence y * is given by the following equation:\ny * = arg max \nA standard hidden Markov model (HMM) can decompose the result of Equation 1 using first order Markov assumption and independence assumptions into P (x|y) = n t=1 P (x t |y t ) and P (y) = n t=1 P (y t |y t\u22121 ). The tag sequence y * can then be efficiently decoded using the Viterbi algorithm (Forney, 1973). A better decoding strategy is presented below.\nInspired by Tsuruoka and Tsujii (2005), we devise a greedy heuristic for decoding y * using the same first order Markov assumptions but with bidirectional decoding.\nFirst, we estimate the local emission probabilities P (x t |y t ) using a factored model given in the following equation:\nP (x t |y t ) \u221dP (x t |y t ) P (x t |y t ) =P m (x t |y t )P p (x t |y t )P a (x t |y t )(2)\nIn Equation 2,P m (x t |y t ) corresponds to the probability/score returned by a morphological disambiguation classifier, representing the uncertainty of the morphology of x t .P p (x t |y t ) corresponds to a local precedence weight between competing POS tags. These precedence weights are man-ually crafted through qualitative evaluation (See Table 12 in Appendix for examples).P a (x t |y t ) quantifies the local neighborhood syntactic agreement between Bantu class markers. When there are two or more agreeing class markers in neighboring words, the tagger should be more confident of the agreeing parts of speech. A basic agreement score can be the number of agreeing class markers within a window of seven words around a given candidate x t . We manually designed a more elaborate set of agreement rules and their weights for different contexts. Therefore, the actual agreement scor\u1ebd P a (x t |y t ) is a weighted sum of the matched agreement rules. Each of the unnormalized measuresP in Equation 2 is mapped to the [0, 1] range using a sigmoid function \u03c3(z|z A , z B ) given in Equation 3, where z is the score of the measure and [z A , z B ] is its estimated active range.\n\u03c3(z|z A , z B ) = [1 + exp(\u22128 z \u2212 z A z B \u2212 z A )] \u22128 (3)\nAfter estimating the local emission model, we greedily decode y * t = arg max y tP (y t |x) in decreasing order ofP (x t |y t ) using a first order bidirectional inference ofP (y t |x) as given in the following equation:\nP (y t |x) = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3P (x t |y t )P (y t |y * t\u22121 , y * t+1 )P (y * t\u22121 |x)P (y * t+1 |x) if both y *\nt\u22121 and y * t+1 have been decoded; P (x t |y t )P (y t |y * t\u22121 )P (y * t\u22121 |x) if only y * t\u22121 has been decoded; P (x t |y t )P (y t |y * t+1 )P (y * t+1 |x) if only y * t+1 has been decoded; P (x t |y t ) otherwise ( 4)\nThe first order transition measuresP (y t |y t\u22121 ), P (y t |y t+1 ) andP (y t |y t\u22121 , y t+1 ) are estimated using count tables computed over the entire corpus by aggregating local emission marginals P (y t ) = xtP (x t , y t ) obtained through morphological analysis and disambiguation.", "publication_ref": ["b46", "b47", "b28", "b17", "b59"], "figure_ref": [], "table_ref": ["tab_0", "tab_0"]}, {"heading": "Morphology Encoding", "text": "The overall architecture of our model is depicted in Figure 1. This is a two-tier transformer encoder architecture made of a token-level morphology encoder that feeds into a sentence/document-level encoder. The morphology encoder is made of a small transformer encoder that is applied to each analyzed token separately in order to extract its morphological features. The extracted morphological features are then concatenated with the token's stem embedding to form the input vector fed to the sentence/document encoder. The sentence/document encoder is made of a standard transformer encoder as used in other BERT models. The sentence/document encoder uses untied position encoding with relative bias as proposed in Ke et al. (2020).\nThe input to the morphology encoder is a set of embedding vectors, three vectors relating to the part of speech, one for the stem and one for each affix when available. The transformer encoder operation is applied to these embedding vectors without any positional information. This is because positional information at the morphology level is inherent since no morpheme repeats and each morpheme always occupies a known (i.e. fixed) slot in the morphotactics model. The extracted morphological features are four encoder output vectors corresponding to the three POS embeddings and one stem embedding. Vectors corresponding to the affixes are left out since they are of variable length and the role of the affixes in this case is to be attended to by the stem and the POS tag so that morphological information can be captured. The four morphological output feature vectors are further concatenated with another stem embedding at the sentence level to form the input vector for the main sentence/document encoder.\nThe choice of this transformer-based architecture for morphology encoding is motivated by two factors. First, Zaheer et al. (2020) has demonstrated the importance of having \"global tokens\" such as [CLS] token in BERT models. These are tokens that attend to all other tokens in the modeled sequence. These \"global tokens\" effectively encapsulate some \"meaning\" of the encoded sequence. Second, the POS tag and stem represent the high level information content of a word. Therefore, having the POS tag and stem embeddings be transformed into morphological features is a viable option. The POS tag and stem embeddings thus serve as the \"global tokens\" at the morphology encoder level since they attend to all other morphemes that can be associated with them.\nIn order to capture subtle morphological information, we make one of the three POS embeddings span an affix set vocabulary that is a subset of the all affixes power set. We form an affix set vocabu-lary V a that is made of the N most frequent affix combinations in the corpus. In fact, the morphological model of the language enforces constraints on which affixes can go together for any given part of speech, resulting in an affix set vocabulary that is much smaller than the power set of all affixes. Even with limiting the affix set vocabulary V a to a fixed size, we can still map any affix combination to V a by dropping zero or very few affixes from the combination. Note that the affix set embedding still has to attend to all morphemes at the morphology encoder level, making it adapt to the whole morphological context. The affix set embedding is depicted by the purple units in Figure 1 and a sample of V a is given in Table 13 in the Appendix.", "publication_ref": ["b22", "b64"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Pre-training Objective", "text": "Similar to other BERT models, we use a masked language model objective. Specifically, 15% of all tokens in the training set are considered for prediction, of which 80% are replaced with [MASK] tokens, 10% are replaced with random tokens and 10% are left unchanged. When prediction tokens are replaced with [MASK] or random tokens, the corresponding affixes are randomly omitted 70% of the time or left in place for 30% of the time, while the units corresponding to POS tags and affix sets are also masked. The pre-training objective is then to predict stems and the associated affixes for all tokens considered for prediction using a two-layer feed-forward module on top of the encoder output.\nFor the affix prediction task, we face a multilabel classification problem where for each prediction token, we predict a variable number of affixes.\nIn our experiments, we tried two methods. For one, we use the Kullback-Leibler (KL) divergence 2 loss function to solve a regression task of predicting the N -length affix distribution vector. For this case, we use a target affix probability vector a t \u2208 R N in which each target affix index is assigned 1 m probability and 0 probability for non-target affixes. Here m is the number of affixes in the word to be predicted and N is the total number of all affixes. We call this method \"Affix Distribution Regression\" (ADR) and model variant KinyaBERT ADR . Alternatively, we use cross entropy loss and just predict the affix set associated with the prediction word; we call this method \"Affix Set Classification\" (ASC) and the model variant KinyaBERT ASC .\n2 https://en.wikipedia.org/wiki/ Kullback%E2%80%93Leibler_divergence", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In order to evaluate the proposed architecture, we pre-train KinyaBERT (101M parameters for KinyaBERT ADR and 105M for KinyaBERT ASC ) on a 2.4 GB of Kinyarwanda text along with 3 baseline BERT models. The first baseline is a BERT model pre-trained on the same Kinyarwanda corpus and with the same position encoding (Ke et al., 2020), same batch size and pre-training steps, but using the standard BPE tokenization. We call this first baseline model BERT BP E (120M parameters). The second baseline is a similar BERT model pretrained on the same Kinyarwanda corpus but tokenized by a morphological analyzer. For this model, the input is just a sequence of morphemes, in a similar fashion to Mohseni and Tebbifakhr (2019). We call this second baseline model BERT M ORP HO (127M parameters). For BERT M ORP HO , we found that predicting 30% of the tokens achieves better results than using 15% because of the many affixes generated. The third baseline is XLM-R (Conneau et al., 2020) (270M parameters) which is pretrained on 2.5 TB of multilingual text. We evaluate the above models by comparing their performance on downstream NLP tasks.  ", "publication_ref": ["b22", "b40"], "figure_ref": [], "table_ref": []}, {"heading": "Pre-training details", "text": "KinyaBERT model was implemented using Pytorch version 1.9. The morphological analyzer and POS tagger were implemented in a shared library using POSIX C. Morphological parsing of the corpus was performed as a pre-processing step, taking 20 hours to segment the 390M-token corpus on an 12-core desktop machine.  4.\nNews Categorization Task (NEWS) -For a document classification experiment, we collected a set of categorized news articles from seven major news websites that regularly publish in Kinyarwanda. The articles were already categorized, so no more manual labeling was needed. This dataset is similar to Niyongabo et al. ( 2020), but in our case, we limited the number collected articles per category to 3000 in order to have a more balanced label distribution (See Table 10 in the Appendix). The final dataset contains a total of 25.7K articles spanning 12 categories and has been split into training, validation and test sets in the ratios of 70%, 5% and 25% respectively. Results on this NEWS task are presented in Table 5.\nFor each evaluation task, we use a two-layer feedforward network on top of the sentence encoder as it is typically done in other BERT models. The finetuning hyper-parameters are presented in Table 14 in the Appendix.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0", "tab_0"]}, {"heading": "Main results", "text": "The main results are presented in Table 3, Table 4,  and Table 5. Each result is the average of 10 independent fine-tuning runs. Each average result is shown with the standard deviation of the 10 runs. Except for XLM-R, all other models are pre-trained on the same corpus (See Table 2) for 32K steps using the same hyper-parameters.\nOn the GLUE task, KinyaBERT ASC achieves 4.3% better average score than the strongest baseline. KinyaBERT ASC also leads to more robust results on multiple tasks. It is also shown that having just a morphological analyzer is not enough: BERT M ORP HO still under-performs even though it uses morphological tokenization. Multilingual XLM-R achieves least performance in most cases, possibly because it was not pre-trained on Kinyarwanda text and uses inadequate tokenization.\nOn the NER task, KinyaBERT ADR achieves best performance, about 3.2% better average F1 score than the strongest baseline. One of the architectural differences between KinyaBERT ADR and KinyaBERT ASC is that KinyaBERT ADR uses three POS tag embeddings while KinyaBERT ASC uses two. Assuming that POS tagging facilitates named entity recognition, this empirical result suggests that increasing the amount of POS tag information  (Wang et al., 2019). The translation score is the sample average translation quality score assigned by volunteers. For MRPC, we report accuracy and F1. For STS-B, we report Pearson and Spearman correlations. For all others, we report accuracy. The best results are shown in bold while equal top results are underlined. in the model, possibly through diversification (i.e. multiple POS tag embedding vectors per word), can lead to better NER performance. The NEWS categorization task resulted in differing performances between validation and test sets. This may be a result that solving such task does not require high level language modeling but rather depends on spotting few keywords. Previous research on a similar task (Niyongabo et al., 2020) has shown that simple classifiers based on TF-IDF features suffice to achieve best performance.\nThe morphological analyzer and POS tagger inherently have some level of noise because they do not always perform with perfect accuracy. While we did not have a simple way of assessing the impact of this noise in this work, we can logically expect that the lower the noise the better the results could be. Improving the morphological analyzer and POS tagger and quantitatively evaluating its accuracy is part of future work. Even though our POS tagger uses heuristic methods and was evaluated mainly through qualitative exploration, we can still see its positive impact on the pre-trained language model. We did not use previous work on Kinyarwanda POS tagging because it is largely different from this work in terms of scale, tag dictionary and dataset size and availability.\nWe plot the learning curves during fine-tuning process of KinyaBERT and the baselines. The results in Figure 2 indicate that KinyaBERT finetuning has better convergence across all tasks. Additional results also show that positional attention (Ke et al., 2020) learned by KinyaBERT has more uniform and smoother relative bias while BERT BP E and BERT M ORP HO have more noisy relative positional bias (See Figure 3 in Appendix). This is possibly an indication that KinyaBERT allows learning better word-relative syntactic regularities. However, this aspect needs to be investigated more systematically in future research.\nWhile the main sentence/document encoder of KinyaBERT is equivalent to a standard BERT \"BASE\" configuration on top of a small morphology encoder, overall, the model actually decreases the number of parameters by more than 12% through embedding layer savings. This is because using morphological representation reduces the vocabulary size. Using smaller embedding vectors at the morphology encoder level also significantly reduces the overall number of parameters. Table 8 in Appendix shows the vocabulary sizes and parameter count of KinyaBERT in comparison to the baselines. While the sizing of the embeddings was done essentially to match BERT \"BASE\" configuration, future studies can shed more light on how different model sizes affect performance.", "publication_ref": ["b62", "b45", "b22"], "figure_ref": ["fig_2"], "table_ref": ["tab_2"]}, {"heading": "Ablation study", "text": "We conducted an ablation study to clarify some of the design choices made for KinyaBERT architecture. We make variations along two axes: (i) morphology input and (ii) pre-training task, which gave us four variants that we pre-trained for 32K steps and evaluated on the same downstream tasks.\n\u2022 AFS\u2192STEM+ASC: Morphological features are captured by two POS tag and one affix set vectors. We predict both the stem and affix set. This corresponds to KinyaBERT ASC presented in the main results.\n\u2022 POS\u2192STEM+ADR: Morphological features are carried by three POS tag vectors and we predict the stem and affix probability vector. This corresponds to KinyaBERT ADR .\n\u2022 AVG\u2192STEM+ADR: Morphological features are captured by two POS tag vectors and the pointwise average of affix hidden vectors from the morphology encoder. We predict the stem and affix probability vector.\n\u2022 STEM\u2192STEM: We omit the morphology encoder and train a model with only the stem parts without affixes and only predict the stem.\nAblation results presented in Table 6 indicate that using affix sets for both morphology encoding and prediction gives better results for many GLUE tasks. The under-performance of \"STEM\u2192STEM\" on high resource tasks (QNLI and SST-2) is an indication that morphological information from affixes is important. However, the utility of this information depends on the task as we see mixed results on other tasks.\nDue to a large design space for a morphologyaware language model, there are still a number of other design choices that can be explored in future studies. One may vary the amount of POS tag embeddings used, vary the size affix set vocabulary or the dimension of the morphology encoder embeddings. One may also investigate the potential of other architectures for the morphology encoder, such as convolutional networks. Our early attempt of using recurrent neural networks (RNNs) for the morphology encoder was abandoned because it was too slow to train. Table 6: Ablation results: each result is an average of 10 independent fine-tuning runs. Metrics, dataset sizes and noise statistics are the same as for the main results in Table 3, Table 4 and Table 5.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "BERT-variant pre-trained language models (PLMs) were initially pre-trained on monolingual highresource languages. Multilingual PLMs that include both high-resource and low-resource languages have also been introduced (Devlin et al., 2019;Conneau et al., 2020;Xue et al., 2021;Chung et al., 2020). However, it has been found that these multilingual models are biased towards high-resource languages and use fewer low quality and uncleaned low-resource data (Kreutzer et al., 2022). The included low-resource languages are also very limited because they are mainly sourced from Wikipedia articles, where languages with few articles like Kinyarwanda are often left behind (Joshi et al., 2020;Nekoto et al., 2020). Joshi et al. (2020) classify the state of NLP for Kinyarwanda as \"Scraping-By\", meaning it has been mostly excluded from previous NLP research, and require the creation of dedicated resources and models. Kinyarwanda has been studied mostly in descriptive linguistics (Kimenyi, 1976(Kimenyi, , 1978a(Kimenyi, ,b, 1988Jerro, 2016). Few recent NLP works on Kinyarwanda include Morphological Analysis (Muhirwe, 2009;Nzeyimana, 2020), Text Classification (Niyongabo et al., 2020, Named Entity Recognition (Rijhwani et al., 2020;Adelani et al., 2021;S\u00e4lev\u00e4 and Lignos, 2021), POS tagging (Garrette and Baldridge, 2013; Duong et al., 2014;Fang and Cohn, 2016;Cardenas et al., 2019), and Parsing (Sun et al., 2014;Mielens et al., 2015). There is no prior study on pre-trained language modeling for Kinyarwanda.\nThere are very few works on monolingual PLMs for African languages. To the best of our knowledge there is currently only AfriBERT (Ralethe, 2020) that has been pre-trained on Afrikaans, a language spoken in South Africa. In this paper, we aim to increase the inclusion of African languages in NLP community by introducing a PLM for Kinyarwanda. Differently to the previous works (see Table 15 in Appendix) which solely pretrained unmodified BERT models, we propose an improved BERT architecture for morphologically rich languages.\nRecently, there has been a research push to improve sub-word tokenization by adopting characterbased models (Ma et al., 2020;Clark et al., 2022). While these methods are promising for the \"language-agnostic\" case, they are still solely based on the surface form of words, and thus have the same limitations as BPE when processing morphologically rich languages. We leave it to future research to empirically explore how these characterbased methods compare to morphology-aware models.", "publication_ref": ["b14", "b12", "b10", "b31", "b21", "b21", "b23", "b24", "b20", "b42", "b47", "b52", "b54", "b15", "b16", "b8", "b58", "b38", "b34", "b11"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Conclusion", "text": "This work demonstrates the effectiveness of explicitly incorporating morphological information in language model pre-training. The proposed twotier Transformer architecture allows the model to represent morphological compositionality. Experiments conducted on Kinyarwanda, a low resource morphologically rich language, reveal significant performance improvement on several downstream NLP tasks when using the proposed architecture. These findings should motivate more research into morphology-aware language models.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Affix Set", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Example", "text": "Surface form V:2:ku-V:18:a ku-gend-a kugenda 'to walk' N:0:u-N:1:mu u-mu-ntu umuntu 'a person' PO:1:i i-a-cu yacu 'our' N:0:i-N:1:n i-n-kiko inkiko 'courts' PO:1:u u-a-bo wabo 'their' V:2:a-V:4:a-V:18:ye a-a-bon-ye yabonye 'she saw' DE:1:u-DE:2:u u-u-o uwo 'that' V:2:u-V:4:a-V:17:w-V:18:ye u-a-vug-w-ye wavuzwe 'who was talked about' QA:1:ki-QA:3:ki-QA:4:re ki-re-ki-re kirekire 'tall'   ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "This work was supported with Cloud TPUs from Google's TPU Research Cloud (TRC) program and Google Cloud Research Credits with the award GCP19980904. We also thank the anonymous reviewers for their insightful feedback.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "", "journal": "Abdoulaye Diallo, Adewale Akinfaderin", "year": "", "authors": "Jade David Ifeoluwa Adelani; Graham Abbott;  Neubig; D Daniel; Julia 'souza; Constantine Kreutzer; Chester Lignos; Happy Palen-Michel; Shruti Buzaaba; Sebastian Rijhwani; Stephen Ruder;  Mayhew;  Israel Abebe Azime; H Shamsuddeen; Chris Chinenye Muhammad; Joyce Emezue; Perez Nakatumba-Nabende; Aremu Ogayo; Catherine Anuoluwapo; Derguene Gitau; Jesujoba Mbaye;  Alabi; Tajuddeen Seid Muhie Yimam; Ignatius Rabiu Gwadabe; Rubungo Andre Ezeani; Jonathan Niyongabo; Verrah Mukiibi; Iroro Otiende; Davis Orife; Samba David;  Ngom"}, {"ref_id": "b1", "title": "", "journal": "", "year": "2020", "authors": "Fady Baly; Hazem Hajj"}, {"ref_id": "b2", "title": "Finitestate non-concatenative morphotactics", "journal": "", "year": "2000", "authors": "R Kenneth; Lauri Beesley;  Karttunen"}, {"ref_id": "b3", "title": "Finitestate morphology: Xerox tools and techniques", "journal": "CSLI", "year": "2003", "authors": "R Kenneth; Lauri Beesley;  Karttunen"}, {"ref_id": "b4", "title": "The# benderrule: On naming the languages we study and why it matters. The Gradient", "journal": "", "year": "2019", "authors": "M Emily;  Bender"}, {"ref_id": "b5", "title": "A neural probabilistic language model. The journal of machine learning research", "journal": "", "year": "2003", "authors": "Yoshua Bengio; R\u00e9jean Ducharme; Pascal Vincent; Christian Janvin"}, {"ref_id": "b6", "title": "Enriching word vectors with subword information", "journal": "Transactions of the Association for Computational Linguistics", "year": "2017", "authors": "Piotr Bojanowski; Edouard Grave; Armand Joulin; Tomas Mikolov"}, {"ref_id": "b7", "title": "Spanish pre-trained bert model and evaluation data", "journal": "", "year": "2020", "authors": "Jos\u00e9 Canete; Gabriel Chaperon; Rodrigo Fuentes; Jorge P\u00e9rez"}, {"ref_id": "b8", "title": "A grounded unsupervised universal partof-speech tagger for low-resource languages", "journal": "", "year": "2019-05", "authors": "Ronald Cardenas; Ying Lin; Ji Heng; Jonathan "}, {"ref_id": "b9", "title": "German's next language model", "journal": "", "year": "2020", "authors": "Branden Chan; Stefan Schweter; Timo M\u00f6ller"}, {"ref_id": "b10", "title": "Rethinking embedding coupling in pre-trained language models", "journal": "", "year": "2020", "authors": " Hyung Won; Thibault Chung; Henry Fevry; Melvin Tsai; Sebastian Johnson;  Ruder"}, {"ref_id": "b11", "title": "Canine: Pre-training an efficient tokenization-free encoder for language representation", "journal": "Transactions of the Association for Computational Linguistics", "year": "2022", "authors": "H Jonathan; Dan Clark; Iulia Garrette; John Turc;  Wieting"}, {"ref_id": "b12", "title": "Unsupervised cross-lingual representation learning at scale", "journal": "", "year": "2020", "authors": "Alexis Conneau; Kartikay Khandelwal; Naman Goyal; Vishrav Chaudhary; Guillaume Wenzek; Francisco Guzm\u00e1n; Edouard Grave; Myle Ott; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b13", "title": "RobBERT: a Dutch RoBERTa-based Language Model", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Pieter Delobelle; Thomas Winters; Bettina Berendt"}, {"ref_id": "b14", "title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b15", "title": "What can we get from 1000 tokens? a case study of multilingual pos tagging for resource-poor languages", "journal": "", "year": "2014", "authors": "Long Duong; Trevor Cohn; Karin Verspoor; Steven Bird; Paul Cook"}, {"ref_id": "b16", "title": "Learning when to trust distant supervision: An application to lowresource pos tagging using cross-lingual projection", "journal": "", "year": "2016", "authors": "Meng Fang; Trevor Cohn"}, {"ref_id": "b17", "title": "The viterbi algorithm. Proceedings of the IEEE", "journal": "", "year": "1973", "authors": "David Forney"}, {"ref_id": "b18", "title": "Learning a part-of-speech tagger from two hours of annotation", "journal": "", "year": "2013", "authors": "Dan Garrette; Jason Baldridge"}, {"ref_id": "b19", "title": "Real-world semi-supervised learning of postaggers for low-resource languages", "journal": "Long Papers", "year": "2013", "authors": "Dan Garrette; Jason Mielens; Jason Baldridge"}, {"ref_id": "b20", "title": "The locative applicative and the semantics of verb class in kinyarwanda. Diversity in African languages", "journal": "", "year": "2016", "authors": "Kyle Jerro"}, {"ref_id": "b21", "title": "The state and fate of linguistic diversity and inclusion in the nlp world", "journal": "", "year": "2020", "authors": "Pratik Joshi; Sebastin Santy; Amar Budhiraja; Kalika Bali; Monojit Choudhury"}, {"ref_id": "b22", "title": "Rethinking positional encoding in language pre-training", "journal": "", "year": "2020", "authors": "Guolin Ke; Di He; Tie-Yan Liu"}, {"ref_id": "b23", "title": "Subjectivization rules in kinyarwanda", "journal": "", "year": "1976", "authors": "Alexandre Kimenyi"}, {"ref_id": "b24", "title": "Aspects of naming in kinyarwanda", "journal": "Anthropological linguistics", "year": "1978", "authors": "Alexandre Kimenyi"}, {"ref_id": "b25", "title": "A relational grammar of kinyarwanda", "journal": "Cal", "year": "1978", "authors": "Alexandre Kimenyi"}, {"ref_id": "b26", "title": "Passiveness in kinyarwanda", "journal": "John Benjamins", "year": "1988", "authors": "Alexandre Kimenyi"}, {"ref_id": "b27", "title": "Getting the ##life out of living: How adequate are word-pieces for modelling complex morphology?", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Stav Klein; Reut Tsarfaty"}, {"ref_id": "b28", "title": "Two-level model for morphological analysis", "journal": "", "year": "1983", "authors": "Kimmo Koskenniemi"}, {"ref_id": "b29", "title": "Indolem and indobert: A benchmark dataset and pre-trained language model for indonesian nlp", "journal": "", "year": "2020", "authors": "Fajri Koto; Afshin Rahimi; Jey Han Lau; Timothy Baldwin"}, {"ref_id": "b30", "title": "Prodromos Malakasiotis, and Ion Androutsopoulos", "journal": "", "year": "2020", "authors": "John Koutsikakis; Ilias Chalkidis"}, {"ref_id": "b31", "title": "Quality at a glance: An audit of web-crawled multilingual datasets", "journal": "Transactions of the Association for Computational Linguistics", "year": "2022", "authors": "Julia Kreutzer; Isaac Caswell; Lisa Wang; Ahsan Wahab; Nasanbayar Daan Van Esch; Allahsera Ulzii-Orshikh; Nishant Tapo; Artem Subramani; Claytone Sokolov;  Sikasote"}, {"ref_id": "b32", "title": "Adaptation of deep bidirectional multilingual transformers for russian language", "journal": "", "year": "2019", "authors": "Y Kuratov;  Arkhipov"}, {"ref_id": "b33", "title": "Flaubert: Unsupervised language model pre-training for french", "journal": "", "year": "2020", "authors": "Hang Le; Lo\u00efc Vial; Jibril Frej; Vincent Segonne; Maximin Coavoux; Benjamin Lecouteux; Alexandre Allauzen; Benoit Crabbe; Laurent Besacier; Didier Schwab"}, {"ref_id": "b34", "title": "CharBERT: Character-aware pre-trained language model", "journal": "", "year": "2020", "authors": "Wentao Ma; Yiming Cui; Chenglei Si; Ting Liu; Shijin Wang; Guoping Hu"}, {"ref_id": "b35", "title": "\u00c9ric de la Clergerie, Djam\u00e9 Seddah, and Beno\u00eet Sagot", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Louis Martin; Benjamin Muller; Pedro Javier Ortiz Su\u00e1rez; Yoann Dupont; Laurent Romary"}, {"ref_id": "b36", "title": "Robert-a romanian bert model", "journal": "", "year": "2020", "authors": "Mihai Masala; Stefan Ruseti; Mihai Dascalu"}, {"ref_id": "b37", "title": "A prosodic theory of nonconcatenative morphology", "journal": "Linguistic inquiry", "year": "1981", "authors": "J John;  Mccarthy"}, {"ref_id": "b38", "title": "Parse imputation for dependency annotations", "journal": "Long Papers", "year": "2015", "authors": "Jason Mielens; Liang Sun; Jason Baldridge"}, {"ref_id": "b39", "title": "Distributed representations of words and phrases and their compositionality", "journal": "", "year": "2013", "authors": "Tomas Mikolov; Ilya Sutskever; Kai Chen; Greg S Corrado; Jeff Dean"}, {"ref_id": "b40", "title": "MorphoBERT: a Persian NER system with BERT and morphological analysis", "journal": "", "year": "2019", "authors": "Mahdi Mohseni; Amirhossein Tebbifakhr"}, {"ref_id": "b41", "title": "Computational analysis of kinyarwanda morphology: The morphological alternations", "journal": "International Journal of computing and ICT Research", "year": "2007", "authors": "Jackson Muhirwe"}, {"ref_id": "b42", "title": "Morphological analysis of tone marked kinyarwanda text", "journal": "Springer", "year": "2009", "authors": "Jackson Muhirwe"}, {"ref_id": "b43", "title": "Blessing Sibanda, Blessing Bassey, Ayodele Olabiyi, Arshath Ramkilowan, Alp \u00d6ktem, Adewale Akinfaderin, and Abdallah Bashir. 2020. Participatory research for low-resourced machine translation: A case study in African languages", "journal": "Association for Computational Linguistics", "year": "", "authors": "Wilhelmina Nekoto; Vukosi Marivate; Tshinondiwa Matsila; Timi Fasubaa; Taiwo Fagbohungbe; Shamsuddeen Solomon Oluwole Akinola; Salomon Kabongo Muhammad; Salomey Kabenamualu; Freshia Osei; Rubungo Andre Sackey; Ricky Niyongabo; Perez Macharm; Orevaoghene Ogayo; Musie Ahia; Mofetoluwa Meressa Berhe; Masabata Adeyemi; Lawrence Mokgesi-Selinga; Laura Okegbemi; Kolawole Martinus; Kevin Tajudeen; Kelechi Degila; Kathleen Ogueji; Julia Siminyu; Jason Kreutzer; Jamiil Toure Webster; Jade Ali; Iroro Abbott; Ignatius Orife;  Ezeani; Abdulkadir Idris; Herman Dangana; Hady Kamper; Goodness Elsahar; Ghollah Duru; Murhabazi Kioko;  Espoir; Daniel Elan Van Biljon; Christopher Whitenack;  Onyefuluchi"}, {"ref_id": "b44", "title": "PhoBERT: Pre-trained language models for Vietnamese", "journal": "Online. Association for Computational Linguistics", "year": "", "authors": ""}, {"ref_id": "b45", "title": "Kinnews and kirnews: Benchmarking cross-lingual text classification for kinyarwanda and kirundi", "journal": "", "year": "2020", "authors": "Qu Rubungo Andre Niyongabo; Julia Hong; Li Kreutzer;  Huang"}, {"ref_id": "b46", "title": "The bantu languages. Routledge", "journal": "", "year": "2006", "authors": "Derek Nurse; G\u00e9rard Philippson"}, {"ref_id": "b47", "title": "Morphological disambiguation from stemming data", "journal": "", "year": "2020", "authors": "Antoine Nzeyimana"}, {"ref_id": "b48", "title": "fairseq: A fast, extensible toolkit for sequence modeling", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Myle Ott; Sergey Edunov; Alexei Baevski; Angela Fan; Sam Gross; Nathan Ng; David Grangier; Michael Auli"}, {"ref_id": "b49", "title": "Glove: Global vectors for word representation", "journal": "", "year": "2014", "authors": "Jeffrey Pennington; Richard Socher; Christopher D Manning"}, {"ref_id": "b50", "title": "Deep contextualized word representations", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Matthew E Peters; Mark Neumann; Mohit Iyyer; Matt Gardner; Christopher Clark; Kenton Lee; Luke Zettlemoyer"}, {"ref_id": "b51", "title": "Adaptation of deep bidirectional transformers for afrikaans language", "journal": "", "year": "2020", "authors": "Sello Ralethe"}, {"ref_id": "b52", "title": "Soft gazetteers for lowresource named entity recognition", "journal": "", "year": "2020", "authors": "Shruti Rijhwani; Shuyan Zhou; Graham Neubig; Jaime G Carbonell"}, {"ref_id": "b53", "title": "Klej: Comprehensive benchmark for polish language understanding", "journal": "", "year": "2020", "authors": "Piotr Rybak; Robert Mroczkowski; Janusz Tracz; Ireneusz Gawlik"}, {"ref_id": "b54", "title": "Mining wikidata for name resources for african languages", "journal": "", "year": "2021", "authors": "Jonne S\u00e4lev\u00e4; Constantine Lignos"}, {"ref_id": "b55", "title": "Gottbert: a pure german language model", "journal": "", "year": "2020", "authors": "Raphael Scheible; Fabian Thomczyk; Patric Tippmann; Victor Jaravine; Martin Boeker"}, {"ref_id": "b56", "title": "Neural machine translation of rare words with subword units", "journal": "Long Papers", "year": "2016", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"ref_id": "b57", "title": "Bertimbau: Pretrained bert models for brazilian portuguese", "journal": "Springer", "year": "2020", "authors": "F\u00e1bio Souza; Rodrigo Nogueira; Roberto Lotufo"}, {"ref_id": "b58", "title": "Parsing low-resource languages using gibbs sampling for pcfgs with latent annotations", "journal": "", "year": "2014", "authors": "Liang Sun; Jason Mielens; Jason Baldridge"}, {"ref_id": "b59", "title": "Bidirectional inference with the easiest-first strategy for tagging sequence data", "journal": "", "year": "2005", "authors": "Yoshimasa Tsuruoka; Jun'ichi Tsujii"}, {"ref_id": "b60", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Lukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b61", "title": "Multilingual is not enough: Bert for finnish", "journal": "", "year": "2019", "authors": "Antti Virtanen; Jenna Kanerva; Rami Ilo; Jouni Luoma; Juhani Luotolahti; Tapio Salakoski; Filip Ginter; Sampo Pyysalo"}, {"ref_id": "b62", "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding", "journal": "", "year": "2019", "authors": "Alex Wang; Amanpreet Singh; Julian Michael; Felix Hill; Omer Levy; Samuel R Bowman"}, {"ref_id": "b63", "title": "Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer", "journal": "", "year": "", "authors": "Linting Xue; Noah Constant; Adam Roberts; Mihir Kale; Rami Al-Rfou; Aditya Siddhant"}, {"ref_id": "b64", "title": "Big bird: Transformers for longer sequences", "journal": "Advances in Neural Information Processing Systems", "year": "2020", "authors": "Manzil Zaheer; Guru Guruganesh; Joshua Kumar Avinava Dubey; Chris Ainslie; Santiago Alberti; Philip Ontanon; Anirudh Pham; Qifan Ravula; Li Wang;  Yang"}, {"ref_id": "b65", "title": "Average non-adjacent diagonal STDEV = 0.81 for |i \u2212 j|", "journal": "", "year": "", "authors": "Bert Bp E"}, {"ref_id": "b66", "title": "Average non-adjacent diagonal STDEV = 0.80 for |i \u2212 j|", "journal": "", "year": "", "authors": "Bert M Orp Ho"}, {"ref_id": "b67", "title": "Average non-adjacent diagonal STDEV = 0.75 for |i \u2212 j|", "journal": "", "year": "", "authors": "Adr Kinyabert"}, {"ref_id": "b68", "title": "Average non-adjacent diagonal STDEV = 0.75 for |i \u2212 j| \u2208 [2, 10] Figure 3: Visualization of the positional attention bias (normalized) of the 12 attention heads. Each (i, j) attention bias (Ke et al., 2020) indicates the positional correlations between the i th and j th words/tokens in a sentence", "journal": "", "year": "", "authors": "Asc Kinyabert"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "(We were surprised to find John there)", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: Comparison of fine-tuning loss curves between KinyaBERT and baselines on the evaluation tasks. KinyaBERT ASC achieves the best convergence in most cases, indicating better effectiveness of its model architecture and pre-training objective.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Comparison between morphemes and BPE-produced sub-word tokens. Stems are underlined.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "XLA 3 package and a TPU-optimized fairseq toolkit(Ott et al., 2019). Pre-training on TPU took 2.3 hours per 1000 steps. The baselines were trained on TPU because there were no major changes needed to the existing RoBERTA (base) ORG), and date and time (DATE). Results on this NER task are presented in Table", "figure_data": "ganizations (architecture implemented in fairseq and the TPUresources were available and efficient. In all cases,pre-training batch size was set to 2560 sequences,with maximum 512 tokens in each sequence. Themaximum learning rates was set to 4 \u00d7 10 \u22124 whichis achieved after 2000 steps and then linearly de-cays. Our main results and ablation results wereobtained from models pre-trained for 32K steps inall cases. Other pre-training details, model archi-tectural dimensions and other hyper-parameters aregiven in the Appendix.3.2 Evaluation tasksMachine translated GLUE benchmark -TheGeneral Language Understanding Evaluation(GLUE) benchmark (Wang et al., 2019) has beenwidely used to evaluate pre-trained language mod-els. In order to assess KinyaBERT performanceon such high level language tasks, we used GoogleTranslate API to translate a subset of the GLUEbenchmark (MRPC, QNLI, RTE, SST-2, STS-Band WNLI tasks) into Kinyarwanda. CoLA taskwas left because it is English-specific. MNLI andQQP tasks were also not translated because theywere too expensive to translate with Google's com-mercial API. While machine translation adds morenoise to the data, evaluating on this dataset is stillrelevant because all models compared have to copewith the same noise. To understand this transla-tion noise, we also run user evaluation experiments,whereby four volunteers proficient in both Englishand Kinyarwanda evaluated a random sample of6000 translated GLUE examples, and assigned ascore to each example on a scale from 1 to 4 (SeeTable 11 in Appendix). These scores help us char-acterize the noise in the data and contextualize ourresults with regards to other GLUE evaluations.Results on these GLUE tasks are shown in Table 3.Named entity recognition (NER) -We use the Kinyarwanda subset of the MasakhaNER dataset (Adelani et al., 2021) for NER task. This is a high quality NER dataset annotated by nativePre-training was per-formed using RTX 3090 and RTX 2080Ti desktop GPUs. Each KinyaBERT model takes on aver-speakers for major African languages includingKinyarwanda. The task requires predicting fourentity types: Persons (PER), Locations (LOC), Or-"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "/78.3\u00b10.8/1.0 79.0\u00b10.3 58.4\u00b13.2 78.7\u00b10.6 77.7/77.8\u00b10.7/0.6 55.4\u00b12.0 BERT", "figure_data": "Task:MRPCQNLIRTESST-2STS-BWNLI#Train examples:3.4K104.7K2.5K67.4K5.8K0.6KTranslation score:2.7/4.02.9/4.03.0/4.02.7/4.03.1/4.02.9/4.0ModelValidation SetXLM-R 84.2BP E 83.3/76.6\u00b10.8/1.4 81.9\u00b10.2 59.2\u00b11.5 80.1\u00b10.4 75.6/75.7\u00b17.8/7.3 55.4\u00b11.9BERT M ORP HO84.3/77.4\u00b10.6/1.1 81.6\u00b10.2 59.2\u00b11.5 81.6\u00b10.5 76.8/77.0\u00b10.8/0.7 54.2\u00b12.5KinyaBERT ADR87.1/82.1\u00b10.5/0.7 81.6\u00b10.1 61.8\u00b11.4 81.8\u00b10.6 79.6/79.5\u00b10.4/0.3 54.5\u00b12.2KinyaBERT ASC86.6/81.3\u00b10.5/0.7 82.3\u00b10.3 64.3\u00b11.4 82.4\u00b10.5 80.0/79.9\u00b10.5/0.5 56.2\u00b10.8ModelTest SetXLM-R82.6/76.0\u00b10.6/0.6 78.1\u00b10.3 56.4\u00b13.2 76.3\u00b10.4 69.5/68.9\u00b11.0/1.1 63.7\u00b13.9BERT BP E82.8/76.2\u00b10.6/0.8 81.1\u00b10.3 55.6\u00b12.8 79.1\u00b10.4 68.9/67.8\u00b11.8/1.7 63.4\u00b14.1BERT M ORP HO82.7/75.4\u00b10.8/1.3 80.8\u00b10.4 56.7\u00b11.0 80.7\u00b10.5 68.9/67.8\u00b11.5/1.3 65.0\u00b10.3KinyaBERT ADR84.4/78.7\u00b10.5/0.6 81.2\u00b10.3 58.1\u00b11.1 80.9\u00b10.5 73.2/72.0\u00b10.4/0.3 65.1\u00b10.0KinyaBERT ASC84.6/78.4\u00b10.2/0.3 82.2\u00b10.6 58.8\u00b10.7 81.4\u00b10.6 74.5/73.5\u00b10.2/0.2 65.0\u00b10.2"}, {"figure_label": "13", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Examples of affix sets used by KinyaBERT ASC ; there are 34K sets in total.", "figure_data": "HyperparameterMRPCQNLIRTESST-2STS-BWNLINERNEWSPeak Learning Rate1e-51e-52e-51e-52e-51e-55e-51e-5Batch Size1632163216163232Learning Rate DecayLinearLinearLinearLinearLinearLinearLinearLinearWeight Decay0.10.10.10.10.10.10.10.1Max Epochs1515151515153015Warmup Steps proportion6%6%6%6%6%6%6%6%OptimizerAdamW AdamW AdamW AdamW AdamW AdamW AdamW AdamW"}, {"figure_label": "14", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Downstream task fine-tuning hyper-parameters.", "figure_data": "PaperLanguagePre-training TasksPositional Embedding Representation InputMohseni and Tebbifakhr (2019)PersianMLM+NSPAbsoluteMorphemesKuratov and Arkhipov (2019)RussianMLM+NSPAbsoluteBPEMasala et al. (2020)RomanianMLM+NSPAbsoluteBPEBaly et al. (2020)ArabicWWM+NSPAbsoluteBPEKoto et al. (2020)IndonesianMLM+NSPAbsoluteBPEChan et al. (2020)GermanWWMAbsoluteBPEDelobelle et al. (2020)DutchMLMAbsoluteBPENguyen and Tuan Nguyen (2020) VietnameseMLMAbsoluteBPECanete et al. (2020)SpanishWWMAbsoluteBPERybak et al. (2020)PolishMLMAbsoluteBPEMartin et al. (2020)FrenchMLMAbsoluteBPELe et al. (2020)FrenchMLMAbsoluteBPEKoutsikakis et al. (2020)GreekMLM+NSPAbsoluteBPESouza et al. (2020)PortugueseMLMAbsoluteBPERalethe (2020)AfrikaansMLM+NSPAbsoluteBPEThis workKinyarwanda MLM: STEM+AFFIXES TUPE-RMorphemes+BPE"}, {"figure_label": "15", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Comparison between KinyaBERT and other monolingual BERT-variant PLMs. We only compare with previous works that have been published in either journals or conferences as of August 2021. We excluded some extremely high-resource languages such as English and Chinese. MLM: Masked language model; NSP: Next Sentence Prediction; WWM: Whole Word Masked.", "figure_data": ""}], "formulas": [{"formula_id": "formula_1", "formula_text": "P (x t |y t ) \u221dP (x t |y t ) P (x t |y t ) =P m (x t |y t )P p (x t |y t )P a (x t |y t )(2)", "formula_coordinates": [3.0, 317.59, 658.21, 206.84, 28.73]}, {"formula_id": "formula_2", "formula_text": "\u03c3(z|z A , z B ) = [1 + exp(\u22128 z \u2212 z A z B \u2212 z A )] \u22128 (3)", "formula_coordinates": [4.0, 82.26, 317.41, 206.87, 25.55]}, {"formula_id": "formula_3", "formula_text": "P (y t |x) = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3P (x t |y t )P (y t |y * t\u22121 , y * t+1 )P (y * t\u22121 |x)P (y * t+1 |x) if both y *", "formula_coordinates": [4.0, 70.86, 429.65, 215.48, 124.78]}], "doi": "10.1162/tacl_a_00416"}