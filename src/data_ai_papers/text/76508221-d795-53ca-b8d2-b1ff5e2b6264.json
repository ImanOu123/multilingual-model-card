{"title": "Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields", "authors": "Jonathan T Barron; Ben Mildenhall; Matthew Tancik; Peter Hedman; Ricardo Martin-Brualla; Pratul P Srinivasan;  Google; U C Berkeley", "pub_date": "", "abstract": "The rendering procedure used by neural radiance fields (NeRF) samples a scene with a single ray per pixel and may therefore produce renderings that are excessively blurred or aliased when training or testing images observe scene content at different resolutions. The straightforward solution of supersampling by rendering with multiple rays per pixel is impractical for NeRF, because rendering each ray requires querying a multilayer perceptron hundreds of times. Our solution, which we call \"mip-NeRF\" (\u00e0 la \"mipmap\"), extends NeRF to represent the scene at a continuously-valued scale. By efficiently rendering anti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable aliasing artifacts and significantly improves NeRF's ability to represent fine details, while also being 7% faster than NeRF and half the size. Compared to NeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with NeRF and by 60% on a challenging multiscale variant of that dataset that we present. Mip-NeRF is also able to match the accuracy of a brute-force supersampled NeRF on our multiscale dataset while being 22\u00d7 faster.", "sections": [{"heading": "Introduction", "text": "Neural volumetric representations such as neural radiance fields (NeRF) [30] have emerged as a compelling strategy for learning to represent 3D objects and scenes from images for the purpose of rendering photorealistic novel views. Although NeRF and its variants have demonstrated impressive results across a range of view synthesis tasks, NeRF's rendering model is flawed in a manner that can cause excessive blurring and aliasing. NeRF replaces traditional discrete sampled geometry with a continuous volumetric function, parameterized as a multilayer perceptron (MLP) that maps from an input 5D coordinate (3D position and 2D viewing direction) to properties of the scene (volume density and view-dependent emitted radiance) at that location. To render a pixel's color, NeRF casts a single ray through that pixel and out into its volumetric representation, queries  the MLP for scene properties at samples along that ray, and composites these values into a single color. While this approach works well when all training and testing images observe scene content from a roughly constant distance (as done in NeRF and most follow-ups), NeRF renderings exhibit significant artifacts in less contrived scenarios. When the training images observe scene content at multiple resolutions, renderings from the recovered NeRF appear excessively blurred in close-up views and contain aliasing artifacts in distant views. A straightforward solution is to adopt the strategy used in offline raytracing: supersampling each pixel by marching multiple rays through its footprint. But this is prohibitively expensive for neural volumetric representations such as NeRF, which require hundreds of MLP evaluations to render a single ray and several hours to reconstruct a single scene.\nIn this paper, we take inspiration from the mipmapping approach used to prevent aliasing in computer graphics rendering pipelines. A mipmap represents a signal (typically an image or a texture map) at a set of different discrete downsampling scales and selects the appropriate scale to use for a ray based on the projection of the pixel footprint onto the geometry intersected by that ray. This strategy is known as pre-filtering, since the computational burden of anti-aliasing is shifted from render time (as in the brute force supersampling solution) to a precomputation phasethe mipmap need only be created once for a given texture, regardless of how many times that texture is rendered.\nOur solution, which we call mip-NeRF (multum in parvo NeRF, as in \"mipmap\"), extends NeRF to simultaneously represent the prefiltered radiance field for a continuous space of scales. The input to mip-NeRF is a 3D Gaussian that represents the region over which the radiance field should be integrated. As illustrated in Figure 1, we can then render a prefiltered pixel by querying mip-NeRF at intervals along a cone, using Gaussians that approximate the conical frustums corresponding to the pixel. To encode a 3D position and its surrounding Gaussian region, we propose a new feature representation: an integrated positional encoding (IPE). This is a generalization of NeRF's positional encoding (PE) that allows a region of space to be compactly featurized, as opposed to a single point in space.\nMip-NeRF substantially improves upon the accuracy of NeRF, and this benefit is even greater in situations where scene content is observed at different resolutions (i.e. setups where the camera moves closer and farther from the scene). On a challenging multiresolution benchmark we present, mip-NeRF is able to reduce error rates relative to NeRF by 60% on average (see Figure 2 for visualisations). Mip-NeRF's scale-aware structure also allows us to merge the separate \"coarse\" and \"fine\" MLPs used by NeRF for hierarchical sampling [30] into a single MLP. As a consequence, mip-NeRF is slightly faster than NeRF (\u223c 7%), and has half as many parameters.", "publication_ref": ["b30", "b30"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Related Work", "text": "Our work directly extends NeRF [30], a highly influential technique for learning a 3D scene representation from observed images in order to synthesize novel photorealistic views. Here we review the 3D representations used by computer graphics and view synthesis, including recentlyintroduced continuous neural representations such as NeRF, with a focus on sampling and aliasing. Anti-aliasing in Rendering Sampling and aliasing are fundamental issues that have been extensively studied throughout the development of rendering algorithms in computer graphics. Reducing aliasing artifacts (\"antialiasing\") is typically done via either supersampling or prefiltering. Supersampling-based techniques [46] cast multiple rays per pixel while rendering in order to sample closer to the Nyquist frequency. This is an effective strategy to reduce aliasing, but it is expensive, as runtime generally scales linearly with the supersampling rate. Supersampling is therefore typically used only in offline rendering contexts. Instead of sampling more rays to match the Nyquist fre-  [18,20,32,49] are better suited for realtime rendering, because filtered versions of scene content can be precomputed ahead of time, and the correct \"scale\" can be used at render time depending on the target sampling rate.\nIn the context of rendering, prefiltering can be thought of as tracing a cone instead of a ray through each pixel [1,16]: wherever the cone intersects scene content, a precomputed multiscale representation of the scene content (such as a sparse voxel octree [15,21] or a mipmap [47]) is queried at the scale corresponding to the cone's footprint.\nOur work takes inspiration from this line of work in graphics and presents a multiscale scene representation for NeRF. Our strategy differs from multiscale representations used in traditional graphics pipelines in two crucial ways. First, we cannot precompute the multiscale representation because the scene's geometry is not known ahead of time in our problem setting -we are recovering a model of the scene using computer vision, not rendering a predefined CGI asset. Mip-NeRF therefore must learn a prefiltered representation of the scene during training. Second, our notion of scale is continuous instead of discrete. Instead of representing the scene using multiple copies at a fixed number of scales (like in a mipmap), mip-NeRF learns a single neural scene model that can be queried at arbitrary scales. Scene Representations for View Synthesis Various scene representations have been proposed for the task of view synthesis: using observed images of a scene to recover a representation that supports rendering novel photorealistic images of the scene from unobserved camera viewpoints. When images of the scene are captured densely, light field interpolation techniques [9,14,22] can be used to render novel views without reconstructing an intermediate representation of the scene. Issues related to sampling and aliasing have been thoroughly studied within this setting [7].\nMethods that synthesize novel views from sparselycaptured images typically reconstruct explicit representations of the scene's 3D geometry and appearance. Many classic view synthesis algorithms use mesh-based representations along with either diffuse [28] or view-dependent [6,10,48] textures. Mesh-based representations can be stored efficiently and are naturally compatible with existing graphics rendering pipelines. However, using gradient-based methods to optimize mesh geometry and topology is typically difficult due to discontinuities and local minima. Volumetric representations have therefore become increasingly popular for view synthesis. Early approaches directly color voxel grids using observed images [37], and more recent volumetric approaches use gradient-based learning to train deep networks to predict voxel grid representations of scenes [12,25,29,38,41,53]. Discrete voxel-based representations are effective for view synthesis, but they do not scale well to scenes at higher resolutions.\nA recent trend within computer vision and graphics research is to replace these discrete representations with coordinate-based neural representations, which represent 3D scenes as continuous functions parameterized by MLPs that map from a 3D coordinate to properties of the scene at that location. Some recent methods use coordinatebased neural representations to model scenes as implicit surfaces [31,50], but the majority of recent view synthesis methods are based on the volumetric NeRF representation [30]. NeRF has inspired many subsequent works that extend its continuous neural volumetric representation for generative modeling [8,36], dynamic scenes [23,33], nonrigidly deforming objects [13,34], phototourism settings with changing illumination and occluders [26,43], and reflectance modeling for relighting [2,3,40].\nRelatively little attention has been paid to the issues of sampling and aliasing within the context of view synthesis using coordinate-based neural representations. Discrete representations used for view synthesis, such as polygon meshes and voxel grids, can be efficiently rendered without aliasing using traditional multiscale prefiltering approaches such as mipmaps and octrees. However, coordinate-based neural representations for view synthesis can currently only be anti-aliased using supersampling, which exacerbates their already slow rendering procedure. Recent work by Takikawa et al. [42] proposes a multiscale representation based on sparse voxel octrees for continuous neural representations of implicit surfaces, but their approach requires that the scene geometry be known a priori, as opposed to our view synthesis setting where the only inputs are observed images. Mip-NeRF addresses this open problem, enabling the efficient rendering of anti-aliased images during both training and testing as well as the use of multiscale images during training.", "publication_ref": ["b30", "b46", "b18", "b20", "b32", "b48", "b0", "b16", "b15", "b21", "b9", "b14", "b22", "b7", "b28", "b6", "b10", "b47", "b37", "b12", "b25", "b29", "b38", "b41", "b52", "b31", "b49", "b30", "b8", "b36", "b23", "b33", "b13", "b34", "b26", "b43", "b1", "b2", "b40", "b42"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries: NeRF", "text": "NeRF uses the weights of a multilayer perceptron (MLP) to represent a scene as a continuous volumetric field of particles that block and emit light. NeRF renders each pixel of a camera as follows: A ray r(t) = o + td is emitted from the camera's center of projection o along the direction d such that it passes through the pixel. A sampling strategy (discussed later) is used to determine a vector of sorted distances t between the camera's predefined near and far planes t n and t f . For each distance t k \u2208 t, we compute its corresponding 3D position along the ray x = r(t k ), then transform each position using a positional encoding:\n\u03b3(x) = sin(x), cos(x), . . . , sin 2 L\u22121 x , cos 2 L\u22121 x T . (1)\nThis is simply the concatenation of the sines and cosines of each dimension of the 3D position x scaled by powers of 2 from 1 to 2 L\u22121 , where L is a hyperparameter. The fidelity of NeRF depends critically on the use of positional encoding, as it allows the MLP parameterizing the scene to behave as an interpolation function, where L determines the bandwidth of the interpolation kernel (see Tancik et al. [44] for details). The positional encoding of each ray position \u03b3(r(t k )) is provided as input to an MLP parameterized by weights \u0398, which outputs a density \u03c4 and an RGB color c:\n\u2200t k \u2208 t, [\u03c4 k , c k ] = MLP(\u03b3(r(t k )); \u0398) .(2)\nThe MLP also takes the view direction as input, which is omitted from notation for simplicity. These estimated densities and colors are used to approximate the volume rendering integral using numerical quadrature, as per Max [27]:\nC(r; \u0398, t) = k T k (1 \u2212 exp(\u2212\u03c4 k (t k+1 \u2212 t k )))c k , with T k = exp \u2212 k \u2032 <k \u03c4 k \u2032 (t k \u2032 +1 \u2212 t k \u2032 ) ,(3)\nwhere C(r; \u0398, t) is the final predicted color of the pixel. With this procedure for rendering a NeRF parameterized by \u0398, training a NeRF is straightforward: using a set of observed images with known camera poses, we minimize the sum of squared differences between all input pixel values and all predicted pixel values using gradient descent. To improve sample efficiency, NeRF trains two separate MLPs, one \"coarse\" and one \"fine\", with parameters \u0398 c and \u0398 f :\nmin \u0398 c ,\u0398 f r\u2208R C * (r) \u2212 C(r; \u0398 c , t c ) 2 2 (4\n)\n+ C * (r) \u2212 C(r; \u0398 f , sort(t c \u222a t f )) 2 2 ,\nwhere C * (r) is the observed pixel color taken from the input image, and R is the set of all pixels/rays across all images. Mildenhall et al. construct t c by sampling 64 evenlyspaced random t values with stratified sampling. The compositing weights\nw k = T k (1 \u2212 exp(\u2212\u03c4 k (t k+1 \u2212 t k )\n)) produced by the \"coarse\" model are then taken as a piecewise constant PDF describing the distribution of visible scene content, and 128 new t values are drawn from that PDF using inverse transform sampling to produce t f . The union of these 192 t values are then sorted and passed to the \"fine\" MLP to produce a final predicted pixel color.", "publication_ref": ["b44", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "Method", "text": "As discussed, NeRF's point-sampling makes it vulnerable to issues related to sampling and aliasing: Though a pixel's color is the integration of all incoming radiance within the pixel's frustum, NeRF casts a single infinitesimally narrow ray per pixel, resulting in aliasing. Mip-NeRF ameliorates this issue by casting a cone from each pixel. Instead of performing point-sampling along each ray, we divide the cone being cast into a series of conical frustums (cones cut perpendicular to their axis). And instead of constructing positional encoding (PE) features from an infinitesimally small point in space, we construct an integrated positional encoding (IPE) representation of the volume covered by each conical frustum. These changes allow the MLP to reason about the size and shape of each conical frustum, instead of just its centroid. The ambiguity resulting from NeRF's insensitivity to scale and mip-NeRF's solution to this problem are visualized in Figure 3. This use of conical frustums and IPE features also allows us to reduce NeRF's two separate \"coarse\" and \"fine\" MLPs into a single multiscale MLP, which increases training and evaluation speed and reduces model size by 50%.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Cone Tracing and Positional Encoding", "text": "Here we describe mip-NeRF's rendering and featurization procedure, in which we cast a cone and featurize conical frustums along that cone. As in NeRF, images in mip-NeRF are rendered one pixel at a time, so we can describe our procedure in terms of an individual pixel of interest being rendered. For that pixel, we cast a cone from the cam-Figure 3: NeRF works by extracting point-sampled positional encoding features (shown here as dots) along each pixel's ray. Those point-sampled features ignore the shape and size of the volume viewed by each ray, so two different cameras imaging the same position at different scales may produce the same ambiguous point-sampled feature, thereby significantly degrading NeRF's performance. In contrast, Mip-NeRF casts cones instead of rays and explicitly models the volume of each sampled conical frustum (shown here as trapezoids), thus resolving this ambiguity. era's center of projection o along the direction d that passes through the pixel's center. The apex of that cone lies at o, and the radius of the cone at the image plane o + d is parameterized as\u1e59. We set\u1e59 to the width of the pixel in world coordinates scaled by 2 / \u221a 12, which yields a cone whose section on the image plane has a variance in x and y that matches the variance of the pixel's footprint. The set of positions x that lie within a conical frustum between two t values [t 0 , t 1 ] (visualized in Figure 1) is:\nF(x, o, d,\u1e59, t 0 , t 1 ) = 1 t 0 < d T (x \u2212 o) \u2225d\u2225 2 2 < t 1 \u2227 d T (x \u2212 o) \u2225d\u2225 2 \u2225x \u2212 o\u2225 2 > 1 1 + (\u1e59/\u2225d\u2225 2 ) 2 ,(5)\nwhere 1{\u2022} is an indicator function: F(x, \u2022) = 1 iff x is within the conical frustum defined by (o, d,\u1e59, t 0 , t 1 ).\nWe must now construct a featurized representation of the volume inside this conical frustum. Ideally, this featurized representation should be of a similar form to the positional encoding features used in NeRF, as Mildenhall et al. show that this feature representation is critical to NeRF's success [30]. There are many viable approaches for this (see the supplement for further discussion) but the simplest and most effective solution we found was to simply compute the expected positional encoding of all coordinates that lie within the conical frustum:\n\u03b3 * (o, d,\u1e59, t 0 , t 1 ) = \u03b3(x) F(x, o, d,\u1e59, t 0 , t 1 ) dx F(x, o, d,\u1e59, t 0 , t 1 ) dx . (6)\nHowever, it is unclear how such a feature could be computed efficiently, as the integral in the numerator has no closed form solution. We therefore approximate the conical frustum with a multivariate Gaussian which allows for an efficient approximation to the desired feature, which we will call an \"integrated positional encoding\" (IPE).\nTo approximate a conical frustum with a multivariate Gaussian, we must compute the mean and covariance of F(x, \u2022). Because each conical frustum is assumed to be circular, and because conical frustums are symmetric around the axis of the cone, such a Gaussian is fully characterized by three values (in addition to o and d): the mean distance along the ray \u00b5 t , the variance along the ray \u03c3 2 t , and the variance perpendicular to the ray \u03c3 2 r :\n\u00b5 t = t \u00b5 + 2t \u00b5 t 2 \u03b4 3t 2 \u00b5 + t 2 \u03b4 , \u03c3 2 t = t 2 \u03b4 3 \u2212 4t 4 \u03b4 (12t 2 \u00b5 \u2212 t 2 \u03b4 ) 15(3t 2 \u00b5 + t 2 \u03b4 ) 2 , \u03c3 2 r =\u1e59 2 t 2 \u00b5 4 + 5t 2 \u03b4 12 \u2212 4t 4 \u03b4 15(3t 2 \u00b5 + t 2 \u03b4 ) .(7)\nThese quantities are parameterized with respect to a midpoint t \u00b5 = (t 0 + t 1 )/2 and a half-width t \u03b4 = (t 1 \u2212 t 0 )/2, which is critical for numerical stability. Please refer to the supplement for a detailed derivation. We can transform this Gaussian from the coordinate frame of the conical frustum into world coordinates as follows:\n\u00b5 = o + \u00b5 t d , \u03a3 = \u03c3 2 t dd T + \u03c3 2 r I \u2212 dd T \u2225d\u2225 2 2 ,(8)\ngiving us our final multivariate Gaussian.\nNext, we derive the IPE, which is the expectation of a positionally-encoded coordinate distributed according to the aforementioned Gaussian. To accomplish this, it is helpful to first rewrite the PE in Equation 1 as a Fourier feature [35,44]:\nP = \uf8ee \uf8f0 1 0 0 2 0 0 2 L\u22121 0 0 0 1 0 0 2 0 \u2022 \u2022 \u2022 0 2 L\u22121 0 0 0 1 0 0 2 0 0 2 L\u22121 \uf8f9 \uf8fb T , \u03b3(x) = sin(Px) cos(Px) .(9)\nThis reparameterization allows us to derive a closed form for IPE. Using the fact that the covariance of a linear transformation of a variable is a linear transformation of the variable's covariance (Cov[Ax, By] = A Cov[x, y]B T ) we can identify the mean and covariance of our conical frustum Gaussian after it has been lifted into the PE basis P:\n\u00b5 \u03b3 = P\u00b5 , \u03a3 \u03b3 = P\u03a3P T .(10)\nThe final step in producing an IPE feature is computing the expectation over this lifted multivariate Gaussian, modulated by the sine and the cosine of position. These expectations have simple closed-form expressions:\nE x\u223cN (\u00b5,\u03c3 2 ) [sin(x)] = sin(\u00b5) exp \u2212( 1 /2)\u03c3 2 , (11\n)\nE x\u223cN (\u00b5,\u03c3 2 ) [cos(x)] = cos(\u00b5) exp \u2212( 1 /2)\u03c3 2 . (12\n)\nWe see that this expected sine or cosine is simply the sine or cosine of the mean attenuated by a Gaussian function of the variance. With this we can compute our final IPE feature as the expected sines and cosines of the mean and the diagonal of the covariance matrix:\n\u03b3(\u00b5, \u03a3) = E x\u223cN (\u00b5 \u03b3 ,\u03a3\u03b3 ) [\u03b3(x)](13)\n= sin(\u00b5 \u03b3 ) \u2022 exp(\u2212( 1 /2) diag(\u03a3 \u03b3 )) cos(\u00b5 \u03b3 ) \u2022 exp(\u2212( 1 /2) diag(\u03a3 \u03b3 )) ,(14)\nwhere \u2022 denotes element-wise multiplication. Because positional encoding encodes each dimension independently, this expected encoding relies on only the marginal distribution of \u03b3(x), and only the diagonal of the covariance matrix (a vector of per-dimension variances) is needed. Because \u03a3 \u03b3 is prohibitively expensive to compute due its relatively large size, we directly compute the diagonal of \u03a3 \u03b3 :\ndiag(\u03a3\u03b3) = diag(\u03a3), 4 diag(\u03a3), . . . , 4 L\u22121 diag(\u03a3) T(15)\nThis vector depends on just the diagonal of the 3D position's covariance \u03a3, which can be computed as:\ndiag(\u03a3) = \u03c3 2 t (d \u2022 d) + \u03c3 2 r 1 \u2212 d \u2022 d \u2225d\u2225 2 2 . (16\n)\nIf these diagonals are computed directly, IPE features are roughly as expensive as PE features to construct. Figure 4 visualizes the difference between IPE and conventional PE features in a toy 1D domain. IPE features behave intuitively: If a particular frequency in the positional encoding has a period that is larger than the width of the interval being used to construct the IPE feature, then the encoding at that frequency is unaffected. But if the period is smaller than the interval (in which case the PE over that interval will oscillate repeatedly), then the encoding at that frequency is scaled down towards zero. In short, IPE preserves frequencies that are constant over an interval and softly \"removes\" frequencies that vary over an interval, while PE preserves all frequencies up to some manuallytuned hyperparameter L. By scaling each sine and cosine in this way, IPE features are effectively anti-aliased positional encoding features that smoothly encode the size and shape of a volume of space. IPE also effectively removes L as a hyperparameter: it can simply be set to an extremely large value and then never tuned (see supplement).", "publication_ref": ["b30", "b35", "b44"], "figure_ref": ["fig_1", "fig_2"], "table_ref": []}, {"heading": "Architecture", "text": "Aside from cone-tracing and IPE features, mip-NeRF behaves similarly to NeRF, as described in Section 2.1. For each pixel being rendered, instead of a ray as in NeRF, a cone is cast. Instead of sampling n values for t k along the ray, we sample n + 1 values for t k , computing IPE features for the interval spanning each adjacent pair of sampled t k values as previously described. These IPE features Because NeRF samples points along each ray and encodes all frequencies equally, the highfrequency PE features are aliased, which results in rendering artifacts. By integrating PE features over each interval, the high frequency dimensions of IPE features shrink towards zero when the period of the frequency is small compared to the size of the interval being integrated, resulting in anti-aliased features that implicitly encode the size (and in higher dimensions, the shape) of the interval. are passed as input into an MLP to produce a density \u03c4 k and a color c k , as in Equation 2. Rendering in mip-NeRF follows Equation 3.\nRecall that NeRF uses a hierarchical sampling procedure with two distinct MLPs, one \"coarse\" and one \"fine\" (see Equation 4). This was necessary in NeRF because its PE features meant that its MLPs were only able to learn a model of the scene for one single scale. But our cone casting and IPE features allow us to explicitly encode scale into our input features and thereby enable an MLP to learn a multiscale representation of the scene. Mip-NeRF therefore uses a single MLP with parameters \u0398, which we repeatedly query in a hierarchical sampling strategy. This has multiple benefits: model size is cut in half, renderings are more accurate, sampling is more efficient, and the overall algorithm becomes simpler. Our optimization problem is:\nmin \u0398 r\u2208R \u03bb C * (r)\u2212C(r; \u0398, t c ) 2 2 + C * (r)\u2212C(r; \u0398, t f ) 2 2(17)\nBecause we have a single MLP, the \"coarse\" loss must be balanced against the \"fine\" loss, which is accomplished using a hyperparameter \u03bb (we set \u03bb = 0.1 in all experiments). As in Mildenhall et al. [30], our coarse samples t c are produced with stratified sampling, and our fine samples t f are sampled from the resulting alpha compositing weights w using inverse transform sampling. Unlike NeRF, in which the fine MLP is given the sorted union of 64 coarse samples and 128 fine samples, in mip-NeRF we simply sample 128 samples for the coarse model and 128 samples from the fine model (yielding the same number of total MLP evaluations as in NeRF, for fair comparison). Before sampling t f , we modify the weights w slightly:\nw \u2032 k = 1 2 (max(w k\u22121 , w k ) + max(w k , w k+1 )) + \u03b1 . (18\n)\nWe filter w with a 2-tap max filter followed by a 2-tap blur filter (a \"blurpool\" [51]), which produces a wide and smooth upper envelope on w. A hyperparameter \u03b1 is added to that envelope before it is re-normalized to sum to 1, which ensures that some samples are drawn even in empty regions of space (we set \u03b1 = 0.01 in all experiments). Mip-NeRF is implemented on top of JaxNeRF [11], a JAX [4] reimplementation of NeRF that achieves better accuracy and trains faster than the original TensorFlow implementation. We follow NeRF's training procedure: 1 million iterations of Adam [19] with a batch size of 4096 and a learning rate that is annealed logarithmically from 5 \u2022 10 \u22124 to 5 \u2022 10 \u22126 . See the supplement for additional details and some additional differences between JaxNeRF and mip-NeRF that do not affect performance significantly and are incidental to our primary contributions: cone-tracing, IPE, and the use of a single multiscale MLP.", "publication_ref": ["b30", "b50", "b11", "b3", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "We evaluate mip-NeRF on the Blender dataset presented in the original NeRF paper [30] and also on a simple multiscale variant of that dataset designed to better probe accuracy on multi-resolution scenes and to highlight NeRF's critical vulnerability on such tasks. We report the three error metrics used by NeRF: PSNR, SSIM [45], and LPIPS [52]. To enable easier comparison, we also present an \"average\" error metric that summarizes all three metrics: the geometric mean of MSE = 10 \u2212PSNR/10 , \u221a 1 \u2212 SSIM (as per [5]), and LPIPS. We additionally report runtimes (median and median absolute deviation of wall time) as well as the number of network parameters for each variant of NeRF and mip-NeRF. All JaxNeRF and mip-NeRF experiments are trained on a TPU v2 with 32 cores [17].\nWe constructed our multiscale Blender benchmark because the original Blender dataset used by NeRF has a subtle but critical weakness: all cameras have the same focal length and resolution and are placed at the same distance from the object. As a result, this Blender task is significantly easier than most real-world datasets, where cameras may be more close or more distant from the subject or may zoom in and out. The limitation of this dataset is complemented by the limitations of NeRF: despite NeRF's tendency to produce aliased renderings, it is able to produce excellent results on the Blender dataset because that dataset systematically avoids this failure mode.   NeRF on this dataset, we scale the loss of each pixel by the area of that pixel's footprint in the original image (the loss for pixels from the 1 /4 images is scaled by 16, etc) so that the few low-resolution pixels have comparable influence to the many high-resolution pixels. The average error metric for this task uses the arithmetic mean of each error metric across all four scales. The performance of mip-NeRF for this multiscale dataset can be seen in Table 1. Because NeRF is the state of the art on the Blender dataset (as will be shown in Table 2), we evaluate against only NeRF and several improved versions of NeRF: \"Area Loss\" adds the aforementioned scaling of the loss function by pixel area used by mip-NeRF, \"Centered Pixels\" adds a half-pixel offset added to each ray's direction such that rays pass through the center of each pixel (as opposed to the corner of each pixel as was done in Mildenhall et al.) and \"Misc\" adds some small changes that slightly improve the stability of training (see supplement). We also evaluate against several ablations of mip-NeRF: \"w/o Misc\" removes those small changes, \"w/o Single MLP\" uses NeRF's two-MLP training scheme from Equation 4, \"w/o Area Loss\" removes the loss scaling by pixel area, and \"w/o IPE\" uses PE instead of IPE, which causes mip-NeRF to use NeRF's ray-casting (with centered pixels) instead of our cone-casting.\nMip-NeRF reduces average error by 60% on this task and outperforms NeRF by a large margin on all metrics and all scales. \"Centering\" pixels improves NeRF's performance substantially, but not enough to approach mip-NeRF. Removing IPE features causes mip-NeRF's performance to degrade to the performance of \"Centered\" NeRF, thereby demonstrating that cone-casting and IPE features are the primary factors driving performance (though the area loss contributes substantially). The \"Single MLP\" mip-NeRF ablation performs well but has twice as many parameters and is nearly 20% slower than mip-NeRF (likely due to this ablation's need to sort t values and poor hardware through-  put due to its changing tensor sizes across its \"coarse\" and \"fine\" scales). Mip-NeRF is also \u223c 7% faster than NeRF. See Figure 9 and the supplement for visualizations. Blender Dataset Though the sampling issues that mip-NeRF was designed to fix are most prominent in the Multiscale Blender dataset, mip-NeRF also outperforms NeRF on the easier single-scale Blender dataset presented in Mildenhall et al. [30], as shown in Table 2. We evaluate against the baselines used by NeRF, NSVF [24], and the same variants and ablations that were used previously (excluding \"Area Loss\", which is not used by mip-NeRF for this task). Though less striking than the multiscale Blender dataset, mip-NeRF is able to reduce average error by \u223c 17% compared to NeRF while also being faster. This improvement in performance is most visually apparent in challenging cases such as small or thin structures, as shown in Figure 6. Supersampling As discussed in the introduction, mip-NeRF is a prefiltering approach for anti-aliasing. An alternative approach is supersampling, which can be accomplished in NeRF by casting multiple jittered rays per pixel. Because our multiscale dataset consists of downsampled   1 and 2).\nversions of full-resolution images, we can construct a \"supersampled NeRF\" by training a NeRF (the \"NeRF + Area, Center, Misc.\" variant that performed best previously) using only full-resolution images, and then rendering only fullresolution images, which we then manually downsample. This baseline has an unfair advantage: we manually remove the low-resolution images in the multiscale dataset, which would otherwise degrade NeRF's performance as previously demonstrated. This strategy is not viable in most real-world datasets, as it is usually not possible to known a-priori which images correspond to which scales of image content. Despite this baseline's advantage, mip-NeRF matches its accuracy while being \u223c 22\u00d7 faster (see Table 3).", "publication_ref": ["b30", "b45", "b51", "b5", "b17", "b30", "b24"], "figure_ref": [], "table_ref": ["tab_2", "tab_4", "tab_4", "tab_2", "tab_6"]}, {"heading": "Conclusion", "text": "We have presented mip-NeRF, a multiscale NeRF-like model that addresses the inherent aliasing of NeRF. NeRF works by casting rays, encoding the positions of points along those rays, and training separate neural networks at distinct scales. In contrast, mip-NeRF casts cones, encodes the positions and sizes of conical frustums, and trains a single neural network that models the scene at multiple scales. By reasoning explicitly about sampling and scale, mip-NeRF is able to reduce error rates relative to NeRF by 60% on our own multiscale dataset, and by 17% on NeRF's single-scale dataset, while also being 7% faster than NeRF. Mip-NeRF is also able to match the accuracy of a bruteforce supersampled NeRF variant, while being 22\u00d7 faster. We hope that the general techniques presented here will be valuable to other researchers working to improve the performance of raytracing-based neural rendering models.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Conical Frustum Integral Derivations", "text": "In order to derive formulas for the various moments of the uniform distribution over a conical frustum, we consider an axis-aligned cone parameterized as (x, y, z) = \u03c6(r, t, \u03b8) = (rt cos \u03b8, rt sin \u03b8, t) for \u03b8 \u2208 [0, 2\u03c0), t \u2265 0, |r| \u2264\u1e59. This change of variables from Cartesian space gives us a differential term:\ndx dy dz = | det(D\u03c6)(r, t, \u03b8)|dr dt d\u03b8(19)\n= t cos \u03b8 t sin \u03b8 0 r cos \u03b8 r sin \u03b8 1 \u2212rt sin \u03b8 rt cos \u03b8 0 dr dt d\u03b8 (20)\n= (rt 2 cos 2 \u03b8 + rt 2 sin \u03b8)dr dt d\u03b8\n= rt 2 dr dt d\u03b8 .\nThe volume of the conical frustum (which serves as the normalizing constant for the uniform distribution) is:\nV = 2\u03c0 0 t1 t0 \u1e59 0 rt 2 dr dt d\u03b8 (23) =\u1e59 2 2 \u2022 t 3 1 \u2212 t 3 0 3 \u2022 2\u03c0 (24) = \u03c0\u1e59 2 t 3 1 \u2212 t 3 0 3 (25)\nThus the probability density function for points uniformly sampled from the conical frustum is rt 2 /V . The first moment of t is:\nE[t] = 1 V 2\u03c0 0 t1 t0 \u1e59 0 t \u2022 rt 2 dr dt d\u03b8 (26) = 1 V 2\u03c0 0 t1 t0 \u1e59 0 rt 3 dr dt d\u03b8 (27) = 1 V \u2022 \u03c0\u1e59 2 t 4 1 \u2212 t 4 0 4 (28) = 3(t 4 1 \u2212 t 4 0 ) 4(t 3 1 \u2212 t 3 0 ) . (29\n)\nThe moments of x and y are both zero by symmetry. The second moment of t is\nE[t 2 ] = 1 V 2\u03c0 0 t1 t0 \u1e59 0 t 2 \u2022 rt 2 dr dt d\u03b8 (30) = 1 V 2\u03c0 0 t1 t0 \u1e59 0 rt 4 dr dt d\u03b8 (31) = 1 V \u2022 \u03c0\u1e59 2 t 5 1 \u2212 t 5 0 5 (32) = 3(t 5 1 \u2212 t 5 0 ) 5(t 3 1 \u2212 t 3 0 ) . (33\n)\nAnd the second moment of x is:\nE[x 2 ] = 1 V 2\u03c0 0 t1 t0 \u1e59 0 (rt cos \u03b8) 2 \u2022 rt 2 dr dt d\u03b8 (34) = 1 V t1 t0 \u1e59 0 r 3 t 4 2\u03c0 0 cos 2 \u03b8 d\u03b8 dr dt (35) = 1 V \u2022\u1e59 4 4 \u2022 t 5 1 \u2212 t 5 0 5 \u2022 \u03c0 (36) =\u1e59 2 4 \u2022 3(t 5 1 \u2212 t 5 0 ) 5(t 3 1 \u2212 t 3 0 ) . (37\n)\nThe second moment of y is the same by symmetry. All cross terms in the covariance are z, also by symmetry. With these moments defined, we can construct the mean and covariance for a random point within our conical frustum. The mean along the ray direction \u00b5 t is simply the first moment with respect to t:\n\u00b5 t = 3 t 4 1 \u2212 t 4 0 4(t 3 1 \u2212 t 3 0 ) . (38\n)\nThe variance of the conical frustum with respect to t follows from the definition of variance as\nVar(t) = E t 2 \u2212 E[t] 2 : \u03c3 2 t = 3 t 5 1 \u2212 t 5 0 5(t 3 1 \u2212 t 3 0 ) \u2212 \u00b5 2 t .(39)\nThe variance of the conical frustum with respect to its radius r is equal to the variance of the frustum with respect to x or (by symmetry) y. Since the first moment with respect to x is zero, the variance is equal to the second moment:\n\u03c3 2 r =\u1e59 2 3 t 5 1 \u2212 t 5 0 20(t 3 1 \u2212 t 3 0 ) . (40\n)\nComputing all three of these quantities in their given form is numerically unstable -the ratio of the differences between t 1 and t 0 raised to large powers is difficult to compute accurately when t 0 and t 1 are near each other, which occurs frequently during training. Using these quantities in practice often produces 0 or NaN instead of accurate values, which causes training to fail. We therefore reparameterize these equations as a function of the center and spread of t 0 and t 1 : t \u00b5 = (t 0 + t 1 )/2, t \u03b4 = (t 1 \u2212 t 0 )/2. This allows us to rewrite each mean and variance as a first-order term that is then corrected by higher-order terms, which are scaled by t \u03b4 . This gives us stable and accurate values even when t \u03b4 is small. Our reparameterized values are:  In NeRF, performance decreases due to overfitting for large values of L, but in mip-NeRF this parameter is effectively removed from tuning -it can just be set to a large value and forgotten, because IPE features \"tune\" their own frequencies automatically.\n\u00b5 t = t \u00b5 + 2t \u00b5 t 2 \u03b4 3t 2 \u00b5 + t 2 \u03b4 , \u03c3 2 t = t 2 \u03b4 3 \u2212 4t 4 \u03b4 (12t 2 \u00b5 \u2212 t 2 \u03b4 ) 15(3t 2 \u00b5 + t 2 \u03b4 ) 2 , \u03c3 2 r =\u1e59 2 t 2 \u00b5 4 + 5t 2 \u03b4 12 \u2212 4t 4 \u03b4 15(3t 2 \u00b5 + t 2 \u03b4 ) .(41)\nNote that our multivariate Gaussian approximation of a conical frustum will be inaccurate if there is a significant difference between the base and top radii of the frustum, which will be true for frustums that are very near the camera's center of projection when the camera FOV is large. This is highly uncommon in most datasets, but may be an issue if one were to use mip-NeRF in unusual circumstances, such as macro photography with a fisheye lens.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B. The L Hyperparameter in PE and IPE", "text": "IPE features can be viewed as a generalization of PE features: \u03b3(x) = \u03b3(\u00b5 = x, \u03a3 = 0). Or more rigorously, PE features can be thought of as \"hard\" IPE features in which all points are assumed to have identical isotropic covariance matrices whose variance has been heuristically determined by the L hyperparameter: the value of L determines the frequency at which PE features are truncated, just as the Gaussian function of variance in IPE serves as a \"soft\" truncation of IPE features. Because the \"soft\" maximum frequency of IPE features is determined entirely by the geometry and intrinsics of the camera, IPE features do not depend on the L hyperparameter, and so using IPE features removes the need for tuning L. This is because in PE the L parameter determines where the high frequencies in the PE are truncated, but in IPE those high frequencies are naturally attenuated by the size of the multivariate Gaussian used as input to the encoding: the smaller the Gaussian, the more high frequencies will be retained. To demonstrate this, we performed as \"sweep\" of L in both mip-NeRF and NeRF, and report the test-set PSNR for a single scene, which is visualized in Figure 7. We see that in NeRF, there is a range of values for L in which performance is maximized, but values that are too large or too small will hurt performance. But in mip-NeRF, we see that L can be set to an arbitrarily large value and performance is unaffected. In practice, in all mip-NeRF experiments in the paper we set L = 16, which is a value that results in the last dimension of all IPE features constructed during training to be less than numerical epsilon.", "publication_ref": [], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "C. Hyperparameters", "text": "In all experiments in the paper we take care to use exactly the same set of hyperparameters that were used in Mildenhall et al. [30], so as to isolate the specific contributions of mip-NeRF as they relate to cone-casting and IPE features. The three relevant hyperparameters that govern mip-NeRF's behavior are: 1) the number of samples N drawn at each of the two levels (N = 128), 2) the histogram \"padding\" hyperparameter \u03b1 on the coarse transmittance weights that are used to sample the fine t values (\u03b1 = 0.01), and 3) the multiplier \u03bb on the \"coarse\" component of the loss function (\u03bb = 0.1). And though mip-NeRF adds these three hyperparameters, it also deprecates three NeRF hyperparameters that are no longer used: 1) The number of samples N c drawn for the \"coarse\" MLP (N c = 64), 2) The number of samples N f drawn for the \"fine\" MLP (N f = 128), and 3) The degree L used for the spatial positional encoding (L = 10). The \u03b1 parameter used by mip-NeRF serves a similar purpose as the balance between N c and N f did in NeRF -a larger value of \u03b1 biases the final samples used during rendering towards a uniform distribution, just as a larger value of N c biases the final samples (which are the sorted union of the uniform coarse samples and the biased fine samples) towards a uniform distribution. Mip-NeRF's multiplier \u03bb has no analog in NeRF, as NeRF's usage of two distinct MLPs means that the \"coarse\" and \"fine\" losses in NeRF do not need to be balanced -thankfully, though mip-NeRF adds the need to tune this new hyperparameter \u03bb, it simultaneously removes the need to tune the L hyperparameter as discussed in Section B, so the total number of hyperparameters that require tuning remains constant across the two models.\nBefore running the experiments in the paper, we briefly tuned the \u03b1 and \u03bb hyperparameters by hand on the validation set of the lego scene. N was not tuned, and was just set to 128 such that the total number of MLP evaluations used by mip-NeRF matched the total number used by NeRF.", "publication_ref": ["b30"], "figure_ref": [], "table_ref": []}, {"heading": "D. Forward-Facing Scenes", "text": "Note that this paper does not evaluate on the LLFF dataset [29], which consists of scenes captured by a \"forward-facing\" handheld cellphone camera. For these scenes, NeRF trained and evaluated models in a \"normalized device coordinates\" (NDC) space. NDC coordinates work by nonlinearly warping a frustum-shaped space into a unit cube, which sidesteps some otherwise challenging design decisions (such as how an unbounded 3D space should be represented using positional encoding). NDC coordinates can only be used for these \"forward-facing\" scenes; in scenes where the camera rotates significantly (which is the case for the vast majority of 3D datasets) NeRF uses conventional 3D \"world coordinates\". One interesting consequence of NDC space is that the 3D volume corresponding to a pixel is not a frustum, but is instead a rectanglein NDC the spatial support of a pixel in the xy plane does not increase with the distance from the image plane, as it would in conventional projective geometry.\nWe briefly experimented with a variant of mip-NeRF that works in NDC space by casting cylinders instead of cones. The average PSNR achieved by JaxNeRF on this task is 26.843, and this cylinder-casting variant of mip-NeRF achieves an average PSNR of 26.838. Because this mip-NeRF variant roughly matches the accuracy of NeRF, the only substantial benefit it appears to provide is removing the need to tune the L parameter in positional encoding. This result provides some insight into why NeRF works so well on forward-facing scenes: in NDC space there is little difference between NeRF's \"incorrect\" aliased approach of casting rays and tuning the L hyperparameter (which as discussed in Section B, is approximately equivalent to using IPE features with isotropic Gaussians) and the more \"correct\" anti-aliased approach of mip-NeRF. In essence, NeRF is already able to get most of the benefit provided by conecasting and IPE features in NDC space, because in NDC space NeRF's aliased model is already very similar to mip-NeRF's approach. This interplay between scene parameterization and anti-aliasing suggests that a signal processing analysis of coordinate spaces in neural rendering problems may provide additional unexpected benefits or insights.", "publication_ref": ["b29"], "figure_ref": [], "table_ref": []}, {"heading": "E. Model Details", "text": "The primary contributions of this paper are the use of cone tracing, integrated positional encoding features, and our use of a single unified multiscale model (as opposed to NeRF's separate per-scale models), which together allow mip-NeRF to better handle multiscale data and reduce aliasing. Additionally, mip-NeRF includes a small number of changes that do not meaningfully change mip-NeRF's accuracy or speed, but slightly simplify our method and increase its robustness during optimization. These \"miscellaneous\" changes, as noted by the \"w/o Misc.\" ablation in the main paper, do not significantly affect mip-NeRF's performance, but are described here in full for the sake of reproducibility with the hopes that future work will find them useful.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.1. Identity Concatenation", "text": "In the original NeRF paper, the input to the MLP is not just the positional encoding of the position and view direction, but is instead the concatenation of the positional encoding with the position and view direction being encoded. We found this \"identity\" encoding to not contribute meaningfully to performance or speed, and its presence makes the formalization of our IPE features somewhat challenging, so this in mip-NeRF this identity mapping is removed and the only input to the MLP is the integrated positional encoding itself.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.2. Activation Functions", "text": "In the original NeRF paper, the activation functions used by the MLP to construct the predicted density \u03c4 and color c are a ReLU and a sigmoid, respectively. Instead of a ReLU as the activation function to produce \u03c4 , we use a shifted softplus: log(1 + exp(x \u2212 1)). We found that using a softplus yielded a smoother optimization problem that is less prone to catastrophic failure modes in which the MLP emits negative values everywhere (in which case all gradients from \u03c4 are zero and optimization will fail). The shift by \u22121 within the softplus is equivalent to initializing the biases that produce \u03c4 in mip-NeRF to \u22121, and this causes initial \u03c4 values to be small. Initializing the density of the NeRF to small values results in slightly faster optimization at the beginning of training, as dense scene content causes gradients from scene content \"behind\" that dense content to be suppressed. Instead of a sigmoid to produce color c, we use a \"widened\" sigmoid that saturates slightly outside of [0, 1] (the range of input RGB intensities): (1 + 2\u03f5)/(1 + exp(\u2212x)) \u2212 \u03f5, with \u03f5 = 0.001. This avoids an uncommon failure mode in which training tries to explain away a black or white pixel by saturating network activations into the tails of the sigmoid where the gradient is zero, which may cause optimization to fail. By having the network saturate at values slightly outside of the range of input values, activations are never encouraged to saturate. These changes to activation functions have little effect on performance, but we found that they improved training stability when using large learning rates (though all results in this paper use the same lower learning rates used by Mildenhall et al. [30] for fair comparison).", "publication_ref": ["b30"], "figure_ref": [], "table_ref": []}, {"heading": "E.3. Optimization", "text": "In all experiments we train mip-NeRF and JaxNeRF using the default training procedure specified in the JaxNeRF codebase: 1 million iterations of Adam [19] with a batch size of 4096 and a learning rate that is annealed logarithmically from \u03b7 0 = 5 \u2022 10 \u22124 to \u03b7 n = 5 \u2022 10 \u22126 . We additionally \"warm up\" the learning rate using the functionality provided by JaxNeRF, which does not improve the performance of mip-NeRF itself, but which we found to improve the stability of some of the mip-NeRF ablations. To allow   Learning Rate our ablations to be competitive, and to enable a fair comparison across all models, we therefore use this warm up strategy in all mip-NeRF and JaxNeRF experiments. Because the warm up procedure in JaxNeRF is not described in its documentation [11], for the sake of reproducibility we will describe it here. For the first n w = 2500 iterations of optimization, we scale the basic learning rate by an additional scale factor that is smoothly annealed between \u03bb w = 0.01 and 1 during this warm up period. The learning rate at iteration i during training is:\n\u03b7 i =(\u03bb w + (1 \u2212 \u03bb w ) sin(( \u03c0 /2) clip( i /nw, 0, 1))) \u00d7 (exp((1 \u2212 i /n) log(\u03b7 0 ) + ( i /n) log(\u03b7 n ))) (42)\nSee Figure 8 for a visualization.", "publication_ref": ["b19", "b11"], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "E.4. View Dependent Effects", "text": "We handle viewing directions exactly as was done in NeRF: the ray direction d is normalized, positionally encoded (L = 4), and injected into the last layer of the MLP after \u03c4 is predicted but before c is predicted. This is omitted from our notation in the main paper for simplicity's sake. see Mildenhall et al. for details [30].", "publication_ref": ["b30"], "figure_ref": [], "table_ref": []}, {"heading": "F. Supersampling Baseline", "text": "In the main paper we presented a generous baseline approach in which NeRF is trained on only full-resolution images (thereby sidestepping its poor performance when trained on multi-resolution data) and then evaluated on our multiscale Blender dataset by brute-force supersampling: rendering a full-resolution image that is then downsampled to match the resolution of the ground truth. This roughly matches the performance of mip-NeRF, but is 22\u00d7 slower and relies on \"oracle\" scale information that does not exist for most datasets. Here we explore an alternative supersampling baseline, in which we train an extension of NeRF on the multiscale dataset while supersampling during both training and evaluation: for every pixel we cast multiple jittered rays (sampled uniformly at random) through the spatial footprint of each pixel, render each ray with the NeRF, and then use the mean of those rendered values as the predicted color of that pixel in the loss function. As shown by the results of this experiment (Table 4) this brute-force supersampling model not only performs worse than mip-NeRF even when casting as many as 16 rays per pixel, but is also significantly more expensive during both training and evaluation.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_8"]}, {"heading": "G. Alternative Gaussian Positional Encoding", "text": "During experimentation we explored alternative approaches for featurizing the mean and covariance matrix of the multivariate Gaussians used by mip-NeRF. One such alternative strategy is to simply apply positional encoding to the mean and to the (signed) square root of the elements of the covariance matrix, and use the concatenation of the two as input. Specifically, we compute the positional encoding of \u00b5 with L = 12, and compute the positional encoding of vec(triu(sign(\u03a3) \u2022 |\u03a3|)) with L = 2. We found that this approach performs comparably to the IPE features presented in the main paper, as shown in Table 5  no hyperparameters (while this concatenation alternative is sensitive to its two L hyperparameters and the design decisions used when parameterizing \u03a3). This experiment with using this alternative to IPE also provides some insight into the inner workings of mip-NeRF. While IPE features are insensitive to the off-diagonal elements of \u03a3, this concatenation alternative should endow the MLP with the ability to reason about the correlation of dimensions of the multivariate Gaussian. The fact that this ability does not improve accuracy may suggest that correlation is not a helpful cue, which contradicted the intuition of the authors. Additionally, this experiment reinforces the assertions made in the paper that the reason for mip-NeRF's improved performance is its explicit modeling of conical frustums, as opposed to NeRF's usage of point samples along a ray. Though it is critical that the geometry of image formation be modeled accurately, there are likely many effective ways to featurize that geometry.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_10"]}, {"heading": "H. Additional Results", "text": "Multiscale Blender Dataset. To demonstrate the relative accuracy of mip-NeRF compared to NeRF on each individual scene in the multiscale Blender dataset, the error metrics for each individual scene are provided in Table 6. Mip-NeRF yields a significant reduction in error compared to NeRF across all scenes. Renderings produced by mip-NeRF and baseline algorithms compared to the ground truth can be visually inspected in Figures 9 and 10 ", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": ["tab_12"]}, {"heading": "", "text": "Acknowledgements We thank Janne Kontkanen and David Salesin for their comments on the text, Paul Debevec for constructive discussions, and Boyang Deng for JaxN-eRF. MT is funded by an NSF Graduate Fellowship.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Ray tracing with cones. SIGGRAPH", "journal": "", "year": "1984", "authors": "John Amanatides"}, {"ref_id": "b1", "title": "Neural reflectance fields for appearance acquisition. arXiv cs", "journal": "", "year": "2020", "authors": "Sai Bi; Zexiang Xu; Pratul P Srinivasan; Ben Mildenhall; Kalyan Sunkavalli; Milo\u0161 Ha\u0161an; Yannick Hold-Geoffroy; David Kriegman; Ravi Ramamoorthi"}, {"ref_id": "b2", "title": "NeRD: Neural reflectance decomposition from image collections", "journal": "", "year": "2020", "authors": "Mark Boss; Raphael Braun; Varun Jampani; Jonathan T Barron; Ce Liu; Hendrik P A Lensch"}, {"ref_id": "b3", "title": "", "journal": "Peter Hawkins", "year": "", "authors": "James Bradbury; Roy Frostig"}, {"ref_id": "b4", "title": "JAX: composable transformations of Python+NumPy programs", "journal": "", "year": "2018", "authors": "Matthew James Johnson; Chris Leary; Dougal Maclaurin; George Necula; Adam Paszke; Jake Vanderplas; Skye Wanderman-Milne; Qiao Zhang"}, {"ref_id": "b5", "title": "On the mathematical properties of the structural similarity index", "journal": "IEEE TIP", "year": "2011", "authors": "Dominique Brunet; R Edward; Zhou Vrscay;  Wang"}, {"ref_id": "b6", "title": "Unstructured lumigraph rendering", "journal": "SIGGRAPH", "year": "2001", "authors": "Chris Buehler; Michael Bosse; Leonard Mcmillan; Steven Gortler; Michael Cohen"}, {"ref_id": "b7", "title": "Plenoptic sampling. SIGGRAPH", "journal": "", "year": "2000", "authors": "Jin-Xiang Chai; Xin Tong; Shing-Chow Chan; Heung-Yeung Shum"}, {"ref_id": "b8", "title": "Periodic implicit generative adversarial networks for 3D-aware image synthesis. CVPR", "journal": "", "year": "2021", "authors": "Eric R Chan; Marco Monteiro; Petr Kellnhofer; Jiajun Wu; Gordon Wetzstein;  Pi-Gan"}, {"ref_id": "b9", "title": "Unstructured light fields", "journal": "Computer Graphics Forum", "year": "2012", "authors": "Abe Davis; Marc Levoy; Fredo Durand"}, {"ref_id": "b10", "title": "Modeling and rendering architecture from photographs: a hybrid geometry-and image-based approach", "journal": "", "year": "1992", "authors": "Paul Debevec; C J Taylor; Jitendra Malik"}, {"ref_id": "b11", "title": "JaxNeRF: an efficient JAX implementation of NeRF", "journal": "", "year": "2020", "authors": "Boyang Deng; Jonathan T Barron; Pratul P Srinivasan"}, {"ref_id": "b12", "title": "DeepView: View synthesis with learned gradient descent", "journal": "CVPR", "year": "2019", "authors": "John Flynn; Michael Broxton; Paul Debevec; Matthew Du-Vall; Graham Fyffe; Ryan Overbeck; Noah Snavely; Richard Tucker"}, {"ref_id": "b13", "title": "Dynamic neural radiance fields for monocular 4D facial avatar reconstruction", "journal": "CVPR", "year": "2021", "authors": "Guy Gafni; Justus Thies; Michael Zollh\u00f6fer; Matthias Nie\u00dfner"}, {"ref_id": "b14", "title": "The lumigraph. SIGGRAPH", "journal": "", "year": "1996", "authors": "J Steven; Radek Gortler; Richard Grzeszczuk; Michael F Szeliski;  Cohen"}, {"ref_id": "b15", "title": "Representing appearance and pre-filtering subpixel data in sparse voxel octrees", "journal": "", "year": "2012", "authors": "Eric Heitz; Fabrice Neyret"}, {"ref_id": "b16", "title": "Tracing ray differentials. SIGGRAPH", "journal": "", "year": "1999", "authors": "Homan Igehy"}, {"ref_id": "b17", "title": "In-datacenter performance analysis of a tensor processing unit", "journal": "", "year": "2017", "authors": "P Norman; Cliff Jouppi; Nishant Young; David Patil; Gaurav Patterson; Raminder Agrawal; Sarah Bajwa; Suresh Bates; Nan Bhatia; Al Boden;  Borchers"}, {"ref_id": "b18", "title": "Filtering distributions of normals for shading antialiasing", "journal": "HPG", "year": "2016", "authors": "A S Kaplanyan; S Hill; A Patney; A Lefohn"}, {"ref_id": "b19", "title": "Adam: A method for stochastic optimization", "journal": "ICLR", "year": "2015", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b20", "title": "NeuMIP: Multi-resolution neural materials", "journal": "ACM Trans. Graph", "year": "2021", "authors": "Alexandr Kuznetsov; Krishna Mullia; Zexiang Xu; Milo\u0161 Ha\u0161an; Ravi Ramamoorthi"}, {"ref_id": "b21", "title": "Efficient sparse voxel octrees", "journal": "", "year": "2010", "authors": "Samuli Laine; Tero Karras"}, {"ref_id": "b22", "title": "Light field rendering. SIG-GRAPH", "journal": "", "year": "1996", "authors": "Marc Levoy; Pat Hanrahan"}, {"ref_id": "b23", "title": "Neural scene flow fields for space-time view synthesis of dynamic scenes", "journal": "CVPR", "year": "2021", "authors": "Zhengqi Li; Simon Niklaus; Noah Snavely; Oliver Wang"}, {"ref_id": "b24", "title": "Kyaw Zaw Lin", "journal": "", "year": "2020", "authors": "Lingjie Liu; Jiatao Gu"}, {"ref_id": "b25", "title": "Learning dynamic renderable volumes from images. SIGGRAPH", "journal": "", "year": "2019", "authors": "Stephen Lombardi; Tomas Simon; Jason Saragih; Gabriel Schwartz"}, {"ref_id": "b26", "title": "Alexey Dosovitskiy, and Daniel Duckworth. NeRF in the Wild: Neural radiance fields for unconstrained photo collections. CVPR", "journal": "", "year": "2021", "authors": "Ricardo Martin-Brualla; Noha Radwan; S M Mehdi; Jonathan T Sajjadi;  Barron"}, {"ref_id": "b27", "title": "Optical models for direct volume rendering", "journal": "IEEE TVCG", "year": "1995", "authors": "Nelson Max"}, {"ref_id": "b28", "title": "Let there be color! Large-scale texturing of 3D reconstructions", "journal": "ECCV", "year": "2014", "authors": "Michael Michael Goesele; Nils Waechter;  Moehrle"}, {"ref_id": "b29", "title": "Local light field fusion: Practical view synthesis with prescriptive sampling guidelines", "journal": "SIGGRAPH", "year": "2019", "authors": "Ben Mildenhall; P Pratul; Rodrigo Srinivasan; Nima K Ortiz-Cayon; Ravi Kalantari; Ren Ramamoorthi; Abhishek Ng;  Kar"}, {"ref_id": "b30", "title": "Representing scenes as neural radiance fields for view synthesis. ECCV", "journal": "", "year": "2020", "authors": "Ben Mildenhall; P Pratul; Matthew Srinivasan; Jonathan T Tancik; Ravi Barron; Ren Ramamoorthi;  Ng;  Nerf"}, {"ref_id": "b31", "title": "Differentiable volumetric rendering: Learning implicit 3D representations without 3d supervision. CVPR", "journal": "", "year": "2020", "authors": "Michael Niemeyer; Lars Mescheder; Michael Oechsle; Andreas Geiger"}, {"ref_id": "b32", "title": "LEAN mapping. SIGGRAPH Symposium on Interactive 3D Graphics and Games", "journal": "", "year": "2010", "authors": "Marc Olano; Dan Baker"}, {"ref_id": "b33", "title": "Neural scene graphs for dynamic scenes", "journal": "CVPR", "year": "2021", "authors": "Julian Ost; Fahim Mannan; Nils Thuerey; Julian Knodt; Felix Heide"}, {"ref_id": "b34", "title": "Deformable neural radiance fields. arXiv cs.CV", "journal": "", "year": "2011", "authors": "Keunhong Park; Utkarsh Sinha; Jonathan T Barron; Sofien Bouaziz; Dan B Goldman; Steven M Seitz; Ricardo Martin-Brualla"}, {"ref_id": "b35", "title": "Random features for largescale kernel machines. NeurIPS", "journal": "", "year": "2007", "authors": "Ali Rahimi; Benjamin Recht"}, {"ref_id": "b36", "title": "Generative radiance fields for 3D-aware image synthesis. NeurIPS", "journal": "", "year": "2020", "authors": "Katja Schwarz; Yiyi Liao; Michael Niemeyer; Andreas Geiger;  Graf"}, {"ref_id": "b37", "title": "Photorealistic scene reconstruction by voxel coloring. IJCV", "journal": "", "year": "1999", "authors": "M Steven; Charles R Seitz;  Dyer"}, {"ref_id": "b38", "title": "Deep-Voxels: Learning persistent 3D feature embeddings", "journal": "CVPR", "year": "2019", "authors": "Vincent Sitzmann; Justus Thies; Felix Heide; Matthias Niessner; Gordon Wetzstein; Michael Zollh\u00f6fer"}, {"ref_id": "b39", "title": "Scene representation networks: Continuous 3D-structure-aware neural scene representations. NeurIPS", "journal": "", "year": "2019", "authors": "Vincent Sitzmann; Michael Zollhoefer; Gordon Wetzstein"}, {"ref_id": "b40", "title": "NeRV: Neural reflectance and visibility fields for relighting and view synthesis. CVPR", "journal": "", "year": "2021", "authors": "P Pratul; Boyang Srinivasan; Xiuming Deng; Matthew Zhang; Ben Tancik; Jonathan T Mildenhall;  Barron"}, {"ref_id": "b41", "title": "Pushing the boundaries of view extrapolation with multiplane images", "journal": "CVPR", "year": "2019", "authors": "P Pratul; Richard Srinivasan; Jonathan T Tucker; Ravi Barron; Ren Ramamoorthi; Noah Ng;  Snavely"}, {"ref_id": "b42", "title": "Neural geometric level of detail: Real-time rendering with implicit 3d shapes", "journal": "CVPR", "year": "2021", "authors": "Towaki Takikawa; Joey Litalien; Kangxue Yin; Karsten Kreis; Charles Loop; Derek Nowrouzezahrai; Alec Jacobson; Morgan Mcguire; Sanja Fidler"}, {"ref_id": "b43", "title": "Learned initializations for optimizing coordinate-based neural representations", "journal": "CVPR", "year": "2021", "authors": "Matthew Tancik; Ben Mildenhall; Terrance Wang; Divi Schmidt; Pratul P Srinivasan; Jonathan T Barron; Ren Ng"}, {"ref_id": "b44", "title": "Fourier features let networks learn high frequency functions in low dimensional domains", "journal": "NeurIPS", "year": "2020", "authors": "Matthew Tancik; P Pratul; Ben Srinivasan; Sara Mildenhall; Nithin Fridovich-Keil; Utkarsh Raghavan; Ravi Singhal; Jonathan T Ramamoorthi; Ren Barron;  Ng"}, {"ref_id": "b45", "title": "Image quality assessment: from error visibility to structural similarity", "journal": "IEEE TIP", "year": "2004", "authors": "Zhou Wang; Alan C Bovik; R Hamid; Eero P Sheikh;  Simoncelli"}, {"ref_id": "b46", "title": "An improved illumination model for shaded display", "journal": "Communications of the ACM", "year": "1980", "authors": "Turner Whitted"}, {"ref_id": "b47", "title": "Surface light fields for 3D photography. SIGGRAPH", "journal": "", "year": "2000", "authors": "Daniel Wood; Daniel Azuma; Wyvern Aldinger; Brian Curless; Tom Duchamp; David Salesin; Werner Stuetzle"}, {"ref_id": "b48", "title": "Accurate appearance preserving prefiltering for rendering displacement-mapped surfaces", "journal": "ACM TOG", "year": "2019", "authors": "Lifan Wu; Shuang Zhao; Ling-Qi Yan; Ravi Ramamoorthi"}, {"ref_id": "b49", "title": "Multiview neural surface reconstruction by disentangling geometry and appearance", "journal": "NeurIPS", "year": "2020", "authors": "Lior Yariv; Yoni Kasten; Dror Moran; Meirav Galun; Matan Atzmon; Basri Ronen; Yaron Lipman"}, {"ref_id": "b50", "title": "Making convolutional networks shiftinvariant again. ICML", "journal": "", "year": "2019", "authors": "Richard Zhang"}, {"ref_id": "b51", "title": "The unreasonable effectiveness of deep features as a perceptual metric", "journal": "CVPR", "year": "2018", "authors": "Richard Zhang; Phillip Isola; Alexei A Efros; Eli Shechtman; Oliver Wang"}, {"ref_id": "b52", "title": "Stereo magnification: Learning view synthesis using multiplane images", "journal": "SIGGRAPH", "year": "2018", "authors": "Tinghui Zhou; Richard Tucker; John Flynn; Graham Fyffe; Noah Snavely"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: NeRF (a) samples points x along rays that are traced from the camera center of projection through each pixel, then encodes those points with a positional encoding (PE) \u03b3 to produce a feature \u03b3(x). Mip-NeRF (b) instead reasons about the 3D conical frustum defined by a camera pixel. These conical frustums are then featurized with our integrated positional encoding (IPE), which works by approximating the frustum with a multivariate Gaussian and then computing the (closed form) integral E[\u03b3(x)] over the positional encodings of the coordinates within the Gaussian.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure4: Toy 1D visualizations of the positional encoding (PE) used by NeRF (left) and our integrated positional encoding (IPE) (right). Because NeRF samples points along each ray and encodes all frequencies equally, the highfrequency PE features are aliased, which results in rendering artifacts. By integrating PE features over each interval, the high frequency dimensions of IPE features shrink towards zero when the period of the frequency is small compared to the size of the interval being integrated, resulting in anti-aliased features that implicitly encode the size (and in higher dimensions, the shape) of the interval.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure5: Visualizations of the output of mip-NeRF compared to the ground truth, NeRF, and an improved version of NeRF on test set images from two scenes in our multiscale Blender dataset. We visualize a cropped region of both scenes at 4 different scales, displayed as an image pyramid with the SSIM for each scale shown to its lower right and with the highest SSIM at each scale highlighted in red. Mip-NeRF outperforms NeRF and its improved version by a significant margin, both visually and quantitatively. See the supplement for more such visualizations.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 7 :7Figure7: PSNRs for NeRF and mip-NeRF on the test set of the lego scene, as we vary the positional encoding degree L. In NeRF, performance decreases due to overfitting for large values of L, but in mip-NeRF this parameter is effectively removed from tuning -it can just be set to a large value and forgotten, because IPE features \"tune\" their own frequencies automatically.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 8 :8Figure 8: The learning rate used in all JaxNeRF and mip-NeRF experiments.", "figure_data": ""}, {"figure_label": "910", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 9 :Figure 10 :910Figure9: Visualizations of the output renderings from mip-NeRF compared to the ground truth, NeRF, and our improved version of NeRF, on test set images from the 8 scenes in our multiscale Blender dataset. We visualize a cropped region of each scene for better visualization, and render out that scene at 4 different resolutions, displayed as an image pyramid. The SSIM for each scale of each image pyramid truth is shown to its lower right, with the highest SSIM for each algorithm at each scale highlighted in red.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Even on the less challenging single-scale Blender dataset of Mildenhall et al.[30], mip-NeRF significantly outperforms NeRF and our improved version of NeRF, particularly on small or thin objects such as the holes of the LEGO truck (top) and the ropes of the ship (bottom).", "figure_data": "0.8090.8590.9230.6740.7140.772Ground-TruthNeRFNeRF+Cent,MiscMip-NeRFFigure 6: hours)# ParamsSRN [39]22.260.8460.1700.0735--Neural Volumes [25]26.050.8930.1600.0507--LLFF [29]24.880.9110.1140.0480\u223c0.16-NSVF [24]31.740.9530.0470.0190-3.2M -16MNeRF (TF Impl.) [30]31.010.9470.0810.0245>121,191KNeRF (Jax Impl.) [11, 30]31.740.9530.0500.01943.05 \u00b1 0.011,191KNeRF + Centered Pixels32.300.9570.0460.01782.99 \u00b1 0.061,191KNeRF + Center, Misc.32.280.9570.0460.01783.06 \u00b1 0.031,191KMip-NeRF33.090.9610.0430.0161 2.89 \u00b1 0.00612KMip-NeRF w/o Misc.33.040.9600.0430.0162 2.89 \u00b1 0.01612KMip-NeRF w/o Single MLP32.710.9590.0440.0168 3.63 \u00b1 0.021,191KMip-NeRF w/o IPE32.480.9580.0450.0173 2.84 \u00b1 0.00612K"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Avg. Time Full Res. 1 /2 Res. 1 /4 Res. 1 /8 Res.", "figure_data": "Mean(sec./MP)NeRF + Area, Center, Misc.29.9032.1333.4029.4731.232.61SS NeRF + Area, Center, Misc.32.2534.2735.9935.7334.5655.52Mip-NeRF32.6034.3035.4135.5534.462.48SS Mip-NeRF32.6034.7836.5936.1635.0352.75"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "A comparison of mip-NeRF and our improved NeRF variant where both algorithms are supersampled (\"SS\"). Mip-NeRF nearly matches the accuracy of \"SS NeRF\" while being 22\u00d7 faster. Adding supersampling to mip-NeRF improves its accuracy slightly. We report times for rendering the test set, normalized to seconds-permegapixel (training times are the same as Tables", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Train Time Test Time Full Res. 1 /2 Res. 1 /4 Res. 1 /8 Res. Full Res. 1 /2 Res. 1 /4 Res. 1 /8 Res. Full Res. 1 /2 Res. 1 /4 Res. 1 /8 Res Avg. \u2193 (hours) (sec/MP) # Params NeRF + Area, Center, 1\u00d7 SS 27.471 28.016 27.816 26.657 0.9187 0.9301 0.9365 0.9304 0.1064 0.0924 0.0934 0.1064 0.0362 Center, 16\u00d7 SS 31.566 33.116 33.982 32.933 0.9524 0.9660 0.9753 0.9768 0.0537 0.0316 0.0227 0.0216 0.0144", "figure_data": "PSNR \u2191SSIM \u2191LPIPS \u21932.852.611,191KNeRF + Area, Center, 4\u00d7 SS28.424 29.420 29.863 29.233 0.9297 0.9426 0.9526 0.9547 0.0807 0.0598 0.0530 0.0536 0.025917.6910.441,191KNeRF + Area, 37.1841.761,191KMip-NeRF32.629 34.336 35.471 35.602 0.9579 0.9703 0.9786 0.9833 0.0469 0.0260 0.0168 0.0120 0.01142.792.48612K"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Here we evaluate mip-NeRF against an extension of NeRF in which brute-force supersampling with jittered rays is used during training and evaluation, on our multiscale Blender dataset (\"16\u00d7 SS\" indicates that 16 rays are cast per pixel, etc). Mip-NeRF is able to outperform this baseline by a significant margin in terms of quality, while also being 13\u00d7 faster to train and 16\u00d7 faster to evaluate.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": ". We chose to advocate for IPE features in the main paper instead of this concatenation alternative because 1) IPE features are more compact (thereby reducing model size and evaluation time), 2) IPE features are easy to justify and reason about (as they approximate an expectation of positional encoding features with respect to a conical frustum), and 3) IPE features have Multiscale Blender PSNR \u2191 SSIM \u2191 LPIPS \u2193 Avg. \u2193", "figure_data": "Integrated PE34.510.9730.0250.0113Concatenated PE34.400.9730.0250.0114BlenderPSNR \u2191 SSIM \u2191 LPIPS \u2193 Avg. \u2193Integrated PE33.090.9610.0430.0161Concatenated PE33.090.9610.0420.0160"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "An evaluation of the IPE features against an alternative approach in which the mean and covariance of the multivariate Gaussian corresponding to a conical frustum are positionally encoded and concatenated. Both approaches perform comparably on the multiscale and singlescale Blender datasets.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Blender Dataset. Test-set error metrics for each individual scene in the (single scale) Blender dataset of Mildenhall et al.[30] can be seen in Table7. Mip-NeRF yields lower error rates than NeRF on all scenes and all metrics.", "figure_data": "Average PSNRchairdrumsficushotdoglegomaterialsmicshipNeRF (Jax Implementation) [11, 30]29.92323.27327.15332.00127.74826.29528.40126.462NeRF + Area Loss30.27724.03227.14932.02527.60226.53328.12026.834NeRF + Area, Centered Pixels33.46025.80230.40035.67231.60630.15532.63330.019NeRF + Area, Center, Misc.33.39425.87430.36935.64131.64630.18432.60130.092Mip-NeRF37.14127.02133.18839.31335.73632.55838.03633.083Mip-NeRF w/o Misc.37.27526.97933.16039.35735.74932.56337.99733.078Mip-NeRF w/o Single MLP37.31026.92233.04539.37835.60532.63538.01633.011Mip-NeRF w/o Area Loss35.18826.06332.54237.16534.31931.00435.92231.636Mip-NeRF w/o IPE33.55925.86430.49935.79331.72830.27232.73630.276Average SSIMchairdrumsficushotdoglegomaterialsmicshipNeRF (Jax Implementation) [11, 30]0.94360.89080.94230.95860.92560.93350.95800.8607NeRF + Area Loss0.94880.90280.94290.96220.92740.93720.95920.8610NeRF + Area, Centered Pixels0.97100.93100.97050.97940.96430.96700.98000.8994NeRF + Area, Center, Misc.0.97070.93180.97050.97930.96460.96710.97990.9004Mip-NeRF0.98750.94500.98360.98800.98430.97670.99280.9221Mip-NeRF w/o Misc.0.98770.94480.98350.98800.98420.97670.99270.9227Mip-NeRF w/o Single MLP0.98750.94320.98290.98760.98360.97630.99220.9211Mip-NeRF w/o Area Loss0.9817 0.93710.98230.9849 0.97920.97310.99110.9175Mip-NeRF w/o IPE0.97140.93220.97130.97960.96580.96780.98040.9039Average LPIPSchairdrumsficushotdoglegomaterialsmicshipNeRF (Jax Implementation) [11, 30]0.03470.06890.03240.02790.04100.04520.03070.0948NeRF + Area Loss0.04140.07620.04380.03650.05680.04990.04440.1139NeRF + Area, Centered Pixels0.02810.05930.02640.02400.03480.03300.02490.0865NeRF + Area, Center, Misc.0.02830.05860.02640.02410.03460.03300.02490.0850Mip-NeRF0.01110.04390.01350.01210.01270.01860.00650.0624Mip-NeRF w/o Misc.0.01110.04360.01360.01230.01270.01860.00660.0620Mip-NeRF w/o Single MLP0.01130.04430.01420.01220.01320.01870.00680.0628Mip-NeRF w/o Area Loss0.0171 0.05030.01460.01510.01630.02590.00950.0665Mip-NeRF w/o IPE0.02760.05780.02590.02400.03400.03200.02310.0829"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Per-scene results on the test set images of the multiscale Blender dataset presented in this work. We report the arithmetic mean of each metric averaged over the four scales used in the dataset.", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Per-scene results on the test set images of the (single-scale) Blender dataset from Mildenhall et al.[30] ", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u03b3(x) = sin(x), cos(x), . . . , sin 2 L\u22121 x , cos 2 L\u22121 x T . (1)", "formula_coordinates": [3.0, 313.74, 380.51, 231.37, 13.87]}, {"formula_id": "formula_1", "formula_text": "\u2200t k \u2208 t, [\u03c4 k , c k ] = MLP(\u03b3(r(t k )); \u0398) .(2)", "formula_coordinates": [3.0, 341.09, 532.72, 204.02, 9.68]}, {"formula_id": "formula_2", "formula_text": "C(r; \u0398, t) = k T k (1 \u2212 exp(\u2212\u03c4 k (t k+1 \u2212 t k )))c k , with T k = exp \u2212 k \u2032 <k \u03c4 k \u2032 (t k \u2032 +1 \u2212 t k \u2032 ) ,(3)", "formula_coordinates": [3.0, 322.9, 619.4, 222.21, 51.58]}, {"formula_id": "formula_3", "formula_text": "min \u0398 c ,\u0398 f r\u2208R C * (r) \u2212 C(r; \u0398 c , t c ) 2 2 (4", "formula_coordinates": [4.0, 63.61, 150.9, 218.88, 24.03]}, {"formula_id": "formula_4", "formula_text": ")", "formula_coordinates": [4.0, 282.49, 155.18, 3.87, 8.64]}, {"formula_id": "formula_5", "formula_text": "+ C * (r) \u2212 C(r; \u0398 f , sort(t c \u222a t f )) 2 2 ,", "formula_coordinates": [4.0, 105.55, 179.77, 167.31, 16.11]}, {"formula_id": "formula_6", "formula_text": "w k = T k (1 \u2212 exp(\u2212\u03c4 k (t k+1 \u2212 t k )", "formula_coordinates": [4.0, 117.85, 254.48, 141.85, 9.65]}, {"formula_id": "formula_7", "formula_text": "F(x, o, d,\u1e59, t 0 , t 1 ) = 1 t 0 < d T (x \u2212 o) \u2225d\u2225 2 2 < t 1 \u2227 d T (x \u2212 o) \u2225d\u2225 2 \u2225x \u2212 o\u2225 2 > 1 1 + (\u1e59/\u2225d\u2225 2 ) 2 ,(5)", "formula_coordinates": [4.0, 323.05, 438.7, 222.06, 61.3]}, {"formula_id": "formula_8", "formula_text": "\u03b3 * (o, d,\u1e59, t 0 , t 1 ) = \u03b3(x) F(x, o, d,\u1e59, t 0 , t 1 ) dx F(x, o, d,\u1e59, t 0 , t 1 ) dx . (6)", "formula_coordinates": [4.0, 317.85, 659.16, 227.26, 23.54]}, {"formula_id": "formula_9", "formula_text": "\u00b5 t = t \u00b5 + 2t \u00b5 t 2 \u03b4 3t 2 \u00b5 + t 2 \u03b4 , \u03c3 2 t = t 2 \u03b4 3 \u2212 4t 4 \u03b4 (12t 2 \u00b5 \u2212 t 2 \u03b4 ) 15(3t 2 \u00b5 + t 2 \u03b4 ) 2 , \u03c3 2 r =\u1e59 2 t 2 \u00b5 4 + 5t 2 \u03b4 12 \u2212 4t 4 \u03b4 15(3t 2 \u00b5 + t 2 \u03b4 ) .(7)", "formula_coordinates": [5.0, 61.72, 224.79, 224.64, 59.35]}, {"formula_id": "formula_10", "formula_text": "\u00b5 = o + \u00b5 t d , \u03a3 = \u03c3 2 t dd T + \u03c3 2 r I \u2212 dd T \u2225d\u2225 2 2 ,(8)", "formula_coordinates": [5.0, 55.09, 372.68, 231.27, 27.87]}, {"formula_id": "formula_11", "formula_text": "P = \uf8ee \uf8f0 1 0 0 2 0 0 2 L\u22121 0 0 0 1 0 0 2 0 \u2022 \u2022 \u2022 0 2 L\u22121 0 0 0 1 0 0 2 0 0 2 L\u22121 \uf8f9 \uf8fb T , \u03b3(x) = sin(Px) cos(Px) .(9)", "formula_coordinates": [5.0, 51.88, 480.53, 234.48, 44.21]}, {"formula_id": "formula_12", "formula_text": "\u00b5 \u03b3 = P\u00b5 , \u03a3 \u03b3 = P\u03a3P T .(10)", "formula_coordinates": [5.0, 107.78, 609.57, 178.59, 12.67]}, {"formula_id": "formula_13", "formula_text": "E x\u223cN (\u00b5,\u03c3 2 ) [sin(x)] = sin(\u00b5) exp \u2212( 1 /2)\u03c3 2 , (11", "formula_coordinates": [5.0, 66.95, 682.36, 215.27, 12.03]}, {"formula_id": "formula_14", "formula_text": ")", "formula_coordinates": [5.0, 282.21, 684.75, 4.15, 8.64]}, {"formula_id": "formula_15", "formula_text": "E x\u223cN (\u00b5,\u03c3 2 ) [cos(x)] = cos(\u00b5) exp \u2212( 1 /2)\u03c3 2 . (12", "formula_coordinates": [5.0, 65.84, 698.54, 216.37, 12.03]}, {"formula_id": "formula_16", "formula_text": ")", "formula_coordinates": [5.0, 282.21, 700.93, 4.15, 8.64]}, {"formula_id": "formula_17", "formula_text": "\u03b3(\u00b5, \u03a3) = E x\u223cN (\u00b5 \u03b3 ,\u03a3\u03b3 ) [\u03b3(x)](13)", "formula_coordinates": [5.0, 321.17, 141.72, 223.94, 11.45]}, {"formula_id": "formula_18", "formula_text": "= sin(\u00b5 \u03b3 ) \u2022 exp(\u2212( 1 /2) diag(\u03a3 \u03b3 )) cos(\u00b5 \u03b3 ) \u2022 exp(\u2212( 1 /2) diag(\u03a3 \u03b3 )) ,(14)", "formula_coordinates": [5.0, 357.15, 158.72, 187.96, 22.75]}, {"formula_id": "formula_19", "formula_text": "diag(\u03a3\u03b3) = diag(\u03a3), 4 diag(\u03a3), . . . , 4 L\u22121 diag(\u03a3) T(15)", "formula_coordinates": [5.0, 315.7, 277.61, 229.41, 13.87]}, {"formula_id": "formula_20", "formula_text": "diag(\u03a3) = \u03c3 2 t (d \u2022 d) + \u03c3 2 r 1 \u2212 d \u2022 d \u2225d\u2225 2 2 . (16", "formula_coordinates": [5.0, 331.96, 334.89, 209.0, 26.32]}, {"formula_id": "formula_21", "formula_text": ")", "formula_coordinates": [5.0, 540.96, 341.98, 4.15, 8.64]}, {"formula_id": "formula_22", "formula_text": "min \u0398 r\u2208R \u03bb C * (r)\u2212C(r; \u0398, t c ) 2 2 + C * (r)\u2212C(r; \u0398, t f ) 2 2(17)", "formula_coordinates": [6.0, 50.11, 575.02, 236.25, 30.32]}, {"formula_id": "formula_23", "formula_text": "w \u2032 k = 1 2 (max(w k\u22121 , w k ) + max(w k , w k+1 )) + \u03b1 . (18", "formula_coordinates": [6.0, 315.94, 132.42, 225.02, 22.31]}, {"formula_id": "formula_24", "formula_text": ")", "formula_coordinates": [6.0, 540.96, 139.48, 4.15, 8.64]}, {"formula_id": "formula_25", "formula_text": "dx dy dz = | det(D\u03c6)(r, t, \u03b8)|dr dt d\u03b8(19)", "formula_coordinates": [11.0, 70.68, 179.36, 215.68, 8.96]}, {"formula_id": "formula_28", "formula_text": "V = 2\u03c0 0 t1 t0 \u1e59 0 rt 2 dr dt d\u03b8 (23) =\u1e59 2 2 \u2022 t 3 1 \u2212 t 3 0 3 \u2022 2\u03c0 (24) = \u03c0\u1e59 2 t 3 1 \u2212 t 3 0 3 (25)", "formula_coordinates": [11.0, 106.21, 311.26, 180.16, 79.2]}, {"formula_id": "formula_29", "formula_text": "E[t] = 1 V 2\u03c0 0 t1 t0 \u1e59 0 t \u2022 rt 2 dr dt d\u03b8 (26) = 1 V 2\u03c0 0 t1 t0 \u1e59 0 rt 3 dr dt d\u03b8 (27) = 1 V \u2022 \u03c0\u1e59 2 t 4 1 \u2212 t 4 0 4 (28) = 3(t 4 1 \u2212 t 4 0 ) 4(t 3 1 \u2212 t 3 0 ) . (29", "formula_coordinates": [11.0, 90.25, 445.97, 196.11, 111.13]}, {"formula_id": "formula_30", "formula_text": ")", "formula_coordinates": [11.0, 282.21, 539.77, 4.15, 8.64]}, {"formula_id": "formula_31", "formula_text": "E[t 2 ] = 1 V 2\u03c0 0 t1 t0 \u1e59 0 t 2 \u2022 rt 2 dr dt d\u03b8 (30) = 1 V 2\u03c0 0 t1 t0 \u1e59 0 rt 4 dr dt d\u03b8 (31) = 1 V \u2022 \u03c0\u1e59 2 t 5 1 \u2212 t 5 0 5 (32) = 3(t 5 1 \u2212 t 5 0 ) 5(t 3 1 \u2212 t 3 0 ) . (33", "formula_coordinates": [11.0, 85.78, 601.23, 200.58, 111.13]}, {"formula_id": "formula_32", "formula_text": ")", "formula_coordinates": [11.0, 282.21, 695.02, 4.15, 8.64]}, {"formula_id": "formula_33", "formula_text": "E[x 2 ] = 1 V 2\u03c0 0 t1 t0 \u1e59 0 (rt cos \u03b8) 2 \u2022 rt 2 dr dt d\u03b8 (34) = 1 V t1 t0 \u1e59 0 r 3 t 4 2\u03c0 0 cos 2 \u03b8 d\u03b8 dr dt (35) = 1 V \u2022\u1e59 4 4 \u2022 t 5 1 \u2212 t 5 0 5 \u2022 \u03c0 (36) =\u1e59 2 4 \u2022 3(t 5 1 \u2212 t 5 0 ) 5(t 3 1 \u2212 t 3 0 ) . (37", "formula_coordinates": [11.0, 318.95, 92.02, 226.16, 111.13]}, {"formula_id": "formula_34", "formula_text": ")", "formula_coordinates": [11.0, 540.96, 185.82, 4.15, 8.64]}, {"formula_id": "formula_35", "formula_text": "\u00b5 t = 3 t 4 1 \u2212 t 4 0 4(t 3 1 \u2212 t 3 0 ) . (38", "formula_coordinates": [11.0, 390.97, 290.64, 149.99, 26.6]}, {"formula_id": "formula_36", "formula_text": ")", "formula_coordinates": [11.0, 540.96, 299.91, 4.15, 8.64]}, {"formula_id": "formula_37", "formula_text": "Var(t) = E t 2 \u2212 E[t] 2 : \u03c3 2 t = 3 t 5 1 \u2212 t 5 0 5(t 3 1 \u2212 t 3 0 ) \u2212 \u00b5 2 t .(39)", "formula_coordinates": [11.0, 379.14, 336.27, 165.97, 48.57]}, {"formula_id": "formula_38", "formula_text": "\u03c3 2 r =\u1e59 2 3 t 5 1 \u2212 t 5 0 20(t 3 1 \u2212 t 3 0 ) . (40", "formula_coordinates": [11.0, 376.16, 450.01, 164.8, 26.59]}, {"formula_id": "formula_39", "formula_text": ")", "formula_coordinates": [11.0, 540.96, 459.28, 4.15, 8.64]}, {"formula_id": "formula_40", "formula_text": "\u00b5 t = t \u00b5 + 2t \u00b5 t 2 \u03b4 3t 2 \u00b5 + t 2 \u03b4 , \u03c3 2 t = t 2 \u03b4 3 \u2212 4t 4 \u03b4 (12t 2 \u00b5 \u2212 t 2 \u03b4 ) 15(3t 2 \u00b5 + t 2 \u03b4 ) 2 , \u03c3 2 r =\u1e59 2 t 2 \u00b5 4 + 5t 2 \u03b4 12 \u2212 4t 4 \u03b4 15(3t 2 \u00b5 + t 2 \u03b4 ) .(41)", "formula_coordinates": [11.0, 320.47, 650.39, 224.64, 59.35]}, {"formula_id": "formula_41", "formula_text": "\u03b7 i =(\u03bb w + (1 \u2212 \u03bb w ) sin(( \u03c0 /2) clip( i /nw, 0, 1))) \u00d7 (exp((1 \u2212 i /n) log(\u03b7 0 ) + ( i /n) log(\u03b7 n ))) (42)", "formula_coordinates": [14.0, 63.45, 554.81, 222.91, 24.6]}], "doi": ""}