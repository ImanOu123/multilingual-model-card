{"title": "Supervised Clustering of Streaming Data for Email Batch Detection", "authors": "Peter Haider; Tobias Scheffer", "pub_date": "", "abstract": "We address the problem of detecting batches of emails that have been created according to the same template. This problem is motivated by the desire to filter spam more effectively by exploiting collective information about entire batches of jointly generated messages. The application matches the problem setting of supervised clustering, because examples of correct clusterings can be collected. Known decoding procedures for supervised clustering are cubic in the number of instances. When decisions cannot be reconsidered once they have been made -owing to the streaming nature of the data -then the decoding problem can be solved in linear time. We devise a sequential decoding procedure and derive the corresponding optimization problem of supervised clustering. We study the impact of collective attributes of email batches on the effectiveness of recognizing spam emails.", "sections": [{"heading": "Introduction", "text": "Senders of spam, phishing, and virus emails avoid mailing multiple identical copies of their messages. Once a message is known to be malicious, all subsequent identical copies of the message could be blocked easily, and without any risk of erroneously blocking regular emails. Collective features of jointly generated batches of messages could provide additional hints for automatic classification, if batches could be recognized as such. Tools for spam, phishing, and virus dissemination employ templates and stochastic grammars, for text messages as well as for images and the source code of viruses. The templates are instantiated for each message. Table 1 shows two illustrative spam messages, generated from the same template.\nA natural approach to identifying batches in incoming messages is to cluster groups of similar instances. But unlike for exploratory data analysis, a ground truth of correct clusterings exists. In order to decide which technique to use, one has to consider the characteristics of electronic messaging.\nThe overall amount of spam in electronic messages is estimated to be approximately 80 percent. Currently, 80 to 90 percent of these messages are generated by only a few spam senders, each of them maintaining a small number of templates at a time, but exchanging them rapidly. Thus, examining the total email traffic of a short time window, the bulk of incoming messages has been generated by a small number of templates while the remaining 20 percent cover newsletters, personal, and business communications. In a clustering solution, the latter would result in a large number of singleton clusters while newsletters and spam batches congregate in many large and some very large groups. An appropriate clustering algorithm needs to allow for arbitrarily many clusters and an adjustable similarity measure that can be adapted to yield the ground truth of correct clusterings.\nAt first blush, correlation clustering meets all these requirements. Finley and Joachims (2005) adapt the similarity measure of correlation clustering by structural support vector machines. The solution is equivalent to a poly-cut in a fully connected graph spanned by the messages and their pairwise similarities. However, this solution ignores the temporal structure of the data. And although training can be performed offline, the correlation clustering procedure has to make a decision for each incoming message in real time as to whether it is part of a batch. Larger email service providers have to deal with an amount of emails in the order of 10 8 emails each day. Being cubic in the We devise a sequential clustering technique that overcomes these drawbacks. Exploiting the temporal nature of the data, it is linear in the number of instances. Sequential clustering can easily be integrated in structural SVMs, allowing for the similarity measure to be adapted on a labeled training set.\nOur paper is structured as follows. We discuss related work in Section 2 and introduce the problem setting in Section 3. In Section 4, we derive a learning method starting from a relaxed clustering variant. In Section 5, we exploit the temporal nature of the data and devise a sequential clustering algorithm with an appropriate learning variant. We report on experimental results in Section 6. Section 7 concludes.", "publication_ref": ["b6"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Related Work", "text": "Prior work on clustering of streaming data mainly focused on finding single-pass approximations to k-Center algorithms. Guha et al. (2003) develop a constant-factor approximation to k-Median clustering, whereas Ordonez (2003) use an incremental version of k-Means for clustering streams of binary data.\nPrior information about the clustering structure of a data set allows for enhancements to clustering algorithms such as k-Means. For instance, Wagstaff et al. (2001) incorporate the background knowledge as mustlink and cannot-link constraints into the clustering process, while Bar-Hillel et al. (2003) and Xing et al. (2002) learn a metric over the data space that incorporates the prior knowledge.\nUsing batch information for spam classification has been studied for settings where multiple users receive spam emails from the same batch. Gray and Haahr (2004) as well as Damiani et al. (2004) discuss difficulties concerning the distribution of batch information and trust between users, while mostly heuristics are used to identify duplicate emails from the same batch. More sophisticated exploration of robust identification of duplicates has been done in other domains. Learning adaptive similarity measures from data has previously been studied by Ristad and Yianilos (1997).\nCorrelation clustering on fully connected graphs is introduced in (Bansal et al., 2002). A generalization to arbitrary graphs is presented in (Charikar et al., 2005), and Emanuel and Fiat (2003) show the equivalence to a poly-cut problem. Approximation strategies to the NP-complete decoding are presented in (Demaine & Immorlica, 2003;Swamy, 2004). Finley and Joachims (2005) investigated supervised clustering with structural support vector machines.\nSeveral discriminative algorithms have been studied that use joint spaces of input and output variables; these include max-margin Markov models (Taskar et al., 2004) and structural support vector machines (Tsochantaridis et al., 2005). These methods use kernels to compute the inner product in input output space. This approach allows to capture arbitrary dependencies between inputs and outputs. An application-specific learning method is constructed by defining appropriate features, and choosing a decoding procedure that efficiently calculates the argmax, exploiting the dependency structure of the features.", "publication_ref": ["b8", "b9", "b14", "b1", "b15", "b7", "b3", "b10", "b0", "b2", "b5", "b4", "b11", "b6", "b12", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Problem Setting", "text": "In this section, we abstract the problem of detecting batches in an email stream into a well-defined problem setting. We decompose the problem into decoding and parameter estimation and derive an appropriate loss function for the parameter estimation step.\nA mail transfer agent processes a continuous stream of messages; for each message, it needs to decide which action to take. Possible actions are to accept the message from the connecting agent and to deliver it to the recipient; to reject the message within the SMTP session; or to accept the message and file it into the recipient's spam folder. We focus on the decision on which messages are part of the same batch. The policy on a final action to take can depend on whether this batch is already blacklisted as being malicious, and possibly on the output of a classifier that uses information in the email as well as in the entire batch.\nThe agent can take only a fixed number of messages into account when making decisions, for obvious memory constraints. We model the problem such that at each time, a window of messages x is visible. The output is an adjacency matrix y, where y jk = 1 if x j and x k are elements of the same batch, and 0 otherwise.\nTraining data consists of n sets of training emails x (1) , . . . , x (n) with T (1) , . . . T (n) elements. Each set x (i) represents a snapshot of the window of observable messages. For each training set we are given the correct partitioning into batches and singleton emails by means of adjacency matrices y (1) , . . . , y (n) .\nA set of pairwise feature functions \u03c6 d : (x j , x k ) \u2192 r \u2208 R with d = 1, . . . , D is available. The feature functions implement aspects of the correspondence between x j and x k . Examples of such functions are the TFIDF similarity of the message bodies, the edit distance of the subject lines, or the similarity of color histograms of images included in the messages. All feature functions are stacked into a similarity vector \u03a6(x j , x k ).\nThe desired solution is a procedure that produces an adjacency matrix minimizing the number of incorrect assignments of emails to batches, where incorrect refers to the ground truth that is reflected in the training data. The number of incorrect assignments is measured by the following loss function \u2206 : (y,\u0177) \u2192 r \u2208 R + 0 . Mis-assigning an element x j to a batch corrupts a number of matrix elements y jk equal to the size of the batch. Intuitively, mis-assigning a message to a small batch is as bad as mis-assigning it to a large batch. Therefore, in order to quantify the total number of incorrect assignments, the number of bad links for each x j is divided by the size of the batch that x i is assigned to:\n\u2206 N (y,\u0177) = j,k:k<j |y jk \u2212\u0177 jk | k =j y k k .\nWe will now introduce the model parameters and decompose the problem into decoding and parameter estimation. It is natural to find a similarity value sim w (x j , x k ) by linearly combining the pairwise feature functions with a weight vector w, forging the parameterized similarity measure of Equation 1.\nsim w (x j , x k ) = D d=1 w d \u03c6 d (x j , x k ) = w \u03a6(x j , x k ) (1)\nApplying the similarity function to all pairs of emails in a set yields a similarity matrix. The problem of cre-ating a consistent clustering of instances from a similarity matrix is equivalent to the problem of correlation clustering (Bansal et al., 2002).\nGiven the parameters w, the decoding problem is to produce an adjacency matrix\u0177 = argmax y f (x, y) that maximizes a decision function f , subject to the constraint that\u0177 be a consistent clustering. In standard correlation clustering, the objective is the intracluster similarity:\nf (x, y) = j,k y jk sim w (x j , x k ).\n(\n)2\nThe parameter learning problem is to obtain weights w such that, for a new stream of messages, the wparameterized decoding procedure produces clusterings that minimize risk; i.e., the expected loss\nR(f ) = \u2206(y, argmax\u0233 f (x,\u0233))p(x, y)dxdy,(3)\nwhere p(x, y) is the (unknown) distribution of sets of objects and their correct clusterings.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "Learning to Cluster", "text": "Supervised clustering elegantly fits into the framework of learning support vector machines with structured output spaces (Tsochantaridis et al., 2005). Finley and Joachims ( 2005) use an iterative algorithm for learning the weight vector; it starts with an empty set of constraints and adds the most strongly violated constraint in each iteration. We briefly review the model and decoding problem and derive the parameter optimization problem for our loss function. We arrive at a compact optimization problem that can be solved using standard tools instead of an iterative procedure.\nIn standard correlation clustering, the decision function to be maximized by the clustering is the intracluster similarity. Substituting Equation 1 into Equation 4 shows that the decision function is an inner product of parameters and a vector \u03a8(x, y) that jointly represents input x and output y (Equation 5).\nf (x, y) = T t=1 t\u22121 k=1 y tk sim w (x t , x k ) (4) = T t=1 t\u22121 k=1 y tk w \u03a6(x t , x k ) = w T t=1 t\u22121 k=1 y tk \u03a6(x t , x k ) = w \u03a8(x, y). (5\n)\nGiven parameters w and a set of instances x, the decoding problem is to find the highest-scoring clusterin\u011d\ny = argmax y f (x, y) s.t. \u2200 jkl : (1 \u2212 y jk ) + (1 \u2212 y kl ) \u2265 (1 \u2212 y jl ) (6) \u2200 jk : y jk \u2208 {0, 1}.\nEquation 6 requires\u0177 to be a consistent clustering: if x j and x k are elements of the same cluster and x k and x l are in the same cluster, then x j and x l have to be in the same cluster as well. Unfortunately, maximizing f (x, y) over integer assignments of matrix elements y jk is NP-complete. A common approach is to approximate it by relaxing the binary edge labels y jk to continuous variables z\njk \u2208 [0, 1]. z = argmax z f (x, z) s.t. \u2200 jkl : (1\u2212z jk ) + (1\u2212z kl ) \u2265 (1\u2212z jl ) (7) \u2200 jk : z jk \u2208 [0, 1]\nWe refer to this decoding strategy as the LP decoding; it is cubic in the size of the window x. Parameter w is chosen as to minimize the regularized empirical counterpart of the risk in Equation 3 (Tsochantaridis et al., 2005):\nmin 1 2 w 2 + C n i=1 \u03be (i) (8) s.t. \u2200 i w \u03a8(x (i) , y (i) ) + \u03be (i) \u2265 max y w \u03a8(x (i) ,\u0233) + \u2206(y (i) ,\u0233) (9\n)\n\u2200 i \u03be (i) \u2265 0. (10\n)\nReplacing the right-hand side of constraint 9 with their continuous approximations and substituting the normalized loss function \u2206 N , we can write it as\nmax z w \u03a8(x (i) ,z) + \u2206 N (y (i) ,z) = max z w \u03a8(x (i) ,z) + k<j |y (i) jk \u2212z jk | k =j y (i) k k = max z d (i) + j,k<j z (i) jk (w \u03a6(x (i) j , x (i) k ) \u2212 e (i) jk ),\nwhere\nd (i) = j,k<j y (i) jk P k =j y (i) k k\nand e\n(i) jk = 2y (i) jk \u22121 P k =j y (i) k k ,\nandz ranges over all relaxed adjacency matrices which satisfy the triangle inequality (Equation 7). Integrating these constraints into the objective function leads to the corresponding Lagrangian\nL(z (i) , \u03bb (i) , \u03bd (i) , \u03ba (i) ) = d (i) + \u03bd (i) 1 + \u03bb (i) 1 + h \u03a6(x (i) )w \u2212 e (i) \u2212 A (i) \u03bb (i) \u2212 \u03bd (i) + \u03ba (i) i z (i) ,\nwhere the coefficient matrix A (i) is defined as\nA (i) jkl,j k = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 +1 : if (j = j \u2227 k = k) : \u2228(j = k \u2227 k = l) \u22121 : if j = j \u2227 k = l 0 : otherwise.\nThe substitution of the derivatives with respect to z (i) into the Lagrangian and elimination of \u03ba (i) removes its dependence on the primal variables and we resolve the corresponding dual that is given by min\n\u03bb (i) ,\u03bd (i) d (i) + \u03bd (i) 1 + \u03bb (i) 1 s.t. \u03a6(x (i) )w \u2212 e (i) \u2212 A (i) \u03bb (i) \u2212 \u03bd (i) \u2264 0 \u03bb (i) , \u03bd (i) \u2265 0.\nStrong duality holds and the minimization over \u03bb and \u03bd can be combined with the minimization over w. The reintegration into Equations 8-10 finally leads to the integrated Optimization Problem 1.\nOptimization Problem 1 Given n labeled clusterings, C > 0; over all w, \u03be (i) , \u03bb (i) , and \u03bd (i) , minimize\n1 2 ||w|| 2 + C n i=1 \u03be (i) subject to the constraints \u2200 n i=1 w \u03a8(x (i) , y (i) ) + \u03be (i) \u2265 d (i) + \u03bd (i) 1 + \u03bb (i) 1, \u2200 n i=1 w \u03a6(x (i) ) \u2212 e (i) \u2264 A (i) \u03bb (i) + \u03bd (i) , \u2200 n i=1 \u03bb (i) , \u03bd (i) \u2265 0.\nOptimization Problem 1 can be solved directly using standard QP-solvers. Because of the cubic number of triangle inequalities, the number of Lagrange multipliers \u03bb (i) in Optimization Problem 1 is cubic in the number of emails T (i) per set. Finley and Joachims (2005) chose a similar approach but arrive at an iterative algorithm to learn the weight vector. The iterative algorithm represents only a subset of the constraints and therefore achieves a speedup at training time. In our case, the training samples are modestly sized whereas, at application time, a high-speed stream has to be processed. Therefore, we will develop a linear decoder in the next section. The linear decoder will also reduce the complexity of the parameter optimization problem from cubic to quadratic.", "publication_ref": ["b13", "b13", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Clustering of Streaming Data", "text": "In our batch detection application, incoming emails are processed sequentially. The decision on the cluster assignment has to be made immediately, within an SMTP session, and cannot be altered thereafter. Because of the high volume of the email stream, any\nAlgorithm 1 Sequential Clustering C \u2190 {} for t = 1 . . . T do c j \u2190 argmax c\u2208C x k \u2208c w \u03a6(x k , x t ) if x k \u2208c j w \u03a6(x k , x t ) < 0 then C \u2190 C \u222a {{x t }} else C \u2190 C \\ {c j } \u222a {c j \u222a {x t }} end if end for return C\ndecoding algorithm requiring more than linear execution time in the number of emails processed and the number of emails in the window would be prohibitive.\nWe therefore impose the constraint that cluster membership cannot be reconsidered once a decision has been made in the decoding procedure. When the partitioning of all previous emails in the window is fixed, a new mail is processed by either assigning it to one of the existing clusters, or creating a new singleton batch. Algorithm 1 details this approach; the initially empty partitioning C becomes a singelton cluster when the first message arrives. Every new message then either groups to an existing cluster c j or extends C by forming its own singelton cluster {x t }, respectively.\nIn general, given a fixed clustering of x 1 , . . . , x T \u22121 , the decoding problem of finding the y that maximizes Equation 5 reduces to max\ny T t=1 t\u22121 k=1 y tk sim w (x t , x k ) (11) = max y T \u22121 t=1 t\u22121 k=1 y tk sim w (x t , x k ) + T \u22121 k=1 y T k sim w (x T , x k ). (12\n)\nThe first summand is constant. Finding the maximum in Equation 11 therefore amounts to assigning it to the cluster which is most similar to x T or, if no existing cluster has positive total similarity, establishing a new singleton cluster.\nIn terms of the adjacency matrix y (i) of the i-th input, the task is to find entries for the T -th row and column, realizing the optimal clustering of x T . We denote the set of matrices that are consistent clusterings and are equal to the i-th example, y (i) , in all rows/columns except for the T -th row/column, by\nY (i)\nT . If we denote the potential new cluster (which is empty before inserting x T ) withc, Y\n(i)\nT is of the size |C \u222a {c}| \u2264 T (i) .\nFinding the new optimal clustering can be expressed as the following maximization problem.\nDecoding Strategy 1 Given T (i) instances x 1 , . . . , x T (i) , similarity measure sim w : (x j , x k ) \u2192 r \u2208 R, and a clustering of instances x 1 , . . . , x T (i) \u22121 ; the sequential decoding problem is defined a\u015d\ny = max y\u2208Y (i) T T (i) \u22121 k=1\u0233 T (i) k sim w (x T (i) , x k ). (13\n)\nNow, we derive an optimization problem that requires the sequential clustering to produce the correct output for all training data. Optimization Problem 2 constitutes a compact formulation for finding the desired optimal weight vector by treating every message as the most recent message once, in order to exploit the available training data as effectively as possible.\nOptimization Problem 2 Given n labeled clusterings, C > 0; over all w and \u03be, minimize\n1 2 w 2 + C i,j \u03be (i) j subject to the constraints w \u03a8(x (i) , y (i) ) + \u03be (i) t \u2265 w \u03a8(x (i) ,\u0233) + \u2206 N (y (i) ,\u0233) for all 1 \u2264 i \u2264 n, 1 \u2264 t \u2264 T (i) , and\u0233 \u2208 Y (i) t .\nNote that Optimization Problem 2 has at most n i=1 (T (i) ) 2 constraints and can efficiently be solved with standard QP-solving techniques.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Results", "text": "In this section we evaluate the performance and benefit of batch detection on a collection of emails. We compare our learning methods with the iterative learning procedure for supervised clustering by Finley and Joachims (2005) and perform an error analysis. We evaluate how the identification of email batches can actually support the classification of emails as spam or non-spam. Furthermore, we assess the execution time of the presented decoding methods. Quadratic programs are solved with CPLEX.", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "Email Batch Data", "text": "Email batch detection is performed at a mail transfer agent that processes a dense stream of messages. Standard email collections such as the Enron corpus or the TREC spam collection are collected from final recipients and therefore exhibit different characteristics. A mail transfer agent experiences many large batches over a short period of time. Existing spam corpora were harvested over a longer period from clients and contain fewer and more scattered copies of each batch. We therefore create an email corpus that reflects the characteristics of an email stream, but remedies the obvious privacy concerns that would arise from simply recording an email stream at a mail transfer agent. We do record the email stream for a short period of time, but only extract spam messages from this record. We randomly insert non-spam messages from the Enron collection and batches of newsletters. We remove the headers except for the sender address, MIME part information, and the header size.\nThe final corpus contains 2,000 spam messages, 500 Enron messages, and 500 newsletters (copies of 50 distinct newsletters). We manually group these emails into 136 batches with an average of 17.7 emails, and 598 remaining singleton mails. We implement 47 feature functions. They include the TFIDF similarity, equality of sender, equality of the MIME type, and differences in letter-bigram-counts.\nWe design a cross validation procedure such that no elements of the same newsletter or spam batch occur in both the training and test set at any time. To this end, we construct each test set by using one non-singular batch, and filling the test sample with singletons and emails of other batches to a total size of 100. Batches with more than 50 emails are divided over several test sets, to ensure a reasonable mixture of emails from the test batch and other emails. Overall, there are 153 test sets. For each of these test sets, nine training sets x (1) , . . . , (9) are generated by sampling randomly from the remaining emails, excluding emails from the test batch in case of split test batches. All reported results are averaged over the results from each of the 153 training/test combinations.\nx", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Batch Identification", "text": "We compare the parameter vectors obtained by four strategies. Parameters are estimated by solving Op- timization Problem 1 (compact), solving Optimization Problem 2 (sequential), and by using the iterative training algorithm of Finley and Joachims ( 2005) (iterative). As an additional baseline, we train a pairwise classifier (pairwise) classifier: each pair of emails within a set constitutes a training example, with label +1 if they belong to the same cluster, and \u22121 otherwise. On these pairs, a linear SVM is trained, and the weight vector is directly used as parameter of the similarity measure. The final clustering is then obtained by one of the decoding strategies, using the similarity matrix obtained from pairwise learning.\nThough three of the four optimization problems refer to a specific decoding strategy, we evaluate each of them with every decoder for comparison. We study three decoders: LP decoding (exact solution of Equation 8), the sequential decoder (Decoding Strategy 1), and the greedy agglomerative clustering described in (Finley & Joachims, 2005). Figure 1 shows the average normalized loss per mail of these combinations with standard error. For this problem, there are no significant differences between either of these training and decoding methods. The sequential decoder operates under the constraint of linearity, and it would be plausible to assume that it incurs a higher loss than the LP decoding on average. The data suggests that this might be the case, but the difference is at most slight and by no means significant.\nFigure 2 gives more insight into the characteristics of the compared methods. On the y-axis, the number of disagreeing edges with respect to the true clustering is depicted. The hatched areas indicate the number of disagreements between the true clustering and the signs of the similarity matrix induced by the weight vector and the pairwise features. The similarity matrix serves as input to the decoder; the decoder transforms it into a consistent partitioning. The colored bars in-dicate the numbers of wrong edges after clustering.\nIt is apparent that the simplest learning method, pairwise learning, leads to the fewest wrong edges before clustering, but the induced similarity matrix is furthest away from being a consistent partitioning. This corresponds to the intuition that the training constraints of pairwise learning refer to individual links instead of the entire partitioning. The iterative algorithm leads to similarity matrices which are significantly nearer to a consistent clustering (the colored bars are shorter). The similarity measures learned by the compact optimization problems lead to a similarity matrix with still more disagreeing edges, while yielding comparable error rates after decoding. This indicates that the decoding step has to resolve fewer inconsistencies, making it more robust to approximations.", "publication_ref": ["b6"], "figure_ref": ["fig_0", "fig_1"], "table_ref": []}, {"heading": "Classification Using Batch Information", "text": "We evaluate how the classification of emails as spam or non-spam benefits from identification of batches. As a baseline, we train a linear support vector machine with the word-counts of the training emails as features.\nWe remove all email header information except for the subject line in order to eliminate artefacts from the data collection procedure.\nWe construct a collective filter that sums up the word counts of all emails in a batch, and includes four additional features: the size of the batch, a binary feature indicating whether the batch is larger than one, a binary feature indicating whether the subject of all emails in the batch is identical, and a binary feature indicating whether the sender address of all emails in the batch is identical. This results in all emails within a batch having the same feature representation.\nWe examine how the classification performance is affected by the batch detection. As an upper bound, we investigate the performance of the collective classifier given perfect clustering information, based on the manual clustering. In addition to that, we assess how sensitive the benefit of collective classification is with respect to the accuracy of the clustering. In the setting of clustering with noise, each email is collectively classified in a cluster that contains increasingly many wrongly clustered emails. is not significant. The collective classifiers perform indistinguishably well; sequential and LP decoder perform alike. We can see that using ideal batch information, the risk of misclassification (1 -AUC) is reduced by 43.8%, while with non-ideal batch information obtained through approximate clustering still 41.4% reduction are achieved. Even though the AUC of the baseline appears high already, in spam filtering a 40% reduction of the risk is a substantial improvement!", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Clustering Runtime", "text": "An important aspect in clustering on streams and especially in identifying spam batches is efficiency. The window size has to be sufficiently large to contain at least one representative of each currently active batch. The time required to cluster one additional email depending on the window size is therefore a crucial criterion for selecting an appropriate clustering method. Figure 4 illustrates the observed time required for processing an email by LP-decoding and sequential decoding with respect to the window size. While the computation time of the LP approximation grows at least cubicly, the time for an incremental update for a single email with sequential decoding grows only linearly. Due to the different time-scales of the two methods (note that the center graph shows micro-seconds instead of seconds), we use a logarithmic time-scale to plot the curves in a single diagram (right-hand graph).", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Conclusion", "text": "We devised a sequential clusering algorithm and two integrated formulations for learning a similarity measure to be used with correlation clustering. First, we derived a compact optimization problem based on the LP approximation to correlation clustering to learn the weights of the similarity measure. Starting from the assumption that decisions for already processed emails cannot be reconsidered, we devised an efficient cluster- ing algorithm with computational complexity linear in the number of emails in the window. From this algorithm we derived a second integrated method for learning the weight vector.\nOur empirical results indicate that there are no significant differences between the learning or decoding methods in terms of accuracy. Yet the integrated learning formulations optimize the weight vector more directly to yield consistent partitionings. Using the batch information obtained from decoding with the learned models, email spam classification performance increases substantially over the baseline with no batch information. The efficiency of the sequential clustering algorithm makes supervised batch detection in enterprise-level scales, with millions of emails per hour and thousands of recent emails as reference, feasible.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We gratefully acknowledge support from STRATO AG and from the German Science Foundation DFG.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Correlation clustering. Proceedings of the Symposium on Foundations of Computer Science", "journal": "", "year": "2002", "authors": "N Bansal; A Blum; S Chawla"}, {"ref_id": "b1", "title": "Learning distance functions using equivalence relations", "journal": "", "year": "2003", "authors": "A Bar-Hillel; T Hertz; N Shental; D Weinshall"}, {"ref_id": "b2", "title": "Clustering with qualitative information", "journal": "Journal of Computer and System Sciences", "year": "2005", "authors": "M Charikar; V Guruswami; A Wirth"}, {"ref_id": "b3", "title": "P2P-based collaborative spam detection and filtering", "journal": "", "year": "2004", "authors": "E Damiani; S D C Di Vimercati; S Paraboschi; P Samarati"}, {"ref_id": "b4", "title": "Correlation clustering with partial information", "journal": "", "year": "2003", "authors": "E D Demaine; N Immorlica"}, {"ref_id": "b5", "title": "Correlation clusteringminimizing disagreements on arbitrary weighted graphs", "journal": "", "year": "2003", "authors": "D Emanuel; A Fiat"}, {"ref_id": "b6", "title": "Supervised clustering with support vector machines", "journal": "", "year": "2005", "authors": "T Finley; T Joachims"}, {"ref_id": "b7", "title": "Personalised, collaborative spam filtering", "journal": "", "year": "2004", "authors": "A Gray; M Haahr"}, {"ref_id": "b8", "title": "Clustering data streams: Theory and practice", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": "2003", "authors": "S Guha; A Meyerson; N Mishra; R Motwani; L Callaghan"}, {"ref_id": "b9", "title": "Clustering binary data streams with kmeans", "journal": "", "year": "2003", "authors": "C Ordonez"}, {"ref_id": "b10", "title": "Learning string edit distance", "journal": "", "year": "1997", "authors": "E S Ristad; P N Yianilos"}, {"ref_id": "b11", "title": "Correlation clustering: maximizing agreements via semidefinite programming", "journal": "", "year": "2004", "authors": "C Swamy"}, {"ref_id": "b12", "title": "Maxmargin Markov networks", "journal": "", "year": "2004", "authors": "B Taskar; C Guestrin; D Koller"}, {"ref_id": "b13", "title": "Large margin methods for structured and interdependent output variables", "journal": "Journal of Machine Learning Research", "year": "2005", "authors": "I Tsochantaridis; T Joachims; T Hofmann; Y Altun"}, {"ref_id": "b14", "title": "Constrained k-means clustering with background knowledge", "journal": "", "year": "2001", "authors": "K Wagstaff; C Cardie; S Rogers; S Schr\u00f6dl"}, {"ref_id": "b15", "title": "Distance metric learning, with application to clustering with side-information", "journal": "", "year": "2002", "authors": "E P Xing; A Y Ng; M I Jordan; S Russell"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. Average loss for window size m = 100.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 .2Figure 2. Fraction of the loss induced by the learning algorithm (similarity matrix) and the decoding.", "figure_data": ""}, {"figure_label": "33", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 Figure 3 .33Figure3shows the area under the ROC curve (AUC) for the classifiers under investigation. The performance of the collective classifier based on a perfect clustering can be seen on the right hand side of the graph (ideal clustering at 0% noise). The difference between the collective classification based on a perfect clustering and based on the inferred clusterings", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 .4Figure 4. Computation time for adding one email depending on window size.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Two spam mails from the same batch.Hello, This is Terry Hagan.We are accepting your mo rtgage application. Our company confirms you are legible for a $250.000 loan for a $380.00/month.", "figure_data": "Approval process will take 1 minute, so please fillout the form on our website:http://www.competentagent.com/application/Best Regards, Terry Hagan;Senior Account DirectorTrades/Fin ance Department North OfficeDear Mr/Mrs,This is Brenda Dunn.We are accepting your mortgage application. Our office confirms you can get a$228.000 lo an for a $371.00 per month payment.Follow the link to our website and submit your con-tact information. Easy as 1,2,3.http://www.competentagent.com/application/Best Regards, Brenda Dunn;Accounts ManagerTrades/Fin ance Department East Officenumber of instances, this solution leads to intractableproblems in practice."}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u2206 N (y,\u0177) = j,k:k<j |y jk \u2212\u0177 jk | k =j y k k .", "formula_coordinates": [3.0, 95.66, 539.4, 133.64, 28.59]}, {"formula_id": "formula_1", "formula_text": "sim w (x j , x k ) = D d=1 w d \u03c6 d (x j , x k ) = w \u03a6(x j , x k ) (1)", "formula_coordinates": [3.0, 58.76, 657.69, 230.68, 31.76]}, {"formula_id": "formula_2", "formula_text": "f (x, y) = j,k y jk sim w (x j , x k ).", "formula_coordinates": [3.0, 351.87, 194.82, 145.14, 21.85]}, {"formula_id": "formula_3", "formula_text": ")2", "formula_coordinates": [3.0, 532.95, 194.82, 8.49, 10.46]}, {"formula_id": "formula_4", "formula_text": "R(f ) = \u2206(y, argmax\u0233 f (x,\u0233))p(x, y)dxdy,(3)", "formula_coordinates": [3.0, 315.68, 291.92, 225.76, 10.46]}, {"formula_id": "formula_5", "formula_text": "f (x, y) = T t=1 t\u22121 k=1 y tk sim w (x t , x k ) (4) = T t=1 t\u22121 k=1 y tk w \u03a6(x t , x k ) = w T t=1 t\u22121 k=1 y tk \u03a6(x t , x k ) = w \u03a8(x, y). (5", "formula_coordinates": [3.0, 336.2, 600.4, 205.24, 116.65]}, {"formula_id": "formula_6", "formula_text": ")", "formula_coordinates": [3.0, 537.19, 706.59, 4.24, 10.46]}, {"formula_id": "formula_7", "formula_text": "y = argmax y f (x, y) s.t. \u2200 jkl : (1 \u2212 y jk ) + (1 \u2212 y kl ) \u2265 (1 \u2212 y jl ) (6) \u2200 jk : y jk \u2208 {0, 1}.", "formula_coordinates": [4.0, 71.19, 102.65, 218.25, 41.24]}, {"formula_id": "formula_8", "formula_text": "jk \u2208 [0, 1]. z = argmax z f (x, z) s.t. \u2200 jkl : (1\u2212z jk ) + (1\u2212z kl ) \u2265 (1\u2212z jl ) (7) \u2200 jk : z jk \u2208 [0, 1]", "formula_coordinates": [4.0, 76.55, 238.36, 212.89, 63.39]}, {"formula_id": "formula_9", "formula_text": "min 1 2 w 2 + C n i=1 \u03be (i) (8) s.t. \u2200 i w \u03a8(x (i) , y (i) ) + \u03be (i) \u2265 max y w \u03a8(x (i) ,\u0233) + \u2206(y (i) ,\u0233) (9", "formula_coordinates": [4.0, 87.19, 380.56, 202.24, 66.78]}, {"formula_id": "formula_10", "formula_text": ")", "formula_coordinates": [4.0, 285.19, 431.5, 4.24, 10.46]}, {"formula_id": "formula_11", "formula_text": "\u2200 i \u03be (i) \u2265 0. (10", "formula_coordinates": [4.0, 120.46, 451.59, 164.55, 12.92]}, {"formula_id": "formula_12", "formula_text": ")", "formula_coordinates": [4.0, 285.01, 453.16, 4.43, 10.46]}, {"formula_id": "formula_13", "formula_text": "max z w \u03a8(x (i) ,z) + \u2206 N (y (i) ,z) = max z w \u03a8(x (i) ,z) + k<j |y (i) jk \u2212z jk | k =j y (i) k k = max z d (i) + j,k<j z (i) jk (w \u03a6(x (i) j , x (i) k ) \u2212 e (i) jk ),", "formula_coordinates": [4.0, 65.99, 519.78, 212.91, 81.8]}, {"formula_id": "formula_14", "formula_text": "d (i) = j,k<j y (i) jk P k =j y (i) k k", "formula_coordinates": [4.0, 84.94, 613.23, 103.64, 24.03]}, {"formula_id": "formula_15", "formula_text": "(i) jk = 2y (i) jk \u22121 P k =j y (i) k k ,", "formula_coordinates": [4.0, 219.08, 613.23, 70.36, 24.03]}, {"formula_id": "formula_16", "formula_text": "L(z (i) , \u03bb (i) , \u03bd (i) , \u03ba (i) ) = d (i) + \u03bd (i) 1 + \u03bb (i) 1 + h \u03a6(x (i) )w \u2212 e (i) \u2212 A (i) \u03bb (i) \u2212 \u03bd (i) + \u03ba (i) i z (i) ,", "formula_coordinates": [4.0, 65.55, 686.6, 218.35, 29.41]}, {"formula_id": "formula_17", "formula_text": "A (i) jkl,j k = \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 +1 : if (j = j \u2227 k = k) : \u2228(j = k \u2227 k = l) \u22121 : if j = j \u2227 k = l 0 : otherwise.", "formula_coordinates": [4.0, 321.26, 79.98, 180.26, 54.69]}, {"formula_id": "formula_18", "formula_text": "\u03bb (i) ,\u03bd (i) d (i) + \u03bd (i) 1 + \u03bb (i) 1 s.t. \u03a6(x (i) )w \u2212 e (i) \u2212 A (i) \u03bb (i) \u2212 \u03bd (i) \u2264 0 \u03bb (i) , \u03bd (i) \u2265 0.", "formula_coordinates": [4.0, 320.45, 204.39, 198.02, 52.34]}, {"formula_id": "formula_19", "formula_text": "1 2 ||w|| 2 + C n i=1 \u03be (i) subject to the constraints \u2200 n i=1 w \u03a8(x (i) , y (i) ) + \u03be (i) \u2265 d (i) + \u03bd (i) 1 + \u03bb (i) 1, \u2200 n i=1 w \u03a6(x (i) ) \u2212 e (i) \u2264 A (i) \u03bb (i) + \u03bd (i) , \u2200 n i=1 \u03bb (i) , \u03bd (i) \u2265 0.", "formula_coordinates": [4.0, 308.64, 348.1, 229.87, 71.66]}, {"formula_id": "formula_20", "formula_text": "Algorithm 1 Sequential Clustering C \u2190 {} for t = 1 . . . T do c j \u2190 argmax c\u2208C x k \u2208c w \u03a6(x k , x t ) if x k \u2208c j w \u03a6(x k , x t ) < 0 then C \u2190 C \u222a {{x t }} else C \u2190 C \\ {c j } \u222a {c j \u222a {x t }} end if end for return C", "formula_coordinates": [5.0, 55.44, 68.3, 176.91, 134.85]}, {"formula_id": "formula_21", "formula_text": "y T t=1 t\u22121 k=1 y tk sim w (x t , x k ) (11) = max y T \u22121 t=1 t\u22121 k=1 y tk sim w (x t , x k ) + T \u22121 k=1 y T k sim w (x T , x k ). (12", "formula_coordinates": [5.0, 104.99, 445.46, 184.44, 102.12]}, {"formula_id": "formula_22", "formula_text": ")", "formula_coordinates": [5.0, 285.01, 525.73, 4.43, 10.46]}, {"formula_id": "formula_23", "formula_text": "Y (i)", "formula_coordinates": [5.0, 212.62, 678.12, 16.53, 13.09]}, {"formula_id": "formula_24", "formula_text": "(i)", "formula_coordinates": [5.0, 148.45, 703.96, 9.04, 7.32]}, {"formula_id": "formula_25", "formula_text": "y = max y\u2208Y (i) T T (i) \u22121 k=1\u0233 T (i) k sim w (x T (i) , x k ). (13", "formula_coordinates": [5.0, 325.55, 160.03, 211.46, 33.08]}, {"formula_id": "formula_26", "formula_text": ")", "formula_coordinates": [5.0, 537.01, 171.26, 4.43, 10.46]}, {"formula_id": "formula_27", "formula_text": "1 2 w 2 + C i,j \u03be (i) j subject to the constraints w \u03a8(x (i) , y (i) ) + \u03be (i) t \u2265 w \u03a8(x (i) ,\u0233) + \u2206 N (y (i) ,\u0233) for all 1 \u2264 i \u2264 n, 1 \u2264 t \u2264 T (i) , and\u0233 \u2208 Y (i) t .", "formula_coordinates": [5.0, 307.45, 312.19, 233.99, 77.92]}, {"formula_id": "formula_28", "formula_text": "x", "formula_coordinates": [6.0, 113.19, 603.03, 6.05, 10.46]}], "doi": ""}