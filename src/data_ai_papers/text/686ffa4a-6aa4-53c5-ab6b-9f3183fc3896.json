{"title": "ApplicaAI at SemEval-2020 Task 11: On RoBERTa-CRF, Span CLS and Whether Self-Training Helps Them", "authors": "Dawid Jurkiewicz; \u0141ukasz Borchmann; Izabela Kosmala; Filip Grali\u0144ski", "pub_date": "", "abstract": "This paper presents the winning system for the propaganda Technique Classification (TC) task and the second-placed system for the propaganda Span Identification (SI) task. The purpose of the TC task was to identify an applied propaganda technique given propaganda text fragment. The goal of SI task was to find specific text fragments which contain at least one propaganda technique. Both of the developed solutions used semi-supervised learning technique of selftraining. Interestingly, although CRF is barely used with transformer-based language models, the SI task was approached with RoBERTa-CRF architecture. An ensemble of RoBERTa-based models was proposed for the TC task, with one of them making use of Span CLS layers we introduce in the present paper. In addition to describing the submitted systems, an impact of architectural decisions and training schemes is investigated along with remarks regarding training models of the same or better quality with lower computational budget. Finally, the results of error analysis are presented.", "sections": [{"heading": "Introduction", "text": "The idea of fine-grained propaganda detection was introduced by Da San Martino et al. (2019), whose intention was to facilitate research on this topic by publishing a corpus with detailed annotations of high reliability. There was a chance to propose NLP systems solving this task automatically as a part of this year's SemEval series. It was expected to detect all fragments of news articles that contain propaganda techniques, and to identify the exact type of used technique (Da San Martino et al., 2020).\nThe authors decided to evaluate Technique Classification (TC) and Span Identification (SI) tasks separately. The purpose of the TC task was to identify an applied propaganda technique given the propaganda text fragment. In contrast, the goal of the SI task was to find specific text fragments that contain at least one propaganda technique. This paper presents the winning system for the propaganda Technique Classification task and the second-placed system for the propaganda Span Identification task.", "publication_ref": ["b2", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "Systems Description", "text": "Systems proposed for both SI and TC tasks were based on RoBERTa model (Liu et al., 2019) with task-specific modifications and training schemes applied.\nThe central motif behind our submissions is a commonly used semi-supervised learning technique of self-training (Yarowsky, 1995;Liao and Veeramachaneni, 2009;Liu et al., 2011;Wang et al., 2020), sometimes referred to as incremental semi-supervised training (Rosenberg et al., 2005) or selflearning (Lin et al., 2010). In general, these terms stand for a process of training an initial model on a manually annotated dataset first and using it to further extend the train set by automatically annotating other dataset. Usually, only a selected subset of auto-annotated data is used, however neither selection of high-confidence examples nor loss correction for noisy annotations is performed in our case. This is why it can be considered a simplification of mainstream approaches-the na\u00efve self-training.  ", "publication_ref": ["b16", "b22", "b13", "b15", "b21", "b18", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Span Identification", "text": "The problem of span identification was treated as a sequence labeling task, which in the case of Transformer-based language models is often solved by means of classifying selected sub-tokens (e.g., first BPE of each word considered) with or without applying LSTM before the classification layer (Devlin et al., 2019).\nAlthough pre-Transformer sequence labeling solutions exploited CRF layer in the output (Huang et al., 2015;Lample et al., 2016), this practice was abandoned by the authors of BERT (Devlin et al., 2019) and subsequent researchers developing the idea of bidirectional Transformers, with rare exceptions, such as Souza et al. (2019) who used BERT-CRF for Portuguese NER. Contrary to the above, we approached Span Identification task with RoBERTa-CRF architecture.\nThe impact of this decision will be discussed in Section 3 along with remarks regarding training models of the same or better quality with a lower computational budget in an orderly fashion. In contrast, the following narrative aims at a faithful reflection of the actual way the model which we used was trained.\nRecipe Take one pretrained RoBERTa LARGE model, add CRF layer and train on original (gold) dataset until progress is no longer achieved with Viterbi loss, SGD optimizer, and hyperparameters defined in Table 1. Use the best-performing model to annotate random 500k OpenWebText 1 sentences automatically. Train the second model on both original (gold) dataset and autotagged (silver) one with hyperparameters defined in Table 1. Repeat the procedure two more times with the best model from the previous step, hyperparameters from Table 2, and other OpenWebText sentences.\nNote that hyperparameters were indeed not overwritten during the first self-training iteration. Scores achieved by the best-performing models were respectively 50.91 (without self-training) and 50.98, 51.45, 52.24 in consecutive self-training iterations.\nMany questions may arise regarding this procedure and the role of purely random factors. It is not a problem when rather the best score than its explanation is desired. In a leaderboard-driven exploration, one can simply conduct a broad set of experiments and choose the best-performing model without reflection, whether it is a byproduct of training instability. What actually happened here was investigated afterward and will be discussed in Section 3. ", "publication_ref": ["b4", "b8", "b12", "b4", "b20"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Technique Classification", "text": "Transformer-based language models used in the sentence classification setting assume that representations of special tokens (such as [CLS] or [BOS]) are passed to the classification layer. Since TC task is aimed at the classification of spans, it might be beneficial to introduce information about the text fragment to be classified. We experimented with two approaches addressing this requirement.\nThe first assumes an injection of special tokens indicating the beginning and the end of the text marked as propaganda, such as a sample sentence before BPE applied appears as:\n[BOS] Democrats acted like [BOP] babies [EOP] at the SOTU [EOS]\nIn this approach we continue with representation of [BOS], as in the usual sentence classification task. The second approach is to stack a small Transformer only on the selected tokens. 2 This one has no own embeddings apart from the ones for [BOS] but uses the host model's representations instead. This technique is roughly equivalent to adding consecutive layers and masking attention outside the selected span and will be referred to as Span CLS. Figure 2 summarizes differences between Span CLS and classification using special [BOP] and [EOP] tokens.\nThe initial experiments have shown that underrepresented classes achieve lower scores. To overcome this problem, we experimented with class-dependent rescaling applied to binary cross-entropy. In this setting (further referred to as re-weighting) factor for each class was determined as its inverse frequency multiplied by the frequency of the most popular class. The modified loss is equal to:\n(x, y) = \u2212 1 N d N n=1 d k=1 p k y k n log x k n + (1 \u2212 y k n ) log(1 \u2212 x k n ) p k = 1 f k max(f )\nwhere N is the batch size, n index denotes nth batch element, d is the number of classes, f stands for a vector of class absolute frequencies calculated on the train set, x is the output vector from the last sigmoid layer and y is a vector of multi-hot encoded ground truth labels. Note that the only difference from the original binary cross entropy for multi-label classification is the addition of the p k class weights.\nIn addition to the above, a part of the tested models took the use of the self-training approach. In the case of TC task one had to identify spans first and then predict their classes to generate silver train   set (Figure 1). We reused our best-performing model from SI task to identify spans, and the TC model trained on ground truth to automatically annotate these spans.\n.0 \u2192 .1 + \u2212 \u22121.1 + \u22121.6 .0 \u2212 \u22120.4 + \u22121.1 8 \u2192 16 .1 \u2192 .0 \u2212 \u2212 \u22123.9 + \u22127.0 .1 \u2212 \u22120.7 + \u22121.3\nRegardless of the approach taken, context as broad as possible within the 256 subword units limit was provided on both sides of the span to be classified. Note that it was a maximum equal extension of the span text in both directions, and we did not limit the extension to the sentence boundaries.\nThe winning TC model (described in the recipe below) was an ensemble of three models. Each of them used a different mix of previously described approaches with hyperparameters defined in Table 1 for first and second model, and those from Table 2 in case of the third model.\nRecipe Add classification layer (described in Figure 2 on the left) to the pretrained RoBERTa LARGE model in order to obtain the first model and train until no score gain is observed on development set. Train the second model in the same manner, but this time using the re-weighting. Combine re-weighting, Span CLS and self-training approaches to get the third model, and again train until no score improvement on development set is observed. Finally, ensemble all three models by averaging class probabilities from their final layers.\nAs shown later, the approach we took and reported above turned out to be sub-optimal. An in-depth analysis of this system and a better one is proposed in Section 3.2.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Ablation Studies", "text": "Since different random initialization or data order can result in considerably higher scores, 3 models with different random seeds were trained for the purposes of ablation studies. In the case of the SI task, results were evaluated on the original development set. In contrast, in the case of TC, where fewer data points are available, we decided to use cross-validation instead.  (1) (6) 72.3 \u00b1 1.7 (1) (2) 72.9 \u00b1 1.8 (3) (5) 73.6 \u00b1 1.5 (1) (5) ( 8)\n74.1 \u00b1 1.7 (2) (4) ( 7)\n74.4 \u00b1 1.5 (1) (4) ( 7)\n74.6 \u00b1 1.4 (1) (4) (7) (8)\n74.9 \u00b1 1.2 (1) (2) (4) (5) ( 7)\n75.1 \u00b1 1.5\nTable 6: Average scores achieved with ensembles of individual models described in Table 5. Micro-averaged F1 metric.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Span Identification", "text": "Models with different random seeds were trained for 60K steps with an evaluation performed every 2K steps. This is equivalent to approximately 30 epochs, and per-epoch validation in a scenario without data generated during the self-training procedure. Table 3 summarizes the best scores achieved across 10 runs for each configuration. CRF has a noticeable positive impact on FLC-F1 (Da San Martino et al., 2020) scores achieved without self-training in the setting we consider. The presence of the CRF layer is correlated positively with the score (\u03c1 = 0.27, p < 0.001). The difference is significant (p < 0.001), according to the Kruskal-Wallis test (Kruskal and Wallis, 1952) . Unless said otherwise, all further statistical statements within this section were confirmed with statistically significant positive Spearman rank correlation and Kruskal-Wallis test results. Differences in variance were confirmed using Bartlett's test (Snedecor and Cochran, 1989). The 0.05 significance level was assumed.\nThe statistically significant influence of CRF disappears when the self-training is investigated. In the case of first self-training, regardless of whether or not CRF was used, a considerable increase in median score can be observed. Self-trained models with and without the CRF layer, however, are indistinguishable.\nImprovement offered by further self-training iterations is not so evident but is statistically significant. In particular, they slightly improve mean scores and decrease variance (see Table 3). As it comes to the latter, CRF-extended models generally have higher variance and scores achieved across the runs.\nTable 4 analyzes the importance of using different hyperparameters. Whereas use of a smaller batch size and dropout is beneficial for the initial training without noisy data, it negatively impacts the selftraining phase. The most substantial negative impact is observed when dropout is disabled during training on the small amount of manually annotated data.\nFigure 3 illustrates scores achieved by models trained for the same number of steps on subsets or supersets of manually annotated data. CRF layer has a positive impact regardless of the percentage of train set available. Once again, a large variance in scores of CRF-equipped models can be observed, however, it is substantially reduced with the increase of a batch size. Interestingly, figures suggest the proportion of automatically annotated data we used might be suboptimal since it was an equivalent of around 3000% in line with the chart's convention. One may hypothesize better scores would be achieved by models trained with 1 : 4 gold to silver proportion.", "publication_ref": ["b10", "b19"], "figure_ref": [], "table_ref": ["tab_4", "tab_4", "tab_5"]}, {"heading": "Technique Classification", "text": "6-fold cross-validation was conducted. The results are presented in Table 5. Folds were created by mixing training and development datasets, then shuffling them and splitting into even folds. Parameters were set according to Table 1 and Table 2, whereas experiments were carried out as follows. Each approach from Table 5 was separately evaluated on each fold using the micro-averaged F1 metric. Then, for each approach, the average score and the standard deviation were obtained using six scores from every fold.\n( 1) (2) (3) (4) (5) (6) (7) (8  Moreover, all the 247 possible ensembles 4 were evaluated in the same fashion as in experiments from Table 5. Table 6 shows the performance achieved by selected combinations when simple averaging of the probabilities returned by individual models was used as the final prediction.\nDue to a large number of available results, it is beneficial to conduct a statistical analysis to formulate remarks regarding the general observed trends. Each component model of the ensemble was treated as a categorical variable with respect to the ensemble score. Spearman rank correlation between the presence of an ensemble component (approaches from Table 5) and achieved scores shows that adding model to the ensemble correlates with a significant increase in score, except for ( 6) model (see Table 7). Boxplots from Figure 4 lead to the same conclusions. 5 Re-weighting seems to be beneficial only when ensembled with other models. An interesting finding is that Span CLS offers a small but consistent increase of performance both in models from Table 5 and when used in ensembles. Bear in mind, we outperformed the second-placed team by \u03b5, so an improvement of a point or half is not negligible.\nWhat is most conspicuous, however, is that self-training based solutions from Table 5 seem to be detrimental in the case of TC task. This damaging effect can be potentially attributed to the fact that automatically generated data accumulate errors from both Span Identification and Classification. Another possible explanation is that much fewer data points are available for span classification task than for span identification attempted as a sequence labeling task. The latter would be somehow consistent with what was found in the field of Neural Machine Translation, where the use of the back-translation technique in low-resource setting was determined to be harmful (Edunov et al., 2018).\nOn the other hand, self-training has a positive, statistically significant impact on the score when used in ensembles (see Figure 4 and Table 7). It is not surprising as the beneficial impact of combining individual estimates was observed in many disciplines and is known since the times of Laplace (Clemen, 1989).", "publication_ref": ["b6", "b0"], "figure_ref": ["fig_0", "fig_0"], "table_ref": ["tab_7", "tab_1", "tab_7", "tab_1", "tab_7", "tab_7", "tab_8", "tab_7", "tab_7", "tab_8"]}, {"heading": "Error analysis", "text": "In addition to providing an overview of problematic classes, the question of which shallow features influence score and worsen the results was addressed. This problem was analyzed in a no-box manner, as proposed by Grali\u0144ski et al. (2019). The main idea is to create two dataset subsets for each feature considered (one for data points with the feature present and one for data points without the feature), rank subsets by per-item scores, and use Mann-Whitney rank U (Mann and Whitney, 1947) to determine whether there is a non-accidental difference between subsets. A low p-value indicates that feature reduces the evaluation score of the model. ", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "Span Identification", "text": "Since the FLC-F1 metric used in the SI task gives non-zero scores for partial matches; it is interesting to analyze what was the proportion of entirely missed (partially identified) spans. Table 8 investigates this question broken down by the propaganda technique used.\nOur system was unable to identify one-third of expected spans, whereas a majority of those correctly identified were the partial matches. The spans the easiest to identify in the text represented Flag-Waving, Appeal to fear/prejudice, and Slogans techniques. In contrast, Bandwagon, Doubt, and the group of {Whataboutism, Strawman, Red Herring} turned out to be the hardest. The highest proportion of fully identified spans was achieved for Flag-Waving, Repetition, and Loaded Language. Unfortunately, it is not possible to investigate precision in this manner, without training separate models for each label or estimating one-to-one alignments between output and expected spans.\nFurther investigation of problematic cases in a paradigm of no-box debugging with the GEval tool (Grali\u0144ski et al., 2019) revealed the most worsening features, that are features whose presence impacts span identification evaluation metrics negatively (Table 9). It seems that our system tends to return ranges without adjacent punctuation. This is the case of sentences such as The new CIA Director Haspel, who 'tortured some folks,' probably can't travel to the EU, where only the quoted text was returned, whereas annotation assumes it should be returned with apostrophes and commas. This remark can be used to improve overall results with simple post-processing slightly. Returned and conjunction refers to the cases where it connects two propaganda spans. The system frequently returns them as a single span, contrary to what is expected in the gold standard.", "publication_ref": ["b7"], "figure_ref": [], "table_ref": ["tab_9", "tab_11"]}, {"heading": "Technique Classification", "text": "Figure 5 presents the normalized confusion matrix of the submitted system predictions. Interestingly, there are a few commonly confused pairs. Loaded Language and Black-and-white Fallacy were frequently misclassified as Appeal to fear/prejudice. Similarly, Causal Oversimplification was often predicted as Doubt and Clich\u00e9s as Loaded Language.\nThe most worsening features are presented in Table 10. One of the frequent predictors of low accuracy is a comma character present within the span to be classified. It can probably be attributed to the fact that its presence is a good indicator of span linguistic complexity. Another determinant of inefficiency turned out to be a negation-around half of the sentences containing word not were misclassified by the system. Suggested features of a quotation mark before the span and the digram according to after the span are related to reported or indirect speech. The explanation of the worsening effect of other features is not as evident as in the case mentioned above. Moreover, it seems there is no obvious way of improving the final results with our findings, and a more detailed analysis might be required.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": ["tab_12"]}, {"heading": "Discussion and Summary", "text": "The winning system for the propaganda Technique Classification (TC) task and the second-placed system for the propaganda Span Identification (SI) task has been described. Both of the developed solutions used a semi-supervised learning technique of self-training. Although CRF is barely used with Transformerbased language models, the SI task was approached with RoBERTa-CRF architecture. An ensemble of    RoBERTa-based models has been proposed for the TC task, with one of them making use of Span CLS layers we introduce in the present paper.\nAnalysis conducted afterward can be applied in a rather straightforward manner to further improve the scores for both SI and TC tasks. It is because some of the decisions we have made given lack of or uncertain information, during the post-hoc inquiry turned out to be sub-optimal. These include the proportion of data from self-training in the SI task, and the possibility of providing a better ensemble in the case of TC.\nThe ablation studies conducted, however, have some limitations. The same subset of OpenWebText was used in experiments conducted within one self-training iteration. This means a random seed did not impact which sentences were used during the first, second, and third self-training phase, and in each, we were manipulating only the data order. Moreover, an analysis we reported was limited to few hyperparameter combinations and no extensive hyperparameter space search was performed. Finally, only one and a rather simple method of cost-sensitive re-weighting was tested, and there is a great chance it was sub-optimal. It would be interesting to investigate other schemes, such as the one proposed by Cui et al. (2019).\nThe error analysis revealed propaganda techniques commonly confused in TC task, and the techniques we were unable to detect effectively within the SI input articles. In addition to providing an overview of problematic classes, the question of which shallow features influence score and worsen the results was addressed. A few of these were identified and our remarks can be used to slightly improve results on SI task with simple post-processing. This is not the case for TC task, where one is unable to propose how to improve the final results with our findings.\nAn interesting future research direction seems to be the application of the CRF layer and Span CLS to Transformer-based language models when dealing with other tasks outside the propaganda detection problem. These may include Named Entity Recognition in the case of RoBERTa-CRF, and an aspectbased sentiment analysis that can be viewed through the lens of span classification with Span CLS we proposed.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "Outro", "text": "Developed systems were used to identify and classify spans in the present paper to detect fragments one may suspect to represent one or more propaganda techniques. Unfortunately for the entertaining value of this work, none of such were identified by our SI model.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Combining forecasts: A review and annotated bibliography", "journal": "International Journal of Forecasting", "year": "1989", "authors": "References Robert; T Clemen"}, {"ref_id": "b1", "title": "Class-balanced loss based on effective number of samples", "journal": "", "year": "2019", "authors": "Yin Cui; Menglin Jia; Tsung-Yi Lin; Yang Song; Serge J Belongie"}, {"ref_id": "b2", "title": "Fine-grained analysis of propaganda in news articles", "journal": "", "year": "2019-11", "authors": "Giovanni Da San; Seunghak Martino; Alberto Yu; Rostislav Barr'on-Cede No; Preslav Petrov;  Nakov"}, {"ref_id": "b3", "title": "SemEval-2020 task 11: Detection of propaganda techniques in news articles", "journal": "", "year": "2020-09", "authors": "Giovanni Da San; Alberto Martino; Rostislav Barr ; Henning Wachsmuth; Preslav Petrov;  Nakov"}, {"ref_id": "b4", "title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b5", "title": "Finetuning pretrained language models: Weight initializations, data orders, and early stopping", "journal": "", "year": "2020", "authors": "Jesse Dodge; Gabriel Ilharco; Roy Schwartz; Ali Farhadi; Hannaneh Hajishirzi; Noah Smith"}, {"ref_id": "b6", "title": "Understanding back-translation at scale", "journal": "", "year": "2018", "authors": "Sergey Edunov; Myle Ott; Michael Auli; David Grangier"}, {"ref_id": "b7", "title": "GEval: Tool for debugging NLP datasets and models", "journal": "Association for Computational Linguistics", "year": "2019-08", "authors": "Filip Grali\u0144ski; Anna Wr\u00f3blewska; Tomasz Stanis\u0142awek; Kamil Grabowski; Tomasz G\u00f3recki"}, {"ref_id": "b8", "title": "Bidirectional LSTM-CRF models for sequence tagging", "journal": "", "year": "2015", "authors": "Zhiheng Huang; Wei Xu; Kai Yu"}, {"ref_id": "b9", "title": "Approaching neural grammatical error correction as a low-resource machine translation task", "journal": "Association for Computational Linguistics", "year": "2018-06", "authors": "Marcin Junczys-Dowmunt; Roman Grundkiewicz; Shubha Guha; Kenneth Heafield"}, {"ref_id": "b10", "title": "Use of ranks in one-criterion variance analysis", "journal": "Journal of the American statistical Association", "year": "1952", "authors": "H William; W Allen Kruskal;  Wallis"}, {"ref_id": "b11", "title": "Boruta-a system for feature selection", "journal": "Fundamenta Informaticae", "year": "2010", "authors": "Aleksander Miron B Kursa; Witold R Jankowski;  Rudnicki"}, {"ref_id": "b12", "title": "Neural architectures for named entity recognition", "journal": "", "year": "2016", "authors": "Guillaume Lample; Miguel Ballesteros; Sandeep Subramanian; Kazuya Kawakami; Chris Dyer"}, {"ref_id": "b13", "title": "A simple semi-supervised algorithm for named entity recognition", "journal": "Association for Computational Linguistics", "year": "2009-06", "authors": "Wenhui Liao; Sriharsha Veeramachaneni"}, {"ref_id": "b14", "title": "Combining self learning and active learning for chinese named entity recognition", "journal": "Journal of Software", "year": "2010", "authors": "Yao Lin; Chengjie Sun; Wang Xiaolong; Wang Xuan"}, {"ref_id": "b15", "title": "Recognizing named entities in tweets", "journal": "Association for Computational Linguistics", "year": "2011-06", "authors": "Xiaohua Liu; Shaodian Zhang; Furu Wei; Ming Zhou"}, {"ref_id": "b16", "title": "Roberta: A robustly optimized bert pretraining approach", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b17", "title": "On a test of whether one of two random variables is stochastically larger than the other. The annals of mathematical statistics", "journal": "", "year": "1947", "authors": "B Henry; Donald R Mann;  Whitney"}, {"ref_id": "b18", "title": "Semi-supervised self-training of object detection models", "journal": "", "year": "2005", "authors": "Chuck Rosenberg; Henry Hebert;  Schneiderman"}, {"ref_id": "b19", "title": "Statistical Methods. Iowa State University Press", "journal": "", "year": "1989", "authors": "W George; William G Snedecor;  Cochran"}, {"ref_id": "b20", "title": "Portuguese named entity recognition using BERT-CRF. CoRR, abs", "journal": "", "year": "1909", "authors": "F\u00e1bio Souza; Rodrigo Frassetto Nogueira; Roberto De; Alencar Lotufo"}, {"ref_id": "b21", "title": "Classification Model on Big Data in Medical Diagnosis Based on Semi-Supervised Learning", "journal": "The Computer Journal", "year": "2020", "authors": "Lei Wang; Qing Qian; Qiang Zhang; Jishuai Wang; Wenbo Cheng; Wei Yan"}, {"ref_id": "b22", "title": "Unsupervised word sense disambiguation rivaling supervised methods", "journal": "Association for Computational Linguistics", "year": "1995", "authors": "David Yarowsky"}], "figures": [{"figure_label": "4", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 4 :4Figure 4: Impact of adding a particular model to the ensemble has on mean scores from different folds. Comparison of results with and without it present in tested combination.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 5 :5Figure 5: Confusion matrix of the submitted system predictions normalized over the number of correct labels. Rows represent the correct labels and columns -the predicted ones (TC).", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "and [EOP] tokens are introduced, and the span is further classified as in the usual Transformer-based sentence classification task. On the right, an additional, small Transformer is stacked only over the selected tokens. It has no own embeddings apart from one for the [BOS] token, but uses representations provided by the host model instead.", "figure_data": "Span labelT [BOS] \u2605T 1\u2605...T M \u2605Span labelE [BOS] 'T 1 '...T MT [BOS]T 1...T NT [BOP]T 1 '...T MT [EOP]T 1 \"...T LT [BOS]T 1...T NT 1 '...T MT 1 \"...T LRoBERTaRoBERTaE [BOS]E 1...E NE [BOP]E 1 '...E ME [EOP]E 1 \"...E LE [BOS]E 1...E NE 1 '...E ME 1 \"...E L[BOS]Tok 1...Tok N[BOP]Tok 1'...Tok M[EOP]Tok 1\"...Tok L[BOS]Tok 1...Tok NTok 1'...Tok MTok 1\"...Tok LLeft contextSpan to classifyRight contextLeft contextSpan to classifyRight contextFigure 2: Comparison of span classification by means of special tokens (left) and in Span CLS approach (right). On the left,special [BOP]"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Best scores on the dev set achieved with RoBERTa large model on SI task. Mean, standard deviation and maximum across 10 runs with different random seeds. Numbers in brackets indicate how many self-training iterations were used.", "figure_data": "BatchDropouts Self-train CRF \u2206 FLC-F116 \u2192 8"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Impact of hypothetical lowering batch size during self training or enlarging batch size during initial training, as well as of enabling or disabling both hidden and attention dropouts. Change between means across 10 runs with different random seeds.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Average of 6-fold cross-validation score on TC task with micro-averaged F1 metric.", "figure_data": "EnsembleMicro-F1 (std)"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Spearman's \u03c1 between presence of ensemble component (models from Table5) and score achieved by ensemble. * indicate results were not significant, assuming 0.05 significance level.", "figure_data": ""}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Proportion of partially and fully identified spans (SI task) depending on the propaganda technique used. All the experiments conducted on the original development set.", "figure_data": "AuthorityFearBandwagonB&WSimplificationDoubtMinimizationFlag-WavingLoadedLabelingRepetitionSlogansClich\u00e9sStrawmanOverallIdentified subsequence57 56 20 36 50 42 48 40444526 62 41 4143Fully identified%7 180 1856 11 502521337 23 1023Not identified35 25 80 45 44 51 399293340 30 35 4833Number of instances14 445 22 18 66 68 87 325 183 145 40 17 291063"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Selected shallow features one may hypothesize impact evaluation scores negatively (SI).", "figure_data": "FeatureCount P-valuecommainside119 < 0.001we150.002this280.007will400.008not620.013exclamation160.014CIAbefore25 < 0.001according to after8 < 0.001quotationbefore650.004"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Selected shallow features one may hypothesize impact evaluation scores negatively (TC).", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "[BOS] Democrats acted like [BOP] babies [EOP] at the SOTU [EOS]", "formula_coordinates": [3.0, 137.85, 426.1, 321.84, 9.46]}, {"formula_id": "formula_1", "formula_text": "(x, y) = \u2212 1 N d N n=1 d k=1 p k y k n log x k n + (1 \u2212 y k n ) log(1 \u2212 x k n ) p k = 1 f k max(f )", "formula_coordinates": [3.0, 165.09, 588.44, 267.36, 61.52]}, {"formula_id": "formula_2", "formula_text": ".0 \u2192 .1 + \u2212 \u22121.1 + \u22121.6 .0 \u2212 \u22120.4 + \u22121.1 8 \u2192 16 .1 \u2192 .0 \u2212 \u2212 \u22123.9 + \u22127.0 .1 \u2212 \u22120.7 + \u22121.3", "formula_coordinates": [4.0, 310.79, 229.87, 196.34, 83.07]}], "doi": ""}