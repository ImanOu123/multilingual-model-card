{"title": "DIBIMT: A Novel Benchmark for Measuring Word Sense Disambiguation Biases in Machine Translation", "authors": "Niccol\u00f2 Campolungo; Federico Martelli; Francesco Saina; Roberto Navigli", "pub_date": "", "abstract": "Lexical ambiguity poses one of the greatest challenges in the field of Machine Translation. Over the last few decades, multiple efforts have been undertaken to investigate incorrect translations caused by the polysemous nature of words. Within this body of research, some studies have posited that models pick up semantic biases existing in the training data, thus producing translation errors. In this paper, we present DIBIMT, the first entirely manuallycurated evaluation benchmark which enables an extensive study of semantic biases in Machine Translation of nominal and verbal words in five different language combinations, namely, English and one or other of the following languages: Chinese, German, Italian, Russian and Spanish. Furthermore, we test state-of-the-art Machine Translation systems, both commercial and non-commercial ones, against our new test bed and provide a thorough statistical and linguistic analysis of the results. We release DIBIMT at https:// nlp.uniroma1.it/dibimt as a closed benchmark with a public leaderboard. He poured a shot of whiskey.", "sections": [{"heading": "Introduction", "text": "The polysemous nature of words poses a longstanding challenge in a wide range of Natural Language Processing (NLP) tasks such as Word Sense Disambiguation (Navigli, 2009; (WSD), Information Retrieval (Krovetz and Croft, 1992) (IR) and Machine Translation (Emelin et al., 2020) ", "publication_ref": ["b15", "b7", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "(MT).", "text": "In MT, some research works have addressed the ability of systems to disambiguate polysemous words. For instance, given the sentence He poured a shot of whiskey, the polysemous target word shot unequivocally means a small quantity and therefore a possible translation into Italian could be: Vers\u00f2 un goccio di whiskey. However, some MT systems propose the following translation: Vers\u00f2 uno sparo * Equal contribution.\ndi whiskey in which the noun sparo means gunshot. This is one of many examples that seem to encourage a deeper performance analysis in scenarios in which MT systems are required to deal with polysemous words and, specifically, with infrequent meanings of polysemous words. Although state-of-the-art MT systems, both commercial and non-commercial ones, achieve impressive BLEU scores on standard benchmarks, in our work we demonstrate that they still present significant limitations when dealing with infrequent word senses, which standard metrics fail to recognize.\nIn the last few decades, attempts have been made to investigate the aforementioned phenomena. In fact, recent studies have observed a direct correlation between semantic biases in the training data and semantic errors in translation. However, their findings are limited by the following shortcomings: i) they are not based on entirely manually-curated benchmarks; ii) they rely heavily on automaticallygenerated resources to determine the correctness of a translation; and iii) they do not cover multiple language combinations.\nIn this work, we address the aforementioned drawbacks and present DIBIMT, to the best of our knowledge the first fully manually-curated evaluation benchmark aimed at investigating the impact of semantic biases in MT in five language combinations, covering both nouns and verbs. This benchmark allows the community not only to better explore the described phenomena, but also to devise innovative MT systems which better deal with lexical ambiguity. Specifically, the contributions of the present work are threefold:\n\u2022 We present DIBIMT, a novel gold-quality test bed for semantic biases in MT that goes beyond a simple accuracy score, covering five language combinations, namely English and one or other of the following languages: Chinese, German, Italian, Russian and Spanish;\nFigure 1: Example of an annotated dataset item. Target word is shot, in its meaning of a \"small drink of liquor\". We expect translations to contain, for example in Italian, goccio (lit. a drop), but not, for example in Spanish, pistolero (a person who shoots).\n\u2022 We define four novel metrics that better clarify the semantic biases within MT models;\n\u2022 We provide a thorough statistical and linguistic analysis in which we compare 7 state-ofthe-art MT systems, including both commercial and non-commercial ones, against our new benchmark. Furthermore, we extensively discuss the results.\nTo enable further research, we release DIBIMT as a closed benchmark with a public leaderboard at https://nlp.uniroma1.it/dibimt.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Over the course of the last few decades, several approaches to the evaluation of the lexical choice in MT have been proposed. To this end, cross-lingual benchmarks were created in which systems were required to provide the translation or a substitute for a given target word in context in a target language (Vickrey et al., 2005;Mihalcea et al., 2010;Lefever and Hoste, 2013).\nMore recently, Gonzales et al. (2017) put forward ContraWSD, a dataset which includes 7,200 instances of lexical ambiguity for German \u2192 English, and 6,700 for German \u2192 French. This dataset pairs every reference translation with a set of contrastive examples which contain incorrect translations of a polysemous target word. For each instance, the answer provided by systems is considered correct if the reference translation is scored higher. Based on a denoised version of the ContraWSD dataset and focusing on the language combination German \u2192 English, Gonzales et al. (2018) present the Word Sense Disambiguation Test Suite which, unlike ContraWSD, evaluates MT output directly rather than by scoring translations. The suite consists of a collection of 3,249 sentence pairs in which the German source sentences contain one ambiguous target word. As target words, the authors considered only words in German whose translation into English does not cover multiple senses, thus making the evaluation more straightforward. Despite their effectiveness, such benchmarks do not allow systems to be tested in multiple language combinations, and only cover a very limited number of words and senses. To address these limitations, Raganato et al. (2019) proposed MuCoW, an automatically-created test suite covering 16 language pairs, with more than 200,000 sentence pairs derived from word-aligned parallel corpora.\nOther research studies investigated the disambiguation capabilities of MT systems by exploring their internal representations (Marvin and Koehn, 2018;Michel et al., 2019), or improving them via context-aware word embeddings (Liu et al., 2018). More recently, Emelin et al. (2020) introduced a statistical method for the identification of disambiguation errors in neural MT (NMT) and demonstrated that models capture data biases within the training corpora, which leads these models to produce incorrect translations. Although the authors expected their approach to be transferable to other language combinations, they only focused on German \u2192 English.\nBased on the findings and open research questions raised in the aforementioned works, the present paper aims at investigating not only the presence, but also, most importantly, the nature and properties of semantic biases in MT in multiple language combinations, via a novel entirely manually-curated benchmark called DIBIMT and a thorough performance analysis.", "publication_ref": ["b22", "b13", "b9", "b5", "b6", "b19", "b11", "b12", "b10", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "Building DIBIMT", "text": "The DIBIMT benchmark focuses on detecting Word Sense Disambiguation biases in NMT, i.e., biases of certain words towards some of their more frequent meanings. The creation of such a dataset requires i) a set of unambiguous and grammaticallycorrect sentences containing a polysemous target word; ii) a set of correct and incorrect translations of each target word into the languages to be covered. Figure 1 depicts an example of a dataset item.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "BabelNet Similarly to previous studies, we rely on BabelNet 1 , a large multilin-gual encyclopedic dictionary whose nodes are concepts represented by synsets, i.e., sets of synonyms, containing lexicalizations in multiple languages and coming from various heterogeneous resources, including, inter alia, WordNet (Miller et al., 1990) and Wiktionary. 2 Let us define B as an abstraction used to query the subset of synsets in BabelNet that contain at least one sense 3 from WordNet and one or more senses in languages other than English, 4 while only considering senses coming from highquality sources, i.e., language-specific wordnets.\nFormal Notation Given an arbitrary synset \u03c3, we define \u039b L (\u03c3) as the set of lexicalizations of \u03c3 in language L contained within B. As an example, let us consider the synset\u03c3 corresponding to the drink meaning of the word shot.\u03c3 contains lexicalizations in different languages, including: Shot DE , shot EN , nip EN , chupito ES , trago ES , bicchierino IT and goccio IT . Hence, \u039b EN (\u03c3) = {shot, nip}, while \u039b ES (\u03c3) = {chupito, trago}.\nFurthermore, let \u03bb P represent a (lemma, part of speech) pair, where P is the part of speech. We denote \u2126 L (\u03bb P ) = {\u03c3 1 , . . . , \u03c3 n } as the set of synsets which contain \u03bb P as a lexicalization in language L according to B. Additionally, we define \u03b4 L (\u03bb P ) = |\u2126 L (\u03bb P )| as the polysemy degree, i.e., the number of senses, of \u03bb P in language L. For example, given \u03bb P = shot N OU N , \u2126 EN (\u03bb P ) would be the set of synsets associated with the nominal term shot (e.g., the act of firing, a photograph and a drink, among others).", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}, {"heading": "Sentence Selection Process", "text": "In this section, we detail the creation process of our dataset, i.e., the selection of our sentences as well as the construction and filtering of our items.\nItem Structure and Notation Before we proceed, let us formally state how each item in the dataset is structured: given a source sentence s = [w 1 , . . . , w n ] as a sequence of words, and given a target word 5 w i in s tagged with some synset \u03c3, we consider X = (s, w i , \u03c3) as an initial item of the dataset, i.e., an instance composed of an English sentence s, a target word w i and its associated synset \u03c3; this instance can be annotated for candidate translations of w i in some language L. We also denote \u03bb X P as the (lemma, POS) pair of w i .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Starting Sentence Pool", "text": "We collect our initial items from two main sources: WordNet and Wiktionary. 6 Specifically, we use the examples from WordNet Tagged Glosses (Langone et al., 2004), where each sentence's target word was manually associated with its synset 7 , thereby readily providing the first batch of initial items.\nAs for Wiktionary, instead, we start by obtaining every usage example s and its associated definition d (filtering out archaic usages and slang), then, we automatically extract the target words from the corresponding example. 8 Now, the only step that remains in order to construct an initial item is to associate a synset \u03c3 with the word w i used in the example s. We perform this association in two phases: first, we try to map the definition d related to the example s to a BabelNet synset by relying on the automatic mappings available in BabelNet 5 between WordNet and Wiktionary, discarding examples for which this association can not be found; second, we manually validate and correct these successful associations to ensure that our initial items are of high quality.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "Sentence Filtering", "text": "We apply a filtering step to the original sentences in order to select examples that are likely to be more challenging for the models to translate: i) we discard every initial item X for which \u03b4 EN (\u03bb X P ) < 3, i.e., we retain only sentences whose associated (lemma, POS) pair has a polysemy degree of at least 3 in B EN ; ii) we retain at most only one sentence per sense per source 9 ; iii) differently from previous works, which impose a strict requirement on synsets that are monosemous in the target language, we retain sentences satisfying the following requirement. Let us consider the nominal senses of the word bank: among them, one represents a specific aviation maneuver. In Italian, this synset includes one lexicalization, avvitamento; although this is not monosemous in Italian (e.g., avvitamento might also refer to a screw thread), neither of the other possible senses of avvitamento has bank as an English lexicalization, which, for Italian, satisfies our third condition. If the same holds true for all languages, the synset passes the test and thus the sentence is retained.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Annotating the Dataset", "text": "Once the set of initial items is ready, we can proceed with the annotation phase, which will produce our annotated items.\nSpecifically, given a language L and an initial item X = (s, w i , \u03c3), we associate a set of good (G L ) and bad (B L ) translation candidates with X, which represent words that, respectively, we do, and do not, expect to see in a translation of sentence s in language L. Finally, we refer to X L as an annotated item, i.e., the tuple (s, w i , \u03c3, G L , B L ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Pre-annotation Item Creation", "text": "Before moving forward with the annotation phase, we pre-populate the sets of good (G L ) and bad (B L ) lexicalizations for a given initial item X in language L extracting them from B. Formally, we assign G L = \u039b L (\u03c3), i.e., the set of lemmas in language L of the BabelNet synset associated with \u03c3; furthermore, we set B L = \u03c3\u2208\u2126 L (\u03bb X P )\\{\u03c3} \u039b L (\u03c3), i.e., the set of all lemmas in language L of BabelNet synsets associated with any\u03c3 excluding \u03c3. With this step, we produce an automatically populated version of our annotated items.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Annotation Guidelines", "text": "We instruct annotators to update the set of good (G L ) and bad (B L ) lexicalizations of w i \u2208 s such that each lexicalization contained in the respective set can be considered a good or a bad translation equivalent for the target word in the provided sentential context. 10 We also instruct annotators to discard sentences in which i) the target word w i is an idiomatic expression or a proper noun, and ii) the semantic context is not sufficient to properly disambiguate w i .\nGiven the expertise required to carry out this task, we rely on three highly qualified translators: one for Italian, German and Russian; one for Spanish and one for Chinese. Our annotators satisfy the   following requirements: they are native speakers or hold C2-level certifications and work as professional translators in the given language combinations. The full instructions provided to the annotators can be found in Appendix C.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Resulting Dataset", "text": "Our annotators analyzed around 800 sentences, discarding 200 of them, finally obtaining approximately 600 annotated items in 5 languages. Due to a coverage issue of the Russian language in Babel-Net, we retain only sentences tagged with nominal or verbal synsets. Dataset statistics are reported in Table 1.\nAs expected, we note that the lexicalizations found in B have been substantially refined by our annotators in all languages, as reported in Table 2. Indeed, across languages, on average, 54% of the good lexicalizations have been added by our annotators, while 42% of the pre-existing lexicalizations have been removed. More importantly, given a language and two sentences containing words referring to the same synset, on average only in 55% of cases do they also share those words' good lexicalizations, confirming that the assumption that all synonyms of a word are valid replacements can lead to incorrect results.\nThese statistics lead us to a straightforward, but important, conclusion: only in a limited number of cases is a lexicalization belonging to a given synset to be considered as a suitable translation equivalent for the provided target word and its context. Examined jointly, these metrics suggest that relying on synset lexicalizations from BabelNet alone is prone to producing errors, either due to BabelNet's intrinsic noise, or due to the lack of different granularity of synsets and contextualized words.\nSentences' Properties Description As we stated in Section 3.2.1, the sentences we annotate are all usage examples of specific concepts obtained from WordNet or Wiktionary. Such examples are typically short main clauses with no subordinates, featuring on average 9 words (around 50 characters per sentence). All selected sentences include a semantic context which allows the meaning of the target word to be properly identified.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1", "tab_2"]}, {"heading": "Analysis Procedure", "text": "DIBIMT's analysis procedure is fairly simple: given an annotated item X L = (s, w i , \u03c3, G L , B L ) and a translation model M, we compute t L = M L (s), i.e., the translation of s in language L according to M. Then, we use Stanza (Qi et al., 2020) to perform tokenization, part-of-speech tagging and lemmatization of t L and, finally, we check if there is any match 11 between the lemmas of the translated sentence and those contained in G L or B L . In case there is no match, we mark the translation as a MISS; otherwise, we mark it as GOOD or BAD depending on which set matched the lemma.\nThis produces an analyzed item, which for simplicity we denote as X\nM L = (X L , t L , R, \u03c9 L ),\nwhere R is one of GOOD, BAD or MISS and \u03c9 L represents the matched lemma in case there was a match (GOOD or BAD), \u01eb otherwise. of semantic bias; and iv) offer some insights into the causes of such biases. In Appendix D we include a model-specific breakdown of the various scores and metrics reported throughout this section.", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}, {"heading": "Comparison Systems", "text": "We test a wide range of models, both commercial and non-commercial ones, and report their performances on DIBIMT's evaluation metrics:\n\u2022 DeepL Translator 12 , a state-of-the-art commercial NMT system.\n\u2022 Google Translate 13 , arguably the most popular commercial NMT system.\n\u2022 OPUS (Tiedemann and Thottingal, 2020), the smallest state-of-the-art NMT model available to date, a base Transformer (each model has approximately 74M parameters) trained on a single language pair on large amounts of data.\n\u2022 MBart50 (Tang et al., 2021), multilingual BART fine-tuned on the translation task for 50 languages (610M parameters). We refer to MBart50 as the English-to-many model, and to MBart50 MTM as the many-to-many model.\n\u2022 M2M100 , a multilingual model able to translate from/to 100 languages. We test both versions of the model, the 418M parameter one (which we dub M2M100) and the 1.2B parameter one (dubbed M2M100 LG ).   do with the source text 14 (around 23%); and v) missing terms from either B L (around 18%) or G L (around 11%). We intend to thoroughly investigate and tackle these issues and translation phenomena as future work.", "publication_ref": ["b21", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Discussion of MISS", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "General Results", "text": "Table 3 reports accuracy for non-MISS analyzed items (i.e., #GOOD #GOOD+#BAD ). With the sole exception of DeepL, which greatly outperforms every other competitor, models achieve extremely low scores, in the range of 20%-33%. Surprisingly, Google Translate performs worst across languages.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Analyzing the Semantic Biases", "text": "In addition to accuracy, DIBIMT analyzes the semantic biases of a translation model via four novel metrics, which we define in detail in what follows.\nSense Frequency Index Influence (SFII) We study the sensitivity of models to disambiguating senses with respect to their frequency. To do this, we define \u00b5 \u03bb P (\u03c3) as the index of synset \u03c3 in \u2126 EN (\u03bb P ) ordered according to WordNet's sense frequency, as computed from SemCor. That is, in-dex k means that synset \u03c3 is the k-th most frequent meaning for \u03bb P .\nIn Figure 3(a), we plot the number and percentage of errors made on average by the models, grouping items by \u00b5 \u03bb X P (\u03c3 X ), where X is a non-MISS analyzed item. As expected, the less frequent a meaning for a given word is, the harder it is for the model to correctly disambiguate it.\nFinally, given a (model, language) pair, we define the Sense Frequency Index Influence (SFII) as the average percentage of errors, for each group, that we detected. Values are reported in Table 4. Interestingly, DeepL proves once again to be the best, obtaining a score of 51%, far below the average 80% achieved by the other models, with most non-commercial models performing \u2264 80%.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": ["tab_5"]}, {"heading": "Sense Polysemy Degree Importance (SPDI)", "text": "Similarly to SFII, we also study the extent to which the polysemy degree, i.e., how many senses a given word can have, impacts the models' disambiguation capabilities. This experiment mirrors SFII, but groups items by their lemma's polysemy degree \u03b4 EN (\u03bb X P ) instead of \u00b5. Figure 3(b) reports the results on all items. Unsurprisingly, similarly to the frequency index, we observe that higher polysemy leads to more errors, confirming that models still struggle with very polysemous words. Similarly to SFII, SPDI is defined as the average percentage of   A full-page version of this image, for readability purposes, is available in the Appendix (Figure 16).\nerrors at varying polysemy degrees, and its values are reported in Table 4: once again, DeepL outperforms all other systems by a large margin, confirming that it is the least biased across the board.", "publication_ref": [], "figure_ref": ["fig_1", "fig_11"], "table_ref": ["tab_5"]}, {"heading": "Most and More Frequent Senses", "text": "To further corroborate our findings about semantic biases, we study how often models predict senses that are more frequent than the target one. Given a BAD analyzed item X M L , we denote\u03c3 as the synset associated with the wrongly translated lemma \u03c9 L . 15 Then, we check the frequency of \u03c3 and\u03c3 with respect to \u03bb X P : if \u00b5 \u03bb X P (\u03c3) < \u00b5 \u03bb X P (\u03c3), then the system's disambiguation steered towards a sense that is more frequent than the target one, which we 15 In the case in which there are multiple possible synsets, we take the most frequent according to \u00b5 \u03bb X P , as we need to rely on the assumption that the surface form represents the intrinsic disambiguation performed by the NMT system.  dub More Frequent Sense (MFS+); additionally, if \u00b5 \u03bb X P (\u03c3) = 1, then the model disambiguated the source word w i to the Most Frequent Sense (MFS) of the associated lemma \u03bb X P . The results of both these analyses are reported in Table 5.\nWe can observe a few interesting results: first, on average, almost 60% of the time a mistake reflects the Most Frequent Sense of the target word (secondlast column); second, almost 90% of the errors concern translations towards more frequent senses of the target word (last column). Importantly, these results are consistent across systems, whether commercial or not. Although it might seem straightforward, NMT models are still strongly biased towards senses that are more likely to be encountered during training; while this could be related to the patternmatching nature of neural networks, it also depends heavily on the training data the model was trained upon, and this needs to be further investigated in future research.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_7"]}, {"heading": "Are verbs harder than nouns?", "text": "The existing literature in WSD points to the fact that verbs are generally harder than nouns, mostly due to their highly polysemous nature (Barba et al., 2021b). We try to analyze whether MT models are affected by the same phenomenon: in Table 6, we report the average results obtained by running DIBIMT on all its sentences (column ALL) and the subset of sentences whose target word was either a NOUN or a VERB. In general, we observe an average drop of accuracy of 4 points, as well as an astounding difference of 18 percentage points in MISS handling, which we will investigate more thoroughly in future work. Interestingly, MT models are much more inclined to translate nouns into their most frequent sense; we attribute this difference to the generally higher polysemy of verbs compared to nouns, which increases the size of the space of possible translations for a given verb, thus decreasing the chance that it gets translated into the MFS. Aside from this, we draw the same conclusion as that drawn by previous works in the field of WSD, with nouns being generally easier to translate than verbs.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": ["tab_9"]}, {"heading": "Is the encoder disambiguating?", "text": "We try to assess to what extent, in a multilingual encoder-decoder architecture, the encoder is determining the implicit disambiguation of the source sentence before generating the translation. For instance, we ask ourselves this question: given an ambiguous word w i in the source sentence s, how often does the model translate it into a lexicalization representing the same sense, if prompted to translate s into different languages? Intuitively, if the encoder was the sole contributor to the implicit disambiguation performed by the model, we would expect to see the meaning to always be the same, regardless of the target language.\nTo measure this, we perform the following experiment: given a model M, 16 two languages L1 16 We disregard OPUS here as it is a set of bilingual models, rather than a single model capable of translating into multiple We observe that, on average, this phenomenon occurs around 70% of the time. Hence, it is safe to assume that, while the encoder certainly plays an important role in the disambiguation of the input sentence, the decoder is also contributing significantly. Another interesting observation is that the alphabet of the target language does not seem languages. We also disregard DeepL and Google Translate as their architecture is proprietary. 17 We skip item X if either  to have any influence, as language pairs involving Russian display scores that are very similar to those of the other three European languages. We attribute lower scores in Chinese to coverage issues in Ba-belNet, which would hinder a correct fulfillment of the condition defined for this experiment.\nX M L1 or X M L2 is a MISS.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "How challenging is DIBIMT?", "text": "Given the low performances achieved by MT models, we test a WSD system on the English sentences within DIBIMT, both to assess the toughness of our system and to establish an additional baseline. We use ESCHER 18 (Barba et al., 2021a), a stateof-the-art model on English WSD. Interestingly, ESCHER achieves an overall accuracy score of 66.33, almost 15 points lower than the results on the standard WSD benchmark (80.7 on ALL, Raganato et al., 2017), therefore confirming the challenging nature of DIBIMT. Furthermore, in order to estimate the difference in disambiguation capability between NMT models and a dedicated WSD system, we compute ESCHER's performances on the set of English sentences of non-MISS analyzed items for each (model, language) pair. We report these results in Table 7, whose accuracy scores can be directly compared to those in Table 3. As expected, the average MT accuracy is significantly lower than ESCHER's, with the sole exception of DeepL, which manages to surpass it on German and Russian. These results clearly demonstrate that current NMT models are still not on par with dedicated WSD systems, and thus that they might benefit from the inclusion of such WSD systems within the NMT ecosystem.", "publication_ref": ["b0", "b18"], "figure_ref": [], "table_ref": ["tab_11", "tab_4"]}, {"heading": "Is this a decoding issue?", "text": "As a final experiment, we assess whether the semantic biases are caused by search errors (i.e., failures of the decoding algorithm), or model errors (i.e., the models deemed their translations the best 18 The publicly available version trained on SemCor data only.  possible). For each (model M, language L) pair, we sample a BAD translation (t BAD ), pair it with a GOOD translation (t GOOD ) produced by another model (prioritizing DeepL), and ask annotators to check their correctness and apply corrections where needed, 19 then compute the perplexities according to M with the corresponding English sentence, and call them p GOOD and p BAD respectively. We repeat this sampling 50 times per (M, L) pair and check how often p BAD < p GOOD . Table 8 shows that, on average, this happens in 93% of cases, thus confirming that most semantic biases are embedded within models and are not caused by the decoding strategy.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_13"]}, {"heading": "Conclusions", "text": "In this work, we presented DIBIMT, a novel benchmark for measuring and understanding semantic biases in NMT, which goes beyond simple accuracy and provides novel metrics that summarize how biased NMT models are. We tested DIBIMT on 7 widely adopted NMT systems, extensively discussing their performances and providing novel insights into the possible causes and relations of semantic biases within NMT models. Furthermore, statistics of our annotations suggest that, when dealing with translations, synsets' lexicalizations cannot be used interchangeably, as their choice depends heavily on the context.\nIn the future, we plan to improve DIBIMT by introducing better heuristics to recognize and handle MISS cases, especially covering the linguistic phenomena we described (see Section 4.2); we also aim at widening language coverage and increasing the number of sentences in the benchmark, consequently improving word and sense coverage. To enable further research, we release DIBIMT as a closed benchmark with a public leaderboard at: https://nlp.uniroma1.it/dibimt.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Analysis Procedure Details", "text": "Our analysis procedure, which we described in Section 3.4, involves steps that go beyond simple lemma matching. For instance, in case of multiword expressions, we allowed annotators to specify a wildcard, i.e., any number of tokens (including zero) were allowed to expand and still trigger a match. Additionally, since Stanza has multi-word expansion tokenization for some of the languages in our list, when available, we try to perform matching on both the list of words (alongside the list of tokens) in the translated sentence. Finally, in case no match is produced by the aforementioned steps, we apply a surface-level string matching heuristic which, especially in Chinese, helps us increase coverage.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Neural Models Implementation", "text": "We use HuggingFace's Transformers library (Wolf et al., 2020) for all neural models. As per standard practice, we generate translations using beam search as decoding algorithm with beam size 5.", "publication_ref": ["b23"], "figure_ref": [], "table_ref": []}, {"heading": "C Instructions for Dataset Annotation", "text": "In this work, we investigate semantic biases in Machine Translation across languages. You are provided with a spreadsheet containing 300 instances, each including the following information: a lemma, its part of speech, a definition and some good and bad translation candidates derived from BabelNet. Your task is to manually verify the correctness of the good candidates and add new good candidates if deemed necessary. Furthermore, you are asked to verify that all bad candidates are wrong.\nFrom a translation perspective, a good candidate is a word which correctly translates the English target word in the given context. Instead, a bad candidate is a wrong translation of the English target word in the given context.\nPlease adopt the following guidelines while annotating:\n\u2022 Do not annotate idioms.\n\u2022 Do not annotate instances in which the semantic context does not allow us to unequivocally determine the meaning of the target word.\n\u2022 Do not annotate proper names, e.g., \"Run\" in the sentence The military campaign near that creek was known as \"The battle of Bull Run\".\n\u2022 You are allowed to include cross-PoS candidates (that is, candidates whose PoS is different from that of the target word), in this case please include the candidate in square brackets like this:\n[candidate_with_different_pos|Px],\nwhere x represents the part-of-speech tag of the translated word. Do this for multi-word expressions as well.\nMark with the tag \"DISCUSS\" difficult instances which you would like to discuss.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D Model-specific Analyses", "text": "We include model-specific analyses with perlanguage breakdown of the scores achieved on our benchmark. The column named ESCHER provides the scores of the WSD system on the subset of sentences of the specified model and language, and should be treated as an additional baseline to compare with the accuracy achieved by the system. Details can be found in Section 4.\n\u2022 DeepL\n\u2022 Google\n\u2022 OPUS\n\u2022 M2M100\n\u2022 M2M100 LG \u2022 MBart50\n\u2022 MBart50 MTM                ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "bn:00057755n", "text": "He poured a shot of whiskey.\nA small drink of liquor.    ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "The authors gratefully acknowledge the support of the ERC Consolidator Grant MOUSSE No. 726487 and the ELEXIS project No. 731015 under the European Union's Horizon 2020 research and innovation programme, and the PerLIR project (Personal Linguistic resources in Information Retrieval) funded by the MIUR Progetti di ricerca di Rilevante Interesse Nazionale programme (PRIN  2017). This work was also partially supported by the MIUR under the grant \"Dipartimenti di eccellenza 2018-2022\" of the Department of Computer Science of Sapienza University.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Esc: Redesigning WSD with extractive sense comprehension", "journal": "", "year": "2021", "authors": "Edoardo Barba; Tommaso Pasini; Roberto Navigli"}, {"ref_id": "b1", "title": "ConSeC: Word Sense Disambiguation as continuous sense comprehension", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Edoardo Barba; Luigi Procopio; Roberto Navigli"}, {"ref_id": "b2", "title": "Recent trends in word sense disambiguation: A survey", "journal": "", "year": "2021", "authors": "Michele Bevilacqua; Tommaso Pasini; Alessandro Raganato; Roberto Navigli"}, {"ref_id": "b3", "title": "Detecting word sense disambiguation biases in machine translation for model-agnostic adversarial attacks", "journal": "", "year": "2020", "authors": "Denis Emelin; Ivan Titov; Rico Sennrich"}, {"ref_id": "b4", "title": "Beyond English-Centric Multilingual machine translation", "journal": "Journal of Machine Learning Research", "year": "2021", "authors": "Angela Fan; Shruti Bhosale; Holger Schwenk; Zhiyi Ma; Ahmed El-Kishky; Siddharth Goyal; Mandeep Baines; Onur Celebi; Guillaume Wenzek; Vishrav Chaudhary"}, {"ref_id": "b5", "title": "Improving Word Sense Disambiguation in Neural Machine Translation with Sense Embeddings", "journal": "", "year": "2017", "authors": "Annette Rios Gonzales; Laura Mascarell; Rico Sennrich"}, {"ref_id": "b6", "title": "The Word Sense Disambiguation Test Suite at WMT18", "journal": "", "year": "2018", "authors": "Annette Rios Gonzales; Mathias M\u00fcller; Rico Sennrich"}, {"ref_id": "b7", "title": "Lexical ambiguity and information retrieval", "journal": "ACM Transactions on Information Systems (TOIS)", "year": "1992", "authors": "Robert Krovetz; Bruce Croft"}, {"ref_id": "b8", "title": "Annotating WordNet", "journal": "", "year": "2004", "authors": "Helen Langone; R Benjamin; George A Haskell;  Miller"}, {"ref_id": "b9", "title": "Semeval-2013 task 10: Cross-lingual Word Sense Disambiguation", "journal": "", "year": "2013", "authors": "Els Lefever; V\u00e9ronique Hoste"}, {"ref_id": "b10", "title": "Handling Homographs in Neural Machine Translation", "journal": "Long Papers", "year": "2018", "authors": "Frederick Liu; Han Lu; Graham Neubig"}, {"ref_id": "b11", "title": "Exploring Word Sense Disambiguation Abilities of Neural Machine Translation Systems", "journal": "", "year": "2018", "authors": "Rebecca Marvin; Philipp Koehn"}, {"ref_id": "b12", "title": "On Evaluation of Adversarial Perturbations for Sequence-to-Sequence Models", "journal": "Long and Short Papers", "year": "2019", "authors": "Paul Michel; Xian Li; Graham Neubig; Juan Pino"}, {"ref_id": "b13", "title": "Semeval-2010 task 2: Cross-Lingual Lexical Substitution", "journal": "", "year": "2010", "authors": "Rada Mihalcea; Ravi Sinha; Diana Mccarthy"}, {"ref_id": "b14", "title": "Introduction to wordnet: An on-line lexical database", "journal": "International journal of lexicography", "year": "1990", "authors": "A George; Richard Miller; Christiane Beckwith; Derek Fellbaum; Katherine J Gross;  Miller"}, {"ref_id": "b15", "title": "Word Sense Disambiguation: A Survey", "journal": "", "year": "2009", "authors": "Roberto Navigli"}, {"ref_id": "b16", "title": "Ten Years of Babelnet: A Survey", "journal": "", "year": "2021", "authors": "Roberto Navigli; Michele Bevilacqua; Simone Conia; Dario Montagnini; Francesco Cecconi"}, {"ref_id": "b17", "title": "Stanza: A Python Natural Language Processing Toolkit for Many Human Languages", "journal": "", "year": "2020", "authors": "Peng Qi; Yuhao Zhang; Yuhui Zhang; Jason Bolton; Christopher D Manning"}, {"ref_id": "b18", "title": "Word Sense Disambiguation: A Unified Evaluation Framework and Empirical Comparison", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Alessandro Raganato; Jose Camacho-Collados; Roberto Navigli"}, {"ref_id": "b19", "title": "The MuCoW test suite at WMT 2019: Automatically harvested multilingual contrastive word sense disambiguation test sets for machine translation", "journal": "", "year": "2019", "authors": "Alessandro Raganato; Yves Scherrer; J\u00f6rg Tiedemann"}, {"ref_id": "b20", "title": "Multilingual Translation from Denoising Pre-Training", "journal": "", "year": "2021", "authors": "Yuqing Tang; Chau Tran; Xian Li; Peng-Jen Chen; Naman Goyal; Vishrav Chaudhary; Jiatao Gu; Angela Fan"}, {"ref_id": "b21", "title": "OPUS-MT -Building open translation services for the World", "journal": "", "year": "2020", "authors": "J\u00f6rg Tiedemann; Santhosh Thottingal"}, {"ref_id": "b22", "title": "Word-Sense Disambiguation for Machine Translation", "journal": "", "year": "2005", "authors": "David Vickrey; Luke Biewald; Marc Teyssier; Daphne Koller"}, {"ref_id": "b23", "title": "Transformers: State-of-the-Art Natural Language Processing", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; R\u00e9mi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger; Quentin Drame; Alexander M Lhoest;  Rush"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: General results of the analysis. Numbers represent percentages of the whole dataset (600 items).A full-page version of this image, for readability purposes, is available in the Appendix (Figure16).", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Overall distribution of errors, summed across all models and languages, with respect to (a) Sense Frequency Index (\u00b5 \u03bb P (\u03c3)) and (b) Sense Polysemy Degree (\u03b4 EN (\u03bb P )). Red bars represent the number of errors (i.e., BAD items) for a given group, grey bars represent the number of correct (i.e., GOOD items) items. Orange lines represent the percentage of errors (i.e., #BAD #GOOD+#BAD ) for a given group.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Language Frequency Correlation: percentage of times that an item translates to the same synset.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "FigureFigure 5: Evaluation on DeepL", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "FigureFigure 6: Evaluation on Google", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "FigureFigure 7: Evaluation on OPUS", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "FigureFigure 8: Evaluation on M2M100", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 9 :9Figure 9: Overall Language Cooccurrence Heatmap for M2M100", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "FigureFigure 10: Evaluation on M2M100 LG", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 11 :11Figure 11: Overall Language Cooccurrence Heatmap for M2M100 LG", "figure_data": ""}, {"figure_label": "13", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "FigureFigure 13 :13Figure 12: Evaluation on MBart50", "figure_data": ""}, {"figure_label": "16", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "FigureFigure 16 :16Figure 14: Evaluation on MBart50 MTM", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "General statistics of our annotated dataset. POS-specific lemmas do not sum to \"All\" as they can overlap across POS tags (e.g., run).", "figure_data": "%OG %RG %SLDE50.925.0 59.7ES49.619.5 47.7IT49.138.2 67.1RU67.457.3 54.4ZH55.269.0 46.3Mean54.441.8 55.0"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Figure2reports general results of the analysis per (model, language) pair. Given the high percentage of analyzed items classified as MISS, we asked our annotators to perform an inspection on a random sample of 70 items per language in order to unearth the reasons, with varying results.", "figure_data": "DeepL Google M2M100 M2M100 LG MBart50 MBart50 MTM OPUS MeanDE74.6021.9022.1926.9628.7328.65 27.99 33.00ES57.8722.5425.5130.0033.8932.66 36.66 34.16IT53.4918.0421.8325.1429.3430.54 29.95 29.76RU71.5822.8926.2235.1936.0633.33 41.07 38.05ZH46.0015.0416.9922.3531.2134.15 27.75 27.64Mean60.7120.0822.5527.9331.8531.87 32.68 32.52We identifiedmultiple causes, namely: i) word omission in thetranslation (around 19% of items, mostly in Chi-nese and Italian); ii) issues with Stanza's tokeniza-tion (around 11%, mostly Chinese and Russian)and lemmatization (around 12%, mostly Italianand German); iii) words translated as themselves(approximately 5%, often in multilingual neuralmodels); iv) translations which have nothing to"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "General results: accuracy on DIBIMT across models and languages. Higher is better. SPDI SFII SPDI SFII SPDI SFII SPDI SFII SPDI SFII SPDI SFII SPDI DE 34.78 28.30 86.61 79.54 82.00 76.15 78.90 74.71 84.10 73.86 84.95 74.24 79.85 76.25 75.89 69.00 ES 56.04 46.14 83.84 78.41 83.08 77.95 79.87 73.84 77.13 71.06 79.06 71.57 74.85 69.12 76.27 69.73 IT 57.71 49.01 85.47 80.62 80.22 76.58 78.69 76.10 78.67 71.51 79.41 69.48 80.59 72.02 77.25 70.76 RU 41.97 33.64 84.01 83.49 79.85 78.34 74.72 69.69 73.86 70.11 78.58 72.87 68.49 69.27 71.64 68.20 ZH 64.97 59.58 91.97 87.98 91.81 87.18 88.79 82.17 80.39 73.14 76.59 71.50 79.96 75.66 82.07 76.75 Mean 51.10 43.33 86.38 82.01 83.39 79.24 80.19 75.30 78.83 71.94 79.72 71.93 76.75 72.46 76.62 70.89", "figure_data": "DeepLGoogleM2M100M2M100 LGMBart50MBart50 MTMOPUSMeanSFII SPDI SFII"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "MFS MFS+ MFS MFS+ MFS MFS+ MFS MFS+ MFS MFS+ MFS MFS+ MFS MFS+ MFS MFS+ DE 53.68 84.21 56.76 86.82 61.28 87.23 59.13 87.30 58.89 89.72 55.82 89.56 56.98 87.92 57.51 87.54 ES 59.89 87.91 61.96 89.05 61.81 89.37 61.78 88.03 60.17 91.10 63.09 91.85 64.47 91.21 61.88 89.79 IT 68.08 86.38 61.96 87.23 60.75 86.79 62.82 88.81 62.90 87.50 68.97 91.81 64.48 89.66 64.28 88.31 RU 50.00 83.33 48.12 83.28 47.87 83.41 45.25 84.16 47.39 87.20 44.91 87.96 48.40 84.04 47.42 84.77 ZH 49.07 88.89 56.05 88.20 59.06 91.34 59.35 92.45 50.66 89.87 54.17 90.28 51.71 87.45 54.30 89.78 Mean 56.14 86.15 56.97 86.92 58.15 87.63 57.66 88.15 56.00 89.08 57.39 90.29 57.21 88.06 57.08 88.04", "figure_data": "DeepLGoogleM2M100M2M100 LGMBart50MBart50 MTMOPUSMean"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Frequency Analysis: MFS represents the average percentage of times the model mistakenly translates the target word into a lexicalization belonging to the Most Frequent Sense associated with \u03bb P . MFS+, instead, checks whether the wrong translation belongs to any synset that is more frequent than the target one. Lower is better.", "figure_data": "DEESITRUZH0.2 0.4 0 0.2 0.40.09 0.480.36 0.160.56 0.360.11 0.430.41 0.320.48 0.250.08 0.430.46 0.370.46 0.20.08 0.460.37 0.190.55 0.350.08 0.310.4 0.370.51 0.32Google DeepL00 0.2 0.4 0 0.2 0.40.17 0.170.43 0.450.4 0.380.21 0.270.4 0.470.39 0.260.18 0.210.42 0.50.4 0.290.2 0.220.36 0.320.44 0.460.17 0.170.38 0.450.44 0.38MBart50 OPUS0 0.2 0.40.170.420.410.190.40.410.170.390.430.180.370.450.190.360.45MBart50-MTM0 0.2 0.4 0 0.2 0.40.16 0.110.42 0.40.42 0.490.19 0.150.44 0.430.37 0.420.16 0.130.47 0.450.37 0.420.2 0.120.38 0.360.42 0.520.13 0.090.47 0.430.4 0.49M2M100-LG M2M100GOODBADMISS"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "WSD Results: ESCHER's accuracy on the set of English sentences of non-MISS analyzed samples for each (model, language) pair. Higher is better.", "figure_data": ""}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Model Errors: percentage of times a model thought its BAD translation was better than a GOOD one.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "GoogleBack to Model-specific Analyses.", "figure_data": "%MISS Accuracy MFS MFS+ SPDISFII ESCHERDE35.8721.90 56.76 86.82 79.54 86.6171.04ES23.2922.54 61.96 89.05 78.41 83.8472.76IT23.1218.04 61.96 87.23 80.62 85.4772.58RU35.3722.89 48.12 83.28 83.49 84.0169.55ZH32.4915.04 56.05 88.20 87.98 91.9771.89Mean30.0320.08 56.97 86.92 82.01 86.3871.56"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "OPUSBack to Model-specific Analyses.", "figure_data": "%MISS Accuracy MFS MFS+ SPDISFII ESCHERDE37.8427.99 56.98 87.92 76.25 79.8566.95ES25.6936.66 64.47 91.21 69.12 74.8566.83IT29.1129.95 64.48 89.66 72.02 80.5965.82RU45.8441.07 48.40 84.04 69.27 68.4969.21ZH38.3127.75 51.71 87.45 75.66 79.9669.88Mean35.3632.68 57.21 88.06 72.46 76.7567.74"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "M2M100Back to Model-specific Analyses.", "figure_data": "%MISS Accuracy MFS MFS+ SPDISFII ESCHERDE49.4122.19 61.28 87.23 76.15 82.0065.85ES41.9125.51 61.81 89.37 77.95 83.0866.77IT42.4421.83 60.75 86.79 76.58 80.2266.35RU51.7726.22 47.87 83.41 78.34 79.8566.42ZH48.6616.99 59.06 91.34 87.18 91.8169.26Mean46.8422.55 58.15 87.63 79.24 83.3966.93"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "Back to Model-specific Analyses.", "figure_data": "%MISS Accuracy MFS MFS+ SPDISFII ESCHERDE42.0226.96 59.13 87.30 74.71 78.9067.18ES36.7530.00 61.78 88.03 73.84 79.8766.86IT37.0725.14 62.82 88.81 76.10 78.6968.50RU42.5035.19 45.25 84.16 69.69 74.7267.69ZH39.7322.35 59.35 92.45 82.17 88.7969.82Mean39.6127.93 57.66 88.15 75.30 80.1968.01"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_19", "figure_caption": "MBart50Back to Model-specific Analyses.", "figure_data": "%MISS Accuracy MFS MFS+ SPDISFII ESCHERDE40.2428.73 58.89 89.72 73.86 84.1066.87ES39.2933.89 60.17 91.10 71.06 77.1365.37IT40.1029.34 62.90 87.50 71.51 78.6764.33RU44.1636.06 47.39 87.20 70.11 73.8666.35ZH44.3531.21 50.66 89.87 73.14 80.3968.93Mean41.6331.85 56.00 89.08 71.94 78.8366.37"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_20", "figure_caption": "Back to Model-specific Analyses.", "figure_data": "%MISS Accuracy MFS MFS+ SPDISFII ESCHERDE41.2528.65 55.82 89.56 74.24 84.9567.77ES41.0632.66 63.09 91.85 71.57 79.0667.18IT43.2930.54 68.97 91.81 69.48 79.4165.81RU45.1833.33 44.91 87.96 72.87 78.5864.29ZH44.5934.15 54.17 90.28 71.50 76.5969.58Mean43.0731.87 57.39 90.29 71.93 79.7266.93"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_22", "figure_caption": "Example of item annotated in all languages. First row is the example, target word is in bold, second row is the definition of the synset associated with the word in the example.bn:00036083nThey tracked him back toward the head of the stream. The source of water from which a stream arises.", "figure_data": "\u2713\u2717GermanUrsprungKopfQuelleKommando\u2713\u2717Spanishfuentecabezamanantialjefe\u2713\u2717Italianfontetestasorgentecapo\u2713\u2717Russian\u0438\u0441\u0442\u043e\u043a\u043f\u0440\u043e\u0445\u043e\u0434\u0432\u043e\u043f\u0440\u043e\u0441\u2713\u2717Chinese\u6e90 \u5934\u5934\u65cf \u957f"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_23", "figure_caption": "Example of item annotated in all languages. First row is the example, target word is in bold, second row is the definition of the synset associated with the word in the example.", "figure_data": "bn:00094769vIf you take off for Thanksgiving you must work Christmas and vice versa.To absent oneself from work or other re-sponsibility, especially with permission.\u2713\u2717Germansich eine Auszeit nehmenlosgehensich freinehmenstarten\u2713\u2717Spanishpedir un permisosalircogerllevar\u2713\u2717Italianprendersi dei giornitogliersiprendersi un permessodecollare\u2713\u2717Russian\u0431\u0440\u0430\u0442\u044c \u0432\u044b\u0445\u043e\u0434\u043d\u043e\u0439\u0432\u044b\u0447\u0435\u0441\u0442\u044c\u043e\u0442\u0434\u044b\u0445\u0430\u0442\u044c\u0443\u0431\u0438\u0442\u044c\u2713\u2717Chinese\u8bf7 \u5047\u79bb \u5f00\u4f11 \u5047\u51cf"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_24", "figure_caption": "Example of item annotated in all languages. First row is the example, target word is in bold, second row is the definition of the synset associated with the word in the example.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "M L = (X L , t L , R, \u03c9 L ),", "formula_coordinates": [5.0, 182.82, 593.03, 107.67, 13.34]}, {"formula_id": "formula_1", "formula_text": "X M L1 or X M L2 is a MISS.", "formula_coordinates": [8.0, 412.56, 763.01, 87.09, 9.97]}, {"formula_id": "formula_2", "formula_text": "[candidate_with_different_pos|Px],", "formula_coordinates": [12.0, 327.96, 144.41, 180.16, 6.83]}], "doi": "10.18653/v1/2021.emnlp-main.112"}