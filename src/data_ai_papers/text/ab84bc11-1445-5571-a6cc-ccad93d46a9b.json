{"title": "Manifold Learning for Jointly Modeling Topic and Visualization", "authors": "Tuan M V Le; Hady W Lauw", "pub_date": "", "abstract": "Classical approaches to visualization directly reduce a document's high-dimensional representation into visualizable two or three dimensions, using techniques such as multidimensional scaling. More recent approaches consider an intermediate representation in topic space, between word space and visualization space, which preserves the semantics by topic modeling. We call the latter semantic visualization problem, as it seeks to jointly model topic and visualization. While previous approaches aim to preserve the global consistency, they do not consider the local consistency in terms of the intrinsic geometric structure of the document manifold. We therefore propose an unsupervised probabilistic model, called SE-MAFORE, which aims to preserve the manifold in the lowerdimensional spaces. Comprehensive experiments on several real-life text datasets of news articles and web pages show that SEMAFORE significantly outperforms the state-of-the-art baselines on objective evaluation metrics.", "sections": [{"heading": "Introduction", "text": "Visualization of high-dimensional data is an important exploratory data analysis task, which is actively studied by various academic communities. While the HCI community is interested in the presentation of information, as well as other interface aspects (Chi 2000), the machine learning community (as in this paper) is interested in the quality of dimensionality reduction (Van der Maaten and Hinton 2008), i.e., how to transform the high-dimensional representation into a lower-dimensional representation that can be shown on a scatterplot. This visualization form is simple, and widely applicable across various domains. One pioneering technique is Multidimensional Scaling (MDS) (Kruskal 1964). The goal is to preserve the distances in the high-dimensional space in the low-dimensional embedding. This goal also allows an objective evaluation, by verifying how well the relationships among data points are preserved by the scatterplot.\nConsider the problem of visualizing documents on a scatterplot. Commonly, a document is represented as a bag of words, i.e., a vector of word counts. This high-dimensional representation would be reduced into coordinates on a visualizable 2D (or 3D) space. When applied to documents, a visualization technique for generic high-dimensional data, e.g., MDS, may not necessarily preserve the topical semantics. Words are often ambiguous, with issues such as polysemy (same word carries multiple senses), and synonymy (different words carry the same sense).\nIn text mining, the current approach to model semantics in documents in a way that can resolve some of this ambiguity is topic modeling, such as PLSA (Hofmann 1999) or LDA (Blei, Ng, and Jordan 2003). However, a topic model by itself is not designed for visualization. While one possible visualization is to plot documents' topic distributions on a simplex, a 2D visualization space could express only three topics, which is very limiting. By going from word space to topic space, topic modeling is also a form of dimensionality reduction. Given its utility in modeling document semantics, we are interested in achieving both forms of dimensionality reductions (visualization and topic modeling) together.\nThis coupling is a distinct task from topic modeling or visualization respectively, as it enables novel capabilities. For one thing, topic modeling helps to create a richer visualization, as we can now associate each coordinate on the visualization space with both topic and word distributions, providing semantics to the visualization space. For another, the tight integration potentially allows the visualization to serve as a way to explore and tune topic models, allowing users to introduce feedback to the model through a visual interface. These capabilities support several use case scenarios. One potential use case is a document organizer system. The visualization can help in assigning categories to documents, by showing how related documents have been labeled. Another is an augmented retrieval system. Given a query, the results may include not just relevant documents, but also other similar documents (neighbors in the visualization).\nProblem Statement. We refer to the task of jointly modeling topics and visualization as semantic visualization. The input is a set of documents D. For a specified number of topics Z and visualization dimensionality (assumed to be 2D, without losing any generality), the goal is to derive, for every document in D, a latent coordinate on the visualization space, and a probability distribution over the Z topics. While we focus on documents in our description, the same approach would apply to visualization of other data types for which latent factor modeling, i.e., topic model, makes sense.\nOne approach to solve this problem is to undergo twostep reductions: going from word space to topic space using topic modeling, followed by going from topic space to coordinate space using visualization. This pipeline approach is not ideal, because the disjoint reductions could mean that errors may propagate from the first to the second reduction.\nA better way is a joint approach that builds both reductions into a single, consistent whole that produces topic distributions and visualization coordinates simultaneously. The joint approach was attempted by PLSV (Iwata, Yamada, and Ueda 2008), which derives the latent parameters by maximizing the likelihood of observing the documents. This objective is known as global consistency, which is concerned with the \"error\" between the model and the observation.\nCrucially, PLSV has not cared to meet the local consistency objective (Zhou et al. 2004), which is concerned with preserving the observed proximity or distances between documents. Local consistency is reminiscent of the objective in classical visualization (Kruskal 1964). This shortcoming is related to PLSV's assumption that the document space is Euclidean (a geometrically flat space), by sampling documents' coordinates independently in a Euclidean space.\nThe local consistency objective arises naturally from the assumption that the intrinsic geometry of the data is a low-rank, non-linear manifold within the high-dimensional space. This manifold assumption is well-accepted in the machine learning community (Lafferty and Wasserman 2007), and finds application in both supervised and unsupervised learning (Belkin and Niyogi 2003;Zhou et al. 2004;Zhu et al. 2003). Recently, there is a preponderance of evidence that manifold assumption also applies to text data in particular (Cai et al. 2008;Cai, Wang, and He 2009;Huh and Fienberg 2012). We therefore propose to incorporate this manifold assumption into a new unsupervised, semantic visualization model, which we call SEMAFORE.\nContributions. While visualization and topic modeling are, separately, well-studied problems, the interface between the two, semantic visualization, is a relatively new problem, with very few previous work. To our best knowledge, we are the first to propose incorporating manifold learning in semantic visualization, which is our first contribution. As a second contribution, to realize the manifold assumption, we propose a probabilistic model SEMAFORE, with a specific manifold learning framework for semantic visualization. Our third contribution is in describing the requisite learning algorithm to fit the parameters. Our final contribution is the evaluation of SEMAFORE's effectiveness on a series of real-life, public datasets of different languages, which shows that SEMAFORE outperforms existing baselines on a well-established and objective visualization metric.", "publication_ref": ["b7", "b17", "b12", "b3", "b15", "b26", "b17", "b18", "b0", "b26", "b27", "b4", "b5", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Classical visualization aims to preserve the highdimensional similarities in the low-dimensional embedding. One pioneering work is multidimensional scaling (MDS) (Kruskal 1964), which uses linear distance. Isomap (Tenenbaum, De Silva, and Langford 2000) uses geodesic distance, whereas LLE (Roweis and Saul 2000) uses linear distance but only locally. These are followed by a body of probabilistic approaches (Iwata et al. 2007;Hinton and Roweis 2002;Van der Maaten and Hinton 2008;Bishop, Svens\u00e9n, and Williams 1998). They are not meant for semantic visualization, as they do not model topics.\nSemantic visualization is a new problem explored in very few works. The state-of-the-art is the joint approach PLSV (Iwata, Yamada, and Ueda 2008), which we use as a baseline. In the same paper, it is shown that PLSV outperforms the pipeline approach of PLSA (Hofmann 1999) followed by PE (Iwata et al. 2007). LDA-SOM (Millar, Peterson, and Mendenhall 2009) is another pipeline approach of LDA (Blei, Ng, and Jordan 2003) followed by SOM (Kohonen 1990), which produces a different type of visualization.\nSemantic visualization refers to joint topic modeling and visualization of documents. A different task is topic visualization, where the objective is to visualize the topics themselves (Chaney and Blei 2012;Chuang, Manning, and Heer 2012;Wei et al. 2010;Gretarsson et al. 2012), in terms of dominant keywords, prevalence of topics, etc. (Cai et al. 2008;Cai, Wang, and He 2009;Wu et al. 2012;Huh and Fienberg 2012) study manifold in the context of topic models only. The key difference is that we also need to contend with the visualization aspect, and not only topic modeling, which creates new research issues.", "publication_ref": ["b17", "b22", "b21", "b14", "b11", "b23", "b2", "b15", "b14", "b20", "b3", "b16", "b6", "b8", "b24", "b10", "b4", "b5", "b25", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Semantic Visualization Model", "text": "We now describe our model, SEMAFORE, which stands for SEmantic visualization with MAniFOld REgularization.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Problem Definition", "text": "The input is a corpus of documents D = {d 1 , . . . , d N }. Every d n is a bag of words, and w nm denotes the m th word in d n . The total number of words in d n is M n . The objective is to learn, for each d n , a latent distribution over Z topics {P(z|d n )} Z z=1 . Each topic z is associated with a parameter \u03b8 z , which is a probability distribution {P(w|\u03b8 z )} w\u2208W over words in the vocabulary W . The words with highest probabilities for a given topic capture the semantic of that topic.\nUnlike topic modeling, in semantic visualization, there is an additional objective, which is to learn, for each document d n , its latent coordinate x n on a low-dimensionality visualization space. Similarly, each topic z is associated with a latent coordinate \u03c6 z on the visualization space. A document d n 's topic distribution is then expressed in terms of the Euclidean distance between its coordinate x n and the different topic coordinates \u03a6 = {\u03c6 z } Z z=1 , as shown in Equation 1. The closer is x n to \u03c6 z , the higher is the probability P(z|d n ).\nP(z|dn) = P(z|xn, \u03a6) = exp( 1 2 ||xn \u2212 \u03c6z|| 2 ) Z z =1 exp( 1 2 ||xn \u2212 \u03c6 z || 2 ) (1)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Generative Process", "text": "We now describe the assumed generative process of documents based on both topics and visualization coordinates. Our focus in this paper is on the effects of the manifold assumption on the semantic visualization task. We figure that the clearest way to showcase these effects is to design a manifold learning framework over and above an existing generative process, such as PLSV (Iwata, Yamada, and Ueda 2008), which we review below.\nThe generative process is as follows: 1. For each topic z = 1, . . . , Z:\n(a) Draw z's word distribution:\n\u03b8 z \u223c Dirichlet(\u03b1) (b) Draw z's coordinate: \u03c6 z \u223c Normal(0, \u03b2 \u22121 I) 2.\nFor each document d n , where n = 1, . . . , N :\n(a) Draw d n 's coordinate:\nx n \u223c Normal(0, \u03b3 \u22121 I) (b) For each word w nm \u2208 d n : i. Draw a topic: z \u223c Multi({P(z|x n , \u03a6)} Z z=1 ) ii. Draw a word: w nm \u223c Multi(\u03b8 z )\nHere, \u03b1 is a Dirichlet prior, I is an identity matrix, \u03b2 and \u03b3 control the variance of the Normal distributions. The parameters\n\u03c7 = {x n } N n=1 , \u03a6 = {\u03c6 z } Z z=1 , \u0398 = {\u03b8 z } Z z=1\n, collectively denoted as \u03a8 = \u03c7, \u03a6, \u0398 , are learned from documents D based on maximum a posteriori estimation. The log likelihood function is shown in Equation 2.\nL(\u03a8|D) = N n=1 Mn m=1 log Z z=1 P(z|xn, \u03a6)P(wnm|\u03b8z)(2)\nManifold Learning\nIn the above generative process, the document parameters are sampled independently, which may not necessarily reflect the underlying manifold. We therefore assume that when two documents d i and d j are close in the intrinsic geometry of the manifold \u2126, then their parameters \u03c8 i and \u03c8 j are similar as well. To realize this assumption, we need to address several issues, including the representation of the manifold, and the mechanism to incorporate the manifold. As a starting point, we consider the Laplacian Eigenmaps framework for manifold learning (Belkin and Niyogi 2003). It postulates that a low-dimensionality manifold relating N high-dimensional data points can be approximated by a k\u2212nearest neighbors graph. The manifold graph contains an edge connecting two data points d i and d j , with the weight\n\u03c9 ij = 1, if d i is in the set N k (d j ) of the k\u2212nearest neighbors of d j , or d j is in the set N k (d i ).\nOtherwise, \u03c9 ij = 0. By definition, edges are symmetric, i.e., \u03c9 ij = \u03c9 ji . The collection of edge weights are collectively denoted as \u2126 = {\u03c9 ij }. The edge weights are binary to isolate the effects of the manifold graph structure. More complex similarity-based weighting schemes are possible, and will be explored in the future.\n\u03c9ij = 1, if di \u2208 N k (dj) or dj \u2208 N k (di) 0, otherwise(3)\nOne effective means to incorporate a manifold structure into a learning model is through a regularization framework (Belkin, Niyogi, and Sindhwani 2006). This leads to a redesign of the log-likelihood function in Equation 2 into a new regularized function L (Equation 4), where \u03a8 consists of the parameters (visualization coordinates and topic distributions), and D and \u2126 are the documents and manifold.\nL(\u03a8|D, \u2126) = L(\u03a8|D) \u2212 \u03bb 2 \u2022 R(\u03a8|\u2126)(4)\nThe first component L is the log-likelihood function in Equation 2, which reflects the global consistency between the latent parameters \u03a8 and the observation D. The second component R is a regularization function, which reflects the local consistency between the latent parameters \u03a8 of neighboring documents in the manifold \u2126. \u03bb is the regularization parameter, commonly found in manifold learning algorithms (Belkin, Niyogi, and Sindhwani 2006;Cai et al. 2008;Cai, Wang, and He 2009), which controls the extent of regularization (we experiment with different \u03bb's in experiments).\nThis design effectively subsumes PLSV as a special case when \u03bb = 0, and enables us to directly showcase the effects of the manifold as the key differentiator in the model.\nWe now turn to the definition of the R function. The intuition is that the data points that are close in the highdimensional space, should also be close in their low-rank representations, i.e., local consistency. The justification is the embedding maps approximate the Eigenmaps of the Laplace Beltrami operator, which provides an optimal embedding for the manifold. One function that satisfies this is R + in Equation 5. Here, F is a distance function that operates on the low-rank space. Minimizing R + leads to minimizing the distance F(\u03c8 i , \u03c8 j ) between neighbors (\u03c9 ij = 1).\nR+(\u03a8|\u2126) = N i,j=1;i =j \u03c9ij \u2022 F (\u03c8i, \u03c8j)(5)\nThe above level of local consistency is still insufficient, because it does not regulate how non-neighbors (i.e., \u03c9 ij = 0) behave. For instance, it does not prevent non-neighbors from having similar low-rank representations. Another valid objective in visualization is to keep non-neighbors apart, which is satisfied by another objective function R \u2212 in Equation 6. R \u2212 is minimized when two non-neighbors d i and d j (i.e., \u03c9 ij = 0) are distant in their low-rank representations. The addition of 1 to F is to prevent division-by-zero error.", "publication_ref": ["b15", "b0", "b1", "b1", "b4", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "R\u2212(\u03a8|\u2126)", "text": "= N i,j=1;i =j 1 \u2212 \u03c9ij F(\u03c8i, \u03c8j) + 1 (6)\nWe hypothesize that neither objective is effective on its own. A more complete objective would capture the spirits of both keeping neighbors close, and keeping non-neighbors apart. Therefore, in this paper, we propose a single function that combines Equation 5 and Equation 6 in a natural way. A suitable combination, which we propose in this paper, is summation, as shown in Equation 7.\nR * (\u03a8|\u2126) = R+(\u03a8|\u2126) + R\u2212(\u03a8|\u2126)(7)\nSummation preserves the absolute magnitude of the distance, and helps to improve the visualization task by keeping non-neighbors separated on a visualizable Euclidean space. Taking the product is unsuitable, because it constraints the ratio of distances between neighbors to distances between non-neighbors. This may result in the crowding effect, where many documents are clustered together, because the relative ratio may be maintained, but the absolute distances on the visualization space could be too small.\nEnforcing Manifold: Visualization vs. Topic Space. We turn to the definition of F(\u03c8 1 , \u03c8 2 ). In classical manifold learning, there is one low-rank representative space. For semantic visualization, there are two: topic and visualization. We look into where and how to enforce the manifold.\nAt first glance, they seem equivalent. After all, they are representations of the same documents. However, this is not necessarily the case. Consider a simple example of two topics z 1 and z 2 with visualization coordinates \u03c6 1 = (0, 0) and \u03c6 2 = (2, 0) respectively. Meanwhile, there are three documents {d 1 , d 2 , d 3 } with coordinates x 1 = (1, 1), x 2 = (1, 1), and x 3 = (1, \u22121). If two documents have the same coordinates, they will also have the same topic distributions. In this example, x 1 and x 2 are both equidistant from \u03c6 1 and \u03c6 2 , and therefore according to Equation 1, they have the same topic distribution P(z 1 |d 1 ) = P(z 1 |d 2 ) = 0.5, and P(z 2 |d 1 ) = P(z 2 |d 2 ) = 0.5. If two documents have the same topic distributions, they may not necessarily have the same coordinates. d 3 also has the same topic distribution as d 1 and d 2 , but a different coordinate. In fact, any coordinate of the form (1, ?) will have the same topic distribution.\nThis example suggests that enforcing manifold on the topic space may not necessarily lead to having data points closer on the visualization space. We postulate that regularizing the visualization space is more effective. There are also advantages in computational efficiency to doing so, which we will describe further shortly. Therefore, we define F(\u03c8 i , \u03c8 j ) as the squared Euclidean distance ||x i \u2212 x j || 2 between the corresponding visualization coordinates.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Model Fitting", "text": "One well-accepted framework to learn model parameters using maximum a posteriori (MAP) estimation is the EM algorithm (Dempster, Laird, and Rubin 1977). For our model, the regularized conditional expectation of the complete-data log likelihood in MAP estimation with priors is: \u03a8 is the current estimate. P(z|n, m,\u03a8) is the class posterior probability of the n th document and the m th word in the current estimate. P(\u03b8 z ) is a symmetric Dirichlet prior with parameter \u03b1 for word probability \u03b8 z . P(x n ) and P(\u03c6 z ) are Gaussian priors with a zero mean and a spherical covariance for the document coordinates x n and topic coordinates \u03c6 z . We set the hyper-parameters to \u03b1 = 0.01, \u03b2 = 0.1N and \u03b3 = 0.1Z following (Iwata, Yamada, and Ueda 2008).\nQ(\u03a8|\u03a8) =\nIn the E-step, P(z|n, m,\u03a8) is updated as follows:\nP(z|n, m,\u03a8) = P(z|xn,\u03a6)P(wnm|\u03b8z) In the M-step, by maximizing Q(\u03a8|\u03a8) w.r.t \u03b8 zw , the next estimate of word probability \u03b8 zw is as follows:\n\u03b8zw = N n=1 Mn m=1 I(wnm = w)P(z|n, m,\u03a8) + \u03b1 W w =1 N n=1\nMn m=1 I(wnm = w )P(z|n, m,\u03a8) + \u03b1W I(.) is the indicator function. \u03c6 z and x n cannot be solved in a closed form, and are estimated by maximizing Q(\u03a8|\u03a8) using quasi-Newton (Liu and Nocedal 1989).\nWe compute the gradients of Q(\u03a8|\u03a8) w.r.t \u03c6 z and x n respectively as follows:\n\u2202Q \u2202\u03c6z = N n=1 Mn m=1 P(z|xn, \u03a6) \u2212 P(z|n, m,\u03a8) (\u03c6z \u2212 xn) \u2212 \u03b2\u03c6z \u2202Q \u2202xn = Mn m=1 Z z=1 P(z|xn, \u03a6) \u2212 P(z|n, m,\u03a8) (xn \u2212 \u03c6z) \u2212 \u03b3xn \u2212 \u03bb 2", "publication_ref": ["b9", "b15", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "\u2202R(\u03a8|\u2126) \u2202xn", "text": "The gradient of R(\u03a8|\u2126) w.r.t. x n is computed as follows:\n\u2202R(\u03a8|\u2126) \u2202xn = j=1;j =n 4\u03c9nj(xn \u2212 xj) \u2212 j=1;j =n 4(1 \u2212 \u03c9nj) (xn \u2212 xj) (F(\u03c8n, \u03c8j) + 1) 2\nAs mentioned earlier, there is an efficiency advantage to regularizing on the visualization space. R(\u03a8|\u2126) does not contain the variable \u03c6 z if we do regularization on visualization space. The complexity of computing all \u2202R(\u03a8|\u2126) \u2202xn is O(N 2 ). In contrast, if we do regularization on topic space, we have to take the gradient of R(\u03a8|\u2126) w.r.t to \u03c6 z . That contributes towards a greater complexity of O(Z 2 \u00d7 N 2 ) to compute all \u2202R(\u03a8|\u2126) \u2202\u03b8z . Therefore, regularization on topic space would run much slower than on visualization space.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments Experimental Setup", "text": "Datasets. We use three real-life, publicly available datasets 1 for evaluation. 20N ews contains newsgroup articles (in English) from 20 classes. Reuters8 contains newswire articles (in English) from 8 classes. Cade12 contains web pages (in Brazilian Portuguese) classified into 12 classes. These are benchmark datasets frequently used for document classification. While our task is fully unsupervised, the ground-truth class labels are useful for an objective evaluation.\nFollowing (Iwata, Yamada, and Ueda 2008), we create balanced classes by sampling fifty documents from each class. This results in, for one sample, 1000 documents for 20N ews, 400 for Reuters8, and 600 for Cade12. The vocabulary sizes are 5.4K for 20N ews, 1.9K for Reuters8, 7.6K for Cade12. As the algorithms are probabilistic, we generate five samples for each dataset, conduct five runs for each sample, and average the results across a total of 25 runs.\nMetric. For a suitable metric, we return to the fundamental principle that a good visualization should preserve the relationship between documents (in high-dimensional space) in the lower-dimensional visualization space. User studies, even when well-designed, could be overly subjective and may not be repeatable across different users reliably. Therefore, for a more objective evaluation, we rely on the ground-truth class labels found in the datasets. This is a well-established practice in many clustering and visualization works in machine learning. The basis for this evaluation is the reasonable assumption that documents of the same class are more related than documents of different classes, and therefore a good visualization would place documents of the same class as near neighbors on the visualization space.\nFor each document, we hide its true class, and predict its class by taking the majority class among its t-nearest neighbors as determined by Euclidean distance on the visualization space. Accuracy(t) is defined as the fraction of documents whose predicted class matches the truth. By default, we use t = 50, because there are 50 documents in each class. The same metric is used in (Iwata, Yamada, and Ueda 2008). While accuracy is computed based on documents' coordinates, the same trends will be produced if computed based on topic distributions (due to their coupling in Equation 1).\nComparative Methods. As semantic visualization seeks to ensure consistency between topic model and visualization, the comparison focuses on methods producing both topics and visualization coordinates, which are listed in Table 1. SEMAFORE is our proposed method that incorporates manifold learning into semantic visualization. PLSV is the state-of-the-art, representing the joint approach without manifold. LDA/MDS represents the pipeline approach, topic modeling with LDA (Blei, Ng, and Jordan 2003), followed by visualizing documents' topic distributions with MDS (Kruskal 1964). There are other pipeline methods, shown inferior to PLSV in (Iwata, Yamada, and Ueda 2008), which are not reproduced here to avoid duplication.", "publication_ref": ["b15", "b15", "b3", "b17", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Parameter Study", "text": "We study the effects of model parameters. Due to space constraint, we rely on 20N ews for this discussion (similar observations can be made for the other two datasets). When unvaried, the defaults are number of topics Z = 20, neighborhood size k = 10, and regularization R * with \u03bb = 1.\nRegularization. One consideration is the regularization component, both the function as well as the \u03bb. To investigate this, we compare our three proposed functions: neighbor only R + (Equation 5), non-neighbor only R \u2212 (Equation 6), and combined R * (Equation 7). For completeness, we include another function R DT M , proposed by (Huh and Fienberg 2012) for a different context (topic modeling alone).\nFigure 1(a) shows the accuracy at different settings of \u03bb \u2208 [0.1, 1000] (log scale). Among the three proposed functions, R * has the best accuracy at any \u03bb, which is as hypothesized given that it incorporates the manifold information from both neighbors and non-neighbors. R * is also significantly better than R DT M , which is not designed for semantic visualization. R + and R \u2212 are worse than R DT M , which also incorporates some information from non-neighbors. As\nVisualization Topic model Joint model Manifold SEMAFORE X X X X PLSV X X X LDA/MDS X X  Neighborhood Size. To construct the manifold graph \u2126 = {\u03c9 ij }, we represent each document as a tf-idf vector. We have experimented with different vector representations, including word counts and term frequencies, and found tf-idf to give the best results. The distance between two document vectors is measured using cosine distance. The k\u2212nearest neighbors to i is assigned \u03c9 ij = 1. The rest are assigned \u03c9 ij = 0. In Figure 1(b), we plot the accuracy for different k's, with R * and \u03bb = 1. As k increases, the accuracy at first increases, and then decreases. This is expected as neighbors that are too far away may no longer be related, and begin to introduce noise into the manifold. The optimum is k = 10.", "publication_ref": ["b13"], "figure_ref": ["fig_2", "fig_2"], "table_ref": []}, {"heading": "Comparison against Baselines", "text": "Accuracy. In Figure 2(a), we show the performance in accuracy(50) on 20N ews, while varying the number of topics Z. Figures 2(c) and 2(e) show the same for Reuters8 and Cade12 respectively. From these figures, we draw the following observations about the comparative methods. (#1) SEMAFORE performs the best on all datasets across various numbers of topics (Z). The margin of performance with respect to PLSV is statistically significant in all cases. SE-MAFORE beats PLSV by 20% to 42% on 20N ews, by 8-21% on Reuters8, and by 22-32% on Cade12. This effectively showcases the utility of manifold learning in enhancing the quality of visualization. (#2) PLSV performs better than LDA/MDS, which shows that there is utility to having a joint, instead of separate, modeling of topics and visualization. (#3) In Figures 2(b), 2(d), and 2(f), we show the accuracy(t) at different t's for Z = 20 for the three datasets. The accuracy(t) values are stable. At any t, the comparison shows outperformance by SEMAFORE over the baselines. (#4) The above accuracy results are based on visualization coordinates. We have also computed accuracies based on topic distributions, which have similar trends.\nHeretofore, we will focus on the comparison between SE-MAFORE and the closest competitor PLSV.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Figure 2: Accuracy Comparison", "text": "Visualization. To provide an intuitive appreciation, we briefly describe a qualitative comparison of visualizations. Figure 4 shows a visualization of 20N ews dataset as a scatterplot (best seen in color). Each document has a coordinate, and is assigned a shape and color based on its class. Each topic also has a coordinate, drawn as a black, hollow circle. SEMAFORE's Figure 4(a) shows that the different classes are well separated. There are distinct blue cluster and purple cluster on the right for hockey and baseball classes respectively, orange and pink clusters at the top for cryptography and medicine, etc. Beyond individual classes, the visualization also places related classes 2 nearby. Computerrelated classes are found on the lower left. Politics and religion classes are on the lower right. Figure 4(b) by PLSV is significantly worse. There is a lot of crowding at the center. For instance, motorcycle (green) and autos (red) are mixed at the center without a good separation.\nFigure 5 shows the visualization outputs for Reuters8 dataset. SEMAFORE in Figure 5(a) is better at separating the eight classes into distinct clusters. In an anti-clockwise direction from the top, we have green triangles (acq), red squares (crude), purple crosses (ship), blue asterisks (grain), red dashes (interest), navy blue diamonds (money-fx), orange circles (trade), and finally the light blue earn on the  shows that several classes are intermixed at the center, including red dashes (interest), orange circles (trade), and navy blue diamonds (money-fx). Figure 6 shows the visualization outputs for Cade12. This is the most challenging dataset. Even so, SEMAFORE in Figure 6(a) still achieves a better separation between the classes, as compared to PLSV in Figure 6(b).\nPerplexity. One question is whether SEMAFORE's gain in visualization quality over the closest baseline PLSV is at the expense of the topic model. To investigate this, we compare the perplexity of SEMAFORE and PLSV, which share a core generative process. Perplexity is a well-accepted metric that measures the generalization ability of a topic model on a held-out test set. For each dataset, we draw a sixth sample as test set, excluding documents that already exist in the first five samples. Perplexity is measured as exp{\u2212 Figure 3 shows the perplexity as the number of topics Z varies. Perplexity values for both SEMAFORE and PLSV are close. In most cases (13 out of 15 cases), t-tests at 1% significance level indicate that the differences are not significant, except for a couple of data points (in 1 case SEMAFORE is better, in 1 case PLSV is better). This result is not unexpected, as both are optimized for log-likelihood. SEMAFORE further ensures that the document parameters (coordinates and topic distributions) that optimize the log-likelihood also better reflect the manifold. Our emphasis is on enhancing visualization, and indeed SEMAFORE's gain in visualization quality has not hurt the generalizability of its topic model.", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Conclusion", "text": "We address the semantic visualization problem, which jointly conducts topic modeling and visualization of documents. We propose a new framework to incorporate manifold learning within a probabilistic semantic visualization model called SEMAFORE. Experiments on real-life datasets show that SEMAFORE significantly outperforms the baselines in terms of visualization quality, providing evidence that manifold learning, together with joint modeling of topics and visualization, is important for semantic visualization. ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Laplacian eigenmaps for dimensionality reduction and data representation", "journal": "Neural Computation", "year": "2003", "authors": "M Belkin; P Niyogi"}, {"ref_id": "b1", "title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "journal": "JMLR", "year": "2006", "authors": "M Belkin; P Niyogi; V Sindhwani"}, {"ref_id": "b2", "title": "GTM: The generative topographic mapping", "journal": "Neural Computation", "year": "1998", "authors": "C M Bishop; M Svens\u00e9n; C K Williams"}, {"ref_id": "b3", "title": "Latent dirichlet allocation", "journal": "JMLR", "year": "2003", "authors": "D M Blei; A Y Ng; M I Jordan"}, {"ref_id": "b4", "title": "Modeling hidden topics on document manifold", "journal": "", "year": "2008", "authors": "D Cai; Q Mei; J Han; C Zhai"}, {"ref_id": "b5", "title": "Probabilistic dyadic data analysis with local and global consistency", "journal": "", "year": "2009", "authors": "D Cai; X Wang; X He"}, {"ref_id": "b6", "title": "Visualizing topic models", "journal": "", "year": "2012", "authors": "A J Chaney; .-B Blei; D M "}, {"ref_id": "b7", "title": "A taxonomy of visualization techniques using the data state reference model", "journal": "", "year": "2000", "authors": "E H Chi"}, {"ref_id": "b8", "title": "Termite: visualization techniques for assessing textual topic models", "journal": "", "year": "2012", "authors": "J Chuang; C D Manning; J Heer"}, {"ref_id": "b9", "title": "Maximum likelihood from incomplete data via the EM algorithm", "journal": "Journal of the Royal Statistical Society, Series B", "year": "1977", "authors": "A P Dempster; N M Laird; D B Rubin"}, {"ref_id": "b10", "title": "TopicNets: Visual analysis of large text corpora with topic modeling", "journal": "TIST", "year": "2012", "authors": "B Gretarsson; J O'donovan; S Bostandjiev; T H\u00f6llerer; A Asuncion; D Newman; P Smyth"}, {"ref_id": "b11", "title": "Stochastic neighbor embedding", "journal": "", "year": "2002", "authors": "G E Hinton; S T Roweis"}, {"ref_id": "b12", "title": "Probabilistic latent semantic indexing", "journal": "", "year": "1999", "authors": "T Hofmann"}, {"ref_id": "b13", "title": "Discriminative topic modeling based on manifold learning", "journal": "TKDD", "year": "2012", "authors": "S Huh; S E Fienberg"}, {"ref_id": "b14", "title": "Parametric embedding for class visualization", "journal": "Neural Computation", "year": "2007", "authors": "T Iwata; K Saito; N Ueda; S Stromsten; T L Griffiths; J B Tenenbaum"}, {"ref_id": "b15", "title": "Probabilistic latent semantic visualization: topic model for visualizing documents", "journal": "", "year": "2008", "authors": "T Iwata; T Yamada; N Ueda"}, {"ref_id": "b16", "title": "The self-organizing map", "journal": "Proceedings of the IEEE", "year": "1990", "authors": "T Kohonen"}, {"ref_id": "b17", "title": "Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis", "journal": "Psychometrika", "year": "1964", "authors": "J B Kruskal"}, {"ref_id": "b18", "title": "Statistical analysis of semi-supervised regression", "journal": "", "year": "2007", "authors": "J D Lafferty; L Wasserman"}, {"ref_id": "b19", "title": "On the limited memory BFGS method for large scale optimization", "journal": "Mathematical Programming", "year": "1989", "authors": "D C Liu; J Nocedal"}, {"ref_id": "b20", "title": "Document clustering and visualization with latent dirichlet allocation and self-organizing maps", "journal": "", "year": "2009", "authors": "J R Millar; G L Peterson; M J Mendenhall"}, {"ref_id": "b21", "title": "Nonlinear dimensionality reduction by locally linear embedding", "journal": "Science", "year": "2000", "authors": "S T Roweis; L K Saul"}, {"ref_id": "b22", "title": "A global geometric framework for nonlinear dimensionality reduction", "journal": "Science", "year": "2000", "authors": "J B Tenenbaum; V De Silva; J C Langford"}, {"ref_id": "b23", "title": "Visualizing data using t-SNE", "journal": "JMLR", "year": "2008", "authors": "L Van Der Maaten; G Hinton"}, {"ref_id": "b24", "title": "Tiara: a visual exploratory text analytic system", "journal": "", "year": "2010", "authors": "F Wei; S Liu; Y Song; S Pan; M X Zhou; W Qian; L Shi; L Tan; Q Zhang"}, {"ref_id": "b25", "title": "Locally discriminative topic modeling", "journal": "Pattern Recognition", "year": "2012", "authors": "H Wu; J Bu; C Chen; J Zhu; L Zhang; H Liu; C Wang; D Cai"}, {"ref_id": "b26", "title": "Learning with local and global consistency", "journal": "NIPS", "year": "2004", "authors": "D Zhou; O Bousquet; T N Lal; J Weston; B Sch\u00f6lkopf"}, {"ref_id": "b27", "title": "Semisupervised learning using gaussian fields and harmonic functions", "journal": "", "year": "2003", "authors": "X Zhu; Z Ghahramani; J Lafferty"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Zz=1 P(z |xn,\u03a6)P(wnm|\u03b8 z )", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 1 :1Figure 1: SEMAFORE: Vary Parameters", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "2 http://qwone.com/\u223cjason/20Newsgroups/", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 3 :3Figure 3: Perplexity Comparison", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "M is the number of documents in the test set, N d is the number of words in a document, and p(w d ) is the likelihood of a test document by a topic model. Lower perplexity is better.", "figure_data": ""}, {"figure_label": "45", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 4 :Figure 5 :45Figure 4: Visualization of 20N ews for Z = 20", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Comparative Methods", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "P(z|dn) = P(z|xn, \u03a6) = exp( 1 2 ||xn \u2212 \u03c6z|| 2 ) Z z =1 exp( 1 2 ||xn \u2212 \u03c6 z || 2 ) (1)", "formula_coordinates": [2.0, 332.27, 569.23, 225.73, 26.13]}, {"formula_id": "formula_1", "formula_text": "\u03b8 z \u223c Dirichlet(\u03b1) (b) Draw z's coordinate: \u03c6 z \u223c Normal(0, \u03b2 \u22121 I) 2.", "formula_coordinates": [3.0, 51.51, 85.89, 209.48, 36.08]}, {"formula_id": "formula_2", "formula_text": "x n \u223c Normal(0, \u03b3 \u22121 I) (b) For each word w nm \u2208 d n : i. Draw a topic: z \u223c Multi({P(z|x n , \u03a6)} Z z=1 ) ii. Draw a word: w nm \u223c Multi(\u03b8 z )", "formula_coordinates": [3.0, 57.33, 125.99, 203.66, 48.12]}, {"formula_id": "formula_3", "formula_text": "\u03c7 = {x n } N n=1 , \u03a6 = {\u03c6 z } Z z=1 , \u0398 = {\u03b8 z } Z z=1", "formula_coordinates": [3.0, 91.1, 199.16, 180.13, 12.2]}, {"formula_id": "formula_4", "formula_text": "L(\u03a8|D) = N n=1 Mn m=1 log Z z=1 P(z|xn, \u03a6)P(wnm|\u03b8z)(2)", "formula_coordinates": [3.0, 79.01, 258.41, 213.49, 27.01]}, {"formula_id": "formula_5", "formula_text": "\u03c9 ij = 1, if d i is in the set N k (d j ) of the k\u2212nearest neighbors of d j , or d j is in the set N k (d i ).", "formula_coordinates": [3.0, 54.0, 462.57, 238.5, 20.61]}, {"formula_id": "formula_6", "formula_text": "\u03c9ij = 1, if di \u2208 N k (dj) or dj \u2208 N k (di) 0, otherwise(3)", "formula_coordinates": [3.0, 91.58, 550.82, 200.92, 20.02]}, {"formula_id": "formula_7", "formula_text": "L(\u03a8|D, \u2126) = L(\u03a8|D) \u2212 \u03bb 2 \u2022 R(\u03a8|\u2126)(4)", "formula_coordinates": [3.0, 102.57, 662.41, 189.93, 19.74]}, {"formula_id": "formula_8", "formula_text": "R+(\u03a8|\u2126) = N i,j=1;i =j \u03c9ij \u2022 F (\u03c8i, \u03c8j)(5)", "formula_coordinates": [3.0, 369.03, 299.31, 188.97, 27.03]}, {"formula_id": "formula_9", "formula_text": "= N i,j=1;i =j 1 \u2212 \u03c9ij F(\u03c8i, \u03c8j) + 1 (6)", "formula_coordinates": [3.0, 408.37, 444.02, 149.63, 27.03]}, {"formula_id": "formula_10", "formula_text": "R * (\u03a8|\u2126) = R+(\u03a8|\u2126) + R\u2212(\u03a8|\u2126)(7)", "formula_coordinates": [3.0, 371.57, 568.66, 186.43, 8.06]}, {"formula_id": "formula_11", "formula_text": "Q(\u03a8|\u03a8) =", "formula_coordinates": [4.0, 62.89, 455.93, 35.74, 6.12]}, {"formula_id": "formula_12", "formula_text": "\u03b8zw = N n=1 Mn m=1 I(wnm = w)P(z|n, m,\u03a8) + \u03b1 W w =1 N n=1", "formula_coordinates": [4.0, 319.5, 72.42, 216.42, 26.13]}, {"formula_id": "formula_13", "formula_text": "\u2202Q \u2202\u03c6z = N n=1 Mn m=1 P(z|xn, \u03a6) \u2212 P(z|n, m,\u03a8) (\u03c6z \u2212 xn) \u2212 \u03b2\u03c6z \u2202Q \u2202xn = Mn m=1 Z z=1 P(z|xn, \u03a6) \u2212 P(z|n, m,\u03a8) (xn \u2212 \u03c6z) \u2212 \u03b3xn \u2212 \u03bb 2", "formula_coordinates": [4.0, 333.33, 183.51, 211.36, 69.3]}, {"formula_id": "formula_14", "formula_text": "\u2202R(\u03a8|\u2126) \u2202xn = j=1;j =n 4\u03c9nj(xn \u2212 xj) \u2212 j=1;j =n 4(1 \u2212 \u03c9nj) (xn \u2212 xj) (F(\u03c8n, \u03c8j) + 1) 2", "formula_coordinates": [4.0, 339.94, 294.66, 192.89, 52.92]}], "doi": ""}