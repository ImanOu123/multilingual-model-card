{"title": "Oops I Took A Gradient: Scalable Sampling for Discrete Distributions", "authors": "Will Grathwohl; Kevin Swersky; Milad Hashemi; David Duvenaud; Chris J Maddison", "pub_date": "", "abstract": "We propose a general and scalable approximate sampling strategy for probabilistic models with discrete variables. Our approach uses gradients of the likelihood function with respect to its discrete inputs to propose updates in a Metropolis-Hastings sampler. We show empirically that this approach outperforms generic samplers in a number of difficult settings including Ising models, Potts models, restricted Boltzmann machines, and factorial hidden Markov models. We also demonstrate the use of our improved sampler for training deep energy-based models (EBM) on high dimensional discrete data. This approach outperforms variational auto-encoders and existing energy-based models. Finally, we give bounds showing that our approach is near-optimal in the class of samplers which propose local updates.", "sections": [{"heading": "Introduction", "text": "Discrete structure is everywhere in the real world, from text to genome sequences. The scientific community is building increasingly complex models for this discrete data, increasing the need for methods to sample from discrete distributions. Sampling from a discrete distribution may seem like a simpler task than sampling from a continuous one: even a one-dimensional continuous distribution can have an uncountable infinity of outcomes, whereas a discrete distribution is at most countable. However, most common continuous distributions have some kind of simplifying structure, such as differentiable densities, which can be exploited to speed up sampling and inference.\nOf course, many discrete distributions have structure as well. Notably, discrete distributions over combinatorial spaces often have some kind of block independence structure among their variables. Although this can be used to speed up sampling and inference, it may be difficult to de-1 University of Toronto and Vector Institute 2 Google Research, Brain Team. Correspondence to: Will Grathwohl <wgrathwohl@cs.toronto.edu>.  Our approach visualized. Often discrete distributions are defined by continuous functions whose input is restricted to a discrete set; here R 2 restricted to Z 2 . We use a Taylor series computed on the underlying continuous function to estimate likelihood ratios of making discrete moves; here \u00b11 in either direction. These estimated likelihood ratios are used to inform a proposal distribution over moves in the original discrete space. tect such structure automatically. Typically, users need to know this structure a priori and must hard-code it into an algorithm to speed up sampling.\nIn search for a unifying structure, we notice that many discrete distributions are written (and implemented) as differentiable functions of real-valued inputs. The discrete structure is created by restricting the continuous inputs to a discrete subset of their domain. In this paper, we use the gradients of these underlying continuous functions to inform proposal distributions for MCMC sampling in large discrete probabilistic models. This new family of gradient-informed proposals for MCMC may be seen as a class of adaptive Gibbs samplers or a fast approximation to locally-informed proposals (Umrigar, 1993;Liu, 1996;Zanella, 2020).\nAs we show, this gradient information is cheaply available for many discrete distributions and can lead to orders of magnitude improvements in sampling efficiency. In some cases, it even outperforms samplers that exploit hard-coded independence structure.", "publication_ref": ["b43", "b25", "b44"], "figure_ref": [], "table_ref": []}, {"heading": "arXiv:2102.04509v2 [cs.LG] 6 Jun 2021", "text": "We apply these sampling techniques to improve parameter inference in discrete energy-based models such as Ising and Potts models. We also find that our method is particularly well-suited to sample from unnormalized discrete distributions parameterized by deep neural networks. The improved efficiency of our sampler allows us to apply many techniques from large-scale continuous EBMs to successfully train deep EBMs for discrete data. These models are simple to define, flexible, and outperform baseline variational auto-encoders (Kingma & Welling, 2013) and existing energy-based models.", "publication_ref": ["b21"], "figure_ref": [], "table_ref": []}, {"heading": "Background", "text": "In this work we are concerned with sampling from unnormalized distributions over discrete data log p(x) = f (x) \u2212 log Z where f (x) is the unnormalized log-probability of x and Z = x e f (x) is the normalizing constant which we assume is unknown. We restrict our study to D-dimensional binary x \u2208 {0, 1} D and categorical x \u2208 {0, 1, . . . , K} D data as all finite-dimensional discrete distributions can be embedded in this way. Throughout the paper we assume all categorical variables are one-hot encoded.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Gibbs Sampling", "text": "Gibbs sampling is perhaps the simplest and most generic method for sampling from discrete distributions. At each step, the sampler partitions the dimensions of an input x into two groups x u and x \u2212u where u is a subset of the D dimensions of x and \u2212u is its complement. The next sample in the chain is created by updating x u to be a sample from p(x u |x \u2212u ), the conditional distribution of the chosen dimensions, given all others. In some distributions, blockindependence structure exists, making certain partitions easy to sample from and update in parallel.\nIn the worst case, if such structure does not exist, we can let x u = x i , simply the i-th dimension of x. In this setting p(x i |x \u2212i ) is simply a one-dimensional categorical distribution over K possible outcomes, which requires K evaluations of the unnormalized log-density function. We typically fix an ordering of the dimensions and iterate through this ordering to ensure that each dimension is updated.\nWhile simple, updating one dimension at a time can be problematic for high-dimensional data. Consider for example a binary model of the MNIST dataset. Almost all dimensions will represent the background and if chosen for a Gibbs update, they will almost certainly not change. Similarly, the pixels on the interior of a digit will also not change. This amounts to wasted computation every time we propose a dimension, which does not change. Ideally, if we could bias our partition choice to dimensions which are likely to change, we could build a more efficient Gibbs sampler.\nConsider a similar sampler for the binary case; Metropolis-Hastings with the proposal distribution q(x |x) = i q(x |x, i)q(i) where q(i) is a distribution over indices i \u2208 {1, . . . , D} and q(x |x, i) = \u03b4(x = x \u2212i ), where \u03b4(\u2022) is the Dirac-delta distribution. With this sampler we will first sample an index i \u223c q(i), then flip the i-th bit of x to obtain x and accept this proposed update with probability\nmin exp(f (x ) \u2212 f (x)) q(x|x ) q(x |x) , 1 .(1)\nThis approach can lead to considerable performance improvements if q(i) is biased towards dimensions which are more likely to flip. To this end, considerable work has been done to use prior sampling data to adaptively update the q(i) while maintaining the validity of MCMC sampling (\u0141atuszy\u0144ski et al., 2013;Richardson et al., 2010).\nRevisiting our MNIST example, we can see the pixels most likely to change are those at the edge of a digit. Where this edge is varies considerably and depends on the entire input x. Thus, we can imagine an input dependent proposal q(i|x) should be able to outperform an unconditional proposal q(i).\nOur proposed approach does exactly that, using gradient information to inform a proposal over dimensions which leads to more efficient sampling.", "publication_ref": ["b23", "b35"], "figure_ref": [], "table_ref": []}, {"heading": "Locally-Informed Proposals", "text": "A good Metropolis-Hastings proposal needs to balance the increase in the likelihood of the proposed point x with the decrease in the probability of the reverse proposal q(x|x ).\nA natural strategy for increasing the likelihood is to use locally-informed proposals:\nq \u03c4 (x |x) \u221d e 1 \u03c4 (f (x )\u2212f (x)) 1(x \u2208 H(x)).(2)\nwhere H(x) is the Hamming ball of some size around x and \u03c4 > 0 is a temperature parameter. In this case, the proposal is simply a tempered Softmax over f (x ) \u2212 f (x) for all x \u2208 H(x). If the temperature \u03c4 is too low, this proposal will aggressively optimize the local likelihood increase from x \u2192 x , but possibly collapse the reverse proposal probability q \u03c4 (x|x ). If the temperature \u03c4 is too high, this proposal may increase the reverse proposal probability q \u03c4 (x|x ), but ignore the local likelihood increase.\nThe temperature that balances both of these terms is \u03c4 = 2 (Umrigar, 1993;Zanella, 2020). We include a derivation of this fact in Appendix A. In fact, Zanella (2020) showed that q 2 (x |x) is in the optimal subclass of locally-informed proposals. Zanella (2020) also demonstrated that this can lead to large improvements in empirical performance per sam-pling step compared to other generic samplers like Gibbs and the Hamming-Ball sampler (Titsias & Yau, 2017).\nUnfortunately, while powerful, these locally-informed proposals requires us to compute f (x ) \u2212 f (x) for every x \u2208 H(x). For D-dimensional data and a Hamming window size of 1, this requires O(D) evaluations of f which can become prohibitive as D grows. Our proposed approach reduces this to O(1) evaluations while incurring a minimal decrease in the efficiency of each sampling step.", "publication_ref": ["b43", "b44", "b44", "b44", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "Searching for Structure", "text": "For some distributions, structure exists which enables the local differences f (x ) \u2212 f (x) to be computed efficiently. This is not true in general, but even in settings where it is, bespoke derivations and implementations are required. Ideally, we could identify a structure that is ubiquitous across many interesting discrete distributions, can be exploited in a generic and generalizable way, and can allow us to accurately estimate the local differences.\nTo find such structure, we examine the functional form of the unnormalized log-probability for some common, diverse families of discrete distributions in Table 1. The formulas in Table 1 are not only the standard way these distributions are written down, but they are also the standard way these distributions are implemented in probabilisitic programming frameworks.\nDistribution log p(x) + log Z Categorical x T \u03b8 Poisson 1 x log \u03bb \u2212 log \u0393(x + 1) HMM T t=1 x T t+1 Ax t \u2212 (w T x\u2212y) 2 2\u03c3 2 RBM i softplus(W x + b) i + c T x Ising x T W x + b T x Potts L i=1 h T i x i + L i,j=1 x T i J ij x j Deep EBM f \u03b8 (x)\nThe key insight here is that these are all continuous, differentiable functions accepting real-valued inputs, even though they are evaluated only on a discrete subset of their domain. We propose to exploit this structure and that gradients, in the form of Taylor-series approximations, can be used to efficiently estimate likelihood ratios between a given input x and other discrete states x .\nWhen we are dealing with D-dimensional binary data, we can estimate the likelihood ratios of flipping each bit with\nd(x) = \u2212(2x \u2212 1) \u2207 x f (x) (3) whered(x) i \u2248 f (x \u2212i ) \u2212 f (x)\nand x \u2212i is x with the i-th bit flipped. If we are dealing with D-dimensional categorical data we can estimate a similar quantit\u1ef9\nd(x) ij = \u2207 x f (x) ij \u2212 x T i \u2207 x f (x) i(4)\nwhered(x) ij approximates the log-likelihood ratio of flipping the i-th dimension of x from its current value to the value j. Similar first-order approximations can easily be derived for larger window sizes as well with linear operators applied to the gradient of the log-probability function.\nWe note, there may be many different underlying continuous functions which correspond to a given discrete distribution.\nWe later present theory (Theorem 1) which can guide this choice in settings where there is any ambiguity.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0", "tab_0"]}, {"heading": "Gibbs With Gradients", "text": "We now present our main algorithm. We use a Taylor-series (Equations 3, 4) to approximate the likelihood ratios within a local window of a point x. We use these estimated likelihood ratios to produce an approximation\nq \u2207 (x |x) \u221d ed (x) 2 1(x \u2208 H(x))(5)\nto q 2 (x |x) of Equation 2, which we use in the standard Metropolis-Hastings algorithm.\nOur experiments focus on a simple and fast instance of this approach which only considers local moves inside of a Hamming window of size 1. For binary data, these window-1 proposals have an even simpler form since all x \u2208 H(x) differ by only dimension. Proposing a move from x to x is equivalent to choosing which dimension i to change. We can sample this from a categorical distribution over D choices:\nq(i|x) = Categorical Softmax d (x) 2 (6)\nThus when x binary, to sample from q \u2207 (x |x), we simply sample which dimension to change i \u223c q(i|x), and then deterministically set x = flipdim(x, i). In this case, when x and x differ only in dimension i, we have q \u2207 (x |x) = q(i|x) and q \u2207 (x|x ) = q(i|x ).\nBecause of the relationship to Adaptive Gibbs, we call our sampler Gibbs-With-Gradients (GWG). Pseudo-code describing our sampler can be found in Algorithm 1. In the Algorithm 1 Gibbs With Gradients Input: unnormalized log-prob f (\u2022), current sample x Computed(x) {Eq. 3 if binary, Eq. 4 if categorical.} Compute q(i|x) = Categorical Softmax\nd (x) 2 Sample i \u223c q(i|x) x = flipdim(x, i) Compute q(i|x ) = Categorical Softmax d (x ) 2\nAccept with probability:\nmin exp(f (x ) \u2212 f (x)) q(i|x ) q(i|x) , 1\ncategorical data setting, the proposal must choose not only which dimension to change, but also to what value. Thus, q(i|x) in this setting is a D(K \u2212 1)-way Softmax.\nWe describe some simple extensions in Appendix D and code to replicate our experiments is available here.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Analyzing Approximations", "text": "Zanella (2020) proved that \"locally-balanced\" proposals, like q 2 (x |x) in Equation 2, are the optimal locally-informed proposals for Metropolis-Hastings. In this section we show that, under smoothness assumptions on f , our methods are within a constant factor of q 2 (x |x) in terms of asymptotic efficiency.\nTo understand the asymptotic efficiency of MCMC transition kernels, we can study the asymptotic variance and spectral gap of the kernel. The asymptotic variance is defined as\nvar p (h, Q) = lim T \u2192\u221e 1 T var T t=1 h(x t )(7)\nwhere h : X \u2192 R is a scalar-valued function, Q is a pstationary Markov transition kernel, and X 1 \u223c p(x). The spectral gap is defined as\nGap(Q) = 1 \u2212 \u03bb 2 (8)\nwhere \u03bb 2 is the second largest eigenvalue of the transition probability matrix of Q. Both of these quantities measure the asymptotic efficiency of Q. The asymptotic variances measures the additional variance incurred when using sequential samples from Q to estimate E p [h(x)]. For transition probability matrices with non-negative eigenvalues, the spectral gap is related to the mixing time, with larger values corresponding to faster mixing chains (Levin & Peres, 2017).\nSince our method approximates q 2 (x |x), we should expect some decrease in efficiency. We characterize this decrease in terms of the asymptotic variance and spectral gap, under the assumption of Lipschitz continuity of \u2207 x f (x). In particular, we show that the decrease is a constant factor that depends on the Lipschitz constant of \u2207 x f (x) and the window size of our proposal.\nTheorem 1 Let Q(x , x) and Q \u2207 (x , x) be the Markov transition kernels given by the Metropolis-Hastings algorithm using the locally balanced proposal q 2 (x |x) and our approximation q \u2207 (x |x). Let f be an L-smooth logprobability function and p(x) = exp(f (x))", "publication_ref": ["b24"], "figure_ref": [], "table_ref": []}, {"heading": "Z", "text": ". Then it holds\n(a) var p (h, Q \u2207 ) \u2264 varp(h,Q) c + 1\u2212c c \u2022 var p (h) (b) Gap(Q \u2207 ) \u2265 c \u2022 Gap (Q)\nwhere\nc = e \u2212 1 2 LD 2 H and D H = sup x \u2208H(x) ||x \u2212 x ||.\nA proof can be found in Appendix B. This roughly states that Q \u2207 (x , x) is no less than c-times as efficient than Q(x , x) per step for estimating expectations. As expected, our approach matches the efficiency of the target proposal when the Taylor-series approximation is accurate. We can derive from Theorem 1 that when considering which functional representation of our distribution to choose, we should choose the representation whose gradient has the smallest Lipschitz constant.\nAn example: Consider an Ising model on a cyclic 2D lattice. This model has log-probability function f (x) = \u03b8 \u2022 x T Jx \u2212 log Z where J is the binary adjacency matrix of the lattice, \u03b8 is the connectivity strength and Z is the unknown normalizing constant. We can see the gradient is \u2207 x f (x) = 2\u03b8 \u2022 Jx and can bound L \u2264 2\u03c3(J)\u03b8 = 8\u03b8.\nFor \u03b8 = .1 and a Hamming window of size 1, this gives c = .67, regardless of D. Since a single evaluation of f has an O(D 2 ) cost, it costs O(D 3 ) to compute 2 q 2 (x |x).\nCompared to the exact local-differences proposal, the Gibbs-With-Gradients proposal q \u2207 (x |x) incurs at most a constant loss in sampling efficiency per-iteration but gives a O(D) increase in speed.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Relationship to Continuous Relaxations", "text": "Why hasn't this relatively simple approach been proposed before? A number of prior works have used gradient information for discrete sampling. Instead of using gradients to inform discrete updates directly, these methods transport the problem into a continuous relaxation, perform updates there, and transform back after sampling. This approach incurs many of the pitfalls of continuous sampling without providing the scalability. We find these methods are not competitive with Gibbs-With-Gradients in high dimensions.\nIn more detail, these methods use the discrete target distribution to create a related continuous distribution (relaxation) whose samples can be transformed to samples from the target distribution. They then apply gradient-based sampling methods such as Stein Variational Gradient Descent (Liu & Wang, 2016) or Hamiltonian Monte-Carlo (HMC) (Neal et al., 2011) to the new distribution. Examples of such methods are the recently proposed Discrete-SVGD (D-SVGD) (Han et al., 2020) and Discontinuous HMC (Nishimura et al., 2017).\nA key challenge of these approaches is that the relaxed distribution can be arbitrarily difficult to sample from, highly multi-modal and require small step-sizes. Further, metrics of MCMC performance and mixing in the relaxed space may not indicate performance and mixing in the discrete space. These methods also require the tuning of many additional hyper-parameters such as step-size, momentum, and the temperature of the relaxation. In contrast, our approach operates directly in discrete space, and has no hyper-parameters.\nFigure 2 compares these approaches on the task of sampling from restricted Boltzmann machines (RBMs) of up to 1000 dimensions. We compare to D-SVGD and two relaxation-based baselines derived from the Metropolis-Adjusted Langevin Algorithm (Besag, 1994) and HMC.\nWe compare the log-MMD between generated samples and \"ground-truth\" samples generated with Block-Gibbs. We also display samples from an MNIST-trained model. In contrast to all three baselines, our approach does not degrade with dimension. Additional results, details, and discussion can be found in Appendix C. These relaxation-based approaches do not scale beyond 200 dimensions, so we do not compare to them in our main experimental results section", "publication_ref": ["b26", "b31", "b12", "b33", "b1"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Sampling From EBMs", "text": "To demonstrate the benefits and generality of our proposed approach to sampling, we present results sampling from 3 distinct and challenging distributions; Restricted Boltzmann Machines, Lattice Ising models, and Factorial Hidden Markov Models. Each is evaluated differently based on the properties of the distribution. We compare our sampler, Gibbs-With-Gradients, against standard Gibbs sampling and the Hamming Ball Sampler (Titsias & Yau, 2017) two generic approaches for discrete sampling. When available, we also compare with samplers which exploit known structure in the distribution of interest.\nIn the following, Gibbs-X refers to Gibbs sampling with a block-size of X, and HB-X-Y refers to the Hamming Ball sampler with a block size of X and a hamming ball size of Y , and GWG refers to Gibbs-With-Gradients. Gibbs-1 is the fastest sampler tested. In our current implementation, we find Gibbs-2, HB-10-1, and GWG have approximately 1.6, 6.6, 2.1 times the cost of Gibbs-1 per step, respectively. Thus the run-time of GWG is most comparable to Gibbs-2.\nRestricted Boltzmann Machines are unnormalized latent-variable models defined as:\nlog p(x) = log(1 + exp(W x + c)) + b T x \u2212 log Z (9)\nwhere {W, b, c} define its parameters and x \u2208 {0, 1} D . We train an RBM with 500 hidden units on the MNIST dataset using contrastive divergence (Hinton, 2002). We generate samples with various MCMC samplers and compare them in two ways. First, using the Maximum Mean Discrepancy (MMD) (Gretton et al., 2012) between a set of samples from each sampler and a set of \"ground-truth\" samples generated using the structured Block-Gibbs sampler available to RBMs (see Appendix E for details). Next, we report the Effective Sample Size (ESS) of a summary statistic over sampling trajectories. Results can be seen in Figure 3.\nWe see on the left that GWG matches the structured Block-Gibbs sampler in MMD (\"Target\" in the Figure ), while the other samplers do not. On the right we see that the effective sample size of GWG is notably above the baselines and is approximately halfway between the baselines and the Block-Gibbs sampler (in log-space). We note, Block-Gibbs can update all 784 dimensions in each iterations. GWG and Gibbs-1 can update 1 and Gibbs-2 and Hamming Ball can update 2 dimensions per iteration. Lattice Ising Models are models for binary data defined by\nlog p(x) = \u03b8 \u2022 x T Jx \u2212 log Z (10\n)\nwhere \u03b8 is the connectivity strength and J is the binary adjacency matrix, here restricted to be a 2D cyclic lattice. This model was originally proposed to model the spin magnetic particles (Ising, 1924). We sample from models with increasing dimension and connectivity strength. We evaluate using Effective Sample Size (ESS) 3 of a summary statistic (full details in Appendix F). Results can be seen in Figure 4. We see Gibbs-With-Gradients provides a notable increase in ESS for Ising models with higher connectivity. These models are harder to sample from as their dimensions are more correlated.\nFactorial Hidden Markov Models (FHMM) are latentvariable time-series models, similar to HMMs but their hid-3 Computed using Tensorflow Probability den state consists of distinct, independent factors. The continuous data y \u2208 R L of length L is generated by the binary hidden state x \u2208 {0, 1} L\u00d7K with K factors as p(x, y) = p(y|x)p(x)\np(y|x) = L t=1 N (y t ; W x t + b, \u03c3 2 ) p(x) = p(x 1 ) L t=2 p(x t |x t\u22121 )(11)\nWe create a random FHMM with 1000 time-steps and a 10-dimensional hidden state and then draw samples y. We generate posterior samples p(x|y) and evaluate our samplers using reconstruction error and joint likelihood. Full model description and experimental details can be found in Appendix G and results can be seen in Figure 5. In this setting, the Hamming-Ball sampler exploits known structure in the problem. Each block chosen by the sampler consists of the 10-dimensional latent state x t , as opposed to 10 random dimensions. Thus, the Hamming-Ball sampler in this setting is a stronger baseline. Despite this, we find Gibbs-With-Gradients notably outperforms the baseline samplers.", "publication_ref": ["b41", "b16", "b9", "b19"], "figure_ref": ["fig_3", "fig_4", "fig_5"], "table_ref": []}, {"heading": "Training EBMs", "text": "Training EBMs is a challenging task. Computing the likelihood for Maximum Likelihood inference requires computation of the normalizing constant Z = x e f \u03b8 (x) which is typically intractable. Thankfully, the gradient of the loglikelihood can be more easily expressed as:\n\u2207 \u03b8 log p(x) = \u2207 \u03b8 f \u03b8 (x) \u2212 E p(x) [\u2207 \u03b8 f \u03b8 (x)](12)\ntherefore, if samples can be drawn from p(x), then an unbiased gradient estimator can be derived. We can approximate this estimator using MCMC. When a slow-mixing MCMC sampler is used to draw these samples, we obtain biased gradient estimates and this leads to sub-par learning. Improvements in MCMC can then lead to improvements in parameter inference for unnormalized models. We explore how Gibbs-With-Gradients can be applied to parameter inference for some classical discrete EBMs.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Training Ising models on generated data", "text": "We generate Ising models with different sparse graph structures; a 2D cyclic lattice and a random Erdos-Renyi graph.\nWe generate training data with a long-run Gibbs chain and train models using Persistent Contrastive Divergence (Tieleman, 2008) with an 1 penalty to encourage sparsity. We evaluate our models using the Root-Mean-Squared-Error (RMSE) between the inferred connectivity matrix\u0134 and the true matrix J.\nFull experimental details and additional results can be found in Appendix F.2, F.3 and results can be seen in Figure 6. In all settings, GWG greatly outperforms Gibbs sampling. This allows for much faster training than standard Gibbs while recovering higher-quality solutions. GWG leads to better solutions with lower computational cost.", "publication_ref": [], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "Protein Coupling Prediction with Potts Models", "text": "Proteins are defined by a discrete sequence of 20 amino acids x \u2208 {1, . . . , 20} D where D is the length of the protein. The Potts model has long been a popular approach for modelling the evolutionary distribution of protein sequences (Lapedes et al., 1999). The model takes the form\nlog p(x) = D i=1 h T i x i + D i,j=1 x T i J ij x j \u2212 log Z (13)\nwhere x i is a one-hot encoding of the i-th amino acid in x, J \u2208 R {D\u00d7D\u00d720\u00d720} and h \u2208 R {D\u00d720} are the model's parameters and Z is the model's normalizing constant. The Potts model's likelihood is the sum of pairwise interactions. Marks et al. (2011) demonstrated that the strength of these interactions can correspond to whether or not two amino acids touch when the protein folds. These inferred contacts can then be used to infer the 3D structure of the protein.\nSince the Potts model is unnormalized, maximum likelihood learning is difficult, and 1-regularized Pseudo-likelihood Maximization (PLM) (Besag, 1975) is used to train the model. Recently Ingraham & Marks (2017) found that improved contact prediction could be achieved with MCMCbased maximum likelihood learning. Unfortunately, due to the limitations of discrete MCMC samplers, their study was restricted to small proteins (less than 50 amino acids).\nGWG allows these performance improvements to scale to large proteins as well. We train Potts models on 2 large proteins: OPSD BOVIN, and CADH1 HUMAN. We train using PCD where samples are generated with GWG and Gibbs. We run PLM as a baseline. These proteins are much larger than those studied in Ingraham & Marks (2017) with OPSD BOVIN, and CADH1 HUMAN having 276, and 526 amino acids, respectively 4 .\nWe predict couplings using the J parameter of our models.\nWe compute a \"coupling-strength\" for each pair of aminoacids as ||J ij || 2 which gives a measure of how much indices i and j interact to determine the fitness of a protein. We sort index pairs by their coupling strength and compare the highest scoring pairs with known contacts in the proteins.\nFull experimental details and additional results can be found in Appendix H and results can be seen in Figure 7. For the smaller protein, Gibbs sampling outperforms PLM but for the larger protein, the slow-mixing of the sampler causes the performance to drop below that of PLM. Despite the increased size, Gibbs-With-Gradients performs the best.  ", "publication_ref": ["b22", "b29", "b0", "b18", "b18"], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "Deep EBMs for Discrete Data", "text": "Deep Energy-Based Models have rapidly gained popularity for generative modeling. These models take the form\nlog p(x) = f \u03b8 (x) \u2212 log Z (14)\nwhere f \u03b8 : R D \u2192 R is a deep neural network. The recent success of these models can be attributed to a few advancements including; the use of tempered Langevin samplers (Nijkamp et al., 2020) and large persistent chains (Du & Mordatch, 2019). This has enabled EBMs to become a competitive approach for image-generation (Song & Ermon, 2019), adversarial robustness (Grathwohl et al., 2019;Hill et al., 2020), semi-supervised learning (Song & Ou, 2018;Grathwohl et al., 2020a) and many other problems. PCD training is very sensitive to the choice of MCMC sampler. As an initial experiment, we attempted to train these models using standard Gibbs but found that the sampler was too slow to enable stable training within a reasonable compute budget. On the binary data we needed to train with 800 Gibbs sampling steps per training iteration. All models we trained with fewer steps quickly diverged. GWG required only 40. This made training with Gibbs 9.52x slower than GWG. For a fair comparison, the Gibbs results in Table 2 were trained for an equal amount of wall-clock time as the GWG models.\nFor the categorical data, we could not train models with Gibbs sampling. Each step of Gibbs requires us to evaluate the energy function 256 (for each possible pixel value) times. GWG requires 2 function evaluations. Thus the amount of compute per iteration for Gibbs is 128x greater than GWG. Further, to make Gibbs train stably, we would need to use many more steps, as with the binary data. This would give roughly a 870x increase in run-time. Therefore, training a model of this form with Gibbs is simply not feasible.\nFull experimental details can be found in Appendix I. We present long-run samples from our trained models in Figure 8 and test-set likelihoods in Table 2. Likelihoods are estimated using Annealed Importance Sampling (Neal, 2001). We compare the performance of our models to Variational Autoencoders (Kingma & Welling, 2013) and two other EBMs; an RBM and a Deep Belief Network (DBN) (Hinton, 2009). On most datasets, our Resnet EBM outperforms the other two EBMs and the VAEs. Our improved sampler enables deep EBMs to become a competitive approach to generative modeling on high-dimensional discrete data.\nWe include some preliminary results using Gibbs-With-Gradients to train EBMs for text data in Appendix J. Samples generated with annealed Markov chain using 300,000 Gibbs-With-Gradients steps. Top to bottom: MNIST, omniglot, Caltech Silhouettes, Frey Faces, Histopathology.", "publication_ref": ["b32", "b4", "b36", "b6", "b15", "b37", "b7", "b30", "b21"], "figure_ref": ["fig_8"], "table_ref": ["tab_1", "tab_1"]}, {"heading": "Future Directions and Conclusion", "text": "In this work we have presented Gibbs-With-Gradients, a new approach to MCMC sampling for discrete distributions.\nOur approach exploits a powerful structure, gradient information, which is available to a very large class of important discrete distributions. We then use this gradient information to construct proposal distributions for Metropolis-Hastings.\nWe have demonstrated on a diverse set of distributions that this approach to sampling considerably outperforms baseline samplers which do not exploit known structure in the target distribution as well as many that do. Further, we find our approach outperforms prior discrete samplers which use gradient information with continuous relaxations.\nWe find Gibbs-With-Gradients performs very well at sampling from deep energy-based models and allows, for the first time, unconstrained deep EBMs to be trained on discrete data and outperform other deep generative models.\nWe believe there is considerable room for future work building on top of our method. We only explored samplers which modify 1 variable per proposed update. We believe considerable improvements could be made if the window size of the sampler was expanded but this would require more efficient algorithms to sample from the larger proposal.\nNext, we have shown that gradient-based approximations to the local difference function can be accurate and useful for complex discrete distributions. Local difference functions have been used in the past to generalize Score Matching (Lyu, 2012), and Stein Discrepancies (Han & Liu, 2018). We believe there is great potential to explore how gradient-based approximations could enable the generalization of recent deep EBM training methods based on Score Matching and Stein Discrepancies (Song & Ermon, 2019;Grathwohl et al., 2020b) to models of discrete data.", "publication_ref": ["b27", "b11", "b36", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We would like to thank Eli Weinstein for helping us properly present our protein model results and we would like to thank Kelly Brock for help and feedback for working with the protein data. We thank Jesse Bettencourt, James Lucas, and Jacob Kelly for helpful feedback on our draft. Last, we would like to thank Jeffrey Rosenthal for providing information on related methods and prior work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Balancing Locally-Informed Proposals", "text": "As discussed in Section 2.2, locally informed proposals need to balance the likelihood increase with the reverse proposal probability. In particular, consider proposals of the form:\nq \u03c4 (x |x) \u221d e 1 \u03c4 (f (x )\u2212f (x)) 1(x \u2208 H(x)). (15\n)\nwhere H(x) is the Hamming ball of some size around x and \u03c4 > 0 is a temperature parameter. Here we provide a derivation of the fact that \u03c4 = 2 balances these two terms.\nWhen we examine the acceptance rate (Equation 1) of this proposal we find\nexp(f (x ) \u2212 f (x)) q \u03c4 (x|x ) q \u03c4 (x |x) = exp(f (x ) \u2212 f (x)) exp( 1 \u03c4 (f (x) \u2212 f (x ))Z(x) exp( 1 \u03c4 (f (x ) \u2212 f (x))Z(x ) = exp 1 \u2212 2 \u03c4 (f (x ) \u2212 f (x)) Z(x) Z(x )(16)\nwhere\nZ(x) = x \u2208H(x) exp( 1 \u03c4 (f (x ) \u2212 f (x)\n) is the normalizing constant of the proposal. By setting \u03c4 = 2, the most variable terms cancel and we are left with Z(x) Z(x ) . Thus, the acceptance rate is equal to the ratio of the normalizing constants of the proposal distributions. If the Hamming ball is small and the function f is well behaved (i.e Lipschitz) then, since x is near x, Z(x ) will be near Z(x) and the acceptance rate will be high.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B. Proof of Theorem 1", "text": "Our proof follows from Theorem 2 of Zanella (2020) which states that for two p-reversible Markov transition kernels Q 1 (x , x) and Q 2 (x , x), if there exists c > 0 for all\nx = x such that Q 1 (x , x) > c \u2022 Q 2 (x , x) then (a) var p (h, Q 1 ) \u2264 varp(h,Q1) c + 1\u2212c c \u2022 var p (h) (b) Gap(Q 1 ) \u2265 c \u2022 Gap(Q 2 )\nwhere var p (h, Q) is the asymptotic variance defined in Equation 7, Gap(Q) is the spectral gap defined in Equation 8, and var p (h) is the standard variance\nE p [h(x) 2 ] \u2212 E p [h(x)] 2 .\nOur proof proceeds by showing we can bound x , x), and the results of the theorem then follow directly from Theorem 2 of Zanella (2020).\nQ \u2207 (x , x) \u2265 c \u2022 Q(", "publication_ref": ["b44"], "figure_ref": [], "table_ref": []}, {"heading": "B.1. Definitions", "text": "We begin by writing down the proposal distribution of interest and their corresponding Markov transition kernels. For ease of notion we define some values\n\u2206(x , x) := f (x ) \u2212 f (x) \u2207(x , x) := \u2207 x f (x) T (x \u2212 x) D H := sup x \u2208H(x) ||x \u2212 x|| We restate the target proposal for x \u2208 H(x) q 2 (x |x) = exp \u2206(x ,x) 2 Z(x)\nwhere we have defined\nZ(x) = x \u2208H(x) exp \u2206(x , x) 2 .\nThis proposal leads to the Markov transition kernel\nQ(x , x) = q 2 (x |x) min 1, Z(x) Z(x ) = min \uf8f1 \uf8f2 \uf8f3 exp \u2206(x ,x) 2 Z(x) , exp \u2206(x ,x) 2 Z(x ) \uf8fc \uf8fd \uf8fe .\nWe now restate our approximate proposal for x \u2208 H(x)\nq \u2207 (x |x) = exp \u2207(x ,x) 2 Z (x)\nwhere we have defined\nZ(x) = x \u2208H(x) exp \u2207(x , x) 2\nwhich leads to the Markov transition kernel\nQ \u2207 (x , x) = q \u2207 (x |x) min \uf8f1 \uf8f2 \uf8f3 1, exp (\u2206(x , x)) exp \u2207(x ,x)\u2212\u2207(x,x ) 2 Z (x) Z(x ) \uf8fc \uf8fd \uf8fe = min \uf8f1 \uf8f2 \uf8f3 exp \u2207(x ,x) 2 Z (x) , exp \u2206(x , x) + \u2207(x,x ) 2 Z (x ) \uf8fc \uf8fd \uf8fe .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.2. Preliminaries", "text": "It can be seen that \u2207(x , x) is a first order Taylor-series approximation to \u2206(x , x) and it follows directly from the Lipschitz continuity of\n\u2207 x f (x) that |\u2207(x , x) \u2212 \u2206(x , x)| \u2264 L 2 ||x \u2212 x || 2 (17)\nand since we restrict x \u2208 H(x) we have\n\u2212 L 2 D 2 H \u2264 \u2207(x , x) \u2212 \u2206(x , x) \u2264 L 2 D 2 H (18)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.3. Normalizing Constant Bounds", "text": "We derive upper-and lower-bounds onZ(x) in terms of Z(x).\nZ(x) = x \u2208H(x) exp \u2207(x , x) 2 = x \u2208H(x) exp \u2206(x , x) 2 exp \u2207(x , x) \u2212 \u2206(x , x) 2 \u2264 x \u2208H(x) exp \u2206(x , x) 2 exp LD 2 H 4 \u2264 exp LD 2 H 4 x \u2208H(x) exp \u2206(x , x) 2 = exp LD 2 H 4 Z(x)(19)\nFollowing the same argument we can show\nZ(x) \u2265 exp \u2212LD 2 H 4 Z(x).(20)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.4. Inequalities of Minimums", "text": "We show\nQ \u2207 (x , x) \u2265 c \u2022 Q(x , x) for c = exp \u2212LD 2 H 2 . Since both Q(x , x) = min{a, b} and Q \u2207 (x , x) = min{a \u2207 , b \u2207 } it is sufficient to show a \u2207 \u2265 c \u2022 a and b \u2207 \u2265 c \u2022 b to prove the desired result.\nWe begin with the a terms\na \u2207 a = exp \u2207(x ,x) 2 Z (x) Z(x) exp \u2206(x ,x) 2 = Z(x) Z(x) exp \u2207(x , x) 2 \u2212 \u2206(x , x) 2 \u2265 exp \u2212LD 2 H 4 exp \u2207(x , x) 2 \u2212 \u2206(x , x) 2 \u2265 exp \u2212LD 2 H 4 exp \u2212LD 2 H 4 = exp \u2212LD 2 H 2 (21) Now the b terms b \u2207 b = exp \u2206(x , x) + \u2207(x,x ) 2 Z (x ) Z(x ) exp \u2206(x ,x) 2 = Z(x ) Z(x ) exp \u2206(x , x) + \u2207(x,x ) 2 exp \u2206(x ,x) 2 = Z(x ) Z(x ) exp \u2206(x , x) 2 + \u2207(x, x ) 2 \u2265 exp \u2212LD 2 H 4 exp \u2206(x , x) 2 + \u2207(x, x ) 2 = exp \u2212LD 2 H 4 exp \u2207(x, x ) 2 \u2212 \u2206(x, x ) 2 \u2265 exp \u2212LD 2 H 4 exp \u2212LD 2 H 4 = exp \u2212LD 2 H 2(22)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.5. Conclusions", "text": "We have shown that a \u2207 \u2265 exp\n\u2212LD 2 H 2 a and b \u2207 \u2265 exp \u2212LD 2 H 2\nb and therefore it holds that\nQ \u2207 (x , x) \u2265 exp \u2212LD 2 H 2 Q(x , x)(23)\nFrom this, the main result follows directly from Theorem 2 of Zanella (2020).\nC. Relationship to Relaxations Han et al. (2020) show that sampling from any discrete distribution can be transformed into sampling from a continuous distribution with a piece-wise density function. For simplicity we focus on a distribution p(x) over binary data x \u2208 {0, 1} D . To do this we will create a D-dimensional continuous distribution p c (z) where z \u2208 R D . We must specify a base distribution p 0 (z) which we choose to be N (0, I).\nWe must then specify a function \u0393(z) : R D \u2192 {0, 1} D which maps regions of equal mass under the base distribution to values in {0, 1} D . A natural choice is \u0393(z) = sign(z)\nWe then define p c (z) as p c (z) = N (z; 0, I)p(\u0393(z))\nand it can be easily verified that generating\nz \u223c p c (z), x = \u0393(z)\nwill produce a sample from p(x).\nThus, we have transformed a discrete sampling task into a task of sampling from a continuous distribution with a piece-wise density. Han et al. (2020) further relax this by defining\np \u03bb c (x) = N (z; 0, I)p(\u0393 \u03bb (z))\nwhere \u0393 \u03bb (x) is a continuous approximation to \u0393(x). A natural choice for sign(x) is a tempered sigmoid function\n\u0393 \u03bb (x) = 1 1 + e \u2212x/\u03bb\nwith temperature \u03bb which controls the smoothness of the relaxation. This is similar to the Concrete relaxation (Maddison et al., 2016) for binary variables. D-SVGD proposes to use the gradients of log p \u03bb c (x) to produce updates for their continuous samples which are adjusted using an importance-weighted correction as proposed in Han & Liu (2018).\nWe can apply this same approach to other sampling methods such as MALA and HMC.", "publication_ref": ["b12", "b12", "b28", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "C.1. Relaxed MCMC", "text": "Gradient-based MCMC samplers such as MALA or HMC consist of a proposal distribution, q(x |x), and a metropolis accept/reject step. As a baseline of comparison, we present two straight-forward applications of the above relaxation to sampling from discrete distributions. In both settings we will use the continuous, differentiable surrogate p \u03bb c (x) to generate a proposal and we will perform our Metropolis step using the piece-wise target p c (x).\nRelaxed MALA (R-MALA): Given a sample x, we sample a proposed update x from:\nq(x |x) = N x ; x + 2 \u2207 x log p \u03bb c (x), 2(24)\nand we accept this proposal with probability\nmin p c (x )q(x|x ) p c (x)q(x |x) , 1 .(25)\nR-MALA has two parameters; the stepsize and the temperature of the relaxation \u03bb. We search over \u2208 {.1, .01, .001} and \u03bb \u2208 {2., 1., .5}\nRelaxed HMC (R-HMC) works similarly to R-MALA. Given a sample x we sample an initial momentum vector v \u223c N (0, M ) where M is the mass matrix. We perform k steps of leapfrog integration on the relaxed Hamiltonian\nH \u03bb (x, v) = \u2212 log p \u03bb c (x) + 1 2 v T M v\nwith step-size .\nThe proposal x , v is accepted with probability\nmin H(x, v) H(x , v ) , 1(26)\nwhere H is the target Hamiltonian\nH(x, v) = \u2212 log p c (x) + 1 2 v T M v.\nWe fix the mass matrix M = I and the number of steps k = 5. This leaves two parameters, the ste-psize and temperature \u03bb. As with R-MALA we search over \u2208 {.1, .01, .001} and \u03bb \u2208 {.5, 1, 2.}.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.2. Experimental Details", "text": "We compare D-SVGD, R-MALA, and R-HMC to Gibbs-With-Gradients at the task of sampling from RBMs. We present results in two settings; random RBMs with increasing dimension, and an RBM trained on MNIST using Contrastive Divergence.\nThe random RBMs have visible dimension [25,50,100,250,500,1000] and all have 100 hidden units. The MNIST RBM has 784 visible units and 500 hidden units and is trained as in Appendix E.1. Following Han et al. (2020) the random RBMs are initialized as\nW \u223c N (0, .05I), b, c \u223c N (0, I).\nAll samples are initialized to a random uniform Bernoulli distribution and all samplers are run for 2000 steps. We evaluate by computing the Kernelized MMD between each sampler's set of samples and a set of approximate \"groundtruth\" samples generated with the RBMs efficient block-Gibbs sampler. We generate 500 ground truth samples and 100 samples for each sampler tested. In Figure 2 we plot the final log-MMD with standard-error over 5 runs with different random seeds. Samples on the right of the figure are generated in the same way from the MNIST RBM.\nFor D-SVGD we search over relaxation temperatures \u03bb \u2208 {.5, 1., 2.}. We optimize the samples with the Adam optimizer (Kingma & Ba, 2014). We search over learning rates in {.01, .001, .0001}. We use an RBF kernel k(x, x ) = exp \u2212||x\u2212x || 2 h and h = med 2 /(2 log(n + 1)) where med is the median pairwise distance between the current set of n samples.\nAll reported results for D-SVGD, R-MALA, and R-HMC are the best results obtained over all tested hyper-parameters. We found all of these methods to be very sensitive to their hyper-parameters -in particular, the relaxation temperature \u03bb. We believe it may be possible to improve the performance of these methods through further tuning of these parameters but we found doing so beyond the scope of this comparison.", "publication_ref": ["b12", "b20"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "D. Gibbs-With-Gradients Extensions", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.1. Extensions To Larger Windows", "text": "We can easily extend our approach to proposals with larger window sizes. This would amount to a a Taylor-series approximation to likelihood ratios where more than 1 dimension of the data has been perturbed. These would come in the form of linear functions of f (x) and \u2207 x f (x). It is likely, of course, that as the window-size is increased, the accuracy of our approximation will decrease as will the quality of the sampler.\nIn all of our experiments, we found a window-size of 1 to give a considerable improvement over various baselines so we did not explore further. We believe this is an exciting avenue for future work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.2. Multi-Sample Variant", "text": "As mentioned, all experiments presented in the main paper use a window size of 1 meaning only 1 dimension can be changed at a time. In the binary case, we sample a dimension i \u223c q(i|x) which tells us which dimension to flip to generate our proposed update. A simple, and effective extension to this is to simply re-sample multiple indices from this same distribution\ni 1 , . . . , i N \u223c q(i|x)\nwhere N is the number of draws. We then generate x by flipping the bit at each sampled index i n . This changes the acceptance probability to\nmin exp(f (x ) \u2212 f (x)) N n=1 q(i n |x ) N n=1 q(i n |x) , 1 .(27)\nThis proposal makes a larger number of approximations and assumptions but we find that in some settings it can provide faster convergence and can have reasonable acceptance rates. We demonstrate this in our RBM experiments in Figure 9. We replicate Figure 3 but add the multi-sample variant described above with N = 3 and N = 5 samples. We find in this case the multi-sample variant has faster convergence and greater ESS.", "publication_ref": [], "figure_ref": ["fig_9", "fig_3"], "table_ref": []}, {"heading": "E. Restricted Boltzmann Machines", "text": "Restricted Boltzmann Machines define a distribution over binary data x and latent variables h. The model is defined as:\nlog p(x, h) = h T W x + b T x + c T h \u2212 log Z (28\n)\nwhere Z is the normalizing constant and {W, b, c} are the model's parameters. In this model we can efficiently marginalize out the latent variable h to obtain:\nlog p(x) = log h p(x, h) = log h exp(h T W x + b T x + c T h) = log(1 + exp(W x + c)) + b T x \u2212 log Z (29)\nWhile the joint and marginal are both unnormalized, we can see the conditional distirbutions can be easily normalized and take the form:\np(x|h) = Bernoulli(W x + c) p(h|x) = Bernoulli(W T h + b).\nWe can exploit this structure to more efficiently sample from RBMs. We can perform Block-Gibbs updates by starting at some initial x, and repeatedly sample h \u223c p(h|x), x \u223c p(x|h). Exploiting this structure leads to much more efficient sampling than standard Gibbs and other samplers (see Figure 3).", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "E.1. Experimental Details", "text": "We explore the performance of various approaches to sample from an RBM trained on the MNIST dataset. The RBM has 500 hidden units (and 784 visible units). We train the model using contrastive divergence (Hinton, 2002) for 1 epoch through the dataset using a batch size of 100. We use 10 steps of MCMC sampling using the Block-Gibbs sampler defined above to generate samples for each training iteration. We use the Adam (Kingma & Ba, 2014) optimizer with a learning rate of .001.\nOur first result compares samples generated by various approaches with samples generated with the Block-Gibbs sampler described above. We generate a set of 500 samples using the Block-Gibbs sampler run for 10,000 steps.", "publication_ref": ["b16", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "F.3. Additional Results: Training Ising Models", "text": "In Figure 11, we present an expanded version of Figure 6 which presents additional results. In these additional experiments we find Gibbs-With-Gradients considerably outperforms training with Gibbs sampling. ", "publication_ref": [], "figure_ref": ["fig_1", "fig_6"], "table_ref": []}, {"heading": "G. Factorial Hidden Markov Models", "text": "Factorial Hidden Markov Models (FHMM) are a generalization of Hidden Markov Models and model real-valued time-series data. The observed data y \u2208 R L is generated conditioned on a discrete latent-variable x \u2208 {0, 1} L\u00d7K . This latent-variable is drawn from the product of K independent Markov processes as seen below. The data y t is generated by by the K-dimensional state vector x t with Gaussian noise added.\np(x, y) = p(y|x)p(x)\np(y|x) = L t=1 N (y t ; W x t + b, \u03c3 2 ) p(x) = p(x 1 ) L t=2 p(x t |x t\u22121 ) p(x 1 ) = K k=1 Bernoulli(x 1k ; \u03b1 k ) p(x t+1 |x t ) = K k=1 Bernoulli(x (t+1)k ; \u03b2 x tk k (1 \u2212 \u03b2 k ) 1\u2212x tk )(32)\nThe posterior p(x|y) has no closed form and thus we must rely on MCMC techniques to sample from it.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G.1. Experimental Details", "text": "We sample the parameters of an FHMM randomly as\nW, b \u223c N (0, I)(33)\nand set \u03c3 2 = .5, \u03b1 k = .1 and \u03b2 k = .95 for for all k.\nWe then sample x \u223c p(x) and y \u223c p(y|x) and run all samplers for 10,000 steps to generate samples from p(x|y).\nThe Hamming Ball Sampler (Titsias & Yau, 2017) is special for this model as it exploits the known block-structure of the posterior. We use a block size of 10 and the blocks are chosen to be all 10 dims of the latent state at a randomly chosen time x t . Thus, this sampler is aware of more hardcoded structure in the model than the Gibbs baseline and Gibbs-With-Gradients.", "publication_ref": ["b41"], "figure_ref": [], "table_ref": []}, {"heading": "H. Potts Models for Proteins", "text": "We train the MCMC models using PCD (Tieleman, 2008) with a buffer size of 2560. At each training iteration we sample a mini batch of 256 examples and 256 random samples from the persistent sample buffer. These are updated using 50 steps of either Gibbs or Gibbs-With-Gradients and the gradient estimator of Equation 12 is used to update the model parameters. We train for 10,000 iterations using the Adam optimizer (Kingma & Ba, 2014). Following Marks et al. (2011) we use block-1 regularization. This regularizer takes the form\nL 1 (J) = ij ||J ij || 2 . (34\n)\nWe add this regularizer to the maximum likelihood gradient estimator. We tested regularization strength parameters in {.1, .03, .01} and found .01 to work best for PLM, Gibbs, and Gibbs-With-Gradients.\nGround truth couplings were extracted from an experimentally validated distance-map. As is standard, we consider any pair of amino acids to be a contact if they are within 5 angstroms of each other.", "publication_ref": ["b39", "b20", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "H.1. Recall on PF10018", "text": "We do not present results on PF10018 in the main body as it was used to tune hyper-parameters. For completeness, we present them here in Figure 12. As with the other protiens, the MCMC-based training outperforms PLM but by a smaller margin and GWG and Gibbs perform comparably here. This further supports the benefit of MCMC training over PLM sets in on larger data as does the benefit of GWG over Gibbs. ", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "H.2. Visualized Contacts", "text": "We visualize the inferred couplings for CADH1 HUMAN in Figure 13. We see that GWG most accurately matches the known structure with Gibbs inferring spurious couplings and PLM missing many couplings near the diagonal.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "I. Deep EBMs", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "I.1. Architectures", "text": "We train on two types of data; binary and categorical. For the binary datasets, the data is represented as {0, 1} D where D is the dimension of the data.\nFor the categorical datasets, each categorical variable is represented as a \"one-hot\" vector. Thus, for image data, each pixel is represented with a 256-dimensional vector. To deal with the very high dimensionality of this input parameterization, we first map each one-hot vector to a learned, low-dimensional embedding with a linear transformation. We map to D p = 4 dimensions for all models tested. We then feed this (D \u00d7 D p )-dimensional input to our network. There are certainly more efficient ways to represent this data, but our intention was not to achieve state-of-the-art results on these datasets and instead to demonstrate our sampler could enable the training of energy-based models on high-dimensional discrete data, so we use the most straightforward parameterization.\nThe ResNet used for our EBM is identical for all datasets. The network has 8 residual blocks with 64 feature maps. Each residual block has 2 convolutional layers. The first two residual blocks have a stride of 2. The output features are mean-pooled across the spatial dimensions and a single linear layer is used on top to provide the energy. The Swish (Ramachandran et al., 2017) nonlinearity (x \u2022 \u03c3(x)) is used throughout.", "publication_ref": ["b34"], "figure_ref": [], "table_ref": []}, {"heading": "I.2. Experimental Details", "text": "We trained all models Adam (Kingma & Ba, 2014) using a learning rate of 0.0001. We linearly warm-up the learning rate for the first 10,000 iterations. We found this was necessary to help burn in the replay buffer of samples.\nFor the large datasets (static/dynamic MNIST, Omniglot) we use a replay buffer with 10,000 samples. For the smaller datasets (Caltech, Freyfaces, Histopathology) the buffer size is 1000. Unlike recent work on continuous EBMs (Du & Mordatch, 2019;Grathwohl et al., 2019), we do not reinitialize the buffer samples to noise. We found this resulted in unstable training and lower likelihoods.\nWe train all models for 50,000 iterations. We use the same training/validation/testing splits as Tomczak & Welling (2018). We evaluate models every 5000 iterations using 10,000 steps of AIS. We select the model which performs the best on the validation data under this procedure. Final results in Table 2 are generated from the selected models by running 300,000 iterations of AIS. We evaluate using a model who's weights are given by an exponential moving average of the training model's weights. This is analogous to training with \"fast-weights\" as in Tieleman & Hinton (2009). We find this greatly improves likelihood performance and sample quality. We use an exponential moving average with weight 0.999 and did not experiment with other values.\nWe believe better results could be obtained with larger models or alternative architectures, but we leave this for future work.", "publication_ref": ["b20", "b4", "b6", "b42", "b40"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "I.2.1. PARTITION FUNCTION ESTIMATION WITH AIS", "text": "We estimate likelihoods by estimating the partition function using Annealed Importance Sampling (AIS) (Neal, 2001). AIS underestimates the log-partition-function leading to over-estimating the likelihood. The estimation error can be reduced by using a larger number of intermediate distributions or a more efficient MCMC sampler. Results presented in Table 2 were generated with 300,000 intermediate distributions. We chose this number as it appears to be sufficiently high for our partition function estimate to converge. Despite this, these are upper-bounds and therefore should not be considered definitive proof that one model achieves higher likelihoods than another.\nWe anneal between our model's unnormalized logprobability f (x) and a multivariate Bernoulli or Categorical distribution, log p n (x), fit to the training data, for binary and categorical data, respectively.\nf t (x) = \u03b2 t f (x) + (1 \u2212 \u03b2 t ) log p n (x)(35)\nwhere \u03b2 t is linearly annealed from 0 to 1. Alternative strategies such as sigmoid annealing could be used, but we leave this for future work.\nIn Figure 14 we plot the estimated likleihoods for our Caltech Silhouettes models as the number of intermediate distributions increases. It can be seen that between 30,000 and 300,000 (\u2248 10 4.5 \u2192 10 5.5 ) the values appear to be converged, thus we feel our reported number faithfully represent our models' performance.", "publication_ref": ["b30"], "figure_ref": ["fig_1"], "table_ref": ["tab_1"]}, {"heading": "I.3. Additional Results", "text": "We present additional long-run samples from our convolutional EBM. These samples were generated using an annealed Markov Chain (as described above) and Gibbs-With-Gradients as the base MCMC sampler. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "At this length, the samples are very close to true samples from the model. Next we generate a set of 100 samples from a number of other samplers: Gibbs, Hamming Ball and Gibbs-With-Gradients. After every MCMC transition we compute the Kernelized Maximum Mean Discrepancy (Gretton et al., 2012) between the current set of samples and our \"ground-truth\" long-run Block-Gibbs samples. We use an exponential average Hamming kernel K(x, x ) = exp \u2212\nto compute the MMD.\nThe next result is the effective sample size of a test statistic for each sampler. Following Zanella (2020), our test statistic is the Hamming distance between the current sample and a random input configuration. We present a box-plot showing the median, standard-deviation, and outliers over 32 chains.", "publication_ref": ["b9", "b44"], "figure_ref": [], "table_ref": []}, {"heading": "E.2. Additional Results", "text": "We visualize the samples after 10,000 steps of each tested sampler in Figure 10. We can see the Gibbs-With-Gradients samples much more closely matches the Block-Gibbs samples. This result is reflected in the improved MMD scores see in Figure 3 (left). ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F. Ising Models", "text": "Ising models are unnormalized models for binary data defined as\nwhere J and b are the model's parameters and Z is the normalizing constant. J determines which other variables each x i is correlated with. If J = 0 then the model becomes a factorized Bernoulli distribution. If all of the non-zero indices of J are the same, then we can pull out this value and rewrite the model as\nwhere now \u03b8 controls how correlated each x i is with its connected variables and J controls which variables each x i is connected to. Our lattice Ising models take this form where the J is the adjacency matrix of a cyclic 2D lattice and \u03b8 controls the strength of the connectivity.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F.1. Experimental Details: Sampling", "text": "We experiment with our sampler's ability to sample from Ising models on the 2D cyclic lattice as various factors chage. These include the connectivity strength and the size of the lattice. We run each sampler for 100,000 steps and evaluate using the ESS of a test statistic. Following Zanella (2020) our test statistic is the Hamming distance between the current sample and a random input configuration. We present the ESS (in log-scale), averaged with standard-errors, over 32 random seeds.\nWe can see in both 10x10 and 40x40 lattice sizes, our sampler outperforms Gibbs and the Hamming ball.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F.2. Experimental Details: Training", "text": "We create Ising models with 2 kinds of graph structure; a cyclic 2D lattice and a random Erdos-Renyi (ER) graph.\nFor the lattice we create models with a 10x10, 25x25, and 40x40 lattice leading to 100, 625, and 1600 dimensional distributions. We train models with connectivity \u03b8 \u2208 [\u2212.1, 0.0, .1, .25, .5].\nFor the ER graph, we create a model with 200 nodes. The ER edge probability is chosen so each node has an average of 4 neighbors. The strength of each edge is IID sampled from N 0, 1 4 . A dataset of 2000 examples is generated from each model using 1,000,000 steps of Gibbs sampling. We train models using persistent contrastive divergence (Tieleman, 2008) with a buffer size of 256 examples. Models are trained with the Adam optimizer (Kingma & Ba, 2014) using a learning rate of .001 and a batch size of 256. We update the persistent samples using Gibbs and Gibbs-With-Gradients. We train models with {5, 10, 25, 50, 100} steps of MCMC per training iteration and compare their results. We train all models with an 1 penalty to encourage sparsity with strength .01.\nWe compare results using the root-mean-squared-error between the true connectivity matrix J and the inferred connectivity matrix\u0134. ", "publication_ref": ["b39", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "J. Preliminary Text EBM Results", "text": "There has recently been interest in learning Energy-Based Models of text data. An EBM for text could enable nonautoregressive generation and more flexible conditioning than autoregressive models. For example, an EBM trained on language pairs p(x, y) could be used to translate in either direction without retraining and could be structured as log p(x, y) = f \u03b8 (x) T g \u03c6 (y) \u2212 log Z so that each language component could be trained separately. We also have much more architectural freedom when specifying EBMs meaning f \u03b8 is free to be a CNN, RNN, transformer, or MLP.\nA few works have had success training and applying text EBMs. Deng et al. (2020) find that EBMs can be used to improve the generative modeling performance of large-scale transformer language models and He et al. ( 2021) find that Joint Energy-Based Models (Grathwohl et al., 2019) can improve the calibration of text classifiers. Because of the discrete structure of the text data, both of these works train using Noise Contrastive Estimation (Gutmann & Hyv\u00e4rinen, 2010) using a pretrained autoregressive language model as the noise distribution. NCE requires a noise distribution which can be sampled from and enables exact likelihood computation. Thus, these approaches rely on and are limited by the quality of these autoregressive models.\nIdeally, we could train a text EBM on its own, without an auxiliary model. One way to do this is to use the gradient estimator of Equation 12 but the MCMC sampling task is very challenging. Text models typically have a vocabulary above 10,000 words so the size of the discrete sample space is tremendous. Further, as noted in Section 8, to apply Gibbs sampling to a model like this we would need to evaluate the energy function over 10,000 times to perform a single step of sampling! We believe Gibbs-With-Gradients can provide an avenue to train and sample from these kinds of models. As a preliminary experiment we train non-autoregressvive language models on a shortened version of the Penn Tree Bank dataset (Taylor et al., 2003). This is a dataset of short sentences with 10,000 words. We cut out all sentences with greater than 20 words and pad all shorter sentences with an \"end of sequence\" token. We feel this simplified setting is sufficient for a proof-of-concept as the configuration space is very large and Gibbs sampling is not feasible.\nOur model consists of a bidirectional LSTM (Gers et al., 1999) with 512 hidden units. We project the 10,000 words to an embedding of size 256 with a learned mapping. To compute the energy, we take the last hidden-state from each direction, concatenate them together to a 1024-dimensional vector. We project this to 512 dimensions with a linear layer, apply a Swish nonlinearity and then map to 1 dimension with another linear layer.\nWe train using PCD with a buffer size of 1000 and we use 20 steps of MCMC with Gibbs-With-Gradients to update the samples at every training iteration. Besides this, training was identical to our image EBMs in section 8.\nWe compare with a simple autoregressive language model which is based on an LSTM with 512 hidden units and use a learned word embedding of size 256.\nWe find the autoregressive model slightly outperforms the EBM. The test-set log-likelihoods of the EBM and autoregressive model are \u221277.16 and \u221274.0, respectively. For comparison, a uniform distribution over possible tokens obtains \u2212184.21 and a Categorical distribution fit to the training data obtains \u2212100.05.\nWhile we are aware these are far from state-of-the-art language modelling results, we believe they demonstrate that Gibbs-With-Gradients can enable MCMC-trained EBMs to successfully model text data with large vocabulary sizes. At every step, the sampler has 10, 000\u00d720 = 200, 000 choices for possible updates. Despite this massive sampling space, we find our acceptance rates during training are just above 70% making our approach at least 3500 times more efficient than Gibbs sampling.\nWe believe improvements could be obtained through larger models and more tuning. To further scale this approach, we believe we will need to develop further approximations which make sampling from very large categorical distributions more efficient and numerically stable. We leave this for future work.", "publication_ref": ["b3", "b6", "b10", "b38", "b5"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Statistical analysis of non-lattice data", "journal": "Journal of the Royal Statistical Society: Series D (The Statistician)", "year": "1975", "authors": "J Besag"}, {"ref_id": "b1", "title": "Comments on \"representations of knowledge in complex systems\" by u. grenander and mi miller", "journal": "J. Roy. Statist. Soc. Ser. B", "year": "1994", "authors": "J Besag"}, {"ref_id": "b2", "title": "Accurate and conservative estimates of mrf log-likelihood using reverse annealing", "journal": "", "year": "2015", "authors": "Y Burda; R Grosse; R Salakhutdinov"}, {"ref_id": "b3", "title": "Residual energy-based models for text generation", "journal": "", "year": "2020", "authors": "Y Deng; A Bakhtin; M Ott; A Szlam; M Ranzato"}, {"ref_id": "b4", "title": "Implicit generation and generalization in energy-based models", "journal": "", "year": "2019", "authors": "Y Du; I Mordatch"}, {"ref_id": "b5", "title": "Learning to forget: Continual prediction with lstm", "journal": "", "year": "1999", "authors": "F A Gers; J Schmidhuber; F Cummins"}, {"ref_id": "b6", "title": "Your classifier is secretly an energy based model and you should treat it like one", "journal": "", "year": "2019", "authors": "W Grathwohl; K.-C Wang; J.-H Jacobsen; D Duvenaud; M Norouzi; K Swersky"}, {"ref_id": "b7", "title": "No mcmc for me: Amortized sampling for fast and stable training of energy-based models", "journal": "", "year": "2020", "authors": "W Grathwohl; J Kelly; M Hashemi; M Norouzi; K Swersky; D Duvenaud"}, {"ref_id": "b8", "title": "Learning the stein discrepancy for training and evaluating energy-based models without sampling", "journal": "PMLR", "year": "2020", "authors": "W Grathwohl; K.-C Wang; J.-H Jacobsen; D Duvenaud; R Zemel"}, {"ref_id": "b9", "title": "A kernel two-sample test", "journal": "The Journal of Machine Learning Research", "year": "2012", "authors": "A Gretton; K M Borgwardt; M J Rasch; B Sch\u00f6lkopf; A Smola"}, {"ref_id": "b10", "title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "journal": "", "year": "2010", "authors": "M Gutmann; A Hyv\u00e4rinen"}, {"ref_id": "b11", "title": "Stein variational gradient descent without gradient", "journal": "", "year": "2018", "authors": "J Han; Q Liu"}, {"ref_id": "b12", "title": "Stein variational inference for discrete distributions", "journal": "", "year": "2020", "authors": "J Han; F Ding; X Liu; L Torresani; J Peng; Q Liu"}, {"ref_id": "b13", "title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "K He; X Zhang; S Ren; J Sun"}, {"ref_id": "b14", "title": "Joint energy-based model training for better calibrated natural language understanding models", "journal": "", "year": "2021", "authors": "T He; B Mccann; C Xiong; E Hosseini-Asl"}, {"ref_id": "b15", "title": "Adversarial defense using long-run dynamics of energybased models", "journal": "", "year": "2020", "authors": "M Hill; J Mitchell; S.-C. Stochastic Zhu;  Security"}, {"ref_id": "b16", "title": "Training products of experts by minimizing contrastive divergence", "journal": "Neural computation", "year": "2002", "authors": "G E Hinton"}, {"ref_id": "b17", "title": "Deep belief networks", "journal": "Scholarpedia", "year": "2009", "authors": "G E Hinton"}, {"ref_id": "b18", "title": "Variational inference for sparse and undirected models", "journal": "PMLR", "year": "2017", "authors": "J Ingraham; D Marks"}, {"ref_id": "b19", "title": "Beitrag zur Theorie des Ferround Paramagnetismus", "journal": "", "year": "1924", "authors": "E Ising"}, {"ref_id": "b20", "title": "A method for stochastic optimization", "journal": "", "year": "2014", "authors": "D P Kingma; J Ba;  Adam"}, {"ref_id": "b21", "title": "Auto-encoding variational bayes", "journal": "", "year": "2013", "authors": "D P Kingma; M Welling"}, {"ref_id": "b22", "title": "Correlated mutations in models of protein sequences: phylogenetic and structural effects", "journal": "", "year": "1999", "authors": "A S Lapedes; B G Giraud; L Liu; G D Stormo"}, {"ref_id": "b23", "title": "Adaptive gibbs samplers and related mcmc methods", "journal": "The Annals of Applied Probability", "year": "2013", "authors": "K \u0141atuszy\u0144ski; G O Roberts; J S Rosenthal"}, {"ref_id": "b24", "title": "Markov chains and mixing times", "journal": "American Mathematical Soc", "year": "2017", "authors": "D A Levin; Y Peres"}, {"ref_id": "b25", "title": "Peskun's theorem and a modified discrete-state gibbs sampler", "journal": "Biometrika", "year": "1996", "authors": "J S Liu"}, {"ref_id": "b26", "title": "Stein variational gradient descent: A general purpose bayesian inference algorithm", "journal": "", "year": "2016", "authors": "Q Liu; D Wang"}, {"ref_id": "b27", "title": "Interpretation and generalization of score matching", "journal": "", "year": "2012", "authors": "S Lyu"}, {"ref_id": "b28", "title": "The concrete distribution: A continuous relaxation of discrete random variables", "journal": "", "year": "2016", "authors": "C J Maddison; A Mnih; Y W Teh"}, {"ref_id": "b29", "title": "Protein 3d structure computed from evolutionary sequence variation", "journal": "PloS one", "year": "2011", "authors": "D S Marks; L J Colwell; R Sheridan; T A Hopf; A Pagnani; R Zecchina; C Sander"}, {"ref_id": "b30", "title": "Annealed importance sampling", "journal": "Statistics and computing", "year": "2001", "authors": "R M Neal"}, {"ref_id": "b31", "title": "Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo", "journal": "", "year": "2011", "authors": "R M Neal"}, {"ref_id": "b32", "title": "On the anatomy of mcmc-based maximum likelihood learning of energy-based models", "journal": "", "year": "2020", "authors": "E Nijkamp; M Hill; T Han; S.-C Zhu; Y N Wu"}, {"ref_id": "b33", "title": "Discontinuous hamiltonian monte carlo for sampling discrete parameters", "journal": "", "year": "2017", "authors": "A Nishimura; D Dunson; J Lu"}, {"ref_id": "b34", "title": "Searching for activation functions", "journal": "", "year": "2017", "authors": "P Ramachandran; B Zoph; Q V Le"}, {"ref_id": "b35", "title": "Bayesian models for sparse regression analysis of high dimensional data", "journal": "Bayesian Statistics", "year": "2010", "authors": "S Richardson; L Bottolo; J S Rosenthal"}, {"ref_id": "b36", "title": "Generative modeling by estimating gradients of the data distribution", "journal": "", "year": "2019", "authors": "Y Song; S Ermon"}, {"ref_id": "b37", "title": "Learning neural random fields with inclusive auxiliary generators", "journal": "", "year": "2018", "authors": "Y Song; Z Ou"}, {"ref_id": "b38", "title": "The penn treebank: an overview. Treebanks", "journal": "", "year": "2003", "authors": "A Taylor; M Marcus; B Santorini"}, {"ref_id": "b39", "title": "Training restricted boltzmann machines using approximations to the likelihood gradient", "journal": "", "year": "2008", "authors": "T Tieleman"}, {"ref_id": "b40", "title": "Using fast weights to improve persistent contrastive divergence", "journal": "", "year": "2009", "authors": "T Tieleman; G Hinton"}, {"ref_id": "b41", "title": "The hamming ball sampler", "journal": "Journal of the American Statistical Association", "year": "2017", "authors": "M K Titsias; C Yau"}, {"ref_id": "b42", "title": "Vae with a vampprior", "journal": "PMLR", "year": "2018", "authors": "J Tomczak; M Welling"}, {"ref_id": "b43", "title": "Accelerated metropolis method", "journal": "Phys. Rev. Lett", "year": "1993-07", "authors": "C J Umrigar"}, {"ref_id": "b44", "title": "Informed proposals for local mcmc in discrete spaces", "journal": "Journal of the American Statistical Association", "year": "2020", "authors": "G Zanella"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 .1Figure1. Our approach visualized. Often discrete distributions are defined by continuous functions whose input is restricted to a discrete set; here R 2 restricted to Z 2 . We use a Taylor series computed on the underlying continuous function to estimate likelihood ratios of making discrete moves; here \u00b11 in either direction. These estimated likelihood ratios are used to inform a proposal distribution over moves in the original discrete space.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 .2Figure 2. Comparison to gradient-based samplers with continuous relaxations. GWG, D-SVGD, R-HMC, and R-MALA refer to Gibbs-With-Gradients, Discrete SVGD, Relaxed HMC and Relaxed MALA, respectively. Left: Log-MMD (lower is better)between true samples and generated samples for RBMs of increasing dimension (over 3 runs). \"Target\" is log-MMD between two sets of Block-Gibbs samples. Right: Visualized samples from an RBM trained on MNIST.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 .3Figure3. RBM sampling results. Left: Log-MMD of samples over steps (lower is better). \"Target\" is Log-MMD between two sets of Block-Gibbs samples. Right: Log-ESS of various samplers after 10,000 steps. Gibbs-With-Gradients matches Block-Gibbs in MMD and outperforms unstructured baselines in ESS.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 4 .4Figure 4. Ising model sampling results. The y-axis shows log-ESS over 100,000 samples steps. Left: 10x10 lattice, right: 40x40 lattice. We see Gibbs-With-Gradients (GWG) outperforms in most settings.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 .5Figure 5. Factorial Hidden Markov Model results. \"Block-HB\" refers to the block-structured hamming ball sampler. Left, log-joint density and right, mean log-reconstruction error. GWG performs best in both evaluations, outperforming the Hamming Ball sampler which exploits model structure.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 6 .6Figure 6. Training Ising models with increasing MCMC steps. Left: Lattice Ising (dim = 625, \u03b8 = .25). Right: Erdos-Renyi Ising. Values are log(RMSE) between the learned and true J. GWG leads to better solutions with lower computational cost.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 7 .7Figure 7. Recall Curves for contact prediction with Potts models. Gibbs-With-Gradients leads to higher recall.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 8 .8Figure8. Left: data. Right: Samples from ResNet EBM. Samples generated with annealed Markov chain using 300,000 Gibbs-With-Gradients steps. Top to bottom: MNIST, omniglot, Caltech Silhouettes, Frey Faces, Histopathology.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 9 .9Figure9. RBM Sampling with Gibbs-With-Gradients extensions. GWG-3 and GWG-5 are the multi-sample variant of GWG described above with n = 3 and n = 5, respectively.", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 11 .11Figure11. Training Ising models. Top Left: Lattice Ising with increasing \u03b8 (dim = 50, steps = 50). Top Right: Lattice Ising with increasing dimension (\u03b8 = .25, steps = 25). Bottom Right: Lattice Ising with increasing steps (dim = 25, \u03b8 = .25). Bottom Right: Erdos-Renyi Ising with increasing steps. Values are log(RMSE) between the learned J and the true J. Gibbs-With-Gradients leads to better solutions with lower computational cost.", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 12 .12Figure 12. Recall Curves for contact prediction with Potts models.", "figure_data": ""}, {"figure_label": "13", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Figure 13 .13Figure 13. Inferred Couplings for CADH1 HUMAN. \"Ground Truth\" is the matrix of known distances between amino acids. All other matrices are the norms of the Potts model Jij parameter.", "figure_data": ""}, {"figure_label": "1415", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Figure 14 .Figure 15 .1415Figure 14. AIS likelihood estimates as the number of intermediate distributions increases for our Caltech Silhouettes Resnet EBM. Values converge after 30, 000 \u2248 10 4.5 step s", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Unnormalized log-likelihoods of common discrete distributions. All are differentiable with respect to x.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Test-set log-likelihoods for models trained on discrete image datasets. RBM and DBN results are taken fromBurda et al. (2015), VAE results taken fromTomczak & Welling (2018).", "figure_data": "Data TypeDatasetVAE (MLP)VAE (Conv) (GWG) (Gibbs) EBM EBMRBMDBNBinaryStatic MNIST Dynamic MNIST-86.05 -82.42-82.41 -80.40-80.01 -117.17 -86.39 -80.51 -121.19 --85.67 -(log-likelihood \u2191)Omniglot Caltech Silhouettes -112.08 -106.35 -96.20 -163.50 -103.52 -97.65 -94.72 -142.06 -100.47 -100.78 --CategoricalFrey Faces4.614.494.65---(bits/dim \u2193)Histopathology5.825.595.08---"}], "formulas": [{"formula_id": "formula_0", "formula_text": "min exp(f (x ) \u2212 f (x)) q(x|x ) q(x |x) , 1 .(1)", "formula_coordinates": [2.0, 346.61, 204.19, 194.83, 22.31]}, {"formula_id": "formula_1", "formula_text": "q \u03c4 (x |x) \u221d e 1 \u03c4 (f (x )\u2212f (x)) 1(x \u2208 H(x)).(2)", "formula_coordinates": [2.0, 342.28, 510.04, 199.16, 13.02]}, {"formula_id": "formula_2", "formula_text": "Distribution log p(x) + log Z Categorical x T \u03b8 Poisson 1 x log \u03bb \u2212 log \u0393(x + 1) HMM T t=1 x T t+1 Ax t \u2212 (w T x\u2212y) 2 2\u03c3 2 RBM i softplus(W x + b) i + c T x Ising x T W x + b T x Potts L i=1 h T i x i + L i,j=1 x T i J ij x j Deep EBM f \u03b8 (x)", "formula_coordinates": [3.0, 78.82, 373.45, 183.85, 135.92]}, {"formula_id": "formula_3", "formula_text": "d(x) = \u2212(2x \u2212 1) \u2207 x f (x) (3) whered(x) i \u2248 f (x \u2212i ) \u2212 f (x)", "formula_coordinates": [3.0, 307.08, 143.74, 234.36, 31.28]}, {"formula_id": "formula_4", "formula_text": "d(x) ij = \u2207 x f (x) ij \u2212 x T i \u2207 x f (x) i(4)", "formula_coordinates": [3.0, 355.03, 206.94, 186.41, 12.69]}, {"formula_id": "formula_5", "formula_text": "q \u2207 (x |x) \u221d ed (x) 2 1(x \u2208 H(x))(5)", "formula_coordinates": [3.0, 360.86, 435.1, 180.59, 13.15]}, {"formula_id": "formula_6", "formula_text": "q(i|x) = Categorical Softmax d (x) 2 (6)", "formula_coordinates": [3.0, 337.86, 579.62, 203.58, 24.94]}, {"formula_id": "formula_7", "formula_text": "d (x) 2 Sample i \u223c q(i|x) x = flipdim(x, i) Compute q(i|x ) = Categorical Softmax d (x ) 2", "formula_coordinates": [4.0, 65.4, 106.06, 193.09, 58.07]}, {"formula_id": "formula_8", "formula_text": "min exp(f (x ) \u2212 f (x)) q(i|x ) q(i|x) , 1", "formula_coordinates": [4.0, 103.07, 187.62, 141.37, 22.31]}, {"formula_id": "formula_9", "formula_text": "var p (h, Q) = lim T \u2192\u221e 1 T var T t=1 h(x t )(7)", "formula_coordinates": [4.0, 93.58, 475.34, 195.86, 30.2]}, {"formula_id": "formula_10", "formula_text": "Gap(Q) = 1 \u2212 \u03bb 2 (8)", "formula_coordinates": [4.0, 135.97, 561.23, 153.47, 9.65]}, {"formula_id": "formula_11", "formula_text": "(a) var p (h, Q \u2207 ) \u2264 varp(h,Q) c + 1\u2212c c \u2022 var p (h) (b) Gap(Q \u2207 ) \u2265 c \u2022 Gap (Q)", "formula_coordinates": [4.0, 310.77, 217.37, 183.01, 33.0]}, {"formula_id": "formula_12", "formula_text": "c = e \u2212 1 2 LD 2 H and D H = sup x \u2208H(x) ||x \u2212 x ||.", "formula_coordinates": [4.0, 333.91, 266.41, 189.59, 13.8]}, {"formula_id": "formula_13", "formula_text": "log p(x) = log(1 + exp(W x + c)) + b T x \u2212 log Z (9)", "formula_coordinates": [5.0, 317.32, 447.73, 224.12, 11.03]}, {"formula_id": "formula_14", "formula_text": "log p(x) = \u03b8 \u2022 x T Jx \u2212 log Z (10", "formula_coordinates": [6.0, 114.05, 305.54, 171.24, 11.03]}, {"formula_id": "formula_15", "formula_text": ")", "formula_coordinates": [6.0, 285.29, 307.94, 4.15, 8.64]}, {"formula_id": "formula_16", "formula_text": "p(y|x) = L t=1 N (y t ; W x t + b, \u03c3 2 ) p(x) = p(x 1 ) L t=2 p(x t |x t\u22121 )(11)", "formula_coordinates": [6.0, 357.22, 130.97, 184.23, 65.03]}, {"formula_id": "formula_17", "formula_text": "\u2207 \u03b8 log p(x) = \u2207 \u03b8 f \u03b8 (x) \u2212 E p(x) [\u2207 \u03b8 f \u03b8 (x)](12)", "formula_coordinates": [6.0, 329.93, 674.83, 211.51, 9.96]}, {"formula_id": "formula_18", "formula_text": "log p(x) = D i=1 h T i x i + D i,j=1 x T i J ij x j \u2212 log Z (13)", "formula_coordinates": [7.0, 71.75, 629.66, 217.69, 30.32]}, {"formula_id": "formula_19", "formula_text": "log p(x) = f \u03b8 (x) \u2212 log Z (14)", "formula_coordinates": [8.0, 120.78, 284.97, 168.66, 9.65]}, {"formula_id": "formula_20", "formula_text": "q \u03c4 (x |x) \u221d e 1 \u03c4 (f (x )\u2212f (x)) 1(x \u2208 H(x)). (15", "formula_coordinates": [12.0, 90.28, 133.84, 195.01, 13.03]}, {"formula_id": "formula_21", "formula_text": ")", "formula_coordinates": [12.0, 285.29, 137.53, 4.15, 8.64]}, {"formula_id": "formula_22", "formula_text": "exp(f (x ) \u2212 f (x)) q \u03c4 (x|x ) q \u03c4 (x |x) = exp(f (x ) \u2212 f (x)) exp( 1 \u03c4 (f (x) \u2212 f (x ))Z(x) exp( 1 \u03c4 (f (x ) \u2212 f (x))Z(x ) = exp 1 \u2212 2 \u03c4 (f (x ) \u2212 f (x)) Z(x) Z(x )(16)", "formula_coordinates": [12.0, 61.08, 234.72, 228.36, 80.55]}, {"formula_id": "formula_23", "formula_text": "Z(x) = x \u2208H(x) exp( 1 \u03c4 (f (x ) \u2212 f (x)", "formula_coordinates": [12.0, 82.46, 327.24, 160.74, 13.47]}, {"formula_id": "formula_24", "formula_text": "x = x such that Q 1 (x , x) > c \u2022 Q 2 (x , x) then (a) var p (h, Q 1 ) \u2264 varp(h,Q1) c + 1\u2212c c \u2022 var p (h) (b) Gap(Q 1 ) \u2265 c \u2022 Gap(Q 2 )", "formula_coordinates": [12.0, 55.44, 486.93, 234.0, 71.03]}, {"formula_id": "formula_25", "formula_text": "E p [h(x) 2 ] \u2212 E p [h(x)] 2 .", "formula_coordinates": [12.0, 184.28, 597.14, 96.96, 11.23]}, {"formula_id": "formula_26", "formula_text": "Q \u2207 (x , x) \u2265 c \u2022 Q(", "formula_coordinates": [12.0, 55.44, 627.03, 77.81, 10.31]}, {"formula_id": "formula_27", "formula_text": "\u2206(x , x) := f (x ) \u2212 f (x) \u2207(x , x) := \u2207 x f (x) T (x \u2212 x) D H := sup x \u2208H(x) ||x \u2212 x|| We restate the target proposal for x \u2208 H(x) q 2 (x |x) = exp \u2206(x ,x) 2 Z(x)", "formula_coordinates": [12.0, 306.97, 88.37, 179.04, 111.73]}, {"formula_id": "formula_28", "formula_text": "Z(x) = x \u2208H(x) exp \u2206(x , x) 2 .", "formula_coordinates": [12.0, 354.78, 223.56, 139.31, 27.27]}, {"formula_id": "formula_29", "formula_text": "Q(x , x) = q 2 (x |x) min 1, Z(x) Z(x ) = min \uf8f1 \uf8f2 \uf8f3 exp \u2206(x ,x) 2 Z(x) , exp \u2206(x ,x) 2 Z(x ) \uf8fc \uf8fd \uf8fe .", "formula_coordinates": [12.0, 317.83, 283.12, 213.21, 61.27]}, {"formula_id": "formula_30", "formula_text": "q \u2207 (x |x) = exp \u2207(x ,x) 2 Z (x)", "formula_coordinates": [12.0, 369.2, 380.27, 102.14, 29.97]}, {"formula_id": "formula_31", "formula_text": "Z(x) = x \u2208H(x) exp \u2207(x , x) 2", "formula_coordinates": [12.0, 357.0, 433.7, 126.36, 27.27]}, {"formula_id": "formula_32", "formula_text": "Q \u2207 (x , x) = q \u2207 (x |x) min \uf8f1 \uf8f2 \uf8f3 1, exp (\u2206(x , x)) exp \u2207(x ,x)\u2212\u2207(x,x ) 2 Z (x) Z(x ) \uf8fc \uf8fd \uf8fe = min \uf8f1 \uf8f2 \uf8f3 exp \u2207(x ,x) 2 Z (x) , exp \u2206(x , x) + \u2207(x,x ) 2 Z (x ) \uf8fc \uf8fd \uf8fe .", "formula_coordinates": [12.0, 307.48, 486.3, 233.91, 91.97]}, {"formula_id": "formula_33", "formula_text": "\u2207 x f (x) that |\u2207(x , x) \u2212 \u2206(x , x)| \u2264 L 2 ||x \u2212 x || 2 (17)", "formula_coordinates": [12.0, 347.35, 637.39, 194.09, 39.15]}, {"formula_id": "formula_34", "formula_text": "\u2212 L 2 D 2 H \u2264 \u2207(x , x) \u2212 \u2206(x , x) \u2264 L 2 D 2 H (18)", "formula_coordinates": [12.0, 341.56, 699.66, 199.88, 22.31]}, {"formula_id": "formula_35", "formula_text": "Z(x) = x \u2208H(x) exp \u2207(x , x) 2 = x \u2208H(x) exp \u2206(x , x) 2 exp \u2207(x , x) \u2212 \u2206(x , x) 2 \u2264 x \u2208H(x) exp \u2206(x , x) 2 exp LD 2 H 4 \u2264 exp LD 2 H 4 x \u2208H(x) exp \u2206(x , x) 2 = exp LD 2 H 4 Z(x)(19)", "formula_coordinates": [13.0, 55.44, 125.32, 243.46, 157.69]}, {"formula_id": "formula_36", "formula_text": "Z(x) \u2265 exp \u2212LD 2 H 4 Z(x).(20)", "formula_coordinates": [13.0, 110.49, 326.96, 178.95, 23.89]}, {"formula_id": "formula_37", "formula_text": "Q \u2207 (x , x) \u2265 c \u2022 Q(x , x) for c = exp \u2212LD 2 H 2 . Since both Q(x , x) = min{a, b} and Q \u2207 (x , x) = min{a \u2207 , b \u2207 } it is sufficient to show a \u2207 \u2265 c \u2022 a and b \u2207 \u2265 c \u2022 b to prove the desired result.", "formula_coordinates": [13.0, 55.44, 388.39, 235.74, 141.89]}, {"formula_id": "formula_38", "formula_text": "a \u2207 a = exp \u2207(x ,x) 2 Z (x) Z(x) exp \u2206(x ,x) 2 = Z(x) Z(x) exp \u2207(x , x) 2 \u2212 \u2206(x , x) 2 \u2265 exp \u2212LD 2 H 4 exp \u2207(x , x) 2 \u2212 \u2206(x , x) 2 \u2265 exp \u2212LD 2 H 4 exp \u2212LD 2 H 4 = exp \u2212LD 2 H 2 (21) Now the b terms b \u2207 b = exp \u2206(x , x) + \u2207(x,x ) 2 Z (x ) Z(x ) exp \u2206(x ,x) 2 = Z(x ) Z(x ) exp \u2206(x , x) + \u2207(x,x ) 2 exp \u2206(x ,x) 2 = Z(x ) Z(x ) exp \u2206(x , x) 2 + \u2207(x, x ) 2 \u2265 exp \u2212LD 2 H 4 exp \u2206(x , x) 2 + \u2207(x, x ) 2 = exp \u2212LD 2 H 4 exp \u2207(x, x ) 2 \u2212 \u2206(x, x ) 2 \u2265 exp \u2212LD 2 H 4 exp \u2212LD 2 H 4 = exp \u2212LD 2 H 2(22)", "formula_coordinates": [13.0, 67.71, 70.22, 473.73, 644.14]}, {"formula_id": "formula_39", "formula_text": "\u2212LD 2 H 2 a and b \u2207 \u2265 exp \u2212LD 2 H 2", "formula_coordinates": [13.0, 307.44, 340.47, 234.0, 35.19]}, {"formula_id": "formula_40", "formula_text": "Q \u2207 (x , x) \u2265 exp \u2212LD 2 H 2 Q(x , x)(23)", "formula_coordinates": [13.0, 347.02, 386.99, 194.42, 23.89]}, {"formula_id": "formula_41", "formula_text": "z \u223c p c (z), x = \u0393(z)", "formula_coordinates": [13.0, 373.13, 687.4, 102.62, 9.65]}, {"formula_id": "formula_42", "formula_text": "p \u03bb c (x) = N (z; 0, I)p(\u0393 \u03bb (z))", "formula_coordinates": [14.0, 114.71, 125.39, 115.45, 12.69]}, {"formula_id": "formula_43", "formula_text": "\u0393 \u03bb (x) = 1 1 + e \u2212x/\u03bb", "formula_coordinates": [14.0, 131.23, 180.11, 80.72, 22.49]}, {"formula_id": "formula_44", "formula_text": "q(x |x) = N x ; x + 2 \u2207 x log p \u03bb c (x), 2(24)", "formula_coordinates": [14.0, 79.0, 505.57, 210.44, 17.64]}, {"formula_id": "formula_45", "formula_text": "min p c (x )q(x|x ) p c (x)q(x |x) , 1 .(25)", "formula_coordinates": [14.0, 120.32, 551.96, 169.12, 23.23]}, {"formula_id": "formula_46", "formula_text": "H \u03bb (x, v) = \u2212 log p \u03bb c (x) + 1 2 v T M v", "formula_coordinates": [14.0, 99.83, 694.68, 144.86, 22.31]}, {"formula_id": "formula_47", "formula_text": "min H(x, v) H(x , v ) , 1(26)", "formula_coordinates": [14.0, 383.07, 104.49, 158.37, 22.31]}, {"formula_id": "formula_48", "formula_text": "H(x, v) = \u2212 log p c (x) + 1 2 v T M v.", "formula_coordinates": [14.0, 353.64, 149.02, 141.6, 22.31]}, {"formula_id": "formula_49", "formula_text": "W \u223c N (0, .05I), b, c \u223c N (0, I).", "formula_coordinates": [14.0, 347.92, 392.62, 153.04, 8.74]}, {"formula_id": "formula_50", "formula_text": "i 1 , . . . , i N \u223c q(i|x)", "formula_coordinates": [15.0, 133.06, 376.45, 78.76, 9.65]}, {"formula_id": "formula_51", "formula_text": "min exp(f (x ) \u2212 f (x)) N n=1 q(i n |x ) N n=1 q(i n |x) , 1 .(27)", "formula_coordinates": [15.0, 70.88, 444.41, 218.56, 29.67]}, {"formula_id": "formula_52", "formula_text": "log p(x, h) = h T W x + b T x + c T h \u2212 log Z (28", "formula_coordinates": [15.0, 76.1, 670.15, 209.19, 11.03]}, {"formula_id": "formula_53", "formula_text": ")", "formula_coordinates": [15.0, 285.29, 672.54, 4.15, 8.64]}, {"formula_id": "formula_54", "formula_text": "log p(x) = log h p(x, h) = log h exp(h T W x + b T x + c T h) = log(1 + exp(W x + c)) + b T x \u2212 log Z (29)", "formula_coordinates": [15.0, 314.83, 281.57, 226.61, 62.22]}, {"formula_id": "formula_55", "formula_text": "p(x|h) = Bernoulli(W x + c) p(h|x) = Bernoulli(W T h + b).", "formula_coordinates": [15.0, 360.61, 410.13, 127.66, 25.41]}, {"formula_id": "formula_56", "formula_text": "p(y|x) = L t=1 N (y t ; W x t + b, \u03c3 2 ) p(x) = p(x 1 ) L t=2 p(x t |x t\u22121 ) p(x 1 ) = K k=1 Bernoulli(x 1k ; \u03b1 k ) p(x t+1 |x t ) = K k=1 Bernoulli(x (t+1)k ; \u03b2 x tk k (1 \u2212 \u03b2 k ) 1\u2212x tk )(32)", "formula_coordinates": [17.0, 309.77, 107.63, 231.67, 146.55]}, {"formula_id": "formula_57", "formula_text": "W, b \u223c N (0, I)(33)", "formula_coordinates": [17.0, 392.9, 345.15, 148.54, 8.96]}, {"formula_id": "formula_58", "formula_text": "L 1 (J) = ij ||J ij || 2 . (34", "formula_coordinates": [17.0, 381.56, 658.34, 155.73, 19.91]}, {"formula_id": "formula_59", "formula_text": ")", "formula_coordinates": [17.0, 537.29, 658.66, 4.15, 8.64]}, {"formula_id": "formula_60", "formula_text": "f t (x) = \u03b2 t f (x) + (1 \u2212 \u03b2 t ) log p n (x)(35)", "formula_coordinates": [19.0, 97.47, 508.07, 191.97, 9.65]}], "doi": "10.1103/PhysRevLett.71.408"}