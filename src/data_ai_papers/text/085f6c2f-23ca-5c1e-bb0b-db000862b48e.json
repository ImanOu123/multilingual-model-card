{"title": "Discriminative Models for Multi-Class Object Layout", "authors": "Chaitanya Desai; Deva Ramanan; Charless C Fowlkes", "pub_date": "", "abstract": "Many state-of-the-art approaches for object recognition reduce the problem to a 0-1 classification task. This allows one to leverage sophisticated machine learning techniques for training classifiers from labeled examples. However, these models are typically trained independently for each class using positive and negative examples cropped from images. At test-time, various post-processing heuristics such as non-maxima suppression (NMS) are required to reconcile multiple detections within and between different classes for each image. Though crucial to good performance on benchmarks, this post-processing is usually defined heuristically. We introduce a unified model for multi-class object recognition that casts the problem as a structured prediction task. Rather than predicting a binary label for each image window independently, our model simultaneously predicts a structured labeling of the entire image (Fig. 1). Our model learns statistics that capture the spatial arrangements of various object classes in real images, both in terms of which arrangements to suppress through NMS and which arrangements to favor through spatial co-occurrence statistics. We formulate parameter estimation in our model as a max-margin learning problem. Given training images with ground-truth object locations, we show how to formulate learning as a convex optimization problem. We employ the cutting plane algorithm of Joachims et al. (Mach.  Learn. 2009)  to efficiently learn a model from thousands", "sections": [{"heading": "Introduction", "text": "A contemporary and successful approach to object recognition is to formulate it as a classification task, e.g. \"Does an image window at location i contain a given object o?\". The classification formulation allows immediate application of a variety of sophisticated machine learning techniques in order to learn optimal detectors from training data. Such methods have the potential to encapsulate those subtle statistical regularities of the visual world which separate object from background. As a result, learning approaches have often yielded detectors that are more robust and accurate than their hand built counterparts for a range of applications, from edge and face detection to general purpose object recognition (see e.g., Rowley et al. 1996;Viola and Jones 2004).\nIn contrast to the well founded techniques used for classification of individual image patches, the problem of correctly detecting and localizing multiple objects from multiple classes within an image of a scene has generally been approached in a far more ad-hoc manner. For example, nonmax suppression (NMS) is required to remove some detections returned by a classifier based on overlap criteria or more complicated heuristics (e.g. the mode finding approach Fig. 1 Our framework. Classification-based approaches for recognition predict a binary label for a cropped window (left). We formulate the recognition problem as predicting a sparse, structured label vector specifying which windows, if any, contain particular objects in an entire input image. The latter allows our model to capture a wide range of contextual constraints among objects as described in Table 1 and Fig. 2 Fig. 2 Our novel contributions include the ability to learn inhibitory intra-class constraints (NMS) and inhibitory inter-class constraints (Mutual Exclusion) in a single unified model along with contextual cuing and spatial co-occurrence. Na\u00efve methods for NMS or mutual exclusion may fail for objects that tend to overlap themselves (left) and other objects (right). In contrast, our framework learns how best to enforce such constraints from training data. We formulate the tasks of NMS and Mutual Exclusion using the language of structured prediction. This allows us to compute an optimal model by minimizing a convex objective function of Dalal and Triggs 2005). Such tricks of the trade are essential to good performance on benchmarks designed to penalize multiple non-localized detections, however, they highlight a clear disconnect between training and testing phases. The objective optimized during learning only characterizes a sub-component of the final system used at runtime. Furthermore, there is a wide range of possible interactions between object detections which is not fully captured by ad-hoc approaches. In street-level views, pedestrians are likely to occur standing next to each other, nearly overlapping, but unlikely to occur directly above or below each other (Fig. 2). In general, spatial object-object interactions may be arbitrarily complex and depend on latent information which is not readily available from single image. As an extreme example, studies of proxemics (Hall 1966), the body spacing and pose of people as they interact, shows that physical spacing between people depends in complicated ways on their \"social distance\". While such complex interactions are difficult to encode, we argue there does exist useful information that is being ignored by current ad-hoc approaches to NMS.\nTable 1 A taxonomy of interactions captured in our model. Within a single object class, our model can favor typical spatial layouts of objects (people often stand in crowds) while directly learning how to inhibit overlapping detections in such cases (NMS). Our model also captures long-range interactions between objects, such as the constraint that there exists at most one object instance (counting). Analogous interactions exist between object classes, including typical spatial relations between objects (bottles sit on tables), mutual exclusion (dog and cat detectors should not respond to the same image region), and cooccurrence (couches and cars do not commonly co-occur)", "publication_ref": ["b27", "b34", "b4", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Within-class", "text": "Between-class NMS is generally described in terms of intra-class inhibition, but can be generalized to suppression of overlapping detections between different classes. We refer to this more general constraint, that two objects cannot occupy the same 3D volume at the same time, as mutual exclusion. As seen in a 2D image projection, the exact nature of this constraint depends on the object classes. Figure 2 (right) shows an example of ground-truth labelings in the PASCAL VOC (Everingham et al. 2007) dataset in which strict mutual-exclusion would produce sub-optimal performance.\nObject detections can also serve to enhance rather than inhibit other detections within a scene. This has been an area of active research in object recognition over the last few years (Torralba et al. 2004;Murphy et al. 2003;Galleguillos et al. 2008;He et al. 2004;Hoiem et al. 2008;Baur et al. 2008;Kumar and Hebert 2005). For example, different object classes may be likely to co-occur in a particular spatial layout. People ride on bikes, bottles rest on tables, and so on. In contextual cueing, a confident detection of one object (a bike) provides evidence that increases the likelihood of detecting another object (a person above the bike) (Baur et al. 2008;Galleguillos et al. 2008;Kumar and Hebert 2005). Contextual cueing can also occur within an object category, e.g., a crowd of pedestrians reinforcing each other's detection responses. An extreme example of this phenomena is near-regular texture in which the spatial locations of nearly identical elements provides a strong prior on the expected locations of additional elements, lowering their detection threshold (Liu et al. 2004).\nIn Table 1 we outline a simplified taxonomy of different types of object-object interactions, both positive and negative, within and between classes. The contribution of this paper is a single model that incorporates all interactions from Table 1 through the framework of structured prediction. Rather than returning a binary label for a each image window, our model simultaneously predicts a set of detections for multiple objects from multiple classes over the entire image. Given training images with ground-truth object locations, we show how to formulate parameter estimation as a convex max-margin learning problem. We employ the cutting plane algorithm of Joachims et al. (2009) to efficiently learn globally optimal parameters from thousands of training images.\nIn the sections that follow we formulate the structured output model in detail, describe how to perform inference and learning, and detail the optimization procedures used to efficiently learn parameters. We show state-of-the-art results on the PASCAL 2007 VOC benchmark (Everingham et al. 2007), indicating the benefits of learning a global model that encapsulates the layout statistics of multiple objects classes in real images. We conclude with a discussion of related work and future directions.", "publication_ref": ["b8", "b31", "b23", "b12", "b14", "b15", "b1", "b18", "b1", "b12", "b18", "b20", "b16", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Model", "text": "We describe a model for capturing interactions across a family of object detectors. To do so, we will explicitly represent an image as a collection of overlapping windows at various scales. The location of the ith window is given by its center and scale, written as l i = (x, y, s). The collection of N windows are precisely the regions scored by a scanningwindow detector. Write x i for the features extracted from window i. For example, in our experiments x i is a normalized histogram of gradient features (Dalal and Triggs 2005). The entire image can then be represented as the collection of feature vectors X = {x i : i = 1, . . . , N}.\nAssume we have K object models. We write y i \u2208 {0, . . . , K} for the label of the ith window, where the 0 label designates the background. Let Y = {y i : i = 1, . . . , N} be the entire label vector for the set of all sub-windows in an image. We define the score of labeling image X with vector Y as:\nS(X, Y ) = i,j w T y i ,y j d ij + i w T y i x i (1)\nwhere w y i ,y j represent weights that encode valid geometric configurations of object classes y i and y j , and w y i represents a local template for object class i. d ij is a spatial context feature that bins the relative location of window i and j into one of D canonical relations including above, below, overlapping, next-to, near, and far (Fig. 3). Hence d ij is a sparse binary vector of length D with a 1 for the kth element when the kth relation is satisfied between the current pair of windows. w y i ,y i encodes the valid geometric arrangements of a single class. For example, if people occur beside one another but not above, the weight from w y i ,y i associated with next-to relations would then be large. Local model. In our current implementation, rather than learning a local template, we simply use the output of the local detector as the single feature. To learn biases between Fig. 3 A visualization of our spatial histogram feature d ij . We consider the location of the center of window j with respect to a coordinate frame defined by window i, denoted by the thickly outlined box. The dashed and dotted rectangles represent regions over which the center of window j are binned. The relative location of j must either be far or near. For near windows, we consider above, ontop, below, and symmetric next-to bins as shown. To allow our model to reproduce the behavior of baseline modules that perform NMS with a criteria of 50% relative overlap, we also include a binary overlap feature. This makes d ij a 7 dimensional sparse binary vector different object classes, we append a constant 1 to make x i two-dimensional.\nBackground class. Since we are concerned only with the relative difference in scores between labelings, we have an extra degree of freedom in defining the weights. We constrain local and pairwise background weights w 0 and w i0 and w 0i to be 0. Since the majority of windows in an image will be labelled as background, this significantly speeds up computations with the model.", "publication_ref": ["b4"], "figure_ref": [], "table_ref": []}, {"heading": "Inference", "text": "Computing arg max Y S(X, Y ) is NP hard unless the pairwise potentials happen to have some particular structure (e.g., super-modularity with K = 1). For more general cases, one must resort to search techniques such as branch-and-bound or A* to find exact minima. In our experiments, we use a simple greedy forward search. We extensively evaluate the effectiveness of our greedy inference procedure in Sect. 7.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Greedy Forward Search", "text": "Our algorithm for optimizing (1) is analogous to greedy algorithms typically used for NMS (Leibe et al. 2004). (1) Initialize the label vector Y to the background class for each window. (2) Greedily select the single window that, when labelled as a non-background class, increases the score S by the largest amount. (3) Stop when instancing any other detection decreases the total score. Na\u00efvely re-computing the score at each step of the algorithm takes excessively long but we can track the potential gain of adding each detection incrementally.\nWe write I for a particular set of instanced window-class pairs {(i, c)} and write Y (I ) for the associated label vector where y i = c for all pairs in I and y i = 0 otherwise. We define the change in score obtained by adding window-class pair (i, c) to the set of instances I as\n\u0394(i, c) = S(X, Y (I \u222a {(i, c)})) \u2212 S", "publication_ref": ["b19"], "figure_ref": [], "table_ref": []}, {"heading": "(X, Y (I ))", "text": "Initialize I = { }, S = 0 and \u0394(i, c) = w T c x i and repeat:\n1. (i * , c * ) = arg max (i,c) \u2208I \u0394(i, c) 2. I = I \u222a {(i * , c * )} 3. S = S + \u0394(i * , c * ) 4. \u0394(i, c) = \u0394(i, c) + w T c * ,c d i * ,i + w T c,c * d i,i * until \u0394(i * , c * ) < 0\nor all windows are instanced. In Step 4, we update \u0394(i, c) for un-instanced window-class pairs by adding the pairwise costs due to the newly instanced pair (i * , c * ). For additional speed ups, we ran the above algorithm on a set of windows that passed an initial minimal threshold and conservative NMS step. This substantially reduces the number of windows the algorithm must consider.\nWhile this greedy selection procedure can produce suboptimal results, we have found that in practice it yields quite good solutions. On our datasets, the greedy procedure produces globally optimal solutions on at least 97.6% of the test cases. In Sect. 7 we present an empirical comparison to other optimization techniques as well as discussing theoretical justifications for this good performance.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Marginals", "text": "Many object recognition benchmarks such as PASCAL are scored by ranking detections with a precision-recall curve. This means we need to associate a score with each detected window. To obtain a score, we can appeal to a probabilistic version of our model, which would correspond to a conditional random field (CRF) written as P (Y |X) = 1 Z(X) e S(X,Y ) . One natural score for an individual detection is to use the marginal posterior P (y i = c|X) however this requires marginalizing over an exponential number of configurations which is intractable in our model. Instead we develop an approximation based on the log-odds ratio 1\nm(y i = c) = log P (y i = c|X) P (y i = c|X) = log y r P (y i = c, y r |X) y s ,c =c P (y i = c , y s |X)(2)\nWe write y r and y s for a N \u2212 1 dimensional vector of labels for the remaining N \u2212 1 windows other than i. Both sums above still require marginalizing out an exponential number of labels, but let us assume the posterior mass inside each sum is dominated by the most probable label y * r and the second best label y * s with class c * respectively.\ny * r = arg max y r S(X, y i = c, y r ) (y * s , c * ) = arg max (y s ,c =c) S(X, y i = c , y s ) (3)\nThen the marginal log-odds ratio equation ( 2) can approximated by\nm(y i = c) \u2248 log P (y i = c, y r * |X) P (y i = c * , y s * |X) = S(X, y i = c, y r * ) \u2212 S(X, y i = c * , y s * )\nIt is straightforward to extend our greedy maximization procedure for optimizing (1) to solve (3). This is used for the per detection scoring presented in the result section. In practice, we approximate the marginal by m(y i = c) \u2248 \u0394(i, c) computed during the greedy optimization.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Learning", "text": "In order to describe the learning algorithm, we first re-write the score function from (1) in terms of a single linear parameter vector w. To do this, we encapsulate the effect of Y and X in a potential function, writing\nS(X, Y ) = i,j w T s \u03c8(y i , y j , d ij ) + i w T a \u03c6(x i , y i ) (4)\nwhere w s and \u03c8() are vectors of length DK 2 , and w a and \u03c6() are vectors of length KF , where D is the number of spatial relations, K is the number of classes and F is the length of feature vector x i . In general, each object class may use a feature vector of different length. The vector \u03c8() will contain at most D nonzero entries and the vector \u03c6() will contain only F nonzero entries. We can then write the score as S(X, Y ) = w T \u03a8 (X, Y ) where\nw = w s w a , \u03a8(X,Y)= ij \u03c8(y i , y j , d ij ) i \u03c6(x i , y i ) (5)\nwhere our greedy inference procedure solves\nY * = arg max Y w T \u03a8 (X, Y ) (6)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Convex Training", "text": "Assume we are given a collection of training images X i and labels Y i . We want to find a model w that, given a new image X i , tends to produce the true label vector Y * i = Y i . We formulate this as a regularized learning problem:\narg min w,\u03be i \u22650 w T w + C i \u03be i s.t. \u2200i, H i w T \u0394\u03a8 (X i , Y i , H i ) \u2265 l(Y i , H i ) \u2212 \u03be i (7)\nwhere \u0394\u03a8 (X i , Y i , H i ) = \u03a8 (X i , Y i ) \u2212 \u03a8 (X i , H i ). The constraint from ( 7) specifies the following: Consider the ith training image X i and its true label Y i . We want the true label to score higher than all other hypothesized labelings {H i }. However not all incorrect labelings are equally bad. The loss function l(Y i , H i ) measures how incorrect H i is and penalizes the slack variable \u03be i in proportion. This loss function formulation from ( 7) is often called marginrescaling (Tsochantaridis et al. 2004).\nWe consider notions of loss that decompose across the N windows: l(Y, H ) = N i=1 l(y i , h i ). One simple windowspecific loss is 0-1:\nl 01 (y i , h i ) = I (y i = h i )\nHence, the constraint from ( 7) requires that label Y scores much higher than those hypotheses H that differ from the ground-truth on many windows. However note that l 01 incorrectly penalizes detections that overlap true positives as false positives. A more appropriate loss that handles overlap a bit better is:\nl ov (y i , h i ) = \u23a7 \u23aa \u23aa \u23a8 \u23aa \u23aa \u23a9 1: y i = bg \u2227 h i = y i 1: h i = bg \u2227 \u00ac\u2203j s.t. [ov(i, j ) > .5 \u2227 y j = h i ] 0: otherwise (8)\nThe top condition corresponds to a missed detection, while the second corresponds to a false positive (where we check to make there does not exist an overlapping true detection). One may also define a soft loss that assigns a value between 0 and 1 for partially overlapping windows, as in Blaschko and Lampert (2008).", "publication_ref": ["b32", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Cutting Plane Optimization", "text": "Consider the following unconstrained formulation that is equivalent to the constrained problem from (7):\nw * = arg min w L(w) where L(w) = 1 2 w 2 + CR(w) R(w) = N i=1 max H (0, l(Y i , H ) \u2212 w T \u0394\u03a8 (X i , Y i , H )) (9)\nIn the above formulation, R(w) is a convex function since it is the maximum of a set of linear functions and N is the total number of training examples. This proves that the overall objective function L(w) in convex since it is the sum of two convex functions.\nWe follow the derivation from Teo et al. (2007) and call ( 7) the master problem. We define the following reduced problem w t = arg min w L t (w) where L t (w) = 1 2 w 2 + CR t (w) (\nwhere the convex hinge loss R is approximated by a piecewise linear function R t . The approximation is constructed from a small set of lower-tangent planes called cutting planes. Each cutting plane will be a sub-gradient g of the function R(w) computed at a particular point w j . The subgradient is computed as:\ng(w j ) = \u2212 N i=1 \u03c0 i \u0394\u03a8 (X i , Y i , H * i ) \u03c0 i = 1 if l(Y i , H * i ) \u2212 w T j \u0394\u03a8 (X i , Y i , H * i ) \u2265 0 0 otherwise H * i = arg max H l(Y i , H ) \u2212 w T \u0394\u03a8 (X i , Y i , H ) (11)\nwhere H * i is the most violated constraint for image i under the current weight vector w. The subgradient provides a linear lower bound for R(w). R(w) \u2265 R(w j ) + g(w j ) T (w \u2212 w j ) \u2200w (12)\nTo obtain a tighter lower bound of R(w), we will take the point-wise maximum of cutting planes computed at points w 1 , . . . , w t\u22121 , adding the zero-plane to the set since the hinge loss R is nonnegative: R t (w) = max 0, max j =1,...,t\u22121 w T g(w j ) + b j \u2200w (13)", "publication_ref": ["b30"], "figure_ref": [], "table_ref": []}, {"heading": "Dual QP for Cutting Planes", "text": "Consider the reduced problem from (10) L t (w) = 1 2 w 2 + CR t (w), where R t (w) is as defined in ( 13). The primal QP can be written as:\narg min w,\u03be >0 1 2 w 2 + C\u03be s.t. w \u2022 g(w i ) + b i \u2264 \u03be, \u2200i = 1, . . . , t\nThe full Lagrangian and the associated KKT conditions are:\nL(w, \u03be, \u03b1, \u03bc) = 1 2 w 2 + C\u03be + t i=1 \u03b1 i (w \u2022 g(w i ) + b i \u2212 \u03be) \u2212 \u03bc\u03be\nTaking the required partial derivatives for the KKT conditions gives:\n\u2202L \u2202w = 0 =\u21d2 w = t i=1 \u03b1 i g(w i ) \u2202L \u2202\u03be = 0 =\u21d2 C \u2265 t i=1 \u03b1 i\nPlugging in the KKT conditions into the Lagrangian yields the dual QP:\narg max \u03b1>0 \u2212 1 2 t i=1 t j =1 \u03b1 i g(w i ) T g(w j )\u03b1 j s.t. t i=1 \u03b1 i \u2264 C\nThe solution vector \u03b1 to the QP is used to recover w using the 1st KKT condition. Note that solving the dual QP of the reduced problem is a function of t variables and is independent of the dimensionality of the feature vector \u03a8 (..). This makes the cutting plane approach easily scalable to learning from high dimensional features.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Standard Cutting Plane Algorithm", "text": "Initialize t = 0 and the set of cutting planes to be empty. Iterate:\n1. Compute w t = arg min t L t (w) where L t (w) = 1 2 w 2 + CR t (w). This can be solved with a dual QP with t variables. Since t is typically small (10-100), this can be solved with off-the-shelf solvers. We use the publicly available simplex solver from Franc (2006). Compute L t (w t ). 2. Compute the subgradient g(w t ) and add the new cutting plane w T g(w t ) + b t to the set. Compute L(w t ).\nAs in Teo et al. (2007), we iterate until the stopping condition L(w t ) \u2212 L t (w t ) < . Define the optimal solution as L * = min w L(w). It is relatively straightforward to show that \u2200t, we have the lower and upper bounds L t (w t ) \u2264 L * \u2264 L(w t ). The iteration must terminate because the lower bound is non-decreasing L t (w t ) \u2265 L t\u22121 (w t\u22121 ).\nWe give the following intuition behind why the bounds hold: Since w * is the globally optimal solution for problem (9), L * = L(w * ). By definition, L(w * ) = 1 2 w * 2 + CR(w * ) and L t (w * ) = 1 2 w * 2 + CR t (w * ). Since R t (w * ) is a point-wise max taken over a bunch of lower tangent planes to R(w * ), we know that R t (w * ) \u2264 R(w * ). Therefore L t (w * ) \u2264 L(w * ). For any arbitrary w t , such that w t = w * , the envelope of lower tangent planes will not be as tight as that constructed using w * . Mathematically this translates to L t (w t ) \u2264 L t (w * ) whenever w t = w * . Thus L t (w t ) \u2264 L t (w * ) \u2264 L(w * ) Likewise, since L() is the original cost function that we wish to minimize, when w t = w * , L(w * ) \u2264 L(w t ) Therefore, L t (w t ) \u2264 L t (w * ) \u2264 L(w * ) \u2264 L(w t )", "publication_ref": ["b11", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "Online Cutting Plane Algorithm", "text": "Note that computing g(w t ) in step 2 of Sect. 5.2 requires knowledge of H * i for all the N images in the training set. For large N , this is inefficient both, in terms of the number of computations required, as well as the memory needed to store all {H * i } before the next true subgradient can be computed. This is in contrast to many online optimization techniques like perceptrons and stochastic gradient descent that are able to learn a reasonably good model without having to make a complete pass through the entire dataset. Motivated by such techniques, we observe that one can construct a cutting plane with a partial subgradient computed from a small number of examples n N . We define a partial gradient g(w j ), bias b(w j ), and loss L(w) computed on n examples as follows:\ng (n) (w j ) = \u2212 n i=1 \u03c0 i (w j )\u0394\u03a8 (X i , Y i , H * i ) b (n) (w j ) = n i=1 \u03c0 i (w j ) L (n) (w) = 1 2 w 2 + CR (n) (w) R (n) (w) = n i=1 max(0, l(Y i , H * i ) \u2212 w T \u0394\u03a8 (X i , Y i , H * i ))\nWe modify the standard cutting plane approach as follows: Initialize t = 0 and the set of cutting planes to be empty. Iterate:\n1. Identical to step 1 in Sect. 5.2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Iterate through the data in any order until one collects", "text": "n examples for which L (n) (w t ) \u2212 L t (w t ) > . Add in the partial cutting plane w T g (n) (w t ) + b (n) (w t ), and goto\nStep 1. If the condition is never met, stop.\nBecause L (n) (w t ) \u2264 L(w t ), we have that L (n) (w t ) \u2212 L t (w t ) > implies L(w t ) \u2212 L t (w t ) > . This means we only need n examples to discover that w t is not -optimal. Once we discover this, we construct w T g (n) (w t ) + b (n) ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "(w t ).", "text": "This cutting plane is a lower-bound to R(w). However, because we have not used all N examples, it is no longer tight. Hence L t (w t ) \u2264 L * . If we cannot find N examples that violate -optimality, then L (N ) (w t ) = L(w t ) and w t is optimal.\nDuring initial iterations of the algorithm, updates occur very frequently. This is because a single example often suffices to discover that w t is not -optimal. Towards later iterations, when w t is more accurate, updates are less common because n will need to be large to trigger a tolerance violation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Finding Most-Violated Constraint", "text": "In step (2) of Sect. 5.3, we need to compute the partial subgradient of R(w) at the current w t . To do so, we need to compute the most violated constraint H * i for an image i in (11). Dropping the i subscript notation, we can rewrite (11) as\nH * = arg max H l(Y, H ) + w T \u03a8 (X, H ) = arg max H i,j w T h i ,h j d ij + i (w T h i x i + l(h i , y i ))\nSince the loss function decomposes into a sum over windows, solving for H * is very similar to the original maximization (1) except that the local match costs have been augmented by the loss function. Using the loss function in (8), the local scores for invalid object labels for a given window are incremented by one. This makes these labels more attractive in the maximization, and so they are more likely to be included in the most-violated constraint H * . We can compute an approximation to H * with a greedy forward search as in Sect. 3.1. Our algorithm is an under-generating approximation (Finley and Joachims 2008), so there are not formal guarantees optimality. However, as stated in Sect. 3.1, greedy forward search tends to produce scores similar to the brute-force solution, and so we suspect our solutions are close to optimal. A detailed empirical evaluation of our greedy approach is presented in Sect. 7.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "We have focused our experimental results for multiclass object recognition on the PASCAL Visual Object Challenge. It is widely regarded as the most difficult available benchmark for recognition. We use the 2007 data which is the latest for which test annotations are available. The data consists of 10000 images spanning 20 object classes with a 50% testtrain split. The images are quite varied, making this an especially difficult testbed for high-level contextual reasoning.\nBaseline: State-of-the-art approaches tend to be scanning window detectors (Everingham et al. 2007). We use the publicly available code (Felzenszwalb 2008) as a baseline. It implements a intra-class NMS post-processing step. The code is an improved version of Felzenszwalb et al. (2008) that out-scores many of the previous best performers from the 2007 competition, suggesting it is a strong baseline for comparison.\nPer-class scores: We follow the VOC protocol for reporting results (Everingham et al. 2007). A putative detection is considered correct if the intersection of its bounding box with the ground-truth bounding box is greater than 50% of their union. Multiple detections for the same ground-truth are considered false positives. We compute Precision-Recall (PR) curves and score the average precision (AP) across classes in Table 2. For twelve of the twenty classes, we achieve the best score when compared to the 2007 competition and the baseline model. We also compare to a version of Felzenszwalb et al. (2008) in which detections from multiple classes are pooled before apply-  (Everingham et al. 2007). We show the winning score from the 2007 challenge in the first data column. This column is composed of various state-of-the-art recognition algorithms. The second column is our baseline obtained by running the code from (Felzenszwalb 2008). It outperforms many of the 2007 entries, suggesting it is a strong baseline for comparison. The third column pools detections across multiple classes before applying NMS procedure from (Felzenszwalb 2008) (MC-NMS). The third column is our approach, which provides a stark improvement over MC-NMS and generally improves performance over classification-trained approaches", "publication_ref": ["b8", "b9", "b9", "b8", "b9", "b8", "b9", "b9"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Class", "text": "Baseline  (Felzenszwalb 2008) Fig. 5 We visualize the weights for our overlap threshold across all our models. Light areas correspond to an increase in score. The structure in these weights indicate the subtlety required for applying mutual exclusion across classes. For example, because people and bottles have similar shapes, the local detectors we use (Felzenszwalb et al. 2008) can confuse them. Our global model learns to strongly compete such overlapping detections using a negative weight. However, people and sofas tend to overlap because people partially occlude sofas when sitting down. In this case, we learn a positive weight that reinforces both detections ing NMS (MC-NMS). This tends to hurt performance, indicating the need for proper training of multiclass inhibition. The improvement over MC-NMS is generally large.\nIn most cases, the improvement over the baseline is small, but for indoor classes such as tables and bottles and outdoor classes such as motorbikes and trains, the improvement is close to 10%. Multi-class scores: Per-class APs do not score the consistency of detections across classes on an image, which is one of our goals for multi-class recognition. We consider two approaches for multiclass scores in Fig. 4. First we pool detections across classes and images (running the default NMS procedure in Felzenszwalb (2008) before pooling), and generate a single PR curve. Our model provides a noticeable improvement, particularly in the high precision-low recall regime. We also pool detections on a per image bases, generating a per-image multi-class AP. We average this AP across all images. Our model again provided a strong improvement of 10% over the baseline. This is because the baseline does not correctly reconcile detections from various classes due to the fact that the detectors were trained independently.\nModels: We visualize the pairwise weights learned in our models in both Figs. 5 and 6. These are trained discriminatively, taking into account the behavior of the local detector. For example, our model learns to aggressively compete bottle and person detections because local detectors confuse the two. This is contrast to simple co-occurrence weights that are trained by frequency counting as in Galleguillos et al.\nFig. 6 We visualize the pairwise spatial weights for each pair of classes as a 5 \u00d7 5 image (analogous to Fig. 3). Light areas indicate a favorable arrangement. We show a closeup for particular relations from classes where the global model helps performance. On the top, we see that bottles tend to sit above tables. In the middle, cars lie both near and far from trains, but rarely above or directly next to them. On the bottom, we see that motorbikes tend to occur next to one another in images Fig. 7 Example test images. On the top row, we show the top 10 detections from the baseline model after standard NMS. On the bottom row, we show the top 10 marginal detections from our global model. On the left, we see that horse and person detections are better localized by the globally tuned NMS model. In the left center, our model seems to favor patterns of chair detections that overlap, as maybe common in scenes of tables. In the right center, our model exploits co-occurrence cues favoring groups of animals. Finally, on the right, our model appears to be exploiting relational cues about sofas and people while enforcing mutual exclusion between the bottle and people detections (2008), Baur et al. (2008). We also learn meaningful multiclass spatial layouts-e.g., bottles tend to occur above tables. We refer the reader to the captions for additional analysis. Figure 7 shows example multi-class detections from our model as compared to the baseline. Our model appears to produce better detections by understanding interactions between objects that spatially overlap, such as people when riding horses. It also learns how to correctly enforce mutual exclusion between classes, allowing people and sofas to overlap but not people and bottles.\nDoes context help? Our results suggest that the benefit from using context to improve per-class AP is only marginal on PASCAL. We provide a couple of hypotheses as to why this is so:\n1. Contextual layout models are better suited for images with multiple objects. The PASCAL dataset is somewhat impoverished as far as presence of sufficient interclass and intra-class context is concerned. The PASCAL dataset contains 20 object classes. However, more than half the images contain only a single object class with two instances of that object class typically present in the image. We agree with the sentiment from Choi et al. (2010) that \"contextual information is most useful when many different object categories are present simultaneously in an image, with some object instances that are easy to detect (i.e. large objects) and some instances that are hard to detect (i.e. small objects).\" Along similar lines, Park et al. (2010) suggest that context provides a stronger improvement for detecting small objects rather than large objects. We hypothesize that our models may similarly exhibit a stronger improvement on datasets containing such variety. 2. Context is more useful for higher-level semantic tasks.\nOur baseline local detectors are state-of-the-art models that have consistently produced competitive results on PASCAL in terms of per class AP. We believe that contextual reasoning may only provide limited improvement over highly-tuned local detectors when scored for tasks such as object detection and localization. This view is corroborated by other empirical evaluations of context using tuned local detectors (Divvala et al. 2009) and (Galleguillos et al. 2008). However, we argue that context is helpful for higher level semantic inferences such as scene or action understanding. In the extreme case, given a perfect person and bottle detector, context cannot improve detection performance of either class. But even given such perfect detectors, one still requires a contextual layout model to recognize \"drinking\" actions because people and bottles must be simultaneously found in particular spatial relationships.\nOur per-image AP and overall AP scores partially validate the second hypothesis. Per-image AP scores can be interpreted as a loose proxy for a holistic understanding of a \"scene\" since one must reconcile detections across multiple classes simultaneously. Under this criteria, our model does improve AP from 37% to 40%, which is noticeably stronger than the 1% improvement in overall AP. In subsequent work (Desai et al. 2010), we further investigate the effect of contextual layout models on the high-level semantic task of action recognition. We demonstrate that the contextual models developed here can be used to increase the accuracy of a static-image action classifier by 12%. Notably, this increase is obtained over a baseline using the exact same state-ofthe-art local detectors used here. Hence we believe that our contextual layout model is more rewarding when used for higher level semantic tasks.", "publication_ref": ["b9", "b9", "b9", "b1", "b3", "b25", "b7", "b12", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Analysis of Greedy Inference", "text": "We compare our greedy inference algorithm to two other approximate inference approaches: Loopy Belief Propagation (LBP) and Tree Re-Weighted Belief Propagation (TRW) (Wainwright et al. 2002;Kolmogorov 2006). Although LBP has been widely used for approximate inference in graphical models with cycles, LBP is not guaranteed to converge and is susceptible to getting trapped at non-optimal fix points.\nIn the TRW approach, the original MAP problem is initially formulated as an integer program, whose binary constraints are \"relaxed\" to give a linear program (LP). The TRW algorithm is a variant of Belief Propagation that solves the resulting LP and has been shown to be significantly faster for this problem structure than off-the-shelf LP solvers (Yanover and Meltzer 2006). The solution given by TRW provides an upper bound on the solution of our score maximization problem. Notably, if the solution to the LP relaxation is integral, then the bound is tight and the solution is guaranteed to be a global optimum. We took the model learned using the approach discussed in Sect. 5 and ran the 3 approximation techniques: Greedy, LBP and TRW on the test set comprising 4952 images from PASCAL VOC 2007 dataset. We used the publicly available software from Meltzer (2006) for LBP and the software from MSR (2006) for TRW-S. 2 Table 3 compares the approximation techniques in terms of how well they maximize the score function, and their accuracy on PASCAL 2007 test set. For 4942 out of 4952 images (99.8%), Greedy and LBP yield identical results. More importantly, for 4832 of the 4952 images (97.6%), all the three schemes produce identical labels. For these cases, we verified that TRW-S produces integer solutions. This means that greedy produces the provably globally optimal solution in almost all images, while being two orders of magnitude faster than either approach. One theoretical explanation for the near-optimal performance of the greedy search procedure comes from the study of maximizing sub-modular set functions. While such problems are NP hard in general, simple greedy heuristics can be shown to have strong approximation guarantees (Nemhauser et al. 1978). If the pairwise weights are all \u22640, then one can show that S(X, Y ) from ( 1) is a submodular set function because it satisfies the diminishing returns property: consider two sets of instanced windows I 1 and I 2 , where I 1 \u2286 I 2 , and a particular un-instanced window i. The increase in S(X, Y ) due to instancing i must be smaller for I 2 because all pairwise interactions are negative. This means that greedy inference algorithms enjoy strong theoretical guarantees for contextual layout models with solely negative interactions. In practice, we observe that 90% of all the pairwise weights associated with the model trained on PASCAL 2007 images are \u22640, which makes S(X, Y ) close to sub-modular and our greedy maximization algorithm theoretically wellmotivated.", "publication_ref": ["b35", "b17", "b36", "b21", "b24"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Discussion and Related Work", "text": "There has been a wide variety of work in the last few years on contextual modeling in image parsing (Torralba et al. 2004;Sudderth et al. 2005;Hoiem et al. 2008;Galleguillos et al. 2008;Shotton et al. 2006;He et al. 2004;Anguelov et al. 2005). These approaches have typically treated the problem as that of finding a joint labeling for a set of pixels, super-pixels, or image segments and are usually formulated as a CRF. Such CRFs for pixel/segment labeling use singleton potential features that capture local distributions of color, textons, or visual words. Pairwise potentials incorporate the labelings of neighboring pixels but in contrast to older work on MRFs these pairwise potentials may span a very large set of neighboring sites (e.g. Torralba et al. 2004;Tu 2008). Learning such complicated potentials is a difficult problem and authors have relied primarily on boosting (Shotton et al. 2006;Torralba et al. 2004;Tu 2008) to do feature selection in a large space of possible potential functions.\nThese approaches are appealing in that they can simultaneously produce a segmentation and detection of the objects in a scene. Thus they automatically enforce NMS and hard mutual exclusion (although as our examples show, this may not be entirely desirable). However, the discriminative power of these methods for detection seems limited. While local image features work for some object classes (grass, sky etc.), a clear difficulty with the pixel/segment labeling approach is that it is hard to build features for objects defined primarily by shape. It still remains to be shown whether such approaches are competitive with scanning window templates on object detection benchmarks.\nIn principle, one could define unary potentials for CRFs using, say, HOG templates centered on individual pixels. However, the templates must score well when centered on every pixel within a particular segment. Thus templates will tend to be overly-smoothed. Our method is fundamentally different in that the output is sparse. A complete object detection is represented by the activation of a single pixel and so the unary potential can be quite strong. Furthermore, a detection in our model represses detections corresponding to small translations while, in the pixel labeling model, exactly the opposite has to happen. We thus make a tradeoff, moving to more powerful discriminative unary features but sacrificing tractable pairwise potentials.\nAlternatively, (Galleguillos et al. 2008;Kumar and Hebert 2005) group pixels into object-sized segments and then define a CRF over the labels of the segments. This approach has the advantage that unary potentials can now be defined with object templates, say, centered on the segment. However, the initial segmentation must be fairly accurate and enforces NMS and mutual exclusion without objectlevel layout models.\nTo our knowledge, the problem of end-to-end learning of multi-object detection (i.e. learning NMS) has not been explored. The closest work we know of is that of Blaschko and Lampert (2008) who use structured regression to predict the bounding box of a single detection within an image. Both models are trained using images rather an cropped windows. Both are optimized using the structural SVM formalism of Tsochantaridis et al. (2004). However, the underlying assumptions and resulting models are quite different. In the regression formalism of Blaschko and Lampert (2008), one assumes that each training image contains a single object instance, and so one cannot leverage information about the layout of multiple object instances, beit from the same class or not. The models may not perform well on images without the object because such images are never encountered during training. In our model, we can use all bounding-box labels from all training images, including those that do not contain any object, to train a model that will predict those very labels.", "publication_ref": ["b31", "b29", "b15", "b12", "b28", "b14", "b0", "b31", "b33", "b28", "b31", "b33", "b12", "b18", "b2", "b32", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We have presented a system for multi-class object detection with spatial interactions that can be efficiently trained in a discriminative, end-to-end manner. This approach is able to fuse the outputs of state of the art template based object detectors with information about contextual relations between objects. Rather than resorting to post-processing to clean up detections, our model learns optimal non-max suppression parameters and detection thresholds for each class. The resulting system outperforms published results on the PAS-CAL VOC 2007 object detection dataset.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Discriminative learning of Markov random fields for segmentation of 3d scan data", "journal": "", "year": "2005", "authors": "D Anguelov; B Taskar; V Chatalbashev; D Koller; D Gupta; G Heitz; A Ng"}, {"ref_id": "b1", "title": "Statistics of 3d object locations in images", "journal": "", "year": "2008", "authors": "R Baur; A A Efros; M Hebert"}, {"ref_id": "b2", "title": "Learning to localize objects with structured output regression", "journal": "Springer", "year": "2008", "authors": "M B Blaschko; C H Lampert"}, {"ref_id": "b3", "title": "Exploiting hierarchical context on a large database of object categories", "journal": "", "year": "2010", "authors": "M Choi; J Lim; A Torralba; A Willsky"}, {"ref_id": "b4", "title": "Histograms of oriented gradients for human detection", "journal": "", "year": "2005", "authors": "N Dalal; B Triggs"}, {"ref_id": "b5", "title": "Discriminative models for multi-class object layout", "journal": "", "year": "2009", "authors": "C Desai; D Ramanan; C Fowlkes"}, {"ref_id": "b6", "title": "Discriminative models for static human-object interactions", "journal": "", "year": "2010", "authors": "C Desai; D Ramanan; C Fowlkes"}, {"ref_id": "b7", "title": "An empirical study of context in object detection", "journal": "", "year": "2009", "authors": "S Divvala; D Hoiem; J Hays; A Efros"}, {"ref_id": "b8", "title": "The PASCAL visual object classes challenge 2007 (VOC2007) results", "journal": "", "year": "2007", "authors": "M Everingham; L Van Gool; C K I Williams; J Winn; A Zisserman"}, {"ref_id": "b9", "title": "A discriminatively trained, multiscale, deformable part model", "journal": "", "year": "2008", "authors": "P Felzenszwalb; P Felzenszwalb; D Mcallester; D Ramanan"}, {"ref_id": "b10", "title": "Training structural svms when exact inference is intractable", "journal": "ACM", "year": "2008", "authors": "T Finley; T Joachims"}, {"ref_id": "b11", "title": "", "journal": "", "year": "2006", "authors": "V Franc"}, {"ref_id": "b12", "title": "Object categorization using co-occurrence, location and appearance", "journal": "", "year": "2008", "authors": "C Galleguillos; A Rabinovich; S Belongie"}, {"ref_id": "b13", "title": "The hidden dimension", "journal": "Anchor Books", "year": "1966", "authors": "E Hall"}, {"ref_id": "b14", "title": "Multiscale conditional random fields for image labeling", "journal": "IEEE Comput. Soc", "year": "2004", "authors": "X He; R Zemel; M Carreira-Perpinan"}, {"ref_id": "b15", "title": "Putting objects in perspective", "journal": "IJCV", "year": "2008", "authors": "D Hoiem; A Efros; M Hebert"}, {"ref_id": "b16", "title": "Cutting plane training of structural SVMs", "journal": "", "year": "2009", "authors": "T Joachims; T Finley; C Yu"}, {"ref_id": "b17", "title": "Convergent tree-reweighted message passing for energy minimization", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2006", "authors": "V Kolmogorov"}, {"ref_id": "b18", "title": "A hierarchical field framework for unified context-based classification", "journal": "", "year": "2005", "authors": "S Kumar; M Hebert"}, {"ref_id": "b19", "title": "Combined object categorization and segmentation with an implicit shape model", "journal": "", "year": "2004", "authors": "B Leibe; A Leonardis; B Schiele"}, {"ref_id": "b20", "title": "Near-regular texture analysis and manipulation", "journal": "ACM Transactions on Graphics", "year": "2004", "authors": "Y Liu; W Lin; J Hays"}, {"ref_id": "b21", "title": "", "journal": "", "year": "2006", "authors": "T Meltzer"}, {"ref_id": "b22", "title": "", "journal": "", "year": "2006", "authors": " Msr"}, {"ref_id": "b23", "title": "Using the forest to see the trees: a graphical model relating features, objects and scenes", "journal": "", "year": "2003", "authors": "K Murphy; A Torralba; W Freeman"}, {"ref_id": "b24", "title": "An analysis of approximations for maximizing submodular set functions", "journal": "Mathematical Programming", "year": "1978", "authors": "G Nemhauser; L Wolsey; M Fisher"}, {"ref_id": "b25", "title": "Multiresolution models for object detection", "journal": "", "year": "2010", "authors": "D Park; D Ramanan; C Fowlkes"}, {"ref_id": "b26", "title": "Optimizing binary mrfs via extended roof duality", "journal": "", "year": "2007", "authors": "C Rother; V Kolmogorov; V Lempitsky; M Szummer"}, {"ref_id": "b27", "title": "Neural network-based face detection", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "1996", "authors": "H A Rowley; S Baluja; T Kanade"}, {"ref_id": "b28", "title": "Textonboost: joint appearance, shape and context modeling for multi-class object recognition and segmentation", "journal": "", "year": "2006", "authors": "J Shotton; J Winn; C Rother; A Criminisi"}, {"ref_id": "b29", "title": "Learning hierarchical models of scenes, objects, and parts", "journal": "", "year": "2005", "authors": "E Sudderth; A Torralba; W Freeman; A Willsky"}, {"ref_id": "b30", "title": "A scalable modular convex solver for regularized risk minimization", "journal": "ACM", "year": "2007", "authors": "C Teo; A Smola; S Vishwanathan; Q Le"}, {"ref_id": "b31", "title": "Contextual models for object detection using boosted random fields. NIPS", "journal": "", "year": "2004", "authors": "A Torralba; K Murphy; W Freeman"}, {"ref_id": "b32", "title": "Support vector machine learning for interdependent and structured output spaces", "journal": "ACM", "year": "2004", "authors": "I Tsochantaridis; T Hofmann; T Joachims; Y Altun"}, {"ref_id": "b33", "title": "Auto-context and its application to high-level vision tasks", "journal": "", "year": "2008", "authors": "Z Tu"}, {"ref_id": "b34", "title": "Robust real-time face detection", "journal": "IJCV", "year": "2004", "authors": "P A Viola; M J Jones"}, {"ref_id": "b35", "title": "Map estimation via agreement on (hyper)trees: message-passing and linear programming approaches", "journal": "IEEE Transactions on Information Theory", "year": "2002", "authors": "M Wainwright; T Jaakkola; A Willsky"}, {"ref_id": "b36", "title": "Linear programming relaxations and belief propagation-an empirical study", "journal": "", "year": "2006", "authors": "C Yanover; T Y W Meltzer"}], "figures": [{"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Per-class AP scores on PASCAL 2007", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Multi-class AP scores on PASCAL 2007. On the left, we score overall AP. We construct the baseline curve by pooling detections across classes and images when computing PR curves. Our global model clearly provides a noticeable boost in performance in the low-recall high-precision regime. On the right, we pool detections on a perimage base, compute the per-image AP, and average the result over images. We see a noticeable improvement of 10% over our baseline", "figure_data": "Fig. 4MC-NMSOur modelPlane.2620.2780.2700.288Bike.4090.5590.4440.562Bird.0980.0140.0150.032Boat.0940.1460.1250.142Bottle.2140.2570.1850.294Bus.3930.3810.2990.387Car.4320.4700.4660.487Cat.2400.1510.1330.124Chair.1280.1630.1450.160Cow.1400.1670.1090.177Table.0980.2280.1910.240Dog.1620.1110.0910.117Horse.3350.4380.3710.450Motbike.3750.3730.3250.394Person.2210.3520.3420.355Plant.1200.1400.0910.152Sheep.1750.1690.0910.161Sofa.1470.1930.1880.201Train.3340.3190.3180.342TV.2890.3730.3590.354"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": "comparing the average energy, precision and recallacross different approximation techniquesAv. scoreAv. precAv. recallGreedy1.1740.79390.4673TRW-S1.1850.7710.4707LBP1.1850.7710.4707"}], "formulas": [{"formula_id": "formula_0", "formula_text": "S(X, Y ) = i,j w T y i ,y j d ij + i w T y i x i (1)", "formula_coordinates": [3.0, 50.83, 497.89, 238.11, 22.9]}, {"formula_id": "formula_1", "formula_text": "\u0394(i, c) = S(X, Y (I \u222a {(i, c)})) \u2212 S", "formula_coordinates": [4.0, 50.83, 128.08, 139.45, 9.96]}, {"formula_id": "formula_2", "formula_text": "1. (i * , c * ) = arg max (i,c) \u2208I \u0394(i, c) 2. I = I \u222a {(i * , c * )} 3. S = S + \u0394(i * , c * ) 4. \u0394(i, c) = \u0394(i, c) + w T c * ,c d i * ,i + w T c,c * d i,i * until \u0394(i * , c * ) < 0", "formula_coordinates": [4.0, 50.83, 167.54, 178.13, 67.51]}, {"formula_id": "formula_3", "formula_text": "m(y i = c) = log P (y i = c|X) P (y i = c|X) = log y r P (y i = c, y r |X) y s ,c =c P (y i = c , y s |X)(2)", "formula_coordinates": [4.0, 50.83, 583.51, 238.11, 58.88]}, {"formula_id": "formula_4", "formula_text": "y * r = arg max y r S(X, y i = c, y r ) (y * s , c * ) = arg max (y s ,c =c) S(X, y i = c , y s ) (3)", "formula_coordinates": [4.0, 307.26, 112.86, 236.72, 44.0]}, {"formula_id": "formula_5", "formula_text": "m(y i = c) \u2248 log P (y i = c, y r * |X) P (y i = c * , y s * |X) = S(X, y i = c, y r * ) \u2212 S(X, y i = c * , y s * )", "formula_coordinates": [4.0, 305.87, 199.7, 202.65, 42.38]}, {"formula_id": "formula_6", "formula_text": "S(X, Y ) = i,j w T s \u03c8(y i , y j , d ij ) + i w T a \u03c6(x i , y i ) (4)", "formula_coordinates": [4.0, 305.87, 425.09, 238.11, 22.88]}, {"formula_id": "formula_7", "formula_text": "w = w s w a , \u03a8(X,Y)= ij \u03c8(y i , y j , d ij ) i \u03c6(x i , y i ) (5)", "formula_coordinates": [4.0, 305.87, 570.79, 238.11, 23.97]}, {"formula_id": "formula_8", "formula_text": "Y * = arg max Y w T \u03a8 (X, Y ) (6)", "formula_coordinates": [4.0, 305.87, 624.33, 238.11, 18.33]}, {"formula_id": "formula_9", "formula_text": "arg min w,\u03be i \u22650 w T w + C i \u03be i s.t. \u2200i, H i w T \u0394\u03a8 (X i , Y i , H i ) \u2265 l(Y i , H i ) \u2212 \u03be i (7)", "formula_coordinates": [5.0, 52.21, 78.34, 236.72, 41.76]}, {"formula_id": "formula_10", "formula_text": "l 01 (y i , h i ) = I (y i = h i )", "formula_coordinates": [5.0, 50.83, 293.82, 94.04, 10.61]}, {"formula_id": "formula_11", "formula_text": "l ov (y i , h i ) = \u23a7 \u23aa \u23aa \u23a8 \u23aa \u23aa \u23a9 1: y i = bg \u2227 h i = y i 1: h i = bg \u2227 \u00ac\u2203j s.t. [ov(i, j ) > .5 \u2227 y j = h i ] 0: otherwise (8)", "formula_coordinates": [5.0, 50.83, 393.11, 238.11, 54.72]}, {"formula_id": "formula_12", "formula_text": "w * = arg min w L(w) where L(w) = 1 2 w 2 + CR(w) R(w) = N i=1 max H (0, l(Y i , H ) \u2212 w T \u0394\u03a8 (X i , Y i , H )) (9)", "formula_coordinates": [5.0, 50.83, 623.85, 238.11, 60.51]}, {"formula_id": "formula_14", "formula_text": "g(w j ) = \u2212 N i=1 \u03c0 i \u0394\u03a8 (X i , Y i , H * i ) \u03c0 i = 1 if l(Y i , H * i ) \u2212 w T j \u0394\u03a8 (X i , Y i , H * i ) \u2265 0 0 otherwise H * i = arg max H l(Y i , H ) \u2212 w T \u0394\u03a8 (X i , Y i , H ) (11)", "formula_coordinates": [5.0, 307.26, 269.48, 236.72, 88.69]}, {"formula_id": "formula_15", "formula_text": "arg min w,\u03be >0 1 2 w 2 + C\u03be s.t. w \u2022 g(w i ) + b i \u2264 \u03be, \u2200i = 1, . . . , t", "formula_coordinates": [5.0, 305.87, 583.28, 161.87, 41.97]}, {"formula_id": "formula_16", "formula_text": "L(w, \u03be, \u03b1, \u03bc) = 1 2 w 2 + C\u03be + t i=1 \u03b1 i (w \u2022 g(w i ) + b i \u2212 \u03be) \u2212 \u03bc\u03be", "formula_coordinates": [5.0, 307.26, 666.82, 216.57, 48.96]}, {"formula_id": "formula_17", "formula_text": "\u2202L \u2202w = 0 =\u21d2 w = t i=1 \u03b1 i g(w i ) \u2202L \u2202\u03be = 0 =\u21d2 C \u2265 t i=1 \u03b1 i", "formula_coordinates": [6.0, 52.02, 89.88, 143.43, 69.84]}, {"formula_id": "formula_18", "formula_text": "arg max \u03b1>0 \u2212 1 2 t i=1 t j =1 \u03b1 i g(w i ) T g(w j )\u03b1 j s.t. t i=1 \u03b1 i \u2264 C", "formula_coordinates": [6.0, 50.83, 204.95, 152.42, 71.26]}, {"formula_id": "formula_19", "formula_text": "g (n) (w j ) = \u2212 n i=1 \u03c0 i (w j )\u0394\u03a8 (X i , Y i , H * i ) b (n) (w j ) = n i=1 \u03c0 i (w j ) L (n) (w) = 1 2 w 2 + CR (n) (w) R (n) (w) = n i=1 max(0, l(Y i , H * i ) \u2212 w T \u0394\u03a8 (X i , Y i , H * i ))", "formula_coordinates": [6.0, 305.87, 409.85, 223.71, 134.57]}, {"formula_id": "formula_20", "formula_text": "H * = arg max H l(Y, H ) + w T \u03a8 (X, H ) = arg max H i,j w T h i ,h j d ij + i (w T h i x i + l(h i , y i ))", "formula_coordinates": [7.0, 50.83, 298.28, 210.43, 49.18]}], "doi": "10.1007/s11263-011-0439-x"}