{"title": "CNM: An Interpretable Complex-valued Network for Matching", "authors": "Qiuchi Li; Benyou Wang; Massimo Melucci", "pub_date": "", "abstract": "This paper seeks to model human language by the mathematical framework of quantum physics. With the well-designed mathematical formulations in quantum physics, this framework unifies different linguistic units in a single complex-valued vector space, e.g. words as particles in quantum states and sentences as mixed systems. A complex-valued network is built to implement this framework for semantic matching. With well-constrained complex-valued components, the network admits interpretations to explicit physical meanings. The proposed complex-valued network for matching (CNM) 1 achieves comparable performances to strong CNN and RNN baselines on two benchmarking question answering (QA) datasets.", "sections": [{"heading": "Introduction", "text": "There is a growing concern on the interpretability of neural networks. Along with the increasing power of neural networks comes the challenge of interpreting the numerical representation of network components into human-understandable language. Lipton (2018) points out two important factors for a model to be interpretable, namely post-hoc interpretability and transparency. The former refers to explanations of why a model works after it is executed, while the latter concerns self-explainability of components through some mechanisms in the designing phase of the model.\nWe seek inspirations from quantum physics to build transparent and post-hoc interpretable networks for modeling human language. The emerging research field of cognition suggests that there exist quantum-like phenomena in human cognition (Aerts and Sozzo, 2014), especially language understanding (Bruza et al., 2008). Intuitively, a sentence can be treated as a physical system with multiple words (like particles), and these words are usually polysemous (superposed) and correlated (entangled) with each other. Motivated by these existing works, we aim to investigate the following Research Question (RQ).\nRQ1: Is it possible to model human language with the mathematical framework of quantum physics?\nTowards this question, we build a novel quantum-theoretic framework for modeling language, in an attempt to capture the quantumness in the cognitive aspect of human language. The framework models different linguistic units as quantum states with the adoption of quantum probability (QP), which is the mathematical framework of quantum physics that models uncertainly on a uniform Semantic Hilbert Space (SHS).\nComplex values are crucial in the mathematical framework of characterizing quantum physics. In order to preserve physical properties, the linguistic units have to be represented as complex vectors or matrices. This naturally gives rise to another research question:\nRQ2: Can we benefit from the complex-valued representation of human language in a real natural language processing (NLP) scenario?\nTo this end, we formulate a linguistic unit as a complex-valued vector, and link its length and direction to different physical meanings: the length represents the relative weight of the word while the direction is viewed as a superposition state. The superposition state is further represented in an amplitude-phase manner, with amplitudes corresponding to the lexical meaning and phases implicitly reflecting the higher-level semantic aspects such as polarity, ambiguity or emotion.\nIn order to evaluate the above framework, we implement it as a complex-valued network (CNM) for semantic matching. The network is applied to the question answering task, which is the most typical matching task that aims at selecting the best answer for a question from a pool of candidates. In order to facilitate local matching with ngrams of a sentence pair, we design a local matching scheme in CNM. Most of State-of-the-art QA models are mainly based on Convolution Neural Network (CNN), Recurrent Neural Network (RNN) and many variants thereof (Wang and Nyberg, 2015;Yang et al., 2016;Hu et al., 2014;Tan et al., 2015). However, with opaque structures of convolutional kernels and recurrent cells, these models are hard to understand for humans. We argue that our model is advantageous in terms of interpretability.\nOur proposed CNM is transparent in that it is designed in alignment with quantum physics. Experiments on benchmarking QA datasets show that CNM has comparable performance to strong CNN and RNN baselines, whilst admitting post-hoc interpretations to human-understandable language. We therefore answer RQ1 by claiming that it is possible to model human language with the proposed quantum-theoretical framework in this paper. Furthermore, an ablation study shows that the complex-valued word embedding performs better than its real counterpart, which allows us to answer RQ2 by claiming that we benefit from the complex-valued representation of natural language on the QA task.", "publication_ref": ["b10", "b0", "b1", "b24", "b25", "b7", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Background", "text": "Here we briefly introduce quantum probability and discuss a relevant work on quantum-inspired framework for QA.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Quantum Probability", "text": "Quantum probability provides a sound explanation for the phenomena and concepts of quantum mechanics, by formulating events as subspaces in a vector space with projective geometry.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Quantum Superposition", "text": "Quantum Superposition is one of the fundamental concepts in Quantum Physics, which describes the uncertainty of a single particle. In the micro world, a particle like a photon can be in multiple mutualexclusive basis states simultaneously with a probability distribution. In a two-dimensional example, two basis vectors are denoted as |0 and |1 2 .\nSuperposition is implemented to model a general state which is a linear combination of basis vectors with complex-valued weights such that\n|\u03c6 = \u03b1 0 |0 + \u03b1 1 |1 ,(1)\nwhere \u03b1 0 and \u03b1 1 are complex scalars satisfying\n0 \u2264 |\u03b1 0 | 2 \u2264 1, 0 \u2264 |\u03b1 1 | 2 \u2264 1 and |\u03b1 0 | 2 + |\u03b1 1 | 2 = 1.\nIt follows that |\u03c6 is defined over the complex field. When \u03b1 0 and \u03b1 1 are non-zero values, the state |\u03c6 is said to be a superposition of the states |0 and |1 , and the scalars \u03b1 0 and \u03b1 1 denote the probability amplitudes of the superposition.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Measurement", "text": "The uncertainty of an ensemble system with multiple particles is encapsulated as a mixed state, represented by a positive semi-definite matrix with unitary trace called density matrix: \u03c1 = m i |\u03c6 i \u03c6 i |, where {|\u03c6 i } m i=0 are pure states like Eq. 1. In order to infer the probabilistic properties of \u03c1 in the state space, Gleason's theorem (Gleason, 1957;Hughes, 1992) is used to calculate probability to observe x through projection measurements |x x| that is a rank-one projector denoted as a outer product of |x .\np x (\u03c1) = x| \u03c1 |x = tr(\u03c1 |x x|)(2)\nThe measured probability p x (\u03c1) is a nonnegative real-valued scalar, since both \u03c1 and |x x| are Hermitian. The unitary trace property guarantees x\u2208X p x (\u03c1) = 1 for X being a set of orthogonal basis states.", "publication_ref": ["b3", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Neural Network based Quantum-like Language Model (NNQLM)", "text": "Based on the density matrices representation for documents in information retrieval (Van Rijsbergen, 2004;Sordoni et al., 2013), Zhang et al. (2018a) built a neural network with density matrix for question answering. This Neural Network based Quantum Language Model (NNQLM) embeds a word as a unit vector and a sentence as a real-valued density matrix. The distance between a pair of density matrices is obtained by extracting features of their matrix multiplication in two ways: NNQLM-I directly takes the trace of the resulting matrix, while NNQLM-II applies convolutional structures on top of the matrix to determine whether the pair of sentences match or not. NNQLM is limited in that it does not make proper use of the full potential of probabilistic property of a density matrices.By treating density matrices as ordinary real vectors (NNQLM-I) or matrices (NNQLM-II), the full potential with complex-valued formulations is largely ignored. Meanwhile, adding convolutional layers on top of a density matrix is more of an empirical workaround than an implementation of a theoretical framework.\nIn contrast, a complex-valued matching network is built on top of a quantum-theoretical framework for natural language. In particular, an indirect way to measure the distance between two density matrices through trainable measurement operations, which makes advantage of the probabilistic properties of density matrices and also provides flexible matching score driven by training data.", "publication_ref": ["b21", "b17", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "Semantic Hilbert Space", "text": "Here we introduce the Semantic Hilbert Space H defined on a complex vector space C n , and three different linguistic units, namely sememes, words and word combinations on the space. The concept of semantic measurement is introduced at last.\nSememes. We assume H is spanned by the set of orthogonal basis states {|e j } n j=1 for sememes, which are the minimum semantic units of word meanings in language universals (Goddard and Wierzbicka, 1994). The unit state |e j can be seen as a one-hot vector, i.e., the j-th element in |e j is one while other elements are zero, in order to obtain a set of orthogonal unit states. Semantic units with larger granularities are based on the set of sememe basis.\nWords. Words are composed of sememes in superposition. Each word w is a superposition over all sememes {|e j } n j=1 , or equivalently a unitlength vector on H:\n|w = n j=1 r j e i\u03c6 j |e j ,(3)\ni is the imaginary number with i 2 = \u22121. In the above expression, {r j } n j=1 are non-negative real-valued amplitudes satisfying n j=1 r j 2 =1 and \u03c6 j \u2208 [\u2212\u03c0, \u03c0] are the corresponding complex phases. In comparison to Eq. 1, {r j e i\u03c6 j } n j=0 are the polar form representation of the complexvalued scalars {\u03b1 j } 1 j=0 . Word Combinations. We view a combination of words (e.g. phrase, n-gram, sentence or document) as a mixed system composed of individual words, and its representation is computed as follows:\n\u03c1 = m j 1 m |w j w j |,(4)\nwhere m is the number of words and |w j is word superposition state in Eq. 3, allowing multiple occurrences. Eq. 4 produces a density matrix \u03c1 for semantic composition of words. It also describes a non-classical distribution over the set of sememes: the complex-valued off-diagonal elements describes the correlations between sememes, while the diagonal entries (guaranteed to be real by its original property) correspond to a standard probability distribution. The off-diagonal elements provide our framework some potentials to model the possible interactions between the basic sememe basis, which was usually considered mutually independent with each other. Semantic Measurements. The high-level features of a sequence of words are extracted through measurements on its mixed state. Given a density matrix \u03c1 of a mixed state, a rank-one projector P , which is the outer product of a unit complex vector, i.e. P = |x x|, is applied as a measurement projector. It is worth mentioning that |x could be any pure state in this Hilbert space (not only limited to a specific word w). The measured probability is computed by Gleason's Theorem in Eq. 2.", "publication_ref": ["b4"], "figure_ref": [], "table_ref": []}, {"heading": "Complex-valued Network for Matching", "text": "We implemented an end-to-end network for matching on the Semantic Hilbert Space. Fig. 1 shows the overall structure of the proposed Complex-valued Network for Matching (CNM). Each component of the network is further discussed in this section.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Complex-valued Embedding", "text": "On the Semantic Hilbert Space, each word w is embedded as a complex-valued vector w. Here we link its length and direction to different physical meanings: the length of a vector represents the relative weight of the word while the vector direction is viewed as a superposition state. Each word w adopts a normalization into a superposition state |w and a word-dependent weight \u03c0(w):\n|w = w || w|| , \u03c0(w) = || w||,(5)\nwhere || w|| denotes the 2-norm length of w. \u03c0(w) is used to compute the relative weight of a word in ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sentence Modeling with Local Mixture Scheme", "text": "A sentence is modeled as a combination of individual words in it. NNQLM (Zhang et al., 2018a) models a sentence as a global mixture of all words, which implicitly assumes a global interaction among all sentence words. This seems to be unreasonable in practice, especially for a long text segment such as a paragraph or a document, where the interaction between the first word and the last word is often negligible. Therefore, we address this limitation by proposing a local mixture of words, which tends to capture the semantic relations between neighboring words and undermine the long-range word dependencies. As is shown in Fig. 2, a sliding window is applied and a density matrix is constructed for a local window of length l (e.g. 3). Therefore, a sentence is composed of a sequence of density matrices for l-grams.\nThe representation of a local l-gram window is obtained by an improved approach over Eq. 4. In Eq. 4, each word is assigned with the same weight, which does not hold from an empirical point of view. In this study, we take the L2-norm of the word vector as the relative weight in a local context window for a specific word, which could be updated during training. To some extent, L2-norm is a measure of semantic richness of a word, i.e. the longer the vector the richer the meaning. The means that a matrix multiplies a number with each element. denotes the outer product of a vector with itself. density matrix of an l-gram is computed as follows:\n\u03c1 = l i p(w i ) |w i w i |,(6)\nwhere the relative importance of each word p(w i ) in an l-gram is the soft-max normalized worddependent weight: p(w i ) = e \u03c0(w i ) l j e \u03c0(w j ) , where \u03c0(w i ) is the word-dependent weight. By converting word-dependent weights to a probability distribution, a legal density matrix is produced, because l i p(w i ) = 1 gives tr(\u03c1) = 1. Moreover, the weight of a word also depends on its neighboring words in a local context.", "publication_ref": ["b28"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Matching of Question and Answer", "text": "In quantum information, there have been works trying to estimate a quantum state from the results of a series of measurements (\u0158eh\u00e1\u010dek et al., 2001;Lvovsky, 2004). Inspired by these works, we introduce trainable measurements to extract density matrix features and match a pair of sentences.\nSuppose a pair of sentences with length L are represented as two sets of density matrices {\u03c1 1j } L j=1 and {\u03c1 2j } L j=1 respectively. The same set of K semantic measurement operators {|v k } K k=1 are applied to both sets, producing a pair of kby-L probability matrix p 1 and p 2 , where\np 1 jk = v k | \u03c1 1j |v k and p 2 jk = v k | \u03c1 2j |v k for k \u2208 {1, .\n.., K} and j \u2208 {1, ..., L}. A classical vectorbased distances between p 1 and p 2 can be computed as the matching score of the sentence pair. By involving a set of semantic measurements, the properties of density matrix are taken into consideration in computing the density matrix distance.\nWe believe that this way of computing density matrix distance is both theoretically sound and applicable in practice. The trace inner product of density matrices (Zhang et al., 2018a) breaks the basic axioms of metric, namely the non-negativity, identity of indiscernibles and triangle inequality. The CNN-based feature extraction (Zhang et al., 2018a) for density matrix multiplication loses the property of density matrix as a probability distribution. Nielsen and Chuang (2010) introduced three measures namely trace distance, fidelity, and VN-divergence. However, it is computationally costly to compute these metrics and propagate the loss in an end-to-end training framework.\nWe set the measurements to be trainable so that the matching of question and answering can be integrated into the whole neural network, and identify the discriminative semantic measurements in a data-driven manner. From the perspective of linear discriminant analysis (LDA) (Fisher, 1936), this approach is intended to find a group of finite discriminative projection directions for a better division of different classes, but in a more sound framework inspired by quantum probability with complex-valued values. From an empirical point of view, the data-driven measurements make it flexible to match two sentences.  ", "publication_ref": ["b11", "b28", "b28", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Datasets and Evaluation Metrics", "text": "The experiments were conducted on two benchmarking question answering datasets for question answering (QA), namely TREC QA (Voorhees and Tice, 2000) and WikiQA (Yang et al., 2015). TREC QA is a standard QA dataset in the Text REtrieval Conference (TREC). WikiQA is released by Microsoft Research on open domain question answering. On both datasets, the task is to select the most appropriate answer from the candidate answers for a question, which requires a ranking of candidate answers. After removing the questions with no correct answers, the statistics of the cleaned datasets are given in the Tab. 1. Two common rank-based metrics, namely mean average precision (MAP) and mean reciprocal rank (MRR), are used to measure the performance of models.", "publication_ref": ["b23", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "Experiment Details", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Baselines", "text": "We conduct a comprehensive comparison across a wide range of models. On TREC QA the experimented models include Bigram-CNN (Yu et al., 2014) (Miao et al., 2015).\nOn both datasets, we report the results of quantum language model (Sordoni et al., 2013) and two models NNQLM-I, NNQLM-II by (Zhang et al., 2018a) for comparison.", "publication_ref": ["b27", "b12", "b17", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "Parameter Settings", "text": "The parameters in the network are \u0398 = {R, \u03a6, {|v i } k i=1 }, in which R and \u03a6 denote the lookup tables for amplitudes and complex phases of each word, and {|v i } k i=1 denotes the set of semantic measurements. We use 50-dimension complex word embedding. The amplitudes are initialized with 50-dimension Glove vectors (Pennington et al., 2014) and L2-norm regularized during training. The phases are randomly initialized under a normal distribution of [\u2212\u03c0, \u03c0]. The semantic measurements {|v i } k i=1 } are initialized with orthogonal real-valued one-hot vectors, and each measurement is constrained to be of unit length during training. We perform max pooling over the sentence dimension on the measurement probability matrices, resulting in a k-dim vector for both a question and an answer. We concatenate the vectors for l = 1, 2, 3, 4 for questions and answers, and the larger size of windows are also tried. We will use a longer sliding window in datasets with longer sentences. The cosine similarity is used as the distance metric of measured probabilities. We use triplet hinge loss and set the margin \u03b1 = 0.1. A dropout layer is built over the embedding layer and measurement probabilities with a dropout rate of 0.9.\nA grid search is conducted over the parameter pools to explore the best parameters. The parameters under exploration include {0.01, 0.05, 0.1} for the learning rate, {1e \u2212 5, 1e \u2212 6, 1e \u2212 7, 1e \u2212 8} for the L2-normalization of complex word embeddings, {8, 16, 32} for batch size, and {50, 100, 300, 500} for the number of semantic measurements.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Parameter Scale", "text": "The proposed CNM has a limited scale of parameters. Apart from the complex word embeddings which are |V | \u00d7 2n by size, the only set of parameters are {|v i } k i=1 which is k \u00d7 2n, with |V |, k, n being the vocabulary size, number of semantic measurements and the embedding dimension, respectively. In comparison, a single-layered CNN has at least l \u00d7 k \u00d7 n additional parameters with l being the filter width, while a single-layered LSTM is 4 \u00d7 k \u00d7 (k + n) by the minimum parameter scale. Although we use both amplitude part and phase part for word embedding, lower dimensions of embedding are adopted, namely 50, with   the comparable performance.\nTherefore, our network scales better than the advanced models on the CNN or LSTM basis.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiment Results", "text": "Tab. 2 and 3 show the experiment results on TREC QA and WikiQA respectively, where bold values are the best performances out of all models. Our model achieves 3 best performances out of the 4 metrics on TREC QA and WikiQA, and performs slightly worse than the best-performed models on the remaining metric. This illustrates the effectiveness of our proposed model from a general perspective.\nSpecifically, CNM outperforms most CNN and LSTM-based models, which have more complicated structures and a relatively larger parameters scale. Also, CNM performs better than existing quantum-inspired QA models, QLM and NNQLM on both datasets, which means that the quantum theoretical framework gives rise to better performs model. Moreover, a significant improvement over NNQLM-1 is observed on these two datasets, supporting our claim that the trace inner product is not an effective distance metric of two density matrices.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ablation Test", "text": "An ablation test is conducted to examine the influence of each component on our proposed CNM. The following models are implemented in the ablation test. FastText-MaxPool adopt max pooling over word-embedding, just like FastText (Joulin et al., 2016). CNM-Real replaces word embeddings and measurements with their real counterparts. CNM-Global-Mixture adopts a global mixture of the whole sentence, in which a sentence is represented as a single density matrix, leading to a probability vector for the measurement result. CNM-trace-inner-product replaces the trainable measurements with trace inner product like NNQLM.\nFor the real-valued models, we replace the embedding with double size of dimension, in order to eliminate the impact of the parameter scale on the performance. Due to limited space, we only report the ablation test result on TREC QA, and Wik-iQA has similar trends. The test results in Tab. 4 demonstrate that each component plays a crucial role in the CNM model. In particular, the comparison with CNM-Real and FastText-MaxPool shows the effectiveness of introducing complexvalued components, the increase in performance over CNM-Global-Mixture reveals the superiority of local mixture, and the comparison with CNM-trace-inner-product confirms the usefulness of trainable measurements.", "publication_ref": ["b9"], "figure_ref": [], "table_ref": []}, {"heading": "Discussions", "text": "This section aims to investigate the proposed research questions mentioned in Sec 1. For RQ1, we explain the physical meaning of each component in term of the transparency (Sec. 6.1), and design some case studies for the post-hoc interpretability (Sec. 6.2). For RQ2, we argue that the complex-valued representation can model different aspects of semantics and naturally address the non-linear semantic compositionality, as discussed in Sec. 6.3.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Components DNN CNM", "text": "Sememe -basis one-hot vector / basis state {e|e \u2208 R n , ||e||2 = 1} complete &orthogonal Word real vector (\u2212\u221e, \u221e)\nunit complex vector / superposition state {w|w \u2208 C n , ||w||2 = 1} N-gram/ Word combinations real vector (\u2212\u221e, \u221e) density matrix / mixed system {\u03c1|\u03c1 = \u03c1 * , tr(\u03c1) = 1 Abstraction CNN/RNN (\u2212\u221e, \u221e) projector / measurement {vv T |v \u2208 C n , ||v||2 = 1} Sentence representation real vector (\u2212\u221e, \u221e)\nreal value/ measured probability (0, 1)  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Transparency", "text": "CNM aims to unify many semantic units with different granularity e.g. sememes, words, phrases (or N-gram) and document in a single complexvalued vector space, as shown in Tab. 5. In particular, we formulate atomic sememes as a group of complete orthogonal basis states and words as superposition states over them. A linguistic unit with larger-granularity e.g. a word phrase or a sentence is represented as a mixed system over the words (with a density matrix, i.e. a positive semi-definite matrix with unit trace).\nMore importantly, trainable projection measurements are used to extract high-level representation for a word phrase or a sentence. Each measurement is also directly embedded in this unified Hilbert space, as a specific unit state (like words), thus making it easily understood by the neighbor words near this specific state. The corresponding trainable components in state-of-art neural network architectures, namely, kernels in CNN and cells in RNN, are represented as arbitrary realvalued without any constraints, lead to difficulty to be understood.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Post-hoc Interpretability", "text": "The post-hoc Interpretability is shown in three groups of case studies, namely word weight scheme, matching pattern, and discriminative semantic measurements.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Word Weighting Scheme", "text": "Tab. 6 shows the words selected from the top-50 most important words as well as top-50 unimpor-  tant ones. The importance of words is based on the L2-norm of its learned amplitude embedding according to Eq. 5. It is consistent with the intuition that, the important words are more about specific topics or discriminative nouns, while the unimportant words include meaningless numbers or super-high frequency words. Note that some special form (e.g. plural form in the last row ) of words are also identified as unimportant words, since we commonly did not stem the words.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Matching Pattern", "text": "Tab. 7 shows the match schema with local sliding windows. In a local context window, we visualize the relative weights (i.e. the weights after normalized by softmax) for each word with darkness degrees. The table illustrates that our model is capable of identifying true matched local windows of a sentence pair. Even some words are replaced with similar forms (e.g. commit and committing in the last case) or meanings (e.g. change and new in the fourth case), it could be robust to get a relatively high matching score. From an empirical point of view, our model outperforms other models in situations where specific matching pattern are crucial to the sentence meaning, such as when two sentences share some unordered bag-of-word combinations. To some extent, it is robust up to replacement of words with similar ones in the Semantic Hilbert Space.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Discriminative Semantic Measurements", "text": "The semantic measurements are performed through rank-one projectors {|x x|} . From a classical point of view, each projector is associated with a superposition of fundamental sememes, which is not necessarily linked to a particular word. Since the similarity metric in the Semantic Hilbert Space can be used to indicate semantic relatedness, we rely on the nearby words of the learned measurement projectors to understand what they may refer to. Essentially, we identified the 10 most similar words to a measurement based on the cosine sim-Selected neighborhood words for a measurement vector 1 andes, nagoya, inter-american, low-caste 2 cools, injection, boiling,adrift 3 andrews, paul, manson, bair 4 historically, 19th-century, genetic, hatchback 5 missile, exile, rebellion, darkness ilarity metric. Tab. 8 shows part of the most similar words of 5 measurements, which are randomly chosen from the total number of k=10 trainable measurements for the TREC QA dataset. It can be seen that the first three selected measurements were about positions, movement verbs, and people's names, while the rest were about the topic of history and rebellion respectively. Even though a clear explanation of the measurements is not available, we are still able to roughly understand the meaning of the measurements in the proposed data-driven approach.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Complex-valued Representation", "text": "In CNM, each word is naturally embedded as a complex vector, composed of a complex phase part, a unit amplitude part, and a scalar-valued length. We argue that the amplitude part (i.e. the squared root of a probabilistic weight), corresponds to the classical word embedding with the lexical meaning, while the phase part implicitly reflects the higher-level semantic aspect e.g. polarity, ambiguity or emotion. The scalar-valued length is considered as the relative weight in a mixed system. The ablation study in Sec. 5.4 confirms that the complex-valued word embedding performs better than the real word embedding, which indicates that we benefit from the complexvalued embedding on the QA task.\nFrom a mathematical point of view, complexvalued word embedding and other complex-valued components forms a new Hilbert vector space for modelling language, with a new definitions of addition and multiplication, as well as a new inner product operation. For instance, addition in the word meaning combination is defined as z =z 1 + z 2 = r 1 e i\u03b8 1 + r 2 e i\u03b8 2 = r 2 1 + r 2 2 + 2r 1 r 2 cos(\u03b8 2 \u2212 \u03b8 1 ) \u00d7 e i arctan r 1 sin(\u03b8 1 )+r 2 sin(\u03b8 2 ) r 1 cos(\u03b8 1 )+r 2 cos(\u03b8 2 )\nwhere z 1 and z 2 are the values for the corresponding element for two different word vectors |w 1 and |w 2 respectively. Both the amplitudes and complex phases of z are added with a nonlinear combination of phases and amplitudes of z 1 and z 2 . A classical linear addition gives\u1e91 = r 1 + r 2 , which can be viewed as a degenerating case of the complex-valued addition with the phase information being removed (\u03b8 1 = \u03b8 2 = 0 in the example).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusions and Future Work", "text": "Towards the interpretable matching issue, we propose two research questions to investigate the possibility of language modelling with quantum mathematical framework. To this end, we design a new framework to model all the linguistic units in a unified Hilbert space with well-defined mathematical constraints and explicit physical meaning. We implement the above framework with neural network and then demonstrate its effectiveness in question answering (QA) task. Due to the welldesigned components, our model is advantageous with its interpretability in term of transparency and post-hoc interpretability, and also shows its potential to use complex-valued components in NLP. Despite the effectiveness of the current network, we would like to further explore the phase part in complex-valued word embedding to directly link to concrete semantics such as word sentiment or word position. Another possible direction is to borrow other quantum concepts to capture the interaction and non-interaction between word semantics, such as the Fock Space (Sozzo, 2014) which considers both interacting and noninteracting entities in different Hilbert Spaces. Furthermore, a deeper and robust quantuminspired neural architecture in a higher-dimension Hilbert space like (Zhang et al., 2018b) is also worth to be investigated for achieving stronger performances with better explanatory power.", "publication_ref": ["b18", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We thank Sagar Uprety, Dawei Song, and Prayag Tiwari for helpful discussions. Peng Zhang and Peter Bruza gave us constructive comments to improve the paper. The GPU computing resources are partly supported by Beijing Ultrapower Software Co., Ltd and Jianquan Li. Anne Elize R. V. Lima helped us redraw the figures using her talent.\nThe three authors are supported by the Quantum Access and Retrieval Theory (QUARTZ) project, which has received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sk\u0142odowska-Curie grant agreement No. 721321.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Quantum Entanglement in Concept Combinations", "journal": "International Journal of Theoretical Physics", "year": "2014", "authors": "Diederik Aerts; Sandro Sozzo"}, {"ref_id": "b1", "title": "Entangling words and meaning. pages QI", "journal": "College Publications", "year": "2008", "authors": "Kirsty Peter D Bruza; Douglas Kitto; Cathy Mcevoy;  Mcevoy"}, {"ref_id": "b2", "title": "The use of multiple measurements in taxonomic problems", "journal": "Annals of Eugenics", "year": "1936", "authors": "A Ronald;  Fisher"}, {"ref_id": "b3", "title": "Measures on the closed subspaces of a hilbert space", "journal": "J. of Math. and Mech", "year": "1957", "authors": " Andrew M Gleason"}, {"ref_id": "b4", "title": "Semantic and lexical universals: Theory and empirical findings", "journal": "John Benjamins Publishing", "year": "1994", "authors": "Cliff Goddard; Anna Wierzbicka"}, {"ref_id": "b5", "title": "Multi-Perspective Sentence Similarity Modeling with Convolutional Neural Networks", "journal": "ACL", "year": "2015", "authors": "Hua He; Kevin Gimpel; Jimmy Lin"}, {"ref_id": "b6", "title": "Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement", "journal": "ACL", "year": "2016", "authors": "Hua He; Jimmy Lin"}, {"ref_id": "b7", "title": "Convolutional Neural Network Architectures for Matching Natural Language Sentences", "journal": "Curran Associates, Inc", "year": "2014", "authors": "Baotian Hu; Zhengdong Lu; Hang Li; Qingcai Chen"}, {"ref_id": "b8", "title": "The structure and interpretation of quantum mechanics", "journal": "Harvard university press", "year": "1992", "authors": "I G Richard;  Hughes"}, {"ref_id": "b9", "title": "Bag of tricks for efficient text classification", "journal": "", "year": "2016", "authors": "Armand Joulin; Edouard Grave; Piotr Bojanowski; Tomas Mikolov"}, {"ref_id": "b10", "title": "The Mythos of Model Interpretability", "journal": "Queue", "year": "2018", "authors": "Zachary C Lipton"}, {"ref_id": "b11", "title": "Iterative maximum-likelihood reconstruction in quantum homodyne tomography", "journal": "Journal of Optics B: Quantum and Semiclassical Optics", "year": "2004", "authors": "A I Lvovsky"}, {"ref_id": "b12", "title": "Neural Variational Inference for Text Processing", "journal": "", "year": "2015", "authors": "Yishu Miao; Lei Yu; Phil Blunsom"}, {"ref_id": "b13", "title": "Quantum computation and quantum information", "journal": "Cambridge University Press", "year": "2010", "authors": "A Michael; Isaac L Nielsen;  Chuang"}, {"ref_id": "b14", "title": "Glove: Global vectors for word representation", "journal": "", "year": "2014", "authors": "Jeffrey Pennington; Richard Socher; Christopher D Manning"}, {"ref_id": "b15", "title": "Convolutional neural tensor network architecture for communitybased question answering", "journal": "", "year": "2015", "authors": "Xipeng Qiu; Xuanjing Huang"}, {"ref_id": "b16", "title": "Attentive pooling networks", "journal": "", "year": "2016", "authors": "Santos Cicero Dos; Ming Tan; Bing Xiang; Bowen Zhou"}, {"ref_id": "b17", "title": "Modeling term dependencies with quantum language models for ir", "journal": "ACM", "year": "2013", "authors": "Alessandro Sordoni; Jian-Yun Nie; Yoshua Bengio"}, {"ref_id": "b18", "title": "A quantum probability explanation in Fock space for borderline contradictions", "journal": "Journal of Mathematical Psychology", "year": "2014", "authors": "Sandro Sozzo"}, {"ref_id": "b19", "title": "LSTM-based Deep Learning Models for Non-factoid Answer Selection", "journal": "", "year": "2015", "authors": "Ming Tan; Bing Cicero Dos Santos; Bowen Xiang;  Zhou"}, {"ref_id": "b20", "title": "Document Modeling with Gated Recurrent Neural Network for Sentiment Classification", "journal": "", "year": "2015", "authors": "Duyu Tang; Bing Qin; Ting Liu"}, {"ref_id": "b21", "title": "The geometry of information retrieval", "journal": "Cambridge University Press", "year": "2004", "authors": "Cornelis Joost Van Rijsbergen"}, {"ref_id": "b22", "title": "Iterative algorithm for reconstruction of entangled states", "journal": "Phys. Rev. A", "year": "2001", "authors": "J \u0158eh\u00e1\u010dek; Z Hradil; M Je\u017eek"}, {"ref_id": "b23", "title": "Building a question answering test collection. SIGIR", "journal": "", "year": "2000", "authors": "M Ellen; Dawn M Voorhees;  Tice"}, {"ref_id": "b24", "title": "A Long Short-Term Memory Model for Answer Sentence Selection in Question Answering", "journal": "", "year": "2015", "authors": "Di Wang; Eric Nyberg"}, {"ref_id": "b25", "title": "aNMM: Ranking Short Answer Texts with Attention-Based Neural Matching Model", "journal": "ACM", "year": "2016", "authors": "Liu Yang; Qingyao Ai; Jiafeng Guo; W Bruce Croft"}, {"ref_id": "b26", "title": "WikiQA: A Challenge Dataset for Open-Domain Question Answering", "journal": "Association for Computational Linguistics", "year": "2015", "authors": "Yi Yang; Yih Wen-Tau; Christopher Meek"}, {"ref_id": "b27", "title": "Deep Learning for Answer Sentence Selection", "journal": "", "year": "2014", "authors": "Lei Yu; Karl Moritz Hermann; Phil Blunsom; Stephen Pulman"}, {"ref_id": "b28", "title": "End-to-End Quantumlike Language Models with Application to Question Answering", "journal": "AAAI", "year": "2018", "authors": "Peng Zhang; Jiabin Niu; Zhan Su; Benyou Wang; Liqun Ma; Dawei Song"}, {"ref_id": "b29", "title": "A quantum manybody wave function inspired language modeling approach", "journal": "ACM", "year": "2018", "authors": "Peng Zhang; Zhan Su; Lipeng Zhang; Benyou Wang; Dawei Song"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Architecture of Complex-valued Network for Matching. M refers to the measurement operation in Eq. 2.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Architecture of local mixture component. A sliding window in the black color is applied to the sentence, generating a local mixture density matrix for each local window of length l.means that a matrix multiplies a number with each element. denotes the outer product of a vector with itself.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Dataset Statistics. For each cell, the values denote the number of questions and question-answer pairs respectively.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Experiment Results on TREC QA Dataset. The best performed values are in bold.", "figure_data": "ModelMAPMRRBigram-CNN0.61900.6281QA-BILSTM0.65570.6695AP-BILSTM0.67050.6842LSTM-attn0.66390.6828CNN-Cnt0.65200.6652QLM0.51200.5150NNQLM-I0.54620.5574NNQLM-II0.64960.6594CNM0.67480.6864Over NNQLM-II 3.88% \u2191 4.09% \u2191"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Experiment Results on WikiQA Dataset.The best performed values for each dataset are in bold.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Ablation Test. The values in parenthesis are the performance differences between the model and CNM.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Physical meanings and constraints.", "figure_data": "Selected wordsstudio, president, women, philosophyImportantscandinavian, washingtonian, berliner, championshipdefiance, reporting, adjusted, jarred71.2, 5.5, 4m, 296036, 3.5Unimportantmay, be, all, bornmovements, economists, revenues, computers"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Selected learned important words in TREC QA. All words are converted to lower cases.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Who is the [ president or chief executive of Amtrak ] ? \" Long-term success ... \" said George Warrington , [ Amtrak 's president and chief executive ] .\" When [ was Florence Nightingale born ] ? ,\"On May 12 , 1820 , the founder of modern nursing , [ Florence Nightingale , was born ] in Florence , Italy .\" When [ was the IFC established ] ? [ IFC was established in ] 1956 as a member of the World Bank Group . [ how did women 's role change during the war ] ..., the [ World Wars started a new era for women 's ] opportunities to .... [ Why did the Heaven 's Gate members commit suicide ] ?, This is not just a case of [ members of the Heaven 's Gate cult committing suicide ] to ...", "figure_data": "QuestionCorrect Answer"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "The matching patterns for specific sentence pairs in TREC QA. The darker the color, the bigger the word weight is. [ and ] denotes the possible border of the current sliding windows.", "figure_data": ""}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Selected learned measurements for TREC QA. They were selected according to nearest words for a measurement vector in Semantic Hilbert Space.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "|\u03c6 = \u03b1 0 |0 + \u03b1 1 |1 ,(1)", "formula_coordinates": [2.0, 366.9, 117.81, 158.64, 10.63]}, {"formula_id": "formula_1", "formula_text": "0 \u2264 |\u03b1 0 | 2 \u2264 1, 0 \u2264 |\u03b1 1 | 2 \u2264 1 and |\u03b1 0 | 2 + |\u03b1 1 | 2 = 1.", "formula_coordinates": [2.0, 307.28, 153.81, 218.27, 25.07]}, {"formula_id": "formula_2", "formula_text": "p x (\u03c1) = x| \u03c1 |x = tr(\u03c1 |x x|)(2)", "formula_coordinates": [2.0, 344.31, 420.29, 181.23, 10.63]}, {"formula_id": "formula_3", "formula_text": "|w = n j=1 r j e i\u03c6 j |e j ,(3)", "formula_coordinates": [3.0, 134.15, 587.67, 156.12, 33.71]}, {"formula_id": "formula_4", "formula_text": "\u03c1 = m j 1 m |w j w j |,(4)", "formula_coordinates": [3.0, 368.75, 87.54, 156.8, 33.71]}, {"formula_id": "formula_5", "formula_text": "|w = w || w|| , \u03c0(w) = || w||,(5)", "formula_coordinates": [3.0, 344.53, 706.76, 181.02, 24.43]}, {"formula_id": "formula_6", "formula_text": "\u03c1 = l i p(w i ) |w i w i |,(6)", "formula_coordinates": [4.0, 363.15, 594.54, 162.39, 33.71]}, {"formula_id": "formula_7", "formula_text": "p 1 jk = v k | \u03c1 1j |v k and p 2 jk = v k | \u03c1 2j |v k for k \u2208 {1, .", "formula_coordinates": [5.0, 72.0, 267.75, 218.27, 40.03]}, {"formula_id": "formula_8", "formula_text": "unit complex vector / superposition state {w|w \u2208 C n , ||w||2 = 1} N-gram/ Word combinations real vector (\u2212\u221e, \u221e) density matrix / mixed system {\u03c1|\u03c1 = \u03c1 * , tr(\u03c1) = 1 Abstraction CNN/RNN (\u2212\u221e, \u221e) projector / measurement {vv T |v \u2208 C n , ||v||2 = 1} Sentence representation real vector (\u2212\u221e, \u221e)", "formula_coordinates": [7.0, 315.69, 92.34, 203.05, 56.16]}], "doi": "10.1007/s10773-013-1946-z"}